{"files":[{"patch":"@@ -52,0 +52,1 @@\n+    - [Exploring Triton GPU programming for neural networks in Java](articles\/triton)  (February 2024)\n","filename":"site\/_index.md","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,1200 @@\n+# Exploring Triton GPU programming for neural networks in Java\n+\n+#### Paul Sandoz {.author}\n+\n+#### February 2024 {.date}\n+\n+In this article we will explain how we can use Code Reflection to implement the\n+[Triton][Triton-intro] programming model in Java as an alternative to Python.\n+\n+Code Reflection is a Java platform feature being researched and developed under\n+OpenJDK Project [Babylon](https:\/\/openjdk.org\/projects\/babylon\/).\n+\n+We will introduce Code Reflection concepts and APIs as we explain the problem\n+and present a solution. The explanations are neither exhaustive nor very\n+detailed, they are designed to give the reader an intuitive sense and\n+understanding of Code Reflection and its capabilities.\n+\n+## Triton\n+\n+[Triton][Triton-intro] is a domain-specific programming model and compiler\n+developers can use to write programs in Python that compile to GPU code.\n+\n+Triton enables developers with little or no experience of GPU hardware and\n+GPU-specific programming languages, such as CUDA, to write very efficient\n+parallel programs.\n+\n+[Triton-intro]: https:\/\/triton-lang.org\/main\/programming-guide\/chapter-1\/introduction.html\n+\n+The release announcement for Triton [states][Triton-blog]:\n+\n+> Triton makes it possible to reach peak hardware performance with relatively\n+> little effort; for example, it can be used to write FP16 matrix multiplication\n+> kernels that match the performance of cuBLAS—something that many GPU\n+> programmers can’t do—in under 25 lines of code. Our researchers have already\n+> used it to produce kernels that are up to 2x more efficient than equivalent\n+> Torch implementations, and we’re excited to work with the community to make\n+> GPU programming more accessible to everyone.\n+\n+[Triton-blog]: https:\/\/openai.com\/research\/triton\n+\n+The Triton programming model hides the thread-based programming model of CUDA.\n+Thereby, the Triton compiler is able to better leverage the GPU hardware e.g.,\n+such as optimizing for cases that might otherwise require explicit\n+synchronization.\n+\n+To enable this abstraction the developer programs against a Triton Python API,\n+where arithmetic computations are performed on tensors rather than scalars. Such\n+tensors are required to have constant shape, the number of dimensions and their\n+size must be constant (in addition the size must be a power of two).\n+\n+### Vector addition\n+\n+To explain the programing model we shall present a simple example, vector\n+addition. This example is instructive even though it can be easily written in\n+CUDA.\n+\n+The complete example is presented as a [tutorial][tutorial-vector-addition] at\n+the Triton website, including how Triton integrates with PyTorch. We shall focus\n+on the Triton program.\n+\n+[tutorial-vector-addition]: https:\/\/triton-lang.org\/main\/getting-started\/tutorials\/01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py\n+\n+```python\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def add_kernel(x_ptr,  # *Pointer* to first input vector.\n+               y_ptr,  # *Pointer* to second input vector.\n+               output_ptr,  # *Pointer* to output vector.\n+               n_elements,  # Size of the vector.\n+               BLOCK_SIZE: tl.constexpr,\n+               # Number of elements each program should process.\n+               # NOTE: `constexpr` so it can be used as a shape value.\n+               ):\n+    # There are multiple 'programs' processing different data. We identify which program\n+    # we are here:\n+    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n+    # This program will process inputs that are offset from the initial data.\n+    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n+    # would each access the elements [0:64, 64:128, 128:192, 192:256].\n+    # Note that offsets is a list of pointers:\n+    block_start = pid * BLOCK_SIZE\n+    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+    # Create a mask to guard memory operations against out-of-bounds accesses.\n+    mask = offsets < n_elements\n+    # Load x and y from DRAM, masking out any extra elements in case the input is not a\n+    # multiple of the block size.\n+    x = tl.load(x_ptr + offsets, mask=mask)\n+    y = tl.load(y_ptr + offsets, mask=mask)\n+    output = x + y\n+    # Write x + y back to DRAM.\n+    tl.store(output_ptr + offsets, output, mask=mask)\n+```\n+\n+The comments are quite informative, and we recommend taking a few moments to\n+read them carefully. The program is annotated with `@triton.jit`, which\n+identifies it as a Triton program.\n+\n+This program is designed to be compiled to a GPU program and executed multiple\n+times in parallel on the GPU. Each execution will have a program identifier\n+associated with it, obtained by calling the Triton langauge API\n+method `program_id`. This is not a thread identifier, although developers\n+familiar with CUDA will likely recognize that it is used in similar manner.\n+\n+The program identifier is used to locate the start index in the input and output\n+vectors from which to compute on. The end index is determined by the method\n+parameter, `BLOCK_SIZE`. Notice that this is annotated with `tl.constexpr`. When\n+the program is compiled it is required that `BLOCK_SIZE` be passed as a constant\n+value that is a power of two. Therefore, the interval of computation\n+is `[pid * BLOCK_SIZE, pid * BLOCK_SIZE + BLOCK_SIZE)`. Program execution will\n+be arranged such that program identifiers are proportioned according to the\n+total size of the computation with respect to `BLOCK_SIZE`.\n+\n+The program computes the start index, held in the scalar variable `block_start`,\n+but does not compute the end index. Instead, the program creates a _tensor_, by\n+calling the method `tl.arange`:\n+\n+```python\n+tl.arange(0, BLOCK_SIZE)\n+```\n+\n+This method creates a tensor of one dimension whose size is `BLOCK_SIZE`. The\n+elements of the tensor are 32-bit integers, and they are initialized to range\n+from `0` to `BLOCK_SIZE - 1` consecutively. If, for example, the program was\n+compiled with `BLOCK_SIZE=64` then we know the tensor's shape i.e., we know the\n+_type_ of the tensor. This is a very important property. It means we can\n+statically check that expressions with tensors are type safe with respect to\n+their shape, and if necessary insert operations to convert tensors and scalars\n+(known as splatting or broadcasting). (In this case we also know the tensor's\n+elements are constant.)\n+\n+The result of that tensor is then input to the following arithmetic expression\n+which computes offsets into the vectors (pointed to by `x_ptr` etc.).\n+\n+```python\n+offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+```\n+\n+Python's dynamic typing and flexible operator overloading allows the program to\n+express the addition of a scalar with a tensor. Since we know the type of the\n+tensor we can convert the scalar `block_start` to a tensor of the same type and\n+whose elements all have the same value as `block_start`. This is generally\n+referred to as a _broadcasting_ operation (or sometimes referred to as splatting\n+when broadcasting scalars). Then we can add the two tensors together, resulting\n+in the `offsets` tensor. We know this is a 1D tensor of size `BLOCK_SIZE`, whose\n+elements range from `[block_start, block_start + BLOCK_SIZE)`.\n+\n+The `offsets` tensor may reference vector elements that are out of bounds i.e.\n+values greater than the size of the input and output vectors, `n_elements`. To\n+protect against out of bounds access the program creates a tensor _mask_.\n+\n+```python\n+mask = offsets < n_elements\n+```\n+\n+Again, Python's dynamic typing allows the program to compare a tensor with a\n+scalar value. Similarly to the prior addition, we can broadcast the value of\n+`n_elements` to a tensor with the same type as `offsets`. The elements of each\n+tensor at the same index are compared, producing a `0` or `1` element at the\n+same index in the resulting `mask` tensor if the comparison returned `false`\n+or `true` respectively.\n+\n+Given the `offsets` and `mask` tensors the program can safely load tensors from\n+memory pointed to by the pointers `x_ptr` and `y_ptr`.\n+\n+```python\n+x = tl.load(x_ptr + offsets, mask=mask)\n+y = tl.load(y_ptr + offsets, mask=mask)\n+```\n+\n+If `x_ptr` points to 32-bit floating point values, the\n+expression `x_ptr + offsets` results in a tensor of pointers to 32-bit floating\n+point values (and similarly for `y_ptr`). We know the pointers are contiguous in\n+memory, in the\n+interval `[x_ptr + block_start, x_ptr + block_start + BLOCK_SIZE)`.\n+\n+The `tl.load` method will load a tensor of values from memory pointed to by\n+tensor of pointers. The resulting tensor has the same shape as the tensor of\n+pointers, and a zero value will be placed in the resulting tensor for any\n+corresponding zero mask value (for access that is out of bounds).\n+\n+The program can then add the two floating point tensors together and store the\n+result.\n+\n+### Triton compiler\n+\n+The Triton compiler is responsible for compiling a Triton program (a program\n+written in Python and annotated with `@triton.jit`) to a GPU program, commonly\n+referred to as a kernel.\n+\n+The stages of the compiler can be broadly described in the following diagram.\n+\n+```text\n+    Python program\n+      |\n+      |  AST visitor\n+      V\n+    Triton MLIR\n+      |\n+      |  Triton MLIR compiler\n+      V\n+     PTX\n+```\n+\n+The Triton compiler transforms the Python program to Triton MLIR which is then\n+compiled to PTX by the native Triton MLIR compiler.\n+\n+The Multi-Level IR Compiler Framework ([MLIR][MLIR]) provides reusable and\n+extensible compiler infrastructure. It defines a metamodel for representing and\n+transforming code, with corresponding C\/C++ APIs and modular infrastructure for\n+building compilers. Program functionality is specified by MLIR _dialects_\n+that define a set of types and operations e.g., such as types and operations\n+associated with linear algebra.\n+\n+[MLIR]:https:\/\/mlir.llvm.org\/\n+\n+The Triton compiler supports a set of MLIR [dialects][Triton-dialects], which\n+define types and operations specific to Triton programs. The Triton dialect uses\n+and depends on other MLIR dialects. For example, it uses the\n+[arith][arith-dialect] dialect and the [ranked tensor type][tensor-type] of the\n+builtin dialect. Reusing dialects means the Triton MLIR compiler can reuse\n+existing compiler infrastructure to compile Triton MLIR. In fact, the Triton\n+MLIR compiler is itself likely composed of multiple transformations that\n+progressively lower the program to PTX code.\n+\n+[Triton-dialects]:https:\/\/triton-lang.org\/main\/dialects\/dialects.html\n+\n+[arith-dialect]:https:\/\/mlir.llvm.org\/docs\/Dialects\/ArithOps\/\n+\n+[tensor-type]:https:\/\/mlir.llvm.org\/docs\/Dialects\/Builtin\/#rankedtensortype\n+\n+Transformation to Triton MLIR is performed by visiting the AST of the Python\n+program. The AST visitor is responsible for type checking the Triton program.\n+This will include ensuring all tensors are of known shape (derived from\n+constants input to the compiler, as explained in the next section), and\n+expressions using tensors are shape compatible. (Some aspects of this were\n+mentioned when describing the vector addition example.)\n+\n+Given type correct tensor expressions the AST visitor can build a Triton MLIR\n+program, inserting appropriate tensor operations and broadcasting conversions.\n+\n+### MLIR of a Triton program\n+\n+We can execute the Triton compiler against the prior Triton program and view the\n+Triton MLIR program. (Note that we had to make a few modifications to the Triton\n+compiler to print out the Triton MLIR program in the absence of available CUDA\n+software and supporting hardware.)\n+\n+```shell\n+python3 python\/triton\/tools\/compile.py \\\n+  --kernel-name add_kernel \\\n+  --signature \"*fp32,*fp32,*fp32,i32,64\" \\\n+  --grid=1024,1024,1024 \\\n+  python\/tutorials\/01-vector-add.py\n+```\n+\n+The compiler requires a signature of the kernel that declares the types of the\n+Triton program's parameters. Thereby, the compiler can perform static type\n+checking. Notice the string \"64\" represents a constant type of integer whose\n+value is 64, and is associated with the constant parameter `BLOCK_SIZE`.\n+\n+The textual-form of the Triton MLIR program is as follows:\n+\n+```mlir\n+module {\n+  tt.func public @add_kernel_0123(%arg0: !tt.ptr<f32, 1> ,\n+                                  %arg1: !tt.ptr<f32, 1> , \n+                                  %arg2: !tt.ptr<f32, 1> , \n+                                  %arg3: i32 ) \n+                                  attributes {noinline = false} {\n+    %0 = tt.get_program_id x : i32\n+    %c64_i32 = arith.constant 64 : i32\n+    %1 = arith.muli %0, %c64_i32 : i32\n+    %2 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+    %3 = tt.splat %1 : (i32) -> tensor<64xi32>\n+    %4 = arith.addi %3, %2 : tensor<64xi32>\n+    %5 = tt.splat %arg3 : (i32) -> tensor<64xi32>\n+    %6 = arith.cmpi slt, %4, %5 : tensor<64xi32>\n+    %7 = tt.splat %arg0 : (!tt.ptr<f32, 1>) -> tensor<64x!tt.ptr<f32, 1>>\n+    %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32, 1>>, tensor<64xi32>\n+    %9 = tt.load %8, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n+    %10 = tt.splat %arg1 : (!tt.ptr<f32, 1>) -> tensor<64x!tt.ptr<f32, 1>>\n+    %11 = tt.addptr %10, %4 : tensor<64x!tt.ptr<f32, 1>>, tensor<64xi32>\n+    %12 = tt.load %11, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n+    %13 = arith.addf %9, %12 : tensor<64xf32>\n+    %14 = tt.splat %arg2 : (!tt.ptr<f32, 1>) -> tensor<64x!tt.ptr<f32, 1>>\n+    %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32, 1>>, tensor<64xi32>\n+    tt.store %15, %13, %6 {cache = 1 : i32, evict = 1 : i32} : tensor<64xf32>\n+    tt.return\n+  }\n+}\n+```\n+\n+MLIR programs have the property of Static Single-Assignment (SSA). We refer to\n+variables that can only be assigned once as values (they are a bit like final\n+variables in Java) .e.g., value `%0` can never be modified.\n+\n+Notice that there are only four parameters corresponding to values `%arg0`\n+to `%arg3`: three pointers to 32-bit floats corresponding to the two input\n+vectors and the output vector; and the 32-bit integer corresponding to the size\n+of the vectors. The constant parameter has been folded away.\n+\n+The Python method call to `tl.arange` has been transformed to the Triton MLIR\n+operation `tt.make_range`. The constant values passed to the Python method (the\n+constant 0, and constant `BLOCK_SIZE` whose value is 64) are present as the\n+operation's attributes. The operation returns a value, `%2`, whose type\n+is `tensor<64xi32>`, a ranked tensor of one dimension with a size of 64, and\n+whose elements are 32-bit integers.\n+\n+The addition in the Python expression `block_start + tl.arange(0, BLOCK_SIZE)`\n+has been transformed into two operations:\n+\n+```\n+%3 = tt.splat %1 : (i32) -> tensor<64xi32>\n+%4 = arith.addi %3, %2 : tensor<64xi32>\n+```\n+\n+Before performing the addition the scalar value is converted to a tensor\n+(\"splatting\"). If we carefully follow the use of values in subsequent operations\n+we can observe that all the tensors are of constant shape.\n+\n+## Triton programs in Java\n+\n+We will show that it is feasible for developers to write Triton programs in Java\n+that are surprisingly comparable to the Triton programs in Python, and have the\n+potential to compile to PTX code. The focus will be on the front-end of a Java\n+Triton compiler that has the following stages.\n+\n+```text\n+    Java program\n+      |\n+      |  Code reflection\n+      V\n+    Java code model  \n+      |\n+      |  Code model transformer\n+      V\n+    Triton code model    \n+```\n+\n+Using Code Reflection we can obtain a _symbolic representation_ of the Java\n+program, called a Java *code model*. We can then traverse the symbolic\n+information in the Java code model, *operations* modeling Java program\n+behaviour, and apply the rules of the Triton programming model to produce a\n+Triton code model that contains operations modeling Triton program behaviour.\n+Note that this transformation will not preserve Java program meaning. The\n+resulting Triton code model is not a Java program and will not be executed by\n+the Java runtime.\n+\n+The Triton code model can then be input to subsequent compiler stages that\n+transform it to PTX. We shall not focus on that aspect. In theory, it should be\n+possible to reuse the existing native Triton MLIR compiler, first transforming\n+the Triton code model to Triton MLIR.\n+\n+We shall see later that code models have similar properties to MLIR. That is by\n+design. One goal of Code Reflection is to enable interoperation with and reuse\n+of native compiler tool chains. In combination with the Foreign Function and\n+Memory API we have the potential to interact natively with MLIR APIs and\n+toolchains. Triton programming in Java represents an important use-case in this\n+respect.\n+\n+A proof of concept Java Triton API and front-end compiler has been implemented,\n+and is located in the Babylon repo [here][java-triton].\n+\n+[java-triton]:https:\/\/github.com\/openjdk\/babylon\/blob\/code-reflection\/cr-examples\/triton\n+\n+### Vector addition\n+\n+In this section we shall present the [Java version][java-vector-addition] of the\n+Triton vector addition program, and it's Java code model.\n+\n+[java-vector-addition]: https:\/\/github.com\/openjdk\/babylon\/blob\/code-reflection\/cr-examples\/triton\/src\/test\/java\/oracle\/code\/triton\/TestAddKernel.java\n+\n+[\/\/]: # (@formatter:off)\n+```java\n+@CodeReflection\n+static void add_kernel2(Ptr x_ptr,  \/\/ *Pointer* to first input vector.\n+                        Ptr y_ptr,  \/\/ *Pointer* to second input vector.\n+                        Ptr output_ptr,  \/\/ *Pointer* to output vector.\n+                        int n_elements,  \/\/ Size of the vector.\n+                        @Constant int BLOCK_SIZE)  \/\/ Number of elements each program should process.\n+\/\/ NOTE: @Constant so it can be used as a shape value\n+{\n+    \/\/ There are multiple 'programs' processing different data. We identify which program\n+    \/\/ we are here:\n+    var pid = Triton.programId(0); \/\/ We use a 1D launch grid so axis is 0.\n+    \/\/ This program will process inputs that are offset from the initial data.\n+    \/\/ For instance, if you had a vector of length 256 and block_size of 64, the programs\n+    \/\/ would each access the elements [0:64, 64:128, 128:192, 192:256].\n+    \/\/ Note that offsets is a list of pointers:\n+    var block_start = pid * BLOCK_SIZE;\n+    var range = Triton.arange(0, BLOCK_SIZE);\n+    var offsets = Triton.add(block_start, range);\n+    \/\/ Create a mask to guard memory operations against out-of-bounds accesses.\n+    var mask = Triton.compare(offsets, n_elements, Triton.CompareKind.LessThan);\n+    \/\/ Load x and y from DRAM, masking out any extra elements in case the input is not a\n+    \/\/ multiple of the block size.\n+    var x = Triton.load(Triton.add(x_ptr, offsets), mask);\n+    var y = Triton.load(Triton.add(y_ptr, offsets), mask);\n+    var output = Triton.add(x, y);\n+    \/\/ Write x + y back to DRAM.\n+    Triton.store(Triton.add(output_ptr, offsets), output, mask);\n+}\n+```\n+[\/\/]: # (@formatter:on)\n+\n+The Java method, `add_kernel2`, is annotated with `@CodeReflection`. This\n+ensures there is a Java code model available and accessible under similar access\n+control rules as for its invocation.\n+\n+The `Triton` [class][Triton-class] has static methods, such as `arange`, that\n+define the Triton functionality, similarly named as their Python equivalents.\n+There are many other similarities to the equivalent Python version. Vertically\n+very similar. Less so horizontally. An obvious and notable difference is Java's\n+lack of operator overloading, hence the additional `Triton` static methods such\n+as `add` and `compare`.\n+\n+(Is it possible for Java to support operator overloading both for numeric\n+scalars and tensors? The author thinks so and asks the reader to be patient -\n+stay tuned for possible details at a future date.)\n+\n+[Triton-class]: https:\/\/github.com\/openjdk\/babylon\/blob\/code-reflection\/cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/Triton.java\n+\n+However, notice that no explicit broadcasting is required. The arithmetic\n+methods accept arguments that are instances of `Number`. The Triton `Tensor`\n+and `Ptr` classes extend from `Number`. With autoboxing it is possible to mix\n+instances of `Tensor` and boxed scalars e.g., as with the\n+expression `Triton.add(block_start, range)`\n+\n+### Explaining the Java code model\n+\n+A code model is a tree containing operations, bodies, and blocks. An operation\n+contains zero or more bodies. A body contains one or more blocks. A block\n+contains a sequence of one or more operations. A block can declare zero or more\n+block parameters, values. An operation declares an operation result, a value. An\n+operation may use values as operands, but only after they have been declared.\n+\n+Using this simple tree structure we can define operations that model many Java\n+language constructs, and therefore we can build code models that model many Java\n+programs. This may appear surprising at first. Readers may be more familiar with\n+term \"operation\" in a more conventional sense, such as arithmetic operations.\n+However, given the structure described above, there is no need to limit\n+ourselves to this conventional sense. We are free to define an operation whose\n+operational semantics *declare* a function (instances of `CoreOps.FuncOp`),\n+model a Java lambda expression (instances of `CoreOps.LambdaOp`), or model a\n+Java `try` statement (instances of `ExtendedOps.JavaTryOp`). Or, as we shall see\n+later define operations that model Triton programs.\n+\n+What does the code model of `add_kernel2` look like? We can obtain the code\n+model at runtime and serialize its in-memory form to a textual form.\n+\n+```\n+func @\"add_kernel2\" (%0 : oracle.code.triton.Ptr,\n+                     %1 : oracle.code.triton.Ptr, \n+                     %2 : oracle.code.triton.Ptr, \n+                     %3 : int, \n+                     %4 : int)void -> {\n+    %5 : Var<oracle.code.triton.Ptr> = var %0 @\"x_ptr\";\n+    %6 : Var<oracle.code.triton.Ptr> = var %1 @\"y_ptr\";\n+    %7 : Var<oracle.code.triton.Ptr> = var %2 @\"output_ptr\";\n+    %8 : Var<int> = var %3 @\"n_elements\";\n+    %9 : Var<int> = var %4 @\"BLOCK_SIZE\";\n+    %10 : int = constant @\"0\";\n+    %11 : int = invoke %10 \n+            @\"oracle.code.triton.Triton::programId(int)int\";\n+    %12 : Var<int> = var %11 @\"pid\";\n+    %13 : int = var.load %12;\n+    %14 : int = var.load %9;\n+    %15 : int = mul %13 %14;\n+    %16 : Var<int> = var %15 @\"block_start\";\n+    %17 : int = constant @\"0\";\n+    %18 : int = var.load %9;\n+    %19 : oracle.code.triton.Tensor = invoke %17 %18\n+            @\"oracle.code.triton.Triton::arange(int, int)oracle.code.triton.Tensor\";\n+    %20 : Var<oracle.code.triton.Tensor> = var %19 @\"range\";\n+    %21 : int = var.load %16;\n+    %22 : java.lang.Integer = invoke %21\n+            @\"java.lang.Integer::valueOf(int)java.lang.Integer\";\n+    %23 : oracle.code.triton.Tensor = var.load %20;\n+    %24 : oracle.code.triton.Tensor = invoke %22 %23\n+            @\"oracle.code.triton.Triton::add(java.lang.Number, java.lang.Number)oracle.code.triton.Tensor\";\n+    %25 : Var<oracle.code.triton.Tensor> = var %24 @\"offsets\";\n+    %26 : oracle.code.triton.Tensor = var.load %25;\n+    %27 : int = var.load %8;\n+    %28 : java.lang.Integer = invoke %27\n+            @\"java.lang.Integer::valueOf(int)java.lang.Integer\";\n+    %29 : oracle.code.triton.Triton$CompareKind = field.load\n+            @\"oracle.code.triton.Triton$CompareKind::LessThan()oracle.code.triton.Triton$CompareKind\";\n+    %30 : oracle.code.triton.Tensor = invoke %26 %28 %29\n+            @\"oracle.code.triton.Triton::compare(java.lang.Number, java.lang.Number, oracle.code.triton.Triton$CompareKind)oracle.code.triton.Tensor\";\n+    %31 : Var<oracle.code.triton.Tensor> = var %30 @\"mask\";\n+    %32 : oracle.code.triton.Ptr = var.load %5;\n+    %33 : oracle.code.triton.Tensor = var.load %25;\n+    %34 : oracle.code.triton.Tensor = invoke %32 %33\n+            @\"oracle.code.triton.Triton::add(java.lang.Number, java.lang.Number)oracle.code.triton.Tensor\";\n+    %35 : oracle.code.triton.Tensor = var.load %31;\n+    %36 : oracle.code.triton.Tensor = invoke %34 %35\n+            @\"oracle.code.triton.Triton::load(oracle.code.triton.Tensor, oracle.code.triton.Tensor)oracle.code.triton.Tensor\";\n+    %37 : Var<oracle.code.triton.Tensor> = var %36 @\"x\";\n+    %38 : oracle.code.triton.Ptr = var.load %6;\n+    %39 : oracle.code.triton.Tensor = var.load %25;\n+    %40 : oracle.code.triton.Tensor = invoke %38 %39\n+            @\"oracle.code.triton.Triton::add(java.lang.Number, java.lang.Number)oracle.code.triton.Tensor\";\n+    %41 : oracle.code.triton.Tensor = var.load %31;\n+    %42 : oracle.code.triton.Tensor = invoke %40 %41\n+            @\"oracle.code.triton.Triton::load(oracle.code.triton.Tensor, oracle.code.triton.Tensor)oracle.code.triton.Tensor\";\n+    %43 : Var<oracle.code.triton.Tensor> = var %42 @\"y\";\n+    %44 : oracle.code.triton.Tensor = var.load %37;\n+    %45 : oracle.code.triton.Tensor = var.load %43;\n+    %46 : oracle.code.triton.Tensor = invoke %44 %45\n+            @\"oracle.code.triton.Triton::add(java.lang.Number, java.lang.Number)oracle.code.triton.Tensor\";\n+    %47 : Var<oracle.code.triton.Tensor> = var %46 @\"output\";\n+    %48 : oracle.code.triton.Ptr = var.load %7;\n+    %49 : oracle.code.triton.Tensor = var.load %25;\n+    %50 : oracle.code.triton.Tensor = invoke %48 %49\n+            @\"oracle.code.triton.Triton::add(java.lang.Number, java.lang.Number)oracle.code.triton.Tensor\";\n+    %51 : oracle.code.triton.Tensor = var.load %47;\n+    %52 : oracle.code.triton.Tensor = var.load %31;\n+    invoke %50 %51 %52\n+            @\"oracle.code.triton.Triton::store(oracle.code.triton.Tensor, oracle.code.triton.Tensor, oracle.code.triton.Tensor)void\";\n+    return;\n+};\n+```\n+\n+The textual form shows the code model's root is a function declaration (`func`)\n+operation. The function declaration operation has an operation result, like all\n+other operations, but since it's the root of the tree there is no need to\n+present it.\n+\n+The lambda-like expression represents the fusion of the function declaration\n+operation's single body and the body's first and only block, called the entry\n+block. Then there is a sequence of operations in the entry block. For each\n+operation there is an instance of a corresponding class present in the in-memory\n+form, all of which extend from the abstract class `java.lang.reflect.code.Op`.\n+\n+The entry block has four block parameters which model `add_kernel2`'s method\n+parameters. These parameters are used as operands of various operations. Many\n+operations produce operation results, e.g., `%15` the result of a multiplication\n+operation, that are used as operands of subsequent operations, and so on.\n+The `return` operation has a result, again like all other operations, but since\n+that result cannot be meaningfully used we don't present it.\n+\n+Code models have the property of Static Single-Assignment (SSA). We refer to\n+variables that can only be assigned once as values (they are a bit like final\n+variables in Java) .e.g., value `%15` can never be modified. A variable\n+declaration is modeled as an operation that produces a value that holds a\n+value (a box), and access operations load or store to that box.\n+\n+We can see how the operations model Java language constructs like method\n+declarations, variables (method parameters or local variables) and access of\n+variables, binary and unary mathematical operations, or method invocations (\n+e.g., to method `Triton::programId`).\n+\n+We can also see that the general structure of the code model is very similar to\n+the Triton MLIR program presented earlier. The contents are naturally very\n+different, since this code model models a Java program. The compiler we will\n+describe will transform this code model into another code model with similar\n+structure and content as the Triton MLIR program.\n+\n+### Analyzing the Java code model\n+\n+Before we can transform the Java code model we need to analyze it, performing\n+type checking and attributing richer types to all the values declared in the\n+Java code model.\n+\n+To do this we shall traverse the code model building a map of value to computed\n+type (an instance of `Map<j.l.r.code.Value, j.l.r.code.TypeElement>`), that\n+represents the result of attribution after traversal is complete. In effect the\n+traversal performs an _abstract interpretation_ of the code. For each operation\n+encountered we will perform some type-based computation based on its semantics.\n+\n+We first have to initialize (or seed) the map with the method's parameters and\n+their richer types. Here is the test method for testing the vector addition\n+program. It provides the list of _code model types_ that are attributed to the\n+program's parameters.\n+\n+[\/\/]: # (@formatter:off)\n+```java\n+@TritonTestExtension.Kernel(\"add_kernel2\")\n+@Test\n+public void test2(TritonTestData t) {\n+    List<TypeElement> argTypes = List.of(\n+            new PtrType(JavaType.FLOAT),\n+            new PtrType(JavaType.FLOAT),\n+            new PtrType(JavaType.FLOAT),\n+            JavaType.INT,\n+            new ConstantType(JavaType.INT, 64));\n+\n+    t.test(argTypes);\n+}\n+```\n+[\/\/]: # (@formatter:on)\n+\n+This is conceptually similar to the signature shown earlier that was passed to\n+the (Python) Triton compiler. We have:\n+\n+- instances of `PtrType` encapsulating the type of value it points to, that are\n+  attributed to values of `Ptr`\n+- instances of `ConstantType` encapsulating the constant type and its value,\n+  that are attributed to values that are constant\n+- (similarly, but not shown), we also have instances of `TensorType`\n+  encapsulating shape and element type, that are attributed to values\n+  of `Tensor`.\n+\n+These classes model Triton types, instances are Triton code model types\n+extending from `TritonType` which in turn extends from `j.l.r.code.TypeElement`.\n+The class `JavaType` models Java types, and similarly extends\n+from `j.l.r.code.TypeElement`. Therefore, we can use pattern matching to operate\n+on specific kinds of code model types, or alternatively we can uniformly operate\n+on all code model types.\n+\n+Triton types are not Java types i.e., they cannot be declared in Java programs\n+as the types of variables. However, notice that Triton types can conveniently\n+reuse other kinds of code model types for their own purposes e.g., a pointer to\n+32-bit signed integers, conveniently reusing the `JavaType.INT` code model type\n+that models the Java primitive `int` type.\n+\n+When we encounter an operation we look up the attributed types of its operands\n+(values) that were previously computed, and from those types compute a type that\n+is attributed to the operation's result. And so on for subsequent operations.\n+\n+We can lean into this concept and provide a higher-level abstraction for\n+invocation operations to methods on the `Triton` class and implement a \"mirror\"\n+class, `TritonTypeInterpreter`, that has methods of the same name with\n+parameters corresponding to the expected attributed types. When we encounter\n+such an invoke operation we obtain the attributed types from the operands using\n+the map and then, using reflection, invoke the method on the mirror class.\n+\n+Here's the attribute implementation for the `Triton.arange` method:\n+\n+```java\n+\/\/                Tensor arange(@Constant int start, @Constant int end)\n+public static TensorType arange(ConstantType start, ConstantType end) {\n+    assert start.cType().equals(JavaType.INT);\n+    assert end.cType().equals(JavaType.INT);\n+\n+    int startValue = (int) start.value();\n+    int endValue = (int) end.value();\n+\n+    return new TensorType(JavaType.INT, List.of(endValue - startValue));\n+}\n+```\n+\n+We expect that both arguments passed to `Triton.arange` are integer constant\n+types. From the constant parameters, `start` and `end`, we can construct and\n+return a `TensorType` of one dimension with the appropriate size, and whose\n+elements are 32-bit integers.\n+\n+Here's the attribute implementation for the `Triton.load` method:\n+\n+```java\n+\/\/                Tensor load(Tensor ptr, Tensor mask)\n+public static TensorType load(TensorType ptr, TensorType mask) {\n+    checkTensorShape(ptr, mask);\n+    if (ptr.eType() instanceof PtrType eptr) {\n+        return new TensorType(eptr.rType(), ptr.shape());\n+    }\n+\n+    throw new IllegalStateException();\n+}\n+```\n+\n+In this case we know that `Triton.load` can only accept values that are tensors,\n+and so they are attributed tensor types. We first check that the tensor of\n+pointers is shape compatible with the mask (either its of the same shape as the\n+pointer or can be broadcast to that shape). Then we check that the tensor's\n+element type is a pointer type. If so, we construct and return a tensor type\n+whose shape is the same as the tensor of pointers, and whose element type is\n+type pointed to.\n+\n+Below is a snippet of the Java code model presented above with comments showing\n+the mapping of values to attributed types.\n+\n+```\n+    %16 : Var<int> = var %15 @\"block_start\";\n+ \/\/ %16 : Var<int> -> int    \n+    %17 : int = constant @\"0\";\n+ \/\/ %17 : int -> constant<int, c0>\n+    %18 : int = var.load %9;\n+ \/\/ %18 : int -> constant<int, c64>    \n+    %19 : oracle.code.triton.Tensor = invoke %17 %18\n+            @\"oracle.code.triton.Triton::arange(int, int)oracle.code.triton.Tensor\";\n+ \/\/ %19 : oracle.code.triton.Tensor -> tensor<x64, int>\n+    %20 : Var<oracle.code.triton.Tensor> = var %19 @\"range\";\n+ \/\/ %20 : Var<oracle.code.triton.Tensor> -> tensor<x64, int>\n+    %21 : int = var.load %16;\n+ \/\/ %21 : int -> int\n+    %22 : java.lang.Integer = invoke %21\n+            @\"java.lang.Integer::valueOf(int)java.lang.Integer\";\n+ \/\/ %22 : java.lang.Integer -> int\n+    %23 : oracle.code.triton.Tensor = var.load %20;\n+ \/\/ %23 : oracle.code.triton.Tensor -> tensor<x64, int>\n+    %24 : oracle.code.triton.Tensor = invoke %22 %23\n+            @\"oracle.code.triton.Triton::add(java.lang.Number, java.lang.Number)oracle.code.triton.Tensor\";\n+ \/\/ %24 : oracle.code.triton.Tensor -> tensor<x64, int>\n+```\n+\n+### Transforming the Java code model\n+\n+Once we have computed the map of values to attributed types we can transform the\n+Java code model to a Triton code model.\n+\n+Code models are immutable. Code models can be produced by building, or by\n+transforming an existing code model. Transforming takes an input code model and\n+builds an output code model. For each input operation encountered in the input\n+code model we have the choice to add that operation to the builder of the output\n+code model (copying), to not add it (removal), or add new output operations\n+(replacement or addition). When an input operation is copied it's input result\n+is associated with the output result of the copied output operation. In other\n+cases it is possible to explicitly control this association. If a subsequently\n+encountered input operation uses that input result then the transformation can\n+obtain the output result from the input result.\n+\n+In general code models are not bound to just modeling Java programs. It is\n+possible to define a set of operations and types that can be used to model\n+programs in a specific domain. The Triton code model will contain Triton\n+specific operations, and also those that correlate closely with dependent MLIR\n+dialects of Triton MLIR.\n+\n+Values in a Triton code model can have Triton specific code model types. In\n+fact, we have already mentioned them in the prior section, they are types\n+attributed to values in the Java code model e.g., `TensorType` and `PtrType`. We\n+reuse them when building the Triton code model.\n+\n+The goal here is to produce a code model that very similar to Triton MLIR. Note,\n+it is not a goal to compete with MLIR - we want to interoperate with it.\n+\n+We can define a \"mirror\" of the `Triton` API for building operations associated\n+with invocations to that API called `TritonBuilderInterpreter` that has methods\n+of the same name (similar to `TritonTypeInterpreter` we discussed earlier). The\n+method parameters will be pairs of attributed type and input value, one pair for\n+each parameter and one pair for the return.\n+\n+Here's the builder implementation for the `Triton.arange` method:\n+\n+```java\n+\/\/    Tensor arange(@Constant int start, @Constant int end)\n+public Value arange(TensorType rType, Op.Result r,\n+                    ConstantType startType, Value start,\n+                    ConstantType endType, Value end) {\n+    return block.op(TritonOps.makeRange(\n+            (int) startType.value(),\n+            (int) endType.value()));\n+}\n+```\n+\n+We use the output block builder, `block`, to replace the invocation to\n+`Triton.arange` with the Triton make range operation, whose result type is the\n+same as `rType`. Note in this case we don't use `rType`, since the operation\n+reconstructs an equivalent instance. Instances of `TypeElement` are value-based,\n+so we could assert such equality e.g.,\n+\n+[\/\/]: # (@formatter:off)\n+```java\n+assert arangeOp.result().type().equals(rType);\n+```\n+[\/\/]: # (@formatter:on)\n+\n+Here's the builder implementation for the `Triton.load` method:\n+\n+```java\n+\/\/    Tensor load(Tensor ptr, Tensor mask)\n+public Value load(TensorType rType, Op.Result r,\n+                  TensorType ptrType, Value ptr,\n+                  TensorType maskType, Value mask) {\n+    broadcastConversionRight(ptrType, maskType, mask);\n+    return block.op(TritonOps.load(\n+            rType,\n+            block.context().getValue(ptr),\n+            block.context().getValue(mask)));\n+}\n+```\n+\n+First, if necessary, we add a Triton operation that broadcasts the mask tensor\n+to a new mask tensor with same shape as the pointer tensor. Then we add the\n+Triton load operation whose result type is `rType`. The operands of the load\n+operation are the output values associated with the input values to the input\n+invocation operation. We must have previously encountered the input operations\n+whose results are those input values, hence there will be a mapping to the\n+output values. If a broadcast of the mask tensor is inserted then we will\n+re-associate input mask value with the new output mask value.\n+\n+Here is the textual form of the Triton code model produced by compiling the\n+`add_kernel2`.\n+\n+```\n+module ()void -> {\n+    tt.func @\"add_kernel2_ptr<float>_ptr<float>_ptr<float>_int_64_void\" (\n+            %0 : ptr<float>, \n+            %1 : ptr<float>, \n+            %2 : ptr<float>, \n+            %3 : int)void \n+            -> {\n+        %4 : int = arith.constant @\"64\";\n+        %5 : int = tt.get_program_id @\"0\";\n+        %6 : int = arith.muli %5 %4;\n+        %7 : tensor<x64, int> = tt.make_range @start=\"0\" @end=\"64\";\n+        %8 : tensor<x64, int> = tt.splat %6;\n+        %9 : tensor<x64, int> = arith.addi %8 %7;\n+        %10 : tensor<x64, int> = tt.splat %3;\n+        %11 : tensor<x64, int> = arith.cmpi %9 %10 @\"slt\";\n+        %12 : tensor<x64, ptr<float>> = tt.splat %0;\n+        %13 : tensor<x64, ptr<float>> = tt.addptr %12 %9;\n+        %14 : tensor<x64, float> = tt.load %13 %11;\n+        %15 : tensor<x64, ptr<float>> = tt.splat %1;\n+        %16 : tensor<x64, ptr<float>> = tt.addptr %15 %9;\n+        %17 : tensor<x64, float> = tt.load %16 %11;\n+        %18 : tensor<x64, float> = arith.addf %14 %17;\n+        %19 : tensor<x64, ptr<float>> = tt.splat %2;\n+        %20 : tensor<x64, ptr<float>> = tt.addptr %19 %9;\n+        tt.store %20 %18 %11;\n+        tt.return;\n+    };\n+    unreachable;\n+};\n+```\n+\n+We can see by design it is remarkably similar to the previously shown MLIR\n+version produced by the actual Triton compiler. In fact, it contains exactly the\n+same number of operations. Notice how the constant value for `BLOCK_SIZE`\n+has been folded into the operations and types.\n+\n+### Further examples\n+\n+The proof of concept Java Triton compiler has additional tests case that\n+implement Triton programs in Java for:\n+\n+1. fused softmax [example][softmax], see [here][softmax-java] for the Java code;\n+   and\n+2. matrix multiply [example][matrix-multiply], see [here][matrix-multiply-java]\n+   for the Java code.\n+\n+[softmax]: https:\/\/triton-lang.org\/main\/getting-started\/tutorials\/02-fused-softmax.html\n+\n+[softmax-java]: https:\/\/github.com\/openjdk\/babylon\/blob\/code-reflection\/cr-examples\/triton\/src\/test\/java\/oracle\/code\/triton\/TestSoftMax.java\n+\n+[matrix-multiply]: https:\/\/triton-lang.org\/main\/getting-started\/tutorials\/03-matrix-multiplication.html\n+\n+[matrix-multiply-java]: https:\/\/github.com\/openjdk\/babylon\/blob\/code-reflection\/cr-examples\/triton\/src\/test\/java\/oracle\/code\/triton\/TestMatrix.java\n+\n+The matrix multiply example is a compelling test case, with 2D tensors, various\n+forms of broadcast, tensor shape expansion, computations using 16-bit floats\n+expanding to 32-bits floats and back, and control flow. In the appendix we\n+describe aspects of this example in more detail.\n+\n+## Appendix: Triton matrix multiply loop\n+\n+Below we show snippets of the matrix multiply accumulating loop in Python and\n+Java.\n+\n+Python:\n+\n+[\/\/]: # (@formatter:off)\n+```python\n+    # -----------------------------------------------------------\n+    # Iterate to compute a block of the C matrix.\n+    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n+    # of fp32 values for higher accuracy.\n+    # `accumulator` will be converted back to fp16 after the loop.\n+    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n+        # Load the next block of A and B, generate a mask by checking the K dimension.\n+        # If it is out of bounds, set it to 0.\n+        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n+        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n+        # We accumulate along the K dimension.\n+        accumulator += tl.dot(a, b)\n+        # Advance the ptrs to the next K block.\n+        a_ptrs += BLOCK_SIZE_K * stride_ak\n+        b_ptrs += BLOCK_SIZE_K * stride_bk\n+    # You can fuse arbitrary activation functions here\n+    # while the accumulator is still in FP32!\n+    if ACTIVATION == \"leaky_relu\":\n+        accumulator = leaky_relu(accumulator)\n+    c = accumulator.to(tl.float16)\n+```\n+[\/\/]: # (@formatter:on)\n+\n+Java:\n+\n+[\/\/]: # (@formatter:off)\n+```java\n+        \/\/ -----------------------------------------------------------\n+        \/\/ Iterate to compute a block of the C matrix.\n+        \/\/ We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n+        \/\/ of fp32 values for higher accuracy.\n+        \/\/ `accumulator` will be converted back to fp16 after the loop.\n+        var accumulator = zeros(float.class, BLOCK_SIZE_M, BLOCK_SIZE_N);\n+        for (int k = 0; k < cdiv(K, BLOCK_SIZE_K); k++) {\n+            \/\/ Load the next block of A and B, generate a mask by checking the K dimension.\n+            \/\/ If it is out of bounds, set it to 0.\n+            var a = load(a_ptrs,\n+                    compare(expand(offs_k, 0), K - k * BLOCK_SIZE_K, LessThan));\n+            var b = load(b_ptrs,\n+                    compare(expand(offs_k, 1), K - k * BLOCK_SIZE_K, LessThan));\n+            \/\/ We accumulate along the K dimension.\n+            accumulator = add(accumulator, dot(a, b));\n+            \/\/ Advance the ptrs to the next K block.\n+            a_ptrs = add(a_ptrs, BLOCK_SIZE_K * stride_ak);\n+            b_ptrs = add(b_ptrs, BLOCK_SIZE_K * stride_bk);\n+        }\n+\n+        \/\/ You can fuse arbitrary activation functions here\n+        \/\/ while the accumulator is still in FP32!\n+\/\/        if (ACTIVATION) {\n+\/\/            \/\/ ...\n+\/\/        }\n+        var c = Triton.conv(Float16.class, accumulator);\n+```\n+[\/\/]: # (@formatter:on)\n+\n+The Java code is more verbose because Java does not support operator\n+overloading, including that for overloading array slicing to expand a tensor's\n+dimensions. (Note the Java code in this example statically imports\n+`Triton`'s static methods.)\n+\n+The matrix multiply cleverly organizes the computation into groups of blocks,\n+ensuring efficient use of memory. The matrix multiplication loops over blocks\n+of `K`, loading tensors from matrices `A(M, K)` and `B(K, N)`, accumulating the\n+multiplication of those tensors, the final result of which is stored to\n+`C(M, N)`. The multiplication of the tensors is performed by the Triton \"dot\"\n+operation. The MLIR Triton compiler can compile this operation to instructions\n+leveraging mixed precision Tensor Cores.\n+\n+The Python Triton compiler transforms the AST of the Python `for` loop to the\n+MLIR `scf.for` [operation][scf.for]. The `for` loop must be a counted loop with\n+a well-defined lower bound, upper bound, and step. Further, the compiler has to\n+identify variables updated within the loop that are declared outside of it.\n+These will become loop-carried variables, the final values of which are returned\n+by the operation when the loop terminates. In this case there are three such\n+variables, `accumulator`, `a_ptrs`, and `b_ptrs`.\n+\n+[scf.for]: https:\/\/mlir.llvm.org\/docs\/Dialects\/SCFDialect\/#scffor-scfforop\n+\n+The Java Triton compiler needs to perform a similar transformation. Here is an\n+abridged snippet of the Java code model showing the modelled `for` loop (the\n+complete snippet is presented below in a subsequent section).\n+\n+```\n+java.for\n+    ()Var<int> -> {\n+        %148 : int = constant @\"0\";\n+        %149 : Var<int> = var %148 @\"k\";\n+        yield %149;\n+    }\n+    (%150 : Var<int>)boolean -> {\n+        %151 : int = var.load %150;\n+        %152 : int = var.load %22;\n+        %153 : java.lang.Integer = invoke %152 @\"java.lang.Integer::valueOf(int)java.lang.Integer\";\n+        %154 : int = var.load %31;\n+        %155 : java.lang.Integer = invoke %154 @\"java.lang.Integer::valueOf(int)java.lang.Integer\";\n+        %156 : int = invoke %153 %155 @\"oracle.code.triton.Triton::cdiv(java.lang.Number, java.lang.Number)int\";\n+        %157 : boolean = lt %151 %156;\n+        yield %157;\n+    }\n+    (%158 : Var<int>)void -> {\n+        %159 : int = var.load %158;\n+        %160 : int = constant @\"1\";\n+        %161 : int = add %159 %160;\n+        var.store %158 %161;\n+        yield;\n+    }\n+    (%162 : Var<int>)void -> {\n+        ...\n+    };\n+```\n+\n+We can clearly see that this operation contain four bodies, each of which\n+corresponds to a nested expression or statement as specified by the Java\n+Language Specification (see [here][jls-for]).\n+\n+[jls-for]: https:\/\/docs.oracle.com\/javase\/specs\/jls\/se21\/html\/jls-14.html#jls-14.14.1\n+\n+The Java Triton compiler checks if the for loop is a counted loop by analyzing\n+the operations in the first three bodies. If so then the compiler extracts and\n+transforms the operations associated with computing the bounds and step, and\n+identifies and transforms loop-carried variables. In the latter case we need to\n+identify all `var.store` operations to values of variables that are declared\n+outside the loop i.e., a `var.store` operation must be a descendant of its\n+associated `var` operation in the code model tree.\n+\n+(Note, since this is a proof of concept the analysis is currently very basic,\n+more extensive capabilities would be useful as functionality in the code\n+reflection analysis package.)\n+\n+The resulting transformed snippet is shown below.\n+\n+```\n+%76 : int = arith.constant @\"0\";\n+%77 : int = tt.call %17 @\"cdiv_int_32_int\";\n+%78 : int = arith.constant @\"1\";\n+%79 : Tuple<tensor<x32, x64, float>, \n+            tensor<x32, x32, ptr<oracle.code.triton.Float16>>, \n+            tensor<x32, x64, ptr<oracle.code.triton.Float16>>> = \n+      scf.for %76 %77 %78 %75 %63 %74 \n+      (%80 : int, \n+       %81 : tensor<x32, x64, float>, \n+       %82 : tensor<x32, x32, ptr<oracle.code.triton.Float16>>, \n+       %83 : tensor<x32, x64, ptr<oracle.code.triton.Float16>>)\n+           Tuple<tensor<x32, x64, float>, \n+                 tensor<x32, x32, ptr<oracle.code.triton.Float16>>, \n+                 tensor<x32, x64, ptr<oracle.code.triton.Float16>>> \n+      -> {\n+    ...\n+    scf.yield %99 %102 %105;\n+};\n+%106 : tensor<x32, x64, float> = tuple.load %79 @\"0\";\n+%107 : tensor<x32, x32, ptr<oracle.code.triton.Float16>> = tuple.load %79 @\"1\";\n+%108 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tuple.load %79 @\"2\";\n+```\n+\n+We can see that the values `%76`, `%77` and `%78` correspond to the bounds and\n+step. They are hoisted out of the counted loop and passed as operands. The\n+values `%75`, `%63`, and `%74`, also passed as operands, correspond to the\n+initial values of the loop-carried variables (the `accumulator`, `a_ptr`\n+and `b_ptr`). Within the body of the loop operation there is a terminal\n+operation, `scf.yield`, that yields the updated values of the loop-carried\n+variables for the next iteration or loop's result.\n+\n+The code model design does not support operations returning multiple results.\n+Instead, we model that capability using the code model `Tuple` type, that\n+declares how many components there are and each component's type. Hence, the\n+loop operation returns a `Tuple` with three component values corresponding to\n+the final values of the loop-carried variables, after which the tuple's\n+component values are unpacked.\n+\n+The Triton code model snippet of Java Triton matrix multiply loop and the Triton\n+MLIR snippet of (Python) Triton matrix multiply loop are presented below in\n+subsequent sections (see the Java [test][matrix-multiply-java] for all details).\n+\n+### Java code model snippet of Java Triton matrix multiply loop\n+\n+```\n+java.for\n+()Var<int> -> {\n+    %148 : int = constant @\"0\";\n+    %149 : Var<int> = var %148 @\"k\";\n+    yield %149;\n+}\n+(%150 : Var<int>)boolean -> {\n+    %151 : int = var.load %150;\n+    %152 : int = var.load %22;\n+    %153 : java.lang.Integer = invoke %152 @\"java.lang.Integer::valueOf(int)java.lang.Integer\";\n+    %154 : int = var.load %31;\n+    %155 : java.lang.Integer = invoke %154 @\"java.lang.Integer::valueOf(int)java.lang.Integer\";\n+    %156 : int = invoke %153 %155 @\"oracle.code.triton.Triton::cdiv(java.lang.Number, java.lang.Number)int\";\n+    %157 : boolean = lt %151 %156;\n+    yield %157;\n+}\n+(%158 : Var<int>)void -> {\n+    %159 : int = var.load %158;\n+    %160 : int = constant @\"1\";\n+    %161 : int = add %159 %160;\n+    var.store %158 %161;\n+    yield;\n+}\n+(%162 : Var<int>)void -> {\n+    %163 : oracle.code.triton.Tensor = var.load %126;\n+    %164 : oracle.code.triton.Tensor = var.load %110;\n+    %165 : int = constant @\"0\";\n+    %166 : oracle.code.triton.Tensor = invoke %164 %165 @\"oracle.code.triton.Triton::expand(oracle.code.triton.Tensor, int)oracle.code.triton.Tensor\";\n+    %167 : int = var.load %22;\n+    %168 : int = var.load %162;\n+    %169 : int = var.load %31;\n+    %170 : int = mul %168 %169;\n+    %171 : int = sub %167 %170;\n+    %172 : java.lang.Integer = invoke %171 @\"java.lang.Integer::valueOf(int)java.lang.Integer\";\n+    %173 : oracle.code.triton.Triton$CompareKind = field.load @\"oracle.code.triton.Triton$CompareKind::LessThan()oracle.code.triton.Triton$CompareKind\";\n+    %174 : oracle.code.triton.Tensor = invoke %166 %172 %173 @\"oracle.code.triton.Triton::compare(java.lang.Number, java.lang.Number, oracle.code.triton.Triton$CompareKind)oracle.code.triton.Tensor\";\n+    %175 : oracle.code.triton.Tensor = invoke %163 %174 @\"oracle.code.triton.Triton::load(oracle.code.triton.Tensor, oracle.code.triton.Tensor)oracle.code.triton.Tensor\";\n+    %176 : Var<oracle.code.triton.Tensor> = var %175 @\"a\";\n+    %177 : oracle.code.triton.Tensor = var.load %142;\n+    %178 : oracle.code.triton.Tensor = var.load %110;\n+    %179 : int = constant @\"1\";\n+    %180 : oracle.code.triton.Tensor = invoke %178 %179 @\"oracle.code.triton.Triton::expand(oracle.code.triton.Tensor, int)oracle.code.triton.Tensor\";\n+    %181 : int = var.load %22;\n+    %182 : int = var.load %162;\n+    %183 : int = var.load %31;\n+    %184 : int = mul %182 %183;\n+    %185 : int = sub %181 %184;\n+    %186 : java.lang.Integer = invoke %185 @\"java.lang.Integer::valueOf(int)java.lang.Integer\";\n+    %187 : oracle.code.triton.Triton$CompareKind = field.load @\"oracle.code.triton.Triton$CompareKind::LessThan()oracle.code.triton.Triton$CompareKind\";\n+    %188 : oracle.code.triton.Tensor = invoke %180 %186 %187 @\"oracle.code.triton.Triton::compare(java.lang.Number, java.lang.Number, oracle.code.triton.Triton$CompareKind)oracle.code.triton.Tensor\";\n+    %189 : oracle.code.triton.Tensor = invoke %177 %188 @\"oracle.code.triton.Triton::load(oracle.code.triton.Tensor, oracle.code.triton.Tensor)oracle.code.triton.Tensor\";\n+    %190 : Var<oracle.code.triton.Tensor> = var %189 @\"b\";\n+    %191 : oracle.code.triton.Tensor = var.load %147;\n+    %192 : oracle.code.triton.Tensor = var.load %176;\n+    %193 : oracle.code.triton.Tensor = var.load %190;\n+    %194 : oracle.code.triton.Tensor = invoke %192 %193 @\"oracle.code.triton.Triton::dot(oracle.code.triton.Tensor, oracle.code.triton.Tensor)oracle.code.triton.Tensor\";\n+    %195 : oracle.code.triton.Tensor = invoke %191 %194 @\"oracle.code.triton.Triton::add(java.lang.Number, java.lang.Number)oracle.code.triton.Tensor\";\n+    var.store %147 %195;\n+    %196 : oracle.code.triton.Tensor = var.load %126;\n+    %197 : int = var.load %31;\n+    %198 : int = var.load %24;\n+    %199 : int = mul %197 %198;\n+    %200 : java.lang.Integer = invoke %199 @\"java.lang.Integer::valueOf(int)java.lang.Integer\";\n+    %201 : oracle.code.triton.Tensor = invoke %196 %200 @\"oracle.code.triton.Triton::add(java.lang.Number, java.lang.Number)oracle.code.triton.Tensor\";\n+    var.store %126 %201;\n+    %202 : oracle.code.triton.Tensor = var.load %142;\n+    %203 : int = var.load %31;\n+    %204 : int = var.load %25;\n+    %205 : int = mul %203 %204;\n+    %206 : java.lang.Integer = invoke %205 @\"java.lang.Integer::valueOf(int)java.lang.Integer\";\n+    %207 : oracle.code.triton.Tensor = invoke %202 %206 @\"oracle.code.triton.Triton::add(java.lang.Number, java.lang.Number)oracle.code.triton.Tensor\";\n+    var.store %142 %207;\n+    java.continue;\n+};\n+```\n+\n+### MLIR snippet of (Python) Triton matrix multiply loop\n+\n+```\n+%47 = tt.call @\"zeros____0cconstexpr_(constexpr_32_, constexpr_64_)__1cconstexpr_fp32_\"() : () -> tensor<32x64xf32>\n+%48 = tt.call @cdiv__i32__1cconstexpr_32_(%arg5) : (i32) -> i32\n+%c0_i32 = arith.constant 0 : i32\n+%c1_i32 = arith.constant 1 : i32\n+%49 = arith.bitcast %c0_i32 : i32 to i32\n+%50 = arith.bitcast %48 : i32 to i32\n+%51 = arith.bitcast %c1_i32 : i32 to i32\n+%52 = llvm.mlir.undef : i32\n+%53:3 = scf.for %arg12 = %49 to %50 step %51 iter_args(%arg13 = %47, %arg14 = %35, %arg15 = %46) -> (tensor<32x64xf32>, tensor<32x32x!tt.ptr<f16, 1>>, tensor<32x64x!tt.ptr<f16, 1>>)  : i32 {\n+  %83 = tt.expand_dims %24 {axis = 0 : i32} : (tensor<32xi32>) -> tensor<1x32xi32>\n+  %c32_i32_3 = arith.constant 32 : i32\n+  %84 = arith.muli %arg12, %c32_i32_3 : i32\n+  %85 = arith.subi %arg5, %84 : i32\n+  %86 = tt.splat %85 : (i32) -> tensor<1x32xi32>\n+  %87 = arith.cmpi slt, %83, %86 : tensor<1x32xi32>\n+  %cst = arith.constant 0.000000e+00 : f32\n+  %88 = tt.broadcast %87 : (tensor<1x32xi1>) -> tensor<32x32xi1>\n+  %cst_4 = arith.constant dense<0.000000e+00> : tensor<32x32xf32>\n+  %89 = arith.truncf %cst_4 : tensor<32x32xf32> to tensor<32x32xf16>\n+  %90 = tt.load %arg14, %88, %89 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf16>\n+  %91 = tt.expand_dims %24 {axis = 1 : i32} : (tensor<32xi32>) -> tensor<32x1xi32>\n+  %c32_i32_5 = arith.constant 32 : i32\n+  %92 = arith.muli %arg12, %c32_i32_5 : i32\n+  %93 = arith.subi %arg5, %92 : i32\n+  %94 = tt.splat %93 : (i32) -> tensor<32x1xi32>\n+  %95 = arith.cmpi slt, %91, %94 : tensor<32x1xi32>\n+  %cst_6 = arith.constant 0.000000e+00 : f32\n+  %96 = tt.broadcast %95 : (tensor<32x1xi1>) -> tensor<32x64xi1>\n+  %cst_7 = arith.constant dense<0.000000e+00> : tensor<32x64xf32>\n+  %97 = arith.truncf %cst_7 : tensor<32x64xf32> to tensor<32x64xf16>\n+  %98 = tt.load %arg15, %96, %97 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x64xf16>\n+  %cst_8 = arith.constant 0.000000e+00 : f32\n+  %cst_9 = arith.constant dense<0.000000e+00> : tensor<32x64xf32>\n+  %99 = tt.dot %90, %98, %cst_9 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<32x32xf16> * tensor<32x64xf16> -> tensor<32x64xf32>\n+  %100 = arith.addf %arg13, %99 : tensor<32x64xf32>\n+  %c32_i32_10 = arith.constant 32 : i32\n+  %101 = arith.muli %arg7, %c32_i32_10 : i32\n+  %102 = tt.splat %101 : (i32) -> tensor<32x32xi32>\n+  %103 = tt.addptr %arg14, %102 : tensor<32x32x!tt.ptr<f16, 1>>, tensor<32x32xi32>\n+  %c32_i32_11 = arith.constant 32 : i32\n+  %104 = arith.muli %arg8, %c32_i32_11 : i32\n+  %105 = tt.splat %104 : (i32) -> tensor<32x64xi32>\n+  %106 = tt.addptr %arg15, %105 : tensor<32x64x!tt.ptr<f16, 1>>, tensor<32x64xi32>\n+  scf.yield %100, %103, %106 : tensor<32x64xf32>, tensor<32x32x!tt.ptr<f16, 1>>, tensor<32x64x!tt.ptr<f16, 1>>\n+}\n+%54 = arith.truncf %53#0 : tensor<32x64xf32> to tensor<32x64xf16>\n+```\n+\n+### Triton code model snippet of Java Triton matrix multiply loop\n+\n+```\n+%76 : int = arith.constant @\"0\";\n+%77 : int = tt.call %17 @\"cdiv_int_32_int\";\n+%78 : int = arith.constant @\"1\";\n+%79 : Tuple<tensor<x32, x64, float>, tensor<x32, x32, ptr<oracle.code.triton.Float16>>, tensor<x32, x64, ptr<oracle.code.triton.Float16>>> = scf.for %76 %77 %78 %75 %63 %74 (%80 : int, %81 : tensor<x32, x64, float>, %82 : tensor<x32, x32, ptr<oracle.code.triton.Float16>>, %83 : tensor<x32, x64, ptr<oracle.code.triton.Float16>>)Tuple<tensor<x32, x64, float>, tensor<x32, x32, ptr<oracle.code.triton.Float16>>, tensor<x32, x64, ptr<oracle.code.triton.Float16>>> -> {\n+    %84 : tensor<x1, x32, int> = tt.expand_dims %52 @\"0\";\n+    %85 : int = arith.muli %80 %26;\n+    %86 : int = arith.subi %17 %85;\n+    %87 : tensor<x1, x32, int> = tt.splat %86;\n+    %88 : tensor<x1, x32, int> = arith.cmpi %84 %87 @\"slt\";\n+    %89 : tensor<x32, x32, int> = tt.broadcast %88;\n+    %90 : tensor<x32, x32, oracle.code.triton.Float16> = tt.load %82 %89;\n+    %91 : tensor<x32, x1, int> = tt.expand_dims %52 @\"1\";\n+    %92 : int = arith.muli %80 %26;\n+    %93 : int = arith.subi %17 %92;\n+    %94 : tensor<x32, x1, int> = tt.splat %93;\n+    %95 : tensor<x32, x1, int> = arith.cmpi %91 %94 @\"slt\";\n+    %96 : tensor<x32, x64, int> = tt.broadcast %95;\n+    %97 : tensor<x32, x64, oracle.code.triton.Float16> = tt.load %83 %96;\n+    %98 : tensor<x32, x64, float> = tt.dot %90 %97;\n+    %99 : tensor<x32, x64, float> = arith.addf %81 %98;\n+    %100 : int = arith.muli %26 %19;\n+    %101 : tensor<x32, x32, int> = tt.splat %100;\n+    %102 : tensor<x32, x32, ptr<oracle.code.triton.Float16>> = tt.addptr %82 %101;\n+    %103 : int = arith.muli %26 %20;\n+    %104 : tensor<x32, x64, int> = tt.splat %103;\n+    %105 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tt.addptr %83 %104;\n+    scf.yield %99 %102 %105;\n+};\n+%106 : tensor<x32, x64, float> = tuple.load %79 @\"0\";\n+%107 : tensor<x32, x32, ptr<oracle.code.triton.Float16>> = tuple.load %79 @\"1\";\n+%108 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tuple.load %79 @\"2\";\n+%109 : tensor<x32, x64, oracle.code.triton.Float16> = arith.truncf %106;\n+```\n+\n+\n","filename":"site\/articles\/triton.md","additions":1200,"deletions":0,"binary":false,"changes":1200,"status":"added"}]}