{"files":[{"patch":"@@ -7,0 +7,7 @@\n+## Key Takeaways \n+\n+- [The Heterogeneous Accelerator Toolkit (HAT)](https:\/\/github.com\/openjdk\/babylon\/tree\/code-reflection\/hat) is a parallel programming framework that allows Java developers to offload Java code and dispatch the generated code on modern hardware accelerators, such as Graphics Processing Units (GPUs).\n+- HAT can be used to speed up massive parallel workloads such as Deep Learning, AI, big data analytics and physic simulations, just to name a few, by automatically offloading and running these workloads on specialized hardware.\n+- HAT provides programming abstractions to facilitate GPU programming from Java: some key abstractions are an ND-Range API, a compute-layer and a kernel layer which allow Java developers to write explicit parallel code that can be offloaded to GPUs. Besides HAT provides memory abstractions that facilitate the usage of custom data structures and efficiently map them to the different memory regions of the GPUs.\n+- This article provides an overview of the HAT programming model: using the matrix-multiplication as an example, we demonstrate how Java developers can tune GPU workloads from the Java side to achieve performance close to native cuBLAS, scaling from 7 GFLOP\/s on CPUs to 14 TFLOP\/s on an NVIDIA A10 GPU. \n+\n@@ -8,1 +15,0 @@\n-- [Key Takeaways](#key-takeaways)\n@@ -24,7 +30,0 @@\n-## Key Takeaways \n-\n-- [The Heterogeneous Accelerator Toolkit (HAT)](https:\/\/github.com\/openjdk\/babylon\/tree\/code-reflection\/hat) is a parallel programming framework that allows Java developers to offload Java code and dispatch the generated code on modern hardware accelerators, such as Graphics Processing Units (GPUs).\n-- HAT can be used to speed up massive parallel workloads such as Deep Learning, AI, big data analytics and physic simulations, just to name a few, by automatically offloading and running these workloads on specialized hardware.\n-- HAT provides programming abstractions to facilitate GPU programming from Java: some key abstractions are an ND-Range API, a compute-layer and a kernel layer which allow Java developers to write explicit parallel code that can be offloaded to GPUs. Besides HAT provides memory abstractions that facilitate the usage of custom data structures and efficiently map them to the different memory regions of the GPUs.\n-- This article provides an overview of the HAT programming model: using the matrix-multiplication as an example, we demonstrate how Java developers can tune GPU workloads from the Java side to achieve performance close to native cuBLAS, scaling from 7 GFLOP\/s on CPUs to 14 TFLOP\/s on an NVIDIA A10 GPU. \n-\n@@ -99,1 +98,4 @@\n-public static void vectorMultiplicationHAT(@RO KernelContext kc, @RO F32Array arrayA, @RO F32Array arrayB, @RW F32Array arrayC) {\n+public static void vectorMultiplicationHAT(@RO KernelContext kc, \n+                                           @RO F32Array arrayA, \n+                                           @RO F32Array arrayB, \n+                                           @RW F32Array arrayC) {\n@@ -140,1 +142,4 @@\n-public static void vectorMultiplicationHAT(@RO KernelContext kc, @RO F32Array arrayA, @RO F32Array arrayB, @RW F32Array arrayC) {\n+public static void vectorMultiplicationHAT(@RO KernelContext kc, \n+                                           @RO F32Array arrayA, \n+                                           @RO F32Array arrayB, \n+                                           @RW F32Array arrayC) {\n@@ -184,1 +189,4 @@\n-public static void computeContext(@RO ComputeContext cc, @RO F32Array arrayA, @RO F32Array arrayB, @RW F32Array arrayC) {\n+public static void computeContext(@RO ComputeContext cc, \n+                                  @RO F32Array arrayA, \n+                                  @RO F32Array arrayB, \n+                                  @RW F32Array arrayC) {\n@@ -422,3 +430,9 @@\n-public static void matrixMultiplyKernel2D(@RO KernelContext kc, @RO F32Array matrixA, @RO F32Array matrixB, @RW F32Array matrixC, int size) {\n-    if (kc.gix < kc.gsx) {       \/\/ control for 1D range  ( thread id 1D -> kc.gix )\n-        if (kc.giy < kc.gsy) {   \/\/ conntrol for 2D range ( thread id 2D -> kc.giy )\n+public static void matrixMultiplyKernel2D(@RO KernelContext kc, \n+                                          @RO F32Array matrixA, \n+                                          @RO F32Array matrixB, \n+                                          @RW F32Array matrixC, \n+                                          int size) {\n+    \/\/ control for 1D range  ( thread id 1D -> kc.gix )\n+    if (kc.gix < kc.gsx) {\n+        \/\/ conntrol for 2D range ( thread id 2D -> kc.giy )\n+        if (kc.giy < kc.gsy) {   \n@@ -427,1 +441,2 @@\n-                acc += (matrixA.array(kc.gix * size + k) * matrixB.array(k * size + kc.giy));\n+                acc += (matrixA.array(kc.gix * size + k) \n+                        * matrixB.array(k * size + kc.giy));\n@@ -564,1 +579,5 @@\n-public static void matrixMultiplyKernel2DLI(@RO KernelContext kc, @RO F32Array matrixA, @RO F32Array matrixB, @RW F32Array matrixC, int size) {\n+public static void matrixMultiplyKernel2DLI(@RO KernelContext kc, \n+                                            @RO F32Array matrixA, \n+                                            @RO F32Array matrixB, \n+                                            @RW F32Array matrixC, \n+                                            int size) {\n@@ -716,2 +735,5 @@\n-public static void matrixMultiplyKernel2DTiling(@RO KernelContext kc, @RO F32Array matrixA, @RO F32Array matrixB, @RW F32Array matrixC, int size) {\n-\n+public static void matrixMultiplyKernel2DTiling(@RO KernelContext kc, \n+                                                @RO F32Array matrixA, \n+                                                @RO F32Array matrixB, \n+                                                @RW F32Array matrixC, \n+                                                int size) {\n","filename":"site\/articles\/hat-matmul.md","additions":40,"deletions":18,"binary":false,"changes":58,"status":"modified"}]}