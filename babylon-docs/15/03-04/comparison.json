{"files":[{"patch":"@@ -1175,0 +1175,9 @@\n+### Is it always possible performance close to native?\n+\n+Achieving peak performance from high-level programming languages remains a formidable challenge, as it needs the precise orchestration of multiple architectural factors, including thread-block selection, block tiling strategies, and the deployment of hardware-specific intrinsics, just to name a few.\n+Even with different GPU micro-archictecture generations and different vendors, all previous parameters can be different.\n+\n+In this article, we have focused on a single data point (data size) on a specific GPU to illustrate the mechanisms of code-reflection and HAT. \n+These results demonstrate that with the appropriate language constructs, compiler support, and runtime systems, it is possible to bridge the performance gap between high-level abstractions and native parallel code.\n+While performance portability and automated tuning remain open research questions, one of the most common techniques to achieve this is via auto-tuning, which could be included in future extensions of HAT.\n+\n","filename":"site\/articles\/hat-matmul.md","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"}]}