{"files":[{"patch":"@@ -11,1 +11,1 @@\n-- [How Code-Reflection is used in HAT?](#how-code-reflection-is-used-in-hat)\n+- [How Code Reflection is used in HAT?](#how-code-reflection-is-used-in-hat)\n@@ -33,3 +33,2 @@\n-The project [Babylon](https:\/\/github.com\/openjdk\/babylon) is a new OpenJDK project with the goal of enhancing code reflection APIs for the Java platform, allowing to reflect code from Java methods and Java lambdas, and being able\n-to query their intermediate representation, called code-models. \n-These code models can be used at runtime to modify the code, perform optimizations, and\/or perform code transformations to other programming models. Furthermore, code-reflection allows Java developers to interact with foreign programming models and foreign programming languages without using any 3rd party libraries. \n+The project [Babylon](https:\/\/github.com\/openjdk\/babylon) is a new OpenJDK project with the goal of enhancing Java reflection, allowing to reflect code from Java methods and Java lambdas, and being able to query their symbolic representation, called code models. \n+These code models can be used at runtime to modify the code, perform optimizations, and\/or perform code transformations to other programming models. Furthermore, code reflection allows Java developers to interact with foreign programming models and foreign programming languages without using any 3rd party libraries. \n@@ -45,1 +44,1 @@\n-The second question goes a step further and rethink about how Java programmers could approach native performance on hardware accelerators while still maintaining reasonable high-level constructs. This is a very thin line between what to expose from low-level APIs and from what level of the native software stack. The HAT project tackles GPU programming from a perspective of a performance engineer wanting to efficiently interconnect the Java software stack with foreign GPU programming models. As we will see in this article, this allows programmers to perform fine-tuned optimizations for GPUs. We will look at the details, but in a nutshell, this is being possible because of recent innovations within the JDK development, such as Project Panama and its foreign function API, Project Babylon with its enhanced code-reflection APIs and HAT.\n+The second question goes a step further and rethink about how Java programmers could approach native performance on hardware accelerators while still maintaining reasonable high-level constructs. This is a very thin line between what to expose from low-level APIs and from what level of the native software stack. The HAT project tackles GPU programming from a perspective of a performance engineer wanting to efficiently interconnect the Java software stack with foreign GPU programming models. As we will see in this article, this allows programmers to perform fine-tuned optimizations for GPUs. We will look at the details, but in a nutshell, this is being possible because of recent innovations within the JDK development, such as Project Panama and its foreign function API, Project Babylon with its enhanced code reflection APIs and HAT.\n@@ -56,1 +55,1 @@\n-We specifically target Java developers interested in how the new code-reflection enables GPU acceleration directly from the Java ecosystem, and how some Java applications can be accelerated on modern hardware. Furthermore, while the primary focus of HAT is GPUs, the optimization techniques discussed in this article can be applicable to other hardware accelerators and foreign language interfaces.\n+We specifically target Java developers interested in how the new code reflection enables GPU acceleration directly from the Java ecosystem, and how some Java applications can be accelerated on modern hardware. Furthermore, while the primary focus of HAT is GPUs, the optimization techniques discussed in this article can be applicable to other hardware accelerators and foreign language interfaces.\n@@ -107,1 +106,1 @@\n-First, we annotate the code with the `@Reflect` annotation from the code-reflection API. \n+First, we annotate the code with the `@Reflect` annotation from the code reflection API. \n@@ -110,1 +109,1 @@\n-From the HAT kernel code above, we also see that there are different argument types that are passed as parameters:\n+From the HAT kernel code above, we also see that there are different method parameter types:\n@@ -127,1 +126,1 @@\n-This indicates the HAT runtime how to move data from the CPU to the GPU and vice-versa, since memory between discrete GPUs and the CPU is not shared. \n+This indicates the HAT runtime how to move data from the CPU to the GPU and vice versa, since memory between discrete GPUs and the CPU is not shared. \n@@ -208,1 +207,1 @@\n-Next, let's take a look at how HAT compiles code and how it interacts with code-reflection. \n+Next, let's take a look at how HAT compiles code and how it interacts with code reflection. \n@@ -210,1 +209,1 @@\n-## How Code-Reflection is used in HAT? \n+## How Code Reflection is used in HAT? \n@@ -212,1 +211,1 @@\n-The following diagram shows an abstract representation of all components of the HAT software stack and how they interact with code-reflection. \n+The following diagram shows an abstract representation of all components of the HAT software stack and how they interact with code reflection. \n@@ -223,1 +222,1 @@\n-This is the time in which HAT retrieves all reachable code-models from the code-reflection API that are annotated with `@Reflect`, builds its code-model, and passes a set of transformation to translate the original code-model into a GPU-friendly code-model. \n+This is the time in which HAT retrieves all reachable code models from the code reflection API that are annotated with `@Reflect`, builds its code model, and passes a set of transformation to translate the original code model into a GPU-friendly code model. \n@@ -373,1 +372,1 @@\n-To profiler the generated GPU kernel by HAT, we use the NVIDIA Nsight profiler analysis.\n+To profile the generated GPU kernel by HAT, we use the NVIDIA Nsight profiler analysis.\n@@ -410,1 +409,1 @@\n-But this strengths one of the key messages of this article: **code-reflection, and Babylon, allows developers to enable specific optimizations when targeting foreign programming languages**, such as CUDA. \n+But this strengths one of the key messages of this article: **code reflection, and Babylon, allows developers to enable specific optimizations when targeting foreign programming languages**, such as CUDA. \n@@ -425,1 +424,1 @@\n-        if (kc.giy < kc.gsy) {.  \/\/ conntrol for 2D range ( thread id 2D -> kc.giy )\n+        if (kc.giy < kc.gsy) {   \/\/ conntrol for 2D range ( thread id 2D -> kc.giy )\n@@ -540,1 +539,1 @@\n-But, what does this means in practice? For instance, if we have a $4x4$ thread block, the way consecutive threads are organized are as follows:\n+But, what does this mean in practice? For instance, if we have a $4x4$ thread block, the way consecutive threads are organized are as follows:\n@@ -558,1 +557,4 @@\n-To achieve this we can swap the indexes between `kc.gix` and `kc.gsy`. The following code snippet shows the transformation in HAT:\n+To achieve this, we can swap the indexes between `kc.gix` and `kc.giy`. \n+Thus, instead of `matrixA.array(kc.gix * size + k)`, we change the indexing to `matrixA.array(kc.giy * size + k)`.\n+We also do the same for the second matrix (`matrixB`).\n+The following code snippet shows the new version in HAT:\n@@ -563,2 +565,2 @@\n-    if (kc.gix < kc.gsx) {\n-        if (kc.giy < kc.gsy) {\n+    if (kc.giy < kc.gsy) {            \/\/ swapped gix -> giy\n+        if (kc.gix < kc.gsx) {        \/\/ swapped giy -> gix\n@@ -624,1 +626,1 @@\n-In HAT, we use the global memory when passing parameters to the kernel (e.g, the `F32Array` that is pass as a parameter to the kernel function).\n+In HAT, we use the global memory when passing arguments to the kernel (e.g, the `F32Array` that is pass as a parameter to the kernel function).\n@@ -683,1 +685,1 @@\n-These two methods can return `null` from the Java implementation because they are never meant to be executed on the host side (the CPU from Java, at least for now), only from on the device (the GPU). Thus they are only used as markers for the HAT code generator. \n+These two methods can return `null` from the Java implementation because they are never meant to be executed on the host side (the CPU from Java, at least for now), only from on the device (the GPU). Thus, they are only used as markers for the HAT code generator. \n@@ -780,1 +782,1 @@\n-By analyzing the source profiling (the source section of the `ncu` tool), we see that the 33% of the time is spent in data copies from global memory to local memory and vice-versa. Besides, a high number of warp (set of 32 consecutive threads on an NVIDIA GPU) stalls are spent in the actual reduction (computation). We can alleviate this problem with our next optimization, register tiling, and increase the work to be done per thread. \n+By analyzing the source profiling (the source section of the `ncu` tool), we see that the 33% of the time is spent in data copies from global memory to local memory and vice versa. Besides, a high number of warp (set of 32 consecutive threads on an NVIDIA GPU) stalls are spent in the actual reduction (computation). We can alleviate this problem with our next optimization, register tiling, and increase the work to be done per thread. \n@@ -1154,1 +1156,1 @@\n-In this article we have explored how the project Babylon with its enhanced code-reflection APIs along with HAT, a parallel programming framework for offloading Java programs onto modern such as GPUs can unlock GPU acceleration and achieve performance close to native CUDA implementations. \n+In this article we have explored how the project Babylon with its enhanced code reflection APIs along with HAT, a parallel programming framework for offloading Java programs onto modern such as GPUs can unlock GPU acceleration and achieve performance close to native CUDA implementations. \n","filename":"site\/articles\/hat-matmul.md","additions":26,"deletions":24,"binary":false,"changes":50,"status":"modified"}]}