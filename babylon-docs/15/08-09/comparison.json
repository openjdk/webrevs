{"files":[{"patch":"@@ -9,4 +9,17 @@\n-- [The Heterogeneous Accelerator Toolkit (HAT)](https:\/\/github.com\/openjdk\/babylon\/tree\/code-reflection\/hat) is a parallel programming framework that allows Java developers to offload Java code and dispatch the generated code on modern hardware accelerators, such as Graphics Processing Units (GPUs).\n-- HAT can be used to speed up massive parallel workloads such as Deep Learning, AI, big data analytics and physic simulations, just to name a few, by automatically offloading and running these workloads on specialized hardware.\n-- HAT provides programming abstractions to facilitate GPU programming from Java: some key abstractions are an ND-Range API, a compute-layer and a kernel layer which allow Java developers to write explicit parallel code that can be offloaded to GPUs. Besides HAT provides memory abstractions that facilitate the usage of custom data structures and efficiently map them to the different memory regions of the GPUs.\n-- This article provides an overview of the HAT programming model: using the matrix-multiplication as an example, we demonstrate how Java developers can tune GPU workloads from the Java side to achieve performance close to native cuBLAS, scaling from 7 GFLOP\/s on CPUs to 14 TFLOP\/s on an NVIDIA A10 GPU. \n+- [The Heterogeneous Accelerator Toolkit (HAT)](https:\/\/github.com\/openjdk\/babylon\/tree\/code-reflection\/hat)\n+  is a parallel programming framework that allows Java developers to offload\n+  Java code and dispatch the generated code on modern hardware accelerators,\n+  such as Graphics Processing Units (GPUs).\n+- HAT can be used to speed up massive parallel workloads such as Deep Learning,\n+  AI, big data analytics and physic simulations, just to name a few, by\n+  automatically offloading and running these workloads on specialized hardware.\n+- HAT provides programming abstractions to facilitate GPU programming from Java:\n+  some key abstractions are an ND-Range API, a compute-layer and a kernel layer\n+  which allow Java developers to write explicit parallel code that can be\n+  offloaded to GPUs. Besides HAT provides memory abstractions that facilitate\n+  the usage of custom data structures and efficiently map them to the different\n+  memory regions of the GPUs.\n+- This article provides an overview of the HAT programming model: using the\n+  matrix-multiplication as an example, we demonstrate how Java developers can\n+  tune GPU workloads from the Java side to achieve performance close to native\n+  cuBLAS, scaling from 7 GFLOP\/s on CPUs to 14 TFLOP\/s on an NVIDIA A10 GPU.\n@@ -32,4 +45,15 @@\n-The project [Babylon](https:\/\/github.com\/openjdk\/babylon) is a new OpenJDK project with the goal of enhancing Java reflection, allowing to reflect code from Java methods and Java lambdas, and being able to query their symbolic representation, called code models. \n-These code models can be used at runtime to modify the code, perform optimizations, and\/or perform code transformations to other programming models. Furthermore, code reflection allows Java developers to interact with foreign programming models and foreign programming languages without using any 3rd party libraries. \n-\n-One of the foreign programming environments we are exploring in the project Babylon is the GPU environment through the CUDA and OpenCL programming models, called [HAT](https:\/\/github.com\/openjdk\/babylon\/tree\/code-reflection\/hat) (Heterogeneous Accelerator Toolkit). The goal for HAT is to be able to offload and run efficient parallel workloads on hardware accelerators.\n+The project [Babylon](https:\/\/github.com\/openjdk\/babylon) is a new OpenJDK\n+project with the goal of enhancing Java reflection, allowing to reflect code\n+from Java methods and Java lambdas, and being able to query their symbolic\n+representation, called code models.\n+These code models can be used at runtime to modify the code, perform\n+optimizations, and\/or perform code transformations to other programming models.\n+Furthermore, code reflection allows Java developers to interact with foreign\n+programming models and foreign programming languages without using any 3rd party\n+libraries.\n+\n+One of the foreign programming environments we are exploring in the project\n+Babylon is the GPU environment through the CUDA and OpenCL programming models,\n+called [HAT](https:\/\/github.com\/openjdk\/babylon\/tree\/code-reflection\/hat) (\n+Heterogeneous Accelerator Toolkit). The goal for HAT is to be able to offload\n+and run efficient parallel workloads on hardware accelerators.\n@@ -38,0 +62,1 @@\n+\n@@ -41,9 +66,36 @@\n-\n-Each of these questions presents its own set of challenges. The majority of the projects focus on the first challenge with projects such as [Sumatra](https:\/\/openjdk.org\/projects\/sumatra\/), [Aparapi](https:\/\/github.com\/Syncleus\/aparapi), [Marawacc](https:\/\/github.com\/jjfumero\/marawacc), [RootBeer](https:\/\/github.com\/bsletten\/rootbeer1), [JaBEE](https:\/\/dl.acm.org\/doi\/10.1145\/2159430.2159439), [IBM J9](https:\/\/github.com\/eclipse-openj9\/openj9\/blob\/master\/jcl\/src\/openj9.cuda\/share\/classes\/com\/ibm\/cuda\/Cuda.java), and more recently [TornadoVM](https:\/\/github.com\/beehive-lab\/TornadoVM). These projects have focused on abstracting GPU programmability and make it easier for Java developers. While they achieve reasonable high performance (e.g., [TornadoVM's study](https:\/\/dl.acm.org\/doi\/epdf\/10.1145\/3313808.3313819)) by leveraging specialized accelerators, they often do so at the cost of hindering access to advanced GPU optimizations. However, in the era of AI and high-demand computing, simply being faster than Java on CPUs might not be enough.\n-\n-The second question goes a step further and rethink about how Java programmers could approach native performance on hardware accelerators while still maintaining reasonable high-level constructs. This is a very thin line between what to expose from low-level APIs and from what level of the native software stack. The HAT project tackles GPU programming from a perspective of a performance engineer wanting to efficiently interconnect the Java software stack with foreign GPU programming models. As we will see in this article, this allows programmers to perform fine-tuned optimizations for GPUs. We will look at the details, but in a nutshell, this is being possible because of recent innovations within the JDK development, such as Project Panama and its foreign function API, Project Babylon with its enhanced code reflection APIs and HAT.\n-\n-In this article, we’ll take a hands-on approach to GPU programming from Java, using Babylon and HAT to implement and optimize the matrix multiplication application, one of the key algorithms widely used in AI and Large Language Models (LLMs) these days.\n-Some implementations are inspired on the excellent blog from [Simon Boehm](https:\/\/siboehm.com\/articles\/22\/CUDA-MMM), who applied a set of CUDA optimizations for the SGEMM algorithm to be able to run as close as cuBLAS (native GPU library for linear algebra) as possible. \n-\n-We’ll step through each layer of optimization expressed in Java and HAT, analyze their performance using real-world profiling data, and compare our results to best-in-class libraries like NVIDIA’s cuBLAS.\n+Each of these questions presents its own set of challenges. The majority of the\n+projects focus on the first challenge with projects such\n+as [Sumatra](https:\/\/openjdk.org\/projects\/sumatra\/), [Aparapi](https:\/\/github.com\/Syncleus\/aparapi), [Marawacc](https:\/\/github.com\/jjfumero\/marawacc), [RootBeer](https:\/\/github.com\/bsletten\/rootbeer1), [JaBEE](https:\/\/dl.acm.org\/doi\/10.1145\/2159430.2159439)\n+, [IBM J9](https:\/\/github.com\/eclipse-openj9\/openj9\/blob\/master\/jcl\/src\/openj9.cuda\/share\/classes\/com\/ibm\/cuda\/Cuda.java), and more recently [TornadoVM](https:\/\/github.com\/beehive-lab\/TornadoVM). These\n+projects have focused on abstracting GPU programmability and make it easier for\n+Java developers. While they achieve reasonable high performance (\n+e.g., [TornadoVM's study](https:\/\/dl.acm.org\/doi\/epdf\/10.1145\/3313808.3313819))\n+by leveraging specialized accelerators, they often do so at the cost of\n+hindering access to advanced GPU optimizations. However, in the era of AI and\n+high-demand computing, simply being faster than Java on CPUs might not be\n+enough.\n+\n+The second question goes a step further and rethink about how Java programmers\n+could approach native performance on hardware accelerators while still\n+maintaining reasonable high-level constructs. This is a very thin line between\n+what to expose from low-level APIs and from what level of the native software\n+stack. The HAT project tackles GPU programming from a perspective of a\n+performance engineer wanting to efficiently interconnect the Java software stack\n+with foreign GPU programming models. As we will see in this article, this allows\n+programmers to perform fine-tuned optimizations for GPUs. We will look at the\n+details, but in a nutshell, this is being possible because of recent innovations\n+within the JDK development, such as Project Panama and its foreign function API,\n+Project Babylon with its enhanced code reflection APIs and HAT.\n+\n+In this article, we’ll take a hands-on approach to GPU programming from Java,\n+using Babylon and HAT to implement and optimize the matrix multiplication\n+application, one of the key algorithms widely used in AI and Large Language\n+Models (LLMs) these days.\n+Some implementations are inspired on the excellent blog\n+from [Simon Boehm](https:\/\/siboehm.com\/articles\/22\/CUDA-MMM), who applied a set\n+of CUDA optimizations for the SGEMM algorithm to be able to run as close as\n+cuBLAS (native GPU library for linear algebra) as possible.\n+\n+We’ll step through each layer of optimization expressed in Java and HAT, analyze\n+their performance using real-world profiling data, and compare our results to\n+best-in-class libraries like NVIDIA’s cuBLAS.\n@@ -53,1 +105,4 @@\n-This article explores how Project Babylon and HAT can be leveraged to optimize code for GPU architectures. While we cover the fundamental GPU programming and execution models necessary for the HAT development, readers with prior experience in GPU programming will find these concepts familiar.\n+This article explores how Project Babylon and HAT can be leveraged to optimize\n+code for GPU architectures. While we cover the fundamental GPU programming and\n+execution models necessary for the HAT development, readers with prior\n+experience in GPU programming will find these concepts familiar.\n@@ -55,1 +110,6 @@\n-We specifically target Java developers interested in how the new code reflection enables GPU acceleration directly from the Java ecosystem, and how some Java applications can be accelerated on modern hardware. Furthermore, while the primary focus of HAT is GPUs, the optimization techniques discussed in this article can be applicable to other hardware accelerators and foreign language interfaces.\n+We specifically target Java developers interested in how the new code reflection\n+enables GPU acceleration directly from the Java ecosystem, and how some Java\n+applications can be accelerated on modern hardware. Furthermore, while the\n+primary focus of HAT is GPUs, the optimization techniques discussed in this\n+article can be applicable to other hardware accelerators and foreign language\n+interfaces.\n@@ -59,11 +119,36 @@\n-As we have mentioned, HAT is a programming toolkit that enables Java developers to target heterogeneous computing systems from Java. To do so, HAT offers a set of programming abstractions on top of the lower-level foreign programming models for GPUs:\n-\n-- [ND-Range API](https:\/\/github.com\/openjdk\/babylon\/blob\/6be4588edb846b777a9dff41fccfc243f0d00a03\/hat\/core\/src\/main\/java\/hat\/NDRange.java#L38-L156) that defines GPU's global thread configuration and block thread configurations. This is a similar concept to OpenCL, CUDA and SYCL in which programmers can define how thread blocks are structured. This makes HAT programs scalable and flexible when running on multiple hardware accelerators from different architectures and vendors.\n-\n-- A Kernel-Context Layer: this abstraction represents the code to be offloaded to the GPUs. These are Java methods that express the work to be done per thread (as in a GPU thread), similar to CUDA and OpenCL. To do so, HAT also exposes a [KernelContext](https:\/\/github.com\/openjdk\/babylon\/blob\/6be4588edb846b777a9dff41fccfc243f0d00a03\/hat\/core\/src\/main\/java\/hat\/KernelContext.java#L41) object, which gives access to Java developers to GPU's global and local thread-ids, block partitions, and barriers. \n-\n-- Compute-Context Layer: this is a higher-level programming abstraction on top of the kernel context API to allow developers to compose a graph of compute kernels and their data dependencies. For example, if we want to launch multiple programs on a GPU, we can group all invocation under the same compute-context layer.  \n-\n-- Interface Mapper for global and device memory: good memory handling is as important as good compute handling when it comes to achieving performance on GPUs from managed runtime systems. HAT exposes an API on top of the Panama FFM Memory Segments and an API for programming different device types from Java. With these APIs, developers can express their own data types and map them efficiently to different memory regions of the GPUs. \n-\n-Let's take a simple example to see all these concepts in practice. We are going to express vector multiplication for `float` arrays. Let's start with the Java code:\n+As we have mentioned, HAT is a programming toolkit that enables Java developers\n+to target heterogeneous computing systems from Java. To do so, HAT offers a set\n+of programming abstractions on top of the lower-level foreign programming models\n+for GPUs:\n+\n+- [ND-Range API](https:\/\/github.com\/openjdk\/babylon\/blob\/6be4588edb846b777a9dff41fccfc243f0d00a03\/hat\/core\/src\/main\/java\/hat\/NDRange.java#L38-L156)\n+  that defines GPU's global thread configuration and block thread\n+  configurations. This is a similar concept to OpenCL, CUDA and SYCL in which\n+  programmers can define how thread blocks are structured. This makes HAT\n+  programs scalable and flexible when running on multiple hardware accelerators\n+  from different architectures and vendors.\n+\n+- A Kernel-Context Layer: this abstraction represents the code to be offloaded\n+  to the GPUs. These are Java methods that express the work to be done per\n+  thread (as in a GPU thread), similar to CUDA and OpenCL. To do so, HAT also\n+  exposes\n+  a [KernelContext](https:\/\/github.com\/openjdk\/babylon\/blob\/6be4588edb846b777a9dff41fccfc243f0d00a03\/hat\/core\/src\/main\/java\/hat\/KernelContext.java#L41)\n+  object, which gives access to Java developers to GPU's global and local\n+  thread-ids, block partitions, and barriers.\n+\n+- Compute-Context Layer: this is a higher-level programming abstraction on top\n+  of the kernel context API to allow developers to compose a graph of compute\n+  kernels and their data dependencies. For example, if we want to launch\n+  multiple programs on a GPU, we can group all invocation under the same\n+  compute-context layer.\n+\n+- Interface Mapper for global and device memory: good memory handling is as\n+  important as good compute handling when it comes to achieving performance on\n+  GPUs from managed runtime systems. HAT exposes an API on top of the Panama FFM\n+  Memory Segments and an API for programming different device types from Java.\n+  With these APIs, developers can express their own data types and map them\n+  efficiently to different memory regions of the GPUs.\n+\n+Let's take a simple example to see all these concepts in practice. We are going\n+to express vector multiplication for `float` arrays. Let's start with the Java\n+code:\n@@ -79,3 +164,6 @@\n-This is a straightforward code that iterates over the input arrays to perform the computation.\n-Now, let's do a minor modification, that hopefully, facilitates understanding when we will see the equivalent code in HAT. \n-We could express the same vector computation by splitting into two methods between the loop structure and the loop body as follows:\n+This is a straightforward code that iterates over the input arrays to perform\n+the computation.\n+Now, let's do a minor modification, that hopefully, facilitates understanding\n+when we will see the equivalent code in HAT.\n+We could express the same vector computation by splitting into two methods\n+between the loop structure and the loop body as follows:\n@@ -95,1 +183,4 @@\n-What we have done is to define a method that expresses the work to be done per iteration. And this is the idea behind expressing kernels in HAT. The following code snippet shows the kernel function (Java method that will be offloaded and accelerated on a GPU) expressed in HAT:\n+What we have done is to define a method that expresses the work to be done per\n+iteration. And this is the idea behind expressing kernels in HAT. The following\n+code snippet shows the kernel function (Java method that will be offloaded and\n+accelerated on a GPU) expressed in HAT:\n@@ -109,6 +200,19 @@\n-First, we annotate the code with the `@Reflect` annotation from the code reflection API. \n-This indicates the HAT compiler to offload this entire method to the GPU, from which it can obtain the code-model, perform a set of transformations (for example represent parallel constructs directly in the code-model), and generate the GPU code. \n-\n-From the HAT kernel code above, we also see that there are different method parameter types:\n-- `KernelContext`: each kernel method receives a kernel context object. This is a special object in HAT, as we introduced earlier, that provides the built-ins functions to access parallel GPU constructs (e.g., the global thread-id - `kc.gix`).\n-- `F32Array`: These objects correspond to HAT data types to represent `float[]` in Java. The difference is that they are implemented on top of the Panama FFM API. Programmers can also provide their own data types by extending the HAT `Buffer` interface. For example, the following code snippet defines an array of `short` values:\n+First, we annotate the code with the `@Reflect` annotation from the code\n+reflection API.\n+This indicates the HAT compiler to offload this entire method to the GPU, from\n+which it can obtain the code-model, perform a set of transformations (for\n+example represent parallel constructs directly in the code-model), and generate\n+the GPU code.\n+\n+From the HAT kernel code above, we also see that there are different method\n+parameter types:\n+\n+- `KernelContext`: each kernel method receives a kernel context object. This is\n+  a special object in HAT, as we introduced earlier, that provides the built-ins\n+  functions to access parallel GPU constructs (e.g., the global thread-id -\n+  `kc.gix`).\n+- `F32Array`: These objects correspond to HAT data types to represent `float[]`\n+  in Java. The difference is that they are implemented on top of the Panama FFM\n+  API. Programmers can also provide their own data types by extending the HAT\n+  `Buffer` interface. For example, the following code snippet defines an array\n+  of `short` values:\n@@ -128,11 +232,21 @@\n-Note also that parameters are tagged with the `@RO` (Read-Only), and `@RW` (Read-Write) Java annotations. \n-This indicates the HAT runtime how to move data from the CPU to the GPU and vice versa, since memory between discrete GPUs and the CPU is not shared. \n-However, this is quite likely going to change in future versions of HAT, becoming an optional tag.\n-\n-From the kernel-context object, we can obtain the thread-id, and that's the equivalent to our loop-index from the Java version.\n-We access the different elements of the arrays by using the `kc.gix` (GPU's global thread index), and store the result of the multiplication in the resulting array (array C).\n-\n-But, how do we know how many iterations (threads) to run? Here we map the iteration with the thread-index. \n-In GPU programming, it is a common practice to map the thread-id with the data-item to compute. \n-Thus, ideally, we will launch as many threads as elements from the input arrays (e.g., `a.length`). \n-Since we specify the threads to run at runtime, we need a way to protect us from getting a buffer overflow.\n+Note also that parameters are tagged with the `@RO` (Read-Only), and `@RW` \n+(Read-Write) Java annotations.\n+This indicates the HAT runtime how to move data from the CPU to the GPU and vice\n+versa, since memory between discrete GPUs and the CPU is not shared.\n+However, this is quite likely going to change in future versions of HAT,\n+becoming an optional tag.\n+\n+From the kernel-context object, we can obtain the thread-id, and that's the\n+equivalent to our loop-index from the Java version.\n+We access the different elements of the arrays by using the `kc.gix` (GPU's\n+global thread index), and store the result of the multiplication in the\n+resulting array (array C).\n+\n+But, how do we know how many iterations (threads) to run? Here we map the\n+iteration with the thread-index.\n+In GPU programming, it is a common practice to map the thread-id with the\n+data-item to compute.\n+Thus, ideally, we will launch as many threads as elements from the input\n+arrays (e.g., `a.length`).\n+Since we specify the threads to run at runtime, we need a way to protect us from\n+getting a buffer overflow.\n@@ -155,1 +269,2 @@\n-Then, we only perform the vector multiplication only if the current thread-id is not higher than the maximum threads launched. \n+Then, we only perform the vector multiplication only if the current thread-id is\n+not higher than the maximum threads launched.\n@@ -159,2 +274,6 @@\n-To access the data from a `F32Array`, or any array type that the HAT API exposes, we need to call the `array` method. This essentially is a getter method. \n-However, HAT also provides high-level abstraction for array types that allows us to create views of arrays to facilitate GPU programming, particularly when we have to access multiple arrays within the kernel method. \n+To access the data from a `F32Array`, or any array type that the HAT API\n+exposes, we need to call the `array` method. This essentially is a getter\n+method.\n+However, HAT also provides high-level abstraction for array types that allows us\n+to create views of arrays to facilitate GPU programming, particularly when we\n+have to access multiple arrays within the kernel method.\n@@ -180,1 +299,4 @@\n-Java programmers can choose which representation to use, and since both approaches generate the exact low-level GPU code, programmers do not have to worry about these abstractions impacting performance. For the rest of the article, we use the usual notation `get` and `set` through the `array` method. \n+Java programmers can choose which representation to use, and since both\n+approaches generate the exact low-level GPU code, programmers do not have to\n+worry about these abstractions impacting performance. For the rest of the\n+article, we use the usual notation `get` and `set` through the `array` method.\n@@ -184,1 +306,3 @@\n-So far we have seen two of the core API components: the compute-kernel API (the actual Java method to be offloaded and the kernel-context object), and the `IFace` memory buffers with the `F32Array` object type. \n+So far we have seen two of the core API components: the compute-kernel API (the\n+actual Java method to be offloaded and the kernel-context object), and the\n+`IFace` memory buffers with the `F32Array` object type.\n@@ -186,1 +310,3 @@\n-Next, we need a way to specify the total number of threads (or how many iterations) we want to process. This is done through and ND-Range object. We define the ND-Range in a compute-context method as follows:\n+Next, we need a way to specify the total number of threads (or how many\n+iterations) we want to process. This is done through and ND-Range object. We\n+define the ND-Range in a compute-context method as follows:\n@@ -200,8 +326,18 @@\n-HAT defines ranges in 1D, 2D and 3D. We will dive in into these multidimensional ranges later when we optimize the matrix multiplication algorithm, but in a nutshell, they represent a way to organize and map threads to data. \n-Usually, when we have 1D data structures, we define a 1D range. Similarly, when we have 2D data structures, we usually define a 2D range.  As a programmer, you decide the range that best suits your application.\n-\n-In HAT, you can also define a `Local` range. This corresponds to thread-partitioning (we can create smaller partition of threads from the global sizes, called blocks). \n-If we don't specify a block size, the HAT runtime will decide one for us. \n-Thread-blocks are particular important when we want to share data across different threads, because, on GPUs, data can only be shared across threads within the same block. \n-We will analyze this in more detail when we optimize the matrix-multiplication application. \n-For example, if we want to specify a block size of 256 threads, we can do the following:\n+HAT defines ranges in 1D, 2D and 3D. We will dive in into these multidimensional\n+ranges later when we optimize the matrix multiplication algorithm, but in a\n+nutshell, they represent a way to organize and map threads to data.\n+Usually, when we have 1D data structures, we define a 1D range. Similarly, when\n+we have 2D data structures, we usually define a 2D range. As a programmer, you\n+decide the range that best suits your application.\n+\n+In HAT, you can also define a `Local` range. This corresponds to\n+thread-partitioning (we can create smaller partition of threads from the global\n+sizes, called blocks).\n+If we don't specify a block size, the HAT runtime will decide one for us.\n+Thread-blocks are particular important when we want to share data across\n+different threads, because, on GPUs, data can only be shared across threads\n+within the same block.\n+We will analyze this in more detail when we optimize the matrix-multiplication\n+application.\n+For example, if we want to specify a block size of 256 threads, we can do the\n+following:\n@@ -214,1 +350,2 @@\n-If, for example, `arrayA` is of size 1024, we can create four groups of 256 threads. \n+If, for example, `arrayA` is of size 1024, we can create four groups of 256\n+threads.\n@@ -216,3 +353,6 @@\n-Another important observation is that the `computeContext` is also annotated with `@Reflect`. \n-This allows the HAT runtime and the HAT compiler to inspect all reachable kernels (methods to accelerate) and optimize the data flow across those kernels.\n-Next, let's take a look at how HAT compiles code and how it interacts with code reflection. \n+Another important observation is that the `computeContext` is also annotated\n+with `@Reflect`.\n+This allows the HAT runtime and the HAT compiler to inspect all reachable\n+kernels (methods to accelerate) and optimize the data flow across those kernels.\n+Next, let's take a look at how HAT compiles code and how it interacts with code\n+reflection.\n@@ -222,2 +362,4 @@\n-The following diagram shows an abstract representation of all components of the HAT software stack and how they interact with code reflection. \n-The left-hand side of the diagram shows the Java program with the kernel method on the top, and the compute method with its ND-range definition.\n+The following diagram shows an abstract representation of all components of the\n+HAT software stack and how they interact with code reflection.\n+The left-hand side of the diagram shows the Java program with the kernel method\n+on the top, and the compute method with its ND-range definition.\n@@ -225,2 +367,4 @@\n-We haven't introduced this yet. This is just an object that bundles a hardware accelerator with an entry point in the compute layer. \n-Between `accelerator` and the `compute` layers, programmers can compose complex applications with multiple kernels and multiple thread-configurations. \n+We haven't introduced this yet. This is just an object that bundles a hardware\n+accelerator with an entry point in the compute layer.\n+Between `accelerator` and the `compute` layers, programmers can compose complex\n+applications with multiple kernels and multiple thread-configurations.\n@@ -232,2 +376,6 @@\n-Furthermore, the accelerator and the compute-layers are used to build the whole compute-graph. \n-This is the time in which HAT retrieves all reachable code models from the code reflection API that are annotated with `@Reflect`, builds its code model, and passes a set of transformation to translate the original code model into a GPU-friendly code model. \n+Furthermore, the accelerator and the compute-layers are used to build the whole\n+compute-graph.\n+This is the time in which HAT retrieves all reachable code models from the code\n+reflection API that are annotated with `@Reflect`, builds its code model, and\n+passes a set of transformation to translate the original code model into a\n+GPU-friendly code model.\n@@ -235,3 +383,6 @@\n-When all transformation are performed, HAT generates the corresponding low-level code.\n-If we dispatch the application with the CUDA backend, HAT generates an equivalent CUDA program.\n-Similarly, if we dispatch the application with the OpenCL backend, HAT generates an equivalent OpenCL program.\n+When all transformation are performed, HAT generates the corresponding low-level\n+code.\n+If we dispatch the application with the CUDA backend, HAT generates an\n+equivalent CUDA program.\n+Similarly, if we dispatch the application with the OpenCL backend, HAT generates\n+an equivalent OpenCL program.\n@@ -239,2 +390,3 @@\n-Then, HAT launches the application on the corresponding accelerator. \n-Note that the data movement, compilation, and execution handler to be able to launch programs on GPUs are hidden from the developer.\n+Then, HAT launches the application on the corresponding accelerator.\n+Note that the data movement, compilation, and execution handler to be able to\n+launch programs on GPUs are hidden from the developer.\n@@ -244,3 +396,5 @@\n-At the time of writing this article, HAT offers two backends for GPUs: a CUDA backend to target NVIDIA GPUs, and an OpenCL backend to target any OpenCL-compatible device (including Apple M-chip, Intel CPUs, GPUs and FPGAs) and even ARM and RISC-V CPU system via OpenCL implementations such as [OCK](https:\/\/github.com\/uxlfoundation\/oneapi-construction-kit).\n-\n-Furthermore, it offers a Java multithreaded backend, although, at the time of writing this article, some advanced constructs are not implemented just yet (e.g, 2D and 3D ranges and shared memory).\n+At the time of writing this article, HAT offers two backends for GPUs: a CUDA\n+backend to target NVIDIA GPUs, and an OpenCL backend to target any\n+OpenCL-compatible device (including Apple M-chip, Intel CPUs, GPUs and FPGAs)\n+and even ARM and RISC-V CPU system via OpenCL implementations such\n+as [OCK](https:\/\/github.com\/uxlfoundation\/oneapi-construction-kit).\n@@ -248,0 +402,3 @@\n+Furthermore, it offers a Java multithreaded backend, although, at the time of\n+writing this article, some advanced constructs are not implemented just yet \n+(e.g, 2D and 3D ranges and shared memory).\n@@ -251,29 +408,40 @@\n-Let's show HAT in action! We are going to program and run the Matrix-Multiplication example on an OCI instance, [BM.GPU.A10](https:\/\/docs.oracle.com\/en-us\/iaas\/Content\/Compute\/References\/computeshapes.htm), with an [NVIDIA A10 GPU](https:\/\/www.nvidia.com\/en-us\/data-center\/products\/a10-gpu\/).  \n-\n-The source code of this example is [fully available on GitHub](https:\/\/github.com\/openjdk\/babylon\/blob\/code-reflection\/hat\/examples\/matmul\/src\/main\/java\/matmul\/Main.java), under the `hat` subdirectory of the project Babylon.\n-\n-Before we dive in into the performance evaluation and HAT feature testing, let's define the hardware and software stack we are using. The following table summarizes the hw\/sw components:\n-\n-\n-| System Component        | Version                                                                                          |\n-| ----------------------- | ------------------------------------------------------------------------------------------------ |\n-| OCI Instance            | [BM.GPU.A10](https:\/\/docs.oracle.com\/en-us\/iaas\/Content\/Compute\/References\/computeshapes.htm)    | \n-| CPU                     | [Intel Xeon Platinum 8358 CPU @ 2.60GHz](https:\/\/www.intel.com\/content\/www\/us\/en\/products\/sku\/212282\/intel-xeon-platinum-8358-processor-48m-cache-2-60-ghz\/specifications.html)|\n-| System RAM              | 236 GB                                                                                           |\n-| GPU                     | [NVIDIA A10](https:\/\/www.nvidia.com\/en-us\/data-center\/products\/a10-gpu\/)                         |\n-| OS                      | Ubuntu 22.04.5 LTS                                                                               |\n-| Linux Kernel            | 6.8.0-1039-oracle                                                                                |\n-| NVIDIA Driver           | 580.105.08                                                                                       |\n-| CUDA SDK                | 13.0.88                                                                                          |\n-| Java Version (Babylon)  | [9b1ef462b0a](https:\/\/github.com\/openjdk\/babylon\/commit\/9b1ef462b0ac73f24eed78ab35f7f093f788ca9d)|\n-| JDK Base Version        | 26.ea.10-open (downloaded from [sdkman](https:\/\/sdkman.io\/))                                     |\n-| Application             | Matrix-Multiplication                                                                            |\n-| Matrix Sizes            | 1024x1024 (squared)                                                                              |\n-| HAT Backend             | CUDA                                                                                             |\n-\n-A side note: The `@Reflect` annotation used to be called `@CodeReflection`. \n-The evaluation was performed [before this refactoring](https:\/\/github.com\/openjdk\/babylon\/pull\/693).\n-\n-We use [NVIDIA Nsight Compute (`ncu`)](https:\/\/developer.nvidia.com\/nsight-compute) for profiling CUDA kernels on the GPU. This tool is a very useful kernel profiler used by CUDA developers that facilitates understanding of the computation and memory behavior of our kernels. \n-\n-For measuring the elapsed time on the CPU, we run the Java multithreaded version multiple times (100 times) and take the average of all runs. \n+Let's show HAT in action! We are going to program and run the\n+Matrix-Multiplication example on an OCI\n+instance, [BM.GPU.A10](https:\/\/docs.oracle.com\/en-us\/iaas\/Content\/Compute\/References\/computeshapes.htm),\n+with an [NVIDIA A10 GPU](https:\/\/www.nvidia.com\/en-us\/data-center\/products\/a10-gpu\/).\n+\n+The source code of this example\n+is [fully available on GitHub](https:\/\/github.com\/openjdk\/babylon\/blob\/code-reflection\/hat\/examples\/matmul\/src\/main\/java\/matmul\/Main.java),\n+under the `hat` subdirectory of the project Babylon.\n+\n+Before we dive in into the performance evaluation and HAT feature testing, let's\n+define the hardware and software stack we are using. The following table\n+summarizes the hw\/sw components:\n+\n+| System Component       | Version                                                                                                                                                                         |\n+|------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n+| OCI Instance           | [BM.GPU.A10](https:\/\/docs.oracle.com\/en-us\/iaas\/Content\/Compute\/References\/computeshapes.htm)                                                                                   | \n+| CPU                    | [Intel Xeon Platinum 8358 CPU @ 2.60GHz](https:\/\/www.intel.com\/content\/www\/us\/en\/products\/sku\/212282\/intel-xeon-platinum-8358-processor-48m-cache-2-60-ghz\/specifications.html) |\n+| System RAM             | 236 GB                                                                                                                                                                          |\n+| GPU                    | [NVIDIA A10](https:\/\/www.nvidia.com\/en-us\/data-center\/products\/a10-gpu\/)                                                                                                        |\n+| OS                     | Ubuntu 22.04.5 LTS                                                                                                                                                              |\n+| Linux Kernel           | 6.8.0-1039-oracle                                                                                                                                                               |\n+| NVIDIA Driver          | 580.105.08                                                                                                                                                                      |\n+| CUDA SDK               | 13.0.88                                                                                                                                                                         |\n+| Java Version (Babylon) | [9b1ef462b0a](https:\/\/github.com\/openjdk\/babylon\/commit\/9b1ef462b0ac73f24eed78ab35f7f093f788ca9d)                                                                               |\n+| JDK Base Version       | 26.ea.10-open (downloaded from [sdkman](https:\/\/sdkman.io\/))                                                                                                                    |\n+| Application            | Matrix-Multiplication                                                                                                                                                           |\n+| Matrix Sizes           | 1024x1024 (squared)                                                                                                                                                             |\n+| HAT Backend            | CUDA                                                                                                                                                                            |\n+\n+A side note: The `@Reflect` annotation used to be called `@CodeReflection`.\n+The evaluation was\n+performed [before this refactoring](https:\/\/github.com\/openjdk\/babylon\/pull\/693).\n+\n+We use [NVIDIA Nsight Compute (`ncu`)](https:\/\/developer.nvidia.com\/nsight-compute) \n+for profiling CUDA kernels on the GPU. This tool is a very useful kernel \n+profiler used by CUDA developers that facilitates understanding of the \n+computation and memory behavior of our kernels.\n+\n+For measuring the elapsed time on the CPU, we run the Java multithreaded version\n+multiple times (100 times) and take the average of all runs.\n@@ -283,2 +451,5 @@\n-We will establish two baselines: one for the CPU and one for the GPU. We start with the CPU because it is likely more familiar for Java programmers. This baseline utilizes [Java parallel streams to perform matrix-multiplication](https:\/\/github.com\/openjdk\/babylon\/blob\/code-reflection\/hat\/examples\/matmul\/src\/main\/java\/matmul\/Main.java#L964-L974). \n-Each CPU thread will perform a dot-product. \n+We will establish two baselines: one for the CPU and one for the GPU. We start\n+with the CPU because it is likely more familiar for Java programmers. This\n+baseline\n+utilizes [Java parallel streams to perform matrix-multiplication](https:\/\/github.com\/openjdk\/babylon\/blob\/code-reflection\/hat\/examples\/matmul\/src\/main\/java\/matmul\/Main.java#L964-L974).\n+Each CPU thread will perform a dot-product.\n@@ -286,1 +457,4 @@\n-While we show the performance on the CPU, our primary goal is to benchmark multiple optimizations for GPUs expressed with HAT against the GPU baseline implementation. However, Java developers can get a high-level view of what GPU performance is when it compares to the CPU with Java. \n+While we show the performance on the CPU, our primary goal is to benchmark\n+multiple optimizations for GPUs expressed with HAT against the GPU baseline\n+implementation. However, Java developers can get a high-level view of what GPU\n+performance is when it compares to the CPU with Java.\n@@ -288,1 +462,4 @@\n-Another important note is that, some techniques presented in this article to improve GPU kernel performance could be also applied for CPUs. However, many of these techniques are not directly available for Java developers, and the only solution would be to run native code via a JNI call. \n+Another important note is that, some techniques presented in this article to\n+improve GPU kernel performance could be also applied for CPUs. However, many of\n+these techniques are not directly available for Java developers, and the only\n+solution would be to run native code via a JNI call.\n@@ -303,1 +480,2 @@\n-The CPU version takes, on average, ~299ms (after warm-up). Note the OCI instance we are using is a paravirtualzed VM with 15 CPU cores. \n+The CPU version takes, on average, ~299ms (after warm-up). Note the OCI instance\n+we are using is a paravirtualzed VM with 15 CPU cores.\n@@ -305,1 +483,2 @@\n-Let's calculate also the throughput for this benchmark, as the number of floating point operations per second expressed in GFLOP\/s.\n+Let's calculate also the throughput for this benchmark, as the number of\n+floating point operations per second expressed in GFLOP\/s.\n@@ -307,1 +486,2 @@\n-The following code snippet shows the floating point operations within the matrix multiplication kernel:\n+The following code snippet shows the floating point operations within the matrix\n+multiplication kernel:\n@@ -313,2 +493,4 @@\n-We have an addition and a multiplication per iteration, and we have `size * size * size` iterations:\n-Since we are running with matrix sizes of 1024, the FLOPS are calculated as follows:\n+We have an addition and a multiplication per iteration, and we have\n+`size * size * size` iterations:\n+Since we are running with matrix sizes of 1024, the FLOPS are calculated as\n+follows:\n@@ -334,3 +516,9 @@\n-This Java method is able to run at 7.1 GFLOP\/s. This will be our reference for CPU.\n-Note that, in this synthetic benchmark, while we are running multiple times and reach the JITed code by Java JIT compiler, we don't flush the caches and don't set the CPU frequency. \n-This is important to consider when compare against the GPU versions, in which we do control the cache behavior and set the GPU frequency. Thus, this will give the CPU version a slightly better advantage when comparing with the GPU versions. \n+This Java method is able to run at 7.1 GFLOP\/s. This will be our reference for\n+CPU.\n+Note that, in this synthetic benchmark, while we are running multiple times and\n+reach the JITed code by Java JIT compiler, we don't flush the caches and don't\n+set the CPU frequency.\n+This is important to consider when compare against the GPU versions, in which we\n+do control the cache behavior and set the GPU frequency. Thus, this will give\n+the CPU version a slightly better advantage when comparing with the GPU\n+versions.\n@@ -340,1 +528,6 @@\n-Let’s jump to our first GPU implementation. To be able to understand performance of the GPU kernel, we are going to measure just the GPU kernel elapsed time by using the [NVIDIA Nsight Compute Profiler (ncu)](https:\/\/developer.nvidia.com\/nsight-compute). Later, we will show the overall performance, including the cost of data transfers between the CPU and the GPU.\n+Let’s jump to our first GPU implementation. To be able to understand performance\n+of the GPU kernel, we are going to measure just the GPU kernel elapsed time by\n+using\n+the [NVIDIA Nsight Compute Profiler (ncu)](https:\/\/developer.nvidia.com\/nsight-compute).\n+Later, we will show the overall performance, including the cost of data\n+transfers between the CPU and the GPU.\n@@ -342,1 +535,3 @@\n-The `ncu` kernel profiling tool sets the GPU frequency and sets the memory system into a deterministic state. This will facilitate comparing GPU kernels obtain useful profiling information, not just the GPU kernel time.\n+The `ncu` kernel profiling tool sets the GPU frequency and sets the memory\n+system into a deterministic state. This will facilitate comparing GPU kernels\n+obtain useful profiling information, not just the GPU kernel time.\n@@ -344,1 +539,1 @@\n-Our first kernel is configured to run in 1D kernel. \n+Our first kernel is configured to run in 1D kernel.\n@@ -368,1 +563,2 @@\n-Each GPU thread will run the two innermost loops to perform the matrix-matrix multiplication. \n+Each GPU thread will run the two innermost loops to perform the matrix-matrix\n+multiplication.\n@@ -373,2 +569,2 @@\n-cc.dispatchKernel(ndRange,\n-    kc -> matrixMultiplyKernel1D(kc, matrixA, matrixB, matrixC, 1024)\n+cc.dispatchKernel(ndRange, \n+ kc->matrixMultiplyKernel1D(kc, matrixA, matrixB, matrixC, 1024)\n@@ -378,1 +574,2 @@\n-In this case, we are launching a 1D kernel with its global size set to 1024 threads.\n+In this case, we are launching a 1D kernel with its global size set to 1024\n+threads.\n@@ -380,7 +577,13 @@\n-This is an important value, and the best value will vary between GPUs. \n-In our case we set it to 16 threads per block, and this will give us the highest performance with 1D configuration for this kernel on the A10 GPU. \n-However, if you are running this example on another GPU, you may need to tune this number. \n-\n-The GPU version takes **95ms (3x faster than the Java Parallel Stream version)** (the GPU kernel elapsed time only, meaning that we exclude communications between the CPU and the GPU; more in this later).\n-Thus, this means $3x$ times faster than the previous version on the CPU, but, can we do better with HAT?  \n-To profile the generated GPU kernel by HAT, we use the NVIDIA Nsight profiler analysis.\n+This is an important value, and the best value will vary between GPUs.\n+In our case we set it to 16 threads per block, and this will give us the highest\n+performance with 1D configuration for this kernel on the A10 GPU.\n+However, if you are running this example on another GPU, you may need to tune\n+this number.\n+\n+The GPU version takes **95ms (3x faster than the Java Parallel Stream version)\n+** (the GPU kernel elapsed time only, meaning that we exclude communications\n+between the CPU and the GPU; more in this later).\n+Thus, this means $3x$ times faster than the previous version on the CPU, but,\n+can we do better with HAT?  \n+To profile the generated GPU kernel by HAT, we use the NVIDIA Nsight profiler\n+analysis.\n@@ -413,3 +616,5 @@\n-This table shows one of the main section of the `ncu` profiler report, the *GPU Speed of Light* (we only show a few entries). \n-This section summarizes key metrics, such as the percentage of the peak memory and compute throughput achieved,\n-the kernel duration, and L1 and L2 sustained throughput, etc. \n+This table shows one of the main section of the `ncu` profiler report, the *GPU\n+Speed of Light* (we only show a few entries).\n+This section summarizes key metrics, such as the percentage of the peak memory\n+and compute throughput achieved,\n+the kernel duration, and L1 and L2 sustained throughput, etc.\n@@ -417,2 +622,3 @@\n-For now, let’s look at the memory and compute throughput. \n-From just these two values, we can see that the GPU utilization is extremely low, and in fact, the `ncu` profiler tool gives a hint about how to improve it. \n+For now, let’s look at the memory and compute throughput.\n+From just these two values, we can see that the GPU utilization is extremely\n+low, and in fact, the `ncu` profiler tool gives a hint about how to improve it.\n@@ -420,1 +626,2 @@\n- > This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs.\n+> This kernel grid is too small to fill the available resources on this device,\n+> resulting in only 0.1 full waves across all SMs.\n@@ -422,2 +629,4 @@\n-In fact, this kernel can be represented as a 2D grid kernel, instead of 1D. We can also configure 2D as local thread-block size.\n-Thus, we can increase the total number of threads, and we can increase the number of blocks.\n+In fact, this kernel can be represented as a 2D grid kernel, instead of 1D. We\n+can also configure 2D as local thread-block size.\n+Thus, we can increase the total number of threads, and we can increase the\n+number of blocks.\n@@ -429,7 +638,17 @@\n-For GPU programmers (e.g., CUDA, OpenCL or SYCL programmers), this 2D representation is a straightforward representation of the program, and for GPU developers, this should be the initial way to express the GPU kernel. However, this might not be so trivial for Java developers, and for good reasons! Java developers do not have to think about GPU architectures and GPU execution models. \n-\n-But this strengths one of the key messages of this article: **code reflection, and Babylon, allows developers to enable specific optimizations when targeting foreign programming languages**, such as CUDA. \n-\n-The parallelization constructs of many the programming languages are in 1D (one dimensional), and this includes Java. \n-However, some parallel programming models (e.g., CUDA and OpenCL) exposes multidimensional constructs to facilitate representation of parallel programs. \n-Let's say, if we have input matrices and each cell of the matrix can be computed completely in parallel, we usually represent a 2D kernel. \n+For GPU programmers (e.g., CUDA, OpenCL or SYCL programmers), this 2D\n+representation is a straightforward representation of the program, and for GPU\n+developers, this should be the initial way to express the GPU kernel. However,\n+this might not be so trivial for Java developers, and for good reasons! Java\n+developers do not have to think about GPU architectures and GPU execution\n+models.\n+\n+But this strengths one of the key messages of this article: **code reflection,\n+and Babylon, allows developers to enable specific optimizations when targeting\n+foreign programming languages**, such as CUDA.\n+\n+The parallelization constructs of many the programming languages are in 1D (one\n+dimensional), and this includes Java.\n+However, some parallel programming models (e.g., CUDA and OpenCL) exposes\n+multidimensional constructs to facilitate representation of parallel programs.\n+Let's say, if we have input matrices and each cell of the matrix can be computed\n+completely in parallel, we usually represent a 2D kernel.\n@@ -437,1 +656,3 @@\n-In reality, these multidimensional constructs are more a convenience for the programmer, and the actual runtime (e.g., OpenCL runtime and drivers) could implement multidimensionality on top of a 1D range space. \n+In reality, these multidimensional constructs are more a convenience for the\n+programmer, and the actual runtime (e.g., OpenCL runtime and drivers) could\n+implement multidimensionality on top of a 1D range space.\n@@ -439,2 +660,4 @@\n-As programmers in HAT, we could also take advantage of this multidimensionality, and this is when the `ND-Range` API becomes handy. \n-The following Java code snippet shows the HAT GPU kernel for expressing the matrix multiplication in 2D. \n+As programmers in HAT, we could also take advantage of this multidimensionality,\n+and this is when the `ND-Range` API becomes handy.\n+The following Java code snippet shows the HAT GPU kernel for expressing the\n+matrix multiplication in 2D.\n@@ -470,1 +693,2 @@\n-Instead, we can access the thread-id for the second dimension using the HAT construct `kc.giy`.\n+Instead, we can access the thread-id for the second dimension using the HAT\n+construct `kc.giy`.\n@@ -486,4 +710,8 @@\n-This new version launches $1024x1024$ threads in blocks of $16x16$. As a HAT programmer, we can play with different values of block sizes.\n-And, if you are curious, I invite you to do so. \n-You will experiment with another value for performance tuning on GPUs, deciding the right block size. \n-I can tell, for our experiments on the A10 GPU, this value seems to be the most efficient one, but keep in mind that if you use a different GPU, this is one of the parameters to be tuned. \n+This new version launches $1024x1024$ threads in blocks of $16x16$. As a HAT\n+programmer, we can play with different values of block sizes.\n+And, if you are curious, I invite you to do so.\n+You will experiment with another value for performance tuning on GPUs, deciding\n+the right block size.\n+I can tell, for our experiments on the A10 GPU, this value seems to be the most\n+efficient one, but keep in mind that if you use a different GPU, this is one of\n+the parameters to be tuned.\n@@ -491,1 +719,2 @@\n-For convenience to Java developers, HAT also offers a simplified version to define ND-ranges.\n+For convenience to Java developers, HAT also offers a simplified version to\n+define ND-ranges.\n@@ -497,1 +726,2 @@\n-The first two parameters corresponds to the dimensions of the global size, and the last two parameters correspond to local block size. \n+The first two parameters corresponds to the dimensions of the global size, and\n+the last two parameters correspond to local block size.\n@@ -499,2 +729,3 @@\n-Let's go back to our profiler. This new 2D version in HAT takes $9.05ms$ (this is $10x$ faster compared to the previous version!). \n-Let’s take a look at the performance metrics.  \n+Let's go back to our profiler. This new 2D version in HAT takes $9.05ms$ (this\n+is $10x$ faster compared to the previous version!).\n+Let’s take a look at the performance metrics.\n@@ -519,2 +750,5 @@\n-Now the compute-throughput has increased to 23%, and the memory throughput to 99% of the peak value on this GPU. \n-Usually, when there is a big difference between these two metrics and one of them is higher than 80%, the main suggestion is to shift one to the other. The NCU profiler also gives us another useful hint:\n+Now the compute-throughput has increased to 23%, and the memory throughput to\n+99% of the peak value on this GPU.\n+Usually, when there is a big difference between these two metrics and one of\n+them is higher than 80%, the main suggestion is to shift one to the other. The\n+NCU profiler also gives us another useful hint:\n@@ -522,1 +756,2 @@\n->  On average, only 4.2 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. \n+> On average, only 4.2 of the 32 bytes transmitted per sector are utilized by\n+> each thread. This could possibly be caused by a stride between threads.\n@@ -526,1 +761,3 @@\n-> The memory access pattern for global loads from L1 might not be optimal. On average, only 4.2 of the 32 bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between threads. \n+> The memory access pattern for global loads from L1 might not be optimal. On\n+> average, only 4.2 of the 32 bytes transmitted per sector are utilized by each\n+> thread. This could possibly be caused by a stride between threads.\n@@ -529,5 +766,11 @@\n-If you are familiar with the GPU programming model, you will quickly recognize this. \n-One of the possible techniques to improve this kernel is to reorganize the memory access patterns to **have global coalesced memory accesses**.\n-\n-Before we jump in into our next optimization, I want to highlight one of many optimizations the CUDA JIT compiler is doing for us:\n-From the source section of the `ncu` profiler tool, we can also inspect the generated PTX (the CUDA intermediate representation) and SASS code (GPU program assembly code). For the previous 1D kernel as well as this new version, we see that a bunch of FMA (fused-multiply-add) instructions are generated.\n+If you are familiar with the GPU programming model, you will quickly recognize\n+this.\n+One of the possible techniques to improve this kernel is to reorganize the\n+memory access patterns to **have global coalesced memory accesses**.\n+\n+Before we jump in into our next optimization, I want to highlight one of many\n+optimizations the CUDA JIT compiler is doing for us:\n+From the source section of the `ncu` profiler tool, we can also inspect the\n+generated PTX (the CUDA intermediate representation) and SASS code (GPU program\n+assembly code). For the previous 1D kernel as well as this new version, we see\n+that a bunch of FMA (fused-multiply-add) instructions are generated.\n@@ -542,1 +785,5 @@\n-This is great, because, from the Java side, we did not specify any FMA operation, and the HAT JIT compiler does not currently have any compiler passes to optimize these math operations. But we see that the `nvcc` compiler and the driver JIT compiler are able to optimize these math instructions, very similar to what the Java JIT compilers (C1, C2 and GraalVM) do.\n+This is great, because, from the Java side, we did not specify any FMA\n+operation, and the HAT JIT compiler does not currently have any compiler passes\n+to optimize these math operations. But we see that the `nvcc` compiler and the\n+driver JIT compiler are able to optimize these math instructions, very similar\n+to what the Java JIT compilers (C1, C2 and GraalVM) do.\n@@ -548,5 +795,14 @@\n-If data to be accessed on the GPU are not contiguous, the GPU might need to fetch multiple cache lines to be able to load the right data, and if the data is also not present in the L2 cache, then the worst case is to continuously fetch the data from the GPU's global. \n-\n-In this article, we are not going to detail how the GPU programming and execution models work, and how GPU threads accesses memory, \n-but, in a nutshell, GPU coalesced memory accesses occur when consecutive threads within a warp access consecutive memory locations, minimizing cache misses. \n-A **warp** is a fundamental unit of execution on NVIDIA GPUs, representing a set of 32 threads that run in parallel on the same Streaming Multiprocessor (SM). An SM would be equivalent to a CPU core, and all GPU cores (CUDA cores in the case of NVIDIA GPUs) would be equivalent to functional units and vector units of CPU cores.\n+If data to be accessed on the GPU are not contiguous, the GPU might need to\n+fetch multiple cache lines to be able to load the right data, and if the data is\n+also not present in the L2 cache, then the worst case is to continuously fetch\n+the data from the GPU's global.\n+\n+In this article, we are not going to detail how the GPU programming and\n+execution models work, and how GPU threads accesses memory,\n+but, in a nutshell, GPU coalesced memory accesses occur when consecutive threads\n+within a warp access consecutive memory locations, minimizing cache misses.\n+A **warp** is a fundamental unit of execution on NVIDIA GPUs, representing a set\n+of 32 threads that run in parallel on the same Streaming Multiprocessor (SM). An\n+SM would be equivalent to a CPU core, and all GPU cores (CUDA cores in the case\n+of NVIDIA GPUs) would be equivalent to functional units and vector units of CPU\n+cores.\n@@ -555,2 +811,0 @@\n-- https:\/\/developer.nvidia.com\/blog\/unlock-gpu-performance-global-memory-access-in-cuda\/ \n-- https:\/\/siboehm.com\/articles\/22\/CUDA-MMM \n@@ -558,0 +812,2 @@\n+- [Unlock GPU Performance: Global Memory Access in CUDA](https:\/\/developer.nvidia.com\/blog\/unlock-gpu-performance-global-memory-access-in-cuda\/)\n+- [Optimize a CUDA Matmul Kernel for cuBLAS-like Performance](https:\/\/siboehm.com\/articles\/22\/CUDA-MMM)\n@@ -559,1 +815,2 @@\n-Analyzing our previous 2D kernel, we see that the memory accessing pattern is not optimal:\n+Analyzing our previous 2D kernel, we see that the memory accessing pattern is\n+not optimal:\n@@ -567,2 +824,4 @@\n-We are using the thread-index `kc.gix` to access, in theory, the same row of a matrix for all `kc.giy` thread-indexes. \n-But this is not how the CUDA thread mapping works. As mentioned in the [NVIDIA CUDA blogs](https:\/\/developer.nvidia.com\/blog\/unlock-gpu-performance-global-memory-access-in-cuda\/):\n+We are using the thread-index `kc.gix` to access, in theory, the same row of a\n+matrix for all `kc.giy` thread-indexes.\n+But this is not how the CUDA thread mapping works. As mentioned in\n+the [NVIDIA CUDA blogs](https:\/\/developer.nvidia.com\/blog\/unlock-gpu-performance-global-memory-access-in-cuda\/):\n@@ -570,1 +829,2 @@\n-> When using 2 or 3-dimensional thread blocks in a CUDA kernel, the threads are laid out linearly with the X index, or `threadIdx.x`, moving the fastest.\n+> When using 2 or 3-dimensional thread blocks in a CUDA kernel, the threads are\n+> laid out linearly with the X index, or `threadIdx.x`, moving the fastest.\n@@ -572,1 +832,7 @@\n-The `threadIdx.x` in CUDA is a built-in to access the local thread-id (thread-id within a block). In HAT, this is the equivalent of `kernelContext.lix`. Note that these built-in functions in HAT are common for all backends, including OpenCL and CUDA. \n+The `threadIdx.x` in CUDA is a built-in to access the local thread-id (thread-id\n+within a block). In HAT, this is the equivalent of `kernelContext.lix`. Note\n+that these built-in functions in HAT are common for all backends, including\n+OpenCL and CUDA.\n+\n+But, what does this mean in practice? For instance, if we have a $4x4$ thread\n+block, the way consecutive threads are organized are as follows:\n@@ -574,1 +840,0 @@\n-But, what does this mean in practice? For instance, if we have a $4x4$ thread block, the way consecutive threads are organized are as follows:\n@@ -579,1 +844,2 @@\n-Thus, for matrix A of our example, the following indexing applies for consecutive threads based on the previous kernel:\n+Thus, for matrix A of our example, the following indexing applies for\n+consecutive threads based on the previous kernel:\n@@ -592,2 +858,4 @@\n-To achieve this in our example in HAT, we swap the indexes between `kc.gix` and `kc.giy`. \n-Thus, instead of `matrixA.array(kc.gix * size + k)`, we change the indexing to `matrixA.array(kc.giy * size + k)`.\n+To achieve this in our example in HAT, we swap the indexes between `kc.gix` and\n+`kc.giy`.\n+Thus, instead of `matrixA.array(kc.gix * size + k)`, we change the indexing to\n+`matrixA.array(kc.giy * size + k)`.\n@@ -617,1 +885,2 @@\n-The kernel-dispatch (`NDRange`) remains the same. We can also keep the same block size ($16x16$ threads).\n+The kernel-dispatch (`NDRange`) remains the same. We can also keep the same\n+block size ($16x16$ threads).\n@@ -619,1 +888,3 @@\n-When we profile this GPU kernel with `ncu`, we see that the kernel time takes **2.19 ms** ($4.1x$ times faster than the previous kernel!). But we are not done yet. What else can we do? Let’s take a look at the summary of the profiler:\n+When we profile this GPU kernel with `ncu`, we see that the kernel time takes \n+**2.19 ms** ($4.1x$ times faster than the previous kernel!). But we are not done\n+yet. What else can we do? Let’s take a look at the summary of the profiler:\n@@ -637,1 +908,1 @@\n-Now, the memory and compute throughput have increased to 96%. \n+Now, the memory and compute throughput have increased to 96%.\n@@ -641,3 +912,3 @@\n-> The memory access pattern for global loads from L1TEX might not be optimal. On average, only 14.4 of the 32 bytes transmitted per sector are utilized by each thread.\n-\n-So we have improved from 6 bytes per sector to 14 bytes per sector out of 32 bytes per sector. Still, room for improvements here. \n+> The memory access pattern for global loads from L1TEX might not be optimal. On\n+> average, only 14.4 of the 32 bytes transmitted per sector are utilized by each\n+> thread.\n@@ -645,2 +916,2 @@\n-Since both compute and memory throughput are in equal percentage, the suggestion could be to reduce both by increasing the work per thread, and storing reusable data in shared memory of the GPU (scratchpad memory in which all threads running on the same Streaming multiprocessor of the GPU with the goal of having reusable data across coalescing threads).\n-Let's first tackle how we can express shared memory in HAT.  \n+So we have improved from 6 bytes per sector to 14 bytes per sector out of 32\n+bytes per sector. Still, room for improvements here.\n@@ -648,0 +919,6 @@\n+Since both compute and memory throughput are in equal percentage, the suggestion\n+could be to reduce both by increasing the work per thread, and storing reusable\n+data in shared memory of the GPU (scratchpad memory in which all threads running\n+on the same Streaming multiprocessor of the GPU with the goal of having reusable\n+data across coalescing threads).\n+Let's first tackle how we can express shared memory in HAT.\n@@ -651,1 +928,4 @@\n-In HAT, we can exploit the different levels of the GPU’s memory hierarchy. Similarly to CUDA and OpenCL, HAT can allocate data (potentially arrays, but it could be any other compatible data structures) into the shared memory of the GPU.\n+In HAT, we can exploit the different levels of the GPU’s memory hierarchy.\n+Similarly to CUDA and OpenCL, HAT can allocate data (potentially arrays, but it\n+could be any other compatible data structures) into the shared memory of the\n+GPU.\n@@ -653,2 +933,4 @@\n-Before diving in into this change, let's explain, very briefly, the memory hierarchy of GPUs and how HAT developers can use it. \n-The following figure shows **a simplified representation** of the different levels of GPU's memory hierarchy. \n+Before diving in into this change, let's explain, very briefly, the memory\n+hierarchy of GPUs and how HAT developers can use it.\n+The following figure shows **a simplified representation** of the different\n+levels of GPU's memory hierarchy.\n@@ -660,13 +942,19 @@\n-Starting from the bottom, GPUs have an off-chip memory called global memory. \n-This off-chip memory resides physically on the GPU, but not on the same die area of the GPU cores, or SMs.\n-All SMs (recall, SM means Streaming Multiprocessor) can read\/write to this memory. \n-The global memory is also often called VRAM. \n-There is also a section in the GPU's global memory dedicated to constant memory, in which programmers can place data that is not going to be written back. \n-For some applications, using the constant memory can provide some performance boost. \n-\n-In HAT, we use the global memory when passing arguments to the kernel (e.g, the `F32Array` that is pass as a parameter to the kernel function).\n-Currently, in HAT, we can't use the constant memory of the GPU, but there are plans to expose it to Java programmers too. \n-\n-Moving on, we have the L2 cache. \n-This cache is not directly programmable from OpenCL or CUDA, so neither for HAT. \n-This cache is shared across all SMs within the GPU. \n+Starting from the bottom, GPUs have an off-chip memory called global memory.\n+This off-chip memory resides physically on the GPU, but not on the same die area\n+of the GPU cores, or SMs.\n+All SMs (recall, SM means Streaming Multiprocessor) can read\/write to this\n+memory.\n+The global memory is also often called VRAM.\n+There is also a section in the GPU's global memory dedicated to constant memory,\n+in which programmers can place data that is not going to be written back.\n+For some applications, using the constant memory can provide some performance\n+boost.\n+\n+In HAT, we use the global memory when passing arguments to the kernel (e.g, the\n+`F32Array` that is pass as a parameter to the kernel function).\n+Currently, in HAT, we can't use the constant memory of the GPU, but there are\n+plans to expose it to Java programmers too.\n+\n+Moving on, we have the L2 cache.\n+This cache is not directly programmable from OpenCL or CUDA, so neither for HAT.\n+This cache is shared across all SMs within the GPU.\n@@ -675,2 +963,0 @@\n-- On-chip L1 cache that is private per SM. \n-- The shared memory: this area is a programmable scratchpad memory, and it is also on-chip. It resides in L1 cache, and it is special because it is intended to be used as a temporary workplace. **This area of the GPU is also programmable in HAT via `DeviceType`.**\n@@ -678,2 +964,10 @@\n-Finally, each SM contains a set of register files. This is private memory per thread. \n-If we store a value into a variable, and the variable is only visible per thread, the value is likely stored in private memory. \n+- On-chip L1 cache that is private per SM.\n+- The shared memory: this area is a programmable scratchpad memory, and it is\n+  also on-chip. It resides in L1 cache, and it is special because it is intended\n+  to be used as a temporary workplace. **This area of the GPU is also\n+  programmable in HAT via `DeviceType`.**\n+\n+Finally, each SM contains a set of register files. This is private memory per\n+thread.\n+If we store a value into a variable, and the variable is only visible per\n+thread, the value is likely stored in private memory.\n@@ -681,1 +975,1 @@\n-In this case, variables are stored in global memory. \n+In this case, variables are stored in global memory.\n@@ -683,2 +977,4 @@\n-Thread within the same thread-block can share memory using the same shared memory banks present in the SM.\n-Data can't be shared across different thread-blocks, because there is no guarantee that they run on the same SM. \n+Thread within the same thread-block can share memory using the same shared\n+memory banks present in the SM.\n+Data can't be shared across different thread-blocks, because there is no\n+guarantee that they run on the same SM.\n@@ -686,1 +982,4 @@\n-Another hardware detail to keep in mind is that [L1 cache and shared memory are non-coherent](https:\/\/docs.nvidia.com\/cuda\/cuda-c-programming-guide\/index.html#shared-and-local-memory).  This means that, if we place data into shared memory, we need a barrier to wait for threads copying data to this memory area before the data is reused. \n+Another hardware detail to keep in mind is\n+that [L1 cache and shared memory are non-coherent](https:\/\/docs.nvidia.com\/cuda\/cuda-c-programming-guide\/index.html#shared-and-local-memory).\n+This means that, if we place data into shared memory, we need a barrier to wait\n+for threads copying data to this memory area before the data is reused.\n@@ -688,2 +987,5 @@\n-HAT exposes primitives to perform synchronization operation as well as constructs to place user data types into shared memory and private memory of the GPU. \n-**This is one of the key strengths of the HAT project when it compares to other projects.**\n+HAT exposes primitives to perform synchronization operation as well as\n+constructs to place user data types into shared memory and private memory of the\n+GPU.\n+**This is one of the key strengths of the HAT project when it compares to other\n+projects.**\n@@ -693,1 +995,2 @@\n-HAT provides an interface, called `DeviceType` that can be used to compose user data types that are meant to be stored in private and shared memory of the GPU. \n+HAT provides an interface, called `DeviceType` that can be used to compose user\n+data types that are meant to be stored in private and shared memory of the GPU.\n@@ -720,2 +1023,4 @@\n-The `DeviceSchema` is a type from the HAT API that defines the layout of the user data type for the HAT code generator. \n-`DeviceType` also supports nesting different types, as we will see later when we compose a custom type for FP-16 bits (half of a float).\n+The `DeviceSchema` is a type from the HAT API that defines the layout of the\n+user data type for the HAT code generator.\n+`DeviceType` also supports nesting different types, as we will see later when we\n+compose a custom type for FP-16 bits (half of a float).\n@@ -725,5 +1030,12 @@\n-HAT provides some utility marker methods such as `createLocal` for a shared data structure, and `createPrivate` for private data structure. \n-Note that we follow the [OpenCL terminology](https:\/\/registry.khronos.org\/OpenCL\/specs\/3.0-unified\/html\/OpenCL_API.html) in this case to define shared memory.\n-These two methods (`createLocal` and `createPrivate`) can return `null` from the Java implementation because they are never meant to be executed on the host side (the CPU from Java, at least for now), only from on the device side (e.g., a GPU). Thus, they are only used as **markers** for the HAT code generator. \n-\n-The HAT compiler will build a C99-struct with the members of the passed interface. \n+HAT provides some utility marker methods such as `createLocal` for a shared data\n+structure, and `createPrivate` for private data structure.\n+Note that we follow\n+the [OpenCL terminology](https:\/\/registry.khronos.org\/OpenCL\/specs\/3.0-unified\/html\/OpenCL_API.html)\n+in this case to define shared memory.\n+These two methods (`createLocal` and `createPrivate`) can return `null` from the\n+Java implementation because they are never meant to be executed on the host\n+side (the CPU from Java, at least for now), only from on the device side (e.g.,\n+a GPU). Thus, they are only used as **markers** for the HAT code generator.\n+\n+The HAT compiler will build a C99-struct with the members of the passed\n+interface.\n@@ -751,1 +1063,2 @@\n-The following code snippet shows the matrix-multiplication using shared memory in HAT. \n+The following code snippet shows the matrix-multiplication using shared memory\n+in HAT.\n@@ -753,2 +1066,3 @@\n-Each tile copies a data from global memory to shared memory, performs the dot-product using the shared memory,\n-and copies back the result from shared to global memory in the result array. \n+Each tile copies a data from global memory to shared memory, performs the\n+dot-product using the shared memory,\n+and copies back the result from shared to global memory in the result array.\n@@ -808,4 +1122,6 @@\n-Another key GPU key concept that is exposed in HAT is the thread-block indexing. \n-Similarly to the global indexes such as `kernelContext.gix` to access the global thread-id, \n-we can access the local-thread-id (thread identifier within the block we are running) by using the `kernelContext.lix`,\n-and we can access in the 3-dimensions (`lix`, `liy` and `lix` for 1D, 2D and 3D respectively).\n+Another key GPU key concept that is exposed in HAT is the thread-block indexing.\n+Similarly to the global indexes such as `kernelContext.gix` to access the global\n+thread-id, we can access the local-thread-id (thread identifier within the block\n+we are running) by using the `kernelContext.lix`, and we can access in the\n+3-dimensions (`lix`, `liy` and `lix` for 1D, 2D and 3D\n+respectively).\n@@ -830,2 +1146,5 @@\n-This version takes $1.65ms$ ($1.33x$ faster than the previous version). This one makes use of many of the GPU programmable features, such as accessing local memory, barriers, accessing local-thread-ids and group IDs.\n-However, we still see the GPU utilization higher than 80% for both compute and memory throughput (95% for both). \n+This version takes $1.65ms$ ($1.33x$ faster than the previous version). This one\n+makes use of many of the GPU programmable features, such as accessing local\n+memory, barriers, accessing local-thread-ids and group IDs.\n+However, we still see the GPU utilization higher than 80% for both compute and\n+memory throughput (95% for both).\n@@ -833,1 +1152,6 @@\n-By analyzing the source profiling (the source section of the `ncu` tool), we see that the 33% of the time is spent in data copies from global memory to local memory and vice versa. Besides, a high number of warp (set of 32 consecutive threads on an NVIDIA GPU) stalls are spent in the actual reduction (computation). We can alleviate this problem with our next optimization, register tiling, and increase the work to be done per thread. \n+By analyzing the source profiling (the source section of the `ncu` tool), we see\n+that the 33% of the time is spent in data copies from global memory to local\n+memory and vice versa. Besides, a high number of warp (set of 32 consecutive\n+threads on an NVIDIA GPU) stalls are spent in the actual reduction \n+(computation). We can alleviate this problem with our next optimization, \n+register tiling, and increase the work to be done per thread.\n@@ -837,10 +1161,22 @@\n-Let's re-think what we did in the previous kernel. \n-The previous implementation decomposes the input matrices into small matrices (tiles).\n-The smaller matrices were stored in shared memory of the GPU, and compute the dot-product using the smaller matrices. \n-\n-Similarly, we can build on the previous optimization by also copying the data from shared memory to private memory, and decomposing again arrays stored in shared memory to compute the dot-product in private memory using even smaller tiles. \n-Besides, we are going to increase the work to be done per-thread, so each GPU thread will compute a sub-matrix (e.g., 4x4).\n-\n-This version is heavily inspired by [Simon Boehm's](https:\/\/siboehm.com\/articles\/22\/CUDA-MMM) on optimizing matrix multiplication, and it is equivalent to his version 5. If you are interested into the details of this optimization, I highly recommend reading his excellent blog.\n-\n-The key differences compared to the previous kernel is that we have two types of `DeviceType` data structures: one for shared data structures to be allocated in shared memory, and another one for allocating into the private memory of the GPU.\n+Let's re-think what we did in the previous kernel.\n+The previous implementation decomposes the input matrices into small matrices \n+(tiles). The smaller matrices were stored in shared memory of the GPU, and \n+compute the dot-product using the smaller matrices.\n+\n+Similarly, we can build on the previous optimization by also copying the data\n+from shared memory to private memory, and decomposing again arrays stored in\n+shared memory to compute the dot-product in private memory using even smaller\n+tiles.\n+Besides, we are going to increase the work to be done per-thread, so each GPU\n+thread will compute a sub-matrix (e.g., 4x4).\n+\n+This version is heavily inspired\n+by [Simon Boehm's](https:\/\/siboehm.com\/articles\/22\/CUDA-MMM) on optimizing\n+matrix multiplication, and it is equivalent to his version 5. If you are\n+interested into the details of this optimization, I highly recommend reading his\n+excellent blog.\n+\n+The key differences compared to the previous kernel is that we have two types of\n+`DeviceType` data structures: one for shared data structures to be allocated in\n+shared memory, and another one for allocating into the private memory of the\n+GPU.\n@@ -848,2 +1184,5 @@\n-- One for allocating a small 2D array in which we are going to perform the computation (dot-product).\n-- One dedicated to store partial results of each matrix from the shared memory space. \n+\n+- One for allocating a small 2D array in which we are going to perform the\n+  computation (dot-product).\n+- One dedicated to store partial results of each matrix from the shared memory\n+  space.\n@@ -904,2 +1243,2 @@\n-\n-The first part of the loop-tile we copy data from the GPU's global memory to the shared memory:\n+The first part of the loop-tile we copy data from the GPU's global memory to the\n+shared memory:\n@@ -925,1 +1264,2 @@\n-Then, within the register-tile, we compute load from shared memory to private and pre-compute perform the computation:\n+Then, within the register-tile, we compute load from shared memory to private\n+and pre-compute perform the computation:\n@@ -949,1 +1289,0 @@\n-\n@@ -961,3 +1300,2 @@\n-This version is more complicated due to indexing the right block and the right tile. \n-\n-Another key change is the number of global threads to be deployed. This changes because we are going to process more cells of the output matrix per thread.\n+This version is more complicated due to indexing the right block and the right\n+tile.\n@@ -965,0 +1303,2 @@\n+Another key change is the number of global threads to be deployed. This changes\n+because we are going to process more cells of the output matrix per thread.\n@@ -976,1 +1316,3 @@\n-Since we process blocks of $4x4$, each thread will process, in our version, 16 items. Thus, since our input size is 1024 elements, the new global size is reduced to $256x256$ threads.\n+Since we process blocks of $4x4$, each thread will process, in our version, 16\n+items. Thus, since our input size is 1024 elements, the new global size is\n+reduced to $256x256$ threads.\n@@ -998,1 +1340,2 @@\n-This kernel takes 373 microseconds! This is $5.8x$ faster than our previous version, and we are now below 1 ms. \n+This kernel takes 373 microseconds! This is $5.8x$ faster than our previous\n+version, and we are now below 1 ms.\n@@ -1001,2 +1344,5 @@\n-From the `ncu` report (source section of the profiler), we can also see that the memory throughput as well as the compute-throughput have decreased to 66% and 54% respectively. This is not necessarily a bad thing, as we just discussed, we increased the amount of work per-thread. But we will need to see other sections of the `ncu` report to check what we can do better.\n-\n+From the `ncu` report (source section of the profiler), we can also see that the\n+memory throughput as well as the compute-throughput have decreased to 66% and\n+54% respectively. This is not necessarily a bad thing, as we just discussed, we\n+increased the amount of work per-thread. But we will need to see other sections\n+of the `ncu` report to check what we can do better.\n@@ -1004,1 +1350,2 @@\n-Having a closer look at the `source` section of the profiler, it identifies the following warning in the SASS code:\n+Having a closer look at the `source` section of the profiler, it identifies the\n+following warning in the SASS code:\n@@ -1024,1 +1371,2 @@\n-What we can try to alleviate this by using vector types to load multiple elements at once accesses, using, for example, four floats per transaction.\n+What we can try to alleviate this by using vector types to load multiple\n+elements at once accesses, using, for example, four floats per transaction.\n@@ -1028,2 +1376,4 @@\n-This new version is also heavily inspired by Simon's blog. \n-HAT offers a set of vector types, such as `Float4` that can be used to load and store a set of float consecutive values in one transaction, as well as operate on float vectors. \n+This new version is also heavily inspired by Simon's blog.\n+HAT offers a set of vector types, such as `Float4` that can be used to load and\n+store a set of float consecutive values in one transaction, as well as operate\n+on float vectors.\n@@ -1031,1 +1381,2 @@\n-In addition, the main types, such as `F32Array`, contain a method called `float4View`, which indicates the HAT compiler to perform a `vload4` operation.\n+In addition, the main types, such as `F32Array`, contain a method called\n+`float4View`, which indicates the HAT compiler to perform a `vload4` operation.\n@@ -1049,2 +1400,6 @@\n-What we have done here is to create a view of the original `F32Array` that was not vectorized into a vector type element. \n-The resulting value from the `float4View` operation is stored in private memory of the GPU. Then we need to copy elements from that vector type to shared memory. We can access individual members of that vector by selecting the lane (`x` for the first lane, `y` for the second, and so on).\n+What we have done here is to create a view of the original `F32Array` that was\n+not vectorized into a vector type element.\n+The resulting value from the `float4View` operation is stored in private memory\n+of the GPU. Then we need to copy elements from that vector type to shared\n+memory. We can access individual members of that vector by selecting the lane (\n+`x` for the first lane, `y` for the second, and so on).\n@@ -1052,1 +1407,2 @@\n-These are the main changes for this new version of the kernel. Now, let's take a look at its performance. \n+These are the main changes for this new version of the kernel. Now, let's take a\n+look at its performance.\n@@ -1070,1 +1426,2 @@\n-As we can see, the kernel time is almost identical to the previous version. However, compute throughput has decreased from 54% to 43%. \n+As we can see, the kernel time is almost identical to the previous version.\n+However, compute throughput has decreased from 54% to 43%.\n@@ -1072,3 +1429,4 @@\n-We are now entering a territory in which every tiny transformation matters. And one of the overlook optimizations we could apply is to perform auto-tuning (this is the testing of the different scheduling parameters such as block size, number of blocks, register tile sizes, etc.).\n-\n-However, instead, we are going to explore one final optimization, that it is actually relevant to run Large Language Models (LLMs), and it is the ability to represent narrowed data types (e.g., half floating point number, aka `FP16`).\n+We are now entering a territory in which every tiny transformation matters. And\n+one of the overlook optimizations we could apply is to perform auto-tuning (this\n+is the testing of the different scheduling parameters such as block size, number\n+of blocks, register tile sizes, etc.).\n@@ -1076,0 +1434,3 @@\n+However, instead, we are going to explore one final optimization, that it is\n+actually relevant to run Large Language Models (LLMs), and it is the ability to\n+represent narrowed data types (e.g., half floating point number, aka `FP16`).\n@@ -1079,1 +1440,4 @@\n-I wouldn't call this an optimization of the previous kernel, unless we know we can process our algorithms with less precision data types.  However, this is an important feature these days with regard to LLMs that we are going to explore before we wrap up all optimizations. \n+I wouldn't call this an optimization of the previous kernel, unless we know we\n+can process our algorithms with less precision data types. However, this is an\n+important feature these days with regard to LLMs that we are going to explore\n+before we wrap up all optimizations.\n@@ -1081,1 +1445,1 @@\n-So, how we can express narrow data types in HAT? \n+So, how we can express narrow data types in HAT?\n@@ -1083,2 +1447,3 @@\n-HAT exposes `FP16` and array of `FP16` using `F16Array` data structure. \n-Similarly to the `Float4` data type, the `FP16` also contains a set of operations such as `add`, `mul`, etc. \n+HAT exposes `FP16` and array of `FP16` using `F16Array` data structure.\n+Similarly to the `Float4` data type, the `FP16` also contains a set of\n+operations such as `add`, `mul`, etc.\n@@ -1086,1 +1451,3 @@\n-Furthermore, the `FP16` type can be used to compose custom types to combine it with `DeviceTypes`, and use it to store values in shared memory and private memory of the GPU. \n+Furthermore, the `FP16` type can be used to compose custom types to combine it\n+with `DeviceTypes`, and use it to store values in shared memory and private\n+memory of the GPU.\n@@ -1088,1 +1455,4 @@\n-In the case of the matrix-multiplication, we use both, the `F16Array` that is used to pass as a parameter to the HAT kernel, and custom device types.\n+In the case of the matrix-multiplication, we use both, the `F16Array` that is\n+used to pass as a parameter to the HAT kernel, and custom device types.\n+We can declare arrays of type `F16` to be used as shared memory and private \n+memory as follows:\n@@ -1114,0 +1484,3 @@\n+Then, within the kernel, we can create the arrays for each each region as \n+follows:\n+\n@@ -1128,2 +1501,0 @@\n-- `FP16` needs hardware support to be able to take advantage of the faster computation.\n-- When using `FP16`, we transfer half the amount of data compared to `FP32` (e.g., Java `float`). Thus, even if the hardware does not support it, but emulates `FP16`, it is still possible to take advantage if we take into account data communications. This is something we have overlooked in this article, since we are focusing on GPU kernel optimizations. \n@@ -1131,1 +1502,10 @@\n-If we run this new version on the A10 GPU, we see the following profiling metrics:\n+- `FP16` needs hardware support to be able to take advantage of the faster\n+  computation.\n+- When using `FP16`, we transfer half the amount of data compared to `FP32`\n+  (e.g., Java `float`). Thus, even if the hardware does not support it, but\n+  emulates `FP16`, it is still possible to take advantage if we take into\n+  account data communications. This is something we have overlooked in this\n+  article, since we are focusing on GPU kernel optimizations.\n+\n+If we run this new version on the A10 GPU, we see the following profiling\n+metrics:\n@@ -1150,1 +1530,2 @@\n-This kernel takes 281 microseconds, this is an 33% improvement over the previous version. \n+This kernel takes 281 microseconds, this is an 33% improvement over the previous\n+version.\n@@ -1154,1 +1535,3 @@\n-[cuBLAS](https:\/\/developer.nvidia.com\/cublas) is a GPU-accelerated linear algebra library developed by NVIDIA for dispatching common Basic Linear Algebra Subprograms (BLAS) on NVIDIA GPUs.\n+[cuBLAS](https:\/\/developer.nvidia.com\/cublas) is a GPU-accelerated linear\n+algebra library developed by NVIDIA for dispatching common Basic Linear Algebra\n+Subprograms (BLAS) on NVIDIA GPUs.\n@@ -1156,1 +1539,2 @@\n-These GPU kernels have been highly optimized by NVIDIA for their hardware. So now the question is, close far, or how close is HAT comparing to cuBLAS? \n+These GPU kernels have been highly optimized by NVIDIA for their hardware. So\n+now the question is, close far, or how close is HAT comparing to cuBLAS?\n@@ -1174,7 +1558,16 @@\n-The GPU kernel takes 241 microseconds. This is $1.54x$ times better than our faster kernel in FP32, and $1.16x$ faster compared to our `FP16` kernel. \n-However, keep in mind that the cuBLAS version we are running does not use `FP16`, therefore, use this metrics just to put things into perspective, the fair-comparison should be against the FP32 kernel.  \n-\n-Another key observation is that the native cuBLAS kernel launches a kernel called `ampere_sgemm_128x64_nn`, as we see from the `ncu` profiler report, suggesting probably the block size of the selected kernel for the input data sizes. \n-cuBLAS library contains a bunch of pre-compiled GPU kernels with different block sizes and different implementations. \n-Then the library runtime selects the best fit for each GPU architecture and input sizes. \n-Thus, there is still room for improvements when it comes to GPU thread scheduling in HAT.\n+The GPU kernel takes 241 microseconds. This is $1.54x$ times better than our\n+faster kernel in FP32, and $1.16x$ faster compared to our `FP16` kernel.\n+However, keep in mind that the cuBLAS version we are running does not use\n+`FP16`, therefore, use this metrics just to put things into perspective, the\n+fair-comparison should be against the FP32 kernel.\n+\n+Another key observation is that the native cuBLAS kernel launches a kernel\n+called `ampere_sgemm_128x64_nn`, as we see from the `ncu` profiler report,\n+suggesting probably the block size of the selected kernel for the input data\n+sizes.\n+cuBLAS library contains a bunch of pre-compiled GPU kernels with different block\n+sizes and different implementations.\n+Then the library runtime selects the best fit for each GPU architecture and\n+input sizes.\n+Thus, there is still room for improvements when it comes to GPU thread\n+scheduling in HAT.\n@@ -1184,1 +1577,2 @@\n-The following plot summarizes the performance obtained for each of the GPU kernels expressed in HAT compared to cuBLAS.\n+The following plot summarizes the performance obtained for each of the GPU\n+kernels expressed in HAT compared to cuBLAS.\n@@ -1187,2 +1581,4 @@\n-The following performance graph shows the throughput (in number of floating point operations per seconds expressed in GFLOP\/s) for each of the presented implementations. The higher, the better. \n-As mentioned, we use the `ncu` profiler to obtain this data. \n+The following performance graph shows the throughput (in number of floating\n+point operations per seconds expressed in GFLOP\/s) for each of the presented\n+implementations. The higher, the better.\n+As mentioned, we use the `ncu` profiler to obtain this data.\n@@ -1194,3 +1590,7 @@\n-The `ncu` profiler, in order to deterministically compare different kernels, fixes the GPU frequency and controls the cache behavior. If we want just to measure the kernel time, we can use the CUDA Kernel events instead, as shown in the following performance graph.\n-In this case, caches are not flushed, and the control of the GPU frequency is left to the GPU driver. \n-From my perspective, this case represents a more real-world scenario. \n+The `ncu` profiler, in order to deterministically compare different kernels,\n+fixes the GPU frequency and controls the cache behavior. If we want just to\n+measure the kernel time, we can use the CUDA Kernel events instead, as shown in\n+the following performance graph.\n+In this case, caches are not flushed, and the control of the GPU frequency is\n+left to the GPU driver.\n+From my perspective, this case represents a more real-world scenario.\n@@ -1202,2 +1602,2 @@\n-We see higher performance compared to the previous graph: cuBLAS achieving 12.7 TFLOP\/s and HAT achieving 10.2 TFLOP\/s in `FP32`, and 14 TFLOP\/s in `FP16`. \n-\n+We see higher performance compared to the previous graph: cuBLAS achieving 12.7\n+TFLOP\/s and HAT achieving 10.2 TFLOP\/s in `FP32`, and 14 TFLOP\/s in `FP16`.\n@@ -1205,1 +1605,5 @@\n-The last performance graph shows the end-to-end speedups (including kernel and data transfers between the GPU and the CPU) of each version HAT version of the matrix multiplication compared to the Java parallel streams on CPU. We see that HAT is able to achieve $83x$ and $132x$ over parallel streams when using an optimized version for FP32 and FP16 data types respectively. \n+The last performance graph shows the end-to-end speedups (including kernel and\n+data transfers between the GPU and the CPU) of each version HAT version of the\n+matrix multiplication compared to the Java parallel streams on CPU. We see that\n+HAT is able to achieve $83x$ and $132x$ over parallel streams when using an\n+optimized version for FP32 and FP16 data types respectively.\n@@ -1213,1 +1617,2 @@\n-We have examined a set of common optimizations for matrix multiplication using HAT, but can we push these kernels further? The answer is yes.\n+We have examined a set of common optimizations for matrix multiplication using\n+HAT, but can we push these kernels further? The answer is yes.\n@@ -1215,1 +1620,3 @@\n-Although we have demonstrated eight specific kernels, HAT is actively evolving. Future iterations aim to unlock some other advanced techniques, such as leveraging Tensor Core operations, or applying double buffering techniques.\n+Although we have demonstrated eight specific kernels, HAT is actively evolving.\n+Future iterations aim to unlock some other advanced techniques, such as\n+leveraging Tensor Core operations, or applying double buffering techniques.\n@@ -1219,7 +1626,16 @@\n-Achieving peak performance from high-level programming languages remains a formidable challenge, as it needs the precise orchestration of multiple architectural factors, including thread-block selection, block tiling strategies, and the deployment of hardware-specific intrinsics, just to name a few.\n-Even with different GPU micro-archictecture generations and different vendors, all previous parameters can be different.\n-\n-In this article, we have focused on a single data point (data size) on a specific GPU to illustrate the mechanisms of code-reflection and HAT. \n-These results demonstrate that with the appropriate language constructs, compiler support, and runtime systems, it is possible to bridge the performance gap between high-level abstractions and native parallel code.\n-While performance portability and automated tuning remain open research questions, one of the most common techniques to achieve this is via auto-tuning, which could be included in future extensions of HAT.\n-\n+Achieving peak performance from high-level programming languages remains a\n+formidable challenge, as it needs the precise orchestration of multiple\n+architectural factors, including thread-block selection, block tiling\n+strategies, and the deployment of hardware-specific intrinsics, just to name a\n+few.\n+Even with different GPU micro-architecture generations and different vendors,\n+all previous parameters can be different.\n+\n+In this article, we have focused on a single data point (data size) on a\n+specific GPU to illustrate the mechanisms of code-reflection and HAT.\n+These results demonstrate that with the appropriate language constructs,\n+compiler support, and runtime systems, it is possible to bridge the performance\n+gap between high-level abstractions and native parallel code.\n+While performance portability and automated tuning remain open research\n+questions, one of the most common techniques to achieve this is via auto-tuning,\n+which could be included in future extensions of HAT.\n@@ -1229,9 +1645,25 @@\n-In this article we have explored how the project Babylon with its enhanced code reflection APIs along with HAT, a parallel programming framework for offloading Java programs onto modern such as GPUs can unlock GPU acceleration and achieve performance close to native CUDA implementations. \n-\n-By progressively optimizing the matrix-multiplication example from Java using HAT built-ins and HAT parallel constructs, we have shown how to apply advanced GPU optimizations such as 2D Local Ranges, loop tiling, shared memory, register tiling, vectorization and transformations to narrowed types such as FP16 in order to increase performance from single-digit GFLOP\/s to over 12 TFLOP\/s on GPUs. \n-\n-Furthermore, along the explanation for each optimization, this article explained some key components of the HAT programming model and how those components relate to the GPU architecture and hardware accelerators. Since HAT is in active development, parallel constructs and APIs might evolve to higher level-abstractions, with the goal of facilitating GPU and heterogeneous programming for Java developers. \n-\n-The innovations in Babylon and HAT illustrate how modern Java is evolving to address heterogeneous computing, bridging the gap between productivity and performance for foreign programming languages. \n-\n-For those interested in further exploration, I recommend the following list of resources. \n+In this article we have explored how the project Babylon with its enhanced code\n+reflection APIs along with HAT, a parallel programming framework for offloading\n+Java programs onto modern such as GPUs can unlock GPU acceleration and achieve\n+performance close to native CUDA implementations.\n+\n+By progressively optimizing the matrix-multiplication example from Java using\n+HAT built-ins and HAT parallel constructs, we have shown how to apply advanced\n+GPU optimizations such as 2D Local Ranges, loop tiling, shared memory, register\n+tiling, vectorization and transformations to narrowed types such as FP16 in\n+order to increase performance from single-digit GFLOP\/s to over 12 TFLOP\/s on\n+GPUs.\n+\n+Furthermore, along the explanation for each optimization, this article explained\n+some key components of the HAT programming model and how those components relate\n+to the GPU architecture and hardware accelerators. Since HAT is in active\n+development, parallel constructs and APIs might evolve to higher\n+level-abstractions, with the goal of facilitating GPU and heterogeneous\n+programming for Java developers.\n+\n+The innovations in Babylon and HAT illustrate how modern Java is evolving to\n+address heterogeneous computing, bridging the gap between productivity and\n+performance for foreign programming languages.\n+\n+For those interested in further exploration, I recommend the following list of\n+resources.\n","filename":"site\/articles\/hat-matmul.md","additions":756,"deletions":324,"binary":false,"changes":1080,"status":"modified"}]}