{"files":[{"patch":"@@ -808,3 +808,1 @@\n-In this article, we are not going to detail how the GPU programming and\n-execution models work, and how GPU threads accesses memory,\n-but, in a nutshell, GPU coalesced memory accesses occur when consecutive threads\n+In a nutshell, GPU coalesced memory accesses occur when consecutive threads\n@@ -818,1 +816,2 @@\n-To know more about GPU memory coalescing, I refer to the following blogs:\n+To know more about GPU memory coalescing and the GPU execution model, I refer \n+to the following articles:\n@@ -824,2 +823,2 @@\n-not optimal:\n-This refers to this line of the Java code:\n+not optimal. This refers to this line of the Java code of our \n+matrix-multiplication:\n@@ -832,9 +831,19 @@\n-We are using the thread-index `kc.gix` to access, what we pretended, \n-contiguous elements of the the same row of the matrix (row-major).\n-HAT maps the parallel construct `kc.gix` to an equivalent parallel construct\n-of the underlying programming model, and our case, CUDA. \n-When we have 2D matrices, or multi-dimensional arrays for that matter, \n-this indexing is not how the CUDA thread mapping works. As mentioned in\n-the [NVIDIA CUDA blogs](https:\/\/developer.nvidia.com\/blog\/unlock-gpu-performance-global-memory-access-in-cuda\/):\n-\n-> When using 2 or 3-dimensional thread blocks in a CUDA kernel, the threads are\n+The performance of matrix operations is heavily dictated by the memory layout,\n+and specifically whether elements are stored in \n+[row-major or column-major order](https:\/\/en.wikipedia.org\/wiki\/Row-_and_column-major_order).\n+Programming languages like CUDA and OpenCL uses row-major memory layout to \n+store n-dimensional arrays (e.g., a matrix) in memory.\n+Thus, in HAT, we have also used the `F32Array` array-type to represent a \n+2D matrix into a flatten 1D array using row-major layout.\n+To maximize GPU performance, we must ensure that memory access patterns \n+are also coalesced, meaning the iteration over elements should align with \n+the thread-id.\n+\n+In our example we are using the global thread-id `kc.gix` \n+(thread-id in the first dimension), and `kc.giy` (thread-id in the second \n+dimension) to map and access the data elements of each matrix. \n+However, [following the NVIDIA documentation](https:\/\/developer.nvidia.com\/blog\/unlock-gpu-performance-global-memory-access-in-cuda\/),\n+the way we mapped thread-ids to access data results in not coalesced memory \n+accesses. The reason is as follows:\n+\n+> \"When using 2 or 3-dimensional thread blocks in a CUDA kernel, the threads are\n@@ -842,1 +851,1 @@\n-> moving the fastest.\n+> moving the fastest.\"\n@@ -845,2 +854,2 @@\n-within a block). In HAT, this is the equivalent of `kc.lix`. But\n-our our purpose here, it is also equivalent to our global thread-index (`kc.gix`).\n+within a block). In HAT, this is the equivalent of `kc.lix`. But our purpose \n+here, it is also equivalent to our global thread-id (`kc.gix`).\n@@ -848,3 +857,3 @@\n-But, what does this mean in practice? For instance, if we have a $4x4$ thread\n-block, the way consecutive threads using `kc.gix, kc.giy` are \n-organized are as follows:\n+What does this mean in practice? For instance, if we have a $4x4$ thread\n+block, the way consecutive threads using a 2D configuration (`kc.gix, kc.giy`) \n+is mapped are as follows:\n@@ -856,9 +865,7 @@\n-As mentioned in the CUDA documentation, since the first index moves faster,\n-the `kc.gix` index moves faster in HAT, and we are using this parallel built-in\n-to access the rows of `matrixA`.\n-\n-That means that, for each consecutive threads in the first dimension `kc.gix`, \n-(gix, gix+1), we are accessing the matrices in column-major, instead of \n-row-major. Thus, for matrix A of our example, \n-the following indexing applies for consecutive threads based on the \n-previous kernel:\n+If we map this to the HAT terminology, since the global-id in the first \n+dimension moves faster in a 2D-range, this means that the `kc.gix` id moves \n+faster. Thus, for this expression:\n+```java\n+matrixA.array(kc.gix * size + k)\n+```\n+The way memory is accessed is as follows:\n@@ -867,1 +874,1 @@\n-<img src=\".\/images\/hat-matmul\/uncoalesced.png\" width=\"600\"\/>\n+<img src=\".\/images\/hat-matmul\/uncoalesced.png\" width=\"420\"\/>\n@@ -876,2 +883,6 @@\n-To achieve this in our example in HAT, we swap the indexes between `kc.gix` and\n-`kc.giy`.\n+In the case of HAT, to achieve optimal memory coalescing, the mapping must\n+ensure that threads with consecutive `kc.gix` values within a warp access \n+consecutive memory addresses. By assigning threads with the same `kc.giy` to \n+a single row, the access pattern aligns with the matrixâ€™s row-major layout.\n+\n+To achieve this, we simply swap the indexes between `kc.gix` and `kc.giy`.\n@@ -881,0 +892,3 @@\n+Note that `matrixB` is still not coalesced, but we will address this when we \n+introduce shared memory.\n+\n","filename":"site\/articles\/hat-matmul.md","additions":47,"deletions":33,"binary":false,"changes":80,"status":"modified"}]}