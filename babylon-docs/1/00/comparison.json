{"files":[{"patch":"@@ -47,0 +47,4 @@\n+## Documents\n+  - Articles\n+    - [Automatic differentiation of Java code using Code Reflection](articles\/auto-diff.md) (February 2024)\n+    \n","filename":"site\/_index.md","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,916 @@\n+# Automatic differentiation of Java code using Code Reflection\n+#### Paul Sandoz {.author}\n+#### February 2024 {.date}\n+\n+In this article we will explain what is automatic differentiation, why it is\n+useful, and how we can use Code Reflection to help implement automatic\n+differentiation of Java methods.\n+\n+Code Reflection is a Java platform feature being researched and developed under \n+OpenJDK Project [Babylon](https:\/\/openjdk.org\/projects\/babylon\/).\n+\n+We will introduce Code Reflection concepts and APIs as we explain the problem\n+and present a solution. The explanations are neither exhaustive nor very\n+detailed, they are designed to give the reader an intuitive sense and\n+understanding of Code Reflection and its capabilities.\n+\n+## Machine learning\n+\n+Let's consider the field of machine learning, which is the science of producing\n+models that can predict what number is present in an image of a numerical digit,\n+or predicts what text a person is saying in an audio recording of human speech.\n+\n+A machine learning model is really a mathematical function, `f` say (albeit a\n+possibly complex one with many inputs, outputs, and constants or parameters).\n+\n+For a model to be effective it needs be trained, and many models are trained\n+using a *gradient descent algorithm*. This algorithm repeatedly applies known\n+inputs to `f`, comparing the computed output of `f` with expected output,\n+adjusting the parameters of `f`, until the difference between the computed\n+output and expected output, the error, is sufficiently reduced. The algorithm\n+adjusts the model by a gradient vector that is the result of applying the error\n+to the *differentiation* of `f`, `f'`\n+\n+Machine learning developers implement such models using computer code, which\n+requires an implementation of `f` **and** its differential, `f'`, be available\n+for execution.\n+\n+Consider the following method `f` implementing some mathematical function:\n+\n+```\n+f(x, y) = x * (-sin(x * y) + y) * 4\n+```\n+\n+The partial derivatives of `f` with respect to the input (or independent\n+variable) `x` can be manually calculated and written:\n+\n+```jshelllanguage\n+df_dx(x, y) = (-sin(x * y) + y - x * cos(x * y) * y) * 4\n+```\n+\n+(The partial derivative of `f` with respect to `y` is shown later as Java code.)\n+\n+From the partial derivatives a gradient vector can be produced by combining the\n+results of calling partial derivative methods.\n+\n+## Automatic differentiation\n+\n+Manual differentiation is a very error-prone process. Although differentiation\n+is a mechanical process it's easy to make mistakes that are hard to debug, even\n+for a simple mathematical function as presented above, and the process quickly\n+becomes arduous as the mathematical function becomes more complex. This is\n+especially so for machine learning models. Computers are ideally suited to this\n+mechanical task.\n+\n+To automatically differentiate a mathematical function written as a Java\n+method, `f` say, we need to write a Java program, `D` say, that implements the\n+rules of differentiation and applies those rules to a *symbolic representation*\n+of `f` to produce the differential method `f'`.\n+\n+Program `D` can use *Code Reflection* to obtain a symbolic representation of\n+method `f`, called a *code model*. `D` can then traverse symbolic information\n+in `f`'s code model, *operations*, and apply the rules of differentiation to\n+those operations. For example, operations may be mathematical operations,\n+representing addition or multiplication, or invocation operations representing\n+invocations to Java methods that implement transcendental functions (such as\n+method `java.lang.Math::sin`).\n+\n+`D` will produce a new code model, representing `f`', containing operations that\n+compute the differential, which can then be compiled to bytecode and invoked as\n+a Java program.\n+\n+There are two approaches to automatic differentiation, forward-mode and\n+reverse-mode. For `N` independent variables, forward-mode automatic\n+differentiation requires the generation of `N` partial derivative methods, and\n+therefore `N` method calls to produce a gradient. Reverse-mode automatic\n+differentiation does not have these limitations, although it may be less\n+efficient for fewer independent variables.\n+\n+Program `D` could be encapsulated in a Java library. We might ideally use it\n+like this:\n+\n+```java\n+\n+@CodeReflection\n+double f(double x, double y) {\n+    return x * (-sin(x * y) + y) * 4;\n+}\n+\n+Function<double[], double[]> g_f = AD.gradientFunction(this::f);\n+double[] g = g_f.apply(new double[]{x, y});\n+```\n+\n+We annotate our function to be differentiated, method `f`,\n+with `@CodeReflection`. This ensures there is a code model available for `f`,\n+and that it is accessible under similar access control rules as for its\n+invocation . Then we call the method `AD.gradientFunction` passing `f` as a\n+method reference. The method reference is targeted to a code reflection type\n+whose instance gives access to `f`'s code model.\n+\n+How can the library author of method `gradientFunction` differentiate\n+method `f`?\n+\n+## Implementing forward-mode automatic differentiation\n+\n+In the following sections we will explain how to implement\n+the `gradientFunction` method using Code Reflection. We will focus on\n+forward-mode automatic differentiation since that is easier to understand, but\n+the same general principles could apply to reverse-mode automatic\n+differentiation.\n+\n+A proof of concept implementation is available as a [test][ad-test] located in\n+the Babylon repository. The implementation is far from complete, and is just one\n+of many possible ways to approach the problem.\n+\n+[ad-test]:https:\/\/github.com\/openjdk\/babylon\/tree\/code-reflection\/test\/jdk\/java\/lang\/reflect\/code\/ad\n+\n+## Differentiating simple functions\n+\n+Let's focus on the simple mathematical function we presented earlier. It has two\n+independent variables, `x` and `y`.\n+\n+```java\n+\n+@CodeReflection\n+static double f(double x, double y) {\n+    return x * (-Math.sin(x * y) + y) * 4.0d;\n+}\n+```\n+\n+It is annotated it with `@CodeReflection`. When it is compiled a code model will\n+be generated and made accessible at run time via reflection.\n+\n+We can also write, by hand, the partial derivatives of `f` with respect to `x`\n+and `y` so we can test against what we generate.\n+\n+```java\n+static double df_dx(double x, double y) {\n+    return (-Math.sin(x * y) + y - x * Math.cos(x * y) * y) * 4.0d;\n+}\n+\n+static double df_dy(double x, double y) {\n+    return x * (1 - Math.cos(x * y) * x) * 4.0d;\n+}\n+```\n+\n+### Obtaining a code model\n+\n+Fundamentally, we need to implement a method that accepts a code model and a\n+reference to an independent variable to differentiate against, and produces a\n+new code model that is the partial derivative of the input. We will focus on\n+this aspect, although it is not as user-friendly as the prior example (\n+using program `D`, where we can observe the author only minimally used the Code\n+Reflection API to annotate their method).\n+\n+First let's assume `f` is declared as a static method in a class `T`, say. To\n+obtain its code model using reflection we would do this.\n+\n+```java\n+Method fm = T.class.getDeclaredMethod(\"f\", double.class, double.class);\n+Optional<CoreOps.FuncOp> o = fm.getCodeModel();\n+CoreOps.FuncOp fcm = o.orElseThrow();\n+```\n+\n+Using the reflection API we find the `java.lang.reflect.Method` instance of `f`,\n+and then ask it for its code model by invoking the method `getCodeModel`. Only\n+methods annotated with `@CodeReflection` will have code models, hence this\n+method is partial.\n+\n+`f`'s code model is represented as an instance of `CoreOps.FuncOp`,\n+corresponding to a *function declaration* operation that models a Java method\n+declaration.\n+\n+### Explaining the code model\n+\n+A code model is a tree containing operations, bodies, and blocks. An operation\n+contains zero or more bodies. A body contains one or more blocks. A block\n+contains a sequence of one or more operations. A block can declare zero or more\n+block parameters, values. An operation declares an operation result, a value. An\n+operation may use values as operands, but only after they have been declared.\n+\n+Using this simple tree structure we can define operations that model many Java\n+language constructs, and therefore we can build code models that model many Java\n+programs. This may appear surprising at first. Readers may be more familiar with\n+term \"operation\" in a more conventional sense, such as arithmetic operations.\n+However, given the structure described above, there is no need to limit\n+ourselves to this conventional sense. We are free to define an operation whose\n+operational semantics *declare* a function (instances of `CoreOps.FuncOp`),\n+model a Java lambda expression (instances of `CoreOps.LambdaOp`), or model a\n+Java `try` statement (instances of `ExtendedOps.JavaTryOp`).\n+\n+What does the code model of `f` look like? We can serialize its in-memory form (\n+the instance of `CoreOps.FuncOp`) to a textual form.\n+\n+```java\n+System.out.println(fcm.toText());\n+```\n+\n+Which prints the following text.\n+\n+```text\n+func @\"f\" (%0 : double, %1 : double)double -> {\n+    %2 : Var<double> = var %0 @\"x\";\n+    %3 : Var<double> = var %1 @\"y\";\n+    %4 : double = var.load %2;\n+    %5 : double = var.load %2;\n+    %6 : double = var.load %3;\n+    %7 : double = mul %5 %6;\n+    %8 : double = invoke %7 @\"java.lang.Math::sin(double)double\";\n+    %9 : double = neg %8;\n+    %10 : double = var.load %3;\n+    %11 : double = add %9 %10;\n+    %12 : double = mul %4 %11;\n+    %13 : double = constant @\"4.0\";\n+    %14 : double = mul %12 %13;\n+    return %14;\n+};\n+```\n+\n+The textual form shows the code model's root is a function declaration (`func`)\n+operation. The function declaration operation has an operation result, like all\n+other operations, but since it's the root of the tree there is no need to\n+present it.\n+\n+The lambda-like expression represents the fusion of the function declaration\n+operation's single body and the body's first and only block, called the entry\n+block. Then there is a sequence of operations in the entry block. For each\n+operation there is an instance of a corresponding class present in the in-memory\n+form, all of which extend from the abstract class `java.lang.reflect.code.Op`.\n+\n+The entry block has two block parameters, `%0` and `%1` (corresponding to `x`\n+and `y`), each described by a type of `double`, which model `f`'s method\n+parameters. These parameters are used as operands of various operations. Many\n+operations produce operation results, e.g., `%12` the result of a multiplication\n+operation, that are used as operands of subsequent operations, and so on.\n+The `return` operation has a result, again like all other operations, but since\n+that result cannot be meaningfully used we don't present it.\n+\n+Code models have the property of Static Single-Assignment (SSA). We refer to\n+variables that can only be assigned once as values (they are a bit like final\n+variables in Java) .e.g., value `%12` can never be modified. A variable\n+declaration is modeled as an operation that produces a value that holds a\n+value (a box), and access operations load or store to that box.\n+\n+(Some readers may be thinking this looks very similar\n+to [MLIR](https:\/\/mlir.llvm.org\/) and that is by design.)\n+\n+We can see how the operations model Java language constructs like method\n+declarations, variables (method parameters or local variables) and access of\n+variables, binary and unary mathematical operations, or method invocations (\n+e.g., to method `java.lang.Math::sin`).\n+\n+### Analyzing the model\n+\n+We can simplify this model by transforming it into one that removes the variable\n+declarations and accesses. We call this a pure SSA transform.\n+\n+```jshelllanguage\n+fcm = SSA.transform(fcm);\n+```\n+\n+The textual form of the resulting code model is as follows.\n+\n+```text\n+func @\"f\" (%0 : double, %1 : double)double -> {\n+    %2 : double = mul %0 %1;\n+    %3 : double = invoke %2 @\"java.lang.Math::sin(double)double\";\n+    %4 : double = neg %3;\n+    %5 : double = add %4 %1;\n+    %6 : double = mul %0 %5;\n+    %7 : double = constant @\"4.0\";\n+    %8 : double = mul %6 %7;\n+    return %8;\n+};\n+```\n+\n+This is a simpler model, where program meaning is preserved. Since the model is\n+simpler it becomes simpler to analyze in preparation for automatic\n+differentiation.\n+\n+When differentiating with respect to an independent variable we will analyze the\n+model with respect to that variable and compute an *active set* of values, those\n+which depend transitively on the independent variable.\n+\n+For example the block parameter `%0` (representing the independent variable `x`)\n+is used as an operand by the operation that produces the operation result `%2` (\n+the result of a multiplication), and so on.\n+\n+The active set for `%0` is `{%0, %2, %3, %4, %5, %6, %8, %9}`. Note the\n+value `%9` represents the result of the `return` operation, whose type\n+is `void`, and whose value is not explicitly shown in the textual form.\n+\n+The active set for `%1` (representing the independent variable `y`) is the same\n+in this case.\n+\n+We can compute this set by traversing the *uses* of values in the code model.\n+The in-memory representation of a code model is constructed such that the uses\n+are easily available.\n+\n+A simple and naive implementation might be as follows.\n+\n+```java\n+static Set<Value> activeSet(Value root) {\n+    Set<Value> s = new LinkedHashSet<>();\n+    activeSet(s, root);\n+    return s;\n+}\n+\n+static void activeSet(Set<Value> s, Value v) {\n+    s.add(v);\n+    \/\/ Iterate over the uses of v\n+    for (Op.Result use : v.uses()) {\n+        activeSet(s, use);\n+    }\n+}\n+```\n+\n+In practice the active set computation needs to be a little more complex, as\n+implemented in the test code and used later in an example that has mathematical\n+expressions embedded within some control flow.\n+\n+### Reporting programming errors\n+\n+In general not all Java programs are differentiable, it's a constrained\n+programming model. The author of a Java method, to be differentiated, needs to\n+be aware of the constraints, and the automatic differentiation program should\n+report programming errors.\n+\n+After computing the active set (or during that computation) we could perform\n+checks and report errors if values cannot be differentiated or if the\n+independent value does not contribute to a result etc. Further, we might want to\n+do additional checks on the model and fail if unsupported language constructs\n+are present e.g., perhaps try statements, or ignore other constructs. The test\n+code does not perform such extensive checks.\n+\n+Ideally the reporting of such errors would occur when the method is compiled by\n+the source compiler rather than at runtime. Code reflection can also make\n+available the same code model at compile time for such purposes, but we will not\n+explore this capability in this article.\n+\n+### Differentiating a code model\n+\n+Once we have the active set we can use it to drive differentiation of the code\n+model. The operation results in the active set refer to the operations we need\n+to differentiate.\n+\n+We can *transform* the code model, applying the rules of differentiation to the\n+required operations we encounter.\n+\n+Code models are immutable. Code models can be produced by building, or by\n+transforming an existing code model. Transforming takes an input code model and\n+builds an output code model. For each input operation encountered in the input\n+code model we have the choice to add that operation to the builder of the output\n+code model (copying), to not add it (removal), or add new output operations\n+(replacement or addition).\n+\n+First lets declare a class, `ForwardDifferentiation`, whose constructor computes\n+the active set.\n+\n+```java\n+import static java.lang.reflect.code.op.CoreOps.*;\n+\n+public final class ForwardDifferentiation {\n+    \/\/ The function to differentiate\n+    final FuncOp fcm;\n+    \/\/ The independent variable\n+    final Block.Parameter ind;\n+    \/\/ The active set for the independent variable\n+    final Set<Value> activeSet;\n+    \/\/ The map of input value to it's (output) differentiated value\n+    final Map<Value, Value> diffValueMapping;\n+\n+    \/\/ The constant value 0.0d\n+    \/\/ Declared in the (output) function's entry block\n+    Value zero;\n+\n+    private ForwardDifferentiation(FuncOp fcm, Block.Parameter ind) {\n+        int indI = f.body().entryBlock().parameters().indexOf(ind);\n+        if (indI == -1) {\n+            throw new IllegalArgumentException(\"Independent argument not defined by function\");\n+        }\n+        this.fcm = fcm;\n+        this.ind = ind;\n+\n+        \/\/ Calculate the active set of dependent values for the independent value\n+        this.activeSet = ActiveSet.activeSet(f, ind);\n+        \/\/ A mapping of input values to their (output) differentiated values\n+        this.diffValueMapping = new HashMap<>();\n+    }\n+}\n+```\n+\n+A map from input operation results to (output) differentiated\n+values, `diffValueMapping` is also constructed. This will be used to keep track\n+of differentiated values that may be used in dependent computations. Because the\n+code model is immutable there is no danger that values referred to in the input\n+code model will become stale or be modified.\n+\n+Next we declare a static factory method on `ForwardDifferentiation` to compute\n+the partial derivative:\n+\n+```java\n+public static FuncOp partialDiff(FuncOp fcm, Block.Parameter ind) {\n+    return new ForwardDifferentiation(fcm, ind).partialDiff();\n+}\n+```\n+\n+This static method constructs an instance of `ForwardDifferentiation` and calls\n+the method `partialDiff`, which performs the transformation.\n+\n+```java\n+FuncOp partialDiff() {\n+    int indI = fcm.body().entryBlock().parameters().indexOf(ind);\n+\n+    \/\/ Transform f to f' w.r.t ind\n+    AtomicBoolean first = new AtomicBoolean(true);\n+    FuncOp dfcm = f.transform(STR.\"d\\{f.funcName()}_darg\\{indI}\",\n+            (block, op) -> {\n+                \/\/ Initialization\n+                if (first.getAndSet(false)) {\n+                    \/\/ Declare the zero value constant\n+                    zero = block.op(constant(ind.type(), 0.0d));\n+                    \/\/ Declare the one value constant\n+                    Value one = block.op(constant(ind.type(), 1.0d));\n+                    \/\/ The differential of ind is one\n+                    \/\/ For all other parameters it is zero (absence from the map)\n+                    diffValueMapping.put(ind, one);\n+                }\n+\n+                \/\/ If the result of the operation is in the active set,\n+                \/\/ then differentiate it, otherwise copy it\n+                if (activeSet.contains(op.result())) {\n+                    Value dor = diffOp(block, op);\n+                    \/\/ Map the input result to its (output) differentiated result\n+                    \/\/ so that it can be used when differentiating subsequent operations\n+                    diffValueMapping.put(op.result(), dor);\n+                } else {\n+                    \/\/ Block is not part of the active set, just copy it\n+                    block.op(op);\n+                }\n+                return block;\n+            });\n+\n+    return dfcm;\n+}\n+```\n+\n+We transform the code model using the `FuncOp` operation's `transform` method.\n+It accepts a name for the function of the output code model, and a lambda\n+expression that accepts an (output) block builder and an input operation. The\n+transform method will traverse all operations in the input code model and report\n+encountered input operations to the lambda expression.\n+\n+On first encounter we declare a few constant values in the output model by\n+adding constant operations, instances of `ConstantOp`, to the output model.\n+Specifically, we declare the zero constant value of `0.0d`, which will be used\n+as an operand of subsequent operations we add to output model.\n+\n+On each encounter, if the operation's result is a member of the active set then\n+we differentiate it, and map the (input) result to its (output) differential\n+value. We apply the chain rule from inside to outside.\n+\n+The method `diffOp` applies the rules of differentiation. The method consists of\n+a switch expression with pattern matching whose target is the instance of the\n+operation to differentiate. We show below a subset of interesting switch cases\n+from that method.\n+\n+```java\n+Value diffOp(Block.Builder block, Op op) {\n+    \/\/ Switch on the op, using pattern matching\n+    return switch (op) {\n+        case ... ->{\n+        }\n+        case CoreOps.MulOp _ -> {\n+            \/\/ Copy input operation\n+            block.op(op);\n+\n+            \/\/ Product rule\n+            \/\/ diff(l) * r + l * diff(r)\n+            Value lhs = op.operands().get(0);\n+            Value rhs = op.operands().get(1);\n+            Value dlhs = diffValueMapping.getOrDefault(lhs, zero);\n+            Value drhs = diffValueMapping.getOrDefault(rhs, zero);\n+            Value outputLhs = block.context().getValue(lhs);\n+            Value outputRhs = block.context().getValue(rhs);\n+            yield block.op(add(\n+                    block.op(mul(dlhs, outputRhs)),\n+                    block.op(mul(outputLhs, drhs))));\n+        }\n+        case CoreOps.InvokeOp c -> {\n+            MethodDesc md = c.invokeDescriptor();\n+            String operationName = null;\n+            if (md.refType().equals(J_L_MATH)) {\n+                operationName = md.name();\n+            }\n+            \/\/ Differentiate sin(x)\n+            if (\"sin\".equals(operationName)) {\n+                \/\/ Copy input operation\n+                block.op(op);\n+\n+                \/\/ Chain rule\n+                \/\/ cos(expr) * diff(expr)\n+                Value a = op.operands().get(0);\n+                Value da = diffValueMapping.getOrDefault(a, zero);\n+                Value outputA = block.context().getValue(a);\n+                Op.Result cosx = block.op(invoke(J_L_MATH_COS, outputA));\n+                yield block.op(mul(cosx, da));\n+            } else {\n+                throw new UnsupportedOperationException(\"Operation not supported: \" + op.opName());\n+            }\n+        }\n+    };\n+}\n+```\n+\n+The first case shows how we compute the differential of a multiply operation, an\n+instance of `MulOp`, by applying the product rule.\n+\n+We add the input multiply operation to the builder, which copies it from the\n+input model to the output model, since its result might be used in further\n+computations added to the output model.\n+\n+Given the input values for the first (left) and second (right) operands of the\n+multiply operation we obtain their differentiated values, which must have been\n+computed earlier, or if not then they must be zero. Then we add two new multiply\n+operations corresponding to the product rule. To do that we also need to obtain\n+the output values that map to the input values of the first and second\n+operands (again these must have been computed earlier when copying prior input\n+operations). Finally, we sum the results of the multiplications with an add\n+operation and yield the result.\n+\n+The second cases shows how we compute the differential of `sin(x)` (which\n+is `cos(x)x'`). We match on an invoke operation, an instance of `InvokeOp`, that\n+invokes the method `java.lang.Math::sin`. We copy the invocation operation, add\n+an invocation operation to invoke `java.lang.Math::cos`, then add a multiply\n+operation, the result of which is yielded.\n+\n+We anticipate patterns and switches will be used for many kinds transformation\n+and are designing the code reflection APIs with this in mind. We are looking\n+forward to future language capabilities where we can write our own patterns,\n+which will enable more sophisticated tree-based matching of operations (\n+including on their uses or their operands).\n+\n+Let's piece it all together and print out the textual form of the differentiated\n+code model.\n+\n+```java\n+import ad.ForwardDifferentiation;\n+\n+Method fm = T.class.getDeclaredMethod(\"f\", double.class, double.class);\n+Optional<CoreOps.FuncOp> o = fm.getCodeModel();\n+CoreOps.FuncOp fcm = SSA.transform(o.orElseThrow());\n+Block.Parameter x = fcm.body().entryBlock().parameters().get(0);\n+\/\/ Code model in, code model out\n+CoreOps.FuncOp dfcm_x = ForwardDifferentiation.partialDiff(fcm, x);\n+```\n+\n+```text\n+func @\"df_darg0\" (%0 : double, %1 : double)double -> {\n+    %2 : double = constant @\"0.0\";\n+    %3 : double = constant @\"1.0\";\n+    %4 : double = mul %0 %1;\n+    %5 : double = mul %3 %1;\n+    %6 : double = mul %0 %2;\n+    %7 : double = add %5 %6;\n+    %8 : double = invoke %4 @\"java.lang.Math::sin(double)double\";\n+    %9 : double = invoke %4 @\"java.lang.Math::cos(double)double\";\n+    %10 : double = mul %9 %7;\n+    %11 : double = neg %8;\n+    %12 : double = neg %10;\n+    %13 : double = add %11 %1;\n+    %14 : double = add %12 %2;\n+    %15 : double = mul %0 %13;\n+    %16 : double = mul %3 %13;\n+    %17 : double = mul %0 %14;\n+    %18 : double = add %16 %17;\n+    %19 : double = constant @\"4.0\";\n+    %20 : double = mul %15 %19;\n+    %21 : double = mul %18 %19;\n+    %22 : double = mul %15 %2;\n+    %23 : double = add %21 %22;\n+    return %23;\n+};\n+```\n+\n+and compare against the handwritten Java code:\n+\n+```java\n+static double df_dx(double x, double y) {\n+    return (-Math.sin(x * y) + y - x * Math.cos(x * y) * y) * 4.0d;\n+}\n+```\n+\n+We can immediately see the differentiated code model has more mathematical\n+operations, many are redundant e.g., there are multiplications by 0 or 1, and\n+the result of one operation (`%20`) is not used.\n+\n+We need to transform the code model further to remove redundant operations (\n+commonly referred to as expression elimination). The resulting code model with\n+eliminated expressions is show below.\n+\n+(We will not get into the specifics of how expression elimination is\n+implemented. It uses the same code reflection APIs and similar techniques we\n+have shown above, and the curious reader may be interested in looking at the\n+code.)\n+\n+```text\n+func @\"df_darg0\" (%0 : double, %1 : double)double -> {\n+    %2 : double = constant @\"0.0\";\n+    %3 : double = mul %0 %1;\n+    %4 : double = add %1 %2;\n+    %5 : double = invoke %3 @\"java.lang.Math::sin(double)double\";\n+    %6 : double = invoke %3 @\"java.lang.Math::cos(double)double\";\n+    %7 : double = mul %6 %4;\n+    %8 : double = sub %1 %5;\n+    %9 : double = sub %2 %7;\n+    %10 : double = mul %0 %9;\n+    %11 : double = add %8 %10;\n+    %12 : double = constant @\"4.0\";\n+    %13 : double = mul %11 %12;\n+    %14 : double = add %13 %2;\n+    return %14;\n+};\n+```\n+\n+We could eliminate some expressions in the differentiation transformation, but\n+this will complicate the transformation. Often it is better to keep\n+transformations focused, and separate into two or more transformation stages, at\n+the potential expense of more work.\n+\n+From this code model we can interpret it or translate to bytecode, and execute\n+it by invoking a method handle.\n+\n+If we apply the same to the independent variable `y` we will get two code\n+models, from which we can compute the gradient, and therefore implement the\n+`gradientFunction` method.\n+\n+## Differentiating models with control flow\n+\n+In the prior example we differentiated a Java method implementing a simple\n+mathematical expression. In this section we will consider a more complex method\n+that contains mathematical expressions embedded in control flow statements.\n+\n+```java\n+\n+@CodeReflection\n+static double f(\/* independent *\/ double x, int y) {\n+    \/* dependent *\/\n+    double o = 1.0;\n+    for (int i = 0; i < y; i = i + 1) {\n+        if (i > 1) {\n+            if (i < 5) {\n+                o = o * x;\n+            }\n+        }\n+    }\n+    return o;\n+}\n+```\n+\n+Method `f` has one independent variable, `x`. The parameter `y` indirectly\n+affects computations using `x`, but there is no direct data dependency between\n+`x` and `y`. We can see a multiplication operation embedded within a loop and\n+some conditional control flow. This method can be differentiating using the same\n+techniques we have presented.\n+\n+Here is the hand-coded version.\n+\n+```java\n+static double df_dx(\/* independent *\/ double x, int y) {\n+    double d_o = 0.0;\n+    double o = 1.0;\n+    for (int i = 0; i < y; i = i + 1) {\n+        if (i > 1) {\n+            if (i < 5) {\n+                d_o = d_o * x + o * 1.0;\n+                o = o * x;\n+            }\n+        }\n+    }\n+    return d_o;\n+}\n+```\n+\n+Notice the product rule applied to the multiplication of `o * x`, and in source\n+we have to be careful that this computation is applied before `o` is updated.\n+\n+(Astute readers may observe that and `o` and `d_o`, and `x` and `1`, correspond\n+to the two components of\n+a [dual number](https:\/\/en.wikipedia.org\/wiki\/Dual_number#Differentiation).)\n+\n+`f`'s code model will contain operations modeling the `for` loop and `if`\n+statements, retaining the structure of the code.\n+\n+```text\n+func @\"f\" (%0 : double, %1 : int)double -> {\n+    %2 : Var<double> = var %0 @\"x\";\n+    %3 : Var<int> = var %1 @\"y\";\n+    %4 : double = constant @\"1.0\";\n+    %5 : Var<double> = var %4 @\"o\";\n+    java.for\n+        ()Var<int> -> {\n+            %6 : int = constant @\"0\";\n+            %7 : Var<int> = var %6 @\"i\";\n+            yield %7;\n+        }\n+        (%8 : Var<int>)boolean -> {\n+            %9 : int = var.load %8;\n+            %10 : int = var.load %3;\n+            %11 : boolean = lt %9 %10;\n+            yield %11;\n+        }\n+        (%12 : Var<int>)void -> {\n+            %13 : int = var.load %12;\n+            %14 : int = constant @\"1\";\n+            %15 : int = add %13 %14;\n+            var.store %12 %15;\n+            yield;\n+        }\n+        (%16 : Var<int>)void -> {\n+            java.if\n+                ()boolean -> {\n+                    %17 : int = var.load %16;\n+                    %18 : int = constant @\"1\";\n+                    %19 : boolean = gt %17 %18;\n+                    yield %19;\n+                }\n+                ()void -> {\n+                    java.if\n+                        ()boolean -> {\n+                            %20 : int = var.load %16;\n+                            %21 : int = constant @\"5\";\n+                            %22 : boolean = lt %20 %21;\n+                            yield %22;\n+                        }\n+                        ()void -> {\n+                            %23 : double = var.load %5;\n+                            %24 : double = var.load %2;\n+                            %25 : double = mul %23 %24;\n+                            var.store %5 %25;\n+                            yield;\n+                        }\n+                        ()void -> {\n+                            yield;\n+                        };\n+                    yield;\n+                }\n+                ()void -> {\n+                    yield;\n+                };\n+            java.continue;\n+        };\n+    %26 : double = var.load %5;\n+    return %26;\n+};\n+```\n+\n+We can clearly see that some operations contain bodies at many levels. In this\n+case each body has one (entry) block as in the prior example. The four bodies in\n+the `for` operation correspond to the nested expressions and statements as\n+specified by the Java Language Specification.\n+\n+Rather than modifying the `diffOp` method to know about such operations and\n+their behaviour we can instead simplify and generalize the solution. Code\n+Reflection defines two sets of operations. Core operations which can be used to\n+model a wide set of Java programs, and extended (or auxiliary) operations. The\n+extended operations model Java langauge constructs such as `for`\n+statements, `if` statements, and `try` statements. The extended operations may\n+be lowered into a sequence of core operations within a connected set of blocks\n+within a body.\n+\n+We can lower `f`'s code model using a lowering transformation and then transform\n+to pure SSA.\n+\n+```text\n+fcm = fcm.transform((block, op) -> {\n+    if (op instanceof Op.Lowerable lop) {\n+        return lop.lower(block);\n+    } else {\n+        block.op(op);\n+        return block;\n+    }\n+});\n+fcm = SSA.transform(fcm);\n+```\n+\n+```text\n+func @\"fcf\" (%0 : double, %1 : int)double -> {\n+    %2 : double = constant @\"1.0\";\n+    %3 : int = constant @\"0\";\n+    branch ^block_0(%2, %3);\n+  \n+  ^block_0(%4 : double, %5 : int):\n+    %6 : boolean = lt %5 %1;\n+    cbranch %6 ^block_1 ^block_2;\n+  \n+  ^block_1:\n+    %7 : int = constant @\"1\";\n+    %8 : boolean = gt %5 %7;\n+    cbranch %8 ^block_3 ^block_4;\n+  \n+  ^block_3:\n+    %9 : int = constant @\"5\";\n+    %10 : boolean = lt %5 %9;\n+    cbranch %10 ^block_5 ^block_6;\n+  \n+  ^block_5:\n+    %11 : double = mul %4 %0;\n+    branch ^block_7(%11);\n+  \n+  ^block_6:\n+    branch ^block_7(%4);\n+  \n+  ^block_7(%12 : double):\n+    branch ^block_8(%12);\n+  \n+  ^block_4:\n+    branch ^block_8(%4);\n+  \n+  ^block_8(%13 : double):\n+    branch ^block_9;\n+  \n+  ^block_9:\n+    %14 : int = constant @\"1\";\n+    %15 : int = add %5 %14;\n+    branch ^block_0(%13, %15);\n+  \n+  ^block_2:\n+    return %4;\n+};\n+```\n+\n+We can see that the resulting code model (program meaning is preserved) has\n+multiple blocks within the `func` operation's body. There is no longer any\n+nesting of bodies.\n+\n+Control flow of the program is modelled by the blocks and how they are connected\n+together to form a *control flow graph*. In `block_9` we can observe a branch\n+back to `block_0` which passes values of `o` and `i` as block arguments for the\n+next loop iteration. (Block arguments and parameters are analogous to phi nodes\n+in other approaches that model code.)\n+\n+We need to extend our implementation, computing the active set and\n+differentiating operations, to understand these connections between blocks.\n+\n+With some modest enhancements we can compute the active set for `x` and\n+differentiate this Java method. The active set will have block parameters as\n+members, such as parameter `%4`, which represents the value `o` for the current\n+loop iteration (since `o` is dependent on `x`). The differentiated model is\n+presented below.\n+\n+```text\n+func @\"dfcf_darg0\" (%0 : double, %1 : int)double -> {\n+    %2 : double = constant @\"0.0\";\n+    %3 : double = constant @\"1.0\";\n+    %4 : double = constant @\"1.0\";\n+    %5 : int = constant @\"0\";\n+    branch ^block_0(%4, %5, %2);\n+  \n+  ^block_0(%6 : double, %7 : int, %8 : double):\n+    %9 : boolean = lt %7 %1;\n+    cbranch %9 ^block_1 ^block_2;\n+  \n+  ^block_1:\n+    %10 : int = constant @\"1\";\n+    %11 : boolean = gt %7 %10;\n+    cbranch %11 ^block_3 ^block_4;\n+  \n+  ^block_3:\n+    %12 : int = constant @\"5\";\n+    %13 : boolean = lt %7 %12;\n+    cbranch %13 ^block_5 ^block_6;\n+  \n+  ^block_5:\n+    %14 : double = mul %6 %0;\n+    %15 : double = mul %8 %0;\n+    %16 : double = mul %6 %3;\n+    %17 : double = add %15 %16;\n+    branch ^block_7(%14, %17);\n+  \n+  ^block_6:\n+    branch ^block_7(%6, %8);\n+  \n+  ^block_7(%18 : double, %19 : double):\n+    branch ^block_8(%18, %19);\n+  \n+  ^block_4:\n+    branch ^block_8(%6, %8);\n+  \n+  ^block_8(%20 : double, %21 : double):\n+    branch ^block_9;\n+  \n+  ^block_9:\n+    %22 : int = constant @\"1\";\n+    %23 : int = add %7 %22;\n+    branch ^block_0(%20, %23, %21);\n+  \n+  ^block_2:\n+    return %8;\n+};\n+```\n+\n+We can observe in `block_5` the application of the product rule. Further we can\n+observe additional block arguments and parameters required to pass along the\n+differential of `o`, `d_o`. In `block_9` there is a branch with an additional\n+block argument appended, value `%21`, that becomes the next value of `d_o`\n+in `block_0`, parameter `%8`\n\\ No newline at end of file\n","filename":"site\/articles\/auto-diff.md","additions":916,"deletions":0,"binary":false,"changes":916,"status":"added"}]}