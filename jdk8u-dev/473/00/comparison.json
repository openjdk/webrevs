{"files":[{"patch":"@@ -0,0 +1,3023 @@\n+\/*\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2023 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/\/ According to the AIX OS doc #pragma alloca must be used\n+\/\/ with C++ compiler before referencing the function alloca()\n+#pragma alloca\n+\n+\/\/ no precompiled headers\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"code\/vtableStubs.hpp\"\n+#include \"compiler\/compileBroker.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"jvm.h\"\n+#include \"jvmtifiles\/jvmti.h\"\n+#include \"libo4.hpp\"\n+#include \"libperfstat_aix.hpp\"\n+#include \"libodm_aix.hpp\"\n+#include \"loadlib_aix.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"memory\/allocation.inline.hpp\"\n+#include \"misc_aix.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"os_aix.inline.hpp\"\n+#include \"os_posix.hpp\"\n+#include \"porting_aix.hpp\"\n+#include \"prims\/jniFastGetField.hpp\"\n+#include \"prims\/jvm_misc.hpp\"\n+#include \"runtime\/arguments.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/javaCalls.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"runtime\/objectMonitor.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"runtime\/osInfo.hpp\"\n+#include \"runtime\/osThread.hpp\"\n+#include \"runtime\/perfMemory.hpp\"\n+#include \"runtime\/safefetch.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/statSampler.hpp\"\n+#include \"runtime\/threadCritical.hpp\"\n+#include \"runtime\/threads.hpp\"\n+#include \"runtime\/timer.hpp\"\n+#include \"runtime\/vm_version.hpp\"\n+#include \"services\/attachListener.hpp\"\n+#include \"services\/runtimeService.hpp\"\n+#include \"signals_posix.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/checkedCast.hpp\"\n+#include \"utilities\/decoder.hpp\"\n+#include \"utilities\/defaultStream.hpp\"\n+#include \"utilities\/events.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/vmError.hpp\"\n+#if INCLUDE_JFR\n+#include \"jfr\/support\/jfrNativeLibraryLoadEvent.hpp\"\n+#endif\n+\n+\/\/ put OS-includes here (sorted alphabetically)\n+#ifdef AIX_XLC_GE_17\n+#include <alloca.h>\n+#endif\n+#include <errno.h>\n+#include <fcntl.h>\n+#include <inttypes.h>\n+#include <poll.h>\n+#include <procinfo.h>\n+#include <pthread.h>\n+#include <pwd.h>\n+#include <semaphore.h>\n+#include <signal.h>\n+#include <stdint.h>\n+#include <stdio.h>\n+#include <string.h>\n+#include <unistd.h>\n+#include <sys\/ioctl.h>\n+#include <sys\/ipc.h>\n+#include <sys\/mman.h>\n+#include <sys\/resource.h>\n+#include <sys\/select.h>\n+#include <sys\/shm.h>\n+#include <sys\/socket.h>\n+#include <sys\/stat.h>\n+#include <sys\/sysinfo.h>\n+#include <sys\/systemcfg.h>\n+#include <sys\/time.h>\n+#include <sys\/times.h>\n+#include <sys\/types.h>\n+#include <sys\/utsname.h>\n+#include <sys\/vminfo.h>\n+\n+#ifndef _LARGE_FILES\n+#error Hotspot on AIX must be compiled with -D_LARGE_FILES\n+#endif\n+\n+\/\/ Missing prototypes for various system APIs.\n+extern \"C\"\n+int mread_real_time(timebasestruct_t *t, size_t size_of_timebasestruct_t);\n+\n+#if !defined(_AIXVERSION_610)\n+extern \"C\" int getthrds64(pid_t, struct thrdentry64*, int, tid64_t*, int);\n+extern \"C\" int getprocs64(procentry64*, int, fdsinfo*, int, pid_t*, int);\n+extern \"C\" int getargs(procsinfo*, int, char*, int);\n+#endif\n+\n+#define MAX_PATH (2 * K)\n+\n+\/\/ for timer info max values which include all bits\n+#define ALL_64_BITS CONST64(0xFFFFFFFFFFFFFFFF)\n+\/\/ for multipage initialization error analysis (in 'g_multipage_error')\n+#define ERROR_MP_OS_TOO_OLD                          100\n+#define ERROR_MP_EXTSHM_ACTIVE                       101\n+#define ERROR_MP_VMGETINFO_FAILED                    102\n+#define ERROR_MP_VMGETINFO_CLAIMS_NO_SUPPORT_FOR_64K 103\n+\n+\/\/ excerpts from systemcfg.h that might be missing on older os levels\n+#ifndef PV_7\n+  #define PV_7 0x200000          \/* Power PC 7 *\/\n+#endif\n+#ifndef PV_7_Compat\n+  #define PV_7_Compat 0x208000   \/* Power PC 7 *\/\n+#endif\n+#ifndef PV_8\n+  #define PV_8 0x300000          \/* Power PC 8 *\/\n+#endif\n+#ifndef PV_8_Compat\n+  #define PV_8_Compat 0x308000   \/* Power PC 8 *\/\n+#endif\n+#ifndef PV_9\n+  #define PV_9 0x400000          \/* Power PC 9 *\/\n+#endif\n+#ifndef PV_9_Compat\n+  #define PV_9_Compat  0x408000  \/* Power PC 9 *\/\n+#endif\n+\n+\n+static address resolve_function_descriptor_to_code_pointer(address p);\n+\n+static void vmembk_print_on(outputStream* os);\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/ global variables (for a description see os_aix.hpp)\n+\n+julong    os::Aix::_physical_memory = 0;\n+\n+pthread_t os::Aix::_main_thread = ((pthread_t)0);\n+\n+\/\/ -1 = uninitialized, 0 if AIX, 1 if OS\/400 pase\n+int       os::Aix::_on_pase = -1;\n+\n+\/\/ 0 = uninitialized, otherwise 32 bit number:\n+\/\/  0xVVRRTTSS\n+\/\/  VV - major version\n+\/\/  RR - minor version\n+\/\/  TT - tech level, if known, 0 otherwise\n+\/\/  SS - service pack, if known, 0 otherwise\n+uint32_t  os::Aix::_os_version = 0;\n+\n+\/\/ -1 = uninitialized, 0 - no, 1 - yes\n+int       os::Aix::_xpg_sus_mode = -1;\n+\n+\/\/ -1 = uninitialized, 0 - no, 1 - yes\n+int       os::Aix::_extshm = -1;\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/ local variables\n+\n+static volatile jlong max_real_time = 0;\n+\n+\/\/ Process break recorded at startup.\n+static address g_brk_at_startup = nullptr;\n+\n+\/\/ This describes the state of multipage support of the underlying\n+\/\/ OS. Note that this is of no interest to the outsize world and\n+\/\/ therefore should not be defined in AIX class.\n+\/\/\n+\/\/ AIX supports four different page sizes - 4K, 64K, 16MB, 16GB. The\n+\/\/ latter two (16M \"large\" resp. 16G \"huge\" pages) require special\n+\/\/ setup and are normally not available.\n+\/\/\n+\/\/ AIX supports multiple page sizes per process, for:\n+\/\/  - Stack (of the primordial thread, so not relevant for us)\n+\/\/  - Data - data, bss, heap, for us also pthread stacks\n+\/\/  - Text - text code\n+\/\/  - shared memory\n+\/\/\n+\/\/ Default page sizes can be set via linker options (-bdatapsize, -bstacksize, ...)\n+\/\/ and via environment variable LDR_CNTRL (DATAPSIZE, STACKPSIZE, ...).\n+\/\/\n+\/\/ For shared memory, page size can be set dynamically via\n+\/\/ shmctl(). Different shared memory regions can have different page\n+\/\/ sizes.\n+\/\/\n+\/\/ More information can be found at AIBM info center:\n+\/\/   http:\/\/publib.boulder.ibm.com\/infocenter\/aix\/v6r1\/index.jsp?topic=\/com.ibm.aix.prftungd\/doc\/prftungd\/multiple_page_size_app_support.htm\n+\/\/\n+static struct {\n+  size_t pagesize;            \/\/ sysconf _SC_PAGESIZE (4K)\n+  size_t datapsize;           \/\/ default data page size (LDR_CNTRL DATAPSIZE)\n+  size_t shmpsize;            \/\/ default shared memory page size (LDR_CNTRL SHMPSIZE)\n+  size_t pthr_stack_pagesize; \/\/ stack page size of pthread threads\n+  size_t textpsize;           \/\/ default text page size (LDR_CNTRL STACKPSIZE)\n+  bool can_use_64K_pages;     \/\/ True if we can alloc 64K pages dynamically with Sys V shm.\n+  bool can_use_16M_pages;     \/\/ True if we can alloc 16M pages dynamically with Sys V shm.\n+  int error;                  \/\/ Error describing if something went wrong at multipage init.\n+} g_multipage_support = {\n+  (size_t) -1,\n+  (size_t) -1,\n+  (size_t) -1,\n+  (size_t) -1,\n+  (size_t) -1,\n+  false, false,\n+  0\n+};\n+\n+\/\/ We must not accidentally allocate memory close to the BRK - even if\n+\/\/ that would work - because then we prevent the BRK segment from\n+\/\/ growing which may result in a malloc OOM even though there is\n+\/\/ enough memory. The problem only arises if we shmat() or mmap() at\n+\/\/ a specific wish address, e.g. to place the heap in a\n+\/\/ compressed-oops-friendly way.\n+static bool is_close_to_brk(address a) {\n+  assert0(g_brk_at_startup != nullptr);\n+  if (a >= g_brk_at_startup &&\n+      a < (g_brk_at_startup + MaxExpectedDataSegmentSize)) {\n+    return true;\n+  }\n+  return false;\n+}\n+\n+julong os::free_memory() {\n+  return Aix::available_memory();\n+}\n+\n+julong os::available_memory() {\n+  return Aix::available_memory();\n+}\n+\n+julong os::Aix::available_memory() {\n+  \/\/ Avoid expensive API call here, as returned value will always be null.\n+  if (os::Aix::on_pase()) {\n+    return 0x0LL;\n+  }\n+  os::Aix::meminfo_t mi;\n+  if (os::Aix::get_meminfo(&mi)) {\n+    return mi.real_free;\n+  } else {\n+    return ULONG_MAX;\n+  }\n+}\n+\n+jlong os::total_swap_space() {\n+  perfstat_memory_total_t memory_info;\n+  if (libperfstat::perfstat_memory_total(NULL, &memory_info, sizeof(perfstat_memory_total_t), 1) == -1) {\n+    return -1;\n+  }\n+  return (jlong)(memory_info.pgsp_total * 4 * K);\n+}\n+\n+jlong os::free_swap_space() {\n+  perfstat_memory_total_t memory_info;\n+  if (libperfstat::perfstat_memory_total(NULL, &memory_info, sizeof(perfstat_memory_total_t), 1) == -1) {\n+    return -1;\n+  }\n+  return (jlong)(memory_info.pgsp_free * 4 * K);\n+}\n+\n+julong os::physical_memory() {\n+  return Aix::physical_memory();\n+}\n+\n+\/\/ Helper function, emulates disclaim64 using multiple 32bit disclaims\n+\/\/ because we cannot use disclaim64() on AS\/400 and old AIX releases.\n+static bool my_disclaim64(char* addr, size_t size) {\n+\n+  if (size == 0) {\n+    return true;\n+  }\n+\n+  \/\/ Maximum size 32bit disclaim() accepts. (Theoretically 4GB, but I just do not trust that.)\n+  const unsigned int maxDisclaimSize = 0x40000000;\n+\n+  const unsigned int numFullDisclaimsNeeded = (size \/ maxDisclaimSize);\n+  const unsigned int lastDisclaimSize = (size % maxDisclaimSize);\n+\n+  char* p = addr;\n+\n+  for (unsigned int i = 0; i < numFullDisclaimsNeeded; i ++) {\n+    if (::disclaim(p, maxDisclaimSize, DISCLAIM_ZEROMEM) != 0) {\n+      trcVerbose(\"Cannot disclaim %p - %p (errno %d)\\n\", p, p + maxDisclaimSize, errno);\n+      return false;\n+    }\n+    p += maxDisclaimSize;\n+  }\n+\n+  if (lastDisclaimSize > 0) {\n+    if (::disclaim(p, lastDisclaimSize, DISCLAIM_ZEROMEM) != 0) {\n+      trcVerbose(\"Cannot disclaim %p - %p (errno %d)\\n\", p, p + lastDisclaimSize, errno);\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+\/\/ Cpu architecture string\n+#if defined(PPC32)\n+static char cpu_arch[] = \"ppc\";\n+#elif defined(PPC64)\n+static char cpu_arch[] = \"ppc64\";\n+#else\n+#error Add appropriate cpu_arch setting\n+#endif\n+\n+\/\/ Wrap the function \"vmgetinfo\" which is not available on older OS releases.\n+static int checked_vmgetinfo(void *out, int command, int arg) {\n+  if (os::Aix::on_pase() && os::Aix::os_version_short() < 0x0601) {\n+    guarantee(false, \"cannot call vmgetinfo on AS\/400 older than V6R1\");\n+  }\n+  return ::vmgetinfo(out, command, arg);\n+}\n+\n+\/\/ Given an address, returns the size of the page backing that address.\n+size_t os::Aix::query_pagesize(void* addr) {\n+\n+  if (os::Aix::on_pase() && os::Aix::os_version_short() < 0x0601) {\n+    \/\/ AS\/400 older than V6R1: no vmgetinfo here, default to 4K\n+    return 4*K;\n+  }\n+\n+  vm_page_info pi;\n+  pi.addr = (uint64_t)addr;\n+  if (checked_vmgetinfo(&pi, VM_PAGE_INFO, sizeof(pi)) == 0) {\n+    return pi.pagesize;\n+  } else {\n+    assert(false, \"vmgetinfo failed to retrieve page size\");\n+    return 4*K;\n+  }\n+}\n+\n+void os::Aix::initialize_system_info() {\n+\n+  \/\/ Get the number of online(logical) cpus instead of configured.\n+  os::_processor_count = sysconf(_SC_NPROCESSORS_ONLN);\n+  assert(_processor_count > 0, \"_processor_count must be > 0\");\n+\n+  \/\/ Retrieve total physical storage.\n+  os::Aix::meminfo_t mi;\n+  if (!os::Aix::get_meminfo(&mi)) {\n+    assert(false, \"os::Aix::get_meminfo failed.\");\n+  }\n+  _physical_memory = (julong) mi.real_total;\n+}\n+\n+\/\/ Helper function for tracing page sizes.\n+static const char* describe_pagesize(size_t pagesize) {\n+  switch (pagesize) {\n+    case 4*K : return \"4K\";\n+    case 64*K: return \"64K\";\n+    case 16*M: return \"16M\";\n+    case 16*G: return \"16G\";\n+    default:\n+      assert(false, \"surprise\");\n+      return \"??\";\n+  }\n+}\n+\n+\/\/ Probe OS for multipage support.\n+\/\/ Will fill the global g_multipage_support structure.\n+\/\/ Must be called before calling os::large_page_init().\n+static void query_multipage_support() {\n+\n+  guarantee(g_multipage_support.pagesize == (size_t)-1,\n+            \"do not call twice\");\n+\n+  g_multipage_support.pagesize = ::sysconf(_SC_PAGESIZE);\n+\n+  \/\/ This really would surprise me.\n+  assert(g_multipage_support.pagesize == 4*K, \"surprise!\");\n+\n+  \/\/ Query default data page size (default page size for C-Heap, pthread stacks and .bss).\n+  \/\/ Default data page size is defined either by linker options (-bdatapsize)\n+  \/\/ or by environment variable LDR_CNTRL (suboption DATAPSIZE). If none is given,\n+  \/\/ default should be 4K.\n+  {\n+    void* p = ::malloc(16*M);\n+    g_multipage_support.datapsize = os::Aix::query_pagesize(p);\n+    ::free(p);\n+  }\n+\n+  \/\/ Query default shm page size (LDR_CNTRL SHMPSIZE).\n+  \/\/ Note that this is pure curiosity. We do not rely on default page size but set\n+  \/\/ our own page size after allocated.\n+  {\n+    const int shmid = ::shmget(IPC_PRIVATE, 1, IPC_CREAT | S_IRUSR | S_IWUSR);\n+    guarantee(shmid != -1, \"shmget failed\");\n+    void* p = ::shmat(shmid, nullptr, 0);\n+    ::shmctl(shmid, IPC_RMID, nullptr);\n+    guarantee(p != (void*) -1, \"shmat failed\");\n+    g_multipage_support.shmpsize = os::Aix::query_pagesize(p);\n+    ::shmdt(p);\n+  }\n+\n+  \/\/ Before querying the stack page size, make sure we are not running as primordial\n+  \/\/ thread (because primordial thread's stack may have different page size than\n+  \/\/ pthread thread stacks). Running a VM on the primordial thread won't work for a\n+  \/\/ number of reasons so we may just as well guarantee it here.\n+  guarantee0(!os::is_primordial_thread());\n+\n+  \/\/ Query pthread stack page size. Should be the same as data page size because\n+  \/\/ pthread stacks are allocated from C-Heap.\n+  {\n+    int dummy = 0;\n+    g_multipage_support.pthr_stack_pagesize = os::Aix::query_pagesize(&dummy);\n+  }\n+\n+  \/\/ Query default text page size (LDR_CNTRL TEXTPSIZE).\n+  {\n+    address any_function =\n+      resolve_function_descriptor_to_code_pointer((address)describe_pagesize);\n+    g_multipage_support.textpsize = os::Aix::query_pagesize(any_function);\n+  }\n+\n+  \/\/ Now probe for support of 64K pages and 16M pages.\n+\n+  \/\/ Before OS\/400 V6R1, there is no support for pages other than 4K.\n+  if (os::Aix::on_pase_V5R4_or_older()) {\n+    trcVerbose(\"OS\/400 < V6R1 - no large page support.\");\n+    g_multipage_support.error = ERROR_MP_OS_TOO_OLD;\n+    goto query_multipage_support_end;\n+  }\n+\n+  \/\/ Now check which page sizes the OS claims it supports, and of those, which actually can be used.\n+  {\n+    const int MAX_PAGE_SIZES = 4;\n+    psize_t sizes[MAX_PAGE_SIZES];\n+    const int num_psizes = checked_vmgetinfo(sizes, VMINFO_GETPSIZES, MAX_PAGE_SIZES);\n+    if (num_psizes == -1) {\n+      trcVerbose(\"vmgetinfo(VMINFO_GETPSIZES) failed (errno: %d)\", errno);\n+      trcVerbose(\"disabling multipage support.\");\n+      g_multipage_support.error = ERROR_MP_VMGETINFO_FAILED;\n+      goto query_multipage_support_end;\n+    }\n+    guarantee(num_psizes > 0, \"vmgetinfo(.., VMINFO_GETPSIZES, ...) failed.\");\n+    assert(num_psizes <= MAX_PAGE_SIZES, \"Surprise! more than 4 page sizes?\");\n+    trcVerbose(\"vmgetinfo(.., VMINFO_GETPSIZES, ...) returns %d supported page sizes: \", num_psizes);\n+    for (int i = 0; i < num_psizes; i ++) {\n+      trcVerbose(\" %s \", describe_pagesize(sizes[i]));\n+    }\n+\n+    \/\/ Can we use 64K, 16M pages?\n+    for (int i = 0; i < num_psizes; i ++) {\n+      const size_t pagesize = sizes[i];\n+      if (pagesize != 64*K && pagesize != 16*M) {\n+        continue;\n+      }\n+      bool can_use = false;\n+      trcVerbose(\"Probing support for %s pages...\", describe_pagesize(pagesize));\n+      const int shmid = ::shmget(IPC_PRIVATE, pagesize,\n+        IPC_CREAT | S_IRUSR | S_IWUSR);\n+      guarantee0(shmid != -1); \/\/ Should always work.\n+      \/\/ Try to set pagesize.\n+      struct shmid_ds shm_buf = { };\n+      shm_buf.shm_pagesize = pagesize;\n+      if (::shmctl(shmid, SHM_PAGESIZE, &shm_buf) != 0) {\n+        const int en = errno;\n+        ::shmctl(shmid, IPC_RMID, nullptr); \/\/ As early as possible!\n+        trcVerbose(\"shmctl(SHM_PAGESIZE) failed with errno=%d\", errno);\n+      } else {\n+        \/\/ Attach and double check pageisze.\n+        void* p = ::shmat(shmid, nullptr, 0);\n+        ::shmctl(shmid, IPC_RMID, nullptr); \/\/ As early as possible!\n+        guarantee0(p != (void*) -1); \/\/ Should always work.\n+        const size_t real_pagesize = os::Aix::query_pagesize(p);\n+        if (real_pagesize != pagesize) {\n+          trcVerbose(\"real page size (\" SIZE_FORMAT_X \") differs.\", real_pagesize);\n+        } else {\n+          can_use = true;\n+        }\n+        ::shmdt(p);\n+      }\n+      trcVerbose(\"Can use: %s\", (can_use ? \"yes\" : \"no\"));\n+      if (pagesize == 64*K) {\n+        g_multipage_support.can_use_64K_pages = can_use;\n+      } else if (pagesize == 16*M) {\n+        g_multipage_support.can_use_16M_pages = can_use;\n+      }\n+    }\n+\n+  } \/\/ end: check which pages can be used for shared memory\n+\n+query_multipage_support_end:\n+\n+  trcVerbose(\"base page size (sysconf _SC_PAGESIZE): %s\",\n+      describe_pagesize(g_multipage_support.pagesize));\n+  trcVerbose(\"Data page size (C-Heap, bss, etc): %s\",\n+      describe_pagesize(g_multipage_support.datapsize));\n+  trcVerbose(\"Text page size: %s\",\n+      describe_pagesize(g_multipage_support.textpsize));\n+  trcVerbose(\"Thread stack page size (pthread): %s\",\n+      describe_pagesize(g_multipage_support.pthr_stack_pagesize));\n+  trcVerbose(\"Default shared memory page size: %s\",\n+      describe_pagesize(g_multipage_support.shmpsize));\n+  trcVerbose(\"Can use 64K pages dynamically with shared memory: %s\",\n+      (g_multipage_support.can_use_64K_pages ? \"yes\" :\"no\"));\n+  trcVerbose(\"Can use 16M pages dynamically with shared memory: %s\",\n+      (g_multipage_support.can_use_16M_pages ? \"yes\" :\"no\"));\n+  trcVerbose(\"Multipage error details: %d\",\n+      g_multipage_support.error);\n+\n+  \/\/ sanity checks\n+  assert0(g_multipage_support.pagesize == 4*K);\n+  assert0(g_multipage_support.datapsize == 4*K || g_multipage_support.datapsize == 64*K);\n+  assert0(g_multipage_support.textpsize == 4*K || g_multipage_support.textpsize == 64*K);\n+  assert0(g_multipage_support.pthr_stack_pagesize == g_multipage_support.datapsize);\n+  assert0(g_multipage_support.shmpsize == 4*K || g_multipage_support.shmpsize == 64*K);\n+\n+}\n+\n+void os::init_system_properties_values() {\n+\n+#ifndef OVERRIDE_LIBPATH\n+  #define DEFAULT_LIBPATH \"\/lib:\/usr\/lib\"\n+#else\n+  #define DEFAULT_LIBPATH OVERRIDE_LIBPATH\n+#endif\n+#define EXTENSIONS_DIR  \"\/lib\/ext\"\n+\n+  \/\/ Buffer that fits several snprintfs.\n+  \/\/ Note that the space for the trailing null is provided\n+  \/\/ by the nulls included by the sizeof operator.\n+  const size_t bufsize =\n+    MAX2((size_t)MAXPATHLEN,  \/\/ For dll_dir & friends.\n+         (size_t)MAXPATHLEN + sizeof(EXTENSIONS_DIR)); \/\/ extensions dir\n+  char *buf = NEW_C_HEAP_ARRAY(char, bufsize, mtInternal);\n+\n+  \/\/ sysclasspath, java_home, dll_dir\n+  {\n+    char *pslash;\n+    os::jvm_path(buf, bufsize);\n+\n+    \/\/ Found the full path to libjvm.so.\n+    \/\/ Now cut the path to <java_home>\/jre if we can.\n+    pslash = strrchr(buf, '\/');\n+    if (pslash != nullptr) {\n+      *pslash = '\\0';            \/\/ Get rid of \/libjvm.so.\n+    }\n+    pslash = strrchr(buf, '\/');\n+    if (pslash != nullptr) {\n+      *pslash = '\\0';            \/\/ Get rid of \/{client|server|hotspot}.\n+    }\n+    Arguments::set_dll_dir(buf);\n+\n+    if (pslash != nullptr) {\n+      pslash = strrchr(buf, '\/');\n+      if (pslash != nullptr) {\n+        *pslash = '\\0';        \/\/ Get rid of \/lib.\n+      }\n+    }\n+    Arguments::set_java_home(buf);\n+    if (!set_boot_path('\/', ':')) {\n+      vm_exit_during_initialization(\"Failed setting boot class path.\", nullptr);\n+    }\n+  }\n+\n+  \/\/ Where to look for native libraries.\n+\n+  \/\/ On Aix we get the user setting of LIBPATH.\n+  \/\/ Eventually, all the library path setting will be done here.\n+  \/\/ Get the user setting of LIBPATH.\n+  const char *v = ::getenv(\"LIBPATH\");\n+  const char *v_colon = \":\";\n+  if (v == nullptr) { v = \"\"; v_colon = \"\"; }\n+\n+  \/\/ Concatenate user and invariant part of ld_library_path.\n+  \/\/ That's +1 for the colon and +1 for the trailing '\\0'.\n+  size_t pathsize = strlen(v) + 1 + sizeof(DEFAULT_LIBPATH) + 1;\n+  char *ld_library_path = NEW_C_HEAP_ARRAY(char, pathsize, mtInternal);\n+  os::snprintf_checked(ld_library_path, pathsize, \"%s%s\" DEFAULT_LIBPATH, v, v_colon);\n+  Arguments::set_library_path(ld_library_path);\n+  FREE_C_HEAP_ARRAY(char, ld_library_path);\n+\n+  \/\/ Extensions directories.\n+  os::snprintf_checked(buf, bufsize, \"%s\" EXTENSIONS_DIR, Arguments::get_java_home());\n+  Arguments::set_ext_dirs(buf);\n+\n+  FREE_C_HEAP_ARRAY(char, buf);\n+\n+#undef DEFAULT_LIBPATH\n+#undef EXTENSIONS_DIR\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/ breakpoint support\n+\n+void os::breakpoint() {\n+  BREAKPOINT;\n+}\n+\n+extern \"C\" void breakpoint() {\n+  \/\/ use debugger to set breakpoint here\n+}\n+\n+\/\/ retrieve memory information.\n+\/\/ Returns false if something went wrong;\n+\/\/ content of pmi undefined in this case.\n+bool os::Aix::get_meminfo(meminfo_t* pmi) {\n+\n+  assert(pmi, \"get_meminfo: invalid parameter\");\n+\n+  memset(pmi, 0, sizeof(meminfo_t));\n+\n+  if (os::Aix::on_pase()) {\n+    \/\/ On PASE, use the libo4 porting library.\n+\n+    unsigned long long virt_total = 0;\n+    unsigned long long real_total = 0;\n+    unsigned long long real_free = 0;\n+    unsigned long long pgsp_total = 0;\n+    unsigned long long pgsp_free = 0;\n+    if (libo4::get_memory_info(&virt_total, &real_total, &real_free, &pgsp_total, &pgsp_free)) {\n+      pmi->virt_total = virt_total;\n+      pmi->real_total = real_total;\n+      pmi->real_free = real_free;\n+      pmi->pgsp_total = pgsp_total;\n+      pmi->pgsp_free = pgsp_free;\n+      return true;\n+    }\n+    return false;\n+\n+  } else {\n+\n+    \/\/ On AIX, I use the (dynamically loaded) perfstat library to retrieve memory statistics\n+    \/\/ See:\n+    \/\/ http:\/\/publib.boulder.ibm.com\/infocenter\/systems\/index.jsp\n+    \/\/        ?topic=\/com.ibm.aix.basetechref\/doc\/basetrf1\/perfstat_memtot.htm\n+    \/\/ http:\/\/publib.boulder.ibm.com\/infocenter\/systems\/index.jsp\n+    \/\/        ?topic=\/com.ibm.aix.files\/doc\/aixfiles\/libperfstat.h.htm\n+\n+    perfstat_memory_total_t psmt;\n+    memset (&psmt, '\\0', sizeof(psmt));\n+    const int rc = libperfstat::perfstat_memory_total(nullptr, &psmt, sizeof(psmt), 1);\n+    if (rc == -1) {\n+      trcVerbose(\"perfstat_memory_total() failed (errno=%d)\", errno);\n+      assert(0, \"perfstat_memory_total() failed\");\n+      return false;\n+    }\n+\n+    assert(rc == 1, \"perfstat_memory_total() - weird return code\");\n+\n+    \/\/ excerpt from\n+    \/\/ http:\/\/publib.boulder.ibm.com\/infocenter\/systems\/index.jsp\n+    \/\/        ?topic=\/com.ibm.aix.files\/doc\/aixfiles\/libperfstat.h.htm\n+    \/\/ The fields of perfstat_memory_total_t:\n+    \/\/ u_longlong_t virt_total         Total virtual memory (in 4 KB pages).\n+    \/\/ u_longlong_t real_total         Total real memory (in 4 KB pages).\n+    \/\/ u_longlong_t real_free          Free real memory (in 4 KB pages).\n+    \/\/ u_longlong_t pgsp_total         Total paging space (in 4 KB pages).\n+    \/\/ u_longlong_t pgsp_free          Free paging space (in 4 KB pages).\n+\n+    pmi->virt_total = psmt.virt_total * 4096;\n+    pmi->real_total = psmt.real_total * 4096;\n+    pmi->real_free = psmt.real_free * 4096;\n+    pmi->pgsp_total = psmt.pgsp_total * 4096;\n+    pmi->pgsp_free = psmt.pgsp_free * 4096;\n+\n+    return true;\n+\n+  }\n+} \/\/ end os::Aix::get_meminfo\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/ create new thread\n+\n+\/\/ Thread start routine for all newly created threads\n+static void *thread_native_entry(Thread *thread) {\n+\n+  thread->record_stack_base_and_size();\n+\n+  const pthread_t pthread_id = ::pthread_self();\n+  const tid_t kernel_thread_id = ::thread_self();\n+\n+  LogTarget(Info, os, thread) lt;\n+  if (lt.is_enabled()) {\n+    address low_address = thread->stack_end();\n+    address high_address = thread->stack_base();\n+    lt.print(\"Thread is alive (tid: \" UINTX_FORMAT \", kernel thread id: \" UINTX_FORMAT\n+             \", stack [\" PTR_FORMAT \" - \" PTR_FORMAT \" (\" SIZE_FORMAT \"k using %uk pages)).\",\n+             os::current_thread_id(), (uintx) kernel_thread_id, low_address, high_address,\n+             (high_address - low_address) \/ K, os::Aix::query_pagesize(low_address) \/ K);\n+  }\n+\n+  \/\/ Normally, pthread stacks on AIX live in the data segment (are allocated with malloc()\n+  \/\/ by the pthread library). In rare cases, this may not be the case, e.g. when third-party\n+  \/\/ tools hook pthread_create(). In this case, we may run into problems establishing\n+  \/\/ guard pages on those stacks, because the stacks may reside in memory which is not\n+  \/\/ protectable (shmated).\n+  if (thread->stack_base() > ::sbrk(0)) {\n+    log_warning(os, thread)(\"Thread stack not in data segment.\");\n+  }\n+\n+  \/\/ Try to randomize the cache line index of hot stack frames.\n+  \/\/ This helps when threads of the same stack traces evict each other's\n+  \/\/ cache lines. The threads can be either from the same JVM instance, or\n+  \/\/ from different JVM instances. The benefit is especially true for\n+  \/\/ processors with hyperthreading technology.\n+\n+  static int counter = 0;\n+  int pid = os::current_process_id();\n+  alloca(((pid ^ counter++) & 7) * 128);\n+\n+  thread->initialize_thread_current();\n+\n+  OSThread* osthread = thread->osthread();\n+\n+  \/\/ Thread_id is pthread id.\n+  osthread->set_thread_id(pthread_id);\n+\n+  \/\/ .. but keep kernel thread id too for diagnostics\n+  osthread->set_kernel_thread_id(kernel_thread_id);\n+\n+  \/\/ Initialize signal mask for this thread.\n+  PosixSignals::hotspot_sigmask(thread);\n+\n+  \/\/ Initialize floating point control register.\n+  os::Aix::init_thread_fpu_state();\n+\n+  assert(osthread->get_state() == RUNNABLE, \"invalid os thread state\");\n+\n+  \/\/ Call one more level start routine.\n+  thread->call_run();\n+\n+  \/\/ Note: at this point the thread object may already have deleted itself.\n+  \/\/ Prevent dereferencing it from here on out.\n+  thread = nullptr;\n+\n+  log_info(os, thread)(\"Thread finished (tid: \" UINTX_FORMAT \", kernel thread id: \" UINTX_FORMAT \").\",\n+    os::current_thread_id(), (uintx) kernel_thread_id);\n+\n+  return 0;\n+}\n+\n+bool os::create_thread(Thread* thread, ThreadType thr_type,\n+                       size_t req_stack_size) {\n+\n+  assert(thread->osthread() == nullptr, \"caller responsible\");\n+\n+  \/\/ Allocate the OSThread object.\n+  OSThread* osthread = new (std::nothrow) OSThread();\n+  if (osthread == nullptr) {\n+    return false;\n+  }\n+\n+  \/\/ Set the correct thread state.\n+  osthread->set_thread_type(thr_type);\n+\n+  \/\/ Initial state is ALLOCATED but not INITIALIZED\n+  osthread->set_state(ALLOCATED);\n+\n+  thread->set_osthread(osthread);\n+\n+  \/\/ Init thread attributes.\n+  pthread_attr_t attr;\n+  int rslt = pthread_attr_init(&attr);\n+  guarantee(rslt == 0, \"pthread_attr_init has to return 0\");\n+  guarantee(pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED) == 0, \"???\");\n+\n+  \/\/ Make sure we run in 1:1 kernel-user-thread mode.\n+  if (os::Aix::on_aix()) {\n+    guarantee(pthread_attr_setscope(&attr, PTHREAD_SCOPE_SYSTEM) == 0, \"???\");\n+    guarantee(pthread_attr_setinheritsched(&attr, PTHREAD_EXPLICIT_SCHED) == 0, \"???\");\n+  }\n+\n+  \/\/ Start in suspended state, and in os::thread_start, wake the thread up.\n+  guarantee(pthread_attr_setsuspendstate_np(&attr, PTHREAD_CREATE_SUSPENDED_NP) == 0, \"???\");\n+\n+  \/\/ Calculate stack size if it's not specified by caller.\n+  size_t stack_size = os::Posix::get_initial_stack_size(thr_type, req_stack_size);\n+\n+  \/\/ JDK-8187028: It was observed that on some configurations (4K backed thread stacks)\n+  \/\/ the real thread stack size may be smaller than the requested stack size, by as much as 64K.\n+  \/\/ This very much looks like a pthread lib error. As a workaround, increase the stack size\n+  \/\/ by 64K for small thread stacks (arbitrarily chosen to be < 4MB)\n+  if (stack_size < 4096 * K) {\n+    stack_size += 64 * K;\n+  }\n+\n+  \/\/ On Aix, pthread_attr_setstacksize fails with huge values and leaves the\n+  \/\/ thread size in attr unchanged. If this is the minimal stack size as set\n+  \/\/ by pthread_attr_init this leads to crashes after thread creation. E.g. the\n+  \/\/ guard pages might not fit on the tiny stack created.\n+  int ret = pthread_attr_setstacksize(&attr, stack_size);\n+  if (ret != 0) {\n+    log_warning(os, thread)(\"The %sthread stack size specified is invalid: \" SIZE_FORMAT \"k\",\n+                            (thr_type == compiler_thread) ? \"compiler \" : ((thr_type == java_thread) ? \"\" : \"VM \"),\n+                            stack_size \/ K);\n+    thread->set_osthread(nullptr);\n+    delete osthread;\n+    pthread_attr_destroy(&attr);\n+    return false;\n+  }\n+\n+  \/\/ Save some cycles and a page by disabling OS guard pages where we have our own\n+  \/\/ VM guard pages (in java threads). For other threads, keep system default guard\n+  \/\/ pages in place.\n+  if (thr_type == java_thread || thr_type == compiler_thread) {\n+    ret = pthread_attr_setguardsize(&attr, 0);\n+  }\n+\n+  ResourceMark rm;\n+  pthread_t tid = 0;\n+\n+  if (ret == 0) {\n+    int limit = 3;\n+    do {\n+      ret = pthread_create(&tid, &attr, (void* (*)(void*)) thread_native_entry, thread);\n+    } while (ret == EAGAIN && limit-- > 0);\n+  }\n+\n+  if (ret == 0) {\n+    char buf[64];\n+    log_info(os, thread)(\"Thread \\\"%s\\\" started (pthread id: \" UINTX_FORMAT \", attributes: %s). \",\n+                         thread->name(), (uintx) tid, os::Posix::describe_pthread_attr(buf, sizeof(buf), &attr));\n+  } else {\n+    char buf[64];\n+    log_warning(os, thread)(\"Failed to start thread \\\"%s\\\" - pthread_create failed (%d=%s) for attributes: %s.\",\n+                            thread->name(), ret, os::errno_name(ret), os::Posix::describe_pthread_attr(buf, sizeof(buf), &attr));\n+    \/\/ Log some OS information which might explain why creating the thread failed.\n+    log_warning(os, thread)(\"Number of threads approx. running in the VM: %d\", Threads::number_of_threads());\n+    log_warning(os, thread)(\"Checking JVM parameter MaxExpectedDataSegmentSize (currently \" SIZE_FORMAT \"k)  might be helpful\", MaxExpectedDataSegmentSize\/K);\n+    LogStream st(Log(os, thread)::info());\n+    os::Posix::print_rlimit_info(&st);\n+    os::print_memory_info(&st);\n+  }\n+\n+  pthread_attr_destroy(&attr);\n+\n+  if (ret != 0) {\n+    \/\/ Need to clean up stuff we've allocated so far.\n+    thread->set_osthread(nullptr);\n+    delete osthread;\n+    return false;\n+  }\n+\n+  \/\/ OSThread::thread_id is the pthread id.\n+  osthread->set_thread_id(tid);\n+\n+  return true;\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/ attach existing thread\n+\n+\/\/ bootstrap the main thread\n+bool os::create_main_thread(JavaThread* thread) {\n+  assert(os::Aix::_main_thread == pthread_self(), \"should be called inside main thread\");\n+  return create_attached_thread(thread);\n+}\n+\n+bool os::create_attached_thread(JavaThread* thread) {\n+#ifdef ASSERT\n+    thread->verify_not_published();\n+#endif\n+\n+  \/\/ Allocate the OSThread object\n+  OSThread* osthread = new (std::nothrow) OSThread();\n+\n+  if (osthread == nullptr) {\n+    return false;\n+  }\n+\n+  const pthread_t pthread_id = ::pthread_self();\n+  const tid_t kernel_thread_id = ::thread_self();\n+\n+  \/\/ OSThread::thread_id is the pthread id.\n+  osthread->set_thread_id(pthread_id);\n+\n+  \/\/ .. but keep kernel thread id too for diagnostics\n+  osthread->set_kernel_thread_id(kernel_thread_id);\n+\n+  \/\/ initialize floating point control register\n+  os::Aix::init_thread_fpu_state();\n+\n+  \/\/ Initial thread state is RUNNABLE\n+  osthread->set_state(RUNNABLE);\n+\n+  thread->set_osthread(osthread);\n+\n+  if (UseNUMA) {\n+    int lgrp_id = os::numa_get_group_id();\n+    if (lgrp_id != -1) {\n+      thread->set_lgrp_id(lgrp_id);\n+    }\n+  }\n+\n+  \/\/ initialize signal mask for this thread\n+  \/\/ and save the caller's signal mask\n+  PosixSignals::hotspot_sigmask(thread);\n+\n+  log_info(os, thread)(\"Thread attached (tid: \" UINTX_FORMAT \", kernel thread  id: \" UINTX_FORMAT\n+                       \", stack: \" PTR_FORMAT \" - \" PTR_FORMAT \" (\" SIZE_FORMAT \"K) ).\",\n+                       os::current_thread_id(), (uintx) kernel_thread_id,\n+                       p2i(thread->stack_base()), p2i(thread->stack_end()), thread->stack_size() \/ K);\n+\n+  return true;\n+}\n+\n+void os::pd_start_thread(Thread* thread) {\n+  int status = pthread_continue_np(thread->osthread()->pthread_id());\n+  assert(status == 0, \"thr_continue failed\");\n+}\n+\n+\/\/ Free OS resources related to the OSThread\n+void os::free_thread(OSThread* osthread) {\n+  assert(osthread != nullptr, \"osthread not set\");\n+\n+  \/\/ We are told to free resources of the argument thread,\n+  \/\/ but we can only really operate on the current thread.\n+  assert(Thread::current()->osthread() == osthread,\n+         \"os::free_thread but not current thread\");\n+\n+  \/\/ Restore caller's signal mask\n+  sigset_t sigmask = osthread->caller_sigmask();\n+  pthread_sigmask(SIG_SETMASK, &sigmask, nullptr);\n+\n+  delete osthread;\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/ time support\n+\n+double os::elapsedVTime() {\n+  struct rusage usage;\n+  int retval = getrusage(RUSAGE_THREAD, &usage);\n+  if (retval == 0) {\n+    return usage.ru_utime.tv_sec + usage.ru_stime.tv_sec + (usage.ru_utime.tv_usec + usage.ru_stime.tv_usec) \/ (1000.0 * 1000);\n+  } else {\n+    \/\/ better than nothing, but not much\n+    return elapsedTime();\n+  }\n+}\n+\n+\/\/ We use mread_real_time here.\n+\/\/ On AIX: If the CPU has a time register, the result will be RTC_POWER and\n+\/\/ it has to be converted to real time. AIX documentations suggests to do\n+\/\/ this unconditionally, so we do it.\n+\/\/\n+\/\/ See: https:\/\/www.ibm.com\/support\/knowledgecenter\/ssw_aix_61\/com.ibm.aix.basetrf2\/read_real_time.htm\n+\/\/\n+\/\/ On PASE: mread_real_time will always return RTC_POWER_PC data, so no\n+\/\/ conversion is necessary. However, mread_real_time will not return\n+\/\/ monotonic results but merely matches read_real_time. So we need a tweak\n+\/\/ to ensure monotonic results.\n+\/\/\n+\/\/ For PASE no public documentation exists, just word by IBM\n+jlong os::javaTimeNanos() {\n+  timebasestruct_t time;\n+  int rc = mread_real_time(&time, TIMEBASE_SZ);\n+  if (os::Aix::on_pase()) {\n+    assert(rc == RTC_POWER, \"expected time format RTC_POWER from mread_real_time in PASE\");\n+    jlong now = jlong(time.tb_high) * NANOSECS_PER_SEC + jlong(time.tb_low);\n+    jlong prev = max_real_time;\n+    if (now <= prev) {\n+      return prev;   \/\/ same or retrograde time;\n+    }\n+    jlong obsv = Atomic::cmpxchg(&max_real_time, prev, now);\n+    assert(obsv >= prev, \"invariant\");   \/\/ Monotonicity\n+    \/\/ If the CAS succeeded then we're done and return \"now\".\n+    \/\/ If the CAS failed and the observed value \"obsv\" is >= now then\n+    \/\/ we should return \"obsv\".  If the CAS failed and now > obsv > prv then\n+    \/\/ some other thread raced this thread and installed a new value, in which case\n+    \/\/ we could either (a) retry the entire operation, (b) retry trying to install now\n+    \/\/ or (c) just return obsv.  We use (c).   No loop is required although in some cases\n+    \/\/ we might discard a higher \"now\" value in deference to a slightly lower but freshly\n+    \/\/ installed obsv value.   That's entirely benign -- it admits no new orderings compared\n+    \/\/ to (a) or (b) -- and greatly reduces coherence traffic.\n+    \/\/ We might also condition (c) on the magnitude of the delta between obsv and now.\n+    \/\/ Avoiding excessive CAS operations to hot RW locations is critical.\n+    \/\/ See https:\/\/blogs.oracle.com\/dave\/entry\/cas_and_cache_trivia_invalidate\n+    return (prev == obsv) ? now : obsv;\n+  } else {\n+    if (rc != RTC_POWER) {\n+      rc = time_base_to_time(&time, TIMEBASE_SZ);\n+      assert(rc != -1, \"error calling time_base_to_time()\");\n+    }\n+    return jlong(time.tb_high) * NANOSECS_PER_SEC + jlong(time.tb_low);\n+  }\n+}\n+\n+void os::javaTimeNanos_info(jvmtiTimerInfo *info_ptr) {\n+  info_ptr->max_value = ALL_64_BITS;\n+  \/\/ mread_real_time() is monotonic (see 'os::javaTimeNanos()')\n+  info_ptr->may_skip_backward = false;\n+  info_ptr->may_skip_forward = false;\n+  info_ptr->kind = JVMTI_TIMER_ELAPSED;    \/\/ elapsed not CPU time\n+}\n+\n+intx os::current_thread_id() {\n+  return (intx)pthread_self();\n+}\n+\n+int os::current_process_id() {\n+  return getpid();\n+}\n+\n+\/\/ DLL functions\n+\n+\/\/ This must be hard coded because it's the system's temporary\n+\/\/ directory not the java application's temp directory, ala java.io.tmpdir.\n+const char* os::get_temp_directory() { return \"\/tmp\"; }\n+\n+void os::prepare_native_symbols() {\n+  LoadedLibraries::reload();\n+}\n+\n+\/\/ Check if addr is inside libjvm.so.\n+bool os::address_is_in_vm(address addr) {\n+\n+  \/\/ Input could be a real pc or a function pointer literal. The latter\n+  \/\/ would be a function descriptor residing in the data segment of a module.\n+  loaded_module_t lm;\n+  if (LoadedLibraries::find_for_text_address(addr, &lm)) {\n+    return lm.is_in_vm;\n+  } else if (LoadedLibraries::find_for_data_address(addr, &lm)) {\n+    return lm.is_in_vm;\n+  } else {\n+    return false;\n+  }\n+\n+}\n+\n+\/\/ Resolve an AIX function descriptor literal to a code pointer.\n+\/\/ If the input is a valid code pointer to a text segment of a loaded module,\n+\/\/   it is returned unchanged.\n+\/\/ If the input is a valid AIX function descriptor, it is resolved to the\n+\/\/   code entry point.\n+\/\/ If the input is neither a valid function descriptor nor a valid code pointer,\n+\/\/   null is returned.\n+static address resolve_function_descriptor_to_code_pointer(address p) {\n+\n+  if (LoadedLibraries::find_for_text_address(p, nullptr)) {\n+    \/\/ It is a real code pointer.\n+    return p;\n+  } else if (LoadedLibraries::find_for_data_address(p, nullptr)) {\n+    \/\/ Pointer to data segment, potential function descriptor.\n+    address code_entry = (address)(((FunctionDescriptor*)p)->entry());\n+    if (LoadedLibraries::find_for_text_address(code_entry, nullptr)) {\n+      \/\/ It is a function descriptor.\n+      return code_entry;\n+    }\n+  }\n+\n+  return nullptr;\n+}\n+\n+bool os::dll_address_to_function_name(address addr, char *buf,\n+                                      int buflen, int *offset,\n+                                      bool demangle) {\n+  if (offset) {\n+    *offset = -1;\n+  }\n+  \/\/ Buf is not optional, but offset is optional.\n+  assert(buf != nullptr, \"sanity check\");\n+  buf[0] = '\\0';\n+\n+  \/\/ Resolve function ptr literals first.\n+  addr = resolve_function_descriptor_to_code_pointer(addr);\n+  if (!addr) {\n+    return false;\n+  }\n+\n+  return AixSymbols::get_function_name(addr, buf, buflen, offset, nullptr, demangle);\n+}\n+\n+bool os::dll_address_to_library_name(address addr, char* buf,\n+                                     int buflen, int* offset) {\n+  if (offset) {\n+    *offset = -1;\n+  }\n+  \/\/ Buf is not optional, but offset is optional.\n+  assert(buf != nullptr, \"sanity check\");\n+  buf[0] = '\\0';\n+\n+  \/\/ Resolve function ptr literals first.\n+  addr = resolve_function_descriptor_to_code_pointer(addr);\n+  if (!addr) {\n+    return false;\n+  }\n+\n+  address  base = nullptr;\n+  if (!AixSymbols::get_module_name_and_base(addr, buf, buflen, &base)\n+      || base == nullptr) {\n+    return false;\n+  }\n+  assert(addr >= base && addr <= base + INT_MAX, \"address not in library text range\");\n+  if (offset != nullptr) {\n+    *offset = addr - base;\n+  }\n+\n+  return true;\n+}\n+\n+\/\/ Loads .dll\/.so and in case of error it checks if .dll\/.so was built\n+\/\/ for the same architecture as Hotspot is running on.\n+static void* dll_load_library(const char *filename, char *ebuf, int ebuflen) {\n+\n+  log_info(os)(\"attempting shared library load of %s\", filename);\n+  if (ebuf && ebuflen > 0) {\n+    ebuf[0] = '\\0';\n+    ebuf[ebuflen - 1] = '\\0';\n+  }\n+\n+  if (!filename || strlen(filename) == 0) {\n+    if (ebuf != nullptr && ebuflen > 0) {\n+      ::strncpy(ebuf, \"dll_load: empty filename specified\", ebuflen - 1);\n+    }\n+    return nullptr;\n+  }\n+\n+  \/\/ RTLD_LAZY has currently the same behavior as RTLD_NOW\n+  \/\/ The dl is loaded immediately with all its dependants.\n+  int dflags = RTLD_LAZY;\n+  \/\/ check for filename ending with ')', it indicates we want to load\n+  \/\/ a MEMBER module that is a member of an archive.\n+  int flen = strlen(filename);\n+  if (flen > 0 && filename[flen - 1] == ')') {\n+    dflags |= RTLD_MEMBER;\n+  }\n+\n+  void* result;\n+  const char* error_report = nullptr;\n+  JFR_ONLY(NativeLibraryLoadEvent load_event(filename, &result);)\n+  result = Aix_dlopen(filename, dflags, &error_report);\n+  if (result != nullptr) {\n+    Events::log_dll_message(nullptr, \"Loaded shared library %s\", filename);\n+    \/\/ Reload dll cache. Don't do this in signal handling.\n+    LoadedLibraries::reload();\n+    log_info(os)(\"shared library load of %s was successful\", filename);\n+    return result;\n+  } else {\n+    \/\/ error analysis when dlopen fails\n+    if (error_report == nullptr) {\n+      error_report = \"dlerror returned no error description\";\n+    }\n+    if (ebuf != nullptr && ebuflen > 0) {\n+      snprintf(ebuf, ebuflen - 1, \"%s, LIBPATH=%s, LD_LIBRARY_PATH=%s : %s\",\n+               filename, ::getenv(\"LIBPATH\"), ::getenv(\"LD_LIBRARY_PATH\"), error_report);\n+    }\n+    Events::log_dll_message(nullptr, \"Loading shared library %s failed, %s\", filename, error_report);\n+    log_info(os)(\"shared library load of %s failed, %s\", filename, error_report);\n+    JFR_ONLY(load_event.set_error_msg(error_report);)\n+  }\n+  return nullptr;\n+}\n+\/\/ Load library named <filename>\n+\/\/ If filename matches <name>.so, and loading fails, repeat with <name>.a.\n+void *os::dll_load(const char *filename, char *ebuf, int ebuflen) {\n+  void* result = nullptr;\n+  char* const file_path = strdup(filename);\n+  char* const pointer_to_dot = strrchr(file_path, '.');\n+  const char old_extension[] = \".so\";\n+  const char new_extension[] = \".a\";\n+  STATIC_ASSERT(sizeof(old_extension) >= sizeof(new_extension));\n+  \/\/ First try to load the existing file.\n+  result = dll_load_library(filename, ebuf, ebuflen);\n+  \/\/ If the load fails,we try to reload by changing the extension to .a for .so files only.\n+  \/\/ Shared object in .so format dont have braces, hence they get removed for archives with members.\n+  if (result == nullptr && pointer_to_dot != nullptr && strcmp(pointer_to_dot, old_extension) == 0) {\n+    snprintf(pointer_to_dot, sizeof(old_extension), \"%s\", new_extension);\n+    result = dll_load_library(file_path, ebuf, ebuflen);\n+  }\n+  FREE_C_HEAP_ARRAY(char, file_path);\n+  return result;\n+}\n+\n+void os::print_dll_info(outputStream *st) {\n+  st->print_cr(\"Dynamic libraries:\");\n+  LoadedLibraries::print(st);\n+}\n+\n+void os::get_summary_os_info(char* buf, size_t buflen) {\n+  \/\/ There might be something more readable than uname results for AIX.\n+  struct utsname name;\n+  uname(&name);\n+  snprintf(buf, buflen, \"%s %s\", name.release, name.version);\n+}\n+\n+int os::get_loaded_modules_info(os::LoadedModulesCallbackFunc callback, void *param) {\n+\n+  if (!LoadedLibraries::for_each(callback, param)) {\n+    return -1;\n+  }\n+\n+  return 0;\n+}\n+\n+void os::print_os_info_brief(outputStream* st) {\n+  uint32_t ver = os::Aix::os_version();\n+  st->print_cr(\"AIX kernel version %u.%u.%u.%u\",\n+               (ver >> 24) & 0xFF, (ver >> 16) & 0xFF, (ver >> 8) & 0xFF, ver & 0xFF);\n+\n+  os::Posix::print_uname_info(st);\n+\n+  \/\/ Linux uses print_libversion_info(st); here.\n+}\n+\n+void os::print_os_info(outputStream* st) {\n+  st->print_cr(\"OS:\");\n+\n+  os::Posix::print_uname_info(st);\n+\n+  uint32_t ver = os::Aix::os_version();\n+  st->print_cr(\"AIX kernel version %u.%u.%u.%u\",\n+               (ver >> 24) & 0xFF, (ver >> 16) & 0xFF, (ver >> 8) & 0xFF, ver & 0xFF);\n+\n+  os::Posix::print_uptime_info(st);\n+\n+  os::Posix::print_rlimit_info(st);\n+\n+  os::Posix::print_load_average(st);\n+\n+  \/\/ _SC_THREAD_THREADS_MAX is the maximum number of threads within a process.\n+  long tmax = sysconf(_SC_THREAD_THREADS_MAX);\n+  st->print_cr(\"maximum #threads within a process:%ld\", tmax);\n+\n+  \/\/ print wpar info\n+  libperfstat::wparinfo_t wi;\n+  if (libperfstat::get_wparinfo(&wi)) {\n+    st->print_cr(\"wpar info\");\n+    st->print_cr(\"name: %s\", wi.name);\n+    st->print_cr(\"id:   %d\", wi.wpar_id);\n+    st->print_cr(\"type: %s\", (wi.app_wpar ? \"application\" : \"system\"));\n+  }\n+\n+  VM_Version::print_platform_virtualization_info(st);\n+}\n+\n+void os::print_memory_info(outputStream* st) {\n+\n+  st->print_cr(\"Memory:\");\n+\n+  st->print_cr(\"  Base page size (sysconf _SC_PAGESIZE):  %s\",\n+    describe_pagesize(g_multipage_support.pagesize));\n+  st->print_cr(\"  Data page size (C-Heap, bss, etc):      %s\",\n+    describe_pagesize(g_multipage_support.datapsize));\n+  st->print_cr(\"  Text page size:                         %s\",\n+    describe_pagesize(g_multipage_support.textpsize));\n+  st->print_cr(\"  Thread stack page size (pthread):       %s\",\n+    describe_pagesize(g_multipage_support.pthr_stack_pagesize));\n+  st->print_cr(\"  Default shared memory page size:        %s\",\n+    describe_pagesize(g_multipage_support.shmpsize));\n+  st->print_cr(\"  Can use 64K pages dynamically with shared memory:  %s\",\n+    (g_multipage_support.can_use_64K_pages ? \"yes\" :\"no\"));\n+  st->print_cr(\"  Can use 16M pages dynamically with shared memory: %s\",\n+    (g_multipage_support.can_use_16M_pages ? \"yes\" :\"no\"));\n+  st->print_cr(\"  Multipage error: %d\",\n+    g_multipage_support.error);\n+  st->cr();\n+  st->print_cr(\"  os::vm_page_size:       %s\", describe_pagesize(os::vm_page_size()));\n+\n+  \/\/ print out LDR_CNTRL because it affects the default page sizes\n+  const char* const ldr_cntrl = ::getenv(\"LDR_CNTRL\");\n+  st->print_cr(\"  LDR_CNTRL=%s.\", ldr_cntrl ? ldr_cntrl : \"<unset>\");\n+\n+  \/\/ Print out EXTSHM because it is an unsupported setting.\n+  const char* const extshm = ::getenv(\"EXTSHM\");\n+  st->print_cr(\"  EXTSHM=%s.\", extshm ? extshm : \"<unset>\");\n+  if ( (strcmp(extshm, \"on\") == 0) || (strcmp(extshm, \"ON\") == 0) ) {\n+    st->print_cr(\"  *** Unsupported! Please remove EXTSHM from your environment! ***\");\n+  }\n+\n+  \/\/ Print out AIXTHREAD_GUARDPAGES because it affects the size of pthread stacks.\n+  const char* const aixthread_guardpages = ::getenv(\"AIXTHREAD_GUARDPAGES\");\n+  st->print_cr(\"  AIXTHREAD_GUARDPAGES=%s.\",\n+      aixthread_guardpages ? aixthread_guardpages : \"<unset>\");\n+  st->cr();\n+\n+  os::Aix::meminfo_t mi;\n+  if (os::Aix::get_meminfo(&mi)) {\n+    if (os::Aix::on_aix()) {\n+      st->print_cr(\"physical total : \" SIZE_FORMAT, mi.real_total);\n+      st->print_cr(\"physical free  : \" SIZE_FORMAT, mi.real_free);\n+      st->print_cr(\"swap total     : \" SIZE_FORMAT, mi.pgsp_total);\n+      st->print_cr(\"swap free      : \" SIZE_FORMAT, mi.pgsp_free);\n+    } else {\n+      \/\/ PASE - Numbers are result of QWCRSSTS; they mean:\n+      \/\/ real_total: Sum of all system pools\n+      \/\/ real_free: always 0\n+      \/\/ pgsp_total: we take the size of the system ASP\n+      \/\/ pgsp_free: size of system ASP times percentage of system ASP unused\n+      st->print_cr(\"physical total     : \" SIZE_FORMAT, mi.real_total);\n+      st->print_cr(\"system asp total   : \" SIZE_FORMAT, mi.pgsp_total);\n+      st->print_cr(\"%% system asp used : %.2f\",\n+        mi.pgsp_total ? (100.0f * (mi.pgsp_total - mi.pgsp_free) \/ mi.pgsp_total) : -1.0f);\n+    }\n+  }\n+  st->cr();\n+\n+  \/\/ Print program break.\n+  st->print_cr(\"Program break at VM startup: \" PTR_FORMAT \".\", p2i(g_brk_at_startup));\n+  address brk_now = (address)::sbrk(0);\n+  if (brk_now != (address)-1) {\n+    st->print_cr(\"Program break now          : \" PTR_FORMAT \" (distance: \" SIZE_FORMAT \"k).\",\n+                 p2i(brk_now), (size_t)((brk_now - g_brk_at_startup) \/ K));\n+  }\n+  st->print_cr(\"MaxExpectedDataSegmentSize    : \" SIZE_FORMAT \"k.\", MaxExpectedDataSegmentSize \/ K);\n+  st->cr();\n+\n+  \/\/ Print segments allocated with os::reserve_memory.\n+  st->print_cr(\"internal virtual memory regions used by vm:\");\n+  vmembk_print_on(st);\n+}\n+\n+\/\/ Get a string for the cpuinfo that is a summary of the cpu type\n+void os::get_summary_cpu_info(char* buf, size_t buflen) {\n+  \/\/ read _system_configuration.version\n+  switch (_system_configuration.version) {\n+  case PV_9:\n+    strncpy(buf, \"Power PC 9\", buflen);\n+    break;\n+  case PV_8:\n+    strncpy(buf, \"Power PC 8\", buflen);\n+    break;\n+  case PV_7:\n+    strncpy(buf, \"Power PC 7\", buflen);\n+    break;\n+  case PV_6_1:\n+    strncpy(buf, \"Power PC 6 DD1.x\", buflen);\n+    break;\n+  case PV_6:\n+    strncpy(buf, \"Power PC 6\", buflen);\n+    break;\n+  case PV_5:\n+    strncpy(buf, \"Power PC 5\", buflen);\n+    break;\n+  case PV_5_2:\n+    strncpy(buf, \"Power PC 5_2\", buflen);\n+    break;\n+  case PV_5_3:\n+    strncpy(buf, \"Power PC 5_3\", buflen);\n+    break;\n+  case PV_5_Compat:\n+    strncpy(buf, \"PV_5_Compat\", buflen);\n+    break;\n+  case PV_6_Compat:\n+    strncpy(buf, \"PV_6_Compat\", buflen);\n+    break;\n+  case PV_7_Compat:\n+    strncpy(buf, \"PV_7_Compat\", buflen);\n+    break;\n+  case PV_8_Compat:\n+    strncpy(buf, \"PV_8_Compat\", buflen);\n+    break;\n+  case PV_9_Compat:\n+    strncpy(buf, \"PV_9_Compat\", buflen);\n+    break;\n+  default:\n+    strncpy(buf, \"unknown\", buflen);\n+  }\n+}\n+\n+void os::pd_print_cpu_info(outputStream* st, char* buf, size_t buflen) {\n+  \/\/ Nothing to do beyond of what os::print_cpu_info() does.\n+}\n+\n+static char saved_jvm_path[MAXPATHLEN] = {0};\n+\n+\/\/ Find the full path to the current module, libjvm.so.\n+void os::jvm_path(char *buf, jint buflen) {\n+  \/\/ Error checking.\n+  if (buflen < MAXPATHLEN) {\n+    assert(false, \"must use a large-enough buffer\");\n+    buf[0] = '\\0';\n+    return;\n+  }\n+  \/\/ Lazy resolve the path to current module.\n+  if (saved_jvm_path[0] != 0) {\n+    strcpy(buf, saved_jvm_path);\n+    return;\n+  }\n+\n+  Dl_info dlinfo;\n+  int ret = dladdr(CAST_FROM_FN_PTR(void *, os::jvm_path), &dlinfo);\n+  assert(ret != 0, \"cannot locate libjvm\");\n+  char* rp = os::Posix::realpath((char *)dlinfo.dli_fname, buf, buflen);\n+  assert(rp != nullptr, \"error in realpath(): maybe the 'path' argument is too long?\");\n+\n+  if (Arguments::sun_java_launcher_is_altjvm()) {\n+    \/\/ Support for the java launcher's '-XXaltjvm=<path>' option. Typical\n+    \/\/ value for buf is \"<JAVA_HOME>\/jre\/lib\/<vmtype>\/libjvm.so\".\n+    \/\/ If \"\/jre\/lib\/\" appears at the right place in the string, then\n+    \/\/ assume we are installed in a JDK and we're done. Otherwise, check\n+    \/\/ for a JAVA_HOME environment variable and fix up the path so it\n+    \/\/ looks like libjvm.so is installed there (append a fake suffix\n+    \/\/ hotspot\/libjvm.so).\n+    const char *p = buf + strlen(buf) - 1;\n+    for (int count = 0; p > buf && count < 4; ++count) {\n+      for (--p; p > buf && *p != '\/'; --p)\n+        \/* empty *\/ ;\n+    }\n+\n+    if (strncmp(p, \"\/jre\/lib\/\", 9) != 0) {\n+      \/\/ Look for JAVA_HOME in the environment.\n+      char* java_home_var = ::getenv(\"JAVA_HOME\");\n+      if (java_home_var != nullptr && java_home_var[0] != 0) {\n+        char* jrelib_p;\n+        int len;\n+\n+        \/\/ Check the current module name \"libjvm.so\".\n+        p = strrchr(buf, '\/');\n+        if (p == nullptr) {\n+          return;\n+        }\n+        assert(strstr(p, \"\/libjvm\") == p, \"invalid library name\");\n+\n+        rp = os::Posix::realpath(java_home_var, buf, buflen);\n+        if (rp == nullptr) {\n+          return;\n+        }\n+\n+        \/\/ determine if this is a legacy image or modules image\n+        \/\/ modules image doesn't have \"jre\" subdirectory\n+        len = strlen(buf);\n+        assert(len < buflen, \"Ran out of buffer room\");\n+        jrelib_p = buf + len;\n+        snprintf(jrelib_p, buflen-len, \"\/jre\/lib\");\n+        if (0 != access(buf, F_OK)) {\n+          snprintf(jrelib_p, buflen-len, \"\/lib\");\n+        }\n+\n+        if (0 == access(buf, F_OK)) {\n+          \/\/ Use current module name \"libjvm.so\"\n+          len = strlen(buf);\n+          snprintf(buf + len, buflen-len, \"\/hotspot\/libjvm.so\");\n+        } else {\n+          \/\/ Go back to path of .so\n+          rp = os::Posix::realpath((char *)dlinfo.dli_fname, buf, buflen);\n+          if (rp == nullptr) {\n+            return;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  strncpy(saved_jvm_path, buf, sizeof(saved_jvm_path));\n+  saved_jvm_path[sizeof(saved_jvm_path) - 1] = '\\0';\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/ Virtual Memory\n+\n+\/\/ We need to keep small simple bookkeeping for os::reserve_memory and friends.\n+\n+#define VMEM_MAPPED  1\n+#define VMEM_SHMATED 2\n+\n+struct vmembk_t {\n+  int type;         \/\/ 1 - mmap, 2 - shmat\n+  char* addr;\n+  size_t size;      \/\/ Real size, may be larger than usersize.\n+  size_t pagesize;  \/\/ page size of area\n+  vmembk_t* next;\n+\n+  bool contains_addr(char* p) const {\n+    return p >= addr && p < (addr + size);\n+  }\n+\n+  bool contains_range(char* p, size_t s) const {\n+    return contains_addr(p) && contains_addr(p + s - 1);\n+  }\n+\n+  void print_on(outputStream* os) const {\n+    os->print(\"[\" PTR_FORMAT \" - \" PTR_FORMAT \"] (\" UINTX_FORMAT\n+      \" bytes, %d %s pages), %s\",\n+      addr, addr + size - 1, size, size \/ pagesize, describe_pagesize(pagesize),\n+      (type == VMEM_SHMATED ? \"shmat\" : \"mmap\")\n+    );\n+  }\n+\n+  \/\/ Check that range is a sub range of memory block (or equal to memory block);\n+  \/\/ also check that range is fully page aligned to the page size if the block.\n+  void assert_is_valid_subrange(char* p, size_t s) const {\n+    if (!contains_range(p, s)) {\n+      trcVerbose(\"[\" PTR_FORMAT \" - \" PTR_FORMAT \"] is not a sub \"\n+              \"range of [\" PTR_FORMAT \" - \" PTR_FORMAT \"].\",\n+              p2i(p), p2i(p + s), p2i(addr), p2i(addr + size));\n+      guarantee0(false);\n+    }\n+    if (!is_aligned_to(p, pagesize) || !is_aligned_to(p + s, pagesize)) {\n+      trcVerbose(\"range [\" PTR_FORMAT \" - \" PTR_FORMAT \"] is not\"\n+              \" aligned to pagesize (%lu)\", p2i(p), p2i(p + s), (unsigned long) pagesize);\n+      guarantee0(false);\n+    }\n+  }\n+};\n+\n+static struct {\n+  vmembk_t* first;\n+  MiscUtils::CritSect cs;\n+} vmem;\n+\n+static void vmembk_add(char* addr, size_t size, size_t pagesize, int type) {\n+  vmembk_t* p = (vmembk_t*) ::malloc(sizeof(vmembk_t));\n+  assert0(p);\n+  if (p) {\n+    MiscUtils::AutoCritSect lck(&vmem.cs);\n+    p->addr = addr; p->size = size;\n+    p->pagesize = pagesize;\n+    p->type = type;\n+    p->next = vmem.first;\n+    vmem.first = p;\n+  }\n+}\n+\n+static vmembk_t* vmembk_find(char* addr) {\n+  MiscUtils::AutoCritSect lck(&vmem.cs);\n+  for (vmembk_t* p = vmem.first; p; p = p->next) {\n+    if (p->addr <= addr && (p->addr + p->size) > addr) {\n+      return p;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+static void vmembk_remove(vmembk_t* p0) {\n+  MiscUtils::AutoCritSect lck(&vmem.cs);\n+  assert0(p0);\n+  assert0(vmem.first); \/\/ List should not be empty.\n+  for (vmembk_t** pp = &(vmem.first); *pp; pp = &((*pp)->next)) {\n+    if (*pp == p0) {\n+      *pp = p0->next;\n+      ::free(p0);\n+      return;\n+    }\n+  }\n+  assert0(false); \/\/ Not found?\n+}\n+\n+static void vmembk_print_on(outputStream* os) {\n+  MiscUtils::AutoCritSect lck(&vmem.cs);\n+  for (vmembk_t* vmi = vmem.first; vmi; vmi = vmi->next) {\n+    vmi->print_on(os);\n+    os->cr();\n+  }\n+}\n+\n+\/\/ Reserve and attach a section of System V memory.\n+\/\/ If <requested_addr> is not null, function will attempt to attach the memory at the given\n+\/\/ address. Failing that, it will attach the memory anywhere.\n+\/\/ If <requested_addr> is null, function will attach the memory anywhere.\n+static char* reserve_shmated_memory (size_t bytes, char* requested_addr) {\n+\n+  trcVerbose(\"reserve_shmated_memory \" UINTX_FORMAT \" bytes, wishaddress \"\n+    PTR_FORMAT \"...\", bytes, p2i(requested_addr));\n+\n+  \/\/ We must prevent anyone from attaching too close to the\n+  \/\/ BRK because that may cause malloc OOM.\n+  if (requested_addr != nullptr && is_close_to_brk((address)requested_addr)) {\n+    trcVerbose(\"Wish address \" PTR_FORMAT \" is too close to the BRK segment.\", p2i(requested_addr));\n+    \/\/ Since we treat an attach to the wrong address as an error later anyway,\n+    \/\/ we return null here\n+    return nullptr;\n+  }\n+\n+  \/\/ For old AS\/400's (V5R4 and older) we should not even be here - System V shared memory is not\n+  \/\/ really supported (max size 4GB), so reserve_mmapped_memory should have been used instead.\n+  if (os::Aix::on_pase_V5R4_or_older()) {\n+    ShouldNotReachHere();\n+  }\n+\n+  \/\/ Align size of shm up to 64K to avoid errors if we later try to change the page size.\n+  const size_t size = align_up(bytes, 64*K);\n+\n+  \/\/ Reserve the shared segment.\n+  int shmid = shmget(IPC_PRIVATE, size, IPC_CREAT | S_IRUSR | S_IWUSR);\n+  if (shmid == -1) {\n+    trcVerbose(\"shmget(.., \" UINTX_FORMAT \", ..) failed (errno: %d).\", size, errno);\n+    return nullptr;\n+  }\n+\n+  \/\/ Important note:\n+  \/\/ It is very important that we, upon leaving this function, do not leave a shm segment alive.\n+  \/\/ We must right after attaching it remove it from the system. System V shm segments are global and\n+  \/\/ survive the process.\n+  \/\/ So, from here on: Do not assert, do not return, until we have called shmctl(IPC_RMID) (A).\n+\n+  struct shmid_ds shmbuf;\n+  memset(&shmbuf, 0, sizeof(shmbuf));\n+  shmbuf.shm_pagesize = 64*K;\n+  if (shmctl(shmid, SHM_PAGESIZE, &shmbuf) != 0) {\n+    trcVerbose(\"Failed to set page size (need \" UINTX_FORMAT \" 64K pages) - shmctl failed with %d.\",\n+               size \/ (64*K), errno);\n+    \/\/ I want to know if this ever happens.\n+    assert(false, \"failed to set page size for shmat\");\n+  }\n+\n+  \/\/ Now attach the shared segment.\n+  \/\/ Note that we deliberately *don't* pass SHM_RND. The contract of os::attempt_reserve_memory_at() -\n+  \/\/ which invokes this function with a request address != nullptr - is to map at the specified address\n+  \/\/ excactly, or to fail. If the caller passed us an address that is not usable (aka not a valid segment\n+  \/\/ boundary), shmat should not round down the address, or think up a completely new one.\n+  \/\/ (In places where this matters, e.g. when reserving the heap, we take care of passing segment-aligned\n+  \/\/ addresses on Aix. See, e.g., ReservedHeapSpace.\n+  char* const addr = (char*) shmat(shmid, requested_addr, 0);\n+  const int errno_shmat = errno;\n+\n+  \/\/ (A) Right after shmat and before handing shmat errors delete the shm segment.\n+  if (::shmctl(shmid, IPC_RMID, nullptr) == -1) {\n+    trcVerbose(\"shmctl(%u, IPC_RMID) failed (%d)\\n\", shmid, errno);\n+    assert(false, \"failed to remove shared memory segment!\");\n+  }\n+\n+  \/\/ Handle shmat error. If we failed to attach, just return.\n+  if (addr == (char*)-1) {\n+    trcVerbose(\"Failed to attach segment at \" PTR_FORMAT \" (%d).\", p2i(requested_addr), errno_shmat);\n+    return nullptr;\n+  }\n+\n+  \/\/ Just for info: query the real page size. In case setting the page size did not\n+  \/\/ work (see above), the system may have given us something other then 4K (LDR_CNTRL).\n+  const size_t real_pagesize = os::Aix::query_pagesize(addr);\n+  if (real_pagesize != (size_t)shmbuf.shm_pagesize) {\n+    trcVerbose(\"pagesize is, surprisingly, \" SIZE_FORMAT, real_pagesize);\n+  }\n+\n+  if (addr) {\n+    trcVerbose(\"shm-allocated \" PTR_FORMAT \" .. \" PTR_FORMAT \" (\" UINTX_FORMAT \" bytes, \" UINTX_FORMAT \" %s pages)\",\n+      p2i(addr), p2i(addr + size - 1), size, size\/real_pagesize, describe_pagesize(real_pagesize));\n+  } else {\n+    if (requested_addr != nullptr) {\n+      trcVerbose(\"failed to shm-allocate \" UINTX_FORMAT \" bytes at with address \" PTR_FORMAT \".\", size, p2i(requested_addr));\n+    } else {\n+      trcVerbose(\"failed to shm-allocate \" UINTX_FORMAT \" bytes at any address.\", size);\n+    }\n+  }\n+\n+  \/\/ book-keeping\n+  vmembk_add(addr, size, real_pagesize, VMEM_SHMATED);\n+  assert0(is_aligned_to(addr, os::vm_page_size()));\n+\n+  return addr;\n+}\n+\n+static bool release_shmated_memory(char* addr, size_t size) {\n+\n+  trcVerbose(\"release_shmated_memory [\" PTR_FORMAT \" - \" PTR_FORMAT \"].\",\n+    p2i(addr), p2i(addr + size - 1));\n+\n+  bool rc = false;\n+\n+  \/\/ TODO: is there a way to verify shm size without doing bookkeeping?\n+  if (::shmdt(addr) != 0) {\n+    trcVerbose(\"error (%d).\", errno);\n+  } else {\n+    trcVerbose(\"ok.\");\n+    rc = true;\n+  }\n+  return rc;\n+}\n+\n+static bool uncommit_shmated_memory(char* addr, size_t size) {\n+  trcVerbose(\"uncommit_shmated_memory [\" PTR_FORMAT \" - \" PTR_FORMAT \"].\",\n+    p2i(addr), p2i(addr + size - 1));\n+\n+  const bool rc = my_disclaim64(addr, size);\n+\n+  if (!rc) {\n+    trcVerbose(\"my_disclaim64(\" PTR_FORMAT \", \" UINTX_FORMAT \") failed.\\n\", p2i(addr), size);\n+    return false;\n+  }\n+  return true;\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/  mmap-based routines \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/\/ Reserve memory via mmap.\n+\/\/ If <requested_addr> is given, an attempt is made to attach at the given address.\n+\/\/ Failing that, memory is allocated at any address.\n+static char* reserve_mmaped_memory(size_t bytes, char* requested_addr) {\n+  trcVerbose(\"reserve_mmaped_memory \" UINTX_FORMAT \" bytes, wishaddress \" PTR_FORMAT \"...\",\n+    bytes, p2i(requested_addr));\n+\n+  if (requested_addr && !is_aligned_to(requested_addr, os::vm_page_size()) != 0) {\n+    trcVerbose(\"Wish address \" PTR_FORMAT \" not aligned to page boundary.\", p2i(requested_addr));\n+    return nullptr;\n+  }\n+\n+  \/\/ We must prevent anyone from attaching too close to the\n+  \/\/ BRK because that may cause malloc OOM.\n+  if (requested_addr != nullptr && is_close_to_brk((address)requested_addr)) {\n+    trcVerbose(\"Wish address \" PTR_FORMAT \" is too close to the BRK segment.\", p2i(requested_addr));\n+    \/\/ Since we treat an attach to the wrong address as an error later anyway,\n+    \/\/ we return null here\n+    return nullptr;\n+  }\n+\n+  \/\/ In 64K mode, we lie and claim the global page size (os::vm_page_size()) is 64K\n+  \/\/  (complicated story). This mostly works just fine since 64K is a multiple of the\n+  \/\/  actual 4K lowest page size. Only at a few seams light shines thru, e.g. when\n+  \/\/  calling mmap. mmap will return memory aligned to the lowest pages size - 4K -\n+  \/\/  so we must make sure - transparently - that the caller only ever sees 64K\n+  \/\/  aligned mapping start addresses.\n+  const size_t alignment = os::vm_page_size();\n+\n+  \/\/ Size shall always be a multiple of os::vm_page_size (esp. in 64K mode).\n+  const size_t size = align_up(bytes, os::vm_page_size());\n+\n+  \/\/ alignment: Allocate memory large enough to include an aligned range of the right size and\n+  \/\/ cut off the leading and trailing waste pages.\n+  assert0(alignment != 0 && is_aligned_to(alignment, os::vm_page_size())); \/\/ see above\n+  const size_t extra_size = size + alignment;\n+\n+  \/\/ Note: MAP_SHARED (instead of MAP_PRIVATE) needed to be able to\n+  \/\/ later use msync(MS_INVALIDATE) (see os::uncommit_memory).\n+  int flags = MAP_ANONYMOUS | MAP_SHARED;\n+\n+  \/\/ MAP_FIXED is needed to enforce requested_addr - manpage is vague about what\n+  \/\/ it means if wishaddress is given but MAP_FIXED is not set.\n+  \/\/\n+  \/\/ Important! Behaviour differs depending on whether SPEC1170 mode is active or not.\n+  \/\/ SPEC1170 mode active: behaviour like POSIX, MAP_FIXED will clobber existing mappings.\n+  \/\/ SPEC1170 mode not active: behaviour, unlike POSIX, is that no existing mappings will\n+  \/\/ get clobbered.\n+  if (requested_addr != nullptr) {\n+    if (!os::Aix::xpg_sus_mode()) {  \/\/ not SPEC1170 Behaviour\n+      flags |= MAP_FIXED;\n+    }\n+  }\n+\n+  char* addr = (char*)::mmap(requested_addr, extra_size,\n+      PROT_READ|PROT_WRITE|PROT_EXEC, flags, -1, 0);\n+\n+  if (addr == MAP_FAILED) {\n+    trcVerbose(\"mmap(\" PTR_FORMAT \", \" UINTX_FORMAT \", ..) failed (%d)\", p2i(requested_addr), size, errno);\n+    return nullptr;\n+  } else if (requested_addr != nullptr && addr != requested_addr) {\n+    trcVerbose(\"mmap(\" PTR_FORMAT \", \" UINTX_FORMAT \", ..) succeeded, but at a different address than requested (\" PTR_FORMAT \"), will unmap\",\n+               p2i(requested_addr), size, p2i(addr));\n+    ::munmap(addr, extra_size);\n+    return nullptr;\n+  }\n+\n+  \/\/ Handle alignment.\n+  char* const addr_aligned = align_up(addr, alignment);\n+  const size_t waste_pre = addr_aligned - addr;\n+  char* const addr_aligned_end = addr_aligned + size;\n+  const size_t waste_post = extra_size - waste_pre - size;\n+  if (waste_pre > 0) {\n+    ::munmap(addr, waste_pre);\n+  }\n+  if (waste_post > 0) {\n+    ::munmap(addr_aligned_end, waste_post);\n+  }\n+  addr = addr_aligned;\n+\n+  trcVerbose(\"mmap-allocated \" PTR_FORMAT \" .. \" PTR_FORMAT \" (\" UINTX_FORMAT \" bytes)\",\n+    p2i(addr), p2i(addr + bytes), bytes);\n+\n+  \/\/ bookkeeping\n+  vmembk_add(addr, size, 4*K, VMEM_MAPPED);\n+\n+  \/\/ Test alignment, see above.\n+  assert0(is_aligned_to(addr, os::vm_page_size()));\n+\n+  return addr;\n+}\n+\n+static bool release_mmaped_memory(char* addr, size_t size) {\n+  assert0(is_aligned_to(addr, os::vm_page_size()));\n+  assert0(is_aligned_to(size, os::vm_page_size()));\n+\n+  trcVerbose(\"release_mmaped_memory [\" PTR_FORMAT \" - \" PTR_FORMAT \"].\",\n+    p2i(addr), p2i(addr + size - 1));\n+  bool rc = false;\n+\n+  if (::munmap(addr, size) != 0) {\n+    trcVerbose(\"failed (%d)\\n\", errno);\n+    rc = false;\n+  } else {\n+    trcVerbose(\"ok.\");\n+    rc = true;\n+  }\n+\n+  return rc;\n+}\n+\n+static bool uncommit_mmaped_memory(char* addr, size_t size) {\n+\n+  assert0(is_aligned_to(addr, os::vm_page_size()));\n+  assert0(is_aligned_to(size, os::vm_page_size()));\n+\n+  trcVerbose(\"uncommit_mmaped_memory [\" PTR_FORMAT \" - \" PTR_FORMAT \"].\",\n+    p2i(addr), p2i(addr + size - 1));\n+  bool rc = false;\n+\n+  \/\/ Uncommit mmap memory with msync MS_INVALIDATE.\n+  if (::msync(addr, size, MS_INVALIDATE) != 0) {\n+    trcVerbose(\"failed (%d)\\n\", errno);\n+    rc = false;\n+  } else {\n+    trcVerbose(\"ok.\");\n+    rc = true;\n+  }\n+\n+  return rc;\n+}\n+\n+#ifdef PRODUCT\n+static void warn_fail_commit_memory(char* addr, size_t size, bool exec,\n+                                    int err) {\n+  warning(\"INFO: os::commit_memory(\" PTR_FORMAT \", \" SIZE_FORMAT\n+          \", %d) failed; error='%s' (errno=%d)\", p2i(addr), size, exec,\n+          os::errno_name(err), err);\n+}\n+#endif\n+\n+void os::pd_commit_memory_or_exit(char* addr, size_t size, bool exec,\n+                                  const char* mesg) {\n+  assert(mesg != nullptr, \"mesg must be specified\");\n+  if (!pd_commit_memory(addr, size, exec)) {\n+    \/\/ Add extra info in product mode for vm_exit_out_of_memory():\n+    PRODUCT_ONLY(warn_fail_commit_memory(addr, size, exec, errno);)\n+    vm_exit_out_of_memory(size, OOM_MMAP_ERROR, \"%s\", mesg);\n+  }\n+}\n+\n+bool os::pd_commit_memory(char* addr, size_t size, bool exec) {\n+\n+  assert(is_aligned_to(addr, os::vm_page_size()),\n+    \"addr \" PTR_FORMAT \" not aligned to vm_page_size (\" SIZE_FORMAT \")\",\n+    p2i(addr), os::vm_page_size());\n+  assert(is_aligned_to(size, os::vm_page_size()),\n+    \"size \" PTR_FORMAT \" not aligned to vm_page_size (\" SIZE_FORMAT \")\",\n+    size, os::vm_page_size());\n+\n+  vmembk_t* const vmi = vmembk_find(addr);\n+  guarantee0(vmi);\n+  vmi->assert_is_valid_subrange(addr, size);\n+\n+  trcVerbose(\"commit_memory [\" PTR_FORMAT \" - \" PTR_FORMAT \"].\", p2i(addr), p2i(addr + size - 1));\n+\n+  if (UseExplicitCommit) {\n+    \/\/ AIX commits memory on touch. So, touch all pages to be committed.\n+    for (char* p = addr; p < (addr + size); p += 4*K) {\n+      *p = '\\0';\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+bool os::pd_commit_memory(char* addr, size_t size, size_t alignment_hint, bool exec) {\n+  return pd_commit_memory(addr, size, exec);\n+}\n+\n+void os::pd_commit_memory_or_exit(char* addr, size_t size,\n+                                  size_t alignment_hint, bool exec,\n+                                  const char* mesg) {\n+  \/\/ Alignment_hint is ignored on this OS.\n+  pd_commit_memory_or_exit(addr, size, exec, mesg);\n+}\n+\n+bool os::pd_uncommit_memory(char* addr, size_t size, bool exec) {\n+  assert(is_aligned_to(addr, os::vm_page_size()),\n+    \"addr \" PTR_FORMAT \" not aligned to vm_page_size (\" SIZE_FORMAT \")\",\n+    p2i(addr), os::vm_page_size());\n+  assert(is_aligned_to(size, os::vm_page_size()),\n+    \"size \" PTR_FORMAT \" not aligned to vm_page_size (\" SIZE_FORMAT \")\",\n+    size, os::vm_page_size());\n+\n+  \/\/ Dynamically do different things for mmap\/shmat.\n+  const vmembk_t* const vmi = vmembk_find(addr);\n+  guarantee0(vmi);\n+  vmi->assert_is_valid_subrange(addr, size);\n+\n+  if (vmi->type == VMEM_SHMATED) {\n+    return uncommit_shmated_memory(addr, size);\n+  } else {\n+    return uncommit_mmaped_memory(addr, size);\n+  }\n+}\n+\n+bool os::pd_create_stack_guard_pages(char* addr, size_t size) {\n+  \/\/ Do not call this; no need to commit stack pages on AIX.\n+  ShouldNotReachHere();\n+  return true;\n+}\n+\n+bool os::remove_stack_guard_pages(char* addr, size_t size) {\n+  \/\/ Do not call this; no need to commit stack pages on AIX.\n+  ShouldNotReachHere();\n+  return true;\n+}\n+\n+void os::pd_realign_memory(char *addr, size_t bytes, size_t alignment_hint) {\n+}\n+\n+void os::pd_free_memory(char *addr, size_t bytes, size_t alignment_hint) {\n+}\n+\n+size_t os::pd_pretouch_memory(void* first, void* last, size_t page_size) {\n+  return page_size;\n+}\n+\n+void os::numa_make_global(char *addr, size_t bytes) {\n+}\n+\n+void os::numa_make_local(char *addr, size_t bytes, int lgrp_hint) {\n+}\n+\n+bool os::numa_topology_changed() {\n+  return false;\n+}\n+\n+size_t os::numa_get_groups_num() {\n+  return 1;\n+}\n+\n+int os::numa_get_group_id() {\n+  return 0;\n+}\n+\n+size_t os::numa_get_leaf_groups(uint *ids, size_t size) {\n+  if (size > 0) {\n+    ids[0] = 0;\n+    return 1;\n+  }\n+  return 0;\n+}\n+\n+int os::numa_get_group_id_for_address(const void* address) {\n+  return 0;\n+}\n+\n+bool os::numa_get_group_ids_for_range(const void** addresses, int* lgrp_ids, size_t count) {\n+  return false;\n+}\n+\n+\/\/ Reserves and attaches a shared memory segment.\n+char* os::pd_reserve_memory(size_t bytes, bool exec) {\n+  \/\/ Always round to os::vm_page_size(), which may be larger than 4K.\n+  bytes = align_up(bytes, os::vm_page_size());\n+\n+  \/\/ In 4K mode always use mmap.\n+  \/\/ In 64K mode allocate small sizes with mmap, large ones with 64K shmatted.\n+  if (os::vm_page_size() == 4*K) {\n+    return reserve_mmaped_memory(bytes, nullptr \/* requested_addr *\/);\n+  } else {\n+    if (bytes >= Use64KPagesThreshold) {\n+      return reserve_shmated_memory(bytes, nullptr \/* requested_addr *\/);\n+    } else {\n+      return reserve_mmaped_memory(bytes, nullptr \/* requested_addr *\/);\n+    }\n+  }\n+}\n+\n+bool os::pd_release_memory(char* addr, size_t size) {\n+\n+  \/\/ Dynamically do different things for mmap\/shmat.\n+  vmembk_t* const vmi = vmembk_find(addr);\n+  guarantee0(vmi);\n+  vmi->assert_is_valid_subrange(addr, size);\n+\n+  \/\/ Always round to os::vm_page_size(), which may be larger than 4K.\n+  size = align_up(size, os::vm_page_size());\n+  addr = align_up(addr, os::vm_page_size());\n+\n+  bool rc = false;\n+  bool remove_bookkeeping = false;\n+  if (vmi->type == VMEM_SHMATED) {\n+    \/\/ For shmatted memory, we do:\n+    \/\/ - If user wants to release the whole range, release the memory (shmdt).\n+    \/\/ - If user only wants to release a partial range, uncommit (disclaim) that\n+    \/\/   range. That way, at least, we do not use memory anymore (bust still page\n+    \/\/   table space).\n+    if (addr == vmi->addr && size == vmi->size) {\n+      rc = release_shmated_memory(addr, size);\n+      remove_bookkeeping = true;\n+    } else {\n+      rc = uncommit_shmated_memory(addr, size);\n+    }\n+  } else {\n+    \/\/ In mmap-mode:\n+    \/\/  - If the user wants to release the full range, we do that and remove the mapping.\n+    \/\/  - If the user wants to release part of the range, we release that part, but need\n+    \/\/    to adjust bookkeeping.\n+    assert(is_aligned(size, 4 * K), \"Sanity\");\n+    rc = release_mmaped_memory(addr, size);\n+    if (addr == vmi->addr && size == vmi->size) {\n+      remove_bookkeeping = true;\n+    } else {\n+      if (addr == vmi->addr && size < vmi->size) {\n+        \/\/ Chopped from head\n+        vmi->addr += size;\n+        vmi->size -= size;\n+      } else if (addr + size == vmi->addr + vmi->size) {\n+        \/\/ Chopped from tail\n+        vmi->size -= size;\n+      } else {\n+        \/\/ releasing a mapping in the middle of the original mapping:\n+        \/\/ For now we forbid this, since this is an invalid scenario\n+        \/\/ (the bookkeeping is easy enough to fix if needed but there\n+        \/\/  is no use case for it; any occurrence is likely an error.\n+        ShouldNotReachHere();\n+      }\n+    }\n+  }\n+\n+  \/\/ update bookkeeping\n+  if (rc && remove_bookkeeping) {\n+    vmembk_remove(vmi);\n+  }\n+\n+  return rc;\n+}\n+\n+static bool checked_mprotect(char* addr, size_t size, int prot) {\n+\n+  \/\/ Little problem here: if SPEC1170 behaviour is off, mprotect() on AIX will\n+  \/\/ not tell me if protection failed when trying to protect an un-protectable range.\n+  \/\/\n+  \/\/ This means if the memory was allocated using shmget\/shmat, protection won't work\n+  \/\/ but mprotect will still return 0:\n+  \/\/\n+  \/\/ See http:\/\/publib.boulder.ibm.com\/infocenter\/pseries\/v5r3\/index.jsp?topic=\/com.ibm.aix.basetechref\/doc\/basetrf1\/mprotect.htm\n+\n+  Events::log(nullptr, \"Protecting memory [\" INTPTR_FORMAT \",\" INTPTR_FORMAT \"] with protection modes %x\", p2i(addr), p2i(addr+size), prot);\n+  bool rc = ::mprotect(addr, size, prot) == 0 ? true : false;\n+\n+  if (!rc) {\n+    const char* const s_errno = os::errno_name(errno);\n+    warning(\"mprotect(\" PTR_FORMAT \"-\" PTR_FORMAT \", 0x%X) failed (%s).\", addr, addr + size, prot, s_errno);\n+    return false;\n+  }\n+\n+  \/\/ mprotect success check\n+  \/\/\n+  \/\/ Mprotect said it changed the protection but can I believe it?\n+  \/\/\n+  \/\/ To be sure I need to check the protection afterwards. Try to\n+  \/\/ read from protected memory and check whether that causes a segfault.\n+  \/\/\n+  if (!os::Aix::xpg_sus_mode()) {\n+\n+    const bool read_protected =\n+      (SafeFetch32((int*)addr, 0x12345678) == 0x12345678 &&\n+       SafeFetch32((int*)addr, 0x76543210) == 0x76543210) ? true : false;\n+\n+    if (prot & PROT_READ) {\n+      rc = !read_protected;\n+    } else {\n+      rc = read_protected;\n+    }\n+\n+    if (!rc) {\n+      if (os::Aix::on_pase()) {\n+        \/\/ There is an issue on older PASE systems where mprotect() will return success but the\n+        \/\/ memory will not be protected.\n+        \/\/ This has nothing to do with the problem of using mproect() on SPEC1170 incompatible\n+        \/\/ machines; we only see it rarely, when using mprotect() to protect the guard page of\n+        \/\/ a stack. It is an OS error.\n+        \/\/\n+        \/\/ A valid strategy is just to try again. This usually works. :-\/\n+\n+        ::usleep(1000);\n+        Events::log(nullptr, \"Protecting memory [\" INTPTR_FORMAT \",\" INTPTR_FORMAT \"] with protection modes %x\", p2i(addr), p2i(addr+size), prot);\n+        if (::mprotect(addr, size, prot) == 0) {\n+          const bool read_protected_2 =\n+            (SafeFetch32((int*)addr, 0x12345678) == 0x12345678 &&\n+            SafeFetch32((int*)addr, 0x76543210) == 0x76543210) ? true : false;\n+          rc = true;\n+        }\n+      }\n+    }\n+  }\n+\n+  assert(rc == true, \"mprotect failed.\");\n+\n+  return rc;\n+}\n+\n+\/\/ Set protections specified\n+bool os::protect_memory(char* addr, size_t size, ProtType prot, bool is_committed) {\n+  unsigned int p = 0;\n+  switch (prot) {\n+  case MEM_PROT_NONE: p = PROT_NONE; break;\n+  case MEM_PROT_READ: p = PROT_READ; break;\n+  case MEM_PROT_RW:   p = PROT_READ|PROT_WRITE; break;\n+  case MEM_PROT_RWX:  p = PROT_READ|PROT_WRITE|PROT_EXEC; break;\n+  default:\n+    ShouldNotReachHere();\n+  }\n+  \/\/ is_committed is unused.\n+  return checked_mprotect(addr, size, p);\n+}\n+\n+bool os::guard_memory(char* addr, size_t size) {\n+  return checked_mprotect(addr, size, PROT_NONE);\n+}\n+\n+bool os::unguard_memory(char* addr, size_t size) {\n+  return checked_mprotect(addr, size, PROT_READ|PROT_WRITE|PROT_EXEC);\n+}\n+\n+\/\/ Large page support\n+\n+static size_t _large_page_size = 0;\n+\n+\/\/ Enable large page support if OS allows that.\n+void os::large_page_init() {\n+  return; \/\/ Nothing to do. See query_multipage_support and friends.\n+}\n+\n+char* os::pd_reserve_memory_special(size_t bytes, size_t alignment, size_t page_size, char* req_addr, bool exec) {\n+  fatal(\"os::reserve_memory_special should not be called on AIX.\");\n+  return nullptr;\n+}\n+\n+bool os::pd_release_memory_special(char* base, size_t bytes) {\n+  fatal(\"os::release_memory_special should not be called on AIX.\");\n+  return false;\n+}\n+\n+size_t os::large_page_size() {\n+  return _large_page_size;\n+}\n+\n+bool os::can_commit_large_page_memory() {\n+  \/\/ Does not matter, we do not support huge pages.\n+  return false;\n+}\n+\n+char* os::pd_attempt_map_memory_to_file_at(char* requested_addr, size_t bytes, int file_desc) {\n+  assert(file_desc >= 0, \"file_desc is not valid\");\n+  char* result = nullptr;\n+\n+  \/\/ Always round to os::vm_page_size(), which may be larger than 4K.\n+  bytes = align_up(bytes, os::vm_page_size());\n+  result = reserve_mmaped_memory(bytes, requested_addr);\n+\n+  if (result != nullptr) {\n+    if (replace_existing_mapping_with_file_mapping(result, bytes, file_desc) == nullptr) {\n+      vm_exit_during_initialization(err_msg(\"Error in mapping Java heap at the given filesystem directory\"));\n+    }\n+  }\n+  return result;\n+}\n+\n+\/\/ Reserve memory at an arbitrary address, only if that area is\n+\/\/ available (and not reserved for something else).\n+char* os::pd_attempt_reserve_memory_at(char* requested_addr, size_t bytes, bool exec) {\n+  char* addr = nullptr;\n+\n+  \/\/ Always round to os::vm_page_size(), which may be larger than 4K.\n+  bytes = align_up(bytes, os::vm_page_size());\n+\n+  \/\/ In 4K mode always use mmap.\n+  \/\/ In 64K mode allocate small sizes with mmap, large ones with 64K shmatted.\n+  if (os::vm_page_size() == 4*K) {\n+    return reserve_mmaped_memory(bytes, requested_addr);\n+  } else {\n+    if (bytes >= Use64KPagesThreshold) {\n+      return reserve_shmated_memory(bytes, requested_addr);\n+    } else {\n+      return reserve_mmaped_memory(bytes, requested_addr);\n+    }\n+  }\n+\n+  return addr;\n+}\n+\n+size_t os::vm_min_address() {\n+  \/\/ On AIX, we need to make sure we don't block the sbrk. However, this is\n+  \/\/ done at actual reservation time, where we honor a \"no-mmap\" area following\n+  \/\/ the break. See MaxExpectedDataSegmentSize. So we can return a very low\n+  \/\/ address here.\n+  assert(is_aligned(_vm_min_address_default, os::vm_allocation_granularity()), \"Sanity\");\n+  return _vm_min_address_default;\n+}\n+\n+\/\/ Used to convert frequent JVM_Yield() to nops\n+bool os::dont_yield() {\n+  return DontYieldALot;\n+}\n+\n+void os::naked_yield() {\n+  sched_yield();\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/ thread priority support\n+\n+\/\/ From AIX manpage to pthread_setschedparam\n+\/\/ (see: http:\/\/publib.boulder.ibm.com\/infocenter\/pseries\/v5r3\/index.jsp?\n+\/\/    topic=\/com.ibm.aix.basetechref\/doc\/basetrf1\/pthread_setschedparam.htm):\n+\/\/\n+\/\/ \"If schedpolicy is SCHED_OTHER, then sched_priority must be in the\n+\/\/ range from 40 to 80, where 40 is the least favored priority and 80\n+\/\/ is the most favored.\"\n+\/\/\n+\/\/ (Actually, I doubt this even has an impact on AIX, as we do kernel\n+\/\/ scheduling there; however, this still leaves iSeries.)\n+\/\/\n+\/\/ We use the same values for AIX and PASE.\n+int os::java_to_os_priority[CriticalPriority + 1] = {\n+  54,             \/\/ 0 Entry should never be used\n+\n+  55,             \/\/ 1 MinPriority\n+  55,             \/\/ 2\n+  56,             \/\/ 3\n+\n+  56,             \/\/ 4\n+  57,             \/\/ 5 NormPriority\n+  57,             \/\/ 6\n+\n+  58,             \/\/ 7\n+  58,             \/\/ 8\n+  59,             \/\/ 9 NearMaxPriority\n+\n+  60,             \/\/ 10 MaxPriority\n+\n+  60              \/\/ 11 CriticalPriority\n+};\n+\n+static int prio_init() {\n+  if (ThreadPriorityPolicy == 1) {\n+    if (geteuid() != 0) {\n+      if (!FLAG_IS_DEFAULT(ThreadPriorityPolicy) && !FLAG_IS_JIMAGE_RESOURCE(ThreadPriorityPolicy)) {\n+        warning(\"-XX:ThreadPriorityPolicy=1 may require system level permission, \" \\\n+                \"e.g., being the root user. If the necessary permission is not \" \\\n+                \"possessed, changes to priority will be silently ignored.\");\n+      }\n+    }\n+  }\n+  if (UseCriticalJavaThreadPriority) {\n+    os::java_to_os_priority[MaxPriority] = os::java_to_os_priority[CriticalPriority];\n+  }\n+  return 0;\n+}\n+\n+OSReturn os::set_native_priority(Thread* thread, int newpri) {\n+  if (!UseThreadPriorities || ThreadPriorityPolicy == 0) return OS_OK;\n+  pthread_t thr = thread->osthread()->pthread_id();\n+  int policy = SCHED_OTHER;\n+  struct sched_param param;\n+  param.sched_priority = newpri;\n+  int ret = pthread_setschedparam(thr, policy, &param);\n+\n+  if (ret != 0) {\n+    trcVerbose(\"Could not change priority for thread %d to %d (error %d, %s)\",\n+        (int)thr, newpri, ret, os::errno_name(ret));\n+  }\n+  return (ret == 0) ? OS_OK : OS_ERR;\n+}\n+\n+OSReturn os::get_native_priority(const Thread* const thread, int *priority_ptr) {\n+  if (!UseThreadPriorities || ThreadPriorityPolicy == 0) {\n+    *priority_ptr = java_to_os_priority[NormPriority];\n+    return OS_OK;\n+  }\n+  pthread_t thr = thread->osthread()->pthread_id();\n+  int policy = SCHED_OTHER;\n+  struct sched_param param;\n+  int ret = pthread_getschedparam(thr, &policy, &param);\n+  *priority_ptr = param.sched_priority;\n+\n+  return (ret == 0) ? OS_OK : OS_ERR;\n+}\n+\n+\/\/ To install functions for atexit system call\n+extern \"C\" {\n+  static void perfMemory_exit_helper() {\n+    perfMemory_exit();\n+  }\n+}\n+\n+static void set_page_size(size_t page_size) {\n+  OSInfo::set_vm_page_size(page_size);\n+  OSInfo::set_vm_allocation_granularity(page_size);\n+}\n+\n+\/\/ This is called _before_ the most of global arguments have been parsed.\n+void os::init(void) {\n+  \/\/ This is basic, we want to know if that ever changes.\n+  \/\/ (Shared memory boundary is supposed to be a 256M aligned.)\n+  assert(SHMLBA == ((uint64_t)0x10000000ULL)\/*256M*\/, \"unexpected\");\n+\n+  \/\/ Record process break at startup.\n+  g_brk_at_startup = (address) ::sbrk(0);\n+  assert(g_brk_at_startup != (address) -1, \"sbrk failed\");\n+\n+  \/\/ First off, we need to know whether we run on AIX or PASE, and\n+  \/\/ the OS level we run on.\n+  os::Aix::initialize_os_info();\n+\n+  \/\/ Scan environment (SPEC1170 behaviour, etc).\n+  os::Aix::scan_environment();\n+\n+  \/\/ Probe multipage support.\n+  query_multipage_support();\n+\n+  \/\/ Act like we only have one page size by eliminating corner cases which\n+  \/\/ we did not support very well anyway.\n+  \/\/ We have two input conditions:\n+  \/\/ 1) Data segment page size. This is controlled by linker setting (datapsize) on the\n+  \/\/    launcher, and\/or by LDR_CNTRL environment variable. The latter overrules the linker\n+  \/\/    setting.\n+  \/\/    Data segment page size is important for us because it defines the thread stack page\n+  \/\/    size, which is needed for guard page handling, stack banging etc.\n+  \/\/ 2) The ability to allocate 64k pages dynamically. If this is a given, java heap can\n+  \/\/    and should be allocated with 64k pages.\n+  \/\/\n+  \/\/ So, we do the following:\n+  \/\/ LDR_CNTRL    can_use_64K_pages_dynamically       what we do                      remarks\n+  \/\/ 4K           no                                  4K                              old systems (aix 5.2, as\/400 v5r4) or new systems with AME activated\n+  \/\/ 4k           yes                                 64k (treat 4k stacks as 64k)    different loader than java and standard settings\n+  \/\/ 64k          no              --- AIX 5.2 ? ---\n+  \/\/ 64k          yes                                 64k                             new systems and standard java loader (we set datapsize=64k when linking)\n+\n+  \/\/ We explicitly leave no option to change page size, because only upgrading would work,\n+  \/\/ not downgrading (if stack page size is 64k you cannot pretend its 4k).\n+\n+  if (g_multipage_support.datapsize == 4*K) {\n+    \/\/ datapsize = 4K. Data segment, thread stacks are 4K paged.\n+    if (g_multipage_support.can_use_64K_pages) {\n+      \/\/ .. but we are able to use 64K pages dynamically.\n+      \/\/ This would be typical for java launchers which are not linked\n+      \/\/ with datapsize=64K (like, any other launcher but our own).\n+      \/\/\n+      \/\/ In this case it would be smart to allocate the java heap with 64K\n+      \/\/ to get the performance benefit, and to fake 64k pages for the\n+      \/\/ data segment (when dealing with thread stacks).\n+      \/\/\n+      \/\/ However, leave a possibility to downgrade to 4K, using\n+      \/\/ -XX:-Use64KPages.\n+      if (Use64KPages) {\n+        trcVerbose(\"64K page mode (faked for data segment)\");\n+        set_page_size(64*K);\n+      } else {\n+        trcVerbose(\"4K page mode (Use64KPages=off)\");\n+        set_page_size(4*K);\n+      }\n+    } else {\n+      \/\/ .. and not able to allocate 64k pages dynamically. Here, just\n+      \/\/ fall back to 4K paged mode and use mmap for everything.\n+      trcVerbose(\"4K page mode\");\n+      set_page_size(4*K);\n+      FLAG_SET_ERGO(Use64KPages, false);\n+    }\n+  } else {\n+    \/\/ datapsize = 64k. Data segment, thread stacks are 64k paged.\n+    \/\/ This normally means that we can allocate 64k pages dynamically.\n+    \/\/ (There is one special case where this may be false: EXTSHM=on.\n+    \/\/ but we decided to not support that mode).\n+    assert0(g_multipage_support.can_use_64K_pages);\n+    set_page_size(64*K);\n+    trcVerbose(\"64K page mode\");\n+    FLAG_SET_ERGO(Use64KPages, true);\n+  }\n+\n+  \/\/ For now UseLargePages is just ignored.\n+  FLAG_SET_ERGO(UseLargePages, false);\n+  _page_sizes.add(os::vm_page_size());\n+\n+  \/\/ debug trace\n+  trcVerbose(\"os::vm_page_size %s\", describe_pagesize(os::vm_page_size()));\n+\n+  \/\/ Next, we need to initialize libo4 and libperfstat libraries.\n+  if (os::Aix::on_pase()) {\n+    os::Aix::initialize_libo4();\n+  } else {\n+    os::Aix::initialize_libperfstat();\n+  }\n+\n+  \/\/ Reset the perfstat information provided by ODM.\n+  if (os::Aix::on_aix()) {\n+    libperfstat::perfstat_reset();\n+  }\n+\n+  \/\/ Now initialize basic system properties. Note that for some of the values we\n+  \/\/ need libperfstat etc.\n+  os::Aix::initialize_system_info();\n+\n+  \/\/ _main_thread points to the thread that created\/loaded the JVM.\n+  Aix::_main_thread = pthread_self();\n+\n+  os::Posix::init();\n+}\n+\n+\/\/ This is called _after_ the global arguments have been parsed.\n+jint os::init_2(void) {\n+\n+  \/\/ This could be set after os::Posix::init() but all platforms\n+  \/\/ have to set it the same so we have to mirror Solaris.\n+  DEBUG_ONLY(os::set_mutex_init_done();)\n+\n+  os::Posix::init_2();\n+\n+  if (os::Aix::on_pase()) {\n+    trcVerbose(\"Running on PASE.\");\n+  } else {\n+    trcVerbose(\"Running on AIX (not PASE).\");\n+  }\n+\n+  trcVerbose(\"processor count: %d\", os::_processor_count);\n+  trcVerbose(\"physical memory: %lu\", Aix::_physical_memory);\n+\n+  \/\/ Initially build up the loaded dll map.\n+  LoadedLibraries::reload();\n+  if (Verbose) {\n+    trcVerbose(\"Loaded Libraries: \");\n+    LoadedLibraries::print(tty);\n+  }\n+\n+  if (PosixSignals::init() == JNI_ERR) {\n+    return JNI_ERR;\n+  }\n+\n+  \/\/ Check and sets minimum stack sizes against command line options\n+  if (set_minimum_stack_sizes() == JNI_ERR) {\n+    return JNI_ERR;\n+  }\n+\n+  \/\/ Not supported.\n+  FLAG_SET_ERGO(UseNUMA, false);\n+  FLAG_SET_ERGO(UseNUMAInterleaving, false);\n+\n+  if (MaxFDLimit) {\n+    \/\/ Set the number of file descriptors to max. print out error\n+    \/\/ if getrlimit\/setrlimit fails but continue regardless.\n+    struct rlimit nbr_files;\n+    int status = getrlimit(RLIMIT_NOFILE, &nbr_files);\n+    if (status != 0) {\n+      log_info(os)(\"os::init_2 getrlimit failed: %s\", os::strerror(errno));\n+    } else {\n+      nbr_files.rlim_cur = nbr_files.rlim_max;\n+      status = setrlimit(RLIMIT_NOFILE, &nbr_files);\n+      if (status != 0) {\n+        log_info(os)(\"os::init_2 setrlimit failed: %s\", os::strerror(errno));\n+      }\n+    }\n+  }\n+\n+  if (PerfAllowAtExitRegistration) {\n+    \/\/ Only register atexit functions if PerfAllowAtExitRegistration is set.\n+    \/\/ At exit functions can be delayed until process exit time, which\n+    \/\/ can be problematic for embedded VM situations. Embedded VMs should\n+    \/\/ call DestroyJavaVM() to assure that VM resources are released.\n+\n+    \/\/ Note: perfMemory_exit_helper atexit function may be removed in\n+    \/\/ the future if the appropriate cleanup code can be added to the\n+    \/\/ VM_Exit VMOperation's doit method.\n+    if (atexit(perfMemory_exit_helper) != 0) {\n+      warning(\"os::init_2 atexit(perfMemory_exit_helper) failed\");\n+    }\n+  }\n+\n+  \/\/ initialize thread priority policy\n+  prio_init();\n+\n+  return JNI_OK;\n+}\n+\n+int os::active_processor_count() {\n+  \/\/ User has overridden the number of active processors\n+  if (ActiveProcessorCount > 0) {\n+    log_trace(os)(\"active_processor_count: \"\n+                  \"active processor count set by user : %d\",\n+                  ActiveProcessorCount);\n+    return ActiveProcessorCount;\n+  }\n+\n+  int online_cpus = ::sysconf(_SC_NPROCESSORS_ONLN);\n+  assert(online_cpus > 0 && online_cpus <= processor_count(), \"sanity check\");\n+  return online_cpus;\n+}\n+\n+void os::set_native_thread_name(const char *name) {\n+  \/\/ Not yet implemented.\n+  return;\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/ debug support\n+\n+bool os::find(address addr, outputStream* st) {\n+\n+  st->print(PTR_FORMAT \": \", addr);\n+\n+  loaded_module_t lm;\n+  if (LoadedLibraries::find_for_text_address(addr, &lm) ||\n+      LoadedLibraries::find_for_data_address(addr, &lm)) {\n+    st->print_cr(\"%s\", lm.path);\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/ misc\n+\n+\/\/ This does not do anything on Aix. This is basically a hook for being\n+\/\/ able to use structured exception handling (thread-local exception filters)\n+\/\/ on, e.g., Win32.\n+void\n+os::os_exception_wrapper(java_call_t f, JavaValue* value, const methodHandle& method,\n+                         JavaCallArguments* args, JavaThread* thread) {\n+  f(value, method, args, thread);\n+}\n+\n+\/\/ This code originates from JDK's sysOpen and open64_w\n+\/\/ from src\/solaris\/hpi\/src\/system_md.c\n+\n+int os::open(const char *path, int oflag, int mode) {\n+\n+  if (strlen(path) > MAX_PATH - 1) {\n+    errno = ENAMETOOLONG;\n+    return -1;\n+  }\n+  \/\/ AIX 7.X now supports O_CLOEXEC too, like modern Linux; but we have to be careful, see\n+  \/\/ IV90804: OPENING A FILE IN AFS WITH O_CLOEXEC FAILS WITH AN EINVAL ERROR APPLIES TO AIX 7100-04 17\/04\/14 PTF PECHANGE\n+  int oflag_with_o_cloexec = oflag | O_CLOEXEC;\n+\n+  int fd = ::open(path, oflag_with_o_cloexec, mode);\n+  if (fd == -1) {\n+    \/\/ we might fail in the open call when O_CLOEXEC is set, so try again without (see IV90804)\n+    fd = ::open(path, oflag, mode);\n+    if (fd == -1) {\n+      return -1;\n+    }\n+  }\n+\n+  \/\/ If the open succeeded, the file might still be a directory.\n+  {\n+    struct stat buf64;\n+    int ret = ::fstat(fd, &buf64);\n+    int st_mode = buf64.st_mode;\n+\n+    if (ret != -1) {\n+      if ((st_mode & S_IFMT) == S_IFDIR) {\n+        errno = EISDIR;\n+        ::close(fd);\n+        return -1;\n+      }\n+    } else {\n+      ::close(fd);\n+      return -1;\n+    }\n+  }\n+\n+  \/\/ All file descriptors that are opened in the JVM and not\n+  \/\/ specifically destined for a subprocess should have the\n+  \/\/ close-on-exec flag set. If we don't set it, then careless 3rd\n+  \/\/ party native code might fork and exec without closing all\n+  \/\/ appropriate file descriptors (e.g. as we do in closeDescriptors in\n+  \/\/ UNIXProcess.c), and this in turn might:\n+  \/\/\n+  \/\/ - cause end-of-file to fail to be detected on some file\n+  \/\/   descriptors, resulting in mysterious hangs, or\n+  \/\/\n+  \/\/ - might cause an fopen in the subprocess to fail on a system\n+  \/\/   suffering from bug 1085341.\n+\n+  \/\/ Validate that the use of the O_CLOEXEC flag on open above worked.\n+  static sig_atomic_t O_CLOEXEC_is_known_to_work = 0;\n+  if (O_CLOEXEC_is_known_to_work == 0) {\n+    int flags = ::fcntl(fd, F_GETFD);\n+    if (flags != -1) {\n+      if ((flags & FD_CLOEXEC) != 0) {\n+        O_CLOEXEC_is_known_to_work = 1;\n+      } else { \/\/ it does not work\n+        ::fcntl(fd, F_SETFD, flags | FD_CLOEXEC);\n+        O_CLOEXEC_is_known_to_work = -1;\n+      }\n+    }\n+  } else if (O_CLOEXEC_is_known_to_work == -1) {\n+    int flags = ::fcntl(fd, F_GETFD);\n+    if (flags != -1) {\n+      ::fcntl(fd, F_SETFD, flags | FD_CLOEXEC);\n+    }\n+  }\n+\n+  return fd;\n+}\n+\n+\/\/ create binary file, rewriting existing file if required\n+int os::create_binary_file(const char* path, bool rewrite_existing) {\n+  int oflags = O_WRONLY | O_CREAT;\n+  oflags |= rewrite_existing ? O_TRUNC : O_EXCL;\n+  return ::open(path, oflags, S_IREAD | S_IWRITE);\n+}\n+\n+\/\/ return current position of file pointer\n+jlong os::current_file_offset(int fd) {\n+  return (jlong)::lseek(fd, (off_t)0, SEEK_CUR);\n+}\n+\n+\/\/ move file pointer to the specified offset\n+jlong os::seek_to_file_offset(int fd, jlong offset) {\n+  return (jlong)::lseek(fd, (off_t)offset, SEEK_SET);\n+}\n+\n+\/\/ current_thread_cpu_time(bool) and thread_cpu_time(Thread*, bool)\n+\/\/ are used by JVM M&M and JVMTI to get user+sys or user CPU time\n+\/\/ of a thread.\n+\/\/\n+\/\/ current_thread_cpu_time() and thread_cpu_time(Thread*) returns\n+\/\/ the fast estimate available on the platform.\n+\n+jlong os::current_thread_cpu_time() {\n+  \/\/ return user + sys since the cost is the same\n+  const jlong n = os::thread_cpu_time(Thread::current(), true \/* user + sys *\/);\n+  assert(n >= 0, \"negative CPU time\");\n+  return n;\n+}\n+\n+jlong os::thread_cpu_time(Thread* thread) {\n+  \/\/ consistent with what current_thread_cpu_time() returns\n+  const jlong n = os::thread_cpu_time(thread, true \/* user + sys *\/);\n+  assert(n >= 0, \"negative CPU time\");\n+  return n;\n+}\n+\n+jlong os::current_thread_cpu_time(bool user_sys_cpu_time) {\n+  const jlong n = os::thread_cpu_time(Thread::current(), user_sys_cpu_time);\n+  assert(n >= 0, \"negative CPU time\");\n+  return n;\n+}\n+\n+static bool thread_cpu_time_unchecked(Thread* thread, jlong* p_sys_time, jlong* p_user_time) {\n+  bool error = false;\n+\n+  jlong sys_time = 0;\n+  jlong user_time = 0;\n+\n+  \/\/ Reimplemented using getthrds64().\n+  \/\/\n+  \/\/ Works like this:\n+  \/\/ For the thread in question, get the kernel thread id. Then get the\n+  \/\/ kernel thread statistics using that id.\n+  \/\/\n+  \/\/ This only works of course when no pthread scheduling is used,\n+  \/\/ i.e. there is a 1:1 relationship to kernel threads.\n+  \/\/ On AIX, see AIXTHREAD_SCOPE variable.\n+\n+  pthread_t pthtid = thread->osthread()->pthread_id();\n+\n+  \/\/ retrieve kernel thread id for the pthread:\n+  tid64_t tid = 0;\n+  struct __pthrdsinfo pinfo;\n+  \/\/ I just love those otherworldly IBM APIs which force me to hand down\n+  \/\/ dummy buffers for stuff I dont care for...\n+  char dummy[1];\n+  int dummy_size = sizeof(dummy);\n+  if (pthread_getthrds_np(&pthtid, PTHRDSINFO_QUERY_TID, &pinfo, sizeof(pinfo),\n+                          dummy, &dummy_size) == 0) {\n+    tid = pinfo.__pi_tid;\n+  } else {\n+    tty->print_cr(\"pthread_getthrds_np failed.\");\n+    error = true;\n+  }\n+\n+  \/\/ retrieve kernel timing info for that kernel thread\n+  if (!error) {\n+    struct thrdentry64 thrdentry;\n+    if (getthrds64(getpid(), &thrdentry, sizeof(thrdentry), &tid, 1) == 1) {\n+      sys_time = thrdentry.ti_ru.ru_stime.tv_sec * 1000000000LL + thrdentry.ti_ru.ru_stime.tv_usec * 1000LL;\n+      user_time = thrdentry.ti_ru.ru_utime.tv_sec * 1000000000LL + thrdentry.ti_ru.ru_utime.tv_usec * 1000LL;\n+    } else {\n+      tty->print_cr(\"pthread_getthrds_np failed.\");\n+      error = true;\n+    }\n+  }\n+\n+  if (p_sys_time) {\n+    *p_sys_time = sys_time;\n+  }\n+\n+  if (p_user_time) {\n+    *p_user_time = user_time;\n+  }\n+\n+  if (error) {\n+    return false;\n+  }\n+\n+  return true;\n+}\n+\n+jlong os::thread_cpu_time(Thread *thread, bool user_sys_cpu_time) {\n+  jlong sys_time;\n+  jlong user_time;\n+\n+  if (!thread_cpu_time_unchecked(thread, &sys_time, &user_time)) {\n+    return -1;\n+  }\n+\n+  return user_sys_cpu_time ? sys_time + user_time : user_time;\n+}\n+\n+void os::current_thread_cpu_time_info(jvmtiTimerInfo *info_ptr) {\n+  info_ptr->max_value = ALL_64_BITS;       \/\/ will not wrap in less than 64 bits\n+  info_ptr->may_skip_backward = false;     \/\/ elapsed time not wall time\n+  info_ptr->may_skip_forward = false;      \/\/ elapsed time not wall time\n+  info_ptr->kind = JVMTI_TIMER_TOTAL_CPU;  \/\/ user+system time is returned\n+}\n+\n+void os::thread_cpu_time_info(jvmtiTimerInfo *info_ptr) {\n+  info_ptr->max_value = ALL_64_BITS;       \/\/ will not wrap in less than 64 bits\n+  info_ptr->may_skip_backward = false;     \/\/ elapsed time not wall time\n+  info_ptr->may_skip_forward = false;      \/\/ elapsed time not wall time\n+  info_ptr->kind = JVMTI_TIMER_TOTAL_CPU;  \/\/ user+system time is returned\n+}\n+\n+bool os::is_thread_cpu_time_supported() {\n+  return true;\n+}\n+\n+\/\/ System loadavg support. Returns -1 if load average cannot be obtained.\n+\/\/ For now just return the system wide load average (no processor sets).\n+int os::loadavg(double values[], int nelem) {\n+\n+  guarantee(nelem >= 0 && nelem <= 3, \"argument error\");\n+  guarantee(values, \"argument error\");\n+\n+  if (os::Aix::on_pase()) {\n+\n+    \/\/ AS\/400 PASE: use libo4 porting library\n+    double v[3] = { 0.0, 0.0, 0.0 };\n+\n+    if (libo4::get_load_avg(v, v + 1, v + 2)) {\n+      for (int i = 0; i < nelem; i ++) {\n+        values[i] = v[i];\n+      }\n+      return nelem;\n+    } else {\n+      return -1;\n+    }\n+\n+  } else {\n+\n+    \/\/ AIX: use libperfstat\n+    libperfstat::cpuinfo_t ci;\n+    if (libperfstat::get_cpuinfo(&ci)) {\n+      for (int i = 0; i < nelem; i++) {\n+        values[i] = ci.loadavg[i];\n+      }\n+    } else {\n+      return -1;\n+    }\n+    return nelem;\n+  }\n+}\n+\n+bool os::is_primordial_thread(void) {\n+  if (pthread_self() == (pthread_t)1) {\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+\/\/ OS recognitions (PASE\/AIX, OS level) call this before calling any\n+\/\/ one of Aix::on_pase(), Aix::os_version() static\n+void os::Aix::initialize_os_info() {\n+\n+  assert(_on_pase == -1 && _os_version == 0, \"already called.\");\n+\n+  struct utsname uts;\n+  memset(&uts, 0, sizeof(uts));\n+  strcpy(uts.sysname, \"?\");\n+  if (::uname(&uts) == -1) {\n+    trcVerbose(\"uname failed (%d)\", errno);\n+    guarantee(0, \"Could not determine whether we run on AIX or PASE\");\n+  } else {\n+    trcVerbose(\"uname says: sysname \\\"%s\\\" version \\\"%s\\\" release \\\"%s\\\" \"\n+               \"node \\\"%s\\\" machine \\\"%s\\\"\\n\",\n+               uts.sysname, uts.version, uts.release, uts.nodename, uts.machine);\n+    const int major = atoi(uts.version);\n+    assert(major > 0, \"invalid OS version\");\n+    const int minor = atoi(uts.release);\n+    assert(minor > 0, \"invalid OS release\");\n+    _os_version = (major << 24) | (minor << 16);\n+    char ver_str[20] = {0};\n+    const char* name_str = \"unknown OS\";\n+    if (strcmp(uts.sysname, \"OS400\") == 0) {\n+      \/\/ We run on AS\/400 PASE. We do not support versions older than V5R4M0.\n+      _on_pase = 1;\n+      if (os_version_short() < 0x0504) {\n+        trcVerbose(\"OS\/400 releases older than V5R4M0 not supported.\");\n+        assert(false, \"OS\/400 release too old.\");\n+      }\n+      name_str = \"OS\/400 (pase)\";\n+      jio_snprintf(ver_str, sizeof(ver_str), \"%u.%u\", major, minor);\n+    } else if (strcmp(uts.sysname, \"AIX\") == 0) {\n+      \/\/ We run on AIX. We do not support versions older than AIX 7.1.\n+      _on_pase = 0;\n+      \/\/ Determine detailed AIX version: Version, Release, Modification, Fix Level.\n+      odmWrapper::determine_os_kernel_version(&_os_version);\n+      if (os_version_short() < 0x0701) {\n+        trcVerbose(\"AIX releases older than AIX 7.1 are not supported.\");\n+        assert(false, \"AIX release too old.\");\n+      }\n+      name_str = \"AIX\";\n+      jio_snprintf(ver_str, sizeof(ver_str), \"%u.%u.%u.%u\",\n+                   major, minor, (_os_version >> 8) & 0xFF, _os_version & 0xFF);\n+    } else {\n+      assert(false, \"%s\", name_str);\n+    }\n+    trcVerbose(\"We run on %s %s\", name_str, ver_str);\n+  }\n+\n+  guarantee(_on_pase != -1 && _os_version, \"Could not determine AIX\/OS400 release\");\n+} \/\/ end: os::Aix::initialize_os_info()\n+\n+\/\/ Scan environment for important settings which might effect the VM.\n+\/\/ Trace out settings. Warn about invalid settings and\/or correct them.\n+\/\/\n+\/\/ Must run after os::Aix::initialue_os_info().\n+void os::Aix::scan_environment() {\n+\n+  char* p;\n+  int rc;\n+\n+  \/\/ Warn explicitly if EXTSHM=ON is used. That switch changes how\n+  \/\/ System V shared memory behaves. One effect is that page size of\n+  \/\/ shared memory cannot be change dynamically, effectivly preventing\n+  \/\/ large pages from working.\n+  \/\/ This switch was needed on AIX 32bit, but on AIX 64bit the general\n+  \/\/ recommendation is (in OSS notes) to switch it off.\n+  p = ::getenv(\"EXTSHM\");\n+  trcVerbose(\"EXTSHM=%s.\", p ? p : \"<unset>\");\n+  if (p && strcasecmp(p, \"ON\") == 0) {\n+    _extshm = 1;\n+    trcVerbose(\"*** Unsupported mode! Please remove EXTSHM from your environment! ***\");\n+    if (!AllowExtshm) {\n+      \/\/ We allow under certain conditions the user to continue. However, we want this\n+      \/\/ to be a fatal error by default. On certain AIX systems, leaving EXTSHM=ON means\n+      \/\/ that the VM is not able to allocate 64k pages for the heap.\n+      \/\/ We do not want to run with reduced performance.\n+      vm_exit_during_initialization(\"EXTSHM is ON. Please remove EXTSHM from your environment.\");\n+    }\n+  } else {\n+    _extshm = 0;\n+  }\n+\n+  \/\/ SPEC1170 behaviour: will change the behaviour of a number of POSIX APIs.\n+  \/\/ Not tested, not supported.\n+  \/\/\n+  \/\/ Note that it might be worth the trouble to test and to require it, if only to\n+  \/\/ get useful return codes for mprotect.\n+  \/\/\n+  \/\/ Note: Setting XPG_SUS_ENV in the process is too late. Must be set earlier (before\n+  \/\/ exec() ? before loading the libjvm ? ....)\n+  p = ::getenv(\"XPG_SUS_ENV\");\n+  trcVerbose(\"XPG_SUS_ENV=%s.\", p ? p : \"<unset>\");\n+  if (p && strcmp(p, \"ON\") == 0) {\n+    _xpg_sus_mode = 1;\n+    trcVerbose(\"Unsupported setting: XPG_SUS_ENV=ON\");\n+    \/\/ This is not supported. Worst of all, it changes behaviour of mmap MAP_FIXED to\n+    \/\/ clobber address ranges. If we ever want to support that, we have to do some\n+    \/\/ testing first.\n+    guarantee(false, \"XPG_SUS_ENV=ON not supported\");\n+  } else {\n+    _xpg_sus_mode = 0;\n+  }\n+\n+  if (os::Aix::on_pase()) {\n+    p = ::getenv(\"QIBM_MULTI_THREADED\");\n+    trcVerbose(\"QIBM_MULTI_THREADED=%s.\", p ? p : \"<unset>\");\n+  }\n+\n+  p = ::getenv(\"LDR_CNTRL\");\n+  trcVerbose(\"LDR_CNTRL=%s.\", p ? p : \"<unset>\");\n+  if (os::Aix::on_pase() && os::Aix::os_version_short() == 0x0701) {\n+    if (p && ::strstr(p, \"TEXTPSIZE\")) {\n+      trcVerbose(\"*** WARNING - LDR_CNTRL contains TEXTPSIZE. \"\n+        \"you may experience hangs or crashes on OS\/400 V7R1.\");\n+    }\n+  }\n+\n+  p = ::getenv(\"AIXTHREAD_GUARDPAGES\");\n+  trcVerbose(\"AIXTHREAD_GUARDPAGES=%s.\", p ? p : \"<unset>\");\n+\n+} \/\/ end: os::Aix::scan_environment()\n+\n+\/\/ PASE: initialize the libo4 library (PASE porting library).\n+void os::Aix::initialize_libo4() {\n+  guarantee(os::Aix::on_pase(), \"OS\/400 only.\");\n+  if (!libo4::init()) {\n+    trcVerbose(\"libo4 initialization failed.\");\n+    assert(false, \"libo4 initialization failed\");\n+  } else {\n+    trcVerbose(\"libo4 initialized.\");\n+  }\n+}\n+\n+\/\/ AIX: initialize the libperfstat library.\n+void os::Aix::initialize_libperfstat() {\n+  assert(os::Aix::on_aix(), \"AIX only\");\n+  if (!libperfstat::init()) {\n+    trcVerbose(\"libperfstat initialization failed.\");\n+    assert(false, \"libperfstat initialization failed\");\n+  } else {\n+    trcVerbose(\"libperfstat initialized.\");\n+  }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/ thread stack\n+\n+\/\/ Get the current stack base and size from the OS (actually, the pthread library).\n+\/\/ Note: base usually not page aligned.\n+\/\/ Returned size is such that (base - size) is always aligned to page size.\n+void os::current_stack_base_and_size(address* stack_base, size_t* stack_size) {\n+  AixMisc::stackbounds_t bounds;\n+  bool rc = AixMisc::query_stack_bounds_for_current_thread(&bounds);\n+  guarantee(rc, \"Unable to retrieve stack bounds.\");\n+  *stack_base = bounds.base;\n+\n+  \/\/ Align the reported stack size such that the stack low address\n+  \/\/ is aligned to page size (Note: base is usually not and we do not care).\n+  \/\/ We need to do this because caller code will assume stack low address is\n+  \/\/ page aligned and will place guard pages without checking.\n+  address low = bounds.base - bounds.size;\n+  address low_aligned = (address)align_up(low, os::vm_page_size());\n+  *stack_size = bounds.base - low_aligned;\n+}\n+\n+\/\/ Get the default path to the core file\n+\/\/ Returns the length of the string\n+int os::get_core_path(char* buffer, size_t bufferSize) {\n+  const char* p = get_current_directory(buffer, bufferSize);\n+\n+  if (p == nullptr) {\n+    assert(p != nullptr, \"failed to get current directory\");\n+    return 0;\n+  }\n+\n+  jio_snprintf(buffer, bufferSize, \"%s\/core or core.%d\",\n+                                               p, current_process_id());\n+\n+  return checked_cast<int>(strlen(buffer));\n+}\n+\n+bool os::start_debugging(char *buf, int buflen) {\n+  int len = (int)strlen(buf);\n+  char *p = &buf[len];\n+\n+  jio_snprintf(p, buflen -len,\n+                 \"\\n\\n\"\n+                 \"Do you want to debug the problem?\\n\\n\"\n+                 \"To debug, run 'dbx -a %d'; then switch to thread tid \" INTX_FORMAT \", k-tid \" INTX_FORMAT \"\\n\"\n+                 \"Enter 'yes' to launch dbx automatically (PATH must include dbx)\\n\"\n+                 \"Otherwise, press RETURN to abort...\",\n+                 os::current_process_id(),\n+                 os::current_thread_id(), thread_self());\n+\n+  bool yes = os::message_box(\"Unexpected Error\", buf);\n+\n+  if (yes) {\n+    \/\/ yes, user asked VM to launch debugger\n+    jio_snprintf(buf, buflen, \"dbx -a %d\", os::current_process_id());\n+\n+    os::fork_and_exec(buf);\n+    yes = false;\n+  }\n+  return yes;\n+}\n+\n+static inline time_t get_mtime(const char* filename) {\n+  struct stat st;\n+  int ret = os::stat(filename, &st);\n+  assert(ret == 0, \"failed to stat() file '%s': %s\", filename, os::strerror(errno));\n+  return st.st_mtime;\n+}\n+\n+int os::compare_file_modified_times(const char* file1, const char* file2) {\n+  time_t t1 = get_mtime(file1);\n+  time_t t2 = get_mtime(file2);\n+  return primitive_compare(t1, t2);\n+}\n+\n+bool os::supports_map_sync() {\n+  return false;\n+}\n+\n+void os::print_memory_mappings(char* addr, size_t bytes, outputStream* st) {}\n+\n+#if INCLUDE_JFR\n+\n+void os::jfr_report_memory_info() {}\n+\n+#endif \/\/ INCLUDE_JFR\n","filename":"src\/hotspot\/os\/aix\/os_aix.cpp","additions":3023,"deletions":0,"binary":false,"changes":3023,"status":"added"}]}