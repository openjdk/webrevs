{"files":[{"patch":"@@ -144,0 +144,13 @@\n+  \/\/ If GC was cancelled before final mark, then the safepoint operation will do nothing\n+  \/\/ and the concurrent mark will still be in progress. In this case it is safe to resume\n+  \/\/ the degenerated cycle from the marking phase. On the other hand, if the GC is cancelled\n+  \/\/ after final mark (but before this check), then the final mark safepoint operation\n+  \/\/ will have finished the mark (setting concurrent mark in progress to false). Final mark\n+  \/\/ will also have setup state (in concurrent stack processing) that will not be safe to\n+  \/\/ resume from the marking phase in the degenerated cycle. That is, if the cancellation\n+  \/\/ occurred after final mark, we must resume the degenerated cycle after the marking phase.\n+  if (_generation->is_concurrent_mark_in_progress() && check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_mark)) {\n+    assert(!heap->is_concurrent_weak_root_in_progress(), \"Weak roots should not be in progress when concurrent mark is in progress\");\n+    return false;\n+  }\n+\n@@ -221,1 +234,25 @@\n-    ShenandoahGenerationalHeap::heap()->complete_concurrent_cycle();\n+    if (!heap->old_generation()->is_parseable()) {\n+      \/\/ Class unloading may render the card offsets unusable, so we must rebuild them before\n+      \/\/ the next remembered set scan. We _could_ let the control thread do this sometime after\n+      \/\/ the global cycle has completed and before the next young collection, but under memory\n+      \/\/ pressure the control thread may not have the time (that is, because it's running back\n+      \/\/ to back GCs). In that scenario, we would have to make the old regions parsable before\n+      \/\/ we could start a young collection. This could delay the start of the young cycle and\n+      \/\/ throw off the heuristics.\n+      entry_global_coalesce_and_fill();\n+    }\n+\n+    ShenandoahGenerationalHeap::TransferResult result;\n+    {\n+      ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+      ShenandoahHeapLocker locker(gen_heap->lock());\n+\n+      result = gen_heap->balance_generations();\n+      gen_heap->reset_generation_reserves();\n+    }\n+\n+    LogTarget(Info, gc, ergo) lt;\n+    if (lt.is_enabled()) {\n+      LogStream ls(lt);\n+      result.print_on(\"Concurrent GC\", &ls);\n+    }\n@@ -628,1 +665,0 @@\n-    shenandoah_assert_generational();\n@@ -716,9 +752,49 @@\n-    if (!heap->collection_set()->is_empty() || has_in_place_promotions(heap)) {\n-      \/\/ Even if the collection set is empty, we need to do evacuation if there are regions to be promoted in place.\n-      \/\/ Concurrent evacuation takes responsibility for registering objects and setting the remembered set cards to dirty.\n-\n-      LogTarget(Debug, gc, cset) lt;\n-      if (lt.is_enabled()) {\n-        ResourceMark rm;\n-        LogStream ls(lt);\n-        heap->collection_set()->print_on(&ls);\n+    if (heap->mode()->is_generational()) {\n+      if (!heap->collection_set()->is_empty() || heap->old_generation()->has_in_place_promotions()) {\n+        \/\/ Even if the collection set is empty, we need to do evacuation if there are regions to be promoted in place.\n+        \/\/ Concurrent evacuation takes responsibility for registering objects and setting the remembered set cards to dirty.\n+\n+        LogTarget(Debug, gc, cset) lt;\n+        if (lt.is_enabled()) {\n+          ResourceMark rm;\n+          LogStream ls(lt);\n+          heap->collection_set()->print_on(&ls);\n+        }\n+\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_before_evacuation();\n+        }\n+\n+        heap->set_evacuation_in_progress(true);\n+\n+        \/\/ Verify before arming for concurrent processing.\n+        \/\/ Otherwise, verification can trigger stack processing.\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_during_evacuation();\n+        }\n+\n+        \/\/ Generational mode may promote objects in place during the evacuation phase.\n+        \/\/ If that is the only reason we are evacuating, we don't need to update references\n+        \/\/ and there will be no forwarded objects on the heap.\n+        heap->set_has_forwarded_objects(!heap->collection_set()->is_empty());\n+\n+        \/\/ Arm nmethods\/stack for concurrent processing\n+        if (!heap->collection_set()->is_empty()) {\n+          \/\/ Iff objects will be evaluated, arm the nmethod barriers. These will be disarmed\n+          \/\/ under the same condition (established in prepare_concurrent_roots) after strong\n+          \/\/ root evacuation has completed (see op_strong_roots).\n+          ShenandoahCodeRoots::arm_nmethods_for_evac();\n+          ShenandoahStackWatermark::change_epoch_id();\n+        }\n+\n+        if (ShenandoahPacing) {\n+          heap->pacer()->setup_for_evac();\n+        }\n+      } else {\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_after_concmark();\n+        }\n+\n+        if (VerifyAfterGC) {\n+          Universe::verify();\n+        }\n@@ -726,0 +802,9 @@\n+    } else {\n+      \/\/ Not is_generational()\n+      if (!heap->collection_set()->is_empty()) {\n+        LogTarget(Debug, gc, ergo) lt;\n+        if (lt.is_enabled()) {\n+          ResourceMark rm;\n+          LogStream ls(lt);\n+          heap->collection_set()->print_on(&ls);\n+        }\n@@ -727,3 +812,3 @@\n-      if (ShenandoahVerify) {\n-        heap->verifier()->verify_before_evacuation();\n-      }\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_before_evacuation();\n+        }\n@@ -731,2 +816,1 @@\n-      \/\/ TODO: Do we need to set this if we are only promoting regions in place? We don't need the barriers on for that.\n-      heap->set_evacuation_in_progress(true);\n+        heap->set_evacuation_in_progress(true);\n@@ -734,5 +818,5 @@\n-      \/\/ Verify before arming for concurrent processing.\n-      \/\/ Otherwise, verification can trigger stack processing.\n-      if (ShenandoahVerify) {\n-        heap->verifier()->verify_during_evacuation();\n-      }\n+        \/\/ Verify before arming for concurrent processing.\n+        \/\/ Otherwise, verification can trigger stack processing.\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_during_evacuation();\n+        }\n@@ -740,4 +824,2 @@\n-      \/\/ Generational mode may promote objects in place during the evacuation phase.\n-      \/\/ If that is the only reason we are evacuating, we don't need to update references\n-      \/\/ and there will be no forwarded objects on the heap.\n-      heap->set_has_forwarded_objects(!heap->collection_set()->is_empty());\n+        \/\/ From here on, we need to update references.\n+        heap->set_has_forwarded_objects(true);\n@@ -745,5 +827,1 @@\n-      \/\/ Arm nmethods\/stack for concurrent processing\n-      if (!heap->collection_set()->is_empty()) {\n-        \/\/ Iff objects will be evaluated, arm the nmethod barriers. These will be disarmed\n-        \/\/ under the same condition (established in prepare_concurrent_roots) after strong\n-        \/\/ root evacuation has completed (see op_strong_roots).\n+        \/\/ Arm nmethods\/stack for concurrent processing\n@@ -752,1 +830,0 @@\n-      }\n@@ -754,10 +831,11 @@\n-      if (ShenandoahPacing) {\n-        heap->pacer()->setup_for_evac();\n-      }\n-    } else {\n-      if (ShenandoahVerify) {\n-        heap->verifier()->verify_after_concmark();\n-      }\n-\n-      if (VerifyAfterGC) {\n-        Universe::verify();\n+        if (ShenandoahPacing) {\n+          heap->pacer()->setup_for_evac();\n+        }\n+      } else {\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_after_concmark();\n+        }\n+\n+        if (VerifyAfterGC) {\n+          Universe::verify();\n+        }\n@@ -769,5 +847,0 @@\n-bool ShenandoahConcurrentGC::has_in_place_promotions(ShenandoahHeap* heap) {\n-  return heap->mode()->is_generational() && heap->old_generation()->has_in_place_promotions();\n-}\n-\n-template<bool GENERATIONAL>\n@@ -777,2 +850,0 @@\n-public:\n-  explicit ShenandoahConcurrentEvacThreadClosure(OopClosure* oops) : _oops(oops) {}\n@@ -780,7 +851,3 @@\n-  void do_thread(Thread* thread) override {\n-    JavaThread* const jt = JavaThread::cast(thread);\n-    StackWatermarkSet::finish_processing(jt, _oops, StackWatermarkKind::gc);\n-    if (GENERATIONAL) {\n-      ShenandoahThreadLocalData::enable_plab_promotions(thread);\n-    }\n-  }\n+public:\n+  ShenandoahConcurrentEvacThreadClosure(OopClosure* oops);\n+  void do_thread(Thread* thread);\n@@ -789,1 +856,10 @@\n-template<bool GENERATIONAL>\n+ShenandoahConcurrentEvacThreadClosure::ShenandoahConcurrentEvacThreadClosure(OopClosure* oops) :\n+  _oops(oops) {\n+}\n+\n+void ShenandoahConcurrentEvacThreadClosure::do_thread(Thread* thread) {\n+  JavaThread* const jt = JavaThread::cast(thread);\n+  StackWatermarkSet::finish_processing(jt, _oops, StackWatermarkKind::gc);\n+  ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+}\n+\n@@ -795,1 +871,1 @@\n-  explicit ShenandoahConcurrentEvacUpdateThreadTask(uint n_workers) :\n+  ShenandoahConcurrentEvacUpdateThreadTask(uint n_workers) :\n@@ -800,5 +876,3 @@\n-  void work(uint worker_id) override {\n-    if (GENERATIONAL) {\n-      Thread* worker_thread = Thread::current();\n-      ShenandoahThreadLocalData::enable_plab_promotions(worker_thread);\n-    }\n+  void work(uint worker_id) {\n+    Thread* worker_thread = Thread::current();\n+    ShenandoahThreadLocalData::enable_plab_promotions(worker_thread);\n@@ -809,1 +883,1 @@\n-    ShenandoahConcurrentEvacThreadClosure<GENERATIONAL> thr_cl(&oops_cl);\n+    ShenandoahConcurrentEvacThreadClosure thr_cl(&oops_cl);\n@@ -818,7 +892,2 @@\n-  if (heap->mode()->is_generational()) {\n-    ShenandoahConcurrentEvacUpdateThreadTask<true> task(heap->workers()->active_workers());\n-    heap->workers()->run_task(&task);\n-  } else {\n-    ShenandoahConcurrentEvacUpdateThreadTask<false> task(heap->workers()->active_workers());\n-    heap->workers()->run_task(&task);\n-  }\n+  ShenandoahConcurrentEvacUpdateThreadTask task(heap->workers()->active_workers());\n+  heap->workers()->run_task(&task);\n@@ -1202,1 +1271,13 @@\n-    ShenandoahGenerationalHeap::heap()->update_region_ages();\n+    ShenandoahMarkingContext *ctx = heap->complete_marking_context();\n+    for (size_t i = 0; i < heap->num_regions(); i++) {\n+      ShenandoahHeapRegion *r = heap->get_region(i);\n+      if (r->is_active() && r->is_young()) {\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        HeapWord* top = r->top();\n+        if (top > tams) {\n+          r->reset_age();\n+        } else if (ShenandoahGenerationalHeap::heap()->is_aging_cycle()) {\n+          r->increment_age();\n+        }\n+      }\n+    }\n@@ -1206,0 +1287,19 @@\n+void ShenandoahConcurrentGC::entry_global_coalesce_and_fill() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n+  const char* msg = \"Coalescing and filling old regions in global collect\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_coalesce_and_fill);\n+\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  EventMark em(\"%s\", msg);\n+  ShenandoahWorkerScope scope(heap->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_marking(),\n+                              \"concurrent coalesce and fill\");\n+\n+  op_global_coalesce_and_fill();\n+}\n+\n+void ShenandoahConcurrentGC::op_global_coalesce_and_fill() {\n+  ShenandoahGenerationalHeap::heap()->coalesce_and_fill_old_regions(true);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":171,"deletions":71,"binary":false,"changes":242,"status":"modified"},{"patch":"@@ -107,1 +107,1 @@\n-\n+  void entry_global_coalesce_and_fill();\n@@ -127,1 +127,1 @@\n-\n+  void op_global_coalesce_and_fill();\n@@ -136,2 +136,0 @@\n-  static bool has_in_place_promotions(ShenandoahHeap* heap) ;\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.hpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -167,0 +167,1 @@\n+      }\n@@ -168,8 +169,7 @@\n-        if (_degen_point == ShenandoahDegenPoint::_degenerated_roots) {\n-          \/\/ We only need this if the concurrent cycle has already swapped the card tables.\n-          \/\/ Marking will use the 'read' table, but interesting pointers may have been\n-          \/\/ recorded in the 'write' table in the time between the cancelled concurrent cycle\n-          \/\/ and this degenerated cycle. These pointers need to be included the 'read' table\n-          \/\/ used to scan the remembered set during the STW mark which follows here.\n-          _generation->merge_write_table();\n-        }\n+      if (_degen_point == ShenandoahDegenPoint::_degenerated_roots) {\n+        \/\/ We only need this if the concurrent cycle has already swapped the card tables.\n+        \/\/ Marking will use the 'read' table, but interesting pointers may have been\n+        \/\/ recorded in the 'write' table in the time between the cancelled concurrent cycle\n+        \/\/ and this degenerated cycle. These pointers need to be included the 'read' table\n+        \/\/ used to scan the remembered set during the STW mark which follows here.\n+        _generation->merge_write_table();\n@@ -283,1 +283,6 @@\n-      op_cleanup_complete();\n+      if (heap->mode()->is_generational() && heap->is_concurrent_old_mark_in_progress()) {\n+        \/\/ This is still necessary for degenerated cycles because the degeneration point may occur\n+        \/\/ after final mark of the young generation. See ShenandoahConcurrentGC::op_final_updaterefs for\n+        \/\/ a more detailed explanation.\n+        heap->old_generation()->transfer_pointers_from_satb();\n+      }\n@@ -285,0 +290,2 @@\n+      op_cleanup_complete();\n+      \/\/ We defer generation resizing actions until after cset regions have been recycled.\n@@ -286,1 +293,6 @@\n-        ShenandoahGenerationalHeap::heap()->complete_degenerated_cycle();\n+        auto result = ShenandoahGenerationalHeap::heap()->balance_generations();\n+        LogTarget(Info, gc, ergo) lt;\n+        if (lt.is_enabled()) {\n+          LogStream ls(lt);\n+          result.print_on(\"Degenerated GC\", &ls);\n+        }\n@@ -288,1 +300,0 @@\n-\n@@ -294,0 +305,10 @@\n+  if (heap->mode()->is_generational()) {\n+    \/\/ In case degeneration interrupted concurrent evacuation or update references, we need to clean up transient state.\n+    \/\/ Otherwise, these actions have no effect.\n+    ShenandoahGenerationalHeap::heap()->reset_generation_reserves();\n+\n+    if (!ShenandoahGenerationalHeap::heap()->old_generation()->is_parseable()) {\n+      op_global_coalesce_and_fill();\n+    }\n+  }\n+\n@@ -385,0 +406,5 @@\n+void ShenandoahDegenGC::op_global_coalesce_and_fill() {\n+  ShenandoahGCPhase phase(ShenandoahPhaseTimings::degen_gc_coalesce_and_fill);\n+  ShenandoahGenerationalHeap::heap()->coalesce_and_fill_old_regions(false);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.cpp","additions":37,"deletions":11,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -61,0 +61,3 @@\n+  \/\/ This will rebuild card offsets, which is necessary if classes were unloaded\n+  void op_global_coalesce_and_fill();\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -36,1 +36,0 @@\n-#include \"gc\/shenandoah\/shenandoahMonitoringSupport.hpp\"\n@@ -42,1 +41,0 @@\n-#include \"gc\/shenandoah\/shenandoahWorkerPolicy.hpp\"\n@@ -46,1 +44,0 @@\n-#include \"utilities\/events.hpp\"\n@@ -950,83 +947,0 @@\n-\n-void ShenandoahGenerationalHeap::complete_degenerated_cycle() {\n-  shenandoah_assert_heaplocked_or_safepoint();\n-  if (is_concurrent_old_mark_in_progress()) {\n-    \/\/ This is still necessary for degenerated cycles because the degeneration point may occur\n-    \/\/ after final mark of the young generation. See ShenandoahConcurrentGC::op_final_updaterefs for\n-    \/\/ a more detailed explanation.\n-    old_generation()->transfer_pointers_from_satb();\n-  }\n-\n-  \/\/ We defer generation resizing actions until after cset regions have been recycled.\n-  TransferResult result = balance_generations();\n-  LogTarget(Info, gc, ergo) lt;\n-  if (lt.is_enabled()) {\n-    LogStream ls(lt);\n-    result.print_on(\"Degenerated GC\", &ls);\n-  }\n-\n-  \/\/ In case degeneration interrupted concurrent evacuation or update references, we need to clean up\n-  \/\/ transient state. Otherwise, these actions have no effect.\n-  reset_generation_reserves();\n-\n-  if (!old_generation()->is_parseable()) {\n-    ShenandoahGCPhase phase(ShenandoahPhaseTimings::degen_gc_coalesce_and_fill);\n-    coalesce_and_fill_old_regions(false);\n-  }\n-}\n-\n-void ShenandoahGenerationalHeap::complete_concurrent_cycle() {\n-  if (!old_generation()->is_parseable()) {\n-    \/\/ Class unloading may render the card offsets unusable, so we must rebuild them before\n-    \/\/ the next remembered set scan. We _could_ let the control thread do this sometime after\n-    \/\/ the global cycle has completed and before the next young collection, but under memory\n-    \/\/ pressure the control thread may not have the time (that is, because it's running back\n-    \/\/ to back GCs). In that scenario, we would have to make the old regions parsable before\n-    \/\/ we could start a young collection. This could delay the start of the young cycle and\n-    \/\/ throw off the heuristics.\n-    entry_global_coalesce_and_fill();\n-  }\n-\n-  TransferResult result;\n-  {\n-    ShenandoahHeapLocker locker(lock());\n-\n-    result = balance_generations();\n-    reset_generation_reserves();\n-  }\n-\n-  LogTarget(Info, gc, ergo) lt;\n-  if (lt.is_enabled()) {\n-    LogStream ls(lt);\n-    result.print_on(\"Concurrent GC\", &ls);\n-  }\n-}\n-\n-void ShenandoahGenerationalHeap::entry_global_coalesce_and_fill() {\n-  const char* msg = \"Coalescing and filling old regions\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_coalesce_and_fill);\n-\n-  TraceCollectorStats tcs(monitoring_support()->concurrent_collection_counters());\n-  EventMark em(\"%s\", msg);\n-  ShenandoahWorkerScope scope(workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_conc_marking(),\n-                              \"concurrent coalesce and fill\");\n-\n-  coalesce_and_fill_old_regions(true);\n-}\n-\n-void ShenandoahGenerationalHeap::update_region_ages() {\n-  ShenandoahMarkingContext *ctx = complete_marking_context();\n-  for (size_t i = 0; i < num_regions(); i++) {\n-    ShenandoahHeapRegion *r = get_region(i);\n-    if (r->is_active() && r->is_young()) {\n-      HeapWord* tams = ctx->top_at_mark_start(r);\n-      HeapWord* top = r->top();\n-      if (top > tams) {\n-        r->reset_age();\n-      } else if (is_aging_cycle()) {\n-        r->increment_age();\n-      }\n-    }\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":0,"deletions":86,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -53,4 +53,0 @@\n-  \/\/ Ages regions that haven't been used for allocations in the current cycle.\n-  \/\/ Resets ages for regions that have been used for allocations.\n-  void update_region_ages();\n-\n@@ -69,1 +65,0 @@\n-\n@@ -111,3 +106,3 @@\n-  \/\/ Balances generations, coalesces and fills old regions if necessary\n-  void complete_degenerated_cycle();\n-  void complete_concurrent_cycle();\n+  \/\/ Makes old regions parsable\n+  void coalesce_and_fill_old_regions(bool concurrent);\n+\n@@ -116,4 +111,0 @@\n-  void entry_global_coalesce_and_fill();\n-\n-  \/\/ Makes old regions parsable. This will also rebuild card offsets, which is necessary if classes were unloaded\n-  void coalesce_and_fill_old_regions(bool concurrent);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.hpp","additions":3,"deletions":12,"binary":false,"changes":15,"status":"modified"}]}