{"files":[{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -24,1 +25,0 @@\n-\n@@ -27,0 +27,4 @@\n+\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahSpaceInfo.hpp\"\n@@ -29,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n@@ -34,0 +39,1 @@\n+#include \"runtime\/globals_extension.hpp\"\n@@ -61,1 +67,2 @@\n-  _last_trigger(OTHER) { }\n+  _last_trigger(OTHER),\n+  _available(Moving_Average_Samples, ShenandoahAdaptiveDecayFactor) { }\n@@ -89,2 +96,2 @@\n-  size_t free_target = (capacity \/ 100 * ShenandoahMinFreeThreshold) + max_cset;\n-  size_t min_garbage = (free_target > actual_free ? (free_target - actual_free) : 0);\n+  size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_cset;\n+  size_t min_garbage = (free_target > actual_free) ? (free_target - actual_free) : 0;\n@@ -93,1 +100,1 @@\n-                     SIZE_FORMAT \"%s, Max CSet: \" SIZE_FORMAT \"%s, Min Garbage: \" SIZE_FORMAT \"%s\",\n+                     SIZE_FORMAT \"%s, Max Evacuation: \" SIZE_FORMAT \"%s, Min Garbage: \" SIZE_FORMAT \"%s\",\n@@ -133,1 +140,0 @@\n-  _available.add(available);\n@@ -135,2 +141,10 @@\n-  if (_available.sd() > 0) {\n-    z_score = (available - _available.avg()) \/ _available.sd();\n+  double available_sd = _available.sd();\n+  if (available_sd > 0) {\n+    double available_avg = _available.avg();\n+    z_score = (double(available) - available_avg) \/ available_sd;\n+    log_debug(gc, ergo)(\"%s Available: \" SIZE_FORMAT \" %sB, z-score=%.3f. Average available: %.1f %sB +\/- %.1f %sB.\",\n+                        _space_info->name(),\n+                        byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n+                        z_score,\n+                        byte_size_in_proper_unit(available_avg), proper_unit_for_byte_size(available_avg),\n+                        byte_size_in_proper_unit(available_sd), proper_unit_for_byte_size(available_sd));\n@@ -139,5 +153,1 @@\n-  log_debug(gc, ergo)(\"Available: \" SIZE_FORMAT \" %sB, z-score=%.3f. Average available: %.1f %sB +\/- %.1f %sB.\",\n-                      byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n-                      z_score,\n-                      byte_size_in_proper_unit(_available.avg()), proper_unit_for_byte_size(_available.avg()),\n-                      byte_size_in_proper_unit(_available.sd()), proper_unit_for_byte_size(_available.sd()));\n+  _available.add(double(available));\n@@ -199,1 +209,0 @@\n-  size_t max_capacity = _space_info->max_capacity();\n@@ -201,1 +210,1 @@\n-  size_t available = _space_info->available();\n+  size_t available = _space_info->soft_available();\n@@ -204,3 +213,3 @@\n-  \/\/ Make sure the code below treats available without the soft tail.\n-  size_t soft_tail = max_capacity - capacity;\n-  available = (available > soft_tail) ? (available - soft_tail) : 0;\n+  log_debug(gc)(\"should_start_gc (%s)? available: \" SIZE_FORMAT \", soft_max_capacity: \" SIZE_FORMAT\n+                \", allocated: \" SIZE_FORMAT,\n+                _space_info->name(), available, capacity, allocated);\n@@ -212,1 +221,1 @@\n-  size_t min_threshold = capacity \/ 100 * ShenandoahMinFreeThreshold;\n+  size_t min_threshold = min_free_threshold();\n@@ -214,2 +223,2 @@\n-    log_info(gc)(\"Trigger: Free (\" SIZE_FORMAT \"%s) is below minimum threshold (\" SIZE_FORMAT \"%s)\",\n-                 byte_size_in_proper_unit(available),     proper_unit_for_byte_size(available),\n+    log_info(gc)(\"Trigger (%s): Free (\" SIZE_FORMAT \"%s) is below minimum threshold (\" SIZE_FORMAT \"%s)\", _space_info->name(),\n+                 byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n@@ -220,0 +229,1 @@\n+  \/\/ Check if we need to learn a bit about the application\n@@ -224,3 +234,3 @@\n-      log_info(gc)(\"Trigger: Learning \" SIZE_FORMAT \" of \" SIZE_FORMAT \". Free (\" SIZE_FORMAT \"%s) is below initial threshold (\" SIZE_FORMAT \"%s)\",\n-                   _gc_times_learned + 1, max_learn,\n-                   byte_size_in_proper_unit(available),      proper_unit_for_byte_size(available),\n+      log_info(gc)(\"Trigger (%s): Learning \" SIZE_FORMAT \" of \" SIZE_FORMAT \". Free (\" SIZE_FORMAT \"%s) is below initial threshold (\" SIZE_FORMAT \"%s)\",\n+                   _space_info->name(), _gc_times_learned + 1, max_learn,\n+                   byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n@@ -231,1 +241,28 @@\n-\n+  \/\/  Rationale:\n+  \/\/    The idea is that there is an average allocation rate and there are occasional abnormal bursts (or spikes) of\n+  \/\/    allocations that exceed the average allocation rate.  What do these spikes look like?\n+  \/\/\n+  \/\/    1. At certain phase changes, we may discard large amounts of data and replace it with large numbers of newly\n+  \/\/       allocated objects.  This \"spike\" looks more like a phase change.  We were in steady state at M bytes\/sec\n+  \/\/       allocation rate and now we're in a \"reinitialization phase\" that looks like N bytes\/sec.  We need the \"spike\"\n+  \/\/       accommodation to give us enough runway to recalibrate our \"average allocation rate\".\n+  \/\/\n+  \/\/   2. The typical workload changes.  \"Suddenly\", our typical workload of N TPS increases to N+delta TPS.  This means\n+  \/\/       our average allocation rate needs to be adjusted.  Once again, we need the \"spike\" accomodation to give us\n+  \/\/       enough runway to recalibrate our \"average allocation rate\".\n+  \/\/\n+  \/\/    3. Though there is an \"average\" allocation rate, a given workload's demand for allocation may be very bursty.  We\n+  \/\/       allocate a bunch of LABs during the 5 ms that follow completion of a GC, then we perform no more allocations for\n+  \/\/       the next 150 ms.  It seems we want the \"spike\" to represent the maximum divergence from average within the\n+  \/\/       period of time between consecutive evaluation of the should_start_gc() service.  Here's the thinking:\n+  \/\/\n+  \/\/       a) Between now and the next time I ask whether should_start_gc(), we might experience a spike representing\n+  \/\/          the anticipated burst of allocations.  If that would put us over budget, then we should start GC immediately.\n+  \/\/       b) Between now and the anticipated depletion of allocation pool, there may be two or more bursts of allocations.\n+  \/\/          If there are more than one of these bursts, we can \"approximate\" that these will be separated by spans of\n+  \/\/          time with very little or no allocations so the \"average\" allocation rate should be a suitable approximation\n+  \/\/          of how this will behave.\n+  \/\/\n+  \/\/    For cases 1 and 2, we need to \"quickly\" recalibrate the average allocation rate whenever we detect a change\n+  \/\/    in operation mode.  We want some way to decide that the average rate has changed.  Make average allocation rate\n+  \/\/    computations an independent effort.\n@@ -233,1 +270,1 @@\n-  \/\/   1. Some space to absorb allocation spikes\n+  \/\/   1. Some space to absorb allocation spikes (ShenandoahAllocSpikeFactor)\n@@ -243,1 +280,1 @@\n-  double avg_cycle_time = _gc_time_history->davg() + (_margin_of_error_sd * _gc_time_history->dsd());\n+  double avg_cycle_time = _gc_cycle_time_history->davg() + (_margin_of_error_sd * _gc_cycle_time_history->dsd());\n@@ -245,0 +282,1 @@\n+<<<<<<< HEAD\n@@ -248,0 +286,9 @@\n+=======\n+  log_debug(gc)(\"%s: average GC time: %.2f ms, allocation rate: %.0f %s\/s\",\n+                _space_info->name(),\n+          avg_cycle_time * 1000, byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate));\n+  if (avg_cycle_time > allocation_headroom \/ avg_alloc_rate) {\n+    log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for average allocation rate (%.0f %sB\/s)\"\n+                 \" to deplete free headroom (\" SIZE_FORMAT \"%s) (margin of error = %.2f)\",\n+                 _space_info->name(), avg_cycle_time * 1000,\n+>>>>>>> e30fa929b19e8b7d6db9073fdf640938d3db39fd\n@@ -251,1 +298,0 @@\n-\n@@ -257,1 +303,0 @@\n-\n@@ -264,2 +309,2 @@\n-    log_info(gc)(\"Trigger: Average GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (spike threshold = %.2f)\",\n-                 avg_cycle_time * 1000,\n+    log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (spike threshold = %.2f)\",\n+                 _space_info->name(), avg_cycle_time * 1000,\n@@ -302,0 +347,7 @@\n+size_t ShenandoahAdaptiveHeuristics::min_free_threshold() {\n+  \/\/ Note that soft_max_capacity() \/ 100 * min_free_threshold is smaller than max_capacity() \/ 100 * min_free_threshold.\n+  \/\/ We want to behave conservatively here, so use max_capacity().  By returning a larger value, we cause the GC to\n+  \/\/ trigger when the remaining amount of free shrinks below the larger threshold.\n+  return _space_info->max_capacity() \/ 100 * ShenandoahMinFreeThreshold;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.cpp","additions":83,"deletions":31,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -28,0 +28,2 @@\n+#include \"gc\/shenandoah\/shenandoahAffiliation.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n@@ -32,0 +34,3 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -43,0 +48,1 @@\n+    case ShenandoahFreeSetPartitionId::OldCollector: return \"OldCollector\";\n@@ -51,1 +57,2 @@\n-  log_info(gc)(\"Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"], Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+  log_info(gc)(\"Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"], Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT\n+               \"], Old Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n@@ -55,1 +62,3 @@\n-               _rightmosts[int(ShenandoahFreeSetPartitionId::Collector)]);\n+               _rightmosts[int(ShenandoahFreeSetPartitionId::Collector)],\n+               _leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+               _rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)]);\n@@ -57,1 +66,2 @@\n-               \"], Empty Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+               \"], Empty Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT\n+               \"], Empty Old Collecto range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n@@ -61,1 +71,3 @@\n-               _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)]);\n+               _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)],\n+               _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+               _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)]);\n@@ -63,1 +75,1 @@\n-  log_info(gc)(\"%6s: %18s %18s %18s\", \"index\", \"Mutator Bits\", \"Collector Bits\", \"NotFree Bits\");\n+  log_info(gc)(\"%6s: %18s %18s %18s %18s\", \"index\", \"Mutator Bits\", \"Collector Bits\", \"Old Collector Bits\", \"NotFree Bits\");\n@@ -84,1 +96,2 @@\n-  uintx free_bits = mutator_bits | collector_bits;\n+  uintx old_collector_bits = _membership[int(ShenandoahFreeSetPartitionId::OldCollector)].bits_at(aligned_idx);\n+  uintx free_bits = mutator_bits | collector_bits | old_collector_bits;\n@@ -86,2 +99,2 @@\n-  log_info(gc)(SSIZE_FORMAT_W(6) \": \" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0,\n-               aligned_idx, mutator_bits, collector_bits, notfree_bits);\n+  log_info(gc)(SSIZE_FORMAT_W(6) \": \" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0,\n+               aligned_idx, mutator_bits, collector_bits, old_collector_bits, notfree_bits);\n@@ -95,1 +108,1 @@\n-    _membership{ ShenandoahSimpleBitMap(max_regions), ShenandoahSimpleBitMap(max_regions) }\n+    _membership{ ShenandoahSimpleBitMap(max_regions), ShenandoahSimpleBitMap(max_regions) , ShenandoahSimpleBitMap(max_regions) }\n@@ -165,1 +178,0 @@\n-  _region_counts[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_region_count;\n@@ -185,0 +197,14 @@\n+void ShenandoahRegionPartitions::establish_old_collector_intervals(idx_t old_collector_leftmost, idx_t old_collector_rightmost,\n+                                                                   idx_t old_collector_leftmost_empty,\n+                                                                   idx_t old_collector_rightmost_empty,\n+                                                                   size_t old_collector_region_count, size_t old_collector_used) {\n+  _leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_leftmost;\n+  _rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_rightmost;\n+  _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_leftmost_empty;\n+  _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_rightmost_empty;\n+\n+  _region_counts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_region_count;\n+  _used[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_used;\n+  _capacity[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_region_count * _region_size_bytes;\n+}\n+\n@@ -205,1 +231,1 @@\n-      _leftmosts_empty[int(partition)] = leftmost(partition);\n+      _leftmosts_empty[int(partition)] = _leftmosts[int(partition)];\n@@ -292,1 +318,0 @@\n-\n@@ -296,0 +321,17 @@\n+bool ShenandoahRegionPartitions::is_mutator_partition(ShenandoahFreeSetPartitionId p) {\n+  return (p == ShenandoahFreeSetPartitionId::Mutator);\n+}\n+\n+bool ShenandoahRegionPartitions::is_young_collector_partition(ShenandoahFreeSetPartitionId p) {\n+  return (p == ShenandoahFreeSetPartitionId::Collector);\n+}\n+\n+bool ShenandoahRegionPartitions::is_old_collector_partition(ShenandoahFreeSetPartitionId p) {\n+  return (p == ShenandoahFreeSetPartitionId::OldCollector);\n+}\n+\n+bool ShenandoahRegionPartitions::available_implies_empty(size_t available_in_region) {\n+  return (available_in_region == _region_size_bytes);\n+}\n+\n+\n@@ -298,0 +340,1 @@\n+  ShenandoahHeapRegion* r = ShenandoahHeap::heap()->get_region(idx);\n@@ -302,0 +345,5 @@\n+  assert (_membership[int(orig_partition)].is_set(idx), \"Cannot move from partition unless in partition\");\n+  assert ((r != nullptr) && ((r->is_trash() && (available == _region_size_bytes)) ||\n+                             (r->used() + available == _region_size_bytes)),\n+          \"Used: \" SIZE_FORMAT \" + available: \" SIZE_FORMAT \" should equal region size: \" SIZE_FORMAT,\n+          ShenandoahHeap::heap()->get_region(idx)->used(), available, _region_size_bytes);\n@@ -305,0 +353,2 @@\n+  \/\/                          Mutator empty => Collector\n+  \/\/                          Mutator empty => OldCollector\n@@ -306,0 +356,1 @@\n+  \/\/                          Mutator empty => OldCollector\n@@ -307,8 +358,10 @@\n-  assert (((available <= _region_size_bytes) &&\n-           (((orig_partition == ShenandoahFreeSetPartitionId::Mutator)\n-             && (new_partition == ShenandoahFreeSetPartitionId::Collector)) ||\n-            ((orig_partition == ShenandoahFreeSetPartitionId::Collector)\n-             && (new_partition == ShenandoahFreeSetPartitionId::Mutator)))) ||\n-          ((available == _region_size_bytes) &&\n-           ((orig_partition == ShenandoahFreeSetPartitionId::Mutator)\n-            && (new_partition == ShenandoahFreeSetPartitionId::Collector))), \"Unexpected movement between partitions\");\n+  \/\/                          OldCollector Empty => Mutator\n+  assert ((is_mutator_partition(orig_partition) && is_young_collector_partition(new_partition)) ||\n+          (is_mutator_partition(orig_partition) &&\n+           available_implies_empty(available) && is_old_collector_partition(new_partition)) ||\n+          (is_young_collector_partition(orig_partition) && is_mutator_partition(new_partition)) ||\n+          (is_old_collector_partition(orig_partition)\n+           && available_implies_empty(available) && is_mutator_partition(new_partition)),\n+          \"Unexpected movement between partitions, available: \" SIZE_FORMAT \", _region_size_bytes: \" SIZE_FORMAT\n+          \", orig_partition: %s, new_partition: %s\",\n+          available, _region_size_bytes, partition_name(orig_partition), partition_name(new_partition));\n@@ -317,0 +370,3 @@\n+  assert (_used[int(orig_partition)] >= used,\n+          \"Orig partition used: \" SIZE_FORMAT \" must exceed moved used: \" SIZE_FORMAT \" within region \" SSIZE_FORMAT,\n+          _used[int(orig_partition)], used, idx);\n@@ -485,0 +541,1 @@\n+      case ShenandoahFreeSetPartitionId::OldCollector:\n@@ -574,0 +631,35 @@\n+\n+  \/\/ Performance invariants. Failing these would not break the free partition, but performance would suffer.\n+  assert (leftmost(ShenandoahFreeSetPartitionId::OldCollector) <= _max, \"leftmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT,\n+          leftmost(ShenandoahFreeSetPartitionId::OldCollector),  _max);\n+  assert (rightmost(ShenandoahFreeSetPartitionId::OldCollector) < _max, \"rightmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT,\n+          rightmost(ShenandoahFreeSetPartitionId::OldCollector),  _max);\n+\n+  assert (leftmost(ShenandoahFreeSetPartitionId::OldCollector) == _max\n+          || partition_id_matches(leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                                  ShenandoahFreeSetPartitionId::OldCollector),\n+          \"leftmost region should be free: \" SSIZE_FORMAT,  leftmost(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (leftmost(ShenandoahFreeSetPartitionId::OldCollector) == _max\n+          || partition_id_matches(rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                                  ShenandoahFreeSetPartitionId::OldCollector),\n+          \"rightmost region should be free: \" SSIZE_FORMAT, rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+\n+  \/\/ If OldCollector partition is empty, leftmosts will both equal max, rightmosts will both equal zero.\n+  \/\/ Likewise for empty region partitions.\n+  beg_off = leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  end_off = rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  assert (beg_off >= leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+          \"free regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (end_off <= rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n+          \"free regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+\n+  beg_off = empty_leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  end_off = empty_rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  assert (beg_off >= _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+          \"free empty regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (end_off <= _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+          \"free empty regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n@@ -580,0 +672,1 @@\n+<<<<<<< HEAD\n@@ -582,0 +675,2 @@\n+=======\n+>>>>>>> e30fa929b19e8b7d6db9073fdf640938d3db39fd\n@@ -587,0 +682,52 @@\n+void ShenandoahFreeSet::add_promoted_in_place_region_to_old_collector(ShenandoahHeapRegion* region) {\n+  shenandoah_assert_heaplocked();\n+  size_t plab_min_size_in_bytes = ShenandoahGenerationalHeap::heap()->plab_min_size() * HeapWordSize;\n+  size_t idx = region->index();\n+  size_t capacity = alloc_capacity(region);\n+  assert(_partitions.membership(idx) == ShenandoahFreeSetPartitionId::NotFree,\n+         \"Regions promoted in place should have been excluded from Mutator partition\");\n+  if (capacity >= plab_min_size_in_bytes) {\n+    _partitions.make_free(idx, ShenandoahFreeSetPartitionId::OldCollector, capacity);\n+    _heap->old_generation()->augment_promoted_reserve(capacity);\n+  }\n+}\n+\n+HeapWord* ShenandoahFreeSet::allocate_from_partition_with_affiliation(ShenandoahFreeSetPartitionId which_partition,\n+                                                                      ShenandoahAffiliation affiliation,\n+                                                                      ShenandoahAllocRequest& req, bool& in_new_region) {\n+  shenandoah_assert_heaplocked();\n+  idx_t rightmost_collector = ((affiliation == ShenandoahAffiliation::FREE)?\n+                               _partitions.rightmost_empty(which_partition): _partitions.rightmost(which_partition));\n+  idx_t leftmost_collector = ((affiliation == ShenandoahAffiliation::FREE)?\n+                              _partitions.leftmost_empty(which_partition): _partitions.leftmost(which_partition));\n+  if (_partitions.alloc_from_left_bias(which_partition)) {\n+    for (idx_t idx = leftmost_collector; idx <= rightmost_collector; ) {\n+      assert(_partitions.in_free_set(which_partition, idx), \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n+      ShenandoahHeapRegion* r = _heap->get_region(idx);\n+      if (r->affiliation() == affiliation) {\n+        HeapWord* result = try_allocate_in(r, req, in_new_region);\n+        if (result != nullptr) {\n+          return result;\n+        }\n+      }\n+      idx = _partitions.find_index_of_next_available_region(which_partition, idx + 1);\n+    }\n+  } else {\n+    for (idx_t idx = rightmost_collector; idx >= leftmost_collector; ) {\n+      assert(_partitions.in_free_set(which_partition, idx),\n+             \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n+      ShenandoahHeapRegion* r = _heap->get_region(idx);\n+      if (r->affiliation() == affiliation) {\n+        HeapWord* result = try_allocate_in(r, req, in_new_region);\n+        if (result != nullptr) {\n+          return result;\n+        }\n+      }\n+      idx = _partitions.find_index_of_previous_available_region(which_partition, idx - 1);\n+    }\n+  }\n+  log_debug(gc, free)(\"Could not allocate collector region with affiliation: %s for request \" PTR_FORMAT,\n+                      shenandoah_affiliation_name(affiliation), p2i(&req));\n+  return nullptr;\n+}\n+\n@@ -592,1 +739,2 @@\n-  \/\/ Leftmost and rightmost bounds provide enough caching to quickly find a region from which to allocate.\n+  \/\/ Leftmost and rightmost bounds provide enough caching to walk bitmap efficiently. Normally,\n+  \/\/ we would find the region to allocate at right away.\n@@ -598,3 +746,23 @@\n-  \/\/ Free set maintains mutator and collector partitions.  Mutator can only allocate from the\n-  \/\/ Mutator partition.  Collector prefers to allocate from the Collector partition, but may steal\n-  \/\/ regions from the Mutator partition if the Collector partition has been depleted.\n+  \/\/ Free set maintains mutator and collector partitions.  Normally, each allocates only from its partition,\n+  \/\/ except in special cases when the collector steals regions from the mutator partition.\n+\n+  \/\/ Overwrite with non-zero (non-NULL) values only if necessary for allocation bookkeeping.\n+  bool allow_new_region = true;\n+  if (_heap->mode()->is_generational()) {\n+    switch (req.affiliation()) {\n+      case ShenandoahAffiliation::OLD_GENERATION:\n+        \/\/ Note: unsigned result from free_unaffiliated_regions() will never be less than zero, but it may equal zero.\n+        if (_heap->old_generation()->free_unaffiliated_regions() <= 0) {\n+          allow_new_region = false;\n+        }\n+        break;\n+\n+      case ShenandoahAffiliation::YOUNG_GENERATION:\n+        \/\/ Note: unsigned result from free_unaffiliated_regions() will never be less than zero, but it may equal zero.\n+        if (_heap->young_generation()->free_unaffiliated_regions() <= 0) {\n+          allow_new_region = false;\n+        }\n+        break;\n+\n+      case ShenandoahAffiliation::FREE:\n+        fatal(\"Should request affiliation\");\n@@ -602,0 +770,5 @@\n+      default:\n+        ShouldNotReachHere();\n+        break;\n+    }\n+  }\n@@ -626,1 +799,1 @@\n-        _right_to_left_bias = (non_empty_on_right > non_empty_on_left);\n+        _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::Mutator, (non_empty_on_right < non_empty_on_left));\n@@ -629,1 +802,1 @@\n-      if (_right_to_left_bias) {\n+      if (!_partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::Mutator)) {\n@@ -674,0 +847,4 @@\n+    case ShenandoahAllocRequest::_alloc_plab: {\n+      \/\/ PLABs always reside in old-gen and are only allocated during\n+      \/\/ evacuation phase.\n+\n@@ -676,5 +853,11 @@\n-      idx_t leftmost_collector = _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector);\n-      for (idx_t idx = _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector); idx >= leftmost_collector; ) {\n-        assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Collector, idx),\n-               \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n-        HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n+      HeapWord* result;\n+      result = allocate_from_partition_with_affiliation(req.is_old()? ShenandoahFreeSetPartitionId::OldCollector:\n+                                                        ShenandoahFreeSetPartitionId::Collector,\n+                                                        req.affiliation(), req, in_new_region);\n+      if (result != nullptr) {\n+        return result;\n+      } else if (allow_new_region) {\n+        \/\/ Try a free region that is dedicated to GC allocations.\n+        result = allocate_from_partition_with_affiliation(req.is_old()? ShenandoahFreeSetPartitionId::OldCollector:\n+                                                          ShenandoahFreeSetPartitionId::Collector,\n+                                                          ShenandoahAffiliation::FREE, req, in_new_region);\n@@ -684,1 +867,0 @@\n-        idx = _partitions.find_index_of_previous_available_region(ShenandoahFreeSetPartitionId::Collector, idx - 1);\n@@ -691,0 +873,4 @@\n+      if (!allow_new_region && req.is_old() && (_heap->young_generation()->free_unaffiliated_regions() > 0)) {\n+        \/\/ This allows us to flip a mutator region to old_collector\n+        allow_new_region = true;\n+      }\n@@ -692,12 +878,31 @@\n-      \/\/ Try to steal an empty region from the mutator view.\n-      idx_t leftmost_mutator_empty = _partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n-      for (idx_t idx = _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Mutator); idx >= leftmost_mutator_empty; ) {\n-        assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx),\n-               \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n-        ShenandoahHeapRegion* r = _heap->get_region(idx);\n-        if (can_allocate_from(r)) {\n-          flip_to_gc(r);\n-          HeapWord *result = try_allocate_in(r, req, in_new_region);\n-          if (result != nullptr) {\n-            log_debug(gc)(\"Flipped region \" SIZE_FORMAT \" to gc for request: \" PTR_FORMAT, idx, p2i(&req));\n-            return result;\n+      \/\/ We should expand old-gen if this can prevent an old-gen evacuation failure.  We don't care so much about\n+      \/\/ promotion failures since they can be mitigated in a subsequent GC pass.  Would be nice to know if this\n+      \/\/ allocation request is for evacuation or promotion.  Individual threads limit their use of PLAB memory for\n+      \/\/ promotions, so we already have an assurance that any additional memory set aside for old-gen will be used\n+      \/\/ only for old-gen evacuations.\n+\n+      \/\/ TODO:\n+      \/\/ if (GC is idle (out of cycle) and mutator allocation fails and there is memory reserved in Collector\n+      \/\/ or OldCollector sets, transfer a region of memory so that we can satisfy the allocation request, and\n+      \/\/ immediately trigger the start of GC.  Is better to satisfy the allocation than to trigger out-of-cycle\n+      \/\/ allocation failure (even if this means we have a little less memory to handle evacuations during the\n+      \/\/ subsequent GC pass).\n+\n+      if (allow_new_region) {\n+        \/\/ Try to steal an empty region from the mutator view.\n+        idx_t rightmost_mutator = _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n+        idx_t leftmost_mutator =  _partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n+        for (idx_t idx = rightmost_mutator; idx >= leftmost_mutator; ) {\n+          assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx),\n+                 \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n+          ShenandoahHeapRegion* r = _heap->get_region(idx);\n+          if (can_allocate_from(r)) {\n+            if (req.is_old()) {\n+              flip_to_old_gc(r);\n+            } else {\n+              flip_to_gc(r);\n+            }\n+            \/\/ Region r is entirely empty.  If try_allocat_in fails on region r, something else is really wrong.\n+            \/\/ Don't bother to retry with other regions.\n+            log_debug(gc, free)(\"Flipped region \" SIZE_FORMAT \" to gc for request: \" PTR_FORMAT, idx, p2i(&req));\n+            return try_allocate_in(r, req, in_new_region);\n@@ -705,0 +910,1 @@\n+          idx = _partitions.find_index_of_previous_available_region(ShenandoahFreeSetPartitionId::Mutator, idx - 1);\n@@ -706,1 +912,0 @@\n-        idx = _partitions.find_index_of_previous_available_region(ShenandoahFreeSetPartitionId::Mutator, idx - 1);\n@@ -708,1 +913,0 @@\n-\n@@ -713,0 +917,1 @@\n+    }\n@@ -719,0 +924,50 @@\n+\/\/ This work method takes an argument corresponding to the number of bytes\n+\/\/ free in a region, and returns the largest amount in heapwords that can be allocated\n+\/\/ such that both of the following conditions are satisfied:\n+\/\/\n+\/\/ 1. it is a multiple of card size\n+\/\/ 2. any remaining shard may be filled with a filler object\n+\/\/\n+\/\/ The idea is that the allocation starts and ends at card boundaries. Because\n+\/\/ a region ('s end) is card-aligned, the remainder shard that must be filled is\n+\/\/ at the start of the free space.\n+\/\/\n+\/\/ This is merely a helper method to use for the purpose of such a calculation.\n+size_t ShenandoahFreeSet::get_usable_free_words(size_t free_bytes) const {\n+  \/\/ e.g. card_size is 512, card_shift is 9, min_fill_size() is 8\n+  \/\/      free is 514\n+  \/\/      usable_free is 512, which is decreased to 0\n+  size_t usable_free = (free_bytes \/ CardTable::card_size()) << CardTable::card_shift();\n+  assert(usable_free <= free_bytes, \"Sanity check\");\n+  if ((free_bytes != usable_free) && (free_bytes - usable_free < ShenandoahHeap::min_fill_size() * HeapWordSize)) {\n+    \/\/ After aligning to card multiples, the remainder would be smaller than\n+    \/\/ the minimum filler object, so we'll need to take away another card's\n+    \/\/ worth to construct a filler object.\n+    if (usable_free >= CardTable::card_size()) {\n+      usable_free -= CardTable::card_size();\n+    } else {\n+      assert(usable_free == 0, \"usable_free is a multiple of card_size and card_size > min_fill_size\");\n+    }\n+  }\n+\n+  return usable_free \/ HeapWordSize;\n+}\n+\n+\/\/ Given a size argument, which is a multiple of card size, a request struct\n+\/\/ for a PLAB, and an old region, return a pointer to the allocated space for\n+\/\/ a PLAB which is card-aligned and where any remaining shard in the region\n+\/\/ has been suitably filled by a filler object.\n+\/\/ It is assumed (and assertion-checked) that such an allocation is always possible.\n+HeapWord* ShenandoahFreeSet::allocate_aligned_plab(size_t size, ShenandoahAllocRequest& req, ShenandoahHeapRegion* r) {\n+  assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+  assert(r->is_old(), \"All PLABs reside in old-gen\");\n+  assert(!req.is_mutator_alloc(), \"PLABs should not be allocated by mutators.\");\n+  assert(is_aligned(size, CardTable::card_size_in_words()), \"Align by design\");\n+\n+  HeapWord* result = r->allocate_aligned(size, req, CardTable::card_size());\n+  assert(result != nullptr, \"Allocation cannot fail\");\n+  assert(r->top() <= r->end(), \"Allocation cannot span end of region\");\n+  assert(is_aligned(result, CardTable::card_size_in_words()), \"Align by design\");\n+  return result;\n+}\n+\n@@ -724,1 +979,0 @@\n-\n@@ -732,0 +986,28 @@\n+    assert(!r->is_affiliated(), \"New region \" SIZE_FORMAT \" should be unaffiliated\", r->index());\n+    r->set_affiliation(req.affiliation());\n+    if (r->is_old()) {\n+      \/\/ Any OLD region allocated during concurrent coalesce-and-fill does not need to be coalesced and filled because\n+      \/\/ all objects allocated within this region are above TAMS (and thus are implicitly marked).  In case this is an\n+      \/\/ OLD region and concurrent preparation for mixed evacuations visits this region before the start of the next\n+      \/\/ old-gen concurrent mark (i.e. this region is allocated following the start of old-gen concurrent mark but before\n+      \/\/ concurrent preparations for mixed evacuations are completed), we mark this region as not requiring any\n+      \/\/ coalesce-and-fill processing.\n+      r->end_preemptible_coalesce_and_fill();\n+      _heap->old_generation()->clear_cards_for(r);\n+    }\n+    _heap->generation_for(r->affiliation())->increment_affiliated_region_count();\n+\n+#ifdef ASSERT\n+    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+    assert(ctx->top_at_mark_start(r) == r->bottom(), \"Newly established allocation region starts with TAMS equal to bottom\");\n+    assert(ctx->is_bitmap_clear_range(ctx->top_bitmap(r), r->end()), \"Bitmap above top_bitmap() must be clear\");\n+#endif\n+    log_debug(gc)(\"Using new region (\" SIZE_FORMAT \") for %s (\" PTR_FORMAT \").\",\n+                       r->index(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(&req));\n+  } else {\n+    assert(r->is_affiliated(), \"Region \" SIZE_FORMAT \" that is not new should be affiliated\", r->index());\n+    if (r->affiliation() != req.affiliation()) {\n+      assert(_heap->mode()->is_generational(), \"Request for %s from %s region should only happen in generational mode.\",\n+             req.affiliation_name(), r->affiliation_name());\n+      return nullptr;\n+    }\n@@ -736,1 +1018,0 @@\n-    \/\/ This is a GCLAB or a TLAB allocation\n@@ -738,12 +1019,23 @@\n-    size_t free = align_down(r->free() >> LogHeapWordSize, MinObjAlignment);\n-    if (adjusted_size > free) {\n-      adjusted_size = free;\n-    }\n-    if (adjusted_size >= req.min_size()) {\n-      result = r->allocate(adjusted_size, req.type());\n-      log_debug(gc)(\"Allocated \" SIZE_FORMAT \" words (adjusted from \" SIZE_FORMAT \") for %s @\" PTR_FORMAT\n-                          \" from %s region \" SIZE_FORMAT \", free bytes remaining: \" SIZE_FORMAT,\n-                          adjusted_size, req.size(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(result),\n-                          _partitions.partition_membership_name(r->index()), r->index(), r->free());\n-      assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, adjusted_size);\n-      req.set_actual_size(adjusted_size);\n+    size_t free = r->free();    \/\/ free represents bytes available within region r\n+    if (req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+      \/\/ This is a PLAB allocation\n+      assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+      assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, r->index()),\n+             \"PLABS must be allocated in old_collector_free regions\");\n+\n+      \/\/ Need to assure that plabs are aligned on multiple of card region\n+      \/\/ Convert free from unaligned bytes to aligned number of words\n+      size_t usable_free = get_usable_free_words(free);\n+      if (adjusted_size > usable_free) {\n+        adjusted_size = usable_free;\n+      }\n+      adjusted_size = align_down(adjusted_size, CardTable::card_size_in_words());\n+      if (adjusted_size >= req.min_size()) {\n+        result = allocate_aligned_plab(adjusted_size, req, r);\n+        assert(result != nullptr, \"allocate must succeed\");\n+        req.set_actual_size(adjusted_size);\n+      } else {\n+        \/\/ Otherwise, leave result == nullptr because the adjusted size is smaller than min size.\n+        log_trace(gc, free)(\"Failed to shrink PLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n+                            \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n+      }\n@@ -751,2 +1043,14 @@\n-      log_trace(gc, free)(\"Failed to shrink TLAB or GCLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n-                          \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n+      \/\/ This is a GCLAB or a TLAB allocation\n+      \/\/ Convert free from unaligned bytes to aligned number of words\n+      free = align_down(free >> LogHeapWordSize, MinObjAlignment);\n+      if (adjusted_size > free) {\n+        adjusted_size = free;\n+      }\n+      if (adjusted_size >= req.min_size()) {\n+        result = r->allocate(adjusted_size, req);\n+        assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, adjusted_size);\n+        req.set_actual_size(adjusted_size);\n+      } else {\n+        log_trace(gc, free)(\"Failed to shrink TLAB or GCLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n+                            \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n+      }\n@@ -756,1 +1060,1 @@\n-    result = r->allocate(size, req.type());\n+    result = r->allocate(size, req);\n@@ -759,4 +1063,0 @@\n-      log_debug(gc)(\"Allocated \" SIZE_FORMAT \" words for %s @\" PTR_FORMAT\n-                          \" from %s region \" SIZE_FORMAT \", free bytes remaining: \" SIZE_FORMAT,\n-                          size, ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(result),\n-                          _partitions.partition_membership_name(r->index()),  r->index(), r->free());\n@@ -770,0 +1070,1 @@\n+      assert(req.is_young(), \"Mutator allocations always come from young generation.\");\n@@ -775,1 +1076,6 @@\n-      \/\/ evacuation are not updated during evacuation.\n+      \/\/ evacuation are not updated during evacuation.  For both young and old regions r, it is essential that all\n+      \/\/ PLABs be made parsable at the end of evacuation.  This is enabled by retiring all plabs at end of evacuation.\n+      \/\/ TODO: Making a PLAB parsable involves placing a filler object in its remnant memory but does not require\n+      \/\/ that the PLAB be disabled for all future purposes.  We may want to introduce a new service to make the\n+      \/\/ PLABs parsable while still allowing the PLAB to serve future allocation requests that arise during the\n+      \/\/ next evacuation pass.\n@@ -777,0 +1083,7 @@\n+      if (r->is_old()) {\n+        _partitions.increase_used(ShenandoahFreeSetPartitionId::OldCollector, req.actual_size() * HeapWordSize);\n+        assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"old-gen allocations use PLAB or shared allocation\");\n+        \/\/ for plabs, we'll sort the difference between evac and promotion usage when we retire the plab\n+      } else {\n+        _partitions.increase_used(ShenandoahFreeSetPartitionId::Collector, req.actual_size() * HeapWordSize);\n+      }\n@@ -791,3 +1104,16 @@\n-    _partitions.retire_from_partition(req.is_mutator_alloc()?\n-                                      ShenandoahFreeSetPartitionId::Mutator: ShenandoahFreeSetPartitionId::Collector,\n-                                      idx, r->used());\n+    ShenandoahFreeSetPartitionId orig_partition;\n+    if (req.is_mutator_alloc()) {\n+      orig_partition = ShenandoahFreeSetPartitionId::Mutator;\n+    } else if (req.type() == ShenandoahAllocRequest::_alloc_gclab) {\n+      orig_partition = ShenandoahFreeSetPartitionId::Collector;\n+    } else if (req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+      orig_partition = ShenandoahFreeSetPartitionId::OldCollector;\n+    } else {\n+      assert(req.type() == ShenandoahAllocRequest::_alloc_shared_gc, \"Unexpected allocation type\");\n+      if (req.is_old()) {\n+        orig_partition = ShenandoahFreeSetPartitionId::OldCollector;\n+      } else {\n+        orig_partition = ShenandoahFreeSetPartitionId::Collector;\n+      }\n+    }\n+    _partitions.retire_from_partition(orig_partition, idx, r->used());\n@@ -806,0 +1132,3 @@\n+  assert(req.is_young(), \"Humongous regions always allocated in YOUNG\");\n+  ShenandoahGeneration* generation = _heap->generation_for(req.affiliation());\n+\n@@ -818,1 +1147,1 @@\n-                                                                            start_range, num);\n+                                                                          start_range, num);\n@@ -863,0 +1192,1 @@\n+  bool is_generational = _heap->mode()->is_generational();\n@@ -885,0 +1215,2 @@\n+    r->set_affiliation(req.affiliation());\n+    r->set_update_watermark(r->bottom());\n@@ -887,1 +1219,1 @@\n-\n+  generation->increase_affiliated_region_count(num);\n@@ -900,0 +1232,3 @@\n+  if (remainder != 0) {\n+    req.set_waste(ShenandoahHeapRegion::region_size_words() - remainder);\n+  }\n@@ -905,1 +1240,0 @@\n-    _heap->decrease_used(r->used());\n@@ -913,0 +1247,1 @@\n+<<<<<<< HEAD\n@@ -915,0 +1250,2 @@\n+=======\n+>>>>>>> e30fa929b19e8b7d6db9073fdf640938d3db39fd\n@@ -935,0 +1272,21 @@\n+void ShenandoahFreeSet::flip_to_old_gc(ShenandoahHeapRegion* r) {\n+  size_t idx = r->index();\n+\n+  assert(_partitions.partition_id_matches(idx, ShenandoahFreeSetPartitionId::Mutator), \"Should be in mutator view\");\n+  assert(can_allocate_from(r), \"Should not be allocated\");\n+\n+  ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+  size_t region_capacity = alloc_capacity(r);\n+  _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                               ShenandoahFreeSetPartitionId::OldCollector, region_capacity);\n+  _partitions.assert_bounds();\n+  _heap->old_generation()->augment_evacuation_reserve(region_capacity);\n+  bool transferred = gen_heap->generation_sizer()->transfer_to_old(1);\n+  if (!transferred) {\n+    log_warning(gc, free)(\"Forcing transfer of \" SIZE_FORMAT \" to old reserve.\", idx);\n+    gen_heap->generation_sizer()->force_transfer_to_old(1);\n+  }\n+  \/\/ We do not ensure that the region is no longer trash, relying on try_allocate_in(), which always comes next,\n+  \/\/ to recycle trash before attempting to allocate anything in the region.\n+}\n+\n@@ -957,1 +1315,0 @@\n-}\n@@ -959,1 +1316,5 @@\n-void ShenandoahFreeSet::find_regions_with_alloc_capacity(size_t &cset_regions) {\n+  _alloc_bias_weight = 0;\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::Mutator, true);\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::Collector, false);\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::OldCollector, false);\n+}\n@@ -961,1 +1322,3 @@\n-  cset_regions = 0;\n+void ShenandoahFreeSet::find_regions_with_alloc_capacity(size_t &young_cset_regions, size_t &old_cset_regions,\n+                                                         size_t &first_old_region, size_t &last_old_region,\n+                                                         size_t &old_region_count) {\n@@ -963,0 +1326,7 @@\n+\n+  first_old_region = _heap->num_regions();\n+  last_old_region = 0;\n+  old_region_count = 0;\n+  old_cset_regions = 0;\n+  young_cset_regions = 0;\n+\n@@ -970,1 +1340,0 @@\n-\n@@ -974,0 +1343,7 @@\n+  size_t old_collector_leftmost = max_regions;\n+  size_t old_collector_rightmost = 0;\n+  size_t old_collector_leftmost_empty = max_regions;\n+  size_t old_collector_rightmost_empty = 0;\n+  size_t old_collector_regions = 0;\n+  size_t old_collector_used = 0;\n+\n@@ -979,1 +1355,13 @@\n-      cset_regions++;\n+      if (region->is_old()) {\n+        old_cset_regions++;\n+      } else {\n+        assert(region->is_young(), \"Trashed region should be old or young\");\n+        young_cset_regions++;\n+      }\n+    } else if (region->is_old()) {\n+      \/\/ count both humongous and regular regions, but don't count trash (cset) regions.\n+      old_region_count++;\n+      if (first_old_region > idx) {\n+        first_old_region = idx;\n+      }\n+      last_old_region = idx;\n@@ -982,0 +1370,1 @@\n+      assert(!region->is_cset(), \"Shouldn't be adding cset regions to the free set\");\n@@ -986,11 +1375,5 @@\n-        _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::Mutator);\n-\n-        if (idx < mutator_leftmost) {\n-          mutator_leftmost = idx;\n-        }\n-        if (idx > mutator_rightmost) {\n-          mutator_rightmost = idx;\n-        }\n-        if (ac == region_size_bytes) {\n-          if (idx < mutator_leftmost_empty) {\n-            mutator_leftmost_empty = idx;\n+        if (region->is_trash() || !region->is_old()) {\n+          \/\/ Both young and old collected regions (trashed) are placed into the Mutator set\n+          _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::Mutator);\n+          if (idx < mutator_leftmost) {\n+            mutator_leftmost = idx;\n@@ -998,2 +1381,2 @@\n-          if (idx > mutator_rightmost_empty) {\n-            mutator_rightmost_empty = idx;\n+          if (idx > mutator_rightmost) {\n+            mutator_rightmost = idx;\n@@ -1001,0 +1384,29 @@\n+          if (ac == region_size_bytes) {\n+            if (idx < mutator_leftmost_empty) {\n+              mutator_leftmost_empty = idx;\n+            }\n+            if (idx > mutator_rightmost_empty) {\n+              mutator_rightmost_empty = idx;\n+            }\n+          }\n+          mutator_regions++;\n+          mutator_used += (region_size_bytes - ac);\n+        } else {\n+          \/\/ !region->is_trash() && region is_old()\n+          _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::OldCollector);\n+          if (idx < old_collector_leftmost) {\n+            old_collector_leftmost = idx;\n+          }\n+          if (idx > old_collector_rightmost) {\n+            old_collector_rightmost = idx;\n+          }\n+          if (ac == region_size_bytes) {\n+            if (idx < old_collector_leftmost_empty) {\n+              old_collector_leftmost_empty = idx;\n+            }\n+            if (idx > old_collector_rightmost_empty) {\n+              old_collector_rightmost_empty = idx;\n+            }\n+          }\n+          old_collector_regions++;\n+          old_collector_used += (region_size_bytes - ac);\n@@ -1002,7 +1414,0 @@\n-        mutator_regions++;\n-        mutator_used += (region_size_bytes - ac);\n-\n-        log_debug(gc)(\n-          \"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator partition\",\n-          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n-          byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n@@ -1012,0 +1417,18 @@\n+  log_debug(gc)(\"  At end of prep_to_rebuild, mutator_leftmost: \" SIZE_FORMAT\n+                \", mutator_rightmost: \" SIZE_FORMAT\n+                \", mutator_leftmost_empty: \" SIZE_FORMAT\n+                \", mutator_rightmost_empty: \" SIZE_FORMAT\n+                \", mutator_regions: \" SIZE_FORMAT\n+                \", mutator_used: \" SIZE_FORMAT,\n+                mutator_leftmost, mutator_rightmost, mutator_leftmost_empty, mutator_rightmost_empty,\n+                mutator_regions, mutator_used);\n+\n+  log_debug(gc)(\"  old_collector_leftmost: \" SIZE_FORMAT\n+                \", old_collector_rightmost: \" SIZE_FORMAT\n+                \", old_collector_leftmost_empty: \" SIZE_FORMAT\n+                \", old_collector_rightmost_empty: \" SIZE_FORMAT\n+                \", old_collector_regions: \" SIZE_FORMAT\n+                \", old_collector_used: \" SIZE_FORMAT,\n+                old_collector_leftmost, old_collector_rightmost, old_collector_leftmost_empty, old_collector_rightmost_empty,\n+                old_collector_regions, old_collector_used);\n+\n@@ -1014,0 +1437,8 @@\n+  _partitions.establish_old_collector_intervals(old_collector_leftmost, old_collector_rightmost, old_collector_leftmost_empty,\n+                                                old_collector_rightmost_empty, old_collector_regions, old_collector_used);\n+  log_debug(gc)(\"  After find_regions_with_alloc_capacity(), Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"],\"\n+                \"  Old Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+                _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n@@ -1016,1 +1447,5 @@\n-void ShenandoahFreeSet::move_regions_from_collector_to_mutator(size_t max_xfer_regions) {\n+\/\/ Returns number of regions transferred, adds transferred bytes to var argument bytes_transferred\n+size_t ShenandoahFreeSet::transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId which_collector,\n+                                                                                   size_t max_xfer_regions,\n+                                                                                   size_t& bytes_transferred) {\n+  shenandoah_assert_heaplocked();\n@@ -1018,2 +1453,38 @@\n-  size_t collector_empty_xfer = 0;\n-  size_t collector_not_empty_xfer = 0;\n+  size_t transferred_regions = 0;\n+  idx_t rightmost = _partitions.rightmost_empty(which_collector);\n+  for (idx_t idx = _partitions.leftmost_empty(which_collector); (transferred_regions < max_xfer_regions) && (idx <= rightmost); ) {\n+    assert(_partitions.in_free_set(which_collector, idx), \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, idx);\n+    \/\/ Note: can_allocate_from() denotes that region is entirely empty\n+    if (can_allocate_from(idx)) {\n+      _partitions.move_from_partition_to_partition(idx, which_collector, ShenandoahFreeSetPartitionId::Mutator, region_size_bytes);\n+      transferred_regions++;\n+      bytes_transferred += region_size_bytes;\n+    }\n+    idx = _partitions.find_index_of_next_available_region(which_collector, idx + 1);\n+  }\n+  return transferred_regions;\n+}\n+\n+\/\/ Returns number of regions transferred, adds transferred bytes to var argument bytes_transferred\n+size_t ShenandoahFreeSet::transfer_non_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId collector_id,\n+                                                                                       size_t max_xfer_regions,\n+                                                                                       size_t& bytes_transferred) {\n+  shenandoah_assert_heaplocked();\n+  size_t transferred_regions = 0;\n+  idx_t rightmost = _partitions.rightmost(collector_id);\n+  for (idx_t idx = _partitions.leftmost(collector_id); (transferred_regions < max_xfer_regions) && (idx <= rightmost); ) {\n+    assert(_partitions.in_free_set(collector_id, idx), \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, idx);\n+    size_t ac = alloc_capacity(idx);\n+    if (ac > 0) {\n+      _partitions.move_from_partition_to_partition(idx, collector_id, ShenandoahFreeSetPartitionId::Mutator, ac);\n+      transferred_regions++;\n+      bytes_transferred += ac;\n+    }\n+    idx = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Collector, idx + 1);\n+  }\n+  return transferred_regions;\n+}\n+\n+void ShenandoahFreeSet::move_regions_from_collector_to_mutator(size_t max_xfer_regions) {\n+  size_t collector_xfer = 0;\n+  size_t old_collector_xfer = 0;\n@@ -1026,13 +1497,16 @@\n-    idx_t rightmost = _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Collector);\n-    for (idx_t idx = _partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Collector);\n-         (max_xfer_regions > 0) && (idx <= rightmost); ) {\n-      assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Collector, idx),\n-             \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, idx);\n-      \/\/ Note: can_allocate_from() denotes that region is entirely empty\n-      if (can_allocate_from(idx)) {\n-        _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Collector,\n-                                                     ShenandoahFreeSetPartitionId::Mutator, region_size_bytes);\n-        max_xfer_regions--;\n-        collector_empty_xfer += region_size_bytes;\n-      }\n-      idx = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Collector, idx + 1);\n+    max_xfer_regions -=\n+      transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId::Collector, max_xfer_regions,\n+                                                               collector_xfer);\n+  }\n+\n+  \/\/ Process empty regions within the OldCollector free partition\n+  if ((max_xfer_regions > 0) &&\n+      (_partitions.leftmost_empty(ShenandoahFreeSetPartitionId::OldCollector)\n+       <= _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::OldCollector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    size_t old_collector_regions =\n+      transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId::OldCollector, max_xfer_regions,\n+                                                               old_collector_xfer);\n+    max_xfer_regions -= old_collector_regions;\n+    if (old_collector_regions > 0) {\n+      ShenandoahGenerationalHeap::cast(_heap)->generation_sizer()->transfer_to_young(old_collector_regions);\n@@ -1046,14 +1520,3 @@\n-    idx_t rightmost = _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector);\n-    for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector);\n-         (max_xfer_regions > 0) && (idx <= rightmost); ) {\n-      assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Collector, idx),\n-             \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, idx);\n-      size_t ac = alloc_capacity(idx);\n-      if (ac > 0) {\n-        _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Collector,\n-                                                     ShenandoahFreeSetPartitionId::Mutator, ac);\n-        max_xfer_regions--;\n-        collector_not_empty_xfer += ac;\n-      }\n-      idx = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Collector, idx + 1);\n-    }\n+    max_xfer_regions -=\n+      transfer_non_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId::Collector, max_xfer_regions,\n+                                                                   collector_xfer);\n@@ -1062,3 +1525,6 @@\n-  size_t collector_xfer = collector_empty_xfer + collector_not_empty_xfer;\n-  log_info(gc)(\"At start of update refs, moving \" SIZE_FORMAT \"%s to Mutator free partition from Collector Reserve\",\n-               byte_size_in_proper_unit(collector_xfer), proper_unit_for_byte_size(collector_xfer));\n+  size_t total_xfer = collector_xfer + old_collector_xfer;\n+  log_info(gc, free)(\"At start of update refs, moving \" SIZE_FORMAT \"%s to Mutator free set from Collector Reserve (\"\n+                     SIZE_FORMAT \"%s) and from Old Collector Reserve (\" SIZE_FORMAT \"%s)\",\n+                     byte_size_in_proper_unit(total_xfer), proper_unit_for_byte_size(total_xfer),\n+                     byte_size_in_proper_unit(collector_xfer), proper_unit_for_byte_size(collector_xfer),\n+                     byte_size_in_proper_unit(old_collector_xfer), proper_unit_for_byte_size(old_collector_xfer));\n@@ -1067,1 +1533,4 @@\n-void ShenandoahFreeSet::prepare_to_rebuild(size_t &cset_regions) {\n+\n+\/\/ Overwrite arguments to represent the amount of memory in each generation that is about to be recycled\n+void ShenandoahFreeSet::prepare_to_rebuild(size_t &young_cset_regions, size_t &old_cset_regions,\n+                                           size_t &first_old_region, size_t &last_old_region, size_t &old_region_count) {\n@@ -1069,0 +1538,3 @@\n+  \/\/ This resets all state information, removing all regions from all sets.\n+  clear();\n+  log_debug(gc, free)(\"Rebuilding FreeSet\");\n@@ -1070,1 +1542,4 @@\n-  log_debug(gc)(\"Rebuilding FreeSet\");\n+  \/\/ This places regions that have alloc_capacity into the old_collector set if they identify as is_old() or the\n+  \/\/ mutator set otherwise.  All trashed (cset) regions are affiliated young and placed in mutator set.\n+  find_regions_with_alloc_capacity(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+}\n@@ -1072,2 +1547,11 @@\n-  \/\/ This places regions that have alloc_capacity into the mutator partition.\n-  find_regions_with_alloc_capacity(cset_regions);\n+void ShenandoahFreeSet::establish_generation_sizes(size_t young_region_count, size_t old_region_count) {\n+  assert(young_region_count + old_region_count == ShenandoahHeap::heap()->num_regions(), \"Sanity\");\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+    ShenandoahOldGeneration* old_gen = heap->old_generation();\n+    ShenandoahYoungGeneration* young_gen = heap->young_generation();\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+    old_gen->set_capacity(old_region_count * region_size_bytes);\n+    young_gen->set_capacity(young_region_count * region_size_bytes);\n+  }\n@@ -1076,1 +1560,2 @@\n-void ShenandoahFreeSet::finish_rebuild(size_t cset_regions) {\n+void ShenandoahFreeSet::finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t old_region_count,\n+                                       bool have_evacuation_reserves) {\n@@ -1078,0 +1563,1 @@\n+  size_t young_reserve(0), old_reserve(0);\n@@ -1079,9 +1565,3 @@\n-  \/\/ Our desire is to reserve this much memory for future evacuation.  We may end up reserving less, if\n-  \/\/ memory is in short supply.\n-\n-  size_t reserve = _heap->max_capacity() * ShenandoahEvacReserve \/ 100;\n-  size_t available_in_collector_partition = (_partitions.capacity_of(ShenandoahFreeSetPartitionId::Collector)\n-                                             - _partitions.used_by(ShenandoahFreeSetPartitionId::Collector));\n-  size_t additional_reserve;\n-  if (available_in_collector_partition < reserve) {\n-    additional_reserve = reserve - available_in_collector_partition;\n+  if (_heap->mode()->is_generational()) {\n+    compute_young_and_old_reserves(young_cset_regions, old_cset_regions, have_evacuation_reserves,\n+                                   young_reserve, old_reserve);\n@@ -1089,1 +1569,2 @@\n-    additional_reserve = 0;\n+    young_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+    old_reserve = 0;\n@@ -1092,1 +1573,6 @@\n-  reserve_regions(reserve);\n+  \/\/ Move some of the mutator regions in the Collector and OldCollector partitions in order to satisfy\n+  \/\/ young_reserve and old_reserve.\n+  reserve_regions(young_reserve, old_reserve, old_region_count);\n+  size_t young_region_count = _heap->num_regions() - old_region_count;\n+  establish_generation_sizes(young_region_count, old_region_count);\n+  establish_old_collector_alloc_bias();\n@@ -1097,4 +1583,71 @@\n-void ShenandoahFreeSet::rebuild() {\n-  size_t cset_regions;\n-  prepare_to_rebuild(cset_regions);\n-  finish_rebuild(cset_regions);\n+void ShenandoahFreeSet::compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions,\n+                                                       bool have_evacuation_reserves,\n+                                                       size_t& young_reserve_result, size_t& old_reserve_result) const {\n+  shenandoah_assert_generational();\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+  ShenandoahOldGeneration* const old_generation = _heap->old_generation();\n+  size_t old_available = old_generation->available();\n+  size_t old_unaffiliated_regions = old_generation->free_unaffiliated_regions();\n+  ShenandoahYoungGeneration* const young_generation = _heap->young_generation();\n+  size_t young_capacity = young_generation->max_capacity();\n+  size_t young_unaffiliated_regions = young_generation->free_unaffiliated_regions();\n+\n+  \/\/ Add in the regions we anticipate to be freed by evacuation of the collection set\n+  old_unaffiliated_regions += old_cset_regions;\n+  young_unaffiliated_regions += young_cset_regions;\n+\n+  \/\/ Consult old-region balance to make adjustments to current generation capacities and availability.\n+  \/\/ The generation region transfers take place after we rebuild.\n+  const ssize_t old_region_balance = old_generation->get_region_balance();\n+  if (old_region_balance != 0) {\n+#ifdef ASSERT\n+    if (old_region_balance > 0) {\n+      assert(old_region_balance <= checked_cast<ssize_t>(old_unaffiliated_regions), \"Cannot transfer regions that are affiliated\");\n+    } else {\n+      assert(0 - old_region_balance <= checked_cast<ssize_t>(young_unaffiliated_regions), \"Cannot transfer regions that are affiliated\");\n+    }\n+#endif\n+\n+    ssize_t xfer_bytes = old_region_balance * checked_cast<ssize_t>(region_size_bytes);\n+    old_available -= xfer_bytes;\n+    old_unaffiliated_regions -= old_region_balance;\n+    young_capacity += xfer_bytes;\n+    young_unaffiliated_regions += old_region_balance;\n+  }\n+\n+  \/\/ All allocations taken from the old collector set are performed by GC, generally using PLABs for both\n+  \/\/ promotions and evacuations.  The partition between which old memory is reserved for evacuation and\n+  \/\/ which is reserved for promotion is enforced using thread-local variables that prescribe intentions for\n+  \/\/ each PLAB's available memory.\n+  if (have_evacuation_reserves) {\n+    \/\/ We are rebuilding at the end of final mark, having already established evacuation budgets for this GC pass.\n+    const size_t promoted_reserve = old_generation->get_promoted_reserve();\n+    const size_t old_evac_reserve = old_generation->get_evacuation_reserve();\n+    young_reserve_result = young_generation->get_evacuation_reserve();\n+    old_reserve_result = promoted_reserve + old_evac_reserve;\n+    assert(old_reserve_result <= old_available,\n+           \"Cannot reserve (\" SIZE_FORMAT \" + \" SIZE_FORMAT\") more OLD than is available: \" SIZE_FORMAT,\n+           promoted_reserve, old_evac_reserve, old_available);\n+  } else {\n+    \/\/ We are rebuilding at end of GC, so we set aside budgets specified on command line (or defaults)\n+    young_reserve_result = (young_capacity * ShenandoahEvacReserve) \/ 100;\n+    \/\/ The auto-sizer has already made old-gen large enough to hold all anticipated evacuations and promotions.\n+    \/\/ Affiliated old-gen regions are already in the OldCollector free set.  Add in the relevant number of\n+    \/\/ unaffiliated regions.\n+    old_reserve_result = old_available;\n+  }\n+\n+  \/\/ Old available regions that have less than PLAB::min_size() of available memory are not placed into the OldCollector\n+  \/\/ free set.  Because of this, old_available may not have enough memory to represent the intended reserve.  Adjust\n+  \/\/ the reserve downward to account for this possibility. This loss is part of the reason why the original budget\n+  \/\/ was adjusted with ShenandoahOldEvacWaste and ShenandoahOldPromoWaste multipliers.\n+  if (old_reserve_result >\n+      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n+    old_reserve_result =\n+      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes;\n+  }\n+\n+  if (young_reserve_result > young_unaffiliated_regions * region_size_bytes) {\n+    young_reserve_result = young_unaffiliated_regions * region_size_bytes;\n+  }\n@@ -1103,1 +1656,6 @@\n-void ShenandoahFreeSet::reserve_regions(size_t to_reserve) {\n+\/\/ Having placed all regions that have allocation capacity into the mutator set if they identify as is_young()\n+\/\/ or into the old collector set if they identify as is_old(), move some of these regions from the mutator set\n+\/\/ into the collector set or old collector set in order to assure that the memory available for allocations within\n+\/\/ the collector set is at least to_reserve and the memory available for allocations within the old collector set\n+\/\/ is at least to_reserve_old.\n+void ShenandoahFreeSet::reserve_regions(size_t to_reserve, size_t to_reserve_old, size_t &old_region_count) {\n@@ -1107,1 +1665,0 @@\n-\n@@ -1113,1 +1670,2 @@\n-    assert (ac > 0, \"Membership in free partition implies has capacity\");\n+    assert (ac > 0, \"Membership in free set implies has capacity\");\n+    assert (!r->is_old() || r->is_trash(), \"Except for trash, mutator_is_free regions should not be affiliated OLD\");\n@@ -1115,0 +1673,1 @@\n+    bool move_to_old_collector = _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector) < to_reserve_old;\n@@ -1116,2 +1675,3 @@\n-    if (!move_to_collector) {\n-      \/\/ We've satisfied to_reserve\n+\n+    if (!move_to_collector && !move_to_old_collector) {\n+      \/\/ We've satisfied both to_reserve and to_reserved_old\n@@ -1121,0 +1681,20 @@\n+    if (move_to_old_collector) {\n+      \/\/ We give priority to OldCollector partition because we desire to pack OldCollector regions into higher\n+      \/\/ addresses than Collector regions.  Presumably, OldCollector regions are more \"stable\" and less likely to\n+      \/\/ be collected in the near future.\n+      if (r->is_trash() || !r->is_affiliated()) {\n+        \/\/ OLD regions that have available memory are already in the old_collector free set.\n+        _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                                     ShenandoahFreeSetPartitionId::OldCollector, ac);\n+        log_debug(gc)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to old_collector_free\", idx);\n+        log_debug(gc)(\"  Shifted Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"],\"\n+                      \"  Old Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+                      _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                      _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                      _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                      _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+        old_region_count++;\n+        continue;\n+      }\n+    }\n+\n@@ -1133,0 +1713,6 @@\n+      log_debug(gc)(\"  Shifted Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"],\"\n+                    \"  Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+                    _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                    _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                    _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector),\n+                    _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector));\n@@ -1137,0 +1723,5 @@\n+    size_t old_reserve = _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector);\n+    if (old_reserve < to_reserve_old) {\n+      log_info(gc, free)(\"Wanted \" PROPERFMT \" for old reserve, but only reserved: \" PROPERFMT,\n+                         PROPERFMTARGS(to_reserve_old), PROPERFMTARGS(old_reserve));\n+    }\n@@ -1145,0 +1736,34 @@\n+void ShenandoahFreeSet::establish_old_collector_alloc_bias() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  shenandoah_assert_heaplocked();\n+\n+  idx_t left_idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+  idx_t right_idx = _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector);\n+  idx_t middle = (left_idx + right_idx) \/ 2;\n+  size_t available_in_first_half = 0;\n+  size_t available_in_second_half = 0;\n+\n+  for (idx_t index = left_idx; index < middle; index++) {\n+    if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, index)) {\n+      ShenandoahHeapRegion* r = heap->get_region((size_t) index);\n+      available_in_first_half += r->free();\n+    }\n+  }\n+  for (idx_t index = middle; index <= right_idx; index++) {\n+    if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, index)) {\n+      ShenandoahHeapRegion* r = heap->get_region(index);\n+      available_in_second_half += r->free();\n+    }\n+  }\n+\n+  \/\/ We desire to first consume the sparsely distributed regions in order that the remaining regions are densely packed.\n+  \/\/ Densely packing regions reduces the effort to search for a region that has sufficient memory to satisfy a new allocation\n+  \/\/ request.  Regions become sparsely distributed following a Full GC, which tends to slide all regions to the front of the\n+  \/\/ heap rather than allowing survivor regions to remain at the high end of the heap where we intend for them to congregate.\n+\n+  \/\/ TODO: In the future, we may modify Full GC so that it slides old objects to the end of the heap and young objects to the\n+  \/\/ front of the heap. If this is done, we can always search survivor Collector and OldCollector regions right to left.\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::OldCollector,\n+                                          (available_in_second_half > available_in_first_half));\n+}\n+\n@@ -1162,0 +1787,4 @@\n+    size_t retired_old = 0;\n+    size_t retired_old_humongous = 0;\n+    size_t retired_young = 0;\n+    size_t retired_young_humongous = 0;\n@@ -1163,0 +1792,2 @@\n+    size_t retired_young_waste = 0;\n+    size_t retired_old_waste = 0;\n@@ -1164,1 +1795,1 @@\n-    size_t available_collector = 0;\n+    size_t consumed_old_collector = 0;\n@@ -1166,0 +1797,2 @@\n+    size_t available_old = 0;\n+    size_t available_young = 0;\n@@ -1167,0 +1800,2 @@\n+    size_t available_collector = 0;\n+    size_t available_old_collector = 0;\n@@ -1172,3 +1807,7 @@\n-    log_debug(gc)(\"FreeSet map legend: M:mutator_free C:collector_free H:humongous _:retired\");\n-    log_debug(gc)(\" mutator free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"],\"\n-                  \" collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"]\",\n+\n+    log_debug(gc)(\"FreeSet map legend:\"\n+                       \" M:mutator_free C:collector_free O:old_collector_free\"\n+                       \" H:humongous ~:retired old _:retired young\");\n+    log_debug(gc)(\" mutator free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"] allocating from %s, \"\n+                  \" collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                  \"old collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"] allocates from %s\",\n@@ -1177,0 +1816,1 @@\n+                  _partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::Mutator)? \"left to right\": \"right to left\",\n@@ -1178,1 +1818,4 @@\n-                  _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector));\n+                  _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector),\n+                  _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                  _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                  _partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::OldCollector)? \"left to right\": \"right to left\");\n@@ -1188,0 +1831,1 @@\n+        assert(!r->is_old() || r->is_trash(), \"Old regions except trash regions should not be in mutator_free set\");\n@@ -1193,0 +1837,1 @@\n+        assert(!r->is_old() || r->is_trash(), \"Old regions except trash regions should not be in collector_free set\");\n@@ -1196,0 +1841,5 @@\n+      } else if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, i)) {\n+        size_t capacity = alloc_capacity(r);\n+        available_old_collector += capacity;\n+        consumed_old_collector += region_size_bytes - capacity;\n+        buffer[idx] = (capacity == region_size_bytes)? 'O': 'o';\n@@ -1197,1 +1847,7 @@\n-        buffer[idx] = 'h';\n+        if (r->is_old()) {\n+          buffer[idx] = 'H';\n+          retired_old_humongous += region_size_bytes;\n+        } else {\n+          buffer[idx] = 'h';\n+          retired_young_humongous += region_size_bytes;\n+        }\n@@ -1199,1 +1855,9 @@\n-        buffer[idx] = '_';\n+        if (r->is_old()) {\n+          buffer[idx] = '~';\n+          retired_old_waste += alloc_capacity(r);\n+          retired_old += region_size_bytes;\n+        } else {\n+          buffer[idx] = '_';\n+          retired_young_waste += alloc_capacity(r);\n+          retired_young += region_size_bytes;\n+        }\n@@ -1257,1 +1921,0 @@\n-\n@@ -1259,1 +1922,1 @@\n-               byte_size_in_proper_unit(free),          proper_unit_for_byte_size(free),\n+               byte_size_in_proper_unit(total_free),    proper_unit_for_byte_size(total_free),\n@@ -1306,0 +1969,21 @@\n+\n+    if (_heap->mode()->is_generational()) {\n+      size_t max = 0;\n+      size_t total_free = 0;\n+      size_t total_used = 0;\n+\n+      for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+           idx <= _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector); idx++) {\n+        if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, idx)) {\n+          ShenandoahHeapRegion *r = _heap->get_region(idx);\n+          size_t free = alloc_capacity(r);\n+          max = MAX2(max, free);\n+          total_free += free;\n+          total_used += r->used();\n+        }\n+      }\n+      ls.print_cr(\" Old Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n+                  byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),\n+                  byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max),\n+                  byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used));\n+    }\n@@ -1317,0 +2001,1 @@\n+      case ShenandoahAllocRequest::_alloc_plab:\n@@ -1349,0 +2034,9 @@\n+  if (_heap->mode()->is_generational()) {\n+    out->print_cr(\"Old Collector Free Set: \" SIZE_FORMAT \"\", _partitions.count(ShenandoahFreeSetPartitionId::OldCollector));\n+    for (idx_t index = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+         index <= _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector); index++) {\n+      if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, index)) {\n+        _heap->get_region(index)->print_on(out);\n+      }\n+    }\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":860,"deletions":166,"binary":false,"changes":1026,"status":"modified"},{"patch":"@@ -0,0 +1,1 @@\n+\n@@ -37,0 +38,2 @@\n+  OldCollector,                 \/\/ Region is in the Old Collector free set:\n+                                \/\/    available memory is reserved for old evacuations and for promotions..\n@@ -83,0 +86,4 @@\n+  \/\/ For each partition p, _left_to_right_bias is true iff allocations are normally made from lower indexed regions\n+  \/\/ before higher indexed regions.\n+  bool _left_to_right_bias[UIntNumPartitions];\n+\n@@ -91,0 +98,5 @@\n+  inline bool is_mutator_partition(ShenandoahFreeSetPartitionId p);\n+  inline bool is_young_collector_partition(ShenandoahFreeSetPartitionId p);\n+  inline bool is_old_collector_partition(ShenandoahFreeSetPartitionId p);\n+  inline bool available_implies_empty(size_t available);\n+\n@@ -115,0 +127,7 @@\n+  \/\/ Set the OldCollector intervals, usage, and capacity according to arguments.  We use this at the end of rebuild_free_set()\n+  \/\/ to avoid the overhead of making many redundant incremental adjustments to the mutator intervals as the free set is being\n+  \/\/ rebuilt.\n+  void establish_old_collector_intervals(ssize_t old_collector_leftmost, ssize_t old_collector_rightmost,\n+                                         ssize_t old_collector_leftmost_empty, ssize_t old_collector_rightmost_empty,\n+                                         size_t old_collector_region_count, size_t old_collector_used);\n+\n@@ -183,0 +202,10 @@\n+  inline void set_bias_from_left_to_right(ShenandoahFreeSetPartitionId which_partition, bool value) {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    _left_to_right_bias[int(which_partition)] = value;\n+  }\n+\n+  inline bool alloc_from_left_bias(ShenandoahFreeSetPartitionId which_partition) const {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    return _left_to_right_bias[int(which_partition)];\n+  }\n+\n@@ -240,1 +269,1 @@\n-\/\/ The ShenandoahFreeSet endeavors to congregrate survivor objects (objects that have been evacuated at least once) at the\n+\/\/ The ShenandoahFreeSet tries to colocate survivor objects (objects that have been evacuated at least once) at the\n@@ -261,0 +290,1 @@\n+<<<<<<< HEAD\n@@ -262,0 +292,5 @@\n+=======\n+  size_t _retired_old_regions;\n+>>>>>>> e30fa929b19e8b7d6db9073fdf640938d3db39fd\n+\n+  HeapWord* allocate_aligned_plab(size_t size, ShenandoahAllocRequest& req, ShenandoahHeapRegion* r);\n@@ -263,4 +298,5 @@\n-  \/\/ Mutator allocations are biased from left-to-right or from right-to-left based on which end of mutator range\n-  \/\/ is most likely to hold partially used regions.  In general, we want to finish consuming partially used\n-  \/\/ regions and retire them in order to reduce the regions that must be searched for each allocation request.\n-  bool _right_to_left_bias;\n+  \/\/ Return the address of memory allocated, setting in_new_region to true iff the allocation is taken\n+  \/\/ from a region that was previously empty.  Return nullptr if memory could not be allocated.\n+  inline HeapWord* allocate_from_partition_with_affiliation(ShenandoahFreeSetPartitionId which_partition,\n+                                                            ShenandoahAffiliation affiliation,\n+                                                            ShenandoahAllocRequest& req, bool& in_new_region);\n@@ -290,1 +326,3 @@\n-  \/\/ Change region r from the Mutator partition to the GC's Collector partition.  This requires that the region is entirely empty.\n+  \/\/ Change region r from the Mutator partition to the GC's Collector or OldCollector partition.  This requires that the\n+  \/\/ region is entirely empty.\n+  \/\/\n@@ -293,1 +331,1 @@\n-  \/\/ the Mutator free set into the Collector free set.\n+  \/\/ the Mutator free set into the Collector or OldCollector free set.\n@@ -295,0 +333,2 @@\n+  void flip_to_old_gc(ShenandoahHeapRegion* r);\n+\n@@ -306,5 +346,6 @@\n-  \/\/ This function places all regions that have allocation capacity into the mutator_partition, identifying regions\n-  \/\/ that have no allocation capacity as NotFree.  Subsequently, we will move some of the mutator regions into the\n-  \/\/ collector partition with the intent of packing collector memory into the highest (rightmost) addresses of the\n-  \/\/ heap, with mutator memory consuming the lowest addresses of the heap.\n-  void find_regions_with_alloc_capacity(size_t &cset_regions);\n+  size_t transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId which_collector,\n+                                                                  size_t max_xfer_regions,\n+                                                                  size_t& bytes_transferred);\n+  size_t transfer_non_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId collector_id,\n+                                                                      size_t max_xfer_regions,\n+                                                                      size_t& bytes_transferred);\n@@ -312,4 +353,0 @@\n-  \/\/ Having placed all regions that have allocation capacity into the mutator partition, move some of these regions from\n-  \/\/ the mutator partition into the collector partition in order to assure that the memory available for allocations within\n-  \/\/ the collector partition is at least to_reserve.\n-  void reserve_regions(size_t to_reserve);\n@@ -317,2 +354,2 @@\n-  \/\/ Overwrite arguments to represent the number of regions to be reclaimed from the cset\n-  void prepare_to_rebuild(size_t &cset_regions);\n+  \/\/ Determine whether we prefer to allocate from left to right or from right to left within the OldCollector free-set.\n+  void establish_old_collector_alloc_bias();\n@@ -320,1 +357,3 @@\n-  void finish_rebuild(size_t cset_regions);\n+  \/\/ Set max_capacity for young and old generations\n+  void establish_generation_sizes(size_t young_region_count, size_t old_region_count);\n+  size_t get_usable_free_words(size_t free_bytes) const;\n@@ -333,1 +372,37 @@\n-  void rebuild();\n+\n+  \/\/ Examine the existing free set representation, capturing the current state into var arguments:\n+  \/\/\n+  \/\/ young_cset_regions is the number of regions currently in the young cset if we are starting to evacuate, or zero\n+  \/\/   old_cset_regions is the number of regions currently in the old cset if we are starting a mixed evacuation, or zero\n+  \/\/   first_old_region is the index of the first region that is part of the OldCollector set\n+  \/\/    last_old_region is the index of the last region that is part of the OldCollector set\n+  \/\/   old_region_count is the number of regions in the OldCollector set that have memory available to be allocated\n+  void prepare_to_rebuild(size_t &young_cset_regions, size_t &old_cset_regions,\n+                          size_t &first_old_region, size_t &last_old_region, size_t &old_region_count);\n+\n+  \/\/ At the end of final mark, but before we begin evacuating, heuristics calculate how much memory is required to\n+  \/\/ hold the results of evacuating to young-gen and to old-gen, and have_evacuation_reserves should be true.\n+  \/\/ These quantities, stored as reserves for their respective generations, are consulted prior to rebuilding\n+  \/\/ the free set (ShenandoahFreeSet) in preparation for evacuation.  When the free set is rebuilt, we make sure\n+  \/\/ to reserve sufficient memory in the collector and old_collector sets to hold evacuations.\n+  \/\/\n+  \/\/ We also rebuild the free set at the end of GC, as we prepare to idle GC until the next trigger.  In this case,\n+  \/\/ have_evacuation_reserves is false because we don't yet know how much memory will need to be evacuated in the\n+  \/\/ next GC cycle.  When have_evacuation_reserves is false, the free set rebuild operation reserves for the collector\n+  \/\/ and old_collector sets based on alternative mechanisms, such as ShenandoahEvacReserve, ShenandoahOldEvacReserve, and\n+  \/\/ ShenandoahOldCompactionReserve.  In a future planned enhancement, the reserve for old_collector set when the\n+  \/\/ evacuation reserves are unknown, is based in part on anticipated promotion as determined by analysis of live data\n+  \/\/ found during the previous GC pass which is one less than the current tenure age.\n+  \/\/\n+  \/\/ young_cset_regions is the number of regions currently in the young cset if we are starting to evacuate, or zero\n+  \/\/   old_cset_regions is the number of regions currently in the old cset if we are starting a mixed evacuation, or zero\n+  \/\/    num_old_regions is the number of old-gen regions that have available memory for further allocations (excluding old cset)\n+  \/\/ have_evacuation_reserves is true iff the desired values of young-gen and old-gen evacuation reserves and old-gen\n+  \/\/                    promotion reserve have been precomputed (and can be obtained by invoking\n+  \/\/                    <generation>->get_evacuation_reserve() or old_gen->get_promoted_reserve()\n+  void finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t num_old_regions,\n+                      bool have_evacuation_reserves = false);\n+\n+  \/\/ When a region is promoted in place, we add the region's available memory if it is greater than plab_min_size()\n+  \/\/ into the old collector partition by invoking this method.\n+  void add_promoted_in_place_region_to_old_collector(ShenandoahHeapRegion* region);\n@@ -339,5 +414,0 @@\n-  \/\/\n-  \/\/ Note that we plan to replenish the Collector reserve at the end of update refs, at which time all\n-  \/\/ of the regions recycled from the collection set will be available.  If the very unlikely event that there\n-  \/\/ are fewer regions in the collection set than remain in the collector set, we limit the transfer in order\n-  \/\/ to assure that the replenished Collector reserve can be sufficiently large.\n@@ -347,0 +417,1 @@\n+\n@@ -399,0 +470,22 @@\n+\n+  \/\/ This function places all regions that have allocation capacity into the mutator partition, or if the region\n+  \/\/ is already affiliated with old, into the old collector partition, identifying regions that have no allocation\n+  \/\/ capacity as NotFree.  Capture the modified state of the freeset into var arguments:\n+  \/\/\n+  \/\/ young_cset_regions is the number of regions currently in the young cset if we are starting to evacuate, or zero\n+  \/\/   old_cset_regions is the number of regions currently in the old cset if we are starting a mixed evacuation, or zero\n+  \/\/   first_old_region is the index of the first region that is part of the OldCollector set\n+  \/\/    last_old_region is the index of the last region that is part of the OldCollector set\n+  \/\/   old_region_count is the number of regions in the OldCollector set that have memory available to be allocated\n+  void find_regions_with_alloc_capacity(size_t &young_cset_regions, size_t &old_cset_regions,\n+                                        size_t &first_old_region, size_t &last_old_region, size_t &old_region_count);\n+\n+  \/\/ Ensure that Collector has at least to_reserve bytes of available memory, and OldCollector has at least old_reserve\n+  \/\/ bytes of available memory.  On input, old_region_count holds the number of regions already present in the\n+  \/\/ OldCollector partition.  Upon return, old_region_count holds the updated number of regions in the OldCollector partition.\n+  void reserve_regions(size_t to_reserve, size_t old_reserve, size_t &old_region_count);\n+\n+  \/\/ Reserve space for evacuations, with regions reserved for old evacuations placed to the right\n+  \/\/ of regions reserved of young evacuations.\n+  void compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions, bool have_evacuation_reserves,\n+                                      size_t &young_reserve_result, size_t &old_reserve_result) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":118,"deletions":25,"binary":false,"changes":143,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -39,0 +40,3 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n@@ -47,0 +51,3 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalEvacuationTask.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n@@ -54,1 +61,0 @@\n-#include \"gc\/shenandoah\/shenandoahMetrics.hpp\"\n@@ -56,0 +62,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -62,0 +69,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -69,0 +77,2 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -72,0 +82,2 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n@@ -164,3 +176,0 @@\n-  \/\/ Now we know the number of regions, initialize the heuristics.\n-  initialize_heuristics();\n-\n@@ -220,0 +229,22 @@\n+  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this, _heap_region));\n+\n+  \/\/ Now we know the number of regions and heap sizes, initialize the heuristics.\n+  initialize_heuristics();\n+\n+  assert(_heap_region.byte_size() == heap_rs.size(), \"Need to know reserved size for card table\");\n+\n+  \/\/\n+  \/\/ Worker threads must be initialized after the barrier is configured\n+  \/\/\n+  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n+  if (_workers == nullptr) {\n+    vm_exit_during_initialization(\"Failed necessary allocation.\");\n+  } else {\n+    _workers->initialize_workers();\n+  }\n+\n+  if (ParallelGCThreads > 1) {\n+    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\", ParallelGCThreads);\n+    _safepoint_workers->initialize_workers();\n+  }\n+\n@@ -260,1 +291,1 @@\n-                              align_up(num_committed_regions, _bitmap_regions_per_slice) \/ _bitmap_regions_per_slice;\n+    align_up(num_committed_regions, _bitmap_regions_per_slice) \/ _bitmap_regions_per_slice;\n@@ -267,1 +298,1 @@\n-  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions, _max_workers);\n+  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions);\n@@ -358,0 +389,1 @@\n+  _affiliations = NEW_C_HEAP_ARRAY(uint8_t, _num_regions, mtGC);\n@@ -363,0 +395,1 @@\n+\n@@ -374,0 +407,2 @@\n+\n+      _affiliations[i] = ShenandoahAffiliation::FREE;\n@@ -378,0 +413,1 @@\n+    size_t young_cset_regions, old_cset_regions;\n@@ -379,1 +415,4 @@\n-    _free_set->rebuild();\n+    \/\/ We are initializing free set.  We ignore cset region tallies.\n+    size_t first_old, last_old, num_old;\n+    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+    _free_set->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n@@ -438,1 +477,1 @@\n-  _control_thread = new ShenandoahControlThread();\n+  initialize_controller();\n@@ -440,1 +479,1 @@\n-  ShenandoahInitLogger::print();\n+  print_init_logger();\n@@ -445,0 +484,8 @@\n+void ShenandoahHeap::initialize_controller() {\n+  _control_thread = new ShenandoahControlThread();\n+}\n+\n+void ShenandoahHeap::print_init_logger() const {\n+  ShenandoahInitLogger::print();\n+}\n+\n@@ -453,0 +500,2 @@\n+    } else if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+      _gc_mode = new ShenandoahGenerationalMode();\n@@ -473,13 +522,3 @@\n-  assert(_gc_mode != nullptr, \"Must be initialized\");\n-  _heuristics = _gc_mode->initialize_heuristics();\n-\n-  if (_heuristics->is_diagnostic() && !UnlockDiagnosticVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.\",\n-                    _heuristics->name()));\n-  }\n-  if (_heuristics->is_experimental() && !UnlockExperimentalVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.\",\n-                    _heuristics->name()));\n-  }\n+  _global_generation = new ShenandoahGlobalGeneration(mode()->is_generational(), max_workers(), max_capacity(), max_capacity());\n+  _global_generation->initialize_heuristics(mode());\n+  _evac_tracker = new ShenandoahEvacuationTracker(mode()->is_generational());\n@@ -495,0 +534,2 @@\n+  _gc_generation(nullptr),\n+  _active_generation(nullptr),\n@@ -496,1 +537,0 @@\n-  _used(0),\n@@ -498,2 +538,1 @@\n-  _bytes_allocated_since_gc_start(0),\n-  _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),\n+  _max_workers(MAX3(ConcGCThreads, ParallelGCThreads, 1U)),\n@@ -505,1 +544,1 @@\n-  _update_refs_iterator(this),\n+  _affiliations(nullptr),\n@@ -508,0 +547,3 @@\n+  _cancel_requested_time(0),\n+  _update_refs_iterator(this),\n+  _global_generation(nullptr),\n@@ -509,0 +551,2 @@\n+  _young_generation(nullptr),\n+  _old_generation(nullptr),\n@@ -511,1 +555,0 @@\n-  _heuristics(nullptr),\n@@ -516,0 +559,2 @@\n+  _evac_tracker(nullptr),\n+  _mmu_tracker(),\n@@ -522,1 +567,0 @@\n-  _ref_processor(new ShenandoahReferenceProcessor(MAX2(_max_workers, 1U))),\n@@ -532,1 +576,1 @@\n-  \/\/ Initialize GC mode early, so we can adjust barrier support\n+  \/\/ Initialize GC mode early, many subsequent initialization procedures depend on it\n@@ -534,15 +578,0 @@\n-  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));\n-\n-  _max_workers = MAX2(_max_workers, 1U);\n-  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n-  if (_workers == nullptr) {\n-    vm_exit_during_initialization(\"Failed necessary allocation.\");\n-  } else {\n-    _workers->initialize_workers();\n-  }\n-\n-  if (ParallelGCThreads > 1) {\n-    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\",\n-                                                ParallelGCThreads);\n-    _safepoint_workers->initialize_workers();\n-  }\n@@ -555,29 +584,0 @@\n-class ShenandoahResetBitmapTask : public WorkerTask {\n-private:\n-  ShenandoahRegionIterator _regions;\n-\n-public:\n-  ShenandoahResetBitmapTask() :\n-    WorkerTask(\"Shenandoah Reset Bitmap\") {}\n-\n-  void work(uint worker_id) {\n-    ShenandoahHeapRegion* region = _regions.next();\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahMarkingContext* const ctx = heap->marking_context();\n-    while (region != nullptr) {\n-      if (heap->is_bitmap_slice_committed(region)) {\n-        ctx->clear_bitmap(region);\n-      }\n-      region = _regions.next();\n-    }\n-  }\n-};\n-\n-void ShenandoahHeap::reset_mark_bitmap() {\n-  assert_gc_workers(_workers->active_workers());\n-  mark_incomplete_marking_context();\n-\n-  ShenandoahResetBitmapTask task;\n-  _workers->run_task(&task);\n-}\n-\n@@ -598,1 +598,2 @@\n-  if (is_concurrent_mark_in_progress())        st->print(\"marking, \");\n+  if (is_concurrent_old_mark_in_progress())    st->print(\"old marking, \");\n+  if (is_concurrent_young_mark_in_progress())  st->print(\"young marking, \");\n@@ -649,0 +650,2 @@\n+  _mmu_tracker.initialize();\n+\n@@ -662,2 +665,0 @@\n-  _heuristics->initialize();\n-\n@@ -667,0 +668,4 @@\n+ShenandoahHeuristics* ShenandoahHeap::heuristics() {\n+  return _global_generation->heuristics();\n+}\n+\n@@ -668,1 +673,1 @@\n-  return Atomic::load(&_used);\n+  return global_generation()->used();\n@@ -675,4 +680,0 @@\n-size_t ShenandoahHeap::available() const {\n-  return free_set()->available();\n-}\n-\n@@ -689,2 +690,43 @@\n-void ShenandoahHeap::increase_used(size_t bytes) {\n-  Atomic::add(&_used, bytes, memory_order_relaxed);\n+\/\/ For tracking usage based on allocations, it should be the case that:\n+\/\/ * The sum of regions::used == heap::used\n+\/\/ * The sum of a generation's regions::used == generation::used\n+\/\/ * The sum of a generation's humongous regions::free == generation::humongous_waste\n+\/\/ These invariants are checked by the verifier on GC safepoints.\n+\/\/\n+\/\/ Additional notes:\n+\/\/ * When a mutator's allocation request causes a region to be retired, the\n+\/\/   free memory left in that region is considered waste. It does not contribute\n+\/\/   to the usage, but it _does_ contribute to allocation rate.\n+\/\/ * The bottom of a PLAB must be aligned on card size. In some cases this will\n+\/\/   require padding in front of the PLAB (a filler object). Because this padding\n+\/\/   is included in the region's used memory we include the padding in the usage\n+\/\/   accounting as waste.\n+\/\/ * Mutator allocations are used to compute an allocation rate. They are also\n+\/\/   sent to the Pacer for those purposes.\n+\/\/ * There are three sources of waste:\n+\/\/  1. The padding used to align a PLAB on card size\n+\/\/  2. Region's free is less than minimum TLAB size and is retired\n+\/\/  3. The unused portion of memory in the last region of a humongous object\n+void ShenandoahHeap::increase_used(const ShenandoahAllocRequest& req) {\n+  size_t actual_bytes = req.actual_size() * HeapWordSize;\n+  size_t wasted_bytes = req.waste() * HeapWordSize;\n+  ShenandoahGeneration* generation = generation_for(req.affiliation());\n+\n+  if (req.is_gc_alloc()) {\n+    assert(wasted_bytes == 0 || req.type() == ShenandoahAllocRequest::_alloc_plab, \"Only PLABs have waste\");\n+    increase_used(generation, actual_bytes + wasted_bytes);\n+  } else {\n+    assert(req.is_mutator_alloc(), \"Expected mutator alloc here\");\n+    \/\/ padding and actual size both count towards allocation counter\n+    generation->increase_allocated(actual_bytes + wasted_bytes);\n+\n+    \/\/ only actual size counts toward usage for mutator allocations\n+    increase_used(generation, actual_bytes);\n+\n+    \/\/ notify pacer of both actual size and waste\n+    notify_mutator_alloc_words(req.actual_size(), req.waste());\n+\n+    if (wasted_bytes > 0 && req.actual_size() > ShenandoahHeapRegion::humongous_threshold_words()) {\n+      increase_humongous_waste(generation,wasted_bytes);\n+    }\n+  }\n@@ -693,2 +735,5 @@\n-void ShenandoahHeap::set_used(size_t bytes) {\n-  Atomic::store(&_used, bytes);\n+void ShenandoahHeap::increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_humongous_waste(bytes);\n+  }\n@@ -697,3 +742,5 @@\n-void ShenandoahHeap::decrease_used(size_t bytes) {\n-  assert(used() >= bytes, \"never decrease heap size by more than we've left\");\n-  Atomic::sub(&_used, bytes, memory_order_relaxed);\n+void ShenandoahHeap::decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_humongous_waste(bytes);\n+  }\n@@ -702,2 +749,5 @@\n-void ShenandoahHeap::increase_allocated(size_t bytes) {\n-  Atomic::add(&_bytes_allocated_since_gc_start, bytes, memory_order_relaxed);\n+void ShenandoahHeap::increase_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_used(bytes);\n+  }\n@@ -706,4 +756,4 @@\n-void ShenandoahHeap::notify_mutator_alloc_words(size_t words, bool waste) {\n-  size_t bytes = words * HeapWordSize;\n-  if (!waste) {\n-    increase_used(bytes);\n+void ShenandoahHeap::decrease_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_used(bytes);\n@@ -711,1 +761,3 @@\n-  increase_allocated(bytes);\n+}\n+\n+void ShenandoahHeap::notify_mutator_alloc_words(size_t words, size_t waste) {\n@@ -714,2 +766,2 @@\n-    if (waste) {\n-      pacer()->claim_for_alloc(words, true);\n+    if (waste > 0) {\n+      pacer()->claim_for_alloc(waste, true);\n@@ -751,6 +803,0 @@\n-bool ShenandoahHeap::is_in(const void* p) const {\n-  HeapWord* heap_base = (HeapWord*) base();\n-  HeapWord* last_region_end = heap_base + ShenandoahHeapRegion::region_size_words() * num_regions();\n-  return p >= heap_base && p < last_region_end;\n-}\n-\n@@ -836,3 +882,1 @@\n-\n-  \/\/ This is called from allocation path, and thus should be fast.\n-  _heap_changed.try_set();\n+  _heap_changed.set();\n@@ -855,0 +899,1 @@\n+\n@@ -861,0 +906,1 @@\n+  log_debug(gc, free)(\"Set new GCLAB size: \" SIZE_FORMAT, new_size);\n@@ -866,0 +912,1 @@\n+    log_debug(gc, free)(\"New gclab size (\" SIZE_FORMAT \") is too small for \" SIZE_FORMAT, new_size, size);\n@@ -895,0 +942,1 @@\n+\/\/ Called from stubs in JIT code or interpreter\n@@ -943,0 +991,1 @@\n+    \/\/ gc_no_progress_count is incremented following each degen or full GC that fails to achieve is_good_progress().\n@@ -945,0 +994,1 @@\n+      req.set_actual_size(0);\n@@ -990,0 +1040,8 @@\n+  if (result == nullptr) {\n+    req.set_actual_size(0);\n+  }\n+\n+  \/\/ This is called regardless of the outcome of the allocation to account\n+  \/\/ for any waste created by retiring regions with this request.\n+  increase_used(req);\n+\n@@ -999,2 +1057,0 @@\n-      notify_mutator_alloc_words(actual, false);\n-\n@@ -1007,2 +1063,0 @@\n-    } else {\n-      increase_used(actual*HeapWordSize);\n@@ -1021,1 +1075,37 @@\n-  return _free_set->allocate(req, in_new_region);\n+\n+  \/\/ Make sure the old generation has room for either evacuations or promotions before trying to allocate.\n+  if (req.is_old() && !old_generation()->can_allocate(req)) {\n+    return nullptr;\n+  }\n+\n+  \/\/ If TLAB request size is greater than available, allocate() will attempt to downsize request to fit within available\n+  \/\/ memory.\n+  HeapWord* result = _free_set->allocate(req, in_new_region);\n+\n+  \/\/ Record the plab configuration for this result and register the object.\n+  if (result != nullptr && req.is_old()) {\n+    old_generation()->configure_plab_for_current_thread(req);\n+    if (req.type() == ShenandoahAllocRequest::_alloc_shared_gc) {\n+      \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+      \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+      \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+      \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+      \/\/\n+      \/\/ objects being \"concurrently\" allocated:\n+      \/\/    [-----a------][-----b-----][--------------c------------------]\n+      \/\/            [---- card table memory range --------------]\n+      \/\/\n+      \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n+      \/\/ wants to set the starts-object, first-start, and last-start attributes of the preceding card region.\n+      \/\/ Allocation of object b wants to set the starts-object, first-start, and last-start attributes of this card region.\n+      \/\/ Allocation of object c also wants to set the starts-object, first-start, and last-start attributes of this\n+      \/\/ card region.\n+      \/\/\n+      \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n+      \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n+      \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+      old_generation()->card_scan()->register_object(result);\n+    }\n+  }\n+\n+  return result;\n@@ -1036,2 +1126,2 @@\n-  if (heuristics()->can_unload_classes()) {\n-    ShenandoahHeuristics* h = heuristics();\n+  ShenandoahHeuristics* h = global_generation()->heuristics();\n+  if (h->can_unload_classes()) {\n@@ -1130,2 +1220,8 @@\n-  ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n-  workers()->run_task(&task);\n+  if (mode()->is_generational()) {\n+    ShenandoahRegionIterator regions;\n+    ShenandoahGenerationalEvacuationTask task(ShenandoahGenerationalHeap::heap(), &regions, concurrent);\n+    workers()->run_task(&task);\n+  } else {\n+    ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n+    workers()->run_task(&task);\n+  }\n@@ -1135,3 +1231,4 @@\n-  if (ShenandoahThreadLocalData::is_oom_during_evac(Thread::current())) {\n-    \/\/ This thread went through the OOM during evac protocol and it is safe to return\n-    \/\/ the forward pointer. It must not attempt to evacuate any more.\n+  assert(thread == Thread::current(), \"Expected thread parameter to be current thread.\");\n+  if (ShenandoahThreadLocalData::is_oom_during_evac(thread)) {\n+    \/\/ This thread went through the OOM during evac protocol. It is safe to return\n+    \/\/ the forward pointer. It must not attempt to evacuate any other objects.\n@@ -1143,1 +1240,2 @@\n-  size_t size = p->size();\n+  ShenandoahHeapRegion* r = heap_region_containing(p);\n+  assert(!r->is_humongous(), \"never evacuate humongous objects\");\n@@ -1145,1 +1243,3 @@\n-  assert(!heap_region_containing(p)->is_humongous(), \"never evacuate humongous objects\");\n+  ShenandoahAffiliation target_gen = r->affiliation();\n+  return try_evacuate_object(p, thread, r, target_gen);\n+}\n@@ -1147,1 +1247,5 @@\n-  bool alloc_from_gclab = true;\n+oop ShenandoahHeap::try_evacuate_object(oop p, Thread* thread, ShenandoahHeapRegion* from_region,\n+                                               ShenandoahAffiliation target_gen) {\n+  assert(target_gen == YOUNG_GENERATION, \"Only expect evacuations to young in this mode\");\n+  assert(from_region->is_young(), \"Only expect evacuations from young in this mode\");\n+  bool alloc_from_lab = true;\n@@ -1149,0 +1253,1 @@\n+  size_t size = p->size();\n@@ -1158,0 +1263,8 @@\n+      if ((copy == nullptr) && (size < ShenandoahThreadLocalData::gclab_size(thread))) {\n+        \/\/ GCLAB allocation failed because we are bumping up against the limit on young evacuation reserve.  Try resetting\n+        \/\/ the desired GCLAB size and retry GCLAB allocation to avoid cascading of shared memory allocations.\n+        \/\/ TODO: is this right? using PLAB::min_size() here for gc lab size?\n+        ShenandoahThreadLocalData::set_gclab_size(thread, PLAB::min_size());\n+        copy = allocate_from_gclab(thread, size);\n+        \/\/ If we still get nullptr, we'll try a shared allocation below.\n+      }\n@@ -1159,0 +1272,1 @@\n+\n@@ -1160,1 +1274,2 @@\n-      ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size);\n+      \/\/ If we failed to allocate in LAB, we'll try a shared allocation.\n+      ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen);\n@@ -1162,1 +1277,1 @@\n-      alloc_from_gclab = false;\n+      alloc_from_lab = false;\n@@ -1177,0 +1292,1 @@\n+  _evac_tracker->begin_evacuation(thread, size * HeapWordSize);\n@@ -1179,1 +1295,0 @@\n-  \/\/ Try to install the new forwarding pointer.\n@@ -1181,0 +1296,1 @@\n+<<<<<<< HEAD\n@@ -1185,0 +1301,10 @@\n+=======\n+\n+  \/\/ Try to install the new forwarding pointer.\n+  ContinuationGCSupport::relativize_stack_chunk(copy_val);\n+\n+  oop result = ShenandoahForwarding::try_update_forwardee(p, copy_val);\n+  if (result == copy_val) {\n+    \/\/ Successfully evacuated. Our copy is now the public one!\n+    _evac_tracker->end_evacuation(thread, size * HeapWordSize);\n+>>>>>>> e30fa929b19e8b7d6db9073fdf640938d3db39fd\n@@ -1193,7 +1319,4 @@\n-    \/\/\n-    \/\/ For GCLAB allocations, it is enough to rollback the allocation ptr. Either the next\n-    \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n-    \/\/ do this. For non-GCLAB allocations, we have no way to retract the allocation, and\n-    \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n-    \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n-    if (alloc_from_gclab) {\n+    if (alloc_from_lab) {\n+      \/\/ For LAB allocations, it is enough to rollback the allocation ptr. Either the next\n+      \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n+      \/\/ do this.\n@@ -1202,0 +1325,4 @@\n+      \/\/ For non-LAB allocations, we have no way to retract the allocation, and\n+      \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n+      \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n+      assert(size >= ShenandoahHeap::min_fill_size(), \"previously allocated object known to be larger than min_size\");\n@@ -1204,0 +1331,1 @@\n+      \/\/ For non-LAB allocations, the object has already been registered\n@@ -1237,1 +1365,1 @@\n-void ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n+size_t ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n@@ -1257,0 +1385,1 @@\n+  return required_regions;\n@@ -1266,0 +1395,6 @@\n+\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+      assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+      assert(plab->words_remaining() == 0, \"PLAB should not need retirement\");\n+    }\n@@ -1281,0 +1416,13 @@\n+\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+      assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+\n+      \/\/ There are two reasons to retire all plabs between old-gen evacuation passes.\n+      \/\/  1. We need to make the plab memory parsable by remembered-set scanning.\n+      \/\/  2. We need to establish a trustworthy UpdateWaterMark value within each old-gen heap region\n+      ShenandoahGenerationalHeap::heap()->retire_plab(plab, thread);\n+      if (_resize && ShenandoahThreadLocalData::plab_size(thread) > 0) {\n+        ShenandoahThreadLocalData::set_plab_size(thread, 0);\n+      }\n+    }\n@@ -1409,0 +1557,4 @@\n+    ls.cr();\n+\n+    evac_tracker()->print_global_on(&ls);\n+\n@@ -1414,0 +1566,40 @@\n+void ShenandoahHeap::set_gc_generation(ShenandoahGeneration* generation) {\n+  shenandoah_assert_control_or_vm_thread_at_safepoint();\n+  _gc_generation = generation;\n+}\n+\n+\/\/ Active generation may only be set by the VM thread at a safepoint.\n+void ShenandoahHeap::set_active_generation() {\n+  assert(Thread::current()->is_VM_thread(), \"Only the VM Thread\");\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Only at a safepoint!\");\n+  assert(_gc_generation != nullptr, \"Will set _active_generation to nullptr\");\n+  _active_generation = _gc_generation;\n+}\n+\n+void ShenandoahHeap::on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation) {\n+  shenandoah_policy()->record_collection_cause(cause);\n+\n+  assert(gc_cause()  == GCCause::_no_gc, \"Over-writing cause\");\n+  assert(_gc_generation == nullptr, \"Over-writing _gc_generation\");\n+\n+  set_gc_cause(cause);\n+  set_gc_generation(generation);\n+\n+  generation->heuristics()->record_cycle_start();\n+}\n+\n+void ShenandoahHeap::on_cycle_end(ShenandoahGeneration* generation) {\n+  assert(gc_cause() != GCCause::_no_gc, \"cause wasn't set\");\n+  assert(_gc_generation != nullptr, \"_gc_generation wasn't set\");\n+\n+  generation->heuristics()->record_cycle_end();\n+  if (mode()->is_generational() && generation->is_global()) {\n+    \/\/ If we just completed a GLOBAL GC, claim credit for completion of young-gen and old-gen GC as well\n+    young_generation()->heuristics()->record_cycle_end();\n+    old_generation()->heuristics()->record_cycle_end();\n+  }\n+\n+  set_gc_generation(nullptr);\n+  set_gc_cause(GCCause::_no_gc);\n+}\n+\n@@ -1750,99 +1942,0 @@\n-class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n-      \/\/ anyway to capture any updates that happened since now.\n-      r->clear_live_data();\n-      _ctx->capture_top_at_mark_start(r);\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_gc() {\n-  reset_mark_bitmap();\n-\n-  ShenandoahResetUpdateRegionStateClosure cl;\n-  parallel_heap_region_iterate(&cl);\n-}\n-\n-class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalMarkUpdateRegionStateClosure() :\n-    _ctx(ShenandoahHeap::heap()->complete_marking_context()), _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n-      \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n-      HeapWord *tams = _ctx->top_at_mark_start(r);\n-      HeapWord *top = r->top();\n-      if (top > tams) {\n-        r->increase_live_data_alloc_words(pointer_delta(top, tams));\n-      }\n-\n-      \/\/ We are about to select the collection set, make sure it knows about\n-      \/\/ current pinning status. Also, this allows trashing more regions that\n-      \/\/ now have their pinning status dropped.\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n-      }\n-\n-      \/\/ Remember limit for updating refs. It's guaranteed that we get no\n-      \/\/ from-space-refs written from here on.\n-      r->set_update_watermark_at_safepoint(r->top());\n-    } else {\n-      assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_regions_and_collection_set(bool concurrent) {\n-  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states :\n-                                         ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n-    ShenandoahFinalMarkUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n-\n-    assert_pinned_region_status();\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset :\n-                                         ShenandoahPhaseTimings::degen_gc_choose_cset);\n-    ShenandoahHeapLocker locker(lock());\n-    _collection_set->clear();\n-    heuristics()->choose_collection_set(_collection_set);\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset :\n-                                         ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n-  }\n-}\n-\n@@ -1851,0 +1944,3 @@\n+  if (mode()->is_generational()) {\n+    old_generation()->set_parseable(false);\n+  }\n@@ -1859,1 +1955,2 @@\n-  ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n+  shenandoah_assert_generations_reconciled();\n+  gc_generation()->ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n@@ -1893,0 +1990,41 @@\n+  \/\/ Check that if concurrent weak root is set then active_gen isn't null\n+  assert(!is_concurrent_weak_root_in_progress() || active_generation() != nullptr, \"Error\");\n+  shenandoah_assert_generations_reconciled();\n+}\n+\n+void ShenandoahHeap::set_concurrent_young_mark_in_progress(bool in_progress) {\n+  uint mask;\n+  assert(!has_forwarded_objects(), \"Young marking is not concurrent with evacuation\");\n+  if (!in_progress && is_concurrent_old_mark_in_progress()) {\n+    assert(mode()->is_generational(), \"Only generational GC has old marking\");\n+    assert(_gc_state.is_set(MARKING), \"concurrent_old_marking_in_progress implies MARKING\");\n+    \/\/ If old-marking is in progress when we turn off YOUNG_MARKING, leave MARKING (and OLD_MARKING) on\n+    mask = YOUNG_MARKING;\n+  } else {\n+    mask = MARKING | YOUNG_MARKING;\n+  }\n+  set_gc_state(mask, in_progress);\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::set_concurrent_old_mark_in_progress(bool in_progress) {\n+#ifdef ASSERT\n+  \/\/ has_forwarded_objects() iff UPDATEREFS or EVACUATION\n+  bool has_forwarded = has_forwarded_objects();\n+  bool updating_or_evacuating = _gc_state.is_set(UPDATEREFS | EVACUATION);\n+  bool evacuating = _gc_state.is_set(EVACUATION);\n+  assert ((has_forwarded == updating_or_evacuating) || (evacuating && !has_forwarded && collection_set()->is_empty()),\n+          \"Updating or evacuating iff has forwarded objects, or if evacuation phase is promoting in place without forwarding\");\n+#endif\n+  if (!in_progress && is_concurrent_young_mark_in_progress()) {\n+    \/\/ If young-marking is in progress when we turn off OLD_MARKING, leave MARKING (and YOUNG_MARKING) on\n+    assert(_gc_state.is_set(MARKING), \"concurrent_young_marking_in_progress implies MARKING\");\n+    set_gc_state(OLD_MARKING, in_progress);\n+  } else {\n+    set_gc_state(MARKING | OLD_MARKING, in_progress);\n+  }\n+  manage_satb_barrier(in_progress);\n+}\n+\n+bool ShenandoahHeap::is_prepare_for_old_mark_in_progress() const {\n+  return old_generation()->is_preparing_for_mark();\n@@ -1895,4 +2033,14 @@\n-void ShenandoahHeap::set_concurrent_mark_in_progress(bool in_progress) {\n-  assert(!has_forwarded_objects(), \"Not expected before\/after mark phase\");\n-  set_gc_state(MARKING, in_progress);\n-  ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(in_progress, !in_progress);\n+void ShenandoahHeap::manage_satb_barrier(bool active) {\n+  if (is_concurrent_mark_in_progress()) {\n+    \/\/ Ignore request to deactivate barrier while concurrent mark is in progress.\n+    \/\/ Do not attempt to re-activate the barrier if it is already active.\n+    if (active && !ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  } else {\n+    \/\/ No concurrent marking is in progress so honor request to deactivate,\n+    \/\/ but only if the barrier is already active.\n+    if (!active && ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  }\n@@ -1931,0 +2079,11 @@\n+void ShenandoahHeap::cancel_concurrent_mark() {\n+  if (mode()->is_generational()) {\n+    young_generation()->cancel_marking();\n+    old_generation()->cancel_marking();\n+  }\n+\n+  global_generation()->cancel_marking();\n+\n+  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+}\n+\n@@ -1936,0 +2095,1 @@\n+    _cancel_requested_time = os::elapsedTime();\n@@ -2059,4 +2219,0 @@\n-size_t ShenandoahHeap::bytes_allocated_since_gc_start() const {\n-  return Atomic::load(&_bytes_allocated_since_gc_start);\n-}\n-\n@@ -2064,1 +2220,6 @@\n-  Atomic::store(&_bytes_allocated_since_gc_start, (size_t)0);\n+  if (mode()->is_generational()) {\n+    young_generation()->reset_bytes_allocated_since_gc_start();\n+    old_generation()->reset_bytes_allocated_since_gc_start();\n+  }\n+\n+  global_generation()->reset_bytes_allocated_since_gc_start();\n@@ -2128,2 +2289,5 @@\n-    assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n-           \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    shenandoah_assert_generations_reconciled();\n+    if (gc_generation()->contains(r)) {\n+      assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n+             \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    }\n@@ -2184,1 +2348,1 @@\n-  ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n+  explicit ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n@@ -2204,1 +2368,0 @@\n-    T cl;\n@@ -2209,2 +2372,5 @@\n-      \/\/ We cannot transfer any more regions than will be reclaimed when the existing collection set is recycled because\n-      \/\/ we need the reclaimed collection set regions to replenish the collector reserves\n+\n+      \/\/ Now that evacuation is done, we can reassign any regions that had been reserved to hold the results of evacuation\n+      \/\/ to the mutator free set.  At the end of GC, we will have cset_regions newly evacuated fully empty regions from\n+      \/\/ which we will be able to replenish the Collector free set and the OldCollector free set in preparation for the\n+      \/\/ next GC cycle.\n@@ -2214,1 +2380,1 @@\n-\n+    T cl;\n@@ -2216,1 +2382,0 @@\n-    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n@@ -2222,3 +2387,3 @@\n-      }\n-      if (ShenandoahPacing) {\n-        _heap->pacer()->report_updaterefs(pointer_delta(update_watermark, r->bottom()));\n+        if (ShenandoahPacing) {\n+          _heap->pacer()->report_updaterefs(pointer_delta(update_watermark, r->bottom()));\n+        }\n@@ -2246,0 +2411,1 @@\n+ShenandoahSynchronizePinnedRegionStates::ShenandoahSynchronizePinnedRegionStates() : _lock(ShenandoahHeap::heap()->lock()) { }\n@@ -2247,22 +2413,13 @@\n-class ShenandoahFinalUpdateRefsUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalUpdateRefsUpdateRegionStateClosure() : _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    \/\/ Drop unnecessary \"pinned\" state from regions that does not have CP marks\n-    \/\/ anymore, as this would allow trashing them.\n-\n-    if (r->is_active()) {\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n+void ShenandoahSynchronizePinnedRegionStates::heap_region_do(ShenandoahHeapRegion* r) {\n+  \/\/ Drop \"pinned\" state from regions that no longer have a pinned count. Put\n+  \/\/ regions with a pinned count into the \"pinned\" state.\n+  if (r->is_active()) {\n+    if (r->is_pinned()) {\n+      if (r->pin_count() == 0) {\n+        ShenandoahHeapLocker locker(_lock);\n+        r->make_unpinned();\n+      }\n+    } else {\n+      if (r->pin_count() > 0) {\n+        ShenandoahHeapLocker locker(_lock);\n+        r->make_pinned();\n@@ -2272,3 +2429,1 @@\n-\n-  bool is_thread_safe() { return true; }\n-};\n+}\n@@ -2284,2 +2439,2 @@\n-    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n+\n+    final_update_refs_update_region_states();\n@@ -2298,0 +2453,5 @@\n+void ShenandoahHeap::final_update_refs_update_region_states() {\n+  ShenandoahSynchronizePinnedRegionStates cl;\n+  parallel_heap_region_iterate(&cl);\n+}\n+\n@@ -2299,6 +2459,42 @@\n-  {\n-    ShenandoahGCPhase phase(concurrent ?\n-                            ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n-                            ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n+  ShenandoahGCPhase phase(concurrent ?\n+                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n+                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+  ShenandoahHeapLocker locker(lock());\n+  size_t young_cset_regions, old_cset_regions;\n+  size_t first_old_region, last_old_region, old_region_count;\n+  _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+  \/\/ If there are no old regions, first_old_region will be greater than last_old_region\n+  assert((first_old_region > last_old_region) ||\n+         ((last_old_region + 1 - first_old_region >= old_region_count) &&\n+          get_region(first_old_region)->is_old() && get_region(last_old_region)->is_old()),\n+         \"sanity: old_region_count: \" SIZE_FORMAT \", first_old_region: \" SIZE_FORMAT \", last_old_region: \" SIZE_FORMAT,\n+         old_region_count, first_old_region, last_old_region);\n+\n+  if (mode()->is_generational()) {\n+#ifdef ASSERT\n+    if (ShenandoahVerify) {\n+      verifier()->verify_before_rebuilding_free_set();\n+    }\n+#endif\n+\n+    \/\/ The computation of bytes_of_allocation_runway_before_gc_trigger is quite conservative so consider all of this\n+    \/\/ available for transfer to old. Note that transfer of humongous regions does not impact available.\n+    ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+    size_t allocation_runway = gen_heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions);\n+    gen_heap->compute_old_generation_balance(allocation_runway, old_cset_regions);\n+\n+    \/\/ Total old_available may have been expanded to hold anticipated promotions.  We trigger if the fragmented available\n+    \/\/ memory represents more than 16 regions worth of data.  Note that fragmentation may increase when we promote regular\n+    \/\/ regions in place when many of these regular regions have an abundant amount of available memory within them.  Fragmentation\n+    \/\/ will decrease as promote-by-copy consumes the available memory within these partially consumed regions.\n+    \/\/\n+    \/\/ We consider old-gen to have excessive fragmentation if more than 12.5% of old-gen is free memory that resides\n+    \/\/ within partially consumed regions of memory.\n+  }\n+  \/\/ Rebuild free set based on adjusted generation sizes.\n+  _free_set->finish_rebuild(young_cset_regions, old_cset_regions, old_region_count);\n+\n+  if (mode()->is_generational()) {\n+    ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+    ShenandoahOldGeneration* old_gen = gen_heap->old_generation();\n+    old_gen->heuristics()->trigger_maybe(first_old_region, last_old_region, old_region_count, num_regions());\n@@ -2427,1 +2623,1 @@\n-  return _memory_pool->get_memory_usage();\n+  return MemoryUsage(_initial_size, used(), committed(), max_capacity());\n@@ -2492,0 +2688,22 @@\n+\n+ShenandoahGeneration* ShenandoahHeap::generation_for(ShenandoahAffiliation affiliation) const {\n+  if (!mode()->is_generational()) {\n+    return global_generation();\n+  } else if (affiliation == YOUNG_GENERATION) {\n+    return young_generation();\n+  } else if (affiliation == OLD_GENERATION) {\n+    return old_generation();\n+  }\n+\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n+\n+void ShenandoahHeap::log_heap_status(const char* msg) const {\n+  if (mode()->is_generational()) {\n+    young_generation()->log_status(msg);\n+    old_generation()->log_status(msg);\n+  } else {\n+    global_generation()->log_status(msg);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":504,"deletions":286,"binary":false,"changes":790,"status":"modified"}]}