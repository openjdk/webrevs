{"files":[{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -48,0 +49,4 @@\n+  volatile size_t _consecutive_young_gcs;\n+  size_t _mixed_gcs;\n+  size_t _success_old_gcs;\n+  size_t _interrupted_old_gcs;\n@@ -61,0 +66,4 @@\n+  void record_mixed_cycle();\n+  void record_success_old();\n+  void record_interrupted_old();\n+\n@@ -66,2 +75,2 @@\n-  void record_success_concurrent(bool is_abbreviated);\n-  void record_success_degenerated(bool is_abbreviated);\n+  void record_success_concurrent(bool is_young, bool is_abbreviated);\n+  void record_success_degenerated(bool is_young, bool is_abbreviated);\n@@ -92,0 +101,1 @@\n+<<<<<<< HEAD\n@@ -94,0 +104,8 @@\n+=======\n+  inline size_t consecutive_young_gc_count() const {\n+    return _consecutive_young_gcs;\n+  }\n+\n+private:\n+  void update_young(bool is_young);\n+>>>>>>> 8f4e6e226de7cb08f60bfd8dbbede466463d5b9d\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectorPolicy.hpp","additions":20,"deletions":2,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -31,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n@@ -33,0 +35,4 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -35,0 +41,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGC.hpp\"\n@@ -38,0 +45,2 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+#include \"logging\/log.hpp\"\n@@ -45,3 +54,6 @@\n-  _alloc_failure_waiters_lock(Mutex::safepoint-2, \"ShenandoahAllocFailureGC_lock\", true),\n-  _gc_waiters_lock(Mutex::safepoint-2, \"ShenandoahRequestedGC_lock\", true),\n-  _requested_gc_cause(GCCause::_no_cause_specified),\n+  _alloc_failure_waiters_lock(Mutex::safepoint - 2, \"ShenandoahAllocFailureGC_lock\", true),\n+  _gc_waiters_lock(Mutex::safepoint - 2, \"ShenandoahRequestedGC_lock\", true),\n+  _control_lock(Mutex::nosafepoint - 2, \"ShenandoahControlGC_lock\", true),\n+  _regulator_lock(Mutex::nosafepoint - 2, \"ShenandoahRegulatorGC_lock\", true),\n+  _requested_gc_cause(GCCause::_no_gc),\n+  _requested_generation(select_global_generation()),\n@@ -49,1 +61,3 @@\n-  _allocs_seen(0) {\n+  _degen_generation(nullptr),\n+  _allocs_seen(0),\n+  _mode(none) {\n@@ -56,1 +70,1 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n@@ -59,0 +73,1 @@\n+<<<<<<< HEAD\n@@ -61,0 +76,3 @@\n+=======\n+  ShenandoahGenerationType generation = select_global_generation();\n+>>>>>>> 8f4e6e226de7cb08f60bfd8dbbede466463d5b9d\n@@ -63,1 +81,1 @@\n-  double last_sleep_adjust_time = os::elapsedTime();\n+  uint age_period = 0;\n@@ -72,0 +90,1 @@\n+<<<<<<< HEAD\n@@ -78,0 +97,17 @@\n+=======\n+\n+  \/\/ Heuristics are notified of allocation failures here and other outcomes\n+  \/\/ of the cycle. They're also used here to control whether the Nth consecutive\n+  \/\/ degenerated cycle should be 'promoted' to a full cycle. The decision to\n+  \/\/ trigger a cycle or not is evaluated on the regulator thread.\n+  ShenandoahHeuristics* global_heuristics = heap->global_generation()->heuristics();\n+  while (!in_graceful_shutdown() && !should_terminate()) {\n+    \/\/ Figure out if we have pending requests.\n+    const bool alloc_failure_pending = _alloc_failure_gc.is_set();\n+    const bool humongous_alloc_failure_pending = _humongous_alloc_failure_gc.is_set();\n+\n+    GCCause::Cause cause = Atomic::xchg(&_requested_gc_cause, GCCause::_no_gc);\n+\n+    const bool explicit_gc_requested = is_explicit_gc(cause);\n+    const bool implicit_gc_requested = is_implicit_gc(cause);\n+>>>>>>> 8f4e6e226de7cb08f60bfd8dbbede466463d5b9d\n@@ -86,2 +122,1 @@\n-    GCMode mode = none;\n-    GCCause::Cause cause = GCCause::_last_gc_cause;\n+    set_gc_mode(none);\n@@ -100,4 +135,3 @@\n-      if (ShenandoahDegeneratedGC && heuristics->should_degenerate_cycle()) {\n-        heuristics->record_allocation_failure_gc();\n-        policy->record_alloc_failure_to_degenerated(degen_point);\n-        mode = stw_degenerated;\n+      if (degen_point == ShenandoahGC::_degenerated_outside_cycle) {\n+        _degen_generation = heap->mode()->is_generational() ?\n+                heap->young_generation() : heap->global_generation();\n@@ -105,3 +139,1 @@\n-        heuristics->record_allocation_failure_gc();\n-        policy->record_alloc_failure_to_full();\n-        mode = stw_full;\n+        assert(_degen_generation != nullptr, \"Need to know which generation to resume\");\n@@ -110,0 +142,1 @@\n+<<<<<<< HEAD\n@@ -121,0 +154,23 @@\n+=======\n+      ShenandoahHeuristics* heuristics = _degen_generation->heuristics();\n+      generation = _degen_generation->type();\n+      bool old_gen_evacuation_failed = heap->clear_old_evacuation_failure();\n+\n+      \/\/ Do not bother with degenerated cycle if old generation evacuation failed or if humongous allocation failed\n+      if (ShenandoahDegeneratedGC && heuristics->should_degenerate_cycle() &&\n+          !old_gen_evacuation_failed && !humongous_alloc_failure_pending) {\n+        heuristics->record_allocation_failure_gc();\n+        policy->record_alloc_failure_to_degenerated(degen_point);\n+        set_gc_mode(stw_degenerated);\n+      } else {\n+        \/\/ TODO: if humongous_alloc_failure_pending, there might be value in trying a \"compacting\" degen before\n+        \/\/ going all the way to full.  But it's a lot of work to implement this, and it may not provide value.\n+        \/\/ A compacting degen can move young regions around without doing full old-gen mark (relying upon the\n+        \/\/ remembered set scan), so it might be faster than a full gc.\n+        \/\/\n+        \/\/ Longer term, think about how to defragment humongous memory concurrently.\n+\n+        heuristics->record_allocation_failure_gc();\n+        policy->record_alloc_failure_to_full();\n+        generation = select_global_generation();\n+        set_gc_mode(stw_full);\n@@ -122,5 +178,14 @@\n-    } else {\n-      \/\/ Potential normal cycle: ask heuristics if it wants to act\n-      if (heuristics->should_start_gc()) {\n-        mode = default_mode;\n-        cause = default_cause;\n+    } else if (explicit_gc_requested) {\n+      generation = select_global_generation();\n+      log_info(gc)(\"Trigger: Explicit GC request (%s)\", GCCause::to_string(cause));\n+\n+      global_heuristics->record_requested_gc();\n+\n+      if (ExplicitGCInvokesConcurrent) {\n+        policy->record_explicit_to_concurrent();\n+        set_gc_mode(default_mode);\n+        \/\/ Unload and clean up everything\n+        heap->set_unload_classes(global_heuristics->can_unload_classes());\n+      } else {\n+        policy->record_explicit_to_full();\n+        set_gc_mode(stw_full);\n@@ -128,0 +193,33 @@\n+    } else if (implicit_gc_requested) {\n+      generation = select_global_generation();\n+      log_info(gc)(\"Trigger: Implicit GC request (%s)\", GCCause::to_string(cause));\n+\n+      global_heuristics->record_requested_gc();\n+\n+      if (ShenandoahImplicitGCInvokesConcurrent) {\n+        policy->record_implicit_to_concurrent();\n+        set_gc_mode(default_mode);\n+\n+        \/\/ Unload and clean up everything\n+        heap->set_unload_classes(global_heuristics->can_unload_classes());\n+      } else {\n+        policy->record_implicit_to_full();\n+        set_gc_mode(stw_full);\n+>>>>>>> 8f4e6e226de7cb08f60bfd8dbbede466463d5b9d\n+      }\n+    } else {\n+      \/\/ We should only be here if the regulator requested a cycle or if\n+      \/\/ there is an old generation mark in progress.\n+      if (cause == GCCause::_shenandoah_concurrent_gc) {\n+        if (_requested_generation == OLD && heap->doing_mixed_evacuations()) {\n+          \/\/ If a request to start an old cycle arrived while an old cycle was running, but _before_\n+          \/\/ it chose any regions for evacuation we don't want to start a new old cycle. Rather, we want\n+          \/\/ the heuristic to run a young collection so that we can evacuate some old regions.\n+          assert(!heap->is_concurrent_old_mark_in_progress(), \"Should not be running mixed collections and concurrent marking\");\n+          generation = YOUNG;\n+        } else {\n+          generation = _requested_generation;\n+        }\n+\n+        \/\/ preemption was requested or this is a regular cycle\n+        set_gc_mode(default_mode);\n@@ -129,2 +227,22 @@\n-      \/\/ Ask policy if this cycle wants to process references or unload classes\n-      heap->set_unload_classes(heuristics->should_unload_classes());\n+        \/\/ Don't start a new old marking if there is one already in progress\n+        if (generation == OLD && heap->is_concurrent_old_mark_in_progress()) {\n+          set_gc_mode(servicing_old);\n+        }\n+\n+        if (generation == select_global_generation()) {\n+          heap->set_unload_classes(global_heuristics->should_unload_classes());\n+        } else {\n+          heap->set_unload_classes(false);\n+        }\n+      } else if (heap->is_concurrent_old_mark_in_progress() || heap->is_prepare_for_old_mark_in_progress()) {\n+        \/\/ Nobody asked us to do anything, but we have an old-generation mark or old-generation preparation for\n+        \/\/ mixed evacuation in progress, so resume working on that.\n+        log_info(gc)(\"Resume old GC: marking is%s in progress, preparing is%s in progress\",\n+                     heap->is_concurrent_old_mark_in_progress() ? \"\" : \" NOT\",\n+                     heap->is_prepare_for_old_mark_in_progress() ? \"\" : \" NOT\");\n+\n+        cause = GCCause::_shenandoah_concurrent_gc;\n+        generation = OLD;\n+        set_gc_mode(servicing_old);\n+        heap->set_unload_classes(false);\n+      }\n@@ -133,0 +251,1 @@\n+<<<<<<< HEAD\n@@ -141,0 +260,4 @@\n+=======\n+    const bool gc_requested = (gc_mode() != none);\n+    assert (!gc_requested || cause != GCCause::_no_gc, \"GC cause should be set\");\n+>>>>>>> 8f4e6e226de7cb08f60bfd8dbbede466463d5b9d\n@@ -143,0 +266,6 @@\n+      \/\/ Blow away all soft references on this cycle, if handling allocation failure,\n+      \/\/ either implicit or explicit GC request, or we are requested to do so unconditionally.\n+      if (generation == select_global_generation() && (alloc_failure_pending || implicit_gc_requested || explicit_gc_requested || ShenandoahAlwaysClearSoftRefs)) {\n+        heap->soft_ref_policy()->set_should_clear_all_soft_refs(true);\n+      }\n+\n@@ -159,4 +288,16 @@\n-\n-      switch (mode) {\n-        case concurrent_normal:\n-          service_concurrent_normal_cycle(cause);\n+      \/\/ In case this is a degenerated cycle, remember whether original cycle was aging.\n+      const bool was_aging_cycle = heap->is_aging_cycle();\n+      heap->set_aging_cycle(false);\n+\n+      switch (gc_mode()) {\n+        case concurrent_normal: {\n+          \/\/ At this point:\n+          \/\/  if (generation == YOUNG), this is a normal YOUNG cycle\n+          \/\/  if (generation == OLD), this is a bootstrap OLD cycle\n+          \/\/  if (generation == GLOBAL), this is a GLOBAL cycle triggered by System.gc()\n+          \/\/ In all three cases, we want to age old objects if this is an aging cycle\n+          if (age_period-- == 0) {\n+             heap->set_aging_cycle(true);\n+             age_period = ShenandoahAgingCyclePeriod - 1;\n+          }\n+          service_concurrent_normal_cycle(heap, generation, cause);\n@@ -164,1 +305,3 @@\n-        case stw_degenerated:\n+        }\n+        case stw_degenerated: {\n+          heap->set_aging_cycle(was_aging_cycle);\n@@ -167,1 +310,6 @@\n-        case stw_full:\n+        }\n+        case stw_full: {\n+          if (age_period-- == 0) {\n+            heap->set_aging_cycle(true);\n+            age_period = ShenandoahAgingCyclePeriod - 1;\n+          }\n@@ -170,0 +318,7 @@\n+        }\n+        case servicing_old: {\n+          assert(generation == OLD, \"Expected old generation here\");\n+          GCIdMark gc_id_mark;\n+          service_concurrent_old_cycle(heap, cause);\n+          break;\n+        }\n@@ -209,20 +364,1 @@\n-        heuristics->clear_metaspace_oom();\n-      }\n-\n-      \/\/ Commit worker statistics to cycle data\n-      heap->phase_timings()->flush_par_workers_to_cycle();\n-      if (ShenandoahPacing) {\n-        heap->pacer()->flush_stats_to_cycle();\n-      }\n-\n-      \/\/ Print GC stats for current cycle\n-      {\n-        LogTarget(Info, gc, stats) lt;\n-        if (lt.is_enabled()) {\n-          ResourceMark rm;\n-          LogStream ls(lt);\n-          heap->phase_timings()->print_cycle_on(&ls);\n-          if (ShenandoahPacing) {\n-            heap->pacer()->print_cycle_on(&ls);\n-          }\n-        }\n+        global_heuristics->clear_metaspace_oom();\n@@ -231,2 +367,1 @@\n-      \/\/ Commit statistics to globals\n-      heap->phase_timings()->flush_cycle_to_global();\n+      process_phase_timings(heap);\n@@ -242,1 +377,1 @@\n-      \/\/ Allow allocators to know we have seen this much regions\n+      \/\/ Allow pacer to know we have seen this many allocations\n@@ -268,8 +403,6 @@\n-    \/\/ Wait before performing the next action. If allocation happened during this wait,\n-    \/\/ we exit sooner, to let heuristics re-evaluate new conditions. If we are at idle,\n-    \/\/ back off exponentially.\n-    if (heap->has_changed()) {\n-      sleep = ShenandoahControlIntervalMin;\n-    } else if ((current - last_sleep_adjust_time) * 1000 > ShenandoahControlIntervalAdjustPeriod){\n-      sleep = MIN2<int>(ShenandoahControlIntervalMax, MAX2(1, sleep * 2));\n-      last_sleep_adjust_time = current;\n+    \/\/ Wait for ShenandoahControlIntervalMax unless there was an allocation failure or another request was made mid-cycle.\n+    if (!is_alloc_failure_gc() && _requested_gc_cause == GCCause::_no_gc) {\n+      \/\/ The timed wait is necessary because this thread has a responsibility to send\n+      \/\/ 'alloc_words' to the pacer when it does not perform a GC.\n+      MonitorLocker lock(&_control_lock, Mutex::_no_safepoint_check_flag);\n+      lock.wait(ShenandoahControlIntervalMax);\n@@ -277,1 +410,0 @@\n-    os::naked_short_sleep(sleep);\n@@ -286,1 +418,207 @@\n-void ShenandoahControlThread::service_concurrent_normal_cycle(GCCause::Cause cause) {\n+void ShenandoahControlThread::process_phase_timings(const ShenandoahHeap* heap) {\n+  \/\/ Commit worker statistics to cycle data\n+  heap->phase_timings()->flush_par_workers_to_cycle();\n+  if (ShenandoahPacing) {\n+    heap->pacer()->flush_stats_to_cycle();\n+  }\n+\n+  ShenandoahEvacuationTracker* evac_tracker = heap->evac_tracker();\n+  ShenandoahCycleStats         evac_stats   = evac_tracker->flush_cycle_to_global();\n+\n+  \/\/ Print GC stats for current cycle\n+  {\n+    LogTarget(Info, gc, stats) lt;\n+    if (lt.is_enabled()) {\n+      ResourceMark rm;\n+      LogStream ls(lt);\n+      heap->phase_timings()->print_cycle_on(&ls);\n+      evac_tracker->print_evacuations_on(&ls, &evac_stats.workers,\n+                                              &evac_stats.mutators);\n+      if (ShenandoahPacing) {\n+        heap->pacer()->print_cycle_on(&ls);\n+      }\n+    }\n+  }\n+\n+  \/\/ Commit statistics to globals\n+  heap->phase_timings()->flush_cycle_to_global();\n+}\n+\n+\/\/ Young and old concurrent cycles are initiated by the regulator. Implicit\n+\/\/ and explicit GC requests are handled by the controller thread and always\n+\/\/ run a global cycle (which is concurrent by default, but may be overridden\n+\/\/ by command line options). Old cycles always degenerate to a global cycle.\n+\/\/ Young cycles are degenerated to complete the young cycle.  Young\n+\/\/ and old degen may upgrade to Full GC.  Full GC may also be\n+\/\/ triggered directly by a System.gc() invocation.\n+\/\/\n+\/\/\n+\/\/      +-----+ Idle +-----+-----------+---------------------+\n+\/\/      |         +        |           |                     |\n+\/\/      |         |        |           |                     |\n+\/\/      |         |        v           |                     |\n+\/\/      |         |  Bootstrap Old +-- | ------------+       |\n+\/\/      |         |   +                |             |       |\n+\/\/      |         |   |                |             |       |\n+\/\/      |         v   v                v             v       |\n+\/\/      |    Resume Old <----------+ Young +--> Young Degen  |\n+\/\/      |     +  +   ^                            +  +       |\n+\/\/      v     |  |   |                            |  |       |\n+\/\/   Global <-+  |   +----------------------------+  |       |\n+\/\/      +        |                                   |       |\n+\/\/      |        v                                   v       |\n+\/\/      +--->  Global Degen +--------------------> Full <----+\n+\/\/\n+void ShenandoahControlThread::service_concurrent_normal_cycle(ShenandoahHeap* heap,\n+                                                              const ShenandoahGenerationType generation,\n+                                                              GCCause::Cause cause) {\n+  GCIdMark gc_id_mark;\n+  ShenandoahGeneration* the_generation = nullptr;\n+  switch (generation) {\n+    case YOUNG: {\n+      \/\/ Run a young cycle. This might or might not, have interrupted an ongoing\n+      \/\/ concurrent mark in the old generation. We need to think about promotions\n+      \/\/ in this case. Promoted objects should be above the TAMS in the old regions\n+      \/\/ they end up in, but we have to be sure we don't promote into any regions\n+      \/\/ that are in the cset.\n+      log_info(gc, ergo)(\"Start GC cycle (YOUNG)\");\n+      the_generation = heap->young_generation();\n+      service_concurrent_cycle(the_generation, cause, false);\n+      break;\n+    }\n+    case OLD: {\n+      log_info(gc, ergo)(\"Start GC cycle (OLD)\");\n+      the_generation = heap->old_generation();\n+      service_concurrent_old_cycle(heap, cause);\n+      break;\n+    }\n+    case GLOBAL_GEN: {\n+      log_info(gc, ergo)(\"Start GC cycle (GLOBAL)\");\n+      the_generation = heap->global_generation();\n+      service_concurrent_cycle(the_generation, cause, false);\n+      break;\n+    }\n+    case GLOBAL_NON_GEN: {\n+      log_info(gc, ergo)(\"Start GC cycle\");\n+      the_generation = heap->global_generation();\n+      service_concurrent_cycle(the_generation, cause, false);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void ShenandoahControlThread::service_concurrent_old_cycle(ShenandoahHeap* heap, GCCause::Cause &cause) {\n+  ShenandoahOldGeneration* old_generation = heap->old_generation();\n+  ShenandoahYoungGeneration* young_generation = heap->young_generation();\n+  ShenandoahOldGeneration::State original_state = old_generation->state();\n+\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+\n+  switch (original_state) {\n+    case ShenandoahOldGeneration::FILLING: {\n+      _allow_old_preemption.set();\n+      old_generation->entry_coalesce_and_fill();\n+      _allow_old_preemption.unset();\n+\n+      \/\/ Before bootstrapping begins, we must acknowledge any cancellation request.\n+      \/\/ If the gc has not been cancelled, this does nothing. If it has been cancelled,\n+      \/\/ this will clear the cancellation request and exit before starting the bootstrap\n+      \/\/ phase. This will allow the young GC cycle to proceed normally. If we do not\n+      \/\/ acknowledge the cancellation request, the subsequent young cycle will observe\n+      \/\/ the request and essentially cancel itself.\n+      if (check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle)) {\n+        log_info(gc)(\"Preparation for old generation cycle was cancelled\");\n+        return;\n+      }\n+\n+      \/\/ Coalescing threads completed and nothing was cancelled. it is safe to transition from this state.\n+      old_generation->transition_to(ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP);\n+      return;\n+    }\n+    case ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP:\n+      old_generation->transition_to(ShenandoahOldGeneration::BOOTSTRAPPING);\n+    case ShenandoahOldGeneration::BOOTSTRAPPING: {\n+      \/\/ Configure the young generation's concurrent mark to put objects in\n+      \/\/ old regions into the concurrent mark queues associated with the old\n+      \/\/ generation. The young cycle will run as normal except that rather than\n+      \/\/ ignore old references it will mark and enqueue them in the old concurrent\n+      \/\/ task queues but it will not traverse them.\n+      set_gc_mode(bootstrapping_old);\n+      young_generation->set_old_gen_task_queues(old_generation->task_queues());\n+      ShenandoahGCSession session(cause, young_generation);\n+      service_concurrent_cycle(heap, young_generation, cause, true);\n+      process_phase_timings(heap);\n+      if (heap->cancelled_gc()) {\n+        \/\/ Young generation bootstrap cycle has failed. Concurrent mark for old generation\n+        \/\/ is going to resume after degenerated bootstrap cycle completes.\n+        log_info(gc)(\"Bootstrap cycle for old generation was cancelled\");\n+        return;\n+      }\n+\n+      \/\/ Reset the degenerated point. Normally this would happen at the top\n+      \/\/ of the control loop, but here we have just completed a young cycle\n+      \/\/ which has bootstrapped the old concurrent marking.\n+      _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n+\n+      \/\/ From here we will 'resume' the old concurrent mark. This will skip reset\n+      \/\/ and init mark for the concurrent mark. All of that work will have been\n+      \/\/ done by the bootstrapping young cycle.\n+      set_gc_mode(servicing_old);\n+      old_generation->transition_to(ShenandoahOldGeneration::MARKING);\n+    }\n+    case ShenandoahOldGeneration::MARKING: {\n+      ShenandoahGCSession session(cause, old_generation);\n+      bool marking_complete = resume_concurrent_old_cycle(old_generation, cause);\n+      if (marking_complete) {\n+        assert(old_generation->state() != ShenandoahOldGeneration::MARKING, \"Should not still be marking\");\n+        if (original_state == ShenandoahOldGeneration::MARKING) {\n+          heap->mmu_tracker()->record_old_marking_increment(true);\n+          heap->log_heap_status(\"At end of Concurrent Old Marking finishing increment\");\n+        }\n+      } else if (original_state == ShenandoahOldGeneration::MARKING) {\n+        heap->mmu_tracker()->record_old_marking_increment(false);\n+        heap->log_heap_status(\"At end of Concurrent Old Marking increment\");\n+      }\n+      break;\n+    }\n+    default:\n+      fatal(\"Unexpected state for old GC: %s\", ShenandoahOldGeneration::state_name(old_generation->state()));\n+  }\n+}\n+\n+bool ShenandoahControlThread::resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause) {\n+  assert(ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress(), \"Old mark should be in progress\");\n+  log_debug(gc)(\"Resuming old generation with \" UINT32_FORMAT \" marking tasks queued\", generation->task_queues()->tasks());\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  \/\/ We can only tolerate being cancelled during concurrent marking or during preparation for mixed\n+  \/\/ evacuation. This flag here (passed by reference) is used to control precisely where the regulator\n+  \/\/ is allowed to cancel a GC.\n+  ShenandoahOldGC gc(generation, _allow_old_preemption);\n+  if (gc.collect(cause)) {\n+    generation->record_success_concurrent(false);\n+  }\n+\n+  if (heap->cancelled_gc()) {\n+    \/\/ It's possible the gc cycle was cancelled after the last time\n+    \/\/ the collection checked for cancellation. In which case, the\n+    \/\/ old gc cycle is still completed, and we have to deal with this\n+    \/\/ cancellation. We set the degeneration point to be outside\n+    \/\/ the cycle because if this is an allocation failure, that is\n+    \/\/ what must be done (there is no degenerated old cycle). If the\n+    \/\/ cancellation was due to a heuristic wanting to start a young\n+    \/\/ cycle, then we are not actually going to a degenerated cycle,\n+    \/\/ so the degenerated point doesn't matter here.\n+    check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle);\n+    if (_requested_gc_cause == GCCause::_shenandoah_concurrent_gc) {\n+      heap->shenandoah_policy()->record_interrupted_old();\n+    }\n+    return false;\n+  }\n+  return true;\n+}\n+\n+void ShenandoahControlThread::service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause, bool do_old_gc_bootstrap) {\n@@ -322,1 +660,0 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -325,3 +662,2 @@\n-  GCIdMark gc_id_mark;\n-  ShenandoahGCSession session(cause);\n-\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahGCSession session(cause, generation);\n@@ -330,1 +666,8 @@\n-  ShenandoahConcurrentGC gc;\n+  service_concurrent_cycle(heap, generation, cause, do_old_gc_bootstrap);\n+}\n+\n+void ShenandoahControlThread::service_concurrent_cycle(ShenandoahHeap* heap,\n+                                                       ShenandoahGeneration* generation,\n+                                                       GCCause::Cause& cause,\n+                                                       bool do_old_gc_bootstrap) {\n+  ShenandoahConcurrentGC gc(generation, do_old_gc_bootstrap);\n@@ -333,2 +676,1 @@\n-    heap->heuristics()->record_success_concurrent();\n-    heap->shenandoah_policy()->record_success_concurrent(gc.abbreviated());\n+    generation->record_success_concurrent(gc.abbreviated());\n@@ -338,0 +680,38 @@\n+    assert(!generation->is_old(), \"Old GC takes a different control path\");\n+    \/\/ Concurrent young-gen collection degenerates to young\n+    \/\/ collection.  Same for global collections.\n+    _degen_generation = generation;\n+  }\n+  const char* msg;\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahMmuTracker* mmu_tracker = heap->mmu_tracker();\n+    if (generation->is_young()) {\n+      if (heap->cancelled_gc()) {\n+        msg = (do_old_gc_bootstrap) ? \"At end of Interrupted Concurrent Bootstrap GC\":\n+                                      \"At end of Interrupted Concurrent Young GC\";\n+      } else {\n+        \/\/ We only record GC results if GC was successful\n+        msg = (do_old_gc_bootstrap) ? \"At end of Concurrent Bootstrap GC\":\n+                                      \"At end of Concurrent Young GC\";\n+        if (heap->collection_set()->has_old_regions()) {\n+          mmu_tracker->record_mixed(get_gc_id());\n+        } else if (do_old_gc_bootstrap) {\n+          mmu_tracker->record_bootstrap(get_gc_id());\n+        } else {\n+          mmu_tracker->record_young(get_gc_id());\n+        }\n+      }\n+    } else {\n+      assert(generation->is_global(), \"If not young, must be GLOBAL\");\n+      assert(!do_old_gc_bootstrap, \"Do not bootstrap with GLOBAL GC\");\n+      if (heap->cancelled_gc()) {\n+        msg = \"At end of Interrupted Concurrent GLOBAL GC\";\n+      } else {\n+        \/\/ We only record GC results if GC was successful\n+        msg = \"At end of Concurrent Global GC\";\n+        mmu_tracker->record_global(get_gc_id());\n+      }\n+    }\n+  } else {\n+    msg = heap->cancelled_gc() ? \"At end of cancelled GC\" :\n+                                 \"At end of GC\";\n@@ -339,0 +719,1 @@\n+  heap->log_heap_status(msg);\n@@ -343,7 +724,28 @@\n-  if (heap->cancelled_gc()) {\n-    assert (is_alloc_failure_gc() || in_graceful_shutdown(), \"Cancel GC either for alloc failure GC, or gracefully exiting\");\n-    if (!in_graceful_shutdown()) {\n-      assert (_degen_point == ShenandoahGC::_degenerated_outside_cycle,\n-              \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n-      _degen_point = point;\n-    }\n+  if (!heap->cancelled_gc()) {\n+    return false;\n+  }\n+\n+  if (in_graceful_shutdown()) {\n+    return true;\n+  }\n+\n+  assert(_degen_point == ShenandoahGC::_degenerated_outside_cycle,\n+         \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n+\n+  if (is_alloc_failure_gc()) {\n+    _degen_point = point;\n+    _preemption_requested.unset();\n+    return true;\n+  }\n+\n+  if (_preemption_requested.is_set()) {\n+    assert(_requested_generation == YOUNG, \"Only young GCs may preempt old.\");\n+    _preemption_requested.unset();\n+\n+    \/\/ Old generation marking is only cancellable during concurrent marking.\n+    \/\/ Once final mark is complete, the code does not check again for cancellation.\n+    \/\/ If old generation was cancelled for an allocation failure, we wouldn't\n+    \/\/ make it to this case. The calling code is responsible for forcing a\n+    \/\/ cancellation due to allocation failure into a degenerated cycle.\n+    _degen_point = point;\n+    heap->clear_cancelled_gc(false \/* clear oom handler *\/);\n@@ -352,0 +754,2 @@\n+\n+  fatal(\"Cancel GC either for alloc failure GC, or gracefully exiting, or to pause old generation marking\");\n@@ -360,0 +764,2 @@\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n@@ -361,1 +767,1 @@\n-  ShenandoahGCSession session(cause);\n+  ShenandoahGCSession session(cause, heap->global_generation());\n@@ -367,2 +773,4 @@\n-void ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahGC::ShenandoahDegenPoint point) {\n-  assert (point != ShenandoahGC::_degenerated_unset, \"Degenerated point should be set\");\n+void ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause,\n+                                                            ShenandoahGC::ShenandoahDegenPoint point) {\n+  assert(point != ShenandoahGC::_degenerated_unset, \"Degenerated point should be set\");\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n@@ -371,1 +779,1 @@\n-  ShenandoahGCSession session(cause);\n+  ShenandoahGCSession session(cause, _degen_generation);\n@@ -373,1 +781,1 @@\n-  ShenandoahDegenGC gc(point);\n+  ShenandoahDegenGC gc(point, _degen_generation);\n@@ -375,0 +783,19 @@\n+\n+  assert(heap->young_generation()->task_queues()->is_empty(), \"Unexpected young generation marking tasks\");\n+  if (_degen_generation->is_global()) {\n+    assert(heap->old_generation()->task_queues()->is_empty(), \"Unexpected old generation marking tasks\");\n+    assert(heap->global_generation()->task_queues()->is_empty(), \"Unexpected global generation marking tasks\");\n+  } else {\n+    assert(_degen_generation->is_young(), \"Expected degenerated young cycle, if not global.\");\n+    ShenandoahOldGeneration* old = heap->old_generation();\n+    if (old->state() == ShenandoahOldGeneration::BOOTSTRAPPING) {\n+      old->transition_to(ShenandoahOldGeneration::MARKING);\n+    }\n+  }\n+}\n+\n+<<<<<<< HEAD\n+=======\n+bool ShenandoahControlThread::is_explicit_gc(GCCause::Cause cause) const {\n+  return GCCause::is_user_requested_gc(cause) ||\n+         GCCause::is_serviceability_requested_gc(cause);\n@@ -377,0 +804,7 @@\n+bool ShenandoahControlThread::is_implicit_gc(GCCause::Cause cause) const {\n+  return !is_explicit_gc(cause)\n+      && cause != GCCause::_shenandoah_concurrent_gc\n+      && cause != GCCause::_no_gc;\n+}\n+\n+>>>>>>> 8f4e6e226de7cb08f60bfd8dbbede466463d5b9d\n@@ -383,0 +817,63 @@\n+bool ShenandoahControlThread::request_concurrent_gc(ShenandoahGenerationType generation) {\n+  if (_preemption_requested.is_set() || _requested_gc_cause != GCCause::_no_gc || ShenandoahHeap::heap()->cancelled_gc()) {\n+    \/\/ Ignore subsequent requests from the heuristics\n+    log_debug(gc, thread)(\"Reject request for concurrent gc: preemption_requested: %s, gc_requested: %s, gc_cancelled: %s\",\n+                          BOOL_TO_STR(_preemption_requested.is_set()),\n+                          GCCause::to_string(_requested_gc_cause),\n+                          BOOL_TO_STR(ShenandoahHeap::heap()->cancelled_gc()));\n+    return false;\n+  }\n+\n+  if (gc_mode() == none) {\n+    GCCause::Cause existing = Atomic::cmpxchg(&_requested_gc_cause, GCCause::_no_gc, GCCause::_shenandoah_concurrent_gc);\n+    if (existing != GCCause::_no_gc) {\n+      log_debug(gc, thread)(\"Reject request for concurrent gc because another gc is pending: %s\", GCCause::to_string(existing));\n+      return false;\n+    }\n+\n+    _requested_generation = generation;\n+    notify_control_thread();\n+\n+    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n+    while (gc_mode() == none) {\n+      ml.wait();\n+    }\n+    return true;\n+  }\n+\n+  if (preempt_old_marking(generation)) {\n+    assert(gc_mode() == servicing_old, \"Expected to be servicing old, but was: %s.\", gc_mode_name(gc_mode()));\n+    GCCause::Cause existing = Atomic::cmpxchg(&_requested_gc_cause, GCCause::_no_gc, GCCause::_shenandoah_concurrent_gc);\n+    if (existing != GCCause::_no_gc) {\n+      log_debug(gc, thread)(\"Reject request to interrupt old gc because another gc is pending: %s\", GCCause::to_string(existing));\n+      return false;\n+    }\n+\n+    log_info(gc)(\"Preempting old generation mark to allow %s GC\", shenandoah_generation_name(generation));\n+    _requested_generation = generation;\n+    _preemption_requested.set();\n+    ShenandoahHeap::heap()->cancel_gc(GCCause::_shenandoah_concurrent_gc);\n+    notify_control_thread();\n+\n+    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n+    while (gc_mode() == servicing_old) {\n+      ml.wait();\n+    }\n+    return true;\n+  }\n+\n+  log_debug(gc, thread)(\"Reject request for concurrent gc: mode: %s, allow_old_preemption: %s\",\n+                        gc_mode_name(gc_mode()),\n+                        BOOL_TO_STR(_allow_old_preemption.is_set()));\n+  return false;\n+}\n+\n+void ShenandoahControlThread::notify_control_thread() {\n+  MonitorLocker locker(&_control_lock, Mutex::_no_safepoint_check_flag);\n+  _control_lock.notify();\n+}\n+\n+bool ShenandoahControlThread::preempt_old_marking(ShenandoahGenerationType generation) {\n+  return (generation == YOUNG) && _allow_old_preemption.try_unset();\n+}\n+\n@@ -397,5 +894,7 @@\n-    \/\/ Although setting gc request is under _gc_waiters_lock, but read side (run_service())\n-    \/\/ does not take the lock. We need to enforce following order, so that read side sees\n-    \/\/ latest requested gc cause when the flag is set.\n-    _requested_gc_cause = cause;\n-    _gc_requested.set();\n+    \/\/ This races with the regulator thread to start a concurrent gc and the\n+    \/\/ control thread to clear it at the start of a cycle. Threads here are\n+    \/\/ allowed to escalate a heuristic's request for concurrent gc.\n+    GCCause::Cause existing = Atomic::xchg(&_requested_gc_cause, cause);\n+    if (existing != GCCause::_no_gc) {\n+      log_debug(gc, thread)(\"GC request supersedes existing request: %s\", GCCause::to_string(existing));\n+    }\n@@ -403,0 +902,1 @@\n+    notify_control_thread();\n@@ -414,0 +914,1 @@\n+  bool is_humongous = req.size() > ShenandoahHeapRegion::region_size_words();\n@@ -415,1 +916,1 @@\n-  if (try_set_alloc_failure_gc()) {\n+  if (try_set_alloc_failure_gc(is_humongous)) {\n@@ -420,1 +921,0 @@\n-\n@@ -436,0 +936,1 @@\n+  bool is_humongous = (words > ShenandoahHeapRegion::region_size_words());\n@@ -437,1 +938,1 @@\n-  if (try_set_alloc_failure_gc()) {\n+  if (try_set_alloc_failure_gc(is_humongous)) {\n@@ -449,0 +950,1 @@\n+  _humongous_alloc_failure_gc.unset();\n@@ -453,1 +955,4 @@\n-bool ShenandoahControlThread::try_set_alloc_failure_gc() {\n+bool ShenandoahControlThread::try_set_alloc_failure_gc(bool is_humongous) {\n+  if (is_humongous) {\n+    _humongous_alloc_failure_gc.try_set();\n+  }\n@@ -462,1 +967,0 @@\n-  _gc_requested.unset();\n@@ -495,0 +999,29 @@\n+\n+const char* ShenandoahControlThread::gc_mode_name(ShenandoahControlThread::GCMode mode) {\n+  switch (mode) {\n+    case none:              return \"idle\";\n+    case concurrent_normal: return \"normal\";\n+    case stw_degenerated:   return \"degenerated\";\n+    case stw_full:          return \"full\";\n+    case servicing_old:     return \"old\";\n+    case bootstrapping_old: return \"bootstrap\";\n+    default:                return \"unknown\";\n+  }\n+}\n+\n+void ShenandoahControlThread::set_gc_mode(ShenandoahControlThread::GCMode new_mode) {\n+  if (_mode != new_mode) {\n+    log_info(gc)(\"Transition from: %s to: %s\", gc_mode_name(_mode), gc_mode_name(new_mode));\n+    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n+    _mode = new_mode;\n+    ml.notify_all();\n+  }\n+}\n+\n+ShenandoahGenerationType ShenandoahControlThread::select_global_generation() {\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return GLOBAL_GEN;\n+  } else {\n+    return GLOBAL_NON_GEN;\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":623,"deletions":90,"binary":false,"changes":713,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -38,7 +39,0 @@\n-  typedef enum {\n-    none,\n-    concurrent_normal,\n-    stw_degenerated,\n-    stw_full\n-  } GCMode;\n-\n@@ -47,1 +41,1 @@\n-  \/\/ to make complete explicit cycle for for demanding customers.\n+  \/\/ to make complete explicit cycle for demanding customers.\n@@ -50,0 +44,2 @@\n+  Monitor _control_lock;\n+  Monitor _regulator_lock;\n@@ -52,0 +48,9 @@\n+  typedef enum {\n+    none,\n+    concurrent_normal,\n+    stw_degenerated,\n+    stw_full,\n+    bootstrapping_old,\n+    servicing_old\n+  } GCMode;\n+\n@@ -55,0 +60,2 @@\n+  size_t get_gc_id();\n+\n@@ -56,1 +63,2 @@\n-  ShenandoahSharedFlag _gc_requested;\n+  ShenandoahSharedFlag _allow_old_preemption;\n+  ShenandoahSharedFlag _preemption_requested;\n@@ -58,0 +66,1 @@\n+  ShenandoahSharedFlag _humongous_alloc_failure_gc;\n@@ -59,1 +68,3 @@\n-  GCCause::Cause       _requested_gc_cause;\n+\n+  GCCause::Cause  _requested_gc_cause;\n+  volatile ShenandoahGenerationType _requested_generation;\n@@ -61,0 +72,1 @@\n+  ShenandoahGeneration* _degen_generation;\n@@ -67,0 +79,2 @@\n+  volatile GCMode _mode;\n+  shenandoah_padding(3);\n@@ -68,0 +82,1 @@\n+  \/\/ Returns true if the cycle has been cancelled or degenerated.\n@@ -69,1 +84,4 @@\n-  void service_concurrent_normal_cycle(GCCause::Cause cause);\n+\n+  \/\/ Returns true if the old generation marking completed (i.e., final mark executed for old generation).\n+  bool resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause);\n+  void service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause, bool reset_old_bitmap_specially);\n@@ -73,1 +91,4 @@\n-  bool try_set_alloc_failure_gc();\n+  \/\/ Return true if setting the flag which indicates allocation failure succeeds.\n+  bool try_set_alloc_failure_gc(bool is_humongous);\n+\n+  \/\/ Notify threads waiting for GC to complete.\n@@ -75,0 +96,2 @@\n+\n+  \/\/ True if allocation failure flag has been set.\n@@ -79,1 +102,0 @@\n-  size_t get_gc_id();\n@@ -87,0 +109,11 @@\n+<<<<<<< HEAD\n+=======\n+  bool is_explicit_gc(GCCause::Cause cause) const;\n+  bool is_implicit_gc(GCCause::Cause cause) const;\n+\n+  \/\/ Returns true if the old generation marking was interrupted to allow a young cycle.\n+  bool preempt_old_marking(ShenandoahGenerationType generation);\n+\n+  void process_phase_timings(const ShenandoahHeap* heap);\n+\n+>>>>>>> 8f4e6e226de7cb08f60bfd8dbbede466463d5b9d\n@@ -101,0 +134,2 @@\n+  \/\/ Return true if the request to start a concurrent GC for the given generation succeeded.\n+  bool request_concurrent_gc(ShenandoahGenerationType generation);\n@@ -107,0 +142,24 @@\n+\n+  void service_concurrent_normal_cycle(ShenandoahHeap* heap,\n+                                       const ShenandoahGenerationType generation,\n+                                       GCCause::Cause cause);\n+\n+  void service_concurrent_old_cycle(ShenandoahHeap* heap,\n+                                    GCCause::Cause &cause);\n+\n+  void set_gc_mode(GCMode new_mode);\n+  GCMode gc_mode() {\n+    return _mode;\n+  }\n+\n+  static ShenandoahGenerationType select_global_generation();\n+\n+ private:\n+  static const char* gc_mode_name(GCMode mode);\n+  void notify_control_thread();\n+\n+  void service_concurrent_cycle(ShenandoahHeap* heap,\n+                                ShenandoahGeneration* generation,\n+                                GCCause::Cause &cause,\n+                                bool do_old_gc_bootstrap);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.hpp","additions":72,"deletions":13,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -35,0 +36,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -37,0 +39,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -41,1 +44,1 @@\n-ShenandoahGCSession::ShenandoahGCSession(GCCause::Cause cause) :\n+ShenandoahGCSession::ShenandoahGCSession(GCCause::Cause cause, ShenandoahGeneration* generation) :\n@@ -43,0 +46,1 @@\n+  _generation(generation),\n@@ -47,0 +51,1 @@\n+<<<<<<< HEAD\n@@ -49,0 +54,4 @@\n+=======\n+  _heap->on_cycle_start(cause, _generation);\n+\n+>>>>>>> 8f4e6e226de7cb08f60bfd8dbbede466463d5b9d\n@@ -53,1 +62,0 @@\n-  _heap->heuristics()->record_cycle_start();\n@@ -68,1 +76,1 @@\n-  _heap->heuristics()->record_cycle_end();\n+  _heap->on_cycle_end(_generation);\n@@ -71,1 +79,1 @@\n-  _tracer->report_gc_reference_stats(_heap->ref_processor()->reference_process_stats());\n+  _tracer->report_gc_reference_stats(_generation->ref_processor()->reference_process_stats());\n@@ -74,1 +82,0 @@\n-  _heap->set_gc_cause(GCCause::_no_gc);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUtils.cpp","additions":12,"deletions":5,"binary":false,"changes":17,"status":"modified"}]}