{"files":[{"patch":"@@ -39,0 +39,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -79,2 +80,0 @@\n-  __ block_comment(\"arraycopy_prologue (shenandoahgc) {\");\n-\n@@ -103,0 +102,1 @@\n+  __ block_comment(\"arraycopy_prologue (shenandoahgc) {\");\n@@ -176,0 +176,10 @@\n+void ShenandoahBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                                       Register dst, Register count,\n+                                                       Register preserve) {\n+  if (ShenandoahCardBarrier && is_reference_type(type)) {\n+    __ block_comment(\"arraycopy_epilogue (shenandoahgc) {\");\n+    gen_write_ref_array_post_barrier(masm, decorators, dst, count, preserve);\n+    __ block_comment(\"} arraycopy_epilogue (shenandoahgc)\");\n+  }\n+}\n+\n@@ -579,0 +589,19 @@\n+void ShenandoahBarrierSetAssembler::store_check(MacroAssembler* masm, Register base, RegisterOrConstant ind_or_offs, Register tmp) {\n+  assert(ShenandoahCardBarrier, \"Should have been checked by caller\");\n+\n+  ShenandoahBarrierSet* ctbs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = ctbs->card_table();\n+  assert_different_registers(base, tmp, R0);\n+\n+  if (ind_or_offs.is_constant()) {\n+    __ add_const_optimized(base, base, ind_or_offs.as_constant(), tmp);\n+  } else {\n+    __ add(base, ind_or_offs.as_register(), base);\n+  }\n+\n+  __ load_const_optimized(tmp, (address)ct->byte_map_base(), R0);\n+  __ srdi(base, base, CardTable::card_shift());\n+  __ li(R0, CardTable::dirty_card_val());\n+  __ stbx(R0, tmp, base);\n+}\n+\n@@ -597,0 +626,5 @@\n+\n+  \/\/ No need for post barrier if storing NULL\n+  if (ShenandoahCardBarrier && is_reference_type(type) && val != noreg) {\n+    store_check(masm, base, ind_or_offs, tmp1);\n+  }\n@@ -746,0 +780,34 @@\n+void ShenandoahBarrierSetAssembler::gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                                                     Register addr, Register count, Register preserve) {\n+  assert(ShenandoahCardBarrier, \"Should have been checked by caller\");\n+\n+  ShenandoahBarrierSet* bs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = bs->card_table();\n+  assert_different_registers(addr, count, R0);\n+\n+  Label L_skip_loop, L_store_loop;\n+\n+  __ sldi_(count, count, LogBytesPerHeapOop);\n+\n+  \/\/ Zero length? Skip.\n+  __ beq(CCR0, L_skip_loop);\n+\n+  __ addi(count, count, -BytesPerHeapOop);\n+  __ add(count, addr, count);\n+  \/\/ Use two shifts to clear out those low order two bits! (Cannot opt. into 1.)\n+  __ srdi(addr, addr, CardTable::card_shift());\n+  __ srdi(count, count, CardTable::card_shift());\n+  __ subf(count, addr, count);\n+  __ add_const_optimized(addr, addr, (address)ct->byte_map_base(), R0);\n+  __ addi(count, count, 1);\n+  __ li(R0, 0);\n+  __ mtctr(count);\n+\n+  \/\/ Byte store loop\n+  __ bind(L_store_loop);\n+  __ stb(R0, 0, addr);\n+  __ addi(addr, addr, 1);\n+  __ bdnz(L_store_loop);\n+  __ bind(L_skip_loop);\n+}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/shenandoahBarrierSetAssembler_ppc.cpp","additions":70,"deletions":2,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -74,0 +74,3 @@\n+  if (heap->mode()->is_generational() && !obj->is_forwarded()) {\n+    msg.append(\"  age: %d\\n\", obj->age());\n+  }\n@@ -427,1 +430,1 @@\n-  ShenandoahMessageBuffer msg(\"Must ba at a Shenandoah safepoint or held %s lock\", lock->name());\n+  ShenandoahMessageBuffer msg(\"Must be at a Shenandoah safepoint or held %s lock\", lock->name());\n@@ -460,1 +463,1 @@\n-  if (ShenandoahSafepoint::is_at_shenandoah_safepoint() && Thread::current()->is_VM_thread()) {\n+  if (ShenandoahSafepoint::is_at_shenandoah_safepoint()) {\n@@ -467,0 +470,45 @@\n+\n+void ShenandoahAsserts::assert_generational(const char* file, int line) {\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  ShenandoahMessageBuffer msg(\"Must be in generational mode\");\n+  report_vm_error(file, line, msg.buffer());\n+}\n+\n+void ShenandoahAsserts::assert_control_or_vm_thread_at_safepoint(bool at_safepoint, const char* file, int line) {\n+  Thread* thr = Thread::current();\n+  if (thr == ShenandoahHeap::heap()->control_thread()) {\n+    return;\n+  }\n+  if (thr->is_VM_thread()) {\n+    if (!at_safepoint) {\n+      return;\n+    } else if (SafepointSynchronize::is_at_safepoint()) {\n+      return;\n+    }\n+  }\n+\n+  ShenandoahMessageBuffer msg(\"Must be either control thread, or vm thread\");\n+  if (at_safepoint) {\n+    msg.append(\" at a safepoint\");\n+  }\n+  report_vm_error(file, line, msg.buffer());\n+}\n+\n+void ShenandoahAsserts::assert_generations_reconciled(const char* file, int line) {\n+  if (!SafepointSynchronize::is_at_safepoint()) {\n+    return;\n+  }\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahGeneration* ggen = heap->gc_generation();\n+  ShenandoahGeneration* agen = heap->active_generation();\n+  if (agen == ggen) {\n+    return;\n+  }\n+\n+  ShenandoahMessageBuffer msg(\"Active(%d) & GC(%d) Generations aren't reconciled\", agen->type(), ggen->type());\n+  report_vm_error(file, line, msg.buffer());\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.cpp","additions":50,"deletions":2,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -75,0 +76,3 @@\n+  static void assert_control_or_vm_thread_at_safepoint(bool at_safepoint, const char* file, int line);\n+  static void assert_generational(const char* file, int line);\n+  static void assert_generations_reconciled(const char* file, int line);\n@@ -166,0 +170,15 @@\n+\n+#define shenandoah_assert_control_or_vm_thread() \\\n+                    ShenandoahAsserts::assert_control_or_vm_thread(false \/* at_safepoint *\/, __FILE__, __LINE__)\n+\n+\/\/ A stronger version of the above that checks that we are at a safepoint if the vm thread\n+#define shenandoah_assert_control_or_vm_thread_at_safepoint()                                                                                                               \\\n+                    ShenandoahAsserts::assert_control_or_vm_thread_at_safepoint(true \/* at_safepoint *\/, __FILE__, __LINE__)\n+\n+#define shenandoah_assert_generational() \\\n+                    ShenandoahAsserts::assert_generational(__FILE__, __LINE__)\n+\n+\/\/ Some limited sanity checking of the _gc_generation and _active_generation fields of ShenandoahHeap\n+#define shenandoah_assert_generations_reconciled()                                                             \\\n+                    ShenandoahAsserts::assert_generations_reconciled(__FILE__, __LINE__)\n+\n@@ -216,0 +235,4 @@\n+#define shenandoah_assert_control_or_vm_thread()\n+#define shenandoah_assert_control_or_vm_thread_at_safepoint()\n+#define shenandoah_assert_generational()\n+#define shenandoah_assert_generations_reconciled()                                                             \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.hpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -56,0 +57,16 @@\n+size_t ShenandoahCollectionSet::get_old_bytes_reserved_for_evacuation() {\n+  return _old_bytes_to_evacuate;\n+}\n+\n+size_t ShenandoahCollectionSet::get_young_bytes_reserved_for_evacuation() {\n+  return _young_bytes_to_evacuate - _young_bytes_to_promote;\n+}\n+\n+size_t ShenandoahCollectionSet::get_young_bytes_to_be_promoted() {\n+  return _young_bytes_to_promote;\n+}\n+\n+size_t ShenandoahCollectionSet::get_old_garbage() {\n+  return _old_garbage;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.inline.hpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -35,0 +36,4 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -89,2 +94,2 @@\n-ShenandoahConcurrentGC::ShenandoahConcurrentGC() :\n-  _mark(),\n+ShenandoahConcurrentGC::ShenandoahConcurrentGC(ShenandoahGeneration* generation, bool do_old_gc_bootstrap) :\n+  _mark(generation),\n@@ -92,1 +97,3 @@\n-  _abbreviated(false) {\n+  _abbreviated(false),\n+  _do_old_gc_bootstrap(do_old_gc_bootstrap),\n+  _generation(generation) {\n@@ -99,4 +106,0 @@\n-void ShenandoahConcurrentGC::cancel() {\n-  ShenandoahConcurrentMark::cancel();\n-}\n-\n@@ -105,0 +108,1 @@\n+\n@@ -115,0 +119,9 @@\n+\n+    \/\/ Reset task queue stats here, rather than in mark_concurrent_roots,\n+    \/\/ because remembered set scan will `push` oops into the queues and\n+    \/\/ resetting after this happens will lose those counts.\n+    TASKQUEUE_STATS_ONLY(_mark.task_queues()->reset_taskqueue_stats());\n+\n+    \/\/ Concurrent remembered set scanning\n+    entry_scan_remembered_set();\n+\n@@ -117,1 +130,1 @@\n-    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_outside_cycle)) {\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_roots)) {\n@@ -135,1 +148,1 @@\n-  if (heap->is_concurrent_mark_in_progress()) {\n+  if (_generation->is_concurrent_mark_in_progress()) {\n@@ -153,1 +166,2 @@\n-  \/\/ the space. This would be the last action if there is nothing to evacuate.\n+  \/\/ the space. This would be the last action if there is nothing to evacuate.  Note that\n+  \/\/ we will not age young-gen objects in the case that we skip evacuation.\n@@ -180,0 +194,1 @@\n+  }\n@@ -181,0 +196,1 @@\n+  if (heap->has_forwarded_objects()) {\n@@ -199,0 +215,4 @@\n+    \/\/ We chose not to evacuate because we found sufficient immediate garbage. Note that we\n+    \/\/ do not check for cancellation here because, at this point, the cycle is effectively\n+    \/\/ complete. If the cycle has been cancelled here, the control thread will detect it\n+    \/\/ on its next iteration and run a degenerated young cycle.\n@@ -203,0 +223,5 @@\n+  \/\/ We defer generation resizing actions until after cset regions have been recycled.  We do this even following an\n+  \/\/ abbreviated cycle.\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahGenerationalHeap::heap()->complete_concurrent_cycle();\n+  }\n@@ -303,1 +328,1 @@\n-  static const char* msg = \"Pause Final Roots\";\n+  const char* msg = final_roots_event_message();\n@@ -312,0 +337,2 @@\n+  heap->try_inject_alloc_failure();\n+\n@@ -313,3 +340,10 @@\n-  static const char* msg = \"Concurrent reset\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset);\n-  EventMark em(\"%s\", msg);\n+  {\n+    const char* msg = conc_reset_event_message();\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset);\n+    EventMark em(\"%s\", msg);\n+\n+    ShenandoahWorkerScope scope(heap->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n+                                msg);\n+    op_reset();\n+  }\n@@ -317,3 +351,7 @@\n-  ShenandoahWorkerScope scope(heap->workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n-                              \"concurrent reset\");\n+  if (_do_old_gc_bootstrap) {\n+    static const char* msg = \"Concurrent reset (OLD)\";\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset_old);\n+    ShenandoahWorkerScope scope(ShenandoahHeap::heap()->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n+                                msg);\n+    EventMark em(\"%s\", msg);\n@@ -321,2 +359,19 @@\n-  heap->try_inject_alloc_failure();\n-  op_reset();\n+    heap->old_generation()->prepare_gc();\n+  }\n+}\n+\n+void ShenandoahConcurrentGC::entry_scan_remembered_set() {\n+  if (_generation->is_young()) {\n+    ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+    TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+    const char* msg = \"Concurrent remembered set scanning\";\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::init_scan_rset);\n+    EventMark em(\"%s\", msg);\n+\n+    ShenandoahWorkerScope scope(heap->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_rs_scanning(),\n+                                msg);\n+\n+    heap->try_inject_alloc_failure();\n+    _generation->scan_remembered_set(true \/* is_concurrent *\/);\n+  }\n@@ -371,1 +426,1 @@\n-  static const char* msg = \"Concurrent weak references\";\n+  const char* msg = conc_weak_refs_event_message();\n@@ -386,1 +441,1 @@\n-  static const char* msg = \"Concurrent weak roots\";\n+  const char* msg = conc_weak_roots_event_message();\n@@ -433,1 +488,1 @@\n-  static const char* msg = \"Concurrent cleanup\";\n+  const char* msg = conc_cleanup_event_message();\n@@ -489,1 +544,1 @@\n-  static const char* msg = \"Concurrent cleanup\";\n+  const char* msg = conc_cleanup_event_message();\n@@ -503,2 +558,1 @@\n-\n-  heap->prepare_gc();\n+  _generation->prepare_gc();\n@@ -517,1 +571,2 @@\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n+      \/\/ reset, so it is very likely we don't need to do another write here.  Since most regions\n+      \/\/ are not \"active\", this path is relatively rare.\n@@ -539,2 +594,2 @@\n-  assert(heap->marking_context()->is_bitmap_clear(), \"need clear marking bitmap\");\n-  assert(!heap->marking_context()->is_complete(), \"should not be complete\");\n+  assert(_generation->is_bitmap_clear(), \"need clear marking bitmap\");\n+  assert(!_generation->is_mark_complete(), \"should not be complete\");\n@@ -543,0 +598,19 @@\n+\n+  if (heap->mode()->is_generational()) {\n+    if (_generation->is_young()) {\n+      \/\/ The current implementation of swap_remembered_set() copies the write-card-table to the read-card-table.\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_swap_rset);\n+      _generation->swap_remembered_set();\n+    }\n+\n+    if (_generation->is_global()) {\n+      heap->old_generation()->cancel_gc();\n+    } else if (heap->is_concurrent_old_mark_in_progress()) {\n+      \/\/ Purge the SATB buffers, transferring any valid, old pointers to the\n+      \/\/ old generation mark queue. Any pointers in a young region will be\n+      \/\/ abandoned.\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_transfer_satb);\n+      heap->old_generation()->transfer_pointers_from_satb();\n+    }\n+  }\n+\n@@ -551,1 +625,1 @@\n-  heap->set_concurrent_mark_in_progress(true);\n+  _generation->set_concurrent_mark_in_progress(true);\n@@ -555,1 +629,3 @@\n-  {\n+  if (_do_old_gc_bootstrap) {\n+    shenandoah_assert_generational();\n+    \/\/ Update region state for both young and old regions\n@@ -559,0 +635,6 @@\n+    heap->old_generation()->ref_processor()->reset_thread_locals();\n+  } else {\n+    \/\/ Update region state for only young regions\n+    ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_region_states);\n+    ShenandoahInitMarkUpdateRegionStateClosure cl;\n+    _generation->parallel_heap_region_iterate(&cl);\n@@ -562,1 +644,1 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = _generation->ref_processor();\n@@ -602,1 +684,4 @@\n-    heap->prepare_regions_and_collection_set(true \/*concurrent*\/);\n+    \/\/ The collection set is chosen by prepare_regions_and_collection_set(). Additionally, certain parameters have been\n+    \/\/ established to govern the evacuation efforts that are about to begin.  Refer to comments on reserve members in\n+    \/\/ ShenandoahGeneration and ShenandoahOldGeneration for more detail.\n+    _generation->prepare_regions_and_collection_set(true \/*concurrent*\/);\n@@ -607,1 +692,11 @@\n-    if (!heap->collection_set()->is_empty()) {\n+    if (!heap->collection_set()->is_empty() || has_in_place_promotions(heap)) {\n+      \/\/ Even if the collection set is empty, we need to do evacuation if there are regions to be promoted in place.\n+      \/\/ Concurrent evacuation takes responsibility for registering objects and setting the remembered set cards to dirty.\n+\n+      LogTarget(Debug, gc, cset) lt;\n+      if (lt.is_enabled()) {\n+        ResourceMark rm;\n+        LogStream ls(lt);\n+        heap->collection_set()->print_on(&ls);\n+      }\n+\n@@ -613,2 +708,5 @@\n-      \/\/ From here on, we need to update references.\n-      heap->set_has_forwarded_objects(true);\n+\n+      \/\/ Generational mode may promote objects in place during the evacuation phase.\n+      \/\/ If that is the only reason we are evacuating, we don't need to update references\n+      \/\/ and there will be no forwarded objects on the heap.\n+      heap->set_has_forwarded_objects(!heap->collection_set()->is_empty());\n@@ -617,2 +715,7 @@\n-      ShenandoahCodeRoots::arm_nmethods_for_evac();\n-      ShenandoahStackWatermark::change_epoch_id();\n+      if (!heap->collection_set()->is_empty()) {\n+        \/\/ Iff objects will be evaluated, arm the nmethod barriers. These will be disarmed\n+        \/\/ under the same condition (established in prepare_concurrent_roots) after strong\n+        \/\/ root evacuation has completed (see op_strong_roots).\n+        ShenandoahCodeRoots::arm_nmethods_for_evac();\n+        ShenandoahStackWatermark::change_epoch_id();\n+      }\n@@ -635,0 +738,5 @@\n+bool ShenandoahConcurrentGC::has_in_place_promotions(ShenandoahHeap* heap) {\n+  return heap->mode()->is_generational() && heap->old_generation()->has_in_place_promotions();\n+}\n+\n+template<bool GENERATIONAL>\n@@ -638,4 +746,1 @@\n-\n-  ShenandoahConcurrentEvacThreadClosure(OopClosure* oops);\n-  void do_thread(Thread* thread);\n-};\n+  explicit ShenandoahConcurrentEvacThreadClosure(OopClosure* oops) : _oops(oops) {}\n@@ -644,8 +749,8 @@\n-ShenandoahConcurrentEvacThreadClosure::ShenandoahConcurrentEvacThreadClosure(OopClosure* oops) :\n-  _oops(oops) {\n-}\n-\n-void ShenandoahConcurrentEvacThreadClosure::do_thread(Thread* thread) {\n-  JavaThread* const jt = JavaThread::cast(thread);\n-  StackWatermarkSet::finish_processing(jt, _oops, StackWatermarkKind::gc);\n-}\n+  void do_thread(Thread* thread) override {\n+    JavaThread* const jt = JavaThread::cast(thread);\n+    StackWatermarkSet::finish_processing(jt, _oops, StackWatermarkKind::gc);\n+    if (GENERATIONAL) {\n+      ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+    }\n+  }\n+};\n@@ -653,0 +758,1 @@\n+template<bool GENERATIONAL>\n@@ -658,1 +764,1 @@\n-  ShenandoahConcurrentEvacUpdateThreadTask(uint n_workers) :\n+  explicit ShenandoahConcurrentEvacUpdateThreadTask(uint n_workers) :\n@@ -663,1 +769,6 @@\n-  void work(uint worker_id) {\n+  void work(uint worker_id) override {\n+    if (GENERATIONAL) {\n+      Thread* worker_thread = Thread::current();\n+      ShenandoahThreadLocalData::enable_plab_promotions(worker_thread);\n+    }\n+\n@@ -667,1 +778,1 @@\n-    ShenandoahConcurrentEvacThreadClosure thr_cl(&oops_cl);\n+    ShenandoahConcurrentEvacThreadClosure<GENERATIONAL> thr_cl(&oops_cl);\n@@ -676,2 +787,7 @@\n-  ShenandoahConcurrentEvacUpdateThreadTask task(heap->workers()->active_workers());\n-  heap->workers()->run_task(&task);\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahConcurrentEvacUpdateThreadTask<true> task(heap->workers()->active_workers());\n+    heap->workers()->run_task(&task);\n+  } else {\n+    ShenandoahConcurrentEvacUpdateThreadTask<false> task(heap->workers()->active_workers());\n+    heap->workers()->run_task(&task);\n+  }\n@@ -688,1 +804,1 @@\n-  heap->ref_processor()->process_references(ShenandoahPhaseTimings::conc_weak_refs, heap->workers(), true \/* concurrent *\/);\n+  _generation->ref_processor()->process_references(ShenandoahPhaseTimings::conc_weak_refs, heap->workers(), true \/* concurrent *\/);\n@@ -715,2 +831,5 @@\n-      \/\/ Note: The obj is dead here. Do not touch it, just clear.\n-      ShenandoahHeap::atomic_clear_oop(p, obj);\n+      shenandoah_assert_generations_reconciled();\n+      if (_heap->is_in_active_generation(obj)) {\n+        \/\/ Note: The obj is dead here. Do not touch it, just clear.\n+        ShenandoahHeap::atomic_clear_oop(p, obj);\n+      }\n@@ -822,0 +941,3 @@\n+  \/\/ We can only toggle concurrent_weak_root_in_progress flag\n+  \/\/ at a safepoint, so that mutators see a consistent\n+  \/\/ value. The flag will be cleared at the next safepoint.\n@@ -918,0 +1040,1 @@\n+  heap->set_update_refs_in_progress(true);\n@@ -921,2 +1044,0 @@\n-\n-  heap->set_update_refs_in_progress(true);\n@@ -967,1 +1088,1 @@\n-    heap->clear_cancelled_gc();\n+    heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n@@ -975,0 +1096,2 @@\n+  \/\/ If we are running in generational mode and this is an aging cycle, this will also age active\n+  \/\/ regions that haven't been used for allocation.\n@@ -980,0 +1103,22 @@\n+  if (heap->mode()->is_generational() && heap->is_concurrent_old_mark_in_progress()) {\n+    \/\/ When the SATB barrier is left on to support concurrent old gen mark, it may pick up writes to\n+    \/\/ objects in the collection set. After those objects are evacuated, the pointers in the\n+    \/\/ SATB are no longer safe. Once we have finished update references, we are guaranteed that\n+    \/\/ no more writes to the collection set are possible.\n+    \/\/\n+    \/\/ This will transfer any old pointers in _active_ regions from the SATB to the old gen\n+    \/\/ mark queues. All other pointers will be discarded. This would also discard any pointers\n+    \/\/ in old regions that were included in a mixed evacuation. We aren't using the SATB filter\n+    \/\/ methods here because we cannot control when they execute. If the SATB filter runs _after_\n+    \/\/ a region has been recycled, we will not be able to detect the bad pointer.\n+    \/\/\n+    \/\/ We are not concerned about skipping this step in abbreviated cycles because regions\n+    \/\/ with no live objects cannot have been written to and so cannot have entries in the SATB\n+    \/\/ buffers.\n+    heap->old_generation()->transfer_pointers_from_satb();\n+\n+    \/\/ Aging_cycle is only relevant during evacuation cycle for individual objects and during final mark for\n+    \/\/ entire regions.  Both of these relevant operations occur before final update refs.\n+    ShenandoahGenerationalHeap::heap()->set_aging_cycle(false);\n+  }\n+\n@@ -992,1 +1137,17 @@\n-  ShenandoahHeap::heap()->set_concurrent_weak_root_in_progress(false);\n+\n+  ShenandoahHeap *heap = ShenandoahHeap::heap();\n+  heap->set_concurrent_weak_root_in_progress(false);\n+  heap->set_evacuation_in_progress(false);\n+\n+  if (heap->mode()->is_generational()) {\n+    \/\/ If the cycle was shortened for having enough immediate garbage, this could be\n+    \/\/ the last GC safepoint before concurrent marking of old resumes. We must be sure\n+    \/\/ that old mark threads don't see any pointers to garbage in the SATB buffers.\n+    if (heap->is_concurrent_old_mark_in_progress()) {\n+      heap->old_generation()->transfer_pointers_from_satb();\n+    }\n+\n+    if (!_generation->is_old()) {\n+      ShenandoahGenerationalHeap::heap()->update_region_ages(_generation->complete_marking_context());\n+    }\n+  }\n@@ -1011,1 +1172,1 @@\n-    return \"Pause Init Mark (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Init Mark\", \" (unload classes)\");\n@@ -1013,1 +1174,1 @@\n-    return \"Pause Init Mark\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Init Mark\", \"\");\n@@ -1019,1 +1180,3 @@\n-  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  assert(!heap->has_forwarded_objects() || heap->is_concurrent_old_mark_in_progress(),\n+         \"Should not have forwarded objects during final mark, unless old gen concurrent mark is running\");\n+\n@@ -1021,1 +1184,1 @@\n-    return \"Pause Final Mark (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Final Mark\", \" (unload classes)\");\n@@ -1023,1 +1186,1 @@\n-    return \"Pause Final Mark\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Final Mark\", \"\");\n@@ -1029,1 +1192,2 @@\n-  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  assert(!heap->has_forwarded_objects() || heap->is_concurrent_old_mark_in_progress(),\n+         \"Should not have forwarded objects concurrent mark, unless old gen concurrent mark is running\");\n@@ -1031,1 +1195,41 @@\n-    return \"Concurrent marking (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent marking\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent marking\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_reset_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent reset\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent reset\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::final_roots_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Final Roots\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Final Roots\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_weak_refs_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent weak references\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent weak references\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_weak_roots_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent weak roots\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent weak roots\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_cleanup_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent cleanup\", \" (unload classes)\");\n@@ -1033,1 +1237,1 @@\n-    return \"Concurrent marking\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent cleanup\", \"\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":273,"deletions":69,"binary":false,"changes":342,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -32,0 +33,2 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n@@ -35,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -40,0 +44,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -45,1 +50,1 @@\n-ShenandoahDegenGC::ShenandoahDegenGC(ShenandoahDegenPoint degen_point) :\n+ShenandoahDegenGC::ShenandoahDegenGC(ShenandoahDegenPoint degen_point, ShenandoahGeneration* generation) :\n@@ -48,0 +53,1 @@\n+  _generation(generation),\n@@ -53,0 +59,7 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  if (heap->mode()->is_generational()) {\n+    bool is_bootstrap_gc = heap->old_generation()->is_bootstrapping();\n+    heap->mmu_tracker()->record_degenerated(GCId::current(), is_bootstrap_gc);\n+    const char* msg = is_bootstrap_gc? \"At end of Degenerated Bootstrap Old GC\": \"At end of Degenerated Young GC\";\n+    heap->log_heap_status(msg);\n+  }\n@@ -68,1 +81,0 @@\n-\n@@ -83,1 +95,20 @@\n-  heap->clear_cancelled_gc();\n+  heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n+\n+#ifdef ASSERT\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahOldGeneration* old_generation = heap->old_generation();\n+    if (!heap->is_concurrent_old_mark_in_progress()) {\n+      \/\/ If we are not marking the old generation, there should be nothing in the old mark queues\n+      assert(old_generation->task_queues()->is_empty(), \"Old gen task queues should be empty\");\n+    }\n+\n+    if (_generation->is_global()) {\n+      \/\/ If we are in a global cycle, the old generation should not be marking. It is, however,\n+      \/\/ allowed to be holding regions for evacuation or coalescing.\n+      assert(old_generation->is_idle()\n+             || old_generation->is_doing_mixed_evacuations()\n+             || old_generation->is_preparing_for_mark(),\n+             \"Old generation cannot be in state: %s\", old_generation->state_name());\n+    }\n+  }\n+#endif\n@@ -99,7 +130,0 @@\n-      \/\/\n-\n-      \/\/ Degenerated from concurrent root mark, reset the flag for STW mark\n-      if (heap->is_concurrent_mark_in_progress()) {\n-        ShenandoahConcurrentMark::cancel();\n-        heap->set_concurrent_mark_in_progress(false);\n-      }\n@@ -109,1 +133,42 @@\n-      heap->set_unload_classes(heap->heuristics()->can_unload_classes());\n+      heap->set_unload_classes(_generation->heuristics()->can_unload_classes() &&\n+                                (!heap->mode()->is_generational() || _generation->is_global()));\n+\n+      if (heap->mode()->is_generational() && _generation->is_young()) {\n+        \/\/ Swap remembered sets for young\n+        _generation->swap_remembered_set();\n+      }\n+\n+    case _degenerated_roots:\n+      \/\/ Degenerated from concurrent root mark, reset the flag for STW mark\n+      if (!heap->mode()->is_generational()) {\n+        if (heap->is_concurrent_mark_in_progress()) {\n+          heap->cancel_concurrent_mark();\n+        }\n+      } else {\n+        if (_generation->is_concurrent_mark_in_progress()) {\n+          \/\/ We want to allow old generation marking to be punctuated by young collections\n+          \/\/ (even if they have degenerated). If this is a global cycle, we'd have cancelled\n+          \/\/ the entire old gc before coming into this switch. Note that cancel_marking on\n+          \/\/ the generation does NOT abandon incomplete SATB buffers as cancel_concurrent_mark does.\n+          \/\/ We need to separate out the old pointers which is done below.\n+          _generation->cancel_marking();\n+        }\n+\n+        if (heap->is_concurrent_mark_in_progress()) {\n+          \/\/ If either old or young marking is in progress, the SATB barrier will be enabled.\n+          \/\/ The SATB buffer may hold a mix of old and young pointers. The old pointers need to be\n+          \/\/ transferred to the old generation mark queues and the young pointers are NOT part\n+          \/\/ of this snapshot, so they must be dropped here. It is safe to drop them here because\n+          \/\/ we will rescan the roots on this safepoint.\n+          heap->old_generation()->transfer_pointers_from_satb();\n+        }\n+\n+        if (_degen_point == ShenandoahDegenPoint::_degenerated_roots) {\n+          \/\/ We only need this if the concurrent cycle has already swapped the card tables.\n+          \/\/ Marking will use the 'read' table, but interesting pointers may have been\n+          \/\/ recorded in the 'write' table in the time between the cancelled concurrent cycle\n+          \/\/ and this degenerated cycle. These pointers need to be included the 'read' table\n+          \/\/ used to scan the remembered set during the STW mark which follows here.\n+          _generation->merge_write_table();\n+        }\n+      }\n@@ -173,1 +238,0 @@\n-\n@@ -192,0 +256,5 @@\n+      \/\/ Update collector state regardless of whether or not there are forwarded objects\n+      heap->set_evacuation_in_progress(false);\n+      heap->set_concurrent_weak_root_in_progress(false);\n+      heap->set_concurrent_strong_root_in_progress(false);\n+\n@@ -213,0 +282,5 @@\n+\n+      if (heap->mode()->is_generational()) {\n+        ShenandoahGenerationalHeap::heap()->complete_degenerated_cycle();\n+      }\n+\n@@ -235,2 +309,2 @@\n-    heap->shenandoah_policy()->record_success_degenerated(_abbreviated);\n-    heap->heuristics()->record_success_degenerated();\n+    heap->shenandoah_policy()->record_success_degenerated(_generation->is_young(), _abbreviated);\n+    _generation->heuristics()->record_success_degenerated();\n@@ -241,1 +315,1 @@\n-  ShenandoahHeap::heap()->prepare_gc();\n+  _generation->prepare_gc();\n@@ -245,1 +319,1 @@\n-  assert(!ShenandoahHeap::heap()->is_concurrent_mark_in_progress(), \"Should be reset\");\n+  assert(!_generation->is_concurrent_mark_in_progress(), \"Should be reset\");\n@@ -247,2 +321,1 @@\n-  ShenandoahSTWMark mark(false \/*full gc*\/);\n-  mark.clear();\n+  ShenandoahSTWMark mark(_generation, false \/*full gc*\/);\n@@ -253,1 +326,1 @@\n-  ShenandoahConcurrentMark mark;\n+  ShenandoahConcurrentMark mark(_generation);\n@@ -265,0 +338,1 @@\n+\n@@ -266,1 +340,1 @@\n-  heap->prepare_regions_and_collection_set(false \/*concurrent*\/);\n+  _generation->prepare_regions_and_collection_set(false \/*concurrent*\/);\n@@ -278,1 +352,4 @@\n-  if (!heap->collection_set()->is_empty()) {\n+  if (!heap->collection_set()->is_empty() || has_in_place_promotions(heap)) {\n+    \/\/ Even if the collection set is empty, we need to do evacuation if there are regions to be promoted in place.\n+    \/\/ Degenerated evacuation takes responsibility for registering objects and setting the remembered set cards to dirty.\n+\n@@ -284,1 +361,1 @@\n-    heap->set_has_forwarded_objects(true);\n+    heap->set_has_forwarded_objects(!heap->collection_set()->is_empty());\n@@ -296,0 +373,4 @@\n+bool ShenandoahDegenGC::has_in_place_promotions(const ShenandoahHeap* heap) const {\n+  return heap->mode()->is_generational() && heap->old_generation()->has_in_place_promotions();\n+}\n+\n@@ -308,4 +389,0 @@\n-  heap->set_evacuation_in_progress(false);\n-  heap->set_concurrent_weak_root_in_progress(false);\n-  heap->set_concurrent_strong_root_in_progress(false);\n-\n@@ -360,1 +437,1 @@\n-      return \"Pause Degenerated GC (<UNSET>)\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (<UNSET>)\");\n@@ -362,1 +439,3 @@\n-      return \"Pause Degenerated GC (Outside of Cycle)\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (Outside of Cycle)\");\n+    case _degenerated_roots:\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (Roots)\");\n@@ -364,1 +443,1 @@\n-      return \"Pause Degenerated GC (Mark)\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (Mark)\");\n@@ -366,1 +445,1 @@\n-      return \"Pause Degenerated GC (Evacuation)\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (Evacuation)\");\n@@ -368,1 +447,1 @@\n-      return \"Pause Degenerated GC (Update Refs)\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (Update Refs)\");\n@@ -371,1 +450,1 @@\n-      return \"ERROR\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (?)\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.cpp","additions":111,"deletions":32,"binary":false,"changes":143,"status":"modified"},{"patch":"@@ -28,0 +28,2 @@\n+#include \"gc\/shenandoah\/shenandoahAffiliation.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n@@ -32,0 +34,3 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -43,0 +48,1 @@\n+    case ShenandoahFreeSetPartitionId::OldCollector: return \"OldCollector\";\n@@ -51,5 +57,8 @@\n-  log_debug(gc)(\"Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"], Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n-                _leftmosts[int(ShenandoahFreeSetPartitionId::Mutator)],\n-                _rightmosts[int(ShenandoahFreeSetPartitionId::Mutator)],\n-                _leftmosts[int(ShenandoahFreeSetPartitionId::Collector)],\n-                _rightmosts[int(ShenandoahFreeSetPartitionId::Collector)]);\n+  log_debug(gc)(\"Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"], Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT\n+               \"], Old Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+               _leftmosts[int(ShenandoahFreeSetPartitionId::Mutator)],\n+               _rightmosts[int(ShenandoahFreeSetPartitionId::Mutator)],\n+               _leftmosts[int(ShenandoahFreeSetPartitionId::Collector)],\n+               _rightmosts[int(ShenandoahFreeSetPartitionId::Collector)],\n+               _leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+               _rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)]);\n@@ -57,7 +66,10 @@\n-                \"], Empty Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n-                _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n-                _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n-                _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)],\n-                _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)]);\n-\n-  log_debug(gc)(\"%6s: %18s %18s %18s\", \"index\", \"Mutator Bits\", \"Collector Bits\", \"NotFree Bits\");\n+               \"], Empty Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT\n+               \"], Empty Old Collecto range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+               _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n+               _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n+               _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)],\n+               _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)],\n+               _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+               _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)]);\n+\n+  log_debug(gc)(\"%6s: %18s %18s %18s %18s\", \"index\", \"Mutator Bits\", \"Collector Bits\", \"Old Collector Bits\", \"NotFree Bits\");\n@@ -84,1 +96,2 @@\n-  uintx free_bits = mutator_bits | collector_bits;\n+  uintx old_collector_bits = _membership[int(ShenandoahFreeSetPartitionId::OldCollector)].bits_at(aligned_idx);\n+  uintx free_bits = mutator_bits | collector_bits | old_collector_bits;\n@@ -86,2 +99,2 @@\n-  log_debug(gc)(SSIZE_FORMAT_W(6) \": \" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0,\n-                aligned_idx, mutator_bits, collector_bits, notfree_bits);\n+  log_debug(gc)(SSIZE_FORMAT_W(6) \": \" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0,\n+               aligned_idx, mutator_bits, collector_bits, old_collector_bits, notfree_bits);\n@@ -95,1 +108,1 @@\n-    _membership{ ShenandoahSimpleBitMap(max_regions), ShenandoahSimpleBitMap(max_regions) }\n+    _membership{ ShenandoahSimpleBitMap(max_regions), ShenandoahSimpleBitMap(max_regions) , ShenandoahSimpleBitMap(max_regions) }\n@@ -165,1 +178,0 @@\n-  _region_counts[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_region_count;\n@@ -185,0 +197,14 @@\n+void ShenandoahRegionPartitions::establish_old_collector_intervals(idx_t old_collector_leftmost, idx_t old_collector_rightmost,\n+                                                                   idx_t old_collector_leftmost_empty,\n+                                                                   idx_t old_collector_rightmost_empty,\n+                                                                   size_t old_collector_region_count, size_t old_collector_used) {\n+  _leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_leftmost;\n+  _rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_rightmost;\n+  _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_leftmost_empty;\n+  _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_rightmost_empty;\n+\n+  _region_counts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_region_count;\n+  _used[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_used;\n+  _capacity[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_region_count * _region_size_bytes;\n+}\n+\n@@ -205,1 +231,1 @@\n-      _leftmosts_empty[int(partition)] = leftmost(partition);\n+      _leftmosts_empty[int(partition)] = _leftmosts[int(partition)];\n@@ -292,1 +318,0 @@\n-\n@@ -296,0 +321,17 @@\n+bool ShenandoahRegionPartitions::is_mutator_partition(ShenandoahFreeSetPartitionId p) {\n+  return (p == ShenandoahFreeSetPartitionId::Mutator);\n+}\n+\n+bool ShenandoahRegionPartitions::is_young_collector_partition(ShenandoahFreeSetPartitionId p) {\n+  return (p == ShenandoahFreeSetPartitionId::Collector);\n+}\n+\n+bool ShenandoahRegionPartitions::is_old_collector_partition(ShenandoahFreeSetPartitionId p) {\n+  return (p == ShenandoahFreeSetPartitionId::OldCollector);\n+}\n+\n+bool ShenandoahRegionPartitions::available_implies_empty(size_t available_in_region) {\n+  return (available_in_region == _region_size_bytes);\n+}\n+\n+\n@@ -298,0 +340,1 @@\n+  ShenandoahHeapRegion* r = ShenandoahHeap::heap()->get_region(idx);\n@@ -302,0 +345,5 @@\n+  assert (_membership[int(orig_partition)].is_set(idx), \"Cannot move from partition unless in partition\");\n+  assert ((r != nullptr) && ((r->is_trash() && (available == _region_size_bytes)) ||\n+                             (r->used() + available == _region_size_bytes)),\n+          \"Used: \" SIZE_FORMAT \" + available: \" SIZE_FORMAT \" should equal region size: \" SIZE_FORMAT,\n+          ShenandoahHeap::heap()->get_region(idx)->used(), available, _region_size_bytes);\n@@ -305,0 +353,2 @@\n+  \/\/                          Mutator empty => Collector\n+  \/\/                          Mutator empty => OldCollector\n@@ -306,0 +356,1 @@\n+  \/\/                          Mutator empty => OldCollector\n@@ -307,8 +358,10 @@\n-  assert (((available <= _region_size_bytes) &&\n-           (((orig_partition == ShenandoahFreeSetPartitionId::Mutator)\n-             && (new_partition == ShenandoahFreeSetPartitionId::Collector)) ||\n-            ((orig_partition == ShenandoahFreeSetPartitionId::Collector)\n-             && (new_partition == ShenandoahFreeSetPartitionId::Mutator)))) ||\n-          ((available == _region_size_bytes) &&\n-           ((orig_partition == ShenandoahFreeSetPartitionId::Mutator)\n-            && (new_partition == ShenandoahFreeSetPartitionId::Collector))), \"Unexpected movement between partitions\");\n+  \/\/                          OldCollector Empty => Mutator\n+  assert ((is_mutator_partition(orig_partition) && is_young_collector_partition(new_partition)) ||\n+          (is_mutator_partition(orig_partition) &&\n+           available_implies_empty(available) && is_old_collector_partition(new_partition)) ||\n+          (is_young_collector_partition(orig_partition) && is_mutator_partition(new_partition)) ||\n+          (is_old_collector_partition(orig_partition)\n+           && available_implies_empty(available) && is_mutator_partition(new_partition)),\n+          \"Unexpected movement between partitions, available: \" SIZE_FORMAT \", _region_size_bytes: \" SIZE_FORMAT\n+          \", orig_partition: %s, new_partition: %s\",\n+          available, _region_size_bytes, partition_name(orig_partition), partition_name(new_partition));\n@@ -317,0 +370,3 @@\n+  assert (_used[int(orig_partition)] >= used,\n+          \"Orig partition used: \" SIZE_FORMAT \" must exceed moved used: \" SIZE_FORMAT \" within region \" SSIZE_FORMAT,\n+          _used[int(orig_partition)], used, idx);\n@@ -485,0 +541,1 @@\n+      case ShenandoahFreeSetPartitionId::OldCollector:\n@@ -574,0 +631,35 @@\n+\n+  \/\/ Performance invariants. Failing these would not break the free partition, but performance would suffer.\n+  assert (leftmost(ShenandoahFreeSetPartitionId::OldCollector) <= _max, \"leftmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT,\n+          leftmost(ShenandoahFreeSetPartitionId::OldCollector),  _max);\n+  assert (rightmost(ShenandoahFreeSetPartitionId::OldCollector) < _max, \"rightmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT,\n+          rightmost(ShenandoahFreeSetPartitionId::OldCollector),  _max);\n+\n+  assert (leftmost(ShenandoahFreeSetPartitionId::OldCollector) == _max\n+          || partition_id_matches(leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                                  ShenandoahFreeSetPartitionId::OldCollector),\n+          \"leftmost region should be free: \" SSIZE_FORMAT,  leftmost(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (leftmost(ShenandoahFreeSetPartitionId::OldCollector) == _max\n+          || partition_id_matches(rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                                  ShenandoahFreeSetPartitionId::OldCollector),\n+          \"rightmost region should be free: \" SSIZE_FORMAT, rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+\n+  \/\/ If OldCollector partition is empty, leftmosts will both equal max, rightmosts will both equal zero.\n+  \/\/ Likewise for empty region partitions.\n+  beg_off = leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  end_off = rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  assert (beg_off >= leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+          \"free regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (end_off <= rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n+          \"free regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+\n+  beg_off = empty_leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  end_off = empty_rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  assert (beg_off >= _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+          \"free empty regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (end_off <= _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+          \"free empty regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n@@ -581,1 +673,0 @@\n-  _right_to_left_bias(false),\n@@ -587,0 +678,52 @@\n+void ShenandoahFreeSet::add_promoted_in_place_region_to_old_collector(ShenandoahHeapRegion* region) {\n+  shenandoah_assert_heaplocked();\n+  size_t plab_min_size_in_bytes = ShenandoahGenerationalHeap::heap()->plab_min_size() * HeapWordSize;\n+  size_t idx = region->index();\n+  size_t capacity = alloc_capacity(region);\n+  assert(_partitions.membership(idx) == ShenandoahFreeSetPartitionId::NotFree,\n+         \"Regions promoted in place should have been excluded from Mutator partition\");\n+  if (capacity >= plab_min_size_in_bytes) {\n+    _partitions.make_free(idx, ShenandoahFreeSetPartitionId::OldCollector, capacity);\n+    _heap->old_generation()->augment_promoted_reserve(capacity);\n+  }\n+}\n+\n+HeapWord* ShenandoahFreeSet::allocate_from_partition_with_affiliation(ShenandoahFreeSetPartitionId which_partition,\n+                                                                      ShenandoahAffiliation affiliation,\n+                                                                      ShenandoahAllocRequest& req, bool& in_new_region) {\n+  shenandoah_assert_heaplocked();\n+  idx_t rightmost_collector = ((affiliation == ShenandoahAffiliation::FREE)?\n+                               _partitions.rightmost_empty(which_partition): _partitions.rightmost(which_partition));\n+  idx_t leftmost_collector = ((affiliation == ShenandoahAffiliation::FREE)?\n+                              _partitions.leftmost_empty(which_partition): _partitions.leftmost(which_partition));\n+  if (_partitions.alloc_from_left_bias(which_partition)) {\n+    for (idx_t idx = leftmost_collector; idx <= rightmost_collector; ) {\n+      assert(_partitions.in_free_set(which_partition, idx), \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n+      ShenandoahHeapRegion* r = _heap->get_region(idx);\n+      if (r->affiliation() == affiliation) {\n+        HeapWord* result = try_allocate_in(r, req, in_new_region);\n+        if (result != nullptr) {\n+          return result;\n+        }\n+      }\n+      idx = _partitions.find_index_of_next_available_region(which_partition, idx + 1);\n+    }\n+  } else {\n+    for (idx_t idx = rightmost_collector; idx >= leftmost_collector; ) {\n+      assert(_partitions.in_free_set(which_partition, idx),\n+             \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n+      ShenandoahHeapRegion* r = _heap->get_region(idx);\n+      if (r->affiliation() == affiliation) {\n+        HeapWord* result = try_allocate_in(r, req, in_new_region);\n+        if (result != nullptr) {\n+          return result;\n+        }\n+      }\n+      idx = _partitions.find_index_of_previous_available_region(which_partition, idx - 1);\n+    }\n+  }\n+  log_debug(gc, free)(\"Could not allocate collector region with affiliation: %s for request \" PTR_FORMAT,\n+                      shenandoah_affiliation_name(affiliation), p2i(&req));\n+  return nullptr;\n+}\n+\n@@ -592,1 +735,2 @@\n-  \/\/ Leftmost and rightmost bounds provide enough caching to quickly find a region from which to allocate.\n+  \/\/ Leftmost and rightmost bounds provide enough caching to walk bitmap efficiently. Normally,\n+  \/\/ we would find the region to allocate at right away.\n@@ -598,3 +742,20 @@\n-  \/\/ Free set maintains mutator and collector partitions.  Mutator can only allocate from the\n-  \/\/ Mutator partition.  Collector prefers to allocate from the Collector partition, but may steal\n-  \/\/ regions from the Mutator partition if the Collector partition has been depleted.\n+  \/\/ Free set maintains mutator and collector partitions.  Normally, each allocates only from its partition,\n+  \/\/ except in special cases when the collector steals regions from the mutator partition.\n+\n+  \/\/ Overwrite with non-zero (non-NULL) values only if necessary for allocation bookkeeping.\n+  bool allow_new_region = true;\n+  if (_heap->mode()->is_generational()) {\n+    switch (req.affiliation()) {\n+      case ShenandoahAffiliation::OLD_GENERATION:\n+        \/\/ Note: unsigned result from free_unaffiliated_regions() will never be less than zero, but it may equal zero.\n+        if (_heap->old_generation()->free_unaffiliated_regions() <= 0) {\n+          allow_new_region = false;\n+        }\n+        break;\n+\n+      case ShenandoahAffiliation::YOUNG_GENERATION:\n+        \/\/ Note: unsigned result from free_unaffiliated_regions() will never be less than zero, but it may equal zero.\n+        if (_heap->young_generation()->free_unaffiliated_regions() <= 0) {\n+          allow_new_region = false;\n+        }\n+        break;\n@@ -602,0 +763,8 @@\n+      case ShenandoahAffiliation::FREE:\n+        fatal(\"Should request affiliation\");\n+\n+      default:\n+        ShouldNotReachHere();\n+        break;\n+    }\n+  }\n@@ -626,1 +795,1 @@\n-        _right_to_left_bias = (non_empty_on_right > non_empty_on_left);\n+        _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::Mutator, (non_empty_on_right < non_empty_on_left));\n@@ -629,1 +798,1 @@\n-      if (_right_to_left_bias) {\n+      if (!_partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::Mutator)) {\n@@ -674,0 +843,4 @@\n+    case ShenandoahAllocRequest::_alloc_plab: {\n+      \/\/ PLABs always reside in old-gen and are only allocated during\n+      \/\/ evacuation phase.\n+\n@@ -676,5 +849,11 @@\n-      idx_t leftmost_collector = _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector);\n-      for (idx_t idx = _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector); idx >= leftmost_collector; ) {\n-        assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Collector, idx),\n-               \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n-        HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n+      HeapWord* result;\n+      result = allocate_from_partition_with_affiliation(req.is_old()? ShenandoahFreeSetPartitionId::OldCollector:\n+                                                        ShenandoahFreeSetPartitionId::Collector,\n+                                                        req.affiliation(), req, in_new_region);\n+      if (result != nullptr) {\n+        return result;\n+      } else if (allow_new_region) {\n+        \/\/ Try a free region that is dedicated to GC allocations.\n+        result = allocate_from_partition_with_affiliation(req.is_old()? ShenandoahFreeSetPartitionId::OldCollector:\n+                                                          ShenandoahFreeSetPartitionId::Collector,\n+                                                          ShenandoahAffiliation::FREE, req, in_new_region);\n@@ -684,1 +863,0 @@\n-        idx = _partitions.find_index_of_previous_available_region(ShenandoahFreeSetPartitionId::Collector, idx - 1);\n@@ -691,0 +869,4 @@\n+      if (!allow_new_region && req.is_old() && (_heap->young_generation()->free_unaffiliated_regions() > 0)) {\n+        \/\/ This allows us to flip a mutator region to old_collector\n+        allow_new_region = true;\n+      }\n@@ -692,12 +874,23 @@\n-      \/\/ Try to steal an empty region from the mutator view.\n-      idx_t leftmost_mutator_empty = _partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n-      for (idx_t idx = _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Mutator); idx >= leftmost_mutator_empty; ) {\n-        assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx),\n-               \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n-        ShenandoahHeapRegion* r = _heap->get_region(idx);\n-        if (can_allocate_from(r)) {\n-          flip_to_gc(r);\n-          HeapWord *result = try_allocate_in(r, req, in_new_region);\n-          if (result != nullptr) {\n-            log_debug(gc)(\"Flipped region \" SIZE_FORMAT \" to gc for request: \" PTR_FORMAT, idx, p2i(&req));\n-            return result;\n+      \/\/ We should expand old-gen if this can prevent an old-gen evacuation failure.  We don't care so much about\n+      \/\/ promotion failures since they can be mitigated in a subsequent GC pass.  Would be nice to know if this\n+      \/\/ allocation request is for evacuation or promotion.  Individual threads limit their use of PLAB memory for\n+      \/\/ promotions, so we already have an assurance that any additional memory set aside for old-gen will be used\n+      \/\/ only for old-gen evacuations.\n+      if (allow_new_region) {\n+        \/\/ Try to steal an empty region from the mutator view.\n+        idx_t rightmost_mutator = _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n+        idx_t leftmost_mutator =  _partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n+        for (idx_t idx = rightmost_mutator; idx >= leftmost_mutator; ) {\n+          assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx),\n+                 \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n+          ShenandoahHeapRegion* r = _heap->get_region(idx);\n+          if (can_allocate_from(r)) {\n+            if (req.is_old()) {\n+              flip_to_old_gc(r);\n+            } else {\n+              flip_to_gc(r);\n+            }\n+            \/\/ Region r is entirely empty.  If try_allocat_in fails on region r, something else is really wrong.\n+            \/\/ Don't bother to retry with other regions.\n+            log_debug(gc, free)(\"Flipped region \" SIZE_FORMAT \" to gc for request: \" PTR_FORMAT, idx, p2i(&req));\n+            return try_allocate_in(r, req, in_new_region);\n@@ -705,0 +898,1 @@\n+          idx = _partitions.find_index_of_previous_available_region(ShenandoahFreeSetPartitionId::Mutator, idx - 1);\n@@ -706,2 +900,0 @@\n-        idx = _partitions.find_index_of_previous_available_region(ShenandoahFreeSetPartitionId::Mutator, idx - 1);\n-\n@@ -713,0 +905,1 @@\n+    }\n@@ -719,0 +912,50 @@\n+\/\/ This work method takes an argument corresponding to the number of bytes\n+\/\/ free in a region, and returns the largest amount in heapwords that can be allocated\n+\/\/ such that both of the following conditions are satisfied:\n+\/\/\n+\/\/ 1. it is a multiple of card size\n+\/\/ 2. any remaining shard may be filled with a filler object\n+\/\/\n+\/\/ The idea is that the allocation starts and ends at card boundaries. Because\n+\/\/ a region ('s end) is card-aligned, the remainder shard that must be filled is\n+\/\/ at the start of the free space.\n+\/\/\n+\/\/ This is merely a helper method to use for the purpose of such a calculation.\n+size_t ShenandoahFreeSet::get_usable_free_words(size_t free_bytes) const {\n+  \/\/ e.g. card_size is 512, card_shift is 9, min_fill_size() is 8\n+  \/\/      free is 514\n+  \/\/      usable_free is 512, which is decreased to 0\n+  size_t usable_free = (free_bytes \/ CardTable::card_size()) << CardTable::card_shift();\n+  assert(usable_free <= free_bytes, \"Sanity check\");\n+  if ((free_bytes != usable_free) && (free_bytes - usable_free < ShenandoahHeap::min_fill_size() * HeapWordSize)) {\n+    \/\/ After aligning to card multiples, the remainder would be smaller than\n+    \/\/ the minimum filler object, so we'll need to take away another card's\n+    \/\/ worth to construct a filler object.\n+    if (usable_free >= CardTable::card_size()) {\n+      usable_free -= CardTable::card_size();\n+    } else {\n+      assert(usable_free == 0, \"usable_free is a multiple of card_size and card_size > min_fill_size\");\n+    }\n+  }\n+\n+  return usable_free \/ HeapWordSize;\n+}\n+\n+\/\/ Given a size argument, which is a multiple of card size, a request struct\n+\/\/ for a PLAB, and an old region, return a pointer to the allocated space for\n+\/\/ a PLAB which is card-aligned and where any remaining shard in the region\n+\/\/ has been suitably filled by a filler object.\n+\/\/ It is assumed (and assertion-checked) that such an allocation is always possible.\n+HeapWord* ShenandoahFreeSet::allocate_aligned_plab(size_t size, ShenandoahAllocRequest& req, ShenandoahHeapRegion* r) {\n+  assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+  assert(r->is_old(), \"All PLABs reside in old-gen\");\n+  assert(!req.is_mutator_alloc(), \"PLABs should not be allocated by mutators.\");\n+  assert(is_aligned(size, CardTable::card_size_in_words()), \"Align by design\");\n+\n+  HeapWord* result = r->allocate_aligned(size, req, CardTable::card_size());\n+  assert(result != nullptr, \"Allocation cannot fail\");\n+  assert(r->top() <= r->end(), \"Allocation cannot span end of region\");\n+  assert(is_aligned(result, CardTable::card_size_in_words()), \"Align by design\");\n+  return result;\n+}\n+\n@@ -724,1 +967,0 @@\n-\n@@ -732,0 +974,28 @@\n+    assert(!r->is_affiliated(), \"New region \" SIZE_FORMAT \" should be unaffiliated\", r->index());\n+    r->set_affiliation(req.affiliation());\n+    if (r->is_old()) {\n+      \/\/ Any OLD region allocated during concurrent coalesce-and-fill does not need to be coalesced and filled because\n+      \/\/ all objects allocated within this region are above TAMS (and thus are implicitly marked).  In case this is an\n+      \/\/ OLD region and concurrent preparation for mixed evacuations visits this region before the start of the next\n+      \/\/ old-gen concurrent mark (i.e. this region is allocated following the start of old-gen concurrent mark but before\n+      \/\/ concurrent preparations for mixed evacuations are completed), we mark this region as not requiring any\n+      \/\/ coalesce-and-fill processing.\n+      r->end_preemptible_coalesce_and_fill();\n+      _heap->old_generation()->clear_cards_for(r);\n+    }\n+    _heap->generation_for(r->affiliation())->increment_affiliated_region_count();\n+\n+#ifdef ASSERT\n+    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+    assert(ctx->top_at_mark_start(r) == r->bottom(), \"Newly established allocation region starts with TAMS equal to bottom\");\n+    assert(ctx->is_bitmap_clear_range(ctx->top_bitmap(r), r->end()), \"Bitmap above top_bitmap() must be clear\");\n+#endif\n+    log_debug(gc)(\"Using new region (\" SIZE_FORMAT \") for %s (\" PTR_FORMAT \").\",\n+                       r->index(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(&req));\n+  } else {\n+    assert(r->is_affiliated(), \"Region \" SIZE_FORMAT \" that is not new should be affiliated\", r->index());\n+    if (r->affiliation() != req.affiliation()) {\n+      assert(_heap->mode()->is_generational(), \"Request for %s from %s region should only happen in generational mode.\",\n+             req.affiliation_name(), r->affiliation_name());\n+      return nullptr;\n+    }\n@@ -736,13 +1006,23 @@\n-    \/\/ This is a GCLAB or a TLAB allocation\n-    size_t free = align_down(r->free() >> LogHeapWordSize, MinObjAlignment);\n-    if (adjusted_size > free) {\n-      adjusted_size = free;\n-    }\n-    if (adjusted_size >= req.min_size()) {\n-      result = r->allocate(adjusted_size, req.type());\n-      log_debug(gc)(\"Allocated \" SIZE_FORMAT \" words (adjusted from \" SIZE_FORMAT \") for %s @\" PTR_FORMAT\n-                          \" from %s region \" SIZE_FORMAT \", free bytes remaining: \" SIZE_FORMAT,\n-                          adjusted_size, req.size(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(result),\n-                          _partitions.partition_membership_name(r->index()), r->index(), r->free());\n-      assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, adjusted_size);\n-      req.set_actual_size(adjusted_size);\n+    size_t free = r->free();    \/\/ free represents bytes available within region r\n+    if (req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+      \/\/ This is a PLAB allocation\n+      assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+      assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, r->index()),\n+             \"PLABS must be allocated in old_collector_free regions\");\n+\n+      \/\/ Need to assure that plabs are aligned on multiple of card region\n+      \/\/ Convert free from unaligned bytes to aligned number of words\n+      size_t usable_free = get_usable_free_words(free);\n+      if (adjusted_size > usable_free) {\n+        adjusted_size = usable_free;\n+      }\n+      adjusted_size = align_down(adjusted_size, CardTable::card_size_in_words());\n+      if (adjusted_size >= req.min_size()) {\n+        result = allocate_aligned_plab(adjusted_size, req, r);\n+        assert(result != nullptr, \"allocate must succeed\");\n+        req.set_actual_size(adjusted_size);\n+      } else {\n+        \/\/ Otherwise, leave result == nullptr because the adjusted size is smaller than min size.\n+        log_trace(gc, free)(\"Failed to shrink PLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n+                            \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n+      }\n@@ -751,2 +1031,14 @@\n-      log_trace(gc, free)(\"Failed to shrink TLAB or GCLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n-                          \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n+      \/\/ This is a GCLAB or a TLAB allocation\n+      \/\/ Convert free from unaligned bytes to aligned number of words\n+      free = align_down(free >> LogHeapWordSize, MinObjAlignment);\n+      if (adjusted_size > free) {\n+        adjusted_size = free;\n+      }\n+      if (adjusted_size >= req.min_size()) {\n+        result = r->allocate(adjusted_size, req);\n+        assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, adjusted_size);\n+        req.set_actual_size(adjusted_size);\n+      } else {\n+        log_trace(gc, free)(\"Failed to shrink TLAB or GCLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n+                            \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n+      }\n@@ -756,1 +1048,1 @@\n-    result = r->allocate(size, req.type());\n+    result = r->allocate(size, req);\n@@ -759,4 +1051,0 @@\n-      log_debug(gc)(\"Allocated \" SIZE_FORMAT \" words for %s @\" PTR_FORMAT\n-                          \" from %s region \" SIZE_FORMAT \", free bytes remaining: \" SIZE_FORMAT,\n-                          size, ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(result),\n-                          _partitions.partition_membership_name(r->index()),  r->index(), r->free());\n@@ -770,0 +1058,1 @@\n+      assert(req.is_young(), \"Mutator allocations always come from young generation.\");\n@@ -775,1 +1064,2 @@\n-      \/\/ evacuation are not updated during evacuation.\n+      \/\/ evacuation are not updated during evacuation.  For both young and old regions r, it is essential that all\n+      \/\/ PLABs be made parsable at the end of evacuation.  This is enabled by retiring all plabs at end of evacuation.\n@@ -777,0 +1067,7 @@\n+      if (r->is_old()) {\n+        _partitions.increase_used(ShenandoahFreeSetPartitionId::OldCollector, req.actual_size() * HeapWordSize);\n+        assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"old-gen allocations use PLAB or shared allocation\");\n+        \/\/ for plabs, we'll sort the difference between evac and promotion usage when we retire the plab\n+      } else {\n+        _partitions.increase_used(ShenandoahFreeSetPartitionId::Collector, req.actual_size() * HeapWordSize);\n+      }\n@@ -791,3 +1088,16 @@\n-    _partitions.retire_from_partition(req.is_mutator_alloc()?\n-                                      ShenandoahFreeSetPartitionId::Mutator: ShenandoahFreeSetPartitionId::Collector,\n-                                      idx, r->used());\n+    ShenandoahFreeSetPartitionId orig_partition;\n+    if (req.is_mutator_alloc()) {\n+      orig_partition = ShenandoahFreeSetPartitionId::Mutator;\n+    } else if (req.type() == ShenandoahAllocRequest::_alloc_gclab) {\n+      orig_partition = ShenandoahFreeSetPartitionId::Collector;\n+    } else if (req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+      orig_partition = ShenandoahFreeSetPartitionId::OldCollector;\n+    } else {\n+      assert(req.type() == ShenandoahAllocRequest::_alloc_shared_gc, \"Unexpected allocation type\");\n+      if (req.is_old()) {\n+        orig_partition = ShenandoahFreeSetPartitionId::OldCollector;\n+      } else {\n+        orig_partition = ShenandoahFreeSetPartitionId::Collector;\n+      }\n+    }\n+    _partitions.retire_from_partition(orig_partition, idx, r->used());\n@@ -806,0 +1116,3 @@\n+  assert(req.is_young(), \"Humongous regions always allocated in YOUNG\");\n+  ShenandoahGeneration* generation = _heap->generation_for(req.affiliation());\n+\n@@ -818,1 +1131,1 @@\n-                                                                            start_range, num);\n+                                                                          start_range, num);\n@@ -885,0 +1198,2 @@\n+    r->set_affiliation(req.affiliation());\n+    r->set_update_watermark(r->bottom());\n@@ -887,1 +1202,1 @@\n-\n+  generation->increase_affiliated_region_count(num);\n@@ -900,0 +1215,3 @@\n+  if (remainder != 0) {\n+    req.set_waste(ShenandoahHeapRegion::region_size_words() - remainder);\n+  }\n@@ -905,1 +1223,0 @@\n-    _heap->decrease_used(r->used());\n@@ -935,0 +1252,21 @@\n+void ShenandoahFreeSet::flip_to_old_gc(ShenandoahHeapRegion* r) {\n+  size_t idx = r->index();\n+\n+  assert(_partitions.partition_id_matches(idx, ShenandoahFreeSetPartitionId::Mutator), \"Should be in mutator view\");\n+  assert(can_allocate_from(r), \"Should not be allocated\");\n+\n+  ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+  size_t region_capacity = alloc_capacity(r);\n+  _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                               ShenandoahFreeSetPartitionId::OldCollector, region_capacity);\n+  _partitions.assert_bounds();\n+  _heap->old_generation()->augment_evacuation_reserve(region_capacity);\n+  bool transferred = gen_heap->generation_sizer()->transfer_to_old(1);\n+  if (!transferred) {\n+    log_warning(gc, free)(\"Forcing transfer of \" SIZE_FORMAT \" to old reserve.\", idx);\n+    gen_heap->generation_sizer()->force_transfer_to_old(1);\n+  }\n+  \/\/ We do not ensure that the region is no longer trash, relying on try_allocate_in(), which always comes next,\n+  \/\/ to recycle trash before attempting to allocate anything in the region.\n+}\n+\n@@ -957,2 +1295,5 @@\n-}\n-void ShenandoahFreeSet::find_regions_with_alloc_capacity(size_t &cset_regions) {\n+  _alloc_bias_weight = 0;\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::Mutator, true);\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::Collector, false);\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::OldCollector, false);\n+}\n@@ -961,1 +1302,3 @@\n-  cset_regions = 0;\n+void ShenandoahFreeSet::find_regions_with_alloc_capacity(size_t &young_cset_regions, size_t &old_cset_regions,\n+                                                         size_t &first_old_region, size_t &last_old_region,\n+                                                         size_t &old_region_count) {\n@@ -963,0 +1306,7 @@\n+\n+  first_old_region = _heap->num_regions();\n+  last_old_region = 0;\n+  old_region_count = 0;\n+  old_cset_regions = 0;\n+  young_cset_regions = 0;\n+\n@@ -970,1 +1320,0 @@\n-\n@@ -974,0 +1323,7 @@\n+  size_t old_collector_leftmost = max_regions;\n+  size_t old_collector_rightmost = 0;\n+  size_t old_collector_leftmost_empty = max_regions;\n+  size_t old_collector_rightmost_empty = 0;\n+  size_t old_collector_regions = 0;\n+  size_t old_collector_used = 0;\n+\n@@ -979,1 +1335,13 @@\n-      cset_regions++;\n+      if (region->is_old()) {\n+        old_cset_regions++;\n+      } else {\n+        assert(region->is_young(), \"Trashed region should be old or young\");\n+        young_cset_regions++;\n+      }\n+    } else if (region->is_old()) {\n+      \/\/ count both humongous and regular regions, but don't count trash (cset) regions.\n+      old_region_count++;\n+      if (first_old_region > idx) {\n+        first_old_region = idx;\n+      }\n+      last_old_region = idx;\n@@ -982,0 +1350,1 @@\n+      assert(!region->is_cset(), \"Shouldn't be adding cset regions to the free set\");\n@@ -986,11 +1355,24 @@\n-        _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::Mutator);\n-\n-        if (idx < mutator_leftmost) {\n-          mutator_leftmost = idx;\n-        }\n-        if (idx > mutator_rightmost) {\n-          mutator_rightmost = idx;\n-        }\n-        if (ac == region_size_bytes) {\n-          if (idx < mutator_leftmost_empty) {\n-            mutator_leftmost_empty = idx;\n+        if (region->is_trash() || !region->is_old()) {\n+          \/\/ Both young and old collected regions (trashed) are placed into the Mutator set\n+          _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::Mutator);\n+          if (idx < mutator_leftmost) {\n+            mutator_leftmost = idx;\n+          }\n+          if (idx > mutator_rightmost) {\n+            mutator_rightmost = idx;\n+          }\n+          if (ac == region_size_bytes) {\n+            if (idx < mutator_leftmost_empty) {\n+              mutator_leftmost_empty = idx;\n+            }\n+            if (idx > mutator_rightmost_empty) {\n+              mutator_rightmost_empty = idx;\n+            }\n+          }\n+          mutator_regions++;\n+          mutator_used += (region_size_bytes - ac);\n+        } else {\n+          \/\/ !region->is_trash() && region is_old()\n+          _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::OldCollector);\n+          if (idx < old_collector_leftmost) {\n+            old_collector_leftmost = idx;\n@@ -998,2 +1380,2 @@\n-          if (idx > mutator_rightmost_empty) {\n-            mutator_rightmost_empty = idx;\n+          if (idx > old_collector_rightmost) {\n+            old_collector_rightmost = idx;\n@@ -1001,0 +1383,10 @@\n+          if (ac == region_size_bytes) {\n+            if (idx < old_collector_leftmost_empty) {\n+              old_collector_leftmost_empty = idx;\n+            }\n+            if (idx > old_collector_rightmost_empty) {\n+              old_collector_rightmost_empty = idx;\n+            }\n+          }\n+          old_collector_regions++;\n+          old_collector_used += (region_size_bytes - ac);\n@@ -1002,7 +1394,0 @@\n-        mutator_regions++;\n-        mutator_used += (region_size_bytes - ac);\n-\n-        log_debug(gc)(\n-          \"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator partition\",\n-          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n-          byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n@@ -1012,0 +1397,18 @@\n+  log_debug(gc)(\"  At end of prep_to_rebuild, mutator_leftmost: \" SIZE_FORMAT\n+                \", mutator_rightmost: \" SIZE_FORMAT\n+                \", mutator_leftmost_empty: \" SIZE_FORMAT\n+                \", mutator_rightmost_empty: \" SIZE_FORMAT\n+                \", mutator_regions: \" SIZE_FORMAT\n+                \", mutator_used: \" SIZE_FORMAT,\n+                mutator_leftmost, mutator_rightmost, mutator_leftmost_empty, mutator_rightmost_empty,\n+                mutator_regions, mutator_used);\n+\n+  log_debug(gc)(\"  old_collector_leftmost: \" SIZE_FORMAT\n+                \", old_collector_rightmost: \" SIZE_FORMAT\n+                \", old_collector_leftmost_empty: \" SIZE_FORMAT\n+                \", old_collector_rightmost_empty: \" SIZE_FORMAT\n+                \", old_collector_regions: \" SIZE_FORMAT\n+                \", old_collector_used: \" SIZE_FORMAT,\n+                old_collector_leftmost, old_collector_rightmost, old_collector_leftmost_empty, old_collector_rightmost_empty,\n+                old_collector_regions, old_collector_used);\n+\n@@ -1014,0 +1417,8 @@\n+  _partitions.establish_old_collector_intervals(old_collector_leftmost, old_collector_rightmost, old_collector_leftmost_empty,\n+                                                old_collector_rightmost_empty, old_collector_regions, old_collector_used);\n+  log_debug(gc)(\"  After find_regions_with_alloc_capacity(), Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"],\"\n+                \"  Old Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+                _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n@@ -1016,1 +1427,5 @@\n-void ShenandoahFreeSet::move_regions_from_collector_to_mutator(size_t max_xfer_regions) {\n+\/\/ Returns number of regions transferred, adds transferred bytes to var argument bytes_transferred\n+size_t ShenandoahFreeSet::transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId which_collector,\n+                                                                                   size_t max_xfer_regions,\n+                                                                                   size_t& bytes_transferred) {\n+  shenandoah_assert_heaplocked();\n@@ -1018,2 +1433,38 @@\n-  size_t collector_empty_xfer = 0;\n-  size_t collector_not_empty_xfer = 0;\n+  size_t transferred_regions = 0;\n+  idx_t rightmost = _partitions.rightmost_empty(which_collector);\n+  for (idx_t idx = _partitions.leftmost_empty(which_collector); (transferred_regions < max_xfer_regions) && (idx <= rightmost); ) {\n+    assert(_partitions.in_free_set(which_collector, idx), \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, idx);\n+    \/\/ Note: can_allocate_from() denotes that region is entirely empty\n+    if (can_allocate_from(idx)) {\n+      _partitions.move_from_partition_to_partition(idx, which_collector, ShenandoahFreeSetPartitionId::Mutator, region_size_bytes);\n+      transferred_regions++;\n+      bytes_transferred += region_size_bytes;\n+    }\n+    idx = _partitions.find_index_of_next_available_region(which_collector, idx + 1);\n+  }\n+  return transferred_regions;\n+}\n+\n+\/\/ Returns number of regions transferred, adds transferred bytes to var argument bytes_transferred\n+size_t ShenandoahFreeSet::transfer_non_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId collector_id,\n+                                                                                       size_t max_xfer_regions,\n+                                                                                       size_t& bytes_transferred) {\n+  shenandoah_assert_heaplocked();\n+  size_t transferred_regions = 0;\n+  idx_t rightmost = _partitions.rightmost(collector_id);\n+  for (idx_t idx = _partitions.leftmost(collector_id); (transferred_regions < max_xfer_regions) && (idx <= rightmost); ) {\n+    assert(_partitions.in_free_set(collector_id, idx), \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, idx);\n+    size_t ac = alloc_capacity(idx);\n+    if (ac > 0) {\n+      _partitions.move_from_partition_to_partition(idx, collector_id, ShenandoahFreeSetPartitionId::Mutator, ac);\n+      transferred_regions++;\n+      bytes_transferred += ac;\n+    }\n+    idx = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Collector, idx + 1);\n+  }\n+  return transferred_regions;\n+}\n+\n+void ShenandoahFreeSet::move_regions_from_collector_to_mutator(size_t max_xfer_regions) {\n+  size_t collector_xfer = 0;\n+  size_t old_collector_xfer = 0;\n@@ -1026,13 +1477,16 @@\n-    idx_t rightmost = _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Collector);\n-    for (idx_t idx = _partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Collector);\n-         (max_xfer_regions > 0) && (idx <= rightmost); ) {\n-      assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Collector, idx),\n-             \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, idx);\n-      \/\/ Note: can_allocate_from() denotes that region is entirely empty\n-      if (can_allocate_from(idx)) {\n-        _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Collector,\n-                                                     ShenandoahFreeSetPartitionId::Mutator, region_size_bytes);\n-        max_xfer_regions--;\n-        collector_empty_xfer += region_size_bytes;\n-      }\n-      idx = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Collector, idx + 1);\n+    max_xfer_regions -=\n+      transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId::Collector, max_xfer_regions,\n+                                                               collector_xfer);\n+  }\n+\n+  \/\/ Process empty regions within the OldCollector free partition\n+  if ((max_xfer_regions > 0) &&\n+      (_partitions.leftmost_empty(ShenandoahFreeSetPartitionId::OldCollector)\n+       <= _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::OldCollector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    size_t old_collector_regions =\n+      transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId::OldCollector, max_xfer_regions,\n+                                                               old_collector_xfer);\n+    max_xfer_regions -= old_collector_regions;\n+    if (old_collector_regions > 0) {\n+      ShenandoahGenerationalHeap::cast(_heap)->generation_sizer()->transfer_to_young(old_collector_regions);\n@@ -1046,14 +1500,3 @@\n-    idx_t rightmost = _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector);\n-    for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector);\n-         (max_xfer_regions > 0) && (idx <= rightmost); ) {\n-      assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Collector, idx),\n-             \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, idx);\n-      size_t ac = alloc_capacity(idx);\n-      if (ac > 0) {\n-        _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Collector,\n-                                                     ShenandoahFreeSetPartitionId::Mutator, ac);\n-        max_xfer_regions--;\n-        collector_not_empty_xfer += ac;\n-      }\n-      idx = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Collector, idx + 1);\n-    }\n+    max_xfer_regions -=\n+      transfer_non_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId::Collector, max_xfer_regions,\n+                                                                   collector_xfer);\n@@ -1062,3 +1505,6 @@\n-  size_t collector_xfer = collector_empty_xfer + collector_not_empty_xfer;\n-  log_info(gc, ergo)(\"At start of update refs, moving \" SIZE_FORMAT \"%s to Mutator free partition from Collector Reserve\",\n-                     byte_size_in_proper_unit(collector_xfer), proper_unit_for_byte_size(collector_xfer));\n+  size_t total_xfer = collector_xfer + old_collector_xfer;\n+  log_info(gc, ergo)(\"At start of update refs, moving \" SIZE_FORMAT \"%s to Mutator free set from Collector Reserve (\"\n+                     SIZE_FORMAT \"%s) and from Old Collector Reserve (\" SIZE_FORMAT \"%s)\",\n+                     byte_size_in_proper_unit(total_xfer), proper_unit_for_byte_size(total_xfer),\n+                     byte_size_in_proper_unit(collector_xfer), proper_unit_for_byte_size(collector_xfer),\n+                     byte_size_in_proper_unit(old_collector_xfer), proper_unit_for_byte_size(old_collector_xfer));\n@@ -1067,1 +1513,4 @@\n-void ShenandoahFreeSet::prepare_to_rebuild(size_t &cset_regions) {\n+\n+\/\/ Overwrite arguments to represent the amount of memory in each generation that is about to be recycled\n+void ShenandoahFreeSet::prepare_to_rebuild(size_t &young_cset_regions, size_t &old_cset_regions,\n+                                           size_t &first_old_region, size_t &last_old_region, size_t &old_region_count) {\n@@ -1069,0 +1518,3 @@\n+  \/\/ This resets all state information, removing all regions from all sets.\n+  clear();\n+  log_debug(gc, free)(\"Rebuilding FreeSet\");\n@@ -1070,1 +1522,4 @@\n-  log_debug(gc)(\"Rebuilding FreeSet\");\n+  \/\/ This places regions that have alloc_capacity into the old_collector set if they identify as is_old() or the\n+  \/\/ mutator set otherwise.  All trashed (cset) regions are affiliated young and placed in mutator set.\n+  find_regions_with_alloc_capacity(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+}\n@@ -1072,2 +1527,11 @@\n-  \/\/ This places regions that have alloc_capacity into the mutator partition.\n-  find_regions_with_alloc_capacity(cset_regions);\n+void ShenandoahFreeSet::establish_generation_sizes(size_t young_region_count, size_t old_region_count) {\n+  assert(young_region_count + old_region_count == ShenandoahHeap::heap()->num_regions(), \"Sanity\");\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+    ShenandoahOldGeneration* old_gen = heap->old_generation();\n+    ShenandoahYoungGeneration* young_gen = heap->young_generation();\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+    old_gen->set_capacity(old_region_count * region_size_bytes);\n+    young_gen->set_capacity(young_region_count * region_size_bytes);\n+  }\n@@ -1076,1 +1540,2 @@\n-void ShenandoahFreeSet::finish_rebuild(size_t cset_regions) {\n+void ShenandoahFreeSet::finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t old_region_count,\n+                                       bool have_evacuation_reserves) {\n@@ -1078,0 +1543,1 @@\n+  size_t young_reserve(0), old_reserve(0);\n@@ -1079,9 +1545,3 @@\n-  \/\/ Our desire is to reserve this much memory for future evacuation.  We may end up reserving less, if\n-  \/\/ memory is in short supply.\n-\n-  size_t reserve = _heap->max_capacity() * ShenandoahEvacReserve \/ 100;\n-  size_t available_in_collector_partition = (_partitions.capacity_of(ShenandoahFreeSetPartitionId::Collector)\n-                                             - _partitions.used_by(ShenandoahFreeSetPartitionId::Collector));\n-  size_t additional_reserve;\n-  if (available_in_collector_partition < reserve) {\n-    additional_reserve = reserve - available_in_collector_partition;\n+  if (_heap->mode()->is_generational()) {\n+    compute_young_and_old_reserves(young_cset_regions, old_cset_regions, have_evacuation_reserves,\n+                                   young_reserve, old_reserve);\n@@ -1089,1 +1549,2 @@\n-    additional_reserve = 0;\n+    young_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+    old_reserve = 0;\n@@ -1092,1 +1553,6 @@\n-  reserve_regions(reserve);\n+  \/\/ Move some of the mutator regions in the Collector and OldCollector partitions in order to satisfy\n+  \/\/ young_reserve and old_reserve.\n+  reserve_regions(young_reserve, old_reserve, old_region_count);\n+  size_t young_region_count = _heap->num_regions() - old_region_count;\n+  establish_generation_sizes(young_region_count, old_region_count);\n+  establish_old_collector_alloc_bias();\n@@ -1097,4 +1563,71 @@\n-void ShenandoahFreeSet::rebuild() {\n-  size_t cset_regions;\n-  prepare_to_rebuild(cset_regions);\n-  finish_rebuild(cset_regions);\n+void ShenandoahFreeSet::compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions,\n+                                                       bool have_evacuation_reserves,\n+                                                       size_t& young_reserve_result, size_t& old_reserve_result) const {\n+  shenandoah_assert_generational();\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+  ShenandoahOldGeneration* const old_generation = _heap->old_generation();\n+  size_t old_available = old_generation->available();\n+  size_t old_unaffiliated_regions = old_generation->free_unaffiliated_regions();\n+  ShenandoahYoungGeneration* const young_generation = _heap->young_generation();\n+  size_t young_capacity = young_generation->max_capacity();\n+  size_t young_unaffiliated_regions = young_generation->free_unaffiliated_regions();\n+\n+  \/\/ Add in the regions we anticipate to be freed by evacuation of the collection set\n+  old_unaffiliated_regions += old_cset_regions;\n+  young_unaffiliated_regions += young_cset_regions;\n+\n+  \/\/ Consult old-region balance to make adjustments to current generation capacities and availability.\n+  \/\/ The generation region transfers take place after we rebuild.\n+  const ssize_t old_region_balance = old_generation->get_region_balance();\n+  if (old_region_balance != 0) {\n+#ifdef ASSERT\n+    if (old_region_balance > 0) {\n+      assert(old_region_balance <= checked_cast<ssize_t>(old_unaffiliated_regions), \"Cannot transfer regions that are affiliated\");\n+    } else {\n+      assert(0 - old_region_balance <= checked_cast<ssize_t>(young_unaffiliated_regions), \"Cannot transfer regions that are affiliated\");\n+    }\n+#endif\n+\n+    ssize_t xfer_bytes = old_region_balance * checked_cast<ssize_t>(region_size_bytes);\n+    old_available -= xfer_bytes;\n+    old_unaffiliated_regions -= old_region_balance;\n+    young_capacity += xfer_bytes;\n+    young_unaffiliated_regions += old_region_balance;\n+  }\n+\n+  \/\/ All allocations taken from the old collector set are performed by GC, generally using PLABs for both\n+  \/\/ promotions and evacuations.  The partition between which old memory is reserved for evacuation and\n+  \/\/ which is reserved for promotion is enforced using thread-local variables that prescribe intentions for\n+  \/\/ each PLAB's available memory.\n+  if (have_evacuation_reserves) {\n+    \/\/ We are rebuilding at the end of final mark, having already established evacuation budgets for this GC pass.\n+    const size_t promoted_reserve = old_generation->get_promoted_reserve();\n+    const size_t old_evac_reserve = old_generation->get_evacuation_reserve();\n+    young_reserve_result = young_generation->get_evacuation_reserve();\n+    old_reserve_result = promoted_reserve + old_evac_reserve;\n+    assert(old_reserve_result <= old_available,\n+           \"Cannot reserve (\" SIZE_FORMAT \" + \" SIZE_FORMAT\") more OLD than is available: \" SIZE_FORMAT,\n+           promoted_reserve, old_evac_reserve, old_available);\n+  } else {\n+    \/\/ We are rebuilding at end of GC, so we set aside budgets specified on command line (or defaults)\n+    young_reserve_result = (young_capacity * ShenandoahEvacReserve) \/ 100;\n+    \/\/ The auto-sizer has already made old-gen large enough to hold all anticipated evacuations and promotions.\n+    \/\/ Affiliated old-gen regions are already in the OldCollector free set.  Add in the relevant number of\n+    \/\/ unaffiliated regions.\n+    old_reserve_result = old_available;\n+  }\n+\n+  \/\/ Old available regions that have less than PLAB::min_size() of available memory are not placed into the OldCollector\n+  \/\/ free set.  Because of this, old_available may not have enough memory to represent the intended reserve.  Adjust\n+  \/\/ the reserve downward to account for this possibility. This loss is part of the reason why the original budget\n+  \/\/ was adjusted with ShenandoahOldEvacWaste and ShenandoahOldPromoWaste multipliers.\n+  if (old_reserve_result >\n+      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n+    old_reserve_result =\n+      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes;\n+  }\n+\n+  if (young_reserve_result > young_unaffiliated_regions * region_size_bytes) {\n+    young_reserve_result = young_unaffiliated_regions * region_size_bytes;\n+  }\n@@ -1103,1 +1636,6 @@\n-void ShenandoahFreeSet::reserve_regions(size_t to_reserve) {\n+\/\/ Having placed all regions that have allocation capacity into the mutator set if they identify as is_young()\n+\/\/ or into the old collector set if they identify as is_old(), move some of these regions from the mutator set\n+\/\/ into the collector set or old collector set in order to assure that the memory available for allocations within\n+\/\/ the collector set is at least to_reserve and the memory available for allocations within the old collector set\n+\/\/ is at least to_reserve_old.\n+void ShenandoahFreeSet::reserve_regions(size_t to_reserve, size_t to_reserve_old, size_t &old_region_count) {\n@@ -1107,1 +1645,0 @@\n-\n@@ -1113,1 +1650,2 @@\n-    assert (ac > 0, \"Membership in free partition implies has capacity\");\n+    assert (ac > 0, \"Membership in free set implies has capacity\");\n+    assert (!r->is_old() || r->is_trash(), \"Except for trash, mutator_is_free regions should not be affiliated OLD\");\n@@ -1115,0 +1653,1 @@\n+    bool move_to_old_collector = _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector) < to_reserve_old;\n@@ -1116,2 +1655,3 @@\n-    if (!move_to_collector) {\n-      \/\/ We've satisfied to_reserve\n+\n+    if (!move_to_collector && !move_to_old_collector) {\n+      \/\/ We've satisfied both to_reserve and to_reserved_old\n@@ -1121,0 +1661,20 @@\n+    if (move_to_old_collector) {\n+      \/\/ We give priority to OldCollector partition because we desire to pack OldCollector regions into higher\n+      \/\/ addresses than Collector regions.  Presumably, OldCollector regions are more \"stable\" and less likely to\n+      \/\/ be collected in the near future.\n+      if (r->is_trash() || !r->is_affiliated()) {\n+        \/\/ OLD regions that have available memory are already in the old_collector free set.\n+        _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                                     ShenandoahFreeSetPartitionId::OldCollector, ac);\n+        log_debug(gc)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to old_collector_free\", idx);\n+        log_debug(gc)(\"  Shifted Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"],\"\n+                      \"  Old Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+                      _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                      _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                      _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                      _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+        old_region_count++;\n+        continue;\n+      }\n+    }\n+\n@@ -1133,0 +1693,6 @@\n+      log_debug(gc)(\"  Shifted Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"],\"\n+                    \"  Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+                    _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                    _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                    _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector),\n+                    _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector));\n@@ -1137,0 +1703,5 @@\n+    size_t old_reserve = _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector);\n+    if (old_reserve < to_reserve_old) {\n+      log_info(gc, free)(\"Wanted \" PROPERFMT \" for old reserve, but only reserved: \" PROPERFMT,\n+                         PROPERFMTARGS(to_reserve_old), PROPERFMTARGS(old_reserve));\n+    }\n@@ -1145,0 +1716,31 @@\n+void ShenandoahFreeSet::establish_old_collector_alloc_bias() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  shenandoah_assert_heaplocked();\n+\n+  idx_t left_idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+  idx_t right_idx = _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector);\n+  idx_t middle = (left_idx + right_idx) \/ 2;\n+  size_t available_in_first_half = 0;\n+  size_t available_in_second_half = 0;\n+\n+  for (idx_t index = left_idx; index < middle; index++) {\n+    if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, index)) {\n+      ShenandoahHeapRegion* r = heap->get_region((size_t) index);\n+      available_in_first_half += r->free();\n+    }\n+  }\n+  for (idx_t index = middle; index <= right_idx; index++) {\n+    if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, index)) {\n+      ShenandoahHeapRegion* r = heap->get_region(index);\n+      available_in_second_half += r->free();\n+    }\n+  }\n+\n+  \/\/ We desire to first consume the sparsely distributed regions in order that the remaining regions are densely packed.\n+  \/\/ Densely packing regions reduces the effort to search for a region that has sufficient memory to satisfy a new allocation\n+  \/\/ request.  Regions become sparsely distributed following a Full GC, which tends to slide all regions to the front of the\n+  \/\/ heap rather than allowing survivor regions to remain at the high end of the heap where we intend for them to congregate.\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::OldCollector,\n+                                          (available_in_second_half > available_in_first_half));\n+}\n+\n@@ -1162,0 +1764,4 @@\n+    size_t retired_old = 0;\n+    size_t retired_old_humongous = 0;\n+    size_t retired_young = 0;\n+    size_t retired_young_humongous = 0;\n@@ -1163,0 +1769,2 @@\n+    size_t retired_young_waste = 0;\n+    size_t retired_old_waste = 0;\n@@ -1164,1 +1772,1 @@\n-    size_t available_collector = 0;\n+    size_t consumed_old_collector = 0;\n@@ -1166,0 +1774,2 @@\n+    size_t available_old = 0;\n+    size_t available_young = 0;\n@@ -1167,0 +1777,2 @@\n+    size_t available_collector = 0;\n+    size_t available_old_collector = 0;\n@@ -1172,3 +1784,7 @@\n-    log_debug(gc)(\"FreeSet map legend: M:mutator_free C:collector_free H:humongous _:retired\");\n-    log_debug(gc)(\" mutator free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"],\"\n-                  \" collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"]\",\n+\n+    log_debug(gc)(\"FreeSet map legend:\"\n+                       \" M:mutator_free C:collector_free O:old_collector_free\"\n+                       \" H:humongous ~:retired old _:retired young\");\n+    log_debug(gc)(\" mutator free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"] allocating from %s, \"\n+                  \" collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                  \"old collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"] allocates from %s\",\n@@ -1177,0 +1793,1 @@\n+                  _partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::Mutator)? \"left to right\": \"right to left\",\n@@ -1178,1 +1795,4 @@\n-                  _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector));\n+                  _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector),\n+                  _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                  _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                  _partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::OldCollector)? \"left to right\": \"right to left\");\n@@ -1188,0 +1808,1 @@\n+        assert(!r->is_old() || r->is_trash(), \"Old regions except trash regions should not be in mutator_free set\");\n@@ -1193,0 +1814,1 @@\n+        assert(!r->is_old() || r->is_trash(), \"Old regions except trash regions should not be in collector_free set\");\n@@ -1196,0 +1818,5 @@\n+      } else if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, i)) {\n+        size_t capacity = alloc_capacity(r);\n+        available_old_collector += capacity;\n+        consumed_old_collector += region_size_bytes - capacity;\n+        buffer[idx] = (capacity == region_size_bytes)? 'O': 'o';\n@@ -1197,1 +1824,7 @@\n-        buffer[idx] = 'h';\n+        if (r->is_old()) {\n+          buffer[idx] = 'H';\n+          retired_old_humongous += region_size_bytes;\n+        } else {\n+          buffer[idx] = 'h';\n+          retired_young_humongous += region_size_bytes;\n+        }\n@@ -1199,1 +1832,9 @@\n-        buffer[idx] = '_';\n+        if (r->is_old()) {\n+          buffer[idx] = '~';\n+          retired_old_waste += alloc_capacity(r);\n+          retired_old += region_size_bytes;\n+        } else {\n+          buffer[idx] = '_';\n+          retired_young_waste += alloc_capacity(r);\n+          retired_young += region_size_bytes;\n+        }\n@@ -1257,2 +1898,1 @@\n-\n-               byte_size_in_proper_unit(free),          proper_unit_for_byte_size(free),\n+               byte_size_in_proper_unit(total_free),    proper_unit_for_byte_size(total_free),\n@@ -1306,0 +1946,21 @@\n+\n+    if (_heap->mode()->is_generational()) {\n+      size_t max = 0;\n+      size_t total_free = 0;\n+      size_t total_used = 0;\n+\n+      for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+           idx <= _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector); idx++) {\n+        if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, idx)) {\n+          ShenandoahHeapRegion *r = _heap->get_region(idx);\n+          size_t free = alloc_capacity(r);\n+          max = MAX2(max, free);\n+          total_free += free;\n+          total_used += r->used();\n+        }\n+      }\n+      ls.print_cr(\" Old Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n+                  byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),\n+                  byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max),\n+                  byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used));\n+    }\n@@ -1317,0 +1978,1 @@\n+      case ShenandoahAllocRequest::_alloc_plab:\n@@ -1348,0 +2010,9 @@\n+  if (_heap->mode()->is_generational()) {\n+    out->print_cr(\"Old Collector Free Set: \" SIZE_FORMAT \"\", _partitions.count(ShenandoahFreeSetPartitionId::OldCollector));\n+    for (idx_t index = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+         index <= _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector); index++) {\n+      if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, index)) {\n+        _heap->get_region(index)->print_on(out);\n+      }\n+    }\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":845,"deletions":174,"binary":false,"changes":1019,"status":"modified"},{"patch":"@@ -1,0 +1,1 @@\n+\n@@ -37,0 +38,2 @@\n+  OldCollector,                 \/\/ Region is in the Old Collector free set:\n+                                \/\/    available memory is reserved for old evacuations and for promotions..\n@@ -83,0 +86,4 @@\n+  \/\/ For each partition p, _left_to_right_bias is true iff allocations are normally made from lower indexed regions\n+  \/\/ before higher indexed regions.\n+  bool _left_to_right_bias[UIntNumPartitions];\n+\n@@ -91,0 +98,5 @@\n+  inline bool is_mutator_partition(ShenandoahFreeSetPartitionId p);\n+  inline bool is_young_collector_partition(ShenandoahFreeSetPartitionId p);\n+  inline bool is_old_collector_partition(ShenandoahFreeSetPartitionId p);\n+  inline bool available_implies_empty(size_t available);\n+\n@@ -115,0 +127,7 @@\n+  \/\/ Set the OldCollector intervals, usage, and capacity according to arguments.  We use this at the end of rebuild_free_set()\n+  \/\/ to avoid the overhead of making many redundant incremental adjustments to the mutator intervals as the free set is being\n+  \/\/ rebuilt.\n+  void establish_old_collector_intervals(ssize_t old_collector_leftmost, ssize_t old_collector_rightmost,\n+                                         ssize_t old_collector_leftmost_empty, ssize_t old_collector_rightmost_empty,\n+                                         size_t old_collector_region_count, size_t old_collector_used);\n+\n@@ -183,0 +202,10 @@\n+  inline void set_bias_from_left_to_right(ShenandoahFreeSetPartitionId which_partition, bool value) {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    _left_to_right_bias[int(which_partition)] = value;\n+  }\n+\n+  inline bool alloc_from_left_bias(ShenandoahFreeSetPartitionId which_partition) const {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    return _left_to_right_bias[int(which_partition)];\n+  }\n+\n@@ -240,1 +269,1 @@\n-\/\/ The ShenandoahFreeSet endeavors to congregrate survivor objects (objects that have been evacuated at least once) at the\n+\/\/ The ShenandoahFreeSet tries to colocate survivor objects (objects that have been evacuated at least once) at the\n@@ -262,0 +291,3 @@\n+  size_t _retired_old_regions;\n+\n+  HeapWord* allocate_aligned_plab(size_t size, ShenandoahAllocRequest& req, ShenandoahHeapRegion* r);\n@@ -263,4 +295,5 @@\n-  \/\/ Mutator allocations are biased from left-to-right or from right-to-left based on which end of mutator range\n-  \/\/ is most likely to hold partially used regions.  In general, we want to finish consuming partially used\n-  \/\/ regions and retire them in order to reduce the regions that must be searched for each allocation request.\n-  bool _right_to_left_bias;\n+  \/\/ Return the address of memory allocated, setting in_new_region to true iff the allocation is taken\n+  \/\/ from a region that was previously empty.  Return nullptr if memory could not be allocated.\n+  inline HeapWord* allocate_from_partition_with_affiliation(ShenandoahFreeSetPartitionId which_partition,\n+                                                            ShenandoahAffiliation affiliation,\n+                                                            ShenandoahAllocRequest& req, bool& in_new_region);\n@@ -290,1 +323,3 @@\n-  \/\/ Change region r from the Mutator partition to the GC's Collector partition.  This requires that the region is entirely empty.\n+  \/\/ Change region r from the Mutator partition to the GC's Collector or OldCollector partition.  This requires that the\n+  \/\/ region is entirely empty.\n+  \/\/\n@@ -293,1 +328,1 @@\n-  \/\/ the Mutator free set into the Collector free set.\n+  \/\/ the Mutator free set into the Collector or OldCollector free set.\n@@ -295,0 +330,2 @@\n+  void flip_to_old_gc(ShenandoahHeapRegion* r);\n+\n@@ -306,5 +343,6 @@\n-  \/\/ This function places all regions that have allocation capacity into the mutator_partition, identifying regions\n-  \/\/ that have no allocation capacity as NotFree.  Subsequently, we will move some of the mutator regions into the\n-  \/\/ collector partition with the intent of packing collector memory into the highest (rightmost) addresses of the\n-  \/\/ heap, with mutator memory consuming the lowest addresses of the heap.\n-  void find_regions_with_alloc_capacity(size_t &cset_regions);\n+  size_t transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId which_collector,\n+                                                                  size_t max_xfer_regions,\n+                                                                  size_t& bytes_transferred);\n+  size_t transfer_non_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId collector_id,\n+                                                                      size_t max_xfer_regions,\n+                                                                      size_t& bytes_transferred);\n@@ -312,6 +350,2 @@\n-  \/\/ Having placed all regions that have allocation capacity into the mutator partition, move some of these regions from\n-  \/\/ the mutator partition into the collector partition in order to assure that the memory available for allocations within\n-  \/\/ the collector partition is at least to_reserve.\n-  void reserve_regions(size_t to_reserve);\n-  \/\/ Overwrite arguments to represent the number of regions to be reclaimed from the cset\n-  void prepare_to_rebuild(size_t &cset_regions);\n+  \/\/ Determine whether we prefer to allocate from left to right or from right to left within the OldCollector free-set.\n+  void establish_old_collector_alloc_bias();\n@@ -320,1 +354,3 @@\n-  void finish_rebuild(size_t cset_regions);\n+  \/\/ Set max_capacity for young and old generations\n+  void establish_generation_sizes(size_t young_region_count, size_t old_region_count);\n+  size_t get_usable_free_words(size_t free_bytes) const;\n@@ -333,1 +369,37 @@\n-  void rebuild();\n+\n+  \/\/ Examine the existing free set representation, capturing the current state into var arguments:\n+  \/\/\n+  \/\/ young_cset_regions is the number of regions currently in the young cset if we are starting to evacuate, or zero\n+  \/\/   old_cset_regions is the number of regions currently in the old cset if we are starting a mixed evacuation, or zero\n+  \/\/   first_old_region is the index of the first region that is part of the OldCollector set\n+  \/\/    last_old_region is the index of the last region that is part of the OldCollector set\n+  \/\/   old_region_count is the number of regions in the OldCollector set that have memory available to be allocated\n+  void prepare_to_rebuild(size_t &young_cset_regions, size_t &old_cset_regions,\n+                          size_t &first_old_region, size_t &last_old_region, size_t &old_region_count);\n+\n+  \/\/ At the end of final mark, but before we begin evacuating, heuristics calculate how much memory is required to\n+  \/\/ hold the results of evacuating to young-gen and to old-gen, and have_evacuation_reserves should be true.\n+  \/\/ These quantities, stored as reserves for their respective generations, are consulted prior to rebuilding\n+  \/\/ the free set (ShenandoahFreeSet) in preparation for evacuation.  When the free set is rebuilt, we make sure\n+  \/\/ to reserve sufficient memory in the collector and old_collector sets to hold evacuations.\n+  \/\/\n+  \/\/ We also rebuild the free set at the end of GC, as we prepare to idle GC until the next trigger.  In this case,\n+  \/\/ have_evacuation_reserves is false because we don't yet know how much memory will need to be evacuated in the\n+  \/\/ next GC cycle.  When have_evacuation_reserves is false, the free set rebuild operation reserves for the collector\n+  \/\/ and old_collector sets based on alternative mechanisms, such as ShenandoahEvacReserve, ShenandoahOldEvacReserve, and\n+  \/\/ ShenandoahOldCompactionReserve.  In a future planned enhancement, the reserve for old_collector set when the\n+  \/\/ evacuation reserves are unknown, is based in part on anticipated promotion as determined by analysis of live data\n+  \/\/ found during the previous GC pass which is one less than the current tenure age.\n+  \/\/\n+  \/\/ young_cset_regions is the number of regions currently in the young cset if we are starting to evacuate, or zero\n+  \/\/   old_cset_regions is the number of regions currently in the old cset if we are starting a mixed evacuation, or zero\n+  \/\/    num_old_regions is the number of old-gen regions that have available memory for further allocations (excluding old cset)\n+  \/\/ have_evacuation_reserves is true iff the desired values of young-gen and old-gen evacuation reserves and old-gen\n+  \/\/                    promotion reserve have been precomputed (and can be obtained by invoking\n+  \/\/                    <generation>->get_evacuation_reserve() or old_gen->get_promoted_reserve()\n+  void finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t num_old_regions,\n+                      bool have_evacuation_reserves = false);\n+\n+  \/\/ When a region is promoted in place, we add the region's available memory if it is greater than plab_min_size()\n+  \/\/ into the old collector partition by invoking this method.\n+  void add_promoted_in_place_region_to_old_collector(ShenandoahHeapRegion* region);\n@@ -339,5 +411,0 @@\n-  \/\/\n-  \/\/ Note that we plan to replenish the Collector reserve at the end of update refs, at which time all\n-  \/\/ of the regions recycled from the collection set will be available.  If the very unlikely event that there\n-  \/\/ are fewer regions in the collection set than remain in the collector set, we limit the transfer in order\n-  \/\/ to assure that the replenished Collector reserve can be sufficiently large.\n@@ -347,0 +414,1 @@\n+\n@@ -399,0 +467,22 @@\n+\n+  \/\/ This function places all regions that have allocation capacity into the mutator partition, or if the region\n+  \/\/ is already affiliated with old, into the old collector partition, identifying regions that have no allocation\n+  \/\/ capacity as NotFree.  Capture the modified state of the freeset into var arguments:\n+  \/\/\n+  \/\/ young_cset_regions is the number of regions currently in the young cset if we are starting to evacuate, or zero\n+  \/\/   old_cset_regions is the number of regions currently in the old cset if we are starting a mixed evacuation, or zero\n+  \/\/   first_old_region is the index of the first region that is part of the OldCollector set\n+  \/\/    last_old_region is the index of the last region that is part of the OldCollector set\n+  \/\/   old_region_count is the number of regions in the OldCollector set that have memory available to be allocated\n+  void find_regions_with_alloc_capacity(size_t &young_cset_regions, size_t &old_cset_regions,\n+                                        size_t &first_old_region, size_t &last_old_region, size_t &old_region_count);\n+\n+  \/\/ Ensure that Collector has at least to_reserve bytes of available memory, and OldCollector has at least old_reserve\n+  \/\/ bytes of available memory.  On input, old_region_count holds the number of regions already present in the\n+  \/\/ OldCollector partition.  Upon return, old_region_count holds the updated number of regions in the OldCollector partition.\n+  void reserve_regions(size_t to_reserve, size_t old_reserve, size_t &old_region_count);\n+\n+  \/\/ Reserve space for evacuations, with regions reserved for old evacuations placed to the right\n+  \/\/ of regions reserved of young evacuations.\n+  void compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions, bool have_evacuation_reserves,\n+                                      size_t &young_reserve_result, size_t &old_reserve_result) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":115,"deletions":25,"binary":false,"changes":140,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -39,0 +40,3 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n@@ -47,0 +51,3 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalEvacuationTask.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n@@ -54,1 +61,1 @@\n-#include \"gc\/shenandoah\/shenandoahMetrics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -62,0 +69,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -69,0 +77,2 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -71,0 +81,2 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n@@ -164,3 +176,0 @@\n-  \/\/ Now we know the number of regions, initialize the heuristics.\n-  initialize_heuristics();\n-\n@@ -220,0 +229,22 @@\n+  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this, _heap_region));\n+\n+  \/\/ Now we know the number of regions and heap sizes, initialize the heuristics.\n+  initialize_heuristics();\n+\n+  assert(_heap_region.byte_size() == heap_rs.size(), \"Need to know reserved size for card table\");\n+\n+  \/\/\n+  \/\/ Worker threads must be initialized after the barrier is configured\n+  \/\/\n+  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n+  if (_workers == nullptr) {\n+    vm_exit_during_initialization(\"Failed necessary allocation.\");\n+  } else {\n+    _workers->initialize_workers();\n+  }\n+\n+  if (ParallelGCThreads > 1) {\n+    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\", ParallelGCThreads);\n+    _safepoint_workers->initialize_workers();\n+  }\n+\n@@ -260,1 +291,1 @@\n-                              align_up(num_committed_regions, _bitmap_regions_per_slice) \/ _bitmap_regions_per_slice;\n+    align_up(num_committed_regions, _bitmap_regions_per_slice) \/ _bitmap_regions_per_slice;\n@@ -267,1 +298,1 @@\n-  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions, _max_workers);\n+  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions);\n@@ -351,0 +382,1 @@\n+  _affiliations = NEW_C_HEAP_ARRAY(uint8_t, _num_regions, mtGC);\n@@ -367,0 +399,2 @@\n+\n+      _affiliations[i] = ShenandoahAffiliation::FREE;\n@@ -371,0 +405,1 @@\n+    size_t young_cset_regions, old_cset_regions;\n@@ -372,1 +407,4 @@\n-    _free_set->rebuild();\n+    \/\/ We are initializing free set.  We ignore cset region tallies.\n+    size_t first_old, last_old, num_old;\n+    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+    _free_set->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n@@ -421,1 +459,1 @@\n-  _control_thread = new ShenandoahControlThread();\n+  initialize_controller();\n@@ -423,1 +461,1 @@\n-  ShenandoahInitLogger::print();\n+  print_init_logger();\n@@ -428,0 +466,8 @@\n+void ShenandoahHeap::initialize_controller() {\n+  _control_thread = new ShenandoahControlThread();\n+}\n+\n+void ShenandoahHeap::print_init_logger() const {\n+  ShenandoahInitLogger::print();\n+}\n+\n@@ -434,0 +480,2 @@\n+    } else if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+      _gc_mode = new ShenandoahGenerationalMode();\n@@ -454,13 +502,3 @@\n-  assert(_gc_mode != nullptr, \"Must be initialized\");\n-  _heuristics = _gc_mode->initialize_heuristics();\n-\n-  if (_heuristics->is_diagnostic() && !UnlockDiagnosticVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.\",\n-                    _heuristics->name()));\n-  }\n-  if (_heuristics->is_experimental() && !UnlockExperimentalVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.\",\n-                    _heuristics->name()));\n-  }\n+  _global_generation = new ShenandoahGlobalGeneration(mode()->is_generational(), max_workers(), max_capacity(), max_capacity());\n+  _global_generation->initialize_heuristics(mode());\n+  _evac_tracker = new ShenandoahEvacuationTracker(mode()->is_generational());\n@@ -476,0 +514,2 @@\n+  _gc_generation(nullptr),\n+  _active_generation(nullptr),\n@@ -477,3 +517,1 @@\n-  _used(0),\n-  _bytes_allocated_since_gc_start(0),\n-  _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),\n+  _max_workers(MAX3(ConcGCThreads, ParallelGCThreads, 1U)),\n@@ -486,1 +524,1 @@\n-  _update_refs_iterator(this),\n+  _affiliations(nullptr),\n@@ -489,0 +527,3 @@\n+  _cancel_requested_time(0),\n+  _update_refs_iterator(this),\n+  _global_generation(nullptr),\n@@ -490,0 +531,2 @@\n+  _young_generation(nullptr),\n+  _old_generation(nullptr),\n@@ -492,1 +535,0 @@\n-  _heuristics(nullptr),\n@@ -497,0 +539,2 @@\n+  _evac_tracker(nullptr),\n+  _mmu_tracker(),\n@@ -503,1 +547,0 @@\n-  _ref_processor(new ShenandoahReferenceProcessor(MAX2(_max_workers, 1U))),\n@@ -513,1 +556,1 @@\n-  \/\/ Initialize GC mode early, so we can adjust barrier support\n+  \/\/ Initialize GC mode early, many subsequent initialization procedures depend on it\n@@ -515,15 +558,0 @@\n-  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));\n-\n-  _max_workers = MAX2(_max_workers, 1U);\n-  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n-  if (_workers == nullptr) {\n-    vm_exit_during_initialization(\"Failed necessary allocation.\");\n-  } else {\n-    _workers->initialize_workers();\n-  }\n-\n-  if (ParallelGCThreads > 1) {\n-    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\",\n-                                                ParallelGCThreads);\n-    _safepoint_workers->initialize_workers();\n-  }\n@@ -536,29 +564,0 @@\n-class ShenandoahResetBitmapTask : public WorkerTask {\n-private:\n-  ShenandoahRegionIterator _regions;\n-\n-public:\n-  ShenandoahResetBitmapTask() :\n-    WorkerTask(\"Shenandoah Reset Bitmap\") {}\n-\n-  void work(uint worker_id) {\n-    ShenandoahHeapRegion* region = _regions.next();\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahMarkingContext* const ctx = heap->marking_context();\n-    while (region != nullptr) {\n-      if (heap->is_bitmap_slice_committed(region)) {\n-        ctx->clear_bitmap(region);\n-      }\n-      region = _regions.next();\n-    }\n-  }\n-};\n-\n-void ShenandoahHeap::reset_mark_bitmap() {\n-  assert_gc_workers(_workers->active_workers());\n-  mark_incomplete_marking_context();\n-\n-  ShenandoahResetBitmapTask task;\n-  _workers->run_task(&task);\n-}\n-\n@@ -579,1 +578,2 @@\n-  if (is_concurrent_mark_in_progress())        st->print(\"marking, \");\n+  if (is_concurrent_old_mark_in_progress())    st->print(\"old marking, \");\n+  if (is_concurrent_young_mark_in_progress())  st->print(\"young marking, \");\n@@ -630,0 +630,2 @@\n+  _mmu_tracker.initialize();\n+\n@@ -643,2 +645,0 @@\n-  _heuristics->initialize();\n-\n@@ -648,0 +648,4 @@\n+ShenandoahHeuristics* ShenandoahHeap::heuristics() {\n+  return _global_generation->heuristics();\n+}\n+\n@@ -649,1 +653,1 @@\n-  return Atomic::load(&_used);\n+  return global_generation()->used();\n@@ -656,4 +660,0 @@\n-size_t ShenandoahHeap::available() const {\n-  return free_set()->available();\n-}\n-\n@@ -670,2 +670,43 @@\n-void ShenandoahHeap::increase_used(size_t bytes) {\n-  Atomic::add(&_used, bytes, memory_order_relaxed);\n+\/\/ For tracking usage based on allocations, it should be the case that:\n+\/\/ * The sum of regions::used == heap::used\n+\/\/ * The sum of a generation's regions::used == generation::used\n+\/\/ * The sum of a generation's humongous regions::free == generation::humongous_waste\n+\/\/ These invariants are checked by the verifier on GC safepoints.\n+\/\/\n+\/\/ Additional notes:\n+\/\/ * When a mutator's allocation request causes a region to be retired, the\n+\/\/   free memory left in that region is considered waste. It does not contribute\n+\/\/   to the usage, but it _does_ contribute to allocation rate.\n+\/\/ * The bottom of a PLAB must be aligned on card size. In some cases this will\n+\/\/   require padding in front of the PLAB (a filler object). Because this padding\n+\/\/   is included in the region's used memory we include the padding in the usage\n+\/\/   accounting as waste.\n+\/\/ * Mutator allocations are used to compute an allocation rate. They are also\n+\/\/   sent to the Pacer for those purposes.\n+\/\/ * There are three sources of waste:\n+\/\/  1. The padding used to align a PLAB on card size\n+\/\/  2. Region's free is less than minimum TLAB size and is retired\n+\/\/  3. The unused portion of memory in the last region of a humongous object\n+void ShenandoahHeap::increase_used(const ShenandoahAllocRequest& req) {\n+  size_t actual_bytes = req.actual_size() * HeapWordSize;\n+  size_t wasted_bytes = req.waste() * HeapWordSize;\n+  ShenandoahGeneration* generation = generation_for(req.affiliation());\n+\n+  if (req.is_gc_alloc()) {\n+    assert(wasted_bytes == 0 || req.type() == ShenandoahAllocRequest::_alloc_plab, \"Only PLABs have waste\");\n+    increase_used(generation, actual_bytes + wasted_bytes);\n+  } else {\n+    assert(req.is_mutator_alloc(), \"Expected mutator alloc here\");\n+    \/\/ padding and actual size both count towards allocation counter\n+    generation->increase_allocated(actual_bytes + wasted_bytes);\n+\n+    \/\/ only actual size counts toward usage for mutator allocations\n+    increase_used(generation, actual_bytes);\n+\n+    \/\/ notify pacer of both actual size and waste\n+    notify_mutator_alloc_words(req.actual_size(), req.waste());\n+\n+    if (wasted_bytes > 0 && ShenandoahHeapRegion::requires_humongous(req.actual_size())) {\n+      increase_humongous_waste(generation,wasted_bytes);\n+    }\n+  }\n@@ -674,2 +715,5 @@\n-void ShenandoahHeap::set_used(size_t bytes) {\n-  Atomic::store(&_used, bytes);\n+void ShenandoahHeap::increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_humongous_waste(bytes);\n+  }\n@@ -678,3 +722,5 @@\n-void ShenandoahHeap::decrease_used(size_t bytes) {\n-  assert(used() >= bytes, \"never decrease heap size by more than we've left\");\n-  Atomic::sub(&_used, bytes, memory_order_relaxed);\n+void ShenandoahHeap::decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_humongous_waste(bytes);\n+  }\n@@ -683,2 +729,5 @@\n-void ShenandoahHeap::increase_allocated(size_t bytes) {\n-  Atomic::add(&_bytes_allocated_since_gc_start, bytes, memory_order_relaxed);\n+void ShenandoahHeap::increase_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_used(bytes);\n+  }\n@@ -687,4 +736,4 @@\n-void ShenandoahHeap::notify_mutator_alloc_words(size_t words, bool waste) {\n-  size_t bytes = words * HeapWordSize;\n-  if (!waste) {\n-    increase_used(bytes);\n+void ShenandoahHeap::decrease_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_used(bytes);\n@@ -692,1 +741,3 @@\n-  increase_allocated(bytes);\n+}\n+\n+void ShenandoahHeap::notify_mutator_alloc_words(size_t words, size_t waste) {\n@@ -695,2 +746,2 @@\n-    if (waste) {\n-      pacer()->claim_for_alloc(words, true);\n+    if (waste > 0) {\n+      pacer()->claim_for_alloc(waste, true);\n@@ -826,3 +877,1 @@\n-\n-  \/\/ This is called from allocation path, and thus should be fast.\n-  _heap_changed.try_set();\n+  _heap_changed.set();\n@@ -845,0 +894,1 @@\n+\n@@ -851,0 +901,1 @@\n+  log_debug(gc, free)(\"Set new GCLAB size: \" SIZE_FORMAT, new_size);\n@@ -856,0 +907,1 @@\n+    log_debug(gc, free)(\"New gclab size (\" SIZE_FORMAT \") is too small for \" SIZE_FORMAT, new_size, size);\n@@ -885,0 +937,1 @@\n+\/\/ Called from stubs in JIT code or interpreter\n@@ -933,0 +986,1 @@\n+    \/\/ gc_no_progress_count is incremented following each degen or full GC that fails to achieve is_good_progress().\n@@ -935,0 +989,1 @@\n+      req.set_actual_size(0);\n@@ -949,2 +1004,0 @@\n-      \/\/\n-      \/\/ TODO: Consider GLOBAL GC rather than Full GC to remediate OOM condition: https:\/\/bugs.openjdk.org\/browse\/JDK-8335910\n@@ -961,1 +1014,1 @@\n-      if (log_is_enabled(Debug, gc, alloc)) {\n+      if (log_develop_is_enabled(Debug, gc, alloc)) {\n@@ -980,0 +1033,8 @@\n+  if (result == nullptr) {\n+    req.set_actual_size(0);\n+  }\n+\n+  \/\/ This is called regardless of the outcome of the allocation to account\n+  \/\/ for any waste created by retiring regions with this request.\n+  increase_used(req);\n+\n@@ -989,2 +1050,0 @@\n-      notify_mutator_alloc_words(actual, false);\n-\n@@ -997,2 +1056,0 @@\n-    } else {\n-      increase_used(actual*HeapWordSize);\n@@ -1011,1 +1068,37 @@\n-  return _free_set->allocate(req, in_new_region);\n+\n+  \/\/ Make sure the old generation has room for either evacuations or promotions before trying to allocate.\n+  if (req.is_old() && !old_generation()->can_allocate(req)) {\n+    return nullptr;\n+  }\n+\n+  \/\/ If TLAB request size is greater than available, allocate() will attempt to downsize request to fit within available\n+  \/\/ memory.\n+  HeapWord* result = _free_set->allocate(req, in_new_region);\n+\n+  \/\/ Record the plab configuration for this result and register the object.\n+  if (result != nullptr && req.is_old()) {\n+    old_generation()->configure_plab_for_current_thread(req);\n+    if (req.type() == ShenandoahAllocRequest::_alloc_shared_gc) {\n+      \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+      \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+      \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+      \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+      \/\/\n+      \/\/ objects being \"concurrently\" allocated:\n+      \/\/    [-----a------][-----b-----][--------------c------------------]\n+      \/\/            [---- card table memory range --------------]\n+      \/\/\n+      \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n+      \/\/ wants to set the starts-object, first-start, and last-start attributes of the preceding card region.\n+      \/\/ Allocation of object b wants to set the starts-object, first-start, and last-start attributes of this card region.\n+      \/\/ Allocation of object c also wants to set the starts-object, first-start, and last-start attributes of this\n+      \/\/ card region.\n+      \/\/\n+      \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n+      \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n+      \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+      old_generation()->card_scan()->register_object(result);\n+    }\n+  }\n+\n+  return result;\n@@ -1026,2 +1119,2 @@\n-  if (heuristics()->can_unload_classes()) {\n-    ShenandoahHeuristics* h = heuristics();\n+  ShenandoahHeuristics* h = global_generation()->heuristics();\n+  if (h->can_unload_classes()) {\n@@ -1120,2 +1213,8 @@\n-  ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n-  workers()->run_task(&task);\n+  if (mode()->is_generational()) {\n+    ShenandoahRegionIterator regions;\n+    ShenandoahGenerationalEvacuationTask task(ShenandoahGenerationalHeap::heap(), &regions, concurrent);\n+    workers()->run_task(&task);\n+  } else {\n+    ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n+    workers()->run_task(&task);\n+  }\n@@ -1125,3 +1224,4 @@\n-  if (ShenandoahThreadLocalData::is_oom_during_evac(Thread::current())) {\n-    \/\/ This thread went through the OOM during evac protocol and it is safe to return\n-    \/\/ the forward pointer. It must not attempt to evacuate any more.\n+  assert(thread == Thread::current(), \"Expected thread parameter to be current thread.\");\n+  if (ShenandoahThreadLocalData::is_oom_during_evac(thread)) {\n+    \/\/ This thread went through the OOM during evac protocol. It is safe to return\n+    \/\/ the forward pointer. It must not attempt to evacuate any other objects.\n@@ -1133,1 +1233,2 @@\n-  size_t size = p->size();\n+  ShenandoahHeapRegion* r = heap_region_containing(p);\n+  assert(!r->is_humongous(), \"never evacuate humongous objects\");\n@@ -1135,1 +1236,3 @@\n-  assert(!heap_region_containing(p)->is_humongous(), \"never evacuate humongous objects\");\n+  ShenandoahAffiliation target_gen = r->affiliation();\n+  return try_evacuate_object(p, thread, r, target_gen);\n+}\n@@ -1137,1 +1240,5 @@\n-  bool alloc_from_gclab = true;\n+oop ShenandoahHeap::try_evacuate_object(oop p, Thread* thread, ShenandoahHeapRegion* from_region,\n+                                               ShenandoahAffiliation target_gen) {\n+  assert(target_gen == YOUNG_GENERATION, \"Only expect evacuations to young in this mode\");\n+  assert(from_region->is_young(), \"Only expect evacuations from young in this mode\");\n+  bool alloc_from_lab = true;\n@@ -1139,0 +1246,1 @@\n+  size_t size = p->size();\n@@ -1148,0 +1256,7 @@\n+      if ((copy == nullptr) && (size < ShenandoahThreadLocalData::gclab_size(thread))) {\n+        \/\/ GCLAB allocation failed because we are bumping up against the limit on young evacuation reserve.  Try resetting\n+        \/\/ the desired GCLAB size and retry GCLAB allocation to avoid cascading of shared memory allocations.\n+        ShenandoahThreadLocalData::set_gclab_size(thread, PLAB::min_size());\n+        copy = allocate_from_gclab(thread, size);\n+        \/\/ If we still get nullptr, we'll try a shared allocation below.\n+      }\n@@ -1149,0 +1264,1 @@\n+\n@@ -1150,1 +1266,2 @@\n-      ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size);\n+      \/\/ If we failed to allocate in LAB, we'll try a shared allocation.\n+      ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen);\n@@ -1152,1 +1269,1 @@\n-      alloc_from_gclab = false;\n+      alloc_from_lab = false;\n@@ -1167,0 +1284,1 @@\n+  _evac_tracker->begin_evacuation(thread, size * HeapWordSize);\n@@ -1175,0 +1293,1 @@\n+    _evac_tracker->end_evacuation(thread, size * HeapWordSize);\n@@ -1183,7 +1302,4 @@\n-    \/\/\n-    \/\/ For GCLAB allocations, it is enough to rollback the allocation ptr. Either the next\n-    \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n-    \/\/ do this. For non-GCLAB allocations, we have no way to retract the allocation, and\n-    \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n-    \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n-    if (alloc_from_gclab) {\n+    if (alloc_from_lab) {\n+      \/\/ For LAB allocations, it is enough to rollback the allocation ptr. Either the next\n+      \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n+      \/\/ do this.\n@@ -1192,0 +1308,4 @@\n+      \/\/ For non-LAB allocations, we have no way to retract the allocation, and\n+      \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n+      \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n+      assert(size >= ShenandoahHeap::min_fill_size(), \"previously allocated object known to be larger than min_size\");\n@@ -1194,0 +1314,1 @@\n+      \/\/ For non-LAB allocations, the object has already been registered\n@@ -1227,1 +1348,1 @@\n-void ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n+size_t ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n@@ -1247,0 +1368,1 @@\n+  return required_regions;\n@@ -1256,0 +1378,6 @@\n+\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+      assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+      assert(plab->words_remaining() == 0, \"PLAB should not need retirement\");\n+    }\n@@ -1271,0 +1399,13 @@\n+\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+      assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+\n+      \/\/ There are two reasons to retire all plabs between old-gen evacuation passes.\n+      \/\/  1. We need to make the plab memory parsable by remembered-set scanning.\n+      \/\/  2. We need to establish a trustworthy UpdateWaterMark value within each old-gen heap region\n+      ShenandoahGenerationalHeap::heap()->retire_plab(plab, thread);\n+      if (_resize && ShenandoahThreadLocalData::plab_size(thread) > 0) {\n+        ShenandoahThreadLocalData::set_plab_size(thread, 0);\n+      }\n+    }\n@@ -1400,0 +1541,3 @@\n+\n+    evac_tracker()->print_global_on(&ls);\n+\n@@ -1401,0 +1545,38 @@\n+    ls.cr();\n+  }\n+}\n+\n+void ShenandoahHeap::set_gc_generation(ShenandoahGeneration* generation) {\n+  shenandoah_assert_control_or_vm_thread_at_safepoint();\n+  _gc_generation = generation;\n+}\n+\n+\/\/ Active generation may only be set by the VM thread at a safepoint.\n+void ShenandoahHeap::set_active_generation() {\n+  assert(Thread::current()->is_VM_thread(), \"Only the VM Thread\");\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Only at a safepoint!\");\n+  assert(_gc_generation != nullptr, \"Will set _active_generation to nullptr\");\n+  _active_generation = _gc_generation;\n+}\n+\n+void ShenandoahHeap::on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation) {\n+  shenandoah_policy()->record_collection_cause(cause);\n+\n+  assert(gc_cause()  == GCCause::_no_gc, \"Over-writing cause\");\n+  assert(_gc_generation == nullptr, \"Over-writing _gc_generation\");\n+\n+  set_gc_cause(cause);\n+  set_gc_generation(generation);\n+\n+  generation->heuristics()->record_cycle_start();\n+}\n+\n+void ShenandoahHeap::on_cycle_end(ShenandoahGeneration* generation) {\n+  assert(gc_cause() != GCCause::_no_gc, \"cause wasn't set\");\n+  assert(_gc_generation != nullptr, \"_gc_generation wasn't set\");\n+\n+  generation->heuristics()->record_cycle_end();\n+  if (mode()->is_generational() && generation->is_global()) {\n+    \/\/ If we just completed a GLOBAL GC, claim credit for completion of young-gen and old-gen GC as well\n+    young_generation()->heuristics()->record_cycle_end();\n+    old_generation()->heuristics()->record_cycle_end();\n@@ -1402,0 +1584,3 @@\n+\n+  set_gc_generation(nullptr);\n+  set_gc_cause(GCCause::_no_gc);\n@@ -1753,99 +1938,0 @@\n-class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n-      \/\/ anyway to capture any updates that happened since now.\n-      r->clear_live_data();\n-      _ctx->capture_top_at_mark_start(r);\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_gc() {\n-  reset_mark_bitmap();\n-\n-  ShenandoahResetUpdateRegionStateClosure cl;\n-  parallel_heap_region_iterate(&cl);\n-}\n-\n-class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalMarkUpdateRegionStateClosure() :\n-    _ctx(ShenandoahHeap::heap()->complete_marking_context()), _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n-      \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n-      HeapWord *tams = _ctx->top_at_mark_start(r);\n-      HeapWord *top = r->top();\n-      if (top > tams) {\n-        r->increase_live_data_alloc_words(pointer_delta(top, tams));\n-      }\n-\n-      \/\/ We are about to select the collection set, make sure it knows about\n-      \/\/ current pinning status. Also, this allows trashing more regions that\n-      \/\/ now have their pinning status dropped.\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n-      }\n-\n-      \/\/ Remember limit for updating refs. It's guaranteed that we get no\n-      \/\/ from-space-refs written from here on.\n-      r->set_update_watermark_at_safepoint(r->top());\n-    } else {\n-      assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_regions_and_collection_set(bool concurrent) {\n-  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states :\n-                                         ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n-    ShenandoahFinalMarkUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n-\n-    assert_pinned_region_status();\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset :\n-                                         ShenandoahPhaseTimings::degen_gc_choose_cset);\n-    ShenandoahHeapLocker locker(lock());\n-    _collection_set->clear();\n-    heuristics()->choose_collection_set(_collection_set);\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset :\n-                                         ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n-  }\n-}\n-\n@@ -1854,0 +1940,3 @@\n+  if (mode()->is_generational()) {\n+    old_generation()->set_parsable(false);\n+  }\n@@ -1862,1 +1951,2 @@\n-  ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n+  shenandoah_assert_generations_reconciled();\n+  gc_generation()->ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n@@ -1896,0 +1986,37 @@\n+  \/\/ Check that if concurrent weak root is set then active_gen isn't null\n+  assert(!is_concurrent_weak_root_in_progress() || active_generation() != nullptr, \"Error\");\n+  shenandoah_assert_generations_reconciled();\n+}\n+\n+void ShenandoahHeap::set_concurrent_young_mark_in_progress(bool in_progress) {\n+  uint mask;\n+  assert(!has_forwarded_objects(), \"Young marking is not concurrent with evacuation\");\n+  if (!in_progress && is_concurrent_old_mark_in_progress()) {\n+    assert(mode()->is_generational(), \"Only generational GC has old marking\");\n+    assert(_gc_state.is_set(MARKING), \"concurrent_old_marking_in_progress implies MARKING\");\n+    \/\/ If old-marking is in progress when we turn off YOUNG_MARKING, leave MARKING (and OLD_MARKING) on\n+    mask = YOUNG_MARKING;\n+  } else {\n+    mask = MARKING | YOUNG_MARKING;\n+  }\n+  set_gc_state(mask, in_progress);\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::set_concurrent_old_mark_in_progress(bool in_progress) {\n+#ifdef ASSERT\n+  \/\/ has_forwarded_objects() iff UPDATEREFS or EVACUATION\n+  bool has_forwarded = has_forwarded_objects();\n+  bool updating_or_evacuating = _gc_state.is_set(UPDATEREFS | EVACUATION);\n+  bool evacuating = _gc_state.is_set(EVACUATION);\n+  assert ((has_forwarded == updating_or_evacuating) || (evacuating && !has_forwarded && collection_set()->is_empty()),\n+          \"Updating or evacuating iff has forwarded objects, or if evacuation phase is promoting in place without forwarding\");\n+#endif\n+  if (!in_progress && is_concurrent_young_mark_in_progress()) {\n+    \/\/ If young-marking is in progress when we turn off OLD_MARKING, leave MARKING (and YOUNG_MARKING) on\n+    assert(_gc_state.is_set(MARKING), \"concurrent_young_marking_in_progress implies MARKING\");\n+    set_gc_state(OLD_MARKING, in_progress);\n+  } else {\n+    set_gc_state(MARKING | OLD_MARKING, in_progress);\n+  }\n+  manage_satb_barrier(in_progress);\n@@ -1898,4 +2025,18 @@\n-void ShenandoahHeap::set_concurrent_mark_in_progress(bool in_progress) {\n-  assert(!has_forwarded_objects(), \"Not expected before\/after mark phase\");\n-  set_gc_state(MARKING, in_progress);\n-  ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(in_progress, !in_progress);\n+bool ShenandoahHeap::is_prepare_for_old_mark_in_progress() const {\n+  return old_generation()->is_preparing_for_mark();\n+}\n+\n+void ShenandoahHeap::manage_satb_barrier(bool active) {\n+  if (is_concurrent_mark_in_progress()) {\n+    \/\/ Ignore request to deactivate barrier while concurrent mark is in progress.\n+    \/\/ Do not attempt to re-activate the barrier if it is already active.\n+    if (active && !ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  } else {\n+    \/\/ No concurrent marking is in progress so honor request to deactivate,\n+    \/\/ but only if the barrier is already active.\n+    if (!active && ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  }\n@@ -1934,0 +2075,11 @@\n+void ShenandoahHeap::cancel_concurrent_mark() {\n+  if (mode()->is_generational()) {\n+    young_generation()->cancel_marking();\n+    old_generation()->cancel_marking();\n+  }\n+\n+  global_generation()->cancel_marking();\n+\n+  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+}\n+\n@@ -1939,0 +2091,1 @@\n+    _cancel_requested_time = os::elapsedTime();\n@@ -2062,5 +2215,6 @@\n-size_t ShenandoahHeap::bytes_allocated_since_gc_start() const {\n-  return Atomic::load(&_bytes_allocated_since_gc_start);\n-}\n-\n-  Atomic::store(&_bytes_allocated_since_gc_start, (size_t)0);\n+  if (mode()->is_generational()) {\n+    young_generation()->reset_bytes_allocated_since_gc_start();\n+    old_generation()->reset_bytes_allocated_since_gc_start();\n+  }\n+\n+  global_generation()->reset_bytes_allocated_since_gc_start();\n@@ -2131,2 +2285,5 @@\n-    assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n-           \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    shenandoah_assert_generations_reconciled();\n+    if (gc_generation()->contains(r)) {\n+      assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n+             \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    }\n@@ -2187,1 +2344,1 @@\n-  ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n+  explicit ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n@@ -2207,1 +2364,0 @@\n-    T cl;\n@@ -2212,2 +2368,5 @@\n-      \/\/ We cannot transfer any more regions than will be reclaimed when the existing collection set is recycled because\n-      \/\/ we need the reclaimed collection set regions to replenish the collector reserves\n+\n+      \/\/ Now that evacuation is done, we can reassign any regions that had been reserved to hold the results of evacuation\n+      \/\/ to the mutator free set.  At the end of GC, we will have cset_regions newly evacuated fully empty regions from\n+      \/\/ which we will be able to replenish the Collector free set and the OldCollector free set in preparation for the\n+      \/\/ next GC cycle.\n@@ -2217,1 +2376,1 @@\n-\n+    T cl;\n@@ -2219,1 +2378,0 @@\n-    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n@@ -2225,3 +2383,3 @@\n-      }\n-      if (ShenandoahPacing) {\n-        _heap->pacer()->report_updaterefs(pointer_delta(update_watermark, r->bottom()));\n+        if (ShenandoahPacing) {\n+          _heap->pacer()->report_updaterefs(pointer_delta(update_watermark, r->bottom()));\n+        }\n@@ -2249,0 +2407,1 @@\n+ShenandoahSynchronizePinnedRegionStates::ShenandoahSynchronizePinnedRegionStates() : _lock(ShenandoahHeap::heap()->lock()) { }\n@@ -2250,22 +2409,13 @@\n-class ShenandoahFinalUpdateRefsUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalUpdateRefsUpdateRegionStateClosure() : _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    \/\/ Drop unnecessary \"pinned\" state from regions that does not have CP marks\n-    \/\/ anymore, as this would allow trashing them.\n-\n-    if (r->is_active()) {\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n+void ShenandoahSynchronizePinnedRegionStates::heap_region_do(ShenandoahHeapRegion* r) {\n+  \/\/ Drop \"pinned\" state from regions that no longer have a pinned count. Put\n+  \/\/ regions with a pinned count into the \"pinned\" state.\n+  if (r->is_active()) {\n+    if (r->is_pinned()) {\n+      if (r->pin_count() == 0) {\n+        ShenandoahHeapLocker locker(_lock);\n+        r->make_unpinned();\n+      }\n+    } else {\n+      if (r->pin_count() > 0) {\n+        ShenandoahHeapLocker locker(_lock);\n+        r->make_pinned();\n@@ -2275,3 +2425,1 @@\n-\n-  bool is_thread_safe() { return true; }\n-};\n+}\n@@ -2287,2 +2435,2 @@\n-    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n+\n+    final_update_refs_update_region_states();\n@@ -2301,0 +2449,5 @@\n+void ShenandoahHeap::final_update_refs_update_region_states() {\n+  ShenandoahSynchronizePinnedRegionStates cl;\n+  parallel_heap_region_iterate(&cl);\n+}\n+\n@@ -2302,6 +2455,42 @@\n-  {\n-    ShenandoahGCPhase phase(concurrent ?\n-                            ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n-                            ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n+  ShenandoahGCPhase phase(concurrent ?\n+                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n+                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+  ShenandoahHeapLocker locker(lock());\n+  size_t young_cset_regions, old_cset_regions;\n+  size_t first_old_region, last_old_region, old_region_count;\n+  _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+  \/\/ If there are no old regions, first_old_region will be greater than last_old_region\n+  assert((first_old_region > last_old_region) ||\n+         ((last_old_region + 1 - first_old_region >= old_region_count) &&\n+          get_region(first_old_region)->is_old() && get_region(last_old_region)->is_old()),\n+         \"sanity: old_region_count: \" SIZE_FORMAT \", first_old_region: \" SIZE_FORMAT \", last_old_region: \" SIZE_FORMAT,\n+         old_region_count, first_old_region, last_old_region);\n+\n+  if (mode()->is_generational()) {\n+#ifdef ASSERT\n+    if (ShenandoahVerify) {\n+      verifier()->verify_before_rebuilding_free_set();\n+    }\n+#endif\n+\n+    \/\/ The computation of bytes_of_allocation_runway_before_gc_trigger is quite conservative so consider all of this\n+    \/\/ available for transfer to old. Note that transfer of humongous regions does not impact available.\n+    ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+    size_t allocation_runway = gen_heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions);\n+    gen_heap->compute_old_generation_balance(allocation_runway, old_cset_regions);\n+\n+    \/\/ Total old_available may have been expanded to hold anticipated promotions.  We trigger if the fragmented available\n+    \/\/ memory represents more than 16 regions worth of data.  Note that fragmentation may increase when we promote regular\n+    \/\/ regions in place when many of these regular regions have an abundant amount of available memory within them.  Fragmentation\n+    \/\/ will decrease as promote-by-copy consumes the available memory within these partially consumed regions.\n+    \/\/\n+    \/\/ We consider old-gen to have excessive fragmentation if more than 12.5% of old-gen is free memory that resides\n+    \/\/ within partially consumed regions of memory.\n+  }\n+  \/\/ Rebuild free set based on adjusted generation sizes.\n+  _free_set->finish_rebuild(young_cset_regions, old_cset_regions, old_region_count);\n+\n+  if (mode()->is_generational()) {\n+    ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+    ShenandoahOldGeneration* old_gen = gen_heap->old_generation();\n+    old_gen->heuristics()->trigger_maybe(first_old_region, last_old_region, old_region_count, num_regions());\n@@ -2430,1 +2619,1 @@\n-  return _memory_pool->get_memory_usage();\n+  return MemoryUsage(_initial_size, used(), committed(), max_capacity());\n@@ -2572,0 +2761,23 @@\n+\n+ShenandoahGeneration* ShenandoahHeap::generation_for(ShenandoahAffiliation affiliation) const {\n+  if (!mode()->is_generational()) {\n+    return global_generation();\n+  } else if (affiliation == YOUNG_GENERATION) {\n+    return young_generation();\n+  } else if (affiliation == OLD_GENERATION) {\n+    return old_generation();\n+  }\n+\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n+\n+void ShenandoahHeap::log_heap_status(const char* msg) const {\n+  if (mode()->is_generational()) {\n+    young_generation()->log_status(msg);\n+    old_generation()->log_status(msg);\n+  } else {\n+    global_generation()->log_status(msg);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":494,"deletions":282,"binary":false,"changes":776,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -33,1 +34,2 @@\n-#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n+#include \"gc\/shenandoah\/shenandoahController.hpp\"\n@@ -37,0 +39,5 @@\n+#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationSizer.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMmuTracker.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -48,1 +55,0 @@\n-class ShenandoahControlThread;\n@@ -51,0 +57,3 @@\n+class ShenandoahGeneration;\n+class ShenandoahYoungGeneration;\n+class ShenandoahOldGeneration;\n@@ -116,0 +125,11 @@\n+class ShenandoahSynchronizePinnedRegionStates : public ShenandoahHeapRegionClosure {\n+private:\n+  ShenandoahHeapLock* const _lock;\n+\n+public:\n+  ShenandoahSynchronizePinnedRegionStates();\n+\n+  void heap_region_do(ShenandoahHeapRegion* r) override;\n+  bool is_thread_safe() override { return true; }\n+};\n+\n@@ -120,1 +140,1 @@\n-class ShenandoahHeap : public CollectedHeap, public ShenandoahSpaceInfo {\n+class ShenandoahHeap : public CollectedHeap {\n@@ -130,0 +150,1 @@\n+  friend class ShenandoahOldGC;\n@@ -139,0 +160,13 @@\n+  \/\/ Indicates the generation whose collection is in\n+  \/\/ progress. Mutator threads aren't allowed to read\n+  \/\/ this field.\n+  ShenandoahGeneration* _gc_generation;\n+\n+  \/\/ This is set and cleared by only the VMThread\n+  \/\/ at each STW pause (safepoint) to the value seen in\n+  \/\/ _gc_generation. This allows the value to be always consistently\n+  \/\/ seen by all mutators as well as all GC worker threads.\n+  \/\/ In that sense, it's a stable snapshot of _gc_generation that is\n+  \/\/ updated at each STW pause associated with a ShenandoahVMOp.\n+  ShenandoahGeneration* _active_generation;\n+\n@@ -144,0 +178,22 @@\n+  ShenandoahGeneration* gc_generation() const {\n+    \/\/ We don't want this field read by a mutator thread\n+    assert(!Thread::current()->is_Java_thread(), \"Not allowed\");\n+    \/\/ value of _gc_generation field, see above\n+    return _gc_generation;\n+  }\n+\n+  ShenandoahGeneration* active_generation() const {\n+    \/\/ value of _active_generation field, see above\n+    return _active_generation;\n+  }\n+\n+  \/\/ Set the _gc_generation field\n+  void set_gc_generation(ShenandoahGeneration* generation);\n+\n+  \/\/ Copy the value in the _gc_generation field into\n+  \/\/ the _active_generation field: can only be called at\n+  \/\/ a safepoint by the VMThread.\n+  void set_active_generation();\n+\n+  ShenandoahHeuristics* heuristics();\n+\n@@ -156,2 +212,2 @@\n-  void initialize_heuristics();\n-\n+  virtual void initialize_heuristics();\n+  virtual void print_init_logger() const;\n@@ -178,2 +234,3 @@\n-           size_t _initial_size;\n-           size_t _minimum_size;\n+  size_t _initial_size;\n+  size_t _minimum_size;\n+\n@@ -182,2 +239,0 @@\n-  volatile size_t _used;\n-  volatile size_t _bytes_allocated_since_gc_start;\n@@ -187,0 +242,2 @@\n+  void increase_used(const ShenandoahAllocRequest& req);\n+\n@@ -188,3 +245,4 @@\n-  void increase_used(size_t bytes);\n-  void decrease_used(size_t bytes);\n-  void set_used(size_t bytes);\n+  void increase_used(ShenandoahGeneration* generation, size_t bytes);\n+  void decrease_used(ShenandoahGeneration* generation, size_t bytes);\n+  void increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes);\n+  void decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes);\n@@ -194,2 +252,0 @@\n-  void increase_allocated(size_t bytes);\n-  size_t bytes_allocated_since_gc_start() const override;\n@@ -201,1 +257,1 @@\n-  size_t soft_max_capacity() const override;\n+  size_t soft_max_capacity() const;\n@@ -206,1 +262,0 @@\n-  size_t available()         const override;\n@@ -226,0 +281,2 @@\n+  virtual void initialize_controller();\n+\n@@ -242,1 +299,1 @@\n-  ShenandoahRegionIterator _update_refs_iterator;\n+  uint8_t* _affiliations;       \/\/ Holds array of enum ShenandoahAffiliation, including FREE status in non-generational mode\n@@ -259,0 +316,2 @@\n+  inline ShenandoahMmuTracker* mmu_tracker() { return &_mmu_tracker; };\n+\n@@ -274,0 +333,1 @@\n+    \/\/ For generational mode, it means either young or old marking, or both.\n@@ -284,0 +344,6 @@\n+\n+    \/\/ Young regions are under marking, need SATB barriers.\n+    YOUNG_MARKING_BITPOS = 5,\n+\n+    \/\/ Old regions are under marking, need SATB barriers.\n+    OLD_MARKING_BITPOS = 6\n@@ -293,0 +359,2 @@\n+    YOUNG_MARKING = 1 << YOUNG_MARKING_BITPOS,\n+    OLD_MARKING   = 1 << OLD_MARKING_BITPOS\n@@ -298,2 +366,0 @@\n-\n-  \/\/ tracks if new regions have been allocated or retired since last check\n@@ -328,1 +394,2 @@\n-  void set_concurrent_mark_in_progress(bool in_progress);\n+  void set_concurrent_young_mark_in_progress(bool in_progress);\n+  void set_concurrent_old_mark_in_progress(bool in_progress);\n@@ -340,0 +407,1 @@\n+\n@@ -341,0 +409,2 @@\n+  inline bool is_concurrent_young_mark_in_progress() const;\n+  inline bool is_concurrent_old_mark_in_progress() const;\n@@ -351,0 +421,1 @@\n+  bool is_prepare_for_old_mark_in_progress() const;\n@@ -353,0 +424,2 @@\n+  void manage_satb_barrier(bool active);\n+\n@@ -363,0 +436,1 @@\n+  double _cancel_requested_time;\n@@ -364,0 +438,5 @@\n+\n+  \/\/ Returns true if cancel request was successfully communicated.\n+  \/\/ Returns false if some other thread already communicated cancel\n+  \/\/ request.  A true return value does not mean GC has been\n+  \/\/ cancelled, only that the process of cancelling GC has begun.\n@@ -367,1 +446,0 @@\n-\n@@ -371,1 +449,1 @@\n-  inline void clear_cancelled_gc();\n+  inline void clear_cancelled_gc(bool clear_oom_handler = true);\n@@ -373,0 +451,1 @@\n+  void cancel_concurrent_mark();\n@@ -384,0 +463,6 @@\n+protected:\n+  \/\/ This is shared between shConcurrentGC and shDegenerateGC so that degenerated\n+  \/\/ GC can resume update refs from where the concurrent GC was cancelled. It is\n+  \/\/ also used in shGenerationalHeap, which uses a different closure for update refs.\n+  ShenandoahRegionIterator _update_refs_iterator;\n+\n@@ -386,3 +471,0 @@\n-  \/\/ Reset bitmap, prepare regions for new GC cycle\n-  void prepare_gc();\n-  void prepare_regions_and_collection_set(bool concurrent);\n@@ -398,1 +480,1 @@\n-  void update_heap_references(bool concurrent);\n+  virtual void update_heap_references(bool concurrent);\n@@ -401,1 +483,1 @@\n-  void rebuild_free_set(bool concurrent);\n+  virtual void final_update_refs_update_region_states();\n@@ -406,0 +488,1 @@\n+  void rebuild_free_set(bool concurrent);\n@@ -413,1 +496,9 @@\n-  ShenandoahControlThread*   _control_thread;\n+  ShenandoahGeneration*  _global_generation;\n+\n+protected:\n+  ShenandoahController*  _control_thread;\n+\n+  ShenandoahYoungGeneration* _young_generation;\n+  ShenandoahOldGeneration*   _old_generation;\n+\n+private:\n@@ -416,1 +507,0 @@\n-  ShenandoahHeuristics*      _heuristics;\n@@ -421,3 +511,3 @@\n-  ShenandoahPhaseTimings*    _phase_timings;\n-\n-  ShenandoahControlThread*   control_thread()          { return _control_thread;    }\n+  ShenandoahPhaseTimings*       _phase_timings;\n+  ShenandoahEvacuationTracker*  _evac_tracker;\n+  ShenandoahMmuTracker          _mmu_tracker;\n@@ -426,0 +516,15 @@\n+  ShenandoahController*   control_thread() { return _control_thread; }\n+\n+  ShenandoahGeneration*      global_generation() const { return _global_generation; }\n+  ShenandoahYoungGeneration* young_generation()  const {\n+    assert(mode()->is_generational(), \"Young generation requires generational mode\");\n+    return _young_generation;\n+  }\n+\n+  ShenandoahOldGeneration*   old_generation()    const {\n+    assert(mode()->is_generational(), \"Old generation requires generational mode\");\n+    return _old_generation;\n+  }\n+\n+  ShenandoahGeneration*      generation_for(ShenandoahAffiliation affiliation) const;\n+\n@@ -428,1 +533,0 @@\n-  ShenandoahHeuristics*      heuristics()        const { return _heuristics;        }\n@@ -432,1 +536,7 @@\n-  ShenandoahPhaseTimings*    phase_timings()     const { return _phase_timings;     }\n+  ShenandoahPhaseTimings*      phase_timings()   const { return _phase_timings;     }\n+  ShenandoahEvacuationTracker* evac_tracker()    const { return _evac_tracker;      }\n+\n+  ShenandoahEvacOOMHandler* oom_evac_handler() { return &_oom_evac_handler; }\n+\n+  void on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation);\n+  void on_cycle_end(ShenandoahGeneration* generation);\n@@ -447,1 +557,1 @@\n-  ShenandoahMonitoringSupport* monitoring_support()          { return _monitoring_support;    }\n+  ShenandoahMonitoringSupport* monitoring_support() const    { return _monitoring_support;    }\n@@ -457,8 +567,0 @@\n-\/\/ ---------- Reference processing\n-\/\/\n-private:\n-  ShenandoahReferenceProcessor* const _ref_processor;\n-\n-public:\n-  ShenandoahReferenceProcessor* ref_processor() { return _ref_processor; }\n-\n@@ -483,0 +585,3 @@\n+  inline void assert_lock_for_affiliation(ShenandoahAffiliation orig_affiliation,\n+                                          ShenandoahAffiliation new_affiliation);\n+\n@@ -500,0 +605,10 @@\n+  inline bool is_in_active_generation(oop obj) const;\n+  inline bool is_in_young(const void* p) const;\n+  inline bool is_in_old(const void* p) const;\n+  inline bool is_old(oop pobj) const;\n+\n+  inline ShenandoahAffiliation region_affiliation(const ShenandoahHeapRegion* r);\n+  inline void set_affiliation(ShenandoahHeapRegion* r, ShenandoahAffiliation new_affiliation);\n+\n+  inline ShenandoahAffiliation region_affiliation(size_t index);\n+\n@@ -546,1 +661,1 @@\n-  bool can_load_archived_objects() const override { return UseCompressedOops; }\n+  bool can_load_archived_objects() const override { return UseCompressedOops && !ShenandoahCardBarrier; }\n@@ -552,0 +667,3 @@\n+protected:\n+  inline HeapWord* allocate_from_gclab(Thread* thread, size_t size);\n+\n@@ -554,1 +672,0 @@\n-  inline HeapWord* allocate_from_gclab(Thread* thread, size_t size);\n@@ -565,1 +682,1 @@\n-  void notify_mutator_alloc_words(size_t words, bool waste);\n+  void notify_mutator_alloc_words(size_t words, size_t waste);\n@@ -603,2 +720,0 @@\n-  inline void mark_complete_marking_context();\n-  inline void mark_incomplete_marking_context();\n@@ -615,2 +730,0 @@\n-  void reset_mark_bitmap();\n-\n@@ -637,0 +750,1 @@\n+  oop try_evacuate_object(oop src, Thread* thread, ShenandoahHeapRegion* from_region, ShenandoahAffiliation target_gen);\n@@ -638,0 +752,1 @@\n+\n@@ -648,1 +763,1 @@\n-  \/\/ Evacuates object src. Returns the evacuated object, either evacuated\n+  \/\/ Evacuates or promotes object src. Returns the evacuated object, either evacuated\n@@ -650,1 +765,1 @@\n-  oop evacuate_object(oop src, Thread* thread);\n+  virtual oop evacuate_object(oop src, Thread* thread);\n@@ -677,1 +792,10 @@\n-  void trash_humongous_region_at(ShenandoahHeapRegion *r);\n+  size_t trash_humongous_region_at(ShenandoahHeapRegion *r);\n+\n+  static inline void increase_object_age(oop obj, uint additional_age);\n+\n+  \/\/ Return the object's age, or a sentinel value when the age can't\n+  \/\/ necessarily be determined because of concurrent locking by the\n+  \/\/ mutator\n+  static inline uint get_object_age(oop obj);\n+\n+  void log_heap_status(const char *msg) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":177,"deletions":53,"binary":false,"changes":230,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -43,1 +44,1 @@\n-#include \"gc\/shenandoah\/shenandoahControlThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -46,0 +47,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -51,0 +53,1 @@\n+#include \"runtime\/objectMonitor.inline.hpp\"\n@@ -267,1 +270,1 @@\n-inline void ShenandoahHeap::clear_cancelled_gc() {\n+inline void ShenandoahHeap::clear_cancelled_gc(bool clear_oom_handler) {\n@@ -269,1 +272,9 @@\n-  _oom_evac_handler.clear();\n+  if (_cancel_requested_time > 0) {\n+    double cancel_time = os::elapsedTime() - _cancel_requested_time;\n+    log_info(gc)(\"GC cancellation took %.3fs\", cancel_time);\n+    _cancel_requested_time = 0;\n+  }\n+\n+  if (clear_oom_handler) {\n+    _oom_evac_handler.clear();\n+  }\n@@ -286,1 +297,0 @@\n-  \/\/ Otherwise...\n@@ -290,0 +300,136 @@\n+void ShenandoahHeap::increase_object_age(oop obj, uint additional_age) {\n+  \/\/ This operates on new copy of an object. This means that the object's mark-word\n+  \/\/ is thread-local and therefore safe to access. However, when the mark is\n+  \/\/ displaced (i.e. stack-locked or monitor-locked), then it must be considered\n+  \/\/ a shared memory location. It can be accessed by other threads.\n+  \/\/ In particular, a competing evacuating thread can succeed to install its copy\n+  \/\/ as the forwardee and continue to unlock the object, at which point 'our'\n+  \/\/ write to the foreign stack-location would potentially over-write random\n+  \/\/ information on that stack. Writing to a monitor is less problematic,\n+  \/\/ but still not safe: while the ObjectMonitor would not randomly disappear,\n+  \/\/ the other thread would also write to the same displaced header location,\n+  \/\/ possibly leading to increase the age twice.\n+  \/\/ For all these reasons, we take the conservative approach and not attempt\n+  \/\/ to increase the age when the header is displaced.\n+  markWord w = obj->mark();\n+  \/\/ The mark-word has been copied from the original object. It can not be\n+  \/\/ inflating, because inflation can not be interrupted by a safepoint,\n+  \/\/ and after a safepoint, a Java thread would first have to successfully\n+  \/\/ evacuate the object before it could inflate the monitor.\n+  assert(!w.is_being_inflated() || LockingMode == LM_LIGHTWEIGHT, \"must not inflate monitor before evacuation of object succeeds\");\n+  \/\/ It is possible that we have copied the object after another thread has\n+  \/\/ already successfully completed evacuation. While harmless (we would never\n+  \/\/ publish our copy), don't even attempt to modify the age when that\n+  \/\/ happens.\n+  if (!w.has_displaced_mark_helper() && !w.is_marked()) {\n+    w = w.set_age(MIN2(markWord::max_age, w.age() + additional_age));\n+    obj->set_mark(w);\n+  }\n+}\n+\n+\/\/ Return the object's age, or a sentinel value when the age can't\n+\/\/ necessarily be determined because of concurrent locking by the\n+\/\/ mutator\n+uint ShenandoahHeap::get_object_age(oop obj) {\n+  \/\/ This is impossible to do unless we \"freeze\" ABA-type oscillations\n+  \/\/ With Lilliput, we can do this more easily.\n+  markWord w = obj->mark();\n+  assert(!w.is_marked(), \"must not be forwarded\");\n+  if (w.has_monitor()) {\n+    w = w.monitor()->header();\n+  } else if (w.is_being_inflated() || w.has_displaced_mark_helper()) {\n+    \/\/ Informs caller that we aren't able to determine the age\n+    return markWord::max_age + 1; \/\/ sentinel\n+  }\n+  assert(w.age() <= markWord::max_age, \"Impossible!\");\n+  return w.age();\n+}\n+\n+inline bool ShenandoahHeap::is_in_active_generation(oop obj) const {\n+  if (!mode()->is_generational()) {\n+    \/\/ everything is the same single generation\n+    assert(is_in_reserved(obj), \"Otherwise shouldn't return true below\");\n+    return true;\n+  }\n+\n+  ShenandoahGeneration* const gen = active_generation();\n+\n+  if (gen == nullptr) {\n+    \/\/ no collection is happening: only expect this to be called\n+    \/\/ when concurrent processing is active, but that could change\n+    return false;\n+  }\n+\n+  assert(is_in_reserved(obj), \"only check if is in active generation for objects (\" PTR_FORMAT \") in heap\", p2i(obj));\n+  assert(gen->is_old() || gen->is_young() || gen->is_global(),\n+         \"Active generation must be old, young, or global\");\n+\n+  size_t index = heap_region_containing(obj)->index();\n+\n+  \/\/ No flickering!\n+  assert(gen == active_generation(), \"Race?\");\n+\n+  switch (_affiliations[index]) {\n+  case ShenandoahAffiliation::FREE:\n+    \/\/ Free regions are in Old, Young, Global\n+    return true;\n+  case ShenandoahAffiliation::YOUNG_GENERATION:\n+    \/\/ Young regions are in young_generation and global_generation, not in old_generation\n+    return gen != (ShenandoahGeneration*)old_generation();\n+  case ShenandoahAffiliation::OLD_GENERATION:\n+    \/\/ Old regions are in old_generation and global_generation, not in young_generation\n+    return gen != (ShenandoahGeneration*)young_generation();\n+  default:\n+    assert(false, \"Bad affiliation (%d) for region \" SIZE_FORMAT, _affiliations[index], index);\n+    return false;\n+  }\n+}\n+\n+inline bool ShenandoahHeap::is_in_young(const void* p) const {\n+  return is_in_reserved(p) && (_affiliations[heap_region_index_containing(p)] == ShenandoahAffiliation::YOUNG_GENERATION);\n+}\n+\n+inline bool ShenandoahHeap::is_in_old(const void* p) const {\n+  return is_in_reserved(p) && (_affiliations[heap_region_index_containing(p)] == ShenandoahAffiliation::OLD_GENERATION);\n+}\n+\n+inline bool ShenandoahHeap::is_old(oop obj) const {\n+  return active_generation()->is_young() && is_in_old(obj);\n+}\n+\n+inline ShenandoahAffiliation ShenandoahHeap::region_affiliation(const ShenandoahHeapRegion *r) {\n+  return (ShenandoahAffiliation) _affiliations[r->index()];\n+}\n+\n+inline void ShenandoahHeap::assert_lock_for_affiliation(ShenandoahAffiliation orig_affiliation,\n+                                                        ShenandoahAffiliation new_affiliation) {\n+  \/\/ A lock is required when changing from FREE to NON-FREE.  Though it may be possible to elide the lock when\n+  \/\/ transitioning from in-use to FREE, the current implementation uses a lock for this transition.  A lock is\n+  \/\/ not required to change from YOUNG to OLD (i.e. when promoting humongous region).\n+  \/\/\n+  \/\/         new_affiliation is:     FREE   YOUNG   OLD\n+  \/\/  orig_affiliation is:  FREE      X       L      L\n+  \/\/                       YOUNG      L       X\n+  \/\/                         OLD      L       X      X\n+  \/\/  X means state transition won't happen (so don't care)\n+  \/\/  L means lock should be held\n+  \/\/  Blank means no lock required because affiliation visibility will not be required until subsequent safepoint\n+  \/\/\n+  \/\/ Note: during full GC, all transitions between states are possible.  During Full GC, we should be in a safepoint.\n+\n+  if ((orig_affiliation == ShenandoahAffiliation::FREE) || (new_affiliation == ShenandoahAffiliation::FREE)) {\n+    shenandoah_assert_heaplocked_or_safepoint();\n+  }\n+}\n+\n+inline void ShenandoahHeap::set_affiliation(ShenandoahHeapRegion* r, ShenandoahAffiliation new_affiliation) {\n+#ifdef ASSERT\n+  assert_lock_for_affiliation(region_affiliation(r), new_affiliation);\n+#endif\n+  _affiliations[r->index()] = (uint8_t) new_affiliation;\n+}\n+\n+inline ShenandoahAffiliation ShenandoahHeap::region_affiliation(size_t index) {\n+  return (ShenandoahAffiliation) _affiliations[index];\n+}\n+\n@@ -317,0 +463,8 @@\n+inline bool ShenandoahHeap::is_concurrent_young_mark_in_progress() const {\n+  return _gc_state.is_set(YOUNG_MARKING);\n+}\n+\n+inline bool ShenandoahHeap::is_concurrent_old_mark_in_progress() const {\n+  return _gc_state.is_set(OLD_MARKING);\n+}\n+\n@@ -358,2 +512,1 @@\n-  ShenandoahMarkingContext* const ctx = complete_marking_context();\n-  assert(ctx->is_complete(), \"sanity\");\n+  ShenandoahMarkingContext* const ctx = marking_context();\n@@ -490,8 +643,0 @@\n-inline void ShenandoahHeap::mark_complete_marking_context() {\n-  _marking_context->mark_complete();\n-}\n-\n-inline void ShenandoahHeap::mark_incomplete_marking_context() {\n-  _marking_context->mark_incomplete();\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":159,"deletions":14,"binary":false,"changes":173,"status":"modified"},{"patch":"@@ -3,1 +3,2 @@\n- * Copyright (c) 2013, 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2013, 2020, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -27,0 +28,1 @@\n+#include \"gc\/shared\/cardTable.hpp\"\n@@ -29,0 +31,2 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n@@ -33,0 +37,4 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -63,0 +71,1 @@\n+  _top_before_promoted(nullptr),\n@@ -67,0 +76,1 @@\n+  _plab_allocs(0),\n@@ -69,1 +79,6 @@\n-  _update_watermark(start) {\n+  _update_watermark(start),\n+  _age(0)\n+#ifdef SHENANDOAH_CENSUS_NOISE\n+  , _youth(0)\n+#endif \/\/ SHENANDOAH_CENSUS_NOISE\n+  {\n@@ -85,1 +100,1 @@\n-void ShenandoahHeapRegion::make_regular_allocation() {\n+void ShenandoahHeapRegion::make_regular_allocation(ShenandoahAffiliation affiliation) {\n@@ -87,1 +102,1 @@\n-\n+  reset_age();\n@@ -92,0 +107,1 @@\n+      assert(this->affiliation() == affiliation, \"Region affiliation should already be established\");\n@@ -101,0 +117,25 @@\n+\/\/ Change affiliation to YOUNG_GENERATION if _state is not _pinned_cset, _regular, or _pinned.  This implements\n+\/\/ behavior previously performed as a side effect of make_regular_bypass().  This is used by Full GC in non-generational\n+\/\/ modes to transition regions from FREE. Note that all non-free regions in single-generational modes are young.\n+void ShenandoahHeapRegion::make_affiliated_maybe() {\n+  shenandoah_assert_heaplocked();\n+  assert(!ShenandoahHeap::heap()->mode()->is_generational(), \"Only call if non-generational\");\n+  switch (_state) {\n+   case _empty_uncommitted:\n+   case _empty_committed:\n+   case _cset:\n+   case _humongous_start:\n+   case _humongous_cont:\n+     if (affiliation() != YOUNG_GENERATION) {\n+       set_affiliation(YOUNG_GENERATION);\n+     }\n+     return;\n+   case _pinned_cset:\n+   case _regular:\n+   case _pinned:\n+     return;\n+   default:\n+     assert(false, \"Unexpected _state in make_affiliated_maybe\");\n+  }\n+}\n+\n@@ -107,1 +148,1 @@\n-\n+  reset_age();\n@@ -115,0 +156,8 @@\n+      if (_state == _humongous_start || _state == _humongous_cont) {\n+        \/\/ CDS allocates chunks of the heap to fill with regular objects. The allocator\n+        \/\/ will dutifully track any waste in the unused portion of the last region. Once\n+        \/\/ CDS has finished initializing the objects, it will convert these regions to\n+        \/\/ regular regions. The 'waste' in the last region is no longer wasted at this point,\n+        \/\/ so we must stop treating it as such.\n+        decrement_humongous_waste();\n+      }\n@@ -130,0 +179,1 @@\n+  reset_age();\n@@ -141,1 +191,1 @@\n-void ShenandoahHeapRegion::make_humongous_start_bypass() {\n+void ShenandoahHeapRegion::make_humongous_start_bypass(ShenandoahAffiliation affiliation) {\n@@ -144,1 +194,3 @@\n-\n+  \/\/ Don't bother to account for affiliated regions during Full GC.  We recompute totals at end.\n+  set_affiliation(affiliation);\n+  reset_age();\n@@ -159,0 +211,1 @@\n+  reset_age();\n@@ -170,1 +223,1 @@\n-void ShenandoahHeapRegion::make_humongous_cont_bypass() {\n+void ShenandoahHeapRegion::make_humongous_cont_bypass(ShenandoahAffiliation affiliation) {\n@@ -173,1 +226,3 @@\n-\n+  set_affiliation(affiliation);\n+  \/\/ Don't bother to account for affiliated regions during Full GC.  We recompute totals at end.\n+  reset_age();\n@@ -214,0 +269,1 @@\n+      assert(is_affiliated(), \"Pinned region should be affiliated\");\n@@ -232,0 +288,1 @@\n+  \/\/ Leave age untouched.  We need to consult the age when we are deciding whether to promote evacuated objects.\n@@ -244,0 +301,1 @@\n+  reset_age();\n@@ -245,2 +303,0 @@\n-    case _cset:\n-      \/\/ Reclaiming cset regions\n@@ -249,1 +305,7 @@\n-      \/\/ Reclaiming humongous regions\n+    {\n+      \/\/ Reclaiming humongous regions and reclaim humongous waste.  When this region is eventually recycled, we'll reclaim\n+      \/\/ its used memory.  At recycle time, we no longer recognize this as a humongous region.\n+      decrement_humongous_waste();\n+    }\n+    case _cset:\n+      \/\/ Reclaiming cset regions\n@@ -264,1 +326,3 @@\n-  ShenandoahHeap::heap()->complete_marking_context()->reset_top_bitmap(this);\n+  assert(ShenandoahHeap::heap()->gc_generation()->is_mark_complete(), \"Marking should be complete here.\");\n+  shenandoah_assert_generations_reconciled();\n+  ShenandoahHeap::heap()->marking_context()->reset_top_bitmap(this);\n@@ -269,0 +333,2 @@\n+  reset_age();\n+  CENSUS_NOISE(clear_youth();)\n@@ -308,0 +374,1 @@\n+  _plab_allocs = 0;\n@@ -311,1 +378,1 @@\n-  return used() - (_tlab_allocs + _gclab_allocs) * HeapWordSize;\n+  return used() - (_tlab_allocs + _gclab_allocs + _plab_allocs) * HeapWordSize;\n@@ -322,0 +389,4 @@\n+size_t ShenandoahHeapRegion::get_plab_allocs() const {\n+  return _plab_allocs * HeapWordSize;\n+}\n+\n@@ -366,0 +437,2 @@\n+  st->print(\"|%s\", shenandoah_affiliation_code(affiliation()));\n+\n@@ -377,0 +450,3 @@\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    st->print(\"|P \" SIZE_FORMAT_W(5) \"%1s\", byte_size_in_proper_unit(get_plab_allocs()),   proper_unit_for_byte_size(get_plab_allocs()));\n+  }\n@@ -385,6 +461,7 @@\n-void ShenandoahHeapRegion::oop_iterate(OopIterateClosure* blk) {\n-  if (!is_active()) return;\n-  if (is_humongous()) {\n-    oop_iterate_humongous(blk);\n-  } else {\n-    oop_iterate_objects(blk);\n+\/\/ oop_iterate without closure, return true if completed without cancellation\n+bool ShenandoahHeapRegion::oop_coalesce_and_fill(bool cancellable) {\n+\n+  assert(!is_humongous(), \"No need to fill or coalesce humongous regions\");\n+  if (!is_active()) {\n+    end_preemptible_coalesce_and_fill();\n+    return true;\n@@ -392,6 +469,17 @@\n-}\n-void ShenandoahHeapRegion::oop_iterate_objects(OopIterateClosure* blk) {\n-  assert(! is_humongous(), \"no humongous region here\");\n-  HeapWord* obj_addr = bottom();\n-  HeapWord* t = top();\n-  \/\/ Could call objects iterate, but this is easier.\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  ShenandoahMarkingContext* marking_context = heap->marking_context();\n+\n+  \/\/ Expect marking to be completed before these threads invoke this service.\n+  assert(heap->gc_generation()->is_mark_complete(), \"sanity\");\n+  shenandoah_assert_generations_reconciled();\n+\n+  \/\/ All objects above TAMS are considered live even though their mark bits will not be set.  Note that young-\n+  \/\/ gen evacuations that interrupt a long-running old-gen concurrent mark may promote objects into old-gen\n+  \/\/ while the old-gen concurrent marking is ongoing.  These newly promoted objects will reside above TAMS\n+  \/\/ and will be treated as live during the current old-gen marking pass, even though they will not be\n+  \/\/ explicitly marked.\n+  HeapWord* t = marking_context->top_at_mark_start(this);\n+\n+  \/\/ Resume coalesce and fill from this address\n+  HeapWord* obj_addr = resume_coalesce_and_fill();\n+\n@@ -401,1 +489,17 @@\n-    obj_addr += obj->oop_iterate_size(blk);\n+    if (marking_context->is_marked(obj)) {\n+      assert(obj->klass() != nullptr, \"klass should not be nullptr\");\n+      obj_addr += obj->size();\n+    } else {\n+      \/\/ Object is not marked.  Coalesce and fill dead object with dead neighbors.\n+      HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, t);\n+      assert(next_marked_obj <= t, \"next marked object cannot exceed top\");\n+      size_t fill_size = next_marked_obj - obj_addr;\n+      assert(fill_size >= ShenandoahHeap::min_fill_size(), \"previously allocated object known to be larger than min_size\");\n+      ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n+      heap->old_generation()->card_scan()->coalesce_objects(obj_addr, fill_size);\n+      obj_addr = next_marked_obj;\n+    }\n+    if (cancellable && heap->cancelled_gc()) {\n+      suspend_coalesce_and_fill(obj_addr);\n+      return false;\n+    }\n@@ -403,0 +507,10 @@\n+  \/\/ Mark that this region has been coalesced and filled\n+  end_preemptible_coalesce_and_fill();\n+  return true;\n+}\n+\n+size_t get_card_count(size_t words) {\n+  assert(words % CardTable::card_size_in_words() == 0, \"Humongous iteration must span whole number of cards\");\n+  assert(CardTable::card_size_in_words() * (words \/ CardTable::card_size_in_words()) == words,\n+         \"slice must be integral number of cards\");\n+  return words \/ CardTable::card_size_in_words();\n@@ -405,1 +519,2 @@\n-void ShenandoahHeapRegion::oop_iterate_humongous(OopIterateClosure* blk) {\n+void ShenandoahHeapRegion::oop_iterate_humongous_slice_dirty(OopIterateClosure* blk,\n+                                                             HeapWord* start, size_t words, bool write_table) const {\n@@ -407,1 +522,1 @@\n-  \/\/ Find head.\n+\n@@ -409,2 +524,30 @@\n-  assert(r->is_humongous_start(), \"need humongous head here\");\n-  obj->oop_iterate(blk, MemRegion(bottom(), top()));\n+  size_t num_cards = get_card_count(words);\n+\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  ShenandoahScanRemembered* scanner = heap->old_generation()->card_scan();\n+  size_t card_index = scanner->card_index_for_addr(start);\n+  if (write_table) {\n+    while (num_cards-- > 0) {\n+      if (scanner->is_write_card_dirty(card_index++)) {\n+        obj->oop_iterate(blk, MemRegion(start, start + CardTable::card_size_in_words()));\n+      }\n+      start += CardTable::card_size_in_words();\n+    }\n+  } else {\n+    while (num_cards-- > 0) {\n+      if (scanner->is_card_dirty(card_index++)) {\n+        obj->oop_iterate(blk, MemRegion(start, start + CardTable::card_size_in_words()));\n+      }\n+      start += CardTable::card_size_in_words();\n+    }\n+  }\n+}\n+\n+void ShenandoahHeapRegion::oop_iterate_humongous_slice_all(OopIterateClosure* cl, HeapWord* start, size_t words) const {\n+  assert(is_humongous(), \"only humongous region here\");\n+\n+  ShenandoahHeapRegion* r = humongous_start_region();\n+  oop obj = cast_to_oop(r->bottom());\n+\n+  \/\/ Scan all data, regardless of whether cards are dirty\n+  obj->oop_iterate(cl, MemRegion(start, start + words));\n@@ -430,0 +573,7 @@\n+  shenandoah_assert_heaplocked();\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahGeneration* generation = heap->generation_for(affiliation());\n+\n+  heap->decrease_used(generation, used());\n+  generation->decrement_affiliated_region_count();\n+\n@@ -435,1 +585,1 @@\n-  ShenandoahHeap::heap()->marking_context()->reset_top_at_mark_start(this);\n+  heap->marking_context()->reset_top_at_mark_start(this);\n@@ -440,0 +590,1 @@\n+  set_affiliation(FREE);\n@@ -483,0 +634,5 @@\n+  \/\/ Generational Shenandoah needs this alignment for card tables.\n+  if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+    max_heap_size = align_up(max_heap_size , CardTable::ct_max_alignment_constraint());\n+  }\n+\n@@ -661,0 +817,62 @@\n+\n+void ShenandoahHeapRegion::set_affiliation(ShenandoahAffiliation new_affiliation) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  ShenandoahAffiliation region_affiliation = heap->region_affiliation(this);\n+  {\n+    ShenandoahMarkingContext* const ctx = heap->complete_marking_context();\n+    log_debug(gc)(\"Setting affiliation of Region \" SIZE_FORMAT \" from %s to %s, top: \" PTR_FORMAT \", TAMS: \" PTR_FORMAT\n+                  \", watermark: \" PTR_FORMAT \", top_bitmap: \" PTR_FORMAT,\n+                  index(), shenandoah_affiliation_name(region_affiliation), shenandoah_affiliation_name(new_affiliation),\n+                  p2i(top()), p2i(ctx->top_at_mark_start(this)), p2i(_update_watermark), p2i(ctx->top_bitmap(this)));\n+  }\n+\n+#ifdef ASSERT\n+  {\n+    \/\/ During full gc, heap->complete_marking_context() is not valid, may equal nullptr.\n+    ShenandoahMarkingContext* const ctx = heap->complete_marking_context();\n+    size_t idx = this->index();\n+    HeapWord* top_bitmap = ctx->top_bitmap(this);\n+\n+    assert(ctx->is_bitmap_clear_range(top_bitmap, _end),\n+           \"Region \" SIZE_FORMAT \", bitmap should be clear between top_bitmap: \" PTR_FORMAT \" and end: \" PTR_FORMAT, idx,\n+           p2i(top_bitmap), p2i(_end));\n+  }\n+#endif\n+\n+  if (region_affiliation == new_affiliation) {\n+    return;\n+  }\n+\n+  if (!heap->mode()->is_generational()) {\n+    log_trace(gc)(\"Changing affiliation of region %zu from %s to %s\",\n+                  index(), affiliation_name(), shenandoah_affiliation_name(new_affiliation));\n+    heap->set_affiliation(this, new_affiliation);\n+    return;\n+  }\n+\n+  switch (new_affiliation) {\n+    case FREE:\n+      assert(!has_live(), \"Free region should not have live data\");\n+      break;\n+    case YOUNG_GENERATION:\n+      reset_age();\n+      break;\n+    case OLD_GENERATION:\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      return;\n+  }\n+  heap->set_affiliation(this, new_affiliation);\n+}\n+\n+void ShenandoahHeapRegion::decrement_humongous_waste() const {\n+  assert(is_humongous(), \"Should only use this for humongous regions\");\n+  size_t waste_bytes = free();\n+  if (waste_bytes > 0) {\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    ShenandoahGeneration* generation = heap->generation_for(affiliation());\n+    heap->decrease_humongous_waste(generation, waste_bytes);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":250,"deletions":32,"binary":false,"changes":282,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -30,0 +31,2 @@\n+#include \"gc\/shenandoah\/shenandoahAffiliation.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAgeCensus.hpp\"\n@@ -125,0 +128,1 @@\n+public:\n@@ -143,0 +147,1 @@\n+private:\n@@ -170,1 +175,2 @@\n-  void make_regular_allocation();\n+  void make_regular_allocation(ShenandoahAffiliation affiliation);\n+  void make_affiliated_maybe();\n@@ -174,2 +180,2 @@\n-  void make_humongous_start_bypass();\n-  void make_humongous_cont_bypass();\n+  void make_humongous_start_bypass(ShenandoahAffiliation affiliation);\n+  void make_humongous_cont_bypass(ShenandoahAffiliation affiliation);\n@@ -200,0 +206,5 @@\n+  bool is_regular_pinned()         const { return _state == _pinned; }\n+\n+  inline bool is_young() const;\n+  inline bool is_old() const;\n+  inline bool is_affiliated() const;\n@@ -232,0 +243,2 @@\n+  HeapWord* _top_before_promoted;\n+\n@@ -234,0 +247,1 @@\n+  HeapWord* _coalesce_and_fill_boundary; \/\/ for old regions not selected as collection set candidates.\n@@ -240,0 +254,1 @@\n+  size_t _plab_allocs;\n@@ -246,0 +261,3 @@\n+  uint _age;\n+  CENSUS_NOISE(uint _youth;)   \/\/ tracks epochs of retrograde ageing (rejuvenation)\n+\n@@ -330,2 +348,13 @@\n-  \/\/ Allocation (return null if full)\n-  inline HeapWord* allocate(size_t word_size, ShenandoahAllocRequest::Type type);\n+  inline void save_top_before_promote();\n+  inline HeapWord* get_top_before_promote() const { return _top_before_promoted; }\n+  inline void restore_top_before_promote();\n+  inline size_t garbage_before_padded_for_promote() const;\n+\n+  \/\/ If next available memory is not aligned on address that is multiple of alignment, fill the empty space\n+  \/\/ so that returned object is aligned on an address that is a multiple of alignment_in_bytes.  Requested\n+  \/\/ size is in words.  It is assumed that this->is_old().  A pad object is allocated, filled, and registered\n+  \/\/ if necessary to assure the new allocation is properly aligned.  Return nullptr if memory is not available.\n+  inline HeapWord* allocate_aligned(size_t word_size, ShenandoahAllocRequest &req, size_t alignment_in_bytes);\n+\n+  \/\/ Allocation (return nullptr if full)\n+  inline HeapWord* allocate(size_t word_size, ShenandoahAllocRequest req);\n@@ -352,1 +381,30 @@\n-  void oop_iterate(OopIterateClosure* cl);\n+  inline void begin_preemptible_coalesce_and_fill() {\n+    _coalesce_and_fill_boundary = _bottom;\n+  }\n+\n+  inline void end_preemptible_coalesce_and_fill() {\n+    _coalesce_and_fill_boundary = _end;\n+  }\n+\n+  inline void suspend_coalesce_and_fill(HeapWord* next_focus) {\n+    _coalesce_and_fill_boundary = next_focus;\n+  }\n+\n+  inline HeapWord* resume_coalesce_and_fill() {\n+    return _coalesce_and_fill_boundary;\n+  }\n+\n+  \/\/ Coalesce contiguous spans of garbage objects by filling header and registering start locations with remembered set.\n+  \/\/ This is used by old-gen GC following concurrent marking to make old-gen HeapRegions parsable. Old regions must be\n+  \/\/ parsable because the mark bitmap is not reliable during the concurrent old mark.\n+  \/\/ Return true iff region is completely coalesced and filled.  Returns false if cancelled before task is complete.\n+  bool oop_coalesce_and_fill(bool cancellable);\n+\n+  \/\/ Invoke closure on every reference contained within the humongous object that spans this humongous\n+  \/\/ region if the reference is contained within a DIRTY card and the reference is no more than words following\n+  \/\/ start within the humongous object.\n+  void oop_iterate_humongous_slice_dirty(OopIterateClosure* cl, HeapWord* start, size_t words, bool write_table) const;\n+\n+  \/\/ Invoke closure on every reference contained within the humongous object starting from start and\n+  \/\/ ending at start + words.\n+  void oop_iterate_humongous_slice_all(OopIterateClosure* cl, HeapWord* start, size_t words) const;\n@@ -372,0 +430,1 @@\n+  size_t used_before_promote() const { return byte_size(bottom(), get_top_before_promote()); }\n@@ -374,0 +433,5 @@\n+  \/\/ Does this region contain this address?\n+  bool contains(HeapWord* p) const {\n+    return (bottom() <= p) && (p < top());\n+  }\n+\n@@ -379,0 +443,1 @@\n+  size_t get_plab_allocs() const;\n@@ -384,0 +449,24 @@\n+  inline ShenandoahAffiliation affiliation() const;\n+  inline const char* affiliation_name() const;\n+\n+  void set_affiliation(ShenandoahAffiliation new_affiliation);\n+\n+  \/\/ Region ageing and rejuvenation\n+  uint age() const { return _age; }\n+  CENSUS_NOISE(uint youth() const { return _youth; })\n+\n+  void increment_age() {\n+    const uint max_age = markWord::max_age;\n+    assert(_age <= max_age, \"Error\");\n+    if (_age++ >= max_age) {\n+      _age = max_age;   \/\/ clamp\n+    }\n+  }\n+\n+  void reset_age() {\n+    CENSUS_NOISE(_youth += _age;)\n+    _age = 0;\n+  }\n+\n+  CENSUS_NOISE(void clear_youth() { _youth = 0; })\n+\n@@ -385,0 +474,1 @@\n+  void decrement_humongous_waste() const;\n@@ -388,3 +478,0 @@\n-  void oop_iterate_objects(OopIterateClosure* cl);\n-  void oop_iterate_humongous(OopIterateClosure* cl);\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.hpp","additions":96,"deletions":9,"binary":false,"changes":105,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -46,0 +47,14 @@\n+bool ShenandoahMarkBitMap::is_bitmap_clear_range(const HeapWord* start, const HeapWord* end) const {\n+  \/\/ Similar to get_next_marked_addr(), without assertion.\n+  \/\/ Round addr up to a possible object boundary to be safe.\n+  if (start == end) {\n+    return true;\n+  }\n+  size_t const addr_offset = address_to_index(align_up(start, HeapWordSize << LogMinObjAlignment));\n+  size_t const limit_offset = address_to_index(end);\n+  size_t const next_offset = get_next_one_offset(addr_offset, limit_offset);\n+  HeapWord* result = index_to_address(next_offset);\n+  return (result == end);\n+}\n+\n+\n@@ -48,0 +63,5 @@\n+#ifdef ASSERT\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahHeapRegion* r = heap->heap_region_containing(addr);\n+  ShenandoahMarkingContext* ctx = heap->marking_context();\n+  HeapWord* tams = ctx->top_at_mark_start(r);\n@@ -49,0 +69,4 @@\n+  assert(limit <= r->top(), \"limit must be less than top\");\n+  assert(addr <= tams, \"addr must be less than TAMS\");\n+#endif\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkBitMap.cpp","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -35,0 +36,1 @@\n+class ShenandoahHeapRegion;\n@@ -50,5 +52,1 @@\n-  \/\/ Marking task queues\n-  ShenandoahObjToScanQueueSet* _task_queues;\n-\n-  ShenandoahMarkingContext(MemRegion heap_region, MemRegion bitmap_region, size_t num_regions, uint max_queues);\n-  ~ShenandoahMarkingContext();\n+  ShenandoahMarkingContext(MemRegion heap_region, MemRegion bitmap_region, size_t num_regions);\n@@ -71,0 +69,2 @@\n+  inline bool is_marked_or_old(oop obj) const;\n+  inline bool is_marked_strong_or_old(oop obj) const;\n@@ -72,1 +72,1 @@\n-  inline HeapWord* get_next_marked_addr(HeapWord* addr, HeapWord* limit) const;\n+  inline HeapWord* get_next_marked_addr(const HeapWord* addr, const HeapWord* limit) const;\n@@ -74,2 +74,2 @@\n-  inline bool allocated_after_mark_start(oop obj) const;\n-  inline bool allocated_after_mark_start(HeapWord* addr) const;\n+  inline bool allocated_after_mark_start(const oop obj) const;\n+  inline bool allocated_after_mark_start(const HeapWord* addr) const;\n@@ -77,1 +77,1 @@\n-  inline HeapWord* top_at_mark_start(ShenandoahHeapRegion* r) const;\n+  inline HeapWord* top_at_mark_start(const ShenandoahHeapRegion* r) const;\n@@ -82,0 +82,2 @@\n+  HeapWord* top_bitmap(ShenandoahHeapRegion* r);\n+\n@@ -86,1 +88,1 @@\n-  bool is_bitmap_clear_range(HeapWord* start, HeapWord* end) const;\n+  bool is_bitmap_clear_range(const HeapWord* start, const HeapWord* end) const;\n@@ -91,3 +93,0 @@\n-\n-  \/\/ Task queues\n-  ShenandoahObjToScanQueueSet* task_queues() const { return _task_queues; }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.hpp","additions":12,"deletions":13,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -29,1 +30,1 @@\n-\n+#include \"logging\/log.hpp\"\n@@ -60,1 +61,9 @@\n-inline HeapWord* ShenandoahMarkingContext::get_next_marked_addr(HeapWord* start, HeapWord* limit) const {\n+inline bool ShenandoahMarkingContext::is_marked_or_old(oop obj) const {\n+  return is_marked(obj) || ShenandoahHeap::heap()->is_old(obj);\n+}\n+\n+inline bool ShenandoahMarkingContext::is_marked_strong_or_old(oop obj) const {\n+  return is_marked_strong(obj) || ShenandoahHeap::heap()->is_old(obj);\n+}\n+\n+inline HeapWord* ShenandoahMarkingContext::get_next_marked_addr(const HeapWord* start, const HeapWord* limit) const {\n@@ -65,1 +74,1 @@\n-  HeapWord* addr = cast_from_oop<HeapWord*>(obj);\n+  const HeapWord* addr = cast_from_oop<HeapWord*>(obj);\n@@ -69,1 +78,1 @@\n-inline bool ShenandoahMarkingContext::allocated_after_mark_start(HeapWord* addr) const {\n+inline bool ShenandoahMarkingContext::allocated_after_mark_start(const HeapWord* addr) const {\n@@ -72,1 +81,1 @@\n-  bool alloc_after_mark_start = addr >= top_at_mark_start;\n+  const bool alloc_after_mark_start = addr >= top_at_mark_start;\n@@ -77,0 +86,5 @@\n+  if (!r->is_affiliated()) {\n+    \/\/ Non-affiliated regions do not need their TAMS updated\n+    return;\n+  }\n+\n@@ -84,1 +98,4 @@\n-  assert(is_bitmap_clear_range(old_tams, new_tams),\n+  assert((new_tams == r->bottom()) || (old_tams == r->bottom()) || (new_tams >= _top_bitmaps[idx]),\n+         \"Region \" SIZE_FORMAT\", top_bitmaps updates should be monotonic: \" PTR_FORMAT \" -> \" PTR_FORMAT,\n+         idx, p2i(_top_bitmaps[idx]), p2i(new_tams));\n+  assert(old_tams == r->bottom() || is_bitmap_clear_range(old_tams, new_tams),\n@@ -88,0 +105,3 @@\n+  log_debug(gc)(\"Capturing TAMS for %s Region \" SIZE_FORMAT \", was: \" PTR_FORMAT \", now: \" PTR_FORMAT,\n+                r->affiliation_name(), idx, p2i(old_tams), p2i(new_tams));\n+\n@@ -96,1 +116,1 @@\n-inline HeapWord* ShenandoahMarkingContext::top_at_mark_start(ShenandoahHeapRegion* r) const {\n+inline HeapWord* ShenandoahMarkingContext::top_at_mark_start(const ShenandoahHeapRegion* r) const {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.inline.hpp","additions":27,"deletions":7,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -29,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -31,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -60,0 +63,17 @@\n+template <typename T>\n+static void card_mark_barrier(T* field, oop value) {\n+  assert(ShenandoahCardBarrier, \"Card-mark barrier should be on\");\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  assert(heap->is_in_or_null(value), \"Should be in heap\");\n+  if (heap->is_in_old(field) && heap->is_in_young(value)) {\n+    \/\/ For Shenandoah, each generation collects all the _referents_ that belong to the\n+    \/\/ collected generation. We can end up with discovered lists that contain a mixture\n+    \/\/ of old and young _references_. These references are linked together through the\n+    \/\/ discovered field in java.lang.Reference. In some cases, creating or editing this\n+    \/\/ list may result in the creation of _new_ old-to-young pointers which must dirty\n+    \/\/ the corresponding card. Failing to do this may cause heap verification errors and\n+    \/\/ lead to incorrect GC behavior.\n+    heap->old_generation()->mark_card_as_dirty(field);\n+  }\n+}\n+\n@@ -66,0 +86,3 @@\n+  if (ShenandoahCardBarrier) {\n+    card_mark_barrier(field, value);\n+  }\n@@ -71,0 +94,3 @@\n+  if (ShenandoahCardBarrier) {\n+    card_mark_barrier(field, value);\n+  }\n@@ -271,0 +297,1 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -287,0 +314,5 @@\n+  if (!heap->is_in_active_generation(referent)) {\n+    log_trace(gc,ref)(\"Referent outside of active generation: \" PTR_FORMAT, p2i(referent));\n+    return false;\n+  }\n+\n@@ -352,0 +384,3 @@\n+  \/\/ Each worker thread has a private copy of refproc_data, which includes a private discovered list.  This means\n+  \/\/ there's no risk that a different worker thread will try to manipulate my discovered list head while I'm making\n+  \/\/ reference the head of my discovered list.\n@@ -360,0 +395,11 @@\n+    \/\/ We successfully set this reference object's next pointer to discovered_head.  This marks reference as discovered.\n+    \/\/ If reference_cas_discovered fails, that means some other worker thread took credit for discovery of this reference,\n+    \/\/ and that other thread will place reference on its discovered list, so I can ignore reference.\n+\n+    \/\/ In case we have created an interesting pointer, mark the remembered set card as dirty.\n+    if (ShenandoahCardBarrier) {\n+      T* addr = reinterpret_cast<T*>(java_lang_ref_Reference::discovered_addr_raw(reference));\n+      card_mark_barrier(addr, discovered_head);\n+    }\n+\n+    \/\/ Make the discovered_list_head point to reference.\n@@ -374,1 +420,2 @@\n-  log_trace(gc, ref)(\"Encountered Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+  log_trace(gc, ref)(\"Encountered Reference: \" PTR_FORMAT \" (%s, %s)\",\n+          p2i(reference), reference_type_name(type), ShenandoahHeap::heap()->heap_region_containing(reference)->affiliation_name());\n@@ -389,1 +436,0 @@\n-#ifdef ASSERT\n@@ -391,0 +437,2 @@\n+\n+#ifdef ASSERT\n@@ -395,0 +443,2 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n@@ -398,0 +448,8 @@\n+  \/\/ When this reference was discovered, it would not have been marked. If it ends up surviving\n+  \/\/ the cycle, we need to dirty the card if the reference is old and the referent is young.  Note\n+  \/\/ that if the reference is not dropped, then its pointer to the referent will be nulled before\n+  \/\/ evacuation begins so card does not need to be dirtied.\n+  if (heap->mode()->is_generational() && heap->is_in_old(reference) && heap->is_in_young(raw_referent)) {\n+    \/\/ Note: would be sufficient to mark only the card that holds the start of this Reference object.\n+    heap->old_generation()->card_scan()->mark_range_as_dirty(cast_from_oop<HeapWord*>(reference), reference->size());\n+  }\n@@ -449,0 +507,1 @@\n+  \/\/ set_oop_field maintains the card mark barrier as this list is constructed.\n@@ -453,1 +512,1 @@\n-    RawAccess<>::oop_store(p, prev);\n+    set_oop_field(p, prev);\n@@ -525,0 +584,13 @@\n+\n+  \/\/ During reference processing, we maintain a local list of references that are identified by\n+  \/\/   _pending_list and _pending_list_tail.  _pending_list_tail points to the next field of the last Reference object on\n+  \/\/   the local list.\n+  \/\/\n+  \/\/ There is also a global list of reference identified by Universe::_reference_pending_list\n+\n+  \/\/ The following code has the effect of:\n+  \/\/  1. Making the global Universe::_reference_pending_list point to my local list\n+  \/\/  2. Overwriting the next field of the last Reference on my local list to point at the previous head of the\n+  \/\/     global Universe::_reference_pending_list\n+\n+  oop former_head_of_global_list = Universe::swap_reference_pending_list(_pending_list);\n@@ -526,1 +598,1 @@\n-    *reinterpret_cast<narrowOop*>(_pending_list_tail) = CompressedOops::encode(Universe::swap_reference_pending_list(_pending_list));\n+    set_oop_field<narrowOop>(reinterpret_cast<narrowOop*>(_pending_list_tail), former_head_of_global_list);\n@@ -528,1 +600,1 @@\n-    *reinterpret_cast<oop*>(_pending_list_tail) = Universe::swap_reference_pending_list(_pending_list);\n+    set_oop_field<oop>(reinterpret_cast<oop*>(_pending_list_tail), former_head_of_global_list);\n@@ -537,1 +609,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahReferenceProcessor.cpp","additions":77,"deletions":6,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -30,0 +31,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -32,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -33,0 +36,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -36,0 +40,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -65,0 +70,1 @@\n+  ShenandoahGeneration* _generation;\n@@ -76,1 +82,2 @@\n-    _loc(nullptr) {\n+    _loc(nullptr),\n+    _generation(nullptr) {\n@@ -78,0 +85,1 @@\n+        options._verify_marked == ShenandoahVerifier::_verify_marked_complete_satb_empty ||\n@@ -87,0 +95,6 @@\n+\n+    if (_heap->mode()->is_generational()) {\n+      _generation = _heap->gc_generation();\n+      assert(_generation != nullptr, \"Expected active generation in this mode\");\n+      shenandoah_assert_generations_reconciled();\n+    }\n@@ -113,2 +127,1 @@\n-\n-      if (_map->par_mark(obj)) {\n+      if (in_generation(obj) && _map->par_mark(obj)) {\n@@ -121,0 +134,9 @@\n+  bool in_generation(oop obj) {\n+    if (_generation == nullptr) {\n+      return true;\n+    }\n+\n+    ShenandoahHeapRegion* region = _heap->heap_region_containing(obj);\n+    return _generation->contains(region);\n+  }\n+\n@@ -171,1 +193,2 @@\n-          check(ShenandoahAsserts::_safe_oop, obj, obj_reg->has_live(),\n+          check(ShenandoahAsserts::_safe_oop, obj, obj_reg->has_live() ||\n+                (obj_reg->is_old() && _heap->gc_generation()->is_young()),\n@@ -173,0 +196,1 @@\n+          shenandoah_assert_generations_reconciled();\n@@ -238,1 +262,0 @@\n-\n@@ -252,0 +275,1 @@\n+      case ShenandoahVerifier::_verify_marked_complete_satb_empty:\n@@ -334,2 +358,2 @@\n-  virtual void do_oop(oop* p) override { do_oop_work(p); }\n-  virtual void do_oop(narrowOop* p) override { do_oop_work(p); }\n+  void do_oop(oop* p) override { do_oop_work(p); }\n+  void do_oop(narrowOop* p) override { do_oop_work(p); }\n@@ -338,0 +362,2 @@\n+\/\/ This closure computes the amounts of used, committed, and garbage memory and the number of regions contained within\n+\/\/ a subset (e.g. the young generation or old generation) of the total heap.\n@@ -340,1 +366,1 @@\n-  size_t _used, _committed, _garbage;\n+  size_t _used, _committed, _garbage, _regions, _humongous_waste, _trashed_regions;\n@@ -342,1 +368,2 @@\n-  ShenandoahCalculateRegionStatsClosure() : _used(0), _committed(0), _garbage(0) {};\n+  ShenandoahCalculateRegionStatsClosure() :\n+      _used(0), _committed(0), _garbage(0), _regions(0), _humongous_waste(0), _trashed_regions(0) {};\n@@ -344,1 +371,1 @@\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n@@ -348,0 +375,9 @@\n+    if (r->is_humongous()) {\n+      _humongous_waste += r->free();\n+    }\n+    if (r->is_trash()) {\n+      _trashed_regions++;\n+    }\n+    _regions++;\n+    log_debug(gc)(\"ShenandoahCalculateRegionStatsClosure: adding \" SIZE_FORMAT \" for %s Region \" SIZE_FORMAT \", yielding: \" SIZE_FORMAT,\n+            r->used(), (r->is_humongous() ? \"humongous\" : \"regular\"), r->index(), _used);\n@@ -350,3 +386,71 @@\n-  size_t used() { return _used; }\n-  size_t committed() { return _committed; }\n-  size_t garbage() { return _garbage; }\n+  size_t used() const { return _used; }\n+  size_t committed() const { return _committed; }\n+  size_t garbage() const { return _garbage; }\n+  size_t regions() const { return _regions; }\n+  size_t waste() const { return _humongous_waste; }\n+\n+  \/\/ span is the total memory affiliated with these stats (some of which is in use and other is available)\n+  size_t span() const { return _regions * ShenandoahHeapRegion::region_size_bytes(); }\n+  size_t non_trashed_span() const { return (_regions - _trashed_regions) * ShenandoahHeapRegion::region_size_bytes(); }\n+};\n+\n+class ShenandoahGenerationStatsClosure : public ShenandoahHeapRegionClosure {\n+ public:\n+  ShenandoahCalculateRegionStatsClosure old;\n+  ShenandoahCalculateRegionStatsClosure young;\n+  ShenandoahCalculateRegionStatsClosure global;\n+\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n+    switch (r->affiliation()) {\n+      case FREE:\n+        return;\n+      case YOUNG_GENERATION:\n+        young.heap_region_do(r);\n+        global.heap_region_do(r);\n+        break;\n+      case OLD_GENERATION:\n+        old.heap_region_do(r);\n+        global.heap_region_do(r);\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  }\n+\n+  static void log_usage(ShenandoahGeneration* generation, ShenandoahCalculateRegionStatsClosure& stats) {\n+    log_debug(gc)(\"Safepoint verification: %s verified usage: \" SIZE_FORMAT \"%s, recorded usage: \" SIZE_FORMAT \"%s\",\n+                  generation->name(),\n+                  byte_size_in_proper_unit(generation->used()), proper_unit_for_byte_size(generation->used()),\n+                  byte_size_in_proper_unit(stats.used()),       proper_unit_for_byte_size(stats.used()));\n+  }\n+\n+  static void validate_usage(const bool adjust_for_padding,\n+                             const char* label, ShenandoahGeneration* generation, ShenandoahCalculateRegionStatsClosure& stats) {\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    size_t generation_used = generation->used();\n+    size_t generation_used_regions = generation->used_regions();\n+    if (adjust_for_padding && (generation->is_young() || generation->is_global())) {\n+      size_t pad = heap->old_generation()->get_pad_for_promote_in_place();\n+      generation_used += pad;\n+    }\n+\n+    guarantee(stats.used() == generation_used,\n+              \"%s: generation (%s) used size must be consistent: generation-used: \" PROPERFMT \", regions-used: \" PROPERFMT,\n+              label, generation->name(), PROPERFMTARGS(generation_used), PROPERFMTARGS(stats.used()));\n+\n+    guarantee(stats.regions() == generation_used_regions,\n+              \"%s: generation (%s) used regions (\" SIZE_FORMAT \") must equal regions that are in use (\" SIZE_FORMAT \")\",\n+              label, generation->name(), generation->used_regions(), stats.regions());\n+\n+    size_t generation_capacity = generation->max_capacity();\n+    guarantee(stats.non_trashed_span() <= generation_capacity,\n+              \"%s: generation (%s) size spanned by regions (\" SIZE_FORMAT \") * region size (\" PROPERFMT\n+              \") must not exceed current capacity (\" PROPERFMT \")\",\n+              label, generation->name(), stats.regions(), PROPERFMTARGS(ShenandoahHeapRegion::region_size_bytes()),\n+              PROPERFMTARGS(generation_capacity));\n+\n+    size_t humongous_waste = generation->get_humongous_waste();\n+    guarantee(stats.waste() == humongous_waste,\n+              \"%s: generation (%s) humongous waste must be consistent: generation: \" PROPERFMT \", regions: \" PROPERFMT,\n+              label, generation->name(), PROPERFMTARGS(humongous_waste), PROPERFMTARGS(stats.waste()));\n+  }\n@@ -384,1 +488,1 @@\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n@@ -436,2 +540,5 @@\n-    verify(r, r->get_shared_allocs() + r->get_tlab_allocs() + r->get_gclab_allocs() == r->used(),\n-           \"Accurate accounting: shared + TLAB + GCLAB = used\");\n+    verify(r, r->get_plab_allocs() <= r->capacity(),\n+           \"PLAB alloc count should not be larger than capacity\");\n+\n+    verify(r, r->get_shared_allocs() + r->get_tlab_allocs() + r->get_gclab_allocs() + r->get_plab_allocs() == r->used(),\n+           \"Accurate accounting: shared + TLAB + GCLAB + PLAB = used\");\n@@ -469,1 +576,1 @@\n-  size_t processed() {\n+  size_t processed() const {\n@@ -473,1 +580,1 @@\n-  virtual void work(uint worker_id) {\n+  void work(uint worker_id) override {\n@@ -510,0 +617,10 @@\n+class ShenandoahVerifyNoIncompleteSatbBuffers : public ThreadClosure {\n+public:\n+  void do_thread(Thread* thread) override {\n+    SATBMarkQueue& queue = ShenandoahThreadLocalData::satb_mark_queue(thread);\n+    if (!queue.is_empty()) {\n+      fatal(\"All SATB buffers should have been flushed during mark\");\n+    }\n+  }\n+};\n+\n@@ -519,0 +636,1 @@\n+  ShenandoahGeneration* _generation;\n@@ -532,1 +650,12 @@\n-          _processed(0) {};\n+          _processed(0),\n+          _generation(nullptr) {\n+    if (_options._verify_marked == ShenandoahVerifier::_verify_marked_complete_satb_empty) {\n+      Threads::change_thread_claim_token();\n+    }\n+\n+    if (_heap->mode()->is_generational()) {\n+      _generation = _heap->gc_generation();\n+      assert(_generation != nullptr, \"Expected active generation in this mode.\");\n+      shenandoah_assert_generations_reconciled();\n+    }\n+  };\n@@ -538,1 +667,6 @@\n-  virtual void work(uint worker_id) {\n+  void work(uint worker_id) override {\n+    if (_options._verify_marked == ShenandoahVerifier::_verify_marked_complete_satb_empty) {\n+      ShenandoahVerifyNoIncompleteSatbBuffers verify_satb;\n+      Threads::possibly_parallel_threads_do(true, &verify_satb);\n+    }\n+\n@@ -548,0 +682,4 @@\n+        if (!in_generation(r)) {\n+          continue;\n+        }\n+\n@@ -559,0 +697,4 @@\n+  bool in_generation(ShenandoahHeapRegion* r) {\n+    return _generation == nullptr || _generation->contains(r);\n+  }\n+\n@@ -629,1 +771,1 @@\n-  void do_thread(Thread* t) {\n+  void do_thread(Thread* t) override {\n@@ -631,1 +773,1 @@\n-    if (actual != _expected) {\n+    if (!verify_gc_state(actual, _expected)) {\n@@ -635,0 +777,10 @@\n+\n+  static bool verify_gc_state(char actual, char expected) {\n+    \/\/ Old generation marking is allowed in all states.\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      return ((actual & ~(ShenandoahHeap::OLD_MARKING | ShenandoahHeap::MARKING)) == expected);\n+    } else {\n+      assert((actual & ShenandoahHeap::OLD_MARKING) == 0, \"Should not mark old in non-generational mode\");\n+      return (actual == expected);\n+    }\n+  }\n@@ -637,1 +789,2 @@\n-void ShenandoahVerifier::verify_at_safepoint(const char *label,\n+void ShenandoahVerifier::verify_at_safepoint(const char* label,\n+                                             VerifyRememberedSet remembered,\n@@ -641,0 +794,1 @@\n+                                             VerifySize sizeness,\n@@ -664,0 +818,4 @@\n+      case _verify_gcstate_updating:\n+        enabled = true;\n+        expected = ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::UPDATEREFS;\n+        break;\n@@ -683,1 +841,7 @@\n-      if (actual != expected) {\n+\n+      bool is_marking = (actual & ShenandoahHeap::MARKING);\n+      bool is_marking_young_or_old = (actual & (ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING));\n+      assert(is_marking == is_marking_young_or_old, \"MARKING iff (YOUNG_MARKING or OLD_MARKING), gc_state is: %x\", actual);\n+\n+      \/\/ Old generation marking is allowed in all states.\n+      if (!VerifyThreadGCState::verify_gc_state(actual, expected)) {\n@@ -701,7 +865,14 @@\n-    size_t heap_used = _heap->used();\n-    guarantee(cl.used() == heap_used,\n-              \"%s: heap used size must be consistent: heap-used = \" SIZE_FORMAT \"%s, regions-used = \" SIZE_FORMAT \"%s\",\n-              label,\n-              byte_size_in_proper_unit(heap_used), proper_unit_for_byte_size(heap_used),\n-              byte_size_in_proper_unit(cl.used()), proper_unit_for_byte_size(cl.used()));\n-\n+    size_t heap_used;\n+    if (_heap->mode()->is_generational() && (sizeness == _verify_size_adjusted_for_padding)) {\n+      \/\/ Prior to evacuation, regular regions that are to be evacuated in place are padded to prevent further allocations\n+      heap_used = _heap->used() + _heap->old_generation()->get_pad_for_promote_in_place();\n+    } else if (sizeness != _verify_size_disable) {\n+      heap_used = _heap->used();\n+    }\n+    if (sizeness != _verify_size_disable) {\n+      guarantee(cl.used() == heap_used,\n+                \"%s: heap used size must be consistent: heap-used = \" SIZE_FORMAT \"%s, regions-used = \" SIZE_FORMAT \"%s\",\n+                label,\n+                byte_size_in_proper_unit(heap_used), proper_unit_for_byte_size(heap_used),\n+                byte_size_in_proper_unit(cl.used()), proper_unit_for_byte_size(cl.used()));\n+    }\n@@ -716,0 +887,55 @@\n+  log_debug(gc)(\"Safepoint verification finished heap usage verification\");\n+\n+  ShenandoahGeneration* generation;\n+  if (_heap->mode()->is_generational()) {\n+    generation = _heap->gc_generation();\n+    guarantee(generation != nullptr, \"Need to know which generation to verify.\");\n+    shenandoah_assert_generations_reconciled();\n+  } else {\n+    generation = nullptr;\n+  }\n+\n+  if (generation != nullptr) {\n+    ShenandoahHeapLocker lock(_heap->lock());\n+\n+    switch (remembered) {\n+      case _verify_remembered_disable:\n+        break;\n+      case _verify_remembered_before_marking:\n+        log_debug(gc)(\"Safepoint verification of remembered set at mark\");\n+        verify_rem_set_before_mark();\n+        break;\n+      case _verify_remembered_before_updating_references:\n+        log_debug(gc)(\"Safepoint verification of remembered set at update ref\");\n+        verify_rem_set_before_update_ref();\n+        break;\n+      case _verify_remembered_after_full_gc:\n+        log_debug(gc)(\"Safepoint verification of remembered set after full gc\");\n+        verify_rem_set_after_full_gc();\n+        break;\n+      default:\n+        fatal(\"Unhandled remembered set verification mode\");\n+    }\n+\n+    ShenandoahGenerationStatsClosure cl;\n+    _heap->heap_region_iterate(&cl);\n+\n+    if (LogTarget(Debug, gc)::is_enabled()) {\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->old_generation(),    cl.old);\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->young_generation(),  cl.young);\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->global_generation(), cl.global);\n+    }\n+    if (sizeness == _verify_size_adjusted_for_padding) {\n+      ShenandoahGenerationStatsClosure::validate_usage(false, label, _heap->old_generation(), cl.old);\n+      ShenandoahGenerationStatsClosure::validate_usage(true, label, _heap->young_generation(), cl.young);\n+      ShenandoahGenerationStatsClosure::validate_usage(true, label, _heap->global_generation(), cl.global);\n+    } else if (sizeness == _verify_size_exact) {\n+      ShenandoahGenerationStatsClosure::validate_usage(false, label, _heap->old_generation(), cl.old);\n+      ShenandoahGenerationStatsClosure::validate_usage(false, label, _heap->young_generation(), cl.young);\n+      ShenandoahGenerationStatsClosure::validate_usage(false, label, _heap->global_generation(), cl.global);\n+    }\n+    \/\/ else: sizeness must equal _verify_size_disable\n+  }\n+\n+  log_debug(gc)(\"Safepoint verification finished remembered set verification\");\n+\n@@ -719,1 +945,5 @@\n-    _heap->heap_region_iterate(&cl);\n+    if (generation != nullptr) {\n+      generation->heap_region_iterate(&cl);\n+    } else {\n+      _heap->heap_region_iterate(&cl);\n+    }\n@@ -722,0 +952,2 @@\n+  log_debug(gc)(\"Safepoint verification finished heap region closure verification\");\n+\n@@ -746,0 +978,2 @@\n+  log_debug(gc)(\"Safepoint verification finished getting initial reachable set\");\n+\n@@ -754,1 +988,4 @@\n-  if (ShenandoahVerifyLevel >= 4 && (marked == _verify_marked_complete || marked == _verify_marked_complete_except_references)) {\n+  if (ShenandoahVerifyLevel >= 4 &&\n+        (marked == _verify_marked_complete ||\n+         marked == _verify_marked_complete_except_references ||\n+         marked == _verify_marked_complete_satb_empty)) {\n@@ -763,0 +1000,2 @@\n+  log_debug(gc)(\"Safepoint verification finished walking marked objects\");\n+\n@@ -769,0 +1008,3 @@\n+      if (generation != nullptr && !generation->contains(r)) {\n+        continue;\n+      }\n@@ -792,0 +1034,3 @@\n+  log_debug(gc)(\"Safepoint verification finished accumulation of liveness data\");\n+\n+\n@@ -801,0 +1046,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -806,0 +1052,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -813,0 +1060,2 @@\n+          _verify_remembered_before_marking,\n+                                       \/\/ verify read-only remembered set from bottom() to top()\n@@ -818,0 +1067,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -825,0 +1075,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -826,1 +1077,2 @@\n-          _verify_marked_complete_except_references, \/\/ bitmaps as precise as we can get, except dangling j.l.r.Refs\n+          _verify_marked_complete_satb_empty,\n+                                       \/\/ bitmaps as precise as we can get, except dangling j.l.r.Refs\n@@ -830,0 +1082,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -837,0 +1090,1 @@\n+          _verify_remembered_disable,                \/\/ do not verify remembered set\n@@ -842,0 +1096,2 @@\n+          _verify_size_adjusted_for_padding,         \/\/ expect generation and heap sizes to match after adjustments\n+                                                     \/\/  for promote in place padding\n@@ -849,0 +1105,1 @@\n+          _verify_remembered_before_updating_references,  \/\/ verify read-write remembered set\n@@ -854,1 +1111,2 @@\n-          _verify_gcstate_forwarded    \/\/ evacuation should have produced some forwarded objects\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n+          _verify_gcstate_updating     \/\/ evacuation should have produced some forwarded objects\n@@ -858,0 +1116,1 @@\n+\/\/ We have not yet cleanup (reclaimed) the collection set\n@@ -861,0 +1120,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -866,0 +1126,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -873,0 +1134,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -878,0 +1140,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -885,0 +1148,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -890,0 +1154,1 @@\n+          _verify_size_disable,        \/\/ if we degenerate during evacuation, usage not valid: padding and deferred accounting\n@@ -897,0 +1162,1 @@\n+          _verify_remembered_after_full_gc,  \/\/ verify read-write remembered set\n@@ -902,0 +1168,1 @@\n+          _verify_size_exact,           \/\/ expect generation and heap sizes to match exactly\n@@ -906,1 +1173,1 @@\n-class ShenandoahVerifyNoForwared : public OopClosure {\n+class ShenandoahVerifyNoForwarded : public BasicOopIterateClosure {\n@@ -926,1 +1193,1 @@\n-class ShenandoahVerifyInToSpaceClosure : public OopClosure {\n+class ShenandoahVerifyInToSpaceClosure : public BasicOopIterateClosure {\n@@ -935,1 +1202,1 @@\n-      if (!heap->marking_context()->is_marked(obj)) {\n+      if (!heap->marking_context()->is_marked_or_old(obj)) {\n@@ -954,2 +1221,2 @@\n-  void do_oop(narrowOop* p) { do_oop_work(p); }\n-  void do_oop(oop* p)       { do_oop_work(p); }\n+  void do_oop(narrowOop* p) override { do_oop_work(p); }\n+  void do_oop(oop* p)       override { do_oop_work(p); }\n@@ -964,1 +1231,1 @@\n-  ShenandoahVerifyNoForwared cl;\n+  ShenandoahVerifyNoForwarded cl;\n@@ -967,0 +1234,165 @@\n+\n+template<typename Scanner>\n+class ShenandoahVerifyRemSetClosure : public BasicOopIterateClosure {\n+protected:\n+  ShenandoahGenerationalHeap* const _heap;\n+  Scanner*   const _scanner;\n+  const char* _message;\n+\n+public:\n+  \/\/ Argument distinguishes between initial mark or start of update refs verification.\n+  explicit ShenandoahVerifyRemSetClosure(Scanner* scanner, const char* message) :\n+            _heap(ShenandoahGenerationalHeap::heap()),\n+            _scanner(scanner),\n+            _message(message) {}\n+\n+  template<class T>\n+  inline void work(T* p) {\n+    T o = RawAccess<>::oop_load(p);\n+    if (!CompressedOops::is_null(o)) {\n+      oop obj = CompressedOops::decode_not_null(o);\n+      if (_heap->is_in_young(obj) && !_scanner->is_card_dirty((HeapWord*) p)) {\n+        ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, p, nullptr,\n+                                         _message, \"clean card should be dirty\", __FILE__, __LINE__);\n+      }\n+    }\n+  }\n+\n+  void do_oop(narrowOop* p) override { work(p); }\n+  void do_oop(oop* p)       override { work(p); }\n+};\n+\n+ShenandoahMarkingContext* ShenandoahVerifier::get_marking_context_for_old() {\n+  shenandoah_assert_generations_reconciled();\n+  if (_heap->old_generation()->is_mark_complete() || _heap->gc_generation()->is_global()) {\n+    return _heap->complete_marking_context();\n+  }\n+  return nullptr;\n+}\n+\n+template<typename Scanner>\n+void ShenandoahVerifier::help_verify_region_rem_set(Scanner* scanner, ShenandoahHeapRegion* r, ShenandoahMarkingContext* ctx,\n+                                                    HeapWord* registration_watermark, const char* message) {\n+  ShenandoahVerifyRemSetClosure<Scanner> check_interesting_pointers(scanner, message);\n+  HeapWord* from = r->bottom();\n+  HeapWord* obj_addr = from;\n+  if (r->is_humongous_start()) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if ((ctx == nullptr) || ctx->is_marked(obj)) {\n+      \/\/ For humongous objects, the typical object is an array, so the following checks may be overkill\n+      \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+      \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+      if (!scanner->is_card_dirty(obj_addr) || obj->is_objArray()) {\n+        obj->oop_iterate(&check_interesting_pointers);\n+      }\n+      \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+    }\n+    \/\/ else, this humongous object is not live so no need to verify its internal pointers\n+\n+    if ((obj_addr < registration_watermark) && !scanner->verify_registration(obj_addr, ctx)) {\n+      ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, nullptr, message,\n+                                       \"object not properly registered\", __FILE__, __LINE__);\n+    }\n+  } else if (!r->is_humongous()) {\n+    HeapWord* top = r->top();\n+    while (obj_addr < top) {\n+      oop obj = cast_to_oop(obj_addr);\n+      \/\/ ctx->is_marked() returns true if mark bit set or if obj above TAMS.\n+      if ((ctx == nullptr) || ctx->is_marked(obj)) {\n+        \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+        \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+        if (!scanner->is_card_dirty(obj_addr) || obj->is_objArray()) {\n+          obj->oop_iterate(&check_interesting_pointers);\n+        }\n+        \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+\n+        if ((obj_addr < registration_watermark) && !scanner->verify_registration(obj_addr, ctx)) {\n+          ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, nullptr, message,\n+                                           \"object not properly registered\", __FILE__, __LINE__);\n+        }\n+        obj_addr += obj->size();\n+      } else {\n+        \/\/ This object is not live so we don't verify dirty cards contained therein\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        obj_addr = ctx->get_next_marked_addr(obj_addr, tams);\n+      }\n+    }\n+  }\n+}\n+\n+class ShenandoahWriteTableScanner {\n+private:\n+  ShenandoahScanRemembered* _scanner;\n+public:\n+  explicit ShenandoahWriteTableScanner(ShenandoahScanRemembered* scanner) : _scanner(scanner) {}\n+\n+  bool is_card_dirty(HeapWord* obj_addr) {\n+    return _scanner->is_write_card_dirty(obj_addr);\n+  }\n+\n+  bool verify_registration(HeapWord* obj_addr, ShenandoahMarkingContext* ctx) {\n+    return _scanner->verify_registration(obj_addr, ctx);\n+  }\n+};\n+\n+\/\/ Assure that the remember set has a dirty card everywhere there is an interesting pointer.\n+\/\/ This examines the read_card_table between bottom() and top() since all PLABS are retired\n+\/\/ before the safepoint for init_mark.  Actually, we retire them before update-references and don't\n+\/\/ restore them until the start of evacuation.\n+void ShenandoahVerifier::verify_rem_set_before_mark() {\n+  shenandoah_assert_safepoint();\n+  shenandoah_assert_generational();\n+\n+  ShenandoahMarkingContext* ctx = get_marking_context_for_old();\n+  ShenandoahOldGeneration* old_generation = _heap->old_generation();\n+\n+  log_debug(gc)(\"Verifying remembered set at %s mark\", old_generation->is_doing_mixed_evacuations() ? \"mixed\" : \"young\");\n+\n+  ShenandoahScanRemembered* scanner = old_generation->card_scan();\n+  for (size_t i = 0, n = _heap->num_regions(); i < n; ++i) {\n+    ShenandoahHeapRegion* r = _heap->get_region(i);\n+    if (r->is_old() && r->is_active()) {\n+      help_verify_region_rem_set(scanner, r, ctx, r->end(), \"Verify init-mark remembered set violation\");\n+    }\n+  }\n+}\n+\n+void ShenandoahVerifier::verify_rem_set_after_full_gc() {\n+  shenandoah_assert_safepoint();\n+  shenandoah_assert_generational();\n+\n+  ShenandoahWriteTableScanner scanner(ShenandoahGenerationalHeap::heap()->old_generation()->card_scan());\n+  for (size_t i = 0, n = _heap->num_regions(); i < n; ++i) {\n+    ShenandoahHeapRegion* r = _heap->get_region(i);\n+    if (r->is_old() && !r->is_cset()) {\n+      help_verify_region_rem_set(&scanner, r, nullptr, r->top(), \"Remembered set violation at end of Full GC\");\n+    }\n+  }\n+}\n+\n+\/\/ Assure that the remember set has a dirty card everywhere there is an interesting pointer.  Even though\n+\/\/ the update-references scan of remembered set only examines cards up to update_watermark, the remembered\n+\/\/ set should be valid through top.  This examines the write_card_table between bottom() and top() because\n+\/\/ all PLABS are retired immediately before the start of update refs.\n+void ShenandoahVerifier::verify_rem_set_before_update_ref() {\n+  shenandoah_assert_safepoint();\n+  shenandoah_assert_generational();\n+\n+  ShenandoahMarkingContext* ctx = get_marking_context_for_old();\n+  ShenandoahWriteTableScanner scanner(_heap->old_generation()->card_scan());\n+  for (size_t i = 0, n = _heap->num_regions(); i < n; ++i) {\n+    ShenandoahHeapRegion* r = _heap->get_region(i);\n+    if (r->is_old() && !r->is_cset()) {\n+      help_verify_region_rem_set(&scanner, r, ctx, r->get_update_watermark(), \"Remembered set violation at init-update-references\");\n+    }\n+  }\n+}\n+\n+void ShenandoahVerifier::verify_before_rebuilding_free_set() {\n+  ShenandoahGenerationStatsClosure cl;\n+  _heap->heap_region_iterate(&cl);\n+\n+  ShenandoahGenerationStatsClosure::validate_usage(false, \"Before free set rebuild\", _heap->old_generation(), cl.old);\n+  ShenandoahGenerationStatsClosure::validate_usage(false, \"Before free set rebuild\", _heap->young_generation(), cl.young);\n+  ShenandoahGenerationStatsClosure::validate_usage(false, \"Before free set rebuild\", _heap->global_generation(), cl.global);\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":473,"deletions":41,"binary":false,"changes":514,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -35,0 +36,1 @@\n+class ShenandoahMarkingContext;\n@@ -60,0 +62,18 @@\n+  typedef enum {\n+    \/\/ Disable remembered set verification.\n+    _verify_remembered_disable,\n+\n+    \/\/ Old objects should be registered and RS cards within *read-only* RS are dirty for all\n+    \/\/ inter-generational pointers.\n+    _verify_remembered_before_marking,\n+\n+    \/\/ Old objects should be registered and RS cards within *read-write* RS are dirty for all\n+    \/\/ inter-generational pointers.\n+    _verify_remembered_before_updating_references,\n+\n+    \/\/ Old objects should be registered and RS cards within *read-write* RS are dirty for all\n+    \/\/ inter-generational pointers. Differs from previous verification modes by using top instead\n+    \/\/ of update watermark and not using the marking context.\n+    _verify_remembered_after_full_gc\n+  } VerifyRememberedSet;\n+\n@@ -72,1 +92,6 @@\n-    _verify_marked_complete_except_references\n+    _verify_marked_complete_except_references,\n+\n+    \/\/ Objects should be marked in \"complete\" bitmap, except j.l.r.Reference referents, which\n+    \/\/ may be dangling after marking but before conc-weakrefs-processing. All SATB buffers must\n+    \/\/ be empty.\n+    _verify_marked_complete_satb_empty,\n@@ -125,0 +150,11 @@\n+  typedef enum {\n+    \/\/ Disable size verification\n+    _verify_size_disable,\n+\n+    \/\/ Enforce exact consistency\n+    _verify_size_exact,\n+\n+    \/\/ Expect promote-in-place adjustments: padding inserted to temporarily prevent further allocation in regular regions\n+    _verify_size_adjusted_for_padding\n+  } VerifySize;\n+\n@@ -136,1 +172,4 @@\n-    _verify_gcstate_forwarded\n+    _verify_gcstate_forwarded,\n+\n+    \/\/ Evacuation is done, some objects are forwarded, updating is in progress\n+    _verify_gcstate_updating\n@@ -160,1 +199,2 @@\n-  void verify_at_safepoint(const char *label,\n+  void verify_at_safepoint(const char* label,\n+                           VerifyRememberedSet remembered,\n@@ -166,0 +206,1 @@\n+                           VerifySize sizeness,\n@@ -184,1 +225,13 @@\n-\n+\n+  \/\/ Check that generation usages are accurate before rebuilding free set\n+  void verify_before_rebuilding_free_set();\n+private:\n+  template<typename Scanner>\n+  void help_verify_region_rem_set(Scanner* scanner, ShenandoahHeapRegion* r, ShenandoahMarkingContext* ctx,\n+                                  HeapWord* update_watermark, const char* message);\n+\n+  void verify_rem_set_before_mark();\n+  void verify_rem_set_before_update_ref();\n+  void verify_rem_set_after_full_gc();\n+\n+  ShenandoahMarkingContext* get_marking_context_for_old();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.hpp","additions":57,"deletions":4,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -36,0 +37,80 @@\n+  product(uintx, ShenandoahGenerationalHumongousReserve, 0, EXPERIMENTAL,   \\\n+          \"(Generational mode only) What percent of the heap should be \"    \\\n+          \"reserved for humongous objects if possible.  Old-generation \"    \\\n+          \"collections will endeavor to evacuate old-gen regions within \"   \\\n+          \"this reserved area even if these regions do not contain high \"   \\\n+          \"percentage of garbage.  Setting a larger value will cause \"      \\\n+          \"more frequent old-gen collections.  A smaller value will \"       \\\n+          \"increase the likelihood that humongous object allocations \"      \\\n+          \"fail, resulting in stop-the-world full GCs.\")                    \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(double, ShenandoahMinOldGenGrowthPercent, 12.5, EXPERIMENTAL,     \\\n+          \"(Generational mode only) If the usage within old generation \"    \\\n+          \"has grown by at least this percent of its live memory size \"     \\\n+          \"at completion of the most recent old-generation marking \"        \\\n+          \"effort, heuristics may trigger the start of a new old-gen \"      \\\n+          \"collection.\")                                                    \\\n+          range(0.0,100.0)                                                  \\\n+                                                                            \\\n+  product(uintx, ShenandoahIgnoreOldGrowthBelowPercentage,10, EXPERIMENTAL, \\\n+          \"(Generational mode only) If the total usage of the old \"         \\\n+          \"generation is smaller than this percent, we do not trigger \"     \\\n+          \"old gen collections even if old has grown, except when \"         \\\n+          \"ShenandoahGenerationalDoNotIgnoreGrowthAfterYoungCycles \"        \\\n+          \"consecutive cycles have been completed following the \"           \\\n+          \"preceding old-gen collection.\")                                  \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahDoNotIgnoreGrowthAfterYoungCycles,               \\\n+          50, EXPERIMENTAL,                                                 \\\n+          \"(Generational mode only) Even if the usage of old generation \"   \\\n+          \"is below ShenandoahIgnoreOldGrowthBelowPercentage, \"             \\\n+          \"trigger an old-generation mark if old has grown and this \"       \\\n+          \"many consecutive young-gen collections have been \"               \\\n+          \"completed following the preceding old-gen collection.\")          \\\n+                                                                            \\\n+  product(bool, ShenandoahGenerationalCensusAtEvac, false, EXPERIMENTAL,    \\\n+          \"(Generational mode only) Object age census at evacuation, \"      \\\n+          \"rather than during marking.\")                                    \\\n+                                                                            \\\n+  product(bool, ShenandoahGenerationalAdaptiveTenuring, true, EXPERIMENTAL, \\\n+          \"(Generational mode only) Dynamically adapt tenuring age.\")       \\\n+                                                                            \\\n+  product(bool, ShenandoahGenerationalCensusIgnoreOlderCohorts, true,       \\\n+                                                               EXPERIMENTAL,\\\n+          \"(Generational mode only) Ignore mortality rates older than the \" \\\n+          \"oldest cohort under the tenuring age for the last cycle.\" )      \\\n+                                                                            \\\n+  product(uintx, ShenandoahGenerationalMinTenuringAge, 1, EXPERIMENTAL,     \\\n+          \"(Generational mode only) Floor for adaptive tenuring age. \"      \\\n+          \"Setting floor and ceiling to the same value fixes the tenuring \" \\\n+          \"age; setting both to 1 simulates a poor approximation to \"       \\\n+          \"AlwaysTenure, and setting both to 16 simulates NeverTenure.\")    \\\n+          range(1,16)                                                       \\\n+                                                                            \\\n+  product(uintx, ShenandoahGenerationalMaxTenuringAge, 15, EXPERIMENTAL,    \\\n+          \"(Generational mode only) Ceiling for adaptive tenuring age. \"    \\\n+          \"Setting floor and ceiling to the same value fixes the tenuring \" \\\n+          \"age; setting both to 1 simulates a poor approximation to \"       \\\n+          \"AlwaysTenure, and setting both to 16 simulates NeverTenure.\")    \\\n+          range(1,16)                                                       \\\n+                                                                            \\\n+  product(double, ShenandoahGenerationalTenuringMortalityRateThreshold,     \\\n+                                                         0.1, EXPERIMENTAL, \\\n+          \"(Generational mode only) Cohort mortality rates below this \"     \\\n+          \"value will be treated as indicative of longevity, leading to \"   \\\n+          \"tenuring. A lower value delays tenuring, a higher value hastens \"\\\n+          \"it. Used only when ShenandoahGenerationalhenAdaptiveTenuring is \"\\\n+          \"enabled.\")                                                       \\\n+          range(0.001,0.999)                                                \\\n+                                                                            \\\n+  product(size_t, ShenandoahGenerationalTenuringCohortPopulationThreshold,  \\\n+                                                         4*K, EXPERIMENTAL, \\\n+          \"(Generational mode only) Cohorts whose population is lower than \"\\\n+          \"this value in the previous census are ignored wrt tenuring \"     \\\n+          \"decisions. Effectively this makes then tenurable as soon as all \"\\\n+          \"older cohorts are. Set this value to the largest cohort \"        \\\n+          \"population volume that you are comfortable ignoring when making \"\\\n+          \"tenuring decisions.\")                                            \\\n+                                                                            \\\n@@ -57,1 +138,2 @@\n-          \" passive - stop the world GC only (either degenerated or full)\") \\\n+          \" passive - stop the world GC only (either degenerated or full);\" \\\n+          \" generational - generational concurrent GC\")                     \\\n@@ -71,0 +153,10 @@\n+  product(uintx, ShenandoahExpeditePromotionsThreshold, 5, EXPERIMENTAL,    \\\n+          \"When Shenandoah expects to promote at least this percentage \"    \\\n+          \"of the young generation, trigger a young collection to \"         \\\n+          \"expedite these promotions.\")                                     \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahExpediteMixedThreshold, 10, EXPERIMENTAL,        \\\n+          \"When there are this many old regions waiting to be collected, \"  \\\n+          \"trigger a mixed collection immediately.\")                        \\\n+                                                                            \\\n@@ -79,0 +171,14 @@\n+  product(uintx, ShenandoahOldGarbageThreshold, 15, EXPERIMENTAL,           \\\n+          \"How much garbage an old region has to contain before it would \"  \\\n+          \"be taken for collection.\")                                       \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahIgnoreGarbageThreshold, 5, EXPERIMENTAL,         \\\n+          \"When less than this amount of garbage (as a percentage of \"      \\\n+          \"region size) exists within a region, the region will not be \"    \\\n+          \"added to the collection set, even when the heuristic has \"       \\\n+          \"chosen to aggressively add regions with less than \"              \\\n+          \"ShenandoahGarbageThreshold amount of garbage into the \"          \\\n+          \"collection set.\")                                                \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n@@ -80,4 +186,6 @@\n-          \"How much heap should be free before some heuristics trigger the \"\\\n-          \"initial (learning) cycles. Affects cycle frequency on startup \"  \\\n-          \"and after drastic state changes, e.g. after degenerated\/full \"   \\\n-          \"GC cycles. In percents of (soft) max heap size.\")                \\\n+          \"When less than this amount of memory is free within the\"         \\\n+          \"heap or generation, trigger a learning cycle if we are \"         \\\n+          \"in learning mode.  Learning mode happens during initialization \" \\\n+          \"and following a drastic state change, such as following a \"      \\\n+          \"degenerated or Full GC cycle.  In percents of soft max \"         \\\n+          \"heap size.\")                                                     \\\n@@ -87,3 +195,5 @@\n-          \"How much heap should be free before most heuristics trigger the \"\\\n-          \"collection, even without other triggers. Provides the safety \"   \\\n-          \"margin for many heuristics. In percents of (soft) max heap size.\")\\\n+          \"Percentage of free heap memory (or young generation, in \"        \\\n+          \"generational mode) below which most heuristics trigger \"         \\\n+          \"collection independent of other triggers. Provides a safety \"    \\\n+          \"margin for many heuristics. In percents of (soft) max heap \"     \\\n+          \"size.\")                                                          \\\n@@ -152,0 +262,10 @@\n+  product(uintx, ShenandoahGuaranteedOldGCInterval, 10*60*1000, EXPERIMENTAL, \\\n+          \"Run a collection of the old generation at least this often. \"    \\\n+          \"Heuristics may trigger collections more frequently. Time is in \" \\\n+          \"milliseconds. Setting this to 0 disables the feature.\")          \\\n+                                                                            \\\n+  product(uintx, ShenandoahGuaranteedYoungGCInterval, 5*60*1000,  EXPERIMENTAL,  \\\n+          \"Run a collection of the young generation at least this often. \"  \\\n+          \"Heuristics may trigger collections more frequently. Time is in \" \\\n+          \"milliseconds. Setting this to 0 disables the feature.\")          \\\n+                                                                            \\\n@@ -211,4 +331,12 @@\n-          \"How much of heap to reserve for evacuations. Larger values make \"\\\n-          \"GC evacuate more live objects on every cycle, while leaving \"    \\\n-          \"less headroom for application to allocate in. In percents of \"   \\\n-          \"total heap size.\")                                               \\\n+          \"How much of (young-generation) heap to reserve for \"             \\\n+          \"(young-generation) evacuations.  Larger values allow GC to \"     \\\n+          \"evacuate more live objects on every cycle, while leaving \"       \\\n+          \"less headroom for application to allocate while GC is \"          \\\n+          \"evacuating and updating references. This parameter is \"          \\\n+          \"consulted at the end of marking, before selecting the \"          \\\n+          \"collection set.  If available memory at this time is smaller \"   \\\n+          \"than the indicated reserve, the bound on collection set size is \"\\\n+          \"adjusted downward.  The size of a generational mixed \"           \\\n+          \"evacuation collection set (comprised of both young and old \"     \\\n+          \"regions) is also bounded by this parameter.  In percents of \"    \\\n+          \"total (young-generation) heap size.\")                            \\\n@@ -221,1 +349,18 @@\n-          \"GC cycle.\")                                                      \\\n+          \"GC cycle.  Smaller values increase the risk of evacuation \"      \\\n+          \"failures, which will trigger stop-the-world Full GC passes.\")    \\\n+          range(1.0,100.0)                                                  \\\n+                                                                            \\\n+  product(double, ShenandoahOldEvacWaste, 1.4, EXPERIMENTAL,                \\\n+          \"How much waste evacuations produce within the reserved space. \"  \\\n+          \"Larger values make evacuations more resilient against \"          \\\n+          \"evacuation conflicts, at expense of evacuating less on each \"    \\\n+          \"GC cycle.  Smaller values increase the risk of evacuation \"      \\\n+          \"failures, which will trigger stop-the-world Full GC passes.\")    \\\n+          range(1.0,100.0)                                                  \\\n+                                                                            \\\n+  product(double, ShenandoahPromoEvacWaste, 1.2, EXPERIMENTAL,              \\\n+          \"How much waste promotions produce within the reserved space. \"   \\\n+          \"Larger values make evacuations more resilient against \"          \\\n+          \"evacuation conflicts, at expense of promoting less on each \"     \\\n+          \"GC cycle.  Smaller values increase the risk of evacuation \"      \\\n+          \"failures, which will trigger stop-the-world Full GC passes.\")    \\\n@@ -230,0 +375,35 @@\n+  product(uintx, ShenandoahOldEvacRatioPercent, 75, EXPERIMENTAL,           \\\n+          \"The maximum proportion of evacuation from old-gen memory, \"      \\\n+          \"expressed as a percentage. The default value 75 denotes that no\" \\\n+          \"more than 75% of the collection set evacuation workload may be \" \\\n+          \"towards evacuation of old-gen heap regions. This limits both the\"\\\n+          \"promotion of aged regions and the compaction of existing old \"   \\\n+          \"regions.  A value of 75 denotes that the total evacuation work\"  \\\n+          \"may increase to up to four times the young gen evacuation work.\" \\\n+          \"A larger value allows quicker promotion and allows\"              \\\n+          \"a smaller number of mixed evacuations to process \"               \\\n+          \"the entire list of old-gen collection candidates at the cost \"   \\\n+          \"of an increased disruption of the normal cadence of young-gen \"  \\\n+          \"collections.  A value of 100 allows a mixed evacuation to \"      \\\n+          \"focus entirely on old-gen memory, allowing no young-gen \"        \\\n+          \"regions to be collected, likely resulting in subsequent \"        \\\n+          \"allocation failures because the allocation pool is not \"         \\\n+          \"replenished.  A value of 0 allows a mixed evacuation to\"         \\\n+          \"focus entirely on young-gen memory, allowing no old-gen \"        \\\n+          \"regions to be collected, likely resulting in subsequent \"        \\\n+          \"promotion failures and triggering of stop-the-world full GC \"    \\\n+          \"events.\")                                                        \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahMinYoungPercentage, 20, EXPERIMENTAL,            \\\n+          \"The minimum percentage of the heap to use for the young \"        \\\n+          \"generation. Heuristics will not adjust the young generation \"    \\\n+          \"to be less than this.\")                                          \\\n+          range(0, 100)                                                     \\\n+                                                                            \\\n+  product(uintx, ShenandoahMaxYoungPercentage, 100, EXPERIMENTAL,           \\\n+          \"The maximum percentage of the heap to use for the young \"        \\\n+          \"generation. Heuristics will not adjust the young generation \"    \\\n+          \"to be more than this.\")                                          \\\n+          range(0, 100)                                                     \\\n+                                                                            \\\n@@ -304,0 +484,9 @@\n+  product(uintx, ShenandoahCoalesceChance, 0, DIAGNOSTIC,                   \\\n+          \"Testing: Abandon remaining mixed collections with this \"         \\\n+          \"likelihood. Following each mixed collection, abandon all \"       \\\n+          \"remaining mixed collection candidate regions with likelihood \"   \\\n+          \"ShenandoahCoalesceChance. Abandoning a mixed collection will \"   \\\n+          \"cause the old regions to be made parsable, rather than being \"   \\\n+          \"evacuated.\")                                                     \\\n+          range(0, 100)                                                     \\\n+                                                                            \\\n@@ -330,0 +519,4 @@\n+  product(bool, ShenandoahCardBarrier, false, DIAGNOSTIC,                   \\\n+          \"Turn on\/off card-marking post-write barrier in Shenandoah: \"     \\\n+          \" true when ShenandoahGCMode is generational, false otherwise\")   \\\n+                                                                            \\\n@@ -345,2 +538,29 @@\n-\n-\/\/ end of GC_SHENANDOAH_FLAGS\n+  product(uintx, ShenandoahOldCompactionReserve, 8, EXPERIMENTAL,           \\\n+          \"During generational GC, prevent promotions from filling \"        \\\n+          \"this number of heap regions.  These regions are reserved \"       \\\n+          \"for the purpose of supporting compaction of old-gen \"            \\\n+          \"memory.  Otherwise, old-gen memory cannot be compacted.\")        \\\n+          range(0, 128)                                                     \\\n+                                                                            \\\n+  product(bool, ShenandoahAllowOldMarkingPreemption, true, DIAGNOSTIC,      \\\n+          \"Allow young generation collections to suspend concurrent\"        \\\n+          \" marking in the old generation.\")                                \\\n+                                                                            \\\n+  product(uintx, ShenandoahAgingCyclePeriod, 1, EXPERIMENTAL,               \\\n+          \"With generational mode, increment the age of objects and\"        \\\n+          \"regions each time this many young-gen GC cycles are completed.\") \\\n+                                                                            \\\n+  develop(bool, ShenandoahEnableCardStats, false,                           \\\n+          \"Enable statistics collection related to clean & dirty cards\")    \\\n+                                                                            \\\n+  develop(int, ShenandoahCardStatsLogInterval, 50,                          \\\n+          \"Log cumulative card stats every so many remembered set or \"      \\\n+          \"update refs scans\")                                              \\\n+                                                                            \\\n+  product(uintx, ShenandoahMinimumOldMarkTimeMs, 100, EXPERIMENTAL,         \\\n+         \"Minimum amount of time in milliseconds to run old marking \"       \\\n+         \"before a young collection is allowed to run. This is intended \"   \\\n+         \"to prevent starvation of the old collector. Setting this to \"     \\\n+         \"0 will allow back to back young collections to run during old \"   \\\n+         \"marking.\")                                                        \\\n+  \/\/ end of GC_SHENANDOAH_FLAGS\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":235,"deletions":15,"binary":false,"changes":250,"status":"modified"},{"patch":"@@ -436,1 +436,2 @@\n-    return Universe::heap()->is_in(p);\n+    ShenandoahHeap* sh = ShenandoahHeap::heap();\n+    return sh->mode()->is_generational() ?  sh->is_in_old(p) : sh->is_in(p);\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -93,0 +94,22 @@\n+\/*\n+ * @test id=generational\n+ * @summary Acceptance tests: collector can deal with retained objects\n+ * @key randomness\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahOOMDuringEvacALot -XX:+ShenandoahVerify\n+ *      TestSieveObjects\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahAllocFailureALot -XX:+ShenandoahVerify\n+ *      TestSieveObjects\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      TestSieveObjects\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestSieveObjects.java","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"}]}