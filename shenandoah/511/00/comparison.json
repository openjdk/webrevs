{"files":[{"patch":"@@ -746,9 +746,0 @@\n-  bool allow_new_region = true;\n-  if (_heap->mode()->is_generational()) {\n-    switch (req.affiliation()) {\n-      case ShenandoahAffiliation::OLD_GENERATION:\n-        \/\/ Note: unsigned result from free_unaffiliated_regions() will never be less than zero, but it may equal zero.\n-        if (_heap->old_generation()->free_unaffiliated_regions() <= 0) {\n-          allow_new_region = false;\n-        }\n-        break;\n@@ -756,6 +747,13 @@\n-      case ShenandoahAffiliation::YOUNG_GENERATION:\n-        \/\/ Note: unsigned result from free_unaffiliated_regions() will never be less than zero, but it may equal zero.\n-        if (_heap->young_generation()->free_unaffiliated_regions() <= 0) {\n-          allow_new_region = false;\n-        }\n-        break;\n+  switch (req.type()) {\n+    case ShenandoahAllocRequest::_alloc_tlab:\n+    case ShenandoahAllocRequest::_alloc_shared:\n+      return allocate_for_mutator(req, in_new_region);\n+    case ShenandoahAllocRequest::_alloc_gclab:\n+    case ShenandoahAllocRequest::_alloc_plab:\n+    case ShenandoahAllocRequest::_alloc_shared_gc:\n+      return allocate_for_collector(req, in_new_region);\n+    default:\n+      ShouldNotReachHere();\n+  }\n+  return nullptr;\n+}\n@@ -763,2 +761,2 @@\n-      case ShenandoahAffiliation::FREE:\n-        fatal(\"Should request affiliation\");\n+HeapWord* ShenandoahFreeSet::allocate_for_mutator(ShenandoahAllocRequest &req, bool &in_new_region) {\n+  maybe_change_allocation_bias();\n@@ -766,3 +764,54 @@\n-      default:\n-        ShouldNotReachHere();\n-        break;\n+  if (_partitions.is_empty(ShenandoahFreeSetPartitionId::Mutator)) {\n+    \/\/ There is no recovery. Mutator does not touch collector view at all.\n+    return nullptr;\n+  }\n+\n+  \/\/ Try to allocate in the mutator view\n+  if (_partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::Mutator)) {\n+    return allocate_from_left_to_right(req, in_new_region);\n+  }\n+\n+  return allocate_from_right_to_left(req, in_new_region);\n+}\n+\n+void ShenandoahFreeSet::maybe_change_allocation_bias() {\n+  if (_alloc_bias_weight-- <= 0) {\n+    \/\/ We have observed that regions not collected in previous GC cycle tend to congregate at one end or the other\n+    \/\/ of the heap.  Typically, these are the more recently engaged regions and the objects in these regions have not\n+    \/\/ yet had a chance to die (and\/or are treated as floating garbage).  If we use the same allocation bias on each\n+    \/\/ GC pass, these \"most recently\" engaged regions for GC pass N will also be the \"most recently\" engaged regions\n+    \/\/ for GC pass N+1, and the relatively large amount of live data and\/or floating garbage introduced\n+    \/\/ during the most recent GC pass may once again prevent the region from being collected.  We have found that\n+    \/\/ alternating the allocation behavior between GC passes improves evacuation performance by 3-7% on certain\n+    \/\/ benchmarks.  In the best case, this has the effect of consuming these partially consumed regions before\n+    \/\/ the start of the next mark cycle so all of their garbage can be efficiently reclaimed.\n+    \/\/\n+    \/\/ First, finish consuming regions that are already partially consumed so as to more tightly limit ranges of\n+    \/\/ available regions.  Other potential benefits:\n+    \/\/  1. Eventual collection set has fewer regions because we have packed newly allocated objects into fewer regions\n+    \/\/  2. We preserve the \"empty\" regions longer into the GC cycle, reducing likelihood of allocation failures\n+    \/\/     late in the GC cycle.\n+    idx_t non_empty_on_left = (_partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator)\n+                               - _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator));\n+    idx_t non_empty_on_right = (_partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator)\n+                                - _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n+    _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::Mutator, (non_empty_on_right < non_empty_on_left));\n+    _alloc_bias_weight = _InitialAllocBiasWeight;\n+  }\n+}\n+\n+HeapWord* ShenandoahFreeSet::allocate_from_left_to_right(ShenandoahAllocRequest &req, bool &in_new_region) {\n+  \/\/ Allocate from low to high memory.  This keeps the range of fully empty regions more tightly packed.\n+  \/\/ Note that the most recently allocated regions tend not to be evacuated in a given GC cycle.  So this\n+  \/\/ tends to accumulate \"fragmented\" uncollected regions in high memory.\n+  \/\/ Use signed idx.  Otherwise, loop will never terminate.\n+  idx_t rightmost = _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator);\n+  for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator); idx <= rightmost;) {\n+    assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx),\n+           \"Boundaries or find_last_set_bit failed: \" SSIZE_FORMAT, idx);\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    \/\/ try_allocate_in() increases used if the allocation is successful.\n+    HeapWord* result;\n+    size_t min_size = (req.type() == ShenandoahAllocRequest::_alloc_tlab) ? req.min_size() : req.size();\n+    if ((alloc_capacity(r) >= min_size) && ((result = try_allocate_in(r, req, in_new_region)) != nullptr)) {\n+      return result;\n@@ -770,0 +819,1 @@\n+    idx = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Mutator, idx + 1);\n@@ -771,68 +821,16 @@\n-  switch (req.type()) {\n-    case ShenandoahAllocRequest::_alloc_tlab:\n-    case ShenandoahAllocRequest::_alloc_shared: {\n-      \/\/ Try to allocate in the mutator view\n-      if (_alloc_bias_weight-- <= 0) {\n-        \/\/ We have observed that regions not collected in previous GC cycle tend to congregate at one end or the other\n-        \/\/ of the heap.  Typically, these are the more recently engaged regions and the objects in these regions have not\n-        \/\/ yet had a chance to die (and\/or are treated as floating garbage).  If we use the same allocation bias on each\n-        \/\/ GC pass, these \"most recently\" engaged regions for GC pass N will also be the \"most recently\" engaged regions\n-        \/\/ for GC pass N+1, and the relatively large amount of live data and\/or floating garbage introduced\n-        \/\/ during the most recent GC pass may once again prevent the region from being collected.  We have found that\n-        \/\/ alternating the allocation behavior between GC passes improves evacuation performance by 3-7% on certain\n-        \/\/ benchmarks.  In the best case, this has the effect of consuming these partially consumed regions before\n-        \/\/ the start of the next mark cycle so all of their garbage can be efficiently reclaimed.\n-        \/\/\n-        \/\/ First, finish consuming regions that are already partially consumed so as to more tightly limit ranges of\n-        \/\/ available regions.  Other potential benefits:\n-        \/\/  1. Eventual collection set has fewer regions because we have packed newly allocated objects into fewer regions\n-        \/\/  2. We preserve the \"empty\" regions longer into the GC cycle, reducing likelihood of allocation failures\n-        \/\/     late in the GC cycle.\n-        idx_t non_empty_on_left = (_partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator)\n-                                     - _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator));\n-        idx_t non_empty_on_right = (_partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator)\n-                                      - _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n-        _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::Mutator, (non_empty_on_right < non_empty_on_left));\n-        _alloc_bias_weight = _InitialAllocBiasWeight;\n-      }\n-      if (!_partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::Mutator)) {\n-        \/\/ Allocate within mutator free from high memory to low so as to preserve low memory for humongous allocations\n-        if (!_partitions.is_empty(ShenandoahFreeSetPartitionId::Mutator)) {\n-          \/\/ Use signed idx.  Otherwise, loop will never terminate.\n-          idx_t leftmost = _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator);\n-          for (idx_t idx = _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator); idx >= leftmost; ) {\n-            assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx),\n-                   \"Boundaries or find_last_set_bit failed: \" SSIZE_FORMAT, idx);\n-            ShenandoahHeapRegion* r = _heap->get_region(idx);\n-            \/\/ try_allocate_in() increases used if the allocation is successful.\n-            HeapWord* result;\n-            size_t min_size = (req.type() == ShenandoahAllocRequest::_alloc_tlab)? req.min_size(): req.size();\n-            if ((alloc_capacity(r) >= min_size) && ((result = try_allocate_in(r, req, in_new_region)) != nullptr)) {\n-              return result;\n-            }\n-            idx = _partitions.find_index_of_previous_available_region(ShenandoahFreeSetPartitionId::Mutator, idx - 1);\n-          }\n-        }\n-      } else {\n-        \/\/ Allocate from low to high memory.  This keeps the range of fully empty regions more tightly packed.\n-        \/\/ Note that the most recently allocated regions tend not to be evacuated in a given GC cycle.  So this\n-        \/\/ tends to accumulate \"fragmented\" uncollected regions in high memory.\n-        if (!_partitions.is_empty(ShenandoahFreeSetPartitionId::Mutator)) {\n-          \/\/ Use signed idx.  Otherwise, loop will never terminate.\n-          idx_t rightmost = _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator);\n-          for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator); idx <= rightmost; ) {\n-            assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx),\n-                   \"Boundaries or find_last_set_bit failed: \" SSIZE_FORMAT, idx);\n-            ShenandoahHeapRegion* r = _heap->get_region(idx);\n-            \/\/ try_allocate_in() increases used if the allocation is successful.\n-            HeapWord* result;\n-            size_t min_size = (req.type() == ShenandoahAllocRequest::_alloc_tlab)? req.min_size(): req.size();\n-            if ((alloc_capacity(r) >= min_size) && ((result = try_allocate_in(r, req, in_new_region)) != nullptr)) {\n-              return result;\n-            }\n-            idx = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Mutator, idx + 1);\n-          }\n-        }\n-      }\n-      \/\/ There is no recovery. Mutator does not touch collector view at all.\n-      break;\n+  return nullptr;\n+}\n+\n+HeapWord* ShenandoahFreeSet::allocate_from_right_to_left(ShenandoahAllocRequest &req, bool &in_new_region) {\n+  \/\/ Allocate within mutator free from high memory to low so as to preserve low memory for humongous allocations\n+  \/\/ Use signed idx.  Otherwise, loop will never terminate.\n+  idx_t leftmost = _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator);\n+  for (idx_t idx = _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator); idx >= leftmost;) {\n+    assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx),\n+           \"Boundaries or find_last_set_bit failed: \" SSIZE_FORMAT, idx);\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    \/\/ try_allocate_in() increases used if the allocation is successful.\n+    HeapWord* result;\n+    size_t min_size = (req.type() == ShenandoahAllocRequest::_alloc_tlab) ? req.min_size() : req.size();\n+    if ((alloc_capacity(r) >= min_size) && ((result = try_allocate_in(r, req, in_new_region)) != nullptr)) {\n+      return result;\n@@ -840,24 +838,4 @@\n-    case ShenandoahAllocRequest::_alloc_gclab:\n-      \/\/ GCLABs are for evacuation so we must be in evacuation phase.\n-\n-    case ShenandoahAllocRequest::_alloc_plab: {\n-      \/\/ PLABs always reside in old-gen and are only allocated during\n-      \/\/ evacuation phase.\n-\n-    case ShenandoahAllocRequest::_alloc_shared_gc: {\n-      \/\/ Fast-path: try to allocate in the collector view first\n-      HeapWord* result;\n-      result = allocate_from_partition_with_affiliation(req.is_old()? ShenandoahFreeSetPartitionId::OldCollector:\n-                                                        ShenandoahFreeSetPartitionId::Collector,\n-                                                        req.affiliation(), req, in_new_region);\n-      if (result != nullptr) {\n-        return result;\n-      } else if (allow_new_region) {\n-        \/\/ Try a free region that is dedicated to GC allocations.\n-        result = allocate_from_partition_with_affiliation(req.is_old()? ShenandoahFreeSetPartitionId::OldCollector:\n-                                                          ShenandoahFreeSetPartitionId::Collector,\n-                                                          ShenandoahAffiliation::FREE, req, in_new_region);\n-        if (result != nullptr) {\n-          return result;\n-        }\n-      }\n+    idx = _partitions.find_index_of_previous_available_region(ShenandoahFreeSetPartitionId::Mutator, idx - 1);\n+  }\n+  return nullptr;\n+}\n@@ -865,8 +843,9 @@\n-      \/\/ No dice. Can we borrow space from mutator view?\n-      if (!ShenandoahEvacReserveOverflow) {\n-        return nullptr;\n-      }\n-      if (!allow_new_region && req.is_old() && (_heap->young_generation()->free_unaffiliated_regions() > 0)) {\n-        \/\/ This allows us to flip a mutator region to old_collector\n-        allow_new_region = true;\n-      }\n+HeapWord* ShenandoahFreeSet::allocate_for_collector(ShenandoahAllocRequest &req, bool &in_new_region) {\n+  \/\/ Fast-path: try to allocate in the collector view first\n+  HeapWord* result;\n+  result = allocate_from_partition_with_affiliation(req.is_old()? ShenandoahFreeSetPartitionId::OldCollector:\n+                                                    ShenandoahFreeSetPartitionId::Collector,\n+                                                    req.affiliation(), req, in_new_region);\n+  if (result != nullptr) {\n+    return result;\n+  }\n@@ -874,30 +853,8 @@\n-      \/\/ We should expand old-gen if this can prevent an old-gen evacuation failure.  We don't care so much about\n-      \/\/ promotion failures since they can be mitigated in a subsequent GC pass.  Would be nice to know if this\n-      \/\/ allocation request is for evacuation or promotion.  Individual threads limit their use of PLAB memory for\n-      \/\/ promotions, so we already have an assurance that any additional memory set aside for old-gen will be used\n-      \/\/ only for old-gen evacuations.\n-      if (allow_new_region) {\n-        \/\/ Try to steal an empty region from the mutator view.\n-        idx_t rightmost_mutator = _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n-        idx_t leftmost_mutator =  _partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n-        for (idx_t idx = rightmost_mutator; idx >= leftmost_mutator; ) {\n-          assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx),\n-                 \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n-          ShenandoahHeapRegion* r = _heap->get_region(idx);\n-          if (can_allocate_from(r)) {\n-            if (req.is_old()) {\n-              flip_to_old_gc(r);\n-            } else {\n-              flip_to_gc(r);\n-            }\n-            \/\/ Region r is entirely empty.  If try_allocat_in fails on region r, something else is really wrong.\n-            \/\/ Don't bother to retry with other regions.\n-            log_debug(gc, free)(\"Flipped region \" SIZE_FORMAT \" to gc for request: \" PTR_FORMAT, idx, p2i(&req));\n-            return try_allocate_in(r, req, in_new_region);\n-          }\n-          idx = _partitions.find_index_of_previous_available_region(ShenandoahFreeSetPartitionId::Mutator, idx - 1);\n-        }\n-      }\n-      \/\/ No dice. Do not try to mix mutator and GC allocations, because adjusting region UWM\n-      \/\/ due to GC allocations would expose unparsable mutator allocations.\n-      break;\n+  bool allow_new_region = can_allocate_in_new_region(req);\n+  if (allow_new_region) {\n+    \/\/ Try a free region that is dedicated to GC allocations.\n+    result = allocate_from_partition_with_affiliation(req.is_old()? ShenandoahFreeSetPartitionId::OldCollector:\n+                                                      ShenandoahFreeSetPartitionId::Collector,\n+                                                      ShenandoahAffiliation::FREE, req, in_new_region);\n+    if (result != nullptr) {\n+      return result;\n@@ -905,0 +862,54 @@\n+  }\n+\n+  \/\/ No dice. Can we borrow space from mutator view?\n+  if (!ShenandoahEvacReserveOverflow) {\n+    return nullptr;\n+  }\n+\n+  if (!allow_new_region && req.is_old() && (_heap->young_generation()->free_unaffiliated_regions() > 0)) {\n+    \/\/ This allows us to flip a mutator region to old_collector\n+    allow_new_region = true;\n+  }\n+\n+  \/\/ We should expand old-gen if this can prevent an old-gen evacuation failure.  We don't care so much about\n+  \/\/ promotion failures since they can be mitigated in a subsequent GC pass.  Would be nice to know if this\n+  \/\/ allocation request is for evacuation or promotion.  Individual threads limit their use of PLAB memory for\n+  \/\/ promotions, so we already have an assurance that any additional memory set aside for old-gen will be used\n+  \/\/ only for old-gen evacuations.\n+  if (allow_new_region) {\n+    \/\/ Try to steal an empty region from the mutator view.\n+    result = try_allocate_from_mutator(req, in_new_region);\n+  }\n+\n+  \/\/ This is it. Do not try to mix mutator and GC allocations, because adjusting region UWM\n+  \/\/ due to GC allocations would expose unparsable mutator allocations.\n+  return result;\n+}\n+\n+bool ShenandoahFreeSet::can_allocate_in_new_region(const ShenandoahAllocRequest& req) {\n+  if (!_heap->mode()->is_generational()) {\n+    return true;\n+  }\n+\n+  assert(req.is_old() || req.is_young(), \"Should request affiliation\");\n+  return (req.is_old() && _heap->old_generation()->free_unaffiliated_regions() > 0)\n+         || (req.is_young() && _heap->young_generation()->free_unaffiliated_regions() > 0);\n+}\n+\n+HeapWord* ShenandoahFreeSet::try_allocate_from_mutator(ShenandoahAllocRequest& req, bool& in_new_region) {\n+  idx_t rightmost_mutator = _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n+  idx_t leftmost_mutator =  _partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n+  for (idx_t idx = rightmost_mutator; idx >= leftmost_mutator; ) {\n+    assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx),\n+           \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (can_allocate_from(r)) {\n+      if (req.is_old()) {\n+        flip_to_old_gc(r);\n+      } else {\n+        flip_to_gc(r);\n+      }\n+      \/\/ Region r is entirely empty.  If try_allocate_in fails on region r, something else is really wrong.\n+      \/\/ Don't bother to retry with other regions.\n+      log_debug(gc, free)(\"Flipped region \" SIZE_FORMAT \" to gc for request: \" PTR_FORMAT, idx, p2i(&req));\n+      return try_allocate_in(r, req, in_new_region);\n@@ -906,2 +917,1 @@\n-    default:\n-      ShouldNotReachHere();\n+    idx = _partitions.find_index_of_previous_available_region(ShenandoahFreeSetPartitionId::Mutator, idx - 1);\n@@ -909,0 +919,1 @@\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":163,"deletions":152,"binary":false,"changes":315,"status":"modified"},{"patch":"@@ -291,1 +291,0 @@\n-  size_t _retired_old_regions;\n@@ -332,0 +331,11 @@\n+  \/\/ Handle allocation from mutator\n+  HeapWord* allocate_for_mutator(ShenandoahAllocRequest &req, bool &in_new_region);\n+  void maybe_change_allocation_bias();\n+  HeapWord* allocate_from_right_to_left(ShenandoahAllocRequest& req, bool& in_new_region);\n+  HeapWord* allocate_from_left_to_right(ShenandoahAllocRequest& req, bool& in_new_region);\n+\n+  \/\/ Handle allocation from collector (for evacuation)\n+  HeapWord* allocate_for_collector(ShenandoahAllocRequest& req, bool& in_new_region);\n+  bool can_allocate_in_new_region(const ShenandoahAllocRequest& req);\n+  HeapWord* try_allocate_from_mutator(ShenandoahAllocRequest& req, bool& in_new_region);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"}]}