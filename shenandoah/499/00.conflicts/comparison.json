{"files":[{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -35,0 +36,4 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -89,2 +94,2 @@\n-ShenandoahConcurrentGC::ShenandoahConcurrentGC() :\n-  _mark(),\n+ShenandoahConcurrentGC::ShenandoahConcurrentGC(ShenandoahGeneration* generation, bool do_old_gc_bootstrap) :\n+  _mark(generation),\n@@ -92,1 +97,3 @@\n-  _abbreviated(false) {\n+  _abbreviated(false),\n+  _do_old_gc_bootstrap(do_old_gc_bootstrap),\n+  _generation(generation) {\n@@ -99,4 +106,0 @@\n-void ShenandoahConcurrentGC::cancel() {\n-  ShenandoahConcurrentMark::cancel();\n-}\n-\n@@ -105,0 +108,1 @@\n+\n@@ -115,0 +119,9 @@\n+\n+    \/\/ Reset task queue stats here, rather than in mark_concurrent_roots,\n+    \/\/ because remembered set scan will `push` oops into the queues and\n+    \/\/ resetting after this happens will lose those counts.\n+    TASKQUEUE_STATS_ONLY(_mark.task_queues()->reset_taskqueue_stats());\n+\n+    \/\/ Concurrent remembered set scanning\n+    entry_scan_remembered_set();\n+\n@@ -117,1 +130,1 @@\n-    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_outside_cycle)) {\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_roots)) {\n@@ -135,1 +148,1 @@\n-  if (heap->is_concurrent_mark_in_progress()) {\n+  if (_generation->is_concurrent_mark_in_progress()) {\n@@ -153,1 +166,2 @@\n-  \/\/ the space. This would be the last action if there is nothing to evacuate.\n+  \/\/ the space. This would be the last action if there is nothing to evacuate.  Note that\n+  \/\/ we will not age young-gen objects in the case that we skip evacuation.\n@@ -180,0 +194,1 @@\n+  }\n@@ -181,0 +196,1 @@\n+  if (heap->has_forwarded_objects()) {\n@@ -199,0 +215,4 @@\n+    \/\/ We chose not to evacuate because we found sufficient immediate garbage. Note that we\n+    \/\/ do not check for cancellation here because, at this point, the cycle is effectively\n+    \/\/ complete. If the cycle has been cancelled here, the control thread will detect it\n+    \/\/ on its next iteration and run a degenerated young cycle.\n@@ -203,0 +223,5 @@\n+  \/\/ We defer generation resizing actions until after cset regions have been recycled.  We do this even following an\n+  \/\/ abbreviated cycle.\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahGenerationalHeap::heap()->complete_concurrent_cycle();\n+  }\n@@ -303,1 +328,1 @@\n-  static const char* msg = \"Pause Final Roots\";\n+  const char* msg = final_roots_event_message();\n@@ -312,0 +337,2 @@\n+  heap->try_inject_alloc_failure();\n+\n@@ -313,3 +340,10 @@\n-  static const char* msg = \"Concurrent reset\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset);\n-  EventMark em(\"%s\", msg);\n+  {\n+    const char* msg = conc_reset_event_message();\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset);\n+    EventMark em(\"%s\", msg);\n+\n+    ShenandoahWorkerScope scope(heap->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n+                                msg);\n+    op_reset();\n+  }\n@@ -317,3 +351,7 @@\n-  ShenandoahWorkerScope scope(heap->workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n-                              \"concurrent reset\");\n+  if (_do_old_gc_bootstrap) {\n+    static const char* msg = \"Concurrent reset (OLD)\";\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset_old);\n+    ShenandoahWorkerScope scope(ShenandoahHeap::heap()->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n+                                msg);\n+    EventMark em(\"%s\", msg);\n@@ -321,2 +359,19 @@\n-  heap->try_inject_alloc_failure();\n-  op_reset();\n+    heap->old_generation()->prepare_gc();\n+  }\n+}\n+\n+void ShenandoahConcurrentGC::entry_scan_remembered_set() {\n+  if (_generation->is_young()) {\n+    ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+    TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+    const char* msg = \"Concurrent remembered set scanning\";\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::init_scan_rset);\n+    EventMark em(\"%s\", msg);\n+\n+    ShenandoahWorkerScope scope(heap->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_rs_scanning(),\n+                                msg);\n+\n+    heap->try_inject_alloc_failure();\n+    _generation->scan_remembered_set(true \/* is_concurrent *\/);\n+  }\n@@ -371,1 +426,1 @@\n-  static const char* msg = \"Concurrent weak references\";\n+  const char* msg = conc_weak_refs_event_message();\n@@ -386,1 +441,1 @@\n-  static const char* msg = \"Concurrent weak roots\";\n+  const char* msg = conc_weak_roots_event_message();\n@@ -433,1 +488,1 @@\n-  static const char* msg = \"Concurrent cleanup\";\n+  const char* msg = conc_cleanup_event_message();\n@@ -489,1 +544,1 @@\n-  static const char* msg = \"Concurrent cleanup\";\n+  const char* msg = conc_cleanup_event_message();\n@@ -503,2 +558,1 @@\n-\n-  heap->prepare_gc();\n+  _generation->prepare_gc();\n@@ -517,1 +571,2 @@\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n+      \/\/ reset, so it is very likely we don't need to do another write here.  Since most regions\n+      \/\/ are not \"active\", this path is relatively rare.\n@@ -539,2 +594,2 @@\n-  assert(heap->marking_context()->is_bitmap_clear(), \"need clear marking bitmap\");\n-  assert(!heap->marking_context()->is_complete(), \"should not be complete\");\n+  assert(_generation->is_bitmap_clear(), \"need clear marking bitmap\");\n+  assert(!_generation->is_mark_complete(), \"should not be complete\");\n@@ -543,0 +598,19 @@\n+\n+  if (heap->mode()->is_generational()) {\n+    if (_generation->is_young()) {\n+      \/\/ The current implementation of swap_remembered_set() copies the write-card-table to the read-card-table.\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_swap_rset);\n+      _generation->swap_remembered_set();\n+    }\n+\n+    if (_generation->is_global()) {\n+      heap->old_generation()->cancel_gc();\n+    } else if (heap->is_concurrent_old_mark_in_progress()) {\n+      \/\/ Purge the SATB buffers, transferring any valid, old pointers to the\n+      \/\/ old generation mark queue. Any pointers in a young region will be\n+      \/\/ abandoned.\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_transfer_satb);\n+      heap->old_generation()->transfer_pointers_from_satb();\n+    }\n+  }\n+\n@@ -551,1 +625,1 @@\n-  heap->set_concurrent_mark_in_progress(true);\n+  _generation->set_concurrent_mark_in_progress(true);\n@@ -555,1 +629,3 @@\n-  {\n+  if (_do_old_gc_bootstrap) {\n+    shenandoah_assert_generational();\n+    \/\/ Update region state for both young and old regions\n@@ -559,0 +635,6 @@\n+    heap->old_generation()->ref_processor()->reset_thread_locals();\n+  } else {\n+    \/\/ Update region state for only young regions\n+    ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_region_states);\n+    ShenandoahInitMarkUpdateRegionStateClosure cl;\n+    _generation->parallel_heap_region_iterate(&cl);\n@@ -562,1 +644,1 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = _generation->ref_processor();\n@@ -602,1 +684,4 @@\n-    heap->prepare_regions_and_collection_set(true \/*concurrent*\/);\n+    \/\/ The collection set is chosen by prepare_regions_and_collection_set(). Additionally, certain parameters have been\n+    \/\/ established to govern the evacuation efforts that are about to begin.  Refer to comments on reserve members in\n+    \/\/ ShenandoahGeneration and ShenandoahOldGeneration for more detail.\n+    _generation->prepare_regions_and_collection_set(true \/*concurrent*\/);\n@@ -607,1 +692,11 @@\n-    if (!heap->collection_set()->is_empty()) {\n+    if (!heap->collection_set()->is_empty() || has_in_place_promotions(heap)) {\n+      \/\/ Even if the collection set is empty, we need to do evacuation if there are regions to be promoted in place.\n+      \/\/ Concurrent evacuation takes responsibility for registering objects and setting the remembered set cards to dirty.\n+\n+      LogTarget(Debug, gc, cset) lt;\n+      if (lt.is_enabled()) {\n+        ResourceMark rm;\n+        LogStream ls(lt);\n+        heap->collection_set()->print_on(&ls);\n+      }\n+\n@@ -613,2 +708,0 @@\n-      \/\/ From here on, we need to update references.\n-      heap->set_has_forwarded_objects(true);\n@@ -616,0 +709,14 @@\n+<<<<<<< HEAD\n+=======\n+      \/\/ Verify before arming for concurrent processing.\n+      \/\/ Otherwise, verification can trigger stack processing.\n+      if (ShenandoahVerify) {\n+        heap->verifier()->verify_during_evacuation();\n+      }\n+\n+      \/\/ Generational mode may promote objects in place during the evacuation phase.\n+      \/\/ If that is the only reason we are evacuating, we don't need to update references\n+      \/\/ and there will be no forwarded objects on the heap.\n+      heap->set_has_forwarded_objects(!heap->collection_set()->is_empty());\n+\n+>>>>>>> b7405b19d531269ccb611cb5dc6a07f491a7b9de\n@@ -617,2 +724,7 @@\n-      ShenandoahCodeRoots::arm_nmethods_for_evac();\n-      ShenandoahStackWatermark::change_epoch_id();\n+      if (!heap->collection_set()->is_empty()) {\n+        \/\/ Iff objects will be evaluated, arm the nmethod barriers. These will be disarmed\n+        \/\/ under the same condition (established in prepare_concurrent_roots) after strong\n+        \/\/ root evacuation has completed (see op_strong_roots).\n+        ShenandoahCodeRoots::arm_nmethods_for_evac();\n+        ShenandoahStackWatermark::change_epoch_id();\n+      }\n@@ -635,0 +747,5 @@\n+bool ShenandoahConcurrentGC::has_in_place_promotions(ShenandoahHeap* heap) {\n+  return heap->mode()->is_generational() && heap->old_generation()->has_in_place_promotions();\n+}\n+\n+template<bool GENERATIONAL>\n@@ -638,1 +755,0 @@\n-\n@@ -640,7 +756,1 @@\n-  ShenandoahConcurrentEvacThreadClosure(OopClosure* oops);\n-  void do_thread(Thread* thread);\n-};\n-\n-ShenandoahConcurrentEvacThreadClosure::ShenandoahConcurrentEvacThreadClosure(OopClosure* oops) :\n-  _oops(oops) {\n-}\n+  explicit ShenandoahConcurrentEvacThreadClosure(OopClosure* oops) : _oops(oops) {}\n@@ -648,4 +758,8 @@\n-void ShenandoahConcurrentEvacThreadClosure::do_thread(Thread* thread) {\n-  JavaThread* const jt = JavaThread::cast(thread);\n-  StackWatermarkSet::finish_processing(jt, _oops, StackWatermarkKind::gc);\n-}\n+  void do_thread(Thread* thread) override {\n+    JavaThread* const jt = JavaThread::cast(thread);\n+    StackWatermarkSet::finish_processing(jt, _oops, StackWatermarkKind::gc);\n+    if (GENERATIONAL) {\n+      ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+    }\n+  }\n+};\n@@ -653,0 +767,1 @@\n+template<bool GENERATIONAL>\n@@ -658,1 +773,1 @@\n-  ShenandoahConcurrentEvacUpdateThreadTask(uint n_workers) :\n+  explicit ShenandoahConcurrentEvacUpdateThreadTask(uint n_workers) :\n@@ -663,1 +778,6 @@\n-  void work(uint worker_id) {\n+  void work(uint worker_id) override {\n+    if (GENERATIONAL) {\n+      Thread* worker_thread = Thread::current();\n+      ShenandoahThreadLocalData::enable_plab_promotions(worker_thread);\n+    }\n+\n@@ -667,1 +787,1 @@\n-    ShenandoahConcurrentEvacThreadClosure thr_cl(&oops_cl);\n+    ShenandoahConcurrentEvacThreadClosure<GENERATIONAL> thr_cl(&oops_cl);\n@@ -676,2 +796,7 @@\n-  ShenandoahConcurrentEvacUpdateThreadTask task(heap->workers()->active_workers());\n-  heap->workers()->run_task(&task);\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahConcurrentEvacUpdateThreadTask<true> task(heap->workers()->active_workers());\n+    heap->workers()->run_task(&task);\n+  } else {\n+    ShenandoahConcurrentEvacUpdateThreadTask<false> task(heap->workers()->active_workers());\n+    heap->workers()->run_task(&task);\n+  }\n@@ -688,1 +813,1 @@\n-  heap->ref_processor()->process_references(ShenandoahPhaseTimings::conc_weak_refs, heap->workers(), true \/* concurrent *\/);\n+  _generation->ref_processor()->process_references(ShenandoahPhaseTimings::conc_weak_refs, heap->workers(), true \/* concurrent *\/);\n@@ -715,0 +840,1 @@\n+<<<<<<< HEAD\n@@ -717,0 +843,10 @@\n+=======\n+      shenandoah_assert_generations_reconciled();\n+      if (_heap->is_in_active_generation(obj)) {\n+        \/\/ Here we are asserting that an unmarked from-space object is 'correct'. There seems to be a legitimate\n+        \/\/ use-case for accessing from-space objects during concurrent class unloading. In all modes of Shenandoah,\n+        \/\/ concurrent class unloading only happens during a global collection.\n+        shenandoah_assert_correct(p, obj);\n+        ShenandoahHeap::atomic_clear_oop(p, obj);\n+      }\n+>>>>>>> b7405b19d531269ccb611cb5dc6a07f491a7b9de\n@@ -822,0 +958,3 @@\n+  \/\/ We can only toggle concurrent_weak_root_in_progress flag\n+  \/\/ at a safepoint, so that mutators see a consistent\n+  \/\/ value. The flag will be cleared at the next safepoint.\n@@ -918,0 +1057,1 @@\n+<<<<<<< HEAD\n@@ -923,0 +1063,6 @@\n+=======\n+  heap->set_update_refs_in_progress(true);\n+  if (ShenandoahVerify) {\n+    heap->verifier()->verify_before_updaterefs();\n+  }\n+>>>>>>> b7405b19d531269ccb611cb5dc6a07f491a7b9de\n@@ -967,1 +1113,1 @@\n-    heap->clear_cancelled_gc();\n+    heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n@@ -975,0 +1121,2 @@\n+  \/\/ If we are running in generational mode and this is an aging cycle, this will also age active\n+  \/\/ regions that haven't been used for allocation.\n@@ -980,0 +1128,22 @@\n+  if (heap->mode()->is_generational() && heap->is_concurrent_old_mark_in_progress()) {\n+    \/\/ When the SATB barrier is left on to support concurrent old gen mark, it may pick up writes to\n+    \/\/ objects in the collection set. After those objects are evacuated, the pointers in the\n+    \/\/ SATB are no longer safe. Once we have finished update references, we are guaranteed that\n+    \/\/ no more writes to the collection set are possible.\n+    \/\/\n+    \/\/ This will transfer any old pointers in _active_ regions from the SATB to the old gen\n+    \/\/ mark queues. All other pointers will be discarded. This would also discard any pointers\n+    \/\/ in old regions that were included in a mixed evacuation. We aren't using the SATB filter\n+    \/\/ methods here because we cannot control when they execute. If the SATB filter runs _after_\n+    \/\/ a region has been recycled, we will not be able to detect the bad pointer.\n+    \/\/\n+    \/\/ We are not concerned about skipping this step in abbreviated cycles because regions\n+    \/\/ with no live objects cannot have been written to and so cannot have entries in the SATB\n+    \/\/ buffers.\n+    heap->old_generation()->transfer_pointers_from_satb();\n+\n+    \/\/ Aging_cycle is only relevant during evacuation cycle for individual objects and during final mark for\n+    \/\/ entire regions.  Both of these relevant operations occur before final update refs.\n+    ShenandoahGenerationalHeap::heap()->set_aging_cycle(false);\n+  }\n+\n@@ -992,1 +1162,17 @@\n-  ShenandoahHeap::heap()->set_concurrent_weak_root_in_progress(false);\n+\n+  ShenandoahHeap *heap = ShenandoahHeap::heap();\n+  heap->set_concurrent_weak_root_in_progress(false);\n+  heap->set_evacuation_in_progress(false);\n+\n+  if (heap->mode()->is_generational()) {\n+    \/\/ If the cycle was shortened for having enough immediate garbage, this could be\n+    \/\/ the last GC safepoint before concurrent marking of old resumes. We must be sure\n+    \/\/ that old mark threads don't see any pointers to garbage in the SATB buffers.\n+    if (heap->is_concurrent_old_mark_in_progress()) {\n+      heap->old_generation()->transfer_pointers_from_satb();\n+    }\n+\n+    if (!_generation->is_old()) {\n+      ShenandoahGenerationalHeap::heap()->update_region_ages(_generation->complete_marking_context());\n+    }\n+  }\n@@ -1011,1 +1197,1 @@\n-    return \"Pause Init Mark (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Init Mark\", \" (unload classes)\");\n@@ -1013,1 +1199,1 @@\n-    return \"Pause Init Mark\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Init Mark\", \"\");\n@@ -1019,1 +1205,3 @@\n-  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  assert(!heap->has_forwarded_objects() || heap->is_concurrent_old_mark_in_progress(),\n+         \"Should not have forwarded objects during final mark, unless old gen concurrent mark is running\");\n+\n@@ -1021,1 +1209,1 @@\n-    return \"Pause Final Mark (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Final Mark\", \" (unload classes)\");\n@@ -1023,1 +1211,1 @@\n-    return \"Pause Final Mark\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Final Mark\", \"\");\n@@ -1029,1 +1217,2 @@\n-  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  assert(!heap->has_forwarded_objects() || heap->is_concurrent_old_mark_in_progress(),\n+         \"Should not have forwarded objects concurrent mark, unless old gen concurrent mark is running\");\n@@ -1031,1 +1220,41 @@\n-    return \"Concurrent marking (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent marking\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent marking\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_reset_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent reset\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent reset\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::final_roots_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Final Roots\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Final Roots\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_weak_refs_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent weak references\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent weak references\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_weak_roots_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent weak roots\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent weak roots\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_cleanup_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent cleanup\", \" (unload classes)\");\n@@ -1033,1 +1262,1 @@\n-    return \"Concurrent marking\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent cleanup\", \"\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":294,"deletions":65,"binary":false,"changes":359,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -32,0 +33,2 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n@@ -35,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -40,0 +44,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -45,1 +50,1 @@\n-ShenandoahDegenGC::ShenandoahDegenGC(ShenandoahDegenPoint degen_point) :\n+ShenandoahDegenGC::ShenandoahDegenGC(ShenandoahDegenPoint degen_point, ShenandoahGeneration* generation) :\n@@ -48,0 +53,1 @@\n+  _generation(generation),\n@@ -53,0 +59,7 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  if (heap->mode()->is_generational()) {\n+    bool is_bootstrap_gc = heap->old_generation()->is_bootstrapping();\n+    heap->mmu_tracker()->record_degenerated(GCId::current(), is_bootstrap_gc);\n+    const char* msg = is_bootstrap_gc? \"At end of Degenerated Bootstrap Old GC\": \"At end of Degenerated Young GC\";\n+    heap->log_heap_status(msg);\n+  }\n@@ -68,1 +81,0 @@\n-\n@@ -83,1 +95,20 @@\n-  heap->clear_cancelled_gc();\n+  heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n+\n+#ifdef ASSERT\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahOldGeneration* old_generation = heap->old_generation();\n+    if (!heap->is_concurrent_old_mark_in_progress()) {\n+      \/\/ If we are not marking the old generation, there should be nothing in the old mark queues\n+      assert(old_generation->task_queues()->is_empty(), \"Old gen task queues should be empty\");\n+    }\n+\n+    if (_generation->is_global()) {\n+      \/\/ If we are in a global cycle, the old generation should not be marking. It is, however,\n+      \/\/ allowed to be holding regions for evacuation or coalescing.\n+      assert(old_generation->is_idle()\n+             || old_generation->is_doing_mixed_evacuations()\n+             || old_generation->is_preparing_for_mark(),\n+             \"Old generation cannot be in state: %s\", old_generation->state_name());\n+    }\n+  }\n+#endif\n@@ -99,7 +130,0 @@\n-      \/\/\n-\n-      \/\/ Degenerated from concurrent root mark, reset the flag for STW mark\n-      if (heap->is_concurrent_mark_in_progress()) {\n-        ShenandoahConcurrentMark::cancel();\n-        heap->set_concurrent_mark_in_progress(false);\n-      }\n@@ -109,1 +133,42 @@\n-      heap->set_unload_classes(heap->heuristics()->can_unload_classes());\n+      heap->set_unload_classes(_generation->heuristics()->can_unload_classes() &&\n+                                (!heap->mode()->is_generational() || _generation->is_global()));\n+\n+      if (heap->mode()->is_generational() && _generation->is_young()) {\n+        \/\/ Swap remembered sets for young\n+        _generation->swap_remembered_set();\n+      }\n+\n+    case _degenerated_roots:\n+      \/\/ Degenerated from concurrent root mark, reset the flag for STW mark\n+      if (!heap->mode()->is_generational()) {\n+        if (heap->is_concurrent_mark_in_progress()) {\n+          heap->cancel_concurrent_mark();\n+        }\n+      } else {\n+        if (_generation->is_concurrent_mark_in_progress()) {\n+          \/\/ We want to allow old generation marking to be punctuated by young collections\n+          \/\/ (even if they have degenerated). If this is a global cycle, we'd have cancelled\n+          \/\/ the entire old gc before coming into this switch. Note that cancel_marking on\n+          \/\/ the generation does NOT abandon incomplete SATB buffers as cancel_concurrent_mark does.\n+          \/\/ We need to separate out the old pointers which is done below.\n+          _generation->cancel_marking();\n+        }\n+\n+        if (heap->is_concurrent_mark_in_progress()) {\n+          \/\/ If either old or young marking is in progress, the SATB barrier will be enabled.\n+          \/\/ The SATB buffer may hold a mix of old and young pointers. The old pointers need to be\n+          \/\/ transferred to the old generation mark queues and the young pointers are NOT part\n+          \/\/ of this snapshot, so they must be dropped here. It is safe to drop them here because\n+          \/\/ we will rescan the roots on this safepoint.\n+          heap->old_generation()->transfer_pointers_from_satb();\n+        }\n+\n+        if (_degen_point == ShenandoahDegenPoint::_degenerated_roots) {\n+          \/\/ We only need this if the concurrent cycle has already swapped the card tables.\n+          \/\/ Marking will use the 'read' table, but interesting pointers may have been\n+          \/\/ recorded in the 'write' table in the time between the cancelled concurrent cycle\n+          \/\/ and this degenerated cycle. These pointers need to be included the 'read' table\n+          \/\/ used to scan the remembered set during the STW mark which follows here.\n+          _generation->merge_write_table();\n+        }\n+      }\n@@ -173,1 +238,0 @@\n-\n@@ -192,0 +256,5 @@\n+      \/\/ Update collector state regardless of whether or not there are forwarded objects\n+      heap->set_evacuation_in_progress(false);\n+      heap->set_concurrent_weak_root_in_progress(false);\n+      heap->set_concurrent_strong_root_in_progress(false);\n+\n@@ -213,0 +282,5 @@\n+\n+      if (heap->mode()->is_generational()) {\n+        ShenandoahGenerationalHeap::heap()->complete_degenerated_cycle();\n+      }\n+\n@@ -235,2 +309,2 @@\n-    heap->shenandoah_policy()->record_success_degenerated(_abbreviated);\n-    heap->heuristics()->record_success_degenerated();\n+    heap->shenandoah_policy()->record_success_degenerated(_generation->is_young(), _abbreviated);\n+    _generation->heuristics()->record_success_degenerated();\n@@ -241,1 +315,1 @@\n-  ShenandoahHeap::heap()->prepare_gc();\n+  _generation->prepare_gc();\n@@ -245,1 +319,1 @@\n-  assert(!ShenandoahHeap::heap()->is_concurrent_mark_in_progress(), \"Should be reset\");\n+  assert(!_generation->is_concurrent_mark_in_progress(), \"Should be reset\");\n@@ -247,2 +321,1 @@\n-  ShenandoahSTWMark mark(false \/*full gc*\/);\n-  mark.clear();\n+  ShenandoahSTWMark mark(_generation, false \/*full gc*\/);\n@@ -253,1 +326,1 @@\n-  ShenandoahConcurrentMark mark;\n+  ShenandoahConcurrentMark mark(_generation);\n@@ -265,0 +338,1 @@\n+\n@@ -266,1 +340,1 @@\n-  heap->prepare_regions_and_collection_set(false \/*concurrent*\/);\n+  _generation->prepare_regions_and_collection_set(false \/*concurrent*\/);\n@@ -278,0 +352,1 @@\n+<<<<<<< HEAD\n@@ -279,0 +354,6 @@\n+=======\n+  if (!heap->collection_set()->is_empty() || has_in_place_promotions(heap)) {\n+    \/\/ Even if the collection set is empty, we need to do evacuation if there are regions to be promoted in place.\n+    \/\/ Degenerated evacuation takes responsibility for registering objects and setting the remembered set cards to dirty.\n+\n+>>>>>>> b7405b19d531269ccb611cb5dc6a07f491a7b9de\n@@ -284,0 +365,1 @@\n+<<<<<<< HEAD\n@@ -285,0 +367,8 @@\n+=======\n+\n+    if(ShenandoahVerify) {\n+      heap->verifier()->verify_during_evacuation();\n+    }\n+\n+    heap->set_has_forwarded_objects(!heap->collection_set()->is_empty());\n+>>>>>>> b7405b19d531269ccb611cb5dc6a07f491a7b9de\n@@ -296,0 +386,4 @@\n+bool ShenandoahDegenGC::has_in_place_promotions(const ShenandoahHeap* heap) const {\n+  return heap->mode()->is_generational() && heap->old_generation()->has_in_place_promotions();\n+}\n+\n@@ -308,4 +402,0 @@\n-  heap->set_evacuation_in_progress(false);\n-  heap->set_concurrent_weak_root_in_progress(false);\n-  heap->set_concurrent_strong_root_in_progress(false);\n-\n@@ -360,1 +450,1 @@\n-      return \"Pause Degenerated GC (<UNSET>)\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (<UNSET>)\");\n@@ -362,1 +452,3 @@\n-      return \"Pause Degenerated GC (Outside of Cycle)\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (Outside of Cycle)\");\n+    case _degenerated_roots:\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (Roots)\");\n@@ -364,1 +456,1 @@\n-      return \"Pause Degenerated GC (Mark)\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (Mark)\");\n@@ -366,1 +458,1 @@\n-      return \"Pause Degenerated GC (Evacuation)\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (Evacuation)\");\n@@ -368,1 +460,1 @@\n-      return \"Pause Degenerated GC (Update Refs)\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (Update Refs)\");\n@@ -371,1 +463,1 @@\n-      return \"ERROR\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (?)\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.cpp","additions":122,"deletions":30,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -39,0 +40,3 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n@@ -47,0 +51,3 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalEvacuationTask.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n@@ -54,1 +61,0 @@\n-#include \"gc\/shenandoah\/shenandoahMetrics.hpp\"\n@@ -56,0 +62,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -62,0 +69,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -69,0 +77,2 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -71,0 +81,2 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n@@ -164,3 +176,0 @@\n-  \/\/ Now we know the number of regions, initialize the heuristics.\n-  initialize_heuristics();\n-\n@@ -220,0 +229,22 @@\n+  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this, _heap_region));\n+\n+  \/\/ Now we know the number of regions and heap sizes, initialize the heuristics.\n+  initialize_heuristics();\n+\n+  assert(_heap_region.byte_size() == heap_rs.size(), \"Need to know reserved size for card table\");\n+\n+  \/\/\n+  \/\/ Worker threads must be initialized after the barrier is configured\n+  \/\/\n+  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n+  if (_workers == nullptr) {\n+    vm_exit_during_initialization(\"Failed necessary allocation.\");\n+  } else {\n+    _workers->initialize_workers();\n+  }\n+\n+  if (ParallelGCThreads > 1) {\n+    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\", ParallelGCThreads);\n+    _safepoint_workers->initialize_workers();\n+  }\n+\n@@ -260,1 +291,1 @@\n-                              align_up(num_committed_regions, _bitmap_regions_per_slice) \/ _bitmap_regions_per_slice;\n+    align_up(num_committed_regions, _bitmap_regions_per_slice) \/ _bitmap_regions_per_slice;\n@@ -267,1 +298,1 @@\n-  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions, _max_workers);\n+  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions);\n@@ -351,0 +382,1 @@\n+  _affiliations = NEW_C_HEAP_ARRAY(uint8_t, _num_regions, mtGC);\n@@ -367,0 +399,2 @@\n+\n+      _affiliations[i] = ShenandoahAffiliation::FREE;\n@@ -371,0 +405,1 @@\n+    size_t young_cset_regions, old_cset_regions;\n@@ -372,1 +407,4 @@\n-    _free_set->rebuild();\n+    \/\/ We are initializing free set.  We ignore cset region tallies.\n+    size_t first_old, last_old, num_old;\n+    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+    _free_set->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n@@ -421,1 +459,1 @@\n-  _control_thread = new ShenandoahControlThread();\n+  initialize_controller();\n@@ -423,1 +461,1 @@\n-  ShenandoahInitLogger::print();\n+  print_init_logger();\n@@ -428,0 +466,8 @@\n+void ShenandoahHeap::initialize_controller() {\n+  _control_thread = new ShenandoahControlThread();\n+}\n+\n+void ShenandoahHeap::print_init_logger() const {\n+  ShenandoahInitLogger::print();\n+}\n+\n@@ -434,0 +480,2 @@\n+    } else if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+      _gc_mode = new ShenandoahGenerationalMode();\n@@ -454,13 +502,3 @@\n-  assert(_gc_mode != nullptr, \"Must be initialized\");\n-  _heuristics = _gc_mode->initialize_heuristics();\n-\n-  if (_heuristics->is_diagnostic() && !UnlockDiagnosticVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.\",\n-                    _heuristics->name()));\n-  }\n-  if (_heuristics->is_experimental() && !UnlockExperimentalVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.\",\n-                    _heuristics->name()));\n-  }\n+  _global_generation = new ShenandoahGlobalGeneration(mode()->is_generational(), max_workers(), max_capacity(), max_capacity());\n+  _global_generation->initialize_heuristics(mode());\n+  _evac_tracker = new ShenandoahEvacuationTracker(mode()->is_generational());\n@@ -476,0 +514,2 @@\n+  _gc_generation(nullptr),\n+  _active_generation(nullptr),\n@@ -477,1 +517,0 @@\n-  _used(0),\n@@ -479,2 +518,1 @@\n-  _bytes_allocated_since_gc_start(0),\n-  _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),\n+  _max_workers(MAX3(ConcGCThreads, ParallelGCThreads, 1U)),\n@@ -486,1 +524,1 @@\n-  _update_refs_iterator(this),\n+  _affiliations(nullptr),\n@@ -489,0 +527,3 @@\n+  _cancel_requested_time(0),\n+  _update_refs_iterator(this),\n+  _global_generation(nullptr),\n@@ -490,0 +531,2 @@\n+  _young_generation(nullptr),\n+  _old_generation(nullptr),\n@@ -492,1 +535,0 @@\n-  _heuristics(nullptr),\n@@ -497,0 +539,2 @@\n+  _evac_tracker(nullptr),\n+  _mmu_tracker(),\n@@ -503,1 +547,0 @@\n-  _ref_processor(new ShenandoahReferenceProcessor(MAX2(_max_workers, 1U))),\n@@ -513,1 +556,1 @@\n-  \/\/ Initialize GC mode early, so we can adjust barrier support\n+  \/\/ Initialize GC mode early, many subsequent initialization procedures depend on it\n@@ -515,15 +558,0 @@\n-  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));\n-\n-  _max_workers = MAX2(_max_workers, 1U);\n-  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n-  if (_workers == nullptr) {\n-    vm_exit_during_initialization(\"Failed necessary allocation.\");\n-  } else {\n-    _workers->initialize_workers();\n-  }\n-\n-  if (ParallelGCThreads > 1) {\n-    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\",\n-                                                ParallelGCThreads);\n-    _safepoint_workers->initialize_workers();\n-  }\n@@ -536,29 +564,0 @@\n-class ShenandoahResetBitmapTask : public WorkerTask {\n-private:\n-  ShenandoahRegionIterator _regions;\n-\n-public:\n-  ShenandoahResetBitmapTask() :\n-    WorkerTask(\"Shenandoah Reset Bitmap\") {}\n-\n-  void work(uint worker_id) {\n-    ShenandoahHeapRegion* region = _regions.next();\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahMarkingContext* const ctx = heap->marking_context();\n-    while (region != nullptr) {\n-      if (heap->is_bitmap_slice_committed(region)) {\n-        ctx->clear_bitmap(region);\n-      }\n-      region = _regions.next();\n-    }\n-  }\n-};\n-\n-void ShenandoahHeap::reset_mark_bitmap() {\n-  assert_gc_workers(_workers->active_workers());\n-  mark_incomplete_marking_context();\n-\n-  ShenandoahResetBitmapTask task;\n-  _workers->run_task(&task);\n-}\n-\n@@ -579,1 +578,2 @@\n-  if (is_concurrent_mark_in_progress())        st->print(\"marking, \");\n+  if (is_concurrent_old_mark_in_progress())    st->print(\"old marking, \");\n+  if (is_concurrent_young_mark_in_progress())  st->print(\"young marking, \");\n@@ -630,0 +630,2 @@\n+  _mmu_tracker.initialize();\n+\n@@ -643,2 +645,0 @@\n-  _heuristics->initialize();\n-\n@@ -648,0 +648,4 @@\n+ShenandoahHeuristics* ShenandoahHeap::heuristics() {\n+  return _global_generation->heuristics();\n+}\n+\n@@ -649,1 +653,1 @@\n-  return Atomic::load(&_used);\n+  return global_generation()->used();\n@@ -656,4 +660,0 @@\n-size_t ShenandoahHeap::available() const {\n-  return free_set()->available();\n-}\n-\n@@ -670,2 +670,43 @@\n-void ShenandoahHeap::increase_used(size_t bytes) {\n-  Atomic::add(&_used, bytes, memory_order_relaxed);\n+\/\/ For tracking usage based on allocations, it should be the case that:\n+\/\/ * The sum of regions::used == heap::used\n+\/\/ * The sum of a generation's regions::used == generation::used\n+\/\/ * The sum of a generation's humongous regions::free == generation::humongous_waste\n+\/\/ These invariants are checked by the verifier on GC safepoints.\n+\/\/\n+\/\/ Additional notes:\n+\/\/ * When a mutator's allocation request causes a region to be retired, the\n+\/\/   free memory left in that region is considered waste. It does not contribute\n+\/\/   to the usage, but it _does_ contribute to allocation rate.\n+\/\/ * The bottom of a PLAB must be aligned on card size. In some cases this will\n+\/\/   require padding in front of the PLAB (a filler object). Because this padding\n+\/\/   is included in the region's used memory we include the padding in the usage\n+\/\/   accounting as waste.\n+\/\/ * Mutator allocations are used to compute an allocation rate. They are also\n+\/\/   sent to the Pacer for those purposes.\n+\/\/ * There are three sources of waste:\n+\/\/  1. The padding used to align a PLAB on card size\n+\/\/  2. Region's free is less than minimum TLAB size and is retired\n+\/\/  3. The unused portion of memory in the last region of a humongous object\n+void ShenandoahHeap::increase_used(const ShenandoahAllocRequest& req) {\n+  size_t actual_bytes = req.actual_size() * HeapWordSize;\n+  size_t wasted_bytes = req.waste() * HeapWordSize;\n+  ShenandoahGeneration* generation = generation_for(req.affiliation());\n+\n+  if (req.is_gc_alloc()) {\n+    assert(wasted_bytes == 0 || req.type() == ShenandoahAllocRequest::_alloc_plab, \"Only PLABs have waste\");\n+    increase_used(generation, actual_bytes + wasted_bytes);\n+  } else {\n+    assert(req.is_mutator_alloc(), \"Expected mutator alloc here\");\n+    \/\/ padding and actual size both count towards allocation counter\n+    generation->increase_allocated(actual_bytes + wasted_bytes);\n+\n+    \/\/ only actual size counts toward usage for mutator allocations\n+    increase_used(generation, actual_bytes);\n+\n+    \/\/ notify pacer of both actual size and waste\n+    notify_mutator_alloc_words(req.actual_size(), req.waste());\n+\n+    if (wasted_bytes > 0 && req.actual_size() > ShenandoahHeapRegion::humongous_threshold_words()) {\n+      increase_humongous_waste(generation,wasted_bytes);\n+    }\n+  }\n@@ -674,2 +715,5 @@\n-void ShenandoahHeap::set_used(size_t bytes) {\n-  Atomic::store(&_used, bytes);\n+void ShenandoahHeap::increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_humongous_waste(bytes);\n+  }\n@@ -678,3 +722,5 @@\n-void ShenandoahHeap::decrease_used(size_t bytes) {\n-  assert(used() >= bytes, \"never decrease heap size by more than we've left\");\n-  Atomic::sub(&_used, bytes, memory_order_relaxed);\n+void ShenandoahHeap::decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_humongous_waste(bytes);\n+  }\n@@ -683,2 +729,5 @@\n-void ShenandoahHeap::increase_allocated(size_t bytes) {\n-  Atomic::add(&_bytes_allocated_since_gc_start, bytes, memory_order_relaxed);\n+void ShenandoahHeap::increase_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_used(bytes);\n+  }\n@@ -687,4 +736,4 @@\n-void ShenandoahHeap::notify_mutator_alloc_words(size_t words, bool waste) {\n-  size_t bytes = words * HeapWordSize;\n-  if (!waste) {\n-    increase_used(bytes);\n+void ShenandoahHeap::decrease_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_used(bytes);\n@@ -692,1 +741,3 @@\n-  increase_allocated(bytes);\n+}\n+\n+void ShenandoahHeap::notify_mutator_alloc_words(size_t words, size_t waste) {\n@@ -695,2 +746,2 @@\n-    if (waste) {\n-      pacer()->claim_for_alloc(words, true);\n+    if (waste > 0) {\n+      pacer()->claim_for_alloc(waste, true);\n@@ -732,0 +783,1 @@\n+<<<<<<< HEAD\n@@ -747,0 +799,2 @@\n+=======\n+>>>>>>> b7405b19d531269ccb611cb5dc6a07f491a7b9de\n@@ -826,3 +880,1 @@\n-\n-  \/\/ This is called from allocation path, and thus should be fast.\n-  _heap_changed.try_set();\n+  _heap_changed.set();\n@@ -845,0 +897,1 @@\n+\n@@ -851,0 +904,1 @@\n+  log_debug(gc, free)(\"Set new GCLAB size: \" SIZE_FORMAT, new_size);\n@@ -856,0 +910,1 @@\n+    log_debug(gc, free)(\"New gclab size (\" SIZE_FORMAT \") is too small for \" SIZE_FORMAT, new_size, size);\n@@ -885,0 +940,1 @@\n+\/\/ Called from stubs in JIT code or interpreter\n@@ -933,0 +989,1 @@\n+    \/\/ gc_no_progress_count is incremented following each degen or full GC that fails to achieve is_good_progress().\n@@ -935,0 +992,1 @@\n+      req.set_actual_size(0);\n@@ -949,2 +1007,0 @@\n-      \/\/\n-      \/\/ TODO: Consider GLOBAL GC rather than Full GC to remediate OOM condition: https:\/\/bugs.openjdk.org\/browse\/JDK-8335910\n@@ -961,1 +1017,1 @@\n-      if (log_is_enabled(Debug, gc, alloc)) {\n+      if (log_develop_is_enabled(Debug, gc, alloc)) {\n@@ -980,0 +1036,8 @@\n+  if (result == nullptr) {\n+    req.set_actual_size(0);\n+  }\n+\n+  \/\/ This is called regardless of the outcome of the allocation to account\n+  \/\/ for any waste created by retiring regions with this request.\n+  increase_used(req);\n+\n@@ -989,2 +1053,0 @@\n-      notify_mutator_alloc_words(actual, false);\n-\n@@ -997,2 +1059,0 @@\n-    } else {\n-      increase_used(actual*HeapWordSize);\n@@ -1011,1 +1071,37 @@\n-  return _free_set->allocate(req, in_new_region);\n+\n+  \/\/ Make sure the old generation has room for either evacuations or promotions before trying to allocate.\n+  if (req.is_old() && !old_generation()->can_allocate(req)) {\n+    return nullptr;\n+  }\n+\n+  \/\/ If TLAB request size is greater than available, allocate() will attempt to downsize request to fit within available\n+  \/\/ memory.\n+  HeapWord* result = _free_set->allocate(req, in_new_region);\n+\n+  \/\/ Record the plab configuration for this result and register the object.\n+  if (result != nullptr && req.is_old()) {\n+    old_generation()->configure_plab_for_current_thread(req);\n+    if (req.type() == ShenandoahAllocRequest::_alloc_shared_gc) {\n+      \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+      \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+      \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+      \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+      \/\/\n+      \/\/ objects being \"concurrently\" allocated:\n+      \/\/    [-----a------][-----b-----][--------------c------------------]\n+      \/\/            [---- card table memory range --------------]\n+      \/\/\n+      \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n+      \/\/ wants to set the starts-object, first-start, and last-start attributes of the preceding card region.\n+      \/\/ Allocation of object b wants to set the starts-object, first-start, and last-start attributes of this card region.\n+      \/\/ Allocation of object c also wants to set the starts-object, first-start, and last-start attributes of this\n+      \/\/ card region.\n+      \/\/\n+      \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n+      \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n+      \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+      old_generation()->card_scan()->register_object(result);\n+    }\n+  }\n+\n+  return result;\n@@ -1026,2 +1122,2 @@\n-  if (heuristics()->can_unload_classes()) {\n-    ShenandoahHeuristics* h = heuristics();\n+  ShenandoahHeuristics* h = global_generation()->heuristics();\n+  if (h->can_unload_classes()) {\n@@ -1120,2 +1216,8 @@\n-  ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n-  workers()->run_task(&task);\n+  if (mode()->is_generational()) {\n+    ShenandoahRegionIterator regions;\n+    ShenandoahGenerationalEvacuationTask task(ShenandoahGenerationalHeap::heap(), &regions, concurrent);\n+    workers()->run_task(&task);\n+  } else {\n+    ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n+    workers()->run_task(&task);\n+  }\n@@ -1125,3 +1227,4 @@\n-  if (ShenandoahThreadLocalData::is_oom_during_evac(Thread::current())) {\n-    \/\/ This thread went through the OOM during evac protocol and it is safe to return\n-    \/\/ the forward pointer. It must not attempt to evacuate any more.\n+  assert(thread == Thread::current(), \"Expected thread parameter to be current thread.\");\n+  if (ShenandoahThreadLocalData::is_oom_during_evac(thread)) {\n+    \/\/ This thread went through the OOM during evac protocol. It is safe to return\n+    \/\/ the forward pointer. It must not attempt to evacuate any other objects.\n@@ -1133,1 +1236,2 @@\n-  size_t size = p->size();\n+  ShenandoahHeapRegion* r = heap_region_containing(p);\n+  assert(!r->is_humongous(), \"never evacuate humongous objects\");\n@@ -1135,1 +1239,3 @@\n-  assert(!heap_region_containing(p)->is_humongous(), \"never evacuate humongous objects\");\n+  ShenandoahAffiliation target_gen = r->affiliation();\n+  return try_evacuate_object(p, thread, r, target_gen);\n+}\n@@ -1137,1 +1243,5 @@\n-  bool alloc_from_gclab = true;\n+oop ShenandoahHeap::try_evacuate_object(oop p, Thread* thread, ShenandoahHeapRegion* from_region,\n+                                               ShenandoahAffiliation target_gen) {\n+  assert(target_gen == YOUNG_GENERATION, \"Only expect evacuations to young in this mode\");\n+  assert(from_region->is_young(), \"Only expect evacuations from young in this mode\");\n+  bool alloc_from_lab = true;\n@@ -1139,0 +1249,1 @@\n+  size_t size = p->size();\n@@ -1148,0 +1259,7 @@\n+      if ((copy == nullptr) && (size < ShenandoahThreadLocalData::gclab_size(thread))) {\n+        \/\/ GCLAB allocation failed because we are bumping up against the limit on young evacuation reserve.  Try resetting\n+        \/\/ the desired GCLAB size and retry GCLAB allocation to avoid cascading of shared memory allocations.\n+        ShenandoahThreadLocalData::set_gclab_size(thread, PLAB::min_size());\n+        copy = allocate_from_gclab(thread, size);\n+        \/\/ If we still get nullptr, we'll try a shared allocation below.\n+      }\n@@ -1149,0 +1267,1 @@\n+\n@@ -1150,1 +1269,2 @@\n-      ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size);\n+      \/\/ If we failed to allocate in LAB, we'll try a shared allocation.\n+      ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen);\n@@ -1152,1 +1272,1 @@\n-      alloc_from_gclab = false;\n+      alloc_from_lab = false;\n@@ -1167,0 +1287,1 @@\n+  _evac_tracker->begin_evacuation(thread, size * HeapWordSize);\n@@ -1175,0 +1296,1 @@\n+    _evac_tracker->end_evacuation(thread, size * HeapWordSize);\n@@ -1183,7 +1305,4 @@\n-    \/\/\n-    \/\/ For GCLAB allocations, it is enough to rollback the allocation ptr. Either the next\n-    \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n-    \/\/ do this. For non-GCLAB allocations, we have no way to retract the allocation, and\n-    \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n-    \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n-    if (alloc_from_gclab) {\n+    if (alloc_from_lab) {\n+      \/\/ For LAB allocations, it is enough to rollback the allocation ptr. Either the next\n+      \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n+      \/\/ do this.\n@@ -1192,0 +1311,4 @@\n+      \/\/ For non-LAB allocations, we have no way to retract the allocation, and\n+      \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n+      \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n+      assert(size >= ShenandoahHeap::min_fill_size(), \"previously allocated object known to be larger than min_size\");\n@@ -1194,0 +1317,1 @@\n+      \/\/ For non-LAB allocations, the object has already been registered\n@@ -1227,1 +1351,1 @@\n-void ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n+size_t ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n@@ -1247,0 +1371,1 @@\n+  return required_regions;\n@@ -1256,0 +1381,6 @@\n+\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+      assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+      assert(plab->words_remaining() == 0, \"PLAB should not need retirement\");\n+    }\n@@ -1271,0 +1402,13 @@\n+\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+      assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+\n+      \/\/ There are two reasons to retire all plabs between old-gen evacuation passes.\n+      \/\/  1. We need to make the plab memory parsable by remembered-set scanning.\n+      \/\/  2. We need to establish a trustworthy UpdateWaterMark value within each old-gen heap region\n+      ShenandoahGenerationalHeap::heap()->retire_plab(plab, thread);\n+      if (_resize && ShenandoahThreadLocalData::plab_size(thread) > 0) {\n+        ShenandoahThreadLocalData::set_plab_size(thread, 0);\n+      }\n+    }\n@@ -1399,0 +1543,4 @@\n+    ls.cr();\n+\n+    evac_tracker()->print_global_on(&ls);\n+\n@@ -1404,0 +1552,40 @@\n+void ShenandoahHeap::set_gc_generation(ShenandoahGeneration* generation) {\n+  shenandoah_assert_control_or_vm_thread_at_safepoint();\n+  _gc_generation = generation;\n+}\n+\n+\/\/ Active generation may only be set by the VM thread at a safepoint.\n+void ShenandoahHeap::set_active_generation() {\n+  assert(Thread::current()->is_VM_thread(), \"Only the VM Thread\");\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Only at a safepoint!\");\n+  assert(_gc_generation != nullptr, \"Will set _active_generation to nullptr\");\n+  _active_generation = _gc_generation;\n+}\n+\n+void ShenandoahHeap::on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation) {\n+  shenandoah_policy()->record_collection_cause(cause);\n+\n+  assert(gc_cause()  == GCCause::_no_gc, \"Over-writing cause\");\n+  assert(_gc_generation == nullptr, \"Over-writing _gc_generation\");\n+\n+  set_gc_cause(cause);\n+  set_gc_generation(generation);\n+\n+  generation->heuristics()->record_cycle_start();\n+}\n+\n+void ShenandoahHeap::on_cycle_end(ShenandoahGeneration* generation) {\n+  assert(gc_cause() != GCCause::_no_gc, \"cause wasn't set\");\n+  assert(_gc_generation != nullptr, \"_gc_generation wasn't set\");\n+\n+  generation->heuristics()->record_cycle_end();\n+  if (mode()->is_generational() && generation->is_global()) {\n+    \/\/ If we just completed a GLOBAL GC, claim credit for completion of young-gen and old-gen GC as well\n+    young_generation()->heuristics()->record_cycle_end();\n+    old_generation()->heuristics()->record_cycle_end();\n+  }\n+\n+  set_gc_generation(nullptr);\n+  set_gc_cause(GCCause::_no_gc);\n+}\n+\n@@ -1753,99 +1941,0 @@\n-class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n-      \/\/ anyway to capture any updates that happened since now.\n-      r->clear_live_data();\n-      _ctx->capture_top_at_mark_start(r);\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_gc() {\n-  reset_mark_bitmap();\n-\n-  ShenandoahResetUpdateRegionStateClosure cl;\n-  parallel_heap_region_iterate(&cl);\n-}\n-\n-class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalMarkUpdateRegionStateClosure() :\n-    _ctx(ShenandoahHeap::heap()->complete_marking_context()), _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n-      \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n-      HeapWord *tams = _ctx->top_at_mark_start(r);\n-      HeapWord *top = r->top();\n-      if (top > tams) {\n-        r->increase_live_data_alloc_words(pointer_delta(top, tams));\n-      }\n-\n-      \/\/ We are about to select the collection set, make sure it knows about\n-      \/\/ current pinning status. Also, this allows trashing more regions that\n-      \/\/ now have their pinning status dropped.\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n-      }\n-\n-      \/\/ Remember limit for updating refs. It's guaranteed that we get no\n-      \/\/ from-space-refs written from here on.\n-      r->set_update_watermark_at_safepoint(r->top());\n-    } else {\n-      assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_regions_and_collection_set(bool concurrent) {\n-  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states :\n-                                         ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n-    ShenandoahFinalMarkUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n-\n-    assert_pinned_region_status();\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset :\n-                                         ShenandoahPhaseTimings::degen_gc_choose_cset);\n-    ShenandoahHeapLocker locker(lock());\n-    _collection_set->clear();\n-    heuristics()->choose_collection_set(_collection_set);\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset :\n-                                         ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n-  }\n-}\n-\n@@ -1854,0 +1943,3 @@\n+  if (mode()->is_generational()) {\n+    old_generation()->set_parsable(false);\n+  }\n@@ -1862,1 +1954,2 @@\n-  ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n+  shenandoah_assert_generations_reconciled();\n+  gc_generation()->ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n@@ -1896,0 +1989,41 @@\n+  \/\/ Check that if concurrent weak root is set then active_gen isn't null\n+  assert(!is_concurrent_weak_root_in_progress() || active_generation() != nullptr, \"Error\");\n+  shenandoah_assert_generations_reconciled();\n+}\n+\n+void ShenandoahHeap::set_concurrent_young_mark_in_progress(bool in_progress) {\n+  uint mask;\n+  assert(!has_forwarded_objects(), \"Young marking is not concurrent with evacuation\");\n+  if (!in_progress && is_concurrent_old_mark_in_progress()) {\n+    assert(mode()->is_generational(), \"Only generational GC has old marking\");\n+    assert(_gc_state.is_set(MARKING), \"concurrent_old_marking_in_progress implies MARKING\");\n+    \/\/ If old-marking is in progress when we turn off YOUNG_MARKING, leave MARKING (and OLD_MARKING) on\n+    mask = YOUNG_MARKING;\n+  } else {\n+    mask = MARKING | YOUNG_MARKING;\n+  }\n+  set_gc_state(mask, in_progress);\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::set_concurrent_old_mark_in_progress(bool in_progress) {\n+#ifdef ASSERT\n+  \/\/ has_forwarded_objects() iff UPDATEREFS or EVACUATION\n+  bool has_forwarded = has_forwarded_objects();\n+  bool updating_or_evacuating = _gc_state.is_set(UPDATEREFS | EVACUATION);\n+  bool evacuating = _gc_state.is_set(EVACUATION);\n+  assert ((has_forwarded == updating_or_evacuating) || (evacuating && !has_forwarded && collection_set()->is_empty()),\n+          \"Updating or evacuating iff has forwarded objects, or if evacuation phase is promoting in place without forwarding\");\n+#endif\n+  if (!in_progress && is_concurrent_young_mark_in_progress()) {\n+    \/\/ If young-marking is in progress when we turn off OLD_MARKING, leave MARKING (and YOUNG_MARKING) on\n+    assert(_gc_state.is_set(MARKING), \"concurrent_young_marking_in_progress implies MARKING\");\n+    set_gc_state(OLD_MARKING, in_progress);\n+  } else {\n+    set_gc_state(MARKING | OLD_MARKING, in_progress);\n+  }\n+  manage_satb_barrier(in_progress);\n+}\n+\n+bool ShenandoahHeap::is_prepare_for_old_mark_in_progress() const {\n+  return old_generation()->is_preparing_for_mark();\n@@ -1898,4 +2032,14 @@\n-void ShenandoahHeap::set_concurrent_mark_in_progress(bool in_progress) {\n-  assert(!has_forwarded_objects(), \"Not expected before\/after mark phase\");\n-  set_gc_state(MARKING, in_progress);\n-  ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(in_progress, !in_progress);\n+void ShenandoahHeap::manage_satb_barrier(bool active) {\n+  if (is_concurrent_mark_in_progress()) {\n+    \/\/ Ignore request to deactivate barrier while concurrent mark is in progress.\n+    \/\/ Do not attempt to re-activate the barrier if it is already active.\n+    if (active && !ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  } else {\n+    \/\/ No concurrent marking is in progress so honor request to deactivate,\n+    \/\/ but only if the barrier is already active.\n+    if (!active && ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  }\n@@ -1934,0 +2078,11 @@\n+void ShenandoahHeap::cancel_concurrent_mark() {\n+  if (mode()->is_generational()) {\n+    young_generation()->cancel_marking();\n+    old_generation()->cancel_marking();\n+  }\n+\n+  global_generation()->cancel_marking();\n+\n+  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+}\n+\n@@ -1939,0 +2094,1 @@\n+    _cancel_requested_time = os::elapsedTime();\n@@ -2062,4 +2218,0 @@\n-size_t ShenandoahHeap::bytes_allocated_since_gc_start() const {\n-  return Atomic::load(&_bytes_allocated_since_gc_start);\n-}\n-\n@@ -2067,1 +2219,6 @@\n-  Atomic::store(&_bytes_allocated_since_gc_start, (size_t)0);\n+  if (mode()->is_generational()) {\n+    young_generation()->reset_bytes_allocated_since_gc_start();\n+    old_generation()->reset_bytes_allocated_since_gc_start();\n+  }\n+\n+  global_generation()->reset_bytes_allocated_since_gc_start();\n@@ -2131,2 +2288,5 @@\n-    assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n-           \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    shenandoah_assert_generations_reconciled();\n+    if (gc_generation()->contains(r)) {\n+      assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n+             \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    }\n@@ -2187,1 +2347,1 @@\n-  ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n+  explicit ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n@@ -2207,1 +2367,0 @@\n-    T cl;\n@@ -2212,2 +2371,5 @@\n-      \/\/ We cannot transfer any more regions than will be reclaimed when the existing collection set is recycled because\n-      \/\/ we need the reclaimed collection set regions to replenish the collector reserves\n+\n+      \/\/ Now that evacuation is done, we can reassign any regions that had been reserved to hold the results of evacuation\n+      \/\/ to the mutator free set.  At the end of GC, we will have cset_regions newly evacuated fully empty regions from\n+      \/\/ which we will be able to replenish the Collector free set and the OldCollector free set in preparation for the\n+      \/\/ next GC cycle.\n@@ -2217,1 +2379,1 @@\n-\n+    T cl;\n@@ -2219,1 +2381,0 @@\n-    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n@@ -2225,3 +2386,3 @@\n-      }\n-      if (ShenandoahPacing) {\n-        _heap->pacer()->report_updaterefs(pointer_delta(update_watermark, r->bottom()));\n+        if (ShenandoahPacing) {\n+          _heap->pacer()->report_updaterefs(pointer_delta(update_watermark, r->bottom()));\n+        }\n@@ -2249,0 +2410,1 @@\n+ShenandoahSynchronizePinnedRegionStates::ShenandoahSynchronizePinnedRegionStates() : _lock(ShenandoahHeap::heap()->lock()) { }\n@@ -2250,22 +2412,13 @@\n-class ShenandoahFinalUpdateRefsUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalUpdateRefsUpdateRegionStateClosure() : _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    \/\/ Drop unnecessary \"pinned\" state from regions that does not have CP marks\n-    \/\/ anymore, as this would allow trashing them.\n-\n-    if (r->is_active()) {\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n+void ShenandoahSynchronizePinnedRegionStates::heap_region_do(ShenandoahHeapRegion* r) {\n+  \/\/ Drop \"pinned\" state from regions that no longer have a pinned count. Put\n+  \/\/ regions with a pinned count into the \"pinned\" state.\n+  if (r->is_active()) {\n+    if (r->is_pinned()) {\n+      if (r->pin_count() == 0) {\n+        ShenandoahHeapLocker locker(_lock);\n+        r->make_unpinned();\n+      }\n+    } else {\n+      if (r->pin_count() > 0) {\n+        ShenandoahHeapLocker locker(_lock);\n+        r->make_pinned();\n@@ -2275,3 +2428,1 @@\n-\n-  bool is_thread_safe() { return true; }\n-};\n+}\n@@ -2287,2 +2438,2 @@\n-    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n+\n+    final_update_refs_update_region_states();\n@@ -2301,0 +2452,5 @@\n+void ShenandoahHeap::final_update_refs_update_region_states() {\n+  ShenandoahSynchronizePinnedRegionStates cl;\n+  parallel_heap_region_iterate(&cl);\n+}\n+\n@@ -2302,6 +2458,42 @@\n-  {\n-    ShenandoahGCPhase phase(concurrent ?\n-                            ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n-                            ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n+  ShenandoahGCPhase phase(concurrent ?\n+                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n+                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+  ShenandoahHeapLocker locker(lock());\n+  size_t young_cset_regions, old_cset_regions;\n+  size_t first_old_region, last_old_region, old_region_count;\n+  _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+  \/\/ If there are no old regions, first_old_region will be greater than last_old_region\n+  assert((first_old_region > last_old_region) ||\n+         ((last_old_region + 1 - first_old_region >= old_region_count) &&\n+          get_region(first_old_region)->is_old() && get_region(last_old_region)->is_old()),\n+         \"sanity: old_region_count: \" SIZE_FORMAT \", first_old_region: \" SIZE_FORMAT \", last_old_region: \" SIZE_FORMAT,\n+         old_region_count, first_old_region, last_old_region);\n+\n+  if (mode()->is_generational()) {\n+#ifdef ASSERT\n+    if (ShenandoahVerify) {\n+      verifier()->verify_before_rebuilding_free_set();\n+    }\n+#endif\n+\n+    \/\/ The computation of bytes_of_allocation_runway_before_gc_trigger is quite conservative so consider all of this\n+    \/\/ available for transfer to old. Note that transfer of humongous regions does not impact available.\n+    ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+    size_t allocation_runway = gen_heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions);\n+    gen_heap->compute_old_generation_balance(allocation_runway, old_cset_regions);\n+\n+    \/\/ Total old_available may have been expanded to hold anticipated promotions.  We trigger if the fragmented available\n+    \/\/ memory represents more than 16 regions worth of data.  Note that fragmentation may increase when we promote regular\n+    \/\/ regions in place when many of these regular regions have an abundant amount of available memory within them.  Fragmentation\n+    \/\/ will decrease as promote-by-copy consumes the available memory within these partially consumed regions.\n+    \/\/\n+    \/\/ We consider old-gen to have excessive fragmentation if more than 12.5% of old-gen is free memory that resides\n+    \/\/ within partially consumed regions of memory.\n+  }\n+  \/\/ Rebuild free set based on adjusted generation sizes.\n+  _free_set->finish_rebuild(young_cset_regions, old_cset_regions, old_region_count);\n+\n+  if (mode()->is_generational()) {\n+    ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+    ShenandoahOldGeneration* old_gen = gen_heap->old_generation();\n+    old_gen->heuristics()->trigger_maybe(first_old_region, last_old_region, old_region_count, num_regions());\n@@ -2430,1 +2622,1 @@\n-  return _memory_pool->get_memory_usage();\n+  return MemoryUsage(_initial_size, used(), committed(), max_capacity());\n@@ -2496,0 +2688,1 @@\n+<<<<<<< HEAD\n@@ -2571,0 +2764,22 @@\n+=======\n+ShenandoahGeneration* ShenandoahHeap::generation_for(ShenandoahAffiliation affiliation) const {\n+  if (!mode()->is_generational()) {\n+    return global_generation();\n+  } else if (affiliation == YOUNG_GENERATION) {\n+    return young_generation();\n+  } else if (affiliation == OLD_GENERATION) {\n+    return old_generation();\n+  }\n+\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n+\n+void ShenandoahHeap::log_heap_status(const char* msg) const {\n+  if (mode()->is_generational()) {\n+    young_generation()->log_status(msg);\n+    old_generation()->log_status(msg);\n+  } else {\n+    global_generation()->log_status(msg);\n+  }\n+>>>>>>> b7405b19d531269ccb611cb5dc6a07f491a7b9de\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":497,"deletions":282,"binary":false,"changes":779,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -33,1 +34,0 @@\n-#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n@@ -35,0 +35,2 @@\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n+#include \"gc\/shenandoah\/shenandoahController.hpp\"\n@@ -37,0 +39,5 @@\n+#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationSizer.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMmuTracker.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -48,1 +55,0 @@\n-class ShenandoahControlThread;\n@@ -51,0 +57,3 @@\n+class ShenandoahGeneration;\n+class ShenandoahYoungGeneration;\n+class ShenandoahOldGeneration;\n@@ -116,0 +125,11 @@\n+class ShenandoahSynchronizePinnedRegionStates : public ShenandoahHeapRegionClosure {\n+private:\n+  ShenandoahHeapLock* const _lock;\n+\n+public:\n+  ShenandoahSynchronizePinnedRegionStates();\n+\n+  void heap_region_do(ShenandoahHeapRegion* r) override;\n+  bool is_thread_safe() override { return true; }\n+};\n+\n@@ -120,1 +140,1 @@\n-class ShenandoahHeap : public CollectedHeap, public ShenandoahSpaceInfo {\n+class ShenandoahHeap : public CollectedHeap {\n@@ -130,0 +150,1 @@\n+  friend class ShenandoahOldGC;\n@@ -139,0 +160,13 @@\n+  \/\/ Indicates the generation whose collection is in\n+  \/\/ progress. Mutator threads aren't allowed to read\n+  \/\/ this field.\n+  ShenandoahGeneration* _gc_generation;\n+\n+  \/\/ This is set and cleared by only the VMThread\n+  \/\/ at each STW pause (safepoint) to the value seen in\n+  \/\/ _gc_generation. This allows the value to be always consistently\n+  \/\/ seen by all mutators as well as all GC worker threads.\n+  \/\/ In that sense, it's a stable snapshot of _gc_generation that is\n+  \/\/ updated at each STW pause associated with a ShenandoahVMOp.\n+  ShenandoahGeneration* _active_generation;\n+\n@@ -144,0 +178,22 @@\n+  ShenandoahGeneration* gc_generation() const {\n+    \/\/ We don't want this field read by a mutator thread\n+    assert(!Thread::current()->is_Java_thread(), \"Not allowed\");\n+    \/\/ value of _gc_generation field, see above\n+    return _gc_generation;\n+  }\n+\n+  ShenandoahGeneration* active_generation() const {\n+    \/\/ value of _active_generation field, see above\n+    return _active_generation;\n+  }\n+\n+  \/\/ Set the _gc_generation field\n+  void set_gc_generation(ShenandoahGeneration* generation);\n+\n+  \/\/ Copy the value in the _gc_generation field into\n+  \/\/ the _active_generation field: can only be called at\n+  \/\/ a safepoint by the VMThread.\n+  void set_active_generation();\n+\n+  ShenandoahHeuristics* heuristics();\n+\n@@ -156,2 +212,2 @@\n-  void initialize_heuristics();\n-\n+  virtual void initialize_heuristics();\n+  virtual void print_init_logger() const;\n@@ -178,2 +234,3 @@\n-           size_t _initial_size;\n-           size_t _minimum_size;\n+  size_t _initial_size;\n+  size_t _minimum_size;\n+\n@@ -182,1 +239,0 @@\n-  volatile size_t _used;\n@@ -184,1 +240,0 @@\n-  volatile size_t _bytes_allocated_since_gc_start;\n@@ -187,0 +242,2 @@\n+  void increase_used(const ShenandoahAllocRequest& req);\n+\n@@ -188,3 +245,4 @@\n-  void increase_used(size_t bytes);\n-  void decrease_used(size_t bytes);\n-  void set_used(size_t bytes);\n+  void increase_used(ShenandoahGeneration* generation, size_t bytes);\n+  void decrease_used(ShenandoahGeneration* generation, size_t bytes);\n+  void increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes);\n+  void decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes);\n@@ -194,1 +252,0 @@\n-  void increase_allocated(size_t bytes);\n@@ -196,1 +253,0 @@\n-  size_t bytes_allocated_since_gc_start() const override;\n@@ -201,1 +257,1 @@\n-  size_t soft_max_capacity() const override;\n+  size_t soft_max_capacity() const;\n@@ -206,1 +262,0 @@\n-  size_t available()         const override;\n@@ -226,0 +281,2 @@\n+  virtual void initialize_controller();\n+\n@@ -242,1 +299,1 @@\n-  ShenandoahRegionIterator _update_refs_iterator;\n+  uint8_t* _affiliations;       \/\/ Holds array of enum ShenandoahAffiliation, including FREE status in non-generational mode\n@@ -259,0 +316,2 @@\n+  inline ShenandoahMmuTracker* mmu_tracker() { return &_mmu_tracker; };\n+\n@@ -274,0 +333,1 @@\n+    \/\/ For generational mode, it means either young or old marking, or both.\n@@ -284,0 +344,6 @@\n+\n+    \/\/ Young regions are under marking, need SATB barriers.\n+    YOUNG_MARKING_BITPOS = 5,\n+\n+    \/\/ Old regions are under marking, need SATB barriers.\n+    OLD_MARKING_BITPOS = 6\n@@ -293,0 +359,2 @@\n+    YOUNG_MARKING = 1 << YOUNG_MARKING_BITPOS,\n+    OLD_MARKING   = 1 << OLD_MARKING_BITPOS\n@@ -298,2 +366,0 @@\n-\n-  \/\/ tracks if new regions have been allocated or retired since last check\n@@ -328,1 +394,2 @@\n-  void set_concurrent_mark_in_progress(bool in_progress);\n+  void set_concurrent_young_mark_in_progress(bool in_progress);\n+  void set_concurrent_old_mark_in_progress(bool in_progress);\n@@ -340,0 +407,1 @@\n+\n@@ -341,0 +409,2 @@\n+  inline bool is_concurrent_young_mark_in_progress() const;\n+  inline bool is_concurrent_old_mark_in_progress() const;\n@@ -351,0 +421,1 @@\n+  bool is_prepare_for_old_mark_in_progress() const;\n@@ -353,0 +424,2 @@\n+  void manage_satb_barrier(bool active);\n+\n@@ -363,0 +436,1 @@\n+  double _cancel_requested_time;\n@@ -364,0 +438,5 @@\n+\n+  \/\/ Returns true if cancel request was successfully communicated.\n+  \/\/ Returns false if some other thread already communicated cancel\n+  \/\/ request.  A true return value does not mean GC has been\n+  \/\/ cancelled, only that the process of cancelling GC has begun.\n@@ -367,1 +446,0 @@\n-\n@@ -371,1 +449,1 @@\n-  inline void clear_cancelled_gc();\n+  inline void clear_cancelled_gc(bool clear_oom_handler = true);\n@@ -373,0 +451,1 @@\n+  void cancel_concurrent_mark();\n@@ -384,0 +463,6 @@\n+protected:\n+  \/\/ This is shared between shConcurrentGC and shDegenerateGC so that degenerated\n+  \/\/ GC can resume update refs from where the concurrent GC was cancelled. It is\n+  \/\/ also used in shGenerationalHeap, which uses a different closure for update refs.\n+  ShenandoahRegionIterator _update_refs_iterator;\n+\n@@ -386,3 +471,0 @@\n-  \/\/ Reset bitmap, prepare regions for new GC cycle\n-  void prepare_gc();\n-  void prepare_regions_and_collection_set(bool concurrent);\n@@ -398,1 +480,1 @@\n-  void update_heap_references(bool concurrent);\n+  virtual void update_heap_references(bool concurrent);\n@@ -401,1 +483,1 @@\n-  void rebuild_free_set(bool concurrent);\n+  virtual void final_update_refs_update_region_states();\n@@ -406,0 +488,1 @@\n+  void rebuild_free_set(bool concurrent);\n@@ -413,1 +496,9 @@\n-  ShenandoahControlThread*   _control_thread;\n+  ShenandoahGeneration*  _global_generation;\n+\n+protected:\n+  ShenandoahController*  _control_thread;\n+\n+  ShenandoahYoungGeneration* _young_generation;\n+  ShenandoahOldGeneration*   _old_generation;\n+\n+private:\n@@ -416,1 +507,0 @@\n-  ShenandoahHeuristics*      _heuristics;\n@@ -421,3 +511,3 @@\n-  ShenandoahPhaseTimings*    _phase_timings;\n-\n-  ShenandoahControlThread*   control_thread()          { return _control_thread;    }\n+  ShenandoahPhaseTimings*       _phase_timings;\n+  ShenandoahEvacuationTracker*  _evac_tracker;\n+  ShenandoahMmuTracker          _mmu_tracker;\n@@ -426,0 +516,15 @@\n+  ShenandoahController*   control_thread() { return _control_thread; }\n+\n+  ShenandoahGeneration*      global_generation() const { return _global_generation; }\n+  ShenandoahYoungGeneration* young_generation()  const {\n+    assert(mode()->is_generational(), \"Young generation requires generational mode\");\n+    return _young_generation;\n+  }\n+\n+  ShenandoahOldGeneration*   old_generation()    const {\n+    assert(mode()->is_generational(), \"Old generation requires generational mode\");\n+    return _old_generation;\n+  }\n+\n+  ShenandoahGeneration*      generation_for(ShenandoahAffiliation affiliation) const;\n+\n@@ -428,1 +533,0 @@\n-  ShenandoahHeuristics*      heuristics()        const { return _heuristics;        }\n@@ -432,1 +536,7 @@\n-  ShenandoahPhaseTimings*    phase_timings()     const { return _phase_timings;     }\n+  ShenandoahPhaseTimings*      phase_timings()   const { return _phase_timings;     }\n+  ShenandoahEvacuationTracker* evac_tracker()    const { return _evac_tracker;      }\n+\n+  ShenandoahEvacOOMHandler* oom_evac_handler() { return &_oom_evac_handler; }\n+\n+  void on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation);\n+  void on_cycle_end(ShenandoahGeneration* generation);\n@@ -447,1 +557,1 @@\n-  ShenandoahMonitoringSupport* monitoring_support()          { return _monitoring_support;    }\n+  ShenandoahMonitoringSupport* monitoring_support() const    { return _monitoring_support;    }\n@@ -457,8 +567,0 @@\n-\/\/ ---------- Reference processing\n-\/\/\n-private:\n-  ShenandoahReferenceProcessor* const _ref_processor;\n-\n-public:\n-  ShenandoahReferenceProcessor* ref_processor() { return _ref_processor; }\n-\n@@ -483,0 +585,3 @@\n+  inline void assert_lock_for_affiliation(ShenandoahAffiliation orig_affiliation,\n+                                          ShenandoahAffiliation new_affiliation);\n+\n@@ -496,0 +601,1 @@\n+<<<<<<< HEAD\n@@ -499,0 +605,13 @@\n+=======\n+  inline bool is_in(const void* p) const override;\n+\n+  inline bool is_in_active_generation(oop obj) const;\n+  inline bool is_in_young(const void* p) const;\n+  inline bool is_in_old(const void* p) const;\n+  inline bool is_old(oop pobj) const;\n+\n+  inline ShenandoahAffiliation region_affiliation(const ShenandoahHeapRegion* r);\n+  inline void set_affiliation(ShenandoahHeapRegion* r, ShenandoahAffiliation new_affiliation);\n+\n+  inline ShenandoahAffiliation region_affiliation(size_t index);\n+>>>>>>> b7405b19d531269ccb611cb5dc6a07f491a7b9de\n@@ -552,0 +671,3 @@\n+protected:\n+  inline HeapWord* allocate_from_gclab(Thread* thread, size_t size);\n+\n@@ -554,1 +676,0 @@\n-  inline HeapWord* allocate_from_gclab(Thread* thread, size_t size);\n@@ -565,1 +686,1 @@\n-  void notify_mutator_alloc_words(size_t words, bool waste);\n+  void notify_mutator_alloc_words(size_t words, size_t waste);\n@@ -603,2 +724,0 @@\n-  inline void mark_complete_marking_context();\n-  inline void mark_incomplete_marking_context();\n@@ -615,2 +734,0 @@\n-  void reset_mark_bitmap();\n-\n@@ -637,0 +754,1 @@\n+  oop try_evacuate_object(oop src, Thread* thread, ShenandoahHeapRegion* from_region, ShenandoahAffiliation target_gen);\n@@ -638,0 +756,1 @@\n+\n@@ -648,1 +767,1 @@\n-  \/\/ Evacuates object src. Returns the evacuated object, either evacuated\n+  \/\/ Evacuates or promotes object src. Returns the evacuated object, either evacuated\n@@ -650,1 +769,1 @@\n-  oop evacuate_object(oop src, Thread* thread);\n+  virtual oop evacuate_object(oop src, Thread* thread);\n@@ -677,1 +796,10 @@\n-  void trash_humongous_region_at(ShenandoahHeapRegion *r);\n+  size_t trash_humongous_region_at(ShenandoahHeapRegion *r);\n+\n+  static inline void increase_object_age(oop obj, uint additional_age);\n+\n+  \/\/ Return the object's age, or a sentinel value when the age can't\n+  \/\/ necessarily be determined because of concurrent locking by the\n+  \/\/ mutator\n+  static inline uint get_object_age(oop obj);\n+\n+  void log_heap_status(const char *msg) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":180,"deletions":52,"binary":false,"changes":232,"status":"modified"},{"patch":"@@ -3,1 +3,2 @@\n- * Copyright (c) 2013, 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2013, 2020, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -27,0 +28,1 @@\n+#include \"gc\/shared\/cardTable.hpp\"\n@@ -29,0 +31,2 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n@@ -33,0 +37,4 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -47,0 +55,1 @@\n+\n@@ -63,0 +72,1 @@\n+  _top_before_promoted(nullptr),\n@@ -67,0 +77,1 @@\n+  _plab_allocs(0),\n@@ -69,1 +80,6 @@\n-  _update_watermark(start) {\n+  _update_watermark(start),\n+  _age(0)\n+#ifdef SHENANDOAH_CENSUS_NOISE\n+  , _youth(0)\n+#endif \/\/ SHENANDOAH_CENSUS_NOISE\n+  {\n@@ -85,1 +101,1 @@\n-void ShenandoahHeapRegion::make_regular_allocation() {\n+void ShenandoahHeapRegion::make_regular_allocation(ShenandoahAffiliation affiliation) {\n@@ -87,1 +103,1 @@\n-\n+  reset_age();\n@@ -92,0 +108,1 @@\n+      assert(this->affiliation() == affiliation, \"Region affiliation should already be established\");\n@@ -101,0 +118,25 @@\n+\/\/ Change affiliation to YOUNG_GENERATION if _state is not _pinned_cset, _regular, or _pinned.  This implements\n+\/\/ behavior previously performed as a side effect of make_regular_bypass().  This is used by Full GC in non-generational\n+\/\/ modes to transition regions from FREE. Note that all non-free regions in single-generational modes are young.\n+void ShenandoahHeapRegion::make_affiliated_maybe() {\n+  shenandoah_assert_heaplocked();\n+  assert(!ShenandoahHeap::heap()->mode()->is_generational(), \"Only call if non-generational\");\n+  switch (_state) {\n+   case _empty_uncommitted:\n+   case _empty_committed:\n+   case _cset:\n+   case _humongous_start:\n+   case _humongous_cont:\n+     if (affiliation() != YOUNG_GENERATION) {\n+       set_affiliation(YOUNG_GENERATION);\n+     }\n+     return;\n+   case _pinned_cset:\n+   case _regular:\n+   case _pinned:\n+     return;\n+   default:\n+     assert(false, \"Unexpected _state in make_affiliated_maybe\");\n+  }\n+}\n+\n@@ -103,0 +145,1 @@\n+<<<<<<< HEAD\n@@ -108,0 +151,5 @@\n+=======\n+  assert (ShenandoahHeap::heap()->is_full_gc_in_progress() || ShenandoahHeap::heap()->is_degenerated_gc_in_progress(),\n+          \"only for full or degen GC\");\n+  reset_age();\n+>>>>>>> b7405b19d531269ccb611cb5dc6a07f491a7b9de\n@@ -130,0 +178,1 @@\n+  reset_age();\n@@ -141,1 +190,1 @@\n-void ShenandoahHeapRegion::make_humongous_start_bypass() {\n+void ShenandoahHeapRegion::make_humongous_start_bypass(ShenandoahAffiliation affiliation) {\n@@ -144,1 +193,3 @@\n-\n+  \/\/ Don't bother to account for affiliated regions during Full GC.  We recompute totals at end.\n+  set_affiliation(affiliation);\n+  reset_age();\n@@ -159,0 +210,1 @@\n+  reset_age();\n@@ -170,1 +222,1 @@\n-void ShenandoahHeapRegion::make_humongous_cont_bypass() {\n+void ShenandoahHeapRegion::make_humongous_cont_bypass(ShenandoahAffiliation affiliation) {\n@@ -173,1 +225,3 @@\n-\n+  set_affiliation(affiliation);\n+  \/\/ Don't bother to account for affiliated regions during Full GC.  We recompute totals at end.\n+  reset_age();\n@@ -214,0 +268,1 @@\n+      assert(is_affiliated(), \"Pinned region should be affiliated\");\n@@ -232,0 +287,1 @@\n+  \/\/ Leave age untouched.  We need to consult the age when we are deciding whether to promote evacuated objects.\n@@ -244,0 +300,1 @@\n+  reset_age();\n@@ -245,2 +302,0 @@\n-    case _cset:\n-      \/\/ Reclaiming cset regions\n@@ -249,1 +304,7 @@\n-      \/\/ Reclaiming humongous regions\n+    {\n+      \/\/ Reclaiming humongous regions and reclaim humongous waste.  When this region is eventually recycled, we'll reclaim\n+      \/\/ its used memory.  At recycle time, we no longer recognize this as a humongous region.\n+      decrement_humongous_waste();\n+    }\n+    case _cset:\n+      \/\/ Reclaiming cset regions\n@@ -264,1 +325,3 @@\n-  ShenandoahHeap::heap()->complete_marking_context()->reset_top_bitmap(this);\n+  assert(ShenandoahHeap::heap()->gc_generation()->is_mark_complete(), \"Marking should be complete here.\");\n+  shenandoah_assert_generations_reconciled();\n+  ShenandoahHeap::heap()->marking_context()->reset_top_bitmap(this);\n@@ -269,0 +332,2 @@\n+  reset_age();\n+  CENSUS_NOISE(clear_youth();)\n@@ -308,0 +373,1 @@\n+  _plab_allocs = 0;\n@@ -311,1 +377,1 @@\n-  return used() - (_tlab_allocs + _gclab_allocs) * HeapWordSize;\n+  return used() - (_tlab_allocs + _gclab_allocs + _plab_allocs) * HeapWordSize;\n@@ -322,0 +388,4 @@\n+size_t ShenandoahHeapRegion::get_plab_allocs() const {\n+  return _plab_allocs * HeapWordSize;\n+}\n+\n@@ -366,0 +436,2 @@\n+  st->print(\"|%s\", shenandoah_affiliation_code(affiliation()));\n+\n@@ -377,0 +449,3 @@\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    st->print(\"|P \" SIZE_FORMAT_W(5) \"%1s\", byte_size_in_proper_unit(get_plab_allocs()),   proper_unit_for_byte_size(get_plab_allocs()));\n+  }\n@@ -385,6 +460,7 @@\n-void ShenandoahHeapRegion::oop_iterate(OopIterateClosure* blk) {\n-  if (!is_active()) return;\n-  if (is_humongous()) {\n-    oop_iterate_humongous(blk);\n-  } else {\n-    oop_iterate_objects(blk);\n+\/\/ oop_iterate without closure, return true if completed without cancellation\n+bool ShenandoahHeapRegion::oop_coalesce_and_fill(bool cancellable) {\n+\n+  assert(!is_humongous(), \"No need to fill or coalesce humongous regions\");\n+  if (!is_active()) {\n+    end_preemptible_coalesce_and_fill();\n+    return true;\n@@ -392,1 +468,0 @@\n-}\n@@ -394,5 +469,17 @@\n-void ShenandoahHeapRegion::oop_iterate_objects(OopIterateClosure* blk) {\n-  assert(! is_humongous(), \"no humongous region here\");\n-  HeapWord* obj_addr = bottom();\n-  HeapWord* t = top();\n-  \/\/ Could call objects iterate, but this is easier.\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  ShenandoahMarkingContext* marking_context = heap->marking_context();\n+\n+  \/\/ Expect marking to be completed before these threads invoke this service.\n+  assert(heap->gc_generation()->is_mark_complete(), \"sanity\");\n+  shenandoah_assert_generations_reconciled();\n+\n+  \/\/ All objects above TAMS are considered live even though their mark bits will not be set.  Note that young-\n+  \/\/ gen evacuations that interrupt a long-running old-gen concurrent mark may promote objects into old-gen\n+  \/\/ while the old-gen concurrent marking is ongoing.  These newly promoted objects will reside above TAMS\n+  \/\/ and will be treated as live during the current old-gen marking pass, even though they will not be\n+  \/\/ explicitly marked.\n+  HeapWord* t = marking_context->top_at_mark_start(this);\n+\n+  \/\/ Resume coalesce and fill from this address\n+  HeapWord* obj_addr = resume_coalesce_and_fill();\n+\n@@ -401,1 +488,17 @@\n-    obj_addr += obj->oop_iterate_size(blk);\n+    if (marking_context->is_marked(obj)) {\n+      assert(obj->klass() != nullptr, \"klass should not be nullptr\");\n+      obj_addr += obj->size();\n+    } else {\n+      \/\/ Object is not marked.  Coalesce and fill dead object with dead neighbors.\n+      HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, t);\n+      assert(next_marked_obj <= t, \"next marked object cannot exceed top\");\n+      size_t fill_size = next_marked_obj - obj_addr;\n+      assert(fill_size >= ShenandoahHeap::min_fill_size(), \"previously allocated object known to be larger than min_size\");\n+      ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n+      heap->old_generation()->card_scan()->coalesce_objects(obj_addr, fill_size);\n+      obj_addr = next_marked_obj;\n+    }\n+    if (cancellable && heap->cancelled_gc()) {\n+      suspend_coalesce_and_fill(obj_addr);\n+      return false;\n+    }\n@@ -403,0 +506,10 @@\n+  \/\/ Mark that this region has been coalesced and filled\n+  end_preemptible_coalesce_and_fill();\n+  return true;\n+}\n+\n+size_t get_card_count(size_t words) {\n+  assert(words % CardTable::card_size_in_words() == 0, \"Humongous iteration must span whole number of cards\");\n+  assert(CardTable::card_size_in_words() * (words \/ CardTable::card_size_in_words()) == words,\n+         \"slice must be integral number of cards\");\n+  return words \/ CardTable::card_size_in_words();\n@@ -405,1 +518,2 @@\n-void ShenandoahHeapRegion::oop_iterate_humongous(OopIterateClosure* blk) {\n+void ShenandoahHeapRegion::oop_iterate_humongous_slice_dirty(OopIterateClosure* blk,\n+                                                             HeapWord* start, size_t words, bool write_table) const {\n@@ -407,1 +521,1 @@\n-  \/\/ Find head.\n+\n@@ -409,1 +523,0 @@\n-  assert(r->is_humongous_start(), \"need humongous head here\");\n@@ -411,1 +524,30 @@\n-  obj->oop_iterate(blk, MemRegion(bottom(), top()));\n+  size_t num_cards = get_card_count(words);\n+\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  ShenandoahScanRemembered* scanner = heap->old_generation()->card_scan();\n+  size_t card_index = scanner->card_index_for_addr(start);\n+  if (write_table) {\n+    while (num_cards-- > 0) {\n+      if (scanner->is_write_card_dirty(card_index++)) {\n+        obj->oop_iterate(blk, MemRegion(start, start + CardTable::card_size_in_words()));\n+      }\n+      start += CardTable::card_size_in_words();\n+    }\n+  } else {\n+    while (num_cards-- > 0) {\n+      if (scanner->is_card_dirty(card_index++)) {\n+        obj->oop_iterate(blk, MemRegion(start, start + CardTable::card_size_in_words()));\n+      }\n+      start += CardTable::card_size_in_words();\n+    }\n+  }\n+}\n+\n+void ShenandoahHeapRegion::oop_iterate_humongous_slice_all(OopIterateClosure* cl, HeapWord* start, size_t words) const {\n+  assert(is_humongous(), \"only humongous region here\");\n+\n+  ShenandoahHeapRegion* r = humongous_start_region();\n+  oop obj = cast_to_oop(r->bottom());\n+\n+  \/\/ Scan all data, regardless of whether cards are dirty\n+  obj->oop_iterate(cl, MemRegion(start, start + words));\n@@ -430,0 +572,7 @@\n+  shenandoah_assert_heaplocked();\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahGeneration* generation = heap->generation_for(affiliation());\n+\n+  heap->decrease_used(generation, used());\n+  generation->decrement_affiliated_region_count();\n+\n@@ -435,1 +584,1 @@\n-  ShenandoahHeap::heap()->marking_context()->reset_top_at_mark_start(this);\n+  heap->marking_context()->reset_top_at_mark_start(this);\n@@ -440,0 +589,1 @@\n+  set_affiliation(FREE);\n@@ -483,0 +633,5 @@\n+  \/\/ Generational Shenandoah needs this alignment for card tables.\n+  if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+    max_heap_size = align_up(max_heap_size , CardTable::ct_max_alignment_constraint());\n+  }\n+\n@@ -661,0 +816,62 @@\n+\n+void ShenandoahHeapRegion::set_affiliation(ShenandoahAffiliation new_affiliation) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  ShenandoahAffiliation region_affiliation = heap->region_affiliation(this);\n+  {\n+    ShenandoahMarkingContext* const ctx = heap->complete_marking_context();\n+    log_debug(gc)(\"Setting affiliation of Region \" SIZE_FORMAT \" from %s to %s, top: \" PTR_FORMAT \", TAMS: \" PTR_FORMAT\n+                  \", watermark: \" PTR_FORMAT \", top_bitmap: \" PTR_FORMAT,\n+                  index(), shenandoah_affiliation_name(region_affiliation), shenandoah_affiliation_name(new_affiliation),\n+                  p2i(top()), p2i(ctx->top_at_mark_start(this)), p2i(_update_watermark), p2i(ctx->top_bitmap(this)));\n+  }\n+\n+#ifdef ASSERT\n+  {\n+    \/\/ During full gc, heap->complete_marking_context() is not valid, may equal nullptr.\n+    ShenandoahMarkingContext* const ctx = heap->complete_marking_context();\n+    size_t idx = this->index();\n+    HeapWord* top_bitmap = ctx->top_bitmap(this);\n+\n+    assert(ctx->is_bitmap_clear_range(top_bitmap, _end),\n+           \"Region \" SIZE_FORMAT \", bitmap should be clear between top_bitmap: \" PTR_FORMAT \" and end: \" PTR_FORMAT, idx,\n+           p2i(top_bitmap), p2i(_end));\n+  }\n+#endif\n+\n+  if (region_affiliation == new_affiliation) {\n+    return;\n+  }\n+\n+  if (!heap->mode()->is_generational()) {\n+    log_trace(gc)(\"Changing affiliation of region %zu from %s to %s\",\n+                  index(), affiliation_name(), shenandoah_affiliation_name(new_affiliation));\n+    heap->set_affiliation(this, new_affiliation);\n+    return;\n+  }\n+\n+  switch (new_affiliation) {\n+    case FREE:\n+      assert(!has_live(), \"Free region should not have live data\");\n+      break;\n+    case YOUNG_GENERATION:\n+      reset_age();\n+      break;\n+    case OLD_GENERATION:\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      return;\n+  }\n+  heap->set_affiliation(this, new_affiliation);\n+}\n+\n+void ShenandoahHeapRegion::decrement_humongous_waste() const {\n+  assert(is_humongous(), \"Should only use this for humongous regions\");\n+  size_t waste_bytes = free();\n+  if (waste_bytes > 0) {\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    ShenandoahGeneration* generation = heap->generation_for(affiliation());\n+    heap->decrease_humongous_waste(generation, waste_bytes);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":248,"deletions":31,"binary":false,"changes":279,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -29,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -31,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -60,0 +63,17 @@\n+template <typename T>\n+static void card_mark_barrier(T* field, oop value) {\n+  assert(ShenandoahCardBarrier, \"Card-mark barrier should be on\");\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  assert(heap->is_in_or_null(value), \"Should be in heap\");\n+  if (heap->is_in_old(field) && heap->is_in_young(value)) {\n+    \/\/ For Shenandoah, each generation collects all the _referents_ that belong to the\n+    \/\/ collected generation. We can end up with discovered lists that contain a mixture\n+    \/\/ of old and young _references_. These references are linked together through the\n+    \/\/ discovered field in java.lang.Reference. In some cases, creating or editing this\n+    \/\/ list may result in the creation of _new_ old-to-young pointers which must dirty\n+    \/\/ the corresponding card. Failing to do this may cause heap verification errors and\n+    \/\/ lead to incorrect GC behavior.\n+    heap->old_generation()->mark_card_as_dirty(field);\n+  }\n+}\n+\n@@ -66,0 +86,3 @@\n+  if (ShenandoahCardBarrier) {\n+    card_mark_barrier(field, value);\n+  }\n@@ -71,0 +94,3 @@\n+  if (ShenandoahCardBarrier) {\n+    card_mark_barrier(field, value);\n+  }\n@@ -271,0 +297,1 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -287,0 +314,5 @@\n+  if (!heap->is_in_active_generation(referent)) {\n+    log_trace(gc,ref)(\"Referent outside of active generation: \" PTR_FORMAT, p2i(referent));\n+    return false;\n+  }\n+\n@@ -352,0 +384,3 @@\n+  \/\/ Each worker thread has a private copy of refproc_data, which includes a private discovered list.  This means\n+  \/\/ there's no risk that a different worker thread will try to manipulate my discovered list head while I'm making\n+  \/\/ reference the head of my discovered list.\n@@ -360,0 +395,11 @@\n+    \/\/ We successfully set this reference object's next pointer to discovered_head.  This marks reference as discovered.\n+    \/\/ If reference_cas_discovered fails, that means some other worker thread took credit for discovery of this reference,\n+    \/\/ and that other thread will place reference on its discovered list, so I can ignore reference.\n+\n+    \/\/ In case we have created an interesting pointer, mark the remembered set card as dirty.\n+    if (ShenandoahCardBarrier) {\n+      T* addr = reinterpret_cast<T*>(java_lang_ref_Reference::discovered_addr_raw(reference));\n+      card_mark_barrier(addr, discovered_head);\n+    }\n+\n+    \/\/ Make the discovered_list_head point to reference.\n@@ -374,1 +420,2 @@\n-  log_trace(gc, ref)(\"Encountered Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+  log_trace(gc, ref)(\"Encountered Reference: \" PTR_FORMAT \" (%s, %s)\",\n+          p2i(reference), reference_type_name(type), ShenandoahHeap::heap()->heap_region_containing(reference)->affiliation_name());\n@@ -389,0 +436,1 @@\n+<<<<<<< HEAD\n@@ -394,0 +442,5 @@\n+=======\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  oop referent = reference_referent<T>(reference);\n+  assert(referent == nullptr || heap->marking_context()->is_marked(referent), \"only drop references with alive referents\");\n+>>>>>>> b7405b19d531269ccb611cb5dc6a07f491a7b9de\n@@ -398,0 +451,8 @@\n+  \/\/ When this reference was discovered, it would not have been marked. If it ends up surviving\n+  \/\/ the cycle, we need to dirty the card if the reference is old and the referent is young.  Note\n+  \/\/ that if the reference is not dropped, then its pointer to the referent will be nulled before\n+  \/\/ evacuation begins so card does not need to be dirtied.\n+  if (heap->mode()->is_generational() && heap->is_in_old(reference) && heap->is_in_young(referent)) {\n+    \/\/ Note: would be sufficient to mark only the card that holds the start of this Reference object.\n+    heap->old_generation()->card_scan()->mark_range_as_dirty(cast_from_oop<HeapWord*>(reference), reference->size());\n+  }\n@@ -449,0 +510,1 @@\n+  \/\/ set_oop_field maintains the card mark barrier as this list is constructed.\n@@ -453,1 +515,1 @@\n-    RawAccess<>::oop_store(p, prev);\n+    set_oop_field(p, prev);\n@@ -525,0 +587,13 @@\n+\n+  \/\/ During reference processing, we maintain a local list of references that are identified by\n+  \/\/   _pending_list and _pending_list_tail.  _pending_list_tail points to the next field of the last Reference object on\n+  \/\/   the local list.\n+  \/\/\n+  \/\/ There is also a global list of reference identified by Universe::_reference_pending_list\n+\n+  \/\/ The following code has the effect of:\n+  \/\/  1. Making the global Universe::_reference_pending_list point to my local list\n+  \/\/  2. Overwriting the next field of the last Reference on my local list to point at the previous head of the\n+  \/\/     global Universe::_reference_pending_list\n+\n+  oop former_head_of_global_list = Universe::swap_reference_pending_list(_pending_list);\n@@ -526,1 +601,1 @@\n-    *reinterpret_cast<narrowOop*>(_pending_list_tail) = CompressedOops::encode(Universe::swap_reference_pending_list(_pending_list));\n+    set_oop_field<narrowOop>(reinterpret_cast<narrowOop*>(_pending_list_tail), former_head_of_global_list);\n@@ -528,1 +603,1 @@\n-    *reinterpret_cast<oop*>(_pending_list_tail) = Universe::swap_reference_pending_list(_pending_list);\n+    set_oop_field<oop>(reinterpret_cast<oop*>(_pending_list_tail), former_head_of_global_list);\n@@ -537,1 +612,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahReferenceProcessor.cpp","additions":79,"deletions":5,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -30,0 +31,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -32,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -33,0 +36,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -36,0 +40,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -64,0 +69,1 @@\n+<<<<<<< HEAD\n@@ -65,0 +71,3 @@\n+=======\n+  ShenandoahGeneration* _generation;\n+>>>>>>> b7405b19d531269ccb611cb5dc6a07f491a7b9de\n@@ -76,1 +85,2 @@\n-    _loc(nullptr) {\n+    _loc(nullptr),\n+    _generation(nullptr) {\n@@ -78,0 +88,1 @@\n+        options._verify_marked == ShenandoahVerifier::_verify_marked_complete_satb_empty ||\n@@ -87,0 +98,6 @@\n+\n+    if (_heap->mode()->is_generational()) {\n+      _generation = _heap->gc_generation();\n+      assert(_generation != nullptr, \"Expected active generation in this mode\");\n+      shenandoah_assert_generations_reconciled();\n+    }\n@@ -113,2 +130,1 @@\n-\n-      if (_map->par_mark(obj)) {\n+      if (in_generation(obj) && _map->par_mark(obj)) {\n@@ -121,0 +137,9 @@\n+  bool in_generation(oop obj) {\n+    if (_generation == nullptr) {\n+      return true;\n+    }\n+\n+    ShenandoahHeapRegion* region = _heap->heap_region_containing(obj);\n+    return _generation->contains(region);\n+  }\n+\n@@ -171,1 +196,2 @@\n-          check(ShenandoahAsserts::_safe_oop, obj, obj_reg->has_live(),\n+          check(ShenandoahAsserts::_safe_oop, obj, obj_reg->has_live() ||\n+                (obj_reg->is_old() && _heap->gc_generation()->is_young()),\n@@ -173,0 +199,1 @@\n+          shenandoah_assert_generations_reconciled();\n@@ -238,1 +265,0 @@\n-\n@@ -252,0 +278,1 @@\n+      case ShenandoahVerifier::_verify_marked_complete_satb_empty:\n@@ -334,0 +361,1 @@\n+<<<<<<< HEAD\n@@ -336,0 +364,4 @@\n+=======\n+  void do_oop(oop* p) override { do_oop_work(p); }\n+  void do_oop(narrowOop* p) override { do_oop_work(p); }\n+>>>>>>> b7405b19d531269ccb611cb5dc6a07f491a7b9de\n@@ -338,0 +370,2 @@\n+\/\/ This closure computes the amounts of used, committed, and garbage memory and the number of regions contained within\n+\/\/ a subset (e.g. the young generation or old generation) of the total heap.\n@@ -340,1 +374,1 @@\n-  size_t _used, _committed, _garbage;\n+  size_t _used, _committed, _garbage, _regions, _humongous_waste, _trashed_regions;\n@@ -342,1 +376,2 @@\n-  ShenandoahCalculateRegionStatsClosure() : _used(0), _committed(0), _garbage(0) {};\n+  ShenandoahCalculateRegionStatsClosure() :\n+      _used(0), _committed(0), _garbage(0), _regions(0), _humongous_waste(0), _trashed_regions(0) {};\n@@ -344,1 +379,1 @@\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n@@ -348,0 +383,43 @@\n+    if (r->is_humongous()) {\n+      _humongous_waste += r->free();\n+    }\n+    if (r->is_trash()) {\n+      _trashed_regions++;\n+    }\n+    _regions++;\n+    log_debug(gc)(\"ShenandoahCalculateRegionStatsClosure: adding \" SIZE_FORMAT \" for %s Region \" SIZE_FORMAT \", yielding: \" SIZE_FORMAT,\n+            r->used(), (r->is_humongous() ? \"humongous\" : \"regular\"), r->index(), _used);\n+  }\n+\n+  size_t used() const { return _used; }\n+  size_t committed() const { return _committed; }\n+  size_t garbage() const { return _garbage; }\n+  size_t regions() const { return _regions; }\n+  size_t waste() const { return _humongous_waste; }\n+\n+  \/\/ span is the total memory affiliated with these stats (some of which is in use and other is available)\n+  size_t span() const { return _regions * ShenandoahHeapRegion::region_size_bytes(); }\n+  size_t non_trashed_span() const { return (_regions - _trashed_regions) * ShenandoahHeapRegion::region_size_bytes(); }\n+};\n+\n+class ShenandoahGenerationStatsClosure : public ShenandoahHeapRegionClosure {\n+ public:\n+  ShenandoahCalculateRegionStatsClosure old;\n+  ShenandoahCalculateRegionStatsClosure young;\n+  ShenandoahCalculateRegionStatsClosure global;\n+\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n+    switch (r->affiliation()) {\n+      case FREE:\n+        return;\n+      case YOUNG_GENERATION:\n+        young.heap_region_do(r);\n+        global.heap_region_do(r);\n+        break;\n+      case OLD_GENERATION:\n+        old.heap_region_do(r);\n+        global.heap_region_do(r);\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n@@ -350,3 +428,37 @@\n-  size_t used() { return _used; }\n-  size_t committed() { return _committed; }\n-  size_t garbage() { return _garbage; }\n+  static void log_usage(ShenandoahGeneration* generation, ShenandoahCalculateRegionStatsClosure& stats) {\n+    log_debug(gc)(\"Safepoint verification: %s verified usage: \" SIZE_FORMAT \"%s, recorded usage: \" SIZE_FORMAT \"%s\",\n+                  generation->name(),\n+                  byte_size_in_proper_unit(generation->used()), proper_unit_for_byte_size(generation->used()),\n+                  byte_size_in_proper_unit(stats.used()),       proper_unit_for_byte_size(stats.used()));\n+  }\n+\n+  static void validate_usage(const bool adjust_for_padding,\n+                             const char* label, ShenandoahGeneration* generation, ShenandoahCalculateRegionStatsClosure& stats) {\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    size_t generation_used = generation->used();\n+    size_t generation_used_regions = generation->used_regions();\n+    if (adjust_for_padding && (generation->is_young() || generation->is_global())) {\n+      size_t pad = heap->old_generation()->get_pad_for_promote_in_place();\n+      generation_used += pad;\n+    }\n+\n+    guarantee(stats.used() == generation_used,\n+              \"%s: generation (%s) used size must be consistent: generation-used: \" PROPERFMT \", regions-used: \" PROPERFMT,\n+              label, generation->name(), PROPERFMTARGS(generation_used), PROPERFMTARGS(stats.used()));\n+\n+    guarantee(stats.regions() == generation_used_regions,\n+              \"%s: generation (%s) used regions (\" SIZE_FORMAT \") must equal regions that are in use (\" SIZE_FORMAT \")\",\n+              label, generation->name(), generation->used_regions(), stats.regions());\n+\n+    size_t generation_capacity = generation->max_capacity();\n+    guarantee(stats.non_trashed_span() <= generation_capacity,\n+              \"%s: generation (%s) size spanned by regions (\" SIZE_FORMAT \") * region size (\" PROPERFMT\n+              \") must not exceed current capacity (\" PROPERFMT \")\",\n+              label, generation->name(), stats.regions(), PROPERFMTARGS(ShenandoahHeapRegion::region_size_bytes()),\n+              PROPERFMTARGS(generation_capacity));\n+\n+    size_t humongous_waste = generation->get_humongous_waste();\n+    guarantee(stats.waste() == humongous_waste,\n+              \"%s: generation (%s) humongous waste must be consistent: generation: \" PROPERFMT \", regions: \" PROPERFMT,\n+              label, generation->name(), PROPERFMTARGS(humongous_waste), PROPERFMTARGS(stats.waste()));\n+  }\n@@ -384,1 +496,1 @@\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n@@ -436,2 +548,5 @@\n-    verify(r, r->get_shared_allocs() + r->get_tlab_allocs() + r->get_gclab_allocs() == r->used(),\n-           \"Accurate accounting: shared + TLAB + GCLAB = used\");\n+    verify(r, r->get_plab_allocs() <= r->capacity(),\n+           \"PLAB alloc count should not be larger than capacity\");\n+\n+    verify(r, r->get_shared_allocs() + r->get_tlab_allocs() + r->get_gclab_allocs() + r->get_plab_allocs() == r->used(),\n+           \"Accurate accounting: shared + TLAB + GCLAB + PLAB = used\");\n@@ -469,1 +584,1 @@\n-  size_t processed() {\n+  size_t processed() const {\n@@ -473,1 +588,1 @@\n-  virtual void work(uint worker_id) {\n+  void work(uint worker_id) override {\n@@ -510,0 +625,10 @@\n+class ShenandoahVerifyNoIncompleteSatbBuffers : public ThreadClosure {\n+public:\n+  void do_thread(Thread* thread) override {\n+    SATBMarkQueue& queue = ShenandoahThreadLocalData::satb_mark_queue(thread);\n+    if (!queue.is_empty()) {\n+      fatal(\"All SATB buffers should have been flushed during mark\");\n+    }\n+  }\n+};\n+\n@@ -519,0 +644,1 @@\n+  ShenandoahGeneration* _generation;\n@@ -532,1 +658,12 @@\n-          _processed(0) {};\n+          _processed(0),\n+          _generation(nullptr) {\n+    if (_options._verify_marked == ShenandoahVerifier::_verify_marked_complete_satb_empty) {\n+      Threads::change_thread_claim_token();\n+    }\n+\n+    if (_heap->mode()->is_generational()) {\n+      _generation = _heap->gc_generation();\n+      assert(_generation != nullptr, \"Expected active generation in this mode.\");\n+      shenandoah_assert_generations_reconciled();\n+    }\n+  };\n@@ -538,1 +675,6 @@\n-  virtual void work(uint worker_id) {\n+  void work(uint worker_id) override {\n+    if (_options._verify_marked == ShenandoahVerifier::_verify_marked_complete_satb_empty) {\n+      ShenandoahVerifyNoIncompleteSatbBuffers verify_satb;\n+      Threads::possibly_parallel_threads_do(true, &verify_satb);\n+    }\n+\n@@ -548,0 +690,4 @@\n+        if (!in_generation(r)) {\n+          continue;\n+        }\n+\n@@ -559,0 +705,4 @@\n+  bool in_generation(ShenandoahHeapRegion* r) {\n+    return _generation == nullptr || _generation->contains(r);\n+  }\n+\n@@ -629,1 +779,1 @@\n-  void do_thread(Thread* t) {\n+  void do_thread(Thread* t) override {\n@@ -631,1 +781,1 @@\n-    if (actual != _expected) {\n+    if (!verify_gc_state(actual, _expected)) {\n@@ -635,0 +785,10 @@\n+\n+  static bool verify_gc_state(char actual, char expected) {\n+    \/\/ Old generation marking is allowed in all states.\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      return ((actual & ~(ShenandoahHeap::OLD_MARKING | ShenandoahHeap::MARKING)) == expected);\n+    } else {\n+      assert((actual & ShenandoahHeap::OLD_MARKING) == 0, \"Should not mark old in non-generational mode\");\n+      return (actual == expected);\n+    }\n+  }\n@@ -637,1 +797,2 @@\n-void ShenandoahVerifier::verify_at_safepoint(const char *label,\n+void ShenandoahVerifier::verify_at_safepoint(const char* label,\n+                                             VerifyRememberedSet remembered,\n@@ -641,0 +802,1 @@\n+                                             VerifySize sizeness,\n@@ -664,0 +826,15 @@\n+<<<<<<< HEAD\n+=======\n+      case _verify_gcstate_evacuation:\n+        enabled = true;\n+        expected = ShenandoahHeap::EVACUATION;\n+        if (!_heap->is_stw_gc_in_progress()) {\n+          \/\/ Only concurrent GC sets this.\n+          expected |= ShenandoahHeap::WEAK_ROOTS;\n+        }\n+        break;\n+      case _verify_gcstate_updating:\n+        enabled = true;\n+        expected = ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::UPDATEREFS;\n+        break;\n+>>>>>>> b7405b19d531269ccb611cb5dc6a07f491a7b9de\n@@ -683,1 +860,7 @@\n-      if (actual != expected) {\n+\n+      bool is_marking = (actual & ShenandoahHeap::MARKING);\n+      bool is_marking_young_or_old = (actual & (ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING));\n+      assert(is_marking == is_marking_young_or_old, \"MARKING iff (YOUNG_MARKING or OLD_MARKING), gc_state is: %x\", actual);\n+\n+      \/\/ Old generation marking is allowed in all states.\n+      if (!VerifyThreadGCState::verify_gc_state(actual, expected)) {\n@@ -701,7 +884,14 @@\n-    size_t heap_used = _heap->used();\n-    guarantee(cl.used() == heap_used,\n-              \"%s: heap used size must be consistent: heap-used = \" SIZE_FORMAT \"%s, regions-used = \" SIZE_FORMAT \"%s\",\n-              label,\n-              byte_size_in_proper_unit(heap_used), proper_unit_for_byte_size(heap_used),\n-              byte_size_in_proper_unit(cl.used()), proper_unit_for_byte_size(cl.used()));\n-\n+    size_t heap_used;\n+    if (_heap->mode()->is_generational() && (sizeness == _verify_size_adjusted_for_padding)) {\n+      \/\/ Prior to evacuation, regular regions that are to be evacuated in place are padded to prevent further allocations\n+      heap_used = _heap->used() + _heap->old_generation()->get_pad_for_promote_in_place();\n+    } else if (sizeness != _verify_size_disable) {\n+      heap_used = _heap->used();\n+    }\n+    if (sizeness != _verify_size_disable) {\n+      guarantee(cl.used() == heap_used,\n+                \"%s: heap used size must be consistent: heap-used = \" SIZE_FORMAT \"%s, regions-used = \" SIZE_FORMAT \"%s\",\n+                label,\n+                byte_size_in_proper_unit(heap_used), proper_unit_for_byte_size(heap_used),\n+                byte_size_in_proper_unit(cl.used()), proper_unit_for_byte_size(cl.used()));\n+    }\n@@ -716,0 +906,55 @@\n+  log_debug(gc)(\"Safepoint verification finished heap usage verification\");\n+\n+  ShenandoahGeneration* generation;\n+  if (_heap->mode()->is_generational()) {\n+    generation = _heap->gc_generation();\n+    guarantee(generation != nullptr, \"Need to know which generation to verify.\");\n+    shenandoah_assert_generations_reconciled();\n+  } else {\n+    generation = nullptr;\n+  }\n+\n+  if (generation != nullptr) {\n+    ShenandoahHeapLocker lock(_heap->lock());\n+\n+    switch (remembered) {\n+      case _verify_remembered_disable:\n+        break;\n+      case _verify_remembered_before_marking:\n+        log_debug(gc)(\"Safepoint verification of remembered set at mark\");\n+        verify_rem_set_before_mark();\n+        break;\n+      case _verify_remembered_before_updating_references:\n+        log_debug(gc)(\"Safepoint verification of remembered set at update ref\");\n+        verify_rem_set_before_update_ref();\n+        break;\n+      case _verify_remembered_after_full_gc:\n+        log_debug(gc)(\"Safepoint verification of remembered set after full gc\");\n+        verify_rem_set_after_full_gc();\n+        break;\n+      default:\n+        fatal(\"Unhandled remembered set verification mode\");\n+    }\n+\n+    ShenandoahGenerationStatsClosure cl;\n+    _heap->heap_region_iterate(&cl);\n+\n+    if (LogTarget(Debug, gc)::is_enabled()) {\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->old_generation(),    cl.old);\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->young_generation(),  cl.young);\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->global_generation(), cl.global);\n+    }\n+    if (sizeness == _verify_size_adjusted_for_padding) {\n+      ShenandoahGenerationStatsClosure::validate_usage(false, label, _heap->old_generation(), cl.old);\n+      ShenandoahGenerationStatsClosure::validate_usage(true, label, _heap->young_generation(), cl.young);\n+      ShenandoahGenerationStatsClosure::validate_usage(true, label, _heap->global_generation(), cl.global);\n+    } else if (sizeness == _verify_size_exact) {\n+      ShenandoahGenerationStatsClosure::validate_usage(false, label, _heap->old_generation(), cl.old);\n+      ShenandoahGenerationStatsClosure::validate_usage(false, label, _heap->young_generation(), cl.young);\n+      ShenandoahGenerationStatsClosure::validate_usage(false, label, _heap->global_generation(), cl.global);\n+    }\n+    \/\/ else: sizeness must equal _verify_size_disable\n+  }\n+\n+  log_debug(gc)(\"Safepoint verification finished remembered set verification\");\n+\n@@ -719,1 +964,5 @@\n-    _heap->heap_region_iterate(&cl);\n+    if (generation != nullptr) {\n+      generation->heap_region_iterate(&cl);\n+    } else {\n+      _heap->heap_region_iterate(&cl);\n+    }\n@@ -722,0 +971,2 @@\n+  log_debug(gc)(\"Safepoint verification finished heap region closure verification\");\n+\n@@ -746,0 +997,2 @@\n+  log_debug(gc)(\"Safepoint verification finished getting initial reachable set\");\n+\n@@ -754,1 +1007,4 @@\n-  if (ShenandoahVerifyLevel >= 4 && (marked == _verify_marked_complete || marked == _verify_marked_complete_except_references)) {\n+  if (ShenandoahVerifyLevel >= 4 &&\n+        (marked == _verify_marked_complete ||\n+         marked == _verify_marked_complete_except_references ||\n+         marked == _verify_marked_complete_satb_empty)) {\n@@ -763,0 +1019,2 @@\n+  log_debug(gc)(\"Safepoint verification finished walking marked objects\");\n+\n@@ -769,0 +1027,3 @@\n+      if (generation != nullptr && !generation->contains(r)) {\n+        continue;\n+      }\n@@ -792,0 +1053,3 @@\n+  log_debug(gc)(\"Safepoint verification finished accumulation of liveness data\");\n+\n+\n@@ -801,0 +1065,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -806,0 +1071,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -813,0 +1079,2 @@\n+          _verify_remembered_before_marking,\n+                                       \/\/ verify read-only remembered set from bottom() to top()\n@@ -818,0 +1086,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -825,0 +1094,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -826,1 +1096,2 @@\n-          _verify_marked_complete_except_references, \/\/ bitmaps as precise as we can get, except dangling j.l.r.Refs\n+          _verify_marked_complete_satb_empty,\n+                                       \/\/ bitmaps as precise as we can get, except dangling j.l.r.Refs\n@@ -830,0 +1101,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -837,0 +1109,1 @@\n+          _verify_remembered_disable,                \/\/ do not verify remembered set\n@@ -842,0 +1115,2 @@\n+          _verify_size_adjusted_for_padding,         \/\/ expect generation and heap sizes to match after adjustments\n+                                                     \/\/  for promote in place padding\n@@ -846,0 +1121,31 @@\n+<<<<<<< HEAD\n+=======\n+void ShenandoahVerifier::verify_during_evacuation() {\n+  verify_at_safepoint(\n+          \"During Evacuation\",\n+          _verify_remembered_disable, \/\/ do not verify remembered set\n+          _verify_forwarded_allow,    \/\/ some forwarded references are allowed\n+          _verify_marked_disable,     \/\/ walk only roots\n+          _verify_cset_disable,       \/\/ some cset references are not forwarded yet\n+          _verify_liveness_disable,   \/\/ liveness data might be already stale after pre-evacs\n+          _verify_regions_disable,    \/\/ trash regions not yet recycled\n+          _verify_size_disable,       \/\/ we don't know how much of promote-in-place work has been completed\n+          _verify_gcstate_evacuation  \/\/ evacuation is in progress\n+  );\n+}\n+\n+void ShenandoahVerifier::verify_after_evacuation() {\n+  verify_at_safepoint(\n+          \"After Evacuation\",\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n+          _verify_forwarded_allow,     \/\/ objects are still forwarded\n+          _verify_marked_complete,     \/\/ bitmaps might be stale, but alloc-after-mark should be well\n+          _verify_cset_forwarded,      \/\/ all cset refs are fully forwarded\n+          _verify_liveness_disable,    \/\/ no reliable liveness data anymore\n+          _verify_regions_notrash,     \/\/ trash regions have been recycled already\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n+          _verify_gcstate_forwarded    \/\/ evacuation produced some forwarded objects\n+  );\n+}\n+\n+>>>>>>> b7405b19d531269ccb611cb5dc6a07f491a7b9de\n@@ -849,0 +1155,1 @@\n+          _verify_remembered_before_updating_references,  \/\/ verify read-write remembered set\n@@ -854,1 +1161,2 @@\n-          _verify_gcstate_forwarded    \/\/ evacuation should have produced some forwarded objects\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n+          _verify_gcstate_updating     \/\/ evacuation should have produced some forwarded objects\n@@ -858,0 +1166,1 @@\n+\/\/ We have not yet cleanup (reclaimed) the collection set\n@@ -861,0 +1170,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -866,0 +1176,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -873,0 +1184,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -878,0 +1190,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -885,0 +1198,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -890,0 +1204,1 @@\n+          _verify_size_disable,        \/\/ if we degenerate during evacuation, usage not valid: padding and deferred accounting\n@@ -897,0 +1212,1 @@\n+          _verify_remembered_after_full_gc,  \/\/ verify read-write remembered set\n@@ -902,0 +1218,1 @@\n+          _verify_size_exact,           \/\/ expect generation and heap sizes to match exactly\n@@ -906,1 +1223,1 @@\n-class ShenandoahVerifyNoForwared : public OopClosure {\n+class ShenandoahVerifyNoForwarded : public BasicOopIterateClosure {\n@@ -926,1 +1243,1 @@\n-class ShenandoahVerifyInToSpaceClosure : public OopClosure {\n+class ShenandoahVerifyInToSpaceClosure : public BasicOopIterateClosure {\n@@ -935,1 +1252,1 @@\n-      if (!heap->marking_context()->is_marked(obj)) {\n+      if (!heap->marking_context()->is_marked_or_old(obj)) {\n@@ -954,2 +1271,2 @@\n-  void do_oop(narrowOop* p) { do_oop_work(p); }\n-  void do_oop(oop* p)       { do_oop_work(p); }\n+  void do_oop(narrowOop* p) override { do_oop_work(p); }\n+  void do_oop(oop* p)       override { do_oop_work(p); }\n@@ -964,1 +1281,1 @@\n-  ShenandoahVerifyNoForwared cl;\n+  ShenandoahVerifyNoForwarded cl;\n@@ -967,0 +1284,165 @@\n+\n+template<typename Scanner>\n+class ShenandoahVerifyRemSetClosure : public BasicOopIterateClosure {\n+protected:\n+  ShenandoahGenerationalHeap* const _heap;\n+  Scanner*   const _scanner;\n+  const char* _message;\n+\n+public:\n+  \/\/ Argument distinguishes between initial mark or start of update refs verification.\n+  explicit ShenandoahVerifyRemSetClosure(Scanner* scanner, const char* message) :\n+            _heap(ShenandoahGenerationalHeap::heap()),\n+            _scanner(scanner),\n+            _message(message) {}\n+\n+  template<class T>\n+  inline void work(T* p) {\n+    T o = RawAccess<>::oop_load(p);\n+    if (!CompressedOops::is_null(o)) {\n+      oop obj = CompressedOops::decode_not_null(o);\n+      if (_heap->is_in_young(obj) && !_scanner->is_card_dirty((HeapWord*) p)) {\n+        ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, p, nullptr,\n+                                         _message, \"clean card should be dirty\", __FILE__, __LINE__);\n+      }\n+    }\n+  }\n+\n+  void do_oop(narrowOop* p) override { work(p); }\n+  void do_oop(oop* p)       override { work(p); }\n+};\n+\n+ShenandoahMarkingContext* ShenandoahVerifier::get_marking_context_for_old() {\n+  shenandoah_assert_generations_reconciled();\n+  if (_heap->old_generation()->is_mark_complete() || _heap->gc_generation()->is_global()) {\n+    return _heap->complete_marking_context();\n+  }\n+  return nullptr;\n+}\n+\n+template<typename Scanner>\n+void ShenandoahVerifier::help_verify_region_rem_set(Scanner* scanner, ShenandoahHeapRegion* r, ShenandoahMarkingContext* ctx,\n+                                                    HeapWord* registration_watermark, const char* message) {\n+  ShenandoahVerifyRemSetClosure<Scanner> check_interesting_pointers(scanner, message);\n+  HeapWord* from = r->bottom();\n+  HeapWord* obj_addr = from;\n+  if (r->is_humongous_start()) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if ((ctx == nullptr) || ctx->is_marked(obj)) {\n+      \/\/ For humongous objects, the typical object is an array, so the following checks may be overkill\n+      \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+      \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+      if (!scanner->is_card_dirty(obj_addr) || obj->is_objArray()) {\n+        obj->oop_iterate(&check_interesting_pointers);\n+      }\n+      \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+    }\n+    \/\/ else, this humongous object is not live so no need to verify its internal pointers\n+\n+    if ((obj_addr < registration_watermark) && !scanner->verify_registration(obj_addr, ctx)) {\n+      ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, nullptr, message,\n+                                       \"object not properly registered\", __FILE__, __LINE__);\n+    }\n+  } else if (!r->is_humongous()) {\n+    HeapWord* top = r->top();\n+    while (obj_addr < top) {\n+      oop obj = cast_to_oop(obj_addr);\n+      \/\/ ctx->is_marked() returns true if mark bit set or if obj above TAMS.\n+      if ((ctx == nullptr) || ctx->is_marked(obj)) {\n+        \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+        \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+        if (!scanner->is_card_dirty(obj_addr) || obj->is_objArray()) {\n+          obj->oop_iterate(&check_interesting_pointers);\n+        }\n+        \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+\n+        if ((obj_addr < registration_watermark) && !scanner->verify_registration(obj_addr, ctx)) {\n+          ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, nullptr, message,\n+                                           \"object not properly registered\", __FILE__, __LINE__);\n+        }\n+        obj_addr += obj->size();\n+      } else {\n+        \/\/ This object is not live so we don't verify dirty cards contained therein\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        obj_addr = ctx->get_next_marked_addr(obj_addr, tams);\n+      }\n+    }\n+  }\n+}\n+\n+class ShenandoahWriteTableScanner {\n+private:\n+  ShenandoahScanRemembered* _scanner;\n+public:\n+  explicit ShenandoahWriteTableScanner(ShenandoahScanRemembered* scanner) : _scanner(scanner) {}\n+\n+  bool is_card_dirty(HeapWord* obj_addr) {\n+    return _scanner->is_write_card_dirty(obj_addr);\n+  }\n+\n+  bool verify_registration(HeapWord* obj_addr, ShenandoahMarkingContext* ctx) {\n+    return _scanner->verify_registration(obj_addr, ctx);\n+  }\n+};\n+\n+\/\/ Assure that the remember set has a dirty card everywhere there is an interesting pointer.\n+\/\/ This examines the read_card_table between bottom() and top() since all PLABS are retired\n+\/\/ before the safepoint for init_mark.  Actually, we retire them before update-references and don't\n+\/\/ restore them until the start of evacuation.\n+void ShenandoahVerifier::verify_rem_set_before_mark() {\n+  shenandoah_assert_safepoint();\n+  shenandoah_assert_generational();\n+\n+  ShenandoahMarkingContext* ctx = get_marking_context_for_old();\n+  ShenandoahOldGeneration* old_generation = _heap->old_generation();\n+\n+  log_debug(gc)(\"Verifying remembered set at %s mark\", old_generation->is_doing_mixed_evacuations() ? \"mixed\" : \"young\");\n+\n+  ShenandoahScanRemembered* scanner = old_generation->card_scan();\n+  for (size_t i = 0, n = _heap->num_regions(); i < n; ++i) {\n+    ShenandoahHeapRegion* r = _heap->get_region(i);\n+    if (r->is_old() && r->is_active()) {\n+      help_verify_region_rem_set(scanner, r, ctx, r->end(), \"Verify init-mark remembered set violation\");\n+    }\n+  }\n+}\n+\n+void ShenandoahVerifier::verify_rem_set_after_full_gc() {\n+  shenandoah_assert_safepoint();\n+  shenandoah_assert_generational();\n+\n+  ShenandoahWriteTableScanner scanner(ShenandoahGenerationalHeap::heap()->old_generation()->card_scan());\n+  for (size_t i = 0, n = _heap->num_regions(); i < n; ++i) {\n+    ShenandoahHeapRegion* r = _heap->get_region(i);\n+    if (r->is_old() && !r->is_cset()) {\n+      help_verify_region_rem_set(&scanner, r, nullptr, r->top(), \"Remembered set violation at end of Full GC\");\n+    }\n+  }\n+}\n+\n+\/\/ Assure that the remember set has a dirty card everywhere there is an interesting pointer.  Even though\n+\/\/ the update-references scan of remembered set only examines cards up to update_watermark, the remembered\n+\/\/ set should be valid through top.  This examines the write_card_table between bottom() and top() because\n+\/\/ all PLABS are retired immediately before the start of update refs.\n+void ShenandoahVerifier::verify_rem_set_before_update_ref() {\n+  shenandoah_assert_safepoint();\n+  shenandoah_assert_generational();\n+\n+  ShenandoahMarkingContext* ctx = get_marking_context_for_old();\n+  ShenandoahWriteTableScanner scanner(_heap->old_generation()->card_scan());\n+  for (size_t i = 0, n = _heap->num_regions(); i < n; ++i) {\n+    ShenandoahHeapRegion* r = _heap->get_region(i);\n+    if (r->is_old() && !r->is_cset()) {\n+      help_verify_region_rem_set(&scanner, r, ctx, r->get_update_watermark(), \"Remembered set violation at init-update-references\");\n+    }\n+  }\n+}\n+\n+void ShenandoahVerifier::verify_before_rebuilding_free_set() {\n+  ShenandoahGenerationStatsClosure cl;\n+  _heap->heap_region_iterate(&cl);\n+\n+  ShenandoahGenerationStatsClosure::validate_usage(false, \"Before free set rebuild\", _heap->old_generation(), cl.old);\n+  ShenandoahGenerationStatsClosure::validate_usage(false, \"Before free set rebuild\", _heap->young_generation(), cl.young);\n+  ShenandoahGenerationStatsClosure::validate_usage(false, \"Before free set rebuild\", _heap->global_generation(), cl.global);\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":521,"deletions":39,"binary":false,"changes":560,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -35,0 +36,1 @@\n+class ShenandoahMarkingContext;\n@@ -60,0 +62,18 @@\n+  typedef enum {\n+    \/\/ Disable remembered set verification.\n+    _verify_remembered_disable,\n+\n+    \/\/ Old objects should be registered and RS cards within *read-only* RS are dirty for all\n+    \/\/ inter-generational pointers.\n+    _verify_remembered_before_marking,\n+\n+    \/\/ Old objects should be registered and RS cards within *read-write* RS are dirty for all\n+    \/\/ inter-generational pointers.\n+    _verify_remembered_before_updating_references,\n+\n+    \/\/ Old objects should be registered and RS cards within *read-write* RS are dirty for all\n+    \/\/ inter-generational pointers. Differs from previous verification modes by using top instead\n+    \/\/ of update watermark and not using the marking context.\n+    _verify_remembered_after_full_gc\n+  } VerifyRememberedSet;\n+\n@@ -72,1 +92,6 @@\n-    _verify_marked_complete_except_references\n+    _verify_marked_complete_except_references,\n+\n+    \/\/ Objects should be marked in \"complete\" bitmap, except j.l.r.Reference referents, which\n+    \/\/ may be dangling after marking but before conc-weakrefs-processing. All SATB buffers must\n+    \/\/ be empty.\n+    _verify_marked_complete_satb_empty,\n@@ -125,0 +150,11 @@\n+  typedef enum {\n+    \/\/ Disable size verification\n+    _verify_size_disable,\n+\n+    \/\/ Enforce exact consistency\n+    _verify_size_exact,\n+\n+    \/\/ Expect promote-in-place adjustments: padding inserted to temporarily prevent further allocation in regular regions\n+    _verify_size_adjusted_for_padding\n+  } VerifySize;\n+\n@@ -136,0 +172,1 @@\n+<<<<<<< HEAD\n@@ -137,0 +174,9 @@\n+=======\n+    _verify_gcstate_forwarded,\n+\n+    \/\/ Evacuation is in progress, some objects are forwarded\n+    _verify_gcstate_evacuation,\n+\n+    \/\/ Evacuation is done, some objects are forwarded, updating is in progress\n+    _verify_gcstate_updating\n+>>>>>>> b7405b19d531269ccb611cb5dc6a07f491a7b9de\n@@ -160,1 +206,2 @@\n-  void verify_at_safepoint(const char *label,\n+  void verify_at_safepoint(const char* label,\n+                           VerifyRememberedSet remembered,\n@@ -166,0 +213,1 @@\n+                           VerifySize sizeness,\n@@ -184,1 +232,0 @@\n-\n@@ -186,0 +233,13 @@\n+\n+  \/\/ Check that generation usages are accurate before rebuilding free set\n+  void verify_before_rebuilding_free_set();\n+private:\n+  template<typename Scanner>\n+  void help_verify_region_rem_set(Scanner* scanner, ShenandoahHeapRegion* r, ShenandoahMarkingContext* ctx,\n+                                  HeapWord* update_watermark, const char* message);\n+\n+  void verify_rem_set_before_mark();\n+  void verify_rem_set_before_update_ref();\n+  void verify_rem_set_after_full_gc();\n+\n+  ShenandoahMarkingContext* get_marking_context_for_old();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.hpp","additions":63,"deletions":3,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -0,0 +1,211 @@\n+\/*\n+ * Copyright (c) 2017, 2018, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/*\n+ * @test id=default\n+ * @key randomness\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:+ShenandoahVerify\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=50\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=90\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=99\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=100\n+ *                   TestHumongousThreshold\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:-UseTLAB -XX:+ShenandoahVerify\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:-UseTLAB -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=50\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:-UseTLAB -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=90\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:-UseTLAB -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=99\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:-UseTLAB -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=100\n+ *                   TestHumongousThreshold\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:ShenandoahHumongousThreshold=90 -XX:ShenandoahGCHeuristics=aggressive\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:-UseTLAB -XX:ShenandoahHumongousThreshold=90 -XX:ShenandoahGCHeuristics=aggressive\n+ *                   TestHumongousThreshold\n+ *\/\n+\n+\/*\n+ * @test id=16b\n+ * @key randomness\n+ * @requires vm.gc.Shenandoah\n+ * @requires vm.bits == \"64\"\n+ * @library \/test\/lib\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=50\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=90\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=99\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=100\n+ *                   TestHumongousThreshold\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:-UseTLAB -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:-UseTLAB -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=50\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:-UseTLAB -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=90\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:-UseTLAB -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=99\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n+ *                   -XX:-UseTLAB -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=100\n+ *                   TestHumongousThreshold\n+ *\/\n+\n+\/*\n+ * @test id=generational\n+ * @key randomness\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g\n+ *                   -XX:+ShenandoahVerify\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g\n+ *                   -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=50\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g\n+ *                   -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=90\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g\n+ *                   -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=99\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g\n+ *                   -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=100\n+ *                   TestHumongousThreshold\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g\n+ *                   -XX:-UseTLAB -XX:+ShenandoahVerify\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g\n+ *                   -XX:-UseTLAB -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=50\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g\n+ *                   -XX:-UseTLAB -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=90\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g\n+ *                   -XX:-UseTLAB -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=99\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g\n+ *                   -XX:-UseTLAB -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=100\n+ *                   TestHumongousThreshold\n+ *\/\n+\n+\/*\n+ * @test id=generational-16b\n+ * @key randomness\n+ * @requires vm.gc.Shenandoah\n+ * @requires vm.bits == \"64\"\n+ * @library \/test\/lib\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g\n+ *                   -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g\n+ *                   -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=50\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g\n+ *                   -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=90\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g\n+ *                   -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=99\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g\n+ *                   -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=100\n+ *                   TestHumongousThreshold\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g\n+ *                   -XX:-UseTLAB -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g\n+ *                   -XX:-UseTLAB -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=50\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g\n+ *                   -XX:-UseTLAB -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=90\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g\n+ *                   -XX:-UseTLAB -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=99\n+ *                   TestHumongousThreshold\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g\n+ *                   -XX:-UseTLAB -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=100\n+ *                   TestHumongousThreshold\n+ *\/\n+\n+import java.util.Random;\n+import jdk.test.lib.Utils;\n+\n+public class TestHumongousThreshold {\n+\n+    static final long TARGET_MB = Long.getLong(\"target\", 20_000); \/\/ 20 Gb allocation\n+\n+    static volatile Object sink;\n+\n+    public static void main(String[] args) throws Exception {\n+        final int min = 0;\n+        final int max = 384 * 1024;\n+        long count = TARGET_MB * 1024 * 1024 \/ (16 + 4 * (min + (max - min) \/ 2));\n+\n+        Random r = Utils.getRandomInstance();\n+        for (long c = 0; c < count; c++) {\n+            sink = new int[min + r.nextInt(max - min)];\n+        }\n+    }\n+\n+}\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestHumongousThreshold.java","additions":211,"deletions":0,"binary":false,"changes":211,"status":"added"}]}