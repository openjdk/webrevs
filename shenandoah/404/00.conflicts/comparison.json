{"files":[{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -33,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -42,0 +44,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -60,1 +63,2 @@\n-    ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+    ShenandoahParallelWorkerSession worker_session(worker_id);\n+    ShenandoahWorkerTimingsTracker timer(ShenandoahPhaseTimings::conc_mark, ShenandoahPhaseTimings::ParallelMark, worker_id, true);\n@@ -62,1 +66,1 @@\n-    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+    ShenandoahReferenceProcessor* rp = heap->active_generation()->ref_processor();\n@@ -93,0 +97,1 @@\n+<<<<<<< HEAD\n@@ -94,0 +99,3 @@\n+=======\n+template<ShenandoahGenerationType GENERATION>\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -109,1 +117,0 @@\n-    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n@@ -111,0 +118,1 @@\n+    ShenandoahReferenceProcessor* rp = heap->active_generation()->ref_processor();\n@@ -115,0 +123,1 @@\n+      ShenandoahObjToScanQueue* old_q = _cm->get_old_queue(worker_id);\n@@ -116,0 +125,1 @@\n+<<<<<<< HEAD\n@@ -117,0 +127,3 @@\n+=======\n+      ShenandoahSATBBufferClosure<GENERATION> cl(q, old_q);\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -121,0 +134,1 @@\n+<<<<<<< HEAD\n@@ -122,0 +136,3 @@\n+=======\n+      ShenandoahMarkRefsClosure<GENERATION> mark_cl(q, rp, old_q);\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -133,2 +150,2 @@\n-ShenandoahConcurrentMark::ShenandoahConcurrentMark() :\n-  ShenandoahMark() {}\n+ShenandoahConcurrentMark::ShenandoahConcurrentMark(ShenandoahGeneration* generation) :\n+  ShenandoahMark(generation) {}\n@@ -137,0 +154,1 @@\n+<<<<<<< HEAD\n@@ -138,0 +156,3 @@\n+=======\n+template<ShenandoahGenerationType GENERATION>\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -143,0 +164,1 @@\n+  ShenandoahObjToScanQueueSet* const  _old_queue_set;\n@@ -147,0 +169,1 @@\n+                                    ShenandoahObjToScanQueueSet* old,\n@@ -153,0 +176,1 @@\n+<<<<<<< HEAD\n@@ -158,0 +182,8 @@\n+=======\n+template<ShenandoahGenerationType GENERATION>\n+ShenandoahMarkConcurrentRootsTask<GENERATION>::ShenandoahMarkConcurrentRootsTask(ShenandoahObjToScanQueueSet* qs,\n+                                                                                 ShenandoahObjToScanQueueSet* old,\n+                                                                                 ShenandoahReferenceProcessor* rp,\n+                                                                                 ShenandoahPhaseTimings::Phase phase,\n+                                                                                 uint nworkers) :\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -161,0 +193,1 @@\n+  _old_queue_set(old),\n@@ -165,0 +198,1 @@\n+<<<<<<< HEAD\n@@ -170,0 +204,9 @@\n+=======\n+template<ShenandoahGenerationType GENERATION>\n+void ShenandoahMarkConcurrentRootsTask<GENERATION>::work(uint worker_id) {\n+  ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+  ShenandoahObjToScanQueue* q = _queue_set->queue(worker_id);\n+  ShenandoahObjToScanQueue* old_q = (_old_queue_set == nullptr) ?\n+          nullptr : _old_queue_set->queue(worker_id);\n+  ShenandoahMarkRefsClosure<GENERATION> cl(q, _rp, old_q);\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -177,2 +220,0 @@\n-  TASKQUEUE_STATS_ONLY(task_queues()->reset_taskqueue_stats());\n-\n@@ -180,0 +221,1 @@\n+<<<<<<< HEAD\n@@ -185,0 +227,33 @@\n+=======\n+  ShenandoahReferenceProcessor* rp = _generation->ref_processor();\n+  _generation->reserve_task_queues(workers->active_workers());\n+  switch (_generation->type()) {\n+    case YOUNG: {\n+      ShenandoahMarkConcurrentRootsTask<YOUNG> task(task_queues(), old_task_queues(), rp,\n+                                                    ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      workers->run_task(&task);\n+      break;\n+    }\n+    case GLOBAL: {\n+      assert(old_task_queues() == nullptr, \"Global mark should not have old gen mark queues\");\n+      ShenandoahMarkConcurrentRootsTask<GLOBAL> task(task_queues(), nullptr, rp,\n+                                                     ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      workers->run_task(&task);\n+      break;\n+    }\n+    case NON_GEN: {\n+      assert(old_task_queues() == nullptr, \"Non-generational mark should not have old gen mark queues\");\n+      ShenandoahMarkConcurrentRootsTask<NON_GEN> task(task_queues(), nullptr, rp,\n+                                                      ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      workers->run_task(&task);\n+      break;\n+    }\n+    case OLD: {\n+      \/\/ We use a YOUNG generation cycle to bootstrap concurrent old marking.\n+      ShouldNotReachHere();\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -209,0 +284,1 @@\n+<<<<<<< HEAD\n@@ -212,0 +288,36 @@\n+=======\n+    switch (_generation->type()) {\n+      case YOUNG: {\n+        \/\/ Clear any old\/partial local census data before the start of marking.\n+        heap->age_census()->reset_local();\n+        assert(heap->age_census()->is_clear_local(), \"Error\");\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<YOUNG> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      case OLD: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<OLD> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      case GLOBAL: {\n+        \/\/ Clear any old\/partial local census data before the start of marking.\n+        heap->age_census()->reset_local();\n+        assert(heap->age_census()->is_clear_local(), \"Error\");\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<GLOBAL> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      case NON_GEN: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<NON_GEN> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      default:\n+        ShouldNotReachHere();\n+    }\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -238,3 +350,2 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  heap->set_concurrent_mark_in_progress(false);\n-  heap->mark_complete_marking_context();\n+  _generation->set_concurrent_mark_in_progress(false);\n+  _generation->set_mark_complete();\n@@ -260,0 +371,1 @@\n+<<<<<<< HEAD\n@@ -262,0 +374,1 @@\n+=======\n@@ -263,2 +376,24 @@\n-  assert(task_queues()->is_empty(), \"Should be empty\");\n-}\n+  switch (_generation->type()) {\n+    case YOUNG:{\n+      ShenandoahFinalMarkingTask<YOUNG> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    case OLD:{\n+      ShenandoahFinalMarkingTask<OLD> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    case GLOBAL:{\n+      ShenandoahFinalMarkingTask<GLOBAL> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    case NON_GEN:{\n+      ShenandoahFinalMarkingTask<NON_GEN> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -266,0 +401,1 @@\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -267,4 +403,1 @@\n-void ShenandoahConcurrentMark::cancel() {\n-  clear();\n-  ShenandoahReferenceProcessor* rp = ShenandoahHeap::heap()->ref_processor();\n-  rp->abandon_partial_discovery();\n+  assert(task_queues()->is_empty(), \"Should be empty\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.cpp","additions":149,"deletions":16,"binary":false,"changes":165,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -37,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n@@ -39,0 +41,2 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalFullGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n@@ -59,1 +63,0 @@\n-#include \"runtime\/javaThread.hpp\"\n@@ -111,0 +114,4 @@\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahGenerationalFullGC::handle_completion(heap);\n+  }\n+\n@@ -122,1 +129,1 @@\n-  heap->heuristics()->record_success_full();\n+  heap->global_generation()->heuristics()->record_success_full();\n@@ -129,0 +136,4 @@\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahGenerationalFullGC::prepare(heap);\n+  }\n+\n@@ -171,1 +182,1 @@\n-    \/\/ b. Cancel concurrent mark, if in progress\n+    \/\/ b. Cancel all concurrent marks, if in progress\n@@ -173,2 +184,2 @@\n-      ShenandoahConcurrentGC::cancel();\n-      heap->set_concurrent_mark_in_progress(false);\n+      \/\/ TODO: Send cancel_concurrent_mark upstream? Does it really not have it already?\n+      heap->cancel_concurrent_mark();\n@@ -184,1 +195,1 @@\n-    heap->reset_mark_bitmap();\n+    heap->global_generation()->reset_mark_bitmap();\n@@ -186,1 +197,1 @@\n-    assert(!heap->marking_context()->is_complete(), \"sanity\");\n+    assert(!heap->global_generation()->is_mark_complete(), \"sanity\");\n@@ -189,1 +200,1 @@\n-    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+    ShenandoahReferenceProcessor* rp = heap->global_generation()->ref_processor();\n@@ -195,0 +206,4 @@\n+    if (heap->mode()->is_generational()) {\n+      ShenandoahGenerationalFullGC::restore_top_before_promote(heap);\n+    }\n+\n@@ -202,0 +217,1 @@\n+    \/\/ TODO: Do we need to explicitly retire PLABs?\n@@ -258,0 +274,1 @@\n+  \/\/ Humongous regions are promoted on demand and are accounted for by normal Full GC mechanisms.\n@@ -276,2 +293,5 @@\n-    _ctx->capture_top_at_mark_start(r);\n-    r->clear_live_data();\n+    \/\/ TODO: Add API to heap to skip free regions\n+    if (r->affiliation() != FREE) {\n+      _ctx->capture_top_at_mark_start(r);\n+      r->clear_live_data();\n+    }\n@@ -292,1 +312,1 @@\n-  heap->set_unload_classes(heap->heuristics()->can_unload_classes());\n+  heap->set_unload_classes(heap->global_generation()->heuristics()->can_unload_classes());\n@@ -294,1 +314,1 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = heap->global_generation()->ref_processor();\n@@ -298,1 +318,1 @@\n-  ShenandoahSTWMark mark(true \/*full_gc*\/);\n+  ShenandoahSTWMark mark(heap->global_generation(), true \/*full_gc*\/);\n@@ -301,0 +321,4 @@\n+\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahGenerationalFullGC::log_live_in_old(heap);\n+  }\n@@ -426,0 +450,1 @@\n+<<<<<<< HEAD\n@@ -428,0 +453,10 @@\n+=======\n+  if (_heap->mode()->is_generational()) {\n+    ShenandoahPrepareForGenerationalCompactionObjectClosure cl(_preserved_marks->get(worker_id),\n+                                                               empty_regions, from_region, worker_id);\n+    prepare_for_compaction(cl, empty_regions, it, from_region);\n+  } else {\n+    ShenandoahPrepareForCompactionObjectClosure cl(_preserved_marks->get(worker_id), empty_regions, from_region);\n+    prepare_for_compaction(cl, empty_regions, it, from_region);\n+  }\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -474,0 +509,1 @@\n+  log_debug(gc)(\"Full GC calculating target humongous objects from end \" SIZE_FORMAT, to_end);\n@@ -516,0 +552,1 @@\n+      \/\/ Leave affiliation unchanged\n@@ -540,0 +577,6 @@\n+    if (!r->is_affiliated()) {\n+      \/\/ Ignore free regions\n+      \/\/ TODO: change iterators so they do not process FREE regions.\n+      return;\n+    }\n+\n@@ -704,0 +747,5 @@\n+\/\/ TODO:\n+\/\/  Consider compacting old-gen objects toward the high end of memory and young-gen objects towards the low-end\n+\/\/  of memory.  As currently implemented, all regions are compacted toward the low-end of memory.  This creates more\n+\/\/  fragmentation of the heap, because old-gen regions get scattered among low-address regions such that it becomes\n+\/\/  more difficult to find contiguous regions for humongous objects.\n@@ -731,0 +779,2 @@\n+    \/\/ TODO: This is ResourceMark is missing upstream.\n+    ResourceMark rm;\n@@ -805,0 +855,3 @@\n+      if (_heap->mode()->is_generational()) {\n+        ShenandoahGenerationalFullGC::maybe_coalesce_and_fill_region(r);\n+      }\n@@ -908,1 +961,3 @@\n-  size_t _live;\n+  bool _is_generational;\n+  size_t _young_regions, _young_usage, _young_humongous_waste;\n+  size_t _old_regions, _old_usage, _old_humongous_waste;\n@@ -911,1 +966,9 @@\n-  ShenandoahPostCompactClosure() : _heap(ShenandoahHeap::heap()), _live(0) {\n+  ShenandoahPostCompactClosure() : _heap(ShenandoahHeap::heap()),\n+                                   _is_generational(_heap->mode()->is_generational()),\n+                                   _young_regions(0),\n+                                   _young_usage(0),\n+                                   _young_humongous_waste(0),\n+                                   _old_regions(0),\n+                                   _old_usage(0),\n+                                   _old_humongous_waste(0)\n+  {\n@@ -931,0 +994,4 @@\n+      if (!_is_generational) {\n+        r->make_young_maybe();\n+      }\n+      \/\/ else, generational mode compaction has already established affiliation.\n@@ -946,0 +1013,6 @@\n+    } else {\n+      if (r->is_old()) {\n+        ShenandoahGenerationalFullGC::account_for_region(r, _old_regions, _old_usage, _old_humongous_waste);\n+      } else if (r->is_young()) {\n+        ShenandoahGenerationalFullGC::account_for_region(r, _young_regions, _young_usage, _young_humongous_waste);\n+      }\n@@ -947,1 +1020,0 @@\n-\n@@ -950,1 +1022,0 @@\n-    _live += live;\n@@ -953,2 +1024,15 @@\n-  size_t get_live() {\n-    return _live;\n+  void update_generation_usage() {\n+    if (_is_generational) {\n+      _heap->old_generation()->establish_usage(_old_regions, _old_usage, _old_humongous_waste);\n+      _heap->young_generation()->establish_usage(_young_regions, _young_usage, _young_humongous_waste);\n+    } else {\n+      assert(_old_regions == 0, \"Old regions only expected in generational mode\");\n+      assert(_old_usage == 0, \"Old usage only expected in generational mode\");\n+      assert(_old_humongous_waste == 0, \"Old humongous waste only expected in generational mode\");\n+    }\n+\n+    \/\/ In generational mode, global usage should be the sum of young and old. This is also true\n+    \/\/ for non-generational modes except that there are no old regions.\n+    _heap->global_generation()->establish_usage(_old_regions + _young_regions,\n+                                                _old_usage + _young_usage,\n+                                                _old_humongous_waste + _young_humongous_waste);\n@@ -985,0 +1069,1 @@\n+      log_debug(gc)(\"Full GC compaction moves humongous object from region \" SIZE_FORMAT \" to region \" SIZE_FORMAT, old_start, new_start);\n@@ -992,0 +1077,1 @@\n+        ShenandoahAffiliation original_affiliation = r->affiliation();\n@@ -994,0 +1080,1 @@\n+          \/\/ Leave humongous region affiliation unchanged.\n@@ -1001,1 +1088,1 @@\n-            r->make_humongous_start_bypass();\n+            r->make_humongous_start_bypass(original_affiliation);\n@@ -1003,1 +1090,1 @@\n-            r->make_humongous_cont_bypass();\n+            r->make_humongous_cont_bypass(original_affiliation);\n@@ -1088,1 +1175,5 @@\n-    heap->set_used(post_compact.get_live());\n+    post_compact.update_generation_usage();\n+\n+    if (heap->mode()->is_generational()) {\n+      ShenandoahGenerationalFullGC::balance_generations_after_gc(heap);\n+    }\n@@ -1091,0 +1182,1 @@\n+<<<<<<< HEAD\n@@ -1093,0 +1185,30 @@\n+=======\n+    size_t young_cset_regions, old_cset_regions;\n+    size_t first_old, last_old, num_old;\n+    heap->free_set()->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+\n+    \/\/ We also do not expand old generation size following Full GC because we have scrambled age populations and\n+    \/\/ no longer have objects separated by age into distinct regions.\n+\n+    \/\/ TODO: Do we need to fix FullGC so that it maintains aged segregation of objects into distinct regions?\n+    \/\/       A partial solution would be to remember how many objects are of tenure age following Full GC, but\n+    \/\/       this is probably suboptimal, because most of these objects will not reside in a region that will be\n+    \/\/       selected for the next evacuation phase.\n+\n+\n+    if (heap->mode()->is_generational()) {\n+      \/\/ In case this Full GC resulted from degeneration, clear the tally on anticipated promotion.\n+      heap->clear_promotion_potential();\n+      \/\/ Invoke this in case we are able to transfer memory from OLD to YOUNG.\n+      heap->compute_old_generation_balance(0, 0);\n+    }\n+    heap->free_set()->rebuild(young_cset_regions, old_cset_regions);\n+\n+    \/\/ We defer generation resizing actions until after cset regions have been recycled.  We do this even following an\n+    \/\/ abbreviated cycle.\n+    if (heap->mode()->is_generational()) {\n+      ShenandoahGenerationalFullGC::balance_generations_after_rebuilding_free_set(heap);\n+      ShenandoahGenerationalFullGC::rebuild_remembered_set(heap);\n+    }\n+    heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":144,"deletions":22,"binary":false,"changes":166,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+<<<<<<< HEAD\n@@ -30,0 +31,6 @@\n+=======\n+    NON_GEN,         \/\/ non-generational\n+    GLOBAL,          \/\/ generational: Global\n+    YOUNG,           \/\/ generational: Young\n+    OLD              \/\/ generational: Old\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -36,0 +43,9 @@\n+<<<<<<< HEAD\n+=======\n+    case GLOBAL:\n+      return \"Global\";\n+    case OLD:\n+      return \"Old\";\n+    case YOUNG:\n+      return \"Young\";\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationType.hpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -39,0 +40,4 @@\n+#include \"gc\/shenandoah\/shenandoahAgeCensus.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n@@ -40,0 +45,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -47,0 +53,1 @@\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n@@ -56,0 +63,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -62,0 +70,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -69,0 +78,2 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -72,0 +83,2 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n@@ -163,3 +176,0 @@\n-  \/\/ Now we know the number of regions, initialize the heuristics.\n-  initialize_heuristics();\n-\n@@ -181,0 +191,3 @@\n+  \/\/ Now we know the number of regions and heap sizes, initialize the heuristics.\n+  initialize_heuristics_generations();\n+\n@@ -219,0 +232,28 @@\n+  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this, _heap_region));\n+\n+  \/\/\n+  \/\/ After reserving the Java heap, create the card table, barriers, and workers, in dependency order\n+  \/\/\n+  if (mode()->is_generational()) {\n+    ShenandoahDirectCardMarkRememberedSet *rs;\n+    ShenandoahCardTable* card_table = ShenandoahBarrierSet::barrier_set()->card_table();\n+    size_t card_count = card_table->cards_required(heap_rs.size() \/ HeapWordSize);\n+    rs = new ShenandoahDirectCardMarkRememberedSet(ShenandoahBarrierSet::barrier_set()->card_table(), card_count);\n+    _card_scan = new ShenandoahScanRemembered<ShenandoahDirectCardMarkRememberedSet>(rs);\n+\n+    \/\/ Age census structure\n+    _age_census = new ShenandoahAgeCensus();\n+  }\n+\n+  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n+  if (_workers == nullptr) {\n+    vm_exit_during_initialization(\"Failed necessary allocation.\");\n+  } else {\n+    _workers->initialize_workers();\n+  }\n+\n+  if (ParallelGCThreads > 1) {\n+    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\", ParallelGCThreads);\n+    _safepoint_workers->initialize_workers();\n+  }\n+\n@@ -266,1 +307,1 @@\n-  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions, _max_workers);\n+  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions);\n@@ -357,0 +398,1 @@\n+  _affiliations = NEW_C_HEAP_ARRAY(uint8_t, _num_regions, mtGC);\n@@ -362,0 +404,1 @@\n+\n@@ -373,0 +416,2 @@\n+\n+      _affiliations[i] = ShenandoahAffiliation::FREE;\n@@ -377,0 +422,1 @@\n+    size_t young_cset_regions, old_cset_regions;\n@@ -378,1 +424,4 @@\n-    _free_set->rebuild();\n+    \/\/ We are initializing free set.  We ignore cset region tallies.\n+    size_t first_old, last_old, num_old;\n+    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+    _free_set->rebuild(young_cset_regions, old_cset_regions);\n@@ -437,0 +486,8 @@\n+  initialize_controller();\n+\n+  print_init_logger();\n+\n+  return JNI_OK;\n+}\n+\n+void ShenandoahHeap::initialize_controller() {\n@@ -438,0 +495,1 @@\n+}\n@@ -439,0 +497,1 @@\n+void ShenandoahHeap::print_init_logger() const {\n@@ -440,0 +499,1 @@\n+}\n@@ -441,1 +501,28 @@\n-  return JNI_OK;\n+size_t ShenandoahHeap::max_size_for(ShenandoahGeneration* generation) const {\n+  switch (generation->type()) {\n+    case YOUNG:\n+      return _generation_sizer.max_young_size();\n+    case OLD:\n+      return max_capacity() - _generation_sizer.min_young_size();\n+    case GLOBAL:\n+    case NON_GEN:\n+      return max_capacity();\n+    default:\n+      ShouldNotReachHere();\n+      return 0;\n+  }\n+}\n+\n+size_t ShenandoahHeap::min_size_for(ShenandoahGeneration* generation) const {\n+  switch (generation->type()) {\n+    case YOUNG:\n+      return _generation_sizer.min_young_size();\n+    case OLD:\n+      return max_capacity() - _generation_sizer.max_young_size();\n+    case GLOBAL:\n+    case NON_GEN:\n+      return min_capacity();\n+    default:\n+      ShouldNotReachHere();\n+      return 0;\n+  }\n@@ -444,1 +531,1 @@\n-void ShenandoahHeap::initialize_mode() {\n+void ShenandoahHeap::initialize_heuristics_generations() {\n@@ -452,0 +539,2 @@\n+    } else if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+      _gc_mode = new ShenandoahGenerationalMode();\n@@ -469,1 +558,0 @@\n-}\n@@ -471,3 +559,9 @@\n-void ShenandoahHeap::initialize_heuristics() {\n-  assert(_gc_mode != nullptr, \"Must be initialized\");\n-  _heuristics = _gc_mode->initialize_heuristics();\n+  \/\/ Max capacity is the maximum _allowed_ capacity. That is, the maximum allowed capacity\n+  \/\/ for old would be total heap - minimum capacity of young. This means the sum of the maximum\n+  \/\/ allowed for old and young could exceed the total heap size. It remains the case that the\n+  \/\/ _actual_ capacity of young + old = total.\n+  _generation_sizer.heap_size_changed(max_capacity());\n+  size_t initial_capacity_young = _generation_sizer.max_young_size();\n+  size_t max_capacity_young = _generation_sizer.max_young_size();\n+  size_t initial_capacity_old = max_capacity() - max_capacity_young;\n+  size_t max_capacity_old = max_capacity() - initial_capacity_young;\n@@ -475,9 +569,7 @@\n-  if (_heuristics->is_diagnostic() && !UnlockDiagnosticVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.\",\n-                    _heuristics->name()));\n-  }\n-  if (_heuristics->is_experimental() && !UnlockExperimentalVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.\",\n-                    _heuristics->name()));\n+  _young_generation = new ShenandoahYoungGeneration(_max_workers, max_capacity_young, initial_capacity_young);\n+  _old_generation = new ShenandoahOldGeneration(_max_workers, max_capacity_old, initial_capacity_old);\n+  _global_generation = new ShenandoahGlobalGeneration(_gc_mode->is_generational(), _max_workers, max_capacity(), max_capacity());\n+  _global_generation->initialize_heuristics(_gc_mode);\n+  if (mode()->is_generational()) {\n+    _young_generation->initialize_heuristics(_gc_mode);\n+    _old_generation->initialize_heuristics(_gc_mode);\n@@ -485,0 +577,1 @@\n+  _evac_tracker = new ShenandoahEvacuationTracker(mode()->is_generational());\n@@ -494,0 +587,1 @@\n+  _gc_generation(nullptr),\n@@ -495,1 +589,1 @@\n-  _used(0),\n+  _promotion_potential(0),\n@@ -497,2 +591,1 @@\n-  _bytes_allocated_since_gc_start(0),\n-  _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),\n+  _max_workers(MAX3(ConcGCThreads, ParallelGCThreads, 1U)),\n@@ -504,0 +597,1 @@\n+  _affiliations(nullptr),\n@@ -507,0 +601,9 @@\n+  _promoted_reserve(0),\n+  _old_evac_reserve(0),\n+  _young_evac_reserve(0),\n+  _age_census(nullptr),\n+  _has_evacuation_reserve_quantities(false),\n+  _cancel_requested_time(0),\n+  _young_generation(nullptr),\n+  _global_generation(nullptr),\n+  _old_generation(nullptr),\n@@ -509,2 +612,0 @@\n-  _gc_mode(nullptr),\n-  _heuristics(nullptr),\n@@ -515,0 +616,3 @@\n+  _evac_tracker(nullptr),\n+  _mmu_tracker(),\n+  _generation_sizer(),\n@@ -517,0 +621,2 @@\n+  _young_gen_memory_pool(nullptr),\n+  _old_gen_memory_pool(nullptr),\n@@ -521,1 +627,2 @@\n-  _ref_processor(new ShenandoahReferenceProcessor(MAX2(_max_workers, 1U))),\n+  _old_regions_surplus(0),\n+  _old_regions_deficit(0),\n@@ -529,1 +636,2 @@\n-  _collection_set(nullptr)\n+  _collection_set(nullptr),\n+  _card_scan(nullptr)\n@@ -531,17 +639,0 @@\n-  \/\/ Initialize GC mode early, so we can adjust barrier support\n-  initialize_mode();\n-  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));\n-\n-  _max_workers = MAX2(_max_workers, 1U);\n-  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n-  if (_workers == nullptr) {\n-    vm_exit_during_initialization(\"Failed necessary allocation.\");\n-  } else {\n-    _workers->initialize_workers();\n-  }\n-\n-  if (ParallelGCThreads > 1) {\n-    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\",\n-                                                ParallelGCThreads);\n-    _safepoint_workers->initialize_workers();\n-  }\n@@ -554,29 +645,0 @@\n-class ShenandoahResetBitmapTask : public WorkerTask {\n-private:\n-  ShenandoahRegionIterator _regions;\n-\n-public:\n-  ShenandoahResetBitmapTask() :\n-    WorkerTask(\"Shenandoah Reset Bitmap\") {}\n-\n-  void work(uint worker_id) {\n-    ShenandoahHeapRegion* region = _regions.next();\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahMarkingContext* const ctx = heap->marking_context();\n-    while (region != nullptr) {\n-      if (heap->is_bitmap_slice_committed(region)) {\n-        ctx->clear_bitmap(region);\n-      }\n-      region = _regions.next();\n-    }\n-  }\n-};\n-\n-void ShenandoahHeap::reset_mark_bitmap() {\n-  assert_gc_workers(_workers->active_workers());\n-  mark_incomplete_marking_context();\n-\n-  ShenandoahResetBitmapTask task;\n-  _workers->run_task(&task);\n-}\n-\n@@ -597,1 +659,2 @@\n-  if (is_concurrent_mark_in_progress())        st->print(\"marking, \");\n+  if (is_concurrent_old_mark_in_progress())    st->print(\"old marking, \");\n+  if (is_concurrent_young_mark_in_progress())  st->print(\"young marking, \");\n@@ -648,0 +711,2 @@\n+  _mmu_tracker.initialize();\n+\n@@ -661,0 +726,1 @@\n+<<<<<<< HEAD\n@@ -664,0 +730,27 @@\n+=======\n+  JFR_ONLY(ShenandoahJFRSupport::register_jfr_type_serializers());\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n+}\n+\n+ShenandoahHeuristics* ShenandoahHeap::heuristics() {\n+  return _global_generation->heuristics();\n+}\n+\n+ShenandoahOldHeuristics* ShenandoahHeap::old_heuristics() {\n+  return (ShenandoahOldHeuristics*) _old_generation->heuristics();\n+}\n+\n+ShenandoahYoungHeuristics* ShenandoahHeap::young_heuristics() {\n+  return (ShenandoahYoungHeuristics*) _young_generation->heuristics();\n+}\n+\n+bool ShenandoahHeap::doing_mixed_evacuations() {\n+  return _old_generation->state() == ShenandoahOldGeneration::EVACUATING;\n+}\n+\n+bool ShenandoahHeap::is_old_bitmap_stable() const {\n+  return _old_generation->is_mark_complete();\n+}\n+\n+bool ShenandoahHeap::is_gc_generation_young() const {\n+  return _gc_generation != nullptr && _gc_generation->is_young();\n@@ -667,1 +760,1 @@\n-  return Atomic::load(&_used);\n+  return global_generation()->used();\n@@ -674,4 +767,0 @@\n-size_t ShenandoahHeap::available() const {\n-  return free_set()->available();\n-}\n-\n@@ -688,2 +777,43 @@\n-void ShenandoahHeap::increase_used(size_t bytes) {\n-  Atomic::add(&_used, bytes, memory_order_relaxed);\n+\/\/ For tracking usage based on allocations, it should be the case that:\n+\/\/ * The sum of regions::used == heap::used\n+\/\/ * The sum of a generation's regions::used == generation::used\n+\/\/ * The sum of a generation's humongous regions::free == generation::humongous_waste\n+\/\/ These invariants are checked by the verifier on GC safepoints.\n+\/\/\n+\/\/ Additional notes:\n+\/\/ * When a mutator's allocation request causes a region to be retired, the\n+\/\/   free memory left in that region is considered waste. It does not contribute\n+\/\/   to the usage, but it _does_ contribute to allocation rate.\n+\/\/ * The bottom of a PLAB must be aligned on card size. In some cases this will\n+\/\/   require padding in front of the PLAB (a filler object). Because this padding\n+\/\/   is included in the region's used memory we include the padding in the usage\n+\/\/   accounting as waste.\n+\/\/ * Mutator allocations are used to compute an allocation rate. They are also\n+\/\/   sent to the Pacer for those purposes.\n+\/\/ * There are three sources of waste:\n+\/\/  1. The padding used to align a PLAB on card size\n+\/\/  2. Region's free is less than minimum TLAB size and is retired\n+\/\/  3. The unused portion of memory in the last region of a humongous object\n+void ShenandoahHeap::increase_used(const ShenandoahAllocRequest& req) {\n+  size_t actual_bytes = req.actual_size() * HeapWordSize;\n+  size_t wasted_bytes = req.waste() * HeapWordSize;\n+  ShenandoahGeneration* generation = generation_for(req.affiliation());\n+\n+  if (req.is_gc_alloc()) {\n+    assert(wasted_bytes == 0 || req.type() == ShenandoahAllocRequest::_alloc_plab, \"Only PLABs have waste\");\n+    increase_used(generation, actual_bytes + wasted_bytes);\n+  } else {\n+    assert(req.is_mutator_alloc(), \"Expected mutator alloc here\");\n+    \/\/ padding and actual size both count towards allocation counter\n+    generation->increase_allocated(actual_bytes + wasted_bytes);\n+\n+    \/\/ only actual size counts toward usage for mutator allocations\n+    increase_used(generation, actual_bytes);\n+\n+    \/\/ notify pacer of both actual size and waste\n+    notify_mutator_alloc_words(req.actual_size(), req.waste());\n+\n+    if (wasted_bytes > 0 && req.actual_size() > ShenandoahHeapRegion::humongous_threshold_words()) {\n+      increase_humongous_waste(generation,wasted_bytes);\n+    }\n+  }\n@@ -692,2 +822,5 @@\n-void ShenandoahHeap::set_used(size_t bytes) {\n-  Atomic::store(&_used, bytes);\n+void ShenandoahHeap::increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_humongous_waste(bytes);\n+  }\n@@ -696,3 +829,5 @@\n-void ShenandoahHeap::decrease_used(size_t bytes) {\n-  assert(used() >= bytes, \"never decrease heap size by more than we've left\");\n-  Atomic::sub(&_used, bytes, memory_order_relaxed);\n+void ShenandoahHeap::decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_humongous_waste(bytes);\n+  }\n@@ -701,2 +836,5 @@\n-void ShenandoahHeap::increase_allocated(size_t bytes) {\n-  Atomic::add(&_bytes_allocated_since_gc_start, bytes, memory_order_relaxed);\n+void ShenandoahHeap::increase_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_used(bytes);\n+  }\n@@ -705,4 +843,4 @@\n-void ShenandoahHeap::notify_mutator_alloc_words(size_t words, bool waste) {\n-  size_t bytes = words * HeapWordSize;\n-  if (!waste) {\n-    increase_used(bytes);\n+void ShenandoahHeap::decrease_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_used(bytes);\n@@ -710,1 +848,3 @@\n-  increase_allocated(bytes);\n+}\n+\n+void ShenandoahHeap::notify_mutator_alloc_words(size_t words, size_t waste) {\n@@ -713,2 +853,2 @@\n-    if (waste) {\n-      pacer()->claim_for_alloc(words, true);\n+    if (waste > 0) {\n+      pacer()->claim_for_alloc(waste, true);\n@@ -750,6 +890,0 @@\n-bool ShenandoahHeap::is_in(const void* p) const {\n-  HeapWord* heap_base = (HeapWord*) base();\n-  HeapWord* last_region_end = heap_base + ShenandoahHeapRegion::region_size_words() * num_regions();\n-  return p >= heap_base && p < last_region_end;\n-}\n-\n@@ -835,3 +969,1 @@\n-\n-  \/\/ This is called from allocation path, and thus should be fast.\n-  _heap_changed.try_set();\n+  _heap_changed.set();\n@@ -848,0 +980,64 @@\n+void ShenandoahHeap::handle_old_evacuation(HeapWord* obj, size_t words, bool promotion) {\n+  \/\/ Only register the copy of the object that won the evacuation race.\n+  card_scan()->register_object_without_lock(obj);\n+\n+  \/\/ Mark the entire range of the evacuated object as dirty.  At next remembered set scan,\n+  \/\/ we will clear dirty bits that do not hold interesting pointers.  It's more efficient to\n+  \/\/ do this in batch, in a background GC thread than to try to carefully dirty only cards\n+  \/\/ that hold interesting pointers right now.\n+  card_scan()->mark_range_as_dirty(obj, words);\n+\n+  if (promotion) {\n+    \/\/ This evacuation was a promotion, track this as allocation against old gen\n+    old_generation()->increase_allocated(words * HeapWordSize);\n+  }\n+}\n+\n+void ShenandoahHeap::handle_old_evacuation_failure() {\n+  if (_old_gen_oom_evac.try_set()) {\n+    log_info(gc)(\"Old gen evac failure.\");\n+  }\n+}\n+\n+void ShenandoahHeap::report_promotion_failure(Thread* thread, size_t size) {\n+  \/\/ We squelch excessive reports to reduce noise in logs.\n+  const size_t MaxReportsPerEpoch = 4;\n+  static size_t last_report_epoch = 0;\n+  static size_t epoch_report_count = 0;\n+\n+  size_t promotion_reserve;\n+  size_t promotion_expended;\n+\n+  size_t gc_id = control_thread()->get_gc_id();\n+\n+  if ((gc_id != last_report_epoch) || (epoch_report_count++ < MaxReportsPerEpoch)) {\n+    {\n+      \/\/ Promotion failures should be very rare.  Invest in providing useful diagnostic info.\n+      ShenandoahHeapLocker locker(lock());\n+      promotion_reserve = get_promoted_reserve();\n+      promotion_expended = get_promoted_expended();\n+    }\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    size_t words_remaining = (plab == nullptr)? 0: plab->words_remaining();\n+    const char* promote_enabled = ShenandoahThreadLocalData::allow_plab_promotions(thread)? \"enabled\": \"disabled\";\n+    ShenandoahGeneration* old_gen = old_generation();\n+    size_t old_capacity = old_gen->max_capacity();\n+    size_t old_usage = old_gen->used();\n+    size_t old_free_regions = old_gen->free_unaffiliated_regions();\n+\n+    log_info(gc, ergo)(\"Promotion failed, size \" SIZE_FORMAT \", has plab? %s, PLAB remaining: \" SIZE_FORMAT\n+                       \", plab promotions %s, promotion reserve: \" SIZE_FORMAT \", promotion expended: \" SIZE_FORMAT\n+                       \", old capacity: \" SIZE_FORMAT \", old_used: \" SIZE_FORMAT \", old unaffiliated regions: \" SIZE_FORMAT,\n+                       size * HeapWordSize, plab == nullptr? \"no\": \"yes\",\n+                       words_remaining * HeapWordSize, promote_enabled, promotion_reserve, promotion_expended,\n+                       old_capacity, old_usage, old_free_regions);\n+\n+    if ((gc_id == last_report_epoch) && (epoch_report_count >= MaxReportsPerEpoch)) {\n+      log_info(gc, ergo)(\"Squelching additional promotion failure reports for current epoch\");\n+    } else if (gc_id != last_report_epoch) {\n+      last_report_epoch = gc_id;\n+      epoch_report_count = 1;\n+    }\n+  }\n+}\n+\n@@ -854,0 +1050,8 @@\n+\n+  \/\/ Limit growth of GCLABs to ShenandoahMaxEvacLABRatio * the minimum size.  This enables more equitable distribution of\n+  \/\/ available evacuation buidget between the many threads that are coordinating in the evacuation effort.\n+  if (ShenandoahMaxEvacLABRatio > 0) {\n+    log_debug(gc, free)(\"Allocate new gclab: \" SIZE_FORMAT \", \" SIZE_FORMAT, new_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+    new_size = MIN2(new_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+  }\n+\n@@ -865,0 +1069,1 @@\n+    log_debug(gc, free)(\"New gclab size (\" SIZE_FORMAT \") is too small for \" SIZE_FORMAT, new_size, size);\n@@ -897,0 +1102,250 @@\n+\/\/ Establish a new PLAB and allocate size HeapWords within it.\n+HeapWord* ShenandoahHeap::allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion) {\n+  \/\/ New object should fit the PLAB size\n+  size_t min_size = MAX2(size, PLAB::min_size());\n+\n+  \/\/ Figure out size of new PLAB, looking back at heuristics. Expand aggressively.\n+  size_t cur_size = ShenandoahThreadLocalData::plab_size(thread);\n+  if (cur_size == 0) {\n+    cur_size = PLAB::min_size();\n+  }\n+  size_t future_size = cur_size * 2;\n+  \/\/ Limit growth of PLABs to ShenandoahMaxEvacLABRatio * the minimum size.  This enables more equitable distribution of\n+  \/\/ available evacuation buidget between the many threads that are coordinating in the evacuation effort.\n+  if (ShenandoahMaxEvacLABRatio > 0) {\n+    future_size = MIN2(future_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+  }\n+  future_size = MIN2(future_size, PLAB::max_size());\n+  future_size = MAX2(future_size, PLAB::min_size());\n+\n+  size_t unalignment = future_size % CardTable::card_size_in_words();\n+  if (unalignment != 0) {\n+    future_size = future_size - unalignment + CardTable::card_size_in_words();\n+  }\n+\n+  \/\/ Record new heuristic value even if we take any shortcut. This captures\n+  \/\/ the case when moderately-sized objects always take a shortcut. At some point,\n+  \/\/ heuristics should catch up with them.  Note that the requested cur_size may\n+  \/\/ not be honored, but we remember that this is the preferred size.\n+  ShenandoahThreadLocalData::set_plab_size(thread, future_size);\n+  if (cur_size < size) {\n+    \/\/ The PLAB to be allocated is still not large enough to hold the object. Fall back to shared allocation.\n+    \/\/ This avoids retiring perfectly good PLABs in order to represent a single large object allocation.\n+    return nullptr;\n+  }\n+\n+  \/\/ Retire current PLAB, and allocate a new one.\n+  PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+  if (plab->words_remaining() < PLAB::min_size()) {\n+    \/\/ Retire current PLAB, and allocate a new one.\n+    \/\/ CAUTION: retire_plab may register the remnant filler object with the remembered set scanner without a lock.  This\n+    \/\/ is safe iff it is assured that each PLAB is a whole-number multiple of card-mark memory size and each PLAB is\n+    \/\/ aligned with the start of a card's memory range.\n+    retire_plab(plab, thread);\n+\n+    size_t actual_size = 0;\n+    \/\/ allocate_new_plab resets plab_evacuated and plab_promoted and disables promotions if old-gen available is\n+    \/\/ less than the remaining evacuation need.  It also adjusts plab_preallocated and expend_promoted if appropriate.\n+    HeapWord* plab_buf = allocate_new_plab(min_size, cur_size, &actual_size);\n+    if (plab_buf == nullptr) {\n+      if (min_size == PLAB::min_size()) {\n+        \/\/ Disable plab promotions for this thread because we cannot even allocate a plab of minimal size.  This allows us\n+        \/\/ to fail faster on subsequent promotion attempts.\n+        ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+      }\n+      return NULL;\n+    } else {\n+      ShenandoahThreadLocalData::enable_plab_retries(thread);\n+    }\n+    assert (size <= actual_size, \"allocation should fit\");\n+    if (ZeroTLAB) {\n+      \/\/ ..and clear it.\n+      Copy::zero_to_words(plab_buf, actual_size);\n+    } else {\n+      \/\/ ...and zap just allocated object.\n+#ifdef ASSERT\n+      \/\/ Skip mangling the space corresponding to the object header to\n+      \/\/ ensure that the returned space is not considered parsable by\n+      \/\/ any concurrent GC thread.\n+      size_t hdr_size = oopDesc::header_size();\n+      Copy::fill_to_words(plab_buf + hdr_size, actual_size - hdr_size, badHeapWordVal);\n+#endif \/\/ ASSERT\n+    }\n+    plab->set_buf(plab_buf, actual_size);\n+    if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+      return nullptr;\n+    }\n+    return plab->allocate(size);\n+  } else {\n+    \/\/ If there's still at least min_size() words available within the current plab, don't retire it.  Let's gnaw\n+    \/\/ away on this plab as long as we can.  Meanwhile, return nullptr to force this particular allocation request\n+    \/\/ to be satisfied with a shared allocation.  By packing more promotions into the previously allocated PLAB, we\n+    \/\/ reduce the likelihood of evacuation failures, and we we reduce the need for downsizing our PLABs.\n+    return nullptr;\n+  }\n+}\n+\n+\/\/ TODO: It is probably most efficient to register all objects (both promotions and evacuations) that were allocated within\n+\/\/ this plab at the time we retire the plab.  A tight registration loop will run within both code and data caches.  This change\n+\/\/ would allow smaller and faster in-line implementation of alloc_from_plab().  Since plabs are aligned on card-table boundaries,\n+\/\/ this object registration loop can be performed without acquiring a lock.\n+void ShenandoahHeap::retire_plab(PLAB* plab, Thread* thread) {\n+  \/\/ We don't enforce limits on plab_evacuated.  We let it consume all available old-gen memory in order to reduce\n+  \/\/ probability of an evacuation failure.  We do enforce limits on promotion, to make sure that excessive promotion\n+  \/\/ does not result in an old-gen evacuation failure.  Note that a failed promotion is relatively harmless.  Any\n+  \/\/ object that fails to promote in the current cycle will be eligible for promotion in a subsequent cycle.\n+\n+  \/\/ When the plab was instantiated, its entirety was treated as if the entire buffer was going to be dedicated to\n+  \/\/ promotions.  Now that we are retiring the buffer, we adjust for the reality that the plab is not entirely promotions.\n+  \/\/  1. Some of the plab may have been dedicated to evacuations.\n+  \/\/  2. Some of the plab may have been abandoned due to waste (at the end of the plab).\n+  size_t not_promoted =\n+    ShenandoahThreadLocalData::get_plab_preallocated_promoted(thread) - ShenandoahThreadLocalData::get_plab_promoted(thread);\n+  ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+  ShenandoahThreadLocalData::reset_plab_evacuated(thread);\n+  ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+  if (not_promoted > 0) {\n+    unexpend_promoted(not_promoted);\n+  }\n+  size_t waste = plab->waste();\n+  HeapWord* top = plab->top();\n+  plab->retire();\n+  if (top != nullptr && plab->waste() > waste && is_in_old(top)) {\n+    \/\/ If retiring the plab created a filler object, then we\n+    \/\/ need to register it with our card scanner so it can\n+    \/\/ safely walk the region backing the plab.\n+    log_debug(gc)(\"retire_plab() is registering remnant of size \" SIZE_FORMAT \" at \" PTR_FORMAT,\n+                  plab->waste() - waste, p2i(top));\n+    card_scan()->register_object_without_lock(top);\n+  }\n+}\n+\n+void ShenandoahHeap::retire_plab(PLAB* plab) {\n+  Thread* thread = Thread::current();\n+  retire_plab(plab, thread);\n+}\n+\n+void ShenandoahHeap::cancel_old_gc() {\n+  shenandoah_assert_safepoint();\n+  assert(_old_generation != nullptr, \"Should only have mixed collections in generation mode.\");\n+  if (_old_generation->state() == ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP) {\n+    assert(!old_generation()->is_concurrent_mark_in_progress(), \"Cannot be marking in IDLE\");\n+    assert(!old_heuristics()->has_coalesce_and_fill_candidates(), \"Cannot have coalesce and fill candidates in IDLE\");\n+    assert(!old_heuristics()->unprocessed_old_collection_candidates(), \"Cannot have mixed collection candidates in IDLE\");\n+    assert(!young_generation()->is_bootstrap_cycle(), \"Cannot have old mark queues if IDLE\");\n+  } else {\n+    log_info(gc)(\"Terminating old gc cycle.\");\n+    \/\/ Stop marking\n+    old_generation()->cancel_marking();\n+    \/\/ Stop tracking old regions\n+    old_heuristics()->abandon_collection_candidates();\n+    \/\/ Remove old generation access to young generation mark queues\n+    young_generation()->set_old_gen_task_queues(nullptr);\n+    \/\/ Transition to IDLE now.\n+    _old_generation->transition_to(ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP);\n+  }\n+}\n+\n+\/\/ Make sure old-generation is large enough, but no larger than is necessary, to hold mixed evacuations\n+\/\/ and promotions, if we anticipate either. Any deficit is provided by the young generation, subject to\n+\/\/ xfer_limit, and any surplus is transferred to the young generation.\n+\/\/ xfer_limit is the maximum we're able to transfer from young to old.\n+void ShenandoahHeap::compute_old_generation_balance(size_t old_xfer_limit, size_t old_cset_regions) {\n+\n+  \/\/ We can limit the old reserve to the size of anticipated promotions:\n+  \/\/ max_old_reserve is an upper bound on memory evacuated from old and promoted to old,\n+  \/\/ clamped by the old generation space available.\n+  \/\/\n+  \/\/ Here's the algebra.\n+  \/\/ Let SOEP = ShenandoahOldEvacRatioPercent,\n+  \/\/     OE = old evac,\n+  \/\/     YE = young evac, and\n+  \/\/     TE = total evac = OE + YE\n+  \/\/ By definition:\n+  \/\/            SOEP\/100 = OE\/TE\n+  \/\/                     = OE\/(OE+YE)\n+  \/\/  => SOEP\/(100-SOEP) = OE\/((OE+YE)-OE)      \/\/ componendo-dividendo: If a\/b = c\/d, then a\/(b-a) = c\/(d-c)\n+  \/\/                     = OE\/YE\n+  \/\/  =>              OE = YE*SOEP\/(100-SOEP)\n+\n+  \/\/ We have to be careful in the event that SOEP is set to 100 by the user.\n+  assert(ShenandoahOldEvacRatioPercent <= 100, \"Error\");\n+  const size_t old_available = old_generation()->available();\n+  \/\/ The free set will reserve this amount of memory to hold young evacuations\n+  const size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+\n+  \/\/ In the case that ShenandoahOldEvacRatioPercent equals 100, max_old_reserve is limited only by xfer_limit.\n+\n+  const size_t bound_on_old_reserve = old_available + old_xfer_limit + young_reserve;\n+  const size_t max_old_reserve = (ShenandoahOldEvacRatioPercent == 100)?\n+    bound_on_old_reserve: MIN2((young_reserve * ShenandoahOldEvacRatioPercent) \/ (100 - ShenandoahOldEvacRatioPercent),\n+                               bound_on_old_reserve);\n+\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+  \/\/ Decide how much old space we should reserve for a mixed collection\n+  size_t reserve_for_mixed = 0;\n+  const size_t mixed_candidates = old_heuristics()->unprocessed_old_collection_candidates();\n+  const bool doing_mixed = (mixed_candidates > 0);\n+  if (doing_mixed) {\n+    \/\/ We want this much memory to be unfragmented in order to reliably evacuate old.  This is conservative because we\n+    \/\/ may not evacuate the entirety of unprocessed candidates in a single mixed evacuation.\n+    size_t max_evac_need = (size_t)\n+      (old_heuristics()->unprocessed_old_collection_candidates_live_memory() * ShenandoahOldEvacWaste);\n+    assert(old_available >= old_generation()->free_unaffiliated_regions() * region_size_bytes,\n+           \"Unaffiliated available must be less than total available\");\n+    size_t old_fragmented_available =\n+      old_available - old_generation()->free_unaffiliated_regions() * region_size_bytes;\n+    reserve_for_mixed = max_evac_need + old_fragmented_available;\n+    if (reserve_for_mixed > max_old_reserve) {\n+      reserve_for_mixed = max_old_reserve;\n+    }\n+  }\n+\n+  \/\/ Decide how much space we should reserve for promotions from young\n+  size_t reserve_for_promo = 0;\n+  const size_t promo_load = get_promotion_potential();\n+  const bool doing_promotions = promo_load > 0;\n+  if (doing_promotions) {\n+    \/\/ We're promoting and have a bound on the maximum amount that can be promoted\n+    assert(max_old_reserve >= reserve_for_mixed, \"Sanity\");\n+    const size_t available_for_promotions = max_old_reserve - reserve_for_mixed;\n+    reserve_for_promo = MIN2((size_t)(promo_load * ShenandoahPromoEvacWaste), available_for_promotions);\n+  }\n+\n+  \/\/ This is the total old we want to ideally reserve\n+  const size_t old_reserve = reserve_for_mixed + reserve_for_promo;\n+  assert(old_reserve <= max_old_reserve, \"cannot reserve more than max for old evacuations\");\n+\n+  \/\/ We now check if the old generation is running a surplus or a deficit.\n+  size_t old_region_deficit = 0;\n+  size_t old_region_surplus = 0;\n+\n+  const size_t max_old_available = old_generation()->available() + old_cset_regions * region_size_bytes;\n+  if (max_old_available >= old_reserve) {\n+    \/\/ We are running a surplus, so the old region surplus can go to young\n+    const size_t old_surplus = max_old_available - old_reserve;\n+    old_region_surplus = old_surplus \/ region_size_bytes;\n+    const size_t unaffiliated_old_regions = old_generation()->free_unaffiliated_regions() + old_cset_regions;\n+    old_region_surplus = MIN2(old_region_surplus, unaffiliated_old_regions);\n+  } else {\n+    \/\/ We are running a deficit which we'd like to fill from young.\n+    \/\/ Ignore that this will directly impact young_generation()->max_capacity(),\n+    \/\/ indirectly impacting young_reserve and old_reserve.  These computations are conservative.\n+    const size_t old_need = old_reserve - max_old_available;\n+    \/\/ The old region deficit (rounded up) will come from young\n+    old_region_deficit = (old_need + region_size_bytes - 1) \/ region_size_bytes;\n+\n+    \/\/ Round down the regions we can transfer from young to old. If we're running short\n+    \/\/ on young-gen memory, we restrict the xfer. Old-gen collection activities will be\n+    \/\/ curtailed if the budget is restricted.\n+    const size_t max_old_region_xfer = old_xfer_limit \/ region_size_bytes;\n+    old_region_deficit = MIN2(old_region_deficit, max_old_region_xfer);\n+  }\n+  assert(old_region_deficit == 0 || old_region_surplus == 0, \"Only surplus or deficit, never both\");\n+\n+  set_old_region_surplus(old_region_surplus);\n+  set_old_region_deficit(old_region_deficit);\n+}\n+\n+\/\/ Called from stubs in JIT code or interpreter\n@@ -901,1 +1356,1 @@\n-  HeapWord* res = allocate_memory(req);\n+  HeapWord* res = allocate_memory(req, false);\n@@ -914,1 +1369,1 @@\n-  HeapWord* res = allocate_memory(req);\n+  HeapWord* res = allocate_memory(req, false);\n@@ -923,1 +1378,23 @@\n-HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest& req) {\n+HeapWord* ShenandoahHeap::allocate_new_plab(size_t min_size,\n+                                            size_t word_size,\n+                                            size_t* actual_size) {\n+  \/\/ Align requested sizes to card sized multiples\n+  size_t words_in_card = CardTable::card_size_in_words();\n+  size_t align_mask = ~(words_in_card - 1);\n+  min_size = (min_size + words_in_card - 1) & align_mask;\n+  word_size = (word_size + words_in_card - 1) & align_mask;\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(min_size, word_size);\n+  \/\/ Note that allocate_memory() sets a thread-local flag to prohibit further promotions by this thread\n+  \/\/ if we are at risk of infringing on the old-gen evacuation budget.\n+  HeapWord* res = allocate_memory(req, false);\n+  if (res != nullptr) {\n+    *actual_size = req.actual_size();\n+  } else {\n+    *actual_size = 0;\n+  }\n+  return res;\n+}\n+\n+\/\/ is_promotion is true iff this allocation is known for sure to hold the result of young-gen evacuation\n+\/\/ to old-gen.  plab allocates are not known as such, since they may hold old-gen evacuations.\n+HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest& req, bool is_promotion) {\n@@ -935,1 +1412,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -960,2 +1437,2 @@\n-      control_thread()->handle_alloc_failure(req);\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      control_thread()->handle_alloc_failure(req, true);\n+      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -969,0 +1446,1 @@\n+\n@@ -971,1 +1449,1 @@\n-    result = allocate_memory_under_lock(req, in_new_region);\n+    result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -980,0 +1458,8 @@\n+  if (result == nullptr) {\n+    req.set_actual_size(0);\n+  }\n+\n+  \/\/ This is called regardless of the outcome of the allocation to account\n+  \/\/ for any waste created by retiring regions with this request.\n+  increase_used(req);\n+\n@@ -989,2 +1475,0 @@\n-      notify_mutator_alloc_words(actual, false);\n-\n@@ -997,2 +1481,0 @@\n-    } else {\n-      increase_used(actual*HeapWordSize);\n@@ -1005,0 +1487,1 @@\n+<<<<<<< HEAD\n@@ -1012,0 +1495,184 @@\n+=======\n+HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest& req, bool& in_new_region, bool is_promotion) {\n+  bool try_smaller_lab_size = false;\n+  size_t smaller_lab_size;\n+  {\n+    \/\/ promotion_eligible pertains only to PLAB allocations, denoting that the PLAB is allowed to allocate for promotions.\n+    bool promotion_eligible = false;\n+    bool allow_allocation = true;\n+    bool plab_alloc = false;\n+    size_t requested_bytes = req.size() * HeapWordSize;\n+    HeapWord* result = nullptr;\n+    ShenandoahHeapLocker locker(lock());\n+    Thread* thread = Thread::current();\n+\n+    if (mode()->is_generational()) {\n+      if (req.affiliation() == YOUNG_GENERATION) {\n+        if (req.is_mutator_alloc()) {\n+          size_t young_words_available = young_generation()->available() \/ HeapWordSize;\n+          if (req.is_lab_alloc() && (req.min_size() < young_words_available)) {\n+            \/\/ Allow ourselves to try a smaller lab size even if requested_bytes <= young_available.  We may need a smaller\n+            \/\/ lab size because young memory has become too fragmented.\n+            try_smaller_lab_size = true;\n+            smaller_lab_size = (young_words_available < req.size())? young_words_available: req.size();\n+          } else if (req.size() > young_words_available) {\n+            \/\/ Can't allocate because even min_size() is larger than remaining young_available\n+            log_info(gc, ergo)(\"Unable to shrink %s alloc request of minimum size: \" SIZE_FORMAT\n+                               \", young words available: \" SIZE_FORMAT, req.type_string(),\n+                               HeapWordSize * (req.is_lab_alloc()? req.min_size(): req.size()), young_words_available);\n+            return nullptr;\n+          }\n+        }\n+      } else {                    \/\/ reg.affiliation() == OLD_GENERATION\n+        assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"GCLAB pertains only to young-gen memory\");\n+        if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+          plab_alloc = true;\n+          size_t promotion_avail = get_promoted_reserve();\n+          size_t promotion_expended = get_promoted_expended();\n+          if (promotion_expended + requested_bytes > promotion_avail) {\n+            promotion_avail = 0;\n+            if (get_old_evac_reserve() == 0) {\n+              \/\/ There are no old-gen evacuations in this pass.  There's no value in creating a plab that cannot\n+              \/\/ be used for promotions.\n+              allow_allocation = false;\n+            }\n+          } else {\n+            promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n+            promotion_eligible = true;\n+          }\n+        } else if (is_promotion) {\n+          \/\/ This is a shared alloc for promotion\n+          size_t promotion_avail = get_promoted_reserve();\n+          size_t promotion_expended = get_promoted_expended();\n+          if (promotion_expended + requested_bytes > promotion_avail) {\n+            promotion_avail = 0;\n+          } else {\n+            promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n+          }\n+          if (promotion_avail == 0) {\n+            \/\/ We need to reserve the remaining memory for evacuation.  Reject this allocation.  The object will be\n+            \/\/ evacuated to young-gen memory and promoted during a future GC pass.\n+            return nullptr;\n+          }\n+          \/\/ Else, we'll allow the allocation to proceed.  (Since we hold heap lock, the tested condition remains true.)\n+        } else {\n+          \/\/ This is a shared allocation for evacuation.  Memory has already been reserved for this purpose.\n+        }\n+      }\n+    } \/\/ This ends the is_generational() block\n+\n+    \/\/ First try the original request.  If TLAB request size is greater than available, allocate() will attempt to downsize\n+    \/\/ request to fit within available memory.\n+    result = (allow_allocation)? _free_set->allocate(req, in_new_region): nullptr;\n+    if (result != nullptr) {\n+      if (req.is_old()) {\n+        ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+        if (req.is_gc_alloc()) {\n+          bool disable_plab_promotions = false;\n+          if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+            if (promotion_eligible) {\n+              size_t actual_size = req.actual_size() * HeapWordSize;\n+              \/\/ The actual size of the allocation may be larger than the requested bytes (due to alignment on card boundaries).\n+              \/\/ If this puts us over our promotion budget, we need to disable future PLAB promotions for this thread.\n+              if (get_promoted_expended() + actual_size <= get_promoted_reserve()) {\n+                \/\/ Assume the entirety of this PLAB will be used for promotion.  This prevents promotion from overreach.\n+                \/\/ When we retire this plab, we'll unexpend what we don't really use.\n+                ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+                expend_promoted(actual_size);\n+                assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+                ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, actual_size);\n+              } else {\n+                disable_plab_promotions = true;\n+              }\n+            } else {\n+              disable_plab_promotions = true;\n+            }\n+            if (disable_plab_promotions) {\n+              \/\/ Disable promotions in this thread because entirety of this PLAB must be available to hold old-gen evacuations.\n+              ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+              ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+            }\n+          } else if (is_promotion) {\n+            \/\/ Shared promotion.  Assume size is requested_bytes.\n+            expend_promoted(requested_bytes);\n+            assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+          }\n+        }\n+\n+        \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+        \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+        \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+        \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+        \/\/\n+        \/\/ objects being \"concurrently\" allocated:\n+        \/\/    [-----a------][-----b-----][--------------c------------------]\n+        \/\/            [---- card table memory range --------------]\n+        \/\/\n+        \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n+        \/\/   wants to set the starts-object, first-start, and last-start attributes of the preceding card region.\n+        \/\/   allocation of object b wants to set the starts-object, first-start, and last-start attributes of this card region.\n+        \/\/   allocation of object c also wants to set the starts-object, first-start, and last-start attributes of this\n+        \/\/   card region.\n+        \/\/\n+        \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n+        \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n+        \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+        ShenandoahHeap::heap()->card_scan()->register_object(result);\n+      }\n+    } else {\n+      \/\/ The allocation failed.  If this was a plab allocation, We've already retired it and no longer have a plab.\n+      if (req.is_old() && req.is_gc_alloc() && (req.type() == ShenandoahAllocRequest::_alloc_plab)) {\n+        \/\/ We don't need to disable PLAB promotions because there is no PLAB.  We leave promotions enabled because\n+        \/\/ this allows the surrounding infrastructure to retry alloc_plab_slow() with a smaller PLAB size.\n+        ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+      }\n+    }\n+    if ((result != nullptr) || !try_smaller_lab_size) {\n+      return result;\n+    }\n+    \/\/ else, fall through to try_smaller_lab_size\n+  } \/\/ This closes the block that holds the heap lock, releasing the lock.\n+\n+  \/\/ We failed to allocate the originally requested lab size.  Let's see if we can allocate a smaller lab size.\n+  if (req.size() == smaller_lab_size) {\n+    \/\/ If we were already trying to allocate min size, no value in attempting to repeat the same.  End the recursion.\n+    return nullptr;\n+  }\n+\n+  \/\/ We arrive here if the tlab allocation request can be resized to fit within young_available\n+  assert((req.affiliation() == YOUNG_GENERATION) && req.is_lab_alloc() && req.is_mutator_alloc() &&\n+         (smaller_lab_size < req.size()), \"Only shrink allocation request size for TLAB allocations\");\n+\n+  \/\/ By convention, ShenandoahAllocationRequest is primarily read-only.  The only mutable instance data is represented by\n+  \/\/ actual_size(), which is overwritten with the size of the allocaion when the allocation request is satisfied.  We use a\n+  \/\/ recursive call here rather than introducing new methods to mutate the existing ShenandoahAllocationRequest argument.\n+  \/\/ Mutation of the existing object might result in astonishing results if calling contexts assume the content of immutable\n+  \/\/ fields remain constant.  The original TLAB allocation request was for memory that exceeded the current capacity.  We'll\n+  \/\/ attempt to allocate a smaller TLAB.  If this is successful, we'll update actual_size() of our incoming\n+  \/\/ ShenandoahAllocRequest.  If the recursive request fails, we'll simply return nullptr.\n+\n+  \/\/ Note that we've relinquished the HeapLock and some other thread may perform additional allocation before our recursive\n+  \/\/ call reacquires the lock.  If that happens, we will need another recursive call to further reduce the size of our request\n+  \/\/ for each time another thread allocates young memory during the brief intervals that the heap lock is available to\n+  \/\/ interfering threads.  We expect this interference to be rare.  The recursion bottoms out when young_available is\n+  \/\/ smaller than req.min_size().  The inner-nested call to allocate_memory_under_lock() uses the same min_size() value\n+  \/\/ as this call, but it uses a preferred size() that is smaller than our preferred size, and is no larger than what we most\n+  \/\/ recently saw as the memory currently available within the young generation.\n+\n+  \/\/ TODO: At the expense of code clarity, we could rewrite this recursive solution to use iteration.  We need at most one\n+  \/\/ extra instance of the ShenandoahAllocRequest, which we can re-initialize multiple times inside a loop, with one iteration\n+  \/\/ of the loop required for each time the existing solution would recurse.  An iterative solution would be more efficient\n+  \/\/ in CPU time and stack memory utilization.  The expectation is that it is very rare that we would recurse more than once\n+  \/\/ so making this change is not currently seen as a high priority.\n+\n+  ShenandoahAllocRequest smaller_req = ShenandoahAllocRequest::for_tlab(req.min_size(), smaller_lab_size);\n+\n+  \/\/ Note that shrinking the preferred size gets us past the gatekeeper that checks whether there's available memory to\n+  \/\/ satisfy the allocation request.  The reality is the actual TLAB size is likely to be even smaller, because it will\n+  \/\/ depend on how much memory is available within mutator regions that are not yet fully used.\n+  HeapWord* result = allocate_memory_under_lock(smaller_req, in_new_region, is_promotion);\n+  if (result != nullptr) {\n+    req.set_actual_size(smaller_req.actual_size());\n+  }\n+  return result;\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -1017,1 +1684,1 @@\n-  return allocate_memory(req);\n+  return allocate_memory(req, false);\n@@ -1026,2 +1693,2 @@\n-  if (heuristics()->can_unload_classes()) {\n-    ShenandoahHeuristics* h = heuristics();\n+  ShenandoahHeuristics* h = global_generation()->heuristics();\n+  if (h->can_unload_classes()) {\n@@ -1106,0 +1773,1 @@\n+\n@@ -1111,0 +1779,90 @@\n+      if (_sh->check_cancelled_gc_and_yield(_concurrent)) {\n+        break;\n+      }\n+    }\n+  }\n+};\n+\n+\/\/ Unlike ShenandoahEvacuationTask, this iterates over all regions rather than just the collection set.\n+\/\/ This is needed in order to promote humongous start regions if age() >= tenure threshold.\n+class ShenandoahGenerationalEvacuationTask : public WorkerTask {\n+private:\n+  ShenandoahHeap* const _sh;\n+  ShenandoahRegionIterator *_regions;\n+  bool _concurrent;\n+  uint _tenuring_threshold;\n+\n+public:\n+  ShenandoahGenerationalEvacuationTask(ShenandoahHeap* sh,\n+                                       ShenandoahRegionIterator* iterator,\n+                                       bool concurrent) :\n+    WorkerTask(\"Shenandoah Evacuation\"),\n+    _sh(sh),\n+    _regions(iterator),\n+    _concurrent(concurrent),\n+    _tenuring_threshold(0)\n+  {\n+    if (_sh->mode()->is_generational()) {\n+      _tenuring_threshold = _sh->age_census()->tenuring_threshold();\n+    }\n+  }\n+\n+  void work(uint worker_id) {\n+    if (_concurrent) {\n+      ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+      ShenandoahSuspendibleThreadSetJoiner stsj;\n+      ShenandoahEvacOOMScope oom_evac_scope;\n+      do_work();\n+    } else {\n+      ShenandoahParallelWorkerSession worker_session(worker_id);\n+      ShenandoahEvacOOMScope oom_evac_scope;\n+      do_work();\n+    }\n+  }\n+\n+private:\n+  void do_work() {\n+    ShenandoahConcurrentEvacuateRegionObjectClosure cl(_sh);\n+    ShenandoahHeapRegion* r;\n+    ShenandoahMarkingContext* const ctx = ShenandoahHeap::heap()->marking_context();\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    size_t old_garbage_threshold = (region_size_bytes * ShenandoahOldGarbageThreshold) \/ 100;\n+    while ((r = _regions->next()) != nullptr) {\n+      log_debug(gc)(\"GenerationalEvacuationTask do_work(), looking at %s region \" SIZE_FORMAT \", (age: %d) [%s, %s, %s]\",\n+                    r->is_old()? \"old\": r->is_young()? \"young\": \"free\", r->index(), r->age(),\n+                    r->is_active()? \"active\": \"inactive\",\n+                    r->is_humongous()? (r->is_humongous_start()? \"humongous_start\": \"humongous_continuation\"): \"regular\",\n+                    r->is_cset()? \"cset\": \"not-cset\");\n+\n+      if (r->is_cset()) {\n+        assert(r->has_live(), \"Region \" SIZE_FORMAT \" should have been reclaimed early\", r->index());\n+        _sh->marked_object_iterate(r, &cl);\n+        if (ShenandoahPacing) {\n+          _sh->pacer()->report_evac(r->used() >> LogHeapWordSize);\n+        }\n+      } else if (r->is_young() && r->is_active() && (r->age() >= _tenuring_threshold)) {\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        if (r->is_humongous_start()) {\n+          \/\/ We promote humongous_start regions along with their affiliated continuations during evacuation rather than\n+          \/\/ doing this work during a safepoint.  We cannot put humongous regions into the collection set because that\n+          \/\/ triggers the load-reference barrier (LRB) to copy on reference fetch.\n+          r->promote_humongous();\n+        } else if (r->is_regular() && (r->get_top_before_promote() != nullptr)) {\n+          assert(r->garbage_before_padded_for_promote() < old_garbage_threshold,\n+                 \"Region \" SIZE_FORMAT \" has too much garbage for promotion\", r->index());\n+          assert(r->get_top_before_promote() == tams,\n+                 \"Region \" SIZE_FORMAT \" has been used for allocations before promotion\", r->index());\n+          \/\/ Likewise, we cannot put promote-in-place regions into the collection set because that would also trigger\n+          \/\/ the LRB to copy on reference fetch.\n+          r->promote_in_place();\n+        }\n+        \/\/ Aged humongous continuation regions are handled with their start region.  If an aged regular region has\n+        \/\/ more garbage than ShenandoahOldGarbageTrheshold, we'll promote by evacuation.  If there is room for evacuation\n+        \/\/ in this cycle, the region will be in the collection set.  If there is not room, the region will be promoted\n+        \/\/ by evacuation in some future GC cycle.\n+\n+        \/\/ If an aged regular region has received allocations during the current cycle, we do not promote because the\n+        \/\/ newly allocated objects do not have appropriate age; this region's age will be reset to zero at end of cycle.\n+      }\n+      \/\/ else, region is free, or OLD, or not in collection set, or humongous_continuation,\n+      \/\/ or is young humongous_start that is too young to be promoted\n@@ -1120,2 +1878,8 @@\n-  ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n-  workers()->run_task(&task);\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahRegionIterator regions;\n+    ShenandoahGenerationalEvacuationTask task(this, &regions, concurrent);\n+    workers()->run_task(&task);\n+  } else {\n+    ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n+    workers()->run_task(&task);\n+  }\n@@ -1151,1 +1915,1 @@\n-void ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n+size_t ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n@@ -1171,0 +1935,1 @@\n+  return required_regions;\n@@ -1180,0 +1945,4 @@\n+\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+    assert(plab->words_remaining() == 0, \"PLAB should not need retirement\");\n@@ -1195,0 +1964,11 @@\n+\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+\n+    \/\/ There are two reasons to retire all plabs between old-gen evacuation passes.\n+    \/\/  1. We need to make the plab memory parsable by remembered-set scanning.\n+    \/\/  2. We need to establish a trustworthy UpdateWaterMark value within each old-gen heap region\n+    ShenandoahHeap::heap()->retire_plab(plab, thread);\n+    if (_resize && ShenandoahThreadLocalData::plab_size(thread) > 0) {\n+      ShenandoahThreadLocalData::set_plab_size(thread, 0);\n+    }\n@@ -1254,3 +2034,6 @@\n-  \/\/ Return the max allowed size, and let the allocation path\n-  \/\/ figure out the safe size for current allocation.\n-  return ShenandoahHeapRegion::max_tlab_size_bytes();\n+  if (mode()->is_generational()) {\n+    return MIN2(ShenandoahHeapRegion::max_tlab_size_bytes(), young_generation()->available());\n+  } else {\n+    \/\/ Return the max allowed size, and let the allocation path figure out the safe size for current allocation.\n+    return ShenandoahHeapRegion::max_tlab_size_bytes();\n+  }\n@@ -1296,1 +2079,8 @@\n-  tcl->do_thread(_control_thread);\n+  if (_shenandoah_policy->is_at_shutdown()) {\n+    return;\n+  }\n+\n+  if (_control_thread != nullptr) {\n+    tcl->do_thread(_control_thread);\n+  }\n+\n@@ -1316,0 +2106,4 @@\n+    ls.cr();\n+\n+    evac_tracker()->print_global_on(&ls);\n+\n@@ -1321,0 +2115,19 @@\n+void ShenandoahHeap::on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation) {\n+  shenandoah_policy()->record_collection_cause(cause);\n+\n+  set_gc_cause(cause);\n+  set_gc_generation(generation);\n+\n+  generation->heuristics()->record_cycle_start();\n+}\n+\n+void ShenandoahHeap::on_cycle_end(ShenandoahGeneration* generation) {\n+  generation->heuristics()->record_cycle_end();\n+  if (mode()->is_generational() && generation->is_global()) {\n+    \/\/ If we just completed a GLOBAL GC, claim credit for completion of young-gen and old-gen GC as well\n+    young_generation()->heuristics()->record_cycle_end();\n+    old_generation()->heuristics()->record_cycle_end();\n+  }\n+  set_gc_cause(GCCause::_no_gc);\n+}\n+\n@@ -1642,23 +2455,0 @@\n-class ShenandoahInitMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahInitMarkUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-    if (r->is_active()) {\n-      \/\/ Check if region needs updating its TAMS. We have updated it already during concurrent\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n-      if (_ctx->top_at_mark_start(r) != r->top()) {\n-        _ctx->capture_top_at_mark_start(r);\n-      }\n-    } else {\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should already have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n@@ -1680,99 +2470,0 @@\n-class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n-      \/\/ anyway to capture any updates that happened since now.\n-      r->clear_live_data();\n-      _ctx->capture_top_at_mark_start(r);\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_gc() {\n-  reset_mark_bitmap();\n-\n-  ShenandoahResetUpdateRegionStateClosure cl;\n-  parallel_heap_region_iterate(&cl);\n-}\n-\n-class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalMarkUpdateRegionStateClosure() :\n-    _ctx(ShenandoahHeap::heap()->complete_marking_context()), _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n-      \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n-      HeapWord *tams = _ctx->top_at_mark_start(r);\n-      HeapWord *top = r->top();\n-      if (top > tams) {\n-        r->increase_live_data_alloc_words(pointer_delta(top, tams));\n-      }\n-\n-      \/\/ We are about to select the collection set, make sure it knows about\n-      \/\/ current pinning status. Also, this allows trashing more regions that\n-      \/\/ now have their pinning status dropped.\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n-      }\n-\n-      \/\/ Remember limit for updating refs. It's guaranteed that we get no\n-      \/\/ from-space-refs written from here on.\n-      r->set_update_watermark_at_safepoint(r->top());\n-    } else {\n-      assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_regions_and_collection_set(bool concurrent) {\n-  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states :\n-                                         ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n-    ShenandoahFinalMarkUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n-\n-    assert_pinned_region_status();\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset :\n-                                         ShenandoahPhaseTimings::degen_gc_choose_cset);\n-    ShenandoahHeapLocker locker(lock());\n-    _collection_set->clear();\n-    heuristics()->choose_collection_set(_collection_set);\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset :\n-                                         ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n-  }\n-}\n-\n@@ -1789,1 +2480,1 @@\n-  ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n+  active_generation()->ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n@@ -1825,4 +2516,60 @@\n-void ShenandoahHeap::set_concurrent_mark_in_progress(bool in_progress) {\n-  assert(!has_forwarded_objects(), \"Not expected before\/after mark phase\");\n-  set_gc_state(MARKING, in_progress);\n-  ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(in_progress, !in_progress);\n+void ShenandoahHeap::set_evacuation_reserve_quantities(bool is_valid) {\n+  _has_evacuation_reserve_quantities = is_valid;\n+}\n+\n+void ShenandoahHeap::set_concurrent_young_mark_in_progress(bool in_progress) {\n+  uint mask;\n+  assert(!has_forwarded_objects(), \"Young marking is not concurrent with evacuation\");\n+  if (!in_progress && is_concurrent_old_mark_in_progress()) {\n+    assert(mode()->is_generational(), \"Only generational GC has old marking\");\n+    assert(_gc_state.is_set(MARKING), \"concurrent_old_marking_in_progress implies MARKING\");\n+    \/\/ If old-marking is in progress when we turn off YOUNG_MARKING, leave MARKING (and OLD_MARKING) on\n+    mask = YOUNG_MARKING;\n+  } else {\n+    mask = MARKING | YOUNG_MARKING;\n+  }\n+  set_gc_state(mask, in_progress);\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::set_concurrent_old_mark_in_progress(bool in_progress) {\n+#ifdef ASSERT\n+  \/\/ has_forwarded_objects() iff UPDATEREFS or EVACUATION\n+  bool has_forwarded = has_forwarded_objects();\n+  bool updating_or_evacuating = _gc_state.is_set(UPDATEREFS | EVACUATION);\n+  bool evacuating = _gc_state.is_set(EVACUATION);\n+  assert ((has_forwarded == updating_or_evacuating) || (evacuating && !has_forwarded && collection_set()->is_empty()),\n+          \"Updating or evacuating iff has forwarded objects, or if evacuation phase is promoting in place without forwarding\");\n+#endif\n+  if (!in_progress && is_concurrent_young_mark_in_progress()) {\n+    \/\/ If young-marking is in progress when we turn off OLD_MARKING, leave MARKING (and YOUNG_MARKING) on\n+    assert(_gc_state.is_set(MARKING), \"concurrent_young_marking_in_progress implies MARKING\");\n+    set_gc_state(OLD_MARKING, in_progress);\n+  } else {\n+    set_gc_state(MARKING | OLD_MARKING, in_progress);\n+  }\n+  manage_satb_barrier(in_progress);\n+}\n+\n+bool ShenandoahHeap::is_prepare_for_old_mark_in_progress() const {\n+  return old_generation()->state() == ShenandoahOldGeneration::FILLING;\n+}\n+\n+void ShenandoahHeap::set_aging_cycle(bool in_progress) {\n+  _is_aging_cycle.set_cond(in_progress);\n+}\n+\n+void ShenandoahHeap::manage_satb_barrier(bool active) {\n+  if (is_concurrent_mark_in_progress()) {\n+    \/\/ Ignore request to deactivate barrier while concurrent mark is in progress.\n+    \/\/ Do not attempt to re-activate the barrier if it is already active.\n+    if (active && !ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  } else {\n+    \/\/ No concurrent marking is in progress so honor request to deactivate,\n+    \/\/ but only if the barrier is already active.\n+    if (!active && ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  }\n@@ -1861,0 +2608,8 @@\n+void ShenandoahHeap::cancel_concurrent_mark() {\n+  _young_generation->cancel_marking();\n+  _old_generation->cancel_marking();\n+  _global_generation->cancel_marking();\n+\n+  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+}\n+\n@@ -1866,0 +2621,1 @@\n+    _cancel_requested_time = os::elapsedTime();\n@@ -1876,1 +2632,1 @@\n-  \/\/ Step 0. Notify policy to disable event recording.\n+  \/\/ Step 1. Notify policy to disable event recording and prevent visiting gc threads during shutdown\n@@ -1879,1 +2635,1 @@\n-  \/\/ Step 1. Notify control thread that we are in shutdown.\n+  \/\/ Step 2. Notify control thread that we are in shutdown.\n@@ -1884,1 +2640,1 @@\n-  \/\/ Step 2. Notify GC workers that we are cancelling GC.\n+  \/\/ Step 3. Notify GC workers that we are cancelling GC.\n@@ -1887,1 +2643,1 @@\n-  \/\/ Step 3. Wait until GC worker exits normally.\n+  \/\/ Step 4. Wait until GC worker exits normally.\n@@ -1989,4 +2745,0 @@\n-size_t ShenandoahHeap::bytes_allocated_since_gc_start() const {\n-  return Atomic::load(&_bytes_allocated_since_gc_start);\n-}\n-\n@@ -1994,1 +2746,6 @@\n-  Atomic::store(&_bytes_allocated_since_gc_start, (size_t)0);\n+  if (mode()->is_generational()) {\n+    young_generation()->reset_bytes_allocated_since_gc_start();\n+    old_generation()->reset_bytes_allocated_since_gc_start();\n+  }\n+\n+  global_generation()->reset_bytes_allocated_since_gc_start();\n@@ -2058,2 +2815,4 @@\n-    assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n-           \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    if (active_generation()->contains(r)) {\n+      assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n+             \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    }\n@@ -2113,0 +2872,2 @@\n+  ShenandoahRegionChunkIterator* _work_chunks;\n+\n@@ -2114,1 +2875,2 @@\n-  ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n+  explicit ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions,\n+                                        ShenandoahRegionChunkIterator* work_chunks) :\n@@ -2117,1 +2879,4 @@\n-    _regions(regions) {\n+    _regions(regions),\n+    _work_chunks(work_chunks)\n+  {\n+    log_info(gc, remset)(\"Scan remembered set using bitmap: %s\", BOOL_TO_STR(_heap->is_old_bitmap_stable()));\n@@ -2124,1 +2889,1 @@\n-      do_work<ShenandoahConcUpdateRefsClosure>();\n+      do_work<ShenandoahConcUpdateRefsClosure>(worker_id);\n@@ -2127,1 +2892,1 @@\n-      do_work<ShenandoahSTWUpdateRefsClosure>();\n+      do_work<ShenandoahSTWUpdateRefsClosure>(worker_id);\n@@ -2133,1 +2898,1 @@\n-  void do_work() {\n+  void do_work(uint worker_id) {\n@@ -2135,0 +2900,10 @@\n+    if (CONCURRENT && (worker_id == 0)) {\n+      \/\/ We ask the first worker to replenish the Mutator free set by moving regions previously reserved to hold the\n+      \/\/ results of evacuation.  These reserves are no longer necessary because evacuation has completed.\n+      size_t cset_regions = _heap->collection_set()->count();\n+      \/\/ We cannot transfer any more regions than will be reclaimed when the existing collection set is recycled, because\n+      \/\/ we need the reclaimed collection set regions to replenish the collector reserves\n+      _heap->free_set()->move_collector_sets_to_mutator(cset_regions);\n+    }\n+    \/\/ If !CONCURRENT, there's no value in expanding Mutator free set\n+\n@@ -2136,1 +2911,4 @@\n-    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+    \/\/ We update references for global, old, and young collections.\n+    assert(_heap->active_generation()->is_mark_complete(), \"Expected complete marking\");\n+    ShenandoahMarkingContext* const ctx = _heap->marking_context();\n+    bool is_mixed = _heap->collection_set()->has_old_regions();\n@@ -2140,0 +2918,3 @@\n+\n+      log_debug(gc)(\"ShenandoahUpdateHeapRefsTask::do_work(%u) looking at region \" SIZE_FORMAT, worker_id, r->index());\n+      bool region_progress = false;\n@@ -2141,1 +2922,31 @@\n-        _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+        if (!_heap->mode()->is_generational() || r->is_young()) {\n+          _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+          region_progress = true;\n+        } else if (r->is_old()) {\n+          if (_heap->active_generation()->is_global()) {\n+            \/\/ Note that GLOBAL collection is not as effectively balanced as young and mixed cycles.  This is because\n+            \/\/ concurrent GC threads are parceled out entire heap regions of work at a time and there\n+            \/\/ is no \"catchup phase\" consisting of remembered set scanning, during which parcels of work are smaller\n+            \/\/ and more easily distributed more fairly across threads.\n+\n+            \/\/ TODO: Consider an improvement to load balance GLOBAL GC.\n+            _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+            region_progress = true;\n+          }\n+          \/\/ Otherwise, this is an old region in a young or mixed cycle.  Process it during a second phase, below.\n+          \/\/ Don't bother to report pacing progress in this case.\n+        } else {\n+          \/\/ Because updating of references runs concurrently, it is possible that a FREE inactive region transitions\n+          \/\/ to a non-free active region while this loop is executing.  Whenever this happens, the changing of a region's\n+          \/\/ active status may propagate at a different speed than the changing of the region's affiliation.\n+\n+          \/\/ When we reach this control point, it is because a race has allowed a region's is_active() status to be seen\n+          \/\/ by this thread before the region's affiliation() is seen by this thread.\n+\n+          \/\/ It's ok for this race to occur because the newly transformed region does not have any references to be\n+          \/\/ updated.\n+\n+          assert(r->get_update_watermark() == r->bottom(),\n+                 \"%s Region \" SIZE_FORMAT \" is_active but not recognized as YOUNG or OLD so must be newly transitioned from FREE\",\n+                 r->affiliation_name(), r->index());\n+        }\n@@ -2143,1 +2954,1 @@\n-      if (ShenandoahPacing) {\n+      if (region_progress && ShenandoahPacing) {\n@@ -2151,0 +2962,114 @@\n+\n+    if (_heap->mode()->is_generational() && !_heap->active_generation()->is_global()) {\n+      \/\/ Since this is generational and not GLOBAL, we have to process the remembered set.  There's no remembered\n+      \/\/ set processing if not in generational mode or if GLOBAL mode.\n+\n+      \/\/ After this thread has exhausted its traditional update-refs work, it continues with updating refs within remembered set.\n+      \/\/ The remembered set workload is better balanced between threads, so threads that are \"behind\" can catch up with other\n+      \/\/ threads during this phase, allowing all threads to work more effectively in parallel.\n+      struct ShenandoahRegionChunk assignment;\n+      RememberedScanner* scanner = _heap->card_scan();\n+\n+      while (!_heap->check_cancelled_gc_and_yield(CONCURRENT) && _work_chunks->next(&assignment)) {\n+        \/\/ Keep grabbing next work chunk to process until finished, or asked to yield\n+        ShenandoahHeapRegion* r = assignment._r;\n+        if (r->is_active() && !r->is_cset() && r->is_old()) {\n+          HeapWord* start_of_range = r->bottom() + assignment._chunk_offset;\n+          HeapWord* end_of_range = r->get_update_watermark();\n+          if (end_of_range > start_of_range + assignment._chunk_size) {\n+            end_of_range = start_of_range + assignment._chunk_size;\n+          }\n+\n+          \/\/ Old region in a young cycle or mixed cycle.\n+          if (is_mixed) {\n+            \/\/ TODO: For mixed evac, consider building an old-gen remembered set that allows restricted updating\n+            \/\/ within old-gen HeapRegions.  This remembered set can be constructed by old-gen concurrent marking\n+            \/\/ and augmented by card marking.  For example, old-gen concurrent marking can remember for each old-gen\n+            \/\/ card which other old-gen regions it refers to: none, one-other specifically, multiple-other non-specific.\n+            \/\/ Update-references when _mixed_evac processess each old-gen memory range that has a traditional DIRTY\n+            \/\/ card or if the \"old-gen remembered set\" indicates that this card holds pointers specifically to an\n+            \/\/ old-gen region in the most recent collection set, or if this card holds pointers to other non-specific\n+            \/\/ old-gen heap regions.\n+\n+            if (r->is_humongous()) {\n+              if (start_of_range < end_of_range) {\n+                \/\/ Need to examine both dirty and clean cards during mixed evac.\n+                r->oop_iterate_humongous_slice(&cl, false, start_of_range, assignment._chunk_size, true);\n+              }\n+            } else {\n+              \/\/ Since this is mixed evacuation, old regions that are candidates for collection have not been coalesced\n+              \/\/ and filled.  Use mark bits to find objects that need to be updated.\n+              \/\/\n+              \/\/ Future TODO: establish a second remembered set to identify which old-gen regions point to other old-gen\n+              \/\/ regions which are in the collection set for a particular mixed evacuation.\n+              if (start_of_range < end_of_range) {\n+                HeapWord* p = nullptr;\n+                size_t card_index = scanner->card_index_for_addr(start_of_range);\n+                \/\/ In case last object in my range spans boundary of my chunk, I may need to scan all the way to top()\n+                ShenandoahObjectToOopBoundedClosure<T> objs(&cl, start_of_range, r->top());\n+\n+                \/\/ Any object that begins in a previous range is part of a different scanning assignment.  Any object that\n+                \/\/ starts after end_of_range is also not my responsibility.  (Either allocated during evacuation, so does\n+                \/\/ not hold pointers to from-space, or is beyond the range of my assigned work chunk.)\n+\n+                \/\/ Find the first object that begins in my range, if there is one.\n+                p = start_of_range;\n+                oop obj = cast_to_oop(p);\n+                HeapWord* tams = ctx->top_at_mark_start(r);\n+                if (p >= tams) {\n+                  \/\/ We cannot use ctx->is_marked(obj) to test whether an object begins at this address.  Instead,\n+                  \/\/ we need to use the remembered set crossing map to advance p to the first object that starts\n+                  \/\/ within the enclosing card.\n+\n+                  while (true) {\n+                    HeapWord* first_object = scanner->first_object_in_card(card_index);\n+                    if (first_object != nullptr) {\n+                      p = first_object;\n+                      break;\n+                    } else if (scanner->addr_for_card_index(card_index + 1) < end_of_range) {\n+                      card_index++;\n+                    } else {\n+                      \/\/ Force the loop that follows to immediately terminate.\n+                      p = end_of_range;\n+                      break;\n+                    }\n+                  }\n+                  obj = cast_to_oop(p);\n+                  \/\/ Note: p may be >= end_of_range\n+                } else if (!ctx->is_marked(obj)) {\n+                  p = ctx->get_next_marked_addr(p, tams);\n+                  obj = cast_to_oop(p);\n+                  \/\/ If there are no more marked objects before tams, this returns tams.\n+                  \/\/ Note that tams is either >= end_of_range, or tams is the start of an object that is marked.\n+                }\n+                while (p < end_of_range) {\n+                  \/\/ p is known to point to the beginning of marked object obj\n+                  objs.do_object(obj);\n+                  HeapWord* prev_p = p;\n+                  p += obj->size();\n+                  if (p < tams) {\n+                    p = ctx->get_next_marked_addr(p, tams);\n+                    \/\/ If there are no more marked objects before tams, this returns tams.  Note that tams is\n+                    \/\/ either >= end_of_range, or tams is the start of an object that is marked.\n+                  }\n+                  assert(p != prev_p, \"Lack of forward progress\");\n+                  obj = cast_to_oop(p);\n+                }\n+              }\n+            }\n+          } else {\n+            \/\/ This is a young evac..\n+            if (start_of_range < end_of_range) {\n+              size_t cluster_size =\n+                CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+              size_t clusters = assignment._chunk_size \/ cluster_size;\n+              assert(clusters * cluster_size == assignment._chunk_size, \"Chunk assignment must align on cluster boundaries\");\n+              scanner->process_region_slice(r, assignment._chunk_offset, clusters, end_of_range, &cl, true, worker_id);\n+            }\n+          }\n+          if (ShenandoahPacing && (start_of_range < end_of_range)) {\n+            _heap->pacer()->report_updaterefs(pointer_delta(end_of_range, start_of_range));\n+          }\n+        }\n+      }\n+    }\n@@ -2156,0 +3081,2 @@\n+  uint nworkers = workers()->active_workers();\n+  ShenandoahRegionChunkIterator work_list(nworkers);\n@@ -2158,1 +3085,1 @@\n-    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator, &work_list);\n@@ -2161,1 +3088,1 @@\n-    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator, &work_list);\n@@ -2164,0 +3091,3 @@\n+  if (ShenandoahEnableCardStats && card_scan()!=nullptr) { \/\/ generational check proxy\n+    card_scan()->log_card_stats(nworkers, CARD_STAT_UPDATE_REFS);\n+  }\n@@ -2166,1 +3096,0 @@\n-\n@@ -2169,0 +3098,1 @@\n+  ShenandoahMarkingContext* _ctx;\n@@ -2170,0 +3100,1 @@\n+  bool _is_generational;\n@@ -2172,1 +3103,3 @@\n-  ShenandoahFinalUpdateRefsUpdateRegionStateClosure() : _lock(ShenandoahHeap::heap()->lock()) {}\n+  ShenandoahFinalUpdateRefsUpdateRegionStateClosure(\n+    ShenandoahMarkingContext* ctx) : _ctx(ctx), _lock(ShenandoahHeap::heap()->lock()),\n+                                     _is_generational(ShenandoahHeap::heap()->mode()->is_generational()) { }\n@@ -2175,0 +3108,21 @@\n+\n+    \/\/ Maintenance of region age must follow evacuation in order to account for evacuation allocations within survivor\n+    \/\/ regions.  We consult region age during the subsequent evacuation to determine whether certain objects need to\n+    \/\/ be promoted.\n+    if (_is_generational && r->is_young() && r->is_active()) {\n+      HeapWord *tams = _ctx->top_at_mark_start(r);\n+      HeapWord *top = r->top();\n+\n+      \/\/ Allocations move the watermark when top moves.  However compacting\n+      \/\/ objects will sometimes lower top beneath the watermark, after which,\n+      \/\/ attempts to read the watermark will assert out (watermark should not be\n+      \/\/ higher than top).\n+      if (top > tams) {\n+        \/\/ There have been allocations in this region since the start of the cycle.\n+        \/\/ Any objects new to this region must not assimilate elevated age.\n+        r->reset_age();\n+      } else if (ShenandoahHeap::heap()->is_aging_cycle()) {\n+        r->increment_age();\n+      }\n+    }\n+\n@@ -2177,1 +3131,0 @@\n-\n@@ -2204,1 +3157,1 @@\n-    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl;\n+    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl (active_generation()->complete_marking_context());\n@@ -2219,6 +3172,84 @@\n-  {\n-    ShenandoahGCPhase phase(concurrent ?\n-                            ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n-                            ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n+  ShenandoahGCPhase phase(concurrent ?\n+                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n+                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  ShenandoahHeapLocker locker(lock());\n+  size_t young_cset_regions, old_cset_regions;\n+  size_t first_old_region, last_old_region, old_region_count;\n+  _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+  \/\/ If there are no old regions, first_old_region will be greater than last_old_region\n+  assert((first_old_region > last_old_region) ||\n+         ((last_old_region + 1 - first_old_region >= old_region_count) &&\n+          get_region(first_old_region)->is_old() && get_region(last_old_region)->is_old()),\n+         \"sanity: old_region_count: \" SIZE_FORMAT \", first_old_region: \" SIZE_FORMAT \", last_old_region: \" SIZE_FORMAT,\n+         old_region_count, first_old_region, last_old_region);\n+\n+  if (mode()->is_generational()) {\n+    assert(verify_generation_usage(true, old_generation()->used_regions(),\n+                                   old_generation()->used(), old_generation()->get_humongous_waste(),\n+                                   true, young_generation()->used_regions(),\n+                                   young_generation()->used(), young_generation()->get_humongous_waste()),\n+           \"Generation accounts are inaccurate\");\n+\n+    \/\/ The computation of bytes_of_allocation_runway_before_gc_trigger is quite conservative so consider all of this\n+    \/\/ available for transfer to old. Note that transfer of humongous regions does not impact available.\n+    size_t allocation_runway = young_heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions);\n+    compute_old_generation_balance(allocation_runway, old_cset_regions);\n+\n+    \/\/ Total old_available may have been expanded to hold anticipated promotions.  We trigger if the fragmented available\n+    \/\/ memory represents more than 16 regions worth of data.  Note that fragmentation may increase when we promote regular\n+    \/\/ regions in place when many of these regular regions have an abundant amount of available memory within them.  Fragmentation\n+    \/\/ will decrease as promote-by-copy consumes the available memory within these partially consumed regions.\n+    \/\/\n+    \/\/ We consider old-gen to have excessive fragmentation if more than 12.5% of old-gen is free memory that resides\n+    \/\/ within partially consumed regions of memory.\n+  }\n+  \/\/ Rebuild free set based on adjusted generation sizes.\n+  _free_set->rebuild(young_cset_regions, old_cset_regions);\n+\n+  if (mode()->is_generational() && (ShenandoahGenerationalHumongousReserve > 0)) {\n+    size_t old_region_span = (first_old_region <= last_old_region)? (last_old_region + 1 - first_old_region): 0;\n+    size_t allowed_old_gen_span = num_regions() - (ShenandoahGenerationalHumongousReserve * num_regions() \/ 100);\n+\n+    \/\/ Tolerate lower density if total span is small.  Here's the implementation:\n+    \/\/   if old_gen spans more than 100% and density < 75%, trigger old-defrag\n+    \/\/   else if old_gen spans more than 87.5% and density < 62.5%, trigger old-defrag\n+    \/\/   else if old_gen spans more than 75% and density < 50%, trigger old-defrag\n+    \/\/   else if old_gen spans more than 62.5% and density < 37.5%, trigger old-defrag\n+    \/\/   else if old_gen spans more than 50% and density < 25%, trigger old-defrag\n+    \/\/\n+    \/\/ A previous implementation was more aggressive in triggering, resulting in degraded throughput when\n+    \/\/ humongous allocation was not required.\n+\n+    ShenandoahGeneration* old_gen = old_generation();\n+    size_t old_available = old_gen->available();\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    size_t old_unaffiliated_available = old_gen->free_unaffiliated_regions() * region_size_bytes;\n+    assert(old_available >= old_unaffiliated_available, \"sanity\");\n+    size_t old_fragmented_available = old_available - old_unaffiliated_available;\n+\n+    size_t old_bytes_consumed = old_region_count * region_size_bytes - old_fragmented_available;\n+    size_t old_bytes_spanned = old_region_span * region_size_bytes;\n+    double old_density = ((double) old_bytes_consumed) \/ old_bytes_spanned;\n+\n+    uint eighths = 8;\n+    for (uint i = 0; i < 5; i++) {\n+      size_t span_threshold = eighths * allowed_old_gen_span \/ 8;\n+      double density_threshold = (eighths - 2) \/ 8.0;\n+      if ((old_region_span >= span_threshold) && (old_density < density_threshold)) {\n+        old_heuristics()->trigger_old_is_fragmented(old_density, first_old_region, last_old_region);\n+        break;\n+      }\n+      eighths--;\n+    }\n+\n+    size_t old_used = old_generation()->used() + old_generation()->get_humongous_waste();\n+    size_t trigger_threshold = old_generation()->usage_trigger_threshold();\n+    \/\/ Detects unsigned arithmetic underflow\n+    assert(old_used <= capacity(),\n+           \"Old used (\" SIZE_FORMAT \", \" SIZE_FORMAT\") must not be more than heap capacity (\" SIZE_FORMAT \")\",\n+           old_generation()->used(), old_generation()->get_humongous_waste(), capacity());\n+\n+    if (old_used > trigger_threshold) {\n+      old_heuristics()->trigger_old_has_grown();\n+    }\n@@ -2327,3 +3358,12 @@\n-  _memory_pool = new ShenandoahMemoryPool(this);\n-  _cycle_memory_manager.add_pool(_memory_pool);\n-  _stw_memory_manager.add_pool(_memory_pool);\n+  if (mode()->is_generational()) {\n+    _young_gen_memory_pool = new ShenandoahYoungGenMemoryPool(this);\n+    _old_gen_memory_pool = new ShenandoahOldGenMemoryPool(this);\n+    _cycle_memory_manager.add_pool(_young_gen_memory_pool);\n+    _cycle_memory_manager.add_pool(_old_gen_memory_pool);\n+    _stw_memory_manager.add_pool(_young_gen_memory_pool);\n+    _stw_memory_manager.add_pool(_old_gen_memory_pool);\n+  } else {\n+    _memory_pool = new ShenandoahMemoryPool(this);\n+    _cycle_memory_manager.add_pool(_memory_pool);\n+    _stw_memory_manager.add_pool(_memory_pool);\n+  }\n@@ -2341,1 +3381,6 @@\n-  memory_pools.append(_memory_pool);\n+  if (mode()->is_generational()) {\n+    memory_pools.append(_young_gen_memory_pool);\n+    memory_pools.append(_old_gen_memory_pool);\n+  } else {\n+    memory_pools.append(_memory_pool);\n+  }\n@@ -2346,1 +3391,1 @@\n-  return _memory_pool->get_memory_usage();\n+  return MemoryUsage(_initial_size, used(), committed(), max_capacity());\n@@ -2384,0 +3429,1 @@\n+\n@@ -2411,0 +3457,103 @@\n+\n+void ShenandoahHeap::transfer_old_pointers_from_satb() {\n+  _old_generation->transfer_pointers_from_satb();\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<YOUNG>::heap_region_do(ShenandoahHeapRegion* region) {\n+  \/\/ Visit young regions\n+  if (region->is_young()) {\n+    _cl->heap_region_do(region);\n+  }\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<OLD>::heap_region_do(ShenandoahHeapRegion* region) {\n+  \/\/ Visit old regions\n+  if (region->is_old()) {\n+    _cl->heap_region_do(region);\n+  }\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<GLOBAL>::heap_region_do(ShenandoahHeapRegion* region) {\n+  _cl->heap_region_do(region);\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<NON_GEN>::heap_region_do(ShenandoahHeapRegion* region) {\n+  _cl->heap_region_do(region);\n+}\n+\n+bool ShenandoahHeap::verify_generation_usage(bool verify_old, size_t old_regions, size_t old_bytes, size_t old_waste,\n+                                             bool verify_young, size_t young_regions, size_t young_bytes, size_t young_waste) {\n+  size_t tally_old_regions = 0;\n+  size_t tally_old_bytes = 0;\n+  size_t tally_old_waste = 0;\n+  size_t tally_young_regions = 0;\n+  size_t tally_young_bytes = 0;\n+  size_t tally_young_waste = 0;\n+\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  for (size_t i = 0; i < num_regions(); i++) {\n+    ShenandoahHeapRegion* r = get_region(i);\n+    if (r->is_old()) {\n+      tally_old_regions++;\n+      tally_old_bytes += r->used();\n+      if (r->is_humongous()) {\n+        ShenandoahHeapRegion* start = r->humongous_start_region();\n+        HeapWord* obj_addr = start->bottom();\n+        oop obj = cast_to_oop(obj_addr);\n+        size_t word_size = obj->size();\n+        HeapWord* end_addr = obj_addr + word_size;\n+        if (end_addr <= r->end()) {\n+          tally_old_waste += (r->end() - end_addr) * HeapWordSize;\n+        }\n+      }\n+    } else if (r->is_young()) {\n+      tally_young_regions++;\n+      tally_young_bytes += r->used();\n+      if (r->is_humongous()) {\n+        ShenandoahHeapRegion* start = r->humongous_start_region();\n+        HeapWord* obj_addr = start->bottom();\n+        oop obj = cast_to_oop(obj_addr);\n+        size_t word_size = obj->size();\n+        HeapWord* end_addr = obj_addr + word_size;\n+        if (end_addr <= r->end()) {\n+          tally_young_waste += (r->end() - end_addr) * HeapWordSize;\n+        }\n+      }\n+    }\n+  }\n+  if (verify_young &&\n+      ((young_regions != tally_young_regions) || (young_bytes != tally_young_bytes) || (young_waste != tally_young_waste))) {\n+    return false;\n+  } else if (verify_old &&\n+             ((old_regions != tally_old_regions) || (old_bytes != tally_old_bytes) || (old_waste != tally_old_waste))) {\n+    return false;\n+  } else {\n+    return true;\n+  }\n+}\n+\n+ShenandoahGeneration* ShenandoahHeap::generation_for(ShenandoahAffiliation affiliation) const {\n+  if (!mode()->is_generational()) {\n+    return global_generation();\n+  } else if (affiliation == YOUNG_GENERATION) {\n+    return young_generation();\n+  } else if (affiliation == OLD_GENERATION) {\n+    return old_generation();\n+  }\n+\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n+\n+void ShenandoahHeap::log_heap_status(const char* msg) const {\n+  if (mode()->is_generational()) {\n+    young_generation()->log_status(msg);\n+    old_generation()->log_status(msg);\n+  } else {\n+    global_generation()->log_status(msg);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":1437,"deletions":288,"binary":false,"changes":1725,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -30,0 +31,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -37,11 +39,0 @@\n-ShenandoahMarkRefsSuperClosure::ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q,  ShenandoahReferenceProcessor* rp) :\n-  MetadataVisitingOopIterateClosure(rp),\n-  _queue(q),\n-  _mark_context(ShenandoahHeap::heap()->marking_context()),\n-  _weak(false)\n-{ }\n-\n-ShenandoahMark::ShenandoahMark() :\n-  _task_queues(ShenandoahHeap::heap()->marking_context()->task_queues()) {\n-}\n-\n@@ -57,1 +48,3 @@\n-  CodeCache::on_gc_marking_cycle_finish();\n+  if (!ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress()) {\n+    CodeCache::on_gc_marking_cycle_finish();\n+  }\n@@ -60,4 +53,7 @@\n-void ShenandoahMark::clear() {\n-  \/\/ Clean up marking stacks.\n-  ShenandoahObjToScanQueueSet* queues = ShenandoahHeap::heap()->marking_context()->task_queues();\n-  queues->clear();\n+ShenandoahMarkRefsSuperClosure::ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q,  ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old_q) :\n+  MetadataVisitingOopIterateClosure(rp),\n+  _queue(q),\n+  _old_queue(old_q),\n+  _mark_context(ShenandoahHeap::heap()->marking_context()),\n+  _weak(false)\n+{ }\n@@ -65,2 +61,4 @@\n-  \/\/ Cancel SATB buffers.\n-  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+ShenandoahMark::ShenandoahMark(ShenandoahGeneration* generation) :\n+  _generation(generation),\n+  _task_queues(generation->task_queues()),\n+  _old_gen_task_queues(generation->old_gen_task_queues()) {\n@@ -70,0 +68,1 @@\n+<<<<<<< HEAD\n@@ -71,0 +70,3 @@\n+=======\n+void ShenandoahMark::mark_loop_prework(uint w, TaskTerminator *t, ShenandoahReferenceProcessor *rp, StringDedup::Requests* const req, bool update_refs) {\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -72,0 +74,1 @@\n+  ShenandoahObjToScanQueue* old_q = get_old_queue(w);\n@@ -78,0 +81,1 @@\n+<<<<<<< HEAD\n@@ -85,0 +89,9 @@\n+=======\n+  if (update_refs) {\n+    using Closure = ShenandoahMarkUpdateRefsClosure<GENERATION>;\n+    Closure cl(q, rp, old_q);\n+    mark_loop_work<Closure, GENERATION, CANCELLABLE, STRING_DEDUP>(&cl, ld, w, t, req);\n+  } else {\n+    using Closure = ShenandoahMarkRefsClosure<GENERATION>;\n+    Closure cl(q, rp, old_q);\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -94,0 +107,1 @@\n+<<<<<<< HEAD\n@@ -100,0 +114,25 @@\n+=======\n+  bool update_refs = ShenandoahHeap::heap()->has_forwarded_objects();\n+  switch (generation) {\n+    case YOUNG:\n+      mark_loop_prework<YOUNG, CANCELLABLE, STRING_DEDUP>(worker_id, terminator, rp, req, update_refs);\n+      break;\n+    case OLD:\n+      \/\/ Old generation collection only performs marking, it should not update references.\n+      mark_loop_prework<OLD, CANCELLABLE, STRING_DEDUP>(worker_id, terminator, rp, req, false);\n+      break;\n+    case GLOBAL:\n+      mark_loop_prework<GLOBAL, CANCELLABLE, STRING_DEDUP>(worker_id, terminator, rp, req, update_refs);\n+      break;\n+    case NON_GEN:\n+      mark_loop_prework<NON_GEN, CANCELLABLE, STRING_DEDUP>(worker_id, terminator, rp, req, update_refs);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+  }\n+}\n+\n+void ShenandoahMark::mark_loop(uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp,\n+                               ShenandoahGenerationType generation, bool cancellable, StringDedupMode dedup_mode, StringDedup::Requests* const req) {\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -136,1 +175,2 @@\n-  heap->ref_processor()->set_mark_closure(worker_id, cl);\n+  assert(heap->active_generation()->type() == GENERATION, \"Sanity\");\n+  heap->active_generation()->ref_processor()->set_mark_closure(worker_id, cl);\n@@ -156,0 +196,1 @@\n+<<<<<<< HEAD\n@@ -157,0 +198,3 @@\n+=======\n+        do_task<T, GENERATION, STRING_DEDUP>(q, cl, live_data, req, &t, worker_id);\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -165,0 +209,1 @@\n+  ShenandoahObjToScanQueue* old_q = get_old_queue(worker_id);\n@@ -166,0 +211,1 @@\n+<<<<<<< HEAD\n@@ -167,0 +213,3 @@\n+=======\n+  ShenandoahSATBBufferClosure<GENERATION> drain_satb(q, old_q);\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -176,1 +225,0 @@\n-\n@@ -185,0 +233,1 @@\n+<<<<<<< HEAD\n@@ -186,0 +235,3 @@\n+=======\n+        do_task<T, GENERATION, STRING_DEDUP>(q, cl, live_data, req, &t, worker_id);\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.cpp","additions":72,"deletions":20,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -28,0 +29,1 @@\n+#include \"gc\/shared\/ageTable.hpp\"\n@@ -32,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -48,0 +51,1 @@\n+  ShenandoahGeneration* const _generation;\n@@ -49,0 +53,1 @@\n+  ShenandoahObjToScanQueueSet* const _old_gen_task_queues;\n@@ -51,1 +56,1 @@\n-  ShenandoahMark();\n+  ShenandoahMark(ShenandoahGeneration* generation);\n@@ -55,0 +60,1 @@\n+<<<<<<< HEAD\n@@ -58,0 +64,3 @@\n+=======\n+  static inline void mark_through_ref(T* p, ShenandoahObjToScanQueue* q, ShenandoahObjToScanQueue* old_q, ShenandoahMarkingContext* const mark_context, bool weak);\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -65,0 +74,4 @@\n+  ShenandoahObjToScanQueueSet* old_task_queues() {\n+    return _old_gen_task_queues;\n+  }\n+\n@@ -66,0 +79,3 @@\n+  inline ShenandoahObjToScanQueue* get_old_queue(uint index) const;\n+\n+  inline ShenandoahGeneration* generation() { return _generation; };\n@@ -67,1 +83,0 @@\n-\/\/ ---------- Marking loop and tasks\n@@ -69,0 +84,1 @@\n+<<<<<<< HEAD\n@@ -71,0 +87,6 @@\n+=======\n+\/\/ ---------- Marking loop and tasks\n+\n+  template <class T, ShenandoahGenerationType GENERATION, StringDedupMode STRING_DEDUP>\n+  inline void do_task(ShenandoahObjToScanQueue* q, T* cl, ShenandoahLiveData* live_data, StringDedup::Requests* const req, ShenandoahMarkTask* task, uint worker_id);\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -79,0 +101,1 @@\n+<<<<<<< HEAD\n@@ -80,0 +103,3 @@\n+=======\n+  inline void count_liveness(ShenandoahLiveData* live_data, oop obj, uint worker_id);\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -85,0 +111,1 @@\n+<<<<<<< HEAD\n@@ -86,0 +113,10 @@\n+=======\n+  void mark_loop_prework(uint worker_id, TaskTerminator *terminator, ShenandoahReferenceProcessor *rp, StringDedup::Requests* const req, bool update_refs);\n+\n+  template <ShenandoahGenerationType GENERATION>\n+  static bool in_generation(ShenandoahHeap* const heap, oop obj);\n+\n+  static void mark_ref(ShenandoahObjToScanQueue* q,\n+                       ShenandoahMarkingContext* const mark_context,\n+                       bool weak, oop obj);\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -93,0 +130,4 @@\n+<<<<<<< HEAD\n+=======\n+\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.hpp","additions":43,"deletions":2,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -61,0 +62,1 @@\n+<<<<<<< HEAD\n@@ -62,0 +64,3 @@\n+=======\n+void ShenandoahMark::do_task(ShenandoahObjToScanQueue* q, T* cl, ShenandoahLiveData* live_data, StringDedup::Requests* const req, ShenandoahMarkTask* task, uint worker_id) {\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -64,0 +69,4 @@\n+  \/\/ TODO: This will push array chunks into the mark queue with no regard for\n+  \/\/ generations. I don't think it will break anything, but the young generation\n+  \/\/ scan might end up processing some old generation array chunks.\n+\n@@ -98,0 +107,1 @@\n+<<<<<<< HEAD\n@@ -99,0 +109,3 @@\n+=======\n+      count_liveness<GENERATION>(live_data, obj, worker_id);\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -107,0 +120,1 @@\n+<<<<<<< HEAD\n@@ -112,0 +126,18 @@\n+=======\n+inline void ShenandoahMark::count_liveness(ShenandoahLiveData* live_data, oop obj, uint worker_id) {\n+  const ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  const size_t region_idx = heap->heap_region_index_containing(obj);\n+  ShenandoahHeapRegion* const region = heap->get_region(region_idx);\n+  const size_t size = obj->size();\n+\n+  \/\/ Age census for objects in the young generation\n+  if (GENERATION == YOUNG || (GENERATION == GLOBAL && region->is_young())) {\n+    assert(heap->mode()->is_generational(), \"Only if generational\");\n+    if (ShenandoahGenerationalAdaptiveTenuring && !ShenandoahGenerationalCensusAtEvac) {\n+      assert(region->is_young(), \"Only for young objects\");\n+      uint age = ShenandoahHeap::get_object_age(obj);\n+      CENSUS_NOISE(heap->age_census()->add(age, region->age(), region->youth(), size, worker_id);)\n+      NO_CENSUS_NOISE(heap->age_census()->add(age, region->age(), size, worker_id);)\n+    }\n+  }\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -115,0 +147,1 @@\n+    assert(region->is_affiliated(), \"Do not count live data within Free Regular Region \" SIZE_FORMAT, region_idx);\n@@ -129,0 +162,1 @@\n+    assert(region->is_affiliated(), \"Do not count live data within FREE Humongous Start Region \" SIZE_FORMAT, region_idx);\n@@ -132,0 +166,1 @@\n+      assert(chain_reg->is_affiliated(), \"Do not count live data within FREE Humongous Continuation Region \" SIZE_FORMAT, i);\n@@ -238,0 +273,1 @@\n+  ShenandoahObjToScanQueue* _old_queue;\n@@ -241,1 +277,1 @@\n-  ShenandoahSATBBufferClosure(ShenandoahObjToScanQueue* q) :\n+  ShenandoahSATBBufferClosure(ShenandoahObjToScanQueue* q, ShenandoahObjToScanQueue* old_q) :\n@@ -243,0 +279,1 @@\n+    _old_queue(old_q),\n@@ -249,1 +286,1 @@\n-    assert(size == 0 || !_heap->has_forwarded_objects(), \"Forwarded objects are not expected here\");\n+    assert(size == 0 || !_heap->has_forwarded_objects() || _heap->is_concurrent_old_mark_in_progress(), \"Forwarded objects are not expected here\");\n@@ -252,0 +289,1 @@\n+<<<<<<< HEAD\n@@ -253,0 +291,3 @@\n+=======\n+      ShenandoahMark::mark_through_ref<oop, GENERATION>(p, _queue, _old_queue, _mark_context, false);\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -257,0 +298,1 @@\n+<<<<<<< HEAD\n@@ -259,0 +301,21 @@\n+=======\n+template<ShenandoahGenerationType GENERATION>\n+bool ShenandoahMark::in_generation(ShenandoahHeap* const heap, oop obj) {\n+  \/\/ Each in-line expansion of in_generation() resolves GENERATION at compile time.\n+  if (GENERATION == YOUNG) {\n+    return heap->is_in_young(obj);\n+  } else if (GENERATION == OLD) {\n+    return heap->is_in_old(obj);\n+  } else if (GENERATION == GLOBAL || GENERATION == NON_GEN) {\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+template<class T, ShenandoahGenerationType GENERATION>\n+inline void ShenandoahMark::mark_through_ref(T *p, ShenandoahObjToScanQueue* q, ShenandoahObjToScanQueue* old_q, ShenandoahMarkingContext* const mark_context, bool weak) {\n+  \/\/ Note: This is a very hot code path, so the code should be conditional on GENERATION template\n+  \/\/ parameter where possible, in order to generate the most efficient code.\n+\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -263,0 +326,1 @@\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -264,12 +328,28 @@\n-    shenandoah_assert_not_in_cset_except(p, obj, ShenandoahHeap::heap()->cancelled_gc());\n-\n-    bool skip_live = false;\n-    bool marked;\n-    if (weak) {\n-      marked = mark_context->mark_weak(obj);\n-    } else {\n-      marked = mark_context->mark_strong(obj, \/* was_upgraded = *\/ skip_live);\n-    }\n-    if (marked) {\n-      bool pushed = q->push(ShenandoahMarkTask(obj, skip_live, weak));\n-      assert(pushed, \"overflow queue should always succeed pushing\");\n+    shenandoah_assert_not_in_cset_except(p, obj, heap->cancelled_gc());\n+    if (in_generation<GENERATION>(heap, obj)) {\n+      mark_ref(q, mark_context, weak, obj);\n+      shenandoah_assert_marked(p, obj);\n+      \/\/ TODO: As implemented herein, GLOBAL collections reconstruct the card table during GLOBAL concurrent\n+      \/\/ marking. Note that the card table is cleaned at init_mark time so it needs to be reconstructed to support\n+      \/\/ future young-gen collections.  It might be better to reconstruct card table in\n+      \/\/ ShenandoahHeapRegion::global_oop_iterate_and_fill_dead.  We could either mark all live memory as dirty, or could\n+      \/\/ use the GLOBAL update-refs scanning of pointers to determine precisely which cards to flag as dirty.\n+      if (GENERATION == YOUNG && heap->is_in_old(p)) {\n+        \/\/ Mark card as dirty because remembered set scanning still finds interesting pointer.\n+        heap->mark_card_as_dirty((HeapWord*)p);\n+      } else if (GENERATION == GLOBAL && heap->is_in_old(p) && heap->is_in_young(obj)) {\n+        \/\/ Mark card as dirty because GLOBAL marking finds interesting pointer.\n+        heap->mark_card_as_dirty((HeapWord*)p);\n+      }\n+    } else if (old_q != nullptr) {\n+      \/\/ Young mark, bootstrapping old_q or concurrent with old_q marking.\n+      mark_ref(old_q, mark_context, weak, obj);\n+      shenandoah_assert_marked(p, obj);\n+    } else if (GENERATION == OLD) {\n+      \/\/ Old mark, found a young pointer.\n+      \/\/ TODO:  Rethink this: may be redundant with dirtying of cards identified during young-gen remembered set scanning\n+      \/\/ and by mutator write barriers.  Assert\n+      if (heap->is_in(p)) {\n+        assert(heap->is_in_young(obj), \"Expected young object.\");\n+        heap->mark_card_as_dirty(p);\n+      }\n@@ -277,0 +357,2 @@\n+  }\n+}\n@@ -278,1 +360,13 @@\n-    shenandoah_assert_marked(p, obj);\n+inline void ShenandoahMark::mark_ref(ShenandoahObjToScanQueue* q,\n+                              ShenandoahMarkingContext* const mark_context,\n+                              bool weak, oop obj) {\n+  bool skip_live = false;\n+  bool marked;\n+  if (weak) {\n+    marked = mark_context->mark_weak(obj);\n+  } else {\n+    marked = mark_context->mark_strong(obj, \/* was_upgraded = *\/ skip_live);\n+  }\n+  if (marked) {\n+    bool pushed = q->push(ShenandoahMarkTask(obj, skip_live, weak));\n+    assert(pushed, \"overflow queue should always succeed pushing\");\n@@ -289,0 +383,8 @@\n+\n+ShenandoahObjToScanQueue* ShenandoahMark::get_old_queue(uint index) const {\n+  if (_old_gen_task_queues != nullptr) {\n+    return _old_gen_task_queues->queue(index);\n+  }\n+  return nullptr;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.inline.hpp","additions":117,"deletions":15,"binary":false,"changes":132,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -35,0 +36,1 @@\n+<<<<<<< HEAD\n@@ -36,0 +38,3 @@\n+=======\n+  ShenandoahMark::mark_through_ref<T, GENERATION>(p, _queue, _old_queue, _mark_context, _weak);\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOopClosures.inline.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -32,0 +33,4 @@\n+<<<<<<< HEAD\n+=======\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -48,0 +53,1 @@\n+\n@@ -64,0 +70,1 @@\n+<<<<<<< HEAD\n@@ -65,0 +72,4 @@\n+=======\n+  \/\/ Only called from STW mark, should not be used to bootstrap old generation marking.\n+  ShenandoahMark::mark_through_ref<T, GENERATION>(p, _queue, nullptr, _mark_context, false);\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -87,2 +98,2 @@\n-ShenandoahSTWMark::ShenandoahSTWMark(bool full_gc) :\n-  ShenandoahMark(),\n+ShenandoahSTWMark::ShenandoahSTWMark(ShenandoahGeneration* generation, bool full_gc) :\n+  ShenandoahMark(generation),\n@@ -90,1 +101,1 @@\n-  _terminator(ShenandoahHeap::heap()->workers()->active_workers(), ShenandoahHeap::heap()->marking_context()->task_queues()),\n+  _terminator(ShenandoahHeap::heap()->workers()->active_workers(), task_queues()),\n@@ -103,1 +114,1 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = heap->active_generation()->ref_processor();\n@@ -122,0 +133,5 @@\n+    if (_generation->is_young()) {\n+      \/\/ But only scan the remembered set for young generation.\n+      _generation->scan_remembered_set(false \/* is_concurrent *\/);\n+    }\n+\n@@ -129,1 +145,1 @@\n-  heap->mark_complete_marking_context();\n+  _generation->set_mark_complete();\n@@ -141,0 +157,1 @@\n+<<<<<<< HEAD\n@@ -143,0 +160,22 @@\n+=======\n+  switch (_generation->type()) {\n+    case NON_GEN: {\n+      ShenandoahInitMarkRootsClosure<NON_GEN> init_mark(task_queues()->queue(worker_id));\n+      _root_scanner.roots_do(&init_mark, worker_id);\n+      break;\n+    }\n+    case GLOBAL: {\n+      ShenandoahInitMarkRootsClosure<GLOBAL> init_mark(task_queues()->queue(worker_id));\n+      _root_scanner.roots_do(&init_mark, worker_id);\n+      break;\n+    }\n+    case YOUNG: {\n+      ShenandoahInitMarkRootsClosure<YOUNG> init_mark(task_queues()->queue(worker_id));\n+      _root_scanner.roots_do(&init_mark, worker_id);\n+      break;\n+    }\n+    case OLD:\n+    default:\n+      ShouldNotReachHere();\n+  }\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n@@ -148,1 +187,1 @@\n-  ShenandoahReferenceProcessor* rp = ShenandoahHeap::heap()->ref_processor();\n+  ShenandoahReferenceProcessor* rp = ShenandoahHeap::heap()->active_generation()->ref_processor();\n@@ -151,0 +190,1 @@\n+<<<<<<< HEAD\n@@ -152,0 +192,4 @@\n+=======\n+  mark_loop(worker_id, &_terminator, rp,\n+            _generation->type(), false \/* not cancellable *\/,\n+>>>>>>> e9a1c69952753979e5d1b0f440cfa1ee71f18033\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSTWMark.cpp","additions":50,"deletions":6,"binary":false,"changes":56,"status":"modified"}]}