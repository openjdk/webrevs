{"files":[{"patch":"@@ -43,1 +43,0 @@\n-  \/\/ TODO: Why rs_align is 0 on page_size == os::vm_page_size?\n@@ -79,8 +78,0 @@\n-\n-  \/\/ TODO: As currently implemented, we do not swap pointers between _read_byte_map and _write_byte_map\n-  \/\/ because the mutator write barrier hard codes the address of the _write_byte_map_base.  Instead,\n-  \/\/ the current implementation simply copies contents of _write_byte_map onto _read_byte_map and cleans\n-  \/\/ the entirety of _write_byte_map at the init_mark safepoint.\n-  \/\/\n-  \/\/ Alternatively, we may switch to a SATB-based write barrier and replace the direct card-marking\n-  \/\/ remembered set with something entirely different.\n@@ -115,32 +106,0 @@\n-\n-\/\/ TODO: This service is not currently used because we are not able to swap _read_byte_map_base and\n-\/\/ _write_byte_map_base pointers.  If we were able to do so, we would invoke clear_read_table \"immediately\"\n-\/\/ following the end of concurrent remembered set scanning so that this read card table would be ready\n-\/\/ to serve as the new write card table at the time these pointer values were next swapped.\n-\/\/\n-\/\/ In the current implementation, the write-table is cleared immediately after its contents is copied to\n-\/\/ the read table, obviating the need for this service.\n-void ShenandoahCardTable::clear_read_table() {\n-  for (size_t i = 0; i < _byte_map_size; i++) {\n-    _read_byte_map[i] = clean_card;\n-  }\n-}\n-\n-\/\/ TODO: This service is not currently used because the mutator write barrier implementation hard codes the\n-\/\/ location of the _write_byte_may_base.  If we change the mutator's write barrier implementation, then we\n-\/\/ may use this service to exchange the roles of the read-card-table and write-card-table.\n-void ShenandoahCardTable::swap_card_tables() {\n-  shenandoah_assert_safepoint();\n-\n-  CardValue* save_value = _read_byte_map;\n-  _read_byte_map = _write_byte_map;\n-  _write_byte_map = save_value;\n-\n-  save_value = _read_byte_map_base;\n-  _read_byte_map_base = _write_byte_map_base;\n-  _write_byte_map_base = save_value;\n-\n-  \/\/ update the superclass instance variables\n-  _byte_map = _write_byte_map;\n-  _byte_map_base = _write_byte_map_base;\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardTable.cpp","additions":0,"deletions":41,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -71,5 +71,0 @@\n-  void clear_read_table();\n-\n-  \/\/ Exchange the roles of the read and write card tables.\n-  void swap_card_tables();\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardTable.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -127,1 +127,0 @@\n-    \/\/ TODO: When RS scanning yields, we will need a check_cancellation_and_abort() degeneration point here.\n@@ -730,1 +729,0 @@\n-      \/\/ TODO: Do we need to set this if we are only promoting regions in place? We don't need the barriers on for that.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1173,7 +1173,0 @@\n-\n-    \/\/ TODO: Do we need to fix FullGC so that it maintains aged segregation of objects into distinct regions?\n-    \/\/       A partial solution would be to remember how many objects are of tenure age following Full GC, but\n-    \/\/       this is probably suboptimal, because most of these objects will not reside in a region that will be\n-    \/\/       selected for the next evacuation phase.\n-\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -210,1 +210,0 @@\n-  \/\/ TODO: Eventually, we want to replace this with a constant-time exchange of pointers.\n@@ -602,5 +601,0 @@\n-      \/\/\n-      \/\/ TODO:\n-      \/\/   If we are auto-tuning the tenure age and regions that were anticipated to be promoted in place end up\n-      \/\/   being promoted by evacuation, this event should feed into the tenure-age-selection heuristic so that\n-      \/\/   the tenure age can be increased.\n@@ -709,8 +703,0 @@\n-      \/\/ TODO: young_available can include available (between top() and end()) within each young region that is not\n-      \/\/ part of the collection set.  Making this memory available to the young_evacuation_reserve allows a larger\n-      \/\/ young collection set to be chosen when available memory is under extreme pressure.  Implementing this \"improvement\"\n-      \/\/ is tricky, because the incremental construction of the collection set actually changes the amount of memory\n-      \/\/ available to hold evacuated young-gen objects.  As currently implemented, the memory that is available within\n-      \/\/ non-empty regions that are not selected as part of the collection set can be allocated by the mutator while\n-      \/\/ GC is evacuating and updating references.\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":0,"deletions":14,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -131,7 +131,0 @@\n-        \/\/ TODO: if humongous_alloc_failure_pending, there might be value in trying a \"compacting\" degen before\n-        \/\/ going all the way to full.  But it's a lot of work to implement this, and it may not provide value.\n-        \/\/ A compacting degen can move young regions around without doing full old-gen mark (relying upon the\n-        \/\/ remembered set scan), so it might be faster than a full gc.\n-        \/\/\n-        \/\/ Longer term, think about how to defragment humongous memory concurrently.\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalControlThread.cpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -157,1 +157,0 @@\n-  \/\/ TODO: use an existing coalesce-and-fill function rather than replicating the code here.\n@@ -226,6 +225,0 @@\n-  \/\/ TODO: Consider not promoting humongous objects that represent primitive arrays.  Leaving a primitive array\n-  \/\/ (obj->is_typeArray()) in young-gen is harmless because these objects are never relocated and they are not\n-  \/\/ scanned.  Leaving primitive arrays in young-gen memory allows their memory to be reclaimed more quickly when\n-  \/\/ it becomes garbage.  Better to not make this change until sizes of young-gen and old-gen are completely\n-  \/\/ adaptive, as leaving primitive arrays in young-gen might be perceived as an \"astonishing result\" by someone\n-  \/\/ has carefully analyzed the required sizes of an application's young-gen and old-gen.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalEvacuationTask.cpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -53,2 +53,1 @@\n-    \/\/ TODO: Should this assert is_in()?\n-    return true;\n+    return ShenandoahHeap::heap()->is_in_reserved(obj);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGlobalGeneration.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -988,2 +988,0 @@\n-      \/\/\n-      \/\/ TODO: Consider GLOBAL GC rather than Full GC to remediate OOM condition: https:\/\/bugs.openjdk.org\/browse\/JDK-8335910\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -865,2 +865,0 @@\n-      \/\/ TODO: should we reset_age() for OLD as well?  Examine invocations of set_affiliation(). Some contexts redundantly\n-      \/\/       invoke reset_age().\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -68,4 +68,0 @@\n-  \/\/ TODO: This will push array chunks into the mark queue with no regard for\n-  \/\/ generations. I don't think it will break anything, but the young generation\n-  \/\/ scan might end up processing some old generation array chunks.\n-\n@@ -313,5 +309,0 @@\n-      \/\/ TODO: As implemented herein, GLOBAL collections reconstruct the card table during GLOBAL concurrent\n-      \/\/ marking. Note that the card table is cleaned at init_mark time so it needs to be reconstructed to support\n-      \/\/ future young-gen collections.  It might be better to reconstruct card table in a different phase.  We could\n-      \/\/ either mark all live memory as dirty, or could use the GLOBAL update-refs scanning of pointers to determine\n-      \/\/ precisely which cards to flag as dirty.\n@@ -331,2 +322,0 @@\n-      \/\/ TODO: Rethink this: may be redundant with dirtying of cards identified during young-gen remembered set scanning\n-      \/\/ and by mutator write barriers.\n@@ -335,2 +324,0 @@\n-        \/\/ TODO: This assert _should not_ pop, but it does. We need to figure out why.\n-        \/\/ assert(heap->old_generation()->card_scan()->is_card_dirty((HeapWord*)p), \"Card should already be marked.\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.inline.hpp","additions":0,"deletions":13,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -64,6 +64,5 @@\n-  \/\/ TODO: Do these really need to be const?\n-  inline bool is_marked(const oop) const;\n-  inline bool is_marked_strong(const oop obj) const;\n-  inline bool is_marked_weak(const oop obj) const;\n-  inline bool is_marked_or_old(const oop obj) const;\n-  inline bool is_marked_strong_or_old(const oop obj) const;\n+  inline bool is_marked(oop) const;\n+  inline bool is_marked_strong(oop obj) const;\n+  inline bool is_marked_weak(oop obj) const;\n+  inline bool is_marked_or_old(oop obj) const;\n+  inline bool is_marked_strong_or_old(oop obj) const;\n@@ -73,1 +72,1 @@\n-  inline bool allocated_after_mark_start(const oop obj) const;\n+  inline bool allocated_after_mark_start(oop obj) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.hpp","additions":6,"deletions":7,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n-inline bool ShenandoahMarkingContext::is_marked(const oop obj) const {\n+inline bool ShenandoahMarkingContext::is_marked(oop obj) const {\n@@ -45,1 +45,1 @@\n-inline bool ShenandoahMarkingContext::is_marked_strong(const oop obj) const {\n+inline bool ShenandoahMarkingContext::is_marked_strong(oop obj) const {\n@@ -49,1 +49,1 @@\n-inline bool ShenandoahMarkingContext::is_marked_weak(const oop obj) const {\n+inline bool ShenandoahMarkingContext::is_marked_weak(oop obj) const {\n@@ -53,1 +53,1 @@\n-inline bool ShenandoahMarkingContext::is_marked_or_old(const oop obj) const {\n+inline bool ShenandoahMarkingContext::is_marked_or_old(oop obj) const {\n@@ -57,1 +57,1 @@\n-inline bool ShenandoahMarkingContext::is_marked_strong_or_old(const oop obj) const {\n+inline bool ShenandoahMarkingContext::is_marked_strong_or_old(oop obj) const {\n@@ -65,1 +65,1 @@\n-inline bool ShenandoahMarkingContext::allocated_after_mark_start(const oop obj) const {\n+inline bool ShenandoahMarkingContext::allocated_after_mark_start(oop obj) const {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.inline.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -146,1 +146,0 @@\n-    \/\/ TODO: avoid making the call to record_degenerated() in the case that this degenerated upgraded to full gc.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMmuTracker.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -136,9 +136,0 @@\n-\n-  \/\/ TODO: Old marking doesn't support class unloading yet\n-  \/\/ Perform concurrent class unloading\n-  \/\/ if (heap->unload_classes() &&\n-  \/\/     heap->is_concurrent_weak_root_in_progress()) {\n-  \/\/   entry_class_unloading();\n-  \/\/ }\n-\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.cpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -127,4 +127,0 @@\n-size_t ShenandoahDirectCardMarkRememberedSet::cluster_count() const {\n-  return _cluster_count;\n-}\n-\n@@ -140,1 +136,1 @@\n-    object_starts[card_at_start + i].short_word = 0;\n+    _object_starts[card_at_start + i].short_word = 0;\n@@ -222,1 +218,1 @@\n-  return object_starts[card_index].offsets.first & FirstStartBits;\n+  return _object_starts[card_index].offsets.first & FirstStartBits;\n@@ -227,1 +223,1 @@\n-  return object_starts[card_index].offsets.last;\n+  return _object_starts[card_index].offsets.last;\n@@ -234,3 +230,0 @@\n-\/\/ TODO: collect some stats for the size of walks backward over cards.\n-\/\/ For larger objects, a logarithmic BOT such as used by G1 might make the\n-\/\/ backwards walk potentially faster.\n@@ -301,8 +294,0 @@\n-size_t ShenandoahScanRemembered::last_valid_index() {\n-  return _rs->last_valid_index();\n-}\n-\n-size_t ShenandoahScanRemembered::total_cards() {\n-  return _rs->total_cards();\n-}\n-\n@@ -325,16 +310,0 @@\n-void ShenandoahScanRemembered::mark_card_as_dirty(size_t card_index) {\n-  _rs->mark_card_as_dirty(card_index);\n-}\n-\n-void ShenandoahScanRemembered::mark_range_as_dirty(size_t card_index, size_t num_cards) {\n-  _rs->mark_range_as_dirty(card_index, num_cards);\n-}\n-\n-void ShenandoahScanRemembered::mark_card_as_clean(size_t card_index) {\n-  _rs->mark_card_as_clean(card_index);\n-}\n-\n-void ShenandoahScanRemembered::mark_range_as_clean(size_t card_index, size_t num_cards) {\n-  _rs->mark_range_as_clean(card_index, num_cards);\n-}\n-\n@@ -361,4 +330,0 @@\n-size_t ShenandoahScanRemembered::cluster_count() {\n-  return _rs->cluster_count();\n-}\n-\n@@ -617,1 +582,0 @@\n-  _cluster_count = total_card_count \/ ShenandoahCardCluster::CardsPerCluster;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.cpp","additions":3,"deletions":39,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -211,1 +211,0 @@\n-  size_t _cluster_count;\n@@ -241,7 +240,0 @@\n-  inline size_t cluster_count() const;\n-\n-  \/\/ Called by GC thread at start of concurrent mark to exchange roles of read and write remembered sets.\n-  \/\/ Not currently used because mutator write barrier does not honor changes to the location of card table.\n-  \/\/ Instead of swap_remset, the current implementation of concurrent remembered set scanning does reset_remset\n-  \/\/ in parallel threads, each invocation processing one entire HeapRegion at a time.\n-  void swap_remset() {  _card_table->swap_card_tables(); }\n@@ -255,3 +247,0 @@\n-\n-  \/\/ Called by GC thread after scanning old remembered set in order to prepare for next GC pass\n-  void clear_old_remset() {  _card_table->clear_read_table(); }\n@@ -378,1 +367,1 @@\n-  crossing_info *object_starts;\n+  crossing_info *_object_starts;\n@@ -383,1 +372,1 @@\n-    object_starts[card_index].offsets.first = ObjectStartsInCardRegion | value;\n+    _object_starts[card_index].offsets.first = ObjectStartsInCardRegion | value;\n@@ -387,1 +376,1 @@\n-    object_starts[card_index].offsets.last = value;\n+    _object_starts[card_index].offsets.last = value;\n@@ -391,1 +380,1 @@\n-    object_starts[card_index].offsets.first |= ObjectStartsInCardRegion;\n+    _object_starts[card_index].offsets.first |= ObjectStartsInCardRegion;\n@@ -395,1 +384,1 @@\n-    object_starts[card_index].offsets.first &= ~ObjectStartsInCardRegion;\n+    _object_starts[card_index].offsets.first &= ~ObjectStartsInCardRegion;\n@@ -400,1 +389,1 @@\n-    return (object_starts[card_index].offsets.first & ObjectStartsInCardRegion) != 0;\n+    return (_object_starts[card_index].offsets.first & ObjectStartsInCardRegion) != 0;\n@@ -407,1 +396,1 @@\n-      object_starts[card_index++].short_word = 0;\n+      _object_starts[card_index++].short_word = 0;\n@@ -412,3 +401,1 @@\n-    \/\/ TODO: We don't really need object_starts entries for every card entry.  We only need these for\n-    \/\/ the card entries that correspond to old-gen memory.  But for now, let's be quick and dirty.\n-    object_starts = NEW_C_HEAP_ARRAY(crossing_info, rs->total_cards(), mtGC);\n+    _object_starts = NEW_C_HEAP_ARRAY(crossing_info, rs->total_cards(), mtGC);\n@@ -416,1 +403,1 @@\n-      object_starts[i].short_word = 0;\n+      _object_starts[i].short_word = 0;\n@@ -421,2 +408,2 @@\n-    FREE_C_HEAP_ARRAY(crossing_info, object_starts);\n-    object_starts = nullptr;\n+    FREE_C_HEAP_ARRAY(crossing_info, _object_starts);\n+    _object_starts = nullptr;\n@@ -431,1 +418,1 @@\n-  \/\/  second object had been been the first object within card memory,\n+  \/\/  second object had been the first object within card memory,\n@@ -606,1 +593,1 @@\n-  \/\/ caller has assure no other thread will endeavor to concurrently\n+  \/\/ caller has assured no other thread will endeavor to concurrently\n@@ -776,2 +763,0 @@\n-  size_t last_valid_index();\n-  size_t total_cards();\n@@ -782,4 +767,0 @@\n-  void mark_card_as_dirty(size_t card_index);\n-  void mark_range_as_dirty(size_t card_index, size_t num_cards);\n-  void mark_card_as_clean(size_t card_index);\n-  void mark_range_as_clean(size_t card_index, size_t num_cards);\n@@ -791,4 +772,0 @@\n-  size_t cluster_count();\n-\n-  \/\/ Called by GC thread at start of concurrent mark to exchange roles of read and write remembered sets.\n-  void swap_remset() { _rs->swap_remset(); }\n@@ -800,3 +777,0 @@\n-  \/\/ Called by GC thread after scanning old remembered set in order to prepare for next GC pass\n-  void clear_old_remset() { _rs->clear_old_remset(); }\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.hpp","additions":13,"deletions":39,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -174,2 +174,0 @@\n-      \/\/ TODO: It is worthwhile to memoize this, so as to avoid that\n-      \/\/ overhead, and it is easy to do, but deferred to a follow-up.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.inline.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"}]}