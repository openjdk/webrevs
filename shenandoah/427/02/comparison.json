{"files":[{"patch":"@@ -336,2 +336,0 @@\n-          } else {\n-            ShenandoahThreadLocalData::subtract_from_plab_evacuated(thread, size * HeapWordSize);\n@@ -386,2 +384,0 @@\n-  } else {\n-    ShenandoahThreadLocalData::add_to_plab_evacuated(thread, size * HeapWordSize);\n@@ -502,1 +498,1 @@\n-  \/\/ We don't enforce limits on plab_evacuated.  We let it consume all available old-gen memory in order to reduce\n+  \/\/ We don't enforce limits on plab evacuations.  We let it consume all available old-gen memory in order to reduce\n@@ -512,1 +508,1 @@\n-          ShenandoahThreadLocalData::get_plab_preallocated_promoted(thread) - ShenandoahThreadLocalData::get_plab_promoted(thread);\n+          ShenandoahThreadLocalData::get_plab_actual_size(thread) - ShenandoahThreadLocalData::get_plab_promoted(thread);\n@@ -514,2 +510,1 @@\n-  ShenandoahThreadLocalData::reset_plab_evacuated(thread);\n-  ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+  ShenandoahThreadLocalData::set_plab_actual_size(thread, 0);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":3,"deletions":8,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1111,145 +1111,8 @@\n-  bool try_smaller_lab_size = false;\n-  size_t smaller_lab_size;\n-  {\n-    \/\/ promotion_eligible pertains only to PLAB allocations, denoting that the PLAB is allowed to allocate for promotions.\n-    bool promotion_eligible = false;\n-    bool allow_allocation = true;\n-    bool plab_alloc = false;\n-    size_t requested_bytes = req.size() * HeapWordSize;\n-    HeapWord* result = nullptr;\n-\n-    \/\/ If we are dealing with mutator allocation, then we may need to block for safepoint.\n-    \/\/ We cannot block for safepoint for GC allocations, because there is a high chance\n-    \/\/ we are already running at safepoint or from stack watermark machinery, and we cannot\n-    \/\/ block again.\n-    ShenandoahHeapLocker locker(lock(), req.is_mutator_alloc());\n-    Thread* thread = Thread::current();\n-\n-    if (mode()->is_generational()) {\n-      if (req.affiliation() == YOUNG_GENERATION) {\n-        if (req.is_mutator_alloc()) {\n-          size_t young_words_available = young_generation()->available() \/ HeapWordSize;\n-          if (req.is_lab_alloc() && (req.min_size() < young_words_available)) {\n-            \/\/ Allow ourselves to try a smaller lab size even if requested_bytes <= young_available.  We may need a smaller\n-            \/\/ lab size because young memory has become too fragmented.\n-            try_smaller_lab_size = true;\n-            smaller_lab_size = (young_words_available < req.size())? young_words_available: req.size();\n-          } else if (req.size() > young_words_available) {\n-            \/\/ Can't allocate because even min_size() is larger than remaining young_available\n-            log_info(gc, ergo)(\"Unable to shrink %s alloc request of minimum size: \" SIZE_FORMAT\n-                               \", young words available: \" SIZE_FORMAT, req.type_string(),\n-                               HeapWordSize * (req.is_lab_alloc()? req.min_size(): req.size()), young_words_available);\n-            return nullptr;\n-          }\n-        }\n-      } else {                    \/\/ reg.affiliation() == OLD_GENERATION\n-        assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"GCLAB pertains only to young-gen memory\");\n-        if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n-          plab_alloc = true;\n-          size_t promotion_avail = old_generation()->get_promoted_reserve();\n-          size_t promotion_expended = old_generation()->get_promoted_expended();\n-          if (promotion_expended + requested_bytes > promotion_avail) {\n-            promotion_avail = 0;\n-            if (old_generation()->get_evacuation_reserve() == 0) {\n-              \/\/ There are no old-gen evacuations in this pass.  There's no value in creating a plab that cannot\n-              \/\/ be used for promotions.\n-              allow_allocation = false;\n-            }\n-          } else {\n-            promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n-            promotion_eligible = true;\n-          }\n-        } else if (req.is_promotion()) {\n-          \/\/ This is a shared alloc for promotion\n-          size_t promotion_avail = old_generation()->get_promoted_reserve();\n-          size_t promotion_expended = old_generation()->get_promoted_expended();\n-          if (promotion_expended + requested_bytes > promotion_avail) {\n-            promotion_avail = 0;\n-          } else {\n-            promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n-          }\n-          if (promotion_avail == 0) {\n-            \/\/ We need to reserve the remaining memory for evacuation.  Reject this allocation.  The object will be\n-            \/\/ evacuated to young-gen memory and promoted during a future GC pass.\n-            return nullptr;\n-          }\n-          \/\/ Else, we'll allow the allocation to proceed.  (Since we hold heap lock, the tested condition remains true.)\n-        } else {\n-          \/\/ This is a shared allocation for evacuation.  Memory has already been reserved for this purpose.\n-        }\n-      }\n-    } \/\/ This ends the is_generational() block\n-\n-    \/\/ First try the original request.  If TLAB request size is greater than available, allocate() will attempt to downsize\n-    \/\/ request to fit within available memory.\n-    result = (allow_allocation)? _free_set->allocate(req, in_new_region): nullptr;\n-    if (result != nullptr) {\n-      if (req.is_old()) {\n-        ShenandoahThreadLocalData::reset_plab_promoted(thread);\n-        if (req.is_gc_alloc()) {\n-          bool disable_plab_promotions = false;\n-          if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n-            if (promotion_eligible) {\n-              size_t actual_size = req.actual_size() * HeapWordSize;\n-              \/\/ The actual size of the allocation may be larger than the requested bytes (due to alignment on card boundaries).\n-              \/\/ If this puts us over our promotion budget, we need to disable future PLAB promotions for this thread.\n-              if (old_generation()->get_promoted_expended() + actual_size <= old_generation()->get_promoted_reserve()) {\n-                \/\/ Assume the entirety of this PLAB will be used for promotion.  This prevents promotion from overreach.\n-                \/\/ When we retire this plab, we'll unexpend what we don't really use.\n-                ShenandoahThreadLocalData::enable_plab_promotions(thread);\n-                old_generation()->expend_promoted(actual_size);\n-                ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, actual_size);\n-              } else {\n-                disable_plab_promotions = true;\n-              }\n-            } else {\n-              disable_plab_promotions = true;\n-            }\n-            if (disable_plab_promotions) {\n-              \/\/ Disable promotions in this thread because entirety of this PLAB must be available to hold old-gen evacuations.\n-              ShenandoahThreadLocalData::disable_plab_promotions(thread);\n-              ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n-            }\n-          } else if (req.is_promotion()) {\n-            \/\/ Shared promotion.  Assume size is requested_bytes.\n-            old_generation()->expend_promoted(requested_bytes);\n-          }\n-        }\n-\n-        \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n-        \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n-        \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n-        \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n-        \/\/\n-        \/\/ objects being \"concurrently\" allocated:\n-        \/\/    [-----a------][-----b-----][--------------c------------------]\n-        \/\/            [---- card table memory range --------------]\n-        \/\/\n-        \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n-        \/\/   wants to set the starts-object, first-start, and last-start attributes of the preceding card region.\n-        \/\/   allocation of object b wants to set the starts-object, first-start, and last-start attributes of this card region.\n-        \/\/   allocation of object c also wants to set the starts-object, first-start, and last-start attributes of this\n-        \/\/   card region.\n-        \/\/\n-        \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n-        \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n-        \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n-        card_scan()->register_object(result);\n-      }\n-    } else {\n-      \/\/ The allocation failed.  If this was a plab allocation, We've already retired it and no longer have a plab.\n-      if (req.is_old() && req.is_gc_alloc() && (req.type() == ShenandoahAllocRequest::_alloc_plab)) {\n-        \/\/ We don't need to disable PLAB promotions because there is no PLAB.  We leave promotions enabled because\n-        \/\/ this allows the surrounding infrastructure to retry alloc_plab_slow() with a smaller PLAB size.\n-        ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n-      }\n-    }\n-    if ((result != nullptr) || !try_smaller_lab_size) {\n-      return result;\n-    }\n-    \/\/ else, fall through to try_smaller_lab_size\n-  } \/\/ This closes the block that holds the heap lock, releasing the lock.\n-\n-  \/\/ We failed to allocate the originally requested lab size.  Let's see if we can allocate a smaller lab size.\n-  if (req.size() == smaller_lab_size) {\n-    \/\/ If we were already trying to allocate min size, no value in attempting to repeat the same.  End the recursion.\n+  \/\/ If we are dealing with mutator allocation, then we may need to block for safepoint.\n+  \/\/ We cannot block for safepoint for GC allocations, because there is a high chance\n+  \/\/ we are already running at safepoint or from stack watermark machinery, and we cannot\n+  \/\/ block again.\n+  ShenandoahHeapLocker locker(lock(), req.is_mutator_alloc());\n+\n+  \/\/ Make sure the old generation has room for either evacuations or promotions before trying to allocate.\n+  if (req.is_old() && !old_generation()->can_allocate(req)) {\n@@ -1259,34 +1122,28 @@\n-  \/\/ We arrive here if the tlab allocation request can be resized to fit within young_available\n-  assert((req.affiliation() == YOUNG_GENERATION) && req.is_lab_alloc() && req.is_mutator_alloc() &&\n-         (smaller_lab_size < req.size()), \"Only shrink allocation request size for TLAB allocations\");\n-\n-  \/\/ By convention, ShenandoahAllocationRequest is primarily read-only.  The only mutable instance data is represented by\n-  \/\/ actual_size(), which is overwritten with the size of the allocaion when the allocation request is satisfied.  We use a\n-  \/\/ recursive call here rather than introducing new methods to mutate the existing ShenandoahAllocationRequest argument.\n-  \/\/ Mutation of the existing object might result in astonishing results if calling contexts assume the content of immutable\n-  \/\/ fields remain constant.  The original TLAB allocation request was for memory that exceeded the current capacity.  We'll\n-  \/\/ attempt to allocate a smaller TLAB.  If this is successful, we'll update actual_size() of our incoming\n-  \/\/ ShenandoahAllocRequest.  If the recursive request fails, we'll simply return nullptr.\n-\n-  \/\/ Note that we've relinquished the HeapLock and some other thread may perform additional allocation before our recursive\n-  \/\/ call reacquires the lock.  If that happens, we will need another recursive call to further reduce the size of our request\n-  \/\/ for each time another thread allocates young memory during the brief intervals that the heap lock is available to\n-  \/\/ interfering threads.  We expect this interference to be rare.  The recursion bottoms out when young_available is\n-  \/\/ smaller than req.min_size().  The inner-nested call to allocate_memory_under_lock() uses the same min_size() value\n-  \/\/ as this call, but it uses a preferred size() that is smaller than our preferred size, and is no larger than what we most\n-  \/\/ recently saw as the memory currently available within the young generation.\n-\n-  \/\/ TODO: At the expense of code clarity, we could rewrite this recursive solution to use iteration.  We need at most one\n-  \/\/ extra instance of the ShenandoahAllocRequest, which we can re-initialize multiple times inside a loop, with one iteration\n-  \/\/ of the loop required for each time the existing solution would recurse.  An iterative solution would be more efficient\n-  \/\/ in CPU time and stack memory utilization.  The expectation is that it is very rare that we would recurse more than once\n-  \/\/ so making this change is not currently seen as a high priority.\n-\n-  ShenandoahAllocRequest smaller_req = ShenandoahAllocRequest::for_tlab(req.min_size(), smaller_lab_size);\n-\n-  \/\/ Note that shrinking the preferred size gets us past the gatekeeper that checks whether there's available memory to\n-  \/\/ satisfy the allocation request.  The reality is the actual TLAB size is likely to be even smaller, because it will\n-  \/\/ depend on how much memory is available within mutator regions that are not yet fully used.\n-  HeapWord* result = allocate_memory_under_lock(smaller_req, in_new_region);\n-  if (result != nullptr) {\n-    req.set_actual_size(smaller_req.actual_size());\n+  \/\/ If TLAB request size is greater than available, allocate() will attempt to downsize request to fit within available\n+  \/\/ memory.\n+  HeapWord* result = _free_set->allocate(req, in_new_region);\n+\n+  \/\/ Record the plab configuration for this result and register the object.\n+  if (result != nullptr && req.is_old()) {\n+    old_generation()->configure_plab_for_current_thread(req);\n+    if (req.type() == ShenandoahAllocRequest::_alloc_shared_gc) {\n+      \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+      \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+      \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+      \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+      \/\/\n+      \/\/ objects being \"concurrently\" allocated:\n+      \/\/    [-----a------][-----b-----][--------------c------------------]\n+      \/\/            [---- card table memory range --------------]\n+      \/\/\n+      \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n+      \/\/ wants to set the starts-object, first-start, and last-start attributes of the preceding card region.\n+      \/\/ Allocation of object b wants to set the starts-object, first-start, and last-start attributes of this card region.\n+      \/\/ Allocation of object c also wants to set the starts-object, first-start, and last-start attributes of this\n+      \/\/ card region.\n+      \/\/\n+      \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n+      \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n+      \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+      card_scan()->register_object(result);\n+    }\n@@ -1294,0 +1151,1 @@\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":37,"deletions":179,"binary":false,"changes":216,"status":"modified"},{"patch":"@@ -221,1 +221,1 @@\n-size_t ShenandoahOldGeneration::get_promoted_expended() {\n+size_t ShenandoahOldGeneration::get_promoted_expended() const {\n@@ -225,0 +225,56 @@\n+bool ShenandoahOldGeneration::can_allocate(const ShenandoahAllocRequest &req) const {\n+  assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"GCLAB pertains only to young-gen memory\");\n+\n+  const size_t requested_bytes = req.size() * HeapWordSize;\n+  \/\/ If there is room to promote, there is also room to evacuate.\n+  if (can_promote(requested_bytes)) {\n+    assert(get_evacuation_reserve() > 0, \"Promotion reserve greater than evacuation reserve\");\n+    \/\/ The promotion reserve should be able to accommodate this request. The request\n+    \/\/ might still fail if alignment with the card table increases the size. The request\n+    \/\/ may also fail if the heap is badly fragmented and the free set cannot find room for it.\n+    return true;\n+  }\n+\n+  if (req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+    \/\/ The promotion reserve cannot accommodate this plab request. Check if we still have room for\n+    \/\/ evacuations. Note that we cannot really know how much of the plab will be used for evacuations,\n+    \/\/ so here we only check that some evacuation reserve still exists.\n+    return get_evacuation_reserve() > 0;\n+  }\n+\n+  \/\/ If this is a shared allocation promotion request, we do not have room for any it.\n+  \/\/ This promotion into old will fail, so we will just evacuate the object in the young generation.\n+  return !req.is_promotion();\n+}\n+\n+void\n+ShenandoahOldGeneration::configure_plab_for_current_thread(const ShenandoahAllocRequest &req) {\n+  \/\/ Note: Even when a mutator is performing a promotion outside a LAB, we use a 'shared_gc' request.\n+  if (req.is_gc_alloc()) {\n+    const size_t actual_size = req.actual_size() * HeapWordSize;\n+    if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+      \/\/ We've created a new plab. Now we configure it whether it will be used for promotions\n+      \/\/ and evacuations - or just evacuations.\n+      Thread* thread = Thread::current();\n+      ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+\n+      \/\/ The actual size of the allocation may be larger than the requested bytes (due to alignment on card boundaries).\n+      \/\/ If this puts us over our promotion budget, we need to disable future PLAB promotions for this thread.\n+      if (can_promote(actual_size)) {\n+        \/\/ Assume the entirety of this PLAB will be used for promotion.  This prevents promotion from overreach.\n+        \/\/ When we retire this plab, we'll unexpend what we don't really use.\n+        expend_promoted(actual_size);\n+        ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+        ShenandoahThreadLocalData::set_plab_actual_size(thread, actual_size);\n+      } else {\n+        \/\/ Disable promotions in this thread because entirety of this PLAB must be available to hold old-gen evacuations.\n+        ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+        ShenandoahThreadLocalData::set_plab_actual_size(thread, 0);\n+      }\n+    } else if (req.is_promotion()) {\n+      \/\/ Shared promotion.\n+      expend_promoted(actual_size);\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":57,"deletions":1,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n@@ -115,1 +116,16 @@\n-  size_t get_promoted_expended();\n+  size_t get_promoted_expended() const;\n+\n+  \/\/ Test if there is enough memory reserved for this promotion\n+  bool can_promote(size_t requested_bytes) const {\n+    size_t promotion_avail = get_promoted_reserve();\n+    size_t promotion_expended = get_promoted_expended();\n+    return promotion_expended + requested_bytes <= promotion_avail;\n+  }\n+\n+  \/\/ Test if there is enough memory available in the old generation to accommodate this request.\n+  \/\/ The request will be subject to constraints on promotion and evacuation reserves.\n+  bool can_allocate(const ShenandoahAllocRequest& req) const;\n+\n+  \/\/ Updates the promotion expenditure tracking and configures whether the plab may be used\n+  \/\/ for promotions and evacuations, or just evacuations.\n+  void configure_plab_for_current_thread(const ShenandoahAllocRequest &req);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.hpp","additions":17,"deletions":1,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -43,2 +43,2 @@\n-  _plab_size(0),\n-  _plab_evacuated(0),\n+  _plab_desired_size(0),\n+  _plab_actual_size(0),\n@@ -46,1 +46,0 @@\n-  _plab_preallocated_promoted(0),\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahThreadLocalData.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -63,1 +63,0 @@\n-  size_t _plab_size;\n@@ -65,1 +64,9 @@\n-  size_t _plab_evacuated;\n+  \/\/ Heuristics will grow the desired size of plabs.\n+  size_t _plab_desired_size;\n+\n+  \/\/ Once the plab has been allocated, and we know the actual size, we record it here.\n+  size_t _plab_actual_size;\n+\n+  \/\/ As the plab is used for promotions, this value is incremented. When the plab is\n+  \/\/ retired, the difference between 'actual_size' and 'promoted' will be returned to\n+  \/\/ the old generation's promotion reserve (i.e., it will be 'unexpended').\n@@ -67,2 +74,5 @@\n-  size_t _plab_preallocated_promoted;\n-  bool   _plab_allows_promotion; \/\/ If false, no more promotion by this thread during this evacuation phase.\n+\n+  \/\/ If false, no more promotion by this thread during this evacuation phase.\n+  bool   _plab_allows_promotion;\n+\n+  \/\/ If true, evacuations may attempt to allocate a smaller plab if the original size fails.\n@@ -113,8 +123,0 @@\n-    \/\/ TODO:\n-    \/\/   Only initialize _plab if (!Universe::is_fully_initialized() || ShenandoahHeap::heap()->mode()->is_generational())\n-    \/\/   Otherwise, set _plab to nullptr\n-    \/\/ Problem is there is code sprinkled throughout that asserts (plab != nullptr) that need to be fixed up.  Perhaps\n-    \/\/ those assertions are overzealous.\n-\n-    \/\/ In theory, plabs are only need if heap->mode()->is_generational().  However, some threads\n-    \/\/ instantiated before we are able to answer that question.\n@@ -123,1 +125,1 @@\n-      data(thread)->_plab_size = 0;\n+      data(thread)->_plab_desired_size = 0;\n@@ -160,1 +162,1 @@\n-    return data(thread)->_plab_size;\n+    return data(thread)->_plab_desired_size;\n@@ -164,1 +166,1 @@\n-    data(thread)->_plab_size = v;\n+    data(thread)->_plab_desired_size = v;\n@@ -191,17 +193,0 @@\n-  static void reset_plab_evacuated(Thread* thread) {\n-    data(thread)->_plab_evacuated = 0;\n-  }\n-\n-  static void add_to_plab_evacuated(Thread* thread, size_t increment) {\n-    data(thread)->_plab_evacuated += increment;\n-  }\n-\n-  static void subtract_from_plab_evacuated(Thread* thread, size_t increment) {\n-    \/\/ TODO: Assert underflow\n-    data(thread)->_plab_evacuated -= increment;\n-  }\n-\n-  static size_t get_plab_evacuated(Thread* thread) {\n-    return data(thread)->_plab_evacuated;\n-  }\n-\n@@ -217,1 +202,1 @@\n-    \/\/ TODO: Assert underflow\n+    assert(data(thread)->_plab_promoted >= increment, \"Cannot subtract more than remaining promoted\");\n@@ -225,2 +210,2 @@\n-  static void set_plab_preallocated_promoted(Thread* thread, size_t value) {\n-    data(thread)->_plab_preallocated_promoted = value;\n+  static void set_plab_actual_size(Thread* thread, size_t value) {\n+    data(thread)->_plab_actual_size = value;\n@@ -229,2 +214,2 @@\n-  static size_t get_plab_preallocated_promoted(Thread* thread) {\n-    return data(thread)->_plab_preallocated_promoted;\n+  static size_t get_plab_actual_size(Thread* thread) {\n+    return data(thread)->_plab_actual_size;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahThreadLocalData.hpp","additions":22,"deletions":37,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -0,0 +1,188 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"unittest.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahThreadLocalData.hpp\"\n+\n+#define SKIP_IF_NOT_SHENANDOAH()                                                \\\n+  if (!UseShenandoahGC || !ShenandoahHeap::heap()->mode()->is_generational()) { \\\n+    tty->print_cr(\"skipped\");                                                   \\\n+    return;                                                                     \\\n+  }\n+\n+class ShenandoahOldGenerationTest : public ::testing::Test {\n+protected:\n+  static const size_t INITIAL_PLAB_SIZE;\n+  static const size_t INITIAL_PLAB_PROMOTED;\n+\n+  ShenandoahOldGeneration old;\n+\n+  ShenandoahOldGenerationTest()\n+    : old(8, 1024 * 1024, 1024)\n+    , _locker(ShenandoahHeap::heap()->lock())\n+  {\n+    old.set_promoted_reserve(512 * HeapWordSize);\n+    old.expend_promoted(256 * HeapWordSize);\n+    old.set_evacuation_reserve(512 * HeapWordSize);\n+    Thread* thread = Thread::current();\n+    ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+    ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+    ShenandoahThreadLocalData::set_plab_actual_size(thread, INITIAL_PLAB_SIZE);\n+    ShenandoahThreadLocalData::add_to_plab_promoted(thread, INITIAL_PLAB_PROMOTED);\n+  }\n+\n+  static bool promotions_enabled() {\n+    return ShenandoahThreadLocalData::allow_plab_promotions(Thread::current());\n+  }\n+\n+  static size_t plab_size() {\n+    return ShenandoahThreadLocalData::get_plab_actual_size(Thread::current());\n+  }\n+\n+  static size_t plab_promoted() {\n+    return ShenandoahThreadLocalData::get_plab_promoted(Thread::current());\n+  }\n+\n+private:\n+  ShenandoahHeapLocker _locker;\n+};\n+\n+const size_t ShenandoahOldGenerationTest::INITIAL_PLAB_SIZE = 42;\n+const size_t ShenandoahOldGenerationTest::INITIAL_PLAB_PROMOTED = 128;\n+\n+TEST_VM_F(ShenandoahOldGenerationTest, test_can_promote) {\n+  SKIP_IF_NOT_SHENANDOAH()\n+  EXPECT_TRUE(old.can_promote(128 * HeapWordSize)) << \"Should have room to promote\";\n+  EXPECT_FALSE(old.can_promote(384 * HeapWordSize)) << \"Should not have room to promote\";\n+}\n+\n+TEST_VM_F(ShenandoahOldGenerationTest, test_can_allocate_plab_for_promotion) {\n+  SKIP_IF_NOT_SHENANDOAH()\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(128, 128);\n+  EXPECT_TRUE(old.can_allocate(req)) << \"Should have room to promote\";\n+}\n+\n+TEST_VM_F(ShenandoahOldGenerationTest, test_can_allocate_plab_for_evacuation) {\n+  SKIP_IF_NOT_SHENANDOAH()\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(384, 384);\n+  EXPECT_FALSE(old.can_promote(req.size() * HeapWordSize)) << \"No room for promotions\";\n+  EXPECT_TRUE(old.can_allocate(req)) << \"Should have room to evacuate\";\n+}\n+\n+TEST_VM_F(ShenandoahOldGenerationTest, test_cannot_allocate_plab) {\n+  SKIP_IF_NOT_SHENANDOAH()\n+  \/\/ Simulate having exhausted the evacuation reserve when request is too big to be promoted\n+  old.set_evacuation_reserve(0);\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(384, 384);\n+  EXPECT_FALSE(old.can_allocate(req)) << \"No room for promotions or evacuations\";\n+}\n+\n+TEST_VM_F(ShenandoahOldGenerationTest, test_can_allocate_for_shared_evacuation) {\n+  SKIP_IF_NOT_SHENANDOAH()\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(768, ShenandoahAffiliation::OLD_GENERATION, false);\n+  EXPECT_FALSE(old.can_promote(req.size() * HeapWordSize)) << \"No room for promotion\";\n+  EXPECT_TRUE(old.can_allocate(req)) << \"Should have room to evacuate shared (even though evacuation reserve is smaller than request)\";\n+}\n+\n+TEST_VM_F(ShenandoahOldGenerationTest, test_cannot_allocate_for_shared_promotion) {\n+  SKIP_IF_NOT_SHENANDOAH()\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(768, ShenandoahAffiliation::OLD_GENERATION, true);\n+  EXPECT_FALSE(old.can_promote(req.size() * HeapWordSize)) << \"No room for promotion\";\n+  EXPECT_FALSE(old.can_allocate(req)) << \"No room to promote, should fall back to evacuation in young gen\";\n+}\n+\n+TEST_VM_F(ShenandoahOldGenerationTest, test_expend_promoted) {\n+  SKIP_IF_NOT_SHENANDOAH()\n+\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(128, 128);\n+\n+  \/\/ simulate the allocation\n+  req.set_actual_size(128);\n+\n+  size_t actual_size = req.actual_size() * HeapWordSize;\n+  EXPECT_TRUE(old.can_promote(actual_size)) << \"Should have room for promotion\";\n+\n+  size_t expended_before = old.get_promoted_expended();\n+  old.configure_plab_for_current_thread(req);\n+  size_t expended_after = old.get_promoted_expended();\n+  EXPECT_EQ(expended_before + actual_size, expended_after) << \"Should expend promotion reserve\";\n+  EXPECT_EQ(plab_promoted(), 0UL) << \"Nothing promoted yet\";\n+  EXPECT_EQ(plab_size(), actual_size) << \"New plab should be able to hold this much promotion\";\n+  EXPECT_TRUE(promotions_enabled()) << \"Plab should be available for promotions\";\n+}\n+\n+TEST_VM_F(ShenandoahOldGenerationTest, test_actual_size_exceeds_promotion_reserve) {\n+  SKIP_IF_NOT_SHENANDOAH()\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(128, 128);\n+\n+  \/\/ simulate an allocation that exceeds the promotion reserve after allocation\n+  req.set_actual_size(384);\n+  EXPECT_FALSE(old.can_promote(req.actual_size() * HeapWordSize)) << \"Should have room for promotion\";\n+\n+  size_t expended_before = old.get_promoted_expended();\n+  old.configure_plab_for_current_thread(req);\n+  size_t expended_after = old.get_promoted_expended();\n+\n+  EXPECT_EQ(expended_before, expended_after) << \"Did not promote, should not expend promotion\";\n+  EXPECT_EQ(plab_promoted(), 0UL) << \"Cannot promote in new plab\";\n+  EXPECT_EQ(plab_size(), 0UL) << \"Should not have space for promotions\";\n+  EXPECT_FALSE(promotions_enabled()) << \"New plab can only be used for evacuations\";\n+}\n+\n+TEST_VM_F(ShenandoahOldGenerationTest, test_shared_expends_promoted_but_does_not_change_plab) {\n+  SKIP_IF_NOT_SHENANDOAH()\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(128, ShenandoahAffiliation::OLD_GENERATION, true);\n+  req.set_actual_size(128);\n+  size_t actual_size = req.actual_size() * HeapWordSize;\n+\n+  size_t expended_before = old.get_promoted_expended();\n+  old.configure_plab_for_current_thread(req);\n+  size_t expended_after = old.get_promoted_expended();\n+\n+  EXPECT_EQ(expended_before + actual_size, expended_after) << \"Shared promotion still expends promotion\";\n+  EXPECT_EQ(plab_promoted(), INITIAL_PLAB_PROMOTED) << \"Shared promotion should not count in plab\";\n+  EXPECT_EQ(plab_size(), INITIAL_PLAB_SIZE) << \"Shared promotion should not change size of plab\";\n+  EXPECT_FALSE(promotions_enabled());\n+}\n+\n+TEST_VM_F(ShenandoahOldGenerationTest, test_shared_evacuation_has_no_side_effects) {\n+  SKIP_IF_NOT_SHENANDOAH()\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(128, ShenandoahAffiliation::OLD_GENERATION, false);\n+  req.set_actual_size(128);\n+\n+  size_t expended_before = old.get_promoted_expended();\n+  old.configure_plab_for_current_thread(req);\n+  size_t expended_after = old.get_promoted_expended();\n+\n+  EXPECT_EQ(expended_before, expended_after) << \"Not a promotion, should not expend promotion reserve\";\n+  EXPECT_EQ(plab_promoted(), INITIAL_PLAB_PROMOTED) << \"Not a plab, should not have touched plab\";\n+  EXPECT_EQ(plab_size(), INITIAL_PLAB_SIZE) << \"Not a plab, should not have touched plab\";\n+  EXPECT_FALSE(promotions_enabled());\n+}\n+\n+#undef SKIP_IF_NOT_SHENANDOAH\n","filename":"test\/hotspot\/gtest\/gc\/shenandoah\/test_shenandoahOldGeneration.cpp","additions":188,"deletions":0,"binary":false,"changes":188,"status":"added"}]}