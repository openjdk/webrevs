{"files":[{"patch":"@@ -4,1 +4,0 @@\n- * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -32,1 +31,0 @@\n-#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n@@ -36,3 +34,0 @@\n-#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n-#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n-#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -44,1 +39,0 @@\n-#include \"gc\/shenandoah\/shenandoahOldGC.hpp\"\n@@ -50,2 +44,0 @@\n-#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n-#include \"logging\/log.hpp\"\n@@ -59,11 +51,5 @@\n-  ConcurrentGCThread(),\n-  _alloc_failure_waiters_lock(Mutex::safepoint - 2, \"ShenandoahAllocFailureGC_lock\", true),\n-  _gc_waiters_lock(Mutex::safepoint - 2, \"ShenandoahRequestedGC_lock\", true),\n-  _control_lock(Mutex::nosafepoint - 2, \"ShenandoahControlGC_lock\", true),\n-  _regulator_lock(Mutex::nosafepoint - 2, \"ShenandoahRegulatorGC_lock\", true),\n-  _requested_gc_cause(GCCause::_no_gc),\n-  _requested_generation(select_global_generation()),\n-  _degen_point(ShenandoahGC::_degenerated_outside_cycle),\n-  _degen_generation(nullptr),\n-  _allocs_seen(0),\n-  _mode(none) {\n+  ShenandoahController(),\n+  _alloc_failure_waiters_lock(Mutex::safepoint-2, \"ShenandoahAllocFailureGC_lock\", true),\n+  _gc_waiters_lock(Mutex::safepoint-2, \"ShenandoahRequestedGC_lock\", true),\n+  _requested_gc_cause(GCCause::_no_cause_specified),\n+  _degen_point(ShenandoahGC::_degenerated_outside_cycle) {\n@@ -71,1 +57,0 @@\n-  reset_gc_id();\n@@ -76,1 +61,1 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -78,2 +63,3 @@\n-  const GCMode default_mode = concurrent_normal;\n-  ShenandoahGenerationType generation = select_global_generation();\n+  GCMode default_mode = concurrent_normal;\n+  GCCause::Cause default_cause = GCCause::_shenandoah_concurrent_gc;\n+  int sleep = ShenandoahControlIntervalMin;\n@@ -82,1 +68,1 @@\n-  uint age_period = 0;\n+  double last_sleep_adjust_time = os::elapsedTime();\n@@ -88,3 +74,1 @@\n-  const double shrink_period = (double)ShenandoahUncommitDelay \/ 1000 \/ 10;\n-\n-  ShenandoahCollectorPolicy* const policy = heap->shenandoah_policy();\n+  double shrink_period = (double)ShenandoahUncommitDelay \/ 1000 \/ 10;\n@@ -92,5 +76,2 @@\n-  \/\/ Heuristics are notified of allocation failures here and other outcomes\n-  \/\/ of the cycle. They're also used here to control whether the Nth consecutive\n-  \/\/ degenerated cycle should be 'promoted' to a full cycle. The decision to\n-  \/\/ trigger a cycle or not is evaluated on the regulator thread.\n-  ShenandoahHeuristics* global_heuristics = heap->global_generation()->heuristics();\n+  ShenandoahCollectorPolicy* policy = heap->shenandoah_policy();\n+  ShenandoahHeuristics* heuristics = heap->heuristics();\n@@ -99,2 +80,5 @@\n-    const bool alloc_failure_pending = _alloc_failure_gc.is_set();\n-    const bool humongous_alloc_failure_pending = _humongous_alloc_failure_gc.is_set();\n+    bool alloc_failure_pending = _alloc_failure_gc.is_set();\n+    bool is_gc_requested = _gc_requested.is_set();\n+    GCCause::Cause requested_gc_cause = _requested_gc_cause;\n+    bool explicit_gc_requested = is_gc_requested && is_explicit_gc(requested_gc_cause);\n+    bool implicit_gc_requested = is_gc_requested && !is_explicit_gc(requested_gc_cause);\n@@ -102,7 +86,2 @@\n-    GCCause::Cause cause = Atomic::xchg(&_requested_gc_cause, GCCause::_no_gc);\n-\n-    const bool explicit_gc_requested = is_explicit_gc(cause);\n-    const bool implicit_gc_requested = is_implicit_gc(cause);\n-\n-    \/\/ This control loop iteration have seen this much allocations.\n-    const size_t allocs_seen = Atomic::xchg(&_allocs_seen, (size_t)0, memory_order_relaxed);\n+    \/\/ This control loop iteration has seen this much allocation.\n+    size_t allocs_seen = reset_allocs_seen();\n@@ -111,1 +90,1 @@\n-    const bool soft_max_changed = check_soft_max_changed();\n+    bool soft_max_changed = heap->check_soft_max_changed();\n@@ -114,1 +93,2 @@\n-    set_gc_mode(none);\n+    GCMode mode = none;\n+    GCCause::Cause cause = GCCause::_last_gc_cause;\n@@ -127,14 +107,1 @@\n-      if (degen_point == ShenandoahGC::_degenerated_outside_cycle) {\n-        _degen_generation = heap->mode()->is_generational() ?\n-                heap->young_generation() : heap->global_generation();\n-      } else {\n-        assert(_degen_generation != nullptr, \"Need to know which generation to resume\");\n-      }\n-\n-      ShenandoahHeuristics* heuristics = _degen_generation->heuristics();\n-      generation = _degen_generation->type();\n-      bool old_gen_evacuation_failed = heap->clear_old_evacuation_failure();\n-\n-      \/\/ Do not bother with degenerated cycle if old generation evacuation failed or if humongous allocation failed\n-      if (ShenandoahDegeneratedGC && heuristics->should_degenerate_cycle() &&\n-          !old_gen_evacuation_failed && !humongous_alloc_failure_pending) {\n+      if (ShenandoahDegeneratedGC && heuristics->should_degenerate_cycle()) {\n@@ -143,1 +110,1 @@\n-        set_gc_mode(stw_degenerated);\n+        mode = stw_degenerated;\n@@ -145,7 +112,0 @@\n-        \/\/ TODO: if humongous_alloc_failure_pending, there might be value in trying a \"compacting\" degen before\n-        \/\/ going all the way to full.  But it's a lot of work to implement this, and it may not provide value.\n-        \/\/ A compacting degen can move young regions around without doing full old-gen mark (relying upon the\n-        \/\/ remembered set scan), so it might be faster than a full gc.\n-        \/\/\n-        \/\/ Longer term, think about how to defragment humongous memory concurrently.\n-\n@@ -154,2 +114,1 @@\n-        generation = select_global_generation();\n-        set_gc_mode(stw_full);\n+        mode = stw_full;\n@@ -157,0 +116,1 @@\n+\n@@ -158,1 +118,1 @@\n-      generation = select_global_generation();\n+      cause = requested_gc_cause;\n@@ -161,1 +121,1 @@\n-      global_heuristics->record_requested_gc();\n+      heuristics->record_requested_gc();\n@@ -165,1 +125,1 @@\n-        set_gc_mode(default_mode);\n+        mode = default_mode;\n@@ -167,1 +127,1 @@\n-        heap->set_unload_classes(global_heuristics->can_unload_classes());\n+        heap->set_unload_classes(heuristics->can_unload_classes());\n@@ -170,1 +130,1 @@\n-        set_gc_mode(stw_full);\n+        mode = stw_full;\n@@ -173,1 +133,1 @@\n-      generation = select_global_generation();\n+      cause = requested_gc_cause;\n@@ -176,1 +136,1 @@\n-      global_heuristics->record_requested_gc();\n+      heuristics->record_requested_gc();\n@@ -180,1 +140,1 @@\n-        set_gc_mode(default_mode);\n+        mode = default_mode;\n@@ -183,1 +143,1 @@\n-        heap->set_unload_classes(global_heuristics->can_unload_classes());\n+        heap->set_unload_classes(heuristics->can_unload_classes());\n@@ -186,1 +146,1 @@\n-        set_gc_mode(stw_full);\n+        mode = stw_full;\n@@ -189,15 +149,5 @@\n-      \/\/ We should only be here if the regulator requested a cycle or if\n-      \/\/ there is an old generation mark in progress.\n-      if (cause == GCCause::_shenandoah_concurrent_gc) {\n-        if (_requested_generation == OLD && heap->doing_mixed_evacuations()) {\n-          \/\/ If a request to start an old cycle arrived while an old cycle was running, but _before_\n-          \/\/ it chose any regions for evacuation we don't want to start a new old cycle. Rather, we want\n-          \/\/ the heuristic to run a young collection so that we can evacuate some old regions.\n-          assert(!heap->is_concurrent_old_mark_in_progress(), \"Should not be running mixed collections and concurrent marking\");\n-          generation = YOUNG;\n-        } else {\n-          generation = _requested_generation;\n-        }\n-\n-        \/\/ preemption was requested or this is a regular cycle\n-        set_gc_mode(default_mode);\n+      \/\/ Potential normal cycle: ask heuristics if it wants to act\n+      if (heuristics->should_start_gc()) {\n+        mode = default_mode;\n+        cause = default_cause;\n+      }\n@@ -205,4 +155,3 @@\n-        \/\/ Don't start a new old marking if there is one already in progress\n-        if (generation == OLD && heap->is_concurrent_old_mark_in_progress()) {\n-          set_gc_mode(servicing_old);\n-        }\n+      \/\/ Ask policy if this cycle wants to process references or unload classes\n+      heap->set_unload_classes(heuristics->should_unload_classes());\n+    }\n@@ -210,17 +159,4 @@\n-        if (generation == select_global_generation()) {\n-          heap->set_unload_classes(global_heuristics->should_unload_classes());\n-        } else {\n-          heap->set_unload_classes(false);\n-        }\n-      } else if (heap->is_concurrent_old_mark_in_progress() || heap->is_prepare_for_old_mark_in_progress()) {\n-        \/\/ Nobody asked us to do anything, but we have an old-generation mark or old-generation preparation for\n-        \/\/ mixed evacuation in progress, so resume working on that.\n-        log_info(gc)(\"Resume old GC: marking is%s in progress, preparing is%s in progress\",\n-                     heap->is_concurrent_old_mark_in_progress() ? \"\" : \" NOT\",\n-                     heap->is_prepare_for_old_mark_in_progress() ? \"\" : \" NOT\");\n-\n-        cause = GCCause::_shenandoah_concurrent_gc;\n-        generation = OLD;\n-        set_gc_mode(servicing_old);\n-        heap->set_unload_classes(false);\n-      }\n+    \/\/ Blow all soft references on this cycle, if handling allocation failure,\n+    \/\/ either implicit or explicit GC request,  or we are requested to do so unconditionally.\n+    if (alloc_failure_pending || implicit_gc_requested || explicit_gc_requested || ShenandoahAlwaysClearSoftRefs) {\n+      heap->soft_ref_policy()->set_should_clear_all_soft_refs(true);\n@@ -229,2 +165,2 @@\n-    const bool gc_requested = (gc_mode() != none);\n-    assert (!gc_requested || cause != GCCause::_no_gc, \"GC cause should be set\");\n+    bool gc_requested = (mode != none);\n+    assert (!gc_requested || cause != GCCause::_last_gc_cause, \"GC cause should be set\");\n@@ -233,6 +169,0 @@\n-      \/\/ Blow away all soft references on this cycle, if handling allocation failure,\n-      \/\/ either implicit or explicit GC request, or we are requested to do so unconditionally.\n-      if (generation == select_global_generation() && (alloc_failure_pending || implicit_gc_requested || explicit_gc_requested || ShenandoahAlwaysClearSoftRefs)) {\n-        heap->soft_ref_policy()->set_should_clear_all_soft_refs(true);\n-      }\n-\n@@ -255,16 +185,4 @@\n-      \/\/ In case this is a degenerated cycle, remember whether original cycle was aging.\n-      const bool was_aging_cycle = heap->is_aging_cycle();\n-      heap->set_aging_cycle(false);\n-\n-      switch (gc_mode()) {\n-        case concurrent_normal: {\n-          \/\/ At this point:\n-          \/\/  if (generation == YOUNG), this is a normal YOUNG cycle\n-          \/\/  if (generation == OLD), this is a bootstrap OLD cycle\n-          \/\/  if (generation == GLOBAL), this is a GLOBAL cycle triggered by System.gc()\n-          \/\/ In all three cases, we want to age old objects if this is an aging cycle\n-          if (age_period-- == 0) {\n-             heap->set_aging_cycle(true);\n-             age_period = ShenandoahAgingCyclePeriod - 1;\n-          }\n-          service_concurrent_normal_cycle(heap, generation, cause);\n+\n+      switch (mode) {\n+        case concurrent_normal:\n+          service_concurrent_normal_cycle(cause);\n@@ -272,3 +190,1 @@\n-        }\n-        case stw_degenerated: {\n-          heap->set_aging_cycle(was_aging_cycle);\n+        case stw_degenerated:\n@@ -277,6 +193,1 @@\n-        }\n-        case stw_full: {\n-          if (age_period-- == 0) {\n-            heap->set_aging_cycle(true);\n-            age_period = ShenandoahAgingCyclePeriod - 1;\n-          }\n+        case stw_full:\n@@ -285,7 +196,0 @@\n-        }\n-        case servicing_old: {\n-          assert(generation == OLD, \"Expected old generation here\");\n-          GCIdMark gc_id_mark;\n-          service_concurrent_old_cycle(heap, cause);\n-          break;\n-        }\n@@ -331,1 +235,7 @@\n-        global_heuristics->clear_metaspace_oom();\n+        heuristics->clear_metaspace_oom();\n+      }\n+\n+      \/\/ Commit worker statistics to cycle data\n+      heap->phase_timings()->flush_par_workers_to_cycle();\n+      if (ShenandoahPacing) {\n+        heap->pacer()->flush_stats_to_cycle();\n@@ -334,1 +244,15 @@\n-      process_phase_timings(heap);\n+      \/\/ Print GC stats for current cycle\n+      {\n+        LogTarget(Info, gc, stats) lt;\n+        if (lt.is_enabled()) {\n+          ResourceMark rm;\n+          LogStream ls(lt);\n+          heap->phase_timings()->print_cycle_on(&ls);\n+          if (ShenandoahPacing) {\n+            heap->pacer()->print_cycle_on(&ls);\n+          }\n+        }\n+      }\n+\n+      \/\/ Commit statistics to globals\n+      heap->phase_timings()->flush_cycle_to_global();\n@@ -344,1 +268,1 @@\n-      \/\/ Allow pacer to know we have seen this many allocations\n+      \/\/ Allow allocators to know we have seen this much regions\n@@ -365,1 +289,1 @@\n-      service_uncommit(shrink_before, shrink_until);\n+      heap->service_uncommit(shrink_before, shrink_until);\n@@ -370,6 +294,8 @@\n-    \/\/ Wait for ShenandoahControlIntervalMax unless there was an allocation failure or another request was made mid-cycle.\n-    if (!is_alloc_failure_gc() && _requested_gc_cause == GCCause::_no_gc) {\n-      \/\/ The timed wait is necessary because this thread has a responsibility to send\n-      \/\/ 'alloc_words' to the pacer when it does not perform a GC.\n-      MonitorLocker lock(&_control_lock, Mutex::_no_safepoint_check_flag);\n-      lock.wait(ShenandoahControlIntervalMax);\n+    \/\/ Wait before performing the next action. If allocation happened during this wait,\n+    \/\/ we exit sooner, to let heuristics re-evaluate new conditions. If we are at idle,\n+    \/\/ back off exponentially.\n+    if (_heap_changed.try_unset()) {\n+      sleep = ShenandoahControlIntervalMin;\n+    } else if ((current - last_sleep_adjust_time) * 1000 > ShenandoahControlIntervalAdjustPeriod){\n+      sleep = MIN2<int>(ShenandoahControlIntervalMax, MAX2(1, sleep * 2));\n+      last_sleep_adjust_time = current;\n@@ -377,0 +303,1 @@\n+    os::naked_short_sleep(sleep);\n@@ -385,226 +312,1 @@\n-void ShenandoahControlThread::process_phase_timings(const ShenandoahHeap* heap) {\n-  \/\/ Commit worker statistics to cycle data\n-  heap->phase_timings()->flush_par_workers_to_cycle();\n-  if (ShenandoahPacing) {\n-    heap->pacer()->flush_stats_to_cycle();\n-  }\n-\n-  ShenandoahEvacuationTracker* evac_tracker = heap->evac_tracker();\n-  ShenandoahCycleStats         evac_stats   = evac_tracker->flush_cycle_to_global();\n-\n-  \/\/ Print GC stats for current cycle\n-  {\n-    LogTarget(Info, gc, stats) lt;\n-    if (lt.is_enabled()) {\n-      ResourceMark rm;\n-      LogStream ls(lt);\n-      heap->phase_timings()->print_cycle_on(&ls);\n-      evac_tracker->print_evacuations_on(&ls, &evac_stats.workers,\n-                                              &evac_stats.mutators);\n-      if (ShenandoahPacing) {\n-        heap->pacer()->print_cycle_on(&ls);\n-      }\n-    }\n-  }\n-\n-  \/\/ Commit statistics to globals\n-  heap->phase_timings()->flush_cycle_to_global();\n-}\n-\n-\/\/ Young and old concurrent cycles are initiated by the regulator. Implicit\n-\/\/ and explicit GC requests are handled by the controller thread and always\n-\/\/ run a global cycle (which is concurrent by default, but may be overridden\n-\/\/ by command line options). Old cycles always degenerate to a global cycle.\n-\/\/ Young cycles are degenerated to complete the young cycle.  Young\n-\/\/ and old degen may upgrade to Full GC.  Full GC may also be\n-\/\/ triggered directly by a System.gc() invocation.\n-\/\/\n-\/\/\n-\/\/      +-----+ Idle +-----+-----------+---------------------+\n-\/\/      |         +        |           |                     |\n-\/\/      |         |        |           |                     |\n-\/\/      |         |        v           |                     |\n-\/\/      |         |  Bootstrap Old +-- | ------------+       |\n-\/\/      |         |   +                |             |       |\n-\/\/      |         |   |                |             |       |\n-\/\/      |         v   v                v             v       |\n-\/\/      |    Resume Old <----------+ Young +--> Young Degen  |\n-\/\/      |     +  +   ^                            +  +       |\n-\/\/      v     |  |   |                            |  |       |\n-\/\/   Global <-+  |   +----------------------------+  |       |\n-\/\/      +        |                                   |       |\n-\/\/      |        v                                   v       |\n-\/\/      +--->  Global Degen +--------------------> Full <----+\n-\/\/\n-void ShenandoahControlThread::service_concurrent_normal_cycle(ShenandoahHeap* heap,\n-                                                              const ShenandoahGenerationType generation,\n-                                                              GCCause::Cause cause) {\n-  GCIdMark gc_id_mark;\n-  ShenandoahGeneration* the_generation = nullptr;\n-  switch (generation) {\n-    case YOUNG: {\n-      \/\/ Run a young cycle. This might or might not, have interrupted an ongoing\n-      \/\/ concurrent mark in the old generation. We need to think about promotions\n-      \/\/ in this case. Promoted objects should be above the TAMS in the old regions\n-      \/\/ they end up in, but we have to be sure we don't promote into any regions\n-      \/\/ that are in the cset.\n-      log_info(gc, ergo)(\"Start GC cycle (YOUNG)\");\n-      the_generation = heap->young_generation();\n-      service_concurrent_cycle(the_generation, cause, false);\n-      break;\n-    }\n-    case OLD: {\n-      log_info(gc, ergo)(\"Start GC cycle (OLD)\");\n-      the_generation = heap->old_generation();\n-      service_concurrent_old_cycle(heap, cause);\n-      break;\n-    }\n-    case GLOBAL_GEN: {\n-      log_info(gc, ergo)(\"Start GC cycle (GLOBAL)\");\n-      the_generation = heap->global_generation();\n-      service_concurrent_cycle(the_generation, cause, false);\n-      break;\n-    }\n-    case GLOBAL_NON_GEN: {\n-      log_info(gc, ergo)(\"Start GC cycle\");\n-      the_generation = heap->global_generation();\n-      service_concurrent_cycle(the_generation, cause, false);\n-      break;\n-    }\n-    default:\n-      ShouldNotReachHere();\n-  }\n-}\n-\n-void ShenandoahControlThread::service_concurrent_old_cycle(ShenandoahHeap* heap, GCCause::Cause &cause) {\n-  ShenandoahOldGeneration* old_generation = heap->old_generation();\n-  ShenandoahYoungGeneration* young_generation = heap->young_generation();\n-  ShenandoahOldGeneration::State original_state = old_generation->state();\n-\n-  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n-\n-  switch (original_state) {\n-    case ShenandoahOldGeneration::FILLING: {\n-      _allow_old_preemption.set();\n-      old_generation->entry_coalesce_and_fill();\n-      _allow_old_preemption.unset();\n-\n-      \/\/ Before bootstrapping begins, we must acknowledge any cancellation request.\n-      \/\/ If the gc has not been cancelled, this does nothing. If it has been cancelled,\n-      \/\/ this will clear the cancellation request and exit before starting the bootstrap\n-      \/\/ phase. This will allow the young GC cycle to proceed normally. If we do not\n-      \/\/ acknowledge the cancellation request, the subsequent young cycle will observe\n-      \/\/ the request and essentially cancel itself.\n-      if (check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle)) {\n-        log_info(gc)(\"Preparation for old generation cycle was cancelled\");\n-        return;\n-      }\n-\n-      \/\/ Coalescing threads completed and nothing was cancelled. it is safe to transition from this state.\n-      old_generation->transition_to(ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP);\n-      return;\n-    }\n-    case ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP:\n-      old_generation->transition_to(ShenandoahOldGeneration::BOOTSTRAPPING);\n-    case ShenandoahOldGeneration::BOOTSTRAPPING: {\n-      \/\/ Configure the young generation's concurrent mark to put objects in\n-      \/\/ old regions into the concurrent mark queues associated with the old\n-      \/\/ generation. The young cycle will run as normal except that rather than\n-      \/\/ ignore old references it will mark and enqueue them in the old concurrent\n-      \/\/ task queues but it will not traverse them.\n-      set_gc_mode(bootstrapping_old);\n-      young_generation->set_old_gen_task_queues(old_generation->task_queues());\n-      ShenandoahGCSession session(cause, young_generation);\n-      service_concurrent_cycle(heap, young_generation, cause, true);\n-      process_phase_timings(heap);\n-      if (heap->cancelled_gc()) {\n-        \/\/ Young generation bootstrap cycle has failed. Concurrent mark for old generation\n-        \/\/ is going to resume after degenerated bootstrap cycle completes.\n-        log_info(gc)(\"Bootstrap cycle for old generation was cancelled\");\n-        return;\n-      }\n-\n-      \/\/ Reset the degenerated point. Normally this would happen at the top\n-      \/\/ of the control loop, but here we have just completed a young cycle\n-      \/\/ which has bootstrapped the old concurrent marking.\n-      _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n-\n-      \/\/ From here we will 'resume' the old concurrent mark. This will skip reset\n-      \/\/ and init mark for the concurrent mark. All of that work will have been\n-      \/\/ done by the bootstrapping young cycle.\n-      set_gc_mode(servicing_old);\n-      old_generation->transition_to(ShenandoahOldGeneration::MARKING);\n-    }\n-    case ShenandoahOldGeneration::MARKING: {\n-      ShenandoahGCSession session(cause, old_generation);\n-      bool marking_complete = resume_concurrent_old_cycle(old_generation, cause);\n-      if (marking_complete) {\n-        assert(old_generation->state() != ShenandoahOldGeneration::MARKING, \"Should not still be marking\");\n-        if (original_state == ShenandoahOldGeneration::MARKING) {\n-          heap->mmu_tracker()->record_old_marking_increment(true);\n-          heap->log_heap_status(\"At end of Concurrent Old Marking finishing increment\");\n-        }\n-      } else if (original_state == ShenandoahOldGeneration::MARKING) {\n-        heap->mmu_tracker()->record_old_marking_increment(false);\n-        heap->log_heap_status(\"At end of Concurrent Old Marking increment\");\n-      }\n-      break;\n-    }\n-    default:\n-      fatal(\"Unexpected state for old GC: %s\", ShenandoahOldGeneration::state_name(old_generation->state()));\n-  }\n-}\n-\n-bool ShenandoahControlThread::resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause) {\n-  assert(ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress(), \"Old mark should be in progress\");\n-  log_debug(gc)(\"Resuming old generation with \" UINT32_FORMAT \" marking tasks queued\", generation->task_queues()->tasks());\n-\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-\n-  \/\/ We can only tolerate being cancelled during concurrent marking or during preparation for mixed\n-  \/\/ evacuation. This flag here (passed by reference) is used to control precisely where the regulator\n-  \/\/ is allowed to cancel a GC.\n-  ShenandoahOldGC gc(generation, _allow_old_preemption);\n-  if (gc.collect(cause)) {\n-    generation->record_success_concurrent(false);\n-  }\n-\n-  if (heap->cancelled_gc()) {\n-    \/\/ It's possible the gc cycle was cancelled after the last time\n-    \/\/ the collection checked for cancellation. In which case, the\n-    \/\/ old gc cycle is still completed, and we have to deal with this\n-    \/\/ cancellation. We set the degeneration point to be outside\n-    \/\/ the cycle because if this is an allocation failure, that is\n-    \/\/ what must be done (there is no degenerated old cycle). If the\n-    \/\/ cancellation was due to a heuristic wanting to start a young\n-    \/\/ cycle, then we are not actually going to a degenerated cycle,\n-    \/\/ so the degenerated point doesn't matter here.\n-    check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle);\n-    if (_requested_gc_cause == GCCause::_shenandoah_concurrent_gc) {\n-      heap->shenandoah_policy()->record_interrupted_old();\n-    }\n-    return false;\n-  }\n-  return true;\n-}\n-\n-bool ShenandoahControlThread::check_soft_max_changed() const {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  size_t new_soft_max = Atomic::load(&SoftMaxHeapSize);\n-  size_t old_soft_max = heap->soft_max_capacity();\n-  if (new_soft_max != old_soft_max) {\n-    new_soft_max = MAX2(heap->min_capacity(), new_soft_max);\n-    new_soft_max = MIN2(heap->max_capacity(), new_soft_max);\n-    if (new_soft_max != old_soft_max) {\n-      log_info(gc)(\"Soft Max Heap Size: \" SIZE_FORMAT \"%s -> \" SIZE_FORMAT \"%s\",\n-                   byte_size_in_proper_unit(old_soft_max), proper_unit_for_byte_size(old_soft_max),\n-                   byte_size_in_proper_unit(new_soft_max), proper_unit_for_byte_size(new_soft_max)\n-      );\n-      heap->set_soft_max_capacity(new_soft_max);\n-      return true;\n-    }\n-  }\n-  return false;\n-}\n-\n-void ShenandoahControlThread::service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause, bool do_old_gc_bootstrap) {\n+void ShenandoahControlThread::service_concurrent_normal_cycle(GCCause::Cause cause) {\n@@ -646,0 +348,1 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -648,3 +351,2 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  ShenandoahGCSession session(cause, generation);\n-  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  GCIdMark gc_id_mark;\n+  ShenandoahGCSession session(cause, heap->global_generation());\n@@ -652,2 +354,1 @@\n-  service_concurrent_cycle(heap, generation, cause, do_old_gc_bootstrap);\n-}\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n@@ -655,5 +356,1 @@\n-void ShenandoahControlThread::service_concurrent_cycle(ShenandoahHeap* heap,\n-                                                       ShenandoahGeneration* generation,\n-                                                       GCCause::Cause& cause,\n-                                                       bool do_old_gc_bootstrap) {\n-  ShenandoahConcurrentGC gc(generation, do_old_gc_bootstrap);\n+  ShenandoahConcurrentGC gc(heap->global_generation(), false);\n@@ -662,1 +359,2 @@\n-    generation->record_success_concurrent(gc.abbreviated());\n+    heap->global_generation()->heuristics()->record_success_concurrent(gc.abbreviated());\n+    heap->shenandoah_policy()->record_success_concurrent(false, gc.abbreviated());\n@@ -666,38 +364,0 @@\n-    assert(!generation->is_old(), \"Old GC takes a different control path\");\n-    \/\/ Concurrent young-gen collection degenerates to young\n-    \/\/ collection.  Same for global collections.\n-    _degen_generation = generation;\n-  }\n-  const char* msg;\n-  if (heap->mode()->is_generational()) {\n-    ShenandoahMmuTracker* mmu_tracker = heap->mmu_tracker();\n-    if (generation->is_young()) {\n-      if (heap->cancelled_gc()) {\n-        msg = (do_old_gc_bootstrap) ? \"At end of Interrupted Concurrent Bootstrap GC\":\n-                                      \"At end of Interrupted Concurrent Young GC\";\n-      } else {\n-        \/\/ We only record GC results if GC was successful\n-        msg = (do_old_gc_bootstrap) ? \"At end of Concurrent Bootstrap GC\":\n-                                      \"At end of Concurrent Young GC\";\n-        if (heap->collection_set()->has_old_regions()) {\n-          mmu_tracker->record_mixed(get_gc_id());\n-        } else if (do_old_gc_bootstrap) {\n-          mmu_tracker->record_bootstrap(get_gc_id());\n-        } else {\n-          mmu_tracker->record_young(get_gc_id());\n-        }\n-      }\n-    } else {\n-      assert(generation->is_global(), \"If not young, must be GLOBAL\");\n-      assert(!do_old_gc_bootstrap, \"Do not bootstrap with GLOBAL GC\");\n-      if (heap->cancelled_gc()) {\n-        msg = \"At end of Interrupted Concurrent GLOBAL GC\";\n-      } else {\n-        \/\/ We only record GC results if GC was successful\n-        msg = \"At end of Concurrent Global GC\";\n-        mmu_tracker->record_global(get_gc_id());\n-      }\n-    }\n-  } else {\n-    msg = heap->cancelled_gc() ? \"At end of cancelled GC\" :\n-                                 \"At end of GC\";\n@@ -705,1 +365,0 @@\n-  heap->log_heap_status(msg);\n@@ -710,28 +369,7 @@\n-  if (!heap->cancelled_gc()) {\n-    return false;\n-  }\n-\n-  if (in_graceful_shutdown()) {\n-    return true;\n-  }\n-\n-  assert(_degen_point == ShenandoahGC::_degenerated_outside_cycle,\n-         \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n-\n-  if (is_alloc_failure_gc()) {\n-    _degen_point = point;\n-    _preemption_requested.unset();\n-    return true;\n-  }\n-\n-  if (_preemption_requested.is_set()) {\n-    assert(_requested_generation == YOUNG, \"Only young GCs may preempt old.\");\n-    _preemption_requested.unset();\n-\n-    \/\/ Old generation marking is only cancellable during concurrent marking.\n-    \/\/ Once final mark is complete, the code does not check again for cancellation.\n-    \/\/ If old generation was cancelled for an allocation failure, we wouldn't\n-    \/\/ make it to this case. The calling code is responsible for forcing a\n-    \/\/ cancellation due to allocation failure into a degenerated cycle.\n-    _degen_point = point;\n-    heap->clear_cancelled_gc(false \/* clear oom handler *\/);\n+  if (heap->cancelled_gc()) {\n+    assert (is_alloc_failure_gc() || in_graceful_shutdown(), \"Cancel GC either for alloc failure GC, or gracefully exiting\");\n+    if (!in_graceful_shutdown()) {\n+      assert (_degen_point == ShenandoahGC::_degenerated_outside_cycle,\n+              \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n+      _degen_point = point;\n+    }\n@@ -740,2 +378,0 @@\n-\n-  fatal(\"Cancel GC either for alloc failure GC, or gracefully exiting, or to pause old generation marking\");\n@@ -751,1 +387,0 @@\n-\n@@ -759,3 +394,2 @@\n-void ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause,\n-                                                            ShenandoahGC::ShenandoahDegenPoint point) {\n-  assert(point != ShenandoahGC::_degenerated_unset, \"Degenerated point should be set\");\n+void ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahGC::ShenandoahDegenPoint point) {\n+  assert (point != ShenandoahGC::_degenerated_unset, \"Degenerated point should be set\");\n@@ -763,1 +397,0 @@\n-\n@@ -765,1 +398,1 @@\n-  ShenandoahGCSession session(cause, _degen_generation);\n+  ShenandoahGCSession session(cause, heap->global_generation());\n@@ -767,1 +400,1 @@\n-  ShenandoahDegenGC gc(point, _degen_generation);\n+  ShenandoahDegenGC gc(point, heap->global_generation());\n@@ -769,35 +402,0 @@\n-\n-  assert(heap->young_generation()->task_queues()->is_empty(), \"Unexpected young generation marking tasks\");\n-  if (_degen_generation->is_global()) {\n-    assert(heap->old_generation()->task_queues()->is_empty(), \"Unexpected old generation marking tasks\");\n-    assert(heap->global_generation()->task_queues()->is_empty(), \"Unexpected global generation marking tasks\");\n-  } else {\n-    assert(_degen_generation->is_young(), \"Expected degenerated young cycle, if not global.\");\n-    ShenandoahOldGeneration* old = heap->old_generation();\n-    if (old->state() == ShenandoahOldGeneration::BOOTSTRAPPING) {\n-      old->transition_to(ShenandoahOldGeneration::MARKING);\n-    }\n-  }\n-}\n-\n-void ShenandoahControlThread::service_uncommit(double shrink_before, size_t shrink_until) {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-\n-  \/\/ Determine if there is work to do. This avoids taking heap lock if there is\n-  \/\/ no work available, avoids spamming logs with superfluous logging messages,\n-  \/\/ and minimises the amount of work while locks are taken.\n-\n-  if (heap->committed() <= shrink_until) return;\n-\n-  bool has_work = false;\n-  for (size_t i = 0; i < heap->num_regions(); i++) {\n-    ShenandoahHeapRegion *r = heap->get_region(i);\n-    if (r->is_empty_committed() && (r->empty_time() < shrink_before)) {\n-      has_work = true;\n-      break;\n-    }\n-  }\n-\n-  if (has_work) {\n-    heap->entry_uncommit(shrink_before, shrink_until);\n-  }\n@@ -811,6 +409,0 @@\n-bool ShenandoahControlThread::is_implicit_gc(GCCause::Cause cause) const {\n-  return !is_explicit_gc(cause)\n-      && cause != GCCause::_shenandoah_concurrent_gc\n-      && cause != GCCause::_no_gc;\n-}\n-\n@@ -839,63 +431,0 @@\n-bool ShenandoahControlThread::request_concurrent_gc(ShenandoahGenerationType generation) {\n-  if (_preemption_requested.is_set() || _requested_gc_cause != GCCause::_no_gc || ShenandoahHeap::heap()->cancelled_gc()) {\n-    \/\/ Ignore subsequent requests from the heuristics\n-    log_debug(gc, thread)(\"Reject request for concurrent gc: preemption_requested: %s, gc_requested: %s, gc_cancelled: %s\",\n-                          BOOL_TO_STR(_preemption_requested.is_set()),\n-                          GCCause::to_string(_requested_gc_cause),\n-                          BOOL_TO_STR(ShenandoahHeap::heap()->cancelled_gc()));\n-    return false;\n-  }\n-\n-  if (gc_mode() == none) {\n-    GCCause::Cause existing = Atomic::cmpxchg(&_requested_gc_cause, GCCause::_no_gc, GCCause::_shenandoah_concurrent_gc);\n-    if (existing != GCCause::_no_gc) {\n-      log_debug(gc, thread)(\"Reject request for concurrent gc because another gc is pending: %s\", GCCause::to_string(existing));\n-      return false;\n-    }\n-\n-    _requested_generation = generation;\n-    notify_control_thread();\n-\n-    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n-    while (gc_mode() == none) {\n-      ml.wait();\n-    }\n-    return true;\n-  }\n-\n-  if (preempt_old_marking(generation)) {\n-    assert(gc_mode() == servicing_old, \"Expected to be servicing old, but was: %s.\", gc_mode_name(gc_mode()));\n-    GCCause::Cause existing = Atomic::cmpxchg(&_requested_gc_cause, GCCause::_no_gc, GCCause::_shenandoah_concurrent_gc);\n-    if (existing != GCCause::_no_gc) {\n-      log_debug(gc, thread)(\"Reject request to interrupt old gc because another gc is pending: %s\", GCCause::to_string(existing));\n-      return false;\n-    }\n-\n-    log_info(gc)(\"Preempting old generation mark to allow %s GC\", shenandoah_generation_name(generation));\n-    _requested_generation = generation;\n-    _preemption_requested.set();\n-    ShenandoahHeap::heap()->cancel_gc(GCCause::_shenandoah_concurrent_gc);\n-    notify_control_thread();\n-\n-    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n-    while (gc_mode() == servicing_old) {\n-      ml.wait();\n-    }\n-    return true;\n-  }\n-\n-  log_debug(gc, thread)(\"Reject request for concurrent gc: mode: %s, allow_old_preemption: %s\",\n-                        gc_mode_name(gc_mode()),\n-                        BOOL_TO_STR(_allow_old_preemption.is_set()));\n-  return false;\n-}\n-\n-void ShenandoahControlThread::notify_control_thread() {\n-  MonitorLocker locker(&_control_lock, Mutex::_no_safepoint_check_flag);\n-  _control_lock.notify();\n-}\n-\n-bool ShenandoahControlThread::preempt_old_marking(ShenandoahGenerationType generation) {\n-  return (generation == YOUNG) && _allow_old_preemption.try_unset();\n-}\n-\n@@ -916,7 +445,5 @@\n-    \/\/ This races with the regulator thread to start a concurrent gc and the\n-    \/\/ control thread to clear it at the start of a cycle. Threads here are\n-    \/\/ allowed to escalate a heuristic's request for concurrent gc.\n-    GCCause::Cause existing = Atomic::xchg(&_requested_gc_cause, cause);\n-    if (existing != GCCause::_no_gc) {\n-      log_debug(gc, thread)(\"GC request supersedes existing request: %s\", GCCause::to_string(existing));\n-    }\n+    \/\/ Although setting gc request is under _gc_waiters_lock, but read side (run_service())\n+    \/\/ does not take the lock. We need to enforce following order, so that read side sees\n+    \/\/ latest requested gc cause when the flag is set.\n+    _requested_gc_cause = cause;\n+    _gc_requested.set();\n@@ -924,1 +451,0 @@\n-    notify_control_thread();\n@@ -936,1 +462,0 @@\n-  bool is_humongous = req.size() > ShenandoahHeapRegion::region_size_words();\n@@ -938,1 +463,1 @@\n-  if (try_set_alloc_failure_gc(is_humongous)) {\n+  if (try_set_alloc_failure_gc()) {\n@@ -943,0 +468,1 @@\n+\n@@ -958,1 +484,0 @@\n-  bool is_humongous = (words > ShenandoahHeapRegion::region_size_words());\n@@ -960,1 +485,1 @@\n-  if (try_set_alloc_failure_gc(is_humongous)) {\n+  if (try_set_alloc_failure_gc()) {\n@@ -972,1 +497,0 @@\n-  _humongous_alloc_failure_gc.unset();\n@@ -977,4 +501,1 @@\n-bool ShenandoahControlThread::try_set_alloc_failure_gc(bool is_humongous) {\n-  if (is_humongous) {\n-    _humongous_alloc_failure_gc.try_set();\n-  }\n+bool ShenandoahControlThread::try_set_alloc_failure_gc() {\n@@ -989,0 +510,1 @@\n+  _gc_requested.unset();\n@@ -995,17 +517,4 @@\n-}\n-\n-void ShenandoahControlThread::pacing_notify_alloc(size_t words) {\n-  assert(ShenandoahPacing, \"should only call when pacing is enabled\");\n-  Atomic::add(&_allocs_seen, words, memory_order_relaxed);\n-}\n-\n-void ShenandoahControlThread::reset_gc_id() {\n-  Atomic::store(&_gc_id, (size_t)0);\n-}\n-\n-void ShenandoahControlThread::update_gc_id() {\n-  Atomic::inc(&_gc_id);\n-}\n-\n-size_t ShenandoahControlThread::get_gc_id() {\n-  return Atomic::load(&_gc_id);\n+  \/\/ Notify that something had changed.\n+  if (_heap_changed.is_unset()) {\n+    _heap_changed.set();\n+  }\n@@ -1017,37 +526,0 @@\n-\n-void ShenandoahControlThread::prepare_for_graceful_shutdown() {\n-  _graceful_shutdown.set();\n-}\n-\n-bool ShenandoahControlThread::in_graceful_shutdown() {\n-  return _graceful_shutdown.is_set();\n-}\n-\n-const char* ShenandoahControlThread::gc_mode_name(ShenandoahControlThread::GCMode mode) {\n-  switch (mode) {\n-    case none:              return \"idle\";\n-    case concurrent_normal: return \"normal\";\n-    case stw_degenerated:   return \"degenerated\";\n-    case stw_full:          return \"full\";\n-    case servicing_old:     return \"old\";\n-    case bootstrapping_old: return \"bootstrap\";\n-    default:                return \"unknown\";\n-  }\n-}\n-\n-void ShenandoahControlThread::set_gc_mode(ShenandoahControlThread::GCMode new_mode) {\n-  if (_mode != new_mode) {\n-    log_info(gc)(\"Transition from: %s to: %s\", gc_mode_name(_mode), gc_mode_name(new_mode));\n-    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n-    _mode = new_mode;\n-    ml.notify_all();\n-  }\n-}\n-\n-ShenandoahGenerationType ShenandoahControlThread::select_global_generation() {\n-  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n-    return GLOBAL_GEN;\n-  } else {\n-    return GLOBAL_NON_GEN;\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":123,"deletions":651,"binary":false,"changes":774,"status":"modified"},{"patch":"@@ -3,1 +3,0 @@\n- * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -32,1 +31,0 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n@@ -37,1 +35,1 @@\n-class ShenandoahControlThread: public ConcurrentGCThread {\n+class ShenandoahControlThread: public ShenandoahController {\n@@ -41,0 +39,7 @@\n+  typedef enum {\n+    none,\n+    concurrent_normal,\n+    stw_degenerated,\n+    stw_full\n+  } GCMode;\n+\n@@ -46,2 +51,0 @@\n-  Monitor _control_lock;\n-  Monitor _regulator_lock;\n@@ -50,9 +53,0 @@\n-  typedef enum {\n-    none,\n-    concurrent_normal,\n-    stw_degenerated,\n-    stw_full,\n-    bootstrapping_old,\n-    servicing_old\n-  } GCMode;\n-\n@@ -62,2 +56,0 @@\n-  size_t get_gc_id();\n-\n@@ -65,2 +57,1 @@\n-  ShenandoahSharedFlag _allow_old_preemption;\n-  ShenandoahSharedFlag _preemption_requested;\n+  ShenandoahSharedFlag _gc_requested;\n@@ -68,5 +59,2 @@\n-  ShenandoahSharedFlag _humongous_alloc_failure_gc;\n-  ShenandoahSharedFlag _graceful_shutdown;\n-\n-  GCCause::Cause  _requested_gc_cause;\n-  volatile ShenandoahGenerationType _requested_generation;\n+  ShenandoahSharedFlag _heap_changed;\n+  GCCause::Cause       _requested_gc_cause;\n@@ -74,9 +62,0 @@\n-  ShenandoahGeneration* _degen_generation;\n-\n-  shenandoah_padding(0);\n-  volatile size_t _allocs_seen;\n-  shenandoah_padding(1);\n-  volatile size_t _gc_id;\n-  shenandoah_padding(2);\n-  volatile GCMode _mode;\n-  shenandoah_padding(3);\n@@ -84,1 +63,0 @@\n-  \/\/ Returns true if the cycle has been cancelled or degenerated.\n@@ -86,4 +64,1 @@\n-\n-  \/\/ Returns true if the old generation marking completed (i.e., final mark executed for old generation).\n-  bool resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause);\n-  void service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause, bool reset_old_bitmap_specially);\n+  void service_concurrent_normal_cycle(GCCause::Cause cause);\n@@ -92,4 +67,0 @@\n-  void service_uncommit(double shrink_before, size_t shrink_until);\n-\n-  \/\/ Return true if setting the flag which indicates allocation failure succeeds.\n-  bool try_set_alloc_failure_gc(bool is_humongous);\n@@ -97,1 +68,1 @@\n-  \/\/ Notify threads waiting for GC to complete.\n+  bool try_set_alloc_failure_gc();\n@@ -99,2 +70,0 @@\n-\n-  \/\/ True if allocation failure flag has been set.\n@@ -103,3 +72,0 @@\n-  void reset_gc_id();\n-  void update_gc_id();\n-\n@@ -113,9 +79,0 @@\n-  bool is_implicit_gc(GCCause::Cause cause) const;\n-\n-  \/\/ Returns true if the old generation marking was interrupted to allow a young cycle.\n-  bool preempt_old_marking(ShenandoahGenerationType generation);\n-\n-  \/\/ Returns true if the soft maximum heap has been changed using management APIs.\n-  bool check_soft_max_changed() const;\n-\n-  void process_phase_timings(const ShenandoahHeap* heap);\n@@ -137,2 +94,0 @@\n-  \/\/ Return true if the request to start a concurrent GC for the given generation succeeded.\n-  bool request_concurrent_gc(ShenandoahGenerationType generation);\n@@ -142,2 +97,0 @@\n-  void pacing_notify_alloc(size_t words);\n-\n@@ -145,26 +98,0 @@\n-  void prepare_for_graceful_shutdown();\n-  bool in_graceful_shutdown();\n-\n-  void service_concurrent_normal_cycle(ShenandoahHeap* heap,\n-                                       const ShenandoahGenerationType generation,\n-                                       GCCause::Cause cause);\n-\n-  void service_concurrent_old_cycle(ShenandoahHeap* heap,\n-                                    GCCause::Cause &cause);\n-\n-  void set_gc_mode(GCMode new_mode);\n-  GCMode gc_mode() {\n-    return _mode;\n-  }\n-\n-  static ShenandoahGenerationType select_global_generation();\n-\n- private:\n-  static const char* gc_mode_name(GCMode mode);\n-  void notify_control_thread();\n-\n-  void service_concurrent_cycle(ShenandoahHeap* heap,\n-                                ShenandoahGeneration* generation,\n-                                GCCause::Cause &cause,\n-                                bool do_old_gc_bootstrap);\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.hpp","additions":13,"deletions":86,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -0,0 +1,52 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shenandoah\/shenandoahController.hpp\"\n+\n+void ShenandoahController::pacing_notify_alloc(size_t words) {\n+  assert(ShenandoahPacing, \"should only call when pacing is enabled\");\n+  Atomic::add(&_allocs_seen, words, memory_order_relaxed);\n+}\n+\n+size_t ShenandoahController::reset_allocs_seen() {\n+  return Atomic::xchg(&_allocs_seen, (size_t)0, memory_order_relaxed);\n+}\n+\n+void ShenandoahController::prepare_for_graceful_shutdown() {\n+  _graceful_shutdown.set();\n+}\n+\n+bool ShenandoahController::in_graceful_shutdown() {\n+  return _graceful_shutdown.is_set();\n+}\n+\n+void ShenandoahController::update_gc_id() {\n+  Atomic::inc(&_gc_id);\n+}\n+\n+size_t ShenandoahController::get_gc_id() {\n+  return Atomic::load(&_gc_id);\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahController.cpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"added"},{"patch":"@@ -0,0 +1,88 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef LINUX_X86_64_SERVER_SLOWDEBUG_SHENANDOAHCONTROLLER_HPP\n+#define LINUX_X86_64_SERVER_SLOWDEBUG_SHENANDOAHCONTROLLER_HPP\n+\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shared\/concurrentGCThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n+\n+\/**\n+ * This interface exposes methods necessary for the heap to interact\n+ * with the threads responsible for driving the collection cycle.\n+ *\/\n+class ShenandoahController: public ConcurrentGCThread {\n+private:\n+  ShenandoahSharedFlag _graceful_shutdown;\n+\n+  shenandoah_padding(0);\n+  volatile size_t _allocs_seen;\n+  shenandoah_padding(1);\n+  volatile size_t _gc_id;\n+  shenandoah_padding(2);\n+\n+public:\n+  ShenandoahController():\n+    ConcurrentGCThread(),\n+    _allocs_seen(0),\n+    _gc_id(0) { }\n+\n+  \/\/ This is invoked by the heap when it allocates an object\n+  \/\/ in a region for the first time. It is also called when regions\n+  \/\/ are uncommitted.\n+  virtual void notify_heap_changed() = 0;\n+\n+  \/\/ Request a collection cycle. This handles \"explicit\" gc requests\n+  \/\/ like System.gc and \"implicit\" gc requests, like metaspace oom.\n+  virtual void request_gc(GCCause::Cause cause) = 0;\n+\n+  \/\/ Invoked for allocation failures during evacuation. This cancels\n+  \/\/ the collection cycle without blocking.\n+  virtual void handle_alloc_failure_evac(size_t words) = 0;\n+\n+  \/\/ This cancels the collection cycle and has an option to block\n+  \/\/ until another cycle runs and clears the alloc failure gc flag.\n+  virtual void handle_alloc_failure(ShenandoahAllocRequest& req, bool block) = 0;\n+\n+  \/\/ This is called for every allocation. The control thread accumulates\n+  \/\/ this value when idle. During the gc cycle, the control resets it\n+  \/\/ and reports it to the pacer.\n+  void pacing_notify_alloc(size_t words);\n+  size_t reset_allocs_seen();\n+\n+  \/\/ These essentially allows to cancel a collection cycle for the\n+  \/\/ purpose of shutting down the JVM, without trying to start a degenerated\n+  \/\/ cycle.\n+  void prepare_for_graceful_shutdown();\n+  bool in_graceful_shutdown();\n+\n+\n+  \/\/ Returns the internal gc count used by the control thread. Probably\n+  \/\/ doesn't need to be exposed.\n+  size_t get_gc_id();\n+  void update_gc_id();\n+};\n+#endif \/\/LINUX_X86_64_SERVER_SLOWDEBUG_SHENANDOAHCONTROLLER_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahController.hpp","additions":88,"deletions":0,"binary":false,"changes":88,"status":"added"},{"patch":"@@ -0,0 +1,984 @@\n+\/*\n+ * Copyright (c) 2013, 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright (C) 2022 THL A29 Limited, a Tencent company. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahConcurrentGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalControlThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahDegeneratedGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFullGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahPhaseTimings.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMark.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMonitoringSupport.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahRootProcessor.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+#include \"gc\/shenandoah\/shenandoahVMOperations.hpp\"\n+#include \"gc\/shenandoah\/shenandoahWorkerPolicy.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"memory\/metaspaceUtils.hpp\"\n+#include \"memory\/metaspaceStats.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+\n+ShenandoahGenerationalControlThread::ShenandoahGenerationalControlThread() :\n+  ShenandoahController(),\n+  _alloc_failure_waiters_lock(Mutex::safepoint - 2, \"ShenandoahAllocFailureGC_lock\", true),\n+  _gc_waiters_lock(Mutex::safepoint - 2, \"ShenandoahRequestedGC_lock\", true),\n+  _control_lock(Mutex::nosafepoint - 2, \"ShenandoahControlGC_lock\", true),\n+  _regulator_lock(Mutex::nosafepoint - 2, \"ShenandoahRegulatorGC_lock\", true),\n+  _requested_gc_cause(GCCause::_no_gc),\n+  _requested_generation(select_global_generation()),\n+  _degen_point(ShenandoahGC::_degenerated_outside_cycle),\n+  _degen_generation(nullptr),\n+  _mode(none) {\n+  set_name(\"Shenandoah Control Thread\");\n+  create_and_start();\n+}\n+\n+void ShenandoahGenerationalControlThread::run_service() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n+  const GCMode default_mode = concurrent_normal;\n+  ShenandoahGenerationType generation = select_global_generation();\n+\n+  double last_shrink_time = os::elapsedTime();\n+  uint age_period = 0;\n+\n+  \/\/ Shrink period avoids constantly polling regions for shrinking.\n+  \/\/ Having a period 10x lower than the delay would mean we hit the\n+  \/\/ shrinking with lag of less than 1\/10-th of true delay.\n+  \/\/ ShenandoahUncommitDelay is in msecs, but shrink_period is in seconds.\n+  const double shrink_period = (double)ShenandoahUncommitDelay \/ 1000 \/ 10;\n+\n+  ShenandoahCollectorPolicy* const policy = heap->shenandoah_policy();\n+\n+  \/\/ Heuristics are notified of allocation failures here and other outcomes\n+  \/\/ of the cycle. They're also used here to control whether the Nth consecutive\n+  \/\/ degenerated cycle should be 'promoted' to a full cycle. The decision to\n+  \/\/ trigger a cycle or not is evaluated on the regulator thread.\n+  ShenandoahHeuristics* global_heuristics = heap->global_generation()->heuristics();\n+  while (!in_graceful_shutdown() && !should_terminate()) {\n+    \/\/ Figure out if we have pending requests.\n+    const bool alloc_failure_pending = _alloc_failure_gc.is_set();\n+    const bool humongous_alloc_failure_pending = _humongous_alloc_failure_gc.is_set();\n+\n+    GCCause::Cause cause = Atomic::xchg(&_requested_gc_cause, GCCause::_no_gc);\n+\n+    const bool explicit_gc_requested = is_explicit_gc(cause);\n+    const bool implicit_gc_requested = is_implicit_gc(cause);\n+\n+    \/\/ This control loop iteration has seen this much allocation.\n+    const size_t allocs_seen = reset_allocs_seen();\n+\n+    \/\/ Check if we have seen a new target for soft max heap size.\n+    const bool soft_max_changed = heap->check_soft_max_changed();\n+\n+    \/\/ Choose which GC mode to run in. The block below should select a single mode.\n+    set_gc_mode(none);\n+    ShenandoahGC::ShenandoahDegenPoint degen_point = ShenandoahGC::_degenerated_unset;\n+\n+    if (alloc_failure_pending) {\n+      \/\/ Allocation failure takes precedence: we have to deal with it first thing\n+      log_info(gc)(\"Trigger: Handle Allocation Failure\");\n+\n+      cause = GCCause::_allocation_failure;\n+\n+      \/\/ Consume the degen point, and seed it with default value\n+      degen_point = _degen_point;\n+      _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n+\n+      if (degen_point == ShenandoahGC::_degenerated_outside_cycle) {\n+        _degen_generation = heap->mode()->is_generational() ?\n+                heap->young_generation() : heap->global_generation();\n+      } else {\n+        assert(_degen_generation != nullptr, \"Need to know which generation to resume\");\n+      }\n+\n+      ShenandoahHeuristics* heuristics = _degen_generation->heuristics();\n+      generation = _degen_generation->type();\n+      bool old_gen_evacuation_failed = heap->clear_old_evacuation_failure();\n+\n+      \/\/ Do not bother with degenerated cycle if old generation evacuation failed or if humongous allocation failed\n+      if (ShenandoahDegeneratedGC && heuristics->should_degenerate_cycle() &&\n+          !old_gen_evacuation_failed && !humongous_alloc_failure_pending) {\n+        heuristics->record_allocation_failure_gc();\n+        policy->record_alloc_failure_to_degenerated(degen_point);\n+        set_gc_mode(stw_degenerated);\n+      } else {\n+        \/\/ TODO: if humongous_alloc_failure_pending, there might be value in trying a \"compacting\" degen before\n+        \/\/ going all the way to full.  But it's a lot of work to implement this, and it may not provide value.\n+        \/\/ A compacting degen can move young regions around without doing full old-gen mark (relying upon the\n+        \/\/ remembered set scan), so it might be faster than a full gc.\n+        \/\/\n+        \/\/ Longer term, think about how to defragment humongous memory concurrently.\n+\n+        heuristics->record_allocation_failure_gc();\n+        policy->record_alloc_failure_to_full();\n+        generation = select_global_generation();\n+        set_gc_mode(stw_full);\n+      }\n+    } else if (explicit_gc_requested) {\n+      generation = select_global_generation();\n+      log_info(gc)(\"Trigger: Explicit GC request (%s)\", GCCause::to_string(cause));\n+\n+      global_heuristics->record_requested_gc();\n+\n+      if (ExplicitGCInvokesConcurrent) {\n+        policy->record_explicit_to_concurrent();\n+        set_gc_mode(default_mode);\n+        \/\/ Unload and clean up everything\n+        heap->set_unload_classes(global_heuristics->can_unload_classes());\n+      } else {\n+        policy->record_explicit_to_full();\n+        set_gc_mode(stw_full);\n+      }\n+    } else if (implicit_gc_requested) {\n+      generation = select_global_generation();\n+      log_info(gc)(\"Trigger: Implicit GC request (%s)\", GCCause::to_string(cause));\n+\n+      global_heuristics->record_requested_gc();\n+\n+      if (ShenandoahImplicitGCInvokesConcurrent) {\n+        policy->record_implicit_to_concurrent();\n+        set_gc_mode(default_mode);\n+\n+        \/\/ Unload and clean up everything\n+        heap->set_unload_classes(global_heuristics->can_unload_classes());\n+      } else {\n+        policy->record_implicit_to_full();\n+        set_gc_mode(stw_full);\n+      }\n+    } else {\n+      \/\/ We should only be here if the regulator requested a cycle or if\n+      \/\/ there is an old generation mark in progress.\n+      if (cause == GCCause::_shenandoah_concurrent_gc) {\n+        if (_requested_generation == OLD && heap->doing_mixed_evacuations()) {\n+          \/\/ If a request to start an old cycle arrived while an old cycle was running, but _before_\n+          \/\/ it chose any regions for evacuation we don't want to start a new old cycle. Rather, we want\n+          \/\/ the heuristic to run a young collection so that we can evacuate some old regions.\n+          assert(!heap->is_concurrent_old_mark_in_progress(), \"Should not be running mixed collections and concurrent marking\");\n+          generation = YOUNG;\n+        } else {\n+          generation = _requested_generation;\n+        }\n+\n+        \/\/ preemption was requested or this is a regular cycle\n+        set_gc_mode(default_mode);\n+\n+        \/\/ Don't start a new old marking if there is one already in progress\n+        if (generation == OLD && heap->is_concurrent_old_mark_in_progress()) {\n+          set_gc_mode(servicing_old);\n+        }\n+\n+        if (generation == select_global_generation()) {\n+          heap->set_unload_classes(global_heuristics->should_unload_classes());\n+        } else {\n+          heap->set_unload_classes(false);\n+        }\n+      } else if (heap->is_concurrent_old_mark_in_progress() || heap->is_prepare_for_old_mark_in_progress()) {\n+        \/\/ Nobody asked us to do anything, but we have an old-generation mark or old-generation preparation for\n+        \/\/ mixed evacuation in progress, so resume working on that.\n+        log_info(gc)(\"Resume old GC: marking is%s in progress, preparing is%s in progress\",\n+                     heap->is_concurrent_old_mark_in_progress() ? \"\" : \" NOT\",\n+                     heap->is_prepare_for_old_mark_in_progress() ? \"\" : \" NOT\");\n+\n+        cause = GCCause::_shenandoah_concurrent_gc;\n+        generation = OLD;\n+        set_gc_mode(servicing_old);\n+        heap->set_unload_classes(false);\n+      }\n+    }\n+\n+    const bool gc_requested = (gc_mode() != none);\n+    assert (!gc_requested || cause != GCCause::_no_gc, \"GC cause should be set\");\n+\n+    if (gc_requested) {\n+      \/\/ Blow away all soft references on this cycle, if handling allocation failure,\n+      \/\/ either implicit or explicit GC request, or we are requested to do so unconditionally.\n+      if (generation == select_global_generation() && (alloc_failure_pending || implicit_gc_requested || explicit_gc_requested || ShenandoahAlwaysClearSoftRefs)) {\n+        heap->soft_ref_policy()->set_should_clear_all_soft_refs(true);\n+      }\n+\n+      \/\/ GC is starting, bump the internal ID\n+      update_gc_id();\n+\n+      heap->reset_bytes_allocated_since_gc_start();\n+\n+      MetaspaceCombinedStats meta_sizes = MetaspaceUtils::get_combined_statistics();\n+\n+      \/\/ If GC was requested, we are sampling the counters even without actual triggers\n+      \/\/ from allocation machinery. This captures GC phases more accurately.\n+      heap->set_forced_counters_update(true);\n+\n+      \/\/ If GC was requested, we better dump freeset data for performance debugging\n+      {\n+        ShenandoahHeapLocker locker(heap->lock());\n+        heap->free_set()->log_status();\n+      }\n+      \/\/ In case this is a degenerated cycle, remember whether original cycle was aging.\n+      const bool was_aging_cycle = heap->is_aging_cycle();\n+      heap->set_aging_cycle(false);\n+\n+      switch (gc_mode()) {\n+        case concurrent_normal: {\n+          \/\/ At this point:\n+          \/\/  if (generation == YOUNG), this is a normal YOUNG cycle\n+          \/\/  if (generation == OLD), this is a bootstrap OLD cycle\n+          \/\/  if (generation == GLOBAL), this is a GLOBAL cycle triggered by System.gc()\n+          \/\/ In all three cases, we want to age old objects if this is an aging cycle\n+          if (age_period-- == 0) {\n+             heap->set_aging_cycle(true);\n+             age_period = ShenandoahAgingCyclePeriod - 1;\n+          }\n+          service_concurrent_normal_cycle(heap, generation, cause);\n+          break;\n+        }\n+        case stw_degenerated: {\n+          heap->set_aging_cycle(was_aging_cycle);\n+          service_stw_degenerated_cycle(cause, degen_point);\n+          break;\n+        }\n+        case stw_full: {\n+          if (age_period-- == 0) {\n+            heap->set_aging_cycle(true);\n+            age_period = ShenandoahAgingCyclePeriod - 1;\n+          }\n+          service_stw_full_cycle(cause);\n+          break;\n+        }\n+        case servicing_old: {\n+          assert(generation == OLD, \"Expected old generation here\");\n+          GCIdMark gc_id_mark;\n+          service_concurrent_old_cycle(heap, cause);\n+          break;\n+        }\n+        default:\n+          ShouldNotReachHere();\n+      }\n+\n+      \/\/ If this was the requested GC cycle, notify waiters about it\n+      if (explicit_gc_requested || implicit_gc_requested) {\n+        notify_gc_waiters();\n+      }\n+\n+      \/\/ If this was the allocation failure GC cycle, notify waiters about it\n+      if (alloc_failure_pending) {\n+        notify_alloc_failure_waiters();\n+      }\n+\n+      \/\/ Report current free set state at the end of cycle, whether\n+      \/\/ it is a normal completion, or the abort.\n+      {\n+        ShenandoahHeapLocker locker(heap->lock());\n+        heap->free_set()->log_status();\n+\n+        \/\/ Notify Universe about new heap usage. This has implications for\n+        \/\/ global soft refs policy, and we better report it every time heap\n+        \/\/ usage goes down.\n+        heap->update_capacity_and_used_at_gc();\n+\n+        \/\/ Signal that we have completed a visit to all live objects.\n+        heap->record_whole_heap_examined_timestamp();\n+      }\n+\n+      \/\/ Disable forced counters update, and update counters one more time\n+      \/\/ to capture the state at the end of GC session.\n+      heap->handle_force_counters_update();\n+      heap->set_forced_counters_update(false);\n+\n+      \/\/ Retract forceful part of soft refs policy\n+      heap->soft_ref_policy()->set_should_clear_all_soft_refs(false);\n+\n+      \/\/ Clear metaspace oom flag, if current cycle unloaded classes\n+      if (heap->unload_classes()) {\n+        global_heuristics->clear_metaspace_oom();\n+      }\n+\n+      process_phase_timings(heap);\n+\n+      \/\/ Print Metaspace change following GC (if logging is enabled).\n+      MetaspaceUtils::print_metaspace_change(meta_sizes);\n+\n+      \/\/ GC is over, we are at idle now\n+      if (ShenandoahPacing) {\n+        heap->pacer()->setup_for_idle();\n+      }\n+    } else {\n+      \/\/ Allow pacer to know we have seen this many allocations\n+      if (ShenandoahPacing && (allocs_seen > 0)) {\n+        heap->pacer()->report_alloc(allocs_seen);\n+      }\n+    }\n+\n+    double current = os::elapsedTime();\n+\n+    if (ShenandoahUncommit && (explicit_gc_requested || soft_max_changed || (current - last_shrink_time > shrink_period))) {\n+      \/\/ Explicit GC tries to uncommit everything down to min capacity.\n+      \/\/ Soft max change tries to uncommit everything down to target capacity.\n+      \/\/ Periodic uncommit tries to uncommit suitable regions down to min capacity.\n+\n+      double shrink_before = (explicit_gc_requested || soft_max_changed) ?\n+                             current :\n+                             current - (ShenandoahUncommitDelay \/ 1000.0);\n+\n+      size_t shrink_until = soft_max_changed ?\n+                             heap->soft_max_capacity() :\n+                             heap->min_capacity();\n+\n+      heap->service_uncommit(shrink_before, shrink_until);\n+      heap->phase_timings()->flush_cycle_to_global();\n+      last_shrink_time = current;\n+    }\n+\n+    \/\/ Wait for ShenandoahControlIntervalMax unless there was an allocation failure or another request was made mid-cycle.\n+    if (!is_alloc_failure_gc() && _requested_gc_cause == GCCause::_no_gc) {\n+      \/\/ The timed wait is necessary because this thread has a responsibility to send\n+      \/\/ 'alloc_words' to the pacer when it does not perform a GC.\n+      MonitorLocker lock(&_control_lock, Mutex::_no_safepoint_check_flag);\n+      lock.wait(ShenandoahControlIntervalMax);\n+    }\n+  }\n+\n+  \/\/ Wait for the actual stop(), can't leave run_service() earlier.\n+  while (!should_terminate()) {\n+    os::naked_short_sleep(ShenandoahControlIntervalMin);\n+  }\n+}\n+\n+void ShenandoahGenerationalControlThread::process_phase_timings(const ShenandoahHeap* heap) {\n+  \/\/ Commit worker statistics to cycle data\n+  heap->phase_timings()->flush_par_workers_to_cycle();\n+  if (ShenandoahPacing) {\n+    heap->pacer()->flush_stats_to_cycle();\n+  }\n+\n+  ShenandoahEvacuationTracker* evac_tracker = heap->evac_tracker();\n+  ShenandoahCycleStats         evac_stats   = evac_tracker->flush_cycle_to_global();\n+\n+  \/\/ Print GC stats for current cycle\n+  {\n+    LogTarget(Info, gc, stats) lt;\n+    if (lt.is_enabled()) {\n+      ResourceMark rm;\n+      LogStream ls(lt);\n+      heap->phase_timings()->print_cycle_on(&ls);\n+      evac_tracker->print_evacuations_on(&ls, &evac_stats.workers,\n+                                              &evac_stats.mutators);\n+      if (ShenandoahPacing) {\n+        heap->pacer()->print_cycle_on(&ls);\n+      }\n+    }\n+  }\n+\n+  \/\/ Commit statistics to globals\n+  heap->phase_timings()->flush_cycle_to_global();\n+}\n+\n+\/\/ Young and old concurrent cycles are initiated by the regulator. Implicit\n+\/\/ and explicit GC requests are handled by the controller thread and always\n+\/\/ run a global cycle (which is concurrent by default, but may be overridden\n+\/\/ by command line options). Old cycles always degenerate to a global cycle.\n+\/\/ Young cycles are degenerated to complete the young cycle.  Young\n+\/\/ and old degen may upgrade to Full GC.  Full GC may also be\n+\/\/ triggered directly by a System.gc() invocation.\n+\/\/\n+\/\/\n+\/\/      +-----+ Idle +-----+-----------+---------------------+\n+\/\/      |         +        |           |                     |\n+\/\/      |         |        |           |                     |\n+\/\/      |         |        v           |                     |\n+\/\/      |         |  Bootstrap Old +-- | ------------+       |\n+\/\/      |         |   +                |             |       |\n+\/\/      |         |   |                |             |       |\n+\/\/      |         v   v                v             v       |\n+\/\/      |    Resume Old <----------+ Young +--> Young Degen  |\n+\/\/      |     +  +   ^                            +  +       |\n+\/\/      v     |  |   |                            |  |       |\n+\/\/   Global <-+  |   +----------------------------+  |       |\n+\/\/      +        |                                   |       |\n+\/\/      |        v                                   v       |\n+\/\/      +--->  Global Degen +--------------------> Full <----+\n+\/\/\n+void ShenandoahGenerationalControlThread::service_concurrent_normal_cycle(ShenandoahHeap* heap,\n+                                                              const ShenandoahGenerationType generation,\n+                                                              GCCause::Cause cause) {\n+  GCIdMark gc_id_mark;\n+  ShenandoahGeneration* the_generation = nullptr;\n+  switch (generation) {\n+    case YOUNG: {\n+      \/\/ Run a young cycle. This might or might not, have interrupted an ongoing\n+      \/\/ concurrent mark in the old generation. We need to think about promotions\n+      \/\/ in this case. Promoted objects should be above the TAMS in the old regions\n+      \/\/ they end up in, but we have to be sure we don't promote into any regions\n+      \/\/ that are in the cset.\n+      log_info(gc, ergo)(\"Start GC cycle (YOUNG)\");\n+      the_generation = heap->young_generation();\n+      service_concurrent_cycle(the_generation, cause, false);\n+      break;\n+    }\n+    case OLD: {\n+      log_info(gc, ergo)(\"Start GC cycle (OLD)\");\n+      the_generation = heap->old_generation();\n+      service_concurrent_old_cycle(heap, cause);\n+      break;\n+    }\n+    case GLOBAL_GEN: {\n+      log_info(gc, ergo)(\"Start GC cycle (GLOBAL)\");\n+      the_generation = heap->global_generation();\n+      service_concurrent_cycle(the_generation, cause, false);\n+      break;\n+    }\n+    case GLOBAL_NON_GEN: {\n+      log_info(gc, ergo)(\"Start GC cycle\");\n+      the_generation = heap->global_generation();\n+      service_concurrent_cycle(the_generation, cause, false);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void ShenandoahGenerationalControlThread::service_concurrent_old_cycle(ShenandoahHeap* heap, GCCause::Cause &cause) {\n+  ShenandoahOldGeneration* old_generation = heap->old_generation();\n+  ShenandoahYoungGeneration* young_generation = heap->young_generation();\n+  ShenandoahOldGeneration::State original_state = old_generation->state();\n+\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+\n+  switch (original_state) {\n+    case ShenandoahOldGeneration::FILLING: {\n+      _allow_old_preemption.set();\n+      old_generation->entry_coalesce_and_fill();\n+      _allow_old_preemption.unset();\n+\n+      \/\/ Before bootstrapping begins, we must acknowledge any cancellation request.\n+      \/\/ If the gc has not been cancelled, this does nothing. If it has been cancelled,\n+      \/\/ this will clear the cancellation request and exit before starting the bootstrap\n+      \/\/ phase. This will allow the young GC cycle to proceed normally. If we do not\n+      \/\/ acknowledge the cancellation request, the subsequent young cycle will observe\n+      \/\/ the request and essentially cancel itself.\n+      if (check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle)) {\n+        log_info(gc)(\"Preparation for old generation cycle was cancelled\");\n+        return;\n+      }\n+\n+      \/\/ Coalescing threads completed and nothing was cancelled. it is safe to transition from this state.\n+      old_generation->transition_to(ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP);\n+      return;\n+    }\n+    case ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP:\n+      old_generation->transition_to(ShenandoahOldGeneration::BOOTSTRAPPING);\n+    case ShenandoahOldGeneration::BOOTSTRAPPING: {\n+      \/\/ Configure the young generation's concurrent mark to put objects in\n+      \/\/ old regions into the concurrent mark queues associated with the old\n+      \/\/ generation. The young cycle will run as normal except that rather than\n+      \/\/ ignore old references it will mark and enqueue them in the old concurrent\n+      \/\/ task queues but it will not traverse them.\n+      set_gc_mode(bootstrapping_old);\n+      young_generation->set_old_gen_task_queues(old_generation->task_queues());\n+      ShenandoahGCSession session(cause, young_generation);\n+      service_concurrent_cycle(heap, young_generation, cause, true);\n+      process_phase_timings(heap);\n+      if (heap->cancelled_gc()) {\n+        \/\/ Young generation bootstrap cycle has failed. Concurrent mark for old generation\n+        \/\/ is going to resume after degenerated bootstrap cycle completes.\n+        log_info(gc)(\"Bootstrap cycle for old generation was cancelled\");\n+        return;\n+      }\n+\n+      \/\/ Reset the degenerated point. Normally this would happen at the top\n+      \/\/ of the control loop, but here we have just completed a young cycle\n+      \/\/ which has bootstrapped the old concurrent marking.\n+      _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n+\n+      \/\/ From here we will 'resume' the old concurrent mark. This will skip reset\n+      \/\/ and init mark for the concurrent mark. All of that work will have been\n+      \/\/ done by the bootstrapping young cycle.\n+      set_gc_mode(servicing_old);\n+      old_generation->transition_to(ShenandoahOldGeneration::MARKING);\n+    }\n+    case ShenandoahOldGeneration::MARKING: {\n+      ShenandoahGCSession session(cause, old_generation);\n+      bool marking_complete = resume_concurrent_old_cycle(old_generation, cause);\n+      if (marking_complete) {\n+        assert(old_generation->state() != ShenandoahOldGeneration::MARKING, \"Should not still be marking\");\n+        if (original_state == ShenandoahOldGeneration::MARKING) {\n+          heap->mmu_tracker()->record_old_marking_increment(true);\n+          heap->log_heap_status(\"At end of Concurrent Old Marking finishing increment\");\n+        }\n+      } else if (original_state == ShenandoahOldGeneration::MARKING) {\n+        heap->mmu_tracker()->record_old_marking_increment(false);\n+        heap->log_heap_status(\"At end of Concurrent Old Marking increment\");\n+      }\n+      break;\n+    }\n+    default:\n+      fatal(\"Unexpected state for old GC: %s\", ShenandoahOldGeneration::state_name(old_generation->state()));\n+  }\n+}\n+\n+bool ShenandoahGenerationalControlThread::resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause) {\n+  assert(ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress(), \"Old mark should be in progress\");\n+  log_debug(gc)(\"Resuming old generation with \" UINT32_FORMAT \" marking tasks queued\", generation->task_queues()->tasks());\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  \/\/ We can only tolerate being cancelled during concurrent marking or during preparation for mixed\n+  \/\/ evacuation. This flag here (passed by reference) is used to control precisely where the regulator\n+  \/\/ is allowed to cancel a GC.\n+  ShenandoahOldGC gc(generation, _allow_old_preemption);\n+  if (gc.collect(cause)) {\n+    generation->record_success_concurrent(false);\n+  }\n+\n+  if (heap->cancelled_gc()) {\n+    \/\/ It's possible the gc cycle was cancelled after the last time\n+    \/\/ the collection checked for cancellation. In which case, the\n+    \/\/ old gc cycle is still completed, and we have to deal with this\n+    \/\/ cancellation. We set the degeneration point to be outside\n+    \/\/ the cycle because if this is an allocation failure, that is\n+    \/\/ what must be done (there is no degenerated old cycle). If the\n+    \/\/ cancellation was due to a heuristic wanting to start a young\n+    \/\/ cycle, then we are not actually going to a degenerated cycle,\n+    \/\/ so the degenerated point doesn't matter here.\n+    check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle);\n+    if (_requested_gc_cause == GCCause::_shenandoah_concurrent_gc) {\n+      heap->shenandoah_policy()->record_interrupted_old();\n+    }\n+    return false;\n+  }\n+  return true;\n+}\n+\n+void ShenandoahGenerationalControlThread::service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause, bool do_old_gc_bootstrap) {\n+  \/\/ Normal cycle goes via all concurrent phases. If allocation failure (af) happens during\n+  \/\/ any of the concurrent phases, it first degrades to Degenerated GC and completes GC there.\n+  \/\/ If second allocation failure happens during Degenerated GC cycle (for example, when GC\n+  \/\/ tries to evac something and no memory is available), cycle degrades to Full GC.\n+  \/\/\n+  \/\/ There are also a shortcut through the normal cycle: immediate garbage shortcut, when\n+  \/\/ heuristics says there are no regions to compact, and all the collection comes from immediately\n+  \/\/ reclaimable regions.\n+  \/\/\n+  \/\/ ................................................................................................\n+  \/\/\n+  \/\/                                    (immediate garbage shortcut)                Concurrent GC\n+  \/\/                             \/-------------------------------------------\\\n+  \/\/                             |                                           |\n+  \/\/                             |                                           |\n+  \/\/                             |                                           |\n+  \/\/                             |                                           v\n+  \/\/ [START] ----> Conc Mark ----o----> Conc Evac --o--> Conc Update-Refs ---o----> [END]\n+  \/\/                   |                    |                 |              ^\n+  \/\/                   | (af)               | (af)            | (af)         |\n+  \/\/ ..................|....................|.................|..............|.......................\n+  \/\/                   |                    |                 |              |\n+  \/\/                   |                    |                 |              |      Degenerated GC\n+  \/\/                   v                    v                 v              |\n+  \/\/               STW Mark ----------> STW Evac ----> STW Update-Refs ----->o\n+  \/\/                   |                    |                 |              ^\n+  \/\/                   | (af)               | (af)            | (af)         |\n+  \/\/ ..................|....................|.................|..............|.......................\n+  \/\/                   |                    |                 |              |\n+  \/\/                   |                    v                 |              |      Full GC\n+  \/\/                   \\------------------->o<----------------\/              |\n+  \/\/                                        |                                |\n+  \/\/                                        v                                |\n+  \/\/                                      Full GC  --------------------------\/\n+  \/\/\n+  if (check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle)) return;\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahGCSession session(cause, generation);\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+\n+  service_concurrent_cycle(heap, generation, cause, do_old_gc_bootstrap);\n+}\n+\n+void ShenandoahGenerationalControlThread::service_concurrent_cycle(ShenandoahHeap* heap,\n+                                                       ShenandoahGeneration* generation,\n+                                                       GCCause::Cause& cause,\n+                                                       bool do_old_gc_bootstrap) {\n+  ShenandoahConcurrentGC gc(generation, do_old_gc_bootstrap);\n+  if (gc.collect(cause)) {\n+    \/\/ Cycle is complete\n+    generation->record_success_concurrent(gc.abbreviated());\n+  } else {\n+    assert(heap->cancelled_gc(), \"Must have been cancelled\");\n+    check_cancellation_or_degen(gc.degen_point());\n+    assert(!generation->is_old(), \"Old GC takes a different control path\");\n+    \/\/ Concurrent young-gen collection degenerates to young\n+    \/\/ collection.  Same for global collections.\n+    _degen_generation = generation;\n+  }\n+  const char* msg;\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahMmuTracker* mmu_tracker = heap->mmu_tracker();\n+    if (generation->is_young()) {\n+      if (heap->cancelled_gc()) {\n+        msg = (do_old_gc_bootstrap) ? \"At end of Interrupted Concurrent Bootstrap GC\":\n+                                      \"At end of Interrupted Concurrent Young GC\";\n+      } else {\n+        \/\/ We only record GC results if GC was successful\n+        msg = (do_old_gc_bootstrap) ? \"At end of Concurrent Bootstrap GC\":\n+                                      \"At end of Concurrent Young GC\";\n+        if (heap->collection_set()->has_old_regions()) {\n+          mmu_tracker->record_mixed(get_gc_id());\n+        } else if (do_old_gc_bootstrap) {\n+          mmu_tracker->record_bootstrap(get_gc_id());\n+        } else {\n+          mmu_tracker->record_young(get_gc_id());\n+        }\n+      }\n+    } else {\n+      assert(generation->is_global(), \"If not young, must be GLOBAL\");\n+      assert(!do_old_gc_bootstrap, \"Do not bootstrap with GLOBAL GC\");\n+      if (heap->cancelled_gc()) {\n+        msg = \"At end of Interrupted Concurrent GLOBAL GC\";\n+      } else {\n+        \/\/ We only record GC results if GC was successful\n+        msg = \"At end of Concurrent Global GC\";\n+        mmu_tracker->record_global(get_gc_id());\n+      }\n+    }\n+  } else {\n+    msg = heap->cancelled_gc() ? \"At end of cancelled GC\" :\n+                                 \"At end of GC\";\n+  }\n+  heap->log_heap_status(msg);\n+}\n+\n+bool ShenandoahGenerationalControlThread::check_cancellation_or_degen(ShenandoahGC::ShenandoahDegenPoint point) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  if (!heap->cancelled_gc()) {\n+    return false;\n+  }\n+\n+  if (in_graceful_shutdown()) {\n+    return true;\n+  }\n+\n+  assert(_degen_point == ShenandoahGC::_degenerated_outside_cycle,\n+         \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n+\n+  if (is_alloc_failure_gc()) {\n+    _degen_point = point;\n+    _preemption_requested.unset();\n+    return true;\n+  }\n+\n+  if (_preemption_requested.is_set()) {\n+    assert(_requested_generation == YOUNG, \"Only young GCs may preempt old.\");\n+    _preemption_requested.unset();\n+\n+    \/\/ Old generation marking is only cancellable during concurrent marking.\n+    \/\/ Once final mark is complete, the code does not check again for cancellation.\n+    \/\/ If old generation was cancelled for an allocation failure, we wouldn't\n+    \/\/ make it to this case. The calling code is responsible for forcing a\n+    \/\/ cancellation due to allocation failure into a degenerated cycle.\n+    _degen_point = point;\n+    heap->clear_cancelled_gc(false \/* clear oom handler *\/);\n+    return true;\n+  }\n+\n+  fatal(\"Cancel GC either for alloc failure GC, or gracefully exiting, or to pause old generation marking\");\n+  return false;\n+}\n+\n+void ShenandoahGenerationalControlThread::stop_service() {\n+  \/\/ Nothing to do here.\n+}\n+\n+void ShenandoahGenerationalControlThread::service_stw_full_cycle(GCCause::Cause cause) {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n+  GCIdMark gc_id_mark;\n+  ShenandoahGCSession session(cause, heap->global_generation());\n+\n+  ShenandoahFullGC gc;\n+  gc.collect(cause);\n+}\n+\n+void ShenandoahGenerationalControlThread::service_stw_degenerated_cycle(GCCause::Cause cause,\n+                                                            ShenandoahGC::ShenandoahDegenPoint point) {\n+  assert(point != ShenandoahGC::_degenerated_unset, \"Degenerated point should be set\");\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n+  GCIdMark gc_id_mark;\n+  ShenandoahGCSession session(cause, _degen_generation);\n+\n+  ShenandoahDegenGC gc(point, _degen_generation);\n+  gc.collect(cause);\n+\n+  assert(heap->young_generation()->task_queues()->is_empty(), \"Unexpected young generation marking tasks\");\n+  if (_degen_generation->is_global()) {\n+    assert(heap->old_generation()->task_queues()->is_empty(), \"Unexpected old generation marking tasks\");\n+    assert(heap->global_generation()->task_queues()->is_empty(), \"Unexpected global generation marking tasks\");\n+  } else {\n+    assert(_degen_generation->is_young(), \"Expected degenerated young cycle, if not global.\");\n+    ShenandoahOldGeneration* old = heap->old_generation();\n+    if (old->state() == ShenandoahOldGeneration::BOOTSTRAPPING) {\n+      old->transition_to(ShenandoahOldGeneration::MARKING);\n+    }\n+  }\n+}\n+\n+\n+bool ShenandoahGenerationalControlThread::is_explicit_gc(GCCause::Cause cause) const {\n+  return GCCause::is_user_requested_gc(cause) ||\n+         GCCause::is_serviceability_requested_gc(cause);\n+}\n+\n+bool ShenandoahGenerationalControlThread::is_implicit_gc(GCCause::Cause cause) const {\n+  return !is_explicit_gc(cause)\n+      && cause != GCCause::_shenandoah_concurrent_gc\n+      && cause != GCCause::_no_gc;\n+}\n+\n+void ShenandoahGenerationalControlThread::request_gc(GCCause::Cause cause) {\n+  assert(GCCause::is_user_requested_gc(cause) ||\n+         GCCause::is_serviceability_requested_gc(cause) ||\n+         cause == GCCause::_metadata_GC_clear_soft_refs ||\n+         cause == GCCause::_codecache_GC_aggressive ||\n+         cause == GCCause::_codecache_GC_threshold ||\n+         cause == GCCause::_full_gc_alot ||\n+         cause == GCCause::_wb_young_gc ||\n+         cause == GCCause::_wb_full_gc ||\n+         cause == GCCause::_wb_breakpoint ||\n+         cause == GCCause::_scavenge_alot,\n+         \"only requested GCs here: %s\", GCCause::to_string(cause));\n+\n+  if (is_explicit_gc(cause)) {\n+    if (!DisableExplicitGC) {\n+      handle_requested_gc(cause);\n+    }\n+  } else {\n+    handle_requested_gc(cause);\n+  }\n+}\n+\n+bool ShenandoahGenerationalControlThread::request_concurrent_gc(ShenandoahGenerationType generation) {\n+  if (_preemption_requested.is_set() || _requested_gc_cause != GCCause::_no_gc || ShenandoahHeap::heap()->cancelled_gc()) {\n+    \/\/ Ignore subsequent requests from the heuristics\n+    log_debug(gc, thread)(\"Reject request for concurrent gc: preemption_requested: %s, gc_requested: %s, gc_cancelled: %s\",\n+                          BOOL_TO_STR(_preemption_requested.is_set()),\n+                          GCCause::to_string(_requested_gc_cause),\n+                          BOOL_TO_STR(ShenandoahHeap::heap()->cancelled_gc()));\n+    return false;\n+  }\n+\n+  if (gc_mode() == none) {\n+    GCCause::Cause existing = Atomic::cmpxchg(&_requested_gc_cause, GCCause::_no_gc, GCCause::_shenandoah_concurrent_gc);\n+    if (existing != GCCause::_no_gc) {\n+      log_debug(gc, thread)(\"Reject request for concurrent gc because another gc is pending: %s\", GCCause::to_string(existing));\n+      return false;\n+    }\n+\n+    _requested_generation = generation;\n+    notify_control_thread();\n+\n+    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n+    while (gc_mode() == none) {\n+      ml.wait();\n+    }\n+    return true;\n+  }\n+\n+  if (preempt_old_marking(generation)) {\n+    assert(gc_mode() == servicing_old, \"Expected to be servicing old, but was: %s.\", gc_mode_name(gc_mode()));\n+    GCCause::Cause existing = Atomic::cmpxchg(&_requested_gc_cause, GCCause::_no_gc, GCCause::_shenandoah_concurrent_gc);\n+    if (existing != GCCause::_no_gc) {\n+      log_debug(gc, thread)(\"Reject request to interrupt old gc because another gc is pending: %s\", GCCause::to_string(existing));\n+      return false;\n+    }\n+\n+    log_info(gc)(\"Preempting old generation mark to allow %s GC\", shenandoah_generation_name(generation));\n+    _requested_generation = generation;\n+    _preemption_requested.set();\n+    ShenandoahHeap::heap()->cancel_gc(GCCause::_shenandoah_concurrent_gc);\n+    notify_control_thread();\n+\n+    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n+    while (gc_mode() == servicing_old) {\n+      ml.wait();\n+    }\n+    return true;\n+  }\n+\n+  log_debug(gc, thread)(\"Reject request for concurrent gc: mode: %s, allow_old_preemption: %s\",\n+                        gc_mode_name(gc_mode()),\n+                        BOOL_TO_STR(_allow_old_preemption.is_set()));\n+  return false;\n+}\n+\n+void ShenandoahGenerationalControlThread::notify_control_thread() {\n+  MonitorLocker locker(&_control_lock, Mutex::_no_safepoint_check_flag);\n+  _control_lock.notify();\n+}\n+\n+bool ShenandoahGenerationalControlThread::preempt_old_marking(ShenandoahGenerationType generation) {\n+  return (generation == YOUNG) && _allow_old_preemption.try_unset();\n+}\n+\n+void ShenandoahGenerationalControlThread::handle_requested_gc(GCCause::Cause cause) {\n+  \/\/ Make sure we have at least one complete GC cycle before unblocking\n+  \/\/ from the explicit GC request.\n+  \/\/\n+  \/\/ This is especially important for weak references cleanup and\/or native\n+  \/\/ resources (e.g. DirectByteBuffers) machinery: when explicit GC request\n+  \/\/ comes very late in the already running cycle, it would miss lots of new\n+  \/\/ opportunities for cleanup that were made available before the caller\n+  \/\/ requested the GC.\n+\n+  MonitorLocker ml(&_gc_waiters_lock);\n+  size_t current_gc_id = get_gc_id();\n+  size_t required_gc_id = current_gc_id + 1;\n+  while (current_gc_id < required_gc_id) {\n+    \/\/ This races with the regulator thread to start a concurrent gc and the\n+    \/\/ control thread to clear it at the start of a cycle. Threads here are\n+    \/\/ allowed to escalate a heuristic's request for concurrent gc.\n+    GCCause::Cause existing = Atomic::xchg(&_requested_gc_cause, cause);\n+    if (existing != GCCause::_no_gc) {\n+      log_debug(gc, thread)(\"GC request supersedes existing request: %s\", GCCause::to_string(existing));\n+    }\n+\n+    notify_control_thread();\n+    if (cause != GCCause::_wb_breakpoint) {\n+      ml.wait();\n+    }\n+    current_gc_id = get_gc_id();\n+  }\n+}\n+\n+void ShenandoahGenerationalControlThread::handle_alloc_failure(ShenandoahAllocRequest& req, bool block) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  assert(current()->is_Java_thread(), \"expect Java thread here\");\n+  bool is_humongous = req.size() > ShenandoahHeapRegion::region_size_words();\n+\n+  if (try_set_alloc_failure_gc(is_humongous)) {\n+    \/\/ Only report the first allocation failure\n+    log_info(gc)(\"Failed to allocate %s, \" SIZE_FORMAT \"%s\",\n+                 req.type_string(),\n+                 byte_size_in_proper_unit(req.size() * HeapWordSize), proper_unit_for_byte_size(req.size() * HeapWordSize));\n+    \/\/ Now that alloc failure GC is scheduled, we can abort everything else\n+    heap->cancel_gc(GCCause::_allocation_failure);\n+  }\n+\n+\n+  if (block) {\n+    MonitorLocker ml(&_alloc_failure_waiters_lock);\n+    while (is_alloc_failure_gc()) {\n+      ml.wait();\n+    }\n+  }\n+}\n+\n+void ShenandoahGenerationalControlThread::handle_alloc_failure_evac(size_t words) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  bool is_humongous = (words > ShenandoahHeapRegion::region_size_words());\n+\n+  if (try_set_alloc_failure_gc(is_humongous)) {\n+    \/\/ Only report the first allocation failure\n+    log_info(gc)(\"Failed to allocate \" SIZE_FORMAT \"%s for evacuation\",\n+                 byte_size_in_proper_unit(words * HeapWordSize), proper_unit_for_byte_size(words * HeapWordSize));\n+  }\n+\n+  \/\/ Forcefully report allocation failure\n+  heap->cancel_gc(GCCause::_shenandoah_allocation_failure_evac);\n+}\n+\n+void ShenandoahGenerationalControlThread::notify_alloc_failure_waiters() {\n+  _alloc_failure_gc.unset();\n+  _humongous_alloc_failure_gc.unset();\n+  MonitorLocker ml(&_alloc_failure_waiters_lock);\n+  ml.notify_all();\n+}\n+\n+bool ShenandoahGenerationalControlThread::try_set_alloc_failure_gc(bool is_humongous) {\n+  if (is_humongous) {\n+    _humongous_alloc_failure_gc.try_set();\n+  }\n+  return _alloc_failure_gc.try_set();\n+}\n+\n+bool ShenandoahGenerationalControlThread::is_alloc_failure_gc() {\n+  return _alloc_failure_gc.is_set();\n+}\n+\n+void ShenandoahGenerationalControlThread::notify_gc_waiters() {\n+  MonitorLocker ml(&_gc_waiters_lock);\n+  ml.notify_all();\n+}\n+\n+void ShenandoahGenerationalControlThread::notify_heap_changed() {\n+}\n+\n+void ShenandoahGenerationalControlThread::start() {\n+  create_and_start();\n+}\n+\n+const char* ShenandoahGenerationalControlThread::gc_mode_name(ShenandoahGenerationalControlThread::GCMode mode) {\n+  switch (mode) {\n+    case none:              return \"idle\";\n+    case concurrent_normal: return \"normal\";\n+    case stw_degenerated:   return \"degenerated\";\n+    case stw_full:          return \"full\";\n+    case servicing_old:     return \"old\";\n+    case bootstrapping_old: return \"bootstrap\";\n+    default:                return \"unknown\";\n+  }\n+}\n+\n+void ShenandoahGenerationalControlThread::set_gc_mode(ShenandoahGenerationalControlThread::GCMode new_mode) {\n+  if (_mode != new_mode) {\n+    log_info(gc)(\"Transition from: %s to: %s\", gc_mode_name(_mode), gc_mode_name(new_mode));\n+    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n+    _mode = new_mode;\n+    ml.notify_all();\n+  }\n+}\n+\n+ShenandoahGenerationType ShenandoahGenerationalControlThread::select_global_generation() {\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return GLOBAL_GEN;\n+  } else {\n+    return GLOBAL_NON_GEN;\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalControlThread.cpp","additions":984,"deletions":0,"binary":false,"changes":984,"status":"added"},{"patch":"@@ -0,0 +1,156 @@\n+\/*\n+ * Copyright (c) 2013, 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALCONTROLTHREAD_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALCONTROLTHREAD_HPP\n+\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shenandoah\/shenandoahController.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahPadding.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n+#include \"runtime\/task.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+class ShenandoahGenerationalControlThread: public ShenandoahController {\n+  friend class VMStructs;\n+\n+private:\n+  \/\/ While we could have a single lock for these, it may risk unblocking\n+  \/\/ GC waiters when alloc failure GC cycle finishes. We want instead\n+  \/\/ to make complete explicit cycle for demanding customers.\n+  Monitor _alloc_failure_waiters_lock;\n+  Monitor _gc_waiters_lock;\n+  Monitor _control_lock;\n+  Monitor _regulator_lock;\n+\n+public:\n+  typedef enum {\n+    none,\n+    concurrent_normal,\n+    stw_degenerated,\n+    stw_full,\n+    bootstrapping_old,\n+    servicing_old\n+  } GCMode;\n+\n+  void run_service();\n+  void stop_service();\n+\n+private:\n+  ShenandoahSharedFlag _allow_old_preemption;\n+  ShenandoahSharedFlag _preemption_requested;\n+  ShenandoahSharedFlag _alloc_failure_gc;\n+  ShenandoahSharedFlag _humongous_alloc_failure_gc;\n+\n+  GCCause::Cause  _requested_gc_cause;\n+  volatile ShenandoahGenerationType _requested_generation;\n+  ShenandoahGC::ShenandoahDegenPoint _degen_point;\n+  ShenandoahGeneration* _degen_generation;\n+\n+  shenandoah_padding(0);\n+  volatile GCMode _mode;\n+  shenandoah_padding();\n+\n+  \/\/ Returns true if the cycle has been cancelled or degenerated.\n+  bool check_cancellation_or_degen(ShenandoahGC::ShenandoahDegenPoint point);\n+\n+  \/\/ Returns true if the old generation marking completed (i.e., final mark executed for old generation).\n+  bool resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause);\n+  void service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause, bool reset_old_bitmap_specially);\n+  void service_stw_full_cycle(GCCause::Cause cause);\n+  void service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahGC::ShenandoahDegenPoint point);\n+\n+  \/\/ Return true if setting the flag which indicates allocation failure succeeds.\n+  bool try_set_alloc_failure_gc(bool is_humongous);\n+\n+  \/\/ Notify threads waiting for GC to complete.\n+  void notify_alloc_failure_waiters();\n+\n+  \/\/ True if allocation failure flag has been set.\n+  bool is_alloc_failure_gc();\n+\n+  void notify_gc_waiters();\n+\n+  \/\/ Handle GC request.\n+  \/\/ Blocks until GC is over.\n+  void handle_requested_gc(GCCause::Cause cause);\n+\n+  bool is_explicit_gc(GCCause::Cause cause) const;\n+  bool is_implicit_gc(GCCause::Cause cause) const;\n+\n+  \/\/ Returns true if the old generation marking was interrupted to allow a young cycle.\n+  bool preempt_old_marking(ShenandoahGenerationType generation);\n+\n+  void process_phase_timings(const ShenandoahHeap* heap);\n+\n+public:\n+  \/\/ Constructor\n+  ShenandoahGenerationalControlThread();\n+\n+  \/\/ Handle allocation failure from a mutator allocation.\n+  \/\/ Optionally blocks while collector is handling the failure. If the GC\n+  \/\/ threshold has been exceeded, the mutator allocation will not block so\n+  \/\/ that the out of memory error can be raised promptly.\n+  void handle_alloc_failure(ShenandoahAllocRequest& req, bool block = true);\n+\n+  \/\/ Handle allocation failure from evacuation path.\n+  void handle_alloc_failure_evac(size_t words);\n+\n+  void request_gc(GCCause::Cause cause);\n+  \/\/ Return true if the request to start a concurrent GC for the given generation succeeded.\n+  bool request_concurrent_gc(ShenandoahGenerationType generation);\n+\n+  void notify_heap_changed();\n+\n+  void start();\n+\n+  void service_concurrent_normal_cycle(ShenandoahHeap* heap,\n+                                       const ShenandoahGenerationType generation,\n+                                       GCCause::Cause cause);\n+\n+  void service_concurrent_old_cycle(ShenandoahHeap* heap,\n+                                    GCCause::Cause &cause);\n+\n+  void set_gc_mode(GCMode new_mode);\n+  GCMode gc_mode() {\n+    return _mode;\n+  }\n+\n+  static ShenandoahGenerationType select_global_generation();\n+\n+ private:\n+  static const char* gc_mode_name(GCMode mode);\n+  void notify_control_thread();\n+\n+  void service_concurrent_cycle(ShenandoahHeap* heap,\n+                                ShenandoahGeneration* generation,\n+                                GCCause::Cause &cause,\n+                                bool do_old_gc_bootstrap);\n+\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALCONTROLTHREAD_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalControlThread.hpp","additions":156,"deletions":0,"binary":false,"changes":156,"status":"added"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahRegulatorThread.hpp\"\n@@ -48,2 +50,2 @@\n-    log_info(gc, init)(\"Young Generation Soft Size: \" PROPERFMT, PROPERFMTARGS(young->soft_max_capacity()));\n-    log_info(gc, init)(\"Young Generation Max: \" PROPERFMT, PROPERFMTARGS(young->max_capacity()));\n+    log_info(gc, init)(\"Young Generation Soft Size: \" EXACTFMT, EXACTFMTARGS(young->soft_max_capacity()));\n+    log_info(gc, init)(\"Young Generation Max: \" EXACTFMT, EXACTFMTARGS(young->max_capacity()));\n@@ -52,2 +54,2 @@\n-    log_info(gc, init)(\"Old Generation Soft Size: \" PROPERFMT, PROPERFMTARGS(old->soft_max_capacity()));\n-    log_info(gc, init)(\"Old Generation Max: \" PROPERFMT, PROPERFMTARGS(old->max_capacity()));\n+    log_info(gc, init)(\"Old Generation Soft Size: \" EXACTFMT, EXACTFMTARGS(old->soft_max_capacity()));\n+    log_info(gc, init)(\"Old Generation Max: \" EXACTFMT, EXACTFMTARGS(old->max_capacity()));\n@@ -75,0 +77,27 @@\n+\n+ShenandoahGenerationalHeap::ShenandoahGenerationalHeap(ShenandoahCollectorPolicy* policy) :\n+  ShenandoahHeap(policy),\n+  _regulator_thread(nullptr) { }\n+\n+void ShenandoahGenerationalHeap::initialize_control_thread() {\n+  auto control_thread = new ShenandoahGenerationalControlThread();\n+  _control_thread = control_thread;\n+  _regulator_thread = new ShenandoahRegulatorThread(control_thread);\n+}\n+\n+void ShenandoahGenerationalHeap::gc_threads_do(ThreadClosure* tcl) const {\n+  if (!shenandoah_policy()->is_at_shutdown()) {\n+    ShenandoahHeap::gc_threads_do(tcl);\n+    tcl->do_thread(regulator_thread());\n+  }\n+}\n+\n+void ShenandoahGenerationalHeap::notify_control_thread_heap_changed() {\n+  control_thread()->notify_heap_changed();\n+  regulator_thread()->notify_heap_changed();\n+}\n+\n+void ShenandoahGenerationalHeap::stop() {\n+  regulator_thread()->stop();\n+  ShenandoahHeap::stop();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":33,"deletions":4,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -30,0 +30,3 @@\n+class ShenandoahRegulatorThread;\n+class ShenandoahGenerationalControlThread;\n+\n@@ -32,1 +35,1 @@\n-  explicit ShenandoahGenerationalHeap(ShenandoahCollectorPolicy* policy) : ShenandoahHeap(policy) {}\n+  explicit ShenandoahGenerationalHeap(ShenandoahCollectorPolicy* policy);\n@@ -37,0 +40,13 @@\n+\n+  ShenandoahRegulatorThread* regulator_thread() const             { return _regulator_thread;  }\n+\n+  void gc_threads_do(ThreadClosure* tcl) const override;\n+\n+  void stop() override;\n+\n+private:\n+  void initialize_control_thread() override;\n+  void notify_control_thread_heap_changed() override;\n+\n+private:\n+  ShenandoahRegulatorThread* _regulator_thread;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.hpp","additions":17,"deletions":1,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -52,1 +52,0 @@\n-#include \"gc\/shenandoah\/shenandoahRegulatorThread.hpp\"\n@@ -487,2 +486,1 @@\n-  _control_thread = new ShenandoahControlThread();\n-  _regulator_thread = new ShenandoahRegulatorThread(_control_thread);\n+  initialize_control_thread();\n@@ -495,0 +493,10 @@\n+void ShenandoahHeap::initialize_control_thread() {\n+  _control_thread = new ShenandoahControlThread();\n+}\n+\n+\n+void ShenandoahHeap::notify_control_thread_heap_changed() {\n+  control_thread()->notify_heap_changed();\n+}\n+\n+\n@@ -609,1 +617,0 @@\n-  _regulator_thread(nullptr),\n@@ -884,0 +891,9 @@\n+\n+void ShenandoahHeap::entry_uncommit(double shrink_before, size_t shrink_until) {\n+  static const char *msg = \"Concurrent uncommit\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_uncommit, true \/* log_heap_usage *\/);\n+  EventMark em(\"%s\", msg);\n+\n+  op_uncommit(shrink_before, shrink_until);\n+}\n+\n@@ -914,0 +930,56 @@\n+void ShenandoahHeap::service_uncommit(double shrink_before, size_t shrink_until) {\n+  assert (ShenandoahUncommit, \"should be enabled\");\n+\n+  \/\/ Determine if there is work to do. This avoids taking heap lock if there is\n+  \/\/ no work available, avoids spamming logs with superfluous logging messages,\n+  \/\/ and minimises the amount of work while locks are taken.\n+\n+  if (committed() <= shrink_until) return;\n+\n+  bool has_work = false;\n+  for (size_t i = 0; i < num_regions(); i++) {\n+    ShenandoahHeapRegion* r = get_region(i);\n+    if (r->is_empty_committed() && (r->empty_time() < shrink_before)) {\n+      has_work = true;\n+      break;\n+    }\n+  }\n+\n+  if (has_work) {\n+    entry_uncommit(shrink_before, shrink_until);\n+  }\n+}\n+\n+bool ShenandoahHeap::check_soft_max_changed() {\n+  size_t new_soft_max = Atomic::load(&SoftMaxHeapSize);\n+  size_t old_soft_max = soft_max_capacity();\n+  if (new_soft_max != old_soft_max) {\n+    new_soft_max = MAX2(min_capacity(), new_soft_max);\n+    new_soft_max = MIN2(max_capacity(), new_soft_max);\n+    if (new_soft_max != old_soft_max) {\n+      log_info(gc)(\"Soft Max Heap Size: \" SIZE_FORMAT \"%s -> \" SIZE_FORMAT \"%s\",\n+                   byte_size_in_proper_unit(old_soft_max), proper_unit_for_byte_size(old_soft_max),\n+                   byte_size_in_proper_unit(new_soft_max), proper_unit_for_byte_size(new_soft_max)\n+      );\n+      set_soft_max_capacity(new_soft_max);\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+void ShenandoahHeap::notify_heap_changed() {\n+  \/\/ Update monitoring counters when we took a new region. This amortizes the\n+  \/\/ update costs on slow path.\n+  monitoring_support()->notify_heap_changed();\n+  notify_control_thread_heap_changed();\n+}\n+\n+void ShenandoahHeap::set_forced_counters_update(bool value) {\n+  monitoring_support()->set_forced_counters_update(value);\n+}\n+\n+void ShenandoahHeap::handle_force_counters_update() {\n+  monitoring_support()->handle_force_counters_update();\n+}\n+\n@@ -978,16 +1050,0 @@\n-void ShenandoahHeap::notify_heap_changed() {\n-  \/\/ Update monitoring counters when we took a new region. This amortizes the\n-  \/\/ update costs on slow path.\n-  monitoring_support()->notify_heap_changed();\n-  control_thread()->notify_heap_changed();\n-  regulator_thread()->notify_heap_changed();\n-}\n-\n-void ShenandoahHeap::set_forced_counters_update(bool value) {\n-  monitoring_support()->set_forced_counters_update(value);\n-}\n-\n-void ShenandoahHeap::handle_force_counters_update() {\n-  monitoring_support()->handle_force_counters_update();\n-}\n-\n@@ -1383,1 +1439,1 @@\n-      control_thread()->handle_alloc_failure(req);\n+      control_thread()->handle_alloc_failure(req, true);\n@@ -2019,2 +2075,4 @@\n-  tcl->do_thread(_control_thread);\n-  tcl->do_thread(_regulator_thread);\n+  if (_control_thread != nullptr) {\n+    tcl->do_thread(_control_thread);\n+  }\n+\n@@ -2567,4 +2625,1 @@\n-  \/\/ Step 2. Stop requesting collections.\n-  regulator_thread()->stop();\n-\n-  \/\/ Step 3. Notify control thread that we are in shutdown.\n+  \/\/ Step 2. Notify control thread that we are in shutdown.\n@@ -2575,1 +2630,1 @@\n-  \/\/ Step 4. Notify GC workers that we are cancelling GC.\n+  \/\/ Step 3. Notify GC workers that we are cancelling GC.\n@@ -2578,1 +2633,1 @@\n-  \/\/ Step 5. Wait until GC worker exits normally.\n+  \/\/ Step 4. Wait until GC worker exits normally.\n@@ -3278,8 +3333,0 @@\n-void ShenandoahHeap::entry_uncommit(double shrink_before, size_t shrink_until) {\n-  static const char *msg = \"Concurrent uncommit\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_uncommit, true \/* log_heap_usage *\/);\n-  EventMark em(\"%s\", msg);\n-\n-  op_uncommit(shrink_before, shrink_until);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":84,"deletions":37,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"gc\/shenandoah\/shenandoahController.hpp\"\n@@ -45,0 +46,1 @@\n+#include \"gc\/shenandoah\/shenandoahPeriodicTasks.hpp\"\n@@ -57,1 +59,1 @@\n-class ShenandoahControlThread;\n+class ShenandoahGenerationalControlThread;\n@@ -275,0 +277,3 @@\n+  virtual void initialize_control_thread();\n+  virtual void notify_control_thread_heap_changed();\n+\n@@ -514,0 +519,4 @@\n+  void service_uncommit(double shrink_before, size_t shrink_until);\n+\n+  \/\/ Returns true if the soft maximum heap has been changed using management APIs.\n+  bool check_soft_max_changed();\n@@ -545,2 +554,4 @@\n-  ShenandoahControlThread*   _control_thread;\n-  ShenandoahRegulatorThread* _regulator_thread;\n+protected:\n+  ShenandoahController*  _control_thread;\n+\n+private:\n@@ -558,2 +569,0 @@\n-  ShenandoahRegulatorThread* regulator_thread()        { return _regulator_thread;  }\n-\n@@ -561,1 +570,2 @@\n-  ShenandoahControlThread*   control_thread()          { return _control_thread;    }\n+  ShenandoahController*   control_thread() { return _control_thread; }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":16,"deletions":6,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-#include \"gc\/shenandoah\/shenandoahControlThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalControlThread.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,65 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMonitoringSupport.hpp\"\n+#include \"gc\/shenandoah\/shenandoahPacer.hpp\"\n+#include \"gc\/shenandoah\/shenandoahPeriodicTasks.hpp\"\n+\n+void ShenandoahPeriodicTask::task() {\n+  handle_force_counters_update();\n+  handle_counters_update();\n+}\n+\n+void ShenandoahPeriodicTask::handle_counters_update() {\n+  if (_do_counters_update.is_set()) {\n+    _do_counters_update.unset();\n+    ShenandoahHeap::heap()->monitoring_support()->update_counters();\n+  }\n+}\n+\n+void ShenandoahPeriodicTask::handle_force_counters_update() {\n+  if (_force_counters_update.is_set()) {\n+    _do_counters_update.unset(); \/\/ reset these too, we do update now!\n+    ShenandoahHeap::heap()->monitoring_support()->update_counters();\n+  }\n+}\n+\n+void ShenandoahPeriodicTask::notify_heap_changed() {\n+  if (_do_counters_update.is_unset()) {\n+    _do_counters_update.set();\n+  }\n+}\n+\n+void ShenandoahPeriodicTask::set_forced_counters_update(bool value) {\n+  _force_counters_update.set_cond(value);\n+}\n+\n+void ShenandoahPeriodicPacerNotify::task() {\n+  assert(ShenandoahPacing, \"Should not be here otherwise\");\n+  ShenandoahHeap::heap()->pacer()->notify_waiters();\n+}\n+\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPeriodicTasks.cpp","additions":65,"deletions":0,"binary":false,"changes":65,"status":"added"},{"patch":"@@ -0,0 +1,55 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHPERIODICTASKS_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHPERIODICTASKS_HPP\n+\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n+#include \"runtime\/task.hpp\"\n+\n+\/\/ Periodic task is useful for doing asynchronous things that do not require (heap) locks,\n+\/\/ or synchronization with other parts of collector. These could run even when ShenandoahConcurrentThread\n+\/\/ is busy driving the GC cycle.\n+class ShenandoahPeriodicTask : public PeriodicTask {\n+private:\n+  ShenandoahSharedFlag _do_counters_update;\n+  ShenandoahSharedFlag _force_counters_update;\n+public:\n+  ShenandoahPeriodicTask() : PeriodicTask(100) {}\n+  void task() override;\n+\n+  void handle_counters_update();\n+  void handle_force_counters_update();\n+  void set_forced_counters_update(bool value);\n+  void notify_heap_changed();\n+};\n+\n+\/\/ Periodic task to notify blocked paced waiters.\n+class ShenandoahPeriodicPacerNotify : public PeriodicTask {\n+public:\n+  ShenandoahPeriodicPacerNotify() : PeriodicTask(PeriodicTask::min_interval) {}\n+  void task() override;\n+};\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_SHENANDOAHPERIODICTASKS_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPeriodicTasks.hpp","additions":55,"deletions":0,"binary":false,"changes":55,"status":"added"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"gc\/shenandoah\/shenandoahControlThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalControlThread.hpp\"\n@@ -39,1 +39,1 @@\n-ShenandoahRegulatorThread::ShenandoahRegulatorThread(ShenandoahControlThread* control_thread) :\n+ShenandoahRegulatorThread::ShenandoahRegulatorThread(ShenandoahGenerationalControlThread* control_thread) :\n@@ -72,2 +72,2 @@\n-    ShenandoahControlThread::GCMode mode = _control_thread->gc_mode();\n-    if (mode == ShenandoahControlThread::none) {\n+    ShenandoahGenerationalControlThread::GCMode mode = _control_thread->gc_mode();\n+    if (mode == ShenandoahGenerationalControlThread::none) {\n@@ -75,1 +75,1 @@\n-        if (request_concurrent_gc(ShenandoahControlThread::select_global_generation())) {\n+        if (request_concurrent_gc(ShenandoahGenerationalControlThread::select_global_generation())) {\n@@ -89,1 +89,1 @@\n-    } else if (mode == ShenandoahControlThread::servicing_old) {\n+    } else if (mode == ShenandoahGenerationalControlThread::servicing_old) {\n@@ -105,1 +105,1 @@\n-    if (_control_thread->gc_mode() == ShenandoahControlThread::none) {\n+    if (_control_thread->gc_mode() == ShenandoahGenerationalControlThread::none) {\n@@ -121,1 +121,1 @@\n-    if (_control_thread->gc_mode() == ShenandoahControlThread::none) {\n+    if (_control_thread->gc_mode() == ShenandoahGenerationalControlThread::none) {\n@@ -163,1 +163,1 @@\n-  return _global_heuristics->should_start_gc() && request_concurrent_gc(ShenandoahControlThread::select_global_generation());\n+  return _global_heuristics->should_start_gc() && request_concurrent_gc(ShenandoahGenerationalControlThread::select_global_generation());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRegulatorThread.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-class ShenandoahControlThread;\n+class ShenandoahGenerationalControlThread;\n@@ -51,1 +51,1 @@\n-  explicit ShenandoahRegulatorThread(ShenandoahControlThread* control_thread);\n+  explicit ShenandoahRegulatorThread(ShenandoahGenerationalControlThread* control_thread);\n@@ -91,1 +91,1 @@\n-  ShenandoahControlThread* _control_thread;\n+  ShenandoahGenerationalControlThread* _control_thread;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRegulatorThread.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSATBMarkQueueSet.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"}]}