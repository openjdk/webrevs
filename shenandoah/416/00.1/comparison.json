{"files":[{"patch":"@@ -47,1 +47,1 @@\n-  assert(UseG1GC || UseParallelGC || UseSerialGC,\n+  assert(UseG1GC || UseParallelGC || UseSerialGC || UseShenandoahGC,\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTable.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,174 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahAgeCensus.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n+#include \"gc\/shenandoah\/shenandoahThreadLocalData.hpp\"\n+#include \"gc\/shenandoah\/shenandoahRootProcessor.hpp\"\n+#include \"runtime\/threadSMR.inline.hpp\"\n+#include \"runtime\/thread.hpp\"\n+\n+ShenandoahEvacuationStats::ShenandoahEvacuationStats(bool generational)\n+  : _evacuations_completed(0), _bytes_completed(0),\n+    _evacuations_attempted(0), _bytes_attempted(0),\n+    _use_age_table(generational && (ShenandoahGenerationalCensusAtEvac || !ShenandoahGenerationalAdaptiveTenuring)) {\n+  if (_use_age_table) {\n+    _age_table = new AgeTable(false);\n+  }\n+}\n+\n+AgeTable* ShenandoahEvacuationStats::age_table() const {\n+  assert(_use_age_table, \"Don't call\");\n+  return _age_table;\n+}\n+\n+void ShenandoahEvacuationStats::begin_evacuation(size_t bytes) {\n+  ++_evacuations_attempted;\n+  _bytes_attempted += bytes;\n+}\n+\n+void ShenandoahEvacuationStats::end_evacuation(size_t bytes) {\n+  ++_evacuations_completed;\n+  _bytes_completed += bytes;\n+}\n+\n+void ShenandoahEvacuationStats::record_age(size_t bytes, uint age) {\n+  assert(_use_age_table, \"Don't call!\");\n+  if (age <= markWord::max_age) { \/\/ Filter age sentinel.\n+    _age_table->add(age, bytes >> LogBytesPerWord);\n+  }\n+}\n+\n+void ShenandoahEvacuationStats::accumulate(const ShenandoahEvacuationStats* other) {\n+  _evacuations_completed += other->_evacuations_completed;\n+  _bytes_completed += other->_bytes_completed;\n+  _evacuations_attempted += other->_evacuations_attempted;\n+  _bytes_attempted += other->_bytes_attempted;\n+  if (_use_age_table) {\n+    _age_table->merge(other->age_table());\n+  }\n+}\n+\n+void ShenandoahEvacuationStats::reset() {\n+  _evacuations_completed = _evacuations_attempted = 0;\n+  _bytes_completed = _bytes_attempted = 0;\n+  if (_use_age_table) {\n+    _age_table->clear();\n+  }\n+}\n+\n+void ShenandoahEvacuationStats::print_on(outputStream* st) {\n+  size_t abandoned_size = _bytes_attempted - _bytes_completed;\n+  size_t abandoned_count = _evacuations_attempted - _evacuations_completed;\n+  st->print_cr(\"Evacuated \" SIZE_FORMAT \"%s across \" SIZE_FORMAT \" objects, \"\n+            \"abandoned \" SIZE_FORMAT \"%s across \" SIZE_FORMAT \" objects.\",\n+            byte_size_in_proper_unit(_bytes_completed), proper_unit_for_byte_size(_bytes_completed),\n+            _evacuations_completed,\n+            byte_size_in_proper_unit(abandoned_size),   proper_unit_for_byte_size(abandoned_size),\n+            abandoned_count);\n+  if (_use_age_table) {\n+    _age_table->print_on(st);\n+  }\n+}\n+\n+void ShenandoahEvacuationTracker::print_global_on(outputStream* st) {\n+  print_evacuations_on(st, &_workers_global, &_mutators_global);\n+}\n+\n+void ShenandoahEvacuationTracker::print_evacuations_on(outputStream* st,\n+                                                       ShenandoahEvacuationStats* workers,\n+                                                       ShenandoahEvacuationStats* mutators) {\n+  st->print(\"Workers: \");\n+  workers->print_on(st);\n+  st->cr();\n+  st->print(\"Mutators: \");\n+  mutators->print_on(st);\n+  st->cr();\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  if (_generational) {\n+    AgeTable young_region_ages(false);\n+    for (uint i = 0; i < heap->num_regions(); ++i) {\n+      ShenandoahHeapRegion* r = heap->get_region(i);\n+      if (r->is_young()) {\n+        young_region_ages.add(r->age(), r->get_live_data_words());\n+      }\n+    }\n+    st->print(\"Young regions: \");\n+    young_region_ages.print_on(st);\n+    st->cr();\n+  }\n+}\n+\n+class ShenandoahStatAggregator : public ThreadClosure {\n+public:\n+  ShenandoahEvacuationStats* _target;\n+  explicit ShenandoahStatAggregator(ShenandoahEvacuationStats* target) : _target(target) {}\n+  virtual void do_thread(Thread* thread) override {\n+    ShenandoahEvacuationStats* local = ShenandoahThreadLocalData::evacuation_stats(thread);\n+    _target->accumulate(local);\n+    local->reset();\n+  }\n+};\n+\n+ShenandoahCycleStats ShenandoahEvacuationTracker::flush_cycle_to_global() {\n+  ShenandoahEvacuationStats mutators(_generational), workers(_generational);\n+\n+  ThreadsListHandle java_threads_iterator;\n+  ShenandoahStatAggregator aggregate_mutators(&mutators);\n+  java_threads_iterator.list()->threads_do(&aggregate_mutators);\n+\n+  ShenandoahStatAggregator aggregate_workers(&workers);\n+  ShenandoahHeap::heap()->gc_threads_do(&aggregate_workers);\n+\n+  _mutators_global.accumulate(&mutators);\n+  _workers_global.accumulate(&workers);\n+\n+  if (_generational && (ShenandoahGenerationalCensusAtEvac || !ShenandoahGenerationalAdaptiveTenuring)) {\n+    \/\/ Ingest population vectors into the heap's global census\n+    \/\/ data, and use it to compute an appropriate tenuring threshold\n+    \/\/ for use in the next cycle.\n+    ShenandoahAgeCensus* census = ShenandoahHeap::heap()->age_census();\n+    census->prepare_for_census_update();\n+    \/\/ The first argument is used for any age 0 cohort population that we may otherwise have\n+    \/\/ missed during the census. This is non-zero only when census happens at marking.\n+    census->update_census(0, _mutators_global.age_table(), _workers_global.age_table());\n+  }\n+\n+  return {workers, mutators};\n+}\n+\n+void ShenandoahEvacuationTracker::begin_evacuation(Thread* thread, size_t bytes) {\n+  ShenandoahThreadLocalData::begin_evacuation(thread, bytes);\n+}\n+\n+void ShenandoahEvacuationTracker::end_evacuation(Thread* thread, size_t bytes) {\n+  ShenandoahThreadLocalData::end_evacuation(thread, bytes);\n+}\n+\n+void ShenandoahEvacuationTracker::record_age(Thread* thread, size_t bytes, uint age) {\n+  ShenandoahThreadLocalData::record_age(thread, bytes, age);\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahEvacTracker.cpp","additions":174,"deletions":0,"binary":false,"changes":174,"status":"added"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -39,0 +40,4 @@\n+#include \"gc\/shenandoah\/shenandoahAgeCensus.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n@@ -40,0 +45,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -44,0 +50,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n@@ -47,0 +54,2 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n@@ -56,0 +65,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -62,0 +72,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -69,0 +80,2 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -72,0 +85,2 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n@@ -164,3 +179,0 @@\n-  \/\/ Now we know the number of regions, initialize the heuristics.\n-  initialize_heuristics();\n-\n@@ -182,0 +194,3 @@\n+  \/\/ Now we know the number of regions and heap sizes, initialize the heuristics.\n+  initialize_heuristics_generations();\n+\n@@ -220,0 +235,28 @@\n+  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this, _heap_region));\n+\n+  \/\/\n+  \/\/ After reserving the Java heap, create the card table, barriers, and workers, in dependency order\n+  \/\/\n+  if (mode()->is_generational()) {\n+    ShenandoahDirectCardMarkRememberedSet *rs;\n+    ShenandoahCardTable* card_table = ShenandoahBarrierSet::barrier_set()->card_table();\n+    size_t card_count = card_table->cards_required(heap_rs.size() \/ HeapWordSize);\n+    rs = new ShenandoahDirectCardMarkRememberedSet(ShenandoahBarrierSet::barrier_set()->card_table(), card_count);\n+    _card_scan = new ShenandoahScanRemembered<ShenandoahDirectCardMarkRememberedSet>(rs);\n+\n+    \/\/ Age census structure\n+    _age_census = new ShenandoahAgeCensus();\n+  }\n+\n+  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n+  if (_workers == nullptr) {\n+    vm_exit_during_initialization(\"Failed necessary allocation.\");\n+  } else {\n+    _workers->initialize_workers();\n+  }\n+\n+  if (ParallelGCThreads > 1) {\n+    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\", ParallelGCThreads);\n+    _safepoint_workers->initialize_workers();\n+  }\n+\n@@ -260,1 +303,1 @@\n-                              align_up(num_committed_regions, _bitmap_regions_per_slice) \/ _bitmap_regions_per_slice;\n+    align_up(num_committed_regions, _bitmap_regions_per_slice) \/ _bitmap_regions_per_slice;\n@@ -267,1 +310,1 @@\n-  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions, _max_workers);\n+  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions);\n@@ -358,0 +401,1 @@\n+  _affiliations = NEW_C_HEAP_ARRAY(uint8_t, _num_regions, mtGC);\n@@ -363,0 +407,1 @@\n+\n@@ -374,0 +419,2 @@\n+\n+      _affiliations[i] = ShenandoahAffiliation::FREE;\n@@ -378,0 +425,1 @@\n+    size_t young_cset_regions, old_cset_regions;\n@@ -379,1 +427,4 @@\n-    _free_set->rebuild();\n+    \/\/ We are initializing free set.  We ignore cset region tallies.\n+    size_t first_old, last_old, num_old;\n+    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+    _free_set->rebuild(young_cset_regions, old_cset_regions);\n@@ -438,1 +489,1 @@\n-  _control_thread = new ShenandoahControlThread();\n+  initialize_controller();\n@@ -440,1 +491,1 @@\n-  ShenandoahInitLogger::print();\n+  print_init_logger();\n@@ -445,1 +496,9 @@\n-void ShenandoahHeap::initialize_mode() {\n+void ShenandoahHeap::initialize_controller() {\n+  _control_thread = new ShenandoahControlThread();\n+}\n+\n+void ShenandoahHeap::print_init_logger() const {\n+  ShenandoahInitLogger::print();\n+}\n+\n+void ShenandoahHeap::initialize_heuristics_generations() {\n@@ -453,0 +512,2 @@\n+    } else if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+      _gc_mode = new ShenandoahGenerationalMode();\n@@ -470,4 +531,9 @@\n-}\n-void ShenandoahHeap::initialize_heuristics() {\n-  assert(_gc_mode != nullptr, \"Must be initialized\");\n-  _heuristics = _gc_mode->initialize_heuristics();\n+  \/\/ Max capacity is the maximum _allowed_ capacity. That is, the maximum allowed capacity\n+  \/\/ for old would be total heap - minimum capacity of young. This means the sum of the maximum\n+  \/\/ allowed for old and young could exceed the total heap size. It remains the case that the\n+  \/\/ _actual_ capacity of young + old = total.\n+  _generation_sizer.heap_size_changed(max_capacity());\n+  size_t initial_capacity_young = _generation_sizer.max_young_size();\n+  size_t max_capacity_young = _generation_sizer.max_young_size();\n+  size_t initial_capacity_old = max_capacity() - max_capacity_young;\n+  size_t max_capacity_old = max_capacity() - initial_capacity_young;\n@@ -476,9 +542,7 @@\n-  if (_heuristics->is_diagnostic() && !UnlockDiagnosticVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.\",\n-                    _heuristics->name()));\n-  }\n-  if (_heuristics->is_experimental() && !UnlockExperimentalVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.\",\n-                    _heuristics->name()));\n+  _young_generation = new ShenandoahYoungGeneration(_max_workers, max_capacity_young, initial_capacity_young);\n+  _old_generation = new ShenandoahOldGeneration(_max_workers, max_capacity_old, initial_capacity_old);\n+  _global_generation = new ShenandoahGlobalGeneration(_gc_mode->is_generational(), _max_workers, max_capacity(), max_capacity());\n+  _global_generation->initialize_heuristics(_gc_mode);\n+  if (mode()->is_generational()) {\n+    _young_generation->initialize_heuristics(_gc_mode);\n+    _old_generation->initialize_heuristics(_gc_mode);\n@@ -486,0 +550,1 @@\n+  _evac_tracker = new ShenandoahEvacuationTracker(mode()->is_generational());\n@@ -495,0 +560,1 @@\n+  _gc_generation(nullptr),\n@@ -496,3 +562,1 @@\n-  _used(0),\n-  _bytes_allocated_since_gc_start(0),\n-  _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),\n+  _max_workers(MAX3(ConcGCThreads, ParallelGCThreads, 1U)),\n@@ -505,0 +569,1 @@\n+  _affiliations(nullptr),\n@@ -508,0 +573,5 @@\n+  _age_census(nullptr),\n+  _cancel_requested_time(0),\n+  _young_generation(nullptr),\n+  _global_generation(nullptr),\n+  _old_generation(nullptr),\n@@ -510,2 +580,0 @@\n-  _gc_mode(nullptr),\n-  _heuristics(nullptr),\n@@ -516,0 +584,3 @@\n+  _evac_tracker(nullptr),\n+  _mmu_tracker(),\n+  _generation_sizer(),\n@@ -522,1 +593,0 @@\n-  _ref_processor(new ShenandoahReferenceProcessor(MAX2(_max_workers, 1U))),\n@@ -530,1 +600,2 @@\n-  _collection_set(nullptr)\n+  _collection_set(nullptr),\n+  _card_scan(nullptr)\n@@ -532,17 +603,0 @@\n-  \/\/ Initialize GC mode early, so we can adjust barrier support\n-  initialize_mode();\n-  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));\n-\n-  _max_workers = MAX2(_max_workers, 1U);\n-  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n-  if (_workers == nullptr) {\n-    vm_exit_during_initialization(\"Failed necessary allocation.\");\n-  } else {\n-    _workers->initialize_workers();\n-  }\n-\n-  if (ParallelGCThreads > 1) {\n-    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\",\n-                                                ParallelGCThreads);\n-    _safepoint_workers->initialize_workers();\n-  }\n@@ -555,29 +609,0 @@\n-class ShenandoahResetBitmapTask : public WorkerTask {\n-private:\n-  ShenandoahRegionIterator _regions;\n-\n-public:\n-  ShenandoahResetBitmapTask() :\n-    WorkerTask(\"Shenandoah Reset Bitmap\") {}\n-\n-  void work(uint worker_id) {\n-    ShenandoahHeapRegion* region = _regions.next();\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahMarkingContext* const ctx = heap->marking_context();\n-    while (region != nullptr) {\n-      if (heap->is_bitmap_slice_committed(region)) {\n-        ctx->clear_bitmap(region);\n-      }\n-      region = _regions.next();\n-    }\n-  }\n-};\n-\n-void ShenandoahHeap::reset_mark_bitmap() {\n-  assert_gc_workers(_workers->active_workers());\n-  mark_incomplete_marking_context();\n-\n-  ShenandoahResetBitmapTask task;\n-  _workers->run_task(&task);\n-}\n-\n@@ -598,1 +623,2 @@\n-  if (is_concurrent_mark_in_progress())        st->print(\"marking, \");\n+  if (is_concurrent_old_mark_in_progress())    st->print(\"old marking, \");\n+  if (is_concurrent_young_mark_in_progress())  st->print(\"young marking, \");\n@@ -649,0 +675,2 @@\n+  _mmu_tracker.initialize();\n+\n@@ -662,2 +690,0 @@\n-  _heuristics->initialize();\n-\n@@ -667,0 +693,24 @@\n+ShenandoahHeuristics* ShenandoahHeap::heuristics() {\n+  return _global_generation->heuristics();\n+}\n+\n+ShenandoahOldHeuristics* ShenandoahHeap::old_heuristics() {\n+  return (ShenandoahOldHeuristics*) _old_generation->heuristics();\n+}\n+\n+ShenandoahYoungHeuristics* ShenandoahHeap::young_heuristics() {\n+  return (ShenandoahYoungHeuristics*) _young_generation->heuristics();\n+}\n+\n+bool ShenandoahHeap::doing_mixed_evacuations() {\n+  return _old_generation->state() == ShenandoahOldGeneration::EVACUATING;\n+}\n+\n+bool ShenandoahHeap::is_old_bitmap_stable() const {\n+  return _old_generation->is_mark_complete();\n+}\n+\n+bool ShenandoahHeap::is_gc_generation_young() const {\n+  return _gc_generation != nullptr && _gc_generation->is_young();\n+}\n+\n@@ -668,1 +718,1 @@\n-  return Atomic::load(&_used);\n+  return global_generation()->used();\n@@ -675,4 +725,0 @@\n-size_t ShenandoahHeap::available() const {\n-  return free_set()->available();\n-}\n-\n@@ -689,2 +735,43 @@\n-void ShenandoahHeap::increase_used(size_t bytes) {\n-  Atomic::add(&_used, bytes, memory_order_relaxed);\n+\/\/ For tracking usage based on allocations, it should be the case that:\n+\/\/ * The sum of regions::used == heap::used\n+\/\/ * The sum of a generation's regions::used == generation::used\n+\/\/ * The sum of a generation's humongous regions::free == generation::humongous_waste\n+\/\/ These invariants are checked by the verifier on GC safepoints.\n+\/\/\n+\/\/ Additional notes:\n+\/\/ * When a mutator's allocation request causes a region to be retired, the\n+\/\/   free memory left in that region is considered waste. It does not contribute\n+\/\/   to the usage, but it _does_ contribute to allocation rate.\n+\/\/ * The bottom of a PLAB must be aligned on card size. In some cases this will\n+\/\/   require padding in front of the PLAB (a filler object). Because this padding\n+\/\/   is included in the region's used memory we include the padding in the usage\n+\/\/   accounting as waste.\n+\/\/ * Mutator allocations are used to compute an allocation rate. They are also\n+\/\/   sent to the Pacer for those purposes.\n+\/\/ * There are three sources of waste:\n+\/\/  1. The padding used to align a PLAB on card size\n+\/\/  2. Region's free is less than minimum TLAB size and is retired\n+\/\/  3. The unused portion of memory in the last region of a humongous object\n+void ShenandoahHeap::increase_used(const ShenandoahAllocRequest& req) {\n+  size_t actual_bytes = req.actual_size() * HeapWordSize;\n+  size_t wasted_bytes = req.waste() * HeapWordSize;\n+  ShenandoahGeneration* generation = generation_for(req.affiliation());\n+\n+  if (req.is_gc_alloc()) {\n+    assert(wasted_bytes == 0 || req.type() == ShenandoahAllocRequest::_alloc_plab, \"Only PLABs have waste\");\n+    increase_used(generation, actual_bytes + wasted_bytes);\n+  } else {\n+    assert(req.is_mutator_alloc(), \"Expected mutator alloc here\");\n+    \/\/ padding and actual size both count towards allocation counter\n+    generation->increase_allocated(actual_bytes + wasted_bytes);\n+\n+    \/\/ only actual size counts toward usage for mutator allocations\n+    increase_used(generation, actual_bytes);\n+\n+    \/\/ notify pacer of both actual size and waste\n+    notify_mutator_alloc_words(req.actual_size(), req.waste());\n+\n+    if (wasted_bytes > 0 && req.actual_size() > ShenandoahHeapRegion::humongous_threshold_words()) {\n+      increase_humongous_waste(generation,wasted_bytes);\n+    }\n+  }\n@@ -693,2 +780,5 @@\n-void ShenandoahHeap::set_used(size_t bytes) {\n-  Atomic::store(&_used, bytes);\n+void ShenandoahHeap::increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_humongous_waste(bytes);\n+  }\n@@ -697,3 +787,5 @@\n-void ShenandoahHeap::decrease_used(size_t bytes) {\n-  assert(used() >= bytes, \"never decrease heap size by more than we've left\");\n-  Atomic::sub(&_used, bytes, memory_order_relaxed);\n+void ShenandoahHeap::decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_humongous_waste(bytes);\n+  }\n@@ -702,2 +794,5 @@\n-void ShenandoahHeap::increase_allocated(size_t bytes) {\n-  Atomic::add(&_bytes_allocated_since_gc_start, bytes, memory_order_relaxed);\n+void ShenandoahHeap::increase_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_used(bytes);\n+  }\n@@ -706,4 +801,4 @@\n-void ShenandoahHeap::notify_mutator_alloc_words(size_t words, bool waste) {\n-  size_t bytes = words * HeapWordSize;\n-  if (!waste) {\n-    increase_used(bytes);\n+void ShenandoahHeap::decrease_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_used(bytes);\n@@ -711,1 +806,3 @@\n-  increase_allocated(bytes);\n+}\n+\n+void ShenandoahHeap::notify_mutator_alloc_words(size_t words, size_t waste) {\n@@ -714,2 +811,2 @@\n-    if (waste) {\n-      pacer()->claim_for_alloc(words, true);\n+    if (waste > 0) {\n+      pacer()->claim_for_alloc(waste, true);\n@@ -751,6 +848,0 @@\n-bool ShenandoahHeap::is_in(const void* p) const {\n-  HeapWord* heap_base = (HeapWord*) base();\n-  HeapWord* last_region_end = heap_base + ShenandoahHeapRegion::region_size_words() * num_regions();\n-  return p >= heap_base && p < last_region_end;\n-}\n-\n@@ -836,3 +927,1 @@\n-\n-  \/\/ This is called from allocation path, and thus should be fast.\n-  _heap_changed.try_set();\n+  _heap_changed.set();\n@@ -855,0 +944,8 @@\n+\n+  \/\/ Limit growth of GCLABs to ShenandoahMaxEvacLABRatio * the minimum size.  This enables more equitable distribution of\n+  \/\/ available evacuation buidget between the many threads that are coordinating in the evacuation effort.\n+  if (ShenandoahMaxEvacLABRatio > 0) {\n+    log_debug(gc, free)(\"Allocate new gclab: \" SIZE_FORMAT \", \" SIZE_FORMAT, new_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+    new_size = MIN2(new_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+  }\n+\n@@ -866,0 +963,1 @@\n+    log_debug(gc, free)(\"New gclab size (\" SIZE_FORMAT \") is too small for \" SIZE_FORMAT, new_size, size);\n@@ -895,0 +993,152 @@\n+\/\/ Establish a new PLAB and allocate size HeapWords within it.\n+HeapWord* ShenandoahHeap::allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion) {\n+  \/\/ New object should fit the PLAB size\n+\n+  assert(mode()->is_generational(), \"PLABs only relevant to generational GC\");\n+  ShenandoahGenerationalHeap* generational_heap = (ShenandoahGenerationalHeap*) this;\n+  const size_t plab_min_size = generational_heap->plab_min_size();\n+  const size_t min_size = (size > plab_min_size)? align_up(size, CardTable::card_size_in_words()): plab_min_size;\n+\n+  \/\/ Figure out size of new PLAB, looking back at heuristics. Expand aggressively.  PLABs must align on size\n+  \/\/ of card table in order to avoid the need for synchronization when registering newly allocated objects within\n+  \/\/ the card table.\n+  size_t cur_size = ShenandoahThreadLocalData::plab_size(thread);\n+  if (cur_size == 0) {\n+    cur_size = plab_min_size;\n+  }\n+\n+  \/\/ Limit growth of PLABs to the smaller of ShenandoahMaxEvacLABRatio * the minimum size and ShenandoahHumongousThreshold.\n+  \/\/ This minimum value is represented by generational_heap->plab_max_size().  Enforcing this limit enables more equitable\n+  \/\/ distribution of available evacuation budget between the many threads that are coordinating in the evacuation effort.\n+  size_t future_size = MIN2(cur_size * 2, generational_heap->plab_max_size());\n+  assert(is_aligned(future_size, CardTable::card_size_in_words()), \"Align by design, future_size: \" SIZE_FORMAT\n+         \", alignment: \" SIZE_FORMAT \", cur_size: \" SIZE_FORMAT \", max: \" SIZE_FORMAT,\n+         future_size, (size_t) CardTable::card_size_in_words(), cur_size, generational_heap->plab_max_size());\n+\n+  \/\/ Record new heuristic value even if we take any shortcut. This captures\n+  \/\/ the case when moderately-sized objects always take a shortcut. At some point,\n+  \/\/ heuristics should catch up with them.  Note that the requested cur_size may\n+  \/\/ not be honored, but we remember that this is the preferred size.\n+  ShenandoahThreadLocalData::set_plab_size(thread, future_size);\n+  if (cur_size < size) {\n+    \/\/ The PLAB to be allocated is still not large enough to hold the object. Fall back to shared allocation.\n+    \/\/ This avoids retiring perfectly good PLABs in order to represent a single large object allocation.\n+    return nullptr;\n+  }\n+\n+  \/\/ Retire current PLAB, and allocate a new one.\n+  PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+  if (plab->words_remaining() < plab_min_size) {\n+    \/\/ Retire current PLAB, and allocate a new one.\n+    \/\/ CAUTION: retire_plab may register the remnant filler object with the remembered set scanner without a lock.  This\n+    \/\/ is safe iff it is assured that each PLAB is a whole-number multiple of card-mark memory size and each PLAB is\n+    \/\/ aligned with the start of a card's memory range.\n+    retire_plab(plab, thread);\n+\n+    size_t actual_size = 0;\n+    \/\/ allocate_new_plab resets plab_evacuated and plab_promoted and disables promotions if old-gen available is\n+    \/\/ less than the remaining evacuation need.  It also adjusts plab_preallocated and expend_promoted if appropriate.\n+    HeapWord* plab_buf = allocate_new_plab(min_size, cur_size, &actual_size);\n+    if (plab_buf == nullptr) {\n+      if (min_size == plab_min_size) {\n+        \/\/ Disable plab promotions for this thread because we cannot even allocate a plab of minimal size.  This allows us\n+        \/\/ to fail faster on subsequent promotion attempts.\n+        ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+      }\n+      return NULL;\n+    } else {\n+      ShenandoahThreadLocalData::enable_plab_retries(thread);\n+    }\n+    \/\/ Since the allocated PLAB may have been down-sized for alignment, plab->allocate(size) below may still fail.\n+    if (ZeroTLAB) {\n+      \/\/ ..and clear it.\n+      Copy::zero_to_words(plab_buf, actual_size);\n+    } else {\n+      \/\/ ...and zap just allocated object.\n+#ifdef ASSERT\n+      \/\/ Skip mangling the space corresponding to the object header to\n+      \/\/ ensure that the returned space is not considered parsable by\n+      \/\/ any concurrent GC thread.\n+      size_t hdr_size = oopDesc::header_size();\n+      Copy::fill_to_words(plab_buf + hdr_size, actual_size - hdr_size, badHeapWordVal);\n+#endif \/\/ ASSERT\n+    }\n+    assert(is_aligned(actual_size, CardTable::card_size_in_words()), \"Align by design\");\n+    plab->set_buf(plab_buf, actual_size);\n+    if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+      return nullptr;\n+    }\n+    return plab->allocate(size);\n+  } else {\n+    \/\/ If there's still at least min_size() words available within the current plab, don't retire it.  Let's gnaw\n+    \/\/ away on this plab as long as we can.  Meanwhile, return nullptr to force this particular allocation request\n+    \/\/ to be satisfied with a shared allocation.  By packing more promotions into the previously allocated PLAB, we\n+    \/\/ reduce the likelihood of evacuation failures, and we we reduce the need for downsizing our PLABs.\n+    return nullptr;\n+  }\n+}\n+\n+\/\/ TODO: It is probably most efficient to register all objects (both promotions and evacuations) that were allocated within\n+\/\/ this plab at the time we retire the plab.  A tight registration loop will run within both code and data caches.  This change\n+\/\/ would allow smaller and faster in-line implementation of alloc_from_plab().  Since plabs are aligned on card-table boundaries,\n+\/\/ this object registration loop can be performed without acquiring a lock.\n+void ShenandoahHeap::retire_plab(PLAB* plab, Thread* thread) {\n+  \/\/ We don't enforce limits on plab_evacuated.  We let it consume all available old-gen memory in order to reduce\n+  \/\/ probability of an evacuation failure.  We do enforce limits on promotion, to make sure that excessive promotion\n+  \/\/ does not result in an old-gen evacuation failure.  Note that a failed promotion is relatively harmless.  Any\n+  \/\/ object that fails to promote in the current cycle will be eligible for promotion in a subsequent cycle.\n+\n+  \/\/ When the plab was instantiated, its entirety was treated as if the entire buffer was going to be dedicated to\n+  \/\/ promotions.  Now that we are retiring the buffer, we adjust for the reality that the plab is not entirely promotions.\n+  \/\/  1. Some of the plab may have been dedicated to evacuations.\n+  \/\/  2. Some of the plab may have been abandoned due to waste (at the end of the plab).\n+  size_t not_promoted =\n+    ShenandoahThreadLocalData::get_plab_preallocated_promoted(thread) - ShenandoahThreadLocalData::get_plab_promoted(thread);\n+  ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+  ShenandoahThreadLocalData::reset_plab_evacuated(thread);\n+  ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+  if (not_promoted > 0) {\n+    old_generation()->unexpend_promoted(not_promoted);\n+  }\n+  const size_t original_waste = plab->waste();\n+  HeapWord* const top = plab->top();\n+\n+  \/\/ plab->retire() overwrites unused memory between plab->top() and plab->hard_end() with a dummy object to make memory parsable.\n+  \/\/ It adds the size of this unused memory, in words, to plab->waste().\n+  plab->retire();\n+  if (top != nullptr && plab->waste() > original_waste && is_in_old(top)) {\n+    \/\/ If retiring the plab created a filler object, then we need to register it with our card scanner so it can\n+    \/\/ safely walk the region backing the plab.\n+    log_debug(gc)(\"retire_plab() is registering remnant of size \" SIZE_FORMAT \" at \" PTR_FORMAT,\n+                  plab->waste() - original_waste, p2i(top));\n+    card_scan()->register_object_without_lock(top);\n+  }\n+}\n+\n+void ShenandoahHeap::retire_plab(PLAB* plab) {\n+  Thread* thread = Thread::current();\n+  retire_plab(plab, thread);\n+}\n+\n+void ShenandoahHeap::cancel_old_gc() {\n+  shenandoah_assert_safepoint();\n+  assert(_old_generation != nullptr, \"Should only have mixed collections in generation mode.\");\n+  if (_old_generation->state() == ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP) {\n+    assert(!old_generation()->is_concurrent_mark_in_progress(), \"Cannot be marking in IDLE\");\n+    assert(!old_heuristics()->has_coalesce_and_fill_candidates(), \"Cannot have coalesce and fill candidates in IDLE\");\n+    assert(!old_heuristics()->unprocessed_old_collection_candidates(), \"Cannot have mixed collection candidates in IDLE\");\n+    assert(!young_generation()->is_bootstrap_cycle(), \"Cannot have old mark queues if IDLE\");\n+  } else {\n+    log_info(gc)(\"Terminating old gc cycle.\");\n+    \/\/ Stop marking\n+    old_generation()->cancel_marking();\n+    \/\/ Stop tracking old regions\n+    old_heuristics()->abandon_collection_candidates();\n+    \/\/ Remove old generation access to young generation mark queues\n+    young_generation()->set_old_gen_task_queues(nullptr);\n+    \/\/ Transition to IDLE now.\n+    _old_generation->transition_to(ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP);\n+  }\n+}\n+\n+\/\/ Called from stubs in JIT code or interpreter\n@@ -899,1 +1149,1 @@\n-  HeapWord* res = allocate_memory(req);\n+  HeapWord* res = allocate_memory(req, false);\n@@ -912,1 +1162,1 @@\n-  HeapWord* res = allocate_memory(req);\n+  HeapWord* res = allocate_memory(req, false);\n@@ -921,1 +1171,21 @@\n-HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest& req) {\n+HeapWord* ShenandoahHeap::allocate_new_plab(size_t min_size, size_t word_size, size_t* actual_size) {\n+  \/\/ Align requested sizes to card-sized multiples.  Align down so that we don't violate max size of TLAB.\n+  assert(is_aligned(min_size, CardTable::card_size_in_words()), \"Align by design\");\n+  assert(word_size >= min_size, \"Requested PLAB is too small\");\n+\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(min_size, word_size);\n+  \/\/ Note that allocate_memory() sets a thread-local flag to prohibit further promotions by this thread\n+  \/\/ if we are at risk of infringing on the old-gen evacuation budget.\n+  HeapWord* res = allocate_memory(req, false);\n+  if (res != nullptr) {\n+    *actual_size = req.actual_size();\n+  } else {\n+    *actual_size = 0;\n+  }\n+  assert(is_aligned(res, CardTable::card_size_in_words()), \"Align by design\");\n+  return res;\n+}\n+\n+\/\/ is_promotion is true iff this allocation is known for sure to hold the result of young-gen evacuation\n+\/\/ to old-gen.  plab allocates are not known as such, since they may hold old-gen evacuations.\n+HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest& req, bool is_promotion) {\n@@ -933,1 +1203,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -958,2 +1228,2 @@\n-      control_thread()->handle_alloc_failure(req);\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      control_thread()->handle_alloc_failure(req, true);\n+      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -967,0 +1237,1 @@\n+\n@@ -969,1 +1240,1 @@\n-    result = allocate_memory_under_lock(req, in_new_region);\n+    result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -978,0 +1249,8 @@\n+  if (result == nullptr) {\n+    req.set_actual_size(0);\n+  }\n+\n+  \/\/ This is called regardless of the outcome of the allocation to account\n+  \/\/ for any waste created by retiring regions with this request.\n+  increase_used(req);\n+\n@@ -987,2 +1266,0 @@\n-      notify_mutator_alloc_words(actual, false);\n-\n@@ -995,2 +1272,0 @@\n-    } else {\n-      increase_used(actual*HeapWordSize);\n@@ -1003,7 +1278,185 @@\n-HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest& req, bool& in_new_region) {\n-  \/\/ If we are dealing with mutator allocation, then we may need to block for safepoint.\n-  \/\/ We cannot block for safepoint for GC allocations, because there is a high chance\n-  \/\/ we are already running at safepoint or from stack watermark machinery, and we cannot\n-  \/\/ block again.\n-  ShenandoahHeapLocker locker(lock(), req.is_mutator_alloc());\n-  return _free_set->allocate(req, in_new_region);\n+HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest& req, bool& in_new_region, bool is_promotion) {\n+  bool try_smaller_lab_size = false;\n+  size_t smaller_lab_size;\n+  {\n+    \/\/ promotion_eligible pertains only to PLAB allocations, denoting that the PLAB is allowed to allocate for promotions.\n+    bool promotion_eligible = false;\n+    bool allow_allocation = true;\n+    bool plab_alloc = false;\n+    size_t requested_bytes = req.size() * HeapWordSize;\n+    HeapWord* result = nullptr;\n+\n+    \/\/ If we are dealing with mutator allocation, then we may need to block for safepoint.\n+    \/\/ We cannot block for safepoint for GC allocations, because there is a high chance\n+    \/\/ we are already running at safepoint or from stack watermark machinery, and we cannot\n+    \/\/ block again.\n+    ShenandoahHeapLocker locker(lock(), req.is_mutator_alloc());\n+    Thread* thread = Thread::current();\n+\n+    if (mode()->is_generational()) {\n+      if (req.affiliation() == YOUNG_GENERATION) {\n+        if (req.is_mutator_alloc()) {\n+          size_t young_words_available = young_generation()->available() \/ HeapWordSize;\n+          if (req.is_lab_alloc() && (req.min_size() < young_words_available)) {\n+            \/\/ Allow ourselves to try a smaller lab size even if requested_bytes <= young_available.  We may need a smaller\n+            \/\/ lab size because young memory has become too fragmented.\n+            try_smaller_lab_size = true;\n+            smaller_lab_size = (young_words_available < req.size())? young_words_available: req.size();\n+          } else if (req.size() > young_words_available) {\n+            \/\/ Can't allocate because even min_size() is larger than remaining young_available\n+            log_info(gc, ergo)(\"Unable to shrink %s alloc request of minimum size: \" SIZE_FORMAT\n+                               \", young words available: \" SIZE_FORMAT, req.type_string(),\n+                               HeapWordSize * (req.is_lab_alloc()? req.min_size(): req.size()), young_words_available);\n+            return nullptr;\n+          }\n+        }\n+      } else {                    \/\/ reg.affiliation() == OLD_GENERATION\n+        assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"GCLAB pertains only to young-gen memory\");\n+        if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+          plab_alloc = true;\n+          size_t promotion_avail = old_generation()->get_promoted_reserve();\n+          size_t promotion_expended = old_generation()->get_promoted_expended();\n+          if (promotion_expended + requested_bytes > promotion_avail) {\n+            promotion_avail = 0;\n+            if (old_generation()->get_evacuation_reserve() == 0) {\n+              \/\/ There are no old-gen evacuations in this pass.  There's no value in creating a plab that cannot\n+              \/\/ be used for promotions.\n+              allow_allocation = false;\n+            }\n+          } else {\n+            promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n+            promotion_eligible = true;\n+          }\n+        } else if (is_promotion) {\n+          \/\/ This is a shared alloc for promotion\n+          size_t promotion_avail = old_generation()->get_promoted_reserve();\n+          size_t promotion_expended = old_generation()->get_promoted_expended();\n+          if (promotion_expended + requested_bytes > promotion_avail) {\n+            promotion_avail = 0;\n+          } else {\n+            promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n+          }\n+          if (promotion_avail == 0) {\n+            \/\/ We need to reserve the remaining memory for evacuation.  Reject this allocation.  The object will be\n+            \/\/ evacuated to young-gen memory and promoted during a future GC pass.\n+            return nullptr;\n+          }\n+          \/\/ Else, we'll allow the allocation to proceed.  (Since we hold heap lock, the tested condition remains true.)\n+        } else {\n+          \/\/ This is a shared allocation for evacuation.  Memory has already been reserved for this purpose.\n+        }\n+      }\n+    } \/\/ This ends the is_generational() block\n+\n+    \/\/ First try the original request.  If TLAB request size is greater than available, allocate() will attempt to downsize\n+    \/\/ request to fit within available memory.\n+    result = (allow_allocation)? _free_set->allocate(req, in_new_region): nullptr;\n+    if (result != nullptr) {\n+      if (req.is_old()) {\n+        ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+        if (req.is_gc_alloc()) {\n+          bool disable_plab_promotions = false;\n+          if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+            if (promotion_eligible) {\n+              size_t actual_size = req.actual_size() * HeapWordSize;\n+              \/\/ The actual size of the allocation may be larger than the requested bytes (due to alignment on card boundaries).\n+              \/\/ If this puts us over our promotion budget, we need to disable future PLAB promotions for this thread.\n+              if (old_generation()->get_promoted_expended() + actual_size <= old_generation()->get_promoted_reserve()) {\n+                \/\/ Assume the entirety of this PLAB will be used for promotion.  This prevents promotion from overreach.\n+                \/\/ When we retire this plab, we'll unexpend what we don't really use.\n+                ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+                old_generation()->expend_promoted(actual_size);\n+                ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, actual_size);\n+              } else {\n+                disable_plab_promotions = true;\n+              }\n+            } else {\n+              disable_plab_promotions = true;\n+            }\n+            if (disable_plab_promotions) {\n+              \/\/ Disable promotions in this thread because entirety of this PLAB must be available to hold old-gen evacuations.\n+              ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+              ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+            }\n+          } else if (is_promotion) {\n+            \/\/ Shared promotion.  Assume size is requested_bytes.\n+            old_generation()->expend_promoted(requested_bytes);\n+          }\n+        }\n+\n+        \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+        \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+        \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+        \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+        \/\/\n+        \/\/ objects being \"concurrently\" allocated:\n+        \/\/    [-----a------][-----b-----][--------------c------------------]\n+        \/\/            [---- card table memory range --------------]\n+        \/\/\n+        \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n+        \/\/   wants to set the starts-object, first-start, and last-start attributes of the preceding card region.\n+        \/\/   allocation of object b wants to set the starts-object, first-start, and last-start attributes of this card region.\n+        \/\/   allocation of object c also wants to set the starts-object, first-start, and last-start attributes of this\n+        \/\/   card region.\n+        \/\/\n+        \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n+        \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n+        \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+        ShenandoahHeap::heap()->card_scan()->register_object(result);\n+      }\n+    } else {\n+      \/\/ The allocation failed.  If this was a plab allocation, We've already retired it and no longer have a plab.\n+      if (req.is_old() && req.is_gc_alloc() && (req.type() == ShenandoahAllocRequest::_alloc_plab)) {\n+        \/\/ We don't need to disable PLAB promotions because there is no PLAB.  We leave promotions enabled because\n+        \/\/ this allows the surrounding infrastructure to retry alloc_plab_slow() with a smaller PLAB size.\n+        ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+      }\n+    }\n+    if ((result != nullptr) || !try_smaller_lab_size) {\n+      return result;\n+    }\n+    \/\/ else, fall through to try_smaller_lab_size\n+  } \/\/ This closes the block that holds the heap lock, releasing the lock.\n+\n+  \/\/ We failed to allocate the originally requested lab size.  Let's see if we can allocate a smaller lab size.\n+  if (req.size() == smaller_lab_size) {\n+    \/\/ If we were already trying to allocate min size, no value in attempting to repeat the same.  End the recursion.\n+    return nullptr;\n+  }\n+\n+  \/\/ We arrive here if the tlab allocation request can be resized to fit within young_available\n+  assert((req.affiliation() == YOUNG_GENERATION) && req.is_lab_alloc() && req.is_mutator_alloc() &&\n+         (smaller_lab_size < req.size()), \"Only shrink allocation request size for TLAB allocations\");\n+\n+  \/\/ By convention, ShenandoahAllocationRequest is primarily read-only.  The only mutable instance data is represented by\n+  \/\/ actual_size(), which is overwritten with the size of the allocaion when the allocation request is satisfied.  We use a\n+  \/\/ recursive call here rather than introducing new methods to mutate the existing ShenandoahAllocationRequest argument.\n+  \/\/ Mutation of the existing object might result in astonishing results if calling contexts assume the content of immutable\n+  \/\/ fields remain constant.  The original TLAB allocation request was for memory that exceeded the current capacity.  We'll\n+  \/\/ attempt to allocate a smaller TLAB.  If this is successful, we'll update actual_size() of our incoming\n+  \/\/ ShenandoahAllocRequest.  If the recursive request fails, we'll simply return nullptr.\n+\n+  \/\/ Note that we've relinquished the HeapLock and some other thread may perform additional allocation before our recursive\n+  \/\/ call reacquires the lock.  If that happens, we will need another recursive call to further reduce the size of our request\n+  \/\/ for each time another thread allocates young memory during the brief intervals that the heap lock is available to\n+  \/\/ interfering threads.  We expect this interference to be rare.  The recursion bottoms out when young_available is\n+  \/\/ smaller than req.min_size().  The inner-nested call to allocate_memory_under_lock() uses the same min_size() value\n+  \/\/ as this call, but it uses a preferred size() that is smaller than our preferred size, and is no larger than what we most\n+  \/\/ recently saw as the memory currently available within the young generation.\n+\n+  \/\/ TODO: At the expense of code clarity, we could rewrite this recursive solution to use iteration.  We need at most one\n+  \/\/ extra instance of the ShenandoahAllocRequest, which we can re-initialize multiple times inside a loop, with one iteration\n+  \/\/ of the loop required for each time the existing solution would recurse.  An iterative solution would be more efficient\n+  \/\/ in CPU time and stack memory utilization.  The expectation is that it is very rare that we would recurse more than once\n+  \/\/ so making this change is not currently seen as a high priority.\n+\n+  ShenandoahAllocRequest smaller_req = ShenandoahAllocRequest::for_tlab(req.min_size(), smaller_lab_size);\n+\n+  \/\/ Note that shrinking the preferred size gets us past the gatekeeper that checks whether there's available memory to\n+  \/\/ satisfy the allocation request.  The reality is the actual TLAB size is likely to be even smaller, because it will\n+  \/\/ depend on how much memory is available within mutator regions that are not yet fully used.\n+  HeapWord* result = allocate_memory_under_lock(smaller_req, in_new_region, is_promotion);\n+  if (result != nullptr) {\n+    req.set_actual_size(smaller_req.actual_size());\n+  }\n+  return result;\n@@ -1015,1 +1468,1 @@\n-  return allocate_memory(req);\n+  return allocate_memory(req, false);\n@@ -1024,2 +1477,2 @@\n-  if (heuristics()->can_unload_classes()) {\n-    ShenandoahHeuristics* h = heuristics();\n+  ShenandoahHeuristics* h = global_generation()->heuristics();\n+  if (h->can_unload_classes()) {\n@@ -1104,0 +1557,1 @@\n+\n@@ -1109,0 +1563,90 @@\n+      if (_sh->check_cancelled_gc_and_yield(_concurrent)) {\n+        break;\n+      }\n+    }\n+  }\n+};\n+\n+\/\/ Unlike ShenandoahEvacuationTask, this iterates over all regions rather than just the collection set.\n+\/\/ This is needed in order to promote humongous start regions if age() >= tenure threshold.\n+class ShenandoahGenerationalEvacuationTask : public WorkerTask {\n+private:\n+  ShenandoahHeap* const _sh;\n+  ShenandoahRegionIterator *_regions;\n+  bool _concurrent;\n+  uint _tenuring_threshold;\n+\n+public:\n+  ShenandoahGenerationalEvacuationTask(ShenandoahHeap* sh,\n+                                       ShenandoahRegionIterator* iterator,\n+                                       bool concurrent) :\n+    WorkerTask(\"Shenandoah Evacuation\"),\n+    _sh(sh),\n+    _regions(iterator),\n+    _concurrent(concurrent),\n+    _tenuring_threshold(0)\n+  {\n+    if (_sh->mode()->is_generational()) {\n+      _tenuring_threshold = _sh->age_census()->tenuring_threshold();\n+    }\n+  }\n+\n+  void work(uint worker_id) {\n+    if (_concurrent) {\n+      ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+      ShenandoahSuspendibleThreadSetJoiner stsj;\n+      ShenandoahEvacOOMScope oom_evac_scope;\n+      do_work();\n+    } else {\n+      ShenandoahParallelWorkerSession worker_session(worker_id);\n+      ShenandoahEvacOOMScope oom_evac_scope;\n+      do_work();\n+    }\n+  }\n+\n+private:\n+  void do_work() {\n+    ShenandoahConcurrentEvacuateRegionObjectClosure cl(_sh);\n+    ShenandoahHeapRegion* r;\n+    ShenandoahMarkingContext* const ctx = ShenandoahHeap::heap()->marking_context();\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    size_t old_garbage_threshold = (region_size_bytes * ShenandoahOldGarbageThreshold) \/ 100;\n+    while ((r = _regions->next()) != nullptr) {\n+      log_debug(gc)(\"GenerationalEvacuationTask do_work(), looking at %s region \" SIZE_FORMAT \", (age: %d) [%s, %s, %s]\",\n+                    r->is_old()? \"old\": r->is_young()? \"young\": \"free\", r->index(), r->age(),\n+                    r->is_active()? \"active\": \"inactive\",\n+                    r->is_humongous()? (r->is_humongous_start()? \"humongous_start\": \"humongous_continuation\"): \"regular\",\n+                    r->is_cset()? \"cset\": \"not-cset\");\n+\n+      if (r->is_cset()) {\n+        assert(r->has_live(), \"Region \" SIZE_FORMAT \" should have been reclaimed early\", r->index());\n+        _sh->marked_object_iterate(r, &cl);\n+        if (ShenandoahPacing) {\n+          _sh->pacer()->report_evac(r->used() >> LogHeapWordSize);\n+        }\n+      } else if (r->is_young() && r->is_active() && (r->age() >= _tenuring_threshold)) {\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        if (r->is_humongous_start()) {\n+          \/\/ We promote humongous_start regions along with their affiliated continuations during evacuation rather than\n+          \/\/ doing this work during a safepoint.  We cannot put humongous regions into the collection set because that\n+          \/\/ triggers the load-reference barrier (LRB) to copy on reference fetch.\n+          r->promote_humongous();\n+        } else if (r->is_regular() && (r->get_top_before_promote() != nullptr)) {\n+          assert(r->garbage_before_padded_for_promote() < old_garbage_threshold,\n+                 \"Region \" SIZE_FORMAT \" has too much garbage for promotion\", r->index());\n+          assert(r->get_top_before_promote() == tams,\n+                 \"Region \" SIZE_FORMAT \" has been used for allocations before promotion\", r->index());\n+          \/\/ Likewise, we cannot put promote-in-place regions into the collection set because that would also trigger\n+          \/\/ the LRB to copy on reference fetch.\n+          r->promote_in_place();\n+        }\n+        \/\/ Aged humongous continuation regions are handled with their start region.  If an aged regular region has\n+        \/\/ more garbage than ShenandoahOldGarbageTrheshold, we'll promote by evacuation.  If there is room for evacuation\n+        \/\/ in this cycle, the region will be in the collection set.  If there is not room, the region will be promoted\n+        \/\/ by evacuation in some future GC cycle.\n+\n+        \/\/ If an aged regular region has received allocations during the current cycle, we do not promote because the\n+        \/\/ newly allocated objects do not have appropriate age; this region's age will be reset to zero at end of cycle.\n+      }\n+      \/\/ else, region is free, or OLD, or not in collection set, or humongous_continuation,\n+      \/\/ or is young humongous_start that is too young to be promoted\n@@ -1118,2 +1662,187 @@\n-  ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n-  workers()->run_task(&task);\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahRegionIterator regions;\n+    ShenandoahGenerationalEvacuationTask task(this, &regions, concurrent);\n+    workers()->run_task(&task);\n+  } else {\n+    ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n+    workers()->run_task(&task);\n+  }\n+}\n+\n+\/\/ try_evacuate_object registers the object and dirties the associated remembered set information when evacuating\n+\/\/ to OLD_GENERATION.\n+oop ShenandoahHeap::try_evacuate_object(oop p, Thread* thread, ShenandoahHeapRegion* from_region,\n+                                               ShenandoahAffiliation target_gen) {\n+  bool alloc_from_lab = true;\n+  bool has_plab = false;\n+  HeapWord* copy = nullptr;\n+  size_t size = p->size();\n+  bool is_promotion = (target_gen == OLD_GENERATION) && from_region->is_young();\n+\n+#ifdef ASSERT\n+  if (ShenandoahOOMDuringEvacALot &&\n+      (os::random() & 1) == 0) { \/\/ Simulate OOM every ~2nd slow-path call\n+    copy = nullptr;\n+  } else {\n+#endif\n+    if (UseTLAB) {\n+      switch (target_gen) {\n+        case YOUNG_GENERATION: {\n+          copy = allocate_from_gclab(thread, size);\n+          if ((copy == nullptr) && (size < ShenandoahThreadLocalData::gclab_size(thread))) {\n+            \/\/ GCLAB allocation failed because we are bumping up against the limit on young evacuation reserve.  Try resetting\n+            \/\/ the desired GCLAB size and retry GCLAB allocation to avoid cascading of shared memory allocations.\n+            ShenandoahThreadLocalData::set_gclab_size(thread, PLAB::min_size());\n+            copy = allocate_from_gclab(thread, size);\n+            \/\/ If we still get nullptr, we'll try a shared allocation below.\n+          }\n+          break;\n+        }\n+        case OLD_GENERATION: {\n+          assert(mode()->is_generational(), \"OLD Generation only exists in generational mode\");\n+          ShenandoahGenerationalHeap* gen_heap = (ShenandoahGenerationalHeap*) this;\n+          PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+          if (plab != nullptr) {\n+            has_plab = true;\n+          }\n+          copy = allocate_from_plab(thread, size, is_promotion);\n+          if ((copy == nullptr) && (size < ShenandoahThreadLocalData::plab_size(thread)) &&\n+              ShenandoahThreadLocalData::plab_retries_enabled(thread)) {\n+            \/\/ PLAB allocation failed because we are bumping up against the limit on old evacuation reserve or because\n+            \/\/ the requested object does not fit within the current plab but the plab still has an \"abundance\" of memory,\n+            \/\/ where abundance is defined as >= ShenGenHeap::plab_min_size().  In the former case, we try resetting the desired\n+            \/\/ PLAB size and retry PLAB allocation to avoid cascading of shared memory allocations.\n+\n+            \/\/ In this situation, PLAB memory is precious.  We'll try to preserve our existing PLAB by forcing\n+            \/\/ this particular allocation to be shared.\n+            if (plab->words_remaining() < gen_heap->plab_min_size()) {\n+              ShenandoahThreadLocalData::set_plab_size(thread, gen_heap->plab_min_size());\n+              copy = allocate_from_plab(thread, size, is_promotion);\n+              \/\/ If we still get nullptr, we'll try a shared allocation below.\n+              if (copy == nullptr) {\n+                \/\/ If retry fails, don't continue to retry until we have success (probably in next GC pass)\n+                ShenandoahThreadLocalData::disable_plab_retries(thread);\n+              }\n+            }\n+            \/\/ else, copy still equals nullptr.  this causes shared allocation below, preserving this plab for future needs.\n+          }\n+          break;\n+        }\n+        default: {\n+          ShouldNotReachHere();\n+          break;\n+        }\n+      }\n+    }\n+\n+    if (copy == nullptr) {\n+      \/\/ If we failed to allocate in LAB, we'll try a shared allocation.\n+      if (!is_promotion || !has_plab || (size > PLAB::min_size())) {\n+        ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen);\n+        copy = allocate_memory(req, is_promotion);\n+        alloc_from_lab = false;\n+      }\n+      \/\/ else, we leave copy equal to nullptr, signaling a promotion failure below if appropriate.\n+      \/\/ We choose not to promote objects smaller than PLAB::min_size() by way of shared allocations, as this is too\n+      \/\/ costly.  Instead, we'll simply \"evacuate\" to young-gen memory (using a GCLAB) and will promote in a future\n+      \/\/ evacuation pass.  This condition is denoted by: is_promotion && has_plab && (size <= PLAB::min_size())\n+    }\n+#ifdef ASSERT\n+  }\n+#endif\n+\n+  if (copy == nullptr) {\n+    if (target_gen == OLD_GENERATION) {\n+      assert(mode()->is_generational(), \"Should only be here in generational mode.\");\n+      if (from_region->is_young()) {\n+        \/\/ Signal that promotion failed. Will evacuate this old object somewhere in young gen.\n+        old_generation()->handle_failed_promotion(thread, size);\n+        return nullptr;\n+      } else {\n+        \/\/ Remember that evacuation to old gen failed. We'll want to trigger a full gc to recover from this\n+        \/\/ after the evacuation threads have finished.\n+        old_generation()->handle_failed_evacuation();\n+      }\n+    }\n+\n+    control_thread()->handle_alloc_failure_evac(size);\n+\n+    _oom_evac_handler.handle_out_of_memory_during_evacuation();\n+\n+    return ShenandoahBarrierSet::resolve_forwarded(p);\n+  }\n+\n+  \/\/ Copy the object:\n+  _evac_tracker->begin_evacuation(thread, size * HeapWordSize);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, size);\n+\n+  oop copy_val = cast_to_oop(copy);\n+\n+  if (mode()->is_generational() && target_gen == YOUNG_GENERATION && is_aging_cycle()) {\n+    ShenandoahHeap::increase_object_age(copy_val, from_region->age() + 1);\n+  }\n+\n+  \/\/ Try to install the new forwarding pointer.\n+  ContinuationGCSupport::relativize_stack_chunk(copy_val);\n+\n+  oop result = ShenandoahForwarding::try_update_forwardee(p, copy_val);\n+  if (result == copy_val) {\n+    \/\/ Successfully evacuated. Our copy is now the public one!\n+    _evac_tracker->end_evacuation(thread, size * HeapWordSize);\n+    if (mode()->is_generational()) {\n+      if (target_gen == OLD_GENERATION) {\n+        old_generation()->handle_evacuation(copy, size, from_region->is_young());\n+      } else {\n+        \/\/ When copying to the old generation above, we don't care\n+        \/\/ about recording object age in the census stats.\n+        assert(target_gen == YOUNG_GENERATION, \"Error\");\n+        \/\/ We record this census only when simulating pre-adaptive tenuring behavior, or\n+        \/\/ when we have been asked to record the census at evacuation rather than at mark\n+        if (ShenandoahGenerationalCensusAtEvac || !ShenandoahGenerationalAdaptiveTenuring) {\n+          _evac_tracker->record_age(thread, size * HeapWordSize, ShenandoahHeap::get_object_age(copy_val));\n+        }\n+      }\n+    }\n+    shenandoah_assert_correct(nullptr, copy_val);\n+    return copy_val;\n+  }  else {\n+    \/\/ Failed to evacuate. We need to deal with the object that is left behind. Since this\n+    \/\/ new allocation is certainly after TAMS, it will be considered live in the next cycle.\n+    \/\/ But if it happens to contain references to evacuated regions, those references would\n+    \/\/ not get updated for this stale copy during this cycle, and we will crash while scanning\n+    \/\/ it the next cycle.\n+    if (alloc_from_lab) {\n+      \/\/ For LAB allocations, it is enough to rollback the allocation ptr. Either the next\n+      \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n+      \/\/ do this.\n+      switch (target_gen) {\n+        case YOUNG_GENERATION: {\n+          ShenandoahThreadLocalData::gclab(thread)->undo_allocation(copy, size);\n+          break;\n+        }\n+        case OLD_GENERATION: {\n+          ShenandoahThreadLocalData::plab(thread)->undo_allocation(copy, size);\n+          if (is_promotion) {\n+            ShenandoahThreadLocalData::subtract_from_plab_promoted(thread, size * HeapWordSize);\n+          } else {\n+            ShenandoahThreadLocalData::subtract_from_plab_evacuated(thread, size * HeapWordSize);\n+          }\n+          break;\n+        }\n+        default: {\n+          ShouldNotReachHere();\n+          break;\n+        }\n+      }\n+    } else {\n+      \/\/ For non-LAB allocations, we have no way to retract the allocation, and\n+      \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n+      \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n+      assert(size >= ShenandoahHeap::min_fill_size(), \"previously allocated object known to be larger than min_size\");\n+      fill_with_object(copy, size);\n+      shenandoah_assert_correct(nullptr, copy_val);\n+      \/\/ For non-LAB allocations, the object has already been registered\n+    }\n+    shenandoah_assert_correct(nullptr, result);\n+    return result;\n+  }\n@@ -1149,1 +1878,1 @@\n-void ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n+size_t ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n@@ -1169,0 +1898,1 @@\n+  return required_regions;\n@@ -1178,0 +1908,4 @@\n+\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+    assert(plab->words_remaining() == 0, \"PLAB should not need retirement\");\n@@ -1193,0 +1927,11 @@\n+\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+\n+    \/\/ There are two reasons to retire all plabs between old-gen evacuation passes.\n+    \/\/  1. We need to make the plab memory parsable by remembered-set scanning.\n+    \/\/  2. We need to establish a trustworthy UpdateWaterMark value within each old-gen heap region\n+    ShenandoahHeap::heap()->retire_plab(plab, thread);\n+    if (_resize && ShenandoahThreadLocalData::plab_size(thread) > 0) {\n+      ShenandoahThreadLocalData::set_plab_size(thread, 0);\n+    }\n@@ -1252,3 +1997,6 @@\n-  \/\/ Return the max allowed size, and let the allocation path\n-  \/\/ figure out the safe size for current allocation.\n-  return ShenandoahHeapRegion::max_tlab_size_bytes();\n+  if (mode()->is_generational()) {\n+    return MIN2(ShenandoahHeapRegion::max_tlab_size_bytes(), young_generation()->available());\n+  } else {\n+    \/\/ Return the max allowed size, and let the allocation path figure out the safe size for current allocation.\n+    return ShenandoahHeapRegion::max_tlab_size_bytes();\n+  }\n@@ -1294,1 +2042,8 @@\n-  tcl->do_thread(_control_thread);\n+  if (_shenandoah_policy->is_at_shutdown()) {\n+    return;\n+  }\n+\n+  if (_control_thread != nullptr) {\n+    tcl->do_thread(_control_thread);\n+  }\n+\n@@ -1314,0 +2069,4 @@\n+    ls.cr();\n+\n+    evac_tracker()->print_global_on(&ls);\n+\n@@ -1319,0 +2078,19 @@\n+void ShenandoahHeap::on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation) {\n+  shenandoah_policy()->record_collection_cause(cause);\n+\n+  set_gc_cause(cause);\n+  set_gc_generation(generation);\n+\n+  generation->heuristics()->record_cycle_start();\n+}\n+\n+void ShenandoahHeap::on_cycle_end(ShenandoahGeneration* generation) {\n+  generation->heuristics()->record_cycle_end();\n+  if (mode()->is_generational() && generation->is_global()) {\n+    \/\/ If we just completed a GLOBAL GC, claim credit for completion of young-gen and old-gen GC as well\n+    young_generation()->heuristics()->record_cycle_end();\n+    old_generation()->heuristics()->record_cycle_end();\n+  }\n+  set_gc_cause(GCCause::_no_gc);\n+}\n+\n@@ -1640,23 +2418,0 @@\n-class ShenandoahInitMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahInitMarkUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-    if (r->is_active()) {\n-      \/\/ Check if region needs updating its TAMS. We have updated it already during concurrent\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n-      if (_ctx->top_at_mark_start(r) != r->top()) {\n-        _ctx->capture_top_at_mark_start(r);\n-      }\n-    } else {\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should already have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n@@ -1678,99 +2433,0 @@\n-class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n-      \/\/ anyway to capture any updates that happened since now.\n-      r->clear_live_data();\n-      _ctx->capture_top_at_mark_start(r);\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_gc() {\n-  reset_mark_bitmap();\n-\n-  ShenandoahResetUpdateRegionStateClosure cl;\n-  parallel_heap_region_iterate(&cl);\n-}\n-\n-class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalMarkUpdateRegionStateClosure() :\n-    _ctx(ShenandoahHeap::heap()->complete_marking_context()), _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n-      \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n-      HeapWord *tams = _ctx->top_at_mark_start(r);\n-      HeapWord *top = r->top();\n-      if (top > tams) {\n-        r->increase_live_data_alloc_words(pointer_delta(top, tams));\n-      }\n-\n-      \/\/ We are about to select the collection set, make sure it knows about\n-      \/\/ current pinning status. Also, this allows trashing more regions that\n-      \/\/ now have their pinning status dropped.\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n-      }\n-\n-      \/\/ Remember limit for updating refs. It's guaranteed that we get no\n-      \/\/ from-space-refs written from here on.\n-      r->set_update_watermark_at_safepoint(r->top());\n-    } else {\n-      assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_regions_and_collection_set(bool concurrent) {\n-  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states :\n-                                         ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n-    ShenandoahFinalMarkUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n-\n-    assert_pinned_region_status();\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset :\n-                                         ShenandoahPhaseTimings::degen_gc_choose_cset);\n-    ShenandoahHeapLocker locker(lock());\n-    _collection_set->clear();\n-    heuristics()->choose_collection_set(_collection_set);\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset :\n-                                         ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n-  }\n-}\n-\n@@ -1787,1 +2443,1 @@\n-  ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n+  active_generation()->ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n@@ -1823,4 +2479,56 @@\n-void ShenandoahHeap::set_concurrent_mark_in_progress(bool in_progress) {\n-  assert(!has_forwarded_objects(), \"Not expected before\/after mark phase\");\n-  set_gc_state(MARKING, in_progress);\n-  ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(in_progress, !in_progress);\n+void ShenandoahHeap::set_concurrent_young_mark_in_progress(bool in_progress) {\n+  uint mask;\n+  assert(!has_forwarded_objects(), \"Young marking is not concurrent with evacuation\");\n+  if (!in_progress && is_concurrent_old_mark_in_progress()) {\n+    assert(mode()->is_generational(), \"Only generational GC has old marking\");\n+    assert(_gc_state.is_set(MARKING), \"concurrent_old_marking_in_progress implies MARKING\");\n+    \/\/ If old-marking is in progress when we turn off YOUNG_MARKING, leave MARKING (and OLD_MARKING) on\n+    mask = YOUNG_MARKING;\n+  } else {\n+    mask = MARKING | YOUNG_MARKING;\n+  }\n+  set_gc_state(mask, in_progress);\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::set_concurrent_old_mark_in_progress(bool in_progress) {\n+#ifdef ASSERT\n+  \/\/ has_forwarded_objects() iff UPDATEREFS or EVACUATION\n+  bool has_forwarded = has_forwarded_objects();\n+  bool updating_or_evacuating = _gc_state.is_set(UPDATEREFS | EVACUATION);\n+  bool evacuating = _gc_state.is_set(EVACUATION);\n+  assert ((has_forwarded == updating_or_evacuating) || (evacuating && !has_forwarded && collection_set()->is_empty()),\n+          \"Updating or evacuating iff has forwarded objects, or if evacuation phase is promoting in place without forwarding\");\n+#endif\n+  if (!in_progress && is_concurrent_young_mark_in_progress()) {\n+    \/\/ If young-marking is in progress when we turn off OLD_MARKING, leave MARKING (and YOUNG_MARKING) on\n+    assert(_gc_state.is_set(MARKING), \"concurrent_young_marking_in_progress implies MARKING\");\n+    set_gc_state(OLD_MARKING, in_progress);\n+  } else {\n+    set_gc_state(MARKING | OLD_MARKING, in_progress);\n+  }\n+  manage_satb_barrier(in_progress);\n+}\n+\n+bool ShenandoahHeap::is_prepare_for_old_mark_in_progress() const {\n+  return old_generation()->state() == ShenandoahOldGeneration::FILLING;\n+}\n+\n+void ShenandoahHeap::set_aging_cycle(bool in_progress) {\n+  _is_aging_cycle.set_cond(in_progress);\n+}\n+\n+void ShenandoahHeap::manage_satb_barrier(bool active) {\n+  if (is_concurrent_mark_in_progress()) {\n+    \/\/ Ignore request to deactivate barrier while concurrent mark is in progress.\n+    \/\/ Do not attempt to re-activate the barrier if it is already active.\n+    if (active && !ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  } else {\n+    \/\/ No concurrent marking is in progress so honor request to deactivate,\n+    \/\/ but only if the barrier is already active.\n+    if (!active && ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  }\n@@ -1859,0 +2567,8 @@\n+void ShenandoahHeap::cancel_concurrent_mark() {\n+  _young_generation->cancel_marking();\n+  _old_generation->cancel_marking();\n+  _global_generation->cancel_marking();\n+\n+  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+}\n+\n@@ -1864,0 +2580,1 @@\n+    _cancel_requested_time = os::elapsedTime();\n@@ -1874,1 +2591,1 @@\n-  \/\/ Step 0. Notify policy to disable event recording.\n+  \/\/ Step 1. Notify policy to disable event recording and prevent visiting gc threads during shutdown\n@@ -1877,1 +2594,1 @@\n-  \/\/ Step 1. Notify control thread that we are in shutdown.\n+  \/\/ Step 2. Notify control thread that we are in shutdown.\n@@ -1882,1 +2599,1 @@\n-  \/\/ Step 2. Notify GC workers that we are cancelling GC.\n+  \/\/ Step 3. Notify GC workers that we are cancelling GC.\n@@ -1885,1 +2602,1 @@\n-  \/\/ Step 3. Wait until GC worker exits normally.\n+  \/\/ Step 4. Wait until GC worker exits normally.\n@@ -1987,5 +2704,6 @@\n-size_t ShenandoahHeap::bytes_allocated_since_gc_start() const {\n-  return Atomic::load(&_bytes_allocated_since_gc_start);\n-}\n-\n-  Atomic::store(&_bytes_allocated_since_gc_start, (size_t)0);\n+  if (mode()->is_generational()) {\n+    young_generation()->reset_bytes_allocated_since_gc_start();\n+    old_generation()->reset_bytes_allocated_since_gc_start();\n+  }\n+\n+  global_generation()->reset_bytes_allocated_since_gc_start();\n@@ -2056,2 +2774,4 @@\n-    assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n-           \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    if (active_generation()->contains(r)) {\n+      assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n+             \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    }\n@@ -2111,0 +2831,2 @@\n+  ShenandoahRegionChunkIterator* _work_chunks;\n+\n@@ -2112,1 +2834,2 @@\n-  ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n+  explicit ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions,\n+                                        ShenandoahRegionChunkIterator* work_chunks) :\n@@ -2115,1 +2838,4 @@\n-    _regions(regions) {\n+    _regions(regions),\n+    _work_chunks(work_chunks)\n+  {\n+    log_info(gc, remset)(\"Scan remembered set using bitmap: %s\", BOOL_TO_STR(_heap->is_old_bitmap_stable()));\n@@ -2122,1 +2848,1 @@\n-      do_work<ShenandoahConcUpdateRefsClosure>();\n+      do_work<ShenandoahConcUpdateRefsClosure>(worker_id);\n@@ -2125,1 +2851,1 @@\n-      do_work<ShenandoahSTWUpdateRefsClosure>();\n+      do_work<ShenandoahSTWUpdateRefsClosure>(worker_id);\n@@ -2131,1 +2857,1 @@\n-  void do_work() {\n+  void do_work(uint worker_id) {\n@@ -2133,0 +2859,10 @@\n+    if (CONCURRENT && (worker_id == 0)) {\n+      \/\/ We ask the first worker to replenish the Mutator free set by moving regions previously reserved to hold the\n+      \/\/ results of evacuation.  These reserves are no longer necessary because evacuation has completed.\n+      size_t cset_regions = _heap->collection_set()->count();\n+      \/\/ We cannot transfer any more regions than will be reclaimed when the existing collection set is recycled, because\n+      \/\/ we need the reclaimed collection set regions to replenish the collector reserves\n+      _heap->free_set()->move_collector_sets_to_mutator(cset_regions);\n+    }\n+    \/\/ If !CONCURRENT, there's no value in expanding Mutator free set\n+\n@@ -2134,1 +2870,4 @@\n-    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+    \/\/ We update references for global, old, and young collections.\n+    assert(_heap->active_generation()->is_mark_complete(), \"Expected complete marking\");\n+    ShenandoahMarkingContext* const ctx = _heap->marking_context();\n+    bool is_mixed = _heap->collection_set()->has_old_regions();\n@@ -2138,0 +2877,3 @@\n+\n+      log_debug(gc)(\"ShenandoahUpdateHeapRefsTask::do_work(%u) looking at region \" SIZE_FORMAT, worker_id, r->index());\n+      bool region_progress = false;\n@@ -2139,1 +2881,31 @@\n-        _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+        if (!_heap->mode()->is_generational() || r->is_young()) {\n+          _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+          region_progress = true;\n+        } else if (r->is_old()) {\n+          if (_heap->active_generation()->is_global()) {\n+            \/\/ Note that GLOBAL collection is not as effectively balanced as young and mixed cycles.  This is because\n+            \/\/ concurrent GC threads are parceled out entire heap regions of work at a time and there\n+            \/\/ is no \"catchup phase\" consisting of remembered set scanning, during which parcels of work are smaller\n+            \/\/ and more easily distributed more fairly across threads.\n+\n+            \/\/ TODO: Consider an improvement to load balance GLOBAL GC.\n+            _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+            region_progress = true;\n+          }\n+          \/\/ Otherwise, this is an old region in a young or mixed cycle.  Process it during a second phase, below.\n+          \/\/ Don't bother to report pacing progress in this case.\n+        } else {\n+          \/\/ Because updating of references runs concurrently, it is possible that a FREE inactive region transitions\n+          \/\/ to a non-free active region while this loop is executing.  Whenever this happens, the changing of a region's\n+          \/\/ active status may propagate at a different speed than the changing of the region's affiliation.\n+\n+          \/\/ When we reach this control point, it is because a race has allowed a region's is_active() status to be seen\n+          \/\/ by this thread before the region's affiliation() is seen by this thread.\n+\n+          \/\/ It's ok for this race to occur because the newly transformed region does not have any references to be\n+          \/\/ updated.\n+\n+          assert(r->get_update_watermark() == r->bottom(),\n+                 \"%s Region \" SIZE_FORMAT \" is_active but not recognized as YOUNG or OLD so must be newly transitioned from FREE\",\n+                 r->affiliation_name(), r->index());\n+        }\n@@ -2141,1 +2913,1 @@\n-      if (ShenandoahPacing) {\n+      if (region_progress && ShenandoahPacing) {\n@@ -2149,0 +2921,114 @@\n+\n+    if (_heap->mode()->is_generational() && !_heap->active_generation()->is_global()) {\n+      \/\/ Since this is generational and not GLOBAL, we have to process the remembered set.  There's no remembered\n+      \/\/ set processing if not in generational mode or if GLOBAL mode.\n+\n+      \/\/ After this thread has exhausted its traditional update-refs work, it continues with updating refs within remembered set.\n+      \/\/ The remembered set workload is better balanced between threads, so threads that are \"behind\" can catch up with other\n+      \/\/ threads during this phase, allowing all threads to work more effectively in parallel.\n+      struct ShenandoahRegionChunk assignment;\n+      RememberedScanner* scanner = _heap->card_scan();\n+\n+      while (!_heap->check_cancelled_gc_and_yield(CONCURRENT) && _work_chunks->next(&assignment)) {\n+        \/\/ Keep grabbing next work chunk to process until finished, or asked to yield\n+        ShenandoahHeapRegion* r = assignment._r;\n+        if (r->is_active() && !r->is_cset() && r->is_old()) {\n+          HeapWord* start_of_range = r->bottom() + assignment._chunk_offset;\n+          HeapWord* end_of_range = r->get_update_watermark();\n+          if (end_of_range > start_of_range + assignment._chunk_size) {\n+            end_of_range = start_of_range + assignment._chunk_size;\n+          }\n+\n+          \/\/ Old region in a young cycle or mixed cycle.\n+          if (is_mixed) {\n+            \/\/ TODO: For mixed evac, consider building an old-gen remembered set that allows restricted updating\n+            \/\/ within old-gen HeapRegions.  This remembered set can be constructed by old-gen concurrent marking\n+            \/\/ and augmented by card marking.  For example, old-gen concurrent marking can remember for each old-gen\n+            \/\/ card which other old-gen regions it refers to: none, one-other specifically, multiple-other non-specific.\n+            \/\/ Update-references when _mixed_evac processess each old-gen memory range that has a traditional DIRTY\n+            \/\/ card or if the \"old-gen remembered set\" indicates that this card holds pointers specifically to an\n+            \/\/ old-gen region in the most recent collection set, or if this card holds pointers to other non-specific\n+            \/\/ old-gen heap regions.\n+\n+            if (r->is_humongous()) {\n+              if (start_of_range < end_of_range) {\n+                \/\/ Need to examine both dirty and clean cards during mixed evac.\n+                r->oop_iterate_humongous_slice(&cl, false, start_of_range, assignment._chunk_size, true);\n+              }\n+            } else {\n+              \/\/ Since this is mixed evacuation, old regions that are candidates for collection have not been coalesced\n+              \/\/ and filled.  Use mark bits to find objects that need to be updated.\n+              \/\/\n+              \/\/ Future TODO: establish a second remembered set to identify which old-gen regions point to other old-gen\n+              \/\/ regions which are in the collection set for a particular mixed evacuation.\n+              if (start_of_range < end_of_range) {\n+                HeapWord* p = nullptr;\n+                size_t card_index = scanner->card_index_for_addr(start_of_range);\n+                \/\/ In case last object in my range spans boundary of my chunk, I may need to scan all the way to top()\n+                ShenandoahObjectToOopBoundedClosure<T> objs(&cl, start_of_range, r->top());\n+\n+                \/\/ Any object that begins in a previous range is part of a different scanning assignment.  Any object that\n+                \/\/ starts after end_of_range is also not my responsibility.  (Either allocated during evacuation, so does\n+                \/\/ not hold pointers to from-space, or is beyond the range of my assigned work chunk.)\n+\n+                \/\/ Find the first object that begins in my range, if there is one.\n+                p = start_of_range;\n+                oop obj = cast_to_oop(p);\n+                HeapWord* tams = ctx->top_at_mark_start(r);\n+                if (p >= tams) {\n+                  \/\/ We cannot use ctx->is_marked(obj) to test whether an object begins at this address.  Instead,\n+                  \/\/ we need to use the remembered set crossing map to advance p to the first object that starts\n+                  \/\/ within the enclosing card.\n+\n+                  while (true) {\n+                    HeapWord* first_object = scanner->first_object_in_card(card_index);\n+                    if (first_object != nullptr) {\n+                      p = first_object;\n+                      break;\n+                    } else if (scanner->addr_for_card_index(card_index + 1) < end_of_range) {\n+                      card_index++;\n+                    } else {\n+                      \/\/ Force the loop that follows to immediately terminate.\n+                      p = end_of_range;\n+                      break;\n+                    }\n+                  }\n+                  obj = cast_to_oop(p);\n+                  \/\/ Note: p may be >= end_of_range\n+                } else if (!ctx->is_marked(obj)) {\n+                  p = ctx->get_next_marked_addr(p, tams);\n+                  obj = cast_to_oop(p);\n+                  \/\/ If there are no more marked objects before tams, this returns tams.\n+                  \/\/ Note that tams is either >= end_of_range, or tams is the start of an object that is marked.\n+                }\n+                while (p < end_of_range) {\n+                  \/\/ p is known to point to the beginning of marked object obj\n+                  objs.do_object(obj);\n+                  HeapWord* prev_p = p;\n+                  p += obj->size();\n+                  if (p < tams) {\n+                    p = ctx->get_next_marked_addr(p, tams);\n+                    \/\/ If there are no more marked objects before tams, this returns tams.  Note that tams is\n+                    \/\/ either >= end_of_range, or tams is the start of an object that is marked.\n+                  }\n+                  assert(p != prev_p, \"Lack of forward progress\");\n+                  obj = cast_to_oop(p);\n+                }\n+              }\n+            }\n+          } else {\n+            \/\/ This is a young evac..\n+            if (start_of_range < end_of_range) {\n+              size_t cluster_size =\n+                CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+              size_t clusters = assignment._chunk_size \/ cluster_size;\n+              assert(clusters * cluster_size == assignment._chunk_size, \"Chunk assignment must align on cluster boundaries\");\n+              scanner->process_region_slice(r, assignment._chunk_offset, clusters, end_of_range, &cl, true, worker_id);\n+            }\n+          }\n+          if (ShenandoahPacing && (start_of_range < end_of_range)) {\n+            _heap->pacer()->report_updaterefs(pointer_delta(end_of_range, start_of_range));\n+          }\n+        }\n+      }\n+    }\n@@ -2154,0 +3040,2 @@\n+  uint nworkers = workers()->active_workers();\n+  ShenandoahRegionChunkIterator work_list(nworkers);\n@@ -2156,1 +3044,1 @@\n-    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator, &work_list);\n@@ -2159,1 +3047,1 @@\n-    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator, &work_list);\n@@ -2162,0 +3050,3 @@\n+  if (ShenandoahEnableCardStats && card_scan()!=nullptr) { \/\/ generational check proxy\n+    card_scan()->log_card_stats(nworkers, CARD_STAT_UPDATE_REFS);\n+  }\n@@ -2164,1 +3055,0 @@\n-\n@@ -2167,0 +3057,1 @@\n+  ShenandoahMarkingContext* _ctx;\n@@ -2168,0 +3059,1 @@\n+  bool _is_generational;\n@@ -2170,1 +3062,3 @@\n-  ShenandoahFinalUpdateRefsUpdateRegionStateClosure() : _lock(ShenandoahHeap::heap()->lock()) {}\n+  ShenandoahFinalUpdateRefsUpdateRegionStateClosure(\n+    ShenandoahMarkingContext* ctx) : _ctx(ctx), _lock(ShenandoahHeap::heap()->lock()),\n+                                     _is_generational(ShenandoahHeap::heap()->mode()->is_generational()) { }\n@@ -2173,0 +3067,21 @@\n+\n+    \/\/ Maintenance of region age must follow evacuation in order to account for evacuation allocations within survivor\n+    \/\/ regions.  We consult region age during the subsequent evacuation to determine whether certain objects need to\n+    \/\/ be promoted.\n+    if (_is_generational && r->is_young() && r->is_active()) {\n+      HeapWord *tams = _ctx->top_at_mark_start(r);\n+      HeapWord *top = r->top();\n+\n+      \/\/ Allocations move the watermark when top moves.  However compacting\n+      \/\/ objects will sometimes lower top beneath the watermark, after which,\n+      \/\/ attempts to read the watermark will assert out (watermark should not be\n+      \/\/ higher than top).\n+      if (top > tams) {\n+        \/\/ There have been allocations in this region since the start of the cycle.\n+        \/\/ Any objects new to this region must not assimilate elevated age.\n+        r->reset_age();\n+      } else if (ShenandoahHeap::heap()->is_aging_cycle()) {\n+        r->increment_age();\n+      }\n+    }\n+\n@@ -2175,1 +3090,0 @@\n-\n@@ -2202,1 +3116,1 @@\n-    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl;\n+    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl (active_generation()->complete_marking_context());\n@@ -2217,6 +3131,84 @@\n-  {\n-    ShenandoahGCPhase phase(concurrent ?\n-                            ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n-                            ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n+  ShenandoahGCPhase phase(concurrent ?\n+                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n+                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  ShenandoahHeapLocker locker(lock());\n+  size_t young_cset_regions, old_cset_regions;\n+  size_t first_old_region, last_old_region, old_region_count;\n+  _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+  \/\/ If there are no old regions, first_old_region will be greater than last_old_region\n+  assert((first_old_region > last_old_region) ||\n+         ((last_old_region + 1 - first_old_region >= old_region_count) &&\n+          get_region(first_old_region)->is_old() && get_region(last_old_region)->is_old()),\n+         \"sanity: old_region_count: \" SIZE_FORMAT \", first_old_region: \" SIZE_FORMAT \", last_old_region: \" SIZE_FORMAT,\n+         old_region_count, first_old_region, last_old_region);\n+\n+  if (mode()->is_generational()) {\n+    assert(verify_generation_usage(true, old_generation()->used_regions(),\n+                                   old_generation()->used(), old_generation()->get_humongous_waste(),\n+                                   true, young_generation()->used_regions(),\n+                                   young_generation()->used(), young_generation()->get_humongous_waste()),\n+           \"Generation accounts are inaccurate\");\n+\n+    \/\/ The computation of bytes_of_allocation_runway_before_gc_trigger is quite conservative so consider all of this\n+    \/\/ available for transfer to old. Note that transfer of humongous regions does not impact available.\n+    size_t allocation_runway = young_heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions);\n+    ShenandoahGenerationalHeap::heap()->compute_old_generation_balance(allocation_runway, old_cset_regions);\n+\n+    \/\/ Total old_available may have been expanded to hold anticipated promotions.  We trigger if the fragmented available\n+    \/\/ memory represents more than 16 regions worth of data.  Note that fragmentation may increase when we promote regular\n+    \/\/ regions in place when many of these regular regions have an abundant amount of available memory within them.  Fragmentation\n+    \/\/ will decrease as promote-by-copy consumes the available memory within these partially consumed regions.\n+    \/\/\n+    \/\/ We consider old-gen to have excessive fragmentation if more than 12.5% of old-gen is free memory that resides\n+    \/\/ within partially consumed regions of memory.\n+  }\n+  \/\/ Rebuild free set based on adjusted generation sizes.\n+  _free_set->rebuild(young_cset_regions, old_cset_regions);\n+\n+  if (mode()->is_generational() && (ShenandoahGenerationalHumongousReserve > 0)) {\n+    size_t old_region_span = (first_old_region <= last_old_region)? (last_old_region + 1 - first_old_region): 0;\n+    size_t allowed_old_gen_span = num_regions() - (ShenandoahGenerationalHumongousReserve * num_regions() \/ 100);\n+\n+    \/\/ Tolerate lower density if total span is small.  Here's the implementation:\n+    \/\/   if old_gen spans more than 100% and density < 75%, trigger old-defrag\n+    \/\/   else if old_gen spans more than 87.5% and density < 62.5%, trigger old-defrag\n+    \/\/   else if old_gen spans more than 75% and density < 50%, trigger old-defrag\n+    \/\/   else if old_gen spans more than 62.5% and density < 37.5%, trigger old-defrag\n+    \/\/   else if old_gen spans more than 50% and density < 25%, trigger old-defrag\n+    \/\/\n+    \/\/ A previous implementation was more aggressive in triggering, resulting in degraded throughput when\n+    \/\/ humongous allocation was not required.\n+\n+    ShenandoahGeneration* old_gen = old_generation();\n+    size_t old_available = old_gen->available();\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    size_t old_unaffiliated_available = old_gen->free_unaffiliated_regions() * region_size_bytes;\n+    assert(old_available >= old_unaffiliated_available, \"sanity\");\n+    size_t old_fragmented_available = old_available - old_unaffiliated_available;\n+\n+    size_t old_bytes_consumed = old_region_count * region_size_bytes - old_fragmented_available;\n+    size_t old_bytes_spanned = old_region_span * region_size_bytes;\n+    double old_density = ((double) old_bytes_consumed) \/ old_bytes_spanned;\n+\n+    uint eighths = 8;\n+    for (uint i = 0; i < 5; i++) {\n+      size_t span_threshold = eighths * allowed_old_gen_span \/ 8;\n+      double density_threshold = (eighths - 2) \/ 8.0;\n+      if ((old_region_span >= span_threshold) && (old_density < density_threshold)) {\n+        old_heuristics()->trigger_old_is_fragmented(old_density, first_old_region, last_old_region);\n+        break;\n+      }\n+      eighths--;\n+    }\n+\n+    size_t old_used = old_generation()->used() + old_generation()->get_humongous_waste();\n+    size_t trigger_threshold = old_generation()->usage_trigger_threshold();\n+    \/\/ Detects unsigned arithmetic underflow\n+    assert(old_used <= capacity(),\n+           \"Old used (\" SIZE_FORMAT \", \" SIZE_FORMAT\") must not be more than heap capacity (\" SIZE_FORMAT \")\",\n+           old_generation()->used(), old_generation()->get_humongous_waste(), capacity());\n+\n+    if (old_used > trigger_threshold) {\n+      old_heuristics()->trigger_old_has_grown();\n+    }\n@@ -2345,1 +3337,1 @@\n-  return _memory_pool->get_memory_usage();\n+  return MemoryUsage(_initial_size, used(), committed(), max_capacity());\n@@ -2383,0 +3375,1 @@\n+\n@@ -2410,0 +3403,77 @@\n+\n+void ShenandoahHeap::transfer_old_pointers_from_satb() {\n+  _old_generation->transfer_pointers_from_satb();\n+}\n+\n+bool ShenandoahHeap::verify_generation_usage(bool verify_old, size_t old_regions, size_t old_bytes, size_t old_waste,\n+                                             bool verify_young, size_t young_regions, size_t young_bytes, size_t young_waste) {\n+  size_t tally_old_regions = 0;\n+  size_t tally_old_bytes = 0;\n+  size_t tally_old_waste = 0;\n+  size_t tally_young_regions = 0;\n+  size_t tally_young_bytes = 0;\n+  size_t tally_young_waste = 0;\n+\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  for (size_t i = 0; i < num_regions(); i++) {\n+    ShenandoahHeapRegion* r = get_region(i);\n+    if (r->is_old()) {\n+      tally_old_regions++;\n+      tally_old_bytes += r->used();\n+      if (r->is_humongous()) {\n+        ShenandoahHeapRegion* start = r->humongous_start_region();\n+        HeapWord* obj_addr = start->bottom();\n+        oop obj = cast_to_oop(obj_addr);\n+        size_t word_size = obj->size();\n+        HeapWord* end_addr = obj_addr + word_size;\n+        if (end_addr <= r->end()) {\n+          tally_old_waste += (r->end() - end_addr) * HeapWordSize;\n+        }\n+      }\n+    } else if (r->is_young()) {\n+      tally_young_regions++;\n+      tally_young_bytes += r->used();\n+      if (r->is_humongous()) {\n+        ShenandoahHeapRegion* start = r->humongous_start_region();\n+        HeapWord* obj_addr = start->bottom();\n+        oop obj = cast_to_oop(obj_addr);\n+        size_t word_size = obj->size();\n+        HeapWord* end_addr = obj_addr + word_size;\n+        if (end_addr <= r->end()) {\n+          tally_young_waste += (r->end() - end_addr) * HeapWordSize;\n+        }\n+      }\n+    }\n+  }\n+  if (verify_young &&\n+      ((young_regions != tally_young_regions) || (young_bytes != tally_young_bytes) || (young_waste != tally_young_waste))) {\n+    return false;\n+  } else if (verify_old &&\n+             ((old_regions != tally_old_regions) || (old_bytes != tally_old_bytes) || (old_waste != tally_old_waste))) {\n+    return false;\n+  } else {\n+    return true;\n+  }\n+}\n+\n+ShenandoahGeneration* ShenandoahHeap::generation_for(ShenandoahAffiliation affiliation) const {\n+  if (!mode()->is_generational()) {\n+    return global_generation();\n+  } else if (affiliation == YOUNG_GENERATION) {\n+    return young_generation();\n+  } else if (affiliation == OLD_GENERATION) {\n+    return old_generation();\n+  }\n+\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n+\n+void ShenandoahHeap::log_heap_status(const char* msg) const {\n+  if (mode()->is_generational()) {\n+    young_generation()->log_status(msg);\n+    old_generation()->log_status(msg);\n+  } else {\n+    global_generation()->log_status(msg);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":1365,"deletions":295,"binary":false,"changes":1660,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -29,0 +30,1 @@\n+#include \"gc\/shared\/ageTable.hpp\"\n@@ -32,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahAgeCensus.hpp\"\n@@ -35,0 +38,2 @@\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n+#include \"gc\/shenandoah\/shenandoahController.hpp\"\n@@ -37,0 +42,3 @@\n+#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMmuTracker.hpp\"\n@@ -38,0 +46,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.hpp\"\n@@ -47,0 +56,1 @@\n+class PLAB;\n@@ -48,1 +58,1 @@\n-class ShenandoahControlThread;\n+class ShenandoahRegulatorThread;\n@@ -51,0 +61,3 @@\n+class ShenandoahGeneration;\n+class ShenandoahYoungGeneration;\n+class ShenandoahOldGeneration;\n@@ -52,0 +65,2 @@\n+class ShenandoahOldHeuristics;\n+class ShenandoahYoungHeuristics;\n@@ -53,1 +68,0 @@\n-class ShenandoahMode;\n@@ -63,0 +77,1 @@\n+class ShenandoahMode;\n@@ -120,1 +135,1 @@\n-class ShenandoahHeap : public CollectedHeap, public ShenandoahSpaceInfo {\n+class ShenandoahHeap : public CollectedHeap {\n@@ -130,0 +145,1 @@\n+  friend class ShenandoahOldGC;\n@@ -138,0 +154,1 @@\n+  ShenandoahGeneration* _gc_generation;\n@@ -144,0 +161,17 @@\n+  ShenandoahGeneration* active_generation() const {\n+    \/\/ last or latest generation might be a better name here.\n+    return _gc_generation;\n+  }\n+\n+  void set_gc_generation(ShenandoahGeneration* generation) {\n+    _gc_generation = generation;\n+  }\n+\n+  ShenandoahHeuristics* heuristics();\n+  ShenandoahOldHeuristics* old_heuristics();\n+  ShenandoahYoungHeuristics* young_heuristics();\n+\n+  bool doing_mixed_evacuations();\n+  bool is_old_bitmap_stable() const;\n+  bool is_gc_generation_young() const;\n+\n@@ -155,3 +189,2 @@\n-  void initialize_mode();\n-  void initialize_heuristics();\n-\n+  void initialize_heuristics_generations();\n+  virtual void print_init_logger() const;\n@@ -170,0 +203,3 @@\n+  bool verify_generation_usage(bool verify_old, size_t old_regions, size_t old_bytes, size_t old_waste,\n+                               bool verify_young, size_t young_regions, size_t young_bytes, size_t young_waste);\n+\n@@ -178,2 +214,3 @@\n-           size_t _initial_size;\n-           size_t _minimum_size;\n+  size_t _initial_size;\n+  size_t _minimum_size;\n+\n@@ -182,2 +219,0 @@\n-  volatile size_t _used;\n-  volatile size_t _bytes_allocated_since_gc_start;\n@@ -187,0 +222,2 @@\n+  void increase_used(const ShenandoahAllocRequest& req);\n+\n@@ -188,3 +225,4 @@\n-  void increase_used(size_t bytes);\n-  void decrease_used(size_t bytes);\n-  void set_used(size_t bytes);\n+  void increase_used(ShenandoahGeneration* generation, size_t bytes);\n+  void decrease_used(ShenandoahGeneration* generation, size_t bytes);\n+  void increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes);\n+  void decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes);\n@@ -194,2 +232,0 @@\n-  void increase_allocated(size_t bytes);\n-  size_t bytes_allocated_since_gc_start() const override;\n@@ -201,1 +237,1 @@\n-  size_t soft_max_capacity() const override;\n+  size_t soft_max_capacity() const;\n@@ -206,1 +242,0 @@\n-  size_t available()         const override;\n@@ -226,0 +261,2 @@\n+  virtual void initialize_controller();\n+\n@@ -242,0 +279,1 @@\n+  uint8_t* _affiliations;       \/\/ Holds array of enum ShenandoahAffiliation, including FREE status in non-generational mode\n@@ -259,0 +297,2 @@\n+  inline ShenandoahMmuTracker* mmu_tracker() { return &_mmu_tracker; };\n+\n@@ -274,0 +314,1 @@\n+    \/\/ For generational mode, it means either young or old marking, or both.\n@@ -284,0 +325,6 @@\n+\n+    \/\/ Young regions are under marking, need SATB barriers.\n+    YOUNG_MARKING_BITPOS = 5,\n+\n+    \/\/ Old regions are under marking, need SATB barriers.\n+    OLD_MARKING_BITPOS = 6\n@@ -293,0 +340,2 @@\n+    YOUNG_MARKING = 1 << YOUNG_MARKING_BITPOS,\n+    OLD_MARKING   = 1 << OLD_MARKING_BITPOS\n@@ -298,2 +347,0 @@\n-\n-  \/\/ tracks if new regions have been allocated or retired since last check\n@@ -311,0 +358,2 @@\n+  ShenandoahAgeCensus* _age_census;    \/\/ Age census used for adapting tenuring threshold in generational mode\n+\n@@ -328,1 +377,2 @@\n-  void set_concurrent_mark_in_progress(bool in_progress);\n+  void set_concurrent_young_mark_in_progress(bool in_progress);\n+  void set_concurrent_old_mark_in_progress(bool in_progress);\n@@ -338,0 +388,2 @@\n+  void set_aging_cycle(bool cond);\n+\n@@ -340,0 +392,1 @@\n+\n@@ -341,0 +394,2 @@\n+  inline bool is_concurrent_young_mark_in_progress() const;\n+  inline bool is_concurrent_old_mark_in_progress() const;\n@@ -351,0 +406,5 @@\n+  bool is_prepare_for_old_mark_in_progress() const;\n+  inline bool is_aging_cycle() const;\n+\n+  \/\/ Return the age census object for young gen (in generational mode)\n+  inline ShenandoahAgeCensus* age_census() const;\n@@ -353,0 +413,2 @@\n+  void manage_satb_barrier(bool active);\n+\n@@ -363,0 +425,1 @@\n+  double _cancel_requested_time;\n@@ -364,0 +427,5 @@\n+\n+  \/\/ Returns true if cancel request was successfully communicated.\n+  \/\/ Returns false if some other thread already communicated cancel\n+  \/\/ request.  A true return value does not mean GC has been\n+  \/\/ cancelled, only that the process of cancelling GC has begun.\n@@ -367,1 +435,0 @@\n-\n@@ -371,1 +438,1 @@\n-  inline void clear_cancelled_gc();\n+  inline void clear_cancelled_gc(bool clear_oom_handler = true);\n@@ -373,0 +440,1 @@\n+  void cancel_concurrent_mark();\n@@ -386,3 +454,0 @@\n-  \/\/ Reset bitmap, prepare regions for new GC cycle\n-  void prepare_gc();\n-  void prepare_regions_and_collection_set(bool concurrent);\n@@ -401,1 +466,0 @@\n-  void rebuild_free_set(bool concurrent);\n@@ -406,0 +470,1 @@\n+  void rebuild_free_set(bool concurrent);\n@@ -413,1 +478,8 @@\n-  ShenandoahControlThread*   _control_thread;\n+  ShenandoahYoungGeneration* _young_generation;\n+  ShenandoahGeneration*      _global_generation;\n+  ShenandoahOldGeneration*   _old_generation;\n+\n+protected:\n+  ShenandoahController*  _control_thread;\n+\n+private:\n@@ -416,1 +488,0 @@\n-  ShenandoahHeuristics*      _heuristics;\n@@ -421,3 +492,4 @@\n-  ShenandoahPhaseTimings*    _phase_timings;\n-\n-  ShenandoahControlThread*   control_thread()          { return _control_thread;    }\n+  ShenandoahPhaseTimings*       _phase_timings;\n+  ShenandoahEvacuationTracker*  _evac_tracker;\n+  ShenandoahMmuTracker          _mmu_tracker;\n+  ShenandoahGenerationSizer     _generation_sizer;\n@@ -426,0 +498,8 @@\n+  ShenandoahController*   control_thread() { return _control_thread; }\n+\n+  ShenandoahYoungGeneration* young_generation()  const { return _young_generation;  }\n+  ShenandoahGeneration*      global_generation() const { return _global_generation; }\n+  ShenandoahOldGeneration*   old_generation()    const { return _old_generation;    }\n+  ShenandoahGeneration*      generation_for(ShenandoahAffiliation affiliation) const;\n+  const ShenandoahGenerationSizer* generation_sizer()  const { return &_generation_sizer;  }\n+\n@@ -428,1 +508,0 @@\n-  ShenandoahHeuristics*      heuristics()        const { return _heuristics;        }\n@@ -432,1 +511,5 @@\n-  ShenandoahPhaseTimings*    phase_timings()     const { return _phase_timings;     }\n+  ShenandoahPhaseTimings*      phase_timings()   const { return _phase_timings;     }\n+  ShenandoahEvacuationTracker* evac_tracker()    const { return _evac_tracker;      }\n+\n+  void on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation);\n+  void on_cycle_end(ShenandoahGeneration* generation);\n@@ -447,1 +530,1 @@\n-  ShenandoahMonitoringSupport* monitoring_support()          { return _monitoring_support;    }\n+  ShenandoahMonitoringSupport* monitoring_support() const    { return _monitoring_support;    }\n@@ -457,8 +540,0 @@\n-\/\/ ---------- Reference processing\n-\/\/\n-private:\n-  ShenandoahReferenceProcessor* const _ref_processor;\n-\n-public:\n-  ShenandoahReferenceProcessor* ref_processor() { return _ref_processor; }\n-\n@@ -468,0 +543,1 @@\n+  ShenandoahSharedFlag  _is_aging_cycle;\n@@ -483,0 +559,3 @@\n+  inline void assert_lock_for_affiliation(ShenandoahAffiliation orig_affiliation,\n+                                          ShenandoahAffiliation new_affiliation);\n+\n@@ -496,1 +575,11 @@\n-  bool is_in(const void* p) const override;\n+  inline bool is_in(const void* p) const override;\n+\n+  inline bool is_in_active_generation(oop obj) const;\n+  inline bool is_in_young(const void* p) const;\n+  inline bool is_in_old(const void* p) const;\n+  inline bool is_old(oop pobj) const;\n+\n+  inline ShenandoahAffiliation region_affiliation(const ShenandoahHeapRegion* r);\n+  inline void set_affiliation(ShenandoahHeapRegion* r, ShenandoahAffiliation new_affiliation);\n+\n+  inline ShenandoahAffiliation region_affiliation(size_t index);\n@@ -545,1 +634,2 @@\n-  HeapWord* allocate_memory_under_lock(ShenandoahAllocRequest& request, bool& in_new_region);\n+  HeapWord* allocate_memory_under_lock(ShenandoahAllocRequest& request, bool& in_new_region, bool is_promotion);\n+\n@@ -550,0 +640,4 @@\n+  inline HeapWord* allocate_from_plab(Thread* thread, size_t size, bool is_promotion);\n+  HeapWord* allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion);\n+  HeapWord* allocate_new_plab(size_t min_size, size_t word_size, size_t* actual_size);\n+\n@@ -551,1 +645,1 @@\n-  HeapWord* allocate_memory(ShenandoahAllocRequest& request);\n+  HeapWord* allocate_memory(ShenandoahAllocRequest& request, bool is_promotion);\n@@ -557,1 +651,1 @@\n-  void notify_mutator_alloc_words(size_t words, bool waste);\n+  void notify_mutator_alloc_words(size_t words, size_t waste);\n@@ -595,2 +689,0 @@\n-  inline void mark_complete_marking_context();\n-  inline void mark_incomplete_marking_context();\n@@ -607,2 +699,0 @@\n-  void reset_mark_bitmap();\n-\n@@ -629,0 +719,1 @@\n+  oop try_evacuate_object(oop src, Thread* thread, ShenandoahHeapRegion* from_region, ShenandoahAffiliation target_gen);\n@@ -630,0 +721,1 @@\n+\n@@ -640,1 +732,1 @@\n-  \/\/ Evacuates object src. Returns the evacuated object, either evacuated\n+  \/\/ Evacuates or promotes object src. Returns the evacuated object, either evacuated\n@@ -648,0 +740,13 @@\n+\/\/ ---------- Generational support\n+\/\/\n+private:\n+  RememberedScanner* _card_scan;\n+\n+public:\n+  inline RememberedScanner* card_scan() { return _card_scan; }\n+  void clear_cards_for(ShenandoahHeapRegion* region);\n+  void mark_card_as_dirty(void* location);\n+  void retire_plab(PLAB* plab);\n+  void retire_plab(PLAB* plab, Thread* thread);\n+  void cancel_old_gc();\n+\n@@ -669,1 +774,12 @@\n-  void trash_humongous_region_at(ShenandoahHeapRegion *r);\n+  size_t trash_humongous_region_at(ShenandoahHeapRegion *r);\n+\n+  static inline void increase_object_age(oop obj, uint additional_age);\n+\n+  \/\/ Return the object's age, or a sentinel value when the age can't\n+  \/\/ necessarily be determined because of concurrent locking by the\n+  \/\/ mutator\n+  static inline uint get_object_age(oop obj);\n+\n+  void transfer_old_pointers_from_satb();\n+\n+  void log_heap_status(const char *msg) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":168,"deletions":52,"binary":false,"changes":220,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -36,0 +37,80 @@\n+  product(uintx, ShenandoahGenerationalHumongousReserve, 0, EXPERIMENTAL,   \\\n+          \"(Generational mode only) What percent of the heap should be \"    \\\n+          \"reserved for humongous objects if possible.  Old-generation \"    \\\n+          \"collections will endeavor to evacuate old-gen regions within \"   \\\n+          \"this reserved area even if these regions do not contain high \"   \\\n+          \"percentage of garbage.  Setting a larger value will cause \"      \\\n+          \"more frequent old-gen collections.  A smaller value will \"       \\\n+          \"increase the likelihood that humongous object allocations \"      \\\n+          \"fail, resulting in stop-the-world full GCs.\")                    \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(double, ShenandoahMinOldGenGrowthPercent, 12.5, EXPERIMENTAL,     \\\n+          \"(Generational mode only) If the usage within old generation \"    \\\n+          \"has grown by at least this percent of its live memory size \"     \\\n+          \"at completion of the most recent old-generation marking \"        \\\n+          \"effort, heuristics may trigger the start of a new old-gen \"      \\\n+          \"collection.\")                                                    \\\n+          range(0.0,100.0)                                                  \\\n+                                                                            \\\n+  product(uintx, ShenandoahIgnoreOldGrowthBelowPercentage,10, EXPERIMENTAL, \\\n+          \"(Generational mode only) If the total usage of the old \"         \\\n+          \"generation is smaller than this percent, we do not trigger \"     \\\n+          \"old gen collections even if old has grown, except when \"         \\\n+          \"ShenandoahGenerationalDoNotIgnoreGrowthAfterYoungCycles \"        \\\n+          \"consecutive cycles have been completed following the \"           \\\n+          \"preceding old-gen collection.\")                                  \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahDoNotIgnoreGrowthAfterYoungCycles,               \\\n+          50, EXPERIMENTAL,                                                 \\\n+          \"(Generational mode only) Even if the usage of old generation \"   \\\n+          \"is below ShenandoahIgnoreOldGrowthBelowPercentage, \"             \\\n+          \"trigger an old-generation mark if old has grown and this \"       \\\n+          \"many consecutive young-gen collections have been \"               \\\n+          \"completed following the preceding old-gen collection.\")          \\\n+                                                                            \\\n+  product(bool, ShenandoahGenerationalCensusAtEvac, false, EXPERIMENTAL,    \\\n+          \"(Generational mode only) Object age census at evacuation, \"      \\\n+          \"rather than during marking.\")                                    \\\n+                                                                            \\\n+  product(bool, ShenandoahGenerationalAdaptiveTenuring, true, EXPERIMENTAL, \\\n+          \"(Generational mode only) Dynamically adapt tenuring age.\")       \\\n+                                                                            \\\n+  product(bool, ShenandoahGenerationalCensusIgnoreOlderCohorts, true,       \\\n+                                                               EXPERIMENTAL,\\\n+          \"(Generational mode only) Ignore mortality rates older than the \" \\\n+          \"oldest cohort under the tenuring age for the last cycle.\" )      \\\n+                                                                            \\\n+  product(uintx, ShenandoahGenerationalMinTenuringAge, 1, EXPERIMENTAL,     \\\n+          \"(Generational mode only) Floor for adaptive tenuring age. \"      \\\n+          \"Setting floor and ceiling to the same value fixes the tenuring \" \\\n+          \"age; setting both to 1 simulates a poor approximation to \"       \\\n+          \"AlwaysTenure, and setting both to 16 simulates NeverTenure.\")    \\\n+          range(1,16)                                                       \\\n+                                                                            \\\n+  product(uintx, ShenandoahGenerationalMaxTenuringAge, 15, EXPERIMENTAL,    \\\n+          \"(Generational mode only) Ceiling for adaptive tenuring age. \"    \\\n+          \"Setting floor and ceiling to the same value fixes the tenuring \" \\\n+          \"age; setting both to 1 simulates a poor approximation to \"       \\\n+          \"AlwaysTenure, and setting both to 16 simulates NeverTenure.\")    \\\n+          range(1,16)                                                       \\\n+                                                                            \\\n+  product(double, ShenandoahGenerationalTenuringMortalityRateThreshold,     \\\n+                                                         0.1, EXPERIMENTAL, \\\n+          \"(Generational mode only) Cohort mortality rates below this \"     \\\n+          \"value will be treated as indicative of longevity, leading to \"   \\\n+          \"tenuring. A lower value delays tenuring, a higher value hastens \"\\\n+          \"it. Used only when ShenandoahGenerationalhenAdaptiveTenuring is \"\\\n+          \"enabled.\")                                                       \\\n+          range(0.001,0.999)                                                \\\n+                                                                            \\\n+  product(size_t, ShenandoahGenerationalTenuringCohortPopulationThreshold,  \\\n+                                                         4*K, EXPERIMENTAL, \\\n+          \"(Generational mode only) Cohorts whose population is lower than \"\\\n+          \"this value in the previous census are ignored wrt tenuring \"     \\\n+          \"decisions. Effectively this makes then tenurable as soon as all \"\\\n+          \"older cohorts are. Set this value to the largest cohort \"        \\\n+          \"population volume that you are comfortable ignoring when making \"\\\n+          \"tenuring decisions.\")                                            \\\n+                                                                            \\\n@@ -64,1 +145,2 @@\n-          \" passive - stop the world GC only (either degenerated or full)\") \\\n+          \" passive - stop the world GC only (either degenerated or full);\" \\\n+          \" generational - generational concurrent GC\")                     \\\n@@ -78,0 +160,10 @@\n+  product(uintx, ShenandoahExpeditePromotionsThreshold, 5, EXPERIMENTAL,    \\\n+          \"When Shenandoah expects to promote at least this percentage \"    \\\n+          \"of the young generation, trigger a young collection to \"         \\\n+          \"expedite these promotions.\")                                     \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahExpediteMixedThreshold, 10, EXPERIMENTAL,        \\\n+          \"When there are this many old regions waiting to be collected, \"  \\\n+          \"trigger a mixed collection immediately.\")                        \\\n+                                                                            \\\n@@ -86,0 +178,14 @@\n+  product(uintx, ShenandoahOldGarbageThreshold, 15, EXPERIMENTAL,           \\\n+          \"How much garbage an old region has to contain before it would \"  \\\n+          \"be taken for collection.\")                                       \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahIgnoreGarbageThreshold, 5, EXPERIMENTAL,         \\\n+          \"When less than this amount of garbage (as a percentage of \"      \\\n+          \"region size) exists within a region, the region will not be \"    \\\n+          \"added to the collection set, even when the heuristic has \"       \\\n+          \"chosen to aggressively add regions with less than \"              \\\n+          \"ShenandoahGarbageThreshold amount of garbage into the \"          \\\n+          \"collection set.\")                                                \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n@@ -87,4 +193,6 @@\n-          \"How much heap should be free before some heuristics trigger the \"\\\n-          \"initial (learning) cycles. Affects cycle frequency on startup \"  \\\n-          \"and after drastic state changes, e.g. after degenerated\/full \"   \\\n-          \"GC cycles. In percents of (soft) max heap size.\")                \\\n+          \"When less than this amount of memory is free within the\"         \\\n+          \"heap or generation, trigger a learning cycle if we are \"         \\\n+          \"in learning mode.  Learning mode happens during initialization \" \\\n+          \"and following a drastic state change, such as following a \"      \\\n+          \"degenerated or Full GC cycle.  In percents of soft max \"         \\\n+          \"heap size.\")                                                     \\\n@@ -94,3 +202,5 @@\n-          \"How much heap should be free before most heuristics trigger the \"\\\n-          \"collection, even without other triggers. Provides the safety \"   \\\n-          \"margin for many heuristics. In percents of (soft) max heap size.\")\\\n+          \"Percentage of free heap memory (or young generation, in \"        \\\n+          \"generational mode) below which most heuristics trigger \"         \\\n+          \"collection independent of other triggers. Provides a safety \"    \\\n+          \"margin for many heuristics. In percents of (soft) max heap \"     \\\n+          \"size.\")                                                          \\\n@@ -112,1 +222,1 @@\n-  product(uintx, ShenandoahLearningSteps, 5, EXPERIMENTAL,                  \\\n+  product(uintx, ShenandoahLearningSteps, 10, EXPERIMENTAL,                 \\\n@@ -117,1 +227,1 @@\n-  product(uintx, ShenandoahImmediateThreshold, 90, EXPERIMENTAL,            \\\n+  product(uintx, ShenandoahImmediateThreshold, 70, EXPERIMENTAL,            \\\n@@ -146,1 +256,1 @@\n-  product(double, ShenandoahAdaptiveDecayFactor, 0.5, EXPERIMENTAL,         \\\n+  product(double, ShenandoahAdaptiveDecayFactor, 0.1, EXPERIMENTAL,         \\\n@@ -152,0 +262,10 @@\n+  product(bool, ShenandoahAdaptiveIgnoreShortCycles, true, EXPERIMENTAL,    \\\n+          \"The adaptive heuristic tracks a moving average of cycle \"        \\\n+          \"times in order to start a gc before memory is exhausted. \"       \\\n+          \"In some cases, Shenandoah may skip the evacuation and update \"   \\\n+          \"reference phases, resulting in a shorter cycle. These may skew \" \\\n+          \"the average cycle time downward and may cause the heuristic \"    \\\n+          \"to wait too long to start a cycle. Disabling this will have \"    \\\n+          \"the gc run less often, which will reduce CPU utilization, but\"   \\\n+          \"increase the risk of degenerated cycles.\")                       \\\n+                                                                            \\\n@@ -159,0 +279,10 @@\n+  product(uintx, ShenandoahGuaranteedOldGCInterval, 10*60*1000, EXPERIMENTAL, \\\n+          \"Run a collection of the old generation at least this often. \"    \\\n+          \"Heuristics may trigger collections more frequently. Time is in \" \\\n+          \"milliseconds. Setting this to 0 disables the feature.\")          \\\n+                                                                            \\\n+  product(uintx, ShenandoahGuaranteedYoungGCInterval, 5*60*1000,  EXPERIMENTAL,  \\\n+          \"Run a collection of the young generation at least this often. \"  \\\n+          \"Heuristics may trigger collections more frequently. Time is in \" \\\n+          \"milliseconds. Setting this to 0 disables the feature.\")          \\\n+                                                                            \\\n@@ -218,4 +348,12 @@\n-          \"How much of heap to reserve for evacuations. Larger values make \"\\\n-          \"GC evacuate more live objects on every cycle, while leaving \"    \\\n-          \"less headroom for application to allocate in. In percents of \"   \\\n-          \"total heap size.\")                                               \\\n+          \"How much of (young-generation) heap to reserve for \"             \\\n+          \"(young-generation) evacuations.  Larger values allow GC to \"     \\\n+          \"evacuate more live objects on every cycle, while leaving \"       \\\n+          \"less headroom for application to allocate while GC is \"          \\\n+          \"evacuating and updating references. This parameter is \"          \\\n+          \"consulted at the end of marking, before selecting the \"          \\\n+          \"collection set.  If available memory at this time is smaller \"   \\\n+          \"than the indicated reserve, the bound on collection set size is \"\\\n+          \"adjusted downward.  The size of a generational mixed \"           \\\n+          \"evacuation collection set (comprised of both young and old \"     \\\n+          \"regions) is also bounded by this parameter.  In percents of \"    \\\n+          \"total (young-generation) heap size.\")                            \\\n@@ -228,1 +366,10 @@\n-          \"GC cycle.\")                                                      \\\n+          \"GC cycle.  Smaller values increase the risk of evacuation \"      \\\n+          \"failures, which will trigger stop-the-world Full GC passes.\")    \\\n+          range(1.0,100.0)                                                  \\\n+                                                                            \\\n+  product(double, ShenandoahOldEvacWaste, 1.4, EXPERIMENTAL,                \\\n+          \"How much waste evacuations produce within the reserved space. \"  \\\n+          \"Larger values make evacuations more resilient against \"          \\\n+          \"evacuation conflicts, at expense of evacuating less on each \"    \\\n+          \"GC cycle.  Smaller values increase the risk of evacuation \"      \\\n+          \"failures, which will trigger stop-the-world Full GC passes.\")    \\\n@@ -231,0 +378,28 @@\n+  product(double, ShenandoahPromoEvacWaste, 1.2, EXPERIMENTAL,              \\\n+          \"How much waste promotions produce within the reserved space. \"   \\\n+          \"Larger values make evacuations more resilient against \"          \\\n+          \"evacuation conflicts, at expense of promoting less on each \"     \\\n+          \"GC cycle.  Smaller values increase the risk of evacuation \"      \\\n+          \"failures, which will trigger stop-the-world Full GC passes.\")    \\\n+          range(1.0,100.0)                                                  \\\n+                                                                            \\\n+  product(uintx, ShenandoahMaxEvacLABRatio, 0, EXPERIMENTAL,                \\\n+          \"Potentially, each running thread maintains a PLAB for \"          \\\n+          \"evacuating objects into old-gen memory and a GCLAB for \"         \\\n+          \"evacuating objects into young-gen memory.  Each time a thread \"  \\\n+          \"exhausts its PLAB or GCLAB, a new local buffer is allocated. \"   \\\n+          \"By default, the new buffer is twice the size of the previous \"   \\\n+          \"buffer.  The sizes are reset to the minimum at the start of \"    \\\n+          \"each GC pass.  This parameter limits the growth of evacuation \"  \\\n+          \"buffer sizes to its value multiplied by the minimum buffer \"     \\\n+          \"size.  A higher value allows evacuation allocations to be more \" \\\n+          \"efficient because less synchronization is required by \"          \\\n+          \"individual threads.  However, a larger value increases the \"     \\\n+          \"likelihood of evacuation failures, leading to long \"             \\\n+          \"stop-the-world pauses.  This is because a large value \"          \\\n+          \"allows individual threads to consume large percentages of \"      \\\n+          \"the total evacuation budget without necessarily effectively \"    \\\n+          \"filling their local evacuation buffers with evacuated \"          \\\n+          \"objects.  A value of zero means no maximum size is enforced.\")   \\\n+          range(0, 1024)                                                    \\\n+                                                                            \\\n@@ -237,0 +412,35 @@\n+  product(uintx, ShenandoahOldEvacRatioPercent, 75, EXPERIMENTAL,           \\\n+          \"The maximum proportion of evacuation from old-gen memory, \"      \\\n+          \"expressed as a percentage. The default value 75 denotes that no\" \\\n+          \"more than 75% of the collection set evacuation workload may be \" \\\n+          \"towards evacuation of old-gen heap regions. This limits both the\"\\\n+          \"promotion of aged regions and the compaction of existing old \"   \\\n+          \"regions.  A value of 75 denotes that the total evacuation work\"  \\\n+          \"may increase to up to four times the young gen evacuation work.\" \\\n+          \"A larger value allows quicker promotion and allows\"              \\\n+          \"a smaller number of mixed evacuations to process \"               \\\n+          \"the entire list of old-gen collection candidates at the cost \"   \\\n+          \"of an increased disruption of the normal cadence of young-gen \"  \\\n+          \"collections.  A value of 100 allows a mixed evacuation to \"      \\\n+          \"focus entirely on old-gen memory, allowing no young-gen \"        \\\n+          \"regions to be collected, likely resulting in subsequent \"        \\\n+          \"allocation failures because the allocation pool is not \"         \\\n+          \"replenished.  A value of 0 allows a mixed evacuation to\"         \\\n+          \"focus entirely on young-gen memory, allowing no old-gen \"        \\\n+          \"regions to be collected, likely resulting in subsequent \"        \\\n+          \"promotion failures and triggering of stop-the-world full GC \"    \\\n+          \"events.\")                                                        \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahMinYoungPercentage, 20, EXPERIMENTAL,            \\\n+          \"The minimum percentage of the heap to use for the young \"        \\\n+          \"generation. Heuristics will not adjust the young generation \"    \\\n+          \"to be less than this.\")                                          \\\n+          range(0, 100)                                                     \\\n+                                                                            \\\n+  product(uintx, ShenandoahMaxYoungPercentage, 100, EXPERIMENTAL,           \\\n+          \"The maximum percentage of the heap to use for the young \"        \\\n+          \"generation. Heuristics will not adjust the young generation \"    \\\n+          \"to be more than this.\")                                          \\\n+          range(0, 100)                                                     \\\n+                                                                            \\\n@@ -311,0 +521,9 @@\n+  product(uintx, ShenandoahCoalesceChance, 0, DIAGNOSTIC,                   \\\n+          \"Testing: Abandon remaining mixed collections with this \"         \\\n+          \"likelihood. Following each mixed collection, abandon all \"       \\\n+          \"remaining mixed collection candidate regions with likelihood \"   \\\n+          \"ShenandoahCoalesceChance. Abandoning a mixed collection will \"   \\\n+          \"cause the old regions to be made parsable, rather than being \"   \\\n+          \"evacuated.\")                                                     \\\n+          range(0, 100)                                                     \\\n+                                                                            \\\n@@ -339,0 +558,4 @@\n+  product(bool, ShenandoahCardBarrier, false, DIAGNOSTIC,                   \\\n+          \"Turn on\/off card-marking post-write barrier in Shenandoah: \"     \\\n+          \" true when ShenandoahGCMode is generational, false otherwise\")   \\\n+                                                                            \\\n@@ -354,2 +577,29 @@\n-\n-\/\/ end of GC_SHENANDOAH_FLAGS\n+  product(uintx, ShenandoahOldCompactionReserve, 8, EXPERIMENTAL,           \\\n+          \"During generational GC, prevent promotions from filling \"        \\\n+          \"this number of heap regions.  These regions are reserved \"       \\\n+          \"for the purpose of supporting compaction of old-gen \"            \\\n+          \"memory.  Otherwise, old-gen memory cannot be compacted.\")        \\\n+          range(0, 128)                                                     \\\n+                                                                            \\\n+  product(bool, ShenandoahAllowOldMarkingPreemption, true, DIAGNOSTIC,      \\\n+          \"Allow young generation collections to suspend concurrent\"        \\\n+          \" marking in the old generation.\")                                \\\n+                                                                            \\\n+  product(uintx, ShenandoahAgingCyclePeriod, 1, EXPERIMENTAL,               \\\n+          \"With generational mode, increment the age of objects and\"        \\\n+          \"regions each time this many young-gen GC cycles are completed.\") \\\n+                                                                            \\\n+  develop(bool, ShenandoahEnableCardStats, false,                           \\\n+          \"Enable statistics collection related to clean & dirty cards\")    \\\n+                                                                            \\\n+  develop(int, ShenandoahCardStatsLogInterval, 50,                          \\\n+          \"Log cumulative card stats every so many remembered set or \"      \\\n+          \"update refs scans\")                                              \\\n+                                                                            \\\n+  product(uintx, ShenandoahMinimumOldMarkTimeMs, 100, EXPERIMENTAL,         \\\n+         \"Minimum amount of time in milliseconds to run old marking \"       \\\n+         \"before a young collection is allowed to run. This is intended \"   \\\n+         \"to prevent starvation of the old collector. Setting this to \"     \\\n+         \"0 will allow back to back young collections to run during old \"   \\\n+         \"marking.\")                                                        \\\n+  \/\/ end of GC_SHENANDOAH_FLAGS\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":268,"deletions":18,"binary":false,"changes":286,"status":"modified"},{"patch":"@@ -96,0 +96,4 @@\n+gc\/shenandoah\/TestHumongousThreshold.java#default 8327000 generic-all\n+gc\/shenandoah\/TestHumongousThreshold.java#16b 8327000 generic-all\n+gc\/shenandoah\/TestHumongousThreshold.java#generational 8327000 generic-all\n+gc\/shenandoah\/TestHumongousThreshold.java#generational-16b 8327000 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"}]}