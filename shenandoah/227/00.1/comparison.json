{"files":[{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n@@ -31,0 +32,3 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -64,0 +68,45 @@\n+\/\/ This is a temporary solution to work around a shortcoming with the existing free set implementation.\n+\/\/ TODO:\n+\/\/   Remove this function after restructing FreeSet representation.  A problem in the existing implementation is that old-gen\n+\/\/   regions are not considered to reside within the is_collector_free range.\n+\/\/\n+HeapWord* ShenandoahFreeSet::allocate_with_old_affiliation(ShenandoahAllocRequest& req, bool& in_new_region) {\n+  ShenandoahRegionAffiliation affiliation = ShenandoahRegionAffiliation::OLD_GENERATION;\n+\n+  size_t rightmost = MAX2(_collector_rightmost, _mutator_rightmost);\n+  size_t leftmost = MIN2(_collector_leftmost, _mutator_leftmost);\n+\n+  for (size_t c = rightmost + 1; c > leftmost; c--) {\n+    \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+    size_t idx = c - 1;\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (r->affiliation() == affiliation && !r->is_humongous()) {\n+      if (!r->is_cset() && !has_no_alloc_capacity(r)) {\n+        HeapWord* result = try_allocate_in(r, req, in_new_region);\n+        if (result != nullptr) {\n+          return result;\n+        }\n+      }\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+HeapWord* ShenandoahFreeSet::allocate_with_affiliation(ShenandoahRegionAffiliation affiliation, ShenandoahAllocRequest& req, bool& in_new_region) {\n+  for (size_t c = _collector_rightmost + 1; c > _collector_leftmost; c--) {\n+    \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+    size_t idx = c - 1;\n+    if (is_collector_free(idx)) {\n+      ShenandoahHeapRegion* r = _heap->get_region(idx);\n+      if (r->affiliation() == affiliation) {\n+        HeapWord* result = try_allocate_in(r, req, in_new_region);\n+        if (result != nullptr) {\n+          return result;\n+        }\n+      }\n+    }\n+  }\n+  log_debug(gc, free)(\"Could not allocate collector region with affiliation: %s for request \" PTR_FORMAT, affiliation_name(affiliation), p2i(&req));\n+  return nullptr;\n+}\n+\n@@ -77,0 +126,26 @@\n+  \/\/ Overwrite with non-zero (non-NULL) values only if necessary for allocation bookkeeping.\n+\n+  bool allow_new_region = true;\n+  if (_heap->mode()->is_generational()) {\n+    switch (req.affiliation()) {\n+      case ShenandoahRegionAffiliation::OLD_GENERATION:\n+        \/\/ Note: unsigned result from adjusted_unaffiliated_regions() will never be less than zero, but it may equal zero.\n+        if (_heap->old_generation()->adjusted_unaffiliated_regions() <= 0) {\n+          allow_new_region = false;\n+        }\n+        break;\n+\n+      case ShenandoahRegionAffiliation::YOUNG_GENERATION:\n+        \/\/ Note: unsigned result from adjusted_unaffiliated_regions() will never be less than zero, but it may equal zero.\n+        if (_heap->young_generation()->adjusted_unaffiliated_regions() <= 0) {\n+          allow_new_region = false;\n+        }\n+        break;\n+\n+      case ShenandoahRegionAffiliation::FREE:\n+      default:\n+        ShouldNotReachHere();\n+        break;\n+    }\n+  }\n+\n@@ -80,1 +155,0 @@\n-\n@@ -83,2 +157,4 @@\n-        if (is_mutator_free(idx)) {\n-          HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n+        ShenandoahHeapRegion* r = _heap->get_region(idx);\n+        if (is_mutator_free(idx) && (allow_new_region || r->affiliation() != ShenandoahRegionAffiliation::FREE)) {\n+          \/\/ try_allocate_in() increases used if the allocation is successful.\n+          HeapWord* result = try_allocate_in(r, req, in_new_region);\n@@ -90,1 +166,0 @@\n-\n@@ -95,2 +170,2 @@\n-    case ShenandoahAllocRequest::_alloc_shared_gc: {\n-      \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+      \/\/ GCLABs are for evacuation so we must be in evacuation phase.  If this allocation is successful, increment\n+      \/\/ the relevant evac_expended rather than used value.\n@@ -98,5 +173,32 @@\n-      \/\/ Fast-path: try to allocate in the collector view first\n-      for (size_t c = _collector_rightmost + 1; c > _collector_leftmost; c--) {\n-        size_t idx = c - 1;\n-        if (is_collector_free(idx)) {\n-          HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n+    case ShenandoahAllocRequest::_alloc_plab:\n+      \/\/ PLABs always reside in old-gen and are only allocated during evacuation phase.\n+\n+    case ShenandoahAllocRequest::_alloc_shared_gc: {\n+      if (!_heap->mode()->is_generational()) {\n+        \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+        \/\/ Fast-path: try to allocate in the collector view first\n+        for (size_t c = _collector_rightmost + 1; c > _collector_leftmost; c--) {\n+          size_t idx = c - 1;\n+          if (is_collector_free(idx)) {\n+            HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n+            if (result != nullptr) {\n+              return result;\n+            }\n+          }\n+        }\n+      } else {\n+        \/\/ First try to fit into a region that is already in use in the same generation.\n+        HeapWord* result;\n+        if (req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+          \/\/ TODO: this is a work around to address a deficiency in FreeSet representation.  A better solution fixes\n+          \/\/ the FreeSet implementation to deal more efficiently with old-gen regions as being in the \"collector free set\"\n+          result = allocate_with_old_affiliation(req, in_new_region);\n+        } else {\n+          result = allocate_with_affiliation(req.affiliation(), req, in_new_region);\n+        }\n+        if (result != nullptr) {\n+          return result;\n+        }\n+        if (allow_new_region) {\n+          \/\/ Then try a free region that is dedicated to GC allocations.\n+          result = allocate_with_affiliation(FREE, req, in_new_region);\n@@ -114,10 +216,13 @@\n-      \/\/ Try to steal the empty region from the mutator view\n-      for (size_t c = _mutator_rightmost + 1; c > _mutator_leftmost; c--) {\n-        size_t idx = c - 1;\n-        if (is_mutator_free(idx)) {\n-          ShenandoahHeapRegion* r = _heap->get_region(idx);\n-          if (can_allocate_from(r)) {\n-            flip_to_gc(r);\n-            HeapWord *result = try_allocate_in(r, req, in_new_region);\n-            if (result != nullptr) {\n-              return result;\n+      if (allow_new_region) {\n+        \/\/ Try to steal an empty region from the mutator view.\n+        for (size_t c = _mutator_rightmost + 1; c > _mutator_leftmost; c--) {\n+          size_t idx = c - 1;\n+          if (is_mutator_free(idx)) {\n+            ShenandoahHeapRegion* r = _heap->get_region(idx);\n+            if (can_allocate_from(r)) {\n+              flip_to_gc(r);\n+              HeapWord *result = try_allocate_in(r, req, in_new_region);\n+              if (result != nullptr) {\n+                log_debug(gc, free)(\"Flipped region \" SIZE_FORMAT \" to gc for request: \" PTR_FORMAT, idx, p2i(&req));\n+                return result;\n+              }\n@@ -132,1 +237,0 @@\n-\n@@ -138,1 +242,0 @@\n-\n@@ -149,1 +252,13 @@\n-\n+  if (r->affiliation() == ShenandoahRegionAffiliation::FREE) {\n+    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+    r->set_affiliation(req.affiliation());\n+    if (r->is_old()) {\n+      \/\/ Any OLD region allocated during concurrent coalesce-and-fill does not need to be coalesced and filled because\n+      \/\/ all objects allocated within this region are above TAMS (and thus are implicitly marked).  In case this is an\n+      \/\/ OLD region and concurrent preparation for mixed evacuations visits this region before the start of the next\n+      \/\/ old-gen concurrent mark (i.e. this region is allocated following the start of old-gen concurrent mark but before\n+      \/\/ concurrent preparations for mixed evacuations are completed), we mark this region as not requiring any\n+      \/\/ coalesce-and-fill processing.\n+      r->end_preemptible_coalesce_and_fill();\n+      _heap->clear_cards_for(r);\n+    }\n@@ -152,1 +267,7 @@\n-  in_new_region = r->is_empty();\n+    assert(ctx->top_at_mark_start(r) == r->bottom(), \"Newly established allocation region starts with TAMS equal to bottom\");\n+    assert(ctx->is_bitmap_clear_range(ctx->top_bitmap(r), r->end()), \"Bitmap above top_bitmap() must be clear\");\n+  } else if (r->affiliation() != req.affiliation()) {\n+    assert(_heap->mode()->is_generational(), \"Request for %s from %s region should only happen in generational mode.\",\n+           affiliation_name(req.affiliation()), affiliation_name(r->affiliation()));\n+    return nullptr;\n+  }\n@@ -154,0 +275,1 @@\n+  in_new_region = r->is_empty();\n@@ -157,0 +279,6 @@\n+  if (in_new_region) {\n+    log_debug(gc, free)(\"Using new region (\" SIZE_FORMAT \") for %s (\" PTR_FORMAT \").\",\n+                       r->index(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(&req));\n+  }\n+\n+  \/\/ req.size() is in words, r->free() is in bytes.\n@@ -158,3 +286,84 @@\n-    size_t free = align_down(r->free() >> LogHeapWordSize, MinObjAlignment);\n-    if (size > free) {\n-      size = free;\n+    if (req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+      assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+      \/\/ Need to assure that plabs are aligned on multiple of card region.\n+      size_t free = r->free();\n+      size_t usable_free = (free \/ CardTable::card_size()) << CardTable::card_shift();\n+      if ((free != usable_free) && (free - usable_free < ShenandoahHeap::min_fill_size() * HeapWordSize)) {\n+        \/\/ We'll have to add another card's memory to the padding\n+        if (usable_free > CardTable::card_size()) {\n+          usable_free -= CardTable::card_size();\n+        } else {\n+          assert(usable_free == 0, \"usable_free is a multiple of card_size and card_size > min_fill_size\");\n+        }\n+      }\n+      free \/= HeapWordSize;\n+      usable_free \/= HeapWordSize;\n+      size_t remnant = size % CardTable::card_size_in_words();\n+      if (remnant > 0) {\n+        \/\/ Since we have Elastic TLABs, align size up.  This is consistent with aligning min_size up.\n+        size = size - remnant + CardTable::card_size_in_words();\n+      }\n+      if (size > usable_free) {\n+        size = usable_free;\n+        assert(size % CardTable::card_size_in_words() == 0, \"usable_free is a multiple of card table size\");\n+      }\n+\n+      size_t adjusted_min_size = req.min_size();\n+      remnant = adjusted_min_size % CardTable::card_size_in_words();\n+      if (remnant > 0) {\n+        \/\/ Round up adjusted_min_size to a multiple of alignment size\n+        adjusted_min_size = adjusted_min_size - remnant + CardTable::card_size_in_words();\n+      }\n+      if (size >= adjusted_min_size) {\n+        result = r->allocate_aligned(size, req, CardTable::card_size());\n+        assert(result != nullptr, \"Allocation cannot fail\");\n+        size = req.actual_size();\n+        assert(r->top() <= r->end(), \"Allocation cannot span end of region\");\n+        \/\/ actual_size() will be set to size below.\n+        assert((result == nullptr) || (size % CardTable::card_size_in_words() == 0),\n+               \"PLAB size must be multiple of card size\");\n+        assert((result == nullptr) || (((uintptr_t) result) % CardTable::card_size_in_words() == 0),\n+               \"PLAB start must align with card boundary\");\n+        if (free > usable_free) {\n+          \/\/ Account for the alignment padding\n+          size_t padding = (free - usable_free) * HeapWordSize;\n+          increase_used(padding);\n+          assert(r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION, \"All PLABs reside in old-gen\");\n+          _heap->old_generation()->increase_used(padding);\n+          \/\/ For verification consistency, we need to report this padding to _heap\n+          _heap->increase_used(padding);\n+        }\n+      }\n+      \/\/ Otherwise, leave result == nullptr because the adjusted size is smaller than min size.\n+    } else {\n+      \/\/ This is a GCLAB or a TLAB allocation\n+      size_t free = align_down(r->free() >> LogHeapWordSize, MinObjAlignment);\n+      if (size > free) {\n+        size = free;\n+      }\n+      if (size >= req.min_size()) {\n+        result = r->allocate(size, req);\n+        if (result != nullptr) {\n+          \/\/ Record actual allocation size\n+          req.set_actual_size(size);\n+        }\n+        assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, size);\n+      } else {\n+        log_trace(gc, free)(\"Failed to shrink TLAB or GCLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n+                           \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), size, req.min_size());\n+      }\n+    }\n+  } else if (req.is_lab_alloc() && req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+    assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+    \/\/ inelastic PLAB\n+    size_t free = r->free();\n+    size_t usable_free = (free \/ CardTable::card_size()) << CardTable::card_shift();\n+    free \/= HeapWordSize;\n+    usable_free \/= HeapWordSize;\n+    if ((free != usable_free) && (free - usable_free < ShenandoahHeap::min_fill_size() * HeapWordSize)) {\n+      \/\/ We'll have to add another card's memory to the padding\n+      if (usable_free > CardTable::card_size_in_words()) {\n+        usable_free -= CardTable::card_size_in_words();\n+      } else {\n+        assert(usable_free == 0, \"usable_free is a multiple of card_size and card_size > min_fill_size\");\n+      }\n@@ -162,3 +371,17 @@\n-    if (size >= req.min_size()) {\n-      result = r->allocate(size, req.type());\n-      assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, size);\n+    assert(size % CardTable::card_size_in_words() == 0, \"PLAB size must be multiple of remembered set card size\");\n+    if (size <= usable_free) {\n+      result = r->allocate_aligned(size, req, CardTable::card_size());\n+      size = req.actual_size();\n+      assert(result != nullptr, \"Allocation cannot fail\");\n+      assert(r->top() <= r->end(), \"Allocation cannot span end of region\");\n+      assert(req.actual_size() % CardTable::card_size_in_words() == 0, \"PLAB start must align with card boundary\");\n+      assert(((uintptr_t) result) % CardTable::card_size_in_words() == 0, \"PLAB start must align with card boundary\");\n+      if (free > usable_free) {\n+        \/\/ Account for the alignment padding\n+        size_t padding = (free - usable_free) * HeapWordSize;\n+        increase_used(padding);\n+        assert(r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION, \"All PLABs reside in old-gen\");\n+        _heap->old_generation()->increase_used(padding);\n+        \/\/ For verification consistency, we need to report this padding to _heap\n+        _heap->increase_used(padding);\n+      }\n@@ -167,1 +390,5 @@\n-    result = r->allocate(size, req.type());\n+    result = r->allocate(size, req);\n+    if (result != nullptr) {\n+      \/\/ Record actual allocation size\n+      req.set_actual_size(size);\n+    }\n@@ -170,0 +397,1 @@\n+  ShenandoahGeneration* generation = _heap->generation_for(req.affiliation());\n@@ -173,0 +401,2 @@\n+      assert(req.is_young(), \"Mutator allocations always come from young generation.\");\n+      generation->increase_used(size * HeapWordSize);\n@@ -174,6 +404,10 @@\n-    }\n-\n-    \/\/ Record actual allocation size\n-    req.set_actual_size(size);\n-\n-    if (req.is_gc_alloc()) {\n+    } else {\n+      assert(req.is_gc_alloc(), \"Should be gc_alloc since req wasn't mutator alloc\");\n+\n+      \/\/ For GC allocations, we advance update_watermark because the objects relocated into this memory during\n+      \/\/ evacuation are not updated during evacuation.  For both young and old regions r, it is essential that all\n+      \/\/ PLABs be made parsable at the end of evacuation.  This is enabled by retiring all plabs at end of evacuation.\n+      \/\/ TODO: Making a PLAB parsable involves placing a filler object in its remnant memory but does not require\n+      \/\/ that the PLAB be disabled for all future purposes.  We may want to introduce a new service to make the\n+      \/\/ PLABs parsable while still allowing the PLAB to serve future allocation requests that arise during the\n+      \/\/ next evacuation pass.\n@@ -181,0 +415,5 @@\n+      generation->increase_used(size * HeapWordSize);\n+      if (r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+        assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"old-gen allocations use PLAB or shared allocation\");\n+        \/\/ for plabs, we'll sort the difference between evac and promotion usage when we retire the plab\n+      }\n@@ -189,2 +428,3 @@\n-    \/\/ almost-full regions precede the fully-empty region where we want allocate the entire TLAB.\n-    \/\/ TODO: Record first fully-empty region, and use that for large allocations\n+    \/\/ almost-full regions precede the fully-empty region where we want to allocate the entire TLAB.\n+    \/\/ TODO: Record first fully-empty region, and use that for large allocations and\/or organize\n+    \/\/ available free segments within regions for more efficient searches for \"good fit\".\n@@ -197,0 +437,1 @@\n+        generation->increase_allocated(waste);\n@@ -251,3 +492,13 @@\n-  \/\/ No regions left to satisfy allocation, bye.\n-  if (num > mutator_count()) {\n-    return nullptr;\n+  assert(req.affiliation() == ShenandoahRegionAffiliation::YOUNG_GENERATION, \"Humongous regions always allocated in YOUNG\");\n+  ShenandoahGeneration* generation = _heap->generation_for(req.affiliation());\n+\n+  \/\/ Check if there are enough regions left to satisfy allocation.\n+  if (_heap->mode()->is_generational()) {\n+    size_t avail_young_regions = generation->adjusted_unaffiliated_regions();\n+    if (num > mutator_count() || (num > avail_young_regions)) {\n+      return nullptr;\n+    }\n+  } else {\n+    if (num > mutator_count()) {\n+      return nullptr;\n+    }\n@@ -285,0 +536,1 @@\n+  ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n@@ -308,1 +560,14 @@\n-    r->set_top(r->bottom() + used_words);\n+    r->set_affiliation(req.affiliation());\n+    r->set_update_watermark(r->bottom());\n+    r->set_top(r->bottom());    \/\/ Set top to bottom so we can capture TAMS\n+    ctx->capture_top_at_mark_start(r);\n+    r->set_top(r->bottom() + used_words); \/\/ Then change top to reflect allocation of humongous object.\n+    assert(ctx->top_at_mark_start(r) == r->bottom(), \"Newly established allocation region starts with TAMS equal to bottom\");\n+    assert(ctx->is_bitmap_clear_range(ctx->top_bitmap(r), r->end()), \"Bitmap above top_bitmap() must be clear\");\n+\n+    \/\/ Leave top_bitmap alone.  The first time a heap region is put into service, top_bitmap should equal end.\n+    \/\/ Thereafter, it should represent the upper bound on parts of the bitmap that need to be cleared.\n+    \/\/ ctx->clear_bitmap(r);\n+    log_debug(gc, free)(\"NOT clearing bitmap for Humongous region [\" PTR_FORMAT \", \" PTR_FORMAT \"], top_bitmap: \"\n+                        PTR_FORMAT \" at transition from FREE to %s\",\n+                        p2i(r->bottom()), p2i(r->end()), p2i(ctx->top_bitmap(r)), affiliation_name(req.affiliation()));\n@@ -316,0 +581,1 @@\n+  generation->increase_used(words_size * HeapWordSize);\n@@ -319,1 +585,3 @@\n-    _heap->notify_mutator_alloc_words(ShenandoahHeapRegion::region_size_words() - remainder, true);\n+    size_t waste = ShenandoahHeapRegion::region_size_words() - remainder;\n+    _heap->notify_mutator_alloc_words(waste, true);\n+    generation->increase_allocated(waste * HeapWordSize);\n@@ -387,0 +655,4 @@\n+\n+  \/\/ We do not ensure that the region is no longer trash,\n+  \/\/ relying on try_allocate_in(), which always comes next,\n+  \/\/ to recycle trash before attempting to allocate anything in the region.\n@@ -409,0 +681,1 @@\n+  log_debug(gc, free)(\"Rebuilding FreeSet\");\n@@ -422,0 +695,4 @@\n+\n+      log_debug(gc, free)(\"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator free set\",\n+          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n+               byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n@@ -426,1 +703,20 @@\n-  size_t to_reserve = _heap->max_capacity() \/ 100 * ShenandoahEvacReserve;\n+  if (!_heap->mode()->is_generational()) {\n+    size_t to_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+    reserve_regions(to_reserve);\n+  } else {\n+    size_t young_reserve = (_heap->young_generation()->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+    \/\/ Note that all allocations performed from old-gen are performed by GC, generally using PLABs for both\n+    \/\/ promotions and evacuations.  The partition between which old memory is reserved for evacuation and\n+    \/\/ which is reserved for promotion is enforced using thread-local variables that prescribe intentons within\n+    \/\/ each PLAB.  We do not reserve any of old-gen memory in order to facilitate the loaning of old-gen memory\n+    \/\/ to young-gen purposes.\n+    size_t old_reserve = 0;\n+    size_t to_reserve = young_reserve + old_reserve;\n+    reserve_regions(to_reserve);\n+  }\n+\n+  recompute_bounds();\n+  assert_bounds();\n+}\n+\n+void ShenandoahFreeSet::reserve_regions(size_t to_reserve) {\n@@ -439,0 +735,3 @@\n+      log_debug(gc, free)(\"  Shifting Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to collector free set\",\n+                          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n+                               byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n@@ -441,3 +740,0 @@\n-\n-  recompute_bounds();\n-  assert_bounds();\n@@ -449,1 +745,1 @@\n-  LogTarget(Info, gc, ergo) lt;\n+  LogTarget(Info, gc, free) lt;\n@@ -515,0 +811,2 @@\n+      ls.print(\"Used: \" SIZE_FORMAT \"%s, Mutator Free: \" SIZE_FORMAT \" \",\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used), mutator_count());\n@@ -520,0 +818,1 @@\n+      size_t total_used = 0;\n@@ -527,0 +826,1 @@\n+          total_used += r->used();\n@@ -530,1 +830,1 @@\n-      ls.print_cr(\"Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s\",\n+      ls.print_cr(\"Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s\",\n@@ -532,1 +832,2 @@\n-                  byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max));\n+                  byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max),\n+                  byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used));\n@@ -541,0 +842,1 @@\n+  \/\/ Allocation request is known to satisfy all memory budgeting constraints.\n@@ -547,0 +849,1 @@\n+      case ShenandoahAllocRequest::_alloc_plab:\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":355,"deletions":52,"binary":false,"changes":407,"status":"modified"}]}