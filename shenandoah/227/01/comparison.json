{"files":[{"patch":"@@ -2,1 +2,1 @@\n-project=jdk\n+project=shenandoah\n@@ -7,1 +7,1 @@\n-error=author,committer,reviewers,merge,issues,executable,symlink,message,hg-tag,whitespace,problemlists\n+error=author,committer,reviewers,executable,symlink,message,hg-tag,whitespace,problemlists\n@@ -21,3 +21,0 @@\n-[checks \"merge\"]\n-message=Merge\n-\n@@ -25,1 +22,1 @@\n-reviewers=1\n+committers=1\n@@ -31,3 +28,0 @@\n-[checks \"issues\"]\n-pattern=^([124-8][0-9]{6}): (\\S.*)$\n-\n","filename":".jcheck\/conf","additions":3,"deletions":9,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -0,0 +1,9 @@\n+## GenShen: the Generational Shenandoah project\n+\n+Gen Shen stands for Generational Shenandoah,\n+and it also means \"deep roots\" in Mandarin.\n+\n+Documents in this directory:\n+- [glossary.md](glossary.md): glossary of terms used in our documents,\n+- [workplan.summary.md](workplan.summary.md): summary of the plan of record that we strive to follow,\n+- [design.summary.md](design.summary.md): summary of design points that we rely upon as working hypotheses.\n","filename":"genshen-docs\/README.md","additions":9,"deletions":0,"binary":false,"changes":9,"status":"added"},{"patch":"@@ -0,0 +1,329 @@\n+## Design Points\n+\n+This section discusses design decisions, as guided by the following tenets.\n+\n+### Tenets\n+\n+1. Don’t punish the mutator: While mutator code must be added to \n+   implement generational collection, degradations of\n+   mutator throughput and responsiveness \n+   should be contained as much as possible.\n+   If in doubt between design choices, \n+   pick the one that promises to minimize mutator overhead.\n+2. Safeguard incremental progress with milestones that demonstrate key capabilities.\n+3. Minimize the overall development effort.  This includes containing\n+   efforts to prepare and present each milestone deliverable \n+   that do not directly contribute to the end product.\n+\n+### Design Decisions\n+\n+While it is probable that we may revisit some of the following design\n+decisions if we run into implementation difficulties or performance\n+problems, the following is the current plan of record.\n+\n+1. This document is maintained alongside the source code. \n+   1. It is organized hierarchically with sections\n+      dedicated to major design decisions.\n+      Further, more detailed decisions are organized in top-down fashion,\n+      with overarching design decisions preceding derivative design decisions.\n+      The topic numbers assigned to particular design decisions may evolve over time.\n+   2. There is a separate \"rationale\" document that explains more of the reasoning behind design decisions\n+      and links relevant portions between documents.\n+2. Genshen changes to the Shenandoah implementation will be\n+   compartmented.  See discussion [here](rationale.summary.md#compartmentalization\").\n+3. We will support concurrent collection of old gen and young gen,\n+   with the expectation that a single old collection will typically\n+   overlap with the execution of many young collections.\n+4. In order to minimize performance impact on the mutator, we will use\n+   a single Load-Reference-Barrier to implement both evacuation\n+   from young gen and from old gen.\n+   1. Tenuring will be implemented as part of young evacuation.\n+   2. All evacuation for both young and old gen regions\n+      happens under the control of the same GC threads and is\n+      supported by the same Load Reference Barrier (LRB) as in\n+      vanilla Shenandoah, with only small refinements to the\n+      implementation of slow-path code within the LRB.\n+   3. See discussion [here](rationale.summary.md#load-reference-barrier).\n+5. An old collection begins at the same time as a young gen\n+   collection, with both collections leveraging the\n+   results of a single shared root scan.\n+6. Old collection generally runs at lower priority (e.g., fewer\n+   numbers of parallel threads or greater “nice” values for\n+   concurrent threads) than young collection because\n+   replenishing the allocation pool to support ongoing allocation\n+   needs of mutator threads requires urgent completion of young GC.\n+   See discussion [here](rationale.summary.md#prioritization).\n+7. Although the planned future production release of GenShen will\n+   run young collection concurrently with old collection, support\n+   will also be implemented and maintained for alternating executions\n+   of young collections with global collections.  See\n+   discussion [here](rationale.summary.md#young-global-discussion).\n+   1. Since this is not a production release, efficiency of\n+      implementation is not a primary concern.\n+   2. Regression tests will exercise the implementation of this mode\n+      of operation.\n+8. A regression test suite will be developed and maintained to\n+   support all of the enhanced capabilities of GenShen, including\n+   capabilities that are enabled and disabled at build time for the\n+   JVM.\n+9. Though old collections start at the same time as young\n+   collections collections, they do not necessarily end at the same\n+   time.  Typically, many young-gen collections will be completed\n+   concurrently during the time that a single old-gen collection is\n+   running.  See discussion [here](rationale.summary.md#concurrency-of-young-and-old).\n+10. Root scanning will be performed during a JVM safe point.\n+11. Scanning of the remembered set will be performed by parallel\n+    young-gen GC threads during a JVM safe point.  A side\n+    effect of scanning is to mark as CLEAN any cards of the remembered\n+    set that are found by remembered set scanning to no longer be\n+    DIRTY. \n+12. The remembered set maintains a start-of-object representation to\n+    facilitate quick identification of where the first object\n+    pertaining to each DIRTY card region begins.  See discussion\n+    [here](rationale.summary.md#remembered-set-starting-offsets).\n+    This requires that:\n+    1. Newly promoted objects be registered with the remembered set\n+       and start-of-object support by post-processing the associated GCLAB block, and\n+    2. When entire regions are promoted out of young collections into\n+       old gen, the objects contained within those regions must be\n+       registered with the remembered set and the start-of-object support.\n+13. Young marking, including scanning of root pointers,\n+    will place discovered references to old gen into a thread-local SATB\n+    buffer so that they can be processed by an old-gen collection\n+    thread.  See discussion [here](rationale.summary.md#satb-keep-old-alive-entries). \n+    1. Possible optimization: refrain from logging old-gen pointers\n+       that refer to already marked old objects into the SATB buffer.\n+14. The default size of the SATB buffer will increase from 1024 to\n+    4096 words because we will be placing more information into the\n+    SATB buffer.\n+15. Whenever the SATB buffer is full, the slow path for adding to\n+    the SATB buffer will attempt to compress the buffer contents before\n+    communicating the buffer to GC threads.  If compression leaves a\n+    minimum of 256 slots available within the SATB buffer, the thread\n+    continues to add values to the existing buffer.  Compression\n+    consists of:\n+    1. For pointers to young gen:\n+       1. If concurrent marking of young gen is disabled, ignore the\n+          pointer.\n+       2. Otherwise, if the object referenced by this pointer has\n+          already been marked, ignore the pointer.\n+       3. If we choose to use SATB-based remembered set, ignore all\n+          overwritten address values that reside within young gen.\n+    2. For pointers to old gen:\n+       1. If concurrent marking of old gen is disabled, ignore the\n+          pointer.\n+       2. Otherwise, if the object referenced by this pointer\n+          has already been marked, ignore the pointer.\n+       3. If we choose to use an SATB-based remembered set:\n+          - If the card corresponding to an overwritten old gen\n+            address is already DIRTY, ignore this overwritten\n+            address.\n+          - Otherwise, fetch the pointer value held at the overwritten\n+            address.  If the fetched pointer value does not\n+            refer to young gen, ignore this overwritten address.\n+          - Otherwise, use a hash table (suggested size 127) to sift\n+            redundant DIRTY card references for the current batch of\n+            overwritten old-gen addresses.  Preserve only one address\n+            for each card entry that needs to be marked as DIRTY.\n+    3. SATB buffers will be processed by both a young GC\n+       thread and an old GC thread.\n+       1. The young GC thread will mark objects referenced\n+          by young pointers.\n+       2. The old GC thread will:\n+          1. Mark objects referenced by old-gen pointers, and\n+          2. If we choose to use SATB-based remembered set: mark as DIRTY\n+             the card entries corresponding to overwritten addresses.\n+16. GC uses a G1-style heuristic to choose collection sets for both\n+    young and old memory areas.  The collection set represents the\n+    heap regions that offer the greatest opportunity to quickly reclaim\n+    garbage, as with the existing Shenandoah implementation.  See\n+    discussion [here](rationale.summary.md#g1-heuristic).\n+17. Following an evacuation phase that evacuates\n+    both old and young heap regions, the update-references phase is\n+    required to update references throughout all \n+    old regions that were not selected as part of the old\n+    collection set in addition to updating references in young\n+    heap regions.\n+    1. If a particular young collection evacuation phase does not\n+       evacuate any old regions, then its update references phase can\n+       focus solely on young heap regions and the\n+       remembered set.\n+18. Tenuring is based on object age with the enhancements described\n+    below.  See discussion [here](rationale.summary.md#tenuring).\n+    1. The variable TenureCycle counts how many GC cycles correspond to\n+       each increment of an object’s age.  Object ages are not\n+       necessarily incremented each time a GC cycle is completed.  They\n+       are incremented each time TenureCycle GC cycles have been\n+       completed.\n+    2. The variable TenureAge represents the age at which an object\n+       becomes eligible for tenuring.\n+    3. During GC cycles that correspond to TenureCycle, the “age” of\n+       individual objects is incremented by 1 plus the size of the\n+       object’s original heap region age when the object is evacuated.\n+    4. During GC cycles that correspond to TenureCycle, the “age” of\n+       each heap region that has been fully allocated (i.e. no more\n+       available memory for allocation) and that is not in the\n+       collection set is incremented by 1 at the end of the evacuation\n+       phase.  If the resulting “age” equals TenureAge, then the entire\n+       region is reassigned to become part of old gen.\n+       1. The update-refs phase will process this heap region even\n+          though it is “now” considered to be part of old gen.\n+       2. Each of the objects contained within the promoted region\n+          shall be registered with the remembered set abstraction.\n+       3. Each of the objects contained within the promoted region\n+         be scanned to determine any references to young-gen\n+         memory that are contained therein.  For any such pointers, set\n+         the associated card table entry to DIRTY.\n+19. During evacuation, each running mutator thread has both a TLAB\n+    and a GCLAB.\n+    1. The TLAB is associated with a young heap region.\n+    2. The GCLAB is associated with an old heap region.\n+    3. When the mutator’s load reference barrier encounters a\n+       reference for which the associated object needs to be tenured, it\n+       allocates the copy memory from the GCLAB.\n+    4. When the mutator’s load reference barrier encounters a\n+       reference for which the associated object resides within the\n+       collection set of old gen, it allocates the copy memory from the\n+       GCLAB.\n+    5. When the mutator’s load reference barrier encounters a\n+       reference for which the associated object needs to be evacuated to\n+       young gen, it allocates the copy memory from the TLAB.\n+    6. We initially plan to use the same size for TLAB and GCLAB, but\n+       this decision may be revisited.\n+    7. If the size of the object to be evacuated is larger than half\n+       the size of the respective local allocation buffer, allocation\n+       of the replica memory is handled by alternative allocators, to be\n+       designed.  Call these \"odd\" objects.\n+20. During evacuation, each evacuating GC thread will maintain two\n+    GCLAB buffers:\n+    1. GCLAB-Young is associated with young gen.\n+    2. GCLAB-Old is associated with old gen.\n+    3. If the object to be evacuated currently resides in old gen or\n+      if it resides in young gen and it is to be tenured, allocate the\n+      copy memory from GCLAB-Old.\n+    4. Otherwise, allocate the copy memory from GCLAB-Young.\n+    5. At the end of the evacuation phase, consider repurposing any\n+       unspent GCLAB-Young as a TLAB if there is sufficient unallocated\n+       memory remaining within it.\n+    6. At the end of the evacuation phase, consider preserving the\n+       GCLAB-Old for use as a GCLAB for a mutator thread during the next\n+       young collections collection or as a GCLAB-Old during the next\n+       old-gen evacuation pass.\n+    7. See 19.7.\n+21. A certain budget of CPU time is provisioned to perform young-gen\n+    GC in order to support a particular planned workload.\n+    1. The resources dedicated to young-gen GC are limited in order\n+       to assure a certain amount of CPU time is available to mutator\n+       threads.\n+    2. Command line options allow user control over provisioning.  A\n+       TBD API may allow services to adjust provisioning dynamically.\n+    3. In the ideal, provisioning is adjusted automatically based on\n+       TBD heuristics.\n+    4. The provisioned CPU resources can support a range of service\n+       quality, reclaiming large numbers of heap regions with a low GC\n+       frequency or reclaiming small numbers of heap regions with a\n+       high GC frequency.  Given a particular frequency of GC cycles,\n+       the same CPU resources can evacuate a large number of sparsely\n+       populated heap regions or a small number of densely populated\n+       heap regions.  Tradeoffs between these configuration extremes\n+       may be adjusted under software control or by TBD\n+       heuristics.\n+    5. For each young-gen GC pass, a certain TBD percentage\n+       of CPU resources are reserved for old-gen evacuation and\n+       update-refs activities.\n+       1. The old-gen CPU resource budget represents the total amount\n+          of old-gen memory that can be relocated, and is quantified as\n+          a multiple N of the heap region size.\n+       2. The old-gen GC threads determine the composition of the\n+          old-gen collection set, up to but never exceeding the upper\n+          bound N on cumulative evacuation size.\n+       3. The old-gen GC thread may select for the collection set\n+          N heap regions which are known to have 100%\n+          utilization, 2N heap regions known to have 50% utilization,\n+          5N heap regions known to have 20% utilization, and so on.\n+       4. If the old-gen GC refrains from delegating N heap regions\n+          worth of evacuation work to the young-gen evacuation phase,\n+          then the young GC is free to use the excess CPU resources to\n+          more aggressively evacuate more of its own young heap regions,\n+          using a larger than normal young-gen collection set.\n+       5. The budget for updating old-gen references must be large\n+          enough to handle the total old-gen memory size - N.  In the\n+          case that old-gen GC configures a collection set that\n+          represents its full evacuation capacity of N heap regions, the\n+          number of old-gen heap regions that are not part of the\n+          old-gen collection set is never larger than this quantity.\n+          In the case that old-gen GC configures a smaller collection\n+          set, then for each fraction of a heap region that is not\n+          evacuated, this much more of a heap region might have to be\n+          processed during the update-refs phase of GC.  We estimate\n+          that the cost of evacuating M bytes of memory is similar to\n+          the cost of updating the references within M bytes of\n+          memory.\n+22. A smaller (generally) budget of CPU time is provisioned to\n+    perform old-gen GC in order to support a particular planned\n+    workload.\n+    1. The resources dedicated to young-gen GC are limited in order\n+       to assure a certain amount of CPU time is available to mutator\n+       threads.\n+    2. Command-line options allow user control over provisioning.  A\n+       TBD API may allow services to adjust provisioning dynamically.\n+    3. In the ideal, provisioning is adjusted automatically based on\n+       TBD heuristics.\n+    4. As with young-gen GC, the CPU resources provisioned for\n+       old-gen GC can support a range of service quality.\n+    5. The CPU resources dedicated to old-gen collection do not have\n+       responsibility for evacuating old regions as all evacuation\n+       is performed by young-gen GC threads.  Instead, the CPU\n+       resources dedicated to old-gen GC activities are used for\n+       activities such as the following:\n+       1. Processing the content of SATB buffers:\n+          - If old collection is in concurrent marking phase, mark\n+            objects referenced by any keep-alive pointers.\n+          - If we are using SATB-based remembered set, update the\n+            remembered set based on overwritten addresses reported in the\n+            SATB buffer.\n+       2. During concurrent marking, scan the contents of previously\n+          marked objects to complete the transitive closure of\n+          reachability.\n+       3. Perform heuristic computations:\n+          - Determine when to start the next old-gen GC cycle.\n+          - Determine which old regions to evacuate on this and future\n+            passes of young-gen GC.\n+          - Adjust young-gen efficiency parameters such as: How many\n+            heap regions should be dedicated to young gen?  What is\n+            optimal value of TenureCycle?  What is optimal value of\n+            TenureAge?  How much CPU time should be dedicated to\n+            young-gen GC?\n+          - Adjust old-gen efficiency parameters such as: How much CPU\n+            time should be dedicated to old-gen GC?  How many heap regions\n+            should be dedicated to old gen?  Should any heap regions be\n+            decommissioned and returned to the operating system in order\n+            to shrink the memory footprint of this service?\n+       4. Perform routine maintenance as time and schedule permits:\n+          - Potentially sweep up dead memory, accumulating ages at\n+            which dead objects were reclaimed within old regions.\n+          - Potentially, sweep through evacuated memory to accumulate\n+            ages at which dead objects were reclaimed.\n+          - Organize free lists for fast and efficient allocation of\n+            GCLAB and Odd object allocations.\n+          - Return emptied old regions to the free set.\n+          - Eventually, reference processing and string deduplication\n+            should be performed by lower priority old-gen threads\n+            rather than higher priority young-gen threads.\n+       5. Following completion of each old-gen concurrent mark pass,\n+          select regions to be included in the old-gen collection set:\n+          - No more than a total of N bytes of old gen is evacuated by\n+            each pass of the young-gen evacuator.\n+          - If old-gen GC threads desire to evacuate M, which is more\n+            than N bytes of old gen, it does so by piggy backing on\n+            multiple subsequent young-gen evacuation passes, selecting\n+            evacuating no more than the accumulation of N total heap\n+            regions in each of the following young-gen evacuation passes.\n+          - Since it is most efficient to evacuate all M > N regions\n+            of old-gen memory with a single young-gen evacuation pass,\n+            configure the old-gen collection set to include all M\n+            regions if there are sufficient free regions available to\n+            afford the young-gen allocator to continue allocating new\n+            objects during the longer delay that would be required to\n+            evacuate more than the traditionally budgeted N regions of\n+            old-gen memory.\n","filename":"genshen-docs\/design.summary.md","additions":329,"deletions":0,"binary":false,"changes":329,"status":"added"},{"patch":"@@ -0,0 +1,44 @@\n+## Glossary for the Generational Shenandoah (GenShen) Project\n+\n+Shen := Shenandoah := the Shenandoah garbage collector\n+\n+GenShen := Generational Shenandoah\n+\n+gen := generation\n+young gen := the young generation\n+old gen := the old generation\n+\n+collector := garbage collector\n+young collector := young gen collector\n+old collector := old gen collector\n+global collector := single-generation collector that works on the entire heap\n+\n+young\/old\/global collection := a complete cycle through the phases of the young\/old\/global collector\n+\n+cset := collection set\n+remset := remembered set\n+rset := remembered set\n+\n+parallel := same task, dealt with by multiple threads\n+concurrent := different tasks, operated upon simultaneously\n+conc := concurrent\n+\n+conc mark := concurrent marking\n+conc global GC := concurrent global GC, like vanilla Shen\n+evac := evacuation (phase)\n+UR := update references (phase)\n+\n+LRB := Load Reference Barrier\n+SATB := Snapshot At The Beginning\n+TLAB := Thread-Local Allocation Buffer for a mutator thread\n+GCLAB := like a TLAB, but for a GC thread\n+\n+young region := a heap region affiliated with young gen\n+old region := a heap region affiliated with old gen\n+free region := a region that is not affiliated with either generation and available for future reuse by allocators \n+\n+young object := an object in young gen\n+old object := an object in old gen\n+\n+block := an identifiable chunk of space in a region that is or was occupied by a Java object\n+block start := a pointer to the beginning of a block\n","filename":"genshen-docs\/glossary.md","additions":44,"deletions":0,"binary":false,"changes":44,"status":"added"},{"patch":"@@ -0,0 +1,395 @@\n+# Summary Plan of Record: GenShen Prototype (2020)\n+\n+## Planned Milestones\n+\n+### Overview of Initial Milestones\n+\n+1. Pure young collection. (Young gen size unlimited. Old gen untouched.)\n+2. Size-restricted young gen.\n+3. Tenuring and promotion.\n+4. Global collection after young collection.\n+5. Young collection after global collection, repeat alternations.\n+6. Concurrent old marking.\n+7. Concurrent old and young collections.\n+\n+Young collection reclaims garbage only from heap regions that are\n+identified as belonging to the young generation.\n+\n+Old collection reclaims garbage only from heap regions that are\n+identified as belonging to the old generation, which holds objects\n+that have been promoted from young generation.\n+\n+Global collection reclaims garbage from heap regions belonging to\n+either the young generation or the old generation.\n+\n+### Milestone 1: GC of Young Gen Only\n+\n+This demonstration proves successful implementation of:\n+\n+1. Separating out that certain heap regions comprise young gen and\n+   other heap regions are considered to represent old gen. \n+2. Confirming that card marking does not crash.  (Does not prove that\n+   card marking works because we have no objects in old gen.) \n+3. Confirming that remembered set scanning does not crash.  (Does not\n+   prove that remembered set scanning works because we have no objects in\n+   old gen.) \n+4. Demonstrating a simplified form of young collection.\n+\n+Tasks\n+\n+1. Integrate various completed code patches into a common branch\n+2. Test and debug existing code\n+\n+### Milestone 2: Restrict Size of Young Gen\n+\n+This milestone constrains the young generation to a certain number of\n+regions.  After that number is reached, allocation fails.  For now, the\n+number can be fixed, say 25% of all regions, and we can simply crash\n+thereafter. \n+\n+Tasks\n+\n+1. Establish a mechanism to specify and enforce a limit on the number\n+   of heap regions that may comprise young-gen memory.\n+2. Adjust the GC triggering mechanism so that the size of young gen\n+   does not have reason to exceed the young-gen size during young-gen\n+   GC, where the size of young-gen is affected by GC in the\n+   following ways:\n+   1. Certain free heap regions may be added to young gen in order to\n+      support allocation requests that are made by concurrent mutator\n+      threads while GC is being performed.\n+   2. At the end of GC, all heap regions that were part of the\n+      collection set are removed from young-gen memory and placed\n+      in the free pool.\n+\n+\n+### Milestone 3: Young Collection with Promotion of Tenured Objects\n+\n+Add to Milestone 2 the capability of promoting young gen objects.\n+Don’t worry about odd objects or humongous objects at this time.\n+Demonstrate: \n+\n+1. That we can promote objects into old gen.\n+2. That card-marking works.\n+3. That remembered set scanning works (correctly, but perhaps not with\n+   full desired efficiency). \n+\n+The following tasks must be implemented:\n+\n+1. The collection set is selected as a subset of all young collections\n+   heap regions using TBD heuristics. \n+2. During young collection, each time an object is evacuated to a\n+   young consolidation region, the object’s age is incremented. \n+    1. For simplicity, don’t try to implement the complete region\n+       aging or promotion at this time. \n+    2. Also for simplicity, don’t try to implement the TenureCycle\n+       optimization. \n+3. During young collection, each GC thread maintains both a young\n+   GCLAB and and old GCLAB.\n+    1. When evacuating an object whose incremented age is less than\n+       TenureAge, allocate memory for the object’s copy from within the\n+       young GCLAB. \n+    2. When evacuating an object whose incremented age is >=\n+       TenureAge, allocate memory for the object’s copy from within the\n+       old GCLAB. \n+    3. Don’t support Odd or Humongous objects for simplicity.\n+4. During young collection, each mutator thread maintains both a TLAB\n+   and a GCLAB.  The GCLAB is associated with old gen. \n+    1. When evacuating an object whose incremented age is less than\n+       TenureAge, allocate memory for the object’s copy from within the\n+       TLAB. \n+    2. When evacuating an object whose incremented age is >=\n+       TenureAge, allocate memory for the object’s copy from within the\n+       GCLAB. \n+5. Perform maintenance on the contents of each retired old GCLAB,\n+   where this maintenance consists of: \n+    1. Registering each object’s starting location and length with the\n+       remembered set abstraction so that remembered set scanning can\n+       quickly find the first object within each DIRTY card region, \n+    2. Updating the card table to reflect all references from this\n+       object into young gen,  \n+    3. Evacuating all objects directly referenced from this object\n+       which reside within a collection set and have not yet been\n+       evacuated, and \n+    4. Healing all pointers from within newly evacuated old objects\n+       that refer to objects residing within the collection set. \n+    5. The last two items listed above are already performed by\n+       traditional Shenandoah but can be merged with the implementation of\n+       the other maintenance activities in order to perform all this work\n+       in a consolidated pass. \n+\n+### Milestone 4: Sequential Execution of Multiple Young Collections Followed by Multiple Global Collections\n+\n+__Demonstrated Concurrency Between Young-Gen and Old-Gen Activities__\n+\n+   ✓ denotes that this combination of activities is allowed.\n+\n+   ✗ denotes that this combination of activities is disallowed.\n+\n+|                |  Old-Gen Mark  | Old-Gen Evac  | Old-Gen Idle |\n+|:--------------:|:--------------:|:-------------:|:------------:|\n+| Young Gen Mark |      ✓         |     ✗         |     ✓        |\n+| Young Gen Evac |      ✗         |     ✓         |     ✓        |\n+| Young Gen Idle |      ✗         |     ✗         |     ✓        |\n+\n+Add to Milestone 3 the ability to switch to global collection after a\n+series of young Collections.\n+\n+1. The switch is triggered by a simple TBD test, such as when the\n+   space available within old gen is less than the size of young gen. \n+2. The global collection continues to run with the card-marking\n+   barrier enabled though the card values will not be consulted further.\n+3. For this demonstration, the switch to global collections is\n+   one-way.  Following this switch, we can no longer perform young\n+   collections. \n+4. For this demonstration, the global collection does not distinguish\n+   between young regions and old regions. \n+    1. Evacuation of objects that resided in an old region is handled\n+    the same as evacuation of objects that resided in a young heap\n+    region. \n+5. This demonstrates that:\n+    1. Promotion of objects works.  The objects that have been promoted\n+    into old gen maintain whatever protocols and invariants are\n+    assumed by Shenandoah GC. \n+    2. That we can manage the transition from young GC to global GC.\n+    3. That global collection, insofar as we characterize it, works.\n+    4. That Young collections do not corrupt the heap.\n+\n+Tasks:\n+\n+1. Fix bugs in existing code enhancements.\n+2. Implement the transition from young collection to global collection\n+3. Implement global collection with write barrier for card-table marking\n+\n+### Milestone 5: Interleaved Execution of Young and Global Collections\n+\n+__Demonstrated Concurrency Between Young-Gen and Old-Gen Activities__\n+\n+   ✓ denotes that this combination of activities is allowed.\n+\n+   ✗ denotes that this combination of activities is disallowed.\n+\n+|                |  Old-Gen Mark  | Old-Gen Evac  | Old-Gen Idle |\n+|:--------------:|:--------------:|:-------------:|:------------:|\n+| Young Gen Mark |      ✓         |     ✗         |     ✓        |\n+| Young Gen Evac |      ✗         |     ✓         |     ✓        |\n+| Young Gen Idle |      ✗         |     ✗         |     ✓        |\n+\n+Add to Milestone 4 the ability to switch back to Young Collection upon\n+completion of a global collection. \n+\n+1. Assume that the global collection is successful in reclaiming\n+   necessary memory.  No heuristics to resize oldgen or young gen at\n+   this time. Specify sizes of each on command line. \n+2. The switch to old collection is triggered by exhaustion of old gen.\n+   At least one young collection is assumed to execute following\n+   completion of each global collection. \n+3. For this demonstration, the global collection does distinguish\n+   between young collections and old regions. \n+    1. Objects in the old collection set are evacuated to old\n+       consolidation regions. \n+    2. Objects in the young collection set are evacuated\n+       to young collections consolidation regions. \n+    3. There is no tenuring of objects during a global collection (for\n+       simplicity). \n+\n+This demonstrates that:\n+\n+1. Promotion of objects works.  The objects that have been promoted\n+   into old gen maintain whatever protocols and invariants are\n+   assumed by Shenandoah GC. \n+2. That we can manage the transition from young GC to global GC.\n+3. That we can manage the transition from global GC to young GC.\n+4. That the transition from global GC to young GC\n+   establishes all invariants required for correct operation of young\n+   GC. \n+\n+Tasks:\n+\n+1. Distinguish between “global collection” for purposes of\n+   maintaining support for non-generational GC and “global collection”\n+   for purposes of supporting sequential interleaving of young GC\n+   and global GC.\n+2. Implement the transition from global GC to young GC.\n+3. Initialize the remembered set for each consolidation heap region\n+   of old gen to all CLEAN before allocating any GCLAB buffers within\n+   the consolidation heap region.\n+4. At the start of global evacuation, select the collection set as\n+   some combination of existing young regions and\n+   old regions based on heuristics TBD.\n+5. During global collection, maintain old-gen GCLABs for all GC\n+   threads and mutator threads.\n+6. During global collection, distinguish evacuation behavior\n+   depending on whether an object to be evacuated resides within the\n+   young collection set or the old collection set since\n+   young objects are evacuated into young \n+   consolidation regions and old objects are evacuated into old\n+   consolidation regions;\n+7. Add minimal logging reports to describe behavior of young-gen and global\n+   GC.\n+8. During global collection, perform maintenance on the contents of\n+   each retired old GCLAB, where this maintenance consists of:\n+   1. Registering each object’s starting location and length with the\n+      remembered set abstraction so that remembered set scanning can\n+      quickly find the first object within each DIRTY card region,\n+   2. Updating the card table to reflect all references from this\n+      object into young gen,\n+   3. Evacuating all objects directly referenced from this object\n+      that reside within a collection set and that have not already been\n+      evacuated, and\n+   4. Healing all pointers from within new replica objects residing in old\n+      gen that refer to objects residing within the collection set.\n+   5. The last two items listed above are already performed by\n+      traditional Shenandoah but can be merged with the implementation of\n+      the other maintenance activities in order to perform all this work\n+      in a consolidated pass.\n+\n+### Milestone 6: GC of young collections with Concurrent Marking (but not Collecting) of Old Gen\n+\n+__Demonstrated Concurrency Between Young-Gen and Old-Gen Activities__\n+\n+   ✓ denotes that this combination of activities is allowed.\n+\n+   ✗ denotes that this combination of activities is disallowed.\n+\n+|                |  Old-Gen Mark  | Old-Gen Evac  | Old-Gen Idle |\n+|:--------------:|:--------------:|:-------------:|:------------:|\n+| Young Gen Mark |      ✓         |     NA         |     ✓        |\n+| Young Gen Evac |      ✓         |     NA         |     ✓        |\n+| Young Gen Idle |      ✓         |     NA         |     ✓        |\n+\n+This demonstration relies on GC log reports to show that marking of\n+old gen runs concurrently with marking of young gen.\n+Since the results of old-gen marking are not used to support old-gen\n+evacuation, this demonstration does not prove that old-gen marking\n+produces correct results.\n+\n+All pointers to old-gen memory that are discovered during scan of\n+young-gen memory are communicated to the old-gen concurrent mark\n+threads by inserting these pointer values into a SATB buffer as\n+keep-alive values.  Every SATB buffer is post-processed both by a\n+young-gen GC thread and by an old-gen GC thread.\n+\n+Pointers from old-gen memory to young-gen memory that are discovered\n+during the marking of old-gen are ignored.\n+\n+At the start of young-gen concurrent marking, the remembered set is\n+scanned to detect all inter-generational references.\n+\n+The SATB write barrier remains enabled as long as either young-gen or\n+old-gen concurrent marking is active.\n+\n+Tasks:\n+\n+1. Each young GC thread has a dedicated SATB buffer into which it places\n+   discovered references to old-gen memory.\n+2. SATB write barrier is left enabled as long as either young\n+   or old marking is active.\n+3. SATB buffer is enlarged to 4096 entries.\n+4. SATB buffer compression is enhanced to deal with the mix of old\n+   and young pointers.\n+5. SATB buffer processing is performed by both a young collection\n+   thread and an old collection thread.  Pointers to\n+   old gen within the SATB buffer are marked and added to the old-gen\n+   closure queues so that they can be subsequently scanned.\n+6. Certain GC threads (or work items) are dedicated to old GC\n+   and others are dedicated to young GC.\n+7. Old-gen GC threads process the closure of previously marked\n+   old-gen objects, scanning all references contained therein.\n+8. Old-gen GC threads add to the old-gen closures all old-gen objects\n+   referenced by SATB buffers if those objects were not previously marked.\n+\n+### Milestone 7: Concurrent Young and Old Collections\n+\n+\n+__Demonstrated Concurrency Between Young-Gen and Old-Gen Activities__\n+\n+   ✓ denotes that this combination of activities is allowed.\n+\n+   ✗ denotes that this combination of activities is disallowed.\n+\n+|                |  Old-Gen Mark  | Old-Gen Evac  | Old-Gen Idle |\n+|:--------------:|:--------------:|:-------------:|:------------:|\n+| Young Gen Mark |      ✓         |     ✗         |     ✓        |\n+| Young Gen Evac |      ✓         |     ✓         |     ✓        |\n+| Young Gen Idle |      ✓         |     ✗         |     ✓        |\n+\n+In this demonstration, old-gen concurrent marking runs concurrently with all\n+phases of young-gen GC.  Old-gen evacuation only runs while young-gen\n+evacuation is running.  In the case that old-gen needs to evacuate so\n+much memory that doing so in a single uninterruptible batch would\n+significantly extend the duration of the young-gen evacuation phase,\n+the total old-gen evacuation workload is divided into multiple smaller\n+batches of evacuation work, each batch being processed concurrently\n+with a different young-gen evacuation cycle. \n+\n+The demonstration:\n+\n+1. Uses logging reports to describe the results of young\n+   collection and old collection.\n+2. Shows for some “simple” workload (Heapothysis or\n+   Extremem?) that generational GC provides performance benefits over\n+   non-generational GC.\n+\n+Tasks\n+\n+1. Add minimal logging reports to describe behavior of old-gen\n+   GC.\n+2. Decide which old regions comprise the old collection set.\n+3. Divide the old collection set into multiple collection subsets.\n+4. For each of the collection subsets\n+   1. Communicate the subset to young GC tasks to process these\n+      evacuations when it begins its next evacuation cycle.\n+   2. Wait for young GC tasks to signal completion of the evacuation\n+      cycle.\n+\n+## Proposed Future Milestones Not Yet Fully Planned\n+\n+### Milestone 8: Performance Improvements\n+\n+1. Remembered Set scanning sets cards to CLEAN if they are no longer\n+   DIRTY.\n+2. Remembered Set scanning maintains and utilizes start-offset data\n+   structure to quickly find the first object to be scanned within each\n+   DIRTY card.\n+3. Remembered set scanning refrains from scanning the portions of\n+   large objects and arrays that overlap card regions that are not\n+   DIRTY.\n+\n+### Milestone 9: Fix Known Bugs\n+\n+We are aware of bugs in our existing card-marking implementation.\n+\n+### Milestone 10: Multiple Young-Gen Evacuations Process Old Collection Set\n+\n+### Milestone 11: Odd Objects (larger than 50% of TLAB\/GCLAB size)\n+\n+By default, the promotion of such objects is handled by a\n+slower-than-normal path.  Instead of allocating old gen from the\n+GCLAB, the mutator thread obtains memory for the copy by directly\n+accessing free lists. See existing code that does that already. \n+\n+### Micro Milestone 12: Collect and Report New Metrics\n+\n+### Micro Milestone 13: SATB-Based Remembered Set\n+\n+### Micro Milestone 14: Heuristic Pacing of Young Collection Frequency\n+\n+### Micro Milestone 15: Heuristic Pacing of Old Collection Frequency\n+\n+### Micro Milestone 16: Heuristic Sizing of Young and Old Sizes\n+\n+### Micro Milestone 17: Heuristic Adjustments of Tenuring Strategies\n+\n+### Micro Milestone 18: Overlap Evacuation of Cycle N with Marking of Cycle N+1\n+\n+### Micro Milestone 19: Humongous Objects\n+\n+### Micro Milestone 20: Reference Processing\n+\n+### Micro Milestone 21: String Dedup\n+\n+### Micro Milestone 22: Degenerated GC\n+\n+### Micro Milesones TBD: Various Performance Improvements\n+\n","filename":"genshen-docs\/workplan.summary.md","additions":395,"deletions":0,"binary":false,"changes":395,"status":"added"},{"patch":"@@ -91,0 +91,2 @@\n+\n+      post_barrier(access, access.resolved_addr(), new_value.result());\n@@ -94,1 +96,8 @@\n-  return BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+\n+  LIR_Opr result =  BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+\n+  if (access.is_oop()) {\n+    post_barrier(access, access.resolved_addr(), new_value.result());\n+  }\n+\n+  return result;\n@@ -122,0 +131,1 @@\n+    post_barrier(access, access.resolved_addr(), result);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_aarch64.cpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -63,1 +64,1 @@\n-        __ mov(rscratch2, ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::MARKING);\n+        __ mov(rscratch2, ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n@@ -80,0 +81,7 @@\n+void ShenandoahBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                                       Register start, Register count, Register tmp, RegSet saved_regs) {\n+  if (is_oop) {\n+    gen_write_ref_array_post_barrier(masm, decorators, start, count, tmp, saved_regs);\n+  }\n+}\n+\n@@ -378,0 +386,25 @@\n+void ShenandoahBarrierSetAssembler::store_check(MacroAssembler* masm, Register obj) {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+      return;\n+  }\n+\n+  ShenandoahBarrierSet* ctbs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = ctbs->card_table();\n+\n+  __ lsr(obj, obj, CardTable::card_shift());\n+\n+  assert(CardTable::dirty_card_val() == 0, \"must be\");\n+\n+  __ load_byte_map_base(rscratch1);\n+\n+  if (UseCondCardMark) {\n+    Label L_already_dirty;\n+    __ ldrb(rscratch2,  Address(obj, rscratch1));\n+    __ cbz(rscratch2, L_already_dirty);\n+    __ strb(zr, Address(obj, rscratch1));\n+    __ bind(L_already_dirty);\n+  } else {\n+    __ strb(zr, Address(obj, rscratch1));\n+  }\n+}\n+\n@@ -414,0 +447,1 @@\n+    store_check(masm, r3);\n@@ -598,0 +632,29 @@\n+void ShenandoahBarrierSetAssembler::gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                                                     Register start, Register count, Register scratch, RegSet saved_regs) {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  ShenandoahBarrierSet* bs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = bs->card_table();\n+\n+  Label L_loop, L_done;\n+  const Register end = count;\n+\n+  __ cbz(count, L_done); \/\/ zero count - nothing to do\n+\n+  __ lea(end, Address(start, count, Address::lsl(LogBytesPerHeapOop))); \/\/ end = start + count << LogBytesPerHeapOop\n+  __ sub(end, end, BytesPerHeapOop); \/\/ last element address to make inclusive\n+  __ lsr(start, start, CardTable::card_shift());\n+  __ lsr(end, end, CardTable::card_shift());\n+  __ sub(count, end, start); \/\/ number of bytes to copy\n+\n+  __ load_byte_map_base(scratch);\n+  __ add(start, start, scratch);\n+  __ bind(L_loop);\n+  __ strb(zr, Address(start, count));\n+  __ subs(count, count, 1);\n+  __ br(Assembler::GE, L_loop);\n+  __ bind(L_done);\n+}\n+\n@@ -698,1 +761,7 @@\n-  __ tbz(tmp, ShenandoahHeap::MARKING_BITPOS, done);\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    __ tbz(tmp, ShenandoahHeap::YOUNG_MARKING_BITPOS, done);\n+  } else {\n+    __ mov(rscratch2, ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n+    __ tst(tmp, rscratch2);\n+    __ br(Assembler::EQ, done);\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/shenandoahBarrierSetAssembler_aarch64.cpp","additions":71,"deletions":2,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -58,0 +58,2 @@\n+  void store_check(MacroAssembler* masm, Register obj);\n+\n@@ -62,0 +64,2 @@\n+  void gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators, Register start, Register count, Register scratch, RegSet saved_regs);\n+\n@@ -77,0 +81,2 @@\n+  virtual void arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                  Register start, Register count, Register tmp, RegSet saved_regs);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/shenandoahBarrierSetAssembler_aarch64.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -107,0 +107,2 @@\n+      post_barrier(access, access.resolved_addr(), new_value.result());\n+\n@@ -117,1 +119,7 @@\n-  return BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+  LIR_Opr result = BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+\n+  if (access.is_oop()) {\n+    post_barrier(access, access.resolved_addr(), new_value.result());\n+  }\n+\n+  return result;\n@@ -153,0 +161,2 @@\n+\n+    post_barrier(access, access.resolved_addr(), result);\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_ppc.cpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -93,2 +94,0 @@\n-  __ block_comment(\"arraycopy_prologue (shenandoahgc) {\");\n-\n@@ -117,0 +116,1 @@\n+  __ block_comment(\"arraycopy_prologue (shenandoahgc) {\");\n@@ -133,1 +133,1 @@\n-                              : ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::MARKING;\n+                              : ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING;\n@@ -190,0 +190,10 @@\n+void ShenandoahBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                                       Register dst, Register count,\n+                                                       Register preserve) {\n+  if (is_reference_type(type)) {\n+    __ block_comment(\"arraycopy_epilogue (shenandoahgc) {\");\n+    gen_write_ref_array_post_barrier(masm, decorators, dst, count, preserve);\n+    __ block_comment(\"} arraycopy_epilogue (shenandoahgc)\");\n+  }\n+}\n+\n@@ -223,1 +233,1 @@\n-  __ andi_(tmp1, tmp1, ShenandoahHeap::MARKING);\n+  __ andi_(tmp1, tmp1, ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n@@ -589,0 +599,21 @@\n+void ShenandoahBarrierSetAssembler::store_check(MacroAssembler* masm, Register base, RegisterOrConstant ind_or_offs, Register tmp) {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+      return;\n+  }\n+\n+  ShenandoahBarrierSet* ctbs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = ctbs->card_table();\n+  assert_different_registers(base, tmp, R0);\n+\n+  if (ind_or_offs.is_constant()) {\n+    __ add_const_optimized(base, base, ind_or_offs.as_constant(), tmp);\n+  } else {\n+    __ add(base, ind_or_offs.as_register(), base);\n+  }\n+\n+  __ load_const_optimized(tmp, (address)ct->byte_map_base(), R0);\n+  __ srdi(base, base, CardTable::card_shift());\n+  __ li(R0, CardTable::dirty_card_val());\n+  __ stbx(R0, tmp, base);\n+}\n+\n@@ -611,0 +642,5 @@\n+\n+  \/\/ No need for post barrier if storing NULL\n+  if (is_reference_type(type) && val != noreg) {\n+    store_check(masm, base, ind_or_offs, tmp1);\n+  }\n@@ -760,0 +796,32 @@\n+void ShenandoahBarrierSetAssembler::gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                                                     Register addr, Register count, Register preserve) {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  ShenandoahBarrierSet* bs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = bs->card_table();\n+  assert_different_registers(addr, count, R0);\n+\n+  Label Lskip_loop, Lstore_loop;\n+\n+  __ sldi_(count, count, LogBytesPerHeapOop);\n+  __ beq(CCR0, Lskip_loop); \/\/ zero length\n+  __ addi(count, count, -BytesPerHeapOop);\n+  __ add(count, addr, count);\n+  \/\/ Use two shifts to clear out those low order two bits! (Cannot opt. into 1.)\n+  __ srdi(addr, addr, CardTable::card_shift());\n+  __ srdi(count, count, CardTable::card_shift());\n+  __ subf(count, addr, count);\n+  __ add_const_optimized(addr, addr, (address)ct->byte_map_base(), R0);\n+  __ addi(count, count, 1);\n+  __ li(R0, 0);\n+  __ mtctr(count);\n+  \/\/ Byte store loop\n+  __ bind(Lstore_loop);\n+  __ stb(R0, 0, addr);\n+  __ addi(addr, addr, 1);\n+  __ bdnz(Lstore_loop);\n+  __ bind(Lskip_loop);\n+}\n+\n@@ -895,1 +963,1 @@\n-  __ andi_(R12_tmp2, R12_tmp2, ShenandoahHeap::MARKING);\n+  __ andi_(R12_tmp2, R12_tmp2, ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/shenandoahBarrierSetAssembler_ppc.cpp","additions":73,"deletions":5,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -54,0 +54,4 @@\n+  void store_check(MacroAssembler* masm,\n+                   Register base, RegisterOrConstant ind_or_offs,\n+                   Register tmp);\n+\n@@ -63,0 +67,4 @@\n+  void gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                        Register addr, Register count,\n+                                        Register preserve);\n+\n@@ -103,1 +111,5 @@\n-                          Register src, Register dst, Register count, Register preserve1, Register preserve2);\n+                                  Register src, Register dst, Register count,\n+                                  Register preserve1, Register preserve2);\n+  virtual void arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                  Register dst, Register count,\n+                                  Register preserve);\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/shenandoahBarrierSetAssembler_ppc.hpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -67,1 +67,1 @@\n-        __ andi(t0, t0, ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::MARKING);\n+        __ andi(t0, t0, ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n@@ -645,1 +645,1 @@\n-  __ andi(tmp, tmp, ShenandoahHeap::MARKING);\n+  __ andi(tmp, tmp, ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shenandoah\/shenandoahBarrierSetAssembler_riscv.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -90,0 +90,2 @@\n+\n+      post_barrier(access, access.resolved_addr(), new_value.result());\n@@ -93,1 +95,8 @@\n-  return BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+\n+  LIR_Opr result =  BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+\n+  if (access.is_oop()) {\n+    post_barrier(access, access.resolved_addr(), new_value.result());\n+  }\n+\n+  return result;\n@@ -123,0 +132,1 @@\n+    post_barrier(access, access.resolved_addr(), result);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_x86.cpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -123,0 +124,24 @@\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      bool checkcast = (decorators & ARRAYCOPY_CHECKCAST) != 0;\n+      bool disjoint = (decorators & ARRAYCOPY_DISJOINT) != 0;\n+      bool obj_int = type == T_OBJECT LP64_ONLY(&& UseCompressedOops);\n+\n+      \/\/ We need to squirrel away the original element count because the\n+      \/\/ array copy assembly will destroy the value and we need it for the\n+      \/\/ card marking barrier.\n+#ifdef _LP64\n+      if (!checkcast) {\n+        if (!obj_int) {\n+          \/\/ Save count for barrier\n+          __ movptr(r11, count);\n+        } else if (disjoint) {\n+          \/\/ Save dst in r11 in the disjoint case\n+          __ movq(r11, dst);\n+        }\n+      }\n+#else\n+if (disjoint) {\n+        __ mov(rdx, dst);          \/\/ save 'to'\n+      }\n+#endif\n+    }\n@@ -154,1 +179,1 @@\n-        flags = ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::MARKING;\n+        flags = ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING;\n@@ -184,0 +209,29 @@\n+void ShenandoahBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                                       Register src, Register dst, Register count) {\n+  bool checkcast = (decorators & ARRAYCOPY_CHECKCAST) != 0;\n+  bool disjoint = (decorators & ARRAYCOPY_DISJOINT) != 0;\n+  bool obj_int = type == T_OBJECT LP64_ONLY(&& UseCompressedOops);\n+  Register tmp = rax;\n+\n+if (is_reference_type(type)) {\n+#ifdef _LP64\n+    if (!checkcast) {\n+      if (!obj_int) {\n+        \/\/ Save count for barrier\n+        count = r11;\n+      } else if (disjoint) {\n+        \/\/ Use the saved dst in the disjoint case\n+        dst = r11;\n+      }\n+    } else {\n+      tmp = rscratch1;\n+    }\n+#else\n+    if (disjoint) {\n+      __ mov(dst, rdx); \/\/ restore 'to'\n+    }\n+#endif\n+    gen_write_ref_array_post_barrier(masm, decorators, dst, count, tmp);\n+  }\n+}\n+\n@@ -227,1 +281,1 @@\n-  __ testb(gc_state, ShenandoahHeap::MARKING);\n+  __ testb(gc_state, ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n@@ -593,0 +647,45 @@\n+void ShenandoahBarrierSetAssembler::store_check(MacroAssembler* masm, Register obj) {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  \/\/ Does a store check for the oop in register obj. The content of\n+  \/\/ register obj is destroyed afterwards.\n+\n+  ShenandoahBarrierSet* ctbs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = ctbs->card_table();\n+\n+  __ shrptr(obj, CardTable::card_shift());\n+\n+  Address card_addr;\n+\n+  \/\/ The calculation for byte_map_base is as follows:\n+  \/\/ byte_map_base = _byte_map - (uintptr_t(low_bound) >> card_shift);\n+  \/\/ So this essentially converts an address to a displacement and it will\n+  \/\/ never need to be relocated. On 64bit however the value may be too\n+  \/\/ large for a 32bit displacement.\n+  intptr_t byte_map_base = (intptr_t)ct->byte_map_base();\n+  if (__ is_simm32(byte_map_base)) {\n+    card_addr = Address(noreg, obj, Address::times_1, byte_map_base);\n+  } else {\n+    \/\/ By doing it as an ExternalAddress 'byte_map_base' could be converted to a rip-relative\n+    \/\/ displacement and done in a single instruction given favorable mapping and a\n+    \/\/ smarter version of as_Address. However, 'ExternalAddress' generates a relocation\n+    \/\/ entry and that entry is not properly handled by the relocation code.\n+    AddressLiteral cardtable((address)byte_map_base, relocInfo::none);\n+    Address index(noreg, obj, Address::times_1);\n+    card_addr = __ as_Address(ArrayAddress(cardtable, index), rscratch1);\n+  }\n+\n+  int dirty = CardTable::dirty_card_val();\n+  if (UseCondCardMark) {\n+    Label L_already_dirty;\n+    __ cmpb(card_addr, dirty);\n+    __ jcc(Assembler::equal, L_already_dirty);\n+    __ movb(card_addr, dirty);\n+    __ bind(L_already_dirty);\n+  } else {\n+    __ movb(card_addr, dirty);\n+  }\n+}\n+\n@@ -634,0 +733,1 @@\n+      \/\/ XXX: store_check missing from upstream\n@@ -635,0 +735,1 @@\n+      store_check(masm, tmp1);\n@@ -830,0 +931,55 @@\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#else\n+#define BLOCK_COMMENT(str) __ block_comment(str)\n+#endif\n+\n+#define BIND(label) bind(label); BLOCK_COMMENT(#label \":\")\n+\n+#define TIMES_OOP (UseCompressedOops ? Address::times_4 : Address::times_8)\n+\n+void ShenandoahBarrierSetAssembler::gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators, Register addr, Register count, Register tmp) {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  ShenandoahBarrierSet* bs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = bs->card_table();\n+  intptr_t disp = (intptr_t) ct->byte_map_base();\n+\n+  Label L_loop, L_done;\n+  const Register end = count;\n+  assert_different_registers(addr, end);\n+\n+  __ testl(count, count);\n+  __ jcc(Assembler::zero, L_done); \/\/ zero count - nothing to do\n+\n+\n+#ifdef _LP64\n+  __ leaq(end, Address(addr, count, TIMES_OOP, 0));  \/\/ end == addr+count*oop_size\n+  __ subptr(end, BytesPerHeapOop); \/\/ end - 1 to make inclusive\n+  __ shrptr(addr, CardTable::card_shift());\n+  __ shrptr(end, CardTable::card_shift());\n+  __ subptr(end, addr); \/\/ end --> cards count\n+\n+  __ mov64(tmp, disp);\n+  __ addptr(addr, tmp);\n+__ BIND(L_loop);\n+  __ movb(Address(addr, count, Address::times_1), 0);\n+  __ decrement(count);\n+  __ jcc(Assembler::greaterEqual, L_loop);\n+#else\n+  __ lea(end,  Address(addr, count, Address::times_ptr, -wordSize));\n+  __ shrptr(addr, CardTable::card_shift());\n+  __ shrptr(end,   CardTable::card_shift());\n+  __ subptr(end, addr); \/\/ end --> count\n+__ BIND(L_loop);\n+  Address cardtable(addr, count, Address::times_1, disp);\n+  __ movb(cardtable, 0);\n+  __ decrement(count);\n+  __ jcc(Assembler::greaterEqual, L_loop);\n+#endif\n+\n+__ BIND(L_done);\n+}\n+\n@@ -945,1 +1101,1 @@\n-  __ testb(gc_state, ShenandoahHeap::MARKING);\n+  __ testb(gc_state, ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.cpp","additions":159,"deletions":3,"binary":false,"changes":162,"status":"modified"},{"patch":"@@ -61,0 +61,4 @@\n+  void store_check(MacroAssembler* masm, Register obj);\n+\n+  void gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators, Register addr, Register count, Register tmp);\n+\n@@ -77,0 +81,2 @@\n+  virtual void arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                  Register src, Register dst, Register count);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"logging\/logStream.hpp\"\n@@ -40,1 +41,1 @@\n-AgeTable::AgeTable(bool global) {\n+AgeTable::AgeTable(bool global) : _use_perf_data(UsePerfData && global) {\n@@ -44,1 +45,1 @@\n-  if (UsePerfData && global) {\n+  if (_use_perf_data) {\n@@ -73,1 +74,1 @@\n-void AgeTable::merge(AgeTable* subTable) {\n+void AgeTable::merge(const AgeTable* subTable) {\n@@ -108,19 +109,4 @@\n-  if (log_is_enabled(Trace, gc, age) || UsePerfData || AgeTableTracer::is_tenuring_distribution_event_enabled()) {\n-    log_trace(gc, age)(\"Age table with threshold %u (max threshold \" UINTX_FORMAT \")\",\n-                       tenuring_threshold, MaxTenuringThreshold);\n-\n-    size_t total = 0;\n-    uint age = 1;\n-    while (age < table_size) {\n-      size_t wordSize = sizes[age];\n-      total += wordSize;\n-      if (wordSize > 0) {\n-        log_trace(gc, age)(\"- age %3u: \" SIZE_FORMAT_W(10) \" bytes, \" SIZE_FORMAT_W(10) \" total\",\n-                            age, wordSize * oopSize, total * oopSize);\n-      }\n-      AgeTableTracer::send_tenuring_distribution_event(age, wordSize * oopSize);\n-      if (UsePerfData) {\n-        _perf_sizes[age]->set_value(wordSize * oopSize);\n-      }\n-      age++;\n-    }\n+  LogTarget(Trace, gc, age) lt;\n+  if (lt.is_enabled() || _use_perf_data || AgeTableTracer::is_tenuring_distribution_event_enabled()) {\n+    LogStream st(lt);\n+    print_on(&st, tenuring_threshold);\n@@ -130,0 +116,20 @@\n+void AgeTable::print_on(outputStream* st, uint tenuring_threshold) {\n+  st->print_cr(\"Age table with threshold %u (max threshold \" UINTX_FORMAT \")\",\n+           tenuring_threshold, MaxTenuringThreshold);\n+\n+  size_t total = 0;\n+  uint age = 1;\n+  while (age < table_size) {\n+    size_t word_size = sizes[age];\n+    total += word_size;\n+    if (word_size > 0) {\n+      st->print_cr(\"- age %3u: \" SIZE_FORMAT_W(10) \" bytes, \" SIZE_FORMAT_W(10) \" total\",\n+                   age, word_size * oopSize, total * oopSize);\n+    }\n+    AgeTableTracer::send_tenuring_distribution_event(age, word_size * oopSize);\n+    if (_use_perf_data) {\n+      _perf_sizes[age]->set_value(word_size * oopSize);\n+    }\n+    age++;\n+  }\n+}\n\\ No newline at end of file\n","filename":"src\/hotspot\/share\/gc\/shared\/ageTable.cpp","additions":28,"deletions":22,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -66,1 +66,1 @@\n-  void merge(AgeTable* subTable);\n+  void merge(const AgeTable* subTable);\n@@ -71,0 +71,1 @@\n+  void print_on(outputStream* st, uint tenuring_threshold);\n@@ -73,1 +74,1 @@\n-\n+  bool _use_perf_data;\n","filename":"src\/hotspot\/share\/gc\/shared\/ageTable.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -46,1 +46,1 @@\n-  assert(UseG1GC || UseParallelGC || UseSerialGC,\n+  assert(UseG1GC || UseParallelGC || UseSerialGC || UseShenandoahGC,\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTable.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -44,1 +45,10 @@\n-  if (UseZGC || UseShenandoahGC) {\n+  if (UseShenandoahGC) {\n+#if INCLUDE_SHENANDOAHGC\n+    if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+      return Shenandoah;\n+    }\n+#endif\n+    return NA;\n+  }\n+\n+  if (UseZGC) {\n","filename":"src\/hotspot\/share\/gc\/shared\/gcConfiguration.cpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -141,0 +141,4 @@\n+\n+  HeapWord* top() {\n+    return _top;\n+  }\n","filename":"src\/hotspot\/share\/gc\/shared\/plab.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -196,0 +197,10 @@\n+\n+  if (access.is_oop()) {\n+    DecoratorSet decorators = access.decorators();\n+    bool is_array = (decorators & IS_ARRAY) != 0;\n+    bool on_anonymous = (decorators & ON_UNKNOWN_OOP_REF) != 0;\n+\n+    bool precise = is_array || on_anonymous;\n+    LIR_Opr post_addr = precise ? access.resolved_addr() : access.base().opr();\n+    post_barrier(access, post_addr, value);\n+  }\n@@ -294,0 +305,61 @@\n+\n+void ShenandoahBarrierSetC1::post_barrier(LIRAccess& access, LIR_Opr addr, LIR_Opr new_val) {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  DecoratorSet decorators = access.decorators();\n+  LIRGenerator* gen = access.gen();\n+  bool in_heap = (decorators & IN_HEAP) != 0;\n+  if (!in_heap) {\n+    return;\n+  }\n+\n+  BarrierSet* bs = BarrierSet::barrier_set();\n+  ShenandoahBarrierSet* ctbs = barrier_set_cast<ShenandoahBarrierSet>(bs);\n+  CardTable* ct = ctbs->card_table();\n+  LIR_Const* card_table_base = new LIR_Const(ct->byte_map_base());\n+  if (addr->is_address()) {\n+    LIR_Address* address = addr->as_address_ptr();\n+    \/\/ ptr cannot be an object because we use this barrier for array card marks\n+    \/\/ and addr can point in the middle of an array.\n+    LIR_Opr ptr = gen->new_pointer_register();\n+    if (!address->index()->is_valid() && address->disp() == 0) {\n+      __ move(address->base(), ptr);\n+    } else {\n+      assert(address->disp() != max_jint, \"lea doesn't support patched addresses!\");\n+      __ leal(addr, ptr);\n+    }\n+    addr = ptr;\n+  }\n+  assert(addr->is_register(), \"must be a register at this point\");\n+\n+  LIR_Opr tmp = gen->new_pointer_register();\n+  if (two_operand_lir_form) {\n+    __ move(addr, tmp);\n+    __ unsigned_shift_right(tmp, CardTable::card_shift(), tmp);\n+  } else {\n+    __ unsigned_shift_right(addr, CardTable::card_shift(), tmp);\n+  }\n+\n+  LIR_Address* card_addr;\n+  if (gen->can_inline_as_constant(card_table_base)) {\n+    card_addr = new LIR_Address(tmp, card_table_base->as_jint(), T_BYTE);\n+  } else {\n+    card_addr = new LIR_Address(tmp, gen->load_constant(card_table_base), T_BYTE);\n+  }\n+\n+  LIR_Opr dirty = LIR_OprFact::intConst(CardTable::dirty_card_val());\n+  if (UseCondCardMark) {\n+    LIR_Opr cur_value = gen->new_register(T_INT);\n+    __ move(card_addr, cur_value);\n+\n+    LabelObj* L_already_dirty = new LabelObj();\n+    __ cmp(lir_cond_equal, cur_value, dirty);\n+    __ branch(lir_cond_equal, L_already_dirty->label());\n+    __ move(dirty, card_addr);\n+    __ branch_destination(L_already_dirty->label());\n+  } else {\n+    __ move(dirty, card_addr);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1.cpp","additions":72,"deletions":0,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -247,0 +247,2 @@\n+  void post_barrier(LIRAccess& access, LIR_Opr addr, LIR_Opr new_val);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -243,1 +244,1 @@\n-  marking = __ AndI(ld, __ ConI(ShenandoahHeap::MARKING));\n+  marking = __ AndI(ld, __ ConI(ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING));\n@@ -324,1 +325,1 @@\n-      cmpx->in(1)->in(2) == phase->intcon(ShenandoahHeap::MARKING)) {\n+      cmpx->in(1)->in(2) == phase->intcon(ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING)) {\n@@ -453,0 +454,92 @@\n+Node* ShenandoahBarrierSetC2::byte_map_base_node(GraphKit* kit) const {\n+  BarrierSet* bs = BarrierSet::barrier_set();\n+  ShenandoahBarrierSet* ctbs = barrier_set_cast<ShenandoahBarrierSet>(bs);\n+  CardTable::CardValue* card_table_base = ctbs->card_table()->byte_map_base();\n+  if (card_table_base != nullptr) {\n+    return kit->makecon(TypeRawPtr::make((address)card_table_base));\n+  } else {\n+    return kit->null();\n+  }\n+}\n+\n+void ShenandoahBarrierSetC2::post_barrier(GraphKit* kit,\n+                                          Node* ctl,\n+                                          Node* oop_store,\n+                                          Node* obj,\n+                                          Node* adr,\n+                                          uint  adr_idx,\n+                                          Node* val,\n+                                          BasicType bt,\n+                                          bool use_precise) const {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  ShenandoahBarrierSet* ctbs = barrier_set_cast<ShenandoahBarrierSet>(BarrierSet::barrier_set());\n+  CardTable* ct = ctbs->card_table();\n+  \/\/ No store check needed if we're storing a nullptr or an old object\n+  \/\/ (latter case is probably a string constant). The concurrent\n+  \/\/ mark sweep garbage collector, however, needs to have all nonNull\n+  \/\/ oop updates flagged via card-marks.\n+  if (val != nullptr && val->is_Con()) {\n+    \/\/ must be either an oop or NULL\n+    const Type* t = val->bottom_type();\n+    if (t == TypePtr::NULL_PTR || t == Type::TOP)\n+      \/\/ stores of null never (?) need barriers\n+      return;\n+  }\n+\n+  if (ReduceInitialCardMarks && obj == kit->just_allocated_object(kit->control())) {\n+    \/\/ We can skip marks on a freshly-allocated object in Eden.\n+    \/\/ Keep this code in sync with new_deferred_store_barrier() in runtime.cpp.\n+    \/\/ That routine informs GC to take appropriate compensating steps,\n+    \/\/ upon a slow-path allocation, so as to make this card-mark\n+    \/\/ elision safe.\n+    return;\n+  }\n+\n+  if (!use_precise) {\n+    \/\/ All card marks for a (non-array) instance are in one place:\n+    adr = obj;\n+  }\n+  \/\/ (Else it's an array (or unknown), and we want more precise card marks.)\n+  assert(adr != nullptr, \"\");\n+\n+  IdealKit ideal(kit, true);\n+\n+  \/\/ Convert the pointer to an int prior to doing math on it\n+  Node* cast = __ CastPX(__ ctrl(), adr);\n+\n+  \/\/ Divide by card size\n+  Node* card_offset = __ URShiftX( cast, __ ConI(CardTable::card_shift()) );\n+\n+  \/\/ Combine card table base and card offset\n+  Node* card_adr = __ AddP(__ top(), byte_map_base_node(kit), card_offset );\n+\n+  \/\/ Get the alias_index for raw card-mark memory\n+  int adr_type = Compile::AliasIdxRaw;\n+  Node*   zero = __ ConI(0); \/\/ Dirty card value\n+\n+  if (UseCondCardMark) {\n+    \/\/ The classic GC reference write barrier is typically implemented\n+    \/\/ as a store into the global card mark table.  Unfortunately\n+    \/\/ unconditional stores can result in false sharing and excessive\n+    \/\/ coherence traffic as well as false transactional aborts.\n+    \/\/ UseCondCardMark enables MP \"polite\" conditional card mark\n+    \/\/ stores.  In theory we could relax the load from ctrl() to\n+    \/\/ no_ctrl, but that doesn't buy much latitude.\n+    Node* card_val = __ load( __ ctrl(), card_adr, TypeInt::BYTE, T_BYTE, adr_type);\n+    __ if_then(card_val, BoolTest::ne, zero);\n+  }\n+\n+  \/\/ Smash zero into card\n+  __ store(__ ctrl(), card_adr, zero, T_BYTE, adr_type, MemNode::unordered);\n+\n+  if (UseCondCardMark) {\n+    __ end_if();\n+  }\n+\n+  \/\/ Final sync IdealKit and GraphKit.\n+  kit->final_sync(ideal);\n+}\n+\n@@ -516,0 +609,8 @@\n+\n+    Node* result = BarrierSetC2::store_at_resolved(access, val);\n+\n+    bool anonymous = (decorators & ON_UNKNOWN_OOP_REF) != 0;\n+    bool is_array = (decorators & IS_ARRAY) != 0;\n+    bool use_precise = is_array || anonymous;\n+    post_barrier(kit, kit->control(), access.raw_access(), access.base(), adr, adr_idx, val.node(), access.type(), use_precise);\n+    return result;\n@@ -526,0 +627,1 @@\n+    return BarrierSetC2::store_at_resolved(access, val);\n@@ -527,1 +629,0 @@\n-  return BarrierSetC2::store_at_resolved(access, val);\n@@ -598,1 +699,1 @@\n-                                                   Node* new_val, const Type* value_type) const {\n+                                                             Node* new_val, const Type* value_type) const {\n@@ -640,0 +741,1 @@\n+    post_barrier(kit, kit->control(), access.raw_access(), access.base(), access.addr().node(), access.alias_idx(), new_val, T_OBJECT, true);\n@@ -695,0 +797,2 @@\n+    post_barrier(kit, kit->control(), access.raw_access(), access.base(),\n+                 access.addr().node(), access.alias_idx(), new_val, T_OBJECT, true);\n@@ -711,0 +815,2 @@\n+    post_barrier(kit, kit->control(), access.raw_access(), access.base(),\n+                 access.addr().node(), access.alias_idx(), val, T_OBJECT, true);\n@@ -798,1 +904,1 @@\n-        }\n+    }\n@@ -841,1 +947,1 @@\n-      flags |= ShenandoahHeap::MARKING;\n+      flags |= ShenandoahHeap::YOUNG_MARKING;\n@@ -909,3 +1015,20 @@\n-void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* n) const {\n-  if (is_shenandoah_wb_pre_call(n)) {\n-    shenandoah_eliminate_wb_pre(n, &macro->igvn());\n+void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+  if (is_shenandoah_wb_pre_call(node)) {\n+    shenandoah_eliminate_wb_pre(node, &macro->igvn());\n+  }\n+  if (node->Opcode() == Op_CastP2X && ShenandoahHeap::heap()->mode()->is_generational()) {\n+    assert(node->Opcode() == Op_CastP2X, \"ConvP2XNode required\");\n+     Node *shift = node->unique_out();\n+     Node *addp = shift->unique_out();\n+     for (DUIterator_Last jmin, j = addp->last_outs(jmin); j >= jmin; --j) {\n+       Node *mem = addp->last_out(j);\n+       if (UseCondCardMark && mem->is_Load()) {\n+         assert(mem->Opcode() == Op_LoadB, \"unexpected code shape\");\n+         \/\/ The load is checking if the card has been written so\n+         \/\/ replace it with zero to fold the test.\n+         macro->replace_node(mem, macro->intcon(0));\n+         continue;\n+       }\n+       assert(mem->is_Store(), \"store required\");\n+       macro->replace_node(mem, mem->in(MemNode::Memory));\n+     }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.cpp","additions":132,"deletions":9,"binary":false,"changes":141,"status":"modified"},{"patch":"@@ -78,0 +78,12 @@\n+  Node* byte_map_base_node(GraphKit* kit) const;\n+\n+  void post_barrier(GraphKit* kit,\n+                    Node* ctl,\n+                    Node* store,\n+                    Node* obj,\n+                    Node* adr,\n+                    uint adr_idx,\n+                    Node* val,\n+                    BasicType bt,\n+                    bool use_precise) const;\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1486,1 +1486,1 @@\n-    test_gc_state(ctrl, raw_mem, heap_stable_ctrl, phase, ShenandoahHeap::MARKING);\n+    test_gc_state(ctrl, raw_mem, heap_stable_ctrl, phase, (ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,2 +30,2 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n-#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -57,2 +57,4 @@\n-ShenandoahAdaptiveHeuristics::ShenandoahAdaptiveHeuristics() :\n-  ShenandoahHeuristics(),\n+const uint ShenandoahAdaptiveHeuristics::MINIMUM_RESIZE_INTERVAL = 10;\n+\n+ShenandoahAdaptiveHeuristics::ShenandoahAdaptiveHeuristics(ShenandoahGeneration* generation) :\n+  ShenandoahHeuristics(generation),\n@@ -61,1 +63,2 @@\n-  _last_trigger(OTHER) { }\n+  _last_trigger(OTHER),\n+  _available(Moving_Average_Samples, ShenandoahAdaptiveDecayFactor) { }\n@@ -69,0 +72,2 @@\n+  size_t ignore_threshold = ShenandoahHeapRegion::region_size_bytes() * ShenandoahIgnoreGarbageThreshold \/ 100;\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -87,4 +92,15 @@\n-  size_t capacity    = ShenandoahHeap::heap()->soft_max_capacity();\n-  size_t max_cset    = (size_t)((1.0 * capacity \/ 100 * ShenandoahEvacReserve) \/ ShenandoahEvacWaste);\n-  size_t free_target = (capacity \/ 100 * ShenandoahMinFreeThreshold) + max_cset;\n-  size_t min_garbage = (free_target > actual_free ? (free_target - actual_free) : 0);\n+  \/\/ In generational mode, the sort order within the data array is not strictly descending amounts of garbage.  In\n+  \/\/ particular, regions that have reached tenure age will be sorted into this array before younger regions that contain\n+  \/\/ more garbage.  This represents one of the reasons why we keep looking at regions even after we decide, for example,\n+  \/\/ to exclude one of the regions because it might require evacuation of too much live data.\n+  bool is_generational = heap->mode()->is_generational();\n+  bool is_global = (_generation->generation_mode() == GLOBAL);\n+  size_t capacity = heap->young_generation()->max_capacity();\n+\n+  \/\/ cur_young_garbage represents the amount of memory to be reclaimed from young-gen.  In the case that live objects\n+  \/\/ are known to be promoted out of young-gen, we count this as cur_young_garbage because this memory is reclaimed\n+  \/\/ from young-gen and becomes available to serve future young-gen allocation requests.\n+  size_t cur_young_garbage = 0;\n+\n+  \/\/ Better select garbage-first regions\n+  QuickSort::sort<RegionData>(data, (int)size, compare_by_garbage, false);\n@@ -92,2 +108,109 @@\n-  log_info(gc, ergo)(\"Adaptive CSet Selection. Target Free: \" SIZE_FORMAT \"%s, Actual Free: \"\n-                     SIZE_FORMAT \"%s, Max CSet: \" SIZE_FORMAT \"%s, Min Garbage: \" SIZE_FORMAT \"%s\",\n+  if (is_generational) {\n+    if (is_global) {\n+      size_t max_young_cset    = (size_t) (heap->get_young_evac_reserve() \/ ShenandoahEvacWaste);\n+      size_t young_cur_cset = 0;\n+      size_t max_old_cset    = (size_t) (heap->get_old_evac_reserve() \/ ShenandoahEvacWaste);\n+      size_t old_cur_cset = 0;\n+      size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_young_cset;\n+      size_t min_garbage = (free_target > actual_free) ? (free_target - actual_free) : 0;\n+\n+      log_info(gc, ergo)(\"Adaptive CSet Selection for GLOBAL. Max Young Evacuation: \" SIZE_FORMAT\n+                         \"%s, Max Old Evacuation: \" SIZE_FORMAT \"%s, Actual Free: \" SIZE_FORMAT \"%s.\",\n+                         byte_size_in_proper_unit(max_young_cset),    proper_unit_for_byte_size(max_young_cset),\n+                         byte_size_in_proper_unit(max_old_cset),    proper_unit_for_byte_size(max_old_cset),\n+                         byte_size_in_proper_unit(actual_free), proper_unit_for_byte_size(actual_free));\n+\n+      for (size_t idx = 0; idx < size; idx++) {\n+        ShenandoahHeapRegion* r = data[idx]._region;\n+        bool add_region = false;\n+        if (r->is_old()) {\n+          size_t new_cset = old_cur_cset + r->get_live_data_bytes();\n+          if ((new_cset <= max_old_cset) && (r->garbage() > garbage_threshold)) {\n+            add_region = true;\n+            old_cur_cset = new_cset;\n+          }\n+        } else if (cset->is_preselected(r->index())) {\n+          assert(r->age() >= InitialTenuringThreshold, \"Preselected regions must have tenure age\");\n+          \/\/ Entire region will be promoted, This region does not impact young-gen or old-gen evacuation reserve.\n+          \/\/ This region has been pre-selected and its impact on promotion reserve is already accounted for.\n+          add_region = true;\n+          \/\/ r->used() is r->garbage() + r->get_live_data_bytes()\n+          \/\/ Since all live data in this region is being evacuated from young-gen, it is as if this memory\n+          \/\/ is garbage insofar as young-gen is concerned.  Counting this as garbage reduces the need to\n+          \/\/ reclaim highly utilized young-gen regions just for the sake of finding min_garbage to reclaim\n+          \/\/ within youn-gen memory.\n+          cur_young_garbage += r->used();\n+        } else if (r->age() < InitialTenuringThreshold) {\n+          size_t new_cset = young_cur_cset + r->get_live_data_bytes();\n+          size_t region_garbage = r->garbage();\n+          size_t new_garbage = cur_young_garbage + region_garbage;\n+          bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n+          if ((new_cset <= max_young_cset) && (add_regardless || (region_garbage > garbage_threshold))) {\n+            add_region = true;\n+            young_cur_cset = new_cset;\n+            cur_young_garbage = new_garbage;\n+          }\n+        }\n+        \/\/ Note that we do not add aged regions if they were not pre-selected.  The reason they were not preselected\n+        \/\/ is because there is not sufficient room in old-gen to hold their to-be-promoted live objects.\n+\n+        if (add_region) {\n+          cset->add_region(r);\n+        }\n+      }\n+    } else {\n+      \/\/ This is young-gen collection or a mixed evacuation.  If this is mixed evacuation, the old-gen candidate regions\n+      \/\/ have already been added.\n+      size_t max_cset    = (size_t) (heap->get_young_evac_reserve() \/ ShenandoahEvacWaste);\n+      size_t cur_cset = 0;\n+      size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_cset;\n+      size_t min_garbage = (free_target > actual_free) ? (free_target - actual_free) : 0;\n+\n+      log_info(gc, ergo)(\"Adaptive CSet Selection for YOUNG. Max Evacuation: \" SIZE_FORMAT \"%s, Actual Free: \" SIZE_FORMAT \"%s.\",\n+                         byte_size_in_proper_unit(max_cset),    proper_unit_for_byte_size(max_cset),\n+                         byte_size_in_proper_unit(actual_free), proper_unit_for_byte_size(actual_free));\n+\n+      for (size_t idx = 0; idx < size; idx++) {\n+        ShenandoahHeapRegion* r = data[idx]._region;\n+        bool add_region = false;\n+\n+        if (!r->is_old()) {\n+          if (cset->is_preselected(r->index())) {\n+            assert(r->age() >= InitialTenuringThreshold, \"Preselected regions must have tenure age\");\n+            \/\/ Entire region will be promoted, This region does not impact young-gen evacuation reserve.  Memory has already\n+            \/\/ been set aside to hold evacuation results as advance_promotion_reserve.\n+            add_region = true;\n+            \/\/ Since all live data in this region is being evacuated from young-gen, it is as if this memory\n+            \/\/ is garbage insofar as young-gen is concerned.  Counting this as garbage reduces the need to\n+            \/\/ reclaim highly utilized young-gen regions just for the sake of finding min_garbage to reclaim\n+            \/\/ within youn-gen memory\n+            cur_young_garbage += r->get_live_data_bytes();\n+          } else if  (r->age() < InitialTenuringThreshold) {\n+            size_t new_cset = cur_cset + r->get_live_data_bytes();\n+            size_t region_garbage = r->garbage();\n+            size_t new_garbage = cur_young_garbage + region_garbage;\n+            bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n+            if ((new_cset <= max_cset) && (add_regardless || (region_garbage > garbage_threshold))) {\n+              add_region = true;\n+              cur_cset = new_cset;\n+              cur_young_garbage = new_garbage;\n+            }\n+          }\n+          \/\/ Note that we do not add aged regions if they were not pre-selected.  The reason they were not preselected\n+          \/\/ is because there is not sufficient room in old-gen to hold their to-be-promoted live objects.\n+\n+          if (add_region) {\n+            cset->add_region(r);\n+          }\n+        }\n+      }\n+    }\n+  } else {\n+    \/\/ Traditional Shenandoah (non-generational)\n+    size_t capacity    = ShenandoahHeap::heap()->soft_max_capacity();\n+    size_t max_cset    = (size_t)((1.0 * capacity \/ 100 * ShenandoahEvacReserve) \/ ShenandoahEvacWaste);\n+    size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_cset;\n+    size_t min_garbage = (free_target > actual_free) ? (free_target - actual_free) : 0;\n+\n+    log_info(gc, ergo)(\"Adaptive CSet Selection. Target Free: \" SIZE_FORMAT \"%s, Actual Free: \"\n+                     SIZE_FORMAT \"%s, Max Evacuation: \" SIZE_FORMAT \"%s, Min Garbage: \" SIZE_FORMAT \"%s\",\n@@ -99,5 +222,2 @@\n-  \/\/ Better select garbage-first regions\n-  QuickSort::sort<RegionData>(data, (int)size, compare_by_garbage, false);\n-\n-  size_t cur_cset = 0;\n-  size_t cur_garbage = 0;\n+    size_t cur_cset = 0;\n+    size_t cur_garbage = 0;\n@@ -105,2 +225,2 @@\n-  for (size_t idx = 0; idx < size; idx++) {\n-    ShenandoahHeapRegion* r = data[idx]._region;\n+    for (size_t idx = 0; idx < size; idx++) {\n+      ShenandoahHeapRegion* r = data[idx]._region;\n@@ -108,2 +228,2 @@\n-    size_t new_cset    = cur_cset + r->get_live_data_bytes();\n-    size_t new_garbage = cur_garbage + r->garbage();\n+      size_t new_cset    = cur_cset + r->get_live_data_bytes();\n+      size_t new_garbage = cur_garbage + r->garbage();\n@@ -111,3 +231,3 @@\n-    if (new_cset > max_cset) {\n-      break;\n-    }\n+      if (new_cset > max_cset) {\n+        break;\n+      }\n@@ -115,4 +235,5 @@\n-    if ((new_garbage < min_garbage) || (r->garbage() > garbage_threshold)) {\n-      cset->add_region(r);\n-      cur_cset = new_cset;\n-      cur_garbage = new_garbage;\n+      if ((new_garbage < min_garbage) || (r->garbage() > garbage_threshold)) {\n+        cset->add_region(r);\n+        cur_cset = new_cset;\n+        cur_garbage = new_garbage;\n+      }\n@@ -126,0 +247,1 @@\n+  ++_cycles_since_last_resize;\n@@ -128,2 +250,2 @@\n-void ShenandoahAdaptiveHeuristics::record_success_concurrent() {\n-  ShenandoahHeuristics::record_success_concurrent();\n+void ShenandoahAdaptiveHeuristics::record_success_concurrent(bool abbreviated) {\n+  ShenandoahHeuristics::record_success_concurrent(abbreviated);\n@@ -131,1 +253,1 @@\n-  size_t available = ShenandoahHeap::heap()->free_set()->available();\n+  size_t available = MIN2(_generation->available(), ShenandoahHeap::heap()->free_set()->available());\n@@ -133,1 +255,0 @@\n-  _available.add(available);\n@@ -135,2 +256,9 @@\n-  if (_available.sd() > 0) {\n-    z_score = (available - _available.avg()) \/ _available.sd();\n+  double available_sd = _available.sd();\n+  if (available_sd > 0) {\n+    double available_avg = _available.avg();\n+    z_score = (double(available) - available_avg) \/ available_sd;\n+    log_debug(gc, ergo)(\"%s Available: \" SIZE_FORMAT \" %sB, z-score=%.3f. Average available: %.1f %sB +\/- %.1f %sB.\",\n+                        _generation->name(),\n+                        byte_size_in_proper_unit(available), proper_unit_for_byte_size(available), z_score,\n+                        byte_size_in_proper_unit(available_avg), proper_unit_for_byte_size(available_avg),\n+                        byte_size_in_proper_unit(available_sd), proper_unit_for_byte_size(available_sd));\n@@ -139,5 +267,1 @@\n-  log_debug(gc, ergo)(\"Available: \" SIZE_FORMAT \" %sB, z-score=%.3f. Average available: %.1f %sB +\/- %.1f %sB.\",\n-                      byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n-                      z_score,\n-                      byte_size_in_proper_unit(_available.avg()), proper_unit_for_byte_size(_available.avg()),\n-                      byte_size_in_proper_unit(_available.sd()), proper_unit_for_byte_size(_available.sd()));\n+  _available.add(double(available));\n@@ -199,9 +323,18 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  size_t max_capacity = heap->max_capacity();\n-  size_t capacity = heap->soft_max_capacity();\n-  size_t available = heap->free_set()->available();\n-  size_t allocated = heap->bytes_allocated_since_gc_start();\n-\n-  \/\/ Make sure the code below treats available without the soft tail.\n-  size_t soft_tail = max_capacity - capacity;\n-  available = (available > soft_tail) ? (available - soft_tail) : 0;\n+  size_t max_capacity = _generation->max_capacity();\n+  size_t capacity = _generation->soft_max_capacity();\n+  size_t available = _generation->available();\n+  size_t allocated = _generation->bytes_allocated_since_gc_start();\n+\n+  log_debug(gc)(\"should_start_gc (%s)? available: \" SIZE_FORMAT \", soft_max_capacity: \" SIZE_FORMAT\n+                \", max_capacity: \" SIZE_FORMAT \", allocated: \" SIZE_FORMAT,\n+                _generation->name(), available, capacity, max_capacity, allocated);\n+\n+  \/\/ The collector reserve may eat into what the mutator is allowed to use. Make sure we are looking\n+  \/\/ at what is available to the mutator when deciding whether to start a GC.\n+  size_t usable = ShenandoahHeap::heap()->free_set()->available();\n+  if (usable < available) {\n+    log_debug(gc)(\"Usable (\" SIZE_FORMAT \"%s) is less than available (\" SIZE_FORMAT \"%s)\",\n+                  byte_size_in_proper_unit(usable), proper_unit_for_byte_size(usable),\n+                  byte_size_in_proper_unit(available), proper_unit_for_byte_size(available));\n+    available = usable;\n+  }\n@@ -213,1 +346,2 @@\n-  size_t min_threshold = capacity \/ 100 * ShenandoahMinFreeThreshold;\n+  size_t min_threshold = min_free_threshold();\n+\n@@ -215,4 +349,5 @@\n-    log_info(gc)(\"Trigger: Free (\" SIZE_FORMAT \"%s) is below minimum threshold (\" SIZE_FORMAT \"%s)\",\n-                 byte_size_in_proper_unit(available),     proper_unit_for_byte_size(available),\n-                 byte_size_in_proper_unit(min_threshold), proper_unit_for_byte_size(min_threshold));\n-    return true;\n+    log_info(gc)(\"Trigger (%s): Free (\" SIZE_FORMAT \"%s) is below minimum threshold (\" SIZE_FORMAT \"%s)\",\n+                 _generation->name(),\n+                 byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n+                 byte_size_in_proper_unit(min_threshold),       proper_unit_for_byte_size(min_threshold));\n+    return resize_and_evaluate();\n@@ -221,0 +356,1 @@\n+  \/\/ Check if we need to learn a bit about the application\n@@ -225,4 +361,4 @@\n-      log_info(gc)(\"Trigger: Learning \" SIZE_FORMAT \" of \" SIZE_FORMAT \". Free (\" SIZE_FORMAT \"%s) is below initial threshold (\" SIZE_FORMAT \"%s)\",\n-                   _gc_times_learned + 1, max_learn,\n-                   byte_size_in_proper_unit(available),      proper_unit_for_byte_size(available),\n-                   byte_size_in_proper_unit(init_threshold), proper_unit_for_byte_size(init_threshold));\n+      log_info(gc)(\"Trigger (%s): Learning \" SIZE_FORMAT \" of \" SIZE_FORMAT \". Free (\" SIZE_FORMAT \"%s) is below initial threshold (\" SIZE_FORMAT \"%s)\",\n+                   _generation->name(), _gc_times_learned + 1, max_learn,\n+                   byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n+                   byte_size_in_proper_unit(init_threshold),      proper_unit_for_byte_size(init_threshold));\n@@ -233,0 +369,39 @@\n+  \/\/  Rationale:\n+  \/\/    The idea is that there is an average allocation rate and there are occasional abnormal bursts (or spikes) of\n+  \/\/    allocations that exceed the average allocation rate.  What do these spikes look like?\n+  \/\/\n+  \/\/    1. At certain phase changes, we may discard large amounts of data and replace it with large numbers of newly\n+  \/\/       allocated objects.  This \"spike\" looks more like a phase change.  We were in steady state at M bytes\/sec\n+  \/\/       allocation rate and now we're in a \"reinitialization phase\" that looks like N bytes\/sec.  We need the \"spike\"\n+  \/\/       accomodation to give us enough runway to recalibrate our \"average allocation rate\".\n+  \/\/\n+  \/\/   2. The typical workload changes.  \"Suddenly\", our typical workload of N TPS increases to N+delta TPS.  This means\n+  \/\/       our average allocation rate needs to be adjusted.  Once again, we need the \"spike\" accomodation to give us\n+  \/\/       enough runway to recalibrate our \"average allocation rate\".\n+  \/\/\n+  \/\/    3. Though there is an \"average\" allocation rate, a given workload's demand for allocation may be very bursty.  We\n+  \/\/       allocate a bunch of LABs during the 5 ms that follow completion of a GC, then we perform no more allocations for\n+  \/\/       the next 150 ms.  It seems we want the \"spike\" to represent the maximum divergence from average within the\n+  \/\/       period of time between consecutive evaluation of the should_start_gc() service.  Here's the thinking:\n+  \/\/\n+  \/\/       a) Between now and the next time I ask whether should_start_gc(), we might experience a spike representing\n+  \/\/          the anticipated burst of allocations.  If that would put us over budget, then we should start GC immediately.\n+  \/\/       b) Between now and the anticipated depletion of allocation pool, there may be two or more bursts of allocations.\n+  \/\/          If there are more than one of these bursts, we can \"approximate\" that these will be separated by spans of\n+  \/\/          time with very little or no allocations so the \"average\" allocation rate should be a suitable approximation\n+  \/\/          of how this will behave.\n+  \/\/\n+  \/\/    For cases 1 and 2, we need to \"quickly\" recalibrate the average allocation rate whenever we detect a change\n+  \/\/    in operation mode.  We want some way to decide that the average rate has changed.  Make average allocation rate\n+  \/\/    computations an independent effort.\n+\n+\n+  \/\/ TODO: Account for inherent delays in responding to GC triggers\n+  \/\/  1. It has been observed that delays of 200 ms or greater are common between the moment we return true from should_start_gc()\n+  \/\/     and the moment at which we begin execution of the concurrent reset phase.  Add this time into the calculation of\n+  \/\/     avg_cycle_time below.  (What is \"this time\"?  Perhaps we should remember recent history of this delay for the\n+  \/\/     running workload and use the maximum delay recently seen for \"this time\".)\n+  \/\/  2. The frequency of inquiries to should_start_gc() is adaptive, ranging between ShenandoahControlIntervalMin and\n+  \/\/     ShenandoahControlIntervalMax.  The current control interval (or the max control interval) should also be added into\n+  \/\/     the calculation of avg_cycle_time below.\n+\n@@ -234,1 +409,1 @@\n-  \/\/   1. Some space to absorb allocation spikes\n+  \/\/   1. Some space to absorb allocation spikes (ShenandoahAllocSpikeFactor)\n@@ -237,1 +412,0 @@\n-\n@@ -241,1 +415,0 @@\n-  allocation_headroom -= MIN2(allocation_headroom, spike_headroom);\n@@ -243,0 +416,3 @@\n+  allocation_headroom -= MIN2(allocation_headroom, spike_headroom);\n+\n+  double avg_cycle_time = _gc_cycle_time_history->davg() + (_margin_of_error_sd * _gc_cycle_time_history->dsd());\n@@ -244,1 +420,0 @@\n-  double avg_cycle_time = _gc_time_history->davg() + (_margin_of_error_sd * _gc_time_history->dsd());\n@@ -246,0 +421,3 @@\n+  log_debug(gc)(\"%s: average GC time: %.2f ms, allocation rate: %.0f %s\/s\",\n+    _generation->name(), avg_cycle_time * 1000, byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate));\n+\n@@ -247,2 +425,3 @@\n-    log_info(gc)(\"Trigger: Average GC time (%.2f ms) is above the time for average allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (margin of error = %.2f)\",\n-                 avg_cycle_time * 1000,\n+\n+    log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for average allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (margin of error = %.2f)\",\n+                 _generation->name(), avg_cycle_time * 1000,\n@@ -260,1 +439,1 @@\n-    return true;\n+    return resize_and_evaluate();\n@@ -265,2 +444,2 @@\n-    log_info(gc)(\"Trigger: Average GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (spike threshold = %.2f)\",\n-                 avg_cycle_time * 1000,\n+    log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (spike threshold = %.2f)\",\n+                 _generation->name(), avg_cycle_time * 1000,\n@@ -269,0 +448,1 @@\n+\n@@ -271,1 +451,1 @@\n-    return true;\n+    return resize_and_evaluate();\n@@ -277,0 +457,23 @@\n+bool ShenandoahAdaptiveHeuristics::resize_and_evaluate() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  if (!heap->mode()->is_generational()) {\n+    \/\/ We only attempt to resize the generations in generational mode.\n+    return true;\n+  }\n+\n+  if (_cycles_since_last_resize <= MINIMUM_RESIZE_INTERVAL) {\n+    log_info(gc, ergo)(\"Not resizing %s for another \" UINT32_FORMAT \" cycles.\",\n+        _generation->name(),  _cycles_since_last_resize);\n+    return true;\n+  }\n+\n+  if (!heap->generation_sizer()->transfer_capacity(_generation)) {\n+    \/\/ We could not enlarge our generation, so we must start a gc cycle.\n+    log_info(gc, ergo)(\"Could not increase size of %s, begin gc cycle.\", _generation->name());\n+    return true;\n+  }\n+\n+  log_info(gc)(\"Increased size of %s generation, re-evaluate trigger criteria\", _generation->name());\n+  return should_start_gc();\n+}\n+\n@@ -357,4 +560,0 @@\n-double ShenandoahAllocationRate::instantaneous_rate(size_t allocated) const {\n-  return instantaneous_rate(os::elapsedTime(), allocated);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.cpp","additions":269,"deletions":70,"binary":false,"changes":339,"status":"modified"},{"patch":"@@ -39,1 +39,0 @@\n-  double instantaneous_rate(size_t allocated) const;\n@@ -56,1 +55,1 @@\n-  ShenandoahAdaptiveHeuristics();\n+  ShenandoahAdaptiveHeuristics(ShenandoahGeneration* generation);\n@@ -65,1 +64,1 @@\n-  void record_success_concurrent();\n+  void record_success_concurrent(bool abbreviated);\n@@ -88,0 +87,7 @@\n+  \/\/ At least this many cycles must execute before the heuristic will attempt\n+  \/\/ to resize its generation. This is to prevent the heuristic from rapidly\n+  \/\/ maxing out the generation size (which only forces the collector for the\n+  \/\/ other generation to run more frequently, defeating the purpose of improving\n+  \/\/ MMU).\n+  const static uint MINIMUM_RESIZE_INTERVAL;\n+\n@@ -102,0 +108,2 @@\n+  bool resize_and_evaluate();\n+\n@@ -129,0 +137,4 @@\n+\n+  \/\/ Do not attempt to resize the generation for this heuristic until this\n+  \/\/ value is greater than MINIMUM_RESIZE_INTERVAL.\n+  uint _cycles_since_last_resize;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp","additions":15,"deletions":3,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-ShenandoahAggressiveHeuristics::ShenandoahAggressiveHeuristics() : ShenandoahHeuristics() {\n+ShenandoahAggressiveHeuristics::ShenandoahAggressiveHeuristics(ShenandoahGeneration* generation) : ShenandoahHeuristics(generation) {\n@@ -51,0 +51,4 @@\n+  assert(!ShenandoahHeap::heap()->mode()->is_generational(), \"AggressiveHeuristics not appropriate in generational mode\");\n+\n+  \/\/ Note that there's no bound on collection set size.  If we try to collect too much memory, we'll get an alloc\n+  \/\/ failure during collection and we'll degenerate.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  ShenandoahAggressiveHeuristics();\n+  ShenandoahAggressiveHeuristics(ShenandoahGeneration* generation);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -35,1 +35,2 @@\n-ShenandoahCompactHeuristics::ShenandoahCompactHeuristics() : ShenandoahHeuristics() {\n+ShenandoahCompactHeuristics::ShenandoahCompactHeuristics(ShenandoahGeneration* generation) :\n+  ShenandoahHeuristics(generation) {\n@@ -48,5 +49,3 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-\n-  size_t max_capacity = heap->max_capacity();\n-  size_t capacity = heap->soft_max_capacity();\n-  size_t available = heap->free_set()->available();\n+  size_t max_capacity = _generation->max_capacity();\n+  size_t capacity = _generation->soft_max_capacity();\n+  size_t available = _generation->available();\n@@ -59,1 +58,1 @@\n-  size_t min_threshold = capacity \/ 100 * ShenandoahMinFreeThreshold;\n+  size_t min_threshold = min_free_threshold();\n@@ -68,1 +67,1 @@\n-  size_t bytes_allocated = heap->bytes_allocated_since_gc_start();\n+  size_t bytes_allocated = _generation->bytes_allocated_since_gc_start();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.cpp","additions":8,"deletions":9,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  ShenandoahCompactHeuristics();\n+  ShenandoahCompactHeuristics(ShenandoahGeneration* generation);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2020, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2018, 2021, Red Hat, Inc. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n@@ -29,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -32,0 +34,2 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -33,0 +37,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -45,1 +50,2 @@\n-ShenandoahHeuristics::ShenandoahHeuristics() :\n+ShenandoahHeuristics::ShenandoahHeuristics(ShenandoahGeneration* generation) :\n+  _generation(generation),\n@@ -49,0 +55,1 @@\n+  _guaranteed_gc_interval(0),\n@@ -53,1 +60,1 @@\n-  _gc_time_history(new TruncatedSeq(10, ShenandoahAdaptiveDecayFactor)),\n+  _gc_cycle_time_history(new TruncatedSeq(Moving_Average_Samples, ShenandoahAdaptiveDecayFactor)),\n@@ -71,2 +78,19 @@\n-void ShenandoahHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set) {\n-  assert(collection_set->count() == 0, \"Must be empty\");\n+size_t ShenandoahHeuristics::select_aged_regions(size_t old_available, size_t num_regions, bool preselected_regions[]) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  size_t old_consumed = 0;\n+  if (heap->mode()->is_generational()) {\n+    for (size_t i = 0; i < num_regions; i++) {\n+      ShenandoahHeapRegion* region = heap->get_region(i);\n+      if (in_generation(region) && !region->is_empty() && region->is_regular() && (region->age() >= InitialTenuringThreshold)) {\n+        size_t promotion_need = (size_t) (region->get_live_data_bytes() * ShenandoahEvacWaste);\n+        if (old_consumed + promotion_need < old_available) {\n+          old_consumed += promotion_need;\n+          preselected_regions[i] = true;\n+        }\n+        \/\/ Note that we keep going even if one region is excluded from selection.  Subsequent regions may be selected\n+        \/\/ if they have smaller live data.\n+      }\n+    }\n+  }\n+  return old_consumed;\n+}\n@@ -74,0 +98,1 @@\n+void ShenandoahHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set, ShenandoahOldHeuristics* old_heuristics) {\n@@ -75,0 +100,4 @@\n+  bool is_generational = heap->mode()->is_generational();\n+\n+  assert(collection_set->count() == 0, \"Must be empty\");\n+  assert(_generation->generation_mode() != OLD, \"Old GC invokes ShenandoahOldHeuristics::choose_collection_set()\");\n@@ -94,2 +123,1 @@\n-\n-  ShenandoahMarkingContext* const ctx = heap->complete_marking_context();\n+  size_t live_memory = 0;\n@@ -99,0 +127,3 @@\n+    if (is_generational && !in_generation(region)) {\n+      continue;\n+    }\n@@ -102,1 +133,0 @@\n-\n@@ -113,0 +143,2 @@\n+        assert (_generation->generation_mode() != OLD, \"OLD is handled elsewhere\");\n+        live_memory += region->get_live_data_bytes();\n@@ -115,0 +147,4 @@\n+        if (is_generational && collection_set->is_preselected(i)) {\n+          \/\/ If region is preselected, we know mode()->is_generational() and region->age() >= InitialTenuringThreshold)\n+          garbage = ShenandoahHeapRegion::region_size_bytes();\n+        }\n@@ -119,0 +155,1 @@\n+\n@@ -122,1 +159,1 @@\n-      bool bm_live = ctx->is_marked(cast_to_oop(region->bottom()));\n+      bool bm_live = heap->complete_marking_context()->is_marked(cast_to_oop(region->bottom()));\n@@ -133,0 +170,2 @@\n+      } else {\n+        live_memory += region->get_live_data_bytes();\n@@ -138,0 +177,2 @@\n+    } else {                      \/\/ region->is_humongous_cont() and !region->is_trash()\n+      live_memory += region->get_live_data_bytes();\n@@ -150,0 +191,1 @@\n+  collection_set->set_immediate_trash(immediate_garbage);\n@@ -152,0 +194,7 @@\n+    if (old_heuristics != nullptr) {\n+      old_heuristics->prime_collection_set(collection_set);\n+    }\n+    \/\/ else, this is global collection and doesn't need to prime_collection_set\n+\n+    \/\/ Add young-gen regions into the collection set.  This is a virtual call, implemented differently by each\n+    \/\/ of the heuristics subclasses.\n@@ -153,0 +202,3 @@\n+  } else {\n+    \/\/ we're going to skip evacuation and update refs because we reclaimed sufficient amounts of immediate garbage.\n+    heap->shenandoah_policy()->record_abbreviated_cycle();\n@@ -155,1 +207,3 @@\n-  size_t cset_percent = (total_garbage == 0) ? 0 : (collection_set->garbage() * 100 \/ total_garbage);\n+  if (collection_set->has_old_regions()) {\n+    heap->shenandoah_policy()->record_mixed_cycle();\n+  }\n@@ -157,0 +211,1 @@\n+  size_t cset_percent = (total_garbage == 0) ? 0 : (collection_set->garbage() * 100 \/ total_garbage);\n@@ -161,2 +216,2 @@\n-                     \"Immediate: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%), \"\n-                     \"CSet: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%)\",\n+                     \"Immediate: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%) R: \" SIZE_FORMAT \", \"\n+                     \"CSet: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%) R: \" SIZE_FORMAT,\n@@ -171,0 +226,1 @@\n+                     immediate_regions,\n@@ -174,1 +230,17 @@\n-                     cset_percent);\n+                     cset_percent,\n+                     collection_set->count());\n+\n+  if (collection_set->garbage() > 0) {\n+    size_t young_evac_bytes = collection_set->get_young_bytes_reserved_for_evacuation();\n+    size_t promote_evac_bytes = collection_set->get_young_bytes_to_be_promoted();\n+    size_t old_evac_bytes = collection_set->get_old_bytes_reserved_for_evacuation();\n+    size_t total_evac_bytes = young_evac_bytes + promote_evac_bytes + old_evac_bytes;\n+    log_info(gc, ergo)(\"Evacuation Targets: YOUNG: \" SIZE_FORMAT \"%s, \"\n+                       \"PROMOTE: \" SIZE_FORMAT \"%s, \"\n+                       \"OLD: \" SIZE_FORMAT \"%s, \"\n+                       \"TOTAL: \" SIZE_FORMAT \"%s\",\n+                       byte_size_in_proper_unit(young_evac_bytes), proper_unit_for_byte_size(young_evac_bytes),\n+                       byte_size_in_proper_unit(promote_evac_bytes), proper_unit_for_byte_size(promote_evac_bytes),\n+                       byte_size_in_proper_unit(old_evac_bytes), proper_unit_for_byte_size(old_evac_bytes),\n+                       byte_size_in_proper_unit(total_evac_bytes), proper_unit_for_byte_size(total_evac_bytes));\n+  }\n@@ -193,1 +265,1 @@\n-  if (ShenandoahGuaranteedGCInterval > 0) {\n+  if (_guaranteed_gc_interval > 0) {\n@@ -195,3 +267,3 @@\n-    if (last_time_ms > ShenandoahGuaranteedGCInterval) {\n-      log_info(gc)(\"Trigger: Time since last GC (%.0f ms) is larger than guaranteed interval (\" UINTX_FORMAT \" ms)\",\n-                   last_time_ms, ShenandoahGuaranteedGCInterval);\n+    if (last_time_ms > _guaranteed_gc_interval) {\n+      log_info(gc)(\"Trigger (%s): Time since last GC (%.0f ms) is larger than guaranteed interval (\" UINTX_FORMAT \" ms)\",\n+                   _generation->name(), last_time_ms, _guaranteed_gc_interval);\n@@ -211,1 +283,1 @@\n-          \"In range before adjustment: \" INTX_FORMAT, _gc_time_penalties);\n+         \"In range before adjustment: \" INTX_FORMAT, _gc_time_penalties);\n@@ -223,1 +295,1 @@\n-          \"In range after adjustment: \" INTX_FORMAT, _gc_time_penalties);\n+         \"In range after adjustment: \" INTX_FORMAT, _gc_time_penalties);\n@@ -226,1 +298,1 @@\n-void ShenandoahHeuristics::record_success_concurrent() {\n+void ShenandoahHeuristics::record_success_concurrent(bool abbreviated) {\n@@ -230,2 +302,4 @@\n-  _gc_time_history->add(time_since_last_gc());\n-  _gc_times_learned++;\n+  if (!(abbreviated && ShenandoahAdaptiveIgnoreShortCycles)) {\n+    _gc_cycle_time_history->add(elapsed_cycle_time());\n+    _gc_times_learned++;\n+  }\n@@ -257,0 +331,4 @@\n+  reset_gc_learning();\n+}\n+\n+void ShenandoahHeuristics::reset_gc_learning() {\n@@ -288,1 +366,1 @@\n-double ShenandoahHeuristics::time_since_last_gc() const {\n+double ShenandoahHeuristics::elapsed_cycle_time() const {\n@@ -291,0 +369,14 @@\n+\n+bool ShenandoahHeuristics::in_generation(ShenandoahHeapRegion* region) {\n+  return ((_generation->generation_mode() == GLOBAL)\n+          || (_generation->generation_mode() == YOUNG && region->affiliation() == YOUNG_GENERATION)\n+          || (_generation->generation_mode() == OLD && region->affiliation() == OLD_GENERATION));\n+}\n+\n+size_t ShenandoahHeuristics::min_free_threshold() {\n+  size_t min_free_threshold =\n+      _generation->generation_mode() == GenerationMode::OLD\n+          ? ShenandoahOldMinFreeThreshold\n+          : ShenandoahMinFreeThreshold;\n+  return _generation->soft_max_capacity() \/ 100 * min_free_threshold;\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":115,"deletions":23,"binary":false,"changes":138,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n@@ -60,0 +59,2 @@\n+class ShenandoahGeneration;\n+class ShenandoahOldHeuristics;\n@@ -67,0 +68,2 @@\n+  static const uint Moving_Average_Samples = 10; \/\/ Number of samples to store in moving averages\n+\n@@ -72,0 +75,15 @@\n+  ShenandoahGeneration* _generation;\n+\n+  \/\/ if (_generation->generation_mode() == GLOBAL) _region_data represents\n+  \/\/  the results of most recently completed global marking pass\n+  \/\/ if (_generation->generation_mode() == OLD) _region_data represents\n+  \/\/  the results of most recently completed old-gen marking pass\n+  \/\/ if (_generation->generation_mode() == YOUNG) _region_data represents\n+  \/\/  the results of most recently completed young-gen marking pass\n+  \/\/\n+  \/\/ Note that there is some redundancy represented in _region_data because\n+  \/\/ each instance is an array large enough to hold all regions.  However,\n+  \/\/ any region in young-gen is not in old-gen.  And any time we are\n+  \/\/ making use of the GLOBAL data, there is no need to maintain the\n+  \/\/ YOUNG or OLD data.  Consider this redundancy of data structure to\n+  \/\/ have negligible cost unless proven otherwise.\n@@ -77,0 +95,2 @@\n+  size_t _guaranteed_gc_interval;\n+\n@@ -82,1 +102,1 @@\n-  TruncatedSeq* _gc_time_history;\n+  TruncatedSeq* _gc_cycle_time_history;\n@@ -89,0 +109,4 @@\n+  \/\/ TODO: We need to enhance this API to give visibility to accompanying old-gen evacuation effort.\n+  \/\/ In the case that the old-gen evacuation effort is small or zero, the young-gen heuristics\n+  \/\/ should feel free to dedicate increased efforts to young-gen evacuation.\n+\n@@ -95,0 +119,4 @@\n+  bool in_generation(ShenandoahHeapRegion* region);\n+\n+  size_t min_free_threshold();\n+\n@@ -96,1 +124,1 @@\n-  ShenandoahHeuristics();\n+  ShenandoahHeuristics(ShenandoahGeneration* generation);\n@@ -103,0 +131,8 @@\n+  void set_guaranteed_gc_interval(size_t guaranteed_gc_interval) {\n+    _guaranteed_gc_interval = guaranteed_gc_interval;\n+  }\n+\n+  uint degenerated_cycles_in_a_row() {\n+    return _degenerated_cycles_in_a_row;\n+  }\n+\n@@ -111,1 +147,1 @@\n-  virtual void record_success_concurrent();\n+  virtual void record_success_concurrent(bool abbreviated);\n@@ -121,1 +157,5 @@\n-  virtual void choose_collection_set(ShenandoahCollectionSet* collection_set);\n+  virtual void reset_gc_learning();\n+\n+  virtual size_t select_aged_regions(size_t old_available, size_t num_regions, bool preselected_regions[]);\n+\n+  virtual void choose_collection_set(ShenandoahCollectionSet* collection_set, ShenandoahOldHeuristics* old_heuristics);\n@@ -132,1 +172,1 @@\n-  double time_since_last_gc() const;\n+  double elapsed_cycle_time() const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp","additions":46,"deletions":6,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -0,0 +1,432 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com, Inc. or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectionSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"utilities\/quickSort.hpp\"\n+\n+uint ShenandoahOldHeuristics::NOT_FOUND = -1U;\n+\n+ShenandoahOldHeuristics::ShenandoahOldHeuristics(ShenandoahOldGeneration* generation, ShenandoahHeuristics* trigger_heuristic) :\n+  ShenandoahHeuristics(generation),\n+#ifdef ASSERT\n+  _start_candidate(0),\n+#endif\n+  _first_pinned_candidate(NOT_FOUND),\n+  _last_old_collection_candidate(0),\n+  _next_old_collection_candidate(0),\n+  _last_old_region(0),\n+  _trigger_heuristic(trigger_heuristic),\n+  _promotion_failed(false),\n+  _old_generation(generation)\n+{\n+  assert(_generation->generation_mode() == OLD, \"This service only available for old-gc heuristics\");\n+}\n+\n+bool ShenandoahOldHeuristics::prime_collection_set(ShenandoahCollectionSet* collection_set) {\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    return false;\n+  }\n+\n+  _first_pinned_candidate = NOT_FOUND;\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  uint included_old_regions = 0;\n+  size_t evacuated_old_bytes = 0;\n+  size_t collected_old_bytes = 0;\n+\n+  \/\/ If a region is put into the collection set, then this region's free (not yet used) bytes are no longer\n+  \/\/ \"available\" to hold the results of other evacuations.  This may cause a decrease in the remaining amount\n+  \/\/ of memory that can still be evacuated.  We address this by reducing the evacuation budget by the amount\n+  \/\/ of live memory in that region and by the amount of unallocated memory in that region if the evacuation\n+  \/\/ budget is constrained by availability of free memory.\n+  size_t old_evacuation_budget = (size_t) ((double) heap->get_old_evac_reserve() \/ ShenandoahEvacWaste);\n+  size_t remaining_old_evacuation_budget = old_evacuation_budget;\n+  size_t lost_evacuation_capacity = 0;\n+  log_info(gc)(\"Choose old regions for mixed collection: old evacuation budget: \" SIZE_FORMAT \"%s, candidates: %u\",\n+               byte_size_in_proper_unit(old_evacuation_budget), proper_unit_for_byte_size(old_evacuation_budget),\n+               unprocessed_old_collection_candidates());\n+\n+  \/\/ The number of old-gen regions that were selected as candidates for collection at the end of the most recent old-gen\n+  \/\/ concurrent marking phase and have not yet been collected is represented by unprocessed_old_collection_candidates()\n+  while (unprocessed_old_collection_candidates() > 0) {\n+    \/\/ Old collection candidates are sorted in order of decreasing garbage contained therein.\n+    ShenandoahHeapRegion* r = next_old_collection_candidate();\n+    if (r == nullptr) {\n+      break;\n+    }\n+\n+    \/\/ If we choose region r to be collected, then we need to decrease the capacity to hold other evacuations by\n+    \/\/ the size of r's free memory.\n+\n+    \/\/ It's probably overkill to compensate with lost_evacuation_capacity.  But it's the safe thing to do and\n+    \/\/  has minimal impact on content of primed collection set.\n+    if (r->get_live_data_bytes() + lost_evacuation_capacity <= remaining_old_evacuation_budget) {\n+      \/\/ Decrement remaining evacuation budget by bytes that will be copied.\n+      lost_evacuation_capacity += r->free();\n+      remaining_old_evacuation_budget -= r->get_live_data_bytes();\n+      collection_set->add_region(r);\n+      included_old_regions++;\n+      evacuated_old_bytes += r->get_live_data_bytes();\n+      collected_old_bytes += r->garbage();\n+      consume_old_collection_candidate();\n+    } else {\n+      break;\n+    }\n+  }\n+\n+  if (_first_pinned_candidate != NOT_FOUND) {\n+    \/\/ Need to deal with pinned regions\n+    slide_pinned_regions_to_front();\n+  }\n+\n+  if (included_old_regions > 0) {\n+    log_info(gc)(\"Old-gen piggyback evac (\" UINT32_FORMAT \" regions, evacuating \" SIZE_FORMAT \"%s, reclaiming: \" SIZE_FORMAT \"%s)\",\n+                 included_old_regions,\n+                 byte_size_in_proper_unit(evacuated_old_bytes), proper_unit_for_byte_size(evacuated_old_bytes),\n+                 byte_size_in_proper_unit(collected_old_bytes), proper_unit_for_byte_size(collected_old_bytes));\n+  }\n+\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    _old_generation->transition_to(ShenandoahOldGeneration::IDLE);\n+  }\n+\n+  return (included_old_regions > 0);\n+}\n+\n+void ShenandoahOldHeuristics::slide_pinned_regions_to_front() {\n+  \/\/ Find the leftmost unpinned region. The region in this slot will have been\n+  \/\/ added to the cset, so we can use it to hold pointers to regions that were\n+  \/\/ pinned when the cset was chosen.\n+  \/\/ [ r p r p p p r ]\n+  \/\/          ^\n+  \/\/          | first r to the left should be in the collection set now.\n+  uint write_index = NOT_FOUND;\n+  for (uint search = _next_old_collection_candidate - 1; search > _first_pinned_candidate; --search) {\n+    ShenandoahHeapRegion* region = _region_data[search]._region;\n+    if (!region->is_pinned()) {\n+      write_index = search;\n+      assert(region->is_cset(), \"Expected unpinned region to be added to the collection set.\");\n+      break;\n+    }\n+  }\n+\n+  if (write_index == NOT_FOUND) {\n+    if (_first_pinned_candidate > 0) {\n+      _next_old_collection_candidate = _first_pinned_candidate;\n+    }\n+    return;\n+  }\n+\n+  \/\/ Find pinned regions to the left and move their pointer into a slot\n+  \/\/ that was pointing at a region that has been added to the cset.\n+  \/\/ [ r p r p p p r ]\n+  \/\/       ^\n+  \/\/       | Write pointer is here. We know this region is already in the cset\n+  \/\/       | so we can clobber it with the next pinned region we find.\n+  for (size_t search = write_index - 1; search > _first_pinned_candidate; --search) {\n+    RegionData& skipped = _region_data[search];\n+    if (skipped._region->is_pinned()) {\n+      RegionData& added_to_cset = _region_data[write_index];\n+      assert(added_to_cset._region->is_cset(), \"Can only overwrite slots used by regions added to the collection set.\");\n+      added_to_cset._region = skipped._region;\n+      added_to_cset._garbage = skipped._garbage;\n+      --write_index;\n+    }\n+  }\n+\n+  \/\/ Everything left should already be in the cset\n+  \/\/ [ r x p p p p r ]\n+  \/\/       ^\n+  \/\/       | next pointer points at the first region which was not added\n+  \/\/       | to the collection set.\n+#ifdef ASSERT\n+  for (size_t check = write_index - 1; check > _start_candidate; --check) {\n+    ShenandoahHeapRegion* region = _region_data[check]._region;\n+    assert(region->is_cset(), \"All regions here should be in the collection set.\");\n+  }\n+  _start_candidate = write_index;\n+#endif\n+\n+  \/\/ Update to read from the leftmost pinned region.\n+  _next_old_collection_candidate = write_index;\n+}\n+\n+\/\/ Both arguments are don't cares for old-gen collections\n+void ShenandoahOldHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set,\n+                                                    ShenandoahOldHeuristics* old_heuristics) {\n+  assert((collection_set == nullptr) && (old_heuristics == nullptr),\n+         \"Expect null arguments in ShenandoahOldHeuristics::choose_collection_set()\");\n+  \/\/ Old-gen doesn't actually choose a collection set to be evacuated by its own gang of worker tasks.\n+  \/\/ Instead, it computes the set of regions to be evacuated by subsequent young-gen evacuation passes.\n+  prepare_for_old_collections();\n+}\n+\n+void ShenandoahOldHeuristics::prepare_for_old_collections() {\n+  assert(_generation->generation_mode() == OLD, \"This service only available for old-gc heuristics\");\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  size_t cand_idx = 0;\n+  size_t total_garbage = 0;\n+  size_t num_regions = heap->num_regions();\n+  size_t immediate_garbage = 0;\n+  size_t immediate_regions = 0;\n+\n+  RegionData* candidates = _region_data;\n+  for (size_t i = 0; i < num_regions; i++) {\n+    ShenandoahHeapRegion* region = heap->get_region(i);\n+    if (!in_generation(region)) {\n+      continue;\n+    }\n+\n+    size_t garbage = region->garbage();\n+    total_garbage += garbage;\n+\n+    if (region->is_regular() || region->is_pinned()) {\n+      if (!region->has_live()) {\n+        assert(!region->is_pinned(), \"Pinned region should have live (pinned) objects.\");\n+        region->make_trash_immediate();\n+        immediate_regions++;\n+        immediate_garbage += garbage;\n+      } else {\n+        region->begin_preemptible_coalesce_and_fill();\n+        candidates[cand_idx]._region = region;\n+        candidates[cand_idx]._garbage = garbage;\n+        cand_idx++;\n+      }\n+    } else if (region->is_humongous_start()) {\n+      if (!region->has_live()) {\n+        \/\/ The humongous object is dead, we can just return this region and the continuations\n+        \/\/ immediately to the freeset - no evacuations are necessary here. The continuations\n+        \/\/ will be made into trash by this method, so they'll be skipped by the 'is_regular'\n+        \/\/ check above, but we still need to count the start region.\n+        immediate_regions++;\n+        immediate_garbage += garbage;\n+        size_t region_count = heap->trash_humongous_region_at(region);\n+        log_debug(gc)(\"Trashed \" SIZE_FORMAT \" regions for humongous object.\", region_count);\n+      }\n+    } else if (region->is_trash()) {\n+      \/\/ Count humongous objects made into trash here.\n+      immediate_regions++;\n+      immediate_garbage += garbage;\n+    }\n+  }\n+\n+  \/\/ TODO: Consider not running mixed collects if we recovered some threshold percentage of memory from immediate garbage.\n+  \/\/ This would be similar to young and global collections shortcutting evacuation, though we'd probably want a separate\n+  \/\/ threshold for the old generation.\n+\n+  \/\/ Prioritize regions to select garbage-first regions\n+  QuickSort::sort<RegionData>(candidates, cand_idx, compare_by_garbage, false);\n+\n+  \/\/ Any old-gen region that contains (ShenandoahOldGarbageThreshold (default value 25))% garbage or more is to\n+  \/\/ be evacuated.\n+  \/\/\n+  \/\/ TODO: allow ShenandoahOldGarbageThreshold to be determined adaptively, by heuristics.\n+\n+\n+  const size_t garbage_threshold = ShenandoahHeapRegion::region_size_bytes() * ShenandoahOldGarbageThreshold \/ 100;\n+  size_t candidates_garbage = 0;\n+  _last_old_region = (uint)cand_idx;\n+  _last_old_collection_candidate = (uint)cand_idx;\n+  _next_old_collection_candidate = 0;\n+\n+  for (size_t i = 0; i < cand_idx; i++) {\n+    if (candidates[i]._garbage < garbage_threshold) {\n+      \/\/ Candidates are sorted in decreasing order of garbage, so no regions after this will be above the threshold\n+      _last_old_collection_candidate = (uint)i;\n+      break;\n+    }\n+    candidates_garbage += candidates[i]._garbage;\n+  }\n+\n+  \/\/ Note that we do not coalesce and fill occupied humongous regions\n+  \/\/ HR: humongous regions, RR: regular regions, CF: coalesce and fill regions\n+  size_t collectable_garbage = immediate_garbage + candidates_garbage;\n+  log_info(gc)(\"Old-Gen Collectable Garbage: \" SIZE_FORMAT \"%s over \" UINT32_FORMAT \" regions, \"\n+               \"Old-Gen Immediate Garbage: \" SIZE_FORMAT \"%s over \" SIZE_FORMAT \" regions.\",\n+               byte_size_in_proper_unit(collectable_garbage), proper_unit_for_byte_size(collectable_garbage), _last_old_collection_candidate,\n+               byte_size_in_proper_unit(immediate_garbage), proper_unit_for_byte_size(immediate_garbage), immediate_regions);\n+\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    _old_generation->transition_to(ShenandoahOldGeneration::IDLE);\n+  } else {\n+    _old_generation->transition_to(ShenandoahOldGeneration::WAITING);\n+  }\n+}\n+\n+uint ShenandoahOldHeuristics::last_old_collection_candidate_index() {\n+  return _last_old_collection_candidate;\n+}\n+\n+uint ShenandoahOldHeuristics::unprocessed_old_collection_candidates() {\n+  return _last_old_collection_candidate - _next_old_collection_candidate;\n+}\n+\n+ShenandoahHeapRegion* ShenandoahOldHeuristics::next_old_collection_candidate() {\n+  while (_next_old_collection_candidate < _last_old_collection_candidate) {\n+    ShenandoahHeapRegion* next = _region_data[_next_old_collection_candidate]._region;\n+    if (!next->is_pinned()) {\n+      return next;\n+    } else {\n+      assert(next->is_pinned(), \"sanity\");\n+      if (_first_pinned_candidate == NOT_FOUND) {\n+        _first_pinned_candidate = _next_old_collection_candidate;\n+      }\n+    }\n+\n+    _next_old_collection_candidate++;\n+  }\n+  return nullptr;\n+}\n+\n+void ShenandoahOldHeuristics::consume_old_collection_candidate() {\n+  _next_old_collection_candidate++;\n+}\n+\n+uint ShenandoahOldHeuristics::last_old_region_index() const {\n+  return _last_old_region;\n+}\n+\n+unsigned int ShenandoahOldHeuristics::get_coalesce_and_fill_candidates(ShenandoahHeapRegion** buffer) {\n+  uint end = _last_old_region;\n+  uint index = _next_old_collection_candidate;\n+  while (index < end) {\n+    *buffer++ = _region_data[index++]._region;\n+  }\n+  return _last_old_region - _next_old_collection_candidate;\n+}\n+\n+void ShenandoahOldHeuristics::abandon_collection_candidates() {\n+  _last_old_collection_candidate = 0;\n+  _next_old_collection_candidate = 0;\n+  _last_old_region = 0;\n+}\n+\n+void ShenandoahOldHeuristics::handle_promotion_failure() {\n+  if (!_promotion_failed) {\n+    if (ShenandoahHeap::heap()->generation_sizer()->transfer_capacity(_old_generation)) {\n+      log_info(gc)(\"Increased size of old generation due to promotion failure.\");\n+    }\n+    \/\/ TODO: Increase tenuring threshold to push back on promotions.\n+  }\n+  _promotion_failed = true;\n+}\n+\n+void ShenandoahOldHeuristics::record_cycle_start() {\n+  _promotion_failed = false;\n+  _trigger_heuristic->record_cycle_start();\n+}\n+\n+void ShenandoahOldHeuristics::record_cycle_end() {\n+  _trigger_heuristic->record_cycle_end();\n+}\n+\n+bool ShenandoahOldHeuristics::should_start_gc() {\n+  \/\/ Cannot start a new old-gen GC until previous one has finished.\n+  \/\/\n+  \/\/ Future refinement: under certain circumstances, we might be more sophisticated about this choice.\n+  \/\/ For example, we could choose to abandon the previous old collection before it has completed evacuations.\n+  if (unprocessed_old_collection_candidates() > 0) {\n+    return false;\n+  }\n+\n+  \/\/ If there's been a promotion failure (and we don't have regions already scheduled for evacuation),\n+  \/\/ start a new old generation collection.\n+  if (_promotion_failed) {\n+    log_info(gc)(\"Trigger: Promotion Failure\");\n+    return true;\n+  }\n+\n+  \/\/ Otherwise, defer to configured heuristic for gc trigger.\n+  return _trigger_heuristic->should_start_gc();\n+}\n+\n+bool ShenandoahOldHeuristics::should_degenerate_cycle() {\n+  return _trigger_heuristic->should_degenerate_cycle();\n+}\n+\n+void ShenandoahOldHeuristics::record_success_concurrent(bool abbreviated) {\n+  _trigger_heuristic->record_success_concurrent(abbreviated);\n+}\n+\n+void ShenandoahOldHeuristics::record_success_degenerated() {\n+  _trigger_heuristic->record_success_degenerated();\n+}\n+\n+void ShenandoahOldHeuristics::record_success_full() {\n+  _trigger_heuristic->record_success_full();\n+}\n+\n+void ShenandoahOldHeuristics::record_allocation_failure_gc() {\n+  _trigger_heuristic->record_allocation_failure_gc();\n+}\n+\n+void ShenandoahOldHeuristics::record_requested_gc() {\n+  _trigger_heuristic->record_requested_gc();\n+}\n+\n+void ShenandoahOldHeuristics::reset_gc_learning() {\n+  _trigger_heuristic->reset_gc_learning();\n+}\n+\n+bool ShenandoahOldHeuristics::can_unload_classes() {\n+  return _trigger_heuristic->can_unload_classes();\n+}\n+\n+bool ShenandoahOldHeuristics::can_unload_classes_normal() {\n+  return _trigger_heuristic->can_unload_classes_normal();\n+}\n+\n+bool ShenandoahOldHeuristics::should_unload_classes() {\n+  return _trigger_heuristic->should_unload_classes();\n+}\n+\n+const char* ShenandoahOldHeuristics::name() {\n+  static char name[128];\n+  jio_snprintf(name, sizeof(name), \"%s (OLD)\", _trigger_heuristic->name());\n+  return name;\n+}\n+\n+bool ShenandoahOldHeuristics::is_diagnostic() {\n+  return false;\n+}\n+\n+bool ShenandoahOldHeuristics::is_experimental() {\n+  return true;\n+}\n+\n+void ShenandoahOldHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* set,\n+                                                                    ShenandoahHeuristics::RegionData* data,\n+                                                                    size_t data_size, size_t free) {\n+  ShouldNotReachHere();\n+}\n+\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":432,"deletions":0,"binary":false,"changes":432,"status":"added"},{"patch":"@@ -0,0 +1,170 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com, Inc. or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHOLDHEURISTICS_HPP\n+#define SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHOLDHEURISTICS_HPP\n+\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+\n+class ShenandoahCollectionSet;\n+class ShenandoahHeapRegion;\n+class ShenandoahOldGeneration;\n+\n+class ShenandoahOldHeuristics : public ShenandoahHeuristics {\n+\n+private:\n+\n+  static uint NOT_FOUND;\n+\n+  \/\/ After final marking of the old generation, this heuristic will select\n+  \/\/ a set of candidate regions to be included in subsequent mixed collections.\n+  \/\/ The regions are sorted into a `_region_data` array (declared in base\n+  \/\/ class) in decreasing order of garbage. The heuristic will give priority\n+  \/\/ to regions containing more garbage.\n+\n+  \/\/ The following members are used to keep track of which candidate regions\n+  \/\/ have yet to be added to a mixed collection. There is also some special\n+  \/\/ handling for pinned regions, described further below.\n+\n+  \/\/ This points to the first candidate of the current mixed collection. This\n+  \/\/ is only used for an assertion when handling pinned regions.\n+  debug_only(uint _start_candidate);\n+\n+  \/\/ Pinned regions may not be included in the collection set. Any old regions\n+  \/\/ which were pinned at the time when old regions were added to the mixed\n+  \/\/ collection will have been skipped. These regions are still contain garbage,\n+  \/\/ so we want to include them at the start of the list of candidates for the\n+  \/\/ _next_ mixed collection cycle. This variable is the index of the _first_\n+  \/\/ old region which is pinned when the mixed collection set is formed.\n+  uint _first_pinned_candidate;\n+\n+  \/\/ This is the index of the last region which is above the garbage threshold.\n+  \/\/ No regions after this will be considered for inclusion in a mixed collection\n+  \/\/ set.\n+  uint _last_old_collection_candidate;\n+\n+  \/\/ This index points to the first candidate in line to be added to the mixed\n+  \/\/ collection set. It is updated as regions are added to the collection set.\n+  uint _next_old_collection_candidate;\n+\n+  \/\/ This is the last index in the array of old regions which were active at\n+  \/\/ the end of old final mark.\n+  uint _last_old_region;\n+\n+  \/\/ This can be the 'static' or 'adaptive' heuristic.\n+  ShenandoahHeuristics* _trigger_heuristic;\n+\n+  \/\/ Flag is set when promotion failure is detected (by gc thread), and cleared when\n+  \/\/ old generation collection begins (by control thread).\n+  volatile bool _promotion_failed;\n+\n+  \/\/ Keep a pointer to our generation that we can use without down casting a protected member from the base class.\n+  ShenandoahOldGeneration* _old_generation;\n+\n+ protected:\n+  virtual void choose_collection_set_from_regiondata(ShenandoahCollectionSet* set, RegionData* data, size_t data_size,\n+                                                     size_t free) override;\n+\n+public:\n+  ShenandoahOldHeuristics(ShenandoahOldGeneration* generation, ShenandoahHeuristics* trigger_heuristic);\n+\n+  virtual void choose_collection_set(ShenandoahCollectionSet* collection_set, ShenandoahOldHeuristics* old_heuristics) override;\n+\n+  \/\/ Prepare for evacuation of old-gen regions by capturing the mark results of a recently completed concurrent mark pass.\n+  void prepare_for_old_collections();\n+\n+  \/\/ Return true iff the collection set is primed with at least one old-gen region.\n+  bool prime_collection_set(ShenandoahCollectionSet* set);\n+\n+  \/\/ How many old-collection candidates have not yet been processed?\n+  uint unprocessed_old_collection_candidates();\n+\n+  \/\/ How many old or hidden collection candidates have not yet been processed?\n+  uint last_old_collection_candidate_index();\n+\n+  \/\/ Return the next old-collection candidate in order of decreasing amounts of garbage.  (We process most-garbage regions\n+  \/\/ first.)  This does not consume the candidate.  If the candidate is selected for inclusion in a collection set, then\n+  \/\/ the candidate is consumed by invoking consume_old_collection_candidate().\n+  ShenandoahHeapRegion* next_old_collection_candidate();\n+\n+  \/\/ Adjust internal state to reflect that one fewer old-collection candidate remains to be processed.\n+  void consume_old_collection_candidate();\n+\n+  \/\/ How many old-collection regions were identified at the end of the most recent old-gen mark to require their\n+  \/\/ unmarked objects to be coalesced and filled?\n+  uint last_old_region_index() const;\n+\n+  \/\/ Fill in buffer with all of the old-collection regions that were identified at the end of the most recent old-gen\n+  \/\/ mark to require their unmarked objects to be coalesced and filled.  The buffer array must have at least\n+  \/\/ last_old_region_index() entries, or memory may be corrupted when this function overwrites the\n+  \/\/ end of the array.\n+  unsigned int get_coalesce_and_fill_candidates(ShenandoahHeapRegion** buffer);\n+\n+  \/\/ If a GLOBAL gc occurs, it will collect the entire heap which invalidates any collection candidates being\n+  \/\/ held by this heuristic for supplying mixed collections.\n+  void abandon_collection_candidates();\n+\n+  \/\/ Notify the heuristic of promotion failures. The promotion attempt will be skipped and the object will\n+  \/\/ be evacuated into the young generation. The collection should complete normally, but we want to schedule\n+  \/\/ an old collection as soon as possible.\n+  void handle_promotion_failure();\n+\n+  virtual void record_cycle_start() override;\n+\n+  virtual void record_cycle_end() override;\n+\n+  virtual bool should_start_gc() override;\n+\n+  virtual bool should_degenerate_cycle() override;\n+\n+  virtual void record_success_concurrent(bool abbreviated) override;\n+\n+  virtual void record_success_degenerated() override;\n+\n+  virtual void record_success_full() override;\n+\n+  virtual void record_allocation_failure_gc() override;\n+\n+  virtual void record_requested_gc() override;\n+\n+  virtual void reset_gc_learning() override;\n+\n+  virtual bool can_unload_classes() override;\n+\n+  virtual bool can_unload_classes_normal() override;\n+\n+  virtual bool should_unload_classes() override;\n+\n+  virtual const char* name() override;\n+\n+  virtual bool is_diagnostic() override;\n+\n+  virtual bool is_experimental() override;\n+\n+ private:\n+  void slide_pinned_regions_to_front();\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHOLDHEURISTICS_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp","additions":170,"deletions":0,"binary":false,"changes":170,"status":"added"},{"patch":"@@ -57,1 +57,1 @@\n-  size_t available = MAX2(max_capacity \/ 100 * ShenandoahEvacReserve, actual_free);\n+  size_t available = MAX2((max_capacity \/ 100) * ShenandoahEvacReserve, actual_free);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahPassiveHeuristics.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,0 +32,3 @@\n+  ShenandoahPassiveHeuristics(ShenandoahGeneration* generation)\n+    : ShenandoahHeuristics(generation) {}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahPassiveHeuristics.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n@@ -32,0 +31,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -35,1 +35,2 @@\n-ShenandoahStaticHeuristics::ShenandoahStaticHeuristics() : ShenandoahHeuristics() {\n+ShenandoahStaticHeuristics::ShenandoahStaticHeuristics(ShenandoahGeneration* generation) :\n+  ShenandoahHeuristics(generation) {\n@@ -43,5 +44,3 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-\n-  size_t max_capacity = heap->max_capacity();\n-  size_t capacity = heap->soft_max_capacity();\n-  size_t available = heap->free_set()->available();\n+  size_t max_capacity = _generation->max_capacity();\n+  size_t capacity = _generation->soft_max_capacity();\n+  size_t available = _generation->available();\n@@ -53,1 +52,1 @@\n-  size_t threshold_available = capacity \/ 100 * ShenandoahMinFreeThreshold;\n+  size_t threshold_available = min_free_threshold();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.cpp","additions":7,"deletions":8,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  ShenandoahStaticHeuristics();\n+  ShenandoahStaticHeuristics(ShenandoahGeneration* generation);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,102 @@\n+\/*\n+ * Copyright (c) 2019, 2020, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logTag.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n+\n+void ShenandoahGenerationalMode::initialize_flags() const {\n+  if (ClassUnloading) {\n+    FLAG_SET_DEFAULT(ShenandoahSuspendibleWorkers, true);\n+    FLAG_SET_DEFAULT(VerifyBeforeExit, false);\n+  }\n+\n+  SHENANDOAH_ERGO_OVERRIDE_DEFAULT(GCTimeRatio, 70);\n+  SHENANDOAH_ERGO_OVERRIDE_DEFAULT(ShenandoahUnloadClassesFrequency, 0);\n+  SHENANDOAH_ERGO_ENABLE_FLAG(ExplicitGCInvokesConcurrent);\n+  SHENANDOAH_ERGO_ENABLE_FLAG(ShenandoahImplicitGCInvokesConcurrent);\n+\n+  \/\/ This helps most multi-core hardware hosts, enable by default\n+  SHENANDOAH_ERGO_ENABLE_FLAG(UseCondCardMark);\n+\n+  \/\/ Final configuration checks\n+  SHENANDOAH_CHECK_FLAG_SET(ShenandoahLoadRefBarrier);\n+  SHENANDOAH_CHECK_FLAG_UNSET(ShenandoahIUBarrier);\n+  SHENANDOAH_CHECK_FLAG_SET(ShenandoahSATBBarrier);\n+  SHENANDOAH_CHECK_FLAG_SET(ShenandoahCASBarrier);\n+  SHENANDOAH_CHECK_FLAG_SET(ShenandoahCloneBarrier);\n+}\n+\n+const char* affiliation_name(oop ptr) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  assert(heap->is_in(ptr), \"Oop must be in the heap.\");\n+  ShenandoahHeapRegion* region = heap->heap_region_containing(ptr);\n+  return affiliation_name(region->affiliation());\n+}\n+\n+const char affiliation_code(ShenandoahRegionAffiliation type) {\n+  switch(type) {\n+    case ShenandoahRegionAffiliation::FREE:\n+      return 'F';\n+    case ShenandoahRegionAffiliation::YOUNG_GENERATION:\n+      return 'Y';\n+    case ShenandoahRegionAffiliation::OLD_GENERATION:\n+      return 'O';\n+    default:\n+      ShouldNotReachHere();\n+      return 'X';\n+  }\n+}\n+\n+const char* affiliation_name(ShenandoahRegionAffiliation type) {\n+  switch (type) {\n+    case ShenandoahRegionAffiliation::FREE:\n+      return \"FREE\";\n+    case ShenandoahRegionAffiliation::YOUNG_GENERATION:\n+      return \"YOUNG\";\n+    case ShenandoahRegionAffiliation::OLD_GENERATION:\n+      return \"OLD\";\n+    default:\n+      ShouldNotReachHere();\n+      return nullptr;\n+  }\n+}\n+\n+const char* generation_name(GenerationMode mode) {\n+  switch (mode) {\n+    case GenerationMode::GLOBAL:\n+      return \"Global\";\n+    case GenerationMode::OLD:\n+      return \"Old\";\n+    case GenerationMode::YOUNG:\n+      return \"Young\";\n+    default:\n+      ShouldNotReachHere();\n+      return nullptr;\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahGenerationalMode.cpp","additions":102,"deletions":0,"binary":false,"changes":102,"status":"added"},{"patch":"@@ -0,0 +1,57 @@\n+\/*\n+ * Copyright (c) 2020, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_MODE_SHENANDOAHGENERATIONALMODE_HPP\n+#define SHARE_GC_SHENANDOAH_MODE_SHENANDOAHGENERATIONALMODE_HPP\n+\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+\n+enum GenerationMode {\n+  YOUNG,\n+  OLD,\n+  GLOBAL\n+};\n+\n+enum ShenandoahRegionAffiliation {\n+  FREE,\n+  YOUNG_GENERATION,\n+  OLD_GENERATION\n+};\n+\n+const char* affiliation_name(oop ptr);\n+const char* affiliation_name(ShenandoahRegionAffiliation type);\n+const char affiliation_code(ShenandoahRegionAffiliation type);\n+const char* generation_name(GenerationMode mode);\n+\n+class ShenandoahGenerationalMode : public ShenandoahMode {\n+public:\n+  virtual void initialize_flags() const;\n+  virtual const char* name()     { return \"Generational\"; }\n+  virtual bool is_diagnostic()   { return false; }\n+  virtual bool is_experimental() { return false; }\n+  virtual bool is_generational() { return true; }\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_MODE_SHENANDOAHGENERATIONALMODE_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp","additions":57,"deletions":0,"binary":false,"changes":57,"status":"added"},{"patch":"@@ -26,4 +26,1 @@\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n@@ -66,17 +63,0 @@\n-\n-ShenandoahHeuristics* ShenandoahIUMode::initialize_heuristics() const {\n-  if (ShenandoahGCHeuristics == nullptr) {\n-    vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option (null)\");\n-  }\n-  if (strcmp(ShenandoahGCHeuristics, \"aggressive\") == 0) {\n-    return new ShenandoahAggressiveHeuristics();\n-  } else if (strcmp(ShenandoahGCHeuristics, \"static\") == 0) {\n-    return new ShenandoahStaticHeuristics();\n-  } else if (strcmp(ShenandoahGCHeuristics, \"adaptive\") == 0) {\n-    return new ShenandoahAdaptiveHeuristics();\n-  } else if (strcmp(ShenandoahGCHeuristics, \"compact\") == 0) {\n-    return new ShenandoahCompactHeuristics();\n-  }\n-  vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option\");\n-  return nullptr;\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahIUMode.cpp","additions":1,"deletions":21,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -35,2 +35,0 @@\n-  virtual ShenandoahHeuristics* initialize_heuristics() const;\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahIUMode.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,53 @@\n+\/*\n+ * Copyright (c) 2020, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+\n+ShenandoahHeuristics* ShenandoahMode::initialize_heuristics(ShenandoahGeneration* generation) const {\n+  if (ShenandoahGCHeuristics == nullptr) {\n+    vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option (null)\");\n+  }\n+\n+  if (strcmp(ShenandoahGCHeuristics, \"aggressive\") == 0) {\n+    return new ShenandoahAggressiveHeuristics(generation);\n+  } else if (strcmp(ShenandoahGCHeuristics, \"static\") == 0) {\n+    return new ShenandoahStaticHeuristics(generation);\n+  } else if (strcmp(ShenandoahGCHeuristics, \"adaptive\") == 0) {\n+    return new ShenandoahAdaptiveHeuristics(generation);\n+  } else if (strcmp(ShenandoahGCHeuristics, \"compact\") == 0) {\n+    return new ShenandoahCompactHeuristics(generation);\n+  } else {\n+    vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option\");\n+  }\n+\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahMode.cpp","additions":53,"deletions":0,"binary":false,"changes":53,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2019, 2021, Red Hat, Inc. All rights reserved.\n@@ -29,0 +29,2 @@\n+#include \"runtime\/java.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n@@ -30,0 +32,1 @@\n+class ShenandoahGeneration;\n@@ -31,0 +34,1 @@\n+class ShenandoahOldHeuristics;\n@@ -51,1 +55,1 @@\n-  virtual ShenandoahHeuristics* initialize_heuristics() const = 0;\n+  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahGeneration* generation) const;\n@@ -55,0 +59,1 @@\n+  virtual bool is_generational() { return false; }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahMode.hpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n@@ -30,1 +31,0 @@\n-#include \"runtime\/globals_extension.hpp\"\n@@ -58,1 +58,1 @@\n-ShenandoahHeuristics* ShenandoahPassiveMode::initialize_heuristics() const {\n+ShenandoahHeuristics* ShenandoahPassiveMode::initialize_heuristics(ShenandoahGeneration* generation) const {\n@@ -62,1 +62,1 @@\n-  return new ShenandoahPassiveHeuristics();\n+  return new ShenandoahPassiveHeuristics(generation);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahPassiveMode.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2019, 2021, Red Hat, Inc. All rights reserved.\n@@ -33,2 +33,1 @@\n-  virtual ShenandoahHeuristics* initialize_heuristics() const;\n-\n+  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahGeneration* generation) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahPassiveMode.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -26,4 +26,1 @@\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n@@ -54,17 +51,0 @@\n-\n-ShenandoahHeuristics* ShenandoahSATBMode::initialize_heuristics() const {\n-  if (ShenandoahGCHeuristics == nullptr) {\n-    vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option (null)\");\n-  }\n-  if (strcmp(ShenandoahGCHeuristics, \"aggressive\") == 0) {\n-    return new ShenandoahAggressiveHeuristics();\n-  } else if (strcmp(ShenandoahGCHeuristics, \"static\") == 0) {\n-    return new ShenandoahStaticHeuristics();\n-  } else if (strcmp(ShenandoahGCHeuristics, \"adaptive\") == 0) {\n-    return new ShenandoahAdaptiveHeuristics();\n-  } else if (strcmp(ShenandoahGCHeuristics, \"compact\") == 0) {\n-    return new ShenandoahCompactHeuristics();\n-  }\n-  vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option\");\n-  return nullptr;\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahSATBMode.cpp","additions":1,"deletions":21,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -35,1 +35,0 @@\n-  virtual ShenandoahHeuristics* initialize_heuristics() const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahSATBMode.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2018, 2020, Red Hat, Inc. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -34,1 +35,1 @@\n-    _alloc_shared_gc,   \/\/ Allocate common, outside of GCLAB\n+    _alloc_shared_gc,   \/\/ Allocate common, outside of GCLAB\/PLAB\n@@ -37,0 +38,1 @@\n+    _alloc_plab,        \/\/ Allocate PLAB\n@@ -50,0 +52,2 @@\n+      case _alloc_plab:\n+        return \"PLAB\";\n@@ -61,0 +65,1 @@\n+  ShenandoahRegionAffiliation const _affiliation;\n@@ -65,1 +70,1 @@\n-  ShenandoahAllocRequest(size_t _min_size, size_t _requested_size, Type _alloc_type) :\n+  ShenandoahAllocRequest(size_t _min_size, size_t _requested_size, Type _alloc_type, ShenandoahRegionAffiliation affiliation) :\n@@ -67,1 +72,1 @@\n-          _actual_size(0), _alloc_type(_alloc_type)\n+          _actual_size(0), _alloc_type(_alloc_type), _affiliation(affiliation)\n@@ -75,1 +80,1 @@\n-    return ShenandoahAllocRequest(min_size, requested_size, _alloc_tlab);\n+    return ShenandoahAllocRequest(min_size, requested_size, _alloc_tlab, ShenandoahRegionAffiliation::YOUNG_GENERATION);\n@@ -79,1 +84,1 @@\n-    return ShenandoahAllocRequest(min_size, requested_size, _alloc_gclab);\n+    return ShenandoahAllocRequest(min_size, requested_size, _alloc_gclab, ShenandoahRegionAffiliation::YOUNG_GENERATION);\n@@ -82,2 +87,6 @@\n-  static inline ShenandoahAllocRequest for_shared_gc(size_t requested_size) {\n-    return ShenandoahAllocRequest(0, requested_size, _alloc_shared_gc);\n+  static inline ShenandoahAllocRequest for_plab(size_t min_size, size_t requested_size) {\n+    return ShenandoahAllocRequest(min_size, requested_size, _alloc_plab, ShenandoahRegionAffiliation::OLD_GENERATION);\n+  }\n+\n+  static inline ShenandoahAllocRequest for_shared_gc(size_t requested_size, ShenandoahRegionAffiliation affiliation) {\n+    return ShenandoahAllocRequest(0, requested_size, _alloc_shared_gc, affiliation);\n@@ -87,1 +96,1 @@\n-    return ShenandoahAllocRequest(0, requested_size, _alloc_shared);\n+    return ShenandoahAllocRequest(0, requested_size, _alloc_shared, ShenandoahRegionAffiliation::YOUNG_GENERATION);\n@@ -126,0 +135,1 @@\n+      case _alloc_plab:\n@@ -140,0 +150,1 @@\n+      case _alloc_plab:\n@@ -152,0 +163,1 @@\n+      case _alloc_plab:\n@@ -161,0 +173,12 @@\n+\n+  bool is_old() {\n+    return _affiliation == OLD_GENERATION;\n+  }\n+\n+  bool is_young() {\n+    return _affiliation == YOUNG_GENERATION;\n+  }\n+\n+  ShenandoahRegionAffiliation affiliation() const {\n+    return _affiliation;\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAllocRequest.hpp","additions":33,"deletions":9,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -72,0 +72,7 @@\n+  \/\/ We use this as the time period for tracking minimum mutator utilization (MMU).\n+  \/\/ In generational mode, the MMU is used as a signal to adjust the size of the\n+  \/\/ young generation.\n+  if (FLAG_IS_DEFAULT(GCPauseIntervalMillis)) {\n+    FLAG_SET_DEFAULT(GCPauseIntervalMillis, 5000);\n+  }\n+\n@@ -181,0 +188,2 @@\n+  CardTable::initialize_card_size();\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahArguments.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -74,0 +74,3 @@\n+  if (heap->mode()->is_generational() && !obj->is_forwarded()) {\n+    msg.append(\"  age: %d\\n\", obj->age());\n+  }\n@@ -388,1 +391,1 @@\n-  ShenandoahMessageBuffer msg(\"Must ba at a Shenandoah safepoint or held %s lock\", lock->name());\n+  ShenandoahMessageBuffer msg(\"Must be at a Shenandoah safepoint or held %s lock\", lock->name());\n@@ -428,0 +431,16 @@\n+\n+\/\/ unlike assert_heaplocked_or_safepoint(), this does not require current thread in safepoint to be a VM-thread\n+void ShenandoahAsserts::assert_heaplocked_or_fullgc_safepoint(const char* file, int line) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  if (heap->lock()->owned_by_self()) {\n+    return;\n+  }\n+\n+  if (ShenandoahSafepoint::is_at_shenandoah_safepoint()) {\n+    return;\n+  }\n+\n+  ShenandoahMessageBuffer msg(\"Heap lock must be owned by current thread, or be at safepoint\");\n+  report_vm_error(file, line, msg.buffer());\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.cpp","additions":20,"deletions":1,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -75,0 +75,1 @@\n+  static void assert_heaplocked_or_fullgc_safepoint(const char* file, int line);\n@@ -166,0 +167,8 @@\n+\n+#define shenandoah_assert_heaplocked_or_fullgc_safepoint() \\\n+                    ShenandoahAsserts::assert_heaplocked_or_fullgc_safepoint(__FILE__, __LINE__)\n+#define shenandoah_assert_control_or_vm_thread() \\\n+                    assert(Thread::current()->is_VM_thread() || Thread::current() == ShenandoahHeap::heap()->control_thread(), \"Expected control thread or vm thread\")\n+\n+#define shenandoah_assert_generational() \\\n+                    assert(ShenandoahHeap::heap()->mode()->is_generational(), \"Must be in generational mode here.\")\n@@ -216,0 +225,3 @@\n+#define shenandoah_assert_heaplocked_or_fullgc_safepoint()\n+#define shenandoah_assert_control_or_vm_thread()\n+#define shenandoah_assert_generational()\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-ShenandoahBarrierSet::ShenandoahBarrierSet(ShenandoahHeap* heap) :\n+ShenandoahBarrierSet::ShenandoahBarrierSet(ShenandoahHeap* heap, MemRegion heap_region) :\n@@ -55,0 +55,4 @@\n+  if (heap->mode()->is_generational()) {\n+    _card_table = new ShenandoahCardTable(heap_region);\n+    _card_table->initialize();\n+  }\n@@ -127,0 +131,8 @@\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    \/\/ CAUTION: retire_plab may register the remnant filler object with the remembered set scanner without a lock.\n+    \/\/ This is safe iff it is assured that each PLAB is a whole-number multiple of card-mark memory size and each\n+    \/\/ PLAB is aligned with the start of each card's memory range.\n+    if (plab != nullptr) {\n+      _heap->retire_plab(plab);\n+    }\n+\n@@ -145,0 +157,21 @@\n+\n+void ShenandoahBarrierSet::write_ref_array(HeapWord* start, size_t count) {\n+  if (!_heap->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  HeapWord* end = (HeapWord*)((char*) start + (count * heapOopSize));\n+  \/\/ In the case of compressed oops, start and end may potentially be misaligned;\n+  \/\/ so we need to conservatively align the first downward (this is not\n+  \/\/ strictly necessary for current uses, but a case of good hygiene and,\n+  \/\/ if you will, aesthetics) and the second upward (this is essential for\n+  \/\/ current uses) to a HeapWord boundary, so we mark all cards overlapping\n+  \/\/ this write.\n+  HeapWord* aligned_start = align_down(start, HeapWordSize);\n+  HeapWord* aligned_end   = align_up  (end,   HeapWordSize);\n+  \/\/ If compressed oops were not being used, these should already be aligned\n+  assert(UseCompressedOops || (aligned_start == start && aligned_end == end),\n+         \"Expected heap word alignment of start and end\");\n+  _heap->card_scan()->mark_range_as_dirty(aligned_start, (aligned_end - aligned_start));\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.cpp","additions":34,"deletions":1,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -37,0 +38,1 @@\n+  ShenandoahCardTable* _card_table;\n@@ -41,1 +43,1 @@\n-  ShenandoahBarrierSet(ShenandoahHeap* heap);\n+  ShenandoahBarrierSet(ShenandoahHeap* heap, MemRegion heap_region);\n@@ -49,0 +51,2 @@\n+  inline ShenandoahCardTable* card_table()  { return _card_table; }\n+\n@@ -114,0 +118,5 @@\n+  template <DecoratorSet decorators, typename T>\n+  void write_ref_field_post(T* field, oop newVal);\n+\n+  void write_ref_array(HeapWord* start, size_t count);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.hpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shared\/cardTable.hpp\"\n@@ -39,0 +40,2 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n@@ -106,0 +109,1 @@\n+      _heap->is_in_active_generation(obj) &&\n@@ -113,0 +117,1 @@\n+      _heap->is_in_active_generation(obj) &&\n@@ -182,0 +187,8 @@\n+template <DecoratorSet decorators, typename T>\n+inline void ShenandoahBarrierSet::write_ref_field_post(T* field, oop newVal) {\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    volatile CardTable::CardValue* byte = card_table()->byte_for(field);\n+    *byte = CardTable::dirty_card_val();\n+  }\n+}\n+\n@@ -245,1 +258,2 @@\n-  shenandoah_assert_marked_if(nullptr, value, !CompressedOops::is_null(value) && ShenandoahHeap::heap()->is_evacuation_in_progress());\n+  shenandoah_assert_marked_if(nullptr, value, !CompressedOops::is_null(value) && ShenandoahHeap::heap()->is_evacuation_in_progress() &&\n+                              !(ShenandoahHeap::heap()->is_gc_generation_young() && ShenandoahHeap::heap()->heap_region_containing(value)->is_old()));\n@@ -266,0 +280,1 @@\n+  ShenandoahBarrierSet::barrier_set()->write_ref_field_post<decorators>(addr, value);\n@@ -286,1 +301,3 @@\n-  return bs->oop_cmpxchg(decorators, addr, compare_value, new_value);\n+  oop result = bs->oop_cmpxchg(decorators, addr, compare_value, new_value);\n+  bs->write_ref_field_post<decorators>(addr, new_value);\n+  return result;\n@@ -294,1 +311,4 @@\n-  return bs->oop_cmpxchg(resolved_decorators, AccessInternal::oop_field_addr<decorators>(base, offset), compare_value, new_value);\n+  auto addr = AccessInternal::oop_field_addr<decorators>(base, offset);\n+  oop result = bs->oop_cmpxchg(resolved_decorators, addr, compare_value, new_value);\n+  bs->write_ref_field_post<decorators>(addr, new_value);\n+  return result;\n@@ -310,1 +330,3 @@\n-  return bs->oop_xchg(decorators, addr, new_value);\n+  oop result = bs->oop_xchg(decorators, addr, new_value);\n+  bs->write_ref_field_post<decorators>(addr, new_value);\n+  return result;\n@@ -318,1 +340,4 @@\n-  return bs->oop_xchg(resolved_decorators, AccessInternal::oop_field_addr<decorators>(base, offset), new_value);\n+  auto addr = AccessInternal::oop_field_addr<decorators>(base, offset);\n+  oop result = bs->oop_xchg(resolved_decorators, addr, new_value);\n+  bs->write_ref_field_post<decorators>(addr, new_value);\n+  return result;\n@@ -335,0 +360,3 @@\n+  T* src = arrayOopDesc::obj_offset_to_raw(src_obj, src_offset_in_bytes, src_raw);\n+  T* dst = arrayOopDesc::obj_offset_to_raw(dst_obj, dst_offset_in_bytes, dst_raw);\n+\n@@ -336,4 +364,4 @@\n-  bs->arraycopy_barrier(arrayOopDesc::obj_offset_to_raw(src_obj, src_offset_in_bytes, src_raw),\n-                        arrayOopDesc::obj_offset_to_raw(dst_obj, dst_offset_in_bytes, dst_raw),\n-                        length);\n-  return Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);\n+  bs->arraycopy_barrier(src, dst, length);\n+  bool result = Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);\n+  bs->write_ref_array((HeapWord*) dst, length);\n+  return result;\n@@ -344,1 +372,3 @@\n-  assert(HAS_FWD == _heap->has_forwarded_objects(), \"Forwarded object status is sane\");\n+  \/\/ We allow forwarding in young generation and marking in old generation\n+  \/\/ to happen simultaneously.\n+  assert(_heap->mode()->is_generational() || HAS_FWD == _heap->has_forwarded_objects(), \"Forwarded object status is sane\");\n@@ -364,1 +394,1 @@\n-      if (ENQUEUE && !ctx->is_marked_strong(obj)) {\n+      if (ENQUEUE && !ctx->is_marked_strong_or_old(obj)) {\n@@ -377,1 +407,1 @@\n-  if ((gc_state & ShenandoahHeap::MARKING) != 0) {\n+  if ((gc_state & ShenandoahHeap::YOUNG_MARKING) != 0) {\n@@ -379,1 +409,4 @@\n-  } else if ((gc_state & ShenandoahHeap::EVACUATION) != 0) {\n+    return;\n+  }\n+\n+  if ((gc_state & ShenandoahHeap::EVACUATION) != 0) {\n@@ -384,0 +417,11 @@\n+\n+  if (_heap->mode()->is_generational()) {\n+    assert(ShenandoahSATBBarrier, \"Generational mode assumes SATB mode\");\n+    \/\/ TODO: Could we optimize here by checking that dst is in an old region?\n+    if ((gc_state & ShenandoahHeap::OLD_MARKING) != 0) {\n+      \/\/ Note that we can't do the arraycopy marking using the 'src' array when\n+      \/\/ SATB mode is enabled (so we can't do this as part of the iteration for\n+      \/\/ evacuation or update references).\n+      arraycopy_marking(src, dst, count);\n+    }\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":57,"deletions":13,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -107,0 +107,4 @@\n+  \/\/ We only need to handle YOUNG_MARKING here because the clone barrier\n+  \/\/ is only invoked during marking if Shenandoah is in incremental update\n+  \/\/ mode. OLD_MARKING should only happen when Shenandoah is in generational\n+  \/\/ mode, which uses the SATB write barrier.\n@@ -108,1 +112,1 @@\n-  if ((gc_state & ShenandoahHeap::MARKING) != 0) {\n+  if ((gc_state & ShenandoahHeap::YOUNG_MARKING) != 0) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSetClone.inline.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -0,0 +1,43 @@\n+\/*\n+ * Copyright (c) 2022, Amazon.com, Inc. or its affiliates.  All rights reserved.\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahCardStats.hpp\"\n+#include \"logging\/log.hpp\"\n+\n+#ifndef PRODUCT\n+void ShenandoahCardStats::log() const {\n+  if (ShenandoahEnableCardStats) {\n+    log_info(gc,remset)(\"Card stats: dirty \" SIZE_FORMAT \" (max run: \" SIZE_FORMAT \"),\"\n+      \" clean \" SIZE_FORMAT \" (max run: \" SIZE_FORMAT \"),\"\n+      \" dirty scans\/objs \" SIZE_FORMAT,\n+      _dirty_card_cnt, _max_dirty_run, _clean_card_cnt, _max_clean_run,\n+      _dirty_scan_obj_cnt);\n+  }\n+}\n+#endif \/\/ !PRODUCT\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardStats.cpp","additions":43,"deletions":0,"binary":false,"changes":43,"status":"added"},{"patch":"@@ -0,0 +1,136 @@\n+\/*\n+ * Copyright (c) 2022, Amazon.com, Inc. or its affiliates.  All rights reserved.\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHCARDSTATS_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHCARDSTATS_HPP\n+\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shenandoah\/shenandoahNumberSeq.hpp\"\n+\n+enum CardStatType {\n+  DIRTY_RUN = 0,\n+  CLEAN_RUN = 1,\n+  DIRTY_CARDS = 2,\n+  CLEAN_CARDS = 3,\n+  MAX_DIRTY_RUN = 4,\n+  MAX_CLEAN_RUN = 5,\n+  DIRTY_SCAN_OBJS = 6,\n+  ALTERNATIONS = 7,\n+  MAX_CARD_STAT_TYPE = 8\n+};\n+\n+enum CardStatLogType {\n+  CARD_STAT_SCAN_RS = 0,\n+  CARD_STAT_UPDATE_REFS = 1,\n+  MAX_CARD_STAT_LOG_TYPE = 2\n+};\n+\n+class ShenandoahCardStats: public CHeapObj<mtGC> {\n+private:\n+  size_t _cards_in_cluster;\n+  HdrSeq* _local_card_stats;\n+\n+  size_t _dirty_card_cnt;\n+  size_t _clean_card_cnt;\n+\n+  size_t _dirty_run;\n+  size_t _clean_run;\n+\n+  size_t _max_dirty_run;\n+  size_t _max_clean_run;\n+\n+  size_t _dirty_scan_obj_cnt;\n+\n+  size_t _alternation_cnt;\n+\n+public:\n+  ShenandoahCardStats(size_t cards_in_cluster, HdrSeq* card_stats) :\n+    _cards_in_cluster(cards_in_cluster),\n+    _local_card_stats(card_stats),\n+    _dirty_card_cnt(0),\n+    _clean_card_cnt(0),\n+    _max_dirty_run(0),\n+    _max_clean_run(0),\n+    _dirty_scan_obj_cnt(0),\n+    _alternation_cnt(0)\n+  { }\n+\n+  ~ShenandoahCardStats() {\n+    record();\n+   }\n+\n+   void record() {\n+    if (ShenandoahEnableCardStats) {\n+      \/\/ Update global stats for distribution of dirty\/clean cards as a percentage of chunk\n+      _local_card_stats[DIRTY_CARDS].add((double)_dirty_card_cnt*100\/(double)_cards_in_cluster);\n+      _local_card_stats[CLEAN_CARDS].add((double)_clean_card_cnt*100\/(double)_cards_in_cluster);\n+\n+      \/\/ Update global stats for max dirty\/clean run distribution as a percentage of chunk\n+      _local_card_stats[MAX_DIRTY_RUN].add((double)_max_dirty_run*100\/(double)_cards_in_cluster);\n+      _local_card_stats[MAX_CLEAN_RUN].add((double)_max_clean_run*100\/(double)_cards_in_cluster);\n+\n+      \/\/ Update global stats for dirty obj scan counts\n+      _local_card_stats[DIRTY_SCAN_OBJS].add(_dirty_scan_obj_cnt);\n+\n+      \/\/ Update global stats for alternation counts\n+      _local_card_stats[ALTERNATIONS].add(_alternation_cnt);\n+    }\n+  }\n+\n+public:\n+  inline void record_dirty_run(size_t len) {\n+    if (ShenandoahEnableCardStats) {\n+      _alternation_cnt++;\n+      if (len > _max_dirty_run) {\n+        _max_dirty_run = len;\n+      }\n+      _dirty_card_cnt += len;\n+      assert(len <= _cards_in_cluster, \"Error\");\n+      _local_card_stats[DIRTY_RUN].add((double)len*100.0\/(double)_cards_in_cluster);\n+    }\n+  }\n+\n+  inline void record_clean_run(size_t len) {\n+    if (ShenandoahEnableCardStats) {\n+      _alternation_cnt++;\n+      if (len > _max_clean_run) {\n+        _max_clean_run = len;\n+      }\n+      _clean_card_cnt += len;\n+      assert(len <= _cards_in_cluster, \"Error\");\n+      _local_card_stats[CLEAN_RUN].add((double)len*100.0\/(double)_cards_in_cluster);\n+    }\n+  }\n+\n+  inline void record_scan_obj_cnt(size_t i) {\n+    if (ShenandoahEnableCardStats) {\n+      _dirty_scan_obj_cnt += i;\n+    }\n+  }\n+\n+  void log() const PRODUCT_RETURN;\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHCARDSTATS_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardStats.hpp","additions":136,"deletions":0,"binary":false,"changes":136,"status":"added"},{"patch":"@@ -0,0 +1,119 @@\n+\/*\n+ * Copyright (c) 2020, 2021, Amazon.com, Inc. and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+\n+void ShenandoahCardTable::initialize() {\n+  CardTable::initialize();\n+  _write_byte_map = _byte_map;\n+  _write_byte_map_base = _byte_map_base;\n+  const size_t rs_align = _page_size == (size_t) os::vm_page_size() ? 0 :\n+    MAX2(_page_size, (size_t) os::vm_allocation_granularity());\n+\n+  ReservedSpace heap_rs(_byte_map_size, rs_align, _page_size);\n+  if (!heap_rs.is_reserved()) {\n+    vm_exit_during_initialization(\"Could not reserve enough space for second copy of card marking array\");\n+  }\n+  os::commit_memory_or_exit(heap_rs.base(), _byte_map_size, rs_align, false, \"Cannot commit memory for second copy of card table\");\n+\n+  HeapWord* low_bound  = _whole_heap.start();\n+  _read_byte_map = (CardValue*) heap_rs.base();\n+  _read_byte_map_base = _read_byte_map - (uintptr_t(low_bound) >> card_shift());\n+\n+  log_trace(gc, barrier)(\"ShenandoahCardTable::ShenandoahCardTable: \");\n+  log_trace(gc, barrier)(\"    &_read_byte_map[0]: \" INTPTR_FORMAT \"  &_read_byte_map[_last_valid_index]: \" INTPTR_FORMAT,\n+                  p2i(&_read_byte_map[0]), p2i(&_read_byte_map[last_valid_index()]));\n+  log_trace(gc, barrier)(\"    _read_byte_map_base: \" INTPTR_FORMAT, p2i(_read_byte_map_base));\n+\n+  \/\/ TODO: As currently implemented, we do not swap pointers between _read_byte_map and _write_byte_map\n+  \/\/ because the mutator write barrier hard codes the address of the _write_byte_map_base.  Instead,\n+  \/\/ the current implementation simply copies contents of _write_byte_map onto _read_byte_map and cleans\n+  \/\/ the entirety of _write_byte_map at the init_mark safepoint.\n+  \/\/\n+  \/\/ If we choose to modify the mutator write barrier so that we can swap _read_byte_map_base and\n+  \/\/ _write_byte_map_base pointers, we may also have to figure out certain details about how the\n+  \/\/ _guard_region is implemented so that we can replicate the read and write versions of this region.\n+  \/\/\n+  \/\/ Alternatively, we may switch to a SATB-based write barrier and replace the direct card-marking\n+  \/\/ remembered set with something entirely different.\n+\n+  resize_covered_region(_whole_heap);\n+}\n+\n+bool ShenandoahCardTable::is_in_young(const void* obj) const {\n+  return ShenandoahHeap::heap()->is_in_young(obj);\n+}\n+\n+bool ShenandoahCardTable::is_dirty(MemRegion mr) {\n+  for (size_t i = index_for(mr.start()); i <= index_for(mr.end() - 1); i++) {\n+    CardValue* byte = byte_for_index(i);\n+    if (*byte == CardTable::dirty_card_val()) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+size_t ShenandoahCardTable::last_valid_index() {\n+  return CardTable::last_valid_index();\n+}\n+\n+void ShenandoahCardTable::clear() {\n+  CardTable::clear(_whole_heap);\n+}\n+\n+\/\/ TODO: This service is not currently used because we are not able to swap _read_byte_map_base and\n+\/\/ _write_byte_map_base pointers.  If we were able to do so, we would invoke clear_read_table \"immediately\"\n+\/\/ following the end of concurrent remembered set scanning so that this read card table would be ready\n+\/\/ to serve as the new write card table at the time these pointer values were next swapped.\n+\/\/\n+\/\/ In the current implementation, the write-table is cleared immediately after its contents is copied to\n+\/\/ the read table, obviating the need for this service.\n+void ShenandoahCardTable::clear_read_table() {\n+  for (size_t i = 0; i < _byte_map_size; i++) {\n+    _read_byte_map[i] = clean_card;\n+  }\n+}\n+\n+\/\/ TODO: This service is not currently used because the mutator write barrier implementation hard codes the\n+\/\/ location of the _write_byte_may_base.  If we change the mutator's write barrier implementation, then we\n+\/\/ may use this service to exchange the roles of the read-card-table and write-card-table.\n+void ShenandoahCardTable::swap_card_tables() {\n+  shenandoah_assert_safepoint();\n+\n+  CardValue* save_value = _read_byte_map;\n+  _read_byte_map = _write_byte_map;\n+  _write_byte_map = save_value;\n+\n+  save_value = _read_byte_map_base;\n+  _read_byte_map_base = _write_byte_map_base;\n+  _write_byte_map_base = save_value;\n+\n+  \/\/ update the superclass instance variables\n+  _byte_map = _write_byte_map;\n+  _byte_map_base = _write_byte_map_base;\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardTable.cpp","additions":119,"deletions":0,"binary":false,"changes":119,"status":"added"},{"patch":"@@ -0,0 +1,93 @@\n+\/*\n+ * Copyright (c) 2020, 2021, Amazon.com, Inc. and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHCARDTABLE_HPP\n+#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHCARDTABLE_HPP\n+\n+#include \"gc\/g1\/g1RegionToSpaceMapper.hpp\"\n+#include \"gc\/shared\/cardTable.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+class ShenandoahCardTable: public CardTable {\n+  friend class VMStructs;\n+\n+protected:\n+  \/\/ We maintain two copies of the card table to facilitate concurrent remembered set scanning\n+  \/\/ and concurrent clearing of stale remembered set information.  During the init_mark safepoint,\n+  \/\/ we copy the contents of _write_byte_map to _read_byte_map and clear _write_byte_map.\n+  \/\/\n+  \/\/ Concurrent remembered set scanning reads from _read_byte_map while concurrent mutator write\n+  \/\/ barriers are overwriting cards of the _write_byte_map with DIRTY codes.  Concurrent remembered\n+  \/\/ set scanning also overwrites cards of the _write_byte_map with DIRTY codes whenever it discovers\n+  \/\/ interesting pointers.\n+  \/\/\n+  \/\/ During a concurrent update-references phase, we scan the _write_byte_map concurrently to find\n+  \/\/ all old-gen references that may need to be updated.\n+  \/\/\n+  \/\/ In a future implementation, we may swap the values of _read_byte_map and _write_byte_map during\n+  \/\/ the init-mark safepoint to avoid the need for bulk STW copying and initialization.  Doing so\n+  \/\/ requires a change to the implementation of mutator write barriers as the address of the card\n+  \/\/ table is currently in-lined and hard-coded.\n+  CardValue* _read_byte_map;\n+  CardValue* _write_byte_map;\n+  CardValue* _read_byte_map_base;\n+  CardValue* _write_byte_map_base;\n+\n+public:\n+  ShenandoahCardTable(MemRegion whole_heap) : CardTable(whole_heap) { }\n+\n+  virtual void initialize();\n+\n+  virtual bool is_in_young(const void* obj) const;\n+\n+  bool is_dirty(MemRegion mr);\n+\n+  size_t last_valid_index();\n+\n+  void clear();\n+\n+  void clear_read_table();\n+\n+  \/\/ Exchange the roles of the read and write card tables.\n+  void swap_card_tables();\n+\n+  CardValue* read_byte_map() {\n+    return _read_byte_map;\n+  }\n+\n+  CardValue* write_byte_map() {\n+    return _write_byte_map;\n+  }\n+\n+  CardValue* read_byte_map_base() {\n+    return _read_byte_map_base;\n+  }\n+\n+  CardValue* write_byte_map_base() {\n+    return _write_byte_map_base;\n+  }\n+};\n+\n+#endif \/\/ SHARE_VM_GC_SHENANDOAH_SHENANDOAHCARDTABLE_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardTable.hpp","additions":93,"deletions":0,"binary":false,"changes":93,"status":"added"},{"patch":"@@ -51,1 +51,1 @@\n-  return _mark_context->is_marked(obj);\n+  return _mark_context->is_marked_or_old(obj);\n@@ -63,1 +63,1 @@\n-  return _mark_context->is_marked(obj);\n+  return _mark_context->is_marked_or_old(obj);\n@@ -91,1 +91,1 @@\n-  assert(!ShenandoahHeap::heap()->has_forwarded_objects(), \"Not expected\");\n+  assert(ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress() || !ShenandoahHeap::heap()->has_forwarded_objects(), \"Not expected\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahClosures.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+  _has_old_regions(false),\n@@ -45,0 +46,1 @@\n+  _live(0),\n@@ -46,0 +48,1 @@\n+  _old_garbage(0),\n@@ -86,0 +89,2 @@\n+  assert(!r->is_humongous(), \"Only add regular regions to the collection set\");\n+\n@@ -87,0 +92,13 @@\n+\n+  if (r->affiliation() == YOUNG_GENERATION) {\n+    _young_region_count++;\n+    _young_bytes_to_evacuate += r->get_live_data_bytes();\n+    if (r->age() >= InitialTenuringThreshold) {\n+      _young_bytes_to_promote += r->get_live_data_bytes();\n+    }\n+  } else if (r->affiliation() == OLD_GENERATION) {\n+    _old_region_count++;\n+    _old_bytes_to_evacuate += r->get_live_data_bytes();\n+    _old_garbage += r->garbage();\n+  }\n+\n@@ -88,0 +106,1 @@\n+  _has_old_regions |= r->is_old();\n@@ -90,1 +109,1 @@\n-\n+  _live += r->get_live_data_bytes();\n@@ -106,0 +125,1 @@\n+  _old_garbage = 0;\n@@ -107,0 +127,1 @@\n+  _live = 0;\n@@ -110,0 +131,9 @@\n+\n+  _young_region_count = 0;\n+  _young_bytes_to_evacuate = 0;\n+  _young_bytes_to_promote = 0;\n+\n+  _old_region_count = 0;\n+  _old_bytes_to_evacuate = 0;\n+\n+  _has_old_regions = false;\n@@ -153,1 +183,5 @@\n-  out->print_cr(\"Collection Set : \" SIZE_FORMAT \"\", count());\n+  out->print_cr(\"Collection Set: Regions: \"\n+                SIZE_FORMAT \", Garbage: \" SIZE_FORMAT \"%s, Live: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s\", count(),\n+                byte_size_in_proper_unit(garbage()), proper_unit_for_byte_size(garbage()),\n+                byte_size_in_proper_unit(live()), proper_unit_for_byte_size(live()),\n+                byte_size_in_proper_unit(used()), proper_unit_for_byte_size(used()));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.cpp","additions":36,"deletions":2,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+  bool                  _has_old_regions;\n@@ -48,0 +49,1 @@\n+  size_t                _live;\n@@ -49,0 +51,14 @@\n+  size_t                _immediate_trash;\n+\n+  size_t                _young_bytes_to_evacuate;\n+  size_t                _young_bytes_to_promote;\n+  size_t                _old_bytes_to_evacuate;\n+\n+  size_t                _young_region_count;\n+  size_t                _old_region_count;\n+\n+  size_t                _old_garbage;        \/\/ How many bytes of old garbage are present in a mixed collection set?\n+\n+  bool*                 _preselected_regions;   \/\/ Points to array identifying which tenure-age regions have been preselected\n+                                                \/\/ for inclusion in collection set.  This field is only valid during brief\n+                                                \/\/ spans of time while collection set is being constructed.\n@@ -80,2 +96,30 @@\n-  size_t used()      const { return _used; }\n-  size_t garbage()   const { return _garbage;   }\n+  inline size_t get_immediate_trash();\n+  inline void set_immediate_trash(size_t immediate_trash);\n+\n+  \/\/ This represents total amount of work to be performed by evacuation, including evacuations to young, to old,\n+  \/\/ and promotions from young to old.  This equals get_young_bytes_reserved_for_evacuation() plus\n+  \/\/ get_old_bytes_reserved_for_evacuation().\n+  inline size_t get_bytes_reserved_for_evacuation();\n+\n+  \/\/ It is not known how many of these bytes will be promoted.\n+  inline size_t get_young_bytes_reserved_for_evacuation();\n+\n+  inline size_t get_old_bytes_reserved_for_evacuation();\n+\n+  inline size_t get_young_bytes_to_be_promoted();\n+\n+  inline size_t get_old_region_count();\n+\n+  inline size_t get_young_region_count();\n+\n+  inline size_t get_old_garbage();\n+\n+  void establish_preselected(bool *preselected) { _preselected_regions = preselected; }\n+  void abandon_preselected() { _preselected_regions = nullptr; }\n+  bool is_preselected(size_t region_idx) { return (_preselected_regions != nullptr) && _preselected_regions[region_idx]; }\n+\n+  bool has_old_regions() const { return _has_old_regions; }\n+  size_t used()          const { return _used; }\n+  size_t live()          const { return _live; }\n+  size_t garbage()       const { return _garbage; }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.hpp","additions":46,"deletions":2,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -56,0 +56,36 @@\n+void ShenandoahCollectionSet::set_immediate_trash(size_t immediate_trash) {\n+  _immediate_trash = immediate_trash;\n+}\n+\n+size_t ShenandoahCollectionSet::get_immediate_trash() {\n+  return _immediate_trash;\n+}\n+\n+size_t ShenandoahCollectionSet::get_old_bytes_reserved_for_evacuation() {\n+  return _old_bytes_to_evacuate;\n+}\n+\n+size_t ShenandoahCollectionSet::get_young_bytes_reserved_for_evacuation() {\n+  return _young_bytes_to_evacuate - _young_bytes_to_promote;\n+}\n+\n+size_t ShenandoahCollectionSet::get_young_bytes_to_be_promoted() {\n+  return _young_bytes_to_promote;\n+}\n+\n+size_t ShenandoahCollectionSet::get_bytes_reserved_for_evacuation() {\n+  return _young_bytes_to_evacuate + _old_bytes_to_evacuate;\n+}\n+\n+size_t ShenandoahCollectionSet::get_old_region_count() {\n+  return _old_region_count;\n+}\n+\n+size_t ShenandoahCollectionSet::get_young_region_count() {\n+  return _young_region_count;\n+}\n+\n+size_t ShenandoahCollectionSet::get_old_garbage() {\n+  return _old_garbage;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.inline.hpp","additions":36,"deletions":0,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -34,0 +34,4 @@\n+  _mixed_gcs(0),\n+  _abbreviated_cycles(0),\n+  _success_old_gcs(0),\n+  _interrupted_old_gcs(0),\n@@ -78,0 +82,1 @@\n+  ShenandoahHeap::heap()->record_upgrade_to_full();\n@@ -85,0 +90,16 @@\n+void ShenandoahCollectorPolicy::record_mixed_cycle() {\n+  _mixed_gcs++;\n+}\n+\n+void ShenandoahCollectorPolicy::record_abbreviated_cycle() {\n+  _abbreviated_cycles++;\n+}\n+\n+void ShenandoahCollectorPolicy::record_success_old() {\n+  _success_old_gcs++;\n+}\n+\n+void ShenandoahCollectorPolicy::record_interrupted_old() {\n+  _interrupted_old_gcs++;\n+}\n+\n@@ -113,1 +134,2 @@\n-  out->print_cr(\"to avoid Degenerated and Full GC cycles.\");\n+  out->print_cr(\"to avoid Degenerated and Full GC cycles. Abbreviated cycles are those which found\");\n+  out->print_cr(\"enough regions with no live objects to skip evacuation.\");\n@@ -116,1 +138,1 @@\n-  out->print_cr(SIZE_FORMAT_W(5) \" successful concurrent GCs\",         _success_concurrent_gcs);\n+  out->print_cr(SIZE_FORMAT_W(5) \" Successful Concurrent GCs\",         _success_concurrent_gcs);\n@@ -121,0 +143,5 @@\n+  out->print_cr(SIZE_FORMAT_W(5) \" Completed Old GCs\",                 _success_old_gcs);\n+  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" mixed\",                        _mixed_gcs);\n+  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" interruptions\",                _interrupted_old_gcs);\n+  out->cr();\n+\n@@ -132,0 +159,3 @@\n+  out->print_cr(SIZE_FORMAT_W(5) \" Abbreviated GCs\",                   _abbreviated_cycles);\n+  out->cr();\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectorPolicy.cpp","additions":32,"deletions":2,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -42,0 +42,4 @@\n+  size_t _mixed_gcs;\n+  size_t _abbreviated_cycles;\n+  size_t _success_old_gcs;\n+  size_t _interrupted_old_gcs;\n@@ -51,0 +55,1 @@\n+  size_t _cycle_counter;\n@@ -54,1 +59,0 @@\n-\n@@ -57,2 +61,0 @@\n-  size_t _cycle_counter;\n-\n@@ -66,0 +68,2 @@\n+  void record_mixed_cycle();\n+  void record_abbreviated_cycle();\n@@ -67,0 +71,2 @@\n+  void record_success_old();\n+  void record_interrupted_old();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectorPolicy.hpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -34,0 +34,3 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -88,3 +91,6 @@\n-ShenandoahConcurrentGC::ShenandoahConcurrentGC() :\n-  _mark(),\n-  _degen_point(ShenandoahDegenPoint::_degenerated_unset) {\n+ShenandoahConcurrentGC::ShenandoahConcurrentGC(ShenandoahGeneration* generation, bool do_old_gc_bootstrap) :\n+  _mark(generation),\n+  _degen_point(ShenandoahDegenPoint::_degenerated_unset),\n+  _abbreviated(false),\n+  _do_old_gc_bootstrap(do_old_gc_bootstrap),\n+  _generation(generation) {\n@@ -97,4 +103,0 @@\n-void ShenandoahConcurrentGC::cancel() {\n-  ShenandoahConcurrentMark::cancel();\n-}\n-\n@@ -103,0 +105,2 @@\n+  heap->start_conc_gc();\n+\n@@ -113,0 +117,11 @@\n+\n+    \/\/ Reset task queue stats here, rather than in mark_concurrent_roots\n+    \/\/ because remembered set scan will `push` oops into the queues and\n+    \/\/ resetting after this happens will lose those counts.\n+    TASKQUEUE_STATS_ONLY(_mark.task_queues()->reset_taskqueue_stats());\n+\n+    \/\/ Concurrent remembered set scanning\n+    entry_scan_remembered_set();\n+    \/\/ When RS scanning yields, we will need a check_cancellation_and_abort()\n+    \/\/ degeneration point here.\n+\n@@ -115,1 +130,1 @@\n-    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_outside_cycle)) return false;\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_roots)) return false;\n@@ -125,0 +140,13 @@\n+  \/\/ If GC was cancelled before final mark, then the safepoint operation will do nothing\n+  \/\/ and the concurrent mark will still be in progress. In this case it is safe to resume\n+  \/\/ the degenerated cycle from the marking phase. On the other hand, if the GC is cancelled\n+  \/\/ after final mark (but before this check), then the final mark safepoint operation\n+  \/\/ will have finished the mark (setting concurrent mark in progress to false). Final mark\n+  \/\/ will also have setup state (in concurrent stack processing) that will not be safe to\n+  \/\/ resume from the marking phase in the degenerated cycle. That is, if the cancellation\n+  \/\/ occurred after final mark, we must resume the degenerated cycle after the marking phase.\n+  if (_generation->is_concurrent_mark_in_progress() && check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_mark)) {\n+    assert(!heap->is_concurrent_weak_root_in_progress(), \"Weak roots should not be in progress when concurrent mark is in progress\");\n+    return false;\n+  }\n+\n@@ -137,1 +165,2 @@\n-  \/\/ the space. This would be the last action if there is nothing to evacuate.\n+  \/\/ the space. This would be the last action if there is nothing to evacuate.  Note that\n+  \/\/ we will not age young-gen objects in the case that we skip evacuation.\n@@ -158,0 +187,6 @@\n+  \/\/ Global marking has completed. We need to fill in any unmarked objects in the old generation\n+  \/\/ so that subsequent remembered set scans will not walk pointers into reclaimed memory.\n+  if (!heap->cancelled_gc() && heap->mode()->is_generational() && _generation->generation_mode() == GLOBAL) {\n+    entry_global_coalesce_and_fill();\n+  }\n+\n@@ -180,1 +215,3 @@\n-    vmop_entry_final_roots();\n+    \/\/ We chose not to evacuate because we found sufficient immediate garbage.\n+    vmop_entry_final_roots(heap->is_aging_cycle());\n+    _abbreviated = true;\n@@ -183,0 +220,27 @@\n+  if (heap->mode()->is_generational()) {\n+    size_t old_available, young_available;\n+    {\n+      ShenandoahYoungGeneration* young_gen = heap->young_generation();\n+      ShenandoahGeneration* old_gen = heap->old_generation();\n+      ShenandoahHeapLocker locker(heap->lock());\n+\n+      size_t old_usage_before_evac = heap->capture_old_usage(0);\n+      size_t old_usage_now = old_gen->used();\n+      size_t promoted_bytes = old_usage_now - old_usage_before_evac;\n+      heap->set_previous_promotion(promoted_bytes);\n+\n+      young_gen->unadjust_available();\n+      old_gen->unadjust_available();\n+      \/\/ No need to old_gen->increase_used().\n+      \/\/ That was done when plabs were allocated, accounting for both old evacs and promotions.\n+\n+      young_available = young_gen->adjusted_available();\n+      old_available = old_gen->adjusted_available();\n+\n+      heap->set_alloc_supplement_reserve(0);\n+      heap->set_young_evac_reserve(0);\n+      heap->set_old_evac_reserve(0);\n+      heap->reset_old_evac_expended();\n+      heap->set_promoted_reserve(0);\n+    }\n+  }\n@@ -226,1 +290,1 @@\n-void ShenandoahConcurrentGC::vmop_entry_final_roots() {\n+void ShenandoahConcurrentGC::vmop_entry_final_roots(bool increment_region_ages) {\n@@ -233,1 +297,1 @@\n-  VM_ShenandoahFinalRoots op(this);\n+  VM_ShenandoahFinalRoots op(this, increment_region_ages);\n@@ -238,1 +302,2 @@\n-  const char* msg = init_mark_event_message();\n+  char msg[1024];\n+  init_mark_event_message(msg, sizeof(msg));\n@@ -250,1 +315,2 @@\n-  const char* msg = final_mark_event_message();\n+  char msg[1024];\n+  final_mark_event_message(msg, sizeof(msg));\n@@ -305,0 +371,17 @@\n+void ShenandoahConcurrentGC::entry_scan_remembered_set() {\n+  if (_generation->generation_mode() == YOUNG) {\n+    ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+    TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+    const char* msg = \"Concurrent remembered set scanning\";\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::init_scan_rset);\n+    EventMark em(\"%s\", msg);\n+\n+    ShenandoahWorkerScope scope(heap->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_rs_scanning(),\n+                                msg);\n+\n+    heap->try_inject_alloc_failure();\n+    _generation->scan_remembered_set(true \/* is_concurrent *\/);\n+  }\n+}\n+\n@@ -321,0 +404,1 @@\n+  char msg[1024];\n@@ -323,1 +407,1 @@\n-  const char* msg = conc_mark_event_message();\n+  conc_mark_event_message(msg, sizeof(msg));\n@@ -478,0 +562,15 @@\n+void ShenandoahConcurrentGC::entry_global_coalesce_and_fill() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n+  const char* msg = \"Coalescing and filling old regions in global collect\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::coalesce_and_fill);\n+\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  EventMark em(\"%s\", msg);\n+  ShenandoahWorkerScope scope(heap->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_marking(),\n+                              \"concurrent coalesce and fill\");\n+\n+  op_global_coalesce_and_fill();\n+}\n+\n@@ -483,2 +582,1 @@\n-\n-  heap->prepare_gc();\n+  _generation->prepare_gc();\n@@ -497,1 +595,2 @@\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n+      \/\/ reset, so it is very likely we don't need to do another write here.  Since most regions\n+      \/\/ are not \"active\", this path is relatively rare.\n@@ -519,2 +618,2 @@\n-  assert(heap->marking_context()->is_bitmap_clear(), \"need clear marking bitmap\");\n-  assert(!heap->marking_context()->is_complete(), \"should not be complete\");\n+  assert(_generation->is_bitmap_clear(), \"need clear marking bitmap\");\n+  assert(!_generation->is_mark_complete(), \"should not be complete\");\n@@ -523,0 +622,21 @@\n+\n+  if (heap->mode()->is_generational()) {\n+      if (_generation->generation_mode() == YOUNG || (_generation->generation_mode() == GLOBAL && ShenandoahVerify)) {\n+      \/\/ The current implementation of swap_remembered_set() copies the write-card-table\n+      \/\/ to the read-card-table. The remembered sets are also swapped for GLOBAL collections\n+      \/\/ so that the verifier works with the correct copy of the card table when verifying.\n+        ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_swap_rset);\n+        _generation->swap_remembered_set();\n+    }\n+\n+    if (_generation->generation_mode() == GLOBAL) {\n+      heap->cancel_old_gc();\n+    } else if (heap->is_concurrent_old_mark_in_progress()) {\n+      \/\/ Purge the SATB buffers, transferring any valid, old pointers to the\n+      \/\/ old generation mark queue. Any pointers in a young region will be\n+      \/\/ abandoned.\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_transfer_satb);\n+      heap->transfer_old_pointers_from_satb();\n+    }\n+  }\n+\n@@ -531,1 +651,1 @@\n-  heap->set_concurrent_mark_in_progress(true);\n+  _generation->set_concurrent_mark_in_progress(true);\n@@ -535,1 +655,6 @@\n-  {\n+  if (_do_old_gc_bootstrap) {\n+    \/\/ Update region state for both young and old regions\n+    \/\/ TODO: We should be able to pull this out of the safepoint for the bootstrap\n+    \/\/ cycle. The top of an old region will only move when a GC cycle evacuates\n+    \/\/ objects into it. When we start an old cycle, we know that nothing can touch\n+    \/\/ the top of old regions.\n@@ -539,0 +664,5 @@\n+  } else {\n+    \/\/ Update region state for only young regions\n+    ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_region_states);\n+    ShenandoahInitMarkUpdateRegionStateClosure cl;\n+    _generation->parallel_heap_region_iterate(&cl);\n@@ -542,1 +672,1 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = _generation->ref_processor();\n@@ -548,0 +678,1 @@\n+\n@@ -585,1 +716,29 @@\n-    heap->prepare_regions_and_collection_set(true \/*concurrent*\/);\n+    \/\/ The collection set is chosen by prepare_regions_and_collection_set().\n+    \/\/\n+    \/\/ TODO: Under severe memory overload conditions that can be checked here, we may want to limit\n+    \/\/ the inclusion of old-gen candidates within the collection set.  This would allow us to prioritize efforts on\n+    \/\/ evacuating young-gen,  This remediation is most appropriate when old-gen availability is very high (so there\n+    \/\/ are negligible negative impacts from delaying completion of old-gen evacuation) and when young-gen collections\n+    \/\/ are \"under duress\" (as signalled by very low availability of memory within young-gen, indicating that\/ young-gen\n+    \/\/ collections are not triggering frequently enough).\n+    _generation->prepare_regions_and_collection_set(true \/*concurrent*\/);\n+\n+    \/\/ Upon return from prepare_regions_and_collection_set(), certain parameters have been established to govern the\n+    \/\/ evacuation efforts that are about to begin.  In particular:\n+    \/\/\n+    \/\/ heap->get_promoted_reserve() represents the amount of memory within old-gen's available memory that has\n+    \/\/   been set aside to hold objects promoted from young-gen memory.  This represents an estimated percentage\n+    \/\/   of the live young-gen memory within the collection set.  If there is more data ready to be promoted than\n+    \/\/   can fit within this reserve, the promotion of some objects will be deferred until a subsequent evacuation\n+    \/\/   pass.\n+    \/\/\n+    \/\/ heap->get_old_evac_reserve() represents the amount of memory within old-gen's available memory that has been\n+    \/\/  set aside to hold objects evacuated from the old-gen collection set.\n+    \/\/\n+    \/\/ heap->get_young_evac_reserve() represents the amount of memory within young-gen's available memory that has\n+    \/\/  been set aside to hold objects evacuated from the young-gen collection set.  Conservatively, this value\n+    \/\/  equals the entire amount of live young-gen memory within the collection set, even though some of this memory\n+    \/\/  will likely be promoted.\n+    \/\/\n+    \/\/ heap->get_alloc_supplement_reserve() represents the amount of old-gen memory that can be allocated during evacuation\n+    \/\/ and update-refs phases of gc.  The young evacuation reserve has already been removed from this quantity.\n@@ -591,0 +750,7 @@\n+      LogTarget(Info, gc, ergo) lt;\n+      if (lt.is_enabled()) {\n+        ResourceMark rm;\n+        LogStream ls(lt);\n+        heap->collection_set()->print_on(&ls);\n+      }\n+\n@@ -609,0 +775,14 @@\n+      if (heap->mode()->is_generational()) {\n+        \/\/ Calculate the temporary evacuation allowance supplement to young-gen memory capacity (for allocations\n+        \/\/ and young-gen evacuations).\n+        size_t young_available = heap->young_generation()->adjust_available(heap->get_alloc_supplement_reserve());\n+        \/\/ old_available is memory that can hold promotions and evacuations.  Subtract out the memory that is being\n+        \/\/ loaned for young-gen allocations or evacuations.\n+        size_t old_available = heap->old_generation()->adjust_available(-heap->get_alloc_supplement_reserve());\n+\n+        log_info(gc, ergo)(\"After generational memory budget adjustments, old available: \" SIZE_FORMAT\n+                           \"%s, young_available: \" SIZE_FORMAT \"%s\",\n+                           byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n+                           byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n+      }\n+\n@@ -677,1 +857,1 @@\n-  heap->ref_processor()->process_references(ShenandoahPhaseTimings::conc_weak_refs, heap->workers(), true \/* concurrent *\/);\n+  _generation->ref_processor()->process_references(ShenandoahPhaseTimings::conc_weak_refs, heap->workers(), true \/* concurrent *\/);\n@@ -704,2 +884,9 @@\n-      shenandoah_assert_correct(p, obj);\n-      ShenandoahHeap::atomic_clear_oop(p, obj);\n+      if (_heap->is_in_active_generation(obj)) {\n+        \/\/ TODO: This worries me. Here we are asserting that an unmarked from-space object is 'correct'.\n+        \/\/ Normally, I would call this a bogus assert, but there seems to be a legitimate use-case for\n+        \/\/ accessing from-space objects during class unloading. However, the from-space object may have\n+        \/\/ been \"filled\". We've made no effort to prevent old generation classes being unloaded by young\n+        \/\/ gen (and vice-versa).\n+        shenandoah_assert_correct(p, obj);\n+        ShenandoahHeap::atomic_clear_oop(p, obj);\n+      }\n@@ -930,1 +1117,3 @@\n-\n+  if (ShenandoahVerify) {\n+    heap->verifier()->verify_before_updaterefs();\n+  }\n@@ -975,1 +1164,1 @@\n-    heap->clear_cancelled_gc();\n+    heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n@@ -983,0 +1172,18 @@\n+  if (heap->mode()->is_generational() && heap->is_concurrent_old_mark_in_progress()) {\n+    \/\/ When the SATB barrier is left on to support concurrent old gen mark, it may pick up writes to\n+    \/\/ objects in the collection set. After those objects are evacuated, the pointers in the\n+    \/\/ SATB are no longer safe. Once we have finished update references, we are guaranteed that\n+    \/\/ no more writes to the collection set are possible.\n+    \/\/\n+    \/\/ This will transfer any old pointers in _active_ regions from the SATB to the old gen\n+    \/\/ mark queues. All other pointers will be discarded. This would also discard any pointers\n+    \/\/ in old regions that were included in a mixed evacuation. We aren't using the SATB filter\n+    \/\/ methods here because we cannot control when they execute. If the SATB filter runs _after_\n+    \/\/ a region has been recycled, we will not be able to detect the bad pointer.\n+    \/\/\n+    \/\/ We are not concerned about skipping this step in abbreviated cycles because regions\n+    \/\/ with no live objects cannot have been written to and so cannot have entries in the SATB\n+    \/\/ buffers.\n+    heap->transfer_old_pointers_from_satb();\n+  }\n+\n@@ -988,0 +1195,4 @@\n+  \/\/ Aging_cycle is only relevant during evacuation cycle for individual objects and during final mark for\n+  \/\/ entire regions.  Both of these relevant operations occur before final update refs.\n+  heap->set_aging_cycle(false);\n+\n@@ -997,0 +1208,1 @@\n+  heap->adjust_generation_sizes();\n@@ -1007,0 +1219,4 @@\n+void ShenandoahConcurrentGC::op_global_coalesce_and_fill() {\n+  ShenandoahHeap::heap()->coalesce_and_fill_old_regions();\n+}\n+\n@@ -1015,1 +1231,1 @@\n-const char* ShenandoahConcurrentGC::init_mark_event_message() const {\n+void ShenandoahConcurrentGC::init_mark_event_message(char* buf, size_t len) const {\n@@ -1019,1 +1235,1 @@\n-    return \"Pause Init Mark (unload classes)\";\n+    jio_snprintf(buf, len, \"Pause Init Mark (%s) (unload classes)\", _generation->name());\n@@ -1021,1 +1237,1 @@\n-    return \"Pause Init Mark\";\n+    jio_snprintf(buf, len, \"Pause Init Mark (%s)\", _generation->name());\n@@ -1025,1 +1241,1 @@\n-const char* ShenandoahConcurrentGC::final_mark_event_message() const {\n+void ShenandoahConcurrentGC::final_mark_event_message(char* buf, size_t len) const {\n@@ -1027,1 +1243,2 @@\n-  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  assert(!heap->has_forwarded_objects() || heap->is_concurrent_old_mark_in_progress(),\n+         \"Should not have forwarded objects during final mark (unless old gen concurrent mark is running)\");\n@@ -1029,1 +1246,1 @@\n-    return \"Pause Final Mark (unload classes)\";\n+    jio_snprintf(buf, len, \"Pause Final Mark (%s) (unload classes)\", _generation->name());\n@@ -1031,1 +1248,1 @@\n-    return \"Pause Final Mark\";\n+    jio_snprintf(buf, len, \"Pause Final Mark (%s)\", _generation->name());\n@@ -1035,1 +1252,1 @@\n-const char* ShenandoahConcurrentGC::conc_mark_event_message() const {\n+void ShenandoahConcurrentGC::conc_mark_event_message(char* buf, size_t len) const {\n@@ -1037,1 +1254,2 @@\n-  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  assert(!heap->has_forwarded_objects() || heap->is_concurrent_old_mark_in_progress(),\n+         \"Should not have forwarded objects concurrent mark (unless old gen concurrent mark is running\");\n@@ -1039,1 +1257,1 @@\n-    return \"Concurrent marking (unload classes)\";\n+    jio_snprintf(buf, len, \"Concurrent marking (%s) (unload classes)\", _generation->name());\n@@ -1041,1 +1259,1 @@\n-    return \"Concurrent marking\";\n+    jio_snprintf(buf, len, \"Concurrent marking (%s)\", _generation->name());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":258,"deletions":40,"binary":false,"changes":298,"status":"modified"},{"patch":"@@ -33,0 +33,2 @@\n+class ShenandoahGeneration;\n+\n@@ -45,0 +47,3 @@\n+protected:\n+  ShenandoahConcurrentMark    _mark;\n+\n@@ -46,2 +51,6 @@\n-  ShenandoahConcurrentMark  _mark;\n-  ShenandoahDegenPoint      _degen_point;\n+  ShenandoahDegenPoint        _degen_point;\n+  bool                        _abbreviated;\n+  const bool                  _do_old_gc_bootstrap;\n+\n+protected:\n+  ShenandoahGeneration* const _generation;\n@@ -50,1 +59,1 @@\n-  ShenandoahConcurrentGC();\n+  ShenandoahConcurrentGC(ShenandoahGeneration* generation, bool do_old_gc_bootstrap);\n@@ -53,0 +62,1 @@\n+  bool abbreviated() const { return _abbreviated; }\n@@ -54,2 +64,0 @@\n-  \/\/ Cancel ongoing concurrent GC\n-  static void cancel();\n@@ -60,0 +68,2 @@\n+\n+protected:\n@@ -61,0 +71,3 @@\n+  void vmop_entry_final_roots(bool incr_region_ages);\n+\n+private:\n@@ -63,1 +76,0 @@\n-  void vmop_entry_final_roots();\n@@ -77,0 +89,3 @@\n+  void entry_scan_remembered_set();\n+\n+protected:\n@@ -84,0 +99,3 @@\n+  virtual void op_final_mark();\n+\n+private:\n@@ -88,0 +106,1 @@\n+  void entry_global_coalesce_and_fill();\n@@ -94,1 +113,0 @@\n-  void op_final_mark();\n@@ -108,0 +126,1 @@\n+  void op_global_coalesce_and_fill();\n@@ -113,3 +132,3 @@\n-  const char* init_mark_event_message() const;\n-  const char* final_mark_event_message() const;\n-  const char* conc_mark_event_message() const;\n+  void init_mark_event_message(char* buf, size_t len) const;\n+  void final_mark_event_message(char* buf, size_t len) const;\n+  void conc_mark_event_message(char* buf, size_t len) const;\n@@ -117,0 +136,1 @@\n+protected:\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.hpp","additions":30,"deletions":10,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -42,0 +43,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -47,0 +49,1 @@\n+template <GenerationMode GENERATION>\n@@ -59,1 +62,2 @@\n-    ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+    ShenandoahParallelWorkerSession worker_session(worker_id);\n+    ShenandoahWorkerTimingsTracker timer(ShenandoahPhaseTimings::conc_mark, ShenandoahPhaseTimings::ParallelMark, worker_id, true);\n@@ -61,2 +65,1 @@\n-    ShenandoahObjToScanQueue* q = _cm->get_queue(worker_id);\n-    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+    ShenandoahReferenceProcessor* rp = heap->active_generation()->ref_processor();\n@@ -65,1 +68,1 @@\n-    _cm->mark_loop(worker_id, _terminator, rp,\n+    _cm->mark_loop(GENERATION, worker_id, _terminator, rp,\n@@ -95,0 +98,1 @@\n+template<GenerationMode GENERATION>\n@@ -110,1 +114,0 @@\n-    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n@@ -112,0 +115,1 @@\n+    ShenandoahReferenceProcessor* rp = heap->active_generation()->ref_processor();\n@@ -116,0 +120,1 @@\n+      ShenandoahObjToScanQueue* old = _cm->get_old_queue(worker_id);\n@@ -117,1 +122,1 @@\n-      ShenandoahSATBBufferClosure cl(q);\n+      ShenandoahSATBBufferClosure<GENERATION> cl(q, old);\n@@ -122,1 +127,1 @@\n-      ShenandoahMarkRefsClosure             mark_cl(q, rp);\n+      ShenandoahMarkRefsClosure<GENERATION> mark_cl(q, rp, old);\n@@ -127,1 +132,1 @@\n-    _cm->mark_loop(worker_id, _terminator, rp,\n+    _cm->mark_loop(GENERATION, worker_id, _terminator, rp,\n@@ -135,2 +140,2 @@\n-ShenandoahConcurrentMark::ShenandoahConcurrentMark() :\n-  ShenandoahMark() {}\n+ShenandoahConcurrentMark::ShenandoahConcurrentMark(ShenandoahGeneration* generation) :\n+  ShenandoahMark(generation) {}\n@@ -139,0 +144,1 @@\n+template<GenerationMode GENERATION>\n@@ -144,0 +150,1 @@\n+  ShenandoahObjToScanQueueSet* const  _old_queue_set;\n@@ -148,0 +155,1 @@\n+                                    ShenandoahObjToScanQueueSet* old,\n@@ -154,4 +162,6 @@\n-ShenandoahMarkConcurrentRootsTask::ShenandoahMarkConcurrentRootsTask(ShenandoahObjToScanQueueSet* qs,\n-                                                                     ShenandoahReferenceProcessor* rp,\n-                                                                     ShenandoahPhaseTimings::Phase phase,\n-                                                                     uint nworkers) :\n+template<GenerationMode GENERATION>\n+ShenandoahMarkConcurrentRootsTask<GENERATION>::ShenandoahMarkConcurrentRootsTask(ShenandoahObjToScanQueueSet* qs,\n+                                                                                 ShenandoahObjToScanQueueSet* old,\n+                                                                                 ShenandoahReferenceProcessor* rp,\n+                                                                                 ShenandoahPhaseTimings::Phase phase,\n+                                                                                 uint nworkers) :\n@@ -161,0 +171,1 @@\n+  _old_queue_set(old),\n@@ -165,1 +176,2 @@\n-void ShenandoahMarkConcurrentRootsTask::work(uint worker_id) {\n+template<GenerationMode GENERATION>\n+void ShenandoahMarkConcurrentRootsTask<GENERATION>::work(uint worker_id) {\n@@ -168,1 +180,2 @@\n-  ShenandoahMarkRefsClosure cl(q, _rp);\n+  ShenandoahObjToScanQueue* old = _old_queue_set == nullptr ? nullptr : _old_queue_set->queue(worker_id);\n+  ShenandoahMarkRefsClosure<GENERATION> cl(q, _rp, old);\n@@ -176,2 +189,0 @@\n-  TASKQUEUE_STATS_ONLY(task_queues()->reset_taskqueue_stats());\n-\n@@ -179,5 +190,19 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n-  task_queues()->reserve(workers->active_workers());\n-  ShenandoahMarkConcurrentRootsTask task(task_queues(), rp, ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n-\n-  workers->run_task(&task);\n+  ShenandoahReferenceProcessor* rp = _generation->ref_processor();\n+  _generation->reserve_task_queues(workers->active_workers());\n+  switch (_generation->generation_mode()) {\n+    case YOUNG: {\n+      ShenandoahMarkConcurrentRootsTask<YOUNG> task(task_queues(), old_task_queues(), rp, ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      workers->run_task(&task);\n+      break;\n+    }\n+    case GLOBAL: {\n+      assert(old_task_queues() == nullptr, \"Global mark should not have old gen mark queues.\");\n+      ShenandoahMarkConcurrentRootsTask<GLOBAL> task(task_queues(), old_task_queues(), rp, ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      workers->run_task(&task);\n+      break;\n+    }\n+    default:\n+      \/\/ Intentionally haven't added OLD here. We use a YOUNG generation\n+      \/\/ cycle to bootstrap concurrent old marking.\n+      ShouldNotReachHere();\n+  }\n@@ -208,3 +233,22 @@\n-    TaskTerminator terminator(nworkers, task_queues());\n-    ShenandoahConcurrentMarkingTask task(this, &terminator);\n-    workers->run_task(&task);\n+    switch (_generation->generation_mode()) {\n+      case YOUNG: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<YOUNG> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      case OLD: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<OLD> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      case GLOBAL: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<GLOBAL> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      default:\n+        ShouldNotReachHere();\n+    }\n@@ -237,3 +281,2 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  heap->set_concurrent_mark_in_progress(false);\n-  heap->mark_complete_marking_context();\n+  _generation->set_concurrent_mark_in_progress(false);\n+  _generation->set_mark_complete();\n@@ -259,2 +302,0 @@\n-  ShenandoahFinalMarkingTask task(this, &terminator, ShenandoahStringDedup::is_enabled());\n-  heap->workers()->run_task(&task);\n@@ -262,2 +303,19 @@\n-  assert(task_queues()->is_empty(), \"Should be empty\");\n-}\n+  switch (_generation->generation_mode()) {\n+    case YOUNG:{\n+      ShenandoahFinalMarkingTask<YOUNG> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    case OLD:{\n+      ShenandoahFinalMarkingTask<OLD> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    case GLOBAL:{\n+      ShenandoahFinalMarkingTask<GLOBAL> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -266,4 +324,1 @@\n-void ShenandoahConcurrentMark::cancel() {\n-  clear();\n-  ShenandoahReferenceProcessor* rp = ShenandoahHeap::heap()->ref_processor();\n-  rp->abandon_partial_discovery();\n+  assert(task_queues()->is_empty(), \"Should be empty\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.cpp","additions":92,"deletions":37,"binary":false,"changes":129,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+template <GenerationMode GENERATION>\n@@ -31,0 +32,1 @@\n+template<GenerationMode GENERATION>\n@@ -32,0 +34,1 @@\n+class ShenandoahGeneration;\n@@ -34,2 +37,2 @@\n-  friend class ShenandoahConcurrentMarkingTask;\n-  friend class ShenandoahFinalMarkingTask;\n+  template <GenerationMode GENERATION> friend class ShenandoahConcurrentMarkingTask;\n+  template <GenerationMode GENERATION> friend class ShenandoahFinalMarkingTask;\n@@ -38,1 +41,2 @@\n-  ShenandoahConcurrentMark();\n+  ShenandoahConcurrentMark(ShenandoahGeneration* generation);\n+\n@@ -41,0 +45,1 @@\n+\n@@ -43,0 +48,1 @@\n+\n@@ -46,2 +52,0 @@\n-  static void cancel();\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.hpp","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n@@ -33,0 +34,3 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -38,0 +42,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGC.hpp\"\n@@ -43,0 +48,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -51,2 +57,4 @@\n-  _alloc_failure_waiters_lock(Mutex::safepoint-2, \"ShenandoahAllocFailureGC_lock\", true),\n-  _gc_waiters_lock(Mutex::safepoint-2, \"ShenandoahRequestedGC_lock\", true),\n+  _alloc_failure_waiters_lock(Mutex::safepoint - 2, \"ShenandoahAllocFailureGC_lock\", true),\n+  _gc_waiters_lock(Mutex::safepoint - 2, \"ShenandoahRequestedGC_lock\", true),\n+  _control_lock(Mutex::nosafepoint - 2, \"ShenandoahControlGC_lock\", true),\n+  _regulator_lock(Mutex::nosafepoint - 2, \"ShenandoahRegulatorGC_lock\", true),\n@@ -55,0 +63,1 @@\n+  _requested_generation(GenerationMode::GLOBAL),\n@@ -56,1 +65,3 @@\n-  _allocs_seen(0) {\n+  _degen_generation(nullptr),\n+  _allocs_seen(0),\n+  _mode(none) {\n@@ -84,0 +95,1 @@\n+  GenerationMode generation = GLOBAL;\n@@ -85,1 +97,0 @@\n-  int sleep = ShenandoahControlIntervalMin;\n@@ -88,1 +99,1 @@\n-  double last_sleep_adjust_time = os::elapsedTime();\n+  uint age_period = 0;\n@@ -97,1 +108,6 @@\n-  ShenandoahHeuristics* heuristics = heap->heuristics();\n+\n+  \/\/ Heuristics are notified of allocation failures here and other outcomes\n+  \/\/ of the cycle. They're also used here to control whether the Nth consecutive\n+  \/\/ degenerated cycle should be 'promoted' to a full cycle. The decision to\n+  \/\/ trigger a cycle or not is evaluated on the regulator thread.\n+  ShenandoahHeuristics* global_heuristics = heap->global_generation()->heuristics();\n@@ -104,1 +120,1 @@\n-    bool implicit_gc_requested = is_gc_requested && !is_explicit_gc(requested_gc_cause);\n+    bool implicit_gc_requested = is_gc_requested && is_implicit_gc(requested_gc_cause);\n@@ -113,1 +129,1 @@\n-    GCMode mode = none;\n+    set_gc_mode(none);\n@@ -127,1 +143,12 @@\n-      if (ShenandoahDegeneratedGC && heuristics->should_degenerate_cycle()) {\n+      if (degen_point == ShenandoahGC::_degenerated_outside_cycle) {\n+        _degen_generation = heap->mode()->is_generational() ? heap->young_generation() : heap->global_generation();\n+      } else {\n+        assert(_degen_generation != nullptr, \"Need to know which generation to resume.\");\n+      }\n+\n+      ShenandoahHeuristics* heuristics = _degen_generation->heuristics();\n+      generation = _degen_generation->generation_mode();\n+      bool old_gen_evacuation_failed = heap->clear_old_evacuation_failure();\n+\n+      \/\/ Do not bother with degenerated cycle if old generation evacuation failed.\n+      if (ShenandoahDegeneratedGC && heuristics->should_degenerate_cycle() && !old_gen_evacuation_failed) {\n@@ -130,1 +157,1 @@\n-        mode = stw_degenerated;\n+        set_gc_mode(stw_degenerated);\n@@ -134,1 +161,2 @@\n-        mode = stw_full;\n+        generation = GLOBAL;\n+        set_gc_mode(stw_full);\n@@ -136,1 +164,0 @@\n-\n@@ -139,0 +166,1 @@\n+      generation = GLOBAL;\n@@ -141,1 +169,1 @@\n-      heuristics->record_requested_gc();\n+      global_heuristics->record_requested_gc();\n@@ -145,1 +173,1 @@\n-        mode = default_mode;\n+        set_gc_mode(default_mode);\n@@ -147,1 +175,1 @@\n-        heap->set_unload_classes(heuristics->can_unload_classes());\n+        heap->set_unload_classes(global_heuristics->can_unload_classes());\n@@ -150,1 +178,1 @@\n-        mode = stw_full;\n+        set_gc_mode(stw_full);\n@@ -154,0 +182,1 @@\n+      generation = GLOBAL;\n@@ -156,1 +185,1 @@\n-      heuristics->record_requested_gc();\n+      global_heuristics->record_requested_gc();\n@@ -160,1 +189,1 @@\n-        mode = default_mode;\n+        set_gc_mode(default_mode);\n@@ -163,1 +192,1 @@\n-        heap->set_unload_classes(heuristics->can_unload_classes());\n+        heap->set_unload_classes(global_heuristics->can_unload_classes());\n@@ -166,1 +195,1 @@\n-        mode = stw_full;\n+        set_gc_mode(stw_full);\n@@ -169,5 +198,16 @@\n-      \/\/ Potential normal cycle: ask heuristics if it wants to act\n-      if (heuristics->should_start_gc()) {\n-        mode = default_mode;\n-        cause = default_cause;\n-      }\n+      \/\/ We should only be here if the regulator requested a cycle or if\n+      \/\/ there is an old generation mark in progress.\n+      if (_requested_gc_cause == GCCause::_shenandoah_concurrent_gc) {\n+        if (_requested_generation == OLD && heap->doing_mixed_evacuations()) {\n+          \/\/ If a request to start an old cycle arrived while an old cycle was running, but _before_\n+          \/\/ it chose any regions for evacuation we don't want to start a new old cycle. Rather, we want\n+          \/\/ the heuristic to run a young collection so that we can evacuate some old regions.\n+          assert(!heap->is_concurrent_old_mark_in_progress(), \"Should not be running mixed collections and concurrent marking.\");\n+          generation = YOUNG;\n+        } else {\n+          generation = _requested_generation;\n+        }\n+\n+        \/\/ preemption was requested or this is a regular cycle\n+        cause = GCCause::_shenandoah_concurrent_gc;\n+        set_gc_mode(default_mode);\n@@ -175,2 +215,27 @@\n-      \/\/ Ask policy if this cycle wants to process references or unload classes\n-      heap->set_unload_classes(heuristics->should_unload_classes());\n+        \/\/ Don't start a new old marking if there is one already in progress.\n+        if (generation == OLD && heap->is_concurrent_old_mark_in_progress()) {\n+          set_gc_mode(servicing_old);\n+        }\n+\n+        if (generation == GLOBAL) {\n+          heap->set_unload_classes(global_heuristics->should_unload_classes());\n+        } else {\n+          heap->set_unload_classes(false);\n+        }\n+\n+        \/\/ Don't want to spin in this loop and start a cycle every time, so\n+        \/\/ clear requested gc cause. This creates a race with callers of the\n+        \/\/ blocking 'request_gc' method, but there it loops and resets the\n+        \/\/ '_requested_gc_cause' until a full cycle is completed.\n+        _requested_gc_cause = GCCause::_no_gc;\n+      } else if (heap->is_concurrent_old_mark_in_progress() || heap->is_prepare_for_old_mark_in_progress()) {\n+        \/\/ Nobody asked us to do anything, but we have an old-generation mark or old-generation preparation for\n+        \/\/ mixed evacuation in progress, so resume working on that.\n+        log_info(gc)(\"Resume old gc: marking=%s, preparing=%s\",\n+                     BOOL_TO_STR(heap->is_concurrent_old_mark_in_progress()),\n+                     BOOL_TO_STR(heap->is_prepare_for_old_mark_in_progress()));\n+\n+        cause = GCCause::_shenandoah_concurrent_gc;\n+        generation = OLD;\n+        set_gc_mode(servicing_old);\n+      }\n@@ -180,2 +245,2 @@\n-    \/\/ either implicit or explicit GC request,  or we are requested to do so unconditionally.\n-    if (alloc_failure_pending || implicit_gc_requested || explicit_gc_requested || ShenandoahAlwaysClearSoftRefs) {\n+    \/\/ either implicit or explicit GC request, or we are requested to do so unconditionally.\n+    if (generation == GLOBAL && (alloc_failure_pending || implicit_gc_requested || explicit_gc_requested || ShenandoahAlwaysClearSoftRefs)) {\n@@ -185,1 +250,1 @@\n-    bool gc_requested = (mode != none);\n+    bool gc_requested = (_mode != none);\n@@ -205,13 +270,44 @@\n-\n-      switch (mode) {\n-        case concurrent_normal:\n-          service_concurrent_normal_cycle(cause);\n-          break;\n-        case stw_degenerated:\n-          service_stw_degenerated_cycle(cause, degen_point);\n-          break;\n-        case stw_full:\n-          service_stw_full_cycle(cause);\n-          break;\n-        default:\n-          ShouldNotReachHere();\n+      \/\/ In case this is a degenerated cycle, remember whether original cycle was aging.\n+      bool was_aging_cycle = heap->is_aging_cycle();\n+      heap->set_aging_cycle(false);\n+      {\n+        switch (_mode) {\n+          case concurrent_normal: {\n+            \/\/ At this point:\n+            \/\/  if (generation == YOUNG), this is a normal YOUNG cycle\n+            \/\/  if (generation == OLD), this is a bootstrap OLD cycle\n+            \/\/  if (generation == GLOBAL), this is a GLOBAL cycle triggered by System.gc()\n+            \/\/ In all three cases, we want to age old objects if this is an aging cycle\n+            if (age_period-- == 0) {\n+              heap->set_aging_cycle(true);\n+              age_period = ShenandoahAgingCyclePeriod - 1;\n+            }\n+            service_concurrent_normal_cycle(heap, generation, cause);\n+            break;\n+          }\n+          case stw_degenerated: {\n+            heap->set_aging_cycle(was_aging_cycle);\n+            if (!service_stw_degenerated_cycle(cause, degen_point)) {\n+              \/\/ The degenerated GC was upgraded to a Full GC\n+              generation = GLOBAL;\n+            }\n+            break;\n+          }\n+          case stw_full: {\n+            if (age_period-- == 0) {\n+              heap->set_aging_cycle(true);\n+              age_period = ShenandoahAgingCyclePeriod - 1;\n+            }\n+            service_stw_full_cycle(cause);\n+            break;\n+          }\n+          case servicing_old: {\n+            assert(generation == OLD, \"Expected old generation here\");\n+            GCIdMark gc_id_mark;\n+            service_concurrent_old_cycle(heap, cause);\n+            break;\n+          }\n+          default: {\n+            ShouldNotReachHere();\n+          }\n+        }\n@@ -255,20 +351,2 @@\n-        heuristics->clear_metaspace_oom();\n-      }\n-\n-      \/\/ Commit worker statistics to cycle data\n-      heap->phase_timings()->flush_par_workers_to_cycle();\n-      if (ShenandoahPacing) {\n-        heap->pacer()->flush_stats_to_cycle();\n-      }\n-\n-      \/\/ Print GC stats for current cycle\n-      {\n-        LogTarget(Info, gc, stats) lt;\n-        if (lt.is_enabled()) {\n-          ResourceMark rm;\n-          LogStream ls(lt);\n-          heap->phase_timings()->print_cycle_on(&ls);\n-          if (ShenandoahPacing) {\n-            heap->pacer()->print_cycle_on(&ls);\n-          }\n-        }\n+        assert(generation == GLOBAL, \"Only unload classes during GLOBAL cycle\");\n+        global_heuristics->clear_metaspace_oom();\n@@ -277,2 +355,1 @@\n-      \/\/ Commit statistics to globals\n-      heap->phase_timings()->flush_cycle_to_global();\n+      process_phase_timings(heap);\n@@ -314,8 +391,6 @@\n-    \/\/ Wait before performing the next action. If allocation happened during this wait,\n-    \/\/ we exit sooner, to let heuristics re-evaluate new conditions. If we are at idle,\n-    \/\/ back off exponentially.\n-    if (_heap_changed.try_unset()) {\n-      sleep = ShenandoahControlIntervalMin;\n-    } else if ((current - last_sleep_adjust_time) * 1000 > ShenandoahControlIntervalAdjustPeriod){\n-      sleep = MIN2<int>(ShenandoahControlIntervalMax, MAX2(1, sleep * 2));\n-      last_sleep_adjust_time = current;\n+    \/\/ Don't wait around if there was an allocation failure - start the next cycle immediately.\n+    if (!is_alloc_failure_gc()) {\n+      \/\/ The timed wait is necessary because this thread has a responsibility to send\n+      \/\/ 'alloc_words' to the pacer when it does not perform a GC.\n+      MonitorLocker lock(&_control_lock, Mutex::_no_safepoint_check_flag);\n+      lock.wait(ShenandoahControlIntervalMax);\n@@ -323,1 +398,0 @@\n-    os::naked_short_sleep(sleep);\n@@ -332,0 +406,204 @@\n+void ShenandoahControlThread::process_phase_timings(const ShenandoahHeap* heap) {\n+\n+  \/\/ Commit worker statistics to cycle data\n+  heap->phase_timings()->flush_par_workers_to_cycle();\n+  if (ShenandoahPacing) {\n+    heap->pacer()->flush_stats_to_cycle();\n+  }\n+\n+  ShenandoahCycleStats evac_stats = heap->evac_tracker()->flush_cycle_to_global();\n+\n+  \/\/ Print GC stats for current cycle\n+  {\n+    LogTarget(Info, gc, stats) lt;\n+    if (lt.is_enabled()) {\n+      ResourceMark rm;\n+      LogStream ls(lt);\n+      heap->phase_timings()->print_cycle_on(&ls);\n+      ShenandoahEvacuationTracker::print_evacuations_on(&ls, &evac_stats.workers,\n+                                                             &evac_stats.mutators);\n+      if (ShenandoahPacing) {\n+        heap->pacer()->print_cycle_on(&ls);\n+      }\n+    }\n+  }\n+\n+  \/\/ Commit statistics to globals\n+  heap->phase_timings()->flush_cycle_to_global();\n+\n+}\n+\n+\/\/ Young and old concurrent cycles are initiated by the regulator. Implicit\n+\/\/ and explicit GC requests are handled by the controller thread and always\n+\/\/ run a global cycle (which is concurrent by default, but may be overridden\n+\/\/ by command line options). Old cycles always degenerate to a global cycle.\n+\/\/ Young cycles are degenerated to complete the young cycle.  Young\n+\/\/ and old degen may upgrade to Full GC.  Full GC may also be\n+\/\/ triggered directly by a System.gc() invocation.\n+\/\/\n+\/\/\n+\/\/      +-----+ Idle +-----+-----------+---------------------+\n+\/\/      |         +        |           |                     |\n+\/\/      |         |        |           |                     |\n+\/\/      |         |        v           |                     |\n+\/\/      |         |  Bootstrap Old +-- | ------------+       |\n+\/\/      |         |   +                |             |       |\n+\/\/      |         |   |                |             |       |\n+\/\/      |         v   v                v             v       |\n+\/\/      |    Resume Old <----------+ Young +--> Young Degen  |\n+\/\/      |     +  +   ^                            +  +       |\n+\/\/      v     |  |   |                            |  |       |\n+\/\/   Global <-+  |   +----------------------------+  |       |\n+\/\/      +        |                                   |       |\n+\/\/      |        v                                   v       |\n+\/\/      +--->  Global Degen +--------------------> Full <----+\n+\/\/\n+void ShenandoahControlThread::service_concurrent_normal_cycle(\n+  const ShenandoahHeap* heap, const GenerationMode generation, GCCause::Cause cause) {\n+  GCIdMark gc_id_mark;\n+  switch (generation) {\n+    case YOUNG: {\n+      \/\/ Run a young cycle. This might or might not, have interrupted an ongoing\n+      \/\/ concurrent mark in the old generation. We need to think about promotions\n+      \/\/ in this case. Promoted objects should be above the TAMS in the old regions\n+      \/\/ they end up in, but we have to be sure we don't promote into any regions\n+      \/\/ that are in the cset.\n+      log_info(gc, ergo)(\"Start GC cycle (YOUNG)\");\n+      service_concurrent_cycle(heap->young_generation(), cause, false);\n+      break;\n+    }\n+    case GLOBAL: {\n+      log_info(gc, ergo)(\"Start GC cycle (GLOBAL)\");\n+      service_concurrent_cycle(heap->global_generation(), cause, false);\n+      break;\n+    }\n+    case OLD: {\n+      log_info(gc, ergo)(\"Start GC cycle (OLD)\");\n+      service_concurrent_old_cycle(heap, cause);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n+  const char* msg;\n+  if (heap->mode()->is_generational()) {\n+    if (heap->cancelled_gc()) {\n+      msg = (generation == YOUNG)? \"At end of Interrupted Concurrent Young GC\": \"At end of Interrupted Concurrent Bootstrap GC\";\n+    } else {\n+      msg = (generation == YOUNG)? \"At end of Concurrent Young GC\": \"At end of Concurrent Bootstrap GC\";\n+    }\n+  } else {\n+    msg = heap->cancelled_gc() ? \"At end of cancelled GC\" : \"At end of GC\";\n+  }\n+  heap->log_heap_status(msg);\n+}\n+\n+void ShenandoahControlThread::service_concurrent_old_cycle(const ShenandoahHeap* heap, GCCause::Cause &cause) {\n+\n+  ShenandoahOldGeneration* old_generation = heap->old_generation();\n+  ShenandoahYoungGeneration* young_generation = heap->young_generation();\n+  ShenandoahOldGeneration::State original_state = old_generation->state();\n+\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+\n+  switch (original_state) {\n+    case ShenandoahOldGeneration::IDLE: {\n+      assert(!heap->is_concurrent_old_mark_in_progress(), \"Old already in progress.\");\n+      assert(old_generation->task_queues()->is_empty(), \"Old mark queues should be empty.\");\n+    }\n+    case ShenandoahOldGeneration::FILLING: {\n+      _allow_old_preemption.set();\n+      ShenandoahGCSession session(cause, old_generation);\n+      old_generation->prepare_gc();\n+      _allow_old_preemption.unset();\n+\n+      if (heap->is_prepare_for_old_mark_in_progress()) {\n+        assert(old_generation->state() == ShenandoahOldGeneration::FILLING, \"Prepare for mark should be in progress.\");\n+        return;\n+      }\n+\n+      assert(old_generation->state() == ShenandoahOldGeneration::BOOTSTRAPPING, \"Finished with filling, should be bootstrapping.\");\n+    }\n+    case ShenandoahOldGeneration::BOOTSTRAPPING: {\n+      \/\/ Configure the young generation's concurrent mark to put objects in\n+      \/\/ old regions into the concurrent mark queues associated with the old\n+      \/\/ generation. The young cycle will run as normal except that rather than\n+      \/\/ ignore old references it will mark and enqueue them in the old concurrent\n+      \/\/ task queues but it will not traverse them.\n+      young_generation->set_old_gen_task_queues(old_generation->task_queues());\n+      ShenandoahGCSession session(cause, young_generation);\n+      service_concurrent_cycle(heap,young_generation, cause, true);\n+      process_phase_timings(heap);\n+      if (heap->cancelled_gc()) {\n+        \/\/ Young generation bootstrap cycle has failed. Concurrent mark for old generation\n+        \/\/ is going to resume after degenerated bootstrap cycle completes.\n+        log_info(gc)(\"Bootstrap cycle for old generation was cancelled.\");\n+        return;\n+      }\n+\n+      \/\/ Reset the degenerated point. Normally this would happen at the top\n+      \/\/ of the control loop, but here we have just completed a young cycle\n+      \/\/ which has bootstrapped the old concurrent marking.\n+      _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n+\n+      \/\/ From here we will 'resume' the old concurrent mark. This will skip reset\n+      \/\/ and init mark for the concurrent mark. All of that work will have been\n+      \/\/ done by the bootstrapping young cycle. In order to simplify the debugging\n+      \/\/ effort, the old cycle will ONLY complete the mark phase. No actual\n+      \/\/ collection of the old generation is happening here.\n+      set_gc_mode(servicing_old);\n+      old_generation->transition_to(ShenandoahOldGeneration::MARKING);\n+    }\n+    case ShenandoahOldGeneration::MARKING: {\n+      ShenandoahGCSession session(cause, old_generation);\n+      bool marking_complete = resume_concurrent_old_cycle(old_generation, cause);\n+      if (marking_complete) {\n+        assert(old_generation->state() != ShenandoahOldGeneration::MARKING, \"Should not still be marking.\");\n+        if (original_state == ShenandoahOldGeneration::MARKING) {\n+          heap->log_heap_status(\"At end of Concurrent Old Marking finishing increment\");\n+        }\n+      } else if (original_state == ShenandoahOldGeneration::MARKING) {\n+        heap->log_heap_status(\"At end of Concurrent Old Marking increment\");\n+      }\n+      break;\n+    }\n+    default:\n+      log_error(gc)(\"Unexpected state for old GC: %d\", old_generation->state());\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+bool ShenandoahControlThread::resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause) {\n+\n+  assert(ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress(), \"Old mark should be in progress\");\n+  log_debug(gc)(\"Resuming old generation with \" UINT32_FORMAT \" marking tasks queued.\", generation->task_queues()->tasks());\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  \/\/ We can only tolerate being cancelled during concurrent marking or during preparation for mixed\n+  \/\/ evacuation. This flag here (passed by reference) is used to control precisely where the regulator\n+  \/\/ is allowed to cancel a GC.\n+  ShenandoahOldGC gc(generation, _allow_old_preemption);\n+  if (gc.collect(cause)) {\n+    generation->record_success_concurrent(false);\n+  }\n+\n+  if (heap->cancelled_gc()) {\n+    \/\/ It's possible the gc cycle was cancelled after the last time\n+    \/\/ the collection checked for cancellation. In which case, the\n+    \/\/ old gc cycle is still completed, and we have to deal with this\n+    \/\/ cancellation. We set the degeneration point to be outside\n+    \/\/ the cycle because if this is an allocation failure, that is\n+    \/\/ what must be done (there is no degenerated old cycle). If the\n+    \/\/ cancellation was due to a heuristic wanting to start a young\n+    \/\/ cycle, then we are not actually going to a degenerated cycle,\n+    \/\/ so the degenerated point doesn't matter here.\n+    check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle);\n+    if (_requested_gc_cause == GCCause::_shenandoah_concurrent_gc) {\n+      heap->shenandoah_policy()->record_interrupted_old();\n+    }\n+    return false;\n+  }\n+  return true;\n+}\n+\n@@ -351,1 +629,1 @@\n-void ShenandoahControlThread::service_concurrent_normal_cycle(GCCause::Cause cause) {\n+void ShenandoahControlThread::service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause, bool do_old_gc_bootstrap) {\n@@ -387,1 +665,0 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -390,3 +667,2 @@\n-  GCIdMark gc_id_mark;\n-  ShenandoahGCSession session(cause);\n-\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahGCSession session(cause, generation);\n@@ -395,1 +671,6 @@\n-  ShenandoahConcurrentGC gc;\n+  service_concurrent_cycle(heap, generation, cause, do_old_gc_bootstrap);\n+}\n+\n+void ShenandoahControlThread::service_concurrent_cycle(const ShenandoahHeap* heap, ShenandoahGeneration* generation,\n+                                                       GCCause::Cause &cause, bool do_old_gc_bootstrap) {\n+  ShenandoahConcurrentGC gc(generation, do_old_gc_bootstrap);\n@@ -398,2 +679,1 @@\n-    heap->heuristics()->record_success_concurrent();\n-    heap->shenandoah_policy()->record_success_concurrent();\n+    generation->record_success_concurrent(gc.abbreviated());\n@@ -403,0 +683,4 @@\n+    assert(generation->generation_mode() != OLD, \"Old GC takes a different control path\");\n+    \/\/ Concurrent young-gen collection degenerates to young\n+    \/\/ collection.  Same for global collections.\n+    _degen_generation = generation;\n@@ -408,7 +692,5 @@\n-  if (heap->cancelled_gc()) {\n-    assert (is_alloc_failure_gc() || in_graceful_shutdown(), \"Cancel GC either for alloc failure GC, or gracefully exiting\");\n-    if (!in_graceful_shutdown()) {\n-      assert (_degen_point == ShenandoahGC::_degenerated_outside_cycle,\n-              \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n-      _degen_point = point;\n-    }\n+  if (!heap->cancelled_gc()) {\n+    return false;\n+  }\n+\n+  if (in_graceful_shutdown()) {\n@@ -417,0 +699,24 @@\n+\n+  assert(_degen_point == ShenandoahGC::_degenerated_outside_cycle,\n+         \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n+\n+  if (is_alloc_failure_gc()) {\n+    _degen_point = point;\n+    return true;\n+  }\n+\n+  if (_preemption_requested.is_set()) {\n+    assert(_requested_generation == YOUNG, \"Only young GCs may preempt old.\");\n+    _preemption_requested.unset();\n+\n+    \/\/ Old generation marking is only cancellable during concurrent marking.\n+    \/\/ Once final mark is complete, the code does not check again for cancellation.\n+    \/\/ If old generation was cancelled for an allocation failure, we wouldn't\n+    \/\/ make it to this case. The calling code is responsible for forcing a\n+    \/\/ cancellation due to allocation failure into a degenerated cycle.\n+    _degen_point = point;\n+    heap->clear_cancelled_gc(false \/* clear oom handler *\/);\n+    return true;\n+  }\n+\n+  fatal(\"Cancel GC either for alloc failure GC, or gracefully exiting, or to pause old generation marking.\");\n@@ -425,0 +731,2 @@\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n@@ -426,1 +734,1 @@\n-  ShenandoahGCSession session(cause);\n+  ShenandoahGCSession session(cause, heap->global_generation());\n@@ -431,2 +739,1 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  heap->heuristics()->record_success_full();\n+  heap->global_generation()->heuristics()->record_success_full();\n@@ -436,1 +743,1 @@\n-void ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahGC::ShenandoahDegenPoint point) {\n+bool ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahGC::ShenandoahDegenPoint point) {\n@@ -438,0 +745,1 @@\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n@@ -440,1 +748,1 @@\n-  ShenandoahGCSession session(cause);\n+  ShenandoahGCSession session(cause, _degen_generation);\n@@ -442,1 +750,1 @@\n-  ShenandoahDegenGC gc(point);\n+  ShenandoahDegenGC gc(point, _degen_generation);\n@@ -445,2 +753,13 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  heap->heuristics()->record_success_degenerated();\n+  assert(heap->young_generation()->task_queues()->is_empty(), \"Unexpected young generation marking tasks\");\n+  if (_degen_generation->generation_mode() == GLOBAL) {\n+    assert(heap->old_generation()->task_queues()->is_empty(), \"Unexpected old generation marking tasks\");\n+    assert(heap->global_generation()->task_queues()->is_empty(), \"Unexpected global generation marking tasks\");\n+  } else {\n+    assert(_degen_generation->generation_mode() == YOUNG, \"Expected degenerated young cycle, if not global.\");\n+    ShenandoahOldGeneration* old_generation = (ShenandoahOldGeneration*) heap->old_generation();\n+    if (old_generation->state() == ShenandoahOldGeneration::BOOTSTRAPPING && !gc.upgraded_to_full()) {\n+      old_generation->transition_to(ShenandoahOldGeneration::MARKING);\n+    }\n+  }\n+\n+  _degen_generation->heuristics()->record_success_degenerated();\n@@ -448,0 +767,1 @@\n+  return !gc.upgraded_to_full();\n@@ -478,0 +798,4 @@\n+bool ShenandoahControlThread::is_implicit_gc(GCCause::Cause cause) const {\n+  return !is_explicit_gc(cause) && cause != GCCause::_shenandoah_concurrent_gc;\n+}\n+\n@@ -500,0 +824,40 @@\n+bool ShenandoahControlThread::request_concurrent_gc(GenerationMode generation) {\n+  if (_preemption_requested.is_set() || _gc_requested.is_set() || ShenandoahHeap::heap()->cancelled_gc()) {\n+    \/\/ ignore subsequent requests from the heuristics\n+    return false;\n+  }\n+\n+  if (_mode == none) {\n+    _requested_gc_cause = GCCause::_shenandoah_concurrent_gc;\n+    _requested_generation = generation;\n+    notify_control_thread();\n+    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n+    ml.wait();\n+    return true;\n+  }\n+\n+  if (preempt_old_marking(generation)) {\n+    log_info(gc)(\"Preempting old generation mark to allow %s GC.\", generation_name(generation));\n+    _requested_gc_cause = GCCause::_shenandoah_concurrent_gc;\n+    _requested_generation = generation;\n+    _preemption_requested.set();\n+    ShenandoahHeap::heap()->cancel_gc(GCCause::_shenandoah_concurrent_gc);\n+    notify_control_thread();\n+\n+    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n+    ml.wait();\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+void ShenandoahControlThread::notify_control_thread() {\n+  MonitorLocker locker(&_control_lock, Mutex::_no_safepoint_check_flag);\n+  _control_lock.notify();\n+}\n+\n+bool ShenandoahControlThread::preempt_old_marking(GenerationMode generation) {\n+  return generation == YOUNG && _allow_old_preemption.try_unset();\n+}\n+\n@@ -519,1 +883,1 @@\n-\n+    notify_control_thread();\n@@ -603,4 +967,0 @@\n-  \/\/ Notify that something had changed.\n-  if (_heap_changed.is_unset()) {\n-    _heap_changed.set();\n-  }\n@@ -641,0 +1001,20 @@\n+\n+const char* ShenandoahControlThread::gc_mode_name(ShenandoahControlThread::GCMode mode) {\n+  switch (mode) {\n+    case none:              return \"idle\";\n+    case concurrent_normal: return \"normal\";\n+    case stw_degenerated:   return \"degenerated\";\n+    case stw_full:          return \"full\";\n+    case servicing_old:     return \"old\";\n+    default:                return \"unknown\";\n+  }\n+}\n+\n+void ShenandoahControlThread::set_gc_mode(ShenandoahControlThread::GCMode new_mode) {\n+  if (_mode != new_mode) {\n+    log_info(gc)(\"Transition from: %s to: %s\", gc_mode_name(_mode), gc_mode_name(new_mode));\n+    _mode = new_mode;\n+    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n+    ml.notify_all();\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":482,"deletions":102,"binary":false,"changes":584,"status":"modified"},{"patch":"@@ -60,7 +60,0 @@\n-  typedef enum {\n-    none,\n-    concurrent_normal,\n-    stw_degenerated,\n-    stw_full\n-  } GCMode;\n-\n@@ -69,1 +62,1 @@\n-  \/\/ to make complete explicit cycle for for demanding customers.\n+  \/\/ to make complete explicit cycle for demanding customers.\n@@ -72,0 +65,2 @@\n+  Monitor _control_lock;\n+  Monitor _regulator_lock;\n@@ -76,0 +71,8 @@\n+  typedef enum {\n+    none,\n+    concurrent_normal,\n+    stw_degenerated,\n+    stw_full,\n+    servicing_old\n+  } GCMode;\n+\n@@ -79,0 +82,2 @@\n+  size_t get_gc_id();\n+\n@@ -80,0 +85,2 @@\n+  ShenandoahSharedFlag _allow_old_preemption;\n+  ShenandoahSharedFlag _preemption_requested;\n@@ -83,1 +90,0 @@\n-  ShenandoahSharedFlag _heap_changed;\n@@ -87,0 +93,1 @@\n+  GenerationMode       _requested_generation;\n@@ -88,0 +95,1 @@\n+  ShenandoahGeneration* _degen_generation;\n@@ -94,0 +102,2 @@\n+  volatile GCMode _mode;\n+  shenandoah_padding(3);\n@@ -95,0 +105,1 @@\n+  \/\/ Returns true if the cycle has been cancelled or degenerated.\n@@ -96,1 +107,4 @@\n-  void service_concurrent_normal_cycle(GCCause::Cause cause);\n+\n+  \/\/ Returns true if the old generation marking completed (i.e., final mark executed for old generation).\n+  bool resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause);\n+  void service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause, bool reset_old_bitmap_specially);\n@@ -98,1 +112,4 @@\n-  void service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahGC::ShenandoahDegenPoint point);\n+\n+  \/\/ Return true if degenerated cycle finishes normally.  Return false if the degenerated cycle transformed itself\n+  \/\/ into a full GC.\n+  bool service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahGC::ShenandoahDegenPoint point);\n@@ -101,0 +118,1 @@\n+  \/\/ Return true if setting the flag which indicates allocation failure succeeds.\n@@ -102,0 +120,1 @@\n+  \/\/ Notify threads waiting for GC to complete.\n@@ -103,0 +122,1 @@\n+  \/\/ True if allocation failure flag has been set.\n@@ -107,1 +127,0 @@\n-  size_t get_gc_id();\n@@ -116,0 +135,4 @@\n+  bool is_implicit_gc(GCCause::Cause cause) const;\n+\n+  \/\/ Returns true if the old generation marking was interrupted to allow a young cycle.\n+  bool preempt_old_marking(GenerationMode generation);\n@@ -117,0 +140,1 @@\n+  \/\/ Returns true if the soft maximum heap has been changed using management APIs.\n@@ -119,0 +143,2 @@\n+  void process_phase_timings(const ShenandoahHeap* heap);\n+\n@@ -133,0 +159,2 @@\n+  \/\/ Return true if the request to start a concurrent GC for the given generation succeeded.\n+  bool request_concurrent_gc(GenerationMode generation);\n@@ -145,0 +173,20 @@\n+\n+  void service_concurrent_normal_cycle(const ShenandoahHeap* heap,\n+                                       const GenerationMode generation,\n+                                       GCCause::Cause cause);\n+\n+  void service_concurrent_old_cycle(const ShenandoahHeap* heap,\n+                                    GCCause::Cause &cause);\n+\n+  void set_gc_mode(GCMode new_mode);\n+  GCMode gc_mode() {\n+    return _mode;\n+  }\n+\n+ private:\n+  static const char* gc_mode_name(GCMode mode);\n+  void notify_control_thread();\n+\n+  void service_concurrent_cycle(const ShenandoahHeap* heap, ShenandoahGeneration* generation, GCCause::Cause &cause,\n+                                bool do_old_gc_bootstrap);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.hpp","additions":60,"deletions":12,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -40,0 +42,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -45,1 +48,1 @@\n-ShenandoahDegenGC::ShenandoahDegenGC(ShenandoahDegenPoint degen_point) :\n+ShenandoahDegenGC::ShenandoahDegenGC(ShenandoahDegenPoint degen_point, ShenandoahGeneration* generation) :\n@@ -47,1 +50,3 @@\n-  _degen_point(degen_point) {\n+  _degen_point(degen_point),\n+  _generation(generation),\n+  _upgraded_to_full(false) {\n@@ -52,0 +57,4 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  if (heap->mode()->is_generational()) {\n+    heap->log_heap_status(\"At end of Degenerated GC\");\n+  }\n@@ -63,1 +72,2 @@\n-  const char* msg = degen_event_message(_degen_point);\n+  char msg[1024];\n+  degen_event_message(_degen_point, msg, sizeof(msg));\n@@ -67,1 +77,0 @@\n-\n@@ -82,1 +91,17 @@\n-  heap->clear_cancelled_gc();\n+  heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n+\n+#ifdef ASSERT\n+  if (heap->mode()->is_generational()) {\n+    if (_generation->generation_mode() == GenerationMode::GLOBAL) {\n+      \/\/ We can only get to a degenerated global cycle _after_ a concurrent global cycle\n+      \/\/ has been cancelled. In which case, we expect the concurrent global cycle to have\n+      \/\/ cancelled the old gc already.\n+      assert(!heap->is_old_gc_active(), \"Old GC should not be active during global cycle.\");\n+    }\n+\n+    if (!heap->is_concurrent_old_mark_in_progress()) {\n+      \/\/ If we are not marking the old generation, there should be nothing in the old mark queues\n+      assert(heap->old_generation()->task_queues()->is_empty(), \"Old gen task queues should be empty.\");\n+    }\n+  }\n+#endif\n@@ -98,1 +123,0 @@\n-      \/\/\n@@ -100,4 +124,7 @@\n-      \/\/ Degenerated from concurrent root mark, reset the flag for STW mark\n-      if (heap->is_concurrent_mark_in_progress()) {\n-        ShenandoahConcurrentMark::cancel();\n-        heap->set_concurrent_mark_in_progress(false);\n+      if (heap->is_concurrent_old_mark_in_progress()) {\n+        \/\/ We have come straight into a degenerated cycle without running a concurrent cycle\n+        \/\/ first and the SATB barrier is enabled to support concurrent old marking. The SATB buffer\n+        \/\/ may hold a mix of old and young pointers. The old pointers need to be transferred\n+        \/\/ to the old generation mark queues and the young pointers are _not_ part of this\n+        \/\/ snapshot, so they must be dropped here.\n+        heap->transfer_old_pointers_from_satb();\n@@ -108,1 +135,30 @@\n-      heap->set_unload_classes(heap->heuristics()->can_unload_classes());\n+      heap->set_unload_classes((!heap->mode()->is_generational() || _generation->generation_mode() == GLOBAL) && _generation->heuristics()->can_unload_classes());\n+\n+      if (heap->mode()->is_generational() && (_generation->generation_mode() == YOUNG || (_generation->generation_mode() == GLOBAL && ShenandoahVerify))) {\n+        \/\/ Swap remembered sets for young, or if the verifier will run during a global collect\n+        _generation->swap_remembered_set();\n+      }\n+\n+    case _degenerated_roots:\n+      \/\/ Degenerated from concurrent root mark, reset the flag for STW mark\n+      if (!heap->mode()->is_generational()) {\n+        if (heap->is_concurrent_mark_in_progress()) {\n+          heap->cancel_concurrent_mark();\n+        }\n+      } else {\n+        if (_generation->is_concurrent_mark_in_progress()) {\n+          \/\/ We want to allow old generation marking to be punctuated by young collections\n+          \/\/ (even if they have degenerated). If this is a global cycle, we'd have cancelled\n+          \/\/ the entire old gc before coming into this switch.\n+          _generation->cancel_marking();\n+        }\n+      }\n+\n+      if (_degen_point == ShenandoahDegenPoint::_degenerated_roots) {\n+        \/\/ We only need this if the concurrent cycle has already swapped the card tables.\n+        \/\/ Marking will use the 'read' table, but interesting pointers may have been\n+        \/\/ recorded in the 'write' table in the time between the cancelled concurrent cycle\n+        \/\/ and this degenerated cycle. These pointers need to be included the 'read' table\n+        \/\/ used to scan the remembered set during the STW mark which follows here.\n+        _generation->merge_write_table();\n+      }\n@@ -130,0 +186,5 @@\n+\n+      if (heap->mode()->is_generational() && _generation->generation_mode() == GLOBAL) {\n+        op_global_coalesce_and_fill();\n+      }\n+\n@@ -134,0 +195,21 @@\n+        if (_degen_point == _degenerated_evac) {\n+          \/\/ Degeneration under oom-evac protocol allows the mutator LRB to expose\n+          \/\/ references to from-space objects. This is okay, in theory, because we\n+          \/\/ will come to the safepoint here to complete the evacuations and update\n+          \/\/ the references. However, if the from-space reference is written to a\n+          \/\/ region that was EC during final mark or was recycled after final mark\n+          \/\/ it will not have TAMS or UWM updated. Such a region is effectively\n+          \/\/ skipped during update references which can lead to crashes and corruption\n+          \/\/ if the from-space reference is accessed.\n+          if (UseTLAB) {\n+            heap->labs_make_parsable();\n+          }\n+\n+          for (size_t i = 0; i < heap->num_regions(); i++) {\n+            ShenandoahHeapRegion* r = heap->get_region(i);\n+            if (r->is_active() && r->top() > r->get_update_watermark()) {\n+              r->set_update_watermark_at_safepoint(r->top());\n+            }\n+          }\n+        }\n+\n@@ -151,1 +233,0 @@\n-\n@@ -196,0 +277,17 @@\n+  if (heap->mode()->is_generational()) {\n+    \/\/ In case degeneration interrupted concurrent evacuation or update references, we need to clean up transient state.\n+    \/\/ Otherwise, these actions have no effect.\n+\n+    heap->young_generation()->unadjust_available();\n+    heap->old_generation()->unadjust_available();\n+    \/\/ No need to old_gen->increase_used().  That was done when plabs were allocated, accounting for both old evacs and promotions.\n+\n+    heap->set_alloc_supplement_reserve(0);\n+    heap->set_young_evac_reserve(0);\n+    heap->set_old_evac_reserve(0);\n+    heap->reset_old_evac_expended();\n+    heap->set_promoted_reserve(0);\n+\n+    heap->adjust_generation_sizes();\n+  }\n+\n@@ -218,1 +316,1 @@\n-  ShenandoahHeap::heap()->prepare_gc();\n+  _generation->prepare_gc();\n@@ -222,1 +320,1 @@\n-  assert(!ShenandoahHeap::heap()->is_concurrent_mark_in_progress(), \"Should be reset\");\n+  assert(!_generation->is_concurrent_mark_in_progress(), \"Should be reset\");\n@@ -224,2 +322,1 @@\n-  ShenandoahSTWMark mark(false \/*full gc*\/);\n-  mark.clear();\n+  ShenandoahSTWMark mark(_generation, false \/*full gc*\/);\n@@ -230,1 +327,1 @@\n-  ShenandoahConcurrentMark mark;\n+  ShenandoahConcurrentMark mark(_generation);\n@@ -242,0 +339,1 @@\n+\n@@ -243,1 +341,1 @@\n-  heap->prepare_regions_and_collection_set(false \/*concurrent*\/);\n+  _generation->prepare_regions_and_collection_set(false \/*concurrent*\/);\n@@ -277,0 +375,4 @@\n+void ShenandoahDegenGC::op_global_coalesce_and_fill() {\n+  ShenandoahHeap::heap()->coalesce_and_fill_old_regions();\n+}\n+\n@@ -327,3 +429,1 @@\n-  log_info(gc)(\"Cannot finish degeneration, upgrading to Full GC\");\n-  ShenandoahHeap::heap()->shenandoah_policy()->record_degenerated_upgrade_to_full();\n-\n+  upgrade_to_full();\n@@ -335,1 +435,1 @@\n-  ShenandoahHeap::heap()->shenandoah_policy()->record_degenerated_upgrade_to_full();\n+  upgrade_to_full();\n@@ -340,16 +440,12 @@\n-const char* ShenandoahDegenGC::degen_event_message(ShenandoahDegenPoint point) const {\n-  switch (point) {\n-    case _degenerated_unset:\n-      return \"Pause Degenerated GC (<UNSET>)\";\n-    case _degenerated_outside_cycle:\n-      return \"Pause Degenerated GC (Outside of Cycle)\";\n-    case _degenerated_mark:\n-      return \"Pause Degenerated GC (Mark)\";\n-    case _degenerated_evac:\n-      return \"Pause Degenerated GC (Evacuation)\";\n-    case _degenerated_updaterefs:\n-      return \"Pause Degenerated GC (Update Refs)\";\n-    default:\n-      ShouldNotReachHere();\n-      return \"ERROR\";\n-  }\n+void ShenandoahDegenGC::degen_event_message(ShenandoahDegenPoint point, char* buf, size_t len) const {\n+  jio_snprintf(buf, len, \"Pause Degenerated %s GC (%s)\", _generation->name(), ShenandoahGC::degen_point_to_string(point));\n+}\n+\n+void ShenandoahDegenGC::upgrade_to_full() {\n+  log_info(gc)(\"Degenerate GC upgrading to Full GC\");\n+  ShenandoahHeap::heap()->shenandoah_policy()->record_degenerated_upgrade_to_full();\n+  _upgraded_to_full = true;\n+}\n+\n+bool ShenandoahDegenGC::upgraded_to_full() {\n+  return _upgraded_to_full;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.cpp","additions":134,"deletions":38,"binary":false,"changes":172,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+class ShenandoahGeneration;\n@@ -36,0 +37,2 @@\n+  ShenandoahGeneration* _generation;\n+  bool _upgraded_to_full;\n@@ -38,1 +41,1 @@\n-  ShenandoahDegenGC(ShenandoahDegenPoint degen_point);\n+  ShenandoahDegenGC(ShenandoahDegenPoint degen_point, ShenandoahGeneration* generation);\n@@ -40,0 +43,1 @@\n+  bool upgraded_to_full();\n@@ -51,0 +55,1 @@\n+  void op_global_coalesce_and_fill();\n@@ -61,1 +66,2 @@\n-  const char* degen_event_message(ShenandoahDegenPoint point) const;\n+  void degen_event_message(ShenandoahDegenPoint point, char* buf, size_t len) const;\n+  void upgrade_to_full();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.hpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -0,0 +1,137 @@\n+\/*\n+ * Copyright (c) 2022, Amazon, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n+#include \"gc\/shenandoah\/shenandoahThreadLocalData.hpp\"\n+#include \"gc\/shenandoah\/shenandoahRootProcessor.hpp\"\n+#include \"runtime\/threadSMR.inline.hpp\"\n+#include \"runtime\/thread.hpp\"\n+\n+ShenandoahEvacuationStats::ShenandoahEvacuationStats()\n+  : _evacuations_completed(0), _bytes_completed(0),\n+    _evacuations_attempted(0), _bytes_attempted(0),\n+    _age_table(false) {}\n+\n+void ShenandoahEvacuationStats::begin_evacuation(size_t bytes) {\n+  ++_evacuations_attempted;\n+  _bytes_attempted += bytes;\n+}\n+\n+void ShenandoahEvacuationStats::end_evacuation(size_t bytes, uint age) {\n+  ++_evacuations_completed;\n+  _bytes_completed += bytes;\n+  if (age > 0) {\n+    _age_table.add(age, bytes \/ oopSize);\n+  }\n+}\n+\n+void ShenandoahEvacuationStats::accumulate(const ShenandoahEvacuationStats* other) {\n+  _evacuations_completed += other->_evacuations_completed;\n+  _bytes_completed += other->_bytes_completed;\n+  _evacuations_attempted += other->_evacuations_attempted;\n+  _bytes_attempted += other->_bytes_attempted;\n+\n+  _age_table.merge(&other->_age_table);\n+}\n+\n+void ShenandoahEvacuationStats::reset() {\n+  _evacuations_completed = _evacuations_attempted = 0;\n+  _bytes_completed = _bytes_attempted = 0;\n+  _age_table.clear();\n+}\n+\n+void ShenandoahEvacuationStats::print_on(outputStream* st) {\n+  size_t abandoned_size = _bytes_attempted - _bytes_completed;\n+  size_t abandoned_count = _evacuations_attempted - _evacuations_completed;\n+  st->print_cr(\"Evacuated \" SIZE_FORMAT \"%s across \" SIZE_FORMAT \" objects, \"\n+            \"abandoned \" SIZE_FORMAT \"%s across \" SIZE_FORMAT \" objects.\",\n+            byte_size_in_proper_unit(_bytes_completed),\n+            proper_unit_for_byte_size(_bytes_completed), _evacuations_completed,\n+            byte_size_in_proper_unit(abandoned_size),\n+            proper_unit_for_byte_size(abandoned_size), abandoned_count);\n+  _age_table.print_on(st, InitialTenuringThreshold);\n+}\n+\n+void ShenandoahEvacuationTracker::print_global_on(outputStream* st) {\n+  print_evacuations_on(st, &_workers_global, &_mutators_global);\n+}\n+\n+void ShenandoahEvacuationTracker::print_evacuations_on(outputStream* st,\n+                                                       ShenandoahEvacuationStats* workers,\n+                                                       ShenandoahEvacuationStats* mutators) {\n+  st->print(\"Workers: \");\n+  workers->print_on(st);\n+  st->cr();\n+  st->print(\"Mutators: \");\n+  mutators->print_on(st);\n+  st->cr();\n+\n+  AgeTable region_ages(false);\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  for (uint i = 0; i < heap->num_regions(); ++i) {\n+    ShenandoahHeapRegion* r = heap->get_region(i);\n+    if (r->age() > 0 && r->age() < AgeTable::table_size) {\n+      region_ages.add(r->age(), r->get_live_data_words());\n+    }\n+  }\n+  st->print(\"Regions: \");\n+  region_ages.print_on(st, InitialTenuringThreshold);\n+}\n+\n+class ShenandoahStatAggregator : public ThreadClosure {\n+ public:\n+  ShenandoahEvacuationStats* _target;\n+  explicit ShenandoahStatAggregator(ShenandoahEvacuationStats* target) : _target(target) {}\n+  virtual void do_thread(Thread* thread) override {\n+    ShenandoahEvacuationStats* local = ShenandoahThreadLocalData::evacuation_stats(thread);\n+    _target->accumulate(local);\n+    local->reset();\n+  }\n+};\n+\n+ShenandoahCycleStats ShenandoahEvacuationTracker::flush_cycle_to_global() {\n+  ShenandoahEvacuationStats mutators, workers;\n+\n+  ThreadsListHandle java_threads_iterator;\n+  ShenandoahStatAggregator aggregate_mutators(&mutators);\n+  java_threads_iterator.list()->threads_do(&aggregate_mutators);\n+\n+  ShenandoahStatAggregator aggregate_workers(&workers);\n+  ShenandoahHeap::heap()->gc_threads_do(&aggregate_workers);\n+\n+  _mutators_global.accumulate(&mutators);\n+  _workers_global.accumulate(&workers);\n+\n+  return {workers, mutators};\n+}\n+\n+void ShenandoahEvacuationTracker::begin_evacuation(Thread* thread, size_t bytes) {\n+  ShenandoahThreadLocalData::begin_evacuation(thread, bytes);\n+}\n+\n+void ShenandoahEvacuationTracker::end_evacuation(Thread* thread, size_t bytes, uint age) {\n+  ShenandoahThreadLocalData::end_evacuation(thread, bytes, age);\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahEvacTracker.cpp","additions":137,"deletions":0,"binary":false,"changes":137,"status":"added"},{"patch":"@@ -0,0 +1,71 @@\n+\/*\n+ * Copyright (c) 2022, Amazon, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHEVACTRACKER_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHEVACTRACKER_HPP\n+\n+#include \"gc\/shared\/ageTable.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+class ShenandoahEvacuationStats : public CHeapObj<mtGC> {\n+ private:\n+  size_t _evacuations_completed;\n+  size_t _bytes_completed;\n+  size_t _evacuations_attempted;\n+  size_t _bytes_attempted;\n+\n+  AgeTable _age_table;\n+\n+ public:\n+  ShenandoahEvacuationStats();\n+  void begin_evacuation(size_t bytes);\n+  void end_evacuation(size_t bytes, uint age);\n+\n+  void print_on(outputStream* st);\n+  void accumulate(const ShenandoahEvacuationStats* other);\n+  void reset();\n+};\n+\n+struct ShenandoahCycleStats {\n+  ShenandoahEvacuationStats workers;\n+  ShenandoahEvacuationStats mutators;\n+};\n+\n+class ShenandoahEvacuationTracker : public CHeapObj<mtGC> {\n+ private:\n+  ShenandoahEvacuationStats  _workers_global;\n+  ShenandoahEvacuationStats  _mutators_global;\n+\n+ public:\n+  void begin_evacuation(Thread* thread, size_t bytes);\n+  void end_evacuation(Thread* thread, size_t bytes, uint age);\n+\n+  void print_global_on(outputStream* st);\n+  static void print_evacuations_on(outputStream* st, ShenandoahEvacuationStats* workers, ShenandoahEvacuationStats* mutators);\n+\n+  ShenandoahCycleStats flush_cycle_to_global();\n+ private:\n+};\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_SHENANDOAHEVACTRACKER_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahEvacTracker.hpp","additions":71,"deletions":0,"binary":false,"changes":71,"status":"added"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n@@ -31,0 +32,3 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -64,0 +68,45 @@\n+\/\/ This is a temporary solution to work around a shortcoming with the existing free set implementation.\n+\/\/ TODO:\n+\/\/   Remove this function after restructing FreeSet representation.  A problem in the existing implementation is that old-gen\n+\/\/   regions are not considered to reside within the is_collector_free range.\n+\/\/\n+HeapWord* ShenandoahFreeSet::allocate_with_old_affiliation(ShenandoahAllocRequest& req, bool& in_new_region) {\n+  ShenandoahRegionAffiliation affiliation = ShenandoahRegionAffiliation::OLD_GENERATION;\n+\n+  size_t rightmost = MAX2(_collector_rightmost, _mutator_rightmost);\n+  size_t leftmost = MIN2(_collector_leftmost, _mutator_leftmost);\n+\n+  for (size_t c = rightmost + 1; c > leftmost; c--) {\n+    \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+    size_t idx = c - 1;\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (r->affiliation() == affiliation && !r->is_humongous()) {\n+      if (!r->is_cset() && !has_no_alloc_capacity(r)) {\n+        HeapWord* result = try_allocate_in(r, req, in_new_region);\n+        if (result != nullptr) {\n+          return result;\n+        }\n+      }\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+HeapWord* ShenandoahFreeSet::allocate_with_affiliation(ShenandoahRegionAffiliation affiliation, ShenandoahAllocRequest& req, bool& in_new_region) {\n+  for (size_t c = _collector_rightmost + 1; c > _collector_leftmost; c--) {\n+    \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+    size_t idx = c - 1;\n+    if (is_collector_free(idx)) {\n+      ShenandoahHeapRegion* r = _heap->get_region(idx);\n+      if (r->affiliation() == affiliation) {\n+        HeapWord* result = try_allocate_in(r, req, in_new_region);\n+        if (result != nullptr) {\n+          return result;\n+        }\n+      }\n+    }\n+  }\n+  log_debug(gc, free)(\"Could not allocate collector region with affiliation: %s for request \" PTR_FORMAT, affiliation_name(affiliation), p2i(&req));\n+  return nullptr;\n+}\n+\n@@ -77,0 +126,26 @@\n+  \/\/ Overwrite with non-zero (non-NULL) values only if necessary for allocation bookkeeping.\n+\n+  bool allow_new_region = true;\n+  if (_heap->mode()->is_generational()) {\n+    switch (req.affiliation()) {\n+      case ShenandoahRegionAffiliation::OLD_GENERATION:\n+        \/\/ Note: unsigned result from adjusted_unaffiliated_regions() will never be less than zero, but it may equal zero.\n+        if (_heap->old_generation()->adjusted_unaffiliated_regions() <= 0) {\n+          allow_new_region = false;\n+        }\n+        break;\n+\n+      case ShenandoahRegionAffiliation::YOUNG_GENERATION:\n+        \/\/ Note: unsigned result from adjusted_unaffiliated_regions() will never be less than zero, but it may equal zero.\n+        if (_heap->young_generation()->adjusted_unaffiliated_regions() <= 0) {\n+          allow_new_region = false;\n+        }\n+        break;\n+\n+      case ShenandoahRegionAffiliation::FREE:\n+      default:\n+        ShouldNotReachHere();\n+        break;\n+    }\n+  }\n+\n@@ -80,1 +155,0 @@\n-\n@@ -83,2 +157,4 @@\n-        if (is_mutator_free(idx)) {\n-          HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n+        ShenandoahHeapRegion* r = _heap->get_region(idx);\n+        if (is_mutator_free(idx) && (allow_new_region || r->affiliation() != ShenandoahRegionAffiliation::FREE)) {\n+          \/\/ try_allocate_in() increases used if the allocation is successful.\n+          HeapWord* result = try_allocate_in(r, req, in_new_region);\n@@ -90,1 +166,0 @@\n-\n@@ -95,2 +170,2 @@\n-    case ShenandoahAllocRequest::_alloc_shared_gc: {\n-      \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+      \/\/ GCLABs are for evacuation so we must be in evacuation phase.  If this allocation is successful, increment\n+      \/\/ the relevant evac_expended rather than used value.\n@@ -98,5 +173,32 @@\n-      \/\/ Fast-path: try to allocate in the collector view first\n-      for (size_t c = _collector_rightmost + 1; c > _collector_leftmost; c--) {\n-        size_t idx = c - 1;\n-        if (is_collector_free(idx)) {\n-          HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n+    case ShenandoahAllocRequest::_alloc_plab:\n+      \/\/ PLABs always reside in old-gen and are only allocated during evacuation phase.\n+\n+    case ShenandoahAllocRequest::_alloc_shared_gc: {\n+      if (!_heap->mode()->is_generational()) {\n+        \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+        \/\/ Fast-path: try to allocate in the collector view first\n+        for (size_t c = _collector_rightmost + 1; c > _collector_leftmost; c--) {\n+          size_t idx = c - 1;\n+          if (is_collector_free(idx)) {\n+            HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n+            if (result != nullptr) {\n+              return result;\n+            }\n+          }\n+        }\n+      } else {\n+        \/\/ First try to fit into a region that is already in use in the same generation.\n+        HeapWord* result;\n+        if (req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+          \/\/ TODO: this is a work around to address a deficiency in FreeSet representation.  A better solution fixes\n+          \/\/ the FreeSet implementation to deal more efficiently with old-gen regions as being in the \"collector free set\"\n+          result = allocate_with_old_affiliation(req, in_new_region);\n+        } else {\n+          result = allocate_with_affiliation(req.affiliation(), req, in_new_region);\n+        }\n+        if (result != nullptr) {\n+          return result;\n+        }\n+        if (allow_new_region) {\n+          \/\/ Then try a free region that is dedicated to GC allocations.\n+          result = allocate_with_affiliation(FREE, req, in_new_region);\n@@ -114,10 +216,13 @@\n-      \/\/ Try to steal the empty region from the mutator view\n-      for (size_t c = _mutator_rightmost + 1; c > _mutator_leftmost; c--) {\n-        size_t idx = c - 1;\n-        if (is_mutator_free(idx)) {\n-          ShenandoahHeapRegion* r = _heap->get_region(idx);\n-          if (can_allocate_from(r)) {\n-            flip_to_gc(r);\n-            HeapWord *result = try_allocate_in(r, req, in_new_region);\n-            if (result != nullptr) {\n-              return result;\n+      if (allow_new_region) {\n+        \/\/ Try to steal an empty region from the mutator view.\n+        for (size_t c = _mutator_rightmost + 1; c > _mutator_leftmost; c--) {\n+          size_t idx = c - 1;\n+          if (is_mutator_free(idx)) {\n+            ShenandoahHeapRegion* r = _heap->get_region(idx);\n+            if (can_allocate_from(r)) {\n+              flip_to_gc(r);\n+              HeapWord *result = try_allocate_in(r, req, in_new_region);\n+              if (result != nullptr) {\n+                log_debug(gc, free)(\"Flipped region \" SIZE_FORMAT \" to gc for request: \" PTR_FORMAT, idx, p2i(&req));\n+                return result;\n+              }\n@@ -132,1 +237,0 @@\n-\n@@ -138,1 +242,0 @@\n-\n@@ -149,1 +252,0 @@\n-\n@@ -151,0 +253,13 @@\n+  if (r->affiliation() == ShenandoahRegionAffiliation::FREE) {\n+    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+    r->set_affiliation(req.affiliation());\n+    if (r->is_old()) {\n+      \/\/ Any OLD region allocated during concurrent coalesce-and-fill does not need to be coalesced and filled because\n+      \/\/ all objects allocated within this region are above TAMS (and thus are implicitly marked).  In case this is an\n+      \/\/ OLD region and concurrent preparation for mixed evacuations visits this region before the start of the next\n+      \/\/ old-gen concurrent mark (i.e. this region is allocated following the start of old-gen concurrent mark but before\n+      \/\/ concurrent preparations for mixed evacuations are completed), we mark this region as not requiring any\n+      \/\/ coalesce-and-fill processing.\n+      r->end_preemptible_coalesce_and_fill();\n+      _heap->clear_cards_for(r);\n+    }\n@@ -152,1 +267,7 @@\n-  in_new_region = r->is_empty();\n+    assert(ctx->top_at_mark_start(r) == r->bottom(), \"Newly established allocation region starts with TAMS equal to bottom\");\n+    assert(ctx->is_bitmap_clear_range(ctx->top_bitmap(r), r->end()), \"Bitmap above top_bitmap() must be clear\");\n+  } else if (r->affiliation() != req.affiliation()) {\n+    assert(_heap->mode()->is_generational(), \"Request for %s from %s region should only happen in generational mode.\",\n+           affiliation_name(req.affiliation()), affiliation_name(r->affiliation()));\n+    return nullptr;\n+  }\n@@ -154,0 +275,1 @@\n+  in_new_region = r->is_empty();\n@@ -157,0 +279,6 @@\n+  if (in_new_region) {\n+    log_debug(gc, free)(\"Using new region (\" SIZE_FORMAT \") for %s (\" PTR_FORMAT \").\",\n+                       r->index(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(&req));\n+  }\n+\n+  \/\/ req.size() is in words, r->free() is in bytes.\n@@ -158,3 +286,84 @@\n-    size_t free = align_down(r->free() >> LogHeapWordSize, MinObjAlignment);\n-    if (size > free) {\n-      size = free;\n+    if (req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+      assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+      \/\/ Need to assure that plabs are aligned on multiple of card region.\n+      size_t free = r->free();\n+      size_t usable_free = (free \/ CardTable::card_size()) << CardTable::card_shift();\n+      if ((free != usable_free) && (free - usable_free < ShenandoahHeap::min_fill_size() * HeapWordSize)) {\n+        \/\/ We'll have to add another card's memory to the padding\n+        if (usable_free > CardTable::card_size()) {\n+          usable_free -= CardTable::card_size();\n+        } else {\n+          assert(usable_free == 0, \"usable_free is a multiple of card_size and card_size > min_fill_size\");\n+        }\n+      }\n+      free \/= HeapWordSize;\n+      usable_free \/= HeapWordSize;\n+      size_t remnant = size % CardTable::card_size_in_words();\n+      if (remnant > 0) {\n+        \/\/ Since we have Elastic TLABs, align size up.  This is consistent with aligning min_size up.\n+        size = size - remnant + CardTable::card_size_in_words();\n+      }\n+      if (size > usable_free) {\n+        size = usable_free;\n+        assert(size % CardTable::card_size_in_words() == 0, \"usable_free is a multiple of card table size\");\n+      }\n+\n+      size_t adjusted_min_size = req.min_size();\n+      remnant = adjusted_min_size % CardTable::card_size_in_words();\n+      if (remnant > 0) {\n+        \/\/ Round up adjusted_min_size to a multiple of alignment size\n+        adjusted_min_size = adjusted_min_size - remnant + CardTable::card_size_in_words();\n+      }\n+      if (size >= adjusted_min_size) {\n+        result = r->allocate_aligned(size, req, CardTable::card_size());\n+        assert(result != nullptr, \"Allocation cannot fail\");\n+        size = req.actual_size();\n+        assert(r->top() <= r->end(), \"Allocation cannot span end of region\");\n+        \/\/ actual_size() will be set to size below.\n+        assert((result == nullptr) || (size % CardTable::card_size_in_words() == 0),\n+               \"PLAB size must be multiple of card size\");\n+        assert((result == nullptr) || (((uintptr_t) result) % CardTable::card_size_in_words() == 0),\n+               \"PLAB start must align with card boundary\");\n+        if (free > usable_free) {\n+          \/\/ Account for the alignment padding\n+          size_t padding = (free - usable_free) * HeapWordSize;\n+          increase_used(padding);\n+          assert(r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION, \"All PLABs reside in old-gen\");\n+          _heap->old_generation()->increase_used(padding);\n+          \/\/ For verification consistency, we need to report this padding to _heap\n+          _heap->increase_used(padding);\n+        }\n+      }\n+      \/\/ Otherwise, leave result == nullptr because the adjusted size is smaller than min size.\n+    } else {\n+      \/\/ This is a GCLAB or a TLAB allocation\n+      size_t free = align_down(r->free() >> LogHeapWordSize, MinObjAlignment);\n+      if (size > free) {\n+        size = free;\n+      }\n+      if (size >= req.min_size()) {\n+        result = r->allocate(size, req);\n+        if (result != nullptr) {\n+          \/\/ Record actual allocation size\n+          req.set_actual_size(size);\n+        }\n+        assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, size);\n+      } else {\n+        log_trace(gc, free)(\"Failed to shrink TLAB or GCLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n+                           \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), size, req.min_size());\n+      }\n+    }\n+  } else if (req.is_lab_alloc() && req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+    assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+    \/\/ inelastic PLAB\n+    size_t free = r->free();\n+    size_t usable_free = (free \/ CardTable::card_size()) << CardTable::card_shift();\n+    free \/= HeapWordSize;\n+    usable_free \/= HeapWordSize;\n+    if ((free != usable_free) && (free - usable_free < ShenandoahHeap::min_fill_size() * HeapWordSize)) {\n+      \/\/ We'll have to add another card's memory to the padding\n+      if (usable_free > CardTable::card_size_in_words()) {\n+        usable_free -= CardTable::card_size_in_words();\n+      } else {\n+        assert(usable_free == 0, \"usable_free is a multiple of card_size and card_size > min_fill_size\");\n+      }\n@@ -162,3 +371,17 @@\n-    if (size >= req.min_size()) {\n-      result = r->allocate(size, req.type());\n-      assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, size);\n+    assert(size % CardTable::card_size_in_words() == 0, \"PLAB size must be multiple of remembered set card size\");\n+    if (size <= usable_free) {\n+      result = r->allocate_aligned(size, req, CardTable::card_size());\n+      size = req.actual_size();\n+      assert(result != nullptr, \"Allocation cannot fail\");\n+      assert(r->top() <= r->end(), \"Allocation cannot span end of region\");\n+      assert(req.actual_size() % CardTable::card_size_in_words() == 0, \"PLAB start must align with card boundary\");\n+      assert(((uintptr_t) result) % CardTable::card_size_in_words() == 0, \"PLAB start must align with card boundary\");\n+      if (free > usable_free) {\n+        \/\/ Account for the alignment padding\n+        size_t padding = (free - usable_free) * HeapWordSize;\n+        increase_used(padding);\n+        assert(r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION, \"All PLABs reside in old-gen\");\n+        _heap->old_generation()->increase_used(padding);\n+        \/\/ For verification consistency, we need to report this padding to _heap\n+        _heap->increase_used(padding);\n+      }\n@@ -167,1 +390,5 @@\n-    result = r->allocate(size, req.type());\n+    result = r->allocate(size, req);\n+    if (result != nullptr) {\n+      \/\/ Record actual allocation size\n+      req.set_actual_size(size);\n+    }\n@@ -170,0 +397,1 @@\n+  ShenandoahGeneration* generation = _heap->generation_for(req.affiliation());\n@@ -173,0 +401,2 @@\n+      assert(req.is_young(), \"Mutator allocations always come from young generation.\");\n+      generation->increase_used(size * HeapWordSize);\n@@ -174,6 +404,10 @@\n-    }\n-\n-    \/\/ Record actual allocation size\n-    req.set_actual_size(size);\n-\n-    if (req.is_gc_alloc()) {\n+    } else {\n+      assert(req.is_gc_alloc(), \"Should be gc_alloc since req wasn't mutator alloc\");\n+\n+      \/\/ For GC allocations, we advance update_watermark because the objects relocated into this memory during\n+      \/\/ evacuation are not updated during evacuation.  For both young and old regions r, it is essential that all\n+      \/\/ PLABs be made parsable at the end of evacuation.  This is enabled by retiring all plabs at end of evacuation.\n+      \/\/ TODO: Making a PLAB parsable involves placing a filler object in its remnant memory but does not require\n+      \/\/ that the PLAB be disabled for all future purposes.  We may want to introduce a new service to make the\n+      \/\/ PLABs parsable while still allowing the PLAB to serve future allocation requests that arise during the\n+      \/\/ next evacuation pass.\n@@ -181,0 +415,5 @@\n+      generation->increase_used(size * HeapWordSize);\n+      if (r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+        assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"old-gen allocations use PLAB or shared allocation\");\n+        \/\/ for plabs, we'll sort the difference between evac and promotion usage when we retire the plab\n+      }\n@@ -189,2 +428,3 @@\n-    \/\/ almost-full regions precede the fully-empty region where we want allocate the entire TLAB.\n-    \/\/ TODO: Record first fully-empty region, and use that for large allocations\n+    \/\/ almost-full regions precede the fully-empty region where we want to allocate the entire TLAB.\n+    \/\/ TODO: Record first fully-empty region, and use that for large allocations and\/or organize\n+    \/\/ available free segments within regions for more efficient searches for \"good fit\".\n@@ -197,0 +437,1 @@\n+        generation->increase_allocated(waste);\n@@ -251,3 +492,13 @@\n-  \/\/ No regions left to satisfy allocation, bye.\n-  if (num > mutator_count()) {\n-    return nullptr;\n+  assert(req.affiliation() == ShenandoahRegionAffiliation::YOUNG_GENERATION, \"Humongous regions always allocated in YOUNG\");\n+  ShenandoahGeneration* generation = _heap->generation_for(req.affiliation());\n+\n+  \/\/ Check if there are enough regions left to satisfy allocation.\n+  if (_heap->mode()->is_generational()) {\n+    size_t avail_young_regions = generation->adjusted_unaffiliated_regions();\n+    if (num > mutator_count() || (num > avail_young_regions)) {\n+      return nullptr;\n+    }\n+  } else {\n+    if (num > mutator_count()) {\n+      return nullptr;\n+    }\n@@ -285,0 +536,1 @@\n+  ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n@@ -308,1 +560,14 @@\n-    r->set_top(r->bottom() + used_words);\n+    r->set_affiliation(req.affiliation());\n+    r->set_update_watermark(r->bottom());\n+    r->set_top(r->bottom());    \/\/ Set top to bottom so we can capture TAMS\n+    ctx->capture_top_at_mark_start(r);\n+    r->set_top(r->bottom() + used_words); \/\/ Then change top to reflect allocation of humongous object.\n+    assert(ctx->top_at_mark_start(r) == r->bottom(), \"Newly established allocation region starts with TAMS equal to bottom\");\n+    assert(ctx->is_bitmap_clear_range(ctx->top_bitmap(r), r->end()), \"Bitmap above top_bitmap() must be clear\");\n+\n+    \/\/ Leave top_bitmap alone.  The first time a heap region is put into service, top_bitmap should equal end.\n+    \/\/ Thereafter, it should represent the upper bound on parts of the bitmap that need to be cleared.\n+    \/\/ ctx->clear_bitmap(r);\n+    log_debug(gc, free)(\"NOT clearing bitmap for Humongous region [\" PTR_FORMAT \", \" PTR_FORMAT \"], top_bitmap: \"\n+                        PTR_FORMAT \" at transition from FREE to %s\",\n+                        p2i(r->bottom()), p2i(r->end()), p2i(ctx->top_bitmap(r)), affiliation_name(req.affiliation()));\n@@ -316,0 +581,1 @@\n+  generation->increase_used(words_size * HeapWordSize);\n@@ -319,1 +585,3 @@\n-    _heap->notify_mutator_alloc_words(ShenandoahHeapRegion::region_size_words() - remainder, true);\n+    size_t waste = ShenandoahHeapRegion::region_size_words() - remainder;\n+    _heap->notify_mutator_alloc_words(waste, true);\n+    generation->increase_allocated(waste * HeapWordSize);\n@@ -387,0 +655,4 @@\n+\n+  \/\/ We do not ensure that the region is no longer trash,\n+  \/\/ relying on try_allocate_in(), which always comes next,\n+  \/\/ to recycle trash before attempting to allocate anything in the region.\n@@ -409,0 +681,1 @@\n+  log_debug(gc, free)(\"Rebuilding FreeSet\");\n@@ -422,0 +695,4 @@\n+\n+      log_debug(gc, free)(\"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator free set\",\n+          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n+               byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n@@ -426,1 +703,20 @@\n-  size_t to_reserve = _heap->max_capacity() \/ 100 * ShenandoahEvacReserve;\n+  if (!_heap->mode()->is_generational()) {\n+    size_t to_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+    reserve_regions(to_reserve);\n+  } else {\n+    size_t young_reserve = (_heap->young_generation()->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+    \/\/ Note that all allocations performed from old-gen are performed by GC, generally using PLABs for both\n+    \/\/ promotions and evacuations.  The partition between which old memory is reserved for evacuation and\n+    \/\/ which is reserved for promotion is enforced using thread-local variables that prescribe intentons within\n+    \/\/ each PLAB.  We do not reserve any of old-gen memory in order to facilitate the loaning of old-gen memory\n+    \/\/ to young-gen purposes.\n+    size_t old_reserve = 0;\n+    size_t to_reserve = young_reserve + old_reserve;\n+    reserve_regions(to_reserve);\n+  }\n+\n+  recompute_bounds();\n+  assert_bounds();\n+}\n+\n+void ShenandoahFreeSet::reserve_regions(size_t to_reserve) {\n@@ -439,0 +735,3 @@\n+      log_debug(gc, free)(\"  Shifting Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to collector free set\",\n+                          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n+                               byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n@@ -441,3 +740,0 @@\n-\n-  recompute_bounds();\n-  assert_bounds();\n@@ -449,1 +745,1 @@\n-  LogTarget(Info, gc, ergo) lt;\n+  LogTarget(Info, gc, free) lt;\n@@ -515,0 +811,2 @@\n+      ls.print(\"Used: \" SIZE_FORMAT \"%s, Mutator Free: \" SIZE_FORMAT \" \",\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used), mutator_count());\n@@ -520,0 +818,1 @@\n+      size_t total_used = 0;\n@@ -527,0 +826,1 @@\n+          total_used += r->used();\n@@ -530,1 +830,1 @@\n-      ls.print_cr(\"Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s\",\n+      ls.print_cr(\"Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s\",\n@@ -532,1 +832,2 @@\n-                  byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max));\n+                  byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max),\n+                  byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used));\n@@ -541,0 +842,1 @@\n+  \/\/ Allocation request is known to satisfy all memory budgeting constraints.\n@@ -547,0 +849,1 @@\n+      case ShenandoahAllocRequest::_alloc_plab:\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":355,"deletions":52,"binary":false,"changes":407,"status":"modified"},{"patch":"@@ -52,0 +52,9 @@\n+  HeapWord* allocate_with_affiliation(ShenandoahRegionAffiliation affiliation, ShenandoahAllocRequest& req, bool& in_new_region);\n+  HeapWord* allocate_with_old_affiliation(ShenandoahAllocRequest& req, bool& in_new_region);\n+\n+  \/\/ While holding the heap lock, allocate memory for a single object which is to be entirely contained\n+  \/\/ within a single HeapRegion as characterized by req.  The req.size() value is known to be less than or\n+  \/\/ equal to ShenandoahHeapRegion::humongous_threshold_words().  The caller of allocate_single is responsible\n+  \/\/ for registering the resulting object and setting the remembered set card values as appropriate.  The\n+  \/\/ most common case is that we are allocating a PLAB in which case object registering and card dirtying\n+  \/\/ is managed after the PLAB is divided into individual objects.\n@@ -64,3 +73,0 @@\n-  size_t collector_count() const { return _collector_free_bitmap.count_one_bits(); }\n-  size_t mutator_count()   const { return _mutator_free_bitmap.count_one_bits();   }\n-\n@@ -76,0 +82,6 @@\n+  \/\/ Number of regions dedicated to GC allocations (for evacuation or promotion) that are currently free\n+  size_t collector_count() const { return _collector_free_bitmap.count_one_bits(); }\n+\n+  \/\/ Number of regions dedicated to mutator allocations that are currently free\n+  size_t mutator_count()   const { return _mutator_free_bitmap.count_one_bits();   }\n+\n@@ -97,0 +109,2 @@\n+\n+  void reserve_regions(size_t to_reserve);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":17,"deletions":3,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -46,0 +47,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -54,0 +56,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -65,0 +68,63 @@\n+\/\/ After Full GC is done, reconstruct the remembered set by iterating over OLD regions,\n+\/\/ registering all objects between bottom() and top(), and setting remembered set cards to\n+\/\/ DIRTY if they hold interesting pointers.\n+class ShenandoahReconstructRememberedSetTask : public WorkerTask {\n+private:\n+  ShenandoahRegionIterator _regions;\n+\n+public:\n+  ShenandoahReconstructRememberedSetTask() :\n+    WorkerTask(\"Shenandoah Reset Bitmap\") { }\n+\n+  void work(uint worker_id) {\n+    ShenandoahParallelWorkerSession worker_session(worker_id);\n+    ShenandoahHeapRegion* r = _regions.next();\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    RememberedScanner* scanner = heap->card_scan();\n+    ShenandoahSetRememberedCardsToDirtyClosure dirty_cards_for_interesting_pointers;\n+\n+    while (r != nullptr) {\n+      if (r->is_old() && r->is_active()) {\n+        HeapWord* obj_addr = r->bottom();\n+        if (r->is_humongous_start()) {\n+          \/\/ First, clear the remembered set\n+          oop obj = cast_to_oop(obj_addr);\n+          size_t size = obj->size();\n+          HeapWord* end_object = r->bottom() + size;\n+\n+          \/\/ First, clear the remembered set for all spanned humongous regions\n+          size_t num_regions = (size + ShenandoahHeapRegion::region_size_words() - 1) \/ ShenandoahHeapRegion::region_size_words();\n+          size_t region_span = num_regions * ShenandoahHeapRegion::region_size_words();\n+          scanner->reset_remset(r->bottom(), region_span);\n+          size_t region_index = r->index();\n+          ShenandoahHeapRegion* humongous_region = heap->get_region(region_index);\n+          while (num_regions-- != 0) {\n+            scanner->reset_object_range(humongous_region->bottom(), humongous_region->end());\n+            region_index++;\n+            humongous_region = heap->get_region(region_index);\n+          }\n+\n+          \/\/ Then register the humongous object and DIRTY relevant remembered set cards\n+          scanner->register_object_wo_lock(obj_addr);\n+          obj->oop_iterate(&dirty_cards_for_interesting_pointers);\n+        } else if (!r->is_humongous()) {\n+          \/\/ First, clear the remembered set\n+          scanner->reset_remset(r->bottom(), ShenandoahHeapRegion::region_size_words());\n+          scanner->reset_object_range(r->bottom(), r->end());\n+\n+          \/\/ Then iterate over all objects, registering object and DIRTYing relevant remembered set cards\n+          HeapWord* t = r->top();\n+          while (obj_addr < t) {\n+            oop obj = cast_to_oop(obj_addr);\n+            size_t size = obj->size();\n+            scanner->register_object_wo_lock(obj_addr);\n+            obj_addr += obj->oop_iterate_size(&dirty_cards_for_interesting_pointers);\n+          }\n+        } \/\/ else, ignore humongous continuation region\n+      }\n+      \/\/ else, this region is FREE or YOUNG or inactive and we can ignore it.\n+      r = _regions.next();\n+    }\n+  }\n+};\n+\n@@ -102,0 +168,1 @@\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n@@ -109,1 +176,10 @@\n-\n+  if (heap->mode()->is_generational()) {\n+    heap->log_heap_status(\"At end of Full GC\");\n+\n+    \/\/ Since we allow temporary violation of these constraints during Full GC, we want to enforce that the assertions are\n+    \/\/ made valid by the time Full GC completes.\n+    assert(heap->old_generation()->used_regions_size() <= heap->old_generation()->adjusted_capacity(),\n+           \"Old generation affiliated regions must be less than capacity\");\n+    assert(heap->young_generation()->used_regions_size() <= heap->young_generation()->adjusted_capacity(),\n+           \"Young generation affiliated regions must be less than capacity\");\n+  }\n@@ -121,0 +197,18 @@\n+  \/\/ Since we may arrive here from degenerated GC failure of either young or old, establish generation as GLOBAL.\n+  heap->set_gc_generation(heap->global_generation());\n+\n+  if (heap->mode()->is_generational()) {\n+    \/\/ Defer unadjust_available() invocations until after Full GC finishes its efforts because Full GC makes use\n+    \/\/ of young-gen memory that may have been loaned from old-gen.\n+\n+    \/\/ No need to old_gen->increase_used().  That was done when plabs were allocated, accounting for both old evacs and promotions.\n+\n+    heap->set_alloc_supplement_reserve(0);\n+    heap->set_young_evac_reserve(0);\n+    heap->set_old_evac_reserve(0);\n+    heap->reset_old_evac_expended();\n+    heap->set_promoted_reserve(0);\n+\n+    \/\/ Full GC supersedes any marking or coalescing in old generation.\n+    heap->cancel_old_gc();\n+  }\n@@ -164,1 +258,1 @@\n-    \/\/ b. Cancel concurrent mark, if in progress\n+    \/\/ b. Cancel all concurrent marks, if in progress\n@@ -166,2 +260,1 @@\n-      ShenandoahConcurrentGC::cancel();\n-      heap->set_concurrent_mark_in_progress(false);\n+      heap->cancel_concurrent_mark();\n@@ -177,1 +270,1 @@\n-    heap->reset_mark_bitmap();\n+    heap->global_generation()->reset_mark_bitmap();\n@@ -179,1 +272,1 @@\n-    assert(!heap->marking_context()->is_complete(), \"sanity\");\n+    assert(!heap->global_generation()->is_mark_complete(), \"sanity\");\n@@ -182,1 +275,1 @@\n-    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+    ShenandoahReferenceProcessor* rp = heap->global_generation()->ref_processor();\n@@ -195,0 +288,1 @@\n+    \/\/ TODO: Do we need to explicitly retire PLABs?\n@@ -237,0 +331,6 @@\n+\n+    if (heap->mode()->is_generational()) {\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::full_gc_reconstruct_remembered_set);\n+      ShenandoahReconstructRememberedSetTask task;\n+      heap->workers()->run_task(&task);\n+    }\n@@ -242,0 +342,2 @@\n+  heap->adjust_generation_sizes();\n+\n@@ -252,1 +354,5 @@\n-    heap->verifier()->verify_after_fullgc();\n+    if (heap->mode()->is_generational()) {\n+      heap->verifier()->verify_after_generational_fullgc();\n+    } else {\n+      heap->verifier()->verify_after_fullgc();\n+    }\n@@ -255,0 +361,4 @@\n+  \/\/ Having reclaimed all dead memory, it is now safe to restore capacities to original values.\n+  heap->young_generation()->unadjust_available();\n+  heap->old_generation()->unadjust_available();\n+\n@@ -273,2 +383,4 @@\n-    _ctx->capture_top_at_mark_start(r);\n-    r->clear_live_data();\n+    if (r->affiliation() != FREE) {\n+      _ctx->capture_top_at_mark_start(r);\n+      r->clear_live_data();\n+    }\n@@ -276,0 +388,2 @@\n+\n+  bool is_thread_safe() { return true; }\n@@ -285,1 +399,1 @@\n-  heap->heap_region_iterate(&cl);\n+  heap->parallel_heap_region_iterate(&cl);\n@@ -287,1 +401,1 @@\n-  heap->set_unload_classes(heap->heuristics()->can_unload_classes());\n+  heap->set_unload_classes(heap->global_generation()->heuristics()->can_unload_classes());\n@@ -289,1 +403,1 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = heap->global_generation()->ref_processor();\n@@ -293,1 +407,1 @@\n-  ShenandoahSTWMark mark(true \/*full_gc*\/);\n+  ShenandoahSTWMark mark(heap->global_generation(), true \/*full_gc*\/);\n@@ -298,0 +412,231 @@\n+class ShenandoahPrepareForCompactionTask : public WorkerTask {\n+private:\n+  PreservedMarksSet*        const _preserved_marks;\n+  ShenandoahHeap*           const _heap;\n+  ShenandoahHeapRegionSet** const _worker_slices;\n+  size_t                    const _num_workers;\n+\n+public:\n+  ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks, ShenandoahHeapRegionSet **worker_slices,\n+                                     size_t num_workers);\n+\n+  static bool is_candidate_region(ShenandoahHeapRegion* r) {\n+    \/\/ Empty region: get it into the slice to defragment the slice itself.\n+    \/\/ We could have skipped this without violating correctness, but we really\n+    \/\/ want to compact all live regions to the start of the heap, which sometimes\n+    \/\/ means moving them into the fully empty regions.\n+    if (r->is_empty()) return true;\n+\n+    \/\/ Can move the region, and this is not the humongous region. Humongous\n+    \/\/ moves are special cased here, because their moves are handled separately.\n+    return r->is_stw_move_allowed() && !r->is_humongous();\n+  }\n+\n+  void work(uint worker_id);\n+};\n+\n+class ShenandoahPrepareForGenerationalCompactionObjectClosure : public ObjectClosure {\n+private:\n+  ShenandoahPrepareForCompactionTask* _compactor;\n+  PreservedMarks*          const _preserved_marks;\n+  ShenandoahHeap*          const _heap;\n+\n+  \/\/ _empty_regions is a thread-local list of heap regions that have been completely emptied by this worker thread's\n+  \/\/ compaction efforts.  The worker thread that drives these efforts adds compacted regions to this list if the\n+  \/\/ region has not been compacted onto itself.\n+  GrowableArray<ShenandoahHeapRegion*>& _empty_regions;\n+  int _empty_regions_pos;\n+  ShenandoahHeapRegion*          _old_to_region;\n+  ShenandoahHeapRegion*          _young_to_region;\n+  ShenandoahHeapRegion*          _from_region;\n+  ShenandoahRegionAffiliation    _from_affiliation;\n+  HeapWord*                      _old_compact_point;\n+  HeapWord*                      _young_compact_point;\n+  uint                           _worker_id;\n+\n+public:\n+  ShenandoahPrepareForGenerationalCompactionObjectClosure(ShenandoahPrepareForCompactionTask* compactor,\n+                                                          PreservedMarks* preserved_marks,\n+                                                          GrowableArray<ShenandoahHeapRegion*>& empty_regions,\n+                                                          ShenandoahHeapRegion* old_to_region,\n+                                                          ShenandoahHeapRegion* young_to_region, uint worker_id) :\n+      _compactor(compactor),\n+      _preserved_marks(preserved_marks),\n+      _heap(ShenandoahHeap::heap()),\n+      _empty_regions(empty_regions),\n+      _empty_regions_pos(0),\n+      _old_to_region(old_to_region),\n+      _young_to_region(young_to_region),\n+      _from_region(nullptr),\n+      _old_compact_point((old_to_region != nullptr)? old_to_region->bottom(): nullptr),\n+      _young_compact_point((young_to_region != nullptr)? young_to_region->bottom(): nullptr),\n+      _worker_id(worker_id) {}\n+\n+  void set_from_region(ShenandoahHeapRegion* from_region) {\n+    _from_region = from_region;\n+    _from_affiliation = from_region->affiliation();\n+    if (_from_region->has_live()) {\n+      if (_from_affiliation == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+        if (_old_to_region == nullptr) {\n+          _old_to_region = from_region;\n+          _old_compact_point = from_region->bottom();\n+        }\n+      } else {\n+        assert(_from_affiliation == ShenandoahRegionAffiliation::YOUNG_GENERATION, \"from_region must be OLD or YOUNG\");\n+        if (_young_to_region == nullptr) {\n+          _young_to_region = from_region;\n+          _young_compact_point = from_region->bottom();\n+        }\n+      }\n+    } \/\/ else, we won't iterate over this _from_region so we don't need to set up to region to hold copies\n+  }\n+\n+  void finish() {\n+    finish_old_region();\n+    finish_young_region();\n+  }\n+\n+  void finish_old_region() {\n+    if (_old_to_region != nullptr) {\n+      log_debug(gc)(\"Planned compaction into Old Region \" SIZE_FORMAT \", used: \" SIZE_FORMAT \" tabulated by worker %u\",\n+                    _old_to_region->index(), _old_compact_point - _old_to_region->bottom(), _worker_id);\n+      _old_to_region->set_new_top(_old_compact_point);\n+      _old_to_region = nullptr;\n+    }\n+  }\n+\n+  void finish_young_region() {\n+    if (_young_to_region != nullptr) {\n+      log_debug(gc)(\"Worker %u planned compaction into Young Region \" SIZE_FORMAT \", used: \" SIZE_FORMAT,\n+                    _worker_id, _young_to_region->index(), _young_compact_point - _young_to_region->bottom());\n+      _young_to_region->set_new_top(_young_compact_point);\n+      _young_to_region = nullptr;\n+    }\n+  }\n+\n+  bool is_compact_same_region() {\n+    return (_from_region == _old_to_region) || (_from_region == _young_to_region);\n+  }\n+\n+  int empty_regions_pos() {\n+    return _empty_regions_pos;\n+  }\n+\n+  void do_object(oop p) {\n+    assert(_from_region != nullptr, \"must set before work\");\n+    assert((_from_region->bottom() <= cast_from_oop<HeapWord*>(p)) && (cast_from_oop<HeapWord*>(p) < _from_region->top()),\n+           \"Object must reside in _from_region\");\n+    assert(_heap->complete_marking_context()->is_marked(p), \"must be marked\");\n+    assert(!_heap->complete_marking_context()->allocated_after_mark_start(p), \"must be truly marked\");\n+\n+    size_t obj_size = p->size();\n+    uint from_region_age = _from_region->age();\n+    uint object_age = p->age();\n+\n+    bool promote_object = false;\n+    if ((_from_affiliation == ShenandoahRegionAffiliation::YOUNG_GENERATION) &&\n+        (from_region_age + object_age >= InitialTenuringThreshold)) {\n+      if ((_old_to_region != nullptr) && (_old_compact_point + obj_size > _old_to_region->end())) {\n+        finish_old_region();\n+        _old_to_region = nullptr;\n+      }\n+      if (_old_to_region == nullptr) {\n+        if (_empty_regions_pos < _empty_regions.length()) {\n+          ShenandoahHeapRegion* new_to_region = _empty_regions.at(_empty_regions_pos);\n+          _empty_regions_pos++;\n+          new_to_region->set_affiliation(OLD_GENERATION);\n+          _old_to_region = new_to_region;\n+          _old_compact_point = _old_to_region->bottom();\n+          promote_object = true;\n+        }\n+        \/\/ Else this worker thread does not yet have any empty regions into which this aged object can be promoted so\n+        \/\/ we leave promote_object as false, deferring the promotion.\n+      } else {\n+        promote_object = true;\n+      }\n+    }\n+\n+    if (promote_object || (_from_affiliation == ShenandoahRegionAffiliation::OLD_GENERATION)) {\n+      assert(_old_to_region != nullptr, \"_old_to_region should not be nullptr when evacuating to OLD region\");\n+      if (_old_compact_point + obj_size > _old_to_region->end()) {\n+        ShenandoahHeapRegion* new_to_region;\n+\n+        log_debug(gc)(\"Worker %u finishing old region \" SIZE_FORMAT \", compact_point: \" PTR_FORMAT \", obj_size: \" SIZE_FORMAT\n+                      \", &compact_point[obj_size]: \" PTR_FORMAT \", region end: \" PTR_FORMAT,  _worker_id, _old_to_region->index(),\n+                      p2i(_old_compact_point), obj_size, p2i(_old_compact_point + obj_size), p2i(_old_to_region->end()));\n+\n+        \/\/ Object does not fit.  Get a new _old_to_region.\n+        finish_old_region();\n+        if (_empty_regions_pos < _empty_regions.length()) {\n+          new_to_region = _empty_regions.at(_empty_regions_pos);\n+          _empty_regions_pos++;\n+          new_to_region->set_affiliation(OLD_GENERATION);\n+        } else {\n+          \/\/ If we've exhausted the previously selected _old_to_region, we know that the _old_to_region is distinct\n+          \/\/ from _from_region.  That's because there is always room for _from_region to be compacted into itself.\n+          \/\/ Since we're out of empty regions, let's use _from_region to hold the results of its own compaction.\n+          new_to_region = _from_region;\n+        }\n+\n+        assert(new_to_region != _old_to_region, \"must not reuse same OLD to-region\");\n+        assert(new_to_region != nullptr, \"must not be nullptr\");\n+        _old_to_region = new_to_region;\n+        _old_compact_point = _old_to_region->bottom();\n+      }\n+\n+      \/\/ Object fits into current region, record new location:\n+      assert(_old_compact_point + obj_size <= _old_to_region->end(), \"must fit\");\n+      shenandoah_assert_not_forwarded(nullptr, p);\n+      _preserved_marks->push_if_necessary(p, p->mark());\n+      p->forward_to(cast_to_oop(_old_compact_point));\n+      _old_compact_point += obj_size;\n+    } else {\n+      assert(_from_affiliation == ShenandoahRegionAffiliation::YOUNG_GENERATION,\n+             \"_from_region must be OLD_GENERATION or YOUNG_GENERATION\");\n+      assert(_young_to_region != nullptr, \"_young_to_region should not be nullptr when compacting YOUNG _from_region\");\n+\n+      \/\/ After full gc compaction, all regions have age 0.  Embed the region's age into the object's age in order to preserve\n+      \/\/ tenuring progress.\n+      if (_heap->is_aging_cycle()) {\n+        _heap->increase_object_age(p, from_region_age + 1);\n+      } else {\n+        _heap->increase_object_age(p, from_region_age);\n+      }\n+\n+      if (_young_compact_point + obj_size > _young_to_region->end()) {\n+        ShenandoahHeapRegion* new_to_region;\n+\n+        log_debug(gc)(\"Worker %u finishing young region \" SIZE_FORMAT \", compact_point: \" PTR_FORMAT \", obj_size: \" SIZE_FORMAT\n+                      \", &compact_point[obj_size]: \" PTR_FORMAT \", region end: \" PTR_FORMAT,  _worker_id, _young_to_region->index(),\n+                      p2i(_young_compact_point), obj_size, p2i(_young_compact_point + obj_size), p2i(_young_to_region->end()));\n+\n+        \/\/ Object does not fit.  Get a new _young_to_region.\n+        finish_young_region();\n+        if (_empty_regions_pos < _empty_regions.length()) {\n+          new_to_region = _empty_regions.at(_empty_regions_pos);\n+          _empty_regions_pos++;\n+          new_to_region->set_affiliation(YOUNG_GENERATION);\n+        } else {\n+          \/\/ If we've exhausted the previously selected _young_to_region, we know that the _young_to_region is distinct\n+          \/\/ from _from_region.  That's because there is always room for _from_region to be compacted into itself.\n+          \/\/ Since we're out of empty regions, let's use _from_region to hold the results of its own compaction.\n+          new_to_region = _from_region;\n+        }\n+\n+        assert(new_to_region != _young_to_region, \"must not reuse same OLD to-region\");\n+        assert(new_to_region != nullptr, \"must not be nullptr\");\n+        _young_to_region = new_to_region;\n+        _young_compact_point = _young_to_region->bottom();\n+      }\n+\n+      \/\/ Object fits into current region, record new location:\n+      assert(_young_compact_point + obj_size <= _young_to_region->end(), \"must fit\");\n+      shenandoah_assert_not_forwarded(nullptr, p);\n+      _preserved_marks->push_if_necessary(p, p->mark());\n+      p->forward_to(cast_to_oop(_young_compact_point));\n+      _young_compact_point += obj_size;\n+    }\n+  }\n+};\n+\n+\n@@ -326,0 +671,1 @@\n+    assert(!_heap->mode()->is_generational(), \"Generational GC should use different Closure\");\n@@ -371,5 +717,0 @@\n-class ShenandoahPrepareForCompactionTask : public WorkerTask {\n-private:\n-  PreservedMarksSet*        const _preserved_marks;\n-  ShenandoahHeap*           const _heap;\n-  ShenandoahHeapRegionSet** const _worker_slices;\n@@ -377,2 +718,3 @@\n-public:\n-  ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks, ShenandoahHeapRegionSet **worker_slices) :\n+ShenandoahPrepareForCompactionTask::ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks,\n+                                                                       ShenandoahHeapRegionSet **worker_slices,\n+                                                                       size_t num_workers) :\n@@ -380,2 +722,12 @@\n-    _preserved_marks(preserved_marks),\n-    _heap(ShenandoahHeap::heap()), _worker_slices(worker_slices) {\n+    _preserved_marks(preserved_marks), _heap(ShenandoahHeap::heap()),\n+    _worker_slices(worker_slices), _num_workers(num_workers) { }\n+\n+\n+void ShenandoahPrepareForCompactionTask::work(uint worker_id) {\n+  ShenandoahParallelWorkerSession worker_session(worker_id);\n+  ShenandoahHeapRegionSet* slice = _worker_slices[worker_id];\n+  ShenandoahHeapRegionSetIterator it(slice);\n+  ShenandoahHeapRegion* from_region = it.next();\n+  \/\/ No work?\n+  if (from_region == nullptr) {\n+    return;\n@@ -384,6 +736,3 @@\n-  static bool is_candidate_region(ShenandoahHeapRegion* r) {\n-    \/\/ Empty region: get it into the slice to defragment the slice itself.\n-    \/\/ We could have skipped this without violating correctness, but we really\n-    \/\/ want to compact all live regions to the start of the heap, which sometimes\n-    \/\/ means moving them into the fully empty regions.\n-    if (r->is_empty()) return true;\n+  \/\/ Sliding compaction. Walk all regions in the slice, and compact them.\n+  \/\/ Remember empty regions and reuse them as needed.\n+  ResourceMark rm;\n@@ -391,4 +740,1 @@\n-    \/\/ Can move the region, and this is not the humongous region. Humongous\n-    \/\/ moves are special cased here, because their moves are handled separately.\n-    return r->is_stw_move_allowed() && !r->is_humongous();\n-  }\n+  GrowableArray<ShenandoahHeapRegion*> empty_regions((int)_heap->num_regions());\n@@ -396,8 +742,19 @@\n-  void work(uint worker_id) {\n-    ShenandoahParallelWorkerSession worker_session(worker_id);\n-    ShenandoahHeapRegionSet* slice = _worker_slices[worker_id];\n-    ShenandoahHeapRegionSetIterator it(slice);\n-    ShenandoahHeapRegion* from_region = it.next();\n-    \/\/ No work?\n-    if (from_region == nullptr) {\n-       return;\n+  if (_heap->mode()->is_generational()) {\n+    ShenandoahHeapRegion* old_to_region = (from_region->is_old())? from_region: nullptr;\n+    ShenandoahHeapRegion* young_to_region = (from_region->is_young())? from_region: nullptr;\n+    ShenandoahPrepareForGenerationalCompactionObjectClosure cl(this, _preserved_marks->get(worker_id), empty_regions,\n+                                                               old_to_region, young_to_region, worker_id);\n+    while (from_region != nullptr) {\n+      assert(is_candidate_region(from_region), \"Sanity\");\n+      log_debug(gc)(\"Worker %u compacting %s Region \" SIZE_FORMAT \" which had used \" SIZE_FORMAT \" and %s live\",\n+                    worker_id, affiliation_name(from_region->affiliation()),\n+                    from_region->index(), from_region->used(), from_region->has_live()? \"has\": \"does not have\");\n+      cl.set_from_region(from_region);\n+      if (from_region->has_live()) {\n+        _heap->marked_object_iterate(from_region, &cl);\n+      }\n+      \/\/ Compacted the region to somewhere else? From-region is empty then.\n+      if (!cl.is_compact_same_region()) {\n+        empty_regions.append(from_region);\n+      }\n+      from_region = it.next();\n@@ -405,0 +762,1 @@\n+    cl.finish();\n@@ -406,6 +764,6 @@\n-    \/\/ Sliding compaction. Walk all regions in the slice, and compact them.\n-    \/\/ Remember empty regions and reuse them as needed.\n-    ResourceMark rm;\n-\n-    GrowableArray<ShenandoahHeapRegion*> empty_regions((int)_heap->num_regions());\n-\n+    \/\/ Mark all remaining regions as empty\n+    for (int pos = cl.empty_regions_pos(); pos < empty_regions.length(); ++pos) {\n+      ShenandoahHeapRegion* r = empty_regions.at(pos);\n+      r->set_new_top(r->bottom());\n+    }\n+  } else {\n@@ -413,1 +771,0 @@\n-\n@@ -416,1 +773,0 @@\n-\n@@ -436,1 +792,1 @@\n-};\n+}\n@@ -455,0 +811,1 @@\n+  log_debug(gc)(\"Full GC calculating target humongous objects from end \" SIZE_FORMAT, to_end);\n@@ -493,0 +850,1 @@\n+    bool is_generational = _heap->mode()->is_generational();\n@@ -497,0 +855,1 @@\n+      \/\/ Leave afffiliation unchanged.\n@@ -521,17 +880,22 @@\n-    if (r->is_humongous_start()) {\n-      oop humongous_obj = cast_to_oop(r->bottom());\n-      if (!_ctx->is_marked(humongous_obj)) {\n-        assert(!r->has_live(),\n-               \"Region \" SIZE_FORMAT \" is not marked, should not have live\", r->index());\n-        _heap->trash_humongous_region_at(r);\n-      } else {\n-        assert(r->has_live(),\n-               \"Region \" SIZE_FORMAT \" should have live\", r->index());\n-      }\n-    } else if (r->is_humongous_continuation()) {\n-      \/\/ If we hit continuation, the non-live humongous starts should have been trashed already\n-      assert(r->humongous_start_region()->has_live(),\n-             \"Region \" SIZE_FORMAT \" should have live\", r->index());\n-    } else if (r->is_regular()) {\n-      if (!r->has_live()) {\n-        r->make_trash_immediate();\n+    if (r->affiliation() != FREE) {\n+      if (r->is_humongous_start()) {\n+        oop humongous_obj = cast_to_oop(r->bottom());\n+        if (!_ctx->is_marked(humongous_obj)) {\n+          assert(!r->has_live(),\n+                 \"Humongous Start %s Region \" SIZE_FORMAT \" is not marked, should not have live\",\n+                 affiliation_name(r->affiliation()),  r->index());\n+          log_debug(gc)(\"Trashing immediate humongous region \" SIZE_FORMAT \" because not marked\", r->index());\n+          _heap->trash_humongous_region_at(r);\n+        } else {\n+          assert(r->has_live(),\n+                 \"Humongous Start %s Region \" SIZE_FORMAT \" should have live\", affiliation_name(r->affiliation()),  r->index());\n+        }\n+      } else if (r->is_humongous_continuation()) {\n+        \/\/ If we hit continuation, the non-live humongous starts should have been trashed already\n+        assert(r->humongous_start_region()->has_live(),\n+               \"Humongous Continuation %s Region \" SIZE_FORMAT \" should have live\", affiliation_name(r->affiliation()),  r->index());\n+      } else if (r->is_regular()) {\n+        if (!r->has_live()) {\n+          log_debug(gc)(\"Trashing immediate regular region \" SIZE_FORMAT \" because has no live\", r->index());\n+          r->make_trash_immediate();\n+        }\n@@ -540,0 +904,2 @@\n+    \/\/ else, ignore this FREE region.\n+    \/\/ TODO: change iterators so they do not process FREE regions.\n@@ -706,0 +1072,5 @@\n+  if (heap->mode()->is_generational()) {\n+    heap->young_generation()->clear_used();\n+    heap->old_generation()->clear_used();\n+  }\n+\n@@ -712,1 +1083,4 @@\n-    ShenandoahPrepareForCompactionTask task(_preserved_marks, worker_slices);\n+    size_t num_workers = heap->max_workers();\n+\n+    ResourceMark rm;\n+    ShenandoahPrepareForCompactionTask task(_preserved_marks, worker_slices, num_workers);\n@@ -786,0 +1160,7 @@\n+      if (r->is_pinned() && r->is_old() && r->is_active() && !r->is_humongous()) {\n+        \/\/ Pinned regions are not compacted so they may still hold unmarked objects with\n+        \/\/ reference to reclaimed memory. Remembered set scanning will crash if it attempts\n+        \/\/ to iterate the oops in these objects.\n+        r->begin_preemptible_coalesce_and_fill();\n+        r->oop_fill_and_coalesce_wo_cancel();\n+      }\n@@ -898,0 +1279,1 @@\n+    bool is_generational = _heap->mode()->is_generational();\n@@ -912,0 +1294,4 @@\n+      if (!is_generational) {\n+        r->make_young_maybe();\n+      }\n+      \/\/ else, generational mode compaction has already established affiliation.\n@@ -926,0 +1312,9 @@\n+    \/\/ Update final usage for generations\n+    if (is_generational && live != 0) {\n+      if (r->is_young()) {\n+        _heap->young_generation()->increase_used(live);\n+      } else if (r->is_old()) {\n+        _heap->old_generation()->increase_used(live);\n+      }\n+    }\n+\n@@ -963,2 +1358,7 @@\n-      Copy::aligned_conjoint_words(r->bottom(), heap->get_region(new_start)->bottom(), words_size);\n-      ContinuationGCSupport::relativize_stack_chunk(cast_to_oop<HeapWord*>(r->bottom()));\n+      ContinuationGCSupport::relativize_stack_chunk(cast_to_oop<HeapWord*>(heap->get_region(old_start)->bottom()));\n+      log_debug(gc)(\"Full GC compaction moves humongous object from region \" SIZE_FORMAT \" to region \" SIZE_FORMAT,\n+                    old_start, new_start);\n+\n+      Copy::aligned_conjoint_words(heap->get_region(old_start)->bottom(),\n+                                   heap->get_region(new_start)->bottom(),\n+                                   words_size);\n@@ -970,0 +1370,1 @@\n+        ShenandoahRegionAffiliation original_affiliation = r->affiliation();\n@@ -972,0 +1373,1 @@\n+          \/\/ Leave humongous region affiliation unchanged.\n@@ -979,1 +1381,1 @@\n-            r->make_humongous_start_bypass();\n+            r->make_humongous_start_bypass(original_affiliation);\n@@ -981,1 +1383,1 @@\n-            r->make_humongous_cont_bypass();\n+            r->make_humongous_cont_bypass(original_affiliation);\n@@ -1060,0 +1462,5 @@\n+    if (heap->mode()->is_generational()) {\n+      heap->young_generation()->clear_used();\n+      heap->old_generation()->clear_used();\n+    }\n+\n@@ -1063,0 +1470,4 @@\n+    if (heap->mode()->is_generational()) {\n+      log_info(gc)(\"FullGC done: GLOBAL usage: \" SIZE_FORMAT \", young usage: \" SIZE_FORMAT \", old usage: \" SIZE_FORMAT,\n+                    post_compact.get_live(), heap->young_generation()->used(), heap->old_generation()->used());\n+    }\n@@ -1068,1 +1479,1 @@\n-  heap->clear_cancelled_gc();\n+  heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":484,"deletions":73,"binary":false,"changes":557,"status":"modified"},{"patch":"@@ -42,0 +42,2 @@\n+    case _degenerated_roots:\n+      return \"Roots\";\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGC.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+    _degenerated_roots,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGC.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,1068 @@\n+\/*\n+ * Copyright (c) 2020, 2021 Amazon.com, Inc. and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMarkClosures.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMonitoringSupport.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahReferenceProcessor.hpp\"\n+#include \"gc\/shenandoah\/shenandoahTaskqueue.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+#include \"gc\/shenandoah\/shenandoahVerifier.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+\n+class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n+ private:\n+  ShenandoahHeap* _heap;\n+  ShenandoahMarkingContext* const _ctx;\n+ public:\n+  ShenandoahResetUpdateRegionStateClosure() :\n+    _heap(ShenandoahHeap::heap()),\n+    _ctx(_heap->marking_context()) {}\n+\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n+    if (_heap->is_bitmap_slice_committed(r)) {\n+      _ctx->clear_bitmap(r);\n+    }\n+\n+    if (r->is_active()) {\n+      \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n+      \/\/ anyway to capture any updates that happened since now.\n+      _ctx->capture_top_at_mark_start(r);\n+      r->clear_live_data();\n+    }\n+  }\n+\n+  bool is_thread_safe() override { return true; }\n+};\n+\n+class ShenandoahResetBitmapTask : public ShenandoahHeapRegionClosure {\n+ private:\n+  ShenandoahHeap* _heap;\n+  ShenandoahMarkingContext* const _ctx;\n+ public:\n+  ShenandoahResetBitmapTask() :\n+    _heap(ShenandoahHeap::heap()),\n+    _ctx(_heap->marking_context()) {}\n+\n+  void heap_region_do(ShenandoahHeapRegion* region) {\n+    if (_heap->is_bitmap_slice_committed(region)) {\n+      _ctx->clear_bitmap(region);\n+    }\n+  }\n+\n+  bool is_thread_safe() { return true; }\n+};\n+\n+class ShenandoahMergeWriteTable: public ShenandoahHeapRegionClosure {\n+ private:\n+  ShenandoahHeap* _heap;\n+  RememberedScanner* _scanner;\n+ public:\n+  ShenandoahMergeWriteTable() : _heap(ShenandoahHeap::heap()), _scanner(_heap->card_scan()) {}\n+\n+  virtual void heap_region_do(ShenandoahHeapRegion* r) override {\n+    if (r->is_old()) {\n+      _scanner->merge_write_table(r->bottom(), ShenandoahHeapRegion::region_size_words());\n+    }\n+  }\n+\n+  virtual bool is_thread_safe() override {\n+    return true;\n+  }\n+};\n+\n+class ShenandoahSquirrelAwayCardTable: public ShenandoahHeapRegionClosure {\n+ private:\n+  ShenandoahHeap* _heap;\n+  RememberedScanner* _scanner;\n+ public:\n+  ShenandoahSquirrelAwayCardTable() :\n+    _heap(ShenandoahHeap::heap()),\n+    _scanner(_heap->card_scan()) {}\n+\n+  void heap_region_do(ShenandoahHeapRegion* region) {\n+    if (region->is_old()) {\n+      _scanner->reset_remset(region->bottom(), ShenandoahHeapRegion::region_size_words());\n+    }\n+  }\n+\n+  bool is_thread_safe() { return true; }\n+};\n+\n+void ShenandoahGeneration::confirm_heuristics_mode() {\n+  if (_heuristics->is_diagnostic() && !UnlockDiagnosticVMOptions) {\n+    vm_exit_during_initialization(\n+            err_msg(\"Heuristics \\\"%s\\\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.\",\n+                    _heuristics->name()));\n+  }\n+  if (_heuristics->is_experimental() && !UnlockExperimentalVMOptions) {\n+    vm_exit_during_initialization(\n+            err_msg(\"Heuristics \\\"%s\\\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.\",\n+                    _heuristics->name()));\n+  }\n+}\n+\n+ShenandoahHeuristics* ShenandoahGeneration::initialize_heuristics(ShenandoahMode* gc_mode) {\n+  _heuristics = gc_mode->initialize_heuristics(this);\n+  _heuristics->set_guaranteed_gc_interval(ShenandoahGuaranteedGCInterval);\n+  confirm_heuristics_mode();\n+  return _heuristics;\n+}\n+\n+size_t ShenandoahGeneration::bytes_allocated_since_gc_start() {\n+  return Atomic::load(&_bytes_allocated_since_gc_start);;\n+}\n+\n+void ShenandoahGeneration::reset_bytes_allocated_since_gc_start() {\n+  Atomic::store(&_bytes_allocated_since_gc_start, (size_t)0);\n+}\n+\n+void ShenandoahGeneration::increase_allocated(size_t bytes) {\n+  Atomic::add(&_bytes_allocated_since_gc_start, bytes, memory_order_relaxed);\n+}\n+\n+void ShenandoahGeneration::log_status(const char *msg) const {\n+  typedef LogTarget(Info, gc, ergo) LogGcInfo;\n+\n+  if (!LogGcInfo::is_enabled()) {\n+    return;\n+  }\n+\n+  \/\/ Not under a lock here, so read each of these once to make sure\n+  \/\/ byte size in proper unit and proper unit for byte size are consistent.\n+  size_t v_used = used();\n+  size_t v_used_regions = used_regions_size();\n+  size_t v_soft_max_capacity = soft_max_capacity();\n+  size_t v_max_capacity = max_capacity();\n+  size_t v_available = available();\n+  size_t v_adjusted_avail = adjusted_available();\n+  LogGcInfo::print(\"%s: %s generation used: \" SIZE_FORMAT \"%s, used regions: \" SIZE_FORMAT \"%s, \"\n+                   \"soft capacity: \" SIZE_FORMAT \"%s, max capacity: \" SIZE_FORMAT \"%s, available: \" SIZE_FORMAT \"%s, \"\n+                   \"adjusted available: \" SIZE_FORMAT \"%s\",\n+                   msg, name(),\n+                   byte_size_in_proper_unit(v_used), proper_unit_for_byte_size(v_used),\n+                   byte_size_in_proper_unit(v_used_regions), proper_unit_for_byte_size(v_used_regions),\n+                   byte_size_in_proper_unit(v_soft_max_capacity), proper_unit_for_byte_size(v_soft_max_capacity),\n+                   byte_size_in_proper_unit(v_max_capacity), proper_unit_for_byte_size(v_max_capacity),\n+                   byte_size_in_proper_unit(v_available), proper_unit_for_byte_size(v_available),\n+                   byte_size_in_proper_unit(v_adjusted_avail), proper_unit_for_byte_size(v_adjusted_avail));\n+}\n+\n+void ShenandoahGeneration::reset_mark_bitmap() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  heap->assert_gc_workers(heap->workers()->active_workers());\n+\n+  set_mark_incomplete();\n+\n+  ShenandoahResetBitmapTask task;\n+  parallel_heap_region_iterate(&task);\n+}\n+\n+\/\/ The ideal is to swap the remembered set so the safepoint effort is no more than a few pointer manipulations.\n+\/\/ However, limitations in the implementation of the mutator write-barrier make it difficult to simply change the\n+\/\/ location of the card table.  So the interim implementation of swap_remembered_set will copy the write-table\n+\/\/ onto the read-table and will then clear the write-table.\n+void ShenandoahGeneration::swap_remembered_set() {\n+  \/\/ Must be sure that marking is complete before we swap remembered set.\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  heap->assert_gc_workers(heap->workers()->active_workers());\n+  shenandoah_assert_safepoint();\n+\n+  \/\/ TODO: Eventually, we want replace this with a constant-time exchange of pointers.\n+  ShenandoahSquirrelAwayCardTable task;\n+  heap->old_generation()->parallel_heap_region_iterate(&task);\n+}\n+\n+\/\/ If a concurrent cycle fails _after_ the card table has been swapped we need to update the read card\n+\/\/ table with any writes that have occurred during the transition to the degenerated cycle. Without this,\n+\/\/ newly created objects which are only referenced by old objects could be lost when the remembered set\n+\/\/ is scanned during the degenerated mark.\n+void ShenandoahGeneration::merge_write_table() {\n+  \/\/ This should only happen for degenerated cycles\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  heap->assert_gc_workers(heap->workers()->active_workers());\n+  shenandoah_assert_safepoint();\n+\n+  ShenandoahMergeWriteTable task;\n+  heap->old_generation()->parallel_heap_region_iterate(&task);\n+}\n+\n+void ShenandoahGeneration::prepare_gc() {\n+  \/\/ Invalidate the marking context\n+  set_mark_incomplete();\n+\n+  \/\/ Capture Top At Mark Start for this generation (typically young) and reset mark bitmap.\n+  ShenandoahResetUpdateRegionStateClosure cl;\n+  parallel_heap_region_iterate(&cl);\n+}\n+\n+void ShenandoahGeneration::compute_evacuation_budgets(ShenandoahHeap* heap, bool* preselected_regions,\n+                                                      ShenandoahCollectionSet* collection_set,\n+                                                      size_t &consumed_by_advance_promotion) {\n+  assert(heap->mode()->is_generational(), \"Only generational mode uses evacuation budgets.\");\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t regions_available_to_loan = 0;\n+  size_t minimum_evacuation_reserve = ShenandoahOldCompactionReserve * region_size_bytes;\n+  size_t old_regions_loaned_for_young_evac = 0;\n+  consumed_by_advance_promotion = 0;\n+\n+  ShenandoahGeneration* old_generation = heap->old_generation();\n+  ShenandoahYoungGeneration* young_generation = heap->young_generation();\n+  size_t old_evacuation_reserve = 0;\n+  size_t num_regions = heap->num_regions();\n+\n+  \/\/ During initialization and phase changes, it is more likely that fewer objects die young and old-gen\n+  \/\/ memory is not yet full (or is in the process of being replaced).  During these times especially, it\n+  \/\/ is beneficial to loan memory from old-gen to young-gen during the evacuation and update-refs phases\n+  \/\/ of execution.\n+\n+  \/\/ Calculate EvacuationReserve before PromotionReserve.  Evacuation is more critical than promotion.\n+  \/\/ If we cannot evacuate old-gen, we will not be able to reclaim old-gen memory.  Promotions are less\n+  \/\/ critical.  If we cannot promote, there may be degradation of young-gen memory because old objects\n+  \/\/ accumulate there until they can be promoted.  This increases the young-gen marking and evacuation work.\n+\n+  \/\/ Do not fill up old-gen memory with promotions.  Reserve some amount of memory for compaction purposes.\n+  ShenandoahOldHeuristics* old_heuristics = heap->old_heuristics();\n+  size_t young_evac_reserve_max = 0;\n+  if (old_heuristics->unprocessed_old_collection_candidates() > 0) {\n+    \/\/ Compute old_evacuation_reserve: how much memory are we reserving to hold the results of\n+    \/\/ evacuating old-gen heap regions?  In order to sustain a consistent pace of young-gen collections,\n+    \/\/ the goal is to maintain a consistent value for this parameter (when the candidate set is not\n+    \/\/ empty).  This value is the minimum of:\n+    \/\/   1. old_gen->available()\n+    \/\/   2. old-gen->capacity() * ShenandoahOldEvacReserve) \/ 100\n+    \/\/       (e.g. old evacuation should be no larger than 5% of old_gen capacity)\n+    \/\/   3. ((young_gen->capacity * ShenandoahEvacReserve \/ 100) * ShenandoahOldEvacRatioPercent) \/ 100\n+    \/\/       (e.g. old evacuation should be no larger than 12% of young-gen evacuation)\n+    old_evacuation_reserve = old_generation->available();\n+    assert(old_evacuation_reserve > minimum_evacuation_reserve, \"Old-gen available has not been preserved!\");\n+    size_t old_evac_reserve_max = old_generation->soft_max_capacity() * ShenandoahOldEvacReserve \/ 100;\n+    if (old_evac_reserve_max < old_evacuation_reserve) {\n+      old_evacuation_reserve = old_evac_reserve_max;\n+    }\n+    young_evac_reserve_max =\n+      (((young_generation->soft_max_capacity() * ShenandoahEvacReserve) \/ 100) * ShenandoahOldEvacRatioPercent) \/ 100;\n+    if (young_evac_reserve_max < old_evacuation_reserve) {\n+      old_evacuation_reserve = young_evac_reserve_max;\n+    }\n+  }\n+\n+  if (minimum_evacuation_reserve > old_generation->available()) {\n+    \/\/ Due to round-off errors during enforcement of minimum_evacuation_reserve during previous GC passes,\n+    \/\/ there can be slight discrepancies here.\n+    minimum_evacuation_reserve = old_generation->available();\n+  }\n+\n+  heap->set_old_evac_reserve(old_evacuation_reserve);\n+  heap->reset_old_evac_expended();\n+\n+  \/\/ Compute the young evacuation reserve: This is how much memory is available for evacuating young-gen objects.\n+  \/\/ We ignore the possible effect of promotions, which reduce demand for young-gen evacuation memory.\n+  \/\/\n+  \/\/ TODO: We could give special treatment to the regions that have reached promotion age, because we know their\n+  \/\/ live data is entirely eligible for promotion.  This knowledge can feed both into calculations of young-gen\n+  \/\/ evacuation reserve and promotion reserve.\n+  \/\/\n+  \/\/  young_evacuation_reserve for young generation: how much memory are we reserving to hold the results\n+  \/\/  of evacuating young collection set regions?  This is typically smaller than the total amount\n+  \/\/  of available memory, and is also smaller than the total amount of marked live memory within\n+  \/\/  young-gen.  This value is the smaller of\n+  \/\/\n+  \/\/    1. (young_gen->capacity() * ShenandoahEvacReserve) \/ 100\n+  \/\/    2. (young_gen->available() + old_gen_memory_available_to_be_loaned\n+  \/\/\n+  \/\/  ShenandoahEvacReserve represents the configured target size of the evacuation region.  We can only honor\n+  \/\/  this target if there is memory available to hold the evacuations.  Memory is available if it is already\n+  \/\/  free within young gen, or if it can be borrowed from old gen.  Since we have not yet chosen the collection\n+  \/\/  sets, we do not yet know the exact accounting of how many regions will be freed by this collection pass.\n+  \/\/  What we do know is that there will be at least one evacuated young-gen region for each old-gen region that\n+  \/\/  is loaned to the evacuation effort (because regions to be collected consume more memory than the compacted\n+  \/\/  regions that will replace them).  In summary, if there are old-gen regions that are available to hold the\n+  \/\/  results of young-gen evacuations, it is safe to loan them for this purpose.  At this point, we have not yet\n+  \/\/  established a promoted_reserve.  We'll do that after we choose the collection set and analyze its impact\n+  \/\/  on available memory.\n+  \/\/\n+  \/\/ We do not know the evacuation_supplement until after we have computed the collection set.  It is not always\n+  \/\/ the case that young-regions inserted into the collection set will result in net decrease of in-use regions\n+  \/\/ because ShenandoahEvacWaste times multiplied by memory within the region may be larger than the region size.\n+  \/\/ The problem is especially relevant to regions that have been inserted into the collection set because they have\n+  \/\/ reached tenure age.  These regions tend to have much higher utilization (e.g. 95%).  These regions also offer\n+  \/\/ a unique opportunity because we know that every live object contained within the region is elgible to be\n+  \/\/ promoted.  Thus, the following implementation treats these regions specially:\n+  \/\/\n+  \/\/  1. Before beginning collection set selection, we tally the total amount of live memory held within regions\n+  \/\/     that are known to have reached tenure age.  If this memory times ShenandoahEvacWaste is available within\n+  \/\/     old-gen memory, establish an advance promotion reserve to hold all or some percentage of these objects.\n+  \/\/     This advance promotion reserve is excluded from memory available for holding old-gen evacuations and cannot\n+  \/\/     be \"loaned\" to young gen.\n+  \/\/\n+  \/\/  2. Tenure-aged regions are included in the collection set iff their evacuation size * ShenandoahEvacWaste fits\n+  \/\/     within the advance promotion reserve.  It is counter productive to evacuate these regions if they cannot be\n+  \/\/     evacuated directly into old-gen memory.  So if there is not sufficient memory to hold copies of their\n+  \/\/     live data right now, we'll just let these regions remain in young for now, to be evacuated by a subsequent\n+  \/\/     evacuation pass.\n+  \/\/\n+  \/\/  3. Next, we calculate a young-gen evacuation budget, which is the smaller of the two quantities mentioned\n+  \/\/     above.  old_gen_memory_available_to_be_loaned is calculated as:\n+  \/\/       old_gen->available - (advance-promotion-reserve + old-gen_evacuation_reserve)\n+  \/\/\n+  \/\/  4. When choosing the collection set, special care is taken to assure that the amount of loaned memory required to\n+  \/\/     hold the results of evacuation is smaller than the total memory occupied by the regions added to the collection\n+  \/\/     set.  We need to take these precautions because we do not know how much memory will be reclaimed by evacuation\n+  \/\/     until after the collection set has been constructed.  The algorithm is as follows:\n+  \/\/\n+  \/\/     a. We feed into the algorithm (i) young available at the start of evacuation and (ii) the amount of memory\n+  \/\/        loaned from old-gen that is available to hold the results of evacuation.\n+  \/\/     b. As candidate regions are added into the young-gen collection set, we maintain accumulations of the amount\n+  \/\/        of memory spanned by the collection set regions and the amount of memory that must be reserved to hold\n+  \/\/        evacuation results (by multiplying live-data size by ShenandoahEvacWaste).  We process candidate regions\n+  \/\/        in order of decreasing amounts of garbage.  We skip over (and do not include into the collection set) any\n+  \/\/        regions that do not satisfy all of the following conditions:\n+  \/\/\n+  \/\/          i. The amount of live data within the region as scaled by ShenandoahEvacWaste must fit within the\n+  \/\/             relevant evacuation reserve (live data of old-gen regions must fit within the old-evac-reserve, live\n+  \/\/             data of young-gen tenure-aged regions must fit within the advance promotion reserve, live data within\n+  \/\/             other young-gen regions must fit within the youn-gen evacuation reserve).\n+  \/\/         ii. The accumulation of memory consumed by evacuation must not exceed the accumulation of memory reclaimed\n+  \/\/             through evacuation by more than young-gen available.\n+  \/\/        iii. Other conditions may be enforced as appropriate for specific heuristics.\n+  \/\/\n+  \/\/       Note that regions are considered for inclusion in the selection set in order of decreasing amounts of garbage.\n+  \/\/       It is possible that a region with a larger amount of garbage will be rejected because it also has a larger\n+  \/\/       amount of live data and some region that follows this region in candidate order is included in the collection\n+  \/\/       set (because it has less live data and thus can fit within the evacuation limits even though it has less\n+  \/\/       garbage).\n+\n+  size_t young_evacuation_reserve = (young_generation->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+  \/\/ old evacuation can pack into existing partially used regions.  young evacuation and loans for young allocations\n+  \/\/ need to target regions that do not already hold any old-gen objects.  Round down.\n+  regions_available_to_loan = old_generation->free_unaffiliated_regions();\n+\n+  size_t required_evacuation_reserve;\n+  \/\/ Memory evacuated from old-gen on this pass will be available to hold old-gen evacuations in next pass.\n+  if (old_evacuation_reserve > minimum_evacuation_reserve) {\n+    required_evacuation_reserve = 0;\n+  } else {\n+    required_evacuation_reserve = minimum_evacuation_reserve - old_evacuation_reserve;\n+  }\n+\n+  consumed_by_advance_promotion = _heuristics->select_aged_regions(\n+    old_generation->available() - old_evacuation_reserve - required_evacuation_reserve, num_regions, preselected_regions);\n+  size_t net_available_old_regions =\n+    (old_generation->available() - old_evacuation_reserve - consumed_by_advance_promotion) \/ region_size_bytes;\n+\n+ if (regions_available_to_loan > net_available_old_regions) {\n+    regions_available_to_loan = net_available_old_regions;\n+  }\n+\n+  \/\/ Otherwise, regions_available_to_loan is less than net_available_old_regions because available memory is\n+  \/\/ scattered between multiple partially used regions.\n+\n+  if (young_evacuation_reserve > young_generation->available()) {\n+    size_t short_fall = young_evacuation_reserve - young_generation->available();\n+    if (regions_available_to_loan * region_size_bytes >= short_fall) {\n+      old_regions_loaned_for_young_evac = (short_fall + region_size_bytes - 1) \/ region_size_bytes;\n+      regions_available_to_loan -= old_regions_loaned_for_young_evac;\n+    } else {\n+      old_regions_loaned_for_young_evac = regions_available_to_loan;\n+      regions_available_to_loan = 0;\n+      young_evacuation_reserve = young_generation->available() + old_regions_loaned_for_young_evac * region_size_bytes;\n+      \/\/ In this case, there's no memory available for new allocations while evacuating and updating, unless we\n+      \/\/ find more old-gen memory to borrow below.\n+    }\n+  }\n+  \/\/ In generational mode, we may end up choosing a young collection set that contains so many promotable objects\n+  \/\/ that there is not sufficient space in old generation to hold the promoted objects.  That is ok because we have\n+  \/\/ assured there is sufficient space in young generation to hold the rejected promotion candidates.  These rejected\n+  \/\/ promotion candidates will presumably be promoted in a future evacuation cycle.\n+  heap->set_young_evac_reserve(young_evacuation_reserve);\n+  collection_set->establish_preselected(preselected_regions);\n+}\n+\n+\/\/ Having chosen the collection set, adjust the budgets for generatioal mode based on its composition.  Note\n+\/\/ that young_generation->available() now knows about recently discovered immediate garbage.\n+\n+void ShenandoahGeneration::adjust_evacuation_budgets(ShenandoahHeap* heap, ShenandoahCollectionSet* collection_set,\n+                                                     size_t consumed_by_advance_promotion) {\n+  \/\/ We may find that old_evacuation_reserve and\/or loaned_for_young_evacuation are not fully consumed, in which case we may\n+  \/\/  be able to increase regions_available_to_loan\n+\n+  \/\/ The role of adjust_evacuation_budgets() is to compute the correct value of regions_available_to_loan and to make\n+  \/\/ effective use of this memory, including the remnant memory within these regions that may result from rounding loan to\n+  \/\/ integral number of regions.  Excess memory that is available to be loaned is applied to an allocation supplement,\n+  \/\/ which allows mutators to allocate memory beyond the current capacity of young-gen on the promise that the loan\n+  \/\/ will be repaid as soon as we finish updating references for the recently evacuated collection set.\n+\n+  \/\/ We cannot recalculate regions_available_to_loan by simply dividing old_generation->available() by region_size_bytes\n+  \/\/ because the available memory may be distributed between many partially occupied regions that are already holding old-gen\n+  \/\/ objects.  Memory in partially occupied regions is not \"available\" to be loaned.  Note that an increase in old-gen\n+  \/\/ available that results from a decrease in memory consumed by old evacuation is not necessarily available to be loaned\n+  \/\/ to young-gen.\n+\n+  assert(heap->mode()->is_generational(), \"Only generational mode uses evacuation budgets.\");\n+  size_t old_regions_loaned_for_young_evac, regions_available_to_loan;\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  ShenandoahOldGeneration* old_generation = heap->old_generation();\n+  ShenandoahYoungGeneration* young_generation = heap->young_generation();\n+  size_t old_evacuated = collection_set->get_old_bytes_reserved_for_evacuation();\n+  size_t old_evacuated_committed = (size_t) (ShenandoahEvacWaste * old_evacuated);\n+  size_t old_evacuation_reserve = heap->get_old_evac_reserve();\n+  \/\/ Immediate garbage found during choose_collection_set() is all young\n+  size_t immediate_garbage = collection_set->get_immediate_trash();\n+  size_t old_available = old_generation->available();\n+  size_t young_available = young_generation->available() + immediate_garbage;\n+  size_t loaned_regions = 0;\n+  size_t available_loan_remnant = 0; \/\/ loaned memory that is not yet dedicated to any particular budget\n+\n+  assert(((consumed_by_advance_promotion * 33) \/ 32) >= collection_set->get_young_bytes_to_be_promoted() * ShenandoahEvacWaste,\n+         \"Advance promotion (\" SIZE_FORMAT \") should be at least young_bytes_to_be_promoted (\" SIZE_FORMAT\n+         \")* ShenandoahEvacWaste, totalling: \" SIZE_FORMAT \", within round-off errors of up to 3.125%%\",\n+         consumed_by_advance_promotion, collection_set->get_young_bytes_to_be_promoted(),\n+         (size_t) (collection_set->get_young_bytes_to_be_promoted() * ShenandoahEvacWaste));\n+\n+  assert(consumed_by_advance_promotion <= (collection_set->get_young_bytes_to_be_promoted() * ShenandoahEvacWaste * 33) \/ 32,\n+         \"Round-off errors should be less than 3.125%%, consumed by advance: \" SIZE_FORMAT \", promoted: \" SIZE_FORMAT,\n+         consumed_by_advance_promotion, (size_t) (collection_set->get_young_bytes_to_be_promoted() * ShenandoahEvacWaste));\n+\n+  collection_set->abandon_preselected();\n+\n+  if (old_evacuated_committed > old_evacuation_reserve) {\n+    \/\/ This should only happen due to round-off errors when enforcing ShenandoahEvacWaste\n+    assert(old_evacuated_committed <= (33 * old_evacuation_reserve) \/ 32,\n+           \"Round-off errors should be less than 3.125%%, committed: \" SIZE_FORMAT \", reserved: \" SIZE_FORMAT,\n+           old_evacuated_committed, old_evacuation_reserve);\n+    old_evacuated_committed = old_evacuation_reserve;\n+  } else if (old_evacuated_committed < old_evacuation_reserve) {\n+    \/\/ This may happen if the old-gen collection consumes less than full budget.\n+    old_evacuation_reserve = old_evacuated_committed;\n+    heap->set_old_evac_reserve(old_evacuation_reserve);\n+  }\n+\n+  \/\/ Recompute old_regions_loaned_for_young_evac because young-gen collection set may not need all the memory\n+  \/\/ originally reserved.\n+  size_t young_promoted = collection_set->get_young_bytes_to_be_promoted();\n+  size_t young_promoted_reserve_used = (size_t) (ShenandoahEvacWaste * young_promoted);\n+\n+  size_t young_evacuated = collection_set->get_young_bytes_reserved_for_evacuation();\n+  size_t young_evacuated_reserve_used = (size_t) (ShenandoahEvacWaste * young_evacuated);\n+\n+  \/\/ We'll invoke heap->set_young_evac_reserve() further below, after we make additional adjustments to its value\n+\n+  \/\/ Adjust old_regions_loaned_for_young_evac to feed into calculations of promoted_reserve\n+  if (young_evacuated_reserve_used > young_available) {\n+    size_t short_fall = young_evacuated_reserve_used - young_available;\n+\n+    \/\/ region_size_bytes is a power of 2.  loan an integral number of regions.\n+    size_t revised_loan_for_young_evacuation = (short_fall + region_size_bytes - 1) \/ region_size_bytes;\n+\n+    \/\/ available_loan_remnant represents memory loaned from old-gen but not required for young evacuation.\n+    \/\/ This is the excess loaned memory that results from rounding the required loan up to an integral number\n+    \/\/ of heap regions.  This will be dedicated to alloc_supplement below.\n+    available_loan_remnant = (revised_loan_for_young_evacuation * region_size_bytes) - short_fall;\n+\n+    \/\/ We previously loaned more than was required by young-gen evacuation.  So claw some of this memory back.\n+    old_regions_loaned_for_young_evac = revised_loan_for_young_evacuation;\n+    loaned_regions = old_regions_loaned_for_young_evac;\n+  } else {\n+    \/\/ Undo the prevous loan, if any.\n+    old_regions_loaned_for_young_evac = 0;\n+    loaned_regions = 0;\n+  }\n+\n+  size_t old_bytes_loaned_for_young_evac = old_regions_loaned_for_young_evac * region_size_bytes - available_loan_remnant;\n+\n+  \/\/ Recompute regions_available_to_loan based on possible changes to old_regions_loaned_for_young_evac and\n+  \/\/ old_evacuation_reserve.\n+\n+  \/\/ Any decrease in old_regions_loaned_for_young_evac are immediately available to be loaned\n+  \/\/ However, a change to old_evacuation_reserve() is not necessarily available to loan, because this memory may\n+  \/\/ reside within many fragments scattered throughout old-gen.\n+\n+  regions_available_to_loan = old_generation->free_unaffiliated_regions();\n+  size_t working_old_available = old_generation->available();\n+\n+  assert(regions_available_to_loan * region_size_bytes <= working_old_available,\n+         \"Regions available to loan  must be less than available memory\");\n+\n+  \/\/ fragmented_old_total is the amount of memory in old-gen beyond regions_available_to_loan that is otherwise not\n+  \/\/ yet dedicated to a particular budget.  This memory can be used for promotion_reserve.\n+  size_t fragmented_old_total = working_old_available - regions_available_to_loan * region_size_bytes;\n+\n+  \/\/ fragmented_old_usage is the memory that is dedicated to holding evacuated old-gen objects, which does not need\n+  \/\/ to be an integral number of regions.\n+  size_t fragmented_old_usage = old_evacuated_committed + consumed_by_advance_promotion;\n+\n+\n+\n+  if (fragmented_old_total >= fragmented_old_usage) {\n+    \/\/ Seems this will be rare.  In this case, all of the memory required for old-gen evacuations and promotions can be\n+    \/\/ taken from the existing fragments within old-gen.  Reduce this fragmented total by this amount.\n+    fragmented_old_total -= fragmented_old_usage;\n+    \/\/ And reduce regions_available_to_loan by the regions dedicated to young_evac.\n+    regions_available_to_loan -= old_regions_loaned_for_young_evac;\n+  } else {\n+    \/\/ In this case, we need to dedicate some of the regions_available_to_loan to hold the results of old-gen evacuations\n+    \/\/ and promotions.\n+\n+    size_t unaffiliated_memory_required_for_old = fragmented_old_usage - fragmented_old_total;\n+    size_t unaffiliated_regions_used_by_old = (unaffiliated_memory_required_for_old + region_size_bytes - 1) \/ region_size_bytes;\n+    regions_available_to_loan -= (unaffiliated_regions_used_by_old + old_regions_loaned_for_young_evac);\n+\n+    size_t memory_for_promotions_and_old_evac = fragmented_old_total + unaffiliated_regions_used_by_old;\n+    size_t memory_required_for_promotions_and_old_evac = fragmented_old_usage;\n+    size_t excess_fragmented = memory_for_promotions_and_old_evac - memory_required_for_promotions_and_old_evac;\n+    fragmented_old_total = excess_fragmented;\n+  }\n+\n+  \/\/ Subtract from working_old_available old_evacuated_committed and consumed_by_advance_promotion\n+  working_old_available -= fragmented_old_usage;\n+  \/\/ And also subtract out the regions loaned for young evacuation\n+  working_old_available -= old_regions_loaned_for_young_evac * region_size_bytes;\n+\n+  \/\/ Assure that old_evacuated_committed + old_bytes_loaned_for_young_evac >= the minimum evacuation reserve\n+  \/\/ in order to prevent promotion reserve from violating minimum evacuation reserve.\n+  size_t old_regions_reserved_for_alloc_supplement = 0;\n+  size_t old_bytes_reserved_for_alloc_supplement = 0;\n+  size_t reserved_bytes_for_future_old_evac = 0;\n+\n+  old_bytes_reserved_for_alloc_supplement = available_loan_remnant;\n+  available_loan_remnant = 0;\n+\n+  \/\/ Memory that has been loaned for young evacuations and old-gen regions in the current mixed-evacuation collection\n+  \/\/ set will be available to hold future old-gen evacuations.  If this memory is less than the desired amount of memory\n+  \/\/ set aside for old-gen compaction reserve, try to set aside additional memory so that it will be available during\n+  \/\/ the next mixed evacuation cycle.  Note that memory loaned to young-gen for allocation supplement is excluded from\n+  \/\/ the old-gen promotion reserve.\n+  size_t future_evac_reserve_regions = old_regions_loaned_for_young_evac + collection_set->get_old_region_count();\n+  size_t collected_regions = collection_set->get_young_region_count();\n+\n+  if (future_evac_reserve_regions < ShenandoahOldCompactionReserve) {\n+    \/\/ Require that we loan more memory for holding young evacuations to assure that we have adequate reserves to receive\n+    \/\/ old-gen evacuations during subsequent collections.  Loaning this memory for an allocation supplement does not\n+    \/\/ satisfy our needs because newly allocated objects are not necessarily counter-balanced by reclaimed collection\n+    \/\/ set regions.\n+\n+    \/\/ Put this memory into reserve by identifying it as old_regions_loaned_for_young_evac\n+    size_t additional_regions_to_loan = ShenandoahOldCompactionReserve - future_evac_reserve_regions;\n+\n+    \/\/ We can loan additional regions to be repaid from the anticipated recycling of young collection set regions\n+    \/\/ provided that these regions are currently available within old-gen memory.\n+    size_t collected_regions_to_loan;\n+    if (collected_regions >= additional_regions_to_loan) {\n+      collected_regions_to_loan = additional_regions_to_loan;\n+      additional_regions_to_loan = 0;\n+    } else if (collected_regions > 0) {\n+      collected_regions_to_loan = collected_regions;\n+      additional_regions_to_loan -= collected_regions_to_loan;\n+    } else {\n+      collected_regions_to_loan = 0;\n+    }\n+\n+    if (collected_regions_to_loan > 0) {\n+      \/\/ We're evacuating at least this many regions, it's ok to use these regions for allocation supplement since\n+      \/\/ we'll be able to repay the loan at end of this GC pass, assuming the regions are available.\n+      if (collected_regions_to_loan > regions_available_to_loan) {\n+        collected_regions_to_loan = regions_available_to_loan;\n+      }\n+      old_bytes_reserved_for_alloc_supplement += collected_regions_to_loan * region_size_bytes;\n+      regions_available_to_loan -= collected_regions_to_loan;\n+      loaned_regions += collected_regions_to_loan;\n+      working_old_available -= collected_regions_to_loan * region_size_bytes;\n+    }\n+\n+    \/\/ If there's still memory that we want to exclude from the current promotion reserve, but we are unable to loan\n+    \/\/ this memory because fully empty old-gen regions are not available, decrement the working_old_available to make\n+    \/\/ sure that this memory is not used to hold the results of old-gen evacuation.\n+    if (additional_regions_to_loan > regions_available_to_loan) {\n+      size_t unloaned_regions = additional_regions_to_loan - regions_available_to_loan;\n+      size_t unloaned_bytes = unloaned_regions * region_size_bytes;\n+\n+      if (working_old_available < unloaned_bytes) {\n+        \/\/ We're in dire straits.  We won't be able to reserve all the memory that we want to make available for the\n+        \/\/ next old-gen evacuation.  We'll reserve as much of it as possible.  Setting working_old_available to zero\n+        \/\/ means there will be no promotion except for the advance promotion.  Note that if some advance promotion fails,\n+        \/\/ the object will be evacuated to young-gen so we should still end up reclaiming the entire advance promotion\n+        \/\/ collection set.\n+        reserved_bytes_for_future_old_evac = working_old_available;\n+        working_old_available = 0;\n+      } else {\n+        reserved_bytes_for_future_old_evac = unloaned_bytes;\n+        working_old_available -= unloaned_bytes;\n+      }\n+      size_t regions_reserved_for_future_old_evac =\n+        (reserved_bytes_for_future_old_evac + region_size_bytes - 1) \/ region_size_bytes;\n+\n+      if (regions_reserved_for_future_old_evac < regions_available_to_loan) {\n+        regions_available_to_loan -= regions_reserved_for_future_old_evac;\n+      } else {\n+        regions_available_to_loan = 0;\n+      }\n+\n+      \/\/ Since we're in dire straits, zero out fragmented_old_total so this won't be used for promotion;\n+      if (working_old_available > fragmented_old_total) {\n+        working_old_available -= fragmented_old_total;\n+      } else {\n+        working_old_available = 0;\n+      }\n+      fragmented_old_total = 0;\n+    }\n+  }\n+\n+  \/\/ Establish young_evac_reserve so that this young-gen memory is not used for new allocations, allowing the memory\n+  \/\/ to be returned to old-gen as soon as the current collection set regions are reclaimed.\n+  heap->set_young_evac_reserve(young_evacuated_reserve_used);\n+\n+  \/\/ Limit promoted_reserve so that we can set aside memory to be loaned from old-gen to young-gen.  This\n+  \/\/ value is not \"critical\".  If we underestimate, certain promotions will simply be deferred.  If we put\n+  \/\/ \"all the rest\" of old-gen memory into the promotion reserve, we'll have nothing left to loan to young-gen\n+  \/\/ during the evac and update phases of GC.  So we \"limit\" the sizes of the promotion budget to be the smaller of:\n+  \/\/\n+  \/\/  1. old_available\n+  \/\/     (old_available is old_gen->available() -\n+  \/\/      (old_evacuated_committed + consumed_by_advance_promotion + loaned_for_young_evac + reserved_for_alloc_supplement))\n+  \/\/  2. young bytes reserved for evacuation (we can't promote more than young is evacuating)\n+  size_t promotion_reserve = working_old_available;\n+\n+  \/\/ We experimented with constraining promoted_reserve to be no larger than 4 times the size of previously_promoted,\n+  \/\/ but this constraint was too limiting, resulting in failure of legitimate promotions.  This was tried before we\n+  \/\/ had special handling in place for advance promotion.  We should retry now that advance promotion is handled\n+  \/\/ specially.\n+\n+  \/\/ We had also experimented with constraining promoted_reserve to be no more than young_evacuation_committed\n+  \/\/ divided by promotion_divisor, where:\n+  \/\/  size_t promotion_divisor = (0x02 << InitialTenuringThreshold) - 1;\n+  \/\/ This also was found to be too limiting, resulting in failure of legitimate promotions.\n+  \/\/\n+  \/\/ Both experiments were conducted in the presence of other bugs which could have been the root cause for\n+  \/\/ the failures identified above as being \"too limiting\".  TODO: conduct new experiments with the more limiting\n+  \/\/ values of young_evacuation_reserved_used.\n+\n+  \/\/ young_evacuation_reserve_used already excludes bytes known to be promoted, which equals consumed_by_advance_promotion\n+  if (young_evacuated_reserve_used < promotion_reserve) {\n+    \/\/ Shrink promotion_reserve if it is larger than the memory to be consumed by evacuating all young objects in\n+    \/\/ collection set, including anticipated waste.  There's no benefit in using a larger promotion_reserve.\n+    \/\/ young_evacuation_reserve_used does not include live memory within tenure-aged regions.\n+    promotion_reserve = young_evacuated_reserve_used;\n+  }\n+  assert(working_old_available >= promotion_reserve, \"Cannot reserve for promotion more than is available\");\n+  working_old_available -= promotion_reserve;\n+  \/\/ Having reserved this memory for promotion, the regions are no longer available to be loaned.\n+  size_t regions_consumed_by_promotion_reserve = (promotion_reserve + region_size_bytes - 1) \/ region_size_bytes;\n+  if (regions_consumed_by_promotion_reserve > regions_available_to_loan) {\n+    \/\/ This can happen if the promotion reserve makes use of memory that is fragmented between many partially available\n+    \/\/ old-gen regions.\n+    regions_available_to_loan = 0;\n+  } else {\n+    regions_available_to_loan -= regions_consumed_by_promotion_reserve;\n+  }\n+\n+  log_debug(gc)(\"old_gen->available(): \" SIZE_FORMAT \" divided between promotion reserve: \" SIZE_FORMAT\n+                \", old evacuation reserve: \" SIZE_FORMAT \", advance promotion reserve supplement: \" SIZE_FORMAT\n+                \", old loaned for young evacuation: \" SIZE_FORMAT \", old reserved for alloc supplement: \" SIZE_FORMAT,\n+                old_generation->available(), promotion_reserve, old_evacuated_committed, consumed_by_advance_promotion,\n+                old_regions_loaned_for_young_evac * region_size_bytes, old_bytes_reserved_for_alloc_supplement);\n+\n+  promotion_reserve += consumed_by_advance_promotion;\n+  heap->set_promoted_reserve(promotion_reserve);\n+\n+  heap->reset_promoted_expended();\n+  if (collection_set->get_old_bytes_reserved_for_evacuation() == 0) {\n+    \/\/ Setting old evacuation reserve to zero denotes that there is no old-gen evacuation in this pass.\n+    heap->set_old_evac_reserve(0);\n+  }\n+\n+  size_t old_gen_usage_base = old_generation->used() - collection_set->get_old_garbage();\n+  heap->capture_old_usage(old_gen_usage_base);\n+\n+  \/\/ Compute additional evacuation supplement, which is extra memory borrowed from old-gen that can be allocated\n+  \/\/ by mutators while GC is working on evacuation and update-refs.  This memory can be temporarily borrowed\n+  \/\/ from old-gen allotment, then repaid at the end of update-refs from the recycled collection set.  After\n+  \/\/ we have computed the collection set based on the parameters established above, we can make additional\n+  \/\/ loans based on our knowledge of the collection set to determine how much allocation we can allow\n+  \/\/ during the evacuation and update-refs phases of execution.  The total available supplement is the result\n+  \/\/ of adding old_bytes_reserved_for_alloc_supplement to the smaller of:\n+  \/\/\n+  \/\/   1. regions_available_to_loan * region_size_bytes\n+  \/\/   2. The replenishment budget (number of regions in collection set - the number of regions already\n+  \/\/         under lien for the young_evacuation_reserve)\n+  \/\/\n+\n+  \/\/ Regardless of how many regions may be available to be loaned, we can loan no more regions than\n+  \/\/ the total number of young regions to be evacuated.  Call this the regions_for_runway.\n+\n+  if (regions_available_to_loan > 0 && (collected_regions > loaned_regions)) {\n+    assert(regions_available_to_loan * region_size_bytes <= working_old_available,\n+           \"regions_available_to_loan should not exceed working_old_available\");\n+\n+    size_t additional_regions_to_loan = collected_regions - loaned_regions;\n+    if (additional_regions_to_loan > regions_available_to_loan) {\n+      additional_regions_to_loan = regions_available_to_loan;\n+    }\n+    loaned_regions += additional_regions_to_loan;\n+    old_bytes_reserved_for_alloc_supplement += additional_regions_to_loan * region_size_bytes;\n+    working_old_available -= additional_regions_to_loan * region_size_bytes;\n+  }\n+  size_t allocation_supplement = old_bytes_reserved_for_alloc_supplement + old_bytes_loaned_for_young_evac;\n+  assert(allocation_supplement % ShenandoahHeapRegion::region_size_bytes() == 0,\n+         \"allocation_supplement must be multiple of region size\");\n+\n+  heap->set_alloc_supplement_reserve(allocation_supplement);\n+\n+  \/\/ TODO: young_available, which feeds into alloc_budget_evac_and_update is lacking memory available within\n+  \/\/ existing young-gen regions that were not selected for the collection set.  Add this in and adjust the\n+  \/\/ log message (where it says \"empty-region allocation budget\").\n+\n+\n+  log_debug(gc)(\"Memory reserved for young evacuation: \" SIZE_FORMAT \"%s for evacuating \" SIZE_FORMAT\n+                \"%s out of young available: \" SIZE_FORMAT \"%s\",\n+                byte_size_in_proper_unit(young_evacuated_reserve_used),\n+                proper_unit_for_byte_size(young_evacuated_reserve_used),\n+                byte_size_in_proper_unit(young_evacuated), proper_unit_for_byte_size(young_evacuated),\n+                byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n+\n+  log_debug(gc)(\"Memory reserved for old evacuation: \" SIZE_FORMAT \"%s for evacuating \" SIZE_FORMAT\n+                \"%s out of old available: \" SIZE_FORMAT \"%s\",\n+                byte_size_in_proper_unit(old_evacuated), proper_unit_for_byte_size(old_evacuated),\n+                byte_size_in_proper_unit(old_evacuated), proper_unit_for_byte_size(old_evacuated),\n+                byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available));\n+\n+  size_t regular_promotion = promotion_reserve - consumed_by_advance_promotion;\n+  size_t excess =\n+    old_available - (old_evacuation_reserve + promotion_reserve + old_bytes_loaned_for_young_evac + allocation_supplement);\n+  log_info(gc, ergo)(\"Old available: \" SIZE_FORMAT \"%s is partitioned into old evacuation budget: \" SIZE_FORMAT\n+                     \"%s, aged region promotion budget: \" SIZE_FORMAT\n+                     \"%s, regular region promotion budget: \" SIZE_FORMAT\n+                     \"%s, loaned for young evacuation: \" SIZE_FORMAT\n+                     \"%s, loaned for young allocations: \" SIZE_FORMAT\n+                     \"%s, excess: \" SIZE_FORMAT \"%s\",\n+                     byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n+                     byte_size_in_proper_unit(old_evacuation_reserve), proper_unit_for_byte_size(old_evacuation_reserve),\n+                     byte_size_in_proper_unit(consumed_by_advance_promotion),\n+                     proper_unit_for_byte_size(consumed_by_advance_promotion),\n+                     byte_size_in_proper_unit(regular_promotion), proper_unit_for_byte_size(regular_promotion),\n+                     byte_size_in_proper_unit(old_bytes_loaned_for_young_evac),\n+                     proper_unit_for_byte_size(old_bytes_loaned_for_young_evac),\n+                     byte_size_in_proper_unit(allocation_supplement), proper_unit_for_byte_size(allocation_supplement),\n+                     byte_size_in_proper_unit(excess), proper_unit_for_byte_size(excess));\n+}\n+\n+void ShenandoahGeneration::prepare_regions_and_collection_set(bool concurrent) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahCollectionSet* collection_set = heap->collection_set();\n+\n+  assert(!heap->is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n+  assert(generation_mode() != OLD, \"Only YOUNG and GLOBAL GC perform evacuations\");\n+  {\n+    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states :\n+                            ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n+    ShenandoahFinalMarkUpdateRegionStateClosure cl(complete_marking_context());\n+    parallel_heap_region_iterate(&cl);\n+\n+    if (generation_mode() == YOUNG) {\n+      \/\/ We always need to update the watermark for old regions. If there\n+      \/\/ are mixed collections pending, we also need to synchronize the\n+      \/\/ pinned status for old regions. Since we are already visiting every\n+      \/\/ old region here, go ahead and sync the pin status too.\n+      ShenandoahFinalMarkUpdateRegionStateClosure old_cl(nullptr);\n+      heap->old_generation()->parallel_heap_region_iterate(&old_cl);\n+    }\n+  }\n+\n+  {\n+    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset :\n+                            ShenandoahPhaseTimings::degen_gc_choose_cset);\n+\n+    collection_set->clear();\n+    ShenandoahHeapLocker locker(heap->lock());\n+    if (heap->mode()->is_generational()) {\n+      size_t consumed_by_advance_promotion;\n+      bool* preselected_regions = (bool*) alloca(heap->num_regions() * sizeof(bool));\n+      for (unsigned int i = 0; i < heap->num_regions(); i++) {\n+        preselected_regions[i] = false;\n+      }\n+\n+      \/\/ TODO: young_available can include available (between top() and end()) within each young region that is not\n+      \/\/ part of the collection set.  Making this memory available to the young_evacuation_reserve allows a larger\n+      \/\/ young collection set to be chosen when available memory is under extreme pressure.  Implementing this \"improvement\"\n+      \/\/ is tricky, because the incremental construction of the collection set actually changes the amount of memory\n+      \/\/ available to hold evacuated young-gen objects.  As currently implemented, the memory that is available within\n+      \/\/ non-empty regions that are not selected as part of the collection set can be allocated by the mutator while\n+      \/\/ GC is evacuating and updating references.\n+\n+      \/\/ Budgeting parameters to compute_evacuation_budgets are passed by reference.\n+      compute_evacuation_budgets(heap, preselected_regions, collection_set, consumed_by_advance_promotion);\n+\n+      _heuristics->choose_collection_set(collection_set, heap->old_heuristics());\n+      if (!collection_set->is_empty()) {\n+        \/\/ only make use of evacuation budgets when we are evacuating\n+        adjust_evacuation_budgets(heap, collection_set, consumed_by_advance_promotion);\n+      }\n+    } else {\n+      _heuristics->choose_collection_set(collection_set, heap->old_heuristics());\n+    }\n+  }\n+\n+  {\n+    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset :\n+                            ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n+    ShenandoahHeapLocker locker(heap->lock());\n+    heap->free_set()->rebuild();\n+  }\n+}\n+\n+bool ShenandoahGeneration::is_bitmap_clear() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* context = heap->marking_context();\n+  size_t num_regions = heap->num_regions();\n+  for (size_t idx = 0; idx < num_regions; idx++) {\n+    ShenandoahHeapRegion* r = heap->get_region(idx);\n+    if (contains(r) && (r->affiliation() != FREE)) {\n+      if (heap->is_bitmap_slice_committed(r) && (context->top_at_mark_start(r) > r->bottom()) &&\n+          !context->is_bitmap_clear_range(r->bottom(), r->end())) {\n+        return false;\n+      }\n+    }\n+  }\n+  return true;\n+}\n+\n+bool ShenandoahGeneration::is_mark_complete() {\n+  return _is_marking_complete.is_set();\n+}\n+\n+void ShenandoahGeneration::set_mark_complete() {\n+  _is_marking_complete.set();\n+}\n+\n+void ShenandoahGeneration::set_mark_incomplete() {\n+  _is_marking_complete.unset();\n+}\n+\n+ShenandoahMarkingContext* ShenandoahGeneration::complete_marking_context() {\n+  assert(is_mark_complete(), \"Marking must be completed.\");\n+  return ShenandoahHeap::heap()->marking_context();\n+}\n+\n+void ShenandoahGeneration::cancel_marking() {\n+  log_info(gc)(\"Cancel marking: %s\", name());\n+  if (is_concurrent_mark_in_progress()) {\n+    set_mark_incomplete();\n+  }\n+  _task_queues->clear();\n+  ref_processor()->abandon_partial_discovery();\n+  set_concurrent_mark_in_progress(false);\n+}\n+\n+ShenandoahGeneration::ShenandoahGeneration(GenerationMode generation_mode,\n+                                           uint max_workers,\n+                                           size_t max_capacity,\n+                                           size_t soft_max_capacity) :\n+  _generation_mode(generation_mode),\n+  _task_queues(new ShenandoahObjToScanQueueSet(max_workers)),\n+  _ref_processor(new ShenandoahReferenceProcessor(MAX2(max_workers, 1U))),\n+  _collection_thread_time_s(0.0),\n+  _affiliated_region_count(0), _used(0), _bytes_allocated_since_gc_start(0),\n+  _max_capacity(max_capacity), _soft_max_capacity(soft_max_capacity),\n+  _adjusted_capacity(soft_max_capacity), _heuristics(nullptr) {\n+  _is_marking_complete.set();\n+  assert(max_workers > 0, \"At least one queue\");\n+  for (uint i = 0; i < max_workers; ++i) {\n+    ShenandoahObjToScanQueue* task_queue = new ShenandoahObjToScanQueue();\n+    _task_queues->register_queue(i, task_queue);\n+  }\n+}\n+\n+ShenandoahGeneration::~ShenandoahGeneration() {\n+  for (uint i = 0; i < _task_queues->size(); ++i) {\n+    ShenandoahObjToScanQueue* q = _task_queues->queue(i);\n+    delete q;\n+  }\n+  delete _task_queues;\n+}\n+\n+void ShenandoahGeneration::reserve_task_queues(uint workers) {\n+  _task_queues->reserve(workers);\n+}\n+\n+ShenandoahObjToScanQueueSet* ShenandoahGeneration::old_gen_task_queues() const {\n+  return nullptr;\n+}\n+\n+void ShenandoahGeneration::scan_remembered_set(bool is_concurrent) {\n+  assert(generation_mode() == YOUNG, \"Should only scan remembered set for young generation.\");\n+\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  uint nworkers = heap->workers()->active_workers();\n+  reserve_task_queues(nworkers);\n+\n+  ShenandoahReferenceProcessor* rp = ref_processor();\n+  ShenandoahRegionChunkIterator work_list(nworkers);\n+  ShenandoahScanRememberedTask task(task_queues(), old_gen_task_queues(), rp, &work_list, is_concurrent);\n+  heap->assert_gc_workers(nworkers);\n+  heap->workers()->run_task(&task);\n+  if (ShenandoahEnableCardStats) {\n+    assert(heap->card_scan() != nullptr, \"Not generational\");\n+    heap->card_scan()->log_card_stats(nworkers, CARD_STAT_SCAN_RS);\n+  }\n+}\n+\n+size_t ShenandoahGeneration::increment_affiliated_region_count() {\n+  _affiliated_region_count++;\n+  return _affiliated_region_count;\n+}\n+\n+size_t ShenandoahGeneration::decrement_affiliated_region_count() {\n+  _affiliated_region_count--;\n+  return _affiliated_region_count;\n+}\n+\n+void ShenandoahGeneration::clear_used() {\n+  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"must be at a safepoint\");\n+  \/\/ Do this atomically to assure visibility to other threads, even though these other threads may be idle \"right now\"..\n+  Atomic::store(&_used, (size_t)0);\n+}\n+\n+void ShenandoahGeneration::increase_used(size_t bytes) {\n+  Atomic::add(&_used, bytes);\n+}\n+\n+void ShenandoahGeneration::decrease_used(size_t bytes) {\n+  assert(_used >= bytes, \"cannot reduce bytes used by generation below zero\");\n+  Atomic::sub(&_used, bytes);\n+}\n+\n+size_t ShenandoahGeneration::used_regions() const {\n+  return _affiliated_region_count;\n+}\n+\n+size_t ShenandoahGeneration::free_unaffiliated_regions() const {\n+  size_t result = soft_max_capacity() \/ ShenandoahHeapRegion::region_size_bytes();\n+  if (_affiliated_region_count > result) {\n+    result = 0;                 \/\/ If old-gen is loaning regions to young-gen, affiliated regions may exceed capacity temporarily.\n+  } else {\n+    result -= _affiliated_region_count;\n+  }\n+  return result;\n+}\n+\n+size_t ShenandoahGeneration::used_regions_size() const {\n+  return _affiliated_region_count * ShenandoahHeapRegion::region_size_bytes();\n+}\n+\n+size_t ShenandoahGeneration::available() const {\n+  size_t in_use = used();\n+  size_t soft_capacity = soft_max_capacity();\n+  return in_use > soft_capacity ? 0 : soft_capacity - in_use;\n+}\n+\n+size_t ShenandoahGeneration::adjust_available(intptr_t adjustment) {\n+  \/\/ TODO: ysr: remove this check & warning\n+  if (adjustment % ShenandoahHeapRegion::region_size_bytes() != 0) {\n+    log_warning(gc)(\"Adjustment (\" INTPTR_FORMAT \") should be a multiple of region size (\" SIZE_FORMAT \")\",\n+                    adjustment, ShenandoahHeapRegion::region_size_bytes());\n+  }\n+  assert(adjustment % ShenandoahHeapRegion::region_size_bytes() == 0,\n+         \"Adjustment to generation size must be multiple of region size\");\n+  _adjusted_capacity = soft_max_capacity() + adjustment;\n+  return _adjusted_capacity;\n+}\n+\n+size_t ShenandoahGeneration::unadjust_available() {\n+  _adjusted_capacity = soft_max_capacity();\n+  return _adjusted_capacity;\n+}\n+\n+size_t ShenandoahGeneration::adjusted_available() const {\n+  size_t in_use = used();\n+  size_t capacity = _adjusted_capacity;\n+  return in_use > capacity ? 0 : capacity - in_use;\n+}\n+\n+size_t ShenandoahGeneration::adjusted_capacity() const {\n+  return _adjusted_capacity;\n+}\n+\n+size_t ShenandoahGeneration::adjusted_unaffiliated_regions() const {\n+  assert(adjusted_capacity() >= used_regions_size(), \"adjusted_unaffiliated_regions() cannot return negative\");\n+  assert((adjusted_capacity() - used_regions_size()) % ShenandoahHeapRegion::region_size_bytes() == 0,\n+         \"adjusted capacity (\" SIZE_FORMAT \") and used regions size (\" SIZE_FORMAT \") should be multiples of region_size_bytes\",\n+         adjusted_capacity(), used_regions_size());\n+  return (adjusted_capacity() - used_regions_size()) \/ ShenandoahHeapRegion::region_size_bytes();\n+}\n+\n+void ShenandoahGeneration::increase_capacity(size_t increment) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  assert(_max_capacity + increment <= ShenandoahHeap::heap()->max_size_for(this), \"Cannot increase generation capacity beyond maximum.\");\n+  assert(increment % ShenandoahHeapRegion::region_size_bytes() == 0, \"Region-sized changes only\");\n+  \/\/ TODO: ysr: remove this check and warning\n+  if (increment % ShenandoahHeapRegion::region_size_bytes() != 0) {\n+    log_warning(gc)(\"Increment (\" INTPTR_FORMAT \") should be a multiple of region size (\" SIZE_FORMAT \")\",\n+                    increment, ShenandoahHeapRegion::region_size_bytes());\n+  }\n+  _max_capacity += increment;\n+  _soft_max_capacity += increment;\n+  _adjusted_capacity += increment;\n+}\n+\n+void ShenandoahGeneration::decrease_capacity(size_t decrement) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  assert(_max_capacity - decrement >= ShenandoahHeap::heap()->min_size_for(this), \"Cannot decrease generation capacity beyond minimum.\");\n+  assert(decrement % ShenandoahHeapRegion::region_size_bytes() == 0, \"Region-sized changes only\");\n+  \/\/ TODO: ysr: remove this check and warning\n+  if (decrement % ShenandoahHeapRegion::region_size_bytes() != 0) {\n+    log_warning(gc)(\"Decrement (\" INTPTR_FORMAT \") should be a multiple of region size (\" SIZE_FORMAT \")\",\n+                    decrement, ShenandoahHeapRegion::region_size_bytes());\n+  }\n+  _max_capacity -= decrement;\n+  _soft_max_capacity -= decrement;\n+  _adjusted_capacity -= decrement;\n+}\n+\n+void ShenandoahGeneration::record_success_concurrent(bool abbreviated) {\n+  heuristics()->record_success_concurrent(abbreviated);\n+  ShenandoahHeap::heap()->shenandoah_policy()->record_success_concurrent();\n+}\n+\n+void ShenandoahGeneration::record_success_degenerated() {\n+  heuristics()->record_success_degenerated();\n+  ShenandoahHeap::heap()->shenandoah_policy()->record_success_degenerated();\n+}\n+\n+void ShenandoahGeneration::add_collection_time(double time_seconds) {\n+  shenandoah_assert_control_or_vm_thread();\n+  _collection_thread_time_s += time_seconds;\n+}\n+\n+double ShenandoahGeneration::reset_collection_time() {\n+  double t = _collection_thread_time_s;\n+  _collection_thread_time_s = 0.0;\n+  return t;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":1068,"deletions":0,"binary":false,"changes":1068,"status":"added"},{"patch":"@@ -0,0 +1,207 @@\n+\/*\n+ * Copyright (c) 2020, 2021 Amazon.com, Inc. and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHGENERATION_HPP\n+#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHGENERATION_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n+#include \"gc\/shenandoah\/shenandoahLock.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMarkingContext.hpp\"\n+\n+class ShenandoahHeapRegion;\n+class ShenandoahHeapRegionClosure;\n+class ShenandoahReferenceProcessor;\n+class ShenandoahHeap;\n+\n+class ShenandoahGeneration : public CHeapObj<mtGC> {\n+private:\n+  GenerationMode const _generation_mode;\n+\n+  \/\/ Marking task queues and completeness\n+  ShenandoahObjToScanQueueSet* _task_queues;\n+  ShenandoahSharedFlag _is_marking_complete;\n+\n+  ShenandoahReferenceProcessor* const _ref_processor;\n+\n+  double _collection_thread_time_s;\n+\n+protected:\n+  \/\/ Usage\n+  size_t _affiliated_region_count;\n+  volatile size_t _used;\n+  volatile size_t _bytes_allocated_since_gc_start;\n+  size_t _max_capacity;\n+  size_t _soft_max_capacity;\n+\n+  size_t _adjusted_capacity;\n+\n+  ShenandoahHeuristics* _heuristics;\n+\n+private:\n+  \/\/ Compute evacuation budgets prior to choosing collection set.\n+  void compute_evacuation_budgets(ShenandoahHeap* heap, bool* preselected_regions, ShenandoahCollectionSet* collection_set,\n+                                  size_t &consumed_by_advance_promotion);\n+\n+  \/\/ Adjust evacuation budgets after choosing collection set.\n+  void adjust_evacuation_budgets(ShenandoahHeap* heap, ShenandoahCollectionSet* collection_set,\n+                                 size_t consumed_by_advance_promotion);\n+\n+ public:\n+  ShenandoahGeneration(GenerationMode generation_mode, uint max_workers, size_t max_capacity, size_t soft_max_capacity);\n+  ~ShenandoahGeneration();\n+\n+  bool is_young() const  { return _generation_mode == YOUNG; }\n+  bool is_old() const    { return _generation_mode == OLD; }\n+  bool is_global() const { return _generation_mode == GLOBAL; }\n+\n+  inline GenerationMode generation_mode() const { return _generation_mode; }\n+\n+  inline ShenandoahHeuristics* heuristics() const { return _heuristics; }\n+\n+  ShenandoahReferenceProcessor* ref_processor() { return _ref_processor; }\n+\n+  virtual const char* name() const = 0;\n+\n+  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahMode* gc_mode);\n+\n+  virtual size_t soft_max_capacity() const { return _soft_max_capacity; }\n+  virtual size_t max_capacity() const      { return _max_capacity; }\n+  virtual size_t used_regions() const;\n+  virtual size_t used_regions_size() const;\n+  virtual size_t free_unaffiliated_regions() const;\n+  virtual size_t used() const { return _used; }\n+  virtual size_t available() const;\n+\n+  \/\/ During evacuation and update-refs, some memory may be shifted between generations.  In particular, memory\n+  \/\/ may be loaned by old-gen to young-gen based on the promise the loan will be promptly repaid from the memory reclaimed\n+  \/\/ when the current collection set is recycled.  The capacity adjustment also takes into consideration memory that is\n+  \/\/ set aside within each generation to hold the results of evacuation, but not promotion, into that region.  Promotions\n+  \/\/ into old-gen are bounded by adjusted_available() whereas evacuations into old-gen are pre-committed.\n+  virtual size_t adjusted_available() const;\n+  virtual size_t adjusted_capacity() const;\n+\n+  \/\/ This is the number of FREE regions that are eligible to be affiliated with this generation according to the current\n+  \/\/ adjusted capacity.\n+  virtual size_t adjusted_unaffiliated_regions() const;\n+\n+  \/\/ Both of following return new value of available\n+  virtual size_t adjust_available(intptr_t adjustment);\n+  virtual size_t unadjust_available();\n+\n+  size_t bytes_allocated_since_gc_start();\n+  void reset_bytes_allocated_since_gc_start();\n+  void increase_allocated(size_t bytes);\n+\n+  \/\/ These methods change the capacity of the region by adding or subtracting the given number of bytes from the current\n+  \/\/ capacity.\n+  void increase_capacity(size_t increment);\n+  void decrease_capacity(size_t decrement);\n+\n+  void set_soft_max_capacity(size_t soft_max_capacity) {\n+    _soft_max_capacity = soft_max_capacity;\n+  }\n+\n+  void log_status(const char* msg) const;\n+\n+  \/\/ Used directly by FullGC\n+  void reset_mark_bitmap();\n+\n+  \/\/ Used by concurrent and degenerated GC to reset remembered set.\n+  void swap_remembered_set();\n+\n+  \/\/ Update the read cards with the state of the write table (write table is not cleared).\n+  void merge_write_table();\n+\n+  \/\/ Called before init mark, expected to prepare regions for marking.\n+  virtual void prepare_gc();\n+\n+  \/\/ Called during final mark, chooses collection set, rebuilds free set.\n+  virtual void prepare_regions_and_collection_set(bool concurrent);\n+\n+  \/\/ Cancel marking (used by Full collect and when cancelling cycle).\n+  virtual void cancel_marking();\n+\n+  \/\/ Return true if this region is affiliated with this generation.\n+  virtual bool contains(ShenandoahHeapRegion* region) const = 0;\n+\n+  \/\/ Return true if this object is affiliated with this generation.\n+  virtual bool contains(oop obj) const = 0;\n+\n+  \/\/ Apply closure to all regions affiliated with this generation.\n+  virtual void parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl) = 0;\n+\n+  \/\/ Apply closure to all regions affiliated with this generation (single threaded).\n+  virtual void heap_region_iterate(ShenandoahHeapRegionClosure* cl) = 0;\n+\n+  \/\/ This is public to support cancellation of marking when a Full cycle is started.\n+  virtual void set_concurrent_mark_in_progress(bool in_progress) = 0;\n+\n+  \/\/ Check the bitmap only for regions belong to this generation.\n+  bool is_bitmap_clear();\n+\n+  \/\/ We need to track the status of marking for different generations.\n+  bool is_mark_complete();\n+  void set_mark_complete();\n+  void set_mark_incomplete();\n+\n+  ShenandoahMarkingContext* complete_marking_context();\n+\n+  \/\/ Task queues\n+  ShenandoahObjToScanQueueSet* task_queues() const { return _task_queues; }\n+  virtual void reserve_task_queues(uint workers);\n+  virtual ShenandoahObjToScanQueueSet* old_gen_task_queues() const;\n+\n+  \/\/ Scan remembered set at start of concurrent young-gen marking.\n+  void scan_remembered_set(bool is_concurrent);\n+\n+  \/\/ Return the updated value of affiliated_region_count\n+  size_t increment_affiliated_region_count();\n+\n+  \/\/ Return the updated value of affiliated_region_count\n+  size_t decrement_affiliated_region_count();\n+\n+  void clear_used();\n+  void increase_used(size_t bytes);\n+  void decrease_used(size_t bytes);\n+\n+  virtual bool is_concurrent_mark_in_progress() = 0;\n+  void confirm_heuristics_mode();\n+\n+  virtual void record_success_concurrent(bool abbreviated);\n+  virtual void record_success_degenerated();\n+\n+  \/\/ Record the total on-cpu time a thread has spent collecting this\n+  \/\/ generation. This is only called by the control thread (at the start\n+  \/\/ of a collection) and by the VM thread at the end of the collection,\n+  \/\/ so there are no locking concerns.\n+  virtual void add_collection_time(double time_seconds);\n+\n+  \/\/ This returns the accumulated collection time and resets it to zero.\n+  \/\/ This is used to decide which generation should be resized.\n+  double reset_collection_time();\n+};\n+\n+#endif \/\/ SHARE_VM_GC_SHENANDOAH_SHENANDOAHGENERATION_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":207,"deletions":0,"binary":false,"changes":207,"status":"added"},{"patch":"@@ -0,0 +1,92 @@\n+\/*\n+ * Copyright (c) 2020, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+#include \"gc\/shenandoah\/shenandoahVerifier.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+\n+const char* ShenandoahGlobalGeneration::name() const {\n+  return \"GLOBAL\";\n+}\n+\n+size_t ShenandoahGlobalGeneration::max_capacity() const {\n+  return ShenandoahHeap::heap()->max_capacity();\n+}\n+\n+size_t ShenandoahGlobalGeneration::used_regions() const {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  return heap->old_generation()->used_regions() + heap->young_generation()->used_regions();\n+}\n+\n+size_t ShenandoahGlobalGeneration::used_regions_size() const {\n+  return ShenandoahHeap::heap()->capacity();\n+}\n+\n+size_t ShenandoahGlobalGeneration::soft_max_capacity() const {\n+  return ShenandoahHeap::heap()->soft_max_capacity();\n+}\n+\n+size_t ShenandoahGlobalGeneration::used() const {\n+  return ShenandoahHeap::heap()->used();\n+}\n+\n+size_t ShenandoahGlobalGeneration::available() const {\n+  return ShenandoahHeap::heap()->free_set()->available();\n+}\n+\n+void ShenandoahGlobalGeneration::set_concurrent_mark_in_progress(bool in_progress) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  if (in_progress && heap->mode()->is_generational()) {\n+    \/\/ Global collection has preempted an old generation mark. This is fine\n+    \/\/ because the global generation includes the old generation, but we\n+    \/\/ want the global collect to start from a clean slate and we don't want\n+    \/\/ any stale state in the old generation.\n+    heap->cancel_old_gc();\n+  }\n+\n+  heap->set_concurrent_young_mark_in_progress(in_progress);\n+}\n+\n+bool ShenandoahGlobalGeneration::contains(ShenandoahHeapRegion* region) const {\n+  return true;\n+}\n+\n+void ShenandoahGlobalGeneration::parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl) {\n+  ShenandoahHeap::heap()->parallel_heap_region_iterate(cl);\n+}\n+\n+void ShenandoahGlobalGeneration::heap_region_iterate(ShenandoahHeapRegionClosure* cl) {\n+  ShenandoahHeap::heap()->heap_region_iterate(cl);\n+}\n+\n+bool ShenandoahGlobalGeneration::is_concurrent_mark_in_progress() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  return heap->is_concurrent_mark_in_progress();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGlobalGeneration.cpp","additions":92,"deletions":0,"binary":false,"changes":92,"status":"added"},{"patch":"@@ -0,0 +1,62 @@\n+\/*\n+ * Copyright (c) 2020, Amazon.com, Inc. and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHGLOBALGENERATION_HPP\n+#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHGLOBALGENERATION_HPP\n+\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+\n+\/\/ A \"generation\" that represents the whole heap.\n+class ShenandoahGlobalGeneration : public ShenandoahGeneration {\n+public:\n+  ShenandoahGlobalGeneration(uint max_queues, size_t max_capacity, size_t soft_max_capacity)\n+  : ShenandoahGeneration(GLOBAL, max_queues, max_capacity, soft_max_capacity) { }\n+\n+public:\n+  virtual const char* name() const override;\n+\n+  virtual size_t max_capacity() const override;\n+  virtual size_t soft_max_capacity() const override;\n+  virtual size_t used_regions() const override;\n+  virtual size_t used_regions_size() const override;\n+  virtual size_t used() const override;\n+  virtual size_t available() const override;\n+\n+  virtual void set_concurrent_mark_in_progress(bool in_progress)  override;\n+\n+  bool contains(ShenandoahHeapRegion* region) const  override;\n+\n+  bool contains(oop obj) const override { return true; }\n+\n+  void parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl)  override;\n+\n+  void heap_region_iterate(ShenandoahHeapRegionClosure* cl)  override;\n+\n+  bool is_concurrent_mark_in_progress()  override;\n+};\n+\n+#endif \/\/ SHARE_VM_GC_SHENANDOAH_SHENANDOAHGLOBALGENERATION_HPP\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGlobalGeneration.hpp","additions":62,"deletions":0,"binary":false,"changes":62,"status":"added"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -45,0 +46,1 @@\n+#include \"gc\/shenandoah\/shenandoahRegulatorThread.hpp\"\n@@ -46,0 +48,1 @@\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n@@ -55,0 +58,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -61,0 +65,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -69,0 +74,2 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -72,0 +79,1 @@\n+\n@@ -76,0 +84,2 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+\n@@ -163,3 +173,0 @@\n-  \/\/ Now we know the number of regions, initialize the heuristics.\n-  initialize_heuristics();\n-\n@@ -181,0 +188,4 @@\n+  \/\/ Now we know the number of regions and heap sizes, initialize the heuristics.\n+  initialize_generations();\n+  initialize_heuristics();\n+\n@@ -215,0 +226,25 @@\n+  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this, _heap_region));\n+\n+  \/\/\n+  \/\/ After reserving the Java heap, create the card table, barriers, and workers, in dependency order\n+  \/\/\n+  if (mode()->is_generational()) {\n+    ShenandoahDirectCardMarkRememberedSet *rs;\n+    ShenandoahCardTable* card_table = ShenandoahBarrierSet::barrier_set()->card_table();\n+    size_t card_count = card_table->cards_required(heap_rs.size() \/ HeapWordSize);\n+    rs = new ShenandoahDirectCardMarkRememberedSet(ShenandoahBarrierSet::barrier_set()->card_table(), card_count);\n+    _card_scan = new ShenandoahScanRemembered<ShenandoahDirectCardMarkRememberedSet>(rs);\n+  }\n+\n+  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n+  if (_workers == nullptr) {\n+    vm_exit_during_initialization(\"Failed necessary allocation.\");\n+  } else {\n+    _workers->initialize_workers();\n+  }\n+\n+  if (ParallelGCThreads > 1) {\n+    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\", ParallelGCThreads);\n+    _safepoint_workers->initialize_workers();\n+  }\n+\n@@ -258,1 +294,1 @@\n-  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions, _max_workers);\n+  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions);\n@@ -321,0 +357,1 @@\n+  _affiliations = NEW_C_HEAP_ARRAY(uint8_t, _num_regions, mtGC);\n@@ -326,0 +363,1 @@\n+\n@@ -337,0 +375,2 @@\n+\n+      _affiliations[i] = ShenandoahRegionAffiliation::FREE;\n@@ -404,0 +444,1 @@\n+  _regulator_thread = new ShenandoahRegulatorThread(_control_thread);\n@@ -410,1 +451,39 @@\n-void ShenandoahHeap::initialize_mode() {\n+size_t ShenandoahHeap::max_size_for(ShenandoahGeneration* generation) const {\n+  switch (generation->generation_mode()) {\n+    case YOUNG:  return _generation_sizer.max_young_size();\n+    case OLD:    return max_capacity() - _generation_sizer.min_young_size();\n+    case GLOBAL: return max_capacity();\n+    default:\n+      ShouldNotReachHere();\n+      return 0;\n+  }\n+}\n+\n+size_t ShenandoahHeap::min_size_for(ShenandoahGeneration* generation) const {\n+  switch (generation->generation_mode()) {\n+    case YOUNG:  return _generation_sizer.min_young_size();\n+    case OLD:    return max_capacity() - _generation_sizer.max_young_size();\n+    case GLOBAL: return min_capacity();\n+    default:\n+      ShouldNotReachHere();\n+      return 0;\n+  }\n+}\n+\n+void ShenandoahHeap::initialize_generations() {\n+  \/\/ Max capacity is the maximum _allowed_ capacity. That is, the maximum allowed capacity\n+  \/\/ for old would be total heap - minimum capacity of young. This means the sum of the maximum\n+  \/\/ allowed for old and young could exceed the total heap size. It remains the case that the\n+  \/\/ _actual_ capacity of young + old = total.\n+  _generation_sizer.heap_size_changed(soft_max_capacity());\n+  size_t initial_capacity_young = _generation_sizer.max_young_size();\n+  size_t max_capacity_young = _generation_sizer.max_young_size();\n+  size_t initial_capacity_old = max_capacity() - max_capacity_young;\n+  size_t max_capacity_old = max_capacity() - initial_capacity_young;\n+\n+  _young_generation = new ShenandoahYoungGeneration(_max_workers, max_capacity_young, initial_capacity_young);\n+  _old_generation = new ShenandoahOldGeneration(_max_workers, max_capacity_old, initial_capacity_old);\n+  _global_generation = new ShenandoahGlobalGeneration(_max_workers, soft_max_capacity(), soft_max_capacity());\n+}\n+\n+void ShenandoahHeap::initialize_heuristics() {\n@@ -418,0 +497,2 @@\n+    } else if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+      _gc_mode = new ShenandoahGenerationalMode();\n@@ -435,1 +516,0 @@\n-}\n@@ -437,3 +517,4 @@\n-void ShenandoahHeap::initialize_heuristics() {\n-  assert(_gc_mode != nullptr, \"Must be initialized\");\n-  _heuristics = _gc_mode->initialize_heuristics();\n+  _global_generation->initialize_heuristics(_gc_mode);\n+  if (mode()->is_generational()) {\n+    _young_generation->initialize_heuristics(_gc_mode);\n+    _old_generation->initialize_heuristics(_gc_mode);\n@@ -441,9 +522,1 @@\n-  if (_heuristics->is_diagnostic() && !UnlockDiagnosticVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.\",\n-                    _heuristics->name()));\n-  }\n-  if (_heuristics->is_experimental() && !UnlockExperimentalVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.\",\n-                    _heuristics->name()));\n+    ShenandoahEvacWaste = ShenandoahGenerationalEvacWaste;\n@@ -460,0 +533,2 @@\n+  _gc_generation(nullptr),\n+  _prepare_for_old_mark(false),\n@@ -463,2 +538,1 @@\n-  _bytes_allocated_since_gc_start(0),\n-  _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),\n+  _max_workers(MAX3(ConcGCThreads, ParallelGCThreads, 1U)),\n@@ -470,0 +544,1 @@\n+  _affiliations(nullptr),\n@@ -471,0 +546,11 @@\n+  _alloc_supplement_reserve(0),\n+  _promoted_reserve(0),\n+  _old_evac_reserve(0),\n+  _old_evac_expended(0),\n+  _young_evac_reserve(0),\n+  _captured_old_usage(0),\n+  _previous_promotion(0),\n+  _cancel_requested_time(0),\n+  _young_generation(nullptr),\n+  _global_generation(nullptr),\n+  _old_generation(nullptr),\n@@ -472,0 +558,1 @@\n+  _regulator_thread(nullptr),\n@@ -473,2 +560,0 @@\n-  _gc_mode(nullptr),\n-  _heuristics(nullptr),\n@@ -479,0 +564,3 @@\n+  _evac_tracker(new ShenandoahEvacuationTracker()),\n+  _mmu_tracker(),\n+  _generation_sizer(&_mmu_tracker),\n@@ -481,0 +569,2 @@\n+  _young_gen_memory_pool(nullptr),\n+  _old_gen_memory_pool(nullptr),\n@@ -486,1 +576,0 @@\n-  _ref_processor(new ShenandoahReferenceProcessor(MAX2(_max_workers, 1U))),\n@@ -494,1 +583,2 @@\n-  _collection_set(nullptr)\n+  _collection_set(nullptr),\n+  _card_scan(nullptr)\n@@ -496,17 +586,0 @@\n-  \/\/ Initialize GC mode early, so we can adjust barrier support\n-  initialize_mode();\n-  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));\n-\n-  _max_workers = MAX2(_max_workers, 1U);\n-  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n-  if (_workers == nullptr) {\n-    vm_exit_during_initialization(\"Failed necessary allocation.\");\n-  } else {\n-    _workers->initialize_workers();\n-  }\n-\n-  if (ParallelGCThreads > 1) {\n-    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\",\n-                                                ParallelGCThreads);\n-    _safepoint_workers->initialize_workers();\n-  }\n@@ -519,29 +592,0 @@\n-class ShenandoahResetBitmapTask : public WorkerTask {\n-private:\n-  ShenandoahRegionIterator _regions;\n-\n-public:\n-  ShenandoahResetBitmapTask() :\n-    WorkerTask(\"Shenandoah Reset Bitmap\") {}\n-\n-  void work(uint worker_id) {\n-    ShenandoahHeapRegion* region = _regions.next();\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahMarkingContext* const ctx = heap->marking_context();\n-    while (region != nullptr) {\n-      if (heap->is_bitmap_slice_committed(region)) {\n-        ctx->clear_bitmap(region);\n-      }\n-      region = _regions.next();\n-    }\n-  }\n-};\n-\n-void ShenandoahHeap::reset_mark_bitmap() {\n-  assert_gc_workers(_workers->active_workers());\n-  mark_incomplete_marking_context();\n-\n-  ShenandoahResetBitmapTask task;\n-  _workers->run_task(&task);\n-}\n-\n@@ -562,1 +606,2 @@\n-  if (is_concurrent_mark_in_progress())        st->print(\"marking, \");\n+  if (is_concurrent_old_mark_in_progress())    st->print(\"old marking, \");\n+  if (is_concurrent_young_mark_in_progress())  st->print(\"young marking, \");\n@@ -612,0 +657,2 @@\n+  _mmu_tracker.initialize();\n+\n@@ -625,2 +672,0 @@\n-  _heuristics->initialize();\n-\n@@ -630,0 +675,19 @@\n+\n+ShenandoahOldHeuristics* ShenandoahHeap::old_heuristics() {\n+  return (ShenandoahOldHeuristics*) _old_generation->heuristics();\n+}\n+\n+bool ShenandoahHeap::doing_mixed_evacuations() {\n+  return old_heuristics()->unprocessed_old_collection_candidates() > 0;\n+}\n+\n+bool ShenandoahHeap::is_old_bitmap_stable() const {\n+  ShenandoahOldGeneration::State state = _old_generation->state();\n+  return state != ShenandoahOldGeneration::MARKING\n+      && state != ShenandoahOldGeneration::BOOTSTRAPPING;\n+}\n+\n+bool ShenandoahHeap::is_gc_generation_young() const {\n+  return _gc_generation != nullptr && _gc_generation->generation_mode() == YOUNG;\n+}\n+\n@@ -661,4 +725,0 @@\n-void ShenandoahHeap::increase_allocated(size_t bytes) {\n-  Atomic::add(&_bytes_allocated_since_gc_start, bytes, memory_order_relaxed);\n-}\n-\n@@ -670,1 +730,1 @@\n-  increase_allocated(bytes);\n+\n@@ -700,0 +760,8 @@\n+\n+  if (mode()->is_generational()) {\n+    _generation_sizer.heap_size_changed(_soft_max_size);\n+    size_t soft_max_capacity_young = _generation_sizer.max_young_size();\n+    size_t soft_max_capacity_old = _soft_max_size - soft_max_capacity_young;\n+    _young_generation->set_soft_max_capacity(soft_max_capacity_young);\n+    _old_generation->set_soft_max_capacity(soft_max_capacity_old);\n+  }\n@@ -710,6 +778,0 @@\n-bool ShenandoahHeap::is_in(const void* p) const {\n-  HeapWord* heap_base = (HeapWord*) base();\n-  HeapWord* last_region_end = heap_base + ShenandoahHeapRegion::region_size_words() * num_regions();\n-  return p >= heap_base && p < last_region_end;\n-}\n-\n@@ -743,0 +805,64 @@\n+    regulator_thread()->notify_heap_changed();\n+  }\n+}\n+\n+void ShenandoahHeap::handle_old_evacuation(HeapWord* obj, size_t words, bool promotion) {\n+  \/\/ Only register the copy of the object that won the evacuation race.\n+  card_scan()->register_object_wo_lock(obj);\n+\n+  \/\/ Mark the entire range of the evacuated object as dirty.  At next remembered set scan,\n+  \/\/ we will clear dirty bits that do not hold interesting pointers.  It's more efficient to\n+  \/\/ do this in batch, in a background GC thread than to try to carefully dirty only cards\n+  \/\/ that hold interesting pointers right now.\n+  card_scan()->mark_range_as_dirty(obj, words);\n+\n+  if (promotion) {\n+    \/\/ This evacuation was a promotion, track this as allocation against old gen\n+    old_generation()->increase_allocated(words * HeapWordSize);\n+  }\n+}\n+\n+void ShenandoahHeap::handle_old_evacuation_failure() {\n+  if (_old_gen_oom_evac.try_set()) {\n+    log_info(gc)(\"Old gen evac failure.\");\n+  }\n+}\n+\n+void ShenandoahHeap::handle_promotion_failure() {\n+  old_heuristics()->handle_promotion_failure();\n+}\n+\n+void ShenandoahHeap::report_promotion_failure(Thread* thread, size_t size) {\n+  \/\/ We squelch excessive reports to reduce noise in logs.  Squelch enforcement is not \"perfect\" because\n+  \/\/ this same code can be in-lined in multiple contexts, and each context will have its own copy of the static\n+  \/\/ last_report_epoch and this_epoch_report_count variables.\n+  const size_t MaxReportsPerEpoch = 4;\n+  static size_t last_report_epoch = 0;\n+  static size_t epoch_report_count = 0;\n+\n+  size_t promotion_reserve;\n+  size_t promotion_expended;\n+\n+  size_t gc_id = control_thread()->get_gc_id();\n+\n+  if ((gc_id != last_report_epoch) || (epoch_report_count++ < MaxReportsPerEpoch)) {\n+    {\n+      \/\/ Promotion failures should be very rare.  Invest in providing useful diagnostic info.\n+      ShenandoahHeapLocker locker(lock());\n+      promotion_reserve = get_promoted_reserve();\n+      promotion_expended = get_promoted_expended();\n+    }\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    size_t words_remaining = (plab == nullptr)? 0: plab->words_remaining();\n+    const char* promote_enabled = ShenandoahThreadLocalData::allow_plab_promotions(thread)? \"enabled\": \"disabled\";\n+\n+    log_info(gc, ergo)(\"Promotion failed, size \" SIZE_FORMAT \", has plab? %s, PLAB remaining: \" SIZE_FORMAT\n+                       \", plab promotions %s, promotion reserve: \" SIZE_FORMAT \", promotion expended: \" SIZE_FORMAT,\n+                       size, plab == nullptr? \"no\": \"yes\",\n+                       words_remaining, promote_enabled, promotion_reserve, promotion_expended);\n+    if ((gc_id == last_report_epoch) && (epoch_report_count >= MaxReportsPerEpoch)) {\n+      log_info(gc, ergo)(\"Squelching additional promotion failure reports for current epoch\");\n+    } else if (gc_id != last_report_epoch) {\n+      last_report_epoch = gc_id;;\n+      epoch_report_count = 1;\n+    }\n@@ -752,0 +878,8 @@\n+\n+  \/\/ Limit growth of GCLABs to ShenandoahMaxEvacLABRatio * the minimum size.  This enables more equitable distribution of\n+  \/\/ available evacuation buidget between the many threads that are coordinating in the evacuation effort.\n+  if (ShenandoahMaxEvacLABRatio > 0) {\n+    log_debug(gc, free)(\"Allocate new gclab: \" SIZE_FORMAT \", \" SIZE_FORMAT, new_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+    new_size = MIN2(new_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+  }\n+\n@@ -763,0 +897,1 @@\n+    log_debug(gc, free)(\"New gclab size (\" SIZE_FORMAT \") is too small for \" SIZE_FORMAT, new_size, size);\n@@ -795,0 +930,178 @@\n+\/\/ Establish a new PLAB and allocate size HeapWords within it.\n+HeapWord* ShenandoahHeap::allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion) {\n+  \/\/ New object should fit the PLAB size\n+  size_t min_size = MAX2(size, PLAB::min_size());\n+\n+  \/\/ Figure out size of new PLAB, looking back at heuristics. Expand aggressively.\n+  size_t cur_size = ShenandoahThreadLocalData::plab_size(thread);\n+  if (cur_size == 0) {\n+    cur_size = PLAB::min_size();\n+  }\n+  size_t future_size = cur_size * 2;\n+  \/\/ Limit growth of PLABs to ShenandoahMaxEvacLABRatio * the minimum size.  This enables more equitable distribution of\n+  \/\/ available evacuation buidget between the many threads that are coordinating in the evacuation effort.\n+  if (ShenandoahMaxEvacLABRatio > 0) {\n+    future_size = MIN2(future_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+  }\n+  future_size = MIN2(future_size, PLAB::max_size());\n+  future_size = MAX2(future_size, PLAB::min_size());\n+\n+  size_t unalignment = future_size % CardTable::card_size_in_words();\n+  if (unalignment != 0) {\n+    future_size = future_size - unalignment + CardTable::card_size_in_words();\n+  }\n+\n+  \/\/ Record new heuristic value even if we take any shortcut. This captures\n+  \/\/ the case when moderately-sized objects always take a shortcut. At some point,\n+  \/\/ heuristics should catch up with them.  Note that the requested cur_size may\n+  \/\/ not be honored, but we remember that this is the preferred size.\n+  ShenandoahThreadLocalData::set_plab_size(thread, future_size);\n+  if (cur_size < size) {\n+    \/\/ The PLAB to be allocated is still not large enough to hold the object. Fall back to shared allocation.\n+    \/\/ This avoids retiring perfectly good PLABs in order to represent a single large object allocation.\n+    return nullptr;\n+  }\n+\n+  \/\/ Retire current PLAB, and allocate a new one.\n+  PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+  if (plab->words_remaining() < PLAB::min_size()) {\n+    \/\/ Retire current PLAB, and allocate a new one.\n+    \/\/ CAUTION: retire_plab may register the remnant filler object with the remembered set scanner without a lock.  This\n+    \/\/ is safe iff it is assured that each PLAB is a whole-number multiple of card-mark memory size and each PLAB is\n+    \/\/ aligned with the start of a card's memory range.\n+\n+    retire_plab(plab, thread);\n+\n+    size_t actual_size = 0;\n+    \/\/ allocate_new_plab resets plab_evacuated and plab_promoted and disables promotions if old-gen available is\n+    \/\/ less than the remaining evacuation need.  It also adjusts plab_preallocated and expend_promoted if appropriate.\n+    HeapWord* plab_buf = allocate_new_plab(min_size, cur_size, &actual_size);\n+    if (plab_buf == nullptr) {\n+      return nullptr;\n+    } else {\n+      ShenandoahThreadLocalData::enable_plab_retries(thread);\n+    }\n+    assert (size <= actual_size, \"allocation should fit\");\n+    if (ZeroTLAB) {\n+      \/\/ ..and clear it.\n+      Copy::zero_to_words(plab_buf, actual_size);\n+    } else {\n+      \/\/ ...and zap just allocated object.\n+#ifdef ASSERT\n+      \/\/ Skip mangling the space corresponding to the object header to\n+      \/\/ ensure that the returned space is not considered parsable by\n+      \/\/ any concurrent GC thread.\n+      size_t hdr_size = oopDesc::header_size();\n+      Copy::fill_to_words(plab_buf + hdr_size, actual_size - hdr_size, badHeapWordVal);\n+#endif \/\/ ASSERT\n+    }\n+    plab->set_buf(plab_buf, actual_size);\n+\n+    if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+      return nullptr;\n+    }\n+    return plab->allocate(size);\n+  } else {\n+    \/\/ If there's still at least min_size() words available within the current plab, don't retire it.  Let's gnaw\n+    \/\/ away on this plab as long as we can.  Meanwhile, return nullptr to force this particular allocation request\n+    \/\/ to be satisfied with a shared allocation.  By packing more promotions into the previously allocated PLAB, we\n+    \/\/ reduce the likelihood of evacuation failures, and we we reduce the need for downsizing our PLABs.\n+    return nullptr;\n+  }\n+}\n+\n+\/\/ TODO: It is probably most efficient to register all objects (both promotions and evacuations) that were allocated within\n+\/\/ this plab at the time we retire the plab.  A tight registration loop will run within both code and data caches.  This change\n+\/\/ would allow smaller and faster in-line implementation of alloc_from_plab().  Since plabs are aligned on card-table boundaries,\n+\/\/ this object registration loop can be performed without acquiring a lock.\n+void ShenandoahHeap::retire_plab(PLAB* plab, Thread* thread) {\n+  \/\/ We don't enforce limits on plab_evacuated.  We let it consume all available old-gen memory in order to reduce\n+  \/\/ probability of an evacuation failure.  We do enforce limits on promotion, to make sure that excessive promotion\n+  \/\/ does not result in an old-gen evacuation failure.  Note that a failed promotion is relatively harmless.  Any\n+  \/\/ object that fails to promote in the current cycle will be eligible for promotion in a subsequent cycle.\n+\n+  \/\/ When the plab was instantiated, its entirety was treated as if the entire buffer was going to be dedicated to\n+  \/\/ promotions.  Now that we are retiring the buffer, we adjust for the reality that the plab is not entirely promotions.\n+  \/\/  1. Some of the plab may have been dedicated to evacuations.\n+  \/\/  2. Some of the plab may have been abandoned due to waste (at the end of the plab).\n+  size_t not_promoted =\n+    ShenandoahThreadLocalData::get_plab_preallocated_promoted(thread) - ShenandoahThreadLocalData::get_plab_promoted(thread);\n+  ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+  ShenandoahThreadLocalData::reset_plab_evacuated(thread);\n+  ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+  if (not_promoted > 0) {\n+    unexpend_promoted(not_promoted);\n+  }\n+  size_t waste = plab->waste();\n+  HeapWord* top = plab->top();\n+  plab->retire();\n+  if (top != nullptr && plab->waste() > waste && is_in_old(top)) {\n+    \/\/ If retiring the plab created a filler object, then we\n+    \/\/ need to register it with our card scanner so it can\n+    \/\/ safely walk the region backing the plab.\n+    log_debug(gc)(\"retire_plab() is registering remnant of size \" SIZE_FORMAT \" at \" PTR_FORMAT,\n+                  plab->waste() - waste, p2i(top));\n+    card_scan()->register_object_wo_lock(top);\n+  }\n+}\n+\n+void ShenandoahHeap::retire_plab(PLAB* plab) {\n+  Thread* thread = Thread::current();\n+  retire_plab(plab, thread);\n+}\n+\n+void ShenandoahHeap::cancel_old_gc() {\n+  shenandoah_assert_safepoint();\n+  assert(_old_generation != nullptr, \"Should only have mixed collections in generation mode.\");\n+  log_info(gc)(\"Terminating old gc cycle.\");\n+\n+  \/\/ Stop marking\n+  old_generation()->cancel_marking();\n+  \/\/ Stop coalescing undead objects\n+  set_prepare_for_old_mark_in_progress(false);\n+  \/\/ Stop tracking old regions\n+  old_heuristics()->abandon_collection_candidates();\n+  \/\/ Remove old generation access to young generation mark queues\n+  young_generation()->set_old_gen_task_queues(nullptr);\n+  \/\/ Transition to IDLE now.\n+  _old_generation->transition_to(ShenandoahOldGeneration::IDLE);\n+}\n+\n+bool ShenandoahHeap::is_old_gc_active() {\n+  return is_concurrent_old_mark_in_progress()\n+         || is_prepare_for_old_mark_in_progress()\n+         || old_heuristics()->unprocessed_old_collection_candidates() > 0\n+         || young_generation()->old_gen_task_queues() != nullptr;\n+}\n+\n+void ShenandoahHeap::coalesce_and_fill_old_regions() {\n+  class ShenandoahGlobalCoalesceAndFill : public ShenandoahHeapRegionClosure {\n+   public:\n+    virtual void heap_region_do(ShenandoahHeapRegion* region) override {\n+      \/\/ old region is not in the collection set and was not immediately trashed\n+      if (region->is_old() && region->is_active() && !region->is_humongous()) {\n+        \/\/ Reset the coalesce and fill boundary because this is a global collect\n+        \/\/ and cannot be preempted by young collects. We want to be sure the entire\n+        \/\/ region is coalesced here and does not resume from a previously interrupted\n+        \/\/ or completed coalescing.\n+        region->begin_preemptible_coalesce_and_fill();\n+        region->oop_fill_and_coalesce();\n+      }\n+    }\n+\n+    virtual bool is_thread_safe() override {\n+      return true;\n+    }\n+  };\n+  ShenandoahGlobalCoalesceAndFill coalesce;\n+  parallel_heap_region_iterate(&coalesce);\n+}\n+\n+bool ShenandoahHeap::adjust_generation_sizes() {\n+  if (mode()->is_generational()) {\n+    return _generation_sizer.adjust_generation_sizes();\n+  }\n+  return false;\n+}\n+\n+\/\/ Called from stubs in JIT code or interpreter\n@@ -799,1 +1112,1 @@\n-  HeapWord* res = allocate_memory(req);\n+  HeapWord* res = allocate_memory(req, false);\n@@ -812,1 +1125,16 @@\n-  HeapWord* res = allocate_memory(req);\n+  HeapWord* res = allocate_memory(req, false);\n+  if (res != nullptr) {\n+    *actual_size = req.actual_size();\n+  } else {\n+    *actual_size = 0;\n+  }\n+  return res;\n+}\n+\n+HeapWord* ShenandoahHeap::allocate_new_plab(size_t min_size,\n+                                            size_t word_size,\n+                                            size_t* actual_size) {\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(min_size, word_size);\n+  \/\/ Note that allocate_memory() sets a thread-local flag to prohibit further promotions by this thread\n+  \/\/ if we are at risk of exceeding the old-gen evacuation budget.\n+  HeapWord* res = allocate_memory(req, false);\n@@ -821,1 +1149,3 @@\n-HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest& req) {\n+\/\/ is_promotion is true iff this allocation is known for sure to hold the result of young-gen evacuation\n+\/\/ to old-gen.  plab allocates are not known as such, since they may hold old-gen evacuations.\n+HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest& req, bool is_promotion) {\n@@ -833,1 +1163,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -845,1 +1175,0 @@\n-\n@@ -847,1 +1176,0 @@\n-\n@@ -851,1 +1179,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -853,1 +1181,0 @@\n-\n@@ -857,1 +1184,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -859,1 +1186,0 @@\n-\n@@ -862,1 +1188,1 @@\n-    result = allocate_memory_under_lock(req, in_new_region);\n+    result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -869,0 +1195,1 @@\n+    regulator_thread()->notify_heap_changed();\n@@ -872,0 +1199,1 @@\n+    ShenandoahGeneration* alloc_generation = generation_for(req.affiliation());\n@@ -874,0 +1202,1 @@\n+    size_t actual_bytes = actual * HeapWordSize;\n@@ -881,0 +1210,1 @@\n+      alloc_generation->increase_allocated(actual_bytes);\n@@ -889,1 +1219,1 @@\n-      increase_used(actual*HeapWordSize);\n+      increase_used(actual_bytes);\n@@ -896,3 +1226,166 @@\n-HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest& req, bool& in_new_region) {\n-  ShenandoahHeapLocker locker(lock());\n-  return _free_set->allocate(req, in_new_region);\n+HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest& req, bool& in_new_region, bool is_promotion) {\n+  bool try_smaller_lab_size = false;\n+  size_t smaller_lab_size;\n+  {\n+    \/\/ promotion_eligible pertains only to PLAB allocations, denoting that the PLAB is allowed to allocate for promotions.\n+    bool promotion_eligible = false;\n+    bool allow_allocation = true;\n+    bool plab_alloc = false;\n+    size_t requested_bytes = req.size() * HeapWordSize;\n+    HeapWord* result = nullptr;\n+    ShenandoahHeapLocker locker(lock());\n+    Thread* thread = Thread::current();\n+\n+    if (mode()->is_generational()) {\n+      if (req.affiliation() == YOUNG_GENERATION) {\n+        if (req.is_mutator_alloc()) {\n+          size_t young_available = young_generation()->adjusted_available();\n+          if (requested_bytes > young_available) {\n+            \/\/ We know this is not a GCLAB.  This must be a TLAB or a shared allocation.\n+            if (req.is_lab_alloc() && (young_available >= req.min_size())) {\n+              try_smaller_lab_size = true;\n+              smaller_lab_size = young_available \/ HeapWordSize;\n+            } else {\n+              \/\/ Can't allocate because even min_size() is larger than remaining young_available\n+              log_info(gc, ergo)(\"Unable to shrink %s alloc request of minimum size: \" SIZE_FORMAT\n+                                 \", young available: \" SIZE_FORMAT,\n+                                 req.is_lab_alloc()? \"TLAB\": \"shared\",\n+                                 HeapWordSize * (req.is_lab_alloc()? req.min_size(): req.size()), young_available);\n+              return nullptr;\n+            }\n+          }\n+        }\n+      } else {                    \/\/ reg.affiliation() == OLD_GENERATION\n+        assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"GCLAB pertains only to young-gen memory\");\n+        if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+          plab_alloc = true;\n+          size_t promotion_avail = get_promoted_reserve();\n+          size_t promotion_expended = get_promoted_expended();\n+          if (promotion_expended + requested_bytes > promotion_avail) {\n+            promotion_avail = 0;\n+            if (get_old_evac_reserve() == 0) {\n+              \/\/ There are no old-gen evacuations in this pass.  There's no value in creating a plab that cannot\n+              \/\/ be used for promotions.\n+              allow_allocation = false;\n+            }\n+          } else {\n+            promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n+            promotion_eligible = true;\n+          }\n+        } else if (is_promotion) {\n+          \/\/ This is a shared alloc for promotion\n+          size_t promotion_avail = get_promoted_reserve();\n+          size_t promotion_expended = get_promoted_expended();\n+          if (promotion_expended + requested_bytes > promotion_avail) {\n+            promotion_avail = 0;\n+          } else {\n+            promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n+          }\n+          if (promotion_avail == 0) {\n+            \/\/ We need to reserve the remaining memory for evacuation.  Reject this allocation.  The object will be\n+            \/\/ evacuated to young-gen memory and promoted during a future GC pass.\n+            return nullptr;\n+          }\n+          \/\/ Else, we'll allow the allocation to proceed.  (Since we hold heap lock, the tested condition remains true.)\n+        } else {\n+          \/\/ This is a shared allocation for evacuation.  Memory has already been reserved for this purpose.\n+        }\n+      }\n+    } \/\/ This ends the is_generational() block\n+\n+    if (!try_smaller_lab_size) {\n+      result = (allow_allocation)? _free_set->allocate(req, in_new_region): nullptr;\n+      if (result != nullptr) {\n+        if (req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+          ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+          if (req.is_gc_alloc()) {\n+            if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+              if (promotion_eligible) {\n+                size_t actual_size = req.actual_size() * HeapWordSize;\n+                \/\/ Assume the entirety of this PLAB will be used for promotion.  This prevents promotion from overreach.\n+                \/\/ When we retire this plab, we'll unexpend what we don't really use.\n+                ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+                expend_promoted(actual_size);\n+                assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+                ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, actual_size);\n+              } else {\n+                \/\/ Disable promotions in this thread because entirety of this PLAB must be available to hold old-gen evacuations.\n+                ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+                ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+              }\n+            } else if (is_promotion) {\n+              \/\/ Shared promotion.  Assume size is requested_bytes.\n+              expend_promoted(requested_bytes);\n+              assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+            }\n+          }\n+\n+          \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+          \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+          \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+          \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+          \/\/\n+          \/\/ objects being \"concurrently\" allocated:\n+          \/\/    [-----a------][-----b-----][--------------c------------------]\n+          \/\/            [---- card table memory range --------------]\n+          \/\/\n+          \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n+          \/\/   wants to set the has-object, first-start, and last-start attributes of the preceding card region.\n+          \/\/   allocation of object b wants to set the has-object, first-start, and last-start attributes of this card region.\n+          \/\/   allocation of object c also wants to set the has-object, first-start, and last-start attributes of this card region.\n+          \/\/\n+          \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n+          \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n+          \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+          ShenandoahHeap::heap()->card_scan()->register_object(result);\n+        }\n+      } else {\n+        \/\/ The allocation failed.  If this was a plab allocation, We've already retired it and no longer have a plab.\n+        if ((req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) && req.is_gc_alloc() &&\n+            (req.type() == ShenandoahAllocRequest::_alloc_plab)) {\n+          \/\/ We don't need to disable PLAB promotions because there is no PLAB.  We leave promotions enabled because\n+          \/\/ this allows the surrounding infrastructure to retry alloc_plab_slow() with a smaller PLAB size.\n+          ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+        }\n+      }\n+      return result;\n+    }\n+    \/\/ else, try_smaller_lab_size is true so we fall through and recurse with a smaller lab size\n+  } \/\/ This closes the block that holds the heap lock.  This releases the lock.\n+\n+  \/\/ We arrive here if the tlab allocation request can be resized to fit within young_available\n+  assert((req.affiliation() == YOUNG_GENERATION) && req.is_lab_alloc() && req.is_mutator_alloc() &&\n+         (smaller_lab_size < req.size()), \"Only shrink allocation request size for TLAB allocations\");\n+\n+  \/\/ By convention, ShenandoahAllocationRequest is primarily read-only.  The only mutable instance data is represented by\n+  \/\/ actual_size(), which is overwritten with the size of the allocaion when the allocation request is satisfied.  We use a\n+  \/\/ recursive call here rather than introducing new methods to mutate the existing ShenandoahAllocationRequest argument.\n+  \/\/ Mutation of the existing object might result in astonishing results if calling contexts assume the content of immutable\n+  \/\/ fields remain constant.  The original TLAB allocation request was for memory that exceeded the current capacity.  We'll\n+  \/\/ attempt to allocate a smaller TLAB.  If this is successful, we'll update actual_size() of our incoming\n+  \/\/ ShenandoahAllocRequest.  If the recursive request fails, we'll simply return nullptr.\n+\n+  \/\/ Note that we've relinquished the HeapLock and some other thread may perform additional allocation before our recursive\n+  \/\/ call reacquires the lock.  If that happens, we will need another recursive call to further reduce the size of our request\n+  \/\/ for each time another thread allocates young memory during the brief intervals that the heap lock is available to\n+  \/\/ interfering threads.  We expect this interference to be rare.  The recursion bottoms out when young_available is\n+  \/\/ smaller than req.min_size().  The inner-nested call to allocate_memory_under_lock() uses the same min_size() value\n+  \/\/ as this call, but it uses a preferred size() that is smaller than our preferred size, and is no larger than what we most\n+  \/\/ recently saw as the memory currently available within the young generation.\n+\n+  \/\/ TODO: At the expense of code clarity, we could rewrite this recursive solution to use iteration.  We need at most one\n+  \/\/ extra instance of the ShenandoahAllocRequest, which we can re-initialize multiple times inside a loop, with one iteration\n+  \/\/ of the loop required for each time the existing solution would recurse.  An iterative solution would be more efficient\n+  \/\/ in CPU time and stack memory utilization.  The expectation is that it is very rare that we would recurse more than once\n+  \/\/ so making this change is not currently seen as a high priority.\n+\n+  ShenandoahAllocRequest smaller_req = ShenandoahAllocRequest::for_tlab(req.min_size(), smaller_lab_size);\n+\n+  \/\/ Note that shrinking the preferred size gets us past the gatekeeper that checks whether there's available memory to\n+  \/\/ satisfy the allocation request.  The reality is the actual TLAB size is likely to be even smaller, because it will\n+  \/\/ depend on how much memory is available within mutator regions that are not yet fully used.\n+  HeapWord* result = allocate_memory_under_lock(smaller_req, in_new_region, is_promotion);\n+  if (result != nullptr) {\n+    req.set_actual_size(smaller_req.actual_size());\n+  }\n+  return result;\n@@ -904,1 +1397,1 @@\n-  return allocate_memory(req);\n+  return allocate_memory(req, false);\n@@ -913,2 +1406,2 @@\n-  if (heuristics()->can_unload_classes()) {\n-    ShenandoahHeuristics* h = heuristics();\n+  ShenandoahHeuristics* h = global_generation()->heuristics();\n+  if (h->can_unload_classes()) {\n@@ -993,0 +1486,1 @@\n+\n@@ -998,0 +1492,69 @@\n+      if (_sh->check_cancelled_gc_and_yield(_concurrent)) {\n+        break;\n+      }\n+    }\n+  }\n+};\n+\n+\/\/ Unlike ShenandoahEvacuationTask, this iterates over all regions rather than just the collection set.\n+\/\/ This is needed in order to promote humongous start regions if age() >= tenure threshold.\n+class ShenandoahGenerationalEvacuationTask : public WorkerTask {\n+private:\n+  ShenandoahHeap* const _sh;\n+  ShenandoahRegionIterator *_regions;\n+  bool _concurrent;\n+public:\n+  ShenandoahGenerationalEvacuationTask(ShenandoahHeap* sh,\n+                                       ShenandoahRegionIterator* iterator,\n+                                       bool concurrent) :\n+    WorkerTask(\"Shenandoah Evacuation\"),\n+    _sh(sh),\n+    _regions(iterator),\n+    _concurrent(concurrent)\n+  {}\n+\n+  void work(uint worker_id) {\n+    if (_concurrent) {\n+      ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+      ShenandoahSuspendibleThreadSetJoiner stsj(ShenandoahSuspendibleWorkers);\n+      ShenandoahEvacOOMScope oom_evac_scope;\n+      do_work();\n+    } else {\n+      ShenandoahParallelWorkerSession worker_session(worker_id);\n+      ShenandoahEvacOOMScope oom_evac_scope;\n+      do_work();\n+    }\n+  }\n+\n+private:\n+  void do_work() {\n+    ShenandoahConcurrentEvacuateRegionObjectClosure cl(_sh);\n+    ShenandoahHeapRegion* r;\n+    while ((r = _regions->next()) != nullptr) {\n+      log_debug(gc)(\"GenerationalEvacuationTask do_work(), looking at %s region \" SIZE_FORMAT \", (age: %d) [%s, %s]\",\n+                    r->is_old()? \"old\": r->is_young()? \"young\": \"free\", r->index(), r->age(),\n+                    r->is_active()? \"active\": \"inactive\",\n+                    r->is_humongous()? (r->is_humongous_start()? \"humongous_start\": \"humongous_continuation\"): \"regular\");\n+      if (r->is_cset()) {\n+        assert(r->has_live(), \"Region \" SIZE_FORMAT \" should have been reclaimed early\", r->index());\n+        _sh->marked_object_iterate(r, &cl);\n+        if (ShenandoahPacing) {\n+          _sh->pacer()->report_evac(r->used() >> LogHeapWordSize);\n+        }\n+      } else if (r->is_young() && r->is_active() && r->is_humongous_start() && (r->age() > InitialTenuringThreshold)) {\n+        \/\/ We promote humongous_start regions along with their affiliated continuations during evacuation rather than\n+        \/\/ doing this work during a safepoint.  We cannot put humongous regions into the collection set because that\n+        \/\/ triggers the load-reference barrier (LRB) to copy on reference fetch.\n+        if (r->promote_humongous() == 0) {\n+          \/\/ We chose not to promote because old-gen is out of memory.  Report and handle the promotion failure because\n+          \/\/ this suggests need for expanding old-gen and\/or performing collection of old-gen.\n+          ShenandoahHeap* heap = ShenandoahHeap::heap();\n+          oop obj = cast_to_oop(r->bottom());\n+          size_t size = obj->size();\n+          Thread* thread = Thread::current();\n+          heap->report_promotion_failure(thread, size);\n+          heap->handle_promotion_failure();\n+        }\n+      }\n+      \/\/ else, region is free, or OLD, or not in collection set, or humongous_continuation,\n+      \/\/ or is young humongous_start that is too young to be promoted\n@@ -1007,2 +1570,8 @@\n-  ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n-  workers()->run_task(&task);\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahRegionIterator regions;\n+    ShenandoahGenerationalEvacuationTask task(this, &regions, concurrent);\n+    workers()->run_task(&task);\n+  } else {\n+    ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n+    workers()->run_task(&task);\n+  }\n@@ -1035,1 +1604,1 @@\n-void ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n+size_t ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n@@ -1055,0 +1624,1 @@\n+  return required_regions;\n@@ -1064,0 +1634,4 @@\n+\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+    assert(plab->words_remaining() == 0, \"PLAB should not need retirement\");\n@@ -1079,0 +1653,11 @@\n+\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+\n+    \/\/ There are two reasons to retire all plabs between old-gen evacuation passes.\n+    \/\/  1. We need to make the plab memory parseable by remembered-set scanning.\n+    \/\/  2. We need to establish a trustworthy UpdateWaterMark value within each old-gen heap region\n+    ShenandoahHeap::heap()->retire_plab(plab, thread);\n+    if (_resize && ShenandoahThreadLocalData::plab_size(thread) > 0) {\n+      ShenandoahThreadLocalData::set_plab_size(thread, 0);\n+    }\n@@ -1136,0 +1721,31 @@\n+class ShenandoahTagGCLABClosure : public ThreadClosure {\n+public:\n+  void do_thread(Thread* thread) {\n+    PLAB* gclab = ShenandoahThreadLocalData::gclab(thread);\n+    assert(gclab != nullptr, \"GCLAB should be initialized for %s\", thread->name());\n+    if (gclab->words_remaining() > 0) {\n+      ShenandoahHeapRegion* r = ShenandoahHeap::heap()->heap_region_containing(gclab->allocate(0));\n+      r->set_young_lab_flag();\n+    }\n+  }\n+};\n+\n+void ShenandoahHeap::set_young_lab_region_flags() {\n+  if (!UseTLAB) {\n+    return;\n+  }\n+  for (size_t i = 0; i < _num_regions; i++) {\n+    _regions[i]->clear_young_lab_flags();\n+  }\n+  ShenandoahTagGCLABClosure cl;\n+  workers()->threads_do(&cl);\n+  for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {\n+    cl.do_thread(t);\n+    ThreadLocalAllocBuffer& tlab = t->tlab();\n+    if (tlab.end() != nullptr) {\n+      ShenandoahHeapRegion* r = heap_region_containing(tlab.start());\n+      r->set_young_lab_flag();\n+    }\n+  }\n+}\n+\n@@ -1139,3 +1755,7 @@\n-    \/\/ With Elastic TLABs, return the max allowed size, and let the allocation path\n-    \/\/ figure out the safe size for current allocation.\n-    return ShenandoahHeapRegion::max_tlab_size_bytes();\n+    if (mode()->is_generational()) {\n+      return MIN2(ShenandoahHeapRegion::max_tlab_size_bytes(), young_generation()->adjusted_available());\n+    } else {\n+      \/\/ With Elastic TLABs, return the max allowed size, and let the allocation path\n+      \/\/ figure out the safe size for current allocation.\n+      return ShenandoahHeapRegion::max_tlab_size_bytes();\n+    }\n@@ -1185,0 +1805,1 @@\n+  tcl->do_thread(_regulator_thread);\n@@ -1207,0 +1828,4 @@\n+    ls.cr();\n+\n+    evac_tracker()->print_global_on(&ls);\n+\n@@ -1212,0 +1837,27 @@\n+void ShenandoahHeap::on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation) {\n+  set_gc_cause(cause);\n+  set_gc_generation(generation);\n+\n+  shenandoah_policy()->record_cycle_start();\n+  generation->heuristics()->record_cycle_start();\n+\n+  \/\/ When a cycle starts, attribute any thread activity when the collector\n+  \/\/ is idle to the global generation.\n+  _mmu_tracker.record(global_generation());\n+}\n+\n+void ShenandoahHeap::on_cycle_end(ShenandoahGeneration* generation) {\n+  generation->heuristics()->record_cycle_end();\n+\n+  if (mode()->is_generational() &&\n+      ((generation->generation_mode() == GLOBAL) || upgraded_to_full())) {\n+    \/\/ If we just completed a GLOBAL GC, claim credit for completion of young-gen and old-gen GC as well\n+    young_generation()->heuristics()->record_cycle_end();\n+    old_generation()->heuristics()->record_cycle_end();\n+  }\n+  set_gc_cause(GCCause::_no_gc);\n+\n+  \/\/ When a cycle ends, the thread activity is attributed to the respective generation\n+  _mmu_tracker.record(generation);\n+}\n+\n@@ -1533,23 +2185,0 @@\n-class ShenandoahInitMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahInitMarkUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-    if (r->is_active()) {\n-      \/\/ Check if region needs updating its TAMS. We have updated it already during concurrent\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n-      if (_ctx->top_at_mark_start(r) != r->top()) {\n-        _ctx->capture_top_at_mark_start(r);\n-      }\n-    } else {\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should already have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n@@ -1571,99 +2200,0 @@\n-class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n-      \/\/ anyway to capture any updates that happened since now.\n-      r->clear_live_data();\n-      _ctx->capture_top_at_mark_start(r);\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_gc() {\n-  reset_mark_bitmap();\n-\n-  ShenandoahResetUpdateRegionStateClosure cl;\n-  parallel_heap_region_iterate(&cl);\n-}\n-\n-class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalMarkUpdateRegionStateClosure() :\n-    _ctx(ShenandoahHeap::heap()->complete_marking_context()), _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n-      \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n-      HeapWord *tams = _ctx->top_at_mark_start(r);\n-      HeapWord *top = r->top();\n-      if (top > tams) {\n-        r->increase_live_data_alloc_words(pointer_delta(top, tams));\n-      }\n-\n-      \/\/ We are about to select the collection set, make sure it knows about\n-      \/\/ current pinning status. Also, this allows trashing more regions that\n-      \/\/ now have their pinning status dropped.\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n-      }\n-\n-      \/\/ Remember limit for updating refs. It's guaranteed that we get no\n-      \/\/ from-space-refs written from here on.\n-      r->set_update_watermark_at_safepoint(r->top());\n-    } else {\n-      assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_regions_and_collection_set(bool concurrent) {\n-  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states :\n-                                         ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n-    ShenandoahFinalMarkUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n-\n-    assert_pinned_region_status();\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset :\n-                                         ShenandoahPhaseTimings::degen_gc_choose_cset);\n-    ShenandoahHeapLocker locker(lock());\n-    _collection_set->clear();\n-    heuristics()->choose_collection_set(_collection_set);\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset :\n-                                         ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n-  }\n-}\n-\n@@ -1680,1 +2210,1 @@\n-  ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n+  active_generation()->ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n@@ -1711,4 +2241,44 @@\n-void ShenandoahHeap::set_concurrent_mark_in_progress(bool in_progress) {\n-  assert(!has_forwarded_objects(), \"Not expected before\/after mark phase\");\n-  set_gc_state_mask(MARKING, in_progress);\n-  ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(in_progress, !in_progress);\n+void ShenandoahHeap::set_concurrent_young_mark_in_progress(bool in_progress) {\n+  if (has_forwarded_objects()) {\n+    set_gc_state_mask(YOUNG_MARKING | UPDATEREFS, in_progress);\n+  } else {\n+    set_gc_state_mask(YOUNG_MARKING, in_progress);\n+  }\n+\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::set_concurrent_old_mark_in_progress(bool in_progress) {\n+  if (has_forwarded_objects()) {\n+    set_gc_state_mask(OLD_MARKING | UPDATEREFS, in_progress);\n+  } else {\n+    set_gc_state_mask(OLD_MARKING, in_progress);\n+  }\n+\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::set_prepare_for_old_mark_in_progress(bool in_progress) {\n+  \/\/ Unlike other set-gc-state functions, this may happen outside safepoint.\n+  \/\/ Is only set and queried by control thread, so no coherence issues.\n+  _prepare_for_old_mark = in_progress;\n+}\n+\n+void ShenandoahHeap::set_aging_cycle(bool in_progress) {\n+  _is_aging_cycle.set_cond(in_progress);\n+}\n+\n+void ShenandoahHeap::manage_satb_barrier(bool active) {\n+  if (is_concurrent_mark_in_progress()) {\n+    \/\/ Ignore request to deactivate barrier while concurrent mark is in progress.\n+    \/\/ Do not attempt to re-activate the barrier if it is already active.\n+    if (active && !ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  } else {\n+    \/\/ No concurrent marking is in progress so honor request to deactivate,\n+    \/\/ but only if the barrier is already active.\n+    if (!active && ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  }\n@@ -1759,0 +2329,8 @@\n+void ShenandoahHeap::cancel_concurrent_mark() {\n+  _young_generation->cancel_marking();\n+  _old_generation->cancel_marking();\n+  _global_generation->cancel_marking();\n+\n+  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+}\n+\n@@ -1764,0 +2342,4 @@\n+    _cancel_requested_time = os::elapsedTime();\n+    if (cause == GCCause::_shenandoah_upgrade_to_full_gc) {\n+      _upgraded_to_full = true;\n+    }\n@@ -1774,0 +2356,3 @@\n+  \/\/ Step 0a. Stop requesting collections.\n+  regulator_thread()->stop();\n+\n@@ -1879,4 +2464,0 @@\n-address ShenandoahHeap::cancelled_gc_addr() {\n-  return (address) ShenandoahHeap::heap()->_cancelled_gc.addr_of();\n-}\n-\n@@ -1887,4 +2468,0 @@\n-size_t ShenandoahHeap::bytes_allocated_since_gc_start() {\n-  return Atomic::load(&_bytes_allocated_since_gc_start);\n-}\n-\n@@ -1892,1 +2469,6 @@\n-  Atomic::store(&_bytes_allocated_since_gc_start, (size_t)0);\n+  if (mode()->is_generational()) {\n+    young_generation()->reset_bytes_allocated_since_gc_start();\n+    old_generation()->reset_bytes_allocated_since_gc_start();\n+  }\n+\n+  global_generation()->reset_bytes_allocated_since_gc_start();\n@@ -1956,2 +2538,4 @@\n-    assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n-           \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    if (active_generation()->contains(r)) {\n+      assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n+             \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    }\n@@ -2017,0 +2601,2 @@\n+  ShenandoahRegionChunkIterator* _work_chunks;\n+\n@@ -2018,1 +2604,2 @@\n-  ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n+  explicit ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions,\n+                                        ShenandoahRegionChunkIterator* work_chunks) :\n@@ -2021,1 +2608,3 @@\n-    _regions(regions) {\n+    _regions(regions),\n+    _work_chunks(work_chunks)\n+  {\n@@ -2028,1 +2617,1 @@\n-      do_work<ShenandoahConcUpdateRefsClosure>();\n+      do_work<ShenandoahConcUpdateRefsClosure>(worker_id);\n@@ -2031,1 +2620,1 @@\n-      do_work<ShenandoahSTWUpdateRefsClosure>();\n+      do_work<ShenandoahSTWUpdateRefsClosure>(worker_id);\n@@ -2037,1 +2626,1 @@\n-  void do_work() {\n+  void do_work(uint worker_id) {\n@@ -2040,1 +2629,4 @@\n-    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+    \/\/ We update references for global, old, and young collections.\n+    assert(_heap->active_generation()->is_mark_complete(), \"Expected complete marking\");\n+    ShenandoahMarkingContext* const ctx = _heap->marking_context();\n+    bool is_mixed = _heap->collection_set()->has_old_regions();\n@@ -2044,0 +2636,3 @@\n+\n+      log_debug(gc)(\"ShenandoahUpdateHeapRefsTask::do_work(%u) looking at region \" SIZE_FORMAT, worker_id, r->index());\n+      bool region_progress = false;\n@@ -2045,1 +2640,31 @@\n-        _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+        if (!_heap->mode()->is_generational() || (r->affiliation() == ShenandoahRegionAffiliation::YOUNG_GENERATION)) {\n+          _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+          region_progress = true;\n+        } else if (r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+          if (_heap->active_generation()->generation_mode() == GLOBAL) {\n+            \/\/ Note that GLOBAL collection is not as effectively balanced as young and mixed cycles.  This is because\n+            \/\/ concurrent GC threads are parceled out entire heap regions of work at a time and there\n+            \/\/ is no \"catchup phase\" consisting of remembered set scanning, during which parcels of work are smaller\n+            \/\/ and more easily distributed more fairly across threads.\n+\n+            \/\/ TODO: Consider an improvement to load balance GLOBAL GC.\n+            _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+            region_progress = true;\n+          }\n+          \/\/ Otherwise, this is an old region in a young or mixed cycle.  Process it during a second phase, below.\n+          \/\/ Don't bother to report pacing progress in this case.\n+        } else {\n+          \/\/ Because updating of references runs concurrently, it is possible that a FREE inactive region transitions\n+          \/\/ to a non-free active region while this loop is executing.  Whenever this happens, the changing of a region's\n+          \/\/ active status may propagate at a different speed than the changing of the region's affiliation.\n+\n+          \/\/ When we reach this control point, it is because a race has allowed a region's is_active() status to be seen\n+          \/\/ by this thread before the region's affiliation() is seen by this thread.\n+\n+          \/\/ It's ok for this race to occur because the newly transformed region does not have any references to be\n+          \/\/ updated.\n+\n+          assert(r->get_update_watermark() == r->bottom(),\n+                 \"%s Region \" SIZE_FORMAT \" is_active but not recognized as YOUNG or OLD so must be newly transitioned from FREE\",\n+                 affiliation_name(r->affiliation()), r->index());\n+        }\n@@ -2047,1 +2672,1 @@\n-      if (ShenandoahPacing) {\n+      if (region_progress && ShenandoahPacing) {\n@@ -2055,0 +2680,114 @@\n+\n+    if (_heap->mode()->is_generational() && (_heap->active_generation()->generation_mode() != GLOBAL)) {\n+      \/\/ Since this is generational and not GLOBAL, we have to process the remembered set.  There's no remembered\n+      \/\/ set processing if not in generational mode or if GLOBAL mode.\n+\n+      \/\/ After this thread has exhausted its traditional update-refs work, it continues with updating refs within remembered set.\n+      \/\/ The remembered set workload is better balanced between threads, so threads that are \"behind\" can catch up with other\n+      \/\/ threads during this phase, allowing all threads to work more effectively in parallel.\n+      struct ShenandoahRegionChunk assignment;\n+      RememberedScanner* scanner = _heap->card_scan();\n+\n+      while (!_heap->check_cancelled_gc_and_yield(CONCURRENT) && _work_chunks->next(&assignment)) {\n+        \/\/ Keep grabbing next work chunk to process until finished, or asked to yield\n+        ShenandoahHeapRegion* r = assignment._r;\n+        if (r->is_active() && !r->is_cset() && (r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION)) {\n+          HeapWord* start_of_range = r->bottom() + assignment._chunk_offset;\n+          HeapWord* end_of_range = r->get_update_watermark();\n+          if (end_of_range > start_of_range + assignment._chunk_size) {\n+            end_of_range = start_of_range + assignment._chunk_size;\n+          }\n+\n+          \/\/ Old region in a young cycle or mixed cycle.\n+          if (is_mixed) {\n+            \/\/ TODO: For mixed evac, consider building an old-gen remembered set that allows restricted updating\n+            \/\/ within old-gen HeapRegions.  This remembered set can be constructed by old-gen concurrent marking\n+            \/\/ and augmented by card marking.  For example, old-gen concurrent marking can remember for each old-gen\n+            \/\/ card which other old-gen regions it refers to: none, one-other specifically, multiple-other non-specific.\n+            \/\/ Update-references when _mixed_evac processess each old-gen memory range that has a traditional DIRTY\n+            \/\/ card or if the \"old-gen remembered set\" indicates that this card holds pointers specifically to an\n+            \/\/ old-gen region in the most recent collection set, or if this card holds pointers to other non-specific\n+            \/\/ old-gen heap regions.\n+\n+            if (r->is_humongous()) {\n+              if (start_of_range < end_of_range) {\n+                \/\/ Need to examine both dirty and clean cards during mixed evac.\n+                r->oop_iterate_humongous_slice(&cl, false, start_of_range, assignment._chunk_size, true);\n+              }\n+            } else {\n+              \/\/ Since this is mixed evacuation, old regions that are candidates for collection have not been coalesced\n+              \/\/ and filled.  Use mark bits to find objects that need to be updated.\n+              \/\/\n+              \/\/ Future TODO: establish a second remembered set to identify which old-gen regions point to other old-gen\n+              \/\/ regions which are in the collection set for a particular mixed evacuation.\n+              if (start_of_range < end_of_range) {\n+                HeapWord* p = nullptr;\n+                size_t card_index = scanner->card_index_for_addr(start_of_range);\n+                \/\/ In case last object in my range spans boundary of my chunk, I may need to scan all the way to top()\n+                ShenandoahObjectToOopBoundedClosure<T> objs(&cl, start_of_range, r->top());\n+\n+                \/\/ Any object that begins in a previous range is part of a different scanning assignment.  Any object that\n+                \/\/ starts after end_of_range is also not my responsibility.  (Either allocated during evacuation, so does\n+                \/\/ not hold pointers to from-space, or is beyond the range of my assigned work chunk.)\n+\n+                \/\/ Find the first object that begins in my range, if there is one.\n+                p = start_of_range;\n+                oop obj = cast_to_oop(p);\n+                HeapWord* tams = ctx->top_at_mark_start(r);\n+                if (p >= tams) {\n+                  \/\/ We cannot use ctx->is_marked(obj) to test whether an object begins at this address.  Instead,\n+                  \/\/ we need to use the remembered set crossing map to advance p to the first object that starts\n+                  \/\/ within the enclosing card.\n+\n+                  while (true) {\n+                    HeapWord* first_object = scanner->first_object_in_card(card_index);\n+                    if (first_object != nullptr) {\n+                      p = first_object;\n+                      break;\n+                    } else if (scanner->addr_for_card_index(card_index + 1) < end_of_range) {\n+                      card_index++;\n+                    } else {\n+                      \/\/ Force the loop that follows to immediately terminate.\n+                      p = end_of_range;\n+                      break;\n+                    }\n+                  }\n+                  obj = cast_to_oop(p);\n+                  \/\/ Note: p may be >= end_of_range\n+                } else if (!ctx->is_marked(obj)) {\n+                  p = ctx->get_next_marked_addr(p, tams);\n+                  obj = cast_to_oop(p);\n+                  \/\/ If there are no more marked objects before tams, this returns tams.\n+                  \/\/ Note that tams is either >= end_of_range, or tams is the start of an object that is marked.\n+                }\n+                while (p < end_of_range) {\n+                  \/\/ p is known to point to the beginning of marked object obj\n+                  objs.do_object(obj);\n+                  HeapWord* prev_p = p;\n+                  p += obj->size();\n+                  if (p < tams) {\n+                    p = ctx->get_next_marked_addr(p, tams);\n+                    \/\/ If there are no more marked objects before tams, this returns tams.  Note that tams is\n+                    \/\/ either >= end_of_range, or tams is the start of an object that is marked.\n+                  }\n+                  assert(p != prev_p, \"Lack of forward progress\");\n+                  obj = cast_to_oop(p);\n+                }\n+              }\n+            }\n+          } else {\n+            \/\/ This is a young evac..\n+            if (start_of_range < end_of_range) {\n+              size_t cluster_size =\n+                CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+              size_t clusters = assignment._chunk_size \/ cluster_size;\n+              assert(clusters * cluster_size == assignment._chunk_size, \"Chunk assignment must align on cluster boundaries\");\n+              scanner->process_region_slice(r, assignment._chunk_offset, clusters, end_of_range, &cl, true, worker_id);\n+            }\n+          }\n+          if (ShenandoahPacing && (start_of_range < end_of_range)) {\n+            _heap->pacer()->report_updaterefs(pointer_delta(end_of_range, start_of_range));\n+          }\n+        }\n+      }\n+    }\n@@ -2060,0 +2799,2 @@\n+  uint nworkers = workers()->active_workers();\n+  ShenandoahRegionChunkIterator work_list(nworkers);\n@@ -2062,1 +2803,1 @@\n-    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator, &work_list);\n@@ -2065,1 +2806,1 @@\n-    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator, &work_list);\n@@ -2068,0 +2809,3 @@\n+  if (ShenandoahEnableCardStats && card_scan()!=nullptr) { \/\/ generational check proxy\n+    card_scan()->log_card_stats(nworkers, CARD_STAT_UPDATE_REFS);\n+  }\n@@ -2070,1 +2814,0 @@\n-\n@@ -2073,0 +2816,1 @@\n+  ShenandoahMarkingContext* _ctx;\n@@ -2074,0 +2818,1 @@\n+  bool _is_generational;\n@@ -2076,1 +2821,3 @@\n-  ShenandoahFinalUpdateRefsUpdateRegionStateClosure() : _lock(ShenandoahHeap::heap()->lock()) {}\n+  ShenandoahFinalUpdateRefsUpdateRegionStateClosure(\n+    ShenandoahMarkingContext* ctx) : _ctx(ctx), _lock(ShenandoahHeap::heap()->lock()),\n+                                     _is_generational(ShenandoahHeap::heap()->mode()->is_generational()) { }\n@@ -2079,0 +2826,21 @@\n+\n+    \/\/ Maintenance of region age must follow evacuation in order to account for evacuation allocations within survivor\n+    \/\/ regions.  We consult region age during the subsequent evacuation to determine whether certain objects need to\n+    \/\/ be promoted.\n+    if (_is_generational && r->is_young()) {\n+      HeapWord *tams = _ctx->top_at_mark_start(r);\n+      HeapWord *top = r->top();\n+\n+      \/\/ Allocations move the watermark when top moves.  However compacting\n+      \/\/ objects will sometimes lower top beneath the watermark, after which,\n+      \/\/ attempts to read the watermark will assert out (watermark should not be\n+      \/\/ higher than top).\n+      if (top > tams) {\n+        \/\/ There have been allocations in this region since the start of the cycle.\n+        \/\/ Any objects new to this region must not assimilate elevated age.\n+        r->reset_age();\n+      } else if (ShenandoahHeap::heap()->is_aging_cycle()) {\n+        r->increment_age();\n+      }\n+    }\n+\n@@ -2081,1 +2849,0 @@\n-\n@@ -2108,1 +2875,1 @@\n-    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl;\n+    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl (active_generation()->complete_marking_context());\n@@ -2242,3 +3009,12 @@\n-  _memory_pool = new ShenandoahMemoryPool(this);\n-  _cycle_memory_manager.add_pool(_memory_pool);\n-  _stw_memory_manager.add_pool(_memory_pool);\n+  if (mode()->is_generational()) {\n+    _young_gen_memory_pool = new ShenandoahYoungGenMemoryPool(this);\n+    _old_gen_memory_pool = new ShenandoahOldGenMemoryPool(this);\n+    _cycle_memory_manager.add_pool(_young_gen_memory_pool);\n+    _cycle_memory_manager.add_pool(_old_gen_memory_pool);\n+    _stw_memory_manager.add_pool(_young_gen_memory_pool);\n+    _stw_memory_manager.add_pool(_old_gen_memory_pool);\n+  } else {\n+    _memory_pool = new ShenandoahMemoryPool(this);\n+    _cycle_memory_manager.add_pool(_memory_pool);\n+    _stw_memory_manager.add_pool(_memory_pool);\n+  }\n@@ -2256,1 +3032,6 @@\n-  memory_pools.append(_memory_pool);\n+  if (mode()->is_generational()) {\n+    memory_pools.append(_young_gen_memory_pool);\n+    memory_pools.append(_old_gen_memory_pool);\n+  } else {\n+    memory_pools.append(_memory_pool);\n+  }\n@@ -2261,1 +3042,1 @@\n-  return _memory_pool->get_memory_usage();\n+  return MemoryUsage(_initial_size, used(), committed(), max_capacity());\n@@ -2299,0 +3080,1 @@\n+\n@@ -2326,0 +3108,215 @@\n+\n+void ShenandoahHeap::transfer_old_pointers_from_satb() {\n+  _old_generation->transfer_pointers_from_satb();\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<YOUNG>::heap_region_do(ShenandoahHeapRegion* region) {\n+  \/\/ Visit young and free regions\n+  if (region->affiliation() != OLD_GENERATION) {\n+    _cl->heap_region_do(region);\n+  }\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<OLD>::heap_region_do(ShenandoahHeapRegion* region) {\n+  \/\/ Visit old and free regions\n+  if (region->affiliation() != YOUNG_GENERATION) {\n+    _cl->heap_region_do(region);\n+  }\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<GLOBAL>::heap_region_do(ShenandoahHeapRegion* region) {\n+  _cl->heap_region_do(region);\n+}\n+\n+\/\/ Assure that the remember set has a dirty card everywhere there is an interesting pointer.\n+\/\/ This examines the read_card_table between bottom() and top() since all PLABS are retired\n+\/\/ before the safepoint for init_mark.  Actually, we retire them before update-references and don't\n+\/\/ restore them until the start of evacuation.\n+void ShenandoahHeap::verify_rem_set_at_mark() {\n+  shenandoah_assert_safepoint();\n+  assert(mode()->is_generational(), \"Only verify remembered set for generational operational modes\");\n+\n+  ShenandoahRegionIterator iterator;\n+  RememberedScanner* scanner = card_scan();\n+  ShenandoahVerifyRemSetClosure check_interesting_pointers(true);\n+  ShenandoahMarkingContext* ctx;\n+\n+  log_debug(gc)(\"Verifying remembered set at %s mark\", doing_mixed_evacuations()? \"mixed\": \"young\");\n+\n+  if (is_old_bitmap_stable() || active_generation()->generation_mode() == GLOBAL) {\n+    ctx = complete_marking_context();\n+  } else {\n+    ctx = nullptr;\n+  }\n+\n+  while (iterator.has_next()) {\n+    ShenandoahHeapRegion* r = iterator.next();\n+    HeapWord* tams = ctx? ctx->top_at_mark_start(r): nullptr;\n+    if (r == nullptr)\n+      break;\n+    if (r->is_old() && r->is_active()) {\n+      HeapWord* obj_addr = r->bottom();\n+      if (r->is_humongous_start()) {\n+        oop obj = cast_to_oop(obj_addr);\n+        if (!ctx || ctx->is_marked(obj)) {\n+          \/\/ For humongous objects, the typical object is an array, so the following checks may be overkill\n+          \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+          \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+          if (!scanner->is_card_dirty(obj_addr) || obj->is_objArray()) {\n+            obj->oop_iterate(&check_interesting_pointers);\n+          }\n+          \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+        }\n+        \/\/ else, this humongous object is not marked so no need to verify its internal pointers\n+        if (!scanner->verify_registration(obj_addr, ctx)) {\n+          ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, nullptr,\n+                                          \"Verify init-mark remembered set violation\", \"object not properly registered\", __FILE__, __LINE__);\n+        }\n+      } else if (!r->is_humongous()) {\n+        HeapWord* top = r->top();\n+        while (obj_addr < top) {\n+          oop obj = cast_to_oop(obj_addr);\n+          \/\/ ctx->is_marked() returns true if mark bit set (TAMS not relevant during init mark)\n+          if (!ctx || ctx->is_marked(obj)) {\n+            \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+            \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+            if (!scanner->is_card_dirty(obj_addr) || obj->is_objArray()) {\n+              obj->oop_iterate(&check_interesting_pointers);\n+            }\n+            \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+            if (!scanner->verify_registration(obj_addr, ctx)) {\n+              ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, nullptr,\n+                                               \"Verify init-mark remembered set violation\", \"object not properly registered\", __FILE__, __LINE__);\n+            }\n+            obj_addr += obj->size();\n+          } else {\n+            \/\/ This object is not live so we don't verify dirty cards contained therein\n+            assert(tams != nullptr, \"If object is not live, ctx and tams should be non-null\");\n+            obj_addr = ctx->get_next_marked_addr(obj_addr, tams);\n+          }\n+        }\n+      } \/\/ else, we ignore humongous continuation region\n+    } \/\/ else, this is not an OLD region so we ignore it\n+  } \/\/ all regions have been processed\n+}\n+\n+void ShenandoahHeap::help_verify_region_rem_set(ShenandoahHeapRegion* r, ShenandoahMarkingContext* ctx, HeapWord* from,\n+                                                HeapWord* top, HeapWord* registration_watermark, const char* message) {\n+  RememberedScanner* scanner = card_scan();\n+  ShenandoahVerifyRemSetClosure check_interesting_pointers(false);\n+\n+  HeapWord* obj_addr = from;\n+  if (r->is_humongous_start()) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if (!ctx || ctx->is_marked(obj)) {\n+      size_t card_index = scanner->card_index_for_addr(obj_addr);\n+      \/\/ For humongous objects, the typical object is an array, so the following checks may be overkill\n+      \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+      \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+      if (!scanner->is_write_card_dirty(card_index) || obj->is_objArray()) {\n+        obj->oop_iterate(&check_interesting_pointers);\n+      }\n+      \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+    }\n+    \/\/ else, this humongous object is not live so no need to verify its internal pointers\n+\n+    if ((obj_addr < registration_watermark) && !scanner->verify_registration(obj_addr, ctx)) {\n+      ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, nullptr, message,\n+                                       \"object not properly registered\", __FILE__, __LINE__);\n+    }\n+  } else if (!r->is_humongous()) {\n+    while (obj_addr < top) {\n+      oop obj = cast_to_oop(obj_addr);\n+      \/\/ ctx->is_marked() returns true if mark bit set or if obj above TAMS.\n+      if (!ctx || ctx->is_marked(obj)) {\n+        size_t card_index = scanner->card_index_for_addr(obj_addr);\n+        \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+        \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+        if (!scanner->is_write_card_dirty(card_index) || obj->is_objArray()) {\n+          obj->oop_iterate(&check_interesting_pointers);\n+        }\n+        \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+\n+        if ((obj_addr < registration_watermark) && !scanner->verify_registration(obj_addr, ctx)) {\n+          ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, nullptr, message,\n+                                           \"object not properly registered\", __FILE__, __LINE__);\n+        }\n+        obj_addr += obj->size();\n+      } else {\n+        \/\/ This object is not live so we don't verify dirty cards contained therein\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        obj_addr = ctx->get_next_marked_addr(obj_addr, tams);\n+      }\n+    }\n+  }\n+}\n+\n+void ShenandoahHeap::verify_rem_set_after_full_gc() {\n+  shenandoah_assert_safepoint();\n+  assert(mode()->is_generational(), \"Only verify remembered set for generational operational modes\");\n+\n+  ShenandoahRegionIterator iterator;\n+\n+  while (iterator.has_next()) {\n+    ShenandoahHeapRegion* r = iterator.next();\n+    if (r == nullptr)\n+      break;\n+    if (r->is_old() && !r->is_cset()) {\n+      help_verify_region_rem_set(r, nullptr, r->bottom(), r->top(), r->top(), \"Remembered set violation at end of Full GC\");\n+    }\n+  }\n+}\n+\n+\/\/ Assure that the remember set has a dirty card everywhere there is an interesting pointer.  Even though\n+\/\/ the update-references scan of remembered set only examines cards up to update_watermark, the remembered\n+\/\/ set should be valid through top.  This examines the write_card_table between bottom() and top() because\n+\/\/ all PLABS are retired immediately before the start of update refs.\n+void ShenandoahHeap::verify_rem_set_at_update_ref() {\n+  shenandoah_assert_safepoint();\n+  assert(mode()->is_generational(), \"Only verify remembered set for generational operational modes\");\n+\n+  ShenandoahRegionIterator iterator;\n+  ShenandoahMarkingContext* ctx;\n+\n+  if (is_old_bitmap_stable() || active_generation()->generation_mode() == GLOBAL) {\n+    ctx = complete_marking_context();\n+  } else {\n+    ctx = nullptr;\n+  }\n+\n+  while (iterator.has_next()) {\n+    ShenandoahHeapRegion* r = iterator.next();\n+    if (r == nullptr)\n+      break;\n+    if (r->is_old() && !r->is_cset()) {\n+      help_verify_region_rem_set(r, ctx, r->bottom(), r->top(), r->get_update_watermark(),\n+                                 \"Remembered set violation at init-update-references\");\n+    }\n+  }\n+}\n+\n+ShenandoahGeneration* ShenandoahHeap::generation_for(ShenandoahRegionAffiliation affiliation) const {\n+  if (!mode()->is_generational()) {\n+    return global_generation();\n+  } else if (affiliation == YOUNG_GENERATION) {\n+    return young_generation();\n+  } else if (affiliation == OLD_GENERATION) {\n+    return old_generation();\n+  }\n+\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n+\n+void ShenandoahHeap::log_heap_status(const char* msg) const {\n+  if (mode()->is_generational()) {\n+    young_generation()->log_status(msg);\n+    old_generation()->log_status(msg);\n+  } else {\n+    global_generation()->log_status(msg);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":1262,"deletions":265,"binary":false,"changes":1527,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -36,0 +37,2 @@\n+#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMmuTracker.hpp\"\n@@ -39,0 +42,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.hpp\"\n@@ -46,0 +50,1 @@\n+class PLAB;\n@@ -48,0 +53,1 @@\n+class ShenandoahRegulatorThread;\n@@ -50,0 +56,3 @@\n+class ShenandoahGeneration;\n+class ShenandoahYoungGeneration;\n+class ShenandoahOldGeneration;\n@@ -51,0 +60,1 @@\n+class ShenandoahOldHeuristics;\n@@ -52,1 +62,0 @@\n-class ShenandoahMode;\n@@ -111,0 +120,10 @@\n+template<GenerationMode GENERATION>\n+class ShenandoahGenerationRegionClosure : public ShenandoahHeapRegionClosure {\n+ public:\n+  explicit ShenandoahGenerationRegionClosure(ShenandoahHeapRegionClosure* cl) : _cl(cl) {}\n+  void heap_region_do(ShenandoahHeapRegion* r);\n+  virtual bool is_thread_safe() { return _cl->is_thread_safe(); }\n+ private:\n+  ShenandoahHeapRegionClosure* _cl;\n+};\n+\n@@ -128,0 +147,1 @@\n+  friend class ShenandoahOldGC;\n@@ -136,0 +156,4 @@\n+  ShenandoahGeneration* _gc_generation;\n+\n+  \/\/ true iff we are concurrently coalescing and filling old-gen HeapRegions\n+  bool _prepare_for_old_mark;\n@@ -142,0 +166,15 @@\n+  ShenandoahGeneration* active_generation() const {\n+    \/\/ last or latest generation might be a better name here.\n+    return _gc_generation;\n+  }\n+\n+  void set_gc_generation(ShenandoahGeneration* generation) {\n+    _gc_generation = generation;\n+  }\n+\n+  ShenandoahOldHeuristics* old_heuristics();\n+\n+  bool doing_mixed_evacuations();\n+  bool is_old_bitmap_stable() const;\n+  bool is_gc_generation_young() const;\n+\n@@ -155,0 +194,1 @@\n+  void initialize_generations();\n@@ -167,0 +207,3 @@\n+  void verify_rem_set_at_mark();\n+  void verify_rem_set_at_update_ref();\n+  void verify_rem_set_after_full_gc();\n@@ -182,1 +225,0 @@\n-  volatile size_t _bytes_allocated_since_gc_start;\n@@ -185,0 +227,3 @@\n+  void help_verify_region_rem_set(ShenandoahHeapRegion* r, ShenandoahMarkingContext* ctx,\n+                                  HeapWord* from, HeapWord* top, HeapWord* update_watermark, const char* message);\n+\n@@ -192,1 +237,0 @@\n-  void increase_allocated(size_t bytes);\n@@ -194,1 +238,0 @@\n-  size_t bytes_allocated_since_gc_start();\n@@ -230,0 +273,1 @@\n+  uint8_t* _affiliations;       \/\/ Holds array of enum ShenandoahRegionAffiliation, including FREE status in non-generational mode\n@@ -261,2 +305,2 @@\n-    \/\/ Heap is under marking: needs SATB barriers.\n-    MARKING_BITPOS    = 1,\n+    \/\/ Young regions are under marking: needs SATB barriers.\n+    YOUNG_MARKING_BITPOS    = 1,\n@@ -272,0 +316,3 @@\n+\n+    \/\/ Old regions are under marking, still need SATB barriers.\n+    OLD_MARKING_BITPOS = 5\n@@ -277,1 +324,1 @@\n-    MARKING       = 1 << MARKING_BITPOS,\n+    YOUNG_MARKING = 1 << YOUNG_MARKING_BITPOS,\n@@ -281,0 +328,1 @@\n+    OLD_MARKING   = 1 << OLD_MARKING_BITPOS\n@@ -291,0 +339,47 @@\n+  \/\/ _alloc_supplement_reserve is a supplemental budget for new_memory allocations.  During evacuation and update-references,\n+  \/\/ mutator allocation requests are \"authorized\" iff young_gen->available() plus _alloc_supplement_reserve minus\n+  \/\/ _young_evac_reserve is greater than request size.  The values of _alloc_supplement_reserve and _young_evac_reserve\n+  \/\/ are zero except during evacuation and update-reference phases of GC.  Both of these values are established at\n+  \/\/ the start of evacuation, and they remain constant throughout the duration of these two phases of GC.  Since these\n+  \/\/ two values are constant throughout each GC phases, we introduce a new service into ShenandoahGeneration.  This service\n+  \/\/ provides adjusted_available() based on an adjusted capacity.  At the start of evacuation, we adjust young capacity by\n+  \/\/ adding the amount to be borrowed from old-gen and subtracting the _young_evac_reserve, we adjust old capacity by\n+  \/\/ subtracting the amount to be loaned to young-gen.\n+  \/\/\n+  \/\/ We always use adjusted capacities to determine permission to allocate within young and to promote into old.  Note\n+  \/\/ that adjusted capacities equal traditional capacities except during evacuation and update refs.\n+  \/\/\n+  \/\/ During evacuation, we assure that _old_evac_expended does not exceed _old_evac_reserve.\n+  \/\/\n+  \/\/ At the end of update references, we perform the following bookkeeping activities:\n+  \/\/\n+  \/\/ 1. Unadjust the capacity within young-gen and old-gen to undo the effects of borrowing memory from old-gen.  Note that\n+  \/\/    the entirety of the collection set is now available, so allocation capacity naturally increase at this time.\n+  \/\/ 2. Clear (reset to zero) _alloc_supplement_reserve, _young_evac_reserve, _old_evac_reserve, and _promoted_reserve\n+  \/\/\n+  \/\/ _young_evac_reserve and _old_evac_reserve are only non-zero during evacuation and update-references.\n+  \/\/\n+  \/\/ Allocation of old GCLABs assures that _old_evac_expended + request-size < _old_evac_reserved.  If the allocation\n+  \/\/  is authorized, increment _old_evac_expended by request size.  This allocation ignores old_gen->available().\n+  \/\/\n+  \/\/ Note that the typical total expenditure on evacuation is less than the associated evacuation reserve because we generally\n+  \/\/ reserve ShenandoahEvacWaste (> 1.0) times the anticipated evacuation need.  In the case that there is an excessive amount\n+  \/\/ of waste, it may be that one thread fails to grab a new GCLAB, this does not necessarily doom the associated evacuation\n+  \/\/ effort.  If this happens, the requesting thread blocks until some other thread manages to evacuate the offending object.\n+  \/\/ Only after \"all\" threads fail to evacuate an object do we consider the evacuation effort to have failed.\n+\n+  intptr_t _alloc_supplement_reserve;  \/\/ Bytes reserved for young allocations during evac and update refs\n+  size_t _promoted_reserve;            \/\/ Bytes reserved within old-gen to hold the results of promotion\n+  volatile size_t _promoted_expended;  \/\/ Bytes of old-gen memory expended on promotions\n+\n+  size_t _old_evac_reserve;            \/\/ Bytes reserved within old-gen to hold evacuated objects from old-gen collection set\n+  volatile size_t _old_evac_expended;  \/\/ Bytes of old-gen memory expended on old-gen evacuations\n+\n+  size_t _young_evac_reserve;          \/\/ Bytes reserved within young-gen to hold evacuated objects from young-gen collection set\n+\n+  size_t _captured_old_usage;          \/\/ What was old usage (bytes) when last captured?\n+\n+  size_t _previous_promotion;          \/\/ Bytes promoted during previous evacuation\n+\n+  bool _upgraded_to_full;\n+\n@@ -294,0 +389,2 @@\n+\n+\n@@ -298,1 +395,2 @@\n-  void set_concurrent_mark_in_progress(bool in_progress);\n+  void set_concurrent_young_mark_in_progress(bool in_progress);\n+  void set_concurrent_old_mark_in_progress(bool in_progress);\n@@ -307,0 +405,2 @@\n+  void set_prepare_for_old_mark_in_progress(bool cond);\n+  void set_aging_cycle(bool cond);\n@@ -311,0 +411,2 @@\n+  inline bool is_concurrent_young_mark_in_progress() const;\n+  inline bool is_concurrent_old_mark_in_progress() const;\n@@ -321,0 +423,35 @@\n+  inline bool is_prepare_for_old_mark_in_progress() const;\n+  inline bool is_aging_cycle() const;\n+  inline bool upgraded_to_full() { return _upgraded_to_full; }\n+  inline void start_conc_gc() { _upgraded_to_full = false; }\n+  inline void record_upgrade_to_full() { _upgraded_to_full = true; }\n+\n+  inline size_t capture_old_usage(size_t usage);\n+  inline void set_previous_promotion(size_t promoted_bytes);\n+  inline size_t get_previous_promotion() const;\n+\n+  \/\/ Returns previous value\n+  inline size_t set_promoted_reserve(size_t new_val);\n+  inline size_t get_promoted_reserve() const;\n+\n+  inline void reset_promoted_expended();\n+  inline size_t expend_promoted(size_t increment);\n+  inline size_t unexpend_promoted(size_t decrement);\n+  inline size_t get_promoted_expended();\n+\n+  \/\/ Returns previous value\n+  inline size_t set_old_evac_reserve(size_t new_val);\n+  inline size_t get_old_evac_reserve() const;\n+\n+  inline void reset_old_evac_expended();\n+  inline size_t expend_old_evac(size_t increment);\n+  inline size_t get_old_evac_expended();\n+\n+  \/\/ Returns previous value\n+  inline size_t set_young_evac_reserve(size_t new_val);\n+  inline size_t get_young_evac_reserve() const;\n+\n+  \/\/ Returns previous value.  This is a signed value because it is the amount borrowed minus the amount reserved for\n+  \/\/ young-gen evacuation.  In case we cannot borrow much, this value might be negative.\n+  inline intptr_t set_alloc_supplement_reserve(intptr_t new_val);\n+  inline intptr_t get_alloc_supplement_reserve() const;\n@@ -323,0 +460,2 @@\n+  void manage_satb_barrier(bool active);\n+\n@@ -338,0 +477,1 @@\n+  double _cancel_requested_time;\n@@ -339,0 +479,5 @@\n+\n+  \/\/ Returns true if cancel request was successfully communicated.\n+  \/\/ Returns false if some other thread already communicated cancel\n+  \/\/ request.  A true return value does not mean GC has been\n+  \/\/ cancelled, only that the process of cancelling GC has begun.\n@@ -342,2 +487,0 @@\n-  static address cancelled_gc_addr();\n-\n@@ -347,1 +490,1 @@\n-  inline void clear_cancelled_gc();\n+  inline void clear_cancelled_gc(bool clear_oom_handler = true);\n@@ -349,0 +492,1 @@\n+  void cancel_concurrent_mark();\n@@ -358,3 +502,0 @@\n-  \/\/ Reset bitmap, prepare regions for new GC cycle\n-  void prepare_gc();\n-  void prepare_regions_and_collection_set(bool concurrent);\n@@ -362,1 +503,0 @@\n-  void prepare_evacuation(bool concurrent);\n@@ -385,0 +525,4 @@\n+  ShenandoahYoungGeneration* _young_generation;\n+  ShenandoahGeneration*      _global_generation;\n+  ShenandoahOldGeneration*   _old_generation;\n+\n@@ -386,0 +530,1 @@\n+  ShenandoahRegulatorThread* _regulator_thread;\n@@ -388,1 +533,0 @@\n-  ShenandoahHeuristics*      _heuristics;\n@@ -393,1 +537,4 @@\n-  ShenandoahPhaseTimings*    _phase_timings;\n+  ShenandoahPhaseTimings*       _phase_timings;\n+  ShenandoahEvacuationTracker*  _evac_tracker;\n+  ShenandoahMmuTracker          _mmu_tracker;\n+  ShenandoahGenerationSizer     _generation_sizer;\n@@ -395,1 +542,1 @@\n-  ShenandoahControlThread*   control_thread()          { return _control_thread;    }\n+  ShenandoahRegulatorThread* regulator_thread()        { return _regulator_thread;  }\n@@ -398,0 +545,10 @@\n+  ShenandoahControlThread*   control_thread()          { return _control_thread;    }\n+  ShenandoahYoungGeneration* young_generation()  const { return _young_generation;  }\n+  ShenandoahGeneration*      global_generation() const { return _global_generation; }\n+  ShenandoahOldGeneration*   old_generation()    const { return _old_generation;    }\n+  ShenandoahGeneration*      generation_for(ShenandoahRegionAffiliation affiliation) const;\n+  const ShenandoahGenerationSizer* generation_sizer()  const { return &_generation_sizer;  }\n+\n+  size_t max_size_for(ShenandoahGeneration* generation) const;\n+  size_t min_size_for(ShenandoahGeneration* generation) const;\n+\n@@ -400,1 +557,0 @@\n-  ShenandoahHeuristics*      heuristics()        const { return _heuristics;        }\n@@ -404,1 +560,5 @@\n-  ShenandoahPhaseTimings*    phase_timings()     const { return _phase_timings;     }\n+  ShenandoahPhaseTimings*      phase_timings()   const { return _phase_timings;     }\n+  ShenandoahEvacuationTracker* evac_tracker()    const { return  _evac_tracker;     }\n+\n+  void on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation);\n+  void on_cycle_end(ShenandoahGeneration* generation);\n@@ -413,0 +573,3 @@\n+  MemoryPool*                  _young_gen_memory_pool;\n+  MemoryPool*                  _old_gen_memory_pool;\n+\n@@ -421,1 +584,1 @@\n-  ShenandoahMonitoringSupport* monitoring_support()          { return _monitoring_support;    }\n+  ShenandoahMonitoringSupport* monitoring_support() const    { return _monitoring_support;    }\n@@ -432,8 +595,0 @@\n-\/\/ ---------- Reference processing\n-\/\/\n-private:\n-  ShenandoahReferenceProcessor* const _ref_processor;\n-\n-public:\n-  ShenandoahReferenceProcessor* ref_processor() { return _ref_processor; }\n-\n@@ -443,0 +598,1 @@\n+  ShenandoahSharedFlag  _is_aging_cycle;\n@@ -458,0 +614,3 @@\n+  inline void assert_lock_for_affiliation(ShenandoahRegionAffiliation orig_affiliation,\n+                                          ShenandoahRegionAffiliation new_affiliation);\n+\n@@ -471,1 +630,12 @@\n-  bool is_in(const void* p) const override;\n+  inline bool is_in(const void* p) const override;\n+\n+  inline bool is_in_active_generation(oop obj) const;\n+  inline bool is_in_young(const void* p) const;\n+  inline bool is_in_old(const void* p) const;\n+  inline bool is_old(oop pobj) const;\n+\n+  inline ShenandoahRegionAffiliation region_affiliation(const ShenandoahHeapRegion* r);\n+  inline void set_affiliation(ShenandoahHeapRegion* r, ShenandoahRegionAffiliation new_affiliation);\n+\n+  inline ShenandoahRegionAffiliation region_affiliation(size_t index);\n+  inline void set_affiliation(size_t index, ShenandoahRegionAffiliation new_affiliation);\n@@ -525,1 +695,2 @@\n-  HeapWord* allocate_memory_under_lock(ShenandoahAllocRequest& request, bool& in_new_region);\n+  HeapWord* allocate_memory_under_lock(ShenandoahAllocRequest& request, bool& in_new_region, bool is_promotion);\n+\n@@ -530,0 +701,4 @@\n+  inline HeapWord* allocate_from_plab(Thread* thread, size_t size, bool is_promotion);\n+  HeapWord* allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion);\n+  HeapWord* allocate_new_plab(size_t min_size, size_t word_size, size_t* actual_size);\n+\n@@ -531,1 +706,1 @@\n-  HeapWord* allocate_memory(ShenandoahAllocRequest& request);\n+  HeapWord* allocate_memory(ShenandoahAllocRequest& request, bool is_promotion);\n@@ -551,0 +726,2 @@\n+  void set_young_lab_region_flags();\n+\n@@ -575,2 +752,0 @@\n-  inline void mark_complete_marking_context();\n-  inline void mark_incomplete_marking_context();\n@@ -587,2 +762,0 @@\n-  void reset_mark_bitmap();\n-\n@@ -608,0 +781,5 @@\n+  ShenandoahSharedFlag _old_gen_oom_evac;\n+\n+  inline oop try_evacuate_object(oop src, Thread* thread, ShenandoahHeapRegion* from_region, ShenandoahRegionAffiliation target_gen);\n+  void handle_old_evacuation(HeapWord* obj, size_t words, bool promotion);\n+  void handle_old_evacuation_failure();\n@@ -610,0 +788,3 @@\n+  void handle_promotion_failure();\n+  void report_promotion_failure(Thread* thread, size_t size);\n+\n@@ -620,1 +801,1 @@\n-  \/\/ Evacuates object src. Returns the evacuated object, either evacuated\n+  \/\/ Evacuates or promotes object src. Returns the evacuated object, either evacuated\n@@ -628,0 +809,20 @@\n+  inline bool clear_old_evacuation_failure();\n+\n+\/\/ ---------- Generational support\n+\/\/\n+private:\n+  RememberedScanner* _card_scan;\n+\n+public:\n+  inline RememberedScanner* card_scan() { return _card_scan; }\n+  void clear_cards_for(ShenandoahHeapRegion* region);\n+  void dirty_cards(HeapWord* start, HeapWord* end);\n+  void clear_cards(HeapWord* start, HeapWord* end);\n+  void mark_card_as_dirty(void* location);\n+  void retire_plab(PLAB* plab);\n+  void retire_plab(PLAB* plab, Thread* thread);\n+  void cancel_old_gc();\n+  bool is_old_gc_active();\n+  void coalesce_and_fill_old_regions();\n+  bool adjust_generation_sizes();\n+\n@@ -649,1 +850,8 @@\n-  void trash_humongous_region_at(ShenandoahHeapRegion *r);\n+  size_t trash_humongous_region_at(ShenandoahHeapRegion *r);\n+\n+  static inline void increase_object_age(oop obj, uint additional_age);\n+  static inline uint get_object_age(oop obj);\n+\n+  void transfer_old_pointers_from_satb();\n+\n+  void log_heap_status(const char *msg) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":246,"deletions":38,"binary":false,"changes":284,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -46,0 +47,2 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -268,1 +271,1 @@\n-inline void ShenandoahHeap::clear_cancelled_gc() {\n+inline void ShenandoahHeap::clear_cancelled_gc(bool clear_oom_handler) {\n@@ -270,1 +273,9 @@\n-  _oom_evac_handler.clear();\n+  if (_cancel_requested_time > 0) {\n+    double cancel_time = os::elapsedTime() - _cancel_requested_time;\n+    log_info(gc)(\"GC cancellation took %.3fs\", cancel_time);\n+    _cancel_requested_time = 0;\n+  }\n+\n+  if (clear_oom_handler) {\n+    _oom_evac_handler.clear();\n+  }\n@@ -287,1 +298,0 @@\n-  \/\/ Otherwise...\n@@ -291,0 +301,31 @@\n+inline HeapWord* ShenandoahHeap::allocate_from_plab(Thread* thread, size_t size, bool is_promotion) {\n+  assert(UseTLAB, \"TLABs should be enabled\");\n+\n+  PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+  HeapWord* obj;\n+  if (plab == nullptr) {\n+    assert(!thread->is_Java_thread() && !thread->is_Worker_thread(), \"Performance: thread should have PLAB: %s\", thread->name());\n+    \/\/ No PLABs in this thread, fallback to shared allocation\n+    return nullptr;\n+  } else if (is_promotion && (plab->words_remaining() > 0) && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+    return nullptr;\n+  }\n+  \/\/ if plab->word_size() <= 0, thread's plab not yet initialized for this pass, so allow_plab_promotions() is not trustworthy\n+  obj = plab->allocate(size);\n+  if ((obj == nullptr) && (plab->words_remaining() < PLAB::min_size())) {\n+    \/\/ allocate_from_plab_slow will establish allow_plab_promotions(thread) for future invocations\n+    obj = allocate_from_plab_slow(thread, size, is_promotion);\n+  }\n+  \/\/ if plab->words_remaining() >= PLAB::min_size(), just return nullptr so we can use a shared allocation\n+  if (obj == nullptr) {\n+    return nullptr;\n+  }\n+\n+  if (is_promotion) {\n+    ShenandoahThreadLocalData::add_to_plab_promoted(thread, size * HeapWordSize);\n+  } else {\n+    ShenandoahThreadLocalData::add_to_plab_evacuated(thread, size * HeapWordSize);\n+  }\n+  return obj;\n+}\n+\n@@ -292,1 +333,2 @@\n-  if (ShenandoahThreadLocalData::is_oom_during_evac(Thread::current())) {\n+  assert(thread == Thread::current(), \"Expected thread parameter to be current thread.\");\n+  if (ShenandoahThreadLocalData::is_oom_during_evac(thread)) {\n@@ -300,1 +342,2 @@\n-  size_t size = p->size();\n+  ShenandoahHeapRegion* r = heap_region_containing(p);\n+  assert(!r->is_humongous(), \"never evacuate humongous objects\");\n@@ -302,1 +345,21 @@\n-  assert(!heap_region_containing(p)->is_humongous(), \"never evacuate humongous objects\");\n+  ShenandoahRegionAffiliation target_gen = r->affiliation();\n+  if (mode()->is_generational() && ShenandoahHeap::heap()->is_gc_generation_young() &&\n+      target_gen == YOUNG_GENERATION) {\n+    markWord mark = p->mark();\n+    if (mark.is_marked()) {\n+      \/\/ Already forwarded.\n+      return ShenandoahBarrierSet::resolve_forwarded(p);\n+    }\n+    if (mark.has_displaced_mark_helper()) {\n+      \/\/ We don't want to deal with MT here just to ensure we read the right mark word.\n+      \/\/ Skip the potential promotion attempt for this one.\n+    } else if (r->age() + mark.age() >= InitialTenuringThreshold) {\n+      oop result = try_evacuate_object(p, thread, r, OLD_GENERATION);\n+      if (result != nullptr) {\n+        return result;\n+      }\n+      \/\/ If we failed to promote this aged object, we'll fall through to code below and evacuate to young-gen.\n+    }\n+  }\n+  return try_evacuate_object(p, thread, r, target_gen);\n+}\n@@ -304,1 +367,6 @@\n-  bool alloc_from_gclab = true;\n+\/\/ try_evacuate_object registers the object and dirties the associated remembered set information when evacuating\n+\/\/ to OLD_GENERATION.\n+inline oop ShenandoahHeap::try_evacuate_object(oop p, Thread* thread, ShenandoahHeapRegion* from_region,\n+                                               ShenandoahRegionAffiliation target_gen) {\n+  bool alloc_from_lab = true;\n+  bool has_plab = false;\n@@ -306,0 +374,2 @@\n+  size_t size = p->size();\n+  bool is_promotion = (target_gen == OLD_GENERATION) && from_region->is_young();\n@@ -314,1 +384,46 @@\n-      copy = allocate_from_gclab(thread, size);\n+      switch (target_gen) {\n+        case YOUNG_GENERATION: {\n+           copy = allocate_from_gclab(thread, size);\n+           if ((copy == nullptr) && (size < ShenandoahThreadLocalData::gclab_size(thread))) {\n+             \/\/ GCLAB allocation failed because we are bumping up against the limit on young evacuation reserve.  Try resetting\n+             \/\/ the desired GCLAB size and retry GCLAB allocation to avoid cascading of shared memory allocations.\n+             ShenandoahThreadLocalData::set_gclab_size(thread, PLAB::min_size());\n+             copy = allocate_from_gclab(thread, size);\n+             \/\/ If we still get nullptr, we'll try a shared allocation below.\n+           }\n+           break;\n+        }\n+        case OLD_GENERATION: {\n+\n+           PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+           if (plab != nullptr) {\n+             has_plab = true;\n+           }\n+           copy = allocate_from_plab(thread, size, is_promotion);\n+           if ((copy == nullptr) && (size < ShenandoahThreadLocalData::plab_size(thread)) &&\n+               ShenandoahThreadLocalData::plab_retries_enabled(thread)) {\n+             \/\/ PLAB allocation failed because we are bumping up against the limit on old evacuation reserve or because\n+             \/\/ the requested object does not fit within the current plab but the plab still has an \"abundance\" of memory,\n+             \/\/ where abundance is defined as >= PLAB::min_size().  In the former case, we try resetting the desired\n+             \/\/ PLAB size and retry PLAB allocation to avoid cascading of shared memory allocations.\n+\n+             \/\/ In this situation, PLAB memory is precious.  We'll try to preserve our existing PLAB by forcing\n+             \/\/ this particular allocation to be shared.\n+             if (plab->words_remaining() < PLAB::min_size()) {\n+               ShenandoahThreadLocalData::set_plab_size(thread, PLAB::min_size());\n+               copy = allocate_from_plab(thread, size, is_promotion);\n+               \/\/ If we still get nullptr, we'll try a shared allocation below.\n+               if (copy == nullptr) {\n+                 \/\/ If retry fails, don't continue to retry until we have success (probably in next GC pass)\n+                 ShenandoahThreadLocalData::disable_plab_retries(thread);\n+               }\n+             }\n+             \/\/ else, copy still equals nullptr.  this causes shared allocation below, preserving this plab for future needs.\n+           }\n+           break;\n+        }\n+        default: {\n+          ShouldNotReachHere();\n+          break;\n+        }\n+      }\n@@ -316,0 +431,1 @@\n+\n@@ -317,3 +433,10 @@\n-      ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size);\n-      copy = allocate_memory(req);\n-      alloc_from_gclab = false;\n+      \/\/ If we failed to allocate in LAB, we'll try a shared allocation.\n+      if (!is_promotion || !has_plab || (size > PLAB::min_size())) {\n+        ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen);\n+        copy = allocate_memory(req, is_promotion);\n+        alloc_from_lab = false;\n+      }\n+      \/\/ else, we leave copy equal to nullptr, signaling a promotion failure below if appropriate.\n+      \/\/ We choose not to promote objects smaller than PLAB::min_size() by way of shared allocations, as this is too\n+      \/\/ costly.  Instead, we'll simply \"evacuate\" to young-gen memory (using a GCLAB) and will promote in a future\n+      \/\/ evacuation pass.  This condition is denoted by: is_promotion && has_plab && (size <= PLAB::min_size())\n@@ -326,0 +449,14 @@\n+    if (target_gen == OLD_GENERATION) {\n+      assert(mode()->is_generational(), \"Should only be here in generational mode.\");\n+      if (from_region->is_young()) {\n+        \/\/ Signal that promotion failed. Will evacuate this old object somewhere in young gen.\n+        report_promotion_failure(thread, size);\n+        handle_promotion_failure();\n+        return nullptr;\n+      } else {\n+        \/\/ Remember that evacuation to old gen failed. We'll want to trigger a full gc to recover from this\n+        \/\/ after the evacuation threads have finished.\n+        handle_old_evacuation_failure();\n+      }\n+    }\n+\n@@ -334,0 +471,1 @@\n+  _evac_tracker->begin_evacuation(thread, size * HeapWordSize);\n@@ -336,1 +474,0 @@\n-  \/\/ Try to install the new forwarding pointer.\n@@ -338,0 +475,6 @@\n+\n+  if (mode()->is_generational() && target_gen == YOUNG_GENERATION && is_aging_cycle()) {\n+    ShenandoahHeap::increase_object_age(copy_val, from_region->age() + 1);\n+  }\n+\n+  \/\/ Try to install the new forwarding pointer.\n@@ -343,0 +486,4 @@\n+    _evac_tracker->end_evacuation(thread, size * HeapWordSize, ShenandoahHeap::get_object_age(copy_val));\n+    if (mode()->is_generational() && target_gen == OLD_GENERATION) {\n+      handle_old_evacuation(copy, size, from_region->is_young());\n+    }\n@@ -351,8 +498,23 @@\n-    \/\/\n-    \/\/ For GCLAB allocations, it is enough to rollback the allocation ptr. Either the next\n-    \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n-    \/\/ do this. For non-GCLAB allocations, we have no way to retract the allocation, and\n-    \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n-    \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n-    if (alloc_from_gclab) {\n-      ShenandoahThreadLocalData::gclab(thread)->undo_allocation(copy, size);\n+    if (alloc_from_lab) {\n+       \/\/ For LAB allocations, it is enough to rollback the allocation ptr. Either the next\n+       \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n+       \/\/ do this.\n+       switch (target_gen) {\n+         case YOUNG_GENERATION: {\n+             ShenandoahThreadLocalData::gclab(thread)->undo_allocation(copy, size);\n+            break;\n+         }\n+         case OLD_GENERATION: {\n+            ShenandoahThreadLocalData::plab(thread)->undo_allocation(copy, size);\n+            if (is_promotion) {\n+              ShenandoahThreadLocalData::subtract_from_plab_promoted(thread, size * HeapWordSize);\n+            } else {\n+              ShenandoahThreadLocalData::subtract_from_plab_evacuated(thread, size * HeapWordSize);\n+            }\n+            break;\n+         }\n+         default: {\n+           ShouldNotReachHere();\n+           break;\n+         }\n+       }\n@@ -360,0 +522,3 @@\n+      \/\/ For non-LAB allocations, we have no way to retract the allocation, and\n+      \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n+      \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n@@ -362,0 +527,1 @@\n+      \/\/ For non-LAB allocations, the object has already been registered\n@@ -368,0 +534,115 @@\n+void ShenandoahHeap::increase_object_age(oop obj, uint additional_age) {\n+  markWord w = obj->has_displaced_mark() ? obj->displaced_mark() : obj->mark();\n+  w = w.set_age(MIN2(markWord::max_age, w.age() + additional_age));\n+  if (obj->has_displaced_mark()) {\n+    obj->set_displaced_mark(w);\n+  } else {\n+    obj->set_mark(w);\n+  }\n+}\n+\n+uint ShenandoahHeap::get_object_age(oop obj) {\n+  markWord w = obj->has_displaced_mark() ? obj->displaced_mark() : obj->mark();\n+  return w.age();\n+}\n+\n+inline bool ShenandoahHeap::clear_old_evacuation_failure() {\n+  return _old_gen_oom_evac.try_unset();\n+}\n+\n+bool ShenandoahHeap::is_in(const void* p) const {\n+  HeapWord* heap_base = (HeapWord*) base();\n+  HeapWord* last_region_end = heap_base + ShenandoahHeapRegion::region_size_words() * num_regions();\n+  return p >= heap_base && p < last_region_end;\n+}\n+\n+inline bool ShenandoahHeap::is_in_active_generation(oop obj) const {\n+  if (!mode()->is_generational()) {\n+    \/\/ everything is the same single generation\n+    return true;\n+  }\n+\n+  if (active_generation() == nullptr) {\n+    \/\/ no collection is happening, only expect this to be called\n+    \/\/ when concurrent processing is active, but that could change\n+    return false;\n+  }\n+\n+  assert(is_in(obj), \"only check if is in active generation for objects (\" PTR_FORMAT \") in heap\", p2i(obj));\n+  assert((active_generation() == (ShenandoahGeneration*) old_generation()) ||\n+         (active_generation() == (ShenandoahGeneration*) young_generation()) ||\n+         (active_generation() == global_generation()), \"Active generation must be old, young, or global\");\n+\n+  size_t index = heap_region_containing(obj)->index();\n+  switch (_affiliations[index]) {\n+  case ShenandoahRegionAffiliation::FREE:\n+    \/\/ Free regions are in Old, Young, Global\n+    return true;\n+  case ShenandoahRegionAffiliation::YOUNG_GENERATION:\n+    \/\/ Young regions are in young_generation and global_generation, not in old_generation\n+    return (active_generation() != (ShenandoahGeneration*) old_generation());\n+  case ShenandoahRegionAffiliation::OLD_GENERATION:\n+    \/\/ Old regions are in old_generation and global_generation, not in young_generation\n+    return (active_generation() != (ShenandoahGeneration*) young_generation());\n+  default:\n+    assert(false, \"Bad affiliation (%d) for region \" SIZE_FORMAT, _affiliations[index], index);\n+    return false;\n+  }\n+}\n+\n+inline bool ShenandoahHeap::is_in_young(const void* p) const {\n+  return is_in(p) && (_affiliations[heap_region_index_containing(p)] == ShenandoahRegionAffiliation::YOUNG_GENERATION);\n+}\n+\n+inline bool ShenandoahHeap::is_in_old(const void* p) const {\n+  return is_in(p) && (_affiliations[heap_region_index_containing(p)] == ShenandoahRegionAffiliation::OLD_GENERATION);\n+}\n+\n+inline bool ShenandoahHeap::is_old(oop obj) const {\n+  return is_gc_generation_young() && is_in_old(obj);\n+}\n+\n+inline ShenandoahRegionAffiliation ShenandoahHeap::region_affiliation(const ShenandoahHeapRegion *r) {\n+  return (ShenandoahRegionAffiliation) _affiliations[r->index()];\n+}\n+\n+inline void ShenandoahHeap::assert_lock_for_affiliation(ShenandoahRegionAffiliation orig_affiliation,\n+                                                        ShenandoahRegionAffiliation new_affiliation) {\n+  \/\/ A lock is required when changing from FREE to NON-FREE.  Though it may be possible to elide the lock when\n+  \/\/ transitioning from in-use to FREE, the current implementation uses a lock for this transition.  A lock is\n+  \/\/ not required to change from YOUNG to OLD (i.e. when promoting humongous region).\n+  \/\/\n+  \/\/         new_affiliation is:     FREE   YOUNG   OLD\n+  \/\/  orig_affiliation is:  FREE      X       L      L\n+  \/\/                       YOUNG      L       X\n+  \/\/                         OLD      L       X      X\n+  \/\/  X means state transition won't happen (so don't care)\n+  \/\/  L means lock should be held\n+  \/\/  Blank means no lock required because affiliation visibility will not be required until subsequent safepoint\n+  \/\/\n+  \/\/ Note: during full GC, all transitions between states are possible.  During Full GC, we should be in a safepoint.\n+\n+  if ((orig_affiliation == ShenandoahRegionAffiliation::FREE) || (new_affiliation == ShenandoahRegionAffiliation::FREE)) {\n+    extern bool _is_at_shenandoah_safepoint();\n+    shenandoah_assert_heaplocked_or_fullgc_safepoint();\n+  }\n+}\n+\n+inline void ShenandoahHeap::set_affiliation(ShenandoahHeapRegion* r, ShenandoahRegionAffiliation new_affiliation) {\n+#ifdef ASSERT\n+  assert_lock_for_affiliation(region_affiliation(r), new_affiliation);\n+#endif\n+  _affiliations[r->index()] = (uint8_t) new_affiliation;\n+}\n+\n+inline ShenandoahRegionAffiliation ShenandoahHeap::region_affiliation(size_t index) {\n+  return (ShenandoahRegionAffiliation) _affiliations[index];\n+}\n+\n+inline void ShenandoahHeap::set_affiliation(size_t index, ShenandoahRegionAffiliation new_affiliation) {\n+#ifdef ASSERT\n+  assert_lock_for_affiliation(region_affiliation(index), new_affiliation);\n+#endif\n+  _affiliations[index] = (uint8_t) new_affiliation;\n+}\n+\n@@ -388,1 +669,1 @@\n-  return _gc_state.is_unset(MARKING | EVACUATION | UPDATEREFS);\n+  return _gc_state.is_unset(YOUNG_MARKING | OLD_MARKING | EVACUATION | UPDATEREFS);\n@@ -392,1 +673,9 @@\n-  return _gc_state.is_set(MARKING);\n+  return _gc_state.is_set(YOUNG_MARKING | OLD_MARKING);\n+}\n+\n+inline bool ShenandoahHeap::is_concurrent_young_mark_in_progress() const {\n+  return _gc_state.is_set(YOUNG_MARKING);\n+}\n+\n+inline bool ShenandoahHeap::is_concurrent_old_mark_in_progress() const {\n+  return _gc_state.is_set(OLD_MARKING);\n@@ -431,0 +720,92 @@\n+inline bool ShenandoahHeap::is_aging_cycle() const {\n+  return _is_aging_cycle.is_set();\n+}\n+\n+inline bool ShenandoahHeap::is_prepare_for_old_mark_in_progress() const {\n+  return _prepare_for_old_mark;\n+}\n+\n+inline size_t ShenandoahHeap::set_promoted_reserve(size_t new_val) {\n+  size_t orig = _promoted_reserve;\n+  _promoted_reserve = new_val;\n+  return orig;\n+}\n+\n+inline size_t ShenandoahHeap::get_promoted_reserve() const {\n+  return _promoted_reserve;\n+}\n+\n+\/\/ returns previous value\n+size_t ShenandoahHeap::capture_old_usage(size_t old_usage) {\n+  size_t previous_value = _captured_old_usage;\n+  _captured_old_usage = old_usage;\n+  return previous_value;\n+}\n+\n+void ShenandoahHeap::set_previous_promotion(size_t promoted_bytes) {\n+  shenandoah_assert_heaplocked();\n+  _previous_promotion = promoted_bytes;\n+}\n+\n+size_t ShenandoahHeap::get_previous_promotion() const {\n+  return _previous_promotion;\n+}\n+\n+inline size_t ShenandoahHeap::set_old_evac_reserve(size_t new_val) {\n+  size_t orig = _old_evac_reserve;\n+  _old_evac_reserve = new_val;\n+  return orig;\n+}\n+\n+inline size_t ShenandoahHeap::get_old_evac_reserve() const {\n+  return _old_evac_reserve;\n+}\n+\n+inline void ShenandoahHeap::reset_old_evac_expended() {\n+  Atomic::store(&_old_evac_expended, (size_t) 0);\n+}\n+\n+inline size_t ShenandoahHeap::expend_old_evac(size_t increment) {\n+  return Atomic::add(&_old_evac_expended, increment);\n+}\n+\n+inline size_t ShenandoahHeap::get_old_evac_expended() {\n+  return Atomic::load(&_old_evac_expended);\n+}\n+\n+inline void ShenandoahHeap::reset_promoted_expended() {\n+  Atomic::store(&_promoted_expended, (size_t) 0);\n+}\n+\n+inline size_t ShenandoahHeap::expend_promoted(size_t increment) {\n+  return Atomic::add(&_promoted_expended, increment);\n+}\n+\n+inline size_t ShenandoahHeap::unexpend_promoted(size_t decrement) {\n+  return Atomic::sub(&_promoted_expended, decrement);\n+}\n+\n+inline size_t ShenandoahHeap::get_promoted_expended() {\n+  return Atomic::load(&_promoted_expended);\n+}\n+\n+inline size_t ShenandoahHeap::set_young_evac_reserve(size_t new_val) {\n+  size_t orig = _young_evac_reserve;\n+  _young_evac_reserve = new_val;\n+  return orig;\n+}\n+\n+inline size_t ShenandoahHeap::get_young_evac_reserve() const {\n+  return _young_evac_reserve;\n+}\n+\n+inline intptr_t ShenandoahHeap::set_alloc_supplement_reserve(intptr_t new_val) {\n+  intptr_t orig = _alloc_supplement_reserve;\n+  _alloc_supplement_reserve = new_val;\n+  return orig;\n+}\n+\n+inline intptr_t ShenandoahHeap::get_alloc_supplement_reserve() const {\n+  return _alloc_supplement_reserve;\n+}\n+\n@@ -440,2 +821,1 @@\n-  ShenandoahMarkingContext* const ctx = complete_marking_context();\n-  assert(ctx->is_complete(), \"sanity\");\n+  ShenandoahMarkingContext* const ctx = marking_context();\n@@ -572,8 +952,0 @@\n-inline void ShenandoahHeap::mark_complete_marking_context() {\n-  _marking_context->mark_complete();\n-}\n-\n-inline void ShenandoahHeap::mark_incomplete_marking_context() {\n-  _marking_context->mark_incomplete();\n-}\n-\n@@ -589,0 +961,24 @@\n+inline void ShenandoahHeap::clear_cards_for(ShenandoahHeapRegion* region) {\n+  if (mode()->is_generational()) {\n+    _card_scan->mark_range_as_empty(region->bottom(), pointer_delta(region->end(), region->bottom()));\n+  }\n+}\n+\n+inline void ShenandoahHeap::dirty_cards(HeapWord* start, HeapWord* end) {\n+  assert(mode()->is_generational(), \"Should only be used for generational mode\");\n+  size_t words = pointer_delta(end, start);\n+  _card_scan->mark_range_as_dirty(start, words);\n+}\n+\n+inline void ShenandoahHeap::clear_cards(HeapWord* start, HeapWord* end) {\n+  assert(mode()->is_generational(), \"Should only be used for generational mode\");\n+  size_t words = pointer_delta(end, start);\n+  _card_scan->mark_range_as_clean(start, words);\n+}\n+\n+inline void ShenandoahHeap::mark_card_as_dirty(void* location) {\n+  if (mode()->is_generational()) {\n+    _card_scan->mark_card_as_dirty((HeapWord*)location);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":428,"deletions":32,"binary":false,"changes":460,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2013, 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2013, 2020, Red Hat, Inc. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/shared\/cardTable.hpp\"\n@@ -29,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -33,0 +35,4 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -47,0 +53,1 @@\n+\n@@ -69,0 +76,2 @@\n+  _plab_allocs(0),\n+  _has_young_lab(false),\n@@ -71,1 +80,2 @@\n-  _update_watermark(start) {\n+  _update_watermark(start),\n+  _age(0) {\n@@ -87,1 +97,1 @@\n-void ShenandoahHeapRegion::make_regular_allocation() {\n+void ShenandoahHeapRegion::make_regular_allocation(ShenandoahRegionAffiliation affiliation) {\n@@ -89,1 +99,1 @@\n-\n+  reset_age();\n@@ -94,0 +104,1 @@\n+      set_affiliation(affiliation);\n@@ -103,0 +114,21 @@\n+\/\/ Change affiliation to YOUNG_GENERATION if _state is not _pinned_cset, _regular, or _pinned.  This implements\n+\/\/ behavior previously performed as a side effect of make_regular_bypass().\n+void ShenandoahHeapRegion::make_young_maybe() {\n+  shenandoah_assert_heaplocked();\n+  switch (_state) {\n+   case _empty_uncommitted:\n+   case _empty_committed:\n+   case _cset:\n+   case _humongous_start:\n+   case _humongous_cont:\n+     set_affiliation(YOUNG_GENERATION);\n+     return;\n+   case _pinned_cset:\n+   case _regular:\n+   case _pinned:\n+     return;\n+   default:\n+     assert(false, \"Unexpected _state in make_young_maybe\");\n+  }\n+}\n+\n@@ -107,1 +139,1 @@\n-\n+  reset_age();\n@@ -130,0 +162,1 @@\n+  reset_age();\n@@ -141,1 +174,1 @@\n-void ShenandoahHeapRegion::make_humongous_start_bypass() {\n+void ShenandoahHeapRegion::make_humongous_start_bypass(ShenandoahRegionAffiliation affiliation) {\n@@ -144,1 +177,2 @@\n-\n+  set_affiliation(affiliation);\n+  reset_age();\n@@ -159,0 +193,1 @@\n+  reset_age();\n@@ -170,1 +205,1 @@\n-void ShenandoahHeapRegion::make_humongous_cont_bypass() {\n+void ShenandoahHeapRegion::make_humongous_cont_bypass(ShenandoahRegionAffiliation affiliation) {\n@@ -173,1 +208,2 @@\n-\n+  set_affiliation(affiliation);\n+  reset_age();\n@@ -214,0 +250,1 @@\n+      assert(affiliation() != FREE, \"Pinned region should not be FREE\");\n@@ -232,0 +269,1 @@\n+  \/\/ Leave age untouched.  We need to consult the age when we are deciding whether to promote evacuated objects.\n@@ -244,0 +282,1 @@\n+  reset_age();\n@@ -264,1 +303,2 @@\n-  ShenandoahHeap::heap()->complete_marking_context()->reset_top_bitmap(this);\n+  assert(ShenandoahHeap::heap()->active_generation()->is_mark_complete(), \"Marking should be complete here.\");\n+  ShenandoahHeap::heap()->marking_context()->reset_top_bitmap(this);\n@@ -269,0 +309,1 @@\n+  reset_age();\n@@ -308,0 +349,1 @@\n+  _plab_allocs = 0;\n@@ -311,1 +353,1 @@\n-  return used() - (_tlab_allocs + _gclab_allocs) * HeapWordSize;\n+  return used() - (_tlab_allocs + _gclab_allocs + _plab_allocs) * HeapWordSize;\n@@ -322,0 +364,4 @@\n+size_t ShenandoahHeapRegion::get_plab_allocs() const {\n+  return _plab_allocs * HeapWordSize;\n+}\n+\n@@ -366,0 +412,14 @@\n+  switch (ShenandoahHeap::heap()->region_affiliation(this)) {\n+    case ShenandoahRegionAffiliation::FREE:\n+      st->print(\"|F\");\n+      break;\n+    case ShenandoahRegionAffiliation::YOUNG_GENERATION:\n+      st->print(\"|Y\");\n+      break;\n+    case ShenandoahRegionAffiliation::OLD_GENERATION:\n+      st->print(\"|O\");\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+\n@@ -377,0 +437,3 @@\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    st->print(\"|P \" SIZE_FORMAT_W(5) \"%1s\", byte_size_in_proper_unit(get_plab_allocs()),   proper_unit_for_byte_size(get_plab_allocs()));\n+  }\n@@ -385,1 +448,94 @@\n-void ShenandoahHeapRegion::oop_iterate(OopIterateClosure* blk) {\n+\/\/ oop_iterate without closure and without cancellation.  always return true.\n+bool ShenandoahHeapRegion::oop_fill_and_coalesce_wo_cancel() {\n+  HeapWord* obj_addr = resume_coalesce_and_fill();\n+\n+  assert(!is_humongous(), \"No need to fill or coalesce humongous regions\");\n+  if (!is_active()) {\n+    end_preemptible_coalesce_and_fill();\n+    return true;\n+  }\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* marking_context = heap->marking_context();\n+  \/\/ All objects above TAMS are considered live even though their mark bits will not be set.  Note that young-\n+  \/\/ gen evacuations that interrupt a long-running old-gen concurrent mark may promote objects into old-gen\n+  \/\/ while the old-gen concurrent marking is ongoing.  These newly promoted objects will reside above TAMS\n+  \/\/ and will be treated as live during the current old-gen marking pass, even though they will not be\n+  \/\/ explicitly marked.\n+  HeapWord* t = marking_context->top_at_mark_start(this);\n+\n+  \/\/ Expect marking to be completed before these threads invoke this service.\n+  assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n+  while (obj_addr < t) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if (marking_context->is_marked(obj)) {\n+      assert(obj->klass() != nullptr, \"klass should not be nullptr\");\n+      obj_addr += obj->size();\n+    } else {\n+      \/\/ Object is not marked.  Coalesce and fill dead object with dead neighbors.\n+      HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, t);\n+      assert(next_marked_obj <= t, \"next marked object cannot exceed top\");\n+      size_t fill_size = next_marked_obj - obj_addr;\n+      ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n+      heap->card_scan()->coalesce_objects(obj_addr, fill_size);\n+      obj_addr = next_marked_obj;\n+    }\n+  }\n+  \/\/ Mark that this region has been coalesced and filled\n+  end_preemptible_coalesce_and_fill();\n+  return true;\n+}\n+\n+\/\/ oop_iterate without closure, return true if completed without cancellation\n+bool ShenandoahHeapRegion::oop_fill_and_coalesce() {\n+  HeapWord* obj_addr = resume_coalesce_and_fill();\n+  \/\/ Consider yielding to cancel\/preemption request after this many coalesce operations (skip marked, or coalesce free).\n+  const size_t preemption_stride = 128;\n+\n+  assert(!is_humongous(), \"No need to fill or coalesce humongous regions\");\n+  if (!is_active()) {\n+    end_preemptible_coalesce_and_fill();\n+    return true;\n+  }\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* marking_context = heap->marking_context();\n+  \/\/ All objects above TAMS are considered live even though their mark bits will not be set.  Note that young-\n+  \/\/ gen evacuations that interrupt a long-running old-gen concurrent mark may promote objects into old-gen\n+  \/\/ while the old-gen concurrent marking is ongoing.  These newly promoted objects will reside above TAMS\n+  \/\/ and will be treated as live during the current old-gen marking pass, even though they will not be\n+  \/\/ explicitly marked.\n+  HeapWord* t = marking_context->top_at_mark_start(this);\n+\n+  \/\/ Expect marking to be completed before these threads invoke this service.\n+  assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n+\n+  size_t ops_before_preempt_check = preemption_stride;\n+  while (obj_addr < t) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if (marking_context->is_marked(obj)) {\n+      assert(obj->klass() != nullptr, \"klass should not be nullptr\");\n+      obj_addr += obj->size();\n+    } else {\n+      \/\/ Object is not marked.  Coalesce and fill dead object with dead neighbors.\n+      HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, t);\n+      assert(next_marked_obj <= t, \"next marked object cannot exceed top\");\n+      size_t fill_size = next_marked_obj - obj_addr;\n+      ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n+      heap->card_scan()->coalesce_objects(obj_addr, fill_size);\n+      obj_addr = next_marked_obj;\n+    }\n+    if (ops_before_preempt_check-- == 0) {\n+      if (heap->cancelled_gc()) {\n+        suspend_coalesce_and_fill(obj_addr);\n+        return false;\n+      }\n+      ops_before_preempt_check = preemption_stride;\n+    }\n+  }\n+  \/\/ Mark that this region has been coalesced and filled\n+  end_preemptible_coalesce_and_fill();\n+  return true;\n+}\n+\n+void ShenandoahHeapRegion::global_oop_iterate_and_fill_dead(OopIterateClosure* blk) {\n@@ -388,0 +544,2 @@\n+    \/\/ No need to fill dead within humongous regions.  Either the entire region is dead, or the entire region is\n+    \/\/ unchanged.  A humongous region holds no more than one humongous object.\n@@ -390,1 +548,1 @@\n-    oop_iterate_objects(blk);\n+    global_oop_iterate_objects_and_fill_dead(blk);\n@@ -394,2 +552,2 @@\n-void ShenandoahHeapRegion::oop_iterate_objects(OopIterateClosure* blk) {\n-  assert(! is_humongous(), \"no humongous region here\");\n+void ShenandoahHeapRegion::global_oop_iterate_objects_and_fill_dead(OopIterateClosure* blk) {\n+  assert(!is_humongous(), \"no humongous region here\");\n@@ -397,2 +555,30 @@\n-  HeapWord* t = top();\n-  \/\/ Could call objects iterate, but this is easier.\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* marking_context = heap->marking_context();\n+  RememberedScanner* rem_set_scanner = heap->card_scan();\n+  \/\/ Objects allocated above TAMS are not marked, but are considered live for purposes of current GC efforts.\n+  HeapWord* t = marking_context->top_at_mark_start(this);\n+\n+  assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n+\n+  while (obj_addr < t) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if (marking_context->is_marked(obj)) {\n+      assert(obj->klass() != nullptr, \"klass should not be nullptr\");\n+      \/\/ when promoting an entire region, we have to register the marked objects as well\n+      obj_addr += obj->oop_iterate_size(blk);\n+    } else {\n+      \/\/ Object is not marked.  Coalesce and fill dead object with dead neighbors.\n+      HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, t);\n+      assert(next_marked_obj <= t, \"next marked object cannot exceed top\");\n+      size_t fill_size = next_marked_obj - obj_addr;\n+      ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n+\n+      \/\/ coalesce_objects() unregisters all but first object subsumed within coalesced range.\n+      rem_set_scanner->coalesce_objects(obj_addr, fill_size);\n+      obj_addr = next_marked_obj;\n+    }\n+  }\n+\n+  \/\/ Any object above TAMS and below top() is considered live.\n+  t = top();\n@@ -405,0 +591,50 @@\n+\/\/ DO NOT CANCEL.  If this worker thread has accepted responsibility for scanning a particular range of addresses, it\n+\/\/ must finish the work before it can be cancelled.\n+void ShenandoahHeapRegion::oop_iterate_humongous_slice(OopIterateClosure* blk, bool dirty_only,\n+                                                       HeapWord* start, size_t words, bool write_table) {\n+  assert(words % CardTable::card_size_in_words() == 0, \"Humongous iteration must span whole number of cards\");\n+  assert(is_humongous(), \"only humongous region here\");\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  \/\/ Find head.\n+  ShenandoahHeapRegion* r = humongous_start_region();\n+  assert(r->is_humongous_start(), \"need humongous head here\");\n+  assert(CardTable::card_size_in_words() * (words \/ CardTable::card_size_in_words()) == words,\n+         \"slice must be integral number of cards\");\n+\n+  oop obj = cast_to_oop(r->bottom());\n+  RememberedScanner* scanner = ShenandoahHeap::heap()->card_scan();\n+  size_t card_index = scanner->card_index_for_addr(start);\n+  size_t num_cards = words \/ CardTable::card_size_in_words();\n+\n+  if (dirty_only) {\n+    if (write_table) {\n+      while (num_cards-- > 0) {\n+        if (scanner->is_write_card_dirty(card_index++)) {\n+          obj->oop_iterate(blk, MemRegion(start, start + CardTable::card_size_in_words()));\n+        }\n+        start += CardTable::card_size_in_words();\n+      }\n+    } else {\n+      while (num_cards-- > 0) {\n+        if (scanner->is_card_dirty(card_index++)) {\n+          obj->oop_iterate(blk, MemRegion(start, start + CardTable::card_size_in_words()));\n+        }\n+        start += CardTable::card_size_in_words();\n+      }\n+    }\n+  } else {\n+    \/\/ Scan all data, regardless of whether cards are dirty\n+    obj->oop_iterate(blk, MemRegion(start, start + num_cards * CardTable::card_size_in_words()));\n+  }\n+}\n+\n+void ShenandoahHeapRegion::oop_iterate_humongous(OopIterateClosure* blk, HeapWord* start, size_t words) {\n+  assert(is_humongous(), \"only humongous region here\");\n+  \/\/ Find head.\n+  ShenandoahHeapRegion* r = humongous_start_region();\n+  assert(r->is_humongous_start(), \"need humongous head here\");\n+  oop obj = cast_to_oop(r->bottom());\n+  obj->oop_iterate(blk, MemRegion(start, start + words));\n+}\n+\n@@ -430,0 +666,5 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  shenandoah_assert_heaplocked();\n+\n+  heap->generation_for(affiliation())->decrease_used(used());\n+\n@@ -435,1 +676,1 @@\n-  ShenandoahHeap::heap()->marking_context()->reset_top_at_mark_start(this);\n+  heap->marking_context()->reset_top_at_mark_start(this);\n@@ -439,0 +680,1 @@\n+  set_affiliation(FREE);\n@@ -483,0 +725,5 @@\n+  \/\/ Generational Shenandoah needs this alignment for card tables.\n+  if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+    max_heap_size = align_up(max_heap_size , CardTable::ct_max_alignment_constraint());\n+  }\n+\n@@ -689,0 +936,151 @@\n+\n+void ShenandoahHeapRegion::set_affiliation(ShenandoahRegionAffiliation new_affiliation) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  ShenandoahRegionAffiliation region_affiliation = heap->region_affiliation(this);\n+  {\n+    ShenandoahMarkingContext* const ctx = heap->complete_marking_context();\n+    log_debug(gc)(\"Setting affiliation of Region \" SIZE_FORMAT \" from %s to %s, top: \" PTR_FORMAT \", TAMS: \" PTR_FORMAT\n+                  \", watermark: \" PTR_FORMAT \", top_bitmap: \" PTR_FORMAT,\n+                  index(), affiliation_name(region_affiliation), affiliation_name(new_affiliation),\n+                  p2i(top()), p2i(ctx->top_at_mark_start(this)), p2i(_update_watermark), p2i(ctx->top_bitmap(this)));\n+  }\n+\n+#ifdef ASSERT\n+  {\n+    \/\/ During full gc, heap->complete_marking_context() is not valid, may equal nullptr.\n+    ShenandoahMarkingContext* const ctx = heap->complete_marking_context();\n+    size_t idx = this->index();\n+    HeapWord* top_bitmap = ctx->top_bitmap(this);\n+\n+    assert(ctx->is_bitmap_clear_range(top_bitmap, _end),\n+           \"Region \" SIZE_FORMAT \", bitmap should be clear between top_bitmap: \" PTR_FORMAT \" and end: \" PTR_FORMAT, idx,\n+           p2i(top_bitmap), p2i(_end));\n+  }\n+#endif\n+\n+  if (region_affiliation == new_affiliation) {\n+    return;\n+  }\n+\n+  if (!heap->mode()->is_generational()) {\n+    heap->set_affiliation(this, new_affiliation);\n+    return;\n+  }\n+\n+  log_trace(gc)(\"Changing affiliation of region %zu from %s to %s\",\n+    index(), affiliation_name(region_affiliation), affiliation_name(new_affiliation));\n+\n+  if (region_affiliation == ShenandoahRegionAffiliation::YOUNG_GENERATION) {\n+    heap->young_generation()->decrement_affiliated_region_count();\n+  } else if (region_affiliation == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+    heap->old_generation()->decrement_affiliated_region_count();\n+  }\n+\n+  size_t regions;\n+  switch (new_affiliation) {\n+    case FREE:\n+      assert(!has_live(), \"Free region should not have live data\");\n+      break;\n+    case YOUNG_GENERATION:\n+      reset_age();\n+      regions = heap->young_generation()->increment_affiliated_region_count();\n+      \/\/ During Full GC, we allow temporary violation of this requirement.  We enforce that this condition is\n+      \/\/ restored upon completion of Full GC.\n+      assert(heap->is_full_gc_in_progress() ||\n+             (regions * ShenandoahHeapRegion::region_size_bytes() <= heap->young_generation()->adjusted_capacity()),\n+             \"Number of young regions cannot exceed adjusted capacity\");\n+      break;\n+    case OLD_GENERATION:\n+      regions = heap->old_generation()->increment_affiliated_region_count();\n+      \/\/ During Full GC, we allow temporary violation of this requirement.  We enforce that this condition is\n+      \/\/ restored upon completion of Full GC.\n+      assert(heap->is_full_gc_in_progress() ||\n+             (regions * ShenandoahHeapRegion::region_size_bytes() <= heap->old_generation()->adjusted_capacity()),\n+             \"Number of old regions cannot exceed adjusted capacity\");\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      return;\n+  }\n+  heap->set_affiliation(this, new_affiliation);\n+}\n+\n+\/\/ Returns number of regions promoted, or zero if we choose not to promote.\n+size_t ShenandoahHeapRegion::promote_humongous() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* marking_context = heap->marking_context();\n+  assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n+  assert(is_young(), \"Only young regions can be promoted\");\n+  assert(is_humongous_start(), \"Should not promote humongous continuation in isolation\");\n+  assert(age() >= InitialTenuringThreshold, \"Only promote regions that are sufficiently aged\");\n+\n+  ShenandoahGeneration* old_generation = heap->old_generation();\n+  ShenandoahGeneration* young_generation = heap->young_generation();\n+\n+  oop obj = cast_to_oop(bottom());\n+  assert(marking_context->is_marked(obj), \"promoted humongous object should be alive\");\n+\n+  \/\/ TODO: Consider not promoting humongous objects that represent primitive arrays.  Leaving a primitive array\n+  \/\/ (obj->is_typeArray()) in young-gen is harmless because these objects are never relocated and they are not\n+  \/\/ scanned.  Leaving primitive arrays in young-gen memory allows their memory to be reclaimed more quickly when\n+  \/\/ it becomes garbage.  Better to not make this change until sizes of young-gen and old-gen are completely\n+  \/\/ adaptive, as leaving primitive arrays in young-gen might be perceived as an \"astonishing result\" by someone\n+  \/\/ has carefully analyzed the required sizes of an application's young-gen and old-gen.\n+\n+  size_t spanned_regions = ShenandoahHeapRegion::required_regions(obj->size() * HeapWordSize);\n+  size_t index_limit = index() + spanned_regions;\n+\n+  {\n+    \/\/ We need to grab the heap lock in order to avoid a race when changing the affiliations of spanned_regions from\n+    \/\/ young to old.\n+    ShenandoahHeapLocker locker(heap->lock());\n+    size_t available_old_regions = old_generation->adjusted_unaffiliated_regions();\n+    if (spanned_regions <= available_old_regions) {\n+      log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", spanning \" SIZE_FORMAT, index(), spanned_regions);\n+\n+      \/\/ For this region and each humongous continuation region spanned by this humongous object, change\n+      \/\/ affiliation to OLD_GENERATION and adjust the generation-use tallies.  The remnant of memory\n+      \/\/ in the last humongous region that is not spanned by obj is currently not used.\n+      for (size_t i = index(); i < index_limit; i++) {\n+        ShenandoahHeapRegion* r = heap->get_region(i);\n+        log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", from \" PTR_FORMAT \" to \" PTR_FORMAT,\n+                      r->index(), p2i(r->bottom()), p2i(r->top()));\n+        \/\/ We mark the entire humongous object's range as dirty after loop terminates, so no need to dirty the range here\n+        r->set_affiliation(OLD_GENERATION);\n+        old_generation->increase_used(r->used());\n+        young_generation->decrease_used(r->used());\n+      }\n+      \/\/ Then fall through to finish the promotion after releasing the heap lock.\n+    } else {\n+      \/\/ There are not enough available old regions to promote this humongous region at this time, so defer promotion.\n+      \/\/ TODO: Consider allowing the promotion now, with the expectation that we can resize and\/or collect OLD\n+      \/\/ momentarily to address the transient violation of budgets.  Some problems that need to be addressed in order\n+      \/\/ to allow transient violation of capacity budgets are:\n+      \/\/  1. Various size_t subtractions assume usage is less than capacity, and thus assume there will be no\n+      \/\/     arithmetic underflow when we subtract usage from capacity.  The results of such size_t subtractions\n+      \/\/     would need to be guarded and special handling provided.\n+      \/\/  2. ShenandoahVerifier enforces that usage is less than capacity.  If we are going to relax this constraint,\n+      \/\/     we need to think about what conditions allow the constraint to be violated and document and implement the\n+      \/\/     changes.\n+      return 0;\n+    }\n+  }\n+\n+  \/\/ Since this region may have served previously as OLD, it may hold obsolete object range info.\n+  heap->card_scan()->reset_object_range(bottom(), bottom() + spanned_regions * ShenandoahHeapRegion::region_size_words());\n+  \/\/ Since the humongous region holds only one object, no lock is necessary for this register_object() invocation.\n+  heap->card_scan()->register_object_wo_lock(bottom());\n+\n+  if (obj->is_typeArray()) {\n+    \/\/ Primitive arrays don't need to be scanned.\n+    log_debug(gc)(\"Clean cards for promoted humongous object (Region \" SIZE_FORMAT \") from \" PTR_FORMAT \" to \" PTR_FORMAT,\n+                  index(), p2i(bottom()), p2i(bottom() + obj->size()));\n+    heap->card_scan()->mark_range_as_clean(bottom(), obj->size());\n+  } else {\n+    log_debug(gc)(\"Dirty cards for promoted humongous object (Region \" SIZE_FORMAT \") from \" PTR_FORMAT \" to \" PTR_FORMAT,\n+                  index(), p2i(bottom()), p2i(bottom() + obj->size()));\n+    heap->card_scan()->mark_range_as_dirty(bottom(), obj->size());\n+  }\n+  return index_limit - index();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":416,"deletions":18,"binary":false,"changes":434,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2013, 2020, Red Hat, Inc. All rights reserved.\n@@ -171,1 +171,2 @@\n-  void make_regular_allocation();\n+  void make_regular_allocation(ShenandoahRegionAffiliation affiliation);\n+  void make_young_maybe();\n@@ -175,2 +176,2 @@\n-  void make_humongous_start_bypass();\n-  void make_humongous_cont_bypass();\n+  void make_humongous_start_bypass(ShenandoahRegionAffiliation affiliation);\n+  void make_humongous_cont_bypass(ShenandoahRegionAffiliation affiliation);\n@@ -201,0 +202,2 @@\n+  inline bool is_young() const;\n+  inline bool is_old() const;\n@@ -213,0 +216,4 @@\n+  void clear_young_lab_flags();\n+  void set_young_lab_flag();\n+  bool has_young_lab_flag();\n+\n@@ -237,0 +244,1 @@\n+  HeapWord* _coalesce_and_fill_boundary; \/\/ for old regions not selected as collection set candidates.\n@@ -243,0 +251,3 @@\n+  size_t _plab_allocs;\n+\n+  bool _has_young_lab;\n@@ -249,0 +260,2 @@\n+  uint _age;\n+\n@@ -337,2 +350,5 @@\n-  \/\/ Allocation (return null if full)\n-  inline HeapWord* allocate(size_t word_size, ShenandoahAllocRequest::Type type);\n+  \/\/ Allocation (return nullptr if full)\n+  inline HeapWord* allocate_aligned(size_t word_size, ShenandoahAllocRequest &req, size_t alignment_in_words);\n+\n+  \/\/ Allocation (return nullptr if full)\n+  inline HeapWord* allocate(size_t word_size, ShenandoahAllocRequest req);\n@@ -359,1 +375,35 @@\n-  void oop_iterate(OopIterateClosure* cl);\n+  inline void begin_preemptible_coalesce_and_fill() {\n+    _coalesce_and_fill_boundary = _bottom;\n+  }\n+\n+  inline void end_preemptible_coalesce_and_fill() {\n+    _coalesce_and_fill_boundary = _end;\n+  }\n+\n+  inline void suspend_coalesce_and_fill(HeapWord* next_focus) {\n+    _coalesce_and_fill_boundary = next_focus;\n+  }\n+\n+  inline HeapWord* resume_coalesce_and_fill() {\n+    return _coalesce_and_fill_boundary;\n+  }\n+\n+  \/\/ Coalesce contiguous spans of garbage objects by filling header and reregistering start locations with remembered set.\n+  \/\/ This is used by old-gen GC following concurrent marking to make old-gen HeapRegions parseable.  Return true iff\n+  \/\/ region is completely coalesced and filled.  Returns false if cancelled before task is complete.\n+  bool oop_fill_and_coalesce();\n+\n+  \/\/ Like oop_fill_and_coalesce(), but without honoring cancellation requests.\n+  bool oop_fill_and_coalesce_wo_cancel();\n+\n+  \/\/ During global collections, this service iterates through an old-gen heap region that is not part of collection\n+  \/\/ set to fill and register ranges of dead memory.  Note that live objects were previously registered.  Some dead objects\n+  \/\/ that are subsumed into coalesced ranges of dead memory need to be \"unregistered\".\n+  void global_oop_iterate_and_fill_dead(OopIterateClosure* cl);\n+  void oop_iterate_humongous(OopIterateClosure* cl);\n+  void oop_iterate_humongous(OopIterateClosure* cl, HeapWord* start, size_t words);\n+\n+  \/\/ Invoke closure on every reference contained within the humongous object that spans this humongous\n+  \/\/ region if the reference is contained within a DIRTY card and the reference is no more than words following\n+  \/\/ start within the humongous object.\n+  void oop_iterate_humongous_slice(OopIterateClosure* cl, bool dirty_only, HeapWord* start, size_t words, bool write_table);\n@@ -381,0 +431,5 @@\n+  \/\/ Does this region contain this address?\n+  bool contains(HeapWord* p) const {\n+    return (bottom() <= p) && (p < top());\n+  }\n+\n@@ -386,0 +441,1 @@\n+  size_t get_plab_allocs() const;\n@@ -391,0 +447,12 @@\n+  inline ShenandoahRegionAffiliation affiliation() const;\n+\n+  void set_affiliation(ShenandoahRegionAffiliation new_affiliation);\n+\n+  uint age()           { return _age; }\n+  void increment_age() { _age++; }\n+  void decrement_age() { if (_age-- == 0) { _age = 0; } }\n+  void reset_age()     { _age = 0; }\n+\n+  \/\/ Sets all remembered set cards to dirty.  Returns the number of regions spanned by the associated humongous object.\n+  size_t promote_humongous();\n+\n@@ -395,2 +463,3 @@\n-  void oop_iterate_objects(OopIterateClosure* cl);\n-  void oop_iterate_humongous(OopIterateClosure* cl);\n+  \/\/ This is an old-region that was not part of the collection set during a GLOBAL collection.  We coalesce the dead\n+  \/\/ objects, but do not need to register the live objects as they are already registered.\n+  void global_oop_iterate_objects_and_fill_dead(OopIterateClosure* cl);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.hpp","additions":78,"deletions":9,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -34,1 +34,70 @@\n-HeapWord* ShenandoahHeapRegion::allocate(size_t size, ShenandoahAllocRequest::Type type) {\n+\/\/ If next available memory is not aligned on address that is multiple of alignment, fill the empty space\n+\/\/ so that returned object is aligned on an address that is a multiple of alignment_in_words.  Requested\n+\/\/ size is in words.  It is assumed that this->is_old().  A pad object is allocated, filled, and registered\n+\/\/ if necessary to assure the new allocation is properly aligned.\n+HeapWord* ShenandoahHeapRegion::allocate_aligned(size_t size, ShenandoahAllocRequest &req, size_t alignment_in_bytes) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  assert(req.is_lab_alloc(), \"allocate_aligned() only applies to LAB allocations\");\n+  assert(is_object_aligned(size), \"alloc size breaks alignment: \" SIZE_FORMAT, size);\n+  assert(is_old(), \"aligned allocations are only taken from OLD regions to support PLABs\");\n+\n+  HeapWord* orig_top = top();\n+  size_t addr_as_int = (uintptr_t) orig_top;\n+\n+  \/\/ unalignment_bytes is the amount by which current top() exceeds the desired alignment point.  We subtract this amount\n+  \/\/ from alignment_in_bytes to determine padding required to next alignment point.\n+\n+  \/\/ top is HeapWord-aligned so unalignment_bytes is a multiple of HeapWordSize\n+  size_t unalignment_bytes = addr_as_int % alignment_in_bytes;\n+  size_t unalignment_words = unalignment_bytes \/ HeapWordSize;\n+\n+  size_t pad_words;\n+  HeapWord* aligned_obj;\n+  if (unalignment_words > 0) {\n+    pad_words = (alignment_in_bytes \/ HeapWordSize) - unalignment_words;\n+    if (pad_words < ShenandoahHeap::min_fill_size()) {\n+      pad_words += (alignment_in_bytes \/ HeapWordSize);\n+    }\n+    aligned_obj = orig_top + pad_words;\n+  } else {\n+    pad_words = 0;\n+    aligned_obj = orig_top;\n+  }\n+\n+  if (pointer_delta(end(), aligned_obj) < size) {\n+    size = pointer_delta(end(), aligned_obj);\n+    \/\/ Force size to align on multiple of alignment_in_bytes\n+    size_t byte_size = size * HeapWordSize;\n+    size_t excess_bytes = byte_size % alignment_in_bytes;\n+    \/\/ Note: excess_bytes is a multiple of HeapWordSize because it is the difference of HeapWord-aligned end\n+    \/\/       and proposed HeapWord-aligned object address.\n+    if (excess_bytes > 0) {\n+      size -= excess_bytes \/ HeapWordSize;\n+    }\n+  }\n+\n+  \/\/ Both originally requested size and adjusted size must be properly aligned\n+  assert ((size * HeapWordSize) % alignment_in_bytes == 0, \"Size must be multiple of alignment constraint\");\n+  if (size >= req.min_size()) {\n+    \/\/ Even if req.min_size() is not a multiple of card size, we know that size is.\n+    if (pad_words > 0) {\n+      ShenandoahHeap::fill_with_object(orig_top, pad_words);\n+      ShenandoahHeap::heap()->card_scan()->register_object(orig_top);\n+    }\n+\n+    make_regular_allocation(req.affiliation());\n+    adjust_alloc_metadata(req.type(), size);\n+\n+    HeapWord* new_top = aligned_obj + size;\n+    assert(new_top <= end(), \"PLAB cannot span end of heap region\");\n+    set_top(new_top);\n+    req.set_actual_size(size);\n+    assert(is_object_aligned(new_top), \"new top breaks alignment: \" PTR_FORMAT, p2i(new_top));\n+    assert(is_aligned(aligned_obj, alignment_in_bytes), \"obj is not aligned: \" PTR_FORMAT, p2i(aligned_obj));\n+    return aligned_obj;\n+  } else {\n+    return nullptr;\n+  }\n+}\n+\n+HeapWord* ShenandoahHeapRegion::allocate(size_t size, ShenandoahAllocRequest req) {\n@@ -40,2 +109,2 @@\n-    make_regular_allocation();\n-    adjust_alloc_metadata(type, size);\n+    make_regular_allocation(req.affiliation());\n+    adjust_alloc_metadata(req.type(), size);\n@@ -67,0 +136,3 @@\n+    case ShenandoahAllocRequest::_alloc_plab:\n+      _plab_allocs += size;\n+      break;\n@@ -89,1 +161,2 @@\n-         \"can't have more live data than used: \" SIZE_FORMAT \", \" SIZE_FORMAT, live_bytes, used_bytes);\n+         \"%s Region \" SIZE_FORMAT \" can't have more live data than used: \" SIZE_FORMAT \", \" SIZE_FORMAT \" after adding \" SIZE_FORMAT,\n+         affiliation_name(affiliation()), index(), live_bytes, used_bytes, s * HeapWordSize);\n@@ -136,0 +209,24 @@\n+inline ShenandoahRegionAffiliation ShenandoahHeapRegion::affiliation() const {\n+  return ShenandoahHeap::heap()->region_affiliation(this);\n+}\n+\n+inline void ShenandoahHeapRegion::clear_young_lab_flags() {\n+  _has_young_lab = false;\n+}\n+\n+inline void ShenandoahHeapRegion::set_young_lab_flag() {\n+  _has_young_lab = true;\n+}\n+\n+inline bool ShenandoahHeapRegion::has_young_lab_flag() {\n+  return _has_young_lab;\n+}\n+\n+inline bool ShenandoahHeapRegion::is_young() const {\n+  return ShenandoahHeap::heap()->region_affiliation(this) == YOUNG_GENERATION;\n+}\n+\n+inline bool ShenandoahHeapRegion::is_old() const {\n+  return ShenandoahHeap::heap()->region_affiliation(this) == OLD_GENERATION;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.inline.hpp","additions":101,"deletions":4,"binary":false,"changes":105,"status":"modified"},{"patch":"@@ -24,1 +24,0 @@\n-\n@@ -26,0 +25,2 @@\n+\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"logging\/logStream.hpp\"\n@@ -33,0 +35,1 @@\n+#include \"utilities\/defaultStream.hpp\"\n@@ -44,1 +47,1 @@\n-    strcpy(_name_space, cns);\n+    strcpy(_name_space, cns); \/\/ copy cns into _name_space\n@@ -52,0 +55,3 @@\n+    cname = PerfDataManager::counter_name(_name_space, \"protocol_version\"); \/\/creating new protocol_version\n+    PerfDataManager::create_constant(SUN_GC, cname, PerfData::U_None, VERSION_NUMBER, CHECK);\n+\n@@ -60,0 +66,1 @@\n+    \/\/ Initializing performance data resources for each region\n@@ -69,0 +76,1 @@\n+\n@@ -76,0 +84,22 @@\n+void ShenandoahHeapRegionCounters::write_snapshot(PerfLongVariable** regions,\n+                                             PerfLongVariable* ts,\n+                                             PerfLongVariable* status,\n+                                             size_t num_regions,\n+                                             size_t region_size, size_t protocol_version) {\n+  LogTarget(Trace, gc, region) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+\n+    ls.print_cr(JLONG_FORMAT \" \" JLONG_FORMAT \" \" SIZE_FORMAT \" \" SIZE_FORMAT \" \" SIZE_FORMAT,\n+            ts->get_value(), status->get_value(), num_regions, region_size, protocol_version);\n+    if (num_regions > 0) {\n+      ls.print(JLONG_FORMAT, regions[0]->get_value());\n+    }\n+    for (uint i = 1; i < num_regions; ++i) {\n+      ls.print(\" \" JLONG_FORMAT, regions[i]->get_value());\n+    }\n+    ls.cr();\n+  }\n+}\n+\n@@ -80,2 +110,1 @@\n-    if (current - last > ShenandoahRegionSamplingRate &&\n-            Atomic::cmpxchg(&_last_sample_millis, last, current) == last) {\n+    if (current - last > ShenandoahRegionSamplingRate && Atomic::cmpxchg(&_last_sample_millis, last, current) == last) {\n@@ -84,6 +113,1 @@\n-      jlong status = 0;\n-      if (heap->is_concurrent_mark_in_progress())      status |= 1 << 0;\n-      if (heap->is_evacuation_in_progress())           status |= 1 << 1;\n-      if (heap->is_update_refs_in_progress())          status |= 1 << 2;\n-      _status->set_value(status);\n-\n+      _status->set_value(encode_heap_status(heap));\n@@ -92,2 +116,0 @@\n-      size_t num_regions = heap->num_regions();\n-\n@@ -97,0 +119,1 @@\n+        size_t num_regions = heap->num_regions();\n@@ -104,0 +127,1 @@\n+          data |= ((100 * r->get_plab_allocs() \/ rs)     & PERCENT_MASK) << PLAB_SHIFT;\n@@ -105,0 +129,3 @@\n+\n+          data |= (r->age() & AGE_MASK) << AGE_SHIFT;\n+          data |= (r->affiliation() & AFFILIATION_MASK) << AFFILIATION_SHIFT;\n@@ -108,0 +135,3 @@\n+\n+        \/\/ If logging enabled, dump current region snapshot to log file\n+        write_snapshot(_regions_data, _timestamp, _status, num_regions, rs >> 10, VERSION_NUMBER);\n@@ -109,0 +139,34 @@\n+    }\n+  }\n+}\n+\n+static int encode_phase(ShenandoahHeap* heap) {\n+  if (heap->is_evacuation_in_progress() || heap->is_full_gc_move_in_progress()) {\n+    return 2;\n+  }\n+  if (heap->is_update_refs_in_progress() || heap->is_full_gc_move_in_progress()) {\n+    return 3;\n+  }\n+  if (heap->is_concurrent_mark_in_progress() || heap->is_full_gc_in_progress()) {\n+    return 1;\n+  }\n+  assert(heap->is_idle(), \"What is it doing?\");\n+  return 0;\n+}\n+\n+static int get_generation_shift(ShenandoahGeneration* generation) {\n+  switch (generation->generation_mode()) {\n+    case GLOBAL: return 0;\n+    case OLD:    return 2;\n+    case YOUNG:  return 4;\n+    default:\n+      ShouldNotReachHere();\n+      return -1;\n+  }\n+}\n+\n+jlong ShenandoahHeapRegionCounters::encode_heap_status(ShenandoahHeap* heap) {\n+\n+  if (heap->is_idle() && !heap->is_full_gc_in_progress()) {\n+    return 0;\n+  }\n@@ -110,0 +174,11 @@\n+  jlong status = 0;\n+  if (!heap->mode()->is_generational()) {\n+    status = encode_phase(heap);\n+  } else {\n+    int phase = encode_phase(heap);\n+    ShenandoahGeneration* generation = heap->active_generation();\n+    assert(generation != nullptr, \"Expected active generation in this mode.\");\n+    int shift = get_generation_shift(generation);\n+    status |= ((phase & 0x3) << shift);\n+    if (heap->is_concurrent_old_mark_in_progress()) {\n+      status |= (1 << 2);\n@@ -111,0 +186,2 @@\n+    log_develop_trace(gc)(\"%s, phase=%u, old_mark=%s, status=\" JLONG_FORMAT,\n+                          generation->name(), phase, BOOL_TO_STR(heap->is_concurrent_old_mark_in_progress()), status);\n@@ -112,0 +189,9 @@\n+\n+  if (heap->is_degenerated_gc_in_progress()) {\n+    status |= (1 << 6);\n+  }\n+  if (heap->is_full_gc_in_progress()) {\n+    status |= (1 << 7);\n+  }\n+\n+  return status;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegionCounters.cpp","additions":98,"deletions":12,"binary":false,"changes":110,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"logging\/logFileStreamOutput.hpp\"\n@@ -40,3 +41,8 @@\n- *     - bit 0 set when marking in progress\n- *     - bit 1 set when evacuation in progress\n- *     - bit 2 set when update refs in progress\n+ *   | global | old   | young | mode |\n+ *   |  0..1  | 2..3  | 4..5  | 6..7 |\n+ *\n+ *   For each generation:\n+ *   0 = idle, 1 = marking, 2 = evacuating, 3 = updating refs\n+ *\n+ *   For mode:\n+ *   0 = concurrent, 1 = degenerated, 2 = full\n@@ -54,1 +60,1 @@\n- * - bits 35-41  <reserved>\n+ * - bits 35-41  plab allocated memory in percent\n@@ -56,1 +62,2 @@\n- * - bits 51-57  <reserved>\n+ * - bits 51-55  age\n+ * - bits 56-57  affiliation: 0 = free, young = 1, old = 2\n@@ -62,2 +69,4 @@\n-  static const jlong PERCENT_MASK = 0x7f;\n-  static const jlong STATUS_MASK  = 0x3f;\n+  static const jlong PERCENT_MASK      = 0x7f;\n+  static const jlong AGE_MASK          = 0x1f;\n+  static const jlong AFFILIATION_MASK  = 0x03;\n+  static const jlong STATUS_MASK       = 0x3f;\n@@ -65,5 +74,9 @@\n-  static const jlong USED_SHIFT   = 0;\n-  static const jlong LIVE_SHIFT   = 7;\n-  static const jlong TLAB_SHIFT   = 14;\n-  static const jlong GCLAB_SHIFT  = 21;\n-  static const jlong SHARED_SHIFT = 28;\n+  static const jlong USED_SHIFT        = 0;\n+  static const jlong LIVE_SHIFT        = 7;\n+  static const jlong TLAB_SHIFT        = 14;\n+  static const jlong GCLAB_SHIFT       = 21;\n+  static const jlong SHARED_SHIFT      = 28;\n+  static const jlong PLAB_SHIFT        = 35;\n+  static const jlong AGE_SHIFT         = 51;\n+  static const jlong AFFILIATION_SHIFT = 56;\n+  static const jlong STATUS_SHIFT      = 58;\n@@ -71,1 +84,1 @@\n-  static const jlong STATUS_SHIFT = 58;\n+  static const jlong VERSION_NUMBER    = 2;\n@@ -79,0 +92,7 @@\n+  void write_snapshot(PerfLongVariable** regions,\n+                      PerfLongVariable* ts,\n+                      PerfLongVariable* status,\n+                      size_t num_regions,\n+                      size_t region_size, size_t protocolVersion);\n+\n+  uint _count = 0;\n@@ -83,0 +103,3 @@\n+\n+private:\n+  static jlong encode_heap_status(ShenandoahHeap* heap) ;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegionCounters.hpp","additions":36,"deletions":13,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -43,2 +45,20 @@\n-  log_info(gc, init)(\"Heuristics: %s\",\n-                     heap->heuristics()->name());\n+  if (!heap->mode()->is_generational()) {\n+    log_info(gc, init)(\"Heuristics: %s\", heap->global_generation()->heuristics()->name());\n+  } else {\n+    log_info(gc, init)(\"Young Heuristics: %s\", heap->young_generation()->heuristics()->name());\n+    log_info(gc, init)(\"Young Generation Initial Size: \" SIZE_FORMAT \"%s\",\n+                       byte_size_in_proper_unit(heap->young_generation()->soft_max_capacity()),\n+                       proper_unit_for_byte_size(heap->young_generation()->soft_max_capacity()));\n+    log_info(gc, init)(\"Young Generation Max: \" SIZE_FORMAT \"%s\",\n+                       byte_size_in_proper_unit(heap->young_generation()->max_capacity()),\n+                       proper_unit_for_byte_size(heap->young_generation()->max_capacity()));\n+    log_info(gc, init)(\"Old Heuristics: %s\", heap->old_generation()->heuristics()->name());\n+    log_info(gc, init)(\"Old Generation Initial Size: \" SIZE_FORMAT \"%s\",\n+                       byte_size_in_proper_unit(heap->old_generation()->soft_max_capacity()),\n+                       proper_unit_for_byte_size(heap->old_generation()->soft_max_capacity()));\n+    log_info(gc, init)(\"Old Generation Max: \" SIZE_FORMAT \"%s\",\n+                       byte_size_in_proper_unit(heap->old_generation()->max_capacity()),\n+                       proper_unit_for_byte_size(heap->old_generation()->max_capacity()));\n+  }\n+\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahInitLogger.cpp","additions":22,"deletions":2,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -37,11 +38,0 @@\n-ShenandoahMarkRefsSuperClosure::ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q,  ShenandoahReferenceProcessor* rp) :\n-  MetadataVisitingOopIterateClosure(rp),\n-  _queue(q),\n-  _mark_context(ShenandoahHeap::heap()->marking_context()),\n-  _weak(false)\n-{ }\n-\n-ShenandoahMark::ShenandoahMark() :\n-  _task_queues(ShenandoahHeap::heap()->marking_context()->task_queues()) {\n-}\n-\n@@ -57,1 +47,3 @@\n-  CodeCache::on_gc_marking_cycle_finish();\n+  if (!ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress()) {\n+    CodeCache::on_gc_marking_cycle_finish();\n+  }\n@@ -60,4 +52,7 @@\n-void ShenandoahMark::clear() {\n-  \/\/ Clean up marking stacks.\n-  ShenandoahObjToScanQueueSet* queues = ShenandoahHeap::heap()->marking_context()->task_queues();\n-  queues->clear();\n+ShenandoahMarkRefsSuperClosure::ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q,  ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old_q) :\n+  MetadataVisitingOopIterateClosure(rp),\n+  _queue(q),\n+  _old_queue(old_q),\n+  _mark_context(ShenandoahHeap::heap()->marking_context()),\n+  _weak(false)\n+{ }\n@@ -65,2 +60,4 @@\n-  \/\/ Cancel SATB buffers.\n-  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+ShenandoahMark::ShenandoahMark(ShenandoahGeneration* generation) :\n+  _generation(generation),\n+  _task_queues(generation->task_queues()),\n+  _old_gen_task_queues(generation->old_gen_task_queues()) {\n@@ -69,2 +66,2 @@\n-template <bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n-void ShenandoahMark::mark_loop_prework(uint w, TaskTerminator *t, ShenandoahReferenceProcessor *rp, StringDedup::Requests* const req) {\n+template <GenerationMode GENERATION, bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n+void ShenandoahMark::mark_loop_prework(uint w, TaskTerminator *t, ShenandoahReferenceProcessor *rp, StringDedup::Requests* const req, bool update_refs) {\n@@ -72,0 +69,1 @@\n+  ShenandoahObjToScanQueue* old = get_old_queue(w);\n@@ -78,4 +76,4 @@\n-  if (heap->has_forwarded_objects()) {\n-    using Closure = ShenandoahMarkUpdateRefsClosure;\n-    Closure cl(q, rp);\n-    mark_loop_work<Closure, CANCELLABLE, STRING_DEDUP>(&cl, ld, w, t, req);\n+  if (update_refs) {\n+    using Closure = ShenandoahMarkUpdateRefsClosure<GENERATION>;\n+    Closure cl(q, rp, old);\n+    mark_loop_work<Closure, GENERATION, CANCELLABLE, STRING_DEDUP>(&cl, ld, w, t, req);\n@@ -83,3 +81,3 @@\n-    using Closure = ShenandoahMarkRefsClosure;\n-    Closure cl(q, rp);\n-    mark_loop_work<Closure, CANCELLABLE, STRING_DEDUP>(&cl, ld, w, t, req);\n+    using Closure = ShenandoahMarkRefsClosure<GENERATION>;\n+    Closure cl(q, rp, old);\n+    mark_loop_work<Closure, GENERATION, CANCELLABLE, STRING_DEDUP>(&cl, ld, w, t, req);\n@@ -91,1 +89,21 @@\n-void ShenandoahMark::mark_loop(uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp,\n+template<bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n+void ShenandoahMark::mark_loop(GenerationMode generation, uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp, StringDedup::Requests* const req) {\n+  bool update_refs = ShenandoahHeap::heap()->has_forwarded_objects();\n+  switch (generation) {\n+    case YOUNG:\n+      mark_loop_prework<YOUNG, CANCELLABLE, STRING_DEDUP>(worker_id, terminator, rp, req, update_refs);\n+      break;\n+    case OLD:\n+      \/\/ Old generation collection only performs marking, it should not update references.\n+      mark_loop_prework<OLD, CANCELLABLE, STRING_DEDUP>(worker_id, terminator, rp, req, false);\n+      break;\n+    case GLOBAL:\n+      mark_loop_prework<GLOBAL, CANCELLABLE, STRING_DEDUP>(worker_id, terminator, rp, req, update_refs);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+  }\n+}\n+\n+void ShenandoahMark::mark_loop(GenerationMode generation, uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp,\n@@ -96,1 +114,1 @@\n-        mark_loop_prework<true, NO_DEDUP>(worker_id, terminator, rp, req);\n+        mark_loop<true, NO_DEDUP>(generation, worker_id, terminator, rp, req);\n@@ -99,1 +117,1 @@\n-        mark_loop_prework<true, ENQUEUE_DEDUP>(worker_id, terminator, rp, req);\n+        mark_loop<true, ENQUEUE_DEDUP>(generation, worker_id, terminator, rp, req);\n@@ -102,1 +120,1 @@\n-        mark_loop_prework<true, ALWAYS_DEDUP>(worker_id, terminator, rp, req);\n+        mark_loop<true, ALWAYS_DEDUP>(generation, worker_id, terminator, rp, req);\n@@ -108,1 +126,1 @@\n-        mark_loop_prework<false, NO_DEDUP>(worker_id, terminator, rp, req);\n+        mark_loop<false, NO_DEDUP>(generation, worker_id, terminator, rp, req);\n@@ -111,1 +129,1 @@\n-        mark_loop_prework<false, ENQUEUE_DEDUP>(worker_id, terminator, rp, req);\n+        mark_loop<false, ENQUEUE_DEDUP>(generation, worker_id, terminator, rp, req);\n@@ -114,1 +132,1 @@\n-        mark_loop_prework<false, ALWAYS_DEDUP>(worker_id, terminator, rp, req);\n+        mark_loop<false, ALWAYS_DEDUP>(generation, worker_id, terminator, rp, req);\n@@ -120,1 +138,1 @@\n-template <class T, bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n+template <class T, GenerationMode GENERATION, bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n@@ -129,1 +147,2 @@\n-  heap->ref_processor()->set_mark_closure(worker_id, cl);\n+  assert(heap->active_generation()->generation_mode() == GENERATION, \"Sanity\");\n+  heap->active_generation()->ref_processor()->set_mark_closure(worker_id, cl);\n@@ -158,0 +177,1 @@\n+  ShenandoahObjToScanQueue* old = get_old_queue(worker_id);\n@@ -159,1 +179,1 @@\n-  ShenandoahSATBBufferClosure drain_satb(q);\n+  ShenandoahSATBBufferClosure<GENERATION> drain_satb(q, old);\n@@ -169,1 +189,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.cpp","additions":57,"deletions":38,"binary":false,"changes":95,"status":"modified"},{"patch":"@@ -33,2 +33,0 @@\n-class ShenandoahCMDrainMarkingStackClosure;\n-\n@@ -39,1 +37,0 @@\n-  friend class ShenandoahCMDrainMarkingStackClosure;\n@@ -42,0 +39,1 @@\n+  ShenandoahGeneration* const _generation;\n@@ -43,0 +41,1 @@\n+  ShenandoahObjToScanQueueSet* const _old_gen_task_queues;\n@@ -45,1 +44,1 @@\n-  ShenandoahMark();\n+  ShenandoahMark(ShenandoahGeneration* generation);\n@@ -48,4 +47,2 @@\n-  template<class T>\n-  static inline void mark_through_ref(T* p, ShenandoahObjToScanQueue* q, ShenandoahMarkingContext* const mark_context, bool weak);\n-\n-  static void clear();\n+  template<class T, GenerationMode GENERATION>\n+  static inline void mark_through_ref(T* p, ShenandoahObjToScanQueue* q, ShenandoahObjToScanQueue* old, ShenandoahMarkingContext* const mark_context, bool weak);\n@@ -59,0 +56,4 @@\n+  ShenandoahObjToScanQueueSet* old_task_queues() {\n+    return _old_gen_task_queues;\n+  }\n+\n@@ -60,0 +61,3 @@\n+  inline ShenandoahObjToScanQueue* get_old_queue(uint index) const;\n+\n+  inline ShenandoahGeneration* generation() { return _generation; };\n@@ -74,1 +78,1 @@\n-  template <class T, bool CANCELLABLE,StringDedupMode STRING_DEDUP>\n+  template <class T, GenerationMode GENERATION, bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n@@ -77,2 +81,9 @@\n-  template <bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n-  void mark_loop_prework(uint worker_id, TaskTerminator *terminator, ShenandoahReferenceProcessor *rp, StringDedup::Requests* const req);\n+  template <GenerationMode GENERATION, bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n+  void mark_loop_prework(uint worker_id, TaskTerminator *terminator, ShenandoahReferenceProcessor *rp, StringDedup::Requests* const req, bool update_refs);\n+\n+  template <GenerationMode GENERATION>\n+  static bool in_generation(oop obj);\n+\n+  static void mark_ref(ShenandoahObjToScanQueue* q,\n+                       ShenandoahMarkingContext* const mark_context,\n+                       bool weak, oop obj);\n@@ -83,1 +94,5 @@\n-  void mark_loop(uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp,\n+  template<bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n+  void mark_loop(GenerationMode generation, uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp,\n+                 StringDedup::Requests* const req);\n+\n+  void mark_loop(GenerationMode generation, uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.hpp","additions":27,"deletions":12,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -63,0 +63,4 @@\n+  \/\/ TODO: This will push array chunks into the mark queue with no regard for\n+  \/\/ generations. I don't think it will break anything, but the young generation\n+  \/\/ scan might end up processing some old generation array chunks.\n+\n@@ -113,0 +117,1 @@\n+    assert(region->affiliation() != FREE, \"Do not count live data within Free Regular Region \" SIZE_FORMAT, region_idx);\n@@ -127,0 +132,1 @@\n+    assert(region->affiliation() != FREE, \"Do not count live data within FREE Humongous Start Region \" SIZE_FORMAT, region_idx);\n@@ -130,0 +136,1 @@\n+      assert(chain_reg->affiliation() != FREE, \"Do not count live data within FREE Humongous Continuation Region \" SIZE_FORMAT, i);\n@@ -232,0 +239,1 @@\n+template <GenerationMode GENERATION>\n@@ -235,0 +243,1 @@\n+  ShenandoahObjToScanQueue* _old;\n@@ -238,1 +247,1 @@\n-  ShenandoahSATBBufferClosure(ShenandoahObjToScanQueue* q) :\n+  ShenandoahSATBBufferClosure(ShenandoahObjToScanQueue* q, ShenandoahObjToScanQueue* old) :\n@@ -240,0 +249,1 @@\n+    _old(old),\n@@ -246,1 +256,1 @@\n-    assert(size == 0 || !_heap->has_forwarded_objects(), \"Forwarded objects are not expected here\");\n+    assert(size == 0 || !_heap->has_forwarded_objects() || _heap->is_concurrent_old_mark_in_progress(), \"Forwarded objects are not expected here\");\n@@ -249,1 +259,1 @@\n-      ShenandoahMark::mark_through_ref<oop>(p, _queue, _mark_context, false);\n+      ShenandoahMark::mark_through_ref<oop, GENERATION>(p, _queue, _old, _mark_context, false);\n@@ -254,2 +264,15 @@\n-template<class T>\n-inline void ShenandoahMark::mark_through_ref(T* p, ShenandoahObjToScanQueue* q, ShenandoahMarkingContext* const mark_context, bool weak) {\n+template<GenerationMode GENERATION>\n+bool ShenandoahMark::in_generation(oop obj) {\n+  \/\/ Each in-line expansion of in_generation() resolves GENERATION at compile time.\n+  if (GENERATION == YOUNG)\n+    return ShenandoahHeap::heap()->is_in_young(obj);\n+  else if (GENERATION == OLD)\n+    return ShenandoahHeap::heap()->is_in_old(obj);\n+  else if (GENERATION == GLOBAL)\n+    return true;\n+  else\n+    return false;\n+}\n+\n+template<class T, GenerationMode GENERATION>\n+inline void ShenandoahMark::mark_through_ref(T *p, ShenandoahObjToScanQueue* q, ShenandoahObjToScanQueue* old, ShenandoahMarkingContext* const mark_context, bool weak) {\n@@ -260,0 +283,1 @@\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -261,12 +285,30 @@\n-    shenandoah_assert_not_in_cset_except(p, obj, ShenandoahHeap::heap()->cancelled_gc());\n-\n-    bool skip_live = false;\n-    bool marked;\n-    if (weak) {\n-      marked = mark_context->mark_weak(obj);\n-    } else {\n-      marked = mark_context->mark_strong(obj, \/* was_upgraded = *\/ skip_live);\n-    }\n-    if (marked) {\n-      bool pushed = q->push(ShenandoahMarkTask(obj, skip_live, weak));\n-      assert(pushed, \"overflow queue should always succeed pushing\");\n+    shenandoah_assert_not_in_cset_except(p, obj, heap->cancelled_gc());\n+    if (in_generation<GENERATION>(obj)) {\n+      mark_ref(q, mark_context, weak, obj);\n+      shenandoah_assert_marked(p, obj);\n+      if (heap->mode()->is_generational()) {\n+        \/\/ TODO: As implemented herein, GLOBAL collections reconstruct the card table during GLOBAL concurrent\n+        \/\/ marking. Note that the card table is cleaned at init_mark time so it needs to be reconstructed to support\n+        \/\/ future young-gen collections.  It might be better to reconstruct card table in\n+        \/\/ ShenandoahHeapRegion::global_oop_iterate_and_fill_dead.  We could either mark all live memory as dirty, or could\n+        \/\/ use the GLOBAL update-refs scanning of pointers to determine precisely which cards to flag as dirty.\n+        if (GENERATION == YOUNG && heap->is_in_old(p)) {\n+          \/\/ Mark card as dirty because remembered set scanning still finds interesting pointer.\n+          heap->mark_card_as_dirty((HeapWord*)p);\n+        } else if (GENERATION == GLOBAL && heap->is_in_old(p) && heap->is_in_young(obj)) {\n+          \/\/ Mark card as dirty because GLOBAL marking finds interesting pointer.\n+          heap->mark_card_as_dirty((HeapWord*)p);\n+        }\n+      }\n+    } else if (old != nullptr) {\n+      \/\/ Young mark, bootstrapping old or concurrent with old marking.\n+      mark_ref(old, mark_context, weak, obj);\n+      shenandoah_assert_marked(p, obj);\n+    } else if (GENERATION == OLD) {\n+      \/\/ Old mark, found a young pointer.\n+      \/\/ TODO:  Rethink this: may be redundant with dirtying of cards identified during young-gen remembered set scanning\n+      \/\/ and by mutator write barriers.  Assert\n+      if (heap->is_in(p)) {\n+        assert(heap->is_in_young(obj), \"Expected young object.\");\n+        heap->mark_card_as_dirty(p);\n+      }\n@@ -274,0 +316,2 @@\n+  }\n+}\n@@ -275,1 +319,13 @@\n-    shenandoah_assert_marked(p, obj);\n+inline void ShenandoahMark::mark_ref(ShenandoahObjToScanQueue* q,\n+                              ShenandoahMarkingContext* const mark_context,\n+                              bool weak, oop obj) {\n+  bool skip_live = false;\n+  bool marked;\n+  if (weak) {\n+    marked = mark_context->mark_weak(obj);\n+  } else {\n+    marked = mark_context->mark_strong(obj, \/* was_upgraded = *\/ skip_live);\n+  }\n+  if (marked) {\n+    bool pushed = q->push(ShenandoahMarkTask(obj, skip_live, weak));\n+    assert(pushed, \"overflow queue should always succeed pushing\");\n@@ -286,0 +342,8 @@\n+\n+ShenandoahObjToScanQueue* ShenandoahMark::get_old_queue(uint index) const {\n+  if (_old_gen_task_queues != nullptr) {\n+    return _old_gen_task_queues->queue(index);\n+  }\n+  return nullptr;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.inline.hpp","additions":82,"deletions":18,"binary":false,"changes":100,"status":"modified"},{"patch":"@@ -46,0 +46,14 @@\n+bool ShenandoahMarkBitMap::is_bitmap_clear_range(const HeapWord* start, const HeapWord* end) const {\n+  \/\/ Similar to get_next_marked_addr(), without assertion.\n+  \/\/ Round addr up to a possible object boundary to be safe.\n+  if (start == end) {\n+    return true;\n+  }\n+  size_t const addr_offset = address_to_index(align_up(start, HeapWordSize << LogMinObjAlignment));\n+  size_t const limit_offset = address_to_index(end);\n+  size_t const next_offset = get_next_one_offset(addr_offset, limit_offset);\n+  HeapWord* result = index_to_address(next_offset);\n+  return (result == end);\n+}\n+\n+\n@@ -48,0 +62,5 @@\n+#ifdef ASSERT\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahHeapRegion* r = heap->heap_region_containing(addr);\n+  ShenandoahMarkingContext* ctx = heap->marking_context();\n+  HeapWord* tams = ctx->top_at_mark_start(r);\n@@ -49,0 +68,4 @@\n+  assert(limit <= r->top(), \"limit must be less than top\");\n+  assert(addr <= tams, \"addr must be less than TAMS\");\n+#endif\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkBitMap.cpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -162,0 +162,2 @@\n+  bool is_bitmap_clear_range(const HeapWord* start, const HeapWord* end) const;\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkBitMap.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,78 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMarkClosures.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMarkingContext.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n+\n+\n+ShenandoahFinalMarkUpdateRegionStateClosure::ShenandoahFinalMarkUpdateRegionStateClosure(\n+  ShenandoahMarkingContext *ctx) :\n+  _ctx(ctx), _lock(ShenandoahHeap::heap()->lock()) {}\n+\n+void ShenandoahFinalMarkUpdateRegionStateClosure::heap_region_do(ShenandoahHeapRegion* r) {\n+  if (r->is_active()) {\n+    if (_ctx != nullptr) {\n+      \/\/ _ctx may be null when this closure is used to sync only the pin status\n+      \/\/ update the watermark of old regions. For old regions we cannot reset\n+      \/\/ the TAMS because we rely on that to keep promoted objects alive after\n+      \/\/ old marking is complete.\n+\n+      \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n+      \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n+      HeapWord *tams = _ctx->top_at_mark_start(r);\n+      HeapWord *top = r->top();\n+      if (top > tams) {\n+        r->increase_live_data_alloc_words(pointer_delta(top, tams));\n+      }\n+    }\n+\n+    \/\/ We are about to select the collection set, make sure it knows about\n+    \/\/ current pinning status. Also, this allows trashing more regions that\n+    \/\/ now have their pinning status dropped.\n+    if (r->is_pinned()) {\n+      if (r->pin_count() == 0) {\n+        ShenandoahHeapLocker locker(_lock);\n+        r->make_unpinned();\n+      }\n+    } else {\n+      if (r->pin_count() > 0) {\n+        ShenandoahHeapLocker locker(_lock);\n+        r->make_pinned();\n+      }\n+    }\n+\n+    \/\/ Remember limit for updating refs. It's guaranteed that we get no\n+    \/\/ from-space-refs written from here on.\n+    r->set_update_watermark_at_safepoint(r->top());\n+  } else {\n+    assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n+    assert(_ctx == nullptr || _ctx->top_at_mark_start(r) == r->top(),\n+             \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkClosures.cpp","additions":78,"deletions":0,"binary":false,"changes":78,"status":"added"},{"patch":"@@ -0,0 +1,45 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHMARKCLOSURES_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHMARKCLOSURES_HPP\n+\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+\n+class ShenandoahMarkingContext;\n+class ShenandoahHeapRegion;\n+\n+class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n+private:\n+  ShenandoahMarkingContext* const _ctx;\n+  ShenandoahHeapLock* const _lock;\n+public:\n+  explicit ShenandoahFinalMarkUpdateRegionStateClosure(ShenandoahMarkingContext* ctx);\n+\n+  void heap_region_do(ShenandoahHeapRegion* r);\n+\n+  bool is_thread_safe() { return true; }\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHMARKCLOSURES_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkClosures.hpp","additions":45,"deletions":0,"binary":false,"changes":45,"status":"added"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n@@ -30,2 +29,0 @@\n-#include \"gc\/shenandoah\/shenandoahTaskqueue.inline.hpp\"\n-#include \"utilities\/stack.inline.hpp\"\n@@ -33,1 +30,1 @@\n-ShenandoahMarkingContext::ShenandoahMarkingContext(MemRegion heap_region, MemRegion bitmap_region, size_t num_regions, uint max_queues) :\n+ShenandoahMarkingContext::ShenandoahMarkingContext(MemRegion heap_region, MemRegion bitmap_region, size_t num_regions) :\n@@ -38,15 +35,1 @@\n-                      ((uintx) heap_region.start() >> ShenandoahHeapRegion::region_size_bytes_shift())),\n-  _task_queues(new ShenandoahObjToScanQueueSet(max_queues)) {\n-  assert(max_queues > 0, \"At least one queue\");\n-  for (uint i = 0; i < max_queues; ++i) {\n-    ShenandoahObjToScanQueue* task_queue = new ShenandoahObjToScanQueue();\n-    _task_queues->register_queue(i, task_queue);\n-  }\n-}\n-\n-ShenandoahMarkingContext::~ShenandoahMarkingContext() {\n-  for (uint i = 0; i < _task_queues->size(); ++i) {\n-    ShenandoahObjToScanQueue* q = _task_queues->queue(i);\n-    delete q;\n-  }\n-  delete _task_queues;\n+                      ((uintx) heap_region.start() >> ShenandoahHeapRegion::region_size_bytes_shift())) {\n@@ -60,1 +43,1 @@\n-    if (heap->is_bitmap_slice_committed(r) && !is_bitmap_clear_range(r->bottom(), r->end())) {\n+    if ((r->affiliation() != FREE) && heap->is_bitmap_slice_committed(r) && !is_bitmap_clear_range(r->bottom(), r->end())) {\n@@ -67,2 +50,13 @@\n-bool ShenandoahMarkingContext::is_bitmap_clear_range(HeapWord* start, HeapWord* end) const {\n-  return _mark_bit_map.get_next_marked_addr(start, end) == end;\n+bool ShenandoahMarkingContext::is_bitmap_clear_range(const HeapWord* start, const HeapWord* end) const {\n+  if (start < end) {\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    size_t start_idx = heap->heap_region_index_containing(start);\n+    size_t end_idx = heap->heap_region_index_containing(end - 1);\n+    while (start_idx <= end_idx) {\n+      ShenandoahHeapRegion* r = heap->get_region(start_idx);\n+      if (!heap->is_bitmap_slice_committed(r))\n+        return true;\n+      start_idx++;\n+    }\n+  }\n+  return _mark_bit_map.is_bitmap_clear_range(start, end);\n@@ -74,0 +68,1 @@\n+\n@@ -76,0 +71,7 @@\n+\n+  log_debug(gc)(\"SMC:initialize_top_at_mark_start for Region \" SIZE_FORMAT \", TAMS: \" PTR_FORMAT \", TopOfBitMap: \" PTR_FORMAT,\n+                r->index(), p2i(bottom), p2i(r->end()));\n+}\n+\n+HeapWord* ShenandoahMarkingContext::top_bitmap(ShenandoahHeapRegion* r) {\n+  return _top_bitmaps[r->index()];\n@@ -81,3 +83,12 @@\n-  if (top_bitmap > bottom) {\n-    _mark_bit_map.clear_range_large(MemRegion(bottom, top_bitmap));\n-    _top_bitmaps[r->index()] = bottom;\n+\n+  log_debug(gc)(\"SMC:clear_bitmap for %s Region \" SIZE_FORMAT \", top_bitmap: \" PTR_FORMAT,\n+                affiliation_name(r->affiliation()), r->index(), p2i(top_bitmap));\n+\n+  if (r->affiliation() != FREE) {\n+    if (top_bitmap > bottom) {\n+      _mark_bit_map.clear_range_large(MemRegion(bottom, top_bitmap));\n+      _top_bitmaps[r->index()] = bottom;\n+    }\n+    r->clear_live_data();\n+    assert(is_bitmap_clear_range(bottom, r->end()),\n+           \"Region \" SIZE_FORMAT \" should have no marks in bitmap\", r->index());\n@@ -85,2 +96,2 @@\n-  assert(is_bitmap_clear_range(bottom, r->end()),\n-         \"Region \" SIZE_FORMAT \" should have no marks in bitmap\", r->index());\n+  \/\/ heap iterators include FREE regions, which don't need to be cleared.\n+  \/\/ TODO: would be better for certain iterators to not include FREE regions.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.cpp","additions":38,"deletions":27,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -50,3 +50,0 @@\n-  \/\/ Marking task queues\n-  ShenandoahObjToScanQueueSet* _task_queues;\n-\n@@ -54,2 +51,1 @@\n-  ShenandoahMarkingContext(MemRegion heap_region, MemRegion bitmap_region, size_t num_regions, uint max_queues);\n-  ~ShenandoahMarkingContext();\n+  ShenandoahMarkingContext(MemRegion heap_region, MemRegion bitmap_region, size_t num_regions);\n@@ -66,3 +62,5 @@\n-  inline bool is_marked(oop) const;\n-  inline bool is_marked_strong(oop obj) const;\n-  inline bool is_marked_weak(oop obj) const;\n+  inline bool is_marked(const oop) const;\n+  inline bool is_marked_strong(const oop obj) const;\n+  inline bool is_marked_weak(const oop obj) const;\n+  inline bool is_marked_or_old(const oop obj) const;\n+  inline bool is_marked_strong_or_old(const oop obj) const;\n@@ -70,1 +68,1 @@\n-  inline HeapWord* get_next_marked_addr(HeapWord* addr, HeapWord* limit) const;\n+  inline HeapWord* get_next_marked_addr(const HeapWord* addr, const HeapWord* limit) const;\n@@ -72,2 +70,2 @@\n-  inline bool allocated_after_mark_start(oop obj) const;\n-  inline bool allocated_after_mark_start(HeapWord* addr) const;\n+  inline bool allocated_after_mark_start(const oop obj) const;\n+  inline bool allocated_after_mark_start(const HeapWord* addr) const;\n@@ -75,1 +73,1 @@\n-  inline HeapWord* top_at_mark_start(ShenandoahHeapRegion* r) const;\n+  inline HeapWord* top_at_mark_start(const ShenandoahHeapRegion* r) const;\n@@ -80,0 +78,2 @@\n+  HeapWord* top_bitmap(ShenandoahHeapRegion* r);\n+\n@@ -84,1 +84,1 @@\n-  bool is_bitmap_clear_range(HeapWord* start, HeapWord* end) const;\n+  bool is_bitmap_clear_range(const HeapWord* start, const HeapWord* end) const;\n@@ -89,3 +89,0 @@\n-\n-  \/\/ Task queues\n-  ShenandoahObjToScanQueueSet* task_queues() const { return _task_queues; }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.hpp","additions":13,"deletions":16,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-\n@@ -31,0 +30,1 @@\n+#include \"logging\/log.hpp\"\n@@ -40,1 +40,1 @@\n-inline bool ShenandoahMarkingContext::is_marked(oop obj) const {\n+inline bool ShenandoahMarkingContext::is_marked(const oop obj) const {\n@@ -44,1 +44,1 @@\n-inline bool ShenandoahMarkingContext::is_marked_strong(oop obj) const {\n+inline bool ShenandoahMarkingContext::is_marked_strong(const oop obj) const {\n@@ -48,1 +48,1 @@\n-inline bool ShenandoahMarkingContext::is_marked_weak(oop obj) const {\n+inline bool ShenandoahMarkingContext::is_marked_weak(const oop obj) const {\n@@ -52,1 +52,9 @@\n-inline HeapWord* ShenandoahMarkingContext::get_next_marked_addr(HeapWord* start, HeapWord* limit) const {\n+inline bool ShenandoahMarkingContext::is_marked_or_old(const oop obj) const {\n+  return is_marked(obj) || ShenandoahHeap::heap()->is_old(obj);\n+}\n+\n+inline bool ShenandoahMarkingContext::is_marked_strong_or_old(const oop obj) const {\n+  return is_marked_strong(obj) || ShenandoahHeap::heap()->is_old(obj);\n+}\n+\n+inline HeapWord* ShenandoahMarkingContext::get_next_marked_addr(const HeapWord* start, const HeapWord* limit) const {\n@@ -56,2 +64,2 @@\n-inline bool ShenandoahMarkingContext::allocated_after_mark_start(oop obj) const {\n-  HeapWord* addr = cast_from_oop<HeapWord*>(obj);\n+inline bool ShenandoahMarkingContext::allocated_after_mark_start(const oop obj) const {\n+  const HeapWord* addr = cast_from_oop<HeapWord*>(obj);\n@@ -61,1 +69,1 @@\n-inline bool ShenandoahMarkingContext::allocated_after_mark_start(HeapWord* addr) const {\n+inline bool ShenandoahMarkingContext::allocated_after_mark_start(const HeapWord* addr) const {\n@@ -64,1 +72,1 @@\n-  bool alloc_after_mark_start = addr >= top_at_mark_start;\n+  const bool alloc_after_mark_start = addr >= top_at_mark_start;\n@@ -69,13 +77,32 @@\n-  size_t idx = r->index();\n-  HeapWord* old_tams = _top_at_mark_starts_base[idx];\n-  HeapWord* new_tams = r->top();\n-\n-  assert(new_tams >= old_tams,\n-         \"Region \" SIZE_FORMAT\", TAMS updates should be monotonic: \" PTR_FORMAT \" -> \" PTR_FORMAT,\n-         idx, p2i(old_tams), p2i(new_tams));\n-  assert(is_bitmap_clear_range(old_tams, new_tams),\n-         \"Region \" SIZE_FORMAT \", bitmap should be clear while adjusting TAMS: \" PTR_FORMAT \" -> \" PTR_FORMAT,\n-         idx, p2i(old_tams), p2i(new_tams));\n-\n-  _top_at_mark_starts_base[idx] = new_tams;\n-  _top_bitmaps[idx] = new_tams;\n+  if (r->affiliation() != FREE) {\n+    size_t idx = r->index();\n+    HeapWord* old_tams = _top_at_mark_starts_base[idx];\n+    HeapWord* new_tams = r->top();\n+\n+    assert(new_tams >= old_tams,\n+           \"Region \" SIZE_FORMAT\", TAMS updates should be monotonic: \" PTR_FORMAT \" -> \" PTR_FORMAT,\n+           idx, p2i(old_tams), p2i(new_tams));\n+    assert((new_tams == r->bottom()) || (old_tams == r->bottom()) || (new_tams >= _top_bitmaps[idx]),\n+           \"Region \" SIZE_FORMAT\", top_bitmaps updates should be monotonic: \" PTR_FORMAT \" -> \" PTR_FORMAT,\n+           idx, p2i(_top_bitmaps[idx]), p2i(new_tams));\n+    assert(old_tams == r->bottom() || is_bitmap_clear_range(old_tams, new_tams),\n+           \"Region \" SIZE_FORMAT \", bitmap should be clear while adjusting TAMS: \" PTR_FORMAT \" -> \" PTR_FORMAT,\n+           idx, p2i(old_tams), p2i(new_tams));\n+\n+    log_debug(gc)(\"Capturing TAMS for %s Region \" SIZE_FORMAT \", was: %llx, now: %llx\",\n+                  affiliation_name(r->affiliation()), idx, (unsigned long long) old_tams, (unsigned long long) new_tams);\n+\n+    if ((old_tams == r->bottom()) && (new_tams > old_tams)) {\n+      log_debug(gc)(\"Clearing mark bitmap for %s Region \" SIZE_FORMAT \" while capturing TAMS\",\n+                    affiliation_name(r->affiliation()), idx);\n+\n+      clear_bitmap(r);\n+    }\n+\n+    _top_at_mark_starts_base[idx] = new_tams;\n+    if (new_tams > r->bottom()) {\n+      \/\/ In this case, new_tams is greater than old _top_bitmaps[idx]\n+      _top_bitmaps[idx] = new_tams;\n+    }\n+  }\n+  \/\/ else, FREE regions do not need their TAMS updated\n@@ -88,1 +115,1 @@\n-inline HeapWord* ShenandoahMarkingContext::top_at_mark_start(ShenandoahHeapRegion* r) const {\n+inline HeapWord* ShenandoahMarkingContext::top_at_mark_start(const ShenandoahHeapRegion* r) const {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.inline.hpp","additions":50,"deletions":23,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -27,0 +27,2 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -28,2 +30,3 @@\n-ShenandoahMemoryPool::ShenandoahMemoryPool(ShenandoahHeap* heap) :\n-        CollectedMemoryPool(\"Shenandoah\",\n+ShenandoahMemoryPool::ShenandoahMemoryPool(ShenandoahHeap* heap,\n+                                           const char* name) :\n+        CollectedMemoryPool(name,\n@@ -35,0 +38,11 @@\n+ShenandoahMemoryPool::ShenandoahMemoryPool(ShenandoahHeap* heap,\n+                                           const char* name,\n+                                           size_t initial_capacity,\n+                                           size_t max_capacity) :\n+        CollectedMemoryPool(name,\n+                            initial_capacity,\n+                            max_capacity,\n+                            true \/* support_usage_threshold *\/),\n+                            _heap(heap) {}\n+\n+\n@@ -54,0 +68,54 @@\n+\n+size_t ShenandoahMemoryPool::used_in_bytes() {\n+  return _heap->used();\n+}\n+\n+size_t ShenandoahMemoryPool::max_size() const {\n+  return _heap->max_capacity();\n+}\n+\n+ShenandoahYoungGenMemoryPool::ShenandoahYoungGenMemoryPool(ShenandoahHeap* heap) :\n+        ShenandoahMemoryPool(heap,\n+                             \"Shenandoah Young Gen\",\n+                             0,\n+                             heap->max_capacity()) { }\n+\n+MemoryUsage ShenandoahYoungGenMemoryPool::get_memory_usage() {\n+  size_t initial   = initial_size();\n+  size_t max       = max_size();\n+  size_t used      = used_in_bytes();\n+  size_t committed = _heap->young_generation()->used_regions_size();\n+\n+  return MemoryUsage(initial, used, committed, max);\n+}\n+\n+size_t ShenandoahYoungGenMemoryPool::used_in_bytes() {\n+  return _heap->young_generation()->used();\n+}\n+\n+size_t ShenandoahYoungGenMemoryPool::max_size() const {\n+  return _heap->young_generation()->max_capacity();\n+}\n+\n+ShenandoahOldGenMemoryPool::ShenandoahOldGenMemoryPool(ShenandoahHeap* heap) :\n+        ShenandoahMemoryPool(heap,\n+                             \"Shenandoah Old Gen\",\n+                             0,\n+                             heap->max_capacity()) { }\n+\n+MemoryUsage ShenandoahOldGenMemoryPool::get_memory_usage() {\n+  size_t initial   = initial_size();\n+  size_t max       = max_size();\n+  size_t used      = used_in_bytes();\n+  size_t committed = _heap->old_generation()->used_regions_size();\n+\n+  return MemoryUsage(initial, used, committed, max);\n+}\n+\n+size_t ShenandoahOldGenMemoryPool::used_in_bytes() {\n+  return _heap->old_generation()->used();\n+}\n+\n+size_t ShenandoahOldGenMemoryPool::max_size() const {\n+  return _heap->old_generation()->max_capacity();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMemoryPool.cpp","additions":70,"deletions":2,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-private:\n+protected:\n@@ -39,4 +39,27 @@\n-  ShenandoahMemoryPool(ShenandoahHeap* pool);\n-  MemoryUsage get_memory_usage();\n-  size_t used_in_bytes()              { return _heap->used(); }\n-  size_t max_size() const             { return _heap->max_capacity(); }\n+  ShenandoahMemoryPool(ShenandoahHeap* pool,\n+                      const char* name = \"Shenandoah\");\n+  virtual MemoryUsage get_memory_usage();\n+  virtual size_t used_in_bytes();\n+  virtual size_t max_size() const;\n+\n+protected:\n+  ShenandoahMemoryPool(ShenandoahHeap* pool,\n+                       const char* name,\n+                       size_t initial_capacity,\n+                       size_t max_capacity);\n+};\n+\n+class ShenandoahYoungGenMemoryPool : public ShenandoahMemoryPool {\n+public:\n+  ShenandoahYoungGenMemoryPool(ShenandoahHeap* pool);\n+  MemoryUsage get_memory_usage() override;\n+  size_t used_in_bytes() override;\n+  size_t max_size() const override;\n+};\n+\n+class ShenandoahOldGenMemoryPool : public ShenandoahMemoryPool {\n+public:\n+  ShenandoahOldGenMemoryPool(ShenandoahHeap* pool);\n+  MemoryUsage get_memory_usage() override;\n+  size_t used_in_bytes() override;\n+  size_t max_size() const override;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMemoryPool.hpp","additions":28,"deletions":5,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -0,0 +1,317 @@\n+\/*\n+ * Copyright (c) 2022, Amazon, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMmuTracker.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"runtime\/task.hpp\"\n+\n+\n+class ShenandoahMmuTask : public PeriodicTask {\n+  ShenandoahMmuTracker* _mmu_tracker;\n+public:\n+  explicit ShenandoahMmuTask(ShenandoahMmuTracker* mmu_tracker) :\n+    PeriodicTask(GCPauseIntervalMillis), _mmu_tracker(mmu_tracker) {}\n+\n+  void task() override {\n+    _mmu_tracker->report();\n+  }\n+};\n+\n+class ThreadTimeAccumulator : public ThreadClosure {\n+ public:\n+  size_t total_time;\n+  ThreadTimeAccumulator() : total_time(0) {}\n+  void do_thread(Thread* thread) override {\n+    total_time += os::thread_cpu_time(thread);\n+  }\n+};\n+\n+double ShenandoahMmuTracker::gc_thread_time_seconds() {\n+  ThreadTimeAccumulator cl;\n+  \/\/ We include only the gc threads because those are the only threads\n+  \/\/ we are responsible for.\n+  ShenandoahHeap::heap()->gc_threads_do(&cl);\n+  return double(cl.total_time) \/ NANOSECS_PER_SEC;\n+}\n+\n+double ShenandoahMmuTracker::process_time_seconds() {\n+  double process_real_time(0.0), process_user_time(0.0), process_system_time(0.0);\n+  bool valid = os::getTimesSecs(&process_real_time, &process_user_time, &process_system_time);\n+  if (valid) {\n+    return process_user_time + process_system_time;\n+  }\n+  return 0.0;\n+}\n+\n+ShenandoahMmuTracker::ShenandoahMmuTracker() :\n+    _generational_reference_time_s(0.0),\n+    _process_reference_time_s(0.0),\n+    _collector_reference_time_s(0.0),\n+    _mmu_periodic_task(new ShenandoahMmuTask(this)),\n+    _mmu_average(10, ShenandoahAdaptiveDecayFactor) {\n+}\n+\n+ShenandoahMmuTracker::~ShenandoahMmuTracker() {\n+  _mmu_periodic_task->disenroll();\n+  delete _mmu_periodic_task;\n+}\n+\n+void ShenandoahMmuTracker::record(ShenandoahGeneration* generation) {\n+  shenandoah_assert_control_or_vm_thread();\n+  double collector_time_s = gc_thread_time_seconds();\n+  double elapsed_gc_time_s = collector_time_s - _generational_reference_time_s;\n+  generation->add_collection_time(elapsed_gc_time_s);\n+  _generational_reference_time_s = collector_time_s;\n+}\n+\n+void ShenandoahMmuTracker::report() {\n+  \/\/ This is only called by the periodic thread.\n+  double process_time_s = process_time_seconds();\n+  double elapsed_process_time_s = process_time_s - _process_reference_time_s;\n+  if (elapsed_process_time_s <= 0.01) {\n+    \/\/ No cpu time for this interval?\n+    return;\n+  }\n+\n+  _process_reference_time_s = process_time_s;\n+  double collector_time_s = gc_thread_time_seconds();\n+  double elapsed_collector_time_s = collector_time_s - _collector_reference_time_s;\n+  _collector_reference_time_s = collector_time_s;\n+  double minimum_mutator_utilization = ((elapsed_process_time_s - elapsed_collector_time_s) \/ elapsed_process_time_s) * 100;\n+  _mmu_average.add(minimum_mutator_utilization);\n+  log_info(gc)(\"Average MMU = %.3f\", _mmu_average.davg());\n+}\n+\n+void ShenandoahMmuTracker::initialize() {\n+  _process_reference_time_s = process_time_seconds();\n+  _generational_reference_time_s = gc_thread_time_seconds();\n+  _collector_reference_time_s = _generational_reference_time_s;\n+  _mmu_periodic_task->enroll();\n+}\n+\n+ShenandoahGenerationSizer::ShenandoahGenerationSizer(ShenandoahMmuTracker* mmu_tracker)\n+  : _sizer_kind(SizerDefaults),\n+    _use_adaptive_sizing(true),\n+    _min_desired_young_regions(0),\n+    _max_desired_young_regions(0),\n+    _resize_increment(double(YoungGenerationSizeIncrement) \/ 100.0),\n+    _mmu_tracker(mmu_tracker) {\n+\n+  if (FLAG_IS_CMDLINE(NewRatio)) {\n+    if (FLAG_IS_CMDLINE(NewSize) || FLAG_IS_CMDLINE(MaxNewSize)) {\n+      log_warning(gc, ergo)(\"-XX:NewSize and -XX:MaxNewSize override -XX:NewRatio\");\n+    } else {\n+      _sizer_kind = SizerNewRatio;\n+      _use_adaptive_sizing = false;\n+      return;\n+    }\n+  }\n+\n+  if (NewSize > MaxNewSize) {\n+    if (FLAG_IS_CMDLINE(MaxNewSize)) {\n+      log_warning(gc, ergo)(\"NewSize (\" SIZE_FORMAT \"k) is greater than the MaxNewSize (\" SIZE_FORMAT \"k). \"\n+                            \"A new max generation size of \" SIZE_FORMAT \"k will be used.\",\n+                            NewSize\/K, MaxNewSize\/K, NewSize\/K);\n+    }\n+    FLAG_SET_ERGO(MaxNewSize, NewSize);\n+  }\n+\n+  if (FLAG_IS_CMDLINE(NewSize)) {\n+    _min_desired_young_regions = MAX2(uint(NewSize \/ ShenandoahHeapRegion::region_size_bytes()), 1U);\n+    if (FLAG_IS_CMDLINE(MaxNewSize)) {\n+      _max_desired_young_regions = MAX2(uint(MaxNewSize \/ ShenandoahHeapRegion::region_size_bytes()), 1U);\n+      _sizer_kind = SizerMaxAndNewSize;\n+      _use_adaptive_sizing = _min_desired_young_regions != _max_desired_young_regions;\n+    } else {\n+      _sizer_kind = SizerNewSizeOnly;\n+    }\n+  } else if (FLAG_IS_CMDLINE(MaxNewSize)) {\n+    _max_desired_young_regions = MAX2(uint(MaxNewSize \/ ShenandoahHeapRegion::region_size_bytes()), 1U);\n+    _sizer_kind = SizerMaxNewSizeOnly;\n+  }\n+}\n+\n+size_t ShenandoahGenerationSizer::calculate_min_young_regions(size_t heap_region_count) {\n+  size_t min_young_regions = (heap_region_count * ShenandoahMinYoungPercentage) \/ 100;\n+  return MAX2(uint(min_young_regions), 1U);\n+}\n+\n+size_t ShenandoahGenerationSizer::calculate_max_young_regions(size_t heap_region_count) {\n+  size_t max_young_regions = (heap_region_count * ShenandoahMaxYoungPercentage) \/ 100;\n+  return MAX2(uint(max_young_regions), 1U);\n+}\n+\n+void ShenandoahGenerationSizer::recalculate_min_max_young_length(size_t heap_region_count) {\n+  assert(heap_region_count > 0, \"Heap must be initialized\");\n+\n+  switch (_sizer_kind) {\n+    case SizerDefaults:\n+      _min_desired_young_regions = calculate_min_young_regions(heap_region_count);\n+      _max_desired_young_regions = calculate_max_young_regions(heap_region_count);\n+      break;\n+    case SizerNewSizeOnly:\n+      _max_desired_young_regions = calculate_max_young_regions(heap_region_count);\n+      _max_desired_young_regions = MAX2(_min_desired_young_regions, _max_desired_young_regions);\n+      break;\n+    case SizerMaxNewSizeOnly:\n+      _min_desired_young_regions = calculate_min_young_regions(heap_region_count);\n+      _min_desired_young_regions = MIN2(_min_desired_young_regions, _max_desired_young_regions);\n+      break;\n+    case SizerMaxAndNewSize:\n+      \/\/ Do nothing. Values set on the command line, don't update them at runtime.\n+      break;\n+    case SizerNewRatio:\n+      _min_desired_young_regions = MAX2(uint(heap_region_count \/ (NewRatio + 1)), 1U);\n+      _max_desired_young_regions = _min_desired_young_regions;\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+\n+  assert(_min_desired_young_regions <= _max_desired_young_regions, \"Invalid min\/max young gen size values\");\n+}\n+\n+void ShenandoahGenerationSizer::heap_size_changed(size_t heap_size) {\n+  recalculate_min_max_young_length(heap_size \/ ShenandoahHeapRegion::region_size_bytes());\n+}\n+\n+bool ShenandoahGenerationSizer::adjust_generation_sizes() const {\n+  shenandoah_assert_generational();\n+  if (!use_adaptive_sizing()) {\n+    return false;\n+  }\n+\n+  if (_mmu_tracker->average() >= double(GCTimeRatio)) {\n+    return false;\n+  }\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahOldGeneration *old = heap->old_generation();\n+  ShenandoahYoungGeneration *young = heap->young_generation();\n+  ShenandoahGeneration *global = heap->global_generation();\n+  double old_time_s = old->reset_collection_time();\n+  double young_time_s = young->reset_collection_time();\n+  double global_time_s = global->reset_collection_time();\n+\n+  const double transfer_threshold = 3.0;\n+  double delta = young_time_s - old_time_s;\n+\n+  log_info(gc)(\"Thread Usr+Sys YOUNG = %.3f, OLD = %.3f, GLOBAL = %.3f\", young_time_s, old_time_s, global_time_s);\n+\n+  if (abs(delta) <= transfer_threshold) {\n+    log_info(gc, ergo)(\"Difference (%.3f) for thread utilization for each generation is under threshold (%.3f)\", abs(delta), transfer_threshold);\n+    return false;\n+  }\n+\n+  if (delta > 0) {\n+    \/\/ young is busier than old, increase size of young to raise MMU\n+    return transfer_capacity(old, young);\n+  } else {\n+    \/\/ old is busier than young, increase size of old to raise MMU\n+    return transfer_capacity(young, old);\n+  }\n+}\n+\n+bool ShenandoahGenerationSizer::transfer_capacity(ShenandoahGeneration* target) const {\n+  ShenandoahHeapLocker locker(ShenandoahHeap::heap()->lock());\n+  if (target->is_young()) {\n+    return transfer_capacity(ShenandoahHeap::heap()->old_generation(), target);\n+  } else {\n+    assert(target->is_old(), \"Expected old generation, if not young.\");\n+    return transfer_capacity(ShenandoahHeap::heap()->young_generation(), target);\n+  }\n+}\n+\n+bool ShenandoahGenerationSizer::transfer_capacity(ShenandoahGeneration* from, ShenandoahGeneration* to) const {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+\n+  size_t available_regions = from->free_unaffiliated_regions();\n+  if (available_regions <= 0) {\n+    log_info(gc)(\"%s has no regions available for transfer to %s\", from->name(), to->name());\n+    return false;\n+  }\n+\n+  size_t regions_to_transfer = MAX2(1u, uint(double(available_regions) * _resize_increment));\n+  if (from->generation_mode() == YOUNG) {\n+    regions_to_transfer = adjust_transfer_from_young(from, regions_to_transfer);\n+  } else {\n+    regions_to_transfer = adjust_transfer_to_young(to, regions_to_transfer);\n+  }\n+\n+  if (regions_to_transfer == 0) {\n+    log_info(gc)(\"No capacity available to transfer from: %s (\" SIZE_FORMAT \"%s) to: %s (\" SIZE_FORMAT \"%s)\",\n+                  from->name(), byte_size_in_proper_unit(from->max_capacity()), proper_unit_for_byte_size(from->max_capacity()),\n+                  to->name(), byte_size_in_proper_unit(to->max_capacity()), proper_unit_for_byte_size(to->max_capacity()));\n+    return false;\n+  }\n+\n+  log_info(gc)(\"Transfer \" SIZE_FORMAT \" region(s) from %s to %s\", regions_to_transfer, from->name(), to->name());\n+  from->decrease_capacity(regions_to_transfer * ShenandoahHeapRegion::region_size_bytes());\n+  to->increase_capacity(regions_to_transfer * ShenandoahHeapRegion::region_size_bytes());\n+  return true;\n+}\n+\n+size_t ShenandoahGenerationSizer::adjust_transfer_from_young(ShenandoahGeneration* from, size_t regions_to_transfer) const {\n+  assert(from->generation_mode() == YOUNG, \"Expect to transfer from young\");\n+  size_t young_capacity_regions = from->max_capacity() \/ ShenandoahHeapRegion::region_size_bytes();\n+  size_t new_young_regions = young_capacity_regions - regions_to_transfer;\n+  size_t minimum_young_regions = min_young_regions();\n+  \/\/ Check that we are not going to violate the minimum size constraint.\n+  if (new_young_regions < minimum_young_regions) {\n+    assert(minimum_young_regions <= young_capacity_regions, \"Young is under minimum capacity.\");\n+    \/\/ If the transfer violates the minimum size and there is still some capacity to transfer,\n+    \/\/ adjust the transfer to take the size to the minimum. Note that this may be zero.\n+    regions_to_transfer = young_capacity_regions - minimum_young_regions;\n+  }\n+  return regions_to_transfer;\n+}\n+\n+size_t ShenandoahGenerationSizer::adjust_transfer_to_young(ShenandoahGeneration* to, size_t regions_to_transfer) const {\n+  assert(to->generation_mode() == YOUNG, \"Can only transfer between young and old.\");\n+  size_t young_capacity_regions = to->max_capacity() \/ ShenandoahHeapRegion::region_size_bytes();\n+  size_t new_young_regions = young_capacity_regions + regions_to_transfer;\n+  size_t maximum_young_regions = max_young_regions();\n+  \/\/ Check that we are not going to violate the maximum size constraint.\n+  if (new_young_regions > maximum_young_regions) {\n+    assert(maximum_young_regions >= young_capacity_regions, \"Young is over maximum capacity\");\n+    \/\/ If the transfer violates the maximum size and there is still some capacity to transfer,\n+    \/\/ adjust the transfer to take the size to the maximum. Note that this may be zero.\n+    regions_to_transfer = maximum_young_regions - young_capacity_regions;\n+  }\n+  return regions_to_transfer;\n+}\n+\n+size_t ShenandoahGenerationSizer::min_young_size() const {\n+  return min_young_regions() * ShenandoahHeapRegion::region_size_bytes();\n+}\n+\n+size_t ShenandoahGenerationSizer::max_young_size() const {\n+  return max_young_regions() * ShenandoahHeapRegion::region_size_bytes();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMmuTracker.cpp","additions":317,"deletions":0,"binary":false,"changes":317,"status":"added"},{"patch":"@@ -0,0 +1,163 @@\n+\/*\n+ * Copyright (c) 2022, Amazon, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHMMUTRACKER_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHMMUTRACKER_HPP\n+\n+#include \"runtime\/mutex.hpp\"\n+#include \"utilities\/numberSeq.hpp\"\n+\n+class ShenandoahGeneration;\n+class ShenandoahMmuTask;\n+\n+\/**\n+ * This class is responsible for tracking and adjusting the minimum mutator\n+ * utilization (MMU). MMU is defined as the percentage of CPU time available\n+ * to mutator threads over an arbitrary, fixed interval of time. This interval\n+ * defaults to 5 seconds and is configured by GCPauseIntervalMillis. The class\n+ * maintains a decaying average of the last 10 values. The MMU is measured\n+ * by summing all of the time given to the GC threads and comparing this to\n+ * the total CPU time for the process. There are OS APIs to support this on\n+ * all major platforms.\n+ *\n+ * The time spent by GC threads is attributed to the young or old generation.\n+ * The time given to the controller and regulator threads is attributed to the\n+ * global generation. At the end of every collection, the average MMU is inspected.\n+ * If it is below `GCTimeRatio`, this class will attempt to increase the capacity\n+ * of the generation that is consuming the most CPU time. The assumption being\n+ * that increasing memory will reduce the collection frequency and raise the\n+ * MMU.\n+ *\/\n+class ShenandoahMmuTracker {\n+\n+  double _generational_reference_time_s;\n+  double _process_reference_time_s;\n+  double _collector_reference_time_s;\n+\n+  ShenandoahMmuTask* _mmu_periodic_task;\n+  TruncatedSeq _mmu_average;\n+\n+  static double gc_thread_time_seconds();\n+  static double process_time_seconds();\n+\n+public:\n+  explicit ShenandoahMmuTracker();\n+  ~ShenandoahMmuTracker();\n+\n+  \/\/ This enrolls the periodic task after everything is initialized.\n+  void initialize();\n+\n+  \/\/ This is called at the start and end of a GC cycle. The GC thread times\n+  \/\/ will be accumulated in this generation. Note that the bootstrap cycle\n+  \/\/ for an old collection should be counted against the old generation.\n+  \/\/ When the collector is idle, it still runs a regulator and a control.\n+  \/\/ The times for these threads are attributed to the global generation.\n+  void record(ShenandoahGeneration* generation);\n+\n+  \/\/ This is called by the periodic task timer. The interval is defined by\n+  \/\/ GCPauseIntervalMillis and defaults to 5 seconds. This method computes\n+  \/\/ the MMU over the elapsed interval and records it in a running average.\n+  \/\/ This method also logs the average MMU.\n+  void report();\n+\n+  double average() {\n+    return _mmu_average.davg();\n+  }\n+};\n+\n+class ShenandoahGenerationSizer {\n+private:\n+  enum SizerKind {\n+    SizerDefaults,\n+    SizerNewSizeOnly,\n+    SizerMaxNewSizeOnly,\n+    SizerMaxAndNewSize,\n+    SizerNewRatio\n+  };\n+  SizerKind _sizer_kind;\n+\n+  \/\/ False when using a fixed young generation size due to command-line options,\n+  \/\/ true otherwise.\n+  bool _use_adaptive_sizing;\n+\n+  size_t _min_desired_young_regions;\n+  size_t _max_desired_young_regions;\n+\n+  double _resize_increment;\n+  ShenandoahMmuTracker* _mmu_tracker;\n+\n+  static size_t calculate_min_young_regions(size_t heap_region_count);\n+  static size_t calculate_max_young_regions(size_t heap_region_count);\n+\n+  \/\/ Update the given values for minimum and maximum young gen length in regions\n+  \/\/ given the number of heap regions depending on the kind of sizing algorithm.\n+  void recalculate_min_max_young_length(size_t heap_region_count);\n+\n+  \/\/ These two methods are responsible for enforcing the minimum and maximum\n+  \/\/ constraints for the size of the generations.\n+  size_t adjust_transfer_from_young(ShenandoahGeneration* from, size_t regions_to_transfer) const;\n+  size_t adjust_transfer_to_young(ShenandoahGeneration* to, size_t regions_to_transfer) const;\n+\n+  \/\/ This will attempt to transfer capacity from one generation to the other. It\n+  \/\/ returns true if a transfer is made, false otherwise.\n+  bool transfer_capacity(ShenandoahGeneration* from, ShenandoahGeneration* to) const;\n+public:\n+  explicit ShenandoahGenerationSizer(ShenandoahMmuTracker* mmu_tracker);\n+\n+  \/\/ Calculate the maximum length of the young gen given the number of regions\n+  \/\/ depending on the sizing algorithm.\n+  void heap_size_changed(size_t heap_size);\n+\n+  \/\/ Minimum size of young generation in bytes as multiple of region size.\n+  size_t min_young_size() const;\n+  size_t min_young_regions() const {\n+    return _min_desired_young_regions;\n+  }\n+\n+  \/\/ Maximum size of young generation in bytes as multiple of region size.\n+  size_t max_young_size() const;\n+  size_t max_young_regions() const {\n+    return _max_desired_young_regions;\n+  }\n+\n+  bool use_adaptive_sizing() const {\n+    return _use_adaptive_sizing;\n+  }\n+\n+  \/\/ This is invoked at the end of a collection. This happens on a safepoint\n+  \/\/ to avoid any races with allocators (and to avoid interfering with\n+  \/\/ allocators by taking the heap lock). The amount of capacity to move\n+  \/\/ from one generation to another is controlled by YoungGenerationSizeIncrement\n+  \/\/ and defaults to 20% of the available capacity of the donor generation.\n+  \/\/ The minimum and maximum sizes of the young generation are controlled by\n+  \/\/ ShenandoahMinYoungPercentage and ShenandoahMaxYoungPercentage, respectively.\n+  \/\/ The method returns true when an adjustment is made, false otherwise.\n+  bool adjust_generation_sizes() const;\n+\n+  \/\/ This may be invoked by a heuristic (from regulator thread) before it\n+  \/\/ decides to run a collection.\n+  bool transfer_capacity(ShenandoahGeneration* target) const;\n+};\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_SHENANDOAHMMUTRACKER_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMmuTracker.hpp","additions":163,"deletions":0,"binary":false,"changes":163,"status":"added"},{"patch":"@@ -160,5 +160,2 @@\n-  if (heap->is_concurrent_mark_in_progress()) {\n-    ShenandoahKeepAliveClosure cl;\n-    data->oops_do(&cl);\n-  } else if (heap->is_concurrent_weak_root_in_progress() ||\n-             heap->is_concurrent_strong_root_in_progress() ) {\n+  if (heap->is_concurrent_weak_root_in_progress() ||\n+      heap->is_concurrent_strong_root_in_progress()) {\n@@ -167,0 +164,3 @@\n+  } else if (heap->is_concurrent_mark_in_progress()) {\n+    ShenandoahKeepAliveClosure cl;\n+    data->oops_do(&cl);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahNMethod.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -123,0 +123,36 @@\n+\/\/ Merge this HdrSeq into hdr2: clear optional and on-by-default\n+\/\/ Note: this method isn't intrinsically MT-safe; callers must take care\n+\/\/ of any mutual exclusion as necessary.\n+void HdrSeq::merge(HdrSeq& hdr2, bool clear_this) {\n+  for (int mag = 0; mag < MagBuckets; mag++) {\n+    if (_hdr[mag] != nullptr) {\n+      int* that_bucket = hdr2._hdr[mag];\n+      if (that_bucket == nullptr) {\n+        if (clear_this) {\n+          \/\/ the target doesn't have any values, swap in ours.\n+          \/\/ Could this cause native memory fragmentation?\n+          hdr2._hdr[mag] = _hdr[mag];\n+          _hdr[mag] = nullptr;\n+        } else {\n+          \/\/ We can't clear this, so we create the entries & add in below\n+          that_bucket = NEW_C_HEAP_ARRAY(int, ValBuckets, mtInternal);\n+          for (int val = 0; val < ValBuckets; val++) {\n+            that_bucket[val] = _hdr[mag][val];\n+          }\n+          hdr2._hdr[mag] = that_bucket;\n+        }\n+      } else {\n+        \/\/ Add in our values into target\n+        for (int val = 0; val < ValBuckets; val++) {\n+          that_bucket[val] += _hdr[mag][val];\n+          if (clear_this) {\n+            _hdr[mag][val] = 0;\n+          }\n+        }\n+      }\n+    }\n+  }\n+  \/\/ Merge up the class hierarchy\n+  NumberSeq::merge(hdr2, clear_this);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahNumberSeq.cpp","additions":36,"deletions":0,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -53,0 +53,3 @@\n+\n+  \/\/ Merge this HdrSeq into hdr2, optionally clearing this HdrSeq\n+  void merge(HdrSeq& hdr2, bool clear_this = true);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahNumberSeq.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,150 @@\n+\/*\n+ * Copyright (c) 2022, Amazon.com, Inc. or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMonitoringSupport.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"prims\/jvmtiTagMap.hpp\"\n+#include \"utilities\/events.hpp\"\n+\n+\n+\n+\n+ShenandoahOldGC::ShenandoahOldGC(ShenandoahGeneration* generation, ShenandoahSharedFlag& allow_preemption) :\n+    ShenandoahConcurrentGC(generation, false), _allow_preemption(allow_preemption) {\n+}\n+\n+\/\/ Final mark for old-gen is different than for young or old, so we\n+\/\/ override the implementation.\n+void ShenandoahOldGC::op_final_mark() {\n+\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Should be at safepoint\");\n+  assert(!heap->has_forwarded_objects(), \"No forwarded objects on this path\");\n+\n+  if (ShenandoahVerify) {\n+    heap->verifier()->verify_roots_no_forwarded();\n+  }\n+\n+  if (!heap->cancelled_gc()) {\n+    assert(_mark.generation()->generation_mode() == OLD, \"Generation of Old-Gen GC should be OLD\");\n+    _mark.finish_mark();\n+    assert(!heap->cancelled_gc(), \"STW mark cannot OOM\");\n+\n+    \/\/ Old collection is complete, the young generation no longer needs this\n+    \/\/ reference to the old concurrent mark so clean it up.\n+    heap->young_generation()->set_old_gen_task_queues(nullptr);\n+\n+    \/\/ We need to do this because weak root cleaning reports the number of dead handles\n+    JvmtiTagMap::set_needs_cleaning();\n+\n+    _generation->prepare_regions_and_collection_set(true);\n+\n+    heap->set_unload_classes(false);\n+    heap->prepare_concurrent_roots();\n+\n+    \/\/ Believe verification following old-gen concurrent mark needs to be different than verification following\n+    \/\/ young-gen concurrent mark, so am commenting this out for now:\n+    \/\/   if (ShenandoahVerify) {\n+    \/\/     heap->verifier()->verify_after_concmark();\n+    \/\/   }\n+\n+    if (VerifyAfterGC) {\n+      Universe::verify();\n+    }\n+  }\n+}\n+\n+bool ShenandoahOldGC::collect(GCCause::Cause cause) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  assert(!heap->doing_mixed_evacuations(), \"Should not start an old gc with pending mixed evacuations\");\n+  assert(!heap->is_prepare_for_old_mark_in_progress(), \"Old regions need to be parseable during concurrent mark.\");\n+\n+  \/\/ Enable preemption of old generation mark.\n+  _allow_preemption.set();\n+\n+  \/\/ Continue concurrent mark, do not reset regions, do not mark roots, do not collect $200.\n+  entry_mark();\n+\n+  \/\/ If we failed to unset the preemption flag, it means another thread has already unset it.\n+  if (!_allow_preemption.try_unset()) {\n+    \/\/ The regulator thread has unset the preemption guard. That thread will shortly cancel\n+    \/\/ the gc, but the control thread is now racing it. Wait until this thread sees the\n+    \/\/ cancellation.\n+    while (!heap->cancelled_gc()) {\n+      SpinPause();\n+    }\n+  }\n+\n+  if (heap->cancelled_gc()) {\n+    return false;\n+  }\n+\n+  \/\/ Complete marking under STW\n+  vmop_entry_final_mark();\n+\n+  \/\/ We aren't dealing with old generation evacuation yet. Our heuristic\n+  \/\/ should not have built a cset in final mark.\n+  assert(!heap->is_evacuation_in_progress(), \"Old gen evacuations are not supported\");\n+\n+  \/\/ Process weak roots that might still point to regions that would be broken by cleanup\n+  if (heap->is_concurrent_weak_root_in_progress()) {\n+    entry_weak_refs();\n+    entry_weak_roots();\n+  }\n+\n+  \/\/ Final mark might have reclaimed some immediate garbage, kick cleanup to reclaim\n+  \/\/ the space. This would be the last action if there is nothing to evacuate.\n+  entry_cleanup_early();\n+\n+  {\n+    ShenandoahHeapLocker locker(heap->lock());\n+    heap->free_set()->log_status();\n+  }\n+\n+\n+  \/\/ TODO: Old marking doesn't support class unloading yet\n+  \/\/ Perform concurrent class unloading\n+  \/\/ if (heap->unload_classes() &&\n+  \/\/     heap->is_concurrent_weak_root_in_progress()) {\n+  \/\/   entry_class_unloading();\n+  \/\/ }\n+\n+\n+  assert(!heap->is_concurrent_strong_root_in_progress(), \"No evacuations during old gc.\");\n+\n+  \/\/ We must execute this vm operation if we completed final mark. We cannot\n+  \/\/ return from here with weak roots in progress. This is not a valid gc state\n+  \/\/ for any young collections (or allocation failures) that interrupt the old\n+  \/\/ collection.\n+  vmop_entry_final_roots(false);\n+\n+  return true;\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.cpp","additions":150,"deletions":0,"binary":false,"changes":150,"status":"added"},{"patch":"@@ -0,0 +1,48 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com, Inc. or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHOLDGC_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHOLDGC_HPP\n+\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shenandoah\/shenandoahConcurrentGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahVerifier.hpp\"\n+\n+class ShenandoahGeneration;\n+\n+class ShenandoahOldGC : public ShenandoahConcurrentGC {\n+ public:\n+  ShenandoahOldGC(ShenandoahGeneration* generation, ShenandoahSharedFlag& allow_preemption);\n+  bool collect(GCCause::Cause cause);\n+\n+ protected:\n+  virtual void op_final_mark();\n+\n+ private:\n+\n+  ShenandoahSharedFlag& _allow_preemption;\n+};\n+\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_SHENANDOAHOLDGC_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.hpp","additions":48,"deletions":0,"binary":false,"changes":48,"status":"added"},{"patch":"@@ -0,0 +1,442 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com, Inc. or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shared\/strongRootsScope.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMarkClosures.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMark.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMonitoringSupport.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahReferenceProcessor.hpp\"\n+#include \"gc\/shenandoah\/shenandoahStringDedup.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+#include \"gc\/shenandoah\/shenandoahWorkerPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"prims\/jvmtiTagMap.hpp\"\n+#include \"runtime\/threads.hpp\"\n+#include \"utilities\/events.hpp\"\n+\n+class ShenandoahFlushAllSATB : public ThreadClosure {\n+ private:\n+  SATBMarkQueueSet& _satb_qset;\n+\n+ public:\n+  explicit ShenandoahFlushAllSATB(SATBMarkQueueSet& satb_qset) :\n+    _satb_qset(satb_qset) {}\n+\n+  void do_thread(Thread* thread) {\n+    \/\/ Transfer any partial buffer to the qset for completed buffer processing.\n+    _satb_qset.flush_queue(ShenandoahThreadLocalData::satb_mark_queue(thread));\n+  }\n+};\n+\n+class ShenandoahProcessOldSATB : public SATBBufferClosure {\n+ private:\n+  ShenandoahObjToScanQueue* _queue;\n+  ShenandoahHeap* _heap;\n+  ShenandoahMarkingContext* const _mark_context;\n+\n+ public:\n+  size_t _trashed_oops;\n+\n+  explicit ShenandoahProcessOldSATB(ShenandoahObjToScanQueue* q) :\n+    _queue(q),\n+    _heap(ShenandoahHeap::heap()),\n+    _mark_context(_heap->marking_context()),\n+    _trashed_oops(0) {}\n+\n+  void do_buffer(void **buffer, size_t size) {\n+    assert(size == 0 || !_heap->has_forwarded_objects() || _heap->is_concurrent_old_mark_in_progress(), \"Forwarded objects are not expected here\");\n+    for (size_t i = 0; i < size; ++i) {\n+      oop *p = (oop *) &buffer[i];\n+      ShenandoahHeapRegion* region = _heap->heap_region_containing(*p);\n+      if (region->is_old() && region->is_active()) {\n+          ShenandoahMark::mark_through_ref<oop, OLD>(p, _queue, nullptr, _mark_context, false);\n+      } else {\n+        ++_trashed_oops;\n+      }\n+    }\n+  }\n+};\n+\n+class ShenandoahPurgeSATBTask : public WorkerTask {\n+private:\n+  ShenandoahObjToScanQueueSet* _mark_queues;\n+\n+public:\n+  volatile size_t _trashed_oops;\n+\n+  explicit ShenandoahPurgeSATBTask(ShenandoahObjToScanQueueSet* queues) :\n+    WorkerTask(\"Purge SATB\"),\n+    _mark_queues(queues),\n+    _trashed_oops(0) {\n+    Threads::change_thread_claim_token();\n+  }\n+\n+  ~ShenandoahPurgeSATBTask() {\n+    if (_trashed_oops > 0) {\n+      log_info(gc)(\"Purged \" SIZE_FORMAT \" oops from old generation SATB buffers.\", _trashed_oops);\n+    }\n+  }\n+\n+  void work(uint worker_id) {\n+    ShenandoahParallelWorkerSession worker_session(worker_id);\n+    ShenandoahSATBMarkQueueSet &satb_queues = ShenandoahBarrierSet::satb_mark_queue_set();\n+    ShenandoahFlushAllSATB flusher(satb_queues);\n+    Threads::possibly_parallel_threads_do(true \/* is_par *\/, &flusher);\n+\n+    ShenandoahObjToScanQueue* mark_queue = _mark_queues->queue(worker_id);\n+    ShenandoahProcessOldSATB processor(mark_queue);\n+    while (satb_queues.apply_closure_to_completed_buffer(&processor)) {}\n+\n+    Atomic::add(&_trashed_oops, processor._trashed_oops);\n+  }\n+};\n+\n+class ShenandoahConcurrentCoalesceAndFillTask : public WorkerTask {\n+ private:\n+  uint _nworkers;\n+  ShenandoahHeapRegion** _coalesce_and_fill_region_array;\n+  uint _coalesce_and_fill_region_count;\n+  volatile bool _is_preempted;\n+\n+ public:\n+  ShenandoahConcurrentCoalesceAndFillTask(uint nworkers, ShenandoahHeapRegion** coalesce_and_fill_region_array,\n+                                          uint region_count) :\n+    WorkerTask(\"Shenandoah Concurrent Coalesce and Fill\"),\n+    _nworkers(nworkers),\n+    _coalesce_and_fill_region_array(coalesce_and_fill_region_array),\n+    _coalesce_and_fill_region_count(region_count),\n+    _is_preempted(false) {\n+  }\n+\n+  void work(uint worker_id) {\n+    for (uint region_idx = worker_id; region_idx < _coalesce_and_fill_region_count; region_idx += _nworkers) {\n+      ShenandoahHeapRegion* r = _coalesce_and_fill_region_array[region_idx];\n+      if (r->is_humongous()) {\n+        \/\/ there's only one object in this region and it's not garbage, so no need to coalesce or fill\n+        continue;\n+      }\n+\n+      if (!r->oop_fill_and_coalesce()) {\n+        \/\/ Coalesce and fill has been preempted\n+        Atomic::store(&_is_preempted, true);\n+        return;\n+      }\n+    }\n+  }\n+\n+  \/\/ Value returned from is_completed() is only valid after all worker thread have terminated.\n+  bool is_completed() {\n+    return !Atomic::load(&_is_preempted);\n+  }\n+};\n+\n+ShenandoahOldGeneration::ShenandoahOldGeneration(uint max_queues, size_t max_capacity, size_t soft_max_capacity)\n+  : ShenandoahGeneration(OLD, max_queues, max_capacity, soft_max_capacity),\n+    _coalesce_and_fill_region_array(NEW_C_HEAP_ARRAY(ShenandoahHeapRegion*, ShenandoahHeap::heap()->num_regions(), mtGC)),\n+    _state(IDLE)\n+{\n+  \/\/ Always clear references for old generation\n+  ref_processor()->set_soft_reference_policy(true);\n+}\n+\n+const char* ShenandoahOldGeneration::name() const {\n+  return \"OLD\";\n+}\n+\n+bool ShenandoahOldGeneration::contains(ShenandoahHeapRegion* region) const {\n+  return region->affiliation() != YOUNG_GENERATION;\n+}\n+\n+void ShenandoahOldGeneration::parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl) {\n+  ShenandoahGenerationRegionClosure<OLD> old_regions(cl);\n+  ShenandoahHeap::heap()->parallel_heap_region_iterate(&old_regions);\n+}\n+\n+void ShenandoahOldGeneration::heap_region_iterate(ShenandoahHeapRegionClosure* cl) {\n+  ShenandoahGenerationRegionClosure<OLD> old_regions(cl);\n+  ShenandoahHeap::heap()->heap_region_iterate(&old_regions);\n+}\n+\n+void ShenandoahOldGeneration::set_concurrent_mark_in_progress(bool in_progress) {\n+  ShenandoahHeap::heap()->set_concurrent_old_mark_in_progress(in_progress);\n+}\n+\n+bool ShenandoahOldGeneration::is_concurrent_mark_in_progress() {\n+  return ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress();\n+}\n+\n+void ShenandoahOldGeneration::cancel_marking() {\n+  if (is_concurrent_mark_in_progress()) {\n+    log_info(gc)(\"Abandon satb buffers.\");\n+    ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+  }\n+\n+  ShenandoahGeneration::cancel_marking();\n+}\n+\n+void ShenandoahOldGeneration::prepare_gc() {\n+\n+  \/\/ Make the old generation regions parseable, so they can be safely\n+  \/\/ scanned when looking for objects in memory indicated by dirty cards.\n+  if (entry_coalesce_and_fill()) {\n+    \/\/ Now that we have made the old generation parseable, it is safe to reset the mark bitmap.\n+    static const char* msg = \"Concurrent reset (OLD)\";\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset_old);\n+    ShenandoahWorkerScope scope(ShenandoahHeap::heap()->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n+                                msg);\n+    ShenandoahGeneration::prepare_gc();\n+  }\n+  \/\/ Else, coalesce-and-fill has been preempted and we'll finish that effort in the future.  Do not invoke\n+  \/\/ ShenandoahGeneration::prepare_gc() until coalesce-and-fill is done because it resets the mark bitmap\n+  \/\/ and invokes set_mark_incomplete().  Coalesce-and-fill depends on the mark bitmap.\n+}\n+\n+bool ShenandoahOldGeneration::entry_coalesce_and_fill() {\n+  char msg[1024];\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n+  ShenandoahConcurrentPhase gc_phase(\"Coalescing and filling (OLD)\", ShenandoahPhaseTimings::coalesce_and_fill);\n+\n+  \/\/ TODO: I don't think we're using these concurrent collection counters correctly.\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  EventMark em(\"%s\", msg);\n+  ShenandoahWorkerScope scope(heap->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_marking(),\n+                              \"concurrent coalesce and fill\");\n+\n+  return coalesce_and_fill();\n+}\n+\n+bool ShenandoahOldGeneration::coalesce_and_fill() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  heap->set_prepare_for_old_mark_in_progress(true);\n+  transition_to(FILLING);\n+\n+  ShenandoahOldHeuristics* old_heuristics = heap->old_heuristics();\n+  WorkerThreads* workers = heap->workers();\n+  uint nworkers = workers->active_workers();\n+\n+  log_debug(gc)(\"Starting (or resuming) coalesce-and-fill of old heap regions\");\n+  uint coalesce_and_fill_regions_count = old_heuristics->get_coalesce_and_fill_candidates(_coalesce_and_fill_region_array);\n+  assert(coalesce_and_fill_regions_count <= heap->num_regions(), \"Sanity\");\n+  ShenandoahConcurrentCoalesceAndFillTask task(nworkers, _coalesce_and_fill_region_array, coalesce_and_fill_regions_count);\n+\n+  workers->run_task(&task);\n+  if (task.is_completed()) {\n+    \/\/ Remember that we're done with coalesce-and-fill.\n+    heap->set_prepare_for_old_mark_in_progress(false);\n+    transition_to(BOOTSTRAPPING);\n+    return true;\n+  } else {\n+    log_debug(gc)(\"Suspending coalesce-and-fill of old heap regions\");\n+    \/\/ Otherwise, we got preempted before the work was done.\n+    return false;\n+  }\n+}\n+\n+void ShenandoahOldGeneration::transfer_pointers_from_satb() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  shenandoah_assert_safepoint();\n+  assert(heap->is_concurrent_old_mark_in_progress(), \"Only necessary during old marking.\");\n+  log_info(gc)(\"Transfer satb buffers.\");\n+  uint nworkers = heap->workers()->active_workers();\n+  StrongRootsScope scope(nworkers);\n+\n+  ShenandoahPurgeSATBTask purge_satb_task(task_queues());\n+  heap->workers()->run_task(&purge_satb_task);\n+}\n+\n+bool ShenandoahOldGeneration::contains(oop obj) const {\n+  return ShenandoahHeap::heap()->is_in_old(obj);\n+}\n+\n+void ShenandoahOldGeneration::prepare_regions_and_collection_set(bool concurrent) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  assert(!heap->is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n+\n+  {\n+    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states : ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n+    ShenandoahFinalMarkUpdateRegionStateClosure cl(complete_marking_context());\n+\n+    parallel_heap_region_iterate(&cl);\n+    heap->assert_pinned_region_status();\n+  }\n+\n+  {\n+    \/\/ This doesn't actually choose a collection set, but prepares a list of\n+    \/\/ regions as 'candidates' for inclusion in a mixed collection.\n+    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset : ShenandoahPhaseTimings::degen_gc_choose_cset);\n+    ShenandoahHeapLocker locker(heap->lock());\n+    heuristics()->choose_collection_set(nullptr, nullptr);\n+  }\n+\n+  {\n+    \/\/ Though we did not choose a collection set above, we still may have\n+    \/\/ freed up immediate garbage regions so proceed with rebuilding the free set.\n+    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset : ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n+    ShenandoahHeapLocker locker(heap->lock());\n+    heap->free_set()->rebuild();\n+  }\n+}\n+\n+const char* ShenandoahOldGeneration::state_name(State state) {\n+  switch (state) {\n+    case IDLE:          return \"Idle\";\n+    case FILLING:       return \"Coalescing\";\n+    case BOOTSTRAPPING: return \"Bootstrapping\";\n+    case MARKING:       return \"Marking\";\n+    case WAITING:       return \"Waiting\";\n+    default:\n+      ShouldNotReachHere();\n+      return \"Unknown\";\n+  }\n+}\n+\n+void ShenandoahOldGeneration::transition_to(State new_state) {\n+  if (_state != new_state) {\n+    log_info(gc)(\"Old generation transition from %s to %s\", state_name(_state), state_name(new_state));\n+    assert(validate_transition(new_state), \"Invalid state transition.\");\n+    _state = new_state;\n+  }\n+}\n+\n+#ifdef ASSERT\n+\/\/ This diagram depicts the expected state transitions for marking the old generation\n+\/\/ and preparing for old collections. When a young generation cycle executes, the\n+\/\/ remembered set scan must visit objects in old regions. Visiting an object which\n+\/\/ has become dead on previous old cycles will result in crashes. To avoid visiting\n+\/\/ such objects, the remembered set scan will use the old generation mark bitmap when\n+\/\/ possible. It is _not_ possible to use the old generation bitmap when old marking\n+\/\/ is active (bitmap is not complete). For this reason, the old regions are made\n+\/\/ parseable _before_ the old generation bitmap is reset. The diagram does not depict\n+\/\/ global and full collections, both of which cancel any old generation activity.\n+\/\/\n+\/\/                              +-----------------+\n+\/\/               +------------> |      IDLE       |\n+\/\/               |   +--------> |                 |\n+\/\/               |   |          +-----------------+\n+\/\/               |   |            |\n+\/\/               |   |            | Begin Old Mark\n+\/\/               |   |            v\n+\/\/               |   |          +-----------------+     +--------------------+\n+\/\/               |   |          |     FILLING     | <-> |      YOUNG GC      |\n+\/\/               |   |          |                 |     | (RSet Uses Bitmap) |\n+\/\/               |   |          +-----------------+     +--------------------+\n+\/\/               |   |            |\n+\/\/               |   |            | Reset Bitmap\n+\/\/               |   |            v\n+\/\/               |   |          +-----------------+\n+\/\/               |   |          |    BOOTSTRAP    |\n+\/\/               |   |          |                 |\n+\/\/               |   |          +-----------------+\n+\/\/               |   |            |\n+\/\/               |   |            | Continue Marking\n+\/\/               |   |            v\n+\/\/               |   |          +-----------------+     +----------------------+\n+\/\/               |   |          |    MARKING      | <-> |       YOUNG GC       |\n+\/\/               |   +----------|                 |     | (RSet Parses Region) |\n+\/\/               |              +-----------------+     +----------------------+\n+\/\/               |                |\n+\/\/               |                | Has Candidates\n+\/\/               |                v\n+\/\/               |              +-----------------+\n+\/\/               |              |     WAITING     |\n+\/\/               +------------- |                 |\n+\/\/                              +-----------------+\n+\/\/\n+bool ShenandoahOldGeneration::validate_transition(State new_state) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  switch (new_state) {\n+    case IDLE:\n+      assert(!heap->is_concurrent_old_mark_in_progress(), \"Cannot become idle during old mark.\");\n+      assert(_old_heuristics->unprocessed_old_collection_candidates() == 0, \"Cannot become idle with collection candidates\");\n+      assert(!heap->is_prepare_for_old_mark_in_progress(), \"Cannot become idle while making old generation parseable.\");\n+      assert(heap->young_generation()->old_gen_task_queues() == nullptr, \"Cannot become idle when setup for bootstrapping.\");\n+      return true;\n+    case FILLING:\n+      assert(_state == IDLE, \"Cannot begin filling without first being idle.\");\n+      assert(heap->is_prepare_for_old_mark_in_progress(), \"Should be preparing for old mark now.\");\n+      return true;\n+    case BOOTSTRAPPING:\n+      assert(_state == FILLING, \"Cannot reset bitmap without making old regions parseable.\");\n+      \/\/ assert(heap->young_generation()->old_gen_task_queues() != nullptr, \"Cannot bootstrap without old mark queues.\");\n+      assert(!heap->is_prepare_for_old_mark_in_progress(), \"Cannot still be making old regions parseable.\");\n+      return true;\n+    case MARKING:\n+      assert(_state == BOOTSTRAPPING, \"Must have finished bootstrapping before marking.\");\n+      assert(heap->young_generation()->old_gen_task_queues() != nullptr, \"Young generation needs old mark queues.\");\n+      assert(heap->is_concurrent_old_mark_in_progress(), \"Should be marking old now.\");\n+      return true;\n+    case WAITING:\n+      assert(_state == MARKING, \"Cannot have old collection candidates without first marking.\");\n+      assert(_old_heuristics->unprocessed_old_collection_candidates() > 0, \"Must have collection candidates here.\");\n+      return true;\n+    default:\n+      ShouldNotReachHere();\n+      return false;\n+  }\n+}\n+#endif\n+\n+ShenandoahHeuristics* ShenandoahOldGeneration::initialize_heuristics(ShenandoahMode* gc_mode) {\n+  assert(ShenandoahOldGCHeuristics != nullptr, \"ShenandoahOldGCHeuristics should not be unset\");\n+  ShenandoahHeuristics* trigger;\n+  if (strcmp(ShenandoahOldGCHeuristics, \"static\") == 0) {\n+    trigger = new ShenandoahStaticHeuristics(this);\n+  } else if (strcmp(ShenandoahOldGCHeuristics, \"adaptive\") == 0) {\n+    trigger = new ShenandoahAdaptiveHeuristics(this);\n+  } else if (strcmp(ShenandoahOldGCHeuristics, \"compact\") == 0) {\n+    trigger = new ShenandoahCompactHeuristics(this);\n+  } else {\n+    vm_exit_during_initialization(\"Unknown -XX:ShenandoahOldGCHeuristics option (must be one of: static, adaptive, compact)\");\n+    ShouldNotReachHere();\n+    return nullptr;\n+  }\n+  trigger->set_guaranteed_gc_interval(ShenandoahGuaranteedOldGCInterval);\n+  _old_heuristics = new ShenandoahOldHeuristics(this, trigger);\n+  _heuristics = _old_heuristics;\n+  return _heuristics;\n+}\n+\n+void ShenandoahOldGeneration::record_success_concurrent(bool abbreviated) {\n+  heuristics()->record_success_concurrent(abbreviated);\n+  ShenandoahHeap::heap()->shenandoah_policy()->record_success_old();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":442,"deletions":0,"binary":false,"changes":442,"status":"added"},{"patch":"@@ -0,0 +1,108 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com, Inc. or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHOLDGENERATION_HPP\n+#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHOLDGENERATION_HPP\n+\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+\n+class ShenandoahHeapRegion;\n+class ShenandoahHeapRegionClosure;\n+class ShenandoahOldHeuristics;\n+\n+class ShenandoahOldGeneration : public ShenandoahGeneration {\n+ public:\n+  ShenandoahOldGeneration(uint max_queues, size_t max_capacity, size_t soft_max_capacity);\n+\n+  const char* name() const override;\n+\n+  bool contains(ShenandoahHeapRegion* region) const override;\n+\n+  bool contains(oop obj) const override;\n+\n+  void parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl) override;\n+\n+  void heap_region_iterate(ShenandoahHeapRegionClosure* cl) override;\n+\n+  void set_concurrent_mark_in_progress(bool in_progress) override;\n+\n+  virtual void cancel_marking() override;\n+\n+  virtual void prepare_gc() override;\n+\n+  void prepare_regions_and_collection_set(bool concurrent) override;\n+\n+  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahMode* gc_mode) override;\n+\n+  \/\/ We leave the SATB barrier on for the entirety of the old generation\n+  \/\/ marking phase. In some cases, this can cause a write to a perfectly\n+  \/\/ reachable oop to enqueue a pointer that later becomes garbage (because\n+  \/\/ it points at an object in the collection set, for example). There are\n+  \/\/ also cases where the referent of a weak reference ends up in the SATB\n+  \/\/ and is later collected. In these cases the oop in the SATB buffer becomes\n+  \/\/ invalid and the _next_ cycle will crash during its marking phase. To\n+  \/\/ avoid this problem, we \"purge\" the SATB buffers during the final update\n+  \/\/ references phase if (and only if) an old generation mark is in progress.\n+  \/\/ At this stage we can safely determine if any of the oops in the SATB\n+  \/\/ buffer belong to trashed regions (before they are recycled). As it\n+  \/\/ happens, flushing a SATB queue also filters out oops which have already\n+  \/\/ been marked - which is the case for anything that is being evacuated\n+  \/\/ from the collection set.\n+  \/\/\n+  \/\/ Alternatively, we could inspect the state of the heap and the age of the\n+  \/\/ object at the barrier, but we reject this approach because it is likely\n+  \/\/ the performance impact would be too severe.\n+  void transfer_pointers_from_satb();\n+\n+  bool is_concurrent_mark_in_progress() override;\n+\n+  virtual void record_success_concurrent(bool abbreviated) override;\n+\n+  enum State {\n+    IDLE, FILLING, BOOTSTRAPPING, MARKING, WAITING\n+  };\n+\n+  static const char* state_name(State state);\n+\n+  void transition_to(State new_state);\n+\n+#ifdef ASSERT\n+  bool validate_transition(State new_state);\n+#endif\n+\n+  State state() const {\n+    return _state;\n+  }\n+\n+ private:\n+  bool entry_coalesce_and_fill();\n+  bool coalesce_and_fill();\n+\n+  ShenandoahHeapRegion** _coalesce_and_fill_region_array;\n+  ShenandoahOldHeuristics* _old_heuristics;\n+  State _state;\n+};\n+\n+\n+#endif \/\/SHARE_VM_GC_SHENANDOAH_SHENANDOAHOLDGENERATION_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.hpp","additions":108,"deletions":0,"binary":false,"changes":108,"status":"added"},{"patch":"@@ -45,0 +45,1 @@\n+  ShenandoahObjToScanQueue* _old_queue;\n@@ -49,1 +50,1 @@\n-  template <class T>\n+  template <class T, GenerationMode GENERATION>\n@@ -53,1 +54,1 @@\n-  ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp);\n+  ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp,  ShenandoahObjToScanQueue* old_queue = nullptr);\n@@ -73,1 +74,1 @@\n-  template <class T>\n+  template <class T, GenerationMode GENERATION>\n@@ -77,2 +78,2 @@\n-  ShenandoahMarkUpdateRefsSuperClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n-    ShenandoahMarkRefsSuperClosure(q, rp),\n+  ShenandoahMarkUpdateRefsSuperClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old = nullptr) :\n+    ShenandoahMarkRefsSuperClosure(q, rp, old),\n@@ -84,0 +85,1 @@\n+template <GenerationMode GENERATION>\n@@ -87,1 +89,1 @@\n-  inline void do_oop_work(T* p)     { work<T>(p); }\n+  inline void do_oop_work(T* p)     { work<T, GENERATION>(p); }\n@@ -90,2 +92,2 @@\n-  ShenandoahMarkUpdateRefsClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n-    ShenandoahMarkUpdateRefsSuperClosure(q, rp) {}\n+  ShenandoahMarkUpdateRefsClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old = nullptr) :\n+    ShenandoahMarkUpdateRefsSuperClosure(q, rp, old) {}\n@@ -97,0 +99,1 @@\n+template <GenerationMode GENERATION>\n@@ -100,1 +103,1 @@\n-  inline void do_oop_work(T* p)     { work<T>(p); }\n+  inline void do_oop_work(T* p)     { work<T, GENERATION>(p); }\n@@ -103,2 +106,2 @@\n-  ShenandoahMarkRefsClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n-    ShenandoahMarkRefsSuperClosure(q, rp) {};\n+  ShenandoahMarkRefsClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old = nullptr) :\n+    ShenandoahMarkRefsSuperClosure(q, rp, old) {};\n@@ -110,1 +113,0 @@\n-\n@@ -145,0 +147,38 @@\n+class ShenandoahVerifyRemSetClosure : public BasicOopIterateClosure {\n+  protected:\n+  bool _init_mark;\n+  ShenandoahHeap* _heap;\n+  RememberedScanner* _scanner;\n+\n+  public:\n+\/\/ Argument distinguishes between initial mark or start of update refs verification.\n+  ShenandoahVerifyRemSetClosure(bool init_mark) :\n+      _init_mark(init_mark),\n+      _heap(ShenandoahHeap::heap()),\n+      _scanner(_heap->card_scan()) {  }\n+  template<class T>\n+  inline void work(T* p);\n+\n+  virtual void do_oop(narrowOop* p) { work(p); }\n+  virtual void do_oop(oop* p) { work(p); }\n+};\n+\n+class ShenandoahSetRememberedCardsToDirtyClosure : public BasicOopIterateClosure {\n+\n+protected:\n+  ShenandoahHeap* _heap;\n+  RememberedScanner* _scanner;\n+\n+public:\n+\n+  ShenandoahSetRememberedCardsToDirtyClosure() :\n+      _heap(ShenandoahHeap::heap()),\n+      _scanner(_heap->card_scan()) {  }\n+\n+  template<class T>\n+  inline void work(T* p);\n+\n+  virtual void do_oop(narrowOop* p) { work(p); }\n+  virtual void do_oop(oop* p) { work(p); }\n+};\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOopClosures.hpp","additions":52,"deletions":12,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-template<class T>\n+template<class T, GenerationMode GENERATION>\n@@ -35,1 +35,1 @@\n-  ShenandoahMark::mark_through_ref<T>(p, _queue, _mark_context, _weak);\n+  ShenandoahMark::mark_through_ref<T, GENERATION>(p, _queue, _old_queue, _mark_context, _weak);\n@@ -38,1 +38,1 @@\n-template<class T>\n+template<class T, GenerationMode GENERATION>\n@@ -44,1 +44,1 @@\n-  ShenandoahMarkRefsSuperClosure::work<T>(p);\n+  ShenandoahMarkRefsSuperClosure::work<T, GENERATION>(p);\n@@ -57,0 +57,30 @@\n+template<class T>\n+inline void ShenandoahVerifyRemSetClosure::work(T* p) {\n+  T o = RawAccess<>::oop_load(p);\n+  if (!CompressedOops::is_null(o)) {\n+    oop obj = CompressedOops::decode_not_null(o);\n+    if (_heap->is_in_young(obj)) {\n+      size_t card_index = _scanner->card_index_for_addr((HeapWord*) p);\n+      if (_init_mark && !_scanner->is_card_dirty(card_index)) {\n+        ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, p, nullptr,\n+                                        \"Verify init-mark remembered set violation\", \"clean card should be dirty\", __FILE__, __LINE__);\n+      } else if (!_init_mark && !_scanner->is_write_card_dirty(card_index)) {\n+        ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, p, nullptr,\n+                                        \"Verify init-update-refs remembered set violation\", \"clean card should be dirty\", __FILE__, __LINE__);\n+      }\n+    }\n+  }\n+}\n+\n+template<class T>\n+inline void ShenandoahSetRememberedCardsToDirtyClosure::work(T* p) {\n+  T o = RawAccess<>::oop_load(p);\n+  if (!CompressedOops::is_null(o)) {\n+    oop obj = CompressedOops::decode_not_null(o);\n+    if (_heap->is_in_young(obj)) {\n+      \/\/ Found interesting pointer.  Mark the containing card as dirty.\n+      _scanner->mark_card_as_dirty((HeapWord*) p);\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOopClosures.inline.hpp","additions":34,"deletions":4,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -100,0 +100,1 @@\n+    case init_scan_rset:\n@@ -115,0 +116,1 @@\n+    case conc_mark:\n@@ -311,1 +313,1 @@\n-        ShenandoahPhaseTimings::ParPhase par_phase, uint worker_id) :\n+        ShenandoahPhaseTimings::ParPhase par_phase, uint worker_id, bool cumulative) :\n@@ -315,1 +317,1 @@\n-  assert(_timings->worker_data(_phase, _par_phase)->get(_worker_id) == ShenandoahWorkerData::uninitialized(),\n+  assert(_timings->worker_data(_phase, _par_phase)->get(_worker_id) == ShenandoahWorkerData::uninitialized() || cumulative,\n@@ -321,1 +323,1 @@\n-  _timings->worker_data(_phase, _par_phase)->set(_worker_id, os::elapsedTime() - _start_time);\n+  _timings->worker_data(_phase, _par_phase)->set_or_add(_worker_id, os::elapsedTime() - _start_time);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPhaseTimings.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+  f(CNT_PREFIX ## ScanClusters,             DESC_PREFIX \"Scan Clusters\")\n@@ -51,1 +52,1 @@\n-                                                                                       \\\n+  f(conc_reset_old,                                 \"Concurrent Reset (OLD)\")          \\\n@@ -55,0 +56,2 @@\n+  f(init_swap_rset,                                 \"  Swap Remembered Set\")           \\\n+  f(init_transfer_satb,                             \"  Transfer Old From SATB\")        \\\n@@ -57,0 +60,3 @@\n+  f(init_scan_rset,                                 \"Concurrent Scan Remembered Set\")  \\\n+  SHENANDOAH_PAR_PHASE_DO(init_scan_rset_,          \"  RS: \", f)                       \\\n+                                                                                       \\\n@@ -60,0 +66,1 @@\n+  SHENANDOAH_PAR_PHASE_DO(conc_mark,                \"  CM: \", f)                       \\\n@@ -97,0 +104,2 @@\n+  f(coalesce_and_fill,                              \"Coalesce and Fill Old Dead\")      \\\n+  SHENANDOAH_PAR_PHASE_DO(coalesce_and_fill_,       \"  CFOD: \", f)                     \\\n@@ -174,0 +183,1 @@\n+  f(full_gc_reconstruct_remembered_set,             \"    Reconstruct Remembered Set\")  \\\n@@ -252,1 +262,2 @@\n-  ShenandoahWorkerTimingsTracker(ShenandoahPhaseTimings::Phase phase, ShenandoahPhaseTimings::ParPhase par_phase, uint worker_id);\n+  ShenandoahWorkerTimingsTracker(ShenandoahPhaseTimings::Phase phase, ShenandoahPhaseTimings::ParPhase par_phase,\n+                                 uint worker_id, bool cumulative = false);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPhaseTimings.hpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -260,0 +261,1 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -276,0 +278,5 @@\n+  if (!heap->is_in_active_generation(referent)) {\n+    log_trace(gc,ref)(\"Referent outside of active generation: \" PTR_FORMAT, p2i(referent));\n+    return false;\n+  }\n+\n@@ -363,1 +370,2 @@\n-  log_trace(gc, ref)(\"Encountered Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+  log_trace(gc, ref)(\"Encountered Reference: \" PTR_FORMAT \" (%s, %s)\",\n+          p2i(reference), reference_type_name(type), affiliation_name(reference));\n@@ -378,1 +386,1 @@\n-#ifdef ASSERT\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -380,3 +388,1 @@\n-  assert(referent == nullptr || ShenandoahHeap::heap()->marking_context()->is_marked(referent),\n-         \"only drop references with alive referents\");\n-#endif\n+  assert(referent == nullptr || heap->marking_context()->is_marked(referent), \"only drop references with alive referents\");\n@@ -387,0 +393,8 @@\n+  \/\/ When this reference was discovered, it would not have been marked. If it ends up surviving\n+  \/\/ the cycle, we need to dirty the card if the reference is old and the referent is young.  Note\n+  \/\/ that if the reference is not dropped, then its pointer to the referent will be nulled before\n+  \/\/ evacuation begins so card does not need to be dirtied.\n+  if (heap->mode()->is_generational() && heap->is_in_old(reference) && heap->is_in_young(referent)) {\n+    \/\/ Note: would be sufficient to mark only the card that holds the start of this Reference object.\n+    heap->card_scan()->mark_range_as_dirty(cast_from_oop<HeapWord*>(reference), reference->size());\n+  }\n@@ -526,1 +540,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahReferenceProcessor.cpp","additions":19,"deletions":6,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -0,0 +1,163 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com, Inc. and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+#include \"gc\/shenandoah\/shenandoahControlThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahRegulatorThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"logging\/log.hpp\"\n+\n+static ShenandoahHeuristics* get_heuristics(ShenandoahGeneration* nullable) {\n+  return nullable != nullptr ? nullable->heuristics() : nullptr;\n+}\n+\n+ShenandoahRegulatorThread::ShenandoahRegulatorThread(ShenandoahControlThread* control_thread) :\n+  ConcurrentGCThread(),\n+  _control_thread(control_thread),\n+  _sleep(ShenandoahControlIntervalMin),\n+  _last_sleep_adjust_time(os::elapsedTime()) {\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  _old_heuristics = get_heuristics(heap->old_generation());\n+  _young_heuristics = get_heuristics(heap->young_generation());\n+  _global_heuristics = get_heuristics(heap->global_generation());\n+\n+  create_and_start();\n+}\n+\n+void ShenandoahRegulatorThread::run_service() {\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    if (ShenandoahAllowOldMarkingPreemption) {\n+      regulate_concurrent_cycles();\n+    } else {\n+      regulate_interleaved_cycles();\n+    }\n+  } else {\n+    regulate_heap();\n+  }\n+\n+  log_info(gc)(\"%s: Done.\", name());\n+}\n+\n+void ShenandoahRegulatorThread::regulate_concurrent_cycles() {\n+  assert(_young_heuristics != nullptr, \"Need young heuristics.\");\n+  assert(_old_heuristics != nullptr, \"Need old heuristics.\");\n+\n+  while (!should_terminate()) {\n+    ShenandoahControlThread::GCMode mode = _control_thread->gc_mode();\n+    if (mode == ShenandoahControlThread::none) {\n+      if (should_unload_classes()) {\n+        if (_control_thread->request_concurrent_gc(GLOBAL)) {\n+          log_info(gc)(\"Heuristics request for global (unload classes) accepted.\");\n+        }\n+      } else {\n+        if (start_old_cycle()) {\n+          log_info(gc)(\"Heuristics request for old collection accepted\");\n+        } else if (start_young_cycle()) {\n+          log_info(gc)(\"Heuristics request for young collection accepted\");\n+        }\n+      }\n+    } else if (mode == ShenandoahControlThread::servicing_old) {\n+      if (start_young_cycle()) {\n+        log_info(gc)(\"Heuristics request to interrupt old for young collection accepted\");\n+      }\n+    }\n+\n+    regulator_sleep();\n+  }\n+}\n+\n+void ShenandoahRegulatorThread::regulate_interleaved_cycles() {\n+  assert(_young_heuristics != nullptr, \"Need young heuristics.\");\n+  assert(_global_heuristics != nullptr, \"Need global heuristics.\");\n+\n+  while (!should_terminate()) {\n+    if (_control_thread->gc_mode() == ShenandoahControlThread::none) {\n+      if (start_global_cycle()) {\n+        log_info(gc)(\"Heuristics request for global collection accepted.\");\n+      } else if (start_young_cycle()) {\n+        log_info(gc)(\"Heuristics request for young collection accepted.\");\n+      }\n+    }\n+\n+    regulator_sleep();\n+  }\n+}\n+\n+void ShenandoahRegulatorThread::regulate_heap() {\n+  assert(_global_heuristics != nullptr, \"Need global heuristics.\");\n+\n+  while (!should_terminate()) {\n+    if (_control_thread->gc_mode() == ShenandoahControlThread::none) {\n+      if (start_global_cycle()) {\n+        log_info(gc)(\"Heuristics request for global collection accepted.\");\n+      }\n+    }\n+\n+    regulator_sleep();\n+  }\n+}\n+\n+void ShenandoahRegulatorThread::regulator_sleep() {\n+  \/\/ Wait before performing the next action. If allocation happened during this wait,\n+  \/\/ we exit sooner, to let heuristics re-evaluate new conditions. If we are at idle,\n+  \/\/ back off exponentially.\n+  double current = os::elapsedTime();\n+\n+  if (_heap_changed.try_unset()) {\n+    _sleep = ShenandoahControlIntervalMin;\n+  } else if ((current - _last_sleep_adjust_time) * 1000 > ShenandoahControlIntervalAdjustPeriod){\n+    _sleep = MIN2<int>(ShenandoahControlIntervalMax, MAX2(1, _sleep * 2));\n+    _last_sleep_adjust_time = current;\n+  }\n+\n+  os::naked_short_sleep(_sleep);\n+}\n+\n+bool ShenandoahRegulatorThread::start_old_cycle() {\n+  return _old_heuristics->should_start_gc() && _control_thread->request_concurrent_gc(OLD);\n+}\n+\n+bool ShenandoahRegulatorThread::start_young_cycle() {\n+  return _young_heuristics->should_start_gc() && _control_thread->request_concurrent_gc(YOUNG);\n+}\n+\n+bool ShenandoahRegulatorThread::start_global_cycle() {\n+  return _global_heuristics->should_start_gc() && _control_thread->request_concurrent_gc(GLOBAL);\n+}\n+\n+void ShenandoahRegulatorThread::stop_service() {\n+  log_info(gc)(\"%s: Stop requested.\", name());\n+}\n+\n+bool ShenandoahRegulatorThread::should_unload_classes() {\n+  \/\/ The heuristics delegate this decision to the collector policy, which is based on the number\n+  \/\/ of cycles started.\n+  return _global_heuristics->should_unload_classes();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRegulatorThread.cpp","additions":163,"deletions":0,"binary":false,"changes":163,"status":"added"},{"patch":"@@ -0,0 +1,91 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com, Inc. and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHREGULATORTHREAD_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHREGULATORTHREAD_HPP\n+\n+#include \"gc\/shared\/concurrentGCThread.hpp\"\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n+#include \"runtime\/mutex.hpp\"\n+\n+class ShenandoahHeuristics;\n+class ShenandoahControlThread;\n+\n+\/*\n+ * The purpose of this class (and thread) is to allow us to continue\n+ * to evaluate heuristics during a garbage collection. This is necessary\n+ * to allow young generation collections to interrupt and old generation\n+ * collection which is in-progress. This puts heuristic triggers on the\n+ * same footing as other gc requests (alloc failure, System.gc, etc.).\n+ * However, this regulator does not block after submitting a gc request.\n+ *\n+ * We could use a PeriodicTask for this, but this thread will sleep longer\n+ * when the allocation rate is lower and PeriodicTasks cannot adjust their\n+ * sleep time.\n+ *\/\n+class ShenandoahRegulatorThread: public ConcurrentGCThread {\n+  friend class VMStructs;\n+\n+ public:\n+  explicit ShenandoahRegulatorThread(ShenandoahControlThread* control_thread);\n+\n+  const char* name() const { return \"ShenandoahRegulatorThread\";}\n+\n+  \/\/ This is called from allocation path, and thus should be fast.\n+  void notify_heap_changed() {\n+    \/\/ Notify that something had changed.\n+    if (_heap_changed.is_unset()) {\n+      _heap_changed.set();\n+    }\n+  }\n+\n+ protected:\n+  void run_service();\n+  void stop_service();\n+\n+ private:\n+  void regulate_interleaved_cycles();\n+  void regulate_concurrent_cycles();\n+  void regulate_heap();\n+\n+  bool start_old_cycle();\n+  bool start_young_cycle();\n+  bool start_global_cycle();\n+\n+  bool should_unload_classes();\n+\n+  ShenandoahSharedFlag _heap_changed;\n+  ShenandoahControlThread* _control_thread;\n+  ShenandoahHeuristics* _young_heuristics;\n+  ShenandoahHeuristics* _old_heuristics;\n+  ShenandoahHeuristics* _global_heuristics;\n+\n+  int _sleep;\n+  double _last_sleep_adjust_time;\n+\n+  void regulator_sleep();\n+};\n+\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHREGULATORTHREAD_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRegulatorThread.hpp","additions":91,"deletions":0,"binary":false,"changes":91,"status":"added"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -56,1 +57,1 @@\n-void ShenandoahRootVerifier::roots_do(OopClosure* oops) {\n+void ShenandoahRootVerifier::roots_do(OopIterateClosure* oops) {\n@@ -70,0 +71,6 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  if (heap->mode()->is_generational() && heap->is_gc_generation_young()) {\n+    shenandoah_assert_safepoint();\n+    heap->card_scan()->roots_do(oops);\n+  }\n+\n@@ -76,1 +83,1 @@\n-void ShenandoahRootVerifier::strong_roots_do(OopClosure* oops) {\n+void ShenandoahRootVerifier::strong_roots_do(OopIterateClosure* oops) {\n@@ -86,0 +93,6 @@\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  if (heap->mode()->is_generational() && heap->is_gc_generation_young()) {\n+    heap->card_scan()->roots_do(oops);\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRootVerifier.cpp","additions":15,"deletions":2,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -44,2 +44,2 @@\n-  static void roots_do(OopClosure* cl);\n-  static void strong_roots_do(OopClosure* cl);\n+  static void roots_do(OopIterateClosure* cl);\n+  static void strong_roots_do(OopIterateClosure* cl);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRootVerifier.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -39,0 +40,1 @@\n+template<GenerationMode GENERATION>\n@@ -40,1 +42,1 @@\n-private:\n+ private:\n@@ -45,2 +47,3 @@\n-  inline void do_oop_work(T* p);\n-public:\n+    inline void do_oop_work(T* p);\n+\n+ public:\n@@ -53,3 +56,4 @@\n-ShenandoahInitMarkRootsClosure::ShenandoahInitMarkRootsClosure(ShenandoahObjToScanQueue* q) :\n-  _queue(q),\n-  _mark_context(ShenandoahHeap::heap()->marking_context()) {\n+template<GenerationMode GENERATION>\n+ShenandoahInitMarkRootsClosure<GENERATION>::ShenandoahInitMarkRootsClosure(ShenandoahObjToScanQueue* q) :\n+_queue(q),\n+_mark_context(ShenandoahHeap::heap()->marking_context()) {\n@@ -58,0 +62,1 @@\n+template <GenerationMode GENERATION>\n@@ -59,2 +64,3 @@\n-void ShenandoahInitMarkRootsClosure::do_oop_work(T* p) {\n-  ShenandoahMark::mark_through_ref<T>(p, _queue, _mark_context, false);\n+void ShenandoahInitMarkRootsClosure<GENERATION>::do_oop_work(T* p) {\n+  \/\/ Only called from STW mark, should not be used to bootstrap old generation marking.\n+  ShenandoahMark::mark_through_ref<T, GENERATION>(p, _queue, nullptr, _mark_context, false);\n@@ -83,2 +89,2 @@\n-ShenandoahSTWMark::ShenandoahSTWMark(bool full_gc) :\n-  ShenandoahMark(),\n+ShenandoahSTWMark::ShenandoahSTWMark(ShenandoahGeneration* generation, bool full_gc) :\n+  ShenandoahMark(generation),\n@@ -86,1 +92,1 @@\n-  _terminator(ShenandoahHeap::heap()->workers()->active_workers(), ShenandoahHeap::heap()->marking_context()->task_queues()),\n+  _terminator(ShenandoahHeap::heap()->workers()->active_workers(), task_queues()),\n@@ -94,1 +100,1 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = heap->active_generation()->ref_processor();\n@@ -113,0 +119,5 @@\n+    if (_generation->generation_mode() == YOUNG) {\n+      \/\/ But only scan the remembered set for young generation.\n+      _generation->scan_remembered_set(false \/* is_concurrent *\/);\n+    }\n+\n@@ -120,1 +131,1 @@\n-  heap->mark_complete_marking_context();\n+  _generation->set_mark_complete();\n@@ -129,2 +140,14 @@\n-  ShenandoahInitMarkRootsClosure  init_mark(task_queues()->queue(worker_id));\n-  _root_scanner.roots_do(&init_mark, worker_id);\n+  switch (_generation->generation_mode()) {\n+    case GLOBAL: {\n+      ShenandoahInitMarkRootsClosure<GLOBAL> init_mark(task_queues()->queue(worker_id));\n+      _root_scanner.roots_do(&init_mark, worker_id);\n+      break;\n+    }\n+    case YOUNG: {\n+      ShenandoahInitMarkRootsClosure<YOUNG> init_mark(task_queues()->queue(worker_id));\n+      _root_scanner.roots_do(&init_mark, worker_id);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -136,1 +159,1 @@\n-  ShenandoahReferenceProcessor* rp = ShenandoahHeap::heap()->ref_processor();\n+  ShenandoahReferenceProcessor* rp = ShenandoahHeap::heap()->active_generation()->ref_processor();\n@@ -139,1 +162,2 @@\n-  mark_loop(worker_id, &_terminator, rp,\n+  mark_loop(_generation->generation_mode(),\n+            worker_id, &_terminator, rp,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSTWMark.cpp","additions":41,"deletions":17,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+class ShenandoahGeneration;\n@@ -40,1 +41,1 @@\n- ShenandoahSTWMark(bool full_gc);\n+ ShenandoahSTWMark(ShenandoahGeneration* generation, bool full_gc);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSTWMark.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,323 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com, Inc. or its affiliates.  All rights reserved.\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahReferenceProcessor.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+#include \"logging\/log.hpp\"\n+\n+ShenandoahDirectCardMarkRememberedSet::ShenandoahDirectCardMarkRememberedSet(ShenandoahCardTable* card_table, size_t total_card_count) {\n+  _heap = ShenandoahHeap::heap();\n+  _card_table = card_table;\n+  _total_card_count = total_card_count;\n+  _cluster_count = total_card_count \/ ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  _card_shift = CardTable::card_shift();\n+\n+  _byte_map = _card_table->byte_for_index(0);\n+\n+  _whole_heap_base = _card_table->addr_for(_byte_map);\n+  _byte_map_base = _byte_map - (uintptr_t(_whole_heap_base) >> _card_shift);\n+\n+  assert(total_card_count % ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster == 0, \"Invalid card count.\");\n+  assert(total_card_count > 0, \"Card count cannot be zero.\");\n+}\n+\n+ShenandoahScanRememberedTask::ShenandoahScanRememberedTask(ShenandoahObjToScanQueueSet* queue_set,\n+                                                           ShenandoahObjToScanQueueSet* old_queue_set,\n+                                                           ShenandoahReferenceProcessor* rp,\n+                                                           ShenandoahRegionChunkIterator* work_list, bool is_concurrent) :\n+  WorkerTask(\"Scan Remembered Set\"),\n+  _queue_set(queue_set), _old_queue_set(old_queue_set), _rp(rp), _work_list(work_list), _is_concurrent(is_concurrent) {}\n+\n+void ShenandoahScanRememberedTask::work(uint worker_id) {\n+  if (_is_concurrent) {\n+    \/\/ This sets up a thread local reference to the worker_id which is needed by the weak reference processor.\n+    ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+    ShenandoahSuspendibleThreadSetJoiner stsj(ShenandoahSuspendibleWorkers);\n+    do_work(worker_id);\n+  } else {\n+    \/\/ This sets up a thread local reference to the worker_id which is needed by the weak reference processor.\n+    ShenandoahParallelWorkerSession worker_session(worker_id);\n+    do_work(worker_id);\n+  }\n+}\n+\n+void ShenandoahScanRememberedTask::do_work(uint worker_id) {\n+  ShenandoahWorkerTimingsTracker x(ShenandoahPhaseTimings::init_scan_rset, ShenandoahPhaseTimings::ScanClusters, worker_id);\n+\n+  ShenandoahObjToScanQueue* q = _queue_set->queue(worker_id);\n+  ShenandoahObjToScanQueue* old = _old_queue_set == nullptr ? nullptr : _old_queue_set->queue(worker_id);\n+  ShenandoahMarkRefsClosure<YOUNG> cl(q, _rp, old);\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  RememberedScanner* scanner = heap->card_scan();\n+\n+  \/\/ set up thread local closure for shen ref processor\n+  _rp->set_mark_closure(worker_id, &cl);\n+  struct ShenandoahRegionChunk assignment;\n+  while (_work_list->next(&assignment)) {\n+    ShenandoahHeapRegion* region = assignment._r;\n+    log_debug(gc)(\"ShenandoahScanRememberedTask::do_work(%u), processing slice of region \"\n+                  SIZE_FORMAT \" at offset \" SIZE_FORMAT \", size: \" SIZE_FORMAT,\n+                  worker_id, region->index(), assignment._chunk_offset, assignment._chunk_size);\n+    if (region->affiliation() == OLD_GENERATION) {\n+      size_t cluster_size =\n+        CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+      size_t clusters = assignment._chunk_size \/ cluster_size;\n+      assert(clusters * cluster_size == assignment._chunk_size, \"Chunk assignments must align on cluster boundaries\");\n+      HeapWord* end_of_range = region->bottom() + assignment._chunk_offset + assignment._chunk_size;\n+\n+      \/\/ During concurrent mark, region->top() equals TAMS with respect to the current young-gen pass.\n+      if (end_of_range > region->top()) {\n+        end_of_range = region->top();\n+      }\n+      scanner->process_region_slice(region, assignment._chunk_offset, clusters, end_of_range, &cl, false, worker_id);\n+    }\n+#ifdef ENABLE_REMEMBERED_SET_CANCELLATION\n+    \/\/ This check is currently disabled to avoid crashes that occur\n+    \/\/ when we try to cancel remembered set scanning; it should be re-enabled\n+    \/\/ after the issues are fixed, as it would allow more prompt cancellation and\n+    \/\/ transition to degenerated \/ full GCs. Note that work that has been assigned\/\n+    \/\/ claimed above must be completed before we return here upon cancellation.\n+    if (heap->check_cancelled_gc_and_yield(_is_concurrent)) {\n+      return;\n+    }\n+#endif\n+  }\n+}\n+\n+size_t ShenandoahRegionChunkIterator::calc_regular_group_size() {\n+  \/\/ The group size is calculated from the number of regions.  Suppose the heap has N regions.  The first group processes\n+  \/\/ N\/2 regions.  The second group processes N\/4 regions, the third group N\/8 regions and so on.\n+  \/\/ Note that infinite series N\/2 + N\/4 + N\/8 + N\/16 + ...  sums to N.\n+  \/\/\n+  \/\/ The normal group size is the number of regions \/ 2.\n+  \/\/\n+  \/\/ In the case that the region_size_words is greater than _maximum_chunk_size_words, the first group_size is\n+  \/\/ larger than the normal group size because each chunk in the group will be smaller than the region size.\n+  \/\/\n+  \/\/ The last group also has more than the normal entries because it finishes the total scanning effort.  The chunk sizes are\n+  \/\/ different for each group.  The intention is that the first group processes roughly half of the heap, the second processes\n+  \/\/ half of the remaining heap, the third processes half of what remains and so on.  The smallest chunk size\n+  \/\/ is represented by _smallest_chunk_size_words.  We do not divide work any smaller than this.\n+  \/\/\n+\n+  size_t group_size = _heap->num_regions() \/ 2;\n+  return group_size;\n+}\n+\n+size_t ShenandoahRegionChunkIterator::calc_first_group_chunk_size_b4_rebalance() {\n+  size_t words_in_first_chunk = ShenandoahHeapRegion::region_size_words();\n+  return words_in_first_chunk;\n+}\n+\n+size_t ShenandoahRegionChunkIterator::calc_num_groups() {\n+  size_t total_heap_size = _heap->num_regions() * ShenandoahHeapRegion::region_size_words();\n+  size_t num_groups = 0;\n+  size_t cumulative_group_span = 0;\n+  size_t current_group_span = _first_group_chunk_size_b4_rebalance * _regular_group_size;\n+  size_t smallest_group_span = smallest_chunk_size_words() * _regular_group_size;\n+  while ((num_groups < _maximum_groups) && (cumulative_group_span + current_group_span <= total_heap_size)) {\n+    num_groups++;\n+    cumulative_group_span += current_group_span;\n+    if (current_group_span <= smallest_group_span) {\n+      break;\n+    } else {\n+      current_group_span \/= 2;    \/\/ Each group spans half of what the preceding group spanned.\n+    }\n+  }\n+  \/\/ Loop post condition:\n+  \/\/   num_groups <= _maximum_groups\n+  \/\/   cumulative_group_span is the memory spanned by num_groups\n+  \/\/   current_group_span is the span of the last fully populated group (assuming loop iterates at least once)\n+  \/\/   each of num_groups is fully populated with _regular_group_size chunks in each\n+  \/\/ Non post conditions:\n+  \/\/   cumulative_group_span may be less than total_heap size for one or more of the folowing reasons\n+  \/\/   a) The number of regions remaining to be spanned is smaller than a complete group, or\n+  \/\/   b) We have filled up all groups through _maximum_groups and still have not spanned all regions\n+\n+  if (cumulative_group_span < total_heap_size) {\n+    \/\/ We've got more regions to span\n+    if ((num_groups < _maximum_groups) && (current_group_span > smallest_group_span)) {\n+      num_groups++;             \/\/ Place all remaining regions into a new not-full group (chunk_size half that of previous group)\n+    }\n+    \/\/ Else we are unable to create a new group because we've exceed the number of allowed groups or have reached the\n+    \/\/ minimum chunk size.\n+\n+    \/\/ Any remaining regions will be treated as if they are part of the most recently created group.  This group will\n+    \/\/ have more than _regular_group_size chunks within it.\n+  }\n+  return num_groups;\n+}\n+\n+size_t ShenandoahRegionChunkIterator::calc_total_chunks() {\n+  size_t region_size_words = ShenandoahHeapRegion::region_size_words();\n+  size_t unspanned_heap_size = _heap->num_regions() * region_size_words;\n+  size_t num_chunks = 0;\n+  size_t cumulative_group_span = 0;\n+  size_t current_group_span = _first_group_chunk_size_b4_rebalance * _regular_group_size;\n+  size_t smallest_group_span = smallest_chunk_size_words() * _regular_group_size;\n+\n+  \/\/ The first group gets special handling because the first chunk size can be no larger than _largest_chunk_size_words\n+  if (region_size_words > _maximum_chunk_size_words) {\n+    \/\/ In the case that we shrink the first group's chunk size, certain other groups will also be subsumed within the first group\n+    size_t effective_chunk_size = _first_group_chunk_size_b4_rebalance;\n+    while (effective_chunk_size >= _maximum_chunk_size_words) {\n+      num_chunks += current_group_span \/ _maximum_chunk_size_words;\n+      unspanned_heap_size -= current_group_span;\n+      effective_chunk_size \/= 2;\n+      current_group_span \/= 2;\n+    }\n+  } else {\n+    num_chunks = _regular_group_size;\n+    unspanned_heap_size -= current_group_span;\n+    current_group_span \/= 2;\n+  }\n+  size_t spanned_groups = 1;\n+  while (unspanned_heap_size > 0) {\n+    if (current_group_span <= unspanned_heap_size) {\n+      unspanned_heap_size -= current_group_span;\n+      num_chunks += _regular_group_size;\n+      spanned_groups++;\n+\n+      \/\/ _num_groups is the number of groups required to span the configured heap size.  We are not allowed\n+      \/\/ to change the number of groups.  The last group is responsible for spanning all chunks not spanned\n+      \/\/ by previously processed groups.\n+      if (spanned_groups >= _num_groups) {\n+        \/\/ The last group has more than _regular_group_size entries.\n+        size_t chunk_span = current_group_span \/ _regular_group_size;\n+        size_t extra_chunks = unspanned_heap_size \/ chunk_span;\n+        assert (extra_chunks * chunk_span == unspanned_heap_size, \"Chunks must precisely span regions\");\n+        num_chunks += extra_chunks;\n+        return num_chunks;\n+      } else if (current_group_span <= smallest_group_span) {\n+        \/\/ We cannot introduce new groups because we've reached the lower bound on group size.  So this last\n+        \/\/ group may hold extra chunks.\n+        size_t chunk_span = smallest_chunk_size_words();\n+        size_t extra_chunks = unspanned_heap_size \/ chunk_span;\n+        assert (extra_chunks * chunk_span == unspanned_heap_size, \"Chunks must precisely span regions\");\n+        num_chunks += extra_chunks;\n+        return num_chunks;\n+      } else {\n+        current_group_span \/= 2;\n+      }\n+    } else {\n+      \/\/ This last group has fewer than _regular_group_size entries.\n+      size_t chunk_span = current_group_span \/ _regular_group_size;\n+      size_t last_group_size = unspanned_heap_size \/ chunk_span;\n+      assert (last_group_size * chunk_span == unspanned_heap_size, \"Chunks must precisely span regions\");\n+      num_chunks += last_group_size;\n+      return num_chunks;\n+    }\n+  }\n+  return num_chunks;\n+}\n+\n+ShenandoahRegionChunkIterator::ShenandoahRegionChunkIterator(size_t worker_count) :\n+    ShenandoahRegionChunkIterator(ShenandoahHeap::heap(), worker_count)\n+{\n+}\n+\n+ShenandoahRegionChunkIterator::ShenandoahRegionChunkIterator(ShenandoahHeap* heap, size_t worker_count) :\n+    _heap(heap),\n+    _regular_group_size(calc_regular_group_size()),\n+    _first_group_chunk_size_b4_rebalance(calc_first_group_chunk_size_b4_rebalance()),\n+    _num_groups(calc_num_groups()),\n+    _total_chunks(calc_total_chunks()),\n+    _index(0)\n+{\n+#ifdef ASSERT\n+  size_t expected_chunk_size_words = _clusters_in_smallest_chunk * CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  assert(smallest_chunk_size_words() == expected_chunk_size_words, \"_smallest_chunk_size (\" SIZE_FORMAT\") is not valid because it does not equal (\" SIZE_FORMAT \")\",\n+         smallest_chunk_size_words(), expected_chunk_size_words);\n+#endif\n+  assert(_num_groups <= _maximum_groups,\n+         \"The number of remembered set scanning groups must be less than or equal to maximum groups\");\n+  assert(smallest_chunk_size_words() << (_maximum_groups - 1) == _maximum_chunk_size_words,\n+         \"Maximum number of groups needs to span maximum chunk size to smallest chunk size\");\n+\n+  size_t words_in_region = ShenandoahHeapRegion::region_size_words();\n+  _region_index[0] = 0;\n+  _group_offset[0] = 0;\n+  if (words_in_region > _maximum_chunk_size_words) {\n+    \/\/ In the case that we shrink the first group's chunk size, certain other groups will also be subsumed within the first group\n+    size_t num_chunks = 0;\n+    size_t effective_chunk_size = _first_group_chunk_size_b4_rebalance;\n+    size_t  current_group_span = effective_chunk_size * _regular_group_size;\n+    while (effective_chunk_size >= _maximum_chunk_size_words) {\n+      num_chunks += current_group_span \/ _maximum_chunk_size_words;\n+      effective_chunk_size \/= 2;\n+      current_group_span \/= 2;\n+    }\n+    _group_entries[0] = num_chunks;\n+    _group_chunk_size[0] = _maximum_chunk_size_words;\n+  } else {\n+    _group_entries[0] = _regular_group_size;\n+    _group_chunk_size[0] = _first_group_chunk_size_b4_rebalance;\n+  }\n+\n+  size_t previous_group_span = _group_entries[0] * _group_chunk_size[0];\n+  for (size_t i = 1; i < _num_groups; i++) {\n+    size_t previous_group_entries = (i == 1)? _group_entries[0]: (_group_entries[i-1] - _group_entries[i-2]);\n+    _group_chunk_size[i] = _group_chunk_size[i-1] \/ 2;\n+    size_t chunks_in_group = _regular_group_size;\n+    size_t this_group_span = _group_chunk_size[i] * chunks_in_group;\n+    size_t total_span_of_groups = previous_group_span + this_group_span;\n+    _region_index[i] = previous_group_span \/ words_in_region;\n+    _group_offset[i] = previous_group_span % words_in_region;\n+    _group_entries[i] = _group_entries[i-1] + _regular_group_size;\n+    previous_group_span = total_span_of_groups;\n+  }\n+  if (_group_entries[_num_groups-1] < _total_chunks) {\n+    assert((_total_chunks - _group_entries[_num_groups-1]) * _group_chunk_size[_num_groups-1] + previous_group_span ==\n+           heap->num_regions() * words_in_region, \"Total region chunks (\" SIZE_FORMAT\n+           \") do not span total heap regions (\" SIZE_FORMAT \")\", _total_chunks, _heap->num_regions());\n+    previous_group_span += (_total_chunks - _group_entries[_num_groups-1]) * _group_chunk_size[_num_groups-1];\n+    _group_entries[_num_groups-1] = _total_chunks;\n+  }\n+  assert(previous_group_span == heap->num_regions() * words_in_region, \"Total region chunks (\" SIZE_FORMAT\n+         \") do not span total heap regions (\" SIZE_FORMAT \"): \" SIZE_FORMAT \" does not equal \" SIZE_FORMAT,\n+         _total_chunks, _heap->num_regions(), previous_group_span, heap->num_regions() * words_in_region);\n+\n+  \/\/ Not necessary, but keeps things tidy\n+  for (size_t i = _num_groups; i < _maximum_groups; i++) {\n+    _region_index[i] = 0;\n+    _group_offset[i] = 0;\n+    _group_entries[i] = _group_entries[i-1];\n+    _group_chunk_size[i] = 0;\n+  }\n+}\n+\n+void ShenandoahRegionChunkIterator::reset() {\n+  _index = 0;\n+}\n+\n+ShenandoahVerifyNoYoungRefsClosure::ShenandoahVerifyNoYoungRefsClosure():\n+  _heap(ShenandoahHeap::heap()) {\n+  assert(_heap->mode()->is_generational(), \"Don't use when non-generational\");\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.cpp","additions":323,"deletions":0,"binary":false,"changes":323,"status":"added"},{"patch":"@@ -0,0 +1,1078 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com, Inc. or its affiliates.  All rights reserved.\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBERED_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBERED_HPP\n+\n+\/\/ Terminology used within this source file:\n+\/\/\n+\/\/ Card Entry:   This is the information that identifies whether a\n+\/\/               particular card-table entry is Clean or Dirty.  A clean\n+\/\/               card entry denotes that the associated memory does not\n+\/\/               hold references to young-gen memory.\n+\/\/\n+\/\/ Card Region, aka\n+\/\/ Card Memory:  This is the region of memory that is assocated with a\n+\/\/               particular card entry.\n+\/\/\n+\/\/ Card Cluster: A card cluster represents 64 card entries.  A card\n+\/\/               cluster is the minimal amount of work performed at a\n+\/\/               time by a parallel thread.  Note that the work required\n+\/\/               to scan a card cluster is somewhat variable in that the\n+\/\/               required effort depends on how many cards are dirty, how\n+\/\/               many references are held within the objects that span a\n+\/\/               DIRTY card's memory, and on the size of the object\n+\/\/               that spans the end of a DIRTY card's memory (because\n+\/\/               that object, if it's not an array, may need to be scanned in\n+\/\/               its entirety, when the object is imprecisely dirtied. Imprecise\n+\/\/               dirtying is when the card corresponding to the object header\n+\/\/               is dirtied, rather than the card on which the updated field lives).\n+\/\/               To better balance work amongst them, parallel worker threads dynamically\n+\/\/               claim clusters and are flexible in the number of clusters they\n+\/\/               process.\n+\/\/\n+\/\/ A cluster represents a \"natural\" quantum of work to be performed by\n+\/\/ a parallel GC thread's background remembered set scanning efforts.\n+\/\/ The notion of cluster is similar to the notion of stripe in the\n+\/\/ implementation of parallel GC card scanning.  However, a cluster is\n+\/\/ typically smaller than a stripe, enabling finer grain division of\n+\/\/ labor between multiple threads, and potentially better load balancing\n+\/\/ when dirty cards are not uniformly distributed in the heap, as is often\n+\/\/ the case with generational workloads where more recently promoted objects\n+\/\/ may be dirtied more frequently that older objects.\n+\/\/\n+\/\/ For illustration, consider the following possible JVM configurations:\n+\/\/\n+\/\/   Scenario 1:\n+\/\/     RegionSize is 128 MB\n+\/\/     Span of a card entry is 512 B\n+\/\/     Each card table entry consumes 1 B\n+\/\/     Assume one long word (8 B)of the card table represents a cluster.\n+\/\/       This long word holds 8 card table entries, spanning a\n+\/\/       total of 8*512 B = 4 KB of the heap\n+\/\/     The number of clusters per region is 128 MB \/ 4 KB = 32 K\n+\/\/\n+\/\/   Scenario 2:\n+\/\/     RegionSize is 128 MB\n+\/\/     Span of each card entry is 128 B\n+\/\/     Each card table entry consumes 1 bit\n+\/\/     Assume one int word (4 B) of the card table represents a cluster.\n+\/\/       This int word holds 32 b\/1 b = 32 card table entries, spanning a\n+\/\/       total of 32 * 128 B = 4 KB of the heap\n+\/\/     The number of clusters per region is 128 MB \/ 4 KB = 32 K\n+\/\/\n+\/\/   Scenario 3:\n+\/\/     RegionSize is 128 MB\n+\/\/     Span of each card entry is 512 B\n+\/\/     Each card table entry consumes 1 bit\n+\/\/     Assume one long word (8 B) of card table represents a cluster.\n+\/\/       This long word holds 64 b\/ 1 b = 64 card table entries, spanning a\n+\/\/       total of 64 * 512 B = 32 KB of the heap\n+\/\/     The number of clusters per region is 128 MB \/ 32 KB = 4 K\n+\/\/\n+\/\/ At the start of a new young-gen concurrent mark pass, the gang of\n+\/\/ Shenandoah worker threads collaborate in performing the following\n+\/\/ actions:\n+\/\/\n+\/\/  Let old_regions = number of ShenandoahHeapRegion comprising\n+\/\/    old-gen memory\n+\/\/  Let region_size = ShenandoahHeapRegion::region_size_bytes()\n+\/\/    represent the number of bytes in each region\n+\/\/  Let clusters_per_region = region_size \/ 512\n+\/\/  Let rs represent the relevant RememberedSet implementation\n+\/\/    (an instance of ShenandoahDirectCardMarkRememberedSet or an instance\n+\/\/     of a to-be-implemented ShenandoahBufferWithSATBRememberedSet)\n+\/\/\n+\/\/  for each ShenandoahHeapRegion old_region in the whole heap\n+\/\/    determine the cluster number of the first cluster belonging\n+\/\/      to that region\n+\/\/    for each cluster contained within that region\n+\/\/      Assure that exactly one worker thread processes each\n+\/\/      cluster, each thread making a series of invocations of the\n+\/\/      following:\n+\/\/\n+\/\/        rs->process_clusters(worker_id, ReferenceProcessor *,\n+\/\/                             ShenandoahConcurrentMark *, cluster_no, cluster_count,\n+\/\/                             HeapWord *end_of_range, OopClosure *oops);\n+\/\/\n+\/\/  For efficiency, divide up the clusters so that different threads\n+\/\/  are responsible for processing different clusters.  Processing costs\n+\/\/  may vary greatly between clusters for the following reasons:\n+\/\/\n+\/\/        a) some clusters contain mostly dirty cards and other\n+\/\/           clusters contain mostly clean cards\n+\/\/        b) some clusters contain mostly primitive data and other\n+\/\/           clusters contain mostly reference data\n+\/\/        c) some clusters are spanned by very large non-array objects that\n+\/\/           begin in some other cluster.  When a large non-array object\n+\/\/           beginning in a preceding cluster spans large portions of\n+\/\/           this cluster, then because of imprecise dirtying, the\n+\/\/           portion of the object in this cluster may be clean, but\n+\/\/           will need to be processed by the worker responsible for\n+\/\/           this cluster, potentially increasing its work.\n+\/\/        d) in the case that the end of this cluster is spanned by a\n+\/\/           very large non-array object, the worker for this cluster will\n+\/\/           be responsible for processing the portion of the object\n+\/\/           in this cluster.\n+\/\/\n+\/\/ Though an initial division of labor between marking threads may\n+\/\/ assign equal numbers of clusters to be scanned by each thread, it\n+\/\/ should be expected that some threads will finish their assigned\n+\/\/ work before others.  Therefore, some amount of the full remembered\n+\/\/ set scanning effort should be held back and assigned incrementally\n+\/\/ to the threads that end up with excess capacity.  Consider the\n+\/\/ following strategy for dividing labor:\n+\/\/\n+\/\/        1. Assume there are 8 marking threads and 1024 remembered\n+\/\/           set clusters to be scanned.\n+\/\/        2. Assign each thread to scan 64 clusters.  This leaves\n+\/\/           512 (1024 - (8*64)) clusters to still be scanned.\n+\/\/        3. As the 8 server threads complete previous cluster\n+\/\/           scanning assignments, issue each of the next 8 scanning\n+\/\/           assignments as units of 32 additional cluster each.\n+\/\/           In the case that there is high variance in effort\n+\/\/           associated with previous cluster scanning assignments,\n+\/\/           multiples of these next assignments may be serviced by\n+\/\/           the server threads that were previously assigned lighter\n+\/\/           workloads.\n+\/\/        4. Make subsequent scanning assignments as follows:\n+\/\/             a) 8 assignments of size 16 clusters\n+\/\/             b) 8 assignments of size 8 clusters\n+\/\/             c) 16 assignments of size 4 clusters\n+\/\/\n+\/\/    When there is no more remembered set processing work to be\n+\/\/    assigned to a newly idled worker thread, that thread can move\n+\/\/    on to work on other tasks associated with root scanning until such\n+\/\/    time as all clusters have been examined.\n+\/\/\n+\/\/ Remembered set scanning is designed to run concurrently with\n+\/\/ mutator threads, with multiple concurrent workers. Furthermore, the\n+\/\/ current implementation of remembered set scanning never clears a\n+\/\/ card once it has been marked.\n+\/\/\n+\/\/ These limitations will be addressed in future enhancements to the\n+\/\/ existing implementation.\n+\n+#include <stdint.h>\n+#include \"gc\/shared\/workerThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCardStats.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahNumberSeq.hpp\"\n+#include \"gc\/shenandoah\/shenandoahTaskqueue.hpp\"\n+#include \"memory\/iterator.hpp\"\n+\n+class ShenandoahReferenceProcessor;\n+class ShenandoahConcurrentMark;\n+class ShenandoahHeap;\n+class ShenandoahRegionIterator;\n+class ShenandoahMarkingContext;\n+\n+class CardTable;\n+typedef CardTable::CardValue CardValue;\n+\n+class ShenandoahDirectCardMarkRememberedSet: public CHeapObj<mtGC> {\n+\n+private:\n+\n+  \/\/ Use symbolic constants defined in cardTable.hpp\n+  \/\/  CardTable::card_shift = 9;\n+  \/\/  CardTable::card_size = 512;\n+  \/\/  CardTable::card_size_in_words = 64;\n+  \/\/  CardTable::clean_card_val()\n+  \/\/  CardTable::dirty_card_val()\n+\n+  ShenandoahHeap *_heap;\n+  ShenandoahCardTable *_card_table;\n+  size_t _card_shift;\n+  size_t _total_card_count;\n+  size_t _cluster_count;\n+  HeapWord *_whole_heap_base;   \/\/ Points to first HeapWord of data contained within heap memory\n+  CardValue* _byte_map;         \/\/ Points to first entry within the card table\n+  CardValue* _byte_map_base;    \/\/ Points to byte_map minus the bias computed from address of heap memory\n+\n+public:\n+\n+  \/\/ count is the number of cards represented by the card table.\n+  ShenandoahDirectCardMarkRememberedSet(ShenandoahCardTable *card_table, size_t total_card_count);\n+  ~ShenandoahDirectCardMarkRememberedSet();\n+\n+  \/\/ Card index is zero-based relative to _byte_map.\n+  size_t last_valid_index() const;\n+  size_t total_cards() const;\n+  size_t card_index_for_addr(HeapWord *p) const;\n+  HeapWord *addr_for_card_index(size_t card_index) const;\n+  inline const CardValue* get_card_table_byte_map(bool write_table) const;\n+  inline bool is_card_dirty(size_t card_index) const;\n+  inline bool is_write_card_dirty(size_t card_index) const;\n+  inline void mark_card_as_dirty(size_t card_index);\n+  inline void mark_range_as_dirty(size_t card_index, size_t num_cards);\n+  inline void mark_card_as_clean(size_t card_index);\n+  inline void mark_read_card_as_clean(size_t card_index);\n+  inline void mark_range_as_clean(size_t card_index, size_t num_cards);\n+  inline bool is_card_dirty(HeapWord *p) const;\n+  inline void mark_card_as_dirty(HeapWord *p);\n+  inline void mark_range_as_dirty(HeapWord *p, size_t num_heap_words);\n+  inline void mark_card_as_clean(HeapWord *p);\n+  inline void mark_range_as_clean(HeapWord *p, size_t num_heap_words);\n+  inline size_t cluster_count() const;\n+\n+  \/\/ Called by GC thread at start of concurrent mark to exchange roles of read and write remembered sets.\n+  \/\/ Not currently used because mutator write barrier does not honor changes to the location of card table.\n+  void swap_remset() {  _card_table->swap_card_tables(); }\n+\n+  void merge_write_table(HeapWord* start, size_t word_count) {\n+    size_t card_index = card_index_for_addr(start);\n+    size_t num_cards = word_count \/ CardTable::card_size_in_words();\n+    size_t iterations = num_cards \/ (sizeof (intptr_t) \/ sizeof (CardValue));\n+    intptr_t* read_table_ptr = (intptr_t*) &(_card_table->read_byte_map())[card_index];\n+    intptr_t* write_table_ptr = (intptr_t*) &(_card_table->write_byte_map())[card_index];\n+    for (size_t i = 0; i < iterations; i++) {\n+      intptr_t card_value = *write_table_ptr;\n+      *read_table_ptr++ &= card_value;\n+      write_table_ptr++;\n+    }\n+  }\n+\n+  \/\/ Instead of swap_remset, the current implementation of concurrent remembered set scanning does reset_remset\n+  \/\/ in parallel threads, each invocation processing one entire HeapRegion at a time.  Processing of a region\n+  \/\/ consists of copying the write table to the read table and cleaning the write table.\n+  void reset_remset(HeapWord* start, size_t word_count) {\n+    size_t card_index = card_index_for_addr(start);\n+    size_t num_cards = word_count \/ CardTable::card_size_in_words();\n+    size_t iterations = num_cards \/ (sizeof (intptr_t) \/ sizeof (CardValue));\n+    intptr_t* read_table_ptr = (intptr_t*) &(_card_table->read_byte_map())[card_index];\n+    intptr_t* write_table_ptr = (intptr_t*) &(_card_table->write_byte_map())[card_index];\n+    for (size_t i = 0; i < iterations; i++) {\n+      *read_table_ptr++ = *write_table_ptr;\n+      *write_table_ptr++ = CardTable::clean_card_row_val();\n+    }\n+  }\n+\n+  \/\/ Called by GC thread after scanning old remembered set in order to prepare for next GC pass\n+  void clear_old_remset() {  _card_table->clear_read_table(); }\n+\n+};\n+\n+\/\/ A ShenandoahCardCluster represents the minimal unit of work\n+\/\/ performed by independent parallel GC threads during scanning of\n+\/\/ remembered sets.\n+\/\/\n+\/\/ The GC threads that perform card-table remembered set scanning may\n+\/\/ overwrite card-table entries to mark them as clean in the case that\n+\/\/ the associated memory no longer holds references to young-gen\n+\/\/ memory.  Rather than access the card-table entries directly, all GC\n+\/\/ thread access to card-table information is made by way of the\n+\/\/ ShenandoahCardCluster data abstraction.  This abstraction\n+\/\/ effectively manages access to multiple possible underlying\n+\/\/ remembered set implementations, including a traditional card-table\n+\/\/ approach and a SATB-based approach.\n+\/\/\n+\/\/ The API services represent a compromise between efficiency and\n+\/\/ convenience.\n+\/\/\n+\/\/ Multiple GC threads that scan the remembered set\n+\/\/ in parallel.  The desire is to divide the complete scanning effort\n+\/\/ into multiple clusters of work that can be independently processed\n+\/\/ by individual threads without need for synchronizing efforts\n+\/\/ between the work performed by each task.  The term \"cluster\" of\n+\/\/ work is similar to the term \"stripe\" as used in the implementation\n+\/\/ of Parallel GC.\n+\/\/\n+\/\/ Complexity arises when an object to be scanned crosses the boundary\n+\/\/ between adjacent cluster regions.  Here is the protocol that we currently\n+\/\/ follow:\n+\/\/\n+\/\/  1. The thread responsible for scanning the cards in a cluster modifies\n+\/\/     the associated card-table entries. Only cards that are dirty are\n+\/\/     processed, except as described below for the case of objects that\n+\/\/     straddle more than one card.\n+\/\/  2. Object Arrays are precisely dirtied, so only the portion of the obj-array\n+\/\/     that overlaps the range of dirty cards in its cluster are scanned\n+\/\/     by each worker thread. This holds for portions of obj-arrays that extend\n+\/\/     over clusters processed by different workers, with each worked responsible\n+\/\/     for scanning the portion of the obj-array overlapping the dirty cards in\n+\/\/     its cluster.\n+\/\/  3. Non-array objects are precisely dirtied by the interpreter and the compilers\n+\/\/     For such objects that extend over multiple cards, or even multiple clusters,\n+\/\/     the entire object is scanned by the worker that processes the (dirty) card on\n+\/\/     which the object's header lies. (However, GC workers should precisely dirty the\n+\/\/     cards with inter-regional\/inter-generational pointers in the body of this object,\n+\/\/     thus making subsequent scans potentially less expensive.) Such larger non-array\n+\/\/     objects are relatively rare.\n+\/\/\n+\/\/  A possible criticism:\n+\/\/  C. The representation of pointer location descriptive information\n+\/\/     within Klass representations is not designed for efficient\n+\/\/     \"random access\".  An alternative approach to this design would\n+\/\/     be to scan very large objects multiple times, once for each\n+\/\/     cluster that is spanned by the object's range.  This reduces\n+\/\/     unnecessary overscan, but it introduces different sorts of\n+\/\/     overhead effort:\n+\/\/       i) For each spanned cluster, we have to look up the start of\n+\/\/          the crossing object.\n+\/\/      ii) Each time we scan the very large object, we have to\n+\/\/          sequentially walk through its pointer location\n+\/\/          descriptors, skipping over all of the pointers that\n+\/\/          precede the start of the range of addresses that we\n+\/\/          consider relevant.\n+\n+\n+\/\/ Because old-gen heap memory is not necessarily contiguous, and\n+\/\/ because cards are not necessarily maintained for young-gen memory,\n+\/\/ consecutive card numbers do not necessarily correspond to consecutive\n+\/\/ address ranges.  For the traditional direct-card-marking\n+\/\/ implementation of this interface, consecutive card numbers are\n+\/\/ likely to correspond to contiguous regions of memory, but this\n+\/\/ should not be assumed.  Instead, rely only upon the following:\n+\/\/\n+\/\/  1. All card numbers for cards pertaining to the same\n+\/\/     ShenandoahHeapRegion are consecutively numbered.\n+\/\/  2. In the case that neighboring ShenandoahHeapRegions both\n+\/\/     represent old-gen memory, the card regions that span the\n+\/\/     boundary between these neighboring heap regions will be\n+\/\/     consecutively numbered.\n+\/\/  3. (A corollary) In the case that an old-gen object straddles the\n+\/\/     boundary between two heap regions, the card regions that\n+\/\/     correspond to the span of this object will be consecutively\n+\/\/     numbered.\n+\/\/\n+\/\/ ShenandoahCardCluster abstracts access to the remembered set\n+\/\/ and also keeps track of crossing map information to allow efficient\n+\/\/ resolution of object start addresses.\n+\/\/\n+\/\/ ShenandoahCardCluster supports all of the services of\n+\/\/ RememberedSet, plus it supports register_object() and lookup_object().\n+\/\/ Note that we only need to register the start addresses of the object that\n+\/\/ overlays the first address of a card; we need to do this for every card.\n+\/\/ In other words, register_object() checks if the object crosses a card boundary,\n+\/\/ and updates the offset value for each card that the object crosses into.\n+\/\/ For objects that don't straddle cards, nothing needs to be done.\n+\/\/\n+\/\/ The RememberedSet template parameter is intended to represent either\n+\/\/     ShenandoahDirectCardMarkRememberedSet, or a to-be-implemented\n+\/\/     ShenandoahBufferWithSATBRememberedSet.\n+template<typename RememberedSet>\n+class ShenandoahCardCluster: public CHeapObj<mtGC> {\n+\n+private:\n+  RememberedSet *_rs;\n+\n+public:\n+  static const size_t CardsPerCluster = 64;\n+\n+private:\n+  typedef struct cross_map { uint8_t first; uint8_t last; } xmap;\n+  typedef union crossing_info { uint16_t short_word; xmap offsets; } crossing_info;\n+\n+  \/\/ ObjectStartsInCardRegion bit is set within a crossing_info.offsets.start iff at least one object starts within\n+  \/\/ a particular card region.  We pack this bit into start byte under assumption that start byte is accessed less\n+  \/\/ frequently than last byte.  This is true when number of clean cards is greater than number of dirty cards.\n+  static const uint16_t ObjectStartsInCardRegion = 0x80;\n+  static const uint16_t FirstStartBits           = 0x3f;\n+\n+  crossing_info *object_starts;\n+\n+public:\n+  \/\/ If we're setting first_start, assume the card has an object.\n+  inline void set_first_start(size_t card_index, uint8_t value) {\n+    object_starts[card_index].offsets.first = ObjectStartsInCardRegion | value;\n+  }\n+\n+  inline void set_last_start(size_t card_index, uint8_t value) {\n+    object_starts[card_index].offsets.last = value;\n+  }\n+\n+  inline void set_starts_object_bit(size_t card_index) {\n+    object_starts[card_index].offsets.first |= ObjectStartsInCardRegion;\n+  }\n+\n+  inline void clear_starts_object_bit(size_t card_index) {\n+    object_starts[card_index].offsets.first &= ~ObjectStartsInCardRegion;\n+  }\n+\n+  \/\/ Returns true iff an object is known to start within the card memory associated with card card_index.\n+  inline bool starts_object(size_t card_index) const {\n+    return (object_starts[card_index].offsets.first & ObjectStartsInCardRegion) != 0;\n+  }\n+\n+  inline void clear_objects_in_range(HeapWord *addr, size_t num_words) {\n+    size_t card_index = _rs->card_index_for_addr(addr);\n+    size_t last_card_index = _rs->card_index_for_addr(addr + num_words - 1);\n+    while (card_index <= last_card_index)\n+      object_starts[card_index++].short_word = 0;\n+  }\n+\n+  ShenandoahCardCluster(RememberedSet *rs) {\n+    _rs = rs;\n+    \/\/ TODO: We don't really need object_starts entries for every card entry.  We only need these for\n+    \/\/ the card entries that correspond to old-gen memory.  But for now, let's be quick and dirty.\n+    object_starts = NEW_C_HEAP_ARRAY(crossing_info, rs->total_cards(), mtGC);\n+    for (size_t i = 0; i < rs->total_cards(); i++) {\n+      object_starts[i].short_word = 0;\n+    }\n+  }\n+\n+  ~ShenandoahCardCluster() {\n+    FREE_C_HEAP_ARRAY(crossing_info, object_starts);\n+    object_starts = nullptr;\n+  }\n+\n+  \/\/ There is one entry within the object_starts array for each card entry.\n+  \/\/\n+  \/\/  Suppose multiple garbage objects are coalesced during GC sweep\n+  \/\/  into a single larger \"free segment\".  As each two objects are\n+  \/\/  coalesced together, the start information pertaining to the second\n+  \/\/  object must be removed from the objects_starts array.  If the\n+  \/\/  second object had been been the first object within card memory,\n+  \/\/  the new first object is the object that follows that object if\n+  \/\/  that starts within the same card memory, or NoObject if the\n+  \/\/  following object starts within the following cluster.  If the\n+  \/\/  second object had been the last object in the card memory,\n+  \/\/  replace this entry with the newly coalesced object if it starts\n+  \/\/  within the same card memory, or with NoObject if it starts in a\n+  \/\/  preceding card's memory.\n+  \/\/\n+  \/\/  Suppose a large free segment is divided into a smaller free\n+  \/\/  segment and a new object.  The second part of the newly divided\n+  \/\/  memory must be registered as a new object, overwriting at most\n+  \/\/  one first_start and one last_start entry.  Note that one of the\n+  \/\/  newly divided two objects might be a new GCLAB.\n+  \/\/\n+  \/\/  Suppose postprocessing of a GCLAB finds that the original GCLAB\n+  \/\/  has been divided into N objects.  Each of the N newly allocated\n+  \/\/  objects will be registered, overwriting at most one first_start\n+  \/\/  and one last_start entries.\n+  \/\/\n+  \/\/  No object registration operations are linear in the length of\n+  \/\/  the registered objects.\n+  \/\/\n+  \/\/ Consider further the following observations regarding object\n+  \/\/ registration costs:\n+  \/\/\n+  \/\/   1. The cost is paid once for each old-gen object (Except when\n+  \/\/      an object is demoted and repromoted, in which case we would\n+  \/\/      pay the cost again).\n+  \/\/   2. The cost can be deferred so that there is no urgency during\n+  \/\/      mutator copy-on-first-access promotion.  Background GC\n+  \/\/      threads will update the object_starts array by post-\n+  \/\/      processing the contents of retired PLAB buffers.\n+  \/\/   3. The bet is that these costs are paid relatively rarely\n+  \/\/      because:\n+  \/\/      a) Most objects die young and objects that die in young-gen\n+  \/\/         memory never need to be registered with the object_starts\n+  \/\/         array.\n+  \/\/      b) Most objects that are promoted into old-gen memory live\n+  \/\/         there without further relocation for a relatively long\n+  \/\/         time, so we get a lot of benefit from each investment\n+  \/\/         in registering an object.\n+\n+public:\n+\n+  \/\/ The starting locations of objects contained within old-gen memory\n+  \/\/ are registered as part of the remembered set implementation.  This\n+  \/\/ information is required when scanning dirty card regions that are\n+  \/\/ spanned by objects beginning within preceding card regions.  It\n+  \/\/ is necessary to find the first and last objects that begin within\n+  \/\/ this card region.  Starting addresses of objects are required to\n+  \/\/ find the object headers, and object headers provide information\n+  \/\/ about which fields within the object hold addresses.\n+  \/\/\n+  \/\/ The old-gen memory allocator invokes register_object() for any\n+  \/\/ object that is allocated within old-gen memory.  This identifies\n+  \/\/ the starting addresses of objects that span boundaries between\n+  \/\/ card regions.\n+  \/\/\n+  \/\/ It is not necessary to invoke register_object at the very instant\n+  \/\/ an object is allocated.  It is only necessary to invoke it\n+  \/\/ prior to the next start of a garbage collection concurrent mark\n+  \/\/ or concurrent update-references phase.  An \"ideal\" time to register\n+  \/\/ objects is during post-processing of a GCLAB after the GCLAB is\n+  \/\/ retired due to depletion of its memory.\n+  \/\/\n+  \/\/ register_object() does not perform synchronization.  In the case\n+  \/\/ that multiple threads are registering objects whose starting\n+  \/\/ addresses are within the same cluster, races between these\n+  \/\/ threads may result in corruption of the object-start data\n+  \/\/ structures.  Parallel GC threads should avoid registering objects\n+  \/\/ residing within the same cluster by adhering to the following\n+  \/\/ coordination protocols:\n+  \/\/\n+  \/\/  1. Align thread-local GCLAB buffers with some TBD multiple of\n+  \/\/     card clusters.  The card cluster size is 32 KB.  If the\n+  \/\/     desired GCLAB size is 128 KB, align the buffer on a multiple\n+  \/\/     of 4 card clusters.\n+  \/\/  2. Post-process the contents of GCLAB buffers to register the\n+  \/\/     objects allocated therein.  Allow one GC thread at a\n+  \/\/     time to do the post-processing of each GCLAB.\n+  \/\/  3. Since only one GC thread at a time is registering objects\n+  \/\/     belonging to a particular allocation buffer, no locking\n+  \/\/     is performed when registering these objects.\n+  \/\/  4. Any remnant of unallocated memory within an expended GC\n+  \/\/     allocation buffer is not returned to the old-gen allocation\n+  \/\/     pool until after the GC allocation buffer has been post\n+  \/\/     processed.  Before any remnant memory is returned to the\n+  \/\/     old-gen allocation pool, the GC thread that scanned this GC\n+  \/\/     allocation buffer performs a write-commit memory barrier.\n+  \/\/  5. Background GC threads that perform tenuring of young-gen\n+  \/\/     objects without a GCLAB use a CAS lock before registering\n+  \/\/     each tenured object.  The CAS lock assures both mutual\n+  \/\/     exclusion and memory coherency\/visibility.  Note that an\n+  \/\/     object tenured by a background GC thread will not overlap\n+  \/\/     with any of the clusters that are receiving tenured objects\n+  \/\/     by way of GCLAB buffers.  Multiple independent GC threads may\n+  \/\/     attempt to tenure objects into a shared cluster.  This is why\n+  \/\/     sychronization may be necessary.  Consider the following\n+  \/\/     scenarios:\n+  \/\/\n+  \/\/     a) If two objects are tenured into the same card region, each\n+  \/\/        registration may attempt to modify the first-start or\n+  \/\/        last-start information associated with that card region.\n+  \/\/        Furthermore, because the representations of first-start\n+  \/\/        and last-start information within the object_starts array\n+  \/\/        entry uses different bits of a shared uint_16 to represent\n+  \/\/        each, it is necessary to lock the entire card entry\n+  \/\/        before modifying either the first-start or last-start\n+  \/\/        information within the entry.\n+  \/\/     b) Suppose GC thread X promotes a tenured object into\n+  \/\/        card region A and this tenured object spans into\n+  \/\/        neighboring card region B.  Suppose GC thread Y (not equal\n+  \/\/        to X) promotes a tenured object into cluster B.  GC thread X\n+  \/\/        will update the object_starts information for card A.  No\n+  \/\/        synchronization is required.\n+  \/\/     c) In summary, when background GC threads register objects\n+  \/\/        newly tenured into old-gen memory, they must acquire a\n+  \/\/        mutual exclusion lock on the card that holds the starting\n+  \/\/        address of the newly tenured object.  This can be achieved\n+  \/\/        by using a CAS instruction to assure that the previous\n+  \/\/        values of first-offset and last-offset have not been\n+  \/\/        changed since the same thread inquired as to their most\n+  \/\/        current values.\n+  \/\/\n+  \/\/     One way to minimize the need for synchronization between\n+  \/\/     background tenuring GC threads is for each tenuring GC thread\n+  \/\/     to promote young-gen objects into distinct dedicated cluster\n+  \/\/     ranges.\n+  \/\/  6. The object_starts information is only required during the\n+  \/\/     starting of concurrent marking and concurrent evacuation\n+  \/\/     phases of GC.  Before we start either of these GC phases, the\n+  \/\/     JVM enters a safe point and all GC threads perform\n+  \/\/     commit-write barriers to assure that access to the\n+  \/\/     object_starts information is coherent.\n+\n+\n+  \/\/ Notes on synchronization of register_object():\n+  \/\/\n+  \/\/  1. For efficiency, there is no locking in the implementation of register_object()\n+  \/\/  2. Thus, it is required that users of this service assure that concurrent\/parallel invocations of\n+  \/\/     register_object() do pertain to the same card's memory range.  See discussion below to understand\n+  \/\/     the risks.\n+  \/\/  3. When allocating from a TLAB or GCLAB, the mutual exclusion can be guaranteed by assuring that each\n+  \/\/     LAB's start and end are aligned on card memory boundaries.\n+  \/\/  4. Use the same lock that guarantees exclusivity when performing free-list allocation within heap regions.\n+  \/\/\n+  \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+  \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+  \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+  \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+  \/\/\n+  \/\/ objects being \"concurrently\" allocated:\n+  \/\/    [-----a------][-----b-----][--------------c------------------]\n+  \/\/            [---- card table memory range --------------]\n+  \/\/\n+  \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that:\n+  \/\/   allocation of object a wants to set the has-object, first-start, and last-start attributes of the preceding card region.\n+  \/\/   allocation of object b wants to set the has-object, first-start, and last-start attributes of this card region.\n+  \/\/   allocation of object c also wants to set the has-object, first-start, and last-start attributes of this card region.\n+  \/\/\n+  \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as last-start\n+  \/\/ representing object b while first-start represents object c.  This is why we need to require all register_object()\n+  \/\/ invocations associated with objects that are allocated from \"free lists\" to provide their own mutual exclusion locking\n+  \/\/ mechanism.\n+\n+  \/\/ Reset the starts_object() information to false for all cards in the range between from and to.\n+  void reset_object_range(HeapWord *from, HeapWord *to);\n+\n+  \/\/ register_object() requires that the caller hold the heap lock\n+  \/\/ before calling it.\n+  void register_object(HeapWord* address);\n+\n+  \/\/ register_object_wo_lock() does not require that the caller hold\n+  \/\/ the heap lock before calling it, under the assumption that the\n+  \/\/ caller has assure no other thread will endeavor to concurrently\n+  \/\/ register objects that start within the same card's memory region\n+  \/\/ as address.\n+  void register_object_wo_lock(HeapWord* address);\n+\n+  \/\/ During the reference updates phase of GC, we walk through each old-gen memory region that was\n+  \/\/ not part of the collection set and we invalidate all unmarked objects.  As part of this effort,\n+  \/\/ we coalesce neighboring dead objects in order to make future remembered set scanning more\n+  \/\/ efficient (since future remembered set scanning of any card region containing consecutive\n+  \/\/ dead objects can skip over all of them at once by reading only a single dead object header\n+  \/\/ instead of having to read the header of each of the coalesced dead objects.\n+  \/\/\n+  \/\/ At some future time, we may implement a further optimization: satisfy future allocation requests\n+  \/\/ by carving new objects out of the range of memory that represents the coalesced dead objects.\n+  \/\/\n+  \/\/ Suppose we want to combine several dead objects into a single coalesced object.  How does this\n+  \/\/ impact our representation of crossing map information?\n+  \/\/  1. If the newly coalesced range is contained entirely within a card range, that card's last\n+  \/\/     start entry either remains the same or it is changed to the start of the coalesced region.\n+  \/\/  2. For the card that holds the start of the coalesced object, it will not impact the first start\n+  \/\/     but it may impact the last start.\n+  \/\/  3. For following cards spanned entirely by the newly coalesced object, it will change starts_object\n+  \/\/     to false (and make first-start and last-start \"undefined\").\n+  \/\/  4. For a following card that is spanned patially by the newly coalesced object, it may change\n+  \/\/     first-start value, but it will not change the last-start value.\n+  \/\/\n+  \/\/ The range of addresses represented by the arguments to coalesce_objects() must represent a range\n+  \/\/ of memory that was previously occupied exactly by one or more previously registered objects.  For\n+  \/\/ convenience, it is legal to invoke coalesce_objects() with arguments that span a single previously\n+  \/\/ registered object.\n+  \/\/\n+  \/\/ The role of coalesce_objects is to change the crossing map information associated with all of the coalesced\n+  \/\/ objects.\n+  void coalesce_objects(HeapWord* address, size_t length_in_words);\n+\n+  \/\/ The typical use case is going to look something like this:\n+  \/\/   for each heapregion that comprises old-gen memory\n+  \/\/     for each card number that corresponds to this heap region\n+  \/\/       scan the objects contained therein if the card is dirty\n+  \/\/ To avoid excessive lookups in a sparse array, the API queries\n+  \/\/ the card number pertaining to a particular address and then uses the\n+  \/\/ card number for subsequent information lookups and stores.\n+\n+  \/\/ If starts_object(card_index), this returns the word offset within this card\n+  \/\/ memory at which the first object begins.  If !starts_object(card_index), the\n+  \/\/ result is a don't care value -- asserts in a debug build.\n+  size_t get_first_start(size_t card_index) const;\n+\n+  \/\/ If starts_object(card_index), this returns the word offset within this card\n+  \/\/ memory at which the last object begins.  If !starts_object(card_index), the\n+  \/\/ result is a don't care value.\n+  size_t get_last_start(size_t card_index) const;\n+\n+\n+  \/\/ Given a card_index, return the starting address of the first block in the heap\n+  \/\/ that straddles into the card. If the card is co-initial with an object, then\n+  \/\/ this would return the starting address of the heap that this card covers.\n+  \/\/ Expects to be called for a card affiliated with the old generation in\n+  \/\/ generational mode.\n+  HeapWord* block_start(size_t card_index) const;\n+};\n+\n+\/\/ ShenandoahScanRemembered is a concrete class representing the\n+\/\/ ability to scan the old-gen remembered set for references to\n+\/\/ objects residing in young-gen memory.\n+\/\/\n+\/\/ Scanning normally begins with an invocation of numRegions and ends\n+\/\/ after all clusters of all regions have been scanned.\n+\/\/\n+\/\/ Throughout the scanning effort, the number of regions does not\n+\/\/ change.\n+\/\/\n+\/\/ Even though the regions that comprise old-gen memory are not\n+\/\/ necessarily contiguous, the abstraction represented by this class\n+\/\/ identifies each of the old-gen regions with an integer value\n+\/\/ in the range from 0 to (numRegions() - 1) inclusive.\n+\/\/\n+\n+template<typename RememberedSet>\n+class ShenandoahScanRemembered: public CHeapObj<mtGC> {\n+\n+private:\n+  RememberedSet* _rs;\n+  ShenandoahCardCluster<RememberedSet>* _scc;\n+\n+  \/\/ Global card stats (cumulative)\n+  HdrSeq _card_stats_scan_rs[MAX_CARD_STAT_TYPE];\n+  HdrSeq _card_stats_update_refs[MAX_CARD_STAT_TYPE];\n+  \/\/ Per worker card stats (multiplexed by phase)\n+  HdrSeq** _card_stats;\n+\n+  \/\/ The types of card metrics that we gather\n+  const char* _card_stats_name[MAX_CARD_STAT_TYPE] = {\n+   \"dirty_run\", \"clean_run\",\n+   \"dirty_cards\", \"clean_cards\",\n+   \"max_dirty_run\", \"max_clean_run\",\n+   \"dirty_scan_objs\",\n+   \"alternations\"\n+  };\n+\n+  \/\/ The statistics are collected and logged separately for\n+  \/\/ card-scans for initial marking, and for updating refs.\n+  const char* _card_stat_log_type[MAX_CARD_STAT_LOG_TYPE] = {\n+   \"Scan Remembered Set\", \"Update Refs\"\n+  };\n+\n+  int _card_stats_log_counter[2] = {0, 0};\n+\n+public:\n+  \/\/ How to instantiate this object?\n+  \/\/   ShenandoahDirectCardMarkRememberedSet *rs =\n+  \/\/       new ShenandoahDirectCardMarkRememberedSet();\n+  \/\/   scr = new\n+  \/\/     ShenandoahScanRememberd<ShenandoahDirectCardMarkRememberedSet>(rs);\n+  \/\/\n+  \/\/ or, after the planned implementation of\n+  \/\/ ShenandoahBufferWithSATBRememberedSet has been completed:\n+  \/\/\n+  \/\/   ShenandoahBufferWithSATBRememberedSet *rs =\n+  \/\/       new ShenandoahBufferWithSATBRememberedSet();\n+  \/\/   scr = new\n+  \/\/     ShenandoahScanRememberd<ShenandoahBufferWithSATBRememberedSet>(rs);\n+\n+\n+  ShenandoahScanRemembered(RememberedSet *rs) {\n+    _rs = rs;\n+    _scc = new ShenandoahCardCluster<RememberedSet>(rs);\n+\n+    \/\/ We allocate ParallelGCThreads worth even though we usually only\n+    \/\/ use up to ConcGCThreads, because degenerate collections may employ\n+    \/\/ ParallelGCThreads for remembered set scanning.\n+    if (ShenandoahEnableCardStats) {\n+      _card_stats = NEW_C_HEAP_ARRAY(HdrSeq*, ParallelGCThreads, mtGC);\n+      for (uint i = 0; i < ParallelGCThreads; i++) {\n+        _card_stats[i] = new HdrSeq[MAX_CARD_STAT_TYPE];\n+      }\n+    } else {\n+      _card_stats = nullptr;\n+    }\n+  }\n+\n+  ~ShenandoahScanRemembered() {\n+    delete _scc;\n+    if (ShenandoahEnableCardStats) {\n+      for (uint i = 0; i < ParallelGCThreads; i++) {\n+        delete _card_stats[i];\n+      }\n+      FREE_C_HEAP_ARRAY(HdrSeq*, _card_stats);\n+      _card_stats = nullptr;\n+    }\n+    assert(_card_stats == nullptr, \"Error\");\n+  }\n+\n+  HdrSeq* card_stats(uint worker_id) {\n+    assert(worker_id < ParallelGCThreads, \"Error\");\n+    assert(ShenandoahEnableCardStats == (_card_stats != nullptr), \"Error\");\n+    return ShenandoahEnableCardStats ? _card_stats[worker_id] : nullptr;\n+  }\n+\n+  HdrSeq* card_stats_for_phase(CardStatLogType t) {\n+    switch (t) {\n+      case CARD_STAT_SCAN_RS:\n+        return _card_stats_scan_rs;\n+      case CARD_STAT_UPDATE_REFS:\n+        return _card_stats_update_refs;\n+      default:\n+        guarantee(false, \"No such CardStatLogType\");\n+    }\n+    return nullptr; \/\/ Quiet compiler\n+  }\n+\n+  \/\/ TODO:  We really don't want to share all of these APIs with arbitrary consumers of the ShenandoahScanRemembered abstraction.\n+  \/\/ But in the spirit of quick and dirty for the time being, I'm going to go ahead and publish everything for right now.  Some\n+  \/\/ of existing code already depends on having access to these services (because existing code has not been written to honor\n+  \/\/ full abstraction of remembered set scanning.  In the not too distant future, we want to try to make most, if not all, of\n+  \/\/ these services private.  Two problems with publicizing:\n+  \/\/  1. Allowing arbitrary users to reach beneath the hood allows the users to make assumptions about underlying implementation.\n+  \/\/     This will make it more difficult to change underlying implementation at a future time, such as when we eventually experiment\n+  \/\/     with SATB-based implementation of remembered set representation.\n+  \/\/  2. If we carefully control sharing of certain of these services, we can reduce the overhead of synchronization by assuring\n+  \/\/     that all users follow protocols that avoid contention that might require synchronization.  When we publish these APIs, we\n+  \/\/     lose control over who and how the data is accessed.  As a result, we are required to insert more defensive measures into\n+  \/\/     the implementation, including synchronization locks.\n+\n+\n+  \/\/ Card index is zero-based relative to first spanned card region.\n+  size_t last_valid_index();\n+  size_t total_cards();\n+  size_t card_index_for_addr(HeapWord *p);\n+  HeapWord *addr_for_card_index(size_t card_index);\n+  bool is_card_dirty(size_t card_index);\n+  bool is_write_card_dirty(size_t card_index) { return _rs->is_write_card_dirty(card_index); }\n+  void mark_card_as_dirty(size_t card_index);\n+  void mark_range_as_dirty(size_t card_index, size_t num_cards);\n+  void mark_card_as_clean(size_t card_index);\n+  void mark_read_card_as_clean(size_t card_index) { _rs->mark_read_card_clean(card_index); }\n+  void mark_range_as_clean(size_t card_index, size_t num_cards);\n+  bool is_card_dirty(HeapWord *p);\n+  void mark_card_as_dirty(HeapWord *p);\n+  void mark_range_as_dirty(HeapWord *p, size_t num_heap_words);\n+  void mark_card_as_clean(HeapWord *p);\n+  void mark_range_as_clean(HeapWord *p, size_t num_heap_words);\n+  size_t cluster_count();\n+\n+  \/\/ Called by GC thread at start of concurrent mark to exchange roles of read and write remembered sets.\n+  void swap_remset() { _rs->swap_remset(); }\n+\n+  void reset_remset(HeapWord* start, size_t word_count) { _rs->reset_remset(start, word_count); }\n+\n+  void merge_write_table(HeapWord* start, size_t word_count) { _rs->merge_write_table(start, word_count); }\n+\n+  \/\/ Called by GC thread after scanning old remembered set in order to prepare for next GC pass\n+  void clear_old_remset() { _rs->clear_old_remset(); }\n+\n+  size_t cluster_for_addr(HeapWord *addr);\n+  HeapWord* addr_for_cluster(size_t cluster_no);\n+\n+  void reset_object_range(HeapWord *from, HeapWord *to);\n+  void register_object(HeapWord *addr);\n+  void register_object_wo_lock(HeapWord *addr);\n+  void coalesce_objects(HeapWord *addr, size_t length_in_words);\n+\n+  HeapWord* first_object_in_card(size_t card_index) {\n+    if (_scc->starts_object(card_index)) {\n+      return addr_for_card_index(card_index) + _scc->get_first_start(card_index);\n+    } else {\n+      return nullptr;\n+    }\n+  }\n+\n+  \/\/ Return true iff this object is \"properly\" registered.\n+  bool verify_registration(HeapWord* address, ShenandoahMarkingContext* ctx);\n+\n+  \/\/ clear the cards to clean, and clear the object_starts info to no objects\n+  void mark_range_as_empty(HeapWord *addr, size_t length_in_words);\n+\n+  \/\/ process_clusters() scans a portion of the remembered set\n+  \/\/ for references from old gen into young. Several worker threads\n+  \/\/ scan different portions of the remembered set by making parallel invocations\n+  \/\/ of process_clusters() with each invocation scanning different\n+  \/\/ \"clusters\" of the remembered set.\n+  \/\/\n+  \/\/ An invocation of process_clusters() examines all of the\n+  \/\/ intergenerational references spanned by `count` clusters starting\n+  \/\/ with `first_cluster`.  The `oops` argument is a worker-thread-local\n+  \/\/ OopClosure that is applied to all \"valid\" references in the remembered set.\n+  \/\/\n+  \/\/ A side-effect of executing process_clusters() is to update the remembered\n+  \/\/ set entries (e.g. marking dirty cards clean if they no longer\n+  \/\/ hold references to young-gen memory).\n+  \/\/\n+  \/\/ An implementation of process_clusters() may choose to efficiently\n+  \/\/ address more typical scenarios in the structure of remembered sets. E.g.\n+  \/\/ in the generational setting, one might expect remembered sets to be very sparse\n+  \/\/ (low mutation rates in the old generation leading to sparse dirty cards,\n+  \/\/ each with very few intergenerational pointers). Specific implementations\n+  \/\/ may choose to degrade gracefully as the sparsity assumption fails to hold,\n+  \/\/ such as when there are sudden spikes in (premature) promotion or in the\n+  \/\/ case of an underprovisioned, poorly-tuned, or poorly-shaped heap.\n+  \/\/\n+  \/\/ At the start of a concurrent young generation marking cycle, we invoke process_clusters\n+  \/\/ with ClosureType ShenandoahInitMarkRootsClosure.\n+  \/\/\n+  \/\/ At the start of a concurrent evacuation phase, we invoke process_clusters with\n+  \/\/ ClosureType ShenandoahEvacuateUpdateRootsClosure.\n+\n+  \/\/ All template expansions require methods to be defined in the inline.hpp file, but larger\n+  \/\/ such methods need not be declared as inline.\n+  template <typename ClosureType>\n+  void process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range, ClosureType *oops,\n+                               bool use_write_table, uint worker_id);\n+\n+  template <typename ClosureType>\n+  inline void process_humongous_clusters(ShenandoahHeapRegion* r, size_t first_cluster, size_t count,\n+                                         HeapWord *end_of_range, ClosureType *oops, bool use_write_table);\n+\n+  template <typename ClosureType>\n+  inline void process_region_slice(ShenandoahHeapRegion* region, size_t offset, size_t clusters, HeapWord* end_of_range,\n+                                   ClosureType *cl, bool use_write_table, uint worker_id);\n+\n+  \/\/ To Do:\n+  \/\/  Create subclasses of ShenandoahInitMarkRootsClosure and\n+  \/\/  ShenandoahEvacuateUpdateRootsClosure and any other closures\n+  \/\/  that need to participate in remembered set scanning.  Within the\n+  \/\/  subclasses, add a (probably templated) instance variable that\n+  \/\/  refers to the associated ShenandoahCardCluster object.  Use this\n+  \/\/  ShenandoahCardCluster instance to \"enhance\" the do_oops\n+  \/\/  processing so that we can:\n+  \/\/\n+  \/\/   1. Avoid processing references that correspond to clean card\n+  \/\/      regions, and\n+  \/\/   2. Set card status to CLEAN when the associated card region no\n+  \/\/      longer holds inter-generatioanal references.\n+  \/\/\n+  \/\/  To enable efficient implementation of these behaviors, we\n+  \/\/  probably also want to add a few fields into the\n+  \/\/  ShenandoahCardCluster object that allow us to precompute and\n+  \/\/  remember the addresses at which card status is going to change\n+  \/\/  from dirty to clean and clean to dirty.  The do_oops\n+  \/\/  implementations will want to update this value each time they\n+  \/\/  cross one of these boundaries.\n+  void roots_do(OopIterateClosure* cl);\n+\n+  \/\/ Log stats related to card\/RS stats for given phase t\n+  void log_card_stats(uint nworkers, CardStatLogType t) PRODUCT_RETURN;\n+private:\n+  \/\/ Log stats for given worker id related into given cumulative card\/RS stats\n+  void log_worker_card_stats(uint worker_id, HdrSeq* cum_stats) PRODUCT_RETURN;\n+\n+  \/\/ Log given stats\n+  inline void log_card_stats(HdrSeq* stats) PRODUCT_RETURN;\n+\n+  \/\/ Merge the stats from worked_id into the given summary stats, and clear the worker_id's stats.\n+  void merge_worker_card_stats_cumulative(HdrSeq* worker_stats, HdrSeq* cum_stats) PRODUCT_RETURN;\n+};\n+\n+\n+\/\/ A ShenandoahRegionChunk represents a contiguous interval of a ShenandoahHeapRegion, typically representing\n+\/\/ work to be done by a worker thread.\n+struct ShenandoahRegionChunk {\n+  ShenandoahHeapRegion *_r;      \/\/ The region of which this represents a chunk\n+  size_t _chunk_offset;          \/\/ HeapWordSize offset\n+  size_t _chunk_size;            \/\/ HeapWordSize qty\n+};\n+\n+\/\/ ShenandoahRegionChunkIterator divides the total remembered set scanning effort into ShenandoahRegionChunks\n+\/\/ that are assigned one at a time to worker threads. (Here, we use the terms `assignments` and `chunks`\n+\/\/ interchangeably.) Note that the effort required to scan a range of memory is not necessarily a linear\n+\/\/ function of the size of the range.  Some memory ranges hold only a small number of live objects.\n+\/\/ Some ranges hold primarily primitive (non-pointer) data.  We start with larger chunk sizes because larger chunks\n+\/\/ reduce coordination overhead.  We expect that the GC worker threads that receive more difficult assignments\n+\/\/ will work longer on those chunks.  Meanwhile, other worker will threads repeatedly accept and complete multiple\n+\/\/ easier chunks.  As the total amount of work remaining to be completed decreases, we decrease the size of chunks\n+\/\/ given to individual threads.  This reduces the likelihood of significant imbalance between worker thread assignments\n+\/\/ when there is less meaningful work to be performed by the remaining worker threads while they wait for\n+\/\/ worker threads with difficult assignments to finish, reducing the overall duration of the phase.\n+\n+class ShenandoahRegionChunkIterator : public StackObj {\n+private:\n+  \/\/ The largest chunk size is 4 MiB, measured in words.  Otherwise, remembered set scanning may become too unbalanced.\n+  \/\/ If the largest chunk size is too small, there is too much overhead sifting out assignments to individual worker threads.\n+  static const size_t _maximum_chunk_size_words = (4 * 1024 * 1024) \/ HeapWordSize;\n+\n+  static const size_t _clusters_in_smallest_chunk = 4;\n+\n+  \/\/ smallest_chunk_size is 4 clusters.  Each cluster spans 128 KiB.\n+  \/\/ This is computed from CardTable::card_size_in_words() *\n+  \/\/      ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  static const size_t smallest_chunk_size_words() {\n+      return _clusters_in_smallest_chunk * CardTable::card_size_in_words() *\n+             ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  }\n+\n+  \/\/ The total remembered set scanning effort is divided into chunks of work that are assigned to individual worker tasks.\n+  \/\/ The chunks of assigned work are divided into groups, where the size of the typical group (_regular_group_size) is half the\n+  \/\/ total number of regions.  The first group may be larger than\n+  \/\/ _regular_group_size in the case that the first group's chunk\n+  \/\/ size is less than the region size.  The last group may be larger\n+  \/\/ than _regular_group_size because no group is allowed to\n+  \/\/ have smaller assignments than _smallest_chunk_size, which is 128 KB.\n+\n+  \/\/ Under normal circumstances, no configuration needs more than _maximum_groups (default value of 16).\n+  \/\/ The first group \"effectively\" processes chunks of size 1 MiB (or smaller for smaller region sizes).\n+  \/\/ The last group processes chunks of size 128 KiB.  There are four groups total.\n+\n+  \/\/ group[0] is 4 MiB chunk size (_maximum_chunk_size_words)\n+  \/\/ group[1] is 2 MiB chunk size\n+  \/\/ group[2] is 1 MiB chunk size\n+  \/\/ group[3] is 512 KiB chunk size\n+  \/\/ group[4] is 256 KiB chunk size\n+  \/\/ group[5] is 128 Kib shunk size (_smallest_chunk_size_words = 4 * 64 * 64\n+  static const size_t _maximum_groups = 6;\n+\n+  const ShenandoahHeap* _heap;\n+\n+  const size_t _regular_group_size;                        \/\/ Number of chunks in each group\n+  const size_t _first_group_chunk_size_b4_rebalance;\n+  const size_t _num_groups;                        \/\/ Number of groups in this configuration\n+  const size_t _total_chunks;\n+\n+  shenandoah_padding(0);\n+  volatile size_t _index;\n+  shenandoah_padding(1);\n+\n+  size_t _region_index[_maximum_groups];           \/\/ The region index for the first region spanned by this group\n+  size_t _group_offset[_maximum_groups];           \/\/ The offset at which group begins within first region spanned by this group\n+  size_t _group_chunk_size[_maximum_groups];       \/\/ The size of each chunk within this group\n+  size_t _group_entries[_maximum_groups];          \/\/ Total chunks spanned by this group and the ones before it.\n+\n+  \/\/ No implicit copying: iterators should be passed by reference to capture the state\n+  NONCOPYABLE(ShenandoahRegionChunkIterator);\n+\n+  \/\/ Makes use of _heap.\n+  size_t calc_regular_group_size();\n+\n+  \/\/ Makes use of _regular_group_size, which must be initialized before call.\n+  size_t calc_first_group_chunk_size_b4_rebalance();\n+\n+  \/\/ Makes use of _regular_group_size and _first_group_chunk_size_b4_rebalance, both of which must be initialized before call.\n+  size_t calc_num_groups();\n+\n+  \/\/ Makes use of _regular_group_size, _first_group_chunk_size_b4_rebalance, which must be initialized before call.\n+  size_t calc_total_chunks();\n+\n+public:\n+  ShenandoahRegionChunkIterator(size_t worker_count);\n+  ShenandoahRegionChunkIterator(ShenandoahHeap* heap, size_t worker_count);\n+\n+  \/\/ Reset iterator to default state\n+  void reset();\n+\n+  \/\/ Fills in assignment with next chunk of work and returns true iff there is more work.\n+  \/\/ Otherwise, returns false.  This is multi-thread-safe.\n+  inline bool next(struct ShenandoahRegionChunk *assignment);\n+\n+  \/\/ This is *not* MT safe. However, in the absence of multithreaded access, it\n+  \/\/ can be used to determine if there is more work to do.\n+  inline bool has_next() const;\n+};\n+\n+typedef ShenandoahScanRemembered<ShenandoahDirectCardMarkRememberedSet> RememberedScanner;\n+\n+class ShenandoahScanRememberedTask : public WorkerTask {\n+ private:\n+  ShenandoahObjToScanQueueSet* _queue_set;\n+  ShenandoahObjToScanQueueSet* _old_queue_set;\n+  ShenandoahReferenceProcessor* _rp;\n+  ShenandoahRegionChunkIterator* _work_list;\n+  bool _is_concurrent;\n+\n+ public:\n+  ShenandoahScanRememberedTask(ShenandoahObjToScanQueueSet* queue_set,\n+                               ShenandoahObjToScanQueueSet* old_queue_set,\n+                               ShenandoahReferenceProcessor* rp,\n+                               ShenandoahRegionChunkIterator* work_list,\n+                               bool is_concurrent);\n+\n+  void work(uint worker_id);\n+  void do_work(uint worker_id);\n+};\n+\n+\/\/ Verify that the oop doesn't point into the young generation\n+class ShenandoahVerifyNoYoungRefsClosure: public BasicOopIterateClosure {\n+  ShenandoahHeap* _heap;\n+  template<class T> void work(T* p);\n+\n+ public:\n+  ShenandoahVerifyNoYoungRefsClosure();\n+\n+  virtual void do_oop(narrowOop* p) { work(p); }\n+  virtual void do_oop(oop* p)       { work(p); }\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBERED_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.hpp","additions":1078,"deletions":0,"binary":false,"changes":1078,"status":"added"},{"patch":"@@ -0,0 +1,1026 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com, Inc. or its affiliates.  All rights reserved.\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBEREDINLINE_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBEREDINLINE_HPP\n+\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/oop.hpp\"\n+#include \"oops\/objArrayOop.hpp\"\n+#include \"gc\/shared\/collectorCounters.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCardStats.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.hpp\"\n+\n+inline size_t\n+ShenandoahDirectCardMarkRememberedSet::last_valid_index() const {\n+  return _card_table->last_valid_index();\n+}\n+\n+inline size_t\n+ShenandoahDirectCardMarkRememberedSet::total_cards() const {\n+  return _total_card_count;\n+}\n+\n+inline size_t\n+ShenandoahDirectCardMarkRememberedSet::card_index_for_addr(HeapWord *p) const {\n+  return _card_table->index_for(p);\n+}\n+\n+inline HeapWord*\n+ShenandoahDirectCardMarkRememberedSet::addr_for_card_index(size_t card_index) const {\n+  return _whole_heap_base + CardTable::card_size_in_words() * card_index;\n+}\n+\n+inline const CardValue*\n+ShenandoahDirectCardMarkRememberedSet::get_card_table_byte_map(bool use_write_table) const {\n+  return use_write_table ?\n+           _card_table->write_byte_map()\n+           : _card_table->read_byte_map();\n+}\n+\n+inline bool\n+ShenandoahDirectCardMarkRememberedSet::is_write_card_dirty(size_t card_index) const {\n+  CardValue* bp = &(_card_table->write_byte_map())[card_index];\n+  return (bp[0] == CardTable::dirty_card_val());\n+}\n+\n+inline bool\n+ShenandoahDirectCardMarkRememberedSet::is_card_dirty(size_t card_index) const {\n+  CardValue* bp = &(_card_table->read_byte_map())[card_index];\n+  return (bp[0] == CardTable::dirty_card_val());\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_card_as_dirty(size_t card_index) {\n+  CardValue* bp = &(_card_table->write_byte_map())[card_index];\n+  bp[0] = CardTable::dirty_card_val();\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_range_as_dirty(size_t card_index, size_t num_cards) {\n+  CardValue* bp = &(_card_table->write_byte_map())[card_index];\n+  while (num_cards-- > 0) {\n+    *bp++ = CardTable::dirty_card_val();\n+  }\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_card_as_clean(size_t card_index) {\n+  CardValue* bp = &(_card_table->write_byte_map())[card_index];\n+  bp[0] = CardTable::clean_card_val();\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_range_as_clean(size_t card_index, size_t num_cards) {\n+  CardValue* bp = &(_card_table->write_byte_map())[card_index];\n+  while (num_cards-- > 0) {\n+    *bp++ = CardTable::clean_card_val();\n+  }\n+}\n+\n+inline bool\n+ShenandoahDirectCardMarkRememberedSet::is_card_dirty(HeapWord *p) const {\n+  size_t index = card_index_for_addr(p);\n+  CardValue* bp = &(_card_table->read_byte_map())[index];\n+  return (bp[0] == CardTable::dirty_card_val());\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_card_as_dirty(HeapWord *p) {\n+  size_t index = card_index_for_addr(p);\n+  CardValue* bp = &(_card_table->write_byte_map())[index];\n+  bp[0] = CardTable::dirty_card_val();\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_range_as_dirty(HeapWord *p, size_t num_heap_words) {\n+  CardValue* bp = &(_card_table->write_byte_map_base())[uintptr_t(p) >> _card_shift];\n+  CardValue* end_bp = &(_card_table->write_byte_map_base())[uintptr_t(p + num_heap_words) >> _card_shift];\n+  \/\/ If (p + num_heap_words) is not aligned on card boundary, we also need to dirty last card.\n+  if (((unsigned long long) (p + num_heap_words)) & (CardTable::card_size() - 1)) {\n+    end_bp++;\n+  }\n+  while (bp < end_bp) {\n+    *bp++ = CardTable::dirty_card_val();\n+  }\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_card_as_clean(HeapWord *p) {\n+  size_t index = card_index_for_addr(p);\n+  CardValue* bp = &(_card_table->write_byte_map())[index];\n+  bp[0] = CardTable::clean_card_val();\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_read_card_as_clean(size_t index) {\n+  CardValue* bp = &(_card_table->read_byte_map())[index];\n+  bp[0] = CardTable::clean_card_val();\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_range_as_clean(HeapWord *p, size_t num_heap_words) {\n+  CardValue* bp = &(_card_table->write_byte_map_base())[uintptr_t(p) >> _card_shift];\n+  CardValue* end_bp = &(_card_table->write_byte_map_base())[uintptr_t(p + num_heap_words) >> _card_shift];\n+  \/\/ If (p + num_heap_words) is not aligned on card boundary, we also need to clean last card.\n+  if (((unsigned long long) (p + num_heap_words)) & (CardTable::card_size() - 1)) {\n+    end_bp++;\n+  }\n+  while (bp < end_bp) {\n+    *bp++ = CardTable::clean_card_val();\n+  }\n+}\n+\n+inline size_t\n+ShenandoahDirectCardMarkRememberedSet::cluster_count() const {\n+  return _cluster_count;\n+}\n+\n+\/\/ No lock required because arguments align with card boundaries.\n+template<typename RememberedSet>\n+inline void\n+ShenandoahCardCluster<RememberedSet>::reset_object_range(HeapWord* from, HeapWord* to) {\n+  assert(((((unsigned long long) from) & (CardTable::card_size() - 1)) == 0) &&\n+         ((((unsigned long long) to) & (CardTable::card_size() - 1)) == 0),\n+         \"reset_object_range bounds must align with card boundaries\");\n+  size_t card_at_start = _rs->card_index_for_addr(from);\n+  size_t num_cards = (to - from) \/ CardTable::card_size_in_words();\n+\n+  for (size_t i = 0; i < num_cards; i++) {\n+    object_starts[card_at_start + i].short_word = 0;\n+  }\n+}\n+\n+\/\/ Assume only one thread at a time registers objects pertaining to\n+\/\/ each card-table entry's range of memory.\n+template<typename RememberedSet>\n+inline void\n+ShenandoahCardCluster<RememberedSet>::register_object(HeapWord* address) {\n+  shenandoah_assert_heaplocked();\n+\n+  register_object_wo_lock(address);\n+}\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahCardCluster<RememberedSet>::register_object_wo_lock(HeapWord* address) {\n+  size_t card_at_start = _rs->card_index_for_addr(address);\n+  HeapWord *card_start_address = _rs->addr_for_card_index(card_at_start);\n+  uint8_t offset_in_card = address - card_start_address;\n+\n+  if (!starts_object(card_at_start)) {\n+    set_starts_object_bit(card_at_start);\n+    set_first_start(card_at_start, offset_in_card);\n+    set_last_start(card_at_start, offset_in_card);\n+  } else {\n+    if (offset_in_card < get_first_start(card_at_start))\n+      set_first_start(card_at_start, offset_in_card);\n+    if (offset_in_card > get_last_start(card_at_start))\n+      set_last_start(card_at_start, offset_in_card);\n+  }\n+}\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahCardCluster<RememberedSet>::coalesce_objects(HeapWord* address, size_t length_in_words) {\n+\n+  size_t card_at_start = _rs->card_index_for_addr(address);\n+  HeapWord *card_start_address = _rs->addr_for_card_index(card_at_start);\n+  size_t card_at_end = card_at_start + ((address + length_in_words) - card_start_address) \/ CardTable::card_size_in_words();\n+\n+  if (card_at_start == card_at_end) {\n+    \/\/ There are no changes to the get_first_start array.  Either get_first_start(card_at_start) returns this coalesced object,\n+    \/\/ or it returns an object that precedes the coalesced object.\n+    if (card_start_address + get_last_start(card_at_start) < address + length_in_words) {\n+      uint8_t coalesced_offset = static_cast<uint8_t>(address - card_start_address);\n+      \/\/ The object that used to be the last object starting within this card is being subsumed within the coalesced\n+      \/\/ object.  Since we always coalesce entire objects, this condition only occurs if the last object ends before or at\n+      \/\/ the end of the card's memory range and there is no object following this object.  In this case, adjust last_start\n+      \/\/ to represent the start of the coalesced range.\n+      set_last_start(card_at_start, coalesced_offset);\n+    }\n+    \/\/ Else, no changes to last_starts information.  Either get_last_start(card_at_start) returns the object that immediately\n+    \/\/ follows the coalesced object, or it returns an object that follows the object immediately following the coalesced object.\n+  } else {\n+    uint8_t coalesced_offset = static_cast<uint8_t>(address - card_start_address);\n+    if (get_last_start(card_at_start) > coalesced_offset) {\n+      \/\/ Existing last start is being coalesced, create new last start\n+      set_last_start(card_at_start, coalesced_offset);\n+    }\n+    \/\/ otherwise, get_last_start(card_at_start) must equal coalesced_offset\n+\n+    \/\/ All the cards between first and last get cleared.\n+    for (size_t i = card_at_start + 1; i < card_at_end; i++) {\n+      clear_starts_object_bit(i);\n+    }\n+\n+    uint8_t follow_offset = static_cast<uint8_t>((address + length_in_words) - _rs->addr_for_card_index(card_at_end));\n+    if (starts_object(card_at_end) && (get_first_start(card_at_end) < follow_offset)) {\n+      \/\/ It may be that after coalescing within this last card's memory range, the last card\n+      \/\/ no longer holds an object.\n+      if (get_last_start(card_at_end) >= follow_offset) {\n+        set_first_start(card_at_end, follow_offset);\n+      } else {\n+        \/\/ last_start is being coalesced so this card no longer has any objects.\n+        clear_starts_object_bit(card_at_end);\n+      }\n+    }\n+    \/\/ else\n+    \/\/  card_at_end did not have an object, so it still does not have an object, or\n+    \/\/  card_at_end had an object that starts after the coalesced object, so no changes required for card_at_end\n+\n+  }\n+}\n+\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahCardCluster<RememberedSet>::get_first_start(size_t card_index) const {\n+  assert(starts_object(card_index), \"Can't get first start because no object starts here\");\n+  return object_starts[card_index].offsets.first & FirstStartBits;\n+}\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahCardCluster<RememberedSet>::get_last_start(size_t card_index) const {\n+  assert(starts_object(card_index), \"Can't get last start because no object starts here\");\n+  return object_starts[card_index].offsets.last;\n+}\n+\n+\/\/ Given a card_index, return the starting address of the first block in the heap\n+\/\/ that straddles into this card. If this card is co-initial with an object, then\n+\/\/ this would return the first address of the range that this card covers, which is\n+\/\/ where the card's first object also begins.\n+\/\/ TODO: collect some stats for the size of walks backward over cards.\n+\/\/ For larger objects, a logarithmic BOT such as used by G1 might make the\n+\/\/ backwards walk potentially faster.\n+template<typename RememberedSet>\n+HeapWord*\n+ShenandoahCardCluster<RememberedSet>::block_start(const size_t card_index) const {\n+\n+  HeapWord* left = _rs->addr_for_card_index(card_index);\n+\n+#ifdef ASSERT\n+  assert(ShenandoahHeap::heap()->mode()->is_generational(), \"Do not use in non-generational mode\");\n+  ShenandoahHeapRegion* region = ShenandoahHeap::heap()->heap_region_containing(left);\n+  assert(region->is_old(), \"Do not use for young regions\");\n+  \/\/ For HumongousRegion:s it's more efficient to jump directly to the\n+  \/\/ start region.\n+  assert(!region->is_humongous(), \"Use region->humongous_start_region() instead\");\n+#endif\n+  if (starts_object(card_index) && get_first_start(card_index) == 0) {\n+    \/\/ This card contains a co-initial object; a fortiori, it covers\n+    \/\/ also the case of a card being the first in a region.\n+    assert(oopDesc::is_oop(cast_to_oop(left)), \"Should be an object\");\n+    return left;\n+  }\n+\n+  HeapWord* p = nullptr;\n+  oop obj = cast_to_oop(p);\n+  ssize_t cur_index = (ssize_t)card_index;\n+  assert(cur_index >= 0, \"Overflow\");\n+  assert(cur_index > 0, \"Should have returned above\");\n+  \/\/ Walk backwards over the cards...\n+  while (--cur_index > 0 && !starts_object(cur_index)) {\n+   \/\/ ... to the one that starts the object\n+  }\n+  \/\/ cur_index should start an object: we should not have walked\n+  \/\/ past the left end of the region.\n+  assert(cur_index >= 0 && (cur_index <= (ssize_t)card_index), \"Error\");\n+  assert(region->bottom() <= _rs->addr_for_card_index(cur_index),\n+         \"Fell off the bottom of containing region\");\n+  assert(starts_object(cur_index), \"Error\");\n+  size_t offset = get_last_start(cur_index);\n+  \/\/ can avoid call via card size arithmetic below instead\n+  p = _rs->addr_for_card_index(cur_index) + offset;\n+  \/\/ Recall that we already dealt with the co-initial object case above\n+  assert(p < left, \"obj should start before left\");\n+  \/\/ While it is safe to ask an object its size in the loop that\n+  \/\/ follows, the (ifdef'd out) loop should never be needed.\n+  \/\/ 1. we ask this question only for regions in the old generation\n+  \/\/ 2. there is no direct allocation ever by mutators in old generation\n+  \/\/    regions. Only GC will ever allocate in old regions, and then\n+  \/\/    too only during promotion\/evacuation phases. Thus there is no danger\n+  \/\/    of races between reading from and writing to the object start array,\n+  \/\/    or of asking partially initialized objects their size (in the loop below).\n+  \/\/ 3. only GC asks this question during phases when it is not concurrently\n+  \/\/    evacuating\/promoting, viz. during concurrent root scanning (before\n+  \/\/    the evacuation phase) and during concurrent update refs (after the\n+  \/\/    evacuation phase) of young collections. This is never called\n+  \/\/    during old or global collections.\n+  \/\/ 4. Every allocation under TAMS updates the object start array.\n+  NOT_PRODUCT(obj = cast_to_oop(p);)\n+  assert(oopDesc::is_oop(obj), \"Should be an object\");\n+#define WALK_FORWARD_IN_BLOCK_START false\n+  while (WALK_FORWARD_IN_BLOCK_START && p + obj->size() < left) {\n+    p += obj->size();\n+  }\n+#undef WALK_FORWARD_IN_BLOCK_START \/\/ false\n+  assert(p + obj->size() > left, \"obj should end after left\");\n+  return p;\n+}\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahScanRemembered<RememberedSet>::last_valid_index() { return _rs->last_valid_index(); }\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahScanRemembered<RememberedSet>::total_cards() { return _rs->total_cards(); }\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahScanRemembered<RememberedSet>::card_index_for_addr(HeapWord *p) { return _rs->card_index_for_addr(p); };\n+\n+template<typename RememberedSet>\n+inline HeapWord *\n+ShenandoahScanRemembered<RememberedSet>::addr_for_card_index(size_t card_index) { return _rs->addr_for_card_index(card_index); }\n+\n+template<typename RememberedSet>\n+inline bool\n+ShenandoahScanRemembered<RememberedSet>::is_card_dirty(size_t card_index) { return _rs->is_card_dirty(card_index); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_card_as_dirty(size_t card_index) { _rs->mark_card_as_dirty(card_index); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_range_as_dirty(size_t card_index, size_t num_cards) { _rs->mark_range_as_dirty(card_index, num_cards); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_card_as_clean(size_t card_index) { _rs->mark_card_as_clean(card_index); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_range_as_clean(size_t card_index, size_t num_cards) { _rs->mark_range_as_clean(card_index, num_cards); }\n+\n+template<typename RememberedSet>\n+inline bool\n+ShenandoahScanRemembered<RememberedSet>::is_card_dirty(HeapWord *p) { return _rs->is_card_dirty(p); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_card_as_dirty(HeapWord *p) { _rs->mark_card_as_dirty(p); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_range_as_dirty(HeapWord *p, size_t num_heap_words) { _rs->mark_range_as_dirty(p, num_heap_words); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_card_as_clean(HeapWord *p) { _rs->mark_card_as_clean(p); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>:: mark_range_as_clean(HeapWord *p, size_t num_heap_words) { _rs->mark_range_as_clean(p, num_heap_words); }\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahScanRemembered<RememberedSet>::cluster_count() { return _rs->cluster_count(); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::reset_object_range(HeapWord *from, HeapWord *to) {\n+  _scc->reset_object_range(from, to);\n+}\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::register_object(HeapWord *addr) {\n+  _scc->register_object(addr);\n+}\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::register_object_wo_lock(HeapWord *addr) {\n+  _scc->register_object_wo_lock(addr);\n+}\n+\n+template <typename RememberedSet>\n+inline bool\n+ShenandoahScanRemembered<RememberedSet>::verify_registration(HeapWord* address, ShenandoahMarkingContext* ctx) {\n+\n+  size_t index = card_index_for_addr(address);\n+  if (!_scc->starts_object(index)) {\n+    return false;\n+  }\n+  HeapWord* base_addr = addr_for_card_index(index);\n+  size_t offset = _scc->get_first_start(index);\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  \/\/ Verify that I can find this object within its enclosing card by scanning forward from first_start.\n+  while (base_addr + offset < address) {\n+    oop obj = cast_to_oop(base_addr + offset);\n+    if (!ctx || ctx->is_marked(obj)) {\n+      offset += obj->size();\n+    } else {\n+      \/\/ If this object is not live, don't trust its size(); all objects above tams are live.\n+      ShenandoahHeapRegion* r = heap->heap_region_containing(obj);\n+      HeapWord* tams = ctx->top_at_mark_start(r);\n+      offset = ctx->get_next_marked_addr(base_addr + offset, tams) - base_addr;\n+    }\n+  }\n+  if (base_addr + offset != address){\n+    return false;\n+  }\n+\n+  \/\/ At this point, offset represents object whose registration we are verifying.  We know that at least this object resides\n+  \/\/ within this card's memory.\n+\n+  \/\/ Make sure that last_offset is properly set for the enclosing card, but we can't verify this for\n+  \/\/ candidate collection-set regions during mixed evacuations, so disable this check in general\n+  \/\/ during mixed evacuations.\n+\n+  ShenandoahHeapRegion* r = heap->heap_region_containing(base_addr + offset);\n+  size_t max_offset = r->top() - base_addr;\n+  if (max_offset > CardTable::card_size_in_words()) {\n+    max_offset = CardTable::card_size_in_words();\n+  }\n+  size_t prev_offset;\n+  if (!ctx) {\n+    do {\n+      oop obj = cast_to_oop(base_addr + offset);\n+      prev_offset = offset;\n+      offset += obj->size();\n+    } while (offset < max_offset);\n+    if (_scc->get_last_start(index) != prev_offset) {\n+      return false;\n+    }\n+\n+    \/\/ base + offset represents address of first object that starts on following card, if there is one.\n+\n+    \/\/ Notes: base_addr is addr_for_card_index(index)\n+    \/\/        base_addr + offset is end of the object we are verifying\n+    \/\/        cannot use card_index_for_addr(base_addr + offset) because it asserts arg < end of whole heap\n+    size_t end_card_index = index + offset \/ CardTable::card_size_in_words();\n+\n+    if (end_card_index > index && end_card_index <= _rs->last_valid_index()) {\n+      \/\/ If there is a following object registered on the next card, it should begin where this object ends.\n+      if (_scc->starts_object(end_card_index) &&\n+          ((addr_for_card_index(end_card_index) + _scc->get_first_start(end_card_index)) != (base_addr + offset))) {\n+        return false;\n+      }\n+    }\n+\n+    \/\/ Assure that no other objects are registered \"inside\" of this one.\n+    for (index++; index < end_card_index; index++) {\n+      if (_scc->starts_object(index)) {\n+        return false;\n+      }\n+    }\n+  } else {\n+    \/\/ This is a mixed evacuation or a global collect: rely on mark bits to identify which objects need to be properly registered\n+    assert(!ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress(), \"Cannot rely on mark context here.\");\n+    \/\/ If the object reaching or spanning the end of this card's memory is marked, then last_offset for this card\n+    \/\/ should represent this object.  Otherwise, last_offset is a don't care.\n+    ShenandoahHeapRegion* region = heap->heap_region_containing(base_addr + offset);\n+    HeapWord* tams = ctx->top_at_mark_start(region);\n+    oop last_obj = nullptr;\n+    do {\n+      oop obj = cast_to_oop(base_addr + offset);\n+      if (ctx->is_marked(obj)) {\n+        prev_offset = offset;\n+        offset += obj->size();\n+        last_obj = obj;\n+      } else {\n+        offset = ctx->get_next_marked_addr(base_addr + offset, tams) - base_addr;\n+        \/\/ If there are no marked objects remaining in this region, offset equals tams - base_addr.  If this offset is\n+        \/\/ greater than max_offset, we will immediately exit this loop.  Otherwise, the next iteration of the loop will\n+        \/\/ treat the object at offset as marked and live (because address >= tams) and we will continue iterating object\n+        \/\/ by consulting the size() fields of each.\n+      }\n+    } while (offset < max_offset);\n+    if (last_obj != nullptr && prev_offset + last_obj->size() >= max_offset) {\n+      \/\/ last marked object extends beyond end of card\n+      if (_scc->get_last_start(index) != prev_offset) {\n+        return false;\n+      }\n+      \/\/ otherwise, the value of _scc->get_last_start(index) is a don't care because it represents a dead object and we\n+      \/\/ cannot verify its context\n+    }\n+  }\n+  return true;\n+}\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::coalesce_objects(HeapWord *addr, size_t length_in_words) {\n+  _scc->coalesce_objects(addr, length_in_words);\n+}\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_range_as_empty(HeapWord *addr, size_t length_in_words) {\n+  _rs->mark_range_as_clean(addr, length_in_words);\n+  _scc->clear_objects_in_range(addr, length_in_words);\n+}\n+\n+\/\/ Process all objects starting within count clusters beginning with first_cluster and for which the start address is\n+\/\/ less than end_of_range.  For any non-array object whose header lies on a dirty card, scan the entire object,\n+\/\/ even if its end reaches beyond end_of_range. Object arrays, on the other hand, are precisely dirtied and\n+\/\/ only the portions of the array on dirty cards need to be scanned.\n+\/\/\n+\/\/ Do not CANCEL within process_clusters.  It is assumed that if a worker thread accepts responsibility for processing\n+\/\/ a chunk of work, it will finish the work it starts.  Otherwise, the chunk of work will be lost in the transition to\n+\/\/ degenerated execution, leading to dangling references.\n+template<typename RememberedSet>\n+template <typename ClosureType>\n+void ShenandoahScanRemembered<RememberedSet>::process_clusters(size_t first_cluster, size_t count, HeapWord* end_of_range,\n+                                                               ClosureType* cl, bool use_write_table, uint worker_id) {\n+\n+  \/\/ If old-gen evacuation is active, then MarkingContext for old-gen heap regions is valid.  We use the MarkingContext\n+  \/\/ bits to determine which objects within a DIRTY card need to be scanned.  This is necessary because old-gen heap\n+  \/\/ regions that are in the candidate collection set have not been coalesced and filled.  Thus, these heap regions\n+  \/\/ may contain zombie objects.  Zombie objects are known to be dead, but have not yet been \"collected\".  Scanning\n+  \/\/ zombie objects is unsafe because the Klass pointer is not reliable, objects referenced from a zombie may have been\n+  \/\/ collected (if dead), or relocated (if live), or if dead but not yet collected, we don't want to \"revive\" them\n+  \/\/ by marking them (when marking) or evacuating them (when updating references).\n+\n+  \/\/ start and end addresses of range of objects to be scanned, clipped to end_of_range\n+  const size_t start_card_index = first_cluster * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+  const HeapWord* start_addr = _rs->addr_for_card_index(start_card_index);\n+  \/\/ clip at end_of_range (exclusive)\n+  HeapWord* end_addr = MIN2(end_of_range, (HeapWord*)start_addr + (count * ShenandoahCardCluster<RememberedSet>::CardsPerCluster\n+                                                                   * CardTable::card_size_in_words()));\n+  assert(start_addr < end_addr, \"Empty region?\");\n+\n+  const size_t whole_cards = (end_addr - start_addr + CardTable::card_size_in_words() - 1)\/CardTable::card_size_in_words();\n+  const size_t end_card_index = start_card_index + whole_cards - 1;\n+  log_debug(gc, remset)(\"Worker %u: cluster = \" SIZE_FORMAT \" count = \" SIZE_FORMAT \" eor = \" INTPTR_FORMAT\n+                        \" start_addr = \" INTPTR_FORMAT \" end_addr = \" INTPTR_FORMAT \" cards = \" SIZE_FORMAT,\n+                        worker_id, first_cluster, count, p2i(end_of_range), p2i(start_addr), p2i(end_addr), whole_cards);\n+\n+  \/\/ use_write_table states whether we are using the card table that is being\n+  \/\/ marked by the mutators. If false, we are using a snapshot of the card table\n+  \/\/ that is not subject to modifications. Even when this arg is true, and\n+  \/\/ the card table is being actively marked, SATB marking ensures that we need not\n+  \/\/ worry about cards marked after the processing here has passed them.\n+  const CardValue* const ctbm = _rs->get_card_table_byte_map(use_write_table);\n+\n+  \/\/ If old gen evacuation is active, ctx will hold the completed marking of\n+  \/\/ old generation objects. We'll only scan objects that are marked live by\n+  \/\/ the old generation marking. These include objects allocated since the\n+  \/\/ start of old generation marking (being those above TAMS).\n+  const ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  const ShenandoahMarkingContext* ctx = heap->is_old_bitmap_stable() ?\n+                                        heap->marking_context() : nullptr;\n+\n+  \/\/ The region we will scan is the half-open interval [start_addr, end_addr),\n+  \/\/ and lies entirely within a single region.\n+  const ShenandoahHeapRegion* region = ShenandoahHeap::heap()->heap_region_containing(start_addr);\n+  assert(region->contains(end_addr - 1), \"Slice shouldn't cross regions\");\n+\n+  \/\/ This code may have implicit assumptions of examining only old gen regions.\n+  assert(region->is_old(), \"We only expect to be processing old regions\");\n+  assert(!region->is_humongous(), \"Humongous regions can be processed more efficiently;\"\n+                                  \"see process_humongous_clusters()\");\n+  \/\/ tams and ctx below are for old generation marking. As such, young gen roots must\n+  \/\/ consider everything above tams, since it doesn't represent a TAMS for young gen's\n+  \/\/ SATB marking.\n+  const HeapWord* tams = (ctx == nullptr ? region->bottom() : ctx->top_at_mark_start(region));\n+\n+  NOT_PRODUCT(ShenandoahCardStats stats(whole_cards, card_stats(worker_id));)\n+\n+  \/\/ In the case of imprecise marking, we remember the lowest address\n+  \/\/ scanned in a range of dirty cards, as we work our way left from the\n+  \/\/ highest end_addr. This serves as another upper bound on the address we will\n+  \/\/ scan as we move left over each contiguous range of dirty cards.\n+  HeapWord* upper_bound = nullptr;\n+\n+  \/\/ Starting at the right end of the address range, walk backwards accumulating\n+  \/\/ a maximal dirty range of cards, then process those cards.\n+  ssize_t cur_index = (ssize_t) end_card_index;\n+  assert(cur_index >= 0, \"Overflow\");\n+  assert(((ssize_t)start_card_index) >= 0, \"Overflow\");\n+  while (cur_index >= (ssize_t)start_card_index) {\n+\n+    \/\/ We'll continue the search starting with the card for the upper bound\n+    \/\/ address identified by the last dirty range that we processed, if any,\n+    \/\/ skipping any cards at higher addresses.\n+    if (upper_bound != nullptr) {\n+      ssize_t right_index = _rs->card_index_for_addr(upper_bound);\n+      assert(right_index >= 0, \"Overflow\");\n+      cur_index = MIN2(cur_index, right_index);\n+      assert(upper_bound < end_addr, \"Program logic\");\n+      end_addr  = upper_bound;   \/\/ lower end_addr\n+      upper_bound = nullptr;     \/\/ and clear upper_bound\n+      if (end_addr <= start_addr) {\n+        assert(right_index <= (ssize_t)start_card_index, \"Program logic\");\n+        \/\/ We are done with our cluster\n+        return;\n+      }\n+    }\n+\n+    if (ctbm[cur_index] == CardTable::dirty_card_val()) {\n+      \/\/ ==== BEGIN DIRTY card range processing ====\n+\n+      const size_t dirty_r = cur_index;  \/\/ record right end of dirty range (inclusive)\n+      while (--cur_index >= (ssize_t)start_card_index && ctbm[cur_index] == CardTable::dirty_card_val()) {\n+        \/\/ walk back over contiguous dirty cards to find left end of dirty range (inclusive)\n+      }\n+      \/\/ [dirty_l, dirty_r] is a \"maximal\" closed interval range of dirty card indices:\n+      \/\/ it may not be maximal if we are using the write_table, because of concurrent\n+      \/\/ mutations dirtying the card-table. It may also not be maximal if an upper bound\n+      \/\/ was established by the scan of the previous chunk.\n+      const size_t dirty_l = cur_index + 1;   \/\/ record left end of dirty range (inclusive)\n+      \/\/ Check that we identified a boundary on our left\n+      assert(ctbm[dirty_l] == CardTable::dirty_card_val(), \"First card in range should be dirty\");\n+      assert(dirty_l == start_card_index || use_write_table\n+             || ctbm[dirty_l - 1] == CardTable::clean_card_val(),\n+             \"Interval isn't maximal on the left\");\n+      assert(dirty_r >= dirty_l, \"Error\");\n+      assert(ctbm[dirty_r] == CardTable::dirty_card_val(), \"Last card in range should be dirty\");\n+      \/\/ Record alternations, dirty run length, and dirty card count\n+      NOT_PRODUCT(stats.record_dirty_run(dirty_r - dirty_l + 1);)\n+\n+      \/\/ Find first object that starts this range:\n+      \/\/ [left, right) is a maximal right-open interval of dirty cards\n+      HeapWord* left = _rs->addr_for_card_index(dirty_l);        \/\/ inclusive\n+      HeapWord* right = _rs->addr_for_card_index(dirty_r + 1);   \/\/ exclusive\n+      \/\/ Clip right to end_addr established above (still exclusive)\n+      right = MIN2(right, end_addr);\n+      assert(right <= region->top() && end_addr <= region->top(), \"Busted bounds\");\n+      const MemRegion mr(left, right);\n+\n+      \/\/ NOTE: We'll not call block_start() repeatedly\n+      \/\/ on a very large object if its head card is dirty. If not,\n+      \/\/ (i.e. the head card is clean) we'll call it each time we\n+      \/\/ process a new dirty range on the object. This is always\n+      \/\/ the case for large object arrays, which are typically more\n+      \/\/ common.\n+      \/\/ TODO: It is worthwhile to memoize this, so as to avoid that\n+      \/\/ overhead, and it is easy to do, but deferred to a follow-up.\n+      HeapWord* p = _scc->block_start(dirty_l);\n+      oop obj = cast_to_oop(p);\n+\n+      \/\/ PREFIX: The object that straddles into this range of dirty cards\n+      \/\/ from the left may be subject to special treatment unless\n+      \/\/ it is an object array.\n+      if (p < left && !obj->is_objArray()) {\n+        \/\/ The mutator (both compiler and interpreter, but not JNI?)\n+        \/\/ typically dirty imprecisely (i.e. only the head of an object),\n+        \/\/ but GC closures typically dirty the object precisely. (It would\n+        \/\/ be nice to have everything be precise for maximum efficiency.)\n+        \/\/\n+        \/\/ To handle this, we check the head card of the object here and,\n+        \/\/ if dirty, (arrange to) scan the object in its entirety. If we\n+        \/\/ find the head card clean, we'll scan only the portion of the\n+        \/\/ object lying in the dirty card range below, assuming this was\n+        \/\/ the result of precise marking by GC closures.\n+\n+        \/\/ index of the \"head card\" for p\n+        const size_t hc_index = _rs->card_index_for_addr(p);\n+        if (ctbm[hc_index] == CardTable::dirty_card_val()) {\n+          \/\/ Scan or skip the object, depending on location of its\n+          \/\/ head card, and remember that we'll have processed all\n+          \/\/ the objects back up to p, which is thus an upper bound\n+          \/\/ for the next iteration of a dirty card loop.\n+          upper_bound = p;   \/\/ remember upper bound for next chunk\n+          if (p < start_addr) {\n+            \/\/ if object starts in a previous slice, it'll be handled\n+            \/\/ in its entirety by the thread processing that slice; we can\n+            \/\/ skip over it and avoid an unnecessary extra scan.\n+            assert(obj == cast_to_oop(p), \"Inconsistency detected\");\n+            p += obj->size();\n+          } else {\n+            \/\/ the object starts in our slice, we scan it in its entirety\n+            assert(obj == cast_to_oop(p), \"Inconsistency detected\");\n+            if (ctx == nullptr || ctx->is_marked(obj)) {\n+              \/\/ Scan the object in its entirety\n+              p += obj->oop_iterate_size(cl);\n+            } else {\n+              assert(p < tams, \"Error 1 in ctx\/marking\/tams logic\");\n+              \/\/ Skip over any intermediate dead objects\n+              p = ctx->get_next_marked_addr(p, tams);\n+              assert(p <= tams, \"Error 2 in ctx\/marking\/tams logic\");\n+            }\n+          }\n+          assert(p > left, \"Should have processed into interior of dirty range\");\n+        }\n+      }\n+\n+      size_t i = 0;\n+      HeapWord* last_p = nullptr;\n+\n+      \/\/ BODY: Deal with (other) objects in this dirty card range\n+      while (p < right) {\n+        obj = cast_to_oop(p);\n+        \/\/ walk right scanning eligible objects\n+        if (ctx == nullptr || ctx->is_marked(obj)) {\n+          \/\/ we need to remember the last object ptr we scanned, in case we need to\n+          \/\/ complete a partial suffix scan after mr, see below\n+          last_p = p;\n+          \/\/ apply the closure to the oops in the portion of\n+          \/\/ the object within mr.\n+          p += obj->oop_iterate_size(cl, mr);\n+          NOT_PRODUCT(i++);\n+        } else {\n+          \/\/ forget the last object pointer we remembered\n+          last_p = nullptr;\n+          assert(p < tams, \"Tams and above are implicitly marked in ctx\");\n+          \/\/ object under tams isn't marked: skip to next live object\n+          p = ctx->get_next_marked_addr(p, tams);\n+          assert(p <= tams, \"Error 3 in ctx\/marking\/tams logic\");\n+        }\n+      }\n+\n+      \/\/ TODO: if an objArray then only use mr, else just iterate over entire object;\n+      \/\/ that would avoid the special treatment of suffix below.\n+\n+      \/\/ SUFFIX: Fix up a possible incomplete scan at right end of window\n+      \/\/ by scanning the portion of a non-objArray that wasn't done.\n+      if (p > right && last_p != nullptr) {\n+        assert(last_p < right, \"Error\");\n+        \/\/ check if last_p suffix needs scanning\n+        const oop last_obj = cast_to_oop(last_p);\n+        if (!last_obj->is_objArray()) {\n+          \/\/ scan the remaining suffix of the object\n+          const MemRegion last_mr(right, p);\n+          assert(p == last_p + last_obj->size(), \"Would miss portion of last_obj\");\n+          last_obj->oop_iterate(cl, last_mr);\n+          log_debug(gc, remset)(\"Fixed up non-objArray suffix scan in [\" INTPTR_FORMAT \", \" INTPTR_FORMAT \")\",\n+                                p2i(last_mr.start()), p2i(last_mr.end()));\n+        } else {\n+          log_debug(gc, remset)(\"Skipped suffix scan of objArray in [\" INTPTR_FORMAT \", \" INTPTR_FORMAT \")\",\n+                                p2i(right), p2i(p));\n+        }\n+      }\n+      NOT_PRODUCT(stats.record_scan_obj_cnt(i);)\n+\n+      \/\/ ==== END   DIRTY card range processing ====\n+    } else {\n+      \/\/ ==== BEGIN CLEAN card range processing ====\n+\n+      assert(ctbm[cur_index] == CardTable::clean_card_val(), \"Error\");\n+      \/\/ walk back over contiguous clean cards\n+      size_t i = 0;\n+      while (--cur_index >= (ssize_t)start_card_index && ctbm[cur_index] == CardTable::clean_card_val()) {\n+        NOT_PRODUCT(i++);\n+      }\n+      \/\/ Record alternations, clean run length, and clean card count\n+      NOT_PRODUCT(stats.record_clean_run(i);)\n+\n+      \/\/ ==== END CLEAN card range processing ====\n+    }\n+  }\n+}\n+\n+\/\/ Given that this range of clusters is known to span a humongous object spanned by region r, scan the\n+\/\/ portion of the humongous object that corresponds to the specified range.\n+template<typename RememberedSet>\n+template <typename ClosureType>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::process_humongous_clusters(ShenandoahHeapRegion* r, size_t first_cluster, size_t count,\n+                                                                    HeapWord *end_of_range, ClosureType *cl, bool use_write_table) {\n+  ShenandoahHeapRegion* start_region = r->humongous_start_region();\n+  HeapWord* p = start_region->bottom();\n+  oop obj = cast_to_oop(p);\n+  assert(r->is_humongous(), \"Only process humongous regions here\");\n+  assert(start_region->is_humongous_start(), \"Should be start of humongous region\");\n+  assert(p + obj->size() >= end_of_range, \"Humongous object ends before range ends\");\n+\n+  size_t first_card_index = first_cluster * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+  HeapWord* first_cluster_addr = _rs->addr_for_card_index(first_card_index);\n+  size_t spanned_words = count * ShenandoahCardCluster<RememberedSet>::CardsPerCluster * CardTable::card_size_in_words();\n+  start_region->oop_iterate_humongous_slice(cl, true, first_cluster_addr, spanned_words, use_write_table);\n+}\n+\n+\n+\/\/ This method takes a region & determines the end of the region that the worker can scan.\n+template<typename RememberedSet>\n+template <typename ClosureType>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::process_region_slice(ShenandoahHeapRegion *region, size_t start_offset, size_t clusters,\n+                                                              HeapWord *end_of_range, ClosureType *cl, bool use_write_table,\n+                                                              uint worker_id) {\n+\n+  \/\/ This is called only for young gen collection, when we scan old gen regions\n+  assert(region->is_old(), \"Expecting an old region\");\n+  HeapWord *start_of_range = region->bottom() + start_offset;\n+  size_t start_cluster_no = cluster_for_addr(start_of_range);\n+  assert(addr_for_cluster(start_cluster_no) == start_of_range, \"process_region_slice range must align on cluster boundary\");\n+\n+  \/\/ region->end() represents the end of memory spanned by this region, but not all of this\n+  \/\/   memory is eligible to be scanned because some of this memory has not yet been allocated.\n+  \/\/\n+  \/\/ region->top() represents the end of allocated memory within this region.  Any addresses\n+  \/\/   beyond region->top() should not be scanned as that memory does not hold valid objects.\n+\n+  if (use_write_table) {\n+    \/\/ This is update-refs servicing.\n+    if (end_of_range > region->get_update_watermark()) {\n+      end_of_range = region->get_update_watermark();\n+    }\n+  } else {\n+    \/\/ This is concurrent mark servicing.  Note that TAMS for this region is TAMS at start of old-gen\n+    \/\/ collection.  Here, we need to scan up to TAMS for most recently initiated young-gen collection.\n+    \/\/ Since all LABs are retired at init mark, and since replacement LABs are allocated lazily, and since no\n+    \/\/ promotions occur until evacuation phase, TAMS for most recent young-gen is same as top().\n+    if (end_of_range > region->top()) {\n+      end_of_range = region->top();\n+    }\n+  }\n+\n+  log_debug(gc)(\"Remembered set scan processing Region \" SIZE_FORMAT \", from \" PTR_FORMAT \" to \" PTR_FORMAT \", using %s table\",\n+                region->index(), p2i(start_of_range), p2i(end_of_range),\n+                use_write_table? \"read\/write (updating)\": \"read (marking)\");\n+\n+  \/\/ Note that end_of_range may point to the middle of a cluster because we limit scanning to\n+  \/\/ region->top() or region->get_update_watermark(). We avoid processing past end_of_range.\n+  \/\/ Objects that start between start_of_range and end_of_range, including humongous objects, will\n+  \/\/ be fully processed by process_clusters. In no case should we need to scan past end_of_range.\n+  if (start_of_range < end_of_range) {\n+    if (region->is_humongous()) {\n+      ShenandoahHeapRegion* start_region = region->humongous_start_region();\n+      \/\/ TODO: ysr : This will be called multiple times with same start_region, but different start_cluster_no.\n+      \/\/ Check that it does the right thing here, and doesn't do redundant work. Also see if the call API\/interface\n+      \/\/ can be simplified.\n+      process_humongous_clusters(start_region, start_cluster_no, clusters, end_of_range, cl, use_write_table);\n+    } else {\n+      \/\/ TODO: ysr The start_of_range calculated above is discarded and may be calculated again in process_clusters().\n+      \/\/ See if the redundant and wasted calculations can be avoided, and if the call parameters can be cleaned up.\n+      \/\/ It almost sounds like this set of methods needs a working class to stash away some useful info that can be\n+      \/\/ efficiently passed around amongst these methods, as well as related state. Note that we can't use\n+      \/\/ ShenandoahScanRemembered as there seems to be only one instance of that object for the heap which is shared\n+      \/\/ by all workers. Note that there are also task methods which call these which may have per worker storage.\n+      \/\/ We need to be careful however that if the number of workers changes dynamically that state isn't sequestered\n+      \/\/ and become obsolete.\n+      process_clusters(start_cluster_no, clusters, end_of_range, cl, use_write_table, worker_id);\n+    }\n+  }\n+}\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahScanRemembered<RememberedSet>::cluster_for_addr(HeapWordImpl **addr) {\n+  size_t card_index = _rs->card_index_for_addr(addr);\n+  size_t result = card_index \/ ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+  return result;\n+}\n+\n+template<typename RememberedSet>\n+inline HeapWord*\n+ShenandoahScanRemembered<RememberedSet>::addr_for_cluster(size_t cluster_no) {\n+  size_t card_index = cluster_no * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+  return addr_for_card_index(card_index);\n+}\n+\n+\/\/ This is used only for debug verification so don't worry about making the scan parallel.\n+template<typename RememberedSet>\n+void ShenandoahScanRemembered<RememberedSet>::roots_do(OopIterateClosure* cl) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  for (size_t i = 0, n = heap->num_regions(); i < n; ++i) {\n+    ShenandoahHeapRegion* region = heap->get_region(i);\n+    if (region->is_old() && region->is_active() && !region->is_cset()) {\n+      HeapWord* start_of_range = region->bottom();\n+      HeapWord* end_of_range = region->top();\n+      size_t start_cluster_no = cluster_for_addr(start_of_range);\n+      size_t num_heapwords = end_of_range - start_of_range;\n+      unsigned int cluster_size = CardTable::card_size_in_words() *\n+                                  ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+      size_t num_clusters = (size_t) ((num_heapwords - 1 + cluster_size) \/ cluster_size);\n+\n+      \/\/ Remembered set scanner\n+      if (region->is_humongous()) {\n+        process_humongous_clusters(region->humongous_start_region(), start_cluster_no, num_clusters, end_of_range, cl,\n+                                   false \/* use_write_table *\/);\n+      } else {\n+        process_clusters(start_cluster_no, num_clusters, end_of_range, cl,\n+                         false \/* use_write_table *\/, 0 \/* fake worker id *\/);\n+      }\n+    }\n+  }\n+}\n+\n+#ifndef PRODUCT\n+\/\/ Log given card stats\n+template<typename RememberedSet>\n+inline void ShenandoahScanRemembered<RememberedSet>::log_card_stats(HdrSeq* stats) {\n+  for (int i = 0; i < MAX_CARD_STAT_TYPE; i++) {\n+    log_info(gc, remset)(\"%18s: [ %8.2f %8.2f %8.2f %8.2f %8.2f ]\",\n+      _card_stats_name[i],\n+      stats[i].percentile(0), stats[i].percentile(25),\n+      stats[i].percentile(50), stats[i].percentile(75),\n+      stats[i].maximum());\n+  }\n+}\n+\n+\/\/ Log card stats for all nworkers for a specific phase t\n+template<typename RememberedSet>\n+void ShenandoahScanRemembered<RememberedSet>::log_card_stats(uint nworkers, CardStatLogType t) {\n+  assert(ShenandoahEnableCardStats, \"Do not call\");\n+  HdrSeq* cum_stats = card_stats_for_phase(t);\n+  log_info(gc, remset)(\"%s\", _card_stat_log_type[t]);\n+  for (uint i = 0; i < nworkers; i++) {\n+    log_worker_card_stats(i, cum_stats);\n+  }\n+\n+  \/\/ Every so often, log the cumulative global stats\n+  if (++_card_stats_log_counter[t] >= ShenandoahCardStatsLogInterval) {\n+    _card_stats_log_counter[t] = 0;\n+    log_info(gc, remset)(\"Cumulative stats\");\n+    log_card_stats(cum_stats);\n+  }\n+}\n+\n+\/\/ Log card stats for given worker_id, & clear them after merging into given cumulative stats\n+template<typename RememberedSet>\n+void ShenandoahScanRemembered<RememberedSet>::log_worker_card_stats(uint worker_id, HdrSeq* cum_stats) {\n+  assert(ShenandoahEnableCardStats, \"Do not call\");\n+\n+  HdrSeq* worker_card_stats = card_stats(worker_id);\n+  log_info(gc, remset)(\"Worker %u Card Stats: \", worker_id);\n+  log_card_stats(worker_card_stats);\n+  \/\/ Merge worker stats into the cumulative stats & clear worker stats\n+  merge_worker_card_stats_cumulative(worker_card_stats, cum_stats);\n+}\n+\n+template<typename RememberedSet>\n+void ShenandoahScanRemembered<RememberedSet>::merge_worker_card_stats_cumulative(\n+  HdrSeq* worker_stats, HdrSeq* cum_stats) {\n+  for (int i = 0; i < MAX_CARD_STAT_TYPE; i++) {\n+    worker_stats[i].merge(cum_stats[i]);\n+  }\n+}\n+#endif\n+\n+inline bool ShenandoahRegionChunkIterator::has_next() const {\n+  return _index < _total_chunks;\n+}\n+\n+inline bool ShenandoahRegionChunkIterator::next(struct ShenandoahRegionChunk *assignment) {\n+  if (_index >= _total_chunks) {\n+    return false;\n+  }\n+  size_t new_index = Atomic::add(&_index, (size_t) 1, memory_order_relaxed);\n+  if (new_index > _total_chunks) {\n+    \/\/ First worker that hits new_index == _total_chunks continues, other\n+    \/\/ contending workers return false.\n+    return false;\n+  }\n+  \/\/ convert to zero-based indexing\n+  new_index--;\n+  assert(new_index < _total_chunks, \"Error\");\n+\n+  \/\/ Find the group number for the assigned chunk index\n+  size_t group_no;\n+  for (group_no = 0; new_index >= _group_entries[group_no]; group_no++)\n+    ;\n+  assert(group_no < _num_groups, \"Cannot have group no greater or equal to _num_groups\");\n+\n+  \/\/ All size computations measured in HeapWord\n+  size_t region_size_words = ShenandoahHeapRegion::region_size_words();\n+  size_t group_region_index = _region_index[group_no];\n+  size_t group_region_offset = _group_offset[group_no];\n+\n+  size_t index_within_group = (group_no == 0)? new_index: new_index - _group_entries[group_no - 1];\n+  size_t group_chunk_size = _group_chunk_size[group_no];\n+  size_t offset_of_this_chunk = group_region_offset + index_within_group * group_chunk_size;\n+  size_t regions_spanned_by_chunk_offset = offset_of_this_chunk \/ region_size_words;\n+  size_t offset_within_region = offset_of_this_chunk % region_size_words;\n+\n+  size_t region_index = group_region_index + regions_spanned_by_chunk_offset;\n+\n+  assignment->_r = _heap->get_region(region_index);\n+  assignment->_chunk_offset = offset_within_region;\n+  assignment->_chunk_size = group_chunk_size;\n+  return true;\n+}\n+\n+template<class T>\n+inline void ShenandoahVerifyNoYoungRefsClosure::work(T* p) {\n+  T o = RawAccess<>::oop_load(p);\n+  if (!CompressedOops::is_null(o)) {\n+    oop obj = CompressedOops::decode_not_null(o);\n+    assert(!_heap->is_in_young(obj), \"Found a young ref\");\n+  }\n+}\n+\n+#endif   \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBEREDINLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.inline.hpp","additions":1026,"deletions":0,"binary":false,"changes":1026,"status":"added"},{"patch":"@@ -78,3 +78,1 @@\n-    if (_heap->is_concurrent_mark_in_progress()) {\n-      return &_keep_alive_cl;\n-    } else if (_heap->is_concurrent_weak_root_in_progress()) {\n+    if (_heap->is_concurrent_weak_root_in_progress()) {\n@@ -83,0 +81,2 @@\n+    } else if (_heap->is_concurrent_mark_in_progress()) {\n+      return &_keep_alive_cl;\n@@ -95,8 +95,1 @@\n-  if (heap->is_concurrent_mark_in_progress()) {\n-    \/\/ We need to reset all TLABs because they might be below the TAMS, and we need to mark\n-    \/\/ the objects in them. Do not let mutators allocate any new objects in their current TLABs.\n-    \/\/ It is also a good place to resize the TLAB sizes for future allocations.\n-    retire_tlab();\n-\n-    _jt->oops_do_no_frames(closure_from_context(context), &_cb_cl);\n-  } else if (heap->is_concurrent_weak_root_in_progress()) {\n+  if (heap->is_concurrent_weak_root_in_progress()) {\n@@ -111,0 +104,7 @@\n+    _jt->oops_do_no_frames(closure_from_context(context), &_cb_cl);\n+  } else if (heap->is_concurrent_mark_in_progress()) {\n+    \/\/ We need to reset all TLABs because they might be below the TAMS, and we need to mark\n+    \/\/ the objects in them. Do not let mutators allocate any new objects in their current TLABs.\n+    \/\/ It is also a good place to resize the TLAB sizes for future allocations.\n+    retire_tlab();\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahStackWatermark.cpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n@@ -44,0 +45,1 @@\n+\n@@ -45,0 +47,3 @@\n+\n+  \/\/ Thread-local allocation buffer for object evacuations.\n+  \/\/ In generational mode, it is exclusive to the young generation.\n@@ -47,0 +52,1 @@\n+\n@@ -49,0 +55,15 @@\n+  \/\/ Thread-local allocation buffer only used in generational mode.\n+  \/\/ Used both by mutator threads and by GC worker threads\n+  \/\/ for evacuations within the old generation and\n+  \/\/ for promotions from the young generation into the old generation.\n+  PLAB* _plab;\n+  size_t _plab_size;\n+\n+  size_t _plab_evacuated;\n+  size_t _plab_promoted;\n+  size_t _plab_preallocated_promoted;\n+  bool   _plab_allows_promotion; \/\/ If false, no more promotion by this thread during this evacuation phase.\n+  bool   _plab_retries_enabled;\n+\n+  ShenandoahEvacuationStats* _evacuation_stats;\n+\n@@ -56,1 +77,9 @@\n-    _paced_time(0) {\n+    _paced_time(0),\n+    _plab(nullptr),\n+    _plab_size(0),\n+    _plab_evacuated(0),\n+    _plab_promoted(0),\n+    _plab_preallocated_promoted(0),\n+    _plab_allows_promotion(true),\n+    _plab_retries_enabled(true),\n+    _evacuation_stats(new ShenandoahEvacuationStats()) {\n@@ -63,0 +92,8 @@\n+    if (_plab != nullptr) {\n+      ShenandoahHeap::heap()->retire_plab(_plab);\n+      delete _plab;\n+    }\n+\n+    \/\/ TODO: Preserve these stats somewhere for mutator threads.\n+    delete _evacuation_stats;\n+    _evacuation_stats = nullptr;\n@@ -100,0 +137,2 @@\n+    data(thread)->_plab = new PLAB(PLAB::min_size());\n+    data(thread)->_plab_size = 0;\n@@ -114,0 +153,88 @@\n+  static void begin_evacuation(Thread* thread, size_t bytes) {\n+    data(thread)->_evacuation_stats->begin_evacuation(bytes);\n+  }\n+\n+  static void end_evacuation(Thread* thread, size_t bytes, uint age) {\n+    data(thread)->_evacuation_stats->end_evacuation(bytes, age);\n+  }\n+\n+  static ShenandoahEvacuationStats* evacuation_stats(Thread* thread) {\n+    return data(thread)->_evacuation_stats;\n+  }\n+\n+  static PLAB* plab(Thread* thread) {\n+    return data(thread)->_plab;\n+  }\n+\n+  static size_t plab_size(Thread* thread) {\n+    return data(thread)->_plab_size;\n+  }\n+\n+  static void set_plab_size(Thread* thread, size_t v) {\n+    data(thread)->_plab_size = v;\n+  }\n+\n+  static void enable_plab_retries(Thread* thread) {\n+    data(thread)->_plab_retries_enabled = true;\n+  }\n+\n+  static void disable_plab_retries(Thread* thread) {\n+    data(thread)->_plab_retries_enabled = false;\n+  }\n+\n+  static bool plab_retries_enabled(Thread* thread) {\n+    return data(thread)->_plab_retries_enabled;\n+  }\n+\n+  static void enable_plab_promotions(Thread* thread) {\n+    data(thread)->_plab_allows_promotion = true;\n+  }\n+\n+  static void disable_plab_promotions(Thread* thread) {\n+    data(thread)->_plab_allows_promotion = false;\n+  }\n+\n+  static bool allow_plab_promotions(Thread* thread) {\n+    return data(thread)->_plab_allows_promotion;\n+  }\n+\n+  static void reset_plab_evacuated(Thread* thread) {\n+    data(thread)->_plab_evacuated = 0;\n+  }\n+\n+  static void add_to_plab_evacuated(Thread* thread, size_t increment) {\n+    data(thread)->_plab_evacuated += increment;\n+  }\n+\n+  static void subtract_from_plab_evacuated(Thread* thread, size_t increment) {\n+    data(thread)->_plab_evacuated -= increment;\n+  }\n+\n+  static size_t get_plab_evacuated(Thread* thread) {\n+    return data(thread)->_plab_evacuated;\n+  }\n+\n+  static void reset_plab_promoted(Thread* thread) {\n+    data(thread)->_plab_promoted = 0;\n+  }\n+\n+  static void add_to_plab_promoted(Thread* thread, size_t increment) {\n+    data(thread)->_plab_promoted += increment;\n+  }\n+\n+  static void subtract_from_plab_promoted(Thread* thread, size_t increment) {\n+    data(thread)->_plab_promoted -= increment;\n+  }\n+\n+  static size_t get_plab_promoted(Thread* thread) {\n+    return data(thread)->_plab_promoted;\n+  }\n+\n+  static void set_plab_preallocated_promoted(Thread* thread, size_t value) {\n+    data(thread)->_plab_preallocated_promoted = value;\n+  }\n+\n+  static size_t get_plab_preallocated_promoted(Thread* thread) {\n+    return data(thread)->_plab_preallocated_promoted;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahThreadLocalData.hpp","additions":128,"deletions":1,"binary":false,"changes":129,"status":"modified"},{"patch":"@@ -52,1 +52,1 @@\n-    _marking_context(ShenandoahHeap::heap()->complete_marking_context()),\n+    _marking_context(ShenandoahHeap::heap()->marking_context()),\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUnload.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -41,1 +43,1 @@\n-ShenandoahGCSession::ShenandoahGCSession(GCCause::Cause cause) :\n+ShenandoahGCSession::ShenandoahGCSession(GCCause::Cause cause, ShenandoahGeneration* generation) :\n@@ -43,0 +45,1 @@\n+  _generation(generation),\n@@ -47,1 +50,2 @@\n-  _heap->set_gc_cause(cause);\n+  _heap->on_cycle_start(cause, _generation);\n+\n@@ -52,2 +56,0 @@\n-  _heap->shenandoah_policy()->record_cycle_start();\n-  _heap->heuristics()->record_cycle_start();\n@@ -67,1 +69,1 @@\n-  _heap->heuristics()->record_cycle_end();\n+  _heap->on_cycle_end(_generation);\n@@ -70,1 +72,1 @@\n-  _tracer->report_gc_reference_stats(_heap->ref_processor()->reference_process_stats());\n+  _tracer->report_gc_reference_stats(_generation->ref_processor()->reference_process_stats());\n@@ -73,1 +75,1 @@\n-  _heap->set_gc_cause(GCCause::_no_gc);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUtils.cpp","additions":9,"deletions":7,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+class ShenandoahGeneration;\n@@ -48,0 +49,1 @@\n+  ShenandoahGeneration* const _generation;\n@@ -53,1 +55,1 @@\n-  ShenandoahGCSession(GCCause::Cause cause);\n+  ShenandoahGCSession(GCCause::Cause cause, ShenandoahGeneration* generation);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUtils.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -81,0 +82,18 @@\n+  if (_incr_region_ages) {\n+    \/\/ TODO: Do we even care about this?  Do we want to parallelize it?\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    ShenandoahMarkingContext* ctx = heap->complete_marking_context();\n+\n+    for (size_t i = 0; i < heap->num_regions(); i++) {\n+      ShenandoahHeapRegion *r = heap->get_region(i);\n+      if (r->is_active() && r->is_young()) {\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        HeapWord* top = r->top();\n+        if (top > tams) {\n+          r->reset_age();\n+        } else if (heap->is_aging_cycle()){\n+          r->increment_age();\n+        }\n+      }\n+    }\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVMOperations.cpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -64,1 +64,1 @@\n-  VM_ShenandoahInitMark(ShenandoahConcurrentGC* gc) :\n+  explicit VM_ShenandoahInitMark(ShenandoahConcurrentGC* gc) :\n@@ -76,1 +76,1 @@\n-  VM_ShenandoahFinalMarkStartEvac(ShenandoahConcurrentGC* gc) :\n+  explicit VM_ShenandoahFinalMarkStartEvac(ShenandoahConcurrentGC* gc) :\n@@ -135,0 +135,1 @@\n+  bool _incr_region_ages;\n@@ -136,1 +137,1 @@\n-  VM_ShenandoahFinalRoots(ShenandoahConcurrentGC* gc) :\n+  VM_ShenandoahFinalRoots(ShenandoahConcurrentGC* gc, bool incr_region_ages) :\n@@ -138,1 +139,1 @@\n-    _gc(gc) {};\n+    _gc(gc), _incr_region_ages(incr_region_ages) {};\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVMOperations.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -36,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -71,0 +74,1 @@\n+  ShenandoahGeneration* _generation;\n@@ -82,1 +86,2 @@\n-    _loc(nullptr) {\n+    _loc(nullptr),\n+    _generation(nullptr) {\n@@ -87,0 +92,5 @@\n+\n+    if (_heap->mode()->is_generational()) {\n+      _generation = _heap->active_generation();\n+      assert(_generation != nullptr, \"Expected active generation in this mode\");\n+    }\n@@ -110,1 +120,1 @@\n-      if (_map->par_mark(obj)) {\n+      if ( in_generation(obj) && _map->par_mark(obj)) {\n@@ -117,0 +127,9 @@\n+  bool in_generation(oop obj) {\n+    if (_generation == nullptr) {\n+      return true;\n+    }\n+\n+    ShenandoahHeapRegion* region = _heap->heap_region_containing(obj);\n+    return _generation->contains(region);\n+  }\n+\n@@ -127,1 +146,1 @@\n-    ShenandoahHeapRegion *obj_reg = _heap->heap_region_containing(obj);\n+    ShenandoahHeapRegion* obj_reg = _heap->heap_region_containing(obj);\n@@ -138,1 +157,1 @@\n-      HeapWord *obj_addr = cast_from_oop<HeapWord*>(obj);\n+      HeapWord* obj_addr = cast_from_oop<HeapWord*>(obj);\n@@ -167,1 +186,2 @@\n-          check(ShenandoahAsserts::_safe_oop, obj, obj_reg->has_live(),\n+          check(ShenandoahAsserts::_safe_oop, obj, obj_reg->has_live() ||\n+                (obj_reg->is_old() && ShenandoahHeap::heap()->is_gc_generation_young()),\n@@ -202,1 +222,1 @@\n-      HeapWord *fwd_addr = cast_from_oop<HeapWord *>(fwd);\n+      HeapWord* fwd_addr = cast_from_oop<HeapWord* >(fwd);\n@@ -216,1 +236,8 @@\n-\n+    \/\/ We allow for marked or old here for two reasons:\n+    \/\/  1. If this is a young collect, old objects wouldn't be marked. We've\n+    \/\/     recently change the verifier traversal to only follow young objects\n+    \/\/     during a young collect so this _shouldn't_ be necessary.\n+    \/\/  2. At present, we do not clear dead objects from the remembered set.\n+    \/\/     Everything in the remembered set is old (ipso facto), so allowing for\n+    \/\/     'marked_or_old' covers the case of stale objects in rset.\n+    \/\/ TODO: Just use 'is_marked' here.\n@@ -222,1 +249,1 @@\n-        check(ShenandoahAsserts::_safe_all, obj, _heap->marking_context()->is_marked(obj),\n+        check(ShenandoahAsserts::_safe_all, obj, _heap->marking_context()->is_marked_or_old(obj),\n@@ -226,1 +253,1 @@\n-        check(ShenandoahAsserts::_safe_all, obj, _heap->complete_marking_context()->is_marked(obj),\n+        check(ShenandoahAsserts::_safe_all, obj, _heap->complete_marking_context()->is_marked_or_old(obj),\n@@ -230,1 +257,1 @@\n-        check(ShenandoahAsserts::_safe_all, obj, _heap->complete_marking_context()->is_marked(obj),\n+        check(ShenandoahAsserts::_safe_all, obj, _heap->complete_marking_context()->is_marked_or_old(obj),\n@@ -316,0 +343,2 @@\n+\/\/ This closure computes the amounts of used, committed, and garbage memory and the number of regions contained within\n+\/\/ a subset (e.g. the young generation or old generation) of the total heap.\n@@ -318,1 +347,1 @@\n-  size_t _used, _committed, _garbage;\n+  size_t _used, _committed, _garbage, _regions;\n@@ -320,1 +349,1 @@\n-  ShenandoahCalculateRegionStatsClosure() : _used(0), _committed(0), _garbage(0) {};\n+  ShenandoahCalculateRegionStatsClosure() : _used(0), _committed(0), _garbage(0), _regions(0) {};\n@@ -324,0 +353,2 @@\n+    log_debug(gc)(\"ShenandoahCalculateRegionStatsClosure added \" SIZE_FORMAT \" for %s Region \" SIZE_FORMAT \", yielding: \" SIZE_FORMAT,\n+                  r->used(), r->is_humongous()? \"humongous\": \"regular\", r->index(), _used);\n@@ -326,0 +357,1 @@\n+    _regions++;\n@@ -331,0 +363,54 @@\n+  size_t regions() { return _regions; }\n+\n+  \/\/ span is the total memory affiliated with these stats (some of which is in use and other is available)\n+  size_t span() { return _regions * ShenandoahHeapRegion::region_size_bytes(); }\n+};\n+\n+class ShenandoahGenerationStatsClosure : public ShenandoahHeapRegionClosure {\n+ public:\n+  ShenandoahCalculateRegionStatsClosure old;\n+  ShenandoahCalculateRegionStatsClosure young;\n+  ShenandoahCalculateRegionStatsClosure global;\n+\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n+    switch (r->affiliation()) {\n+      default:\n+        ShouldNotReachHere();\n+        return;\n+      case FREE: return;\n+      case YOUNG_GENERATION:\n+        young.heap_region_do(r);\n+        break;\n+      case OLD_GENERATION:\n+        old.heap_region_do(r);\n+        break;\n+    }\n+    global.heap_region_do(r);\n+  }\n+\n+  static void log_usage(ShenandoahGeneration* generation, ShenandoahCalculateRegionStatsClosure& stats) {\n+    log_debug(gc)(\"Safepoint verification: %s verified usage: \" SIZE_FORMAT \"%s, recorded usage: \" SIZE_FORMAT \"%s\",\n+                  generation->name(),\n+                  byte_size_in_proper_unit(generation->used()), proper_unit_for_byte_size(generation->used()),\n+                  byte_size_in_proper_unit(stats.used()), proper_unit_for_byte_size(stats.used()));\n+  }\n+\n+  static void validate_usage(const char* label, ShenandoahGeneration* generation, ShenandoahCalculateRegionStatsClosure& stats) {\n+    size_t generation_used = generation->used();\n+    guarantee(stats.used() == generation_used,\n+              \"%s: generation (%s) used size must be consistent: generation-used = \" SIZE_FORMAT \"%s, regions-used = \" SIZE_FORMAT \"%s\",\n+              label, generation->name(),\n+              byte_size_in_proper_unit(generation_used), proper_unit_for_byte_size(generation_used),\n+              byte_size_in_proper_unit(stats.used()), proper_unit_for_byte_size(stats.used()));\n+\n+    guarantee(stats.regions() == generation->used_regions(),\n+              \"%s: generation (%s) used regions (\" SIZE_FORMAT \") must equal regions that are in use (\" SIZE_FORMAT \")\",\n+              label, generation->name(), generation->used_regions(), stats.regions());\n+\n+    size_t capacity = generation->adjusted_capacity();\n+    guarantee(stats.span() <= capacity,\n+              \"%s: generation (%s) size spanned by regions (\" SIZE_FORMAT \") must not exceed current capacity (\" SIZE_FORMAT \"%s)\",\n+              label, generation->name(), stats.regions(),\n+              byte_size_in_proper_unit(capacity), proper_unit_for_byte_size(capacity));\n+\n+  }\n@@ -414,2 +500,5 @@\n-    verify(r, r->get_shared_allocs() + r->get_tlab_allocs() + r->get_gclab_allocs() == r->used(),\n-           \"Accurate accounting: shared + TLAB + GCLAB = used\");\n+    verify(r, r->get_plab_allocs() <= r->capacity(),\n+           \"PLAB alloc count should not be larger than capacity\");\n+\n+    verify(r, r->get_shared_allocs() + r->get_tlab_allocs() + r->get_gclab_allocs() + r->get_plab_allocs() == r->used(),\n+           \"Accurate accounting: shared + TLAB + GCLAB + PLAB = used\");\n@@ -492,1 +581,1 @@\n-  ShenandoahHeap *_heap;\n+  ShenandoahHeap* _heap;\n@@ -497,0 +586,1 @@\n+  ShenandoahGeneration* _generation;\n@@ -510,1 +600,7 @@\n-          _processed(0) {};\n+          _processed(0),\n+          _generation(nullptr) {\n+    if (_heap->mode()->is_generational()) {\n+      _generation = _heap->active_generation();\n+      assert(_generation != nullptr, \"Expected active generation in this mode.\");\n+    }\n+  };\n@@ -526,0 +622,4 @@\n+        if (!in_generation(r)) {\n+          continue;\n+        }\n+\n@@ -537,1 +637,5 @@\n-  virtual void work_humongous(ShenandoahHeapRegion *r, ShenandoahVerifierStack& stack, ShenandoahVerifyOopClosure& cl) {\n+  bool in_generation(ShenandoahHeapRegion* r) {\n+    return _generation == nullptr || _generation->contains(r);\n+  }\n+\n+  virtual void work_humongous(ShenandoahHeapRegion* r, ShenandoahVerifierStack& stack, ShenandoahVerifyOopClosure& cl) {\n@@ -546,1 +650,1 @@\n-  virtual void work_regular(ShenandoahHeapRegion *r, ShenandoahVerifierStack &stack, ShenandoahVerifyOopClosure &cl) {\n+  virtual void work_regular(ShenandoahHeapRegion* r, ShenandoahVerifierStack &stack, ShenandoahVerifyOopClosure &cl) {\n@@ -579,1 +683,1 @@\n-  void verify_and_follow(HeapWord *addr, ShenandoahVerifierStack &stack, ShenandoahVerifyOopClosure &cl, size_t *processed) {\n+  void verify_and_follow(HeapWord* addr, ShenandoahVerifierStack &stack, ShenandoahVerifyOopClosure &cl, size_t* processed) {\n@@ -609,1 +713,1 @@\n-    if (actual != _expected) {\n+    if (actual != _expected && !(actual & ShenandoahHeap::OLD_MARKING)) {\n@@ -615,1 +719,2 @@\n-void ShenandoahVerifier::verify_at_safepoint(const char *label,\n+void ShenandoahVerifier::verify_at_safepoint(const char* label,\n+                                             VerifyRememberedSet remembered,\n@@ -648,0 +753,4 @@\n+      case _verify_gcstate_updating:\n+        enabled = true;\n+        expected = ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::UPDATEREFS;\n+        break;\n@@ -667,1 +776,2 @@\n-      if (actual != expected) {\n+      \/\/ Old generation marking is allowed in all states.\n+      if (actual != expected && !(actual & ShenandoahHeap::OLD_MARKING)) {\n@@ -700,0 +810,45 @@\n+  log_debug(gc)(\"Safepoint verification finished heap usage verification\");\n+\n+  ShenandoahGeneration* generation;\n+  if (_heap->mode()->is_generational()) {\n+    generation = _heap->active_generation();\n+    guarantee(generation != nullptr, \"Need to know which generation to verify.\");\n+  } else {\n+    generation = nullptr;\n+  }\n+\n+  if (generation != nullptr) {\n+    ShenandoahHeapLocker lock(_heap->lock());\n+\n+    if (remembered == _verify_remembered_for_marking) {\n+      log_debug(gc)(\"Safepoint verification of remembered set at mark\");\n+    } else if (remembered == _verify_remembered_for_updating_references) {\n+      log_debug(gc)(\"Safepoint verification of remembered set at update ref\");\n+    } else if (remembered == _verify_remembered_after_full_gc) {\n+      log_debug(gc)(\"Safepoint verification of remembered set after full gc\");\n+    }\n+\n+    if (remembered == _verify_remembered_for_marking) {\n+      _heap->verify_rem_set_at_mark();\n+    } else if (remembered == _verify_remembered_for_updating_references) {\n+      _heap->verify_rem_set_at_update_ref();\n+    } else if (remembered == _verify_remembered_after_full_gc) {\n+      _heap->verify_rem_set_after_full_gc();\n+    }\n+\n+    ShenandoahGenerationStatsClosure cl;\n+    _heap->heap_region_iterate(&cl);\n+\n+    if (LogTarget(Debug, gc)::is_enabled()) {\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->old_generation(), cl.old);\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->young_generation(), cl.young);\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->global_generation(), cl.global);\n+    }\n+\n+    ShenandoahGenerationStatsClosure::validate_usage(label, _heap->old_generation(), cl.old);\n+    ShenandoahGenerationStatsClosure::validate_usage(label, _heap->young_generation(), cl.young);\n+    ShenandoahGenerationStatsClosure::validate_usage(label, _heap->global_generation(), cl.global);\n+  }\n+\n+  log_debug(gc)(\"Safepoint verification finished remembered set verification\");\n+\n@@ -703,1 +858,5 @@\n-    _heap->heap_region_iterate(&cl);\n+    if (generation != nullptr) {\n+      generation->heap_region_iterate(&cl);\n+    } else {\n+      _heap->heap_region_iterate(&cl);\n+    }\n@@ -706,0 +865,2 @@\n+  log_debug(gc)(\"Safepoint verification finished heap region closure verification\");\n+\n@@ -730,0 +891,2 @@\n+  log_debug(gc)(\"Safepoint verification finished getting initial reachable set\");\n+\n@@ -747,0 +910,2 @@\n+  log_debug(gc)(\"Safepoint verification finished walking marked objects\");\n+\n@@ -753,0 +918,3 @@\n+      if (generation != nullptr && !generation->contains(r)) {\n+        continue;\n+      }\n@@ -776,0 +944,3 @@\n+  log_debug(gc)(\"Safepoint verification finished accumulation of liveness data\");\n+\n+\n@@ -785,0 +956,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -797,0 +969,1 @@\n+          _verify_remembered_for_marking,  \/\/ verify read-only remembered set from bottom() to top()\n@@ -809,0 +982,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -821,0 +995,1 @@\n+          _verify_remembered_disable,                \/\/ do not verify remembered set\n@@ -833,0 +1008,1 @@\n+          _verify_remembered_disable, \/\/ do not verify remembered set\n@@ -845,0 +1021,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -857,6 +1034,7 @@\n-          _verify_forwarded_allow,     \/\/ forwarded references allowed\n-          _verify_marked_complete,     \/\/ bitmaps might be stale, but alloc-after-mark should be well\n-          _verify_cset_forwarded,      \/\/ all cset refs are fully forwarded\n-          _verify_liveness_disable,    \/\/ no reliable liveness data anymore\n-          _verify_regions_notrash,     \/\/ trash regions have been recycled already\n-          _verify_gcstate_forwarded    \/\/ evacuation should have produced some forwarded objects\n+          _verify_remembered_for_updating_references,  \/\/ verify read-write remembered set\n+          _verify_forwarded_allow,                     \/\/ forwarded references allowed\n+          _verify_marked_complete,                     \/\/ bitmaps might be stale, but alloc-after-mark should be well\n+          _verify_cset_forwarded,                      \/\/ all cset refs are fully forwarded\n+          _verify_liveness_disable,                    \/\/ no reliable liveness data anymore\n+          _verify_regions_notrash,                     \/\/ trash regions have been recycled already\n+          _verify_gcstate_updating                     \/\/ evacuation should have produced some forwarded objects\n@@ -869,0 +1047,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -881,0 +1060,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -893,0 +1073,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -902,0 +1083,13 @@\n+void ShenandoahVerifier::verify_after_generational_fullgc() {\n+  verify_at_safepoint(\n+          \"After Full Generational GC\",\n+          _verify_remembered_after_full_gc,  \/\/ verify read-write remembered set\n+          _verify_forwarded_none,      \/\/ all objects are non-forwarded\n+          _verify_marked_complete,     \/\/ all objects are marked in complete bitmap\n+          _verify_cset_none,           \/\/ no cset references\n+          _verify_liveness_disable,    \/\/ no reliable liveness data anymore\n+          _verify_regions_notrash_nocset, \/\/ no trash, no cset\n+          _verify_gcstate_stable       \/\/ full gc cleaned up everything\n+  );\n+}\n+\n@@ -905,0 +1099,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -914,1 +1109,1 @@\n-class ShenandoahVerifyNoForwared : public OopClosure {\n+class ShenandoahVerifyNoForwared : public BasicOopIterateClosure {\n@@ -934,1 +1129,1 @@\n-class ShenandoahVerifyInToSpaceClosure : public OopClosure {\n+class ShenandoahVerifyInToSpaceClosure : public BasicOopIterateClosure {\n@@ -943,1 +1138,1 @@\n-      if (!heap->marking_context()->is_marked(obj)) {\n+      if (!heap->marking_context()->is_marked_or_old(obj)) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":227,"deletions":32,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -60,0 +60,18 @@\n+  typedef enum {\n+    \/\/ Disable remembered set verification.\n+    _verify_remembered_disable,\n+\n+    \/\/ Assure old objects are registered and remembered set cards within the read-only remembered set are dirty\n+    \/\/ for every interesting pointer within each OLD ShenandoahHeapRegion between bottom() and top().  This is\n+    \/\/ appropriate at the init_mark safepoint since all TLABS are retired before we reach this code.\n+    _verify_remembered_for_marking,\n+\n+    \/\/ Assure old objects are registered and remembered set cards within the read-write remembered set are dirty\n+    \/\/ for every interesting pointer within each OLD ShenandoahHeapRegion between bottom() and top().\n+    _verify_remembered_for_updating_references,\n+\n+    \/\/ Assure old objects are registered and remembered set cards within the read-write remembered set are dirty\n+    \/\/ for every interesting pointer within each OLD ShenandoahHeapRegion between bottom() and top().\n+    _verify_remembered_after_full_gc\n+  } VerifyRememberedSet;\n+\n@@ -139,1 +157,4 @@\n-    _verify_gcstate_evacuation\n+    _verify_gcstate_evacuation,\n+\n+    \/\/ Evacuation is done, objects are forwarded, updating is in progress\n+    _verify_gcstate_updating\n@@ -163,1 +184,2 @@\n-  void verify_at_safepoint(const char *label,\n+  void verify_at_safepoint(const char* label,\n+                           VerifyRememberedSet remembered,\n@@ -184,0 +206,1 @@\n+  void verify_after_generational_fullgc();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.hpp","additions":25,"deletions":2,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+uint ShenandoahWorkerPolicy::_prev_conc_rs_scanning = 0;\n@@ -64,0 +65,9 @@\n+uint ShenandoahWorkerPolicy::calc_workers_for_rs_scanning() {\n+  uint active_workers = (_prev_conc_rs_scanning == 0) ? ConcGCThreads : _prev_conc_rs_scanning;\n+  _prev_conc_rs_scanning =\n+    WorkerPolicy::calc_active_conc_workers(ConcGCThreads,\n+                                           active_workers,\n+                                           Threads::number_of_non_daemon_threads());\n+  return _prev_conc_rs_scanning;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahWorkerPolicy.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+  static uint _prev_conc_rs_scanning;\n@@ -51,0 +52,3 @@\n+  \/\/ Calculate the number of workers for remembered set scanning\n+  static uint calc_workers_for_rs_scanning();\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahWorkerPolicy.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,101 @@\n+\/*\n+ * Copyright (c) 2020, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+#include \"gc\/shenandoah\/shenandoahVerifier.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+\n+ShenandoahYoungGeneration::ShenandoahYoungGeneration(uint max_queues, size_t max_capacity, size_t soft_max_capacity) :\n+  ShenandoahGeneration(YOUNG, max_queues, max_capacity, soft_max_capacity),\n+  _old_gen_task_queues(nullptr) {\n+}\n+\n+const char* ShenandoahYoungGeneration::name() const {\n+  return \"YOUNG\";\n+}\n+\n+void ShenandoahYoungGeneration::set_concurrent_mark_in_progress(bool in_progress) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  heap->set_concurrent_young_mark_in_progress(in_progress);\n+  if (is_bootstrap_cycle() && in_progress && !heap->is_prepare_for_old_mark_in_progress()) {\n+    \/\/ This is not a bug. When the bootstrapping marking phase is complete,\n+    \/\/ the old generation marking is still in progress, unless it's not.\n+    \/\/ In the case that old-gen preparation for mixed evacuation has been\n+    \/\/ preempted, we do not want to set concurrent old mark to be in progress.\n+    heap->set_concurrent_old_mark_in_progress(in_progress);\n+  }\n+}\n+\n+bool ShenandoahYoungGeneration::contains(ShenandoahHeapRegion* region) const {\n+  \/\/ TODO: why not test for equals YOUNG_GENERATION?  As written, returns true for regions that are FREE\n+  return region->affiliation() != OLD_GENERATION;\n+}\n+\n+void ShenandoahYoungGeneration::parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl) {\n+  \/\/ Just iterate over the young generation here.\n+  ShenandoahGenerationRegionClosure<YOUNG> young_regions(cl);\n+  ShenandoahHeap::heap()->parallel_heap_region_iterate(&young_regions);\n+}\n+\n+void ShenandoahYoungGeneration::heap_region_iterate(ShenandoahHeapRegionClosure* cl) {\n+  ShenandoahGenerationRegionClosure<YOUNG> young_regions(cl);\n+  ShenandoahHeap::heap()->heap_region_iterate(&young_regions);\n+}\n+\n+bool ShenandoahYoungGeneration::is_concurrent_mark_in_progress() {\n+  return ShenandoahHeap::heap()->is_concurrent_young_mark_in_progress();\n+}\n+\n+void ShenandoahYoungGeneration::reserve_task_queues(uint workers) {\n+  ShenandoahGeneration::reserve_task_queues(workers);\n+  if (is_bootstrap_cycle()) {\n+    _old_gen_task_queues->reserve(workers);\n+  }\n+}\n+\n+bool ShenandoahYoungGeneration::contains(oop obj) const {\n+  return ShenandoahHeap::heap()->is_in_young(obj);\n+}\n+\n+ShenandoahHeuristics* ShenandoahYoungGeneration::initialize_heuristics(ShenandoahMode* gc_mode) {\n+  _heuristics = gc_mode->initialize_heuristics(this);\n+  _heuristics->set_guaranteed_gc_interval(ShenandoahGuaranteedYoungGCInterval);\n+  confirm_heuristics_mode();\n+  return _heuristics;\n+}\n+\n+void ShenandoahYoungGeneration::add_collection_time(double time_seconds) {\n+  if (is_bootstrap_cycle()) {\n+    \/\/ This is a bootstrap cycle, so attribute time to old gc\n+    ShenandoahHeap::heap()->old_generation()->add_collection_time(time_seconds);\n+  } else {\n+    ShenandoahGeneration::add_collection_time(time_seconds);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahYoungGeneration.cpp","additions":101,"deletions":0,"binary":false,"changes":101,"status":"added"},{"patch":"@@ -0,0 +1,72 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com, Inc. or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHYOUNGGENERATION_HPP\n+#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHYOUNGGENERATION_HPP\n+\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+\n+class ShenandoahYoungGeneration : public ShenandoahGeneration {\n+private:\n+  ShenandoahObjToScanQueueSet* _old_gen_task_queues;\n+\n+public:\n+  ShenandoahYoungGeneration(uint max_queues, size_t max_capacity, size_t max_soft_capacity);\n+\n+  const char* name() const override;\n+\n+  void set_concurrent_mark_in_progress(bool in_progress) override;\n+  void parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl) override;\n+\n+  void heap_region_iterate(ShenandoahHeapRegionClosure* cl) override;\n+\n+  bool contains(ShenandoahHeapRegion* region) const override;\n+  bool contains(oop obj) const override;\n+\n+  void set_old_gen_task_queues(ShenandoahObjToScanQueueSet* old_gen_queues) {\n+    _old_gen_task_queues = old_gen_queues;\n+  }\n+\n+  ShenandoahObjToScanQueueSet* old_gen_task_queues() const override {\n+    return _old_gen_task_queues;\n+  }\n+\n+  \/\/ Returns true if the young generation is configured to enqueue old\n+  \/\/ oops for the old generation mark queues.\n+  bool is_bootstrap_cycle() {\n+    return _old_gen_task_queues != nullptr;\n+  }\n+\n+  void reserve_task_queues(uint workers) override;\n+\n+  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahMode* gc_mode) override;\n+\n+  virtual void add_collection_time(double time_seconds) override;\n+\n+ protected:\n+  bool is_concurrent_mark_in_progress() override;\n+\n+};\n+\n+#endif \/\/ SHARE_VM_GC_SHENANDOAH_SHENANDOAHYOUNGGENERATION_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahYoungGeneration.hpp","additions":72,"deletions":0,"binary":false,"changes":72,"status":"added"},{"patch":"@@ -65,1 +65,2 @@\n-          \" passive - stop the world GC only (either degenerated or full)\") \\\n+          \" passive - stop the world GC only (either degenerated or full);\" \\\n+          \" generational - generational concurrent GC\")                     \\\n@@ -79,0 +80,6 @@\n+  product(ccstr, ShenandoahOldGCHeuristics, \"adaptive\",                     \\\n+          \"Similar to ShenandoahGCHeuristics, but applied to the old \"      \\\n+          \"generation. This configuration is only used to trigger old \"     \\\n+          \"collections and does not change how regions are selected \"       \\\n+          \"for collection.\")                                                \\\n+                                                                            \\\n@@ -92,0 +99,14 @@\n+  product(uintx, ShenandoahOldGarbageThreshold, 10, EXPERIMENTAL,           \\\n+          \"How much garbage an old region has to contain before it would \"  \\\n+          \"be taken for collection.\")                                       \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahIgnoreGarbageThreshold, 5, EXPERIMENTAL,         \\\n+          \"When less than this amount of garbage (as a percentage of \"      \\\n+          \"region size) exists within a region, the region will not be \"    \\\n+          \"added to the collection set, even when the heuristic has \"       \\\n+          \"chosen to aggressively add regions with less than \"              \\\n+          \"ShenandoahGarbageThreshold amount of garbage into the \"          \\\n+          \"collection set.\")                                                \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n@@ -93,4 +114,6 @@\n-          \"How much heap should be free before some heuristics trigger the \"\\\n-          \"initial (learning) cycles. Affects cycle frequency on startup \"  \\\n-          \"and after drastic state changes, e.g. after degenerated\/full \"   \\\n-          \"GC cycles. In percents of (soft) max heap size.\")                \\\n+          \"When less than this amount of memory is free within the\"         \\\n+          \"heap or generation, trigger a learning cycle if we are \"         \\\n+          \"in learning mode.  Learning mode happens during initialization \" \\\n+          \"and following a drastic state change, such as following a \"      \\\n+          \"degenerated or Full GC cycle.  In percents of soft max \"         \\\n+          \"heap size.\")                                                     \\\n@@ -100,3 +123,12 @@\n-          \"How much heap should be free before most heuristics trigger the \"\\\n-          \"collection, even without other triggers. Provides the safety \"   \\\n-          \"margin for many heuristics. In percents of (soft) max heap size.\")\\\n+          \"Percentage of free heap memory (or young generation, in \"        \\\n+          \"generational mode) below which most heuristics trigger \"         \\\n+          \"collection independent of other triggers. Provides a safety \"    \\\n+          \"margin for many heuristics. In percents of (soft) max heap \"     \\\n+          \"size.\")                                                          \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahOldMinFreeThreshold, 5, EXPERIMENTAL,            \\\n+          \"Percentage of free old generation heap memory below which most \" \\\n+          \"heuristics trigger collection independent of other triggers. \"   \\\n+          \"Provides a safety margin for many heuristics. In percents of \"   \\\n+          \"(soft) max heap size.\")                                          \\\n@@ -118,1 +150,1 @@\n-  product(uintx, ShenandoahLearningSteps, 5, EXPERIMENTAL,                  \\\n+  product(uintx, ShenandoahLearningSteps, 10, EXPERIMENTAL,                 \\\n@@ -123,1 +155,1 @@\n-  product(uintx, ShenandoahImmediateThreshold, 90, EXPERIMENTAL,            \\\n+  product(uintx, ShenandoahImmediateThreshold, 70, EXPERIMENTAL,            \\\n@@ -152,1 +184,1 @@\n-  product(double, ShenandoahAdaptiveDecayFactor, 0.5, EXPERIMENTAL,         \\\n+  product(double, ShenandoahAdaptiveDecayFactor, 0.1, EXPERIMENTAL,         \\\n@@ -158,0 +190,10 @@\n+  product(bool, ShenandoahAdaptiveIgnoreShortCycles, true, EXPERIMENTAL,    \\\n+          \"The adaptive heuristic tracks a moving average of cycle \"        \\\n+          \"times in order to start a gc before memory is exhausted. \"       \\\n+          \"In some cases, Shenandoah may skip the evacuation and update \"   \\\n+          \"reference phases, resulting in a shorter cycle. These may skew \" \\\n+          \"the average cycle time downward and may cause the heuristic \"    \\\n+          \"to wait too long to start a cycle. Disabling this will have \"    \\\n+          \"the gc run less often, which will reduce CPU utilization, but\"   \\\n+          \"increase the risk of degenerated cycles.\")                       \\\n+                                                                            \\\n@@ -165,0 +207,10 @@\n+  product(uintx, ShenandoahGuaranteedOldGCInterval, 10*60*1000,  EXPERIMENTAL,  \\\n+          \"Run a collection of the old generation at least this often. \"    \\\n+          \"Heuristics may trigger collections more frequently. Time is in \" \\\n+          \"milliseconds. Setting this to 0 disables the feature.\")          \\\n+                                                                            \\\n+  product(uintx, ShenandoahGuaranteedYoungGCInterval, 5*60*1000,  EXPERIMENTAL,  \\\n+          \"Run a collection of the young generation at least this often. \"    \\\n+          \"Heuristics may trigger collections more frequently. Time is in \" \\\n+          \"milliseconds. Setting this to 0 disables the feature.\")          \\\n+                                                                            \\\n@@ -227,4 +279,12 @@\n-          \"How much of heap to reserve for evacuations. Larger values make \"\\\n-          \"GC evacuate more live objects on every cycle, while leaving \"    \\\n-          \"less headroom for application to allocate in. In percents of \"   \\\n-          \"total heap size.\")                                               \\\n+          \"How much of (young-generation) heap to reserve for \"             \\\n+          \"(young-generation) evacuations.  Larger values allow GC to \"     \\\n+          \"evacuate more live objects on every cycle, while leaving \"       \\\n+          \"less headroom for application to allocate while GC is \"          \\\n+          \"evacuating and updating references. This parameter is \"          \\\n+          \"consulted at the of marking, before selecting the collection \"   \\\n+          \"set.  If available memory at this time is smaller than the \"     \\\n+          \"indicated reserve, the bound on collection set size is \"         \\\n+          \"adjusted downward.  The size of a generational mixed \"           \\\n+          \"evacuation collection set (comprised of both young and old \"     \\\n+          \"regions) is also bounded by this parameter.  In percents of \"    \\\n+          \"total (young-generation) heap size.\")                            \\\n@@ -237,1 +297,2 @@\n-          \"GC cycle.\")                                                      \\\n+          \"GC cycle.  Smaller values increase the risk of evacuation \"      \\\n+          \"failures, which will trigger stop-the-world Full GC passes.\")    \\\n@@ -240,0 +301,33 @@\n+  product(double, ShenandoahGenerationalEvacWaste, 2.0, EXPERIMENTAL,       \\\n+          \"For generational mode, how much waste evacuations produce \"      \\\n+          \"within the reserved space.  Larger values make evacuations \"     \\\n+          \"more resilient against evacuation conflicts, at expense of \"     \\\n+          \"evacuating less on each GC cycle.  Smaller values increase \"     \\\n+          \"the risk of evacuation failures, which will trigger \"            \\\n+          \"stop-the-world Full GC passes.  The default value for \"          \\\n+          \"generational mode is 2.0.  The reason for the higher default \"   \\\n+          \"value in generational mode is because generational mode \"        \\\n+          \"enforces the evacuation budget, triggering degenerated GC \"      \\\n+          \"which upgrades to full GC whenever the budget is exceeded.\")     \\\n+          range(1.0,100.0)                                                  \\\n+                                                                            \\\n+  product(uintx, ShenandoahMaxEvacLABRatio, 0, EXPERIMENTAL,                \\\n+          \"Potentially, each running thread maintains a PLAB for \"          \\\n+          \"evacuating objects into old-gen memory and a GCLAB for \"         \\\n+          \"evacuating objects into young-gen memory.  Each time a thread \"  \\\n+          \"exhausts its PLAB or GCLAB, a new local buffer is allocated. \"   \\\n+          \"By default, the new buffer is twice the size of the previous \"   \\\n+          \"buffer.  The sizes are reset to the minimum at the start of \"    \\\n+          \"each GC pass.  This parameter limits the growth of evacuation \"  \\\n+          \"buffer sizes to its value multiplied by the minimum buffer \"     \\\n+          \"size.  A higher value allows evacuation allocations to be more \" \\\n+          \"efficient because less synchronization is required by \"          \\\n+          \"individual threads.  However, a larger value increases the \"     \\\n+          \"likelihood of evacuation failures, leading to long \"             \\\n+          \"stop-the-world pauses.  This is because a large value \"          \\\n+          \"allows individual threads to consume large percentages of \"      \\\n+          \"the total evacuation budget without necessarily effectively \"    \\\n+          \"filling their local evacuation buffers with evacuated \"          \\\n+          \"objects.  A value of zero means no maximum size is enforced.\")   \\\n+          range(0, 1024)                                                    \\\n+                                                                            \\\n@@ -246,0 +340,45 @@\n+  product(uintx, ShenandoahOldEvacReserve, 2, EXPERIMENTAL,                 \\\n+          \"How much of old-generation heap to reserve for old-generation \"  \\\n+          \"evacuations.  Larger values allow GC to evacuate more live \"     \\\n+          \"old-generation objects on every cycle, while potentially \"       \\\n+          \"creating greater impact on the cadence at which the young- \"     \\\n+          \"generation allocation pool is replenished.  During mixed \"       \\\n+          \"evacuations, the bound on amount of old-generation heap \"        \\\n+          \"regions included in the collecdtion set is the smaller \"         \\\n+          \"of the quantities specified by this parameter and the \"          \\\n+          \"size of ShenandoahEvacReserve as adjusted by the value of \"      \\\n+          \"ShenandoahOldEvacRatioPercent.  In percents of total \"           \\\n+          \"old-generation heap size.\")                                      \\\n+          range(1,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahOldEvacRatioPercent, 12, EXPERIMENTAL,           \\\n+          \"The maximum proportion of evacuation from old-gen memory, as \"   \\\n+          \"a percent ratio.  The default value 12 denotes that no more \"    \\\n+          \"than one eighth (12%) of the collection set evacuation \"         \\\n+          \"workload may be comprised of old-gen heap regions.  A larger \"   \\\n+          \"value allows a smaller number of mixed evacuations to process \"  \\\n+          \"the entire list of old-gen collection candidates at the cost \"   \\\n+          \"of an increased disruption of the normal cadence of young-gen \"  \\\n+          \"collections.  A value of 100 allows a mixed evacuation to \"      \\\n+          \"focus entirely on old-gen memory, allowing no young-gen \"        \\\n+          \"regions to be collected, likely resulting in subsequent \"        \\\n+          \"allocation failures because the allocation pool is not \"         \\\n+          \"replenished.  A value of 0 allows a mixed evacuation to\"         \\\n+          \"focus entirely on young-gen memory, allowing no old-gen \"        \\\n+          \"regions to be collected, likely resulting in subsequent \"        \\\n+          \"promotion failures and triggering of stop-the-world full GC \"    \\\n+          \"events.\")                                                        \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahMinYoungPercentage, 20,                                \\\n+          \"The minimum percentage of the heap to use for the young \"        \\\n+          \"generation. Heuristics will not adjust the young generation \"    \\\n+          \"to be less than this.\")                                          \\\n+          range(0, 100)                                                     \\\n+                                                                            \\\n+  product(uintx, ShenandoahMaxYoungPercentage, 80,                                \\\n+          \"The maximum percentage of the heap to use for the young \"        \\\n+          \"generation. Heuristics will not adjust the young generation \"    \\\n+          \"to be more than this.\")                                          \\\n+          range(0, 100)                                                     \\\n+                                                                            \\\n@@ -369,3 +508,38 @@\n-          \"might degrade performance.\")\n-\n-\/\/ end of GC_SHENANDOAH_FLAGS\n+          \"might degrade performance.\")                                     \\\n+                                                                            \\\n+  product(uintx, ShenandoahBorrowPercent, 30, EXPERIMENTAL,                 \\\n+          \"During evacuation and reference updating in generational \"       \\\n+          \"mode, new allocations are allowed to borrow from old-gen \"       \\\n+          \"memory up to ShenandoahBorrowPercent \/ 100 amount of the \"       \\\n+          \"young-generation content of the current collection set.  \"       \\\n+          \"Any memory borrowed from old-gen during evacuation and \"         \\\n+          \"update-references phases of GC will be repaid from the \"         \\\n+          \"abundance of young-gen memory produced when the collection \"     \\\n+          \"set is recycled at the end of updating references.  The \"        \\\n+          \"default value of 30 reserves 70% of the to-be-reclaimed \"        \\\n+          \"young collection set memory to be allocated during the \"         \\\n+          \"subsequent concurrent mark phase of GC.\")                        \\\n+          range(0, 100)                                                     \\\n+                                                                            \\\n+  product(uintx, ShenandoahOldCompactionReserve, 8, EXPERIMENTAL,           \\\n+          \"During generational GC, prevent promotions from filling \"        \\\n+          \"this number of heap regions.  These regions are reserved \"       \\\n+          \"for the purpose of supporting compaction of old-gen \"            \\\n+          \"memory.  Otherwise, old-gen memory cannot be compacted.\")        \\\n+          range(0, 128)                                                     \\\n+                                                                            \\\n+  product(bool, ShenandoahAllowOldMarkingPreemption, true, DIAGNOSTIC,      \\\n+          \"Allow young generation collections to suspend concurrent\"        \\\n+          \" marking in the old generation.\")                                \\\n+                                                                            \\\n+  product(uintx, ShenandoahAgingCyclePeriod, 1, EXPERIMENTAL,               \\\n+          \"With generational mode, increment the age of objects and\"        \\\n+          \"regions each time this many young-gen GC cycles are completed.\") \\\n+                                                                            \\\n+  notproduct(bool, ShenandoahEnableCardStats, trueInDebug,                  \\\n+          \"Enable statistics collection related to clean & dirty cards\")    \\\n+                                                                            \\\n+  notproduct(int, ShenandoahCardStatsLogInterval, 50,                       \\\n+          \"Log cumulative card stats every so many remembered set or \"      \\\n+          \"update refs scans\")                                              \\\n+  \/\/ end of GC_SHENANDOAH_FLAGS\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":193,"deletions":19,"binary":false,"changes":212,"status":"modified"},{"patch":"@@ -113,0 +113,26 @@\n+void AbsSeq::merge(AbsSeq& abs2, bool clear_this) {\n+\n+  if (num() == 0) return;  \/\/ nothing to do\n+\n+  abs2._num += _num;\n+  abs2._sum += _sum;\n+  abs2._sum_of_squares += _sum_of_squares;\n+\n+  \/\/ Decaying stats need a bit more thought\n+  assert(abs2._alpha == _alpha, \"Caution: merge incompatible?\");\n+  \/\/ Until JDK-8298902 is fixed, we taint the decaying statistics\n+  if (abs2._davg != NAN) {\n+    abs2._davg = NAN;\n+    abs2._dvariance = NAN;\n+  }\n+\n+  if (clear_this) {\n+    _num = 0;\n+    _sum = 0;\n+    _sum_of_squares = 0;\n+    _davg = 0;\n+    _dvariance = 0;\n+  }\n+}\n+\n+\n@@ -140,0 +166,16 @@\n+void NumberSeq::merge(NumberSeq& nseq2, bool clear_this) {\n+\n+  if (num() == 0) return;  \/\/ nothing to do\n+\n+  nseq2._last = _last;   \/\/ this is newer than that\n+  nseq2._maximum = MAX2(_maximum, nseq2._maximum);\n+\n+  AbsSeq::merge(nseq2, clear_this);\n+\n+  if (clear_this) {\n+    _last = 0;\n+    _maximum = 0;\n+    assert(num() == 0, \"Not cleared\");\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/utilities\/numberSeq.cpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -86,0 +86,3 @@\n+\n+  \/\/ Merge this AbsSeq into seq2, optionally clearing this AbsSeq\n+  void merge(AbsSeq& seq2, bool clear_this = true);\n@@ -105,0 +108,3 @@\n+\n+  \/\/ Merge this NumberSeq into seq2, optionally clearing this NumberSeq\n+  void merge(NumberSeq& seq2, bool clear_this = true);\n@@ -132,0 +138,3 @@\n+\n+  \/\/ Merge this AbsSeq into seq2, optionally clearing this AbsSeq\n+  void merge(AbsSeq& seq2, bool clear_this = true);\n","filename":"src\/hotspot\/share\/utilities\/numberSeq.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,2 +2,1 @@\n- * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -37,1 +36,25 @@\n-  HdrSeq seq;\n+  const double err = 0.5;\n+\n+  HdrSeq seq1;\n+  HdrSeq seq2;\n+  HdrSeq seq3;\n+\n+  void print() {\n+    if (seq1.num() > 0) {\n+      print(seq1, \"seq1\");\n+    }\n+    if (seq2.num() > 0) {\n+      print(seq2, \"seq2\");\n+    }\n+    if (seq3.num() > 0) {\n+      print(seq3, \"seq3\");\n+    }\n+  }\n+\n+  void print(HdrSeq& seq, const char* msg) {\n+    std::cout << \"[\";\n+    for (int i = 0; i <= 100; i += 10) {\n+      std::cout << \"\\t\" << seq.percentile(i);\n+    }\n+    std::cout << \" ] : \" << msg << \"\\n\";\n+  }\n@@ -41,2 +64,1 @@\n- protected:\n-  const double err = 0.5;\n+ public:\n@@ -44,3 +66,3 @@\n-    seq.add(0);\n-    seq.add(1);\n-    seq.add(10);\n+    seq1.add(0);\n+    seq1.add(1);\n+    seq1.add(10);\n@@ -48,1 +70,1 @@\n-      seq.add(100);\n+      seq1.add(100);\n@@ -50,8 +72,17 @@\n-    std::cout << \" p0 = \" << seq.percentile(0);\n-    std::cout << \" p10 = \" << seq.percentile(10);\n-    std::cout << \" p20 = \" << seq.percentile(20);\n-    std::cout << \" p30 = \" << seq.percentile(30);\n-    std::cout << \" p50 = \" << seq.percentile(50);\n-    std::cout << \" p80 = \" << seq.percentile(80);\n-    std::cout << \" p90 = \" << seq.percentile(90);\n-    std::cout << \" p100 = \" << seq.percentile(100);\n+    ShenandoahNumberSeqTest::print();\n+  }\n+};\n+\n+class ShenandoahNumberSeqMergeTest: public ShenandoahNumberSeqTest {\n+ public:\n+  ShenandoahNumberSeqMergeTest() {\n+    for (int i = 0; i < 80; i++) {\n+      seq1.add(1);\n+      seq3.add(1);\n+    }\n+\n+    for (int i = 0; i < 20; i++) {\n+      seq2.add(100);\n+      seq3.add(100);\n+    }\n+    ShenandoahNumberSeqTest::print();\n@@ -62,1 +93,1 @@\n-  EXPECT_EQ(seq.maximum(), 100);\n+  EXPECT_EQ(seq1.maximum(), 100);\n@@ -66,1 +97,1 @@\n-  EXPECT_EQ(0, seq.percentile(0));\n+  EXPECT_EQ(0, seq1.percentile(0));\n@@ -70,8 +101,36 @@\n-  EXPECT_NEAR(0, seq.percentile(10), err);\n-  EXPECT_NEAR(1, seq.percentile(20), err);\n-  EXPECT_NEAR(10, seq.percentile(30), err);\n-  EXPECT_NEAR(100, seq.percentile(40), err);\n-  EXPECT_NEAR(100, seq.percentile(50), err);\n-  EXPECT_NEAR(100, seq.percentile(75), err);\n-  EXPECT_NEAR(100, seq.percentile(90), err);\n-  EXPECT_NEAR(100, seq.percentile(100), err);\n+  EXPECT_NEAR(0, seq1.percentile(10), err);\n+  EXPECT_NEAR(1, seq1.percentile(20), err);\n+  EXPECT_NEAR(10, seq1.percentile(30), err);\n+  EXPECT_NEAR(100, seq1.percentile(40), err);\n+  EXPECT_NEAR(100, seq1.percentile(50), err);\n+  EXPECT_NEAR(100, seq1.percentile(75), err);\n+  EXPECT_NEAR(100, seq1.percentile(90), err);\n+  EXPECT_NEAR(100, seq1.percentile(100), err);\n+}\n+\n+TEST_VM_F(ShenandoahNumberSeqMergeTest, merge_test) {\n+  EXPECT_EQ(seq1.num(), 80);\n+  EXPECT_EQ(seq2.num(), 20);\n+  EXPECT_FALSE(isnan(seq2.davg()));  \/\/ Exercise the path; not a nan\n+  EXPECT_FALSE(isnan(seq2.dsd()));\n+  EXPECT_FALSE(isnan(seq2.dvariance()));\n+\n+  std::cout << \"Pre-merge: \\n\";\n+  print();\n+  seq1.merge(seq2);    \/\/ clears seq1, after merging into seq2\n+  std::cout << \"Post-merge: \\n\";\n+  print();\n+\n+  EXPECT_EQ(seq1.num(), 0);\n+  EXPECT_EQ(seq2.num(), 100);\n+  EXPECT_EQ(seq2.num(), seq3.num());\n+  EXPECT_TRUE(isnan(seq2.davg()));  \/\/ until we fix decayed stats\n+  EXPECT_TRUE(isnan(seq2.dvariance()));\n+\n+  EXPECT_EQ(seq2.maximum(), seq3.maximum());\n+  EXPECT_EQ(seq2.percentile(0), seq3.percentile(0));\n+  for (int i = 0; i <= 100; i += 10) {\n+    EXPECT_NEAR(seq2.percentile(i), seq3.percentile(i), err);\n+  }\n+  EXPECT_NEAR(seq2.avg(), seq3.avg(), err);\n+  EXPECT_NEAR(seq2.sd(),  seq3.sd(),  err);\n","filename":"test\/hotspot\/gtest\/gc\/shenandoah\/test_shenandoahNumberSeq.cpp","additions":86,"deletions":27,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -0,0 +1,285 @@\n+#include \"precompiled.hpp\"\n+#include \"unittest.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+\n+\/\/ These tests will all be skipped (unless Shenandoah becomes the default\n+\/\/ collector). To execute these tests, you must enable Shenandoah, which\n+\/\/ is done with:\n+\/\/\n+\/\/ % _JAVA_OPTIONS=\"-XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\" make exploded-test TEST=\"gtest:Shenandoah*\"\n+\/\/\n+\/\/ Please note that these 'unit' tests are really integration tests and rely\n+\/\/ on the JVM being initialized. These tests manipulate the state of the\n+\/\/ collector in ways that are not compatible with a normal collection run.\n+\/\/ If these tests take longer than the minimum time between gc intervals -\n+\/\/ or, more likely, if you have them paused in a debugger longer than this\n+\/\/ interval - you can expect trouble. These tests will also not run in a build\n+\/\/ with asserts enabled because they use APIs that expect to run on a safepoint.\n+#ifdef ASSERT\n+#define SKIP_IF_NOT_SHENANDOAH()           \\\n+  tty->print_cr(\"skipped (debug build)\" ); \\\n+  return;\n+#else\n+#define SKIP_IF_NOT_SHENANDOAH() \\\n+    if (!UseShenandoahGC) {      \\\n+      tty->print_cr(\"skipped\");  \\\n+      return;                    \\\n+    }\n+#endif\n+\n+class ShenandoahResetRegions : public ShenandoahHeapRegionClosure {\n+ public:\n+  virtual void heap_region_do(ShenandoahHeapRegion* region) override {\n+    if (!region->is_empty()) {\n+      region->make_trash();\n+      region->make_empty();\n+    }\n+    region->set_affiliation(FREE);\n+    region->clear_live_data();\n+    region->set_top(region->bottom());\n+  }\n+};\n+\n+class ShenandoahOldHeuristicTest : public ::testing::Test {\n+ protected:\n+  ShenandoahHeap* _heap;\n+  ShenandoahOldHeuristics* _heuristics;\n+  ShenandoahCollectionSet* _collection_set;\n+\n+  ShenandoahOldHeuristicTest()\n+    : _heap(nullptr),\n+      _heuristics(nullptr),\n+      _collection_set(nullptr) {\n+    SKIP_IF_NOT_SHENANDOAH();\n+    _heap = ShenandoahHeap::heap();\n+    _heuristics = _heap->old_heuristics();\n+    _collection_set = _heap->collection_set();\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    ShenandoahResetRegions reset;\n+    _heap->heap_region_iterate(&reset);\n+    _heap->set_old_evac_reserve(_heap->old_generation()->soft_max_capacity() \/ 4);\n+    _heuristics->abandon_collection_candidates();\n+    _collection_set->clear();\n+  }\n+\n+  size_t make_garbage(size_t region_idx, size_t garbage_bytes) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    ShenandoahHeapRegion* region = _heap->get_region(region_idx);\n+    region->make_regular_allocation(OLD_GENERATION);\n+    region->increase_live_data_alloc_words(1);\n+    region->set_top(region->bottom() + garbage_bytes \/ HeapWordSize);\n+    return region->garbage();\n+  }\n+\n+  size_t create_too_much_garbage_for_one_mixed_evacuation() {\n+    size_t garbage_target = _heap->old_generation()->soft_max_capacity() \/ 2;\n+    size_t garbage_total = 0;\n+    size_t region_idx = 0;\n+    while (garbage_total < garbage_target && region_idx < _heap->num_regions()) {\n+      garbage_total += make_garbage_above_threshold(region_idx++);\n+    }\n+    return garbage_total;\n+  }\n+\n+  void make_pinned(size_t region_idx) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    ShenandoahHeapRegion* region = _heap->get_region(region_idx);\n+    region->record_pin();\n+    region->make_pinned();\n+  }\n+\n+  void make_unpinned(size_t region_idx) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    ShenandoahHeapRegion* region = _heap->get_region(region_idx);\n+    region->record_unpin();\n+    region->make_unpinned();\n+  }\n+\n+  size_t make_garbage_below_threshold(size_t region_idx) {\n+    return make_garbage(region_idx, collection_threshold() - 100);\n+  }\n+\n+  size_t make_garbage_above_threshold(size_t region_idx) {\n+    return make_garbage(region_idx, collection_threshold() + 100);\n+  }\n+\n+  size_t collection_threshold() const {\n+    return ShenandoahHeapRegion::region_size_bytes() * ShenandoahOldGarbageThreshold \/ 100;\n+  }\n+};\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, select_no_old_regions) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  _heuristics->prepare_for_old_collections();\n+  EXPECT_EQ(0U, _heuristics->last_old_region_index());\n+  EXPECT_EQ(0U, _heuristics->last_old_collection_candidate_index());\n+  EXPECT_EQ(0U, _heuristics->unprocessed_old_collection_candidates());\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, select_no_old_region_above_threshold) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  \/\/ In this case, we have zero regions to add to the collection set,\n+  \/\/ but we will have one region that must still be made parseable.\n+  make_garbage_below_threshold(10);\n+  _heuristics->prepare_for_old_collections();\n+  EXPECT_EQ(1U, _heuristics->last_old_region_index());\n+  EXPECT_EQ(0U, _heuristics->last_old_collection_candidate_index());\n+  EXPECT_EQ(0U, _heuristics->unprocessed_old_collection_candidates());\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, select_one_old_region_above_threshold) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  make_garbage_above_threshold(10);\n+  _heuristics->prepare_for_old_collections();\n+  EXPECT_EQ(1U, _heuristics->last_old_region_index());\n+  EXPECT_EQ(1U, _heuristics->last_old_collection_candidate_index());\n+  EXPECT_EQ(1U, _heuristics->unprocessed_old_collection_candidates());\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, prime_one_old_region) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  size_t garbage = make_garbage_above_threshold(10);\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(garbage, _collection_set->get_old_garbage());\n+  EXPECT_EQ(0U, _heuristics->unprocessed_old_collection_candidates());\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, prime_many_old_regions) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  size_t g1 = make_garbage_above_threshold(100);\n+  size_t g2 = make_garbage_above_threshold(101);\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(g1 + g2, _collection_set->get_old_garbage());\n+  EXPECT_EQ(0U, _heuristics->unprocessed_old_collection_candidates());\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, require_multiple_mixed_evacuations) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  size_t garbage = create_too_much_garbage_for_one_mixed_evacuation();\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_LT(_collection_set->get_old_garbage(), garbage);\n+  EXPECT_GT(_heuristics->unprocessed_old_collection_candidates(), 0UL);\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, skip_pinned_regions) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  \/\/ Create three old regions with enough garbage to be collected.\n+  size_t g1 = make_garbage_above_threshold(1);\n+  size_t g2 = make_garbage_above_threshold(2);\n+  size_t g3 = make_garbage_above_threshold(3);\n+\n+  \/\/ A region can be pinned when we chose collection set candidates.\n+  make_pinned(2);\n+  _heuristics->prepare_for_old_collections();\n+\n+  \/\/ We only excluded pinned regions when we actually add regions to the collection set.\n+  ASSERT_EQ(3UL, _heuristics->unprocessed_old_collection_candidates());\n+\n+  \/\/ Here the region is still pinned, so it cannot be added to the collection set.\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  \/\/ The two unpinned regions should be added to the collection set and the pinned\n+  \/\/ region should be retained at the front of the list of candidates as it would be\n+  \/\/ likely to become unpinned by the next mixed collection cycle.\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g1 + g3);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 1UL);\n+\n+  \/\/ Simulate another mixed collection after making region 2 unpinned. This time,\n+  \/\/ the now unpinned region should be added to the collection set.\n+  make_unpinned(2);\n+  _collection_set->clear();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g2);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 0UL);\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, pinned_region_is_first) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  \/\/ Create three old regions with enough garbage to be collected.\n+  size_t g1 = make_garbage_above_threshold(1);\n+  size_t g2 = make_garbage_above_threshold(2);\n+  size_t g3 = make_garbage_above_threshold(3);\n+\n+  make_pinned(1);\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g2 + g3);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 1UL);\n+\n+  make_unpinned(1);\n+  _collection_set->clear();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g1);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 0UL);\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, pinned_region_is_last) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  \/\/ Create three old regions with enough garbage to be collected.\n+  size_t g1 = make_garbage_above_threshold(1);\n+  size_t g2 = make_garbage_above_threshold(2);\n+  size_t g3 = make_garbage_above_threshold(3);\n+\n+  make_pinned(3);\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g1 + g2);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 1UL);\n+\n+  make_unpinned(3);\n+  _collection_set->clear();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g3);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 0UL);\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, unpinned_region_is_middle) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  \/\/ Create three old regions with enough garbage to be collected.\n+  size_t g1 = make_garbage_above_threshold(1);\n+  size_t g2 = make_garbage_above_threshold(2);\n+  size_t g3 = make_garbage_above_threshold(3);\n+\n+  make_pinned(1);\n+  make_pinned(3);\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g2);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 2UL);\n+\n+  make_unpinned(1);\n+  make_unpinned(3);\n+  _collection_set->clear();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g1 + g3);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 0UL);\n+}\n+\n+#undef SKIP_IF_NOT_SHENANDOAH\n\\ No newline at end of file\n","filename":"test\/hotspot\/gtest\/gc\/shenandoah\/test_shenandoahOldHeuristic.cpp","additions":285,"deletions":0,"binary":false,"changes":285,"status":"added"},{"patch":"@@ -320,0 +320,9 @@\n+tier1_gc_shenandoah_generational = \\\n+  gc\/shenandoah\/generational\/\n+\n+# No tier 2 tests for shenandoah_generational at this time\n+tier2_gc_shenandoah_generational =\n+\n+# No tier 3 tests for shenandoah_generational at this time\n+tier3_gc_shenandoah_generational =\n+\n@@ -351,0 +360,1 @@\n+# include shenandoah generational tests in tier3 shenandoah\n@@ -357,0 +367,1 @@\n+  :hotspot_gc_shenandoah_generational \\\n@@ -364,0 +375,5 @@\n+hotspot_gc_shenandoah_generational = \\\n+  :tier1_gc_shenandoah_generational \\\n+  :tier2_gc_shenandoah_generational \\\n+  :tier3_gc_shenandoah_generational\n+\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-        if (periodic && !output.getOutput().contains(\"Trigger: Time since last GC\")) {\n+        if (periodic && !output.getOutput().contains(\"Trigger (GLOBAL): Time since last GC\")) {\n@@ -52,1 +52,1 @@\n-        if (!periodic && output.getOutput().contains(\"Trigger: Time since last GC\")) {\n+        if (!periodic && output.getOutput().contains(\"Trigger (GLOBAL): Time since last GC\")) {\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestPeriodicGC.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,58 @@\n+\/*\n+ * Copyright (c) 2022 Amazon.com, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/*\n+ * @test id=rotation\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+ShenandoahRegionSampling -XX:+ShenandoahRegionSampling\n+ *      -Xlog:gc+region=debug:region-snapshots-%p.log::filesize=100,filecount=3\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive\n+ *      TestRegionSamplingLogging\n+ *\/\n+\n+import java.io.File;\n+import java.util.Arrays;\n+\n+public class TestRegionSamplingLogging {\n+\n+    static final long TARGET_MB = Long.getLong(\"target\", 2_000); \/\/ 2 Gb allocation\n+\n+    static volatile Object sink;\n+\n+    public static void main(String[] args) throws Exception {\n+        long count = TARGET_MB * 1024 * 1024 \/ 16;\n+        for (long c = 0; c < count; c++) {\n+            sink = new Object();\n+        }\n+\n+        File directory = new File(\".\");\n+        File[] files = directory.listFiles((dir, name) -> name.startsWith(\"region-snapshots\") && name.endsWith(\".log\"));\n+        System.out.println(Arrays.toString(files));\n+        if (files == null || files.length == 0) {\n+            throw new IllegalStateException(\"Did not find expected snapshot log file.\");\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestRegionSamplingLogging.java","additions":58,"deletions":0,"binary":false,"changes":58,"status":"added"},{"patch":"@@ -0,0 +1,74 @@\n+   \/*\n+    * Copyright (c) 2022 Amazon.com, Inc. All rights reserved.\n+    * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+    *\n+    * This code is free software; you can redistribute it and\/or modify it\n+    * under the terms of the GNU General Public License version 2 only, as\n+    * published by the Free Software Foundation.\n+    *\n+    * This code is distributed in the hope that it will be useful, but WITHOUT\n+    * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+    * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+    * version 2 for more details (a copy is included in the LICENSE file that\n+    * accompanied this code).\n+    *\n+    * You should have received a copy of the GNU General Public License version\n+    * 2 along with this work; if not, write to the Free Software Foundation,\n+    * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+    *\n+    * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+    * or visit www.oracle.com if you need additional information or have any\n+    * questions.\n+    *\n+    *\/\n+\n+   \/*\n+    * @test id=rotation\n+    * @requires vm.gc.Shenandoah\n+    *\n+    * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+    *      -XX:+ShenandoahRegionSampling -XX:+ShenandoahRegionSampling\n+    *      -Xlog:gc+region=debug:region-snapshots-%p.log::filesize=100,filecount=3\n+    *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive\n+    *      TestShenandoahLogRotation\n+    *\/\n+\n+   import java.io.File;\n+   import java.util.Arrays;\n+   import java.nio.file.Files;\n+\n+\n+\n+   public class TestShenandoahLogRotation {\n+\n+       static final long TARGET_MB = Long.getLong(\"target\", 1);\n+\n+       static volatile Object sink;\n+\n+       public static void main(String[] args) throws Exception {\n+           long count = TARGET_MB * 1024 * 1024 \/ 16;\n+           for (long c = 0; c < count; c++) {\n+               sink = new Object();\n+               Thread.sleep(1);\n+           }\n+\n+           File directory = new File(\".\");\n+           File[] files = directory.listFiles((dir, name) -> name.startsWith(\"region-snapshots\"));\n+           System.out.println(Arrays.toString(files));\n+           int smallFilesNumber = 0;\n+           for (File file : files) {\n+               if (file.length() < 100) {\n+                   smallFilesNumber++;\n+               }\n+           }\n+           \/\/ Expect one more log file since the ShenandoahLogFileCount doesn't include the active log file\n+           int expectedNumberOfFiles = 4;\n+           if (files.length != expectedNumberOfFiles) {\n+               throw new Error(\"There are \" + files.length + \" logs instead of the expected \" + expectedNumberOfFiles + \" \" + files[0].getAbsolutePath());\n+           }\n+           if (smallFilesNumber > 1) {\n+               throw new Error(\"There should maximum one log with size < \" + 100 + \"B\");\n+           }\n+       }\n+\n+   }\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestShenandoahLogRotation.java","additions":74,"deletions":0,"binary":false,"changes":74,"status":"added"},{"patch":"@@ -0,0 +1,53 @@\n+\/*\n+ * Copyright (c) 2021, Amazon, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package gc.shenandoah.generational;\n+\n+import sun.hotspot.WhiteBox;\n+\n+\/*\n+ * @test TestCLIModeGenerational\n+ * @requires vm.gc.Shenandoah\n+ * @summary Test argument processing for -XX:+ShenandoahGCMode=generational.\n+ * @library \/testlibrary \/test\/lib \/\n+ * @build sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:.\n+ *      -XX:+IgnoreUnrecognizedVMOptions\n+ *      -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      gc.shenandoah.generational.TestCLIModeGenerational\n+ *\/\n+\n+public class TestCLIModeGenerational {\n+\n+  private static WhiteBox wb = WhiteBox.getWhiteBox();\n+\n+  public static void main(String args[]) throws Exception {\n+    Boolean using_shenandoah = wb.getBooleanVMFlag(\"UseShenandoahGC\");\n+    String gc_mode = wb.getStringVMFlag(\"ShenandoahGCMode\");\n+    if (!using_shenandoah || !gc_mode.equals(\"generational\"))\n+      throw new IllegalStateException(\"Command-line options not honored!\");\n+  }\n+}\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/generational\/TestCLIModeGenerational.java","additions":53,"deletions":0,"binary":false,"changes":53,"status":"added"},{"patch":"@@ -0,0 +1,201 @@\n+\/*\n+ * Copyright (c) 2021, Amazon, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package gc.shenandoah.generational;\n+\n+import sun.hotspot.WhiteBox;\n+import java.util.Random;\n+import java.util.HashMap;\n+\n+\/*\n+ *  To avoid the risk of false regressions identified by this test, the heap\n+ *  size is set artificially high.  Though this test is known to run reliably\n+ *  in 66 MB heap, the heap size for this test run is currently set to 256 MB.\n+ *\/\n+\n+\/*\n+ * @test TestConcurrentEvac\n+ * @requires vm.gc.Shenandoah\n+ * @summary Confirm that card marking and remembered set scanning do not crash.\n+ * @library \/testlibrary \/test\/lib \/\n+ * @build sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:.\n+ *      -Xms256m -Xmx256m\n+ *      -XX:+IgnoreUnrecognizedVMOptions\n+ *      -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      -XX:NewRatio=1 -XX:+UnlockExperimentalVMOptions\n+ *      -XX:ShenandoahGuaranteedGCInterval=3000\n+ *      -XX:-UseDynamicNumberOfGCThreads -XX:-ShenandoahPacing\n+ *      gc.shenandoah.generational.TestConcurrentEvac\n+ *\/\n+\n+public class TestConcurrentEvac {\n+  private static WhiteBox wb = WhiteBox.getWhiteBox();\n+\n+  static private final int SeedForRandom = 46;\n+  \/\/ Sequence of random numbers should end with same value\n+\n+  \/\/ Smaller table will cause creation of more old-gen garbage\n+  \/\/ as previous entries in table are overwritten with new values.\n+  static private final int TableSize = 53;\n+  static private final int MaxStringLength = 47;\n+  static private final int SentenceLength = 5;\n+\n+  static private Random random = new Random(SeedForRandom);\n+\n+  public static class Node {\n+    static private final int NeighborCount = 48;\n+    static private final int ChildOverwriteCount = 32;\n+    static private final int IntArraySize = 128;\n+\n+    private String name;\n+\n+    \/\/ Each Node instance holds an array containing all substrings of\n+    \/\/ its name\n+\n+    \/\/ This array has entries from 0 .. (name.length() - 1).\n+    \/\/ num_substrings[i] represents the number of substrings that\n+    \/\/ correspond to a name of length i+1.\n+    private static int [] num_substrings;\n+\n+    static {\n+      \/\/ Initialize num_substrings.\n+      \/\/ For a name of length N, there are\n+      \/\/  N substrings of length 1\n+      \/\/  N-1 substrings of length 2\n+      \/\/  N-2 substrings of length 3\n+      \/\/  ...\n+      \/\/  1 substring of length N\n+      \/\/ Note that:\n+      \/\/   num_substrings[0] = 1\n+      \/\/   num_substrings[1] = 3\n+      \/\/   num_substrings[i] = (i+1)+num_substrings[i-1]\n+\n+      num_substrings = new int[MaxStringLength];\n+      num_substrings[0] = 1;\n+      for (int i = 1; i < MaxStringLength; i++)\n+        num_substrings[i] = (i+1)+num_substrings[i-1];\n+    }\n+\n+    private String [] substrings;\n+    private Node [] neighbors;\n+\n+    public Node(String name) {\n+      this.name = name;\n+      this.substrings = new String[num_substrings[name.length() - 1]];\n+\n+      int index = 0;\n+      for (int substring_length = 1;\n+           substring_length <= name.length(); substring_length++) {\n+        for (int offset = 0;\n+             offset + substring_length <= name.length(); offset++) {\n+          this.substrings[index++] = name.substring(offset,\n+                                                    offset + substring_length);\n+        }\n+      }\n+    }\n+\n+    public String value() {\n+      return name;\n+    }\n+\n+    public String arbitrary_substring() {\n+      int index = TestConcurrentEvac.randomUnsignedInt() % substrings.length;\n+      return substrings[index];\n+    }\n+  }\n+\n+\n+  \/\/ Return random int between 1 and MaxStringLength inclusive\n+  static int randomStringLength() {\n+    int length = randomUnsignedInt();\n+    length %= (MaxStringLength - 1);\n+    length += 1;\n+    return length;\n+  }\n+\n+  static String randomCharacter() {\n+    int index = randomUnsignedInt() % 52;\n+    return (\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\".\n+            substring(index, index+1));\n+  }\n+\n+  static String randomString() {\n+    int length = randomStringLength();\n+    String result = new String(); \/\/ make the compiler work for this garbage...\n+    for (int i = 0; i < length; i++)\n+      result += randomCharacter();\n+    return result;\n+  }\n+\n+  static int randomUnsignedInt() {\n+    int result = random.nextInt();\n+    if (result < 0) result = -result;\n+    if (result < 0) result = 0;\n+    return result;\n+  }\n+\n+  static int randomIndex() {\n+    int index = randomUnsignedInt();\n+    index %= TableSize;\n+    return index;\n+  }\n+\n+  public static void main(String args[]) throws Exception {\n+    HashMap<Integer, Node> table = new HashMap<Integer, Node>(TableSize);\n+\n+    if (!wb.getBooleanVMFlag(\"UseShenandoahGC\") ||\n+        !wb.getStringVMFlag(\"ShenandoahGCMode\").equals(\"generational\"))\n+      throw new IllegalStateException(\"Command-line options not honored!\");\n+\n+    for (int count = java.lang.Integer.MAX_VALUE\/1024; count >= 0; count--) {\n+      int index = randomIndex();\n+      String name = randomString();\n+      table.put(index, new Node(name));\n+    }\n+\n+    String conclusion = \"\";\n+\n+    for (int i = 0; i < SentenceLength; i++) {\n+      Node a_node = table.get(randomIndex());\n+      if (a_node == null)\n+        i--;\n+      else {\n+        String a_string = a_node.arbitrary_substring();\n+        conclusion += a_string;\n+        conclusion += \" \";\n+      }\n+    }\n+    conclusion = conclusion.substring(0, conclusion.length() - 1);\n+\n+    System.out.println(\"Conclusion is [\" + conclusion + \"]\");\n+\n+    if (!conclusion.equals(\"cTy cTykJ kAkKAOWYEHbxFCmRIlyk xjYMdNmtAQXNGdIc sqHKsWnJIP\"))\n+      throw new IllegalStateException(\"Random sequence of words did not end well!\");\n+\n+  }\n+}\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/generational\/TestConcurrentEvac.java","additions":201,"deletions":0,"binary":false,"changes":201,"status":"added"},{"patch":"@@ -0,0 +1,124 @@\n+\/*\n+ * Copyright (c) 2021, Amazon, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package gc.shenandoah.generational;\n+\n+import sun.hotspot.WhiteBox;\n+import java.util.Random;\n+\n+\/*\n+ * @test TestSimpleGenerational\n+ * @requires vm.gc.Shenandoah\n+ * @summary Confirm that card marking and remembered set scanning do not crash.\n+ * @library \/testlibrary \/test\/lib \/\n+ * @build sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:.\n+ *      -XX:+IgnoreUnrecognizedVMOptions\n+ *      -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      gc.shenandoah.generational.TestSimpleGenerational\n+ *\/\n+public class TestSimpleGenerational {\n+  private static WhiteBox wb = WhiteBox.getWhiteBox();\n+  static private final int SeedForRandom = 46;\n+  \/\/ Sequence of random numbers should end with same value\n+  private static int ExpectedLastRandom = 272454100;\n+\n+\n+  public static class Node {\n+    static private final int NeighborCount = 5;\n+    static private final int IntArraySize = 8;\n+    static private Random random = new Random(SeedForRandom);\n+\n+    private int val;\n+    private Object field_o;\n+\n+    \/\/ Each Node instance holds references to two \"private\" arrays.\n+    \/\/ One array holds raw seething bits (primitive integers) and the\n+    \/\/ holds references.\n+\n+    private int[] field_ints;\n+    private Node [] neighbors;\n+\n+    public Node(int val) {\n+      this.val = val;\n+      this.field_o = new Object();\n+      this.field_ints = new int[IntArraySize];\n+      this.field_ints[0] = 0xca;\n+      this.field_ints[1] = 0xfe;\n+      this.field_ints[2] = 0xba;\n+      this.field_ints[3] = 0xbe;\n+      this.field_ints[4] = 0xba;\n+      this.field_ints[5] = 0xad;\n+      this.field_ints[6] = 0xba;\n+      this.field_ints[7] = 0xbe;\n+\n+      this.neighbors = new Node[NeighborCount];\n+    }\n+\n+    public int value() {\n+      return val;\n+    }\n+\n+    \/\/ Copy each neighbor of n into a new node's neighbor array.\n+    \/\/ Then overwrite arbitrarily selected neighbor with newly allocated\n+    \/\/ leaf node.\n+    public static Node upheaval(Node n) {\n+      int first_val = random.nextInt();\n+      if (first_val < 0) first_val = -first_val;\n+      if (first_val < 0) first_val = 0;\n+      Node result = new Node(first_val);\n+      if (n != null) {\n+        for (int i = 0; i < NeighborCount; i++)\n+          result.neighbors[i] = n.neighbors[i];\n+      }\n+      int second_val = random.nextInt();\n+      if (second_val < 0) second_val = -second_val;\n+      if (second_val < 0) second_val = 0;\n+\n+      int overwrite_index = first_val % NeighborCount;\n+      result.neighbors[overwrite_index] = new Node(second_val);\n+      return result;\n+    }\n+  }\n+\n+  public static void main(String args[]) throws Exception {\n+    Node n = null;\n+\n+    if (!wb.getBooleanVMFlag(\"UseShenandoahGC\") ||\n+        !wb.getStringVMFlag(\"ShenandoahGCMode\").equals(\"generational\"))\n+      throw new IllegalStateException(\"Command-line options not honored!\");\n+\n+    for (int count = 10000; count > 0; count--) {\n+      n = Node.upheaval(n);\n+    }\n+\n+    if (n.value() != ExpectedLastRandom)\n+      throw new IllegalStateException(\"Random number sequence ended badly!\");\n+\n+  }\n+\n+}\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/generational\/TestSimpleGenerational.java","additions":124,"deletions":0,"binary":false,"changes":124,"status":"added"}]}