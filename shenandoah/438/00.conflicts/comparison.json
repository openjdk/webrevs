{"files":[{"patch":"@@ -28,0 +28,2 @@\n+#include \"gc\/shenandoah\/shenandoahAffiliation.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n@@ -32,0 +34,1 @@\n+<<<<<<< HEAD\n@@ -34,0 +37,5 @@\n+=======\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -38,0 +46,1 @@\n+<<<<<<< HEAD\n@@ -136,0 +145,167 @@\n+=======\n+ShenandoahSetsOfFree::ShenandoahSetsOfFree(size_t max_regions, ShenandoahFreeSet* free_set) :\n+    _max(max_regions),\n+    _free_set(free_set),\n+    _region_size_bytes(ShenandoahHeapRegion::region_size_bytes())\n+{\n+  _membership = NEW_C_HEAP_ARRAY(ShenandoahFreeMemoryType, max_regions, mtGC);\n+  clear_internal();\n+}\n+\n+ShenandoahSetsOfFree::~ShenandoahSetsOfFree() {\n+  FREE_C_HEAP_ARRAY(ShenandoahFreeMemoryType, _membership);\n+}\n+\n+\n+void ShenandoahSetsOfFree::clear_internal() {\n+  for (size_t idx = 0; idx < _max; idx++) {\n+    _membership[idx] = NotFree;\n+  }\n+\n+  for (size_t idx = 0; idx < NumFreeSets; idx++) {\n+    _leftmosts[idx] = _max;\n+    _rightmosts[idx] = 0;\n+    _leftmosts_empty[idx] = _max;\n+    _rightmosts_empty[idx] = 0;\n+    _capacity_of[idx] = 0;\n+    _used_by[idx] = 0;\n+  }\n+\n+  _left_to_right_bias[Mutator] = true;\n+  _left_to_right_bias[Collector] = false;\n+  _left_to_right_bias[OldCollector] = false;\n+\n+  _region_counts[Mutator] = 0;\n+  _region_counts[Collector] = 0;\n+  _region_counts[OldCollector] = 0;\n+  _region_counts[NotFree] = _max;\n+}\n+\n+void ShenandoahSetsOfFree::clear_all() {\n+  clear_internal();\n+}\n+\n+void ShenandoahSetsOfFree::increase_used(ShenandoahFreeMemoryType which_set, size_t bytes) {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"Set must correspond to a valid freeset\");\n+  _used_by[which_set] += bytes;\n+  assert (_used_by[which_set] <= _capacity_of[which_set],\n+          \"Must not use (\" SIZE_FORMAT \") more than capacity (\" SIZE_FORMAT \") after increase by \" SIZE_FORMAT,\n+          _used_by[which_set], _capacity_of[which_set], bytes);\n+}\n+\n+inline void ShenandoahSetsOfFree::shrink_bounds_if_touched(ShenandoahFreeMemoryType set, size_t idx) {\n+  if (idx == _leftmosts[set]) {\n+    while ((_leftmosts[set] < _max) && !in_free_set(_leftmosts[set], set)) {\n+      _leftmosts[set]++;\n+    }\n+    if (_leftmosts_empty[set] < _leftmosts[set]) {\n+      \/\/ This gets us closer to where we need to be; we'll scan further when leftmosts_empty is requested.\n+      _leftmosts_empty[set] = _leftmosts[set];\n+    }\n+  }\n+  if (idx == _rightmosts[set]) {\n+    while (_rightmosts[set] > 0 && !in_free_set(_rightmosts[set], set)) {\n+      _rightmosts[set]--;\n+    }\n+    if (_rightmosts_empty[set] > _rightmosts[set]) {\n+      \/\/ This gets us closer to where we need to be; we'll scan further when rightmosts_empty is requested.\n+      _rightmosts_empty[set] = _rightmosts[set];\n+    }\n+  }\n+}\n+\n+inline void ShenandoahSetsOfFree::expand_bounds_maybe(ShenandoahFreeMemoryType set, size_t idx, size_t region_capacity) {\n+  if (region_capacity == _region_size_bytes) {\n+    if (_leftmosts_empty[set] > idx) {\n+      _leftmosts_empty[set] = idx;\n+    }\n+    if (_rightmosts_empty[set] < idx) {\n+      _rightmosts_empty[set] = idx;\n+    }\n+  }\n+  if (_leftmosts[set] > idx) {\n+    _leftmosts[set] = idx;\n+  }\n+  if (_rightmosts[set] < idx) {\n+    _rightmosts[set] = idx;\n+  }\n+}\n+\n+void ShenandoahSetsOfFree::remove_from_free_sets(size_t idx) {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  ShenandoahFreeMemoryType orig_set = membership(idx);\n+  assert (orig_set > NotFree && orig_set < NumFreeSets, \"Cannot remove from free sets if not already free\");\n+  _membership[idx] = NotFree;\n+  shrink_bounds_if_touched(orig_set, idx);\n+\n+  _region_counts[orig_set]--;\n+  _region_counts[NotFree]++;\n+}\n+\n+\n+void ShenandoahSetsOfFree::make_free(size_t idx, ShenandoahFreeMemoryType which_set, size_t region_capacity) {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (_membership[idx] == NotFree, \"Cannot make free if already free\");\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  _membership[idx] = which_set;\n+  _capacity_of[which_set] += region_capacity;\n+  expand_bounds_maybe(which_set, idx, region_capacity);\n+\n+  _region_counts[NotFree]--;\n+  _region_counts[which_set]++;\n+}\n+\n+void ShenandoahSetsOfFree::move_to_set(size_t idx, ShenandoahFreeMemoryType new_set, size_t region_capacity) {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert ((new_set > NotFree) && (new_set < NumFreeSets), \"New set must be valid\");\n+  ShenandoahFreeMemoryType orig_set = _membership[idx];\n+  assert ((orig_set > NotFree) && (orig_set < NumFreeSets), \"Cannot move free unless already free\");\n+  \/\/ Expected transitions:\n+  \/\/  During rebuild: Mutator => Collector\n+  \/\/                  Mutator empty => Collector\n+  \/\/  During flip_to_gc:\n+  \/\/                  Mutator empty => Collector\n+  \/\/                  Mutator empty => Old Collector\n+  \/\/ At start of update refs:\n+  \/\/                  Collector => Mutator\n+  \/\/                  OldCollector Empty => Mutator\n+  assert((region_capacity <= _region_size_bytes && ((orig_set == Mutator && new_set == Collector) || (orig_set == Collector && new_set == Mutator)))\n+      || (region_capacity == _region_size_bytes && ((orig_set == Mutator && new_set == Collector) || (orig_set == OldCollector && new_set == Mutator) || new_set == OldCollector)),\n+      \"Unexpected movement between sets\");\n+\n+  _membership[idx] = new_set;\n+  _capacity_of[orig_set] -= region_capacity;\n+  shrink_bounds_if_touched(orig_set, idx);\n+\n+  _capacity_of[new_set] += region_capacity;\n+  expand_bounds_maybe(new_set, idx, region_capacity);\n+\n+  _region_counts[orig_set]--;\n+  _region_counts[new_set]++;\n+}\n+\n+inline ShenandoahFreeMemoryType ShenandoahSetsOfFree::membership(size_t idx) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  return _membership[idx];\n+}\n+\n+  \/\/ Returns true iff region idx is in the test_set free_set.  Before returning true, asserts that the free\n+  \/\/ set is not empty.  Requires that test_set != NotFree or NumFreeSets.\n+inline bool ShenandoahSetsOfFree::in_free_set(size_t idx, ShenandoahFreeMemoryType test_set) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  if (_membership[idx] == test_set) {\n+    assert (test_set == NotFree || _free_set->alloc_capacity(idx) > 0, \"Free regions must have alloc capacity\");\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+inline size_t ShenandoahSetsOfFree::leftmost(ShenandoahFreeMemoryType which_set) const {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  size_t idx = _leftmosts[which_set];\n+  if (idx >= _max) {\n+    return _max;\n+  } else {\n+    assert (in_free_set(idx, which_set), \"left-most region must be free\");\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -140,0 +316,1 @@\n+<<<<<<< HEAD\n@@ -485,0 +662,101 @@\n+=======\n+inline size_t ShenandoahSetsOfFree::rightmost(ShenandoahFreeMemoryType which_set) const {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  size_t idx = _rightmosts[which_set];\n+  assert ((_leftmosts[which_set] == _max) || in_free_set(idx, which_set), \"right-most region must be free\");\n+  return idx;\n+}\n+\n+inline bool ShenandoahSetsOfFree::is_empty(ShenandoahFreeMemoryType which_set) const {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  return (leftmost(which_set) > rightmost(which_set));\n+}\n+\n+size_t ShenandoahSetsOfFree::leftmost_empty(ShenandoahFreeMemoryType which_set) {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  for (size_t idx = _leftmosts_empty[which_set]; idx < _max; idx++) {\n+    if ((membership(idx) == which_set) && (_free_set->alloc_capacity(idx) == _region_size_bytes)) {\n+      _leftmosts_empty[which_set] = idx;\n+      return idx;\n+    }\n+  }\n+  _leftmosts_empty[which_set] = _max;\n+  _rightmosts_empty[which_set] = 0;\n+  return _max;\n+}\n+\n+inline size_t ShenandoahSetsOfFree::rightmost_empty(ShenandoahFreeMemoryType which_set) {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  for (intptr_t idx = _rightmosts_empty[which_set]; idx >= 0; idx--) {\n+    if ((membership(idx) == which_set) && (_free_set->alloc_capacity(idx) == _region_size_bytes)) {\n+      _rightmosts_empty[which_set] = idx;\n+      return idx;\n+    }\n+  }\n+  _leftmosts_empty[which_set] = _max;\n+  _rightmosts_empty[which_set] = 0;\n+  return 0;\n+}\n+\n+inline bool ShenandoahSetsOfFree::alloc_from_left_bias(ShenandoahFreeMemoryType which_set) {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  return _left_to_right_bias[which_set];\n+}\n+\n+void ShenandoahSetsOfFree::establish_alloc_bias(ShenandoahFreeMemoryType which_set) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  shenandoah_assert_heaplocked();\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+\n+  size_t middle = (_leftmosts[which_set] + _rightmosts[which_set]) \/ 2;\n+  size_t available_in_first_half = 0;\n+  size_t available_in_second_half = 0;\n+\n+  for (size_t index = _leftmosts[which_set]; index < middle; index++) {\n+    if (in_free_set(index, which_set)) {\n+      ShenandoahHeapRegion* r = heap->get_region(index);\n+      available_in_first_half += r->free();\n+    }\n+  }\n+  for (size_t index = middle; index <= _rightmosts[which_set]; index++) {\n+    if (in_free_set(index, which_set)) {\n+      ShenandoahHeapRegion* r = heap->get_region(index);\n+      available_in_second_half += r->free();\n+    }\n+  }\n+\n+  \/\/ We desire to first consume the sparsely distributed regions in order that the remaining regions are densely packed.\n+  \/\/ Densely packing regions reduces the effort to search for a region that has sufficient memory to satisfy a new allocation\n+  \/\/ request.  Regions become sparsely distributed following a Full GC, which tends to slide all regions to the front of the\n+  \/\/ heap rather than allowing survivor regions to remain at the high end of the heap where we intend for them to congregate.\n+\n+  \/\/ TODO: In the future, we may modify Full GC so that it slides old objects to the end of the heap and young objects to the\n+  \/\/ front of the heap. If this is done, we can always search survivor Collector and OldCollector regions right to left.\n+  _left_to_right_bias[which_set] = (available_in_second_half > available_in_first_half);\n+}\n+\n+#ifdef ASSERT\n+void ShenandoahSetsOfFree::assert_bounds() {\n+\n+  size_t leftmosts[NumFreeSets];\n+  size_t rightmosts[NumFreeSets];\n+  size_t empty_leftmosts[NumFreeSets];\n+  size_t empty_rightmosts[NumFreeSets];\n+\n+  for (int i = 0; i < NumFreeSets; i++) {\n+    leftmosts[i] = _max;\n+    empty_leftmosts[i] = _max;\n+    rightmosts[i] = 0;\n+    empty_rightmosts[i] = 0;\n+  }\n+\n+  for (size_t i = 0; i < _max; i++) {\n+    ShenandoahFreeMemoryType set = membership(i);\n+    switch (set) {\n+      case NotFree:\n+        break;\n+\n+      case Mutator:\n+      case Collector:\n+      case OldCollector:\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -489,0 +767,1 @@\n+<<<<<<< HEAD\n@@ -500,0 +779,13 @@\n+=======\n+        if (i < leftmosts[set]) {\n+          leftmosts[set] = i;\n+        }\n+        if (is_empty && (i < empty_leftmosts[set])) {\n+          empty_leftmosts[set] = i;\n+        }\n+        if (i > rightmosts[set]) {\n+          rightmosts[set] = i;\n+        }\n+        if (is_empty && (i > empty_rightmosts[set])) {\n+          empty_rightmosts[set] = i;\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -504,0 +796,4 @@\n+<<<<<<< HEAD\n+=======\n+      case NumFreeSets:\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -509,0 +805,1 @@\n+<<<<<<< HEAD\n@@ -574,0 +871,73 @@\n+=======\n+  \/\/ Performance invariants. Failing these would not break the free set, but performance would suffer.\n+  assert (leftmost(Mutator) <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, leftmost(Mutator),  _max);\n+  assert (rightmost(Mutator) < _max, \"rightmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, rightmost(Mutator),  _max);\n+\n+  assert (leftmost(Mutator) == _max || in_free_set(leftmost(Mutator), Mutator),\n+          \"leftmost region should be free: \" SIZE_FORMAT,  leftmost(Mutator));\n+  assert (leftmost(Mutator) == _max || in_free_set(rightmost(Mutator), Mutator),\n+          \"rightmost region should be free: \" SIZE_FORMAT, rightmost(Mutator));\n+\n+  \/\/ If Mutator set is empty, leftmosts will both equal max, rightmosts will both equal zero.  Likewise for empty region sets.\n+  size_t beg_off = leftmosts[Mutator];\n+  size_t end_off = rightmosts[Mutator];\n+  assert (beg_off >= leftmost(Mutator),\n+          \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost(Mutator));\n+  assert (end_off <= rightmost(Mutator),\n+          \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost(Mutator));\n+\n+  beg_off = empty_leftmosts[Mutator];\n+  end_off = empty_rightmosts[Mutator];\n+  assert (beg_off >= leftmost_empty(Mutator),\n+          \"free empty regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost_empty(Mutator));\n+  assert (end_off <= rightmost_empty(Mutator),\n+          \"free empty regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost_empty(Mutator));\n+\n+  \/\/ Performance invariants. Failing these would not break the free set, but performance would suffer.\n+  assert (leftmost(Collector) <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, leftmost(Collector),  _max);\n+  assert (rightmost(Collector) < _max, \"rightmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, rightmost(Collector),  _max);\n+\n+  assert (leftmost(Collector) == _max || in_free_set(leftmost(Collector), Collector),\n+          \"leftmost region should be free: \" SIZE_FORMAT,  leftmost(Collector));\n+  assert (leftmost(Collector) == _max || in_free_set(rightmost(Collector), Collector),\n+          \"rightmost region should be free: \" SIZE_FORMAT, rightmost(Collector));\n+\n+  \/\/ If Collector set is empty, leftmosts will both equal max, rightmosts will both equal zero.  Likewise for empty region sets.\n+  beg_off = leftmosts[Collector];\n+  end_off = rightmosts[Collector];\n+  assert (beg_off >= leftmost(Collector),\n+          \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost(Collector));\n+  assert (end_off <= rightmost(Collector),\n+          \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost(Collector));\n+\n+  beg_off = empty_leftmosts[Collector];\n+  end_off = empty_rightmosts[Collector];\n+  assert (beg_off >= leftmost_empty(Collector),\n+          \"free empty regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost_empty(Collector));\n+  assert (end_off <= rightmost_empty(Collector),\n+          \"free empty regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost_empty(Collector));\n+\n+  \/\/ Performance invariants. Failing these would not break the free set, but performance would suffer.\n+  assert (leftmost(OldCollector) <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, leftmost(OldCollector),  _max);\n+  assert (rightmost(OldCollector) < _max, \"rightmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, rightmost(OldCollector),  _max);\n+\n+  assert (leftmost(OldCollector) == _max || in_free_set(leftmost(OldCollector), OldCollector),\n+          \"leftmost region should be free: \" SIZE_FORMAT,  leftmost(OldCollector));\n+  assert (leftmost(OldCollector) == _max || in_free_set(rightmost(OldCollector), OldCollector),\n+          \"rightmost region should be free: \" SIZE_FORMAT, rightmost(OldCollector));\n+\n+  \/\/ If OldCollector set is empty, leftmosts will both equal max, rightmosts will both equal zero.  Likewise for empty region sets.\n+  beg_off = leftmosts[OldCollector];\n+  end_off = rightmosts[OldCollector];\n+  assert (beg_off >= leftmost(OldCollector),\n+          \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost(OldCollector));\n+  assert (end_off <= rightmost(OldCollector),\n+          \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost(OldCollector));\n+\n+  beg_off = empty_leftmosts[OldCollector];\n+  end_off = empty_rightmosts[OldCollector];\n+  assert (beg_off >= leftmost_empty(OldCollector),\n+          \"free empty regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost_empty(OldCollector));\n+  assert (end_off <= rightmost_empty(OldCollector),\n+          \"free empty regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost_empty(OldCollector));\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -579,0 +949,1 @@\n+<<<<<<< HEAD\n@@ -582,0 +953,3 @@\n+=======\n+  _free_sets(max_regions, this)\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -586,0 +960,85 @@\n+<<<<<<< HEAD\n+=======\n+\/\/ This allocates from a region within the old_collector_set.  If affiliation equals OLD, the allocation must be taken\n+\/\/ from a region that is_old().  Otherwise, affiliation should be FREE, in which case this will put a previously unaffiliated\n+\/\/ region into service.\n+HeapWord* ShenandoahFreeSet::allocate_old_with_affiliation(ShenandoahAffiliation affiliation,\n+                                                           ShenandoahAllocRequest& req, bool& in_new_region) {\n+  shenandoah_assert_heaplocked();\n+\n+  size_t rightmost =\n+    (affiliation == ShenandoahAffiliation::FREE)? _free_sets.rightmost_empty(OldCollector): _free_sets.rightmost(OldCollector);\n+  size_t leftmost =\n+    (affiliation == ShenandoahAffiliation::FREE)? _free_sets.leftmost_empty(OldCollector): _free_sets.leftmost(OldCollector);\n+  if (_free_sets.alloc_from_left_bias(OldCollector)) {\n+    \/\/ This mode picks up stragglers left by a full GC\n+    for (size_t idx = leftmost; idx <= rightmost; idx++) {\n+      if (_free_sets.in_free_set(idx, OldCollector)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(idx);\n+        assert(r->is_trash() || !r->is_affiliated() || r->is_old(), \"old_collector_set region has bad affiliation\");\n+        if (r->affiliation() == affiliation) {\n+          HeapWord* result = try_allocate_in(r, req, in_new_region);\n+          if (result != nullptr) {\n+            return result;\n+          }\n+        }\n+      }\n+    }\n+  } else {\n+    \/\/ This mode picks up stragglers left by a previous concurrent GC\n+    for (size_t count = rightmost + 1; count > leftmost; count--) {\n+      \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+      size_t idx = count - 1;\n+      if (_free_sets.in_free_set(idx, OldCollector)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(idx);\n+        assert(r->is_trash() || !r->is_affiliated() || r->is_old(), \"old_collector_set region has bad affiliation\");\n+        if (r->affiliation() == affiliation) {\n+          HeapWord* result = try_allocate_in(r, req, in_new_region);\n+          if (result != nullptr) {\n+            return result;\n+          }\n+        }\n+      }\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+void ShenandoahFreeSet::add_old_collector_free_region(ShenandoahHeapRegion* region) {\n+  shenandoah_assert_heaplocked();\n+  size_t plab_min_size_in_bytes = ShenandoahGenerationalHeap::heap()->plab_min_size() * HeapWordSize;\n+  size_t idx = region->index();\n+  size_t capacity = alloc_capacity(region);\n+  assert(_free_sets.membership(idx) == NotFree, \"Regions promoted in place should not be in any free set\");\n+  if (capacity >= plab_min_size_in_bytes) {\n+    _free_sets.make_free(idx, OldCollector, capacity);\n+    _heap->old_generation()->augment_promoted_reserve(capacity);\n+  }\n+}\n+\n+HeapWord* ShenandoahFreeSet::allocate_with_affiliation(ShenandoahAffiliation affiliation,\n+                                                       ShenandoahAllocRequest& req, bool& in_new_region) {\n+  shenandoah_assert_heaplocked();\n+  size_t rightmost =\n+    (affiliation == ShenandoahAffiliation::FREE)? _free_sets.rightmost_empty(Collector): _free_sets.rightmost(Collector);\n+  size_t leftmost =\n+    (affiliation == ShenandoahAffiliation::FREE)? _free_sets.leftmost_empty(Collector): _free_sets.leftmost(Collector);\n+  for (size_t c = rightmost + 1; c > leftmost; c--) {\n+    \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+    size_t idx = c - 1;\n+    if (_free_sets.in_free_set(idx, Collector)) {\n+      ShenandoahHeapRegion* r = _heap->get_region(idx);\n+      if (r->affiliation() == affiliation) {\n+        HeapWord* result = try_allocate_in(r, req, in_new_region);\n+        if (result != nullptr) {\n+          return result;\n+        }\n+      }\n+    }\n+  }\n+  log_debug(gc, free)(\"Could not allocate collector region with affiliation: %s for request \" PTR_FORMAT,\n+                      shenandoah_affiliation_name(affiliation), p2i(&req));\n+  return nullptr;\n+}\n+\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -601,0 +1060,27 @@\n+  \/\/ Overwrite with non-zero (non-NULL) values only if necessary for allocation bookkeeping.\n+\n+  bool allow_new_region = true;\n+  if (_heap->mode()->is_generational()) {\n+    switch (req.affiliation()) {\n+      case ShenandoahAffiliation::OLD_GENERATION:\n+        \/\/ Note: unsigned result from free_unaffiliated_regions() will never be less than zero, but it may equal zero.\n+        if (_heap->old_generation()->free_unaffiliated_regions() <= 0) {\n+          allow_new_region = false;\n+        }\n+        break;\n+\n+      case ShenandoahAffiliation::YOUNG_GENERATION:\n+        \/\/ Note: unsigned result from free_unaffiliated_regions() will never be less than zero, but it may equal zero.\n+        if (_heap->young_generation()->free_unaffiliated_regions() <= 0) {\n+          allow_new_region = false;\n+        }\n+        break;\n+\n+      case ShenandoahAffiliation::FREE:\n+        fatal(\"Should request affiliation\");\n+\n+      default:\n+        ShouldNotReachHere();\n+        break;\n+    }\n+  }\n@@ -605,0 +1091,1 @@\n+<<<<<<< HEAD\n@@ -637,0 +1124,9 @@\n+=======\n+      \/\/ Allocate within mutator free from high memory to low so as to preserve low memory for humongous allocations\n+      if (!_free_sets.is_empty(Mutator)) {\n+        \/\/ Use signed idx.  Otherwise, loop will never terminate.\n+        int leftmost = (int) _free_sets.leftmost(Mutator);\n+        for (int idx = (int) _free_sets.rightmost(Mutator); idx >= leftmost; idx--) {\n+          ShenandoahHeapRegion* r = _heap->get_region(idx);\n+          if (_free_sets.in_free_set(idx, Mutator) && (allow_new_region || r->is_affiliated())) {\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -643,0 +1139,1 @@\n+<<<<<<< HEAD\n@@ -664,0 +1161,2 @@\n+=======\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -671,0 +1170,1 @@\n+<<<<<<< HEAD\n@@ -682,0 +1182,42 @@\n+=======\n+      \/\/ GCLABs are for evacuation so we must be in evacuation phase.  If this allocation is successful, increment\n+      \/\/ the relevant evac_expended rather than used value.\n+\n+    case ShenandoahAllocRequest::_alloc_plab:\n+      \/\/ PLABs always reside in old-gen and are only allocated during evacuation phase.\n+\n+    case ShenandoahAllocRequest::_alloc_shared_gc: {\n+      if (!_heap->mode()->is_generational()) {\n+        \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+        \/\/ Fast-path: try to allocate in the collector view first\n+        for (size_t c = _free_sets.rightmost(Collector) + 1; c > _free_sets.leftmost(Collector); c--) {\n+          size_t idx = c - 1;\n+          if (_free_sets.in_free_set(idx, Collector)) {\n+            HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n+            if (result != nullptr) {\n+              return result;\n+            }\n+          }\n+        }\n+      } else {\n+        \/\/ First try to fit into a region that is already in use in the same generation.\n+        HeapWord* result;\n+        if (req.is_old()) {\n+          result = allocate_old_with_affiliation(req.affiliation(), req, in_new_region);\n+        } else {\n+          result = allocate_with_affiliation(req.affiliation(), req, in_new_region);\n+        }\n+        if (result != nullptr) {\n+          return result;\n+        }\n+        if (allow_new_region) {\n+          \/\/ Then try a free region that is dedicated to GC allocations.\n+          if (req.is_old()) {\n+            result = allocate_old_with_affiliation(FREE, req, in_new_region);\n+          } else {\n+            result = allocate_with_affiliation(FREE, req, in_new_region);\n+          }\n+          if (result != nullptr) {\n+            return result;\n+          }\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -685,1 +1227,0 @@\n-\n@@ -691,0 +1232,1 @@\n+<<<<<<< HEAD\n@@ -703,0 +1245,38 @@\n+=======\n+      if (!allow_new_region && req.is_old() && (_heap->young_generation()->free_unaffiliated_regions() > 0)) {\n+        \/\/ This allows us to flip a mutator region to old_collector\n+        allow_new_region = true;\n+      }\n+\n+      \/\/ We should expand old-gen if this can prevent an old-gen evacuation failure.  We don't care so much about\n+      \/\/ promotion failures since they can be mitigated in a subsequent GC pass.  Would be nice to know if this\n+      \/\/ allocation request is for evacuation or promotion.  Individual threads limit their use of PLAB memory for\n+      \/\/ promotions, so we already have an assurance that any additional memory set aside for old-gen will be used\n+      \/\/ only for old-gen evacuations.\n+\n+      \/\/ Also TODO:\n+      \/\/ if (GC is idle (out of cycle) and mutator allocation fails and there is memory reserved in Collector\n+      \/\/ or OldCollector sets, transfer a region of memory so that we can satisfy the allocation request, and\n+      \/\/ immediately trigger the start of GC.  Is better to satisfy the allocation than to trigger out-of-cycle\n+      \/\/ allocation failure (even if this means we have a little less memory to handle evacuations during the\n+      \/\/ subsequent GC pass).\n+\n+      if (allow_new_region) {\n+        \/\/ Try to steal an empty region from the mutator view.\n+        for (size_t c = _free_sets.rightmost_empty(Mutator) + 1; c > _free_sets.leftmost_empty(Mutator); c--) {\n+          size_t idx = c - 1;\n+          if (_free_sets.in_free_set(idx, Mutator)) {\n+            ShenandoahHeapRegion* r = _heap->get_region(idx);\n+            if (can_allocate_from(r)) {\n+              if (req.is_old()) {\n+                flip_to_old_gc(r);\n+              } else {\n+                flip_to_gc(r);\n+              }\n+              HeapWord *result = try_allocate_in(r, req, in_new_region);\n+              if (result != nullptr) {\n+                log_debug(gc, free)(\"Flipped region \" SIZE_FORMAT \" to gc for request: \" PTR_FORMAT, idx, p2i(&req));\n+                return result;\n+              }\n+            }\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -708,0 +1288,1 @@\n+<<<<<<< HEAD\n@@ -710,0 +1291,5 @@\n+=======\n+      \/\/ No dice. Do not try to mix mutator and GC allocations, because\n+      \/\/ URWM moves due to GC allocations would expose unparsable mutator\n+      \/\/ allocations.\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -718,0 +1304,55 @@\n+<<<<<<< HEAD\n+=======\n+\/\/ This work method takes an argument corresponding to the number of bytes\n+\/\/ free in a region, and returns the largest amount in heapwords that can be allocated\n+\/\/ such that both of the following conditions are satisfied:\n+\/\/\n+\/\/ 1. it is a multiple of card size\n+\/\/ 2. any remaining shard may be filled with a filler object\n+\/\/\n+\/\/ The idea is that the allocation starts and ends at card boundaries. Because\n+\/\/ a region ('s end) is card-aligned, the remainder shard that must be filled is\n+\/\/ at the start of the free space.\n+\/\/\n+\/\/ This is merely a helper method to use for the purpose of such a calculation.\n+size_t get_usable_free_words(size_t free_bytes) {\n+  \/\/ e.g. card_size is 512, card_shift is 9, min_fill_size() is 8\n+  \/\/      free is 514\n+  \/\/      usable_free is 512, which is decreased to 0\n+  size_t usable_free = (free_bytes \/ CardTable::card_size()) << CardTable::card_shift();\n+  assert(usable_free <= free_bytes, \"Sanity check\");\n+  if ((free_bytes != usable_free) && (free_bytes - usable_free < ShenandoahHeap::min_fill_size() * HeapWordSize)) {\n+    \/\/ After aligning to card multiples, the remainder would be smaller than\n+    \/\/ the minimum filler object, so we'll need to take away another card's\n+    \/\/ worth to construct a filler object.\n+    if (usable_free >= CardTable::card_size()) {\n+      usable_free -= CardTable::card_size();\n+    } else {\n+      assert(usable_free == 0, \"usable_free is a multiple of card_size and card_size > min_fill_size\");\n+    }\n+  }\n+\n+  return usable_free \/ HeapWordSize;\n+}\n+\n+\/\/ Given a size argument, which is a multiple of card size, a request struct\n+\/\/ for a PLAB, and an old region, return a pointer to the allocated space for\n+\/\/ a PLAB which is card-aligned and where any remaining shard in the region\n+\/\/ has been suitably filled by a filler object.\n+\/\/ It is assumed (and assertion-checked) that such an allocation is always possible.\n+HeapWord* ShenandoahFreeSet::allocate_aligned_plab(size_t size, ShenandoahAllocRequest& req, ShenandoahHeapRegion* r) {\n+  assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+  assert(r->is_old(), \"All PLABs reside in old-gen\");\n+  assert(!req.is_mutator_alloc(), \"PLABs should not be allocated by mutators.\");\n+  assert(is_aligned(size, CardTable::card_size_in_words()), \"Align by design\");\n+\n+  HeapWord* result = r->allocate_aligned(size, req, CardTable::card_size());\n+  assert(result != nullptr, \"Allocation cannot fail\");\n+  assert(r->top() <= r->end(), \"Allocation cannot span end of region\");\n+  assert(req.actual_size() == size, \"Should not have needed to adjust size for PLAB.\");\n+  assert(is_aligned(result, CardTable::card_size_in_words()), \"Align by design\");\n+\n+  return result;\n+}\n+\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -726,0 +1367,1 @@\n+<<<<<<< HEAD\n@@ -764,0 +1406,33 @@\n+=======\n+  if (!r->is_affiliated()) {\n+    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+    r->set_affiliation(req.affiliation());\n+    if (r->is_old()) {\n+      \/\/ Any OLD region allocated during concurrent coalesce-and-fill does not need to be coalesced and filled because\n+      \/\/ all objects allocated within this region are above TAMS (and thus are implicitly marked).  In case this is an\n+      \/\/ OLD region and concurrent preparation for mixed evacuations visits this region before the start of the next\n+      \/\/ old-gen concurrent mark (i.e. this region is allocated following the start of old-gen concurrent mark but before\n+      \/\/ concurrent preparations for mixed evacuations are completed), we mark this region as not requiring any\n+      \/\/ coalesce-and-fill processing.\n+      r->end_preemptible_coalesce_and_fill();\n+      _heap->clear_cards_for(r);\n+      _heap->old_generation()->increment_affiliated_region_count();\n+    } else {\n+      _heap->young_generation()->increment_affiliated_region_count();\n+    }\n+\n+    assert(ctx->top_at_mark_start(r) == r->bottom(), \"Newly established allocation region starts with TAMS equal to bottom\");\n+    assert(ctx->is_bitmap_clear_range(ctx->top_bitmap(r), r->end()), \"Bitmap above top_bitmap() must be clear\");\n+  } else if (r->affiliation() != req.affiliation()) {\n+    assert(_heap->mode()->is_generational(), \"Request for %s from %s region should only happen in generational mode.\",\n+           req.affiliation_name(), r->affiliation_name());\n+    return nullptr;\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n+  }\n+\n+  in_new_region = r->is_empty();\n+  HeapWord* result = nullptr;\n+\n+  if (in_new_region) {\n+    log_debug(gc, free)(\"Using new region (\" SIZE_FORMAT \") for %s (\" PTR_FORMAT \").\",\n+                       r->index(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(&req));\n@@ -766,0 +1441,46 @@\n+  \/\/ req.size() is in words, r->free() is in bytes.\n+  if (req.is_lab_alloc()) {\n+    if (req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+      assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+      assert(_free_sets.in_free_set(r->index(), OldCollector), \"PLABS must be allocated in old_collector_free regions\");\n+      \/\/ Need to assure that plabs are aligned on multiple of card region.\n+      \/\/ Since we have Elastic TLABs, align sizes up. They may be decreased to fit in the usable\n+      \/\/ memory remaining in the region (which will also be aligned to cards).\n+      size_t adjusted_size = align_up(req.size(), CardTable::card_size_in_words());\n+      size_t adjusted_min_size = align_up(req.min_size(), CardTable::card_size_in_words());\n+      size_t usable_free = get_usable_free_words(r->free());\n+\n+      if (adjusted_size > usable_free) {\n+        adjusted_size = usable_free;\n+      }\n+\n+      if (adjusted_size >= adjusted_min_size) {\n+        result = allocate_aligned_plab(adjusted_size, req, r);\n+      }\n+      \/\/ Otherwise, leave result == nullptr because the adjusted size is smaller than min size.\n+    } else {\n+      \/\/ This is a GCLAB or a TLAB allocation\n+      size_t adjusted_size = req.size();\n+      size_t free = align_down(r->free() >> LogHeapWordSize, MinObjAlignment);\n+      if (adjusted_size > free) {\n+        adjusted_size = free;\n+      }\n+      if (adjusted_size >= req.min_size()) {\n+        result = r->allocate(adjusted_size, req);\n+        assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, adjusted_size);\n+        req.set_actual_size(adjusted_size);\n+      } else {\n+        log_trace(gc, free)(\"Failed to shrink TLAB or GCLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n+                           \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n+      }\n+    }\n+  } else {\n+    size_t size = req.size();\n+    result = r->allocate(size, req);\n+    if (result != nullptr) {\n+      \/\/ Record actual allocation size\n+      req.set_actual_size(size);\n+    }\n+  }\n+\n+  ShenandoahGeneration* generation = _heap->generation_for(req.affiliation());\n@@ -769,0 +1490,1 @@\n+<<<<<<< HEAD\n@@ -770,0 +1492,4 @@\n+=======\n+      assert(req.is_young(), \"Mutator allocations always come from young generation.\");\n+      _free_sets.increase_used(Mutator, req.actual_size() * HeapWordSize);\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -774,0 +1500,1 @@\n+<<<<<<< HEAD\n@@ -775,0 +1502,8 @@\n+=======\n+      \/\/ evacuation are not updated during evacuation.  For both young and old regions r, it is essential that all\n+      \/\/ PLABs be made parsable at the end of evacuation.  This is enabled by retiring all plabs at end of evacuation.\n+      \/\/ TODO: Making a PLAB parsable involves placing a filler object in its remnant memory but does not require\n+      \/\/ that the PLAB be disabled for all future purposes.  We may want to introduce a new service to make the\n+      \/\/ PLABs parsable while still allowing the PLAB to serve future allocation requests that arise during the\n+      \/\/ next evacuation pass.\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -776,0 +1511,4 @@\n+      if (r->is_old()) {\n+        assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"old-gen allocations use PLAB or shared allocation\");\n+        \/\/ for plabs, we'll sort the difference between evac and promotion usage when we retire the plab\n+      }\n@@ -779,0 +1518,1 @@\n+<<<<<<< HEAD\n@@ -794,0 +1534,26 @@\n+=======\n+  if (result == nullptr || alloc_capacity(r) < PLAB::min_size() * HeapWordSize) {\n+    \/\/ Region cannot afford this and is likely to not afford future allocations. Retire it.\n+    \/\/\n+    \/\/ While this seems a bit harsh, especially in the case when this large allocation does not\n+    \/\/ fit but the next small one would, we are risking to inflate scan times when lots of\n+    \/\/ almost-full regions precede the fully-empty region where we want to allocate the entire TLAB.\n+\n+    \/\/ Record the remainder as allocation waste\n+    size_t idx = r->index();\n+    if (req.is_mutator_alloc()) {\n+      size_t waste = r->free();\n+      if (waste > 0) {\n+        _free_sets.increase_used(Mutator, waste);\n+        \/\/ This one request could cause several regions to be \"retired\", so we must accumulate the waste\n+        req.set_waste((waste >> LogHeapWordSize) + req.waste());\n+      }\n+      assert(_free_sets.membership(idx) == Mutator, \"Must be mutator free: \" SIZE_FORMAT, idx);\n+    } else {\n+      assert(_free_sets.membership(idx) == Collector || _free_sets.membership(idx) == OldCollector,\n+             \"Must be collector or old-collector free: \" SIZE_FORMAT, idx);\n+    }\n+    \/\/ This region is no longer considered free (in any set)\n+    _free_sets.remove_from_free_sets(idx);\n+    _free_sets.assert_bounds();\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -805,0 +1571,1 @@\n+<<<<<<< HEAD\n@@ -808,0 +1575,15 @@\n+=======\n+  assert(req.is_young(), \"Humongous regions always allocated in YOUNG\");\n+  ShenandoahGeneration* generation = _heap->generation_for(req.affiliation());\n+\n+  \/\/ Check if there are enough regions left to satisfy allocation.\n+  if (_heap->mode()->is_generational()) {\n+    size_t avail_young_regions = generation->free_unaffiliated_regions();\n+    if (num > _free_sets.count(Mutator) || (num > avail_young_regions)) {\n+      return nullptr;\n+    }\n+  } else {\n+    if (num > _free_sets.count(Mutator)) {\n+      return nullptr;\n+    }\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -816,0 +1598,1 @@\n+<<<<<<< HEAD\n@@ -851,0 +1634,18 @@\n+=======\n+\n+  size_t beg = _free_sets.leftmost(Mutator);\n+  size_t end = beg;\n+\n+  while (true) {\n+    if (end >= _free_sets.max()) {\n+      \/\/ Hit the end, goodbye\n+      return nullptr;\n+    }\n+\n+    \/\/ If regions are not adjacent, then current [beg; end] is useless, and we may fast-forward.\n+    \/\/ If region is not completely free, the current [beg; end] is useless, and we may fast-forward.\n+    if (!_free_sets.in_free_set(end, Mutator) || !can_allocate_from(_heap->get_region(end))) {\n+      end++;\n+      beg = end;\n+      continue;\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -862,0 +1663,5 @@\n+<<<<<<< HEAD\n+=======\n+  ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -884,0 +1690,2 @@\n+    r->set_affiliation(req.affiliation());\n+    r->set_update_watermark(r->bottom());\n@@ -885,0 +1693,6 @@\n+<<<<<<< HEAD\n+=======\n+\n+    \/\/ While individual regions report their true use, all humongous regions are marked used in the free set.\n+    _free_sets.remove_from_free_sets(r->index());\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -886,0 +1700,1 @@\n+  _heap->young_generation()->increase_affiliated_region_count(num);\n@@ -887,0 +1702,1 @@\n+<<<<<<< HEAD\n@@ -898,0 +1714,5 @@\n+=======\n+  size_t total_humongous_size = ShenandoahHeapRegion::region_size_bytes() * num;\n+  _free_sets.increase_used(Mutator, total_humongous_size);\n+  _free_sets.assert_bounds();\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -899,0 +1720,3 @@\n+  if (remainder != 0) {\n+    req.set_waste(ShenandoahHeapRegion::region_size_words() - remainder);\n+  }\n@@ -902,0 +1726,33 @@\n+<<<<<<< HEAD\n+=======\n+\/\/ Returns true iff this region is entirely available, either because it is empty() or because it has been found to represent\n+\/\/ immediate trash and we'll be able to immediately recycle it.  Note that we cannot recycle immediate trash if\n+\/\/ concurrent weak root processing is in progress.\n+bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) const {\n+  return r->is_empty() || (r->is_trash() && !_heap->is_concurrent_weak_root_in_progress());\n+}\n+\n+bool ShenandoahFreeSet::can_allocate_from(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return can_allocate_from(r);\n+}\n+\n+size_t ShenandoahFreeSet::alloc_capacity(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return alloc_capacity(r);\n+}\n+\n+size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) const {\n+  if (r->is_trash()) {\n+    \/\/ This would be recycled on allocation path\n+    return ShenandoahHeapRegion::region_size_bytes();\n+  } else {\n+    return r->free();\n+  }\n+}\n+\n+bool ShenandoahFreeSet::has_alloc_capacity(ShenandoahHeapRegion *r) const {\n+  return alloc_capacity(r) > 0;\n+}\n+\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -904,1 +1761,0 @@\n-    _heap->decrease_used(r->used());\n@@ -923,0 +1779,20 @@\n+void ShenandoahFreeSet::flip_to_old_gc(ShenandoahHeapRegion* r) {\n+  size_t idx = r->index();\n+\n+  assert(_free_sets.in_free_set(idx, Mutator), \"Should be in mutator view\");\n+  \/\/ Note: can_allocate_from(r) means r is entirely empty\n+  assert(can_allocate_from(r), \"Should not be allocated\");\n+\n+  size_t region_capacity = alloc_capacity(r);\n+  _free_sets.move_to_set(idx, OldCollector, region_capacity);\n+  _free_sets.assert_bounds();\n+  _heap->old_generation()->augment_evacuation_reserve(region_capacity);\n+  bool transferred = _heap->generation_sizer()->transfer_to_old(1);\n+  if (!transferred) {\n+    log_warning(gc, free)(\"Forcing transfer of \" SIZE_FORMAT \" to old reserve.\", idx);\n+    _heap->generation_sizer()->force_transfer_to_old(1);\n+  }\n+  \/\/ We do not ensure that the region is no longer trash, relying on try_allocate_in(), which always comes next,\n+  \/\/ to recycle trash before attempting to allocate anything in the region.\n+}\n+\n@@ -926,0 +1802,1 @@\n+<<<<<<< HEAD\n@@ -933,0 +1810,8 @@\n+=======\n+  assert(_free_sets.in_free_set(idx, Mutator), \"Should be in mutator view\");\n+  assert(can_allocate_from(r), \"Should not be allocated\");\n+\n+  size_t region_capacity = alloc_capacity(r);\n+  _free_sets.move_to_set(idx, Collector, region_capacity);\n+  _free_sets.assert_bounds();\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -944,0 +1829,1 @@\n+<<<<<<< HEAD\n@@ -995,0 +1881,52 @@\n+=======\n+  _free_sets.clear_all();\n+}\n+\n+\/\/ This function places all is_old() regions that have allocation capacity into the old_collector set.  It places\n+\/\/ all other regions (not is_old()) that have allocation capacity into the mutator_set.  Subsequently, we will\n+\/\/ move some of the mutator regions into the collector set or old_collector set with the intent of packing\n+\/\/ old_collector memory into the highest (rightmost) addresses of the heap and the collector memory into the\n+\/\/ next highest addresses of the heap, with mutator memory consuming the lowest addresses of the heap.\n+void ShenandoahFreeSet::find_regions_with_alloc_capacity(size_t &young_cset_regions, size_t &old_cset_regions,\n+                                                         size_t &first_old_region, size_t &last_old_region,\n+                                                         size_t &old_region_count) {\n+  first_old_region = _heap->num_regions();\n+  last_old_region = 0;\n+  old_region_count = 0;\n+  old_cset_regions = 0;\n+  young_cset_regions = 0;\n+  for (size_t idx = 0; idx < _heap->num_regions(); idx++) {\n+    ShenandoahHeapRegion* region = _heap->get_region(idx);\n+    if (region->is_trash()) {\n+      \/\/ Trashed regions represent regions that had been in the collection set but have not yet been \"cleaned up\".\n+      if (region->is_old()) {\n+        old_cset_regions++;\n+      } else {\n+        assert(region->is_young(), \"Trashed region should be old or young\");\n+        young_cset_regions++;\n+      }\n+    } else if (region->is_old() && region->is_regular()) {\n+      old_region_count++;\n+      if (first_old_region > idx) {\n+        first_old_region = idx;\n+      }\n+      last_old_region = idx;\n+    }\n+    if (region->is_alloc_allowed() || region->is_trash()) {\n+      assert(!region->is_cset(), \"Shouldn't be adding cset regions to the free set\");\n+      assert(_free_sets.in_free_set(idx, NotFree), \"We are about to make region free; it should not be free already\");\n+\n+      \/\/ Do not add regions that would almost surely fail allocation.  Note that PLAB::min_size() is typically less than ShenandoahGenerationalHeap::plab_min_size()\n+      if (alloc_capacity(region) < PLAB::min_size() * HeapWordSize) continue;\n+\n+      if (region->is_old()) {\n+        _free_sets.make_free(idx, OldCollector, alloc_capacity(region));\n+        log_debug(gc, free)(\n+          \"  Adding Region \" SIZE_FORMAT  \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to old collector set\",\n+          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n+          byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+      } else {\n+        _free_sets.make_free(idx, Mutator, alloc_capacity(region));\n+        log_debug(gc, free)(\n+          \"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator set\",\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -1000,0 +1938,1 @@\n+<<<<<<< HEAD\n@@ -1047,0 +1986,53 @@\n+=======\n+}\n+\n+\/\/ Move no more than cset_regions from the existing Collector and OldCollector free sets to the Mutator free set.\n+\/\/ This is called from outside the heap lock.\n+void ShenandoahFreeSet::move_collector_sets_to_mutator(size_t max_xfer_regions) {\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t collector_empty_xfer = 0;\n+  size_t collector_not_empty_xfer = 0;\n+  size_t old_collector_empty_xfer = 0;\n+\n+  \/\/ Process empty regions within the Collector free set\n+  if ((max_xfer_regions > 0) && (_free_sets.leftmost_empty(Collector) <= _free_sets.rightmost_empty(Collector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    for (size_t idx = _free_sets.leftmost_empty(Collector);\n+         (max_xfer_regions > 0) && (idx <= _free_sets.rightmost_empty(Collector)); idx++) {\n+      if (_free_sets.in_free_set(idx, Collector) && can_allocate_from(idx)) {\n+        _free_sets.move_to_set(idx, Mutator, region_size_bytes);\n+        max_xfer_regions--;\n+        collector_empty_xfer += region_size_bytes;\n+      }\n+    }\n+  }\n+\n+  \/\/ Process empty regions within the OldCollector free set\n+  size_t old_collector_regions = 0;\n+  if ((max_xfer_regions > 0) && (_free_sets.leftmost_empty(OldCollector) <= _free_sets.rightmost_empty(OldCollector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    for (size_t idx = _free_sets.leftmost_empty(OldCollector);\n+         (max_xfer_regions > 0) && (idx <= _free_sets.rightmost_empty(OldCollector)); idx++) {\n+      if (_free_sets.in_free_set(idx, OldCollector) && can_allocate_from(idx)) {\n+        _free_sets.move_to_set(idx, Mutator, region_size_bytes);\n+        max_xfer_regions--;\n+        old_collector_empty_xfer += region_size_bytes;\n+        old_collector_regions++;\n+      }\n+    }\n+    if (old_collector_regions > 0) {\n+      _heap->generation_sizer()->transfer_to_young(old_collector_regions);\n+    }\n+  }\n+\n+  \/\/ If there are any non-empty regions within Collector set, we can also move them to the Mutator free set\n+  if ((max_xfer_regions > 0) && (_free_sets.leftmost(Collector) <= _free_sets.rightmost(Collector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    for (size_t idx = _free_sets.leftmost(Collector); (max_xfer_regions > 0) && (idx <= _free_sets.rightmost(Collector)); idx++) {\n+      size_t alloc_capacity = this->alloc_capacity(idx);\n+      if (_free_sets.in_free_set(idx, Collector) && (alloc_capacity > 0)) {\n+        _free_sets.move_to_set(idx, Mutator, alloc_capacity);\n+        max_xfer_regions--;\n+        collector_not_empty_xfer += alloc_capacity;\n+      }\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -1051,0 +2043,1 @@\n+<<<<<<< HEAD\n@@ -1097,0 +2090,121 @@\n+=======\n+  size_t total_xfer = collector_xfer + old_collector_empty_xfer;\n+  log_info(gc, free)(\"At start of update refs, moving \" SIZE_FORMAT \"%s to Mutator free set from Collector Reserve (\"\n+                     SIZE_FORMAT \"%s) and from Old Collector Reserve (\" SIZE_FORMAT \"%s)\",\n+                     byte_size_in_proper_unit(total_xfer), proper_unit_for_byte_size(total_xfer),\n+                     byte_size_in_proper_unit(collector_xfer), proper_unit_for_byte_size(collector_xfer),\n+                     byte_size_in_proper_unit(old_collector_empty_xfer), proper_unit_for_byte_size(old_collector_empty_xfer));\n+}\n+\n+\n+\/\/ Overwrite arguments to represent the amount of memory in each generation that is about to be recycled\n+void ShenandoahFreeSet::prepare_to_rebuild(size_t &young_cset_regions, size_t &old_cset_regions,\n+                                           size_t &first_old_region, size_t &last_old_region, size_t &old_region_count) {\n+  shenandoah_assert_heaplocked();\n+  \/\/ This resets all state information, removing all regions from all sets.\n+  clear();\n+  log_debug(gc, free)(\"Rebuilding FreeSet\");\n+\n+  \/\/ This places regions that have alloc_capacity into the old_collector set if they identify as is_old() or the\n+  \/\/ mutator set otherwise.\n+  find_regions_with_alloc_capacity(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+}\n+\n+void ShenandoahFreeSet::rebuild(size_t young_cset_regions, size_t old_cset_regions, bool have_evacuation_reserves) {\n+  shenandoah_assert_heaplocked();\n+  size_t young_reserve(0), old_reserve(0);\n+\n+  if (!_heap->mode()->is_generational()) {\n+    young_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+    old_reserve = 0;\n+  } else {\n+    compute_young_and_old_reserves(young_cset_regions, old_cset_regions, have_evacuation_reserves,\n+                                   young_reserve, old_reserve);\n+\n+  }\n+\n+  reserve_regions(young_reserve, old_reserve);\n+  _free_sets.establish_alloc_bias(OldCollector);\n+  _free_sets.assert_bounds();\n+  log_status();\n+}\n+\n+void ShenandoahFreeSet::compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions, bool have_evacuation_reserves,\n+                                                       size_t& young_reserve_result, size_t& old_reserve_result) const {\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+  ShenandoahOldGeneration* const old_generation = _heap->old_generation();\n+  size_t old_available = old_generation->available();\n+  size_t old_unaffiliated_regions = old_generation->free_unaffiliated_regions();\n+  ShenandoahYoungGeneration* const young_generation = _heap->young_generation();\n+  size_t young_capacity = young_generation->max_capacity();\n+  size_t young_unaffiliated_regions = young_generation->free_unaffiliated_regions();\n+\n+  \/\/ Add in the regions we anticipate to be freed by evacuation of the collection set\n+  old_unaffiliated_regions += old_cset_regions;\n+  old_available += old_cset_regions * region_size_bytes;\n+  young_unaffiliated_regions += young_cset_regions;\n+\n+  \/\/ Consult old-region balance to make adjustments to current generation capacities and availability.\n+  \/\/ The generation region transfers take place after we rebuild.\n+  const ssize_t old_region_balance = old_generation->get_region_balance();\n+  if (old_region_balance != 0) {\n+    if (old_region_balance > 0) {\n+      assert(old_region_balance <= checked_cast<ssize_t>(old_unaffiliated_regions), \"Cannot transfer regions that are affiliated\");\n+    } else {\n+      assert(0 - old_region_balance <= checked_cast<ssize_t>(young_unaffiliated_regions), \"Cannot transfer regions that are affiliated\");\n+    }\n+\n+    ssize_t xfer_bytes = old_region_balance * checked_cast<ssize_t>(region_size_bytes);\n+    old_available -= xfer_bytes;\n+    old_unaffiliated_regions -= old_region_balance;\n+    young_capacity += xfer_bytes;\n+    young_unaffiliated_regions += old_region_balance;\n+  }\n+\n+  \/\/ All allocations taken from the old collector set are performed by GC, generally using PLABs for both\n+  \/\/ promotions and evacuations.  The partition between which old memory is reserved for evacuation and\n+  \/\/ which is reserved for promotion is enforced using thread-local variables that prescribe intentions for\n+  \/\/ each PLAB's available memory.\n+  if (have_evacuation_reserves) {\n+    \/\/ We are rebuilding at the end of final mark, having already established evacuation budgets for this GC pass.\n+    const size_t promoted_reserve = old_generation->get_promoted_reserve();\n+    const size_t old_evac_reserve = old_generation->get_evacuation_reserve();\n+    young_reserve_result = young_generation->get_evacuation_reserve();\n+    old_reserve_result = promoted_reserve + old_evac_reserve;\n+    assert(old_reserve_result <= old_available,\n+           \"Cannot reserve (\" SIZE_FORMAT \" + \" SIZE_FORMAT\") more OLD than is available: \" SIZE_FORMAT,\n+           promoted_reserve, old_evac_reserve, old_available);\n+  } else {\n+    \/\/ We are rebuilding at end of GC, so we set aside budgets specified on command line (or defaults)\n+    young_reserve_result = (young_capacity * ShenandoahEvacReserve) \/ 100;\n+    \/\/ The auto-sizer has already made old-gen large enough to hold all anticipated evacuations and promotions.\n+    \/\/ Affiliated old-gen regions are already in the OldCollector free set.  Add in the relevant number of\n+    \/\/ unaffiliated regions.\n+    old_reserve_result = old_available;\n+  }\n+\n+  \/\/ Old available regions that have less than PLAB::min_size() of available memory are not placed into the OldCollector\n+  \/\/ free set.  Because of this, old_available may not have enough memory to represent the intended reserve.  Adjust\n+  \/\/ the reserve downward to account for this possibility. This loss is part of the reason why the original budget\n+  \/\/ was adjusted with ShenandoahOldEvacWaste and ShenandoahOldPromoWaste multipliers.\n+  if (old_reserve_result > _free_sets.capacity_of(OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n+    old_reserve_result = _free_sets.capacity_of(OldCollector) + old_unaffiliated_regions * region_size_bytes;\n+  }\n+\n+  if (old_reserve_result > young_unaffiliated_regions * region_size_bytes) {\n+    young_reserve_result = young_unaffiliated_regions * region_size_bytes;\n+  }\n+}\n+\n+\/\/ Having placed all regions that have allocation capacity into the mutator set if they identify as is_young()\n+\/\/ or into the old collector set if they identify as is_old(), move some of these regions from the mutator set\n+\/\/ into the collector set or old collector set in order to assure that the memory available for allocations within\n+\/\/ the collector set is at least to_reserve, and the memory available for allocations within the old collector set\n+\/\/ is at least to_reserve_old.\n+void ShenandoahFreeSet::reserve_regions(size_t to_reserve, size_t to_reserve_old) {\n+  for (size_t i = _heap->num_regions(); i > 0; i--) {\n+    size_t idx = i - 1;\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (!_free_sets.in_free_set(idx, Mutator)) {\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -1101,0 +2215,1 @@\n+<<<<<<< HEAD\n@@ -1120,0 +2235,28 @@\n+=======\n+    assert (ac > 0, \"Membership in free set implies has capacity\");\n+    assert (!r->is_old(), \"mutator_is_free regions should not be affiliated OLD\");\n+\n+    bool move_to_old = _free_sets.capacity_of(OldCollector) < to_reserve_old;\n+    bool move_to_young = _free_sets.capacity_of(Collector) < to_reserve;\n+\n+    if (!move_to_old && !move_to_young) {\n+      \/\/ We've satisfied both to_reserve and to_reserved_old\n+      break;\n+    }\n+\n+    if (move_to_old) {\n+      if (r->is_trash() || !r->is_affiliated()) {\n+        \/\/ OLD regions that have available memory are already in the old_collector free set\n+        _free_sets.move_to_set(idx, OldCollector, ac);\n+        log_debug(gc, free)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to old_collector_free\", idx);\n+        continue;\n+      }\n+    }\n+\n+    if (move_to_young) {\n+      \/\/ Note: In a previous implementation, regions were only placed into the survivor space (collector_is_free) if\n+      \/\/ they were entirely empty.  I'm not sure I understand the rationale for that.  That alternative behavior would\n+      \/\/ tend to mix survivor objects with ephemeral objects, making it more difficult to reclaim the memory for the\n+      \/\/ ephemeral objects.  It also delays aging of regions, causing promotion in place to be delayed.\n+      _free_sets.move_to_set(idx, Collector, ac);\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -1125,0 +2268,1 @@\n+<<<<<<< HEAD\n@@ -1129,0 +2273,11 @@\n+=======\n+    size_t old_reserve = _free_sets.capacity_of(OldCollector);\n+    if (old_reserve < to_reserve_old) {\n+      log_info(gc, free)(\"Wanted \" PROPERFMT \" for old reserve, but only reserved: \" PROPERFMT,\n+                         PROPERFMTARGS(to_reserve_old), PROPERFMTARGS(old_reserve));\n+    }\n+    size_t young_reserve = _free_sets.capacity_of(Collector);\n+    if (young_reserve < to_reserve) {\n+      log_info(gc, free)(\"Wanted \" PROPERFMT \" for young reserve, but only reserved: \" PROPERFMT,\n+                         PROPERFMTARGS(to_reserve), PROPERFMTARGS(young_reserve));\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -1140,0 +2295,1 @@\n+<<<<<<< HEAD\n@@ -1145,0 +2301,17 @@\n+=======\n+    size_t retired_old = 0;\n+    size_t retired_old_humongous = 0;\n+    size_t retired_young = 0;\n+    size_t retired_young_humongous = 0;\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    size_t retired_young_waste = 0;\n+    size_t retired_old_waste = 0;\n+    size_t consumed_collector = 0;\n+    size_t consumed_old_collector = 0;\n+    size_t consumed_mutator = 0;\n+    size_t available_old = 0;\n+    size_t available_young = 0;\n+    size_t available_mutator = 0;\n+    size_t available_collector = 0;\n+    size_t available_old_collector = 0;\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -1150,0 +2323,1 @@\n+<<<<<<< HEAD\n@@ -1157,0 +2331,12 @@\n+=======\n+    log_debug(gc, free)(\"FreeSet map legend:\"\n+                       \" M:mutator_free C:collector_free O:old_collector_free\"\n+                       \" H:humongous ~:retired old _:retired young\");\n+    log_debug(gc, free)(\" mutator free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                       \" collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                       \"old collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"] allocates from %s\",\n+                       _free_sets.leftmost(Mutator), _free_sets.rightmost(Mutator),\n+                       _free_sets.leftmost(Collector), _free_sets.rightmost(Collector),\n+                       _free_sets.leftmost(OldCollector), _free_sets.rightmost(OldCollector),\n+                       _free_sets.alloc_from_left_bias(OldCollector)? \"left to right\": \"right to left\");\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -1162,0 +2348,1 @@\n+<<<<<<< HEAD\n@@ -1165,0 +2352,6 @@\n+=======\n+        log_debug(gc, free)(\" %6u: %s\", i-64, buffer);\n+      }\n+      if (_free_sets.in_free_set(i, Mutator)) {\n+        assert(!r->is_old(), \"Old regions should not be in mutator_free set\");\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -1169,0 +2362,1 @@\n+<<<<<<< HEAD\n@@ -1170,0 +2364,4 @@\n+=======\n+      } else if (_free_sets.in_free_set(i, Collector)) {\n+        assert(!r->is_old(), \"Old regions should not be in collector_free set\");\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -1174,0 +2372,1 @@\n+<<<<<<< HEAD\n@@ -1178,0 +2377,25 @@\n+=======\n+      } else if (_free_sets.in_free_set(i, OldCollector)) {\n+        size_t capacity = alloc_capacity(r);\n+        available_old_collector += capacity;\n+        consumed_old_collector += region_size_bytes - capacity;\n+        buffer[idx] = (capacity == region_size_bytes)? 'O': 'o';\n+      } else if (r->is_humongous()) {\n+        if (r->is_old()) {\n+          buffer[idx] = 'H';\n+          retired_old_humongous += region_size_bytes;\n+        } else {\n+          buffer[idx] = 'h';\n+          retired_young_humongous += region_size_bytes;\n+        }\n+      } else {\n+        if (r->is_old()) {\n+          buffer[idx] = '~';\n+          retired_old_waste += alloc_capacity(r);\n+          retired_old += region_size_bytes;\n+        } else {\n+          buffer[idx] = '_';\n+          retired_young_waste += alloc_capacity(r);\n+          retired_young += region_size_bytes;\n+        }\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -1186,0 +2410,1 @@\n+<<<<<<< HEAD\n@@ -1187,0 +2412,5 @@\n+=======\n+    log_debug(gc, free)(\" %6u: %s\", (uint) (_heap->num_regions() - remnant), buffer);\n+    size_t total_young = retired_young + retired_young_humongous;\n+    size_t total_old = retired_old + retired_old_humongous;\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -1205,0 +2435,1 @@\n+<<<<<<< HEAD\n@@ -1208,0 +2439,4 @@\n+=======\n+      for (size_t idx = _free_sets.leftmost(Mutator); idx <= _free_sets.rightmost(Mutator); idx++) {\n+        if (_free_sets.in_free_set(idx, Mutator)) {\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -1231,0 +2466,1 @@\n+<<<<<<< HEAD\n@@ -1235,0 +2471,5 @@\n+=======\n+      assert(free == total_free, \"Sum of free within mutator regions (\" SIZE_FORMAT\n+             \") should match mutator capacity (\" SIZE_FORMAT \") minus mutator used (\" SIZE_FORMAT \")\",\n+             total_free, capacity(), used());\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -1252,0 +2493,1 @@\n+<<<<<<< HEAD\n@@ -1255,0 +2497,4 @@\n+=======\n+      if (_free_sets.count(Mutator) > 0) {\n+        frag_int = (100 * (total_used \/ _free_sets.count(Mutator)) \/ ShenandoahHeapRegion::region_size_bytes());\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -1260,0 +2506,1 @@\n+<<<<<<< HEAD\n@@ -1262,0 +2509,3 @@\n+=======\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used), _free_sets.count(Mutator));\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -1269,0 +2519,1 @@\n+<<<<<<< HEAD\n@@ -1272,0 +2523,4 @@\n+=======\n+      for (size_t idx = _free_sets.leftmost(Collector); idx <= _free_sets.rightmost(Collector); idx++) {\n+        if (_free_sets.in_free_set(idx, Collector)) {\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -1283,0 +2538,23 @@\n+<<<<<<< HEAD\n+=======\n+    }\n+\n+    if (_heap->mode()->is_generational()) {\n+      size_t max = 0;\n+      size_t total_free = 0;\n+      size_t total_used = 0;\n+\n+      for (size_t idx = _free_sets.leftmost(OldCollector); idx <= _free_sets.rightmost(OldCollector); idx++) {\n+        if (_free_sets.in_free_set(idx, OldCollector)) {\n+          ShenandoahHeapRegion *r = _heap->get_region(idx);\n+          size_t free = alloc_capacity(r);\n+          max = MAX2(max, free);\n+          total_free += free;\n+          total_used += r->used();\n+        }\n+      }\n+      ls.print_cr(\" Old Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n+                  byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),\n+                  byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max),\n+                  byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used));\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -1289,0 +2567,5 @@\n+<<<<<<< HEAD\n+=======\n+\n+  \/\/ Allocation request is known to satisfy all memory budgeting constraints.\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -1295,0 +2578,1 @@\n+      case ShenandoahAllocRequest::_alloc_plab:\n@@ -1310,0 +2594,1 @@\n+<<<<<<< HEAD\n@@ -1326,0 +2611,38 @@\n+=======\n+size_t ShenandoahFreeSet::unsafe_peek_free() const {\n+  \/\/ Deliberately not locked, this method is unsafe when free set is modified.\n+\n+  for (size_t index = _free_sets.leftmost(Mutator); index <= _free_sets.rightmost(Mutator); index++) {\n+    if (index < _free_sets.max() && _free_sets.in_free_set(index, Mutator)) {\n+      ShenandoahHeapRegion* r = _heap->get_region(index);\n+      if (r->free() >= MinTLABSize) {\n+        return r->free();\n+      }\n+    }\n+  }\n+\n+  \/\/ It appears that no regions left\n+  return 0;\n+}\n+\n+void ShenandoahFreeSet::print_on(outputStream* out) const {\n+  out->print_cr(\"Mutator Free Set: \" SIZE_FORMAT \"\", _free_sets.count(Mutator));\n+  for (size_t index = _free_sets.leftmost(Mutator); index <= _free_sets.rightmost(Mutator); index++) {\n+    if (_free_sets.in_free_set(index, Mutator)) {\n+      _heap->get_region(index)->print_on(out);\n+    }\n+  }\n+  out->print_cr(\"Collector Free Set: \" SIZE_FORMAT \"\", _free_sets.count(Collector));\n+  for (size_t index = _free_sets.leftmost(Collector); index <= _free_sets.rightmost(Collector); index++) {\n+    if (_free_sets.in_free_set(index, Collector)) {\n+      _heap->get_region(index)->print_on(out);\n+    }\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n+  }\n+  if (_heap->mode()->is_generational()) {\n+    out->print_cr(\"Old Collector Free Set: \" SIZE_FORMAT \"\", _free_sets.count(OldCollector));\n+    for (size_t index = _free_sets.leftmost(OldCollector); index <= _free_sets.rightmost(OldCollector); index++) {\n+      if (_free_sets.in_free_set(index, OldCollector)) {\n+        _heap->get_region(index)->print_on(out);\n+      }\n+    }\n@@ -1334,0 +2657,1 @@\n+<<<<<<< HEAD\n@@ -1344,0 +2668,10 @@\n+=======\n+  for (size_t index = _free_sets.leftmost(Mutator); index <= _free_sets.rightmost(Mutator); index++) {\n+    if (_free_sets.in_free_set(index, Mutator)) {\n+      ShenandoahHeapRegion* r = _heap->get_region(index);\n+      size_t used = r->used();\n+      squared += used * used;\n+      linear += used;\n+      count++;\n+    }\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -1361,0 +2695,1 @@\n+<<<<<<< HEAD\n@@ -1370,0 +2705,12 @@\n+=======\n+  for (size_t index = _free_sets.leftmost(Mutator); index <= _free_sets.rightmost(Mutator); index++) {\n+    if (_free_sets.in_free_set(index, Mutator)) {\n+      ShenandoahHeapRegion* r = _heap->get_region(index);\n+      if (r->is_empty()) {\n+        free += ShenandoahHeapRegion::region_size_bytes();\n+        if (last_idx + 1 == index) {\n+          empty_contig++;\n+        } else {\n+          empty_contig = 1;\n+        }\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":1349,"deletions":2,"binary":false,"changes":1351,"status":"modified"},{"patch":"@@ -0,0 +1,1 @@\n+\n@@ -257,0 +258,116 @@\n+enum ShenandoahFreeMemoryType : uint8_t {\n+  NotFree,\n+  Mutator,\n+  Collector,\n+  OldCollector,\n+  NumFreeSets\n+};\n+\n+class ShenandoahSetsOfFree {\n+\n+private:\n+  size_t _max;                  \/\/ The maximum number of heap regions\n+  ShenandoahFreeSet* _free_set;\n+  size_t _region_size_bytes;\n+  ShenandoahFreeMemoryType* _membership;\n+  size_t _leftmosts[NumFreeSets];\n+  size_t _rightmosts[NumFreeSets];\n+  size_t _leftmosts_empty[NumFreeSets];\n+  size_t _rightmosts_empty[NumFreeSets];\n+  size_t _capacity_of[NumFreeSets];\n+  size_t _used_by[NumFreeSets];\n+  bool _left_to_right_bias[NumFreeSets];\n+  size_t _region_counts[NumFreeSets];\n+\n+  inline void shrink_bounds_if_touched(ShenandoahFreeMemoryType set, size_t idx);\n+  inline void expand_bounds_maybe(ShenandoahFreeMemoryType set, size_t idx, size_t capacity);\n+\n+  \/\/ Restore all state variables to initial default state.\n+  void clear_internal();\n+\n+public:\n+  ShenandoahSetsOfFree(size_t max_regions, ShenandoahFreeSet* free_set);\n+  ~ShenandoahSetsOfFree();\n+\n+  \/\/ Make all regions NotFree and reset all bounds\n+  void clear_all();\n+\n+  \/\/ Remove or retire region idx from all free sets.  Requires that idx is in a free set.  This does not affect capacity.\n+  void remove_from_free_sets(size_t idx);\n+\n+  \/\/ Place region idx into free set which_set.  Requires that idx is currently NotFree.\n+  void make_free(size_t idx, ShenandoahFreeMemoryType which_set, size_t region_capacity);\n+\n+  \/\/ Place region idx into free set new_set.  Requires that idx is currently not NotFree.\n+  void move_to_set(size_t idx, ShenandoahFreeMemoryType new_set, size_t region_capacity);\n+\n+  \/\/ Returns the ShenandoahFreeMemoryType affiliation of region idx, or NotFree if this region is not currently free.  This does\n+  \/\/ not enforce that free_set membership implies allocation capacity.\n+  inline ShenandoahFreeMemoryType membership(size_t idx) const;\n+\n+  \/\/ Returns true iff region idx is in the test_set free_set.  Before returning true, asserts that the free\n+  \/\/ set is not empty.  Requires that test_set != NotFree or NumFreeSets.\n+  inline bool in_free_set(size_t idx, ShenandoahFreeMemoryType which_set) const;\n+\n+  \/\/ The following four methods return the left-most and right-most bounds on ranges of regions representing\n+  \/\/ the requested set.  The _empty variants represent bounds on the range that holds completely empty\n+  \/\/ regions, which are required for humongous allocations and desired for \"very large\" allocations.  A\n+  \/\/ return value of -1 from leftmost() or leftmost_empty() denotes that the corresponding set is empty.\n+  \/\/ In other words:\n+  \/\/   if the requested which_set is empty:\n+  \/\/     leftmost() and leftmost_empty() return _max, rightmost() and rightmost_empty() return 0\n+  \/\/   otherwise, expect the following:\n+  \/\/     0 <= leftmost <= leftmost_empty <= rightmost_empty <= rightmost < _max\n+  inline size_t leftmost(ShenandoahFreeMemoryType which_set) const;\n+  inline size_t rightmost(ShenandoahFreeMemoryType which_set) const;\n+  size_t leftmost_empty(ShenandoahFreeMemoryType which_set);\n+  size_t rightmost_empty(ShenandoahFreeMemoryType which_set);\n+\n+  inline bool is_empty(ShenandoahFreeMemoryType which_set) const;\n+\n+  inline void increase_used(ShenandoahFreeMemoryType which_set, size_t bytes);\n+\n+  inline size_t capacity_of(ShenandoahFreeMemoryType which_set) const {\n+    assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+    return _capacity_of[which_set];\n+  }\n+\n+  inline size_t used_by(ShenandoahFreeMemoryType which_set) const {\n+    assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+    return _used_by[which_set];\n+  }\n+\n+  inline size_t max() const { return _max; }\n+\n+  inline size_t count(ShenandoahFreeMemoryType which_set) const { return _region_counts[which_set]; }\n+\n+  \/\/ Return true iff regions for allocation from this set should be peformed left to right.  Otherwise, allocate\n+  \/\/ from right to left.\n+  inline bool alloc_from_left_bias(ShenandoahFreeMemoryType which_set);\n+\n+  \/\/ Determine whether we prefer to allocate from left to right or from right to left for this free-set.\n+  void establish_alloc_bias(ShenandoahFreeMemoryType which_set);\n+\n+  \/\/ Assure leftmost, rightmost, leftmost_empty, and rightmost_empty bounds are valid for all free sets.\n+  \/\/ Valid bounds honor all of the following (where max is the number of heap regions):\n+  \/\/   if the set is empty, leftmost equals max and rightmost equals 0\n+  \/\/   Otherwise (the set is not empty):\n+  \/\/     0 <= leftmost < max and 0 <= rightmost < max\n+  \/\/     the region at leftmost is in the set\n+  \/\/     the region at rightmost is in the set\n+  \/\/     rightmost >= leftmost\n+  \/\/     for every idx that is in the set {\n+  \/\/       idx >= leftmost &&\n+  \/\/       idx <= rightmost\n+  \/\/     }\n+  \/\/   if the set has no empty regions, leftmost_empty equals max and rightmost_empty equals 0\n+  \/\/   Otherwise (the region has empty regions):\n+  \/\/     0 <= lefmost_empty < max and 0 <= rightmost_empty < max\n+  \/\/     rightmost_empty >= leftmost_empty\n+  \/\/     for every idx that is in the set and is empty {\n+  \/\/       idx >= leftmost &&\n+  \/\/       idx <= rightmost\n+  \/\/     }\n+  void assert_bounds() NOT_DEBUG_RETURN;\n+};\n+\n@@ -260,0 +377,1 @@\n+<<<<<<< HEAD\n@@ -280,0 +398,22 @@\n+=======\n+  ShenandoahSetsOfFree _free_sets;\n+\n+  HeapWord* try_allocate_in(ShenandoahHeapRegion* region, ShenandoahAllocRequest& req, bool& in_new_region);\n+\n+  HeapWord* allocate_aligned_plab(size_t size, ShenandoahAllocRequest& req, ShenandoahHeapRegion* r);\n+\n+  \/\/ Satisfy young-generation or single-generation collector allocation request req by finding memory that matches\n+  \/\/ affiliation, which either equals req.affiliation or FREE.  We know req.is_young().\n+  HeapWord* allocate_with_affiliation(ShenandoahAffiliation affiliation, ShenandoahAllocRequest& req, bool& in_new_region);\n+\n+  \/\/ Satisfy allocation request req by finding memory that matches affiliation, which either equals req.affiliation\n+  \/\/ or FREE. We know req.is_old().\n+  HeapWord* allocate_old_with_affiliation(ShenandoahAffiliation affiliation, ShenandoahAllocRequest& req, bool& in_new_region);\n+\n+  \/\/ While holding the heap lock, allocate memory for a single object which is to be entirely contained\n+  \/\/ within a single HeapRegion as characterized by req.  The req.size() value is known to be less than or\n+  \/\/ equal to ShenandoahHeapRegion::humongous_threshold_words().  The caller of allocate_single is responsible\n+  \/\/ for registering the resulting object and setting the remembered set card values as appropriate.  The\n+  \/\/ most common case is that we are allocating a PLAB in which case object registering and card dirtying\n+  \/\/ is managed after the PLAB is divided into individual objects.\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -294,0 +434,1 @@\n+<<<<<<< HEAD\n@@ -320,0 +461,11 @@\n+=======\n+  void flip_to_old_gc(ShenandoahHeapRegion* r);\n+\n+  void clear_internal();\n+\n+  void try_recycle_trashed(ShenandoahHeapRegion *r);\n+\n+  bool can_allocate_from(ShenandoahHeapRegion *r) const;\n+  bool can_allocate_from(size_t idx) const;\n+  bool has_alloc_capacity(ShenandoahHeapRegion *r) const;\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -324,0 +476,1 @@\n+<<<<<<< HEAD\n@@ -327,0 +480,4 @@\n+=======\n+  size_t alloc_capacity(ShenandoahHeapRegion *r) const;\n+  size_t alloc_capacity(size_t idx) const;\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -329,1 +486,20 @@\n-  void rebuild();\n+  void prepare_to_rebuild(size_t &young_cset_regions, size_t &old_cset_regions,\n+                          size_t &first_old_region, size_t &last_old_region, size_t &old_region_count);\n+\n+  \/\/ At the end of final mark, but before we begin evacuating, heuristics calculate how much memory is required to\n+  \/\/ hold the results of evacuating to young-gen and to old-gen.  These quantities, stored in reserves for their,\n+  \/\/ respective generations, are consulted prior to rebuilding the free set (ShenandoahFreeSet) in preparation for\n+  \/\/ evacuation.  When the free set is rebuilt, we make sure to reserve sufficient memory in the collector and\n+  \/\/ old_collector sets to hold evacuations, if have_evacuation_reserves is true.  The other time we rebuild the free\n+  \/\/ set is at the end of GC, as we prepare to idle GC until the next trigger.  In this case, have_evacuation_reserves\n+  \/\/ is false because we don't yet know how much memory will need to be evacuated in the next GC cycle.  When\n+  \/\/ have_evacuation_reserves is false, the free set rebuild operation reserves for the collector and old_collector sets\n+  \/\/ based on alternative mechanisms, such as ShenandoahEvacReserve, ShenandoahOldEvacReserve, and\n+  \/\/ ShenandoahOldCompactionReserve.  In a future planned enhancement, the reserve for old_collector set when the\n+  \/\/ evacuation reserves are unknown, is based in part on anticipated promotion as determined by analysis of live data\n+  \/\/ found during the previous GC pass which is one less than the current tenure age.\n+  void rebuild(size_t young_cset_regions, size_t old_cset_regions, bool have_evacuation_reserves = false);\n+\n+  void move_collector_sets_to_mutator(size_t cset_regions);\n+\n+  void add_old_collector_free_region(ShenandoahHeapRegion* region);\n@@ -345,0 +521,1 @@\n+<<<<<<< HEAD\n@@ -347,0 +524,4 @@\n+=======\n+  inline size_t capacity()  const { return _free_sets.capacity_of(Mutator); }\n+  inline size_t used()      const { return _free_sets.used_by(Mutator);     }\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -394,0 +575,9 @@\n+\n+  void find_regions_with_alloc_capacity(size_t &young_cset_regions, size_t &old_cset_regions,\n+                                        size_t &first_old_region, size_t &last_old_region, size_t &old_region_count);\n+  void reserve_regions(size_t young_reserve, size_t old_reserve);\n+\n+  \/\/ Reserve space for evacuations, with regions reserved for old evacuations placed to the right\n+  \/\/ of regions reserved of young evacuations.\n+  void compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions, bool have_evacuation_reserves,\n+                                      size_t &young_reserve_result, size_t &old_reserve_result) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":191,"deletions":1,"binary":false,"changes":192,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -37,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n@@ -39,0 +41,2 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalFullGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n@@ -59,1 +63,0 @@\n-#include \"runtime\/javaThread.hpp\"\n@@ -111,0 +114,4 @@\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahGenerationalFullGC::handle_completion(heap);\n+  }\n+\n@@ -122,1 +129,1 @@\n-  heap->heuristics()->record_success_full();\n+  heap->global_generation()->heuristics()->record_success_full();\n@@ -129,0 +136,4 @@\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahGenerationalFullGC::prepare();\n+  }\n+\n@@ -171,1 +182,1 @@\n-    \/\/ b. Cancel concurrent mark, if in progress\n+    \/\/ b. Cancel all concurrent marks, if in progress\n@@ -173,2 +184,2 @@\n-      ShenandoahConcurrentGC::cancel();\n-      heap->set_concurrent_mark_in_progress(false);\n+      \/\/ TODO: Send cancel_concurrent_mark upstream? Does it really not have it already?\n+      heap->cancel_concurrent_mark();\n@@ -184,1 +195,1 @@\n-    heap->reset_mark_bitmap();\n+    heap->global_generation()->reset_mark_bitmap();\n@@ -186,1 +197,1 @@\n-    assert(!heap->marking_context()->is_complete(), \"sanity\");\n+    assert(!heap->global_generation()->is_mark_complete(), \"sanity\");\n@@ -189,1 +200,1 @@\n-    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+    ShenandoahReferenceProcessor* rp = heap->global_generation()->ref_processor();\n@@ -195,0 +206,4 @@\n+    if (heap->mode()->is_generational()) {\n+      ShenandoahGenerationalFullGC::restore_top_before_promote(heap);\n+    }\n+\n@@ -202,0 +217,1 @@\n+    \/\/ TODO: Do we need to explicitly retire PLABs?\n@@ -258,0 +274,1 @@\n+  \/\/ Humongous regions are promoted on demand and are accounted for by normal Full GC mechanisms.\n@@ -276,2 +293,5 @@\n-    _ctx->capture_top_at_mark_start(r);\n-    r->clear_live_data();\n+    \/\/ TODO: Add API to heap to skip free regions\n+    if (r->is_affiliated()) {\n+      _ctx->capture_top_at_mark_start(r);\n+      r->clear_live_data();\n+    }\n@@ -292,1 +312,1 @@\n-  heap->set_unload_classes(heap->heuristics()->can_unload_classes());\n+  heap->set_unload_classes(heap->global_generation()->heuristics()->can_unload_classes());\n@@ -294,1 +314,1 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = heap->global_generation()->ref_processor();\n@@ -298,1 +318,1 @@\n-  ShenandoahSTWMark mark(true \/*full_gc*\/);\n+  ShenandoahSTWMark mark(heap->global_generation(), true \/*full_gc*\/);\n@@ -301,0 +321,4 @@\n+\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahGenerationalFullGC::log_live_in_old(heap);\n+  }\n@@ -428,2 +452,8 @@\n-  ShenandoahPrepareForCompactionObjectClosure cl(_preserved_marks->get(worker_id), empty_regions, from_region);\n-  prepare_for_compaction(cl, empty_regions, it, from_region);\n+  if (_heap->mode()->is_generational()) {\n+    ShenandoahPrepareForGenerationalCompactionObjectClosure cl(_preserved_marks->get(worker_id),\n+                                                               empty_regions, from_region, worker_id);\n+    prepare_for_compaction(cl, empty_regions, it, from_region);\n+  } else {\n+    ShenandoahPrepareForCompactionObjectClosure cl(_preserved_marks->get(worker_id), empty_regions, from_region);\n+    prepare_for_compaction(cl, empty_regions, it, from_region);\n+  }\n@@ -476,0 +506,1 @@\n+  log_debug(gc)(\"Full GC calculating target humongous objects from end \" SIZE_FORMAT, to_end);\n@@ -518,0 +549,1 @@\n+      \/\/ Leave affiliation unchanged\n@@ -542,0 +574,6 @@\n+    if (!r->is_affiliated()) {\n+      \/\/ Ignore free regions\n+      \/\/ TODO: change iterators so they do not process FREE regions.\n+      return;\n+    }\n+\n@@ -706,0 +744,5 @@\n+\/\/ TODO:\n+\/\/  Consider compacting old-gen objects toward the high end of memory and young-gen objects towards the low-end\n+\/\/  of memory.  As currently implemented, all regions are compacted toward the low-end of memory.  This creates more\n+\/\/  fragmentation of the heap, because old-gen regions get scattered among low-address regions such that it becomes\n+\/\/  more difficult to find contiguous regions for humongous objects.\n@@ -733,0 +776,2 @@\n+    \/\/ TODO: This is ResourceMark is missing upstream.\n+    ResourceMark rm;\n@@ -807,0 +852,3 @@\n+      if (_heap->mode()->is_generational()) {\n+        ShenandoahGenerationalFullGC::maybe_coalesce_and_fill_region(r);\n+      }\n@@ -911,1 +959,3 @@\n-  size_t _live;\n+  bool _is_generational;\n+  size_t _young_regions, _young_usage, _young_humongous_waste;\n+  size_t _old_regions, _old_usage, _old_humongous_waste;\n@@ -914,0 +964,1 @@\n+<<<<<<< HEAD\n@@ -915,0 +966,12 @@\n+=======\n+  ShenandoahPostCompactClosure() : _heap(ShenandoahHeap::heap()),\n+                                   _is_generational(_heap->mode()->is_generational()),\n+                                   _young_regions(0),\n+                                   _young_usage(0),\n+                                   _young_humongous_waste(0),\n+                                   _old_regions(0),\n+                                   _old_usage(0),\n+                                   _old_humongous_waste(0)\n+  {\n+    _heap->free_set()->clear();\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -933,0 +996,4 @@\n+      if (!_is_generational) {\n+        r->make_young_maybe();\n+      }\n+      \/\/ else, generational mode compaction has already established affiliation.\n@@ -948,0 +1015,6 @@\n+    } else {\n+      if (r->is_old()) {\n+        ShenandoahGenerationalFullGC::account_for_region(r, _old_regions, _old_usage, _old_humongous_waste);\n+      } else if (r->is_young()) {\n+        ShenandoahGenerationalFullGC::account_for_region(r, _young_regions, _young_usage, _young_humongous_waste);\n+      }\n@@ -949,1 +1022,0 @@\n-\n@@ -952,1 +1024,0 @@\n-    _live += live;\n@@ -955,2 +1026,15 @@\n-  size_t get_live() {\n-    return _live;\n+  void update_generation_usage() {\n+    if (_is_generational) {\n+      _heap->old_generation()->establish_usage(_old_regions, _old_usage, _old_humongous_waste);\n+      _heap->young_generation()->establish_usage(_young_regions, _young_usage, _young_humongous_waste);\n+    } else {\n+      assert(_old_regions == 0, \"Old regions only expected in generational mode\");\n+      assert(_old_usage == 0, \"Old usage only expected in generational mode\");\n+      assert(_old_humongous_waste == 0, \"Old humongous waste only expected in generational mode\");\n+    }\n+\n+    \/\/ In generational mode, global usage should be the sum of young and old. This is also true\n+    \/\/ for non-generational modes except that there are no old regions.\n+    _heap->global_generation()->establish_usage(_old_regions + _young_regions,\n+                                                _old_usage + _young_usage,\n+                                                _old_humongous_waste + _young_humongous_waste);\n@@ -987,0 +1071,1 @@\n+      log_debug(gc)(\"Full GC compaction moves humongous object from region \" SIZE_FORMAT \" to region \" SIZE_FORMAT, old_start, new_start);\n@@ -994,0 +1079,1 @@\n+        ShenandoahAffiliation original_affiliation = r->affiliation();\n@@ -996,0 +1082,1 @@\n+          \/\/ Leave humongous region affiliation unchanged.\n@@ -1003,1 +1090,1 @@\n-            r->make_humongous_start_bypass();\n+            r->make_humongous_start_bypass(original_affiliation);\n@@ -1005,1 +1092,1 @@\n-            r->make_humongous_cont_bypass();\n+            r->make_humongous_cont_bypass(original_affiliation);\n@@ -1090,1 +1177,5 @@\n-    heap->set_used(post_compact.get_live());\n+    post_compact.update_generation_usage();\n+\n+    if (heap->mode()->is_generational()) {\n+      ShenandoahGenerationalFullGC::balance_generations_after_gc(heap);\n+    }\n@@ -1093,2 +1184,20 @@\n-    heap->free_set()->rebuild();\n-    heap->clear_cancelled_gc();\n+    size_t young_cset_regions, old_cset_regions;\n+    size_t first_old, last_old, num_old;\n+    heap->free_set()->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+\n+    \/\/ We also do not expand old generation size following Full GC because we have scrambled age populations and\n+    \/\/ no longer have objects separated by age into distinct regions.\n+\n+    \/\/ TODO: Do we need to fix FullGC so that it maintains aged segregation of objects into distinct regions?\n+    \/\/       A partial solution would be to remember how many objects are of tenure age following Full GC, but\n+    \/\/       this is probably suboptimal, because most of these objects will not reside in a region that will be\n+    \/\/       selected for the next evacuation phase.\n+\n+\n+    if (heap->mode()->is_generational()) {\n+      ShenandoahGenerationalFullGC::compute_balances();\n+    }\n+\n+    heap->free_set()->rebuild(young_cset_regions, old_cset_regions);\n+\n+    heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n@@ -1099,0 +1208,7 @@\n+\n+  \/\/ We defer generation resizing actions until after cset regions have been recycled.  We do this even following an\n+  \/\/ abbreviated cycle.\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahGenerationalFullGC::balance_generations_after_rebuilding_free_set();\n+    ShenandoahGenerationalFullGC::rebuild_remembered_set(heap);\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":141,"deletions":25,"binary":false,"changes":166,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -39,0 +40,4 @@\n+#include \"gc\/shenandoah\/shenandoahAgeCensus.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n@@ -40,0 +45,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -47,0 +53,3 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalEvacuationTask.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n@@ -56,0 +65,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -62,0 +72,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -69,0 +80,2 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -72,0 +85,2 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n@@ -164,3 +179,0 @@\n-  \/\/ Now we know the number of regions, initialize the heuristics.\n-  initialize_heuristics();\n-\n@@ -182,0 +194,3 @@\n+  \/\/ Now we know the number of regions and heap sizes, initialize the heuristics.\n+  initialize_heuristics_generations();\n+\n@@ -220,0 +235,28 @@\n+  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this, _heap_region));\n+\n+  \/\/\n+  \/\/ After reserving the Java heap, create the card table, barriers, and workers, in dependency order\n+  \/\/\n+  if (mode()->is_generational()) {\n+    ShenandoahDirectCardMarkRememberedSet *rs;\n+    ShenandoahCardTable* card_table = ShenandoahBarrierSet::barrier_set()->card_table();\n+    size_t card_count = card_table->cards_required(heap_rs.size() \/ HeapWordSize);\n+    rs = new ShenandoahDirectCardMarkRememberedSet(ShenandoahBarrierSet::barrier_set()->card_table(), card_count);\n+    _card_scan = new ShenandoahScanRemembered<ShenandoahDirectCardMarkRememberedSet>(rs);\n+\n+    \/\/ Age census structure\n+    _age_census = new ShenandoahAgeCensus();\n+  }\n+\n+  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n+  if (_workers == nullptr) {\n+    vm_exit_during_initialization(\"Failed necessary allocation.\");\n+  } else {\n+    _workers->initialize_workers();\n+  }\n+\n+  if (ParallelGCThreads > 1) {\n+    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\", ParallelGCThreads);\n+    _safepoint_workers->initialize_workers();\n+  }\n+\n@@ -260,1 +303,1 @@\n-                              align_up(num_committed_regions, _bitmap_regions_per_slice) \/ _bitmap_regions_per_slice;\n+    align_up(num_committed_regions, _bitmap_regions_per_slice) \/ _bitmap_regions_per_slice;\n@@ -267,1 +310,1 @@\n-  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions, _max_workers);\n+  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions);\n@@ -358,0 +401,1 @@\n+  _affiliations = NEW_C_HEAP_ARRAY(uint8_t, _num_regions, mtGC);\n@@ -363,0 +407,1 @@\n+\n@@ -374,0 +419,2 @@\n+\n+      _affiliations[i] = ShenandoahAffiliation::FREE;\n@@ -378,0 +425,1 @@\n+    size_t young_cset_regions, old_cset_regions;\n@@ -379,1 +427,4 @@\n-    _free_set->rebuild();\n+    \/\/ We are initializing free set.  We ignore cset region tallies.\n+    size_t first_old, last_old, num_old;\n+    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+    _free_set->rebuild(young_cset_regions, old_cset_regions);\n@@ -438,1 +489,1 @@\n-  _control_thread = new ShenandoahControlThread();\n+  initialize_controller();\n@@ -440,1 +491,1 @@\n-  ShenandoahInitLogger::print();\n+  print_init_logger();\n@@ -445,1 +496,9 @@\n-void ShenandoahHeap::initialize_mode() {\n+void ShenandoahHeap::initialize_controller() {\n+  _control_thread = new ShenandoahControlThread();\n+}\n+\n+void ShenandoahHeap::print_init_logger() const {\n+  ShenandoahInitLogger::print();\n+}\n+\n+void ShenandoahHeap::initialize_heuristics_generations() {\n@@ -453,0 +512,2 @@\n+    } else if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+      _gc_mode = new ShenandoahGenerationalMode();\n@@ -470,1 +531,0 @@\n-}\n@@ -472,3 +532,9 @@\n-void ShenandoahHeap::initialize_heuristics() {\n-  assert(_gc_mode != nullptr, \"Must be initialized\");\n-  _heuristics = _gc_mode->initialize_heuristics();\n+  \/\/ Max capacity is the maximum _allowed_ capacity. That is, the maximum allowed capacity\n+  \/\/ for old would be total heap - minimum capacity of young. This means the sum of the maximum\n+  \/\/ allowed for old and young could exceed the total heap size. It remains the case that the\n+  \/\/ _actual_ capacity of young + old = total.\n+  _generation_sizer.heap_size_changed(max_capacity());\n+  size_t initial_capacity_young = _generation_sizer.max_young_size();\n+  size_t max_capacity_young = _generation_sizer.max_young_size();\n+  size_t initial_capacity_old = max_capacity() - max_capacity_young;\n+  size_t max_capacity_old = max_capacity() - initial_capacity_young;\n@@ -476,9 +542,7 @@\n-  if (_heuristics->is_diagnostic() && !UnlockDiagnosticVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.\",\n-                    _heuristics->name()));\n-  }\n-  if (_heuristics->is_experimental() && !UnlockExperimentalVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.\",\n-                    _heuristics->name()));\n+  _young_generation = new ShenandoahYoungGeneration(_max_workers, max_capacity_young, initial_capacity_young);\n+  _old_generation = new ShenandoahOldGeneration(_max_workers, max_capacity_old, initial_capacity_old);\n+  _global_generation = new ShenandoahGlobalGeneration(_gc_mode->is_generational(), _max_workers, max_capacity(), max_capacity());\n+  _global_generation->initialize_heuristics(_gc_mode);\n+  if (mode()->is_generational()) {\n+    _young_generation->initialize_heuristics(_gc_mode);\n+    _old_generation->initialize_heuristics(_gc_mode);\n@@ -486,0 +550,1 @@\n+  _evac_tracker = new ShenandoahEvacuationTracker(mode()->is_generational());\n@@ -495,0 +560,1 @@\n+  _gc_generation(nullptr),\n@@ -496,1 +562,0 @@\n-  _used(0),\n@@ -498,2 +563,1 @@\n-  _bytes_allocated_since_gc_start(0),\n-  _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),\n+  _max_workers(MAX3(ConcGCThreads, ParallelGCThreads, 1U)),\n@@ -505,1 +569,1 @@\n-  _update_refs_iterator(this),\n+  _affiliations(nullptr),\n@@ -508,0 +572,5 @@\n+  _age_census(nullptr),\n+  _cancel_requested_time(0),\n+  _young_generation(nullptr),\n+  _global_generation(nullptr),\n+  _old_generation(nullptr),\n@@ -510,2 +579,0 @@\n-  _gc_mode(nullptr),\n-  _heuristics(nullptr),\n@@ -516,0 +583,3 @@\n+  _evac_tracker(nullptr),\n+  _mmu_tracker(),\n+  _generation_sizer(),\n@@ -522,1 +592,0 @@\n-  _ref_processor(new ShenandoahReferenceProcessor(MAX2(_max_workers, 1U))),\n@@ -530,1 +599,2 @@\n-  _collection_set(nullptr)\n+  _collection_set(nullptr),\n+  _card_scan(nullptr)\n@@ -532,17 +602,0 @@\n-  \/\/ Initialize GC mode early, so we can adjust barrier support\n-  initialize_mode();\n-  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));\n-\n-  _max_workers = MAX2(_max_workers, 1U);\n-  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n-  if (_workers == nullptr) {\n-    vm_exit_during_initialization(\"Failed necessary allocation.\");\n-  } else {\n-    _workers->initialize_workers();\n-  }\n-\n-  if (ParallelGCThreads > 1) {\n-    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\",\n-                                                ParallelGCThreads);\n-    _safepoint_workers->initialize_workers();\n-  }\n@@ -555,29 +608,0 @@\n-class ShenandoahResetBitmapTask : public WorkerTask {\n-private:\n-  ShenandoahRegionIterator _regions;\n-\n-public:\n-  ShenandoahResetBitmapTask() :\n-    WorkerTask(\"Shenandoah Reset Bitmap\") {}\n-\n-  void work(uint worker_id) {\n-    ShenandoahHeapRegion* region = _regions.next();\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahMarkingContext* const ctx = heap->marking_context();\n-    while (region != nullptr) {\n-      if (heap->is_bitmap_slice_committed(region)) {\n-        ctx->clear_bitmap(region);\n-      }\n-      region = _regions.next();\n-    }\n-  }\n-};\n-\n-void ShenandoahHeap::reset_mark_bitmap() {\n-  assert_gc_workers(_workers->active_workers());\n-  mark_incomplete_marking_context();\n-\n-  ShenandoahResetBitmapTask task;\n-  _workers->run_task(&task);\n-}\n-\n@@ -598,1 +622,2 @@\n-  if (is_concurrent_mark_in_progress())        st->print(\"marking, \");\n+  if (is_concurrent_old_mark_in_progress())    st->print(\"old marking, \");\n+  if (is_concurrent_young_mark_in_progress())  st->print(\"young marking, \");\n@@ -649,0 +674,2 @@\n+  _mmu_tracker.initialize();\n+\n@@ -662,2 +689,0 @@\n-  _heuristics->initialize();\n-\n@@ -667,0 +692,4 @@\n+ShenandoahHeuristics* ShenandoahHeap::heuristics() {\n+  return _global_generation->heuristics();\n+}\n+\n@@ -668,1 +697,1 @@\n-  return Atomic::load(&_used);\n+  return global_generation()->used();\n@@ -675,4 +704,0 @@\n-size_t ShenandoahHeap::available() const {\n-  return free_set()->available();\n-}\n-\n@@ -689,2 +714,43 @@\n-void ShenandoahHeap::increase_used(size_t bytes) {\n-  Atomic::add(&_used, bytes, memory_order_relaxed);\n+\/\/ For tracking usage based on allocations, it should be the case that:\n+\/\/ * The sum of regions::used == heap::used\n+\/\/ * The sum of a generation's regions::used == generation::used\n+\/\/ * The sum of a generation's humongous regions::free == generation::humongous_waste\n+\/\/ These invariants are checked by the verifier on GC safepoints.\n+\/\/\n+\/\/ Additional notes:\n+\/\/ * When a mutator's allocation request causes a region to be retired, the\n+\/\/   free memory left in that region is considered waste. It does not contribute\n+\/\/   to the usage, but it _does_ contribute to allocation rate.\n+\/\/ * The bottom of a PLAB must be aligned on card size. In some cases this will\n+\/\/   require padding in front of the PLAB (a filler object). Because this padding\n+\/\/   is included in the region's used memory we include the padding in the usage\n+\/\/   accounting as waste.\n+\/\/ * Mutator allocations are used to compute an allocation rate. They are also\n+\/\/   sent to the Pacer for those purposes.\n+\/\/ * There are three sources of waste:\n+\/\/  1. The padding used to align a PLAB on card size\n+\/\/  2. Region's free is less than minimum TLAB size and is retired\n+\/\/  3. The unused portion of memory in the last region of a humongous object\n+void ShenandoahHeap::increase_used(const ShenandoahAllocRequest& req) {\n+  size_t actual_bytes = req.actual_size() * HeapWordSize;\n+  size_t wasted_bytes = req.waste() * HeapWordSize;\n+  ShenandoahGeneration* generation = generation_for(req.affiliation());\n+\n+  if (req.is_gc_alloc()) {\n+    assert(wasted_bytes == 0 || req.type() == ShenandoahAllocRequest::_alloc_plab, \"Only PLABs have waste\");\n+    increase_used(generation, actual_bytes + wasted_bytes);\n+  } else {\n+    assert(req.is_mutator_alloc(), \"Expected mutator alloc here\");\n+    \/\/ padding and actual size both count towards allocation counter\n+    generation->increase_allocated(actual_bytes + wasted_bytes);\n+\n+    \/\/ only actual size counts toward usage for mutator allocations\n+    increase_used(generation, actual_bytes);\n+\n+    \/\/ notify pacer of both actual size and waste\n+    notify_mutator_alloc_words(req.actual_size(), req.waste());\n+\n+    if (wasted_bytes > 0 && req.actual_size() > ShenandoahHeapRegion::humongous_threshold_words()) {\n+      increase_humongous_waste(generation,wasted_bytes);\n+    }\n+  }\n@@ -693,2 +759,5 @@\n-void ShenandoahHeap::set_used(size_t bytes) {\n-  Atomic::store(&_used, bytes);\n+void ShenandoahHeap::increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_humongous_waste(bytes);\n+  }\n@@ -697,3 +766,5 @@\n-void ShenandoahHeap::decrease_used(size_t bytes) {\n-  assert(used() >= bytes, \"never decrease heap size by more than we've left\");\n-  Atomic::sub(&_used, bytes, memory_order_relaxed);\n+void ShenandoahHeap::decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_humongous_waste(bytes);\n+  }\n@@ -702,2 +773,5 @@\n-void ShenandoahHeap::increase_allocated(size_t bytes) {\n-  Atomic::add(&_bytes_allocated_since_gc_start, bytes, memory_order_relaxed);\n+void ShenandoahHeap::increase_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_used(bytes);\n+  }\n@@ -706,4 +780,4 @@\n-void ShenandoahHeap::notify_mutator_alloc_words(size_t words, bool waste) {\n-  size_t bytes = words * HeapWordSize;\n-  if (!waste) {\n-    increase_used(bytes);\n+void ShenandoahHeap::decrease_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_used(bytes);\n@@ -711,1 +785,3 @@\n-  increase_allocated(bytes);\n+}\n+\n+void ShenandoahHeap::notify_mutator_alloc_words(size_t words, size_t waste) {\n@@ -714,2 +790,2 @@\n-    if (waste) {\n-      pacer()->claim_for_alloc(words, true);\n+    if (waste > 0) {\n+      pacer()->claim_for_alloc(waste, true);\n@@ -751,6 +827,0 @@\n-bool ShenandoahHeap::is_in(const void* p) const {\n-  HeapWord* heap_base = (HeapWord*) base();\n-  HeapWord* last_region_end = heap_base + ShenandoahHeapRegion::region_size_words() * num_regions();\n-  return p >= heap_base && p < last_region_end;\n-}\n-\n@@ -836,3 +906,1 @@\n-\n-  \/\/ This is called from allocation path, and thus should be fast.\n-  _heap_changed.try_set();\n+  _heap_changed.set();\n@@ -855,0 +923,8 @@\n+\n+  \/\/ Limit growth of GCLABs to ShenandoahMaxEvacLABRatio * the minimum size.  This enables more equitable distribution of\n+  \/\/ available evacuation buidget between the many threads that are coordinating in the evacuation effort.\n+  if (ShenandoahMaxEvacLABRatio > 0) {\n+    log_debug(gc, free)(\"Allocate new gclab: \" SIZE_FORMAT \", \" SIZE_FORMAT, new_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+    new_size = MIN2(new_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+  }\n+\n@@ -866,0 +942,1 @@\n+    log_debug(gc, free)(\"New gclab size (\" SIZE_FORMAT \") is too small for \" SIZE_FORMAT, new_size, size);\n@@ -895,0 +972,1 @@\n+\/\/ Called from stubs in JIT code or interpreter\n@@ -958,1 +1036,1 @@\n-      control_thread()->handle_alloc_failure(req);\n+      control_thread()->handle_alloc_failure(req, true);\n@@ -978,0 +1056,8 @@\n+  if (result == nullptr) {\n+    req.set_actual_size(0);\n+  }\n+\n+  \/\/ This is called regardless of the outcome of the allocation to account\n+  \/\/ for any waste created by retiring regions with this request.\n+  increase_used(req);\n+\n@@ -987,2 +1073,0 @@\n-      notify_mutator_alloc_words(actual, false);\n-\n@@ -995,2 +1079,0 @@\n-    } else {\n-      increase_used(actual*HeapWordSize);\n@@ -1009,1 +1091,37 @@\n-  return _free_set->allocate(req, in_new_region);\n+\n+  \/\/ Make sure the old generation has room for either evacuations or promotions before trying to allocate.\n+  if (req.is_old() && !old_generation()->can_allocate(req)) {\n+    return nullptr;\n+  }\n+\n+  \/\/ If TLAB request size is greater than available, allocate() will attempt to downsize request to fit within available\n+  \/\/ memory.\n+  HeapWord* result = _free_set->allocate(req, in_new_region);\n+\n+  \/\/ Record the plab configuration for this result and register the object.\n+  if (result != nullptr && req.is_old()) {\n+    old_generation()->configure_plab_for_current_thread(req);\n+    if (req.type() == ShenandoahAllocRequest::_alloc_shared_gc) {\n+      \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+      \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+      \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+      \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+      \/\/\n+      \/\/ objects being \"concurrently\" allocated:\n+      \/\/    [-----a------][-----b-----][--------------c------------------]\n+      \/\/            [---- card table memory range --------------]\n+      \/\/\n+      \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n+      \/\/ wants to set the starts-object, first-start, and last-start attributes of the preceding card region.\n+      \/\/ Allocation of object b wants to set the starts-object, first-start, and last-start attributes of this card region.\n+      \/\/ Allocation of object c also wants to set the starts-object, first-start, and last-start attributes of this\n+      \/\/ card region.\n+      \/\/\n+      \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n+      \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n+      \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+      card_scan()->register_object(result);\n+    }\n+  }\n+\n+  return result;\n@@ -1024,2 +1142,2 @@\n-  if (heuristics()->can_unload_classes()) {\n-    ShenandoahHeuristics* h = heuristics();\n+  ShenandoahHeuristics* h = global_generation()->heuristics();\n+  if (h->can_unload_classes()) {\n@@ -1118,2 +1236,109 @@\n-  ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n-  workers()->run_task(&task);\n+  if (mode()->is_generational()) {\n+    ShenandoahRegionIterator regions;\n+    ShenandoahGenerationalEvacuationTask task(ShenandoahGenerationalHeap::heap(), &regions, concurrent);\n+    workers()->run_task(&task);\n+  } else {\n+    ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n+    workers()->run_task(&task);\n+  }\n+}\n+\n+oop ShenandoahHeap::evacuate_object(oop p, Thread* thread) {\n+  assert(thread == Thread::current(), \"Expected thread parameter to be current thread.\");\n+  if (ShenandoahThreadLocalData::is_oom_during_evac(thread)) {\n+    \/\/ This thread went through the OOM during evac protocol. It is safe to return\n+    \/\/ the forward pointer. It must not attempt to evacuate any other objects.\n+    return ShenandoahBarrierSet::resolve_forwarded(p);\n+  }\n+\n+  assert(ShenandoahThreadLocalData::is_evac_allowed(thread), \"must be enclosed in oom-evac scope\");\n+\n+  ShenandoahHeapRegion* r = heap_region_containing(p);\n+  assert(!r->is_humongous(), \"never evacuate humongous objects\");\n+\n+  ShenandoahAffiliation target_gen = r->affiliation();\n+  return try_evacuate_object(p, thread, r, target_gen);\n+}\n+\n+oop ShenandoahHeap::try_evacuate_object(oop p, Thread* thread, ShenandoahHeapRegion* from_region,\n+                                               ShenandoahAffiliation target_gen) {\n+  assert(target_gen == YOUNG_GENERATION, \"Only expect evacuations to young in this mode\");\n+  assert(from_region->is_young(), \"Only expect evacuations from young in this mode\");\n+  bool alloc_from_lab = true;\n+  HeapWord* copy = nullptr;\n+  size_t size = p->size();\n+\n+#ifdef ASSERT\n+  if (ShenandoahOOMDuringEvacALot &&\n+      (os::random() & 1) == 0) { \/\/ Simulate OOM every ~2nd slow-path call\n+    copy = nullptr;\n+  } else {\n+#endif\n+    if (UseTLAB) {\n+      copy = allocate_from_gclab(thread, size);\n+      if ((copy == nullptr) && (size < ShenandoahThreadLocalData::gclab_size(thread))) {\n+        \/\/ GCLAB allocation failed because we are bumping up against the limit on young evacuation reserve.  Try resetting\n+        \/\/ the desired GCLAB size and retry GCLAB allocation to avoid cascading of shared memory allocations.\n+        \/\/ TODO: is this right? using PLAB::min_size() here for gc lab size?\n+        ShenandoahThreadLocalData::set_gclab_size(thread, PLAB::min_size());\n+        copy = allocate_from_gclab(thread, size);\n+        \/\/ If we still get nullptr, we'll try a shared allocation below.\n+      }\n+    }\n+\n+    if (copy == nullptr) {\n+      \/\/ If we failed to allocate in LAB, we'll try a shared allocation.\n+      ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen);\n+      copy = allocate_memory(req);\n+      alloc_from_lab = false;\n+    }\n+#ifdef ASSERT\n+  }\n+#endif\n+\n+  if (copy == nullptr) {\n+    control_thread()->handle_alloc_failure_evac(size);\n+\n+    _oom_evac_handler.handle_out_of_memory_during_evacuation();\n+\n+    return ShenandoahBarrierSet::resolve_forwarded(p);\n+  }\n+\n+  \/\/ Copy the object:\n+  _evac_tracker->begin_evacuation(thread, size * HeapWordSize);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, size);\n+\n+  oop copy_val = cast_to_oop(copy);\n+\n+  \/\/ Try to install the new forwarding pointer.\n+  ContinuationGCSupport::relativize_stack_chunk(copy_val);\n+\n+  oop result = ShenandoahForwarding::try_update_forwardee(p, copy_val);\n+  if (result == copy_val) {\n+    \/\/ Successfully evacuated. Our copy is now the public one!\n+    _evac_tracker->end_evacuation(thread, size * HeapWordSize);\n+    shenandoah_assert_correct(nullptr, copy_val);\n+    return copy_val;\n+  }  else {\n+    \/\/ Failed to evacuate. We need to deal with the object that is left behind. Since this\n+    \/\/ new allocation is certainly after TAMS, it will be considered live in the next cycle.\n+    \/\/ But if it happens to contain references to evacuated regions, those references would\n+    \/\/ not get updated for this stale copy during this cycle, and we will crash while scanning\n+    \/\/ it the next cycle.\n+    if (alloc_from_lab) {\n+      \/\/ For LAB allocations, it is enough to rollback the allocation ptr. Either the next\n+      \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n+      \/\/ do this.\n+      ShenandoahThreadLocalData::gclab(thread)->undo_allocation(copy, size);\n+    } else {\n+      \/\/ For non-LAB allocations, we have no way to retract the allocation, and\n+      \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n+      \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n+      assert(size >= ShenandoahHeap::min_fill_size(), \"previously allocated object known to be larger than min_size\");\n+      fill_with_object(copy, size);\n+      shenandoah_assert_correct(nullptr, copy_val);\n+      \/\/ For non-LAB allocations, the object has already been registered\n+    }\n+    shenandoah_assert_correct(nullptr, result);\n+    return result;\n+  }\n@@ -1226,1 +1451,1 @@\n-void ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n+size_t ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n@@ -1246,0 +1471,1 @@\n+  return required_regions;\n@@ -1255,0 +1481,6 @@\n+\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+      assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+      assert(plab->words_remaining() == 0, \"PLAB should not need retirement\");\n+    }\n@@ -1270,0 +1502,13 @@\n+\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+      assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+\n+      \/\/ There are two reasons to retire all plabs between old-gen evacuation passes.\n+      \/\/  1. We need to make the plab memory parsable by remembered-set scanning.\n+      \/\/  2. We need to establish a trustworthy UpdateWaterMark value within each old-gen heap region\n+      ShenandoahGenerationalHeap::heap()->retire_plab(plab, thread);\n+      if (_resize && ShenandoahThreadLocalData::plab_size(thread) > 0) {\n+        ShenandoahThreadLocalData::set_plab_size(thread, 0);\n+      }\n+    }\n@@ -1398,0 +1643,4 @@\n+    ls.cr();\n+\n+    evac_tracker()->print_global_on(&ls);\n+\n@@ -1403,0 +1652,19 @@\n+void ShenandoahHeap::on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation) {\n+  shenandoah_policy()->record_collection_cause(cause);\n+\n+  set_gc_cause(cause);\n+  set_gc_generation(generation);\n+\n+  generation->heuristics()->record_cycle_start();\n+}\n+\n+void ShenandoahHeap::on_cycle_end(ShenandoahGeneration* generation) {\n+  generation->heuristics()->record_cycle_end();\n+  if (mode()->is_generational() && generation->is_global()) {\n+    \/\/ If we just completed a GLOBAL GC, claim credit for completion of young-gen and old-gen GC as well\n+    young_generation()->heuristics()->record_cycle_end();\n+    old_generation()->heuristics()->record_cycle_end();\n+  }\n+  set_gc_cause(GCCause::_no_gc);\n+}\n+\n@@ -1739,99 +2007,0 @@\n-class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n-      \/\/ anyway to capture any updates that happened since now.\n-      r->clear_live_data();\n-      _ctx->capture_top_at_mark_start(r);\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_gc() {\n-  reset_mark_bitmap();\n-\n-  ShenandoahResetUpdateRegionStateClosure cl;\n-  parallel_heap_region_iterate(&cl);\n-}\n-\n-class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalMarkUpdateRegionStateClosure() :\n-    _ctx(ShenandoahHeap::heap()->complete_marking_context()), _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n-      \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n-      HeapWord *tams = _ctx->top_at_mark_start(r);\n-      HeapWord *top = r->top();\n-      if (top > tams) {\n-        r->increase_live_data_alloc_words(pointer_delta(top, tams));\n-      }\n-\n-      \/\/ We are about to select the collection set, make sure it knows about\n-      \/\/ current pinning status. Also, this allows trashing more regions that\n-      \/\/ now have their pinning status dropped.\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n-      }\n-\n-      \/\/ Remember limit for updating refs. It's guaranteed that we get no\n-      \/\/ from-space-refs written from here on.\n-      r->set_update_watermark_at_safepoint(r->top());\n-    } else {\n-      assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_regions_and_collection_set(bool concurrent) {\n-  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states :\n-                                         ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n-    ShenandoahFinalMarkUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n-\n-    assert_pinned_region_status();\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset :\n-                                         ShenandoahPhaseTimings::degen_gc_choose_cset);\n-    ShenandoahHeapLocker locker(lock());\n-    _collection_set->clear();\n-    heuristics()->choose_collection_set(_collection_set);\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset :\n-                                         ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n-  }\n-}\n-\n@@ -1840,0 +2009,3 @@\n+  if (mode()->is_generational()) {\n+    old_generation()->set_parseable(false);\n+  }\n@@ -1848,1 +2020,1 @@\n-  ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n+  active_generation()->ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n@@ -1863,2 +2035,0 @@\n-\n-  _update_refs_iterator.reset();\n@@ -1884,4 +2054,52 @@\n-void ShenandoahHeap::set_concurrent_mark_in_progress(bool in_progress) {\n-  assert(!has_forwarded_objects(), \"Not expected before\/after mark phase\");\n-  set_gc_state(MARKING, in_progress);\n-  ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(in_progress, !in_progress);\n+void ShenandoahHeap::set_concurrent_young_mark_in_progress(bool in_progress) {\n+  uint mask;\n+  assert(!has_forwarded_objects(), \"Young marking is not concurrent with evacuation\");\n+  if (!in_progress && is_concurrent_old_mark_in_progress()) {\n+    assert(mode()->is_generational(), \"Only generational GC has old marking\");\n+    assert(_gc_state.is_set(MARKING), \"concurrent_old_marking_in_progress implies MARKING\");\n+    \/\/ If old-marking is in progress when we turn off YOUNG_MARKING, leave MARKING (and OLD_MARKING) on\n+    mask = YOUNG_MARKING;\n+  } else {\n+    mask = MARKING | YOUNG_MARKING;\n+  }\n+  set_gc_state(mask, in_progress);\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::set_concurrent_old_mark_in_progress(bool in_progress) {\n+#ifdef ASSERT\n+  \/\/ has_forwarded_objects() iff UPDATEREFS or EVACUATION\n+  bool has_forwarded = has_forwarded_objects();\n+  bool updating_or_evacuating = _gc_state.is_set(UPDATEREFS | EVACUATION);\n+  bool evacuating = _gc_state.is_set(EVACUATION);\n+  assert ((has_forwarded == updating_or_evacuating) || (evacuating && !has_forwarded && collection_set()->is_empty()),\n+          \"Updating or evacuating iff has forwarded objects, or if evacuation phase is promoting in place without forwarding\");\n+#endif\n+  if (!in_progress && is_concurrent_young_mark_in_progress()) {\n+    \/\/ If young-marking is in progress when we turn off OLD_MARKING, leave MARKING (and YOUNG_MARKING) on\n+    assert(_gc_state.is_set(MARKING), \"concurrent_young_marking_in_progress implies MARKING\");\n+    set_gc_state(OLD_MARKING, in_progress);\n+  } else {\n+    set_gc_state(MARKING | OLD_MARKING, in_progress);\n+  }\n+  manage_satb_barrier(in_progress);\n+}\n+\n+bool ShenandoahHeap::is_prepare_for_old_mark_in_progress() const {\n+  return old_generation()->is_preparing_for_mark();\n+}\n+\n+void ShenandoahHeap::manage_satb_barrier(bool active) {\n+  if (is_concurrent_mark_in_progress()) {\n+    \/\/ Ignore request to deactivate barrier while concurrent mark is in progress.\n+    \/\/ Do not attempt to re-activate the barrier if it is already active.\n+    if (active && !ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  } else {\n+    \/\/ No concurrent marking is in progress so honor request to deactivate,\n+    \/\/ but only if the barrier is already active.\n+    if (!active && ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  }\n@@ -1920,0 +2138,8 @@\n+void ShenandoahHeap::cancel_concurrent_mark() {\n+  _young_generation->cancel_marking();\n+  _old_generation->cancel_marking();\n+  _global_generation->cancel_marking();\n+\n+  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+}\n+\n@@ -1925,0 +2151,1 @@\n+    _cancel_requested_time = os::elapsedTime();\n@@ -2048,4 +2275,0 @@\n-size_t ShenandoahHeap::bytes_allocated_since_gc_start() const {\n-  return Atomic::load(&_bytes_allocated_since_gc_start);\n-}\n-\n@@ -2053,1 +2276,6 @@\n-  Atomic::store(&_bytes_allocated_since_gc_start, (size_t)0);\n+  if (mode()->is_generational()) {\n+    young_generation()->reset_bytes_allocated_since_gc_start();\n+    old_generation()->reset_bytes_allocated_since_gc_start();\n+  }\n+\n+  global_generation()->reset_bytes_allocated_since_gc_start();\n@@ -2117,2 +2345,4 @@\n-    assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n-           \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    if (active_generation()->contains(r)) {\n+      assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n+             \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    }\n@@ -2173,1 +2403,1 @@\n-  ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n+  explicit ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n@@ -2193,0 +2423,12 @@\n+<<<<<<< HEAD\n+=======\n+    if (CONCURRENT && (worker_id == 0)) {\n+      \/\/ We ask the first worker to replenish the Mutator free set by moving regions previously reserved to hold the\n+      \/\/ results of evacuation.  These reserves are no longer necessary because evacuation has completed.\n+      size_t cset_regions = _heap->collection_set()->count();\n+      \/\/ We cannot transfer any more regions than will be reclaimed when the existing collection set is recycled, because\n+      \/\/ we need the reclaimed collection set regions to replenish the collector reserves\n+      _heap->free_set()->move_collector_sets_to_mutator(cset_regions);\n+    }\n+    \/\/ If !CONCURRENT, there's no value in expanding Mutator free set\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -2205,1 +2447,0 @@\n-    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n@@ -2211,3 +2452,3 @@\n-      }\n-      if (ShenandoahPacing) {\n-        _heap->pacer()->report_updaterefs(pointer_delta(update_watermark, r->bottom()));\n+        if (ShenandoahPacing) {\n+          _heap->pacer()->report_updaterefs(pointer_delta(update_watermark, r->bottom()));\n+        }\n@@ -2225,1 +2466,1 @@\n-\n+  ShenandoahRegionIterator update_refs_iterator(this);\n@@ -2227,1 +2468,1 @@\n-    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<true> task(&update_refs_iterator);\n@@ -2230,1 +2471,1 @@\n-    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<false> task(&update_refs_iterator);\n@@ -2233,0 +2474,1 @@\n+  assert(cancelled_gc() || !update_refs_iterator.has_next(), \"Should have finished update references\");\n@@ -2235,1 +2477,0 @@\n-\n@@ -2238,0 +2479,1 @@\n+  ShenandoahMarkingContext* _ctx;\n@@ -2239,0 +2481,1 @@\n+  bool _is_generational;\n@@ -2241,1 +2484,25 @@\n-  ShenandoahFinalUpdateRefsUpdateRegionStateClosure() : _lock(ShenandoahHeap::heap()->lock()) {}\n+  ShenandoahFinalUpdateRefsUpdateRegionStateClosure(ShenandoahMarkingContext* ctx) :\n+    _ctx(ctx), _lock(ShenandoahHeap::heap()->lock()),\n+    _is_generational(ShenandoahHeap::heap()->mode()->is_generational()) { }\n+\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n+\n+    \/\/ Maintenance of region age must follow evacuation in order to account for evacuation allocations within survivor\n+    \/\/ regions.  We consult region age during the subsequent evacuation to determine whether certain objects need to\n+    \/\/ be promoted.\n+    if (_is_generational && r->is_young() && r->is_active()) {\n+      HeapWord *tams = _ctx->top_at_mark_start(r);\n+      HeapWord *top = r->top();\n+\n+      \/\/ Allocations move the watermark when top moves.  However, compacting\n+      \/\/ objects will sometimes lower top beneath the watermark, after which,\n+      \/\/ attempts to read the watermark will assert out (watermark should not be\n+      \/\/ higher than top).\n+      if (top > tams) {\n+        \/\/ There have been allocations in this region since the start of the cycle.\n+        \/\/ Any objects new to this region must not assimilate elevated age.\n+        r->reset_age();\n+      } else if (ShenandoahGenerationalHeap::heap()->is_aging_cycle()) {\n+        r->increment_age();\n+      }\n+    }\n@@ -2243,1 +2510,0 @@\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n@@ -2246,1 +2512,0 @@\n-\n@@ -2262,1 +2527,1 @@\n-  bool is_thread_safe() { return true; }\n+  bool is_thread_safe() override { return true; }\n@@ -2273,1 +2538,1 @@\n-    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl;\n+    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl (active_generation()->complete_marking_context());\n@@ -2288,6 +2553,42 @@\n-  {\n-    ShenandoahGCPhase phase(concurrent ?\n-                            ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n-                            ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n+  ShenandoahGCPhase phase(concurrent ?\n+                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n+                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+  ShenandoahHeapLocker locker(lock());\n+  size_t young_cset_regions, old_cset_regions;\n+  size_t first_old_region, last_old_region, old_region_count;\n+  _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+  \/\/ If there are no old regions, first_old_region will be greater than last_old_region\n+  assert((first_old_region > last_old_region) ||\n+         ((last_old_region + 1 - first_old_region >= old_region_count) &&\n+          get_region(first_old_region)->is_old() && get_region(last_old_region)->is_old()),\n+         \"sanity: old_region_count: \" SIZE_FORMAT \", first_old_region: \" SIZE_FORMAT \", last_old_region: \" SIZE_FORMAT,\n+         old_region_count, first_old_region, last_old_region);\n+\n+  if (mode()->is_generational()) {\n+#ifdef ASSERT\n+    if (ShenandoahVerify) {\n+      verifier()->verify_before_rebuilding_free_set();\n+    }\n+#endif\n+\n+    \/\/ The computation of bytes_of_allocation_runway_before_gc_trigger is quite conservative so consider all of this\n+    \/\/ available for transfer to old. Note that transfer of humongous regions does not impact available.\n+    size_t allocation_runway = young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions);\n+    ShenandoahGenerationalHeap::heap()->compute_old_generation_balance(allocation_runway, old_cset_regions);\n+\n+    \/\/ Total old_available may have been expanded to hold anticipated promotions.  We trigger if the fragmented available\n+    \/\/ memory represents more than 16 regions worth of data.  Note that fragmentation may increase when we promote regular\n+    \/\/ regions in place when many of these regular regions have an abundant amount of available memory within them.  Fragmentation\n+    \/\/ will decrease as promote-by-copy consumes the available memory within these partially consumed regions.\n+    \/\/\n+    \/\/ We consider old-gen to have excessive fragmentation if more than 12.5% of old-gen is free memory that resides\n+    \/\/ within partially consumed regions of memory.\n+  }\n+  \/\/ Rebuild free set based on adjusted generation sizes.\n+  _free_set->rebuild(young_cset_regions, old_cset_regions);\n+\n+  if (mode()->is_generational()) {\n+    ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+    ShenandoahOldGeneration* old_gen = gen_heap->old_generation();\n+    ShenandoahOldHeuristics* old_heuristics = old_gen->heuristics();\n+    old_heuristics->trigger_maybe(first_old_region, last_old_region, old_region_count, num_regions());\n@@ -2416,1 +2717,1 @@\n-  return _memory_pool->get_memory_usage();\n+  return MemoryUsage(_initial_size, used(), committed(), max_capacity());\n@@ -2481,0 +2782,34 @@\n+\n+ShenandoahGeneration* ShenandoahHeap::generation_for(ShenandoahAffiliation affiliation) const {\n+  if (!mode()->is_generational()) {\n+    return global_generation();\n+  } else if (affiliation == YOUNG_GENERATION) {\n+    return young_generation();\n+  } else if (affiliation == OLD_GENERATION) {\n+    return old_generation();\n+  }\n+\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n+\n+void ShenandoahHeap::log_heap_status(const char* msg) const {\n+  if (mode()->is_generational()) {\n+    young_generation()->log_status(msg);\n+    old_generation()->log_status(msg);\n+  } else {\n+    global_generation()->log_status(msg);\n+  }\n+}\n+\n+void ShenandoahHeap::clear_cards_for(ShenandoahHeapRegion* region) {\n+  if (mode()->is_generational()) {\n+    _card_scan->mark_range_as_empty(region->bottom(), pointer_delta(region->end(), region->bottom()));\n+  }\n+}\n+\n+void ShenandoahHeap::mark_card_as_dirty(void* location) {\n+  if (mode()->is_generational()) {\n+    _card_scan->mark_card_as_dirty((HeapWord*)location);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":589,"deletions":254,"binary":false,"changes":843,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -29,0 +30,1 @@\n+#include \"gc\/shared\/ageTable.hpp\"\n@@ -32,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahAgeCensus.hpp\"\n@@ -35,0 +38,2 @@\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n+#include \"gc\/shenandoah\/shenandoahController.hpp\"\n@@ -37,0 +42,3 @@\n+#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMmuTracker.hpp\"\n@@ -38,0 +46,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.hpp\"\n@@ -47,0 +56,1 @@\n+class PLAB;\n@@ -48,1 +58,1 @@\n-class ShenandoahControlThread;\n+class ShenandoahRegulatorThread;\n@@ -51,0 +61,3 @@\n+class ShenandoahGeneration;\n+class ShenandoahYoungGeneration;\n+class ShenandoahOldGeneration;\n@@ -52,0 +65,2 @@\n+class ShenandoahOldHeuristics;\n+class ShenandoahYoungHeuristics;\n@@ -53,1 +68,0 @@\n-class ShenandoahMode;\n@@ -63,0 +77,1 @@\n+class ShenandoahMode;\n@@ -120,1 +135,1 @@\n-class ShenandoahHeap : public CollectedHeap, public ShenandoahSpaceInfo {\n+class ShenandoahHeap : public CollectedHeap {\n@@ -130,0 +145,1 @@\n+  friend class ShenandoahOldGC;\n@@ -138,0 +154,1 @@\n+  ShenandoahGeneration* _gc_generation;\n@@ -144,0 +161,11 @@\n+  ShenandoahGeneration* active_generation() const {\n+    \/\/ last or latest generation might be a better name here.\n+    return _gc_generation;\n+  }\n+\n+  void set_gc_generation(ShenandoahGeneration* generation) {\n+    _gc_generation = generation;\n+  }\n+\n+  ShenandoahHeuristics* heuristics();\n+\n@@ -155,3 +183,2 @@\n-  void initialize_mode();\n-  void initialize_heuristics();\n-\n+  void initialize_heuristics_generations();\n+  virtual void print_init_logger() const;\n@@ -178,2 +205,3 @@\n-           size_t _initial_size;\n-           size_t _minimum_size;\n+  size_t _initial_size;\n+  size_t _minimum_size;\n+\n@@ -182,1 +210,0 @@\n-  volatile size_t _used;\n@@ -184,1 +211,0 @@\n-  volatile size_t _bytes_allocated_since_gc_start;\n@@ -187,0 +213,2 @@\n+  void increase_used(const ShenandoahAllocRequest& req);\n+\n@@ -188,3 +216,4 @@\n-  void increase_used(size_t bytes);\n-  void decrease_used(size_t bytes);\n-  void set_used(size_t bytes);\n+  void increase_used(ShenandoahGeneration* generation, size_t bytes);\n+  void decrease_used(ShenandoahGeneration* generation, size_t bytes);\n+  void increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes);\n+  void decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes);\n@@ -194,1 +223,0 @@\n-  void increase_allocated(size_t bytes);\n@@ -196,1 +224,0 @@\n-  size_t bytes_allocated_since_gc_start() const override;\n@@ -201,1 +228,1 @@\n-  size_t soft_max_capacity() const override;\n+  size_t soft_max_capacity() const;\n@@ -206,1 +233,0 @@\n-  size_t available()         const override;\n@@ -226,0 +252,2 @@\n+  virtual void initialize_controller();\n+\n@@ -242,1 +270,1 @@\n-  ShenandoahRegionIterator _update_refs_iterator;\n+  uint8_t* _affiliations;       \/\/ Holds array of enum ShenandoahAffiliation, including FREE status in non-generational mode\n@@ -259,0 +287,2 @@\n+  inline ShenandoahMmuTracker* mmu_tracker() { return &_mmu_tracker; };\n+\n@@ -274,0 +304,1 @@\n+    \/\/ For generational mode, it means either young or old marking, or both.\n@@ -284,0 +315,6 @@\n+\n+    \/\/ Young regions are under marking, need SATB barriers.\n+    YOUNG_MARKING_BITPOS = 5,\n+\n+    \/\/ Old regions are under marking, need SATB barriers.\n+    OLD_MARKING_BITPOS = 6\n@@ -293,0 +330,2 @@\n+    YOUNG_MARKING = 1 << YOUNG_MARKING_BITPOS,\n+    OLD_MARKING   = 1 << OLD_MARKING_BITPOS\n@@ -298,2 +337,0 @@\n-\n-  \/\/ tracks if new regions have been allocated or retired since last check\n@@ -311,0 +348,2 @@\n+  ShenandoahAgeCensus* _age_census;    \/\/ Age census used for adapting tenuring threshold in generational mode\n+\n@@ -328,1 +367,2 @@\n-  void set_concurrent_mark_in_progress(bool in_progress);\n+  void set_concurrent_young_mark_in_progress(bool in_progress);\n+  void set_concurrent_old_mark_in_progress(bool in_progress);\n@@ -340,0 +380,1 @@\n+\n@@ -341,0 +382,2 @@\n+  inline bool is_concurrent_young_mark_in_progress() const;\n+  inline bool is_concurrent_old_mark_in_progress() const;\n@@ -351,0 +394,4 @@\n+  bool is_prepare_for_old_mark_in_progress() const;\n+\n+  \/\/ Return the age census object for young gen (in generational mode)\n+  inline ShenandoahAgeCensus* age_census() const;\n@@ -353,0 +400,2 @@\n+  void manage_satb_barrier(bool active);\n+\n@@ -363,0 +412,1 @@\n+  double _cancel_requested_time;\n@@ -364,0 +414,5 @@\n+\n+  \/\/ Returns true if cancel request was successfully communicated.\n+  \/\/ Returns false if some other thread already communicated cancel\n+  \/\/ request.  A true return value does not mean GC has been\n+  \/\/ cancelled, only that the process of cancelling GC has begun.\n@@ -367,1 +422,0 @@\n-\n@@ -371,1 +425,1 @@\n-  inline void clear_cancelled_gc();\n+  inline void clear_cancelled_gc(bool clear_oom_handler = true);\n@@ -373,0 +427,1 @@\n+  void cancel_concurrent_mark();\n@@ -386,3 +441,0 @@\n-  \/\/ Reset bitmap, prepare regions for new GC cycle\n-  void prepare_gc();\n-  void prepare_regions_and_collection_set(bool concurrent);\n@@ -398,1 +450,1 @@\n-  void update_heap_references(bool concurrent);\n+  virtual void update_heap_references(bool concurrent);\n@@ -401,1 +453,0 @@\n-  void rebuild_free_set(bool concurrent);\n@@ -406,0 +457,1 @@\n+  void rebuild_free_set(bool concurrent);\n@@ -413,1 +465,8 @@\n-  ShenandoahControlThread*   _control_thread;\n+  ShenandoahYoungGeneration* _young_generation;\n+  ShenandoahGeneration*      _global_generation;\n+  ShenandoahOldGeneration*   _old_generation;\n+\n+protected:\n+  ShenandoahController*  _control_thread;\n+\n+private:\n@@ -416,1 +475,0 @@\n-  ShenandoahHeuristics*      _heuristics;\n@@ -421,3 +479,4 @@\n-  ShenandoahPhaseTimings*    _phase_timings;\n-\n-  ShenandoahControlThread*   control_thread()          { return _control_thread;    }\n+  ShenandoahPhaseTimings*       _phase_timings;\n+  ShenandoahEvacuationTracker*  _evac_tracker;\n+  ShenandoahMmuTracker          _mmu_tracker;\n+  ShenandoahGenerationSizer     _generation_sizer;\n@@ -426,0 +485,8 @@\n+  ShenandoahController*   control_thread() { return _control_thread; }\n+\n+  ShenandoahYoungGeneration* young_generation()  const { return _young_generation;  }\n+  ShenandoahGeneration*      global_generation() const { return _global_generation; }\n+  ShenandoahOldGeneration*   old_generation()    const { return _old_generation;    }\n+  ShenandoahGeneration*      generation_for(ShenandoahAffiliation affiliation) const;\n+  const ShenandoahGenerationSizer* generation_sizer()  const { return &_generation_sizer;  }\n+\n@@ -428,1 +495,0 @@\n-  ShenandoahHeuristics*      heuristics()        const { return _heuristics;        }\n@@ -432,1 +498,7 @@\n-  ShenandoahPhaseTimings*    phase_timings()     const { return _phase_timings;     }\n+  ShenandoahPhaseTimings*      phase_timings()   const { return _phase_timings;     }\n+  ShenandoahEvacuationTracker* evac_tracker()    const { return _evac_tracker;      }\n+\n+  ShenandoahEvacOOMHandler* oom_evac_handler() { return &_oom_evac_handler; }\n+\n+  void on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation);\n+  void on_cycle_end(ShenandoahGeneration* generation);\n@@ -447,1 +519,1 @@\n-  ShenandoahMonitoringSupport* monitoring_support()          { return _monitoring_support;    }\n+  ShenandoahMonitoringSupport* monitoring_support() const    { return _monitoring_support;    }\n@@ -457,8 +529,0 @@\n-\/\/ ---------- Reference processing\n-\/\/\n-private:\n-  ShenandoahReferenceProcessor* const _ref_processor;\n-\n-public:\n-  ShenandoahReferenceProcessor* ref_processor() { return _ref_processor; }\n-\n@@ -483,0 +547,3 @@\n+  inline void assert_lock_for_affiliation(ShenandoahAffiliation orig_affiliation,\n+                                          ShenandoahAffiliation new_affiliation);\n+\n@@ -496,1 +563,11 @@\n-  bool is_in(const void* p) const override;\n+  inline bool is_in(const void* p) const override;\n+\n+  inline bool is_in_active_generation(oop obj) const;\n+  inline bool is_in_young(const void* p) const;\n+  inline bool is_in_old(const void* p) const;\n+  inline bool is_old(oop pobj) const;\n+\n+  inline ShenandoahAffiliation region_affiliation(const ShenandoahHeapRegion* r);\n+  inline void set_affiliation(ShenandoahHeapRegion* r, ShenandoahAffiliation new_affiliation);\n+\n+  inline ShenandoahAffiliation region_affiliation(size_t index);\n@@ -544,0 +621,3 @@\n+protected:\n+  inline HeapWord* allocate_from_gclab(Thread* thread, size_t size);\n+\n@@ -546,1 +626,0 @@\n-  inline HeapWord* allocate_from_gclab(Thread* thread, size_t size);\n@@ -557,1 +636,1 @@\n-  void notify_mutator_alloc_words(size_t words, bool waste);\n+  void notify_mutator_alloc_words(size_t words, size_t waste);\n@@ -595,2 +674,0 @@\n-  inline void mark_complete_marking_context();\n-  inline void mark_incomplete_marking_context();\n@@ -607,2 +684,0 @@\n-  void reset_mark_bitmap();\n-\n@@ -629,0 +704,1 @@\n+  oop try_evacuate_object(oop src, Thread* thread, ShenandoahHeapRegion* from_region, ShenandoahAffiliation target_gen);\n@@ -630,0 +706,1 @@\n+\n@@ -640,1 +717,1 @@\n-  \/\/ Evacuates object src. Returns the evacuated object, either evacuated\n+  \/\/ Evacuates or promotes object src. Returns the evacuated object, either evacuated\n@@ -642,0 +719,1 @@\n+<<<<<<< HEAD\n@@ -643,0 +721,3 @@\n+=======\n+  virtual oop evacuate_object(oop src, Thread* thread);\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -648,0 +729,10 @@\n+\/\/ ---------- Generational support\n+\/\/\n+private:\n+  RememberedScanner* _card_scan;\n+\n+public:\n+  inline RememberedScanner* card_scan() { return _card_scan; }\n+  void clear_cards_for(ShenandoahHeapRegion* region);\n+  void mark_card_as_dirty(void* location);\n+\n@@ -669,1 +760,10 @@\n-  void trash_humongous_region_at(ShenandoahHeapRegion *r);\n+  size_t trash_humongous_region_at(ShenandoahHeapRegion *r);\n+\n+  static inline void increase_object_age(oop obj, uint additional_age);\n+\n+  \/\/ Return the object's age, or a sentinel value when the age can't\n+  \/\/ necessarily be determined because of concurrent locking by the\n+  \/\/ mutator\n+  static inline uint get_object_age(oop obj);\n+\n+  void log_heap_status(const char *msg) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":153,"deletions":53,"binary":false,"changes":206,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -43,1 +44,1 @@\n-#include \"gc\/shenandoah\/shenandoahControlThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -46,0 +47,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -51,0 +53,1 @@\n+#include \"runtime\/objectMonitor.inline.hpp\"\n@@ -267,1 +270,1 @@\n-inline void ShenandoahHeap::clear_cancelled_gc() {\n+inline void ShenandoahHeap::clear_cancelled_gc(bool clear_oom_handler) {\n@@ -269,1 +272,9 @@\n-  _oom_evac_handler.clear();\n+  if (_cancel_requested_time > 0) {\n+    double cancel_time = os::elapsedTime() - _cancel_requested_time;\n+    log_info(gc)(\"GC cancellation took %.3fs\", cancel_time);\n+    _cancel_requested_time = 0;\n+  }\n+\n+  if (clear_oom_handler) {\n+    _oom_evac_handler.clear();\n+  }\n@@ -286,1 +297,0 @@\n-  \/\/ Otherwise...\n@@ -290,0 +300,145 @@\n+<<<<<<< HEAD\n+=======\n+inline ShenandoahAgeCensus* ShenandoahHeap::age_census() const {\n+  assert(mode()->is_generational(), \"Only in generational mode\");\n+  assert(_age_census != nullptr, \"Error: not initialized\");\n+  return _age_census;\n+}\n+\n+void ShenandoahHeap::increase_object_age(oop obj, uint additional_age) {\n+  \/\/ This operates on new copy of an object. This means that the object's mark-word\n+  \/\/ is thread-local and therefore safe to access. However, when the mark is\n+  \/\/ displaced (i.e. stack-locked or monitor-locked), then it must be considered\n+  \/\/ a shared memory location. It can be accessed by other threads.\n+  \/\/ In particular, a competing evacuating thread can succeed to install its copy\n+  \/\/ as the forwardee and continue to unlock the object, at which point 'our'\n+  \/\/ write to the foreign stack-location would potentially over-write random\n+  \/\/ information on that stack. Writing to a monitor is less problematic,\n+  \/\/ but still not safe: while the ObjectMonitor would not randomly disappear,\n+  \/\/ the other thread would also write to the same displaced header location,\n+  \/\/ possibly leading to increase the age twice.\n+  \/\/ For all these reasons, we take the conservative approach and not attempt\n+  \/\/ to increase the age when the header is displaced.\n+  markWord w = obj->mark();\n+  \/\/ The mark-word has been copied from the original object. It can not be\n+  \/\/ inflating, because inflation can not be interrupted by a safepoint,\n+  \/\/ and after a safepoint, a Java thread would first have to successfully\n+  \/\/ evacuate the object before it could inflate the monitor.\n+  assert(!w.is_being_inflated() || LockingMode == LM_LIGHTWEIGHT, \"must not inflate monitor before evacuation of object succeeds\");\n+  \/\/ It is possible that we have copied the object after another thread has\n+  \/\/ already successfully completed evacuation. While harmless (we would never\n+  \/\/ publish our copy), don't even attempt to modify the age when that\n+  \/\/ happens.\n+  if (!w.has_displaced_mark_helper() && !w.is_marked()) {\n+    w = w.set_age(MIN2(markWord::max_age, w.age() + additional_age));\n+    obj->set_mark(w);\n+  }\n+}\n+\n+\/\/ Return the object's age, or a sentinel value when the age can't\n+\/\/ necessarily be determined because of concurrent locking by the\n+\/\/ mutator\n+uint ShenandoahHeap::get_object_age(oop obj) {\n+  \/\/ This is impossible to do unless we \"freeze\" ABA-type oscillations\n+  \/\/ With Lilliput, we can do this more easily.\n+  markWord w = obj->mark();\n+  assert(!w.is_marked(), \"must not be forwarded\");\n+  if (w.has_monitor()) {\n+    w = w.monitor()->header();\n+  } else if (w.is_being_inflated() || w.has_displaced_mark_helper()) {\n+    \/\/ Informs caller that we aren't able to determine the age\n+    return markWord::max_age + 1; \/\/ sentinel\n+  }\n+  assert(w.age() <= markWord::max_age, \"Impossible!\");\n+  return w.age();\n+}\n+\n+bool ShenandoahHeap::is_in(const void* p) const {\n+  HeapWord* heap_base = (HeapWord*) base();\n+  HeapWord* last_region_end = heap_base + ShenandoahHeapRegion::region_size_words() * num_regions();\n+  return p >= heap_base && p < last_region_end;\n+}\n+\n+inline bool ShenandoahHeap::is_in_active_generation(oop obj) const {\n+  if (!mode()->is_generational()) {\n+    \/\/ everything is the same single generation\n+    return true;\n+  }\n+\n+  if (active_generation() == nullptr) {\n+    \/\/ no collection is happening, only expect this to be called\n+    \/\/ when concurrent processing is active, but that could change\n+    return false;\n+  }\n+\n+  assert(is_in(obj), \"only check if is in active generation for objects (\" PTR_FORMAT \") in heap\", p2i(obj));\n+  assert((active_generation() == (ShenandoahGeneration*) old_generation()) ||\n+         (active_generation() == (ShenandoahGeneration*) young_generation()) ||\n+         (active_generation() == global_generation()), \"Active generation must be old, young, or global\");\n+\n+  size_t index = heap_region_containing(obj)->index();\n+  switch (_affiliations[index]) {\n+  case ShenandoahAffiliation::FREE:\n+    \/\/ Free regions are in Old, Young, Global\n+    return true;\n+  case ShenandoahAffiliation::YOUNG_GENERATION:\n+    \/\/ Young regions are in young_generation and global_generation, not in old_generation\n+    return (active_generation() != (ShenandoahGeneration*) old_generation());\n+  case ShenandoahAffiliation::OLD_GENERATION:\n+    \/\/ Old regions are in old_generation and global_generation, not in young_generation\n+    return (active_generation() != (ShenandoahGeneration*) young_generation());\n+  default:\n+    assert(false, \"Bad affiliation (%d) for region \" SIZE_FORMAT, _affiliations[index], index);\n+    return false;\n+  }\n+}\n+\n+inline bool ShenandoahHeap::is_in_young(const void* p) const {\n+  return is_in(p) && (_affiliations[heap_region_index_containing(p)] == ShenandoahAffiliation::YOUNG_GENERATION);\n+}\n+\n+inline bool ShenandoahHeap::is_in_old(const void* p) const {\n+  return is_in(p) && (_affiliations[heap_region_index_containing(p)] == ShenandoahAffiliation::OLD_GENERATION);\n+}\n+\n+inline bool ShenandoahHeap::is_old(oop obj) const {\n+  return active_generation()->is_young() && is_in_old(obj);\n+}\n+\n+inline ShenandoahAffiliation ShenandoahHeap::region_affiliation(const ShenandoahHeapRegion *r) {\n+  return (ShenandoahAffiliation) _affiliations[r->index()];\n+}\n+\n+inline void ShenandoahHeap::assert_lock_for_affiliation(ShenandoahAffiliation orig_affiliation,\n+                                                        ShenandoahAffiliation new_affiliation) {\n+  \/\/ A lock is required when changing from FREE to NON-FREE.  Though it may be possible to elide the lock when\n+  \/\/ transitioning from in-use to FREE, the current implementation uses a lock for this transition.  A lock is\n+  \/\/ not required to change from YOUNG to OLD (i.e. when promoting humongous region).\n+  \/\/\n+  \/\/         new_affiliation is:     FREE   YOUNG   OLD\n+  \/\/  orig_affiliation is:  FREE      X       L      L\n+  \/\/                       YOUNG      L       X\n+  \/\/                         OLD      L       X      X\n+  \/\/  X means state transition won't happen (so don't care)\n+  \/\/  L means lock should be held\n+  \/\/  Blank means no lock required because affiliation visibility will not be required until subsequent safepoint\n+  \/\/\n+  \/\/ Note: during full GC, all transitions between states are possible.  During Full GC, we should be in a safepoint.\n+\n+  if ((orig_affiliation == ShenandoahAffiliation::FREE) || (new_affiliation == ShenandoahAffiliation::FREE)) {\n+    shenandoah_assert_heaplocked_or_fullgc_safepoint();\n+  }\n+}\n+\n+inline void ShenandoahHeap::set_affiliation(ShenandoahHeapRegion* r, ShenandoahAffiliation new_affiliation) {\n+#ifdef ASSERT\n+  assert_lock_for_affiliation(region_affiliation(r), new_affiliation);\n+#endif\n+  _affiliations[r->index()] = (uint8_t) new_affiliation;\n+}\n+\n+inline ShenandoahAffiliation ShenandoahHeap::region_affiliation(size_t index) {\n+  return (ShenandoahAffiliation) _affiliations[index];\n+}\n+\n+>>>>>>> 894aefc077d495b3cd892d339e9ae9120cf3be54\n@@ -305,0 +460,1 @@\n+\n@@ -317,0 +473,8 @@\n+inline bool ShenandoahHeap::is_concurrent_young_mark_in_progress() const {\n+  return _gc_state.is_set(YOUNG_MARKING);\n+}\n+\n+inline bool ShenandoahHeap::is_concurrent_old_mark_in_progress() const {\n+  return _gc_state.is_set(OLD_MARKING);\n+}\n+\n@@ -358,2 +522,1 @@\n-  ShenandoahMarkingContext* const ctx = complete_marking_context();\n-  assert(ctx->is_complete(), \"sanity\");\n+  ShenandoahMarkingContext* const ctx = marking_context();\n@@ -490,8 +653,0 @@\n-inline void ShenandoahHeap::mark_complete_marking_context() {\n-  _marking_context->mark_complete();\n-}\n-\n-inline void ShenandoahHeap::mark_incomplete_marking_context() {\n-  _marking_context->mark_incomplete();\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":169,"deletions":14,"binary":false,"changes":183,"status":"modified"}]}