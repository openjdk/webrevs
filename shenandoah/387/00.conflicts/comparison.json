{"files":[{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -27,1 +28,0 @@\n-#include \"gc\/shenandoah\/shenandoahCollectionSet.inline.hpp\"\n@@ -29,1 +29,0 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n@@ -36,0 +35,1 @@\n+#include \"utilities\/quickSort.hpp\"\n@@ -37,0 +37,1 @@\n+\/\/ sort by decreasing garbage (so most garbage comes first)\n@@ -38,1 +39,1 @@\n-  if (a._garbage > b._garbage)\n+  if (a._u._garbage > b._u._garbage)\n@@ -40,1 +41,1 @@\n-  else if (a._garbage < b._garbage)\n+  else if (a._u._garbage < b._u._garbage)\n@@ -48,0 +49,4 @@\n+<<<<<<< HEAD\n+=======\n+  _guaranteed_gc_interval(0),\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -52,1 +57,1 @@\n-  _gc_time_history(new TruncatedSeq(10, ShenandoahAdaptiveDecayFactor)),\n+  _gc_cycle_time_history(new TruncatedSeq(Moving_Average_Samples, ShenandoahAdaptiveDecayFactor)),\n@@ -66,1 +71,1 @@\n-  assert(collection_set->count() == 0, \"Must be empty\");\n+  assert(collection_set->is_empty(), \"Must be empty\");\n@@ -109,1 +114,1 @@\n-        candidates[cand_idx]._garbage = garbage;\n+        candidates[cand_idx]._u._garbage = garbage;\n@@ -150,1 +155,0 @@\n-\n@@ -155,2 +159,2 @@\n-                     \"Immediate: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%), \"\n-                     \"CSet: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%)\",\n+                     \"Immediate: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%), \" SIZE_FORMAT \" regions, \"\n+                     \"CSet: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%), \" SIZE_FORMAT \" regions\",\n@@ -165,0 +169,1 @@\n+                     immediate_regions,\n@@ -168,1 +173,2 @@\n-                     cset_percent);\n+                     cset_percent,\n+                     collection_set->count());\n@@ -187,1 +193,1 @@\n-  if (ShenandoahGuaranteedGCInterval > 0) {\n+  if (_guaranteed_gc_interval > 0) {\n@@ -189,3 +195,3 @@\n-    if (last_time_ms > ShenandoahGuaranteedGCInterval) {\n-      log_info(gc)(\"Trigger: Time since last GC (%.0f ms) is larger than guaranteed interval (\" UINTX_FORMAT \" ms)\",\n-                   last_time_ms, ShenandoahGuaranteedGCInterval);\n+    if (last_time_ms > _guaranteed_gc_interval) {\n+      log_info(gc)(\"Trigger (%s): Time since last GC (%.0f ms) is larger than guaranteed interval (\" UINTX_FORMAT \" ms)\",\n+                   _space_info->name(), last_time_ms, _guaranteed_gc_interval);\n@@ -205,1 +211,1 @@\n-          \"In range before adjustment: \" INTX_FORMAT, _gc_time_penalties);\n+         \"In range before adjustment: \" INTX_FORMAT, _gc_time_penalties);\n@@ -217,1 +223,1 @@\n-          \"In range after adjustment: \" INTX_FORMAT, _gc_time_penalties);\n+         \"In range after adjustment: \" INTX_FORMAT, _gc_time_penalties);\n@@ -220,0 +226,1 @@\n+<<<<<<< HEAD\n@@ -222,0 +229,3 @@\n+=======\n+void ShenandoahHeuristics::record_success_concurrent(bool abbreviated) {\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -225,0 +235,4 @@\n+\n+  if (_gc_times_learned <= ShenandoahLearningSteps || !(abbreviated && ShenandoahAdaptiveIgnoreShortCycles)) {\n+    _gc_cycle_time_history->add(elapsed_cycle_time());\n+  }\n@@ -259,1 +273,1 @@\n-double ShenandoahHeuristics::time_since_last_gc() const {\n+double ShenandoahHeuristics::elapsed_cycle_time() const {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":32,"deletions":18,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -29,1 +30,0 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n@@ -73,0 +73,2 @@\n+  static const uint Moving_Average_Samples = 10; \/\/ Number of samples to store in moving averages\n+\n@@ -75,1 +77,4 @@\n-    size_t _garbage;\n+    union {\n+      size_t _garbage;          \/\/ Not used by old-gen heuristics.\n+      size_t _live_data;        \/\/ Only used for old-gen heuristics, which prioritizes retention of _live_data over garbage reclaim\n+    } _u;\n@@ -81,0 +86,12 @@\n+  \/\/ Depending on generation mode, region data represents the results of the relevant\n+  \/\/ most recently completed marking pass:\n+  \/\/   - in GLOBAL mode, global marking pass\n+  \/\/   - in OLD mode,    old-gen marking pass\n+  \/\/   - in YOUNG mode,  young-gen marking pass\n+  \/\/\n+  \/\/ Note that there is some redundancy represented in region data because\n+  \/\/ each instance is an array large enough to hold all regions. However,\n+  \/\/ any region in young-gen is not in old-gen. And any time we are\n+  \/\/ making use of the GLOBAL data, there is no need to maintain the\n+  \/\/ YOUNG or OLD data. Consider this redundancy of data structure to\n+  \/\/ have negligible cost unless proven otherwise.\n@@ -83,0 +100,5 @@\n+<<<<<<< HEAD\n+=======\n+  size_t _guaranteed_gc_interval;\n+\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -88,1 +110,1 @@\n-  TruncatedSeq* _gc_time_history;\n+  TruncatedSeq* _gc_cycle_time_history;\n@@ -95,0 +117,4 @@\n+  \/\/ TODO: We need to enhance this API to give visibility to accompanying old-gen evacuation effort.\n+  \/\/ In the case that the old-gen evacuation effort is small or zero, the young-gen heuristics\n+  \/\/ should feel free to dedicate increased efforts to young-gen evacuation.\n+\n@@ -109,0 +135,4 @@\n+  void set_guaranteed_gc_interval(size_t guaranteed_gc_interval) {\n+    _guaranteed_gc_interval = guaranteed_gc_interval;\n+  }\n+\n@@ -117,1 +147,1 @@\n-  virtual void record_success_concurrent();\n+  virtual void record_success_concurrent(bool abbreviated);\n@@ -130,0 +160,3 @@\n+\n+  \/\/ This indicates whether or not the current cycle should unload classes.\n+  \/\/ It does NOT indicate that a cycle should be started.\n@@ -137,1 +170,1 @@\n-  double time_since_last_gc() const;\n+  double elapsed_cycle_time() const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp","additions":38,"deletions":5,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -34,0 +35,1 @@\n+<<<<<<< HEAD\n@@ -35,0 +37,7 @@\n+=======\n+  _mixed_gcs(0),\n+  _abbreviated_concurrent_gcs(0),\n+  _abbreviated_degenerated_gcs(0),\n+  _success_old_gcs(0),\n+  _interrupted_old_gcs(0),\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -38,0 +47,4 @@\n+<<<<<<< HEAD\n+=======\n+  _consecutive_young_gcs(0),\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -83,0 +96,1 @@\n+<<<<<<< HEAD\n@@ -84,0 +98,5 @@\n+=======\n+void ShenandoahCollectorPolicy::record_success_concurrent(bool is_young, bool is_abbreviated) {\n+  update_young(is_young);\n+\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -91,0 +110,1 @@\n+<<<<<<< HEAD\n@@ -92,0 +112,19 @@\n+=======\n+void ShenandoahCollectorPolicy::record_mixed_cycle() {\n+  _mixed_gcs++;\n+}\n+\n+void ShenandoahCollectorPolicy::record_success_old() {\n+  _consecutive_young_gcs = 0;\n+  _success_old_gcs++;\n+}\n+\n+void ShenandoahCollectorPolicy::record_interrupted_old() {\n+  _consecutive_young_gcs = 0;\n+  _interrupted_old_gcs++;\n+}\n+\n+void ShenandoahCollectorPolicy::record_success_degenerated(bool is_young, bool is_abbreviated) {\n+  update_young(is_young);\n+\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -97,0 +136,11 @@\n+<<<<<<< HEAD\n+=======\n+}\n+\n+void ShenandoahCollectorPolicy::update_young(bool is_young) {\n+  if (is_young) {\n+    _consecutive_young_gcs++;\n+  } else {\n+    _consecutive_young_gcs = 0;\n+  }\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -101,0 +151,4 @@\n+<<<<<<< HEAD\n+=======\n+  _consecutive_young_gcs = 0;\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -120,0 +174,1 @@\n+<<<<<<< HEAD\n@@ -121,0 +176,3 @@\n+=======\n+  size_t completed_gcs = _success_full_gcs + _success_degenerated_gcs + _success_concurrent_gcs + _success_old_gcs;\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -128,0 +186,10 @@\n+<<<<<<< HEAD\n+=======\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    out->print_cr(SIZE_FORMAT_W(5) \" Completed Old GCs (%.2f%%)\",        _success_old_gcs, percent_of(_success_old_gcs, completed_gcs));\n+    out->print_cr(\"  \" SIZE_FORMAT_W(5) \" mixed\",                        _mixed_gcs);\n+    out->print_cr(\"  \" SIZE_FORMAT_W(5) \" interruptions\",                _interrupted_old_gcs);\n+    out->cr();\n+  }\n+\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectorPolicy.cpp","additions":68,"deletions":0,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -42,0 +43,1 @@\n+<<<<<<< HEAD\n@@ -43,0 +45,7 @@\n+=======\n+  size_t _mixed_gcs;\n+  size_t _abbreviated_concurrent_gcs;\n+  size_t _abbreviated_degenerated_gcs;\n+  size_t _success_old_gcs;\n+  size_t _interrupted_old_gcs;\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -47,0 +56,4 @@\n+<<<<<<< HEAD\n+=======\n+  volatile size_t _consecutive_young_gcs;\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -55,0 +68,1 @@\n+  size_t _cycle_counter;\n@@ -64,0 +78,1 @@\n+<<<<<<< HEAD\n@@ -71,0 +86,12 @@\n+=======\n+  \/\/ TODO: This is different from gc_end: that one encompasses one VM operation.\n+  \/\/ These two encompass the entire cycle.\n+  void record_cycle_start();\n+\n+  void record_mixed_cycle();\n+\n+  void record_success_concurrent(bool is_young, bool is_abbreviated);\n+  void record_success_old();\n+  void record_interrupted_old();\n+  void record_success_degenerated(bool is_young, bool is_abbreviated);\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -91,0 +118,1 @@\n+<<<<<<< HEAD\n@@ -97,0 +125,12 @@\n+=======\n+  inline size_t consecutive_young_gc_count() const {\n+    return _consecutive_young_gcs;\n+  }\n+\n+  inline size_t consecutive_degenerated_gc_count() const {\n+    return _consecutive_degenerated_gcs;\n+  }\n+\n+private:\n+  void update_young(bool is_young);\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectorPolicy.hpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -34,0 +35,3 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -88,0 +92,1 @@\n+<<<<<<< HEAD\n@@ -92,0 +97,8 @@\n+=======\n+ShenandoahConcurrentGC::ShenandoahConcurrentGC(ShenandoahGeneration* generation, bool do_old_gc_bootstrap) :\n+  _mark(generation),\n+  _degen_point(ShenandoahDegenPoint::_degenerated_unset),\n+  _abbreviated(false),\n+  _do_old_gc_bootstrap(do_old_gc_bootstrap),\n+  _generation(generation) {\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -98,4 +111,0 @@\n-void ShenandoahConcurrentGC::cancel() {\n-  ShenandoahConcurrentMark::cancel();\n-}\n-\n@@ -104,0 +113,1 @@\n+\n@@ -114,0 +124,10 @@\n+\n+    \/\/ Reset task queue stats here, rather than in mark_concurrent_roots,\n+    \/\/ because remembered set scan will `push` oops into the queues and\n+    \/\/ resetting after this happens will lose those counts.\n+    TASKQUEUE_STATS_ONLY(_mark.task_queues()->reset_taskqueue_stats());\n+\n+    \/\/ Concurrent remembered set scanning\n+    entry_scan_remembered_set();\n+    \/\/ TODO: When RS scanning yields, we will need a check_cancellation_and_abort() degeneration point here.\n+\n@@ -116,1 +136,1 @@\n-    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_outside_cycle)) {\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_roots)) {\n@@ -130,0 +150,13 @@\n+  \/\/ If GC was cancelled before final mark, then the safepoint operation will do nothing\n+  \/\/ and the concurrent mark will still be in progress. In this case it is safe to resume\n+  \/\/ the degenerated cycle from the marking phase. On the other hand, if the GC is cancelled\n+  \/\/ after final mark (but before this check), then the final mark safepoint operation\n+  \/\/ will have finished the mark (setting concurrent mark in progress to false). Final mark\n+  \/\/ will also have setup state (in concurrent stack processing) that will not be safe to\n+  \/\/ resume from the marking phase in the degenerated cycle. That is, if the cancellation\n+  \/\/ occurred after final mark, we must resume the degenerated cycle after the marking phase.\n+  if (_generation->is_concurrent_mark_in_progress() && check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_mark)) {\n+    assert(!heap->is_concurrent_weak_root_in_progress(), \"Weak roots should not be in progress when concurrent mark is in progress\");\n+    return false;\n+  }\n+\n@@ -142,1 +175,2 @@\n-  \/\/ the space. This would be the last action if there is nothing to evacuate.\n+  \/\/ the space. This would be the last action if there is nothing to evacuate.  Note that\n+  \/\/ we will not age young-gen objects in the case that we skip evacuation.\n@@ -146,0 +180,2 @@\n+    \/\/ TODO: Not sure there is value in logging free-set status right here.  Note that whenever the free set is rebuilt,\n+    \/\/ it logs the newly rebuilt status.\n@@ -172,0 +208,1 @@\n+  }\n@@ -173,0 +210,1 @@\n+  if (heap->has_forwarded_objects()) {\n@@ -191,0 +229,4 @@\n+    \/\/ We chose not to evacuate because we found sufficient immediate garbage. Note that we\n+    \/\/ do not check for cancellation here because, at this point, the cycle is effectively\n+    \/\/ complete. If the cycle has been cancelled here, the control thread will detect it\n+    \/\/ on its next iteration and run a degenerated young cycle.\n@@ -195,0 +237,45 @@\n+  \/\/ We defer generation resizing actions until after cset regions have been recycled.  We do this even following an\n+  \/\/ abbreviated cycle.\n+  if (heap->mode()->is_generational()) {\n+    bool success;\n+    size_t region_xfer;\n+    const char* region_destination;\n+    ShenandoahYoungGeneration* young_gen = heap->young_generation();\n+    ShenandoahGeneration* old_gen = heap->old_generation();\n+    {\n+      ShenandoahHeapLocker locker(heap->lock());\n+\n+      size_t old_region_surplus = heap->get_old_region_surplus();\n+      size_t old_region_deficit = heap->get_old_region_deficit();\n+      if (old_region_surplus) {\n+        success = heap->generation_sizer()->transfer_to_young(old_region_surplus);\n+        region_destination = \"young\";\n+        region_xfer = old_region_surplus;\n+      } else if (old_region_deficit) {\n+        success = heap->generation_sizer()->transfer_to_old(old_region_deficit);\n+        region_destination = \"old\";\n+        region_xfer = old_region_deficit;\n+        if (!success) {\n+          ((ShenandoahOldHeuristics *) old_gen->heuristics())->trigger_cannot_expand();\n+        }\n+      } else {\n+        region_destination = \"none\";\n+        region_xfer = 0;\n+        success = true;\n+      }\n+      heap->set_old_region_surplus(0);\n+      heap->set_old_region_deficit(0);\n+      heap->set_young_evac_reserve(0);\n+      heap->set_old_evac_reserve(0);\n+      heap->set_promoted_reserve(0);\n+    }\n+\n+    \/\/ Report outside the heap lock\n+    size_t young_available = young_gen->available();\n+    size_t old_available = old_gen->available();\n+    log_info(gc, ergo)(\"After cleanup, %s \" SIZE_FORMAT \" regions to %s to prepare for next gc, old available: \"\n+                       SIZE_FORMAT \"%s, young_available: \" SIZE_FORMAT \"%s\",\n+                       success? \"successfully transferred\": \"failed to transfer\", region_xfer, region_destination,\n+                       byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n+                       byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n+  }\n@@ -304,0 +391,2 @@\n+  heap->try_inject_alloc_failure();\n+\n@@ -305,3 +394,10 @@\n-  static const char* msg = \"Concurrent reset\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset);\n-  EventMark em(\"%s\", msg);\n+  {\n+    static const char* msg = \"Concurrent reset\";\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset);\n+    EventMark em(\"%s\", msg);\n+\n+    ShenandoahWorkerScope scope(heap->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n+                                msg);\n+    op_reset();\n+  }\n@@ -309,3 +405,7 @@\n-  ShenandoahWorkerScope scope(heap->workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n-                              \"concurrent reset\");\n+  if (_do_old_gc_bootstrap) {\n+    static const char* msg = \"Concurrent reset (OLD)\";\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset_old);\n+    ShenandoahWorkerScope scope(ShenandoahHeap::heap()->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n+                                msg);\n+    EventMark em(\"%s\", msg);\n@@ -313,2 +413,19 @@\n-  heap->try_inject_alloc_failure();\n-  op_reset();\n+    heap->old_generation()->prepare_gc();\n+  }\n+}\n+\n+void ShenandoahConcurrentGC::entry_scan_remembered_set() {\n+  if (_generation->is_young()) {\n+    ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+    TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+    const char* msg = \"Concurrent remembered set scanning\";\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::init_scan_rset);\n+    EventMark em(\"%s\", msg);\n+\n+    ShenandoahWorkerScope scope(heap->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_rs_scanning(),\n+                                msg);\n+\n+    heap->try_inject_alloc_failure();\n+    _generation->scan_remembered_set(true \/* is_concurrent *\/);\n+  }\n@@ -495,2 +612,1 @@\n-\n-  heap->prepare_gc();\n+  _generation->prepare_gc();\n@@ -509,1 +625,2 @@\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n+      \/\/ reset, so it is very likely we don't need to do another write here.  Since most regions\n+      \/\/ are not \"active\", this path is relatively rare.\n@@ -531,2 +648,2 @@\n-  assert(heap->marking_context()->is_bitmap_clear(), \"need clear marking bitmap\");\n-  assert(!heap->marking_context()->is_complete(), \"should not be complete\");\n+  assert(_generation->is_bitmap_clear(), \"need clear marking bitmap\");\n+  assert(!_generation->is_mark_complete(), \"should not be complete\");\n@@ -535,0 +652,22 @@\n+\n+  if (heap->mode()->is_generational()) {\n+    if (_generation->is_young() || (_generation->is_global() && ShenandoahVerify)) {\n+      \/\/ The current implementation of swap_remembered_set() copies the write-card-table\n+      \/\/ to the read-card-table. The remembered sets are also swapped for GLOBAL collections\n+      \/\/ so that the verifier works with the correct copy of the card table when verifying.\n+      \/\/ TODO: This path should not really depend on ShenandoahVerify.\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_swap_rset);\n+      _generation->swap_remembered_set();\n+    }\n+\n+    if (_generation->is_global()) {\n+      heap->cancel_old_gc();\n+    } else if (heap->is_concurrent_old_mark_in_progress()) {\n+      \/\/ Purge the SATB buffers, transferring any valid, old pointers to the\n+      \/\/ old generation mark queue. Any pointers in a young region will be\n+      \/\/ abandoned.\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_transfer_satb);\n+      heap->transfer_old_pointers_from_satb();\n+    }\n+  }\n+\n@@ -543,1 +682,1 @@\n-  heap->set_concurrent_mark_in_progress(true);\n+  _generation->set_concurrent_mark_in_progress(true);\n@@ -547,1 +686,6 @@\n-  {\n+  if (_do_old_gc_bootstrap) {\n+    \/\/ Update region state for both young and old regions\n+    \/\/ TODO: We should be able to pull this out of the safepoint for the bootstrap\n+    \/\/ cycle. The top of an old region will only move when a GC cycle evacuates\n+    \/\/ objects into it. When we start an old cycle, we know that nothing can touch\n+    \/\/ the top of old regions.\n@@ -551,0 +695,6 @@\n+    heap->old_generation()->ref_processor()->reset_thread_locals();\n+  } else {\n+    \/\/ Update region state for only young regions\n+    ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_region_states);\n+    ShenandoahInitMarkUpdateRegionStateClosure cl;\n+    _generation->parallel_heap_region_iterate(&cl);\n@@ -554,1 +704,1 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = _generation->ref_processor();\n@@ -594,1 +744,26 @@\n-    heap->prepare_regions_and_collection_set(true \/*concurrent*\/);\n+    \/\/ The collection set is chosen by prepare_regions_and_collection_set().\n+    \/\/\n+    \/\/ TODO: Under severe memory overload conditions that can be checked here, we may want to limit\n+    \/\/ the inclusion of old-gen candidates within the collection set.  This would allow us to prioritize efforts on\n+    \/\/ evacuating young-gen,  This remediation is most appropriate when old-gen availability is very high (so there\n+    \/\/ are negligible negative impacts from delaying completion of old-gen evacuation) and when young-gen collections\n+    \/\/ are \"under duress\" (as signalled by very low availability of memory within young-gen, indicating that\/ young-gen\n+    \/\/ collections are not triggering frequently enough).\n+    _generation->prepare_regions_and_collection_set(true \/*concurrent*\/);\n+\n+    \/\/ Upon return from prepare_regions_and_collection_set(), certain parameters have been established to govern the\n+    \/\/ evacuation efforts that are about to begin.  In particular:\n+    \/\/\n+    \/\/ heap->get_promoted_reserve() represents the amount of memory within old-gen's available memory that has\n+    \/\/   been set aside to hold objects promoted from young-gen memory.  This represents an estimated percentage\n+    \/\/   of the live young-gen memory within the collection set.  If there is more data ready to be promoted than\n+    \/\/   can fit within this reserve, the promotion of some objects will be deferred until a subsequent evacuation\n+    \/\/   pass.\n+    \/\/\n+    \/\/ heap->get_old_evac_reserve() represents the amount of memory within old-gen's available memory that has been\n+    \/\/  set aside to hold objects evacuated from the old-gen collection set.\n+    \/\/\n+    \/\/ heap->get_young_evac_reserve() represents the amount of memory within young-gen's available memory that has\n+    \/\/  been set aside to hold objects evacuated from the young-gen collection set.  Conservatively, this value\n+    \/\/  equals the entire amount of live young-gen memory within the collection set, even though some of this memory\n+    \/\/  will likely be promoted.\n@@ -599,21 +774,51 @@\n-    if (!heap->collection_set()->is_empty()) {\n-      if (ShenandoahVerify) {\n-        heap->verifier()->verify_before_evacuation();\n-      }\n-\n-      heap->set_evacuation_in_progress(true);\n-      \/\/ From here on, we need to update references.\n-      heap->set_has_forwarded_objects(true);\n-\n-      \/\/ Verify before arming for concurrent processing.\n-      \/\/ Otherwise, verification can trigger stack processing.\n-      if (ShenandoahVerify) {\n-        heap->verifier()->verify_during_evacuation();\n-      }\n-\n-      \/\/ Arm nmethods\/stack for concurrent processing\n-      ShenandoahCodeRoots::arm_nmethods_for_evac();\n-      ShenandoahStackWatermark::change_epoch_id();\n-\n-      if (ShenandoahPacing) {\n-        heap->pacer()->setup_for_evac();\n+    if (heap->mode()->is_generational()) {\n+      size_t humongous_regions_promoted = heap->get_promotable_humongous_regions();\n+      size_t regular_regions_promoted_in_place = heap->get_regular_regions_promoted_in_place();\n+      if (!heap->collection_set()->is_empty() || (humongous_regions_promoted + regular_regions_promoted_in_place > 0)) {\n+        \/\/ Even if the collection set is empty, we need to do evacuation if there are regions to be promoted in place.\n+        \/\/ Concurrent evacuation takes responsibility for registering objects and setting the remembered set cards to dirty.\n+\n+        LogTarget(Debug, gc, cset) lt;\n+        if (lt.is_enabled()) {\n+          ResourceMark rm;\n+          LogStream ls(lt);\n+          heap->collection_set()->print_on(&ls);\n+        }\n+\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_before_evacuation();\n+        }\n+\n+        heap->set_evacuation_in_progress(true);\n+\n+        \/\/ Verify before arming for concurrent processing.\n+        \/\/ Otherwise, verification can trigger stack processing.\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_during_evacuation();\n+        }\n+\n+        \/\/ Generational mode may promote objects in place during the evacuation phase.\n+        \/\/ If that is the only reason we are evacuating, we don't need to update references\n+        \/\/ and there will be no forwarded objects on the heap.\n+        heap->set_has_forwarded_objects(!heap->collection_set()->is_empty());\n+\n+        \/\/ Arm nmethods\/stack for concurrent processing\n+        if (!heap->collection_set()->is_empty()) {\n+          \/\/ Iff objects will be evaluated, arm the nmethod barriers. These will be disarmed\n+          \/\/ under the same condition (established in prepare_concurrent_roots) after strong\n+          \/\/ root evacuation has completed (see op_strong_roots).\n+          ShenandoahCodeRoots::arm_nmethods_for_evac();\n+          ShenandoahStackWatermark::change_epoch_id();\n+        }\n+\n+        if (ShenandoahPacing) {\n+          heap->pacer()->setup_for_evac();\n+        }\n+      } else {\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_after_concmark();\n+        }\n+\n+        if (VerifyAfterGC) {\n+          Universe::verify();\n+        }\n@@ -622,6 +827,39 @@\n-      if (ShenandoahVerify) {\n-        heap->verifier()->verify_after_concmark();\n-      }\n-\n-      if (VerifyAfterGC) {\n-        Universe::verify();\n+      \/\/ Not is_generational()\n+      if (!heap->collection_set()->is_empty()) {\n+        LogTarget(Info, gc, ergo) lt;\n+        if (lt.is_enabled()) {\n+          ResourceMark rm;\n+          LogStream ls(lt);\n+          heap->collection_set()->print_on(&ls);\n+        }\n+\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_before_evacuation();\n+        }\n+\n+        heap->set_evacuation_in_progress(true);\n+\n+        \/\/ Verify before arming for concurrent processing.\n+        \/\/ Otherwise, verification can trigger stack processing.\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_during_evacuation();\n+        }\n+\n+        \/\/ From here on, we need to update references.\n+        heap->set_has_forwarded_objects(true);\n+\n+        \/\/ Arm nmethods\/stack for concurrent processing\n+        ShenandoahCodeRoots::arm_nmethods_for_evac();\n+        ShenandoahStackWatermark::change_epoch_id();\n+\n+        if (ShenandoahPacing) {\n+          heap->pacer()->setup_for_evac();\n+        }\n+      } else {\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_after_concmark();\n+        }\n+\n+        if (VerifyAfterGC) {\n+          Universe::verify();\n+        }\n@@ -649,0 +887,1 @@\n+  ShenandoahThreadLocalData::enable_plab_promotions(thread);\n@@ -662,0 +901,3 @@\n+    Thread* worker_thread = Thread::current();\n+    ShenandoahThreadLocalData::enable_plab_promotions(worker_thread);\n+\n@@ -686,1 +928,1 @@\n-  heap->ref_processor()->process_references(ShenandoahPhaseTimings::conc_weak_refs, heap->workers(), true \/* concurrent *\/);\n+  _generation->ref_processor()->process_references(ShenandoahPhaseTimings::conc_weak_refs, heap->workers(), true \/* concurrent *\/);\n@@ -713,2 +955,9 @@\n-      shenandoah_assert_correct(p, obj);\n-      ShenandoahHeap::atomic_clear_oop(p, obj);\n+      if (_heap->is_in_active_generation(obj)) {\n+        \/\/ TODO: This worries me. Here we are asserting that an unmarked from-space object is 'correct'.\n+        \/\/ Normally, I would call this a bogus assert, but there seems to be a legitimate use-case for\n+        \/\/ accessing from-space objects during class unloading. However, the from-space object may have\n+        \/\/ been \"filled\". We've made no effort to prevent old generation classes being unloaded by young\n+        \/\/ gen (and vice-versa).\n+        shenandoah_assert_correct(p, obj);\n+        ShenandoahHeap::atomic_clear_oop(p, obj);\n+      }\n@@ -938,1 +1187,3 @@\n-\n+  if (ShenandoahVerify) {\n+    heap->verifier()->verify_before_updaterefs();\n+  }\n@@ -983,1 +1234,1 @@\n-    heap->clear_cancelled_gc();\n+    heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n@@ -991,0 +1242,18 @@\n+  if (heap->mode()->is_generational() && heap->is_concurrent_old_mark_in_progress()) {\n+    \/\/ When the SATB barrier is left on to support concurrent old gen mark, it may pick up writes to\n+    \/\/ objects in the collection set. After those objects are evacuated, the pointers in the\n+    \/\/ SATB are no longer safe. Once we have finished update references, we are guaranteed that\n+    \/\/ no more writes to the collection set are possible.\n+    \/\/\n+    \/\/ This will transfer any old pointers in _active_ regions from the SATB to the old gen\n+    \/\/ mark queues. All other pointers will be discarded. This would also discard any pointers\n+    \/\/ in old regions that were included in a mixed evacuation. We aren't using the SATB filter\n+    \/\/ methods here because we cannot control when they execute. If the SATB filter runs _after_\n+    \/\/ a region has been recycled, we will not be able to detect the bad pointer.\n+    \/\/\n+    \/\/ We are not concerned about skipping this step in abbreviated cycles because regions\n+    \/\/ with no live objects cannot have been written to and so cannot have entries in the SATB\n+    \/\/ buffers.\n+    heap->transfer_old_pointers_from_satb();\n+  }\n+\n@@ -996,0 +1265,4 @@\n+  \/\/ Aging_cycle is only relevant during evacuation cycle for individual objects and during final mark for\n+  \/\/ entire regions.  Both of these relevant operations occur before final update refs.\n+  heap->set_aging_cycle(false);\n+\n@@ -1008,1 +1281,27 @@\n-  ShenandoahHeap::heap()->set_concurrent_weak_root_in_progress(false);\n+\n+  ShenandoahHeap *heap = ShenandoahHeap::heap();\n+  heap->set_concurrent_weak_root_in_progress(false);\n+  heap->set_evacuation_in_progress(false);\n+\n+  if (heap->mode()->is_generational()) {\n+    \/\/ If the cycle was shortened for having enough immediate garbage, this could be\n+    \/\/ the last GC safepoint before concurrent marking of old resumes. We must be sure\n+    \/\/ that old mark threads don't see any pointers to garbage in the SATB buffers.\n+    if (heap->is_concurrent_old_mark_in_progress()) {\n+      heap->transfer_old_pointers_from_satb();\n+    }\n+\n+    ShenandoahMarkingContext *ctx = heap->complete_marking_context();\n+    for (size_t i = 0; i < heap->num_regions(); i++) {\n+      ShenandoahHeapRegion *r = heap->get_region(i);\n+      if (r->is_active() && r->is_young()) {\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        HeapWord* top = r->top();\n+        if (top > tams) {\n+          r->reset_age();\n+        } else if (heap->is_aging_cycle()) {\n+          r->increment_age();\n+        }\n+      }\n+    }\n+  }\n@@ -1027,1 +1326,1 @@\n-    return \"Pause Init Mark (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Init Mark\", \" (unload classes)\");\n@@ -1029,1 +1328,1 @@\n-    return \"Pause Init Mark\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Init Mark\", \"\");\n@@ -1035,1 +1334,3 @@\n-  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  assert(!heap->has_forwarded_objects() || heap->is_concurrent_old_mark_in_progress(),\n+         \"Should not have forwarded objects during final mark, unless old gen concurrent mark is running\");\n+\n@@ -1037,1 +1338,1 @@\n-    return \"Pause Final Mark (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Final Mark\", \" (unload classes)\");\n@@ -1039,1 +1340,1 @@\n-    return \"Pause Final Mark\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Final Mark\", \"\");\n@@ -1045,1 +1346,2 @@\n-  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  assert(!heap->has_forwarded_objects() || heap->is_concurrent_old_mark_in_progress(),\n+         \"Should not have forwarded objects concurrent mark, unless old gen concurrent mark is running\");\n@@ -1047,1 +1349,1 @@\n-    return \"Concurrent marking (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent marking\", \" (unload classes)\");\n@@ -1049,1 +1351,1 @@\n-    return \"Concurrent marking\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent marking\", \"\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":366,"deletions":64,"binary":false,"changes":430,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -33,0 +34,2 @@\n+class ShenandoahGeneration;\n+\n@@ -45,0 +48,3 @@\n+protected:\n+  ShenandoahConcurrentMark    _mark;\n+\n@@ -46,0 +52,1 @@\n+<<<<<<< HEAD\n@@ -49,0 +56,8 @@\n+=======\n+  ShenandoahDegenPoint        _degen_point;\n+  bool                        _abbreviated;\n+  const bool                  _do_old_gc_bootstrap;\n+\n+protected:\n+  ShenandoahGeneration* const _generation;\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -51,1 +66,1 @@\n-  ShenandoahConcurrentGC();\n+  ShenandoahConcurrentGC(ShenandoahGeneration* generation, bool do_old_gc_bootstrap);\n@@ -54,0 +69,1 @@\n+  bool abbreviated() const { return _abbreviated; }\n@@ -55,0 +71,1 @@\n+<<<<<<< HEAD\n@@ -60,0 +77,2 @@\n+=======\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -64,0 +83,2 @@\n+\n+protected:\n@@ -65,0 +86,3 @@\n+  void vmop_entry_final_roots();\n+\n+private:\n@@ -67,1 +91,0 @@\n-  void vmop_entry_final_roots();\n@@ -81,0 +104,3 @@\n+  void entry_scan_remembered_set();\n+\n+protected:\n@@ -88,0 +114,2 @@\n+\n+private:\n@@ -98,1 +126,0 @@\n-  void op_final_mark();\n@@ -113,0 +140,4 @@\n+protected:\n+  virtual void op_final_mark();\n+\n+private:\n@@ -121,0 +152,1 @@\n+protected:\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.hpp","additions":35,"deletions":3,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -31,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n@@ -33,0 +35,4 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -38,0 +44,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGC.hpp\"\n@@ -43,0 +50,2 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+#include \"logging\/log.hpp\"\n@@ -51,0 +60,1 @@\n+<<<<<<< HEAD\n@@ -54,0 +64,9 @@\n+=======\n+  _alloc_failure_waiters_lock(Mutex::safepoint - 2, \"ShenandoahAllocFailureGC_lock\", true),\n+  _gc_waiters_lock(Mutex::safepoint - 2, \"ShenandoahRequestedGC_lock\", true),\n+  _control_lock(Mutex::nosafepoint - 2, \"ShenandoahControlGC_lock\", true),\n+  _regulator_lock(Mutex::nosafepoint - 2, \"ShenandoahRegulatorGC_lock\", true),\n+  _periodic_task(this),\n+  _requested_gc_cause(GCCause::_no_gc),\n+  _requested_generation(select_global_generation()),\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -55,1 +74,3 @@\n-  _allocs_seen(0) {\n+  _degen_generation(nullptr),\n+  _allocs_seen(0),\n+  _mode(none) {\n@@ -62,1 +83,1 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n@@ -64,3 +85,2 @@\n-  GCMode default_mode = concurrent_normal;\n-  GCCause::Cause default_cause = GCCause::_shenandoah_concurrent_gc;\n-  int sleep = ShenandoahControlIntervalMin;\n+  const GCMode default_mode = concurrent_normal;\n+  ShenandoahGenerationType generation = select_global_generation();\n@@ -69,1 +89,1 @@\n-  double last_sleep_adjust_time = os::elapsedTime();\n+  uint age_period = 0;\n@@ -75,1 +95,3 @@\n-  double shrink_period = (double)ShenandoahUncommitDelay \/ 1000 \/ 10;\n+  const double shrink_period = (double)ShenandoahUncommitDelay \/ 1000 \/ 10;\n+\n+  ShenandoahCollectorPolicy* const policy = heap->shenandoah_policy();\n@@ -77,2 +99,5 @@\n-  ShenandoahCollectorPolicy* policy = heap->shenandoah_policy();\n-  ShenandoahHeuristics* heuristics = heap->heuristics();\n+  \/\/ Heuristics are notified of allocation failures here and other outcomes\n+  \/\/ of the cycle. They're also used here to control whether the Nth consecutive\n+  \/\/ degenerated cycle should be 'promoted' to a full cycle. The decision to\n+  \/\/ trigger a cycle or not is evaluated on the regulator thread.\n+  ShenandoahHeuristics* global_heuristics = heap->global_generation()->heuristics();\n@@ -81,5 +106,7 @@\n-    bool alloc_failure_pending = _alloc_failure_gc.is_set();\n-    bool is_gc_requested = _gc_requested.is_set();\n-    GCCause::Cause requested_gc_cause = _requested_gc_cause;\n-    bool explicit_gc_requested = is_gc_requested && is_explicit_gc(requested_gc_cause);\n-    bool implicit_gc_requested = is_gc_requested && !is_explicit_gc(requested_gc_cause);\n+    const bool alloc_failure_pending = _alloc_failure_gc.is_set();\n+    const bool humongous_alloc_failure_pending = _humongous_alloc_failure_gc.is_set();\n+\n+    GCCause::Cause cause = Atomic::xchg(&_requested_gc_cause, GCCause::_no_gc);\n+\n+    const bool explicit_gc_requested = is_explicit_gc(cause);\n+    const bool implicit_gc_requested = is_implicit_gc(cause);\n@@ -88,1 +115,1 @@\n-    size_t allocs_seen = Atomic::xchg(&_allocs_seen, (size_t)0, memory_order_relaxed);\n+    const size_t allocs_seen = Atomic::xchg(&_allocs_seen, (size_t)0, memory_order_relaxed);\n@@ -91,1 +118,1 @@\n-    bool soft_max_changed = check_soft_max_changed();\n+    const bool soft_max_changed = check_soft_max_changed();\n@@ -94,2 +121,1 @@\n-    GCMode mode = none;\n-    GCCause::Cause cause = GCCause::_last_gc_cause;\n+    set_gc_mode(none);\n@@ -108,1 +134,14 @@\n-      if (ShenandoahDegeneratedGC && heuristics->should_degenerate_cycle()) {\n+      if (degen_point == ShenandoahGC::_degenerated_outside_cycle) {\n+        _degen_generation = heap->mode()->is_generational() ?\n+                heap->young_generation() : heap->global_generation();\n+      } else {\n+        assert(_degen_generation != nullptr, \"Need to know which generation to resume\");\n+      }\n+\n+      ShenandoahHeuristics* heuristics = _degen_generation->heuristics();\n+      generation = _degen_generation->type();\n+      bool old_gen_evacuation_failed = heap->clear_old_evacuation_failure();\n+\n+      \/\/ Do not bother with degenerated cycle if old generation evacuation failed or if humongous allocation failed\n+      if (ShenandoahDegeneratedGC && heuristics->should_degenerate_cycle() &&\n+          !old_gen_evacuation_failed && !humongous_alloc_failure_pending) {\n@@ -111,1 +150,1 @@\n-        mode = stw_degenerated;\n+        set_gc_mode(stw_degenerated);\n@@ -113,0 +152,7 @@\n+        \/\/ TODO: if humongous_alloc_failure_pending, there might be value in trying a \"compacting\" degen before\n+        \/\/ going all the way to full.  But it's a lot of work to implement this, and it may not provide value.\n+        \/\/ A compacting degen can move young regions around without doing full old-gen mark (relying upon the\n+        \/\/ remembered set scan), so it might be faster than a full gc.\n+        \/\/\n+        \/\/ Longer term, think about how to defragment humongous memory concurrently.\n+\n@@ -115,1 +161,2 @@\n-        mode = stw_full;\n+        generation = select_global_generation();\n+        set_gc_mode(stw_full);\n@@ -117,1 +164,0 @@\n-\n@@ -119,1 +165,1 @@\n-      cause = requested_gc_cause;\n+      generation = select_global_generation();\n@@ -122,1 +168,1 @@\n-      heuristics->record_requested_gc();\n+      global_heuristics->record_requested_gc();\n@@ -126,1 +172,1 @@\n-        mode = default_mode;\n+        set_gc_mode(default_mode);\n@@ -128,1 +174,1 @@\n-        heap->set_unload_classes(heuristics->can_unload_classes());\n+        heap->set_unload_classes(global_heuristics->can_unload_classes());\n@@ -131,1 +177,1 @@\n-        mode = stw_full;\n+        set_gc_mode(stw_full);\n@@ -134,1 +180,1 @@\n-      cause = requested_gc_cause;\n+      generation = select_global_generation();\n@@ -137,1 +183,1 @@\n-      heuristics->record_requested_gc();\n+      global_heuristics->record_requested_gc();\n@@ -141,1 +187,1 @@\n-        mode = default_mode;\n+        set_gc_mode(default_mode);\n@@ -144,1 +190,1 @@\n-        heap->set_unload_classes(heuristics->can_unload_classes());\n+        heap->set_unload_classes(global_heuristics->can_unload_classes());\n@@ -147,1 +193,1 @@\n-        mode = stw_full;\n+        set_gc_mode(stw_full);\n@@ -150,5 +196,12 @@\n-      \/\/ Potential normal cycle: ask heuristics if it wants to act\n-      if (heuristics->should_start_gc()) {\n-        mode = default_mode;\n-        cause = default_cause;\n-      }\n+      \/\/ We should only be here if the regulator requested a cycle or if\n+      \/\/ there is an old generation mark in progress.\n+      if (cause == GCCause::_shenandoah_concurrent_gc) {\n+        if (_requested_generation == OLD && heap->doing_mixed_evacuations()) {\n+          \/\/ If a request to start an old cycle arrived while an old cycle was running, but _before_\n+          \/\/ it chose any regions for evacuation we don't want to start a new old cycle. Rather, we want\n+          \/\/ the heuristic to run a young collection so that we can evacuate some old regions.\n+          assert(!heap->is_concurrent_old_mark_in_progress(), \"Should not be running mixed collections and concurrent marking\");\n+          generation = YOUNG;\n+        } else {\n+          generation = _requested_generation;\n+        }\n@@ -156,3 +209,7 @@\n-      \/\/ Ask policy if this cycle wants to process references or unload classes\n-      heap->set_unload_classes(heuristics->should_unload_classes());\n-    }\n+        \/\/ preemption was requested or this is a regular cycle\n+        set_gc_mode(default_mode);\n+\n+        \/\/ Don't start a new old marking if there is one already in progress\n+        if (generation == OLD && heap->is_concurrent_old_mark_in_progress()) {\n+          set_gc_mode(servicing_old);\n+        }\n@@ -160,4 +217,17 @@\n-    \/\/ Blow all soft references on this cycle, if handling allocation failure,\n-    \/\/ either implicit or explicit GC request,  or we are requested to do so unconditionally.\n-    if (alloc_failure_pending || implicit_gc_requested || explicit_gc_requested || ShenandoahAlwaysClearSoftRefs) {\n-      heap->soft_ref_policy()->set_should_clear_all_soft_refs(true);\n+        if (generation == select_global_generation()) {\n+          heap->set_unload_classes(global_heuristics->should_unload_classes());\n+        } else {\n+          heap->set_unload_classes(false);\n+        }\n+      } else if (heap->is_concurrent_old_mark_in_progress() || heap->is_prepare_for_old_mark_in_progress()) {\n+        \/\/ Nobody asked us to do anything, but we have an old-generation mark or old-generation preparation for\n+        \/\/ mixed evacuation in progress, so resume working on that.\n+        log_info(gc)(\"Resume old GC: marking is%s in progress, preparing is%s in progress\",\n+                     heap->is_concurrent_old_mark_in_progress() ? \"\" : \" NOT\",\n+                     heap->is_prepare_for_old_mark_in_progress() ? \"\" : \" NOT\");\n+\n+        cause = GCCause::_shenandoah_concurrent_gc;\n+        generation = OLD;\n+        set_gc_mode(servicing_old);\n+        heap->set_unload_classes(false);\n+      }\n@@ -166,2 +236,2 @@\n-    bool gc_requested = (mode != none);\n-    assert (!gc_requested || cause != GCCause::_last_gc_cause, \"GC cause should be set\");\n+    const bool gc_requested = (gc_mode() != none);\n+    assert (!gc_requested || cause != GCCause::_no_gc, \"GC cause should be set\");\n@@ -170,0 +240,6 @@\n+      \/\/ Blow away all soft references on this cycle, if handling allocation failure,\n+      \/\/ either implicit or explicit GC request, or we are requested to do so unconditionally.\n+      if (generation == select_global_generation() && (alloc_failure_pending || implicit_gc_requested || explicit_gc_requested || ShenandoahAlwaysClearSoftRefs)) {\n+        heap->soft_ref_policy()->set_should_clear_all_soft_refs(true);\n+      }\n+\n@@ -186,4 +262,16 @@\n-\n-      switch (mode) {\n-        case concurrent_normal:\n-          service_concurrent_normal_cycle(cause);\n+      \/\/ In case this is a degenerated cycle, remember whether original cycle was aging.\n+      const bool was_aging_cycle = heap->is_aging_cycle();\n+      heap->set_aging_cycle(false);\n+\n+      switch (gc_mode()) {\n+        case concurrent_normal: {\n+          \/\/ At this point:\n+          \/\/  if (generation == YOUNG), this is a normal YOUNG cycle\n+          \/\/  if (generation == OLD), this is a bootstrap OLD cycle\n+          \/\/  if (generation == GLOBAL), this is a GLOBAL cycle triggered by System.gc()\n+          \/\/ In all three cases, we want to age old objects if this is an aging cycle\n+          if (age_period-- == 0) {\n+             heap->set_aging_cycle(true);\n+             age_period = ShenandoahAgingCyclePeriod - 1;\n+          }\n+          service_concurrent_normal_cycle(heap, generation, cause);\n@@ -191,1 +279,3 @@\n-        case stw_degenerated:\n+        }\n+        case stw_degenerated: {\n+          heap->set_aging_cycle(was_aging_cycle);\n@@ -194,1 +284,6 @@\n-        case stw_full:\n+        }\n+        case stw_full: {\n+          if (age_period-- == 0) {\n+            heap->set_aging_cycle(true);\n+            age_period = ShenandoahAgingCyclePeriod - 1;\n+          }\n@@ -197,0 +292,7 @@\n+        }\n+        case servicing_old: {\n+          assert(generation == OLD, \"Expected old generation here\");\n+          GCIdMark gc_id_mark;\n+          service_concurrent_old_cycle(heap, cause);\n+          break;\n+        }\n@@ -236,7 +338,1 @@\n-        heuristics->clear_metaspace_oom();\n-      }\n-\n-      \/\/ Commit worker statistics to cycle data\n-      heap->phase_timings()->flush_par_workers_to_cycle();\n-      if (ShenandoahPacing) {\n-        heap->pacer()->flush_stats_to_cycle();\n+        global_heuristics->clear_metaspace_oom();\n@@ -245,15 +341,1 @@\n-      \/\/ Print GC stats for current cycle\n-      {\n-        LogTarget(Info, gc, stats) lt;\n-        if (lt.is_enabled()) {\n-          ResourceMark rm;\n-          LogStream ls(lt);\n-          heap->phase_timings()->print_cycle_on(&ls);\n-          if (ShenandoahPacing) {\n-            heap->pacer()->print_cycle_on(&ls);\n-          }\n-        }\n-      }\n-\n-      \/\/ Commit statistics to globals\n-      heap->phase_timings()->flush_cycle_to_global();\n+      process_phase_timings(heap);\n@@ -269,1 +351,1 @@\n-      \/\/ Allow allocators to know we have seen this much regions\n+      \/\/ Allow pacer to know we have seen this many allocations\n@@ -295,8 +377,6 @@\n-    \/\/ Wait before performing the next action. If allocation happened during this wait,\n-    \/\/ we exit sooner, to let heuristics re-evaluate new conditions. If we are at idle,\n-    \/\/ back off exponentially.\n-    if (_heap_changed.try_unset()) {\n-      sleep = ShenandoahControlIntervalMin;\n-    } else if ((current - last_sleep_adjust_time) * 1000 > ShenandoahControlIntervalAdjustPeriod){\n-      sleep = MIN2<int>(ShenandoahControlIntervalMax, MAX2(1, sleep * 2));\n-      last_sleep_adjust_time = current;\n+    \/\/ Wait for ShenandoahControlIntervalMax unless there was an allocation failure or another request was made mid-cycle.\n+    if (!is_alloc_failure_gc() && _requested_gc_cause == GCCause::_no_gc) {\n+      \/\/ The timed wait is necessary because this thread has a responsibility to send\n+      \/\/ 'alloc_words' to the pacer when it does not perform a GC.\n+      MonitorLocker lock(&_control_lock, Mutex::_no_safepoint_check_flag);\n+      lock.wait(ShenandoahControlIntervalMax);\n@@ -304,1 +384,0 @@\n-    os::naked_short_sleep(sleep);\n@@ -313,0 +392,206 @@\n+void ShenandoahControlThread::process_phase_timings(const ShenandoahHeap* heap) {\n+  \/\/ Commit worker statistics to cycle data\n+  heap->phase_timings()->flush_par_workers_to_cycle();\n+  if (ShenandoahPacing) {\n+    heap->pacer()->flush_stats_to_cycle();\n+  }\n+\n+  ShenandoahEvacuationTracker* evac_tracker = heap->evac_tracker();\n+  ShenandoahCycleStats         evac_stats   = evac_tracker->flush_cycle_to_global();\n+\n+  \/\/ Print GC stats for current cycle\n+  {\n+    LogTarget(Info, gc, stats) lt;\n+    if (lt.is_enabled()) {\n+      ResourceMark rm;\n+      LogStream ls(lt);\n+      heap->phase_timings()->print_cycle_on(&ls);\n+      evac_tracker->print_evacuations_on(&ls, &evac_stats.workers,\n+                                              &evac_stats.mutators);\n+      if (ShenandoahPacing) {\n+        heap->pacer()->print_cycle_on(&ls);\n+      }\n+    }\n+  }\n+\n+  \/\/ Commit statistics to globals\n+  heap->phase_timings()->flush_cycle_to_global();\n+}\n+\n+\/\/ Young and old concurrent cycles are initiated by the regulator. Implicit\n+\/\/ and explicit GC requests are handled by the controller thread and always\n+\/\/ run a global cycle (which is concurrent by default, but may be overridden\n+\/\/ by command line options). Old cycles always degenerate to a global cycle.\n+\/\/ Young cycles are degenerated to complete the young cycle.  Young\n+\/\/ and old degen may upgrade to Full GC.  Full GC may also be\n+\/\/ triggered directly by a System.gc() invocation.\n+\/\/\n+\/\/\n+\/\/      +-----+ Idle +-----+-----------+---------------------+\n+\/\/      |         +        |           |                     |\n+\/\/      |         |        |           |                     |\n+\/\/      |         |        v           |                     |\n+\/\/      |         |  Bootstrap Old +-- | ------------+       |\n+\/\/      |         |   +                |             |       |\n+\/\/      |         |   |                |             |       |\n+\/\/      |         v   v                v             v       |\n+\/\/      |    Resume Old <----------+ Young +--> Young Degen  |\n+\/\/      |     +  +   ^                            +  +       |\n+\/\/      v     |  |   |                            |  |       |\n+\/\/   Global <-+  |   +----------------------------+  |       |\n+\/\/      +        |                                   |       |\n+\/\/      |        v                                   v       |\n+\/\/      +--->  Global Degen +--------------------> Full <----+\n+\/\/\n+void ShenandoahControlThread::service_concurrent_normal_cycle(ShenandoahHeap* heap,\n+                                                              const ShenandoahGenerationType generation,\n+                                                              GCCause::Cause cause) {\n+  GCIdMark gc_id_mark;\n+  ShenandoahGeneration* the_generation = nullptr;\n+  switch (generation) {\n+    case YOUNG: {\n+      \/\/ Run a young cycle. This might or might not, have interrupted an ongoing\n+      \/\/ concurrent mark in the old generation. We need to think about promotions\n+      \/\/ in this case. Promoted objects should be above the TAMS in the old regions\n+      \/\/ they end up in, but we have to be sure we don't promote into any regions\n+      \/\/ that are in the cset.\n+      log_info(gc, ergo)(\"Start GC cycle (YOUNG)\");\n+      the_generation = heap->young_generation();\n+      service_concurrent_cycle(the_generation, cause, false);\n+      break;\n+    }\n+    case OLD: {\n+      log_info(gc, ergo)(\"Start GC cycle (OLD)\");\n+      the_generation = heap->old_generation();\n+      service_concurrent_old_cycle(heap, cause);\n+      break;\n+    }\n+    case GLOBAL_GEN: {\n+      log_info(gc, ergo)(\"Start GC cycle (GLOBAL)\");\n+      the_generation = heap->global_generation();\n+      service_concurrent_cycle(the_generation, cause, false);\n+      break;\n+    }\n+    case GLOBAL_NON_GEN: {\n+      log_info(gc, ergo)(\"Start GC cycle\");\n+      the_generation = heap->global_generation();\n+      service_concurrent_cycle(the_generation, cause, false);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void ShenandoahControlThread::service_concurrent_old_cycle(ShenandoahHeap* heap, GCCause::Cause &cause) {\n+  ShenandoahOldGeneration* old_generation = heap->old_generation();\n+  ShenandoahYoungGeneration* young_generation = heap->young_generation();\n+  ShenandoahOldGeneration::State original_state = old_generation->state();\n+\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+\n+  switch (original_state) {\n+    case ShenandoahOldGeneration::FILLING: {\n+      _allow_old_preemption.set();\n+      old_generation->entry_coalesce_and_fill();\n+      _allow_old_preemption.unset();\n+\n+      \/\/ Before bootstrapping begins, we must acknowledge any cancellation request.\n+      \/\/ If the gc has not been cancelled, this does nothing. If it has been cancelled,\n+      \/\/ this will clear the cancellation request and exit before starting the bootstrap\n+      \/\/ phase. This will allow the young GC cycle to proceed normally. If we do not\n+      \/\/ acknowledge the cancellation request, the subsequent young cycle will observe\n+      \/\/ the request and essentially cancel itself.\n+      if (check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle)) {\n+        log_info(gc)(\"Preparation for old generation cycle was cancelled\");\n+        return;\n+      }\n+\n+      \/\/ Coalescing threads completed and nothing was cancelled. it is safe to transition from this state.\n+      old_generation->transition_to(ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP);\n+      return;\n+    }\n+    case ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP:\n+      old_generation->transition_to(ShenandoahOldGeneration::BOOTSTRAPPING);\n+    case ShenandoahOldGeneration::BOOTSTRAPPING: {\n+      \/\/ Configure the young generation's concurrent mark to put objects in\n+      \/\/ old regions into the concurrent mark queues associated with the old\n+      \/\/ generation. The young cycle will run as normal except that rather than\n+      \/\/ ignore old references it will mark and enqueue them in the old concurrent\n+      \/\/ task queues but it will not traverse them.\n+      set_gc_mode(bootstrapping_old);\n+      young_generation->set_old_gen_task_queues(old_generation->task_queues());\n+      ShenandoahGCSession session(cause, young_generation);\n+      service_concurrent_cycle(heap, young_generation, cause, true);\n+      process_phase_timings(heap);\n+      if (heap->cancelled_gc()) {\n+        \/\/ Young generation bootstrap cycle has failed. Concurrent mark for old generation\n+        \/\/ is going to resume after degenerated bootstrap cycle completes.\n+        log_info(gc)(\"Bootstrap cycle for old generation was cancelled\");\n+        return;\n+      }\n+\n+      \/\/ Reset the degenerated point. Normally this would happen at the top\n+      \/\/ of the control loop, but here we have just completed a young cycle\n+      \/\/ which has bootstrapped the old concurrent marking.\n+      _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n+\n+      \/\/ From here we will 'resume' the old concurrent mark. This will skip reset\n+      \/\/ and init mark for the concurrent mark. All of that work will have been\n+      \/\/ done by the bootstrapping young cycle.\n+      set_gc_mode(servicing_old);\n+      old_generation->transition_to(ShenandoahOldGeneration::MARKING);\n+    }\n+    case ShenandoahOldGeneration::MARKING: {\n+      ShenandoahGCSession session(cause, old_generation);\n+      bool marking_complete = resume_concurrent_old_cycle(old_generation, cause);\n+      if (marking_complete) {\n+        assert(old_generation->state() != ShenandoahOldGeneration::MARKING, \"Should not still be marking\");\n+        if (original_state == ShenandoahOldGeneration::MARKING) {\n+          heap->mmu_tracker()->record_old_marking_increment(true);\n+          heap->log_heap_status(\"At end of Concurrent Old Marking finishing increment\");\n+        }\n+      } else if (original_state == ShenandoahOldGeneration::MARKING) {\n+        heap->mmu_tracker()->record_old_marking_increment(false);\n+        heap->log_heap_status(\"At end of Concurrent Old Marking increment\");\n+      }\n+      break;\n+    }\n+    default:\n+      fatal(\"Unexpected state for old GC: %s\", ShenandoahOldGeneration::state_name(old_generation->state()));\n+  }\n+}\n+\n+bool ShenandoahControlThread::resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause) {\n+  assert(ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress(), \"Old mark should be in progress\");\n+  log_debug(gc)(\"Resuming old generation with \" UINT32_FORMAT \" marking tasks queued\", generation->task_queues()->tasks());\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  \/\/ We can only tolerate being cancelled during concurrent marking or during preparation for mixed\n+  \/\/ evacuation. This flag here (passed by reference) is used to control precisely where the regulator\n+  \/\/ is allowed to cancel a GC.\n+  ShenandoahOldGC gc(generation, _allow_old_preemption);\n+  if (gc.collect(cause)) {\n+    generation->record_success_concurrent(false);\n+  }\n+\n+  if (heap->cancelled_gc()) {\n+    \/\/ It's possible the gc cycle was cancelled after the last time\n+    \/\/ the collection checked for cancellation. In which case, the\n+    \/\/ old gc cycle is still completed, and we have to deal with this\n+    \/\/ cancellation. We set the degeneration point to be outside\n+    \/\/ the cycle because if this is an allocation failure, that is\n+    \/\/ what must be done (there is no degenerated old cycle). If the\n+    \/\/ cancellation was due to a heuristic wanting to start a young\n+    \/\/ cycle, then we are not actually going to a degenerated cycle,\n+    \/\/ so the degenerated point doesn't matter here.\n+    check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle);\n+    if (_requested_gc_cause == GCCause::_shenandoah_concurrent_gc) {\n+      heap->shenandoah_policy()->record_interrupted_old();\n+    }\n+    return false;\n+  }\n+  return true;\n+}\n+\n@@ -332,1 +617,1 @@\n-void ShenandoahControlThread::service_concurrent_normal_cycle(GCCause::Cause cause) {\n+void ShenandoahControlThread::service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause, bool do_old_gc_bootstrap) {\n@@ -368,1 +653,0 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -371,3 +655,2 @@\n-  GCIdMark gc_id_mark;\n-  ShenandoahGCSession session(cause);\n-\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahGCSession session(cause, generation);\n@@ -376,1 +659,8 @@\n-  ShenandoahConcurrentGC gc;\n+  service_concurrent_cycle(heap, generation, cause, do_old_gc_bootstrap);\n+}\n+\n+void ShenandoahControlThread::service_concurrent_cycle(ShenandoahHeap* heap,\n+                                                       ShenandoahGeneration* generation,\n+                                                       GCCause::Cause& cause,\n+                                                       bool do_old_gc_bootstrap) {\n+  ShenandoahConcurrentGC gc(generation, do_old_gc_bootstrap);\n@@ -379,0 +669,1 @@\n+<<<<<<< HEAD\n@@ -381,0 +672,3 @@\n+=======\n+    generation->record_success_concurrent(gc.abbreviated());\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -384,0 +678,38 @@\n+    assert(!generation->is_old(), \"Old GC takes a different control path\");\n+    \/\/ Concurrent young-gen collection degenerates to young\n+    \/\/ collection.  Same for global collections.\n+    _degen_generation = generation;\n+  }\n+  const char* msg;\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahMmuTracker* mmu_tracker = heap->mmu_tracker();\n+    if (generation->is_young()) {\n+      if (heap->cancelled_gc()) {\n+        msg = (do_old_gc_bootstrap) ? \"At end of Interrupted Concurrent Bootstrap GC\":\n+                                      \"At end of Interrupted Concurrent Young GC\";\n+      } else {\n+        \/\/ We only record GC results if GC was successful\n+        msg = (do_old_gc_bootstrap) ? \"At end of Concurrent Bootstrap GC\":\n+                                      \"At end of Concurrent Young GC\";\n+        if (heap->collection_set()->has_old_regions()) {\n+          mmu_tracker->record_mixed(get_gc_id());\n+        } else if (do_old_gc_bootstrap) {\n+          mmu_tracker->record_bootstrap(get_gc_id());\n+        } else {\n+          mmu_tracker->record_young(get_gc_id());\n+        }\n+      }\n+    } else {\n+      assert(generation->is_global(), \"If not young, must be GLOBAL\");\n+      assert(!do_old_gc_bootstrap, \"Do not bootstrap with GLOBAL GC\");\n+      if (heap->cancelled_gc()) {\n+        msg = \"At end of Interrupted Concurrent GLOBAL GC\";\n+      } else {\n+        \/\/ We only record GC results if GC was successful\n+        msg = \"At end of Concurrent Global GC\";\n+        mmu_tracker->record_global(get_gc_id());\n+      }\n+    }\n+  } else {\n+    msg = heap->cancelled_gc() ? \"At end of cancelled GC\" :\n+                                 \"At end of GC\";\n@@ -385,0 +717,1 @@\n+  heap->log_heap_status(msg);\n@@ -389,7 +722,28 @@\n-  if (heap->cancelled_gc()) {\n-    assert (is_alloc_failure_gc() || in_graceful_shutdown(), \"Cancel GC either for alloc failure GC, or gracefully exiting\");\n-    if (!in_graceful_shutdown()) {\n-      assert (_degen_point == ShenandoahGC::_degenerated_outside_cycle,\n-              \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n-      _degen_point = point;\n-    }\n+  if (!heap->cancelled_gc()) {\n+    return false;\n+  }\n+\n+  if (in_graceful_shutdown()) {\n+    return true;\n+  }\n+\n+  assert(_degen_point == ShenandoahGC::_degenerated_outside_cycle,\n+         \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n+\n+  if (is_alloc_failure_gc()) {\n+    _degen_point = point;\n+    _preemption_requested.unset();\n+    return true;\n+  }\n+\n+  if (_preemption_requested.is_set()) {\n+    assert(_requested_generation == YOUNG, \"Only young GCs may preempt old.\");\n+    _preemption_requested.unset();\n+\n+    \/\/ Old generation marking is only cancellable during concurrent marking.\n+    \/\/ Once final mark is complete, the code does not check again for cancellation.\n+    \/\/ If old generation was cancelled for an allocation failure, we wouldn't\n+    \/\/ make it to this case. The calling code is responsible for forcing a\n+    \/\/ cancellation due to allocation failure into a degenerated cycle.\n+    _degen_point = point;\n+    heap->clear_cancelled_gc(false \/* clear oom handler *\/);\n@@ -398,0 +752,2 @@\n+\n+  fatal(\"Cancel GC either for alloc failure GC, or gracefully exiting, or to pause old generation marking\");\n@@ -406,0 +762,2 @@\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n@@ -407,1 +765,1 @@\n-  ShenandoahGCSession session(cause);\n+  ShenandoahGCSession session(cause, heap->global_generation());\n@@ -413,2 +771,4 @@\n-void ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahGC::ShenandoahDegenPoint point) {\n-  assert (point != ShenandoahGC::_degenerated_unset, \"Degenerated point should be set\");\n+void ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause,\n+                                                            ShenandoahGC::ShenandoahDegenPoint point) {\n+  assert(point != ShenandoahGC::_degenerated_unset, \"Degenerated point should be set\");\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n@@ -417,1 +777,1 @@\n-  ShenandoahGCSession session(cause);\n+  ShenandoahGCSession session(cause, _degen_generation);\n@@ -419,1 +779,1 @@\n-  ShenandoahDegenGC gc(point);\n+  ShenandoahDegenGC gc(point, _degen_generation);\n@@ -421,0 +781,15 @@\n+<<<<<<< HEAD\n+=======\n+\n+  assert(heap->young_generation()->task_queues()->is_empty(), \"Unexpected young generation marking tasks\");\n+  if (_degen_generation->is_global()) {\n+    assert(heap->old_generation()->task_queues()->is_empty(), \"Unexpected old generation marking tasks\");\n+    assert(heap->global_generation()->task_queues()->is_empty(), \"Unexpected global generation marking tasks\");\n+  } else {\n+    assert(_degen_generation->is_young(), \"Expected degenerated young cycle, if not global.\");\n+    ShenandoahOldGeneration* old = heap->old_generation();\n+    if (old->state() == ShenandoahOldGeneration::BOOTSTRAPPING) {\n+      old->transition_to(ShenandoahOldGeneration::MARKING);\n+    }\n+  }\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -451,0 +826,6 @@\n+bool ShenandoahControlThread::is_implicit_gc(GCCause::Cause cause) const {\n+  return !is_explicit_gc(cause)\n+      && cause != GCCause::_shenandoah_concurrent_gc\n+      && cause != GCCause::_no_gc;\n+}\n+\n@@ -473,0 +854,63 @@\n+bool ShenandoahControlThread::request_concurrent_gc(ShenandoahGenerationType generation) {\n+  if (_preemption_requested.is_set() || _requested_gc_cause != GCCause::_no_gc || ShenandoahHeap::heap()->cancelled_gc()) {\n+    \/\/ Ignore subsequent requests from the heuristics\n+    log_debug(gc, thread)(\"Reject request for concurrent gc: preemption_requested: %s, gc_requested: %s, gc_cancelled: %s\",\n+                          BOOL_TO_STR(_preemption_requested.is_set()),\n+                          GCCause::to_string(_requested_gc_cause),\n+                          BOOL_TO_STR(ShenandoahHeap::heap()->cancelled_gc()));\n+    return false;\n+  }\n+\n+  if (gc_mode() == none) {\n+    GCCause::Cause existing = Atomic::cmpxchg(&_requested_gc_cause, GCCause::_no_gc, GCCause::_shenandoah_concurrent_gc);\n+    if (existing != GCCause::_no_gc) {\n+      log_debug(gc, thread)(\"Reject request for concurrent gc because another gc is pending: %s\", GCCause::to_string(existing));\n+      return false;\n+    }\n+\n+    _requested_generation = generation;\n+    notify_control_thread();\n+\n+    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n+    while (gc_mode() == none) {\n+      ml.wait();\n+    }\n+    return true;\n+  }\n+\n+  if (preempt_old_marking(generation)) {\n+    assert(gc_mode() == servicing_old, \"Expected to be servicing old, but was: %s.\", gc_mode_name(gc_mode()));\n+    GCCause::Cause existing = Atomic::cmpxchg(&_requested_gc_cause, GCCause::_no_gc, GCCause::_shenandoah_concurrent_gc);\n+    if (existing != GCCause::_no_gc) {\n+      log_debug(gc, thread)(\"Reject request to interrupt old gc because another gc is pending: %s\", GCCause::to_string(existing));\n+      return false;\n+    }\n+\n+    log_info(gc)(\"Preempting old generation mark to allow %s GC\", shenandoah_generation_name(generation));\n+    _requested_generation = generation;\n+    _preemption_requested.set();\n+    ShenandoahHeap::heap()->cancel_gc(GCCause::_shenandoah_concurrent_gc);\n+    notify_control_thread();\n+\n+    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n+    while (gc_mode() == servicing_old) {\n+      ml.wait();\n+    }\n+    return true;\n+  }\n+\n+  log_debug(gc, thread)(\"Reject request for concurrent gc: mode: %s, allow_old_preemption: %s\",\n+                        gc_mode_name(gc_mode()),\n+                        BOOL_TO_STR(_allow_old_preemption.is_set()));\n+  return false;\n+}\n+\n+void ShenandoahControlThread::notify_control_thread() {\n+  MonitorLocker locker(&_control_lock, Mutex::_no_safepoint_check_flag);\n+  _control_lock.notify();\n+}\n+\n+bool ShenandoahControlThread::preempt_old_marking(ShenandoahGenerationType generation) {\n+  return (generation == YOUNG) && _allow_old_preemption.try_unset();\n+}\n+\n@@ -487,5 +931,7 @@\n-    \/\/ Although setting gc request is under _gc_waiters_lock, but read side (run_service())\n-    \/\/ does not take the lock. We need to enforce following order, so that read side sees\n-    \/\/ latest requested gc cause when the flag is set.\n-    _requested_gc_cause = cause;\n-    _gc_requested.set();\n+    \/\/ This races with the regulator thread to start a concurrent gc and the\n+    \/\/ control thread to clear it at the start of a cycle. Threads here are\n+    \/\/ allowed to escalate a heuristic's request for concurrent gc.\n+    GCCause::Cause existing = Atomic::xchg(&_requested_gc_cause, cause);\n+    if (existing != GCCause::_no_gc) {\n+      log_debug(gc, thread)(\"GC request supersedes existing request: %s\", GCCause::to_string(existing));\n+    }\n@@ -493,0 +939,1 @@\n+    notify_control_thread();\n@@ -504,0 +951,1 @@\n+  bool is_humongous = req.size() > ShenandoahHeapRegion::region_size_words();\n@@ -505,1 +953,1 @@\n-  if (try_set_alloc_failure_gc()) {\n+  if (try_set_alloc_failure_gc(is_humongous)) {\n@@ -510,1 +958,0 @@\n-\n@@ -526,0 +973,1 @@\n+  bool is_humongous = (words > ShenandoahHeapRegion::region_size_words());\n@@ -527,1 +975,1 @@\n-  if (try_set_alloc_failure_gc()) {\n+  if (try_set_alloc_failure_gc(is_humongous)) {\n@@ -539,0 +987,1 @@\n+  _humongous_alloc_failure_gc.unset();\n@@ -543,1 +992,4 @@\n-bool ShenandoahControlThread::try_set_alloc_failure_gc() {\n+bool ShenandoahControlThread::try_set_alloc_failure_gc(bool is_humongous) {\n+  if (is_humongous) {\n+    _humongous_alloc_failure_gc.try_set();\n+  }\n@@ -552,1 +1004,0 @@\n-  _gc_requested.unset();\n@@ -559,0 +1010,1 @@\n+<<<<<<< HEAD\n@@ -562,0 +1014,7 @@\n+=======\n+\n+  \/\/ Update monitoring counters when we took a new region. This amortizes the\n+  \/\/ update costs on slow path.\n+  if (_do_counters_update.is_unset()) {\n+    _do_counters_update.set();\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -593,0 +1052,29 @@\n+\n+const char* ShenandoahControlThread::gc_mode_name(ShenandoahControlThread::GCMode mode) {\n+  switch (mode) {\n+    case none:              return \"idle\";\n+    case concurrent_normal: return \"normal\";\n+    case stw_degenerated:   return \"degenerated\";\n+    case stw_full:          return \"full\";\n+    case servicing_old:     return \"old\";\n+    case bootstrapping_old: return \"bootstrap\";\n+    default:                return \"unknown\";\n+  }\n+}\n+\n+void ShenandoahControlThread::set_gc_mode(ShenandoahControlThread::GCMode new_mode) {\n+  if (_mode != new_mode) {\n+    log_info(gc)(\"Transition from: %s to: %s\", gc_mode_name(_mode), gc_mode_name(new_mode));\n+    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n+    _mode = new_mode;\n+    ml.notify_all();\n+  }\n+}\n+\n+ShenandoahGenerationType ShenandoahControlThread::select_global_generation() {\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return GLOBAL_GEN;\n+  } else {\n+    return GLOBAL_NON_GEN;\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":600,"deletions":112,"binary":false,"changes":712,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -40,7 +41,0 @@\n-  typedef enum {\n-    none,\n-    concurrent_normal,\n-    stw_degenerated,\n-    stw_full\n-  } GCMode;\n-\n@@ -49,1 +43,1 @@\n-  \/\/ to make complete explicit cycle for for demanding customers.\n+  \/\/ to make complete explicit cycle for demanding customers.\n@@ -52,0 +46,7 @@\n+<<<<<<< HEAD\n+=======\n+  Monitor _control_lock;\n+  Monitor _regulator_lock;\n+  ShenandoahPeriodicTask _periodic_task;\n+  ShenandoahPeriodicPacerNotify _periodic_pacer_notify_task;\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -54,0 +55,9 @@\n+  typedef enum {\n+    none,\n+    concurrent_normal,\n+    stw_degenerated,\n+    stw_full,\n+    bootstrapping_old,\n+    servicing_old\n+  } GCMode;\n+\n@@ -57,0 +67,2 @@\n+  size_t get_gc_id();\n+\n@@ -58,1 +70,2 @@\n-  ShenandoahSharedFlag _gc_requested;\n+  ShenandoahSharedFlag _allow_old_preemption;\n+  ShenandoahSharedFlag _preemption_requested;\n@@ -60,0 +73,1 @@\n+  ShenandoahSharedFlag _humongous_alloc_failure_gc;\n@@ -61,0 +75,1 @@\n+<<<<<<< HEAD\n@@ -63,0 +78,7 @@\n+=======\n+  ShenandoahSharedFlag _do_counters_update;\n+  ShenandoahSharedFlag _force_counters_update;\n+\n+  GCCause::Cause  _requested_gc_cause;\n+  volatile ShenandoahGenerationType _requested_generation;\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -64,0 +86,1 @@\n+  ShenandoahGeneration* _degen_generation;\n@@ -70,0 +93,2 @@\n+  volatile GCMode _mode;\n+  shenandoah_padding(3);\n@@ -71,0 +96,1 @@\n+  \/\/ Returns true if the cycle has been cancelled or degenerated.\n@@ -72,1 +98,4 @@\n-  void service_concurrent_normal_cycle(GCCause::Cause cause);\n+\n+  \/\/ Returns true if the old generation marking completed (i.e., final mark executed for old generation).\n+  bool resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause);\n+  void service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause, bool reset_old_bitmap_specially);\n@@ -77,1 +106,4 @@\n-  bool try_set_alloc_failure_gc();\n+  \/\/ Return true if setting the flag which indicates allocation failure succeeds.\n+  bool try_set_alloc_failure_gc(bool is_humongous);\n+\n+  \/\/ Notify threads waiting for GC to complete.\n@@ -79,0 +111,2 @@\n+\n+  \/\/ True if allocation failure flag has been set.\n@@ -83,1 +117,0 @@\n-  size_t get_gc_id();\n@@ -92,0 +125,4 @@\n+  bool is_implicit_gc(GCCause::Cause cause) const;\n+\n+  \/\/ Returns true if the old generation marking was interrupted to allow a young cycle.\n+  bool preempt_old_marking(ShenandoahGenerationType generation);\n@@ -93,0 +130,1 @@\n+  \/\/ Returns true if the soft maximum heap has been changed using management APIs.\n@@ -95,0 +133,2 @@\n+  void process_phase_timings(const ShenandoahHeap* heap);\n+\n@@ -109,0 +149,2 @@\n+  \/\/ Return true if the request to start a concurrent GC for the given generation succeeded.\n+  bool request_concurrent_gc(ShenandoahGenerationType generation);\n@@ -117,0 +159,24 @@\n+\n+  void service_concurrent_normal_cycle(ShenandoahHeap* heap,\n+                                       const ShenandoahGenerationType generation,\n+                                       GCCause::Cause cause);\n+\n+  void service_concurrent_old_cycle(ShenandoahHeap* heap,\n+                                    GCCause::Cause &cause);\n+\n+  void set_gc_mode(GCMode new_mode);\n+  GCMode gc_mode() {\n+    return _mode;\n+  }\n+\n+  static ShenandoahGenerationType select_global_generation();\n+\n+ private:\n+  static const char* gc_mode_name(GCMode mode);\n+  void notify_control_thread();\n+\n+  void service_concurrent_cycle(ShenandoahHeap* heap,\n+                                ShenandoahGeneration* generation,\n+                                GCCause::Cause &cause,\n+                                bool do_old_gc_bootstrap);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.hpp","additions":78,"deletions":12,"binary":false,"changes":90,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -32,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -35,0 +37,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -40,0 +43,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -45,1 +49,1 @@\n-ShenandoahDegenGC::ShenandoahDegenGC(ShenandoahDegenPoint degen_point) :\n+ShenandoahDegenGC::ShenandoahDegenGC(ShenandoahDegenPoint degen_point, ShenandoahGeneration* generation) :\n@@ -48,0 +52,4 @@\n+<<<<<<< HEAD\n+=======\n+  _generation(generation),\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -53,0 +61,7 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  if (heap->mode()->is_generational()) {\n+    bool is_bootstrap_gc = heap->old_generation()->state() == ShenandoahOldGeneration::BOOTSTRAPPING;\n+    heap->mmu_tracker()->record_degenerated(GCId::current(), is_bootstrap_gc);\n+    const char* msg = is_bootstrap_gc? \"At end of Degenerated Bootstrap Old GC\": \"At end of Degenerated Young GC\";\n+    heap->log_heap_status(msg);\n+  }\n@@ -68,1 +83,0 @@\n-\n@@ -83,1 +97,21 @@\n-  heap->clear_cancelled_gc();\n+  heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n+\n+#ifdef ASSERT\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahOldGeneration* old_generation = heap->old_generation();\n+    if (!heap->is_concurrent_old_mark_in_progress()) {\n+      \/\/ If we are not marking the old generation, there should be nothing in the old mark queues\n+      assert(old_generation->task_queues()->is_empty(), \"Old gen task queues should be empty\");\n+    }\n+\n+    if (_generation->is_global()) {\n+      \/\/ If we are in a global cycle, the old generation should not be marking. It is, however,\n+      \/\/ allowed to be holding regions for evacuation or coalescing.\n+      ShenandoahOldGeneration::State state = old_generation->state();\n+      assert(state == ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP\n+             || state == ShenandoahOldGeneration::EVACUATING\n+             || state == ShenandoahOldGeneration::FILLING,\n+             \"Old generation cannot be in state: %s\", old_generation->state_name());\n+    }\n+  }\n+#endif\n@@ -99,1 +133,0 @@\n-      \/\/\n@@ -101,0 +134,13 @@\n+      \/\/ Note that we can only do this for \"outside-cycle\" degens, otherwise we would risk\n+      \/\/ changing the cycle parameters mid-cycle during concurrent -> degenerated handover.\n+      heap->set_unload_classes(_generation->heuristics()->can_unload_classes() &&\n+                                (!heap->mode()->is_generational() || _generation->is_global()));\n+\n+      if (heap->mode()->is_generational() &&\n+            (_generation->is_young() || (_generation->is_global() && ShenandoahVerify))) {\n+        \/\/ Swap remembered sets for young, or if the verifier will run during a global collect\n+        \/\/ TODO: This path should not depend on ShenandoahVerify\n+        _generation->swap_remembered_set();\n+      }\n+\n+    case _degenerated_roots:\n@@ -102,3 +148,22 @@\n-      if (heap->is_concurrent_mark_in_progress()) {\n-        ShenandoahConcurrentMark::cancel();\n-        heap->set_concurrent_mark_in_progress(false);\n+      if (!heap->mode()->is_generational()) {\n+        if (heap->is_concurrent_mark_in_progress()) {\n+          heap->cancel_concurrent_mark();\n+        }\n+      } else {\n+        if (_generation->is_concurrent_mark_in_progress()) {\n+          \/\/ We want to allow old generation marking to be punctuated by young collections\n+          \/\/ (even if they have degenerated). If this is a global cycle, we'd have cancelled\n+          \/\/ the entire old gc before coming into this switch. Note that cancel_marking on\n+          \/\/ the generation does NOT abandon incomplete SATB buffers as cancel_concurrent_mark does.\n+          \/\/ We need to separate out the old pointers which is done below.\n+          _generation->cancel_marking();\n+        }\n+\n+        if (heap->is_concurrent_mark_in_progress()) {\n+          \/\/ If either old or young marking is in progress, the SATB barrier will be enabled.\n+          \/\/ The SATB buffer may hold a mix of old and young pointers. The old pointers need to be\n+          \/\/ transferred to the old generation mark queues and the young pointers are NOT part\n+          \/\/ of this snapshot, so they must be dropped here. It is safe to drop them here because\n+          \/\/ we will rescan the roots on this safepoint.\n+          heap->transfer_old_pointers_from_satb();\n+        }\n@@ -107,3 +172,8 @@\n-      \/\/ Note that we can only do this for \"outside-cycle\" degens, otherwise we would risk\n-      \/\/ changing the cycle parameters mid-cycle during concurrent -> degenerated handover.\n-      heap->set_unload_classes(heap->heuristics()->can_unload_classes());\n+      if (_degen_point == ShenandoahDegenPoint::_degenerated_roots) {\n+        \/\/ We only need this if the concurrent cycle has already swapped the card tables.\n+        \/\/ Marking will use the 'read' table, but interesting pointers may have been\n+        \/\/ recorded in the 'write' table in the time between the cancelled concurrent cycle\n+        \/\/ and this degenerated cycle. These pointers need to be included the 'read' table\n+        \/\/ used to scan the remembered set during the STW mark which follows here.\n+        _generation->merge_write_table();\n+      }\n@@ -173,1 +243,0 @@\n-\n@@ -192,0 +261,5 @@\n+      \/\/ Update collector state regardless of whether or not there are forwarded objects\n+      heap->set_evacuation_in_progress(false);\n+      heap->set_concurrent_weak_root_in_progress(false);\n+      heap->set_concurrent_strong_root_in_progress(false);\n+\n@@ -212,0 +286,7 @@\n+      if (heap->mode()->is_generational() && heap->is_concurrent_old_mark_in_progress()) {\n+        \/\/ This is still necessary for degenerated cycles because the degeneration point may occur\n+        \/\/ after final mark of the young generation. See ShenandoahConcurrentGC::op_final_updaterefs for\n+        \/\/ a more detailed explanation.\n+        heap->transfer_old_pointers_from_satb();\n+      }\n+\n@@ -213,0 +294,35 @@\n+      \/\/ We defer generation resizing actions until after cset regions have been recycled.\n+      if (heap->mode()->is_generational()) {\n+        size_t old_region_surplus = heap->get_old_region_surplus();\n+        size_t old_region_deficit = heap->get_old_region_deficit();\n+        bool success;\n+        size_t region_xfer;\n+        const char* region_destination;\n+        if (old_region_surplus) {\n+          region_xfer = old_region_surplus;\n+          region_destination = \"young\";\n+          success = heap->generation_sizer()->transfer_to_young(old_region_surplus);\n+        } else if (old_region_deficit) {\n+          region_xfer = old_region_surplus;\n+          region_destination = \"old\";\n+          success = heap->generation_sizer()->transfer_to_old(old_region_deficit);\n+          if (!success) {\n+            heap->old_heuristics()->trigger_cannot_expand();\n+          }\n+        } else {\n+          region_destination = \"none\";\n+          region_xfer = 0;\n+          success = true;\n+        }\n+\n+        size_t young_available = heap->young_generation()->available();\n+        size_t old_available = heap->old_generation()->available();\n+        log_info(gc, ergo)(\"After cleanup, %s \" SIZE_FORMAT \" regions to %s to prepare for next gc, old available: \"\n+                           SIZE_FORMAT \"%s, young_available: \" SIZE_FORMAT \"%s\",\n+                           success? \"successfully transferred\": \"failed to transfer\", region_xfer, region_destination,\n+                           byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n+                           byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n+\n+        heap->set_old_region_surplus(0);\n+        heap->set_old_region_deficit(0);\n+      }\n@@ -218,0 +334,8 @@\n+  if (heap->mode()->is_generational()) {\n+    \/\/ In case degeneration interrupted concurrent evacuation or update references, we need to clean up transient state.\n+    \/\/ Otherwise, these actions have no effect.\n+    heap->set_young_evac_reserve(0);\n+    heap->set_old_evac_reserve(0);\n+    heap->set_promoted_reserve(0);\n+  }\n+\n@@ -236,0 +360,1 @@\n+<<<<<<< HEAD\n@@ -238,0 +363,4 @@\n+=======\n+    heap->shenandoah_policy()->record_success_degenerated(_generation->is_young(), _abbreviated);\n+    _generation->heuristics()->record_success_degenerated();\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -242,1 +371,1 @@\n-  ShenandoahHeap::heap()->prepare_gc();\n+  _generation->prepare_gc();\n@@ -246,1 +375,1 @@\n-  assert(!ShenandoahHeap::heap()->is_concurrent_mark_in_progress(), \"Should be reset\");\n+  assert(!_generation->is_concurrent_mark_in_progress(), \"Should be reset\");\n@@ -248,2 +377,1 @@\n-  ShenandoahSTWMark mark(false \/*full gc*\/);\n-  mark.clear();\n+  ShenandoahSTWMark mark(_generation, false \/*full gc*\/);\n@@ -254,1 +382,1 @@\n-  ShenandoahConcurrentMark mark;\n+  ShenandoahConcurrentMark mark(_generation);\n@@ -266,0 +394,1 @@\n+\n@@ -267,1 +396,1 @@\n-  heap->prepare_regions_and_collection_set(false \/*concurrent*\/);\n+  _generation->prepare_regions_and_collection_set(false \/*concurrent*\/);\n@@ -279,1 +408,10 @@\n-  if (!heap->collection_set()->is_empty()) {\n+  size_t humongous_regions_promoted = heap->get_promotable_humongous_regions();\n+  size_t regular_regions_promoted_in_place = heap->get_regular_regions_promoted_in_place();\n+  if (!heap->collection_set()->is_empty() || (humongous_regions_promoted + regular_regions_promoted_in_place > 0)) {\n+    \/\/ Even if the collection set is empty, we need to do evacuation if there are regions to be promoted in place.\n+    \/\/ Degenerated evacuation takes responsibility for registering objects and setting the remembered set cards to dirty.\n+\n+    if (ShenandoahVerify) {\n+      heap->verifier()->verify_before_evacuation();\n+    }\n+\n@@ -281,1 +419,0 @@\n-    heap->set_has_forwarded_objects(true);\n@@ -286,0 +423,2 @@\n+\n+    heap->set_has_forwarded_objects(!heap->collection_set()->is_empty());\n@@ -309,4 +448,0 @@\n-  heap->set_evacuation_in_progress(false);\n-  heap->set_concurrent_weak_root_in_progress(false);\n-  heap->set_concurrent_strong_root_in_progress(false);\n-\n@@ -361,1 +496,1 @@\n-      return \"Pause Degenerated GC (<UNSET>)\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (<UNSET>)\");\n@@ -363,1 +498,3 @@\n-      return \"Pause Degenerated GC (Outside of Cycle)\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (Outside of Cycle)\");\n+    case _degenerated_roots:\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (Roots)\");\n@@ -365,1 +502,1 @@\n-      return \"Pause Degenerated GC (Mark)\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (Mark)\");\n@@ -367,1 +504,1 @@\n-      return \"Pause Degenerated GC (Evacuation)\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (Evacuation)\");\n@@ -369,1 +506,1 @@\n-      return \"Pause Degenerated GC (Update Refs)\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (Update Refs)\");\n@@ -372,1 +509,1 @@\n-      return \"ERROR\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (?)\");\n@@ -377,0 +514,1 @@\n+<<<<<<< HEAD\n@@ -378,0 +516,3 @@\n+=======\n+  log_info(gc)(\"Degenerate GC upgrading to Full GC\");\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.cpp","additions":170,"deletions":29,"binary":false,"changes":199,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+class ShenandoahGeneration;\n@@ -36,0 +37,4 @@\n+<<<<<<< HEAD\n+=======\n+  ShenandoahGeneration* _generation;\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -39,1 +44,1 @@\n-  ShenandoahDegenGC(ShenandoahDegenPoint degen_point);\n+  ShenandoahDegenGC(ShenandoahDegenPoint degen_point, ShenandoahGeneration* generation);\n@@ -63,0 +68,1 @@\n+<<<<<<< HEAD\n@@ -64,0 +70,2 @@\n+=======\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.hpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -37,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n@@ -39,0 +41,1 @@\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n@@ -47,0 +50,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -55,0 +59,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -66,0 +71,63 @@\n+\/\/ After Full GC is done, reconstruct the remembered set by iterating over OLD regions,\n+\/\/ registering all objects between bottom() and top(), and setting remembered set cards to\n+\/\/ DIRTY if they hold interesting pointers.\n+class ShenandoahReconstructRememberedSetTask : public WorkerTask {\n+private:\n+  ShenandoahRegionIterator _regions;\n+\n+public:\n+  ShenandoahReconstructRememberedSetTask() :\n+    WorkerTask(\"Shenandoah Reset Bitmap\") { }\n+\n+  void work(uint worker_id) {\n+    ShenandoahParallelWorkerSession worker_session(worker_id);\n+    ShenandoahHeapRegion* r = _regions.next();\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    RememberedScanner* scanner = heap->card_scan();\n+    ShenandoahSetRememberedCardsToDirtyClosure dirty_cards_for_interesting_pointers;\n+\n+    while (r != nullptr) {\n+      if (r->is_old() && r->is_active()) {\n+        HeapWord* obj_addr = r->bottom();\n+        if (r->is_humongous_start()) {\n+          \/\/ First, clear the remembered set\n+          oop obj = cast_to_oop(obj_addr);\n+          size_t size = obj->size();\n+\n+          \/\/ First, clear the remembered set for all spanned humongous regions\n+          size_t num_regions = ShenandoahHeapRegion::required_regions(size * HeapWordSize);\n+          size_t region_span = num_regions * ShenandoahHeapRegion::region_size_words();\n+          scanner->reset_remset(r->bottom(), region_span);\n+          size_t region_index = r->index();\n+          ShenandoahHeapRegion* humongous_region = heap->get_region(region_index);\n+          while (num_regions-- != 0) {\n+            scanner->reset_object_range(humongous_region->bottom(), humongous_region->end());\n+            region_index++;\n+            humongous_region = heap->get_region(region_index);\n+          }\n+\n+          \/\/ Then register the humongous object and DIRTY relevant remembered set cards\n+          scanner->register_object_without_lock(obj_addr);\n+          obj->oop_iterate(&dirty_cards_for_interesting_pointers);\n+        } else if (!r->is_humongous()) {\n+          \/\/ First, clear the remembered set\n+          scanner->reset_remset(r->bottom(), ShenandoahHeapRegion::region_size_words());\n+          scanner->reset_object_range(r->bottom(), r->end());\n+\n+          \/\/ Then iterate over all objects, registering object and DIRTYing relevant remembered set cards\n+          HeapWord* t = r->top();\n+          while (obj_addr < t) {\n+            oop obj = cast_to_oop(obj_addr);\n+            size_t size = obj->size();\n+            scanner->register_object_without_lock(obj_addr);\n+            obj_addr += obj->oop_iterate_size(&dirty_cards_for_interesting_pointers);\n+          }\n+        } \/\/ else, ignore humongous continuation region\n+      }\n+      \/\/ else, this region is FREE or YOUNG or inactive and we can ignore it.\n+      \/\/ TODO: Assert this.\n+      r = _regions.next();\n+    }\n+  }\n+};\n+\n@@ -103,0 +171,1 @@\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n@@ -112,1 +181,27 @@\n-\n+  if (heap->mode()->is_generational()) {\n+    \/\/ Full GC should reset time since last gc for young and old heuristics\n+    heap->young_generation()->heuristics()->record_cycle_end();\n+    heap->old_generation()->heuristics()->record_cycle_end();\n+\n+    heap->mmu_tracker()->record_full(GCId::current());\n+    heap->log_heap_status(\"At end of Full GC\");\n+\n+    assert(heap->old_generation()->state() == ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP,\n+           \"After full GC, old generation should be waiting for bootstrap.\");\n+\n+    \/\/ Since we allow temporary violation of these constraints during Full GC, we want to enforce that the assertions are\n+    \/\/ made valid by the time Full GC completes.\n+    assert(heap->old_generation()->used_regions_size() <= heap->old_generation()->max_capacity(),\n+           \"Old generation affiliated regions must be less than capacity\");\n+    assert(heap->young_generation()->used_regions_size() <= heap->young_generation()->max_capacity(),\n+           \"Young generation affiliated regions must be less than capacity\");\n+\n+    assert((heap->young_generation()->used() + heap->young_generation()->get_humongous_waste())\n+           <= heap->young_generation()->used_regions_size(), \"Young consumed can be no larger than span of affiliated regions\");\n+    assert((heap->old_generation()->used() + heap->old_generation()->get_humongous_waste())\n+           <= heap->old_generation()->used_regions_size(), \"Old consumed can be no larger than span of affiliated regions\");\n+\n+    \/\/ Establish baseline for next old-has-grown trigger.\n+    heap->old_generation()->set_live_bytes_after_last_mark(heap->old_generation()->used() +\n+                                                           heap->old_generation()->get_humongous_waste());\n+  }\n@@ -122,0 +217,1 @@\n+<<<<<<< HEAD\n@@ -123,0 +219,3 @@\n+=======\n+  heap->global_generation()->heuristics()->record_success_full();\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -128,0 +227,12 @@\n+  \/\/ Since we may arrive here from degenerated GC failure of either young or old, establish generation as GLOBAL.\n+  heap->set_gc_generation(heap->global_generation());\n+\n+  if (heap->mode()->is_generational()) {\n+    \/\/ No need for old_gen->increase_used() as this was done when plabs were allocated.\n+    heap->set_young_evac_reserve(0);\n+    heap->set_old_evac_reserve(0);\n+    heap->set_promoted_reserve(0);\n+\n+    \/\/ Full GC supersedes any marking or coalescing in old generation.\n+    heap->cancel_old_gc();\n+  }\n@@ -171,1 +282,1 @@\n-    \/\/ b. Cancel concurrent mark, if in progress\n+    \/\/ b. Cancel all concurrent marks, if in progress\n@@ -173,2 +284,1 @@\n-      ShenandoahConcurrentGC::cancel();\n-      heap->set_concurrent_mark_in_progress(false);\n+      heap->cancel_concurrent_mark();\n@@ -184,1 +294,1 @@\n-    heap->reset_mark_bitmap();\n+    heap->global_generation()->reset_mark_bitmap();\n@@ -186,1 +296,1 @@\n-    assert(!heap->marking_context()->is_complete(), \"sanity\");\n+    assert(!heap->global_generation()->is_mark_complete(), \"sanity\");\n@@ -189,1 +299,1 @@\n-    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+    ShenandoahReferenceProcessor* rp = heap->global_generation()->ref_processor();\n@@ -195,0 +305,9 @@\n+    if (heap->mode()->is_generational()) {\n+      for (size_t i = 0; i < heap->num_regions(); i++) {\n+        ShenandoahHeapRegion* r = heap->get_region(i);\n+        if (r->get_top_before_promote() != nullptr) {\n+          r->restore_top_before_promote();\n+        }\n+      }\n+    }\n+\n@@ -202,0 +321,1 @@\n+    \/\/ TODO: Do we need to explicitly retire PLABs?\n@@ -238,0 +358,2 @@\n+\n+    phase5_epilog();\n@@ -242,0 +364,1 @@\n+    \/\/ TODO: Merge with phase5_epilog?\n@@ -244,0 +367,6 @@\n+\n+    if (heap->mode()->is_generational()) {\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::full_gc_reconstruct_remembered_set);\n+      ShenandoahReconstructRememberedSetTask task;\n+      heap->workers()->run_task(&task);\n+    }\n@@ -262,0 +391,1 @@\n+  \/\/ Humongous regions are promoted on demand and are accounted for by normal Full GC mechanisms.\n@@ -280,2 +410,4 @@\n-    _ctx->capture_top_at_mark_start(r);\n-    r->clear_live_data();\n+    if (r->affiliation() != FREE) {\n+      _ctx->capture_top_at_mark_start(r);\n+      r->clear_live_data();\n+    }\n@@ -283,0 +415,2 @@\n+\n+  bool is_thread_safe() { return true; }\n@@ -292,1 +426,1 @@\n-  heap->heap_region_iterate(&cl);\n+  heap->parallel_heap_region_iterate(&cl);\n@@ -294,1 +428,1 @@\n-  heap->set_unload_classes(heap->heuristics()->can_unload_classes());\n+  heap->set_unload_classes(heap->global_generation()->heuristics()->can_unload_classes());\n@@ -296,1 +430,1 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = heap->global_generation()->ref_processor();\n@@ -300,1 +434,1 @@\n-  ShenandoahSTWMark mark(true \/*full_gc*\/);\n+  ShenandoahSTWMark mark(heap->global_generation(), true \/*full_gc*\/);\n@@ -303,0 +437,9 @@\n+\n+  size_t live_bytes_in_old = 0;\n+  for (size_t i = 0; i < heap->num_regions(); i++) {\n+    ShenandoahHeapRegion* r = heap->get_region(i);\n+    if (r->is_old()) {\n+      live_bytes_in_old += r->get_live_data_bytes();\n+    }\n+  }\n+  log_info(gc)(\"Live bytes in old after STW mark: \" PROPERFMT, PROPERFMTARGS(live_bytes_in_old));\n@@ -305,0 +448,235 @@\n+class ShenandoahPrepareForCompactionTask : public WorkerTask {\n+private:\n+  PreservedMarksSet*        const _preserved_marks;\n+  ShenandoahHeap*           const _heap;\n+  ShenandoahHeapRegionSet** const _worker_slices;\n+  size_t                    const _num_workers;\n+\n+public:\n+  ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks,\n+                                     ShenandoahHeapRegionSet **worker_slices,\n+                                     size_t num_workers);\n+\n+  static bool is_candidate_region(ShenandoahHeapRegion* r) {\n+    \/\/ Empty region: get it into the slice to defragment the slice itself.\n+    \/\/ We could have skipped this without violating correctness, but we really\n+    \/\/ want to compact all live regions to the start of the heap, which sometimes\n+    \/\/ means moving them into the fully empty regions.\n+    if (r->is_empty()) return true;\n+\n+    \/\/ Can move the region, and this is not the humongous region. Humongous\n+    \/\/ moves are special cased here, because their moves are handled separately.\n+    return r->is_stw_move_allowed() && !r->is_humongous();\n+  }\n+\n+  void work(uint worker_id);\n+};\n+\n+class ShenandoahPrepareForGenerationalCompactionObjectClosure : public ObjectClosure {\n+private:\n+  PreservedMarks*          const _preserved_marks;\n+  ShenandoahHeap*          const _heap;\n+  uint                           _tenuring_threshold;\n+\n+  \/\/ _empty_regions is a thread-local list of heap regions that have been completely emptied by this worker thread's\n+  \/\/ compaction efforts.  The worker thread that drives these efforts adds compacted regions to this list if the\n+  \/\/ region has not been compacted onto itself.\n+  GrowableArray<ShenandoahHeapRegion*>& _empty_regions;\n+  int _empty_regions_pos;\n+  ShenandoahHeapRegion*          _old_to_region;\n+  ShenandoahHeapRegion*          _young_to_region;\n+  ShenandoahHeapRegion*          _from_region;\n+  ShenandoahAffiliation          _from_affiliation;\n+  HeapWord*                      _old_compact_point;\n+  HeapWord*                      _young_compact_point;\n+  uint                           _worker_id;\n+\n+public:\n+  ShenandoahPrepareForGenerationalCompactionObjectClosure(PreservedMarks* preserved_marks,\n+                                                          GrowableArray<ShenandoahHeapRegion*>& empty_regions,\n+                                                          ShenandoahHeapRegion* old_to_region,\n+                                                          ShenandoahHeapRegion* young_to_region, uint worker_id) :\n+      _preserved_marks(preserved_marks),\n+      _heap(ShenandoahHeap::heap()),\n+      _tenuring_threshold(0),\n+      _empty_regions(empty_regions),\n+      _empty_regions_pos(0),\n+      _old_to_region(old_to_region),\n+      _young_to_region(young_to_region),\n+      _from_region(nullptr),\n+      _old_compact_point((old_to_region != nullptr)? old_to_region->bottom(): nullptr),\n+      _young_compact_point((young_to_region != nullptr)? young_to_region->bottom(): nullptr),\n+      _worker_id(worker_id) {\n+    if (_heap->mode()->is_generational()) {\n+      _tenuring_threshold = _heap->age_census()->tenuring_threshold();\n+    }\n+  }\n+\n+  void set_from_region(ShenandoahHeapRegion* from_region) {\n+    _from_region = from_region;\n+    _from_affiliation = from_region->affiliation();\n+    if (_from_region->has_live()) {\n+      if (_from_affiliation == ShenandoahAffiliation::OLD_GENERATION) {\n+        if (_old_to_region == nullptr) {\n+          _old_to_region = from_region;\n+          _old_compact_point = from_region->bottom();\n+        }\n+      } else {\n+        assert(_from_affiliation == ShenandoahAffiliation::YOUNG_GENERATION, \"from_region must be OLD or YOUNG\");\n+        if (_young_to_region == nullptr) {\n+          _young_to_region = from_region;\n+          _young_compact_point = from_region->bottom();\n+        }\n+      }\n+    } \/\/ else, we won't iterate over this _from_region so we don't need to set up to region to hold copies\n+  }\n+\n+  void finish() {\n+    finish_old_region();\n+    finish_young_region();\n+  }\n+\n+  void finish_old_region() {\n+    if (_old_to_region != nullptr) {\n+      log_debug(gc)(\"Planned compaction into Old Region \" SIZE_FORMAT \", used: \" SIZE_FORMAT \" tabulated by worker %u\",\n+                    _old_to_region->index(), _old_compact_point - _old_to_region->bottom(), _worker_id);\n+      _old_to_region->set_new_top(_old_compact_point);\n+      _old_to_region = nullptr;\n+    }\n+  }\n+\n+  void finish_young_region() {\n+    if (_young_to_region != nullptr) {\n+      log_debug(gc)(\"Worker %u planned compaction into Young Region \" SIZE_FORMAT \", used: \" SIZE_FORMAT,\n+                    _worker_id, _young_to_region->index(), _young_compact_point - _young_to_region->bottom());\n+      _young_to_region->set_new_top(_young_compact_point);\n+      _young_to_region = nullptr;\n+    }\n+  }\n+\n+  bool is_compact_same_region() {\n+    return (_from_region == _old_to_region) || (_from_region == _young_to_region);\n+  }\n+\n+  int empty_regions_pos() {\n+    return _empty_regions_pos;\n+  }\n+\n+  void do_object(oop p) {\n+    assert(_from_region != nullptr, \"must set before work\");\n+    assert((_from_region->bottom() <= cast_from_oop<HeapWord*>(p)) && (cast_from_oop<HeapWord*>(p) < _from_region->top()),\n+           \"Object must reside in _from_region\");\n+    assert(_heap->complete_marking_context()->is_marked(p), \"must be marked\");\n+    assert(!_heap->complete_marking_context()->allocated_after_mark_start(p), \"must be truly marked\");\n+\n+    size_t obj_size = p->size();\n+    uint from_region_age = _from_region->age();\n+    uint object_age = p->age();\n+\n+    bool promote_object = false;\n+    if ((_from_affiliation == ShenandoahAffiliation::YOUNG_GENERATION) &&\n+        (from_region_age + object_age >= _tenuring_threshold)) {\n+      if ((_old_to_region != nullptr) && (_old_compact_point + obj_size > _old_to_region->end())) {\n+        finish_old_region();\n+        _old_to_region = nullptr;\n+      }\n+      if (_old_to_region == nullptr) {\n+        if (_empty_regions_pos < _empty_regions.length()) {\n+          ShenandoahHeapRegion* new_to_region = _empty_regions.at(_empty_regions_pos);\n+          _empty_regions_pos++;\n+          new_to_region->set_affiliation(OLD_GENERATION);\n+          _old_to_region = new_to_region;\n+          _old_compact_point = _old_to_region->bottom();\n+          promote_object = true;\n+        }\n+        \/\/ Else this worker thread does not yet have any empty regions into which this aged object can be promoted so\n+        \/\/ we leave promote_object as false, deferring the promotion.\n+      } else {\n+        promote_object = true;\n+      }\n+    }\n+\n+    if (promote_object || (_from_affiliation == ShenandoahAffiliation::OLD_GENERATION)) {\n+      assert(_old_to_region != nullptr, \"_old_to_region should not be nullptr when evacuating to OLD region\");\n+      if (_old_compact_point + obj_size > _old_to_region->end()) {\n+        ShenandoahHeapRegion* new_to_region;\n+\n+        log_debug(gc)(\"Worker %u finishing old region \" SIZE_FORMAT \", compact_point: \" PTR_FORMAT \", obj_size: \" SIZE_FORMAT\n+                      \", &compact_point[obj_size]: \" PTR_FORMAT \", region end: \" PTR_FORMAT,  _worker_id, _old_to_region->index(),\n+                      p2i(_old_compact_point), obj_size, p2i(_old_compact_point + obj_size), p2i(_old_to_region->end()));\n+\n+        \/\/ Object does not fit.  Get a new _old_to_region.\n+        finish_old_region();\n+        if (_empty_regions_pos < _empty_regions.length()) {\n+          new_to_region = _empty_regions.at(_empty_regions_pos);\n+          _empty_regions_pos++;\n+          new_to_region->set_affiliation(OLD_GENERATION);\n+        } else {\n+          \/\/ If we've exhausted the previously selected _old_to_region, we know that the _old_to_region is distinct\n+          \/\/ from _from_region.  That's because there is always room for _from_region to be compacted into itself.\n+          \/\/ Since we're out of empty regions, let's use _from_region to hold the results of its own compaction.\n+          new_to_region = _from_region;\n+        }\n+\n+        assert(new_to_region != _old_to_region, \"must not reuse same OLD to-region\");\n+        assert(new_to_region != nullptr, \"must not be nullptr\");\n+        _old_to_region = new_to_region;\n+        _old_compact_point = _old_to_region->bottom();\n+      }\n+\n+      \/\/ Object fits into current region, record new location:\n+      assert(_old_compact_point + obj_size <= _old_to_region->end(), \"must fit\");\n+      shenandoah_assert_not_forwarded(nullptr, p);\n+      _preserved_marks->push_if_necessary(p, p->mark());\n+      p->forward_to(cast_to_oop(_old_compact_point));\n+      _old_compact_point += obj_size;\n+    } else {\n+      assert(_from_affiliation == ShenandoahAffiliation::YOUNG_GENERATION,\n+             \"_from_region must be OLD_GENERATION or YOUNG_GENERATION\");\n+      assert(_young_to_region != nullptr, \"_young_to_region should not be nullptr when compacting YOUNG _from_region\");\n+\n+      \/\/ After full gc compaction, all regions have age 0.  Embed the region's age into the object's age in order to preserve\n+      \/\/ tenuring progress.\n+      if (_heap->is_aging_cycle()) {\n+        _heap->increase_object_age(p, from_region_age + 1);\n+      } else {\n+        _heap->increase_object_age(p, from_region_age);\n+      }\n+\n+      if (_young_compact_point + obj_size > _young_to_region->end()) {\n+        ShenandoahHeapRegion* new_to_region;\n+\n+        log_debug(gc)(\"Worker %u finishing young region \" SIZE_FORMAT \", compact_point: \" PTR_FORMAT \", obj_size: \" SIZE_FORMAT\n+                      \", &compact_point[obj_size]: \" PTR_FORMAT \", region end: \" PTR_FORMAT,  _worker_id, _young_to_region->index(),\n+                      p2i(_young_compact_point), obj_size, p2i(_young_compact_point + obj_size), p2i(_young_to_region->end()));\n+\n+        \/\/ Object does not fit.  Get a new _young_to_region.\n+        finish_young_region();\n+        if (_empty_regions_pos < _empty_regions.length()) {\n+          new_to_region = _empty_regions.at(_empty_regions_pos);\n+          _empty_regions_pos++;\n+          new_to_region->set_affiliation(YOUNG_GENERATION);\n+        } else {\n+          \/\/ If we've exhausted the previously selected _young_to_region, we know that the _young_to_region is distinct\n+          \/\/ from _from_region.  That's because there is always room for _from_region to be compacted into itself.\n+          \/\/ Since we're out of empty regions, let's use _from_region to hold the results of its own compaction.\n+          new_to_region = _from_region;\n+        }\n+\n+        assert(new_to_region != _young_to_region, \"must not reuse same OLD to-region\");\n+        assert(new_to_region != nullptr, \"must not be nullptr\");\n+        _young_to_region = new_to_region;\n+        _young_compact_point = _young_to_region->bottom();\n+      }\n+\n+      \/\/ Object fits into current region, record new location:\n+      assert(_young_compact_point + obj_size <= _young_to_region->end(), \"must fit\");\n+      shenandoah_assert_not_forwarded(nullptr, p);\n+      _preserved_marks->push_if_necessary(p, p->mark());\n+      p->forward_to(cast_to_oop(_young_compact_point));\n+      _young_compact_point += obj_size;\n+    }\n+  }\n+};\n+\n+\n@@ -333,0 +711,1 @@\n+    assert(!_heap->mode()->is_generational(), \"Generational GC should use different Closure\");\n@@ -378,5 +757,0 @@\n-class ShenandoahPrepareForCompactionTask : public WorkerTask {\n-private:\n-  PreservedMarksSet*        const _preserved_marks;\n-  ShenandoahHeap*           const _heap;\n-  ShenandoahHeapRegionSet** const _worker_slices;\n@@ -384,2 +758,3 @@\n-public:\n-  ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks, ShenandoahHeapRegionSet **worker_slices) :\n+ShenandoahPrepareForCompactionTask::ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks,\n+                                                                       ShenandoahHeapRegionSet **worker_slices,\n+                                                                       size_t num_workers) :\n@@ -387,2 +762,12 @@\n-    _preserved_marks(preserved_marks),\n-    _heap(ShenandoahHeap::heap()), _worker_slices(worker_slices) {\n+    _preserved_marks(preserved_marks), _heap(ShenandoahHeap::heap()),\n+    _worker_slices(worker_slices), _num_workers(num_workers) { }\n+\n+\n+void ShenandoahPrepareForCompactionTask::work(uint worker_id) {\n+  ShenandoahParallelWorkerSession worker_session(worker_id);\n+  ShenandoahHeapRegionSet* slice = _worker_slices[worker_id];\n+  ShenandoahHeapRegionSetIterator it(slice);\n+  ShenandoahHeapRegion* from_region = it.next();\n+  \/\/ No work?\n+  if (from_region == nullptr) {\n+    return;\n@@ -391,6 +776,3 @@\n-  static bool is_candidate_region(ShenandoahHeapRegion* r) {\n-    \/\/ Empty region: get it into the slice to defragment the slice itself.\n-    \/\/ We could have skipped this without violating correctness, but we really\n-    \/\/ want to compact all live regions to the start of the heap, which sometimes\n-    \/\/ means moving them into the fully empty regions.\n-    if (r->is_empty()) return true;\n+  \/\/ Sliding compaction. Walk all regions in the slice, and compact them.\n+  \/\/ Remember empty regions and reuse them as needed.\n+  ResourceMark rm;\n@@ -398,4 +780,1 @@\n-    \/\/ Can move the region, and this is not the humongous region. Humongous\n-    \/\/ moves are special cased here, because their moves are handled separately.\n-    return r->is_stw_move_allowed() && !r->is_humongous();\n-  }\n+  GrowableArray<ShenandoahHeapRegion*> empty_regions((int)_heap->num_regions());\n@@ -403,8 +782,21 @@\n-  void work(uint worker_id) {\n-    ShenandoahParallelWorkerSession worker_session(worker_id);\n-    ShenandoahHeapRegionSet* slice = _worker_slices[worker_id];\n-    ShenandoahHeapRegionSetIterator it(slice);\n-    ShenandoahHeapRegion* from_region = it.next();\n-    \/\/ No work?\n-    if (from_region == nullptr) {\n-       return;\n+  if (_heap->mode()->is_generational()) {\n+    ShenandoahHeapRegion* old_to_region = (from_region->is_old())? from_region: nullptr;\n+    ShenandoahHeapRegion* young_to_region = (from_region->is_young())? from_region: nullptr;\n+    ShenandoahPrepareForGenerationalCompactionObjectClosure cl(_preserved_marks->get(worker_id),\n+                                                               empty_regions,\n+                                                               old_to_region, young_to_region,\n+                                                               worker_id);\n+    while (from_region != nullptr) {\n+      assert(is_candidate_region(from_region), \"Sanity\");\n+      log_debug(gc)(\"Worker %u compacting %s Region \" SIZE_FORMAT \" which had used \" SIZE_FORMAT \" and %s live\",\n+                    worker_id, from_region->affiliation_name(),\n+                    from_region->index(), from_region->used(), from_region->has_live()? \"has\": \"does not have\");\n+      cl.set_from_region(from_region);\n+      if (from_region->has_live()) {\n+        _heap->marked_object_iterate(from_region, &cl);\n+      }\n+      \/\/ Compacted the region to somewhere else? From-region is empty then.\n+      if (!cl.is_compact_same_region()) {\n+        empty_regions.append(from_region);\n+      }\n+      from_region = it.next();\n@@ -412,0 +804,1 @@\n+    cl.finish();\n@@ -413,6 +806,6 @@\n-    \/\/ Sliding compaction. Walk all regions in the slice, and compact them.\n-    \/\/ Remember empty regions and reuse them as needed.\n-    ResourceMark rm;\n-\n-    GrowableArray<ShenandoahHeapRegion*> empty_regions((int)_heap->num_regions());\n-\n+    \/\/ Mark all remaining regions as empty\n+    for (int pos = cl.empty_regions_pos(); pos < empty_regions.length(); ++pos) {\n+      ShenandoahHeapRegion* r = empty_regions.at(pos);\n+      r->set_new_top(r->bottom());\n+    }\n+  } else {\n@@ -420,1 +813,0 @@\n-\n@@ -423,1 +815,0 @@\n-\n@@ -443,1 +834,1 @@\n-};\n+}\n@@ -462,0 +853,1 @@\n+  log_debug(gc)(\"Full GC calculating target humongous objects from end \" SIZE_FORMAT, to_end);\n@@ -504,0 +896,1 @@\n+      \/\/ Leave affiliation unchanged\n@@ -528,0 +921,6 @@\n+    if (!r->is_affiliated()) {\n+      \/\/ Ignore free regions\n+      \/\/ TODO: change iterators so they do not process FREE regions.\n+      return;\n+    }\n+\n@@ -532,1 +931,3 @@\n-               \"Region \" SIZE_FORMAT \" is not marked, should not have live\", r->index());\n+               \"Humongous Start %s Region \" SIZE_FORMAT \" is not marked, should not have live\",\n+               r->affiliation_name(),  r->index());\n+        log_debug(gc)(\"Trashing immediate humongous region \" SIZE_FORMAT \" because not marked\", r->index());\n@@ -536,1 +937,1 @@\n-               \"Region \" SIZE_FORMAT \" should have live\", r->index());\n+               \"Humongous Start %s Region \" SIZE_FORMAT \" should have live\", r->affiliation_name(),  r->index());\n@@ -541,1 +942,1 @@\n-             \"Region \" SIZE_FORMAT \" should have live\", r->index());\n+             \"Humongous Continuation %s Region \" SIZE_FORMAT \" should have live\", r->affiliation_name(),  r->index());\n@@ -544,0 +945,1 @@\n+        log_debug(gc)(\"Trashing immediate regular region \" SIZE_FORMAT \" because has no live\", r->index());\n@@ -692,0 +1094,5 @@\n+\/\/ TODO:\n+\/\/  Consider compacting old-gen objects toward the high end of memory and young-gen objects towards the low-end\n+\/\/  of memory.  As currently implemented, all regions are compacted toward the low-end of memory.  This creates more\n+\/\/  fragmentation of the heap, because old-gen regions get scattered among low-address regions such that it becomes\n+\/\/  more difficult to find contiguous regions for humongous objects.\n@@ -719,1 +1126,4 @@\n-    ShenandoahPrepareForCompactionTask task(_preserved_marks, worker_slices);\n+    size_t num_workers = heap->max_workers();\n+\n+    ResourceMark rm;\n+    ShenandoahPrepareForCompactionTask task(_preserved_marks, worker_slices, num_workers);\n@@ -793,0 +1203,7 @@\n+      if (r->is_pinned() && r->is_old() && r->is_active() && !r->is_humongous()) {\n+        \/\/ Pinned regions are not compacted so they may still hold unmarked objects with\n+        \/\/ reference to reclaimed memory. Remembered set scanning will crash if it attempts\n+        \/\/ to iterate the oops in these objects.\n+        r->begin_preemptible_coalesce_and_fill();\n+        r->oop_fill_and_coalesce_without_cancel();\n+      }\n@@ -893,0 +1310,17 @@\n+static void account_for_region(ShenandoahHeapRegion* r, size_t &region_count, size_t &region_usage, size_t &humongous_waste) {\n+  region_count++;\n+  region_usage += r->used();\n+  if (r->is_humongous_start()) {\n+    \/\/ For each humongous object, we take this path once regardless of how many regions it spans.\n+    HeapWord* obj_addr = r->bottom();\n+    oop obj = cast_to_oop(obj_addr);\n+    size_t word_size = obj->size();\n+    size_t region_size_words = ShenandoahHeapRegion::region_size_words();\n+    size_t overreach = word_size % region_size_words;\n+    if (overreach != 0) {\n+      humongous_waste += (region_size_words - overreach) * HeapWordSize;\n+    }\n+    \/\/ else, this humongous object aligns exactly on region size, so no waste.\n+  }\n+}\n+\n@@ -896,1 +1330,3 @@\n-  size_t _live;\n+  bool _is_generational;\n+  size_t _young_regions, _young_usage, _young_humongous_waste;\n+  size_t _old_regions, _old_usage, _old_humongous_waste;\n@@ -899,1 +1335,9 @@\n-  ShenandoahPostCompactClosure() : _heap(ShenandoahHeap::heap()), _live(0) {\n+  ShenandoahPostCompactClosure() : _heap(ShenandoahHeap::heap()),\n+                                   _is_generational(_heap->mode()->is_generational()),\n+                                   _young_regions(0),\n+                                   _young_usage(0),\n+                                   _young_humongous_waste(0),\n+                                   _old_regions(0),\n+                                   _old_usage(0),\n+                                   _old_humongous_waste(0)\n+  {\n@@ -919,0 +1363,4 @@\n+      if (!_is_generational) {\n+        r->make_young_maybe();\n+      }\n+      \/\/ else, generational mode compaction has already established affiliation.\n@@ -934,0 +1382,6 @@\n+    } else {\n+      if (r->is_old()) {\n+        account_for_region(r, _old_regions, _old_usage, _old_humongous_waste);\n+      } else if (r->is_young()) {\n+        account_for_region(r, _young_regions, _young_usage, _young_humongous_waste);\n+      }\n@@ -935,1 +1389,0 @@\n-\n@@ -938,1 +1391,0 @@\n-    _live += live;\n@@ -941,2 +1393,15 @@\n-  size_t get_live() {\n-    return _live;\n+  void update_generation_usage() {\n+    if (_is_generational) {\n+      _heap->old_generation()->establish_usage(_old_regions, _old_usage, _old_humongous_waste);\n+      _heap->young_generation()->establish_usage(_young_regions, _young_usage, _young_humongous_waste);\n+    } else {\n+      assert(_old_regions == 0, \"Old regions only expected in generational mode\");\n+      assert(_old_usage == 0, \"Old usage only expected in generational mode\");\n+      assert(_old_humongous_waste == 0, \"Old humongous waste only expected in generational mode\");\n+    }\n+\n+    \/\/ In generational mode, global usage should be the sum of young and old. This is also true\n+    \/\/ for non-generational modes except that there are no old regions.\n+    _heap->global_generation()->establish_usage(_old_regions + _young_regions,\n+                                                _old_usage + _young_usage,\n+                                                _old_humongous_waste + _young_humongous_waste);\n@@ -951,1 +1416,1 @@\n-  \/\/ sliding costs. We may consider doing this in parallel in future.\n+  \/\/ sliding costs. We may consider doing this in parallel in the future.\n@@ -973,2 +1438,7 @@\n-      Copy::aligned_conjoint_words(r->bottom(), heap->get_region(new_start)->bottom(), words_size);\n-      ContinuationGCSupport::relativize_stack_chunk(cast_to_oop<HeapWord*>(r->bottom()));\n+      ContinuationGCSupport::relativize_stack_chunk(cast_to_oop<HeapWord*>(heap->get_region(old_start)->bottom()));\n+      log_debug(gc)(\"Full GC compaction moves humongous object from region \" SIZE_FORMAT \" to region \" SIZE_FORMAT,\n+                    old_start, new_start);\n+\n+      Copy::aligned_conjoint_words(heap->get_region(old_start)->bottom(),\n+                                   heap->get_region(new_start)->bottom(),\n+                                   words_size);\n@@ -980,0 +1450,1 @@\n+        ShenandoahAffiliation original_affiliation = r->affiliation();\n@@ -982,0 +1453,1 @@\n+          \/\/ Leave humongous region affiliation unchanged.\n@@ -989,1 +1461,1 @@\n-            r->make_humongous_start_bypass();\n+            r->make_humongous_start_bypass(original_affiliation);\n@@ -991,1 +1463,1 @@\n-            r->make_humongous_cont_bypass();\n+            r->make_humongous_cont_bypass(original_affiliation);\n@@ -1057,0 +1529,5 @@\n+}\n+\n+void ShenandoahFullGC::phase5_epilog() {\n+  GCTraceTime(Info, gc, phases) time(\"Phase 5: Full GC epilog\", _gc_timer);\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -1069,1 +1546,0 @@\n-\n@@ -1072,1 +1548,15 @@\n-    heap->set_used(post_compact.get_live());\n+    post_compact.update_generation_usage();\n+    if (heap->mode()->is_generational()) {\n+      size_t old_usage = heap->old_generation()->used_regions_size();\n+      size_t old_capacity = heap->old_generation()->max_capacity();\n+\n+      assert(old_usage % ShenandoahHeapRegion::region_size_bytes() == 0, \"Old usage must aligh with region size\");\n+      assert(old_capacity % ShenandoahHeapRegion::region_size_bytes() == 0, \"Old capacity must aligh with region size\");\n+\n+      if (old_capacity > old_usage) {\n+        size_t excess_old_regions = (old_capacity - old_usage) \/ ShenandoahHeapRegion::region_size_bytes();\n+        heap->generation_sizer()->transfer_to_young(excess_old_regions);\n+      } else if (old_capacity < old_usage) {\n+        size_t old_regions_deficit = (old_usage - old_capacity) \/ ShenandoahHeapRegion::region_size_bytes();\n+        heap->generation_sizer()->force_transfer_to_old(old_regions_deficit);\n+      }\n@@ -1074,0 +1564,4 @@\n+      log_info(gc)(\"FullGC done: young usage: \" SIZE_FORMAT \"%s, old usage: \" SIZE_FORMAT \"%s\",\n+                   byte_size_in_proper_unit(heap->young_generation()->used()), proper_unit_for_byte_size(heap->young_generation()->used()),\n+                   byte_size_in_proper_unit(heap->old_generation()->used()),   proper_unit_for_byte_size(heap->old_generation()->used()));\n+    }\n@@ -1075,2 +1569,14 @@\n-    heap->free_set()->rebuild();\n-  }\n+    size_t young_cset_regions, old_cset_regions;\n+    size_t first_old, last_old, num_old;\n+    heap->free_set()->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+\n+    \/\/ We also do not expand old generation size following Full GC because we have scrambled age populations and\n+    \/\/ no longer have objects separated by age into distinct regions.\n+\n+    \/\/ TODO: Do we need to fix FullGC so that it maintains aged segregation of objects into distinct regions?\n+    \/\/       A partial solution would be to remember how many objects are of tenure age following Full GC, but\n+    \/\/       this is probably suboptimal, because most of these objects will not reside in a region that will be\n+    \/\/       selected for the next evacuation phase.\n+\n+    \/\/ In case this Full GC resulted from degeneration, clear the tally on anticipated promotion.\n+    heap->clear_promotion_potential();\n@@ -1078,1 +1584,45 @@\n-  heap->clear_cancelled_gc();\n+    if (heap->mode()->is_generational()) {\n+      \/\/ Invoke this in case we are able to transfer memory from OLD to YOUNG.\n+      heap->adjust_generation_sizes_for_next_cycle(0, 0, 0);\n+    }\n+    heap->free_set()->rebuild(young_cset_regions, old_cset_regions);\n+\n+    \/\/ We defer generation resizing actions until after cset regions have been recycled.  We do this even following an\n+    \/\/ abbreviated cycle.\n+    if (heap->mode()->is_generational()) {\n+      bool success;\n+      size_t region_xfer;\n+      const char* region_destination;\n+      ShenandoahYoungGeneration* young_gen = heap->young_generation();\n+      ShenandoahGeneration* old_gen = heap->old_generation();\n+\n+      size_t old_region_surplus = heap->get_old_region_surplus();\n+      size_t old_region_deficit = heap->get_old_region_deficit();\n+      if (old_region_surplus) {\n+        success = heap->generation_sizer()->transfer_to_young(old_region_surplus);\n+        region_destination = \"young\";\n+        region_xfer = old_region_surplus;\n+      } else if (old_region_deficit) {\n+        success = heap->generation_sizer()->transfer_to_old(old_region_deficit);\n+        region_destination = \"old\";\n+        region_xfer = old_region_deficit;\n+        if (!success) {\n+          ((ShenandoahOldHeuristics *) old_gen->heuristics())->trigger_cannot_expand();\n+        }\n+      } else {\n+        region_destination = \"none\";\n+        region_xfer = 0;\n+        success = true;\n+      }\n+      heap->set_old_region_surplus(0);\n+      heap->set_old_region_deficit(0);\n+      size_t young_available = young_gen->available();\n+      size_t old_available = old_gen->available();\n+      log_info(gc, ergo)(\"After cleanup, %s \" SIZE_FORMAT \" regions to %s to prepare for next gc, old available: \"\n+                         SIZE_FORMAT \"%s, young_available: \" SIZE_FORMAT \"%s\",\n+                         success? \"successfully transferred\": \"failed to transfer\", region_xfer, region_destination,\n+                         byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n+                         byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n+    }\n+    heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":619,"deletions":69,"binary":false,"changes":688,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -39,0 +40,4 @@\n+#include \"gc\/shenandoah\/shenandoahAgeCensus.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n@@ -40,0 +45,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -46,0 +52,1 @@\n+#include \"gc\/shenandoah\/shenandoahRegulatorThread.hpp\"\n@@ -47,0 +54,1 @@\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n@@ -56,0 +64,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -62,0 +71,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -69,0 +79,2 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -72,0 +84,2 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n@@ -163,3 +177,0 @@\n-  \/\/ Now we know the number of regions, initialize the heuristics.\n-  initialize_heuristics();\n-\n@@ -181,0 +192,3 @@\n+  \/\/ Now we know the number of regions and heap sizes, initialize the heuristics.\n+  initialize_heuristics_generations();\n+\n@@ -219,0 +233,28 @@\n+  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this, _heap_region));\n+\n+  \/\/\n+  \/\/ After reserving the Java heap, create the card table, barriers, and workers, in dependency order\n+  \/\/\n+  if (mode()->is_generational()) {\n+    ShenandoahDirectCardMarkRememberedSet *rs;\n+    ShenandoahCardTable* card_table = ShenandoahBarrierSet::barrier_set()->card_table();\n+    size_t card_count = card_table->cards_required(heap_rs.size() \/ HeapWordSize);\n+    rs = new ShenandoahDirectCardMarkRememberedSet(ShenandoahBarrierSet::barrier_set()->card_table(), card_count);\n+    _card_scan = new ShenandoahScanRemembered<ShenandoahDirectCardMarkRememberedSet>(rs);\n+\n+    \/\/ Age census structure\n+    _age_census = new ShenandoahAgeCensus();\n+  }\n+\n+  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n+  if (_workers == nullptr) {\n+    vm_exit_during_initialization(\"Failed necessary allocation.\");\n+  } else {\n+    _workers->initialize_workers();\n+  }\n+\n+  if (ParallelGCThreads > 1) {\n+    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\", ParallelGCThreads);\n+    _safepoint_workers->initialize_workers();\n+  }\n+\n@@ -266,1 +308,1 @@\n-  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions, _max_workers);\n+  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions);\n@@ -357,0 +399,1 @@\n+  _affiliations = NEW_C_HEAP_ARRAY(uint8_t, _num_regions, mtGC);\n@@ -362,0 +405,1 @@\n+\n@@ -373,0 +417,2 @@\n+\n+      _affiliations[i] = ShenandoahAffiliation::FREE;\n@@ -377,0 +423,1 @@\n+    size_t young_cset_regions, old_cset_regions;\n@@ -378,1 +425,4 @@\n-    _free_set->rebuild();\n+    \/\/ We are initializing free set.  We ignore cset region tallies.\n+    size_t first_old, last_old, num_old;\n+    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+    _free_set->rebuild(young_cset_regions, old_cset_regions);\n@@ -438,0 +488,1 @@\n+  _regulator_thread = new ShenandoahRegulatorThread(_control_thread);\n@@ -439,1 +490,1 @@\n-  ShenandoahInitLogger::print();\n+  print_init_logger();\n@@ -444,1 +495,35 @@\n-void ShenandoahHeap::initialize_mode() {\n+void ShenandoahHeap::print_init_logger() const {\n+  ShenandoahInitLogger::print();\n+}\n+\n+size_t ShenandoahHeap::max_size_for(ShenandoahGeneration* generation) const {\n+  switch (generation->type()) {\n+    case YOUNG:\n+      return _generation_sizer.max_young_size();\n+    case OLD:\n+      return max_capacity() - _generation_sizer.min_young_size();\n+    case GLOBAL_GEN:\n+    case GLOBAL_NON_GEN:\n+      return max_capacity();\n+    default:\n+      ShouldNotReachHere();\n+      return 0;\n+  }\n+}\n+\n+size_t ShenandoahHeap::min_size_for(ShenandoahGeneration* generation) const {\n+  switch (generation->type()) {\n+    case YOUNG:\n+      return _generation_sizer.min_young_size();\n+    case OLD:\n+      return max_capacity() - _generation_sizer.max_young_size();\n+    case GLOBAL_GEN:\n+    case GLOBAL_NON_GEN:\n+      return min_capacity();\n+    default:\n+      ShouldNotReachHere();\n+      return 0;\n+  }\n+}\n+\n+void ShenandoahHeap::initialize_heuristics_generations() {\n@@ -452,0 +537,2 @@\n+    } else if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+      _gc_mode = new ShenandoahGenerationalMode();\n@@ -469,1 +556,0 @@\n-}\n@@ -471,3 +557,9 @@\n-void ShenandoahHeap::initialize_heuristics() {\n-  assert(_gc_mode != nullptr, \"Must be initialized\");\n-  _heuristics = _gc_mode->initialize_heuristics();\n+  \/\/ Max capacity is the maximum _allowed_ capacity. That is, the maximum allowed capacity\n+  \/\/ for old would be total heap - minimum capacity of young. This means the sum of the maximum\n+  \/\/ allowed for old and young could exceed the total heap size. It remains the case that the\n+  \/\/ _actual_ capacity of young + old = total.\n+  _generation_sizer.heap_size_changed(max_capacity());\n+  size_t initial_capacity_young = _generation_sizer.max_young_size();\n+  size_t max_capacity_young = _generation_sizer.max_young_size();\n+  size_t initial_capacity_old = max_capacity() - max_capacity_young;\n+  size_t max_capacity_old = max_capacity() - initial_capacity_young;\n@@ -475,9 +567,7 @@\n-  if (_heuristics->is_diagnostic() && !UnlockDiagnosticVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.\",\n-                    _heuristics->name()));\n-  }\n-  if (_heuristics->is_experimental() && !UnlockExperimentalVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.\",\n-                    _heuristics->name()));\n+  _young_generation = new ShenandoahYoungGeneration(_max_workers, max_capacity_young, initial_capacity_young);\n+  _old_generation = new ShenandoahOldGeneration(_max_workers, max_capacity_old, initial_capacity_old);\n+  _global_generation = new ShenandoahGlobalGeneration(_gc_mode->is_generational(), _max_workers, max_capacity(), max_capacity());\n+  _global_generation->initialize_heuristics(_gc_mode);\n+  if (mode()->is_generational()) {\n+    _young_generation->initialize_heuristics(_gc_mode);\n+    _old_generation->initialize_heuristics(_gc_mode);\n@@ -485,0 +575,1 @@\n+  _evac_tracker = new ShenandoahEvacuationTracker(mode()->is_generational());\n@@ -494,0 +585,1 @@\n+  _gc_generation(nullptr),\n@@ -495,1 +587,1 @@\n-  _used(0),\n+  _promotion_potential(0),\n@@ -497,2 +589,1 @@\n-  _bytes_allocated_since_gc_start(0),\n-  _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),\n+  _max_workers(MAX3(ConcGCThreads, ParallelGCThreads, 1U)),\n@@ -504,0 +595,1 @@\n+  _affiliations(nullptr),\n@@ -507,0 +599,9 @@\n+  _promoted_reserve(0),\n+  _old_evac_reserve(0),\n+  _young_evac_reserve(0),\n+  _age_census(nullptr),\n+  _has_evacuation_reserve_quantities(false),\n+  _cancel_requested_time(0),\n+  _young_generation(nullptr),\n+  _global_generation(nullptr),\n+  _old_generation(nullptr),\n@@ -508,0 +609,1 @@\n+  _regulator_thread(nullptr),\n@@ -509,2 +611,0 @@\n-  _gc_mode(nullptr),\n-  _heuristics(nullptr),\n@@ -515,0 +615,3 @@\n+  _evac_tracker(nullptr),\n+  _mmu_tracker(),\n+  _generation_sizer(),\n@@ -517,0 +620,2 @@\n+  _young_gen_memory_pool(nullptr),\n+  _old_gen_memory_pool(nullptr),\n@@ -522,1 +627,2 @@\n-  _ref_processor(new ShenandoahReferenceProcessor(MAX2(_max_workers, 1U))),\n+  _old_regions_surplus(0),\n+  _old_regions_deficit(0),\n@@ -530,1 +636,2 @@\n-  _collection_set(nullptr)\n+  _collection_set(nullptr),\n+  _card_scan(nullptr)\n@@ -532,17 +639,0 @@\n-  \/\/ Initialize GC mode early, so we can adjust barrier support\n-  initialize_mode();\n-  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));\n-\n-  _max_workers = MAX2(_max_workers, 1U);\n-  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n-  if (_workers == nullptr) {\n-    vm_exit_during_initialization(\"Failed necessary allocation.\");\n-  } else {\n-    _workers->initialize_workers();\n-  }\n-\n-  if (ParallelGCThreads > 1) {\n-    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\",\n-                                                ParallelGCThreads);\n-    _safepoint_workers->initialize_workers();\n-  }\n@@ -555,29 +645,0 @@\n-class ShenandoahResetBitmapTask : public WorkerTask {\n-private:\n-  ShenandoahRegionIterator _regions;\n-\n-public:\n-  ShenandoahResetBitmapTask() :\n-    WorkerTask(\"Shenandoah Reset Bitmap\") {}\n-\n-  void work(uint worker_id) {\n-    ShenandoahHeapRegion* region = _regions.next();\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahMarkingContext* const ctx = heap->marking_context();\n-    while (region != nullptr) {\n-      if (heap->is_bitmap_slice_committed(region)) {\n-        ctx->clear_bitmap(region);\n-      }\n-      region = _regions.next();\n-    }\n-  }\n-};\n-\n-void ShenandoahHeap::reset_mark_bitmap() {\n-  assert_gc_workers(_workers->active_workers());\n-  mark_incomplete_marking_context();\n-\n-  ShenandoahResetBitmapTask task;\n-  _workers->run_task(&task);\n-}\n-\n@@ -598,1 +659,2 @@\n-  if (is_concurrent_mark_in_progress())        st->print(\"marking, \");\n+  if (is_concurrent_old_mark_in_progress())    st->print(\"old marking, \");\n+  if (is_concurrent_young_mark_in_progress())  st->print(\"young marking, \");\n@@ -649,0 +711,2 @@\n+  _mmu_tracker.initialize();\n+\n@@ -662,2 +726,0 @@\n-  _heuristics->initialize();\n-\n@@ -667,0 +729,24 @@\n+ShenandoahHeuristics* ShenandoahHeap::heuristics() {\n+  return _global_generation->heuristics();\n+}\n+\n+ShenandoahOldHeuristics* ShenandoahHeap::old_heuristics() {\n+  return (ShenandoahOldHeuristics*) _old_generation->heuristics();\n+}\n+\n+ShenandoahYoungHeuristics* ShenandoahHeap::young_heuristics() {\n+  return (ShenandoahYoungHeuristics*) _young_generation->heuristics();\n+}\n+\n+bool ShenandoahHeap::doing_mixed_evacuations() {\n+  return _old_generation->state() == ShenandoahOldGeneration::EVACUATING;\n+}\n+\n+bool ShenandoahHeap::is_old_bitmap_stable() const {\n+  return _old_generation->is_mark_complete();\n+}\n+\n+bool ShenandoahHeap::is_gc_generation_young() const {\n+  return _gc_generation != nullptr && _gc_generation->is_young();\n+}\n+\n@@ -668,1 +754,1 @@\n-  return Atomic::load(&_used);\n+  return global_generation()->used();\n@@ -675,4 +761,0 @@\n-size_t ShenandoahHeap::available() const {\n-  return free_set()->available();\n-}\n-\n@@ -689,2 +771,43 @@\n-void ShenandoahHeap::increase_used(size_t bytes) {\n-  Atomic::add(&_used, bytes, memory_order_relaxed);\n+\/\/ For tracking usage based on allocations, it should be the case that:\n+\/\/ * The sum of regions::used == heap::used\n+\/\/ * The sum of a generation's regions::used == generation::used\n+\/\/ * The sum of a generation's humongous regions::free == generation::humongous_waste\n+\/\/ These invariants are checked by the verifier on GC safepoints.\n+\/\/\n+\/\/ Additional notes:\n+\/\/ * When a mutator's allocation request causes a region to be retired, the\n+\/\/   free memory left in that region is considered waste. It does not contribute\n+\/\/   to the usage, but it _does_ contribute to allocation rate.\n+\/\/ * The bottom of a PLAB must be aligned on card size. In some cases this will\n+\/\/   require padding in front of the PLAB (a filler object). Because this padding\n+\/\/   is included in the region's used memory we include the padding in the usage\n+\/\/   accounting as waste.\n+\/\/ * Mutator allocations are used to compute an allocation rate. They are also\n+\/\/   sent to the Pacer for those purposes.\n+\/\/ * There are three sources of waste:\n+\/\/  1. The padding used to align a PLAB on card size\n+\/\/  2. Region's free is less than minimum TLAB size and is retired\n+\/\/  3. The unused portion of memory in the last region of a humongous object\n+void ShenandoahHeap::increase_used(const ShenandoahAllocRequest& req) {\n+  size_t actual_bytes = req.actual_size() * HeapWordSize;\n+  size_t wasted_bytes = req.waste() * HeapWordSize;\n+  ShenandoahGeneration* generation = generation_for(req.affiliation());\n+\n+  if (req.is_gc_alloc()) {\n+    assert(wasted_bytes == 0 || req.type() == ShenandoahAllocRequest::_alloc_plab, \"Only PLABs have waste\");\n+    increase_used(generation, actual_bytes + wasted_bytes);\n+  } else {\n+    assert(req.is_mutator_alloc(), \"Expected mutator alloc here\");\n+    \/\/ padding and actual size both count towards allocation counter\n+    generation->increase_allocated(actual_bytes + wasted_bytes);\n+\n+    \/\/ only actual size counts toward usage for mutator allocations\n+    increase_used(generation, actual_bytes);\n+\n+    \/\/ notify pacer of both actual size and waste\n+    notify_mutator_alloc_words(req.actual_size(), req.waste());\n+\n+    if (wasted_bytes > 0 && req.actual_size() > ShenandoahHeapRegion::humongous_threshold_words()) {\n+      increase_humongous_waste(generation,wasted_bytes);\n+    }\n+  }\n@@ -693,2 +816,5 @@\n-void ShenandoahHeap::set_used(size_t bytes) {\n-  Atomic::store(&_used, bytes);\n+void ShenandoahHeap::increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_humongous_waste(bytes);\n+  }\n@@ -697,3 +823,5 @@\n-void ShenandoahHeap::decrease_used(size_t bytes) {\n-  assert(used() >= bytes, \"never decrease heap size by more than we've left\");\n-  Atomic::sub(&_used, bytes, memory_order_relaxed);\n+void ShenandoahHeap::decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_humongous_waste(bytes);\n+  }\n@@ -702,2 +830,5 @@\n-void ShenandoahHeap::increase_allocated(size_t bytes) {\n-  Atomic::add(&_bytes_allocated_since_gc_start, bytes, memory_order_relaxed);\n+void ShenandoahHeap::increase_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_used(bytes);\n+  }\n@@ -706,4 +837,4 @@\n-void ShenandoahHeap::notify_mutator_alloc_words(size_t words, bool waste) {\n-  size_t bytes = words * HeapWordSize;\n-  if (!waste) {\n-    increase_used(bytes);\n+void ShenandoahHeap::decrease_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_used(bytes);\n@@ -711,1 +842,3 @@\n-  increase_allocated(bytes);\n+}\n+\n+void ShenandoahHeap::notify_mutator_alloc_words(size_t words, size_t waste) {\n@@ -714,2 +847,2 @@\n-    if (waste) {\n-      pacer()->claim_for_alloc(words, true);\n+    if (waste > 0) {\n+      pacer()->claim_for_alloc(waste, true);\n@@ -751,6 +884,0 @@\n-bool ShenandoahHeap::is_in(const void* p) const {\n-  HeapWord* heap_base = (HeapWord*) base();\n-  HeapWord* last_region_end = heap_base + ShenandoahHeapRegion::region_size_words() * num_regions();\n-  return p >= heap_base && p < last_region_end;\n-}\n-\n@@ -783,0 +910,1 @@\n+<<<<<<< HEAD\n@@ -784,0 +912,68 @@\n+=======\n+    control_thread()->notify_heap_changed();\n+    regulator_thread()->notify_heap_changed();\n+  }\n+}\n+\n+void ShenandoahHeap::handle_old_evacuation(HeapWord* obj, size_t words, bool promotion) {\n+  \/\/ Only register the copy of the object that won the evacuation race.\n+  card_scan()->register_object_without_lock(obj);\n+\n+  \/\/ Mark the entire range of the evacuated object as dirty.  At next remembered set scan,\n+  \/\/ we will clear dirty bits that do not hold interesting pointers.  It's more efficient to\n+  \/\/ do this in batch, in a background GC thread than to try to carefully dirty only cards\n+  \/\/ that hold interesting pointers right now.\n+  card_scan()->mark_range_as_dirty(obj, words);\n+\n+  if (promotion) {\n+    \/\/ This evacuation was a promotion, track this as allocation against old gen\n+    old_generation()->increase_allocated(words * HeapWordSize);\n+  }\n+}\n+\n+void ShenandoahHeap::handle_old_evacuation_failure() {\n+  if (_old_gen_oom_evac.try_set()) {\n+    log_info(gc)(\"Old gen evac failure.\");\n+  }\n+}\n+\n+void ShenandoahHeap::report_promotion_failure(Thread* thread, size_t size) {\n+  \/\/ We squelch excessive reports to reduce noise in logs.\n+  const size_t MaxReportsPerEpoch = 4;\n+  static size_t last_report_epoch = 0;\n+  static size_t epoch_report_count = 0;\n+\n+  size_t promotion_reserve;\n+  size_t promotion_expended;\n+\n+  size_t gc_id = control_thread()->get_gc_id();\n+\n+  if ((gc_id != last_report_epoch) || (epoch_report_count++ < MaxReportsPerEpoch)) {\n+    {\n+      \/\/ Promotion failures should be very rare.  Invest in providing useful diagnostic info.\n+      ShenandoahHeapLocker locker(lock());\n+      promotion_reserve = get_promoted_reserve();\n+      promotion_expended = get_promoted_expended();\n+    }\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    size_t words_remaining = (plab == nullptr)? 0: plab->words_remaining();\n+    const char* promote_enabled = ShenandoahThreadLocalData::allow_plab_promotions(thread)? \"enabled\": \"disabled\";\n+    ShenandoahGeneration* old_gen = old_generation();\n+    size_t old_capacity = old_gen->max_capacity();\n+    size_t old_usage = old_gen->used();\n+    size_t old_free_regions = old_gen->free_unaffiliated_regions();\n+\n+    log_info(gc, ergo)(\"Promotion failed, size \" SIZE_FORMAT \", has plab? %s, PLAB remaining: \" SIZE_FORMAT\n+                       \", plab promotions %s, promotion reserve: \" SIZE_FORMAT \", promotion expended: \" SIZE_FORMAT\n+                       \", old capacity: \" SIZE_FORMAT \", old_used: \" SIZE_FORMAT \", old unaffiliated regions: \" SIZE_FORMAT,\n+                       size * HeapWordSize, plab == nullptr? \"no\": \"yes\",\n+                       words_remaining * HeapWordSize, promote_enabled, promotion_reserve, promotion_expended,\n+                       old_capacity, old_usage, old_free_regions);\n+\n+    if ((gc_id == last_report_epoch) && (epoch_report_count >= MaxReportsPerEpoch)) {\n+      log_info(gc, ergo)(\"Squelching additional promotion failure reports for current epoch\");\n+    } else if (gc_id != last_report_epoch) {\n+      last_report_epoch = gc_id;\n+      epoch_report_count = 1;\n+    }\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -808,0 +1004,8 @@\n+\n+  \/\/ Limit growth of GCLABs to ShenandoahMaxEvacLABRatio * the minimum size.  This enables more equitable distribution of\n+  \/\/ available evacuation buidget between the many threads that are coordinating in the evacuation effort.\n+  if (ShenandoahMaxEvacLABRatio > 0) {\n+    log_debug(gc, free)(\"Allocate new gclab: \" SIZE_FORMAT \", \" SIZE_FORMAT, new_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+    new_size = MIN2(new_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+  }\n+\n@@ -819,0 +1023,1 @@\n+    log_debug(gc, free)(\"New gclab size (\" SIZE_FORMAT \") is too small for \" SIZE_FORMAT, new_size, size);\n@@ -851,0 +1056,246 @@\n+\/\/ Establish a new PLAB and allocate size HeapWords within it.\n+HeapWord* ShenandoahHeap::allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion) {\n+  \/\/ New object should fit the PLAB size\n+  size_t min_size = MAX2(size, PLAB::min_size());\n+\n+  \/\/ Figure out size of new PLAB, looking back at heuristics. Expand aggressively.\n+  size_t cur_size = ShenandoahThreadLocalData::plab_size(thread);\n+  if (cur_size == 0) {\n+    cur_size = PLAB::min_size();\n+  }\n+  size_t future_size = cur_size * 2;\n+  \/\/ Limit growth of PLABs to ShenandoahMaxEvacLABRatio * the minimum size.  This enables more equitable distribution of\n+  \/\/ available evacuation buidget between the many threads that are coordinating in the evacuation effort.\n+  if (ShenandoahMaxEvacLABRatio > 0) {\n+    future_size = MIN2(future_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+  }\n+  future_size = MIN2(future_size, PLAB::max_size());\n+  future_size = MAX2(future_size, PLAB::min_size());\n+\n+  size_t unalignment = future_size % CardTable::card_size_in_words();\n+  if (unalignment != 0) {\n+    future_size = future_size - unalignment + CardTable::card_size_in_words();\n+  }\n+\n+  \/\/ Record new heuristic value even if we take any shortcut. This captures\n+  \/\/ the case when moderately-sized objects always take a shortcut. At some point,\n+  \/\/ heuristics should catch up with them.  Note that the requested cur_size may\n+  \/\/ not be honored, but we remember that this is the preferred size.\n+  ShenandoahThreadLocalData::set_plab_size(thread, future_size);\n+  if (cur_size < size) {\n+    \/\/ The PLAB to be allocated is still not large enough to hold the object. Fall back to shared allocation.\n+    \/\/ This avoids retiring perfectly good PLABs in order to represent a single large object allocation.\n+    return nullptr;\n+  }\n+\n+  \/\/ Retire current PLAB, and allocate a new one.\n+  PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+  if (plab->words_remaining() < PLAB::min_size()) {\n+    \/\/ Retire current PLAB, and allocate a new one.\n+    \/\/ CAUTION: retire_plab may register the remnant filler object with the remembered set scanner without a lock.  This\n+    \/\/ is safe iff it is assured that each PLAB is a whole-number multiple of card-mark memory size and each PLAB is\n+    \/\/ aligned with the start of a card's memory range.\n+    retire_plab(plab, thread);\n+\n+    size_t actual_size = 0;\n+    \/\/ allocate_new_plab resets plab_evacuated and plab_promoted and disables promotions if old-gen available is\n+    \/\/ less than the remaining evacuation need.  It also adjusts plab_preallocated and expend_promoted if appropriate.\n+    HeapWord* plab_buf = allocate_new_plab(min_size, cur_size, &actual_size);\n+    if (plab_buf == nullptr) {\n+      if (min_size == PLAB::min_size()) {\n+        \/\/ Disable plab promotions for this thread because we cannot even allocate a plab of minimal size.  This allows us\n+        \/\/ to fail faster on subsequent promotion attempts.\n+        ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+      }\n+      return NULL;\n+    } else {\n+      ShenandoahThreadLocalData::enable_plab_retries(thread);\n+    }\n+    assert (size <= actual_size, \"allocation should fit\");\n+    if (ZeroTLAB) {\n+      \/\/ ..and clear it.\n+      Copy::zero_to_words(plab_buf, actual_size);\n+    } else {\n+      \/\/ ...and zap just allocated object.\n+#ifdef ASSERT\n+      \/\/ Skip mangling the space corresponding to the object header to\n+      \/\/ ensure that the returned space is not considered parsable by\n+      \/\/ any concurrent GC thread.\n+      size_t hdr_size = oopDesc::header_size();\n+      Copy::fill_to_words(plab_buf + hdr_size, actual_size - hdr_size, badHeapWordVal);\n+#endif \/\/ ASSERT\n+    }\n+    plab->set_buf(plab_buf, actual_size);\n+    if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+      return nullptr;\n+    }\n+    return plab->allocate(size);\n+  } else {\n+    \/\/ If there's still at least min_size() words available within the current plab, don't retire it.  Let's gnaw\n+    \/\/ away on this plab as long as we can.  Meanwhile, return nullptr to force this particular allocation request\n+    \/\/ to be satisfied with a shared allocation.  By packing more promotions into the previously allocated PLAB, we\n+    \/\/ reduce the likelihood of evacuation failures, and we we reduce the need for downsizing our PLABs.\n+    return nullptr;\n+  }\n+}\n+\n+\/\/ TODO: It is probably most efficient to register all objects (both promotions and evacuations) that were allocated within\n+\/\/ this plab at the time we retire the plab.  A tight registration loop will run within both code and data caches.  This change\n+\/\/ would allow smaller and faster in-line implementation of alloc_from_plab().  Since plabs are aligned on card-table boundaries,\n+\/\/ this object registration loop can be performed without acquiring a lock.\n+void ShenandoahHeap::retire_plab(PLAB* plab, Thread* thread) {\n+  \/\/ We don't enforce limits on plab_evacuated.  We let it consume all available old-gen memory in order to reduce\n+  \/\/ probability of an evacuation failure.  We do enforce limits on promotion, to make sure that excessive promotion\n+  \/\/ does not result in an old-gen evacuation failure.  Note that a failed promotion is relatively harmless.  Any\n+  \/\/ object that fails to promote in the current cycle will be eligible for promotion in a subsequent cycle.\n+\n+  \/\/ When the plab was instantiated, its entirety was treated as if the entire buffer was going to be dedicated to\n+  \/\/ promotions.  Now that we are retiring the buffer, we adjust for the reality that the plab is not entirely promotions.\n+  \/\/  1. Some of the plab may have been dedicated to evacuations.\n+  \/\/  2. Some of the plab may have been abandoned due to waste (at the end of the plab).\n+  size_t not_promoted =\n+    ShenandoahThreadLocalData::get_plab_preallocated_promoted(thread) - ShenandoahThreadLocalData::get_plab_promoted(thread);\n+  ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+  ShenandoahThreadLocalData::reset_plab_evacuated(thread);\n+  ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+  if (not_promoted > 0) {\n+    unexpend_promoted(not_promoted);\n+  }\n+  size_t waste = plab->waste();\n+  HeapWord* top = plab->top();\n+  plab->retire();\n+  if (top != nullptr && plab->waste() > waste && is_in_old(top)) {\n+    \/\/ If retiring the plab created a filler object, then we\n+    \/\/ need to register it with our card scanner so it can\n+    \/\/ safely walk the region backing the plab.\n+    log_debug(gc)(\"retire_plab() is registering remnant of size \" SIZE_FORMAT \" at \" PTR_FORMAT,\n+                  plab->waste() - waste, p2i(top));\n+    card_scan()->register_object_without_lock(top);\n+  }\n+}\n+\n+void ShenandoahHeap::retire_plab(PLAB* plab) {\n+  Thread* thread = Thread::current();\n+  retire_plab(plab, thread);\n+}\n+\n+void ShenandoahHeap::cancel_old_gc() {\n+  shenandoah_assert_safepoint();\n+  assert(_old_generation != nullptr, \"Should only have mixed collections in generation mode.\");\n+  if (_old_generation->state() == ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP) {\n+    assert(!old_generation()->is_concurrent_mark_in_progress(), \"Cannot be marking in IDLE\");\n+    assert(!old_heuristics()->has_coalesce_and_fill_candidates(), \"Cannot have coalesce and fill candidates in IDLE\");\n+    assert(!old_heuristics()->unprocessed_old_collection_candidates(), \"Cannot have mixed collection candidates in IDLE\");\n+    assert(!young_generation()->is_bootstrap_cycle(), \"Cannot have old mark queues if IDLE\");\n+  } else {\n+    log_info(gc)(\"Terminating old gc cycle.\");\n+    \/\/ Stop marking\n+    old_generation()->cancel_marking();\n+    \/\/ Stop tracking old regions\n+    old_heuristics()->abandon_collection_candidates();\n+    \/\/ Remove old generation access to young generation mark queues\n+    young_generation()->set_old_gen_task_queues(nullptr);\n+    \/\/ Transition to IDLE now.\n+    _old_generation->transition_to(ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP);\n+  }\n+}\n+\n+\/\/ Make sure old-generation is large enough, but no larger than is necessary, to hold mixed evacuations\n+\/\/ and promotions, if we anticipate either. Any deficit is provided by the young generation, subject to\n+\/\/ xfer_limit, and any excess is transferred to the young generation.\n+\/\/ xfer_limit is the maximum we're able to transfer from young to old.\n+void ShenandoahHeap::adjust_generation_sizes_for_next_cycle(\n+  size_t xfer_limit, size_t young_cset_regions, size_t old_cset_regions) {\n+\n+  \/\/ We can limit the old reserve to the size of anticipated promotions:\n+  \/\/ max_old_reserve is an upper bound on memory evacuated from old and promoted to old,\n+  \/\/ clamped by the old generation space available.\n+  \/\/\n+  \/\/ Here's the algebra.\n+  \/\/ Let SOEP = ShenandoahOldEvacRatioPercent,\n+  \/\/     OE = old evac,\n+  \/\/     YE = young evac, and\n+  \/\/     TE = total evac = OE + YE\n+  \/\/ By definition:\n+  \/\/            SOEP\/100 = OE\/TE\n+  \/\/                     = OE\/(OE+YE)\n+  \/\/  => SOEP\/(100-SOEP) = OE\/((OE+YE)-OE)      \/\/ componendo-dividendo: If a\/b = c\/d, then a\/(b-a) = c\/(d-c)\n+  \/\/                     = OE\/YE\n+  \/\/  =>              OE = YE*SOEP\/(100-SOEP)\n+\n+  \/\/ We have to be careful in the event that SOEP is set to 100 by the user.\n+  assert(ShenandoahOldEvacRatioPercent <= 100, \"Error\");\n+  const size_t old_available = old_generation()->available();\n+  \/\/ The free set will reserve this amount of memory to hold young evacuations\n+  const size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+  const size_t max_old_reserve = (ShenandoahOldEvacRatioPercent == 100) ?\n+     old_available : MIN2((young_reserve * ShenandoahOldEvacRatioPercent) \/ (100 - ShenandoahOldEvacRatioPercent),\n+                          old_available);\n+\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+  \/\/ Decide how much old space we should reserve for a mixed collection\n+  size_t reserve_for_mixed = 0;\n+  const size_t mixed_candidates = old_heuristics()->unprocessed_old_collection_candidates();\n+  const bool doing_mixed = (mixed_candidates > 0);\n+  if (doing_mixed) {\n+    \/\/ We want this much memory to be unfragmented in order to reliably evacuate old.  This is conservative because we\n+    \/\/ may not evacuate the entirety of unprocessed candidates in a single mixed evacuation.\n+    size_t max_evac_need = (size_t)\n+      (old_heuristics()->unprocessed_old_collection_candidates_live_memory() * ShenandoahOldEvacWaste);\n+    assert(old_available >= old_generation()->free_unaffiliated_regions() * region_size_bytes,\n+           \"Unaffiliated available must be less than total available\");\n+    size_t old_fragmented_available =\n+      old_available - old_generation()->free_unaffiliated_regions() * region_size_bytes;\n+    reserve_for_mixed = max_evac_need + old_fragmented_available;\n+    if (reserve_for_mixed > max_old_reserve) {\n+      reserve_for_mixed = max_old_reserve;\n+    }\n+  }\n+\n+  \/\/ Decide how much space we should reserve for promotions from young\n+  size_t reserve_for_promo = 0;\n+  const size_t promo_load = get_promotion_potential();\n+  const bool doing_promotions = promo_load > 0;\n+  if (doing_promotions) {\n+    \/\/ We're promoting and have a bound on the maximum amount that can be promoted\n+    const size_t available_for_promotions = max_old_reserve - reserve_for_mixed;\n+    reserve_for_promo = MIN2((size_t)(promo_load * ShenandoahPromoEvacWaste), available_for_promotions);\n+  }\n+\n+  \/\/ This is the total old we want to ideally reserve\n+  const size_t old_reserve = reserve_for_mixed + reserve_for_promo;\n+  assert(old_reserve <= max_old_reserve, \"cannot reserve more than max for old evacuations\");\n+\n+  \/\/ We now check if the old generation is running a surplus or a deficit.\n+  size_t old_region_deficit = 0;\n+  size_t old_region_surplus = 0;\n+\n+  const size_t max_old_available = old_generation()->available() + old_cset_regions * region_size_bytes;\n+  if (max_old_available >= old_reserve) {\n+    \/\/ We are running a surplus, so the old region surplus can go to young\n+    const size_t old_surplus = max_old_available - old_reserve;\n+    old_region_surplus = old_surplus \/ region_size_bytes;\n+    const size_t unaffiliated_old_regions = old_generation()->free_unaffiliated_regions() + old_cset_regions;\n+    old_region_surplus = MIN2(old_region_surplus, unaffiliated_old_regions);\n+  } else {\n+    \/\/ We are running a deficit which we'd like to fill from young.\n+    \/\/ Ignore that this will directly impact young_generation()->max_capacity(),\n+    \/\/ indirectly impacting young_reserve and old_reserve.  These computations are conservative.\n+    const size_t old_need = old_reserve - max_old_available;\n+    \/\/ The old region deficit (rounded up) will come from young\n+    old_region_deficit = (old_need + region_size_bytes - 1) \/ region_size_bytes;\n+\n+    \/\/ Round down the regions we can transfer from young to old. If we're running short\n+    \/\/ on young-gen memory, we restrict the xfer. Old-gen collection activities will be\n+    \/\/ curtailed if the budget is restricted.\n+    const size_t max_old_region_xfer = xfer_limit \/ region_size_bytes;\n+    old_region_deficit = MIN2(old_region_deficit, max_old_region_xfer);\n+  }\n+  assert(old_region_deficit == 0 || old_region_surplus == 0, \"Only surplus or deficit, never both\");\n+\n+  set_old_region_surplus(old_region_surplus);\n+  set_old_region_deficit(old_region_deficit);\n+}\n+\n+\/\/ Called from stubs in JIT code or interpreter\n@@ -855,1 +1306,1 @@\n-  HeapWord* res = allocate_memory(req);\n+  HeapWord* res = allocate_memory(req, false);\n@@ -868,1 +1319,1 @@\n-  HeapWord* res = allocate_memory(req);\n+  HeapWord* res = allocate_memory(req, false);\n@@ -877,1 +1328,23 @@\n-HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest& req) {\n+HeapWord* ShenandoahHeap::allocate_new_plab(size_t min_size,\n+                                            size_t word_size,\n+                                            size_t* actual_size) {\n+  \/\/ Align requested sizes to card sized multiples\n+  size_t words_in_card = CardTable::card_size_in_words();\n+  size_t align_mask = ~(words_in_card - 1);\n+  min_size = (min_size + words_in_card - 1) & align_mask;\n+  word_size = (word_size + words_in_card - 1) & align_mask;\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(min_size, word_size);\n+  \/\/ Note that allocate_memory() sets a thread-local flag to prohibit further promotions by this thread\n+  \/\/ if we are at risk of infringing on the old-gen evacuation budget.\n+  HeapWord* res = allocate_memory(req, false);\n+  if (res != nullptr) {\n+    *actual_size = req.actual_size();\n+  } else {\n+    *actual_size = 0;\n+  }\n+  return res;\n+}\n+\n+\/\/ is_promotion is true iff this allocation is known for sure to hold the result of young-gen evacuation\n+\/\/ to old-gen.  plab allocates are not known as such, since they may hold old-gen evacuations.\n+HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest& req, bool is_promotion) {\n@@ -889,1 +1362,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -915,1 +1388,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -923,0 +1396,1 @@\n+\n@@ -925,1 +1399,1 @@\n-    result = allocate_memory_under_lock(req, in_new_region);\n+    result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -931,0 +1405,1 @@\n+<<<<<<< HEAD\n@@ -932,0 +1407,4 @@\n+=======\n+    control_thread()->notify_heap_changed();\n+    regulator_thread()->notify_heap_changed();\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -934,0 +1413,8 @@\n+  if (result == nullptr) {\n+    req.set_actual_size(0);\n+  }\n+\n+  \/\/ This is called regardless of the outcome of the allocation to account\n+  \/\/ for any waste created by retiring regions with this request.\n+  increase_used(req);\n+\n@@ -943,2 +1430,0 @@\n-      notify_mutator_alloc_words(actual, false);\n-\n@@ -951,2 +1436,0 @@\n-    } else {\n-      increase_used(actual*HeapWordSize);\n@@ -959,3 +1442,182 @@\n-HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest& req, bool& in_new_region) {\n-  ShenandoahHeapLocker locker(lock());\n-  return _free_set->allocate(req, in_new_region);\n+HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest& req, bool& in_new_region, bool is_promotion) {\n+  bool try_smaller_lab_size = false;\n+  size_t smaller_lab_size;\n+  {\n+    \/\/ promotion_eligible pertains only to PLAB allocations, denoting that the PLAB is allowed to allocate for promotions.\n+    bool promotion_eligible = false;\n+    bool allow_allocation = true;\n+    bool plab_alloc = false;\n+    size_t requested_bytes = req.size() * HeapWordSize;\n+    HeapWord* result = nullptr;\n+    ShenandoahHeapLocker locker(lock());\n+    Thread* thread = Thread::current();\n+\n+    if (mode()->is_generational()) {\n+      if (req.affiliation() == YOUNG_GENERATION) {\n+        if (req.is_mutator_alloc()) {\n+          size_t young_words_available = young_generation()->available() \/ HeapWordSize;\n+          if (req.is_lab_alloc() && (req.min_size() < young_words_available)) {\n+            \/\/ Allow ourselves to try a smaller lab size even if requested_bytes <= young_available.  We may need a smaller\n+            \/\/ lab size because young memory has become too fragmented.\n+            try_smaller_lab_size = true;\n+            smaller_lab_size = (young_words_available < req.size())? young_words_available: req.size();\n+          } else if (req.size() > young_words_available) {\n+            \/\/ Can't allocate because even min_size() is larger than remaining young_available\n+            log_info(gc, ergo)(\"Unable to shrink %s alloc request of minimum size: \" SIZE_FORMAT\n+                               \", young words available: \" SIZE_FORMAT, req.type_string(),\n+                               HeapWordSize * (req.is_lab_alloc()? req.min_size(): req.size()), young_words_available);\n+            return nullptr;\n+          }\n+        }\n+      } else {                    \/\/ reg.affiliation() == OLD_GENERATION\n+        assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"GCLAB pertains only to young-gen memory\");\n+        if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+          plab_alloc = true;\n+          size_t promotion_avail = get_promoted_reserve();\n+          size_t promotion_expended = get_promoted_expended();\n+          if (promotion_expended + requested_bytes > promotion_avail) {\n+            promotion_avail = 0;\n+            if (get_old_evac_reserve() == 0) {\n+              \/\/ There are no old-gen evacuations in this pass.  There's no value in creating a plab that cannot\n+              \/\/ be used for promotions.\n+              allow_allocation = false;\n+            }\n+          } else {\n+            promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n+            promotion_eligible = true;\n+          }\n+        } else if (is_promotion) {\n+          \/\/ This is a shared alloc for promotion\n+          size_t promotion_avail = get_promoted_reserve();\n+          size_t promotion_expended = get_promoted_expended();\n+          if (promotion_expended + requested_bytes > promotion_avail) {\n+            promotion_avail = 0;\n+          } else {\n+            promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n+          }\n+          if (promotion_avail == 0) {\n+            \/\/ We need to reserve the remaining memory for evacuation.  Reject this allocation.  The object will be\n+            \/\/ evacuated to young-gen memory and promoted during a future GC pass.\n+            return nullptr;\n+          }\n+          \/\/ Else, we'll allow the allocation to proceed.  (Since we hold heap lock, the tested condition remains true.)\n+        } else {\n+          \/\/ This is a shared allocation for evacuation.  Memory has already been reserved for this purpose.\n+        }\n+      }\n+    } \/\/ This ends the is_generational() block\n+\n+    \/\/ First try the original request.  If TLAB request size is greater than available, allocate() will attempt to downsize\n+    \/\/ request to fit within available memory.\n+    result = (allow_allocation)? _free_set->allocate(req, in_new_region): nullptr;\n+    if (result != nullptr) {\n+      if (req.is_old()) {\n+        ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+        if (req.is_gc_alloc()) {\n+          bool disable_plab_promotions = false;\n+          if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+            if (promotion_eligible) {\n+              size_t actual_size = req.actual_size() * HeapWordSize;\n+              \/\/ The actual size of the allocation may be larger than the requested bytes (due to alignment on card boundaries).\n+              \/\/ If this puts us over our promotion budget, we need to disable future PLAB promotions for this thread.\n+              if (get_promoted_expended() + actual_size <= get_promoted_reserve()) {\n+                \/\/ Assume the entirety of this PLAB will be used for promotion.  This prevents promotion from overreach.\n+                \/\/ When we retire this plab, we'll unexpend what we don't really use.\n+                ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+                expend_promoted(actual_size);\n+                assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+                ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, actual_size);\n+              } else {\n+                disable_plab_promotions = true;\n+              }\n+            } else {\n+              disable_plab_promotions = true;\n+            }\n+            if (disable_plab_promotions) {\n+              \/\/ Disable promotions in this thread because entirety of this PLAB must be available to hold old-gen evacuations.\n+              ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+              ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+            }\n+          } else if (is_promotion) {\n+            \/\/ Shared promotion.  Assume size is requested_bytes.\n+            expend_promoted(requested_bytes);\n+            assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+          }\n+        }\n+\n+        \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+        \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+        \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+        \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+        \/\/\n+        \/\/ objects being \"concurrently\" allocated:\n+        \/\/    [-----a------][-----b-----][--------------c------------------]\n+        \/\/            [---- card table memory range --------------]\n+        \/\/\n+        \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n+        \/\/   wants to set the starts-object, first-start, and last-start attributes of the preceding card region.\n+        \/\/   allocation of object b wants to set the starts-object, first-start, and last-start attributes of this card region.\n+        \/\/   allocation of object c also wants to set the starts-object, first-start, and last-start attributes of this\n+        \/\/   card region.\n+        \/\/\n+        \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n+        \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n+        \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+        ShenandoahHeap::heap()->card_scan()->register_object(result);\n+      }\n+    } else {\n+      \/\/ The allocation failed.  If this was a plab allocation, We've already retired it and no longer have a plab.\n+      if (req.is_old() && req.is_gc_alloc() && (req.type() == ShenandoahAllocRequest::_alloc_plab)) {\n+        \/\/ We don't need to disable PLAB promotions because there is no PLAB.  We leave promotions enabled because\n+        \/\/ this allows the surrounding infrastructure to retry alloc_plab_slow() with a smaller PLAB size.\n+        ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+      }\n+    }\n+    if ((result != nullptr) || !try_smaller_lab_size) {\n+      return result;\n+    }\n+    \/\/ else, fall through to try_smaller_lab_size\n+  } \/\/ This closes the block that holds the heap lock, releasing the lock.\n+\n+  \/\/ We failed to allocate the originally requested lab size.  Let's see if we can allocate a smaller lab size.\n+  if (req.size() == smaller_lab_size) {\n+    \/\/ If we were already trying to allocate min size, no value in attempting to repeat the same.  End the recursion.\n+    return nullptr;\n+  }\n+\n+  \/\/ We arrive here if the tlab allocation request can be resized to fit within young_available\n+  assert((req.affiliation() == YOUNG_GENERATION) && req.is_lab_alloc() && req.is_mutator_alloc() &&\n+         (smaller_lab_size < req.size()), \"Only shrink allocation request size for TLAB allocations\");\n+\n+  \/\/ By convention, ShenandoahAllocationRequest is primarily read-only.  The only mutable instance data is represented by\n+  \/\/ actual_size(), which is overwritten with the size of the allocaion when the allocation request is satisfied.  We use a\n+  \/\/ recursive call here rather than introducing new methods to mutate the existing ShenandoahAllocationRequest argument.\n+  \/\/ Mutation of the existing object might result in astonishing results if calling contexts assume the content of immutable\n+  \/\/ fields remain constant.  The original TLAB allocation request was for memory that exceeded the current capacity.  We'll\n+  \/\/ attempt to allocate a smaller TLAB.  If this is successful, we'll update actual_size() of our incoming\n+  \/\/ ShenandoahAllocRequest.  If the recursive request fails, we'll simply return nullptr.\n+\n+  \/\/ Note that we've relinquished the HeapLock and some other thread may perform additional allocation before our recursive\n+  \/\/ call reacquires the lock.  If that happens, we will need another recursive call to further reduce the size of our request\n+  \/\/ for each time another thread allocates young memory during the brief intervals that the heap lock is available to\n+  \/\/ interfering threads.  We expect this interference to be rare.  The recursion bottoms out when young_available is\n+  \/\/ smaller than req.min_size().  The inner-nested call to allocate_memory_under_lock() uses the same min_size() value\n+  \/\/ as this call, but it uses a preferred size() that is smaller than our preferred size, and is no larger than what we most\n+  \/\/ recently saw as the memory currently available within the young generation.\n+\n+  \/\/ TODO: At the expense of code clarity, we could rewrite this recursive solution to use iteration.  We need at most one\n+  \/\/ extra instance of the ShenandoahAllocRequest, which we can re-initialize multiple times inside a loop, with one iteration\n+  \/\/ of the loop required for each time the existing solution would recurse.  An iterative solution would be more efficient\n+  \/\/ in CPU time and stack memory utilization.  The expectation is that it is very rare that we would recurse more than once\n+  \/\/ so making this change is not currently seen as a high priority.\n+\n+  ShenandoahAllocRequest smaller_req = ShenandoahAllocRequest::for_tlab(req.min_size(), smaller_lab_size);\n+\n+  \/\/ Note that shrinking the preferred size gets us past the gatekeeper that checks whether there's available memory to\n+  \/\/ satisfy the allocation request.  The reality is the actual TLAB size is likely to be even smaller, because it will\n+  \/\/ depend on how much memory is available within mutator regions that are not yet fully used.\n+  HeapWord* result = allocate_memory_under_lock(smaller_req, in_new_region, is_promotion);\n+  if (result != nullptr) {\n+    req.set_actual_size(smaller_req.actual_size());\n+  }\n+  return result;\n@@ -967,1 +1629,1 @@\n-  return allocate_memory(req);\n+  return allocate_memory(req, false);\n@@ -976,2 +1638,2 @@\n-  if (heuristics()->can_unload_classes()) {\n-    ShenandoahHeuristics* h = heuristics();\n+  ShenandoahHeuristics* h = global_generation()->heuristics();\n+  if (h->can_unload_classes()) {\n@@ -1056,0 +1718,1 @@\n+\n@@ -1061,0 +1724,90 @@\n+      if (_sh->check_cancelled_gc_and_yield(_concurrent)) {\n+        break;\n+      }\n+    }\n+  }\n+};\n+\n+\/\/ Unlike ShenandoahEvacuationTask, this iterates over all regions rather than just the collection set.\n+\/\/ This is needed in order to promote humongous start regions if age() >= tenure threshold.\n+class ShenandoahGenerationalEvacuationTask : public WorkerTask {\n+private:\n+  ShenandoahHeap* const _sh;\n+  ShenandoahRegionIterator *_regions;\n+  bool _concurrent;\n+  uint _tenuring_threshold;\n+\n+public:\n+  ShenandoahGenerationalEvacuationTask(ShenandoahHeap* sh,\n+                                       ShenandoahRegionIterator* iterator,\n+                                       bool concurrent) :\n+    WorkerTask(\"Shenandoah Evacuation\"),\n+    _sh(sh),\n+    _regions(iterator),\n+    _concurrent(concurrent),\n+    _tenuring_threshold(0)\n+  {\n+    if (_sh->mode()->is_generational()) {\n+      _tenuring_threshold = _sh->age_census()->tenuring_threshold();\n+    }\n+  }\n+\n+  void work(uint worker_id) {\n+    if (_concurrent) {\n+      ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+      ShenandoahSuspendibleThreadSetJoiner stsj;\n+      ShenandoahEvacOOMScope oom_evac_scope;\n+      do_work();\n+    } else {\n+      ShenandoahParallelWorkerSession worker_session(worker_id);\n+      ShenandoahEvacOOMScope oom_evac_scope;\n+      do_work();\n+    }\n+  }\n+\n+private:\n+  void do_work() {\n+    ShenandoahConcurrentEvacuateRegionObjectClosure cl(_sh);\n+    ShenandoahHeapRegion* r;\n+    ShenandoahMarkingContext* const ctx = ShenandoahHeap::heap()->marking_context();\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    size_t old_garbage_threshold = (region_size_bytes * ShenandoahOldGarbageThreshold) \/ 100;\n+    while ((r = _regions->next()) != nullptr) {\n+      log_debug(gc)(\"GenerationalEvacuationTask do_work(), looking at %s region \" SIZE_FORMAT \", (age: %d) [%s, %s, %s]\",\n+                    r->is_old()? \"old\": r->is_young()? \"young\": \"free\", r->index(), r->age(),\n+                    r->is_active()? \"active\": \"inactive\",\n+                    r->is_humongous()? (r->is_humongous_start()? \"humongous_start\": \"humongous_continuation\"): \"regular\",\n+                    r->is_cset()? \"cset\": \"not-cset\");\n+\n+      if (r->is_cset()) {\n+        assert(r->has_live(), \"Region \" SIZE_FORMAT \" should have been reclaimed early\", r->index());\n+        _sh->marked_object_iterate(r, &cl);\n+        if (ShenandoahPacing) {\n+          _sh->pacer()->report_evac(r->used() >> LogHeapWordSize);\n+        }\n+      } else if (r->is_young() && r->is_active() && (r->age() >= _tenuring_threshold)) {\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        if (r->is_humongous_start()) {\n+          \/\/ We promote humongous_start regions along with their affiliated continuations during evacuation rather than\n+          \/\/ doing this work during a safepoint.  We cannot put humongous regions into the collection set because that\n+          \/\/ triggers the load-reference barrier (LRB) to copy on reference fetch.\n+          r->promote_humongous();\n+        } else if (r->is_regular() && (r->get_top_before_promote() != nullptr)) {\n+          assert(r->garbage_before_padded_for_promote() < old_garbage_threshold,\n+                 \"Region \" SIZE_FORMAT \" has too much garbage for promotion\", r->index());\n+          assert(r->get_top_before_promote() == tams,\n+                 \"Region \" SIZE_FORMAT \" has been used for allocations before promotion\", r->index());\n+          \/\/ Likewise, we cannot put promote-in-place regions into the collection set because that would also trigger\n+          \/\/ the LRB to copy on reference fetch.\n+          r->promote_in_place();\n+        }\n+        \/\/ Aged humongous continuation regions are handled with their start region.  If an aged regular region has\n+        \/\/ more garbage than ShenandoahOldGarbageTrheshold, we'll promote by evacuation.  If there is room for evacuation\n+        \/\/ in this cycle, the region will be in the collection set.  If there is not room, the region will be promoted\n+        \/\/ by evacuation in some future GC cycle.\n+\n+        \/\/ If an aged regular region has received allocations during the current cycle, we do not promote because the\n+        \/\/ newly allocated objects do not have appropriate age; this region's age will be reset to zero at end of cycle.\n+      }\n+      \/\/ else, region is free, or OLD, or not in collection set, or humongous_continuation,\n+      \/\/ or is young humongous_start that is too young to be promoted\n@@ -1070,2 +1823,8 @@\n-  ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n-  workers()->run_task(&task);\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahRegionIterator regions;\n+    ShenandoahGenerationalEvacuationTask task(this, &regions, concurrent);\n+    workers()->run_task(&task);\n+  } else {\n+    ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n+    workers()->run_task(&task);\n+  }\n@@ -1101,1 +1860,1 @@\n-void ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n+size_t ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n@@ -1121,0 +1880,1 @@\n+  return required_regions;\n@@ -1130,0 +1890,4 @@\n+\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+    assert(plab->words_remaining() == 0, \"PLAB should not need retirement\");\n@@ -1145,0 +1909,11 @@\n+\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+\n+    \/\/ There are two reasons to retire all plabs between old-gen evacuation passes.\n+    \/\/  1. We need to make the plab memory parsable by remembered-set scanning.\n+    \/\/  2. We need to establish a trustworthy UpdateWaterMark value within each old-gen heap region\n+    ShenandoahHeap::heap()->retire_plab(plab, thread);\n+    if (_resize && ShenandoahThreadLocalData::plab_size(thread) > 0) {\n+      ShenandoahThreadLocalData::set_plab_size(thread, 0);\n+    }\n@@ -1204,3 +1979,6 @@\n-  \/\/ Return the max allowed size, and let the allocation path\n-  \/\/ figure out the safe size for current allocation.\n-  return ShenandoahHeapRegion::max_tlab_size_bytes();\n+  if (mode()->is_generational()) {\n+    return MIN2(ShenandoahHeapRegion::max_tlab_size_bytes(), young_generation()->available());\n+  } else {\n+    \/\/ Return the max allowed size, and let the allocation path figure out the safe size for current allocation.\n+    return ShenandoahHeapRegion::max_tlab_size_bytes();\n+  }\n@@ -1246,0 +2024,4 @@\n+  if (_shenandoah_policy->is_at_shutdown()) {\n+    return;\n+  }\n+\n@@ -1247,0 +2029,1 @@\n+  tcl->do_thread(_regulator_thread);\n@@ -1266,0 +2049,4 @@\n+    ls.cr();\n+\n+    evac_tracker()->print_global_on(&ls);\n+\n@@ -1271,0 +2058,18 @@\n+void ShenandoahHeap::on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation) {\n+  set_gc_cause(cause);\n+  set_gc_generation(generation);\n+\n+  shenandoah_policy()->record_cycle_start();\n+  generation->heuristics()->record_cycle_start();\n+}\n+\n+void ShenandoahHeap::on_cycle_end(ShenandoahGeneration* generation) {\n+  generation->heuristics()->record_cycle_end();\n+  if (mode()->is_generational() && generation->is_global()) {\n+    \/\/ If we just completed a GLOBAL GC, claim credit for completion of young-gen and old-gen GC as well\n+    young_generation()->heuristics()->record_cycle_end();\n+    old_generation()->heuristics()->record_cycle_end();\n+  }\n+  set_gc_cause(GCCause::_no_gc);\n+}\n+\n@@ -1592,23 +2397,0 @@\n-class ShenandoahInitMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahInitMarkUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-    if (r->is_active()) {\n-      \/\/ Check if region needs updating its TAMS. We have updated it already during concurrent\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n-      if (_ctx->top_at_mark_start(r) != r->top()) {\n-        _ctx->capture_top_at_mark_start(r);\n-      }\n-    } else {\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should already have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n@@ -1630,99 +2412,0 @@\n-class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n-      \/\/ anyway to capture any updates that happened since now.\n-      r->clear_live_data();\n-      _ctx->capture_top_at_mark_start(r);\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_gc() {\n-  reset_mark_bitmap();\n-\n-  ShenandoahResetUpdateRegionStateClosure cl;\n-  parallel_heap_region_iterate(&cl);\n-}\n-\n-class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalMarkUpdateRegionStateClosure() :\n-    _ctx(ShenandoahHeap::heap()->complete_marking_context()), _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n-      \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n-      HeapWord *tams = _ctx->top_at_mark_start(r);\n-      HeapWord *top = r->top();\n-      if (top > tams) {\n-        r->increase_live_data_alloc_words(pointer_delta(top, tams));\n-      }\n-\n-      \/\/ We are about to select the collection set, make sure it knows about\n-      \/\/ current pinning status. Also, this allows trashing more regions that\n-      \/\/ now have their pinning status dropped.\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n-      }\n-\n-      \/\/ Remember limit for updating refs. It's guaranteed that we get no\n-      \/\/ from-space-refs written from here on.\n-      r->set_update_watermark_at_safepoint(r->top());\n-    } else {\n-      assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_regions_and_collection_set(bool concurrent) {\n-  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states :\n-                                         ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n-    ShenandoahFinalMarkUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n-\n-    assert_pinned_region_status();\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset :\n-                                         ShenandoahPhaseTimings::degen_gc_choose_cset);\n-    ShenandoahHeapLocker locker(lock());\n-    _collection_set->clear();\n-    heuristics()->choose_collection_set(_collection_set);\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset :\n-                                         ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n-  }\n-}\n-\n@@ -1739,1 +2422,1 @@\n-  ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n+  active_generation()->ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n@@ -1775,4 +2458,60 @@\n-void ShenandoahHeap::set_concurrent_mark_in_progress(bool in_progress) {\n-  assert(!has_forwarded_objects(), \"Not expected before\/after mark phase\");\n-  set_gc_state(MARKING, in_progress);\n-  ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(in_progress, !in_progress);\n+void ShenandoahHeap::set_evacuation_reserve_quantities(bool is_valid) {\n+  _has_evacuation_reserve_quantities = is_valid;\n+}\n+\n+void ShenandoahHeap::set_concurrent_young_mark_in_progress(bool in_progress) {\n+  uint mask;\n+  assert(!has_forwarded_objects(), \"Young marking is not concurrent with evacuation\");\n+  if (!in_progress && is_concurrent_old_mark_in_progress()) {\n+    assert(mode()->is_generational(), \"Only generational GC has old marking\");\n+    assert(_gc_state.is_set(MARKING), \"concurrent_old_marking_in_progress implies MARKING\");\n+    \/\/ If old-marking is in progress when we turn off YOUNG_MARKING, leave MARKING (and OLD_MARKING) on\n+    mask = YOUNG_MARKING;\n+  } else {\n+    mask = MARKING | YOUNG_MARKING;\n+  }\n+  set_gc_state(mask, in_progress);\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::set_concurrent_old_mark_in_progress(bool in_progress) {\n+#ifdef ASSERT\n+  \/\/ has_forwarded_objects() iff UPDATEREFS or EVACUATION\n+  bool has_forwarded = has_forwarded_objects();\n+  bool updating_or_evacuating = _gc_state.is_set(UPDATEREFS | EVACUATION);\n+  bool evacuating = _gc_state.is_set(EVACUATION);\n+  assert ((has_forwarded == updating_or_evacuating) || (evacuating && !has_forwarded && collection_set()->is_empty()),\n+          \"Updating or evacuating iff has forwarded objects, or if evacuation phase is promoting in place without forwarding\");\n+#endif\n+  if (!in_progress && is_concurrent_young_mark_in_progress()) {\n+    \/\/ If young-marking is in progress when we turn off OLD_MARKING, leave MARKING (and YOUNG_MARKING) on\n+    assert(_gc_state.is_set(MARKING), \"concurrent_young_marking_in_progress implies MARKING\");\n+    set_gc_state(OLD_MARKING, in_progress);\n+  } else {\n+    set_gc_state(MARKING | OLD_MARKING, in_progress);\n+  }\n+  manage_satb_barrier(in_progress);\n+}\n+\n+bool ShenandoahHeap::is_prepare_for_old_mark_in_progress() const {\n+  return old_generation()->state() == ShenandoahOldGeneration::FILLING;\n+}\n+\n+void ShenandoahHeap::set_aging_cycle(bool in_progress) {\n+  _is_aging_cycle.set_cond(in_progress);\n+}\n+\n+void ShenandoahHeap::manage_satb_barrier(bool active) {\n+  if (is_concurrent_mark_in_progress()) {\n+    \/\/ Ignore request to deactivate barrier while concurrent mark is in progress.\n+    \/\/ Do not attempt to re-activate the barrier if it is already active.\n+    if (active && !ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  } else {\n+    \/\/ No concurrent marking is in progress so honor request to deactivate,\n+    \/\/ but only if the barrier is already active.\n+    if (!active && ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  }\n@@ -1811,0 +2550,8 @@\n+void ShenandoahHeap::cancel_concurrent_mark() {\n+  _young_generation->cancel_marking();\n+  _old_generation->cancel_marking();\n+  _global_generation->cancel_marking();\n+\n+  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+}\n+\n@@ -1816,0 +2563,1 @@\n+    _cancel_requested_time = os::elapsedTime();\n@@ -1826,1 +2574,1 @@\n-  \/\/ Step 0. Notify policy to disable event recording.\n+  \/\/ Step 1. Notify policy to disable event recording and prevent visiting gc threads during shutdown\n@@ -1829,1 +2577,4 @@\n-  \/\/ Step 1. Notify control thread that we are in shutdown.\n+  \/\/ Step 2. Stop requesting collections.\n+  regulator_thread()->stop();\n+\n+  \/\/ Step 3. Notify control thread that we are in shutdown.\n@@ -1834,1 +2585,1 @@\n-  \/\/ Step 2. Notify GC workers that we are cancelling GC.\n+  \/\/ Step 4. Notify GC workers that we are cancelling GC.\n@@ -1837,1 +2588,1 @@\n-  \/\/ Step 3. Wait until GC worker exits normally.\n+  \/\/ Step 5. Wait until GC worker exits normally.\n@@ -1939,4 +2690,0 @@\n-size_t ShenandoahHeap::bytes_allocated_since_gc_start() const {\n-  return Atomic::load(&_bytes_allocated_since_gc_start);\n-}\n-\n@@ -1944,1 +2691,6 @@\n-  Atomic::store(&_bytes_allocated_since_gc_start, (size_t)0);\n+  if (mode()->is_generational()) {\n+    young_generation()->reset_bytes_allocated_since_gc_start();\n+    old_generation()->reset_bytes_allocated_since_gc_start();\n+  }\n+\n+  global_generation()->reset_bytes_allocated_since_gc_start();\n@@ -2008,2 +2760,4 @@\n-    assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n-           \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    if (active_generation()->contains(r)) {\n+      assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n+             \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    }\n@@ -2063,0 +2817,2 @@\n+  ShenandoahRegionChunkIterator* _work_chunks;\n+\n@@ -2064,1 +2820,2 @@\n-  ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n+  explicit ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions,\n+                                        ShenandoahRegionChunkIterator* work_chunks) :\n@@ -2067,1 +2824,4 @@\n-    _regions(regions) {\n+    _regions(regions),\n+    _work_chunks(work_chunks)\n+  {\n+    log_info(gc, remset)(\"Scan remembered set using bitmap: %s\", BOOL_TO_STR(_heap->is_old_bitmap_stable()));\n@@ -2074,1 +2834,1 @@\n-      do_work<ShenandoahConcUpdateRefsClosure>();\n+      do_work<ShenandoahConcUpdateRefsClosure>(worker_id);\n@@ -2077,1 +2837,1 @@\n-      do_work<ShenandoahSTWUpdateRefsClosure>();\n+      do_work<ShenandoahSTWUpdateRefsClosure>(worker_id);\n@@ -2083,1 +2843,1 @@\n-  void do_work() {\n+  void do_work(uint worker_id) {\n@@ -2085,0 +2845,10 @@\n+    if (CONCURRENT && (worker_id == 0)) {\n+      \/\/ We ask the first worker to replenish the Mutator free set by moving regions previously reserved to hold the\n+      \/\/ results of evacuation.  These reserves are no longer necessary because evacuation has completed.\n+      size_t cset_regions = _heap->collection_set()->count();\n+      \/\/ We cannot transfer any more regions than will be reclaimed when the existing collection set is recycled, because\n+      \/\/ we need the reclaimed collection set regions to replenish the collector reserves\n+      _heap->free_set()->move_collector_sets_to_mutator(cset_regions);\n+    }\n+    \/\/ If !CONCURRENT, there's no value in expanding Mutator free set\n+\n@@ -2086,1 +2856,4 @@\n-    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+    \/\/ We update references for global, old, and young collections.\n+    assert(_heap->active_generation()->is_mark_complete(), \"Expected complete marking\");\n+    ShenandoahMarkingContext* const ctx = _heap->marking_context();\n+    bool is_mixed = _heap->collection_set()->has_old_regions();\n@@ -2090,0 +2863,3 @@\n+\n+      log_debug(gc)(\"ShenandoahUpdateHeapRefsTask::do_work(%u) looking at region \" SIZE_FORMAT, worker_id, r->index());\n+      bool region_progress = false;\n@@ -2091,1 +2867,31 @@\n-        _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+        if (!_heap->mode()->is_generational() || r->is_young()) {\n+          _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+          region_progress = true;\n+        } else if (r->is_old()) {\n+          if (_heap->active_generation()->is_global()) {\n+            \/\/ Note that GLOBAL collection is not as effectively balanced as young and mixed cycles.  This is because\n+            \/\/ concurrent GC threads are parceled out entire heap regions of work at a time and there\n+            \/\/ is no \"catchup phase\" consisting of remembered set scanning, during which parcels of work are smaller\n+            \/\/ and more easily distributed more fairly across threads.\n+\n+            \/\/ TODO: Consider an improvement to load balance GLOBAL GC.\n+            _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+            region_progress = true;\n+          }\n+          \/\/ Otherwise, this is an old region in a young or mixed cycle.  Process it during a second phase, below.\n+          \/\/ Don't bother to report pacing progress in this case.\n+        } else {\n+          \/\/ Because updating of references runs concurrently, it is possible that a FREE inactive region transitions\n+          \/\/ to a non-free active region while this loop is executing.  Whenever this happens, the changing of a region's\n+          \/\/ active status may propagate at a different speed than the changing of the region's affiliation.\n+\n+          \/\/ When we reach this control point, it is because a race has allowed a region's is_active() status to be seen\n+          \/\/ by this thread before the region's affiliation() is seen by this thread.\n+\n+          \/\/ It's ok for this race to occur because the newly transformed region does not have any references to be\n+          \/\/ updated.\n+\n+          assert(r->get_update_watermark() == r->bottom(),\n+                 \"%s Region \" SIZE_FORMAT \" is_active but not recognized as YOUNG or OLD so must be newly transitioned from FREE\",\n+                 r->affiliation_name(), r->index());\n+        }\n@@ -2093,1 +2899,1 @@\n-      if (ShenandoahPacing) {\n+      if (region_progress && ShenandoahPacing) {\n@@ -2101,0 +2907,114 @@\n+\n+    if (_heap->mode()->is_generational() && !_heap->active_generation()->is_global()) {\n+      \/\/ Since this is generational and not GLOBAL, we have to process the remembered set.  There's no remembered\n+      \/\/ set processing if not in generational mode or if GLOBAL mode.\n+\n+      \/\/ After this thread has exhausted its traditional update-refs work, it continues with updating refs within remembered set.\n+      \/\/ The remembered set workload is better balanced between threads, so threads that are \"behind\" can catch up with other\n+      \/\/ threads during this phase, allowing all threads to work more effectively in parallel.\n+      struct ShenandoahRegionChunk assignment;\n+      RememberedScanner* scanner = _heap->card_scan();\n+\n+      while (!_heap->check_cancelled_gc_and_yield(CONCURRENT) && _work_chunks->next(&assignment)) {\n+        \/\/ Keep grabbing next work chunk to process until finished, or asked to yield\n+        ShenandoahHeapRegion* r = assignment._r;\n+        if (r->is_active() && !r->is_cset() && r->is_old()) {\n+          HeapWord* start_of_range = r->bottom() + assignment._chunk_offset;\n+          HeapWord* end_of_range = r->get_update_watermark();\n+          if (end_of_range > start_of_range + assignment._chunk_size) {\n+            end_of_range = start_of_range + assignment._chunk_size;\n+          }\n+\n+          \/\/ Old region in a young cycle or mixed cycle.\n+          if (is_mixed) {\n+            \/\/ TODO: For mixed evac, consider building an old-gen remembered set that allows restricted updating\n+            \/\/ within old-gen HeapRegions.  This remembered set can be constructed by old-gen concurrent marking\n+            \/\/ and augmented by card marking.  For example, old-gen concurrent marking can remember for each old-gen\n+            \/\/ card which other old-gen regions it refers to: none, one-other specifically, multiple-other non-specific.\n+            \/\/ Update-references when _mixed_evac processess each old-gen memory range that has a traditional DIRTY\n+            \/\/ card or if the \"old-gen remembered set\" indicates that this card holds pointers specifically to an\n+            \/\/ old-gen region in the most recent collection set, or if this card holds pointers to other non-specific\n+            \/\/ old-gen heap regions.\n+\n+            if (r->is_humongous()) {\n+              if (start_of_range < end_of_range) {\n+                \/\/ Need to examine both dirty and clean cards during mixed evac.\n+                r->oop_iterate_humongous_slice(&cl, false, start_of_range, assignment._chunk_size, true);\n+              }\n+            } else {\n+              \/\/ Since this is mixed evacuation, old regions that are candidates for collection have not been coalesced\n+              \/\/ and filled.  Use mark bits to find objects that need to be updated.\n+              \/\/\n+              \/\/ Future TODO: establish a second remembered set to identify which old-gen regions point to other old-gen\n+              \/\/ regions which are in the collection set for a particular mixed evacuation.\n+              if (start_of_range < end_of_range) {\n+                HeapWord* p = nullptr;\n+                size_t card_index = scanner->card_index_for_addr(start_of_range);\n+                \/\/ In case last object in my range spans boundary of my chunk, I may need to scan all the way to top()\n+                ShenandoahObjectToOopBoundedClosure<T> objs(&cl, start_of_range, r->top());\n+\n+                \/\/ Any object that begins in a previous range is part of a different scanning assignment.  Any object that\n+                \/\/ starts after end_of_range is also not my responsibility.  (Either allocated during evacuation, so does\n+                \/\/ not hold pointers to from-space, or is beyond the range of my assigned work chunk.)\n+\n+                \/\/ Find the first object that begins in my range, if there is one.\n+                p = start_of_range;\n+                oop obj = cast_to_oop(p);\n+                HeapWord* tams = ctx->top_at_mark_start(r);\n+                if (p >= tams) {\n+                  \/\/ We cannot use ctx->is_marked(obj) to test whether an object begins at this address.  Instead,\n+                  \/\/ we need to use the remembered set crossing map to advance p to the first object that starts\n+                  \/\/ within the enclosing card.\n+\n+                  while (true) {\n+                    HeapWord* first_object = scanner->first_object_in_card(card_index);\n+                    if (first_object != nullptr) {\n+                      p = first_object;\n+                      break;\n+                    } else if (scanner->addr_for_card_index(card_index + 1) < end_of_range) {\n+                      card_index++;\n+                    } else {\n+                      \/\/ Force the loop that follows to immediately terminate.\n+                      p = end_of_range;\n+                      break;\n+                    }\n+                  }\n+                  obj = cast_to_oop(p);\n+                  \/\/ Note: p may be >= end_of_range\n+                } else if (!ctx->is_marked(obj)) {\n+                  p = ctx->get_next_marked_addr(p, tams);\n+                  obj = cast_to_oop(p);\n+                  \/\/ If there are no more marked objects before tams, this returns tams.\n+                  \/\/ Note that tams is either >= end_of_range, or tams is the start of an object that is marked.\n+                }\n+                while (p < end_of_range) {\n+                  \/\/ p is known to point to the beginning of marked object obj\n+                  objs.do_object(obj);\n+                  HeapWord* prev_p = p;\n+                  p += obj->size();\n+                  if (p < tams) {\n+                    p = ctx->get_next_marked_addr(p, tams);\n+                    \/\/ If there are no more marked objects before tams, this returns tams.  Note that tams is\n+                    \/\/ either >= end_of_range, or tams is the start of an object that is marked.\n+                  }\n+                  assert(p != prev_p, \"Lack of forward progress\");\n+                  obj = cast_to_oop(p);\n+                }\n+              }\n+            }\n+          } else {\n+            \/\/ This is a young evac..\n+            if (start_of_range < end_of_range) {\n+              size_t cluster_size =\n+                CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+              size_t clusters = assignment._chunk_size \/ cluster_size;\n+              assert(clusters * cluster_size == assignment._chunk_size, \"Chunk assignment must align on cluster boundaries\");\n+              scanner->process_region_slice(r, assignment._chunk_offset, clusters, end_of_range, &cl, true, worker_id);\n+            }\n+          }\n+          if (ShenandoahPacing && (start_of_range < end_of_range)) {\n+            _heap->pacer()->report_updaterefs(pointer_delta(end_of_range, start_of_range));\n+          }\n+        }\n+      }\n+    }\n@@ -2106,0 +3026,2 @@\n+  uint nworkers = workers()->active_workers();\n+  ShenandoahRegionChunkIterator work_list(nworkers);\n@@ -2108,1 +3030,1 @@\n-    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator, &work_list);\n@@ -2111,1 +3033,1 @@\n-    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator, &work_list);\n@@ -2114,0 +3036,3 @@\n+  if (ShenandoahEnableCardStats && card_scan()!=nullptr) { \/\/ generational check proxy\n+    card_scan()->log_card_stats(nworkers, CARD_STAT_UPDATE_REFS);\n+  }\n@@ -2116,1 +3041,0 @@\n-\n@@ -2119,0 +3043,1 @@\n+  ShenandoahMarkingContext* _ctx;\n@@ -2120,0 +3045,1 @@\n+  bool _is_generational;\n@@ -2122,1 +3048,3 @@\n-  ShenandoahFinalUpdateRefsUpdateRegionStateClosure() : _lock(ShenandoahHeap::heap()->lock()) {}\n+  ShenandoahFinalUpdateRefsUpdateRegionStateClosure(\n+    ShenandoahMarkingContext* ctx) : _ctx(ctx), _lock(ShenandoahHeap::heap()->lock()),\n+                                     _is_generational(ShenandoahHeap::heap()->mode()->is_generational()) { }\n@@ -2125,0 +3053,21 @@\n+\n+    \/\/ Maintenance of region age must follow evacuation in order to account for evacuation allocations within survivor\n+    \/\/ regions.  We consult region age during the subsequent evacuation to determine whether certain objects need to\n+    \/\/ be promoted.\n+    if (_is_generational && r->is_young() && r->is_active()) {\n+      HeapWord *tams = _ctx->top_at_mark_start(r);\n+      HeapWord *top = r->top();\n+\n+      \/\/ Allocations move the watermark when top moves.  However compacting\n+      \/\/ objects will sometimes lower top beneath the watermark, after which,\n+      \/\/ attempts to read the watermark will assert out (watermark should not be\n+      \/\/ higher than top).\n+      if (top > tams) {\n+        \/\/ There have been allocations in this region since the start of the cycle.\n+        \/\/ Any objects new to this region must not assimilate elevated age.\n+        r->reset_age();\n+      } else if (ShenandoahHeap::heap()->is_aging_cycle()) {\n+        r->increment_age();\n+      }\n+    }\n+\n@@ -2127,1 +3076,0 @@\n-\n@@ -2154,1 +3102,1 @@\n-    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl;\n+    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl (active_generation()->complete_marking_context());\n@@ -2169,6 +3117,84 @@\n-  {\n-    ShenandoahGCPhase phase(concurrent ?\n-                            ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n-                            ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n+  ShenandoahGCPhase phase(concurrent ?\n+                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n+                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  ShenandoahHeapLocker locker(lock());\n+  size_t young_cset_regions, old_cset_regions;\n+  size_t first_old_region, last_old_region, old_region_count;\n+  _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+  \/\/ If there are no old regions, first_old_region will be greater than last_old_region\n+  assert((first_old_region > last_old_region) ||\n+         ((last_old_region + 1 - first_old_region >= old_region_count) &&\n+          get_region(first_old_region)->is_old() && get_region(last_old_region)->is_old()),\n+         \"sanity: old_region_count: \" SIZE_FORMAT \", first_old_region: \" SIZE_FORMAT \", last_old_region: \" SIZE_FORMAT,\n+         old_region_count, first_old_region, last_old_region);\n+\n+  if (mode()->is_generational()) {\n+    assert(verify_generation_usage(true, old_generation()->used_regions(),\n+                                   old_generation()->used(), old_generation()->get_humongous_waste(),\n+                                   true, young_generation()->used_regions(),\n+                                   young_generation()->used(), young_generation()->get_humongous_waste()),\n+           \"Generation accounts are inaccurate\");\n+\n+    \/\/ The computation of bytes_of_allocation_runway_before_gc_trigger is quite conservative so consider all of this\n+    \/\/ available for transfer to old. Note that transfer of humongous regions does not impact available.\n+    size_t allocation_runway = young_heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions);\n+    adjust_generation_sizes_for_next_cycle(allocation_runway, young_cset_regions, old_cset_regions);\n+\n+    \/\/ Total old_available may have been expanded to hold anticipated promotions.  We trigger if the fragmented available\n+    \/\/ memory represents more than 16 regions worth of data.  Note that fragmentation may increase when we promote regular\n+    \/\/ regions in place when many of these regular regions have an abundant amount of available memory within them.  Fragmentation\n+    \/\/ will decrease as promote-by-copy consumes the available memory within these partially consumed regions.\n+    \/\/\n+    \/\/ We consider old-gen to have excessive fragmentation if more than 12.5% of old-gen is free memory that resides\n+    \/\/ within partially consumed regions of memory.\n+  }\n+  \/\/ Rebuild free set based on adjusted generation sizes.\n+  _free_set->rebuild(young_cset_regions, old_cset_regions);\n+\n+  if (mode()->is_generational() && (ShenandoahGenerationalHumongousReserve > 0)) {\n+    size_t old_region_span = (first_old_region <= last_old_region)? (last_old_region + 1 - first_old_region): 0;\n+    size_t allowed_old_gen_span = num_regions() - (ShenandoahGenerationalHumongousReserve * num_regions() \/ 100);\n+\n+    \/\/ Tolerate lower density if total span is small.  Here's the implementation:\n+    \/\/   if old_gen spans more than 100% and density < 75%, trigger old-defrag\n+    \/\/   else if old_gen spans more than 87.5% and density < 62.5%, trigger old-defrag\n+    \/\/   else if old_gen spans more than 75% and density < 50%, trigger old-defrag\n+    \/\/   else if old_gen spans more than 62.5% and density < 37.5%, trigger old-defrag\n+    \/\/   else if old_gen spans more than 50% and density < 25%, trigger old-defrag\n+    \/\/\n+    \/\/ A previous implementation was more aggressive in triggering, resulting in degraded throughput when\n+    \/\/ humongous allocation was not required.\n+\n+    ShenandoahGeneration* old_gen = old_generation();\n+    size_t old_available = old_gen->available();\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    size_t old_unaffiliated_available = old_gen->free_unaffiliated_regions() * region_size_bytes;\n+    assert(old_available >= old_unaffiliated_available, \"sanity\");\n+    size_t old_fragmented_available = old_available - old_unaffiliated_available;\n+\n+    size_t old_bytes_consumed = old_region_count * region_size_bytes - old_fragmented_available;\n+    size_t old_bytes_spanned = old_region_span * region_size_bytes;\n+    double old_density = ((double) old_bytes_consumed) \/ old_bytes_spanned;\n+\n+    uint eighths = 8;\n+    for (uint i = 0; i < 5; i++) {\n+      size_t span_threshold = eighths * allowed_old_gen_span \/ 8;\n+      double density_threshold = (eighths - 2) \/ 8.0;\n+      if ((old_region_span >= span_threshold) && (old_density < density_threshold)) {\n+        old_heuristics()->trigger_old_is_fragmented(old_density, first_old_region, last_old_region);\n+        break;\n+      }\n+      eighths--;\n+    }\n+\n+    size_t old_used = old_generation()->used() + old_generation()->get_humongous_waste();\n+    size_t trigger_threshold = old_generation()->usage_trigger_threshold();\n+    \/\/ Detects unsigned arithmetic underflow\n+    assert(old_used <= capacity(),\n+           \"Old used (\" SIZE_FORMAT \", \" SIZE_FORMAT\") must not be more than heap capacity (\" SIZE_FORMAT \")\",\n+           old_generation()->used(), old_generation()->get_humongous_waste(), capacity());\n+\n+    if (old_used > trigger_threshold) {\n+      old_heuristics()->trigger_old_has_grown();\n+    }\n@@ -2285,3 +3311,12 @@\n-  _memory_pool = new ShenandoahMemoryPool(this);\n-  _cycle_memory_manager.add_pool(_memory_pool);\n-  _stw_memory_manager.add_pool(_memory_pool);\n+  if (mode()->is_generational()) {\n+    _young_gen_memory_pool = new ShenandoahYoungGenMemoryPool(this);\n+    _old_gen_memory_pool = new ShenandoahOldGenMemoryPool(this);\n+    _cycle_memory_manager.add_pool(_young_gen_memory_pool);\n+    _cycle_memory_manager.add_pool(_old_gen_memory_pool);\n+    _stw_memory_manager.add_pool(_young_gen_memory_pool);\n+    _stw_memory_manager.add_pool(_old_gen_memory_pool);\n+  } else {\n+    _memory_pool = new ShenandoahMemoryPool(this);\n+    _cycle_memory_manager.add_pool(_memory_pool);\n+    _stw_memory_manager.add_pool(_memory_pool);\n+  }\n@@ -2299,1 +3334,6 @@\n-  memory_pools.append(_memory_pool);\n+  if (mode()->is_generational()) {\n+    memory_pools.append(_young_gen_memory_pool);\n+    memory_pools.append(_old_gen_memory_pool);\n+  } else {\n+    memory_pools.append(_memory_pool);\n+  }\n@@ -2304,1 +3344,1 @@\n-  return _memory_pool->get_memory_usage();\n+  return MemoryUsage(_initial_size, used(), committed(), max_capacity());\n@@ -2342,0 +3382,1 @@\n+\n@@ -2369,0 +3410,103 @@\n+\n+void ShenandoahHeap::transfer_old_pointers_from_satb() {\n+  _old_generation->transfer_pointers_from_satb();\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<YOUNG>::heap_region_do(ShenandoahHeapRegion* region) {\n+  \/\/ Visit young regions\n+  if (region->is_young()) {\n+    _cl->heap_region_do(region);\n+  }\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<OLD>::heap_region_do(ShenandoahHeapRegion* region) {\n+  \/\/ Visit old regions\n+  if (region->is_old()) {\n+    _cl->heap_region_do(region);\n+  }\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<GLOBAL_GEN>::heap_region_do(ShenandoahHeapRegion* region) {\n+  _cl->heap_region_do(region);\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<GLOBAL_NON_GEN>::heap_region_do(ShenandoahHeapRegion* region) {\n+  _cl->heap_region_do(region);\n+}\n+\n+bool ShenandoahHeap::verify_generation_usage(bool verify_old, size_t old_regions, size_t old_bytes, size_t old_waste,\n+                                             bool verify_young, size_t young_regions, size_t young_bytes, size_t young_waste) {\n+  size_t tally_old_regions = 0;\n+  size_t tally_old_bytes = 0;\n+  size_t tally_old_waste = 0;\n+  size_t tally_young_regions = 0;\n+  size_t tally_young_bytes = 0;\n+  size_t tally_young_waste = 0;\n+\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  for (size_t i = 0; i < num_regions(); i++) {\n+    ShenandoahHeapRegion* r = get_region(i);\n+    if (r->is_old()) {\n+      tally_old_regions++;\n+      tally_old_bytes += r->used();\n+      if (r->is_humongous()) {\n+        ShenandoahHeapRegion* start = r->humongous_start_region();\n+        HeapWord* obj_addr = start->bottom();\n+        oop obj = cast_to_oop(obj_addr);\n+        size_t word_size = obj->size();\n+        HeapWord* end_addr = obj_addr + word_size;\n+        if (end_addr <= r->end()) {\n+          tally_old_waste += (r->end() - end_addr) * HeapWordSize;\n+        }\n+      }\n+    } else if (r->is_young()) {\n+      tally_young_regions++;\n+      tally_young_bytes += r->used();\n+      if (r->is_humongous()) {\n+        ShenandoahHeapRegion* start = r->humongous_start_region();\n+        HeapWord* obj_addr = start->bottom();\n+        oop obj = cast_to_oop(obj_addr);\n+        size_t word_size = obj->size();\n+        HeapWord* end_addr = obj_addr + word_size;\n+        if (end_addr <= r->end()) {\n+          tally_young_waste += (r->end() - end_addr) * HeapWordSize;\n+        }\n+      }\n+    }\n+  }\n+  if (verify_young &&\n+      ((young_regions != tally_young_regions) || (young_bytes != tally_young_bytes) || (young_waste != tally_young_waste))) {\n+    return false;\n+  } else if (verify_old &&\n+             ((old_regions != tally_old_regions) || (old_bytes != tally_old_bytes) || (old_waste != tally_old_waste))) {\n+    return false;\n+  } else {\n+    return true;\n+  }\n+}\n+\n+ShenandoahGeneration* ShenandoahHeap::generation_for(ShenandoahAffiliation affiliation) const {\n+  if (!mode()->is_generational()) {\n+    return global_generation();\n+  } else if (affiliation == YOUNG_GENERATION) {\n+    return young_generation();\n+  } else if (affiliation == OLD_GENERATION) {\n+    return old_generation();\n+  }\n+\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n+\n+void ShenandoahHeap::log_heap_status(const char* msg) const {\n+  if (mode()->is_generational()) {\n+    young_generation()->log_status(msg);\n+    old_generation()->log_status(msg);\n+  } else {\n+    global_generation()->log_status(msg);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":1432,"deletions":288,"binary":false,"changes":1720,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -35,0 +36,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -37,0 +39,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -41,1 +44,1 @@\n-ShenandoahGCSession::ShenandoahGCSession(GCCause::Cause cause) :\n+ShenandoahGCSession::ShenandoahGCSession(GCCause::Cause cause, ShenandoahGeneration* generation) :\n@@ -43,0 +46,1 @@\n+  _generation(generation),\n@@ -47,1 +51,2 @@\n-  _heap->set_gc_cause(cause);\n+  _heap->on_cycle_start(cause, _generation);\n+\n@@ -52,0 +57,1 @@\n+<<<<<<< HEAD\n@@ -53,0 +59,2 @@\n+=======\n+>>>>>>> 15096a26bfb2c13a9464a112e84f1a57443c7758\n@@ -67,1 +75,1 @@\n-  _heap->heuristics()->record_cycle_end();\n+  _heap->on_cycle_end(_generation);\n@@ -70,1 +78,1 @@\n-  _tracer->report_gc_reference_stats(_heap->ref_processor()->reference_process_stats());\n+  _tracer->report_gc_reference_stats(_generation->ref_processor()->reference_process_stats());\n@@ -73,1 +81,1 @@\n-  _heap->set_gc_cause(GCCause::_no_gc);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUtils.cpp","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"}]}