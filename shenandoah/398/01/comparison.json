{"files":[{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n@@ -35,0 +37,3 @@\n+class ShenandoahGeneration;\n+class ShenandoahHeap;\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalFullGC.hpp\"\n@@ -50,1 +51,0 @@\n-#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -59,1 +59,0 @@\n-#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -64,1 +63,0 @@\n-#include \"runtime\/javaThread.hpp\"\n@@ -71,63 +69,0 @@\n-\/\/ After Full GC is done, reconstruct the remembered set by iterating over OLD regions,\n-\/\/ registering all objects between bottom() and top(), and setting remembered set cards to\n-\/\/ DIRTY if they hold interesting pointers.\n-class ShenandoahReconstructRememberedSetTask : public WorkerTask {\n-private:\n-  ShenandoahRegionIterator _regions;\n-\n-public:\n-  ShenandoahReconstructRememberedSetTask() :\n-    WorkerTask(\"Shenandoah Reset Bitmap\") { }\n-\n-  void work(uint worker_id) {\n-    ShenandoahParallelWorkerSession worker_session(worker_id);\n-    ShenandoahHeapRegion* r = _regions.next();\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    RememberedScanner* scanner = heap->card_scan();\n-    ShenandoahSetRememberedCardsToDirtyClosure dirty_cards_for_interesting_pointers;\n-\n-    while (r != nullptr) {\n-      if (r->is_old() && r->is_active()) {\n-        HeapWord* obj_addr = r->bottom();\n-        if (r->is_humongous_start()) {\n-          \/\/ First, clear the remembered set\n-          oop obj = cast_to_oop(obj_addr);\n-          size_t size = obj->size();\n-\n-          \/\/ First, clear the remembered set for all spanned humongous regions\n-          size_t num_regions = ShenandoahHeapRegion::required_regions(size * HeapWordSize);\n-          size_t region_span = num_regions * ShenandoahHeapRegion::region_size_words();\n-          scanner->reset_remset(r->bottom(), region_span);\n-          size_t region_index = r->index();\n-          ShenandoahHeapRegion* humongous_region = heap->get_region(region_index);\n-          while (num_regions-- != 0) {\n-            scanner->reset_object_range(humongous_region->bottom(), humongous_region->end());\n-            region_index++;\n-            humongous_region = heap->get_region(region_index);\n-          }\n-\n-          \/\/ Then register the humongous object and DIRTY relevant remembered set cards\n-          scanner->register_object_without_lock(obj_addr);\n-          obj->oop_iterate(&dirty_cards_for_interesting_pointers);\n-        } else if (!r->is_humongous()) {\n-          \/\/ First, clear the remembered set\n-          scanner->reset_remset(r->bottom(), ShenandoahHeapRegion::region_size_words());\n-          scanner->reset_object_range(r->bottom(), r->end());\n-\n-          \/\/ Then iterate over all objects, registering object and DIRTYing relevant remembered set cards\n-          HeapWord* t = r->top();\n-          while (obj_addr < t) {\n-            oop obj = cast_to_oop(obj_addr);\n-            size_t size = obj->size();\n-            scanner->register_object_without_lock(obj_addr);\n-            obj_addr += obj->oop_iterate_size(&dirty_cards_for_interesting_pointers);\n-          }\n-        } \/\/ else, ignore humongous continuation region\n-      }\n-      \/\/ else, this region is FREE or YOUNG or inactive and we can ignore it.\n-      \/\/ TODO: Assert this.\n-      r = _regions.next();\n-    }\n-  }\n-};\n-\n@@ -180,27 +115,1 @@\n-  if (heap->mode()->is_generational()) {\n-    \/\/ Full GC should reset time since last gc for young and old heuristics\n-    heap->young_generation()->heuristics()->record_cycle_end();\n-    heap->old_generation()->heuristics()->record_cycle_end();\n-\n-    heap->mmu_tracker()->record_full(GCId::current());\n-    heap->log_heap_status(\"At end of Full GC\");\n-\n-    assert(heap->old_generation()->state() == ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP,\n-           \"After full GC, old generation should be waiting for bootstrap.\");\n-\n-    \/\/ Since we allow temporary violation of these constraints during Full GC, we want to enforce that the assertions are\n-    \/\/ made valid by the time Full GC completes.\n-    assert(heap->old_generation()->used_regions_size() <= heap->old_generation()->max_capacity(),\n-           \"Old generation affiliated regions must be less than capacity\");\n-    assert(heap->young_generation()->used_regions_size() <= heap->young_generation()->max_capacity(),\n-           \"Young generation affiliated regions must be less than capacity\");\n-\n-    assert((heap->young_generation()->used() + heap->young_generation()->get_humongous_waste())\n-           <= heap->young_generation()->used_regions_size(), \"Young consumed can be no larger than span of affiliated regions\");\n-    assert((heap->old_generation()->used() + heap->old_generation()->get_humongous_waste())\n-           <= heap->old_generation()->used_regions_size(), \"Old consumed can be no larger than span of affiliated regions\");\n-\n-    \/\/ Establish baseline for next old-has-grown trigger.\n-    heap->old_generation()->set_live_bytes_after_last_mark(heap->old_generation()->used() +\n-                                                           heap->old_generation()->get_humongous_waste());\n-  }\n+\n@@ -222,2 +131,0 @@\n-  \/\/ Since we may arrive here from degenerated GC failure of either young or old, establish generation as GLOBAL.\n-  heap->set_gc_generation(heap->global_generation());\n@@ -226,7 +133,1 @@\n-    \/\/ No need for old_gen->increase_used() as this was done when plabs were allocated.\n-    heap->set_young_evac_reserve(0);\n-    heap->set_old_evac_reserve(0);\n-    heap->set_promoted_reserve(0);\n-\n-    \/\/ Full GC supersedes any marking or coalescing in old generation.\n-    heap->cancel_old_gc();\n+    ShenandoahGenerationalFullGC::prepare(heap);\n@@ -279,0 +180,1 @@\n+      \/\/ TODO: Send cancel_concurrent_mark upstream? Does it really not have it already?\n@@ -301,6 +203,1 @@\n-      for (size_t i = 0; i < heap->num_regions(); i++) {\n-        ShenandoahHeapRegion* r = heap->get_region(i);\n-        if (r->get_top_before_promote() != nullptr) {\n-          r->restore_top_before_promote();\n-        }\n-      }\n+      ShenandoahGenerationalFullGC::restore_top_before_promote(heap);\n@@ -357,13 +254,0 @@\n-  {\n-    \/\/ Epilogue\n-    \/\/ TODO: Merge with phase5_epilog?\n-    _preserved_marks->restore(heap->workers());\n-    _preserved_marks->reclaim();\n-\n-    if (heap->mode()->is_generational()) {\n-      ShenandoahGCPhase phase(ShenandoahPhaseTimings::full_gc_reconstruct_remembered_set);\n-      ShenandoahReconstructRememberedSetTask task;\n-      heap->workers()->run_task(&task);\n-    }\n-  }\n-\n@@ -405,0 +289,1 @@\n+    \/\/ TODO: Add API to heap to skip free regions\n@@ -433,6 +318,2 @@\n-  size_t live_bytes_in_old = 0;\n-  for (size_t i = 0; i < heap->num_regions(); i++) {\n-    ShenandoahHeapRegion* r = heap->get_region(i);\n-    if (r->is_old()) {\n-      live_bytes_in_old += r->get_live_data_bytes();\n-    }\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahGenerationalFullGC::log_live_in_old(heap);\n@@ -440,1 +321,0 @@\n-  log_info(gc)(\"Live bytes in old after STW mark: \" PROPERFMT, PROPERFMTARGS(live_bytes_in_old));\n@@ -443,235 +323,0 @@\n-class ShenandoahPrepareForCompactionTask : public WorkerTask {\n-private:\n-  PreservedMarksSet*        const _preserved_marks;\n-  ShenandoahHeap*           const _heap;\n-  ShenandoahHeapRegionSet** const _worker_slices;\n-  size_t                    const _num_workers;\n-\n-public:\n-  ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks,\n-                                     ShenandoahHeapRegionSet **worker_slices,\n-                                     size_t num_workers);\n-\n-  static bool is_candidate_region(ShenandoahHeapRegion* r) {\n-    \/\/ Empty region: get it into the slice to defragment the slice itself.\n-    \/\/ We could have skipped this without violating correctness, but we really\n-    \/\/ want to compact all live regions to the start of the heap, which sometimes\n-    \/\/ means moving them into the fully empty regions.\n-    if (r->is_empty()) return true;\n-\n-    \/\/ Can move the region, and this is not the humongous region. Humongous\n-    \/\/ moves are special cased here, because their moves are handled separately.\n-    return r->is_stw_move_allowed() && !r->is_humongous();\n-  }\n-\n-  void work(uint worker_id);\n-};\n-\n-class ShenandoahPrepareForGenerationalCompactionObjectClosure : public ObjectClosure {\n-private:\n-  PreservedMarks*          const _preserved_marks;\n-  ShenandoahHeap*          const _heap;\n-  uint                           _tenuring_threshold;\n-\n-  \/\/ _empty_regions is a thread-local list of heap regions that have been completely emptied by this worker thread's\n-  \/\/ compaction efforts.  The worker thread that drives these efforts adds compacted regions to this list if the\n-  \/\/ region has not been compacted onto itself.\n-  GrowableArray<ShenandoahHeapRegion*>& _empty_regions;\n-  int _empty_regions_pos;\n-  ShenandoahHeapRegion*          _old_to_region;\n-  ShenandoahHeapRegion*          _young_to_region;\n-  ShenandoahHeapRegion*          _from_region;\n-  ShenandoahAffiliation          _from_affiliation;\n-  HeapWord*                      _old_compact_point;\n-  HeapWord*                      _young_compact_point;\n-  uint                           _worker_id;\n-\n-public:\n-  ShenandoahPrepareForGenerationalCompactionObjectClosure(PreservedMarks* preserved_marks,\n-                                                          GrowableArray<ShenandoahHeapRegion*>& empty_regions,\n-                                                          ShenandoahHeapRegion* old_to_region,\n-                                                          ShenandoahHeapRegion* young_to_region, uint worker_id) :\n-      _preserved_marks(preserved_marks),\n-      _heap(ShenandoahHeap::heap()),\n-      _tenuring_threshold(0),\n-      _empty_regions(empty_regions),\n-      _empty_regions_pos(0),\n-      _old_to_region(old_to_region),\n-      _young_to_region(young_to_region),\n-      _from_region(nullptr),\n-      _old_compact_point((old_to_region != nullptr)? old_to_region->bottom(): nullptr),\n-      _young_compact_point((young_to_region != nullptr)? young_to_region->bottom(): nullptr),\n-      _worker_id(worker_id) {\n-    if (_heap->mode()->is_generational()) {\n-      _tenuring_threshold = _heap->age_census()->tenuring_threshold();\n-    }\n-  }\n-\n-  void set_from_region(ShenandoahHeapRegion* from_region) {\n-    _from_region = from_region;\n-    _from_affiliation = from_region->affiliation();\n-    if (_from_region->has_live()) {\n-      if (_from_affiliation == ShenandoahAffiliation::OLD_GENERATION) {\n-        if (_old_to_region == nullptr) {\n-          _old_to_region = from_region;\n-          _old_compact_point = from_region->bottom();\n-        }\n-      } else {\n-        assert(_from_affiliation == ShenandoahAffiliation::YOUNG_GENERATION, \"from_region must be OLD or YOUNG\");\n-        if (_young_to_region == nullptr) {\n-          _young_to_region = from_region;\n-          _young_compact_point = from_region->bottom();\n-        }\n-      }\n-    } \/\/ else, we won't iterate over this _from_region so we don't need to set up to region to hold copies\n-  }\n-\n-  void finish() {\n-    finish_old_region();\n-    finish_young_region();\n-  }\n-\n-  void finish_old_region() {\n-    if (_old_to_region != nullptr) {\n-      log_debug(gc)(\"Planned compaction into Old Region \" SIZE_FORMAT \", used: \" SIZE_FORMAT \" tabulated by worker %u\",\n-                    _old_to_region->index(), _old_compact_point - _old_to_region->bottom(), _worker_id);\n-      _old_to_region->set_new_top(_old_compact_point);\n-      _old_to_region = nullptr;\n-    }\n-  }\n-\n-  void finish_young_region() {\n-    if (_young_to_region != nullptr) {\n-      log_debug(gc)(\"Worker %u planned compaction into Young Region \" SIZE_FORMAT \", used: \" SIZE_FORMAT,\n-                    _worker_id, _young_to_region->index(), _young_compact_point - _young_to_region->bottom());\n-      _young_to_region->set_new_top(_young_compact_point);\n-      _young_to_region = nullptr;\n-    }\n-  }\n-\n-  bool is_compact_same_region() {\n-    return (_from_region == _old_to_region) || (_from_region == _young_to_region);\n-  }\n-\n-  int empty_regions_pos() {\n-    return _empty_regions_pos;\n-  }\n-\n-  void do_object(oop p) {\n-    assert(_from_region != nullptr, \"must set before work\");\n-    assert((_from_region->bottom() <= cast_from_oop<HeapWord*>(p)) && (cast_from_oop<HeapWord*>(p) < _from_region->top()),\n-           \"Object must reside in _from_region\");\n-    assert(_heap->complete_marking_context()->is_marked(p), \"must be marked\");\n-    assert(!_heap->complete_marking_context()->allocated_after_mark_start(p), \"must be truly marked\");\n-\n-    size_t obj_size = p->size();\n-    uint from_region_age = _from_region->age();\n-    uint object_age = p->age();\n-\n-    bool promote_object = false;\n-    if ((_from_affiliation == ShenandoahAffiliation::YOUNG_GENERATION) &&\n-        (from_region_age + object_age >= _tenuring_threshold)) {\n-      if ((_old_to_region != nullptr) && (_old_compact_point + obj_size > _old_to_region->end())) {\n-        finish_old_region();\n-        _old_to_region = nullptr;\n-      }\n-      if (_old_to_region == nullptr) {\n-        if (_empty_regions_pos < _empty_regions.length()) {\n-          ShenandoahHeapRegion* new_to_region = _empty_regions.at(_empty_regions_pos);\n-          _empty_regions_pos++;\n-          new_to_region->set_affiliation(OLD_GENERATION);\n-          _old_to_region = new_to_region;\n-          _old_compact_point = _old_to_region->bottom();\n-          promote_object = true;\n-        }\n-        \/\/ Else this worker thread does not yet have any empty regions into which this aged object can be promoted so\n-        \/\/ we leave promote_object as false, deferring the promotion.\n-      } else {\n-        promote_object = true;\n-      }\n-    }\n-\n-    if (promote_object || (_from_affiliation == ShenandoahAffiliation::OLD_GENERATION)) {\n-      assert(_old_to_region != nullptr, \"_old_to_region should not be nullptr when evacuating to OLD region\");\n-      if (_old_compact_point + obj_size > _old_to_region->end()) {\n-        ShenandoahHeapRegion* new_to_region;\n-\n-        log_debug(gc)(\"Worker %u finishing old region \" SIZE_FORMAT \", compact_point: \" PTR_FORMAT \", obj_size: \" SIZE_FORMAT\n-                      \", &compact_point[obj_size]: \" PTR_FORMAT \", region end: \" PTR_FORMAT,  _worker_id, _old_to_region->index(),\n-                      p2i(_old_compact_point), obj_size, p2i(_old_compact_point + obj_size), p2i(_old_to_region->end()));\n-\n-        \/\/ Object does not fit.  Get a new _old_to_region.\n-        finish_old_region();\n-        if (_empty_regions_pos < _empty_regions.length()) {\n-          new_to_region = _empty_regions.at(_empty_regions_pos);\n-          _empty_regions_pos++;\n-          new_to_region->set_affiliation(OLD_GENERATION);\n-        } else {\n-          \/\/ If we've exhausted the previously selected _old_to_region, we know that the _old_to_region is distinct\n-          \/\/ from _from_region.  That's because there is always room for _from_region to be compacted into itself.\n-          \/\/ Since we're out of empty regions, let's use _from_region to hold the results of its own compaction.\n-          new_to_region = _from_region;\n-        }\n-\n-        assert(new_to_region != _old_to_region, \"must not reuse same OLD to-region\");\n-        assert(new_to_region != nullptr, \"must not be nullptr\");\n-        _old_to_region = new_to_region;\n-        _old_compact_point = _old_to_region->bottom();\n-      }\n-\n-      \/\/ Object fits into current region, record new location:\n-      assert(_old_compact_point + obj_size <= _old_to_region->end(), \"must fit\");\n-      shenandoah_assert_not_forwarded(nullptr, p);\n-      _preserved_marks->push_if_necessary(p, p->mark());\n-      p->forward_to(cast_to_oop(_old_compact_point));\n-      _old_compact_point += obj_size;\n-    } else {\n-      assert(_from_affiliation == ShenandoahAffiliation::YOUNG_GENERATION,\n-             \"_from_region must be OLD_GENERATION or YOUNG_GENERATION\");\n-      assert(_young_to_region != nullptr, \"_young_to_region should not be nullptr when compacting YOUNG _from_region\");\n-\n-      \/\/ After full gc compaction, all regions have age 0.  Embed the region's age into the object's age in order to preserve\n-      \/\/ tenuring progress.\n-      if (_heap->is_aging_cycle()) {\n-        _heap->increase_object_age(p, from_region_age + 1);\n-      } else {\n-        _heap->increase_object_age(p, from_region_age);\n-      }\n-\n-      if (_young_compact_point + obj_size > _young_to_region->end()) {\n-        ShenandoahHeapRegion* new_to_region;\n-\n-        log_debug(gc)(\"Worker %u finishing young region \" SIZE_FORMAT \", compact_point: \" PTR_FORMAT \", obj_size: \" SIZE_FORMAT\n-                      \", &compact_point[obj_size]: \" PTR_FORMAT \", region end: \" PTR_FORMAT,  _worker_id, _young_to_region->index(),\n-                      p2i(_young_compact_point), obj_size, p2i(_young_compact_point + obj_size), p2i(_young_to_region->end()));\n-\n-        \/\/ Object does not fit.  Get a new _young_to_region.\n-        finish_young_region();\n-        if (_empty_regions_pos < _empty_regions.length()) {\n-          new_to_region = _empty_regions.at(_empty_regions_pos);\n-          _empty_regions_pos++;\n-          new_to_region->set_affiliation(YOUNG_GENERATION);\n-        } else {\n-          \/\/ If we've exhausted the previously selected _young_to_region, we know that the _young_to_region is distinct\n-          \/\/ from _from_region.  That's because there is always room for _from_region to be compacted into itself.\n-          \/\/ Since we're out of empty regions, let's use _from_region to hold the results of its own compaction.\n-          new_to_region = _from_region;\n-        }\n-\n-        assert(new_to_region != _young_to_region, \"must not reuse same OLD to-region\");\n-        assert(new_to_region != nullptr, \"must not be nullptr\");\n-        _young_to_region = new_to_region;\n-        _young_compact_point = _young_to_region->bottom();\n-      }\n-\n-      \/\/ Object fits into current region, record new location:\n-      assert(_young_compact_point + obj_size <= _young_to_region->end(), \"must fit\");\n-      shenandoah_assert_not_forwarded(nullptr, p);\n-      _preserved_marks->push_if_necessary(p, p->mark());\n-      p->forward_to(cast_to_oop(_young_compact_point));\n-      _young_compact_point += obj_size;\n-    }\n-  }\n-};\n-\n-\n@@ -704,1 +349,1 @@\n-  void finish_region() {\n+  void finish() {\n@@ -706,1 +351,0 @@\n-    assert(!_heap->mode()->is_generational(), \"Generational GC should use different Closure\");\n@@ -725,1 +369,1 @@\n-      finish_region();\n+      finish();\n@@ -752,0 +396,5 @@\n+class ShenandoahPrepareForCompactionTask : public WorkerTask {\n+private:\n+  PreservedMarksSet*        const _preserved_marks;\n+  ShenandoahHeap*           const _heap;\n+  ShenandoahHeapRegionSet** const _worker_slices;\n@@ -753,3 +402,2 @@\n-ShenandoahPrepareForCompactionTask::ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks,\n-                                                                       ShenandoahHeapRegionSet **worker_slices,\n-                                                                       size_t num_workers) :\n+public:\n+  ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks, ShenandoahHeapRegionSet **worker_slices) :\n@@ -757,2 +405,3 @@\n-    _preserved_marks(preserved_marks), _heap(ShenandoahHeap::heap()),\n-    _worker_slices(worker_slices), _num_workers(num_workers) { }\n+    _preserved_marks(preserved_marks),\n+    _heap(ShenandoahHeap::heap()), _worker_slices(worker_slices) {\n+  }\n@@ -760,0 +409,20 @@\n+  static bool is_candidate_region(ShenandoahHeapRegion* r) {\n+    \/\/ Empty region: get it into the slice to defragment the slice itself.\n+    \/\/ We could have skipped this without violating correctness, but we really\n+    \/\/ want to compact all live regions to the start of the heap, which sometimes\n+    \/\/ means moving them into the fully empty regions.\n+    if (r->is_empty()) return true;\n+\n+    \/\/ Can move the region, and this is not the humongous region. Humongous\n+    \/\/ moves are special cased here, because their moves are handled separately.\n+    return r->is_stw_move_allowed() && !r->is_humongous();\n+  }\n+\n+  void work(uint worker_id) override;\n+private:\n+  template<typename ClosureType>\n+  void prepare_for_compaction(ClosureType& cl,\n+                              GrowableArray<ShenandoahHeapRegion*>& empty_regions,\n+                              ShenandoahHeapRegionSetIterator& it,\n+                              ShenandoahHeapRegion* from_region);\n+};\n@@ -778,2 +447,0 @@\n-    ShenandoahHeapRegion* old_to_region = (from_region->is_old())? from_region: nullptr;\n-    ShenandoahHeapRegion* young_to_region = (from_region->is_young())? from_region: nullptr;\n@@ -781,25 +448,2 @@\n-                                                               empty_regions,\n-                                                               old_to_region, young_to_region,\n-                                                               worker_id);\n-    while (from_region != nullptr) {\n-      assert(is_candidate_region(from_region), \"Sanity\");\n-      log_debug(gc)(\"Worker %u compacting %s Region \" SIZE_FORMAT \" which had used \" SIZE_FORMAT \" and %s live\",\n-                    worker_id, from_region->affiliation_name(),\n-                    from_region->index(), from_region->used(), from_region->has_live()? \"has\": \"does not have\");\n-      cl.set_from_region(from_region);\n-      if (from_region->has_live()) {\n-        _heap->marked_object_iterate(from_region, &cl);\n-      }\n-      \/\/ Compacted the region to somewhere else? From-region is empty then.\n-      if (!cl.is_compact_same_region()) {\n-        empty_regions.append(from_region);\n-      }\n-      from_region = it.next();\n-    }\n-    cl.finish();\n-\n-    \/\/ Mark all remaining regions as empty\n-    for (int pos = cl.empty_regions_pos(); pos < empty_regions.length(); ++pos) {\n-      ShenandoahHeapRegion* r = empty_regions.at(pos);\n-      r->set_new_top(r->bottom());\n-    }\n+                                                               empty_regions, from_region, worker_id);\n+    prepare_for_compaction(cl, empty_regions, it, from_region);\n@@ -808,6 +452,3 @@\n-    while (from_region != nullptr) {\n-      assert(is_candidate_region(from_region), \"Sanity\");\n-      cl.set_from_region(from_region);\n-      if (from_region->has_live()) {\n-        _heap->marked_object_iterate(from_region, &cl);\n-      }\n+    prepare_for_compaction(cl, empty_regions, it, from_region);\n+  }\n+}\n@@ -815,5 +456,10 @@\n-      \/\/ Compacted the region to somewhere else? From-region is empty then.\n-      if (!cl.is_compact_same_region()) {\n-        empty_regions.append(from_region);\n-      }\n-      from_region = it.next();\n+template<typename ClosureType>\n+void ShenandoahPrepareForCompactionTask::prepare_for_compaction(ClosureType& cl,\n+                                                                GrowableArray<ShenandoahHeapRegion*>& empty_regions,\n+                                                                ShenandoahHeapRegionSetIterator& it,\n+                                                                ShenandoahHeapRegion* from_region) {\n+  while (from_region != nullptr) {\n+    assert(is_candidate_region(from_region), \"Sanity\");\n+    cl.set_from_region(from_region);\n+    if (from_region->has_live()) {\n+      _heap->marked_object_iterate(from_region, &cl);\n@@ -821,1 +467,0 @@\n-    cl.finish_region();\n@@ -823,4 +468,3 @@\n-    \/\/ Mark all remaining regions as empty\n-    for (int pos = cl.empty_regions_pos(); pos < empty_regions.length(); ++pos) {\n-      ShenandoahHeapRegion* r = empty_regions.at(pos);\n-      r->set_new_top(r->bottom());\n+    \/\/ Compacted the region to somewhere else? From-region is empty then.\n+    if (!cl.is_compact_same_region()) {\n+      empty_regions.append(from_region);\n@@ -828,0 +472,8 @@\n+    from_region = it.next();\n+  }\n+  cl.finish();\n+\n+  \/\/ Mark all remaining regions as empty\n+  for (int pos = cl.empty_regions_pos(); pos < empty_regions.length(); ++pos) {\n+    ShenandoahHeapRegion* r = empty_regions.at(pos);\n+    r->set_new_top(r->bottom());\n@@ -926,3 +578,1 @@\n-               \"Humongous Start %s Region \" SIZE_FORMAT \" is not marked, should not have live\",\n-               r->affiliation_name(),  r->index());\n-        log_debug(gc)(\"Trashing immediate humongous region \" SIZE_FORMAT \" because not marked\", r->index());\n+               \"Region \" SIZE_FORMAT \" is not marked, should not have live\", r->index());\n@@ -932,1 +582,1 @@\n-               \"Humongous Start %s Region \" SIZE_FORMAT \" should have live\", r->affiliation_name(),  r->index());\n+               \"Region \" SIZE_FORMAT \" should have live\", r->index());\n@@ -937,1 +587,1 @@\n-             \"Humongous Continuation %s Region \" SIZE_FORMAT \" should have live\", r->affiliation_name(),  r->index());\n+             \"Region \" SIZE_FORMAT \" should have live\", r->index());\n@@ -940,1 +590,0 @@\n-        log_debug(gc)(\"Trashing immediate regular region \" SIZE_FORMAT \" because has no live\", r->index());\n@@ -1121,2 +770,1 @@\n-    size_t num_workers = heap->max_workers();\n-\n+    \/\/ TODO: This is ResourceMark is missing upstream.\n@@ -1124,1 +772,1 @@\n-    ShenandoahPrepareForCompactionTask task(_preserved_marks, worker_slices, num_workers);\n+    ShenandoahPrepareForCompactionTask task(_preserved_marks, worker_slices);\n@@ -1198,6 +846,2 @@\n-      if (r->is_pinned() && r->is_old() && r->is_active() && !r->is_humongous()) {\n-        \/\/ Pinned regions are not compacted so they may still hold unmarked objects with\n-        \/\/ reference to reclaimed memory. Remembered set scanning will crash if it attempts\n-        \/\/ to iterate the oops in these objects.\n-        r->begin_preemptible_coalesce_and_fill();\n-        r->oop_fill_and_coalesce_without_cancel();\n+      if (_heap->mode()->is_generational()) {\n+        ShenandoahGenerationalFullGC::maybe_coalesce_and_fill_region(r);\n@@ -1305,17 +949,0 @@\n-static void account_for_region(ShenandoahHeapRegion* r, size_t &region_count, size_t &region_usage, size_t &humongous_waste) {\n-  region_count++;\n-  region_usage += r->used();\n-  if (r->is_humongous_start()) {\n-    \/\/ For each humongous object, we take this path once regardless of how many regions it spans.\n-    HeapWord* obj_addr = r->bottom();\n-    oop obj = cast_to_oop(obj_addr);\n-    size_t word_size = obj->size();\n-    size_t region_size_words = ShenandoahHeapRegion::region_size_words();\n-    size_t overreach = word_size % region_size_words;\n-    if (overreach != 0) {\n-      humongous_waste += (region_size_words - overreach) * HeapWordSize;\n-    }\n-    \/\/ else, this humongous object aligns exactly on region size, so no waste.\n-  }\n-}\n-\n@@ -1379,1 +1006,1 @@\n-        account_for_region(r, _old_regions, _old_usage, _old_humongous_waste);\n+        ShenandoahGenerationalFullGC::account_for_region(r, _old_regions, _old_usage, _old_humongous_waste);\n@@ -1381,1 +1008,1 @@\n-        account_for_region(r, _young_regions, _young_usage, _young_humongous_waste);\n+        ShenandoahGenerationalFullGC::account_for_region(r, _young_regions, _young_usage, _young_humongous_waste);\n@@ -1433,7 +1060,3 @@\n-      ContinuationGCSupport::relativize_stack_chunk(cast_to_oop<HeapWord*>(heap->get_region(old_start)->bottom()));\n-      log_debug(gc)(\"Full GC compaction moves humongous object from region \" SIZE_FORMAT \" to region \" SIZE_FORMAT,\n-                    old_start, new_start);\n-\n-      Copy::aligned_conjoint_words(heap->get_region(old_start)->bottom(),\n-                                   heap->get_region(new_start)->bottom(),\n-                                   words_size);\n+      log_debug(gc)(\"Full GC compaction moves humongous object from region \" SIZE_FORMAT \" to region \" SIZE_FORMAT, old_start, new_start);\n+      Copy::aligned_conjoint_words(r->bottom(), heap->get_region(new_start)->bottom(), words_size);\n+      ContinuationGCSupport::relativize_stack_chunk(cast_to_oop<HeapWord*>(r->bottom()));\n@@ -1544,14 +1167,0 @@\n-    if (heap->mode()->is_generational()) {\n-      size_t old_usage = heap->old_generation()->used_regions_size();\n-      size_t old_capacity = heap->old_generation()->max_capacity();\n-\n-      assert(old_usage % ShenandoahHeapRegion::region_size_bytes() == 0, \"Old usage must aligh with region size\");\n-      assert(old_capacity % ShenandoahHeapRegion::region_size_bytes() == 0, \"Old capacity must aligh with region size\");\n-\n-      if (old_capacity > old_usage) {\n-        size_t excess_old_regions = (old_capacity - old_usage) \/ ShenandoahHeapRegion::region_size_bytes();\n-        heap->generation_sizer()->transfer_to_young(excess_old_regions);\n-      } else if (old_capacity < old_usage) {\n-        size_t old_regions_deficit = (old_usage - old_capacity) \/ ShenandoahHeapRegion::region_size_bytes();\n-        heap->generation_sizer()->force_transfer_to_old(old_regions_deficit);\n-      }\n@@ -1559,3 +1168,2 @@\n-      log_info(gc)(\"FullGC done: young usage: \" SIZE_FORMAT \"%s, old usage: \" SIZE_FORMAT \"%s\",\n-                   byte_size_in_proper_unit(heap->young_generation()->used()), proper_unit_for_byte_size(heap->young_generation()->used()),\n-                   byte_size_in_proper_unit(heap->old_generation()->used()),   proper_unit_for_byte_size(heap->old_generation()->used()));\n+    if (heap->mode()->is_generational()) {\n+      ShenandoahGenerationalFullGC::balance_generations_before_rebuilding_free_set(heap);\n@@ -1563,0 +1171,1 @@\n+\n@@ -1576,2 +1185,0 @@\n-    \/\/ In case this Full GC resulted from degeneration, clear the tally on anticipated promotion.\n-    heap->clear_promotion_potential();\n@@ -1580,0 +1187,2 @@\n+      \/\/ In case this Full GC resulted from degeneration, clear the tally on anticipated promotion.\n+      heap->clear_promotion_potential();\n@@ -1588,33 +1197,3 @@\n-      bool success;\n-      size_t region_xfer;\n-      const char* region_destination;\n-      ShenandoahYoungGeneration* young_gen = heap->young_generation();\n-      ShenandoahGeneration* old_gen = heap->old_generation();\n-\n-      size_t old_region_surplus = heap->get_old_region_surplus();\n-      size_t old_region_deficit = heap->get_old_region_deficit();\n-      if (old_region_surplus) {\n-        success = heap->generation_sizer()->transfer_to_young(old_region_surplus);\n-        region_destination = \"young\";\n-        region_xfer = old_region_surplus;\n-      } else if (old_region_deficit) {\n-        success = heap->generation_sizer()->transfer_to_old(old_region_deficit);\n-        region_destination = \"old\";\n-        region_xfer = old_region_deficit;\n-        if (!success) {\n-          ((ShenandoahOldHeuristics *) old_gen->heuristics())->trigger_cannot_expand();\n-        }\n-      } else {\n-        region_destination = \"none\";\n-        region_xfer = 0;\n-        success = true;\n-      }\n-      heap->set_old_region_surplus(0);\n-      heap->set_old_region_deficit(0);\n-      size_t young_available = young_gen->available();\n-      size_t old_available = old_gen->available();\n-      log_info(gc, ergo)(\"After cleanup, %s \" SIZE_FORMAT \" regions to %s to prepare for next gc, old available: \"\n-                         SIZE_FORMAT \"%s, young_available: \" SIZE_FORMAT \"%s\",\n-                         success? \"successfully transferred\": \"failed to transfer\", region_xfer, region_destination,\n-                         byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n-                         byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n+      ShenandoahGenerationalFullGC::balance_generations_after_rebuilding_free_set(heap);\n+      ShenandoahGenerationalFullGC::rebuild_remembered_set(heap);\n+      ShenandoahGenerationalFullGC::handle_completion(heap);\n@@ -1624,0 +1203,3 @@\n+\n+  _preserved_marks->restore(heap->workers());\n+  _preserved_marks->reclaim();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":89,"deletions":507,"binary":false,"changes":596,"status":"modified"},{"patch":"@@ -126,0 +126,3 @@\n+  size_t used_including_humongous_waste() const {\n+    return used() + get_humongous_waste();\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,388 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shared\/preservedMarks.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalFullGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+\n+#ifdef ASSERT\n+void assert_regions_used_not_more_than_capacity(ShenandoahGeneration* generation) {\n+  assert(generation->used_regions_size() <= generation->max_capacity(),\n+         \"%s generation affiliated regions must be less than capacity\", generation->name());\n+}\n+\n+void assert_usage_not_more_than_regions_used(ShenandoahGeneration* generation) {\n+  assert(generation->used_including_humongous_waste() <= generation->used_regions_size(),\n+         \"%s consumed can be no larger than span of affiliated regions\", generation->name());\n+}\n+#else\n+void assert_regions_used_not_more_than_capacity(ShenandoahGeneration* generation) {}\n+void assert_usage_not_more_than_regions_used(ShenandoahGeneration* generation) {}\n+#endif\n+\n+\n+void ShenandoahGenerationalFullGC::prepare(ShenandoahHeap* heap) {\n+  \/\/ Since we may arrive here from degenerated GC failure of either young or old, establish generation as GLOBAL.\n+  heap->set_gc_generation(heap->global_generation());\n+\n+  \/\/ No need for old_gen->increase_used() as this was done when plabs were allocated.\n+  heap->reset_generation_reserves();\n+\n+  \/\/ Full GC supersedes any marking or coalescing in old generation.\n+  heap->cancel_old_gc();\n+}\n+\n+void ShenandoahGenerationalFullGC::handle_completion(ShenandoahHeap* heap) {\n+  \/\/ Full GC should reset time since last gc for young and old heuristics\n+  ShenandoahYoungGeneration* young = heap->young_generation();\n+  ShenandoahOldGeneration* old = heap->old_generation();\n+  young->heuristics()->record_cycle_end();\n+  old->heuristics()->record_cycle_end();\n+\n+  heap->mmu_tracker()->record_full(GCId::current());\n+  heap->log_heap_status(\"At end of Full GC\");\n+\n+  assert(old->state() == ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP,\n+         \"After full GC, old generation should be waiting for bootstrap.\");\n+\n+  \/\/ Since we allow temporary violation of these constraints during Full GC, we want to enforce that the assertions are\n+  \/\/ made valid by the time Full GC completes.\n+  assert_regions_used_not_more_than_capacity(old);\n+  assert_regions_used_not_more_than_capacity(young);\n+  assert_usage_not_more_than_regions_used(old);\n+  assert_usage_not_more_than_regions_used(young);\n+\n+  \/\/ Establish baseline for next old-has-grown trigger.\n+  old->set_live_bytes_after_last_mark(old->used_including_humongous_waste());\n+}\n+\n+void ShenandoahGenerationalFullGC::rebuild_remembered_set(ShenandoahHeap* heap) {\n+  ShenandoahGCPhase phase(ShenandoahPhaseTimings::full_gc_reconstruct_remembered_set);\n+  ShenandoahRegionIterator regions;\n+  ShenandoahReconstructRememberedSetTask task(&regions);\n+  heap->workers()->run_task(&task);\n+}\n+\n+void ShenandoahGenerationalFullGC::balance_generations_before_rebuilding_free_set(ShenandoahHeap* heap) {\n+  size_t old_usage = heap->old_generation()->used_regions_size();\n+  size_t old_capacity = heap->old_generation()->max_capacity();\n+\n+  assert(old_usage % ShenandoahHeapRegion::region_size_bytes() == 0, \"Old usage must align with region size\");\n+  assert(old_capacity % ShenandoahHeapRegion::region_size_bytes() == 0, \"Old capacity must align with region size\");\n+\n+  if (old_capacity > old_usage) {\n+    size_t excess_old_regions = (old_capacity - old_usage) \/ ShenandoahHeapRegion::region_size_bytes();\n+    heap->generation_sizer()->transfer_to_young(excess_old_regions);\n+  } else if (old_capacity < old_usage) {\n+    size_t old_regions_deficit = (old_usage - old_capacity) \/ ShenandoahHeapRegion::region_size_bytes();\n+    heap->generation_sizer()->force_transfer_to_old(old_regions_deficit);\n+  }\n+\n+  log_info(gc)(\"FullGC done: young usage: \" PROPERFMT \", old usage: \" PROPERFMT,\n+               PROPERFMTARGS(heap->young_generation()->used()),\n+               PROPERFMTARGS(heap->old_generation()->used()));\n+}\n+\n+void ShenandoahGenerationalFullGC::balance_generations_after_rebuilding_free_set(ShenandoahHeap* heap) {\n+  bool success;\n+  size_t region_xfer;\n+  const char* region_destination;\n+  ShenandoahYoungGeneration* young_gen = heap->young_generation();\n+  ShenandoahGeneration* old_gen = heap->old_generation();\n+\n+  size_t old_region_surplus = heap->get_old_region_surplus();\n+  size_t old_region_deficit = heap->get_old_region_deficit();\n+  if (old_region_surplus) {\n+    success = heap->generation_sizer()->transfer_to_young(old_region_surplus);\n+    region_destination = \"young\";\n+    region_xfer = old_region_surplus;\n+  } else if (old_region_deficit) {\n+    success = heap->generation_sizer()->transfer_to_old(old_region_deficit);\n+    region_destination = \"old\";\n+    region_xfer = old_region_deficit;\n+    if (!success) {\n+      ((ShenandoahOldHeuristics *) old_gen->heuristics())->trigger_cannot_expand();\n+    }\n+  } else {\n+    region_destination = \"none\";\n+    region_xfer = 0;\n+    success = true;\n+  }\n+  heap->set_old_region_surplus(0);\n+  heap->set_old_region_deficit(0);\n+  size_t young_available = young_gen->available();\n+  size_t old_available = old_gen->available();\n+  log_info(gc, ergo)(\"After cleanup, %s \" SIZE_FORMAT \" regions to %s to prepare for next gc, old available: \"\n+                     PROPERFMT \", young_available: \" PROPERFMT,\n+                     success? \"successfully transferred\": \"failed to transfer\", region_xfer, region_destination,\n+                     PROPERFMTARGS(old_available), PROPERFMTARGS(young_available));\n+}\n+\n+void ShenandoahGenerationalFullGC::log_live_in_old(ShenandoahHeap* heap) {\n+  LogTarget(Info, gc) lt;\n+  if (lt.is_enabled()) {\n+    size_t live_bytes_in_old = 0;\n+    for (size_t i = 0; i < heap->num_regions(); i++) {\n+      ShenandoahHeapRegion* r = heap->get_region(i);\n+      if (r->is_old()) {\n+        live_bytes_in_old += r->get_live_data_bytes();\n+      }\n+    }\n+    log_info(gc)(\"Live bytes in old after STW mark: \" PROPERFMT, PROPERFMTARGS(live_bytes_in_old));\n+  }\n+}\n+\n+void ShenandoahGenerationalFullGC::restore_top_before_promote(ShenandoahHeap* heap) {\n+  for (size_t i = 0; i < heap->num_regions(); i++) {\n+    ShenandoahHeapRegion* r = heap->get_region(i);\n+    if (r->get_top_before_promote() != nullptr) {\n+      r->restore_top_before_promote();\n+    }\n+  }\n+}\n+\n+void ShenandoahGenerationalFullGC::account_for_region(ShenandoahHeapRegion* r, size_t &region_count, size_t &region_usage, size_t &humongous_waste) {\n+  region_count++;\n+  region_usage += r->used();\n+  if (r->is_humongous_start()) {\n+    \/\/ For each humongous object, we take this path once regardless of how many regions it spans.\n+    HeapWord* obj_addr = r->bottom();\n+    oop obj = cast_to_oop(obj_addr);\n+    size_t word_size = obj->size();\n+    size_t region_size_words = ShenandoahHeapRegion::region_size_words();\n+    size_t overreach = word_size % region_size_words;\n+    if (overreach != 0) {\n+      humongous_waste += (region_size_words - overreach) * HeapWordSize;\n+    }\n+    \/\/ else, this humongous object aligns exactly on region size, so no waste.\n+  }\n+}\n+\n+void ShenandoahGenerationalFullGC::maybe_coalesce_and_fill_region(ShenandoahHeapRegion* r) {\n+  if (r->is_pinned() && r->is_old() && r->is_active() && !r->is_humongous()) {\n+    r->begin_preemptible_coalesce_and_fill();\n+    r->oop_fill_and_coalesce_without_cancel();\n+  }\n+}\n+\n+ShenandoahPrepareForGenerationalCompactionObjectClosure::ShenandoahPrepareForGenerationalCompactionObjectClosure(PreservedMarks* preserved_marks,\n+                                                          GrowableArray<ShenandoahHeapRegion*>& empty_regions,\n+                                                          ShenandoahHeapRegion* from_region, uint worker_id) :\n+        _preserved_marks(preserved_marks),\n+        _heap(ShenandoahHeap::heap()),\n+        _tenuring_threshold(0),\n+        _empty_regions(empty_regions),\n+        _empty_regions_pos(0),\n+        _old_to_region(nullptr),\n+        _young_to_region(nullptr),\n+        _from_region(nullptr),\n+        _from_affiliation(ShenandoahAffiliation::FREE),\n+        _old_compact_point(nullptr),\n+        _young_compact_point(nullptr),\n+        _worker_id(worker_id) {\n+  assert(from_region != nullptr, \"Worker needs from_region\");\n+  \/\/ assert from_region has live?\n+  if (from_region->is_old()) {\n+    _old_to_region = from_region;\n+    _old_compact_point = from_region->bottom();\n+  } else if (from_region->is_young()) {\n+    _young_to_region = from_region;\n+    _young_compact_point = from_region->bottom();\n+  }\n+\n+  _tenuring_threshold = _heap->age_census()->tenuring_threshold();\n+}\n+\n+void ShenandoahPrepareForGenerationalCompactionObjectClosure::set_from_region(ShenandoahHeapRegion* from_region) {\n+  log_debug(gc)(\"Worker %u compacting %s Region \" SIZE_FORMAT \" which had used \" SIZE_FORMAT \" and %s live\",\n+                _worker_id, from_region->affiliation_name(),\n+                from_region->index(), from_region->used(), from_region->has_live()? \"has\": \"does not have\");\n+\n+  _from_region = from_region;\n+  _from_affiliation = from_region->affiliation();\n+  if (_from_region->has_live()) {\n+    if (_from_affiliation == ShenandoahAffiliation::OLD_GENERATION) {\n+      if (_old_to_region == nullptr) {\n+        _old_to_region = from_region;\n+        _old_compact_point = from_region->bottom();\n+      }\n+    } else {\n+      assert(_from_affiliation == ShenandoahAffiliation::YOUNG_GENERATION, \"from_region must be OLD or YOUNG\");\n+      if (_young_to_region == nullptr) {\n+        _young_to_region = from_region;\n+        _young_compact_point = from_region->bottom();\n+      }\n+    }\n+  } \/\/ else, we won't iterate over this _from_region so we don't need to set up to region to hold copies\n+}\n+\n+void ShenandoahPrepareForGenerationalCompactionObjectClosure::finish() {\n+  finish_old_region();\n+  finish_young_region();\n+}\n+\n+void ShenandoahPrepareForGenerationalCompactionObjectClosure::finish_old_region() {\n+  if (_old_to_region != nullptr) {\n+    log_debug(gc)(\"Planned compaction into Old Region \" SIZE_FORMAT \", used: \" SIZE_FORMAT \" tabulated by worker %u\",\n+            _old_to_region->index(), _old_compact_point - _old_to_region->bottom(), _worker_id);\n+    _old_to_region->set_new_top(_old_compact_point);\n+    _old_to_region = nullptr;\n+  }\n+}\n+\n+void ShenandoahPrepareForGenerationalCompactionObjectClosure::finish_young_region() {\n+  if (_young_to_region != nullptr) {\n+    log_debug(gc)(\"Worker %u planned compaction into Young Region \" SIZE_FORMAT \", used: \" SIZE_FORMAT,\n+            _worker_id, _young_to_region->index(), _young_compact_point - _young_to_region->bottom());\n+    _young_to_region->set_new_top(_young_compact_point);\n+    _young_to_region = nullptr;\n+  }\n+}\n+\n+bool ShenandoahPrepareForGenerationalCompactionObjectClosure::is_compact_same_region() {\n+  return (_from_region == _old_to_region) || (_from_region == _young_to_region);\n+}\n+\n+void ShenandoahPrepareForGenerationalCompactionObjectClosure::do_object(oop p) {\n+  assert(_from_region != nullptr, \"must set before work\");\n+  assert((_from_region->bottom() <= cast_from_oop<HeapWord*>(p)) && (cast_from_oop<HeapWord*>(p) < _from_region->top()),\n+         \"Object must reside in _from_region\");\n+  assert(_heap->complete_marking_context()->is_marked(p), \"must be marked\");\n+  assert(!_heap->complete_marking_context()->allocated_after_mark_start(p), \"must be truly marked\");\n+\n+  size_t obj_size = p->size();\n+  uint from_region_age = _from_region->age();\n+  uint object_age = p->age();\n+\n+  bool promote_object = false;\n+  if ((_from_affiliation == ShenandoahAffiliation::YOUNG_GENERATION) &&\n+      (from_region_age + object_age >= _tenuring_threshold)) {\n+    if ((_old_to_region != nullptr) && (_old_compact_point + obj_size > _old_to_region->end())) {\n+      finish_old_region();\n+      _old_to_region = nullptr;\n+    }\n+    if (_old_to_region == nullptr) {\n+      if (_empty_regions_pos < _empty_regions.length()) {\n+        ShenandoahHeapRegion* new_to_region = _empty_regions.at(_empty_regions_pos);\n+        _empty_regions_pos++;\n+        new_to_region->set_affiliation(OLD_GENERATION);\n+        _old_to_region = new_to_region;\n+        _old_compact_point = _old_to_region->bottom();\n+        promote_object = true;\n+      }\n+      \/\/ Else this worker thread does not yet have any empty regions into which this aged object can be promoted so\n+      \/\/ we leave promote_object as false, deferring the promotion.\n+    } else {\n+      promote_object = true;\n+    }\n+  }\n+\n+  if (promote_object || (_from_affiliation == ShenandoahAffiliation::OLD_GENERATION)) {\n+    assert(_old_to_region != nullptr, \"_old_to_region should not be nullptr when evacuating to OLD region\");\n+    if (_old_compact_point + obj_size > _old_to_region->end()) {\n+      ShenandoahHeapRegion* new_to_region;\n+\n+      log_debug(gc)(\"Worker %u finishing old region \" SIZE_FORMAT \", compact_point: \" PTR_FORMAT \", obj_size: \" SIZE_FORMAT\n+      \", &compact_point[obj_size]: \" PTR_FORMAT \", region end: \" PTR_FORMAT,  _worker_id, _old_to_region->index(),\n+              p2i(_old_compact_point), obj_size, p2i(_old_compact_point + obj_size), p2i(_old_to_region->end()));\n+\n+      \/\/ Object does not fit.  Get a new _old_to_region.\n+      finish_old_region();\n+      if (_empty_regions_pos < _empty_regions.length()) {\n+        new_to_region = _empty_regions.at(_empty_regions_pos);\n+        _empty_regions_pos++;\n+        new_to_region->set_affiliation(OLD_GENERATION);\n+      } else {\n+        \/\/ If we've exhausted the previously selected _old_to_region, we know that the _old_to_region is distinct\n+        \/\/ from _from_region.  That's because there is always room for _from_region to be compacted into itself.\n+        \/\/ Since we're out of empty regions, let's use _from_region to hold the results of its own compaction.\n+        new_to_region = _from_region;\n+      }\n+\n+      assert(new_to_region != _old_to_region, \"must not reuse same OLD to-region\");\n+      assert(new_to_region != nullptr, \"must not be nullptr\");\n+      _old_to_region = new_to_region;\n+      _old_compact_point = _old_to_region->bottom();\n+    }\n+\n+    \/\/ Object fits into current region, record new location:\n+    assert(_old_compact_point + obj_size <= _old_to_region->end(), \"must fit\");\n+    shenandoah_assert_not_forwarded(nullptr, p);\n+    _preserved_marks->push_if_necessary(p, p->mark());\n+    p->forward_to(cast_to_oop(_old_compact_point));\n+    _old_compact_point += obj_size;\n+  } else {\n+    assert(_from_affiliation == ShenandoahAffiliation::YOUNG_GENERATION,\n+           \"_from_region must be OLD_GENERATION or YOUNG_GENERATION\");\n+    assert(_young_to_region != nullptr, \"_young_to_region should not be nullptr when compacting YOUNG _from_region\");\n+\n+    \/\/ After full gc compaction, all regions have age 0.  Embed the region's age into the object's age in order to preserve\n+    \/\/ tenuring progress.\n+    if (_heap->is_aging_cycle()) {\n+      ShenandoahHeap::increase_object_age(p, from_region_age + 1);\n+    } else {\n+      ShenandoahHeap::increase_object_age(p, from_region_age);\n+    }\n+\n+    if (_young_compact_point + obj_size > _young_to_region->end()) {\n+      ShenandoahHeapRegion* new_to_region;\n+\n+      log_debug(gc)(\"Worker %u finishing young region \" SIZE_FORMAT \", compact_point: \" PTR_FORMAT \", obj_size: \" SIZE_FORMAT\n+      \", &compact_point[obj_size]: \" PTR_FORMAT \", region end: \" PTR_FORMAT,  _worker_id, _young_to_region->index(),\n+              p2i(_young_compact_point), obj_size, p2i(_young_compact_point + obj_size), p2i(_young_to_region->end()));\n+\n+      \/\/ Object does not fit.  Get a new _young_to_region.\n+      finish_young_region();\n+      if (_empty_regions_pos < _empty_regions.length()) {\n+        new_to_region = _empty_regions.at(_empty_regions_pos);\n+        _empty_regions_pos++;\n+        new_to_region->set_affiliation(YOUNG_GENERATION);\n+      } else {\n+        \/\/ If we've exhausted the previously selected _young_to_region, we know that the _young_to_region is distinct\n+        \/\/ from _from_region.  That's because there is always room for _from_region to be compacted into itself.\n+        \/\/ Since we're out of empty regions, let's use _from_region to hold the results of its own compaction.\n+        new_to_region = _from_region;\n+      }\n+\n+      assert(new_to_region != _young_to_region, \"must not reuse same OLD to-region\");\n+      assert(new_to_region != nullptr, \"must not be nullptr\");\n+      _young_to_region = new_to_region;\n+      _young_compact_point = _young_to_region->bottom();\n+    }\n+\n+    \/\/ Object fits into current region, record new location:\n+    assert(_young_compact_point + obj_size <= _young_to_region->end(), \"must fit\");\n+    shenandoah_assert_not_forwarded(nullptr, p);\n+    _preserved_marks->push_if_necessary(p, p->mark());\n+    p->forward_to(cast_to_oop(_young_compact_point));\n+    _young_compact_point += obj_size;\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.cpp","additions":388,"deletions":0,"binary":false,"changes":388,"status":"added"},{"patch":"@@ -0,0 +1,106 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALFULLGC_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALFULLGC_HPP\n+\n+#include \"gc\/shared\/preservedMarks.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+\n+class ShenandoahHeap;\n+class ShenandoahHeapRegion;\n+\n+class ShenandoahGenerationalFullGC {\n+public:\n+  \/\/ Prepares the generational mode heap for a full collection.\n+  static void prepare(ShenandoahHeap* heap);\n+\n+  \/\/ Full GC may have compacted objects in the old generation, so we need to rebuild the card tables.\n+  static void rebuild_remembered_set(ShenandoahHeap* heap);\n+\n+  \/\/ Records end of cycle for young and old and establishes size of live bytes in old\n+  static void handle_completion(ShenandoahHeap* heap);\n+\n+  static void balance_generations_before_rebuilding_free_set(ShenandoahHeap* heap);\n+  static void balance_generations_after_rebuilding_free_set(ShenandoahHeap* heap);\n+\n+  \/\/ Logs the number of live bytes marked in the old generation. This is _not_ the same\n+  \/\/ value used as the baseline for the old generation _after_ the full gc is complete.\n+  \/\/ The value reported in the logs does not include objects and regions that may be\n+  \/\/ promoted during the full gc.\n+  static void log_live_in_old(ShenandoahHeap* heap);\n+\n+  \/\/ This is used to tally the number, usage and space wasted by humongous objects for each generation.\n+  static void account_for_region(ShenandoahHeapRegion* r, size_t &region_count, size_t &region_usage, size_t &humongous_waste);\n+\n+  \/\/ Regions which are scheduled for in-place promotion during evacuation temporarily\n+  \/\/ have their top set to their end to prevent new objects from being allocated in them\n+  \/\/ before they are promoted. If the full GC encounters such a region, it means the\n+  \/\/ in-place promotion did not happen, and we must restore the original value of top.\n+  static void restore_top_before_promote(ShenandoahHeap* heap);\n+\n+  \/\/ Pinned regions are not compacted, so they may still hold unmarked objects with\n+  \/\/ references to reclaimed memory. Remembered set scanning will crash if it attempts\n+  \/\/ to iterate the oops in these objects. This method fills in dead objects for pinned,\n+  \/\/ old regions.\n+  static void maybe_coalesce_and_fill_region(ShenandoahHeapRegion* r);\n+};\n+\n+class ShenandoahPrepareForGenerationalCompactionObjectClosure : public ObjectClosure {\n+private:\n+  PreservedMarks*          const _preserved_marks;\n+  ShenandoahHeap*          const _heap;\n+  uint                           _tenuring_threshold;\n+\n+  \/\/ _empty_regions is a thread-local list of heap regions that have been completely emptied by this worker thread's\n+  \/\/ compaction efforts.  The worker thread that drives these efforts adds compacted regions to this list if the\n+  \/\/ region has not been compacted onto itself.\n+  GrowableArray<ShenandoahHeapRegion*>& _empty_regions;\n+  int _empty_regions_pos;\n+  ShenandoahHeapRegion*          _old_to_region;\n+  ShenandoahHeapRegion*          _young_to_region;\n+  ShenandoahHeapRegion*          _from_region;\n+  ShenandoahAffiliation          _from_affiliation;\n+  HeapWord*                      _old_compact_point;\n+  HeapWord*                      _young_compact_point;\n+  uint                           _worker_id;\n+\n+public:\n+  ShenandoahPrepareForGenerationalCompactionObjectClosure(PreservedMarks* preserved_marks,\n+                                                          GrowableArray<ShenandoahHeapRegion*>& empty_regions,\n+                                                          ShenandoahHeapRegion* from_region, uint worker_id);\n+\n+  void set_from_region(ShenandoahHeapRegion* from_region);\n+  void finish();\n+  void finish_old_region();\n+  void finish_young_region();\n+  bool is_compact_same_region();\n+  int empty_regions_pos() const { return _empty_regions_pos; }\n+\n+  void do_object(oop p) override;\n+};\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALFULLGC_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.hpp","additions":106,"deletions":0,"binary":false,"changes":106,"status":"added"},{"patch":"@@ -485,0 +485,2 @@\n+  inline void reset_generation_reserves();\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -822,0 +822,6 @@\n+inline void ShenandoahHeap::reset_generation_reserves() {\n+  set_young_evac_reserve(0);\n+  set_old_evac_reserve(0);\n+  set_promoted_reserve(0);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -142,17 +142,0 @@\n-class ShenandoahSetRememberedCardsToDirtyClosure : public BasicOopIterateClosure {\n-protected:\n-  ShenandoahHeap*    const _heap;\n-  RememberedScanner* const _scanner;\n-\n-public:\n-  ShenandoahSetRememberedCardsToDirtyClosure() :\n-      _heap(ShenandoahHeap::heap()),\n-      _scanner(_heap->card_scan()) {}\n-\n-  template<class T>\n-  inline void work(T* p);\n-\n-  virtual void do_oop(narrowOop* p) { work(p); }\n-  virtual void do_oop(oop* p)       { work(p); }\n-};\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOopClosures.hpp","additions":0,"deletions":17,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -58,12 +58,0 @@\n-template<class T>\n-inline void ShenandoahSetRememberedCardsToDirtyClosure::work(T* p) {\n-  T o = RawAccess<>::oop_load(p);\n-  if (!CompressedOops::is_null(o)) {\n-    oop obj = CompressedOops::decode_not_null(o);\n-    if (_heap->is_in_young(obj)) {\n-      \/\/ Found interesting pointer.  Mark the containing card as dirty.\n-      _scanner->mark_card_as_dirty((HeapWord*) p);\n-    }\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOopClosures.inline.hpp","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahRootProcessor.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSTWMark.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -34,0 +34,30 @@\n+\/\/ A closure that takes an oop in the old generation and, if it's pointing\n+\/\/ into the young generation, dirties the corresponding remembered set entry.\n+\/\/ This is only used to rebuild the remembered set after a full GC.\n+class ShenandoahDirtyRememberedSetClosure : public BasicOopIterateClosure {\n+protected:\n+  ShenandoahHeap*    const _heap;\n+  RememberedScanner* const _scanner;\n+\n+public:\n+  ShenandoahDirtyRememberedSetClosure() :\n+          _heap(ShenandoahHeap::heap()),\n+          _scanner(_heap->card_scan()) {}\n+\n+  template<class T>\n+  inline void work(T* p) {\n+    assert(_heap->is_in_old(p), \"Expecting to get an old gen address\");\n+    T o = RawAccess<>::oop_load(p);\n+    if (!CompressedOops::is_null(o)) {\n+      oop obj = CompressedOops::decode_not_null(o);\n+      if (_heap->is_in_young(obj)) {\n+        \/\/ Dirty the card containing the cross-generational pointer.\n+        _scanner->mark_card_as_dirty((HeapWord*) p);\n+      }\n+    }\n+  }\n+\n+  virtual void do_oop(narrowOop* p) { work(p); }\n+  virtual void do_oop(oop* p)       { work(p); }\n+};\n+\n@@ -374,0 +404,54 @@\n+\n+ShenandoahReconstructRememberedSetTask::ShenandoahReconstructRememberedSetTask(ShenandoahRegionIterator* regions)\n+  : WorkerTask(\"Shenandoah Reset Bitmap\")\n+  , _regions(regions) { }\n+\n+void ShenandoahReconstructRememberedSetTask::work(uint worker_id) {\n+  ShenandoahParallelWorkerSession worker_session(worker_id);\n+  ShenandoahHeapRegion* r = _regions->next();\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  RememberedScanner* scanner = heap->card_scan();\n+  ShenandoahDirtyRememberedSetClosure dirty_cards_for_cross_generational_pointers;\n+\n+  while (r != nullptr) {\n+    if (r->is_old() && r->is_active()) {\n+      HeapWord* obj_addr = r->bottom();\n+      if (r->is_humongous_start()) {\n+        \/\/ First, clear the remembered set\n+        oop obj = cast_to_oop(obj_addr);\n+        size_t size = obj->size();\n+\n+        \/\/ First, clear the remembered set for all spanned humongous regions\n+        size_t num_regions = ShenandoahHeapRegion::required_regions(size * HeapWordSize);\n+        size_t region_span = num_regions * ShenandoahHeapRegion::region_size_words();\n+        scanner->reset_remset(r->bottom(), region_span);\n+        size_t region_index = r->index();\n+        ShenandoahHeapRegion* humongous_region = heap->get_region(region_index);\n+        while (num_regions-- != 0) {\n+          scanner->reset_object_range(humongous_region->bottom(), humongous_region->end());\n+          region_index++;\n+          humongous_region = heap->get_region(region_index);\n+        }\n+\n+        \/\/ Then register the humongous object and DIRTY relevant remembered set cards\n+        scanner->register_object_without_lock(obj_addr);\n+        obj->oop_iterate(&dirty_cards_for_cross_generational_pointers);\n+      } else if (!r->is_humongous()) {\n+        \/\/ First, clear the remembered set\n+        scanner->reset_remset(r->bottom(), ShenandoahHeapRegion::region_size_words());\n+        scanner->reset_object_range(r->bottom(), r->end());\n+\n+        \/\/ Then iterate over all objects, registering object and DIRTYing relevant remembered set cards\n+        HeapWord* t = r->top();\n+        while (obj_addr < t) {\n+          oop obj = cast_to_oop(obj_addr);\n+          size_t size = obj->size();\n+          scanner->register_object_without_lock(obj_addr);\n+          obj_addr += obj->oop_iterate_size(&dirty_cards_for_cross_generational_pointers);\n+        }\n+      } \/\/ else, ignore humongous continuation region\n+    }\n+    \/\/ else, this region is FREE or YOUNG or inactive and we can ignore it.\n+    r = _regions->next();\n+  }\n+}\n\\ No newline at end of file\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.cpp","additions":84,"deletions":0,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -177,1 +177,0 @@\n-#include <stdint.h>\n@@ -1050,0 +1049,12 @@\n+\/\/ After Full GC is done, reconstruct the remembered set by iterating over OLD regions,\n+\/\/ registering all objects between bottom() and top(), and dirtying the cards containing\n+\/\/ cross-generational pointers.\n+class ShenandoahReconstructRememberedSetTask : public WorkerTask {\n+private:\n+  ShenandoahRegionIterator* _regions;\n+\n+public:\n+  explicit ShenandoahReconstructRememberedSetTask(ShenandoahRegionIterator* regions);\n+\n+  void work(uint worker_id) override;\n+};\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.hpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"}]}