{"files":[{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2013, 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2013, 2020, Red Hat, Inc. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/shared\/cardTable.hpp\"\n@@ -29,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -33,0 +35,4 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -47,0 +53,1 @@\n+\n@@ -69,0 +76,2 @@\n+  _plab_allocs(0),\n+  _has_young_lab(false),\n@@ -71,1 +80,2 @@\n-  _update_watermark(start) {\n+  _update_watermark(start),\n+  _age(0) {\n@@ -87,1 +97,1 @@\n-void ShenandoahHeapRegion::make_regular_allocation() {\n+void ShenandoahHeapRegion::make_regular_allocation(ShenandoahRegionAffiliation affiliation) {\n@@ -89,1 +99,1 @@\n-\n+  reset_age();\n@@ -94,0 +104,1 @@\n+      set_affiliation(affiliation);\n@@ -103,0 +114,21 @@\n+\/\/ Change affiliation to YOUNG_GENERATION if _state is not _pinned_cset, _regular, or _pinned.  This implements\n+\/\/ behavior previously performed as a side effect of make_regular_bypass().\n+void ShenandoahHeapRegion::make_young_maybe() {\n+  shenandoah_assert_heaplocked();\n+  switch (_state) {\n+   case _empty_uncommitted:\n+   case _empty_committed:\n+   case _cset:\n+   case _humongous_start:\n+   case _humongous_cont:\n+     set_affiliation(YOUNG_GENERATION);\n+     return;\n+   case _pinned_cset:\n+   case _regular:\n+   case _pinned:\n+     return;\n+   default:\n+     assert(false, \"Unexpected _state in make_young_maybe\");\n+  }\n+}\n+\n@@ -107,1 +139,1 @@\n-\n+  reset_age();\n@@ -130,0 +162,1 @@\n+  reset_age();\n@@ -141,1 +174,1 @@\n-void ShenandoahHeapRegion::make_humongous_start_bypass() {\n+void ShenandoahHeapRegion::make_humongous_start_bypass(ShenandoahRegionAffiliation affiliation) {\n@@ -144,1 +177,2 @@\n-\n+  set_affiliation(affiliation);\n+  reset_age();\n@@ -159,0 +193,1 @@\n+  reset_age();\n@@ -170,1 +205,1 @@\n-void ShenandoahHeapRegion::make_humongous_cont_bypass() {\n+void ShenandoahHeapRegion::make_humongous_cont_bypass(ShenandoahRegionAffiliation affiliation) {\n@@ -173,1 +208,2 @@\n-\n+  set_affiliation(affiliation);\n+  reset_age();\n@@ -214,0 +250,1 @@\n+      assert(affiliation() != FREE, \"Pinned region should not be FREE\");\n@@ -232,0 +269,1 @@\n+  \/\/ Leave age untouched.  We need to consult the age when we are deciding whether to promote evacuated objects.\n@@ -244,0 +282,1 @@\n+  reset_age();\n@@ -264,1 +303,2 @@\n-  ShenandoahHeap::heap()->complete_marking_context()->reset_top_bitmap(this);\n+  assert(ShenandoahHeap::heap()->active_generation()->is_mark_complete(), \"Marking should be complete here.\");\n+  ShenandoahHeap::heap()->marking_context()->reset_top_bitmap(this);\n@@ -269,0 +309,1 @@\n+  reset_age();\n@@ -308,0 +349,1 @@\n+  _plab_allocs = 0;\n@@ -311,1 +353,1 @@\n-  return used() - (_tlab_allocs + _gclab_allocs) * HeapWordSize;\n+  return used() - (_tlab_allocs + _gclab_allocs + _plab_allocs) * HeapWordSize;\n@@ -322,0 +364,4 @@\n+size_t ShenandoahHeapRegion::get_plab_allocs() const {\n+  return _plab_allocs * HeapWordSize;\n+}\n+\n@@ -366,0 +412,14 @@\n+  switch (ShenandoahHeap::heap()->region_affiliation(this)) {\n+    case ShenandoahRegionAffiliation::FREE:\n+      st->print(\"|F\");\n+      break;\n+    case ShenandoahRegionAffiliation::YOUNG_GENERATION:\n+      st->print(\"|Y\");\n+      break;\n+    case ShenandoahRegionAffiliation::OLD_GENERATION:\n+      st->print(\"|O\");\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+\n@@ -377,0 +437,3 @@\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    st->print(\"|P \" SIZE_FORMAT_W(5) \"%1s\", byte_size_in_proper_unit(get_plab_allocs()),   proper_unit_for_byte_size(get_plab_allocs()));\n+  }\n@@ -385,1 +448,94 @@\n-void ShenandoahHeapRegion::oop_iterate(OopIterateClosure* blk) {\n+\/\/ oop_iterate without closure and without cancellation.  always return true.\n+bool ShenandoahHeapRegion::oop_fill_and_coalesce_wo_cancel() {\n+  HeapWord* obj_addr = resume_coalesce_and_fill();\n+\n+  assert(!is_humongous(), \"No need to fill or coalesce humongous regions\");\n+  if (!is_active()) {\n+    end_preemptible_coalesce_and_fill();\n+    return true;\n+  }\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* marking_context = heap->marking_context();\n+  \/\/ All objects above TAMS are considered live even though their mark bits will not be set.  Note that young-\n+  \/\/ gen evacuations that interrupt a long-running old-gen concurrent mark may promote objects into old-gen\n+  \/\/ while the old-gen concurrent marking is ongoing.  These newly promoted objects will reside above TAMS\n+  \/\/ and will be treated as live during the current old-gen marking pass, even though they will not be\n+  \/\/ explicitly marked.\n+  HeapWord* t = marking_context->top_at_mark_start(this);\n+\n+  \/\/ Expect marking to be completed before these threads invoke this service.\n+  assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n+  while (obj_addr < t) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if (marking_context->is_marked(obj)) {\n+      assert(obj->klass() != nullptr, \"klass should not be nullptr\");\n+      obj_addr += obj->size();\n+    } else {\n+      \/\/ Object is not marked.  Coalesce and fill dead object with dead neighbors.\n+      HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, t);\n+      assert(next_marked_obj <= t, \"next marked object cannot exceed top\");\n+      size_t fill_size = next_marked_obj - obj_addr;\n+      ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n+      heap->card_scan()->coalesce_objects(obj_addr, fill_size);\n+      obj_addr = next_marked_obj;\n+    }\n+  }\n+  \/\/ Mark that this region has been coalesced and filled\n+  end_preemptible_coalesce_and_fill();\n+  return true;\n+}\n+\n+\/\/ oop_iterate without closure, return true if completed without cancellation\n+bool ShenandoahHeapRegion::oop_fill_and_coalesce() {\n+  HeapWord* obj_addr = resume_coalesce_and_fill();\n+  \/\/ Consider yielding to cancel\/preemption request after this many coalesce operations (skip marked, or coalesce free).\n+  const size_t preemption_stride = 128;\n+\n+  assert(!is_humongous(), \"No need to fill or coalesce humongous regions\");\n+  if (!is_active()) {\n+    end_preemptible_coalesce_and_fill();\n+    return true;\n+  }\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* marking_context = heap->marking_context();\n+  \/\/ All objects above TAMS are considered live even though their mark bits will not be set.  Note that young-\n+  \/\/ gen evacuations that interrupt a long-running old-gen concurrent mark may promote objects into old-gen\n+  \/\/ while the old-gen concurrent marking is ongoing.  These newly promoted objects will reside above TAMS\n+  \/\/ and will be treated as live during the current old-gen marking pass, even though they will not be\n+  \/\/ explicitly marked.\n+  HeapWord* t = marking_context->top_at_mark_start(this);\n+\n+  \/\/ Expect marking to be completed before these threads invoke this service.\n+  assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n+\n+  size_t ops_before_preempt_check = preemption_stride;\n+  while (obj_addr < t) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if (marking_context->is_marked(obj)) {\n+      assert(obj->klass() != nullptr, \"klass should not be nullptr\");\n+      obj_addr += obj->size();\n+    } else {\n+      \/\/ Object is not marked.  Coalesce and fill dead object with dead neighbors.\n+      HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, t);\n+      assert(next_marked_obj <= t, \"next marked object cannot exceed top\");\n+      size_t fill_size = next_marked_obj - obj_addr;\n+      ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n+      heap->card_scan()->coalesce_objects(obj_addr, fill_size);\n+      obj_addr = next_marked_obj;\n+    }\n+    if (ops_before_preempt_check-- == 0) {\n+      if (heap->cancelled_gc()) {\n+        suspend_coalesce_and_fill(obj_addr);\n+        return false;\n+      }\n+      ops_before_preempt_check = preemption_stride;\n+    }\n+  }\n+  \/\/ Mark that this region has been coalesced and filled\n+  end_preemptible_coalesce_and_fill();\n+  return true;\n+}\n+\n+void ShenandoahHeapRegion::global_oop_iterate_and_fill_dead(OopIterateClosure* blk) {\n@@ -388,0 +544,2 @@\n+    \/\/ No need to fill dead within humongous regions.  Either the entire region is dead, or the entire region is\n+    \/\/ unchanged.  A humongous region holds no more than one humongous object.\n@@ -390,1 +548,1 @@\n-    oop_iterate_objects(blk);\n+    global_oop_iterate_objects_and_fill_dead(blk);\n@@ -394,2 +552,2 @@\n-void ShenandoahHeapRegion::oop_iterate_objects(OopIterateClosure* blk) {\n-  assert(! is_humongous(), \"no humongous region here\");\n+void ShenandoahHeapRegion::global_oop_iterate_objects_and_fill_dead(OopIterateClosure* blk) {\n+  assert(!is_humongous(), \"no humongous region here\");\n@@ -397,2 +555,30 @@\n-  HeapWord* t = top();\n-  \/\/ Could call objects iterate, but this is easier.\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* marking_context = heap->marking_context();\n+  RememberedScanner* rem_set_scanner = heap->card_scan();\n+  \/\/ Objects allocated above TAMS are not marked, but are considered live for purposes of current GC efforts.\n+  HeapWord* t = marking_context->top_at_mark_start(this);\n+\n+  assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n+\n+  while (obj_addr < t) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if (marking_context->is_marked(obj)) {\n+      assert(obj->klass() != nullptr, \"klass should not be nullptr\");\n+      \/\/ when promoting an entire region, we have to register the marked objects as well\n+      obj_addr += obj->oop_iterate_size(blk);\n+    } else {\n+      \/\/ Object is not marked.  Coalesce and fill dead object with dead neighbors.\n+      HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, t);\n+      assert(next_marked_obj <= t, \"next marked object cannot exceed top\");\n+      size_t fill_size = next_marked_obj - obj_addr;\n+      ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n+\n+      \/\/ coalesce_objects() unregisters all but first object subsumed within coalesced range.\n+      rem_set_scanner->coalesce_objects(obj_addr, fill_size);\n+      obj_addr = next_marked_obj;\n+    }\n+  }\n+\n+  \/\/ Any object above TAMS and below top() is considered live.\n+  t = top();\n@@ -405,0 +591,50 @@\n+\/\/ DO NOT CANCEL.  If this worker thread has accepted responsibility for scanning a particular range of addresses, it\n+\/\/ must finish the work before it can be cancelled.\n+void ShenandoahHeapRegion::oop_iterate_humongous_slice(OopIterateClosure* blk, bool dirty_only,\n+                                                       HeapWord* start, size_t words, bool write_table) {\n+  assert(words % CardTable::card_size_in_words() == 0, \"Humongous iteration must span whole number of cards\");\n+  assert(is_humongous(), \"only humongous region here\");\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  \/\/ Find head.\n+  ShenandoahHeapRegion* r = humongous_start_region();\n+  assert(r->is_humongous_start(), \"need humongous head here\");\n+  assert(CardTable::card_size_in_words() * (words \/ CardTable::card_size_in_words()) == words,\n+         \"slice must be integral number of cards\");\n+\n+  oop obj = cast_to_oop(r->bottom());\n+  RememberedScanner* scanner = ShenandoahHeap::heap()->card_scan();\n+  size_t card_index = scanner->card_index_for_addr(start);\n+  size_t num_cards = words \/ CardTable::card_size_in_words();\n+\n+  if (dirty_only) {\n+    if (write_table) {\n+      while (num_cards-- > 0) {\n+        if (scanner->is_write_card_dirty(card_index++)) {\n+          obj->oop_iterate(blk, MemRegion(start, start + CardTable::card_size_in_words()));\n+        }\n+        start += CardTable::card_size_in_words();\n+      }\n+    } else {\n+      while (num_cards-- > 0) {\n+        if (scanner->is_card_dirty(card_index++)) {\n+          obj->oop_iterate(blk, MemRegion(start, start + CardTable::card_size_in_words()));\n+        }\n+        start += CardTable::card_size_in_words();\n+      }\n+    }\n+  } else {\n+    \/\/ Scan all data, regardless of whether cards are dirty\n+    obj->oop_iterate(blk, MemRegion(start, start + num_cards * CardTable::card_size_in_words()));\n+  }\n+}\n+\n+void ShenandoahHeapRegion::oop_iterate_humongous(OopIterateClosure* blk, HeapWord* start, size_t words) {\n+  assert(is_humongous(), \"only humongous region here\");\n+  \/\/ Find head.\n+  ShenandoahHeapRegion* r = humongous_start_region();\n+  assert(r->is_humongous_start(), \"need humongous head here\");\n+  oop obj = cast_to_oop(r->bottom());\n+  obj->oop_iterate(blk, MemRegion(start, start + words));\n+}\n+\n@@ -430,0 +666,5 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  shenandoah_assert_heaplocked();\n+\n+  heap->generation_for(affiliation())->decrease_used(used());\n+\n@@ -435,1 +676,1 @@\n-  ShenandoahHeap::heap()->marking_context()->reset_top_at_mark_start(this);\n+  heap->marking_context()->reset_top_at_mark_start(this);\n@@ -439,0 +680,1 @@\n+  set_affiliation(FREE);\n@@ -483,0 +725,5 @@\n+  \/\/ Generational Shenandoah needs this alignment for card tables.\n+  if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+    max_heap_size = align_up(max_heap_size , CardTable::ct_max_alignment_constraint());\n+  }\n+\n@@ -689,0 +936,151 @@\n+\n+void ShenandoahHeapRegion::set_affiliation(ShenandoahRegionAffiliation new_affiliation) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  ShenandoahRegionAffiliation region_affiliation = heap->region_affiliation(this);\n+  {\n+    ShenandoahMarkingContext* const ctx = heap->complete_marking_context();\n+    log_debug(gc)(\"Setting affiliation of Region \" SIZE_FORMAT \" from %s to %s, top: \" PTR_FORMAT \", TAMS: \" PTR_FORMAT\n+                  \", watermark: \" PTR_FORMAT \", top_bitmap: \" PTR_FORMAT,\n+                  index(), affiliation_name(region_affiliation), affiliation_name(new_affiliation),\n+                  p2i(top()), p2i(ctx->top_at_mark_start(this)), p2i(_update_watermark), p2i(ctx->top_bitmap(this)));\n+  }\n+\n+#ifdef ASSERT\n+  {\n+    \/\/ During full gc, heap->complete_marking_context() is not valid, may equal nullptr.\n+    ShenandoahMarkingContext* const ctx = heap->complete_marking_context();\n+    size_t idx = this->index();\n+    HeapWord* top_bitmap = ctx->top_bitmap(this);\n+\n+    assert(ctx->is_bitmap_clear_range(top_bitmap, _end),\n+           \"Region \" SIZE_FORMAT \", bitmap should be clear between top_bitmap: \" PTR_FORMAT \" and end: \" PTR_FORMAT, idx,\n+           p2i(top_bitmap), p2i(_end));\n+  }\n+#endif\n+\n+  if (region_affiliation == new_affiliation) {\n+    return;\n+  }\n+\n+  if (!heap->mode()->is_generational()) {\n+    heap->set_affiliation(this, new_affiliation);\n+    return;\n+  }\n+\n+  log_trace(gc)(\"Changing affiliation of region %zu from %s to %s\",\n+    index(), affiliation_name(region_affiliation), affiliation_name(new_affiliation));\n+\n+  if (region_affiliation == ShenandoahRegionAffiliation::YOUNG_GENERATION) {\n+    heap->young_generation()->decrement_affiliated_region_count();\n+  } else if (region_affiliation == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+    heap->old_generation()->decrement_affiliated_region_count();\n+  }\n+\n+  size_t regions;\n+  switch (new_affiliation) {\n+    case FREE:\n+      assert(!has_live(), \"Free region should not have live data\");\n+      break;\n+    case YOUNG_GENERATION:\n+      reset_age();\n+      regions = heap->young_generation()->increment_affiliated_region_count();\n+      \/\/ During Full GC, we allow temporary violation of this requirement.  We enforce that this condition is\n+      \/\/ restored upon completion of Full GC.\n+      assert(heap->is_full_gc_in_progress() ||\n+             (regions * ShenandoahHeapRegion::region_size_bytes() <= heap->young_generation()->adjusted_capacity()),\n+             \"Number of young regions cannot exceed adjusted capacity\");\n+      break;\n+    case OLD_GENERATION:\n+      regions = heap->old_generation()->increment_affiliated_region_count();\n+      \/\/ During Full GC, we allow temporary violation of this requirement.  We enforce that this condition is\n+      \/\/ restored upon completion of Full GC.\n+      assert(heap->is_full_gc_in_progress() ||\n+             (regions * ShenandoahHeapRegion::region_size_bytes() <= heap->old_generation()->adjusted_capacity()),\n+             \"Number of old regions cannot exceed adjusted capacity\");\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      return;\n+  }\n+  heap->set_affiliation(this, new_affiliation);\n+}\n+\n+\/\/ Returns number of regions promoted, or zero if we choose not to promote.\n+size_t ShenandoahHeapRegion::promote_humongous() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* marking_context = heap->marking_context();\n+  assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n+  assert(is_young(), \"Only young regions can be promoted\");\n+  assert(is_humongous_start(), \"Should not promote humongous continuation in isolation\");\n+  assert(age() >= InitialTenuringThreshold, \"Only promote regions that are sufficiently aged\");\n+\n+  ShenandoahGeneration* old_generation = heap->old_generation();\n+  ShenandoahGeneration* young_generation = heap->young_generation();\n+\n+  oop obj = cast_to_oop(bottom());\n+  assert(marking_context->is_marked(obj), \"promoted humongous object should be alive\");\n+\n+  \/\/ TODO: Consider not promoting humongous objects that represent primitive arrays.  Leaving a primitive array\n+  \/\/ (obj->is_typeArray()) in young-gen is harmless because these objects are never relocated and they are not\n+  \/\/ scanned.  Leaving primitive arrays in young-gen memory allows their memory to be reclaimed more quickly when\n+  \/\/ it becomes garbage.  Better to not make this change until sizes of young-gen and old-gen are completely\n+  \/\/ adaptive, as leaving primitive arrays in young-gen might be perceived as an \"astonishing result\" by someone\n+  \/\/ has carefully analyzed the required sizes of an application's young-gen and old-gen.\n+\n+  size_t spanned_regions = ShenandoahHeapRegion::required_regions(obj->size() * HeapWordSize);\n+  size_t index_limit = index() + spanned_regions;\n+\n+  {\n+    \/\/ We need to grab the heap lock in order to avoid a race when changing the affiliations of spanned_regions from\n+    \/\/ young to old.\n+    ShenandoahHeapLocker locker(heap->lock());\n+    size_t available_old_regions = old_generation->adjusted_unaffiliated_regions();\n+    if (spanned_regions <= available_old_regions) {\n+      log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", spanning \" SIZE_FORMAT, index(), spanned_regions);\n+\n+      \/\/ For this region and each humongous continuation region spanned by this humongous object, change\n+      \/\/ affiliation to OLD_GENERATION and adjust the generation-use tallies.  The remnant of memory\n+      \/\/ in the last humongous region that is not spanned by obj is currently not used.\n+      for (size_t i = index(); i < index_limit; i++) {\n+        ShenandoahHeapRegion* r = heap->get_region(i);\n+        log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", from \" PTR_FORMAT \" to \" PTR_FORMAT,\n+                      r->index(), p2i(r->bottom()), p2i(r->top()));\n+        \/\/ We mark the entire humongous object's range as dirty after loop terminates, so no need to dirty the range here\n+        r->set_affiliation(OLD_GENERATION);\n+        old_generation->increase_used(r->used());\n+        young_generation->decrease_used(r->used());\n+      }\n+      \/\/ Then fall through to finish the promotion after releasing the heap lock.\n+    } else {\n+      \/\/ There are not enough available old regions to promote this humongous region at this time, so defer promotion.\n+      \/\/ TODO: Consider allowing the promotion now, with the expectation that we can resize and\/or collect OLD\n+      \/\/ momentarily to address the transient violation of budgets.  Some problems that need to be addressed in order\n+      \/\/ to allow transient violation of capacity budgets are:\n+      \/\/  1. Various size_t subtractions assume usage is less than capacity, and thus assume there will be no\n+      \/\/     arithmetic underflow when we subtract usage from capacity.  The results of such size_t subtractions\n+      \/\/     would need to be guarded and special handling provided.\n+      \/\/  2. ShenandoahVerifier enforces that usage is less than capacity.  If we are going to relax this constraint,\n+      \/\/     we need to think about what conditions allow the constraint to be violated and document and implement the\n+      \/\/     changes.\n+      return 0;\n+    }\n+  }\n+\n+  \/\/ Since this region may have served previously as OLD, it may hold obsolete object range info.\n+  heap->card_scan()->reset_object_range(bottom(), bottom() + spanned_regions * ShenandoahHeapRegion::region_size_words());\n+  \/\/ Since the humongous region holds only one object, no lock is necessary for this register_object() invocation.\n+  heap->card_scan()->register_object_wo_lock(bottom());\n+\n+  if (obj->is_typeArray()) {\n+    \/\/ Primitive arrays don't need to be scanned.\n+    log_debug(gc)(\"Clean cards for promoted humongous object (Region \" SIZE_FORMAT \") from \" PTR_FORMAT \" to \" PTR_FORMAT,\n+                  index(), p2i(bottom()), p2i(bottom() + obj->size()));\n+    heap->card_scan()->mark_range_as_clean(bottom(), obj->size());\n+  } else {\n+    log_debug(gc)(\"Dirty cards for promoted humongous object (Region \" SIZE_FORMAT \") from \" PTR_FORMAT \" to \" PTR_FORMAT,\n+                  index(), p2i(bottom()), p2i(bottom() + obj->size()));\n+    heap->card_scan()->mark_range_as_dirty(bottom(), obj->size());\n+  }\n+  return index_limit - index();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":416,"deletions":18,"binary":false,"changes":434,"status":"modified"},{"patch":"@@ -320,0 +320,9 @@\n+tier1_gc_shenandoah_generational = \\\n+  gc\/shenandoah\/generational\/\n+\n+# No tier 2 tests for shenandoah_generational at this time\n+tier2_gc_shenandoah_generational =\n+\n+# No tier 3 tests for shenandoah_generational at this time\n+tier3_gc_shenandoah_generational =\n+\n@@ -351,0 +360,1 @@\n+# include shenandoah generational tests in tier3 shenandoah\n@@ -357,0 +367,1 @@\n+  :hotspot_gc_shenandoah_generational \\\n@@ -364,0 +375,5 @@\n+hotspot_gc_shenandoah_generational = \\\n+  :tier1_gc_shenandoah_generational \\\n+  :tier2_gc_shenandoah_generational \\\n+  :tier3_gc_shenandoah_generational\n+\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"}]}