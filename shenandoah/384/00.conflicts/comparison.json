{"files":[{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -34,0 +35,3 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -88,3 +92,6 @@\n-ShenandoahConcurrentGC::ShenandoahConcurrentGC() :\n-  _mark(),\n-  _degen_point(ShenandoahDegenPoint::_degenerated_unset) {\n+ShenandoahConcurrentGC::ShenandoahConcurrentGC(ShenandoahGeneration* generation, bool do_old_gc_bootstrap) :\n+  _mark(generation),\n+  _degen_point(ShenandoahDegenPoint::_degenerated_unset),\n+  _abbreviated(false),\n+  _do_old_gc_bootstrap(do_old_gc_bootstrap),\n+  _generation(generation) {\n@@ -97,4 +104,0 @@\n-void ShenandoahConcurrentGC::cancel() {\n-  ShenandoahConcurrentMark::cancel();\n-}\n-\n@@ -103,0 +106,1 @@\n+\n@@ -113,0 +117,10 @@\n+\n+    \/\/ Reset task queue stats here, rather than in mark_concurrent_roots,\n+    \/\/ because remembered set scan will `push` oops into the queues and\n+    \/\/ resetting after this happens will lose those counts.\n+    TASKQUEUE_STATS_ONLY(_mark.task_queues()->reset_taskqueue_stats());\n+\n+    \/\/ Concurrent remembered set scanning\n+    entry_scan_remembered_set();\n+    \/\/ TODO: When RS scanning yields, we will need a check_cancellation_and_abort() degeneration point here.\n+\n@@ -115,0 +129,1 @@\n+<<<<<<< HEAD\n@@ -116,0 +131,3 @@\n+=======\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_roots)) {\n+>>>>>>> 58d39b04df1e737f112557db32af38f3120baea2\n@@ -129,0 +147,13 @@\n+  \/\/ If GC was cancelled before final mark, then the safepoint operation will do nothing\n+  \/\/ and the concurrent mark will still be in progress. In this case it is safe to resume\n+  \/\/ the degenerated cycle from the marking phase. On the other hand, if the GC is cancelled\n+  \/\/ after final mark (but before this check), then the final mark safepoint operation\n+  \/\/ will have finished the mark (setting concurrent mark in progress to false). Final mark\n+  \/\/ will also have setup state (in concurrent stack processing) that will not be safe to\n+  \/\/ resume from the marking phase in the degenerated cycle. That is, if the cancellation\n+  \/\/ occurred after final mark, we must resume the degenerated cycle after the marking phase.\n+  if (_generation->is_concurrent_mark_in_progress() && check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_mark)) {\n+    assert(!heap->is_concurrent_weak_root_in_progress(), \"Weak roots should not be in progress when concurrent mark is in progress\");\n+    return false;\n+  }\n+\n@@ -141,1 +172,2 @@\n-  \/\/ the space. This would be the last action if there is nothing to evacuate.\n+  \/\/ the space. This would be the last action if there is nothing to evacuate.  Note that\n+  \/\/ we will not age young-gen objects in the case that we skip evacuation.\n@@ -145,0 +177,2 @@\n+    \/\/ TODO: Not sure there is value in logging free-set status right here.  Note that whenever the free set is rebuilt,\n+    \/\/ it logs the newly rebuilt status.\n@@ -171,0 +205,4 @@\n+<<<<<<< HEAD\n+=======\n+  }\n+>>>>>>> 58d39b04df1e737f112557db32af38f3120baea2\n@@ -172,0 +210,1 @@\n+  if (heap->has_forwarded_objects()) {\n@@ -190,0 +229,4 @@\n+    \/\/ We chose not to evacuate because we found sufficient immediate garbage. Note that we\n+    \/\/ do not check for cancellation here because, at this point, the cycle is effectively\n+    \/\/ complete. If the cycle has been cancelled here, the control thread will detect it\n+    \/\/ on its next iteration and run a degenerated young cycle.\n@@ -191,0 +234,1 @@\n+    _abbreviated = true;\n@@ -193,0 +237,45 @@\n+  \/\/ We defer generation resizing actions until after cset regions have been recycled.  We do this even following an\n+  \/\/ abbreviated cycle.\n+  if (heap->mode()->is_generational()) {\n+    bool success;\n+    size_t region_xfer;\n+    const char* region_destination;\n+    ShenandoahYoungGeneration* young_gen = heap->young_generation();\n+    ShenandoahGeneration* old_gen = heap->old_generation();\n+    {\n+      ShenandoahHeapLocker locker(heap->lock());\n+\n+      size_t old_region_surplus = heap->get_old_region_surplus();\n+      size_t old_region_deficit = heap->get_old_region_deficit();\n+      if (old_region_surplus) {\n+        success = heap->generation_sizer()->transfer_to_young(old_region_surplus);\n+        region_destination = \"young\";\n+        region_xfer = old_region_surplus;\n+      } else if (old_region_deficit) {\n+        success = heap->generation_sizer()->transfer_to_old(old_region_deficit);\n+        region_destination = \"old\";\n+        region_xfer = old_region_deficit;\n+        if (!success) {\n+          ((ShenandoahOldHeuristics *) old_gen->heuristics())->trigger_cannot_expand();\n+        }\n+      } else {\n+        region_destination = \"none\";\n+        region_xfer = 0;\n+        success = true;\n+      }\n+      heap->set_old_region_surplus(0);\n+      heap->set_old_region_deficit(0);\n+      heap->set_young_evac_reserve(0);\n+      heap->set_old_evac_reserve(0);\n+      heap->set_promoted_reserve(0);\n+    }\n+\n+    \/\/ Report outside the heap lock\n+    size_t young_available = young_gen->available();\n+    size_t old_available = old_gen->available();\n+    log_info(gc, ergo)(\"After cleanup, %s \" SIZE_FORMAT \" regions to %s to prepare for next gc, old available: \"\n+                       SIZE_FORMAT \"%s, young_available: \" SIZE_FORMAT \"%s\",\n+                       success? \"successfully transferred\": \"failed to transfer\", region_xfer, region_destination,\n+                       byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n+                       byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n+  }\n@@ -302,0 +391,2 @@\n+  heap->try_inject_alloc_failure();\n+\n@@ -303,3 +394,10 @@\n-  static const char* msg = \"Concurrent reset\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset);\n-  EventMark em(\"%s\", msg);\n+  {\n+    static const char* msg = \"Concurrent reset\";\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset);\n+    EventMark em(\"%s\", msg);\n+\n+    ShenandoahWorkerScope scope(heap->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n+                                msg);\n+    op_reset();\n+  }\n@@ -307,3 +405,7 @@\n-  ShenandoahWorkerScope scope(heap->workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n-                              \"concurrent reset\");\n+  if (_do_old_gc_bootstrap) {\n+    static const char* msg = \"Concurrent reset (OLD)\";\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset_old);\n+    ShenandoahWorkerScope scope(ShenandoahHeap::heap()->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n+                                msg);\n+    EventMark em(\"%s\", msg);\n@@ -311,2 +413,19 @@\n-  heap->try_inject_alloc_failure();\n-  op_reset();\n+    heap->old_generation()->prepare_gc();\n+  }\n+}\n+\n+void ShenandoahConcurrentGC::entry_scan_remembered_set() {\n+  if (_generation->is_young()) {\n+    ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+    TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+    const char* msg = \"Concurrent remembered set scanning\";\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::init_scan_rset);\n+    EventMark em(\"%s\", msg);\n+\n+    ShenandoahWorkerScope scope(heap->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_rs_scanning(),\n+                                msg);\n+\n+    heap->try_inject_alloc_failure();\n+    _generation->scan_remembered_set(true \/* is_concurrent *\/);\n+  }\n@@ -493,2 +612,1 @@\n-\n-  heap->prepare_gc();\n+  _generation->prepare_gc();\n@@ -507,1 +625,2 @@\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n+      \/\/ reset, so it is very likely we don't need to do another write here.  Since most regions\n+      \/\/ are not \"active\", this path is relatively rare.\n@@ -529,2 +648,2 @@\n-  assert(heap->marking_context()->is_bitmap_clear(), \"need clear marking bitmap\");\n-  assert(!heap->marking_context()->is_complete(), \"should not be complete\");\n+  assert(_generation->is_bitmap_clear(), \"need clear marking bitmap\");\n+  assert(!_generation->is_mark_complete(), \"should not be complete\");\n@@ -533,0 +652,22 @@\n+\n+  if (heap->mode()->is_generational()) {\n+    if (_generation->is_young() || (_generation->is_global() && ShenandoahVerify)) {\n+      \/\/ The current implementation of swap_remembered_set() copies the write-card-table\n+      \/\/ to the read-card-table. The remembered sets are also swapped for GLOBAL collections\n+      \/\/ so that the verifier works with the correct copy of the card table when verifying.\n+      \/\/ TODO: This path should not really depend on ShenandoahVerify.\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_swap_rset);\n+      _generation->swap_remembered_set();\n+    }\n+\n+    if (_generation->is_global()) {\n+      heap->cancel_old_gc();\n+    } else if (heap->is_concurrent_old_mark_in_progress()) {\n+      \/\/ Purge the SATB buffers, transferring any valid, old pointers to the\n+      \/\/ old generation mark queue. Any pointers in a young region will be\n+      \/\/ abandoned.\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_transfer_satb);\n+      heap->transfer_old_pointers_from_satb();\n+    }\n+  }\n+\n@@ -541,1 +682,1 @@\n-  heap->set_concurrent_mark_in_progress(true);\n+  _generation->set_concurrent_mark_in_progress(true);\n@@ -545,1 +686,6 @@\n-  {\n+  if (_do_old_gc_bootstrap) {\n+    \/\/ Update region state for both young and old regions\n+    \/\/ TODO: We should be able to pull this out of the safepoint for the bootstrap\n+    \/\/ cycle. The top of an old region will only move when a GC cycle evacuates\n+    \/\/ objects into it. When we start an old cycle, we know that nothing can touch\n+    \/\/ the top of old regions.\n@@ -549,0 +695,6 @@\n+    heap->old_generation()->ref_processor()->reset_thread_locals();\n+  } else {\n+    \/\/ Update region state for only young regions\n+    ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_region_states);\n+    ShenandoahInitMarkUpdateRegionStateClosure cl;\n+    _generation->parallel_heap_region_iterate(&cl);\n@@ -552,1 +704,1 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = _generation->ref_processor();\n@@ -592,1 +744,26 @@\n-    heap->prepare_regions_and_collection_set(true \/*concurrent*\/);\n+    \/\/ The collection set is chosen by prepare_regions_and_collection_set().\n+    \/\/\n+    \/\/ TODO: Under severe memory overload conditions that can be checked here, we may want to limit\n+    \/\/ the inclusion of old-gen candidates within the collection set.  This would allow us to prioritize efforts on\n+    \/\/ evacuating young-gen,  This remediation is most appropriate when old-gen availability is very high (so there\n+    \/\/ are negligible negative impacts from delaying completion of old-gen evacuation) and when young-gen collections\n+    \/\/ are \"under duress\" (as signalled by very low availability of memory within young-gen, indicating that\/ young-gen\n+    \/\/ collections are not triggering frequently enough).\n+    _generation->prepare_regions_and_collection_set(true \/*concurrent*\/);\n+\n+    \/\/ Upon return from prepare_regions_and_collection_set(), certain parameters have been established to govern the\n+    \/\/ evacuation efforts that are about to begin.  In particular:\n+    \/\/\n+    \/\/ heap->get_promoted_reserve() represents the amount of memory within old-gen's available memory that has\n+    \/\/   been set aside to hold objects promoted from young-gen memory.  This represents an estimated percentage\n+    \/\/   of the live young-gen memory within the collection set.  If there is more data ready to be promoted than\n+    \/\/   can fit within this reserve, the promotion of some objects will be deferred until a subsequent evacuation\n+    \/\/   pass.\n+    \/\/\n+    \/\/ heap->get_old_evac_reserve() represents the amount of memory within old-gen's available memory that has been\n+    \/\/  set aside to hold objects evacuated from the old-gen collection set.\n+    \/\/\n+    \/\/ heap->get_young_evac_reserve() represents the amount of memory within young-gen's available memory that has\n+    \/\/  been set aside to hold objects evacuated from the young-gen collection set.  Conservatively, this value\n+    \/\/  equals the entire amount of live young-gen memory within the collection set, even though some of this memory\n+    \/\/  will likely be promoted.\n@@ -597,21 +774,51 @@\n-    if (!heap->collection_set()->is_empty()) {\n-      if (ShenandoahVerify) {\n-        heap->verifier()->verify_before_evacuation();\n-      }\n-\n-      heap->set_evacuation_in_progress(true);\n-      \/\/ From here on, we need to update references.\n-      heap->set_has_forwarded_objects(true);\n-\n-      \/\/ Verify before arming for concurrent processing.\n-      \/\/ Otherwise, verification can trigger stack processing.\n-      if (ShenandoahVerify) {\n-        heap->verifier()->verify_during_evacuation();\n-      }\n-\n-      \/\/ Arm nmethods\/stack for concurrent processing\n-      ShenandoahCodeRoots::arm_nmethods_for_evac();\n-      ShenandoahStackWatermark::change_epoch_id();\n-\n-      if (ShenandoahPacing) {\n-        heap->pacer()->setup_for_evac();\n+    if (heap->mode()->is_generational()) {\n+      size_t humongous_regions_promoted = heap->get_promotable_humongous_regions();\n+      size_t regular_regions_promoted_in_place = heap->get_regular_regions_promoted_in_place();\n+      if (!heap->collection_set()->is_empty() || (humongous_regions_promoted + regular_regions_promoted_in_place > 0)) {\n+        \/\/ Even if the collection set is empty, we need to do evacuation if there are regions to be promoted in place.\n+        \/\/ Concurrent evacuation takes responsibility for registering objects and setting the remembered set cards to dirty.\n+\n+        LogTarget(Debug, gc, cset) lt;\n+        if (lt.is_enabled()) {\n+          ResourceMark rm;\n+          LogStream ls(lt);\n+          heap->collection_set()->print_on(&ls);\n+        }\n+\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_before_evacuation();\n+        }\n+\n+        heap->set_evacuation_in_progress(true);\n+\n+        \/\/ Verify before arming for concurrent processing.\n+        \/\/ Otherwise, verification can trigger stack processing.\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_during_evacuation();\n+        }\n+\n+        \/\/ Generational mode may promote objects in place during the evacuation phase.\n+        \/\/ If that is the only reason we are evacuating, we don't need to update references\n+        \/\/ and there will be no forwarded objects on the heap.\n+        heap->set_has_forwarded_objects(!heap->collection_set()->is_empty());\n+\n+        \/\/ Arm nmethods\/stack for concurrent processing\n+        if (!heap->collection_set()->is_empty()) {\n+          \/\/ Iff objects will be evaluated, arm the nmethod barriers. These will be disarmed\n+          \/\/ under the same condition (established in prepare_concurrent_roots) after strong\n+          \/\/ root evacuation has completed (see op_strong_roots).\n+          ShenandoahCodeRoots::arm_nmethods_for_evac();\n+          ShenandoahStackWatermark::change_epoch_id();\n+        }\n+\n+        if (ShenandoahPacing) {\n+          heap->pacer()->setup_for_evac();\n+        }\n+      } else {\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_after_concmark();\n+        }\n+\n+        if (VerifyAfterGC) {\n+          Universe::verify();\n+        }\n@@ -620,6 +827,39 @@\n-      if (ShenandoahVerify) {\n-        heap->verifier()->verify_after_concmark();\n-      }\n-\n-      if (VerifyAfterGC) {\n-        Universe::verify();\n+      \/\/ Not is_generational()\n+      if (!heap->collection_set()->is_empty()) {\n+        LogTarget(Info, gc, ergo) lt;\n+        if (lt.is_enabled()) {\n+          ResourceMark rm;\n+          LogStream ls(lt);\n+          heap->collection_set()->print_on(&ls);\n+        }\n+\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_before_evacuation();\n+        }\n+\n+        heap->set_evacuation_in_progress(true);\n+\n+        \/\/ Verify before arming for concurrent processing.\n+        \/\/ Otherwise, verification can trigger stack processing.\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_during_evacuation();\n+        }\n+\n+        \/\/ From here on, we need to update references.\n+        heap->set_has_forwarded_objects(true);\n+\n+        \/\/ Arm nmethods\/stack for concurrent processing\n+        ShenandoahCodeRoots::arm_nmethods_for_evac();\n+        ShenandoahStackWatermark::change_epoch_id();\n+\n+        if (ShenandoahPacing) {\n+          heap->pacer()->setup_for_evac();\n+        }\n+      } else {\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_after_concmark();\n+        }\n+\n+        if (VerifyAfterGC) {\n+          Universe::verify();\n+        }\n@@ -647,0 +887,1 @@\n+  ShenandoahThreadLocalData::enable_plab_promotions(thread);\n@@ -660,0 +901,3 @@\n+    Thread* worker_thread = Thread::current();\n+    ShenandoahThreadLocalData::enable_plab_promotions(worker_thread);\n+\n@@ -684,1 +928,1 @@\n-  heap->ref_processor()->process_references(ShenandoahPhaseTimings::conc_weak_refs, heap->workers(), true \/* concurrent *\/);\n+  _generation->ref_processor()->process_references(ShenandoahPhaseTimings::conc_weak_refs, heap->workers(), true \/* concurrent *\/);\n@@ -711,2 +955,9 @@\n-      shenandoah_assert_correct(p, obj);\n-      ShenandoahHeap::atomic_clear_oop(p, obj);\n+      if (_heap->is_in_active_generation(obj)) {\n+        \/\/ TODO: This worries me. Here we are asserting that an unmarked from-space object is 'correct'.\n+        \/\/ Normally, I would call this a bogus assert, but there seems to be a legitimate use-case for\n+        \/\/ accessing from-space objects during class unloading. However, the from-space object may have\n+        \/\/ been \"filled\". We've made no effort to prevent old generation classes being unloaded by young\n+        \/\/ gen (and vice-versa).\n+        shenandoah_assert_correct(p, obj);\n+        ShenandoahHeap::atomic_clear_oop(p, obj);\n+      }\n@@ -936,1 +1187,3 @@\n-\n+  if (ShenandoahVerify) {\n+    heap->verifier()->verify_before_updaterefs();\n+  }\n@@ -981,1 +1234,1 @@\n-    heap->clear_cancelled_gc();\n+    heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n@@ -989,0 +1242,18 @@\n+  if (heap->mode()->is_generational() && heap->is_concurrent_old_mark_in_progress()) {\n+    \/\/ When the SATB barrier is left on to support concurrent old gen mark, it may pick up writes to\n+    \/\/ objects in the collection set. After those objects are evacuated, the pointers in the\n+    \/\/ SATB are no longer safe. Once we have finished update references, we are guaranteed that\n+    \/\/ no more writes to the collection set are possible.\n+    \/\/\n+    \/\/ This will transfer any old pointers in _active_ regions from the SATB to the old gen\n+    \/\/ mark queues. All other pointers will be discarded. This would also discard any pointers\n+    \/\/ in old regions that were included in a mixed evacuation. We aren't using the SATB filter\n+    \/\/ methods here because we cannot control when they execute. If the SATB filter runs _after_\n+    \/\/ a region has been recycled, we will not be able to detect the bad pointer.\n+    \/\/\n+    \/\/ We are not concerned about skipping this step in abbreviated cycles because regions\n+    \/\/ with no live objects cannot have been written to and so cannot have entries in the SATB\n+    \/\/ buffers.\n+    heap->transfer_old_pointers_from_satb();\n+  }\n+\n@@ -994,0 +1265,4 @@\n+  \/\/ Aging_cycle is only relevant during evacuation cycle for individual objects and during final mark for\n+  \/\/ entire regions.  Both of these relevant operations occur before final update refs.\n+  heap->set_aging_cycle(false);\n+\n@@ -1006,1 +1281,27 @@\n-  ShenandoahHeap::heap()->set_concurrent_weak_root_in_progress(false);\n+\n+  ShenandoahHeap *heap = ShenandoahHeap::heap();\n+  heap->set_concurrent_weak_root_in_progress(false);\n+  heap->set_evacuation_in_progress(false);\n+\n+  if (heap->mode()->is_generational()) {\n+    \/\/ If the cycle was shortened for having enough immediate garbage, this could be\n+    \/\/ the last GC safepoint before concurrent marking of old resumes. We must be sure\n+    \/\/ that old mark threads don't see any pointers to garbage in the SATB buffers.\n+    if (heap->is_concurrent_old_mark_in_progress()) {\n+      heap->transfer_old_pointers_from_satb();\n+    }\n+\n+    ShenandoahMarkingContext *ctx = heap->complete_marking_context();\n+    for (size_t i = 0; i < heap->num_regions(); i++) {\n+      ShenandoahHeapRegion *r = heap->get_region(i);\n+      if (r->is_active() && r->is_young()) {\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        HeapWord* top = r->top();\n+        if (top > tams) {\n+          r->reset_age();\n+        } else if (heap->is_aging_cycle()) {\n+          r->increment_age();\n+        }\n+      }\n+    }\n+  }\n@@ -1025,1 +1326,1 @@\n-    return \"Pause Init Mark (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Init Mark\", \" (unload classes)\");\n@@ -1027,1 +1328,1 @@\n-    return \"Pause Init Mark\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Init Mark\", \"\");\n@@ -1033,1 +1334,3 @@\n-  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  assert(!heap->has_forwarded_objects() || heap->is_concurrent_old_mark_in_progress(),\n+         \"Should not have forwarded objects during final mark, unless old gen concurrent mark is running\");\n+\n@@ -1035,1 +1338,1 @@\n-    return \"Pause Final Mark (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Final Mark\", \" (unload classes)\");\n@@ -1037,1 +1340,1 @@\n-    return \"Pause Final Mark\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Final Mark\", \"\");\n@@ -1043,1 +1346,2 @@\n-  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  assert(!heap->has_forwarded_objects() || heap->is_concurrent_old_mark_in_progress(),\n+         \"Should not have forwarded objects concurrent mark, unless old gen concurrent mark is running\");\n@@ -1045,1 +1349,1 @@\n-    return \"Concurrent marking (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent marking\", \" (unload classes)\");\n@@ -1047,1 +1351,1 @@\n-    return \"Concurrent marking\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent marking\", \"\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":370,"deletions":66,"binary":false,"changes":436,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+<<<<<<< HEAD\n@@ -47,0 +48,5 @@\n+=======\n+  log_info(gc, init)(\"Heap Region Size: \" PROPERFMT, PROPERFMTARGS(ShenandoahHeapRegion::region_size_bytes()));\n+  log_info(gc, init)(\"TLAB Size Max: \" PROPERFMT, PROPERFMTARGS(ShenandoahHeapRegion::max_tlab_size_bytes()));\n+  log_info(gc, init)(\"Humongous Object Threshold: \" PROPERFMT, PROPERFMTARGS(ShenandoahHeapRegion::humongous_threshold_bytes()));\n+>>>>>>> 58d39b04df1e737f112557db32af38f3120baea2\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahInitLogger.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"}]}