{"files":[{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -89,0 +90,4 @@\n+\n+      if (ShenandoahCardBarrier) {\n+        post_barrier(access, access.resolved_addr(), new_value.result());\n+      }\n@@ -92,1 +97,8 @@\n-  return BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+\n+  LIR_Opr result =  BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+\n+  if (ShenandoahCardBarrier && access.is_oop()) {\n+    post_barrier(access, access.resolved_addr(), new_value.result());\n+  }\n+\n+  return result;\n@@ -116,0 +128,3 @@\n+    if (ShenandoahCardBarrier) {\n+      post_barrier(access, access.resolved_addr(), result);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_aarch64.cpp","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -34,0 +35,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -80,0 +82,7 @@\n+void ShenandoahBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                                       Register start, Register count, Register tmp, RegSet saved_regs) {\n+  if (ShenandoahCardBarrier && is_oop) {\n+    gen_write_ref_array_post_barrier(masm, decorators, start, count, tmp, saved_regs);\n+  }\n+}\n+\n@@ -365,0 +374,20 @@\n+void ShenandoahBarrierSetAssembler::store_check(MacroAssembler* masm, Register obj) {\n+  assert(ShenandoahCardBarrier, \"Did you mean to enable ShenandoahCardBarrier?\");\n+\n+  __ lsr(obj, obj, CardTable::card_shift());\n+\n+  assert(CardTable::dirty_card_val() == 0, \"must be\");\n+\n+  __ load_byte_map_base(rscratch1);\n+\n+  if (UseCondCardMark) {\n+    Label L_already_dirty;\n+    __ ldrb(rscratch2, Address(obj, rscratch1));\n+    __ cbz(rscratch2, L_already_dirty);\n+    __ strb(zr, Address(obj, rscratch1));\n+    __ bind(L_already_dirty);\n+  } else {\n+    __ strb(zr, Address(obj, rscratch1));\n+  }\n+}\n+\n@@ -400,0 +429,3 @@\n+    if (ShenandoahCardBarrier) {\n+      store_check(masm, r3);\n+    }\n@@ -584,0 +616,29 @@\n+void ShenandoahBarrierSetAssembler::gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                                                     Register start, Register count, Register scratch, RegSet saved_regs) {\n+  assert(ShenandoahCardBarrier, \"Did you mean to enable ShenandoahCardBarrier?\");\n+\n+  Label L_loop, L_done;\n+  const Register end = count;\n+\n+  \/\/ Zero count? Nothing to do.\n+  __ cbz(count, L_done);\n+\n+  \/\/ end = start + count << LogBytesPerHeapOop\n+  \/\/ last element address to make inclusive\n+  __ lea(end, Address(start, count, Address::lsl(LogBytesPerHeapOop)));\n+  __ sub(end, end, BytesPerHeapOop);\n+  __ lsr(start, start, CardTable::card_shift());\n+  __ lsr(end, end, CardTable::card_shift());\n+\n+  \/\/ number of bytes to copy\n+  __ sub(count, end, start);\n+\n+  __ load_byte_map_base(scratch);\n+  __ add(start, start, scratch);\n+  __ bind(L_loop);\n+  __ strb(zr, Address(start, count));\n+  __ subs(count, count, 1);\n+  __ br(Assembler::GE, L_loop);\n+  __ bind(L_done);\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/shenandoahBarrierSetAssembler_aarch64.cpp","additions":61,"deletions":0,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -58,0 +59,2 @@\n+  void store_check(MacroAssembler* masm, Register obj);\n+\n@@ -62,0 +65,4 @@\n+  void gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                        Register start, Register count,\n+                                        Register scratch, RegSet saved_regs);\n+\n@@ -74,0 +81,2 @@\n+  virtual void arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                  Register start, Register count, Register tmp, RegSet saved_regs);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/shenandoahBarrierSetAssembler_aarch64.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -105,0 +105,4 @@\n+      if (ShenandoahCardBarrier) {\n+        post_barrier(access, access.resolved_addr(), new_value.result());\n+      }\n+\n@@ -109,1 +113,7 @@\n-  return BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+  LIR_Opr result = BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+\n+  if (ShenandoahCardBarrier && access.is_oop()) {\n+    post_barrier(access, access.resolved_addr(), new_value.result());\n+  }\n+\n+  return result;\n@@ -135,0 +145,4 @@\n+\n+    if (ShenandoahCardBarrier) {\n+      post_barrier(access, access.resolved_addr(), result);\n+    }\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_ppc.cpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -79,2 +80,0 @@\n-  __ block_comment(\"arraycopy_prologue (shenandoahgc) {\");\n-\n@@ -103,0 +102,1 @@\n+  __ block_comment(\"arraycopy_prologue (shenandoahgc) {\");\n@@ -176,0 +176,10 @@\n+void ShenandoahBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                                       Register dst, Register count,\n+                                                       Register preserve) {\n+  if (ShenandoahCardBarrier && is_reference_type(type)) {\n+    __ block_comment(\"arraycopy_epilogue (shenandoahgc) {\");\n+    gen_write_ref_array_post_barrier(masm, decorators, dst, count, preserve);\n+    __ block_comment(\"} arraycopy_epilogue (shenandoahgc)\");\n+  }\n+}\n+\n@@ -579,0 +589,19 @@\n+void ShenandoahBarrierSetAssembler::store_check(MacroAssembler* masm, Register base, RegisterOrConstant ind_or_offs, Register tmp) {\n+  assert(ShenandoahCardBarrier, \"Did you mean to enable ShenandoahCardBarrier?\");\n+\n+  ShenandoahBarrierSet* ctbs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = ctbs->card_table();\n+  assert_different_registers(base, tmp, R0);\n+\n+  if (ind_or_offs.is_constant()) {\n+    __ add_const_optimized(base, base, ind_or_offs.as_constant(), tmp);\n+  } else {\n+    __ add(base, ind_or_offs.as_register(), base);\n+  }\n+\n+  __ load_const_optimized(tmp, (address)ct->byte_map_base(), R0);\n+  __ srdi(base, base, CardTable::card_shift());\n+  __ li(R0, CardTable::dirty_card_val());\n+  __ stbx(R0, tmp, base);\n+}\n+\n@@ -597,0 +626,5 @@\n+\n+  \/\/ No need for post barrier if storing NULL\n+  if (ShenandoahCardBarrier && is_reference_type(type) && val != noreg) {\n+    store_check(masm, base, ind_or_offs, tmp1);\n+  }\n@@ -746,0 +780,34 @@\n+void ShenandoahBarrierSetAssembler::gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                                                     Register addr, Register count, Register preserve) {\n+  assert(ShenandoahCardBarrier, \"Did you mean to enable ShenandoahCardBarrier?\");\n+\n+  ShenandoahBarrierSet* bs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = bs->card_table();\n+  assert_different_registers(addr, count, R0);\n+\n+  Label L_skip_loop, L_store_loop;\n+\n+  __ sldi_(count, count, LogBytesPerHeapOop);\n+\n+  \/\/ Zero length? Skip.\n+  __ beq(CCR0, L_skip_loop);\n+\n+  __ addi(count, count, -BytesPerHeapOop);\n+  __ add(count, addr, count);\n+  \/\/ Use two shifts to clear out those low order two bits! (Cannot opt. into 1.)\n+  __ srdi(addr, addr, CardTable::card_shift());\n+  __ srdi(count, count, CardTable::card_shift());\n+  __ subf(count, addr, count);\n+  __ add_const_optimized(addr, addr, (address)ct->byte_map_base(), R0);\n+  __ addi(count, count, 1);\n+  __ li(R0, 0);\n+  __ mtctr(count);\n+\n+  \/\/ Byte store loop\n+  __ bind(L_store_loop);\n+  __ stb(R0, 0, addr);\n+  __ addi(addr, addr, 1);\n+  __ bdnz(L_store_loop);\n+  __ bind(L_skip_loop);\n+}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/shenandoahBarrierSetAssembler_ppc.cpp","additions":70,"deletions":2,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -54,0 +54,4 @@\n+  void store_check(MacroAssembler* masm,\n+                   Register base, RegisterOrConstant ind_or_offs,\n+                   Register tmp);\n+\n@@ -63,0 +67,4 @@\n+  void gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                        Register addr, Register count,\n+                                        Register preserve);\n+\n@@ -98,1 +106,5 @@\n-                          Register src, Register dst, Register count, Register preserve1, Register preserve2);\n+                                  Register src, Register dst, Register count,\n+                                  Register preserve1, Register preserve2);\n+  virtual void arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                  Register dst, Register count,\n+                                  Register preserve);\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/shenandoahBarrierSetAssembler_ppc.hpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -67,1 +68,1 @@\n-        __ andi(t0, t0, ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::MARKING);\n+        __ andi(t0, t0, ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n@@ -630,1 +631,1 @@\n-  __ test_bit(tmp, tmp, ShenandoahHeap::MARKING_BITPOS);\n+  __ andi(tmp, tmp, ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shenandoah\/shenandoahBarrierSetAssembler_riscv.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -87,0 +88,4 @@\n+\n+      if (ShenandoahCardBarrier) {\n+        post_barrier(access, access.resolved_addr(), new_value.result());\n+      }\n@@ -90,1 +95,8 @@\n-  return BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+\n+  LIR_Opr result = BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+\n+  if (ShenandoahCardBarrier && access.is_oop()) {\n+    post_barrier(access, access.resolved_addr(), new_value.result());\n+  }\n+\n+  return result;\n@@ -116,0 +128,3 @@\n+    if (ShenandoahCardBarrier) {\n+      post_barrier(access, access.resolved_addr(), result);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_x86.cpp","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -34,0 +35,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -123,0 +125,23 @@\n+    if (ShenandoahCardBarrier) {\n+      bool checkcast = (decorators & ARRAYCOPY_CHECKCAST) != 0;\n+      bool disjoint = (decorators & ARRAYCOPY_DISJOINT) != 0;\n+      bool obj_int = type == T_OBJECT LP64_ONLY(&& UseCompressedOops);\n+\n+      \/\/ We need to save the original element count because the array copy stub\n+      \/\/ will destroy the value and we need it for the card marking barrier.\n+#ifdef _LP64\n+      if (!checkcast) {\n+        if (!obj_int) {\n+          \/\/ Save count for barrier\n+          __ movptr(r11, count);\n+        } else if (disjoint) {\n+          \/\/ Save dst in r11 in the disjoint case\n+          __ movq(r11, dst);\n+        }\n+      }\n+#else\n+      if (disjoint) {\n+        __ mov(rdx, dst);          \/\/ save 'to'\n+      }\n+#endif\n+    }\n@@ -143,1 +168,1 @@\n-      Label done;\n+      Label L_done;\n@@ -146,1 +171,1 @@\n-      __ jcc(Assembler::zero, done);\n+      __ jcc(Assembler::zero, L_done);\n@@ -157,1 +182,1 @@\n-      __ jcc(Assembler::zero, done);\n+      __ jcc(Assembler::zero, L_done);\n@@ -177,1 +202,1 @@\n-      __ bind(done);\n+      __ bind(L_done);\n@@ -184,0 +209,30 @@\n+void ShenandoahBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                                       Register src, Register dst, Register count) {\n+\n+  if (ShenandoahCardBarrier && is_reference_type(type)) {\n+    bool checkcast = (decorators & ARRAYCOPY_CHECKCAST) != 0;\n+    bool disjoint = (decorators & ARRAYCOPY_DISJOINT) != 0;\n+    bool obj_int = type == T_OBJECT LP64_ONLY(&& UseCompressedOops);\n+    Register tmp = rax;\n+\n+#ifdef _LP64\n+    if (!checkcast) {\n+      if (!obj_int) {\n+        \/\/ Save count for barrier\n+        count = r11;\n+      } else if (disjoint) {\n+        \/\/ Use the saved dst in the disjoint case\n+        dst = r11;\n+      }\n+    } else {\n+      tmp = rscratch1;\n+    }\n+#else\n+    if (disjoint) {\n+      __ mov(dst, rdx); \/\/ restore 'to'\n+    }\n+#endif\n+    gen_write_ref_array_post_barrier(masm, decorators, dst, count, tmp);\n+  }\n+}\n+\n@@ -558,0 +613,43 @@\n+void ShenandoahBarrierSetAssembler::store_check(MacroAssembler* masm, Register obj) {\n+  assert(ShenandoahCardBarrier, \"Did you mean to enable ShenandoahCardBarrier?\");\n+\n+  \/\/ Does a store check for the oop in register obj. The content of\n+  \/\/ register obj is destroyed afterwards.\n+\n+  ShenandoahBarrierSet* ctbs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = ctbs->card_table();\n+\n+  __ shrptr(obj, CardTable::card_shift());\n+\n+  Address card_addr;\n+\n+  \/\/ The calculation for byte_map_base is as follows:\n+  \/\/ byte_map_base = _byte_map - (uintptr_t(low_bound) >> card_shift);\n+  \/\/ So this essentially converts an address to a displacement and it will\n+  \/\/ never need to be relocated. On 64-bit however the value may be too\n+  \/\/ large for a 32-bit displacement.\n+  intptr_t byte_map_base = (intptr_t)ct->byte_map_base();\n+  if (__ is_simm32(byte_map_base)) {\n+    card_addr = Address(noreg, obj, Address::times_1, byte_map_base);\n+  } else {\n+    \/\/ By doing it as an ExternalAddress 'byte_map_base' could be converted to a rip-relative\n+    \/\/ displacement and done in a single instruction given favorable mapping and a\n+    \/\/ smarter version of as_Address. However, 'ExternalAddress' generates a relocation\n+    \/\/ entry and that entry is not properly handled by the relocation code.\n+    AddressLiteral cardtable((address)byte_map_base, relocInfo::none);\n+    Address index(noreg, obj, Address::times_1);\n+    card_addr = __ as_Address(ArrayAddress(cardtable, index), rscratch1);\n+  }\n+\n+  int dirty = CardTable::dirty_card_val();\n+  if (UseCondCardMark) {\n+    Label L_already_dirty;\n+    __ cmpb(card_addr, dirty);\n+    __ jccb(Assembler::equal, L_already_dirty);\n+    __ movb(card_addr, dirty);\n+    __ bind(L_already_dirty);\n+  } else {\n+    __ movb(card_addr, dirty);\n+  }\n+}\n+\n@@ -595,0 +693,1 @@\n+\n@@ -596,0 +695,5 @@\n+    if (val != noreg) {\n+      if (ShenandoahCardBarrier) {\n+        store_check(masm, tmp1);\n+      }\n+    }\n@@ -790,0 +894,57 @@\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#else\n+#define BLOCK_COMMENT(str) __ block_comment(str)\n+#endif\n+\n+#define BIND(label) bind(label); BLOCK_COMMENT(#label \":\")\n+\n+#define TIMES_OOP (UseCompressedOops ? Address::times_4 : Address::times_8)\n+\n+void ShenandoahBarrierSetAssembler::gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                                                     Register addr, Register count,\n+                                                                     Register tmp) {\n+  assert(ShenandoahCardBarrier, \"Did you mean to enable ShenandoahCardBarrier?\");\n+\n+  ShenandoahBarrierSet* bs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = bs->card_table();\n+  intptr_t disp = (intptr_t) ct->byte_map_base();\n+\n+  Label L_loop, L_done;\n+  const Register end = count;\n+  assert_different_registers(addr, end);\n+\n+  \/\/ Zero count? Nothing to do.\n+  __ testl(count, count);\n+  __ jccb(Assembler::zero, L_done);\n+\n+#ifdef _LP64\n+  __ leaq(end, Address(addr, count, TIMES_OOP, 0));  \/\/ end == addr+count*oop_size\n+  __ subptr(end, BytesPerHeapOop); \/\/ end - 1 to make inclusive\n+  __ shrptr(addr, CardTable::card_shift());\n+  __ shrptr(end, CardTable::card_shift());\n+  __ subptr(end, addr); \/\/ end --> cards count\n+\n+  __ mov64(tmp, disp);\n+  __ addptr(addr, tmp);\n+\n+  __ BIND(L_loop);\n+  __ movb(Address(addr, count, Address::times_1), 0);\n+  __ decrement(count);\n+  __ jccb(Assembler::greaterEqual, L_loop);\n+#else\n+  __ lea(end, Address(addr, count, Address::times_ptr, -wordSize));\n+  __ shrptr(addr, CardTable::card_shift());\n+  __ shrptr(end,  CardTable::card_shift());\n+  __ subptr(end, addr); \/\/ end --> count\n+\n+  __ BIND(L_loop);\n+  Address cardtable(addr, count, Address::times_1, disp);\n+  __ movb(cardtable, 0);\n+  __ decrement(count);\n+  __ jccb(Assembler::greaterEqual, L_loop);\n+#endif\n+\n+  __ BIND(L_done);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.cpp","additions":165,"deletions":4,"binary":false,"changes":169,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -59,0 +60,6 @@\n+  void store_check(MacroAssembler* masm, Register obj);\n+\n+  void gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                        Register addr, Register count,\n+                                        Register tmp);\n+\n@@ -74,0 +81,2 @@\n+  virtual void arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                  Register src, Register dst, Register count);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -28,0 +29,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -193,0 +195,10 @@\n+\n+  if (ShenandoahCardBarrier && access.is_oop()) {\n+    DecoratorSet decorators = access.decorators();\n+    bool is_array = (decorators & IS_ARRAY) != 0;\n+    bool on_anonymous = (decorators & ON_UNKNOWN_OOP_REF) != 0;\n+\n+    bool precise = is_array || on_anonymous;\n+    LIR_Opr post_addr = precise ? access.resolved_addr() : access.base().opr();\n+    post_barrier(access, post_addr, value);\n+  }\n@@ -291,0 +303,59 @@\n+\n+void ShenandoahBarrierSetC1::post_barrier(LIRAccess& access, LIR_Opr addr, LIR_Opr new_val) {\n+  assert(ShenandoahCardBarrier, \"Did you mean to enable ShenandoahCardBarrier?\");\n+\n+  DecoratorSet decorators = access.decorators();\n+  LIRGenerator* gen = access.gen();\n+  bool in_heap = (decorators & IN_HEAP) != 0;\n+  if (!in_heap) {\n+    return;\n+  }\n+\n+  BarrierSet* bs = BarrierSet::barrier_set();\n+  ShenandoahBarrierSet* ctbs = barrier_set_cast<ShenandoahBarrierSet>(bs);\n+  CardTable* ct = ctbs->card_table();\n+  LIR_Const* card_table_base = new LIR_Const(ct->byte_map_base());\n+  if (addr->is_address()) {\n+    LIR_Address* address = addr->as_address_ptr();\n+    \/\/ ptr cannot be an object because we use this barrier for array card marks\n+    \/\/ and addr can point in the middle of an array.\n+    LIR_Opr ptr = gen->new_pointer_register();\n+    if (!address->index()->is_valid() && address->disp() == 0) {\n+      __ move(address->base(), ptr);\n+    } else {\n+      assert(address->disp() != max_jint, \"lea doesn't support patched addresses!\");\n+      __ leal(addr, ptr);\n+    }\n+    addr = ptr;\n+  }\n+  assert(addr->is_register(), \"must be a register at this point\");\n+\n+  LIR_Opr tmp = gen->new_pointer_register();\n+  if (two_operand_lir_form) {\n+    __ move(addr, tmp);\n+    __ unsigned_shift_right(tmp, CardTable::card_shift(), tmp);\n+  } else {\n+    __ unsigned_shift_right(addr, CardTable::card_shift(), tmp);\n+  }\n+\n+  LIR_Address* card_addr;\n+  if (gen->can_inline_as_constant(card_table_base)) {\n+    card_addr = new LIR_Address(tmp, card_table_base->as_jint(), T_BYTE);\n+  } else {\n+    card_addr = new LIR_Address(tmp, gen->load_constant(card_table_base), T_BYTE);\n+  }\n+\n+  LIR_Opr dirty = LIR_OprFact::intConst(CardTable::dirty_card_val());\n+  if (UseCondCardMark) {\n+    LIR_Opr cur_value = gen->new_register(T_INT);\n+    __ move(card_addr, cur_value);\n+\n+    LabelObj* L_already_dirty = new LabelObj();\n+    __ cmp(lir_cond_equal, cur_value, dirty);\n+    __ branch(lir_cond_equal, L_already_dirty->label());\n+    __ move(dirty, card_addr);\n+    __ branch_destination(L_already_dirty->label());\n+  } else {\n+    __ move(dirty, card_addr);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1.cpp","additions":71,"deletions":0,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -246,0 +246,2 @@\n+  void post_barrier(LIRAccess& access, LIR_Opr addr, LIR_Opr new_val);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -36,0 +37,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -428,0 +430,84 @@\n+Node* ShenandoahBarrierSetC2::byte_map_base_node(GraphKit* kit) const {\n+  BarrierSet* bs = BarrierSet::barrier_set();\n+  ShenandoahBarrierSet* ctbs = barrier_set_cast<ShenandoahBarrierSet>(bs);\n+  CardTable::CardValue* card_table_base = ctbs->card_table()->byte_map_base();\n+  if (card_table_base != nullptr) {\n+    return kit->makecon(TypeRawPtr::make((address)card_table_base));\n+  } else {\n+    return kit->null();\n+  }\n+}\n+\n+void ShenandoahBarrierSetC2::post_barrier(GraphKit* kit,\n+                                          Node* ctl,\n+                                          Node* oop_store,\n+                                          Node* obj,\n+                                          Node* adr,\n+                                          uint  adr_idx,\n+                                          Node* val,\n+                                          BasicType bt,\n+                                          bool use_precise) const {\n+  assert(ShenandoahCardBarrier, \"Did you mean to enable ShenandoahCardBarrier?\");\n+\n+  \/\/ No store check needed if we're storing a null.\n+  if (val != nullptr && val->is_Con()) {\n+    \/\/ must be either an oop or NULL\n+    const Type* t = val->bottom_type();\n+    if (t == TypePtr::NULL_PTR || t == Type::TOP)\n+      return;\n+  }\n+\n+  if (ReduceInitialCardMarks && obj == kit->just_allocated_object(kit->control())) {\n+    \/\/ We can skip marks on a freshly-allocated object in Eden.\n+    \/\/ Keep this code in sync with new_deferred_store_barrier() in runtime.cpp.\n+    \/\/ That routine informs GC to take appropriate compensating steps,\n+    \/\/ upon a slow-path allocation, so as to make this card-mark\n+    \/\/ elision safe.\n+    return;\n+  }\n+\n+  if (!use_precise) {\n+    \/\/ All card marks for a (non-array) instance are in one place:\n+    adr = obj;\n+  }\n+  \/\/ (Else it's an array (or unknown), and we want more precise card marks.)\n+  assert(adr != nullptr, \"\");\n+\n+  IdealKit ideal(kit, true);\n+\n+  \/\/ Convert the pointer to an int prior to doing math on it\n+  Node* cast = __ CastPX(__ ctrl(), adr);\n+\n+  \/\/ Divide by card size\n+  Node* card_offset = __ URShiftX( cast, __ ConI(CardTable::card_shift()) );\n+\n+  \/\/ Combine card table base and card offset\n+  Node* card_adr = __ AddP(__ top(), byte_map_base_node(kit), card_offset );\n+\n+  \/\/ Get the alias_index for raw card-mark memory\n+  int adr_type = Compile::AliasIdxRaw;\n+  Node*   zero = __ ConI(0); \/\/ Dirty card value\n+\n+  if (UseCondCardMark) {\n+    \/\/ The classic GC reference write barrier is typically implemented\n+    \/\/ as a store into the global card mark table.  Unfortunately\n+    \/\/ unconditional stores can result in false sharing and excessive\n+    \/\/ coherence traffic as well as false transactional aborts.\n+    \/\/ UseCondCardMark enables MP \"polite\" conditional card mark\n+    \/\/ stores.  In theory we could relax the load from ctrl() to\n+    \/\/ no_ctrl, but that doesn't buy much latitude.\n+    Node* card_val = __ load( __ ctrl(), card_adr, TypeInt::BYTE, T_BYTE, adr_type);\n+    __ if_then(card_val, BoolTest::ne, zero);\n+  }\n+\n+  \/\/ Smash zero into card\n+  __ store(__ ctrl(), card_adr, zero, T_BYTE, adr_type, MemNode::unordered);\n+\n+  if (UseCondCardMark) {\n+    __ end_if();\n+  }\n+\n+  \/\/ Final sync IdealKit and GraphKit.\n+  kit->final_sync(ideal);\n+}\n+\n@@ -488,0 +574,15 @@\n+\n+    Node* result = BarrierSetC2::store_at_resolved(access, val);\n+\n+    if (ShenandoahCardBarrier) {\n+      const bool anonymous = (decorators & ON_UNKNOWN_OOP_REF) != 0;\n+      const bool is_array = (decorators & IS_ARRAY) != 0;\n+      const bool use_precise = is_array || anonymous;\n+      post_barrier(kit, kit->control(), access.raw_access(), access.base(),\n+                   adr, adr_idx, val.node(), access.type(), use_precise);\n+    }\n+    return result;\n+  } else {\n+    assert(access.is_opt_access(), \"only for optimization passes\");\n+    assert(((decorators & C2_TIGHTLY_COUPLED_ALLOC) != 0 || !ShenandoahSATBBarrier) && (decorators & C2_ARRAY_COPY) != 0, \"unexpected caller of this code\");\n+    return BarrierSetC2::store_at_resolved(access, val);\n@@ -489,1 +590,0 @@\n-  return BarrierSetC2::store_at_resolved(access, val);\n@@ -560,1 +660,1 @@\n-                                                   Node* new_val, const Type* value_type) const {\n+                                                             Node* new_val, const Type* value_type) const {\n@@ -601,0 +701,4 @@\n+    if (ShenandoahCardBarrier) {\n+      post_barrier(kit, kit->control(), access.raw_access(), access.base(),\n+                   access.addr().node(), access.alias_idx(), new_val, T_OBJECT, true);\n+    }\n@@ -655,0 +759,4 @@\n+    if (ShenandoahCardBarrier) {\n+      post_barrier(kit, kit->control(), access.raw_access(), access.base(),\n+                   access.addr().node(), access.alias_idx(), new_val, T_OBJECT, true);\n+    }\n@@ -668,0 +776,4 @@\n+    if (ShenandoahCardBarrier) {\n+      post_barrier(kit, kit->control(), access.raw_access(), access.base(),\n+                   access.addr().node(), access.alias_idx(), val, T_OBJECT, true);\n+    }\n@@ -850,3 +962,19 @@\n-void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* n) const {\n-  if (is_shenandoah_wb_pre_call(n)) {\n-    shenandoah_eliminate_wb_pre(n, &macro->igvn());\n+void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+  if (is_shenandoah_wb_pre_call(node)) {\n+    shenandoah_eliminate_wb_pre(node, &macro->igvn());\n+  }\n+  if (ShenandoahCardBarrier && node->Opcode() == Op_CastP2X) {\n+    Node* shift = node->unique_out();\n+    Node* addp = shift->unique_out();\n+    for (DUIterator_Last jmin, j = addp->last_outs(jmin); j >= jmin; --j) {\n+      Node* mem = addp->last_out(j);\n+      if (UseCondCardMark && mem->is_Load()) {\n+        assert(mem->Opcode() == Op_LoadB, \"unexpected code shape\");\n+        \/\/ The load is checking if the card has been written so\n+        \/\/ replace it with zero to fold the test.\n+        macro->replace_node(mem, macro->intcon(0));\n+        continue;\n+      }\n+      assert(mem->is_Store(), \"store required\");\n+      macro->replace_node(mem, mem->in(MemNode::Memory));\n+    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.cpp","additions":133,"deletions":5,"binary":false,"changes":138,"status":"modified"},{"patch":"@@ -70,0 +70,12 @@\n+  Node* byte_map_base_node(GraphKit* kit) const;\n+\n+  void post_barrier(GraphKit* kit,\n+                    Node* ctl,\n+                    Node* store,\n+                    Node* obj,\n+                    Node* adr,\n+                    uint adr_idx,\n+                    Node* val,\n+                    BasicType bt,\n+                    bool use_precise) const;\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -0,0 +1,64 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logTag.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n+\n+void ShenandoahGenerationalMode::initialize_flags() const {\n+\n+#if !(defined AARCH64 || defined AMD64 || defined IA32 || defined PPC64)\n+  vm_exit_during_initialization(\"Shenandoah Generational GC is not supported on this platform.\");\n+#endif\n+\n+  \/\/ Exit if the user has asked ShenandoahCardBarrier to be disabled\n+  if (!FLAG_IS_DEFAULT(ShenandoahCardBarrier)) {\n+    SHENANDOAH_CHECK_FLAG_SET(ShenandoahCardBarrier);\n+  }\n+\n+  \/\/ Enable card-marking post-write barrier for tracking old to young pointers\n+  FLAG_SET_DEFAULT(ShenandoahCardBarrier, true);\n+\n+  if (ClassUnloading) {\n+    FLAG_SET_DEFAULT(VerifyBeforeExit, false);\n+  }\n+\n+  SHENANDOAH_ERGO_OVERRIDE_DEFAULT(GCTimeRatio, 70);\n+  SHENANDOAH_ERGO_ENABLE_FLAG(ExplicitGCInvokesConcurrent);\n+  SHENANDOAH_ERGO_ENABLE_FLAG(ShenandoahImplicitGCInvokesConcurrent);\n+\n+  \/\/ This helps most multi-core hardware hosts, enable by default\n+  SHENANDOAH_ERGO_ENABLE_FLAG(UseCondCardMark);\n+\n+  \/\/ Final configuration checks\n+  SHENANDOAH_CHECK_FLAG_SET(ShenandoahLoadRefBarrier);\n+  SHENANDOAH_CHECK_FLAG_SET(ShenandoahSATBBarrier);\n+  SHENANDOAH_CHECK_FLAG_SET(ShenandoahCASBarrier);\n+  SHENANDOAH_CHECK_FLAG_SET(ShenandoahCloneBarrier);\n+  SHENANDOAH_CHECK_FLAG_SET(ShenandoahCardBarrier);\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahGenerationalMode.cpp","additions":64,"deletions":0,"binary":false,"changes":64,"status":"added"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahSpaceInfo.hpp\"\n@@ -31,1 +33,0 @@\n-#include \"runtime\/globals_extension.hpp\"\n@@ -53,0 +54,1 @@\n+  SHENANDOAH_ERGO_DISABLE_FLAG(ShenandoahCardBarrier);\n@@ -57,1 +59,2 @@\n-ShenandoahHeuristics* ShenandoahPassiveMode::initialize_heuristics() const {\n+\n+ShenandoahHeuristics* ShenandoahPassiveMode::initialize_heuristics(ShenandoahSpaceInfo* space_info) const {\n@@ -61,1 +64,1 @@\n-  return new ShenandoahPassiveHeuristics(ShenandoahHeap::heap());\n+  return new ShenandoahPassiveHeuristics(space_info);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahPassiveMode.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -26,4 +26,1 @@\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n@@ -31,1 +28,0 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n@@ -51,18 +47,2 @@\n-}\n-\n-ShenandoahHeuristics* ShenandoahSATBMode::initialize_heuristics() const {\n-  if (ShenandoahGCHeuristics == nullptr) {\n-    vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option (null)\");\n-  }\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  if (strcmp(ShenandoahGCHeuristics, \"aggressive\") == 0) {\n-    return new ShenandoahAggressiveHeuristics(heap);\n-  } else if (strcmp(ShenandoahGCHeuristics, \"static\") == 0) {\n-    return new ShenandoahStaticHeuristics(heap);\n-  } else if (strcmp(ShenandoahGCHeuristics, \"adaptive\") == 0) {\n-    return new ShenandoahAdaptiveHeuristics(heap);\n-  } else if (strcmp(ShenandoahGCHeuristics, \"compact\") == 0) {\n-    return new ShenandoahCompactHeuristics(heap);\n-  }\n-  vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option\");\n-  return nullptr;\n+  assert(strcmp(ShenandoahGCMode, \"generational\") != 0, \"Error\");\n+  SHENANDOAH_CHECK_FLAG_UNSET(ShenandoahCardBarrier);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahSATBMode.cpp","additions":3,"deletions":23,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -31,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n@@ -51,0 +53,1 @@\n+  FLAG_SET_DEFAULT(ShenandoahCardBarrier,            false);\n@@ -71,0 +74,7 @@\n+  \/\/ We use this as the time period for tracking minimum mutator utilization (MMU).\n+  \/\/ In generational mode, the MMU is used as a signal to adjust the size of the\n+  \/\/ young generation.\n+  if (FLAG_IS_DEFAULT(GCPauseIntervalMillis)) {\n+    FLAG_SET_DEFAULT(GCPauseIntervalMillis, 5000);\n+  }\n+\n@@ -189,0 +199,2 @@\n+  CardTable::initialize_card_size();\n+\n@@ -202,1 +214,6 @@\n-  return new ShenandoahHeap(new ShenandoahCollectorPolicy());\n+  if (strcmp(ShenandoahGCMode, \"generational\") != 0) {\n+    \/\/ Not generational\n+    return new ShenandoahHeap(new ShenandoahCollectorPolicy());\n+  } else {\n+    return new ShenandoahGenerationalHeap(new ShenandoahCollectorPolicy());\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahArguments.cpp","additions":18,"deletions":1,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -33,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -44,1 +46,1 @@\n-ShenandoahBarrierSet::ShenandoahBarrierSet(ShenandoahHeap* heap) :\n+ShenandoahBarrierSet::ShenandoahBarrierSet(ShenandoahHeap* heap, MemRegion heap_region) :\n@@ -55,0 +57,4 @@\n+  if (ShenandoahCardBarrier) {\n+    _card_table = new ShenandoahCardTable(heap_region);\n+    _card_table->initialize();\n+  }\n@@ -127,0 +133,10 @@\n+    if (ShenandoahCardBarrier) {\n+      PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+      \/\/ retire_plab may register the remnant filler object with the remembered set scanner without a lock.\n+      \/\/ This is safe because it is assured that each PLAB is a whole-number multiple of card-mark memory size and each\n+      \/\/ PLAB is aligned with the start of each card's memory range.\n+      if (plab != nullptr) {\n+        ShenandoahGenerationalHeap::heap()->retire_plab(plab);\n+      }\n+    }\n+\n@@ -145,0 +161,19 @@\n+\n+void ShenandoahBarrierSet::write_ref_array(HeapWord* start, size_t count) {\n+  assert(ShenandoahCardBarrier, \"Did you mean to enable ShenandoahCardBarrier?\");\n+\n+  HeapWord* end = (HeapWord*)((char*) start + (count * heapOopSize));\n+  \/\/ In the case of compressed oops, start and end may potentially be misaligned;\n+  \/\/ so we need to conservatively align the first downward (this is not\n+  \/\/ strictly necessary for current uses, but a case of good hygiene and,\n+  \/\/ if you will, aesthetics) and the second upward (this is essential for\n+  \/\/ current uses) to a HeapWord boundary, so we mark all cards overlapping\n+  \/\/ this write.\n+  HeapWord* aligned_start = align_down(start, HeapWordSize);\n+  HeapWord* aligned_end   = align_up  (end,   HeapWordSize);\n+  \/\/ If compressed oops were not being used, these should already be aligned\n+  assert(UseCompressedOops || (aligned_start == start && aligned_end == end),\n+         \"Expected heap word alignment of start and end\");\n+  _heap->old_generation()->card_scan()->mark_range_as_dirty(aligned_start, (aligned_end - aligned_start));\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.cpp","additions":36,"deletions":1,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -30,0 +31,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -38,0 +40,1 @@\n+  ShenandoahCardTable* _card_table;\n@@ -42,1 +45,1 @@\n-  ShenandoahBarrierSet(ShenandoahHeap* heap);\n+  ShenandoahBarrierSet(ShenandoahHeap* heap, MemRegion heap_region);\n@@ -50,0 +53,4 @@\n+  inline ShenandoahCardTable* card_table() {\n+    return _card_table;\n+  }\n+\n@@ -114,0 +121,5 @@\n+  template <DecoratorSet decorators, typename T>\n+  void write_ref_field_post(T* field);\n+\n+  void write_ref_array(HeapWord* start, size_t count);\n+\n@@ -116,1 +128,1 @@\n-  inline void arraycopy_marking(T* src, T* dst, size_t count);\n+  inline void arraycopy_marking(T* src, T* dst, size_t count, bool is_old_marking);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.hpp","additions":14,"deletions":2,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -31,0 +32,1 @@\n+#include \"gc\/shared\/cardTable.hpp\"\n@@ -35,0 +37,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -39,0 +42,2 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n@@ -106,0 +111,1 @@\n+      _heap->is_in_active_generation(obj) &&\n@@ -113,0 +119,1 @@\n+      _heap->is_in_active_generation(obj) &&\n@@ -176,0 +183,7 @@\n+template <DecoratorSet decorators, typename T>\n+inline void ShenandoahBarrierSet::write_ref_field_post(T* field) {\n+  assert(ShenandoahCardBarrier, \"Did you mean to enable ShenandoahCardBarrier?\");\n+  volatile CardTable::CardValue* byte = card_table()->byte_for(field);\n+  *byte = CardTable::dirty_card_val();\n+}\n+\n@@ -237,1 +251,5 @@\n-  shenandoah_assert_marked_if(nullptr, value, !CompressedOops::is_null(value) && ShenandoahHeap::heap()->is_evacuation_in_progress());\n+  shenandoah_assert_marked_if(nullptr, value,\n+                              !CompressedOops::is_null(value) &&\n+                              ShenandoahHeap::heap()->is_evacuation_in_progress() &&\n+                              !(ShenandoahHeap::heap()->active_generation()->is_young() &&\n+                              ShenandoahHeap::heap()->heap_region_containing(value)->is_old()));\n@@ -257,0 +275,4 @@\n+  if (ShenandoahCardBarrier) {\n+    ShenandoahBarrierSet* bs = ShenandoahBarrierSet::barrier_set();\n+    bs->write_ref_field_post<decorators>(addr);\n+  }\n@@ -277,1 +299,5 @@\n-  return bs->oop_cmpxchg(decorators, addr, compare_value, new_value);\n+  oop result = bs->oop_cmpxchg(decorators, addr, compare_value, new_value);\n+  if (ShenandoahCardBarrier) {\n+    bs->write_ref_field_post<decorators>(addr);\n+  }\n+  return result;\n@@ -285,1 +311,6 @@\n-  return bs->oop_cmpxchg(resolved_decorators, AccessInternal::oop_field_addr<decorators>(base, offset), compare_value, new_value);\n+  auto addr = AccessInternal::oop_field_addr<decorators>(base, offset);\n+  oop result = bs->oop_cmpxchg(resolved_decorators, addr, compare_value, new_value);\n+  if (ShenandoahCardBarrier) {\n+    bs->write_ref_field_post<decorators>(addr);\n+  }\n+  return result;\n@@ -301,1 +332,5 @@\n-  return bs->oop_xchg(decorators, addr, new_value);\n+  oop result = bs->oop_xchg(decorators, addr, new_value);\n+  if (ShenandoahCardBarrier) {\n+    bs->write_ref_field_post<decorators>(addr);\n+  }\n+  return result;\n@@ -309,1 +344,6 @@\n-  return bs->oop_xchg(resolved_decorators, AccessInternal::oop_field_addr<decorators>(base, offset), new_value);\n+  auto addr = AccessInternal::oop_field_addr<decorators>(base, offset);\n+  oop result = bs->oop_xchg(resolved_decorators, addr, new_value);\n+  if (ShenandoahCardBarrier) {\n+    bs->write_ref_field_post<decorators>(addr);\n+  }\n+  return result;\n@@ -326,0 +366,3 @@\n+  T* src = arrayOopDesc::obj_offset_to_raw(src_obj, src_offset_in_bytes, src_raw);\n+  T* dst = arrayOopDesc::obj_offset_to_raw(dst_obj, dst_offset_in_bytes, dst_raw);\n+\n@@ -327,4 +370,6 @@\n-  bs->arraycopy_barrier(arrayOopDesc::obj_offset_to_raw(src_obj, src_offset_in_bytes, src_raw),\n-                        arrayOopDesc::obj_offset_to_raw(dst_obj, dst_offset_in_bytes, dst_raw),\n-                        length);\n-  return Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);\n+  bs->arraycopy_barrier(src, dst, length);\n+  bool result = Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);\n+  if (ShenandoahCardBarrier) {\n+    bs->write_ref_array((HeapWord*) dst, length);\n+  }\n+  return result;\n@@ -335,1 +380,3 @@\n-  assert(HAS_FWD == _heap->has_forwarded_objects(), \"Forwarded object status is sane\");\n+  \/\/ We allow forwarding in young generation and marking in old generation\n+  \/\/ to happen simultaneously.\n+  assert(_heap->mode()->is_generational() || HAS_FWD == _heap->has_forwarded_objects(), \"Forwarded object status is sane\");\n@@ -355,1 +402,1 @@\n-      if (ENQUEUE && !ctx->is_marked_strong(obj)) {\n+      if (ENQUEUE && !ctx->is_marked_strong_or_old(obj)) {\n@@ -368,3 +415,6 @@\n-  if ((gc_state & ShenandoahHeap::MARKING) != 0) {\n-    arraycopy_marking(src, dst, count);\n-  } else if ((gc_state & ShenandoahHeap::EVACUATION) != 0) {\n+  if ((gc_state & ShenandoahHeap::YOUNG_MARKING) != 0) {\n+    arraycopy_marking(src, dst, count, false);\n+    return;\n+  }\n+\n+  if ((gc_state & ShenandoahHeap::EVACUATION) != 0) {\n@@ -375,0 +425,11 @@\n+\n+  if (_heap->mode()->is_generational()) {\n+    assert(ShenandoahSATBBarrier, \"Generational mode assumes SATB mode\");\n+    \/\/ TODO: Could we optimize here by checking that dst is in an old region?\n+    if ((gc_state & ShenandoahHeap::OLD_MARKING) != 0) {\n+      \/\/ Note that we can't do the arraycopy marking using the 'src' array when\n+      \/\/ SATB mode is enabled (so we can't do this as part of the iteration for\n+      \/\/ evacuation or update references).\n+      arraycopy_marking(src, dst, count, true);\n+    }\n+  }\n@@ -378,1 +439,1 @@\n-void ShenandoahBarrierSet::arraycopy_marking(T* src, T* dst, size_t count) {\n+void ShenandoahBarrierSet::arraycopy_marking(T* src, T* dst, size_t count, bool is_old_marking) {\n@@ -380,3 +441,48 @@\n-  T* array = ShenandoahSATBBarrier ? dst : src;\n-  if (!_heap->marking_context()->allocated_after_mark_start(reinterpret_cast<HeapWord*>(array))) {\n-    arraycopy_work<T, false, false, true>(array, count);\n+  \/*\n+   * Note that an old-gen object is considered live if it is live at the start of OLD marking or if it is promoted\n+   * following the start of OLD marking.\n+   *\n+   * 1. Every object promoted following the start of OLD marking will be above TAMS within its old-gen region\n+   * 2. Every object live at the start of OLD marking will be referenced from a \"root\" or it will be referenced from\n+   *    another live OLD-gen object.  With regards to old-gen, roots include stack locations and all of live young-gen.\n+   *    All root references to old-gen are identified during a bootstrap young collection.  All references from other\n+   *    old-gen objects will be marked during the traversal of all old objects, or will be marked by the SATB barrier.\n+   *\n+   * During old-gen marking (which is interleaved with young-gen collections), call arraycopy_work() if:\n+   *\n+   * 1. The overwritten array resides in old-gen and it is below TAMS within its old-gen region\n+   * 2. Do not call arraycopy_work for any array residing in young-gen because young-gen collection is idle at this time\n+   *\n+   * During young-gen marking, call arraycopy_work() if:\n+   *\n+   * 1. The overwritten array resides in young-gen and is below TAMS within its young-gen region\n+   * 2. Additionally, if array resides in old-gen, regardless of its relationship to TAMS because this old-gen array\n+   *    may hold references to young-gen\n+   *\/\n+  if (ShenandoahSATBBarrier) {\n+    T* array = dst;\n+    HeapWord* array_addr = reinterpret_cast<HeapWord*>(array);\n+    ShenandoahHeapRegion* r = _heap->heap_region_containing(array_addr);\n+    if (is_old_marking) {\n+      \/\/ Generational, old marking\n+      assert(_heap->mode()->is_generational(), \"Invariant\");\n+      if (r->is_old() && (array_addr < _heap->marking_context()->top_at_mark_start(r))) {\n+        arraycopy_work<T, false, false, true>(array, count);\n+      }\n+    } else if (_heap->mode()->is_generational()) {\n+      \/\/ Generational, young marking\n+      if (r->is_old() || (array_addr < _heap->marking_context()->top_at_mark_start(r))) {\n+        arraycopy_work<T, false, false, true>(array, count);\n+      }\n+    } else if (array_addr < _heap->marking_context()->top_at_mark_start(r)) {\n+      \/\/ Non-generational, marking\n+      arraycopy_work<T, false, false, true>(array, count);\n+    }\n+  } else {\n+    \/\/ Incremental Update mode, marking\n+    T* array = src;\n+    HeapWord* array_addr = reinterpret_cast<HeapWord*>(array);\n+    ShenandoahHeapRegion* r = _heap->heap_region_containing(array_addr);\n+    if (array_addr < _heap->marking_context()->top_at_mark_start(r)) {\n+      arraycopy_work<T, false, false, true>(array, count);\n+    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":124,"deletions":18,"binary":false,"changes":142,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -35,0 +36,4 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -89,2 +94,2 @@\n-ShenandoahConcurrentGC::ShenandoahConcurrentGC() :\n-  _mark(),\n+ShenandoahConcurrentGC::ShenandoahConcurrentGC(ShenandoahGeneration* generation, bool do_old_gc_bootstrap) :\n+  _mark(generation),\n@@ -92,1 +97,3 @@\n-  _abbreviated(false) {\n+  _abbreviated(false),\n+  _do_old_gc_bootstrap(do_old_gc_bootstrap),\n+  _generation(generation) {\n@@ -99,4 +106,0 @@\n-void ShenandoahConcurrentGC::cancel() {\n-  ShenandoahConcurrentMark::cancel();\n-}\n-\n@@ -105,0 +108,1 @@\n+\n@@ -115,0 +119,10 @@\n+\n+    \/\/ Reset task queue stats here, rather than in mark_concurrent_roots,\n+    \/\/ because remembered set scan will `push` oops into the queues and\n+    \/\/ resetting after this happens will lose those counts.\n+    TASKQUEUE_STATS_ONLY(_mark.task_queues()->reset_taskqueue_stats());\n+\n+    \/\/ Concurrent remembered set scanning\n+    entry_scan_remembered_set();\n+    \/\/ TODO: When RS scanning yields, we will need a check_cancellation_and_abort() degeneration point here.\n+\n@@ -117,1 +131,1 @@\n-    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_outside_cycle)) {\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_roots)) {\n@@ -135,1 +149,1 @@\n-  if (heap->is_concurrent_mark_in_progress()) {\n+  if (_generation->is_concurrent_mark_in_progress()) {\n@@ -153,1 +167,2 @@\n-  \/\/ the space. This would be the last action if there is nothing to evacuate.\n+  \/\/ the space. This would be the last action if there is nothing to evacuate.  Note that\n+  \/\/ we will not age young-gen objects in the case that we skip evacuation.\n@@ -180,0 +195,1 @@\n+  }\n@@ -181,0 +197,1 @@\n+  if (heap->has_forwarded_objects()) {\n@@ -199,0 +216,4 @@\n+    \/\/ We chose not to evacuate because we found sufficient immediate garbage. Note that we\n+    \/\/ do not check for cancellation here because, at this point, the cycle is effectively\n+    \/\/ complete. If the cycle has been cancelled here, the control thread will detect it\n+    \/\/ on its next iteration and run a degenerated young cycle.\n@@ -203,0 +224,5 @@\n+  \/\/ We defer generation resizing actions until after cset regions have been recycled.  We do this even following an\n+  \/\/ abbreviated cycle.\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahGenerationalHeap::heap()->complete_concurrent_cycle();\n+  }\n@@ -312,0 +338,2 @@\n+  heap->try_inject_alloc_failure();\n+\n@@ -313,3 +341,10 @@\n-  static const char* msg = \"Concurrent reset\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset);\n-  EventMark em(\"%s\", msg);\n+  {\n+    static const char* msg = \"Concurrent reset\";\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset);\n+    EventMark em(\"%s\", msg);\n+\n+    ShenandoahWorkerScope scope(heap->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n+                                msg);\n+    op_reset();\n+  }\n@@ -317,3 +352,7 @@\n-  ShenandoahWorkerScope scope(heap->workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n-                              \"concurrent reset\");\n+  if (_do_old_gc_bootstrap) {\n+    static const char* msg = \"Concurrent reset (OLD)\";\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset_old);\n+    ShenandoahWorkerScope scope(ShenandoahHeap::heap()->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n+                                msg);\n+    EventMark em(\"%s\", msg);\n@@ -321,2 +360,19 @@\n-  heap->try_inject_alloc_failure();\n-  op_reset();\n+    heap->old_generation()->prepare_gc();\n+  }\n+}\n+\n+void ShenandoahConcurrentGC::entry_scan_remembered_set() {\n+  if (_generation->is_young()) {\n+    ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+    TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+    const char* msg = \"Concurrent remembered set scanning\";\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::init_scan_rset);\n+    EventMark em(\"%s\", msg);\n+\n+    ShenandoahWorkerScope scope(heap->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_rs_scanning(),\n+                                msg);\n+\n+    heap->try_inject_alloc_failure();\n+    _generation->scan_remembered_set(true \/* is_concurrent *\/);\n+  }\n@@ -503,2 +559,1 @@\n-\n-  heap->prepare_gc();\n+  _generation->prepare_gc();\n@@ -517,1 +572,2 @@\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n+      \/\/ reset, so it is very likely we don't need to do another write here.  Since most regions\n+      \/\/ are not \"active\", this path is relatively rare.\n@@ -539,2 +595,2 @@\n-  assert(heap->marking_context()->is_bitmap_clear(), \"need clear marking bitmap\");\n-  assert(!heap->marking_context()->is_complete(), \"should not be complete\");\n+  assert(_generation->is_bitmap_clear(), \"need clear marking bitmap\");\n+  assert(!_generation->is_mark_complete(), \"should not be complete\");\n@@ -543,0 +599,22 @@\n+\n+  if (heap->mode()->is_generational()) {\n+    if (_generation->is_young() || (_generation->is_global() && ShenandoahVerify)) {\n+      \/\/ The current implementation of swap_remembered_set() copies the write-card-table\n+      \/\/ to the read-card-table. The remembered sets are also swapped for GLOBAL collections\n+      \/\/ so that the verifier works with the correct copy of the card table when verifying.\n+      \/\/ TODO: This path should not really depend on ShenandoahVerify.\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_swap_rset);\n+      _generation->swap_remembered_set();\n+    }\n+\n+    if (_generation->is_global()) {\n+      heap->old_generation()->cancel_gc();\n+    } else if (heap->is_concurrent_old_mark_in_progress()) {\n+      \/\/ Purge the SATB buffers, transferring any valid, old pointers to the\n+      \/\/ old generation mark queue. Any pointers in a young region will be\n+      \/\/ abandoned.\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_transfer_satb);\n+      heap->old_generation()->transfer_pointers_from_satb();\n+    }\n+  }\n+\n@@ -551,1 +629,1 @@\n-  heap->set_concurrent_mark_in_progress(true);\n+  _generation->set_concurrent_mark_in_progress(true);\n@@ -555,1 +633,7 @@\n-  {\n+  if (_do_old_gc_bootstrap) {\n+    shenandoah_assert_generational();\n+    \/\/ Update region state for both young and old regions\n+    \/\/ TODO: We should be able to pull this out of the safepoint for the bootstrap\n+    \/\/ cycle. The top of an old region will only move when a GC cycle evacuates\n+    \/\/ objects into it. When we start an old cycle, we know that nothing can touch\n+    \/\/ the top of old regions.\n@@ -559,0 +643,6 @@\n+    heap->old_generation()->ref_processor()->reset_thread_locals();\n+  } else {\n+    \/\/ Update region state for only young regions\n+    ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_region_states);\n+    ShenandoahInitMarkUpdateRegionStateClosure cl;\n+    _generation->parallel_heap_region_iterate(&cl);\n@@ -562,1 +652,1 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = _generation->ref_processor();\n@@ -602,1 +692,26 @@\n-    heap->prepare_regions_and_collection_set(true \/*concurrent*\/);\n+    \/\/ The collection set is chosen by prepare_regions_and_collection_set().\n+    \/\/\n+    \/\/ TODO: Under severe memory overload conditions that can be checked here, we may want to limit\n+    \/\/ the inclusion of old-gen candidates within the collection set.  This would allow us to prioritize efforts on\n+    \/\/ evacuating young-gen,  This remediation is most appropriate when old-gen availability is very high (so there\n+    \/\/ are negligible negative impacts from delaying completion of old-gen evacuation) and when young-gen collections\n+    \/\/ are \"under duress\" (as signalled by very low availability of memory within young-gen, indicating that\/ young-gen\n+    \/\/ collections are not triggering frequently enough).\n+    _generation->prepare_regions_and_collection_set(true \/*concurrent*\/);\n+\n+    \/\/ Upon return from prepare_regions_and_collection_set(), certain parameters have been established to govern the\n+    \/\/ evacuation efforts that are about to begin.  In particular:\n+    \/\/\n+    \/\/ heap->get_promoted_reserve() represents the amount of memory within old-gen's available memory that has\n+    \/\/   been set aside to hold objects promoted from young-gen memory.  This represents an estimated percentage\n+    \/\/   of the live young-gen memory within the collection set.  If there is more data ready to be promoted than\n+    \/\/   can fit within this reserve, the promotion of some objects will be deferred until a subsequent evacuation\n+    \/\/   pass.\n+    \/\/\n+    \/\/ heap->get_old_evac_reserve() represents the amount of memory within old-gen's available memory that has been\n+    \/\/  set aside to hold objects evacuated from the old-gen collection set.\n+    \/\/\n+    \/\/ heap->get_young_evac_reserve() represents the amount of memory within young-gen's available memory that has\n+    \/\/  been set aside to hold objects evacuated from the young-gen collection set.  Conservatively, this value\n+    \/\/  equals the entire amount of live young-gen memory within the collection set, even though some of this memory\n+    \/\/  will likely be promoted.\n@@ -607,1 +722,11 @@\n-    if (!heap->collection_set()->is_empty()) {\n+    if (!heap->collection_set()->is_empty() || has_in_place_promotions(heap)) {\n+      \/\/ Even if the collection set is empty, we need to do evacuation if there are regions to be promoted in place.\n+      \/\/ Concurrent evacuation takes responsibility for registering objects and setting the remembered set cards to dirty.\n+\n+      LogTarget(Debug, gc, cset) lt;\n+      if (lt.is_enabled()) {\n+        ResourceMark rm;\n+        LogStream ls(lt);\n+        heap->collection_set()->print_on(&ls);\n+      }\n+\n@@ -612,0 +737,1 @@\n+      \/\/ TODO: Do we need to set this if we are only promoting regions in place? We don't need the barriers on for that.\n@@ -613,2 +739,0 @@\n-      \/\/ From here on, we need to update references.\n-      heap->set_has_forwarded_objects(true);\n@@ -622,0 +746,5 @@\n+      \/\/ Generational mode may promote objects in place during the evacuation phase.\n+      \/\/ If that is the only reason we are evacuating, we don't need to update references\n+      \/\/ and there will be no forwarded objects on the heap.\n+      heap->set_has_forwarded_objects(!heap->collection_set()->is_empty());\n+\n@@ -623,2 +752,7 @@\n-      ShenandoahCodeRoots::arm_nmethods_for_evac();\n-      ShenandoahStackWatermark::change_epoch_id();\n+      if (!heap->collection_set()->is_empty()) {\n+        \/\/ Iff objects will be evaluated, arm the nmethod barriers. These will be disarmed\n+        \/\/ under the same condition (established in prepare_concurrent_roots) after strong\n+        \/\/ root evacuation has completed (see op_strong_roots).\n+        ShenandoahCodeRoots::arm_nmethods_for_evac();\n+        ShenandoahStackWatermark::change_epoch_id();\n+      }\n@@ -641,0 +775,5 @@\n+bool ShenandoahConcurrentGC::has_in_place_promotions(ShenandoahHeap* heap) {\n+  return heap->mode()->is_generational() && heap->old_generation()->has_in_place_promotions();\n+}\n+\n+template<bool GENERATIONAL>\n@@ -644,4 +783,1 @@\n-\n-  ShenandoahConcurrentEvacThreadClosure(OopClosure* oops);\n-  void do_thread(Thread* thread);\n-};\n+  explicit ShenandoahConcurrentEvacThreadClosure(OopClosure* oops) : _oops(oops) {}\n@@ -650,8 +786,8 @@\n-ShenandoahConcurrentEvacThreadClosure::ShenandoahConcurrentEvacThreadClosure(OopClosure* oops) :\n-  _oops(oops) {\n-}\n-\n-void ShenandoahConcurrentEvacThreadClosure::do_thread(Thread* thread) {\n-  JavaThread* const jt = JavaThread::cast(thread);\n-  StackWatermarkSet::finish_processing(jt, _oops, StackWatermarkKind::gc);\n-}\n+  void do_thread(Thread* thread) override {\n+    JavaThread* const jt = JavaThread::cast(thread);\n+    StackWatermarkSet::finish_processing(jt, _oops, StackWatermarkKind::gc);\n+    if (GENERATIONAL) {\n+      ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+    }\n+  }\n+};\n@@ -659,0 +795,1 @@\n+template<bool GENERATIONAL>\n@@ -664,1 +801,1 @@\n-  ShenandoahConcurrentEvacUpdateThreadTask(uint n_workers) :\n+  explicit ShenandoahConcurrentEvacUpdateThreadTask(uint n_workers) :\n@@ -669,1 +806,6 @@\n-  void work(uint worker_id) {\n+  void work(uint worker_id) override {\n+    if (GENERATIONAL) {\n+      Thread* worker_thread = Thread::current();\n+      ShenandoahThreadLocalData::enable_plab_promotions(worker_thread);\n+    }\n+\n@@ -673,1 +815,1 @@\n-    ShenandoahConcurrentEvacThreadClosure thr_cl(&oops_cl);\n+    ShenandoahConcurrentEvacThreadClosure<GENERATIONAL> thr_cl(&oops_cl);\n@@ -682,2 +824,7 @@\n-  ShenandoahConcurrentEvacUpdateThreadTask task(heap->workers()->active_workers());\n-  heap->workers()->run_task(&task);\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahConcurrentEvacUpdateThreadTask<true> task(heap->workers()->active_workers());\n+    heap->workers()->run_task(&task);\n+  } else {\n+    ShenandoahConcurrentEvacUpdateThreadTask<false> task(heap->workers()->active_workers());\n+    heap->workers()->run_task(&task);\n+  }\n@@ -694,1 +841,1 @@\n-  heap->ref_processor()->process_references(ShenandoahPhaseTimings::conc_weak_refs, heap->workers(), true \/* concurrent *\/);\n+  _generation->ref_processor()->process_references(ShenandoahPhaseTimings::conc_weak_refs, heap->workers(), true \/* concurrent *\/);\n@@ -721,2 +868,10 @@\n-      shenandoah_assert_correct(p, obj);\n-      ShenandoahHeap::atomic_clear_oop(p, obj);\n+      shenandoah_assert_generations_reconciled();\n+      if (_heap->is_in_active_generation(obj)) {\n+        \/\/ TODO: This worries me. Here we are asserting that an unmarked from-space object is 'correct'.\n+        \/\/ Normally, I would call this a bogus assert, but there seems to be a legitimate use-case for\n+        \/\/ accessing from-space objects during class unloading. However, the from-space object may have\n+        \/\/ been \"filled\". We've made no effort to prevent old generation classes being unloaded by young\n+        \/\/ gen (and vice-versa).\n+        shenandoah_assert_correct(p, obj);\n+        ShenandoahHeap::atomic_clear_oop(p, obj);\n+      }\n@@ -828,0 +983,3 @@\n+  \/\/ We can only toggle concurrent_weak_root_in_progress flag\n+  \/\/ at a safepoint, so that mutators see a consistent\n+  \/\/ value. The flag will be cleared at the next safepoint.\n@@ -925,1 +1083,3 @@\n-\n+  if (ShenandoahVerify) {\n+    heap->verifier()->verify_before_updaterefs();\n+  }\n@@ -970,1 +1130,1 @@\n-    heap->clear_cancelled_gc();\n+    heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n@@ -978,0 +1138,2 @@\n+  \/\/ If we are running in generational mode and this is an aging cycle, this will also age active\n+  \/\/ regions that haven't been used for allocation.\n@@ -983,0 +1145,22 @@\n+  if (heap->mode()->is_generational() && heap->is_concurrent_old_mark_in_progress()) {\n+    \/\/ When the SATB barrier is left on to support concurrent old gen mark, it may pick up writes to\n+    \/\/ objects in the collection set. After those objects are evacuated, the pointers in the\n+    \/\/ SATB are no longer safe. Once we have finished update references, we are guaranteed that\n+    \/\/ no more writes to the collection set are possible.\n+    \/\/\n+    \/\/ This will transfer any old pointers in _active_ regions from the SATB to the old gen\n+    \/\/ mark queues. All other pointers will be discarded. This would also discard any pointers\n+    \/\/ in old regions that were included in a mixed evacuation. We aren't using the SATB filter\n+    \/\/ methods here because we cannot control when they execute. If the SATB filter runs _after_\n+    \/\/ a region has been recycled, we will not be able to detect the bad pointer.\n+    \/\/\n+    \/\/ We are not concerned about skipping this step in abbreviated cycles because regions\n+    \/\/ with no live objects cannot have been written to and so cannot have entries in the SATB\n+    \/\/ buffers.\n+    heap->old_generation()->transfer_pointers_from_satb();\n+\n+    \/\/ Aging_cycle is only relevant during evacuation cycle for individual objects and during final mark for\n+    \/\/ entire regions.  Both of these relevant operations occur before final update refs.\n+    ShenandoahGenerationalHeap::heap()->set_aging_cycle(false);\n+  }\n+\n@@ -995,1 +1179,17 @@\n-  ShenandoahHeap::heap()->set_concurrent_weak_root_in_progress(false);\n+\n+  ShenandoahHeap *heap = ShenandoahHeap::heap();\n+  heap->set_concurrent_weak_root_in_progress(false);\n+  heap->set_evacuation_in_progress(false);\n+\n+  if (heap->mode()->is_generational()) {\n+    \/\/ If the cycle was shortened for having enough immediate garbage, this could be\n+    \/\/ the last GC safepoint before concurrent marking of old resumes. We must be sure\n+    \/\/ that old mark threads don't see any pointers to garbage in the SATB buffers.\n+    if (heap->is_concurrent_old_mark_in_progress()) {\n+      heap->old_generation()->transfer_pointers_from_satb();\n+    }\n+\n+    if (!_generation->is_old()) {\n+      ShenandoahGenerationalHeap::heap()->update_region_ages(_generation->complete_marking_context());\n+    }\n+  }\n@@ -1014,1 +1214,1 @@\n-    return \"Pause Init Mark (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Init Mark\", \" (unload classes)\");\n@@ -1016,1 +1216,1 @@\n-    return \"Pause Init Mark\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Init Mark\", \"\");\n@@ -1022,1 +1222,3 @@\n-  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  assert(!heap->has_forwarded_objects() || heap->is_concurrent_old_mark_in_progress(),\n+         \"Should not have forwarded objects during final mark, unless old gen concurrent mark is running\");\n+\n@@ -1024,1 +1226,1 @@\n-    return \"Pause Final Mark (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Final Mark\", \" (unload classes)\");\n@@ -1026,1 +1228,1 @@\n-    return \"Pause Final Mark\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Final Mark\", \"\");\n@@ -1032,1 +1234,2 @@\n-  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  assert(!heap->has_forwarded_objects() || heap->is_concurrent_old_mark_in_progress(),\n+         \"Should not have forwarded objects concurrent mark, unless old gen concurrent mark is running\");\n@@ -1034,1 +1237,1 @@\n-    return \"Concurrent marking (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent marking\", \" (unload classes)\");\n@@ -1036,1 +1239,1 @@\n-    return \"Concurrent marking\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent marking\", \"\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":266,"deletions":63,"binary":false,"changes":329,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -33,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -42,0 +44,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -60,1 +63,2 @@\n-    ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+    ShenandoahParallelWorkerSession worker_session(worker_id);\n+    ShenandoahWorkerTimingsTracker timer(ShenandoahPhaseTimings::conc_mark, ShenandoahPhaseTimings::ParallelMark, worker_id, true);\n@@ -62,1 +66,5 @@\n-    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+    \/\/ Do not use active_generation() : we must use the gc_generation() set by\n+    \/\/ ShenandoahGCScope on the ControllerThread's stack; no safepoint may\n+    \/\/ intervene to update active_generation, so we can't\n+    \/\/ shenandoah_assert_generations_reconciled() here.\n+    ShenandoahReferenceProcessor* rp = heap->gc_generation()->ref_processor();\n@@ -101,1 +109,2 @@\n-    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+    ShenandoahReferenceProcessor* rp = heap->gc_generation()->ref_processor();\n+    shenandoah_assert_generations_reconciled();\n@@ -107,0 +116,1 @@\n+      ShenandoahObjToScanQueue* old_q = _cm->get_old_queue(worker_id);\n@@ -108,1 +118,1 @@\n-      ShenandoahSATBBufferClosure<GENERATION> cl(q);\n+      ShenandoahSATBBufferClosure<GENERATION> cl(q, old_q);\n@@ -123,2 +133,2 @@\n-ShenandoahConcurrentMark::ShenandoahConcurrentMark() :\n-  ShenandoahMark() {}\n+ShenandoahConcurrentMark::ShenandoahConcurrentMark(ShenandoahGeneration* generation) :\n+  ShenandoahMark(generation) {}\n@@ -133,0 +143,1 @@\n+  ShenandoahObjToScanQueueSet* const  _old_queue_set;\n@@ -137,0 +148,1 @@\n+                                    ShenandoahObjToScanQueueSet* old,\n@@ -145,3 +157,4 @@\n-                                                                     ShenandoahReferenceProcessor* rp,\n-                                                                     ShenandoahPhaseTimings::Phase phase,\n-                                                                     uint nworkers) :\n+                                                                                 ShenandoahObjToScanQueueSet* old,\n+                                                                                 ShenandoahReferenceProcessor* rp,\n+                                                                                 ShenandoahPhaseTimings::Phase phase,\n+                                                                                 uint nworkers) :\n@@ -151,0 +164,1 @@\n+  _old_queue_set(old),\n@@ -159,1 +173,3 @@\n-  ShenandoahMarkRefsClosure<GENERATION> cl(q, _rp);\n+  ShenandoahObjToScanQueue* old_q = (_old_queue_set == nullptr) ?\n+          nullptr : _old_queue_set->queue(worker_id);\n+  ShenandoahMarkRefsClosure<GENERATION> cl(q, _rp, old_q);\n@@ -167,7 +183,31 @@\n-  TASKQUEUE_STATS_ONLY(task_queues()->reset_taskqueue_stats());\n-\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n-  task_queues()->reserve(workers->active_workers());\n-  ShenandoahMarkConcurrentRootsTask<NON_GEN> task(task_queues(), rp, ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n-\n-  workers->run_task(&task);\n+  ShenandoahReferenceProcessor* rp = _generation->ref_processor();\n+  _generation->reserve_task_queues(workers->active_workers());\n+  switch (_generation->type()) {\n+    case YOUNG: {\n+      ShenandoahMarkConcurrentRootsTask<YOUNG> task(task_queues(), old_task_queues(), rp,\n+                                                    ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      workers->run_task(&task);\n+      break;\n+    }\n+    case GLOBAL: {\n+      assert(old_task_queues() == nullptr, \"Global mark should not have old gen mark queues\");\n+      ShenandoahMarkConcurrentRootsTask<GLOBAL> task(task_queues(), nullptr, rp,\n+                                                     ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      workers->run_task(&task);\n+      break;\n+    }\n+    case NON_GEN: {\n+      assert(old_task_queues() == nullptr, \"Non-generational mark should not have old gen mark queues\");\n+      ShenandoahMarkConcurrentRootsTask<NON_GEN> task(task_queues(), nullptr, rp,\n+                                                      ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      workers->run_task(&task);\n+      break;\n+    }\n+    case OLD: {\n+      \/\/ We use a YOUNG generation cycle to bootstrap concurrent old marking.\n+      ShouldNotReachHere();\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -196,0 +236,1 @@\n+  ShenandoahGenerationType gen_type = _generation->type();\n@@ -199,3 +240,28 @@\n-    TaskTerminator terminator(nworkers, task_queues());\n-    ShenandoahConcurrentMarkingTask<NON_GEN> task(this, &terminator);\n-    workers->run_task(&task);\n+    switch (gen_type) {\n+      case YOUNG: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<YOUNG> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      case OLD: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<OLD> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      case GLOBAL: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<GLOBAL> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      case NON_GEN: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<NON_GEN> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      default:\n+        ShouldNotReachHere();\n+    }\n@@ -228,3 +294,2 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  heap->set_concurrent_mark_in_progress(false);\n-  heap->mark_complete_marking_context();\n+  _generation->set_concurrent_mark_in_progress(false);\n+  _generation->set_mark_complete();\n@@ -250,4 +315,24 @@\n-  ShenandoahFinalMarkingTask<NON_GEN> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n-  heap->workers()->run_task(&task);\n-  assert(task_queues()->is_empty(), \"Should be empty\");\n-}\n+  switch (_generation->type()) {\n+    case YOUNG:{\n+      ShenandoahFinalMarkingTask<YOUNG> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    case OLD:{\n+      ShenandoahFinalMarkingTask<OLD> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    case GLOBAL:{\n+      ShenandoahFinalMarkingTask<GLOBAL> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    case NON_GEN:{\n+      ShenandoahFinalMarkingTask<NON_GEN> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -257,4 +342,1 @@\n-void ShenandoahConcurrentMark::cancel() {\n-  clear();\n-  ShenandoahReferenceProcessor* rp = ShenandoahHeap::heap()->ref_processor();\n-  rp->abandon_partial_discovery();\n+  assert(task_queues()->is_empty(), \"Should be empty\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.cpp","additions":113,"deletions":31,"binary":false,"changes":144,"status":"modified"},{"patch":"@@ -28,0 +28,2 @@\n+#include \"gc\/shenandoah\/shenandoahAffiliation.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n@@ -32,0 +34,3 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -43,0 +48,1 @@\n+    case ShenandoahFreeSetPartitionId::OldCollector: return \"OldCollector\";\n@@ -51,5 +57,8 @@\n-  log_debug(gc)(\"Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"], Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n-                _leftmosts[int(ShenandoahFreeSetPartitionId::Mutator)],\n-                _rightmosts[int(ShenandoahFreeSetPartitionId::Mutator)],\n-                _leftmosts[int(ShenandoahFreeSetPartitionId::Collector)],\n-                _rightmosts[int(ShenandoahFreeSetPartitionId::Collector)]);\n+  log_debug(gc)(\"Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"], Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT\n+               \"], Old Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+               _leftmosts[int(ShenandoahFreeSetPartitionId::Mutator)],\n+               _rightmosts[int(ShenandoahFreeSetPartitionId::Mutator)],\n+               _leftmosts[int(ShenandoahFreeSetPartitionId::Collector)],\n+               _rightmosts[int(ShenandoahFreeSetPartitionId::Collector)],\n+               _leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+               _rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)]);\n@@ -57,7 +66,10 @@\n-                \"], Empty Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n-                _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n-                _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n-                _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)],\n-                _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)]);\n-\n-  log_debug(gc)(\"%6s: %18s %18s %18s\", \"index\", \"Mutator Bits\", \"Collector Bits\", \"NotFree Bits\");\n+               \"], Empty Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT\n+               \"], Empty Old Collecto range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+               _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n+               _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n+               _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)],\n+               _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)],\n+               _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+               _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)]);\n+\n+  log_debug(gc)(\"%6s: %18s %18s %18s %18s\", \"index\", \"Mutator Bits\", \"Collector Bits\", \"Old Collector Bits\", \"NotFree Bits\");\n@@ -84,1 +96,2 @@\n-  uintx free_bits = mutator_bits | collector_bits;\n+  uintx old_collector_bits = _membership[int(ShenandoahFreeSetPartitionId::OldCollector)].bits_at(aligned_idx);\n+  uintx free_bits = mutator_bits | collector_bits | old_collector_bits;\n@@ -86,2 +99,2 @@\n-  log_debug(gc)(SSIZE_FORMAT_W(6) \": \" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0,\n-                aligned_idx, mutator_bits, collector_bits, notfree_bits);\n+  log_debug(gc)(SSIZE_FORMAT_W(6) \": \" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0,\n+               aligned_idx, mutator_bits, collector_bits, old_collector_bits, notfree_bits);\n@@ -95,1 +108,1 @@\n-    _membership{ ShenandoahSimpleBitMap(max_regions), ShenandoahSimpleBitMap(max_regions) }\n+    _membership{ ShenandoahSimpleBitMap(max_regions), ShenandoahSimpleBitMap(max_regions) , ShenandoahSimpleBitMap(max_regions) }\n@@ -165,1 +178,0 @@\n-  _region_counts[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_region_count;\n@@ -185,0 +197,14 @@\n+void ShenandoahRegionPartitions::establish_old_collector_intervals(idx_t old_collector_leftmost, idx_t old_collector_rightmost,\n+                                                                   idx_t old_collector_leftmost_empty,\n+                                                                   idx_t old_collector_rightmost_empty,\n+                                                                   size_t old_collector_region_count, size_t old_collector_used) {\n+  _leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_leftmost;\n+  _rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_rightmost;\n+  _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_leftmost_empty;\n+  _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_rightmost_empty;\n+\n+  _region_counts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_region_count;\n+  _used[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_used;\n+  _capacity[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_region_count * _region_size_bytes;\n+}\n+\n@@ -205,1 +231,1 @@\n-      _leftmosts_empty[int(partition)] = leftmost(partition);\n+      _leftmosts_empty[int(partition)] = _leftmosts[int(partition)];\n@@ -292,1 +318,0 @@\n-\n@@ -296,0 +321,17 @@\n+bool ShenandoahRegionPartitions::is_mutator_partition(ShenandoahFreeSetPartitionId p) {\n+  return (p == ShenandoahFreeSetPartitionId::Mutator);\n+}\n+\n+bool ShenandoahRegionPartitions::is_young_collector_partition(ShenandoahFreeSetPartitionId p) {\n+  return (p == ShenandoahFreeSetPartitionId::Collector);\n+}\n+\n+bool ShenandoahRegionPartitions::is_old_collector_partition(ShenandoahFreeSetPartitionId p) {\n+  return (p == ShenandoahFreeSetPartitionId::OldCollector);\n+}\n+\n+bool ShenandoahRegionPartitions::available_implies_empty(size_t available_in_region) {\n+  return (available_in_region == _region_size_bytes);\n+}\n+\n+\n@@ -298,0 +340,1 @@\n+  ShenandoahHeapRegion* r = ShenandoahHeap::heap()->get_region(idx);\n@@ -302,0 +345,5 @@\n+  assert (_membership[int(orig_partition)].is_set(idx), \"Cannot move from partition unless in partition\");\n+  assert ((r != nullptr) && ((r->is_trash() && (available == _region_size_bytes)) ||\n+                             (r->used() + available == _region_size_bytes)),\n+          \"Used: \" SIZE_FORMAT \" + available: \" SIZE_FORMAT \" should equal region size: \" SIZE_FORMAT,\n+          ShenandoahHeap::heap()->get_region(idx)->used(), available, _region_size_bytes);\n@@ -305,0 +353,2 @@\n+  \/\/                          Mutator empty => Collector\n+  \/\/                          Mutator empty => OldCollector\n@@ -306,0 +356,1 @@\n+  \/\/                          Mutator empty => OldCollector\n@@ -307,8 +358,10 @@\n-  assert (((available <= _region_size_bytes) &&\n-           (((orig_partition == ShenandoahFreeSetPartitionId::Mutator)\n-             && (new_partition == ShenandoahFreeSetPartitionId::Collector)) ||\n-            ((orig_partition == ShenandoahFreeSetPartitionId::Collector)\n-             && (new_partition == ShenandoahFreeSetPartitionId::Mutator)))) ||\n-          ((available == _region_size_bytes) &&\n-           ((orig_partition == ShenandoahFreeSetPartitionId::Mutator)\n-            && (new_partition == ShenandoahFreeSetPartitionId::Collector))), \"Unexpected movement between partitions\");\n+  \/\/                          OldCollector Empty => Mutator\n+  assert ((is_mutator_partition(orig_partition) && is_young_collector_partition(new_partition)) ||\n+          (is_mutator_partition(orig_partition) &&\n+           available_implies_empty(available) && is_old_collector_partition(new_partition)) ||\n+          (is_young_collector_partition(orig_partition) && is_mutator_partition(new_partition)) ||\n+          (is_old_collector_partition(orig_partition)\n+           && available_implies_empty(available) && is_mutator_partition(new_partition)),\n+          \"Unexpected movement between partitions, available: \" SIZE_FORMAT \", _region_size_bytes: \" SIZE_FORMAT\n+          \", orig_partition: %s, new_partition: %s\",\n+          available, _region_size_bytes, partition_name(orig_partition), partition_name(new_partition));\n@@ -317,0 +370,3 @@\n+  assert (_used[int(orig_partition)] >= used,\n+          \"Orig partition used: \" SIZE_FORMAT \" must exceed moved used: \" SIZE_FORMAT \" within region \" SSIZE_FORMAT,\n+          _used[int(orig_partition)], used, idx);\n@@ -485,0 +541,1 @@\n+      case ShenandoahFreeSetPartitionId::OldCollector:\n@@ -574,0 +631,35 @@\n+\n+  \/\/ Performance invariants. Failing these would not break the free partition, but performance would suffer.\n+  assert (leftmost(ShenandoahFreeSetPartitionId::OldCollector) <= _max, \"leftmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT,\n+          leftmost(ShenandoahFreeSetPartitionId::OldCollector),  _max);\n+  assert (rightmost(ShenandoahFreeSetPartitionId::OldCollector) < _max, \"rightmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT,\n+          rightmost(ShenandoahFreeSetPartitionId::OldCollector),  _max);\n+\n+  assert (leftmost(ShenandoahFreeSetPartitionId::OldCollector) == _max\n+          || partition_id_matches(leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                                  ShenandoahFreeSetPartitionId::OldCollector),\n+          \"leftmost region should be free: \" SSIZE_FORMAT,  leftmost(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (leftmost(ShenandoahFreeSetPartitionId::OldCollector) == _max\n+          || partition_id_matches(rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                                  ShenandoahFreeSetPartitionId::OldCollector),\n+          \"rightmost region should be free: \" SSIZE_FORMAT, rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+\n+  \/\/ If OldCollector partition is empty, leftmosts will both equal max, rightmosts will both equal zero.\n+  \/\/ Likewise for empty region partitions.\n+  beg_off = leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  end_off = rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  assert (beg_off >= leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+          \"free regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (end_off <= rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n+          \"free regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+\n+  beg_off = empty_leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  end_off = empty_rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  assert (beg_off >= _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+          \"free empty regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (end_off <= _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+          \"free empty regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n@@ -581,1 +673,0 @@\n-  _right_to_left_bias(false),\n@@ -587,0 +678,52 @@\n+void ShenandoahFreeSet::add_promoted_in_place_region_to_old_collector(ShenandoahHeapRegion* region) {\n+  shenandoah_assert_heaplocked();\n+  size_t plab_min_size_in_bytes = ShenandoahGenerationalHeap::heap()->plab_min_size() * HeapWordSize;\n+  size_t idx = region->index();\n+  size_t capacity = alloc_capacity(region);\n+  assert(_partitions.membership(idx) == ShenandoahFreeSetPartitionId::NotFree,\n+         \"Regions promoted in place should have been excluded from Mutator partition\");\n+  if (capacity >= plab_min_size_in_bytes) {\n+    _partitions.make_free(idx, ShenandoahFreeSetPartitionId::OldCollector, capacity);\n+    _heap->old_generation()->augment_promoted_reserve(capacity);\n+  }\n+}\n+\n+HeapWord* ShenandoahFreeSet::allocate_from_partition_with_affiliation(ShenandoahFreeSetPartitionId which_partition,\n+                                                                      ShenandoahAffiliation affiliation,\n+                                                                      ShenandoahAllocRequest& req, bool& in_new_region) {\n+  shenandoah_assert_heaplocked();\n+  idx_t rightmost_collector = ((affiliation == ShenandoahAffiliation::FREE)?\n+                               _partitions.rightmost_empty(which_partition): _partitions.rightmost(which_partition));\n+  idx_t leftmost_collector = ((affiliation == ShenandoahAffiliation::FREE)?\n+                              _partitions.leftmost_empty(which_partition): _partitions.leftmost(which_partition));\n+  if (_partitions.alloc_from_left_bias(which_partition)) {\n+    for (idx_t idx = leftmost_collector; idx <= rightmost_collector; ) {\n+      assert(_partitions.in_free_set(which_partition, idx), \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n+      ShenandoahHeapRegion* r = _heap->get_region(idx);\n+      if (r->affiliation() == affiliation) {\n+        HeapWord* result = try_allocate_in(r, req, in_new_region);\n+        if (result != nullptr) {\n+          return result;\n+        }\n+      }\n+      idx = _partitions.find_index_of_next_available_region(which_partition, idx + 1);\n+    }\n+  } else {\n+    for (idx_t idx = rightmost_collector; idx >= leftmost_collector; ) {\n+      assert(_partitions.in_free_set(which_partition, idx),\n+             \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n+      ShenandoahHeapRegion* r = _heap->get_region(idx);\n+      if (r->affiliation() == affiliation) {\n+        HeapWord* result = try_allocate_in(r, req, in_new_region);\n+        if (result != nullptr) {\n+          return result;\n+        }\n+      }\n+      idx = _partitions.find_index_of_previous_available_region(which_partition, idx - 1);\n+    }\n+  }\n+  log_debug(gc, free)(\"Could not allocate collector region with affiliation: %s for request \" PTR_FORMAT,\n+                      shenandoah_affiliation_name(affiliation), p2i(&req));\n+  return nullptr;\n+}\n+\n@@ -592,1 +735,2 @@\n-  \/\/ Leftmost and rightmost bounds provide enough caching to quickly find a region from which to allocate.\n+  \/\/ Leftmost and rightmost bounds provide enough caching to walk bitmap efficiently. Normally,\n+  \/\/ we would find the region to allocate at right away.\n@@ -598,3 +742,20 @@\n-  \/\/ Free set maintains mutator and collector partitions.  Mutator can only allocate from the\n-  \/\/ Mutator partition.  Collector prefers to allocate from the Collector partition, but may steal\n-  \/\/ regions from the Mutator partition if the Collector partition has been depleted.\n+  \/\/ Free set maintains mutator and collector partitions.  Normally, each allocates only from its partition,\n+  \/\/ except in special cases when the collector steals regions from the mutator partition.\n+\n+  \/\/ Overwrite with non-zero (non-NULL) values only if necessary for allocation bookkeeping.\n+  bool allow_new_region = true;\n+  if (_heap->mode()->is_generational()) {\n+    switch (req.affiliation()) {\n+      case ShenandoahAffiliation::OLD_GENERATION:\n+        \/\/ Note: unsigned result from free_unaffiliated_regions() will never be less than zero, but it may equal zero.\n+        if (_heap->old_generation()->free_unaffiliated_regions() <= 0) {\n+          allow_new_region = false;\n+        }\n+        break;\n+\n+      case ShenandoahAffiliation::YOUNG_GENERATION:\n+        \/\/ Note: unsigned result from free_unaffiliated_regions() will never be less than zero, but it may equal zero.\n+        if (_heap->young_generation()->free_unaffiliated_regions() <= 0) {\n+          allow_new_region = false;\n+        }\n+        break;\n@@ -602,0 +763,8 @@\n+      case ShenandoahAffiliation::FREE:\n+        fatal(\"Should request affiliation\");\n+\n+      default:\n+        ShouldNotReachHere();\n+        break;\n+    }\n+  }\n@@ -626,1 +795,1 @@\n-        _right_to_left_bias = (non_empty_on_right > non_empty_on_left);\n+        _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::Mutator, (non_empty_on_right < non_empty_on_left));\n@@ -629,1 +798,1 @@\n-      if (_right_to_left_bias) {\n+      if (!_partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::Mutator)) {\n@@ -674,0 +843,4 @@\n+    case ShenandoahAllocRequest::_alloc_plab: {\n+      \/\/ PLABs always reside in old-gen and are only allocated during\n+      \/\/ evacuation phase.\n+\n@@ -676,5 +849,11 @@\n-      idx_t leftmost_collector = _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector);\n-      for (idx_t idx = _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector); idx >= leftmost_collector; ) {\n-        assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Collector, idx),\n-               \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n-        HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n+      HeapWord* result;\n+      result = allocate_from_partition_with_affiliation(req.is_old()? ShenandoahFreeSetPartitionId::OldCollector:\n+                                                        ShenandoahFreeSetPartitionId::Collector,\n+                                                        req.affiliation(), req, in_new_region);\n+      if (result != nullptr) {\n+        return result;\n+      } else if (allow_new_region) {\n+        \/\/ Try a free region that is dedicated to GC allocations.\n+        result = allocate_from_partition_with_affiliation(req.is_old()? ShenandoahFreeSetPartitionId::OldCollector:\n+                                                          ShenandoahFreeSetPartitionId::Collector,\n+                                                          ShenandoahAffiliation::FREE, req, in_new_region);\n@@ -684,1 +863,0 @@\n-        idx = _partitions.find_index_of_previous_available_region(ShenandoahFreeSetPartitionId::Collector, idx - 1);\n@@ -691,0 +869,4 @@\n+      if (!allow_new_region && req.is_old() && (_heap->young_generation()->free_unaffiliated_regions() > 0)) {\n+        \/\/ This allows us to flip a mutator region to old_collector\n+        allow_new_region = true;\n+      }\n@@ -692,12 +874,31 @@\n-      \/\/ Try to steal an empty region from the mutator view.\n-      idx_t leftmost_mutator_empty = _partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n-      for (idx_t idx = _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Mutator); idx >= leftmost_mutator_empty; ) {\n-        assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx),\n-               \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n-        ShenandoahHeapRegion* r = _heap->get_region(idx);\n-        if (can_allocate_from(r)) {\n-          flip_to_gc(r);\n-          HeapWord *result = try_allocate_in(r, req, in_new_region);\n-          if (result != nullptr) {\n-            log_debug(gc)(\"Flipped region \" SIZE_FORMAT \" to gc for request: \" PTR_FORMAT, idx, p2i(&req));\n-            return result;\n+      \/\/ We should expand old-gen if this can prevent an old-gen evacuation failure.  We don't care so much about\n+      \/\/ promotion failures since they can be mitigated in a subsequent GC pass.  Would be nice to know if this\n+      \/\/ allocation request is for evacuation or promotion.  Individual threads limit their use of PLAB memory for\n+      \/\/ promotions, so we already have an assurance that any additional memory set aside for old-gen will be used\n+      \/\/ only for old-gen evacuations.\n+\n+      \/\/ TODO:\n+      \/\/ if (GC is idle (out of cycle) and mutator allocation fails and there is memory reserved in Collector\n+      \/\/ or OldCollector sets, transfer a region of memory so that we can satisfy the allocation request, and\n+      \/\/ immediately trigger the start of GC.  Is better to satisfy the allocation than to trigger out-of-cycle\n+      \/\/ allocation failure (even if this means we have a little less memory to handle evacuations during the\n+      \/\/ subsequent GC pass).\n+\n+      if (allow_new_region) {\n+        \/\/ Try to steal an empty region from the mutator view.\n+        idx_t rightmost_mutator = _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n+        idx_t leftmost_mutator =  _partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n+        for (idx_t idx = rightmost_mutator; idx >= leftmost_mutator; ) {\n+          assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx),\n+                 \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n+          ShenandoahHeapRegion* r = _heap->get_region(idx);\n+          if (can_allocate_from(r)) {\n+            if (req.is_old()) {\n+              flip_to_old_gc(r);\n+            } else {\n+              flip_to_gc(r);\n+            }\n+            \/\/ Region r is entirely empty.  If try_allocat_in fails on region r, something else is really wrong.\n+            \/\/ Don't bother to retry with other regions.\n+            log_debug(gc, free)(\"Flipped region \" SIZE_FORMAT \" to gc for request: \" PTR_FORMAT, idx, p2i(&req));\n+            return try_allocate_in(r, req, in_new_region);\n@@ -705,0 +906,1 @@\n+          idx = _partitions.find_index_of_previous_available_region(ShenandoahFreeSetPartitionId::Mutator, idx - 1);\n@@ -706,2 +908,0 @@\n-        idx = _partitions.find_index_of_previous_available_region(ShenandoahFreeSetPartitionId::Mutator, idx - 1);\n-\n@@ -713,0 +913,1 @@\n+    }\n@@ -719,0 +920,50 @@\n+\/\/ This work method takes an argument corresponding to the number of bytes\n+\/\/ free in a region, and returns the largest amount in heapwords that can be allocated\n+\/\/ such that both of the following conditions are satisfied:\n+\/\/\n+\/\/ 1. it is a multiple of card size\n+\/\/ 2. any remaining shard may be filled with a filler object\n+\/\/\n+\/\/ The idea is that the allocation starts and ends at card boundaries. Because\n+\/\/ a region ('s end) is card-aligned, the remainder shard that must be filled is\n+\/\/ at the start of the free space.\n+\/\/\n+\/\/ This is merely a helper method to use for the purpose of such a calculation.\n+size_t ShenandoahFreeSet::get_usable_free_words(size_t free_bytes) const {\n+  \/\/ e.g. card_size is 512, card_shift is 9, min_fill_size() is 8\n+  \/\/      free is 514\n+  \/\/      usable_free is 512, which is decreased to 0\n+  size_t usable_free = (free_bytes \/ CardTable::card_size()) << CardTable::card_shift();\n+  assert(usable_free <= free_bytes, \"Sanity check\");\n+  if ((free_bytes != usable_free) && (free_bytes - usable_free < ShenandoahHeap::min_fill_size() * HeapWordSize)) {\n+    \/\/ After aligning to card multiples, the remainder would be smaller than\n+    \/\/ the minimum filler object, so we'll need to take away another card's\n+    \/\/ worth to construct a filler object.\n+    if (usable_free >= CardTable::card_size()) {\n+      usable_free -= CardTable::card_size();\n+    } else {\n+      assert(usable_free == 0, \"usable_free is a multiple of card_size and card_size > min_fill_size\");\n+    }\n+  }\n+\n+  return usable_free \/ HeapWordSize;\n+}\n+\n+\/\/ Given a size argument, which is a multiple of card size, a request struct\n+\/\/ for a PLAB, and an old region, return a pointer to the allocated space for\n+\/\/ a PLAB which is card-aligned and where any remaining shard in the region\n+\/\/ has been suitably filled by a filler object.\n+\/\/ It is assumed (and assertion-checked) that such an allocation is always possible.\n+HeapWord* ShenandoahFreeSet::allocate_aligned_plab(size_t size, ShenandoahAllocRequest& req, ShenandoahHeapRegion* r) {\n+  assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+  assert(r->is_old(), \"All PLABs reside in old-gen\");\n+  assert(!req.is_mutator_alloc(), \"PLABs should not be allocated by mutators.\");\n+  assert(is_aligned(size, CardTable::card_size_in_words()), \"Align by design\");\n+\n+  HeapWord* result = r->allocate_aligned(size, req, CardTable::card_size());\n+  assert(result != nullptr, \"Allocation cannot fail\");\n+  assert(r->top() <= r->end(), \"Allocation cannot span end of region\");\n+  assert(is_aligned(result, CardTable::card_size_in_words()), \"Align by design\");\n+  return result;\n+}\n+\n@@ -724,1 +975,0 @@\n-\n@@ -732,0 +982,28 @@\n+    assert(!r->is_affiliated(), \"New region \" SIZE_FORMAT \" should be unaffiliated\", r->index());\n+    r->set_affiliation(req.affiliation());\n+    if (r->is_old()) {\n+      \/\/ Any OLD region allocated during concurrent coalesce-and-fill does not need to be coalesced and filled because\n+      \/\/ all objects allocated within this region are above TAMS (and thus are implicitly marked).  In case this is an\n+      \/\/ OLD region and concurrent preparation for mixed evacuations visits this region before the start of the next\n+      \/\/ old-gen concurrent mark (i.e. this region is allocated following the start of old-gen concurrent mark but before\n+      \/\/ concurrent preparations for mixed evacuations are completed), we mark this region as not requiring any\n+      \/\/ coalesce-and-fill processing.\n+      r->end_preemptible_coalesce_and_fill();\n+      _heap->old_generation()->clear_cards_for(r);\n+    }\n+    _heap->generation_for(r->affiliation())->increment_affiliated_region_count();\n+\n+#ifdef ASSERT\n+    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+    assert(ctx->top_at_mark_start(r) == r->bottom(), \"Newly established allocation region starts with TAMS equal to bottom\");\n+    assert(ctx->is_bitmap_clear_range(ctx->top_bitmap(r), r->end()), \"Bitmap above top_bitmap() must be clear\");\n+#endif\n+    log_debug(gc)(\"Using new region (\" SIZE_FORMAT \") for %s (\" PTR_FORMAT \").\",\n+                       r->index(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(&req));\n+  } else {\n+    assert(r->is_affiliated(), \"Region \" SIZE_FORMAT \" that is not new should be affiliated\", r->index());\n+    if (r->affiliation() != req.affiliation()) {\n+      assert(_heap->mode()->is_generational(), \"Request for %s from %s region should only happen in generational mode.\",\n+             req.affiliation_name(), r->affiliation_name());\n+      return nullptr;\n+    }\n@@ -736,13 +1014,23 @@\n-    \/\/ This is a GCLAB or a TLAB allocation\n-    size_t free = align_down(r->free() >> LogHeapWordSize, MinObjAlignment);\n-    if (adjusted_size > free) {\n-      adjusted_size = free;\n-    }\n-    if (adjusted_size >= req.min_size()) {\n-      result = r->allocate(adjusted_size, req.type());\n-      log_debug(gc)(\"Allocated \" SIZE_FORMAT \" words (adjusted from \" SIZE_FORMAT \") for %s @\" PTR_FORMAT\n-                          \" from %s region \" SIZE_FORMAT \", free bytes remaining: \" SIZE_FORMAT,\n-                          adjusted_size, req.size(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(result),\n-                          _partitions.partition_membership_name(r->index()), r->index(), r->free());\n-      assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, adjusted_size);\n-      req.set_actual_size(adjusted_size);\n+    size_t free = r->free();    \/\/ free represents bytes available within region r\n+    if (req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+      \/\/ This is a PLAB allocation\n+      assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+      assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, r->index()),\n+             \"PLABS must be allocated in old_collector_free regions\");\n+\n+      \/\/ Need to assure that plabs are aligned on multiple of card region\n+      \/\/ Convert free from unaligned bytes to aligned number of words\n+      size_t usable_free = get_usable_free_words(free);\n+      if (adjusted_size > usable_free) {\n+        adjusted_size = usable_free;\n+      }\n+      adjusted_size = align_down(adjusted_size, CardTable::card_size_in_words());\n+      if (adjusted_size >= req.min_size()) {\n+        result = allocate_aligned_plab(adjusted_size, req, r);\n+        assert(result != nullptr, \"allocate must succeed\");\n+        req.set_actual_size(adjusted_size);\n+      } else {\n+        \/\/ Otherwise, leave result == nullptr because the adjusted size is smaller than min size.\n+        log_trace(gc, free)(\"Failed to shrink PLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n+                            \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n+      }\n@@ -751,2 +1039,14 @@\n-      log_trace(gc, free)(\"Failed to shrink TLAB or GCLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n-                          \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n+      \/\/ This is a GCLAB or a TLAB allocation\n+      \/\/ Convert free from unaligned bytes to aligned number of words\n+      free = align_down(free >> LogHeapWordSize, MinObjAlignment);\n+      if (adjusted_size > free) {\n+        adjusted_size = free;\n+      }\n+      if (adjusted_size >= req.min_size()) {\n+        result = r->allocate(adjusted_size, req);\n+        assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, adjusted_size);\n+        req.set_actual_size(adjusted_size);\n+      } else {\n+        log_trace(gc, free)(\"Failed to shrink TLAB or GCLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n+                            \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n+      }\n@@ -756,1 +1056,1 @@\n-    result = r->allocate(size, req.type());\n+    result = r->allocate(size, req);\n@@ -759,4 +1059,0 @@\n-      log_debug(gc)(\"Allocated \" SIZE_FORMAT \" words for %s @\" PTR_FORMAT\n-                          \" from %s region \" SIZE_FORMAT \", free bytes remaining: \" SIZE_FORMAT,\n-                          size, ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(result),\n-                          _partitions.partition_membership_name(r->index()),  r->index(), r->free());\n@@ -770,0 +1066,1 @@\n+      assert(req.is_young(), \"Mutator allocations always come from young generation.\");\n@@ -775,1 +1072,6 @@\n-      \/\/ evacuation are not updated during evacuation.\n+      \/\/ evacuation are not updated during evacuation.  For both young and old regions r, it is essential that all\n+      \/\/ PLABs be made parsable at the end of evacuation.  This is enabled by retiring all plabs at end of evacuation.\n+      \/\/ TODO: Making a PLAB parsable involves placing a filler object in its remnant memory but does not require\n+      \/\/ that the PLAB be disabled for all future purposes.  We may want to introduce a new service to make the\n+      \/\/ PLABs parsable while still allowing the PLAB to serve future allocation requests that arise during the\n+      \/\/ next evacuation pass.\n@@ -777,0 +1079,7 @@\n+      if (r->is_old()) {\n+        _partitions.increase_used(ShenandoahFreeSetPartitionId::OldCollector, req.actual_size() * HeapWordSize);\n+        assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"old-gen allocations use PLAB or shared allocation\");\n+        \/\/ for plabs, we'll sort the difference between evac and promotion usage when we retire the plab\n+      } else {\n+        _partitions.increase_used(ShenandoahFreeSetPartitionId::Collector, req.actual_size() * HeapWordSize);\n+      }\n@@ -791,3 +1100,16 @@\n-    _partitions.retire_from_partition(req.is_mutator_alloc()?\n-                                      ShenandoahFreeSetPartitionId::Mutator: ShenandoahFreeSetPartitionId::Collector,\n-                                      idx, r->used());\n+    ShenandoahFreeSetPartitionId orig_partition;\n+    if (req.is_mutator_alloc()) {\n+      orig_partition = ShenandoahFreeSetPartitionId::Mutator;\n+    } else if (req.type() == ShenandoahAllocRequest::_alloc_gclab) {\n+      orig_partition = ShenandoahFreeSetPartitionId::Collector;\n+    } else if (req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+      orig_partition = ShenandoahFreeSetPartitionId::OldCollector;\n+    } else {\n+      assert(req.type() == ShenandoahAllocRequest::_alloc_shared_gc, \"Unexpected allocation type\");\n+      if (req.is_old()) {\n+        orig_partition = ShenandoahFreeSetPartitionId::OldCollector;\n+      } else {\n+        orig_partition = ShenandoahFreeSetPartitionId::Collector;\n+      }\n+    }\n+    _partitions.retire_from_partition(orig_partition, idx, r->used());\n@@ -806,0 +1128,3 @@\n+  assert(req.is_young(), \"Humongous regions always allocated in YOUNG\");\n+  ShenandoahGeneration* generation = _heap->generation_for(req.affiliation());\n+\n@@ -818,1 +1143,1 @@\n-                                                                            start_range, num);\n+                                                                          start_range, num);\n@@ -863,0 +1188,1 @@\n+  bool is_generational = _heap->mode()->is_generational();\n@@ -885,0 +1211,2 @@\n+    r->set_affiliation(req.affiliation());\n+    r->set_update_watermark(r->bottom());\n@@ -887,1 +1215,1 @@\n-\n+  generation->increase_affiliated_region_count(num);\n@@ -900,0 +1228,3 @@\n+  if (remainder != 0) {\n+    req.set_waste(ShenandoahHeapRegion::region_size_words() - remainder);\n+  }\n@@ -905,1 +1236,0 @@\n-    _heap->decrease_used(r->used());\n@@ -935,0 +1265,21 @@\n+void ShenandoahFreeSet::flip_to_old_gc(ShenandoahHeapRegion* r) {\n+  size_t idx = r->index();\n+\n+  assert(_partitions.partition_id_matches(idx, ShenandoahFreeSetPartitionId::Mutator), \"Should be in mutator view\");\n+  assert(can_allocate_from(r), \"Should not be allocated\");\n+\n+  ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+  size_t region_capacity = alloc_capacity(r);\n+  _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                               ShenandoahFreeSetPartitionId::OldCollector, region_capacity);\n+  _partitions.assert_bounds();\n+  _heap->old_generation()->augment_evacuation_reserve(region_capacity);\n+  bool transferred = gen_heap->generation_sizer()->transfer_to_old(1);\n+  if (!transferred) {\n+    log_warning(gc, free)(\"Forcing transfer of \" SIZE_FORMAT \" to old reserve.\", idx);\n+    gen_heap->generation_sizer()->force_transfer_to_old(1);\n+  }\n+  \/\/ We do not ensure that the region is no longer trash, relying on try_allocate_in(), which always comes next,\n+  \/\/ to recycle trash before attempting to allocate anything in the region.\n+}\n+\n@@ -957,2 +1308,5 @@\n-}\n-void ShenandoahFreeSet::find_regions_with_alloc_capacity(size_t &cset_regions) {\n+  _alloc_bias_weight = 0;\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::Mutator, true);\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::Collector, false);\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::OldCollector, false);\n+}\n@@ -961,1 +1315,3 @@\n-  cset_regions = 0;\n+void ShenandoahFreeSet::find_regions_with_alloc_capacity(size_t &young_cset_regions, size_t &old_cset_regions,\n+                                                         size_t &first_old_region, size_t &last_old_region,\n+                                                         size_t &old_region_count) {\n@@ -963,0 +1319,7 @@\n+\n+  first_old_region = _heap->num_regions();\n+  last_old_region = 0;\n+  old_region_count = 0;\n+  old_cset_regions = 0;\n+  young_cset_regions = 0;\n+\n@@ -970,1 +1333,0 @@\n-\n@@ -974,0 +1336,7 @@\n+  size_t old_collector_leftmost = max_regions;\n+  size_t old_collector_rightmost = 0;\n+  size_t old_collector_leftmost_empty = max_regions;\n+  size_t old_collector_rightmost_empty = 0;\n+  size_t old_collector_regions = 0;\n+  size_t old_collector_used = 0;\n+\n@@ -979,1 +1348,13 @@\n-      cset_regions++;\n+      if (region->is_old()) {\n+        old_cset_regions++;\n+      } else {\n+        assert(region->is_young(), \"Trashed region should be old or young\");\n+        young_cset_regions++;\n+      }\n+    } else if (region->is_old()) {\n+      \/\/ count both humongous and regular regions, but don't count trash (cset) regions.\n+      old_region_count++;\n+      if (first_old_region > idx) {\n+        first_old_region = idx;\n+      }\n+      last_old_region = idx;\n@@ -982,0 +1363,1 @@\n+      assert(!region->is_cset(), \"Shouldn't be adding cset regions to the free set\");\n@@ -986,11 +1368,5 @@\n-        _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::Mutator);\n-\n-        if (idx < mutator_leftmost) {\n-          mutator_leftmost = idx;\n-        }\n-        if (idx > mutator_rightmost) {\n-          mutator_rightmost = idx;\n-        }\n-        if (ac == region_size_bytes) {\n-          if (idx < mutator_leftmost_empty) {\n-            mutator_leftmost_empty = idx;\n+        if (region->is_trash() || !region->is_old()) {\n+          \/\/ Both young and old collected regions (trashed) are placed into the Mutator set\n+          _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::Mutator);\n+          if (idx < mutator_leftmost) {\n+            mutator_leftmost = idx;\n@@ -998,2 +1374,2 @@\n-          if (idx > mutator_rightmost_empty) {\n-            mutator_rightmost_empty = idx;\n+          if (idx > mutator_rightmost) {\n+            mutator_rightmost = idx;\n@@ -1001,0 +1377,29 @@\n+          if (ac == region_size_bytes) {\n+            if (idx < mutator_leftmost_empty) {\n+              mutator_leftmost_empty = idx;\n+            }\n+            if (idx > mutator_rightmost_empty) {\n+              mutator_rightmost_empty = idx;\n+            }\n+          }\n+          mutator_regions++;\n+          mutator_used += (region_size_bytes - ac);\n+        } else {\n+          \/\/ !region->is_trash() && region is_old()\n+          _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::OldCollector);\n+          if (idx < old_collector_leftmost) {\n+            old_collector_leftmost = idx;\n+          }\n+          if (idx > old_collector_rightmost) {\n+            old_collector_rightmost = idx;\n+          }\n+          if (ac == region_size_bytes) {\n+            if (idx < old_collector_leftmost_empty) {\n+              old_collector_leftmost_empty = idx;\n+            }\n+            if (idx > old_collector_rightmost_empty) {\n+              old_collector_rightmost_empty = idx;\n+            }\n+          }\n+          old_collector_regions++;\n+          old_collector_used += (region_size_bytes - ac);\n@@ -1002,7 +1407,0 @@\n-        mutator_regions++;\n-        mutator_used += (region_size_bytes - ac);\n-\n-        log_debug(gc)(\n-          \"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator partition\",\n-          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n-          byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n@@ -1012,0 +1410,18 @@\n+  log_debug(gc)(\"  At end of prep_to_rebuild, mutator_leftmost: \" SIZE_FORMAT\n+                \", mutator_rightmost: \" SIZE_FORMAT\n+                \", mutator_leftmost_empty: \" SIZE_FORMAT\n+                \", mutator_rightmost_empty: \" SIZE_FORMAT\n+                \", mutator_regions: \" SIZE_FORMAT\n+                \", mutator_used: \" SIZE_FORMAT,\n+                mutator_leftmost, mutator_rightmost, mutator_leftmost_empty, mutator_rightmost_empty,\n+                mutator_regions, mutator_used);\n+\n+  log_debug(gc)(\"  old_collector_leftmost: \" SIZE_FORMAT\n+                \", old_collector_rightmost: \" SIZE_FORMAT\n+                \", old_collector_leftmost_empty: \" SIZE_FORMAT\n+                \", old_collector_rightmost_empty: \" SIZE_FORMAT\n+                \", old_collector_regions: \" SIZE_FORMAT\n+                \", old_collector_used: \" SIZE_FORMAT,\n+                old_collector_leftmost, old_collector_rightmost, old_collector_leftmost_empty, old_collector_rightmost_empty,\n+                old_collector_regions, old_collector_used);\n+\n@@ -1014,0 +1430,8 @@\n+  _partitions.establish_old_collector_intervals(old_collector_leftmost, old_collector_rightmost, old_collector_leftmost_empty,\n+                                                old_collector_rightmost_empty, old_collector_regions, old_collector_used);\n+  log_debug(gc)(\"  After find_regions_with_alloc_capacity(), Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"],\"\n+                \"  Old Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+                _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n@@ -1016,1 +1440,5 @@\n-void ShenandoahFreeSet::move_regions_from_collector_to_mutator(size_t max_xfer_regions) {\n+\/\/ Returns number of regions transferred, adds transferred bytes to var argument bytes_transferred\n+size_t ShenandoahFreeSet::transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId which_collector,\n+                                                                                   size_t max_xfer_regions,\n+                                                                                   size_t& bytes_transferred) {\n+  shenandoah_assert_heaplocked();\n@@ -1018,2 +1446,38 @@\n-  size_t collector_empty_xfer = 0;\n-  size_t collector_not_empty_xfer = 0;\n+  size_t transferred_regions = 0;\n+  idx_t rightmost = _partitions.rightmost_empty(which_collector);\n+  for (idx_t idx = _partitions.leftmost_empty(which_collector); (transferred_regions < max_xfer_regions) && (idx <= rightmost); ) {\n+    assert(_partitions.in_free_set(which_collector, idx), \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, idx);\n+    \/\/ Note: can_allocate_from() denotes that region is entirely empty\n+    if (can_allocate_from(idx)) {\n+      _partitions.move_from_partition_to_partition(idx, which_collector, ShenandoahFreeSetPartitionId::Mutator, region_size_bytes);\n+      transferred_regions++;\n+      bytes_transferred += region_size_bytes;\n+    }\n+    idx = _partitions.find_index_of_next_available_region(which_collector, idx + 1);\n+  }\n+  return transferred_regions;\n+}\n+\n+\/\/ Returns number of regions transferred, adds transferred bytes to var argument bytes_transferred\n+size_t ShenandoahFreeSet::transfer_non_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId collector_id,\n+                                                                                       size_t max_xfer_regions,\n+                                                                                       size_t& bytes_transferred) {\n+  shenandoah_assert_heaplocked();\n+  size_t transferred_regions = 0;\n+  idx_t rightmost = _partitions.rightmost(collector_id);\n+  for (idx_t idx = _partitions.leftmost(collector_id); (transferred_regions < max_xfer_regions) && (idx <= rightmost); ) {\n+    assert(_partitions.in_free_set(collector_id, idx), \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, idx);\n+    size_t ac = alloc_capacity(idx);\n+    if (ac > 0) {\n+      _partitions.move_from_partition_to_partition(idx, collector_id, ShenandoahFreeSetPartitionId::Mutator, ac);\n+      transferred_regions++;\n+      bytes_transferred += ac;\n+    }\n+    idx = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Collector, idx + 1);\n+  }\n+  return transferred_regions;\n+}\n+\n+void ShenandoahFreeSet::move_regions_from_collector_to_mutator(size_t max_xfer_regions) {\n+  size_t collector_xfer = 0;\n+  size_t old_collector_xfer = 0;\n@@ -1026,13 +1490,16 @@\n-    idx_t rightmost = _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Collector);\n-    for (idx_t idx = _partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Collector);\n-         (max_xfer_regions > 0) && (idx <= rightmost); ) {\n-      assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Collector, idx),\n-             \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, idx);\n-      \/\/ Note: can_allocate_from() denotes that region is entirely empty\n-      if (can_allocate_from(idx)) {\n-        _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Collector,\n-                                                     ShenandoahFreeSetPartitionId::Mutator, region_size_bytes);\n-        max_xfer_regions--;\n-        collector_empty_xfer += region_size_bytes;\n-      }\n-      idx = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Collector, idx + 1);\n+    max_xfer_regions -=\n+      transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId::Collector, max_xfer_regions,\n+                                                               collector_xfer);\n+  }\n+\n+  \/\/ Process empty regions within the OldCollector free partition\n+  if ((max_xfer_regions > 0) &&\n+      (_partitions.leftmost_empty(ShenandoahFreeSetPartitionId::OldCollector)\n+       <= _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::OldCollector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    size_t old_collector_regions =\n+      transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId::OldCollector, max_xfer_regions,\n+                                                               old_collector_xfer);\n+    max_xfer_regions -= old_collector_regions;\n+    if (old_collector_regions > 0) {\n+      ShenandoahGenerationalHeap::cast(_heap)->generation_sizer()->transfer_to_young(old_collector_regions);\n@@ -1046,14 +1513,3 @@\n-    idx_t rightmost = _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector);\n-    for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector);\n-         (max_xfer_regions > 0) && (idx <= rightmost); ) {\n-      assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Collector, idx),\n-             \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, idx);\n-      size_t ac = alloc_capacity(idx);\n-      if (ac > 0) {\n-        _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Collector,\n-                                                     ShenandoahFreeSetPartitionId::Mutator, ac);\n-        max_xfer_regions--;\n-        collector_not_empty_xfer += ac;\n-      }\n-      idx = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Collector, idx + 1);\n-    }\n+    max_xfer_regions -=\n+      transfer_non_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId::Collector, max_xfer_regions,\n+                                                                   collector_xfer);\n@@ -1062,3 +1518,6 @@\n-  size_t collector_xfer = collector_empty_xfer + collector_not_empty_xfer;\n-  log_info(gc, ergo)(\"At start of update refs, moving \" SIZE_FORMAT \"%s to Mutator free partition from Collector Reserve\",\n-                     byte_size_in_proper_unit(collector_xfer), proper_unit_for_byte_size(collector_xfer));\n+  size_t total_xfer = collector_xfer + old_collector_xfer;\n+  log_info(gc, ergo)(\"At start of update refs, moving \" SIZE_FORMAT \"%s to Mutator free set from Collector Reserve (\"\n+                     SIZE_FORMAT \"%s) and from Old Collector Reserve (\" SIZE_FORMAT \"%s)\",\n+                     byte_size_in_proper_unit(total_xfer), proper_unit_for_byte_size(total_xfer),\n+                     byte_size_in_proper_unit(collector_xfer), proper_unit_for_byte_size(collector_xfer),\n+                     byte_size_in_proper_unit(old_collector_xfer), proper_unit_for_byte_size(old_collector_xfer));\n@@ -1067,1 +1526,4 @@\n-void ShenandoahFreeSet::prepare_to_rebuild(size_t &cset_regions) {\n+\n+\/\/ Overwrite arguments to represent the amount of memory in each generation that is about to be recycled\n+void ShenandoahFreeSet::prepare_to_rebuild(size_t &young_cset_regions, size_t &old_cset_regions,\n+                                           size_t &first_old_region, size_t &last_old_region, size_t &old_region_count) {\n@@ -1069,0 +1531,3 @@\n+  \/\/ This resets all state information, removing all regions from all sets.\n+  clear();\n+  log_debug(gc, free)(\"Rebuilding FreeSet\");\n@@ -1070,1 +1535,4 @@\n-  log_debug(gc)(\"Rebuilding FreeSet\");\n+  \/\/ This places regions that have alloc_capacity into the old_collector set if they identify as is_old() or the\n+  \/\/ mutator set otherwise.  All trashed (cset) regions are affiliated young and placed in mutator set.\n+  find_regions_with_alloc_capacity(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+}\n@@ -1072,2 +1540,11 @@\n-  \/\/ This places regions that have alloc_capacity into the mutator partition.\n-  find_regions_with_alloc_capacity(cset_regions);\n+void ShenandoahFreeSet::establish_generation_sizes(size_t young_region_count, size_t old_region_count) {\n+  assert(young_region_count + old_region_count == ShenandoahHeap::heap()->num_regions(), \"Sanity\");\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+    ShenandoahOldGeneration* old_gen = heap->old_generation();\n+    ShenandoahYoungGeneration* young_gen = heap->young_generation();\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+    old_gen->set_capacity(old_region_count * region_size_bytes);\n+    young_gen->set_capacity(young_region_count * region_size_bytes);\n+  }\n@@ -1076,1 +1553,2 @@\n-void ShenandoahFreeSet::finish_rebuild(size_t cset_regions) {\n+void ShenandoahFreeSet::finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t old_region_count,\n+                                       bool have_evacuation_reserves) {\n@@ -1078,0 +1556,1 @@\n+  size_t young_reserve(0), old_reserve(0);\n@@ -1079,9 +1558,3 @@\n-  \/\/ Our desire is to reserve this much memory for future evacuation.  We may end up reserving less, if\n-  \/\/ memory is in short supply.\n-\n-  size_t reserve = _heap->max_capacity() * ShenandoahEvacReserve \/ 100;\n-  size_t available_in_collector_partition = (_partitions.capacity_of(ShenandoahFreeSetPartitionId::Collector)\n-                                             - _partitions.used_by(ShenandoahFreeSetPartitionId::Collector));\n-  size_t additional_reserve;\n-  if (available_in_collector_partition < reserve) {\n-    additional_reserve = reserve - available_in_collector_partition;\n+  if (_heap->mode()->is_generational()) {\n+    compute_young_and_old_reserves(young_cset_regions, old_cset_regions, have_evacuation_reserves,\n+                                   young_reserve, old_reserve);\n@@ -1089,1 +1562,2 @@\n-    additional_reserve = 0;\n+    young_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+    old_reserve = 0;\n@@ -1092,1 +1566,6 @@\n-  reserve_regions(reserve);\n+  \/\/ Move some of the mutator regions in the Collector and OldCollector partitions in order to satisfy\n+  \/\/ young_reserve and old_reserve.\n+  reserve_regions(young_reserve, old_reserve, old_region_count);\n+  size_t young_region_count = _heap->num_regions() - old_region_count;\n+  establish_generation_sizes(young_region_count, old_region_count);\n+  establish_old_collector_alloc_bias();\n@@ -1097,4 +1576,71 @@\n-void ShenandoahFreeSet::rebuild() {\n-  size_t cset_regions;\n-  prepare_to_rebuild(cset_regions);\n-  finish_rebuild(cset_regions);\n+void ShenandoahFreeSet::compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions,\n+                                                       bool have_evacuation_reserves,\n+                                                       size_t& young_reserve_result, size_t& old_reserve_result) const {\n+  shenandoah_assert_generational();\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+  ShenandoahOldGeneration* const old_generation = _heap->old_generation();\n+  size_t old_available = old_generation->available();\n+  size_t old_unaffiliated_regions = old_generation->free_unaffiliated_regions();\n+  ShenandoahYoungGeneration* const young_generation = _heap->young_generation();\n+  size_t young_capacity = young_generation->max_capacity();\n+  size_t young_unaffiliated_regions = young_generation->free_unaffiliated_regions();\n+\n+  \/\/ Add in the regions we anticipate to be freed by evacuation of the collection set\n+  old_unaffiliated_regions += old_cset_regions;\n+  young_unaffiliated_regions += young_cset_regions;\n+\n+  \/\/ Consult old-region balance to make adjustments to current generation capacities and availability.\n+  \/\/ The generation region transfers take place after we rebuild.\n+  const ssize_t old_region_balance = old_generation->get_region_balance();\n+  if (old_region_balance != 0) {\n+#ifdef ASSERT\n+    if (old_region_balance > 0) {\n+      assert(old_region_balance <= checked_cast<ssize_t>(old_unaffiliated_regions), \"Cannot transfer regions that are affiliated\");\n+    } else {\n+      assert(0 - old_region_balance <= checked_cast<ssize_t>(young_unaffiliated_regions), \"Cannot transfer regions that are affiliated\");\n+    }\n+#endif\n+\n+    ssize_t xfer_bytes = old_region_balance * checked_cast<ssize_t>(region_size_bytes);\n+    old_available -= xfer_bytes;\n+    old_unaffiliated_regions -= old_region_balance;\n+    young_capacity += xfer_bytes;\n+    young_unaffiliated_regions += old_region_balance;\n+  }\n+\n+  \/\/ All allocations taken from the old collector set are performed by GC, generally using PLABs for both\n+  \/\/ promotions and evacuations.  The partition between which old memory is reserved for evacuation and\n+  \/\/ which is reserved for promotion is enforced using thread-local variables that prescribe intentions for\n+  \/\/ each PLAB's available memory.\n+  if (have_evacuation_reserves) {\n+    \/\/ We are rebuilding at the end of final mark, having already established evacuation budgets for this GC pass.\n+    const size_t promoted_reserve = old_generation->get_promoted_reserve();\n+    const size_t old_evac_reserve = old_generation->get_evacuation_reserve();\n+    young_reserve_result = young_generation->get_evacuation_reserve();\n+    old_reserve_result = promoted_reserve + old_evac_reserve;\n+    assert(old_reserve_result <= old_available,\n+           \"Cannot reserve (\" SIZE_FORMAT \" + \" SIZE_FORMAT\") more OLD than is available: \" SIZE_FORMAT,\n+           promoted_reserve, old_evac_reserve, old_available);\n+  } else {\n+    \/\/ We are rebuilding at end of GC, so we set aside budgets specified on command line (or defaults)\n+    young_reserve_result = (young_capacity * ShenandoahEvacReserve) \/ 100;\n+    \/\/ The auto-sizer has already made old-gen large enough to hold all anticipated evacuations and promotions.\n+    \/\/ Affiliated old-gen regions are already in the OldCollector free set.  Add in the relevant number of\n+    \/\/ unaffiliated regions.\n+    old_reserve_result = old_available;\n+  }\n+\n+  \/\/ Old available regions that have less than PLAB::min_size() of available memory are not placed into the OldCollector\n+  \/\/ free set.  Because of this, old_available may not have enough memory to represent the intended reserve.  Adjust\n+  \/\/ the reserve downward to account for this possibility. This loss is part of the reason why the original budget\n+  \/\/ was adjusted with ShenandoahOldEvacWaste and ShenandoahOldPromoWaste multipliers.\n+  if (old_reserve_result >\n+      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n+    old_reserve_result =\n+      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes;\n+  }\n+\n+  if (young_reserve_result > young_unaffiliated_regions * region_size_bytes) {\n+    young_reserve_result = young_unaffiliated_regions * region_size_bytes;\n+  }\n@@ -1103,1 +1649,6 @@\n-void ShenandoahFreeSet::reserve_regions(size_t to_reserve) {\n+\/\/ Having placed all regions that have allocation capacity into the mutator set if they identify as is_young()\n+\/\/ or into the old collector set if they identify as is_old(), move some of these regions from the mutator set\n+\/\/ into the collector set or old collector set in order to assure that the memory available for allocations within\n+\/\/ the collector set is at least to_reserve and the memory available for allocations within the old collector set\n+\/\/ is at least to_reserve_old.\n+void ShenandoahFreeSet::reserve_regions(size_t to_reserve, size_t to_reserve_old, size_t &old_region_count) {\n@@ -1107,1 +1658,0 @@\n-\n@@ -1113,1 +1663,2 @@\n-    assert (ac > 0, \"Membership in free partition implies has capacity\");\n+    assert (ac > 0, \"Membership in free set implies has capacity\");\n+    assert (!r->is_old() || r->is_trash(), \"Except for trash, mutator_is_free regions should not be affiliated OLD\");\n@@ -1115,0 +1666,1 @@\n+    bool move_to_old_collector = _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector) < to_reserve_old;\n@@ -1116,2 +1668,3 @@\n-    if (!move_to_collector) {\n-      \/\/ We've satisfied to_reserve\n+\n+    if (!move_to_collector && !move_to_old_collector) {\n+      \/\/ We've satisfied both to_reserve and to_reserved_old\n@@ -1121,0 +1674,20 @@\n+    if (move_to_old_collector) {\n+      \/\/ We give priority to OldCollector partition because we desire to pack OldCollector regions into higher\n+      \/\/ addresses than Collector regions.  Presumably, OldCollector regions are more \"stable\" and less likely to\n+      \/\/ be collected in the near future.\n+      if (r->is_trash() || !r->is_affiliated()) {\n+        \/\/ OLD regions that have available memory are already in the old_collector free set.\n+        _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                                     ShenandoahFreeSetPartitionId::OldCollector, ac);\n+        log_debug(gc)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to old_collector_free\", idx);\n+        log_debug(gc)(\"  Shifted Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"],\"\n+                      \"  Old Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+                      _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                      _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                      _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                      _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+        old_region_count++;\n+        continue;\n+      }\n+    }\n+\n@@ -1133,0 +1706,6 @@\n+      log_debug(gc)(\"  Shifted Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"],\"\n+                    \"  Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+                    _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                    _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                    _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector),\n+                    _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector));\n@@ -1137,0 +1716,5 @@\n+    size_t old_reserve = _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector);\n+    if (old_reserve < to_reserve_old) {\n+      log_info(gc, free)(\"Wanted \" PROPERFMT \" for old reserve, but only reserved: \" PROPERFMT,\n+                         PROPERFMTARGS(to_reserve_old), PROPERFMTARGS(old_reserve));\n+    }\n@@ -1145,0 +1729,34 @@\n+void ShenandoahFreeSet::establish_old_collector_alloc_bias() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  shenandoah_assert_heaplocked();\n+\n+  idx_t left_idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+  idx_t right_idx = _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector);\n+  idx_t middle = (left_idx + right_idx) \/ 2;\n+  size_t available_in_first_half = 0;\n+  size_t available_in_second_half = 0;\n+\n+  for (idx_t index = left_idx; index < middle; index++) {\n+    if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, index)) {\n+      ShenandoahHeapRegion* r = heap->get_region((size_t) index);\n+      available_in_first_half += r->free();\n+    }\n+  }\n+  for (idx_t index = middle; index <= right_idx; index++) {\n+    if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, index)) {\n+      ShenandoahHeapRegion* r = heap->get_region(index);\n+      available_in_second_half += r->free();\n+    }\n+  }\n+\n+  \/\/ We desire to first consume the sparsely distributed regions in order that the remaining regions are densely packed.\n+  \/\/ Densely packing regions reduces the effort to search for a region that has sufficient memory to satisfy a new allocation\n+  \/\/ request.  Regions become sparsely distributed following a Full GC, which tends to slide all regions to the front of the\n+  \/\/ heap rather than allowing survivor regions to remain at the high end of the heap where we intend for them to congregate.\n+\n+  \/\/ TODO: In the future, we may modify Full GC so that it slides old objects to the end of the heap and young objects to the\n+  \/\/ front of the heap. If this is done, we can always search survivor Collector and OldCollector regions right to left.\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::OldCollector,\n+                                          (available_in_second_half > available_in_first_half));\n+}\n+\n@@ -1162,0 +1780,4 @@\n+    size_t retired_old = 0;\n+    size_t retired_old_humongous = 0;\n+    size_t retired_young = 0;\n+    size_t retired_young_humongous = 0;\n@@ -1163,0 +1785,2 @@\n+    size_t retired_young_waste = 0;\n+    size_t retired_old_waste = 0;\n@@ -1164,1 +1788,1 @@\n-    size_t available_collector = 0;\n+    size_t consumed_old_collector = 0;\n@@ -1166,0 +1790,2 @@\n+    size_t available_old = 0;\n+    size_t available_young = 0;\n@@ -1167,0 +1793,2 @@\n+    size_t available_collector = 0;\n+    size_t available_old_collector = 0;\n@@ -1172,3 +1800,7 @@\n-    log_debug(gc)(\"FreeSet map legend: M:mutator_free C:collector_free H:humongous _:retired\");\n-    log_debug(gc)(\" mutator free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"],\"\n-                  \" collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"]\",\n+\n+    log_debug(gc)(\"FreeSet map legend:\"\n+                       \" M:mutator_free C:collector_free O:old_collector_free\"\n+                       \" H:humongous ~:retired old _:retired young\");\n+    log_debug(gc)(\" mutator free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"] allocating from %s, \"\n+                  \" collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                  \"old collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"] allocates from %s\",\n@@ -1177,0 +1809,1 @@\n+                  _partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::Mutator)? \"left to right\": \"right to left\",\n@@ -1178,1 +1811,4 @@\n-                  _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector));\n+                  _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector),\n+                  _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                  _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                  _partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::OldCollector)? \"left to right\": \"right to left\");\n@@ -1188,0 +1824,1 @@\n+        assert(!r->is_old() || r->is_trash(), \"Old regions except trash regions should not be in mutator_free set\");\n@@ -1193,0 +1830,1 @@\n+        assert(!r->is_old() || r->is_trash(), \"Old regions except trash regions should not be in collector_free set\");\n@@ -1196,0 +1834,5 @@\n+      } else if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, i)) {\n+        size_t capacity = alloc_capacity(r);\n+        available_old_collector += capacity;\n+        consumed_old_collector += region_size_bytes - capacity;\n+        buffer[idx] = (capacity == region_size_bytes)? 'O': 'o';\n@@ -1197,1 +1840,7 @@\n-        buffer[idx] = 'h';\n+        if (r->is_old()) {\n+          buffer[idx] = 'H';\n+          retired_old_humongous += region_size_bytes;\n+        } else {\n+          buffer[idx] = 'h';\n+          retired_young_humongous += region_size_bytes;\n+        }\n@@ -1199,1 +1848,9 @@\n-        buffer[idx] = '_';\n+        if (r->is_old()) {\n+          buffer[idx] = '~';\n+          retired_old_waste += alloc_capacity(r);\n+          retired_old += region_size_bytes;\n+        } else {\n+          buffer[idx] = '_';\n+          retired_young_waste += alloc_capacity(r);\n+          retired_young += region_size_bytes;\n+        }\n@@ -1257,2 +1914,1 @@\n-\n-               byte_size_in_proper_unit(free),          proper_unit_for_byte_size(free),\n+               byte_size_in_proper_unit(total_free),    proper_unit_for_byte_size(total_free),\n@@ -1306,0 +1962,21 @@\n+\n+    if (_heap->mode()->is_generational()) {\n+      size_t max = 0;\n+      size_t total_free = 0;\n+      size_t total_used = 0;\n+\n+      for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+           idx <= _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector); idx++) {\n+        if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, idx)) {\n+          ShenandoahHeapRegion *r = _heap->get_region(idx);\n+          size_t free = alloc_capacity(r);\n+          max = MAX2(max, free);\n+          total_free += free;\n+          total_used += r->used();\n+        }\n+      }\n+      ls.print_cr(\" Old Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n+                  byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),\n+                  byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max),\n+                  byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used));\n+    }\n@@ -1317,0 +1994,1 @@\n+      case ShenandoahAllocRequest::_alloc_plab:\n@@ -1349,0 +2027,9 @@\n+  if (_heap->mode()->is_generational()) {\n+    out->print_cr(\"Old Collector Free Set: \" SIZE_FORMAT \"\", _partitions.count(ShenandoahFreeSetPartitionId::OldCollector));\n+    for (idx_t index = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+         index <= _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector); index++) {\n+      if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, index)) {\n+        _heap->get_region(index)->print_on(out);\n+      }\n+    }\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":861,"deletions":174,"binary":false,"changes":1035,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -39,0 +40,3 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n@@ -47,0 +51,3 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalEvacuationTask.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n@@ -54,1 +61,1 @@\n-#include \"gc\/shenandoah\/shenandoahMetrics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -62,0 +69,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -69,0 +77,2 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -71,0 +81,2 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n@@ -163,3 +175,0 @@\n-  \/\/ Now we know the number of regions, initialize the heuristics.\n-  initialize_heuristics();\n-\n@@ -219,0 +228,22 @@\n+  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this, _heap_region));\n+\n+  \/\/ Now we know the number of regions and heap sizes, initialize the heuristics.\n+  initialize_heuristics();\n+\n+  assert(_heap_region.byte_size() == heap_rs.size(), \"Need to know reserved size for card table\");\n+\n+  \/\/\n+  \/\/ Worker threads must be initialized after the barrier is configured\n+  \/\/\n+  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n+  if (_workers == nullptr) {\n+    vm_exit_during_initialization(\"Failed necessary allocation.\");\n+  } else {\n+    _workers->initialize_workers();\n+  }\n+\n+  if (ParallelGCThreads > 1) {\n+    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\", ParallelGCThreads);\n+    _safepoint_workers->initialize_workers();\n+  }\n+\n@@ -259,1 +290,1 @@\n-                              align_up(num_committed_regions, _bitmap_regions_per_slice) \/ _bitmap_regions_per_slice;\n+    align_up(num_committed_regions, _bitmap_regions_per_slice) \/ _bitmap_regions_per_slice;\n@@ -266,1 +297,1 @@\n-  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions, _max_workers);\n+  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions);\n@@ -357,0 +388,1 @@\n+  _affiliations = NEW_C_HEAP_ARRAY(uint8_t, _num_regions, mtGC);\n@@ -362,0 +394,1 @@\n+\n@@ -373,0 +406,2 @@\n+\n+      _affiliations[i] = ShenandoahAffiliation::FREE;\n@@ -377,0 +412,1 @@\n+    size_t young_cset_regions, old_cset_regions;\n@@ -378,1 +414,4 @@\n-    _free_set->rebuild();\n+    \/\/ We are initializing free set.  We ignore cset region tallies.\n+    size_t first_old, last_old, num_old;\n+    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+    _free_set->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n@@ -437,1 +476,1 @@\n-  _control_thread = new ShenandoahControlThread();\n+  initialize_controller();\n@@ -439,1 +478,1 @@\n-  ShenandoahInitLogger::print();\n+  print_init_logger();\n@@ -444,0 +483,8 @@\n+void ShenandoahHeap::initialize_controller() {\n+  _control_thread = new ShenandoahControlThread();\n+}\n+\n+void ShenandoahHeap::print_init_logger() const {\n+  ShenandoahInitLogger::print();\n+}\n+\n@@ -450,0 +497,2 @@\n+    } else if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+      _gc_mode = new ShenandoahGenerationalMode();\n@@ -470,13 +519,3 @@\n-  assert(_gc_mode != nullptr, \"Must be initialized\");\n-  _heuristics = _gc_mode->initialize_heuristics();\n-\n-  if (_heuristics->is_diagnostic() && !UnlockDiagnosticVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.\",\n-                    _heuristics->name()));\n-  }\n-  if (_heuristics->is_experimental() && !UnlockExperimentalVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.\",\n-                    _heuristics->name()));\n-  }\n+  _global_generation = new ShenandoahGlobalGeneration(mode()->is_generational(), max_workers(), max_capacity(), max_capacity());\n+  _global_generation->initialize_heuristics(mode());\n+  _evac_tracker = new ShenandoahEvacuationTracker(mode()->is_generational());\n@@ -492,0 +531,2 @@\n+  _gc_generation(nullptr),\n+  _active_generation(nullptr),\n@@ -493,3 +534,1 @@\n-  _used(0),\n-  _bytes_allocated_since_gc_start(0),\n-  _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),\n+  _max_workers(MAX3(ConcGCThreads, ParallelGCThreads, 1U)),\n@@ -502,1 +541,1 @@\n-  _update_refs_iterator(this),\n+  _affiliations(nullptr),\n@@ -505,0 +544,3 @@\n+  _cancel_requested_time(0),\n+  _update_refs_iterator(this),\n+  _global_generation(nullptr),\n@@ -506,0 +548,2 @@\n+  _young_generation(nullptr),\n+  _old_generation(nullptr),\n@@ -508,1 +552,0 @@\n-  _heuristics(nullptr),\n@@ -513,0 +556,2 @@\n+  _evac_tracker(nullptr),\n+  _mmu_tracker(),\n@@ -519,1 +564,0 @@\n-  _ref_processor(new ShenandoahReferenceProcessor(MAX2(_max_workers, 1U))),\n@@ -529,1 +573,1 @@\n-  \/\/ Initialize GC mode early, so we can adjust barrier support\n+  \/\/ Initialize GC mode early, many subsequent initialization procedures depend on it\n@@ -531,15 +575,0 @@\n-  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));\n-\n-  _max_workers = MAX2(_max_workers, 1U);\n-  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n-  if (_workers == nullptr) {\n-    vm_exit_during_initialization(\"Failed necessary allocation.\");\n-  } else {\n-    _workers->initialize_workers();\n-  }\n-\n-  if (ParallelGCThreads > 1) {\n-    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\",\n-                                                ParallelGCThreads);\n-    _safepoint_workers->initialize_workers();\n-  }\n@@ -552,29 +581,0 @@\n-class ShenandoahResetBitmapTask : public WorkerTask {\n-private:\n-  ShenandoahRegionIterator _regions;\n-\n-public:\n-  ShenandoahResetBitmapTask() :\n-    WorkerTask(\"Shenandoah Reset Bitmap\") {}\n-\n-  void work(uint worker_id) {\n-    ShenandoahHeapRegion* region = _regions.next();\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahMarkingContext* const ctx = heap->marking_context();\n-    while (region != nullptr) {\n-      if (heap->is_bitmap_slice_committed(region)) {\n-        ctx->clear_bitmap(region);\n-      }\n-      region = _regions.next();\n-    }\n-  }\n-};\n-\n-void ShenandoahHeap::reset_mark_bitmap() {\n-  assert_gc_workers(_workers->active_workers());\n-  mark_incomplete_marking_context();\n-\n-  ShenandoahResetBitmapTask task;\n-  _workers->run_task(&task);\n-}\n-\n@@ -595,1 +595,2 @@\n-  if (is_concurrent_mark_in_progress())        st->print(\"marking, \");\n+  if (is_concurrent_old_mark_in_progress())    st->print(\"old marking, \");\n+  if (is_concurrent_young_mark_in_progress())  st->print(\"young marking, \");\n@@ -646,0 +647,2 @@\n+  _mmu_tracker.initialize();\n+\n@@ -659,2 +662,0 @@\n-  _heuristics->initialize();\n-\n@@ -664,0 +665,4 @@\n+ShenandoahHeuristics* ShenandoahHeap::heuristics() {\n+  return _global_generation->heuristics();\n+}\n+\n@@ -665,1 +670,1 @@\n-  return Atomic::load(&_used);\n+  return global_generation()->used();\n@@ -672,4 +677,0 @@\n-size_t ShenandoahHeap::available() const {\n-  return free_set()->available();\n-}\n-\n@@ -686,2 +687,43 @@\n-void ShenandoahHeap::increase_used(size_t bytes) {\n-  Atomic::add(&_used, bytes, memory_order_relaxed);\n+\/\/ For tracking usage based on allocations, it should be the case that:\n+\/\/ * The sum of regions::used == heap::used\n+\/\/ * The sum of a generation's regions::used == generation::used\n+\/\/ * The sum of a generation's humongous regions::free == generation::humongous_waste\n+\/\/ These invariants are checked by the verifier on GC safepoints.\n+\/\/\n+\/\/ Additional notes:\n+\/\/ * When a mutator's allocation request causes a region to be retired, the\n+\/\/   free memory left in that region is considered waste. It does not contribute\n+\/\/   to the usage, but it _does_ contribute to allocation rate.\n+\/\/ * The bottom of a PLAB must be aligned on card size. In some cases this will\n+\/\/   require padding in front of the PLAB (a filler object). Because this padding\n+\/\/   is included in the region's used memory we include the padding in the usage\n+\/\/   accounting as waste.\n+\/\/ * Mutator allocations are used to compute an allocation rate. They are also\n+\/\/   sent to the Pacer for those purposes.\n+\/\/ * There are three sources of waste:\n+\/\/  1. The padding used to align a PLAB on card size\n+\/\/  2. Region's free is less than minimum TLAB size and is retired\n+\/\/  3. The unused portion of memory in the last region of a humongous object\n+void ShenandoahHeap::increase_used(const ShenandoahAllocRequest& req) {\n+  size_t actual_bytes = req.actual_size() * HeapWordSize;\n+  size_t wasted_bytes = req.waste() * HeapWordSize;\n+  ShenandoahGeneration* generation = generation_for(req.affiliation());\n+\n+  if (req.is_gc_alloc()) {\n+    assert(wasted_bytes == 0 || req.type() == ShenandoahAllocRequest::_alloc_plab, \"Only PLABs have waste\");\n+    increase_used(generation, actual_bytes + wasted_bytes);\n+  } else {\n+    assert(req.is_mutator_alloc(), \"Expected mutator alloc here\");\n+    \/\/ padding and actual size both count towards allocation counter\n+    generation->increase_allocated(actual_bytes + wasted_bytes);\n+\n+    \/\/ only actual size counts toward usage for mutator allocations\n+    increase_used(generation, actual_bytes);\n+\n+    \/\/ notify pacer of both actual size and waste\n+    notify_mutator_alloc_words(req.actual_size(), req.waste());\n+\n+    if (wasted_bytes > 0 && req.actual_size() > ShenandoahHeapRegion::humongous_threshold_words()) {\n+      increase_humongous_waste(generation,wasted_bytes);\n+    }\n+  }\n@@ -690,2 +732,5 @@\n-void ShenandoahHeap::set_used(size_t bytes) {\n-  Atomic::store(&_used, bytes);\n+void ShenandoahHeap::increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_humongous_waste(bytes);\n+  }\n@@ -694,3 +739,5 @@\n-void ShenandoahHeap::decrease_used(size_t bytes) {\n-  assert(used() >= bytes, \"never decrease heap size by more than we've left\");\n-  Atomic::sub(&_used, bytes, memory_order_relaxed);\n+void ShenandoahHeap::decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_humongous_waste(bytes);\n+  }\n@@ -699,2 +746,5 @@\n-void ShenandoahHeap::increase_allocated(size_t bytes) {\n-  Atomic::add(&_bytes_allocated_since_gc_start, bytes, memory_order_relaxed);\n+void ShenandoahHeap::increase_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_used(bytes);\n+  }\n@@ -703,4 +753,4 @@\n-void ShenandoahHeap::notify_mutator_alloc_words(size_t words, bool waste) {\n-  size_t bytes = words * HeapWordSize;\n-  if (!waste) {\n-    increase_used(bytes);\n+void ShenandoahHeap::decrease_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_used(bytes);\n@@ -708,1 +758,3 @@\n-  increase_allocated(bytes);\n+}\n+\n+void ShenandoahHeap::notify_mutator_alloc_words(size_t words, size_t waste) {\n@@ -711,2 +763,2 @@\n-    if (waste) {\n-      pacer()->claim_for_alloc(words, true);\n+    if (waste > 0) {\n+      pacer()->claim_for_alloc(waste, true);\n@@ -748,6 +800,0 @@\n-bool ShenandoahHeap::is_in(const void* p) const {\n-  HeapWord* heap_base = (HeapWord*) base();\n-  HeapWord* last_region_end = heap_base + ShenandoahHeapRegion::region_size_words() * num_regions();\n-  return p >= heap_base && p < last_region_end;\n-}\n-\n@@ -833,3 +879,1 @@\n-\n-  \/\/ This is called from allocation path, and thus should be fast.\n-  _heap_changed.try_set();\n+  _heap_changed.set();\n@@ -852,0 +896,1 @@\n+\n@@ -858,0 +903,1 @@\n+  log_debug(gc, free)(\"Set new GCLAB size: \" SIZE_FORMAT, new_size);\n@@ -863,0 +909,1 @@\n+    log_debug(gc, free)(\"New gclab size (\" SIZE_FORMAT \") is too small for \" SIZE_FORMAT, new_size, size);\n@@ -892,0 +939,1 @@\n+\/\/ Called from stubs in JIT code or interpreter\n@@ -940,0 +988,1 @@\n+    \/\/ gc_no_progress_count is incremented following each degen or full GC that fails to achieve is_good_progress().\n@@ -942,0 +991,1 @@\n+      req.set_actual_size(0);\n@@ -987,0 +1037,8 @@\n+  if (result == nullptr) {\n+    req.set_actual_size(0);\n+  }\n+\n+  \/\/ This is called regardless of the outcome of the allocation to account\n+  \/\/ for any waste created by retiring regions with this request.\n+  increase_used(req);\n+\n@@ -996,2 +1054,0 @@\n-      notify_mutator_alloc_words(actual, false);\n-\n@@ -1004,2 +1060,0 @@\n-    } else {\n-      increase_used(actual*HeapWordSize);\n@@ -1018,1 +1072,37 @@\n-  return _free_set->allocate(req, in_new_region);\n+\n+  \/\/ Make sure the old generation has room for either evacuations or promotions before trying to allocate.\n+  if (req.is_old() && !old_generation()->can_allocate(req)) {\n+    return nullptr;\n+  }\n+\n+  \/\/ If TLAB request size is greater than available, allocate() will attempt to downsize request to fit within available\n+  \/\/ memory.\n+  HeapWord* result = _free_set->allocate(req, in_new_region);\n+\n+  \/\/ Record the plab configuration for this result and register the object.\n+  if (result != nullptr && req.is_old()) {\n+    old_generation()->configure_plab_for_current_thread(req);\n+    if (req.type() == ShenandoahAllocRequest::_alloc_shared_gc) {\n+      \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+      \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+      \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+      \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+      \/\/\n+      \/\/ objects being \"concurrently\" allocated:\n+      \/\/    [-----a------][-----b-----][--------------c------------------]\n+      \/\/            [---- card table memory range --------------]\n+      \/\/\n+      \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n+      \/\/ wants to set the starts-object, first-start, and last-start attributes of the preceding card region.\n+      \/\/ Allocation of object b wants to set the starts-object, first-start, and last-start attributes of this card region.\n+      \/\/ Allocation of object c also wants to set the starts-object, first-start, and last-start attributes of this\n+      \/\/ card region.\n+      \/\/\n+      \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n+      \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n+      \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+      old_generation()->card_scan()->register_object(result);\n+    }\n+  }\n+\n+  return result;\n@@ -1033,2 +1123,2 @@\n-  if (heuristics()->can_unload_classes()) {\n-    ShenandoahHeuristics* h = heuristics();\n+  ShenandoahHeuristics* h = global_generation()->heuristics();\n+  if (h->can_unload_classes()) {\n@@ -1127,2 +1217,8 @@\n-  ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n-  workers()->run_task(&task);\n+  if (mode()->is_generational()) {\n+    ShenandoahRegionIterator regions;\n+    ShenandoahGenerationalEvacuationTask task(ShenandoahGenerationalHeap::heap(), &regions, concurrent);\n+    workers()->run_task(&task);\n+  } else {\n+    ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n+    workers()->run_task(&task);\n+  }\n@@ -1132,3 +1228,4 @@\n-  if (ShenandoahThreadLocalData::is_oom_during_evac(Thread::current())) {\n-    \/\/ This thread went through the OOM during evac protocol and it is safe to return\n-    \/\/ the forward pointer. It must not attempt to evacuate any more.\n+  assert(thread == Thread::current(), \"Expected thread parameter to be current thread.\");\n+  if (ShenandoahThreadLocalData::is_oom_during_evac(thread)) {\n+    \/\/ This thread went through the OOM during evac protocol. It is safe to return\n+    \/\/ the forward pointer. It must not attempt to evacuate any other objects.\n@@ -1140,1 +1237,2 @@\n-  size_t size = p->size();\n+  ShenandoahHeapRegion* r = heap_region_containing(p);\n+  assert(!r->is_humongous(), \"never evacuate humongous objects\");\n@@ -1142,1 +1240,3 @@\n-  assert(!heap_region_containing(p)->is_humongous(), \"never evacuate humongous objects\");\n+  ShenandoahAffiliation target_gen = r->affiliation();\n+  return try_evacuate_object(p, thread, r, target_gen);\n+}\n@@ -1144,1 +1244,5 @@\n-  bool alloc_from_gclab = true;\n+oop ShenandoahHeap::try_evacuate_object(oop p, Thread* thread, ShenandoahHeapRegion* from_region,\n+                                               ShenandoahAffiliation target_gen) {\n+  assert(target_gen == YOUNG_GENERATION, \"Only expect evacuations to young in this mode\");\n+  assert(from_region->is_young(), \"Only expect evacuations from young in this mode\");\n+  bool alloc_from_lab = true;\n@@ -1146,0 +1250,1 @@\n+  size_t size = p->size();\n@@ -1155,0 +1260,8 @@\n+      if ((copy == nullptr) && (size < ShenandoahThreadLocalData::gclab_size(thread))) {\n+        \/\/ GCLAB allocation failed because we are bumping up against the limit on young evacuation reserve.  Try resetting\n+        \/\/ the desired GCLAB size and retry GCLAB allocation to avoid cascading of shared memory allocations.\n+        \/\/ TODO: is this right? using PLAB::min_size() here for gc lab size?\n+        ShenandoahThreadLocalData::set_gclab_size(thread, PLAB::min_size());\n+        copy = allocate_from_gclab(thread, size);\n+        \/\/ If we still get nullptr, we'll try a shared allocation below.\n+      }\n@@ -1156,0 +1269,1 @@\n+\n@@ -1157,1 +1271,2 @@\n-      ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size);\n+      \/\/ If we failed to allocate in LAB, we'll try a shared allocation.\n+      ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen);\n@@ -1159,1 +1274,1 @@\n-      alloc_from_gclab = false;\n+      alloc_from_lab = false;\n@@ -1174,0 +1289,1 @@\n+  _evac_tracker->begin_evacuation(thread, size * HeapWordSize);\n@@ -1182,0 +1298,1 @@\n+    _evac_tracker->end_evacuation(thread, size * HeapWordSize);\n@@ -1190,7 +1307,4 @@\n-    \/\/\n-    \/\/ For GCLAB allocations, it is enough to rollback the allocation ptr. Either the next\n-    \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n-    \/\/ do this. For non-GCLAB allocations, we have no way to retract the allocation, and\n-    \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n-    \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n-    if (alloc_from_gclab) {\n+    if (alloc_from_lab) {\n+      \/\/ For LAB allocations, it is enough to rollback the allocation ptr. Either the next\n+      \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n+      \/\/ do this.\n@@ -1199,0 +1313,4 @@\n+      \/\/ For non-LAB allocations, we have no way to retract the allocation, and\n+      \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n+      \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n+      assert(size >= ShenandoahHeap::min_fill_size(), \"previously allocated object known to be larger than min_size\");\n@@ -1201,0 +1319,1 @@\n+      \/\/ For non-LAB allocations, the object has already been registered\n@@ -1234,1 +1353,1 @@\n-void ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n+size_t ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n@@ -1254,0 +1373,1 @@\n+  return required_regions;\n@@ -1263,0 +1383,6 @@\n+\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+      assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+      assert(plab->words_remaining() == 0, \"PLAB should not need retirement\");\n+    }\n@@ -1278,0 +1404,13 @@\n+\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+      assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+\n+      \/\/ There are two reasons to retire all plabs between old-gen evacuation passes.\n+      \/\/  1. We need to make the plab memory parsable by remembered-set scanning.\n+      \/\/  2. We need to establish a trustworthy UpdateWaterMark value within each old-gen heap region\n+      ShenandoahGenerationalHeap::heap()->retire_plab(plab, thread);\n+      if (_resize && ShenandoahThreadLocalData::plab_size(thread) > 0) {\n+        ShenandoahThreadLocalData::set_plab_size(thread, 0);\n+      }\n+    }\n@@ -1406,0 +1545,4 @@\n+    ls.cr();\n+\n+    evac_tracker()->print_global_on(&ls);\n+\n@@ -1411,0 +1554,40 @@\n+void ShenandoahHeap::set_gc_generation(ShenandoahGeneration* generation) {\n+  shenandoah_assert_control_or_vm_thread_at_safepoint();\n+  _gc_generation = generation;\n+}\n+\n+\/\/ Active generation may only be set by the VM thread at a safepoint.\n+void ShenandoahHeap::set_active_generation() {\n+  assert(Thread::current()->is_VM_thread(), \"Only the VM Thread\");\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Only at a safepoint!\");\n+  assert(_gc_generation != nullptr, \"Will set _active_generation to nullptr\");\n+  _active_generation = _gc_generation;\n+}\n+\n+void ShenandoahHeap::on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation) {\n+  shenandoah_policy()->record_collection_cause(cause);\n+\n+  assert(gc_cause()  == GCCause::_no_gc, \"Over-writing cause\");\n+  assert(_gc_generation == nullptr, \"Over-writing _gc_generation\");\n+\n+  set_gc_cause(cause);\n+  set_gc_generation(generation);\n+\n+  generation->heuristics()->record_cycle_start();\n+}\n+\n+void ShenandoahHeap::on_cycle_end(ShenandoahGeneration* generation) {\n+  assert(gc_cause() != GCCause::_no_gc, \"cause wasn't set\");\n+  assert(_gc_generation != nullptr, \"_gc_generation wasn't set\");\n+\n+  generation->heuristics()->record_cycle_end();\n+  if (mode()->is_generational() && generation->is_global()) {\n+    \/\/ If we just completed a GLOBAL GC, claim credit for completion of young-gen and old-gen GC as well\n+    young_generation()->heuristics()->record_cycle_end();\n+    old_generation()->heuristics()->record_cycle_end();\n+  }\n+\n+  set_gc_generation(nullptr);\n+  set_gc_cause(GCCause::_no_gc);\n+}\n+\n@@ -1760,99 +1943,0 @@\n-class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n-      \/\/ anyway to capture any updates that happened since now.\n-      r->clear_live_data();\n-      _ctx->capture_top_at_mark_start(r);\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_gc() {\n-  reset_mark_bitmap();\n-\n-  ShenandoahResetUpdateRegionStateClosure cl;\n-  parallel_heap_region_iterate(&cl);\n-}\n-\n-class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalMarkUpdateRegionStateClosure() :\n-    _ctx(ShenandoahHeap::heap()->complete_marking_context()), _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n-      \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n-      HeapWord *tams = _ctx->top_at_mark_start(r);\n-      HeapWord *top = r->top();\n-      if (top > tams) {\n-        r->increase_live_data_alloc_words(pointer_delta(top, tams));\n-      }\n-\n-      \/\/ We are about to select the collection set, make sure it knows about\n-      \/\/ current pinning status. Also, this allows trashing more regions that\n-      \/\/ now have their pinning status dropped.\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n-      }\n-\n-      \/\/ Remember limit for updating refs. It's guaranteed that we get no\n-      \/\/ from-space-refs written from here on.\n-      r->set_update_watermark_at_safepoint(r->top());\n-    } else {\n-      assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_regions_and_collection_set(bool concurrent) {\n-  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states :\n-                                         ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n-    ShenandoahFinalMarkUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n-\n-    assert_pinned_region_status();\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset :\n-                                         ShenandoahPhaseTimings::degen_gc_choose_cset);\n-    ShenandoahHeapLocker locker(lock());\n-    _collection_set->clear();\n-    heuristics()->choose_collection_set(_collection_set);\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset :\n-                                         ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n-  }\n-}\n-\n@@ -1861,0 +1945,3 @@\n+  if (mode()->is_generational()) {\n+    old_generation()->set_parseable(false);\n+  }\n@@ -1869,1 +1956,2 @@\n-  ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n+  shenandoah_assert_generations_reconciled();\n+  gc_generation()->ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n@@ -1903,0 +1991,41 @@\n+  \/\/ Check that if concurrent weak root is set then active_gen isn't null\n+  assert(!is_concurrent_weak_root_in_progress() || active_generation() != nullptr, \"Error\");\n+  shenandoah_assert_generations_reconciled();\n+}\n+\n+void ShenandoahHeap::set_concurrent_young_mark_in_progress(bool in_progress) {\n+  uint mask;\n+  assert(!has_forwarded_objects(), \"Young marking is not concurrent with evacuation\");\n+  if (!in_progress && is_concurrent_old_mark_in_progress()) {\n+    assert(mode()->is_generational(), \"Only generational GC has old marking\");\n+    assert(_gc_state.is_set(MARKING), \"concurrent_old_marking_in_progress implies MARKING\");\n+    \/\/ If old-marking is in progress when we turn off YOUNG_MARKING, leave MARKING (and OLD_MARKING) on\n+    mask = YOUNG_MARKING;\n+  } else {\n+    mask = MARKING | YOUNG_MARKING;\n+  }\n+  set_gc_state(mask, in_progress);\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::set_concurrent_old_mark_in_progress(bool in_progress) {\n+#ifdef ASSERT\n+  \/\/ has_forwarded_objects() iff UPDATEREFS or EVACUATION\n+  bool has_forwarded = has_forwarded_objects();\n+  bool updating_or_evacuating = _gc_state.is_set(UPDATEREFS | EVACUATION);\n+  bool evacuating = _gc_state.is_set(EVACUATION);\n+  assert ((has_forwarded == updating_or_evacuating) || (evacuating && !has_forwarded && collection_set()->is_empty()),\n+          \"Updating or evacuating iff has forwarded objects, or if evacuation phase is promoting in place without forwarding\");\n+#endif\n+  if (!in_progress && is_concurrent_young_mark_in_progress()) {\n+    \/\/ If young-marking is in progress when we turn off OLD_MARKING, leave MARKING (and YOUNG_MARKING) on\n+    assert(_gc_state.is_set(MARKING), \"concurrent_young_marking_in_progress implies MARKING\");\n+    set_gc_state(OLD_MARKING, in_progress);\n+  } else {\n+    set_gc_state(MARKING | OLD_MARKING, in_progress);\n+  }\n+  manage_satb_barrier(in_progress);\n+}\n+\n+bool ShenandoahHeap::is_prepare_for_old_mark_in_progress() const {\n+  return old_generation()->is_preparing_for_mark();\n@@ -1905,4 +2034,14 @@\n-void ShenandoahHeap::set_concurrent_mark_in_progress(bool in_progress) {\n-  assert(!has_forwarded_objects(), \"Not expected before\/after mark phase\");\n-  set_gc_state(MARKING, in_progress);\n-  ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(in_progress, !in_progress);\n+void ShenandoahHeap::manage_satb_barrier(bool active) {\n+  if (is_concurrent_mark_in_progress()) {\n+    \/\/ Ignore request to deactivate barrier while concurrent mark is in progress.\n+    \/\/ Do not attempt to re-activate the barrier if it is already active.\n+    if (active && !ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  } else {\n+    \/\/ No concurrent marking is in progress so honor request to deactivate,\n+    \/\/ but only if the barrier is already active.\n+    if (!active && ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  }\n@@ -1941,0 +2080,11 @@\n+void ShenandoahHeap::cancel_concurrent_mark() {\n+  if (mode()->is_generational()) {\n+    young_generation()->cancel_marking();\n+    old_generation()->cancel_marking();\n+  }\n+\n+  global_generation()->cancel_marking();\n+\n+  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+}\n+\n@@ -1946,0 +2096,1 @@\n+    _cancel_requested_time = os::elapsedTime();\n@@ -2069,5 +2220,6 @@\n-size_t ShenandoahHeap::bytes_allocated_since_gc_start() const {\n-  return Atomic::load(&_bytes_allocated_since_gc_start);\n-}\n-\n-  Atomic::store(&_bytes_allocated_since_gc_start, (size_t)0);\n+  if (mode()->is_generational()) {\n+    young_generation()->reset_bytes_allocated_since_gc_start();\n+    old_generation()->reset_bytes_allocated_since_gc_start();\n+  }\n+\n+  global_generation()->reset_bytes_allocated_since_gc_start();\n@@ -2138,2 +2290,5 @@\n-    assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n-           \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    shenandoah_assert_generations_reconciled();\n+    if (gc_generation()->contains(r)) {\n+      assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n+             \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    }\n@@ -2194,1 +2349,1 @@\n-  ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n+  explicit ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n@@ -2214,1 +2369,0 @@\n-    T cl;\n@@ -2219,2 +2373,5 @@\n-      \/\/ We cannot transfer any more regions than will be reclaimed when the existing collection set is recycled because\n-      \/\/ we need the reclaimed collection set regions to replenish the collector reserves\n+\n+      \/\/ Now that evacuation is done, we can reassign any regions that had been reserved to hold the results of evacuation\n+      \/\/ to the mutator free set.  At the end of GC, we will have cset_regions newly evacuated fully empty regions from\n+      \/\/ which we will be able to replenish the Collector free set and the OldCollector free set in preparation for the\n+      \/\/ next GC cycle.\n@@ -2224,1 +2381,1 @@\n-\n+    T cl;\n@@ -2226,1 +2383,0 @@\n-    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n@@ -2232,3 +2388,3 @@\n-      }\n-      if (ShenandoahPacing) {\n-        _heap->pacer()->report_updaterefs(pointer_delta(update_watermark, r->bottom()));\n+        if (ShenandoahPacing) {\n+          _heap->pacer()->report_updaterefs(pointer_delta(update_watermark, r->bottom()));\n+        }\n@@ -2256,0 +2412,1 @@\n+ShenandoahSynchronizePinnedRegionStates::ShenandoahSynchronizePinnedRegionStates() : _lock(ShenandoahHeap::heap()->lock()) { }\n@@ -2257,22 +2414,13 @@\n-class ShenandoahFinalUpdateRefsUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalUpdateRefsUpdateRegionStateClosure() : _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    \/\/ Drop unnecessary \"pinned\" state from regions that does not have CP marks\n-    \/\/ anymore, as this would allow trashing them.\n-\n-    if (r->is_active()) {\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n+void ShenandoahSynchronizePinnedRegionStates::heap_region_do(ShenandoahHeapRegion* r) {\n+  \/\/ Drop \"pinned\" state from regions that no longer have a pinned count. Put\n+  \/\/ regions with a pinned count into the \"pinned\" state.\n+  if (r->is_active()) {\n+    if (r->is_pinned()) {\n+      if (r->pin_count() == 0) {\n+        ShenandoahHeapLocker locker(_lock);\n+        r->make_unpinned();\n+      }\n+    } else {\n+      if (r->pin_count() > 0) {\n+        ShenandoahHeapLocker locker(_lock);\n+        r->make_pinned();\n@@ -2282,3 +2430,1 @@\n-\n-  bool is_thread_safe() { return true; }\n-};\n+}\n@@ -2294,2 +2440,2 @@\n-    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n+\n+    final_update_refs_update_region_states();\n@@ -2308,0 +2454,5 @@\n+void ShenandoahHeap::final_update_refs_update_region_states() {\n+  ShenandoahSynchronizePinnedRegionStates cl;\n+  parallel_heap_region_iterate(&cl);\n+}\n+\n@@ -2309,6 +2460,42 @@\n-  {\n-    ShenandoahGCPhase phase(concurrent ?\n-                            ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n-                            ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n+  ShenandoahGCPhase phase(concurrent ?\n+                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n+                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+  ShenandoahHeapLocker locker(lock());\n+  size_t young_cset_regions, old_cset_regions;\n+  size_t first_old_region, last_old_region, old_region_count;\n+  _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+  \/\/ If there are no old regions, first_old_region will be greater than last_old_region\n+  assert((first_old_region > last_old_region) ||\n+         ((last_old_region + 1 - first_old_region >= old_region_count) &&\n+          get_region(first_old_region)->is_old() && get_region(last_old_region)->is_old()),\n+         \"sanity: old_region_count: \" SIZE_FORMAT \", first_old_region: \" SIZE_FORMAT \", last_old_region: \" SIZE_FORMAT,\n+         old_region_count, first_old_region, last_old_region);\n+\n+  if (mode()->is_generational()) {\n+#ifdef ASSERT\n+    if (ShenandoahVerify) {\n+      verifier()->verify_before_rebuilding_free_set();\n+    }\n+#endif\n+\n+    \/\/ The computation of bytes_of_allocation_runway_before_gc_trigger is quite conservative so consider all of this\n+    \/\/ available for transfer to old. Note that transfer of humongous regions does not impact available.\n+    ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+    size_t allocation_runway = gen_heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions);\n+    gen_heap->compute_old_generation_balance(allocation_runway, old_cset_regions);\n+\n+    \/\/ Total old_available may have been expanded to hold anticipated promotions.  We trigger if the fragmented available\n+    \/\/ memory represents more than 16 regions worth of data.  Note that fragmentation may increase when we promote regular\n+    \/\/ regions in place when many of these regular regions have an abundant amount of available memory within them.  Fragmentation\n+    \/\/ will decrease as promote-by-copy consumes the available memory within these partially consumed regions.\n+    \/\/\n+    \/\/ We consider old-gen to have excessive fragmentation if more than 12.5% of old-gen is free memory that resides\n+    \/\/ within partially consumed regions of memory.\n+  }\n+  \/\/ Rebuild free set based on adjusted generation sizes.\n+  _free_set->finish_rebuild(young_cset_regions, old_cset_regions, old_region_count);\n+\n+  if (mode()->is_generational()) {\n+    ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+    ShenandoahOldGeneration* old_gen = gen_heap->old_generation();\n+    old_gen->heuristics()->trigger_maybe(first_old_region, last_old_region, old_region_count, num_regions());\n@@ -2437,1 +2624,1 @@\n-  return _memory_pool->get_memory_usage();\n+  return MemoryUsage(_initial_size, used(), committed(), max_capacity());\n@@ -2502,0 +2689,22 @@\n+\n+ShenandoahGeneration* ShenandoahHeap::generation_for(ShenandoahAffiliation affiliation) const {\n+  if (!mode()->is_generational()) {\n+    return global_generation();\n+  } else if (affiliation == YOUNG_GENERATION) {\n+    return young_generation();\n+  } else if (affiliation == OLD_GENERATION) {\n+    return old_generation();\n+  }\n+\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n+\n+void ShenandoahHeap::log_heap_status(const char* msg) const {\n+  if (mode()->is_generational()) {\n+    young_generation()->log_status(msg);\n+    old_generation()->log_status(msg);\n+  } else {\n+    global_generation()->log_status(msg);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":494,"deletions":285,"binary":false,"changes":779,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -162,0 +163,2 @@\n+  bool is_bitmap_clear_range(const HeapWord* start, const HeapWord* end) const;\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkBitMap.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -129,5 +129,2 @@\n-  if (heap->is_concurrent_mark_in_progress()) {\n-    ShenandoahKeepAliveClosure cl;\n-    data->oops_do(&cl);\n-  } else if (heap->is_concurrent_weak_root_in_progress() ||\n-             heap->is_concurrent_strong_root_in_progress() ) {\n+  if (heap->is_concurrent_weak_root_in_progress() ||\n+      heap->is_concurrent_strong_root_in_progress()) {\n@@ -136,0 +133,3 @@\n+  } else if (heap->is_concurrent_mark_in_progress()) {\n+    ShenandoahKeepAliveClosure cl;\n+    data->oops_do(&cl);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahNMethod.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -36,0 +37,80 @@\n+  product(uintx, ShenandoahGenerationalHumongousReserve, 0, EXPERIMENTAL,   \\\n+          \"(Generational mode only) What percent of the heap should be \"    \\\n+          \"reserved for humongous objects if possible.  Old-generation \"    \\\n+          \"collections will endeavor to evacuate old-gen regions within \"   \\\n+          \"this reserved area even if these regions do not contain high \"   \\\n+          \"percentage of garbage.  Setting a larger value will cause \"      \\\n+          \"more frequent old-gen collections.  A smaller value will \"       \\\n+          \"increase the likelihood that humongous object allocations \"      \\\n+          \"fail, resulting in stop-the-world full GCs.\")                    \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(double, ShenandoahMinOldGenGrowthPercent, 12.5, EXPERIMENTAL,     \\\n+          \"(Generational mode only) If the usage within old generation \"    \\\n+          \"has grown by at least this percent of its live memory size \"     \\\n+          \"at completion of the most recent old-generation marking \"        \\\n+          \"effort, heuristics may trigger the start of a new old-gen \"      \\\n+          \"collection.\")                                                    \\\n+          range(0.0,100.0)                                                  \\\n+                                                                            \\\n+  product(uintx, ShenandoahIgnoreOldGrowthBelowPercentage,10, EXPERIMENTAL, \\\n+          \"(Generational mode only) If the total usage of the old \"         \\\n+          \"generation is smaller than this percent, we do not trigger \"     \\\n+          \"old gen collections even if old has grown, except when \"         \\\n+          \"ShenandoahGenerationalDoNotIgnoreGrowthAfterYoungCycles \"        \\\n+          \"consecutive cycles have been completed following the \"           \\\n+          \"preceding old-gen collection.\")                                  \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahDoNotIgnoreGrowthAfterYoungCycles,               \\\n+          50, EXPERIMENTAL,                                                 \\\n+          \"(Generational mode only) Even if the usage of old generation \"   \\\n+          \"is below ShenandoahIgnoreOldGrowthBelowPercentage, \"             \\\n+          \"trigger an old-generation mark if old has grown and this \"       \\\n+          \"many consecutive young-gen collections have been \"               \\\n+          \"completed following the preceding old-gen collection.\")          \\\n+                                                                            \\\n+  product(bool, ShenandoahGenerationalCensusAtEvac, false, EXPERIMENTAL,    \\\n+          \"(Generational mode only) Object age census at evacuation, \"      \\\n+          \"rather than during marking.\")                                    \\\n+                                                                            \\\n+  product(bool, ShenandoahGenerationalAdaptiveTenuring, true, EXPERIMENTAL, \\\n+          \"(Generational mode only) Dynamically adapt tenuring age.\")       \\\n+                                                                            \\\n+  product(bool, ShenandoahGenerationalCensusIgnoreOlderCohorts, true,       \\\n+                                                               EXPERIMENTAL,\\\n+          \"(Generational mode only) Ignore mortality rates older than the \" \\\n+          \"oldest cohort under the tenuring age for the last cycle.\" )      \\\n+                                                                            \\\n+  product(uintx, ShenandoahGenerationalMinTenuringAge, 1, EXPERIMENTAL,     \\\n+          \"(Generational mode only) Floor for adaptive tenuring age. \"      \\\n+          \"Setting floor and ceiling to the same value fixes the tenuring \" \\\n+          \"age; setting both to 1 simulates a poor approximation to \"       \\\n+          \"AlwaysTenure, and setting both to 16 simulates NeverTenure.\")    \\\n+          range(1,16)                                                       \\\n+                                                                            \\\n+  product(uintx, ShenandoahGenerationalMaxTenuringAge, 15, EXPERIMENTAL,    \\\n+          \"(Generational mode only) Ceiling for adaptive tenuring age. \"    \\\n+          \"Setting floor and ceiling to the same value fixes the tenuring \" \\\n+          \"age; setting both to 1 simulates a poor approximation to \"       \\\n+          \"AlwaysTenure, and setting both to 16 simulates NeverTenure.\")    \\\n+          range(1,16)                                                       \\\n+                                                                            \\\n+  product(double, ShenandoahGenerationalTenuringMortalityRateThreshold,     \\\n+                                                         0.1, EXPERIMENTAL, \\\n+          \"(Generational mode only) Cohort mortality rates below this \"     \\\n+          \"value will be treated as indicative of longevity, leading to \"   \\\n+          \"tenuring. A lower value delays tenuring, a higher value hastens \"\\\n+          \"it. Used only when ShenandoahGenerationalhenAdaptiveTenuring is \"\\\n+          \"enabled.\")                                                       \\\n+          range(0.001,0.999)                                                \\\n+                                                                            \\\n+  product(size_t, ShenandoahGenerationalTenuringCohortPopulationThreshold,  \\\n+                                                         4*K, EXPERIMENTAL, \\\n+          \"(Generational mode only) Cohorts whose population is lower than \"\\\n+          \"this value in the previous census are ignored wrt tenuring \"     \\\n+          \"decisions. Effectively this makes then tenurable as soon as all \"\\\n+          \"older cohorts are. Set this value to the largest cohort \"        \\\n+          \"population volume that you are comfortable ignoring when making \"\\\n+          \"tenuring decisions.\")                                            \\\n+                                                                            \\\n@@ -64,1 +145,2 @@\n-          \" passive - stop the world GC only (either degenerated or full)\") \\\n+          \" passive - stop the world GC only (either degenerated or full);\" \\\n+          \" generational - generational concurrent GC\")                     \\\n@@ -78,0 +160,10 @@\n+  product(uintx, ShenandoahExpeditePromotionsThreshold, 5, EXPERIMENTAL,    \\\n+          \"When Shenandoah expects to promote at least this percentage \"    \\\n+          \"of the young generation, trigger a young collection to \"         \\\n+          \"expedite these promotions.\")                                     \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahExpediteMixedThreshold, 10, EXPERIMENTAL,        \\\n+          \"When there are this many old regions waiting to be collected, \"  \\\n+          \"trigger a mixed collection immediately.\")                        \\\n+                                                                            \\\n@@ -86,0 +178,14 @@\n+  product(uintx, ShenandoahOldGarbageThreshold, 15, EXPERIMENTAL,           \\\n+          \"How much garbage an old region has to contain before it would \"  \\\n+          \"be taken for collection.\")                                       \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahIgnoreGarbageThreshold, 5, EXPERIMENTAL,         \\\n+          \"When less than this amount of garbage (as a percentage of \"      \\\n+          \"region size) exists within a region, the region will not be \"    \\\n+          \"added to the collection set, even when the heuristic has \"       \\\n+          \"chosen to aggressively add regions with less than \"              \\\n+          \"ShenandoahGarbageThreshold amount of garbage into the \"          \\\n+          \"collection set.\")                                                \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n@@ -87,4 +193,6 @@\n-          \"How much heap should be free before some heuristics trigger the \"\\\n-          \"initial (learning) cycles. Affects cycle frequency on startup \"  \\\n-          \"and after drastic state changes, e.g. after degenerated\/full \"   \\\n-          \"GC cycles. In percents of (soft) max heap size.\")                \\\n+          \"When less than this amount of memory is free within the\"         \\\n+          \"heap or generation, trigger a learning cycle if we are \"         \\\n+          \"in learning mode.  Learning mode happens during initialization \" \\\n+          \"and following a drastic state change, such as following a \"      \\\n+          \"degenerated or Full GC cycle.  In percents of soft max \"         \\\n+          \"heap size.\")                                                     \\\n@@ -94,3 +202,5 @@\n-          \"How much heap should be free before most heuristics trigger the \"\\\n-          \"collection, even without other triggers. Provides the safety \"   \\\n-          \"margin for many heuristics. In percents of (soft) max heap size.\")\\\n+          \"Percentage of free heap memory (or young generation, in \"        \\\n+          \"generational mode) below which most heuristics trigger \"         \\\n+          \"collection independent of other triggers. Provides a safety \"    \\\n+          \"margin for many heuristics. In percents of (soft) max heap \"     \\\n+          \"size.\")                                                          \\\n@@ -159,0 +269,10 @@\n+  product(uintx, ShenandoahGuaranteedOldGCInterval, 10*60*1000, EXPERIMENTAL, \\\n+          \"Run a collection of the old generation at least this often. \"    \\\n+          \"Heuristics may trigger collections more frequently. Time is in \" \\\n+          \"milliseconds. Setting this to 0 disables the feature.\")          \\\n+                                                                            \\\n+  product(uintx, ShenandoahGuaranteedYoungGCInterval, 5*60*1000,  EXPERIMENTAL,  \\\n+          \"Run a collection of the young generation at least this often. \"  \\\n+          \"Heuristics may trigger collections more frequently. Time is in \" \\\n+          \"milliseconds. Setting this to 0 disables the feature.\")          \\\n+                                                                            \\\n@@ -218,4 +338,12 @@\n-          \"How much of heap to reserve for evacuations. Larger values make \"\\\n-          \"GC evacuate more live objects on every cycle, while leaving \"    \\\n-          \"less headroom for application to allocate in. In percents of \"   \\\n-          \"total heap size.\")                                               \\\n+          \"How much of (young-generation) heap to reserve for \"             \\\n+          \"(young-generation) evacuations.  Larger values allow GC to \"     \\\n+          \"evacuate more live objects on every cycle, while leaving \"       \\\n+          \"less headroom for application to allocate while GC is \"          \\\n+          \"evacuating and updating references. This parameter is \"          \\\n+          \"consulted at the end of marking, before selecting the \"          \\\n+          \"collection set.  If available memory at this time is smaller \"   \\\n+          \"than the indicated reserve, the bound on collection set size is \"\\\n+          \"adjusted downward.  The size of a generational mixed \"           \\\n+          \"evacuation collection set (comprised of both young and old \"     \\\n+          \"regions) is also bounded by this parameter.  In percents of \"    \\\n+          \"total (young-generation) heap size.\")                            \\\n@@ -228,1 +356,18 @@\n-          \"GC cycle.\")                                                      \\\n+          \"GC cycle.  Smaller values increase the risk of evacuation \"      \\\n+          \"failures, which will trigger stop-the-world Full GC passes.\")    \\\n+          range(1.0,100.0)                                                  \\\n+                                                                            \\\n+  product(double, ShenandoahOldEvacWaste, 1.4, EXPERIMENTAL,                \\\n+          \"How much waste evacuations produce within the reserved space. \"  \\\n+          \"Larger values make evacuations more resilient against \"          \\\n+          \"evacuation conflicts, at expense of evacuating less on each \"    \\\n+          \"GC cycle.  Smaller values increase the risk of evacuation \"      \\\n+          \"failures, which will trigger stop-the-world Full GC passes.\")    \\\n+          range(1.0,100.0)                                                  \\\n+                                                                            \\\n+  product(double, ShenandoahPromoEvacWaste, 1.2, EXPERIMENTAL,              \\\n+          \"How much waste promotions produce within the reserved space. \"   \\\n+          \"Larger values make evacuations more resilient against \"          \\\n+          \"evacuation conflicts, at expense of promoting less on each \"     \\\n+          \"GC cycle.  Smaller values increase the risk of evacuation \"      \\\n+          \"failures, which will trigger stop-the-world Full GC passes.\")    \\\n@@ -237,0 +382,35 @@\n+  product(uintx, ShenandoahOldEvacRatioPercent, 75, EXPERIMENTAL,           \\\n+          \"The maximum proportion of evacuation from old-gen memory, \"      \\\n+          \"expressed as a percentage. The default value 75 denotes that no\" \\\n+          \"more than 75% of the collection set evacuation workload may be \" \\\n+          \"towards evacuation of old-gen heap regions. This limits both the\"\\\n+          \"promotion of aged regions and the compaction of existing old \"   \\\n+          \"regions.  A value of 75 denotes that the total evacuation work\"  \\\n+          \"may increase to up to four times the young gen evacuation work.\" \\\n+          \"A larger value allows quicker promotion and allows\"              \\\n+          \"a smaller number of mixed evacuations to process \"               \\\n+          \"the entire list of old-gen collection candidates at the cost \"   \\\n+          \"of an increased disruption of the normal cadence of young-gen \"  \\\n+          \"collections.  A value of 100 allows a mixed evacuation to \"      \\\n+          \"focus entirely on old-gen memory, allowing no young-gen \"        \\\n+          \"regions to be collected, likely resulting in subsequent \"        \\\n+          \"allocation failures because the allocation pool is not \"         \\\n+          \"replenished.  A value of 0 allows a mixed evacuation to\"         \\\n+          \"focus entirely on young-gen memory, allowing no old-gen \"        \\\n+          \"regions to be collected, likely resulting in subsequent \"        \\\n+          \"promotion failures and triggering of stop-the-world full GC \"    \\\n+          \"events.\")                                                        \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahMinYoungPercentage, 20, EXPERIMENTAL,            \\\n+          \"The minimum percentage of the heap to use for the young \"        \\\n+          \"generation. Heuristics will not adjust the young generation \"    \\\n+          \"to be less than this.\")                                          \\\n+          range(0, 100)                                                     \\\n+                                                                            \\\n+  product(uintx, ShenandoahMaxYoungPercentage, 100, EXPERIMENTAL,           \\\n+          \"The maximum percentage of the heap to use for the young \"        \\\n+          \"generation. Heuristics will not adjust the young generation \"    \\\n+          \"to be more than this.\")                                          \\\n+          range(0, 100)                                                     \\\n+                                                                            \\\n@@ -311,0 +491,9 @@\n+  product(uintx, ShenandoahCoalesceChance, 0, DIAGNOSTIC,                   \\\n+          \"Testing: Abandon remaining mixed collections with this \"         \\\n+          \"likelihood. Following each mixed collection, abandon all \"       \\\n+          \"remaining mixed collection candidate regions with likelihood \"   \\\n+          \"ShenandoahCoalesceChance. Abandoning a mixed collection will \"   \\\n+          \"cause the old regions to be made parsable, rather than being \"   \\\n+          \"evacuated.\")                                                     \\\n+          range(0, 100)                                                     \\\n+                                                                            \\\n@@ -337,0 +526,4 @@\n+  product(bool, ShenandoahCardBarrier, false, DIAGNOSTIC,                   \\\n+          \"Turn on\/off card-marking post-write barrier in Shenandoah: \"     \\\n+          \" true when ShenandoahGCMode is generational, false otherwise\")   \\\n+                                                                            \\\n@@ -352,2 +545,29 @@\n-\n-\/\/ end of GC_SHENANDOAH_FLAGS\n+  product(uintx, ShenandoahOldCompactionReserve, 8, EXPERIMENTAL,           \\\n+          \"During generational GC, prevent promotions from filling \"        \\\n+          \"this number of heap regions.  These regions are reserved \"       \\\n+          \"for the purpose of supporting compaction of old-gen \"            \\\n+          \"memory.  Otherwise, old-gen memory cannot be compacted.\")        \\\n+          range(0, 128)                                                     \\\n+                                                                            \\\n+  product(bool, ShenandoahAllowOldMarkingPreemption, true, DIAGNOSTIC,      \\\n+          \"Allow young generation collections to suspend concurrent\"        \\\n+          \" marking in the old generation.\")                                \\\n+                                                                            \\\n+  product(uintx, ShenandoahAgingCyclePeriod, 1, EXPERIMENTAL,               \\\n+          \"With generational mode, increment the age of objects and\"        \\\n+          \"regions each time this many young-gen GC cycles are completed.\") \\\n+                                                                            \\\n+  develop(bool, ShenandoahEnableCardStats, false,                           \\\n+          \"Enable statistics collection related to clean & dirty cards\")    \\\n+                                                                            \\\n+  develop(int, ShenandoahCardStatsLogInterval, 50,                          \\\n+          \"Log cumulative card stats every so many remembered set or \"      \\\n+          \"update refs scans\")                                              \\\n+                                                                            \\\n+  product(uintx, ShenandoahMinimumOldMarkTimeMs, 100, EXPERIMENTAL,         \\\n+         \"Minimum amount of time in milliseconds to run old marking \"       \\\n+         \"before a young collection is allowed to run. This is intended \"   \\\n+         \"to prevent starvation of the old collector. Setting this to \"     \\\n+         \"0 will allow back to back young collections to run during old \"   \\\n+         \"marking.\")                                                        \\\n+  \/\/ end of GC_SHENANDOAH_FLAGS\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":235,"deletions":15,"binary":false,"changes":250,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -97,0 +98,17 @@\n+*\/\n+\n+\/*\n+ * @test id=generational\n+ * @summary Make sure Shenandoah can recover from humongous allocation fragmentation\n+ * @key randomness\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -Xlog:gc -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:ShenandoahTargetNumRegions=2048\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n+ *      TestAllocHumongousFragment\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -Xlog:gc -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:ShenandoahTargetNumRegions=2048\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      TestAllocHumongousFragment\n","filename":"test\/hotspot\/jtreg\/gc\/TestAllocHumongousFragment.java","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -100,0 +101,9 @@\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n+ *      TestAllocIntArrays\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      TestAllocIntArrays\n@@ -150,0 +160,2 @@\n+        \/\/ Each allocated int array is assumed to consume 16 bytes for alignment and header, plus\n+        \/\/  an average of 4 * the average number of elements in the array.\n@@ -153,0 +165,3 @@\n+        \/\/ Repeatedly, allocate an array of int having between 0 and 384K elements, until we have\n+        \/\/ allocated approximately TARGET_MB.  The largest allocated array consumes 384K*4 + 16, which is 1.5 M,\n+        \/\/ which is well below the heap size of 1g.\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestAllocIntArrays.java","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -102,0 +103,29 @@\n+\/*\n+ * @test id=generational\n+ * @summary Acceptance tests: collector can withstand allocation\n+ * @key randomness\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n+ *      TestAllocObjectArrays\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      TestAllocObjectArrays\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahOOMDuringEvacALot\n+ *      -XX:+ShenandoahVerify\n+ *      TestAllocObjectArrays\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahAllocFailureALot\n+ *      -XX:+ShenandoahVerify\n+ *      TestAllocObjectArrays\n+ *\/\n+\n@@ -137,0 +167,5 @@\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:-UseTLAB -XX:+ShenandoahVerify\n+ *      TestAllocObjectArrays\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestAllocObjectArrays.java","additions":35,"deletions":0,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -96,0 +97,15 @@\n+\/*\n+ * @test id=generational\n+ * @summary Acceptance tests: collector can withstand allocation\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n+ *      TestAllocObjects\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      TestAllocObjects\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestAllocObjects.java","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -65,0 +66,11 @@\n+\/*\n+ * @test id=generational\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ *\n+ * @run main\/othervm -Xms16m -Xmx512m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -Dtarget=10000\n+ *      TestDynamicSoftMaxHeapSize\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestDynamicSoftMaxHeapSize.java","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -79,0 +80,18 @@\n+\/**\n+ * @test id=generational\n+ * @summary Test Shenandoah GC uses concurrent\/parallel threads correctly\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm -Xmx16m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC\n+ *      -XX:ConcGCThreads=2 -XX:ParallelGCThreads=4\n+ *      -Dtarget=1000 -XX:ShenandoahGCMode=generational\n+ *      TestGCThreadGroups\n+ *\n+ * @run main\/othervm -Xmx16m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC\n+ *      -XX:-UseDynamicNumberOfGCThreads\n+ *      -Dtarget=1000 -XX:ShenandoahGCMode=generational\n+ *      TestGCThreadGroups\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestGCThreadGroups.java","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -87,0 +88,22 @@\n+\/*\n+ * @test id=generational\n+ * @summary Acceptance tests: collector can withstand allocation\n+ * @key randomness\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ *\n+ * @run main\/othervm -Xmx1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+ShenandoahUncommit -XX:ShenandoahUncommitDelay=0\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n+ *      TestHeapUncommit\n+ *\n+ * @run main\/othervm -Xmx1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+ShenandoahUncommit -XX:ShenandoahUncommitDelay=0\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      TestHeapUncommit\n+ *\n+ * @run main\/othervm -Xmx1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+ShenandoahUncommit -XX:ShenandoahUncommitDelay=0\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:-UseTLAB -XX:+ShenandoahVerify\n+ *      TestHeapUncommit\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestHeapUncommit.java","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -75,0 +76,12 @@\n+\/*\n+ * @test id=generational\n+ * @library \/test\/lib\n+ * @modules jdk.attach\/com.sun.tools.attach\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm\/timeout=480 -Xmx16m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -Dtarget=10000\n+ *      TestJcmdHeapDump\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestJcmdHeapDump.java","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -74,0 +75,10 @@\n+\/*\n+ * @test id=generational\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm\/timeout=480 -Xmx16m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -Dtarget=10000\n+ *      TestLotsOfCycles\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestLotsOfCycles.java","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -58,2 +59,3 @@\n-             {{\"satb\"},    {\"adaptive\", \"compact\", \"static\", \"aggressive\"}},\n-             {{\"passive\"}, {\"passive\"}}\n+             {{\"satb\"},         {\"adaptive\", \"compact\", \"static\", \"aggressive\"}},\n+             {{\"generational\"}, {\"adaptive\"}},\n+             {{\"passive\"},      {\"passive\"}}\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestObjItrWithHeapDump.java","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -48,1 +49,1 @@\n-        if (periodic && !output.getOutput().contains(\"Trigger: Time since last GC\")) {\n+        if (periodic && !output.getOutput().contains(\"Trigger (GLOBAL): Time since last GC\")) {\n@@ -51,1 +52,1 @@\n-        if (!periodic && output.getOutput().contains(\"Trigger: Time since last GC\")) {\n+        if (!periodic && output.getOutput().contains(\"Trigger (GLOBAL): Time since last GC\")) {\n@@ -56,0 +57,25 @@\n+    public static void testGenerational(boolean periodic, String... args) throws Exception {\n+        String[] cmds = Arrays.copyOf(args, args.length + 2);\n+        cmds[args.length] = TestPeriodicGC.class.getName();\n+        cmds[args.length + 1] = \"test\";\n+        ProcessBuilder pb = ProcessTools.createLimitedTestJavaProcessBuilder(cmds);\n+\n+        OutputAnalyzer output = new OutputAnalyzer(pb.start());\n+        output.shouldHaveExitValue(0);\n+        if (periodic) {\n+            if (!output.getOutput().contains(\"Trigger (YOUNG): Time since last GC\")) {\n+                throw new AssertionError(\"Generational mode: Should have periodic young GC in logs\");\n+            }\n+            if (!output.getOutput().contains(\"Trigger (OLD): Time since last GC\")) {\n+                throw new AssertionError(\"Generational mode: Should have periodic old GC in logs\");\n+            }\n+        } else {\n+            if (output.getOutput().contains(\"Trigger (YOUNG): Time since last GC\")) {\n+                throw new AssertionError(\"Generational mode: Should not have periodic young GC in logs\");\n+            }\n+            if (output.getOutput().contains(\"Trigger (OLD): Time since last GC\")) {\n+                throw new AssertionError(\"Generational mode: Should not have periodic old GC in logs\");\n+            }\n+        }\n+    }\n+\n@@ -129,0 +155,20 @@\n+\n+        testGenerational(true,\n+                         \"-Xlog:gc\",\n+                         \"-XX:+UnlockDiagnosticVMOptions\",\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         \"-XX:+UseShenandoahGC\",\n+                         \"-XX:ShenandoahGCMode=generational\",\n+                         \"-XX:ShenandoahGuaranteedYoungGCInterval=1000\",\n+                         \"-XX:ShenandoahGuaranteedOldGCInterval=1500\"\n+        );\n+\n+        testGenerational(false,\n+                         \"-Xlog:gc\",\n+                         \"-XX:+UnlockDiagnosticVMOptions\",\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         \"-XX:+UseShenandoahGC\",\n+                         \"-XX:ShenandoahGCMode=generational\",\n+                         \"-XX:ShenandoahGuaranteedYoungGCInterval=0\",\n+                         \"-XX:ShenandoahGuaranteedOldGCInterval=0\"\n+        );\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestPeriodicGC.java","additions":48,"deletions":2,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -48,0 +48,25 @@\n+ *      gc.shenandoah.TestReferenceRefersToShenandoah\n+ *\/\n+\n+\/* @test id=generational\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm\n+ *      -Xbootclasspath\/a:.\n+ *      -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI\n+ *      -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      gc.shenandoah.TestReferenceRefersToShenandoah\n+ *\/\n+\n+\/* @test id=generational-100\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @modules java.base\n+ * @run main jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm\n+ *      -Xbootclasspath\/a:.\n+ *      -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI\n+ *      -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:ShenandoahGarbageThreshold=100 -Xmx100m\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestReferenceRefersToShenandoah.java","additions":26,"deletions":1,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,0 +39,13 @@\n+\/* @test id=generational-100\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @modules java.base\n+ * @run main jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm\n+ *      -Xbootclasspath\/a:.\n+ *      -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI\n+ *      -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:ShenandoahGarbageThreshold=100 -Xmx100m\n+ *      gc.shenandoah.TestReferenceShortcutCycle\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestReferenceShortcutCycle.java","additions":14,"deletions":1,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -44,0 +45,15 @@\n+\/*\n+ * @test id=generational\n+ * @summary Test that null references\/referents work fine\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm -Xmx128m -Xms128m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n+ *      TestRefprocSanity\n+ *\n+ * @run main\/othervm -Xmx128m -Xms128m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      TestRefprocSanity\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestRefprocSanity.java","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -49,0 +50,9 @@\n+\/*\n+ * @test id=generational\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+ShenandoahRegionSampling\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      TestRegionSampling\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestRegionSampling.java","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -101,0 +102,20 @@\n+\/*\n+ * @test id=generational\n+ * @key randomness\n+ * @summary Test that Shenandoah is able to work with(out) resizeable TLABs\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n+ *      -XX:+ResizeTLAB\n+ *      TestResizeTLAB\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n+ *      -XX:-ResizeTLAB\n+ *      TestResizeTLAB\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestResizeTLAB.java","additions":21,"deletions":0,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -86,0 +87,25 @@\n+\/*\n+ * @test id=generational\n+ * @summary Acceptance tests: collector can deal with retained objects\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n+ *      TestRetainObjects\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      TestRetainObjects\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahOOMDuringEvacALot -XX:+ShenandoahVerify\n+ *      TestRetainObjects\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahAllocFailureALot -XX:+ShenandoahVerify\n+ *      TestRetainObjects\n+ *\/\n+\n@@ -111,1 +137,1 @@\n- * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ * @run main\/othervm\/timeout=300 -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestRetainObjects.java","additions":27,"deletions":1,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -93,0 +94,22 @@\n+\/*\n+ * @test id=generational\n+ * @summary Acceptance tests: collector can deal with retained objects\n+ * @key randomness\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahOOMDuringEvacALot -XX:+ShenandoahVerify\n+ *      TestSieveObjects\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahAllocFailureALot -XX:+ShenandoahVerify\n+ *      TestSieveObjects\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      TestSieveObjects\n+ *\/\n+\n@@ -124,1 +147,1 @@\n- * @run main\/othervm\/timeout=240 -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ * @run main\/othervm\/timeout=300 -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestSieveObjects.java","additions":24,"deletions":1,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -68,0 +69,14 @@\n+\/*\n+ * @test id=generational\n+ * @summary Test Shenandoah string deduplication implementation\n+ * @key randomness\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ * @modules java.base\/java.lang:open\n+ *          java.management\n+ *\n+ * @run main\/othervm -Xmx256m -Xlog:gc+stats -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseStringDeduplication\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:StringDeduplicationAgeThreshold=3\n+ *      TestStringDedup\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestStringDedup.java","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -45,0 +46,16 @@\n+\/*\n+ * @test id=generational\n+ * @summary Test Shenandoah string deduplication implementation\n+ * @key randomness\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ * @modules java.base\/java.lang:open\n+ *          java.management\n+ *\n+ * @run main\/othervm -Xmx1g -Xlog:gc+stats -Xlog:gc -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseStringDeduplication\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahDegeneratedGC\n+ *      -DtargetStrings=3000000\n+ *      TestStringDedupStress\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestStringDedupStress.java","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -78,0 +79,16 @@\n+\/*\n+ * @test id=generational\n+ * @summary Check that Shenandoah cleans up interned strings\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm -Xmx64m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+ClassUnloadingWithConcurrentMark\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n+ *      TestStringInternCleanup\n+ *\n+ * @run main\/othervm -Xmx64m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+ClassUnloadingWithConcurrentMark\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      TestStringInternCleanup\n+ *\/\n+\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestStringInternCleanup.java","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -59,0 +60,5 @@\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n+ *      TestVerifyJCStress\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestVerifyJCStress.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -29,1 +30,2 @@\n- * @run main\/othervm -Xmx128m -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC TestWrongArrayMember\n+ * @run main\/othervm -Xmx128m -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC                                   TestWrongArrayMember\n+ * @run main\/othervm -Xmx128m -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational TestWrongArrayMember\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestWrongArrayMember.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -90,0 +91,12 @@\n+\/*\n+ * @test id=generational\n+ * @summary Check that MX notifications are reported for all cycles\n+ * @library \/test\/lib \/\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm -Xmx128m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      -Dprecise=false -Dmem.pool=Young\n+ *      TestChurnNotifications\n+ *\/\n+\n@@ -114,0 +127,2 @@\n+    private static final String POOL_NAME = \"Young\".equals(System.getProperty(\"mem.pool\")) ? \"Shenandoah Young Gen\" : \"Shenandoah\";\n+\n@@ -127,2 +142,2 @@\n-                    MemoryUsage before = mapBefore.get(\"Shenandoah\");\n-                    MemoryUsage after = mapAfter.get(\"Shenandoah\");\n+                    MemoryUsage before = mapBefore.get(POOL_NAME);\n+                    MemoryUsage after = mapAfter.get(POOL_NAME);\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/mxbeans\/TestChurnNotifications.java","additions":17,"deletions":2,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -86,0 +87,11 @@\n+\/*\n+ * @test id=generational\n+ * @summary Check that MX notifications are reported for all cycles\n+ * @library \/test\/lib \/\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm -Xmx128m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      TestPauseNotifications\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/mxbeans\/TestPauseNotifications.java","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -126,2 +127,3 @@\n-             {{\"satb\"},    {\"adaptive\", \"compact\", \"static\", \"aggressive\"}},\n-             {{\"passive\"}, {\"passive\"}}\n+             {{\"satb\"},         {\"adaptive\", \"compact\", \"static\", \"aggressive\"}},\n+             {{\"passive\"},      {\"passive\"}},\n+             {{\"generational\"}, {\"adaptive\"}}\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/oom\/TestClassLoaderLeak.java","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -47,2 +48,3 @@\n-        testWith(\"-XX:ShenandoahGCMode=satb\",    Mode.PRODUCT);\n-        testWith(\"-XX:ShenandoahGCMode=passive\", Mode.DIAGNOSTIC);\n+        testWith(\"-XX:ShenandoahGCMode=satb\",         Mode.PRODUCT);\n+        testWith(\"-XX:ShenandoahGCMode=passive\",      Mode.DIAGNOSTIC);\n+        testWith(\"-XX:ShenandoahGCMode=generational\", Mode.EXPERIMENTAL);\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/options\/TestModeUnlock.java","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -182,0 +183,37 @@\n+\/*\n+ * @test id=generational\n+ * @key stress\n+ * @library \/\n+ * @requires vm.gc.Shenandoah\n+ * @requires vm.flavor == \"server\" & !vm.emulatedClient\n+ * @summary Stress the Shenandoah GC by trying to make old objects more likely to be garbage than young objects.\n+ *\n+ * @run main\/othervm\/timeout=200 -Xlog:gc*=info -Xmx1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n+ *      gc.stress.gcbasher.TestGCBasherWithShenandoah 120000\n+ *\n+ * @run main\/othervm\/timeout=200 -Xlog:gc*=info -Xmx1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      gc.stress.gcbasher.TestGCBasherWithShenandoah 120000\n+ *\/\n+\n+ \/*\n+  * @test id=generational-deopt-nmethod\n+  * @key stress\n+  * @library \/\n+  * @requires vm.gc.Shenandoah\n+  * @requires vm.flavor == \"server\" & !vm.emulatedClient & vm.opt.ClassUnloading != false\n+  * @summary Stress Shenandoah GC with nmethod barrier forced deoptimization enabled.\n+  *\n+  * @run main\/othervm\/timeout=200 -Xlog:gc*=info,nmethod+barrier=trace -Xmx1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+  *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+  *      -XX:+DeoptimizeNMethodBarriersALot -XX:-Inline\n+  *      -XX:+ShenandoahVerify\n+  *      gc.stress.gcbasher.TestGCBasherWithShenandoah 120000\n+  *\n+  * @run main\/othervm\/timeout=200 -Xlog:gc*=info,nmethod+barrier=trace -Xmx1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+  *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+  *      -XX:+DeoptimizeNMethodBarriersALot -XX:-Inline\n+  *      gc.stress.gcbasher.TestGCBasherWithShenandoah 120000\n+  *\/\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/gcbasher\/TestGCBasherWithShenandoah.java","additions":38,"deletions":0,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+* Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -105,0 +106,16 @@\n+\/*\n+ * @test id=generational\n+ * @key stress randomness\n+ * @library \/ \/test\/lib\n+ * @requires vm.gc.Shenandoah\n+ * @summary Stress the GC by trying to make old objects more likely to be garbage than young objects.\n+ *\n+ * @run main\/othervm\/timeout=600 -Xmx384M -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n+ *      gc.stress.gcold.TestGCOld 50 1 20 10 10000\n+ *\n+ * @run main\/othervm -Xmx384M -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      gc.stress.gcold.TestGCOld 50 1 20 10 10000\n+ *\/\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/gcold\/TestGCOldWithShenandoah.java","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -43,0 +44,17 @@\n+\n+\/*\n+ * @test id=generational\n+ * @key stress\n+ * @library \/\n+ * @requires vm.gc.Shenandoah\n+ * @summary Stress the Shenandoah GC full GC by allocating objects of different lifetimes concurrently with System.gc().\n+ *\n+ * @run main\/othervm\/timeout=300 -Xlog:gc*=info -Xmx512m -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n+ *      gc.stress.systemgc.TestSystemGCWithShenandoah 270\n+ *\n+ * @run main\/othervm\/timeout=300 -Xlog:gc*=info -Xmx512m -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      gc.stress.systemgc.TestSystemGCWithShenandoah 270\n+ *\/\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/systemgc\/TestSystemGCWithShenandoah.java","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"}]}