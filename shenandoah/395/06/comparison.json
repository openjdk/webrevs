{"files":[{"patch":"@@ -173,0 +173,2 @@\n+\n+    bool need_to_finalize_mixed = false;\n@@ -174,1 +176,1 @@\n-      heap->old_generation()->heuristics()->prime_collection_set(collection_set);\n+      need_to_finalize_mixed = heap->old_generation()->heuristics()->prime_collection_set(collection_set);\n@@ -179,0 +181,12 @@\n+\n+    if (_generation->is_young()) {\n+      \/\/ Especially when young-gen trigger is expedited in order to finish mixed evacuations, there may not be\n+      \/\/ enough consolidated garbage to make effective use of young-gen evacuation reserve.  If there is still\n+      \/\/ young-gen reserve available following selection of the young-gen collection set, see if we can use\n+      \/\/ this memory to expand the old-gen evacuation collection set.\n+      need_to_finalize_mixed |=\n+        heap->old_generation()->heuristics()->top_off_collection_set();\n+      if (need_to_finalize_mixed) {\n+        heap->old_generation()->heuristics()->finalize_mixed_evacs();\n+      }\n+    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.cpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -84,1 +84,2 @@\n-  size_t capacity = heap->young_generation()->max_capacity();\n+  size_t young_capacity = heap->young_generation()->max_capacity();\n+  size_t old_capacity = heap->old_generation()->max_capacity();\n@@ -91,10 +92,0 @@\n-  size_t max_young_cset = (size_t) (young_evac_reserve \/ ShenandoahEvacWaste);\n-  size_t young_cur_cset = 0;\n-  size_t max_old_cset = (size_t) (old_evac_reserve \/ ShenandoahOldEvacWaste);\n-  size_t old_cur_cset = 0;\n-\n-  \/\/ Figure out how many unaffiliated young regions are dedicated to mutator and to evacuator.  Allow the young\n-  \/\/ collector's unaffiliated regions to be transferred to old-gen if old-gen has more easily reclaimed garbage\n-  \/\/ than young-gen.  At the end of this cycle, any excess regions remaining in old-gen will be transferred back\n-  \/\/ to young.  Do not transfer the mutator's unaffiliated regions to old-gen.  Those must remain available\n-  \/\/ to the mutator as it needs to be able to consume this memory during concurrent GC.\n@@ -104,6 +95,24 @@\n-\n-  if (unaffiliated_young_memory > max_young_cset) {\n-    size_t unaffiliated_mutator_memory = unaffiliated_young_memory - max_young_cset;\n-    unaffiliated_young_memory -= unaffiliated_mutator_memory;\n-    unaffiliated_young_regions = unaffiliated_young_memory \/ region_size_bytes; \/\/ round down\n-    unaffiliated_young_memory = unaffiliated_young_regions * region_size_bytes;\n+  size_t unaffiliated_old_regions = heap->old_generation()->free_unaffiliated_regions();\n+  size_t unaffiliated_old_memory = unaffiliated_old_regions * region_size_bytes;\n+\n+  \/\/ Figure out how many unaffiliated regions are dedicated to Collector and OldCollector reserves.  Let these\n+  \/\/ be shuffled between young and old generations in order to expedite evacuation of whichever regions have the\n+  \/\/ most garbage, regardless of whether these garbage-first regions reside in young or old generation.\n+  \/\/ Excess reserves will be transferred back to the mutator after collection set has been chosen.  At the end\n+  \/\/ of evacuation, any reserves not consumed by evacuation will also be transferred to the mutator free set.\n+  size_t shared_reserve_regions = 0;\n+  if (young_evac_reserve > unaffiliated_young_memory) {\n+    young_evac_reserve -= unaffiliated_young_memory;\n+    shared_reserve_regions += unaffiliated_young_memory \/ region_size_bytes;\n+  } else {\n+    size_t delta_regions = young_evac_reserve \/ region_size_bytes;\n+    shared_reserve_regions += delta_regions;\n+    young_evac_reserve -= delta_regions * region_size_bytes;\n+  }\n+  if (old_evac_reserve > unaffiliated_old_memory) {\n+    old_evac_reserve -= unaffiliated_old_memory;\n+    shared_reserve_regions += unaffiliated_old_memory \/ region_size_bytes;\n+  } else {\n+    size_t delta_regions = old_evac_reserve \/ region_size_bytes;\n+    shared_reserve_regions += delta_regions;\n+    old_evac_reserve -= delta_regions * region_size_bytes;\n@@ -112,2 +121,6 @@\n-  \/\/ We'll affiliate these unaffiliated regions with either old or young, depending on need.\n-  max_young_cset -= unaffiliated_young_memory;\n+  size_t shared_reserves = shared_reserve_regions * region_size_bytes;\n+  size_t committed_from_shared_reserves = 0;\n+  size_t max_young_cset = (size_t) (young_evac_reserve \/ ShenandoahEvacWaste);\n+  size_t young_cur_cset = 0;\n+  size_t max_old_cset = (size_t) (old_evac_reserve \/ ShenandoahOldEvacWaste);\n+  size_t old_cur_cset = 0;\n@@ -115,2 +128,3 @@\n-  \/\/ Keep track of how many regions we plan to transfer from young to old.\n-  size_t regions_transferred_to_old = 0;\n+  size_t promo_bytes = 0;\n+  size_t old_evac_bytes = 0;\n+  size_t young_evac_bytes = 0;\n@@ -118,1 +132,3 @@\n-  size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_young_cset;\n+  size_t max_total_cset = (max_young_cset + max_old_cset +\n+                           (size_t) (shared_reserve_regions * region_size_bytes) \/ ShenandoahOldEvacWaste);\n+  size_t free_target = ((young_capacity + old_capacity) * ShenandoahMinFreeThreshold) \/ 100 + max_total_cset;\n@@ -122,1 +138,2 @@\n-                     \"%s, Max Old Evacuation: \" SIZE_FORMAT \"%s, Actual Free: \" SIZE_FORMAT \"%s.\",\n+                     \"%s, Max Old Evacuation: \" SIZE_FORMAT \"%s, Discretionary additional evacuation: \" SIZE_FORMAT\n+                     \"%s, Actual Free: \" SIZE_FORMAT \"%s.\",\n@@ -125,0 +142,1 @@\n+                     byte_size_in_proper_unit(shared_reserves), proper_unit_for_byte_size(shared_reserves),\n@@ -127,0 +145,1 @@\n+  size_t cur_garbage = cur_young_garbage;\n@@ -134,0 +153,3 @@\n+    size_t region_garbage = r->garbage();\n+    size_t new_garbage = cur_garbage + region_garbage;\n+    bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n@@ -135,5 +157,9 @@\n-      size_t new_cset = old_cur_cset + r->get_live_data_bytes();\n-      if ((r->garbage() > garbage_threshold)) {\n-        while ((new_cset > max_old_cset) && (unaffiliated_young_regions > 0)) {\n-          unaffiliated_young_regions--;\n-          regions_transferred_to_old++;\n+      if (add_regardless || (region_garbage > garbage_threshold)) {\n+        size_t live_bytes = r->get_live_data_bytes();\n+        size_t new_cset = old_cur_cset + r->get_live_data_bytes();\n+        \/\/ May need multiple reserve regions to evacuate a single region, depending on live data bytes and ShenandoahOldEvacWaste\n+        size_t orig_max_old_cset = max_old_cset;\n+        size_t proposed_old_region_consumption = 0;\n+        while ((new_cset > max_old_cset) && (committed_from_shared_reserves < shared_reserves)) {\n+          committed_from_shared_reserves += region_size_bytes;\n+          proposed_old_region_consumption++;\n@@ -142,4 +168,15 @@\n-      }\n-      if ((new_cset <= max_old_cset) && (r->garbage() > garbage_threshold)) {\n-        add_region = true;\n-        old_cur_cset = new_cset;\n+        \/\/ We already know: add_regardless || region_garbage > garbage_threshold\n+        if (new_cset <= max_old_cset) {\n+          add_region = true;\n+          old_cur_cset = new_cset;\n+          cur_garbage = new_garbage;\n+          if (r->is_old()) {\n+            old_evac_bytes += live_bytes;\n+          } else {\n+            promo_bytes += live_bytes;\n+          }\n+        } else {\n+          \/\/ We failed to sufficiently expand old, so unwind proposed expansion\n+          max_old_cset = orig_max_old_cset;\n+          committed_from_shared_reserves -= proposed_old_region_consumption * region_size_bytes;\n+        }\n@@ -149,8 +186,9 @@\n-      size_t new_cset = young_cur_cset + r->get_live_data_bytes();\n-      size_t region_garbage = r->garbage();\n-      size_t new_garbage = cur_young_garbage + region_garbage;\n-      bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n-\n-      if (add_regardless || (r->garbage() > garbage_threshold)) {\n-        while ((new_cset > max_young_cset) && (unaffiliated_young_regions > 0)) {\n-          unaffiliated_young_regions--;\n+      if (add_regardless || (region_garbage > garbage_threshold)) {\n+        size_t live_bytes = r->get_live_data_bytes();\n+        size_t new_cset = young_cur_cset + live_bytes;\n+        \/\/ May need multiple reserve regions to evacuate a single region, depending on live data bytes and ShenandoahEvacWaste\n+        size_t orig_max_young_cset = max_young_cset;\n+        size_t proposed_young_region_consumption = 0;\n+        while ((new_cset > max_young_cset) && (committed_from_shared_reserves < shared_reserves)) {\n+          committed_from_shared_reserves += region_size_bytes;\n+          proposed_young_region_consumption++;\n@@ -159,5 +197,11 @@\n-      }\n-      if ((new_cset <= max_young_cset) && (add_regardless || (region_garbage > garbage_threshold))) {\n-        add_region = true;\n-        young_cur_cset = new_cset;\n-        cur_young_garbage = new_garbage;\n+        \/\/ We already know: add_regardless || region_garbage > garbage_threshold\n+        if (new_cset <= max_young_cset) {\n+          add_region = true;\n+          young_cur_cset = new_cset;\n+          cur_garbage = new_garbage;\n+          young_evac_bytes += live_bytes;\n+        } else {\n+          \/\/ We failed to sufficiently expand young, so unwind proposed expansion\n+          max_young_cset = orig_max_young_cset;\n+          committed_from_shared_reserves -= proposed_young_region_consumption * region_size_bytes;\n+        }\n@@ -171,5 +215,3 @@\n-  if (regions_transferred_to_old > 0) {\n-    heap->generation_sizer()->force_transfer_to_old(regions_transferred_to_old);\n-    heap->young_generation()->set_evacuation_reserve(young_evac_reserve - regions_transferred_to_old * region_size_bytes);\n-    heap->old_generation()->set_evacuation_reserve(old_evac_reserve + regions_transferred_to_old * region_size_bytes);\n-  }\n+  heap->young_generation()->set_evacuation_reserve((size_t) (young_evac_bytes * ShenandoahEvacWaste));\n+  heap->old_generation()->set_evacuation_reserve((size_t) (old_evac_bytes * ShenandoahOldEvacWaste));\n+  heap->old_generation()->set_promoted_reserve((size_t) (promo_bytes * ShenandoahPromoEvacWaste));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGlobalHeuristics.cpp","additions":92,"deletions":50,"binary":false,"changes":142,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n@@ -33,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -78,3 +80,4 @@\n-bool ShenandoahOldHeuristics::prime_collection_set(ShenandoahCollectionSet* collection_set) {\n-  if (unprocessed_old_collection_candidates() == 0) {\n-    return false;\n+bool ShenandoahOldHeuristics::all_candidates_are_pinned() {\n+#ifdef ASSERT\n+  if (uint(os::random()) % 100 < ShenandoahCoalesceChance) {\n+    return true;\n@@ -82,0 +85,1 @@\n+#endif\n@@ -83,28 +87,4 @@\n-  _first_pinned_candidate = NOT_FOUND;\n-\n-  uint included_old_regions = 0;\n-  size_t evacuated_old_bytes = 0;\n-  size_t collected_old_bytes = 0;\n-\n-  \/\/ If a region is put into the collection set, then this region's free (not yet used) bytes are no longer\n-  \/\/ \"available\" to hold the results of other evacuations.  This may cause a decrease in the remaining amount\n-  \/\/ of memory that can still be evacuated.  We address this by reducing the evacuation budget by the amount\n-  \/\/ of live memory in that region and by the amount of unallocated memory in that region if the evacuation\n-  \/\/ budget is constrained by availability of free memory.\n-  const size_t old_evacuation_reserve = _old_generation->get_evacuation_reserve();\n-  const size_t old_evacuation_budget = (size_t) ((double) old_evacuation_reserve \/ ShenandoahOldEvacWaste);\n-  size_t unfragmented_available = _old_generation->free_unaffiliated_regions() * ShenandoahHeapRegion::region_size_bytes();\n-  size_t fragmented_available;\n-  size_t excess_fragmented_available;\n-\n-  if (unfragmented_available > old_evacuation_budget) {\n-    unfragmented_available = old_evacuation_budget;\n-    fragmented_available = 0;\n-    excess_fragmented_available = 0;\n-  } else {\n-    assert(_old_generation->available() >= old_evacuation_budget, \"Cannot budget more than is available\");\n-    fragmented_available = _old_generation->available() - unfragmented_available;\n-    assert(fragmented_available + unfragmented_available >= old_evacuation_budget, \"Budgets do not add up\");\n-    if (fragmented_available + unfragmented_available > old_evacuation_budget) {\n-      excess_fragmented_available = (fragmented_available + unfragmented_available) - old_evacuation_budget;\n-      fragmented_available -= excess_fragmented_available;\n+  for (uint i = _next_old_collection_candidate; i < _last_old_collection_candidate; ++i) {\n+    ShenandoahHeapRegion* region = _region_data[i]._region;\n+    if (!region->is_pinned()) {\n+      return false;\n@@ -113,0 +93,2 @@\n+  return true;\n+}\n@@ -114,6 +96,5 @@\n-  size_t remaining_old_evacuation_budget = old_evacuation_budget;\n-  log_info(gc)(\"Choose old regions for mixed collection: old evacuation budget: \" SIZE_FORMAT \"%s, candidates: %u\",\n-               byte_size_in_proper_unit(old_evacuation_budget), proper_unit_for_byte_size(old_evacuation_budget),\n-               unprocessed_old_collection_candidates());\n-\n-  size_t lost_evacuation_capacity = 0;\n+bool ShenandoahOldHeuristics::add_old_regions_to_cset() {\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    return false;\n+  }\n+  _first_pinned_candidate = NOT_FOUND;\n@@ -138,6 +119,4 @@\n-\n-    if ((lost_available > 0) && (excess_fragmented_available > 0)) {\n-      if (lost_available < excess_fragmented_available) {\n-        excess_fragmented_available -= lost_available;\n-        lost_evacuation_capacity -= lost_available;\n-        lost_available  = 0;\n+    if ((lost_available > 0) && (_excess_fragmented_available > 0)) {\n+      if (lost_available < _excess_fragmented_available) {\n+        _excess_fragmented_available -= lost_available;\n+        lost_available = 0;\n@@ -145,3 +124,2 @@\n-        lost_available -= excess_fragmented_available;\n-        lost_evacuation_capacity -= excess_fragmented_available;\n-        excess_fragmented_available = 0;\n+        lost_available -= _excess_fragmented_available;\n+        _excess_fragmented_available = 0;\n@@ -150,0 +128,3 @@\n+\n+    ssize_t fragmented_delta = 0;\n+    ssize_t unfragmented_delta = 0;\n@@ -151,3 +132,4 @@\n-    if ((lost_available > 0) && (fragmented_available > 0)) {\n-      if (scaled_loss + live_data_for_evacuation < fragmented_available) {\n-        fragmented_available -= scaled_loss;\n+    if ((lost_available > 0) && (_fragmented_available > 0)) {\n+      if (scaled_loss < _fragmented_available) {\n+        _fragmented_available -= scaled_loss;\n+        fragmented_delta = -scaled_loss;\n@@ -156,2 +138,3 @@\n-        \/\/ We will have to allocate this region's evacuation memory from unfragmented memory, so don't bother\n-        \/\/ to decrement scaled_loss\n+        scaled_loss -= _fragmented_available;\n+        fragmented_delta = -_fragmented_available;\n+        _fragmented_available = 0;\n@@ -160,9 +143,4 @@\n-    if (scaled_loss > 0) {\n-      \/\/ We were not able to account for the lost free memory within fragmented memory, so we need to take this\n-      \/\/ allocation out of unfragmented memory.  Unfragmented memory does not need to account for loss of free.\n-      if (live_data_for_evacuation > unfragmented_available) {\n-        \/\/ There is not room to evacuate this region or any that come after it in within the candidates array.\n-        break;\n-      } else {\n-        unfragmented_available -= live_data_for_evacuation;\n-      }\n+    \/\/ Allocate replica from unfragmented memory if that exists\n+    size_t evacuation_need = live_data_for_evacuation;\n+    if (evacuation_need < _unfragmented_available) {\n+      _unfragmented_available -= evacuation_need;;\n@@ -170,10 +148,4 @@\n-      \/\/ Since scaled_loss == 0, we have accounted for the loss of free memory, so we can allocate from either\n-      \/\/ fragmented or unfragmented available memory.  Use up the fragmented memory budget first.\n-      size_t evacuation_need = live_data_for_evacuation;\n-\n-      if (evacuation_need > fragmented_available) {\n-        evacuation_need -= fragmented_available;\n-        fragmented_available = 0;\n-      } else {\n-        fragmented_available -= evacuation_need;\n-        evacuation_need = 0;\n+      if (_unfragmented_available > 0) {\n+        evacuation_need -= _unfragmented_available;\n+        unfragmented_delta = -_unfragmented_available;\n+        _unfragmented_available = 0;\n@@ -181,3 +153,3 @@\n-      if (evacuation_need > unfragmented_available) {\n-        \/\/ There is not room to evacuate this region or any that come after it in within the candidates array.\n-        break;\n+      \/\/ Take the remaining allocation out of fragmented available\n+      if (_fragmented_available > evacuation_need) {\n+        _fragmented_available -= evacuation_need;\n@@ -185,2 +157,4 @@\n-        unfragmented_available -= evacuation_need;\n-        \/\/ dead code: evacuation_need == 0;\n+        \/\/ We cannot add this region into the collection set.  We're done.  Undo the adjustments to available.\n+        _fragmented_available -= fragmented_delta;\n+        _unfragmented_available -= unfragmented_delta;\n+        break;\n@@ -189,4 +163,4 @@\n-    collection_set->add_region(r);\n-    included_old_regions++;\n-    evacuated_old_bytes += live_data_for_evacuation;\n-    collected_old_bytes += r->garbage();\n+    _mixed_evac_cset->add_region(r);\n+    _included_old_regions++;\n+    _evacuated_old_bytes += live_data_for_evacuation;\n+    _collected_old_bytes += r->garbage();\n@@ -195,0 +169,2 @@\n+  return true;\n+}\n@@ -196,0 +172,1 @@\n+bool ShenandoahOldHeuristics::finalize_mixed_evacs() {\n@@ -200,6 +177,6 @@\n-  decrease_unprocessed_old_collection_candidates_live_memory(evacuated_old_bytes);\n-  if (included_old_regions > 0) {\n-    log_info(gc)(\"Old-gen piggyback evac (\" UINT32_FORMAT \" regions, evacuating \" SIZE_FORMAT \"%s, reclaiming: \" SIZE_FORMAT \"%s)\",\n-                 included_old_regions,\n-                 byte_size_in_proper_unit(evacuated_old_bytes), proper_unit_for_byte_size(evacuated_old_bytes),\n-                 byte_size_in_proper_unit(collected_old_bytes), proper_unit_for_byte_size(collected_old_bytes));\n+  decrease_unprocessed_old_collection_candidates_live_memory(_evacuated_old_bytes);\n+  if (_included_old_regions > 0) {\n+    log_info(gc)(\"Old-gen mixed evac (\" SIZE_FORMAT \" regions, evacuating \" SIZE_FORMAT \"%s, reclaiming: \" SIZE_FORMAT \"%s)\",\n+                 _included_old_regions,\n+                 byte_size_in_proper_unit(_evacuated_old_bytes), proper_unit_for_byte_size(_evacuated_old_bytes),\n+                 byte_size_in_proper_unit(_collected_old_bytes), proper_unit_for_byte_size(_collected_old_bytes));\n@@ -212,1 +189,0 @@\n-\n@@ -214,1 +190,1 @@\n-  } else if (included_old_regions == 0) {\n+  } else if (_included_old_regions == 0) {\n@@ -225,6 +201,2 @@\n-                   \"Old evacuation budget: \" PROPERFMT \", Remaining evacuation budget: \" PROPERFMT\n-                   \", Lost capacity: \" PROPERFMT\n-                   \", Next candidate: \" UINT32_FORMAT \", Last candidate: \" UINT32_FORMAT,\n-                   PROPERFMTARGS(old_evacuation_reserve),\n-                   PROPERFMTARGS(remaining_old_evacuation_budget),\n-                   PROPERFMTARGS(lost_evacuation_capacity),\n+                   \"Old evacuation budget: \" PROPERFMT \", Next candidate: \" UINT32_FORMAT \", Last candidate: \" UINT32_FORMAT,\n+                   PROPERFMTARGS(_old_evacuation_reserve),\n@@ -234,2 +206,1 @@\n-\n-  return (included_old_regions > 0);\n+  return (_included_old_regions > 0);\n@@ -238,4 +209,43 @@\n-bool ShenandoahOldHeuristics::all_candidates_are_pinned() {\n-#ifdef ASSERT\n-  if (uint(os::random()) % 100 < ShenandoahCoalesceChance) {\n-    return true;\n+bool ShenandoahOldHeuristics::prime_collection_set(ShenandoahCollectionSet* collection_set) {\n+  _mixed_evac_cset = collection_set;\n+  _included_old_regions = 0;\n+  _evacuated_old_bytes = 0;\n+  _collected_old_bytes = 0;\n+\n+  \/\/ If a region is put into the collection set, then this region's free (not yet used) bytes are no longer\n+  \/\/ \"available\" to hold the results of other evacuations.  This may cause a decrease in the remaining amount\n+  \/\/ of memory that can still be evacuated.  We address this by reducing the evacuation budget by the amount\n+  \/\/ of live memory in that region and by the amount of unallocated memory in that region if the evacuation\n+  \/\/ budget is constrained by availability of free memory.\n+  _old_evacuation_reserve = _old_generation->get_evacuation_reserve();\n+  _old_evacuation_budget = (size_t) ((double) _old_evacuation_reserve \/ ShenandoahOldEvacWaste);\n+\n+  \/\/ fragmented_available is the amount of memory within partially consumed old regions that may be required to\n+  \/\/ hold the results of old evacuations.  If all of the memory required by the old evacuation reserve is available\n+  \/\/ in unfragmented regions (unaffiliated old regions), then fragmented_available is zero because we do not need\n+  \/\/ to evacuate into the existing partially consumed old regions.\n+\n+  \/\/ if fragmented_available is non-zero, excess_fragmented_available represents the amount of fragmented memory\n+  \/\/ that is available within old, but is not required to hold the resuilts of old evacuation.  As old-gen regions\n+  \/\/ are added into the collection set, their free memory is subtracted from excess_fragmented_available until the\n+  \/\/ excess is exhausted.  For old-gen regions subsequently added to the collection set, their free memory is\n+  \/\/ subtracted from fragmented_available and from the old_evacuation_budget (since the budget decreases when this\n+  \/\/ fragmented_available memory decreases).  After fragmented_available has been exhausted, any further old regions\n+  \/\/ selected for the cset do not further decrease the old_evacuation_budget because all further evacuation is targeted\n+  \/\/ to unfragmented regions.\n+\n+  size_t unaffiliated_available = _old_generation->free_unaffiliated_regions() * ShenandoahHeapRegion::region_size_bytes();\n+  if (unaffiliated_available > _old_evacuation_reserve) {\n+    _unfragmented_available = _old_evacuation_budget;\n+    _fragmented_available = 0;\n+    _excess_fragmented_available = 0;\n+  } else {\n+    assert(_old_generation->available() >= _old_evacuation_reserve, \"Cannot reserve more than is available\");\n+    size_t affiliated_available = _old_generation->available() - unaffiliated_available;\n+    assert(affiliated_available + unaffiliated_available >= _old_evacuation_reserve, \"Budgets do not add up\");\n+    if (affiliated_available + unaffiliated_available > _old_evacuation_reserve) {\n+      _excess_fragmented_available = (affiliated_available + unaffiliated_available) - _old_evacuation_reserve;\n+      affiliated_available -= _excess_fragmented_available;\n+    }\n+    _fragmented_available = (size_t) ((double) affiliated_available \/ ShenandoahOldEvacWaste);\n+    _unfragmented_available = (size_t) ((double) unaffiliated_available \/ ShenandoahOldEvacWaste);\n@@ -243,1 +253,5 @@\n-#endif\n+  log_info(gc)(\"Choose old regions for mixed collection: old evacuation budget: \" SIZE_FORMAT \"%s, candidates: %u\",\n+               byte_size_in_proper_unit(_old_evacuation_budget), proper_unit_for_byte_size(_old_evacuation_budget),\n+               unprocessed_old_collection_candidates());\n+  return add_old_regions_to_cset();\n+}\n@@ -245,3 +259,13 @@\n-  for (uint i = _next_old_collection_candidate; i < _last_old_collection_candidate; ++i) {\n-    ShenandoahHeapRegion* region = _region_data[i]._region;\n-    if (!region->is_pinned()) {\n+bool ShenandoahOldHeuristics::top_off_collection_set() {\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    return false;\n+  } else {\n+    ShenandoahYoungGeneration* young_generation = _heap->young_generation();\n+    size_t young_unaffiliated_regions = young_generation->free_unaffiliated_regions();\n+    size_t max_young_cset = young_generation->get_evacuation_reserve();\n+    size_t planned_young_evac = _mixed_evac_cset->get_young_bytes_reserved_for_evacuation();\n+    size_t consumed_from_young_cset = (size_t) (planned_young_evac * ShenandoahEvacWaste);\n+    size_t available_to_loan_from_young_reserve = ((consumed_from_young_cset >= max_young_cset)?\n+                                                   0: max_young_cset - consumed_from_young_cset);\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    if ((young_unaffiliated_regions == 0) || (available_to_loan_from_young_reserve < region_size_bytes)) {\n@@ -249,0 +273,17 @@\n+    } else {\n+      size_t regions_for_old_expansion = (available_to_loan_from_young_reserve \/ region_size_bytes);\n+      if (regions_for_old_expansion > young_unaffiliated_regions) {\n+        regions_for_old_expansion = young_unaffiliated_regions;\n+      }\n+      log_info(gc)(\"Augmenting old-gen evacuation budget from unexpended young-generation reserve by \" SIZE_FORMAT \" regions\",\n+                   regions_for_old_expansion);\n+      _heap->generation_sizer()->force_transfer_to_old(regions_for_old_expansion);\n+      size_t budget_supplement = region_size_bytes * regions_for_old_expansion;\n+      size_t supplement_after_waste = (size_t) (((double) budget_supplement) \/ ShenandoahOldEvacWaste);\n+      _old_evacuation_budget += supplement_after_waste;\n+      _unfragmented_available += supplement_after_waste;\n+\n+      _old_generation->augment_evacuation_reserve(budget_supplement);\n+      young_generation->set_evacuation_reserve(max_young_cset - budget_supplement);\n+\n+      return add_old_regions_to_cset();\n@@ -251,1 +292,0 @@\n-  return true;\n@@ -318,1 +358,3 @@\n-\n+#ifdef ASSERT\n+  bool reclaimed_immediate = false;\n+#endif\n@@ -331,4 +373,4 @@\n-        \/\/ Only place regular or pinned regions with live data into the candidate set.\n-        \/\/ Pinned regions cannot be evacuated, but we are not actually choosing candidates\n-        \/\/ for the collection set here. That happens later during the next young GC cycle,\n-        \/\/ by which time, the pinned region may no longer be pinned.\n+      \/\/ Only place regular or pinned regions with live data into the candidate set.\n+      \/\/ Pinned regions cannot be evacuated, but we are not actually choosing candidates\n+      \/\/ for the collection set here. That happens later during the next young GC cycle,\n+      \/\/ by which time, the pinned region may no longer be pinned.\n@@ -337,0 +379,8 @@\n+#ifdef ASSERT\n+        if (!reclaimed_immediate) {\n+          reclaimed_immediate = true;\n+          \/\/ Inform the free-set that old trash regions may temporarily violate OldCollector bounds\n+          shenandoah_assert_heaplocked();\n+          heap->free_set()->advise_of_old_trash();\n+        }\n+#endif\n@@ -356,0 +406,8 @@\n+#ifdef ASSERT\n+        if (!reclaimed_immediate) {\n+          reclaimed_immediate = true;\n+          \/\/ Inform the free-set that old trash regions may temporarily violate OldCollector bounds\n+          shenandoah_assert_heaplocked();\n+          heap->free_set()->advise_of_old_trash();\n+        }\n+#endif\n@@ -541,0 +599,1 @@\n+  _live_bytes_in_unprocessed_candidates = 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":168,"deletions":109,"binary":false,"changes":277,"status":"modified"},{"patch":"@@ -106,0 +106,14 @@\n+  \/\/ State variables involved in construction of a mixed-evacuation collection set.  These variables are initialized\n+  \/\/ when client code invokes prime_collection_set().  They are consulted, and sometimes modified, when client code\n+  \/\/ calls top_off_collection_set() to possibly expand the number old-gen regions in a mixed evacuation cset, and by\n+  \/\/ finalize_mixed_evacs(), which prepares the way for mixed evacuations to begin.\n+  ShenandoahCollectionSet* _mixed_evac_cset;\n+  size_t _evacuated_old_bytes;\n+  size_t _collected_old_bytes;\n+  size_t _included_old_regions;\n+  size_t _old_evacuation_reserve;\n+  size_t _old_evacuation_budget;\n+  size_t _unfragmented_available;\n+  size_t _fragmented_available;\n+  size_t _excess_fragmented_available;\n+\n@@ -130,0 +144,7 @@\n+  \/\/ This internal helper route adds as many mixed evacuation candidate regions as fit within the old-gen evacuation budget\n+  \/\/ to the collection set.  This may be called twice to prepare for any given mixed evacuation cycle, the first time with\n+  \/\/ a conservative old evacuation budget, and the second time with a larger more aggressive old evacuation budget.  Returns\n+  \/\/ true iff we need to finalize mixed evacs.  (If no regions are added to the collection set, there is no need to finalize\n+  \/\/ mixed evacuations.)\n+  bool add_old_regions_to_cset();\n+\n@@ -136,2 +157,15 @@\n-  \/\/ Return true iff the collection set is primed with at least one old-gen region.\n-  bool prime_collection_set(ShenandoahCollectionSet* set);\n+  \/\/ Initialize instance variables to support the preparation of a mixed-evacuation collection set.  Adds as many\n+  \/\/ old candidate regions into the collection set as can fit within the iniital conservative old evacuation budget.\n+  \/\/ Returns true iff we need to finalize mixed evacs.\n+  bool prime_collection_set(ShenandoahCollectionSet* collection_set);\n+\n+  \/\/ If young evacuation did not consume all of its available evacuation reserve, add as many additional mixed-\n+  \/\/ evacuation candidate regions into the collection set as will fit within this excess repurposed reserved.\n+  \/\/ Returns true iff we need to finalize mixed evacs.\n+  bool top_off_collection_set();\n+\n+  \/\/ Having added all eligible mixed-evacuation candidates to the collection set, this function updates the total count\n+  \/\/ of how much old-gen memory remains to be evacuated and adjusts the representation of old-gen regions that remain to\n+  \/\/ be evacuated, giving special attention to regions that are currently pinned.  It outputs relevant log messages and\n+  \/\/ returns true iff the collection set holds at least one unpinned mixed evacuation candidate.\n+  bool finalize_mixed_evacs();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp","additions":36,"deletions":2,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -96,1 +96,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -215,0 +215,1 @@\n+    \/\/ vmop_entry_final_updaterefs rebuilds free set in preparation for next GC.\n@@ -224,1 +225,2 @@\n-    vmop_entry_final_roots();\n+\n+    \/\/ vmop_entry_final_updaterefs rebuilds free set in preparation for next GC.\n@@ -226,0 +228,1 @@\n+    vmop_entry_final_roots();\n@@ -336,1 +339,1 @@\n-\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n@@ -338,0 +341,28 @@\n+\n+  if (heap->mode()->is_generational()) {\n+    assert (_abbreviated || _generation->is_old(), \"Only rebuild free set for abbreviated and old-marking cycles\");\n+    \/\/ After concurrent old marking finishes and after an abbreviated cycle, we reclaim immediate garbage.\n+    \/\/ Further, we may also want to expand OLD in order to make room for anticipated promotions and\/or for mixed\n+    \/\/ evacuations.  Mixed evacuations are especially likely to following the end of OLD marking.\n+    {\n+      ShenandoahHeapLocker locker(heap->lock());\n+      ShenandoahGenerationalHeap* const gen_heap = ShenandoahGenerationalHeap::heap();\n+      size_t young_cset_regions, old_cset_regions;\n+      size_t first_old, last_old, num_old;\n+      size_t allocation_runway = heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(0);\n+      heap->free_set()->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+      assert((young_cset_regions == 0) && (old_cset_regions == 0),\n+             \"No ongoing evacuation after abbreviated or concurrent OLD marking cycle\");\n+      gen_heap->compute_old_generation_balance(allocation_runway, 0, 0);\n+      heap->free_set()->finish_rebuild(0, 0, num_old);\n+    }\n+  } else {\n+    assert (_abbreviated, \"Only rebuild free set for abbreviated\");\n+    \/\/ Rebuild free set after reclaiming immediate garbage\n+    ShenandoahHeapLocker locker(heap->lock());\n+    size_t young_cset_regions, old_cset_regions;\n+    size_t first_old, last_old, num_old;\n+    heap->free_set()->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+    assert((young_cset_regions == 0) && (old_cset_regions == 0), \"No ongoing evacuation after abbreviated cycle\");\n+    heap->free_set()->finish_rebuild(0, 0, num_old);\n+  }\n@@ -709,1 +740,1 @@\n-    \/\/ heap->get_promoted_reserve() represents the amount of memory within old-gen's available memory that has\n+    \/\/ old_generation->get_promoted_reserve() represents the amount of memory within old-gen's available memory that has\n@@ -715,1 +746,1 @@\n-    \/\/ heap->get_old_evac_reserve() represents the amount of memory within old-gen's available memory that has been\n+    \/\/ old_generation->get_evacuation_reserve() represents the amount of memory within old-gen's available memory that has been\n@@ -718,1 +749,1 @@\n-    \/\/ heap->get_young_evac_reserve() represents the amount of memory within young-gen's available memory that has\n+    \/\/ young_generation->get_evacuation_reserve() represents the amount of memory within young-gen's available memory that has\n@@ -1199,1 +1230,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":36,"deletions":6,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -276,0 +276,25 @@\n+inline void ShenandoahRegionPartitions::adjust_interval_for_recycled_old_region(ShenandoahHeapRegion* r) {\n+  assert(!r->is_trash() && (r->free() == _region_size_bytes), \"Bad argument\");\n+  idx_t idx = (idx_t) r->index();\n+  \/\/ Note that a recycled old trashed region may be in any one of the free set partitions according to the following scenarios:\n+  \/\/  1. The old region had already been retired, so it was NotFree, and we have not rebuilt free set, so region is still NotFree\n+  \/\/  2. We recycled the region but we have not yet rebuilt the free set, so it is still in the OldCollector region.\n+  \/\/  3. We have found regions with alloc capacity but have not yet reserved_regions, so this is in Mutator set, and\n+  \/\/     the act of placing the region into the Mutator set properly adjusts interval for Mutator set.\n+  \/\/  4. During reserve_regions(), we moved this region into the Collector set, and the act of placing this region into\n+  \/\/     Collector set properly adjusts the interval for the Collector set.\n+  \/\/  5. During reserve_regions, we moved this region into the OldCollector set, and the act of placing this region into\n+  \/\/     OldCollector set properly adjusts the interval for the OldCollector set.\n+  \/\/ Only case 2 needs to be fixed up here.\n+  ShenandoahFreeSetPartitionId old_partition = ShenandoahFreeSetPartitionId::OldCollector;\n+  if (_membership[int(old_partition)].is_set(idx)) {\n+    assert(_leftmosts[int(old_partition)] <= idx && _rightmosts[int(old_partition)] >= idx, \"sanity\");\n+    if (_leftmosts_empty[int(old_partition)] > idx) {\n+      _leftmosts_empty[int(old_partition)] = idx;\n+    }\n+    if (_rightmosts_empty[int(old_partition)] < idx) {\n+      _rightmosts_empty[int(old_partition)] = idx;\n+    }\n+  }\n+}\n+\n@@ -519,1 +544,1 @@\n-void ShenandoahRegionPartitions::assert_bounds() {\n+void ShenandoahRegionPartitions::assert_bounds(bool old_trash_not_in_bounds) {\n@@ -545,0 +570,21 @@\n+        ShenandoahHeapRegion* r = ShenandoahHeap::heap()->get_region(i);\n+\n+        \/\/ When old_trash_not_in_bounds, an old trashed region might reside in:\n+        \/\/ 1. NotFree if the region had already been retired\n+        \/\/ 2. OldCollector because the region was originally in OldCollector when it was identified as immediate garbage, or\n+        \/\/ 3. Mutator because we have run find_regions_with_alloc_capacity(), or\n+        \/\/ 4. Collector because reserve_regions moved from Mutator to Collector but we have not yet recycled the trash\n+        \/\/ 5. OldCollector because reserve_regions moved from Mutator to OldCollector but we have not yet recycled the trash\n+\n+        \/\/ In case 1, there is no issue with empty-free intervals.\n+        \/\/ In cases 3 - 5, there is no issue with empty-free intervals because the act of moving the region into the partition\n+        \/\/    causes the empty-free interval to be updated.\n+        \/\/ Only in case 2 do we need to disable the assert checking, but it is difficult to distinguish case 2 from case 5,\n+        \/\/    so we do not assert bounds for case 2 or case 5.\n+\n+        if (old_trash_not_in_bounds && (partition == ShenandoahFreeSetPartitionId::OldCollector) && r->is_old() && r->is_trash()) {\n+          \/\/ If Old trash has been identified but we have not yet rebuilt the freeset to acount for the trashed regions,\n+          \/\/ or if old trash has not yet been recycled, do not expect these trash regions to be within the OldCollector\n+          \/\/ partition's bounds.\n+          continue;\n+        }\n@@ -592,6 +638,6 @@\n-  assert (beg_off >= leftmost_empty(ShenandoahFreeSetPartitionId::Mutator),\n-          \"free empty regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n-          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n-  assert (end_off <= rightmost_empty(ShenandoahFreeSetPartitionId::Mutator),\n-          \"free empty regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n-          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n+  assert (beg_off >= _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n+          \"free empty region (\" SSIZE_FORMAT \") before the leftmost bound \" SSIZE_FORMAT,\n+          beg_off, _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)]);\n+  assert (end_off <= _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n+          \"free empty region (\" SSIZE_FORMAT \") past the rightmost bound \" SSIZE_FORMAT,\n+          end_off, _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)]);\n@@ -626,2 +672,2 @@\n-          \"free empty regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n-          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::Collector));\n+          \"free empty region (\" SSIZE_FORMAT \") before the leftmost bound \" SSIZE_FORMAT,\n+          beg_off, _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)]);\n@@ -629,2 +675,2 @@\n-          \"free empty regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n-          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::Collector));\n+          \"free empty region (\" SSIZE_FORMAT \") past the rightmost bound \" SSIZE_FORMAT,\n+          end_off, _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)]);\n@@ -661,2 +707,2 @@\n-          \"free empty regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n-          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n+          \"free empty region (\" SSIZE_FORMAT \") before the leftmost bound \" SSIZE_FORMAT,\n+          beg_off, _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)]);\n@@ -664,2 +710,2 @@\n-          \"free empty regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n-          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n+          \"free empty region (\" SSIZE_FORMAT \") past the rightmost bound \" SSIZE_FORMAT,\n+          end_off, _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)]);\n@@ -672,0 +718,3 @@\n+#ifdef ASSERT\n+  _old_trash_not_in_bounds(false),\n+#endif\n@@ -1115,1 +1164,3 @@\n-    _partitions.assert_bounds();\n+#ifdef ASSERT\n+    _partitions.assert_bounds(_old_trash_not_in_bounds);\n+#endif\n@@ -1225,1 +1276,3 @@\n-  _partitions.assert_bounds();\n+#ifdef ASSERT\n+  _partitions.assert_bounds(_old_trash_not_in_bounds);\n+#endif\n@@ -1247,0 +1300,7 @@\n+#ifdef ASSERT\n+      \/\/ Note: if assertions are not enforced, there's no rush to adjust this interval.  We'll adjust the\n+      \/\/ interval when we eventually rebuild the free set.\n+      if (_old_trash_not_in_bounds) {\n+        _partitions.adjust_interval_for_recycled_old_region(r);\n+      }\n+#endif\n@@ -1250,0 +1310,4 @@\n+#ifdef ASSERT\n+  ShenandoahHeapLocker locker(_heap->lock());\n+  _old_trash_not_in_bounds = false;\n+#endif\n@@ -1262,1 +1326,3 @@\n-  _partitions.assert_bounds();\n+#ifdef ASSERT\n+  _partitions.assert_bounds(_old_trash_not_in_bounds);\n+#endif\n@@ -1282,2 +1348,3 @@\n-  _partitions.assert_bounds();\n-\n+#ifdef ASSERT\n+  _partitions.assert_bounds(_old_trash_not_in_bounds);\n+#endif\n@@ -1333,2 +1400,2 @@\n-      \/\/ Trashed regions represent regions that had been in the collection partition but have not yet been \"cleaned up\".\n-      \/\/ The cset regions are not \"trashed\" until we have finished update refs.\n+      \/\/ Trashed regions represent regions that had been in the collection set (or may have been identified as immediate garbage)\n+      \/\/ but have not yet been \"cleaned up\".  The cset regions are not \"trashed\" until we have finished update refs.\n@@ -1336,0 +1403,7 @@\n+        ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+        \/\/ We're going to place this region into the Mutator set.  We increment old_cset_regions because this count represents\n+        \/\/ regions that the old generation is entitled to without any transfer from young.  We do not place this region into\n+        \/\/ the OldCollector partition at this time.  Instead, we let reserve_regions() decide whether to place this region\n+        \/\/ into the OldCollector partition.  Deferring the decision allows reserve_regions() to more effectively pack the\n+        \/\/ OldCollector regions into high-address memory.  We do not adjust capacities of old and young generations at this\n+        \/\/ time.  At the end of finish_rebuild(), the capacities are adjusted based on the results of reserve_regions().\n@@ -1356,1 +1430,1 @@\n-          \/\/ Both young and old collected regions (trashed) are placed into the Mutator set\n+          \/\/ Young and old (possibly immediately) collected regions (trashed) are placed into the Mutator set\n@@ -1492,1 +1566,1 @@\n-      ShenandoahGenerationalHeap::cast(_heap)->generation_sizer()->transfer_to_young(old_collector_regions);\n+      ShenandoahGenerationalHeap::cast(_heap)->generation_sizer()->force_transfer_to_young(old_collector_regions);\n@@ -1535,0 +1609,1 @@\n+    size_t original_old_regions = old_gen->max_capacity() \/ region_size_bytes;\n@@ -1537,0 +1612,11 @@\n+\n+    if (original_old_regions > old_region_count) {\n+      log_info(gc)(\"Transfer \" SIZE_FORMAT \" regions from OLD to YOUNG during rebuild of freeset\",\n+                   original_old_regions - old_region_count);\n+    } else if (original_old_regions < old_region_count) {\n+      log_info(gc)(\"Transfer \" SIZE_FORMAT \" regions from YOUNG to OLD during rebuild of freeset\",\n+                   old_region_count - original_old_regions);\n+    }\n+\n+    \/\/ Having transferred regions based on results of rebuild(), reset the rebalance request.\n+    old_gen->set_region_balance(0);\n@@ -1540,2 +1626,1 @@\n-void ShenandoahFreeSet::finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t old_region_count,\n-                                       bool have_evacuation_reserves) {\n+void ShenandoahFreeSet::finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t old_region_count) {\n@@ -1546,2 +1631,1 @@\n-    compute_young_and_old_reserves(young_cset_regions, old_cset_regions, have_evacuation_reserves,\n-                                   young_reserve, old_reserve);\n+    compute_young_and_old_reserves(young_cset_regions, old_cset_regions, young_reserve, old_reserve);\n@@ -1553,1 +1637,1 @@\n-  \/\/ Move some of the mutator regions in the Collector and OldCollector partitions in order to satisfy\n+  \/\/ Move some of the mutator regions into the Collector and OldCollector partitions in order to satisfy\n@@ -1559,1 +1643,3 @@\n-  _partitions.assert_bounds();\n+#ifdef ASSERT\n+  _partitions.assert_bounds(_old_trash_not_in_bounds);\n+#endif\n@@ -1561,0 +1647,31 @@\n+  \/\/ Even though we have finished rebuild, old trashed regions may not yet have been recycled, so leave\n+  \/\/ _old_trash_not_in_bounds as is.  Following rebuild, old trashed regions may reside in Mutator, Collector,\n+  \/\/ or OldCollector partitions.\n+}\n+\n+\/\/ Reduce old reserve (when there are insufficient resources to satisfy the original request).\n+void ShenandoahFreeSet::reduce_old_reserve(size_t adjusted_old_reserve, size_t requested_old_reserve) {\n+  ShenandoahOldGeneration* const old_generation = _heap->old_generation();\n+  size_t requested_promoted_reserve = old_generation->get_promoted_reserve();\n+  size_t requested_old_evac_reserve = old_generation->get_evacuation_reserve();\n+  assert(adjusted_old_reserve < requested_old_reserve, \"Only allow reduction\");\n+  assert(requested_promoted_reserve + requested_old_evac_reserve >= adjusted_old_reserve, \"Sanity\");\n+  size_t delta = requested_old_reserve - adjusted_old_reserve;\n+\n+  if (requested_promoted_reserve >= delta) {\n+    requested_promoted_reserve -= delta;\n+    old_generation->set_promoted_reserve(requested_promoted_reserve);\n+  } else {\n+    delta -= requested_promoted_reserve;\n+    requested_promoted_reserve = 0;\n+    requested_old_evac_reserve -= delta;\n+    old_generation->set_promoted_reserve(requested_promoted_reserve);\n+    old_generation->set_evacuation_reserve(requested_old_evac_reserve);\n+  }\n+}\n+\n+\/\/ Reduce young reserve (when there are insufficient resources to satisfy the original request).\n+void ShenandoahFreeSet::reduce_young_reserve(size_t adjusted_young_reserve, size_t requested_young_reserve) {\n+  ShenandoahYoungGeneration* const young_generation = _heap->young_generation();\n+  assert(adjusted_young_reserve < requested_young_reserve, \"Only allow reduction\");\n+  young_generation->set_evacuation_reserve(adjusted_young_reserve);\n@@ -1564,1 +1681,0 @@\n-                                                       bool have_evacuation_reserves,\n@@ -1568,1 +1684,0 @@\n-\n@@ -1570,1 +1685,1 @@\n-  size_t old_available = old_generation->available();\n+  size_t old_available = old_generation->available() + old_cset_regions * region_size_bytes;\n@@ -1580,0 +1695,10 @@\n+  assert(young_capacity >= (young_generation->used() + young_generation->get_humongous_waste()),\n+         \"Young capacity (\" SIZE_FORMAT \") must exceed used (\" SIZE_FORMAT \") plus humongous waste (\" SIZE_FORMAT \")\",\n+         young_capacity, young_generation->used(), young_generation->get_humongous_waste());\n+\n+  size_t young_available = young_capacity - (young_generation->used() + young_generation->get_humongous_waste());\n+  young_available += young_cset_regions * region_size_bytes;\n+\n+  assert(young_available >= young_unaffiliated_regions * region_size_bytes, \"sanity\");\n+  assert(old_available >= old_unaffiliated_regions * region_size_bytes, \"sanity\");\n+\n@@ -1595,0 +1720,1 @@\n+    young_available += xfer_bytes;\n@@ -1603,17 +1729,9 @@\n-  if (have_evacuation_reserves) {\n-    \/\/ We are rebuilding at the end of final mark, having already established evacuation budgets for this GC pass.\n-    const size_t promoted_reserve = old_generation->get_promoted_reserve();\n-    const size_t old_evac_reserve = old_generation->get_evacuation_reserve();\n-    young_reserve_result = young_generation->get_evacuation_reserve();\n-    old_reserve_result = promoted_reserve + old_evac_reserve;\n-    assert(old_reserve_result <= old_available,\n-           \"Cannot reserve (\" SIZE_FORMAT \" + \" SIZE_FORMAT\") more OLD than is available: \" SIZE_FORMAT,\n-           promoted_reserve, old_evac_reserve, old_available);\n-  } else {\n-    \/\/ We are rebuilding at end of GC, so we set aside budgets specified on command line (or defaults)\n-    young_reserve_result = (young_capacity * ShenandoahEvacReserve) \/ 100;\n-    \/\/ The auto-sizer has already made old-gen large enough to hold all anticipated evacuations and promotions.\n-    \/\/ Affiliated old-gen regions are already in the OldCollector free set.  Add in the relevant number of\n-    \/\/ unaffiliated regions.\n-    old_reserve_result = old_available;\n-  }\n+\n+  const size_t promoted_reserve = old_generation->get_promoted_reserve();\n+  const size_t old_evac_reserve = old_generation->get_evacuation_reserve();\n+  young_reserve_result = young_generation->get_evacuation_reserve();\n+  old_reserve_result = promoted_reserve + old_evac_reserve;\n+  assert(old_reserve_result + young_reserve_result <= old_available + young_available,\n+         \"Cannot reserve (\" SIZE_FORMAT \" + \" SIZE_FORMAT \" + \" SIZE_FORMAT\n+         \") more than is available: \" SIZE_FORMAT \" + \" SIZE_FORMAT,\n+         promoted_reserve, old_evac_reserve, young_reserve_result, old_available, young_available);\n@@ -1626,1 +1744,1 @@\n-      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n+      _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n@@ -1628,1 +1746,1 @@\n-      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes;\n+      _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes;\n@@ -1653,2 +1771,2 @@\n-    bool move_to_old_collector = _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) < to_reserve_old;\n-    bool move_to_collector = _partitions.capacity_of(ShenandoahFreeSetPartitionId::Collector) < to_reserve;\n+    bool move_to_old_collector = _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector) < to_reserve_old;\n+    bool move_to_collector = _partitions.available_in(ShenandoahFreeSetPartitionId::Collector) < to_reserve;\n@@ -1702,10 +1820,11 @@\n-  if (LogTarget(Info, gc, free)::is_enabled()) {\n-    size_t old_reserve = _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector);\n-    if (old_reserve < to_reserve_old) {\n-      log_info(gc, free)(\"Wanted \" PROPERFMT \" for old reserve, but only reserved: \" PROPERFMT,\n-                         PROPERFMTARGS(to_reserve_old), PROPERFMTARGS(old_reserve));\n-    }\n-    size_t reserve = _partitions.capacity_of(ShenandoahFreeSetPartitionId::Collector);\n-    if (reserve < to_reserve) {\n-      log_debug(gc)(\"Wanted \" PROPERFMT \" for young reserve, but only reserved: \" PROPERFMT,\n-                    PROPERFMTARGS(to_reserve), PROPERFMTARGS(reserve));\n+  size_t old_reserve = _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector);\n+  if (old_reserve < to_reserve_old) {\n+    assert(_heap->mode()->is_generational(), \"to_old_reserve > 0 implies generational mode\");\n+    reduce_old_reserve(old_reserve, to_reserve_old);\n+    log_info(gc, free)(\"Wanted \" PROPERFMT \" for old reserve, but only reserved: \" PROPERFMT,\n+                       PROPERFMTARGS(to_reserve_old), PROPERFMTARGS(old_reserve));\n+  }\n+  size_t young_reserve = _partitions.capacity_of(ShenandoahFreeSetPartitionId::Collector);\n+  if (young_reserve < to_reserve) {\n+    if (_heap->mode()->is_generational()) {\n+      reduce_young_reserve(young_reserve, to_reserve);\n@@ -1713,0 +1832,2 @@\n+    log_info(gc, free)(\"Wanted \" PROPERFMT \" for young reserve, but only reserved: \" PROPERFMT,\n+                       PROPERFMTARGS(to_reserve), PROPERFMTARGS(young_reserve));\n@@ -1778,1 +1899,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":185,"deletions":65,"binary":false,"changes":250,"status":"modified"},{"patch":"@@ -152,0 +152,4 @@\n+  \/\/ In case recycled region r is in the OldCollector partition but not within the interval for empty OldCollector regions, expand\n+  \/\/ the empty interval to include this region.  If recycled region r is not in the OldCollector partition, do nothing.\n+  inline void adjust_interval_for_recycled_old_region(ShenandoahHeapRegion* r);\n+\n@@ -239,0 +243,1 @@\n+#ifdef ASSERT\n@@ -259,1 +264,4 @@\n-  void assert_bounds() NOT_DEBUG_RETURN;\n+  \/\/\n+  \/\/ If old_trash_not_in_bounds, do not require old-generation trashed regions to be within the OldCollector bounds.\n+  void assert_bounds(bool old_trash_not_in_bounds) NOT_DEBUG_RETURN;\n+#endif\n@@ -299,1 +307,3 @@\n-\n+#ifdef ASSERT\n+  bool _old_trash_not_in_bounds;\n+#endif\n@@ -357,0 +367,3 @@\n+  void reduce_young_reserve(size_t adjusted_young_reserve, size_t requested_young_reserve);\n+  void reduce_old_reserve(size_t adjusted_old_reserve, size_t requested_old_reserve);\n+\n@@ -377,12 +390,4 @@\n-  \/\/ hold the results of evacuating to young-gen and to old-gen, and have_evacuation_reserves should be true.\n-  \/\/ These quantities, stored as reserves for their respective generations, are consulted prior to rebuilding\n-  \/\/ the free set (ShenandoahFreeSet) in preparation for evacuation.  When the free set is rebuilt, we make sure\n-  \/\/ to reserve sufficient memory in the collector and old_collector sets to hold evacuations.\n-  \/\/\n-  \/\/ We also rebuild the free set at the end of GC, as we prepare to idle GC until the next trigger.  In this case,\n-  \/\/ have_evacuation_reserves is false because we don't yet know how much memory will need to be evacuated in the\n-  \/\/ next GC cycle.  When have_evacuation_reserves is false, the free set rebuild operation reserves for the collector\n-  \/\/ and old_collector sets based on alternative mechanisms, such as ShenandoahEvacReserve, ShenandoahOldEvacReserve, and\n-  \/\/ ShenandoahOldCompactionReserve.  In a future planned enhancement, the reserve for old_collector set when the\n-  \/\/ evacuation reserves are unknown, is based in part on anticipated promotion as determined by analysis of live data\n-  \/\/ found during the previous GC pass which is one less than the current tenure age.\n+   \/\/ hold the results of evacuating to young-gen and to old-gen.  These quantities, stored in reserves for their,\n+  \/\/ respective generations, are consulted prior to rebuilding the free set (ShenandoahFreeSet) in preparation for\n+  \/\/ evacuation.  When the free set is rebuilt, we make sure to reserve sufficient memory in the collector and\n+  \/\/ old_collector sets to hold evacuations.\n@@ -393,5 +398,1 @@\n-  \/\/ have_evacuation_reserves is true iff the desired values of young-gen and old-gen evacuation reserves and old-gen\n-  \/\/                    promotion reserve have been precomputed (and can be obtained by invoking\n-  \/\/                    <generation>->get_evacuation_reserve() or old_gen->get_promoted_reserve()\n-  void finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t num_old_regions,\n-                      bool have_evacuation_reserves = false);\n+  void finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t num_old_regions);\n@@ -409,0 +410,7 @@\n+#ifdef ASSERT\n+  \/\/ Advise FreeSet that old trash regions have not yet been accounted for in OldCollector partition bounds\n+  void advise_of_old_trash() {\n+    shenandoah_assert_heaplocked();\n+    _old_trash_not_in_bounds = true;\n+  }\n+#endif\n@@ -482,1 +490,1 @@\n-  void compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions, bool have_evacuation_reserves,\n+  void compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":28,"deletions":20,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -1184,3 +1184,0 @@\n-    \/\/ We also do not expand old generation size following Full GC because we have scrambled age populations and\n-    \/\/ no longer have objects separated by age into distinct regions.\n-\n@@ -1191,6 +1188,0 @@\n-\n-\n-    if (heap->mode()->is_generational()) {\n-      ShenandoahGenerationalFullGC::compute_balances();\n-    }\n-\n@@ -1208,1 +1199,0 @@\n-    ShenandoahGenerationalFullGC::balance_generations_after_rebuilding_free_set();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -261,2 +261,3 @@\n-  \/\/ maximum_young_evacuation_reserve is upper bound on memory to be evacuated out of young\n-  const size_t maximum_young_evacuation_reserve = (young_generation->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+  \/\/ maximum_young_evacuation_reserve is upper bound on memory to be evacuated into young Collector Reserve.  This is\n+  \/\/ bounded at the end of previous GC cycle, based on available memory and balancing of evacuation to old and young.\n+  const size_t maximum_young_evacuation_reserve = young_generation->get_evacuation_reserve();\n@@ -356,1 +357,0 @@\n-\/\/\n@@ -458,2 +458,1 @@\n-    bool result = ShenandoahGenerationalHeap::cast(heap)->generation_sizer()->transfer_to_young(regions_to_xfer);\n-    assert(excess_old > regions_to_xfer * region_size_bytes, \"Cannot xfer more than excess old\");\n+    assert(excess_old >= regions_to_xfer * region_size_bytes, \"Cannot xfer more than excess old\");\n@@ -461,2 +460,1 @@\n-    log_info(gc, ergo)(\"%s transferred \" SIZE_FORMAT \" excess regions to young before start of evacuation\",\n-                       result? \"Successfully\": \"Unsuccessfully\", regions_to_xfer);\n+    ShenandoahGenerationalHeap::cast(heap)->generation_sizer()->force_transfer_to_young(regions_to_xfer);\n@@ -718,1 +716,1 @@\n-      \/\/ place, and preselect older regions that will be promoted by evacuation.\n+      \/\/ place, and preselected older regions that will be promoted by evacuation.\n@@ -721,2 +719,1 @@\n-      \/\/ Choose the collection set, including the regions preselected above for\n-      \/\/ promotion into the old generation.\n+      \/\/ Choose the collection set, including the regions preselected above for promotion into the old generation.\n@@ -724,4 +721,3 @@\n-      if (!collection_set->is_empty()) {\n-        \/\/ only make use of evacuation budgets when we are evacuating\n-        adjust_evacuation_budgets(heap, collection_set);\n-      }\n+\n+      \/\/ Even if collection_set->is_empty(), we want to adjust budgets, making reserves available to mutator.\n+      adjust_evacuation_budgets(heap, collection_set);\n@@ -746,1 +742,0 @@\n-\n@@ -757,1 +752,1 @@\n-    heap->free_set()->finish_rebuild(young_cset_regions, old_cset_regions, num_old, true);\n+    heap->free_set()->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":11,"deletions":16,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -197,0 +197,13 @@\n+\/\/ This is used to transfer excess old-gen regions to young at the start of evacuation after collection set is determined.\n+void ShenandoahGenerationSizer::force_transfer_to_young(size_t regions) const {\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  ShenandoahGeneration* old_gen = heap->old_generation();\n+  ShenandoahGeneration* young_gen = heap->young_generation();\n+  const size_t bytes_to_transfer = regions * ShenandoahHeapRegion::region_size_bytes();\n+\n+  young_gen->increase_capacity(bytes_to_transfer);\n+  old_gen->decrease_capacity(bytes_to_transfer);\n+  const size_t new_size = young_gen->max_capacity();\n+  log_info(gc)(\"Forcing transfer of \" SIZE_FORMAT \" region(s) from %s to %s, yielding increased size: \" PROPERFMT,\n+          regions, old_gen->name(), young_gen->name(), PROPERFMTARGS(new_size));\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationSizer.cpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -89,1 +89,2 @@\n-  \/\/ force transfer is used when we promote humongous objects.  May violate min\/max limits on generation sizes\n+  \/\/ Force transfer is used when we promote humongous objects or promote regular regions in place.\n+  \/\/ May violate min\/max limits on generation sizes.\n@@ -91,0 +92,4 @@\n+\n+  \/\/ Force transfer is used when we have excess old and we have confirmed that old unaffiliated >= regions.\n+  \/\/ May violate min\/max limits on generation sizes.\n+  void force_transfer_to_young(size_t regions) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationSizer.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -59,3 +59,0 @@\n-  \/\/ No need for old_gen->increase_used() as this was done when plabs were allocated.\n-  heap->reset_generation_reserves();\n-\n@@ -103,0 +100,3 @@\n+\/\/ Full GC has scattered aged objects throughout the heap.  There are no more aged regions, so there is no anticipated\n+\/\/ promotion.  Furthermore, Full GC has cancelled any ongoing mixed evacuation efforts so there are no anticipated old-gen\n+\/\/ evacuations.  Size old-gen to represent its current usage by setting the balance.  This feeds into rebuild of freeset.\n@@ -113,0 +113,1 @@\n+  ssize_t region_balance;\n@@ -115,1 +116,2 @@\n-    gen_heap->generation_sizer()->transfer_to_young(excess_old_regions);\n+    \/\/ Since the act of FullGC does not honor old and young budgets, excess_old_regions are conceptually unaffiliated.\n+    region_balance = checked_cast<ssize_t>(excess_old_regions);\n@@ -117,0 +119,2 @@\n+    \/\/ Since the old_usage already consumes more regions than in old_capacity, we know these regions are not affiliated young,\n+    \/\/ so arrange to transfer them.\n@@ -118,1 +122,20 @@\n-    gen_heap->generation_sizer()->force_transfer_to_old(old_regions_deficit);\n+    region_balance = 0 - checked_cast<ssize_t>(old_regions_deficit);\n+  } else {\n+    region_balance = 0;\n+  }\n+  old_gen->set_region_balance(region_balance);\n+  \/\/ Rebuild free set will log adjustments to generation sizes.\n+\n+  ShenandoahYoungGeneration* const young_gen = gen_heap->young_generation();\n+  size_t anticipated_young_capacity = young_gen->max_capacity() + region_balance * ShenandoahHeapRegion::region_size_bytes();\n+  size_t young_usage = young_gen->used_regions_size();\n+  assert(anticipated_young_capacity >= young_usage, \"sanity\");\n+\n+  size_t anticipated_max_collector_reserve = anticipated_young_capacity - young_usage;\n+  size_t desired_collector_reserve = (anticipated_young_capacity * ShenandoahEvacReserve) \/ 100;\n+  size_t young_reserve;\n+  if (desired_collector_reserve > anticipated_max_collector_reserve) {\n+    \/\/ Trigger next concurrent GC immediately\n+    young_reserve = anticipated_max_collector_reserve;\n+  } else {\n+    young_reserve = desired_collector_reserve;\n@@ -121,4 +144,2 @@\n-  log_info(gc)(\"FullGC done: young usage: \" PROPERFMT \", old usage: \" PROPERFMT,\n-               PROPERFMTARGS(gen_heap->young_generation()->used()),\n-               PROPERFMTARGS(old_gen->used()));\n-}\n+  size_t reserve_for_promo = 0;\n+  size_t reserve_for_mixed = 0;\n@@ -126,7 +147,4 @@\n-void ShenandoahGenerationalFullGC::balance_generations_after_rebuilding_free_set() {\n-  auto result = ShenandoahGenerationalHeap::heap()->balance_generations();\n-  LogTarget(Info, gc, ergo) lt;\n-  if (lt.is_enabled()) {\n-    LogStream ls(lt);\n-    result.print_on(\"Full GC\", &ls);\n-  }\n+  \/\/ Reserves feed into rebuild calculations\n+  young_gen->set_evacuation_reserve(young_reserve);\n+  old_gen->set_evacuation_reserve(reserve_for_mixed);\n+  old_gen->set_promoted_reserve(reserve_for_promo);\n@@ -182,9 +200,0 @@\n-void ShenandoahGenerationalFullGC::compute_balances() {\n-  auto heap = ShenandoahGenerationalHeap::heap();\n-\n-  \/\/ In case this Full GC resulted from degeneration, clear the tally on anticipated promotion.\n-  heap->old_generation()->set_promotion_potential(0);\n-  \/\/ Invoke this in case we are able to transfer memory from OLD to YOUNG.\n-  heap->compute_old_generation_balance(0, 0);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.cpp","additions":34,"deletions":25,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -58,8 +58,0 @@\n-  \/\/ Rebuilding the free set may have resulted in regions being pulled in to the old generation\n-  \/\/ evacuation reserve. For this reason, we must update the usage and capacity of the generations\n-  \/\/ again. In the distant past, the free set did not know anything about generations, so we had\n-  \/\/ a layer built above it to represent how much young\/old memory was available. This layer is\n-  \/\/ redundant and adds complexity. We would like to one day remove it. Until then, we must keep it\n-  \/\/ synchronized with the free set's view of things.\n-  static void balance_generations_after_rebuilding_free_set();\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.hpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -569,29 +569,0 @@\n-ShenandoahGenerationalHeap::TransferResult ShenandoahGenerationalHeap::balance_generations() {\n-  shenandoah_assert_heaplocked_or_safepoint();\n-\n-  ShenandoahOldGeneration* old_gen = old_generation();\n-  const ssize_t old_region_balance = old_gen->get_region_balance();\n-  old_gen->set_region_balance(0);\n-\n-  if (old_region_balance > 0) {\n-    const auto old_region_surplus = checked_cast<size_t>(old_region_balance);\n-    const bool success = generation_sizer()->transfer_to_young(old_region_surplus);\n-    return TransferResult {\n-      success, old_region_surplus, \"young\"\n-    };\n-  }\n-\n-  if (old_region_balance < 0) {\n-    const auto old_region_deficit = checked_cast<size_t>(-old_region_balance);\n-    const bool success = generation_sizer()->transfer_to_old(old_region_deficit);\n-    if (!success) {\n-      old_gen->handle_failed_transfer();\n-    }\n-    return TransferResult {\n-      success, old_region_deficit, \"old\"\n-    };\n-  }\n-\n-  return TransferResult {true, 0, \"none\"};\n-}\n-\n@@ -600,3 +571,4 @@\n-\/\/ xfer_limit, and any surplus is transferred to the young generation.\n-\/\/ xfer_limit is the maximum we're able to transfer from young to old.\n-void ShenandoahGenerationalHeap::compute_old_generation_balance(size_t old_xfer_limit, size_t old_cset_regions) {\n+\/\/ mutator_xfer_limit, and any surplus is transferred to the young generation.\n+\/\/ mutator_xfer_limit is the maximum we're able to transfer from young to old.\n+void ShenandoahGenerationalHeap::compute_old_generation_balance(size_t mutator_xfer_limit,\n+                                                                size_t old_cset_regions, size_t young_cset_regions) {\n@@ -622,3 +594,3 @@\n-  const size_t old_available = old_generation()->available();\n-  \/\/ The free set will reserve this amount of memory to hold young evacuations\n-  const size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  const size_t old_available = old_generation()->available() + old_cset_regions * region_size_bytes;\n+  const size_t young_available = young_generation()->available() + young_cset_regions * region_size_bytes;\n@@ -626,1 +598,2 @@\n-  \/\/ In the case that ShenandoahOldEvacRatioPercent equals 100, max_old_reserve is limited only by xfer_limit.\n+  \/\/ The free set will reserve this amount of memory to hold young evacuations (initialized to the ideal reserve)\n+  size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n@@ -628,4 +601,6 @@\n-  const size_t bound_on_old_reserve = old_available + old_xfer_limit + young_reserve;\n-  const size_t max_old_reserve = (ShenandoahOldEvacRatioPercent == 100)?\n-                                 bound_on_old_reserve: MIN2((young_reserve * ShenandoahOldEvacRatioPercent) \/ (100 - ShenandoahOldEvacRatioPercent),\n-                                                            bound_on_old_reserve);\n+  \/\/ If ShenandoahOldEvacRatioPercent equals 100, max_old_reserve is limited only by mutator_xfer_limit and young_reserve\n+  const size_t bound_on_old_reserve = ((old_available + mutator_xfer_limit + young_reserve) * ShenandoahOldEvacRatioPercent) \/ 100;\n+  size_t proposed_max_old = ((ShenandoahOldEvacRatioPercent == 100)?\n+                             bound_on_old_reserve:\n+                             MIN2((young_reserve * ShenandoahOldEvacRatioPercent) \/ (100 - ShenandoahOldEvacRatioPercent),\n+                                  bound_on_old_reserve));\n@@ -633,1 +608,3 @@\n-  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  if (young_reserve > young_available) {\n+    young_reserve = young_available;\n+  }\n@@ -637,1 +614,15 @@\n-  if (old_generation()->has_unprocessed_collection_candidates()) {\n+  const size_t old_fragmented_available =\n+    old_available - (old_generation()->free_unaffiliated_regions() + old_cset_regions) * region_size_bytes;\n+\n+  if (old_fragmented_available > proposed_max_old) {\n+    \/\/ After we've promoted regions in place, there may be an abundance of old-fragmented available memory,\n+    \/\/ even more than the desired percentage for old reserve.  We cannot transfer these fragmented regions back\n+    \/\/ to young.  Instead we make the best of the situation by using this fragmented memory for both promotions\n+    \/\/ and evacuations.\n+    proposed_max_old = old_fragmented_available;\n+  }\n+  size_t reserve_for_promo = old_fragmented_available;\n+  const size_t max_old_reserve = proposed_max_old;\n+  const size_t mixed_candidate_live_memory = old_generation()->unprocessed_collection_candidates_live_memory();\n+  const bool doing_mixed = (mixed_candidate_live_memory > 0);\n+  if (doing_mixed) {\n@@ -640,2 +631,1 @@\n-    const size_t max_evac_need = (size_t)\n-            (old_generation()->unprocessed_collection_candidates_live_memory() * ShenandoahOldEvacWaste);\n+    const size_t max_evac_need = (size_t) (mixed_candidate_live_memory * ShenandoahOldEvacWaste);\n@@ -644,5 +634,14 @@\n-    const size_t old_fragmented_available =\n-            old_available - old_generation()->free_unaffiliated_regions() * region_size_bytes;\n-    reserve_for_mixed = max_evac_need + old_fragmented_available;\n-    if (reserve_for_mixed > max_old_reserve) {\n-      reserve_for_mixed = max_old_reserve;\n+\n+    \/\/ We prefer to evacuate all of mixed into unfragmented memory, and will expand old in order to do so, unless\n+    \/\/ we already have too much fragmented available memory in old.\n+    reserve_for_mixed = max_evac_need;\n+    if (reserve_for_mixed + reserve_for_promo > max_old_reserve) {\n+      \/\/ In this case, we'll allow old-evac to target some of the fragmented old memory.\n+      size_t excess_reserves = (reserve_for_mixed + reserve_for_promo) - max_old_reserve;\n+      if (reserve_for_promo > excess_reserves) {\n+        reserve_for_promo -= excess_reserves;\n+      } else {\n+        excess_reserves -= reserve_for_promo;\n+        reserve_for_promo = 0;\n+        reserve_for_mixed -= excess_reserves;\n+      }\n@@ -652,2 +651,2 @@\n-  \/\/ Decide how much space we should reserve for promotions from young\n-  size_t reserve_for_promo = 0;\n+  \/\/ Decide how much additional space we should reserve for promotions from young.  We give priority to mixed evacations\n+  \/\/ over promotions.\n@@ -657,4 +656,15 @@\n-    \/\/ We're promoting and have a bound on the maximum amount that can be promoted\n-    assert(max_old_reserve >= reserve_for_mixed, \"Sanity\");\n-    const size_t available_for_promotions = max_old_reserve - reserve_for_mixed;\n-    reserve_for_promo = MIN2((size_t)(promo_load * ShenandoahPromoEvacWaste), available_for_promotions);\n+    \/\/ We've already set aside all of the fragmented available memory within old-gen to represent old objects\n+    \/\/ to be promoted from young generation.  promo_load represents the memory that we anticipate to be promoted\n+    \/\/ from regions that have reached tenure age.  In the ideal, we will always use fragmented old-gen memory\n+    \/\/ to hold individually promoted objects and will use unfragmented old-gen memory to represent the old-gen\n+    \/\/ evacuation workloa.\n+\n+    \/\/ We're promoting and have an esimate of memory to be promoted from aged regions\n+    assert(max_old_reserve >= (reserve_for_mixed + reserve_for_promo), \"Sanity\");\n+    const size_t available_for_additional_promotions = max_old_reserve - (reserve_for_mixed + reserve_for_promo);\n+    size_t promo_need = (size_t)(promo_load * ShenandoahPromoEvacWaste);\n+    if (promo_need > reserve_for_promo) {\n+      reserve_for_promo += MIN2(promo_need - reserve_for_promo, available_for_additional_promotions);\n+    }\n+    \/\/ We've already reserved all the memory required for the promo_load, and possibly more.  The excess\n+    \/\/ can be consumed by objects promoted from regions that have not yet reached tenure age.\n@@ -663,3 +673,2 @@\n-  \/\/ This is the total old we want to ideally reserve\n-  const size_t old_reserve = reserve_for_mixed + reserve_for_promo;\n-  assert(old_reserve <= max_old_reserve, \"cannot reserve more than max for old evacuations\");\n+  \/\/ This is the total old we want to reserve (initialized to the ideal reserve)\n+  size_t old_reserve = reserve_for_mixed + reserve_for_promo;\n@@ -668,2 +677,7 @@\n-  const size_t max_old_available = old_generation()->available() + old_cset_regions * region_size_bytes;\n-  if (max_old_available >= old_reserve) {\n+  size_t old_region_deficit = 0;\n+  size_t old_region_surplus = 0;\n+\n+  size_t mutator_region_xfer_limit = mutator_xfer_limit \/ region_size_bytes;\n+  \/\/ align the mutator_xfer_limit on region size\n+  mutator_xfer_limit = mutator_region_xfer_limit * region_size_bytes;\n+  if (old_available >= old_reserve) {\n@@ -671,1 +685,2 @@\n-    const size_t old_surplus = (max_old_available - old_reserve) \/ region_size_bytes;\n+    const size_t old_surplus = old_available - old_reserve;\n+    old_region_surplus = old_surplus \/ region_size_bytes;\n@@ -673,1 +688,1 @@\n-    const size_t old_region_surplus = MIN2(old_surplus, unaffiliated_old_regions);\n+    old_region_surplus = MIN2(old_region_surplus, unaffiliated_old_regions);\n@@ -675,0 +690,5 @@\n+  } else if (old_available + mutator_xfer_limit >= old_reserve) {\n+    \/\/ Mutator's xfer limit is sufficient to satisfy our need: transfer all memory from there\n+    size_t old_deficit = old_reserve - old_available;\n+    old_region_deficit = (old_deficit + region_size_bytes - 1) \/ region_size_bytes;\n+    old_generation()->set_region_balance(0 - checked_cast<ssize_t>(old_region_deficit));\n@@ -676,11 +696,38 @@\n-    \/\/ We are running a deficit which we'd like to fill from young.\n-    \/\/ Ignore that this will directly impact young_generation()->max_capacity(),\n-    \/\/ indirectly impacting young_reserve and old_reserve.  These computations are conservative.\n-    \/\/ Note that deficit is rounded up by one region.\n-    const size_t old_need = (old_reserve - max_old_available + region_size_bytes - 1) \/ region_size_bytes;\n-    const size_t max_old_region_xfer = old_xfer_limit \/ region_size_bytes;\n-\n-    \/\/ Round down the regions we can transfer from young to old. If we're running short\n-    \/\/ on young-gen memory, we restrict the xfer. Old-gen collection activities will be\n-    \/\/ curtailed if the budget is restricted.\n-    const size_t old_region_deficit = MIN2(old_need, max_old_region_xfer);\n+    \/\/ We'll try to xfer from both mutator excess and from young collector reserve\n+    size_t available_reserves = old_available + young_reserve + mutator_xfer_limit;\n+    size_t old_entitlement = (available_reserves  * ShenandoahOldEvacRatioPercent) \/ 100;\n+\n+    \/\/ Round old_entitlement down to nearest multiple of regions to be transferred to old\n+    size_t entitled_xfer = old_entitlement - old_available;\n+    entitled_xfer = region_size_bytes * (entitled_xfer \/ region_size_bytes);\n+    size_t unaffiliated_young_regions = young_generation()->free_unaffiliated_regions();\n+    size_t unaffiliated_young_memory = unaffiliated_young_regions * region_size_bytes;\n+    if (entitled_xfer > unaffiliated_young_memory) {\n+      entitled_xfer = unaffiliated_young_memory;\n+    }\n+    old_entitlement = old_available + entitled_xfer;\n+    if (old_entitlement < old_reserve) {\n+      \/\/ There's not enough memory to satisfy our desire.  Scale back our old-gen intentions.\n+      size_t budget_overrun = old_reserve - old_entitlement;;\n+      if (reserve_for_promo > budget_overrun) {\n+        reserve_for_promo -= budget_overrun;\n+        old_reserve -= budget_overrun;\n+      } else {\n+        budget_overrun -= reserve_for_promo;\n+        reserve_for_promo = 0;\n+        reserve_for_mixed = (reserve_for_mixed > budget_overrun)? reserve_for_mixed - budget_overrun: 0;\n+        old_reserve = reserve_for_promo + reserve_for_mixed;\n+      }\n+    }\n+\n+    \/\/ Because of adjustments above, old_reserve may be smaller now than it was when we tested the branch\n+    \/\/   condition above: \"(old_available + mutator_xfer_limit >= old_reserve)\n+    \/\/ Therefore, we do NOT know that: mutator_xfer_limit < old_reserve - old_available\n+\n+    size_t old_deficit = old_reserve - old_available;\n+    old_region_deficit = (old_deficit + region_size_bytes - 1) \/ region_size_bytes;\n+\n+    \/\/ Shrink young_reserve to account for loan to old reserve\n+    const size_t reserve_xfer_regions = old_region_deficit - mutator_region_xfer_limit;\n+    young_reserve -= reserve_xfer_regions * region_size_bytes;\n+\n@@ -688,0 +735,1 @@\n+\n@@ -689,1 +737,0 @@\n-}\n@@ -691,4 +738,9 @@\n-void ShenandoahGenerationalHeap::reset_generation_reserves() {\n-  young_generation()->set_evacuation_reserve(0);\n-  old_generation()->set_evacuation_reserve(0);\n-  old_generation()->set_promoted_reserve(0);\n+  assert(old_region_deficit == 0 || old_region_surplus == 0, \"Only surplus or deficit, never both\");\n+  assert(young_reserve + reserve_for_mixed + reserve_for_promo <= old_available + young_available,\n+         \"Cannot reserve more memory than is available: \" SIZE_FORMAT \" + \" SIZE_FORMAT \" + \" SIZE_FORMAT \" <= \"\n+         SIZE_FORMAT \" + \" SIZE_FORMAT, young_reserve, reserve_for_mixed, reserve_for_promo, old_available, young_available);\n+\n+  \/\/ deficit\/surplus adjustments to generation sizes will precede rebuild\n+  young_generation()->set_evacuation_reserve(young_reserve);\n+  old_generation()->set_evacuation_reserve(reserve_for_mixed);\n+  old_generation()->set_promoted_reserve(reserve_for_promo);\n@@ -1065,13 +1117,0 @@\n-\n-  \/\/ We defer generation resizing actions until after cset regions have been recycled.\n-  TransferResult result = balance_generations();\n-  LogTarget(Info, gc, ergo) lt;\n-  if (lt.is_enabled()) {\n-    LogStream ls(lt);\n-    result.print_on(\"Degenerated GC\", &ls);\n-  }\n-\n-  \/\/ In case degeneration interrupted concurrent evacuation or update references, we need to clean up\n-  \/\/ transient state. Otherwise, these actions have no effect.\n-  reset_generation_reserves();\n-\n@@ -1095,14 +1134,0 @@\n-\n-  TransferResult result;\n-  {\n-    ShenandoahHeapLocker locker(lock());\n-\n-    result = balance_generations();\n-    reset_generation_reserves();\n-  }\n-\n-  LogTarget(Info, gc, ergo) lt;\n-  if (lt.is_enabled()) {\n-    LogStream ls(lt);\n-    result.print_on(\"Concurrent GC\", &ls);\n-  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":130,"deletions":105,"binary":false,"changes":235,"status":"modified"},{"patch":"@@ -121,3 +121,0 @@\n-  \/\/ Zeros out the evacuation and promotion reserves\n-  void reset_generation_reserves();\n-\n@@ -125,4 +122,1 @@\n-  void compute_old_generation_balance(size_t old_xfer_limit, size_t old_cset_regions);\n-\n-  \/\/ Transfers surplus old regions to young, or takes regions from young to satisfy old region deficit\n-  TransferResult balance_generations();\n+  void compute_old_generation_balance(size_t old_xfer_limit, size_t old_cset_regions, size_t young_cset_regions);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.hpp","additions":1,"deletions":7,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -419,0 +419,7 @@\n+\n+    if (mode()->is_generational()) {\n+      size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+      young_generation()->set_evacuation_reserve(young_reserve);\n+      old_generation()->set_evacuation_reserve((size_t) 0);\n+      old_generation()->set_promoted_reserve((size_t) 0);\n+    }\n@@ -2471,0 +2478,1 @@\n+\n@@ -2490,9 +2498,1 @@\n-    gen_heap->compute_old_generation_balance(allocation_runway, old_cset_regions);\n-\n-    \/\/ Total old_available may have been expanded to hold anticipated promotions.  We trigger if the fragmented available\n-    \/\/ memory represents more than 16 regions worth of data.  Note that fragmentation may increase when we promote regular\n-    \/\/ regions in place when many of these regular regions have an abundant amount of available memory within them.  Fragmentation\n-    \/\/ will decrease as promote-by-copy consumes the available memory within these partially consumed regions.\n-    \/\/\n-    \/\/ We consider old-gen to have excessive fragmentation if more than 12.5% of old-gen is free memory that resides\n-    \/\/ within partially consumed regions of memory.\n+    gen_heap->compute_old_generation_balance(allocation_runway, old_cset_regions, young_cset_regions);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -147,1 +147,0 @@\n-\n@@ -150,4 +149,3 @@\n-  \/\/ We must execute this vm operation if we completed final mark. We cannot\n-  \/\/ return from here with weak roots in progress. This is not a valid gc state\n-  \/\/ for any young collections (or allocation failures) that interrupt the old\n-  \/\/ collection.\n+  \/\/ We must execute this vm operation if we completed final mark. We cannot return from here with weak roots in progress.\n+  \/\/ This is not a valid gc state for any young collections (or allocation failures) that interrupt the old collection.\n+  \/\/ This will reclaim immediate garbage.  vmop_entry_final_roots() will also rebuild the free set.\n@@ -155,17 +153,0 @@\n-\n-  \/\/ We do not rebuild_free following increments of old marking because memory has not been reclaimed. However, we may\n-  \/\/ need to transfer memory to OLD in order to efficiently support the mixed evacuations that might immediately follow.\n-  size_t allocation_runway = heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(0);\n-  heap->compute_old_generation_balance(allocation_runway, 0);\n-\n-  ShenandoahGenerationalHeap::TransferResult result;\n-  {\n-    ShenandoahHeapLocker locker(heap->lock());\n-    result = heap->balance_generations();\n-  }\n-\n-  LogTarget(Info, gc, ergo) lt;\n-  if (lt.is_enabled()) {\n-    LogStream ls(lt);\n-    result.print_on(\"Old Mark\", &ls);\n-  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.cpp","additions":3,"deletions":22,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -466,15 +466,0 @@\n-\n-  {\n-    \/\/ Though we did not choose a collection set above, we still may have\n-    \/\/ freed up immediate garbage regions so proceed with rebuilding the free set.\n-    ShenandoahGCPhase phase(concurrent ?\n-        ShenandoahPhaseTimings::final_rebuild_freeset :\n-        ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n-    ShenandoahHeapLocker locker(heap->lock());\n-    size_t cset_young_regions, cset_old_regions;\n-    size_t first_old, last_old, num_old;\n-    heap->free_set()->prepare_to_rebuild(cset_young_regions, cset_old_regions, first_old, last_old, num_old);\n-    \/\/ This is just old-gen completion.  No future budgeting required here.  The only reason to rebuild the freeset here\n-    \/\/ is in case there was any immediate old garbage identified.\n-    heap->free_set()->finish_rebuild(cset_young_regions, cset_old_regions, num_old);\n-  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":0,"deletions":15,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -66,2 +66,2 @@\n-  \/\/ Represents the quantity of live bytes we expect to promote in place during the next\n-  \/\/ evacuation cycle. This value is used by the young heuristic to trigger mixed collections.\n+  \/\/ Represents the quantity of live bytes we expect to promote during the next GC cycle, either by\n+  \/\/ evacuation or by promote-in-place.  This value is used by the young heuristic to trigger mixed collections.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -383,1 +383,8 @@\n-  size_t non_trashed_span() const { return (_regions - _trashed_regions) * ShenandoahHeapRegion::region_size_bytes(); }\n+  size_t non_trashed_span() const {\n+    assert(_regions >= _trashed_regions, \"sanity\");\n+    return (_regions - _trashed_regions) * ShenandoahHeapRegion::region_size_bytes();\n+  }\n+  size_t non_trashed_committed() const {\n+    assert(_committed >= _trashed_regions * ShenandoahHeapRegion::region_size_bytes(), \"sanity\");\n+    return _committed - (_trashed_regions * ShenandoahHeapRegion::region_size_bytes());\n+  }\n@@ -421,0 +428,1 @@\n+    size_t generation_max_capacity = generation->max_capacity();\n@@ -426,0 +434,4 @@\n+    guarantee(stats.non_trashed_committed() <= generation_max_capacity,\n+              \"%s: generation (%s) non_trashed_committed: \" PROPERFMT \" must not exceed generation capacity: \" PROPERFMT,\n+              label, generation->name(), PROPERFMTARGS(stats.non_trashed_committed()), PROPERFMTARGS(generation_max_capacity));\n+\n@@ -440,1 +452,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -383,20 +383,36 @@\n-          \"The maximum proportion of evacuation from old-gen memory, \"      \\\n-          \"expressed as a percentage. The default value 75 denotes that no\" \\\n-          \"more than 75% of the collection set evacuation workload may be \" \\\n-          \"towards evacuation of old-gen heap regions. This limits both the\"\\\n-          \"promotion of aged regions and the compaction of existing old \"   \\\n-          \"regions.  A value of 75 denotes that the total evacuation work\"  \\\n-          \"may increase to up to four times the young gen evacuation work.\" \\\n-          \"A larger value allows quicker promotion and allows\"              \\\n-          \"a smaller number of mixed evacuations to process \"               \\\n-          \"the entire list of old-gen collection candidates at the cost \"   \\\n-          \"of an increased disruption of the normal cadence of young-gen \"  \\\n-          \"collections.  A value of 100 allows a mixed evacuation to \"      \\\n-          \"focus entirely on old-gen memory, allowing no young-gen \"        \\\n-          \"regions to be collected, likely resulting in subsequent \"        \\\n-          \"allocation failures because the allocation pool is not \"         \\\n-          \"replenished.  A value of 0 allows a mixed evacuation to\"         \\\n-          \"focus entirely on young-gen memory, allowing no old-gen \"        \\\n-          \"regions to be collected, likely resulting in subsequent \"        \\\n-          \"promotion failures and triggering of stop-the-world full GC \"    \\\n-          \"events.\")                                                        \\\n+          \"The maximum percent of memory that can be reserved for \"         \\\n+          \"evacuation into old generation.  With the default setting, \"     \\\n+          \"given a total evacuation budget of X, the amount of memory \"     \\\n+          \"reserved to hold objects evacuated to old generation is 0.75x.\"  \\\n+          \"This limits both the promotion of aged young regions and \"       \\\n+          \"the compaction of existing old regions.  It does not restrict \"  \\\n+          \"the collector from copying more objects into old-generation \"    \\\n+          \"memory if the young-generation collection set does not consume \" \\\n+          \"all of the memory originally reserved for young-generation \"     \\\n+          \"evacuation.  It also does not restrict the amount of memory \"    \\\n+          \"that can be promoted in place, by simply changing the \"          \\\n+          \"affiliation of the region from young to old.  If there is an \"   \\\n+          \"abundance of free memory, this will result in a larger total \"   \\\n+          \"evacuation effort, roughly quadrupling the amount of memory \"    \\\n+          \"normally evacuated during young evacuations (so that old \"       \\\n+          \"evacuates three times as much as young, and young evacuates its \"\\\n+          \"normal amount).  If free memory is in short supply, this may \"   \\\n+          \"result in paring back both young-gen and old-gen evacuations, \"  \\\n+          \"such that the fraction of old is 75% (in the default \"           \\\n+          \"configuration) of the total available evacuation reserve, \"      \\\n+          \"with young evacuating one fourth of its normal amount, \"         \\\n+          \"and old evacuating three times as much as young evacuates.  \"    \\\n+          \"Setting a larger value allows for quicker promotion and a \"      \\\n+          \"smaller number of mixed evacuations to process the entire list \" \\\n+          \"of old-gen collection candidates at the cost of increased \"      \\\n+          \"disruption of the normal young-gen collection cadence.  A \"      \\\n+          \"value of 100 allows a mixed evacuation to focus entirely \"       \\\n+          \"on old-gen memory, allowing no young-gen regions to be \"         \\\n+          \"collected.  This would likely result in subsequent allocation \"  \\\n+          \"failures because the young-gen allocation pool would not be \"    \\\n+          \"replenished.  A value of 0 prevents mixed evacuations \"          \\\n+          \"from defragmenting old-gen memory, likely resulting in \"         \\\n+          \"subsequent promotion failures and triggering of stop-the-world \" \\\n+          \"full GC events.  Faiure to defragment old-gen memory can also \"  \\\n+          \"result in unconstrained expansion of old-gen, and shrinkage of \" \\\n+          \"young gen, causing inefficient high frequency of young-gen GC.\") \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":36,"deletions":20,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -201,2 +201,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n-\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -214,2 +215,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n-\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -226,2 +228,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n-\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -248,1 +251,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -261,1 +266,0 @@\n-  _heuristics->prime_collection_set(_collection_set);\n@@ -263,0 +267,3 @@\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -278,1 +285,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -285,1 +294,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -301,2 +312,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n-\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -309,2 +321,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n-\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -327,2 +340,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n-\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -336,2 +350,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n-\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n@@ -354,2 +369,3 @@\n-  _heuristics->prime_collection_set(_collection_set);\n-\n+  if (_heuristics->prime_collection_set(_collection_set)) {\n+    _heuristics->finalize_mixed_evacs();\n+  }\n","filename":"test\/hotspot\/gtest\/gc\/shenandoah\/test_shenandoahOldHeuristic.cpp","additions":36,"deletions":20,"binary":false,"changes":56,"status":"modified"}]}