{"files":[{"patch":"@@ -80,0 +80,50 @@\n+bool ShenandoahOldHeuristics::prime_collection_set(ShenandoahCollectionSet* collection_set) {\n+  _mixed_evac_cset = collection_set;\n+  _included_old_regions = 0;\n+  _evacuated_old_bytes = 0;\n+  _collected_old_bytes = 0;\n+\n+  \/\/ If a region is put into the collection set, then this region's free (not yet used) bytes are no longer\n+  \/\/ \"available\" to hold the results of other evacuations.  This may cause a decrease in the remaining amount\n+  \/\/ of memory that can still be evacuated.  We address this by reducing the evacuation budget by the amount\n+  \/\/ of live memory in that region and by the amount of unallocated memory in that region if the evacuation\n+  \/\/ budget is constrained by availability of free memory.\n+  _old_evacuation_reserve = _old_generation->get_evacuation_reserve();\n+  _old_evacuation_budget = (size_t) ((double) _old_evacuation_reserve \/ ShenandoahOldEvacWaste);\n+\n+  \/\/ fragmented_available is the amount of memory within partially consumed old regions that may be required to\n+  \/\/ hold the results of old evacuations.  If all of the memory required by the old evacuation reserve is available\n+  \/\/ in unfragmented regions (unaffiliated old regions), then fragmented_available is zero because we do not need\n+  \/\/ to evacuate into the existing partially consumed old regions.\n+\n+  \/\/ if fragmented_available is non-zero, excess_fragmented_available represents the amount of fragmented memory\n+  \/\/ that is available within old, but is not required to hold the resuilts of old evacuation.  As old-gen regions\n+  \/\/ are added into the collection set, their free memory is subtracted from excess_fragmented_available until the\n+  \/\/ excess is exhausted.  For old-gen regions subsequently added to the collection set, their free memory is\n+  \/\/ subtracted from fragmented_available and from the old_evacuation_budget (since the budget decreases when this\n+  \/\/ fragmented_available memory decreases).  After fragmented_available has been exhausted, any further old regions\n+  \/\/ selected for the cset do not further decrease the old_evacuation_budget because all further evacuation is targeted\n+  \/\/ to unfragmented regions.\n+\n+  size_t unaffiliated_available = _old_generation->free_unaffiliated_regions() * ShenandoahHeapRegion::region_size_bytes();\n+  if (unaffiliated_available > _old_evacuation_reserve) {\n+    _unfragmented_available = _old_evacuation_budget;\n+    _fragmented_available = 0;\n+    _excess_fragmented_available = 0;\n+  } else {\n+    assert(_old_generation->available() >= _old_evacuation_reserve, \"Cannot reserve more than is available\");\n+    size_t affiliated_available = _old_generation->available() - unaffiliated_available;\n+    assert(affiliated_available + unaffiliated_available >= _old_evacuation_reserve, \"Budgets do not add up\");\n+    if (affiliated_available + unaffiliated_available > _old_evacuation_reserve) {\n+      _excess_fragmented_available = (affiliated_available + unaffiliated_available) - _old_evacuation_reserve;\n+      affiliated_available -= _excess_fragmented_available;\n+    }\n+    _fragmented_available = (size_t) ((double) affiliated_available \/ ShenandoahOldEvacWaste);\n+    _unfragmented_available = (size_t) ((double) unaffiliated_available \/ ShenandoahOldEvacWaste);\n+  }\n+  log_info(gc)(\"Choose old regions for mixed collection: old evacuation budget: \" SIZE_FORMAT \"%s, candidates: %u\",\n+               byte_size_in_proper_unit(_old_evacuation_budget), proper_unit_for_byte_size(_old_evacuation_budget),\n+               unprocessed_old_collection_candidates());\n+  return add_old_regions_to_cset();\n+}\n+\n@@ -96,0 +146,56 @@\n+void ShenandoahOldHeuristics::slide_pinned_regions_to_front() {\n+  \/\/ Find the first unpinned region to the left of the next region that\n+  \/\/ will be added to the collection set. These regions will have been\n+  \/\/ added to the cset, so we can use them to hold pointers to regions\n+  \/\/ that were pinned when the cset was chosen.\n+  \/\/ [ r p r p p p r r ]\n+  \/\/     ^         ^ ^\n+  \/\/     |         | | pointer to next region to add to a mixed collection is here.\n+  \/\/     |         | first r to the left should be in the collection set now.\n+  \/\/     | first pinned region, we don't need to look past this\n+  uint write_index = NOT_FOUND;\n+  for (uint search = _next_old_collection_candidate - 1; search > _first_pinned_candidate; --search) {\n+    ShenandoahHeapRegion* region = _region_data[search]._region;\n+    if (!region->is_pinned()) {\n+      write_index = search;\n+      assert(region->is_cset(), \"Expected unpinned region to be added to the collection set.\");\n+      break;\n+    }\n+  }\n+\n+  \/\/ If we could not find an unpinned region, it means there are no slots available\n+  \/\/ to move up the pinned regions. In this case, we just reset our next index in the\n+  \/\/ hopes that some of these regions will become unpinned before the next mixed\n+  \/\/ collection. We may want to bailout of here instead, as it should be quite\n+  \/\/ rare to have so many pinned regions and may indicate something is wrong.\n+  if (write_index == NOT_FOUND) {\n+    assert(_first_pinned_candidate != NOT_FOUND, \"Should only be here if there are pinned regions.\");\n+    _next_old_collection_candidate = _first_pinned_candidate;\n+    return;\n+  }\n+\n+  \/\/ Find pinned regions to the left and move their pointer into a slot\n+  \/\/ that was pointing at a region that has been added to the cset (or was pointing\n+  \/\/ to a pinned region that we've already moved up). We are done when the leftmost\n+  \/\/ pinned region has been slid up.\n+  \/\/ [ r p r x p p p r ]\n+  \/\/         ^       ^\n+  \/\/         |       | next region for mixed collections\n+  \/\/         | Write pointer is here. We know this region is already in the cset\n+  \/\/         | so we can clobber it with the next pinned region we find.\n+  for (int32_t search = (int32_t)write_index - 1; search >= (int32_t)_first_pinned_candidate; --search) {\n+    RegionData& skipped = _region_data[search];\n+    if (skipped._region->is_pinned()) {\n+      RegionData& available_slot = _region_data[write_index];\n+      available_slot._region = skipped._region;\n+      available_slot._u._live_data = skipped._u._live_data;\n+      --write_index;\n+    }\n+  }\n+\n+  \/\/ Update to read from the leftmost pinned region. Plus one here because we decremented\n+  \/\/ the write index to hold the next found pinned region. We are just moving it back now\n+  \/\/ to point to the first pinned region.\n+  _next_old_collection_candidate = write_index + 1;\n+}\n+\n@@ -209,50 +315,0 @@\n-bool ShenandoahOldHeuristics::prime_collection_set(ShenandoahCollectionSet* collection_set) {\n-  _mixed_evac_cset = collection_set;\n-  _included_old_regions = 0;\n-  _evacuated_old_bytes = 0;\n-  _collected_old_bytes = 0;\n-\n-  \/\/ If a region is put into the collection set, then this region's free (not yet used) bytes are no longer\n-  \/\/ \"available\" to hold the results of other evacuations.  This may cause a decrease in the remaining amount\n-  \/\/ of memory that can still be evacuated.  We address this by reducing the evacuation budget by the amount\n-  \/\/ of live memory in that region and by the amount of unallocated memory in that region if the evacuation\n-  \/\/ budget is constrained by availability of free memory.\n-  _old_evacuation_reserve = _old_generation->get_evacuation_reserve();\n-  _old_evacuation_budget = (size_t) ((double) _old_evacuation_reserve \/ ShenandoahOldEvacWaste);\n-\n-  \/\/ fragmented_available is the amount of memory within partially consumed old regions that may be required to\n-  \/\/ hold the results of old evacuations.  If all of the memory required by the old evacuation reserve is available\n-  \/\/ in unfragmented regions (unaffiliated old regions), then fragmented_available is zero because we do not need\n-  \/\/ to evacuate into the existing partially consumed old regions.\n-\n-  \/\/ if fragmented_available is non-zero, excess_fragmented_available represents the amount of fragmented memory\n-  \/\/ that is available within old, but is not required to hold the resuilts of old evacuation.  As old-gen regions\n-  \/\/ are added into the collection set, their free memory is subtracted from excess_fragmented_available until the\n-  \/\/ excess is exhausted.  For old-gen regions subsequently added to the collection set, their free memory is\n-  \/\/ subtracted from fragmented_available and from the old_evacuation_budget (since the budget decreases when this\n-  \/\/ fragmented_available memory decreases).  After fragmented_available has been exhausted, any further old regions\n-  \/\/ selected for the cset do not further decrease the old_evacuation_budget because all further evacuation is targeted\n-  \/\/ to unfragmented regions.\n-\n-  size_t unaffiliated_available = _old_generation->free_unaffiliated_regions() * ShenandoahHeapRegion::region_size_bytes();\n-  if (unaffiliated_available > _old_evacuation_reserve) {\n-    _unfragmented_available = _old_evacuation_budget;\n-    _fragmented_available = 0;\n-    _excess_fragmented_available = 0;\n-  } else {\n-    assert(_old_generation->available() >= _old_evacuation_reserve, \"Cannot reserve more than is available\");\n-    size_t affiliated_available = _old_generation->available() - unaffiliated_available;\n-    assert(affiliated_available + unaffiliated_available >= _old_evacuation_reserve, \"Budgets do not add up\");\n-    if (affiliated_available + unaffiliated_available > _old_evacuation_reserve) {\n-      _excess_fragmented_available = (affiliated_available + unaffiliated_available) - _old_evacuation_reserve;\n-      affiliated_available -= _excess_fragmented_available;\n-    }\n-    _fragmented_available = (size_t) ((double) affiliated_available \/ ShenandoahOldEvacWaste);\n-    _unfragmented_available = (size_t) ((double) unaffiliated_available \/ ShenandoahOldEvacWaste);\n-  }\n-  log_info(gc)(\"Choose old regions for mixed collection: old evacuation budget: \" SIZE_FORMAT \"%s, candidates: %u\",\n-               byte_size_in_proper_unit(_old_evacuation_budget), proper_unit_for_byte_size(_old_evacuation_budget),\n-               unprocessed_old_collection_candidates());\n-  return add_old_regions_to_cset();\n-}\n-\n@@ -294,56 +350,0 @@\n-void ShenandoahOldHeuristics::slide_pinned_regions_to_front() {\n-  \/\/ Find the first unpinned region to the left of the next region that\n-  \/\/ will be added to the collection set. These regions will have been\n-  \/\/ added to the cset, so we can use them to hold pointers to regions\n-  \/\/ that were pinned when the cset was chosen.\n-  \/\/ [ r p r p p p r r ]\n-  \/\/     ^         ^ ^\n-  \/\/     |         | | pointer to next region to add to a mixed collection is here.\n-  \/\/     |         | first r to the left should be in the collection set now.\n-  \/\/     | first pinned region, we don't need to look past this\n-  uint write_index = NOT_FOUND;\n-  for (uint search = _next_old_collection_candidate - 1; search > _first_pinned_candidate; --search) {\n-    ShenandoahHeapRegion* region = _region_data[search]._region;\n-    if (!region->is_pinned()) {\n-      write_index = search;\n-      assert(region->is_cset(), \"Expected unpinned region to be added to the collection set.\");\n-      break;\n-    }\n-  }\n-\n-  \/\/ If we could not find an unpinned region, it means there are no slots available\n-  \/\/ to move up the pinned regions. In this case, we just reset our next index in the\n-  \/\/ hopes that some of these regions will become unpinned before the next mixed\n-  \/\/ collection. We may want to bailout of here instead, as it should be quite\n-  \/\/ rare to have so many pinned regions and may indicate something is wrong.\n-  if (write_index == NOT_FOUND) {\n-    assert(_first_pinned_candidate != NOT_FOUND, \"Should only be here if there are pinned regions.\");\n-    _next_old_collection_candidate = _first_pinned_candidate;\n-    return;\n-  }\n-\n-  \/\/ Find pinned regions to the left and move their pointer into a slot\n-  \/\/ that was pointing at a region that has been added to the cset (or was pointing\n-  \/\/ to a pinned region that we've already moved up). We are done when the leftmost\n-  \/\/ pinned region has been slid up.\n-  \/\/ [ r p r x p p p r ]\n-  \/\/         ^       ^\n-  \/\/         |       | next region for mixed collections\n-  \/\/         | Write pointer is here. We know this region is already in the cset\n-  \/\/         | so we can clobber it with the next pinned region we find.\n-  for (int32_t search = (int32_t)write_index - 1; search >= (int32_t)_first_pinned_candidate; --search) {\n-    RegionData& skipped = _region_data[search];\n-    if (skipped._region->is_pinned()) {\n-      RegionData& available_slot = _region_data[write_index];\n-      available_slot._region = skipped._region;\n-      available_slot._u._live_data = skipped._u._live_data;\n-      --write_index;\n-    }\n-  }\n-\n-  \/\/ Update to read from the leftmost pinned region. Plus one here because we decremented\n-  \/\/ the write index to hold the next found pinned region. We are just moving it back now\n-  \/\/ to point to the first pinned region.\n-  _next_old_collection_candidate = write_index + 1;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":106,"deletions":106,"binary":false,"changes":212,"status":"modified"},{"patch":"@@ -702,7 +702,0 @@\n-    \/\/\n-    \/\/ TODO: Under severe memory overload conditions that can be checked here, we may want to limit\n-    \/\/ the inclusion of old-gen candidates within the collection set.  This would allow us to prioritize efforts on\n-    \/\/ evacuating young-gen,  This remediation is most appropriate when old-gen availability is very high (so there\n-    \/\/ are negligible negative impacts from delaying completion of old-gen evacuation) and when young-gen collections\n-    \/\/ are \"under duress\" (as signalled by very low availability of memory within young-gen, indicating that\/ young-gen\n-    \/\/ collections are not triggering frequently enough).\n@@ -711,17 +704,0 @@\n-    \/\/ Upon return from prepare_regions_and_collection_set(), certain parameters have been established to govern the\n-    \/\/ evacuation efforts that are about to begin.  In particular:\n-    \/\/\n-    \/\/ old_generation->get_promoted_reserve() represents the amount of memory within old-gen's available memory that has\n-    \/\/   been set aside to hold objects promoted from young-gen memory.  This represents an estimated percentage\n-    \/\/   of the live young-gen memory within the collection set.  If there is more data ready to be promoted than\n-    \/\/   can fit within this reserve, the promotion of some objects will be deferred until a subsequent evacuation\n-    \/\/   pass.\n-    \/\/\n-    \/\/ old_generation->get_evacuation_reserve() represents the amount of memory within old-gen's available memory that has been\n-    \/\/  set aside to hold objects evacuated from the old-gen collection set.\n-    \/\/\n-    \/\/ young_generation->get_evacuation_reserve() represents the amount of memory within young-gen's available memory that has\n-    \/\/  been set aside to hold objects evacuated from the young-gen collection set.  Conservatively, this value\n-    \/\/  equals the entire amount of live young-gen memory within the collection set, even though some of this memory\n-    \/\/  will likely be promoted.\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":0,"deletions":24,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -653,0 +653,7 @@\n+  \/\/ TODO: Under severe memory overload conditions that can be checked here, we may want to limit\n+  \/\/ the inclusion of old-gen candidates within the collection set.  This would allow us to prioritize efforts on\n+  \/\/ evacuating young-gen,  This remediation is most appropriate when old-gen availability is very high (so there\n+  \/\/ are negligible negative impacts from delaying completion of old-gen evacuation) and when young-gen collections\n+  \/\/ are \"under duress\" (as signalled by very low availability of memory within young-gen, indicating that young-gen\n+  \/\/ collections are not triggering frequently enough).\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -173,0 +173,16 @@\n+  \/\/ Upon return from prepare_regions_and_collection_set(), certain parameters have been established to govern the\n+  \/\/ evacuation efforts that are about to begin.  In particular:\n+  \/\/\n+  \/\/ old_generation->get_promoted_reserve() represents the amount of memory within old-gen's available memory that has\n+  \/\/   been set aside to hold objects promoted from young-gen memory.  This represents an estimated percentage\n+  \/\/   of the live young-gen memory within the collection set.  If there is more data ready to be promoted than\n+  \/\/   can fit within this reserve, the promotion of some objects will be deferred until a subsequent evacuation\n+  \/\/   pass.\n+  \/\/\n+  \/\/ old_generation->get_evacuation_reserve() represents the amount of memory within old-gen's available memory that has been\n+  \/\/  set aside to hold objects evacuated from the old-gen collection set.\n+  \/\/\n+  \/\/ young_generation->get_evacuation_reserve() represents the amount of memory within young-gen's available memory that has\n+  \/\/  been set aside to hold objects evacuated from the young-gen collection set.  Conservatively, this value\n+  \/\/  equals the entire amount of live young-gen memory within the collection set, even though some of this memory\n+  \/\/  will likely be promoted.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"}]}