{"files":[{"patch":"@@ -16541,0 +16541,1 @@\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -16544,2 +16545,0 @@\n-  \/\/ TODO\n-  \/\/ identify correct cost\n@@ -16547,1 +16546,1 @@\n-  format %{ \"fastlock $object,$box\\t! kills $tmp,$tmp2\" %}\n+  format %{ \"fastlock $object,$box\\t! kills $tmp,$tmp2,$tmp3\" %}\n@@ -16558,0 +16557,1 @@\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -16571,0 +16571,31 @@\n+instruct cmpFastLockLightweight(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2)\n+%{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastLock object box));\n+  effect(TEMP tmp, TEMP tmp2);\n+\n+  ins_cost(5 * INSN_COST);\n+  format %{ \"fastlock $object,$box\\t! kills $tmp,$tmp2\" %}\n+\n+  ins_encode %{\n+    __ fast_lock_lightweight($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register);\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct cmpFastUnlockLightweight(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2)\n+%{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastUnlock object box));\n+  effect(TEMP tmp, TEMP tmp2);\n+\n+  ins_cost(5 * INSN_COST);\n+  format %{ \"fastunlock $object,$box\\t! kills $tmp, $tmp2\" %}\n+\n+  ins_encode %{\n+    __ fast_unlock_lightweight($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register);\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":34,"deletions":3,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -90,2 +90,0 @@\n-  \/\/ Load object header\n-  ldr(hdr, Address(obj, hdr_offset));\n@@ -95,0 +93,2 @@\n+    \/\/ Load object header\n+    ldr(hdr, Address(obj, hdr_offset));\n@@ -164,5 +164,0 @@\n-    ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n-    \/\/ We cannot use tbnz here, the target might be too far away and cannot\n-    \/\/ be encoded.\n-    tst(hdr, markWord::monitor_value);\n-    br(Assembler::NE, slow_case);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":3,"deletions":8,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -58,0 +59,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_lock_lightweight\");\n@@ -80,1 +82,2 @@\n-  } else if (LockingMode == LM_LEGACY) {\n+  } else {\n+    assert(LockingMode == LM_LEGACY, \"must be\");\n@@ -112,4 +115,0 @@\n-  } else {\n-    assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-    lightweight_lock(oop, disp_hdr, tmp, tmp3Reg, cont);\n-    b(cont);\n@@ -129,8 +128,7 @@\n-  if (LockingMode != LM_LIGHTWEIGHT) {\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n-    mov(tmp, (address)markWord::unused_mark().value());\n-    str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-  }\n+  \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+  \/\/ lock. The fast-path monitor unlock code checks for\n+  \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+  \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n+  mov(tmp, (address)markWord::unused_mark().value());\n+  str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n@@ -160,0 +158,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_unlock_lightweight\");\n@@ -182,1 +181,2 @@\n-  } else if (LockingMode == LM_LEGACY) {\n+  } else {\n+    assert(LockingMode == LM_LEGACY, \"must be\");\n@@ -190,4 +190,0 @@\n-  } else {\n-    assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-    lightweight_unlock(oop, tmp, box, disp_hdr, cont);\n-    b(cont);\n@@ -203,13 +199,0 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    \/\/ If the owner is anonymous, we need to fix it -- in an outline stub.\n-    Register tmp2 = disp_hdr;\n-    ldr(tmp2, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n-    \/\/ We cannot use tbnz here, the target might be too far away and cannot\n-    \/\/ be encoded.\n-    tst(tmp2, (uint64_t)ObjectMonitor::ANONYMOUS_OWNER);\n-    C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmp, tmp2);\n-    Compile::current()->output()->add_stub(stub);\n-    br(Assembler::NE, stub->entry());\n-    bind(stub->continuation());\n-  }\n-\n@@ -242,0 +225,252 @@\n+void C2_MacroAssembler::fast_lock_lightweight(Register obj, Register t1,\n+                                              Register t2, Register t3) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  assert_different_registers(obj, t1, t2, t3);\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated;\n+  \/\/ Finish fast lock successfully. MUST branch to with flag == EQ\n+  Label locked;\n+  \/\/ Finish fast lock unsuccessfully. MUST branch to with flag == NE\n+  Label slow_path;\n+\n+  if (DiagnoseSyncOnValueBasedClasses != 0) {\n+    load_klass(t1, obj);\n+    ldrw(t1, Address(t1, Klass::access_flags_offset()));\n+    tstw(t1, JVM_ACC_IS_VALUE_BASED_CLASS);\n+    br(Assembler::NE, slow_path);\n+  }\n+\n+  const Register t1_mark = t1;\n+\n+  { \/\/ Lightweight locking\n+\n+    \/\/ Push lock to the lock stack and finish successfully. MUST branch to with flag == EQ\n+    Label push;\n+\n+    const Register t2_top = t2;\n+    const Register t3_t = t3;\n+\n+    \/\/ Check if lock-stack is full.\n+    ldrw(t2_top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    cmpw(t2_top, (unsigned)LockStack::end_offset() - 1);\n+    br(Assembler::GT, slow_path);\n+\n+    \/\/ Check if recursive.\n+    subw(t3_t, t2_top, oopSize);\n+    ldr(t3_t, Address(rthread, t3_t));\n+    cmp(obj, t3_t);\n+    br(Assembler::EQ, push);\n+\n+    \/\/ Relaxed normal load to check for monitor. Optimization for monitor case.\n+    ldr(t1_mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    tbnz(t1_mark, exact_log2(markWord::monitor_value), inflated);\n+\n+    \/\/ Not inflated\n+    assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid a lea\");\n+\n+    \/\/ Try to lock. Transition lock-bits 0b01 => 0b00\n+    orr(t1_mark, t1_mark, markWord::unlocked_value);\n+    eor(t3_t, t1_mark, markWord::unlocked_value);\n+    cmpxchg(\/*addr*\/ obj, \/*expected*\/ t1_mark, \/*new*\/ t3_t, Assembler::xword,\n+            \/*acquire*\/ true, \/*release*\/ false, \/*weak*\/ false, noreg);\n+    br(Assembler::NE, slow_path);\n+\n+    bind(push);\n+    \/\/ After successful lock, push object on lock-stack.\n+    str(obj, Address(rthread, t2_top));\n+    addw(t2_top, t2_top, oopSize);\n+    strw(t2_top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    b(locked);\n+  }\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated);\n+\n+    \/\/ mark contains the tagged ObjectMonitor*.\n+    const Register t1_tagged_monitor = t1_mark;\n+    const uintptr_t monitor_tag = markWord::monitor_value;\n+    const Register t2_owner_addr = t2;\n+    const Register t3_owner = t3;\n+\n+    \/\/ Compute owner address.\n+    lea(t2_owner_addr, Address(t1_tagged_monitor, ObjectMonitor::owner_offset_in_bytes() - monitor_tag));\n+\n+    \/\/ CAS owner (null => current thread).\n+    cmpxchg(t2_owner_addr, zr, rthread, Assembler::xword, \/*acquire*\/ true,\n+            \/*release*\/ false, \/*weak*\/ false, t3_owner);\n+    br(Assembler::EQ, locked);\n+\n+    \/\/ Check if recursive.\n+    cmp(t3_owner, rthread);\n+    br(Assembler::NE, slow_path);\n+\n+    \/\/ Recursive.\n+    increment(Address(t1_tagged_monitor, ObjectMonitor::recursions_offset_in_bytes() - monitor_tag), 1);\n+  }\n+\n+  bind(locked);\n+#ifdef ASSERT\n+  \/\/ Check that locked label is reached with Flags == EQ.\n+  Label flag_correct;\n+  br(Assembler::EQ, flag_correct);\n+  stop(\"Fast Lock Flag != EQ\");\n+#endif\n+\n+  bind(slow_path);\n+#ifdef ASSERT\n+  \/\/ Check that slow_path label is reached with Flags == NE.\n+  br(Assembler::NE, flag_correct);\n+  stop(\"Fast Lock Flag != NE\");\n+  bind(flag_correct);\n+#endif\n+  \/\/ C2 uses the value of Flags (NE vs EQ) to determine the continuation.\n+}\n+\n+void C2_MacroAssembler::fast_unlock_lightweight(Register obj, Register t1, Register t2,\n+                                                Register t3) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  assert_different_registers(obj, t1, t2, t3);\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated, inflated_load_monitor;\n+  \/\/ Finish fast unlock successfully. MUST branch to with flag == EQ\n+  Label unlocked;\n+  \/\/ Finish fast unlock unsuccessfully. MUST branch to with flag == NE\n+  Label slow_path;\n+\n+  const Register t1_mark = t1;\n+  const Register t2_top = t2;\n+  const Register t3_t = t3;\n+\n+  { \/\/ Lightweight unlock\n+\n+    \/\/ Check if obj is top of lock-stack.\n+    ldrw(t2_top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    subw(t2_top, t2_top, oopSize);\n+    ldr(t3_t, Address(rthread, t2_top));\n+    cmp(obj, t3_t);\n+    \/\/ Top of lock stack was not obj. Must be monitor.\n+    br(Assembler::NE, inflated_load_monitor);\n+\n+    \/\/ Pop lock-stack.\n+    DEBUG_ONLY(str(zr, Address(rthread, t2_top));)\n+    strw(t2_top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+    \/\/ Check if recursive.\n+    subw(t3_t, t2_top, oopSize);\n+    ldr(t3_t, Address(rthread, t3_t));\n+    cmp(obj, t3_t);\n+    br(Assembler::EQ, unlocked);\n+\n+    \/\/ Not recursive.\n+    \/\/ Load Mark.\n+    ldr(t1_mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+    \/\/ Check header for monitor (0b10).\n+    tbnz(t1_mark, exact_log2(markWord::monitor_value), inflated);\n+\n+    \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+    assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid lea\");\n+    orr(t3_t, t1_mark, markWord::unlocked_value);\n+    cmpxchg(\/*addr*\/ obj, \/*expected*\/ t1_mark, \/*new*\/ t3_t, Assembler::xword,\n+            \/*acquire*\/ false, \/*release*\/ true, \/*weak*\/ false, noreg);\n+    br(Assembler::EQ, unlocked);\n+\n+    \/\/ Compare and exchange failed.\n+    \/\/ Restore lock-stack and handle the unlock in runtime.\n+    DEBUG_ONLY(str(obj, Address(rthread, t2_top));)\n+    addw(t2_top, t2_top, oopSize);\n+    str(t2_top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    b(slow_path);\n+  }\n+\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated_load_monitor);\n+    ldr(t1_mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+#ifdef ASSERT\n+    tbnz(t1_mark, exact_log2(markWord::monitor_value), inflated);\n+    stop(\"Fast Unlock not monitor\");\n+#endif\n+\n+    bind(inflated);\n+\n+#ifdef ASSERT\n+    Label check_done;\n+    subw(t2_top, t2_top, oopSize);\n+    cmpw(t2_top, in_bytes(JavaThread::lock_stack_base_offset()));\n+    br(Assembler::LT, check_done);\n+    ldr(t3_t, Address(rthread, t2_top));\n+    cmp(obj, t3_t);\n+    br(Assembler::NE, inflated);\n+    stop(\"Fast Unlock lock on stack\");\n+    bind(check_done);\n+#endif\n+\n+    \/\/ mark contains the tagged ObjectMonitor*.\n+    const Register t1_monitor = t1_mark;\n+    const uintptr_t monitor_tag = markWord::monitor_value;\n+\n+    \/\/ Untag the monitor.\n+    sub(t1_monitor, t1_mark, monitor_tag);\n+\n+    const Register t2_recursions = t2;\n+    Label not_recursive;\n+\n+    \/\/ Check if recursive.\n+    ldr(t2_recursions, Address(t1_monitor, ObjectMonitor::recursions_offset_in_bytes()));\n+    cbz(t2_recursions, not_recursive);\n+\n+    \/\/ Recursive unlock.\n+    sub(t2_recursions, t2_recursions, 1u);\n+    str(t2_recursions, Address(t1_monitor, ObjectMonitor::recursions_offset_in_bytes()));\n+    \/\/ Set flag == EQ\n+    cmp(t2_recursions, t2_recursions);\n+    b(unlocked);\n+\n+    bind(not_recursive);\n+\n+    Label release;\n+    const Register t2_owner_addr = t2;\n+\n+    \/\/ Compute owner address.\n+    lea(t2_owner_addr, Address(t1_monitor, ObjectMonitor::owner_offset_in_bytes()));\n+\n+    \/\/ Check if the entry lists are empty.\n+    ldr(rscratch1, Address(t1_monitor, ObjectMonitor::EntryList_offset_in_bytes()));\n+    ldr(t3_t, Address(t1_monitor, ObjectMonitor::cxq_offset_in_bytes()));\n+    orr(rscratch1, rscratch1, t3_t);\n+    cmp(rscratch1, zr);\n+    br(Assembler::EQ, release);\n+\n+    \/\/ The owner may be anonymous and we removed the last obj entry in\n+    \/\/ the lock-stack. This loses the information about the owner.\n+    \/\/ Write the thread to the owner field so the runtime knows the owner.\n+    str(rthread, Address(t2_owner_addr));\n+    b(slow_path);\n+\n+    bind(release);\n+    \/\/ Set owner to null.\n+    \/\/ Release to satisfy the JMM\n+    stlr(zr, t2_owner_addr);\n+  }\n+\n+  bind(unlocked);\n+#ifdef ASSERT\n+  \/\/ Check that unlocked label is reached with Flags == EQ.\n+  Label flag_correct;\n+  br(Assembler::EQ, flag_correct);\n+  stop(\"Fast Unlock Flag != EQ\");\n+#endif\n+\n+  bind(slow_path);\n+#ifdef ASSERT\n+  \/\/ Check that slow_path label is reached with Flags == NE.\n+  br(Assembler::NE, flag_correct);\n+  stop(\"Fast Unlock Flag != NE\");\n+  bind(flag_correct);\n+#endif\n+  \/\/ C2 uses the value of Flags (NE vs EQ) to determine the continuation.\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":266,"deletions":31,"binary":false,"changes":297,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,0 @@\n-  \/\/ See full description in macroAssembler_aarch64.cpp.\n@@ -35,0 +34,3 @@\n+  \/\/ Code used by cmpFastLockLightweight and cmpFastUnlockLightweight mach instructions in .ad file.\n+  void fast_lock_lightweight(Register object, Register t1, Register t2, Register t3);\n+  void fast_unlock_lightweight(Register object, Register t1, Register t2, Register t3);\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -760,1 +760,0 @@\n-      ldr(tmp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -893,16 +892,0 @@\n-\n-      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n-      \/\/ must handle it.\n-      Register tmp = rscratch1;\n-      \/\/ First check for lock-stack underflow.\n-      ldrw(tmp, Address(rthread, JavaThread::lock_stack_top_offset()));\n-      cmpw(tmp, (unsigned)LockStack::start_offset());\n-      br(Assembler::LE, slow_case);\n-      \/\/ Then check if the top of the lock-stack matches the unlocked object.\n-      subw(tmp, tmp, oopSize);\n-      ldr(tmp, Address(rthread, tmp));\n-      cmpoop(tmp, obj_reg);\n-      br(Assembler::NE, slow_case);\n-\n-      ldr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      tbnz(header_reg, exact_log2(markWord::monitor_value), slow_case);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":1,"deletions":18,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,2 +26,0 @@\n-#include <sys\/types.h>\n-\n@@ -57,0 +55,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -68,0 +67,2 @@\n+#include <sys\/types.h>\n+\n@@ -5431,2 +5432,0 @@\n-\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n-\/\/ Falls through upon success with ZF set.\n@@ -5435,3 +5434,3 @@\n-\/\/  - hdr: the header, already loaded from obj, will be destroyed\n-\/\/  - t1, t2: temporary registers, will be destroyed\n-void MacroAssembler::lightweight_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+\/\/  - t1, t2, t3: temporary registers, will be destroyed\n+\/\/  - slow: branched to if locking fails, absolute offset may larger than 32KB (imm14 encoding).\n+void MacroAssembler::lightweight_lock(Register obj, Register t1, Register t2, Register t3, Label& slow) {\n@@ -5439,15 +5438,24 @@\n-  assert_different_registers(obj, hdr, t1, t2, rscratch1);\n-\n-  \/\/ Check if we would have space on lock-stack for the object.\n-  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n-  cmpw(t1, (unsigned)LockStack::end_offset() - 1);\n-  br(Assembler::GT, slow);\n-\n-  \/\/ Load (object->mark() | 1) into hdr\n-  orr(hdr, hdr, markWord::unlocked_value);\n-  \/\/ Clear lock-bits, into t2\n-  eor(t2, hdr, markWord::unlocked_value);\n-  \/\/ Try to swing header from unlocked to locked\n-  \/\/ Clobbers rscratch1 when UseLSE is false\n-  cmpxchg(\/*addr*\/ obj, \/*expected*\/ hdr, \/*new*\/ t2, Assembler::xword,\n-          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t1);\n+  assert_different_registers(obj, t1, t2, t3, rscratch1);\n+\n+  Label push;\n+  const Register top = t1;\n+  const Register mark = t2;\n+  const Register t = t3;\n+\n+  \/\/ Preload the markWord. It is important that this is the first\n+  \/\/ instruction emitted as it is part of C1's null check semantics.\n+  ldr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+  \/\/ Check if the lock-stack is full.\n+  ldrw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  cmpw(top, (unsigned)LockStack::end_offset());\n+  br(Assembler::GE, slow);\n+\n+  \/\/ Check for recursion.\n+  subw(t, top, oopSize);\n+  ldr(t, Address(rthread, t));\n+  cmp(obj, t);\n+  br(Assembler::EQ, push);\n+\n+  \/\/ Check header for monitor (0b10).\n+  tst(mark, markWord::monitor_value);\n@@ -5456,5 +5464,13 @@\n-  \/\/ After successful lock, push object on lock-stack\n-  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n-  str(obj, Address(rthread, t1));\n-  addw(t1, t1, oopSize);\n-  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  \/\/ Try to lock. Transition lock bits 0b01 => 0b00\n+  assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid lea\");\n+  orr(mark, mark, markWord::unlocked_value);\n+  eor(t, mark, markWord::unlocked_value);\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ mark, \/*new*\/ t, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ false, \/*weak*\/ false, noreg);\n+  br(Assembler::NE, slow);\n+\n+  bind(push);\n+  \/\/ After successful lock, push object on lock-stack.\n+  str(obj, Address(rthread, top));\n+  addw(top, top, oopSize);\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n@@ -5464,2 +5480,0 @@\n-\/\/ Branches to slow upon failure, with ZF cleared.\n-\/\/ Falls through upon success, with ZF set.\n@@ -5468,3 +5482,3 @@\n-\/\/ - hdr: the (pre-loaded) header of the object\n-\/\/ - t1, t2: temporary registers\n-void MacroAssembler::lightweight_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+\/\/ - t1, t2, t3: temporary registers\n+\/\/ - slow: branched to if unlocking fails, absolute offset may larger than 32KB (imm14 encoding).\n+void MacroAssembler::lightweight_unlock(Register obj, Register t1, Register t2, Register t3, Label& slow) {\n@@ -5472,1 +5486,2 @@\n-  assert_different_registers(obj, hdr, t1, t2, rscratch1);\n+  \/\/ cmpxchg clobbers rscratch1.\n+  assert_different_registers(obj, t1, t2, t3, rscratch1);\n@@ -5476,4 +5491,0 @@\n-    \/\/ The following checks rely on the fact that LockStack is only ever modified by\n-    \/\/ its owning thread, even if the lock got inflated concurrently; removal of LockStack\n-    \/\/ entries after inflation will happen delayed in that case.\n-\n@@ -5484,1 +5495,1 @@\n-    br(Assembler::GT, stack_ok);\n+    br(Assembler::GE, stack_ok);\n@@ -5488,18 +5499,0 @@\n-  {\n-    \/\/ Check if the top of the lock-stack matches the unlocked object.\n-    Label tos_ok;\n-    subw(t1, t1, oopSize);\n-    ldr(t1, Address(rthread, t1));\n-    cmpoop(t1, obj);\n-    br(Assembler::EQ, tos_ok);\n-    STOP(\"Top of lock-stack does not match the unlocked object\");\n-    bind(tos_ok);\n-  }\n-  {\n-    \/\/ Check that hdr is fast-locked.\n-    Label hdr_ok;\n-    tst(hdr, markWord::lock_mask_in_place);\n-    br(Assembler::EQ, hdr_ok);\n-    STOP(\"Header is not fast-locked\");\n-    bind(hdr_ok);\n-  }\n@@ -5508,2 +5501,4 @@\n-  \/\/ Load the new header (unlocked) into t1\n-  orr(t1, hdr, markWord::unlocked_value);\n+  Label unlocked, push_and_slow;\n+  const Register top = t1;\n+  const Register mark = t2;\n+  const Register t = t3;\n@@ -5511,4 +5506,5 @@\n-  \/\/ Try to swing header from locked to unlocked\n-  \/\/ Clobbers rscratch1 when UseLSE is false\n-  cmpxchg(obj, hdr, t1, Assembler::xword,\n-          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t2);\n+  \/\/ Check if obj is top of lock-stack.\n+  ldrw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  subw(top, top, oopSize);\n+  ldr(t, Address(rthread, top));\n+  cmp(obj, t);\n@@ -5517,3 +5513,14 @@\n-  \/\/ After successful unlock, pop object from lock-stack\n-  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n-  subw(t1, t1, oopSize);\n+  \/\/ Pop lock-stack.\n+  DEBUG_ONLY(str(zr, Address(rthread, top));)\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Check if recursive.\n+  subw(t, top, oopSize);\n+  ldr(t, Address(rthread, t));\n+  cmp(obj, t);\n+  br(Assembler::EQ, unlocked);\n+\n+  \/\/ Not recursive. Check header for monitor (0b10).\n+  ldr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  tbnz(mark, log2i_exact(markWord::monitor_value), push_and_slow);\n+\n@@ -5521,1 +5528,5 @@\n-  str(zr, Address(rthread, t1));\n+  \/\/ Check header not unlocked (0b01).\n+  Label not_unlocked;\n+  tbz(mark, log2i_exact(markWord::unlocked_value), not_unlocked);\n+  stop(\"lightweight_unlock already unlocked\");\n+  bind(not_unlocked);\n@@ -5523,1 +5534,16 @@\n-  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+  assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid lea\");\n+  orr(t, mark, markWord::unlocked_value);\n+  cmpxchg(obj, mark, t, Assembler::xword,\n+          \/*acquire*\/ false, \/*release*\/ true, \/*weak*\/ false, noreg);\n+  br(Assembler::EQ, unlocked);\n+\n+  bind(push_and_slow);\n+  \/\/ Restore lock-stack and handle the unlock in runtime.\n+  DEBUG_ONLY(str(obj, Address(rthread, top));)\n+  addw(top, top, oopSize);\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  b(slow);\n+\n+  bind(unlocked);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":94,"deletions":68,"binary":false,"changes":162,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1427,2 +1427,2 @@\n-  void lightweight_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n-  void lightweight_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+  void lightweight_lock(Register obj, Register t1, Register t2, Register t3, Label& slow);\n+  void lightweight_unlock(Register obj, Register t1, Register t2, Register t3, Label& slow);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1815,1 +1815,0 @@\n-      __ ldr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1965,2 +1964,0 @@\n-      __ ldr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ tbnz(old_hdr, exact_log2(markWord::monitor_value), slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -156,0 +156,1 @@\n+  constexpr static bool supports_recursive_lightweight_locking() { return true; }\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"}]}