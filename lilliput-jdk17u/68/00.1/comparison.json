{"files":[{"patch":"@@ -817,0 +817,5 @@\n+  # Add more Lilliput-specific ProblemLists when UCOH is enabled\n+  ifneq ($$(findstring -XX:+UseCompactObjectHeaders, $$(TEST_OPTS)), )\n+    JTREG_EXTRA_PROBLEM_LISTS += $(TOPDIR)\/test\/hotspot\/jtreg\/ProblemList-lilliput.txt\n+  endif\n+\n","filename":"make\/RunTests.gmk","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1242,1 +1242,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n@@ -2359,2 +2359,2 @@\n-        __ ldrw(tmp, src_klass_addr);\n-        __ ldrw(rscratch1, dst_klass_addr);\n+        __ load_nklass(tmp, src);\n+        __ load_nklass(rscratch1, dst);\n@@ -2363,2 +2363,2 @@\n-        __ ldr(tmp, src_klass_addr);\n-        __ ldr(rscratch1, dst_klass_addr);\n+        __ ldr(tmp, Address(src, oopDesc::klass_offset_in_bytes()));\n+        __ ldr(rscratch1, Address(dst, oopDesc::klass_offset_in_bytes()));\n@@ -2488,3 +2488,0 @@\n-    if (UseCompressedClassPointers) {\n-      __ encode_klass_not_null(tmp);\n-    }\n@@ -2493,8 +2490,1 @@\n-\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(dst, tmp, rscratch1);\n@@ -2502,7 +2492,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, src_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, src_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(src, tmp, rscratch1);\n@@ -2511,7 +2495,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(dst, tmp, rscratch1);\n@@ -2565,1 +2543,5 @@\n-  if (!UseFastLocking) {\n+  if (LockingMode == LM_MONITOR) {\n+    if (op->info() != NULL) {\n+      add_debug_info_for_null_check_here(op->info());\n+      __ null_check(obj, -1);\n+    }\n@@ -2598,1 +2580,12 @@\n-    __ ldrw(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n+    if (UseCompactObjectHeaders) {\n+      \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+      __ ldr(result, Address(obj, oopDesc::mark_offset_in_bytes()));\n+      __ tst(result, markWord::monitor_value);\n+      __ br(Assembler::NE, *op->stub()->entry());\n+      __ bind(*op->stub()->continuation());\n+\n+      \/\/ Shift to get proper narrow Klass*.\n+      __ lsr(result, result, markWord::klass_shift);\n+    } else {\n+      __ ldrw(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":25,"deletions":32,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -729,1 +729,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -757,21 +757,3 @@\n-    if (UseBiasedLocking) {\n-      biased_locking_enter(lock_reg, obj_reg, swap_reg, tmp, false, done, &slow_case);\n-    }\n-\n-    \/\/ Load (object->mark() | 1) into swap_reg\n-    ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    orr(swap_reg, rscratch1, 1);\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    str(swap_reg, Address(lock_reg, mark_offset));\n-\n-    assert(lock_offset == 0,\n-           \"displached header must be first word in BasicObjectLock\");\n-\n-    Label fail;\n-    if (PrintBiasedLockingStatistics) {\n-      Label fast;\n-      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, fast, &fail);\n-      bind(fast);\n-      atomic_incw(Address((address)BiasedLocking::fast_path_entry_count_addr()),\n-                  rscratch2, rscratch1, tmp);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      ldr(tmp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock(obj_reg, tmp, rscratch1, rscratch2, slow_case);\n@@ -779,3 +761,3 @@\n-      bind(fail);\n-      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, done, \/*fallthrough*\/NULL);\n-    }\n+      if (UseBiasedLocking) {\n+        biased_locking_enter(lock_reg, obj_reg, swap_reg, tmp, false, done, &slow_case);\n+      }\n@@ -784,42 +766,22 @@\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & 7) == 0, and\n-    \/\/  2) sp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from sp is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 3 bits clear.\n-    \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n-    \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n-    \/\/ copy\n-    mov(rscratch1, sp);\n-    sub(swap_reg, swap_reg, rscratch1);\n-    ands(swap_reg, swap_reg, (uint64_t)(7 - os::vm_page_size()));\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    str(swap_reg, Address(lock_reg, mark_offset));\n-\n-    if (PrintBiasedLockingStatistics) {\n-      br(Assembler::NE, slow_case);\n-      atomic_incw(Address((address)BiasedLocking::fast_path_entry_count_addr()),\n-                  rscratch2, rscratch1, tmp);\n-    }\n-    br(Assembler::EQ, done);\n+      \/\/ Load (object->mark() | 1) into swap_reg\n+      ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      orr(swap_reg, rscratch1, 1);\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      str(swap_reg, Address(lock_reg, mark_offset));\n+\n+      assert(lock_offset == 0,\n+             \"displached header must be first word in BasicObjectLock\");\n+\n+      Label fail;\n+      if (PrintBiasedLockingStatistics) {\n+        Label fast;\n+        cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, fast, &fail);\n+        bind(fast);\n+        atomic_incw(Address((address)BiasedLocking::fast_path_entry_count_addr()),\n+                    rscratch2, rscratch1, tmp);\n+        b(done);\n+        bind(fail);\n+      } else {\n+        cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, done, \/*fallthrough*\/NULL);\n+      }\n@@ -827,0 +789,43 @@\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & 7) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from sp is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 3 bits clear.\n+      \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n+      \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n+      \/\/ copy\n+      mov(rscratch1, sp);\n+      sub(swap_reg, swap_reg, rscratch1);\n+      ands(swap_reg, swap_reg, (uint64_t)(7 - os::vm_page_size()));\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      str(swap_reg, Address(lock_reg, mark_offset));\n+\n+      if (PrintBiasedLockingStatistics) {\n+        br(Assembler::NE, slow_case);\n+        atomic_incw(Address((address)BiasedLocking::fast_path_entry_count_addr()),\n+                    rscratch2, rscratch1, tmp);\n+      }\n+      br(Assembler::EQ, done);\n+    }\n@@ -830,3 +835,9 @@\n-    call_VM(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            lock_reg);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n+              obj_reg);\n+    } else {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+              lock_reg);\n+    }\n@@ -854,1 +865,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -865,3 +876,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into %r0\n-    lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into %r0\n+      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    }\n@@ -875,3 +888,15 @@\n-    if (UseBiasedLocking) {\n-      biased_locking_exit(obj_reg, header_reg, done);\n-    }\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      Label slow_case;\n+\n+      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+      \/\/ must handle it.\n+      Register tmp = rscratch1;\n+      \/\/ First check for lock-stack underflow.\n+      ldrw(tmp, Address(rthread, JavaThread::lock_stack_top_offset()));\n+      cmpw(tmp, (unsigned)LockStack::start_offset());\n+      br(Assembler::LE, slow_case);\n+      \/\/ Then check if the top of the lock-stack matches the unlocked object.\n+      subw(tmp, tmp, oopSize);\n+      ldr(tmp, Address(rthread, tmp));\n+      cmpoop(tmp, obj_reg);\n+      br(Assembler::NE, slow_case);\n@@ -879,3 +904,9 @@\n-    \/\/ Load the old header from BasicLock structure\n-    ldr(header_reg, Address(swap_reg,\n-                            BasicLock::displaced_header_offset_in_bytes()));\n+      ldr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      tbnz(header_reg, exact_log2(markWord::monitor_value), slow_case);\n+      fast_unlock(obj_reg, header_reg, swap_reg, rscratch1, slow_case);\n+      b(done);\n+      bind(slow_case);\n+    } else {\n+      if (UseBiasedLocking) {\n+        biased_locking_exit(obj_reg, header_reg, done);\n+      }\n@@ -883,2 +914,3 @@\n-    \/\/ Test for recursion\n-    cbz(header_reg, done);\n+      \/\/ Load the old header from BasicLock structure\n+      ldr(header_reg, Address(swap_reg,\n+                              BasicLock::displaced_header_offset_in_bytes()));\n@@ -886,2 +918,2 @@\n-    \/\/ Atomic swap back the old header\n-    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, done, \/*fallthrough*\/NULL);\n+      \/\/ Test for recursion\n+      cbz(header_reg, done);\n@@ -889,0 +921,3 @@\n+      \/\/ Atomic swap back the old header\n+      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, done, \/*fallthrough*\/NULL);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":119,"deletions":84,"binary":false,"changes":203,"status":"modified"},{"patch":"@@ -1646,1 +1646,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n@@ -3071,0 +3071,1 @@\n+  Register tmp2 = UseCompactObjectHeaders ? rscratch2 : noreg;\n@@ -3262,7 +3263,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ movl(tmp, src_klass_addr);\n-        __ cmpl(tmp, dst_klass_addr);\n-      } else {\n-        __ movptr(tmp, src_klass_addr);\n-        __ cmpptr(tmp, dst_klass_addr);\n-      }\n+      __ cmp_klass(src, dst, tmp, tmp2);\n@@ -3428,4 +3423,1 @@\n-\n-\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmp_klass(tmp, dst, tmp2);\n@@ -3434,2 +3426,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, src_klass_addr);\n-      else                   __ cmpptr(tmp, src_klass_addr);\n+      __ cmp_klass(tmp, src, tmp2);\n@@ -3438,2 +3429,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmp_klass(tmp, dst, tmp2);\n@@ -3501,1 +3491,1 @@\n-  if (!UseFastLocking) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -3505,1 +3495,1 @@\n-    if (UseBiasedLocking) {\n+    if (UseBiasedLocking || LockingMode == LM_LIGHTWEIGHT) {\n@@ -3534,1 +3524,14 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    Register tmp = rscratch1;\n+    assert_different_registers(tmp, obj);\n+    assert_different_registers(tmp, result);\n+\n+    \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+    __ movq(result, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    __ testb(result, markWord::monitor_value);\n+    __ jcc(Assembler::notZero, *op->stub()->entry());\n+    __ bind(*op->stub()->continuation());\n+    \/\/ Fast-path: shift and decode Klass*.\n+    __ shrq(result, markWord::klass_shift);\n+    __ decode_klass_not_null(result, tmp);\n+  } else if (UseCompressedClassPointers) {\n@@ -3665,4 +3668,0 @@\n-#ifndef ASSERT\n-      __ jmpb(next);\n-    }\n-#else\n@@ -3671,0 +3670,1 @@\n+#ifdef ASSERT\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":23,"deletions":23,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -70,1 +70,1 @@\n-  jmpb(next);\n+  jmp(next);\n@@ -1220,1 +1220,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -1254,2 +1254,14 @@\n-    \/\/ Load immediate 1 into swap_reg %rax\n-    movl(swap_reg, (int32_t)1);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = lock_reg;\n+      get_thread(thread);\n+#endif\n+      \/\/ Load object header, prepare for CAS from unlocked to locked.\n+      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock_impl(obj_reg, swap_reg, thread, tmp_reg, slow_case);\n+      jmp(done);\n+    } else {\n+      \/\/ Load immediate 1 into swap_reg %rax\n+      movl(swap_reg, (int32_t)1);\n@@ -1257,2 +1269,2 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg %rax\n-    orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      \/\/ Load (object->mark() | 1) into swap_reg %rax\n+      orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1260,2 +1272,2 @@\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    movptr(Address(lock_reg, mark_offset), swap_reg);\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      movptr(Address(lock_reg, mark_offset), swap_reg);\n@@ -1263,2 +1275,2 @@\n-    assert(lock_offset == 0,\n-           \"displaced header must be first word in BasicObjectLock\");\n+      assert(lock_offset == 0,\n+             \"displaced header must be first word in BasicObjectLock\");\n@@ -1266,46 +1278,48 @@\n-    lock();\n-    cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    if (PrintBiasedLockingStatistics) {\n-      cond_inc32(Assembler::zero,\n-                 ExternalAddress((address) BiasedLocking::fast_path_entry_count_addr()));\n-    }\n-    jcc(Assembler::zero, done);\n-\n-    const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n-\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & zero_bits) == 0, and\n-    \/\/  2) rsp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from rsp is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant bits clear.\n-    \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n-    subptr(swap_reg, rsp);\n-    andptr(swap_reg, zero_bits - os::vm_page_size());\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    movptr(Address(lock_reg, mark_offset), swap_reg);\n-\n-    if (PrintBiasedLockingStatistics) {\n-      cond_inc32(Assembler::zero,\n-                 ExternalAddress((address) BiasedLocking::fast_path_entry_count_addr()));\n+      lock();\n+      cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      if (PrintBiasedLockingStatistics) {\n+        cond_inc32(Assembler::zero,\n+                   ExternalAddress((address) BiasedLocking::fast_path_entry_count_addr()));\n+      }\n+      jcc(Assembler::zero, done);\n+\n+      const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n+\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & zero_bits) == 0, and\n+      \/\/  2) rsp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from rsp is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant bits clear.\n+      \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n+      subptr(swap_reg, rsp);\n+      andptr(swap_reg, zero_bits - os::vm_page_size());\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      movptr(Address(lock_reg, mark_offset), swap_reg);\n+\n+      if (PrintBiasedLockingStatistics) {\n+        cond_inc32(Assembler::zero,\n+                   ExternalAddress((address) BiasedLocking::fast_path_entry_count_addr()));\n+      }\n+      jcc(Assembler::zero, done);\n@@ -1313,2 +1327,0 @@\n-    jcc(Assembler::zero, done);\n-\n@@ -1318,4 +1330,9 @@\n-    call_VM(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            lock_reg);\n-\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n+              obj_reg);\n+    } else {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+              lock_reg);\n+    }\n@@ -1343,1 +1360,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -1346,1 +1363,1 @@\n-    Label done;\n+    Label done, slow_case;\n@@ -1354,3 +1371,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into %rax\n-    lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into %rax\n+      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    }\n@@ -1364,7 +1383,21 @@\n-    if (UseBiasedLocking) {\n-      biased_locking_exit(obj_reg, header_reg, done);\n-    }\n-\n-    \/\/ Load the old header from BasicLock structure\n-    movptr(header_reg, Address(swap_reg,\n-                               BasicLock::displaced_header_offset_in_bytes()));\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = header_reg;\n+      get_thread(thread);\n+#endif\n+      \/\/ Handle unstructured locking.\n+      Register tmp = swap_reg;\n+      movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n+      cmpptr(obj_reg, Address(thread, tmp, Address::times_1,  -oopSize));\n+      jcc(Assembler::notEqual, slow_case);\n+      \/\/ Try to swing header from locked to unlock.\n+      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+      fast_unlock_impl(obj_reg, swap_reg, header_reg, slow_case);\n+      jmp(done);\n+    } else {\n+      if (UseBiasedLocking) {\n+        biased_locking_exit(obj_reg, header_reg, done);\n+      }\n@@ -1372,2 +1405,3 @@\n-    \/\/ Test for recursion\n-    testptr(header_reg, header_reg);\n+      \/\/ Load the old header from BasicLock structure\n+      movptr(header_reg, Address(swap_reg,\n+                                 BasicLock::displaced_header_offset_in_bytes()));\n@@ -1375,2 +1409,2 @@\n-    \/\/ zero for recursive case\n-    jcc(Assembler::zero, done);\n+      \/\/ Test for recursion\n+      testptr(header_reg, header_reg);\n@@ -1378,3 +1412,2 @@\n-    \/\/ Atomic swap back the old header\n-    lock();\n-    cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      \/\/ zero for recursive case\n+      jcc(Assembler::zero, done);\n@@ -1382,2 +1415,3 @@\n-    \/\/ zero for simple unlock of a stack-lock case\n-    jcc(Assembler::zero, done);\n+      \/\/ Atomic swap back the old header\n+      lock();\n+      cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1385,0 +1419,3 @@\n+      \/\/ zero for simple unlock of a stack-lock case\n+      jcc(Assembler::zero, done);\n+    }\n@@ -1386,0 +1423,1 @@\n+    bind(slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":121,"deletions":83,"binary":false,"changes":204,"status":"modified"},{"patch":"@@ -342,1 +342,4 @@\n-  void load_klass(Register dst, Register src, Register tmp);\n+  void load_klass(Register dst, Register src, Register tmp, bool null_check_src = false);\n+#ifdef _LP64\n+  void load_nklass(Register dst, Register src);\n+#endif\n@@ -345,0 +348,8 @@\n+  \/\/ Compares the Klass pointer of an object to a given Klass (which might be narrow,\n+  \/\/ depending on UseCompressedClassPointers).\n+  void cmp_klass(Register klass, Register dst, Register tmp);\n+\n+  \/\/ Compares the Klass pointer of two objects o1 and o2. Result is in the condition flags.\n+  \/\/ Uses t1 and t2 as temporary registers.\n+  void cmp_klass(Register src, Register dst, Register tmp1, Register tmp2);\n+\n@@ -1916,0 +1927,3 @@\n+\n+  void fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow);\n+  void fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -220,0 +220,1 @@\n+  _compact_headers = UseCompactObjectHeaders;\n@@ -282,0 +283,1 @@\n+  st->print_cr(\"- compact_headers:                %d\", _compact_headers);\n@@ -2261,0 +2263,8 @@\n+  if (compact_headers() != UseCompactObjectHeaders) {\n+    log_info(cds)(\"The shared archive file's UseCompactObjectHeaders setting (%s)\"\n+                  \" does not equal the current UseCompactObjectHeaders setting (%s).\",\n+                  _compact_headers          ? \"enabled\" : \"disabled\",\n+                  UseCompactObjectHeaders   ? \"enabled\" : \"disabled\");\n+    return false;\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -191,0 +191,1 @@\n+  bool   _compact_headers;          \/\/ value of UseCompactObjectHeaders\n@@ -256,0 +257,1 @@\n+  bool compact_headers()                   const { return _compact_headers; }\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -689,1 +689,1 @@\n-  old->forward_to(old);\n+  old->forward_to_self();\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -225,0 +226,2 @@\n+    SlidingForwarding::begin();\n+\n@@ -239,0 +242,1 @@\n+    SlidingForwarding::end();\n@@ -300,0 +304,1 @@\n+template <bool ALT_FWD>\n@@ -368,1 +373,1 @@\n-    p->forward_to(cast_to_oop(_compact_point));\n+    SlidingForwarding::forward_to<ALT_FWD>(p, cast_to_oop(_compact_point));\n@@ -399,0 +404,10 @@\n+    if (UseAltGCForwarding) {\n+      work_impl<true>(worker_id);\n+    } else {\n+      work_impl<false>(worker_id);\n+    }\n+  }\n+\n+private:\n+  template <bool ALT_FWD>\n+  void work_impl(uint worker_id) {\n@@ -414,1 +429,1 @@\n-    ShenandoahPrepareForCompactionObjectClosure cl(_preserved_marks->get(worker_id), empty_regions, from_region);\n+    ShenandoahPrepareForCompactionObjectClosure<ALT_FWD> cl(_preserved_marks->get(worker_id), empty_regions, from_region);\n@@ -440,1 +455,2 @@\n-void ShenandoahFullGC::calculate_target_humongous_objects() {\n+template <bool ALT_FWD>\n+void ShenandoahFullGC::calculate_target_humongous_objects_impl() {\n@@ -476,1 +492,1 @@\n-        old_obj->forward_to(cast_to_oop(heap->get_region(start)->bottom()));\n+        SlidingForwarding::forward_to<ALT_FWD>(old_obj, cast_to_oop(heap->get_region(start)->bottom()));\n@@ -488,0 +504,8 @@\n+void ShenandoahFullGC::calculate_target_humongous_objects() {\n+  if (UseAltGCForwarding) {\n+    calculate_target_humongous_objects_impl<true>();\n+  } else {\n+    calculate_target_humongous_objects_impl<false>();\n+  }\n+}\n+\n@@ -725,0 +749,1 @@\n+template <bool ALT_FWD>\n@@ -736,2 +761,2 @@\n-      if (obj->is_forwarded()) {\n-        oop forw = obj->forwardee();\n+      if (SlidingForwarding::is_forwarded(obj)) {\n+        oop forw = SlidingForwarding::forwardee<ALT_FWD>(obj);\n@@ -752,0 +777,1 @@\n+template <bool ALT_FWD>\n@@ -755,1 +781,1 @@\n-  ShenandoahAdjustPointersClosure _cl;\n+  ShenandoahAdjustPointersClosure<ALT_FWD> _cl;\n@@ -778,1 +804,3 @@\n-  void work(uint worker_id) {\n+private:\n+  template <bool ALT_FWD>\n+  void work_impl(uint worker_id) {\n@@ -780,1 +808,1 @@\n-    ShenandoahAdjustPointersObjectClosure obj_cl;\n+    ShenandoahAdjustPointersObjectClosure<ALT_FWD> obj_cl;\n@@ -789,0 +817,9 @@\n+\n+public:\n+  void work(uint worker_id) {\n+    if (UseAltGCForwarding) {\n+      work_impl<true>(worker_id);\n+    } else {\n+      work_impl<false>(worker_id);\n+    }\n+  }\n@@ -795,0 +832,1 @@\n+\n@@ -801,1 +839,3 @@\n-  void work(uint worker_id) {\n+private:\n+  template <bool ALT_FWD>\n+  void work_impl(uint worker_id) {\n@@ -803,1 +843,1 @@\n-    ShenandoahAdjustPointersClosure cl;\n+    ShenandoahAdjustPointersClosure<ALT_FWD> cl;\n@@ -807,0 +847,9 @@\n+\n+public:\n+  void work(uint worker_id) {\n+    if (UseAltGCForwarding) {\n+      work_impl<true>(worker_id);\n+    } else {\n+      work_impl<false>(worker_id);\n+    }\n+  }\n@@ -833,0 +882,1 @@\n+template <bool ALT_FWD>\n@@ -845,1 +895,1 @@\n-    if (p->is_forwarded()) {\n+    if (SlidingForwarding::is_forwarded(p)) {\n@@ -847,1 +897,1 @@\n-      HeapWord* compact_to = cast_from_oop<HeapWord*>(p->forwardee());\n+      HeapWord* compact_to = cast_from_oop<HeapWord*>(SlidingForwarding::forwardee<ALT_FWD>(p));\n@@ -867,1 +917,3 @@\n-  void work(uint worker_id) {\n+private:\n+  template <bool ALT_FWD>\n+  void work_impl(uint worker_id) {\n@@ -871,1 +923,1 @@\n-    ShenandoahCompactObjectsClosure cl(worker_id);\n+    ShenandoahCompactObjectsClosure<ALT_FWD> cl(worker_id);\n@@ -882,0 +934,9 @@\n+\n+public:\n+  void work(uint worker_id) {\n+    if (UseAltGCForwarding) {\n+      work_impl<true>(worker_id);\n+    } else {\n+      work_impl<false>(worker_id);\n+    }\n+  }\n@@ -937,1 +998,2 @@\n-void ShenandoahFullGC::compact_humongous_objects() {\n+template <bool ALT_FWD>\n+void ShenandoahFullGC::compact_humongous_objects_impl() {\n@@ -950,1 +1012,1 @@\n-      if (!old_obj->is_forwarded()) {\n+      if (SlidingForwarding::is_not_forwarded(old_obj)) {\n@@ -959,1 +1021,1 @@\n-      size_t new_start = heap->heap_region_index_containing(old_obj->forwardee());\n+      size_t new_start = heap->heap_region_index_containing(SlidingForwarding::forwardee<ALT_FWD>(old_obj));\n@@ -1001,0 +1063,8 @@\n+void ShenandoahFullGC::compact_humongous_objects() {\n+  if (UseAltGCForwarding) {\n+    compact_humongous_objects_impl<true>();\n+  } else {\n+    compact_humongous_objects_impl<false>();\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":88,"deletions":18,"binary":false,"changes":106,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -404,0 +405,2 @@\n+  SlidingForwarding::initialize(_heap_region, ShenandoahHeapRegion::region_size_words());\n+\n@@ -956,1 +959,1 @@\n-    if (!p->is_forwarded()) {\n+    if (!ShenandoahForwarding::is_forwarded(p)) {\n@@ -1304,0 +1307,1 @@\n+    shenandoah_assert_not_in_cset_except(NULL, obj, cancelled_gc());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -100,1 +100,1 @@\n-      if (is_instance_ref_klass(obj->klass())) {\n+      if (is_instance_ref_klass(obj->forward_safe_klass())) {\n@@ -127,1 +127,1 @@\n-    Klass* obj_klass = obj->klass_or_null();\n+    Klass* obj_klass = obj->forward_safe_klass();\n@@ -142,1 +142,1 @@\n-        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + obj->size()) <= obj_reg->top(),\n+        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + obj->forward_safe_size()) <= obj_reg->top(),\n@@ -146,1 +146,1 @@\n-        size_t humongous_end = humongous_start + (obj->size() >> ShenandoahHeapRegion::region_size_words_shift());\n+        size_t humongous_end = humongous_start + (obj->forward_safe_size() >> ShenandoahHeapRegion::region_size_words_shift());\n@@ -163,1 +163,1 @@\n-          Atomic::add(&_ld[obj_reg->index()], (uint) obj->size(), memory_order_relaxed);\n+          Atomic::add(&_ld[obj_reg->index()], (uint) obj->forward_safe_size(), memory_order_relaxed);\n@@ -204,1 +204,1 @@\n-      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + fwd->size()) <= fwd_reg->top(),\n+      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + fwd->forward_safe_size()) <= fwd_reg->top(),\n@@ -307,1 +307,2 @@\n-    obj->oop_iterate(this);\n+    Klass* klass = obj->forward_safe_klass();\n+    obj->oop_iterate_backwards(this, klass);\n@@ -587,1 +588,1 @@\n-    if (!is_instance_ref_klass(obj->klass())) {\n+    if (!is_instance_ref_klass(obj->forward_safe_klass())) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2591,1 +2591,7 @@\n-    set_prototype_header(markWord::prototype());\n+    markWord prototype = markWord::prototype();\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      prototype = prototype.set_klass(this);\n+    }\n+#endif\n+    set_prototype_header(prototype);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1650,0 +1650,4 @@\n+      if (UseCompactObjectHeaders) {\n+        if (flat->offset() == in_bytes(Klass::prototype_header_offset()))\n+          alias_type(idx)->set_rewritable(false);\n+      }\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4564,1 +4564,2 @@\n-    int header_size = objArrayOopDesc::header_size() * wordSize;\n+    BasicType basic_elem_type = elem()->basic_type();\n+    int header_size = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -973,10 +973,4 @@\n-  {\n-    \/\/ Revoke any biases before querying the mark word\n-    BiasedLocking::revoke_at_safepoint(hobj);\n-\n-    address owner = NULL;\n-    {\n-      markWord mark = hobj()->mark();\n-\n-      if (!mark.has_monitor()) {\n-        \/\/ this object has a lightweight monitor\n+  owning_thread = ObjectSynchronizer::get_lock_owner(tlh.list(), hobj);\n+  if (owning_thread != NULL) {  \/\/ monitor is owned\n+    Handle th(current_thread, owning_thread->threadObj());\n+    ret.owner = (jthread)jni_reference(calling_thread, th);\n@@ -984,34 +978,5 @@\n-        if (mark.has_locker()) {\n-          owner = (address)mark.locker(); \/\/ save the address of the Lock word\n-        }\n-        \/\/ implied else: no owner\n-      } else {\n-        \/\/ this object has a heavyweight monitor\n-        mon = mark.monitor();\n-\n-        \/\/ The owner field of a heavyweight monitor may be NULL for no\n-        \/\/ owner, a JavaThread * or it may still be the address of the\n-        \/\/ Lock word in a JavaThread's stack. A monitor can be inflated\n-        \/\/ by a non-owning JavaThread, but only the owning JavaThread\n-        \/\/ can change the owner field from the Lock word to the\n-        \/\/ JavaThread * and it may not have done that yet.\n-        owner = (address)mon->owner();\n-      }\n-    }\n-\n-    if (owner != NULL) {\n-      \/\/ This monitor is owned so we have to find the owning JavaThread.\n-      owning_thread = Threads::owning_thread_from_monitor_owner(tlh.list(), owner);\n-      assert(owning_thread != NULL, \"owning JavaThread must not be NULL\");\n-      Handle     th(current_thread, owning_thread->threadObj());\n-      ret.owner = (jthread)jni_reference(calling_thread, th);\n-    }\n-\n-    if (owning_thread != NULL) {  \/\/ monitor is owned\n-      \/\/ The recursions field of a monitor does not reflect recursions\n-      \/\/ as lightweight locks before inflating the monitor are not included.\n-      \/\/ We have to count the number of recursive monitor entries the hard way.\n-      \/\/ We pass a handle to survive any GCs along the way.\n-      ret.entry_count = count_locked_objects(owning_thread, hobj);\n-    }\n-    \/\/ implied else: entry_count == 0\n+    \/\/ The recursions field of a monitor does not reflect recursions\n+    \/\/ as lightweight locks before inflating the monitor are not included.\n+    \/\/ We have to count the number of recursive monitor entries the hard way.\n+    \/\/ We pass a handle to survive any GCs along the way.\n+    ret.entry_count = count_locked_objects(owning_thread, hobj);\n@@ -1019,0 +984,1 @@\n+  \/\/ implied else: entry_count == 0\n@@ -1021,1 +987,4 @@\n-  if (mon != NULL) {\n+  markWord mark = hobj->mark();\n+  if (mark.has_monitor()) {\n+    mon = mark.monitor();\n+    assert(mon != NULL, \"must have monitor\");\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnvBase.cpp","additions":14,"deletions":45,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -69,1 +69,1 @@\n-  ( arrayOopDesc::header_size(T_DOUBLE) * HeapWordSize \\\n+  ( arrayOopDesc::base_offset_in_bytes(T_DOUBLE) \\\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -133,0 +133,3 @@\n+  product(bool, UseCompactObjectHeaders, false, EXPERIMENTAL,               \\\n+          \"Use 64-bit object headers instead of 96-bit headers\")            \\\n+                                                                            \\\n@@ -150,0 +153,1 @@\n+const bool UseCompactObjectHeaders = false;\n@@ -2097,0 +2101,13 @@\n+  product(bool, HeapObjectStats, false, DIAGNOSTIC,                         \\\n+             \"Enable gathering of heap object statistics\")                  \\\n+                                                                            \\\n+  product(size_t, HeapObjectStatsSamplingInterval, 500, DIAGNOSTIC,         \\\n+             \"Heap object statistics sampling interval (ms)\")               \\\n+                                                                            \\\n+  product(int, LockingMode, LM_LEGACY, EXPERIMENTAL,                        \\\n+          \"Select locking mode: \"                                           \\\n+          \"0: monitors only (LM_MONITOR), \"                                 \\\n+          \"1: monitors & legacy stack-locking (LM_LEGACY, default), \"       \\\n+          \"2: monitors & new lightweight locking (LM_LIGHTWEIGHT)\")         \\\n+          range(0, 2)                                                       \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -41,0 +42,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -278,4 +280,12 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-    \/\/ Degenerate notify\n-    \/\/ stack-locked by caller so by definition the implied waitset is empty.\n-    return true;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if (mark.is_fast_locked() && current->lock_stack().contains(cast_to_oop(obj))) {\n+      \/\/ Degenerate notify\n+      \/\/ fast-locked by caller so by definition the implied waitset is empty.\n+      return true;\n+    }\n+  } else if (LockingMode == LM_LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Degenerate notify\n+      \/\/ stack-locked by caller so by definition the implied waitset is empty.\n+      return true;\n+    }\n@@ -351,11 +361,13 @@\n-    \/\/ This Java Monitor is inflated so obj's header will never be\n-    \/\/ displaced to this thread's BasicLock. Make the displaced header\n-    \/\/ non-NULL so this BasicLock is not seen as recursive nor as\n-    \/\/ being locked. We do this unconditionally so that this thread's\n-    \/\/ BasicLock cannot be mis-interpreted by any stack walkers. For\n-    \/\/ performance reasons, stack walkers generally first check for\n-    \/\/ Biased Locking in the object's header, the second check is for\n-    \/\/ stack-locking in the object's header, the third check is for\n-    \/\/ recursive stack-locking in the displaced header in the BasicLock,\n-    \/\/ and last are the inflated Java Monitor (ObjectMonitor) checks.\n-    lock->set_displaced_header(markWord::unused_mark());\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ This Java Monitor is inflated so obj's header will never be\n+      \/\/ displaced to this thread's BasicLock. Make the displaced header\n+      \/\/ non-NULL so this BasicLock is not seen as recursive nor as\n+      \/\/ being locked. We do this unconditionally so that this thread's\n+      \/\/ BasicLock cannot be mis-interpreted by any stack walkers. For\n+      \/\/ performance reasons, stack walkers generally first check for\n+      \/\/ Biased Locking in the object's header, the second check is for\n+      \/\/ stack-locking in the object's header, the third check is for\n+      \/\/ recursive stack-locking in the displaced header in the BasicLock,\n+      \/\/ and last are the inflated Java Monitor (ObjectMonitor) checks.\n+      lock->set_displaced_header(markWord::unused_mark());\n+    }\n@@ -428,0 +440,8 @@\n+static bool useHeavyMonitors() {\n+#if defined(X86) || defined(AARCH64) || defined(PPC64) || defined(RISCV64)\n+  return LockingMode == LM_MONITOR;\n+#else\n+  return false;\n+#endif\n+}\n+\n@@ -439,3 +459,23 @@\n-  if (UseBiasedLocking) {\n-    BiasedLocking::revoke(current, obj);\n-  }\n+  if (!useHeavyMonitors()) {\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ Fast-locking does not use the 'lock' argument.\n+      LockStack& lock_stack = current->lock_stack();\n+      if (lock_stack.can_push()) {\n+        markWord mark = obj()->mark_acquire();\n+        if (mark.is_neutral()) {\n+          assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n+          \/\/ Try to swing into 'fast-locked' state.\n+          markWord locked_mark = mark.set_fast_locked();\n+          markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n+          if (old_mark == mark) {\n+            \/\/ Successfully fast-locked, push object to lock-stack and return.\n+            lock_stack.push(obj());\n+            return;\n+          }\n+        }\n+      }\n+      \/\/ All other paths fall-through to inflate-enter.\n+    } else if (LockingMode == LM_LEGACY) {\n+      if (UseBiasedLocking) {\n+        BiasedLocking::revoke(current, obj);\n+      }\n@@ -443,2 +483,16 @@\n-  markWord mark = obj->mark();\n-  assert(!mark.has_bias_pattern(), \"should not see bias pattern here\");\n+      markWord mark = obj->mark();\n+      if (mark.is_neutral()) {\n+        \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n+        \/\/ be visible <= the ST performed by the CAS.\n+        lock->set_displaced_header(mark);\n+        if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n+          return;\n+        }\n+        \/\/ Fall through to inflate() ...\n+      } else if (mark.has_locker() &&\n+                 current->is_lock_owned((address) mark.locker())) {\n+        assert(lock != mark.locker(), \"must not re-lock the same lock\");\n+        assert(lock != (BasicLock*) obj->mark().value(), \"don't relock with same BasicLock\");\n+        lock->set_displaced_header(markWord::from_pointer(NULL));\n+        return;\n+      }\n@@ -446,6 +500,5 @@\n-  if (mark.is_neutral()) {\n-    \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n-    \/\/ be visible <= the ST performed by the CAS.\n-    lock->set_displaced_header(mark);\n-    if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n-      return;\n+      \/\/ The object header will never be displaced to this lock,\n+      \/\/ so it does not matter what the value is, except that it\n+      \/\/ must be non-zero to avoid looking like a re-entrant lock,\n+      \/\/ and must not look locked either.\n+      lock->set_displaced_header(markWord::unused_mark());\n@@ -453,7 +506,0 @@\n-    \/\/ Fall through to inflate() ...\n-  } else if (mark.has_locker() &&\n-             current->is_lock_owned((address)mark.locker())) {\n-    assert(lock != mark.locker(), \"must not re-lock the same lock\");\n-    assert(lock != (BasicLock*)obj->mark().value(), \"don't relock with same BasicLock\");\n-    lock->set_displaced_header(markWord::from_pointer(NULL));\n-    return;\n@@ -462,5 +508,0 @@\n-  \/\/ The object header will never be displaced to this lock,\n-  \/\/ so it does not matter what the value is, except that it\n-  \/\/ must be non-zero to avoid looking like a re-entrant lock,\n-  \/\/ and must not look locked either.\n-  lock->set_displaced_header(markWord::unused_mark());\n@@ -479,29 +520,21 @@\n-  markWord mark = object->mark();\n-  \/\/ We cannot check for Biased Locking if we are racing an inflation.\n-  assert(mark == markWord::INFLATING() ||\n-         !mark.has_bias_pattern(), \"should not see bias pattern here\");\n-\n-  markWord dhw = lock->displaced_header();\n-  if (dhw.value() == 0) {\n-    \/\/ If the displaced header is NULL, then this exit matches up with\n-    \/\/ a recursive enter. No real work to do here except for diagnostics.\n-#ifndef PRODUCT\n-    if (mark != markWord::INFLATING()) {\n-      \/\/ Only do diagnostics if we are not racing an inflation. Simply\n-      \/\/ exiting a recursive enter of a Java Monitor that is being\n-      \/\/ inflated is safe; see the has_monitor() comment below.\n-      assert(!mark.is_neutral(), \"invariant\");\n-      assert(!mark.has_locker() ||\n-             current->is_lock_owned((address)mark.locker()), \"invariant\");\n-      if (mark.has_monitor()) {\n-        \/\/ The BasicLock's displaced_header is marked as a recursive\n-        \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n-        \/\/ This is a special case where the Java Monitor was inflated\n-        \/\/ after this thread entered the stack-lock recursively. When a\n-        \/\/ Java Monitor is inflated, we cannot safely walk the Java\n-        \/\/ Monitor owner's stack and update the BasicLocks because a\n-        \/\/ Java Monitor can be asynchronously inflated by a thread that\n-        \/\/ does not own the Java Monitor.\n-        ObjectMonitor* m = mark.monitor();\n-        assert(m->object()->mark() == mark, \"invariant\");\n-        assert(m->is_entered(current), \"invariant\");\n+  if (!useHeavyMonitors()) {\n+    markWord mark = object->mark();\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ Fast-locking does not use the 'lock' argument.\n+      if (mark.is_fast_locked()) {\n+        markWord unlocked_mark = mark.set_unlocked();\n+        markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n+        if (old_mark != mark) {\n+          \/\/ Another thread won the CAS, it must have inflated the monitor.\n+          \/\/ It can only have installed an anonymously locked monitor at this point.\n+          \/\/ Fetch that monitor, set owner correctly to this thread, and\n+          \/\/ exit it (allowing waiting threads to enter).\n+          assert(old_mark.has_monitor(), \"must have monitor\");\n+          ObjectMonitor* monitor = old_mark.monitor();\n+          assert(monitor->is_owner_anonymous(), \"must be anonymous owner\");\n+          monitor->set_owner_from_anonymous(current);\n+          monitor->exit(current);\n+        }\n+        LockStack& lock_stack = current->lock_stack();\n+        lock_stack.remove(object);\n+        return;\n@@ -509,1 +542,27 @@\n-    }\n+    } else if (LockingMode == LM_LEGACY) {\n+      markWord dhw = lock->displaced_header();\n+      if (dhw.value() == 0) {\n+        \/\/ If the displaced header is NULL, then this exit matches up with\n+        \/\/ a recursive enter. No real work to do here except for diagnostics.\n+#ifndef PRODUCT\n+        if (mark != markWord::INFLATING()) {\n+          \/\/ Only do diagnostics if we are not racing an inflation. Simply\n+          \/\/ exiting a recursive enter of a Java Monitor that is being\n+          \/\/ inflated is safe; see the has_monitor() comment below.\n+          assert(!mark.is_neutral(), \"invariant\");\n+          assert(!mark.has_locker() ||\n+                 current->is_lock_owned((address)mark.locker()), \"invariant\");\n+          if (mark.has_monitor()) {\n+            \/\/ The BasicLock's displaced_header is marked as a recursive\n+            \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n+            \/\/ This is a special case where the Java Monitor was inflated\n+            \/\/ after this thread entered the stack-lock recursively. When a\n+            \/\/ Java Monitor is inflated, we cannot safely walk the Java\n+            \/\/ Monitor owner's stack and update the BasicLocks because a\n+            \/\/ Java Monitor can be asynchronously inflated by a thread that\n+            \/\/ does not own the Java Monitor.\n+            ObjectMonitor* m = mark.monitor();\n+            assert(m->object()->mark() == mark, \"invariant\");\n+            assert(m->is_entered(current), \"invariant\");\n+          }\n+        }\n@@ -511,2 +570,2 @@\n-    return;\n-  }\n+        return;\n+      }\n@@ -514,6 +573,8 @@\n-  if (mark == markWord::from_pointer(lock)) {\n-    \/\/ If the object is stack-locked by the current thread, try to\n-    \/\/ swing the displaced header from the BasicLock back to the mark.\n-    assert(dhw.is_neutral(), \"invariant\");\n-    if (object->cas_set_mark(dhw, mark) == mark) {\n-      return;\n+      if (mark == markWord::from_pointer(lock)) {\n+        \/\/ If the object is stack-locked by the current thread, try to\n+        \/\/ swing the displaced header from the BasicLock back to the mark.\n+        assert(dhw.is_neutral(), \"invariant\");\n+        if (object->cas_set_mark(dhw, mark) == mark) {\n+          return;\n+        }\n+      }\n@@ -527,0 +588,7 @@\n+  if (LockingMode == LM_LIGHTWEIGHT && monitor->is_owner_anonymous()) {\n+    \/\/ It must be us. Pop lock object from lock stack.\n+    LockStack& lock_stack = current->lock_stack();\n+    oop popped = lock_stack.pop();\n+    assert(popped == object, \"must be owned by this thread\");\n+    monitor->set_owner_from_anonymous(current);\n+  }\n@@ -691,3 +759,10 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-    \/\/ Not inflated so there can't be any waiters to notify.\n-    return;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if ((mark.is_fast_locked() && current->lock_stack().contains(obj()))) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n+  } else if (LockingMode == LM_LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n@@ -710,3 +785,10 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-    \/\/ Not inflated so there can't be any waiters to notify.\n-    return;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if ((mark.is_fast_locked() && current->lock_stack().contains(obj()))) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n+  } else if (LockingMode == LM_LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n@@ -738,1 +820,2 @@\n-  if (!mark.is_being_inflated()) {\n+  if (!mark.is_being_inflated() || LockingMode == LM_LIGHTWEIGHT) {\n+    \/\/ New lightweight locking does not use the markWord::INFLATING() protocol.\n@@ -847,1 +930,1 @@\n-  value &= markWord::hash_mask;\n+  value &= UseCompactObjectHeaders ? markWord::hash_mask_compact : markWord::hash_mask;\n@@ -853,0 +936,7 @@\n+\/\/ Can be called from non JavaThreads (e.g., VMThread) for FastHashCode\n+\/\/ calculations as part of JVM\/TI tagging.\n+static bool is_lock_owned(Thread* thread, oop obj) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only call this with new lightweight locking enabled\");\n+  return thread->is_Java_thread() ? reinterpret_cast<JavaThread*>(thread)->lock_stack().contains(obj) : false;\n+}\n+\n@@ -926,1 +1016,8 @@\n-    } else if (current->is_lock_owned((address)mark.locker())) {\n+    } else if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked() && is_lock_owned(current, obj)) {\n+      \/\/ This is a fast lock owned by the calling thread so use the\n+      \/\/ markWord from the object.\n+      hash = mark.hash();\n+      if (hash != 0) {                  \/\/ if it has a hash, just return it\n+        return hash;\n+      }\n+    } else if (LockingMode == LM_LEGACY && mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n@@ -1003,2 +1100,2 @@\n-  \/\/ Uncontended case, header points to stack\n-  if (mark.has_locker()) {\n+  if (LockingMode == LM_LEGACY && mark.has_locker()) {\n+    \/\/ stack-locked case, header points into owner's stack\n@@ -1007,0 +1104,6 @@\n+\n+  if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+    \/\/ fast-locking case, see if lock is in current's lock stack\n+    return current->lock_stack().contains(h_obj());\n+  }\n+\n@@ -1031,2 +1134,0 @@\n-  address owner = NULL;\n-\n@@ -1035,3 +1136,10 @@\n-  \/\/ Uncontended case, header points to stack\n-  if (mark.has_locker()) {\n-    owner = (address) mark.locker();\n+  if (LockingMode == LM_LEGACY && mark.has_locker()) {\n+    \/\/ stack-locked so header points into owner's stack.\n+    \/\/ owning_thread_from_monitor_owner() may also return null here:\n+    return Threads::owning_thread_from_monitor_owner(t_list, (address) mark.locker());\n+  }\n+\n+  if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+    \/\/ fast-locked so get owner from the object.\n+    \/\/ owning_thread_from_object() may also return null here:\n+    return Threads::owning_thread_from_object(t_list, h_obj());\n@@ -1041,1 +1149,1 @@\n-  else if (mark.has_monitor()) {\n+  if (mark.has_monitor()) {\n@@ -1046,6 +1154,2 @@\n-    owner = (address) monitor->owner();\n-  }\n-\n-  if (owner != NULL) {\n-    \/\/ owning_thread_from_monitor_owner() may also return NULL here\n-    return Threads::owning_thread_from_monitor_owner(t_list, owner);\n+    \/\/ owning_thread_from_monitor() may also return null here:\n+    return Threads::owning_thread_from_monitor(t_list, monitor);\n@@ -1254,2 +1358,8 @@\n-    \/\/ *  Inflated     - just return\n-    \/\/ *  Stack-locked - coerce it to inflated\n+    \/\/ *  inflated     - Just return if using stack-locking.\n+    \/\/                   If using fast-locking and the ObjectMonitor owner\n+    \/\/                   is anonymous and the current thread owns the\n+    \/\/                   object lock, then we make the current thread the\n+    \/\/                   ObjectMonitor owner and remove the lock from the\n+    \/\/                   current thread's lock stack.\n+    \/\/ *  fast-locked  - Coerce it to inflated from fast-locked.\n+    \/\/ *  stack-locked - Coerce it to inflated from stack-locked.\n@@ -1265,0 +1375,5 @@\n+      if (LockingMode == LM_LIGHTWEIGHT && inf->is_owner_anonymous() && is_lock_owned(current, object)) {\n+        inf->set_owner_from_anonymous(current);\n+        assert(current->is_Java_thread(), \"must be Java thread\");\n+        reinterpret_cast<JavaThread*>(current)->lock_stack().remove(object);\n+      }\n@@ -1274,3 +1389,65 @@\n-    if (mark == markWord::INFLATING()) {\n-      read_stable_mark(object);\n-      continue;\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ New lightweight locking does not use INFLATING.\n+      \/\/ CASE: inflation in progress - inflating over a stack-lock.\n+      \/\/ Some other thread is converting from stack-locked to inflated.\n+      \/\/ Only that thread can complete inflation -- other threads must wait.\n+      \/\/ The INFLATING value is transient.\n+      \/\/ Currently, we spin\/yield\/park and poll the markword, waiting for inflation to finish.\n+      \/\/ We could always eliminate polling by parking the thread on some auxiliary list.\n+      if (mark == markWord::INFLATING()) {\n+        read_stable_mark(object);\n+        continue;\n+      }\n+    }\n+\n+    \/\/ CASE: fast-locked\n+    \/\/ Could be fast-locked either by current or by some other thread.\n+    \/\/\n+    \/\/ Note that we allocate the ObjectMonitor speculatively, _before_\n+    \/\/ attempting to set the object's mark to the new ObjectMonitor. If\n+    \/\/ this thread owns the monitor, then we set the ObjectMonitor's\n+    \/\/ owner to this thread. Otherwise, we set the ObjectMonitor's owner\n+    \/\/ to anonymous. If we lose the race to set the object's mark to the\n+    \/\/ new ObjectMonitor, then we just delete it and loop around again.\n+    \/\/\n+    LogStreamHandle(Trace, monitorinflation) lsh;\n+    if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+      ObjectMonitor* monitor = new ObjectMonitor(object);\n+      monitor->set_header(mark.set_unlocked());\n+      bool own = is_lock_owned(current, object);\n+      if (own) {\n+        \/\/ Owned by us.\n+        monitor->set_owner_from(NULL, current);\n+      } else {\n+        \/\/ Owned by somebody else.\n+        monitor->set_owner_anonymous();\n+      }\n+      markWord monitor_mark = markWord::encode(monitor);\n+      markWord old_mark = object->cas_set_mark(monitor_mark, mark);\n+      if (old_mark == mark) {\n+        \/\/ Success! Return inflated monitor.\n+        if (own) {\n+          assert(current->is_Java_thread(), \"must be Java thread\");\n+          reinterpret_cast<JavaThread*>(current)->lock_stack().remove(object);\n+        }\n+        \/\/ Once the ObjectMonitor is configured and object is associated\n+        \/\/ with the ObjectMonitor, it is safe to allow async deflation:\n+        _in_use_list.add(monitor);\n+\n+        \/\/ Hopefully the performance counters are allocated on distinct\n+        \/\/ cache lines to avoid false sharing on MP systems ...\n+        OM_PERFDATA_OP(Inflations, inc());\n+        if (log_is_enabled(Trace, monitorinflation)) {\n+          ResourceMark rm(current);\n+          lsh.print_cr(\"inflate(has_locker): object=\" INTPTR_FORMAT \", mark=\"\n+                       INTPTR_FORMAT \", type='%s'\", p2i(object),\n+                       object->mark().value(), object->klass()->external_name());\n+        }\n+        if (event.should_commit()) {\n+          post_monitor_inflate_event(&event, object, cause);\n+        }\n+        return monitor;\n+      } else {\n+        delete monitor;\n+        continue;  \/\/ Interference -- just retry\n+      }\n@@ -1289,3 +1466,2 @@\n-    LogStreamHandle(Trace, monitorinflation) lsh;\n-\n-    if (mark.has_locker()) {\n+    if (LockingMode == LM_LEGACY && mark.has_locker()) {\n+      assert(LockingMode != LM_LIGHTWEIGHT, \"cannot happen with new lightweight locking\");\n@@ -1484,0 +1660,10 @@\n+class VM_RendezvousGCThreads : public VM_Operation {\n+public:\n+  bool evaluate_at_safepoint() const override { return false; }\n+  VMOp_Type type() const override { return VMOp_RendezvousGCThreads; }\n+  void doit() override {\n+    SuspendibleThreadSet::synchronize();\n+    SuspendibleThreadSet::desynchronize();\n+  };\n+};\n+\n@@ -1536,0 +1722,3 @@\n+      \/\/ Also, we sync and desync GC threads around the handshake, so that they can\n+      \/\/ safely read the mark-word and look-through to the object-monitor, without\n+      \/\/ being afraid that the object-monitor is going away.\n@@ -1538,0 +1727,2 @@\n+      VM_RendezvousGCThreads sync_gc;\n+      VMThread::execute(&sync_gc);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":298,"deletions":107,"binary":false,"changes":405,"status":"modified"},{"patch":"@@ -438,4 +438,2 @@\n-        address currentOwner = (address)waitingToLockMonitor->owner();\n-        if (currentOwner != NULL) {\n-          currentThread = Threads::owning_thread_from_monitor_owner(t_list,\n-                                                                    currentOwner);\n+        if (waitingToLockMonitor->has_owner()) {\n+          currentThread = Threads::owning_thread_from_monitor(t_list, waitingToLockMonitor);\n@@ -1017,2 +1015,1 @@\n-      currentThread = Threads::owning_thread_from_monitor_owner(t_list,\n-                                                                (address)waitingToLockMonitor->owner());\n+      currentThread = Threads::owning_thread_from_monitor(t_list, waitingToLockMonitor);\n","filename":"src\/hotspot\/share\/services\/threadService.cpp","additions":3,"deletions":6,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -579,1 +579,5 @@\n-                \"CreateCoredumpOnCrash\"\n+                \"CreateCoredumpOnCrash\",\n+                \/\/ experimental features unlocking flag does not affect behavior\n+                \"UnlockExperimentalVMOptions\",\n+                \/\/ all compact headers settings should run flagless tests\n+                \"UseCompactObjectHeaders\"\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"}]}