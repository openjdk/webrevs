{"files":[{"patch":"@@ -1915,0 +1915,12 @@\n+  int max_monitors = C->method() != NULL ? C->max_monitors() : 0;\n+  if (UseFastLocking && max_monitors > 0) {\n+    C2CheckLockStackStub* stub = new (C->comp_arena()) C2CheckLockStackStub();\n+    C->output()->add_stub(stub);\n+    __ ldr(r9, Address(rthread, JavaThread::lock_stack_current_offset()));\n+    __ add(r9, r9, max_monitors * oopSize);\n+    __ ldr(r10, Address(rthread, JavaThread::lock_stack_limit_offset()));\n+    __ cmp(r9, r10);\n+    __ br(Assembler::GE, stub->entry());\n+    __ bind(stub->continuation());\n+  }\n+\n@@ -1989,1 +2001,3 @@\n-      code_stub = &C->output()->safepoint_poll_table()->add_safepoint(__ offset());\n+      C2SafepointPollStub* stub = new (C->comp_arena()) C2SafepointPollStub(__ offset());\n+      C->output()->add_stub(stub);\n+      code_stub = &stub->entry();\n@@ -3831,31 +3845,36 @@\n-    \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n-    __ orr(tmp, disp_hdr, markWord::unlocked_value);\n-\n-    \/\/ Initialize the box. (Must happen before we update the object mark!)\n-    __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-    \/\/ Compare object markWord with an unlocked value (tmp) and if\n-    \/\/ equal exchange the stack address of our box with object markWord.\n-    \/\/ On failure disp_hdr contains the possibly locked markWord.\n-    __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n-               \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n-    __ br(Assembler::EQ, cont);\n-\n-    assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-    \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n-    \/\/ object, will have now locked it will continue at label cont\n-\n-    __ bind(cas_failed);\n-    \/\/ We did not see an unlocked object so try the fast recursive case.\n-\n-    \/\/ Check if the owner is self by comparing the value in the\n-    \/\/ markWord of object (disp_hdr) with the stack pointer.\n-    __ mov(rscratch1, sp);\n-    __ sub(disp_hdr, disp_hdr, rscratch1);\n-    __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n-    \/\/ If condition is true we are cont and hence we can store 0 as the\n-    \/\/ displaced header in the box, which indicates that it is a recursive lock.\n-    __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n-    __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n+    if (UseFastLocking) {\n+      __ fast_lock(oop, disp_hdr, tmp, rscratch1, cont, false);\n+      \/\/ Indicate success at cont.\n+      __ cmp(oop, oop);\n+    } else {\n+      \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n+      __ orr(tmp, disp_hdr, markWord::unlocked_value);\n+\n+      \/\/ Initialize the box. (Must happen before we update the object mark!)\n+      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n+      \/\/ Compare object markWord with an unlocked value (tmp) and if\n+      \/\/ equal exchange the stack address of our box with object markWord.\n+      \/\/ On failure disp_hdr contains the possibly locked markWord.\n+      __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n+                 \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n+      __ br(Assembler::EQ, cont);\n+\n+      assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+\n+      \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n+      \/\/ object, will have now locked it will continue at label cont\n+\n+      __ bind(cas_failed);\n+      \/\/ We did not see an unlocked object so try the fast recursive case.\n+\n+      \/\/ Check if the owner is self by comparing the value in the\n+      \/\/ markWord of object (disp_hdr) with the stack pointer.\n+      __ mov(rscratch1, sp);\n+      __ sub(disp_hdr, disp_hdr, rscratch1);\n+      __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n+      \/\/ If condition is true we are cont and hence we can store 0 as the\n+      \/\/ displaced header in the box, which indicates that it is a recursive lock.\n+      __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n+      __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    }\n@@ -3875,7 +3894,8 @@\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n-    __ mov(tmp, (address)markWord::unused_mark().value());\n-    __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n+    if (!UseFastLocking) {\n+      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+      \/\/ lock. The fast-path monitor unlock code checks for\n+      \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+      \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n+      __ mov(tmp, (address)markWord::unused_mark().value());\n+      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    }\n@@ -3911,2 +3931,3 @@\n-    \/\/ Find the lock address and load the displaced header from the stack.\n-    __ ldr(disp_hdr, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    if (!UseFastLocking) {\n+      \/\/ Find the lock address and load the displaced header from the stack.\n+      __ ldr(disp_hdr, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n@@ -3914,3 +3935,4 @@\n-    \/\/ If the displaced header is 0, we have a recursive unlock.\n-    __ cmp(disp_hdr, zr);\n-    __ br(Assembler::EQ, cont);\n+      \/\/ If the displaced header is 0, we have a recursive unlock.\n+      __ cmp(disp_hdr, zr);\n+      __ br(Assembler::EQ, cont);\n+    }\n@@ -3922,3 +3944,0 @@\n-    \/\/ Check if it is still a light weight lock, this is is true if we\n-    \/\/ see the stack address of the basicLock in the markWord of the\n-    \/\/ object.\n@@ -3926,3 +3945,8 @@\n-    __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n-               \/*release*\/ true, \/*weak*\/ false, tmp);\n-    __ b(cont);\n+    if (UseFastLocking) {\n+      __ fast_unlock(oop, tmp, box, disp_hdr, cont);\n+      \/\/ Indicate success at cont.\n+      __ cmp(oop, oop);\n+    } else {\n+      \/\/ Check if it is still a light weight lock, this is is true if we\n+      \/\/ see the stack address of the basicLock in the markWord of the\n+      \/\/ object.\n@@ -3930,0 +3954,4 @@\n+      __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n+                 \/*release*\/ true, \/*weak*\/ false, tmp);\n+    }\n+    __ b(cont);\n@@ -3936,0 +3964,11 @@\n+\n+    if (UseFastLocking) {\n+      \/\/ If the owner is anonymous, we need to fix it -- in the slow-path.\n+      __ ldr(disp_hdr, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+      \/\/ We cannot use tbnz here: tbnz would leave the condition flags untouched,\n+      \/\/ but we want to carry-over the NE condition to the exit at the cont label,\n+      \/\/ in order to take the slow-path.\n+      __ tst(disp_hdr, (uint64_t)(intptr_t) ANONYMOUS_OWNER);\n+      __ br(Assembler::NE, cont);\n+    }\n+\n@@ -7439,1 +7478,1 @@\n-instruct loadNKlass(iRegNNoSp dst, memory4 mem)\n+instruct loadNKlass(iRegNNoSp dst, memory4 mem, rFlagsReg cr)\n@@ -7442,0 +7481,1 @@\n+  effect(TEMP_DEF dst, KILL cr);\n@@ -7446,4 +7486,16 @@\n-\n-  ins_encode(aarch64_enc_ldrw(dst, mem));\n-\n-  ins_pipe(iload_reg_mem);\n+  ins_encode %{\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset\");\n+    assert($mem$$index$$Register == noreg, \"expect no index\");\n+    Register dst = $dst$$Register;\n+    Register obj = $mem$$base$$Register;\n+    C2LoadNKlassStub* stub = new (Compile::current()->comp_arena()) C2LoadNKlassStub(dst);\n+    Compile::current()->output()->add_stub(stub);\n+    __ ldr(dst, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    \/\/ NOTE: We can't use tbnz here, because the target is sometimes too far away\n+    \/\/ and cannot be encoded.\n+    __ tst(dst, markWord::monitor_value);\n+    __ br(Assembler::NE, stub->entry());\n+    __ bind(stub->continuation());\n+    __ lsr(dst, dst, markWord::klass_shift);\n+  %}\n+  ins_pipe(pipe_slow);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":107,"deletions":55,"binary":false,"changes":162,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -256,0 +257,6 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  __ bind(_entry);\n+  Register d = _result->as_register();\n+  __ ldr(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ b(_continuation);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -245,1 +245,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n@@ -987,8 +987,1 @@\n-      \/\/ FIXME: OMG this is a horrible kludge.  Any offset from an\n-      \/\/ address that matches klass_offset_in_bytes() will be loaded\n-      \/\/ as a word, not a long.\n-      if (UseCompressedClassPointers && addr->disp() == oopDesc::klass_offset_in_bytes()) {\n-        __ ldrw(dest->as_register(), as_Address(from_addr));\n-      } else {\n-        __ ldr(dest->as_register(), as_Address(from_addr));\n-      }\n+      __ ldr(dest->as_register(), as_Address(from_addr));\n@@ -1033,4 +1026,0 @@\n-  } else if (type == T_ADDRESS && addr->disp() == oopDesc::klass_offset_in_bytes()) {\n-    if (UseCompressedClassPointers) {\n-      __ decode_klass_not_null(dest->as_register());\n-    }\n@@ -1252,1 +1241,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n@@ -2306,2 +2295,0 @@\n-  Address src_klass_addr = Address(src, oopDesc::klass_offset_in_bytes());\n-  Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());\n@@ -2368,9 +2355,4 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(tmp, src_klass_addr);\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(tmp, src_klass_addr);\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      assert(UseCompressedClassPointers, \"Lilliput\");\n+      __ load_nklass(tmp, src);\n+      __ load_nklass(rscratch1, dst);\n+      __ cmpw(tmp, rscratch1);\n@@ -2502,0 +2484,1 @@\n+    assert(UseCompressedClassPointers, \"Lilliput\");\n@@ -2503,8 +2486,2 @@\n-\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ load_nklass(rscratch1, dst);\n+      __ cmpw(tmp, rscratch1);\n@@ -2512,7 +2489,2 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, src_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, src_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ load_nklass(rscratch1, src);\n+      __ cmpw(tmp, rscratch1);\n@@ -2521,7 +2493,2 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ load_nklass(rscratch1, dst);\n+      __ cmpw(tmp, rscratch1);\n@@ -2598,0 +2565,21 @@\n+void LIR_Assembler::emit_load_klass(LIR_OpLoadKlass* op) {\n+  Register obj = op->obj()->as_pointer_register();\n+  Register result = op->result_opr()->as_pointer_register();\n+\n+  CodeEmitInfo* info = op->info();\n+  if (info != NULL) {\n+    add_debug_info_for_null_check_here(info);\n+  }\n+\n+  assert(UseCompressedClassPointers, \"expects UseCompressedClassPointers\");\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  __ ldr(result, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  __ tst(result, markWord::monitor_value);\n+  __ br(Assembler::NE, *op->stub()->entry());\n+  __ bind(*op->stub()->continuation());\n+\n+  \/\/ Shift and decode Klass*.\n+  __ lsr(result, result, markWord::klass_shift);\n+  __ decode_klass_not_null(result);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":35,"deletions":47,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -92,33 +92,37 @@\n-  \/\/ and mark it as unlocked\n-  orr(hdr, hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  str(hdr, Address(disp_hdr, 0));\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  lea(rscratch2, Address(obj, hdr_offset));\n-  cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/NULL);\n-  \/\/ if the object header was the same, we're done\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) sp <= hdr\n-  \/\/ 3) hdr <= sp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr - sp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  mov(rscratch1, sp);\n-  sub(hdr, hdr, rscratch1);\n-  ands(hdr, hdr, aligned_mask - os::vm_page_size());\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n-  str(hdr, Address(disp_hdr, 0));\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  cbnz(hdr, slow_case);\n-  \/\/ done\n-  bind(done);\n+  if (UseFastLocking) {\n+    fast_lock(obj, hdr, rscratch1, rscratch2, slow_case, false);\n+  } else {\n+    \/\/ and mark it as unlocked\n+    orr(hdr, hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    str(hdr, Address(disp_hdr, 0));\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    lea(rscratch2, Address(obj, hdr_offset));\n+    cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/NULL);\n+    \/\/ if the object header was the same, we're done\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) sp <= hdr\n+    \/\/ 3) hdr <= sp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr - sp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    mov(rscratch1, sp);\n+    sub(hdr, hdr, rscratch1);\n+    ands(hdr, hdr, aligned_mask - os::vm_page_size());\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n+    str(hdr, Address(disp_hdr, 0));\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    cbnz(hdr, slow_case);\n+    \/\/ done\n+    bind(done);\n+  }\n@@ -145,6 +149,1 @@\n-  \/\/ load displaced header\n-  ldr(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is NULL we had recursive locking\n-  \/\/ if we had recursive locking, we are done\n-  cbz(hdr, done);\n-  if (!UseBiasedLocking) {\n+  if (UseFastLocking) {\n@@ -153,10 +152,3 @@\n-  }\n-  verify_oop(obj);\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  if (hdr_offset) {\n-    lea(rscratch1, Address(obj, hdr_offset));\n-    cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &slow_case);\n+    verify_oop(obj);\n+    ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    fast_unlock(obj, hdr, rscratch1, rscratch2, slow_case);\n@@ -164,1 +156,21 @@\n-    cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &slow_case);\n+    \/\/ load displaced header\n+    ldr(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is NULL we had recursive locking\n+    \/\/ if we had recursive locking, we are done\n+    cbz(hdr, done);\n+    if (!UseBiasedLocking) {\n+      \/\/ load object\n+      ldr(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+    }\n+    verify_oop(obj);\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    if (hdr_offset) {\n+      lea(rscratch1, Address(obj, hdr_offset));\n+      cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &slow_case);\n+    } else {\n+      cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &slow_case);\n+    }\n@@ -186,2 +198,1 @@\n-    \/\/ This assumes that all prototype bits fit in an int32_t\n-    mov(t1, (int32_t)(intptr_t)markWord::prototype().value());\n+    ldr(t1, Address(klass, Klass::prototype_header_offset()));\n@@ -191,7 +202,0 @@\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    encode_klass_not_null(t1, klass);\n-    strw(t1, Address(obj, oopDesc::klass_offset_in_bytes()));\n-  } else {\n-    str(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n-  }\n-\n@@ -200,2 +204,0 @@\n-  } else if (UseCompressedClassPointers) {\n-    store_klass_gap(obj, zr);\n@@ -219,0 +221,6 @@\n+  \/\/ Zero first 4 bytes, if start offset is not word aligned.\n+  if (!is_aligned(hdr_size_in_bytes, BytesPerWord)) {\n+    strw(zr, Address(obj, hdr_size_in_bytes));\n+    hdr_size_in_bytes += BytesPerInt;\n+  }\n+\n@@ -268,1 +276,1 @@\n-void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int header_size, int f, Register klass, Label& slow_case) {\n+void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int base_offset_in_bytes, int f, Register klass, Label& slow_case) {\n@@ -281,1 +289,1 @@\n-  mov(arr_size, (int32_t)header_size * BytesPerWord + MinObjAlignmentInBytesMask);\n+  mov(arr_size, (int32_t)base_offset_in_bytes + MinObjAlignmentInBytesMask);\n@@ -290,1 +298,1 @@\n-  initialize_body(obj, arr_size, header_size * BytesPerWord, t1, t2);\n+  initialize_body(obj, arr_size, base_offset_in_bytes, t1, t2);\n@@ -307,1 +315,1 @@\n-  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), \"must add explicit null check\");\n+  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::mark_offset_in_bytes()), \"must add explicit null check\");\n@@ -313,1 +321,1 @@\n-void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes) {\n+void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes, int max_monitors) {\n@@ -320,0 +328,13 @@\n+  if (UseFastLocking && max_monitors > 0) {\n+    Label ok;\n+    ldr(r9, Address(rthread, JavaThread::lock_stack_current_offset()));\n+    add(r9, r9, max_monitors * oopSize);\n+    ldr(r10, Address(rthread, JavaThread::lock_stack_limit_offset()));\n+    cmp(r9, r10);\n+    br(Assembler::LT, ok);\n+    assert(StubRoutines::aarch64::check_lock_stack() != NULL, \"need runtime call stub\");\n+    movptr(rscratch1, (uintptr_t) StubRoutines::aarch64::check_lock_stack());\n+    blr(rscratch1);\n+    bind(ok);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":87,"deletions":66,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -0,0 +1,76 @@\n+\/*\n+ * Copyright (c) 2020, 2022 Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"opto\/c2_MacroAssembler.hpp\"\n+#include \"opto\/c2_CodeStubs.hpp\"\n+#include \"runtime\/objectMonitor.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+\n+#define __ masm.\n+\n+int C2SafepointPollStub::max_size() const {\n+  return 20;\n+}\n+\n+void C2SafepointPollStub::emit(C2_MacroAssembler& masm) {\n+  assert(SharedRuntime::polling_page_return_handler_blob() != NULL,\n+         \"polling page return stub not created yet\");\n+  address stub = SharedRuntime::polling_page_return_handler_blob()->entry_point();\n+\n+  RuntimeAddress callback_addr(stub);\n+\n+  __ bind(entry());\n+  InternalAddress safepoint_pc(masm.pc() - masm.offset() + _safepoint_offset);\n+  __ adr(rscratch1, safepoint_pc);\n+  __ str(rscratch1, Address(rthread, JavaThread::saved_exception_pc_offset()));\n+  __ far_jump(callback_addr);\n+}\n+\n+int C2CheckLockStackStub::max_size() const {\n+  return 20;\n+}\n+\n+void C2CheckLockStackStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  assert(StubRoutines::aarch64::check_lock_stack() != NULL, \"need runtime call stub\");\n+  __ movptr(rscratch1, (uintptr_t) StubRoutines::aarch64::check_lock_stack());\n+  __ blr(rscratch1);\n+  __ b(continuation());\n+}\n+\n+int C2LoadNKlassStub::max_size() const {\n+  return 8;\n+}\n+\n+void C2LoadNKlassStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register d = dst();\n+  __ ldr(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ b(continuation());\n+}\n+\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_CodeStubs_aarch64.cpp","additions":76,"deletions":0,"binary":false,"changes":76,"status":"added"},{"patch":"@@ -1,46 +0,0 @@\n-\/*\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/macroAssembler.hpp\"\n-#include \"opto\/compile.hpp\"\n-#include \"opto\/node.hpp\"\n-#include \"opto\/output.hpp\"\n-#include \"runtime\/sharedRuntime.hpp\"\n-\n-#define __ masm.\n-void C2SafepointPollStubTable::emit_stub_impl(MacroAssembler& masm, C2SafepointPollStub* entry) const {\n-  assert(SharedRuntime::polling_page_return_handler_blob() != NULL,\n-         \"polling page return stub not created yet\");\n-  address stub = SharedRuntime::polling_page_return_handler_blob()->entry_point();\n-\n-  RuntimeAddress callback_addr(stub);\n-\n-  __ bind(entry->_stub_label);\n-  InternalAddress safepoint_pc(masm.pc() - masm.offset() + entry->_safepoint_offset);\n-  __ adr(rscratch1, safepoint_pc);\n-  __ str(rscratch1, Address(rthread, JavaThread::saved_exception_pc_offset()));\n-  __ far_jump(callback_addr);\n-}\n-#undef __\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_safepointPollStubTable_aarch64.cpp","additions":0,"deletions":46,"binary":false,"changes":46,"status":"deleted"},{"patch":"@@ -757,21 +757,3 @@\n-    if (UseBiasedLocking) {\n-      biased_locking_enter(lock_reg, obj_reg, swap_reg, tmp, false, done, &slow_case);\n-    }\n-\n-    \/\/ Load (object->mark() | 1) into swap_reg\n-    ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    orr(swap_reg, rscratch1, 1);\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    str(swap_reg, Address(lock_reg, mark_offset));\n-\n-    assert(lock_offset == 0,\n-           \"displached header must be first word in BasicObjectLock\");\n-\n-    Label fail;\n-    if (PrintBiasedLockingStatistics) {\n-      Label fast;\n-      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, fast, &fail);\n-      bind(fast);\n-      atomic_incw(Address((address)BiasedLocking::fast_path_entry_count_addr()),\n-                  rscratch2, rscratch1, tmp);\n+    if (UseFastLocking) {\n+      ldr(tmp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock(obj_reg, tmp, rscratch1, rscratch2, slow_case);\n@@ -779,1 +761,0 @@\n-      bind(fail);\n@@ -781,2 +762,3 @@\n-      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, done, \/*fallthrough*\/NULL);\n-    }\n+      if (UseBiasedLocking) {\n+        biased_locking_enter(lock_reg, obj_reg, swap_reg, tmp, false, done, &slow_case);\n+      }\n@@ -784,42 +766,22 @@\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & 7) == 0, and\n-    \/\/  2) sp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from sp is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 3 bits clear.\n-    \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n-    \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n-    \/\/ copy\n-    mov(rscratch1, sp);\n-    sub(swap_reg, swap_reg, rscratch1);\n-    ands(swap_reg, swap_reg, (uint64_t)(7 - os::vm_page_size()));\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    str(swap_reg, Address(lock_reg, mark_offset));\n-\n-    if (PrintBiasedLockingStatistics) {\n-      br(Assembler::NE, slow_case);\n-      atomic_incw(Address((address)BiasedLocking::fast_path_entry_count_addr()),\n-                  rscratch2, rscratch1, tmp);\n-    }\n-    br(Assembler::EQ, done);\n+      \/\/ Load (object->mark() | 1) into swap_reg\n+      ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      orr(swap_reg, rscratch1, 1);\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      str(swap_reg, Address(lock_reg, mark_offset));\n+\n+      assert(lock_offset == 0,\n+             \"displached header must be first word in BasicObjectLock\");\n+\n+      Label fail;\n+      if (PrintBiasedLockingStatistics) {\n+        Label fast;\n+        cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, fast, &fail);\n+        bind(fast);\n+        atomic_incw(Address((address)BiasedLocking::fast_path_entry_count_addr()),\n+                    rscratch2, rscratch1, tmp);\n+        b(done);\n+        bind(fail);\n+      } else {\n+        cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, done, \/*fallthrough*\/NULL);\n+      }\n@@ -827,0 +789,43 @@\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & 7) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from sp is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 3 bits clear.\n+      \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n+      \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n+      \/\/ copy\n+      mov(rscratch1, sp);\n+      sub(swap_reg, swap_reg, rscratch1);\n+      ands(swap_reg, swap_reg, (uint64_t)(7 - os::vm_page_size()));\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      str(swap_reg, Address(lock_reg, mark_offset));\n+\n+      if (PrintBiasedLockingStatistics) {\n+        br(Assembler::NE, slow_case);\n+        atomic_incw(Address((address)BiasedLocking::fast_path_entry_count_addr()),\n+                    rscratch2, rscratch1, tmp);\n+      }\n+      br(Assembler::EQ, done);\n+    }\n@@ -832,1 +837,1 @@\n-            lock_reg);\n+            UseFastLocking ? obj_reg : lock_reg);\n@@ -865,3 +870,2 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into %r0\n-    lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (UseFastLocking) {\n+      Label slow_case;\n@@ -869,2 +873,2 @@\n-    \/\/ Load oop into obj_reg(%c_rarg3)\n-    ldr(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n+      \/\/ Load oop into obj_reg(%c_rarg3)\n+      ldr(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n@@ -872,2 +876,2 @@\n-    \/\/ Free entry\n-    str(zr, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n+      \/\/ Free entry\n+      str(zr, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n@@ -875,3 +879,5 @@\n-    if (UseBiasedLocking) {\n-      biased_locking_exit(obj_reg, header_reg, done);\n-    }\n+      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+      \/\/ must handle it.\n+      ldr(header_reg, Address(rthread, JavaThread::lock_stack_current_offset()));\n+      cmpoop(header_reg, obj_reg);\n+      br(Assembler::NE, slow_case);\n@@ -879,3 +885,8 @@\n-    \/\/ Load the old header from BasicLock structure\n-    ldr(header_reg, Address(swap_reg,\n-                            BasicLock::displaced_header_offset_in_bytes()));\n+      ldr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_unlock(obj_reg, header_reg, swap_reg, rscratch1, slow_case);\n+      b(done);\n+      bind(slow_case);\n+    } else {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into %r0\n+      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n@@ -883,2 +894,2 @@\n-    \/\/ Test for recursion\n-    cbz(header_reg, done);\n+      \/\/ Load oop into obj_reg(%c_rarg3)\n+      ldr(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n@@ -886,2 +897,2 @@\n-    \/\/ Atomic swap back the old header\n-    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, done, \/*fallthrough*\/NULL);\n+      \/\/ Free entry\n+      str(zr, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n@@ -889,0 +900,14 @@\n+      if (UseBiasedLocking) {\n+        biased_locking_exit(obj_reg, header_reg, done);\n+      }\n+\n+      \/\/ Load the old header from BasicLock structure\n+      ldr(header_reg, Address(swap_reg,\n+                              BasicLock::displaced_header_offset_in_bytes()));\n+\n+      \/\/ Test for recursion\n+      cbz(header_reg, done);\n+\n+      \/\/ Atomic swap back the old header\n+      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, done, \/*fallthrough*\/NULL);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":109,"deletions":84,"binary":false,"changes":193,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -3808,0 +3809,20 @@\n+\/\/ Loads the obj's Klass* into dst.\n+\/\/ src and dst must be distinct registers\n+\/\/ Preserves all registers (incl src, rscratch1 and rscratch2), but clobbers condition flags\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompressedClassPointers, \"expects UseCompressedClassPointers\");\n+\n+  Label fast;\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  ldr(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  tbz(dst, exact_log2(markWord::monitor_value), fast);\n+\n+  \/\/ Fetch displaced header\n+  ldr(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  \/\/ Fast-path: shift and decode Klass*.\n+  bind(fast);\n+  lsr(dst, dst, markWord::klass_shift);\n+}\n+\n@@ -3809,6 +3830,2 @@\n-  if (UseCompressedClassPointers) {\n-    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n-    decode_klass_not_null(dst);\n-  } else {\n-    ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n-  }\n+  load_nklass(dst, src);\n+  decode_klass_not_null(dst);\n@@ -3849,14 +3866,10 @@\n-  if (UseCompressedClassPointers) {\n-    ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n-    if (CompressedKlassPointers::base() == NULL) {\n-      cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());\n-      return;\n-    } else if (((uint64_t)CompressedKlassPointers::base() & 0xffffffff) == 0\n-               && CompressedKlassPointers::shift() == 0) {\n-      \/\/ Only the bottom 32 bits matter\n-      cmpw(trial_klass, tmp);\n-      return;\n-    }\n-    decode_klass_not_null(tmp);\n-  } else {\n-    ldr(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+  assert(UseCompressedClassPointers, \"Lilliput\");\n+  load_nklass(tmp, oop);\n+  if (CompressedKlassPointers::base() == NULL) {\n+    cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());\n+    return;\n+  } else if (((uint64_t)CompressedKlassPointers::base() & 0xffffffff) == 0\n+             && CompressedKlassPointers::shift() == 0) {\n+    \/\/ Only the bottom 32 bits matter\n+    cmpw(trial_klass, tmp);\n+    return;\n@@ -3864,0 +3877,1 @@\n+  decode_klass_not_null(tmp);\n@@ -3872,18 +3886,0 @@\n-void MacroAssembler::store_klass(Register dst, Register src) {\n-  \/\/ FIXME: Should this be a store release?  concurrent gcs assumes\n-  \/\/ klass length is valid if klass field is not null.\n-  if (UseCompressedClassPointers) {\n-    encode_klass_not_null(src);\n-    strw(src, Address(dst, oopDesc::klass_offset_in_bytes()));\n-  } else {\n-    str(src, Address(dst, oopDesc::klass_offset_in_bytes()));\n-  }\n-}\n-\n-void MacroAssembler::store_klass_gap(Register dst, Register src) {\n-  if (UseCompressedClassPointers) {\n-    \/\/ Store to klass gap in destination\n-    strw(src, Address(dst, oopDesc::klass_gap_offset_in_bytes()));\n-  }\n-}\n-\n@@ -5369,0 +5365,55 @@\n+\n+\/\/ Attempt to fast-lock an object. Fall-through on success, branch to slow label\n+\/\/ on failure.\n+\/\/ Registers:\n+\/\/  - obj: the object to be locked\n+\/\/  - hdr: the header, already loaded from obj, will be destroyed\n+\/\/  - t1, t2, t3: temporary registers, will be destroyed\n+void MacroAssembler::fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow, bool rt_check_stack) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+  if (rt_check_stack) {\n+    \/\/ Check if we would have space on lock-stack for the object.\n+    ldr(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+    ldr(t2, Address(rthread, JavaThread::lock_stack_limit_offset()));\n+    cmp(t1, t2);\n+    br(Assembler::GE, slow);\n+  }\n+\n+  \/\/ Load (object->mark() | 1) into hdr\n+  orr(hdr, hdr, markWord::unlocked_value);\n+  \/\/ Clear lock-bits, into t2\n+  eor(t2, hdr, markWord::unlocked_value);\n+  \/\/ Try to swing header from unlocked to locked\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ hdr, \/*new*\/ t2, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t1);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful lock, push object on lock-stack\n+  ldr(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+  str(obj, Address(t1, 0));\n+  add(t1, t1, oopSize);\n+  str(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+}\n+\n+void MacroAssembler::fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+  \/\/ Load the expected old header (lock-bits cleared to indicate 'locked') into hdr\n+  andr(hdr, hdr, ~markWord::lock_mask_in_place);\n+\n+  \/\/ Load the new header (unlocked) into t1\n+  orr(t1, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  cmpxchg(obj, hdr, t1, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t2);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  ldr(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+  sub(t1, t1, oopSize);\n+  str(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":89,"deletions":38,"binary":false,"changes":127,"status":"modified"},{"patch":"@@ -826,0 +826,1 @@\n+  void load_nklass(Register dst, Register src);\n@@ -827,1 +828,0 @@\n-  void store_klass(Register dst, Register src);\n@@ -855,2 +855,0 @@\n-  void store_klass_gap(Register dst, Register src);\n-\n@@ -1423,0 +1421,3 @@\n+  void fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow, bool rt_check_stack = true);\n+  void fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -310,1 +310,1 @@\n-        __ null_check(receiver_reg, oopDesc::klass_offset_in_bytes());\n+        __ null_check(receiver_reg, oopDesc::mark_offset_in_bytes());\n","filename":"src\/hotspot\/cpu\/aarch64\/methodHandles_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1776,7 +1776,7 @@\n-    if (UseBiasedLocking) {\n-      __ biased_locking_enter(lock_reg, obj_reg, swap_reg, tmp, false, lock_done, &slow_path_lock);\n-    }\n-\n-    \/\/ Load (object->mark() | 1) into swap_reg %r0\n-    __ ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    __ orr(swap_reg, rscratch1, 1);\n+    if (UseFastLocking) {\n+      __ ldr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ fast_lock(obj_reg, swap_reg, tmp, rscratch1, slow_path_lock);\n+    } else {\n+      if (UseBiasedLocking) {\n+        __ biased_locking_enter(lock_reg, obj_reg, swap_reg, tmp, false, lock_done, &slow_path_lock);\n+      }\n@@ -1784,2 +1784,3 @@\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    __ str(swap_reg, Address(lock_reg, mark_word_offset));\n+      \/\/ Load (object->mark() | 1) into swap_reg %r0\n+      __ ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ orr(swap_reg, rscratch1, 1);\n@@ -1787,4 +1788,2 @@\n-    \/\/ src -> dest iff dest == r0 else r0 <- dest\n-    { Label here;\n-      __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, lock_done, \/*fallthrough*\/NULL);\n-    }\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n@@ -1792,1 +1791,4 @@\n-    \/\/ Hmm should this move to the slow path code area???\n+      \/\/ src -> dest iff dest == r0 else r0 <- dest\n+      { Label here;\n+        __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, lock_done, \/*fallthrough*\/NULL);\n+      }\n@@ -1794,8 +1796,1 @@\n-    \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-    \/\/  1) (mark & 3) == 0, and\n-    \/\/  2) sp <= mark < mark + os::pagesize()\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 2 bits clear.\n-    \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n+      \/\/ Hmm should this move to the slow path code area???\n@@ -1803,3 +1798,8 @@\n-    __ sub(swap_reg, sp, swap_reg);\n-    __ neg(swap_reg, swap_reg);\n-    __ ands(swap_reg, swap_reg, 3 - os::vm_page_size());\n+      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+      \/\/  1) (mark & 3) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 2 bits clear.\n+      \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n@@ -1807,3 +1807,3 @@\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    __ str(swap_reg, Address(lock_reg, mark_word_offset));\n-    __ br(Assembler::NE, slow_path_lock);\n+      __ sub(swap_reg, sp, swap_reg);\n+      __ neg(swap_reg, swap_reg);\n+      __ ands(swap_reg, swap_reg, 3 - os::vm_page_size());\n@@ -1811,0 +1811,4 @@\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n+      __ br(Assembler::NE, slow_path_lock);\n+    }\n@@ -1932,4 +1936,5 @@\n-    \/\/ Simple recursive lock?\n-\n-    __ ldr(rscratch1, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-    __ cbz(rscratch1, done);\n+    if (!UseFastLocking) {\n+      \/\/ Simple recursive lock?\n+      __ ldr(rscratch1, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+      __ cbz(rscratch1, done);\n+    }\n@@ -1943,9 +1948,14 @@\n-    \/\/ get address of the stack lock\n-    __ lea(r0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-    \/\/  get old displaced header\n-    __ ldr(old_hdr, Address(r0, 0));\n-\n-    \/\/ Atomic swap old header if oop still contains the stack lock\n-    Label succeed;\n-    __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, succeed, &slow_path_unlock);\n-    __ bind(succeed);\n+    if (UseFastLocking) {\n+      __ ldr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ fast_unlock(obj_reg, old_hdr, swap_reg, rscratch1, slow_path_unlock);\n+    } else {\n+      \/\/ get address of the stack lock\n+      __ lea(r0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+      \/\/  get old displaced header\n+      __ ldr(old_hdr, Address(r0, 0));\n+\n+      \/\/ Atomic swap old header if oop still contains the stack lock\n+      Label succeed;\n+      __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, succeed, &slow_path_unlock);\n+      __ bind(succeed);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":51,"deletions":41,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -587,2 +587,12 @@\n-    __ load_klass(r0, r0);  \/\/ get klass\n-    __ cbz(r0, error);      \/\/ if klass is NULL it is broken\n+    \/\/ NOTE: We used to load the Klass* here, and compare that to zero.\n+    \/\/ However, with current Lilliput implementation, that would require\n+    \/\/ checking the locking bits and calling into the runtime, which\n+    \/\/ clobbers the condition flags, which may be live around this call.\n+    \/\/ OTOH, this is a simple NULL-check, and we can simply load the upper\n+    \/\/ 32bit of the header as narrowKlass, and compare that to 0. The\n+    \/\/ worst that can happen (rarely) is that the object is locked and\n+    \/\/ we have lock pointer bits in the upper 32bits. We can't get a false\n+    \/\/ negative.\n+    assert(oopDesc::klass_offset_in_bytes() % 4 == 0, \"must be 4 byte aligned\");\n+    __ ldrw(r0, Address(r0, oopDesc::klass_offset_in_bytes()));  \/\/ get klass\n+    __ cbzw(r0, error);      \/\/ if klass is NULL it is broken\n@@ -5294,0 +5304,23 @@\n+  address generate_check_lock_stack() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"check_lock_stack\");\n+\n+    address start = __ pc();\n+\n+    __ set_last_Java_frame(sp, rfp, lr, rscratch1);\n+    __ enter();\n+    __ push_call_clobbered_registers();\n+\n+    __ mov(c_rarg0, r9);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, LockStack::ensure_lock_stack_size), 1);\n+\n+\n+    __ pop_call_clobbered_registers();\n+    __ leave();\n+    __ reset_last_Java_frame(true);\n+\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n@@ -7602,0 +7635,3 @@\n+    if (UseFastLocking) {\n+      StubRoutines::aarch64::_check_lock_stack = generate_check_lock_stack();\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":38,"deletions":2,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+address StubRoutines::aarch64::_check_lock_stack = NULL;\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -75,0 +75,2 @@\n+  static address _check_lock_stack;\n+\n@@ -182,0 +184,4 @@\n+  static address check_lock_stack() {\n+    return _check_lock_stack;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3238,1 +3238,1 @@\n-  __ null_check(recv, oopDesc::klass_offset_in_bytes());\n+  __ null_check(recv, oopDesc::mark_offset_in_bytes());\n@@ -3328,1 +3328,1 @@\n-  __ null_check(r2, oopDesc::klass_offset_in_bytes());\n+  __ null_check(r2, oopDesc::mark_offset_in_bytes());\n@@ -3345,1 +3345,1 @@\n-  __ null_check(r2, oopDesc::klass_offset_in_bytes());\n+  __ null_check(r2, oopDesc::mark_offset_in_bytes());\n@@ -3556,5 +3556,1 @@\n-    if (UseBiasedLocking) {\n-      __ ldr(rscratch1, Address(r4, Klass::prototype_header_offset()));\n-    } else {\n-      __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n-    }\n+    __ ldr(rscratch1, Address(r4, Klass::prototype_header_offset()));\n@@ -3562,2 +3558,0 @@\n-    __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n-    __ store_klass(r0, r4);      \/\/ store klass last\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":4,"deletions":10,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -215,1 +215,1 @@\n-  const ptrdiff_t estimate = 124;\n+  const ptrdiff_t estimate = 128;\n","filename":"src\/hotspot\/cpu\/aarch64\/vtableStubs_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -724,5 +724,1 @@\n-      if (UseCompressedClassPointers && addr->disp() == oopDesc::klass_offset_in_bytes()) {\n-        __ ldr_u32(dest->as_pointer_register(), as_Address(addr));\n-      } else {\n-        __ ldr(dest->as_pointer_register(), as_Address(addr));\n-      }\n+      __ ldr(dest->as_pointer_register(), as_Address(addr));\n@@ -2457,0 +2453,15 @@\n+void LIR_Assembler::emit_load_klass(LIR_OpLoadKlass* op) {\n+  Register obj = op->obj()->as_pointer_register();\n+  Register result = op->result_opr()->as_pointer_register();\n+\n+  CodeEmitInfo* info = op->info();\n+  if (info != NULL) {\n+    add_debug_info_for_null_check_here(info);\n+  }\n+\n+  if (UseCompressedClassPointers) { \/\/ On 32 bit arm??\n+    __ ldr_u32(result, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    __ ldr(result, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":16,"deletions":5,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -815,6 +815,1 @@\n-        if (offset == oopDesc::klass_offset_in_bytes() && UseCompressedClassPointers) {\n-          __ lwz(to_reg->as_register(), offset, base);\n-          __ decode_klass_not_null(to_reg->as_register());\n-        } else {\n-          __ ld(to_reg->as_register(), offset, base);\n-        }\n+        __ ld(to_reg->as_register(), offset, base);\n@@ -2738,0 +2733,22 @@\n+void LIR_Assembler::emit_load_klass(LIR_OpLoadKlass* op) {\n+  Register obj = op->obj()->as_pointer_register();\n+  Register result = op->result_opr()->as_pointer_register();\n+\n+  CodeEmitInfo* info = op->info();\n+  if (info != NULL) {\n+    if (info != NULL) {\n+      if (!os::zero_page_read_protected() || !ImplicitNullChecks) {\n+        explicit_null_check(obj, info);\n+      } else {\n+        add_debug_info_for_null_check_here(info);\n+      }\n+    }\n+  }\n+\n+  if (UseCompressedClassPointers) {\n+    __ lwz(result, oopDesc::klass_offset_in_bytes(), obj);\n+    __ decode_klass_not_null(result);\n+  } else {\n+    __ ld(result, oopDesc::klass_offset_in_bytes(), obj);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":23,"deletions":6,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -953,6 +953,1 @@\n-      if (UseCompressedClassPointers && addr->disp() == oopDesc::klass_offset_in_bytes()) {\n-        __ z_llgf(dest->as_register(), disp_value, disp_reg, src);\n-        __ decode_klass_not_null(dest->as_register());\n-      } else {\n-        __ z_lg(dest->as_register(), disp_value, disp_reg, src);\n-      }\n+      __ z_lg(dest->as_register(), disp_value, disp_reg, src);\n@@ -2757,0 +2752,16 @@\n+void LIR_Assembler::emit_load_klass(LIR_OpLoadKlass* op) {\n+  Register obj = op->obj()->as_pointer_register();\n+  Register result = op->result_opr()->as_pointer_register();\n+\n+  CodeEmitInfo* info = op->info();\n+  if (info != NULL) {\n+    add_debug_info_for_null_check_here(info);\n+  }\n+\n+  if (UseCompressedClassPointers) {\n+    __ z_llgf(result, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    __ decode_klass_not_null(result);\n+  } else {\n+    __ z_lg(result, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":17,"deletions":6,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -303,0 +304,10 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  __ bind(_entry);\n+#ifdef _LP64\n+  Register d = _result->as_register();\n+  __ movq(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ jmp(_continuation);\n+#else\n+  __ should_not_reach_here();\n+#endif\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c1_CodeStubs_x86.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -288,1 +288,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n@@ -1187,1 +1187,0 @@\n-  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n@@ -1260,5 +1259,1 @@\n-      if (UseCompressedClassPointers && addr->disp() == oopDesc::klass_offset_in_bytes()) {\n-        __ movl(dest->as_register(), from_addr);\n-      } else {\n-        __ movptr(dest->as_register(), from_addr);\n-      }\n+      __ movptr(dest->as_register(), from_addr);\n@@ -1370,6 +1365,0 @@\n-  } else if (type == T_ADDRESS && addr->disp() == oopDesc::klass_offset_in_bytes()) {\n-#ifdef _LP64\n-    if (UseCompressedClassPointers) {\n-      __ decode_klass_not_null(dest->as_register(), tmp_load_klass);\n-    }\n-#endif\n@@ -1657,1 +1646,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n@@ -3082,0 +3071,1 @@\n+  Register tmp2 = LP64_ONLY(rscratch2) NOT_LP64(noreg);\n@@ -3206,0 +3196,1 @@\n+#ifndef _LP64\n@@ -3208,1 +3199,1 @@\n-\n+#endif\n@@ -3273,7 +3264,8 @@\n-      if (UseCompressedClassPointers) {\n-        __ movl(tmp, src_klass_addr);\n-        __ cmpl(tmp, dst_klass_addr);\n-      } else {\n-        __ movptr(tmp, src_klass_addr);\n-        __ cmpptr(tmp, dst_klass_addr);\n-      }\n+#ifdef _LP64\n+      __ load_nklass(tmp, src);\n+      __ load_nklass(tmp2, dst);\n+      __ cmpl(tmp, tmp2);\n+#else\n+      __ movptr(tmp, src_klass_addr);\n+      __ cmpptr(tmp, dst_klass_addr);\n+#endif\n@@ -3435,5 +3427,2 @@\n-    if (UseCompressedClassPointers) {\n-      __ encode_klass_not_null(tmp, rscratch1);\n-    }\n-#endif\n-\n+    assert(UseCompressedClassPointers, \"Lilliput\");\n+    __ encode_klass_not_null(tmp, rscratch1);\n@@ -3441,3 +3430,12 @@\n-\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ load_nklass(tmp2, dst);\n+      __ cmpl(tmp, tmp2);\n+      __ jcc(Assembler::notEqual, halt);\n+      __ load_nklass(tmp2, src);\n+      __ cmpl(tmp, tmp2);\n+      __ jcc(Assembler::equal, known_ok);\n+    } else {\n+      __ load_nklass(tmp2, dst);\n+      __ cmpl(tmp, tmp2);\n+#else\n+    if (basic_type != T_OBJECT) {\n+      __ cmpptr(tmp, dst_klass_addr);\n@@ -3445,2 +3443,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, src_klass_addr);\n-      else                   __ cmpptr(tmp, src_klass_addr);\n+      __ cmpptr(tmp, src_klass_addr);\n@@ -3449,2 +3446,2 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmpptr(tmp, dst_klass_addr);\n+#endif\n@@ -3516,1 +3513,1 @@\n-    if (UseBiasedLocking) {\n+    if (UseBiasedLocking || UseFastLocking) {\n@@ -3535,0 +3532,27 @@\n+void LIR_Assembler::emit_load_klass(LIR_OpLoadKlass* op) {\n+  Register obj = op->obj()->as_pointer_register();\n+  Register result = op->result_opr()->as_pointer_register();\n+\n+  CodeEmitInfo* info = op->info();\n+  if (info != NULL) {\n+    add_debug_info_for_null_check_here(info);\n+  }\n+#ifdef _LP64\n+  Register tmp = rscratch1;\n+  assert_different_registers(tmp, obj);\n+  assert_different_registers(tmp, result);\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  __ movq(result, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  __ testb(result, markWord::monitor_value);\n+  __ jcc(Assembler::notZero, *op->stub()->entry());\n+  __ bind(*op->stub()->continuation());\n+  \/\/ Fast-path: shift and decode Klass*.\n+  __ shrq(result, markWord::klass_shift);\n+  __ decode_klass_not_null(result, tmp);\n+#else\n+  __ movptr(result, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  \/\/ Not really needed, but bind the label anyway to make compiler happy.\n+  __ bind(*op->stub()->continuation());\n+#endif\n+}\n@@ -3639,4 +3663,0 @@\n-#ifndef ASSERT\n-      __ jmpb(next);\n-    }\n-#else\n@@ -3645,0 +3665,1 @@\n+#ifdef ASSERT\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":60,"deletions":39,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -317,1 +317,1 @@\n-  if (UseBiasedLocking) {\n+  if (UseBiasedLocking || UseFastLocking) {\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -46,2 +46,1 @@\n-  assert(hdr != obj && hdr != disp_hdr && obj != disp_hdr, \"registers must be different\");\n-  Label done;\n+  assert_different_registers(hdr, obj, disp_hdr, scratch);\n@@ -64,4 +63,17 @@\n-  if (UseBiasedLocking) {\n-    assert(scratch != noreg, \"should have scratch register at this point\");\n-    biased_locking_enter(disp_hdr, obj, hdr, scratch, rklass_decode_tmp, false, done, &slow_case);\n-  }\n+  if (UseFastLocking) {\n+#ifdef _LP64\n+    const Register thread = r15_thread;\n+#else\n+    const Register thread = disp_hdr;\n+    get_thread(thread);\n+#endif\n+    \/\/ Load object header\n+    movptr(hdr, Address(obj, hdr_offset));\n+    fast_lock_impl(obj, hdr, thread, scratch, slow_case, LP64_ONLY(false) NOT_LP64(true));\n+  } else {\n+    Label done;\n+\n+    if (UseBiasedLocking) {\n+      assert(scratch != noreg, \"should have scratch register at this point\");\n+      biased_locking_enter(disp_hdr, obj, hdr, scratch, rklass_decode_tmp, false, done, &slow_case);\n+    }\n@@ -69,15 +81,39 @@\n-  \/\/ Load object header\n-  movptr(hdr, Address(obj, hdr_offset));\n-  \/\/ and mark it as unlocked\n-  orptr(hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  movptr(Address(disp_hdr, 0), hdr);\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-  cmpxchgptr(disp_hdr, Address(obj, hdr_offset));\n-  \/\/ if the object header was the same, we're done\n-  if (PrintBiasedLockingStatistics) {\n-    cond_inc32(Assembler::equal,\n-               ExternalAddress((address)BiasedLocking::fast_path_entry_count_addr()));\n+    \/\/ Load object header\n+    movptr(hdr, Address(obj, hdr_offset));\n+    \/\/ and mark it as unlocked\n+    orptr(hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    movptr(Address(disp_hdr, 0), hdr);\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n+    cmpxchgptr(disp_hdr, Address(obj, hdr_offset));\n+    \/\/ if the object header was the same, we're done\n+    if (PrintBiasedLockingStatistics) {\n+      cond_inc32(Assembler::equal,\n+                 ExternalAddress((address)BiasedLocking::fast_path_entry_count_addr()));\n+    }\n+    jcc(Assembler::equal, done);\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) rsp <= hdr\n+    \/\/ 3) hdr <= rsp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr - rsp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    subptr(hdr, rsp);\n+    andptr(hdr, aligned_mask - os::vm_page_size());\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n+    movptr(Address(disp_hdr, 0), hdr);\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    jcc(Assembler::notZero, slow_case);\n+    \/\/ done\n+    bind(done);\n@@ -85,23 +121,0 @@\n-  jcc(Assembler::equal, done);\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) rsp <= hdr\n-  \/\/ 3) hdr <= rsp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr - rsp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  subptr(hdr, rsp);\n-  andptr(hdr, aligned_mask - os::vm_page_size());\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n-  movptr(Address(disp_hdr, 0), hdr);\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  jcc(Assembler::notZero, slow_case);\n-  \/\/ done\n-  bind(done);\n@@ -117,1 +130,0 @@\n-  Label done;\n@@ -119,1 +131,1 @@\n-  if (UseBiasedLocking) {\n+  if (UseFastLocking) {\n@@ -122,2 +134,6 @@\n-    biased_locking_exit(obj, hdr, done);\n-  }\n+    verify_oop(obj);\n+    movptr(disp_hdr, Address(obj, hdr_offset));\n+    andptr(disp_hdr, ~(int32_t)markWord::lock_mask_in_place);\n+    fast_unlock_impl(obj, disp_hdr, hdr, slow_case);\n+  } else {\n+    Label done;\n@@ -125,9 +141,27 @@\n-  \/\/ load displaced header\n-  movptr(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is NULL we had recursive locking\n-  testptr(hdr, hdr);\n-  \/\/ if we had recursive locking, we are done\n-  jcc(Assembler::zero, done);\n-  if (!UseBiasedLocking) {\n-    \/\/ load object\n-    movptr(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+    if (UseBiasedLocking) {\n+      \/\/ load object\n+      movptr(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+      biased_locking_exit(obj, hdr, done);\n+    }\n+\n+    \/\/ load displaced header\n+    movptr(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is NULL we had recursive locking\n+    testptr(hdr, hdr);\n+    \/\/ if we had recursive locking, we are done\n+    jcc(Assembler::zero, done);\n+    if (!UseBiasedLocking) {\n+      \/\/ load object\n+      movptr(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+    }\n+    verify_oop(obj);\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n+    cmpxchgptr(hdr, Address(obj, hdr_offset));\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    jcc(Assembler::notEqual, slow_case);\n+    \/\/ done\n+    bind(done);\n@@ -135,11 +169,0 @@\n-  verify_oop(obj);\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-  cmpxchgptr(hdr, Address(obj, hdr_offset));\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  jcc(Assembler::notEqual, slow_case);\n-  \/\/ done\n-  bind(done);\n@@ -148,1 +171,0 @@\n-\n@@ -160,16 +182,5 @@\n-  assert_different_registers(obj, klass, len);\n-  Register tmp_encode_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n-  if (UseBiasedLocking && !len->is_valid()) {\n-    assert_different_registers(obj, klass, len, t1, t2);\n-    movptr(t1, Address(klass, Klass::prototype_header_offset()));\n-    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), t1);\n-  } else {\n-    \/\/ This assumes that all prototype bits fit in an int32_t\n-    movptr(Address(obj, oopDesc::mark_offset_in_bytes ()), (int32_t)(intptr_t)markWord::prototype().value());\n-  }\n-#ifdef _LP64\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    movptr(t1, klass);\n-    encode_klass_not_null(t1, tmp_encode_klass);\n-    movl(Address(obj, oopDesc::klass_offset_in_bytes()), t1);\n-  } else\n+  assert_different_registers(obj, klass, len, t1, t2);\n+  movptr(t1, Address(klass, Klass::prototype_header_offset()));\n+  movptr(Address(obj, oopDesc::mark_offset_in_bytes()), t1);\n+#ifndef _LP64\n+  movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n@@ -177,3 +188,0 @@\n-  {\n-    movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n-  }\n@@ -184,6 +192,0 @@\n-#ifdef _LP64\n-  else if (UseCompressedClassPointers) {\n-    xorptr(t1, t1);\n-    store_klass_gap(obj, t1);\n-  }\n-#endif\n@@ -267,1 +269,1 @@\n-void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int header_size, Address::ScaleFactor f, Register klass, Label& slow_case) {\n+void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int base_offset_in_bytes, Address::ScaleFactor f, Register klass, Label& slow_case) {\n@@ -280,1 +282,1 @@\n-  movptr(arr_size, (int32_t)header_size * BytesPerWord + MinObjAlignmentInBytesMask);\n+  movptr(arr_size, (int32_t)base_offset_in_bytes + MinObjAlignmentInBytesMask);\n@@ -290,1 +292,1 @@\n-  initialize_body(obj, arr_size, header_size * BytesPerWord, len_zero);\n+  initialize_body(obj, arr_size, base_offset_in_bytes, len_zero);\n@@ -325,1 +327,1 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int max_monitors) {\n@@ -346,0 +348,13 @@\n+#ifdef _LP64\n+  if (UseFastLocking && max_monitors > 0) {\n+    Label ok;\n+    movptr(rax, Address(r15_thread, JavaThread::lock_stack_current_offset()));\n+    addptr(rax, max_monitors * wordSize);\n+    cmpptr(rax, Address(r15_thread, JavaThread::lock_stack_limit_offset()));\n+    jcc(Assembler::less, ok);\n+    assert(StubRoutines::x86::check_lock_stack() != NULL, \"need runtime call stub\");\n+    call(RuntimeAddress(StubRoutines::x86::check_lock_stack()));\n+    bind(ok);\n+  }\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":113,"deletions":98,"binary":false,"changes":211,"status":"modified"},{"patch":"@@ -0,0 +1,90 @@\n+\/*\n+ * Copyright (c) 2020, 2022 Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"opto\/c2_MacroAssembler.hpp\"\n+#include \"opto\/c2_CodeStubs.hpp\"\n+#include \"runtime\/objectMonitor.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+\n+#define __ masm.\n+\n+int C2SafepointPollStub::max_size() const {\n+  return 33;\n+}\n+\n+void C2SafepointPollStub::emit(C2_MacroAssembler& masm) {\n+  assert(SharedRuntime::polling_page_return_handler_blob() != NULL,\n+         \"polling page return stub not created yet\");\n+  address stub = SharedRuntime::polling_page_return_handler_blob()->entry_point();\n+\n+  RuntimeAddress callback_addr(stub);\n+\n+  __ bind(entry());\n+  InternalAddress safepoint_pc(masm.pc() - masm.offset() + _safepoint_offset);\n+#ifdef _LP64\n+  __ lea(rscratch1, safepoint_pc);\n+  __ movptr(Address(r15_thread, JavaThread::saved_exception_pc_offset()), rscratch1);\n+#else\n+  const Register tmp1 = rcx;\n+  const Register tmp2 = rdx;\n+  __ push(tmp1);\n+  __ push(tmp2);\n+\n+  __ lea(tmp1, safepoint_pc);\n+  __ get_thread(tmp2);\n+  __ movptr(Address(tmp2, JavaThread::saved_exception_pc_offset()), tmp1);\n+\n+  __ pop(tmp2);\n+  __ pop(tmp1);\n+#endif\n+  __ jump(callback_addr);\n+}\n+\n+int C2CheckLockStackStub::max_size() const {\n+  return 10;\n+}\n+\n+void C2CheckLockStackStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  assert(StubRoutines::x86::check_lock_stack() != NULL, \"need runtime call stub\");\n+  __ call(RuntimeAddress(StubRoutines::x86::check_lock_stack()));\n+  __ jmp(continuation(), false \/* maybe_short *\/);\n+}\n+\n+#ifdef _LP64\n+int C2LoadNKlassStub::max_size() const {\n+  return 10;\n+}\n+\n+void C2LoadNKlassStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register d = dst();\n+  __ movq(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ jmp(continuation());\n+}\n+#endif\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/x86\/c2_CodeStubs_x86.cpp","additions":90,"deletions":0,"binary":false,"changes":90,"status":"added"},{"patch":"@@ -449,1 +449,1 @@\n-                                 Register scrReg, Register cx1Reg, Register cx2Reg,\n+                                 Register scrReg, Register cx1Reg, Register cx2Reg, Register thread,\n@@ -518,21 +518,39 @@\n-  \/\/ Attempt stack-locking ...\n-  orptr (tmpReg, markWord::unlocked_value);\n-  movptr(Address(boxReg, 0), tmpReg);          \/\/ Anticipate successful CAS\n-  lock();\n-  cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      \/\/ Updates tmpReg\n-  if (counters != NULL) {\n-    cond_inc32(Assembler::equal,\n-               ExternalAddress((address)counters->fast_path_entry_count_addr()));\n-  }\n-  jcc(Assembler::equal, DONE_LABEL);           \/\/ Success\n-\n-  \/\/ Recursive locking.\n-  \/\/ The object is stack-locked: markword contains stack pointer to BasicLock.\n-  \/\/ Locked by current thread if difference with current SP is less than one page.\n-  subptr(tmpReg, rsp);\n-  \/\/ Next instruction set ZFlag == 1 (Success) if difference is less then one page.\n-  andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - os::vm_page_size())) );\n-  movptr(Address(boxReg, 0), tmpReg);\n-  if (counters != NULL) {\n-    cond_inc32(Assembler::equal,\n-               ExternalAddress((address)counters->fast_path_entry_count_addr()));\n+  if (UseFastLocking) {\n+#ifdef _LP64\n+    fast_lock_impl(objReg, tmpReg, thread, scrReg, DONE_LABEL, false);\n+    xorl(tmpReg, tmpReg); \/\/ Set ZF=1 to indicate success\n+#else\n+    \/\/ We can not emit the lock-stack-check in verified_entry() because we don't have enough\n+    \/\/ registers (for thread ptr). Therefor we have to emit the lock-stack-check in\n+    \/\/ fast_lock_impl(). However, that check can take a slow-path with ZF=1, therefore\n+    \/\/ we need to handle it specially and force ZF=0 before taking the actual slow-path.\n+    Label slow;\n+    fast_lock_impl(objReg, tmpReg, thread, scrReg, slow);\n+    xorl(tmpReg, tmpReg);\n+    jmp(DONE_LABEL);\n+    bind(slow);\n+    testptr(objReg, objReg); \/\/ ZF=0 to indicate failure\n+#endif\n+  } else {\n+    \/\/ Attempt stack-locking ...\n+    orptr (tmpReg, markWord::unlocked_value);\n+    movptr(Address(boxReg, 0), tmpReg);          \/\/ Anticipate successful CAS\n+    lock();\n+    cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      \/\/ Updates tmpReg\n+    if (counters != NULL) {\n+      cond_inc32(Assembler::equal,\n+                 ExternalAddress((address)counters->fast_path_entry_count_addr()));\n+    }\n+    jcc(Assembler::equal, DONE_LABEL);           \/\/ Success\n+\n+    \/\/ Recursive locking.\n+    \/\/ The object is stack-locked: markword contains stack pointer to BasicLock.\n+    \/\/ Locked by current thread if difference with current SP is less than one page.\n+    subptr(tmpReg, rsp);\n+    \/\/ Next instruction set ZFlag == 1 (Success) if difference is less then one page.\n+    andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - os::vm_page_size())) );\n+    movptr(Address(boxReg, 0), tmpReg);\n+    if (counters != NULL) {\n+      cond_inc32(Assembler::equal,\n+                 ExternalAddress((address)counters->fast_path_entry_count_addr()));\n+    }\n@@ -579,1 +597,1 @@\n-  cmpxchgptr(scrReg, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+  cmpxchgptr(thread, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n@@ -581,8 +599,0 @@\n-  \/\/ If we weren't able to swing _owner from NULL to the BasicLock\n-  \/\/ then take the slow path.\n-  jccb  (Assembler::notZero, DONE_LABEL);\n-  \/\/ update _owner from BasicLock to thread\n-  get_thread (scrReg);                    \/\/ beware: clobbers ICCs\n-  movptr(Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), scrReg);\n-  xorptr(boxReg, boxReg);                 \/\/ set icc.ZFlag = 1 to indicate success\n-\n@@ -687,2 +697,4 @@\n-  cmpptr(Address(boxReg, 0), (int32_t)NULL_WORD);                   \/\/ Examine the displaced header\n-  jcc   (Assembler::zero, DONE_LABEL);                              \/\/ 0 indicates recursive stack-lock\n+  if (!UseFastLocking) {\n+    cmpptr(Address(boxReg, 0), (int32_t)NULL_WORD);                   \/\/ Examine the displaced header\n+    jcc   (Assembler::zero, DONE_LABEL);                              \/\/ 0 indicates recursive stack-lock\n+  }\n@@ -691,1 +703,7 @@\n-  jccb  (Assembler::zero, Stacked);\n+  jcc(Assembler::zero, Stacked);\n+\n+  if (UseFastLocking) {\n+    \/\/ If the owner is ANONYMOUS, we need to fix it - in the slow-path.\n+    testb(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int) (intptr_t) ANONYMOUS_OWNER);\n+    jcc(Assembler::notEqual, DONE_LABEL);\n+  }\n@@ -742,8 +760,14 @@\n-  \/\/ It's not inflated and it's not recursively stack-locked and it's not biased.\n-  \/\/ It must be stack-locked.\n-  \/\/ Try to reset the header to displaced header.\n-  \/\/ The \"box\" value on the stack is stable, so we can reload\n-  \/\/ and be assured we observe the same value as above.\n-  movptr(tmpReg, Address(boxReg, 0));\n-  lock();\n-  cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+  if (UseFastLocking) {\n+    mov(boxReg, tmpReg);\n+    fast_unlock_impl(objReg, boxReg, tmpReg, DONE_LABEL);\n+    xorl(tmpReg, tmpReg);\n+  } else {\n+    \/\/ It's not inflated and it's not recursively stack-locked and it's not biased.\n+    \/\/ It must be stack-locked.\n+    \/\/ Try to reset the header to displaced header.\n+    \/\/ The \"box\" value on the stack is stable, so we can reload\n+    \/\/ and be assured we observe the same value as above.\n+    movptr(tmpReg, Address(boxReg, 0));\n+    lock();\n+    cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+  }\n@@ -834,3 +858,10 @@\n-  movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n-  lock();\n-  cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+\n+  if (UseFastLocking) {\n+    mov(boxReg, tmpReg);\n+    fast_unlock_impl(objReg, boxReg, tmpReg, DONE_LABEL);\n+    xorl(tmpReg, tmpReg);\n+  } else {\n+    movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n+    lock();\n+    cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":76,"deletions":45,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-                 Register scr, Register cx1, Register cx2,\n+                 Register scr, Register cx1, Register cx2, Register thread,\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1,60 +0,0 @@\n-\/*\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/macroAssembler.hpp\"\n-#include \"opto\/compile.hpp\"\n-#include \"opto\/node.hpp\"\n-#include \"opto\/output.hpp\"\n-#include \"runtime\/sharedRuntime.hpp\"\n-\n-#define __ masm.\n-void C2SafepointPollStubTable::emit_stub_impl(MacroAssembler& masm, C2SafepointPollStub* entry) const {\n-  assert(SharedRuntime::polling_page_return_handler_blob() != NULL,\n-         \"polling page return stub not created yet\");\n-  address stub = SharedRuntime::polling_page_return_handler_blob()->entry_point();\n-\n-  RuntimeAddress callback_addr(stub);\n-\n-  __ bind(entry->_stub_label);\n-  InternalAddress safepoint_pc(masm.pc() - masm.offset() + entry->_safepoint_offset);\n-#ifdef _LP64\n-  __ lea(rscratch1, safepoint_pc);\n-  __ movptr(Address(r15_thread, JavaThread::saved_exception_pc_offset()), rscratch1);\n-#else\n-  const Register tmp1 = rcx;\n-  const Register tmp2 = rdx;\n-  __ push(tmp1);\n-  __ push(tmp2);\n-\n-  __ lea(tmp1, safepoint_pc);\n-  __ get_thread(tmp2);\n-  __ movptr(Address(tmp2, JavaThread::saved_exception_pc_offset()), tmp1);\n-\n-  __ pop(tmp2);\n-  __ pop(tmp1);\n-#endif\n-  __ jump(callback_addr);\n-}\n-#undef __\n","filename":"src\/hotspot\/cpu\/x86\/c2_safepointPollStubTable_x86.cpp","additions":0,"deletions":60,"binary":false,"changes":60,"status":"deleted"},{"patch":"@@ -60,1 +60,1 @@\n-  jmpb(next);\n+  jmp(next);\n@@ -1234,2 +1234,14 @@\n-    \/\/ Load immediate 1 into swap_reg %rax\n-    movl(swap_reg, (int32_t)1);\n+    if (UseFastLocking) {\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = lock_reg;\n+      get_thread(thread);\n+#endif\n+      \/\/ Load object header, prepare for CAS from unlocked to locked.\n+      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock_impl(obj_reg, swap_reg, thread, tmp_reg, slow_case);\n+      jmp(done);\n+    } else {\n+      \/\/ Load immediate 1 into swap_reg %rax\n+      movl(swap_reg, (int32_t)1);\n@@ -1237,2 +1249,2 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg %rax\n-    orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      \/\/ Load (object->mark() | 1) into swap_reg %rax\n+      orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1240,2 +1252,2 @@\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    movptr(Address(lock_reg, mark_offset), swap_reg);\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      movptr(Address(lock_reg, mark_offset), swap_reg);\n@@ -1243,2 +1255,2 @@\n-    assert(lock_offset == 0,\n-           \"displaced header must be first word in BasicObjectLock\");\n+      assert(lock_offset == 0,\n+             \"displaced header must be first word in BasicObjectLock\");\n@@ -1246,46 +1258,48 @@\n-    lock();\n-    cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    if (PrintBiasedLockingStatistics) {\n-      cond_inc32(Assembler::zero,\n-                 ExternalAddress((address) BiasedLocking::fast_path_entry_count_addr()));\n-    }\n-    jcc(Assembler::zero, done);\n-\n-    const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n-\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & zero_bits) == 0, and\n-    \/\/  2) rsp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from rsp is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant bits clear.\n-    \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n-    subptr(swap_reg, rsp);\n-    andptr(swap_reg, zero_bits - os::vm_page_size());\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    movptr(Address(lock_reg, mark_offset), swap_reg);\n-\n-    if (PrintBiasedLockingStatistics) {\n-      cond_inc32(Assembler::zero,\n-                 ExternalAddress((address) BiasedLocking::fast_path_entry_count_addr()));\n+      lock();\n+      cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      if (PrintBiasedLockingStatistics) {\n+        cond_inc32(Assembler::zero,\n+                   ExternalAddress((address) BiasedLocking::fast_path_entry_count_addr()));\n+      }\n+      jcc(Assembler::zero, done);\n+\n+      const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n+\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & zero_bits) == 0, and\n+      \/\/  2) rsp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from rsp is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant bits clear.\n+      \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n+      subptr(swap_reg, rsp);\n+      andptr(swap_reg, zero_bits - os::vm_page_size());\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      movptr(Address(lock_reg, mark_offset), swap_reg);\n+\n+      if (PrintBiasedLockingStatistics) {\n+        cond_inc32(Assembler::zero,\n+                   ExternalAddress((address) BiasedLocking::fast_path_entry_count_addr()));\n+      }\n+      jcc(Assembler::zero, done);\n@@ -1293,2 +1307,0 @@\n-    jcc(Assembler::zero, done);\n-\n@@ -1300,1 +1312,1 @@\n-            lock_reg);\n+            UseFastLocking ? obj_reg : lock_reg);\n@@ -1326,1 +1338,1 @@\n-    Label done;\n+    Label done, slow_case;\n@@ -1334,4 +1346,0 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into %rax\n-    lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n-\n@@ -1344,3 +1352,19 @@\n-    if (UseBiasedLocking) {\n-      biased_locking_exit(obj_reg, header_reg, done);\n-    }\n+    if (UseFastLocking) {\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = header_reg;\n+      get_thread(thread);\n+#endif\n+      \/\/ Handle unstructured locking.\n+      cmpptr(obj_reg, Address(thread, JavaThread::lock_stack_current_offset()));\n+      jcc(Assembler::notEqual, slow_case);\n+      \/\/ Try to swing header from locked to unlock.\n+      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+      fast_unlock_impl(obj_reg, swap_reg, header_reg, slow_case);\n+      jmp(done);\n+    } else {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into %rax\n+      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n@@ -1348,3 +1372,3 @@\n-    \/\/ Load the old header from BasicLock structure\n-    movptr(header_reg, Address(swap_reg,\n-                               BasicLock::displaced_header_offset_in_bytes()));\n+      if (UseBiasedLocking) {\n+        biased_locking_exit(obj_reg, header_reg, done);\n+      }\n@@ -1352,2 +1376,3 @@\n-    \/\/ Test for recursion\n-    testptr(header_reg, header_reg);\n+      \/\/ Load the old header from BasicLock structure\n+      movptr(header_reg, Address(swap_reg,\n+                                 BasicLock::displaced_header_offset_in_bytes()));\n@@ -1355,2 +1380,2 @@\n-    \/\/ zero for recursive case\n-    jcc(Assembler::zero, done);\n+      \/\/ Test for recursion\n+      testptr(header_reg, header_reg);\n@@ -1358,3 +1383,2 @@\n-    \/\/ Atomic swap back the old header\n-    lock();\n-    cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      \/\/ zero for recursive case\n+      jcc(Assembler::zero, done);\n@@ -1362,2 +1386,3 @@\n-    \/\/ zero for simple unlock of a stack-lock case\n-    jcc(Assembler::zero, done);\n+      \/\/ Atomic swap back the old header\n+      lock();\n+      cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1365,0 +1390,3 @@\n+      \/\/ zero for simple unlock of a stack-lock case\n+      jcc(Assembler::zero, done);\n+    }\n@@ -1366,0 +1394,1 @@\n+    bind(slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":107,"deletions":78,"binary":false,"changes":185,"status":"modified"},{"patch":"@@ -57,0 +57,6 @@\n+#ifdef COMPILER2\n+#include \"opto\/c2_CodeStubs.hpp\"\n+#include \"opto\/compile.hpp\"\n+#include \"opto\/output.hpp\"\n+#endif\n+\n@@ -3774,1 +3780,1 @@\n-  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  assert((offset_in_bytes & (BytesPerInt - 1)) == 0, \"offset must be a multiple of BytesPerInt\");\n@@ -3780,0 +3786,13 @@\n+  \/\/ Emit single 32bit store to clear leading bytes, if necessary.\n+  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n+#ifdef _LP64\n+  if (!is_aligned(offset_in_bytes, BytesPerWord)) {\n+    movl(Address(address, offset_in_bytes), temp);\n+    offset_in_bytes += BytesPerInt;\n+    decrement(length_in_bytes, BytesPerInt);\n+  }\n+  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  testptr(length_in_bytes, length_in_bytes);\n+  jcc(Assembler::zero, done);\n+#endif\n+\n@@ -3792,1 +3811,0 @@\n-  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n@@ -4736,1 +4754,18 @@\n-void MacroAssembler::load_klass(Register dst, Register src, Register tmp) {\n+#ifdef _LP64\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompressedClassPointers, \"expect compressed class pointers\");\n+\n+  Label fast;\n+  movq(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  testb(dst, markWord::monitor_value);\n+  jccb(Assembler::zero, fast);\n+\n+  \/\/ Fetch displaced header\n+  movq(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  bind(fast);\n+  shrq(dst, markWord::klass_shift);\n+}\n+#endif\n+\n+void MacroAssembler::load_klass(Register dst, Register src, Register tmp, bool null_check_src) {\n@@ -4740,4 +4775,11 @@\n-  if (UseCompressedClassPointers) {\n-    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n-    decode_klass_not_null(dst, tmp);\n-  } else\n+  assert(UseCompressedClassPointers, \"expect compressed class pointers\");\n+  if (null_check_src) {\n+    null_check(src, oopDesc::mark_offset_in_bytes());\n+  }\n+  load_nklass(dst, src);\n+  decode_klass_not_null(dst, tmp);\n+#else\n+  if (null_check_src) {\n+    null_check(src, oopDesc::klass_offset_in_bytes());\n+  }\n+  movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n@@ -4745,1 +4787,0 @@\n-    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n@@ -4753,10 +4794,3 @@\n-void MacroAssembler::store_klass(Register dst, Register src, Register tmp) {\n-  assert_different_registers(src, tmp);\n-  assert_different_registers(dst, tmp);\n-#ifdef _LP64\n-  if (UseCompressedClassPointers) {\n-    encode_klass_not_null(src, tmp);\n-    movl(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n-  } else\n-#endif\n-    movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n+#ifndef _LP64\n+void MacroAssembler::store_klass(Register dst, Register src) {\n+  movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n@@ -4764,0 +4798,1 @@\n+#endif\n@@ -4811,7 +4846,0 @@\n-void MacroAssembler::store_klass_gap(Register dst, Register src) {\n-  if (UseCompressedClassPointers) {\n-    \/\/ Store to klass gap in destination\n-    movl(Address(dst, oopDesc::klass_gap_offset_in_bytes()), src);\n-  }\n-}\n-\n@@ -5122,1 +5150,1 @@\n-void MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub, int max_monitors) {\n@@ -5204,0 +5232,14 @@\n+#if defined(_LP64) && defined(COMPILER2)\n+  if (UseFastLocking && max_monitors > 0) {\n+    C2CheckLockStackStub* stub = new (Compile::current()->comp_arena()) C2CheckLockStackStub();\n+    Compile::current()->output()->add_stub(stub);\n+    assert(!is_stub, \"only methods have monitors\");\n+    Register thread = r15_thread;\n+    movptr(rax, Address(thread, JavaThread::lock_stack_current_offset()));\n+    addptr(rax, max_monitors * wordSize);\n+    cmpptr(rax, Address(thread, JavaThread::lock_stack_limit_offset()));\n+    jcc(Assembler::greaterEqual, stub->entry());\n+    bind(stub->continuation());\n+  }\n+#endif\n+\n@@ -8686,0 +8728,58 @@\n+\n+void MacroAssembler::fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow, bool rt_check_stack) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, thread, tmp);\n+\n+  \/\/ First we need to check if the lock-stack has room for pushing the object reference.\n+  if (rt_check_stack) {\n+    movptr(tmp, Address(thread, JavaThread::lock_stack_current_offset()));\n+    cmpptr(tmp, Address(thread, JavaThread::lock_stack_limit_offset()));\n+    jcc(Assembler::greaterEqual, slow);\n+  }\n+#ifdef ASSERT\n+  else {\n+    Label ok;\n+    movptr(tmp, Address(thread, JavaThread::lock_stack_current_offset()));\n+    cmpptr(tmp, Address(thread, JavaThread::lock_stack_limit_offset()));\n+    jcc(Assembler::less, ok);\n+    stop(\"Not enough room in lock stack; should have been checked in the method prologue\");\n+    bind(ok);\n+  }\n+#endif\n+\n+  \/\/ Now we attempt to take the fast-lock.\n+  \/\/ Clear lowest two header bits (locked state).\n+  andptr(hdr, ~(int32_t )markWord::lock_mask_in_place);\n+  movptr(tmp, hdr);\n+  \/\/ Set lowest bit (unlocked state).\n+  orptr(hdr, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+\n+  \/\/ If successful, push object to lock-stack.\n+  movptr(tmp, Address(thread, JavaThread::lock_stack_current_offset()));\n+  movptr(Address(tmp, 0), obj);\n+  addptr(tmp, oopSize);\n+  movptr(Address(thread, JavaThread::lock_stack_current_offset()), tmp);\n+}\n+\n+void MacroAssembler::fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, tmp);\n+\n+  \/\/ Mark-word must be 00 now, try to swing it back to 01 (unlocked)\n+  movptr(tmp, hdr); \/\/ The expected old value\n+  orptr(tmp, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+  \/\/ Pop the lock object from the lock-stack.\n+#ifdef _LP64\n+  const Register thread = r15_thread;\n+#else\n+  const Register thread = rax;\n+  get_thread(rax);\n+#endif\n+  subptr(Address(thread, JavaThread::lock_stack_current_offset()), oopSize);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":126,"deletions":26,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -342,2 +342,6 @@\n-  void load_klass(Register dst, Register src, Register tmp);\n-  void store_klass(Register dst, Register src, Register tmp);\n+  void load_klass(Register dst, Register src, Register tmp, bool null_check_src = false);\n+#ifdef _LP64\n+  void load_nklass(Register dst, Register src);\n+#else\n+  void store_klass(Register dst, Register src);\n+#endif\n@@ -364,2 +368,0 @@\n-  void store_klass_gap(Register dst, Register src);\n-\n@@ -1726,1 +1728,1 @@\n-  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);\n+  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub, int max_monitors);\n@@ -1913,0 +1915,3 @@\n+\n+  void fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow, bool rt_check_stack = true);\n+  void fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":10,"deletions":5,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -367,2 +367,1 @@\n-        __ null_check(receiver_reg, oopDesc::klass_offset_in_bytes());\n-        __ load_klass(temp1_recv_klass, receiver_reg, temp2);\n+        __ load_klass(temp1_recv_klass, receiver_reg, temp2, true);\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1826,4 +1826,9 @@\n-    if (UseBiasedLocking) {\n-      \/\/ Note that oop_handle_reg is trashed during this call\n-      __ biased_locking_enter(lock_reg, obj_reg, swap_reg, oop_handle_reg, noreg, false, lock_done, &slow_path_lock);\n-    }\n+    if (UseFastLocking) {\n+     \/\/ Load object header\n+     __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+     __ fast_lock_impl(obj_reg, swap_reg, thread, lock_reg, slow_path_lock);\n+    } else {\n+      if (UseBiasedLocking) {\n+        \/\/ Note that oop_handle_reg is trashed during this call\n+        __ biased_locking_enter(lock_reg, obj_reg, swap_reg, oop_handle_reg, noreg, false, lock_done, &slow_path_lock);\n+      }\n@@ -1831,2 +1836,2 @@\n-    \/\/ Load immediate 1 into swap_reg %rax,\n-    __ movptr(swap_reg, 1);\n+      \/\/ Load immediate 1 into swap_reg %rax,\n+      __ movptr(swap_reg, 1);\n@@ -1834,2 +1839,2 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg %rax,\n-    __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      \/\/ Load (object->mark() | 1) into swap_reg %rax,\n+      __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1837,2 +1842,2 @@\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n@@ -1840,5 +1845,5 @@\n-    \/\/ src -> dest iff dest == rax, else rax, <- dest\n-    \/\/ *obj_reg = lock_reg iff *obj_reg == rax, else rax, = *(obj_reg)\n-    __ lock();\n-    __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    __ jcc(Assembler::equal, lock_done);\n+      \/\/ src -> dest iff dest == rax, else rax, <- dest\n+      \/\/ *obj_reg = lock_reg iff *obj_reg == rax, else rax, = *(obj_reg)\n+      __ lock();\n+      __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ jcc(Assembler::equal, lock_done);\n@@ -1846,8 +1851,8 @@\n-    \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-    \/\/  1) (mark & 3) == 0, and\n-    \/\/  2) rsp <= mark < mark + os::pagesize()\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 2 bits clear.\n-    \/\/ NOTE: the oopMark is in swap_reg %rax, as the result of cmpxchg\n+      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+      \/\/  1) (mark & 3) == 0, and\n+      \/\/  2) rsp <= mark < mark + os::pagesize()\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 2 bits clear.\n+      \/\/ NOTE: the oopMark is in swap_reg %rax, as the result of cmpxchg\n@@ -1855,2 +1860,2 @@\n-    __ subptr(swap_reg, rsp);\n-    __ andptr(swap_reg, 3 - os::vm_page_size());\n+      __ subptr(swap_reg, rsp);\n+      __ andptr(swap_reg, 3 - os::vm_page_size());\n@@ -1858,3 +1863,4 @@\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-    __ jcc(Assembler::notEqual, slow_path_lock);\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+      __ jcc(Assembler::notEqual, slow_path_lock);\n+    }\n@@ -2000,1 +2006,2 @@\n-    \/\/ Simple recursive lock?\n+    if (!UseFastLocking) {\n+      \/\/ Simple recursive lock?\n@@ -2002,2 +2009,3 @@\n-    __ cmpptr(Address(rbp, lock_slot_rbp_offset), (int32_t)NULL_WORD);\n-    __ jcc(Assembler::equal, done);\n+      __ cmpptr(Address(rbp, lock_slot_rbp_offset), (int32_t)NULL_WORD);\n+      __ jcc(Assembler::equal, done);\n+    }\n@@ -2010,12 +2018,18 @@\n-    \/\/  get old displaced header\n-    __ movptr(rbx, Address(rbp, lock_slot_rbp_offset));\n-\n-    \/\/ get address of the stack lock\n-    __ lea(rax, Address(rbp, lock_slot_rbp_offset));\n-\n-    \/\/ Atomic swap old header if oop still contains the stack lock\n-    \/\/ src -> dest iff dest == rax, else rax, <- dest\n-    \/\/ *obj_reg = rbx, iff *obj_reg == rax, else rax, = *(obj_reg)\n-    __ lock();\n-    __ cmpxchgptr(rbx, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    __ jcc(Assembler::notEqual, slow_path_unlock);\n+    if (UseFastLocking) {\n+      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+      __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+    } else {\n+      \/\/  get old displaced header\n+      __ movptr(rbx, Address(rbp, lock_slot_rbp_offset));\n+\n+      \/\/ get address of the stack lock\n+      __ lea(rax, Address(rbp, lock_slot_rbp_offset));\n+\n+      \/\/ Atomic swap old header if oop still contains the stack lock\n+      \/\/ src -> dest iff dest == rax, else rax, <- dest\n+      \/\/ *obj_reg = rbx, iff *obj_reg == rax, else rax, = *(obj_reg)\n+      __ lock();\n+      __ cmpxchgptr(rbx, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ jcc(Assembler::notEqual, slow_path_unlock);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":57,"deletions":43,"binary":false,"changes":100,"status":"modified"},{"patch":"@@ -2075,3 +2075,8 @@\n-    if (UseBiasedLocking) {\n-      __ biased_locking_enter(lock_reg, obj_reg, swap_reg, rscratch1, rscratch2, false, lock_done, &slow_path_lock);\n-    }\n+    if (UseFastLocking) {\n+      \/\/ Load object header\n+      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ fast_lock_impl(obj_reg, swap_reg, r15_thread, rscratch1, slow_path_lock);\n+    } else {\n+      if (UseBiasedLocking) {\n+        __ biased_locking_enter(lock_reg, obj_reg, swap_reg, rscratch1, rscratch2, false, lock_done, &slow_path_lock);\n+      }\n@@ -2079,2 +2084,2 @@\n-    \/\/ Load immediate 1 into swap_reg %rax\n-    __ movl(swap_reg, 1);\n+      \/\/ Load immediate 1 into swap_reg %rax\n+      __ movl(swap_reg, 1);\n@@ -2082,2 +2087,2 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg %rax\n-    __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      \/\/ Load (object->mark() | 1) into swap_reg %rax\n+      __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -2085,2 +2090,2 @@\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n@@ -2088,4 +2093,4 @@\n-    \/\/ src -> dest iff dest == rax else rax <- dest\n-    __ lock();\n-    __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    __ jcc(Assembler::equal, lock_done);\n+      \/\/ src -> dest iff dest == rax else rax <- dest\n+      __ lock();\n+      __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ jcc(Assembler::equal, lock_done);\n@@ -2093,1 +2098,1 @@\n-    \/\/ Hmm should this move to the slow path code area???\n+      \/\/ Hmm should this move to the slow path code area???\n@@ -2095,8 +2100,8 @@\n-    \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-    \/\/  1) (mark & 3) == 0, and\n-    \/\/  2) rsp <= mark < mark + os::pagesize()\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 2 bits clear.\n-    \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n+      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+      \/\/  1) (mark & 3) == 0, and\n+      \/\/  2) rsp <= mark < mark + os::pagesize()\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 2 bits clear.\n+      \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n@@ -2104,2 +2109,2 @@\n-    __ subptr(swap_reg, rsp);\n-    __ andptr(swap_reg, 3 - os::vm_page_size());\n+      __ subptr(swap_reg, rsp);\n+      __ andptr(swap_reg, 3 - os::vm_page_size());\n@@ -2107,3 +2112,4 @@\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-    __ jcc(Assembler::notEqual, slow_path_lock);\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+      __ jcc(Assembler::notEqual, slow_path_lock);\n+    }\n@@ -2234,1 +2240,2 @@\n-    \/\/ Simple recursive lock?\n+    if (!UseFastLocking) {\n+      \/\/ Simple recursive lock?\n@@ -2236,2 +2243,3 @@\n-    __ cmpptr(Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size), (int32_t)NULL_WORD);\n-    __ jcc(Assembler::equal, done);\n+      __ cmpptr(Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size), (int32_t)NULL_WORD);\n+      __ jcc(Assembler::equal, done);\n+    }\n@@ -2244,10 +2252,15 @@\n-\n-    \/\/ get address of the stack lock\n-    __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-    \/\/  get old displaced header\n-    __ movptr(old_hdr, Address(rax, 0));\n-\n-    \/\/ Atomic swap old header if oop still contains the stack lock\n-    __ lock();\n-    __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    __ jcc(Assembler::notEqual, slow_path_unlock);\n+    if (UseFastLocking) {\n+      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+      __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+    } else {\n+      \/\/ get address of the stack lock\n+      __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+      \/\/  get old displaced header\n+      __ movptr(old_hdr, Address(rax, 0));\n+\n+      \/\/ Atomic swap old header if oop still contains the stack lock\n+      __ lock();\n+      __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ jcc(Assembler::notEqual, slow_path_unlock);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":53,"deletions":40,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -6804,0 +6804,52 @@\n+  \/\/ Call runtime to ensure lock-stack size.\n+  \/\/ Arguments:\n+  \/\/ - c_rarg0: the required _limit pointer\n+  address generate_check_lock_stack() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"check_lock_stack\");\n+    address start = __ pc();\n+\n+    BLOCK_COMMENT(\"Entry:\");\n+    __ enter(); \/\/ save rbp\n+\n+    __ pusha();\n+\n+    \/\/ The method may have floats as arguments, and we must spill them before calling\n+    \/\/ the VM runtime.\n+    __ push_FPU_state();\n+    \/*\n+    assert(Argument::n_float_register_parameters_j == 8, \"Assumption\");\n+    const int xmm_size = wordSize * 2;\n+    const int xmm_spill_size = xmm_size * Argument::n_float_register_parameters_j;\n+    __ subptr(rsp, xmm_spill_size);\n+    __ movdqu(Address(rsp, xmm_size * 7), xmm7);\n+    __ movdqu(Address(rsp, xmm_size * 6), xmm6);\n+    __ movdqu(Address(rsp, xmm_size * 5), xmm5);\n+    __ movdqu(Address(rsp, xmm_size * 4), xmm4);\n+    __ movdqu(Address(rsp, xmm_size * 3), xmm3);\n+    __ movdqu(Address(rsp, xmm_size * 2), xmm2);\n+    __ movdqu(Address(rsp, xmm_size * 1), xmm1);\n+    __ movdqu(Address(rsp, xmm_size * 0), xmm0);\n+    *\/\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<void (*)(oop*)>(LockStack::ensure_lock_stack_size)), rax);\n+    \/*\n+    __ movdqu(xmm0, Address(rsp, xmm_size * 0));\n+    __ movdqu(xmm1, Address(rsp, xmm_size * 1));\n+    __ movdqu(xmm2, Address(rsp, xmm_size * 2));\n+    __ movdqu(xmm3, Address(rsp, xmm_size * 3));\n+    __ movdqu(xmm4, Address(rsp, xmm_size * 4));\n+    __ movdqu(xmm5, Address(rsp, xmm_size * 5));\n+    __ movdqu(xmm6, Address(rsp, xmm_size * 6));\n+    __ movdqu(xmm7, Address(rsp, xmm_size * 7));\n+    __ addptr(rsp, xmm_spill_size);\n+    *\/\n+    __ pop_FPU_state();\n+    __ popa();\n+\n+    __ leave();\n+\n+    __ ret(0);\n+\n+    return start;\n+  }\n+\n@@ -7724,0 +7776,3 @@\n+    if (UseFastLocking) {\n+      StubRoutines::x86::_check_lock_stack = generate_check_lock_stack();\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":55,"deletions":0,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+address StubRoutines::x86::_check_lock_stack = NULL;\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -134,0 +134,2 @@\n+  static address _check_lock_stack;\n+\n@@ -269,0 +271,2 @@\n+  static address check_lock_stack() { return _check_lock_stack; }\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3660,1 +3660,0 @@\n-  __ null_check(recv, oopDesc::klass_offset_in_bytes());\n@@ -3662,1 +3661,1 @@\n-  __ load_klass(rax, recv, tmp_load_klass);\n+  __ load_klass(rax, recv, tmp_load_klass, true);\n@@ -3753,1 +3752,0 @@\n-  __ null_check(rcx, oopDesc::klass_offset_in_bytes());\n@@ -3755,1 +3753,1 @@\n-  __ load_klass(rlocals, rcx, tmp_load_klass);\n+  __ load_klass(rlocals, rcx, tmp_load_klass, true);\n@@ -3777,2 +3775,1 @@\n-  __ null_check(rcx, oopDesc::klass_offset_in_bytes());\n-  __ load_klass(rdx, rcx, tmp_load_klass);\n+  __ load_klass(rdx, rcx, tmp_load_klass, true);\n@@ -4026,12 +4023,5 @@\n-    if (UseBiasedLocking) {\n-      __ pop(rcx);   \/\/ get saved klass back in the register.\n-      __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));\n-      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()), rbx);\n-    } else {\n-      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()),\n-                (intptr_t)markWord::prototype().value()); \/\/ header\n-      __ pop(rcx);   \/\/ get saved klass back in the register.\n-    }\n-#ifdef _LP64\n-    __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-    __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n+    __ pop(rcx);   \/\/ get saved klass back in the register.\n+    __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));\n+    __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()), rbx);\n+#ifndef _LP64\n+    __ store_klass(rax, rcx);  \/\/ klass\n@@ -4039,2 +4029,0 @@\n-    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n-    __ store_klass(rax, rcx, tmp_store_klass);  \/\/ klass\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":8,"deletions":20,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -630,1 +630,2 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, C->in_24_bit_fp_mode(), C->stub_function() != NULL);\n+  int max_monitors = C->method() != NULL ? C->max_monitors() : 0;\n+  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, C->in_24_bit_fp_mode(), C->stub_function() != NULL, max_monitors);\n@@ -724,1 +725,3 @@\n-      code_stub = &C->output()->safepoint_poll_table()->add_safepoint(__ offset());\n+      C2SafepointPollStub* stub = new (C->comp_arena()) C2SafepointPollStub(__ offset());\n+      C->output()->add_stub(stub);\n+      code_stub = &stub->entry();\n@@ -13697,1 +13700,1 @@\n-instruct cmpFastLockRTM(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eDXRegI scr, rRegI cx1, rRegI cx2) %{\n+instruct cmpFastLockRTM(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eDXRegI scr, rRegI cx1, rRegI cx2, eRegP thread) %{\n@@ -13700,1 +13703,1 @@\n-  effect(TEMP tmp, TEMP scr, TEMP cx1, TEMP cx2, USE_KILL box);\n+  effect(TEMP tmp, TEMP scr, TEMP cx1, TEMP cx2, USE_KILL box, TEMP thread);\n@@ -13704,0 +13707,1 @@\n+    __ get_thread($thread$$Register);\n@@ -13705,1 +13709,1 @@\n-                 $scr$$Register, $cx1$$Register, $cx2$$Register,\n+                 $scr$$Register, $cx1$$Register, $cx2$$Register, $thread$$Register,\n@@ -13713,1 +13717,1 @@\n-instruct cmpFastLock(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eRegP scr) %{\n+instruct cmpFastLock(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eRegP scr, eRegP thread) %{\n@@ -13716,1 +13720,1 @@\n-  effect(TEMP tmp, TEMP scr, USE_KILL box);\n+  effect(TEMP tmp, TEMP scr, USE_KILL box, TEMP thread);\n@@ -13720,0 +13724,1 @@\n+    __ get_thread($thread$$Register);\n@@ -13721,1 +13726,1 @@\n-                 $scr$$Register, noreg, noreg, _counters, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, noreg, noreg, $thread$$Register, NULL, NULL, NULL, NULL, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":13,"deletions":8,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -903,1 +903,2 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != NULL);\n+  int max_monitors = C->method() != NULL ? C->max_monitors() : 0;\n+  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != NULL, max_monitors);\n@@ -1001,1 +1002,3 @@\n-      code_stub = &C->output()->safepoint_poll_table()->add_safepoint(__ offset());\n+      C2SafepointPollStub* stub = new (C->comp_arena()) C2SafepointPollStub(__ offset());\n+      C->output()->add_stub(stub);\n+      code_stub = &stub->entry();\n@@ -5191,1 +5194,1 @@\n-instruct loadNKlass(rRegN dst, memory mem)\n+instruct loadNKlass(rRegN dst, indOffset8 mem, rFlagsReg cr)\n@@ -5194,1 +5197,1 @@\n-\n+  effect(TEMP_DEF dst, KILL cr);\n@@ -5198,1 +5201,11 @@\n-    __ movl($dst$$Register, $mem$$Address);\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset 4, but got: %d\", $mem$$disp);\n+    assert($mem$$index == 4, \"expect no index register: %d\", $mem$$index);\n+    Register dst = $dst$$Register;\n+    Register obj = $mem$$base$$Register;\n+    C2LoadNKlassStub* stub = new (Compile::current()->comp_arena()) C2LoadNKlassStub(dst);\n+    Compile::current()->output()->add_stub(stub);\n+    __ movq(dst, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    __ testb(dst, markWord::monitor_value);\n+    __ jcc(Assembler::notZero, stub->entry());\n+    __ bind(stub->continuation());\n+    __ shrq(dst, markWord::klass_shift);\n@@ -5200,1 +5213,1 @@\n-  ins_pipe(ialu_reg_mem); \/\/ XXX\n+  ins_pipe(pipe_slow); \/\/ XXX\n@@ -12225,0 +12238,3 @@\n+\/\/ Disabled because the compressed Klass* in header cannot be safely\n+\/\/ accessed. TODO: Re-enable it as soon as synchronization does not\n+\/\/ overload the upper header bits anymore.\n@@ -12227,0 +12243,1 @@\n+  predicate(false);\n@@ -12925,1 +12942,1 @@\n-                 $scr$$Register, $cx1$$Register, $cx2$$Register,\n+                 $scr$$Register, $cx1$$Register, $cx2$$Register, r15_thread,\n@@ -12941,1 +12958,1 @@\n-                 $scr$$Register, $cx1$$Register, noreg, _counters, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, $cx1$$Register, noreg, r15_thread, NULL, NULL, NULL, NULL, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":25,"deletions":8,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -564,0 +564,19 @@\n+class LoadKlassStub: public CodeStub {\n+private:\n+  LIR_Opr          _obj;\n+  LIR_Opr          _result;\n+\n+public:\n+  LoadKlassStub(LIR_Opr obj, LIR_Opr result) :\n+    CodeStub(), _obj(obj), _result(result) {};\n+\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_input(_obj);\n+    visitor->do_output(_result);\n+  }\n+#ifndef PRODUCT\n+virtual void print_name(outputStream* out) const { out->print(\"LoadKlassStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -385,0 +385,4 @@\n+  if (method()->is_synchronized()) {\n+    push_monitor();\n+  }\n+\n@@ -561,0 +565,1 @@\n+, _max_monitors(0)\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+  int                _max_monitors; \/\/ Max number of active monitors, for fast-locking\n@@ -140,0 +141,1 @@\n+  int max_monitors() const                       { return _max_monitors; }\n@@ -169,0 +171,2 @@\n+  void push_monitor()                            { _max_monitors++; }\n+\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2239,0 +2239,1 @@\n+  compilation()->push_monitor();\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -883,0 +883,14 @@\n+\/\/ LIR_OpLoadKlass\n+    case lir_load_klass:\n+    {\n+      LIR_OpLoadKlass* opLoadKlass = op->as_OpLoadKlass();\n+      assert(opLoadKlass != NULL, \"must be\");\n+\n+      do_input(opLoadKlass->_obj);\n+      do_output(opLoadKlass->_result);\n+      do_stub(opLoadKlass->_stub);\n+      if (opLoadKlass->_info) do_info(opLoadKlass->_info);\n+      break;\n+    }\n+\n+\n@@ -1052,0 +1066,5 @@\n+void LIR_OpLoadKlass::emit_code(LIR_Assembler* masm) {\n+  masm->emit_load_klass(this);\n+  masm->append_code_stub(stub());\n+}\n+\n@@ -1976,0 +1995,6 @@\n+void LIR_OpLoadKlass::print_instr(outputStream* out) const {\n+  obj()->print(out);        out->print(\" \");\n+  result_opr()->print(out); out->print(\" \");\n+  out->print(\"[lbl:\" INTPTR_FORMAT \"]\", p2i(stub()->entry()));\n+}\n+\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -880,0 +880,1 @@\n+class    LIR_OpLoadKlass;\n@@ -925,0 +926,1 @@\n+      , lir_load_klass\n@@ -1135,0 +1137,1 @@\n+  virtual LIR_OpLoadKlass* as_OpLoadKlass() { return NULL; }\n@@ -1807,0 +1810,19 @@\n+class LIR_OpLoadKlass: public LIR_Op {\n+  friend class LIR_OpVisitState;\n+\n+ private:\n+  LIR_Opr _obj;\n+  CodeStub* _stub;\n+ public:\n+  LIR_OpLoadKlass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info, CodeStub* stub)\n+    : LIR_Op(lir_load_klass, result, info)\n+    , _obj(obj)\n+    , _stub(stub) {}\n+\n+  LIR_Opr obj()    const { return _obj;  }\n+  CodeStub* stub() const { return _stub; }\n+\n+  virtual LIR_OpLoadKlass* as_OpLoadKlass() { return this; }\n+  virtual void emit_code(LIR_Assembler* masm);\n+  void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n@@ -2252,0 +2274,3 @@\n+\n+  void load_klass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info, CodeStub* stub) { append(new LIR_OpLoadKlass(obj, result, info, stub)); }\n+\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -761,1 +761,1 @@\n-  _masm->build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  _masm->build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -200,0 +200,1 @@\n+  void emit_load_klass(LIR_OpLoadKlass* op);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -639,1 +639,1 @@\n-  CodeStub* slow_path = new MonitorExitStub(lock, UseFastLocking, monitor_no);\n+  CodeStub* slow_path = new MonitorExitStub(lock, !UseHeavyMonitors, monitor_no);\n@@ -1258,0 +1258,5 @@\n+void LIRGenerator::load_klass(LIR_Opr obj, LIR_Opr klass, CodeEmitInfo* null_check_info) {\n+  CodeStub* slow_path = new LoadKlassStub(obj, klass);\n+  __ load_klass(obj, klass, null_check_info, slow_path);\n+}\n+\n@@ -1264,1 +1269,1 @@\n-  LIR_Opr temp = new_register(T_METADATA);\n+  LIR_Opr temp = new_register(T_ADDRESS);\n@@ -1273,4 +1278,3 @@\n-  \/\/ FIXME T_ADDRESS should actually be T_METADATA but it can't because the\n-  \/\/ meaning of these two is mixed up (see JDK-8026837).\n-  __ move(new LIR_Address(rcvr.result(), oopDesc::klass_offset_in_bytes(), T_ADDRESS), temp, info);\n-  __ move_wide(new LIR_Address(temp, in_bytes(Klass::java_mirror_offset()), T_ADDRESS), temp);\n+  LIR_Opr klass = new_register(T_METADATA);\n+  load_klass(rcvr.result(), klass, info);\n+  __ move_wide(new LIR_Address(klass, in_bytes(Klass::java_mirror_offset()), T_ADDRESS), temp);\n@@ -1356,1 +1360,1 @@\n-  __ move(new LIR_Address(value.result(), oopDesc::klass_offset_in_bytes(), T_ADDRESS), klass, NULL);\n+  load_klass(value.result(), klass, NULL);\n@@ -3744,1 +3748,1 @@\n-  __ move(new LIR_Address(array, oopDesc::klass_offset_in_bytes(), T_ADDRESS), klass, null_check_info);\n+  load_klass(array, klass, null_check_info);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":12,"deletions":8,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -242,0 +242,2 @@\n+  void load_klass(LIR_Opr obj, LIR_Opr klass, CodeEmitInfo* null_check_info);\n+\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-  void build_frame(int frame_size_in_bytes, int bang_size_in_bytes);\n+  void build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int max_monitors);\n","filename":"src\/hotspot\/share\/c1\/c1_MacroAssembler.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -702,1 +702,1 @@\n-  if (!UseFastLocking) {\n+  if (UseHeavyMonitors) {\n@@ -705,2 +705,2 @@\n-  assert(obj == lock->obj(), \"must match\");\n-  SharedRuntime::monitor_enter_helper(obj, lock->lock(), current);\n+  assert(UseFastLocking || obj == lock->obj(), \"must match\");\n+  SharedRuntime::monitor_enter_helper(obj, UseFastLocking ? NULL : lock->lock(), current);\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -251,3 +251,0 @@\n-  develop(bool, UseFastLocking, true,                                       \\\n-          \"Use fast inlined locking code\")                                  \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/c1\/c1_globals.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/klass.inline.hpp\"\n@@ -734,0 +735,7 @@\n+    Klass* requested_k = to_requested(k);\n+#ifdef _LP64\n+    narrowKlass nk = CompressedKlassPointers::encode_not_null(requested_k, _requested_static_archive_bottom);\n+    k->set_prototype_header(markWord::prototype().set_narrow_klass(nk));\n+#else\n+    k->set_prototype_header(markWord::prototype());\n+#endif\n@@ -779,1 +787,3 @@\n-  o->set_narrow_klass(nk);\n+#ifdef _LP64\n+  o->set_mark(o->mark().set_narrow_klass(nk));\n+#endif\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"runtime\/safepoint.hpp\"\n@@ -278,1 +279,8 @@\n-    archived_oop->set_mark(markWord::prototype().copy_set_hash(hash_original));\n+\n+    assert(SafepointSynchronize::is_at_safepoint(), \"resolving displaced headers only at safepoint\");\n+    markWord mark = obj->mark();\n+    if (mark.has_displaced_mark_helper()) {\n+      mark = mark.displaced_mark_helper();\n+    }\n+    narrowKlass nklass = mark.narrow_klass();\n+    archived_oop->set_mark(markWord::prototype().copy_set_hash(hash_original) LP64_ONLY(.set_narrow_klass(nklass)));\n@@ -419,5 +427,2 @@\n-    if (UseBiasedLocking) {\n-      oopDesc::set_mark(mem, k->prototype_header());\n-    } else {\n-      oopDesc::set_mark(mem, markWord::prototype());\n-    }\n+    oopDesc::set_mark(mem, k->prototype_header());\n+#ifndef _LP64\n@@ -425,0 +430,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":12,"deletions":6,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -1606,0 +1607,2 @@\n+  _forwarding = new SlidingForwarding(heap_rs.region(), HeapRegion::LogOfHRGrainBytes - LogHeapWordSize);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -90,0 +90,1 @@\n+class SlidingForwarding;\n@@ -240,0 +241,2 @@\n+  SlidingForwarding* _forwarding;\n+\n@@ -269,0 +272,4 @@\n+  SlidingForwarding* forwarding() const {\n+    return _forwarding;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -311,0 +312,3 @@\n+\n+  _heap->forwarding()->clear();\n+\n@@ -314,0 +318,1 @@\n+  \/\/ TODO: Disabled for now because it violates sliding-forwarding assumption.\n@@ -315,3 +320,3 @@\n-  if (!task.has_freed_regions()) {\n-    task.prepare_serial_compaction();\n-  }\n+  \/\/ if (!task.has_freed_regions()) {\n+  \/\/   task.prepare_serial_compaction();\n+  \/\/ }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -98,1 +98,2 @@\n-  marker->preserved_stack()->adjust_during_full_gc();\n+  const SlidingForwarding* const forwarding = G1CollectedHeap::heap()->forwarding();\n+  marker->preserved_stack()->adjust_during_full_gc(forwarding);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCAdjustTask.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -63,2 +64,1 @@\n-  HeapWord* destination = cast_from_oop<HeapWord*>(obj->forwardee());\n-  if (destination == NULL) {\n+  if (!obj->is_forwarded()) {\n@@ -69,0 +69,2 @@\n+  HeapWord* destination = cast_from_oop<HeapWord*>(_forwarding->forwardee(obj));\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+class SlidingForwarding;\n@@ -53,0 +54,1 @@\n+    const SlidingForwarding* const _forwarding;\n@@ -55,1 +57,3 @@\n-    G1CompactRegionClosure(G1CMBitMap* bitmap) : _bitmap(bitmap) { }\n+    G1CompactRegionClosure(G1CMBitMap* bitmap) :\n+      _bitmap(bitmap),\n+      _forwarding(G1CollectedHeap::heap()->forwarding()) { }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -96,1 +97,1 @@\n-void G1FullGCCompactionPoint::forward(oop object, size_t size) {\n+void G1FullGCCompactionPoint::forward(SlidingForwarding* const forwarding, oop object, size_t size) {\n@@ -106,1 +107,1 @@\n-    object->forward_to(cast_to_oop(_compaction_top));\n+    forwarding->forward_to(object, cast_to_oop(_compaction_top));\n@@ -108,0 +109,2 @@\n+    assert(!object->is_forwarded(), \"should not be forwarded\");\n+    \/*\n@@ -126,0 +129,1 @@\n+    *\/\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+class SlidingForwarding;\n@@ -54,1 +55,1 @@\n-  void forward(oop object, size_t size);\n+  void forward(SlidingForwarding* const forwarding, oop object, size_t size);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/g1\/g1CollectedHeap.hpp\"\n@@ -35,0 +36,1 @@\n+class SlidingForwarding;\n@@ -82,0 +84,1 @@\n+  const SlidingForwarding* const _forwarding;\n@@ -85,1 +88,3 @@\n-  G1AdjustClosure(G1FullCollector* collector) : _collector(collector) { }\n+  G1AdjustClosure(G1FullCollector* collector) :\n+    _collector(collector),\n+    _forwarding(G1CollectedHeap::heap()->forwarding()) { }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -80,2 +81,1 @@\n-  oop forwardee = obj->forwardee();\n-  if (forwardee == NULL) {\n+  if (!obj->is_forwarded()) {\n@@ -83,0 +83,1 @@\n+    \/*\n@@ -88,0 +89,1 @@\n+    *\/\n@@ -92,0 +94,1 @@\n+  oop forwardee = _forwarding->forwardee(obj);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.inline.hpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -160,1 +161,1 @@\n-    _cp(cp) { }\n+    _cp(cp), _forwarding(G1CollectedHeap::heap()->forwarding()) { }\n@@ -164,1 +165,1 @@\n-  _cp->forward(object, size);\n+  _cp->forward(_forwarding, object, size);\n@@ -169,0 +170,1 @@\n+  ShouldNotReachHere();\n@@ -171,0 +173,1 @@\n+  \/*\n@@ -178,1 +181,1 @@\n-  _cp->forward(obj, size);\n+  _cp->forward(_forwarding, obj, size);\n@@ -181,0 +184,2 @@\n+  *\/\n+  return 0;\n@@ -201,1 +206,2 @@\n-  GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Prepare Serial Compaction\", collector()->scope()->timer());\n+  ShouldNotReachHere(); \/\/ Disabled in Lilliput.\n+  \/\/ GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Prepare Serial Compaction\", collector()->scope()->timer());\n@@ -206,0 +212,1 @@\n+  \/*\n@@ -212,0 +219,1 @@\n+  *\/\n@@ -215,0 +223,1 @@\n+  \/*\n@@ -230,0 +239,1 @@\n+  *\/\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.cpp","additions":14,"deletions":4,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+class SlidingForwarding;\n@@ -79,0 +80,1 @@\n+    SlidingForwarding* const _forwarding;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -237,1 +237,1 @@\n-      forwardee = cast_to_oop(m.decode_pointer());\n+      forwardee = obj->forwardee(m);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1OopClosures.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -200,1 +200,1 @@\n-    obj = cast_to_oop(m.decode_pointer());\n+    obj = obj->forwardee(m);\n@@ -221,1 +221,0 @@\n-  assert(from_obj->is_objArray(), \"must be obj array\");\n@@ -251,1 +250,0 @@\n-  assert(from_obj->is_objArray(), \"precondition\");\n@@ -370,1 +368,1 @@\n-                                                  oop const old, size_t word_sz, uint age,\n+                                                  oop const old, Klass* klass, size_t word_sz, uint age,\n@@ -374,1 +372,1 @@\n-    _g1h->_gc_tracer_stw->report_promotion_in_new_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->_gc_tracer_stw->report_promotion_in_new_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -378,1 +376,1 @@\n-    _g1h->_gc_tracer_stw->report_promotion_outside_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->_gc_tracer_stw->report_promotion_outside_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -386,0 +384,1 @@\n+                                                   Klass* klass,\n@@ -408,1 +407,1 @@\n-      report_promotion_event(*dest_attr, old, word_sz, age, obj_ptr, node_index);\n+      report_promotion_event(*dest_attr, old, klass, word_sz, age, obj_ptr, node_index);\n@@ -431,0 +430,4 @@\n+  if (old_mark.is_marked()) {\n+    \/\/ Already forwarded by somebody else, return forwardee.\n+    return old->forwardee(old_mark);\n+  }\n@@ -433,0 +436,3 @@\n+#ifdef _LP64\n+  Klass* klass = old_mark.safe_klass();\n+#else\n@@ -434,0 +440,1 @@\n+#endif\n@@ -446,1 +453,1 @@\n-    obj_ptr = allocate_copy_slow(&dest_attr, old, word_sz, age, node_index);\n+    obj_ptr = allocate_copy_slow(&dest_attr, old, klass, word_sz, age, node_index);\n@@ -603,1 +610,1 @@\n-  oop forward_ptr = old->forward_to_atomic(old, m, memory_order_relaxed);\n+  oop forward_ptr = old->forward_to_self_atomic(m, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":16,"deletions":9,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -170,0 +170,1 @@\n+                               Klass* klass,\n@@ -202,1 +203,1 @@\n-                              oop const old, size_t word_sz, uint age,\n+                              oop const old, Klass* klass, size_t word_sz, uint age,\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -863,1 +863,2 @@\n-void HeapRegion::object_iterate(ObjectClosure* blk) {\n+template<bool RESOLVE>\n+void HeapRegion::object_iterate_impl(ObjectClosure* blk) {\n@@ -869,1 +870,9 @@\n-    p += block_size(p);\n+    p += block_size<RESOLVE>(p);\n+  }\n+}\n+\n+void HeapRegion::object_iterate(ObjectClosure* blk) {\n+  if (G1CollectedHeap::heap()->collector_state()->in_full_gc()) {\n+    object_iterate_impl<false>(blk);\n+  } else {\n+    object_iterate_impl<true>(blk);\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.cpp","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -148,0 +148,3 @@\n+  template<bool RESOLVE>\n+  void object_iterate_impl(ObjectClosure* blk);\n+\n@@ -186,0 +189,1 @@\n+  template<bool RESOLVE = false>\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -180,0 +180,1 @@\n+template <bool RESOLVE>\n@@ -186,1 +187,14 @@\n-    return cast_to_oop(addr)->size();\n+    oop obj = cast_to_oop(addr);\n+#ifdef _LP64\n+#ifdef ASSERT\n+    if (RESOLVE) {\n+      assert(!G1CollectedHeap::heap()->collector_state()->in_full_gc(), \"Illegal\/excessive resolve during full-GC\");\n+    } else {\n+      assert(G1CollectedHeap::heap()->collector_state()->in_full_gc() || !obj->is_forwarded(), \"Missing resolve when forwarded during normal GC\");\n+    }\n+#endif\n+    if (RESOLVE && obj->is_forwarded()) {\n+      obj = obj->forwardee();\n+    }\n+#endif\n+    return obj->size();\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.inline.hpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -112,1 +112,1 @@\n-              touched_words = MIN2((size_t)align_object_size(typeArrayOopDesc::header_size(T_INT)),\n+              touched_words = MIN2((size_t)align_object_size(align_up(typeArrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize),\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableNUMASpace.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -411,1 +411,1 @@\n-    HeapWord* test_addr = cast_from_oop<HeapWord*>(obj) + 1;\n+    HeapWord* test_addr = cast_from_oop<HeapWord*>(obj);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psOldGen.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-  filler_header_size = align_object_size(typeArrayOopDesc::header_size(T_INT));\n+  filler_header_size = align_object_size((arrayOopDesc::base_offset_in_bytes(T_INT) + BytesPerWord) \/ BytesPerWord);\n@@ -86,0 +86,3 @@\n+#ifdef _LP64\n+  filler_oop->set_mark(Universe::intArrayKlassObj()->prototype_header());\n+#else\n@@ -88,0 +91,2 @@\n+#endif\n+  int header_size = (arrayOopDesc::base_offset_in_bytes(T_INT) + BytesPerWord) \/ BytesPerWord;\n@@ -89,1 +94,1 @@\n-    pointer_delta(tlab_end, top()) - typeArrayOopDesc::header_size(T_INT);\n+    pointer_delta(tlab_end, top()) - header_size;\n@@ -95,1 +100,1 @@\n-  HeapWord* elt_words = cast_from_oop<HeapWord*>(filler_oop) + typeArrayOopDesc::header_size(T_INT);\n+  HeapWord* elt_words = cast_from_oop<HeapWord*>(filler_oop) + header_size;\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionLAB.cpp","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -317,1 +317,0 @@\n-  assert(old->is_objArray(), \"invariant\");\n@@ -355,1 +354,1 @@\n-  if (obj->cas_forward_to(obj, obj_mark)) {\n+  if (obj->forward_to_self_atomic(obj_mark) == NULL) {\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -107,1 +107,1 @@\n-  inline void promotion_trace_event(oop new_obj, oop old_obj, size_t obj_size,\n+  inline void promotion_trace_event(oop new_obj, oop old_obj, Klass* klass, size_t obj_size,\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -63,1 +63,1 @@\n-inline void PSPromotionManager::promotion_trace_event(oop new_obj, oop old_obj,\n+inline void PSPromotionManager::promotion_trace_event(oop new_obj, oop old_obj, Klass* klass,\n@@ -76,1 +76,1 @@\n-        gc_tracer->report_promotion_in_new_plab_event(old_obj->klass(), obj_bytes,\n+        gc_tracer->report_promotion_in_new_plab_event(klass, obj_bytes,\n@@ -83,1 +83,1 @@\n-        gc_tracer->report_promotion_outside_plab_event(old_obj->klass(), obj_bytes,\n+        gc_tracer->report_promotion_outside_plab_event(klass, obj_bytes,\n@@ -147,1 +147,1 @@\n-    return cast_to_oop(m.decode_pointer());\n+    return o->forwardee(m);\n@@ -163,1 +163,6 @@\n-  size_t new_obj_size = o->size();\n+#ifdef _LP64\n+  Klass* klass = test_mark.safe_klass();\n+#else\n+  Klass* klass = o->klass();\n+#endif\n+  size_t new_obj_size = o->size_given_klass(klass);\n@@ -178,1 +183,1 @@\n-          promotion_trace_event(new_obj, o, new_obj_size, age, false, NULL);\n+          promotion_trace_event(new_obj, o, klass, new_obj_size, age, false, NULL);\n@@ -188,1 +193,1 @@\n-            promotion_trace_event(new_obj, o, new_obj_size, age, false, &_young_lab);\n+            promotion_trace_event(new_obj, o, klass, new_obj_size, age, false, &_young_lab);\n@@ -214,1 +219,1 @@\n-          promotion_trace_event(new_obj, o, new_obj_size, age, true, NULL);\n+          promotion_trace_event(new_obj, o, klass, new_obj_size, age, true, NULL);\n@@ -231,1 +236,1 @@\n-            promotion_trace_event(new_obj, o, new_obj_size, age, true, &_old_lab);\n+            promotion_trace_event(new_obj, o, klass, new_obj_size, age, true, &_old_lab);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.inline.hpp","additions":14,"deletions":9,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -683,1 +683,1 @@\n-  _preserved_marks_set.get()->push_if_necessary(old, old->mark());\n+  \/\/_preserved_marks_set.get()->push_if_necessary(old, old->mark());\n@@ -685,1 +685,1 @@\n-  old->forward_to(old);\n+  old->forward_to_self();\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -274,0 +274,2 @@\n+  AdjustPointerClosure adjust_pointer_closure(gch->forwarding());\n+  CLDToOopClosure adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_strong);\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/genCollectedHeap.hpp\"\n@@ -65,1 +66,0 @@\n-CLDToOopClosure    MarkSweep::adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_strong);\n@@ -147,2 +147,2 @@\n-void PreservedMark::adjust_pointer() {\n-  MarkSweep::adjust_pointer(&_obj);\n+void PreservedMark::adjust_pointer(const SlidingForwarding* const forwarding) {\n+  MarkSweep::adjust_pointer(forwarding, &_obj);\n@@ -176,2 +176,0 @@\n-AdjustPointerClosure MarkSweep::adjust_pointer_closure;\n-\n@@ -179,0 +177,2 @@\n+  const SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n+\n@@ -184,1 +184,1 @@\n-    _preserved_marks[i].adjust_pointer();\n+    _preserved_marks[i].adjust_pointer(forwarding);\n@@ -191,1 +191,1 @@\n-    adjust_pointer(p);\n+    adjust_pointer(forwarding, p);\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+class SlidingForwarding;\n@@ -127,2 +128,0 @@\n-  static AdjustPointerClosure adjust_pointer_closure;\n-  static CLDToOopClosure      adjust_cld_closure;\n@@ -145,1 +144,1 @@\n-  static int adjust_pointers(oop obj);\n+  static int adjust_pointers(const SlidingForwarding* const forwarding, oop obj);\n@@ -153,1 +152,1 @@\n-  template <class T> static inline void adjust_pointer(T* p);\n+  template <class T> static inline void adjust_pointer(const SlidingForwarding* const forwarding, T* p);\n@@ -189,0 +188,2 @@\n+private:\n+  const SlidingForwarding* const _forwarding;\n@@ -190,0 +191,1 @@\n+  AdjustPointerClosure(const SlidingForwarding* forwarding) : _forwarding(forwarding) {}\n@@ -207,1 +209,1 @@\n-  void adjust_pointer();\n+  void adjust_pointer(const SlidingForwarding* const forwarding);\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.hpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -43,1 +44,1 @@\n-  obj->set_mark(markWord::prototype().set_marked());\n+  obj->set_mark(obj->klass()->prototype_header().set_marked());\n@@ -77,1 +78,1 @@\n-template <class T> inline void MarkSweep::adjust_pointer(T* p) {\n+template <class T> inline void MarkSweep::adjust_pointer(const SlidingForwarding* const forwarding, T* p) {\n@@ -83,9 +84,4 @@\n-    oop new_obj = cast_to_oop(obj->mark().decode_pointer());\n-\n-    assert(new_obj != NULL ||                      \/\/ is forwarding ptr?\n-           obj->mark() == markWord::prototype() || \/\/ not gc marked?\n-           (UseBiasedLocking && obj->mark().has_bias_pattern()),\n-           \/\/ not gc marked?\n-           \"should be forwarded\");\n-\n-    if (new_obj != NULL) {\n+    markWord header = obj->mark();\n+    if (header.is_marked()) {\n+      oop new_obj = forwarding->forwardee(obj);\n+      assert(new_obj != NULL, \"must be forwarded\");\n@@ -99,1 +95,1 @@\n-void AdjustPointerClosure::do_oop_work(T* p)           { MarkSweep::adjust_pointer(p); }\n+void AdjustPointerClosure::do_oop_work(T* p)           { MarkSweep::adjust_pointer(_forwarding, p); }\n@@ -104,2 +100,3 @@\n-inline int MarkSweep::adjust_pointers(oop obj) {\n-  return obj->oop_iterate_size(&MarkSweep::adjust_pointer_closure);\n+inline int MarkSweep::adjust_pointers(const SlidingForwarding* const forwarding, oop obj) {\n+  AdjustPointerClosure cl(forwarding);\n+  return obj->oop_iterate_size(&cl);\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.inline.hpp","additions":11,"deletions":14,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -337,1 +337,1 @@\n-      __ move(new LIR_Address(base_reg, oopDesc::klass_offset_in_bytes(), T_ADDRESS), src_klass);\n+      gen->load_klass(base_reg, src_klass, NULL);\n","filename":"src\/hotspot\/share\/gc\/shared\/c1\/barrierSetC1.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -211,4 +211,0 @@\n-  if (is_in(object->klass_or_null())) {\n-    return false;\n-  }\n-\n@@ -233,2 +229,4 @@\n-  _filler_array_max_size = align_object_size(filler_array_hdr_size() +\n-                                             max_len \/ elements_per_word);\n+  int header_size_in_bytes = arrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(header_size_in_bytes % sizeof(jint) == 0, \"must be aligned to int\");\n+  int header_size_in_ints = header_size_in_bytes \/ sizeof(jint);\n+  _filler_array_max_size = align_object_size((header_size_in_ints + max_len) \/ elements_per_word);\n@@ -394,1 +392,3 @@\n-  size_t max_int_size = typeArrayOopDesc::header_size(T_INT) +\n+  int header_size_in_bytes = typeArrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(header_size_in_bytes % sizeof(jint) == 0, \"header size must align to int\");\n+  size_t max_int_size = header_size_in_bytes \/ HeapWordSize +\n@@ -400,4 +400,0 @@\n-size_t CollectedHeap::filler_array_hdr_size() {\n-  return align_object_offset(arrayOopDesc::header_size(T_INT)); \/\/ align to Long\n-}\n-\n@@ -405,1 +401,2 @@\n-  return align_object_size(filler_array_hdr_size()); \/\/ align to MinObjAlignment\n+  int aligned_header_size_words = align_up(arrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize;\n+  return align_object_size(aligned_header_size_words); \/\/ align to MinObjAlignment\n@@ -418,2 +415,3 @@\n-    Copy::fill_to_words(start + filler_array_hdr_size(),\n-                        words - filler_array_hdr_size(), 0XDEAFBABE);\n+  int payload_start = align_up(arrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize;\n+  Copy::fill_to_words(start + payload_start,\n+                      words - payload_start, 0XDEAFBABE);\n@@ -430,2 +428,3 @@\n-  const size_t payload_size = words - filler_array_hdr_size();\n-  const size_t len = payload_size * HeapWordSize \/ sizeof(jint);\n+  const size_t payload_size_bytes = words * HeapWordSize - arrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(payload_size_bytes % sizeof(jint) == 0, \"must be int aligned\");\n+  const size_t len = payload_size_bytes \/ sizeof(jint);\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":15,"deletions":16,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -143,1 +143,0 @@\n-  static inline size_t filler_array_hdr_size();\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -124,0 +125,1 @@\n+  _forwarding = new SlidingForwarding(_reserved);\n@@ -1115,0 +1117,1 @@\n+  _forwarding->clear();\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+class SlidingForwarding;\n@@ -91,0 +92,2 @@\n+  SlidingForwarding* _forwarding;\n+\n@@ -334,0 +337,4 @@\n+  SlidingForwarding* forwarding() const {\n+    return _forwarding;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -379,1 +379,0 @@\n-  oopDesc::set_klass_gap(mem, 0);\n@@ -385,6 +384,0 @@\n-  if (UseBiasedLocking) {\n-    oopDesc::set_mark(mem, _klass->prototype_header());\n-  } else {\n-    \/\/ May be bootstrapping\n-    oopDesc::set_mark(mem, markWord::prototype());\n-  }\n@@ -394,0 +387,4 @@\n+#ifdef _LP64\n+  oopDesc::release_set_mark(mem, _klass->prototype_header());\n+#else\n+  oopDesc::set_mark(mem, _klass->prototype_header());\n@@ -395,0 +392,1 @@\n+#endif\n@@ -408,1 +406,1 @@\n-  const size_t hs = arrayOopDesc::header_size(array_klass->element_type());\n+  const size_t hs = align_up(arrayOopDesc::base_offset_in_bytes(array_klass->element_type()), HeapWordSize) \/ HeapWordSize;\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":6,"deletions":8,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -265,0 +266,1 @@\n+        ObjectMonitor::maybe_deflate_dead(ptr);\n","filename":"src\/hotspot\/share\/gc\/shared\/oopStorage.inline.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -42,0 +43,2 @@\n+\/\/ TODO: This method is unused, except in the gunit test. Change the test\n+\/\/ to exercise the updated method below instead, and remove this one.\n@@ -54,0 +57,12 @@\n+void PreservedMarks::adjust_during_full_gc(const SlidingForwarding* const forwarding) {\n+  StackIterator<OopAndMarkWord, mtGC> iter(_stack);\n+  while (!iter.is_empty()) {\n+    OopAndMarkWord* elem = iter.next_addr();\n+\n+    oop obj = elem->get_oop();\n+    if (obj->is_forwarded()) {\n+      elem->set_oop(forwarding->forwardee(obj));\n+    }\n+  }\n+}\n+\n@@ -75,0 +90,10 @@\n+#ifdef _LP64\n+    oop forwardee = obj->forwardee();\n+    markWord header = forwardee->mark();\n+    if (header.has_displaced_mark_helper()) {\n+      header = header.displaced_mark_helper();\n+    }\n+    assert(UseCompressedClassPointers, \"assume +UseCompressedClassPointers\");\n+    narrowKlass nklass = header.narrow_klass();\n+    obj->set_mark(markWord::prototype().set_narrow_klass(nklass));\n+#else\n@@ -76,0 +101,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shared\/preservedMarks.cpp","additions":26,"deletions":0,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+class SlidingForwarding;\n@@ -66,0 +67,2 @@\n+  \/\/ TODO: This method is unused, except in the gunit test. Change the test\n+  \/\/ to exercise the updated method below instead, and remove this one.\n@@ -68,0 +71,2 @@\n+  void adjust_during_full_gc(const SlidingForwarding* const forwarding);\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/preservedMarks.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,69 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n+\n+#ifdef _LP64\n+HeapWord* const SlidingForwarding::UNUSED_BASE = reinterpret_cast<HeapWord*>(0x1);\n+#endif\n+\n+SlidingForwarding::SlidingForwarding(MemRegion heap)\n+#ifdef _LP64\n+        : _heap_start(heap.start()),\n+          _num_regions(((heap.end() - heap.start()) >> NUM_COMPRESSED_BITS) + 1),\n+          _region_size_words_shift(NUM_COMPRESSED_BITS),\n+          _target_base_table(NEW_C_HEAP_ARRAY(HeapWord*, _num_regions * 2, mtGC)) {\n+  assert(_region_size_words_shift <= NUM_COMPRESSED_BITS, \"regions must not be larger than maximum addressing bits allow\");\n+#else\n+  {\n+#endif\n+}\n+\n+SlidingForwarding::SlidingForwarding(MemRegion heap, size_t region_size_words_shift)\n+#ifdef _LP64\n+        : _heap_start(heap.start()),\n+          _num_regions(((heap.end() - heap.start()) >> region_size_words_shift) + 1),\n+          _region_size_words_shift(region_size_words_shift),\n+          _target_base_table(NEW_C_HEAP_ARRAY(HeapWord*, _num_regions * (ONE << NUM_REGION_BITS), mtGC)) {\n+  assert(region_size_words_shift <= NUM_COMPRESSED_BITS, \"regions must not be larger than maximum addressing bits allow\");\n+#else\n+  {\n+#endif\n+}\n+\n+SlidingForwarding::~SlidingForwarding() {\n+#ifdef _LP64\n+  FREE_C_HEAP_ARRAY(HeapWord*, _target_base_table);\n+#endif\n+}\n+\n+void SlidingForwarding::clear() {\n+#ifdef _LP64\n+  size_t max = _num_regions * (ONE << NUM_REGION_BITS);\n+  for (size_t i = 0; i < max; i++) {\n+    _target_base_table[i] = UNUSED_BASE;\n+  }\n+#endif\n+}\n","filename":"src\/hotspot\/share\/gc\/shared\/slidingForwarding.cpp","additions":69,"deletions":0,"binary":false,"changes":69,"status":"added"},{"patch":"@@ -0,0 +1,112 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHARED_SLIDINGFORWARDING_HPP\n+#define SHARE_GC_SHARED_SLIDINGFORWARDING_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/memRegion.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+\n+\/**\n+ * SlidingForwarding is a method to store forwarding information in a compressed form into the object header,\n+ * that has been specifically designed for sliding compaction GCs.\n+ * It avoids overriding the compressed class pointer in the upper bits of the header, which would otherwise\n+ * be lost. SlidingForwarding requires only small side tables and guarantees constant-time access and modification.\n+ *\n+ * The idea is to use a pointer compression scheme very similar to the one that is used for compressed oops.\n+ * We divide the heap into number of logical regions. Each region spans maximum of 2^NUM_BITS words.\n+ * We take advantage of the fact that sliding compaction can forward objects from one region to a maximum of\n+ * two regions (including itself, but that does not really matter). We need 1 bit to indicate which region is forwarded\n+ * into. We also currently require the two lowest header bits to indicate that the object is forwarded.\n+ *\n+ * For addressing, we need a table with N*2 entries, for N logical regions. For each region, it gives the base\n+ * address of the two target regions, or a special placeholder if not used.\n+ *\n+ * Adding a forwarding then works as follows:\n+ * Given an original address 'orig', and a 'target' address:\n+ * - Look-up first target base of region of orig. If not yet used,\n+ *   establish it to be the base of region of target address. Use that base in step 3.\n+ * - Else, if first target base is already used, check second target base. This must either be unused, or the\n+ *   base of the region of our target address. If unused, establish it to be the base of the region of our target\n+ *   address. Use that base for next step.\n+ * - Now we found a base address. Encode the target address with that base into lowest NUM_BITS bits, and shift\n+ *   that up by 3 bits. Set the 3rd bit if we used the secondary target base, otherwise leave it at 0. Set the\n+ *   lowest two bits to indicate that the object has been forwarded. Store that in the lowest NUM_BITS+3 bits of the\n+ *   original object's header.\n+ *\n+ * Similarily, looking up the target address, given an original object address works as follows:\n+ * - Load lowest NUM_BITS + 3 from original object header. Extract target region bit and compressed address bits.\n+ * - Depending on target region bit, load base address from the target base table by looking up the corresponding entry\n+ *   for the region of the original object.\n+ * - Decode the target address by using the target base address and the compressed address bits.\n+ *\/\n+\n+class SlidingForwarding : public CHeapObj<mtGC> {\n+#ifdef _LP64\n+private:\n+  static const int NUM_REGION_BITS = 1;\n+\n+  static const uintptr_t ONE = 1ULL;\n+\n+  static const size_t NUM_REGIONS = ONE << NUM_REGION_BITS;\n+\n+  \/\/ We need the lowest three bits to indicate a forwarded object and self-forwarding.\n+  static const int BASE_SHIFT = 3;\n+\n+  \/\/ The compressed address bits start here.\n+  static const int COMPRESSED_BITS_SHIFT = BASE_SHIFT + NUM_REGION_BITS;\n+\n+  \/\/ How many bits we use for the compressed pointer (we are going to need one more bit to indicate target region, and\n+  \/\/ two lowest bits to mark objects as forwarded)\n+  static const int NUM_COMPRESSED_BITS = 32 - BASE_SHIFT - NUM_REGION_BITS;\n+\n+  \/\/ Indicates an usused base address in the target base table. We cannot use 0, because that may already be\n+  \/\/ a valid base address in zero-based heaps. 0x1 is safe because heap base addresses must be aligned by 2^X.\n+  static HeapWord* const UNUSED_BASE;\n+\n+  HeapWord*  const _heap_start;\n+  size_t     const _num_regions;\n+  size_t     const _region_size_words_shift;\n+  HeapWord** const _target_base_table;\n+\n+  inline size_t region_index_containing(HeapWord* addr) const;\n+  inline bool region_contains(HeapWord* region_base, HeapWord* addr) const;\n+\n+  inline uintptr_t encode_forwarding(HeapWord* original, HeapWord* target);\n+  inline HeapWord* decode_forwarding(HeapWord* original, uintptr_t encoded) const;\n+\n+#endif\n+\n+public:\n+  SlidingForwarding(MemRegion heap);\n+  SlidingForwarding(MemRegion heap, size_t num_regions);\n+  ~SlidingForwarding();\n+\n+  void clear();\n+  inline void forward_to(oop original, oop target);\n+  inline oop forwardee(oop original) const;\n+};\n+\n+#endif \/\/ SHARE_GC_SHARED_SLIDINGFORWARDING_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/slidingForwarding.hpp","additions":112,"deletions":0,"binary":false,"changes":112,"status":"added"},{"patch":"@@ -0,0 +1,113 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_SHARED_SLIDINGFORWARDING_INLINE_HPP\n+#define SHARE_GC_SHARED_SLIDINGFORWARDING_INLINE_HPP\n+\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+\n+#ifdef _LP64\n+size_t SlidingForwarding::region_index_containing(HeapWord* addr) const {\n+  assert(addr >= _heap_start, \"sanity: addr: \" PTR_FORMAT \" heap base: \" PTR_FORMAT, p2i(addr), p2i(_heap_start));\n+  size_t index = ((size_t) (addr - _heap_start)) >> _region_size_words_shift;\n+  assert(index < _num_regions, \"Region index is in bounds: \" PTR_FORMAT, p2i(addr));\n+  return index;\n+}\n+\n+bool SlidingForwarding::region_contains(HeapWord* region_base, HeapWord* addr) const {\n+  return uintptr_t(addr - region_base) < (ONE << _region_size_words_shift);\n+}\n+\n+\n+uintptr_t SlidingForwarding::encode_forwarding(HeapWord* original, HeapWord* target) {\n+  size_t orig_idx = region_index_containing(original);\n+  size_t base_table_idx = orig_idx * 2;\n+  size_t target_idx = region_index_containing(target);\n+  HeapWord* encode_base;\n+  uintptr_t region_idx;\n+  for (region_idx = 0; region_idx < NUM_REGIONS; region_idx++) {\n+    encode_base = _target_base_table[base_table_idx + region_idx];\n+    if (encode_base == UNUSED_BASE) {\n+      encode_base = _heap_start + target_idx * (ONE << _region_size_words_shift);\n+      _target_base_table[base_table_idx + region_idx] = encode_base;\n+      break;\n+    } else if (region_contains(encode_base, target)) {\n+      break;\n+    }\n+  }\n+  if (region_idx >= NUM_REGIONS) {\n+    tty->print_cr(\"target: \" PTR_FORMAT, p2i(target));\n+    for (region_idx = 0; region_idx < NUM_REGIONS; region_idx++) {\n+      tty->print_cr(\"region_idx: \" INTPTR_FORMAT \", encode_base: \" PTR_FORMAT, region_idx, p2i(_target_base_table[base_table_idx + region_idx]));\n+    }\n+  }\n+  assert(region_idx < NUM_REGIONS, \"need to have found an encoding base\");\n+  assert(target >= encode_base, \"target must be above encode base, target:\" PTR_FORMAT \", encoded_base: \" PTR_FORMAT \",  target_idx: \" SIZE_FORMAT \", heap start: \" PTR_FORMAT \", region_idx: \" INTPTR_FORMAT,\n+         p2i(target), p2i(encode_base), target_idx, p2i(_heap_start), region_idx);\n+  assert(region_contains(encode_base, target), \"region must contain target: original: \" PTR_FORMAT \", target: \" PTR_FORMAT \", encode_base: \" PTR_FORMAT \", region_idx: \" INTPTR_FORMAT, p2i(original), p2i(target), p2i(encode_base), region_idx);\n+  uintptr_t encoded = (((uintptr_t)(target - encode_base)) << COMPRESSED_BITS_SHIFT) |\n+                      (region_idx << BASE_SHIFT) | markWord::marked_value;\n+  assert(target == decode_forwarding(original, encoded), \"must be reversible\");\n+  return encoded;\n+}\n+\n+HeapWord* SlidingForwarding::decode_forwarding(HeapWord* original, uintptr_t encoded) const {\n+  assert((encoded & markWord::marked_value) == markWord::marked_value, \"must be marked as forwarded\");\n+  size_t orig_idx = region_index_containing(original);\n+  size_t region_idx = (encoded >> BASE_SHIFT) & right_n_bits(NUM_REGION_BITS);\n+  size_t base_table_idx = orig_idx * 2 + region_idx;\n+  HeapWord* decoded = _target_base_table[base_table_idx] + (encoded >> COMPRESSED_BITS_SHIFT);\n+  assert(decoded >= _heap_start, \"must be above heap start, encoded: \" INTPTR_FORMAT \", region_idx: \" SIZE_FORMAT \", base: \" PTR_FORMAT, encoded, region_idx, p2i(_target_base_table[base_table_idx]));\n+  return decoded;\n+}\n+#endif\n+\n+void SlidingForwarding::forward_to(oop original, oop target) {\n+#ifdef _LP64\n+  markWord header = original->mark();\n+  if (header.has_displaced_mark_helper()) {\n+    header = header.displaced_mark_helper();\n+  }\n+  uintptr_t encoded = encode_forwarding(cast_from_oop<HeapWord*>(original), cast_from_oop<HeapWord*>(target));\n+  assert((encoded & markWord::klass_mask_in_place) == 0, \"encoded forwardee must not overlap with Klass*: \" PTR_FORMAT, encoded);\n+  header = markWord((header.value() & markWord::klass_mask_in_place) | encoded);\n+  original->set_mark(header);\n+#else\n+  original->forward_to(target);\n+#endif\n+}\n+\n+oop SlidingForwarding::forwardee(oop original) const {\n+#ifdef _LP64\n+  markWord header = original->mark();\n+  uintptr_t encoded = header.value() & ~markWord::klass_mask_in_place;\n+  HeapWord* forwardee = decode_forwarding(cast_from_oop<HeapWord*>(original), encoded);\n+  return cast_to_oop(forwardee);\n+#else\n+  return original->forwardee();\n+#endif\n+}\n+\n+#endif \/\/ SHARE_GC_SHARED_SLIDINGFORWARDING_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/slidingForwarding.inline.hpp","additions":113,"deletions":0,"binary":false,"changes":113,"status":"added"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -350,1 +351,1 @@\n-                                    CompactPoint* cp, HeapWord* compact_top) {\n+                                    CompactPoint* cp, HeapWord* compact_top, SlidingForwarding* const forwarding) {\n@@ -373,1 +374,1 @@\n-    q->forward_to(cast_to_oop(compact_top));\n+    forwarding->forward_to(q, cast_to_oop(compact_top));\n@@ -379,1 +380,1 @@\n-    assert(q->forwardee() == NULL, \"should be forwarded to NULL\");\n+    assert(!q->is_forwarded(), \"should not be forwarded\");\n@@ -487,1 +488,7 @@\n-    mark += cast_to_oop(mark)->size();\n+    oop obj = cast_to_oop(mark);\n+#ifdef _LP64\n+    if (obj->is_forwarded() && CompressedKlassPointers::is_null(obj->mark().narrow_klass())) {\n+      obj = obj->forwardee();\n+    }\n+#endif\n+    mark += obj->size();\n@@ -589,1 +596,1 @@\n-  const size_t array_header_size = typeArrayOopDesc::header_size(T_INT);\n+  const size_t array_header_size = (arrayOopDesc::base_offset_in_bytes(T_INT) + BytesPerWord) \/ BytesPerWord;\n@@ -595,0 +602,3 @@\n+#ifdef _LP64\n+    t->set_mark(Universe::intArrayKlassObj()->prototype_header());\n+#else\n@@ -597,0 +607,1 @@\n+#endif\n@@ -602,0 +613,3 @@\n+#ifdef _LP64\n+    obj->set_mark(vmClasses::Object_klass()->prototype_header());\n+#else\n@@ -603,1 +617,0 @@\n-    obj->set_klass_gap(0);\n@@ -605,0 +618,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shared\/space.cpp","additions":20,"deletions":6,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+class SlidingForwarding;\n@@ -435,1 +436,1 @@\n-                    HeapWord* compact_top);\n+                    HeapWord* compact_top, SlidingForwarding* const forwarding);\n","filename":"src\/hotspot\/share\/gc\/shared\/space.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/shared\/genCollectedHeap.hpp\"\n@@ -166,0 +167,1 @@\n+  SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n@@ -171,1 +173,1 @@\n-      compact_top = cp->space->forward(cast_to_oop(cur_obj), size, cp, compact_top);\n+      compact_top = cp->space->forward(cast_to_oop(cur_obj), size, cp, compact_top, forwarding);\n@@ -187,1 +189,1 @@\n-        compact_top = cp->space->forward(obj, obj->size(), cp, compact_top);\n+        compact_top = cp->space->forward(obj, obj->size(), cp, compact_top, forwarding);\n@@ -226,0 +228,1 @@\n+  const SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n@@ -237,1 +240,1 @@\n-      size_t size = MarkSweep::adjust_pointers(cast_to_oop(cur_obj));\n+      size_t size = MarkSweep::adjust_pointers(forwarding, cast_to_oop(cur_obj));\n@@ -319,0 +322,2 @@\n+  const SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n+\n@@ -332,1 +337,1 @@\n-      HeapWord* compaction_top = cast_from_oop<HeapWord*>(cast_to_oop(cur_obj)->forwardee());\n+      HeapWord* compaction_top = cast_from_oop<HeapWord*>(forwarding->forwardee(cast_to_oop(cur_obj)));\n","filename":"src\/hotspot\/share\/gc\/shared\/space.inline.hpp","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -68,0 +68,1 @@\n+      ObjectMonitor::maybe_deflate_dead(p);\n","filename":"src\/hotspot\/share\/gc\/shared\/weakProcessor.inline.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahObjectUtils.inline.hpp\"\n@@ -200,1 +201,1 @@\n-  Klass* obj_klass = obj->klass_or_null();\n+  Klass* obj_klass = ShenandoahObjectUtils::klass(obj);\n@@ -232,1 +233,1 @@\n-    if (obj_klass != fwd->klass()) {\n+    if (obj_klass != ShenandoahObjectUtils::klass(fwd)) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -191,0 +192,1 @@\n+    heap->forwarding()->clear();\n@@ -302,2 +304,3 @@\n-  PreservedMarks*          const _preserved_marks;\n-  ShenandoahHeap*          const _heap;\n+  PreservedMarks*    const _preserved_marks;\n+  SlidingForwarding* const _forwarding;\n+  ShenandoahHeap*    const _heap;\n@@ -315,0 +318,1 @@\n+    _forwarding(ShenandoahHeap::heap()->forwarding()),\n@@ -368,1 +372,1 @@\n-    p->forward_to(cast_to_oop(_compact_point));\n+    _forwarding->forward_to(p, cast_to_oop(_compact_point));\n@@ -442,0 +446,1 @@\n+  SlidingForwarding* forwarding = heap->forwarding();\n@@ -476,1 +481,1 @@\n-        old_obj->forward_to(cast_to_oop(heap->get_region(start)->bottom()));\n+        forwarding->forward_to(old_obj, cast_to_oop(heap->get_region(start)->bottom()));\n@@ -727,1 +732,2 @@\n-  ShenandoahHeap* const _heap;\n+  ShenandoahHeap*           const _heap;\n+  const SlidingForwarding*  const _forwarding;\n@@ -737,1 +743,1 @@\n-        oop forw = obj->forwardee();\n+        oop forw = _forwarding->forwardee(obj);\n@@ -746,0 +752,1 @@\n+    _forwarding(_heap->forwarding()),\n@@ -805,1 +812,2 @@\n-    _preserved_marks->get(worker_id)->adjust_during_full_gc();\n+    const SlidingForwarding* const forwarding = ShenandoahHeap::heap()->forwarding();\n+    _preserved_marks->get(worker_id)->adjust_during_full_gc(forwarding);\n@@ -835,2 +843,3 @@\n-  ShenandoahHeap* const _heap;\n-  uint            const _worker_id;\n+  ShenandoahHeap*          const _heap;\n+  const SlidingForwarding* const _forwarding;\n+  uint                     const _worker_id;\n@@ -840,1 +849,1 @@\n-    _heap(ShenandoahHeap::heap()), _worker_id(worker_id) {}\n+    _heap(ShenandoahHeap::heap()), _forwarding(_heap->forwarding()), _worker_id(worker_id) {}\n@@ -847,1 +856,1 @@\n-      HeapWord* compact_to = cast_from_oop<HeapWord*>(p->forwardee());\n+      HeapWord* compact_to = cast_from_oop<HeapWord*>(_forwarding->forwardee(p));\n@@ -942,0 +951,1 @@\n+  const SlidingForwarding* const forwarding = heap->forwarding();\n@@ -956,1 +966,1 @@\n-      size_t new_start = heap->heap_region_index_containing(old_obj->forwardee());\n+      size_t new_start = heap->heap_region_index_containing(forwarding->forwardee(old_obj));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":22,"deletions":12,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -194,0 +195,2 @@\n+  _forwarding = new SlidingForwarding(_heap_region, ShenandoahHeapRegion::region_size_words_shift());\n+\n@@ -954,1 +957,1 @@\n-    if (!p->is_forwarded()) {\n+    if (!ShenandoahForwarding::is_forwarded(p)) {\n@@ -1242,1 +1245,1 @@\n-      obj = ShenandoahBarrierSet::resolve_forwarded_not_null(obj);\n+      obj = ShenandoahBarrierSet::barrier_set()->load_reference_barrier(obj);\n@@ -1298,0 +1301,1 @@\n+    shenandoah_assert_not_in_cset_except(NULL, obj, cancelled_gc());\n@@ -1351,1 +1355,1 @@\n-      obj = ShenandoahBarrierSet::resolve_forwarded_not_null(obj);\n+      obj = ShenandoahBarrierSet::barrier_set()->load_reference_barrier(obj);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -65,0 +65,1 @@\n+class SlidingForwarding;\n@@ -230,0 +231,1 @@\n+  SlidingForwarding* _forwarding;\n@@ -246,0 +248,2 @@\n+  SlidingForwarding* forwarding() const { return _forwarding; }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"gc\/shenandoah\/shenandoahObjectUtils.inline.hpp\"\n@@ -218,1 +219,1 @@\n-  size_t size = p->size();\n+  size_t size = ShenandoahObjectUtils::size(p);\n@@ -435,1 +436,1 @@\n-    int size = obj->size();\n+    size_t size = ShenandoahObjectUtils::size(obj);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,43 @@\n+\/*\n+ * Copyright (c) 2022, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHOBJECTUTILS_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHOBJECTUTILS_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+\n+class Klass;\n+\n+class ShenandoahObjectUtils : public AllStatic {\n+public:\n+#ifdef _LP64\n+  static inline markWord stable_mark(oop obj);\n+#endif\n+  static inline Klass* klass(oop obj);\n+  static inline size_t size(oop obj);\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHOBJECTUTILS_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahObjectUtils.hpp","additions":43,"deletions":0,"binary":false,"changes":43,"status":"added"},{"patch":"@@ -0,0 +1,95 @@\n+\/*\n+ * Copyright (c) 2022, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHOBJECTUTILS_INLINE_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHOBJECTUTILS_INLINE_HPP\n+\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahObjectUtils.hpp\"\n+#include \"oops\/klass.hpp\"\n+#include \"oops\/markWord.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"runtime\/objectMonitor.inline.hpp\"\n+#include \"runtime\/thread.hpp\"\n+\n+\/\/ This is a variant of oopDesc::actual_mark(), which does the same thing, but also\n+\/\/ handles forwarded objects. This is intended to be used by concurrent evacuation only. No other\n+\/\/ code is supposed to observe from-space objects.\n+#ifdef _LP64\n+markWord ShenandoahObjectUtils::stable_mark(oop obj) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  assert(heap->is_in(obj), \"object not in heap: \" PTR_FORMAT, p2i(obj));\n+  markWord mark = obj->mark_acquire();\n+\n+  assert(!mark.is_being_inflated(), \"can not be inflating\");\n+  assert(!mark.has_locker(), \"can not be stack-locked\");\n+\n+  \/\/ The mark can be in one of the following states:\n+  \/\/ *  Marked       - object is forwarded, try again on forwardee\n+  \/\/ *  Inflated     - just return mark from inflated monitor\n+  \/\/ *  Fast-locked  - return mark\n+  \/\/ *  Neutral      - return mark\n+\n+  \/\/ Most common cases first.\n+  if (mark.is_neutral() || mark.is_fast_locked()) {\n+    return mark;\n+  }\n+\n+  \/\/ If object is already forwarded, then resolve it, and try again.\n+  if (mark.is_marked()) {\n+    if (heap->is_full_gc_move_in_progress()) {\n+      \/\/ In these cases, we want to return the header as-is: the Klass* would not be overloaded.\n+      return mark;\n+    }\n+    obj = cast_to_oop(mark.decode_pointer());\n+    return stable_mark(obj);\n+  }\n+\n+  \/\/ CASE: inflated\n+  assert(mark.has_monitor(), \"must be monitor-locked at this point\");\n+  \/\/ It is safe to access the object monitor because all Java and GC worker threads\n+  \/\/ participate in the monitor deflation protocol (i.e, they react to handshakes and STS requests).\n+  ObjectMonitor* inf = mark.monitor();\n+  markWord dmw = inf->header();\n+  assert(dmw.is_neutral(), \"invariant: header=\" INTPTR_FORMAT \", original mark: \" INTPTR_FORMAT, dmw.value(), mark.value());\n+  return dmw;\n+}\n+#endif\n+\n+Klass* ShenandoahObjectUtils::klass(oop obj) {\n+#ifdef _LP64\n+  markWord header = stable_mark(obj);\n+  assert(header.narrow_klass() != 0, \"klass must not be NULL: \" INTPTR_FORMAT, header.value());\n+  return header.klass();\n+#else\n+  return obj->klass();\n+#endif\n+}\n+\n+size_t ShenandoahObjectUtils::size(oop obj) {\n+  Klass* kls = klass(obj);\n+  return obj->size_given_klass(kls);\n+}\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHOBJECTUTILS_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahObjectUtils.inline.hpp","additions":95,"deletions":0,"binary":false,"changes":95,"status":"added"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahObjectUtils.inline.hpp\"\n@@ -100,1 +101,1 @@\n-      if (is_instance_ref_klass(obj->klass())) {\n+      if (is_instance_ref_klass(ShenandoahObjectUtils::klass(obj))) {\n@@ -127,1 +128,1 @@\n-    Klass* obj_klass = obj->klass_or_null();\n+    Klass* obj_klass = ShenandoahObjectUtils::klass(obj);\n@@ -142,1 +143,1 @@\n-        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + obj->size()) <= obj_reg->top(),\n+        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + ShenandoahObjectUtils::size(obj)) <= obj_reg->top(),\n@@ -146,1 +147,1 @@\n-        size_t humongous_end = humongous_start + (obj->size() >> ShenandoahHeapRegion::region_size_words_shift());\n+        size_t humongous_end = humongous_start + (ShenandoahObjectUtils::size(obj) >> ShenandoahHeapRegion::region_size_words_shift());\n@@ -163,1 +164,1 @@\n-          Atomic::add(&_ld[obj_reg->index()], (uint) obj->size(), memory_order_relaxed);\n+          Atomic::add(&_ld[obj_reg->index()], (uint) ShenandoahObjectUtils::size(obj), memory_order_relaxed);\n@@ -204,1 +205,1 @@\n-      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + fwd->size()) <= fwd_reg->top(),\n+      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + ShenandoahObjectUtils::size(fwd)) <= fwd_reg->top(),\n@@ -307,1 +308,2 @@\n-    obj->oop_iterate(this);\n+    Klass* klass = ShenandoahObjectUtils::klass(obj);\n+    obj->oop_iterate_backwards(this, klass);\n@@ -587,1 +589,1 @@\n-    if (!is_instance_ref_klass(obj->klass())) {\n+    if (!is_instance_ref_klass(ShenandoahObjectUtils::klass(obj))) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":10,"deletions":8,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -337,1 +337,1 @@\n-  product(bool, ShenandoahSuspendibleWorkers, false, EXPERIMENTAL,          \\\n+  product(bool, ShenandoahSuspendibleWorkers, true, EXPERIMENTAL,           \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -147,4 +147,0 @@\n-    \/\/ Get the size of the object before calling the closure, which\n-    \/\/ might overwrite the object in case we are relocating in-place.\n-    const size_t size = ZUtils::object_size(addr);\n-\n@@ -155,1 +151,1 @@\n-    const uintptr_t next_addr = align_up(addr + size, 1 << page_object_alignment_shift);\n+    const uintptr_t next_addr = align_up(addr + 1, 1 << page_object_alignment_shift);\n","filename":"src\/hotspot\/share\/gc\/z\/zLiveMap.inline.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -46,3 +46,4 @@\n-  const size_t segment_max = ZUtils::bytes_to_words(64 * K);\n-  const size_t skip = arrayOopDesc::header_size(ArrayKlass::cast(_klass)->element_type());\n-  size_t remaining = _word_size - skip;\n+  const size_t segment_max = 64 * K;\n+  const size_t skip = arrayOopDesc::base_offset_in_bytes(ArrayKlass::cast(_klass)->element_type());\n+  size_t byte_size = _word_size * BytesPerWord;\n+  size_t remaining = byte_size - skip;\n@@ -50,0 +51,1 @@\n+  char* const start = reinterpret_cast<char*>(mem);\n@@ -53,1 +55,1 @@\n-    Copy::zero_to_words(mem + (_word_size - remaining), segment);\n+    Copy::zero_to_bytes(start + (byte_size - remaining), segment);\n","filename":"src\/hotspot\/share\/gc\/z\/zObjArrayAllocator.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -335,0 +336,4 @@\n+\n+    if (SuspendibleThreadSet::should_yield()) {\n+      SuspendibleThreadSet::yield();\n+    }\n@@ -406,0 +411,1 @@\n+    SuspendibleThreadSetJoiner sts_joiner;\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -727,0 +727,9 @@\n+  if (!UseHeavyMonitors && UseFastLocking) {\n+    \/\/ This is a hack to get around the limitation of registers in x86_32. We really\n+    \/\/ send an oopDesc* instead of a BasicObjectLock*.\n+    Handle h_obj(current, oop((reinterpret_cast<oopDesc*>(elem))));\n+    assert(Universe::heap()->is_in_or_null(h_obj()),\n+           \"must be NULL or an object\");\n+    ObjectSynchronizer::enter(h_obj, NULL, current);\n+    return;\n+  }\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1958,0 +1958,3 @@\n+#ifdef _LP64\n+              oopDesc::release_set_mark(result, ik->prototype_header());\n+#else\n@@ -1959,1 +1962,0 @@\n-              obj->set_klass_gap(0);\n@@ -1961,1 +1963,1 @@\n-\n+#endif\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -69,1 +69,2 @@\n-    _store->push(ObjectSampleMarkWord(obj, obj->mark()));\n+    markWord mark = obj->mark();\n+    _store->push(ObjectSampleMarkWord(obj, mark));\n@@ -73,1 +74,6 @@\n-    obj->set_mark(markWord::prototype().set_marked());\n+#ifdef _LP64\n+    if (mark.has_displaced_mark_helper()) {\n+      mark = mark.displaced_mark_helper();\n+    }\n+#endif\n+    obj->set_mark(markWord::prototype().set_marked() LP64_ONLY(.set_narrow_klass(mark.narrow_klass())));\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/chains\/objectSampleMarker.hpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2120,1 +2120,1 @@\n-  return arrayOopDesc::header_size(type) * HeapWordSize;\n+  return arrayOopDesc::base_offset_in_bytes(type);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -268,1 +268,0 @@\n-  volatile_nonstatic_field(oopDesc,            _metadata._klass,                              Klass*)                                \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -53,2 +53,1 @@\n-    size_t hs = align_up(length_offset_in_bytes() + sizeof(int),\n-                              HeapWordSize);\n+    size_t hs = length_offset_in_bytes() + sizeof(int);\n@@ -74,0 +73,5 @@\n+#ifdef _LP64\n+    if (type == T_OBJECT || type == T_ARRAY) {\n+      return !UseCompressedOops;\n+    }\n+#endif\n@@ -82,2 +86,1 @@\n-    return UseCompressedClassPointers ? klass_gap_offset_in_bytes() :\n-                               sizeof(arrayOopDesc);\n+    return sizeof(arrayOopDesc);\n@@ -88,1 +91,4 @@\n-    return header_size(type) * HeapWordSize;\n+    size_t typesize_in_bytes = header_size_in_bytes();\n+    return (int)(element_type_should_be_aligned(type)\n+                 ? align_up(typesize_in_bytes, BytesPerLong)\n+                 : typesize_in_bytes);\n@@ -125,11 +131,0 @@\n-  \/\/ Should only be called with constants as argument\n-  \/\/ (will not constant fold otherwise)\n-  \/\/ Returns the header size in words aligned to the requirements of the\n-  \/\/ array object type.\n-  static int header_size(BasicType type) {\n-    size_t typesize_in_bytes = header_size_in_bytes();\n-    return (int)(element_type_should_be_aligned(type)\n-      ? align_object_offset(typesize_in_bytes\/HeapWordSize)\n-      : typesize_in_bytes\/HeapWordSize);\n-  }\n-\n@@ -144,4 +139,2 @@\n-    const size_t max_element_words_per_size_t =\n-      align_down((SIZE_MAX\/HeapWordSize - header_size(type)), MinObjAlignment);\n-    const size_t max_elements_per_size_t =\n-      HeapWordSize * max_element_words_per_size_t \/ type2aelembytes(type);\n+    const size_t max_size_bytes = align_down(SIZE_MAX - base_offset_in_bytes(type), MinObjAlignmentInBytes);\n+    const size_t max_elements_per_size_t = max_size_bytes \/ type2aelembytes(type);\n@@ -153,1 +146,2 @@\n-      return align_down(max_jint - header_size(type), MinObjAlignment);\n+      int header_size_words = align_up(base_offset_in_bytes(type), HeapWordSize) \/ HeapWordSize;\n+      return align_down(max_jint - header_size_words, MinObjAlignment);\n","filename":"src\/hotspot\/share\/oops\/arrayOop.hpp","additions":15,"deletions":21,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -2591,1 +2591,1 @@\n-    set_prototype_header(markWord::prototype());\n+    set_prototype_header(markWord::prototype() LP64_ONLY(.set_klass(this)));\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -40,4 +40,1 @@\n-    return (UseCompressedClassPointers) ?\n-            klass_gap_offset_in_bytes() :\n-            sizeof(instanceOopDesc);\n-\n+    return sizeof(instanceOopDesc);\n","filename":"src\/hotspot\/share\/oops\/instanceOop.hpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -129,1 +129,1 @@\n-  static void trace_reference_gc(const char *s, oop obj) NOT_DEBUG_RETURN;\n+  void trace_reference_gc(const char *s, oop obj) NOT_DEBUG_RETURN;\n","filename":"src\/hotspot\/share\/oops\/instanceRefKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -188,1 +188,1 @@\n-  if (java_lang_ref_Reference::is_phantom(obj)) {\n+  if (reference_type() == REF_PHANTOM) {\n","filename":"src\/hotspot\/share\/oops\/instanceRefKlass.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -205,1 +205,1 @@\n-                           _prototype_header(markWord::prototype()),\n+                           _prototype_header(markWord::prototype() LP64_ONLY(.set_klass(this))),\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -55,1 +55,0 @@\n-  assert(!header.has_bias_pattern() || is_instance_klass(), \"biased locking currently only supported for Java instances\");\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-\/\/  unused:25 hash:31 -->| unused_gap:1   age:4    biased_lock:1 lock:2 (normal object)\n+\/\/  nklass:32 hash:25 -->| unused_gap:1   age:4    biased_lock:1 lock:2 (normal object)\n@@ -97,0 +97,1 @@\n+class Klass;\n@@ -132,3 +133,6 @@\n-  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - biased_lock_bits;\n-  static const int hash_bits                      = max_hash_bits > 31 ? 31 : max_hash_bits;\n-  static const int unused_gap_bits                = LP64_ONLY(1) NOT_LP64(0);\n+  static const int self_forwarded_bits            = 1;\n+  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - self_forwarded_bits;\n+  static const int hash_bits                      = max_hash_bits > 25 ? 25 : max_hash_bits;\n+#ifdef _LP64\n+  static const int klass_bits                     = 32;\n+#endif\n@@ -141,3 +145,6 @@\n-  static const int age_shift                      = lock_bits + biased_lock_bits;\n-  static const int unused_gap_shift               = age_shift + age_bits;\n-  static const int hash_shift                     = unused_gap_shift + unused_gap_bits;\n+  static const int self_forwarded_shift           = lock_shift + lock_bits;\n+  static const int age_shift                      = self_forwarded_shift + self_forwarded_bits;\n+  static const int hash_shift                     = age_shift + age_bits;\n+#ifdef _LP64\n+  static const int klass_shift                    = hash_shift + hash_bits;\n+#endif\n@@ -151,0 +158,2 @@\n+  static const uintptr_t self_forwarded_mask      = right_n_bits(self_forwarded_bits);\n+  static const uintptr_t self_forwarded_mask_in_place = self_forwarded_mask << self_forwarded_shift;\n@@ -159,0 +168,5 @@\n+#ifdef _LP64\n+  static const uintptr_t klass_mask               = right_n_bits(klass_bits);\n+  static const uintptr_t klass_mask_in_place      = klass_mask << klass_shift;\n+#endif\n+\n@@ -270,1 +284,1 @@\n-    return ((value() & lock_mask_in_place) == locked_value);\n+    return !UseFastLocking && ((value() & lock_mask_in_place) == locked_value);\n@@ -276,0 +290,8 @@\n+\n+  bool is_fast_locked() const {\n+    return UseFastLocking && ((value() & lock_mask_in_place) == locked_value);\n+  }\n+  markWord set_fast_locked() const {\n+    return markWord(value() & ~lock_mask_in_place);\n+  }\n+\n@@ -285,1 +307,3 @@\n-    return ((value() & unlocked_value) == 0);\n+    intptr_t lockbits = value() & lock_mask_in_place;\n+    return UseFastLocking ? lockbits == monitor_value   \/\/ monitor?\n+                          : (lockbits & unlocked_value) == 0; \/\/ monitor | stack-locked?\n@@ -339,0 +363,9 @@\n+#ifdef _LP64\n+  inline Klass* klass() const;\n+  inline Klass* klass_or_null() const;\n+  inline Klass* safe_klass() const;\n+  inline markWord set_klass(const Klass* klass) const;\n+  inline narrowKlass narrow_klass() const;\n+  inline markWord set_narrow_klass(const narrowKlass klass) const;\n+#endif\n+\n@@ -355,0 +388,8 @@\n+\n+  inline bool self_forwarded() const {\n+    return mask_bits(value(), self_forwarded_mask_in_place) != 0;\n+  }\n+\n+  inline markWord set_self_forwarded() const {\n+    return markWord(value() | self_forwarded_mask_in_place | marked_value);\n+  }\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":50,"deletions":9,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/safepoint.hpp\"\n@@ -78,0 +79,35 @@\n+#ifdef _LP64\n+narrowKlass markWord::narrow_klass() const {\n+  return narrowKlass(value() >> klass_shift);\n+}\n+\n+Klass* markWord::klass() const {\n+  assert(!CompressedKlassPointers::is_null(narrow_klass()), \"narrow klass must not be null: \" INTPTR_FORMAT, value());\n+  return CompressedKlassPointers::decode_not_null(narrow_klass());\n+}\n+\n+Klass* markWord::klass_or_null() const {\n+  return CompressedKlassPointers::decode(narrow_klass());\n+}\n+\n+markWord markWord::set_narrow_klass(const narrowKlass nklass) const {\n+  return markWord((value() & ~klass_mask_in_place) | ((uintptr_t) nklass << klass_shift));\n+}\n+\n+Klass* markWord::safe_klass() const {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"only call at safepoint\");\n+  markWord m = *this;\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  return CompressedKlassPointers::decode_not_null(m.narrow_klass());\n+}\n+\n+markWord markWord::set_klass(const Klass* klass) const {\n+  assert(UseCompressedClassPointers, \"expect compressed klass pointers\");\n+  \/\/ TODO: Don't cast to non-const, change CKP::encode() to accept const Klass* instead.\n+  narrowKlass nklass = CompressedKlassPointers::encode(const_cast<Klass*>(klass));\n+  return set_narrow_klass(nklass);\n+}\n+#endif\n+\n","filename":"src\/hotspot\/share\/oops\/markWord.inline.hpp","additions":36,"deletions":0,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -159,1 +159,0 @@\n-  assert(obj->is_objArray(), \"must be object array\");\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -73,1 +73,0 @@\n-  assert (obj->is_array(), \"obj must be array\");\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.inline.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -50,23 +50,3 @@\n-  \/\/ Give size of objArrayOop in HeapWords minus the header\n-  static int array_size(int length) {\n-    const uint OopsPerHeapWord = HeapWordSize\/heapOopSize;\n-    assert(OopsPerHeapWord >= 1 && (HeapWordSize % heapOopSize == 0),\n-           \"Else the following (new) computation would be in error\");\n-    uint res = ((uint)length + OopsPerHeapWord - 1)\/OopsPerHeapWord;\n-#ifdef ASSERT\n-    \/\/ The old code is left in for sanity-checking; it'll\n-    \/\/ go away pretty soon. XXX\n-    \/\/ Without UseCompressedOops, this is simply:\n-    \/\/ oop->length() * HeapWordsPerOop;\n-    \/\/ With narrowOops, HeapWordsPerOop is 1\/2 or equal 0 as an integer.\n-    \/\/ The oop elements are aligned up to wordSize\n-    const uint HeapWordsPerOop = heapOopSize\/HeapWordSize;\n-    uint old_res;\n-    if (HeapWordsPerOop > 0) {\n-      old_res = length * HeapWordsPerOop;\n-    } else {\n-      old_res = align_up((uint)length, OopsPerHeapWord)\/OopsPerHeapWord;\n-    }\n-    assert(res == old_res, \"Inconsistency between old and new.\");\n-#endif  \/\/ ASSERT\n-    return res;\n+  \/\/ Give size of objArrayOop in bytes minus the header\n+  static size_t array_size_in_bytes(int length) {\n+    return (size_t)length * heapOopSize;\n@@ -92,1 +72,0 @@\n-  static int header_size()    { return arrayOopDesc::header_size(T_OBJECT); }\n@@ -97,5 +76,5 @@\n-    uint asz = array_size(length);\n-    uint osz = align_object_size(header_size() + asz);\n-    assert(osz >= asz,   \"no overflow\");\n-    assert((int)osz > 0, \"no overflow\");\n-    return (int)osz;\n+    size_t asz = array_size_in_bytes(length);\n+    size_t size_words = align_up(base_offset_in_bytes() + asz, HeapWordSize) \/ HeapWordSize;\n+    size_t osz = align_object_size(size_words);\n+    assert(osz < max_jint, \"no overflow\");\n+    return checked_cast<int>(osz);\n","filename":"src\/hotspot\/share\/oops\/objArrayOop.hpp","additions":8,"deletions":29,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -111,1 +111,1 @@\n-  if (ignore_mark_word) {\n+  if (ignore_mark_word || UseFastLocking) {\n@@ -141,13 +141,0 @@\n-bool oopDesc::has_klass_gap() {\n-  \/\/ Only has a klass gap when compressed class pointers are used.\n-  return UseCompressedClassPointers;\n-}\n-\n-#if INCLUDE_CDS_JAVA_HEAP\n-void oopDesc::set_narrow_klass(narrowKlass nk) {\n-  assert(DumpSharedSpaces, \"Used by CDS only. Do not abuse!\");\n-  assert(UseCompressedClassPointers, \"must be\");\n-  _metadata._compressed_klass = nk;\n-}\n-#endif\n-\n@@ -155,7 +142,8 @@\n-  if (UseCompressedClassPointers) {\n-    narrowKlass narrow_klass = obj->_metadata._compressed_klass;\n-    if (narrow_klass == 0) return NULL;\n-    return (void*)CompressedKlassPointers::decode_raw(narrow_klass);\n-  } else {\n-    return obj->_metadata._klass;\n-  }\n+  \/\/ TODO: Remove method altogether and replace with calls to obj->klass() ?\n+  \/\/ OTOH, we may eventually get rid of locking in header, and then no\n+  \/\/ longer have to deal with that anymore.\n+#ifdef _LP64\n+  return obj->klass();\n+#else\n+  return obj->_klass;\n+#endif\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":9,"deletions":21,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -55,4 +55,3 @@\n-  union _metadata {\n-    Klass*      _klass;\n-    narrowKlass _compressed_klass;\n-  } _metadata;\n+#ifndef _LP64\n+  Klass*            _klass;\n+#endif\n@@ -62,0 +61,1 @@\n+  inline markWord  mark_acquire()  const;\n@@ -68,0 +68,1 @@\n+  static inline void release_set_mark(HeapWord* mem, markWord m);\n@@ -71,0 +72,2 @@\n+  inline markWord resolve_mark() const;\n+\n@@ -79,1 +82,1 @@\n-  void set_narrow_klass(narrowKlass nk) NOT_CDS_JAVA_HEAP_RETURN;\n+#ifndef _LP64\n@@ -82,5 +85,1 @@\n-\n-  \/\/ For klass field compression\n-  inline int klass_gap() const;\n-  inline void set_klass_gap(int z);\n-  static inline void set_klass_gap(HeapWord* mem, int z);\n+#endif\n@@ -252,0 +251,1 @@\n+  inline void forward_to_self();\n@@ -258,0 +258,1 @@\n+  inline oop forward_to_self_atomic(markWord compare, atomic_memory_order order = memory_order_conservative);\n@@ -260,0 +261,1 @@\n+  inline oop forwardee(markWord header) const;\n@@ -301,2 +303,0 @@\n-  static bool has_klass_gap();\n-\n@@ -305,4 +305,7 @@\n-  static int klass_offset_in_bytes()     { return offset_of(oopDesc, _metadata._klass); }\n-  static int klass_gap_offset_in_bytes() {\n-    assert(has_klass_gap(), \"only applicable to compressed klass pointers\");\n-    return klass_offset_in_bytes() + sizeof(narrowKlass);\n+  static int klass_offset_in_bytes()     {\n+#ifdef _LP64\n+    STATIC_ASSERT(markWord::klass_shift % 8 == 0);\n+    return mark_offset_in_bytes() + markWord::klass_shift \/ 8;\n+#else\n+    return offset_of(oopDesc, _klass);\n+#endif\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":19,"deletions":16,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -52,0 +52,3 @@\n+markWord oopDesc::mark_acquire() const {\n+  return Atomic::load_acquire(&_mark);\n+}\n@@ -68,0 +71,4 @@\n+void oopDesc::release_set_mark(HeapWord* mem, markWord m) {\n+  Atomic::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n+}\n+\n@@ -77,0 +84,8 @@\n+markWord oopDesc::resolve_mark() const {\n+  markWord hdr = mark();\n+  if (hdr.has_displaced_mark_helper()) {\n+    hdr = hdr.displaced_mark_helper();\n+  }\n+  return hdr;\n+}\n+\n@@ -78,1 +93,8 @@\n-  set_mark(markWord::prototype_for_klass(klass()));\n+#ifdef _LP64\n+  markWord header = resolve_mark();\n+  assert(UseCompressedClassPointers, \"expect compressed klass pointers\");\n+  header = markWord((header.value() & markWord::klass_mask_in_place) | markWord::prototype().value());\n+#else\n+  markWord header = markWord::prototype();\n+#endif\n+  set_mark(header);\n@@ -82,5 +104,7 @@\n-  if (UseCompressedClassPointers) {\n-    return CompressedKlassPointers::decode_not_null(_metadata._compressed_klass);\n-  } else {\n-    return _metadata._klass;\n-  }\n+#ifdef _LP64\n+  assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+  markWord header = resolve_mark();\n+  return header.klass();\n+#else\n+  return _klass;\n+#endif\n@@ -90,5 +114,7 @@\n-  if (UseCompressedClassPointers) {\n-    return CompressedKlassPointers::decode(_metadata._compressed_klass);\n-  } else {\n-    return _metadata._klass;\n-  }\n+#ifdef _LP64\n+  assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+  markWord header = resolve_mark();\n+  return header.klass_or_null();\n+#else\n+  return _klass;\n+#endif\n@@ -98,8 +124,13 @@\n-  if (UseCompressedClassPointers) {\n-    narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n-    return CompressedKlassPointers::decode(nklass);\n-  } else {\n-    return Atomic::load_acquire(&_metadata._klass);\n-  }\n-}\n-\n+#ifdef _LP64\n+  assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+  markWord header = mark_acquire();\n+  if (header.has_displaced_mark_helper()) {\n+    header = header.displaced_mark_helper();\n+   }\n+  return header.klass_or_null();\n+#else\n+  return Atomic::load_acquire(&_klass);\n+#endif\n+}\n+\n+#ifndef _LP64\n@@ -108,5 +139,1 @@\n-  if (UseCompressedClassPointers) {\n-    _metadata._compressed_klass = CompressedKlassPointers::encode_not_null(k);\n-  } else {\n-    _metadata._klass = k;\n-  }\n+  _klass = k;\n@@ -125,14 +152,1 @@\n-\n-int oopDesc::klass_gap() const {\n-  return *(int*)(((intptr_t)this) + klass_gap_offset_in_bytes());\n-}\n-\n-void oopDesc::set_klass_gap(HeapWord* mem, int v) {\n-  if (UseCompressedClassPointers) {\n-    *(int*)(((char*)mem) + klass_gap_offset_in_bytes()) = v;\n-  }\n-}\n-\n-void oopDesc::set_klass_gap(int v) {\n-  set_klass_gap((HeapWord*)this, v);\n-}\n+#endif\n@@ -277,1 +291,1 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversable\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n@@ -289,0 +303,17 @@\n+void oopDesc::forward_to_self() {\n+#ifdef _LP64\n+  verify_forwardee(this);\n+  markWord m = mark();\n+  \/\/ If mark is displaced, we need to preserve the Klass* from real header.\n+  assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  m = m.set_self_forwarded();\n+  assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversable\");\n+  set_mark(m);\n+#else\n+  forward_to(oop(this));\n+#endif\n+}\n+\n@@ -292,1 +323,20 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversable\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n+  markWord old_mark = cas_set_mark(m, compare, order);\n+  if (old_mark == compare) {\n+    return NULL;\n+  } else {\n+    return forwardee(old_mark);\n+  }\n+}\n+\n+oop oopDesc::forward_to_self_atomic(markWord compare, atomic_memory_order order) {\n+#ifdef _LP64\n+  verify_forwardee(this);\n+  markWord m = compare;\n+  \/\/ If mark is displaced, we need to preserve the Klass* from real header.\n+  assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  m = m.set_self_forwarded();\n+  assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversable\");\n@@ -297,1 +347,2 @@\n-    return cast_to_oop(old_mark.decode_pointer());\n+    assert(old_mark.is_marked(), \"must be marked here\");\n+    return forwardee(old_mark);\n@@ -299,0 +350,3 @@\n+#else\n+  return forward_to_atomic(oop(this), compare, order);\n+#endif\n@@ -305,1 +359,14 @@\n-  return cast_to_oop(mark().decode_pointer());\n+  return forwardee(mark());\n+}\n+\n+oop oopDesc::forwardee(markWord header) const {\n+  assert(header.is_marked(), \"must be forwarded\");\n+#ifdef _LP64\n+  if (header.self_forwarded()) {\n+    return cast_to_oop(this);\n+  } else\n+#endif\n+  {\n+    assert(header.is_marked(), \"only decode when actually forwarded\");\n+    return cast_to_oop(header.decode_pointer());\n+  }\n@@ -360,1 +427,0 @@\n-  assert(k == klass(), \"wrong klass\");\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":109,"deletions":43,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -231,1 +231,0 @@\n-  assert(obj->is_typeArray(),\"must be a type array\");\n@@ -233,1 +232,1 @@\n-  return t->object_size();\n+  return t->object_size(this);\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -38,1 +38,0 @@\n-  assert(obj->is_typeArray(),\"must be a type array\");\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.inline.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -134,1 +134,1 @@\n-  inline int object_size();\n+  inline int object_size(const TypeArrayKlass* tk) const;\n","filename":"src\/hotspot\/share\/oops\/typeArrayOop.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,2 +34,1 @@\n-int typeArrayOopDesc::object_size() {\n-  TypeArrayKlass* tk = TypeArrayKlass::cast(klass());\n+int typeArrayOopDesc::object_size(const TypeArrayKlass* tk) const {\n","filename":"src\/hotspot\/share\/oops\/typeArrayOop.inline.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,55 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/codeBuffer.hpp\"\n+#include \"code\/codeBlob.hpp\"\n+#include \"opto\/c2_CodeStubs.hpp\"\n+#include \"opto\/c2_MacroAssembler.hpp\"\n+#include \"opto\/compile.hpp\"\n+#include \"opto\/output.hpp\"\n+\n+C2CodeStubList::C2CodeStubList() :\n+    _stubs(Compile::current()->comp_arena(), 2, 0, NULL) {}\n+\n+void C2CodeStubList::emit(CodeBuffer& cb) {\n+  C2_MacroAssembler masm(&cb);\n+  for (int i = _stubs.length() - 1; i >= 0; i--) {\n+    C2CodeStub* stub = _stubs.at(i);\n+    int max_size = stub->max_size();\n+    \/\/ Make sure there is enough space in the code buffer\n+    if (cb.insts()->maybe_expand_to_ensure_remaining(max_size) && cb.blob() == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n+\n+    DEBUG_ONLY(int size_before = cb.insts_size();)\n+\n+    stub->emit(masm);\n+\n+    DEBUG_ONLY(int actual_size = cb.insts_size() - size_before;)\n+    assert(max_size >= actual_size, \"Expected stub size (%d) must be larger than or equal to actual stub size (%d)\", max_size, actual_size);\n+  }\n+  _stubs.clear();\n+}\n","filename":"src\/hotspot\/share\/opto\/c2_CodeStubs.cpp","additions":55,"deletions":0,"binary":false,"changes":55,"status":"added"},{"patch":"@@ -0,0 +1,93 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"asm\/assembler.hpp\"\n+#include \"asm\/codeBuffer.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"opto\/c2_MacroAssembler.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+\n+#ifndef SHARE_OPTO_C2_CODESTUBS_HPP\n+#define SHARE_OPTO_C2_CODESTUBS_HPP\n+\n+class C2CodeStub : public ResourceObj {\n+private:\n+  Label _entry;\n+  Label _continuation;\n+\n+protected:\n+  C2CodeStub() :\n+    _entry(),\n+    _continuation() {}\n+  ~C2CodeStub() {}\n+\n+public:\n+  Label& entry()        { return _entry; }\n+  Label& continuation() { return _continuation; }\n+\n+  virtual void emit(C2_MacroAssembler& masm) = 0;\n+  virtual int max_size() const = 0;\n+};\n+\n+class C2CodeStubList {\n+private:\n+  GrowableArray<C2CodeStub*> _stubs;\n+\n+public:\n+  C2CodeStubList();\n+  ~C2CodeStubList() {}\n+  void add_stub(C2CodeStub* stub) { _stubs.append(stub); }\n+  void emit(CodeBuffer& cb);\n+};\n+\n+class C2SafepointPollStub : public C2CodeStub {\n+private:\n+  uintptr_t _safepoint_offset;\n+public:\n+  C2SafepointPollStub(uintptr_t safepoint_offset) :\n+    _safepoint_offset(safepoint_offset) {}\n+  int max_size() const;\n+  void emit(C2_MacroAssembler& masm);\n+};\n+\n+class C2CheckLockStackStub : public C2CodeStub {\n+public:\n+  C2CheckLockStackStub() : C2CodeStub() {}\n+  int max_size() const;\n+  void emit(C2_MacroAssembler& masm);\n+};\n+\n+#ifdef _LP64\n+class C2LoadNKlassStub : public C2CodeStub {\n+private:\n+  Register _dst;\n+public:\n+  C2LoadNKlassStub(Register dst) : C2CodeStub(), _dst(dst) {}\n+  Register dst() { return _dst; }\n+  int max_size() const;\n+  void emit(C2_MacroAssembler& masm);\n+};\n+#endif\n+\n+#endif \/\/ SHARE_OPTO_C2_CODESTUBS_HPP\n","filename":"src\/hotspot\/share\/opto\/c2_CodeStubs.hpp","additions":93,"deletions":0,"binary":false,"changes":93,"status":"added"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n","filename":"src\/hotspot\/share\/opto\/c2_MacroAssembler.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1677,9 +1677,3 @@\n-  Node* mark_node = NULL;\n-  \/\/ For now only enable fast locking for non-array types\n-  if (UseBiasedLocking && Opcode() == Op_Allocate) {\n-    Node* klass_node = in(AllocateNode::KlassNode);\n-    Node* proto_adr = phase->transform(new AddPNode(klass_node, klass_node, phase->MakeConX(in_bytes(Klass::prototype_header_offset()))));\n-    mark_node = LoadNode::make(*phase, control, mem, proto_adr, TypeRawPtr::BOTTOM, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n-  } else {\n-    mark_node = phase->MakeConX(markWord::prototype().value());\n-  }\n+  Node* klass_node = in(AllocateNode::KlassNode);\n+  Node* proto_adr = phase->transform(new AddPNode(klass_node, klass_node, phase->MakeConX(in_bytes(Klass::prototype_header_offset()))));\n+  Node* mark_node = LoadNode::make(*phase, control, mem, proto_adr, TypeRawPtr::BOTTOM, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":3,"deletions":9,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -990,0 +990,1 @@\n+  reset_max_monitors();\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -305,0 +305,1 @@\n+  uint                  _max_monitors;          \/\/ Keep track of maximum number of active monitors in this compilation\n@@ -599,0 +600,4 @@\n+  void          push_monitor() { _max_monitors++; }\n+  void          reset_max_monitors() { _max_monitors = 0; }\n+  uint          max_monitors() { return _max_monitors; }\n+\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -6503,1 +6503,1 @@\n-    state = get_state_from_digest_object(digestBase_obj, \"[I\");\n+    state = get_state_from_digest_object(digestBase_obj, T_INT);\n@@ -6509,1 +6509,1 @@\n-    state = get_state_from_digest_object(digestBase_obj, \"[I\");\n+    state = get_state_from_digest_object(digestBase_obj, T_INT);\n@@ -6515,1 +6515,1 @@\n-    state = get_state_from_digest_object(digestBase_obj, \"[I\");\n+    state = get_state_from_digest_object(digestBase_obj, T_INT);\n@@ -6521,1 +6521,1 @@\n-    state = get_state_from_digest_object(digestBase_obj, \"[J\");\n+    state = get_state_from_digest_object(digestBase_obj, T_LONG);\n@@ -6527,1 +6527,1 @@\n-    state = get_state_from_digest_object(digestBase_obj, \"[B\");\n+    state = get_state_from_digest_object(digestBase_obj, T_BYTE);\n@@ -6591,1 +6591,1 @@\n-  const char* state_type = \"[I\";\n+  BasicType elem_type = T_INT;\n@@ -6620,1 +6620,1 @@\n-      state_type = \"[J\";\n+      elem_type = T_LONG;\n@@ -6628,1 +6628,1 @@\n-      state_type = \"[B\";\n+      elem_type = T_BYTE;\n@@ -6646,1 +6646,1 @@\n-    return inline_digestBase_implCompressMB(digestBase_obj, instklass_digestBase, state_type, stub_addr, stub_name, src_start, ofs, limit);\n+    return inline_digestBase_implCompressMB(digestBase_obj, instklass_digestBase, elem_type, stub_addr, stub_name, src_start, ofs, limit);\n@@ -6653,1 +6653,1 @@\n-                                                      const char* state_type, address stubAddr, const char *stubName,\n+                                                      BasicType elem_type, address stubAddr, const char *stubName,\n@@ -6660,1 +6660,1 @@\n-  Node* state = get_state_from_digest_object(digest_obj, state_type);\n+  Node* state = get_state_from_digest_object(digest_obj, elem_type);\n@@ -6691,1 +6691,8 @@\n-Node * LibraryCallKit::get_state_from_digest_object(Node *digest_object, const char *state_type) {\n+Node * LibraryCallKit::get_state_from_digest_object(Node *digest_object, BasicType elem_type) {\n+  const char* state_type;\n+  switch (elem_type) {\n+    case T_BYTE: state_type = \"[B\"; break;\n+    case T_INT:  state_type = \"[I\"; break;\n+    case T_LONG: state_type = \"[J\"; break;\n+    default: ShouldNotReachHere();\n+  }\n@@ -6697,1 +6704,1 @@\n-  Node* state = array_element_address(digest_state, intcon(0), T_INT);\n+  Node* state = array_element_address(digest_state, intcon(0), elem_type);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":20,"deletions":13,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -283,1 +283,1 @@\n-                                        const char* state_type, address stubAddr, const char *stubName,\n+                                        BasicType elem_type, address stubAddr, const char *stubName,\n@@ -285,1 +285,1 @@\n-  Node* get_state_from_digest_object(Node *digestBase_object, const char* state_type);\n+  Node* get_state_from_digest_object(Node *digestBase_object, BasicType elem_type);\n@@ -342,1 +342,1 @@\n-    if (UseAVX >= 2) {\n+    if (false && UseAVX >= 2) {\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -191,0 +191,2 @@\n+  C->push_monitor();\n+\n","filename":"src\/hotspot\/share\/opto\/locknode.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1673,0 +1673,1 @@\n+#ifndef _LP64\n@@ -1674,0 +1675,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -228,67 +228,0 @@\n-volatile int C2SafepointPollStubTable::_stub_size = 0;\n-\n-Label& C2SafepointPollStubTable::add_safepoint(uintptr_t safepoint_offset) {\n-  C2SafepointPollStub* entry = new (Compile::current()->comp_arena()) C2SafepointPollStub(safepoint_offset);\n-  _safepoints.append(entry);\n-  return entry->_stub_label;\n-}\n-\n-void C2SafepointPollStubTable::emit(CodeBuffer& cb) {\n-  MacroAssembler masm(&cb);\n-  for (int i = _safepoints.length() - 1; i >= 0; i--) {\n-    \/\/ Make sure there is enough space in the code buffer\n-    if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == NULL) {\n-      ciEnv::current()->record_failure(\"CodeCache is full\");\n-      return;\n-    }\n-\n-    C2SafepointPollStub* entry = _safepoints.at(i);\n-    emit_stub(masm, entry);\n-  }\n-}\n-\n-int C2SafepointPollStubTable::stub_size_lazy() const {\n-  int size = Atomic::load(&_stub_size);\n-\n-  if (size != 0) {\n-    return size;\n-  }\n-\n-  Compile* const C = Compile::current();\n-  BufferBlob* const blob = C->output()->scratch_buffer_blob();\n-  CodeBuffer cb(blob->content_begin(), C->output()->scratch_buffer_code_size());\n-  MacroAssembler masm(&cb);\n-  C2SafepointPollStub* entry = _safepoints.at(0);\n-  emit_stub(masm, entry);\n-  size += cb.insts_size();\n-\n-  Atomic::store(&_stub_size, size);\n-\n-  return size;\n-}\n-\n-int C2SafepointPollStubTable::estimate_stub_size() const {\n-  if (_safepoints.length() == 0) {\n-    return 0;\n-  }\n-\n-  int result = stub_size_lazy() * _safepoints.length();\n-\n-#ifdef ASSERT\n-  Compile* const C = Compile::current();\n-  BufferBlob* const blob = C->output()->scratch_buffer_blob();\n-  int size = 0;\n-\n-  for (int i = _safepoints.length() - 1; i >= 0; i--) {\n-    CodeBuffer cb(blob->content_begin(), C->output()->scratch_buffer_code_size());\n-    MacroAssembler masm(&cb);\n-    C2SafepointPollStub* entry = _safepoints.at(i);\n-    emit_stub(masm, entry);\n-    size += cb.insts_size();\n-  }\n-  assert(size == result, \"stubs should not have variable size\");\n-#endif\n-\n-  return result;\n-}\n-\n@@ -301,0 +234,1 @@\n+    _stub_list(),\n@@ -1316,1 +1250,0 @@\n-  stub_req += safepoint_poll_table()->estimate_stub_size();\n@@ -1823,2 +1756,2 @@\n-  \/\/ Fill in stubs for calling the runtime from safepoint polls.\n-  safepoint_poll_table()->emit(*cb);\n+  \/\/ Fill in stubs.\n+  _stub_list.emit(*cb);\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":3,"deletions":70,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"opto\/c2_CodeStubs.hpp\"\n@@ -75,41 +76,0 @@\n-class C2SafepointPollStubTable {\n-private:\n-  struct C2SafepointPollStub: public ResourceObj {\n-    uintptr_t _safepoint_offset;\n-    Label     _stub_label;\n-    Label     _trampoline_label;\n-    C2SafepointPollStub(uintptr_t safepoint_offset) :\n-      _safepoint_offset(safepoint_offset),\n-      _stub_label(),\n-      _trampoline_label() {}\n-  };\n-\n-  GrowableArray<C2SafepointPollStub*> _safepoints;\n-\n-  static volatile int _stub_size;\n-\n-  void emit_stub_impl(MacroAssembler& masm, C2SafepointPollStub* entry) const;\n-\n-  \/\/ The selection logic below relieves the need to add dummy files to unsupported platforms.\n-  template <bool enabled>\n-  typename EnableIf<enabled>::type\n-  select_emit_stub(MacroAssembler& masm, C2SafepointPollStub* entry) const {\n-    emit_stub_impl(masm, entry);\n-  }\n-\n-  template <bool enabled>\n-  typename EnableIf<!enabled>::type\n-  select_emit_stub(MacroAssembler& masm, C2SafepointPollStub* entry) const {}\n-\n-  void emit_stub(MacroAssembler& masm, C2SafepointPollStub* entry) const {\n-    select_emit_stub<VM_Version::supports_stack_watermark_barrier()>(masm, entry);\n-  }\n-\n-  int stub_size_lazy() const;\n-\n-public:\n-  Label& add_safepoint(uintptr_t safepoint_offset);\n-  int estimate_stub_size() const;\n-  void emit(CodeBuffer& cb);\n-};\n-\n@@ -124,1 +84,1 @@\n-  C2SafepointPollStubTable _safepoint_poll_table;\/\/ Table for safepoint polls\n+  C2CodeStubList         _stub_list;             \/\/ List of code stubs\n@@ -172,2 +132,2 @@\n-  \/\/ Safepoint poll table\n-  C2SafepointPollStubTable* safepoint_poll_table() { return &_safepoint_poll_table; }\n+  \/\/ Code stubs list\n+  void add_stub(C2CodeStub* stub) { _stub_list.add_stub(stub); }\n","filename":"src\/hotspot\/share\/opto\/output.hpp","additions":4,"deletions":44,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -426,1 +426,5 @@\n-  _tf = TypeFunc::make(method());\n+  if (parse_method->is_synchronized()) {\n+    C->push_monitor();\n+  }\n+\n+   _tf = TypeFunc::make(method());\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -305,1 +305,1 @@\n-    int size = ((typeArrayOop)result)->object_size();\n+    int size = TypeArrayKlass::cast(array_type)->oop_size(result);\n@@ -307,1 +307,1 @@\n-    const size_t hs = arrayOopDesc::header_size(elem_type);\n+    const size_t hs_bytes = arrayOopDesc::base_offset_in_bytes(elem_type);\n@@ -309,1 +309,1 @@\n-    const size_t aligned_hs = align_object_offset(hs);\n+    const size_t aligned_hs_bytes = align_up(hs_bytes, BytesPerLong);\n@@ -311,2 +311,2 @@\n-    if (aligned_hs > hs) {\n-      Copy::zero_to_words(obj+hs, aligned_hs-hs);\n+    if (aligned_hs_bytes > hs_bytes) {\n+      Copy::zero_to_bytes(obj + hs_bytes, aligned_hs_bytes - hs_bytes);\n@@ -315,0 +315,1 @@\n+    const size_t aligned_hs = aligned_hs_bytes \/ HeapWordSize;\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -4564,1 +4564,2 @@\n-    int header_size = objArrayOopDesc::header_size() * wordSize;\n+    BasicType basic_elem_type = elem()->basic_type();\n+    int header_size = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -973,35 +973,1 @@\n-    \/\/ Revoke any biases before querying the mark word\n-    BiasedLocking::revoke_at_safepoint(hobj);\n-\n-    address owner = NULL;\n-    {\n-      markWord mark = hobj()->mark();\n-\n-      if (!mark.has_monitor()) {\n-        \/\/ this object has a lightweight monitor\n-\n-        if (mark.has_locker()) {\n-          owner = (address)mark.locker(); \/\/ save the address of the Lock word\n-        }\n-        \/\/ implied else: no owner\n-      } else {\n-        \/\/ this object has a heavyweight monitor\n-        mon = mark.monitor();\n-\n-        \/\/ The owner field of a heavyweight monitor may be NULL for no\n-        \/\/ owner, a JavaThread * or it may still be the address of the\n-        \/\/ Lock word in a JavaThread's stack. A monitor can be inflated\n-        \/\/ by a non-owning JavaThread, but only the owning JavaThread\n-        \/\/ can change the owner field from the Lock word to the\n-        \/\/ JavaThread * and it may not have done that yet.\n-        owner = (address)mon->owner();\n-      }\n-    }\n-\n-    if (owner != NULL) {\n-      \/\/ This monitor is owned so we have to find the owning JavaThread.\n-      owning_thread = Threads::owning_thread_from_monitor_owner(tlh.list(), owner);\n-      assert(owning_thread != NULL, \"owning JavaThread must not be NULL\");\n-      Handle     th(current_thread, owning_thread->threadObj());\n-      ret.owner = (jthread)jni_reference(calling_thread, th);\n-    }\n+    owning_thread = ObjectSynchronizer::get_lock_owner(tlh.list(), hobj);\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnvBase.cpp","additions":1,"deletions":35,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -1470,1 +1470,4 @@\n-  o->set_mark(markWord::prototype().set_marked());\n+  if (mark.has_displaced_mark_helper()) {\n+    mark = mark.displaced_mark_helper();\n+  }\n+  o->set_mark(mark.set_marked());\n","filename":"src\/hotspot\/share\/prims\/jvmtiTagMap.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -69,1 +69,1 @@\n-  ( arrayOopDesc::header_size(T_DOUBLE) * HeapWordSize \\\n+  ( arrayOopDesc::base_offset_in_bytes(T_DOUBLE) \\\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3156,0 +3156,6 @@\n+  \/\/ Lilliput requires fast-locking.\n+  FLAG_SET_DEFAULT(UseFastLocking, true);\n+  FLAG_SET_DEFAULT(UseBiasedLocking, false);\n+#ifdef _LP64\n+  FLAG_SET_DEFAULT(UseCompressedClassPointers, true);\n+#endif\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -128,1 +128,1 @@\n-  product(bool, UseCompressedClassPointers, false,                          \\\n+  product(bool, UseCompressedClassPointers, true,                           \\\n@@ -2088,0 +2088,9 @@\n+  product(bool, HeapObjectStats, false, DIAGNOSTIC,                         \\\n+             \"Enable gathering of heap object statistics\")                  \\\n+                                                                            \\\n+  product(size_t, HeapObjectStatsSamplingInterval, 500, DIAGNOSTIC,         \\\n+             \"Heap object statistics sampling interval (ms)\")               \\\n+                                                                            \\\n+  product(bool, UseFastLocking, false, EXPERIMENTAL,                        \\\n+                \"Use fast-locking instead of stack-locking\")                \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -0,0 +1,85 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"runtime\/lockStack.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"utilities\/copy.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+LockStack::LockStack() :\n+        _base(UseHeavyMonitors ? NULL : NEW_C_HEAP_ARRAY(oop, INITIAL_CAPACITY, mtSynchronizer)),\n+        _limit(_base + INITIAL_CAPACITY),\n+        _current(_base) {\n+}\n+\n+LockStack::~LockStack() {\n+  if (!UseHeavyMonitors) {\n+    FREE_C_HEAP_ARRAY(oop, _base);\n+  }\n+}\n+\n+#ifndef PRODUCT\n+void LockStack::validate(const char* msg) const {\n+  assert(!UseHeavyMonitors, \"never use lock-stack when fast-locking is disabled\");\n+  for (oop* loc1 = _base; loc1 < _current - 1; loc1++) {\n+    for (oop* loc2 = loc1 + 1; loc2 < _current; loc2++) {\n+      assert(*loc1 != *loc2, \"entries must be unique: %s\", msg);\n+    }\n+  }\n+}\n+#endif\n+\n+void LockStack::grow(size_t min_capacity) {\n+  \/\/ Grow stack.\n+  assert(_limit > _base, \"invariant\");\n+  size_t capacity = _limit - _base;\n+  size_t index = _current - _base;\n+  size_t new_capacity = MAX2(min_capacity, capacity * 2);\n+  oop* new_stack = NEW_C_HEAP_ARRAY(oop, new_capacity, mtSynchronizer);\n+  for (size_t i = 0; i < index; i++) {\n+    *(new_stack + i) = *(_base + i);\n+  }\n+  FREE_C_HEAP_ARRAY(oop, _base);\n+  _base = new_stack;\n+  _limit = _base + new_capacity;\n+  _current = _base + index;\n+  assert(_current < _limit, \"must fit after growing\");\n+  assert((_limit - _base) >= (ptrdiff_t) min_capacity, \"must grow enough\");\n+}\n+\n+void LockStack::grow() {\n+  grow((_limit - _base) + 1);\n+}\n+\n+void LockStack::grow(oop* required_limit) {\n+  grow(required_limit - _base);\n+}\n+\n+void LockStack::ensure_lock_stack_size(oop* _required_limit) {\n+  JavaThread* jt = JavaThread::current();\n+  jt->lock_stack().grow(_required_limit);\n+}\n","filename":"src\/hotspot\/share\/runtime\/lockStack.cpp","additions":85,"deletions":0,"binary":false,"changes":85,"status":"added"},{"patch":"@@ -0,0 +1,69 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_LOCKSTACK_HPP\n+#define SHARE_RUNTIME_LOCKSTACK_HPP\n+\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/sizes.hpp\"\n+\n+class Thread;\n+class OopClosure;\n+\n+class LockStack {\n+  friend class VMStructs;\n+private:\n+  static const size_t INITIAL_CAPACITY = 1;\n+  oop* _base;\n+  oop* _limit;\n+  oop* _current;\n+\n+  void grow();\n+  void grow(size_t min_capacity);\n+  void grow(oop* required_limit);\n+\n+  void validate(const char* msg) const PRODUCT_RETURN;\n+public:\n+  static ByteSize current_offset()    { return byte_offset_of(LockStack, _current); }\n+  static ByteSize base_offset()       { return byte_offset_of(LockStack, _base); }\n+  static ByteSize limit_offset()      { return byte_offset_of(LockStack, _limit); }\n+\n+  static void ensure_lock_stack_size(oop* _required_limit);\n+\n+  LockStack();\n+  ~LockStack();\n+\n+  inline void push(oop o);\n+  inline oop pop();\n+  inline void remove(oop o);\n+\n+  inline bool contains(oop o) const;\n+\n+  \/\/ GC support\n+  inline void oops_do(OopClosure* cl);\n+\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_LOCKSTACK_HPP\n","filename":"src\/hotspot\/share\/runtime\/lockStack.hpp","additions":69,"deletions":0,"binary":false,"changes":69,"status":"added"},{"patch":"@@ -0,0 +1,93 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_LOCKSTACK_INLINE_HPP\n+#define SHARE_RUNTIME_LOCKSTACK_INLINE_HPP\n+\n+#include \"memory\/iterator.hpp\"\n+#include \"runtime\/lockStack.hpp\"\n+\n+inline void LockStack::push(oop o) {\n+  validate(\"pre-push\");\n+  assert(oopDesc::is_oop(o), \"must be\");\n+  assert(!contains(o), \"entries must be unique\");\n+  if (_current >= _limit) {\n+    grow();\n+  }\n+  *_current = o;\n+  _current++;\n+  validate(\"post-push\");\n+}\n+\n+inline oop LockStack::pop() {\n+  validate(\"pre-pop\");\n+  oop* new_loc = _current - 1;\n+  assert(new_loc < _current, \"underflow, probably unbalanced push\/pop\");\n+  _current = new_loc;\n+  oop o = *_current;\n+  assert(!contains(o), \"entries must be unique\");\n+  validate(\"post-pop\");\n+  return o;\n+}\n+\n+inline void LockStack::remove(oop o) {\n+  validate(\"pre-remove\");\n+  assert(contains(o), \"entry must be present\");\n+  for (oop* loc = _base; loc < _current; loc++) {\n+    if (*loc == o) {\n+      oop* last = _current - 1;\n+      for (; loc < last; loc++) {\n+        *loc = *(loc + 1);\n+      }\n+      _current--;\n+      break;\n+    }\n+  }\n+  assert(!contains(o), \"entries must be unique: \" PTR_FORMAT, p2i(o));\n+  validate(\"post-remove\");\n+}\n+\n+inline bool LockStack::contains(oop o) const {\n+  validate(\"pre-contains\");\n+  bool found = false;\n+  size_t i = 0;\n+  size_t found_i = 0;\n+  for (oop* loc = _current - 1; loc >= _base; loc--) {\n+    if (*loc == o) {\n+      return true;\n+    }\n+  }\n+  validate(\"post-contains\");\n+  return false;\n+}\n+\n+inline void LockStack::oops_do(OopClosure* cl) {\n+  validate(\"pre-oops-do\");\n+  for (oop* loc = _base; loc < _current; loc++) {\n+    cl->do_oop(loc);\n+  }\n+  validate(\"post-oops-do\");\n+}\n+\n+#endif \/\/ SHARE_RUNTIME_LOCKSTACK_INLINE_HPP\n","filename":"src\/hotspot\/share\/runtime\/lockStack.inline.hpp","additions":93,"deletions":0,"binary":false,"changes":93,"status":"added"},{"patch":"@@ -337,1 +337,1 @@\n-  if (current->is_lock_owned((address)cur)) {\n+  if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n@@ -603,0 +603,16 @@\n+\/\/ We might access the dead object headers for parsable heap walk, make sure\n+\/\/ headers are in correct shape, e.g. monitors deflated.\n+void ObjectMonitor::maybe_deflate_dead(oop* p) {\n+  oop obj = *p;\n+  assert(obj != NULL, \"must not yet been cleared\");\n+  markWord mark = obj->mark();\n+  if (mark.has_monitor()) {\n+    ObjectMonitor* monitor = mark.monitor();\n+    if (p == monitor->_object.ptr_raw()) {\n+      assert(monitor->object_peek() == obj, \"lock object must match\");\n+      markWord dmw = monitor->header();\n+      obj->set_mark(dmw);\n+    }\n+  }\n+}\n+\n@@ -1138,1 +1154,1 @@\n-    if (current->is_lock_owned((address)cur)) {\n+    if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n@@ -1358,1 +1374,1 @@\n-    if (current->is_lock_owned((address)cur)) {\n+    if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n@@ -1407,0 +1423,1 @@\n+  assert(cur != ANONYMOUS_OWNER, \"no anon owner here\");\n@@ -1410,1 +1427,1 @@\n-  if (current->is_lock_owned((address)cur)) {\n+  if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":21,"deletions":4,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -148,2 +148,10 @@\n-  \/\/ Used by async deflation as a marker in the _owner field:\n-  #define DEFLATER_MARKER reinterpret_cast<void*>(-1)\n+  \/\/ Used by async deflation as a marker in the _owner field.\n+  \/\/ Note that the choice of the two markers is peculiar:\n+  \/\/ - They need to represent values that cannot be pointers. In particular,\n+  \/\/   we achieve this by using the lowest two bits\n+  \/\/ - ANONYMOUS_OWNER should be a small value, it is used in generated code\n+  \/\/   and small values encode much better\n+  \/\/ - We test for anonymous owner by testing for the lowest bit, therefore\n+  \/\/   DEFLATER_MARKER must *not* have that bit set.\n+  #define DEFLATER_MARKER reinterpret_cast<void*>(2)\n+  #define ANONYMOUS_OWNER reinterpret_cast<void*>(1)\n@@ -249,0 +257,1 @@\n+  bool      has_owner() const;\n@@ -266,0 +275,12 @@\n+  void set_owner_anonymous() {\n+    set_owner_from(NULL, ANONYMOUS_OWNER);\n+  }\n+\n+  bool is_owner_anonymous() const {\n+    return owner_raw() == ANONYMOUS_OWNER;\n+  }\n+\n+  void set_owner_from_anonymous(Thread* owner) {\n+    set_owner_from(ANONYMOUS_OWNER, owner);\n+  }\n+\n@@ -334,0 +355,2 @@\n+  static void maybe_deflate_dead(oop* p);\n+\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":25,"deletions":2,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -36,3 +37,11 @@\n-  void* owner = owner_raw();\n-  if (current == owner || current->is_lock_owned((address)owner)) {\n-    return 1;\n+  if (UseFastLocking) {\n+    if (is_owner_anonymous()) {\n+      return current->lock_stack().contains(object()) ? 1 : 0;\n+    } else {\n+      return current == owner_raw() ? 1 : 0;\n+    }\n+  } else {\n+    void* owner = owner_raw();\n+    if (current == owner || current->is_lock_owned((address)owner)) {\n+      return 1;\n+    }\n@@ -59,0 +68,5 @@\n+inline bool ObjectMonitor::has_owner() const {\n+  void* owner = owner_raw();\n+  return owner != NULL && owner != DEFLATER_MARKER;\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.inline.hpp","additions":17,"deletions":3,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -41,0 +42,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -273,1 +275,2 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+  if ((mark.is_fast_locked() && current->lock_stack().contains(oop(obj))) ||\n+      (mark.has_locker() && current->is_lock_owned((address)mark.locker()))) {\n@@ -356,1 +359,3 @@\n-    lock->set_displaced_header(markWord::unused_mark());\n+    if (!UseFastLocking) {\n+      lock->set_displaced_header(markWord::unused_mark());\n+    }\n@@ -434,6 +439,26 @@\n-  if (UseBiasedLocking) {\n-    BiasedLocking::revoke(current, obj);\n-  }\n-\n-  markWord mark = obj->mark();\n-  assert(!mark.has_bias_pattern(), \"should not see bias pattern here\");\n+  if (UseFastLocking) {\n+    LockStack& lock_stack = current->lock_stack();\n+\n+    markWord header = obj()->mark_acquire();\n+    while (true) {\n+      if (header.is_neutral()) {\n+        assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n+        \/\/ Try to swing into 'fast-locked' state without inflating.\n+        markWord locked_header = header.set_fast_locked();\n+        markWord witness = obj()->cas_set_mark(locked_header, header);\n+        if (witness == header) {\n+          \/\/ Successfully fast-locked, push object to lock-stack and return.\n+          lock_stack.push(obj());\n+          return;\n+        }\n+        \/\/ Otherwise retry.\n+        header = witness;\n+      } else {\n+        \/\/ Fall-through to inflate-enter.\n+        break;\n+      }\n+    }\n+  } else {\n+    if (UseBiasedLocking) {\n+      BiasedLocking::revoke(current, obj);\n+    }\n@@ -441,5 +466,14 @@\n-  if (mark.is_neutral()) {\n-    \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n-    \/\/ be visible <= the ST performed by the CAS.\n-    lock->set_displaced_header(mark);\n-    if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n+    markWord mark = obj->mark();\n+    if (mark.is_neutral()) {\n+      \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n+      \/\/ be visible <= the ST performed by the CAS.\n+      lock->set_displaced_header(mark);\n+      if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n+        return;\n+      }\n+      \/\/ Fall through to inflate() ...\n+    } else if (mark.has_locker() &&\n+               current->is_lock_owned((address) mark.locker())) {\n+      assert(lock != mark.locker(), \"must not re-lock the same lock\");\n+      assert(lock != (BasicLock*) obj->mark().value(), \"don't relock with same BasicLock\");\n+      lock->set_displaced_header(markWord::from_pointer(NULL));\n@@ -448,7 +482,6 @@\n-    \/\/ Fall through to inflate() ...\n-  } else if (mark.has_locker() &&\n-             current->is_lock_owned((address)mark.locker())) {\n-    assert(lock != mark.locker(), \"must not re-lock the same lock\");\n-    assert(lock != (BasicLock*)obj->mark().value(), \"don't relock with same BasicLock\");\n-    lock->set_displaced_header(markWord::from_pointer(NULL));\n-    return;\n+\n+    \/\/ The object header will never be displaced to this lock,\n+    \/\/ so it does not matter what the value is, except that it\n+    \/\/ must be non-zero to avoid looking like a re-entrant lock,\n+    \/\/ and must not look locked either.\n+    lock->set_displaced_header(markWord::unused_mark());\n@@ -457,5 +490,0 @@\n-  \/\/ The object header will never be displaced to this lock,\n-  \/\/ so it does not matter what the value is, except that it\n-  \/\/ must be non-zero to avoid looking like a re-entrant lock,\n-  \/\/ and must not look locked either.\n-  lock->set_displaced_header(markWord::unused_mark());\n@@ -475,28 +503,12 @@\n-  \/\/ We cannot check for Biased Locking if we are racing an inflation.\n-  assert(mark == markWord::INFLATING() ||\n-         !mark.has_bias_pattern(), \"should not see bias pattern here\");\n-\n-  markWord dhw = lock->displaced_header();\n-  if (dhw.value() == 0) {\n-    \/\/ If the displaced header is NULL, then this exit matches up with\n-    \/\/ a recursive enter. No real work to do here except for diagnostics.\n-#ifndef PRODUCT\n-    if (mark != markWord::INFLATING()) {\n-      \/\/ Only do diagnostics if we are not racing an inflation. Simply\n-      \/\/ exiting a recursive enter of a Java Monitor that is being\n-      \/\/ inflated is safe; see the has_monitor() comment below.\n-      assert(!mark.is_neutral(), \"invariant\");\n-      assert(!mark.has_locker() ||\n-             current->is_lock_owned((address)mark.locker()), \"invariant\");\n-      if (mark.has_monitor()) {\n-        \/\/ The BasicLock's displaced_header is marked as a recursive\n-        \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n-        \/\/ This is a special case where the Java Monitor was inflated\n-        \/\/ after this thread entered the stack-lock recursively. When a\n-        \/\/ Java Monitor is inflated, we cannot safely walk the Java\n-        \/\/ Monitor owner's stack and update the BasicLocks because a\n-        \/\/ Java Monitor can be asynchronously inflated by a thread that\n-        \/\/ does not own the Java Monitor.\n-        ObjectMonitor* m = mark.monitor();\n-        assert(m->object()->mark() == mark, \"invariant\");\n-        assert(m->is_entered(current), \"invariant\");\n+    if (UseFastLocking) {\n+      if (mark.is_fast_locked()) {\n+        markWord unlocked_header = mark.set_unlocked();\n+        markWord witness = object->cas_set_mark(unlocked_header, mark);\n+        if (witness != mark) {\n+          \/\/ Another thread beat us, it can only have installed an anonymously locked monitor at this point.\n+          \/\/ Fetch that monitor, set owner correctly to this thread, and exit it (allowing waiting threads to enter).\n+          assert(witness.has_monitor(), \"must have monitor\");\n+          ObjectMonitor* monitor = witness.monitor();\n+          assert(monitor->is_owner_anonymous(), \"must be anonymous owner\");\n+          monitor->set_owner_from_anonymous(current);\n+          monitor->exit(current);\n@@ -504,0 +516,3 @@\n+      LockStack& lock_stack = current->lock_stack();\n+      lock_stack.remove(object);\n+      return;\n@@ -505,0 +520,27 @@\n+  } else {\n+    markWord dhw = lock->displaced_header();\n+    if (dhw.value() == 0) {\n+      \/\/ If the displaced header is NULL, then this exit matches up with\n+      \/\/ a recursive enter. No real work to do here except for diagnostics.\n+#ifndef PRODUCT\n+      if (mark != markWord::INFLATING()) {\n+        \/\/ Only do diagnostics if we are not racing an inflation. Simply\n+        \/\/ exiting a recursive enter of a Java Monitor that is being\n+        \/\/ inflated is safe; see the has_monitor() comment below.\n+        assert(!mark.is_neutral(), \"invariant\");\n+        assert(!mark.has_locker() ||\n+               current->is_lock_owned((address)mark.locker()), \"invariant\");\n+        if (mark.has_monitor()) {\n+          \/\/ The BasicLock's displaced_header is marked as a recursive\n+          \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n+          \/\/ This is a special case where the Java Monitor was inflated\n+          \/\/ after this thread entered the stack-lock recursively. When a\n+          \/\/ Java Monitor is inflated, we cannot safely walk the Java\n+          \/\/ Monitor owner's stack and update the BasicLocks because a\n+          \/\/ Java Monitor can be asynchronously inflated by a thread that\n+          \/\/ does not own the Java Monitor.\n+          ObjectMonitor* m = mark.monitor();\n+          assert(m->object()->mark() == mark, \"invariant\");\n+          assert(m->is_entered(current), \"invariant\");\n+        }\n+      }\n@@ -506,8 +548,0 @@\n-    return;\n-  }\n-\n-  if (mark == markWord::from_pointer(lock)) {\n-    \/\/ If the object is stack-locked by the current thread, try to\n-    \/\/ swing the displaced header from the BasicLock back to the mark.\n-    assert(dhw.is_neutral(), \"invariant\");\n-    if (object->cas_set_mark(dhw, mark) == mark) {\n@@ -516,0 +550,9 @@\n+\n+    if (mark == markWord::from_pointer(lock)) {\n+      \/\/ If the object is stack-locked by the current thread, try to\n+      \/\/ swing the displaced header from the BasicLock back to the mark.\n+      assert(dhw.is_neutral(), \"invariant\");\n+      if (object->cas_set_mark(dhw, mark) == mark) {\n+        return;\n+      }\n+    }\n@@ -522,0 +565,7 @@\n+  if (UseFastLocking && monitor->is_owner_anonymous()) {\n+    \/\/ It must be us. Pop lock object from lock stack.\n+    LockStack& lock_stack = current->lock_stack();\n+    oop popped = lock_stack.pop();\n+    assert(popped == object, \"must be owned by this thread\");\n+    monitor->set_owner_from_anonymous(current);\n+  }\n@@ -686,1 +736,2 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+  if ((mark.is_fast_locked() && current->lock_stack().contains(obj())) ||\n+      (mark.has_locker() && current->is_lock_owned((address)mark.locker()))) {\n@@ -705,1 +756,2 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+  if ((mark.is_fast_locked() && current->lock_stack().contains(obj())) ||\n+      (mark.has_locker() && current->is_lock_owned((address)mark.locker()))) {\n@@ -733,1 +785,1 @@\n-  if (!mark.is_being_inflated()) {\n+  if (!mark.is_being_inflated() || UseFastLocking) {\n@@ -848,0 +900,5 @@\n+static bool is_lock_owned(Thread* thread, oop obj) {\n+  assert(UseFastLocking, \"only call this with fast-locking enabled\");\n+  return thread->is_Java_thread() ? reinterpret_cast<JavaThread*>(thread)->lock_stack().contains(obj) : false;\n+}\n+\n@@ -921,1 +978,8 @@\n-    } else if (current->is_lock_owned((address)mark.locker())) {\n+    } else if (mark.is_fast_locked() && is_lock_owned(current, obj)) {\n+      \/\/ This is a fast lock owned by the calling thread so use the\n+      \/\/ markWord from the object.\n+      hash = mark.hash();\n+      if (hash != 0) {                  \/\/ if it has a hash, just return it\n+        return hash;\n+      }\n+    } else if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n@@ -1002,0 +1066,6 @@\n+\n+  \/\/ Fast-locking case.\n+  if (mark.is_fast_locked()) {\n+    return current->lock_stack().contains(h_obj());\n+  }\n+\n@@ -1026,2 +1096,0 @@\n-  address owner = NULL;\n-\n@@ -1032,1 +1100,5 @@\n-    owner = (address) mark.locker();\n+    return Threads::owning_thread_from_monitor_owner(t_list, (address) mark.locker());\n+  }\n+\n+  if (mark.is_fast_locked()) {\n+    return Threads::owning_thread_from_object(t_list, h_obj());\n@@ -1036,1 +1108,1 @@\n-  else if (mark.has_monitor()) {\n+  if (mark.has_monitor()) {\n@@ -1041,6 +1113,1 @@\n-    owner = (address) monitor->owner();\n-  }\n-\n-  if (owner != NULL) {\n-    \/\/ owning_thread_from_monitor_owner() may also return NULL here\n-    return Threads::owning_thread_from_monitor_owner(t_list, owner);\n+    return Threads::owning_thread_from_monitor(t_list, monitor);\n@@ -1049,5 +1116,0 @@\n-  \/\/ Unlocked case, header in place\n-  \/\/ Cannot have assertion since this object may have been\n-  \/\/ locked by another thread when reaching here.\n-  \/\/ assert(mark.is_neutral(), \"sanity check\");\n-\n@@ -1222,0 +1284,5 @@\n+      if (UseFastLocking && inf->is_owner_anonymous() && is_lock_owned(current, object)) {\n+        inf->set_owner_from_anonymous(current);\n+        assert(current->is_Java_thread(), \"must be Java thread\");\n+        reinterpret_cast<JavaThread*>(current)->lock_stack().remove(object);\n+      }\n@@ -1231,1 +1298,4 @@\n-    if (mark == markWord::INFLATING()) {\n+    \/\/ NOTE: We need to check UseFastLocking here, because with fast-locking, the header\n+    \/\/ may legitimately be zero: cleared lock-bits and all upper header bits zero.\n+    \/\/ With fast-locking, the INFLATING protocol is not used.\n+    if (mark == markWord::INFLATING() && !UseFastLocking) {\n@@ -1247,0 +1317,43 @@\n+    if (mark.is_fast_locked()) {\n+      assert(UseFastLocking, \"can only happen with fast-locking\");\n+      ObjectMonitor* monitor = new ObjectMonitor(object);\n+      monitor->set_header(mark.set_unlocked());\n+      assert(current->is_Java_thread(), \"must be Java thread\");\n+      bool own = is_lock_owned(current, object);\n+      if (own) {\n+        \/\/ Owned by us.\n+        monitor->set_owner_from(NULL, current);\n+      } else {\n+        \/\/ Owned by somebody else.\n+        monitor->set_owner_anonymous();\n+      }\n+      markWord monitor_mark = markWord::encode(monitor);\n+      markWord witness = object->cas_set_mark(monitor_mark, mark);\n+      if (witness == mark) {\n+        \/\/ Success! Return inflated monitor.\n+        if (own) {\n+          assert(current->is_Java_thread(), \"must be: checked in is_lock_owned()\");\n+          reinterpret_cast<JavaThread*>(current)->lock_stack().remove(object);\n+        }\n+        \/\/ Once the ObjectMonitor is configured and object is associated\n+        \/\/ with the ObjectMonitor, it is safe to allow async deflation:\n+        _in_use_list.add(monitor);\n+\n+        \/\/ Hopefully the performance counters are allocated on distinct\n+        \/\/ cache lines to avoid false sharing on MP systems ...\n+        OM_PERFDATA_OP(Inflations, inc());\n+        if (log_is_enabled(Trace, monitorinflation)) {\n+          ResourceMark rm(current);\n+          lsh.print_cr(\"inflate(locked): object=\" INTPTR_FORMAT \", mark=\"\n+                       INTPTR_FORMAT \", type='%s'\", p2i(object),\n+                       object->mark().value(), object->klass()->external_name());\n+        }\n+        if (event.should_commit()) {\n+          post_monitor_inflate_event(&event, object, cause);\n+        }\n+        return monitor;\n+      } else {\n+        delete monitor;\n+        continue;\n+      }\n+    }\n@@ -1249,0 +1362,1 @@\n+      assert(!UseFastLocking, \"can not happen with fast-locking\");\n@@ -1441,0 +1555,10 @@\n+class VM_RendezvousGCThreads : public VM_Operation {\n+public:\n+  bool evaluate_at_safepoint() const override { return false; }\n+  VMOp_Type type() const override { return VMOp_RendezvousGCThreads; }\n+  void doit() override {\n+    SuspendibleThreadSet::synchronize();\n+    SuspendibleThreadSet::desynchronize();\n+  };\n+};\n+\n@@ -1493,0 +1617,3 @@\n+      \/\/ Also, we sync and desync GC threads around the handshake, so that they can\n+      \/\/ safely read the mark-word and look-through to the object-monitor, without\n+      \/\/ being afraid that the object-monitor is going away.\n@@ -1495,0 +1622,2 @@\n+      VM_RendezvousGCThreads sync_gc;\n+      VMThread::execute(&sync_gc);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":210,"deletions":81,"binary":false,"changes":291,"status":"modified"},{"patch":"@@ -92,0 +92,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -95,1 +96,1 @@\n-#include \"runtime\/objectMonitor.hpp\"\n+#include \"runtime\/objectMonitor.inline.hpp\"\n@@ -705,0 +706,1 @@\n+  assert(!UseFastLocking, \"maybe not call that?\");\n@@ -1094,1 +1096,2 @@\n-  _SleepEvent(ParkEvent::Allocate(this))\n+  _SleepEvent(ParkEvent::Allocate(this)),\n+  _lock_stack()\n@@ -1572,1 +1575,2 @@\n-  if (Thread::is_lock_owned(adr)) return true;\n+  assert(!UseFastLocking, \"should not be called with fast-locking\");\n+ if (Thread::is_lock_owned(adr)) return true;\n@@ -2022,0 +2026,4 @@\n+\n+  if (!UseHeavyMonitors && UseFastLocking) {\n+    lock_stack().oops_do(f);\n+  }\n@@ -3712,0 +3720,1 @@\n+  assert(!UseFastLocking, \"only with stack-locking\");\n@@ -3741,0 +3750,39 @@\n+JavaThread* Threads::owning_thread_from_object(ThreadsList * t_list, oop obj) {\n+  assert(UseFastLocking, \"Only with fast-locking\");\n+  DO_JAVA_THREADS(t_list, q) {\n+    if (q->lock_stack().contains(obj)) {\n+      return q;\n+    }\n+  }\n+  return NULL;\n+}\n+\n+JavaThread* Threads::owning_thread_from_monitor(ThreadsList* t_list, ObjectMonitor* monitor) {\n+  if (UseFastLocking) {\n+    void* raw_owner = monitor->owner_raw();\n+    if (raw_owner == ANONYMOUS_OWNER) {\n+      return owning_thread_from_object(t_list, monitor->object());\n+    } else if (raw_owner == DEFLATER_MARKER) {\n+      return NULL;\n+    } else {\n+      Thread* owner = reinterpret_cast<Thread*>(raw_owner);\n+#ifdef ASSERT\n+      if (owner != NULL) {\n+        bool found = false;\n+        DO_JAVA_THREADS(t_list, q) {\n+          if (q == owner) {\n+            found = true;;\n+            break;\n+          }\n+        }\n+        assert(found, \"owner is not a thread: \" PTR_FORMAT, p2i(owner));\n+      }\n+#endif\n+      assert(owner == NULL || owner->is_Java_thread(), \"only JavaThreads own monitors\");\n+      return reinterpret_cast<JavaThread*>(owner);\n+    }\n+  } else {\n+    return owning_thread_from_monitor_owner(t_list, (address)monitor->owner());\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":51,"deletions":3,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"runtime\/lockStack.hpp\"\n@@ -1614,0 +1615,10 @@\n+private:\n+  LockStack _lock_stack;\n+\n+public:\n+  LockStack& lock_stack() { return _lock_stack; }\n+\n+  static ByteSize lock_stack_current_offset()    { return byte_offset_of(JavaThread, _lock_stack) + LockStack::current_offset(); }\n+  static ByteSize lock_stack_limit_offset()    { return byte_offset_of(JavaThread, _lock_stack) + LockStack::limit_offset(); }\n+\n+\n@@ -1737,0 +1748,3 @@\n+  static JavaThread* owning_thread_from_object(ThreadsList* t_list, oop obj);\n+  static JavaThread* owning_thread_from_monitor(ThreadsList* t_list, ObjectMonitor* owner);\n+\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -84,0 +84,1 @@\n+  template(HeapObjectStatistics)                  \\\n@@ -93,0 +94,1 @@\n+  template(RendezvousGCThreads)                   \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -207,2 +207,1 @@\n-  volatile_nonstatic_field(oopDesc,            _metadata._klass,                              Klass*)                                \\\n-  volatile_nonstatic_field(oopDesc,            _metadata._compressed_klass,                   narrowKlass)                           \\\n+  NOT_LP64(volatile_nonstatic_field(oopDesc,   _klass,                                        Klass*))                               \\\n@@ -737,0 +736,3 @@\n+  nonstatic_field(JavaThread,                  _lock_stack,                                   LockStack)                             \\\n+  nonstatic_field(LockStack,                   _current,                                      oop*)                                  \\\n+  nonstatic_field(LockStack,                   _base,                                         oop*)                                  \\\n@@ -1353,0 +1355,1 @@\n+  declare_toplevel_type(LockStack)                                        \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -0,0 +1,177 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"logging\/logTag.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"runtime\/vmThread.hpp\"\n+#include \"services\/heapObjectStatistics.hpp\"\n+#include \"utilities\/copy.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+HeapObjectStatistics* HeapObjectStatistics::_instance = NULL;\n+\n+class HeapObjectStatsObjectClosure : public ObjectClosure {\n+private:\n+  HeapObjectStatistics* const _stats;\n+public:\n+  HeapObjectStatsObjectClosure() : _stats(HeapObjectStatistics::instance()) {}\n+  void do_object(oop obj) {\n+    _stats->visit_object(obj);\n+  }\n+};\n+\n+class VM_HeapObjectStatistics : public VM_Operation {\n+public:\n+  VMOp_Type type() const { return VMOp_HeapObjectStatistics; }\n+  bool doit_prologue() {\n+    Heap_lock->lock();\n+    return true;\n+  }\n+\n+  void doit_epilogue() {\n+    Heap_lock->unlock();\n+  }\n+\n+  void doit() {\n+    assert(SafepointSynchronize::is_at_safepoint(), \"all threads are stopped\");\n+    assert(Heap_lock->is_locked(), \"should have the Heap_lock\");\n+\n+    CollectedHeap* heap = Universe::heap();\n+    heap->ensure_parsability(false);\n+\n+    HeapObjectStatistics* stats = HeapObjectStatistics::instance();\n+    stats->begin_sample();\n+\n+    HeapObjectStatsObjectClosure cl;\n+    heap->object_iterate(&cl);\n+  }\n+};\n+\n+HeapObjectStatisticsTask::HeapObjectStatisticsTask() : PeriodicTask(HeapObjectStatsSamplingInterval) {}\n+\n+void HeapObjectStatisticsTask::task() {\n+  VM_HeapObjectStatistics vmop;\n+  VMThread::execute(&vmop);\n+}\n+\n+void HeapObjectStatistics::initialize() {\n+  assert(_instance == NULL, \"Don't init twice\");\n+  if (HeapObjectStats) {\n+    _instance = new HeapObjectStatistics();\n+    _instance->start();\n+  }\n+}\n+\n+void HeapObjectStatistics::shutdown() {\n+  if (HeapObjectStats) {\n+    assert(_instance != NULL, \"Must be initialized\");\n+    LogTarget(Info, heap, stats) lt;\n+    if (lt.is_enabled()) {\n+      LogStream ls(lt);\n+      ResourceMark rm;\n+      _instance->print(&ls);\n+    }\n+    _instance->stop();\n+    delete _instance;\n+    _instance = NULL;\n+  }\n+}\n+\n+HeapObjectStatistics* HeapObjectStatistics::instance() {\n+  assert(_instance != NULL, \"Must be initialized\");\n+  return _instance;\n+}\n+\n+void HeapObjectStatistics::increase_counter(uint64_t& counter, uint64_t val) {\n+  uint64_t oldval = counter;\n+  uint64_t newval = counter + val;\n+  if (newval < oldval) {\n+    log_warning(heap, stats)(\"HeapObjectStats counter overflow: resulting statistics will be useless\");\n+  }\n+  counter = newval;\n+}\n+\n+HeapObjectStatistics::HeapObjectStatistics() :\n+  _task(), _num_samples(0), _num_objects(0), _num_ihashed(0), _num_locked(0), _lds(0) { }\n+\n+void HeapObjectStatistics::start() {\n+  _task.enroll();\n+}\n+\n+void HeapObjectStatistics::stop() {\n+  _task.disenroll();\n+}\n+\n+void HeapObjectStatistics::begin_sample() {\n+  _num_samples++;\n+}\n+\n+void HeapObjectStatistics::visit_object(oop obj) {\n+  increase_counter(_num_objects);\n+  markWord mark = obj->mark();\n+  if (!mark.has_no_hash()) {\n+    increase_counter(_num_ihashed);\n+    if (mark.age() > 0) {\n+      increase_counter(_num_ihashed_moved);\n+    }\n+  }\n+  if (mark.is_locked()) {\n+    increase_counter(_num_locked);\n+  }\n+#ifdef ASSERT\n+#ifdef _LP64\n+  if (!mark.has_displaced_mark_helper()) {\n+    assert(mark.narrow_klass() == CompressedKlassPointers::encode(obj->klass_or_null()), \"upper 32 mark bits must be narrow klass: mark: \" INTPTR_FORMAT \", compressed-klass: \" INTPTR_FORMAT, (intptr_t)mark.narrow_klass(), (intptr_t)CompressedKlassPointers::encode(obj->klass_or_null()));\n+  }\n+#endif\n+#endif\n+  increase_counter(_lds, obj->size());\n+}\n+\n+void HeapObjectStatistics::print(outputStream* out) const {\n+  if (!HeapObjectStats) {\n+    return;\n+  }\n+  if (_num_samples == 0 || _num_objects == 0) {\n+    return;\n+  }\n+\n+  out->print_cr(\"Number of samples:  \" UINT64_FORMAT, _num_samples);\n+  out->print_cr(\"Average number of objects: \" UINT64_FORMAT, _num_objects \/ _num_samples);\n+  out->print_cr(\"Average object size: \" UINT64_FORMAT \" bytes, %.1f words\", (_lds * HeapWordSize) \/ _num_objects, (float) _lds \/ _num_objects);\n+  out->print_cr(\"Average number of hashed objects: \" UINT64_FORMAT \" (%.2f%%)\", _num_ihashed \/ _num_samples, (float) (_num_ihashed * 100.0) \/ _num_objects);\n+  out->print_cr(\"Average number of moved hashed objects: \" UINT64_FORMAT \" (%.2f%%)\", _num_ihashed_moved \/ _num_samples, (float) (_num_ihashed_moved * 100.0) \/ _num_objects);\n+  out->print_cr(\"Average number of locked objects: \" UINT64_FORMAT \" (%.2f%%)\", _num_locked \/ _num_samples, (float) (_num_locked * 100) \/ _num_objects);\n+  out->print_cr(\"Average LDS: \" UINT64_FORMAT \" bytes\", _lds * HeapWordSize \/ _num_samples);\n+  out->print_cr(\"Avg LDS with (assumed) 64bit header: \" UINT64_FORMAT \" bytes (%.1f%%)\", (_lds - _num_objects) * HeapWordSize \/ _num_samples, ((float) _lds - _num_objects) * 100.0 \/ _lds);\n+}\n","filename":"src\/hotspot\/share\/services\/heapObjectStatistics.cpp","additions":177,"deletions":0,"binary":false,"changes":177,"status":"added"},{"patch":"@@ -0,0 +1,71 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_SERVICES_HEAPOBJECTSTATISTICS_HPP\n+#define SHARE_SERVICES_HEAPOBJECTSTATISTICS_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"runtime\/task.hpp\"\n+#include \"runtime\/vmOperation.hpp\"\n+\n+class outputStream;\n+\n+class HeapObjectStatisticsTask : public PeriodicTask {\n+public:\n+  HeapObjectStatisticsTask();\n+  void task();\n+};\n+\n+class HeapObjectStatistics : public CHeapObj<mtGC> {\n+private:\n+  static HeapObjectStatistics* _instance;\n+\n+  HeapObjectStatisticsTask _task;\n+  uint64_t _num_samples;\n+  uint64_t _num_objects;\n+  uint64_t _num_ihashed;\n+  uint64_t _num_ihashed_moved;\n+  uint64_t _num_locked;\n+  uint64_t _lds;\n+\n+  static void increase_counter(uint64_t& counter, uint64_t val = 1);\n+\n+  void print(outputStream* out) const;\n+\n+public:\n+  static void initialize();\n+  static void shutdown();\n+\n+  static HeapObjectStatistics* instance();\n+\n+  HeapObjectStatistics();\n+  void start();\n+  void stop();\n+\n+  void begin_sample();\n+  void visit_object(oop object);\n+};\n+\n+#endif \/\/ SHARE_SERVICES_HEAPOBJECTSTATISTICS_HPP\n","filename":"src\/hotspot\/share\/services\/heapObjectStatistics.hpp","additions":71,"deletions":0,"binary":false,"changes":71,"status":"added"},{"patch":"@@ -433,4 +433,2 @@\n-        address currentOwner = (address)waitingToLockMonitor->owner();\n-        if (currentOwner != NULL) {\n-          currentThread = Threads::owning_thread_from_monitor_owner(t_list,\n-                                                                    currentOwner);\n+        if (waitingToLockMonitor->has_owner()) {\n+          currentThread = Threads::owning_thread_from_monitor(t_list, waitingToLockMonitor);\n@@ -444,2 +442,0 @@\n-            cycle->set_deadlock(true);\n-\n@@ -1012,2 +1008,1 @@\n-      currentThread = Threads::owning_thread_from_monitor_owner(t_list,\n-                                                                (address)waitingToLockMonitor->owner());\n+      currentThread = Threads::owning_thread_from_monitor(t_list, waitingToLockMonitor);\n","filename":"src\/hotspot\/share\/services\/threadService.cpp","additions":3,"deletions":8,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -64,6 +64,2 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-      headerSize = typeSize;\n-    } else {\n-      headerSize = VM.getVM().alignUp(typeSize + VM.getVM().getIntSize(),\n-                                      VM.getVM().getHeapWordSize());\n-    }\n+    assert(VM.getVM().isCompressedKlassPointersEnabled());\n+    headerSize = typeSize;\n@@ -73,8 +69,0 @@\n-  private static long headerSize(BasicType type) {\n-    if (Universe.elementTypeShouldBeAligned(type)) {\n-       return alignObjectSize(headerSizeInBytes())\/VM.getVM().getHeapWordSize();\n-    } else {\n-      return headerSizeInBytes()\/VM.getVM().getHeapWordSize();\n-    }\n-  }\n-\n@@ -85,5 +73,2 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-      lengthOffsetInBytes = typeSize - VM.getVM().getIntSize();\n-    } else {\n-      lengthOffsetInBytes = typeSize;\n-    }\n+    assert(VM.getVM().isCompressedKlassPointersEnabled());\n+    lengthOffsetInBytes = typeSize;\n@@ -111,1 +96,6 @@\n-    return headerSize(type) * VM.getVM().getHeapWordSize();\n+    long base = lengthOffsetInBytes + VM.getVM().getIntSize();\n+    if (Universe.elementTypeShouldBeAligned(type)) {\n+      VM vm = VM.getVM();\n+      base = vm.alignUp(base, vm.getBytesPerWord());\n+    }\n+    return base;\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Array.java","additions":10,"deletions":20,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -225,0 +225,4 @@\n+  public Klass getKlass() {\n+    return (Klass)Metadata.instantiateWrapperFor(addr.getCompKlassAddressAt(4));\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Mark.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -51,2 +51,3 @@\n-    klass      = new MetadataField(type.getAddressField(\"_metadata._klass\"), 0);\n-    compressedKlass  = new NarrowKlassField(type.getAddressField(\"_metadata._compressed_klass\"), 0);\n+    if (!VM.getVM().isLP64()) {\n+      klass      = new MetadataField(type.getAddressField(\"_klass\"), 0);\n+    }\n@@ -76,1 +77,0 @@\n-  private static NarrowKlassField compressedKlass;\n@@ -80,0 +80,9 @@\n+\n+  private static Klass getKlass(Mark mark) {\n+    if (mark.hasMonitor()) {\n+      ObjectMonitor mon = mark.monitor();\n+      mark = mon.header();\n+    }\n+    return mark.getKlass();\n+  }\n+\n@@ -81,2 +90,3 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-      return (Klass)compressedKlass.getValue(getHandle());\n+    if (VM.getVM().isLP64()) {\n+      assert(VM.getVM().isCompressedKlassPointersEnabled());\n+      return getKlass(getMark());\n@@ -152,5 +162,0 @@\n-      if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-        visitor.doMetadata(compressedKlass, true);\n-      } else {\n-        visitor.doMetadata(klass, true);\n-      }\n@@ -211,2 +216,3 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-      return (Klass)Metadata.instantiateWrapperFor(handle.getCompKlassAddressAt(compressedKlass.getOffset()));\n+    if (VM.getVM().isLP64()) {\n+      Mark mark = new Mark(handle);\n+      return getKlass(mark);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Oop.java","additions":18,"deletions":12,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -47,0 +47,2 @@\n+  private static long          lockStackCurrentOffset;\n+  private static long          lockStackBaseOffset;\n@@ -55,0 +57,1 @@\n+  private static long oopPtrSize;\n@@ -87,0 +90,1 @@\n+    Type typeLockStack = db.lookupType(\"LockStack\");\n@@ -99,0 +103,4 @@\n+    lockStackCurrentOffset = type.getField(\"_lock_stack\").getOffset() + typeLockStack.getField(\"_current\").getOffset();\n+    lockStackBaseOffset = type.getField(\"_lock_stack\").getOffset() + typeLockStack.getField(\"_base\").getOffset();\n+    oopPtrSize = VM.getVM().getAddressSize();\n+\n@@ -395,0 +403,13 @@\n+  public boolean isLockOwned(OopHandle obj) {\n+    Address current = addr.getAddressAt(lockStackCurrentOffset);\n+    Address base = addr.getAddressAt(lockStackBaseOffset);\n+    while (base.lessThan(current)) {\n+      Address oop = base.getAddressAt(0);\n+      if (oop.equals(obj)) {\n+        return true;\n+      }\n+      base = base.addOffsetTo(oopPtrSize);\n+    }\n+    return false;\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/JavaThread.java","additions":21,"deletions":0,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -80,0 +80,1 @@\n+          mark.monitor().isOwnedAnonymous() ||\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/JavaVFrame.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -82,0 +82,4 @@\n+  public boolean isOwnedAnonymous() {\n+    return addr.getAddressAt(ownerFieldOffset).asLongValue() == 1;\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/ObjectMonitor.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -211,0 +211,1 @@\n+        assert(!VM.getVM().getCommandLineBooleanFlag(\"UseFastLocking\"));\n@@ -228,1 +229,18 @@\n-        return owningThreadFromMonitor(monitor.owner());\n+        if (VM.getVM().getCommandLineBooleanFlag(\"UseFastLocking\")) {\n+            if (monitor.isOwnedAnonymous()) {\n+                OopHandle object = monitor.object();\n+                for (int i = 0; i < getNumberOfThreads(); i++) {\n+                    JavaThread thread = getJavaThreadAt(i);\n+                    if (thread.isLockOwned(object)) {\n+                        return thread;\n+                     }\n+                }\n+                throw new InternalError(\"We should have found a thread that owns the anonymous lock\");\n+            }\n+            \/\/ Owner can only be threads at this point.\n+            Address o = monitor.owner();\n+            if (o == null) return null;\n+            return new JavaThread(o);\n+        } else {\n+            return owningThreadFromMonitor(monitor.owner());\n+        }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/Threads.java","additions":19,"deletions":1,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+import sun.jvm.hotspot.oops.Oop;\n@@ -45,19 +46,0 @@\n-  private static AddressField klassField;\n-\n-  static {\n-    VM.registerVMInitializedObserver(new Observer() {\n-        public void update(Observable o, Object data) {\n-          initialize(VM.getVM().getTypeDataBase());\n-        }\n-      });\n-  }\n-\n-  private static void initialize(TypeDataBase db) {\n-    Type type = db.lookupType(\"oopDesc\");\n-\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-      klassField = type.getAddressField(\"_metadata._compressed_klass\");\n-    } else {\n-      klassField = type.getAddressField(\"_metadata._klass\");\n-    }\n-  }\n@@ -74,5 +56,1 @@\n-      if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-        Metadata.instantiateWrapperFor(oop.getCompKlassAddressAt(klassField.getOffset()));\n-      } else {\n-        Metadata.instantiateWrapperFor(klassField.getValue(oop));\n-      }\n+      Oop.getKlassForOopHandle(oop);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/utilities\/RobustOopDeterminator.java","additions":2,"deletions":24,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -76,1 +76,2 @@\n-    final int hubOffset = getFieldOffset(\"oopDesc::_metadata._klass\", Integer.class, \"Klass*\");\n+    \/\/ TODO: Lilliput. Probably ok.\n+    final int hubOffset = 4; \/\/ getFieldOffset(\"oopDesc::_metadata._klass\", Integer.class, \"Klass*\");\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk.vm.ci.hotspot\/src\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -52,1 +52,1 @@\n-  static markWord originalMark() { return markWord(markWord::lock_mask_in_place); }\n+  static markWord originalMark() { return markWord(markWord::unlocked_value); }\n@@ -92,0 +92,3 @@\n+  \/\/ TODO: This is the only use of PM::adjust_during_full_gc().\n+  \/\/ GCs use the variant with a forwarding structure here,\n+  \/\/ test that variant, and remove the method.\n","filename":"test\/hotspot\/gtest\/gc\/shared\/test_preservedMarks.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#ifndef _LP64\n@@ -40,0 +41,1 @@\n+#endif\n","filename":"test\/hotspot\/gtest\/oops\/test_typeArrayOop.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -174,0 +174,8 @@\n+\n+# Lilliput:\n+\n+# Disabled because Lilliput forces +UseCompressedClassPointers\n+gc\/arguments\/TestCompressedClassFlags.java 1234567 generic-all\n+\n+# Dispabled because Lilliput forces -UseBiasedLocking\n+runtime\/logging\/BiasedLockingTest.java 1234567 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n- * @run main\/othervm -XX:+IgnoreUnrecognizedVMOptions -XX:+TieredCompilation -XX:TieredStopAtLevel=1 -Xcomp -XX:-UseCompressedClassPointers -XX:CompileOnly=TestArrayCopyToFromObject.test TestArrayCopyToFromObject\n","filename":"test\/hotspot\/jtreg\/compiler\/c1\/TestArrayCopyToFromObject.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -36,5 +36,0 @@\n- *                                 -XX:+UseCompressedOops -XX:-UseCompressedClassPointers\n- *                                 -XX:CompileCommand=dontinline,compiler.unsafe.OpaqueAccesses::test*\n- *                                 compiler.unsafe.OpaqueAccesses\n- * @run main\/bootclasspath\/othervm -XX:+IgnoreUnrecognizedVMOptions -XX:+UnlockDiagnosticVMOptions\n- *                                 -XX:-TieredCompilation -Xbatch\n@@ -43,5 +38,0 @@\n- *                                 compiler.unsafe.OpaqueAccesses\n- * @run main\/bootclasspath\/othervm -XX:+IgnoreUnrecognizedVMOptions -XX:+UnlockDiagnosticVMOptions\n- *                                 -XX:-TieredCompilation -Xbatch\n- *                                 -XX:-UseCompressedOops -XX:-UseCompressedClassPointers\n- *                                 -XX:CompileCommand=dontinline,compiler.unsafe.OpaqueAccesses::test*\n","filename":"test\/hotspot\/jtreg\/compiler\/unsafe\/OpaqueAccesses.java","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+import jdk.test.lib.Platform;\n@@ -75,1 +76,1 @@\n-    private static final int OBJECT_SIZE_HIGH = 3250;\n+    private static final int OBJECT_SIZE_HIGH = Platform.is64bit() ? 3266 : 3258;\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/plab\/TestPLABPromotion.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -177,1 +177,1 @@\n-        return Platform.is64bit() && InputArguments.contains(\"-XX:+UseCompressedClassPointers\");\n+        return Platform.is64bit();\n","filename":"test\/hotspot\/jtreg\/gc\/metaspace\/TestMetaspacePerfCounters.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -38,2 +38,1 @@\n- * @run driver gc.metaspace.TestSizeTransitions false -XX:+UseSerialGC\n- * @run driver gc.metaspace.TestSizeTransitions true  -XX:+UseSerialGC\n+ * @run driver gc.metaspace.TestSizeTransitions -XX:+UseSerialGC\n@@ -46,2 +45,1 @@\n- * @run driver gc.metaspace.TestSizeTransitions false -XX:+UseParallelGC\n- * @run driver gc.metaspace.TestSizeTransitions true  -XX:+UseParallelGC\n+ * @run driver gc.metaspace.TestSizeTransitions -XX:+UseParallelGC\n@@ -54,2 +52,1 @@\n- * @run driver gc.metaspace.TestSizeTransitions false -XX:+UseG1GC\n- * @run driver gc.metaspace.TestSizeTransitions true  -XX:+UseG1GC\n+ * @run driver gc.metaspace.TestSizeTransitions -XX:+UseG1GC\n@@ -93,2 +90,2 @@\n-    \/\/ args: <use-coops> <gc-arg>\n-    if (args.length != 2) {\n+    \/\/ args: <gc-arg>\n+    if (args.length != 1) {\n@@ -99,8 +96,1 @@\n-    final boolean useCompressedKlassPointers = Boolean.parseBoolean(args[0]);\n-    final String gcArg = args[1];\n-\n-    if (!hasCompressedKlassPointers && useCompressedKlassPointers) {\n-       \/\/ No need to run this configuration.\n-       System.out.println(\"Skipping test.\");\n-       return;\n-    }\n+    final String gcArg = args[0];\n@@ -109,3 +99,0 @@\n-    if (hasCompressedKlassPointers) {\n-      jvmArgs.add(useCompressedKlassPointers ? \"-XX:+UseCompressedClassPointers\" : \"-XX:-UseCompressedClassPointers\");\n-    }\n@@ -127,1 +114,1 @@\n-    if (useCompressedKlassPointers) {\n+    if (hasCompressedKlassPointers) {\n","filename":"test\/hotspot\/jtreg\/gc\/metaspace\/TestSizeTransitions.java","additions":7,"deletions":20,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -52,1 +52,1 @@\n- *   -Xmx1G -XX:G1HeapRegionSize=8m -XX:MaxGCPauseMillis=1000 gc.stress.TestMultiThreadStressRSet 60 16\n+ *   -Xmx1100M -XX:G1HeapRegionSize=8m -XX:MaxGCPauseMillis=1000 gc.stress.TestMultiThreadStressRSet 60 16\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/TestMultiThreadStressRSet.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,11 +30,0 @@\n-\n-\/* @test\n- * @bug 8264008\n- * @summary Run metaspace utils related gtests with compressed class pointers off\n- * @requires vm.bits == 64\n- * @library \/test\/lib\n- * @modules java.base\/jdk.internal.misc\n- *          java.xml\n- * @requires vm.flagless\n- * @run main\/native GTestWrapper --gtest_filter=MetaspaceUtils* -XX:-UseCompressedClassPointers\n- *\/\n","filename":"test\/hotspot\/jtreg\/gtest\/MetaspaceUtilsGtests.java","additions":0,"deletions":11,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -56,0 +56,3 @@\n+    \/* Lilliput: cannot work due to drastically reduced narrow klass pointer range (atm 2g and that may get\n+       smaller still). There is an argument for improving CDS\/CCS reservation and make it more likely to run\n+       zero-based, but that logic has to be rethought.\n@@ -70,0 +73,1 @@\n+     *\/\n@@ -73,0 +77,1 @@\n+    \/* Lilliput: See comment above.\n@@ -87,0 +92,1 @@\n+    *\/\n@@ -91,0 +97,2 @@\n+    \/* Lilliput: I am not sure what the point of this test CCS reservation is independent from\n+       heap. See below the desparate attempts to predict heap reservation on PPC. Why do we even care?\n@@ -111,0 +119,1 @@\n+     *\/\n@@ -116,0 +125,1 @@\n+    \/* Lilliput: narrow klass pointer range drastically reduced. See comments under smallHeapTest().\n@@ -135,0 +145,1 @@\n+    *\/\n@@ -137,0 +148,4 @@\n+    \/* Lilliput: not sure what the point of this test is. The ability to have a class space if heap uses\n+       large pages? Why would that be a problem? Kept alive for now since it makes no problems even with\n+       smaller class pointers.\n+     *\/\n@@ -197,0 +212,1 @@\n+    \/* Lilliput: narrow klass pointer range drastically reduced. See comments under smallHeapTest().\n@@ -212,0 +228,1 @@\n+    *\/\n@@ -213,0 +230,1 @@\n+    \/* Lilliput: narrow klass pointer range drastically reduced. See comments under smallHeapTest().\n@@ -232,0 +250,1 @@\n+    *\/\n@@ -233,0 +252,1 @@\n+    \/* Lilliput: narrow klass pointer range drastically reduced. See comments under smallHeapTest().\n@@ -252,0 +272,1 @@\n+    *\/\n@@ -320,4 +341,4 @@\n-        smallHeapTest();\n-        smallHeapTestWith1G();\n-        largeHeapTest();\n-        largeHeapAbove32GTest();\n+        \/\/ smallHeapTest();\n+        \/\/ smallHeapTestWith1G();\n+        \/\/ largeHeapTest();\n+        \/\/ largeHeapAbove32GTest();\n@@ -335,3 +356,3 @@\n-            smallHeapTestNoCoop();\n-            smallHeapTestWith1GNoCoop();\n-            largeHeapTestNoCoop();\n+            \/\/ smallHeapTestNoCoop();\n+            \/\/ smallHeapTestWith1GNoCoop();\n+            \/\/ largeHeapTestNoCoop();\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedClassPointers.java","additions":28,"deletions":7,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -86,8 +86,0 @@\n-\n-\n-        pb = ProcessTools.createJavaProcessBuilder(\"-XX:-UseCompressedClassPointers\",\n-                                                   \"-XX:CompressedClassSpaceSize=1m\",\n-                                                   \"-version\");\n-        output = new OutputAnalyzer(pb.start());\n-        output.shouldContain(\"Setting CompressedClassSpaceSize has no effect when compressed class pointers are not used\")\n-              .shouldHaveExitValue(0);\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedClassSpaceSize.java","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,83 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test id=default\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run main\/othervm BaseOffsets\n+ *\/\n+\/*\n+ * @test id=no-coops\n+ * @library \/test\/lib\n+ * @requires vm.bits == \"64\"\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run main\/othervm -XX:-UseCompressedOops BaseOffsets\n+ *\/\n+\n+import java.lang.reflect.Field;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import jdk.internal.misc.Unsafe;\n+\n+import jdk.test.lib.Asserts;\n+import jdk.test.lib.Platform;\n+\n+public class BaseOffsets {\n+\n+    static class LIClass {\n+        public int i;\n+    }\n+\n+    \/\/ @0:  8 byte header,  @8: int field\n+    static final long INT_OFFSET  = 8L;\n+\n+    static public void main(String[] args) {\n+        Unsafe unsafe = Unsafe.getUnsafe();\n+        Class c = LIClass.class;\n+        Field[] fields = c.getFields();\n+        for (int i = 0; i < fields.length; i++) {\n+            long offset = unsafe.objectFieldOffset(fields[i]);\n+            if (fields[i].getType() == int.class) {\n+                Asserts.assertEquals(offset, INT_OFFSET, \"Misplaced int field\");\n+            } else {\n+                Asserts.fail(\"Unexpected field type\");\n+            }\n+        }\n+\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(boolean[].class), 12, \"Misplaced boolean array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(byte[].class),    12, \"Misplaced byte    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(char[].class),    12, \"Misplaced char    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(short[].class),   12, \"Misplaced short   array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(int[].class),     12, \"Misplaced int     array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(long[].class),    16, \"Misplaced long    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(float[].class),   12, \"Misplaced float   array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(double[].class),  16, \"Misplaced double  array base\");\n+        boolean narrowOops = System.getProperty(\"java.vm.compressedOopsMode\") != null ||\n+                             !Platform.is64bit();\n+        int expected_objary_offset = narrowOops ? 12 : 16;\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(Object[].class),  expected_objary_offset, \"Misplaced object  array base\");\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/FieldLayout\/BaseOffsets.java","additions":83,"deletions":0,"binary":false,"changes":83,"status":"added"},{"patch":"@@ -41,1 +41,0 @@\n- * @run main\/othervm -XX:+UseCompressedOops -XX:-UseCompressedClassPointers FieldDensityTest\n","filename":"test\/hotspot\/jtreg\/runtime\/FieldLayout\/FieldDensityTest.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -61,2 +61,2 @@\n-    static final long INT_OFFSET  = Platform.is64bit() ? 12L : 16L;\n-    static final long LONG_OFFSET = Platform.is64bit() ? 16L :  8L;\n+    static final long INT_OFFSET  = 16L;\n+    static final long LONG_OFFSET = 8L;\n","filename":"test\/hotspot\/jtreg\/runtime\/FieldLayout\/OldLayoutCheck.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -70,20 +70,0 @@\n-\/*\n- * @test id=test-64bit-noccs\n- * @summary Test the VM.metaspace command\n- * @requires vm.bits == \"64\"\n- * @library \/test\/lib\n- * @modules java.base\/jdk.internal.misc\n- *          java.management\n- * @run main\/othervm -Dwithout-compressed-class-space -XX:MaxMetaspaceSize=201M -Xmx100M -XX:-UseCompressedOops -XX:-UseCompressedClassPointers PrintMetaspaceDcmd\n- *\/\n-\n- \/*\n- * @test id=test-nospecified\n- * @summary Test the VM.metaspace command\n- * @requires vm.bits == \"64\"\n- * @library \/test\/lib\n- * @modules java.base\/jdk.internal.misc\n- *          java.management\n- * @run main\/othervm -Dno-specified-flag -Xmx100M -XX:-UseCompressedOops -XX:-UseCompressedClassPointers PrintMetaspaceDcmd\n- *\/\n-\n","filename":"test\/hotspot\/jtreg\/runtime\/Metaspace\/PrintMetaspaceDcmd.java","additions":0,"deletions":20,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -45,1 +45,0 @@\n-        {\"-Xint\", \"-XX:+UseBiasedLocking\"},\n@@ -47,1 +46,0 @@\n-        {\"-Xcomp\", \"-XX:TieredStopAtLevel=1\", \"-XX:+UseBiasedLocking\"},\n@@ -50,2 +48,0 @@\n-        {\"-Xcomp\", \"-XX:-TieredCompilation\", \"-XX:+UseBiasedLocking\", \"-XX:+IgnoreUnrecognizedVMOptions\", \"-XX:+UseOptoBiasInlining\"},\n-        {\"-Xcomp\", \"-XX:-TieredCompilation\", \"-XX:+UseBiasedLocking\", \"-XX:+IgnoreUnrecognizedVMOptions\", \"-XX:-UseOptoBiasInlining\"}\n","filename":"test\/hotspot\/jtreg\/runtime\/Monitor\/SyncOnValueBasedClassTest.java","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -70,2 +70,0 @@\n-        testTable.add( new TestVector(\"-XX:+UseCompressedClassPointers\", \"-XX:-UseCompressedClassPointers\",\n-           \"The saved state of UseCompressedOops and UseCompressedClassPointers is different from runtime, CDS will be disabled.\", 1) );\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/CommandLineFlagComboNegative.java","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -49,1 +49,0 @@\n-        public boolean useCompressedClassPointers;   \/\/ UseCompressedClassPointers\n@@ -52,1 +51,1 @@\n-        public ConfArg(boolean useCompressedOops, boolean useCompressedClassPointers, String msg, int code) {\n+        public ConfArg(boolean useCompressedOops, String msg, int code) {\n@@ -54,1 +53,0 @@\n-            this.useCompressedClassPointers = useCompressedClassPointers;\n@@ -69,1 +67,1 @@\n-            *          UseCompressedOops   UseCompressedClassPointers  Result\n+            *          UseCompressedOops   Result\n@@ -71,5 +69,3 @@\n-            *    dump: on                  on\n-            *    test: on                  on                          Pass\n-            *          on                  off                         Fail\n-            *          off                 on                          Fail\n-            *          off                 off                         Fail\n+            *    dump: on\n+            *    test: on                  Pass\n+            *          off                 Fail\n@@ -77,15 +73,3 @@\n-            *    dump: on                  off\n-            *    test: on                  off                         Pass\n-            *          on                  on                          Fail\n-            *          off                 on                          Pass\n-            *          off                 off                         Fail\n-            *    3.\n-            *    dump: off                 on\n-            *    test: off                 on                          Pass\n-            *          on                  on                          Fail\n-            *          on                  off                         Fail\n-            *    4.\n-            *    dump: off                 off\n-            *    test: off                 off                         Pass\n-            *          on                  on                          Fail\n-            *          on                  off                         Fail\n+            *    dump: off\n+            *    test: off                 Pass\n+            *          on                  Fail\n@@ -94,1 +78,1 @@\n-            if (dumpArg.useCompressedOops && dumpArg.useCompressedClassPointers) {\n+            if (dumpArg.useCompressedOops) {\n@@ -96,1 +80,1 @@\n-                    .add(new ConfArg(true, true, HELLO_STRING, PASS));\n+                    .add(new ConfArg(true, HELLO_STRING, PASS));\n@@ -98,15 +82,1 @@\n-                    .add(new ConfArg(true, false, EXEC_ABNORMAL_MSG, FAIL));\n-                execArgs\n-                    .add(new ConfArg(false, true, EXEC_ABNORMAL_MSG, FAIL));\n-                execArgs\n-                    .add(new ConfArg(false, false, EXEC_ABNORMAL_MSG, FAIL));\n-\n-            }  else if(dumpArg.useCompressedOops && !dumpArg.useCompressedClassPointers) {\n-                execArgs\n-                    .add(new ConfArg(true, false, HELLO_STRING, PASS));\n-                execArgs\n-                    .add(new ConfArg(true, true, EXEC_ABNORMAL_MSG, FAIL));\n-                execArgs\n-                    .add(new ConfArg(false, true, EXEC_ABNORMAL_MSG, FAIL));\n-                execArgs\n-                    .add(new ConfArg(false, false, EXEC_ABNORMAL_MSG, FAIL));\n+                    .add(new ConfArg(false, EXEC_ABNORMAL_MSG, FAIL));\n@@ -114,3 +84,1 @@\n-            } else if (!dumpArg.useCompressedOops && dumpArg.useCompressedClassPointers) {\n-                execArgs\n-                    .add(new ConfArg(false, true, HELLO_STRING, PASS));\n+            } else if (!dumpArg.useCompressedOops) {\n@@ -118,1 +86,1 @@\n-                    .add(new ConfArg(true, true, EXEC_ABNORMAL_MSG, FAIL));\n+                    .add(new ConfArg(false, HELLO_STRING, PASS));\n@@ -120,8 +88,1 @@\n-                    .add(new ConfArg(true, false, EXEC_ABNORMAL_MSG, FAIL));\n-            } else if (!dumpArg.useCompressedOops && !dumpArg.useCompressedClassPointers) {\n-                execArgs\n-                    .add(new ConfArg(false, false, HELLO_STRING, PASS));\n-                execArgs\n-                    .add(new ConfArg(true, true, EXEC_ABNORMAL_MSG, FAIL));\n-                execArgs\n-                    .add(new ConfArg(true, false, EXEC_ABNORMAL_MSG, FAIL));\n+                    .add(new ConfArg(true, EXEC_ABNORMAL_MSG, FAIL));\n@@ -137,5 +98,0 @@\n-    public static String getCompressedClassPointersArg(boolean on) {\n-        if (on) return \"-XX:+UseCompressedClassPointers\";\n-        else    return \"-XX:-UseCompressedClassPointers\";\n-    }\n-\n@@ -147,5 +103,1 @@\n-            .add(new RunArg(new ConfArg(true, true, null, PASS)));\n-        runList\n-            .add(new RunArg(new ConfArg(true, false, null, PASS)));\n-        runList\n-            .add(new RunArg(new ConfArg(false, true, null, PASS)));\n+            .add(new RunArg(new ConfArg(true, null, PASS)));\n@@ -153,1 +105,1 @@\n-            .add(new RunArg(new ConfArg(false, false, null, PASS)));\n+            .add(new RunArg(new ConfArg(false, null, PASS)));\n@@ -165,1 +117,0 @@\n-                      getCompressedClassPointersArg(t.dumpArg.useCompressedClassPointers),\n@@ -178,1 +129,0 @@\n-                                      getCompressedClassPointersArg(c.useCompressedClassPointers),\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/TestCombinedCompressedFlags.java","additions":16,"deletions":66,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -75,13 +75,1 @@\n-         System.out.println(\"3. Run with -UseCompressedOops -UseCompressedClassPointers\");\n-         out = TestCommon\n-                   .exec(helloJar,\n-                         \"-XX:+UseSerialGC\",\n-                         \"-XX:-UseCompressedOops\",\n-                         \"-XX:-UseCompressedClassPointers\",\n-                         \"-Xlog:cds\",\n-                         \"Hello\");\n-         out.shouldContain(UNABLE_TO_USE_ARCHIVE);\n-         out.shouldContain(ERR_MSG);\n-         out.shouldHaveExitValue(1);\n-\n-         System.out.println(\"4. Run with -UseCompressedOops +UseCompressedClassPointers\");\n+         System.out.println(\"3. Run with -UseCompressedOops +UseCompressedClassPointers\");\n@@ -98,13 +86,1 @@\n-         System.out.println(\"5. Run with +UseCompressedOops -UseCompressedClassPointers\");\n-         out = TestCommon\n-                   .exec(helloJar,\n-                         \"-XX:+UseSerialGC\",\n-                         \"-XX:+UseCompressedOops\",\n-                         \"-XX:-UseCompressedClassPointers\",\n-                         \"-Xlog:cds\",\n-                         \"Hello\");\n-         out.shouldContain(UNABLE_TO_USE_ARCHIVE);\n-         out.shouldContain(ERR_MSG);\n-         out.shouldHaveExitValue(1);\n-\n-         System.out.println(\"6. Run with +UseCompressedOops +UseCompressedClassPointers\");\n+         System.out.println(\"4. Run with +UseCompressedOops +UseCompressedClassPointers\");\n@@ -122,12 +98,1 @@\n-         System.out.println(\"7. Dump with -UseCompressedOops -UseCompressedClassPointers\");\n-         out = TestCommon\n-                   .dump(helloJar,\n-                         new String[] {\"Hello\"},\n-                         \"-XX:+UseSerialGC\",\n-                         \"-XX:-UseCompressedOops\",\n-                         \"-XX:+UseCompressedClassPointers\",\n-                         \"-Xlog:cds\");\n-         out.shouldContain(\"Dumping shared data to file:\");\n-         out.shouldHaveExitValue(0);\n-\n-         System.out.println(\"8. Run with ZGC\");\n+         System.out.println(\"5. Run with ZGC\");\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/TestZGCWithCDS.java","additions":3,"deletions":38,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-        output.shouldContain(\"inflate(has_locker):\");\n+        output.shouldContain(\"inflate(locked):\");\n","filename":"test\/hotspot\/jtreg\/runtime\/logging\/MonitorInflationTest.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,59 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test\n+ * @bug 8303027\n+ * @requires vm.bits == \"64\"\n+ * @summary Test that we're failing with OOME and not with VM crash\n+ * @run main\/othervm -Xmx1g -XX:-UseCompressedOops TestOOM\n+ *\/\n+\/*\n+ * @test\n+ * @bug 8303027\n+ * @requires vm.bits == \"32\"\n+ * @summary Test that we're failing with OOME and not with VM crash\n+ * @run main\/othervm -Xmx1g TestOOM\n+ *\/\n+public class TestOOM {\n+    public static void main(String[] args) {\n+        \/\/ Test that it exits with OOME and not with VM crash.\n+        try {\n+            LinkedInsanity previous = null;\n+            while (true) {\n+                previous = new LinkedInsanity(previous);\n+            }\n+        } catch (OutOfMemoryError e) {\n+            \/\/ That's expected\n+        }\n+    }\n+\n+    private static class LinkedInsanity {\n+        private final LinkedInsanity previous;\n+        private final int[] padding = new int[64000];\n+\n+        public LinkedInsanity(LinkedInsanity previous) {\n+            this.previous = previous;\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/oom\/TestOOM.java","additions":59,"deletions":0,"binary":false,"changes":59,"status":"added"},{"patch":"@@ -103,1 +103,1 @@\n-                       Platform.is64bit() ? 549755813632L: 4294967168L);\n+                       4294967168L);\n","filename":"test\/hotspot\/jtreg\/serviceability\/sa\/ClhsdbLongConstant.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -134,2 +134,1 @@\n-        runCheck(new String[] {\"-XX:+IgnoreUnrecognizedVMOptions\", \"-XX:-UseCompressedClassPointers\"},\n-                 BadFailOnConstraint.create(Loads.class, \"load()\", 1, 1, \"Load\"),\n+        runCheck(BadFailOnConstraint.create(Loads.class, \"load()\", 1, 1, \"Load\"),\n","filename":"test\/hotspot\/jtreg\/testlibrary_tests\/ir_framework\/tests\/TestIRMatching.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -375,1 +375,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(8, OBJ_ALIGN);\n@@ -382,1 +382,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(8, OBJ_ALIGN);\n@@ -392,1 +392,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(8, OBJ_ALIGN);\n","filename":"test\/jdk\/java\/lang\/instrument\/GetObjectSizeIntrinsicsTest.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -73,1 +73,2 @@\n-        int objectHeaderSize = bytesPerWord * 3; \/\/ length will be aligned on 64 bits\n+        \/\/ length will be in klass-gap on 64 bits, extra field on 32 bits.\n+        int objectHeaderSize = bytesPerWord * (runsOn32Bit ? 3 : 2);\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/objectcount\/ObjectCountEventVerifier.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"}]}