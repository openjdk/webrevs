{"files":[{"patch":"@@ -815,0 +815,5 @@\n+  # Add more Lilliput-specific ProblemLists when UCOH is enabled\n+  ifneq ($$(findstring -XX:+UseCompactObjectHeaders, $$(TEST_OPTS)), )\n+    JTREG_EXTRA_PROBLEM_LISTS += $(TOPDIR)\/test\/hotspot\/jtreg\/ProblemList-lilliput.txt\n+  endif\n+\n","filename":"make\/RunTests.gmk","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1989,1 +1989,3 @@\n-      code_stub = &C->output()->safepoint_poll_table()->add_safepoint(__ offset());\n+      C2SafepointPollStub* stub = new (C->comp_arena()) C2SafepointPollStub(__ offset());\n+      C->output()->add_stub(stub);\n+      code_stub = &stub->entry();\n@@ -3831,32 +3833,40 @@\n-    \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n-    __ orr(tmp, disp_hdr, markWord::unlocked_value);\n-\n-    \/\/ Initialize the box. (Must happen before we update the object mark!)\n-    __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-    \/\/ Compare object markWord with an unlocked value (tmp) and if\n-    \/\/ equal exchange the stack address of our box with object markWord.\n-    \/\/ On failure disp_hdr contains the possibly locked markWord.\n-    __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n-               \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n-    __ br(Assembler::EQ, cont);\n-\n-    assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-    \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n-    \/\/ object, will have now locked it will continue at label cont\n-\n-    __ bind(cas_failed);\n-    \/\/ We did not see an unlocked object so try the fast recursive case.\n-\n-    \/\/ Check if the owner is self by comparing the value in the\n-    \/\/ markWord of object (disp_hdr) with the stack pointer.\n-    __ mov(rscratch1, sp);\n-    __ sub(disp_hdr, disp_hdr, rscratch1);\n-    __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n-    \/\/ If condition is true we are cont and hence we can store 0 as the\n-    \/\/ displaced header in the box, which indicates that it is a recursive lock.\n-    __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n-    __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-    __ b(cont);\n+    if (LockingMode == LM_MONITOR) {\n+      __ tst(oop, oop); \/\/ Set NE to indicate 'failure' -> take slow-path. We know that oop != 0.\n+      __ b(cont);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n+      __ orr(tmp, disp_hdr, markWord::unlocked_value);\n+\n+      \/\/ Initialize the box. (Must happen before we update the object mark!)\n+      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n+      \/\/ Compare object markWord with an unlocked value (tmp) and if\n+      \/\/ equal exchange the stack address of our box with object markWord.\n+      \/\/ On failure disp_hdr contains the possibly locked markWord.\n+      __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n+                 \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n+      __ br(Assembler::EQ, cont);\n+\n+      assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+\n+      \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n+      \/\/ object, will have now locked it will continue at label cont\n+\n+      __ bind(cas_failed);\n+      \/\/ We did not see an unlocked object so try the fast recursive case.\n+\n+      \/\/ Check if the owner is self by comparing the value in the\n+      \/\/ markWord of object (disp_hdr) with the stack pointer.\n+      __ mov(rscratch1, sp);\n+      __ sub(disp_hdr, disp_hdr, rscratch1);\n+      __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n+      \/\/ If condition is true we are cont and hence we can store 0 as the\n+      \/\/ displaced header in the box, which indicates that it is a recursive lock.\n+      __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n+      __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+      __ b(cont);\n+    } else {\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ fast_lock(oop, disp_hdr, tmp, rscratch1, cont);\n+      __ b(cont);\n+    }\n@@ -3875,7 +3885,8 @@\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n-    __ mov(tmp, (address)markWord::unused_mark().value());\n-    __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+      \/\/ lock. The fast-path monitor unlock code checks for\n+      \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+      \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n+      __ mov(tmp, (address)markWord::unused_mark().value());\n+      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    }\n@@ -3911,2 +3922,3 @@\n-    \/\/ Find the lock address and load the displaced header from the stack.\n-    __ ldr(disp_hdr, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    if (LockingMode == LM_LEGACY) {\n+      \/\/ Find the lock address and load the displaced header from the stack.\n+      __ ldr(disp_hdr, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n@@ -3914,3 +3926,4 @@\n-    \/\/ If the displaced header is 0, we have a recursive unlock.\n-    __ cmp(disp_hdr, zr);\n-    __ br(Assembler::EQ, cont);\n+      \/\/ If the displaced header is 0, we have a recursive unlock.\n+      __ cmp(disp_hdr, zr);\n+      __ br(Assembler::EQ, cont);\n+    }\n@@ -3922,7 +3935,16 @@\n-    \/\/ Check if it is still a light weight lock, this is is true if we\n-    \/\/ see the stack address of the basicLock in the markWord of the\n-    \/\/ object.\n-\n-    __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n-               \/*release*\/ true, \/*weak*\/ false, tmp);\n-    __ b(cont);\n+    if (LockingMode == LM_MONITOR) {\n+      __ tst(oop, oop); \/\/ Set NE to indicate 'failure' -> take slow-path. We know that oop != 0.\n+      __ b(cont);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ Check if it is still a light weight lock, this is is true if we\n+      \/\/ see the stack address of the basicLock in the markWord of the\n+      \/\/ object.\n+\n+      __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n+                 \/*release*\/ true, \/*weak*\/ false, tmp);\n+      __ b(cont);\n+    } else {\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ fast_unlock(oop, tmp, box, disp_hdr, cont);\n+      __ b(cont);\n+    }\n@@ -3936,0 +3958,14 @@\n+\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ If the owner is anonymous, we need to fix it -- in an outline stub.\n+      Register tmp2 = disp_hdr;\n+      __ ldr(tmp2, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+      \/\/ We cannot use tbnz here, the target might be too far away and cannot\n+      \/\/ be encoded.\n+      __ tst(tmp2, (uint64_t)ObjectMonitor::ANONYMOUS_OWNER);\n+      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmp, tmp2);\n+      Compile::current()->output()->add_stub(stub);\n+      __ br(Assembler::NE, stub->entry());\n+      __ bind(stub->continuation());\n+    }\n+\n@@ -7442,1 +7478,1 @@\n-  predicate(!needs_acquiring_load(n));\n+  predicate(!needs_acquiring_load(n) && !UseCompactObjectHeaders);\n@@ -7452,0 +7488,26 @@\n+instruct loadNKlassLilliput(iRegNNoSp dst, memory4 mem, rFlagsReg cr)\n+%{\n+  match(Set dst (LoadNKlass mem));\n+  effect(KILL cr);\n+  predicate(!needs_acquiring_load(n) && UseCompactObjectHeaders);\n+\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ldrw  $dst, $mem\\t# compressed class ptr\" %}\n+  ins_encode %{\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset\");\n+    assert($mem$$index$$Register == noreg, \"expect no index\");\n+    Register dst = $dst$$Register;\n+    Register obj = $mem$$base$$Register;\n+    C2LoadNKlassStub* stub = new (Compile::current()->comp_arena()) C2LoadNKlassStub(dst);\n+    Compile::current()->output()->add_stub(stub);\n+    __ ldr(dst, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    \/\/ NOTE: We can't use tbnz here, because the target is sometimes too far away\n+    \/\/ and cannot be encoded.\n+    __ tst(dst, markWord::monitor_value);\n+    __ br(Assembler::NE, stub->entry());\n+    __ bind(stub->continuation());\n+    __ lsr(dst, dst, markWord::klass_shift);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":115,"deletions":53,"binary":false,"changes":168,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -256,0 +257,7 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  assert(UseCompactObjectHeaders, \"Only use with compact object headers\");\n+  __ bind(_entry);\n+  Register d = _result->as_register();\n+  __ ldr(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ b(_continuation);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1241,1 +1241,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n@@ -2358,2 +2358,2 @@\n-        __ ldrw(tmp, src_klass_addr);\n-        __ ldrw(rscratch1, dst_klass_addr);\n+        __ load_nklass(tmp, src);\n+        __ load_nklass(rscratch1, dst);\n@@ -2362,2 +2362,2 @@\n-        __ ldr(tmp, src_klass_addr);\n-        __ ldr(rscratch1, dst_klass_addr);\n+        __ ldr(tmp, Address(src, oopDesc::klass_offset_in_bytes()));\n+        __ ldr(rscratch1, Address(dst, oopDesc::klass_offset_in_bytes()));\n@@ -2487,3 +2487,0 @@\n-    if (UseCompressedClassPointers) {\n-      __ encode_klass_not_null(tmp);\n-    }\n@@ -2492,8 +2489,1 @@\n-\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(dst, tmp, rscratch1);\n@@ -2501,7 +2491,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, src_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, src_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(src, tmp, rscratch1);\n@@ -2510,7 +2494,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(dst, tmp, rscratch1);\n@@ -2564,1 +2542,5 @@\n-  if (!UseFastLocking) {\n+  if (LockingMode == LM_MONITOR) {\n+    if (op->info() != NULL) {\n+      add_debug_info_for_null_check_here(op->info());\n+      __ null_check(obj, -1);\n+    }\n@@ -2597,1 +2579,12 @@\n-    __ ldrw(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n+    if (UseCompactObjectHeaders) {\n+      \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+      __ ldr(result, Address(obj, oopDesc::mark_offset_in_bytes()));\n+      __ tst(result, markWord::monitor_value);\n+      __ br(Assembler::NE, *op->stub()->entry());\n+      __ bind(*op->stub()->continuation());\n+\n+      \/\/ Shift to get proper narrow Klass*.\n+      __ lsr(result, result, markWord::klass_shift);\n+    } else {\n+      __ ldrw(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":25,"deletions":32,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -92,33 +92,37 @@\n-  \/\/ and mark it as unlocked\n-  orr(hdr, hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  str(hdr, Address(disp_hdr, 0));\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  lea(rscratch2, Address(obj, hdr_offset));\n-  cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/NULL);\n-  \/\/ if the object header was the same, we're done\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) sp <= hdr\n-  \/\/ 3) hdr <= sp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr - sp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  mov(rscratch1, sp);\n-  sub(hdr, hdr, rscratch1);\n-  ands(hdr, hdr, aligned_mask - os::vm_page_size());\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n-  str(hdr, Address(disp_hdr, 0));\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  cbnz(hdr, slow_case);\n-  \/\/ done\n-  bind(done);\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    fast_lock(obj, hdr, rscratch1, rscratch2, slow_case);\n+  } else {\n+    \/\/ and mark it as unlocked\n+    orr(hdr, hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    str(hdr, Address(disp_hdr, 0));\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    lea(rscratch2, Address(obj, hdr_offset));\n+    cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/NULL);\n+    \/\/ if the object header was the same, we're done\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) sp <= hdr\n+    \/\/ 3) hdr <= sp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr - sp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    mov(rscratch1, sp);\n+    sub(hdr, hdr, rscratch1);\n+    ands(hdr, hdr, aligned_mask - os::vm_page_size());\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n+    str(hdr, Address(disp_hdr, 0));\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    cbnz(hdr, slow_case);\n+    \/\/ done\n+    bind(done);\n+  }\n@@ -145,5 +149,8 @@\n-  \/\/ load displaced header\n-  ldr(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is NULL we had recursive locking\n-  \/\/ if we had recursive locking, we are done\n-  cbz(hdr, done);\n+  if (LockingMode != LM_LIGHTWEIGHT) {\n+    \/\/ load displaced header\n+    ldr(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is null we had recursive locking\n+    \/\/ if we had recursive locking, we are done\n+    cbz(hdr, done);\n+  }\n+\n@@ -155,8 +162,8 @@\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  if (hdr_offset) {\n-    lea(rscratch1, Address(obj, hdr_offset));\n-    cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &slow_case);\n+\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    \/\/ We cannot use tbnz here, the target might be too far away and cannot\n+    \/\/ be encoded.\n+    tst(hdr, markWord::monitor_value);\n+    br(Assembler::NE, slow_case);\n+    fast_unlock(obj, hdr, rscratch1, rscratch2, slow_case);\n@@ -164,1 +171,13 @@\n-    cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &slow_case);\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    if (hdr_offset) {\n+      lea(rscratch1, Address(obj, hdr_offset));\n+      cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &slow_case);\n+    } else {\n+      cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &slow_case);\n+    }\n+    \/\/ done\n+    bind(done);\n@@ -166,2 +185,0 @@\n-  \/\/ done\n-  bind(done);\n@@ -182,1 +199,1 @@\n-  if (UseBiasedLocking && !len->is_valid()) {\n+  if (UseCompactObjectHeaders || (UseBiasedLocking && !len->is_valid())) {\n@@ -191,5 +208,7 @@\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    encode_klass_not_null(t1, klass);\n-    strw(t1, Address(obj, oopDesc::klass_offset_in_bytes()));\n-  } else {\n-    str(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  if (!UseCompactObjectHeaders) {\n+    if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+      encode_klass_not_null(t1, klass);\n+      strw(t1, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    } else {\n+      str(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -200,1 +219,1 @@\n-  } else if (UseCompressedClassPointers) {\n+  } else if (UseCompressedClassPointers && !UseCompactObjectHeaders) {\n@@ -219,0 +238,6 @@\n+  \/\/ Zero first 4 bytes, if start offset is not word aligned.\n+  if (!is_aligned(hdr_size_in_bytes, BytesPerWord)) {\n+    strw(zr, Address(obj, hdr_size_in_bytes));\n+    hdr_size_in_bytes += BytesPerInt;\n+  }\n+\n@@ -268,1 +293,1 @@\n-void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int header_size, int f, Register klass, Label& slow_case) {\n+void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int base_offset_in_bytes, int f, Register klass, Label& slow_case) {\n@@ -281,1 +306,1 @@\n-  mov(arr_size, (int32_t)header_size * BytesPerWord + MinObjAlignmentInBytesMask);\n+  mov(arr_size, (int32_t)base_offset_in_bytes + MinObjAlignmentInBytesMask);\n@@ -290,1 +315,1 @@\n-  initialize_body(obj, arr_size, header_size * BytesPerWord, t1, t2);\n+  initialize_body(obj, arr_size, base_offset_in_bytes, t1, t2);\n@@ -307,2 +332,5 @@\n-  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), \"must add explicit null check\");\n-\n+  if (UseCompactObjectHeaders) {\n+    assert(!MacroAssembler::needs_explicit_null_check(oopDesc::mark_offset_in_bytes()), \"must add explicit null check\");\n+  } else {\n+    assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), \"must add explicit null check\");\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":89,"deletions":61,"binary":false,"changes":150,"status":"modified"},{"patch":"@@ -0,0 +1,91 @@\n+\/*\n+ * Copyright (c) 2020, 2022 Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"opto\/c2_MacroAssembler.hpp\"\n+#include \"opto\/c2_CodeStubs.hpp\"\n+#include \"runtime\/objectMonitor.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+\n+#define __ masm.\n+\n+int C2SafepointPollStub::max_size() const {\n+  return 20;\n+}\n+\n+void C2SafepointPollStub::emit(C2_MacroAssembler& masm) {\n+  assert(SharedRuntime::polling_page_return_handler_blob() != NULL,\n+         \"polling page return stub not created yet\");\n+  address stub = SharedRuntime::polling_page_return_handler_blob()->entry_point();\n+\n+  RuntimeAddress callback_addr(stub);\n+\n+  __ bind(entry());\n+  InternalAddress safepoint_pc(masm.pc() - masm.offset() + _safepoint_offset);\n+  __ adr(rscratch1, safepoint_pc);\n+  __ str(rscratch1, Address(rthread, JavaThread::saved_exception_pc_offset()));\n+  __ far_jump(callback_addr);\n+}\n+\n+int C2HandleAnonOMOwnerStub::max_size() const {\n+  \/\/ Max size of stub has been determined by testing with 0, in which case\n+  \/\/ C2CodeStubList::emit() will throw an assertion and report the actual size that\n+  \/\/ is needed.\n+  return 24;\n+}\n+\n+void C2HandleAnonOMOwnerStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register mon = monitor();\n+  Register t = tmp();\n+  assert(t != noreg, \"need tmp register\");\n+\n+  \/\/ Fix owner to be the current thread.\n+  __ str(rthread, Address(mon, ObjectMonitor::owner_offset_in_bytes()));\n+\n+  \/\/ Pop owner object from lock-stack.\n+  __ ldrw(t, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  __ subw(t, t, oopSize);\n+#ifdef ASSERT\n+  __ str(zr, Address(rthread, t));\n+#endif\n+  __ strw(t, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+  __ b(continuation());\n+}\n+\n+int C2LoadNKlassStub::max_size() const {\n+  return 8;\n+}\n+\n+void C2LoadNKlassStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register d = dst();\n+  __ ldr(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ b(continuation());\n+}\n+\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_CodeStubs_aarch64.cpp","additions":91,"deletions":0,"binary":false,"changes":91,"status":"added"},{"patch":"@@ -1,46 +0,0 @@\n-\/*\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/macroAssembler.hpp\"\n-#include \"opto\/compile.hpp\"\n-#include \"opto\/node.hpp\"\n-#include \"opto\/output.hpp\"\n-#include \"runtime\/sharedRuntime.hpp\"\n-\n-#define __ masm.\n-void C2SafepointPollStubTable::emit_stub_impl(MacroAssembler& masm, C2SafepointPollStub* entry) const {\n-  assert(SharedRuntime::polling_page_return_handler_blob() != NULL,\n-         \"polling page return stub not created yet\");\n-  address stub = SharedRuntime::polling_page_return_handler_blob()->entry_point();\n-\n-  RuntimeAddress callback_addr(stub);\n-\n-  __ bind(entry->_stub_label);\n-  InternalAddress safepoint_pc(masm.pc() - masm.offset() + entry->_safepoint_offset);\n-  __ adr(rscratch1, safepoint_pc);\n-  __ str(rscratch1, Address(rthread, JavaThread::saved_exception_pc_offset()));\n-  __ far_jump(callback_addr);\n-}\n-#undef __\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_safepointPollStubTable_aarch64.cpp","additions":0,"deletions":46,"binary":false,"changes":46,"status":"deleted"},{"patch":"@@ -729,1 +729,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -757,21 +757,3 @@\n-    if (UseBiasedLocking) {\n-      biased_locking_enter(lock_reg, obj_reg, swap_reg, tmp, false, done, &slow_case);\n-    }\n-\n-    \/\/ Load (object->mark() | 1) into swap_reg\n-    ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    orr(swap_reg, rscratch1, 1);\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    str(swap_reg, Address(lock_reg, mark_offset));\n-\n-    assert(lock_offset == 0,\n-           \"displached header must be first word in BasicObjectLock\");\n-\n-    Label fail;\n-    if (PrintBiasedLockingStatistics) {\n-      Label fast;\n-      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, fast, &fail);\n-      bind(fast);\n-      atomic_incw(Address((address)BiasedLocking::fast_path_entry_count_addr()),\n-                  rscratch2, rscratch1, tmp);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      ldr(tmp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock(obj_reg, tmp, rscratch1, rscratch2, slow_case);\n@@ -779,1 +761,0 @@\n-      bind(fail);\n@@ -781,2 +762,3 @@\n-      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, done, \/*fallthrough*\/NULL);\n-    }\n+      if (UseBiasedLocking) {\n+        biased_locking_enter(lock_reg, obj_reg, swap_reg, tmp, false, done, &slow_case);\n+      }\n@@ -784,42 +766,22 @@\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & 7) == 0, and\n-    \/\/  2) sp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from sp is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 3 bits clear.\n-    \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n-    \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n-    \/\/ copy\n-    mov(rscratch1, sp);\n-    sub(swap_reg, swap_reg, rscratch1);\n-    ands(swap_reg, swap_reg, (uint64_t)(7 - os::vm_page_size()));\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    str(swap_reg, Address(lock_reg, mark_offset));\n-\n-    if (PrintBiasedLockingStatistics) {\n-      br(Assembler::NE, slow_case);\n-      atomic_incw(Address((address)BiasedLocking::fast_path_entry_count_addr()),\n-                  rscratch2, rscratch1, tmp);\n-    }\n-    br(Assembler::EQ, done);\n+      \/\/ Load (object->mark() | 1) into swap_reg\n+      ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      orr(swap_reg, rscratch1, 1);\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      str(swap_reg, Address(lock_reg, mark_offset));\n+\n+      assert(lock_offset == 0,\n+             \"displached header must be first word in BasicObjectLock\");\n+\n+      Label fail;\n+      if (PrintBiasedLockingStatistics) {\n+        Label fast;\n+        cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, fast, &fail);\n+        bind(fast);\n+        atomic_incw(Address((address)BiasedLocking::fast_path_entry_count_addr()),\n+                    rscratch2, rscratch1, tmp);\n+        b(done);\n+        bind(fail);\n+      } else {\n+        cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, done, \/*fallthrough*\/NULL);\n+      }\n@@ -827,0 +789,43 @@\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & 7) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from sp is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 3 bits clear.\n+      \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n+      \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n+      \/\/ copy\n+      mov(rscratch1, sp);\n+      sub(swap_reg, swap_reg, rscratch1);\n+      ands(swap_reg, swap_reg, (uint64_t)(7 - os::vm_page_size()));\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      str(swap_reg, Address(lock_reg, mark_offset));\n+\n+      if (PrintBiasedLockingStatistics) {\n+        br(Assembler::NE, slow_case);\n+        atomic_incw(Address((address)BiasedLocking::fast_path_entry_count_addr()),\n+                    rscratch2, rscratch1, tmp);\n+      }\n+      br(Assembler::EQ, done);\n+    }\n@@ -830,3 +835,9 @@\n-    call_VM(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            lock_reg);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n+              obj_reg);\n+    } else {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+              lock_reg);\n+    }\n@@ -854,1 +865,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -865,3 +876,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into %r0\n-    lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into %r0\n+      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    }\n@@ -875,3 +888,15 @@\n-    if (UseBiasedLocking) {\n-      biased_locking_exit(obj_reg, header_reg, done);\n-    }\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      Label slow_case;\n+\n+      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+      \/\/ must handle it.\n+      Register tmp = rscratch1;\n+      \/\/ First check for lock-stack underflow.\n+      ldrw(tmp, Address(rthread, JavaThread::lock_stack_top_offset()));\n+      cmpw(tmp, (unsigned)LockStack::start_offset());\n+      br(Assembler::LE, slow_case);\n+      \/\/ Then check if the top of the lock-stack matches the unlocked object.\n+      subw(tmp, tmp, oopSize);\n+      ldr(tmp, Address(rthread, tmp));\n+      cmpoop(tmp, obj_reg);\n+      br(Assembler::NE, slow_case);\n@@ -879,3 +904,9 @@\n-    \/\/ Load the old header from BasicLock structure\n-    ldr(header_reg, Address(swap_reg,\n-                            BasicLock::displaced_header_offset_in_bytes()));\n+      ldr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      tbnz(header_reg, exact_log2(markWord::monitor_value), slow_case);\n+      fast_unlock(obj_reg, header_reg, swap_reg, rscratch1, slow_case);\n+      b(done);\n+      bind(slow_case);\n+    } else {\n+      if (UseBiasedLocking) {\n+        biased_locking_exit(obj_reg, header_reg, done);\n+      }\n@@ -883,2 +914,3 @@\n-    \/\/ Test for recursion\n-    cbz(header_reg, done);\n+      \/\/ Load the old header from BasicLock structure\n+      ldr(header_reg, Address(swap_reg,\n+                              BasicLock::displaced_header_offset_in_bytes()));\n@@ -886,2 +918,2 @@\n-    \/\/ Atomic swap back the old header\n-    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, done, \/*fallthrough*\/NULL);\n+      \/\/ Test for recursion\n+      cbz(header_reg, done);\n@@ -889,0 +921,3 @@\n+      \/\/ Atomic swap back the old header\n+      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, done, \/*fallthrough*\/NULL);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":119,"deletions":84,"binary":false,"changes":203,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -3808,2 +3809,7 @@\n-void MacroAssembler::load_klass(Register dst, Register src) {\n-  if (UseCompressedClassPointers) {\n+\/\/ Loads the obj's Klass* into dst.\n+\/\/ src and dst must be distinct registers\n+\/\/ Preserves all registers (incl src, rscratch1 and rscratch2), but clobbers condition flags\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompressedClassPointers, \"expects UseCompressedClassPointers\");\n+\n+  if (!UseCompactObjectHeaders) {\n@@ -3811,0 +3817,32 @@\n+    return;\n+  }\n+\n+  Label fast;\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  ldr(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  tbz(dst, exact_log2(markWord::monitor_value), fast);\n+\n+  \/\/ Fetch displaced header\n+  ldr(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  \/\/ Fast-path: shift and decode Klass*.\n+  bind(fast);\n+  lsr(dst, dst, markWord::klass_shift);\n+}\n+\n+void MacroAssembler::load_klass(Register dst, Register src, bool null_check_src) {\n+  if (null_check_src) {\n+    if (UseCompactObjectHeaders) {\n+      null_check(src, oopDesc::mark_offset_in_bytes());\n+    } else {\n+      null_check(src, oopDesc::klass_offset_in_bytes());\n+    }\n+  }\n+\n+  if (UseCompressedClassPointers) {\n+    if (UseCompactObjectHeaders) {\n+      load_nklass(dst, src);\n+    } else {\n+      ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -3849,0 +3887,1 @@\n+  assert_different_registers(oop, trial_klass, tmp);\n@@ -3850,1 +3889,5 @@\n-    ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    if (UseCompactObjectHeaders) {\n+      load_nklass(tmp, oop);\n+    } else {\n+      ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -3867,5 +3910,0 @@\n-void MacroAssembler::load_prototype_header(Register dst, Register src) {\n-  load_klass(dst, src);\n-  ldr(dst, Address(dst, Klass::prototype_header_offset()));\n-}\n-\n@@ -3890,0 +3928,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src) {\n+  load_klass(dst, src);\n+  ldr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -5382,0 +5425,94 @@\n+\n+\/\/ Implements fast-locking.\n+\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n+\/\/ Falls through upon success with ZF set.\n+\/\/\n+\/\/  - obj: the object to be locked\n+\/\/  - hdr: the header, already loaded from obj, will be destroyed\n+\/\/  - t1, t2: temporary registers, will be destroyed\n+void MacroAssembler::fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+  \/\/ Check if we would have space on lock-stack for the object.\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  cmpw(t1, (unsigned)LockStack::end_offset() - 1);\n+  br(Assembler::GT, slow);\n+\n+  \/\/ Load (object->mark() | 1) into hdr\n+  orr(hdr, hdr, markWord::unlocked_value);\n+  \/\/ Clear lock-bits, into t2\n+  eor(t2, hdr, markWord::unlocked_value);\n+  \/\/ Try to swing header from unlocked to locked\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ hdr, \/*new*\/ t2, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t1);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful lock, push object on lock-stack\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  str(obj, Address(rthread, t1));\n+  addw(t1, t1, oopSize);\n+  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+}\n+\n+\/\/ Implements fast-unlocking.\n+\/\/ Branches to slow upon failure, with ZF cleared.\n+\/\/ Falls through upon success, with ZF set.\n+\/\/\n+\/\/ - obj: the object to be unlocked\n+\/\/ - hdr: the (pre-loaded) header of the object\n+\/\/ - t1, t2: temporary registers\n+void MacroAssembler::fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+#ifdef ASSERT\n+  {\n+    \/\/ The following checks rely on the fact that LockStack is only ever modified by\n+    \/\/ its owning thread, even if the lock got inflated concurrently; removal of LockStack\n+    \/\/ entries after inflation will happen delayed in that case.\n+\n+    \/\/ Check for lock-stack underflow.\n+    Label stack_ok;\n+    ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    cmpw(t1, (unsigned)LockStack::start_offset());\n+    br(Assembler::GT, stack_ok);\n+    STOP(\"Lock-stack underflow\");\n+    bind(stack_ok);\n+  }\n+  {\n+    \/\/ Check if the top of the lock-stack matches the unlocked object.\n+    Label tos_ok;\n+    subw(t1, t1, oopSize);\n+    ldr(t1, Address(rthread, t1));\n+    cmpoop(t1, obj);\n+    br(Assembler::EQ, tos_ok);\n+    STOP(\"Top of lock-stack does not match the unlocked object\");\n+    bind(tos_ok);\n+  }\n+  {\n+    \/\/ Check that hdr is fast-locked.\n+    Label hdr_ok;\n+    tst(hdr, markWord::lock_mask_in_place);\n+    br(Assembler::EQ, hdr_ok);\n+    STOP(\"Header is not fast-locked\");\n+    bind(hdr_ok);\n+  }\n+#endif\n+\n+  \/\/ Load the new header (unlocked) into t1\n+  orr(t1, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  cmpxchg(obj, hdr, t1, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t2);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  subw(t1, t1, oopSize);\n+#ifdef ASSERT\n+  str(zr, Address(rthread, t1));\n+#endif\n+  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":145,"deletions":8,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -826,1 +826,2 @@\n-  void load_klass(Register dst, Register src);\n+  void load_nklass(Register dst, Register src);\n+  void load_klass(Register dst, Register src, bool null_check = false);\n@@ -853,2 +854,0 @@\n-  void load_prototype_header(Register dst, Register src);\n-\n@@ -857,0 +856,2 @@\n+  void load_prototype_header(Register dst, Register src);\n+\n@@ -1426,0 +1427,3 @@\n+  void fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+  void fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -310,2 +310,1 @@\n-        __ null_check(receiver_reg, oopDesc::klass_offset_in_bytes());\n-        __ load_klass(temp1_recv_klass, receiver_reg);\n+        __ load_klass(temp1_recv_klass, receiver_reg, true);\n","filename":"src\/hotspot\/cpu\/aarch64\/methodHandles_aarch64.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1775,7 +1775,6 @@\n-    if (UseBiasedLocking) {\n-      __ biased_locking_enter(lock_reg, obj_reg, swap_reg, tmp, false, lock_done, &slow_path_lock);\n-    }\n-\n-    \/\/ Load (object->mark() | 1) into swap_reg %r0\n-    __ ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    __ orr(swap_reg, rscratch1, 1);\n+    if (LockingMode == LM_MONITOR) {\n+      __ b(slow_path_lock);\n+    } else if (LockingMode == LM_LEGACY) {\n+      if (UseBiasedLocking) {\n+        __ biased_locking_enter(lock_reg, obj_reg, swap_reg, tmp, false, lock_done, &slow_path_lock);\n+      }\n@@ -1783,2 +1782,3 @@\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    __ str(swap_reg, Address(lock_reg, mark_word_offset));\n+      \/\/ Load (object->mark() | 1) into swap_reg %r0\n+      __ ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ orr(swap_reg, rscratch1, 1);\n@@ -1786,4 +1786,2 @@\n-    \/\/ src -> dest iff dest == r0 else r0 <- dest\n-    { Label here;\n-      __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, lock_done, \/*fallthrough*\/NULL);\n-    }\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n@@ -1791,1 +1789,4 @@\n-    \/\/ Hmm should this move to the slow path code area???\n+      \/\/ src -> dest iff dest == r0 else r0 <- dest\n+      { Label here;\n+        __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, lock_done, \/*fallthrough*\/NULL);\n+      }\n@@ -1793,8 +1794,1 @@\n-    \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-    \/\/  1) (mark & 3) == 0, and\n-    \/\/  2) sp <= mark < mark + os::pagesize()\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 2 bits clear.\n-    \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n+      \/\/ Hmm should this move to the slow path code area???\n@@ -1802,3 +1796,8 @@\n-    __ sub(swap_reg, sp, swap_reg);\n-    __ neg(swap_reg, swap_reg);\n-    __ ands(swap_reg, swap_reg, 3 - os::vm_page_size());\n+      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+      \/\/  1) (mark & 3) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 2 bits clear.\n+      \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n@@ -1806,3 +1805,3 @@\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    __ str(swap_reg, Address(lock_reg, mark_word_offset));\n-    __ br(Assembler::NE, slow_path_lock);\n+      __ sub(swap_reg, sp, swap_reg);\n+      __ neg(swap_reg, swap_reg);\n+      __ ands(swap_reg, swap_reg, 3 - os::vm_page_size());\n@@ -1810,0 +1809,8 @@\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n+      __ br(Assembler::NE, slow_path_lock);\n+    } else {\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ ldr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ fast_lock(obj_reg, swap_reg, tmp, rscratch1, slow_path_lock);\n+    }\n@@ -1931,4 +1938,5 @@\n-    \/\/ Simple recursive lock?\n-\n-    __ ldr(rscratch1, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-    __ cbz(rscratch1, done);\n+    if (LockingMode == LM_LEGACY) {\n+      \/\/ Simple recursive lock?\n+      __ ldr(rscratch1, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+      __ cbz(rscratch1, done);\n+    }\n@@ -1942,4 +1950,7 @@\n-    \/\/ get address of the stack lock\n-    __ lea(r0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-    \/\/  get old displaced header\n-    __ ldr(old_hdr, Address(r0, 0));\n+    if (LockingMode == LM_MONITOR) {\n+      __ b(slow_path_unlock);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ get address of the stack lock\n+      __ lea(r0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+      \/\/  get old displaced header\n+      __ ldr(old_hdr, Address(r0, 0));\n@@ -1947,4 +1958,10 @@\n-    \/\/ Atomic swap old header if oop still contains the stack lock\n-    Label succeed;\n-    __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, succeed, &slow_path_unlock);\n-    __ bind(succeed);\n+      \/\/ Atomic swap old header if oop still contains the stack lock\n+      Label succeed;\n+      __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, succeed, &slow_path_unlock);\n+      __ bind(succeed);\n+    } else {\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n+      __ ldr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ tbnz(old_hdr, exact_log2(markWord::monitor_value), slow_path_unlock);\n+      __ fast_unlock(obj_reg, old_hdr, swap_reg, rscratch1, slow_path_unlock);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":57,"deletions":40,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -3226,2 +3226,1 @@\n-  __ null_check(recv, oopDesc::klass_offset_in_bytes());\n-  __ load_klass(r0, recv);\n+  __ load_klass(r0, recv, true);\n@@ -3316,2 +3315,1 @@\n-  __ null_check(r2, oopDesc::klass_offset_in_bytes());\n-  __ load_klass(r3, r2);\n+  __ load_klass(r3, r2, true);\n@@ -3333,2 +3331,1 @@\n-  __ null_check(r2, oopDesc::klass_offset_in_bytes());\n-  __ load_klass(r3, r2);\n+  __ load_klass(r3, r2, true);\n@@ -3529,1 +3526,1 @@\n-    __ sub(r3, r3, sizeof(oopDesc));\n+    __ sub(r3, r3, oopDesc::base_offset_in_bytes());\n@@ -3534,1 +3531,6 @@\n-      __ add(r2, r0, sizeof(oopDesc));\n+      __ add(r2, r0, oopDesc::base_offset_in_bytes());\n+      if (!is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong)) {\n+        __ strw(zr, Address(__ post(r2, BytesPerInt)));\n+        __ sub(r3, r3, BytesPerInt);\n+        __ cbz(r3, initialize_header);\n+      }\n@@ -3544,1 +3546,1 @@\n-    if (UseBiasedLocking) {\n+    if (UseBiasedLocking || UseCompactObjectHeaders) {\n@@ -3550,3 +3552,4 @@\n-    __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n-    __ store_klass(r0, r4);      \/\/ store klass last\n-\n+    if (!UseCompactObjectHeaders) {\n+      __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n+      __ store_klass(r0, r4);      \/\/ store klass last\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":15,"deletions":12,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -215,1 +215,1 @@\n-  const ptrdiff_t estimate = 124;\n+  const ptrdiff_t estimate = 128;\n","filename":"src\/hotspot\/cpu\/aarch64\/vtableStubs_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -246,0 +246,3 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  Unimplemented();  \/\/ Only needed with compact object headers.\n+}\n","filename":"src\/hotspot\/cpu\/arm\/c1_CodeStubs_arm.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -320,0 +320,3 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  Unimplemented();  \/\/ Only needed with compact object headers.\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_CodeStubs_ppc.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,60 @@\n+\/*\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2022, SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"opto\/c2_MacroAssembler.hpp\"\n+#include \"opto\/c2_CodeStubs.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+\n+#define __ masm.\n+\n+int C2SafepointPollStub::max_size() const {\n+  return 56;\n+}\n+\n+void C2SafepointPollStub::emit(C2_MacroAssembler& masm) {\n+  assert(SharedRuntime::polling_page_return_handler_blob() != NULL,\n+         \"polling page return stub not created yet\");\n+  address stub = SharedRuntime::polling_page_return_handler_blob()->entry_point();\n+\n+  __ bind(entry());\n+  \/\/ Using pc relative address computation.\n+  {\n+    Label next_pc;\n+    __ bl(next_pc);\n+    __ bind(next_pc);\n+  }\n+  int current_offset = __ offset();\n+  \/\/ Code size should not depend on offset: see _stub_size computation in output.cpp\n+  __ load_const32(R12, _safepoint_offset - current_offset);\n+  __ mflr(R0);\n+  __ add(R12, R12, R0);\n+  __ std(R12, in_bytes(JavaThread::saved_exception_pc_offset()), R16_thread);\n+\n+  __ add_const_optimized(R0, R29_TOC, MacroAssembler::offset_to_global_toc(stub));\n+  __ mtctr(R0);\n+  __ bctr();\n+}\n+#undef __\n","filename":"src\/hotspot\/cpu\/ppc\/c2_CodeStubs_ppc.cpp","additions":60,"deletions":0,"binary":false,"changes":60,"status":"added"},{"patch":"@@ -1,57 +0,0 @@\n-\/*\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2021 SAP SE. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"macroAssembler_ppc.inline.hpp\"\n-#include \"opto\/compile.hpp\"\n-#include \"opto\/node.hpp\"\n-#include \"opto\/output.hpp\"\n-#include \"runtime\/sharedRuntime.hpp\"\n-\n-#define __ masm.\n-void C2SafepointPollStubTable::emit_stub_impl(MacroAssembler& masm, C2SafepointPollStub* entry) const {\n-  assert(SharedRuntime::polling_page_return_handler_blob() != NULL,\n-         \"polling page return stub not created yet\");\n-  address stub = SharedRuntime::polling_page_return_handler_blob()->entry_point();\n-\n-  __ bind(entry->_stub_label);\n-  \/\/ Using pc relative address computation.\n-  {\n-    Label next_pc;\n-    __ bl(next_pc);\n-    __ bind(next_pc);\n-  }\n-  int current_offset = __ offset();\n-  \/\/ Code size should not depend on offset: see _stub_size computation in output.cpp\n-  __ load_const32(R12, entry->_safepoint_offset - current_offset);\n-  __ mflr(R0);\n-  __ add(R12, R12, R0);\n-  __ std(R12, in_bytes(JavaThread::saved_exception_pc_offset()), R16_thread);\n-\n-  __ add_const_optimized(R0, R29_TOC, MacroAssembler::offset_to_global_toc(stub));\n-  __ mtctr(R0);\n-  __ bctr();\n-}\n-#undef __\n","filename":"src\/hotspot\/cpu\/ppc\/c2_safepointPollStubTable_ppc.cpp","additions":0,"deletions":57,"binary":false,"changes":57,"status":"deleted"},{"patch":"@@ -985,0 +985,1 @@\n+#include \"opto\/c2_CodeStubs.hpp\"\n@@ -1624,1 +1625,3 @@\n-      code_stub = &C->output()->safepoint_poll_table()->add_safepoint(__ offset());\n+      C2SafepointPollStub* stub = new (C->comp_arena()) C2SafepointPollStub(__ offset());\n+      C->output()->add_stub(stub);\n+      code_stub = &stub->entry();\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -273,0 +273,4 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  Unimplemented();  \/\/ Only needed with compact object headers.\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/c1_CodeStubs_s390.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -303,0 +304,11 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  assert(UseCompactObjectHeaders, \"only with compact headers\");\n+  __ bind(_entry);\n+#ifdef _LP64\n+  Register d = _result->as_register();\n+  __ movq(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ jmp(_continuation);\n+#else\n+  __ should_not_reach_here();\n+#endif\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c1_CodeStubs_x86.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1646,1 +1646,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n@@ -3071,0 +3071,1 @@\n+  Register tmp2 = UseCompactObjectHeaders ? rscratch2 : noreg;\n@@ -3262,7 +3263,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ movl(tmp, src_klass_addr);\n-        __ cmpl(tmp, dst_klass_addr);\n-      } else {\n-        __ movptr(tmp, src_klass_addr);\n-        __ cmpptr(tmp, dst_klass_addr);\n-      }\n+      __ cmp_klass(src, dst, tmp, tmp2);\n@@ -3428,1 +3423,0 @@\n-\n@@ -3430,3 +3424,1 @@\n-\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmp_klass(tmp, dst, tmp2);\n@@ -3434,2 +3426,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, src_klass_addr);\n-      else                   __ cmpptr(tmp, src_klass_addr);\n+      __ cmp_klass(tmp, src, tmp2);\n@@ -3438,2 +3429,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmp_klass(tmp, dst, tmp2);\n@@ -3501,1 +3491,1 @@\n-  if (!UseFastLocking) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -3505,1 +3495,1 @@\n-    if (UseBiasedLocking) {\n+    if (UseBiasedLocking || LockingMode == LM_LIGHTWEIGHT) {\n@@ -3534,1 +3524,14 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    Register tmp = rscratch1;\n+    assert_different_registers(tmp, obj);\n+    assert_different_registers(tmp, result);\n+\n+    \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+    __ movq(result, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    __ testb(result, markWord::monitor_value);\n+    __ jcc(Assembler::notZero, *op->stub()->entry());\n+    __ bind(*op->stub()->continuation());\n+    \/\/ Fast-path: shift and decode Klass*.\n+    __ shrq(result, markWord::klass_shift);\n+    __ decode_klass_not_null(result, tmp);\n+  } else if (UseCompressedClassPointers) {\n@@ -3645,4 +3648,0 @@\n-#ifndef ASSERT\n-      __ jmpb(next);\n-    }\n-#else\n@@ -3651,0 +3650,1 @@\n+#ifdef ASSERT\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":23,"deletions":23,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -317,1 +317,1 @@\n-  if (UseBiasedLocking) {\n+  if (UseBiasedLocking || LockingMode == LM_LIGHTWEIGHT) {\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -46,2 +46,1 @@\n-  assert(hdr != obj && hdr != disp_hdr && obj != disp_hdr, \"registers must be different\");\n-  Label done;\n+  assert_different_registers(hdr, obj, disp_hdr, scratch);\n@@ -64,4 +63,12 @@\n-  if (UseBiasedLocking) {\n-    assert(scratch != noreg, \"should have scratch register at this point\");\n-    biased_locking_enter(disp_hdr, obj, hdr, scratch, rklass_decode_tmp, false, done, &slow_case);\n-  }\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+#ifdef _LP64\n+    const Register thread = r15_thread;\n+#else\n+    const Register thread = disp_hdr;\n+    get_thread(thread);\n+#endif\n+    \/\/ Load object header\n+    movptr(hdr, Address(obj, hdr_offset));\n+    fast_lock_impl(obj, hdr, thread, scratch, slow_case);\n+  } else {\n+    Label done;\n@@ -69,15 +76,44 @@\n-  \/\/ Load object header\n-  movptr(hdr, Address(obj, hdr_offset));\n-  \/\/ and mark it as unlocked\n-  orptr(hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  movptr(Address(disp_hdr, 0), hdr);\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-  cmpxchgptr(disp_hdr, Address(obj, hdr_offset));\n-  \/\/ if the object header was the same, we're done\n-  if (PrintBiasedLockingStatistics) {\n-    cond_inc32(Assembler::equal,\n-               ExternalAddress((address)BiasedLocking::fast_path_entry_count_addr()));\n+    if (UseBiasedLocking) {\n+      assert(scratch != noreg, \"should have scratch register at this point\");\n+      biased_locking_enter(disp_hdr, obj, hdr, scratch, rklass_decode_tmp, false, done, &slow_case);\n+    }\n+\n+    \/\/ Load object header\n+    movptr(hdr, Address(obj, hdr_offset));\n+    \/\/ and mark it as unlocked\n+    orptr(hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    movptr(Address(disp_hdr, 0), hdr);\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n+    cmpxchgptr(disp_hdr, Address(obj, hdr_offset));\n+    \/\/ if the object header was the same, we're done\n+    if (PrintBiasedLockingStatistics) {\n+      cond_inc32(Assembler::equal,\n+                 ExternalAddress((address)BiasedLocking::fast_path_entry_count_addr()));\n+    }\n+    jcc(Assembler::equal, done);\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) rsp <= hdr\n+    \/\/ 3) hdr <= rsp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr - rsp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    subptr(hdr, rsp);\n+    andptr(hdr, aligned_mask - os::vm_page_size());\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n+    movptr(Address(disp_hdr, 0), hdr);\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    jcc(Assembler::notZero, slow_case);\n+    \/\/ done\n+    bind(done);\n@@ -85,23 +121,0 @@\n-  jcc(Assembler::equal, done);\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) rsp <= hdr\n-  \/\/ 3) hdr <= rsp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr - rsp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  subptr(hdr, rsp);\n-  andptr(hdr, aligned_mask - os::vm_page_size());\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n-  movptr(Address(disp_hdr, 0), hdr);\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  jcc(Assembler::notZero, slow_case);\n-  \/\/ done\n-  bind(done);\n@@ -117,1 +130,0 @@\n-  Label done;\n@@ -119,1 +131,1 @@\n-  if (UseBiasedLocking) {\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n@@ -122,2 +134,6 @@\n-    biased_locking_exit(obj, hdr, done);\n-  }\n+    verify_oop(obj);\n+    movptr(disp_hdr, Address(obj, hdr_offset));\n+    andptr(disp_hdr, ~(int32_t)markWord::lock_mask_in_place);\n+    fast_unlock_impl(obj, disp_hdr, hdr, slow_case);\n+  } else {\n+    Label done;\n@@ -125,9 +141,27 @@\n-  \/\/ load displaced header\n-  movptr(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is NULL we had recursive locking\n-  testptr(hdr, hdr);\n-  \/\/ if we had recursive locking, we are done\n-  jcc(Assembler::zero, done);\n-  if (!UseBiasedLocking) {\n-    \/\/ load object\n-    movptr(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+    if (UseBiasedLocking) {\n+      \/\/ load object\n+      movptr(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+      biased_locking_exit(obj, hdr, done);\n+    }\n+\n+    \/\/ load displaced header\n+    movptr(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is NULL we had recursive locking\n+    testptr(hdr, hdr);\n+    \/\/ if we had recursive locking, we are done\n+    jcc(Assembler::zero, done);\n+    if (!UseBiasedLocking) {\n+      \/\/ load object\n+      movptr(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+    }\n+    verify_oop(obj);\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n+    cmpxchgptr(hdr, Address(obj, hdr_offset));\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    jcc(Assembler::notEqual, slow_case);\n+    \/\/ done\n+    bind(done);\n@@ -135,11 +169,0 @@\n-  verify_oop(obj);\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-  cmpxchgptr(hdr, Address(obj, hdr_offset));\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  jcc(Assembler::notEqual, slow_case);\n-  \/\/ done\n-  bind(done);\n@@ -160,4 +183,2 @@\n-  assert_different_registers(obj, klass, len);\n-  Register tmp_encode_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n-  if (UseBiasedLocking && !len->is_valid()) {\n-    assert_different_registers(obj, klass, len, t1, t2);\n+  assert_different_registers(obj, klass, len, t1, t2);\n+  if (UseCompactObjectHeaders || (UseBiasedLocking && !len->is_valid())) {\n@@ -167,2 +188,1 @@\n-    \/\/ This assumes that all prototype bits fit in an int32_t\n-    movptr(Address(obj, oopDesc::mark_offset_in_bytes ()), (int32_t)(intptr_t)markWord::prototype().value());\n+    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n@@ -170,0 +190,2 @@\n+\n+  if (!UseCompactObjectHeaders) {\n@@ -171,5 +193,5 @@\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    movptr(t1, klass);\n-    encode_klass_not_null(t1, tmp_encode_klass);\n-    movl(Address(obj, oopDesc::klass_offset_in_bytes()), t1);\n-  } else\n+    if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+      movptr(t1, klass);\n+      encode_klass_not_null(t1, rscratch1);\n+      movl(Address(obj, oopDesc::klass_offset_in_bytes()), t1);\n+    } else\n@@ -177,2 +199,3 @@\n-  {\n-    movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n+    {\n+      movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n+    }\n@@ -180,1 +203,0 @@\n-\n@@ -185,1 +207,1 @@\n-  else if (UseCompressedClassPointers) {\n+  else if (UseCompressedClassPointers && !UseCompactObjectHeaders) {\n@@ -228,0 +250,1 @@\n+    int hdr_size_aligned = align_up(hdr_size_in_bytes, BytesPerWord); \/\/ klass gap is already cleared by init_header().\n@@ -230,1 +253,1 @@\n-      initialize_body(obj, index, hdr_size_in_bytes, t1_zero);\n+      initialize_body(obj, index, hdr_size_aligned, t1_zero);\n@@ -235,1 +258,1 @@\n-      for (int i = hdr_size_in_bytes; i < con_size_in_bytes; i += BytesPerWord)\n+      for (int i = hdr_size_aligned; i < con_size_in_bytes; i += BytesPerWord)\n@@ -237,1 +260,1 @@\n-    } else if (con_size_in_bytes > hdr_size_in_bytes) {\n+    } else if (con_size_in_bytes > hdr_size_aligned) {\n@@ -244,1 +267,1 @@\n-      if (((con_size_in_bytes - hdr_size_in_bytes) & 4) != 0)\n+      if (((con_size_in_bytes - hdr_size_aligned) & 4) != 0)\n@@ -249,1 +272,1 @@\n-        movptr(Address(obj, index, Address::times_8, hdr_size_in_bytes - (1*BytesPerWord)),\n+        movptr(Address(obj, index, Address::times_8, hdr_size_aligned - (1*BytesPerWord)),\n@@ -251,1 +274,1 @@\n-        NOT_LP64(movptr(Address(obj, index, Address::times_8, hdr_size_in_bytes - (2*BytesPerWord)),\n+        NOT_LP64(movptr(Address(obj, index, Address::times_8, hdr_size_aligned - (2*BytesPerWord)),\n@@ -267,1 +290,1 @@\n-void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int header_size, Address::ScaleFactor f, Register klass, Label& slow_case) {\n+void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int base_offset_in_bytes, Address::ScaleFactor f, Register klass, Label& slow_case) {\n@@ -280,1 +303,1 @@\n-  movptr(arr_size, (int32_t)header_size * BytesPerWord + MinObjAlignmentInBytesMask);\n+  movptr(arr_size, (int32_t)base_offset_in_bytes + MinObjAlignmentInBytesMask);\n@@ -290,1 +313,1 @@\n-  initialize_body(obj, arr_size, header_size * BytesPerWord, len_zero);\n+  initialize_body(obj, arr_size, base_offset_in_bytes, len_zero);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":115,"deletions":92,"binary":false,"changes":207,"status":"modified"},{"patch":"@@ -0,0 +1,99 @@\n+\/*\n+ * Copyright (c) 2020, 2022 Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"opto\/c2_MacroAssembler.hpp\"\n+#include \"opto\/c2_CodeStubs.hpp\"\n+#include \"runtime\/objectMonitor.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+\n+#define __ masm.\n+\n+int C2SafepointPollStub::max_size() const {\n+  return 33;\n+}\n+\n+void C2SafepointPollStub::emit(C2_MacroAssembler& masm) {\n+  assert(SharedRuntime::polling_page_return_handler_blob() != NULL,\n+         \"polling page return stub not created yet\");\n+  address stub = SharedRuntime::polling_page_return_handler_blob()->entry_point();\n+\n+  RuntimeAddress callback_addr(stub);\n+\n+  __ bind(entry());\n+  InternalAddress safepoint_pc(masm.pc() - masm.offset() + _safepoint_offset);\n+#ifdef _LP64\n+  __ lea(rscratch1, safepoint_pc);\n+  __ movptr(Address(r15_thread, JavaThread::saved_exception_pc_offset()), rscratch1);\n+#else\n+  const Register tmp1 = rcx;\n+  const Register tmp2 = rdx;\n+  __ push(tmp1);\n+  __ push(tmp2);\n+\n+  __ lea(tmp1, safepoint_pc);\n+  __ get_thread(tmp2);\n+  __ movptr(Address(tmp2, JavaThread::saved_exception_pc_offset()), tmp1);\n+\n+  __ pop(tmp2);\n+  __ pop(tmp1);\n+#endif\n+  __ jump(callback_addr);\n+}\n+\n+#ifdef _LP64\n+int C2HandleAnonOMOwnerStub::max_size() const {\n+  \/\/ Max size of stub has been determined by testing with 0, in which case\n+  \/\/ C2CodeStubList::emit() will throw an assertion and report the actual size that\n+  \/\/ is needed.\n+  return DEBUG_ONLY(36) NOT_DEBUG(21);\n+}\n+\n+void C2HandleAnonOMOwnerStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register mon = monitor();\n+  Register t = tmp();\n+  __ movptr(Address(mon, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), r15_thread);\n+  __ subl(Address(r15_thread, JavaThread::lock_stack_top_offset()), oopSize);\n+#ifdef ASSERT\n+  __ movl(t, Address(r15_thread, JavaThread::lock_stack_top_offset()));\n+  __ movptr(Address(r15_thread, t), 0);\n+#endif\n+  __ jmp(continuation());\n+}\n+\n+int C2LoadNKlassStub::max_size() const {\n+  return 10;\n+}\n+\n+void C2LoadNKlassStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register d = dst();\n+  __ movq(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ jmp(continuation());\n+}\n+#endif\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/x86\/c2_CodeStubs_x86.cpp","additions":99,"deletions":0,"binary":false,"changes":99,"status":"added"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"opto\/c2_CodeStubs.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"opto\/output.hpp\"\n@@ -449,1 +451,1 @@\n-                                 Register scrReg, Register cx1Reg, Register cx2Reg,\n+                                 Register scrReg, Register cx1Reg, Register cx2Reg, Register thread,\n@@ -516,1 +518,1 @@\n-  jccb(Assembler::notZero, IsInflated);\n+  jcc(Assembler::notZero, IsInflated);\n@@ -518,21 +520,30 @@\n-  \/\/ Attempt stack-locking ...\n-  orptr (tmpReg, markWord::unlocked_value);\n-  movptr(Address(boxReg, 0), tmpReg);          \/\/ Anticipate successful CAS\n-  lock();\n-  cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      \/\/ Updates tmpReg\n-  if (counters != NULL) {\n-    cond_inc32(Assembler::equal,\n-               ExternalAddress((address)counters->fast_path_entry_count_addr()));\n-  }\n-  jcc(Assembler::equal, DONE_LABEL);           \/\/ Success\n-\n-  \/\/ Recursive locking.\n-  \/\/ The object is stack-locked: markword contains stack pointer to BasicLock.\n-  \/\/ Locked by current thread if difference with current SP is less than one page.\n-  subptr(tmpReg, rsp);\n-  \/\/ Next instruction set ZFlag == 1 (Success) if difference is less then one page.\n-  andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - os::vm_page_size())) );\n-  movptr(Address(boxReg, 0), tmpReg);\n-  if (counters != NULL) {\n-    cond_inc32(Assembler::equal,\n-               ExternalAddress((address)counters->fast_path_entry_count_addr()));\n+  if (LockingMode == LM_MONITOR) {\n+    \/\/ Clear ZF so that we take the slow path at the DONE label. objReg is known to be not 0.\n+    testptr(objReg, objReg);\n+  } else if (LockingMode == LM_LEGACY) {\n+    \/\/ Attempt stack-locking ...\n+    orptr (tmpReg, markWord::unlocked_value);\n+    movptr(Address(boxReg, 0), tmpReg);          \/\/ Anticipate successful CAS\n+    lock();\n+    cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      \/\/ Updates tmpReg\n+    if (counters != NULL) {\n+      cond_inc32(Assembler::equal,\n+                 ExternalAddress((address)counters->fast_path_entry_count_addr()));\n+    }\n+    jcc(Assembler::equal, DONE_LABEL);           \/\/ Success\n+\n+    \/\/ Recursive locking.\n+    \/\/ The object is stack-locked: markword contains stack pointer to BasicLock.\n+    \/\/ Locked by current thread if difference with current SP is less than one page.\n+    subptr(tmpReg, rsp);\n+    \/\/ Next instruction set ZFlag == 1 (Success) if difference is less then one page.\n+    andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - os::vm_page_size())) );\n+    movptr(Address(boxReg, 0), tmpReg);\n+    if (counters != NULL) {\n+      cond_inc32(Assembler::equal,\n+                 ExternalAddress((address)counters->fast_path_entry_count_addr()));\n+    }\n+  } else {\n+    assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n+    fast_lock_impl(objReg, tmpReg, thread, scrReg, DONE_LABEL);\n+    xorl(tmpReg, tmpReg); \/\/ Set ZF=1 to indicate success\n@@ -687,2 +698,4 @@\n-  cmpptr(Address(boxReg, 0), (int32_t)NULL_WORD);                   \/\/ Examine the displaced header\n-  jcc   (Assembler::zero, DONE_LABEL);                              \/\/ 0 indicates recursive stack-lock\n+  if (LockingMode == LM_LEGACY) {\n+    cmpptr(Address(boxReg, 0), (int32_t)NULL_WORD);                   \/\/ Examine the displaced header\n+    jcc   (Assembler::zero, DONE_LABEL);                              \/\/ 0 indicates recursive stack-lock\n+  }\n@@ -690,2 +703,4 @@\n-  testptr(tmpReg, markWord::monitor_value);                         \/\/ Inflated?\n-  jccb  (Assembler::zero, Stacked);\n+  if (LockingMode != LM_MONITOR) {\n+    testptr(tmpReg, markWord::monitor_value);                         \/\/ Inflated?\n+    jcc(Assembler::zero, Stacked);\n+  }\n@@ -694,0 +709,18 @@\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    \/\/ If the owner is ANONYMOUS, we need to fix it.\n+    testb(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t) ObjectMonitor::ANONYMOUS_OWNER);\n+#ifdef _LP64\n+    if (!Compile::current()->output()->in_scratch_emit_size()) {\n+      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmpReg, boxReg);\n+      Compile::current()->output()->add_stub(stub);\n+      jcc(Assembler::notEqual, stub->entry());\n+      bind(stub->continuation());\n+    } else\n+#endif\n+    {\n+      \/\/ We can't easily implement this optimization on 32 bit because we don't have a thread register.\n+      \/\/ Call the slow-path instead.\n+      jcc(Assembler::notEqual, DONE_LABEL);\n+    }\n+  }\n+\n@@ -702,1 +735,1 @@\n-    jmpb(DONE_LABEL);\n+    jmp(DONE_LABEL);\n@@ -737,1 +770,1 @@\n-  jccb  (Assembler::notZero, CheckSucc);\n+  jccb  (Assembler::notZero, DONE_LABEL);\n@@ -741,9 +774,0 @@\n-  bind (Stacked);\n-  \/\/ It's not inflated and it's not recursively stack-locked and it's not biased.\n-  \/\/ It must be stack-locked.\n-  \/\/ Try to reset the header to displaced header.\n-  \/\/ The \"box\" value on the stack is stable, so we can reload\n-  \/\/ and be assured we observe the same value as above.\n-  movptr(tmpReg, Address(boxReg, 0));\n-  lock();\n-  cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n@@ -833,5 +857,0 @@\n-  bind  (Stacked);\n-  movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n-  lock();\n-  cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n-\n@@ -839,0 +858,12 @@\n+  if (LockingMode != LM_MONITOR) {\n+    bind  (Stacked);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      mov(boxReg, tmpReg);\n+      fast_unlock_impl(objReg, boxReg, tmpReg, DONE_LABEL);\n+      xorl(tmpReg, tmpReg);\n+    } else if (LockingMode == LM_LEGACY) {\n+      movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n+      lock();\n+      cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+    }\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":74,"deletions":43,"binary":false,"changes":117,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-                 Register scr, Register cx1, Register cx2,\n+                 Register scr, Register cx1, Register cx2, Register thread,\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1,60 +0,0 @@\n-\/*\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/macroAssembler.hpp\"\n-#include \"opto\/compile.hpp\"\n-#include \"opto\/node.hpp\"\n-#include \"opto\/output.hpp\"\n-#include \"runtime\/sharedRuntime.hpp\"\n-\n-#define __ masm.\n-void C2SafepointPollStubTable::emit_stub_impl(MacroAssembler& masm, C2SafepointPollStub* entry) const {\n-  assert(SharedRuntime::polling_page_return_handler_blob() != NULL,\n-         \"polling page return stub not created yet\");\n-  address stub = SharedRuntime::polling_page_return_handler_blob()->entry_point();\n-\n-  RuntimeAddress callback_addr(stub);\n-\n-  __ bind(entry->_stub_label);\n-  InternalAddress safepoint_pc(masm.pc() - masm.offset() + entry->_safepoint_offset);\n-#ifdef _LP64\n-  __ lea(rscratch1, safepoint_pc);\n-  __ movptr(Address(r15_thread, JavaThread::saved_exception_pc_offset()), rscratch1);\n-#else\n-  const Register tmp1 = rcx;\n-  const Register tmp2 = rdx;\n-  __ push(tmp1);\n-  __ push(tmp2);\n-\n-  __ lea(tmp1, safepoint_pc);\n-  __ get_thread(tmp2);\n-  __ movptr(Address(tmp2, JavaThread::saved_exception_pc_offset()), tmp1);\n-\n-  __ pop(tmp2);\n-  __ pop(tmp1);\n-#endif\n-  __ jump(callback_addr);\n-}\n-#undef __\n","filename":"src\/hotspot\/cpu\/x86\/c2_safepointPollStubTable_x86.cpp","additions":0,"deletions":60,"binary":false,"changes":60,"status":"deleted"},{"patch":"@@ -60,1 +60,1 @@\n-  jmpb(next);\n+  jmp(next);\n@@ -1200,1 +1200,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -1234,2 +1234,14 @@\n-    \/\/ Load immediate 1 into swap_reg %rax\n-    movl(swap_reg, (int32_t)1);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = lock_reg;\n+      get_thread(thread);\n+#endif\n+      \/\/ Load object header, prepare for CAS from unlocked to locked.\n+      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock_impl(obj_reg, swap_reg, thread, tmp_reg, slow_case);\n+      jmp(done);\n+    } else {\n+      \/\/ Load immediate 1 into swap_reg %rax\n+      movl(swap_reg, (int32_t)1);\n@@ -1237,2 +1249,2 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg %rax\n-    orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      \/\/ Load (object->mark() | 1) into swap_reg %rax\n+      orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1240,2 +1252,2 @@\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    movptr(Address(lock_reg, mark_offset), swap_reg);\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      movptr(Address(lock_reg, mark_offset), swap_reg);\n@@ -1243,2 +1255,2 @@\n-    assert(lock_offset == 0,\n-           \"displaced header must be first word in BasicObjectLock\");\n+      assert(lock_offset == 0,\n+             \"displaced header must be first word in BasicObjectLock\");\n@@ -1246,46 +1258,48 @@\n-    lock();\n-    cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    if (PrintBiasedLockingStatistics) {\n-      cond_inc32(Assembler::zero,\n-                 ExternalAddress((address) BiasedLocking::fast_path_entry_count_addr()));\n-    }\n-    jcc(Assembler::zero, done);\n-\n-    const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n-\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & zero_bits) == 0, and\n-    \/\/  2) rsp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from rsp is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant bits clear.\n-    \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n-    subptr(swap_reg, rsp);\n-    andptr(swap_reg, zero_bits - os::vm_page_size());\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    movptr(Address(lock_reg, mark_offset), swap_reg);\n-\n-    if (PrintBiasedLockingStatistics) {\n-      cond_inc32(Assembler::zero,\n-                 ExternalAddress((address) BiasedLocking::fast_path_entry_count_addr()));\n+      lock();\n+      cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      if (PrintBiasedLockingStatistics) {\n+        cond_inc32(Assembler::zero,\n+                   ExternalAddress((address) BiasedLocking::fast_path_entry_count_addr()));\n+      }\n+      jcc(Assembler::zero, done);\n+\n+      const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n+\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & zero_bits) == 0, and\n+      \/\/  2) rsp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from rsp is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant bits clear.\n+      \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n+      subptr(swap_reg, rsp);\n+      andptr(swap_reg, zero_bits - os::vm_page_size());\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      movptr(Address(lock_reg, mark_offset), swap_reg);\n+\n+      if (PrintBiasedLockingStatistics) {\n+        cond_inc32(Assembler::zero,\n+                   ExternalAddress((address) BiasedLocking::fast_path_entry_count_addr()));\n+      }\n+      jcc(Assembler::zero, done);\n@@ -1293,2 +1307,0 @@\n-    jcc(Assembler::zero, done);\n-\n@@ -1298,4 +1310,9 @@\n-    call_VM(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            lock_reg);\n-\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n+              obj_reg);\n+    } else {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+              lock_reg);\n+    }\n@@ -1323,1 +1340,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -1326,1 +1343,1 @@\n-    Label done;\n+    Label done, slow_case;\n@@ -1334,3 +1351,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into %rax\n-    lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into %rax\n+      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    }\n@@ -1344,7 +1363,21 @@\n-    if (UseBiasedLocking) {\n-      biased_locking_exit(obj_reg, header_reg, done);\n-    }\n-\n-    \/\/ Load the old header from BasicLock structure\n-    movptr(header_reg, Address(swap_reg,\n-                               BasicLock::displaced_header_offset_in_bytes()));\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = header_reg;\n+      get_thread(thread);\n+#endif\n+      \/\/ Handle unstructured locking.\n+      Register tmp = swap_reg;\n+      movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n+      cmpptr(obj_reg, Address(thread, tmp, Address::times_1,  -oopSize));\n+      jcc(Assembler::notEqual, slow_case);\n+      \/\/ Try to swing header from locked to unlock.\n+      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+      fast_unlock_impl(obj_reg, swap_reg, header_reg, slow_case);\n+      jmp(done);\n+    } else {\n+      if (UseBiasedLocking) {\n+        biased_locking_exit(obj_reg, header_reg, done);\n+      }\n@@ -1352,2 +1385,3 @@\n-    \/\/ Test for recursion\n-    testptr(header_reg, header_reg);\n+      \/\/ Load the old header from BasicLock structure\n+      movptr(header_reg, Address(swap_reg,\n+                                 BasicLock::displaced_header_offset_in_bytes()));\n@@ -1355,2 +1389,2 @@\n-    \/\/ zero for recursive case\n-    jcc(Assembler::zero, done);\n+      \/\/ Test for recursion\n+      testptr(header_reg, header_reg);\n@@ -1358,3 +1392,2 @@\n-    \/\/ Atomic swap back the old header\n-    lock();\n-    cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      \/\/ zero for recursive case\n+      jcc(Assembler::zero, done);\n@@ -1362,2 +1395,3 @@\n-    \/\/ zero for simple unlock of a stack-lock case\n-    jcc(Assembler::zero, done);\n+      \/\/ Atomic swap back the old header\n+      lock();\n+      cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1365,0 +1399,3 @@\n+      \/\/ zero for simple unlock of a stack-lock case\n+      jcc(Assembler::zero, done);\n+    }\n@@ -1366,0 +1403,1 @@\n+    bind(slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":121,"deletions":83,"binary":false,"changes":204,"status":"modified"},{"patch":"@@ -57,0 +57,6 @@\n+#ifdef COMPILER2\n+#include \"opto\/c2_CodeStubs.hpp\"\n+#include \"opto\/compile.hpp\"\n+#include \"opto\/output.hpp\"\n+#endif\n+\n@@ -3774,1 +3780,1 @@\n-  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  assert((offset_in_bytes & (BytesPerInt - 1)) == 0, \"offset must be a multiple of BytesPerInt\");\n@@ -3780,0 +3786,13 @@\n+  \/\/ Emit single 32bit store to clear leading bytes, if necessary.\n+  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n+#ifdef _LP64\n+  if (!is_aligned(offset_in_bytes, BytesPerWord)) {\n+    movl(Address(address, offset_in_bytes), temp);\n+    offset_in_bytes += BytesPerInt;\n+    decrement(length_in_bytes, BytesPerInt);\n+  }\n+  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  testptr(length_in_bytes, length_in_bytes);\n+  jcc(Assembler::zero, done);\n+#endif\n+\n@@ -3792,1 +3811,0 @@\n-  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n@@ -4736,1 +4754,23 @@\n-void MacroAssembler::load_klass(Register dst, Register src, Register tmp) {\n+#ifdef _LP64\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompressedClassPointers, \"expect compressed class pointers\");\n+\n+  if (!UseCompactObjectHeaders) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    return;\n+  }\n+\n+ Label fast;\n+  movq(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  testb(dst, markWord::monitor_value);\n+  jccb(Assembler::zero, fast);\n+\n+  \/\/ Fetch displaced header\n+  movq(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  bind(fast);\n+  shrq(dst, markWord::klass_shift);\n+}\n+#endif\n+\n+void MacroAssembler::load_klass(Register dst, Register src, Register tmp, bool null_check_src) {\n@@ -4739,0 +4779,7 @@\n+  if (null_check_src) {\n+    if (UseCompactObjectHeaders) {\n+      null_check(src, oopDesc::mark_offset_in_bytes());\n+    } else {\n+      null_check(src, oopDesc::klass_offset_in_bytes());\n+    }\n+  }\n@@ -4741,1 +4788,1 @@\n-    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    load_nklass(dst, src);\n@@ -4754,0 +4801,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -4762,1 +4810,40 @@\n-    movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n+   movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n+}\n+\n+void MacroAssembler::cmp_klass(Register klass, Register obj, Register tmp) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    \/\/ NOTE: We need to deal with possible ObjectMonitor in object header.\n+    \/\/ Eventually we might be able to do simple movl & cmpl like in\n+    \/\/ the CCP path below.\n+    load_nklass(tmp, obj);\n+    cmpl(klass, tmp);\n+  } else if (UseCompressedClassPointers) {\n+    cmpl(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  } else\n+#endif\n+  {\n+    cmpptr(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n+void MacroAssembler::cmp_klass(Register src, Register dst, Register tmp1, Register tmp2) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    \/\/ NOTE: We need to deal with possible ObjectMonitor in object header.\n+    \/\/ Eventually we might be able to do simple movl & cmpl like in\n+    \/\/ the CCP path below.\n+    assert(tmp2 != noreg, \"need tmp2\");\n+    assert_different_registers(src, dst, tmp1, tmp2);\n+    load_nklass(tmp1, src);\n+    load_nklass(tmp2, dst);\n+    cmpl(tmp1, tmp2);\n+  } else if (UseCompressedClassPointers) {\n+    movl(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    cmpl(tmp1, Address(dst, oopDesc::klass_offset_in_bytes()));\n+  } else\n+#endif\n+  {\n+    movptr(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    cmpptr(tmp1, Address(dst, oopDesc::klass_offset_in_bytes()));\n+  }\n@@ -8689,0 +8776,67 @@\n+\n+\/\/ Implements fast-locking.\n+\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n+\/\/ Falls through upon success with unspecified ZF.\n+\/\/\n+\/\/ obj: the object to be locked\n+\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ thread: the thread which attempts to lock obj\n+\/\/ tmp: a temporary register\n+void MacroAssembler::fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, thread, tmp);\n+\n+  \/\/ First we need to check if the lock-stack has room for pushing the object reference.\n+  \/\/ Note: we subtract 1 from the end-offset so that we can do a 'greater' comparison, instead\n+  \/\/ of 'greaterEqual' below, which readily clears the ZF. This makes C2 code a little simpler and\n+  \/\/ avoids one branch.\n+  cmpl(Address(thread, JavaThread::lock_stack_top_offset()), LockStack::end_offset() - 1);\n+  jcc(Assembler::greater, slow);\n+\n+  \/\/ Now we attempt to take the fast-lock.\n+  \/\/ Clear lock_mask bits (locked state).\n+  andptr(hdr, ~(int32_t)markWord::lock_mask_in_place);\n+  movptr(tmp, hdr);\n+  \/\/ Set unlocked_value bit.\n+  orptr(hdr, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+\n+  \/\/ If successful, push object to lock-stack.\n+  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n+  movptr(Address(thread, tmp), obj);\n+  incrementl(tmp, oopSize);\n+  movl(Address(thread, JavaThread::lock_stack_top_offset()), tmp);\n+}\n+\n+\/\/ Implements fast-unlocking.\n+\/\/ Branches to slow upon failure, with ZF cleared.\n+\/\/ Falls through upon success, with unspecified ZF.\n+\/\/\n+\/\/ obj: the object to be unlocked\n+\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ tmp: a temporary register\n+void MacroAssembler::fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, tmp);\n+\n+  \/\/ Mark-word must be lock_mask now, try to swing it back to unlocked_value.\n+  movptr(tmp, hdr); \/\/ The expected old value\n+  orptr(tmp, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+  \/\/ Pop the lock object from the lock-stack.\n+#ifdef _LP64\n+  const Register thread = r15_thread;\n+#else\n+  const Register thread = rax;\n+  get_thread(thread);\n+#endif\n+  subl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+#ifdef ASSERT\n+  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n+  movptr(Address(thread, tmp), 0);\n+#endif\n+}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":159,"deletions":5,"binary":false,"changes":164,"status":"modified"},{"patch":"@@ -342,1 +342,4 @@\n-  void load_klass(Register dst, Register src, Register tmp);\n+  void load_klass(Register dst, Register src, Register tmp, bool null_check_src = false);\n+#ifdef _LP64\n+  void load_nklass(Register dst, Register src);\n+#endif\n@@ -345,0 +348,8 @@\n+  \/\/ Compares the Klass pointer of an object to a given Klass (which might be narrow,\n+  \/\/ depending on UseCompressedClassPointers).\n+  void cmp_klass(Register klass, Register dst, Register tmp);\n+\n+  \/\/ Compares the Klass pointer of two objects o1 and o2. Result is in the condition flags.\n+  \/\/ Uses t1 and t2 as temporary registers.\n+  void cmp_klass(Register src, Register dst, Register tmp1, Register tmp2);\n+\n@@ -1915,0 +1926,3 @@\n+\n+  void fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow);\n+  void fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -367,2 +367,1 @@\n-        __ null_check(receiver_reg, oopDesc::klass_offset_in_bytes());\n-        __ load_klass(temp1_recv_klass, receiver_reg, temp2);\n+        __ load_klass(temp1_recv_klass, receiver_reg, temp2, true);\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -76,2 +76,7 @@\n-  __ shrptr(result, markWord::hash_shift);\n-  __ andptr(result, markWord::hash_mask);\n+  if (UseCompactObjectHeaders) {\n+    __ shrptr(result, markWord::hash_shift_compact);\n+    __ andptr(result, markWord::hash_mask_compact);\n+  } else {\n+    __ shrptr(result, markWord::hash_shift);\n+    __ andptr(result, markWord::hash_mask);\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1825,4 +1825,7 @@\n-    if (UseBiasedLocking) {\n-      \/\/ Note that oop_handle_reg is trashed during this call\n-      __ biased_locking_enter(lock_reg, obj_reg, swap_reg, oop_handle_reg, noreg, false, lock_done, &slow_path_lock);\n-    }\n+    if (LockingMode == LM_MONITOR) {\n+      __ jmp(slow_path_lock);\n+    } else if (LockingMode == LM_LEGACY) {\n+      if (UseBiasedLocking) {\n+        \/\/ Note that oop_handle_reg is trashed during this call\n+        __ biased_locking_enter(lock_reg, obj_reg, swap_reg, oop_handle_reg, noreg, false, lock_done, &slow_path_lock);\n+      }\n@@ -1830,2 +1833,2 @@\n-    \/\/ Load immediate 1 into swap_reg %rax,\n-    __ movptr(swap_reg, 1);\n+      \/\/ Load immediate 1 into swap_reg %rax,\n+      __ movptr(swap_reg, 1);\n@@ -1833,2 +1836,2 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg %rax,\n-    __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      \/\/ Load (object->mark() | 1) into swap_reg %rax,\n+      __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1836,2 +1839,2 @@\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n@@ -1839,5 +1842,5 @@\n-    \/\/ src -> dest iff dest == rax, else rax, <- dest\n-    \/\/ *obj_reg = lock_reg iff *obj_reg == rax, else rax, = *(obj_reg)\n-    __ lock();\n-    __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    __ jcc(Assembler::equal, lock_done);\n+      \/\/ src -> dest iff dest == rax, else rax, <- dest\n+      \/\/ *obj_reg = lock_reg iff *obj_reg == rax, else rax, = *(obj_reg)\n+      __ lock();\n+      __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ jcc(Assembler::equal, lock_done);\n@@ -1845,8 +1848,8 @@\n-    \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-    \/\/  1) (mark & 3) == 0, and\n-    \/\/  2) rsp <= mark < mark + os::pagesize()\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 2 bits clear.\n-    \/\/ NOTE: the oopMark is in swap_reg %rax, as the result of cmpxchg\n+      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+      \/\/  1) (mark & 3) == 0, and\n+      \/\/  2) rsp <= mark < mark + os::pagesize()\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 2 bits clear.\n+      \/\/ NOTE: the oopMark is in swap_reg %rax, as the result of cmpxchg\n@@ -1854,2 +1857,2 @@\n-    __ subptr(swap_reg, rsp);\n-    __ andptr(swap_reg, 3 - os::vm_page_size());\n+      __ subptr(swap_reg, rsp);\n+      __ andptr(swap_reg, 3 - os::vm_page_size());\n@@ -1857,3 +1860,9 @@\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-    __ jcc(Assembler::notEqual, slow_path_lock);\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+      __ jcc(Assembler::notEqual, slow_path_lock);\n+    } else {\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+     \/\/ Load object header\n+     __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+     __ fast_lock_impl(obj_reg, swap_reg, thread, lock_reg, slow_path_lock);\n+    }\n@@ -1999,1 +2008,2 @@\n-    \/\/ Simple recursive lock?\n+    if (LockingMode == LM_LEGACY) {\n+      \/\/ Simple recursive lock?\n@@ -2001,2 +2011,3 @@\n-    __ cmpptr(Address(rbp, lock_slot_rbp_offset), (int32_t)NULL_WORD);\n-    __ jcc(Assembler::equal, done);\n+      __ cmpptr(Address(rbp, lock_slot_rbp_offset), (int32_t)NULL_WORD);\n+      __ jcc(Assembler::equal, done);\n+    }\n@@ -2009,12 +2020,21 @@\n-    \/\/  get old displaced header\n-    __ movptr(rbx, Address(rbp, lock_slot_rbp_offset));\n-\n-    \/\/ get address of the stack lock\n-    __ lea(rax, Address(rbp, lock_slot_rbp_offset));\n-\n-    \/\/ Atomic swap old header if oop still contains the stack lock\n-    \/\/ src -> dest iff dest == rax, else rax, <- dest\n-    \/\/ *obj_reg = rbx, iff *obj_reg == rax, else rax, = *(obj_reg)\n-    __ lock();\n-    __ cmpxchgptr(rbx, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    __ jcc(Assembler::notEqual, slow_path_unlock);\n+    if (LockingMode == LM_MONITOR) {\n+      __ jmp(slow_path_unlock);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/  get old displaced header\n+      __ movptr(rbx, Address(rbp, lock_slot_rbp_offset));\n+\n+      \/\/ get address of the stack lock\n+      __ lea(rax, Address(rbp, lock_slot_rbp_offset));\n+\n+      \/\/ Atomic swap old header if oop still contains the stack lock\n+      \/\/ src -> dest iff dest == rax, else rax, <- dest\n+      \/\/ *obj_reg = rbx, iff *obj_reg == rax, else rax, = *(obj_reg)\n+      __ lock();\n+      __ cmpxchgptr(rbx, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ jcc(Assembler::notEqual, slow_path_unlock);\n+    } else {\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+      __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":63,"deletions":43,"binary":false,"changes":106,"status":"modified"},{"patch":"@@ -2074,3 +2074,6 @@\n-    if (UseBiasedLocking) {\n-      __ biased_locking_enter(lock_reg, obj_reg, swap_reg, rscratch1, rscratch2, false, lock_done, &slow_path_lock);\n-    }\n+    if (LockingMode == LM_MONITOR) {\n+      __ jmp(slow_path_lock);\n+    } else if (LockingMode == LM_LEGACY) {\n+      if (UseBiasedLocking) {\n+        __ biased_locking_enter(lock_reg, obj_reg, swap_reg, rscratch1, rscratch2, false, lock_done, &slow_path_lock);\n+      }\n@@ -2078,2 +2081,2 @@\n-    \/\/ Load immediate 1 into swap_reg %rax\n-    __ movl(swap_reg, 1);\n+      \/\/ Load immediate 1 into swap_reg %rax\n+      __ movl(swap_reg, 1);\n@@ -2081,2 +2084,2 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg %rax\n-    __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      \/\/ Load (object->mark() | 1) into swap_reg %rax\n+      __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -2084,2 +2087,2 @@\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n@@ -2087,4 +2090,4 @@\n-    \/\/ src -> dest iff dest == rax else rax <- dest\n-    __ lock();\n-    __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    __ jcc(Assembler::equal, lock_done);\n+      \/\/ src -> dest iff dest == rax else rax <- dest\n+      __ lock();\n+      __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ jcc(Assembler::equal, lock_done);\n@@ -2092,1 +2095,1 @@\n-    \/\/ Hmm should this move to the slow path code area???\n+      \/\/ Hmm should this move to the slow path code area???\n@@ -2094,8 +2097,8 @@\n-    \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-    \/\/  1) (mark & 3) == 0, and\n-    \/\/  2) rsp <= mark < mark + os::pagesize()\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 2 bits clear.\n-    \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n+      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+      \/\/  1) (mark & 3) == 0, and\n+      \/\/  2) rsp <= mark < mark + os::pagesize()\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 2 bits clear.\n+      \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n@@ -2103,2 +2106,2 @@\n-    __ subptr(swap_reg, rsp);\n-    __ andptr(swap_reg, 3 - os::vm_page_size());\n+      __ subptr(swap_reg, rsp);\n+      __ andptr(swap_reg, 3 - os::vm_page_size());\n@@ -2106,3 +2109,9 @@\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-    __ jcc(Assembler::notEqual, slow_path_lock);\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+      __ jcc(Assembler::notEqual, slow_path_lock);\n+    } else {\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      \/\/ Load object header\n+      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ fast_lock_impl(obj_reg, swap_reg, r15_thread, rscratch1, slow_path_lock);\n+    }\n@@ -2233,1 +2242,2 @@\n-    \/\/ Simple recursive lock?\n+    if (LockingMode == LM_LEGACY) {\n+      \/\/ Simple recursive lock?\n@@ -2235,2 +2245,3 @@\n-    __ cmpptr(Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size), (int32_t)NULL_WORD);\n-    __ jcc(Assembler::equal, done);\n+      __ cmpptr(Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size), (int32_t)NULL_WORD);\n+      __ jcc(Assembler::equal, done);\n+    }\n@@ -2243,10 +2254,18 @@\n-\n-    \/\/ get address of the stack lock\n-    __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-    \/\/  get old displaced header\n-    __ movptr(old_hdr, Address(rax, 0));\n-\n-    \/\/ Atomic swap old header if oop still contains the stack lock\n-    __ lock();\n-    __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    __ jcc(Assembler::notEqual, slow_path_unlock);\n+    if (LockingMode == LM_MONITOR) {\n+      __ jmp(slow_path_unlock);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ get address of the stack lock\n+      __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+      \/\/  get old displaced header\n+      __ movptr(old_hdr, Address(rax, 0));\n+\n+      \/\/ Atomic swap old header if oop still contains the stack lock\n+      __ lock();\n+      __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ jcc(Assembler::notEqual, slow_path_unlock);\n+    } else {\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+      __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":59,"deletions":40,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -3660,1 +3660,0 @@\n-  __ null_check(recv, oopDesc::klass_offset_in_bytes());\n@@ -3662,1 +3661,1 @@\n-  __ load_klass(rax, recv, tmp_load_klass);\n+  __ load_klass(rax, recv, tmp_load_klass, true);\n@@ -3753,1 +3752,0 @@\n-  __ null_check(rcx, oopDesc::klass_offset_in_bytes());\n@@ -3755,1 +3753,1 @@\n-  __ load_klass(rlocals, rcx, tmp_load_klass);\n+  __ load_klass(rlocals, rcx, tmp_load_klass, true);\n@@ -3777,2 +3775,1 @@\n-  __ null_check(rcx, oopDesc::klass_offset_in_bytes());\n-  __ load_klass(rdx, rcx, tmp_load_klass);\n+  __ load_klass(rdx, rcx, tmp_load_klass, true);\n@@ -3996,1 +3993,2 @@\n-    __ decrement(rdx, sizeof(oopDesc));\n+    int header_size = align_up(oopDesc::base_offset_in_bytes(), BytesPerLong);\n+    __ decrement(rdx, header_size);\n@@ -4018,2 +4016,2 @@\n-    __ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 1*oopSize), rcx);\n-    NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 2*oopSize), rcx));\n+    __ movptr(Address(rax, rdx, Address::times_8, header_size - 1*oopSize), rcx);\n+    NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, header_size - 2*oopSize), rcx));\n@@ -4026,1 +4024,1 @@\n-    if (UseBiasedLocking) {\n+    if (UseBiasedLocking || UseCompactObjectHeaders) {\n@@ -4035,0 +4033,1 @@\n+    if (!UseCompactObjectHeaders) {\n@@ -4036,2 +4035,2 @@\n-    __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-    __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n+      __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n+      __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n@@ -4039,2 +4038,2 @@\n-    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n-    __ store_klass(rax, rcx, tmp_store_klass);  \/\/ klass\n+      __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":13,"deletions":14,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -724,1 +724,3 @@\n-      code_stub = &C->output()->safepoint_poll_table()->add_safepoint(__ offset());\n+      C2SafepointPollStub* stub = new (C->comp_arena()) C2SafepointPollStub(__ offset());\n+      C->output()->add_stub(stub);\n+      code_stub = &stub->entry();\n@@ -13698,1 +13700,1 @@\n-instruct cmpFastLockRTM(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eDXRegI scr, rRegI cx1, rRegI cx2) %{\n+instruct cmpFastLockRTM(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eDXRegI scr, rRegI cx1, rRegI cx2, eRegP thread) %{\n@@ -13701,1 +13703,1 @@\n-  effect(TEMP tmp, TEMP scr, TEMP cx1, TEMP cx2, USE_KILL box);\n+  effect(TEMP tmp, TEMP scr, TEMP cx1, TEMP cx2, USE_KILL box, TEMP thread);\n@@ -13705,0 +13707,1 @@\n+    __ get_thread($thread$$Register);\n@@ -13706,1 +13709,1 @@\n-                 $scr$$Register, $cx1$$Register, $cx2$$Register,\n+                 $scr$$Register, $cx1$$Register, $cx2$$Register, $thread$$Register,\n@@ -13714,1 +13717,1 @@\n-instruct cmpFastLock(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eRegP scr) %{\n+instruct cmpFastLock(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eRegP scr, eRegP thread) %{\n@@ -13717,1 +13720,1 @@\n-  effect(TEMP tmp, TEMP scr, USE_KILL box);\n+  effect(TEMP tmp, TEMP scr, USE_KILL box, TEMP thread);\n@@ -13721,0 +13724,1 @@\n+    __ get_thread($thread$$Register);\n@@ -13722,1 +13726,1 @@\n-                 $scr$$Register, noreg, noreg, _counters, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, noreg, noreg, $thread$$Register, _counters, NULL, NULL, NULL, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":11,"deletions":7,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -1001,1 +1001,3 @@\n-      code_stub = &C->output()->safepoint_poll_table()->add_safepoint(__ offset());\n+      C2SafepointPollStub* stub = new (C->comp_arena()) C2SafepointPollStub(__ offset());\n+      C->output()->add_stub(stub);\n+      code_stub = &stub->entry();\n@@ -5193,0 +5195,1 @@\n+  predicate(!UseCompactObjectHeaders);\n@@ -5203,0 +5206,23 @@\n+instruct loadNKlassLilliput(rRegN dst, indOffset8 mem, rFlagsReg cr)\n+%{\n+  predicate(UseCompactObjectHeaders);\n+  match(Set dst (LoadNKlass mem));\n+  effect(KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $dst, $mem\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset 4, but got: %d\", $mem$$disp);\n+    assert($mem$$index == 4, \"expect no index register: %d\", $mem$$index);\n+    Register dst = $dst$$Register;\n+    Register obj = $mem$$base$$Register;\n+    C2LoadNKlassStub* stub = new (Compile::current()->comp_arena()) C2LoadNKlassStub(dst);\n+    Compile::current()->output()->add_stub(stub);\n+    __ movq(dst, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    __ testb(dst, markWord::monitor_value);\n+    __ jcc(Assembler::notZero, stub->entry());\n+    __ bind(stub->continuation());\n+    __ shrq(dst, markWord::klass_shift);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n@@ -12228,0 +12254,1 @@\n+  predicate(!UseCompactObjectHeaders);\n@@ -12926,1 +12953,1 @@\n-                 $scr$$Register, $cx1$$Register, $cx2$$Register,\n+                 $scr$$Register, $cx1$$Register, $cx2$$Register, r15_thread,\n@@ -12942,1 +12969,1 @@\n-                 $scr$$Register, $cx1$$Register, noreg, _counters, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, $cx1$$Register, noreg, r15_thread, _counters, NULL, NULL, NULL, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":30,"deletions":3,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -118,0 +118,5 @@\n+  if ((LockingMode != LM_LEGACY) && (LockingMode != LM_MONITOR)) {\n+    warning(\"Unsupported locking mode for this CPU.\");\n+    FLAG_SET_DEFAULT(LockingMode, LM_LEGACY);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/zero\/vm_version_zero.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -336,1 +336,1 @@\n-    bool call_vm = UseHeavyMonitors;\n+    bool call_vm = (LockingMode == LM_MONITOR);\n","filename":"src\/hotspot\/cpu\/zero\/zeroInterpreter_zero.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -564,0 +564,18 @@\n+class LoadKlassStub: public CodeStub {\n+private:\n+  LIR_Opr          _result;\n+\n+public:\n+  LoadKlassStub(LIR_Opr result) :\n+    CodeStub(), _result(result) {};\n+\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_temp(_result);\n+    visitor->do_output(_result);\n+  }\n+#ifndef PRODUCT\n+virtual void print_name(outputStream* out) const { out->print(\"LoadKlassStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -931,0 +931,1 @@\n+      if (opLoadKlass->_stub) do_stub(opLoadKlass->_stub);\n@@ -1113,0 +1114,3 @@\n+  if (stub()) {\n+    masm->append_code_stub(stub());\n+  }\n@@ -2097,0 +2101,3 @@\n+  if (stub()) {\n+    out->print(\"[lbl:\" INTPTR_FORMAT \"]\", p2i(stub()->entry()));\n+  }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1921,0 +1921,1 @@\n+  CodeStub* _stub;\n@@ -1922,1 +1923,1 @@\n-  LIR_OpLoadKlass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info)\n+  LIR_OpLoadKlass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info, CodeStub* stub)\n@@ -1925,1 +1926,1 @@\n-    {}\n+    , _stub(stub) {}\n@@ -1928,0 +1929,1 @@\n+  CodeStub* stub()     const { return _stub; }\n@@ -2400,1 +2402,1 @@\n-  void load_klass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info) { append(new LIR_OpLoadKlass(obj, result, info)); }\n+  void load_klass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info, CodeStub* stub) { append(new LIR_OpLoadKlass(obj, result, info, stub)); }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -639,1 +639,1 @@\n-  CodeStub* slow_path = new MonitorExitStub(lock, UseFastLocking, monitor_no);\n+  CodeStub* slow_path = new MonitorExitStub(lock, LockingMode != LM_MONITOR, monitor_no);\n@@ -1260,1 +1260,2 @@\n-  __ load_klass(obj, klass, null_check_info);\n+  CodeStub* slow_path = UseCompactObjectHeaders ? new LoadKlassStub(klass) : NULL;\n+  __ load_klass(obj, klass, null_check_info, slow_path);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -702,1 +702,1 @@\n-  if (!UseFastLocking) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -705,2 +705,2 @@\n-  assert(obj == lock->obj(), \"must match\");\n-  SharedRuntime::monitor_enter_helper(obj, lock->lock(), current);\n+  assert(LockingMode == LM_LIGHTWEIGHT || obj == lock->obj(), \"must match\");\n+  SharedRuntime::monitor_enter_helper(obj, LockingMode == LM_LIGHTWEIGHT ? NULL : lock->lock(), current);\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/klass.inline.hpp\"\n@@ -734,0 +735,7 @@\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      Klass* requested_k = to_requested(k);\n+      narrowKlass nk = CompressedKlassPointers::encode_not_null(requested_k, _requested_static_archive_bottom);\n+      k->set_prototype_header(markWord::prototype().set_narrow_klass(nk));\n+    }\n+#endif \/\/_LP64\n@@ -775,0 +783,1 @@\n+#ifdef _LP64\n@@ -779,1 +788,6 @@\n-  o->set_narrow_klass(nk);\n+  if (UseCompactObjectHeaders) {\n+    o->set_mark(o->mark().set_narrow_klass(nk));\n+  } else {\n+    o->set_narrow_klass(nk);\n+  }\n+#endif \/\/ _LP64\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -220,0 +220,1 @@\n+  _compact_headers = UseCompactObjectHeaders;\n@@ -282,0 +283,1 @@\n+  st->print_cr(\"- compact_headers:                %d\", _compact_headers);\n@@ -2288,0 +2290,8 @@\n+  if (compact_headers() != UseCompactObjectHeaders) {\n+    log_info(cds)(\"The shared archive file's UseCompactObjectHeaders setting (%s)\"\n+                  \" does not equal the current UseCompactObjectHeaders setting (%s).\",\n+                  _compact_headers          ? \"enabled\" : \"disabled\",\n+                  UseCompactObjectHeaders   ? \"enabled\" : \"disabled\");\n+    return false;\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -196,0 +196,1 @@\n+  bool   _compact_headers;          \/\/ value of UseCompactObjectHeaders\n@@ -261,0 +262,1 @@\n+  bool compact_headers()                   const { return _compact_headers; }\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"runtime\/safepoint.hpp\"\n@@ -278,1 +279,10 @@\n-    archived_oop->set_mark(markWord::prototype().copy_set_hash(hash_original));\n+    if (UseCompactObjectHeaders) {\n+      markWord mark = obj->mark();\n+      if (mark.has_displaced_mark_helper()) {\n+        mark = mark.displaced_mark_helper();\n+      }\n+      narrowKlass nklass = mark.narrow_klass();\n+      archived_oop->set_mark(markWord::prototype().copy_set_hash(hash_original) LP64_ONLY(.set_narrow_klass(nklass)));\n+    } else {\n+      archived_oop->set_mark(markWord::prototype().copy_set_hash(hash_original));\n+    }\n@@ -421,0 +431,2 @@\n+    } else if (UseCompactObjectHeaders) {\n+      oopDesc::release_set_mark(mem, k->prototype_header());\n@@ -424,1 +436,3 @@\n-    oopDesc::release_set_klass(mem, k);\n+    if (!UseCompactObjectHeaders) {\n+      oopDesc::release_set_klass(mem, k);\n+    }\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":16,"deletions":2,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -252,0 +252,20 @@\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciKlass::prototype_header_offset\n+juint ciKlass::prototype_header_offset() {\n+  assert(is_loaded(), \"must be loaded\");\n+\n+  VM_ENTRY_MARK;\n+  Klass* this_klass = get_Klass();\n+  return in_bytes(this_klass->prototype_header_offset());\n+}\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciKlass::prototype_header\n+uintptr_t ciKlass::prototype_header() {\n+  assert(is_loaded(), \"must be loaded\");\n+\n+  VM_ENTRY_MARK;\n+  Klass* this_klass = get_Klass();\n+  return (uintptr_t)this_klass->prototype_header().to_pointer();\n+}\n","filename":"src\/hotspot\/share\/ci\/ciKlass.cpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -132,0 +132,3 @@\n+\n+  juint prototype_header_offset();\n+  uintptr_t prototype_header();\n","filename":"src\/hotspot\/share\/ci\/ciKlass.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2082,1 +2082,5 @@\n-      ik->set_prototype_header(markWord::prototype());\n+      if (UseCompactObjectHeaders) {\n+        ik->set_prototype_header(markWord::prototype() LP64_ONLY(.set_klass(ik)));\n+      } else {\n+        ik->set_prototype_header(markWord::prototype());\n+      }\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -1777,0 +1778,2 @@\n+  SlidingForwarding::initialize(heap_rs.region(), HeapRegion::GrainWords);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -204,0 +205,2 @@\n+  SlidingForwarding::begin();\n+\n@@ -209,0 +212,2 @@\n+\n+  SlidingForwarding::end();\n@@ -315,0 +320,1 @@\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+template <bool ALT_FWD>\n@@ -43,1 +44,1 @@\n-  G1AdjustClosure* _adjust_closure;\n+  G1AdjustClosure<ALT_FWD>* _adjust_closure;\n@@ -45,1 +46,1 @@\n-  G1AdjustLiveClosure(G1AdjustClosure* cl) :\n+  G1AdjustLiveClosure(G1AdjustClosure<ALT_FWD>* cl) :\n@@ -64,1 +65,11 @@\n-    G1AdjustClosure cl(_collector);\n+    if (UseAltGCForwarding) {\n+      return do_heap_region_impl<true>(r);\n+    } else {\n+      return do_heap_region_impl<false>(r);\n+    }\n+  }\n+\n+ private:\n+  template <bool ALT_FWD>\n+  bool do_heap_region_impl(HeapRegion* r) {\n+    G1AdjustClosure<ALT_FWD> cl(_collector);\n@@ -74,1 +85,1 @@\n-      G1AdjustLiveClosure adjust(&cl);\n+      G1AdjustLiveClosure<ALT_FWD> adjust(&cl);\n@@ -86,2 +97,1 @@\n-    _hrclaimer(collector->workers()),\n-    _adjust(collector) {\n+    _hrclaimer(collector->workers()) {\n@@ -92,1 +102,2 @@\n-void G1FullGCAdjustTask::work(uint worker_id) {\n+template <bool ALT_FWD>\n+void G1FullGCAdjustTask::work_impl(uint worker_id) {\n@@ -100,0 +111,1 @@\n+  G1AdjustClosure<ALT_FWD> adjust(collector());\n@@ -102,1 +114,1 @@\n-    G1CollectedHeap::heap()->ref_processor_stw()->weak_oops_do(&_adjust);\n+    G1CollectedHeap::heap()->ref_processor_stw()->weak_oops_do(&adjust);\n@@ -106,1 +118,1 @@\n-  _weak_proc_task.work(worker_id, &always_alive, &_adjust);\n+  _weak_proc_task.work(worker_id, &always_alive, &adjust);\n@@ -108,3 +120,3 @@\n-  CLDToOopClosure adjust_cld(&_adjust, ClassLoaderData::_claim_strong);\n-  CodeBlobToOopClosure adjust_code(&_adjust, CodeBlobToOopClosure::FixRelocations);\n-  _root_processor.process_all_roots(&_adjust, &adjust_cld, &adjust_code);\n+  CLDToOopClosure adjust_cld(&adjust, ClassLoaderData::_claim_strong);\n+  CodeBlobToOopClosure adjust_code(&adjust, CodeBlobToOopClosure::FixRelocations);\n+  _root_processor.process_all_roots(&adjust, &adjust_cld, &adjust_code);\n@@ -117,0 +129,8 @@\n+\n+void G1FullGCAdjustTask::work(uint worker_id) {\n+  if (UseAltGCForwarding) {\n+    work_impl<true>(worker_id);\n+  } else {\n+    work_impl<false>(worker_id);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCAdjustTask.cpp","additions":32,"deletions":12,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -42,1 +42,0 @@\n-  G1AdjustClosure          _adjust;\n@@ -44,0 +43,2 @@\n+  template <bool ALT_FWD>\n+  void work_impl(uint worker_id);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCAdjustTask.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -61,1 +62,2 @@\n-size_t G1FullGCCompactTask::G1CompactRegionClosure::apply(oop obj) {\n+template <bool ALT_FWD>\n+size_t G1FullGCCompactTask::G1CompactRegionClosure<ALT_FWD>::apply(oop obj) {\n@@ -63,2 +65,1 @@\n-  HeapWord* destination = cast_from_oop<HeapWord*>(obj->forwardee());\n-  if (destination == NULL) {\n+  if (!SlidingForwarding::is_forwarded(obj)) {\n@@ -69,0 +70,2 @@\n+  HeapWord* destination = cast_from_oop<HeapWord*>(SlidingForwarding::forwardee<ALT_FWD>(obj));\n+\n@@ -82,2 +85,7 @@\n-  G1CompactRegionClosure compact(collector()->mark_bitmap());\n-  hr->apply_to_marked_objects(collector()->mark_bitmap(), &compact);\n+  if (UseAltGCForwarding) {\n+    G1CompactRegionClosure<true> compact(collector()->mark_bitmap());\n+    hr->apply_to_marked_objects(collector()->mark_bitmap(), &compact);\n+  } else {\n+    G1CompactRegionClosure<false> compact(collector()->mark_bitmap());\n+    hr->apply_to_marked_objects(collector()->mark_bitmap(), &compact);\n+  }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+  template <bool ALT_FWD>\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -96,0 +97,1 @@\n+template <bool ALT_FWD>\n@@ -106,1 +108,1 @@\n-    object->forward_to(cast_to_oop(_compaction_top));\n+    SlidingForwarding::forward_to<ALT_FWD>(object, cast_to_oop(_compaction_top));\n@@ -108,0 +110,2 @@\n+    assert(!SlidingForwarding::is_forwarded(object), \"should not be forwarded\");\n+    \/*\n@@ -126,0 +130,1 @@\n+    *\/\n@@ -135,0 +140,3 @@\n+template void G1FullGCCompactionPoint::forward<true>(oop object, size_t size);\n+template void G1FullGCCompactionPoint::forward<false>(oop object, size_t size);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.cpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+  template <bool ALT_FWD>\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -80,0 +80,1 @@\n+template <bool ALT_FWD>\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -66,1 +67,2 @@\n-template <class T> inline void G1AdjustClosure::adjust_pointer(T* p) {\n+template <bool ALT_FWD>\n+template <class T> inline void G1AdjustClosure<ALT_FWD>::adjust_pointer(T* p) {\n@@ -80,2 +82,1 @@\n-  oop forwardee = obj->forwardee();\n-  if (forwardee == NULL) {\n+  if (!SlidingForwarding::is_forwarded(obj)) {\n@@ -83,0 +84,1 @@\n+    \/*\n@@ -88,0 +90,1 @@\n+    *\/\n@@ -92,0 +95,1 @@\n+  oop forwardee = SlidingForwarding::forwardee<ALT_FWD>(obj);\n@@ -96,2 +100,4 @@\n-inline void G1AdjustClosure::do_oop(oop* p)       { do_oop_work(p); }\n-inline void G1AdjustClosure::do_oop(narrowOop* p) { do_oop_work(p); }\n+template <bool ALT_FWD>\n+inline void G1AdjustClosure<ALT_FWD>::do_oop(oop* p)       { do_oop_work(p); }\n+template <bool ALT_FWD>\n+inline void G1AdjustClosure<ALT_FWD>::do_oop(narrowOop* p) { do_oop_work(p); }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.inline.hpp","additions":11,"deletions":5,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -159,1 +160,2 @@\n-G1FullGCPrepareTask::G1PrepareCompactLiveClosure::G1PrepareCompactLiveClosure(G1FullGCCompactionPoint* cp) :\n+template <bool ALT_FWD>\n+G1FullGCPrepareTask::G1PrepareCompactLiveClosure<ALT_FWD>::G1PrepareCompactLiveClosure(G1FullGCCompactionPoint* cp) :\n@@ -162,1 +164,2 @@\n-size_t G1FullGCPrepareTask::G1PrepareCompactLiveClosure::apply(oop object) {\n+template <bool ALT_FWD>\n+size_t G1FullGCPrepareTask::G1PrepareCompactLiveClosure<ALT_FWD>::apply(oop object) {\n@@ -164,1 +167,1 @@\n-  _cp->forward(object, size);\n+  _cp->forward<ALT_FWD>(object, size);\n@@ -168,1 +171,2 @@\n-size_t G1FullGCPrepareTask::G1RePrepareClosure::apply(oop obj) {\n+template <bool ALT_FWD>\n+size_t G1FullGCPrepareTask::G1RePrepareClosure<ALT_FWD>::apply(oop obj) {\n@@ -171,3 +175,6 @@\n-  oop forwarded_to = obj->forwardee();\n-  if (forwarded_to != NULL && !_current->is_in(forwarded_to)) {\n-    return obj->size();\n+  if (SlidingForwarding::is_forwarded(obj)) {\n+    oop forwarded_to = SlidingForwarding::forwardee<ALT_FWD>(obj);\n+    assert(forwarded_to != NULL, \"must have forwardee\");\n+    if (!_current->is_in(forwarded_to)) {\n+      return obj->size();\n+    }\n@@ -175,1 +182,0 @@\n-\n@@ -178,1 +184,1 @@\n-  _cp->forward(obj, size);\n+  _cp->forward<ALT_FWD>(obj, size);\n@@ -185,3 +191,9 @@\n-  G1PrepareCompactLiveClosure prepare_compact(cp);\n-  hr->set_compaction_top(hr->bottom());\n-  hr->apply_to_marked_objects(_bitmap, &prepare_compact);\n+  if (UseAltGCForwarding) {\n+    G1PrepareCompactLiveClosure<true> prepare_compact(cp);\n+    hr->set_compaction_top(hr->bottom());\n+    hr->apply_to_marked_objects(_bitmap, &prepare_compact);\n+  } else {\n+    G1PrepareCompactLiveClosure<false> prepare_compact(cp);\n+    hr->set_compaction_top(hr->bottom());\n+    hr->apply_to_marked_objects(_bitmap, &prepare_compact);\n+  }\n@@ -200,1 +212,2 @@\n-void G1FullGCPrepareTask::prepare_serial_compaction() {\n+template <bool ALT_FWD>\n+void G1FullGCPrepareTask::prepare_serial_compaction_impl() {\n@@ -224,1 +237,1 @@\n-      G1RePrepareClosure re_prepare(cp, current);\n+      G1RePrepareClosure<ALT_FWD> re_prepare(cp, current);\n@@ -232,0 +245,8 @@\n+void G1FullGCPrepareTask::prepare_serial_compaction() {\n+  if (UseAltGCForwarding) {\n+    prepare_serial_compaction_impl<true>();\n+  } else {\n+    prepare_serial_compaction_impl<false>();\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.cpp","additions":35,"deletions":14,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -45,0 +45,4 @@\n+private:\n+  template <bool ALT_FWD>\n+  void prepare_serial_compaction_impl();\n+\n@@ -77,0 +81,1 @@\n+  template <bool ALT_FWD>\n@@ -85,0 +90,1 @@\n+  template <bool ALT_FWD>\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -237,1 +237,1 @@\n-      forwardee = cast_to_oop(m.decode_pointer());\n+      forwardee = obj->forwardee(m);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1OopClosures.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -200,1 +200,1 @@\n-    obj = cast_to_oop(m.decode_pointer());\n+    obj = obj->forwardee(m);\n@@ -214,1 +214,1 @@\n-  assert(from_obj->is_objArray(), \"must be obj array\");\n+  assert(UseCompactObjectHeaders || from_obj->is_objArray(), \"must be obj array\");\n@@ -244,1 +244,1 @@\n-  assert(from_obj->is_objArray(), \"precondition\");\n+  assert(UseCompactObjectHeaders || from_obj->is_objArray(), \"precondition\");\n@@ -363,1 +363,1 @@\n-                                                  oop const old, size_t word_sz, uint age,\n+                                                  Klass* klass, size_t word_sz, uint age,\n@@ -367,1 +367,1 @@\n-    _g1h->_gc_tracer_stw->report_promotion_in_new_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->_gc_tracer_stw->report_promotion_in_new_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -371,1 +371,1 @@\n-    _g1h->_gc_tracer_stw->report_promotion_outside_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->_gc_tracer_stw->report_promotion_outside_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -378,1 +378,1 @@\n-                                                   oop old,\n+                                                   Klass* klass,\n@@ -401,1 +401,1 @@\n-      report_promotion_event(*dest_attr, old, word_sz, age, obj_ptr, node_index);\n+      report_promotion_event(*dest_attr, klass, word_sz, age, obj_ptr, node_index);\n@@ -426,1 +426,7 @@\n-  Klass* klass = old->klass();\n+  \/\/ NOTE: With compact headers, it is not safe to load the Klass* from o, because\n+  \/\/ that would access the mark-word, and the mark-word might change at any time by\n+  \/\/ concurrent promotion. The promoted mark-word would point to the forwardee, which\n+  \/\/ may not yet have completed copying. Therefore we must load the Klass* from\n+  \/\/ the mark-word that we have already loaded. This is safe, because we have checked\n+  \/\/ that this is not yet forwarded in the caller.\n+  Klass* klass = old->forward_safe_klass(old_mark);\n@@ -439,1 +445,1 @@\n-    obj_ptr = allocate_copy_slow(&dest_attr, old, word_sz, age, node_index);\n+    obj_ptr = allocate_copy_slow(&dest_attr, klass, word_sz, age, node_index);\n@@ -596,1 +602,1 @@\n-  oop forward_ptr = old->forward_to_atomic(old, m, memory_order_relaxed);\n+  oop forward_ptr = old->forward_to_self_atomic(m, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":17,"deletions":11,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -175,1 +175,1 @@\n-                               oop old,\n+                               Klass* klass,\n@@ -208,1 +208,1 @@\n-                              oop const old, size_t word_sz, uint age,\n+                              Klass* klass, size_t word_sz, uint age,\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -863,1 +863,2 @@\n-void HeapRegion::object_iterate(ObjectClosure* blk) {\n+template<bool RESOLVE>\n+void HeapRegion::object_iterate_impl(ObjectClosure* blk) {\n@@ -869,1 +870,9 @@\n-    p += block_size(p);\n+    p += block_size<RESOLVE>(p);\n+  }\n+}\n+\n+void HeapRegion::object_iterate(ObjectClosure* blk) {\n+  if (!UseCompactObjectHeaders || G1CollectedHeap::heap()->collector_state()->in_full_gc()) {\n+    object_iterate_impl<false>(blk);\n+  } else {\n+    object_iterate_impl<true>(blk);\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.cpp","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -148,0 +148,3 @@\n+  template<bool RESOLVE>\n+  void object_iterate_impl(ObjectClosure* blk);\n+\n@@ -186,0 +189,1 @@\n+  template<bool RESOLVE = false>\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -180,0 +180,1 @@\n+template <bool RESOLVE>\n@@ -186,1 +187,14 @@\n-    return cast_to_oop(addr)->size();\n+    oop obj = cast_to_oop(addr);\n+#ifdef _LP64\n+#ifdef ASSERT\n+    if (RESOLVE) {\n+      assert(UseCompactObjectHeaders && !G1CollectedHeap::heap()->collector_state()->in_full_gc(), \"Illegal\/excessive resolve during full-GC\");\n+    } else {\n+      assert(!UseCompactObjectHeaders || G1CollectedHeap::heap()->collector_state()->in_full_gc() || !obj->is_forwarded(), \"Missing resolve when forwarded during normal GC\");\n+    }\n+#endif\n+    if (RESOLVE && obj->is_forwarded()) {\n+      obj = obj->forwardee();\n+    }\n+#endif\n+    return obj->size();\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.inline.hpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -112,1 +112,1 @@\n-              touched_words = MIN2((size_t)align_object_size(typeArrayOopDesc::header_size(T_INT)),\n+              touched_words = MIN2((size_t)align_object_size(align_up(typeArrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize),\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableNUMASpace.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -411,1 +411,1 @@\n-    HeapWord* test_addr = cast_from_oop<HeapWord*>(obj) + 1;\n+    HeapWord* test_addr = cast_from_oop<HeapWord*>(obj);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psOldGen.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-  filler_header_size = align_object_size(typeArrayOopDesc::header_size(T_INT));\n+  filler_header_size = align_object_size((arrayOopDesc::base_offset_in_bytes(T_INT) + BytesPerWord) \/ BytesPerWord);\n@@ -86,6 +86,10 @@\n-  filler_oop->set_mark(markWord::prototype());\n-  filler_oop->set_klass(Universe::intArrayKlassObj());\n-  const size_t array_length =\n-    pointer_delta(tlab_end, top()) - typeArrayOopDesc::header_size(T_INT);\n-  assert( (array_length * (HeapWordSize\/sizeof(jint))) < (size_t)max_jint, \"array too big in PSPromotionLAB\");\n-  filler_oop->set_length((int)(array_length * (HeapWordSize\/sizeof(jint))));\n+  if (UseCompactObjectHeaders) {\n+    filler_oop->set_mark(Universe::intArrayKlassObj()->prototype_header());\n+  } else {\n+    filler_oop->set_mark(markWord::prototype());\n+    filler_oop->set_klass(Universe::intArrayKlassObj());\n+  }\n+  int header_size = arrayOopDesc::base_offset_in_bytes(T_INT);\n+  const size_t array_length_bytes = pointer_delta(tlab_end, top(), 1) - header_size;\n+  assert((array_length_bytes \/ sizeof(jint)) < (size_t)max_jint, \"array too big in PSPromotionLAB\");\n+  filler_oop->set_length((int)(array_length_bytes \/ sizeof(jint)));\n@@ -95,2 +99,3 @@\n-  HeapWord* elt_words = cast_from_oop<HeapWord*>(filler_oop) + typeArrayOopDesc::header_size(T_INT);\n-  Copy::fill_to_words(elt_words, array_length, 0xDEAABABE);\n+  const size_t array_length_words = pointer_delta(tlab_end, top()) - heap_word_size(header_size);\n+  HeapWord* elt_words = cast_from_oop<HeapWord*>(filler_oop) + heap_word_size(header_size);\n+  Copy::fill_to_words(elt_words, array_length_words, 0xDEAABABE);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionLAB.cpp","additions":14,"deletions":9,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -317,1 +317,1 @@\n-  assert(old->is_objArray(), \"invariant\");\n+  assert(UseCompactObjectHeaders || old->is_objArray(), \"invariant\");\n@@ -355,1 +355,1 @@\n-  if (obj->cas_forward_to(obj, obj_mark)) {\n+  if (obj->forward_to_self_atomic(obj_mark) == NULL) {\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -107,1 +107,2 @@\n-  inline void promotion_trace_event(oop new_obj, oop old_obj, size_t obj_size,\n+  inline void promotion_trace_event(oop new_obj, oop old_obj,\n+                                    Klass* klass, size_t obj_size,\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -64,1 +64,1 @@\n-                                                      size_t obj_size,\n+                                                      Klass* klass, size_t obj_size,\n@@ -76,1 +76,1 @@\n-        gc_tracer->report_promotion_in_new_plab_event(old_obj->klass(), obj_bytes,\n+        gc_tracer->report_promotion_in_new_plab_event(klass, obj_bytes,\n@@ -83,1 +83,1 @@\n-        gc_tracer->report_promotion_outside_plab_event(old_obj->klass(), obj_bytes,\n+        gc_tracer->report_promotion_outside_plab_event(klass, obj_bytes,\n@@ -147,1 +147,1 @@\n-    return cast_to_oop(m.decode_pointer());\n+    return o->forwardee(m);\n@@ -163,1 +163,8 @@\n-  size_t new_obj_size = o->size();\n+  \/\/ NOTE: With compact headers, it is not safe to load the Klass* from o, because\n+  \/\/ that would access the mark-word, and the mark-word might change at any time by\n+  \/\/ concurrent promotion. The promoted mark-word would point to the forwardee, which\n+  \/\/ may not yet have completed copying. Therefore we must load the Klass* from\n+  \/\/ the mark-word that we have already loaded. This is safe, because we have checked\n+  \/\/ that this is not yet forwarded in the caller.\n+  Klass* klass = o->forward_safe_klass(test_mark);\n+  size_t new_obj_size = o->size_given_klass(klass);\n@@ -178,1 +185,1 @@\n-          promotion_trace_event(new_obj, o, new_obj_size, age, false, NULL);\n+          promotion_trace_event(new_obj, o, klass, new_obj_size, age, false, NULL);\n@@ -188,1 +195,1 @@\n-            promotion_trace_event(new_obj, o, new_obj_size, age, false, &_young_lab);\n+            promotion_trace_event(new_obj, o, klass, new_obj_size, age, false, &_young_lab);\n@@ -214,1 +221,1 @@\n-          promotion_trace_event(new_obj, o, new_obj_size, age, true, NULL);\n+          promotion_trace_event(new_obj, o, klass, new_obj_size, age, true, NULL);\n@@ -231,1 +238,1 @@\n-            promotion_trace_event(new_obj, o, new_obj_size, age, true, &_old_lab);\n+            promotion_trace_event(new_obj, o, klass, new_obj_size, age, true, &_old_lab);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.inline.hpp","additions":16,"deletions":9,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -689,1 +689,1 @@\n-  old->forward_to(old);\n+  old->forward_to_self();\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -96,0 +97,2 @@\n+  SlidingForwarding::begin();\n+\n@@ -114,0 +117,2 @@\n+  SlidingForwarding::end();\n+\n@@ -277,8 +282,24 @@\n-  {\n-    StrongRootsScope srs(0);\n-\n-    gch->full_process_roots(true,  \/\/ this is the adjust phase\n-                            GenCollectedHeap::SO_AllCodeCache,\n-                            false, \/\/ all roots\n-                            &adjust_pointer_closure,\n-                            &adjust_cld_closure);\n+  if (UseAltGCForwarding) {\n+    AdjustPointerClosure<true> adjust_pointer_closure;\n+    CLDToOopClosure adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_strong);\n+    {\n+      StrongRootsScope srs(0);\n+      gch->full_process_roots(true,  \/\/ this is the adjust phase\n+                              GenCollectedHeap::SO_AllCodeCache,\n+                              false, \/\/ all roots\n+                              &adjust_pointer_closure,\n+                              &adjust_cld_closure);\n+    }\n+    gch->gen_process_weak_roots(&adjust_pointer_closure);\n+  } else {\n+    AdjustPointerClosure<false> adjust_pointer_closure;\n+    CLDToOopClosure adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_strong);\n+    {\n+      StrongRootsScope srs(0);\n+      gch->full_process_roots(true,  \/\/ this is the adjust phase\n+                              GenCollectedHeap::SO_AllCodeCache,\n+                              false, \/\/ all roots\n+                              &adjust_pointer_closure,\n+                              &adjust_cld_closure);\n+    }\n+    gch->gen_process_weak_roots(&adjust_pointer_closure);\n@@ -286,3 +307,0 @@\n-\n-  gch->gen_process_weak_roots(&adjust_pointer_closure);\n-\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":29,"deletions":11,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -65,1 +65,0 @@\n-CLDToOopClosure    MarkSweep::adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_strong);\n@@ -147,0 +146,1 @@\n+template <bool ALT_FWD>\n@@ -148,1 +148,1 @@\n-  MarkSweep::adjust_pointer(&_obj);\n+  MarkSweep::adjust_pointer<ALT_FWD>(&_obj);\n@@ -176,3 +176,2 @@\n-AdjustPointerClosure MarkSweep::adjust_pointer_closure;\n-\n-void MarkSweep::adjust_marks() {\n+template <bool ALT_FWD>\n+void MarkSweep::adjust_marks_impl() {\n@@ -184,1 +183,1 @@\n-    _preserved_marks[i].adjust_pointer();\n+    _preserved_marks[i].adjust_pointer<ALT_FWD>();\n@@ -191,1 +190,9 @@\n-    adjust_pointer(p);\n+    adjust_pointer<ALT_FWD>(p);\n+  }\n+}\n+\n+void MarkSweep::adjust_marks() {\n+  if (UseAltGCForwarding) {\n+    adjust_marks_impl<true>();\n+  } else {\n+    adjust_marks_impl<false>();\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.cpp","additions":14,"deletions":7,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -53,1 +53,0 @@\n-class AdjustPointerClosure;\n@@ -87,1 +86,0 @@\n-  friend class AdjustPointerClosure;\n@@ -127,2 +125,0 @@\n-  static AdjustPointerClosure adjust_pointer_closure;\n-  static CLDToOopClosure      adjust_cld_closure;\n@@ -142,0 +138,2 @@\n+  template <bool ALT_FWD>\n+  static void adjust_marks_impl();   \/\/ Adjust the pointers in the preserved marks table\n@@ -145,0 +143,1 @@\n+  template <bool ALT_FWD>\n@@ -153,1 +152,2 @@\n-  template <class T> static inline void adjust_pointer(T* p);\n+  template <bool ALT_FWD, class T>\n+  static inline void adjust_pointer(T* p);\n@@ -188,0 +188,1 @@\n+template <bool ALT_FWD>\n@@ -207,0 +208,1 @@\n+  template <bool ALT_FWD>\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.hpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -43,1 +44,1 @@\n-  obj->set_mark(markWord::prototype().set_marked());\n+  obj->set_mark(obj->prototype_mark().set_marked());\n@@ -77,1 +78,2 @@\n-template <class T> inline void MarkSweep::adjust_pointer(T* p) {\n+template <bool ALT_FWD, class T>\n+inline void MarkSweep::adjust_pointer(T* p) {\n@@ -83,9 +85,3 @@\n-    oop new_obj = cast_to_oop(obj->mark().decode_pointer());\n-\n-    assert(new_obj != NULL ||                      \/\/ is forwarding ptr?\n-           obj->mark() == markWord::prototype() || \/\/ not gc marked?\n-           (UseBiasedLocking && obj->mark().has_bias_pattern()),\n-           \/\/ not gc marked?\n-           \"should be forwarded\");\n-\n-    if (new_obj != NULL) {\n+    if (SlidingForwarding::is_forwarded(obj)) {\n+      oop new_obj = SlidingForwarding::forwardee<ALT_FWD>(obj);\n+      assert(new_obj != NULL, \"must be forwarded\");\n@@ -98,0 +94,1 @@\n+template <bool ALT_FWD>\n@@ -99,3 +96,4 @@\n-void AdjustPointerClosure::do_oop_work(T* p)           { MarkSweep::adjust_pointer(p); }\n-inline void AdjustPointerClosure::do_oop(oop* p)       { do_oop_work(p); }\n-inline void AdjustPointerClosure::do_oop(narrowOop* p) { do_oop_work(p); }\n+void AdjustPointerClosure<ALT_FWD>::do_oop_work(T* p)           { MarkSweep::adjust_pointer<ALT_FWD>(p); }\n+\n+template <bool ALT_FWD>\n+inline void AdjustPointerClosure<ALT_FWD>::do_oop(oop* p)       { do_oop_work(p); }\n@@ -103,0 +101,2 @@\n+template <bool ALT_FWD>\n+inline void AdjustPointerClosure<ALT_FWD>::do_oop(narrowOop* p) { do_oop_work(p); }\n@@ -104,0 +104,1 @@\n+template <bool ALT_FWD>\n@@ -105,1 +106,2 @@\n-  return obj->oop_iterate_size(&MarkSweep::adjust_pointer_closure);\n+  AdjustPointerClosure<ALT_FWD> adjust_pointer_closure;\n+  return obj->oop_iterate_size(&adjust_pointer_closure);\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.inline.hpp","additions":17,"deletions":15,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -223,4 +223,0 @@\n-  if (is_in(object->klass_or_null())) {\n-    return false;\n-  }\n-\n@@ -245,2 +241,4 @@\n-  _filler_array_max_size = align_object_size(filler_array_hdr_size() +\n-                                             max_len \/ elements_per_word);\n+  int header_size_in_bytes = arrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(header_size_in_bytes % sizeof(jint) == 0, \"must be aligned to int\");\n+  int header_size_in_ints = header_size_in_bytes \/ sizeof(jint);\n+  _filler_array_max_size = align_object_size((header_size_in_ints + max_len) \/ elements_per_word);\n@@ -395,1 +393,3 @@\n-  size_t max_int_size = typeArrayOopDesc::header_size(T_INT) +\n+  int header_size_in_bytes = typeArrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(header_size_in_bytes % sizeof(jint) == 0, \"header size must align to int\");\n+  size_t max_int_size = header_size_in_bytes \/ HeapWordSize +\n@@ -401,4 +401,0 @@\n-size_t CollectedHeap::filler_array_hdr_size() {\n-  return align_object_offset(arrayOopDesc::header_size(T_INT)); \/\/ align to Long\n-}\n-\n@@ -406,1 +402,2 @@\n-  return align_object_size(filler_array_hdr_size()); \/\/ align to MinObjAlignment\n+  int aligned_header_size_words = align_up(arrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize;\n+  return align_object_size(aligned_header_size_words); \/\/ align to MinObjAlignment\n@@ -419,2 +416,3 @@\n-    Copy::fill_to_words(start + filler_array_hdr_size(),\n-                        words - filler_array_hdr_size(), 0XDEAFBABE);\n+  int payload_start = align_up(arrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize;\n+  Copy::fill_to_words(start + payload_start,\n+                      words - payload_start, 0XDEAFBABE);\n@@ -431,2 +429,3 @@\n-  const size_t payload_size = words - filler_array_hdr_size();\n-  const size_t len = payload_size * HeapWordSize \/ sizeof(jint);\n+  const size_t payload_size_bytes = words * HeapWordSize - arrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(payload_size_bytes % sizeof(jint) == 0, \"must be int aligned\");\n+  const size_t len = payload_size_bytes \/ sizeof(jint);\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":15,"deletions":16,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -157,1 +157,0 @@\n-  static inline size_t filler_array_hdr_size();\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -692,1 +692,4 @@\n-          range(0, max_juint)\n+          range(0, max_juint)                                               \\\n+                                                                            \\\n+  product(bool, UseAltGCForwarding, false, EXPERIMENTAL,                    \\\n+          \"Use alternative GC forwarding that preserves object headers\")    \\\n","filename":"src\/hotspot\/share\/gc\/shared\/gc_globals.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -140,0 +141,2 @@\n+  SlidingForwarding::initialize(_reserved, SpaceAlignment \/ HeapWordSize);\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -360,1 +360,3 @@\n-  oopDesc::set_klass_gap(mem, 0);\n+  if (!UseCompactObjectHeaders) {\n+    oopDesc::set_klass_gap(mem, 0);\n+  }\n@@ -368,0 +370,2 @@\n+  } else if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, _klass->prototype_header());\n@@ -375,1 +379,3 @@\n-  oopDesc::release_set_klass(mem, _klass);\n+  if (!UseCompactObjectHeaders) {\n+    oopDesc::release_set_klass(mem, _klass);\n+  }\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -265,0 +266,1 @@\n+        ObjectMonitor::maybe_deflate_dead(ptr);\n","filename":"src\/hotspot\/share\/gc\/shared\/oopStorage.inline.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -42,1 +43,2 @@\n-void PreservedMarks::adjust_during_full_gc() {\n+template <bool ALT_FWD>\n+void PreservedMarks::adjust_during_full_gc_impl() {\n@@ -48,2 +50,2 @@\n-    if (obj->is_forwarded()) {\n-      elem->set_oop(obj->forwardee());\n+    if (SlidingForwarding::is_forwarded(obj)) {\n+      elem->set_oop(SlidingForwarding::forwardee<ALT_FWD>(obj));\n@@ -54,0 +56,8 @@\n+void PreservedMarks::adjust_during_full_gc() {\n+  if (UseAltGCForwarding) {\n+    adjust_during_full_gc_impl<true>();\n+  } else {\n+    adjust_during_full_gc_impl<false>();\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/preservedMarks.cpp","additions":13,"deletions":3,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -57,0 +57,3 @@\n+  template <bool ALT_FWD>\n+  void adjust_during_full_gc_impl();\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/preservedMarks.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -51,1 +51,1 @@\n-  obj->init_mark();\n+  obj->forward_safe_init_mark();\n","filename":"src\/hotspot\/share\/gc\/shared\/preservedMarks.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,173 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n+#include \"utilities\/fastHash.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+\/\/ We cannot use 0, because that may already be a valid base address in zero-based heaps.\n+\/\/ 0x1 is safe because heap base addresses must be aligned by much larger alignment\n+HeapWord* const SlidingForwarding::UNUSED_BASE = reinterpret_cast<HeapWord*>(0x1);\n+\n+HeapWord* SlidingForwarding::_heap_start = nullptr;\n+size_t SlidingForwarding::_region_size_words = 0;\n+size_t SlidingForwarding::_heap_start_region_bias = 0;\n+size_t SlidingForwarding::_num_regions = 0;\n+uint SlidingForwarding::_region_size_bytes_shift = 0;\n+uintptr_t SlidingForwarding::_region_mask = 0;\n+HeapWord** SlidingForwarding::_biased_bases[SlidingForwarding::NUM_TARGET_REGIONS] = { nullptr, nullptr };\n+HeapWord** SlidingForwarding::_bases_table = nullptr;\n+SlidingForwarding::FallbackTable* SlidingForwarding::_fallback_table = nullptr;\n+\n+void SlidingForwarding::initialize(MemRegion heap, size_t region_size_words) {\n+#ifdef _LP64\n+  if (UseAltGCForwarding) {\n+    _heap_start = heap.start();\n+\n+    \/\/ If the heap is small enough to fit directly into the available offset bits,\n+    \/\/ and we are running Serial GC, we can treat the whole heap as a single region\n+    \/\/ if it happens to be aligned to allow biasing.\n+    size_t rounded_heap_size = round_up_power_of_2(heap.byte_size());\n+\n+    if (UseSerialGC && (heap.word_size() <= (1 << NUM_OFFSET_BITS)) &&\n+        is_aligned((uintptr_t)_heap_start, rounded_heap_size)) {\n+      _num_regions = 1;\n+      _region_size_words = heap.word_size();\n+      _region_size_bytes_shift = log2i_exact(rounded_heap_size);\n+    } else {\n+      _num_regions = align_up(pointer_delta(heap.end(), heap.start()), region_size_words) \/ region_size_words;\n+      _region_size_words = region_size_words;\n+      _region_size_bytes_shift = log2i_exact(_region_size_words) + LogHeapWordSize;\n+    }\n+    _heap_start_region_bias = (uintptr_t)_heap_start >> _region_size_bytes_shift;\n+    _region_mask = ~((uintptr_t(1) << _region_size_bytes_shift) - 1);\n+\n+    guarantee((_heap_start_region_bias << _region_size_bytes_shift) == (uintptr_t)_heap_start, \"must be aligned: _heap_start_region_bias: \" SIZE_FORMAT \", _region_size_byte_shift: %u, _heap_start: \" PTR_FORMAT, _heap_start_region_bias, _region_size_bytes_shift, p2i(_heap_start));\n+\n+    assert(_region_size_words >= 1, \"regions must be at least a word large\");\n+    assert(_bases_table == nullptr, \"should not be initialized yet\");\n+    assert(_fallback_table == nullptr, \"should not be initialized yet\");\n+  }\n+#endif\n+}\n+\n+void SlidingForwarding::begin() {\n+#ifdef _LP64\n+  if (UseAltGCForwarding) {\n+    assert(_bases_table == nullptr, \"should not be initialized yet\");\n+    assert(_fallback_table == nullptr, \"should not be initialized yet\");\n+\n+    size_t max = _num_regions * NUM_TARGET_REGIONS;\n+    _bases_table = NEW_C_HEAP_ARRAY(HeapWord*, max, mtGC);\n+    HeapWord** biased_start = _bases_table - _heap_start_region_bias;\n+    _biased_bases[0] = biased_start;\n+    _biased_bases[1] = biased_start + _num_regions;\n+    for (size_t i = 0; i < max; i++) {\n+      _bases_table[i] = UNUSED_BASE;\n+    }\n+  }\n+#endif\n+}\n+\n+void SlidingForwarding::end() {\n+#ifdef _LP64\n+  if (UseAltGCForwarding) {\n+    assert(_bases_table != nullptr, \"should be initialized\");\n+    FREE_C_HEAP_ARRAY(HeapWord*, _bases_table);\n+    _bases_table = nullptr;\n+    delete _fallback_table;\n+    _fallback_table = nullptr;\n+  }\n+#endif\n+}\n+\n+void SlidingForwarding::fallback_forward_to(HeapWord* from, HeapWord* to) {\n+  if (_fallback_table == nullptr) {\n+    _fallback_table = new FallbackTable();\n+  }\n+  _fallback_table->forward_to(from, to);\n+}\n+\n+HeapWord* SlidingForwarding::fallback_forwardee(HeapWord* from) {\n+  assert(_fallback_table != nullptr, \"fallback table must be present\");\n+  return _fallback_table->forwardee(from);\n+}\n+\n+SlidingForwarding::FallbackTable::FallbackTable() {\n+  for (uint i = 0; i < TABLE_SIZE; i++) {\n+    _table[i]._next = nullptr;\n+    _table[i]._from = nullptr;\n+    _table[i]._to   = nullptr;\n+  }\n+}\n+\n+SlidingForwarding::FallbackTable::~FallbackTable() {\n+  for (uint i = 0; i < TABLE_SIZE; i++) {\n+    FallbackTableEntry* entry = _table[i]._next;\n+    while (entry != nullptr) {\n+      FallbackTableEntry* next = entry->_next;\n+      FREE_C_HEAP_OBJ(entry);\n+      entry = next;\n+    }\n+  }\n+}\n+\n+size_t SlidingForwarding::FallbackTable::home_index(HeapWord* from) {\n+  uint64_t val = reinterpret_cast<uint64_t>(from);\n+  uint64_t hash = FastHash::get_hash64(val, UCONST64(0xAAAAAAAAAAAAAAAA));\n+  return hash >> (64 - log2i_exact(TABLE_SIZE));\n+}\n+\n+void SlidingForwarding::FallbackTable::forward_to(HeapWord* from, HeapWord* to) {\n+  size_t idx = home_index(from);\n+  FallbackTableEntry* head = &_table[idx];\n+#ifdef ASSERT\n+  \/\/ Search existing entry in chain starting at idx.\n+  for (FallbackTableEntry* entry = head; entry != nullptr; entry = entry->_next) {\n+    assert(entry->_from != from, \"Don't re-forward entries into the fallback-table\");\n+  }\n+#endif\n+  \/\/ No entry found, create new one and insert after head.\n+  FallbackTableEntry* new_entry = NEW_C_HEAP_OBJ(FallbackTableEntry, mtGC);\n+  *new_entry = *head;\n+  head->_next = new_entry;\n+  head->_from = from;\n+  head->_to   = to;\n+}\n+\n+HeapWord* SlidingForwarding::FallbackTable::forwardee(HeapWord* from) const {\n+  size_t idx = home_index(from);\n+  const FallbackTableEntry* entry = &_table[idx];\n+  while (entry != nullptr) {\n+    if (entry->_from == from) {\n+      return entry->_to;\n+    }\n+    entry = entry->_next;\n+  }\n+  return nullptr;\n+}\n","filename":"src\/hotspot\/share\/gc\/shared\/slidingForwarding.cpp","additions":173,"deletions":0,"binary":false,"changes":173,"status":"added"},{"patch":"@@ -0,0 +1,188 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHARED_SLIDINGFORWARDING_HPP\n+#define SHARE_GC_SHARED_SLIDINGFORWARDING_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/memRegion.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+\n+\/**\n+ * SlidingForwarding is a method to store forwarding information in a compressed form into the object header,\n+ * that has been specifically designed for sliding compaction GCs and compact object headers. With compact object\n+ * headers, we store the compressed class pointer in the header, which would be overwritten by full forwarding\n+ * pointer, if we allow the legacy forwarding code to act. This would lose the class information for the object,\n+ * which is required later in GC cycle to iterate the reference fields and get the object size for copying.\n+ *\n+ * SlidingForwarding requires only small side tables and guarantees constant-time access and modification.\n+ *\n+ * The idea is to use a pointer compression scheme very similar to the one that is used for compressed oops.\n+ * We divide the heap into number of logical regions. Each region spans maximum of 2^NUM_OFFSET_BITS words.\n+ *\n+ * The key advantage of sliding compaction for encoding efficiency: it can forward objects from one region to a\n+ * maximum of two regions. This is an intuitive property: when we slide the compact region full of data, it can\n+ * only span two adjacent regions. This property allows us to use the off-side table to record the addresses of\n+ * two target regions. The table holds N*2 entries for N logical regions. For each region, it gives the base\n+ * address of the two target regions, or a special placeholder if not used. A single bit in forwarding would\n+ * indicate to which of the two \"to\" regions the object is forwarded into.\n+ *\n+ * This encoding efficiency allows to store the forwarding information in the object header _together_ with the\n+ * compressed class pointer.\n+ *\n+ * When recording the sliding forwarding, the mark word would look roughly like this:\n+ *\n+ *   64                              32                                0\n+ *    [................................OOOOOOOOOOOOOOOOOOOOOOOOOOOOAFTT]\n+ *                                                                    ^----- normal lock bits, would record \"object is forwarded\"\n+ *                                                                  ^------- fallback bit (explained below)\n+ *                                                                 ^-------- alternate region select\n+ *                                     ^------------------------------------ in-region offset\n+ *     ^-------------------------------------------------------------------- protected area, *not touched* by this code, useful for\n+ *                                                                           compressed class pointer with compact object headers\n+ *\n+ * Adding a forwarding then generally works as follows:\n+ *   1. Compute the \"to\" offset in the \"to\" region, this gives \"offset\".\n+ *   2. Check if the primary \"from\" offset at base table contains \"to\" region base, use it.\n+ *      If not usable, continue to next step. If usable, set \"alternate\" = \"false\" and jump to (4).\n+ *   3. Check if the alternate \"from\" offset at base table contains \"to\" region base, use it.\n+ *      This gives us \"alternate\" = \"true\". This should always complete for sliding forwarding.\n+ *   4. Compute the mark word from \"offset\" and \"alternate\", write it out\n+ *\n+ * Similarly, looking up the target address, given an original object address generally works as follows:\n+ *   1. Load the mark from object, and decode \"offset\" and \"alternate\" from there\n+ *   2. Compute the \"from\" base offset from the object\n+ *   3. Look up \"to\" region base from the base table either at primary or alternate indices, using \"alternate\" flag\n+ *   4. Compute the \"to\" address from \"to\" region base and \"offset\"\n+ *\n+ * This algorithm is broken by G1 last-ditch serial compaction: there, object from a single region can be\n+ * forwarded to multiple, more than two regions. To deal with that, we initialize a fallback-hashtable for\n+ * storing those extra forwardings, and set another bit in the header to indicate that the forwardee is not\n+ * encoded but should be looked-up in the hashtable. G1 serial compaction is not very common - it is the\n+ * last-last-ditch GC that is used when the JVM is scrambling to squeeze more space out of the heap, and at\n+ * that point, ultimate performance is no longer the main concern.\n+ *\/\n+class SlidingForwarding : public AllStatic {\n+private:\n+\n+  \/*\n+   * A simple hash-table that acts as fallback for the sliding forwarding.\n+   * This is used in the case of G1 serial compaction, which violates the\n+   * assumption of sliding forwarding that each object of any region is only\n+   * ever forwarded to one of two target regions. At this point, the GC is\n+   * scrambling to free up more Java heap memory, and therefore performance\n+   * is not the major concern.\n+   *\n+   * The implementation is a straightforward open hashtable.\n+   * It is a single-threaded (not thread-safe) implementation, and that\n+   * is sufficient because G1 serial compaction is single-threaded.\n+   *\/\n+  class FallbackTable : public CHeapObj<mtGC>{\n+  private:\n+    struct FallbackTableEntry {\n+      FallbackTableEntry* _next;\n+      HeapWord* _from;\n+      HeapWord* _to;\n+    };\n+\n+    static const uint TABLE_SIZE = 1024;\n+    FallbackTableEntry _table[TABLE_SIZE];\n+\n+    static size_t home_index(HeapWord* from);\n+\n+  public:\n+    FallbackTable();\n+    ~FallbackTable();\n+\n+    void forward_to(HeapWord* from, HeapWord* to);\n+    HeapWord* forwardee(HeapWord* from) const;\n+  };\n+\n+  static const uintptr_t MARK_LOWER_HALF_MASK = right_n_bits(32);\n+\n+  \/\/ We need the lowest two bits to indicate a forwarded object.\n+  \/\/ The next bit indicates that the forwardee should be looked-up in a fallback-table.\n+  static const int FALLBACK_SHIFT = markWord::lock_bits;\n+  static const int FALLBACK_BITS = 1;\n+  static const int FALLBACK_MASK = right_n_bits(FALLBACK_BITS) << FALLBACK_SHIFT;\n+\n+  \/\/ Next bit selects the target region\n+  static const int ALT_REGION_SHIFT = FALLBACK_SHIFT + FALLBACK_BITS;\n+  static const int ALT_REGION_BITS = 1;\n+  \/\/ This will be \"2\" always, but expose it as named constant for clarity\n+  static const size_t NUM_TARGET_REGIONS = 1 << ALT_REGION_BITS;\n+\n+  \/\/ The offset bits start then\n+  static const int OFFSET_BITS_SHIFT = ALT_REGION_SHIFT + ALT_REGION_BITS;\n+\n+  \/\/ How many bits we use for the offset\n+  static const int NUM_OFFSET_BITS = 32 - OFFSET_BITS_SHIFT;\n+\n+  \/\/ Indicates an unused base address in the target base table.\n+  static HeapWord* const UNUSED_BASE;\n+\n+  static HeapWord*      _heap_start;\n+  static size_t         _region_size_words;\n+\n+  static size_t         _heap_start_region_bias;\n+  static size_t         _num_regions;\n+  static uint           _region_size_bytes_shift;\n+  static uintptr_t      _region_mask;\n+\n+  \/\/ The target base table memory.\n+  static HeapWord**     _bases_table;\n+  \/\/ Entries into the target base tables, biased to the start of the heap.\n+  static HeapWord**     _biased_bases[NUM_TARGET_REGIONS];\n+\n+  static FallbackTable* _fallback_table;\n+\n+  static inline size_t biased_region_index_containing(HeapWord* addr);\n+\n+  static inline uintptr_t encode_forwarding(HeapWord* from, HeapWord* to);\n+  static inline HeapWord* decode_forwarding(HeapWord* from, uintptr_t encoded);\n+\n+  static void fallback_forward_to(HeapWord* from, HeapWord* to);\n+  static HeapWord* fallback_forwardee(HeapWord* from);\n+\n+  static inline void forward_to_impl(oop from, oop to);\n+  static inline oop forwardee_impl(oop from);\n+\n+public:\n+  static void initialize(MemRegion heap, size_t region_size_words);\n+\n+  static void begin();\n+  static void end();\n+\n+  static inline bool is_forwarded(oop obj);\n+  static inline bool is_not_forwarded(oop obj);\n+\n+  template <bool ALT_FWD>\n+  static inline void forward_to(oop from, oop to);\n+  template <bool ALT_FWD>\n+  static inline oop forwardee(oop from);\n+};\n+\n+#endif \/\/ SHARE_GC_SHARED_SLIDINGFORWARDING_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/slidingForwarding.hpp","additions":188,"deletions":0,"binary":false,"changes":188,"status":"added"},{"patch":"@@ -0,0 +1,171 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_SHARED_SLIDINGFORWARDING_INLINE_HPP\n+#define SHARE_GC_SHARED_SLIDINGFORWARDING_INLINE_HPP\n+\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+inline bool SlidingForwarding::is_forwarded(oop obj) {\n+  return obj->is_forwarded();\n+}\n+\n+inline bool SlidingForwarding::is_not_forwarded(oop obj) {\n+  return !obj->is_forwarded();\n+}\n+\n+size_t SlidingForwarding::biased_region_index_containing(HeapWord* addr) {\n+  return (uintptr_t)addr >> _region_size_bytes_shift;\n+}\n+\n+uintptr_t SlidingForwarding::encode_forwarding(HeapWord* from, HeapWord* to) {\n+  static_assert(NUM_TARGET_REGIONS == 2, \"Only implemented for this amount\");\n+\n+  size_t from_reg_idx = biased_region_index_containing(from);\n+  HeapWord* to_region_base = (HeapWord*)((uintptr_t)to & _region_mask);\n+\n+  HeapWord** base = &_biased_bases[0][from_reg_idx];\n+  uintptr_t alternate = 0;\n+  if (*base == to_region_base) {\n+    \/\/ Primary is good\n+  } else if (*base == UNUSED_BASE) {\n+    \/\/ Primary is free\n+    *base = to_region_base;\n+  } else {\n+    base = &_biased_bases[1][from_reg_idx];\n+    if (*base == to_region_base) {\n+      \/\/ Alternate is good\n+    } else if (*base == UNUSED_BASE) {\n+      \/\/ Alternate is free\n+      *base = to_region_base;\n+    } else {\n+      \/\/ Both primary and alternate are not fitting\n+      \/\/ This happens only in the following rare situations:\n+      \/\/ - In Serial GC, sometimes when compact-top switches spaces, because the\n+      \/\/   region boudaries are virtual and objects can cross regions\n+      \/\/ - In G1 serial compaction, because tails of various compaction chains\n+      \/\/   are distributed across the remainders of already compacted regions.\n+      return (1 << FALLBACK_SHIFT) | markWord::marked_value;\n+    }\n+    alternate = 1;\n+  }\n+\n+  size_t offset = pointer_delta(to, to_region_base);\n+  assert(offset < _region_size_words, \"Offset should be within the region. from: \" PTR_FORMAT\n+         \", to: \" PTR_FORMAT \", to_region_base: \" PTR_FORMAT \", offset: \" SIZE_FORMAT,\n+         p2i(from), p2i(to), p2i(to_region_base), offset);\n+\n+  uintptr_t encoded = (offset << OFFSET_BITS_SHIFT) |\n+                      (alternate << ALT_REGION_SHIFT) |\n+                      markWord::marked_value;\n+\n+  assert(to == decode_forwarding(from, encoded), \"must be reversible\");\n+  assert((encoded & ~MARK_LOWER_HALF_MASK) == 0, \"must encode to lowest 32 bits\");\n+  return encoded;\n+}\n+\n+HeapWord* SlidingForwarding::decode_forwarding(HeapWord* from, uintptr_t encoded) {\n+  assert((encoded & markWord::lock_mask_in_place) == markWord::marked_value, \"must be marked as forwarded\");\n+  assert((encoded & FALLBACK_MASK) == 0, \"must not be fallback-forwarded\");\n+  assert((encoded & ~MARK_LOWER_HALF_MASK) == 0, \"must decode from lowest 32 bits\");\n+  size_t alternate = (encoded >> ALT_REGION_SHIFT) & right_n_bits(ALT_REGION_BITS);\n+  assert(alternate < NUM_TARGET_REGIONS, \"Sanity\");\n+  uintptr_t offset = (encoded >> OFFSET_BITS_SHIFT);\n+\n+  size_t from_idx = biased_region_index_containing(from);\n+  HeapWord* base = _biased_bases[alternate][from_idx];\n+  assert(base != UNUSED_BASE, \"must not be unused base\");\n+  HeapWord* decoded = base + offset;\n+  assert(decoded >= _heap_start,\n+         \"Address must be above heap start. encoded: \" INTPTR_FORMAT \", alt_region: \" SIZE_FORMAT \", base: \" PTR_FORMAT,\n+         encoded, alternate, p2i(base));\n+\n+  return decoded;\n+}\n+\n+inline void SlidingForwarding::forward_to_impl(oop from, oop to) {\n+  assert(_bases_table != nullptr, \"call begin() before forwarding\");\n+\n+  markWord from_header = from->mark();\n+  if (from_header.has_displaced_mark_helper()) {\n+    from_header = from_header.displaced_mark_helper();\n+  }\n+\n+  HeapWord* from_hw = cast_from_oop<HeapWord*>(from);\n+  HeapWord* to_hw   = cast_from_oop<HeapWord*>(to);\n+  uintptr_t encoded = encode_forwarding(from_hw, to_hw);\n+  markWord new_header = markWord((from_header.value() & ~MARK_LOWER_HALF_MASK) | encoded);\n+  from->set_mark(new_header);\n+\n+  if ((encoded & FALLBACK_MASK) != 0) {\n+    fallback_forward_to(from_hw, to_hw);\n+  }\n+}\n+\n+template <bool ALT_FWD>\n+inline void SlidingForwarding::forward_to(oop obj, oop fwd) {\n+#ifdef _LP64\n+  if (ALT_FWD) {\n+    assert(_bases_table != nullptr, \"expect sliding forwarding initialized\");\n+    forward_to_impl(obj, fwd);\n+    assert(forwardee<ALT_FWD>(obj) == fwd, \"must be forwarded to correct forwardee\");\n+  } else\n+#endif\n+  {\n+    obj->forward_to(fwd);\n+  }\n+}\n+\n+inline oop SlidingForwarding::forwardee_impl(oop from) {\n+  assert(_bases_table != nullptr, \"call begin() before asking for forwarding\");\n+\n+  markWord header = from->mark();\n+  HeapWord* from_hw = cast_from_oop<HeapWord*>(from);\n+  if ((header.value() & FALLBACK_MASK) != 0) {\n+    HeapWord* to = fallback_forwardee(from_hw);\n+    return cast_to_oop(to);\n+  }\n+  uintptr_t encoded = header.value() & MARK_LOWER_HALF_MASK;\n+  HeapWord* to = decode_forwarding(from_hw, encoded);\n+  return cast_to_oop(to);\n+}\n+\n+template <bool ALT_FWD>\n+inline oop SlidingForwarding::forwardee(oop obj) {\n+#ifdef _LP64\n+  if (ALT_FWD) {\n+    assert(_bases_table != nullptr, \"expect sliding forwarding initialized\");\n+    return forwardee_impl(obj);\n+  } else\n+#endif\n+  {\n+    return obj->forwardee();\n+  }\n+}\n+\n+#endif \/\/ SHARE_GC_SHARED_SLIDINGFORWARDING_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/slidingForwarding.inline.hpp","additions":171,"deletions":0,"binary":false,"changes":171,"status":"added"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -349,0 +350,1 @@\n+template <bool ALT_FWD>\n@@ -373,1 +375,1 @@\n-    q->forward_to(cast_to_oop(compact_top));\n+    SlidingForwarding::forward_to<ALT_FWD>(q, cast_to_oop(compact_top));\n@@ -379,1 +381,1 @@\n-    assert(q->forwardee() == NULL, \"should be forwarded to NULL\");\n+    assert(SlidingForwarding::is_not_forwarded(q), \"should not be forwarded\");\n@@ -396,1 +398,5 @@\n-  scan_and_forward(this, cp);\n+  if (UseAltGCForwarding) {\n+    scan_and_forward<true>(this, cp);\n+  } else {\n+    scan_and_forward<false>(this, cp);\n+  }\n@@ -405,1 +411,5 @@\n-  scan_and_adjust_pointers(this);\n+  if (UseAltGCForwarding) {\n+    scan_and_adjust_pointers<true>(this);\n+  } else {\n+    scan_and_adjust_pointers<false>(this);\n+  }\n@@ -409,1 +419,5 @@\n-  scan_and_compact(this);\n+  if (UseAltGCForwarding) {\n+    scan_and_compact<true>(this);\n+  } else {\n+    scan_and_compact<false>(this);\n+  }\n@@ -589,1 +603,1 @@\n-  const size_t array_header_size = typeArrayOopDesc::header_size(T_INT);\n+  const size_t array_header_size = (arrayOopDesc::base_offset_in_bytes(T_INT) + BytesPerWord) \/ BytesPerWord;\n@@ -595,2 +609,6 @@\n-    t->set_mark(markWord::prototype());\n-    t->set_klass(Universe::intArrayKlassObj());\n+    if (UseCompactObjectHeaders) {\n+      t->set_mark(Universe::intArrayKlassObj()->prototype_header());\n+    } else {\n+      t->set_mark(markWord::prototype());\n+      t->set_klass(Universe::intArrayKlassObj());\n+    }\n@@ -602,3 +620,7 @@\n-    obj->set_mark(markWord::prototype());\n-    obj->set_klass_gap(0);\n-    obj->set_klass(vmClasses::Object_klass());\n+    if (UseCompactObjectHeaders) {\n+      obj->set_mark(vmClasses::Object_klass()->prototype_header());\n+    } else {\n+      obj->set_mark(markWord::prototype());\n+      obj->set_klass_gap(0);\n+      obj->set_klass(vmClasses::Object_klass());\n+    }\n","filename":"src\/hotspot\/share\/gc\/shared\/space.cpp","additions":33,"deletions":11,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -434,1 +434,2 @@\n-  virtual HeapWord* forward(oop q, size_t size, CompactPoint* cp,\n+  template <bool ALT_FWD>\n+  HeapWord* forward(oop q, size_t size, CompactPoint* cp,\n@@ -463,1 +464,1 @@\n-  template <class SpaceType>\n+  template <bool ALT_FWD, class SpaceType>\n@@ -468,1 +469,1 @@\n-  template <class SpaceType>\n+  template <bool ALT_FWD, class SpaceType>\n@@ -473,1 +474,1 @@\n-  template <class SpaceType>\n+  template <bool ALT_FWD, class SpaceType>\n@@ -484,1 +485,1 @@\n-  template <typename SpaceType>\n+  template <bool ALT_FWD, typename SpaceType>\n","filename":"src\/hotspot\/share\/gc\/shared\/space.hpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -136,1 +136,1 @@\n-template <class SpaceType>\n+template <bool ALT_FWD, class SpaceType>\n@@ -171,1 +171,1 @@\n-      compact_top = cp->space->forward(cast_to_oop(cur_obj), size, cp, compact_top);\n+      compact_top = cp->space->forward<ALT_FWD>(cast_to_oop(cur_obj), size, cp, compact_top);\n@@ -187,1 +187,1 @@\n-        compact_top = cp->space->forward(obj, obj->size(), cp, compact_top);\n+        compact_top = cp->space->forward<ALT_FWD>(obj, obj->size(), cp, compact_top);\n@@ -218,1 +218,1 @@\n-template <class SpaceType>\n+template <bool ALT_FWD, class SpaceType>\n@@ -237,1 +237,1 @@\n-      size_t size = MarkSweep::adjust_pointers(cast_to_oop(cur_obj));\n+      size_t size = MarkSweep::adjust_pointers<ALT_FWD>(cast_to_oop(cur_obj));\n@@ -291,1 +291,1 @@\n-template <class SpaceType>\n+template <bool ALT_FWD, class SpaceType>\n@@ -332,1 +332,1 @@\n-      HeapWord* compaction_top = cast_from_oop<HeapWord*>(cast_to_oop(cur_obj)->forwardee());\n+      HeapWord* compaction_top = cast_from_oop<HeapWord*>(SlidingForwarding::forwardee<ALT_FWD>(cast_to_oop(cur_obj)));\n","filename":"src\/hotspot\/share\/gc\/shared\/space.inline.hpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -68,0 +68,1 @@\n+      ObjectMonitor::maybe_deflate_dead(p);\n","filename":"src\/hotspot\/share\/gc\/shared\/weakProcessor.inline.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -200,1 +200,1 @@\n-  Klass* obj_klass = obj->klass_or_null();\n+  Klass* obj_klass = obj->forward_safe_klass();\n@@ -232,1 +232,1 @@\n-    if (obj_klass != fwd->klass()) {\n+    if (obj_klass != fwd->forward_safe_klass()) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -225,0 +226,2 @@\n+    SlidingForwarding::begin();\n+\n@@ -239,0 +242,1 @@\n+    SlidingForwarding::end();\n@@ -300,0 +304,1 @@\n+template <bool ALT_FWD>\n@@ -368,1 +373,1 @@\n-    p->forward_to(cast_to_oop(_compact_point));\n+    SlidingForwarding::forward_to<ALT_FWD>(p, cast_to_oop(_compact_point));\n@@ -399,0 +404,10 @@\n+    if (UseAltGCForwarding) {\n+      work_impl<true>(worker_id);\n+    } else {\n+      work_impl<false>(worker_id);\n+    }\n+  }\n+\n+private:\n+  template <bool ALT_FWD>\n+  void work_impl(uint worker_id) {\n@@ -414,1 +429,1 @@\n-    ShenandoahPrepareForCompactionObjectClosure cl(_preserved_marks->get(worker_id), empty_regions, from_region);\n+    ShenandoahPrepareForCompactionObjectClosure<ALT_FWD> cl(_preserved_marks->get(worker_id), empty_regions, from_region);\n@@ -440,1 +455,2 @@\n-void ShenandoahFullGC::calculate_target_humongous_objects() {\n+template <bool ALT_FWD>\n+void ShenandoahFullGC::calculate_target_humongous_objects_impl() {\n@@ -476,1 +492,1 @@\n-        old_obj->forward_to(cast_to_oop(heap->get_region(start)->bottom()));\n+        SlidingForwarding::forward_to<ALT_FWD>(old_obj, cast_to_oop(heap->get_region(start)->bottom()));\n@@ -488,0 +504,8 @@\n+void ShenandoahFullGC::calculate_target_humongous_objects() {\n+  if (UseAltGCForwarding) {\n+    calculate_target_humongous_objects_impl<true>();\n+  } else {\n+    calculate_target_humongous_objects_impl<false>();\n+  }\n+}\n+\n@@ -725,0 +749,1 @@\n+template <bool ALT_FWD>\n@@ -736,2 +761,2 @@\n-      if (obj->is_forwarded()) {\n-        oop forw = obj->forwardee();\n+      if (SlidingForwarding::is_forwarded(obj)) {\n+        oop forw = SlidingForwarding::forwardee<ALT_FWD>(obj);\n@@ -752,0 +777,1 @@\n+template <bool ALT_FWD>\n@@ -755,1 +781,1 @@\n-  ShenandoahAdjustPointersClosure _cl;\n+  ShenandoahAdjustPointersClosure<ALT_FWD> _cl;\n@@ -778,1 +804,3 @@\n-  void work(uint worker_id) {\n+private:\n+  template <bool ALT_FWD>\n+  void work_impl(uint worker_id) {\n@@ -780,1 +808,1 @@\n-    ShenandoahAdjustPointersObjectClosure obj_cl;\n+    ShenandoahAdjustPointersObjectClosure<ALT_FWD> obj_cl;\n@@ -789,0 +817,9 @@\n+\n+public:\n+  void work(uint worker_id) {\n+    if (UseAltGCForwarding) {\n+      work_impl<true>(worker_id);\n+    } else {\n+      work_impl<false>(worker_id);\n+    }\n+  }\n@@ -795,0 +832,1 @@\n+\n@@ -801,1 +839,3 @@\n-  void work(uint worker_id) {\n+private:\n+  template <bool ALT_FWD>\n+  void work_impl(uint worker_id) {\n@@ -803,1 +843,1 @@\n-    ShenandoahAdjustPointersClosure cl;\n+    ShenandoahAdjustPointersClosure<ALT_FWD> cl;\n@@ -807,0 +847,9 @@\n+\n+public:\n+  void work(uint worker_id) {\n+    if (UseAltGCForwarding) {\n+      work_impl<true>(worker_id);\n+    } else {\n+      work_impl<false>(worker_id);\n+    }\n+  }\n@@ -833,0 +882,1 @@\n+template <bool ALT_FWD>\n@@ -845,1 +895,1 @@\n-    if (p->is_forwarded()) {\n+    if (SlidingForwarding::is_forwarded(p)) {\n@@ -847,1 +897,1 @@\n-      HeapWord* compact_to = cast_from_oop<HeapWord*>(p->forwardee());\n+      HeapWord* compact_to = cast_from_oop<HeapWord*>(SlidingForwarding::forwardee<ALT_FWD>(p));\n@@ -867,1 +917,3 @@\n-  void work(uint worker_id) {\n+private:\n+  template <bool ALT_FWD>\n+  void work_impl(uint worker_id) {\n@@ -871,1 +923,1 @@\n-    ShenandoahCompactObjectsClosure cl(worker_id);\n+    ShenandoahCompactObjectsClosure<ALT_FWD> cl(worker_id);\n@@ -882,0 +934,9 @@\n+\n+public:\n+  void work(uint worker_id) {\n+    if (UseAltGCForwarding) {\n+      work_impl<true>(worker_id);\n+    } else {\n+      work_impl<false>(worker_id);\n+    }\n+  }\n@@ -934,1 +995,2 @@\n-void ShenandoahFullGC::compact_humongous_objects() {\n+template <bool ALT_FWD>\n+void ShenandoahFullGC::compact_humongous_objects_impl() {\n@@ -947,1 +1009,1 @@\n-      if (!old_obj->is_forwarded()) {\n+      if (SlidingForwarding::is_not_forwarded(old_obj)) {\n@@ -956,1 +1018,1 @@\n-      size_t new_start = heap->heap_region_index_containing(old_obj->forwardee());\n+      size_t new_start = heap->heap_region_index_containing(SlidingForwarding::forwardee<ALT_FWD>(old_obj));\n@@ -998,0 +1060,8 @@\n+void ShenandoahFullGC::compact_humongous_objects() {\n+  if (UseAltGCForwarding) {\n+    compact_humongous_objects_impl<true>();\n+  } else {\n+    compact_humongous_objects_impl<false>();\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":88,"deletions":18,"binary":false,"changes":106,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+  template <bool ALT_FWD>\n@@ -86,0 +87,2 @@\n+  template <bool ALT_FWD>\n+  void calculate_target_humongous_objects_impl();\n@@ -87,0 +90,2 @@\n+  template <bool ALT_FWD>\n+  void compact_humongous_objects_impl();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -404,0 +405,2 @@\n+  SlidingForwarding::initialize(_heap_region, ShenandoahHeapRegion::region_size_words());\n+\n@@ -955,1 +958,1 @@\n-    if (!p->is_forwarded()) {\n+    if (!ShenandoahForwarding::is_forwarded(p)) {\n@@ -1303,0 +1306,1 @@\n+    shenandoah_assert_not_in_cset_except(NULL, obj, cancelled_gc());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -286,1 +286,1 @@\n-  size_t size = p->size();\n+  size_t size = p->forward_safe_size();\n@@ -503,1 +503,1 @@\n-    int size = obj->size();\n+    size_t size = obj->forward_safe_size();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -100,1 +100,1 @@\n-      if (is_instance_ref_klass(obj->klass())) {\n+      if (is_instance_ref_klass(obj->forward_safe_klass())) {\n@@ -127,1 +127,1 @@\n-    Klass* obj_klass = obj->klass_or_null();\n+    Klass* obj_klass = obj->forward_safe_klass();\n@@ -142,1 +142,1 @@\n-        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + obj->size()) <= obj_reg->top(),\n+        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + obj->forward_safe_size()) <= obj_reg->top(),\n@@ -146,1 +146,1 @@\n-        size_t humongous_end = humongous_start + (obj->size() >> ShenandoahHeapRegion::region_size_words_shift());\n+        size_t humongous_end = humongous_start + (obj->forward_safe_size() >> ShenandoahHeapRegion::region_size_words_shift());\n@@ -163,1 +163,1 @@\n-          Atomic::add(&_ld[obj_reg->index()], (uint) obj->size(), memory_order_relaxed);\n+          Atomic::add(&_ld[obj_reg->index()], (uint) obj->forward_safe_size(), memory_order_relaxed);\n@@ -204,1 +204,1 @@\n-      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + fwd->size()) <= fwd_reg->top(),\n+      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + fwd->forward_safe_size()) <= fwd_reg->top(),\n@@ -307,1 +307,2 @@\n-    obj->oop_iterate(this);\n+    Klass* klass = obj->forward_safe_klass();\n+    obj->oop_iterate_backwards(this, klass);\n@@ -587,1 +588,1 @@\n-    if (!is_instance_ref_klass(obj->klass())) {\n+    if (!is_instance_ref_klass(obj->forward_safe_klass())) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -299,1 +299,1 @@\n-        assert(!UseCompressedClassPointers, \"should only happen without compressed class pointers\");\n+        assert(!UseCompressedClassPointers || UseCompactObjectHeaders, \"should only happen without compressed class pointers or with compact object headers\");\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -46,3 +46,4 @@\n-  const size_t segment_max = ZUtils::bytes_to_words(64 * K);\n-  const size_t skip = arrayOopDesc::header_size(ArrayKlass::cast(_klass)->element_type());\n-  size_t remaining = _word_size - skip;\n+  const size_t segment_max = 64 * K;\n+  const size_t skip = arrayOopDesc::base_offset_in_bytes(ArrayKlass::cast(_klass)->element_type());\n+  size_t byte_size = _word_size * BytesPerWord;\n+  size_t remaining = byte_size - skip;\n@@ -50,0 +51,1 @@\n+  char* const start = reinterpret_cast<char*>(mem);\n@@ -53,1 +55,1 @@\n-    Copy::zero_to_words(mem + (_word_size - remaining), segment);\n+    Copy::zero_to_bytes(start + (byte_size - remaining), segment);\n","filename":"src\/hotspot\/share\/gc\/z\/zObjArrayAllocator.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -727,0 +727,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"Should call monitorenter_obj() when using the new lightweight locking\");\n@@ -744,0 +745,16 @@\n+\/\/ NOTE: We provide a separate implementation for the new lightweight locking to workaround a limitation\n+\/\/ of registers in x86_32. This entry point accepts an oop instead of a BasicObjectLock*.\n+\/\/ The problem is that we would need to preserve the register that holds the BasicObjectLock,\n+\/\/ but we are using that register to hold the thread. We don't have enough registers to\n+\/\/ also keep the BasicObjectLock, but we don't really need it anyway, we only need\n+\/\/ the object. See also InterpreterMacroAssembler::lock_object().\n+\/\/ As soon as legacy stack-locking goes away we could remove the other monitorenter() entry\n+\/\/ point, and only use oop-accepting entries (same for monitorexit() below).\n+JRT_ENTRY_NO_ASYNC(void, InterpreterRuntime::monitorenter_obj(JavaThread* current, oopDesc* obj))\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"Should call monitorenter() when not using the new lightweight locking\");\n+  Handle h_obj(current, cast_to_oop(obj));\n+  assert(Universe::heap()->is_in_or_null(h_obj()),\n+         \"must be null or an object\");\n+  ObjectSynchronizer::enter(h_obj, NULL, current);\n+  return;\n+JRT_END\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -109,0 +109,1 @@\n+  static void    monitorenter_obj(JavaThread* current, oopDesc* obj);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -630,1 +630,1 @@\n-        bool call_vm = UseHeavyMonitors;\n+        bool call_vm = (LockingMode == LM_MONITOR);\n@@ -726,1 +726,1 @@\n-      bool call_vm = UseHeavyMonitors;\n+      bool call_vm = (LockingMode == LM_MONITOR);\n@@ -1636,1 +1636,1 @@\n-          bool call_vm = UseHeavyMonitors;\n+          bool call_vm = (LockingMode == LM_MONITOR);\n@@ -1668,1 +1668,1 @@\n-            bool call_vm = UseHeavyMonitors;\n+            bool call_vm = (LockingMode == LM_MONITOR);\n@@ -1958,4 +1958,6 @@\n-              obj->set_mark(markWord::prototype());\n-              obj->set_klass_gap(0);\n-              obj->set_klass(ik);\n-\n+              if (UseCompactObjectHeaders) {\n+                oopDesc::release_set_mark(result, ik->prototype_header());\n+              } else {\n+                obj->set_mark(markWord::prototype());\n+                obj->set_klass(ik);\n+              }\n@@ -3149,1 +3151,1 @@\n-          } else if (UseHeavyMonitors) {\n+          } else if (LockingMode == LM_MONITOR) {\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":11,"deletions":9,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -25,1 +25,0 @@\n-#include \"jfr\/leakprofiler\/chains\/bitset.inline.hpp\"\n@@ -31,0 +30,1 @@\n+#include \"jfr\/leakprofiler\/chains\/jfrbitset.hpp\"\n@@ -40,1 +40,1 @@\n-BFSClosure::BFSClosure(EdgeQueue* edge_queue, EdgeStore* edge_store, BitSet* mark_bits) :\n+BFSClosure::BFSClosure(EdgeQueue* edge_queue, EdgeStore* edge_store, JFRBitSet* mark_bits) :\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/chains\/bfsClosure.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"jfr\/leakprofiler\/chains\/jfrbitset.hpp\"\n@@ -31,1 +32,0 @@\n-class BitSet;\n@@ -41,1 +41,1 @@\n-  BitSet* _mark_bits;\n+  JFRBitSet* _mark_bits;\n@@ -68,1 +68,1 @@\n-  BFSClosure(EdgeQueue* edge_queue, EdgeStore* edge_store, BitSet* mark_bits);\n+  BFSClosure(EdgeQueue* edge_queue, EdgeStore* edge_store, JFRBitSet* mark_bits);\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/chains\/bfsClosure.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1,57 +0,0 @@\n-\/*\n- * Copyright (c) 2014, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-#include \"precompiled.hpp\"\n-#include \"jfr\/leakprofiler\/chains\/bitset.inline.hpp\"\n-\n-BitSet::BitMapFragment::BitMapFragment(uintptr_t granule, BitMapFragment* next) :\n-    _bits(_bitmap_granularity_size >> LogMinObjAlignmentInBytes, mtTracing, true \/* clear *\/),\n-    _next(next) {\n-}\n-\n-BitSet::BitMapFragmentTable::~BitMapFragmentTable() {\n-  for (int index = 0; index < table_size(); index ++) {\n-    Entry* e = bucket(index);\n-    while (e != nullptr) {\n-      Entry* tmp = e;\n-      e = e->next();\n-      free_entry(tmp);\n-    }\n-  }\n-}\n-\n-BitSet::BitSet() :\n-    _bitmap_fragments(32),\n-    _fragment_list(NULL),\n-    _last_fragment_bits(NULL),\n-    _last_fragment_granule(UINTPTR_MAX) {\n-}\n-\n-BitSet::~BitSet() {\n-  BitMapFragment* current = _fragment_list;\n-  while (current != NULL) {\n-    BitMapFragment* next = current->next();\n-    delete current;\n-    current = next;\n-  }\n-}\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/chains\/bitset.cpp","additions":0,"deletions":57,"binary":false,"changes":57,"status":"deleted"},{"patch":"@@ -1,118 +0,0 @@\n-\/*\n- * Copyright (c) 2014, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_JFR_LEAKPROFILER_CHAINS_BITSET_HPP\n-#define SHARE_JFR_LEAKPROFILER_CHAINS_BITSET_HPP\n-\n-#include \"memory\/allocation.hpp\"\n-#include \"oops\/oop.hpp\"\n-#include \"oops\/oopsHierarchy.hpp\"\n-#include \"utilities\/bitMap.hpp\"\n-#include \"utilities\/hashtable.hpp\"\n-\n-class JfrVirtualMemory;\n-class MemRegion;\n-\n-class BitSet : public CHeapObj<mtTracing> {\n-  const static size_t _bitmap_granularity_shift = 26; \/\/ 64M\n-  const static size_t _bitmap_granularity_size = (size_t)1 << _bitmap_granularity_shift;\n-  const static size_t _bitmap_granularity_mask = _bitmap_granularity_size - 1;\n-\n-  class BitMapFragment;\n-\n-  class BitMapFragmentTable : public BasicHashtable<mtTracing> {\n-    class Entry : public BasicHashtableEntry<mtTracing> {\n-    public:\n-      uintptr_t _key;\n-      CHeapBitMap* _value;\n-\n-      Entry* next() {\n-        return (Entry*)BasicHashtableEntry<mtTracing>::next();\n-      }\n-    };\n-\n-  protected:\n-    Entry* bucket(int i) const;\n-\n-    Entry* new_entry(unsigned int hashValue, uintptr_t key, CHeapBitMap* value);\n-\n-    unsigned hash_segment(uintptr_t key) {\n-      unsigned hash = (unsigned)key;\n-      return hash ^ (hash >> 3);\n-    }\n-\n-    unsigned hash_to_index(unsigned hash) {\n-      return hash & (BasicHashtable<mtTracing>::table_size() - 1);\n-    }\n-\n-  public:\n-    BitMapFragmentTable(int table_size) : BasicHashtable<mtTracing>(table_size, sizeof(Entry)) {}\n-    ~BitMapFragmentTable();\n-    void add(uintptr_t key, CHeapBitMap* value);\n-    CHeapBitMap** lookup(uintptr_t key);\n-  };\n-\n-  CHeapBitMap* get_fragment_bits(uintptr_t addr);\n-\n-  BitMapFragmentTable _bitmap_fragments;\n-  BitMapFragment* _fragment_list;\n-  CHeapBitMap* _last_fragment_bits;\n-  uintptr_t _last_fragment_granule;\n-\n- public:\n-  BitSet();\n-  ~BitSet();\n-\n-  BitMap::idx_t addr_to_bit(uintptr_t addr) const;\n-\n-  void mark_obj(uintptr_t addr);\n-\n-  void mark_obj(oop obj) {\n-    return mark_obj(cast_from_oop<uintptr_t>(obj));\n-  }\n-\n-  bool is_marked(uintptr_t addr);\n-\n-  bool is_marked(oop obj) {\n-    return is_marked(cast_from_oop<uintptr_t>(obj));\n-  }\n-};\n-\n-class BitSet::BitMapFragment : public CHeapObj<mtTracing> {\n-  CHeapBitMap _bits;\n-  BitMapFragment* _next;\n-\n-public:\n-  BitMapFragment(uintptr_t granule, BitMapFragment* next);\n-\n-  BitMapFragment* next() const {\n-    return _next;\n-  }\n-\n-  CHeapBitMap* bits() {\n-    return &_bits;\n-  }\n-};\n-\n-#endif \/\/ SHARE_JFR_LEAKPROFILER_CHAINS_BITSET_HPP\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/chains\/bitset.hpp","additions":0,"deletions":118,"binary":false,"changes":118,"status":"deleted"},{"patch":"@@ -1,107 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_JFR_LEAKPROFILER_CHAINS_BITSET_INLINE_HPP\n-#define SHARE_JFR_LEAKPROFILER_CHAINS_BITSET_INLINE_HPP\n-\n-#include \"jfr\/leakprofiler\/chains\/bitset.hpp\"\n-\n-#include \"jfr\/recorder\/storage\/jfrVirtualMemory.hpp\"\n-#include \"memory\/memRegion.hpp\"\n-#include \"utilities\/bitMap.inline.hpp\"\n-#include \"utilities\/hashtable.inline.hpp\"\n-\n-inline BitSet::BitMapFragmentTable::Entry* BitSet::BitMapFragmentTable::bucket(int i) const {\n-  return (Entry*)BasicHashtable<mtTracing>::bucket(i);\n-}\n-\n-inline BitSet::BitMapFragmentTable::Entry* BitSet::BitMapFragmentTable::new_entry(unsigned int hash,\n-                                                                                  uintptr_t key,\n-                                                                                  CHeapBitMap* value) {\n-  Entry* entry = (Entry*)BasicHashtable<mtTracing>::new_entry(hash);\n-  entry->_key = key;\n-  entry->_value = value;\n-  return entry;\n-}\n-\n-inline void BitSet::BitMapFragmentTable::add(uintptr_t key, CHeapBitMap* value) {\n-  unsigned hash = hash_segment(key);\n-  Entry* entry = new_entry(hash, key, value);\n-  BasicHashtable<mtTracing>::add_entry(hash_to_index(hash), entry);\n-}\n-\n-inline CHeapBitMap** BitSet::BitMapFragmentTable::lookup(uintptr_t key) {\n-  unsigned hash = hash_segment(key);\n-  int index = hash_to_index(hash);\n-  for (Entry* e = bucket(index); e != NULL; e = e->next()) {\n-    if (e->hash() == hash && e->_key == key) {\n-      return &(e->_value);\n-    }\n-  }\n-  return NULL;\n-}\n-\n-inline BitMap::idx_t BitSet::addr_to_bit(uintptr_t addr) const {\n-  return (addr & _bitmap_granularity_mask) >> LogMinObjAlignmentInBytes;\n-}\n-\n-inline CHeapBitMap* BitSet::get_fragment_bits(uintptr_t addr) {\n-  uintptr_t granule = addr >> _bitmap_granularity_shift;\n-  if (granule == _last_fragment_granule) {\n-    return _last_fragment_bits;\n-  }\n-  CHeapBitMap* bits = NULL;\n-\n-  CHeapBitMap** found = _bitmap_fragments.lookup(granule);\n-  if (found != NULL) {\n-    bits = *found;\n-  } else {\n-    BitMapFragment* fragment = new BitMapFragment(granule, _fragment_list);\n-    bits = fragment->bits();\n-    _fragment_list = fragment;\n-    if (_bitmap_fragments.number_of_entries() * 100 \/ _bitmap_fragments.table_size() > 25) {\n-      _bitmap_fragments.resize(_bitmap_fragments.table_size() * 2);\n-    }\n-    _bitmap_fragments.add(granule, bits);\n-  }\n-\n-  _last_fragment_bits = bits;\n-  _last_fragment_granule = granule;\n-\n-  return bits;\n-}\n-\n-inline void BitSet::mark_obj(uintptr_t addr) {\n-  CHeapBitMap* bits = get_fragment_bits(addr);\n-  const BitMap::idx_t bit = addr_to_bit(addr);\n-  bits->set_bit(bit);\n-}\n-\n-inline bool BitSet::is_marked(uintptr_t addr) {\n-  CHeapBitMap* bits = get_fragment_bits(addr);\n-  const BitMap::idx_t bit = addr_to_bit(addr);\n-  return bits->at(bit);\n-}\n-\n-#endif \/\/ SHARE_JFR_LEAKPROFILER_CHAINS_BITSET_INLINE_HPP\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/chains\/bitset.inline.hpp","additions":0,"deletions":107,"binary":false,"changes":107,"status":"deleted"},{"patch":"@@ -26,1 +26,0 @@\n-#include \"jfr\/leakprofiler\/chains\/bitset.inline.hpp\"\n@@ -30,0 +29,1 @@\n+#include \"jfr\/leakprofiler\/chains\/jfrbitset.hpp\"\n@@ -43,1 +43,1 @@\n-                                      BitSet* mark_bits,\n+                                      JFRBitSet* mark_bits,\n@@ -55,1 +55,1 @@\n-                                          BitSet* mark_bits) {\n+                                          JFRBitSet* mark_bits) {\n@@ -71,1 +71,1 @@\n-DFSClosure::DFSClosure(EdgeStore* edge_store, BitSet* mark_bits, const Edge* start_edge)\n+DFSClosure::DFSClosure(EdgeStore* edge_store, JFRBitSet* mark_bits, const Edge* start_edge)\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/chains\/dfsClosure.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"jfr\/leakprofiler\/chains\/jfrbitset.hpp\"\n@@ -31,1 +32,0 @@\n-class BitSet;\n@@ -44,1 +44,1 @@\n-  BitSet* _mark_bits;\n+  JFRBitSet* _mark_bits;\n@@ -50,1 +50,1 @@\n-  DFSClosure(EdgeStore* edge_store, BitSet* mark_bits, const Edge* start_edge);\n+  DFSClosure(EdgeStore* edge_store, JFRBitSet* mark_bits, const Edge* start_edge);\n@@ -58,2 +58,2 @@\n-  static void find_leaks_from_edge(EdgeStore* edge_store, BitSet* mark_bits, const Edge* start_edge);\n-  static void find_leaks_from_root_set(EdgeStore* edge_store, BitSet* mark_bits);\n+  static void find_leaks_from_edge(EdgeStore* edge_store, JFRBitSet* mark_bits, const Edge* start_edge);\n+  static void find_leaks_from_root_set(EdgeStore* edge_store, JFRBitSet* mark_bits);\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/chains\/dfsClosure.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -0,0 +1,33 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_JFR_LEAKPROFILER_JFRBITSET_HPP\n+#define SHARE_JFR_LEAKPROFILER_JFRBITSET_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"utilities\/objectBitSet.inline.hpp\"\n+\n+typedef ObjectBitSet<mtTracing> JFRBitSet;\n+\n+#endif \/\/ SHARE_JFR_LEAKPROFILER_JFRBITSET_HPP\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/chains\/jfrbitset.hpp","additions":33,"deletions":0,"binary":false,"changes":33,"status":"added"},{"patch":"@@ -69,1 +69,2 @@\n-    _store->push(ObjectSampleMarkWord(obj, obj->mark()));\n+    markWord mark = obj->mark();\n+    _store->push(ObjectSampleMarkWord(obj, mark));\n@@ -73,1 +74,11 @@\n-    obj->set_mark(markWord::prototype().set_marked());\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      if (mark.has_displaced_mark_helper()) {\n+        mark = mark.displaced_mark_helper();\n+      }\n+      obj->set_mark(markWord::prototype().set_marked().set_narrow_klass(mark.narrow_klass()));\n+    } else\n+#endif\n+    {\n+      obj->set_mark(markWord::prototype().set_marked());\n+    }\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/chains\/objectSampleMarker.hpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"jfr\/leakprofiler\/chains\/bitset.inline.hpp\"\n@@ -35,0 +34,1 @@\n+#include \"jfr\/leakprofiler\/chains\/jfrbitset.hpp\"\n@@ -87,1 +87,1 @@\n-  BitSet mark_bits;\n+  JFRBitSet mark_bits;\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/chains\/pathToGcRootsOperation.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2120,1 +2120,1 @@\n-  return arrayOopDesc::header_size(type) * HeapWordSize;\n+  return arrayOopDesc::base_offset_in_bytes(type);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -104,0 +104,4 @@\n+    if (UseCompactObjectHeaders) {\n+      log_warning(jvmci)(\"-XX:+UseCompactObjectHeaders not supported by JVMCI, disabling UseCompactObjectHeaders\");\n+      FLAG_SET_DEFAULT(UseCompactObjectHeaders, false);\n+    }\n","filename":"src\/hotspot\/share\/jvmci\/jvmci_globals.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -53,2 +53,1 @@\n-    size_t hs = align_up(length_offset_in_bytes() + sizeof(int),\n-                              HeapWordSize);\n+    size_t hs = length_offset_in_bytes() + sizeof(int);\n@@ -74,0 +73,5 @@\n+#ifdef _LP64\n+    if (type == T_OBJECT || type == T_ARRAY) {\n+      return !UseCompressedOops;\n+    }\n+#endif\n@@ -82,2 +86,1 @@\n-    return UseCompressedClassPointers ? klass_gap_offset_in_bytes() :\n-                               sizeof(arrayOopDesc);\n+    return oopDesc::base_offset_in_bytes();\n@@ -88,1 +91,4 @@\n-    return header_size(type) * HeapWordSize;\n+    size_t typesize_in_bytes = header_size_in_bytes();\n+    return (int)(element_type_should_be_aligned(type)\n+                 ? align_up(typesize_in_bytes, BytesPerLong)\n+                 : typesize_in_bytes);\n@@ -130,0 +136,1 @@\n+    assert(!UseCompactObjectHeaders, \"Don't use this with compact headers\");\n@@ -144,4 +151,2 @@\n-    const size_t max_element_words_per_size_t =\n-      align_down((SIZE_MAX\/HeapWordSize - header_size(type)), MinObjAlignment);\n-    const size_t max_elements_per_size_t =\n-      HeapWordSize * max_element_words_per_size_t \/ type2aelembytes(type);\n+    const size_t max_size_bytes = align_down(SIZE_MAX - base_offset_in_bytes(type), MinObjAlignmentInBytes);\n+    const size_t max_elements_per_size_t = max_size_bytes \/ type2aelembytes(type);\n@@ -153,1 +158,2 @@\n-      return align_down(max_jint - header_size(type), MinObjAlignment);\n+      int header_size_words = align_up(base_offset_in_bytes(type), HeapWordSize) \/ HeapWordSize;\n+      return align_down(max_jint - header_size_words, MinObjAlignment);\n","filename":"src\/hotspot\/share\/oops\/arrayOop.hpp","additions":16,"deletions":10,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2591,1 +2591,7 @@\n-    set_prototype_header(markWord::prototype());\n+    markWord prototype = markWord::prototype();\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      prototype = prototype.set_klass(this);\n+    }\n+#endif\n+    set_prototype_header(prototype);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -35,3 +35,0 @@\n-  \/\/ aligned header size.\n-  static int header_size() { return sizeof(instanceOopDesc)\/HeapWordSize; }\n-\n@@ -40,4 +37,1 @@\n-    return (UseCompressedClassPointers) ?\n-            klass_gap_offset_in_bytes() :\n-            sizeof(instanceOopDesc);\n-\n+    return oopDesc::base_offset_in_bytes();\n","filename":"src\/hotspot\/share\/oops\/instanceOop.hpp","additions":1,"deletions":7,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -129,1 +129,1 @@\n-  static void trace_reference_gc(const char *s, oop obj) NOT_DEBUG_RETURN;\n+  void trace_reference_gc(const char *s, oop obj) NOT_DEBUG_RETURN;\n","filename":"src\/hotspot\/share\/oops\/instanceRefKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -188,1 +188,1 @@\n-  if (java_lang_ref_Reference::is_phantom(obj)) {\n+  if (reference_type() == REF_PHANTOM) {\n","filename":"src\/hotspot\/share\/oops\/instanceRefKlass.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -196,0 +196,10 @@\n+static markWord make_prototype(Klass* kls) {\n+  markWord prototype = markWord::prototype();\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    prototype = prototype.set_klass(kls);\n+  }\n+#endif\n+  return prototype;\n+}\n+\n@@ -201,1 +211,1 @@\n-                           _prototype_header(markWord::prototype()),\n+                           _prototype_header(make_prototype(this)),\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -55,1 +55,1 @@\n-  assert(!header.has_bias_pattern() || is_instance_klass(), \"biased locking currently only supported for Java instances\");\n+  assert(UseCompactObjectHeaders || !header.has_bias_pattern() || is_instance_klass(), \"biased locking currently only supported for Java instances\");\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -47,0 +48,4 @@\n+\/\/  64 bits (with compact headers):\n+\/\/  -------------------------------\n+\/\/  nklass:32 hash:25 -->| unused_gap:1  age:4  self-fwded:1  lock:2 (normal object)\n+\/\/\n@@ -84,1 +89,1 @@\n-\/\/    [ptr             | 10]  monitor            inflated lock (header is wapped out)\n+\/\/    [ptr             | 10]  monitor            inflated lock (header is swapped out)\n@@ -86,1 +91,1 @@\n-\/\/    [0 ............ 0| 00]  inflating          inflation in progress\n+\/\/    [0 ............ 0| 00]  inflating          inflation in progress (stack-locking in use)\n@@ -97,0 +102,1 @@\n+class Klass;\n@@ -132,1 +138,2 @@\n-  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - biased_lock_bits;\n+  static const int self_forwarded_bits            = 1;\n+  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - self_forwarded_bits;\n@@ -134,0 +141,2 @@\n+  static const int hash_bits_compact              = max_hash_bits > 25 ? 25 : max_hash_bits;\n+  \/\/ Used only without compact headers.\n@@ -136,0 +145,4 @@\n+#ifdef _LP64\n+  \/\/ Used only with compact headers.\n+  static const int klass_bits                     = 32;\n+#endif\n@@ -141,1 +154,2 @@\n-  static const int age_shift                      = lock_bits + biased_lock_bits;\n+  static const int self_forwarded_shift           = lock_shift + lock_bits;\n+  static const int age_shift                      = self_forwarded_shift + self_forwarded_bits;\n@@ -144,0 +158,5 @@\n+  static const int hash_shift_compact             = age_shift + age_bits;\n+#ifdef _LP64\n+  \/\/ Used only with compact headers.\n+  static const int klass_shift                    = hash_shift_compact + hash_bits_compact;\n+#endif\n@@ -151,0 +170,2 @@\n+  static const uintptr_t self_forwarded_mask      = right_n_bits(self_forwarded_bits);\n+  static const uintptr_t self_forwarded_mask_in_place = self_forwarded_mask << self_forwarded_shift;\n@@ -158,0 +179,7 @@\n+  static const uintptr_t hash_mask_compact        = right_n_bits(hash_bits_compact);\n+  static const uintptr_t hash_mask_compact_in_place = hash_mask_compact << hash_shift_compact;\n+\n+#ifdef _LP64\n+  static const uintptr_t klass_mask               = right_n_bits(klass_bits);\n+  static const uintptr_t klass_mask_in_place      = klass_mask << klass_shift;\n+#endif\n@@ -239,0 +267,1 @@\n+  \/\/ Fast-locking does not use INFLATING.\n@@ -270,1 +299,2 @@\n-    return ((value() & lock_mask_in_place) == locked_value);\n+    assert(LockingMode == LM_LEGACY, \"should only be called with legacy stack locking\");\n+    return (value() & lock_mask_in_place) == locked_value;\n@@ -276,0 +306,9 @@\n+\n+  bool is_fast_locked() const {\n+    assert(LockingMode == LM_LIGHTWEIGHT, \"should only be called with new lightweight locking\");\n+    return (value() & lock_mask_in_place) == locked_value;\n+  }\n+  markWord set_fast_locked() const {\n+    return markWord(value() & ~lock_mask_in_place);\n+  }\n+\n@@ -285,1 +324,3 @@\n-    return ((value() & unlocked_value) == 0);\n+    intptr_t lockbits = value() & lock_mask_in_place;\n+    return LockingMode == LM_LIGHTWEIGHT  ? lockbits == monitor_value   \/\/ monitor?\n+                                          : (lockbits & unlocked_value) == 0; \/\/ monitor | stack-locked?\n@@ -290,3 +331,9 @@\n-    uintptr_t tmp = value() & (~hash_mask_in_place);\n-    tmp |= ((hash & hash_mask) << hash_shift);\n-    return markWord(tmp);\n+    if (UseCompactObjectHeaders) {\n+      uintptr_t tmp = value() & (~hash_mask_compact_in_place);\n+      tmp |= ((hash & hash_mask_compact) << hash_shift_compact);\n+      return markWord(tmp);\n+    } else {\n+      uintptr_t tmp = value() & (~hash_mask_in_place);\n+      tmp |= ((hash & hash_mask) << hash_shift);\n+      return markWord(tmp);\n+    }\n@@ -332,1 +379,5 @@\n-    return mask_bits(value() >> hash_shift, hash_mask);\n+    if (UseCompactObjectHeaders) {\n+      return mask_bits(value() >> hash_shift_compact, hash_mask_compact);\n+    } else {\n+      return mask_bits(value() >> hash_shift, hash_mask);\n+    }\n@@ -339,0 +390,10 @@\n+#ifdef _LP64\n+  inline markWord actual_mark() const;\n+  inline Klass* klass() const;\n+  inline Klass* klass_or_null() const;\n+  inline Klass* safe_klass() const;\n+  inline markWord set_klass(const Klass* klass) const;\n+  inline narrowKlass narrow_klass() const;\n+  inline markWord set_narrow_klass(const narrowKlass klass) const;\n+#endif\n+\n@@ -355,0 +416,13 @@\n+\n+#ifdef _LP64\n+  inline bool self_forwarded() const {\n+    bool self_fwd = mask_bits(value(), self_forwarded_mask_in_place) != 0;\n+    assert(!self_fwd || UseAltGCForwarding, \"Only set self-fwd bit when using alt GC forwarding\");\n+    return self_fwd;\n+  }\n+\n+  inline markWord set_self_forwarded() const {\n+    assert(UseAltGCForwarding, \"Only call this with alt GC forwarding\");\n+    return markWord(value() | self_forwarded_mask_in_place | marked_value);\n+  }\n+#endif\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":84,"deletions":10,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/safepoint.hpp\"\n@@ -73,1 +74,1 @@\n-  assert(prototype_header == prototype() || prototype_header.has_bias_pattern(), \"corrupt prototype header\");\n+  assert(UseCompactObjectHeaders || prototype_header == prototype() || prototype_header.has_bias_pattern(), \"corrupt prototype header\");\n@@ -78,0 +79,50 @@\n+#ifdef _LP64\n+markWord markWord::actual_mark() const {\n+  assert(UseCompactObjectHeaders, \"only safe when using compact headers\");\n+  if (has_displaced_mark_helper()) {\n+    return displaced_mark_helper();\n+  } else {\n+    return *this;\n+  }\n+}\n+\n+narrowKlass markWord::narrow_klass() const {\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  return narrowKlass(value() >> klass_shift);\n+}\n+\n+Klass* markWord::klass() const {\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  assert(!CompressedKlassPointers::is_null(narrow_klass()), \"narrow klass must not be null: \" INTPTR_FORMAT, value());\n+  return CompressedKlassPointers::decode_not_null(narrow_klass());\n+}\n+\n+Klass* markWord::klass_or_null() const {\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  return CompressedKlassPointers::decode(narrow_klass());\n+}\n+\n+markWord markWord::set_narrow_klass(const narrowKlass nklass) const {\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  return markWord((value() & ~klass_mask_in_place) | ((uintptr_t) nklass << klass_shift));\n+}\n+\n+Klass* markWord::safe_klass() const {\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  assert(SafepointSynchronize::is_at_safepoint(), \"only call at safepoint\");\n+  markWord m = *this;\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  return CompressedKlassPointers::decode_not_null(m.narrow_klass());\n+}\n+\n+markWord markWord::set_klass(const Klass* klass) const {\n+  assert(UseCompactObjectHeaders, \"only used with compact object headers\");\n+  assert(UseCompressedClassPointers, \"expect compressed klass pointers\");\n+  \/\/ TODO: Don't cast to non-const, change CKP::encode() to accept const Klass* instead.\n+  narrowKlass nklass = CompressedKlassPointers::encode(const_cast<Klass*>(klass));\n+  return set_narrow_klass(nklass);\n+}\n+#endif\n+\n","filename":"src\/hotspot\/share\/oops\/markWord.inline.hpp","additions":52,"deletions":1,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -159,1 +159,1 @@\n-  assert(obj->is_objArray(), \"must be object array\");\n+  assert(UseCompactObjectHeaders || obj->is_objArray(), \"must be object array\");\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -73,1 +73,1 @@\n-  assert (obj->is_array(), \"obj must be array\");\n+  assert (UseCompactObjectHeaders || obj->is_array(), \"obj must be array\");\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -50,23 +50,3 @@\n-  \/\/ Give size of objArrayOop in HeapWords minus the header\n-  static int array_size(int length) {\n-    const uint OopsPerHeapWord = HeapWordSize\/heapOopSize;\n-    assert(OopsPerHeapWord >= 1 && (HeapWordSize % heapOopSize == 0),\n-           \"Else the following (new) computation would be in error\");\n-    uint res = ((uint)length + OopsPerHeapWord - 1)\/OopsPerHeapWord;\n-#ifdef ASSERT\n-    \/\/ The old code is left in for sanity-checking; it'll\n-    \/\/ go away pretty soon. XXX\n-    \/\/ Without UseCompressedOops, this is simply:\n-    \/\/ oop->length() * HeapWordsPerOop;\n-    \/\/ With narrowOops, HeapWordsPerOop is 1\/2 or equal 0 as an integer.\n-    \/\/ The oop elements are aligned up to wordSize\n-    const uint HeapWordsPerOop = heapOopSize\/HeapWordSize;\n-    uint old_res;\n-    if (HeapWordsPerOop > 0) {\n-      old_res = length * HeapWordsPerOop;\n-    } else {\n-      old_res = align_up((uint)length, OopsPerHeapWord)\/OopsPerHeapWord;\n-    }\n-    assert(res == old_res, \"Inconsistency between old and new.\");\n-#endif  \/\/ ASSERT\n-    return res;\n+  \/\/ Give size of objArrayOop in bytes minus the header\n+  static size_t array_size_in_bytes(int length) {\n+    return (size_t)length * heapOopSize;\n@@ -92,1 +72,0 @@\n-  static int header_size()    { return arrayOopDesc::header_size(T_OBJECT); }\n@@ -97,5 +76,5 @@\n-    uint asz = array_size(length);\n-    uint osz = align_object_size(header_size() + asz);\n-    assert(osz >= asz,   \"no overflow\");\n-    assert((int)osz > 0, \"no overflow\");\n-    return (int)osz;\n+    size_t asz = array_size_in_bytes(length);\n+    size_t size_words = align_up(base_offset_in_bytes() + asz, HeapWordSize) \/ HeapWordSize;\n+    size_t osz = align_object_size(size_words);\n+    assert(osz < max_jint, \"no overflow\");\n+    return checked_cast<int>(osz);\n","filename":"src\/hotspot\/share\/oops\/objArrayOop.hpp","additions":8,"deletions":29,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -108,1 +108,1 @@\n-  \/\/ at a safepoint, it must not be zero.\n+  \/\/ at a safepoint, it must not be zero, except when using the new lightweight locking.\n@@ -117,1 +117,1 @@\n-  return !SafepointSynchronize::is_at_safepoint();\n+  return LockingMode == LM_LIGHTWEIGHT || !SafepointSynchronize::is_at_safepoint();\n@@ -142,2 +142,3 @@\n-  \/\/ Only has a klass gap when compressed class pointers are used.\n-  return UseCompressedClassPointers;\n+  \/\/ Only has a klass gap when compressed class pointers are used, but\n+  \/\/ only if not using compact headers..\n+  return UseCompressedClassPointers && !UseCompactObjectHeaders;\n@@ -150,0 +151,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -155,7 +157,8 @@\n-  if (UseCompressedClassPointers) {\n-    narrowKlass narrow_klass = obj->_metadata._compressed_klass;\n-    if (narrow_klass == 0) return NULL;\n-    return (void*)CompressedKlassPointers::decode_raw(narrow_klass);\n-  } else {\n-    return obj->_metadata._klass;\n-  }\n+  \/\/ TODO: Remove method altogether and replace with calls to obj->klass() ?\n+  \/\/ OTOH, we may eventually get rid of locking in header, and then no\n+  \/\/ longer have to deal with that anymore.\n+#ifdef _LP64\n+  return obj->klass();\n+#else\n+  return obj->_metadata._klass;\n+#endif\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":14,"deletions":11,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -60,1 +60,1 @@\n- public:\n+public:\n@@ -69,0 +69,1 @@\n+  static inline void release_set_mark(HeapWord* mem, markWord m);\n@@ -72,0 +73,5 @@\n+  inline markWord resolve_mark() const;\n+\n+  \/\/ Returns the prototype mark that should be used for this object.\n+  inline markWord prototype_mark() const;\n+\n@@ -90,1 +96,8 @@\n-  static int header_size() { return sizeof(oopDesc)\/HeapWordSize; }\n+  static int header_size() {\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      return sizeof(markWord) \/ HeapWordSize;\n+    } else\n+#endif\n+    return sizeof(oopDesc)\/HeapWordSize;\n+  }\n@@ -102,0 +115,14 @@\n+  \/\/ The following set of methods is used to access the mark-word and related\n+  \/\/ properties when the object may be forwarded. Be careful where and when\n+  \/\/ using this method. It assumes that the forwardee is installed in\n+  \/\/ the header as a plain pointer (or self-forwarded). In particular,\n+  \/\/ those methods can not deal with the sliding-forwarding that is used\n+  \/\/ in Serial, G1 and Shenandoah full-GCs.\n+private:\n+  inline Klass*   forward_safe_klass_impl(markWord m) const;\n+public:\n+  inline Klass*   forward_safe_klass() const;\n+  inline size_t   forward_safe_size();\n+  inline Klass*   forward_safe_klass(markWord m) const;\n+  inline void     forward_safe_init_mark();\n+\n@@ -252,1 +279,1 @@\n-  inline bool cas_forward_to(oop p, markWord compare, atomic_memory_order order = memory_order_conservative);\n+  inline void forward_to_self();\n@@ -259,0 +286,1 @@\n+  inline oop forward_to_self_atomic(markWord compare, atomic_memory_order order = memory_order_conservative);\n@@ -261,0 +289,1 @@\n+  inline oop forwardee(markWord header) const;\n@@ -306,1 +335,0 @@\n-  static int klass_offset_in_bytes()     { return offset_of(oopDesc, _metadata._klass); }\n@@ -309,0 +337,1 @@\n+    assert(!UseCompactObjectHeaders, \"don't use klass_offset_in_bytes() with compact headers\");\n@@ -312,0 +341,24 @@\n+  static int klass_offset_in_bytes()     {\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      STATIC_ASSERT(markWord::klass_shift % 8 == 0);\n+      return mark_offset_in_bytes() + markWord::klass_shift \/ 8;\n+    } else\n+#endif\n+    return offset_of(oopDesc, _metadata._klass);\n+  }\n+\n+  static int base_offset_in_bytes() {\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      \/\/ With compact headers, the Klass* field is not used for the Klass*\n+      \/\/ and is used for the object fields instead.\n+      assert(sizeof(markWord) == 8, \"sanity\");\n+      return sizeof(markWord);\n+    } else if (UseCompressedClassPointers) {\n+      return sizeof(markWord) + sizeof(narrowKlass);\n+    } else\n+#endif\n+    return sizeof(oopDesc);\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":57,"deletions":4,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -39,0 +39,2 @@\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/objectMonitor.inline.hpp\"\n@@ -54,1 +56,0 @@\n-\n@@ -71,0 +72,4 @@\n+void oopDesc::release_set_mark(HeapWord* mem, markWord m) {\n+  Atomic::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n+}\n+\n@@ -79,0 +84,17 @@\n+markWord oopDesc::resolve_mark() const {\n+  assert(LockingMode != LM_LEGACY, \"Not safe with legacy stack-locking\");\n+  markWord hdr = mark();\n+  if (hdr.has_displaced_mark_helper()) {\n+    hdr = hdr.displaced_mark_helper();\n+  }\n+  return hdr;\n+}\n+\n+markWord oopDesc::prototype_mark() const {\n+  if (UseCompactObjectHeaders) {\n+    return klass()->prototype_header();\n+  } else {\n+    return markWord::prototype();\n+  }\n+}\n+\n@@ -84,1 +106,6 @@\n-  if (UseCompressedClassPointers) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+    markWord header = resolve_mark();\n+    return header.klass();\n+  } else if (UseCompressedClassPointers) {\n@@ -86,3 +113,3 @@\n-  } else {\n-    return _metadata._klass;\n-  }\n+  } else\n+#endif\n+  return _metadata._klass;\n@@ -92,1 +119,6 @@\n-  if (UseCompressedClassPointers) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+    markWord header = resolve_mark();\n+    return header.klass_or_null();\n+  } else if (UseCompressedClassPointers) {\n@@ -94,3 +126,3 @@\n-  } else {\n-    return _metadata._klass;\n-  }\n+  } else\n+#endif\n+  return _metadata._klass;\n@@ -100,6 +132,14 @@\n-  if (UseCompressedClassPointers) {\n-    narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n-    return CompressedKlassPointers::decode(nklass);\n-  } else {\n-    return Atomic::load_acquire(&_metadata._klass);\n-  }\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+    markWord header = mark_acquire();\n+    if (header.has_monitor()) {\n+      header = header.monitor()->header();\n+    }\n+    return header.klass_or_null();\n+  } else if (UseCompressedClassPointers) {\n+     narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n+     return CompressedKlassPointers::decode(nklass);\n+  } else\n+#endif\n+  return Atomic::load_acquire(&_metadata._klass);\n@@ -110,0 +150,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* with compact headers\");\n@@ -119,0 +160,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* with compact headers\");\n@@ -129,0 +171,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't get Klass* gap with compact headers\");\n@@ -133,0 +176,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* gap with compact headers\");\n@@ -139,0 +183,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* gap with compact headers\");\n@@ -206,0 +251,47 @@\n+#ifdef _LP64\n+Klass* oopDesc::forward_safe_klass_impl(markWord m) const {\n+  assert(UseCompactObjectHeaders, \"Only get here with compact headers\");\n+  if (m.is_marked()) {\n+    oop fwd = forwardee(m);\n+    markWord m2 = fwd->mark();\n+    assert(!m2.is_marked() || m2.self_forwarded(), \"no double forwarding: this: \" PTR_FORMAT \" (\" INTPTR_FORMAT \"), fwd: \" PTR_FORMAT \" (\" INTPTR_FORMAT \")\", p2i(this), m.value(), p2i(fwd), m2.value());\n+    m = m2;\n+  }\n+  return m.actual_mark().klass();\n+}\n+#endif\n+\n+Klass* oopDesc::forward_safe_klass(markWord m) const {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    return forward_safe_klass_impl(m);\n+  } else\n+#endif\n+  {\n+    return klass();\n+  }\n+}\n+\n+Klass* oopDesc::forward_safe_klass() const {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    return forward_safe_klass_impl(mark());\n+  } else\n+#endif\n+  {\n+    return klass();\n+  }\n+}\n+\n+size_t oopDesc::forward_safe_size() {\n+  return size_given_klass(forward_safe_klass());\n+}\n+\n+void oopDesc::forward_safe_init_mark() {\n+  if (UseCompactObjectHeaders) {\n+    set_mark(forward_safe_klass()->prototype_header());\n+  } else {\n+    init_mark();\n+  }\n+}\n+\n@@ -277,0 +369,1 @@\n+  assert(p != cast_to_oop(this) || !UseAltGCForwarding, \"Must not be called with self-forwarding\");\n@@ -279,1 +372,1 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversable\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n@@ -283,6 +376,18 @@\n-\/\/ Used by parallel scavengers\n-bool oopDesc::cas_forward_to(oop p, markWord compare, atomic_memory_order order) {\n-  verify_forwardee(p);\n-  markWord m = markWord::encode_pointer_as_mark(p);\n-  assert(m.decode_pointer() == p, \"encoding must be reversable\");\n-  return cas_set_mark(m, compare, order) == compare;\n+void oopDesc::forward_to_self() {\n+#ifdef _LP64\n+  if (UseAltGCForwarding) {\n+    markWord m = mark();\n+    \/\/ If mark is displaced, we need to preserve the real header during GC.\n+    \/\/ It will be restored to the displaced header after GC.\n+    assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+    if (m.has_displaced_mark_helper()) {\n+      m = m.displaced_mark_helper();\n+    }\n+    m = m.set_self_forwarded();\n+    assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversible\");\n+    set_mark(m);\n+  } else\n+#endif\n+  {\n+    forward_to(oop(this));\n+  }\n@@ -292,0 +397,1 @@\n+  assert(p != cast_to_oop(this) || !UseAltGCForwarding, \"Must not be called with self-forwarding\");\n@@ -294,1 +400,1 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversable\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n@@ -299,1 +405,39 @@\n-    return cast_to_oop(old_mark.decode_pointer());\n+    return forwardee(old_mark);\n+  }\n+}\n+\n+oop oopDesc::forward_to_self_atomic(markWord compare, atomic_memory_order order) {\n+#ifdef _LP64\n+  if (UseAltGCForwarding) {\n+   markWord m = compare;\n+    \/\/ If mark is displaced, we need to preserve the real header during GC.\n+    \/\/ It will be restored to the displaced header after GC.\n+    assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+    if (m.has_displaced_mark_helper()) {\n+      m = m.displaced_mark_helper();\n+    }\n+    m = m.set_self_forwarded();\n+    assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversible\");\n+    markWord old_mark = cas_set_mark(m, compare, order);\n+    if (old_mark == compare) {\n+      return nullptr;\n+    } else {\n+      assert(old_mark.is_marked(), \"must be marked here\");\n+      return forwardee(old_mark);\n+    }\n+  } else\n+#endif\n+  {\n+    return forward_to_atomic(cast_to_oop(this), compare, order);\n+  }\n+}\n+\n+oop oopDesc::forwardee(markWord header) const {\n+  assert(header.is_marked(), \"only decode when actually forwarded\");\n+#ifdef _LP64\n+  if (header.self_forwarded()) {\n+    return cast_to_oop(this);\n+  } else\n+#endif\n+  {\n+    return cast_to_oop(header.decode_pointer());\n@@ -307,1 +451,1 @@\n-  return cast_to_oop(mark().decode_pointer());\n+  return forwardee(mark());\n@@ -362,1 +506,1 @@\n-  assert(k == klass(), \"wrong klass\");\n+  assert(UseCompactObjectHeaders || k == klass(), \"wrong klass\");\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":170,"deletions":26,"binary":false,"changes":196,"status":"modified"},{"patch":"@@ -231,1 +231,1 @@\n-  assert(obj->is_typeArray(),\"must be a type array\");\n+  assert(UseCompactObjectHeaders || obj->is_typeArray(),\"must be a type array\");\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-  assert(obj->is_typeArray(),\"must be a type array\");\n+  assert(UseCompactObjectHeaders || obj->is_typeArray(),\"must be a type array\");\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,55 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/codeBuffer.hpp\"\n+#include \"code\/codeBlob.hpp\"\n+#include \"opto\/c2_CodeStubs.hpp\"\n+#include \"opto\/c2_MacroAssembler.hpp\"\n+#include \"opto\/compile.hpp\"\n+#include \"opto\/output.hpp\"\n+\n+C2CodeStubList::C2CodeStubList() :\n+    _stubs(Compile::current()->comp_arena(), 2, 0, NULL) {}\n+\n+void C2CodeStubList::emit(CodeBuffer& cb) {\n+  C2_MacroAssembler masm(&cb);\n+  for (int i = _stubs.length() - 1; i >= 0; i--) {\n+    C2CodeStub* stub = _stubs.at(i);\n+    int max_size = stub->max_size();\n+    \/\/ Make sure there is enough space in the code buffer\n+    if (cb.insts()->maybe_expand_to_ensure_remaining(max_size) && cb.blob() == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n+\n+    DEBUG_ONLY(int size_before = cb.insts_size();)\n+\n+    stub->emit(masm);\n+\n+    DEBUG_ONLY(int actual_size = cb.insts_size() - size_before;)\n+    assert(max_size >= actual_size, \"Expected stub size (%d) must be larger than or equal to actual stub size (%d)\", max_size, actual_size);\n+  }\n+  _stubs.clear();\n+}\n","filename":"src\/hotspot\/share\/opto\/c2_CodeStubs.cpp","additions":55,"deletions":0,"binary":false,"changes":55,"status":"added"},{"patch":"@@ -0,0 +1,106 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"asm\/assembler.hpp\"\n+#include \"asm\/codeBuffer.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"opto\/c2_MacroAssembler.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+\n+#ifndef SHARE_OPTO_C2_CODESTUBS_HPP\n+#define SHARE_OPTO_C2_CODESTUBS_HPP\n+\n+class C2CodeStub : public ResourceObj {\n+private:\n+  Label _entry;\n+  Label _continuation;\n+\n+protected:\n+  C2CodeStub() :\n+    _entry(),\n+    _continuation() {}\n+  ~C2CodeStub() {}\n+\n+public:\n+  Label& entry()        { return _entry; }\n+  Label& continuation() { return _continuation; }\n+\n+  virtual void emit(C2_MacroAssembler& masm) = 0;\n+  virtual int max_size() const = 0;\n+};\n+\n+class C2CodeStubList {\n+private:\n+  GrowableArray<C2CodeStub*> _stubs;\n+\n+public:\n+  C2CodeStubList();\n+  ~C2CodeStubList() {}\n+  void add_stub(C2CodeStub* stub) { _stubs.append(stub); }\n+  void emit(CodeBuffer& cb);\n+};\n+\n+class C2SafepointPollStub : public C2CodeStub {\n+private:\n+  uintptr_t _safepoint_offset;\n+public:\n+  C2SafepointPollStub(uintptr_t safepoint_offset) :\n+    _safepoint_offset(safepoint_offset) {}\n+  int max_size() const;\n+  void emit(C2_MacroAssembler& masm);\n+};\n+\n+class C2CheckLockStackStub : public C2CodeStub {\n+public:\n+  C2CheckLockStackStub() : C2CodeStub() {}\n+  int max_size() const;\n+  void emit(C2_MacroAssembler& masm);\n+};\n+\n+#ifdef _LP64\n+class C2HandleAnonOMOwnerStub : public C2CodeStub {\n+private:\n+  Register _monitor;\n+  Register _tmp;\n+public:\n+  C2HandleAnonOMOwnerStub(Register monitor, Register tmp = noreg) : C2CodeStub(),\n+    _monitor(monitor), _tmp(tmp) {}\n+  Register monitor() { return _monitor; }\n+  Register tmp() { return _tmp; }\n+  int max_size() const;\n+  void emit(C2_MacroAssembler& masm);\n+};\n+\n+class C2LoadNKlassStub : public C2CodeStub {\n+private:\n+  Register _dst;\n+public:\n+  C2LoadNKlassStub(Register dst) : C2CodeStub(), _dst(dst) {}\n+  Register dst() { return _dst; }\n+  int max_size() const;\n+  void emit(C2_MacroAssembler& masm);\n+};\n+#endif\n+\n+#endif \/\/ SHARE_OPTO_C2_CODESTUBS_HPP\n","filename":"src\/hotspot\/share\/opto\/c2_CodeStubs.hpp","additions":106,"deletions":0,"binary":false,"changes":106,"status":"added"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n","filename":"src\/hotspot\/share\/opto\/c2_MacroAssembler.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1680,1 +1680,1 @@\n-  if (UseBiasedLocking && Opcode() == Op_Allocate) {\n+  if ((UseBiasedLocking && Opcode() == Op_Allocate) || UseCompactObjectHeaders) {\n@@ -1685,0 +1685,1 @@\n+    \/\/ For now only enable fast locking for non-array types\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1650,0 +1650,4 @@\n+      if (UseCompactObjectHeaders) {\n+        if (flat->offset() == in_bytes(Klass::prototype_header_offset()))\n+          alias_type(idx)->set_rewritable(false);\n+      }\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3859,2 +3859,2 @@\n-  Node *hash_mask      = _gvn.intcon(markWord::hash_mask);\n-  Node *hash_shift     = _gvn.intcon(markWord::hash_shift);\n+  Node *hash_mask      = _gvn.intcon(UseCompactObjectHeaders ? markWord::hash_mask_compact  : markWord::hash_mask);\n+  Node *hash_shift     = _gvn.intcon(UseCompactObjectHeaders ? markWord::hash_shift_compact : markWord::hash_shift);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1680,1 +1680,4 @@\n-  rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+  if (!UseCompactObjectHeaders) {\n+    rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1850,0 +1850,7 @@\n+  if (UseCompactObjectHeaders) {\n+    if (tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n+      \/\/ The field is Klass::_prototype_header.  Return its (constant) value.\n+      assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n+      return TypeX::make(klass->prototype_header());\n+    }\n+  }\n@@ -2020,0 +2027,7 @@\n+      if (UseCompactObjectHeaders) {\n+        if (tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n+          \/\/ The field is Klass::_prototype_header. Return its (constant) value.\n+          assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n+          return TypeX::make(klass->prototype_header());\n+        }\n+      }\n@@ -2104,1 +2118,1 @@\n-  if (alloc != nullptr && !(alloc->Opcode() == Op_Allocate && UseBiasedLocking)) {\n+  if (alloc != nullptr && !(alloc->Opcode() == Op_Allocate && UseBiasedLocking) && !UseCompactObjectHeaders) {\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -228,67 +228,0 @@\n-volatile int C2SafepointPollStubTable::_stub_size = 0;\n-\n-Label& C2SafepointPollStubTable::add_safepoint(uintptr_t safepoint_offset) {\n-  C2SafepointPollStub* entry = new (Compile::current()->comp_arena()) C2SafepointPollStub(safepoint_offset);\n-  _safepoints.append(entry);\n-  return entry->_stub_label;\n-}\n-\n-void C2SafepointPollStubTable::emit(CodeBuffer& cb) {\n-  MacroAssembler masm(&cb);\n-  for (int i = _safepoints.length() - 1; i >= 0; i--) {\n-    \/\/ Make sure there is enough space in the code buffer\n-    if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == NULL) {\n-      ciEnv::current()->record_failure(\"CodeCache is full\");\n-      return;\n-    }\n-\n-    C2SafepointPollStub* entry = _safepoints.at(i);\n-    emit_stub(masm, entry);\n-  }\n-}\n-\n-int C2SafepointPollStubTable::stub_size_lazy() const {\n-  int size = Atomic::load(&_stub_size);\n-\n-  if (size != 0) {\n-    return size;\n-  }\n-\n-  Compile* const C = Compile::current();\n-  BufferBlob* const blob = C->output()->scratch_buffer_blob();\n-  CodeBuffer cb(blob->content_begin(), C->output()->scratch_buffer_code_size());\n-  MacroAssembler masm(&cb);\n-  C2SafepointPollStub* entry = _safepoints.at(0);\n-  emit_stub(masm, entry);\n-  size += cb.insts_size();\n-\n-  Atomic::store(&_stub_size, size);\n-\n-  return size;\n-}\n-\n-int C2SafepointPollStubTable::estimate_stub_size() const {\n-  if (_safepoints.length() == 0) {\n-    return 0;\n-  }\n-\n-  int result = stub_size_lazy() * _safepoints.length();\n-\n-#ifdef ASSERT\n-  Compile* const C = Compile::current();\n-  BufferBlob* const blob = C->output()->scratch_buffer_blob();\n-  int size = 0;\n-\n-  for (int i = _safepoints.length() - 1; i >= 0; i--) {\n-    CodeBuffer cb(blob->content_begin(), C->output()->scratch_buffer_code_size());\n-    MacroAssembler masm(&cb);\n-    C2SafepointPollStub* entry = _safepoints.at(i);\n-    emit_stub(masm, entry);\n-    size += cb.insts_size();\n-  }\n-  assert(size == result, \"stubs should not have variable size\");\n-#endif\n-\n-  return result;\n-}\n-\n@@ -301,0 +234,1 @@\n+    _stub_list(),\n@@ -1316,1 +1250,0 @@\n-  stub_req += safepoint_poll_table()->estimate_stub_size();\n@@ -1823,2 +1756,2 @@\n-  \/\/ Fill in stubs for calling the runtime from safepoint polls.\n-  safepoint_poll_table()->emit(*cb);\n+  \/\/ Fill in stubs.\n+  _stub_list.emit(*cb);\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":3,"deletions":70,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"opto\/c2_CodeStubs.hpp\"\n@@ -75,41 +76,0 @@\n-class C2SafepointPollStubTable {\n-private:\n-  struct C2SafepointPollStub: public ResourceObj {\n-    uintptr_t _safepoint_offset;\n-    Label     _stub_label;\n-    Label     _trampoline_label;\n-    C2SafepointPollStub(uintptr_t safepoint_offset) :\n-      _safepoint_offset(safepoint_offset),\n-      _stub_label(),\n-      _trampoline_label() {}\n-  };\n-\n-  GrowableArray<C2SafepointPollStub*> _safepoints;\n-\n-  static volatile int _stub_size;\n-\n-  void emit_stub_impl(MacroAssembler& masm, C2SafepointPollStub* entry) const;\n-\n-  \/\/ The selection logic below relieves the need to add dummy files to unsupported platforms.\n-  template <bool enabled>\n-  typename EnableIf<enabled>::type\n-  select_emit_stub(MacroAssembler& masm, C2SafepointPollStub* entry) const {\n-    emit_stub_impl(masm, entry);\n-  }\n-\n-  template <bool enabled>\n-  typename EnableIf<!enabled>::type\n-  select_emit_stub(MacroAssembler& masm, C2SafepointPollStub* entry) const {}\n-\n-  void emit_stub(MacroAssembler& masm, C2SafepointPollStub* entry) const {\n-    select_emit_stub<VM_Version::supports_stack_watermark_barrier()>(masm, entry);\n-  }\n-\n-  int stub_size_lazy() const;\n-\n-public:\n-  Label& add_safepoint(uintptr_t safepoint_offset);\n-  int estimate_stub_size() const;\n-  void emit(CodeBuffer& cb);\n-};\n-\n@@ -124,1 +84,1 @@\n-  C2SafepointPollStubTable _safepoint_poll_table;\/\/ Table for safepoint polls\n+  C2CodeStubList         _stub_list;             \/\/ List of code stubs\n@@ -172,2 +132,2 @@\n-  \/\/ Safepoint poll table\n-  C2SafepointPollStubTable* safepoint_poll_table() { return &_safepoint_poll_table; }\n+  \/\/ Code stubs list\n+  void add_stub(C2CodeStub* stub) { _stub_list.add_stub(stub); }\n","filename":"src\/hotspot\/share\/opto\/output.hpp","additions":4,"deletions":44,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -307,3 +307,2 @@\n-    const size_t hs = arrayOopDesc::header_size(elem_type);\n-    \/\/ Align to next 8 bytes to avoid trashing arrays's length.\n-    const size_t aligned_hs = align_object_offset(hs);\n+    size_t hs_bytes = arrayOopDesc::base_offset_in_bytes(elem_type);\n+    assert(is_aligned(hs_bytes, BytesPerInt), \"must be 4 byte aligned\");\n@@ -311,2 +310,3 @@\n-    if (aligned_hs > hs) {\n-      Copy::zero_to_words(obj+hs, aligned_hs-hs);\n+    if (!is_aligned(hs_bytes, BytesPerLong)) {\n+      *reinterpret_cast<jint*>(reinterpret_cast<char*>(obj) + hs_bytes) = 0;\n+      hs_bytes += BytesPerInt;\n@@ -314,0 +314,1 @@\n+\n@@ -315,0 +316,2 @@\n+    assert(is_aligned(hs_bytes, BytesPerLong), \"must be 8-byte aligned\");\n+    const size_t aligned_hs = hs_bytes \/ BytesPerLong;\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":8,"deletions":5,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -4564,1 +4564,2 @@\n-    int header_size = objArrayOopDesc::header_size() * wordSize;\n+    BasicType basic_elem_type = elem()->basic_type();\n+    int header_size = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -972,10 +972,4 @@\n-  {\n-    \/\/ Revoke any biases before querying the mark word\n-    BiasedLocking::revoke_at_safepoint(hobj);\n-\n-    address owner = NULL;\n-    {\n-      markWord mark = hobj()->mark();\n-\n-      if (!mark.has_monitor()) {\n-        \/\/ this object has a lightweight monitor\n+  owning_thread = ObjectSynchronizer::get_lock_owner(tlh.list(), hobj);\n+  if (owning_thread != NULL) {  \/\/ monitor is owned\n+    Handle th(current_thread, owning_thread->threadObj());\n+    ret.owner = (jthread)jni_reference(calling_thread, th);\n@@ -983,34 +977,5 @@\n-        if (mark.has_locker()) {\n-          owner = (address)mark.locker(); \/\/ save the address of the Lock word\n-        }\n-        \/\/ implied else: no owner\n-      } else {\n-        \/\/ this object has a heavyweight monitor\n-        mon = mark.monitor();\n-\n-        \/\/ The owner field of a heavyweight monitor may be NULL for no\n-        \/\/ owner, a JavaThread * or it may still be the address of the\n-        \/\/ Lock word in a JavaThread's stack. A monitor can be inflated\n-        \/\/ by a non-owning JavaThread, but only the owning JavaThread\n-        \/\/ can change the owner field from the Lock word to the\n-        \/\/ JavaThread * and it may not have done that yet.\n-        owner = (address)mon->owner();\n-      }\n-    }\n-\n-    if (owner != NULL) {\n-      \/\/ This monitor is owned so we have to find the owning JavaThread.\n-      owning_thread = Threads::owning_thread_from_monitor_owner(tlh.list(), owner);\n-      assert(owning_thread != NULL, \"owning JavaThread must not be NULL\");\n-      Handle     th(current_thread, owning_thread->threadObj());\n-      ret.owner = (jthread)jni_reference(calling_thread, th);\n-    }\n-\n-    if (owning_thread != NULL) {  \/\/ monitor is owned\n-      \/\/ The recursions field of a monitor does not reflect recursions\n-      \/\/ as lightweight locks before inflating the monitor are not included.\n-      \/\/ We have to count the number of recursive monitor entries the hard way.\n-      \/\/ We pass a handle to survive any GCs along the way.\n-      ret.entry_count = count_locked_objects(owning_thread, hobj);\n-    }\n-    \/\/ implied else: entry_count == 0\n+    \/\/ The recursions field of a monitor does not reflect recursions\n+    \/\/ as lightweight locks before inflating the monitor are not included.\n+    \/\/ We have to count the number of recursive monitor entries the hard way.\n+    \/\/ We pass a handle to survive any GCs along the way.\n+    ret.entry_count = count_locked_objects(owning_thread, hobj);\n@@ -1018,0 +983,1 @@\n+  \/\/ implied else: entry_count == 0\n@@ -1020,1 +986,4 @@\n-  if (mon != NULL) {\n+  markWord mark = hobj->mark();\n+  if (mark.has_monitor()) {\n+    mon = mark.monitor();\n+    assert(mon != NULL, \"must have monitor\");\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnvBase.cpp","additions":14,"deletions":45,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -69,0 +69,1 @@\n+#include \"utilities\/objectBitSet.inline.hpp\"\n@@ -71,0 +72,2 @@\n+typedef ObjectBitSet<mtServiceability> JVMTIBitSet;\n+\n@@ -1356,136 +1359,0 @@\n-\n-\/\/ ObjectMarker is used to support the marking objects when walking the\n-\/\/ heap.\n-\/\/\n-\/\/ This implementation uses the existing mark bits in an object for\n-\/\/ marking. Objects that are marked must later have their headers restored.\n-\/\/ As most objects are unlocked and don't have their identity hash computed\n-\/\/ we don't have to save their headers. Instead we save the headers that\n-\/\/ are \"interesting\". Later when the headers are restored this implementation\n-\/\/ restores all headers to their initial value and then restores the few\n-\/\/ objects that had interesting headers.\n-\/\/\n-\/\/ Future work: This implementation currently uses growable arrays to save\n-\/\/ the oop and header of interesting objects. As an optimization we could\n-\/\/ use the same technique as the GC and make use of the unused area\n-\/\/ between top() and end().\n-\/\/\n-\n-\/\/ An ObjectClosure used to restore the mark bits of an object\n-class RestoreMarksClosure : public ObjectClosure {\n- public:\n-  void do_object(oop o) {\n-    if (o != NULL) {\n-      markWord mark = o->mark();\n-      if (mark.is_marked()) {\n-        o->init_mark();\n-      }\n-    }\n-  }\n-};\n-\n-\/\/ ObjectMarker provides the mark and visited functions\n-class ObjectMarker : AllStatic {\n- private:\n-  \/\/ saved headers\n-  static GrowableArray<oop>* _saved_oop_stack;\n-  static GrowableArray<markWord>* _saved_mark_stack;\n-  static bool _needs_reset;                  \/\/ do we need to reset mark bits?\n-\n- public:\n-  static void init();                       \/\/ initialize\n-  static void done();                       \/\/ clean-up\n-\n-  static inline void mark(oop o);           \/\/ mark an object\n-  static inline bool visited(oop o);        \/\/ check if object has been visited\n-\n-  static inline bool needs_reset()            { return _needs_reset; }\n-  static inline void set_needs_reset(bool v)  { _needs_reset = v; }\n-};\n-\n-GrowableArray<oop>* ObjectMarker::_saved_oop_stack = NULL;\n-GrowableArray<markWord>* ObjectMarker::_saved_mark_stack = NULL;\n-bool ObjectMarker::_needs_reset = true;  \/\/ need to reset mark bits by default\n-\n-\/\/ initialize ObjectMarker - prepares for object marking\n-void ObjectMarker::init() {\n-  assert(Thread::current()->is_VM_thread(), \"must be VMThread\");\n-  assert(SafepointSynchronize::is_at_safepoint(), \"must be at a safepoint\");\n-\n-  \/\/ prepare heap for iteration\n-  Universe::heap()->ensure_parsability(false);  \/\/ no need to retire TLABs\n-\n-  \/\/ create stacks for interesting headers\n-  _saved_mark_stack = new (ResourceObj::C_HEAP, mtServiceability) GrowableArray<markWord>(4000, mtServiceability);\n-  _saved_oop_stack = new (ResourceObj::C_HEAP, mtServiceability) GrowableArray<oop>(4000, mtServiceability);\n-\n-  if (UseBiasedLocking) {\n-    BiasedLocking::preserve_marks();\n-  }\n-}\n-\n-\/\/ Object marking is done so restore object headers\n-void ObjectMarker::done() {\n-  \/\/ iterate over all objects and restore the mark bits to\n-  \/\/ their initial value\n-  RestoreMarksClosure blk;\n-  if (needs_reset()) {\n-    Universe::heap()->object_iterate(&blk);\n-  } else {\n-    \/\/ We don't need to reset mark bits on this call, but reset the\n-    \/\/ flag to the default for the next call.\n-    set_needs_reset(true);\n-  }\n-\n-  \/\/ now restore the interesting headers\n-  for (int i = 0; i < _saved_oop_stack->length(); i++) {\n-    oop o = _saved_oop_stack->at(i);\n-    markWord mark = _saved_mark_stack->at(i);\n-    o->set_mark(mark);\n-  }\n-\n-  if (UseBiasedLocking) {\n-    BiasedLocking::restore_marks();\n-  }\n-\n-  \/\/ free the stacks\n-  delete _saved_oop_stack;\n-  delete _saved_mark_stack;\n-}\n-\n-\/\/ mark an object\n-inline void ObjectMarker::mark(oop o) {\n-  assert(Universe::heap()->is_in(o), \"sanity check\");\n-  assert(!o->mark().is_marked(), \"should only mark an object once\");\n-\n-  \/\/ object's mark word\n-  markWord mark = o->mark();\n-\n-  if (o->mark_must_be_preserved(mark)) {\n-    _saved_mark_stack->push(mark);\n-    _saved_oop_stack->push(o);\n-  }\n-\n-  \/\/ mark the object\n-  o->set_mark(markWord::prototype().set_marked());\n-}\n-\n-\/\/ return true if object is marked\n-inline bool ObjectMarker::visited(oop o) {\n-  return o->mark().is_marked();\n-}\n-\n-\/\/ Stack allocated class to help ensure that ObjectMarker is used\n-\/\/ correctly. Constructor initializes ObjectMarker, destructor calls\n-\/\/ ObjectMarker's done() function to restore object headers.\n-class ObjectMarkerController : public StackObj {\n- public:\n-  ObjectMarkerController() {\n-    ObjectMarker::init();\n-  }\n-  ~ObjectMarkerController() {\n-    ObjectMarker::done();\n-  }\n-};\n-\n-\n@@ -1624,0 +1491,1 @@\n+  static JVMTIBitSet* _bitset;\n@@ -1633,1 +1501,1 @@\n-    if (!ObjectMarker::visited(obj)) visit_stack()->push(obj);\n+    if (!_bitset->is_marked(obj)) visit_stack()->push(obj);\n@@ -1664,1 +1532,2 @@\n-                                             BasicHeapWalkContext context);\n+                                             BasicHeapWalkContext context,\n+                                             JVMTIBitSet* bitset);\n@@ -1670,1 +1539,2 @@\n-                                                AdvancedHeapWalkContext context);\n+                                                AdvancedHeapWalkContext context,\n+                                                JVMTIBitSet* bitset);\n@@ -1703,0 +1573,1 @@\n+JVMTIBitSet* CallbackInvoker::_bitset;\n@@ -1708,1 +1579,2 @@\n-                                                     BasicHeapWalkContext context) {\n+                                                     BasicHeapWalkContext context,\n+                                                     JVMTIBitSet* bitset) {\n@@ -1715,0 +1587,1 @@\n+  _bitset = bitset;\n@@ -1721,1 +1594,2 @@\n-                                                        AdvancedHeapWalkContext context) {\n+                                                        AdvancedHeapWalkContext context,\n+                                                        JVMTIBitSet* bitset) {\n@@ -1728,0 +1602,1 @@\n+  _bitset = bitset;\n@@ -2399,0 +2274,2 @@\n+  JVMTIBitSet _bitset;\n+\n@@ -2475,1 +2352,1 @@\n-  CallbackInvoker::initialize_for_basic_heap_walk(tag_map, _visit_stack, user_data, callbacks);\n+  CallbackInvoker::initialize_for_basic_heap_walk(tag_map, _visit_stack, user_data, callbacks, &_bitset);\n@@ -2493,1 +2370,1 @@\n-  CallbackInvoker::initialize_for_advanced_heap_walk(tag_map, _visit_stack, user_data, callbacks);\n+  CallbackInvoker::initialize_for_advanced_heap_walk(tag_map, _visit_stack, user_data, callbacks, &_bitset);\n@@ -2929,2 +2806,2 @@\n-  assert(!ObjectMarker::visited(o), \"can't visit same object more than once\");\n-  ObjectMarker::mark(o);\n+  assert(!_bitset.is_marked(o), \"can't visit same object more than once\");\n+  _bitset.mark_obj(o);\n@@ -2959,1 +2836,0 @@\n-  ObjectMarkerController marker;\n@@ -2968,6 +2844,0 @@\n-    \/\/ If either collect_stack_roots() or collect_simple_roots()\n-    \/\/ returns false at this point, then there are no mark bits\n-    \/\/ to reset.\n-    ObjectMarker::set_needs_reset(false);\n-\n-    \/\/ Calling collect_stack_roots() before collect_simple_roots()\n@@ -2979,3 +2849,0 @@\n-\n-    \/\/ no early return so enable heap traversal to reset the mark bits\n-    ObjectMarker::set_needs_reset(true);\n@@ -2993,1 +2860,1 @@\n-      if (!ObjectMarker::visited(o)) {\n+      if (!_bitset.is_marked(o)) {\n","filename":"src\/hotspot\/share\/prims\/jvmtiTagMap.cpp","additions":23,"deletions":156,"binary":false,"changes":179,"status":"modified"},{"patch":"@@ -69,1 +69,1 @@\n-  ( arrayOopDesc::header_size(T_DOUBLE) * HeapWordSize \\\n+  ( arrayOopDesc::base_offset_in_bytes(T_DOUBLE) \\\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2006,0 +2006,16 @@\n+#if !defined(X86) && !defined(AARCH64)\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    FLAG_SET_CMDLINE(LockingMode, LM_LEGACY);\n+    warning(\"New lightweight locking not supported on this platform\");\n+  }\n+#endif\n+\n+  if (UseHeavyMonitors) {\n+    if (FLAG_IS_CMDLINE(LockingMode) && LockingMode != LM_MONITOR) {\n+      jio_fprintf(defaultStream::error_stream(),\n+                  \"Conflicting -XX:+UseHeavyMonitors and -XX:LockingMode=%d flags\", LockingMode);\n+      return false;\n+    }\n+    FLAG_SET_CMDLINE(LockingMode, LM_MONITOR);\n+  }\n+\n@@ -3156,0 +3172,22 @@\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders && UseZGC) {\n+    warning(\"ZGC does not work with compact object headers, disabling UseCompactObjectHeaders\");\n+    FLAG_SET_DEFAULT(UseCompactObjectHeaders, false);\n+  }\n+\n+  if (UseCompactObjectHeaders && FLAG_IS_CMDLINE(UseCompressedClassPointers) && !UseCompressedClassPointers) {\n+    \/\/ If user specifies -UseCompressedClassPointers, disable compact headers with a warning.\n+    warning(\"Compact object headers require compressed class pointers. Disabling compact object headers.\");\n+    FLAG_SET_DEFAULT(UseCompactObjectHeaders, false);\n+  }\n+  if (UseCompactObjectHeaders && LockingMode == LM_LEGACY) {\n+    FLAG_SET_DEFAULT(LockingMode, LM_LIGHTWEIGHT);\n+  }\n+  if (UseCompactObjectHeaders && UseBiasedLocking) {\n+    FLAG_SET_DEFAULT(UseBiasedLocking, false);\n+  }\n+  if (UseCompactObjectHeaders && !UseAltGCForwarding) {\n+    FLAG_SET_DEFAULT(UseAltGCForwarding, true);\n+  }\n+#endif\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":38,"deletions":0,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -1475,1 +1475,1 @@\n-          if (mark.has_locker() && fr.sp() > (intptr_t*)mark.locker()) {\n+          if (LockingMode == LM_LEGACY && mark.has_locker() && fr.sp() > (intptr_t*)mark.locker()) {\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -133,0 +133,3 @@\n+  product(bool, UseCompactObjectHeaders, false, EXPERIMENTAL,               \\\n+          \"Use 64-bit object headers instead of 96-bit headers\")            \\\n+                                                                            \\\n@@ -150,0 +153,1 @@\n+const bool UseCompactObjectHeaders = false;\n@@ -2094,0 +2098,13 @@\n+  product(bool, HeapObjectStats, false, DIAGNOSTIC,                         \\\n+             \"Enable gathering of heap object statistics\")                  \\\n+                                                                            \\\n+  product(size_t, HeapObjectStatsSamplingInterval, 500, DIAGNOSTIC,         \\\n+             \"Heap object statistics sampling interval (ms)\")               \\\n+                                                                            \\\n+  product(int, LockingMode, LM_LEGACY, EXPERIMENTAL,                        \\\n+          \"Select locking mode: \"                                           \\\n+          \"0: monitors only (LM_MONITOR), \"                                 \\\n+          \"1: monitors & legacy stack-locking (LM_LEGACY, default), \"       \\\n+          \"2: monitors & new lightweight locking (LM_LIGHTWEIGHT)\")         \\\n+          range(0, 2)                                                       \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -0,0 +1,79 @@\n+\/*\n+ * Copyright (c) 2022, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"runtime\/lockStack.inline.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/stackWatermark.hpp\"\n+#include \"runtime\/stackWatermarkSet.inline.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"utilities\/copy.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+const int LockStack::lock_stack_offset =      in_bytes(JavaThread::lock_stack_offset());\n+const int LockStack::lock_stack_top_offset =  in_bytes(JavaThread::lock_stack_top_offset());\n+const int LockStack::lock_stack_base_offset = in_bytes(JavaThread::lock_stack_base_offset());\n+\n+LockStack::LockStack(JavaThread* jt) :\n+  _top(lock_stack_base_offset), _base() {\n+#ifdef ASSERT\n+  for (int i = 0; i < CAPACITY; i++) {\n+    _base[i] = NULL;\n+  }\n+#endif\n+}\n+\n+uint32_t LockStack::start_offset() {\n+  int offset = lock_stack_base_offset;\n+  assert(offset > 0, \"must be positive offset\");\n+  return static_cast<uint32_t>(offset);\n+}\n+\n+uint32_t LockStack::end_offset() {\n+  int offset = lock_stack_base_offset + CAPACITY * oopSize;\n+  assert(offset > 0, \"must be positive offset\");\n+  return static_cast<uint32_t>(offset);\n+}\n+\n+#ifndef PRODUCT\n+void LockStack::verify(const char* msg) const {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"never use lock-stack when light weight locking is disabled\");\n+  assert((_top <= end_offset()), \"lockstack overflow: _top %d end_offset %d\", _top, end_offset());\n+  assert((_top >= start_offset()), \"lockstack underflow: _top %d end_offset %d\", _top, start_offset());\n+  if (SafepointSynchronize::is_at_safepoint() || (Thread::current()->is_Java_thread() && is_owning_thread())) {\n+    int top = to_index(_top);\n+    for (int i = 0; i < top; i++) {\n+      assert(_base[i] != NULL, \"no zapped before top\");\n+      for (int j = i + 1; j < top; j++) {\n+        assert(_base[i] != _base[j], \"entries must be unique: %s\", msg);\n+      }\n+    }\n+    for (int i = top; i < CAPACITY; i++) {\n+      assert(_base[i] == NULL, \"only zapped entries after top: i: %d, top: %d, entry: \" PTR_FORMAT, i, top, p2i(_base[i]));\n+    }\n+  }\n+}\n+#endif\n","filename":"src\/hotspot\/share\/runtime\/lockStack.cpp","additions":79,"deletions":0,"binary":false,"changes":79,"status":"added"},{"patch":"@@ -0,0 +1,96 @@\n+\/*\n+ * Copyright (c) 2022, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_LOCKSTACK_HPP\n+#define SHARE_RUNTIME_LOCKSTACK_HPP\n+\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/sizes.hpp\"\n+\n+class Thread;\n+class OopClosure;\n+\n+class LockStack {\n+  friend class VMStructs;\n+private:\n+  static const int CAPACITY = 8;\n+\n+  \/\/ TODO: It would be very useful if JavaThread::lock_stack_offset() and friends were constexpr,\n+  \/\/ but this is currently not the case because we're using offset_of() which is non-constexpr,\n+  \/\/ GCC would warn about non-standard-layout types if we were using offsetof() (which *is* constexpr).\n+  static const int lock_stack_offset;\n+  static const int lock_stack_top_offset;\n+  static const int lock_stack_base_offset;\n+\n+  \/\/ The offset of the next element, in bytes, relative to the JavaThread structure.\n+  \/\/ We do this instead of a simple index into the array because this allows for\n+  \/\/ efficient addressing in generated code.\n+  uint32_t _top;\n+  oop _base[CAPACITY];\n+\n+  \/\/ Get the owning thread of this lock-stack.\n+  inline JavaThread* get_thread() const;\n+\n+  \/\/ Tests if the calling thread is the thread that owns this lock-stack.\n+  bool is_owning_thread() const;\n+\n+  \/\/ Verifies consistency of the lock-stack.\n+  void verify(const char* msg) const PRODUCT_RETURN;\n+\n+  \/\/ Given an offset (in bytes) calculate the index into the lock-stack.\n+  static inline int to_index(uint32_t offset);\n+\n+public:\n+  static ByteSize top_offset()  { return byte_offset_of(LockStack, _top); }\n+  static ByteSize base_offset() { return byte_offset_of(LockStack, _base); }\n+\n+  LockStack(JavaThread* jt);\n+\n+  \/\/ The boundary indicies of the lock-stack.\n+  static uint32_t start_offset();\n+  static uint32_t end_offset();\n+\n+  \/\/ Return true if we have room to push onto this lock-stack, false otherwise.\n+  inline bool can_push() const;\n+\n+  \/\/ Pushes an oop on this lock-stack.\n+  inline void push(oop o);\n+\n+  \/\/ Pops an oop from this lock-stack.\n+  inline oop pop();\n+\n+  \/\/ Removes an oop from an arbitrary location of this lock-stack.\n+  inline void remove(oop o);\n+\n+  \/\/ Tests whether the oop is on this lock-stack.\n+  inline bool contains(oop o) const;\n+\n+  \/\/ GC support\n+  inline void oops_do(OopClosure* cl);\n+\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_LOCKSTACK_HPP\n","filename":"src\/hotspot\/share\/runtime\/lockStack.hpp","additions":96,"deletions":0,"binary":false,"changes":96,"status":"added"},{"patch":"@@ -0,0 +1,133 @@\n+\/*\n+ * Copyright (c) 2022, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_LOCKSTACK_INLINE_HPP\n+#define SHARE_RUNTIME_LOCKSTACK_INLINE_HPP\n+\n+#include \"memory\/iterator.hpp\"\n+#include \"runtime\/lockStack.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/stackWatermark.hpp\"\n+#include \"runtime\/stackWatermarkSet.inline.hpp\"\n+#include \"runtime\/thread.hpp\"\n+\n+inline int LockStack::to_index(uint32_t offset) {\n+  return (offset - lock_stack_base_offset) \/ oopSize;\n+}\n+\n+JavaThread* LockStack::get_thread() const {\n+  char* addr = reinterpret_cast<char*>(const_cast<LockStack*>(this));\n+  return reinterpret_cast<JavaThread*>(addr - lock_stack_offset);\n+}\n+\n+inline bool LockStack::can_push() const {\n+  return to_index(_top) < CAPACITY;\n+}\n+\n+inline bool LockStack::is_owning_thread() const {\n+  JavaThread* thread = JavaThread::current();\n+  bool is_owning = &thread->lock_stack() == this;\n+  assert(is_owning == (get_thread() == thread), \"is_owning sanity\");\n+  return is_owning;\n+}\n+\n+inline void LockStack::push(oop o) {\n+  verify(\"pre-push\");\n+  assert(oopDesc::is_oop(o), \"must be\");\n+  assert(!contains(o), \"entries must be unique\");\n+  assert(can_push(), \"must have room\");\n+  assert(_base[to_index(_top)] == NULL, \"expect zapped entry\");\n+  _base[to_index(_top)] = o;\n+  _top += oopSize;\n+  verify(\"post-push\");\n+}\n+\n+inline oop LockStack::pop() {\n+  verify(\"pre-pop\");\n+  assert(to_index(_top) > 0, \"underflow, probably unbalanced push\/pop\");\n+  _top -= oopSize;\n+  oop o = _base[to_index(_top)];\n+#ifdef ASSERT\n+  _base[to_index(_top)] = NULL;\n+#endif\n+  assert(!contains(o), \"entries must be unique: \" PTR_FORMAT, p2i(o));\n+  verify(\"post-pop\");\n+  return o;\n+}\n+\n+inline void LockStack::remove(oop o) {\n+  verify(\"pre-remove\");\n+  assert(contains(o), \"entry must be present: \" PTR_FORMAT, p2i(o));\n+  int end = to_index(_top);\n+  for (int i = 0; i < end; i++) {\n+    if (_base[i] == o) {\n+      int last = end - 1;\n+      for (; i < last; i++) {\n+        _base[i] = _base[i + 1];\n+      }\n+      _top -= oopSize;\n+#ifdef ASSERT\n+      _base[to_index(_top)] = NULL;\n+#endif\n+      break;\n+    }\n+  }\n+  assert(!contains(o), \"entries must be unique: \" PTR_FORMAT, p2i(o));\n+  verify(\"post-remove\");\n+}\n+\n+inline bool LockStack::contains(oop o) const {\n+  verify(\"pre-contains\");\n+  if (!SafepointSynchronize::is_at_safepoint() && !is_owning_thread()) {\n+    \/\/ When a foreign thread inspects this thread's lock-stack, it may see\n+    \/\/ bad references here when a concurrent collector has not gotten\n+    \/\/ to processing the lock-stack, yet. Call StackWaterMark::start_processing()\n+    \/\/ to ensure that all references are valid.\n+    StackWatermark* watermark = StackWatermarkSet::get(get_thread(), StackWatermarkKind::gc);\n+    if (watermark != NULL) {\n+      watermark->start_processing();\n+    }\n+  }\n+  int end = to_index(_top);\n+  for (int i = end - 1; i >= 0; i--) {\n+    if (_base[i] == o) {\n+      verify(\"post-contains\");\n+      return true;\n+    }\n+  }\n+  verify(\"post-contains\");\n+  return false;\n+}\n+\n+inline void LockStack::oops_do(OopClosure* cl) {\n+  verify(\"pre-oops-do\");\n+  int end = to_index(_top);\n+  for (int i = 0; i < end; i++) {\n+    cl->do_oop(&_base[i]);\n+  }\n+  verify(\"post-oops-do\");\n+}\n+\n+#endif \/\/ SHARE_RUNTIME_LOCKSTACK_INLINE_HPP\n","filename":"src\/hotspot\/share\/runtime\/lockStack.inline.hpp","additions":133,"deletions":0,"binary":false,"changes":133,"status":"added"},{"patch":"@@ -337,1 +337,1 @@\n-  if (current->is_lock_owned((address)cur)) {\n+  if (LockingMode != LM_LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n@@ -603,0 +603,16 @@\n+\/\/ We might access the dead object headers for parsable heap walk, make sure\n+\/\/ headers are in correct shape, e.g. monitors deflated.\n+void ObjectMonitor::maybe_deflate_dead(oop* p) {\n+  oop obj = *p;\n+  assert(obj != NULL, \"must not yet been cleared\");\n+  markWord mark = obj->mark();\n+  if (mark.has_monitor()) {\n+    ObjectMonitor* monitor = mark.monitor();\n+    if (p == monitor->_object.ptr_raw()) {\n+      assert(monitor->object_peek() == obj, \"lock object must match\");\n+      markWord dmw = monitor->header();\n+      obj->set_mark(dmw);\n+    }\n+  }\n+}\n+\n@@ -1138,1 +1154,1 @@\n-    if (current->is_lock_owned((address)cur)) {\n+    if (LockingMode != LM_LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n@@ -1358,1 +1374,1 @@\n-    if (current->is_lock_owned((address)cur)) {\n+    if (LockingMode != LM_LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n@@ -1407,0 +1423,1 @@\n+  assert(cur != anon_owner_ptr(), \"no anon owner here\");\n@@ -1410,1 +1427,1 @@\n-  if (current->is_lock_owned((address)cur)) {\n+  if (LockingMode != LM_LIGHTWEIGHT && current->is_lock_owned((address)cur)) {\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":21,"deletions":4,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -148,2 +148,16 @@\n-  \/\/ Used by async deflation as a marker in the _owner field:\n-  #define DEFLATER_MARKER reinterpret_cast<void*>(-1)\n+  \/\/ Used by async deflation as a marker in the _owner field.\n+  \/\/ Note that the choice of the two markers is peculiar:\n+  \/\/ - They need to represent values that cannot be pointers. In particular,\n+  \/\/   we achieve this by using the lowest two bits.\n+  \/\/ - ANONYMOUS_OWNER should be a small value, it is used in generated code\n+  \/\/   and small values encode much better.\n+  \/\/ - We test for anonymous owner by testing for the lowest bit, therefore\n+  \/\/   DEFLATER_MARKER must *not* have that bit set.\n+  #define DEFLATER_MARKER reinterpret_cast<void*>(2)\n+public:\n+  \/\/ NOTE: Typed as uintptr_t so that we can pick it up in SA, via vmStructs.\n+  static const uintptr_t ANONYMOUS_OWNER = 1;\n+\n+private:\n+  static void* anon_owner_ptr() { return reinterpret_cast<void*>(ANONYMOUS_OWNER); }\n+\n@@ -247,1 +261,1 @@\n-  intptr_t  is_entered(JavaThread* current) const;\n+  bool is_entered(JavaThread* current) const;\n@@ -249,0 +263,1 @@\n+  bool      has_owner() const;\n@@ -266,0 +281,12 @@\n+  void set_owner_anonymous() {\n+    set_owner_from(NULL, anon_owner_ptr());\n+  }\n+\n+  bool is_owner_anonymous() const {\n+    return owner_raw() == anon_owner_ptr();\n+  }\n+\n+  void set_owner_from_anonymous(Thread* owner) {\n+    set_owner_from(anon_owner_ptr(), owner);\n+  }\n+\n@@ -334,0 +361,2 @@\n+  static void maybe_deflate_dead(oop* p);\n+\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":32,"deletions":3,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -35,4 +36,12 @@\n-inline intptr_t ObjectMonitor::is_entered(JavaThread* current) const {\n-  void* owner = owner_raw();\n-  if (current == owner || current->is_lock_owned((address)owner)) {\n-    return 1;\n+inline bool ObjectMonitor::is_entered(JavaThread* current) const {\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if (is_owner_anonymous()) {\n+      return current->lock_stack().contains(object());\n+    } else {\n+      return current == owner_raw();\n+    }\n+  } else {\n+    void* owner = owner_raw();\n+    if (current == owner || current->is_lock_owned((address)owner)) {\n+      return true;\n+    }\n@@ -40,1 +49,1 @@\n-  return 0;\n+  return false;\n@@ -59,0 +68,5 @@\n+inline bool ObjectMonitor::has_owner() const {\n+  void* owner = owner_raw();\n+  return owner != NULL && owner != DEFLATER_MARKER;\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.inline.hpp","additions":19,"deletions":5,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -41,0 +42,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -278,4 +280,12 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-    \/\/ Degenerate notify\n-    \/\/ stack-locked by caller so by definition the implied waitset is empty.\n-    return true;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if (mark.is_fast_locked() && current->lock_stack().contains(cast_to_oop(obj))) {\n+      \/\/ Degenerate notify\n+      \/\/ fast-locked by caller so by definition the implied waitset is empty.\n+      return true;\n+    }\n+  } else if (LockingMode == LM_LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Degenerate notify\n+      \/\/ stack-locked by caller so by definition the implied waitset is empty.\n+      return true;\n+    }\n@@ -351,11 +361,13 @@\n-    \/\/ This Java Monitor is inflated so obj's header will never be\n-    \/\/ displaced to this thread's BasicLock. Make the displaced header\n-    \/\/ non-NULL so this BasicLock is not seen as recursive nor as\n-    \/\/ being locked. We do this unconditionally so that this thread's\n-    \/\/ BasicLock cannot be mis-interpreted by any stack walkers. For\n-    \/\/ performance reasons, stack walkers generally first check for\n-    \/\/ Biased Locking in the object's header, the second check is for\n-    \/\/ stack-locking in the object's header, the third check is for\n-    \/\/ recursive stack-locking in the displaced header in the BasicLock,\n-    \/\/ and last are the inflated Java Monitor (ObjectMonitor) checks.\n-    lock->set_displaced_header(markWord::unused_mark());\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ This Java Monitor is inflated so obj's header will never be\n+      \/\/ displaced to this thread's BasicLock. Make the displaced header\n+      \/\/ non-NULL so this BasicLock is not seen as recursive nor as\n+      \/\/ being locked. We do this unconditionally so that this thread's\n+      \/\/ BasicLock cannot be mis-interpreted by any stack walkers. For\n+      \/\/ performance reasons, stack walkers generally first check for\n+      \/\/ Biased Locking in the object's header, the second check is for\n+      \/\/ stack-locking in the object's header, the third check is for\n+      \/\/ recursive stack-locking in the displaced header in the BasicLock,\n+      \/\/ and last are the inflated Java Monitor (ObjectMonitor) checks.\n+      lock->set_displaced_header(markWord::unused_mark());\n+    }\n@@ -428,0 +440,8 @@\n+static bool useHeavyMonitors() {\n+#if defined(X86) || defined(AARCH64) || defined(PPC64) || defined(RISCV64)\n+  return LockingMode == LM_MONITOR;\n+#else\n+  return false;\n+#endif\n+}\n+\n@@ -439,3 +459,23 @@\n-  if (UseBiasedLocking) {\n-    BiasedLocking::revoke(current, obj);\n-  }\n+  if (!useHeavyMonitors()) {\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ Fast-locking does not use the 'lock' argument.\n+      LockStack& lock_stack = current->lock_stack();\n+      if (lock_stack.can_push()) {\n+        markWord mark = obj()->mark_acquire();\n+        if (mark.is_neutral()) {\n+          assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n+          \/\/ Try to swing into 'fast-locked' state.\n+          markWord locked_mark = mark.set_fast_locked();\n+          markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n+          if (old_mark == mark) {\n+            \/\/ Successfully fast-locked, push object to lock-stack and return.\n+            lock_stack.push(obj());\n+            return;\n+          }\n+        }\n+      }\n+      \/\/ All other paths fall-through to inflate-enter.\n+    } else if (LockingMode == LM_LEGACY) {\n+      if (UseBiasedLocking) {\n+        BiasedLocking::revoke(current, obj);\n+      }\n@@ -443,2 +483,16 @@\n-  markWord mark = obj->mark();\n-  assert(!mark.has_bias_pattern(), \"should not see bias pattern here\");\n+      markWord mark = obj->mark();\n+      if (mark.is_neutral()) {\n+        \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n+        \/\/ be visible <= the ST performed by the CAS.\n+        lock->set_displaced_header(mark);\n+        if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n+          return;\n+        }\n+        \/\/ Fall through to inflate() ...\n+      } else if (mark.has_locker() &&\n+                 current->is_lock_owned((address) mark.locker())) {\n+        assert(lock != mark.locker(), \"must not re-lock the same lock\");\n+        assert(lock != (BasicLock*) obj->mark().value(), \"don't relock with same BasicLock\");\n+        lock->set_displaced_header(markWord::from_pointer(NULL));\n+        return;\n+      }\n@@ -446,6 +500,5 @@\n-  if (mark.is_neutral()) {\n-    \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n-    \/\/ be visible <= the ST performed by the CAS.\n-    lock->set_displaced_header(mark);\n-    if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n-      return;\n+      \/\/ The object header will never be displaced to this lock,\n+      \/\/ so it does not matter what the value is, except that it\n+      \/\/ must be non-zero to avoid looking like a re-entrant lock,\n+      \/\/ and must not look locked either.\n+      lock->set_displaced_header(markWord::unused_mark());\n@@ -453,7 +506,0 @@\n-    \/\/ Fall through to inflate() ...\n-  } else if (mark.has_locker() &&\n-             current->is_lock_owned((address)mark.locker())) {\n-    assert(lock != mark.locker(), \"must not re-lock the same lock\");\n-    assert(lock != (BasicLock*)obj->mark().value(), \"don't relock with same BasicLock\");\n-    lock->set_displaced_header(markWord::from_pointer(NULL));\n-    return;\n@@ -462,5 +508,0 @@\n-  \/\/ The object header will never be displaced to this lock,\n-  \/\/ so it does not matter what the value is, except that it\n-  \/\/ must be non-zero to avoid looking like a re-entrant lock,\n-  \/\/ and must not look locked either.\n-  lock->set_displaced_header(markWord::unused_mark());\n@@ -479,29 +520,21 @@\n-  markWord mark = object->mark();\n-  \/\/ We cannot check for Biased Locking if we are racing an inflation.\n-  assert(mark == markWord::INFLATING() ||\n-         !mark.has_bias_pattern(), \"should not see bias pattern here\");\n-\n-  markWord dhw = lock->displaced_header();\n-  if (dhw.value() == 0) {\n-    \/\/ If the displaced header is NULL, then this exit matches up with\n-    \/\/ a recursive enter. No real work to do here except for diagnostics.\n-#ifndef PRODUCT\n-    if (mark != markWord::INFLATING()) {\n-      \/\/ Only do diagnostics if we are not racing an inflation. Simply\n-      \/\/ exiting a recursive enter of a Java Monitor that is being\n-      \/\/ inflated is safe; see the has_monitor() comment below.\n-      assert(!mark.is_neutral(), \"invariant\");\n-      assert(!mark.has_locker() ||\n-             current->is_lock_owned((address)mark.locker()), \"invariant\");\n-      if (mark.has_monitor()) {\n-        \/\/ The BasicLock's displaced_header is marked as a recursive\n-        \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n-        \/\/ This is a special case where the Java Monitor was inflated\n-        \/\/ after this thread entered the stack-lock recursively. When a\n-        \/\/ Java Monitor is inflated, we cannot safely walk the Java\n-        \/\/ Monitor owner's stack and update the BasicLocks because a\n-        \/\/ Java Monitor can be asynchronously inflated by a thread that\n-        \/\/ does not own the Java Monitor.\n-        ObjectMonitor* m = mark.monitor();\n-        assert(m->object()->mark() == mark, \"invariant\");\n-        assert(m->is_entered(current), \"invariant\");\n+  if (!useHeavyMonitors()) {\n+    markWord mark = object->mark();\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ Fast-locking does not use the 'lock' argument.\n+      if (mark.is_fast_locked()) {\n+        markWord unlocked_mark = mark.set_unlocked();\n+        markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n+        if (old_mark != mark) {\n+          \/\/ Another thread won the CAS, it must have inflated the monitor.\n+          \/\/ It can only have installed an anonymously locked monitor at this point.\n+          \/\/ Fetch that monitor, set owner correctly to this thread, and\n+          \/\/ exit it (allowing waiting threads to enter).\n+          assert(old_mark.has_monitor(), \"must have monitor\");\n+          ObjectMonitor* monitor = old_mark.monitor();\n+          assert(monitor->is_owner_anonymous(), \"must be anonymous owner\");\n+          monitor->set_owner_from_anonymous(current);\n+          monitor->exit(current);\n+        }\n+        LockStack& lock_stack = current->lock_stack();\n+        lock_stack.remove(object);\n+        return;\n@@ -509,1 +542,27 @@\n-    }\n+    } else if (LockingMode == LM_LEGACY) {\n+      markWord dhw = lock->displaced_header();\n+      if (dhw.value() == 0) {\n+        \/\/ If the displaced header is NULL, then this exit matches up with\n+        \/\/ a recursive enter. No real work to do here except for diagnostics.\n+#ifndef PRODUCT\n+        if (mark != markWord::INFLATING()) {\n+          \/\/ Only do diagnostics if we are not racing an inflation. Simply\n+          \/\/ exiting a recursive enter of a Java Monitor that is being\n+          \/\/ inflated is safe; see the has_monitor() comment below.\n+          assert(!mark.is_neutral(), \"invariant\");\n+          assert(!mark.has_locker() ||\n+                 current->is_lock_owned((address)mark.locker()), \"invariant\");\n+          if (mark.has_monitor()) {\n+            \/\/ The BasicLock's displaced_header is marked as a recursive\n+            \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n+            \/\/ This is a special case where the Java Monitor was inflated\n+            \/\/ after this thread entered the stack-lock recursively. When a\n+            \/\/ Java Monitor is inflated, we cannot safely walk the Java\n+            \/\/ Monitor owner's stack and update the BasicLocks because a\n+            \/\/ Java Monitor can be asynchronously inflated by a thread that\n+            \/\/ does not own the Java Monitor.\n+            ObjectMonitor* m = mark.monitor();\n+            assert(m->object()->mark() == mark, \"invariant\");\n+            assert(m->is_entered(current), \"invariant\");\n+          }\n+        }\n@@ -511,2 +570,2 @@\n-    return;\n-  }\n+        return;\n+      }\n@@ -514,6 +573,8 @@\n-  if (mark == markWord::from_pointer(lock)) {\n-    \/\/ If the object is stack-locked by the current thread, try to\n-    \/\/ swing the displaced header from the BasicLock back to the mark.\n-    assert(dhw.is_neutral(), \"invariant\");\n-    if (object->cas_set_mark(dhw, mark) == mark) {\n-      return;\n+      if (mark == markWord::from_pointer(lock)) {\n+        \/\/ If the object is stack-locked by the current thread, try to\n+        \/\/ swing the displaced header from the BasicLock back to the mark.\n+        assert(dhw.is_neutral(), \"invariant\");\n+        if (object->cas_set_mark(dhw, mark) == mark) {\n+          return;\n+        }\n+      }\n@@ -527,0 +588,7 @@\n+  if (LockingMode == LM_LIGHTWEIGHT && monitor->is_owner_anonymous()) {\n+    \/\/ It must be us. Pop lock object from lock stack.\n+    LockStack& lock_stack = current->lock_stack();\n+    oop popped = lock_stack.pop();\n+    assert(popped == object, \"must be owned by this thread\");\n+    monitor->set_owner_from_anonymous(current);\n+  }\n@@ -691,3 +759,10 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-    \/\/ Not inflated so there can't be any waiters to notify.\n-    return;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if ((mark.is_fast_locked() && current->lock_stack().contains(obj()))) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n+  } else if (LockingMode == LM_LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n@@ -710,3 +785,10 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-    \/\/ Not inflated so there can't be any waiters to notify.\n-    return;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if ((mark.is_fast_locked() && current->lock_stack().contains(obj()))) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n+  } else if (LockingMode == LM_LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n@@ -738,1 +820,2 @@\n-  if (!mark.is_being_inflated()) {\n+  if (!mark.is_being_inflated() || LockingMode == LM_LIGHTWEIGHT) {\n+    \/\/ New lightweight locking does not use the markWord::INFLATING() protocol.\n@@ -847,1 +930,1 @@\n-  value &= markWord::hash_mask;\n+  value &= UseCompactObjectHeaders ? markWord::hash_mask_compact : markWord::hash_mask;\n@@ -853,0 +936,7 @@\n+\/\/ Can be called from non JavaThreads (e.g., VMThread) for FastHashCode\n+\/\/ calculations as part of JVM\/TI tagging.\n+static bool is_lock_owned(Thread* thread, oop obj) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only call this with new lightweight locking enabled\");\n+  return thread->is_Java_thread() ? reinterpret_cast<JavaThread*>(thread)->lock_stack().contains(obj) : false;\n+}\n+\n@@ -926,1 +1016,8 @@\n-    } else if (current->is_lock_owned((address)mark.locker())) {\n+    } else if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked() && is_lock_owned(current, obj)) {\n+      \/\/ This is a fast lock owned by the calling thread so use the\n+      \/\/ markWord from the object.\n+      hash = mark.hash();\n+      if (hash != 0) {                  \/\/ if it has a hash, just return it\n+        return hash;\n+      }\n+    } else if (LockingMode == LM_LEGACY && mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n@@ -1003,2 +1100,2 @@\n-  \/\/ Uncontended case, header points to stack\n-  if (mark.has_locker()) {\n+  if (LockingMode == LM_LEGACY && mark.has_locker()) {\n+    \/\/ stack-locked case, header points into owner's stack\n@@ -1007,0 +1104,6 @@\n+\n+  if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+    \/\/ fast-locking case, see if lock is in current's lock stack\n+    return current->lock_stack().contains(h_obj());\n+  }\n+\n@@ -1031,2 +1134,0 @@\n-  address owner = NULL;\n-\n@@ -1035,3 +1136,10 @@\n-  \/\/ Uncontended case, header points to stack\n-  if (mark.has_locker()) {\n-    owner = (address) mark.locker();\n+  if (LockingMode == LM_LEGACY && mark.has_locker()) {\n+    \/\/ stack-locked so header points into owner's stack.\n+    \/\/ owning_thread_from_monitor_owner() may also return null here:\n+    return Threads::owning_thread_from_monitor_owner(t_list, (address) mark.locker());\n+  }\n+\n+  if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+    \/\/ fast-locked so get owner from the object.\n+    \/\/ owning_thread_from_object() may also return null here:\n+    return Threads::owning_thread_from_object(t_list, h_obj());\n@@ -1041,1 +1149,1 @@\n-  else if (mark.has_monitor()) {\n+  if (mark.has_monitor()) {\n@@ -1046,6 +1154,2 @@\n-    owner = (address) monitor->owner();\n-  }\n-\n-  if (owner != NULL) {\n-    \/\/ owning_thread_from_monitor_owner() may also return NULL here\n-    return Threads::owning_thread_from_monitor_owner(t_list, owner);\n+    \/\/ owning_thread_from_monitor() may also return null here:\n+    return Threads::owning_thread_from_monitor(t_list, monitor);\n@@ -1255,2 +1359,8 @@\n-    \/\/ *  Inflated     - just return\n-    \/\/ *  Stack-locked - coerce it to inflated\n+    \/\/ *  inflated     - Just return if using stack-locking.\n+    \/\/                   If using fast-locking and the ObjectMonitor owner\n+    \/\/                   is anonymous and the current thread owns the\n+    \/\/                   object lock, then we make the current thread the\n+    \/\/                   ObjectMonitor owner and remove the lock from the\n+    \/\/                   current thread's lock stack.\n+    \/\/ *  fast-locked  - Coerce it to inflated from fast-locked.\n+    \/\/ *  stack-locked - Coerce it to inflated from stack-locked.\n@@ -1266,0 +1376,5 @@\n+      if (LockingMode == LM_LIGHTWEIGHT && inf->is_owner_anonymous() && is_lock_owned(current, object)) {\n+        inf->set_owner_from_anonymous(current);\n+        assert(current->is_Java_thread(), \"must be Java thread\");\n+        reinterpret_cast<JavaThread*>(current)->lock_stack().remove(object);\n+      }\n@@ -1275,3 +1390,65 @@\n-    if (mark == markWord::INFLATING()) {\n-      read_stable_mark(object);\n-      continue;\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ New lightweight locking does not use INFLATING.\n+      \/\/ CASE: inflation in progress - inflating over a stack-lock.\n+      \/\/ Some other thread is converting from stack-locked to inflated.\n+      \/\/ Only that thread can complete inflation -- other threads must wait.\n+      \/\/ The INFLATING value is transient.\n+      \/\/ Currently, we spin\/yield\/park and poll the markword, waiting for inflation to finish.\n+      \/\/ We could always eliminate polling by parking the thread on some auxiliary list.\n+      if (mark == markWord::INFLATING()) {\n+        read_stable_mark(object);\n+        continue;\n+      }\n+    }\n+\n+    \/\/ CASE: fast-locked\n+    \/\/ Could be fast-locked either by current or by some other thread.\n+    \/\/\n+    \/\/ Note that we allocate the ObjectMonitor speculatively, _before_\n+    \/\/ attempting to set the object's mark to the new ObjectMonitor. If\n+    \/\/ this thread owns the monitor, then we set the ObjectMonitor's\n+    \/\/ owner to this thread. Otherwise, we set the ObjectMonitor's owner\n+    \/\/ to anonymous. If we lose the race to set the object's mark to the\n+    \/\/ new ObjectMonitor, then we just delete it and loop around again.\n+    \/\/\n+    LogStreamHandle(Trace, monitorinflation) lsh;\n+    if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+      ObjectMonitor* monitor = new ObjectMonitor(object);\n+      monitor->set_header(mark.set_unlocked());\n+      bool own = is_lock_owned(current, object);\n+      if (own) {\n+        \/\/ Owned by us.\n+        monitor->set_owner_from(NULL, current);\n+      } else {\n+        \/\/ Owned by somebody else.\n+        monitor->set_owner_anonymous();\n+      }\n+      markWord monitor_mark = markWord::encode(monitor);\n+      markWord old_mark = object->cas_set_mark(monitor_mark, mark);\n+      if (old_mark == mark) {\n+        \/\/ Success! Return inflated monitor.\n+        if (own) {\n+          assert(current->is_Java_thread(), \"must be Java thread\");\n+          reinterpret_cast<JavaThread*>(current)->lock_stack().remove(object);\n+        }\n+        \/\/ Once the ObjectMonitor is configured and object is associated\n+        \/\/ with the ObjectMonitor, it is safe to allow async deflation:\n+        _in_use_list.add(monitor);\n+\n+        \/\/ Hopefully the performance counters are allocated on distinct\n+        \/\/ cache lines to avoid false sharing on MP systems ...\n+        OM_PERFDATA_OP(Inflations, inc());\n+        if (log_is_enabled(Trace, monitorinflation)) {\n+          ResourceMark rm(current);\n+          lsh.print_cr(\"inflate(has_locker): object=\" INTPTR_FORMAT \", mark=\"\n+                       INTPTR_FORMAT \", type='%s'\", p2i(object),\n+                       object->mark().value(), object->klass()->external_name());\n+        }\n+        if (event.should_commit()) {\n+          post_monitor_inflate_event(&event, object, cause);\n+        }\n+        return monitor;\n+      } else {\n+        delete monitor;\n+        continue;  \/\/ Interference -- just retry\n+      }\n@@ -1290,3 +1467,2 @@\n-    LogStreamHandle(Trace, monitorinflation) lsh;\n-\n-    if (mark.has_locker()) {\n+    if (LockingMode == LM_LEGACY && mark.has_locker()) {\n+      assert(LockingMode != LM_LIGHTWEIGHT, \"cannot happen with new lightweight locking\");\n@@ -1485,0 +1661,10 @@\n+class VM_RendezvousGCThreads : public VM_Operation {\n+public:\n+  bool evaluate_at_safepoint() const override { return false; }\n+  VMOp_Type type() const override { return VMOp_RendezvousGCThreads; }\n+  void doit() override {\n+    SuspendibleThreadSet::synchronize();\n+    SuspendibleThreadSet::desynchronize();\n+  };\n+};\n+\n@@ -1537,0 +1723,3 @@\n+      \/\/ Also, we sync and desync GC threads around the handshake, so that they can\n+      \/\/ safely read the mark-word and look-through to the object-monitor, without\n+      \/\/ being afraid that the object-monitor is going away.\n@@ -1539,0 +1728,2 @@\n+      VM_RendezvousGCThreads sync_gc;\n+      VMThread::execute(&sync_gc);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":298,"deletions":107,"binary":false,"changes":405,"status":"modified"},{"patch":"@@ -92,0 +92,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -95,1 +96,1 @@\n-#include \"runtime\/objectMonitor.hpp\"\n+#include \"runtime\/objectMonitor.inline.hpp\"\n@@ -152,0 +153,3 @@\n+#if INCLUDE_VM_STRUCTS\n+#include \"runtime\/vmStructs.hpp\"\n+#endif\n@@ -708,0 +712,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"should not be called with new lightweight locking\");\n@@ -1097,2 +1102,3 @@\n-  _SleepEvent(ParkEvent::Allocate(this))\n-{\n+  _SleepEvent(ParkEvent::Allocate(this)),\n+\n+  _lock_stack(this) {\n@@ -1576,1 +1582,2 @@\n-  if (Thread::is_lock_owned(adr)) return true;\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"should not be called with new lightweight locking\");\n+ if (Thread::is_lock_owned(adr)) return true;\n@@ -2026,0 +2033,4 @@\n+\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    lock_stack().oops_do(f);\n+  }\n@@ -2854,0 +2865,7 @@\n+  \/\/ Should happen before any agent attaches and pokes into vmStructs\n+#if INCLUDE_VM_STRUCTS\n+  if (UseCompactObjectHeaders) {\n+    VMStructs::compact_headers_overrides();\n+  }\n+#endif\n+\n@@ -3743,0 +3761,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"Not with new lightweight locking\");\n@@ -3772,0 +3791,25 @@\n+JavaThread* Threads::owning_thread_from_object(ThreadsList * t_list, oop obj) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"Only with new lightweight locking\");\n+  DO_JAVA_THREADS(t_list, q) {\n+    if (q->lock_stack().contains(obj)) {\n+      return q;\n+    }\n+  }\n+  return NULL;\n+}\n+\n+JavaThread* Threads::owning_thread_from_monitor(ThreadsList* t_list, ObjectMonitor* monitor) {\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if (monitor->is_owner_anonymous()) {\n+      return owning_thread_from_object(t_list, monitor->object());\n+    } else {\n+      Thread* owner = reinterpret_cast<Thread*>(monitor->owner());\n+      assert(owner == NULL || owner->is_Java_thread(), \"only JavaThreads own monitors\");\n+      return reinterpret_cast<JavaThread*>(owner);\n+    }\n+  } else {\n+    address owner = (address)monitor->owner();\n+    return owning_thread_from_monitor_owner(t_list, owner);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":48,"deletions":4,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"runtime\/lockStack.hpp\"\n@@ -1642,0 +1643,13 @@\n+private:\n+  LockStack _lock_stack;\n+\n+public:\n+  LockStack& lock_stack() { return _lock_stack; }\n+\n+  static ByteSize lock_stack_offset()      { return byte_offset_of(JavaThread, _lock_stack); }\n+  \/\/ Those offsets are used in code generators to access the LockStack that is embedded in this\n+  \/\/ JavaThread structure. Those accesses are relative to the current thread, which\n+  \/\/ is typically in a dedicated register.\n+  static ByteSize lock_stack_top_offset()  { return lock_stack_offset() + LockStack::top_offset(); }\n+  static ByteSize lock_stack_base_offset() { return lock_stack_offset() + LockStack::base_offset(); }\n+\n@@ -1774,0 +1788,3 @@\n+  static JavaThread* owning_thread_from_object(ThreadsList* t_list, oop obj);\n+  static JavaThread* owning_thread_from_monitor(ThreadsList* t_list, ObjectMonitor* owner);\n+\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+  template(HeapObjectStatistics)                  \\\n@@ -94,0 +95,1 @@\n+  template(RendezvousGCThreads)                   \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -145,0 +145,10 @@\n+\/\/ Used by VMStructs when CompactObjectHeaders are enabled.\n+\/\/ Must match the relevant parts from the real oopDesc.\n+class fakeOopDesc {\n+private:\n+  union _metadata {\n+    Klass*      _klass;\n+    narrowKlass _compressed_klass;\n+  } _metadata;\n+};\n+\n@@ -737,0 +747,3 @@\n+  nonstatic_field(JavaThread,                  _lock_stack,                                   LockStack)                             \\\n+  nonstatic_field(LockStack,                   _top,                                          uint32_t)                              \\\n+  nonstatic_field(LockStack,                   _base[0],                                      oop)                                   \\\n@@ -1242,0 +1255,2 @@\n+  declare_toplevel_type(fakeOopDesc)                                      \\\n+                                                                          \\\n@@ -1353,0 +1368,1 @@\n+  declare_toplevel_type(LockStack)                                        \\\n@@ -2474,0 +2490,8 @@\n+  \/**********************************************\/                        \\\n+  \/* LockingMode enum (globalDefinitions.hpp) *\/                          \\\n+  \/**********************************************\/                        \\\n+                                                                          \\\n+  declare_constant(LM_MONITOR)                                            \\\n+  declare_constant(LM_LEGACY)                                             \\\n+  declare_constant(LM_LIGHTWEIGHT)                                        \\\n+                                                                          \\\n@@ -2636,0 +2660,1 @@\n+  declare_constant(markWord::hash_bits_compact)                           \\\n@@ -2641,0 +2666,2 @@\n+  declare_constant(markWord::hash_shift_compact)                          \\\n+  LP64_ONLY(declare_constant(markWord::klass_shift))                      \\\n@@ -2653,0 +2680,2 @@\n+  declare_constant(markWord::hash_mask_compact)                           \\\n+  declare_constant(markWord::hash_mask_compact_in_place)                  \\\n@@ -2668,2 +2697,4 @@\n-  declare_constant(InvocationCounter::count_shift)\n-\n+  declare_constant(InvocationCounter::count_shift)                        \\\n+                                                                          \\\n+  \/* ObjectMonitor constants *\/                                           \\\n+  declare_constant(ObjectMonitor::ANONYMOUS_OWNER)                        \\\n@@ -3189,0 +3220,26 @@\n+\n+void VMStructs::compact_headers_overrides() {\n+  assert(UseCompactObjectHeaders, \"Should have been checked before\");\n+\n+  \/\/ We cannot allow SA and other facilities to poke into VM internal fields\n+  \/\/ expecting the class pointers there. This will crash in the best case,\n+  \/\/ or yield incorrect execution in the worst case. This code hides the\n+  \/\/ risky fields from external code by replacing their original container\n+  \/\/ type to a fake one. The fake type should exist for VMStructs verification\n+  \/\/ code to work.\n+\n+  size_t len = localHotSpotVMStructsLength();\n+  for (size_t off = 0; off < len; off++) {\n+    VMStructEntry* e = &localHotSpotVMStructs[off];\n+    if (e == nullptr) continue;\n+    if (e->typeName == nullptr) continue;\n+    if (e->fieldName == nullptr) continue;\n+\n+    if (strcmp(e->typeName, \"oopDesc\") == 0) {\n+      if ((strcmp(e->fieldName, \"_metadata._klass\") == 0) ||\n+          (strcmp(e->fieldName, \"_metadata._compressed_klass\") == 0)) {\n+        e->typeName = \"fakeOopDesc\";\n+      }\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":59,"deletions":2,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -149,0 +149,3 @@\n+\n+public:\n+  static void compact_headers_overrides() NOT_VM_STRUCTS_RETURN;\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,177 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"logging\/logTag.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"runtime\/vmThread.hpp\"\n+#include \"services\/heapObjectStatistics.hpp\"\n+#include \"utilities\/copy.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+HeapObjectStatistics* HeapObjectStatistics::_instance = NULL;\n+\n+class HeapObjectStatsObjectClosure : public ObjectClosure {\n+private:\n+  HeapObjectStatistics* const _stats;\n+public:\n+  HeapObjectStatsObjectClosure() : _stats(HeapObjectStatistics::instance()) {}\n+  void do_object(oop obj) {\n+    _stats->visit_object(obj);\n+  }\n+};\n+\n+class VM_HeapObjectStatistics : public VM_Operation {\n+public:\n+  VMOp_Type type() const { return VMOp_HeapObjectStatistics; }\n+  bool doit_prologue() {\n+    Heap_lock->lock();\n+    return true;\n+  }\n+\n+  void doit_epilogue() {\n+    Heap_lock->unlock();\n+  }\n+\n+  void doit() {\n+    assert(SafepointSynchronize::is_at_safepoint(), \"all threads are stopped\");\n+    assert(Heap_lock->is_locked(), \"should have the Heap_lock\");\n+\n+    CollectedHeap* heap = Universe::heap();\n+    heap->ensure_parsability(false);\n+\n+    HeapObjectStatistics* stats = HeapObjectStatistics::instance();\n+    stats->begin_sample();\n+\n+    HeapObjectStatsObjectClosure cl;\n+    heap->object_iterate(&cl);\n+  }\n+};\n+\n+HeapObjectStatisticsTask::HeapObjectStatisticsTask() : PeriodicTask(HeapObjectStatsSamplingInterval) {}\n+\n+void HeapObjectStatisticsTask::task() {\n+  VM_HeapObjectStatistics vmop;\n+  VMThread::execute(&vmop);\n+}\n+\n+void HeapObjectStatistics::initialize() {\n+  assert(_instance == NULL, \"Don't init twice\");\n+  if (HeapObjectStats) {\n+    _instance = new HeapObjectStatistics();\n+    _instance->start();\n+  }\n+}\n+\n+void HeapObjectStatistics::shutdown() {\n+  if (HeapObjectStats) {\n+    assert(_instance != NULL, \"Must be initialized\");\n+    LogTarget(Info, heap, stats) lt;\n+    if (lt.is_enabled()) {\n+      LogStream ls(lt);\n+      ResourceMark rm;\n+      _instance->print(&ls);\n+    }\n+    _instance->stop();\n+    delete _instance;\n+    _instance = NULL;\n+  }\n+}\n+\n+HeapObjectStatistics* HeapObjectStatistics::instance() {\n+  assert(_instance != NULL, \"Must be initialized\");\n+  return _instance;\n+}\n+\n+void HeapObjectStatistics::increase_counter(uint64_t& counter, uint64_t val) {\n+  uint64_t oldval = counter;\n+  uint64_t newval = counter + val;\n+  if (newval < oldval) {\n+    log_warning(heap, stats)(\"HeapObjectStats counter overflow: resulting statistics will be useless\");\n+  }\n+  counter = newval;\n+}\n+\n+HeapObjectStatistics::HeapObjectStatistics() :\n+  _task(), _num_samples(0), _num_objects(0), _num_ihashed(0), _num_locked(0), _lds(0) { }\n+\n+void HeapObjectStatistics::start() {\n+  _task.enroll();\n+}\n+\n+void HeapObjectStatistics::stop() {\n+  _task.disenroll();\n+}\n+\n+void HeapObjectStatistics::begin_sample() {\n+  _num_samples++;\n+}\n+\n+void HeapObjectStatistics::visit_object(oop obj) {\n+  increase_counter(_num_objects);\n+  markWord mark = obj->mark();\n+  if (!mark.has_no_hash()) {\n+    increase_counter(_num_ihashed);\n+    if (mark.age() > 0) {\n+      increase_counter(_num_ihashed_moved);\n+    }\n+  }\n+  if (mark.is_locked()) {\n+    increase_counter(_num_locked);\n+  }\n+#ifdef ASSERT\n+#ifdef _LP64\n+  if (!mark.has_displaced_mark_helper()) {\n+    assert(mark.narrow_klass() == CompressedKlassPointers::encode(obj->klass_or_null()), \"upper 32 mark bits must be narrow klass: mark: \" INTPTR_FORMAT \", compressed-klass: \" INTPTR_FORMAT, (intptr_t)mark.narrow_klass(), (intptr_t)CompressedKlassPointers::encode(obj->klass_or_null()));\n+  }\n+#endif\n+#endif\n+  increase_counter(_lds, obj->size());\n+}\n+\n+void HeapObjectStatistics::print(outputStream* out) const {\n+  if (!HeapObjectStats) {\n+    return;\n+  }\n+  if (_num_samples == 0 || _num_objects == 0) {\n+    return;\n+  }\n+\n+  out->print_cr(\"Number of samples:  \" UINT64_FORMAT, _num_samples);\n+  out->print_cr(\"Average number of objects: \" UINT64_FORMAT, _num_objects \/ _num_samples);\n+  out->print_cr(\"Average object size: \" UINT64_FORMAT \" bytes, %.1f words\", (_lds * HeapWordSize) \/ _num_objects, (float) _lds \/ _num_objects);\n+  out->print_cr(\"Average number of hashed objects: \" UINT64_FORMAT \" (%.2f%%)\", _num_ihashed \/ _num_samples, (float) (_num_ihashed * 100.0) \/ _num_objects);\n+  out->print_cr(\"Average number of moved hashed objects: \" UINT64_FORMAT \" (%.2f%%)\", _num_ihashed_moved \/ _num_samples, (float) (_num_ihashed_moved * 100.0) \/ _num_objects);\n+  out->print_cr(\"Average number of locked objects: \" UINT64_FORMAT \" (%.2f%%)\", _num_locked \/ _num_samples, (float) (_num_locked * 100) \/ _num_objects);\n+  out->print_cr(\"Average LDS: \" UINT64_FORMAT \" bytes\", _lds * HeapWordSize \/ _num_samples);\n+  out->print_cr(\"Avg LDS with (assumed) 64bit header: \" UINT64_FORMAT \" bytes (%.1f%%)\", (_lds - _num_objects) * HeapWordSize \/ _num_samples, ((float) _lds - _num_objects) * 100.0 \/ _lds);\n+}\n","filename":"src\/hotspot\/share\/services\/heapObjectStatistics.cpp","additions":177,"deletions":0,"binary":false,"changes":177,"status":"added"},{"patch":"@@ -0,0 +1,71 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_SERVICES_HEAPOBJECTSTATISTICS_HPP\n+#define SHARE_SERVICES_HEAPOBJECTSTATISTICS_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"runtime\/task.hpp\"\n+#include \"runtime\/vmOperation.hpp\"\n+\n+class outputStream;\n+\n+class HeapObjectStatisticsTask : public PeriodicTask {\n+public:\n+  HeapObjectStatisticsTask();\n+  void task();\n+};\n+\n+class HeapObjectStatistics : public CHeapObj<mtGC> {\n+private:\n+  static HeapObjectStatistics* _instance;\n+\n+  HeapObjectStatisticsTask _task;\n+  uint64_t _num_samples;\n+  uint64_t _num_objects;\n+  uint64_t _num_ihashed;\n+  uint64_t _num_ihashed_moved;\n+  uint64_t _num_locked;\n+  uint64_t _lds;\n+\n+  static void increase_counter(uint64_t& counter, uint64_t val = 1);\n+\n+  void print(outputStream* out) const;\n+\n+public:\n+  static void initialize();\n+  static void shutdown();\n+\n+  static HeapObjectStatistics* instance();\n+\n+  HeapObjectStatistics();\n+  void start();\n+  void stop();\n+\n+  void begin_sample();\n+  void visit_object(oop object);\n+};\n+\n+#endif \/\/ SHARE_SERVICES_HEAPOBJECTSTATISTICS_HPP\n","filename":"src\/hotspot\/share\/services\/heapObjectStatistics.hpp","additions":71,"deletions":0,"binary":false,"changes":71,"status":"added"},{"patch":"@@ -433,4 +433,2 @@\n-        address currentOwner = (address)waitingToLockMonitor->owner();\n-        if (currentOwner != NULL) {\n-          currentThread = Threads::owning_thread_from_monitor_owner(t_list,\n-                                                                    currentOwner);\n+        if (waitingToLockMonitor->has_owner()) {\n+          currentThread = Threads::owning_thread_from_monitor(t_list, waitingToLockMonitor);\n@@ -1012,2 +1010,1 @@\n-      currentThread = Threads::owning_thread_from_monitor_owner(t_list,\n-                                                                (address)waitingToLockMonitor->owner());\n+      currentThread = Threads::owning_thread_from_monitor(t_list, waitingToLockMonitor);\n","filename":"src\/hotspot\/share\/services\/threadService.cpp","additions":3,"deletions":6,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -0,0 +1,97 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_FASTHASH_HPP\n+#define SHARE_UTILITIES_FASTHASH_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+\n+class FastHash : public AllStatic {\n+private:\n+  static void fullmul64(uint64_t& hi, uint64_t& lo, uint64_t op1, uint64_t op2) {\n+#if defined(__SIZEOF_INT128__)\n+    __uint128_t prod = static_cast<__uint128_t>(op1) * static_cast<__uint128_t>(op2);\n+    hi = static_cast<uint64_t>(prod >> 64);\n+    lo = static_cast<uint64_t>(prod >>  0);\n+#else\n+    \/* First calculate all of the cross products. *\/\n+    uint64_t lo_lo = (op1 & 0xFFFFFFFF) * (op2 & 0xFFFFFFFF);\n+    uint64_t hi_lo = (op1 >> 32)        * (op2 & 0xFFFFFFFF);\n+    uint64_t lo_hi = (op1 & 0xFFFFFFFF) * (op2 >> 32);\n+    uint64_t hi_hi = (op1 >> 32)        * (op2 >> 32);\n+\n+    \/* Now add the products together. These will never overflow. *\/\n+    uint64_t cross = (lo_lo >> 32) + (hi_lo & 0xFFFFFFFF) + lo_hi;\n+    uint64_t upper = (hi_lo >> 32) + (cross >> 32)        + hi_hi;\n+    hi = upper;\n+    lo = (cross << 32) | (lo_lo & 0xFFFFFFFF);\n+#endif\n+  }\n+\n+  static void fullmul32(uint32_t& hi, uint32_t& lo, uint32_t op1, uint32_t op2) {\n+    uint64_t x64 = op1, y64 = op2, xy64 = x64 * y64;\n+    hi = (uint32_t)(xy64 >> 32);\n+    lo = (uint32_t)(xy64 >>  0);\n+  }\n+\n+  static uint64_t ror(uint64_t x, uint64_t distance) {\n+    distance = distance & 0x3F;\n+    return (x >> distance) | (x << (64 - distance));\n+  }\n+\n+public:\n+  static uint64_t get_hash64(uint64_t x, uint64_t y) {\n+    const uint64_t M  = 0x8ADAE89C337954D5;\n+    const uint64_t A  = 0xAAAAAAAAAAAAAAAA; \/\/ REPAA\n+    const uint64_t H0 = (x ^ y), L0 = (x ^ A);\n+\n+    uint64_t U0, V0; fullmul64(U0, V0, L0, M);\n+    const uint64_t Q0 = (H0 * M);\n+    const uint64_t L1 = (Q0 ^ U0);\n+\n+    uint64_t U1, V1; fullmul64(U1, V1, L1, M);\n+    const uint64_t P1 = (V0 ^ M);\n+    const uint64_t Q1 = ror(P1, L1);\n+    const uint64_t L2 = (Q1 ^ U1);\n+    return V1 ^ L2;\n+  }\n+\n+  static uint32_t get_hash32(uint32_t x, uint32_t y) {\n+    const uint32_t M  = 0x337954D5;\n+    const uint32_t A  = 0xAAAAAAAA; \/\/ REPAA\n+    const uint32_t H0 = (x ^ y), L0 = (x ^ A);\n+\n+    uint32_t U0, V0; fullmul32(U0, V0, L0, M);\n+    const uint32_t Q0 = (H0 * M);\n+    const uint32_t L1 = (Q0 ^ U0);\n+\n+    uint32_t U1, V1; fullmul32(U1, V1, L1, M);\n+    const uint32_t P1 = (V0 ^ M);\n+    const uint32_t Q1 = ror(P1, L1);\n+    const uint32_t L2 = (Q1 ^ U1);\n+    return V1 ^ L2;\n+  }\n+};\n+\n+#endif\/\/ SHARE_UTILITIES_FASTHASH_HPP\n","filename":"src\/hotspot\/share\/utilities\/fastHash.hpp","additions":97,"deletions":0,"binary":false,"changes":97,"status":"added"},{"patch":"@@ -963,0 +963,9 @@\n+enum LockingMode {\n+  \/\/ Use only heavy monitors for locking\n+  LM_MONITOR     = 0,\n+  \/\/ Legacy stack-locking, with monitors as 2nd tier\n+  LM_LEGACY      = 1,\n+  \/\/ New lightweight locking, with monitors as 2nd tier\n+  LM_LIGHTWEIGHT = 2\n+};\n+\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -0,0 +1,125 @@\n+\/*\n+ * Copyright (c) 2014, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_OBJECTBITSET_HPP\n+#define SHARE_UTILITIES_OBJECTBITSET_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"oops\/oop.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"utilities\/bitMap.hpp\"\n+#include \"utilities\/hashtable.hpp\"\n+\n+class MemRegion;\n+\n+\/*\n+ * ObjectBitSet is a sparse bitmap for marking objects in the Java heap.\n+ * It holds one bit per ObjAlignmentInBytes-aligned address. Its underlying backing memory is\n+ * allocated on-demand only, in fragments covering 64M heap ranges. Fragments are never deleted\n+ * during the lifetime of the ObjectBitSet. The underlying memory is allocated from C-Heap.\n+ *\/\n+template<MEMFLAGS F>\n+class ObjectBitSet : public CHeapObj<F> {\n+  const static size_t _bitmap_granularity_shift = 26; \/\/ 64M\n+  const static size_t _bitmap_granularity_size = (size_t)1 << _bitmap_granularity_shift;\n+  const static size_t _bitmap_granularity_mask = _bitmap_granularity_size - 1;\n+\n+  class BitMapFragment;\n+\n+  class BitMapFragmentTable : public BasicHashtable<F> {\n+    class Entry : public BasicHashtableEntry<F> {\n+    public:\n+      uintptr_t _key;\n+      CHeapBitMap* _value;\n+\n+      Entry* next() {\n+        return (Entry*)BasicHashtableEntry<F>::next();\n+      }\n+    };\n+\n+  protected:\n+    Entry* bucket(int i) const;\n+\n+    Entry* new_entry(unsigned int hashValue, uintptr_t key, CHeapBitMap* value);\n+\n+    unsigned hash_segment(uintptr_t key) {\n+      unsigned hash = (unsigned)key;\n+      return hash ^ (hash >> 3);\n+    }\n+\n+    unsigned hash_to_index(unsigned hash) {\n+      return hash & (BasicHashtable<F>::table_size() - 1);\n+    }\n+\n+  public:\n+    BitMapFragmentTable(int table_size) : BasicHashtable<F>(table_size, sizeof(Entry)) {}\n+    ~BitMapFragmentTable();\n+    void add(uintptr_t key, CHeapBitMap* value);\n+    CHeapBitMap** lookup(uintptr_t key);\n+  };\n+\n+  CHeapBitMap* get_fragment_bits(uintptr_t addr);\n+\n+  BitMapFragmentTable _bitmap_fragments;\n+  BitMapFragment* _fragment_list;\n+  CHeapBitMap* _last_fragment_bits;\n+  uintptr_t _last_fragment_granule;\n+\n+ public:\n+  ObjectBitSet();\n+  ~ObjectBitSet();\n+\n+  BitMap::idx_t addr_to_bit(uintptr_t addr) const;\n+\n+  void mark_obj(uintptr_t addr);\n+\n+  void mark_obj(oop obj) {\n+    return mark_obj(cast_from_oop<uintptr_t>(obj));\n+  }\n+\n+  bool is_marked(uintptr_t addr);\n+\n+  bool is_marked(oop obj) {\n+    return is_marked(cast_from_oop<uintptr_t>(obj));\n+  }\n+};\n+\n+template<MEMFLAGS F>\n+class ObjectBitSet<F>::BitMapFragment : public CHeapObj<F> {\n+  CHeapBitMap _bits;\n+  BitMapFragment* _next;\n+\n+public:\n+  BitMapFragment(uintptr_t granule, BitMapFragment* next);\n+\n+  BitMapFragment* next() const {\n+    return _next;\n+  }\n+\n+  CHeapBitMap* bits() {\n+    return &_bits;\n+  }\n+};\n+\n+#endif \/\/ SHARE_UTILITIES_OBJECTBITSET_HPP\n","filename":"src\/hotspot\/share\/utilities\/objectBitSet.hpp","additions":125,"deletions":0,"binary":false,"changes":125,"status":"added"},{"patch":"@@ -0,0 +1,150 @@\n+\/*\n+ * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_OBJECTBITSET_INLINE_HPP\n+#define SHARE_UTILITIES_OBJECTBITSET_INLINE_HPP\n+\n+#include \"utilities\/objectBitSet.hpp\"\n+\n+#include \"memory\/memRegion.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n+#include \"utilities\/hashtable.inline.hpp\"\n+\n+template<MEMFLAGS F>\n+ObjectBitSet<F>::BitMapFragment::BitMapFragment(uintptr_t granule, BitMapFragment* next) :\n+        _bits(_bitmap_granularity_size >> LogMinObjAlignmentInBytes, F, true \/* clear *\/),\n+        _next(next) {\n+}\n+\n+template<MEMFLAGS F>\n+ObjectBitSet<F>::ObjectBitSet() :\n+        _bitmap_fragments(32),\n+        _fragment_list(NULL),\n+        _last_fragment_bits(NULL),\n+        _last_fragment_granule(UINTPTR_MAX) {\n+}\n+\n+template<MEMFLAGS F>\n+ObjectBitSet<F>::~ObjectBitSet() {\n+  BitMapFragment* current = _fragment_list;\n+  while (current != NULL) {\n+    BitMapFragment* next = current->next();\n+    delete current;\n+    current = next;\n+  }\n+}\n+\n+template<MEMFLAGS F>\n+ObjectBitSet<F>::BitMapFragmentTable::~BitMapFragmentTable() {\n+  for (int index = 0; index < BasicHashtable<F>::table_size(); index ++) {\n+    Entry* e = bucket(index);\n+    while (e != nullptr) {\n+      Entry* tmp = e;\n+      e = e->next();\n+      BasicHashtable<F>::free_entry(tmp);\n+    }\n+  }\n+}\n+\n+template<MEMFLAGS F>\n+inline typename ObjectBitSet<F>::BitMapFragmentTable::Entry* ObjectBitSet<F>::BitMapFragmentTable::bucket(int i) const {\n+  return (Entry*)BasicHashtable<F>::bucket(i);\n+}\n+\n+template<MEMFLAGS F>\n+inline typename ObjectBitSet<F>::BitMapFragmentTable::Entry*\n+  ObjectBitSet<F>::BitMapFragmentTable::new_entry(unsigned int hash, uintptr_t key, CHeapBitMap* value) {\n+\n+  Entry* entry = (Entry*)BasicHashtable<F>::new_entry(hash);\n+  entry->_key = key;\n+  entry->_value = value;\n+  return entry;\n+}\n+\n+template<MEMFLAGS F>\n+inline void ObjectBitSet<F>::BitMapFragmentTable::add(uintptr_t key, CHeapBitMap* value) {\n+  unsigned hash = hash_segment(key);\n+  Entry* entry = new_entry(hash, key, value);\n+  BasicHashtable<F>::add_entry(hash_to_index(hash), entry);\n+}\n+\n+template<MEMFLAGS F>\n+inline CHeapBitMap** ObjectBitSet<F>::BitMapFragmentTable::lookup(uintptr_t key) {\n+  unsigned hash = hash_segment(key);\n+  int index = hash_to_index(hash);\n+  for (Entry* e = bucket(index); e != NULL; e = e->next()) {\n+    if (e->hash() == hash && e->_key == key) {\n+      return &(e->_value);\n+    }\n+  }\n+  return NULL;\n+}\n+\n+template<MEMFLAGS F>\n+inline BitMap::idx_t ObjectBitSet<F>::addr_to_bit(uintptr_t addr) const {\n+  return (addr & _bitmap_granularity_mask) >> LogMinObjAlignmentInBytes;\n+}\n+\n+template<MEMFLAGS F>\n+inline CHeapBitMap* ObjectBitSet<F>::get_fragment_bits(uintptr_t addr) {\n+  uintptr_t granule = addr >> _bitmap_granularity_shift;\n+  if (granule == _last_fragment_granule) {\n+    return _last_fragment_bits;\n+  }\n+  CHeapBitMap* bits = NULL;\n+\n+  CHeapBitMap** found = _bitmap_fragments.lookup(granule);\n+  if (found != NULL) {\n+    bits = *found;\n+  } else {\n+    BitMapFragment* fragment = new BitMapFragment(granule, _fragment_list);\n+    bits = fragment->bits();\n+    _fragment_list = fragment;\n+    if (_bitmap_fragments.number_of_entries() * 100 \/ _bitmap_fragments.table_size() > 25) {\n+      _bitmap_fragments.resize(_bitmap_fragments.table_size() * 2);\n+    }\n+    _bitmap_fragments.add(granule, bits);\n+  }\n+\n+  _last_fragment_bits = bits;\n+  _last_fragment_granule = granule;\n+\n+  return bits;\n+}\n+\n+template<MEMFLAGS F>\n+inline void ObjectBitSet<F>::mark_obj(uintptr_t addr) {\n+  CHeapBitMap* bits = get_fragment_bits(addr);\n+  const BitMap::idx_t bit = addr_to_bit(addr);\n+  bits->set_bit(bit);\n+}\n+\n+template<MEMFLAGS F>\n+inline bool ObjectBitSet<F>::is_marked(uintptr_t addr) {\n+  CHeapBitMap* bits = get_fragment_bits(addr);\n+  const BitMap::idx_t bit = addr_to_bit(addr);\n+  return bits->at(bit);\n+}\n+\n+#endif \/\/ SHARE_UTILITIES_OBJECTBITSET_INLINE_HPP\n","filename":"src\/hotspot\/share\/utilities\/objectBitSet.inline.hpp","additions":150,"deletions":0,"binary":false,"changes":150,"status":"added"},{"patch":"@@ -27,0 +27,3 @@\n+import sun.jvm.hotspot.oops.Mark;\n+import sun.jvm.hotspot.runtime.VM;\n+\n@@ -477,1 +480,9 @@\n-    long value = readCInteger(address, getKlassPtrSize(), true);\n+    long value;\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      \/\/ On 64 bit systems, the compressed Klass* is currently read from the mark\n+      \/\/ word. We need to load the whole mark, and shift the upper parts.\n+      value = readCInteger(address, machDesc.getAddressSize(), true);\n+      value = value >>> Mark.getKlassShift();\n+    } else {\n+      value = readCInteger(address, getKlassPtrSize(), true);\n+    }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/debugger\/DebuggerBase.java","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -117,7 +117,0 @@\n-  \/\/ Check whether an element of a typeArrayOop with the given type must be\n-  \/\/ aligned 0 mod 8.  The typeArrayOop itself must be aligned at least this\n-  \/\/ strongly.\n-  public static boolean elementTypeShouldBeAligned(BasicType type) {\n-    return type == BasicType.T_DOUBLE || type == BasicType.T_LONG;\n-  }\n-\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/memory\/Universe.java","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -60,0 +60,12 @@\n+  \/\/ Check whether an element of a typeArrayOop with the given type must be\n+  \/\/ aligned 0 mod 8.  The typeArrayOop itself must be aligned at least this\n+  \/\/ strongly.\n+  public static boolean elementTypeShouldBeAligned(BasicType type) {\n+    if (VM.getVM().isLP64()) {\n+      if (type == BasicType.T_OBJECT || type == BasicType.T_ARRAY) {\n+        return !VM.getVM().isCompressedOopsEnabled();\n+      }\n+    }\n+    return type == BasicType.T_DOUBLE || type == BasicType.T_LONG;\n+  }\n+\n@@ -64,2 +76,2 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-      headerSize = typeSize;\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      headerSize = lengthOffsetInBytes() + VM.getVM().getIntSize();\n@@ -67,2 +79,6 @@\n-      headerSize = VM.getVM().alignUp(typeSize + VM.getVM().getIntSize(),\n-                                      VM.getVM().getHeapWordSize());\n+      if (VM.getVM().isCompressedKlassPointersEnabled()) {\n+        headerSize = typeSize;\n+      } else {\n+        headerSize = VM.getVM().alignUp(typeSize + VM.getVM().getIntSize(),\n+                                        VM.getVM().getHeapWordSize());\n+      }\n@@ -74,6 +90,6 @@\n-    if (Universe.elementTypeShouldBeAligned(type)) {\n-       return alignObjectSize(headerSizeInBytes())\/VM.getVM().getHeapWordSize();\n-    } else {\n-      return headerSizeInBytes()\/VM.getVM().getHeapWordSize();\n-    }\n-  }\n+     if (elementTypeShouldBeAligned(type)) {\n+        return alignObjectSize(headerSizeInBytes())\/VM.getVM().getHeapWordSize();\n+     } else {\n+       return headerSizeInBytes()\/VM.getVM().getHeapWordSize();\n+     }\n+   }\n@@ -81,1 +97,1 @@\n-  private long lengthOffsetInBytes() {\n+  private static long lengthOffsetInBytes() {\n@@ -85,2 +101,2 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-      lengthOffsetInBytes = typeSize - VM.getVM().getIntSize();\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      lengthOffsetInBytes = Oop.getHeaderSize();\n@@ -88,1 +104,5 @@\n-      lengthOffsetInBytes = typeSize;\n+      if (VM.getVM().isCompressedKlassPointersEnabled()) {\n+        lengthOffsetInBytes = typeSize - VM.getVM().getIntSize();\n+      } else {\n+        lengthOffsetInBytes = typeSize;\n+      }\n@@ -111,1 +131,11 @@\n-    return headerSize(type) * VM.getVM().getHeapWordSize();\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      long typeSizeInBytes = headerSizeInBytes();\n+      if (elementTypeShouldBeAligned(type)) {\n+        VM vm = VM.getVM();\n+        return vm.alignUp(typeSizeInBytes, vm.getVM().getHeapWordSize());\n+      } else {\n+        return typeSizeInBytes;\n+      }\n+    } else {\n+      return headerSize(type) * VM.getVM().getHeapWordSize();\n+    }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Array.java","additions":45,"deletions":15,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -58,0 +58,3 @@\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      return Oop.getHeaderSize();\n+    }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Instance.java","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+    hashBitsCompact     = db.lookupLongConstant(\"markWord::hash_bits_compact\").longValue();\n@@ -59,0 +60,4 @@\n+    hashShiftCompact    = db.lookupLongConstant(\"markWord::hash_shift_compact\").longValue();\n+    if (VM.getVM().isLP64()) {\n+      klassShift          = db.lookupLongConstant(\"markWord::klass_shift\").longValue();\n+    }\n@@ -68,0 +73,2 @@\n+    hashMaskCompact     = db.lookupLongConstant(\"markWord::hash_mask_compact\").longValue();\n+    hashMaskCompactInPlace = db.lookupLongConstant(\"markWord::hash_mask_compact_in_place\").longValue();\n@@ -89,0 +96,1 @@\n+  private static long hashBitsCompact;\n@@ -94,0 +102,2 @@\n+  private static long hashShiftCompact;\n+  private static long klassShift;\n@@ -104,0 +114,2 @@\n+  private static long hashMaskCompact;\n+  private static long hashMaskCompactInPlace;\n@@ -124,0 +136,4 @@\n+  public static long getKlassShift() {\n+    return klassShift;\n+  }\n+\n@@ -218,1 +234,5 @@\n-    return Bits.maskBitsLong(value() >> hashShift, hashMask);\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      return Bits.maskBitsLong(value() >> hashShiftCompact, hashMaskCompact);\n+    } else {\n+      return Bits.maskBitsLong(value() >> hashShift, hashMask);\n+    }\n@@ -225,0 +245,6 @@\n+  public Klass getKlass() {\n+    assert(VM.getVM().isCompactObjectHeadersEnabled());\n+    assert(!hasMonitor());\n+    return (Klass)Metadata.instantiateWrapperFor(addr.getCompKlassAddressAt(0));\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Mark.java","additions":27,"deletions":1,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -51,3 +51,8 @@\n-    klass      = new MetadataField(type.getAddressField(\"_metadata._klass\"), 0);\n-    compressedKlass  = new NarrowKlassField(type.getAddressField(\"_metadata._compressed_klass\"), 0);\n-    headerSize = type.getSize();\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      Type markType = db.lookupType(\"markWord\");\n+      headerSize = markType.getSize();\n+    } else {\n+      headerSize = type.getSize();\n+      klass      = new MetadataField(type.getAddressField(\"_metadata._klass\"), 0);\n+      compressedKlass  = new NarrowKlassField(type.getAddressField(\"_metadata._compressed_klass\"), 0);\n+    }\n@@ -80,0 +85,10 @@\n+\n+  private static Klass getKlass(Mark mark) {\n+    assert(VM.getVM().isCompactObjectHeadersEnabled());\n+    if (mark.hasMonitor()) {\n+      ObjectMonitor mon = mark.monitor();\n+      mark = mon.header();\n+    }\n+    return mark.getKlass();\n+  }\n+\n@@ -81,1 +96,4 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      assert(VM.getVM().isCompressedKlassPointersEnabled());\n+      return getKlass(getMark());\n+    } else if (VM.getVM().isCompressedKlassPointersEnabled()) {\n@@ -152,4 +170,6 @@\n-      if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-        visitor.doMetadata(compressedKlass, true);\n-      } else {\n-        visitor.doMetadata(klass, true);\n+      if (!VM.getVM().isCompactObjectHeadersEnabled()) {\n+        if (VM.getVM().isCompressedKlassPointersEnabled()) {\n+          visitor.doMetadata(compressedKlass, true);\n+        } else {\n+          visitor.doMetadata(klass, true);\n+        }\n@@ -211,1 +231,4 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      Mark mark = new Mark(handle);\n+      return getKlass(mark);\n+    } else if (VM.getVM().isCompressedKlassPointersEnabled()) {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Oop.java","additions":32,"deletions":9,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -47,0 +47,2 @@\n+  private static long          lockStackTopOffset;\n+  private static long          lockStackBaseOffset;\n@@ -55,0 +57,1 @@\n+  private static long oopPtrSize;\n@@ -87,0 +90,1 @@\n+    Type typeLockStack = db.lookupType(\"LockStack\");\n@@ -99,0 +103,4 @@\n+    lockStackTopOffset = type.getField(\"_lock_stack\").getOffset() + typeLockStack.getField(\"_top\").getOffset();\n+    lockStackBaseOffset = type.getField(\"_lock_stack\").getOffset() + typeLockStack.getField(\"_base[0]\").getOffset();\n+    oopPtrSize = VM.getVM().getAddressSize();\n+\n@@ -395,0 +403,17 @@\n+  public boolean isLockOwned(OopHandle obj) {\n+    long current = lockStackBaseOffset;\n+    long end = addr.getJIntAt(lockStackTopOffset);\n+    if (Assert.ASSERTS_ENABLED) {\n+      Assert.that(current <= end, \"current stack offset must be above base offset\");\n+    }\n+\n+    while (current < end) {\n+      Address oop = addr.getAddressAt(current);\n+      if (oop.equals(obj)) {\n+        return true;\n+      }\n+      current += oopPtrSize;\n+    }\n+    return false;\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/JavaThread.java","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -80,0 +80,4 @@\n+          \/\/ Owned anonymously means that we are not the owner of\n+          \/\/ the monitor and must be waiting for the owner to\n+          \/\/ exit it.\n+          mark.monitor().isOwnedAnonymous() ||\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/JavaVFrame.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,60 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.runtime;\n+\n+import sun.jvm.hotspot.types.TypeDataBase;\n+\n+\n+\/** Encapsulates the LockingMode enum in globalDefinitions.hpp in\n+    the VM. *\/\n+\n+public class LockingMode {\n+  private static int monitor;\n+  private static int legacy;\n+  private static int lightweight;\n+\n+  static {\n+    VM.registerVMInitializedObserver(\n+        (o, d) -> initialize(VM.getVM().getTypeDataBase()));\n+  }\n+\n+  private static synchronized void initialize(TypeDataBase db) {\n+    monitor     = db.lookupIntConstant(\"LM_MONITOR\").intValue();\n+    legacy      = db.lookupIntConstant(\"LM_LEGACY\").intValue();\n+    lightweight = db.lookupIntConstant(\"LM_LIGHTWEIGHT\").intValue();\n+  }\n+\n+  public static int getMonitor() {\n+    return monitor;\n+  }\n+\n+  public static int getLegacy() {\n+    return legacy;\n+  }\n+\n+  public static int getLightweight() {\n+    return lightweight;\n+  }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/LockingMode.java","additions":60,"deletions":0,"binary":false,"changes":60,"status":"added"},{"patch":"@@ -58,0 +58,2 @@\n+\n+    ANONYMOUS_OWNER = db.lookupLongConstant(\"ObjectMonitor::ANONYMOUS_OWNER\").longValue();\n@@ -82,0 +84,4 @@\n+  public boolean isOwnedAnonymous() {\n+    return addr.getAddressAt(ownerFieldOffset).asLongValue() == ANONYMOUS_OWNER;\n+  }\n+\n@@ -117,0 +123,2 @@\n+  private static long          ANONYMOUS_OWNER;\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/ObjectMonitor.java","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -214,0 +214,1 @@\n+        assert(VM.getVM().getCommandLineFlag(\"LockingMode\").getInt() != LockingMode.getLightweight());\n@@ -231,1 +232,18 @@\n-        return owningThreadFromMonitor(monitor.owner());\n+        if (VM.getVM().getCommandLineFlag(\"LockingMode\").getInt() == LockingMode.getLightweight()) {\n+            if (monitor.isOwnedAnonymous()) {\n+                OopHandle object = monitor.object();\n+                for (int i = 0; i < getNumberOfThreads(); i++) {\n+                    JavaThread thread = getJavaThreadAt(i);\n+                    if (thread.isLockOwned(object)) {\n+                        return thread;\n+                     }\n+                }\n+                throw new InternalError(\"We should have found a thread that owns the anonymous lock\");\n+            }\n+            \/\/ Owner can only be threads at this point.\n+            Address o = monitor.owner();\n+            if (o == null) return null;\n+            return new JavaThread(o);\n+        } else {\n+            return owningThreadFromMonitor(monitor.owner());\n+        }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/Threads.java","additions":19,"deletions":1,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -150,0 +150,1 @@\n+  private Boolean compactObjectHeadersEnabled;\n@@ -963,0 +964,9 @@\n+  public boolean isCompactObjectHeadersEnabled() {\n+    if (compactObjectHeadersEnabled == null) {\n+      Flag flag = getCommandLineFlag(\"UseCompactObjectHeaders\");\n+      compactObjectHeadersEnabled = (flag == null) ? Boolean.FALSE:\n+                     (flag.getBool()? Boolean.TRUE: Boolean.FALSE);\n+    }\n+    return compactObjectHeadersEnabled.booleanValue();\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/VM.java","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+import sun.jvm.hotspot.oops.Oop;\n@@ -45,19 +46,0 @@\n-  private static AddressField klassField;\n-\n-  static {\n-    VM.registerVMInitializedObserver(new Observer() {\n-        public void update(Observable o, Object data) {\n-          initialize(VM.getVM().getTypeDataBase());\n-        }\n-      });\n-  }\n-\n-  private static void initialize(TypeDataBase db) {\n-    Type type = db.lookupType(\"oopDesc\");\n-\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-      klassField = type.getAddressField(\"_metadata._compressed_klass\");\n-    } else {\n-      klassField = type.getAddressField(\"_metadata._klass\");\n-    }\n-  }\n@@ -74,5 +56,1 @@\n-      if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-        Metadata.instantiateWrapperFor(oop.getCompKlassAddressAt(klassField.getOffset()));\n-      } else {\n-        Metadata.instantiateWrapperFor(klassField.getValue(oop));\n-      }\n+      Oop.getKlassForOopHandle(oop);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/utilities\/RobustOopDeterminator.java","additions":2,"deletions":24,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -69,0 +70,2 @@\n+  FlagSetting fs(UseAltGCForwarding, false);\n+\n","filename":"test\/hotspot\/gtest\/gc\/shared\/test_preservedMarks.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,124 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"unittest.hpp\"\n+\n+#ifdef _LP64\n+#ifndef PRODUCT\n+\n+static uintptr_t make_mark(uintptr_t target_region, uintptr_t offset) {\n+  return (target_region) << 3 | (offset << 4) | 3 \/* forwarded *\/;\n+}\n+\n+static uintptr_t make_fallback() {\n+  return ((uintptr_t(1) << 2) \/* fallback *\/ | 3 \/* forwarded *\/);\n+}\n+\n+\/\/ Test simple forwarding within the same region.\n+TEST_VM(SlidingForwarding, simple) {\n+  FlagSetting fs(UseAltGCForwarding, true);\n+  HeapWord fakeheap[32] = { nullptr };\n+  HeapWord* heap = align_up(fakeheap, 8 * sizeof(HeapWord));\n+  oop obj1 = cast_to_oop(&heap[2]);\n+  oop obj2 = cast_to_oop(&heap[0]);\n+  SlidingForwarding::initialize(MemRegion(&heap[0], &heap[16]), 8);\n+  obj1->set_mark(markWord::prototype());\n+  SlidingForwarding::begin();\n+\n+  SlidingForwarding::forward_to<true>(obj1, obj2);\n+  ASSERT_EQ(obj1->mark().value(), make_mark(0 \/* target_region *\/, 0 \/* offset *\/));\n+  ASSERT_EQ(SlidingForwarding::forwardee<true>(obj1), obj2);\n+\n+  SlidingForwarding::end();\n+}\n+\n+\/\/ Test forwardings crossing 2 regions.\n+TEST_VM(SlidingForwarding, tworegions) {\n+  FlagSetting fs(UseAltGCForwarding, true);\n+  HeapWord fakeheap[32] = { nullptr };\n+  HeapWord* heap = align_up(fakeheap, 8 * sizeof(HeapWord));\n+  oop obj1 = cast_to_oop(&heap[14]);\n+  oop obj2 = cast_to_oop(&heap[2]);\n+  oop obj3 = cast_to_oop(&heap[10]);\n+  SlidingForwarding::initialize(MemRegion(&heap[0], &heap[16]), 8);\n+  obj1->set_mark(markWord::prototype());\n+  SlidingForwarding::begin();\n+\n+  SlidingForwarding::forward_to<true>(obj1, obj2);\n+  ASSERT_EQ(obj1->mark().value(), make_mark(0 \/* target_region *\/, 2 \/* offset *\/));\n+  ASSERT_EQ(SlidingForwarding::forwardee<true>(obj1), obj2);\n+\n+  SlidingForwarding::forward_to<true>(obj1, obj3);\n+  ASSERT_EQ(obj1->mark().value(), make_mark(1 \/* target_region *\/, 2 \/* offset *\/));\n+  ASSERT_EQ(SlidingForwarding::forwardee<true>(obj1), obj3);\n+\n+  SlidingForwarding::end();\n+}\n+\n+\/\/ Test fallback forwardings crossing 4 regions.\n+TEST_VM(SlidingForwarding, fallback) {\n+  FlagSetting fs(UseAltGCForwarding, true);\n+  HeapWord fakeheap[32] = { nullptr };\n+  HeapWord* heap = align_up(fakeheap, 8 * sizeof(HeapWord));\n+  oop s_obj1 = cast_to_oop(&heap[12]);\n+  oop s_obj2 = cast_to_oop(&heap[13]);\n+  oop s_obj3 = cast_to_oop(&heap[14]);\n+  oop s_obj4 = cast_to_oop(&heap[15]);\n+  oop t_obj1 = cast_to_oop(&heap[2]);\n+  oop t_obj2 = cast_to_oop(&heap[4]);\n+  oop t_obj3 = cast_to_oop(&heap[10]);\n+  oop t_obj4 = cast_to_oop(&heap[12]);\n+  SlidingForwarding::initialize(MemRegion(&heap[0], &heap[16]), 4);\n+  s_obj1->set_mark(markWord::prototype());\n+  s_obj2->set_mark(markWord::prototype());\n+  s_obj3->set_mark(markWord::prototype());\n+  s_obj4->set_mark(markWord::prototype());\n+  SlidingForwarding::begin();\n+\n+  SlidingForwarding::forward_to<true>(s_obj1, t_obj1);\n+  ASSERT_EQ(s_obj1->mark().value(), make_mark(0 \/* target_region *\/, 2 \/* offset *\/));\n+  ASSERT_EQ(SlidingForwarding::forwardee<true>(s_obj1), t_obj1);\n+\n+  SlidingForwarding::forward_to<true>(s_obj2, t_obj2);\n+  ASSERT_EQ(s_obj2->mark().value(), make_mark(1 \/* target_region *\/, 0 \/* offset *\/));\n+  ASSERT_EQ(SlidingForwarding::forwardee<true>(s_obj2), t_obj2);\n+\n+  SlidingForwarding::forward_to<true>(s_obj3, t_obj3);\n+  ASSERT_EQ(s_obj3->mark().value(), make_fallback());\n+  ASSERT_EQ(SlidingForwarding::forwardee<true>(s_obj3), t_obj3);\n+\n+  SlidingForwarding::forward_to<true>(s_obj4, t_obj4);\n+  ASSERT_EQ(s_obj4->mark().value(), make_fallback());\n+  ASSERT_EQ(SlidingForwarding::forwardee<true>(s_obj4), t_obj4);\n+\n+  SlidingForwarding::end();\n+}\n+\n+#endif \/\/ PRODUCT\n+#endif \/\/ _LP64\n","filename":"test\/hotspot\/gtest\/gc\/shared\/test_slidingForwarding.cpp","additions":124,"deletions":0,"binary":false,"changes":124,"status":"added"},{"patch":"@@ -39,1 +39,8 @@\n-  o->set_klass(Universe::boolArrayKlassObj());\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    o->set_mark(Universe::boolArrayKlassObj()->prototype_header());\n+  } else\n+#endif\n+  {\n+    o->set_klass(Universe::boolArrayKlassObj());\n+  }\n","filename":"test\/hotspot\/gtest\/oops\/test_typeArrayOop.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -0,0 +1,66 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"utilities\/objectBitSet.inline.hpp\"\n+#include \"unittest.hpp\"\n+\n+TEST_VM(ObjectBitSet, empty) {\n+  ObjectBitSet<mtTracing> obs;\n+  oopDesc obj1;\n+  ASSERT_FALSE(obs.is_marked(&obj1));\n+}\n+\n+\/\/ NOTE: This is a little weird. NULL is not treated any special: ObjectBitSet will happily\n+\/\/ allocate a fragement for the memory range starting at 0 and mark the first bit when passing NULL.\n+\/\/ In the absense of any error handling, I am not sure what would possibly be a reasonable better\n+\/\/ way to do it, though.\n+TEST_VM(ObjectBitSet, null) {\n+  ObjectBitSet<mtTracing> obs;\n+  ASSERT_FALSE(obs.is_marked((oop)NULL));\n+  obs.mark_obj((oop) NULL);\n+  ASSERT_TRUE(obs.is_marked((oop)NULL));\n+}\n+\n+TEST_VM(ObjectBitSet, mark_single) {\n+  ObjectBitSet<mtTracing> obs;\n+  oopDesc obj1;\n+  ASSERT_FALSE(obs.is_marked(&obj1));\n+  obs.mark_obj(&obj1);\n+  ASSERT_TRUE(obs.is_marked(&obj1));\n+}\n+\n+TEST_VM(ObjectBitSet, mark_multi) {\n+  ObjectBitSet<mtTracing> obs;\n+  oopDesc obj1;\n+  oopDesc obj2;\n+  ASSERT_FALSE(obs.is_marked(&obj1));\n+  ASSERT_FALSE(obs.is_marked(&obj2));\n+  obs.mark_obj(&obj1);\n+  ASSERT_TRUE(obs.is_marked(&obj1));\n+  ASSERT_FALSE(obs.is_marked(&obj2));\n+  obs.mark_obj(&obj2);\n+  ASSERT_TRUE(obs.is_marked(&obj1));\n+  ASSERT_TRUE(obs.is_marked(&obj2));\n+}\n","filename":"test\/hotspot\/gtest\/utilities\/test_objectBitSet.cpp","additions":66,"deletions":0,"binary":false,"changes":66,"status":"added"},{"patch":"@@ -0,0 +1,52 @@\n+#\n+# Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+#\n+# This code is free software; you can redistribute it and\/or modify it\n+# under the terms of the GNU General Public License version 2 only, as\n+# published by the Free Software Foundation.\n+#\n+# This code is distributed in the hope that it will be useful, but WITHOUT\n+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+# version 2 for more details (a copy is included in the LICENSE file that\n+# accompanied this code).\n+#\n+# You should have received a copy of the GNU General Public License version\n+# 2 along with this work; if not, write to the Free Software Foundation,\n+# Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+#\n+# Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+# or visit www.oracle.com if you need additional information or have any\n+# questions.\n+#\n+\n+#\n+# These tests are problematic when +UseCompactObjectHeaders is enabled.\n+# The test exclusions are for the cases when we are sure the tests would fail\n+# for the known and innocuous implementation reasons.\n+#\n+\n+\n+#\n+# Tests require specific UseCompressedClassPointers mode\n+#\n+gc\/arguments\/TestCompressedClassFlags.java                                          1234567 generic-all\n+\n+\n+#\n+# Tests require specific UseBiasedLocking mode\n+#\n+runtime\/logging\/BiasedLockingTest.java                                              8256425 generic-all\n+compiler\/rtm\/cli\/TestUseRTMLockingOptionWithBiasedLocking.java                      8256425 generic-x64,generic-i586\n+jdk\/jfr\/event\/runtime\/TestBiasedLockRevocationEvents.java                           8256425 generic-all\n+\n+\n+#\n+# Test library tests do not like non-whitelisted +UCOH flag\n+#\n+testlibrary_tests\/ir_framework\/tests\/TestCheckedTests.java                          1234567 generic-all\n+testlibrary_tests\/ir_framework\/tests\/TestBadFormat.java                             1234567 generic-all\n+testlibrary_tests\/ir_framework\/tests\/TestRunTests.java                              1234567 generic-all\n+testlibrary_tests\/ir_framework\/tests\/TestScenarios.java                             1234567 generic-all\n+testlibrary_tests\/ir_framework\/tests\/TestIRMatching.java                            1234567 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList-lilliput.txt","additions":52,"deletions":0,"binary":false,"changes":52,"status":"added"},{"patch":"@@ -48,0 +48,1 @@\n+import jdk.test.lib.Platform;\n@@ -75,1 +76,1 @@\n-    private static final int OBJECT_SIZE_HIGH = 3250;\n+    private static final int OBJECT_SIZE_HIGH = Platform.is64bit() ? 3266 : 3258;\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/plab\/TestPLABPromotion.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @test TestSystemGCWithG1\n+ * @test id=default\n@@ -36,0 +36,11 @@\n+\n+\/*\n+ * @test id=alt-forwarding\n+ * @key stress\n+ * @bug 8190703\n+ * @library \/\n+ * @requires vm.gc.G1\n+ * @requires (vm.bits == \"64\")\n+ * @summary Stress the G1 GC full GC by allocating objects of different lifetimes concurrently with System.gc().\n+ * @run main\/othervm\/timeout=300 -XX:+UnlockExperimentalVMOptions -XX:+UseAltGCForwarding -Xlog:gc*=info -Xmx512m -XX:+UseG1GC gc.stress.systemgc.TestSystemGCWithG1 270\n+ *\/\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/systemgc\/TestSystemGCWithG1.java","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @test TestSystemGCWithSerial\n+ * @test id=default\n@@ -36,0 +36,31 @@\n+\n+\/*\n+ * @test id=alt-forwarding\n+ * @key stress\n+ * @bug 8190703\n+ * @library \/\n+ * @requires vm.gc.Serial\n+ * @summary Stress the Serial GC full GC by allocating objects of different lifetimes concurrently with System.gc().\n+ * @run main\/othervm\/timeout=300 -XX:+UnlockExperimentalVMOptions -XX:+UseAltGCForwarding -Xlog:gc*=info -Xmx512m -XX:+UseSerialGC gc.stress.systemgc.TestSystemGCWithSerial 270\n+ *\/\n+\n+\/*\n+ * @test id=alt-forwarding-unaligned\n+ * @key stress\n+ * @bug 8190703\n+ * @library \/\n+ * @requires vm.gc.Serial\n+ * @summary Stress the Serial GC full GC by allocating objects of different lifetimes concurrently with System.gc().\n+ * @run main\/othervm\/timeout=300 -XX:+UnlockExperimentalVMOptions -XX:+UseAltGCForwarding -Xlog:gc*=info -Xmx700m -XX:+UseSerialGC gc.stress.systemgc.TestSystemGCWithSerial 270\n+ *\/\n+\n+\/*\n+ * @test id=alt-forwarding-large-heap\n+ * @key stress\n+ * @bug 8190703\n+ * @library \/\n+ * @requires vm.gc.Serial\n+ * @requires (vm.bits == \"64\") & (os.maxMemory >= 6G)\n+ * @summary Stress the Serial GC full GC by allocating objects of different lifetimes concurrently with System.gc().\n+ * @run main\/othervm\/timeout=300 -XX:+UnlockExperimentalVMOptions -XX:+UseAltGCForwarding -Xlog:gc*=info -Xmx6g -XX:+UseSerialGC gc.stress.systemgc.TestSystemGCWithSerial 270\n+ *\/\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/systemgc\/TestSystemGCWithSerial.java","additions":32,"deletions":1,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -44,0 +44,17 @@\n+\/*\n+ * @test id=alt-forwarding\n+ * @key stress\n+ * @library \/\n+ * @requires vm.gc.Shenandoah\n+ * @summary Stress the Shenandoah GC full GC by allocating objects of different lifetimes concurrently with System.gc().\n+ *\n+ * @run main\/othervm\/timeout=300 -Xlog:gc*=info -Xmx512m -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions\n+ *      -XX:+UseShenandoahGC -XX:+UseAltGCForwarding\n+ *      -XX:+ShenandoahVerify\n+ *      gc.stress.systemgc.TestSystemGCWithShenandoah 270\n+ *\n+ * @run main\/othervm\/timeout=300 -Xlog:gc*=info -Xmx512m -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions\n+ *      -XX:+UseShenandoahGC\n+ *      gc.stress.systemgc.TestSystemGCWithShenandoah 270\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/systemgc\/TestSystemGCWithShenandoah.java","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -0,0 +1,105 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test id=default\n+ * @library \/test\/lib \/\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI BaseOffsets\n+ *\/\n+\/*\n+ * @test id=no-coops\n+ * @library \/test\/lib \/\n+ * @requires vm.bits == \"64\"\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:-UseCompressedOops BaseOffsets\n+ *\/\n+\n+import java.lang.reflect.Field;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import jdk.internal.misc.Unsafe;\n+\n+import jdk.test.lib.Asserts;\n+import jdk.test.lib.Platform;\n+import jdk.test.whitebox.WhiteBox;\n+\n+public class BaseOffsets {\n+\n+    static class LIClass {\n+        public int i;\n+    }\n+\n+    static final long INT_OFFSET;\n+    static final int  INT_ARRAY_OFFSET;\n+    static final int  LONG_ARRAY_OFFSET;\n+    static {\n+        WhiteBox WB = WhiteBox.getWhiteBox();\n+        if (!Platform.is64bit() || WB.getBooleanVMFlag(\"UseCompactObjectHeaders\")) {\n+            INT_OFFSET = 8;\n+            INT_ARRAY_OFFSET = 12;\n+            LONG_ARRAY_OFFSET = 16;\n+        } else if (WB.getBooleanVMFlag(\"UseCompressedClassPointers\")) {\n+            INT_OFFSET = 12;\n+            INT_ARRAY_OFFSET = 16;\n+            LONG_ARRAY_OFFSET = 16;\n+        } else {\n+            INT_OFFSET = 16;\n+            INT_ARRAY_OFFSET = 20;\n+            LONG_ARRAY_OFFSET = 24;\n+        }\n+    }\n+\n+    static public void main(String[] args) {\n+        Unsafe unsafe = Unsafe.getUnsafe();\n+        Class c = LIClass.class;\n+        Field[] fields = c.getFields();\n+        for (int i = 0; i < fields.length; i++) {\n+            long offset = unsafe.objectFieldOffset(fields[i]);\n+            if (fields[i].getType() == int.class) {\n+                Asserts.assertEquals(offset, INT_OFFSET, \"Misplaced int field\");\n+            } else {\n+                Asserts.fail(\"Unexpected field type\");\n+            }\n+        }\n+\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(boolean[].class), INT_ARRAY_OFFSET,  \"Misplaced boolean array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(byte[].class),    INT_ARRAY_OFFSET,  \"Misplaced byte    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(char[].class),    INT_ARRAY_OFFSET,  \"Misplaced char    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(short[].class),   INT_ARRAY_OFFSET,  \"Misplaced short   array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(int[].class),     INT_ARRAY_OFFSET,  \"Misplaced int     array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(long[].class),    LONG_ARRAY_OFFSET, \"Misplaced long    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(float[].class),   INT_ARRAY_OFFSET,  \"Misplaced float   array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(double[].class),  LONG_ARRAY_OFFSET, \"Misplaced double  array base\");\n+        boolean narrowOops = System.getProperty(\"java.vm.compressedOopsMode\") != null ||\n+                             !Platform.is64bit();\n+        int expected_objary_offset = narrowOops ? INT_ARRAY_OFFSET : LONG_ARRAY_OFFSET;\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(Object[].class),  expected_objary_offset, \"Misplaced object  array base\");\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/FieldLayout\/BaseOffsets.java","additions":105,"deletions":0,"binary":false,"changes":105,"status":"added"},{"patch":"@@ -28,1 +28,1 @@\n- * @library \/test\/lib\n+ * @library \/test\/lib \/\n@@ -32,1 +32,3 @@\n- * @run main\/othervm -XX:+UseCompressedClassPointers -XX:-UseEmptySlotsInSupers OldLayoutCheck\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UseCompressedClassPointers -XX:-UseEmptySlotsInSupers OldLayoutCheck\n@@ -38,1 +40,1 @@\n- * @library \/test\/lib\n+ * @library \/test\/lib \/\n@@ -41,1 +43,3 @@\n- * @run main\/othervm -XX:-UseEmptySlotsInSupers OldLayoutCheck\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:-UseEmptySlotsInSupers OldLayoutCheck\n@@ -51,0 +55,1 @@\n+import jdk.test.whitebox.WhiteBox;\n@@ -59,5 +64,12 @@\n-    \/\/ 32-bit VMs: @0:  8 byte header,  @8: long field, @16:  int field\n-    \/\/ 64-bit VMs: @0: 12 byte header, @12:  int field, @16: long field\n-    static final long INT_OFFSET  = Platform.is64bit() ? 12L : 16L;\n-    static final long LONG_OFFSET = Platform.is64bit() ? 16L :  8L;\n-\n+    private static final long INT_OFFSET;\n+    private static final long LONG_OFFSET;\n+    static {\n+      WhiteBox WB = WhiteBox.getWhiteBox();\n+      if (!Platform.is64bit() || WB.getBooleanVMFlag(\"UseCompactObjectHeaders\")) {\n+        INT_OFFSET = 16L;\n+        LONG_OFFSET = 8L;\n+      } else {\n+        INT_OFFSET = 12L;\n+        LONG_OFFSET = 16L;\n+      }\n+    }\n","filename":"test\/hotspot\/jtreg\/runtime\/FieldLayout\/OldLayoutCheck.java","additions":21,"deletions":9,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -0,0 +1,66 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/**\n+ * @test CdsDifferentCompactObjectHeaders\n+ * @summary Testing CDS (class data sharing) using opposite compact object header settings.\n+ *          Using different compact bject headers setting for each dump\/load pair.\n+ *          This is a negative test; using compact header setting for loading that\n+ *          is different from compact headers for creating a CDS file\n+ *          should fail when loading.\n+ * @requires vm.cds\n+ * @requires vm.bits == 64\n+ * @library \/test\/lib\n+ * @run driver CdsDifferentCompactObjectHeaders\n+ *\/\n+\n+import jdk.test.lib.cds.CDSTestUtils;\n+import jdk.test.lib.process.OutputAnalyzer;\n+import jdk.test.lib.Platform;\n+\n+public class CdsDifferentCompactObjectHeaders {\n+\n+    public static void main(String[] args) throws Exception {\n+        createAndLoadSharedArchive(true, false);\n+        createAndLoadSharedArchive(false, true);\n+    }\n+\n+    \/\/ Parameters are object alignment expressed in bytes\n+    private static void\n+    createAndLoadSharedArchive(boolean createCompactHeaders, boolean loadCompactHeaders)\n+    throws Exception {\n+        String createCompactHeadersArg = \"-XX:\" + (createCompactHeaders ? \"+\" : \"-\") + \"UseCompactObjectHeaders\";\n+        String loadCompactHeadersArg   = \"-XX:\" + (loadCompactHeaders   ? \"+\" : \"-\") + \"UseCompactObjectHeaders\";\n+        String expectedErrorMsg =\n+            String.format(\n+            \"The shared archive file's UseCompactObjectHeaders setting (%s)\" +\n+            \" does not equal the current UseCompactObjectHeaders setting (%s)\",\n+            createCompactHeaders ? \"enabled\" : \"disabled\",\n+            loadCompactHeaders   ? \"enabled\" : \"disabled\");\n+\n+        CDSTestUtils.createArchiveAndCheck(\"-XX:+UnlockExperimentalVMOptions\", createCompactHeadersArg);\n+\n+        OutputAnalyzer out = CDSTestUtils.runWithArchive(\"-Xlog:cds\", \"-XX:+UnlockExperimentalVMOptions\", loadCompactHeadersArg);\n+        CDSTestUtils.checkExecExpectError(out, 1, expectedErrorMsg);\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/CdsDifferentCompactObjectHeaders.java","additions":66,"deletions":0,"binary":false,"changes":66,"status":"added"},{"patch":"@@ -0,0 +1,59 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test\n+ * @bug 8303027\n+ * @requires vm.bits == \"64\"\n+ * @summary Test that we're failing with OOME and not with VM crash\n+ * @run main\/othervm -Xmx1g -XX:-UseCompressedOops TestOOM\n+ *\/\n+\/*\n+ * @test\n+ * @bug 8303027\n+ * @requires vm.bits == \"32\"\n+ * @summary Test that we're failing with OOME and not with VM crash\n+ * @run main\/othervm -Xmx1g TestOOM\n+ *\/\n+public class TestOOM {\n+    public static void main(String[] args) {\n+        \/\/ Test that it exits with OOME and not with VM crash.\n+        try {\n+            LinkedInsanity previous = null;\n+            while (true) {\n+                previous = new LinkedInsanity(previous);\n+            }\n+        } catch (OutOfMemoryError e) {\n+            \/\/ That's expected\n+        }\n+    }\n+\n+    private static class LinkedInsanity {\n+        private final LinkedInsanity previous;\n+        private final int[] padding = new int[64000];\n+\n+        public LinkedInsanity(LinkedInsanity previous) {\n+            this.previous = previous;\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/oom\/TestOOM.java","additions":59,"deletions":0,"binary":false,"changes":59,"status":"added"},{"patch":"@@ -316,0 +316,3 @@\n+    static final boolean COMPACT_HEADERS = WhiteBox.getWhiteBox().getBooleanVMFlag(\"UseCompactObjectHeaders\");\n+    static final int HEADER_SIZE = COMPACT_HEADERS ? 8 : (Platform.is64bit() ? 16 : 8);\n+\n@@ -375,1 +378,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(HEADER_SIZE, OBJ_ALIGN);\n@@ -382,1 +385,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(HEADER_SIZE, OBJ_ALIGN);\n@@ -392,1 +395,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(HEADER_SIZE, OBJ_ALIGN);\n","filename":"test\/jdk\/java\/lang\/instrument\/GetObjectSizeIntrinsicsTest.java","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -527,1 +527,5 @@\n-                \"CreateCoredumpOnCrash\"\n+                \"CreateCoredumpOnCrash\",\n+                \/\/ experimental features unlocking flag does not affect behavior\n+                \"UnlockExperimentalVMOptions\",\n+                \/\/ all compact headers settings should run flagless tests\n+                \"UseCompactObjectHeaders\"\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"}]}