{"files":[{"patch":"@@ -69,5 +69,0 @@\n-    -tag beaninfo:X \\\n-    -tag revised:X \\\n-    -tag since.unbundled:X \\\n-    -tag Note:X \\\n-    -tag ToDo:X \\\n","filename":"make\/Docs.gmk","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1727,2 +1727,2 @@\n-    st->print(\"ldr zr, [lr]\\n\\t\");\n-    st->print(\"pacia  lr, rfp\\n\\t\");\n+    st->print(\"ldr  zr, [lr]\\n\\t\");\n+    st->print(\"paciaz\\n\\t\");\n@@ -1814,2 +1814,2 @@\n-    st->print(\"autia lr, rfp\\n\\t\");\n-    st->print(\"ldr zr, [lr]\\n\\t\");\n+    st->print(\"autiaz\\n\\t\");\n+    st->print(\"ldr  zr, [lr]\\n\\t\");\n@@ -2293,1 +2293,0 @@\n-  bool ret_value = true;\n@@ -2301,1 +2300,1 @@\n-        ret_value = false;\n+        return false;\n@@ -2307,1 +2306,9 @@\n-        ret_value = false;\n+        return false;\n+      }\n+      break;\n+    case Op_FmaF:\n+    case Op_FmaD:\n+    case Op_FmaVF:\n+    case Op_FmaVD:\n+      if (!UseFMA) {\n+        return false;\n@@ -2312,1 +2319,1 @@\n-  return ret_value; \/\/ Per default match rules are supported.\n+  return true; \/\/ Per default match rules are supported.\n@@ -3840,201 +3847,0 @@\n-  enc_class aarch64_enc_fast_lock(iRegP object, iRegP box, iRegP tmp, iRegP tmp2) %{\n-    C2_MacroAssembler _masm(&cbuf);\n-    Register oop = as_Register($object$$reg);\n-    Register box = as_Register($box$$reg);\n-    Register disp_hdr = as_Register($tmp$$reg);\n-    Register tmp = as_Register($tmp2$$reg);\n-    Label cont;\n-    Label object_has_monitor;\n-    Label count, no_count;\n-\n-    assert_different_registers(oop, box, tmp, disp_hdr);\n-\n-    \/\/ Load markWord from object into displaced_header.\n-    __ ldr(disp_hdr, Address(oop, oopDesc::mark_offset_in_bytes()));\n-\n-    if (DiagnoseSyncOnValueBasedClasses != 0) {\n-      __ load_klass(tmp, oop);\n-      __ ldrw(tmp, Address(tmp, Klass::access_flags_offset()));\n-      __ tstw(tmp, JVM_ACC_IS_VALUE_BASED_CLASS);\n-      __ br(Assembler::NE, cont);\n-    }\n-\n-    \/\/ Check for existing monitor\n-    __ tbnz(disp_hdr, exact_log2(markWord::monitor_value), object_has_monitor);\n-\n-    if (LockingMode == LM_MONITOR) {\n-      __ tst(oop, oop); \/\/ Set NE to indicate 'failure' -> take slow-path. We know that oop != 0.\n-      __ b(cont);\n-    } else if (LockingMode == LM_LEGACY) {\n-      \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n-      __ orr(tmp, disp_hdr, markWord::unlocked_value);\n-\n-      if (EnableValhalla) {\n-        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n-        __ andr(tmp, tmp, ~((int) markWord::inline_type_bit_in_place));\n-      }\n-\n-      \/\/ Initialize the box. (Must happen before we update the object mark!)\n-      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-      \/\/ Compare object markWord with an unlocked value (tmp) and if\n-      \/\/ equal exchange the stack address of our box with object markWord.\n-      \/\/ On failure disp_hdr contains the possibly locked markWord.\n-      __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n-                 \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n-      __ br(Assembler::EQ, cont);\n-\n-      assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-      \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n-      \/\/ object, will have now locked it will continue at label cont\n-\n-      \/\/ Check if the owner is self by comparing the value in the\n-      \/\/ markWord of object (disp_hdr) with the stack pointer.\n-      __ mov(rscratch1, sp);\n-      __ sub(disp_hdr, disp_hdr, rscratch1);\n-      __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n-      \/\/ If condition is true we are cont and hence we can store 0 as the\n-      \/\/ displaced header in the box, which indicates that it is a recursive lock.\n-      __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n-      __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-      __ b(cont);\n-    } else {\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-      __ fast_lock(oop, disp_hdr, tmp, rscratch1, no_count);\n-      __ b(count);\n-    }\n-\n-    \/\/ Handle existing monitor.\n-    __ bind(object_has_monitor);\n-\n-    \/\/ The object's monitor m is unlocked iff m->owner == nullptr,\n-    \/\/ otherwise m->owner may contain a thread or a stack address.\n-    \/\/\n-    \/\/ Try to CAS m->owner from nullptr to current thread.\n-    __ add(tmp, disp_hdr, (in_bytes(ObjectMonitor::owner_offset())-markWord::monitor_value));\n-    __ cmpxchg(tmp, zr, rthread, Assembler::xword, \/*acquire*\/ true,\n-               \/*release*\/ true, \/*weak*\/ false, rscratch1); \/\/ Sets flags for result\n-\n-    if (LockingMode != LM_LIGHTWEIGHT) {\n-      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-      \/\/ lock. The fast-path monitor unlock code checks for\n-      \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-      \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n-      __ mov(tmp, (address)markWord::unused_mark().value());\n-      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-    }\n-    __ br(Assembler::EQ, cont); \/\/ CAS success means locking succeeded\n-\n-    __ cmp(rscratch1, rthread);\n-    __ br(Assembler::NE, cont); \/\/ Check for recursive locking\n-\n-    \/\/ Recursive lock case\n-    __ increment(Address(disp_hdr, in_bytes(ObjectMonitor::recursions_offset()) - markWord::monitor_value), 1);\n-    \/\/ flag == EQ still from the cmp above, checking if this is a reentrant lock\n-\n-    __ bind(cont);\n-    \/\/ flag == EQ indicates success\n-    \/\/ flag == NE indicates failure\n-    __ br(Assembler::NE, no_count);\n-\n-    __ bind(count);\n-    __ increment(Address(rthread, JavaThread::held_monitor_count_offset()));\n-\n-    __ bind(no_count);\n-  %}\n-\n-  enc_class aarch64_enc_fast_unlock(iRegP object, iRegP box, iRegP tmp, iRegP tmp2) %{\n-    C2_MacroAssembler _masm(&cbuf);\n-    Register oop = as_Register($object$$reg);\n-    Register box = as_Register($box$$reg);\n-    Register disp_hdr = as_Register($tmp$$reg);\n-    Register tmp = as_Register($tmp2$$reg);\n-    Label cont;\n-    Label object_has_monitor;\n-    Label count, no_count;\n-\n-    assert_different_registers(oop, box, tmp, disp_hdr);\n-\n-    if (LockingMode == LM_LEGACY) {\n-      \/\/ Find the lock address and load the displaced header from the stack.\n-      __ ldr(disp_hdr, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-      \/\/ If the displaced header is 0, we have a recursive unlock.\n-      __ cmp(disp_hdr, zr);\n-      __ br(Assembler::EQ, cont);\n-    }\n-\n-    \/\/ Handle existing monitor.\n-    __ ldr(tmp, Address(oop, oopDesc::mark_offset_in_bytes()));\n-    __ tbnz(tmp, exact_log2(markWord::monitor_value), object_has_monitor);\n-\n-    if (LockingMode == LM_MONITOR) {\n-      __ tst(oop, oop); \/\/ Set NE to indicate 'failure' -> take slow-path. We know that oop != 0.\n-      __ b(cont);\n-    } else if (LockingMode == LM_LEGACY) {\n-      \/\/ Check if it is still a light weight lock, this is is true if we\n-      \/\/ see the stack address of the basicLock in the markWord of the\n-      \/\/ object.\n-\n-      __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n-                 \/*release*\/ true, \/*weak*\/ false, tmp);\n-      __ b(cont);\n-    } else {\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-      __ fast_unlock(oop, tmp, box, disp_hdr, no_count);\n-      __ b(count);\n-    }\n-\n-    assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-    \/\/ Handle existing monitor.\n-    __ bind(object_has_monitor);\n-    STATIC_ASSERT(markWord::monitor_value <= INT_MAX);\n-    __ add(tmp, tmp, -(int)markWord::monitor_value); \/\/ monitor\n-\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      \/\/ If the owner is anonymous, we need to fix it -- in an outline stub.\n-      Register tmp2 = disp_hdr;\n-      __ ldr(tmp2, Address(tmp, ObjectMonitor::owner_offset()));\n-      \/\/ We cannot use tbnz here, the target might be too far away and cannot\n-      \/\/ be encoded.\n-      __ tst(tmp2, (uint64_t)ObjectMonitor::ANONYMOUS_OWNER);\n-      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmp, tmp2);\n-      Compile::current()->output()->add_stub(stub);\n-      __ br(Assembler::NE, stub->entry());\n-      __ bind(stub->continuation());\n-    }\n-\n-    __ ldr(disp_hdr, Address(tmp, ObjectMonitor::recursions_offset()));\n-\n-    Label notRecursive;\n-    __ cbz(disp_hdr, notRecursive);\n-\n-    \/\/ Recursive lock\n-    __ sub(disp_hdr, disp_hdr, 1u);\n-    __ str(disp_hdr, Address(tmp, ObjectMonitor::recursions_offset()));\n-    __ cmp(disp_hdr, disp_hdr); \/\/ Sets flags for result\n-    __ b(cont);\n-\n-    __ bind(notRecursive);\n-    __ ldr(rscratch1, Address(tmp, ObjectMonitor::EntryList_offset()));\n-    __ ldr(disp_hdr, Address(tmp, ObjectMonitor::cxq_offset()));\n-    __ orr(rscratch1, rscratch1, disp_hdr); \/\/ Will be 0 if both are 0.\n-    __ cmp(rscratch1, zr); \/\/ Sets flags for result\n-    __ cbnz(rscratch1, cont);\n-    \/\/ need a release store here\n-    __ lea(tmp, Address(tmp, ObjectMonitor::owner_offset()));\n-    __ stlr(zr, tmp); \/\/ set unowned\n-\n-    __ bind(cont);\n-    \/\/ flag == EQ indicates success\n-    \/\/ flag == NE indicates failure\n-    __ br(Assembler::NE, no_count);\n-\n-    __ bind(count);\n-    __ decrement(Address(rthread, JavaThread::held_monitor_count_offset()));\n-\n-    __ bind(no_count);\n-  %}\n-\n@@ -14356,1 +14162,0 @@\n-  predicate(UseFMA);\n@@ -14362,0 +14167,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -14373,1 +14179,0 @@\n-  predicate(UseFMA);\n@@ -14379,0 +14184,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -14388,1 +14194,2 @@\n-\/\/ -src1 * src2 + src3\n+\/\/ src1 * (-src2) + src3\n+\/\/ \"(-src1) * src2 + src3\" has been idealized to \"src2 * (-src1) + src3\"\n@@ -14390,2 +14197,0 @@\n-  predicate(UseFMA);\n-  match(Set dst (FmaF src3 (Binary (NegF src1) src2)));\n@@ -14397,0 +14202,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -14406,1 +14212,2 @@\n-\/\/ -src1 * src2 + src3\n+\/\/ src1 * (-src2) + src3\n+\/\/ \"(-src1) * src2 + src3\" has been idealized to \"src2 * (-src1) + src3\"\n@@ -14408,2 +14215,0 @@\n-  predicate(UseFMA);\n-  match(Set dst (FmaD src3 (Binary (NegD src1) src2)));\n@@ -14415,0 +14220,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -14424,1 +14230,2 @@\n-\/\/ -src1 * src2 - src3\n+\/\/ src1 * (-src2) - src3\n+\/\/ \"(-src1) * src2 - src3\" has been idealized to \"src2 * (-src1) - src3\"\n@@ -14426,2 +14233,0 @@\n-  predicate(UseFMA);\n-  match(Set dst (FmaF (NegF src3) (Binary (NegF src1) src2)));\n@@ -14433,0 +14238,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -14442,1 +14248,2 @@\n-\/\/ -src1 * src2 - src3\n+\/\/ src1 * (-src2) - src3\n+\/\/ \"(-src1) * src2 - src3\" has been idealized to \"src2 * (-src1) - src3\"\n@@ -14444,2 +14251,0 @@\n-  predicate(UseFMA);\n-  match(Set dst (FmaD (NegD src3) (Binary (NegD src1) src2)));\n@@ -14451,0 +14256,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -14462,1 +14268,0 @@\n-  predicate(UseFMA);\n@@ -14468,0 +14273,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -14479,1 +14285,0 @@\n-  predicate(UseFMA);\n@@ -14485,1 +14290,2 @@\n-  \/\/ n.b. insn name should be fnmsubd\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n+    \/\/ n.b. insn name should be fnmsubd\n@@ -16687,1 +16493,3 @@\n-  ins_encode(aarch64_enc_fast_lock(object, box, tmp, tmp2));\n+  ins_encode %{\n+    __ fast_lock($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register);\n+  %}\n@@ -16700,1 +16508,3 @@\n-  ins_encode(aarch64_enc_fast_unlock(object, box, tmp, tmp2));\n+  ins_encode %{\n+    __ fast_unlock($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register);\n+  %}\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":38,"deletions":228,"binary":false,"changes":266,"status":"modified"},{"patch":"@@ -448,3 +448,2 @@\n-    \/\/ 1) this code has been built with branch-protection,\n-    \/\/ 2) the CPU\/OS supports it, and\n-    \/\/ 3) incompatible VMContinuations isn't enabled.\n+    \/\/ 1) this code has been built with branch-protection and\n+    \/\/ 2) the CPU\/OS supports it\n@@ -455,3 +454,0 @@\n-    } else if (VMContinuations) {\n-      \/\/ Not currently compatible with continuation freeze\/thaw.\n-      warning(\"ROP-protection is incompatible with VMContinuations. Disabling ROP-protection.\");\n@@ -472,6 +468,0 @@\n-\n-    \/\/ The frame pointer must be preserved for ROP protection.\n-    if (FLAG_IS_DEFAULT(PreserveFramePointer) == false && PreserveFramePointer == false ) {\n-      vm_exit_during_initialization(err_msg(\"PreserveFramePointer cannot be disabled for ROP-protection\"));\n-    }\n-    PreserveFramePointer = true;\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.cpp","additions":2,"deletions":12,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -114,0 +114,1 @@\n+  static bool profile_all_receivers_at_type_check() { return false; }\n","filename":"src\/hotspot\/cpu\/arm\/vm_version_arm.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2151,0 +2151,3 @@\n+    case Op_FmaF:\n+    case Op_FmaD:\n+      return UseFMA;\n@@ -9649,0 +9652,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -9661,0 +9665,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -9666,1 +9671,2 @@\n-\/\/ -src1 * src2 + src3 = -(src1*src2-src3)\n+\/\/ src1 * (-src2) + src3 = -(src1*src2-src3)\n+\/\/ \"(-src1) * src2 + src3\" has been idealized to \"src2 * (-src1) + src3\"\n@@ -9668,1 +9674,0 @@\n-  match(Set dst (FmaF src3 (Binary (NegF src1) src2)));\n@@ -9674,0 +9679,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -9679,1 +9685,2 @@\n-\/\/ -src1 * src2 + src3 = -(src1*src2-src3)\n+\/\/ src1 * (-src2) + src3 = -(src1*src2-src3)\n+\/\/ \"(-src1) * src2 + src3\" has been idealized to \"src2 * (-src1) + src3\"\n@@ -9681,1 +9688,0 @@\n-  match(Set dst (FmaD src3 (Binary (NegD src1) src2)));\n@@ -9687,0 +9693,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -9692,1 +9699,2 @@\n-\/\/ -src1 * src2 - src3 = -(src1*src2+src3)\n+\/\/ src1 * (-src2) - src3 = -(src1*src2+src3)\n+\/\/ \"(-src1) * src2 - src3\" has been idealized to \"src2 * (-src1) - src3\"\n@@ -9694,1 +9702,0 @@\n-  match(Set dst (FmaF (NegF src3) (Binary (NegF src1) src2)));\n@@ -9700,0 +9707,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -9705,1 +9713,2 @@\n-\/\/ -src1 * src2 - src3 = -(src1*src2+src3)\n+\/\/ src1 * (-src2) - src3 = -(src1*src2+src3)\n+\/\/ \"(-src1) * src2 - src3\" has been idealized to \"src2 * (-src1) - src3\"\n@@ -9707,1 +9716,0 @@\n-  match(Set dst (FmaD (NegD src3) (Binary (NegD src1) src2)));\n@@ -9713,0 +9721,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -9725,0 +9734,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -9737,0 +9747,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -14054,1 +14065,1 @@\n-\/\/ dst + src1 * src2\n+\/\/ src1 * src2 + dst\n@@ -14063,0 +14074,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -14068,1 +14080,2 @@\n-\/\/ dst - src1 * src2\n+\/\/ src1 * (-src2) + dst\n+\/\/ \"(-src1) * src2 + dst\" has been idealized to \"src2 * (-src1) + dst\"\n@@ -14070,1 +14083,0 @@\n-  match(Set dst (FmaVF dst (Binary (NegVF src1) src2)));\n@@ -14078,0 +14090,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -14083,1 +14096,1 @@\n-\/\/ - dst + src1 * src2\n+\/\/ src1 * src2 - dst\n@@ -14092,0 +14105,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -14097,1 +14111,1 @@\n-\/\/ dst + src1 * src2\n+\/\/ src1 * src2 + dst\n@@ -14106,0 +14120,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -14111,1 +14126,2 @@\n-\/\/ dst - src1 * src2\n+\/\/ src1 * (-src2) + dst\n+\/\/ \"(-src1) * src2 + dst\" has been idealized to \"src2 * (-src1) + dst\"\n@@ -14113,1 +14129,0 @@\n-  match(Set dst (FmaVD  dst (Binary (NegVD src1) src2)));\n@@ -14121,0 +14136,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -14126,1 +14142,1 @@\n-\/\/ - dst + src1 * src2\n+\/\/ src1 * src2 - dst\n@@ -14135,0 +14151,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":33,"deletions":16,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -500,1 +500,1 @@\n-      tty->print_cr(\"ContendedPaddingWidth \" INTX_FORMAT, ContendedPaddingWidth);\n+      tty->print_cr(\"ContendedPaddingWidth %d\", ContendedPaddingWidth);\n","filename":"src\/hotspot\/cpu\/ppc\/vm_version_ppc.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1910,0 +1910,5 @@\n+    case Op_FmaF:\n+    case Op_FmaD:\n+    case Op_FmaVF:\n+    case Op_FmaVD:\n+      return UseFMA;\n@@ -2424,217 +2429,0 @@\n-  \/\/ Use cr register to indicate the fast_lock result: zero for success; non-zero for failure.\n-  enc_class riscv_enc_fast_lock(iRegP object, iRegP box, iRegPNoSp tmp1, iRegPNoSp tmp2) %{\n-    C2_MacroAssembler _masm(&cbuf);\n-    Register flag = t1;\n-    Register oop = as_Register($object$$reg);\n-    Register box = as_Register($box$$reg);\n-    Register disp_hdr = as_Register($tmp1$$reg);\n-    Register tmp = as_Register($tmp2$$reg);\n-    Label cont;\n-    Label object_has_monitor;\n-    Label count, no_count;\n-\n-    assert_different_registers(oop, box, tmp, disp_hdr, t0);\n-\n-    \/\/ Load markWord from object into displaced_header.\n-    __ ld(disp_hdr, Address(oop, oopDesc::mark_offset_in_bytes()));\n-\n-    if (DiagnoseSyncOnValueBasedClasses != 0) {\n-      __ load_klass(flag, oop);\n-      __ lwu(flag, Address(flag, Klass::access_flags_offset()));\n-      __ test_bit(flag, flag, exact_log2(JVM_ACC_IS_VALUE_BASED_CLASS), tmp \/* tmp *\/);\n-      __ bnez(flag, cont, true \/* is_far *\/);\n-    }\n-\n-    \/\/ Check for existing monitor\n-    __ test_bit(t0, disp_hdr, exact_log2(markWord::monitor_value));\n-    __ bnez(t0, object_has_monitor);\n-\n-    if (LockingMode == LM_MONITOR) {\n-      __ mv(flag, 1); \/\/ Set non-zero flag to indicate 'failure' -> take slow-path\n-      __ j(cont);\n-    } else if (LockingMode == LM_LEGACY) {\n-      \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n-      __ ori(tmp, disp_hdr, markWord::unlocked_value);\n-\n-      \/\/ Initialize the box. (Must happen before we update the object mark!)\n-      __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-      \/\/ Compare object markWord with an unlocked value (tmp) and if\n-      \/\/ equal exchange the stack address of our box with object markWord.\n-      \/\/ On failure disp_hdr contains the possibly locked markWord.\n-      __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/tmp, \/*new value*\/box, Assembler::int64, Assembler::aq,\n-                 Assembler::rl, \/*result*\/disp_hdr);\n-      __ mv(flag, zr);\n-      __ beq(disp_hdr, tmp, cont); \/\/ prepare zero flag and goto cont if we won the cas\n-\n-      assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-      \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n-      \/\/ object, will have now locked it will continue at label cont\n-      \/\/ We did not see an unlocked object so try the fast recursive case.\n-\n-      \/\/ Check if the owner is self by comparing the value in the\n-      \/\/ markWord of object (disp_hdr) with the stack pointer.\n-      __ sub(disp_hdr, disp_hdr, sp);\n-      __ mv(tmp, (intptr_t) (~(os::vm_page_size()-1) | (uintptr_t)markWord::lock_mask_in_place));\n-      \/\/ If (mark & lock_mask) == 0 and mark - sp < page_size, we are stack-locking and goto cont,\n-      \/\/ hence we can store 0 as the displaced header in the box, which indicates that it is a\n-      \/\/ recursive lock.\n-      __ andr(tmp\/*==0?*\/, disp_hdr, tmp);\n-      __ sd(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-      __ mv(flag, tmp); \/\/ we can use the value of tmp as the result here\n-      __ j(cont);\n-    } else {\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n-      Label slow;\n-      __ fast_lock(oop, disp_hdr, tmp, t0, slow);\n-\n-      \/\/ Indicate success on completion.\n-      __ mv(flag, zr);\n-      __ j(count);\n-      __ bind(slow);\n-      __ mv(flag, 1); \/\/ Set non-zero flag to indicate 'failure' -> take slow-path\n-      __ j(no_count);\n-    }\n-\n-    \/\/ Handle existing monitor.\n-    __ bind(object_has_monitor);\n-    \/\/ The object's monitor m is unlocked iff m->owner == NULL,\n-    \/\/ otherwise m->owner may contain a thread or a stack address.\n-    \/\/\n-    \/\/ Try to CAS m->owner from NULL to current thread.\n-    __ add(tmp, disp_hdr, (in_bytes(ObjectMonitor::owner_offset()) - markWord::monitor_value));\n-    __ cmpxchg(\/*memory address*\/tmp, \/*expected value*\/zr, \/*new value*\/xthread, Assembler::int64, Assembler::aq,\n-             Assembler::rl, \/*result*\/flag); \/\/ cas succeeds if flag == zr(expected)\n-\n-    if (LockingMode != LM_LIGHTWEIGHT) {\n-      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-      \/\/ lock. The fast-path monitor unlock code checks for\n-      \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-      \/\/ relevant bit set, and also matches ObjectSynchronizer::slow_enter.\n-      __ mv(tmp, (address)markWord::unused_mark().value());\n-      __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-    }\n-\n-    __ beqz(flag, cont); \/\/ CAS success means locking succeeded\n-\n-    __ bne(flag, xthread, cont); \/\/ Check for recursive locking\n-\n-    \/\/ Recursive lock case\n-    __ mv(flag, zr);\n-    __ increment(Address(disp_hdr, in_bytes(ObjectMonitor::recursions_offset()) - markWord::monitor_value), 1, t0, tmp);\n-\n-    __ bind(cont);\n-    \/\/ zero flag indicates success\n-    \/\/ non-zero flag indicates failure\n-    __ bnez(flag, no_count);\n-\n-    __ bind(count);\n-    __ increment(Address(xthread, JavaThread::held_monitor_count_offset()), 1, t0, tmp);\n-\n-    __ bind(no_count);\n-  %}\n-\n-  \/\/ Use cr register to indicate the fast_unlock result: zero for success; non-zero for failure.\n-  enc_class riscv_enc_fast_unlock(iRegP object, iRegP box, iRegPNoSp tmp1, iRegPNoSp tmp2) %{\n-    C2_MacroAssembler _masm(&cbuf);\n-    Register flag = t1;\n-    Register oop = as_Register($object$$reg);\n-    Register box = as_Register($box$$reg);\n-    Register disp_hdr = as_Register($tmp1$$reg);\n-    Register tmp = as_Register($tmp2$$reg);\n-    Label cont;\n-    Label object_has_monitor;\n-    Label count, no_count;\n-\n-    assert_different_registers(oop, box, tmp, disp_hdr, flag);\n-\n-    if (LockingMode == LM_LEGACY) {\n-      \/\/ Find the lock address and load the displaced header from the stack.\n-      __ ld(disp_hdr, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-      \/\/ If the displaced header is 0, we have a recursive unlock.\n-      __ mv(flag, disp_hdr);\n-      __ beqz(disp_hdr, cont);\n-    }\n-\n-    \/\/ Handle existing monitor.\n-    __ ld(tmp, Address(oop, oopDesc::mark_offset_in_bytes()));\n-    __ test_bit(t0, tmp, exact_log2(markWord::monitor_value));\n-    __ bnez(t0, object_has_monitor);\n-\n-    if (LockingMode == LM_MONITOR) {\n-      __ mv(flag, 1); \/\/ Set non-zero flag to indicate 'failure' -> take slow path\n-      __ j(cont);\n-    } else if (LockingMode == LM_LEGACY) {\n-      \/\/ Check if it is still a light weight lock, this is true if we\n-      \/\/ see the stack address of the basicLock in the markWord of the\n-      \/\/ object.\n-\n-      __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/box, \/*new value*\/disp_hdr, Assembler::int64, Assembler::relaxed,\n-                 Assembler::rl, \/*result*\/tmp);\n-      __ xorr(flag, box, tmp); \/\/ box == tmp if cas succeeds\n-      __ j(cont);\n-    } else {\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n-      Label slow;\n-      __ fast_unlock(oop, tmp, box, disp_hdr, slow);\n-\n-      \/\/ Indicate success on completion.\n-      __ mv(flag, zr);\n-      __ j(count);\n-      __ bind(slow);\n-      __ mv(flag, 1); \/\/ Set non-zero flag to indicate 'failure' -> take slow path\n-      __ j(no_count);\n-    }\n-\n-    assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-    \/\/ Handle existing monitor.\n-    __ bind(object_has_monitor);\n-    STATIC_ASSERT(markWord::monitor_value <= INT_MAX);\n-    __ add(tmp, tmp, -(int)markWord::monitor_value); \/\/ monitor\n-\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      \/\/ If the owner is anonymous, we need to fix it -- in an outline stub.\n-      Register tmp2 = disp_hdr;\n-      __ ld(tmp2, Address(tmp, ObjectMonitor::owner_offset()));\n-      __ test_bit(t0, tmp2, exact_log2(ObjectMonitor::ANONYMOUS_OWNER));\n-      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmp, tmp2);\n-      Compile::current()->output()->add_stub(stub);\n-      __ bnez(t0, stub->entry(), \/* is_far *\/ true);\n-      __ bind(stub->continuation());\n-    }\n-\n-    __ ld(disp_hdr, Address(tmp, ObjectMonitor::recursions_offset()));\n-\n-    Label notRecursive;\n-    __ beqz(disp_hdr, notRecursive); \/\/ Will be 0 if not recursive.\n-\n-    \/\/ Recursive lock\n-    __ addi(disp_hdr, disp_hdr, -1);\n-    __ sd(disp_hdr, Address(tmp, ObjectMonitor::recursions_offset()));\n-    __ mv(flag, zr);\n-    __ j(cont);\n-\n-    __ bind(notRecursive);\n-    __ ld(flag, Address(tmp, ObjectMonitor::EntryList_offset()));\n-    __ ld(disp_hdr, Address(tmp, ObjectMonitor::cxq_offset()));\n-    __ orr(flag, flag, disp_hdr); \/\/ Will be 0 if both are 0.\n-    __ bnez(flag, cont);\n-    \/\/ need a release store here\n-    __ la(tmp, Address(tmp, ObjectMonitor::owner_offset()));\n-    __ membar(MacroAssembler::LoadStore | MacroAssembler::StoreStore);\n-    __ sd(zr, Address(tmp)); \/\/ set unowned\n-\n-    __ bind(cont);\n-    \/\/ zero flag indicates success\n-    \/\/ non-zero flag indicates failure\n-    __ bnez(flag, no_count);\n-\n-    __ bind(count);\n-    __ decrement(Address(xthread, JavaThread::held_monitor_count_offset()), 1, t0, tmp);\n-\n-    __ bind(no_count);\n-  %}\n-\n@@ -6852,0 +6640,15 @@\n+instruct umulHiL_rReg(iRegLNoSp dst, iRegL src1, iRegL src2)\n+%{\n+  match(Set dst (UMulHiL src1 src2));\n+  ins_cost(IMUL_COST);\n+  format %{ \"mulhu  $dst, $src1, $src2\\t# umulhi, #@umulHiL_rReg\" %}\n+\n+  ins_encode %{\n+    __ mulhu(as_Register($dst$$reg),\n+             as_Register($src1$$reg),\n+             as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(lmul_reg_reg);\n+%}\n+\n@@ -7264,1 +7067,0 @@\n-  predicate(UseFMA);\n@@ -7271,0 +7073,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -7282,1 +7085,0 @@\n-  predicate(UseFMA);\n@@ -7289,0 +7091,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -7300,1 +7103,0 @@\n-  predicate(UseFMA);\n@@ -7307,0 +7109,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -7318,1 +7121,0 @@\n-  predicate(UseFMA);\n@@ -7325,0 +7127,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -7334,1 +7137,2 @@\n-\/\/ -src1 * src2 + src3\n+\/\/ src1 * (-src2) + src3\n+\/\/ \"(-src1) * src2 + src3\" has been idealized to \"src2 * (-src1) + src3\"\n@@ -7336,2 +7140,0 @@\n-  predicate(UseFMA);\n-  match(Set dst (FmaF src3 (Binary (NegF src1) src2)));\n@@ -7344,0 +7146,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -7353,1 +7156,2 @@\n-\/\/ -src1 * src2 + src3\n+\/\/ src1 * (-src2) + src3\n+\/\/ \"(-src1) * src2 + src3\" has been idealized to \"src2 * (-src1) + src3\"\n@@ -7355,2 +7159,0 @@\n-  predicate(UseFMA);\n-  match(Set dst (FmaD src3 (Binary (NegD src1) src2)));\n@@ -7363,0 +7165,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -7372,1 +7175,2 @@\n-\/\/ -src1 * src2 - src3\n+\/\/ src1 * (-src2) - src3\n+\/\/ \"(-src1) * src2 - src3\" has been idealized to \"src2 * (-src1) - src3\"\n@@ -7374,2 +7178,0 @@\n-  predicate(UseFMA);\n-  match(Set dst (FmaF (NegF src3) (Binary (NegF src1) src2)));\n@@ -7382,0 +7184,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -7391,1 +7194,2 @@\n-\/\/ -src1 * src2 - src3\n+\/\/ src1 * (-src2) - src3\n+\/\/ \"(-src1) * src2 - src3\" has been idealized to \"src2 * (-src1) - src3\"\n@@ -7393,2 +7197,0 @@\n-  predicate(UseFMA);\n-  match(Set dst (FmaD (NegD src3) (Binary (NegD src1) src2)));\n@@ -7401,0 +7203,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -7682,0 +7485,14 @@\n+\/\/ Round Instruction\n+instruct roundD_reg(fRegD dst, fRegD src, immI rmode, iRegLNoSp tmp1, iRegLNoSp tmp2, iRegLNoSp tmp3) %{\n+  match(Set dst (RoundDoubleMode src rmode));\n+  ins_cost(2 * XFER_COST + BRANCH_COST);\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+\n+  format %{ \"RoundDoubleMode $src, $rmode\" %}\n+  ins_encode %{\n+    __ round_double_mode(as_FloatRegister($dst$$reg),\n+               as_FloatRegister($src$$reg), $rmode$$constant, $tmp1$$Register, $tmp2$$Register, $tmp3$$Register);\n+  %}\n+  ins_pipe(pipe_class_default);\n+%}\n+\n@@ -10375,1 +10192,3 @@\n-  ins_encode(riscv_enc_fast_lock(object, box, tmp1, tmp2));\n+  ins_encode %{\n+    __ fast_lock($object$$Register, $box$$Register, $tmp1$$Register, $tmp2$$Register);\n+  %}\n@@ -10389,1 +10208,3 @@\n-  ins_encode(riscv_enc_fast_unlock(object, box, tmp1, tmp2));\n+  ins_encode %{\n+    __ fast_unlock($object$$Register, $box$$Register, $tmp1$$Register, $tmp2$$Register);\n+  %}\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":56,"deletions":235,"binary":false,"changes":291,"status":"modified"},{"patch":"@@ -102,0 +102,11 @@\n+  \/\/ Enable vendor specific features\n+\n+  if (mvendorid.enabled()) {\n+    \/\/ Rivos\n+    if (mvendorid.value() == RIVOS) {\n+      if (FLAG_IS_DEFAULT(UseConservativeFence)) {\n+        FLAG_SET_DEFAULT(UseConservativeFence, false);\n+      }\n+    }\n+  }\n+\n@@ -174,3 +185,2 @@\n-  if (UseMD5Intrinsics) {\n-    warning(\"MD5 intrinsics are not available on this CPU.\");\n-    FLAG_SET_DEFAULT(UseMD5Intrinsics, false);\n+  if (FLAG_IS_DEFAULT(UseMD5Intrinsics)) {\n+    FLAG_SET_DEFAULT(UseMD5Intrinsics, true);\n@@ -214,0 +224,8 @@\n+#ifdef __riscv_ztso\n+  \/\/ Hotspot is compiled with TSO support, it will only run on hardware which\n+  \/\/ supports Ztso\n+  if (FLAG_IS_DEFAULT(UseZtso)) {\n+    FLAG_SET_DEFAULT(UseZtso, true);\n+  }\n+#endif\n+\n@@ -271,0 +289,1 @@\n+        MaxVectorSize = _initial_vector_length;\n@@ -272,1 +291,0 @@\n-      MaxVectorSize = _initial_vector_length;\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.cpp","additions":22,"deletions":4,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -43,0 +43,6 @@\n+\n+  \/\/ JEDEC encoded as ((bank - 1) << 7) | (0x7f & JEDEC)\n+  enum VendorId {\n+    RIVOS = 0x6cf, \/\/ JEDEC: 0x4f, Bank: 14\n+  };\n+\n@@ -137,0 +143,1 @@\n+  decl(ext_Ztso        , \"Ztso\"        , RV_NO_FLAG_BIT, true , UPDATE_DEFAULT(UseZtso))        \\\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1508,0 +1508,3 @@\n+    case Op_FmaF:\n+    case Op_FmaD:\n+      return UseFMA;\n@@ -7162,0 +7165,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -7175,0 +7179,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -7188,0 +7193,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -7201,0 +7207,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -7214,0 +7221,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -7228,0 +7236,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -7242,0 +7251,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -7256,0 +7266,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -7270,0 +7281,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -7284,0 +7296,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -7298,0 +7311,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -7312,0 +7326,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n","filename":"src\/hotspot\/cpu\/s390\/s390.ad","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -710,1 +710,1 @@\n-      tty->print_cr(\"ContendedPaddingWidth \" INTX_FORMAT, ContendedPaddingWidth);\n+      tty->print_cr(\"ContendedPaddingWidth %d\", ContendedPaddingWidth);\n","filename":"src\/hotspot\/cpu\/s390\/vm_version_s390.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -573,0 +573,2 @@\n+\n+  static bool profile_all_receivers_at_type_check() { return false; }\n","filename":"src\/hotspot\/cpu\/s390\/vm_version_s390.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -576,5 +577,0 @@\n-\n-  \/\/ Profile the failure of the check.\n-  if (profile) {\n-    profile_typecheck_failed(rcx); \/\/ blows rcx\n-  }\n@@ -804,1 +800,4 @@\n-  movptr(Address(rbp, frame::interpreter_frame_last_sp_offset * wordSize), _bcp_register);\n+  mov(rcx, _bcp_register);\n+  subptr(rcx, rbp);\n+  sarptr(rcx, LogBytesPerWord);\n+  movptr(Address(rbp, frame::interpreter_frame_last_sp_offset * wordSize), rcx);\n@@ -1085,1 +1084,1 @@\n-    const int entry_size = frame::interpreter_frame_monitor_size() * wordSize;\n+    const int entry_size = frame::interpreter_frame_monitor_size_in_bytes();\n@@ -1094,2 +1093,4 @@\n-    movptr(rmon, monitor_block_top); \/\/ points to current entry, starting\n-                                  \/\/ with top-most entry\n+    movptr(rmon, monitor_block_top); \/\/ derelativize pointer\n+    lea(rmon, Address(rbp, rmon, Address::times_ptr));\n+    \/\/ c_rarg1 points to current entry, starting with top-most entry\n+\n@@ -1397,1 +1398,1 @@\n-      fast_lock_impl(obj_reg, swap_reg, thread, tmp_reg, slow_case);\n+      lightweight_lock(obj_reg, swap_reg, thread, tmp_reg, slow_case);\n@@ -1531,1 +1532,1 @@\n-      fast_unlock_impl(obj_reg, swap_reg, header_reg, slow_case);\n+      lightweight_unlock(obj_reg, swap_reg, header_reg, slow_case);\n@@ -1855,1 +1856,1 @@\n-    record_klass_in_profile(receiver, mdp, reg2, true);\n+    record_klass_in_profile(receiver, mdp, reg2);\n@@ -1875,4 +1876,3 @@\n-void InterpreterMacroAssembler::record_klass_in_profile_helper(\n-                                        Register receiver, Register mdp,\n-                                        Register reg2, int start_row,\n-                                        Label& done, bool is_virtual_call) {\n+void InterpreterMacroAssembler::record_klass_in_profile_helper(Register receiver, Register mdp,\n+                                                               Register reg2, int start_row,\n+                                                               Label& done) {\n@@ -1880,8 +1880,1 @@\n-    if (is_virtual_call) {\n-      increment_mdp_data_at(mdp, in_bytes(CounterData::count_offset()));\n-    }\n-#if INCLUDE_JVMCI\n-    else if (EnableJVMCI) {\n-      increment_mdp_data_at(mdp, in_bytes(ReceiverTypeData::nonprofiled_receiver_count_offset()));\n-    }\n-#endif \/\/ INCLUDE_JVMCI\n+    increment_mdp_data_at(mdp, in_bytes(CounterData::count_offset()));\n@@ -1889,11 +1882,1 @@\n-    int non_profiled_offset = -1;\n-    if (is_virtual_call) {\n-      non_profiled_offset = in_bytes(CounterData::count_offset());\n-    }\n-#if INCLUDE_JVMCI\n-    else if (EnableJVMCI) {\n-      non_profiled_offset = in_bytes(ReceiverTypeData::nonprofiled_receiver_count_offset());\n-    }\n-#endif \/\/ INCLUDE_JVMCI\n-\n-        &VirtualCallData::receiver_offset, &VirtualCallData::receiver_count_offset, non_profiled_offset);\n+                                  &VirtualCallData::receiver_offset, &VirtualCallData::receiver_count_offset);\n@@ -1904,4 +1887,4 @@\n-void InterpreterMacroAssembler::record_item_in_profile_helper(Register item, Register mdp,\n-                                        Register reg2, int start_row, Label& done, int total_rows,\n-                                        OffsetFunction item_offset_fn, OffsetFunction item_count_offset_fn,\n-                                        int non_profiled_offset) {\n+void InterpreterMacroAssembler::record_item_in_profile_helper(Register item, Register mdp, Register reg2, int start_row,\n+                                                              Label& done, int total_rows,\n+                                                              OffsetFunction item_offset_fn,\n+                                                              OffsetFunction item_count_offset_fn) {\n@@ -1938,11 +1921,7 @@\n-        if (non_profiled_offset >= 0) {\n-          Label found_null;\n-          jccb(Assembler::zero, found_null);\n-          \/\/ Item did not match any saved item and there is no empty row for it.\n-          \/\/ Increment total counter to indicate polymorphic case.\n-          increment_mdp_data_at(mdp, non_profiled_offset);\n-          jmp(done);\n-          bind(found_null);\n-        } else {\n-          jcc(Assembler::notZero, done);\n-        }\n+        Label found_null;\n+        jccb(Assembler::zero, found_null);\n+        \/\/ Item did not match any saved item and there is no empty row for it.\n+        \/\/ Increment total counter to indicate polymorphic case.\n+        increment_mdp_data_at(mdp, in_bytes(CounterData::count_offset()));\n+        jmp(done);\n+        bind(found_null);\n@@ -1957,1 +1936,1 @@\n-        item_offset_fn, item_count_offset_fn, non_profiled_offset);\n+                                    item_offset_fn, item_count_offset_fn);\n@@ -2003,3 +1982,1 @@\n-void InterpreterMacroAssembler::record_klass_in_profile(Register receiver,\n-                                                        Register mdp, Register reg2,\n-                                                        bool is_virtual_call) {\n+void InterpreterMacroAssembler::record_klass_in_profile(Register receiver, Register mdp, Register reg2) {\n@@ -2009,1 +1986,1 @@\n-  record_klass_in_profile_helper(receiver, mdp, reg2, 0, done, is_virtual_call);\n+  record_klass_in_profile_helper(receiver, mdp, reg2, 0, done);\n@@ -2073,19 +2050,0 @@\n-void InterpreterMacroAssembler::profile_typecheck_failed(Register mdp) {\n-  if (ProfileInterpreter && TypeProfileCasts) {\n-    Label profile_continue;\n-\n-    \/\/ If no method data exists, go to profile_continue.\n-    test_method_data_pointer(mdp, profile_continue);\n-\n-    int count_offset = in_bytes(CounterData::count_offset());\n-    \/\/ Back up the address, since we have already bumped the mdp.\n-    count_offset -= in_bytes(VirtualCallData::virtual_call_data_size());\n-\n-    \/\/ *Decrement* the counter.  We expect to see zero or small negatives.\n-    increment_mdp_data_at(mdp, count_offset, true);\n-\n-    bind (profile_continue);\n-  }\n-}\n-\n-\n@@ -2105,1 +2063,1 @@\n-      record_klass_in_profile(klass, mdp, reg2, false);\n+      record_klass_in_profile(klass, mdp, reg2);\n@@ -2167,3 +2125,3 @@\n-void InterpreterMacroAssembler::profile_array(Register mdp,\n-                                              Register array,\n-                                              Register tmp) {\n+template <class ArrayData> void InterpreterMacroAssembler::profile_array_type(Register mdp,\n+                                                                              Register array,\n+                                                                              Register tmp) {\n@@ -2177,1 +2135,1 @@\n-    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadStoreData::array_offset())));\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayData::array_offset())));\n@@ -2182,1 +2140,1 @@\n-    set_mdp_flag_at(mdp, ArrayLoadStoreData::flat_array_byte_constant());\n+    set_mdp_flag_at(mdp, ArrayData::flat_array_byte_constant());\n@@ -2189,1 +2147,1 @@\n-    set_mdp_flag_at(mdp, ArrayLoadStoreData::null_free_array_byte_constant());\n+    set_mdp_flag_at(mdp, ArrayData::null_free_array_byte_constant());\n@@ -2197,3 +2155,39 @@\n-void InterpreterMacroAssembler::profile_element(Register mdp,\n-                                                Register element,\n-                                                Register tmp) {\n+template void InterpreterMacroAssembler::profile_array_type<ArrayLoadData>(Register mdp,\n+                                                                           Register array,\n+                                                                           Register tmp);\n+template void InterpreterMacroAssembler::profile_array_type<ArrayStoreData>(Register mdp,\n+                                                                            Register array,\n+                                                                            Register tmp);\n+\n+\n+void InterpreterMacroAssembler::profile_multiple_element_types(Register mdp, Register element, Register tmp, const Register tmp2) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    Label done, update;\n+    testptr(element, element);\n+    jccb(Assembler::notZero, update);\n+    set_mdp_flag_at(mdp, BitData::null_seen_byte_constant());\n+    jmp(done);\n+\n+    bind(update);\n+    load_klass(tmp, element, rscratch1);\n+\n+    \/\/ Record the object type.\n+    record_klass_in_profile(tmp, mdp, tmp2);\n+\n+    bind(done);\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayStoreData::array_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_element_type(Register mdp,\n+                                                     Register element,\n+                                                     Register tmp) {\n@@ -2207,1 +2201,1 @@\n-    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadStoreData::element_offset())));\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadData::element_offset())));\n@@ -2210,1 +2204,1 @@\n-    update_mdp_by_constant(mdp, in_bytes(ArrayLoadStoreData::array_load_store_data_size()));\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadData::array_load_data_size()));\n@@ -2359,1 +2353,1 @@\n-    imull(index, index, sizeof(ResolvedIndyEntry)); \/\/ Scale the index to be the entry index * sizeof(ResolvedInvokeDynamicInfo)\n+    imull(index, index, sizeof(ResolvedIndyEntry)); \/\/ Scale the index to be the entry index * sizeof(ResolvedIndyEntry)\n@@ -2363,0 +2357,15 @@\n+\n+void InterpreterMacroAssembler::load_field_entry(Register cache, Register index, int bcp_offset) {\n+  \/\/ Get index out of bytecode pointer\n+  movptr(cache, Address(rbp, frame::interpreter_frame_cache_offset * wordSize));\n+  get_cache_index_at_bcp(index, bcp_offset, sizeof(u2));\n+\n+  movptr(cache, Address(cache, ConstantPoolCache::field_entries_offset()));\n+  \/\/ Take shortcut if the size is a power of 2\n+  if (is_power_of_2(sizeof(ResolvedFieldEntry))) {\n+    shll(index, log2i_exact(sizeof(ResolvedFieldEntry))); \/\/ Scale index by power of 2\n+  } else {\n+    imull(index, index, sizeof(ResolvedFieldEntry)); \/\/ Scale the index to be the entry index * sizeof(ResolvedFieldEntry)\n+  }\n+  lea(cache, Address(cache, index, Address::times_1, Array<ResolvedFieldEntry>::base_offset_in_bytes()));\n+}\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":96,"deletions":87,"binary":false,"changes":183,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"utilities\/checkedCast.hpp\"\n@@ -1808,1 +1809,1 @@\n-  intx cache_line_size = prefetch_data_size();\n+  int cache_line_size = checked_cast<int>(prefetch_data_size());\n@@ -1916,1 +1917,1 @@\n-        log->print_cr(\" at distance %d, %d lines of %d bytes\", (int) AllocatePrefetchDistance, (int) AllocatePrefetchLines, (int) AllocatePrefetchStepSize);\n+        log->print_cr(\" at distance %d, %d lines of %d bytes\", AllocatePrefetchDistance, AllocatePrefetchLines, AllocatePrefetchStepSize);\n@@ -1918,1 +1919,1 @@\n-        log->print_cr(\" at distance %d, one line of %d bytes\", (int) AllocatePrefetchDistance, (int) AllocatePrefetchStepSize);\n+        log->print_cr(\" at distance %d, one line of %d bytes\", AllocatePrefetchDistance, AllocatePrefetchStepSize);\n@@ -3172,1 +3173,1 @@\n-intx VM_Version::allocate_prefetch_distance(bool use_watermark_prefetch) {\n+int VM_Version::allocate_prefetch_distance(bool use_watermark_prefetch) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -753,1 +753,1 @@\n-  static intx allocate_prefetch_distance(bool use_watermark_prefetch);\n+  static int allocate_prefetch_distance(bool use_watermark_prefetch);\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1257,2 +1257,12 @@\n-    Flag_intel_jcc_erratum = Node::_last_flag << 1,\n-    _last_flag             = Flag_intel_jcc_erratum\n+    Flag_intel_jcc_erratum    = Node::_last_flag << 1,\n+    Flag_sets_carry_flag      = Node::_last_flag << 2,\n+    Flag_sets_parity_flag     = Node::_last_flag << 3,\n+    Flag_sets_zero_flag       = Node::_last_flag << 4,\n+    Flag_sets_overflow_flag   = Node::_last_flag << 5,\n+    Flag_sets_sign_flag       = Node::_last_flag << 6,\n+    Flag_clears_carry_flag    = Node::_last_flag << 7,\n+    Flag_clears_parity_flag   = Node::_last_flag << 8,\n+    Flag_clears_zero_flag     = Node::_last_flag << 9,\n+    Flag_clears_overflow_flag = Node::_last_flag << 10,\n+    Flag_clears_sign_flag     = Node::_last_flag << 11,\n+    _last_flag                = Flag_clears_sign_flag\n@@ -1569,0 +1579,2 @@\n+    case Op_FmaF:\n+    case Op_FmaD:\n@@ -3962,1 +3974,0 @@\n-  predicate(UseFMA);\n@@ -3967,0 +3978,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -3974,1 +3986,0 @@\n-  predicate(UseFMA);\n@@ -3979,0 +3990,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -9866,0 +9878,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n@@ -9880,0 +9893,1 @@\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":18,"deletions":4,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -41,0 +41,2 @@\n+\n+  static bool profile_all_receivers_at_type_check() { return false; }\n","filename":"src\/hotspot\/cpu\/zero\/vm_version_zero.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+#include \"utilities\/checkedCast.hpp\"\n@@ -1014,2 +1015,2 @@\n-      int index = stream()->get_constant_pool_index();\n-      BasicType type = stream()->get_basic_type_for_constant_at(index);\n+      int cp_index = stream()->get_constant_pool_index();\n+      BasicType type = stream()->get_basic_type_for_constant_at(cp_index);\n@@ -2455,3 +2456,4 @@\n-          if (cha_monomorphic_target->holder() != compilation()->env()->Object_klass()) {\n-            ciInstanceKlass* holder = cha_monomorphic_target->holder();\n-            ciInstanceKlass* constraint = (holder->is_subtype_of(singleton) ? holder : singleton); \/\/ avoid upcasts\n+          ciInstanceKlass* holder = cha_monomorphic_target->holder();\n+          ciInstanceKlass* constraint = (holder->is_subtype_of(singleton) ? holder : singleton); \/\/ avoid upcasts\n+          if (holder != compilation()->env()->Object_klass() &&\n+              (!type_is_exact || receiver_klass->is_subtype_of(constraint))) {\n@@ -2470,1 +2472,1 @@\n-            cha_monomorphic_target = nullptr; \/\/ subtype check against Object is useless\n+            cha_monomorphic_target = nullptr;\n@@ -4845,1 +4847,1 @@\n-  CompileTask::print_inlining_ul(callee, scope()->level(), bci(), msg);\n+  CompileTask::print_inlining_ul(callee, scope()->level(), bci(), inlining_result_of(success), msg);\n@@ -4850,1 +4852,1 @@\n-  CompileTask::print_inlining_tty(callee, scope()->level(), bci(), msg);\n+  CompileTask::print_inlining_tty(callee, scope()->level(), bci(), inlining_result_of(success), msg);\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":10,"deletions":8,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -1745,0 +1745,1 @@\n+        close(fd);\n@@ -1769,0 +1770,1 @@\n+        close(fd);\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -77,1 +77,1 @@\n-bool ciInlineKlass::flat_array() const {\n+bool ciInlineKlass::flat_in_array() const {\n","filename":"src\/hotspot\/share\/ci\/ciInlineKlass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -56,1 +56,1 @@\n-    ciInstanceKlass(name, loader, protection_domain, T_PRIMITIVE_OBJECT) {}\n+    ciInstanceKlass(name, loader, protection_domain, T_OBJECT) {}\n@@ -81,1 +81,1 @@\n-  bool flat_array() const;\n+  bool flat_in_array() const;\n","filename":"src\/hotspot\/share\/ci\/ciInlineKlass.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -510,1 +510,1 @@\n-    if (element_type == T_OBJECT || element_type == T_PRIMITIVE_OBJECT) {\n+    if (element_type == T_OBJECT) {\n","filename":"src\/hotspot\/share\/ci\/ciObjectFactory.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -281,1 +281,1 @@\n-constantTag ciBytecodeStream::get_constant_pool_tag(int index) const {\n+constantTag ciBytecodeStream::get_constant_pool_tag(int cp_index) const {\n@@ -283,1 +283,1 @@\n-  return _method->get_Method()->constants()->constant_tag_at(index);\n+  return _method->get_Method()->constants()->constant_tag_at(cp_index);\n@@ -297,1 +297,1 @@\n-BasicType ciBytecodeStream::get_basic_type_for_constant_at(int index) const {\n+BasicType ciBytecodeStream::get_basic_type_for_constant_at(int cp_index) const {\n@@ -299,1 +299,1 @@\n-  return _method->get_Method()->constants()->basic_type_for_constant_at(index);\n+  return _method->get_Method()->constants()->basic_type_for_constant_at(cp_index);\n@@ -313,1 +313,1 @@\n-  return get_index_u2_cpcache();\n+  return get_index_u2();\n","filename":"src\/hotspot\/share\/ci\/ciStreams.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-  _basic_type = k->is_array_klass() ? T_ARRAY : (k->is_inline_klass() ? T_PRIMITIVE_OBJECT : T_OBJECT);\n+  _basic_type = k->is_array_klass() ? T_ARRAY : T_OBJECT;\n","filename":"src\/hotspot\/share\/ci\/ciType.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -81,0 +81,1 @@\n+#include \"utilities\/checkedCast.hpp\"\n@@ -875,1 +876,0 @@\n-                                       bool* const is_declared_atomic,\n@@ -993,0 +993,3 @@\n+    _jdk_internal_ImplicitlyConstructible,\n+    _jdk_internal_LooselyConsistentValue,\n+    _jdk_internal_NullRestricted,\n@@ -1010,1 +1013,1 @@\n-    _annotations_present |= nth_bit((int)id);\n+    _annotations_present |= (int)nth_bit((int)id);\n@@ -1015,1 +1018,1 @@\n-    _annotations_present &= ~nth_bit((int)id);\n+    _annotations_present &= (int)~nth_bit((int)id);\n@@ -1224,1 +1227,1 @@\n-      int multifield_arg = cp->int_at(arg_index);\n+      int multifield_arg = const_cast<ConstantPool*>(cp)->int_at(arg_index);\n@@ -1570,0 +1573,2 @@\n+    bool is_null_restricted = false;\n+\n@@ -1586,2 +1591,11 @@\n-\n-        fields_annotations->at_put_grow(field_index, parsed_annotations.field_annotations(), nullptr);\n+        fields_annotations->at_put_grow(n, parsed_annotations.field_annotations(), nullptr);\n+        if (parsed_annotations.has_annotation(AnnotationCollector::_jdk_internal_NullRestricted)) {\n+          if (!Signature::has_envelope(sig)) {\n+            Exceptions::fthrow(\n+              THREAD_AND_LOCATION,\n+              vmSymbols::java_lang_ClassFormatError(),\n+              \"Illegal use of @jdk.internal.vm.annotation.NullRestricted annotation on field %s with signature %s (primitive types can never be null)\",\n+              name->as_C_string(), sig->as_C_string());\n+          }\n+          is_null_restricted = true;\n+        }\n@@ -1590,0 +1604,1 @@\n+\n@@ -1615,1 +1630,1 @@\n-    if (type == T_PRIMITIVE_OBJECT) fieldFlags.update_null_free_inline_type(true);\n+    if (type == T_PRIMITIVE_OBJECT || is_null_restricted) fieldFlags.update_null_free_inline_type(true);\n@@ -1949,2 +1964,2 @@\n-  const unsigned int size =\n-    (*localvariable_table_length) * sizeof(Classfile_LVT_Element) \/ sizeof(u2);\n+  const unsigned int size = checked_cast<unsigned>(\n+    (*localvariable_table_length) * sizeof(Classfile_LVT_Element) \/ sizeof(u2));\n@@ -2179,0 +2194,12 @@\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_ImplicitlyConstructible_signature): {\n+      if (_location != _in_class)   break; \/\/ only allow for classes\n+      return _jdk_internal_ImplicitlyConstructible;\n+    }\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_LooselyConsistentValue_signature): {\n+      if (_location != _in_class)   break; \/\/ only allow for classes\n+      return _jdk_internal_LooselyConsistentValue;\n+    }\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_NullRestricted_signature): {\n+      if (_location != _in_field)   break; \/\/ only allow for fields\n+      return _jdk_internal_NullRestricted;\n+    }\n@@ -2627,1 +2654,1 @@\n-      calculated_attribute_length +=\n+      calculated_attribute_length += checked_cast<unsigned int>(\n@@ -2635,1 +2662,1 @@\n-              sizeof(u2) );  \/\/ catch_type_index\n+              sizeof(u2) )); \/\/ catch_type_index\n@@ -2642,2 +2669,2 @@\n-                                       sizeof(code_attribute_name_index) +\n-                                       sizeof(code_attribute_length);\n+                                       (unsigned)sizeof(code_attribute_name_index) +\n+                                       (unsigned)sizeof(code_attribute_length);\n@@ -2757,1 +2784,1 @@\n-      const u2 real_length = (method_parameters_length * 4u) + 1u;\n+      const u4 real_length = (method_parameters_length * 4u) + 1u;\n@@ -3535,1 +3562,1 @@\n-u2 ClassFileParser::parse_classfile_record_attribute(const ClassFileStream* const cfs,\n+u4 ClassFileParser::parse_classfile_record_attribute(const ClassFileStream* const cfs,\n@@ -3737,1 +3764,1 @@\n-  const int operand_count = (attribute_byte_length - sizeof(u2)) \/ sizeof(u2);\n+  const unsigned int operand_count = (attribute_byte_length - (unsigned)sizeof(u2)) \/ (unsigned)sizeof(u2);\n@@ -5188,0 +5215,10 @@\n+bool ClassFileParser::is_class_in_preload_attribute(Symbol *klass) {\n+  if (_preload_classes == nullptr) return false;\n+  for (int i = 0; i < _preload_classes->length(); i++) {\n+        \/\/ if (_cp->tag_at(_preload_classes->at(i)).is_klass()) continue;\n+        Symbol* class_name = _cp->klass_at_noresolve(_preload_classes->at(i));\n+        if (class_name == klass) return true;\n+  }\n+  return false;\n+}\n+\n@@ -5316,1 +5353,1 @@\n-          int newlen = c - (char*) signature;\n+          int newlen = pointer_delta_as_int(c, (char*) signature);\n@@ -5538,1 +5575,1 @@\n-      length -= nextp - p;\n+      length -= pointer_delta_as_int(nextp, p);\n@@ -5771,1 +5808,1 @@\n-  ik->set_initial_method_idnum(ik->methods()->length());\n+  ik->set_initial_method_idnum(checked_cast<u2>(ik->methods()->length()));\n@@ -5790,3 +5827,6 @@\n-  if (_is_declared_atomic) {\n-    ik->set_is_declared_atomic();\n-  }\n+  if (_must_be_atomic) {\n+    ik->set_must_be_atomic();\n+  }\n+  if (_is_implicitly_constructible) {\n+    ik->set_is_implicitly_constructible();\n+  }\n@@ -6067,1 +6107,2 @@\n-  _is_declared_atomic(false),\n+  _must_be_atomic(true),\n+  _is_implicitly_constructible(false),\n@@ -6070,0 +6111,2 @@\n+  _has_loosely_consistent_annotation(false),\n+  _has_implicitly_constructible_annotation(false),\n@@ -6401,1 +6444,0 @@\n-                   &_is_declared_atomic,\n@@ -6552,2 +6594,13 @@\n-    if (_super_klass->is_declared_atomic()) {\n-      _is_declared_atomic = true;\n+\n+    if (EnableValhalla && _access_flags.is_value_class()) {\n+      const InstanceKlass* k = _super_klass;\n+      int inherited_instance_fields = 0;\n+      while (k != nullptr) {\n+        for (AllFieldStream fs(k); !fs.done(); fs.next()) {\n+          if (!fs.access_flags().is_static()) inherited_instance_fields++;\n+        }\n+        k = k->super() == nullptr ? nullptr :  InstanceKlass::cast(k->super());\n+      }\n+      if (inherited_instance_fields > 0) {\n+        THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(), \"Value classes don't support inherited non-static fields yet\");\n+      }\n@@ -6557,5 +6610,34 @@\n-  if (*ForceNonTearable != '\\0') {\n-    \/\/ Allow a command line switch to force the same atomicity property:\n-    const char* class_name_str = _class_name->as_C_string();\n-    if (StringUtils::class_list_match(ForceNonTearable, class_name_str)) {\n-      _is_declared_atomic = true;\n+  if (_parsed_annotations->has_annotation(AnnotationCollector::_jdk_internal_LooselyConsistentValue) && !_access_flags.is_value_class()) {\n+    THROW_MSG(vmSymbols::java_lang_ClassFormatError(),\n+          err_msg(\"class %s cannot have annotation jdk.internal.vm.annotation.LooselyConsistentValue, because it is not a value class\",\n+                  _class_name->as_klass_external_name()));\n+  }\n+  if (_parsed_annotations->has_annotation(AnnotationCollector::_jdk_internal_ImplicitlyConstructible) && !_access_flags.is_value_class()) {\n+    THROW_MSG(vmSymbols::java_lang_ClassFormatError(),\n+          err_msg(\"class %s cannot have annotation jdk.internal.vm.annotation.ImplicitlyConstructible, because it is not a value class\",\n+                  _class_name->as_klass_external_name()));\n+  }\n+\n+  \/\/ Determining is the class allows tearing or not (default is not)\n+  \/\/ Test might need extensions when field inheritance is added for value classes\n+  if (EnableValhalla && _access_flags.is_value_class()) {\n+    if (_access_flags.is_primitive_class()) {\n+      _must_be_atomic = false;             \/\/ old semantic, primitive classes are always non-atomic\n+      _is_implicitly_constructible = true; \/\/ old semantic, primitive classes are always implicitly constructible\n+    } else {\n+      if (_super_klass != nullptr  \/\/ not j.l.Object\n+               && _parsed_annotations->has_annotation(ClassAnnotationCollector::_jdk_internal_LooselyConsistentValue)\n+               && (_super_klass == vmClasses::Object_klass() || !_super_klass->must_be_atomic())) {\n+        _must_be_atomic = false;\n+      }\n+      if (_parsed_annotations->has_annotation(ClassAnnotationCollector::_jdk_internal_ImplicitlyConstructible)) {\n+        _is_implicitly_constructible = true;\n+      }\n+    }\n+    \/\/ Apply VM options override\n+    if (*ForceNonTearable != '\\0') {\n+      \/\/ Allow a command line switch to force the same atomicity property:\n+      const char* class_name_str = _class_name->as_C_string();\n+      if (StringUtils::class_list_match(ForceNonTearable, class_name_str)) {\n+        _must_be_atomic = true;\n+      }\n@@ -6607,3 +6689,0 @@\n-      if (InstanceKlass::cast(interf)->is_declared_atomic()) {\n-        _is_declared_atomic = true;\n-      }\n@@ -6648,2 +6727,1 @@\n-\n-  if (EnablePrimitiveClasses || is_jdk_internal_class(_super_klass)) {\n+  if (EnableValhalla || is_jdk_internal_class(_super_klass)) {\n@@ -6657,2 +6735,1 @@\n-\n-        \/\/ Pre-load inline class\n+        \/\/ Pre-load classes of fields that are candidate for flattening\n@@ -6667,4 +6744,4 @@\n-            THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n-                      err_msg(\"Class %s expects class %s to be an inline type, but it is not\",\n-                      _class_name->as_C_string(),\n-                      InstanceKlass::cast(klass)->external_name()));\n+          THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                    err_msg(\"Class %s expects class %s to be an inline type, but it is not\",\n+                    _class_name->as_C_string(),\n+                    InstanceKlass::cast(klass)->external_name()));\n@@ -6673,0 +6750,19 @@\n+      } else {\n+        if (sig != _class_name && is_class_in_preload_attribute(sig)) {\n+          oop loader = loader_data()->class_loader();\n+          Klass* klass = SystemDictionary::resolve_or_null(sig, Handle(THREAD, loader), _protection_domain, THREAD);\n+          if (HAS_PENDING_EXCEPTION) {\n+            CLEAR_PENDING_EXCEPTION;\n+          }\n+          \/\/ Should we verify that klass is a value class? What the PreLoad attribute spec says about that?\n+          if (klass != nullptr) {\n+            if (klass->is_inline_klass()) {\n+              _inline_type_field_klasses->at_put(fieldinfo.index(), InlineKlass::cast(klass));\n+              log_info(class, preload)(\"Preloading class %s during linking of class %s because of its Preload attribute\", sig->as_C_string(), _class_name->as_C_string());\n+            } else {\n+              log_info(class, preload)(\"Preloading class %s during linking of class %s because of its Preload attribute but loaded class is not a value class\", sig->as_C_string(), _class_name->as_C_string());\n+            }\n+          } else {\n+            log_warning(class, preload)(\"Preloading of class %s during linking of class %s (Preload attribute) failed\", sig->as_C_string(), _class_name->as_C_string());\n+          }\n+        }\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":138,"deletions":42,"binary":false,"changes":180,"status":"modified"},{"patch":"@@ -211,1 +211,2 @@\n-  bool _is_declared_atomic;\n+  bool _must_be_atomic;\n+  bool _is_implicitly_constructible;\n@@ -214,0 +215,2 @@\n+  bool _has_loosely_consistent_annotation;\n+  bool _has_implicitly_constructible_annotation;\n@@ -262,1 +265,0 @@\n-                        bool* is_declared_atomic,\n@@ -360,1 +362,1 @@\n-  u2 parse_classfile_record_attribute(const ClassFileStream* const cfs,\n+  u4 parse_classfile_record_attribute(const ClassFileStream* const cfs,\n@@ -626,0 +628,2 @@\n+  bool is_class_in_preload_attribute(Symbol *klass);\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.hpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -671,0 +671,2 @@\n+  _atomic_field_count(0),\n+  _fields_size_sum(0),\n@@ -677,1 +679,1 @@\n-  _atomic_field_count(0)\n+  _nullable_atomic_flat_candidate(false)\n@@ -711,1 +713,1 @@\n-void FieldLayoutBuilder::regular_field_sorting() {\n+void FieldLayoutBuilder::regular_field_sorting(TRAPS) {\n@@ -763,0 +765,9 @@\n+            \/\/ Check below is performed for non-static fields, it should be performed for static fields too but at this stage,\n+            \/\/ it is not guaranteed that the klass of the static field has been loaded, so the test for static fields is delayed\n+            \/\/ until the linking phase\n+            Klass* klass =  _inline_type_field_klasses->at(idx);\n+            assert(klass != nullptr, \"Sanity check\");\n+            InlineKlass* vk = InlineKlass::cast(klass);\n+            if (!vk->is_implicitly_constructible()) {\n+              THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(), \"Null restricted fields with a non-implicitly constructible class are not supported\");\n+            }\n@@ -767,4 +778,0 @@\n-            JavaThread* THREAD = JavaThread::current();\n-            Klass* klass =  _inline_type_field_klasses->at(idx);\n-            assert(klass != nullptr, \"Sanity check\");\n-            InlineKlass* vk = InlineKlass::cast(klass);\n@@ -773,1 +780,1 @@\n-            bool too_atomic_to_flatten = vk->is_declared_atomic() || AlwaysAtomicAccesses;\n+            bool too_atomic_to_flatten = vk->must_be_atomic() || AlwaysAtomicAccesses;\n@@ -793,3 +800,3 @@\n-          break;\n-        default:\n-          fatal(\"Something wrong?\");\n+        break;\n+      default:\n+        fatal(\"Something wrong?\");\n@@ -838,1 +845,0 @@\n-      assert(group != nullptr, \"invariant\");\n@@ -869,4 +875,3 @@\n-            \/\/ Flattening decision to be taken here\n-            \/\/ This code assumes all verifications have already been performed\n-            \/\/ (field's type has been loaded and it is an inline klass)\n-            JavaThread* THREAD = JavaThread::current();\n+            \/\/ Check below is performed for non-static fields, it should be performed for static fields too but at this stage,\n+            \/\/ it is not guaranteed that the klass of the static field has been loaded, so the test for static fields is delayed\n+            \/\/ until the linking phase\n@@ -876,0 +881,6 @@\n+            if (!vk->is_implicitly_constructible()) {\n+              THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(), \"Null restricted fields with a non-implicitly constructible class are not supported\");\n+            }\n+            \/\/ Flattening decision to be taken here\n+            \/\/ This code assumes all verifications have already been performed\n+            \/\/ (field's type has been loaded and it is an inline klass)\n@@ -878,1 +889,1 @@\n-            bool too_atomic_to_flatten = vk->is_declared_atomic() || AlwaysAtomicAccesses;\n+            bool too_atomic_to_flatten = vk->must_be_atomic() || AlwaysAtomicAccesses;\n@@ -940,1 +951,1 @@\n-void FieldLayoutBuilder::compute_regular_layout() {\n+void FieldLayoutBuilder::compute_regular_layout(TRAPS) {\n@@ -943,1 +954,1 @@\n-  regular_field_sorting();\n+  regular_field_sorting(CHECK);\n@@ -1155,1 +1166,1 @@\n-    compute_regular_layout();\n+    compute_regular_layout(CHECK);\n","filename":"src\/hotspot\/share\/classfile\/fieldLayoutBuilder.cpp","additions":30,"deletions":19,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -299,0 +299,2 @@\n+  int _atomic_field_count;\n+  int _fields_size_sum;\n@@ -305,1 +307,1 @@\n-  int _atomic_field_count;\n+  bool _nullable_atomic_flat_candidate;\n@@ -331,1 +333,1 @@\n-  void compute_regular_layout();\n+  void compute_regular_layout(TRAPS);\n@@ -338,1 +340,1 @@\n-  void regular_field_sorting();\n+  void regular_field_sorting(TRAPS);\n","filename":"src\/hotspot\/share\/classfile\/fieldLayoutBuilder.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -91,0 +91,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -2067,1 +2068,1 @@\n-  switch (state) {\n+  switch (state & ~SUSPENDED) {\n@@ -2073,1 +2074,0 @@\n-    case RUNNABLE_SUSPENDED :\n@@ -2076,0 +2076,1 @@\n+    case TIMED_PARKING:\n@@ -2080,1 +2081,0 @@\n-    case PARKED_SUSPENDED :\n@@ -2084,0 +2084,4 @@\n+    case TIMED_PARKED:\n+    case TIMED_PINNED:\n+      status = JavaThreadStatus::PARKED_TIMED;\n+      break;\n@@ -3037,0 +3041,61 @@\n+\/\/ java_lang_ClassFrameInfo\n+\n+int java_lang_ClassFrameInfo::_classOrMemberName_offset;\n+int java_lang_ClassFrameInfo::_flags_offset;\n+\n+#define CLASSFRAMEINFO_FIELDS_DO(macro) \\\n+  macro(_classOrMemberName_offset, k, \"classOrMemberName\", object_signature,  false); \\\n+  macro(_flags_offset,             k, vmSymbols::flags_name(), int_signature, false)\n+\n+void java_lang_ClassFrameInfo::compute_offsets() {\n+  InstanceKlass* k = vmClasses::ClassFrameInfo_klass();\n+  CLASSFRAMEINFO_FIELDS_DO(FIELD_COMPUTE_OFFSET);\n+}\n+\n+#if INCLUDE_CDS\n+void java_lang_ClassFrameInfo::serialize_offsets(SerializeClosure* f) {\n+  CLASSFRAMEINFO_FIELDS_DO(FIELD_SERIALIZE_OFFSET);\n+}\n+#endif\n+\n+static int get_flags(const methodHandle& m) {\n+  int flags = (jushort)( m->access_flags().as_short() & JVM_RECOGNIZED_METHOD_MODIFIERS );\n+  if (m->is_object_constructor()) {\n+    flags |= java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR;\n+  } else {\n+    flags |= java_lang_invoke_MemberName::MN_IS_METHOD;\n+  }\n+  if (m->caller_sensitive()) {\n+    flags |= java_lang_invoke_MemberName::MN_CALLER_SENSITIVE;\n+  }\n+  if (m->is_hidden()) {\n+    flags |= java_lang_invoke_MemberName::MN_HIDDEN_MEMBER;\n+  }\n+  assert((flags & 0xFF000000) == 0, \"unexpected flags\");\n+  return flags;\n+}\n+\n+oop java_lang_ClassFrameInfo::classOrMemberName(oop obj) {\n+  return obj->obj_field(_classOrMemberName_offset);\n+}\n+\n+int java_lang_ClassFrameInfo::flags(oop obj) {\n+  return obj->int_field(_flags_offset);\n+}\n+\n+void java_lang_ClassFrameInfo::init_class(Handle stackFrame, const methodHandle& m) {\n+  stackFrame->obj_field_put(_classOrMemberName_offset, m->method_holder()->java_mirror());\n+  \/\/ flags is initialized when ClassFrameInfo object is constructed and retain the value\n+  int flags = java_lang_ClassFrameInfo::flags(stackFrame()) | get_flags(m);\n+  stackFrame->int_field_put(_flags_offset, flags);\n+}\n+\n+void java_lang_ClassFrameInfo::init_method(Handle stackFrame, const methodHandle& m, TRAPS) {\n+  oop rmethod_name = java_lang_invoke_ResolvedMethodName::find_resolved_method(m, CHECK);\n+  stackFrame->obj_field_put(_classOrMemberName_offset, rmethod_name);\n+  \/\/ flags is initialized when ClassFrameInfo object is constructed and retain the value\n+  int flags = java_lang_ClassFrameInfo::flags(stackFrame()) | get_flags(m);\n+  stackFrame->int_field_put(_flags_offset, flags);\n+}\n+\n+\n@@ -3039,1 +3104,2 @@\n-int java_lang_StackFrameInfo::_memberName_offset;\n+int java_lang_StackFrameInfo::_type_offset;\n+int java_lang_StackFrameInfo::_name_offset;\n@@ -3045,1 +3111,2 @@\n-  macro(_memberName_offset, k, \"memberName\", object_signature,            false); \\\n+  macro(_type_offset,       k, \"type\",       object_signature,            false); \\\n+  macro(_name_offset,       k, \"name\",       string_signature,            false); \\\n@@ -3062,6 +3129,3 @@\n-Method* java_lang_StackFrameInfo::get_method(Handle stackFrame, InstanceKlass* holder, TRAPS) {\n-  HandleMark hm(THREAD);\n-  Handle mname(THREAD, stackFrame->obj_field(_memberName_offset));\n-  Method* method = (Method*)java_lang_invoke_MemberName::vmtarget(mname());\n-  \/\/ we should expand MemberName::name when Throwable uses StackTrace\n-  \/\/ MethodHandles::expand_MemberName(mname, MethodHandles::_suppress_defc|MethodHandles::_suppress_type, CHECK_NULL);\n+Method* java_lang_StackFrameInfo::get_method(oop obj) {\n+  oop m = java_lang_ClassFrameInfo::classOrMemberName(obj);\n+  Method* method = java_lang_invoke_ResolvedMethodName::vmtarget(m);\n@@ -3074,5 +3138,3 @@\n-  Handle mname(THREAD, stackFrame->obj_field(_memberName_offset));\n-  Handle cont_h (THREAD, cont);\n-  InstanceKlass* ik = method->method_holder();\n-  CallInfo info(method(), ik, CHECK);\n-  MethodHandles::init_method_MemberName(mname, info);\n+  Handle cont_h(THREAD, cont);\n+  java_lang_ClassFrameInfo::init_method(stackFrame, method, CHECK);\n+\n@@ -3093,4 +3155,2 @@\n-  Handle mname(THREAD, stackFrame->obj_field(java_lang_StackFrameInfo::_memberName_offset));\n-  Klass* clazz = java_lang_Class::as_Klass(java_lang_invoke_MemberName::clazz(mname()));\n-  InstanceKlass* holder = InstanceKlass::cast(clazz);\n-  Method* method = java_lang_StackFrameInfo::get_method(stackFrame, holder, CHECK);\n+  Method* method = java_lang_StackFrameInfo::get_method(stackFrame());\n+  InstanceKlass* holder = method->method_holder();\n@@ -3104,2 +3164,2 @@\n-void java_lang_StackFrameInfo::set_version(oop element, short value) {\n-  element->short_field_put(_version_offset, value);\n+oop java_lang_StackFrameInfo::type(oop obj) {\n+  return obj->obj_field(_type_offset);\n@@ -3108,1 +3168,17 @@\n-void java_lang_StackFrameInfo::set_bci(oop element, int value) {\n+void java_lang_StackFrameInfo::set_type(oop obj, oop value) {\n+  obj->obj_field_put(_type_offset, value);\n+}\n+\n+oop java_lang_StackFrameInfo::name(oop obj) {\n+  return obj->obj_field(_name_offset);\n+}\n+\n+void java_lang_StackFrameInfo::set_name(oop obj, oop value) {\n+  obj->obj_field_put(_name_offset, value);\n+}\n+\n+void java_lang_StackFrameInfo::set_version(oop obj, short value) {\n+  obj->short_field_put(_version_offset, value);\n+}\n+\n+void java_lang_StackFrameInfo::set_bci(oop obj, int value) {\n@@ -3110,1 +3186,1 @@\n-  element->int_field_put(_bci_offset, value);\n+  obj->int_field_put(_bci_offset, value);\n@@ -3113,2 +3189,2 @@\n-void java_lang_StackFrameInfo::set_contScope(oop element, oop value) {\n-  element->obj_field_put(_contScope_offset, value);\n+void java_lang_StackFrameInfo::set_contScope(oop obj, oop value) {\n+  obj->obj_field_put(_contScope_offset, value);\n@@ -3139,2 +3215,2 @@\n-void java_lang_LiveStackFrameInfo::set_monitors(oop element, oop value) {\n-  element->obj_field_put(_monitors_offset, value);\n+void java_lang_LiveStackFrameInfo::set_monitors(oop obj, oop value) {\n+  obj->obj_field_put(_monitors_offset, value);\n@@ -3143,2 +3219,2 @@\n-void java_lang_LiveStackFrameInfo::set_locals(oop element, oop value) {\n-  element->obj_field_put(_locals_offset, value);\n+void java_lang_LiveStackFrameInfo::set_locals(oop obj, oop value) {\n+  obj->obj_field_put(_locals_offset, value);\n@@ -3147,2 +3223,2 @@\n-void java_lang_LiveStackFrameInfo::set_operands(oop element, oop value) {\n-  element->obj_field_put(_operands_offset, value);\n+void java_lang_LiveStackFrameInfo::set_operands(oop obj, oop value) {\n+  obj->obj_field_put(_operands_offset, value);\n@@ -3151,2 +3227,2 @@\n-void java_lang_LiveStackFrameInfo::set_mode(oop element, int value) {\n-  element->int_field_put(_mode_offset, value);\n+void java_lang_LiveStackFrameInfo::set_mode(oop obj, int value) {\n+  obj->int_field_put(_mode_offset, value);\n@@ -3378,1 +3454,1 @@\n-int java_lang_reflect_Field::_trusted_final_offset;\n+int java_lang_reflect_Field::_flags_offset;\n@@ -3388,1 +3464,1 @@\n-  macro(_trusted_final_offset,    k, vmSymbols::trusted_final_name(),    bool_signature,       false); \\\n+  macro(_flags_offset,     k, vmSymbols::flags_name(),     int_signature,    false); \\\n@@ -3453,2 +3529,2 @@\n-void java_lang_reflect_Field::set_trusted_final(oop field) {\n-  field->bool_field_put(_trusted_final_offset, true);\n+void java_lang_reflect_Field::set_flags(oop field, int value) {\n+  field->int_field_put(_flags_offset, value);\n@@ -4028,0 +4104,3 @@\n+#define RESOLVEDMETHOD_FIELDS_DO(macro) \\\n+  macro(_vmholder_offset, k, \"vmholder\", class_signature, false)\n+\n@@ -4031,0 +4110,1 @@\n+  RESOLVEDMETHOD_FIELDS_DO(FIELD_COMPUTE_OFFSET);\n@@ -4036,0 +4116,1 @@\n+  RESOLVEDMETHOD_FIELDS_DO(FIELD_SERIALIZE_OFFSET);\n@@ -4793,1 +4874,1 @@\n-    _page_size = (int)os::vm_page_size();\n+    _page_size = AIX_ONLY(sysconf(_SC_PAGESIZE)) NOT_AIX((int)os::vm_page_size());\n@@ -5278,0 +5359,1 @@\n+  f(java_lang_ClassFrameInfo) \\\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":121,"deletions":39,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -110,1 +110,0 @@\n-  static inline void set_value_raw(oop string, typeArrayOop buffer);\n@@ -538,14 +537,15 @@\n-    NEW          = 0,\n-    STARTED      = 1,\n-    RUNNABLE     = 2,\n-    RUNNING      = 3,\n-    PARKING      = 4,\n-    PARKED       = 5,\n-    PINNED       = 6,\n-    YIELDING     = 7,\n-    TERMINATED   = 99,\n-\n-    \/\/ can be suspended from scheduling when unmounted\n-    SUSPENDED    = 1 << 8,\n-    RUNNABLE_SUSPENDED = (RUNNABLE | SUSPENDED),\n-    PARKED_SUSPENDED   = (PARKED | SUSPENDED)\n+    NEW           = 0,\n+    STARTED       = 1,\n+    RUNNABLE      = 2,\n+    RUNNING       = 3,\n+    PARKING       = 4,\n+    PARKED        = 5,\n+    PINNED        = 6,\n+    TIMED_PARKING = 7,\n+    TIMED_PARKED  = 8,\n+    TIMED_PINNED  = 9,\n+    YIELDING      = 10,\n+    TERMINATED    = 99,\n+\n+    \/\/ additional state bits\n+    SUSPENDED    = 1 << 8,   \/\/ suspended when unmounted\n@@ -774,1 +774,1 @@\n-  static int _trusted_final_offset;\n+  static int _flags_offset;\n@@ -802,1 +802,1 @@\n-  static void set_trusted_final(oop field);\n+  static void set_flags(oop field, int value);\n@@ -1215,1 +1215,0 @@\n-  macro(java_lang_invoke_ResolvedMethodName, vmholder, object_signature, false) \\\n@@ -1304,3 +1303,5 @@\n-    MN_FLAT_FIELD            = 0x00400000, \/\/ flat field\n-    MN_REFERENCE_KIND_SHIFT  = 24, \/\/ refKind\n-    MN_REFERENCE_KIND_MASK   = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT,\n+    MN_HIDDEN_MEMBER         = 0x00400000, \/\/ @Hidden annotation detected\n+    MN_FLAT_FIELD            = 0x00800000, \/\/ flat field\n+    MN_NULL_RESTRICTED_FIELD = 0x01000000, \/\/ null-restricted field\n+    MN_REFERENCE_KIND_SHIFT  = 26, \/\/ refKind\n+    MN_REFERENCE_KIND_MASK   = 0x3C000000 >> MN_REFERENCE_KIND_SHIFT,\n@@ -1608,0 +1609,20 @@\n+class java_lang_ClassFrameInfo: AllStatic {\n+private:\n+  static int _classOrMemberName_offset;\n+  static int _flags_offset;\n+\n+public:\n+  static oop  classOrMemberName(oop info);\n+  static int  flags(oop info);\n+\n+  \/\/ Setters\n+  static void init_class(Handle stackFrame, const methodHandle& m);\n+  static void init_method(Handle stackFrame, const methodHandle& m, TRAPS);\n+\n+  static void compute_offsets();\n+  static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;\n+\n+  \/\/ Debugging\n+  friend class JavaClasses;\n+};\n+\n@@ -1615,1 +1636,2 @@\n-  static int _memberName_offset;\n+  static int _type_offset;\n+  static int _name_offset;\n@@ -1620,2 +1642,5 @@\n-  static Method* get_method(Handle stackFrame, InstanceKlass* holder, TRAPS);\n-\n+  \/\/ Getters\n+  static oop name(oop info);\n+  static oop type(oop info);\n+  static Method* get_method(oop info);\n+\n@@ -1625,0 +1650,2 @@\n+  static void set_name(oop info, oop value);\n+  static void set_type(oop info, oop value);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":51,"deletions":24,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -164,0 +164,1 @@\n+  do_klass(ClassFrameInfo_klass,                        java_lang_ClassFrameInfo                              ) \\\n","filename":"src\/hotspot\/share\/classfile\/vmClassMacros.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -37,1 +37,0 @@\n-#include \"utilities\/tribool.hpp\"\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -320,0 +320,3 @@\n+  template(jdk_internal_vm_annotation_ImplicitlyConstructible_signature,     \"Ljdk\/internal\/vm\/annotation\/ImplicitlyConstructible;\") \\\n+  template(jdk_internal_vm_annotation_LooselyConsistentValue_signature,      \"Ljdk\/internal\/vm\/annotation\/LooselyConsistentValue;\") \\\n+  template(jdk_internal_vm_annotation_NullRestricted_signature,              \"Ljdk\/internal\/vm\/annotation\/NullRestricted;\") \\\n@@ -388,1 +391,0 @@\n-  template(trusted_final_name,                        \"trustedFinal\")                             \\\n@@ -469,0 +471,1 @@\n+  template(java_lang_ClassFrameInfo,                  \"java\/lang\/ClassFrameInfo\")                 \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -256,1 +256,1 @@\n-  jint selector = sv_selector->get_int();\n+  jint selector = sv_selector->get_jint();\n","filename":"src\/hotspot\/share\/code\/debugInfo.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -621,26 +621,0 @@\n-\/\/ Helper class to allocate arrays that may become large.\n-\/\/ Uses the OS malloc for allocations smaller than ArrayAllocatorMallocLimit\n-\/\/ and uses mapped memory for larger allocations.\n-\/\/ Most OS mallocs do something similar but Solaris malloc does not revert\n-\/\/ to mapped memory for large allocations. By default ArrayAllocatorMallocLimit\n-\/\/ is set so that we always use malloc except for Solaris where we set the\n-\/\/ limit to get mapped memory.\n-template <class E>\n-class ArrayAllocator : public AllStatic {\n- private:\n-  static bool should_use_malloc(size_t length);\n-\n-  static E* allocate_malloc(size_t length, MEMFLAGS flags);\n-  static E* allocate_mmap(size_t length, MEMFLAGS flags);\n-\n-  static E* reallocate_malloc(E* addr, size_t new_length, MEMFLAGS flags);\n-\n-  static void free_malloc(E* addr, size_t length);\n-  static void free_mmap(E* addr, size_t length);\n-\n- public:\n-  static E* allocate(size_t length, MEMFLAGS flags);\n-  static E* reallocate(E* old_addr, size_t old_length, size_t new_length, MEMFLAGS flags);\n-  static void free(E* addr, size_t length);\n-};\n-\n","filename":"src\/hotspot\/share\/memory\/allocation.hpp","additions":0,"deletions":26,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"utilities\/checkedCast.hpp\"\n","filename":"src\/hotspot\/share\/oops\/fieldInfo.inline.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -169,1 +169,1 @@\n-  if (is_declared_atomic() && !is_naturally_atomic()) {\n+  if (must_be_atomic() && !is_naturally_atomic()) {\n@@ -232,2 +232,2 @@\n-\/\/ The list of basic types that is returned starts with a T_PRIMITIVE_OBJECT\n-\/\/ and ends with an extra T_VOID. T_PRIMITIVE_OBJECT\/T_VOID pairs are used as\n+\/\/ The list of basic types that is returned starts with a T_METADATA\n+\/\/ and ends with an extra T_VOID. T_METADATA\/T_VOID pairs are used as\n@@ -236,1 +236,1 @@\n-\/\/ with a T_PRIMITIVE_OBJECT and ends with a T_VOID. This is so we can\n+\/\/ with a T_METADATA and ends with a T_VOID. This is so we can\n@@ -240,1 +240,1 @@\n-\/\/ T_PRIMITIVE_OBJECT, drop everything until and including the closing\n+\/\/ T_METADATA, drop everything until and including the closing\n@@ -242,1 +242,1 @@\n-\/\/ types is an argument: drop all T_PRIMITIVE_OBJECT\/T_VOID from the list).\n+\/\/ types is an argument: drop all T_METADATA\/T_VOID from the list).\n@@ -245,1 +245,1 @@\n-  SigEntry::add_entry(sig, T_PRIMITIVE_OBJECT, name(), base_off);\n+  SigEntry::add_entry(sig, T_METADATA, name(), base_off);\n@@ -268,1 +268,1 @@\n-  assert(sig->at(0)._bt == T_PRIMITIVE_OBJECT && sig->at(sig->length()-1)._bt == T_VOID, \"broken structure\");\n+  assert(sig->at(0)._bt == T_METADATA && sig->at(sig->length()-1)._bt == T_VOID, \"broken structure\");\n@@ -371,1 +371,1 @@\n-    if (bt == T_PRIMITIVE_OBJECT) {\n+    if (bt == T_METADATA) {\n@@ -399,1 +399,1 @@\n-    if (bt == T_PRIMITIVE_OBJECT) {\n+    if (bt == T_METADATA) {\n@@ -423,1 +423,1 @@\n-    if (bt == T_PRIMITIVE_OBJECT) {\n+    if (bt == T_METADATA) {\n","filename":"src\/hotspot\/share\/oops\/inlineKlass.cpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -187,1 +187,1 @@\n-  bool is_atomic() { return is_naturally_atomic() || is_declared_atomic(); }\n+  bool is_atomic() { return is_naturally_atomic() || must_be_atomic(); }\n","filename":"src\/hotspot\/share\/oops\/inlineKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -841,0 +841,2 @@\n+  bool debug_logging_enabled = log_is_enabled(Debug, class, init);\n+\n@@ -843,0 +845,5 @@\n+    if (debug_logging_enabled) {\n+      ResourceMark rm(current);\n+      log_debug(class, init)(\"Thread \\\"%s\\\" waiting for linking of %s by thread \\\"%s\\\"\",\n+                             current->name(), external_name(), init_thread_name());\n+    }\n@@ -848,0 +855,5 @@\n+    if (debug_logging_enabled) {\n+      ResourceMark rm(current);\n+      log_debug(class, init)(\"Thread \\\"%s\\\" recursively linking %s\",\n+                             current->name(), external_name());\n+    }\n@@ -853,0 +865,5 @@\n+    if (debug_logging_enabled) {\n+      ResourceMark rm(current);\n+      log_debug(class, init)(\"Thread \\\"%s\\\" linking %s\",\n+                             current->name(), external_name());\n+    }\n@@ -855,0 +872,6 @@\n+  } else {\n+    if (debug_logging_enabled) {\n+      ResourceMark rm(current);\n+      log_debug(class, init)(\"Thread \\\"%s\\\" found %s already linked\",\n+                             current->name(), external_name());\n+      }\n@@ -996,0 +1019,29 @@\n+\n+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {\n+      if (fs.is_null_free_inline_type() && fs.access_flags().is_static()) {\n+        Symbol* sig = fs.signature();\n+        oop loader = class_loader();\n+        oop protection_domain = this->protection_domain();\n+        Klass* klass = SystemDictionary::resolve_or_fail(sig,\n+                                                        Handle(THREAD, loader), Handle(THREAD, protection_domain), true,\n+                                                        CHECK_false);\n+        if (klass == nullptr) {\n+          THROW_(vmSymbols::java_lang_LinkageError(), false);\n+        }\n+        if (!klass->is_inline_klass()) {\n+          Exceptions::fthrow(\n+            THREAD_AND_LOCATION,\n+            vmSymbols::java_lang_IncompatibleClassChangeError(),\n+            \"class %s is not an inline type\",\n+            klass->external_name());\n+        }\n+        InstanceKlass* ik = InstanceKlass::cast(klass);\n+        if (!ik->is_implicitly_constructible()) {\n+          Exceptions::fthrow(\n+            THREAD_AND_LOCATION,\n+            vmSymbols::java_lang_IncompatibleClassChangeError(),\n+            \"class %s is not implicitly constructible and it is used in a null restricted static field (not supported)\",\n+            klass->external_name());\n+        }\n+      }\n+    }\n@@ -1207,0 +1259,2 @@\n+  bool debug_logging_enabled = log_is_enabled(Debug, class, init);\n+\n@@ -1210,1 +1264,1 @@\n-    MonitorLocker ml(THREAD, _init_monitor);\n+    MonitorLocker ml(jt, _init_monitor);\n@@ -1214,0 +1268,6 @@\n+      if (debug_logging_enabled) {\n+        ResourceMark rm(jt);\n+        log_debug(class, init)(\"Thread \\\"%s\\\" waiting for initialization of %s by thread \\\"%s\\\"\",\n+                               jt->name(), external_name(), init_thread_name());\n+      }\n+\n@@ -1222,0 +1282,5 @@\n+      if (debug_logging_enabled) {\n+        ResourceMark rm(jt);\n+        log_debug(class, init)(\"Thread \\\"%s\\\" recursively initializing %s\",\n+                               jt->name(), external_name());\n+      }\n@@ -1228,0 +1293,5 @@\n+      if (debug_logging_enabled) {\n+        ResourceMark rm(jt);\n+        log_debug(class, init)(\"Thread \\\"%s\\\" found %s already initialized\",\n+                               jt->name(), external_name());\n+      }\n@@ -1234,0 +1304,5 @@\n+      if (debug_logging_enabled) {\n+        ResourceMark rm(jt);\n+        log_debug(class, init)(\"Thread \\\"%s\\\" found %s is in error state\",\n+                               jt->name(), external_name());\n+      }\n@@ -1240,0 +1315,5 @@\n+      if (debug_logging_enabled) {\n+        ResourceMark rm(jt);\n+        log_debug(class, init)(\"Thread \\\"%s\\\" is initializing %s\",\n+                               jt->name(), external_name());\n+      }\n@@ -1312,1 +1392,1 @@\n-  if (EnablePrimitiveClasses) {\n+  if (EnableValhalla) {\n@@ -1630,1 +1710,1 @@\n-  Klass* ak = array_klass(n, CHECK_NULL);\n+  ArrayKlass* ak = array_klass(n, CHECK_NULL);\n@@ -1694,1 +1774,1 @@\n-Klass* InstanceKlass::array_klass(int n, TRAPS) {\n+ArrayKlass* InstanceKlass::array_klass(int n, TRAPS) {\n@@ -1717,1 +1797,1 @@\n-Klass* InstanceKlass::array_klass_or_null(int n) {\n+ArrayKlass* InstanceKlass::array_klass_or_null(int n) {\n@@ -1727,1 +1807,1 @@\n-Klass* InstanceKlass::array_klass(TRAPS) {\n+ArrayKlass* InstanceKlass::array_klass(TRAPS) {\n@@ -1731,1 +1811,1 @@\n-Klass* InstanceKlass::array_klass_or_null() {\n+ArrayKlass* InstanceKlass::array_klass_or_null() {\n@@ -1773,1 +1853,3 @@\n-    ls.print_cr(\"%s (\" PTR_FORMAT \")\", h_method() == nullptr ? \"(no method)\" : \"\", p2i(this));\n+    ls.print_cr(\"%s (\" PTR_FORMAT \") by thread \\\"%s\\\"\",\n+                h_method() == nullptr ? \"(no method)\" : \"\", p2i(this),\n+                THREAD->name());\n@@ -1992,1 +2074,0 @@\n-    \/\/ _sort_Fn is defined in growableArray.hpp.\n@@ -2691,1 +2772,1 @@\n-      MutexLocker ml(SafepointSynchronize::is_at_safepoint() ? nullptr : mdo->extra_data_lock());\n+      ConditionalMutexLocker ml(mdo->extra_data_lock(), !SafepointSynchronize::is_at_safepoint());\n@@ -2944,1 +3025,2 @@\n-     MutexLocker ml(MultiArray_lock);\n+    MutexLocker ml(MultiArray_lock);\n+    assert(this == ObjArrayKlass::cast(array_klasses())->bottom_klass(), \"sanity\");\n@@ -2947,1 +3029,1 @@\n-    array_klasses()->restore_unshareable_info(ClassLoaderData::the_null_class_loader_data(), Handle(), CHECK);\n+    array_klasses()->restore_unshareable_info(class_loader_data(), Handle(), CHECK);\n@@ -3648,2 +3730,1 @@\n-  MutexLocker ml(CompiledMethod_lock->owned_by_self() ? nullptr : CompiledMethod_lock\n-                 , Mutex::_no_safepoint_check_flag);\n+  ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n@@ -3690,2 +3771,1 @@\n-  MutexLocker ml(CompiledMethod_lock->owned_by_self() ? nullptr : CompiledMethod_lock,\n-                 Mutex::_no_safepoint_check_flag);\n+  ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n@@ -3706,2 +3786,1 @@\n-  MutexLocker ml(CompiledMethod_lock->owned_by_self() ? nullptr : CompiledMethod_lock,\n-                 Mutex::_no_safepoint_check_flag);\n+  ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n@@ -4696,1 +4775,0 @@\n-\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":97,"deletions":19,"binary":false,"changes":116,"status":"modified"},{"patch":"@@ -379,2 +379,2 @@\n-  bool is_declared_atomic() const { return _misc_flags.is_declared_atomic(); }\n-  void set_is_declared_atomic()   { _misc_flags.set_is_declared_atomic(true); }\n+  bool must_be_atomic() const { return _misc_flags.must_be_atomic(); }\n+  void set_must_be_atomic()   { _misc_flags.set_must_be_atomic(true); }\n@@ -388,0 +388,3 @@\n+  bool is_implicitly_constructible() const { return _misc_flags.is_implicitly_constructible(); }\n+  void set_is_implicitly_constructible()   { _misc_flags.set_is_implicitly_constructible(true); }\n+\n@@ -406,0 +409,1 @@\n+  void set_array_klasses(ArrayKlass* k) { _array_klasses = k; }\n@@ -570,0 +574,8 @@\n+\n+  JavaThread* init_thread()  { return Atomic::load(&_init_thread); }\n+  \/\/ We can safely access the name as long as we hold the _init_monitor.\n+  const char* init_thread_name() {\n+    assert(_init_monitor->owned_by_self(), \"Must hold _init_monitor here\");\n+    return init_thread()->name_raw();\n+  }\n+\n@@ -579,1 +591,1 @@\n-  bool is_init_thread(JavaThread *thread)  { return thread == Atomic::load(&_init_thread); }\n+  bool is_init_thread(JavaThread *thread)  { return thread == init_thread(); }\n@@ -1170,2 +1182,2 @@\n-  virtual Klass* array_klass(int n, TRAPS);\n-  virtual Klass* array_klass_or_null(int n);\n+  virtual ArrayKlass* array_klass(int n, TRAPS);\n+  virtual ArrayKlass* array_klass_or_null(int n);\n@@ -1174,2 +1186,2 @@\n-  virtual Klass* array_klass(TRAPS);\n-  virtual Klass* array_klass_or_null();\n+  virtual ArrayKlass* array_klass(TRAPS);\n+  virtual ArrayKlass* array_klass_or_null();\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":19,"deletions":7,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -1194,1 +1194,1 @@\n-  MutexLocker ml(CompiledMethod_lock->owned_by_self() ? nullptr : CompiledMethod_lock, Mutex::_no_safepoint_check_flag);\n+  ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n@@ -1205,1 +1205,1 @@\n-  MutexLocker ml(CompiledMethod_lock->owned_by_self() ? nullptr : CompiledMethod_lock, Mutex::_no_safepoint_check_flag);\n+  ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n@@ -2361,1 +2361,1 @@\n-  \/\/ Search through signature and check if argument is wrapped in T_PRIMITIVE_OBJECT\/T_VOID\n+  \/\/ Search through signature and check if argument is wrapped in T_METADATA\/T_VOID\n@@ -2366,1 +2366,1 @@\n-    if (bt == T_PRIMITIVE_OBJECT) {\n+    if (bt == T_METADATA) {\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -371,1 +371,1 @@\n-  virtual void print_inlining_late(const char* msg) {\n+  virtual void print_inlining_late(InliningResult result, const char* msg) {\n@@ -375,1 +375,1 @@\n-    C->print_inlining(method(), call->jvms()->depth()-1, call->jvms()->bci(), msg);\n+    C->print_inlining(method(), call->jvms()->depth()-1, call->jvms()->bci(), result, msg);\n@@ -525,1 +525,1 @@\n-  virtual void print_inlining_late(const char* msg) {\n+  virtual void print_inlining_late(InliningResult result, const char* msg) {\n@@ -529,1 +529,1 @@\n-    C->print_inlining(method(), call->jvms()->depth()-1, call->jvms()->bci(), msg);\n+    C->print_inlining(method(), call->jvms()->depth()-1, call->jvms()->bci(), result, msg);\n@@ -558,1 +558,1 @@\n-      C->print_inlining(method(), jvms->depth()-1, call_node()->jvms()->bci(),\n+      C->print_inlining(method(), jvms->depth()-1, call_node()->jvms()->bci(), InliningResult::FAILURE,\n@@ -568,1 +568,1 @@\n-      C->print_inlining(method(), jvms->depth()-1, call_node()->jvms()->bci(),\n+      C->print_inlining(method(), jvms->depth()-1, call_node()->jvms()->bci(), InliningResult::FAILURE,\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"utilities\/checkedCast.hpp\"\n@@ -42,0 +43,3 @@\n+  if (_dependency == UnconditionalDependency) {\n+    return this;\n+  }\n@@ -46,4 +50,1 @@\n-  if (_dependency != RegularDependency) {\n-    return this;\n-  }\n-  return phase->type(in(1))->higher_equal_speculative(_type) ? in(1) : this;\n+  return higher_equal_types(phase, in(1)) ? in(1) : this;\n@@ -113,0 +114,3 @@\n+    if (!_type->maybe_null()) {\n+      vt->as_InlineType()->set_is_init(*phase);\n+    }\n@@ -120,0 +124,4 @@\n+uint ConstraintCastNode::hash() const {\n+  return TypeNode::hash() + (int)_dependency + (_extra_types != nullptr ? _extra_types->hash() : 0);\n+}\n+\n@@ -121,1 +129,11 @@\n-  return TypeNode::cmp(n) && ((ConstraintCastNode&)n)._dependency == _dependency;\n+  if (!TypeNode::cmp(n)) {\n+    return false;\n+  }\n+  ConstraintCastNode& cast = (ConstraintCastNode&) n;\n+  if (cast._dependency != _dependency) {\n+    return false;\n+  }\n+  if (_extra_types == nullptr || cast._extra_types == nullptr) {\n+    return _extra_types == cast._extra_types;\n+  }\n+  return _extra_types->eq(cast._extra_types);\n@@ -128,1 +146,2 @@\n-Node* ConstraintCastNode::make_cast(int opcode, Node* c, Node *n, const Type *t, DependencyType dependency) {\n+Node* ConstraintCastNode::make_cast(int opcode, Node* c, Node* n, const Type* t, DependencyType dependency,\n+                                    const TypeTuple* extra_types) {\n@@ -131,1 +150,1 @@\n-    Node* cast = new CastIINode(n, t, dependency);\n+    Node* cast = new CastIINode(n, t, dependency, false, extra_types);\n@@ -136,1 +155,1 @@\n-    Node* cast = new CastLLNode(n, t, dependency);\n+    Node* cast = new CastLLNode(n, t, dependency, extra_types);\n@@ -141,1 +160,1 @@\n-    Node* cast = new CastPPNode(n, t, dependency);\n+    Node* cast = new CastPPNode(n, t, dependency, extra_types);\n@@ -146,1 +165,1 @@\n-    Node* cast = new CastFFNode(n, t, dependency);\n+    Node* cast = new CastFFNode(n, t, dependency, extra_types);\n@@ -151,1 +170,1 @@\n-    Node* cast = new CastDDNode(n, t, dependency);\n+    Node* cast = new CastDDNode(n, t, dependency, extra_types);\n@@ -156,1 +175,1 @@\n-    Node* cast = new CastVVNode(n, t, dependency);\n+    Node* cast = new CastVVNode(n, t, dependency, extra_types);\n@@ -160,1 +179,1 @@\n-  case Op_CheckCastPP: return new CheckCastPPNode(c, n, t, dependency);\n+  case Op_CheckCastPP: return new CheckCastPPNode(c, n, t, dependency, extra_types);\n@@ -170,1 +189,1 @@\n-    return make_cast(Op_CastII, c, n, t, dependency);\n+    return make_cast(Op_CastII, c, n, t, dependency, nullptr);\n@@ -173,1 +192,1 @@\n-    return make_cast(Op_CastLL, c, n, t, dependency);\n+    return make_cast(Op_CastLL, c, n, t, dependency, nullptr);\n@@ -206,1 +225,1 @@\n-        u->bottom_type()->higher_equal(type())) {\n+        higher_equal_types(gvn, u)) {\n@@ -222,0 +241,15 @@\n+bool ConstraintCastNode::higher_equal_types(PhaseGVN* phase, const Node* other) const {\n+  const Type* t = phase->type(other);\n+  if (!t->higher_equal_speculative(type())) {\n+    return false;\n+  }\n+  if (_extra_types != nullptr) {\n+    for (uint i = 0; i < _extra_types->cnt(); ++i) {\n+      if (!t->higher_equal_speculative(_extra_types->field_at(i))) {\n+        return false;\n+      }\n+    }\n+  }\n+  return true;\n+}\n+\n@@ -225,0 +259,4 @@\n+  if (_extra_types != nullptr) {\n+    st->print(\" extra types: \");\n+    _extra_types->dump_on(st);\n+  }\n@@ -579,1 +617,2 @@\n-Node* ConstraintCastNode::make_cast_for_type(Node* c, Node* in, const Type* type, DependencyType dependency) {\n+Node* ConstraintCastNode::make_cast_for_type(Node* c, Node* in, const Type* type, DependencyType dependency,\n+                                             const TypeTuple* types) {\n@@ -582,1 +621,1 @@\n-    cast = make_cast(Op_CastII, c, in, type, dependency);\n+    cast = make_cast(Op_CastII, c, in, type, dependency, types);\n@@ -584,1 +623,1 @@\n-    cast = make_cast(Op_CastLL, c, in, type, dependency);\n+    cast = make_cast(Op_CastLL, c, in, type, dependency, types);\n@@ -586,1 +625,1 @@\n-    cast = make_cast(Op_CastFF, c, in, type, dependency);\n+    cast = make_cast(Op_CastFF, c, in, type, dependency, types);\n@@ -588,1 +627,1 @@\n-    cast = make_cast(Op_CastDD, c, in, type, dependency);\n+    cast = make_cast(Op_CastDD, c, in, type, dependency, types);\n@@ -590,1 +629,1 @@\n-    cast = make_cast(Op_CastVV, c, in, type, dependency);\n+    cast = make_cast(Op_CastVV, c, in, type, dependency, types);\n@@ -592,1 +631,1 @@\n-    cast = make_cast(Op_CastPP, c, in, type, dependency);\n+    cast = make_cast(Op_CastPP, c, in, type, dependency, types);\n","filename":"src\/hotspot\/share\/opto\/castnode.cpp","additions":62,"deletions":23,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -451,1 +451,1 @@\n-Node* PhiNode::try_clean_mem_phi(PhaseGVN *phase) {\n+void RegionNode::try_clean_mem_phis(PhaseIterGVN* igvn) {\n@@ -468,20 +468,12 @@\n-  \/\/ code below replaces the Phi with the MergeMem so that the Region\n-  \/\/ is simplified.\n-\n-  if (type() == Type::MEMORY && is_diamond_phi(true)) {\n-    MergeMemNode* m = nullptr;\n-    assert(req() == 3, \"same as region\");\n-    Node* r = in(0);\n-    for (uint i = 1; i < 3; ++i) {\n-      Node *mem = in(i);\n-      if (mem && mem->is_MergeMem() && r->in(i)->outcnt() == 1) {\n-        \/\/ Nothing is control-dependent on path #i except the region itself.\n-        m = mem->as_MergeMem();\n-        uint j = 3 - i;\n-        Node* other = in(j);\n-        if (other && other == m->base_memory()) {\n-          \/\/ m is a successor memory to other, and is not pinned inside the diamond, so push it out.\n-          \/\/ This will allow the diamond to collapse completely.\n-          return m;\n-        }\n-      }\n+  \/\/ code in PhiNode::try_clean_memory_phi() replaces the Phi with the\n+  \/\/ MergeMem in order to remove the Region if its last phi dies.\n+\n+  if (!is_diamond()) {\n+    return;\n+  }\n+\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    Node* phi = fast_out(i);\n+    if (phi->is_Phi() && phi->as_Phi()->try_clean_memory_phi(igvn)) {\n+      --i;\n+      --imax;\n@@ -490,1 +482,39 @@\n-  return nullptr;\n+}\n+\n+\/\/ Does this region merge a simple diamond formed by a proper IfNode?\n+\/\/\n+\/\/              Cmp\n+\/\/              \/\n+\/\/     ctrl   Bool\n+\/\/       \\    \/\n+\/\/       IfNode\n+\/\/      \/      \\\n+\/\/  IfFalse   IfTrue\n+\/\/      \\      \/\n+\/\/       Region\n+bool RegionNode::is_diamond() const {\n+  if (req() != 3) {\n+    return false;\n+  }\n+\n+  Node* left_path = in(1);\n+  Node* right_path = in(2);\n+  if (left_path == nullptr || right_path == nullptr) {\n+    return false;\n+  }\n+  Node* diamond_if = left_path->in(0);\n+  if (diamond_if == nullptr || !diamond_if->is_If() || diamond_if != right_path->in(0)) {\n+    \/\/ Not an IfNode merging a diamond or TOP.\n+    return false;\n+  }\n+\n+  \/\/ Check for a proper bool\/cmp\n+  const Node* bol = diamond_if->in(1);\n+  if (!bol->is_Bool()) {\n+    return false;\n+  }\n+  const Node* cmp = bol->in(1);\n+  if (!cmp->is_Cmp()) {\n+    return false;\n+  }\n+  return true;\n@@ -504,0 +534,1 @@\n+    try_clean_mem_phis(phase->is_IterGVN());\n@@ -505,10 +536,0 @@\n-    if (has_phis) {\n-      PhiNode* phi = has_unique_phi();\n-      if (phi != nullptr) {\n-        Node* m = phi->try_clean_mem_phi(phase);\n-        if (m != nullptr) {\n-          phase->is_IterGVN()->replace_node(phi, m);\n-          has_phis = false;\n-        }\n-      }\n-    }\n@@ -1346,2 +1367,0 @@\n-\n-\/\/------------------------------is_diamond_phi---------------------------------\n@@ -1350,28 +1369,9 @@\n-\/\/ If check_control_only is true, do not inspect the If node at the\n-\/\/ top, and return -1 (not an edge number) on success.\n-int PhiNode::is_diamond_phi(bool check_control_only) const {\n-  \/\/ Check for a 2-path merge\n-  Node *region = in(0);\n-  if( !region ) return 0;\n-  if( region->req() != 3 ) return 0;\n-  if(         req() != 3 ) return 0;\n-  \/\/ Check that both paths come from the same If\n-  Node *ifp1 = region->in(1);\n-  Node *ifp2 = region->in(2);\n-  if( !ifp1 || !ifp2 ) return 0;\n-  Node *iff = ifp1->in(0);\n-  if( !iff || !iff->is_If() ) return 0;\n-  if( iff != ifp2->in(0) ) return 0;\n-  if (check_control_only)  return -1;\n-  \/\/ Check for a proper bool\/cmp\n-  const Node *b = iff->in(1);\n-  if( !b->is_Bool() ) return 0;\n-  const Node *cmp = b->in(1);\n-  if( !cmp->is_Cmp() ) return 0;\n-\n-  \/\/ Check for branching opposite expected\n-  if( ifp2->Opcode() == Op_IfTrue ) {\n-    assert( ifp1->Opcode() == Op_IfFalse, \"\" );\n-    return 2;\n-  } else {\n-    assert( ifp1->Opcode() == Op_IfTrue, \"\" );\n+int PhiNode::is_diamond_phi() const {\n+  Node* region = in(0);\n+  assert(region != nullptr && region->is_Region(), \"phi must have region\");\n+  if (!region->as_Region()->is_diamond()) {\n+    return 0;\n+  }\n+\n+  if (region->in(1)->is_IfTrue()) {\n+    assert(region->in(2)->is_IfFalse(), \"bad If\");\n@@ -1379,0 +1379,39 @@\n+  } else {\n+    \/\/ Flipped projections.\n+    assert(region->in(2)->is_IfTrue(), \"bad If\");\n+    return 2;\n+  }\n+}\n+\n+\/\/ Do the following transformation if we find the corresponding graph shape, remove the involved memory phi and return\n+\/\/ true. Otherwise, return false if the transformation cannot be applied.\n+\/\/\n+\/\/           If                                     If\n+\/\/          \/  \\                                   \/  \\\n+\/\/    IfFalse  IfTrue  \/- Some Node          IfFalse  IfTrue\n+\/\/          \\  \/      \/    \/                       \\  \/        Some Node\n+\/\/         Region    \/ \/-MergeMem     ===>        Region          |\n+\/\/          \/   \\---Phi                             |          MergeMem\n+\/\/ [other phis]      \\                        [other phis]        |\n+\/\/                   use                                         use\n+bool PhiNode::try_clean_memory_phi(PhaseIterGVN* igvn) {\n+  if (_type != Type::MEMORY) {\n+    return false;\n+  }\n+  assert(is_diamond_phi() > 0, \"sanity\");\n+  assert(req() == 3, \"same as region\");\n+  const Node* region = in(0);\n+  for (uint i = 1; i < 3; i++) {\n+    Node* phi_input = in(i);\n+    if (phi_input != nullptr && phi_input->is_MergeMem() && region->in(i)->outcnt() == 1) {\n+      \/\/ Nothing is control-dependent on path #i except the region itself.\n+      MergeMemNode* merge_mem = phi_input->as_MergeMem();\n+      uint j = 3 - i;\n+      Node* other_phi_input = in(j);\n+      if (other_phi_input != nullptr && other_phi_input == merge_mem->base_memory()) {\n+        \/\/ merge_mem is a successor memory to other_phi_input, and is not pinned inside the diamond, so push it out.\n+        \/\/ This will allow the diamond to collapse completely if there are no other phis left.\n+        igvn->replace_node(this, merge_mem);\n+        return true;\n+      }\n+    }\n@@ -1380,0 +1419,1 @@\n+  return false;\n@@ -1446,8 +1486,0 @@\n-  if (phase->is_IterGVN()) {\n-    Node* m = try_clean_mem_phi(phase);\n-    if (m != nullptr) {\n-      return m;\n-    }\n-  }\n-\n-\n@@ -2000,2 +2032,2 @@\n-InlineTypeNode* PhiNode::push_inline_types_through(PhaseGVN* phase, bool can_reshape, ciInlineKlass* vk, bool is_init) {\n-  InlineTypeNode* vt = InlineTypeNode::make_null(*phase, vk)->clone_with_phis(phase, in(0), is_init);\n+InlineTypeNode* PhiNode::push_inline_types_through(PhaseGVN* phase, bool can_reshape, ciInlineKlass* vk) {\n+  InlineTypeNode* vt = InlineTypeNode::make_null(*phase, vk)->clone_with_phis(phase, in(0), !_type->maybe_null());\n@@ -2027,1 +2059,1 @@\n-      n = phase->transform(n->as_Phi()->push_inline_types_through(phase, can_reshape, vk, is_init));\n+      n = phase->transform(n->as_Phi()->push_inline_types_through(phase, can_reshape, vk));\n@@ -2160,0 +2192,1 @@\n+      const TypeTuple* extra_types = collect_types(phase);\n@@ -2163,1 +2196,2 @@\n-          cast = ConstraintCastNode::make_cast(Op_CastPP, r, uin, phi_type, ConstraintCastNode::StrongDependency);\n+          cast = ConstraintCastNode::make_cast(Op_CastPP, r, uin, phi_type, ConstraintCastNode::StrongDependency,\n+                                               extra_types);\n@@ -2173,1 +2207,2 @@\n-            cast = ConstraintCastNode::make_cast(Op_CastPP, r, uin, TypePtr::NOTNULL, ConstraintCastNode::StrongDependency);\n+            cast = ConstraintCastNode::make_cast(Op_CastPP, r, uin, TypePtr::NOTNULL,\n+                                                 ConstraintCastNode::StrongDependency, extra_types);\n@@ -2185,1 +2220,2 @@\n-            cast = ConstraintCastNode::make_cast(Op_CheckCastPP, r, n, phi_type, ConstraintCastNode::StrongDependency);\n+            cast = ConstraintCastNode::make_cast(Op_CheckCastPP, r, n, phi_type, ConstraintCastNode::StrongDependency,\n+                                                 extra_types);\n@@ -2188,1 +2224,2 @@\n-            cast = ConstraintCastNode::make_cast(Op_CastPP, r, uin, phi_type, ConstraintCastNode::StrongDependency);\n+            cast = ConstraintCastNode::make_cast(Op_CastPP, r, uin, phi_type, ConstraintCastNode::StrongDependency,\n+                                                 extra_types);\n@@ -2192,1 +2229,1 @@\n-        cast = ConstraintCastNode::make_cast_for_type(r, uin, phi_type, ConstraintCastNode::StrongDependency);\n+        cast = ConstraintCastNode::make_cast_for_type(r, uin, phi_type, ConstraintCastNode::StrongDependency, extra_types);\n@@ -2597,2 +2634,0 @@\n-    \/\/ true if all IsInit inputs of all InlineType* nodes are true\n-    bool is_init = true;\n@@ -2650,3 +2685,0 @@\n-          if (phase->find_int_con(n->as_InlineType()->get_is_init(), 0) != 1) {\n-            is_init = false;\n-          }\n@@ -2655,3 +2687,1 @@\n-        } else if (t->is_zero_type()) {\n-          is_init = false;\n-        } else {\n+        } else if (!t->is_zero_type()) {\n@@ -2671,3 +2701,1 @@\n-\/\/ TODO 8302217\n-\/\/      assert(!_type->isa_ptr() || _type->maybe_null() || is_init, \"Phi not null but a possible null was seen\");\n-      return push_inline_types_through(phase, can_reshape, vk, is_init);\n+      return push_inline_types_through(phase, can_reshape, vk);\n@@ -2685,0 +2713,46 @@\n+static int compare_types(const Type* const& e1, const Type* const& e2) {\n+  return (intptr_t)e1 - (intptr_t)e2;\n+}\n+\n+\/\/ Collect types at casts that are going to be eliminated at that Phi and store them in a TypeTuple.\n+\/\/ Sort the types using an arbitrary order so a list of some types always hashes to the same TypeTuple (and TypeTuple\n+\/\/ pointer comparison is enough to tell if 2 list of types are the same or not)\n+const TypeTuple* PhiNode::collect_types(PhaseGVN* phase) const {\n+  const Node* region = in(0);\n+  const Type* phi_type = bottom_type();\n+  ResourceMark rm;\n+  GrowableArray<const Type*> types;\n+  for (uint i = 1; i < req(); i++) {\n+    if (region->in(i) == nullptr || phase->type(region->in(i)) == Type::TOP) {\n+      continue;\n+    }\n+    Node* in = Node::in(i);\n+    const Type* t = phase->type(in);\n+    if (in == nullptr || in == this || t == Type::TOP) {\n+      continue;\n+    }\n+    if (t != phi_type && t->higher_equal_speculative(phi_type)) {\n+      types.insert_sorted<compare_types>(t);\n+    }\n+    while (in != nullptr && in->is_ConstraintCast()) {\n+      Node* next = in->in(1);\n+      if (phase->type(next)->isa_rawptr() && phase->type(in)->isa_oopptr()) {\n+        break;\n+      }\n+      ConstraintCastNode* cast = in->as_ConstraintCast();\n+      for (int j = 0; j < cast->extra_types_count(); ++j) {\n+        const Type* extra_t = cast->extra_type_at(j);\n+        if (extra_t != phi_type && extra_t->higher_equal_speculative(phi_type)) {\n+          types.insert_sorted<compare_types>(extra_t);\n+        }\n+      }\n+      in = next;\n+    }\n+  }\n+  const Type **flds = (const Type **)(phase->C->type_arena()->AmallocWords(types.length()*sizeof(Type*)));\n+  for (int i = 0; i < types.length(); ++i) {\n+    flds[i] = types.at(i);\n+  }\n+  return TypeTuple::make(types.length(), flds);\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":163,"deletions":89,"binary":false,"changes":252,"status":"modified"},{"patch":"@@ -135,0 +135,2 @@\n+  bool is_diamond() const;\n+  void try_clean_mem_phis(PhaseIterGVN* phase);\n@@ -235,1 +237,2 @@\n-  int  is_diamond_phi(bool check_control_only = false) const;\n+  int is_diamond_phi() const;\n+  bool try_clean_memory_phi(PhaseIterGVN* igvn);\n@@ -253,2 +256,1 @@\n-  Node* try_clean_mem_phi(PhaseGVN *phase);\n-  InlineTypeNode* push_inline_types_through(PhaseGVN* phase, bool can_reshape, ciInlineKlass* vk, bool is_init);\n+  InlineTypeNode* push_inline_types_through(PhaseGVN* phase, bool can_reshape, ciInlineKlass* vk);\n@@ -271,0 +273,2 @@\n+\n+  const TypeTuple* collect_types(PhaseGVN* phase) const;\n@@ -342,1 +346,1 @@\n-  Node* search_identical(int dist);\n+  Node* search_identical(int dist, PhaseIterGVN* igvn);\n@@ -443,0 +447,2 @@\n+\n+  bool same_condition(const Node* dom, PhaseIterGVN* igvn) const;\n@@ -460,2 +466,3 @@\n-\/\/ create Runtime Predicates above it. They all share the same uncommon trap. The Parse Predicate will follow the\n-\/\/ Runtime Predicates. Together they form a Regular Predicate Block. There are three kinds of Parse Predicates:\n+\/\/ create Regular Predicates (Runtime Predicates with possible Assertion Predicates) above it. Together they form a\n+\/\/ Predicate Block. The Parse Predicate and Regular Predicates share the same uncommon trap.\n+\/\/ There are three kinds of Parse Predicates:\n@@ -467,0 +474,1 @@\n+  bool _useless; \/\/ If the associated loop dies, this parse predicate becomes useless and can be cleaned up by Value().\n@@ -468,1 +476,1 @@\n-  ParsePredicateNode(Node* control, Node* bol, Deoptimization::DeoptReason deopt_reason);\n+  ParsePredicateNode(Node* control, Deoptimization::DeoptReason deopt_reason, PhaseGVN* gvn);\n@@ -476,0 +484,19 @@\n+  bool is_useless() const {\n+    return _useless;\n+  }\n+\n+  void mark_useless() {\n+    _useless = true;\n+  }\n+\n+  void mark_useful() {\n+    _useless = false;\n+  }\n+\n+  Node* uncommon_trap() const;\n+\n+  Node* Ideal(PhaseGVN* phase, bool can_reshape) {\n+    return nullptr; \/\/ Don't optimize\n+  }\n+\n+  const Type* Value(PhaseGVN* phase) const;\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":34,"deletions":7,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -364,1 +364,2 @@\n-void Compile::remove_useless_nodes(GrowableArray<Node*>& node_list, Unique_Node_List& useful) {\n+template<typename N, ENABLE_IF_SDEFN(std::is_base_of<Node, N>::value)>\n+void Compile::remove_useless_nodes(GrowableArray<N*>& node_list, Unique_Node_List& useful) {\n@@ -366,2 +367,2 @@\n-    Node* n = node_list.at(i);\n-    if (!useful.member(n)) {\n+    N* node = node_list.at(i);\n+    if (!useful.member(node)) {\n@@ -393,0 +394,3 @@\n+  if (dead->is_ParsePredicate()) {\n+    remove_parse_predicate(dead->as_ParsePredicate());\n+  }\n@@ -446,1 +450,1 @@\n-  remove_useless_nodes(_parse_predicate_opaqs, useful); \/\/ remove useless Parse Predicate opaque nodes\n+  remove_useless_nodes(_parse_predicates,   useful); \/\/ remove useless Parse Predicate nodes\n@@ -648,1 +652,1 @@\n-                  _parse_predicate_opaqs (comp_arena(), 8, 0, nullptr),\n+                  _parse_predicates  (comp_arena(), 8, 0, nullptr),\n@@ -1880,3 +1884,3 @@\n-\/\/ Remove the opaque nodes that protect the Parse Predicates so that all unused\n-\/\/ checks and uncommon_traps will be eliminated from the ideal graph.\n-void Compile::cleanup_parse_predicates(PhaseIterGVN& igvn) const {\n+\/\/ Mark all ParsePredicateNodes as useless. They will later be removed from the graph in IGVN together with their\n+\/\/ uncommon traps if no Runtime Predicates were created from the Parse Predicates.\n+void Compile::mark_parse_predicate_nodes_useless(PhaseIterGVN& igvn) {\n@@ -1886,4 +1890,4 @@\n-  for (int i = parse_predicate_count(); i > 0; i--) {\n-    Node* n = parse_predicate_opaque1_node(i - 1);\n-    assert(n->Opcode() == Op_Opaque1, \"must be\");\n-    igvn.replace_node(n, n->in(1));\n+  for (int i = 0; i < parse_predicate_count(); i++) {\n+    ParsePredicateNode* parse_predicate = _parse_predicates.at(i);\n+    parse_predicate->mark_useless();\n+    igvn._worklist.push(parse_predicate);\n@@ -1891,1 +1895,1 @@\n-  assert(parse_predicate_count() == 0, \"should be clean!\");\n+  _parse_predicates.clear();\n@@ -1924,0 +1928,1 @@\n+    assert(C->parse_predicate_count() == 0, \"all parse predicates should have been removed now\");\n@@ -2581,1 +2586,1 @@\n-              cg->print_inlining_late(msg);\n+              cg->print_inlining_late(InliningResult::FAILURE, msg);\n@@ -2815,1 +2820,1 @@\n-        igvn.set_delay_transform(false);\n+        if (failing()) return;\n@@ -2817,0 +2822,1 @@\n+        igvn.set_delay_transform(false);\n@@ -4855,1 +4861,4 @@\n-    _compile(nullptr), _log(nullptr), _phase_name(name), _dolog(CITimeVerbose)\n+    _compile(Compile::current()),\n+    _log(nullptr),\n+    _phase_name(name),\n+    _dolog(CITimeVerbose)\n@@ -4857,0 +4866,1 @@\n+  assert(_compile != nullptr, \"sanity check\");\n@@ -4858,1 +4868,0 @@\n-    _compile = Compile::current();\n@@ -4876,1 +4885,1 @@\n-    Compile::current()->print_missing_nodes();\n+    _compile->print_missing_nodes();\n@@ -5420,1 +5429,10 @@\n-          assert(in_hash, \"node should be in igvn hash table\");\n+#ifdef ASSERT\n+          if (!in_hash) {\n+            tty->print_cr(\"current graph:\");\n+            n->dump_bfs(MaxNodeLimit, nullptr, \"S$\");\n+            tty->cr();\n+            tty->print_cr(\"erroneous node:\");\n+            n->dump();\n+            assert(false, \"node should be in igvn hash table\");\n+          }\n+#endif\n@@ -5661,1 +5679,1 @@\n-bool Compile::should_print_igv(int level) {\n+bool Compile::should_print_igv(const int level) {\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":38,"deletions":20,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -1306,1 +1306,0 @@\n-    case T_PRIMITIVE_OBJECT : \/\/ fall through\n@@ -1618,1 +1617,1 @@\n-  if (((bt == T_OBJECT || bt == T_PRIMITIVE_OBJECT) && C->do_escape_analysis()) || C->eliminate_boxing()) {\n+  if (((bt == T_OBJECT) && C->do_escape_analysis()) || C->eliminate_boxing()) {\n@@ -1835,1 +1834,0 @@\n-  assert(elembt != T_PRIMITIVE_OBJECT, \"inline types are not supported by this method\");\n@@ -2823,1 +2821,2 @@\n-Node* Phase::gen_subtype_check(Node* subklass, Node* superklass, Node** ctrl, Node* mem, PhaseGVN& gvn) {\n+Node* Phase::gen_subtype_check(Node* subklass, Node* superklass, Node** ctrl, Node* mem, PhaseGVN& gvn,\n+                               ciMethod* method, int bci) {\n@@ -2880,1 +2879,3 @@\n-  bool might_be_cache = (gvn.find_int_con(chk_off, cacheoff_con) == cacheoff_con);\n+  const TypeInt* chk_off_t = chk_off->Value(&gvn)->isa_int();\n+  int chk_off_con = (chk_off_t != nullptr && chk_off_t->is_con()) ? chk_off_t->get_con() : cacheoff_con;\n+  bool might_be_cache = (chk_off_con == cacheoff_con);\n@@ -2908,1 +2909,1 @@\n-  if( superklass == nkls )\n+  if (superklass == nkls) {\n@@ -2910,0 +2911,41 @@\n+  }\n+\n+  \/\/ Gather the various success & failures here\n+  RegionNode* r_not_subtype = new RegionNode(3);\n+  gvn.record_for_igvn(r_not_subtype);\n+  RegionNode* r_ok_subtype = new RegionNode(4);\n+  gvn.record_for_igvn(r_ok_subtype);\n+\n+  \/\/ If we might perform an expensive check, first try to take advantage of profile data that was attached to the\n+  \/\/ SubTypeCheck node\n+  if (might_be_cache && method != nullptr && VM_Version::profile_all_receivers_at_type_check()) {\n+    ciCallProfile profile = method->call_profile_at_bci(bci);\n+    float total_prob = 0;\n+    for (int i = 0; profile.has_receiver(i); ++i) {\n+      float prob = profile.receiver_prob(i);\n+      total_prob += prob;\n+    }\n+    if (total_prob * 100. >= TypeProfileSubTypeCheckCommonThreshold) {\n+      const TypeKlassPtr* superk = gvn.type(superklass)->is_klassptr();\n+      for (int i = 0; profile.has_receiver(i); ++i) {\n+        ciKlass* klass = profile.receiver(i);\n+        const TypeKlassPtr* klass_t = TypeKlassPtr::make(klass);\n+        Compile::SubTypeCheckResult result = C->static_subtype_check(superk, klass_t);\n+        if (result != Compile::SSC_always_true && result != Compile::SSC_always_false) {\n+          continue;\n+        }\n+        float prob = profile.receiver_prob(i);\n+        ConNode* klass_node = gvn.makecon(klass_t);\n+        IfNode* iff = gen_subtype_check_compare(*ctrl, subklass, klass_node, BoolTest::eq, prob, gvn, T_ADDRESS);\n+        Node* iftrue = gvn.transform(new IfTrueNode(iff));\n+\n+        if (result == Compile::SSC_always_true) {\n+          r_ok_subtype->add_req(iftrue);\n+        } else {\n+          assert(result == Compile::SSC_always_false, \"\");\n+          r_not_subtype->add_req(iftrue);\n+        }\n+        *ctrl = gvn.transform(new IfFalseNode(iff));\n+      }\n+    }\n+  }\n@@ -2925,0 +2967,5 @@\n+    PhaseIterGVN* igvn = gvn.is_IterGVN();\n+    if (igvn != nullptr) {\n+      igvn->remove_globally_dead_node(r_ok_subtype);\n+      igvn->remove_globally_dead_node(r_not_subtype);\n+    }\n@@ -2928,6 +2975,0 @@\n-  \/\/ Gather the various success & failures here\n-  RegionNode *r_ok_subtype = new RegionNode(4);\n-  gvn.record_for_igvn(r_ok_subtype);\n-  RegionNode *r_not_subtype = new RegionNode(3);\n-  gvn.record_for_igvn(r_not_subtype);\n-\n@@ -3001,1 +3042,2 @@\n-    Node* n = Phase::gen_subtype_check(subklass, superklass, &ctrl, mem, _gvn);\n+\n+    Node* n = Phase::gen_subtype_check(subklass, superklass, &ctrl, mem, _gvn, method(), bci());\n@@ -3006,1 +3048,1 @@\n-  Node* check = _gvn.transform(new SubTypeCheckNode(C, obj_or_subklass, superklass));\n+  Node* check = _gvn.transform(new SubTypeCheckNode(C, obj_or_subklass, superklass, method(), bci()));\n@@ -3114,3 +3156,0 @@\n-    if (java_bc() == Bytecodes::_aastore) {\n-      return ((ciArrayLoadStoreData*)data->as_ArrayLoadStoreData())->element()->ptr_kind() == ProfileNeverNull;\n-    }\n@@ -3580,2 +3619,2 @@\n-  bool not_flat = !UseFlatArray || not_inline || (toop->is_inlinetypeptr() && !toop->inline_klass()->flat_array());\n-  if (EnableValhalla && not_flat) {\n+  bool not_flat_in_array = !UseFlatArray || not_inline || (toop->is_inlinetypeptr() && !toop->inline_klass()->flat_in_array());\n+  if (EnableValhalla && not_flat_in_array) {\n@@ -3602,1 +3641,1 @@\n-      if (ary_t != nullptr) {\n+      if (ary_t != nullptr && !ary_t->is_flat()) {\n@@ -3883,1 +3922,1 @@\n-      can_be_flat = ary_type->can_be_inline_array() && (!elem->is_inlinetypeptr() || elem->inline_klass()->flat_array());\n+      can_be_flat = ary_type->can_be_inline_array() && (!elem->is_inlinetypeptr() || elem->inline_klass()->flat_in_array());\n@@ -4122,1 +4161,4 @@\n-\/\/ See comments on new_instance for the meaning of the other arguments.\n+\/\/ The optional arguments are for specialized use by intrinsics:\n+\/\/  - If 'return_size_val', report the non-padded array size (sum of header size\n+\/\/    and array body) to the caller.\n+\/\/  - deoptimize_on_exception controls how Java exceptions are handled (rethrow vs deoptimize)\n@@ -4173,1 +4215,0 @@\n-  int   header_size_min  = arrayOopDesc::base_offset_in_bytes(T_BYTE);\n@@ -4182,0 +4223,1 @@\n+    int header_size_min = arrayOopDesc::base_offset_in_bytes(T_BYTE);\n@@ -4183,2 +4225,1 @@\n-    header_size_min = hsize;\n-    header_size = intcon(hsize + round_mask);\n+    header_size = intcon(hsize);\n@@ -4188,4 +4229,2 @@\n-    Node* hsize = _gvn.transform( new URShiftINode(layout_val, hss) );\n-    hsize       = _gvn.transform( new AndINode(hsize, hsm) );\n-    Node* mask  = intcon(round_mask);\n-    header_size = _gvn.transform( new AddINode(hsize, mask) );\n+    header_size = _gvn.transform(new URShiftINode(layout_val, hss));\n+    header_size = _gvn.transform(new AndINode(header_size, hsm));\n@@ -4243,4 +4282,4 @@\n-  \/\/ Combine header size (plus rounding) and body size.  Then round down.\n-  \/\/ This computation cannot overflow, because it is used only in two\n-  \/\/ places, one where the length is sharply limited, and the other\n-  \/\/ after a successful allocation.\n+  \/\/ Combine header size and body size for the array copy part, then align (if\n+  \/\/ necessary) for the allocation part. This computation cannot overflow,\n+  \/\/ because it is used only in two places, one where the length is sharply\n+  \/\/ limited, and the other after a successful allocation.\n@@ -4248,6 +4287,2 @@\n-  if (elem_shift != nullptr)\n-    abody     = _gvn.transform( new LShiftXNode(lengthx, elem_shift) );\n-  Node* size  = _gvn.transform( new AddXNode(headerx, abody) );\n-  if (round_mask != 0) {\n-    Node* mask = MakeConX(~round_mask);\n-    size       = _gvn.transform( new AndXNode(size, mask) );\n+  if (elem_shift != nullptr) {\n+    abody = _gvn.transform(new LShiftXNode(lengthx, elem_shift));\n@@ -4255,1 +4290,1 @@\n-  \/\/ else if round_mask == 0, the size computation is self-rounding\n+  Node* non_rounded_size = _gvn.transform(new AddXNode(headerx, abody));\n@@ -4259,1 +4294,1 @@\n-    (*return_size_val) = size;\n+    (*return_size_val) = non_rounded_size;\n@@ -4262,0 +4297,9 @@\n+  Node* size = non_rounded_size;\n+  if (round_mask != 0) {\n+    Node* mask1 = MakeConX(round_mask);\n+    size = _gvn.transform(new AddXNode(size, mask1));\n+    Node* mask2 = MakeConX(~round_mask);\n+    size = _gvn.transform(new AndXNode(size, mask2));\n+  }\n+  \/\/ else if round_mask == 0, the size computation is self-rounding\n+\n@@ -4449,5 +4493,1 @@\n-  Node* cont = _gvn.intcon(1);\n-  Node* opaq = _gvn.transform(new Opaque1Node(C, cont));\n-  C->add_parse_predicate_opaq(opaq);\n-  Node* bol = _gvn.transform(new Conv2BNode(opaq));\n-  ParsePredicateNode* parse_predicate = new ParsePredicateNode(control(), bol, reason);\n+  ParsePredicateNode* parse_predicate = new ParsePredicateNode(control(), reason, &_gvn);\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":86,"deletions":46,"binary":false,"changes":132,"status":"modified"},{"patch":"@@ -819,1 +819,1 @@\n-  Node* oop = (vk->is_empty() && vk->is_initialized()) ? default_oop(gvn, vk) : gvn.zerocon(T_PRIMITIVE_OBJECT);\n+  Node* oop = (vk->is_empty() && vk->is_initialized()) ? default_oop(gvn, vk) : gvn.zerocon(T_OBJECT);\n@@ -852,1 +852,1 @@\n-  Node* oop = vk->is_initialized() ? default_oop(gvn, vk) : gvn.zerocon(T_PRIMITIVE_OBJECT);\n+  Node* oop = vk->is_initialized() ? default_oop(gvn, vk) : gvn.zerocon(T_OBJECT);\n","filename":"src\/hotspot\/share\/opto\/inlinetypenode.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n@@ -121,1 +122,1 @@\n-    CompileTask::print_inlining_ul(callee, jvms->depth() - 1, bci, inline_msg);\n+    CompileTask::print_inlining_ul(callee, jvms->depth() - 1, bci, InliningResult::SUCCESS, inline_msg);\n@@ -123,1 +124,1 @@\n-      C->print_inlining(callee, jvms->depth() - 1, bci, inline_msg);\n+      C->print_inlining(callee, jvms->depth() - 1, bci, InliningResult::SUCCESS, inline_msg);\n@@ -149,1 +150,1 @@\n-    CompileTask::print_inlining_ul(callee, jvms->depth() - 1, bci, msg);\n+    CompileTask::print_inlining_ul(callee, jvms->depth() - 1, bci, InliningResult::FAILURE, msg);\n@@ -151,1 +152,1 @@\n-      C->print_inlining(callee, jvms->depth() - 1, bci, msg);\n+      C->print_inlining(callee, jvms->depth() - 1, bci, InliningResult::FAILURE, msg);\n@@ -192,1 +193,1 @@\n-    CompileTask::print_inlining_ul(callee, jvms->depth() - 1, bci, inline_msg);\n+    CompileTask::print_inlining_ul(callee, jvms->depth() - 1, bci, InliningResult::SUCCESS, inline_msg);\n@@ -194,1 +195,1 @@\n-      C->print_inlining(callee, jvms->depth() - 1, bci, inline_msg);\n+      C->print_inlining(callee, jvms->depth() - 1, bci, InliningResult::SUCCESS, inline_msg);\n@@ -210,1 +211,1 @@\n-    CompileTask::print_inlining_ul(kit.callee(), jvms->depth() - 1, bci, msg);\n+    CompileTask::print_inlining_ul(kit.callee(), jvms->depth() - 1, bci, InliningResult::FAILURE, msg);\n@@ -212,1 +213,1 @@\n-      C->print_inlining(kit.callee(), jvms->depth() - 1, bci, msg);\n+      C->print_inlining(kit.callee(), jvms->depth() - 1, bci, InliningResult::FAILURE, msg);\n@@ -336,1 +337,1 @@\n-  case vmIntrinsics::_getValue:                 return inline_unsafe_access(!is_store, T_PRIMITIVE_OBJECT,Relaxed, false);\n+  case vmIntrinsics::_getValue:                 return inline_unsafe_access(!is_store, T_OBJECT,   Relaxed, false, true);\n@@ -347,1 +348,1 @@\n-  case vmIntrinsics::_putValue:                 return inline_unsafe_access( is_store, T_PRIMITIVE_OBJECT,Relaxed, false);\n+  case vmIntrinsics::_putValue:                 return inline_unsafe_access( is_store, T_OBJECT,   Relaxed, false, true);\n@@ -511,0 +512,1 @@\n+  case vmIntrinsics::_isFlattenedArray:         return inline_unsafe_isFlattenedArray();\n@@ -2034,1 +2036,1 @@\n-    } else if (type == T_OBJECT || type == T_PRIMITIVE_OBJECT) {\n+    } else if (type == T_OBJECT) {\n@@ -2274,1 +2276,1 @@\n-bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned) {\n+bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned, const bool is_flat) {\n@@ -2298,2 +2300,2 @@\n-      assert(rtype == type || (rtype == T_OBJECT && type == T_PRIMITIVE_OBJECT), \"getter must return the expected value\");\n-      assert(sig->count() == 2 || (type == T_PRIMITIVE_OBJECT && sig->count() == 3), \"oop getter has 2 or 3 arguments\");\n+      assert(rtype == type, \"getter must return the expected value\");\n+      assert(sig->count() == 2 || (is_flat && sig->count() == 3), \"oop getter has 2 or 3 arguments\");\n@@ -2305,1 +2307,1 @@\n-      assert(sig->count() == 3 || (type == T_PRIMITIVE_OBJECT && sig->count() == 4), \"oop putter has 3 arguments\");\n+      assert(sig->count() == 3 || (is_flat && sig->count() == 4), \"oop putter has 3 arguments\");\n@@ -2309,1 +2311,1 @@\n-      assert(vtype == type || (type == T_PRIMITIVE_OBJECT && vtype == T_OBJECT), \"putter must accept the expected value\");\n+      assert(vtype == type, \"putter must accept the expected value\");\n@@ -2344,1 +2346,1 @@\n-  if (type == T_PRIMITIVE_OBJECT) {\n+  if (is_flat) {\n@@ -2379,1 +2381,1 @@\n-          if (bt == T_ARRAY || bt == T_NARROWOOP || (bt == T_PRIMITIVE_OBJECT && !field->is_flat())) {\n+          if (bt == T_ARRAY || bt == T_NARROWOOP) {\n@@ -2382,1 +2384,1 @@\n-          if (bt == type && (bt != T_PRIMITIVE_OBJECT || field->type() == inline_klass)) {\n+          if (bt == type && (!field->is_flat() || field->type() == inline_klass)) {\n@@ -2430,1 +2432,1 @@\n-  Node* val = is_store ? argument(4 + (type == T_PRIMITIVE_OBJECT ? 1 : 0)) : nullptr;\n+  Node* val = is_store ? argument(4 + (is_flat ? 1 : 0)) : nullptr;\n@@ -2468,4 +2470,1 @@\n-    assert(bt == alias_type->basic_type() || bt == T_PRIMITIVE_OBJECT, \"should match\");\n-    if (field != nullptr && bt == T_PRIMITIVE_OBJECT && !field->is_flat()) {\n-      bt = T_OBJECT;\n-    }\n+    assert(bt == alias_type->basic_type() || is_flat, \"should match\");\n@@ -2478,3 +2477,0 @@\n-    if (adr_type->is_flat()) {\n-      bt = T_PRIMITIVE_OBJECT;\n-    }\n@@ -2486,1 +2482,1 @@\n-    if (bt != T_PRIMITIVE_OBJECT && is_reference_type(bt, true)) {\n+    if (is_reference_type(bt, true)) {\n@@ -2501,1 +2497,1 @@\n-  if (type == T_PRIMITIVE_OBJECT) {\n+  if (is_flat) {\n@@ -2525,1 +2521,1 @@\n-  assert(!mismatched || type == T_PRIMITIVE_OBJECT || alias_type->adr_type()->is_oopptr(), \"off-heap access can't be mismatched\");\n+  assert(!mismatched || is_flat || alias_type->adr_type()->is_oopptr(), \"off-heap access can't be mismatched\");\n@@ -2538,1 +2534,1 @@\n-    if (type == T_OBJECT) {\n+    if (type == T_OBJECT && !is_flat) {\n@@ -2543,2 +2539,0 @@\n-    } else if (type == T_PRIMITIVE_OBJECT) {\n-      value_type = nullptr;\n@@ -2567,1 +2561,1 @@\n-      if (type == T_PRIMITIVE_OBJECT) {\n+      if (is_flat) {\n@@ -2620,1 +2614,1 @@\n-    if (type == T_PRIMITIVE_OBJECT) {\n+    if (is_flat) {\n@@ -3034,0 +3028,7 @@\n+\n+#if INCLUDE_JVMTI\n+  if (too_many_traps(Deoptimization::Reason_intrinsic)) {\n+    return false;\n+  }\n+#endif \/\/INCLUDE_JVMTI\n+\n@@ -3044,0 +3045,18 @@\n+#if INCLUDE_JVMTI\n+    \/\/ Don't try to access new allocated obj in the intrinsic.\n+    \/\/ It causes perfomance issues even when jvmti event VmObjectAlloc is disabled.\n+    \/\/ Deoptimize and allocate in interpreter instead.\n+    Node* addr = makecon(TypeRawPtr::make((address) &JvmtiExport::_should_notify_object_alloc));\n+    Node* should_post_vm_object_alloc = make_load(this->control(), addr, TypeInt::INT, T_INT, MemNode::unordered);\n+    Node* chk = _gvn.transform(new CmpINode(should_post_vm_object_alloc, intcon(0)));\n+    Node* tst = _gvn.transform(new BoolNode(chk, BoolTest::eq));\n+    {\n+      BuildCutout unless(this, tst, PROB_MAX);\n+      uncommon_trap(Deoptimization::Reason_intrinsic,\n+                    Deoptimization::Action_make_not_entrant);\n+    }\n+    if (stopped()) {\n+      return true;\n+    }\n+#endif \/\/INCLUDE_JVMTI\n+\n@@ -3093,0 +3112,1 @@\n+  Node* vt_oop = _gvn.transform(must_be_not_null(argument(0), true)); \/\/ VirtualThread this argument\n@@ -3101,0 +3121,1 @@\n+    sync_kit(ideal);\n@@ -3103,3 +3124,0 @@\n-    Node* vt_oop = _gvn.transform(must_be_not_null(argument(0), true)); \/\/ VirtualThread this argument\n-\n-    sync_kit(ideal);\n@@ -3110,1 +3128,0 @@\n-    Node* vt_oop = _gvn.transform(argument(0)); \/\/ this argument - VirtualThread oop\n@@ -3815,3 +3832,4 @@\n-Node* LibraryCallKit::scopedValueCache_helper() {\n-  ciKlass *objects_klass = ciObjArrayKlass::make(env()->Object_klass());\n-  const TypeOopPtr *etype = TypeOopPtr::make_from_klass(env()->Object_klass());\n+const Type* LibraryCallKit::scopedValueCache_type() {\n+  ciKlass* objects_klass = ciObjArrayKlass::make(env()->Object_klass());\n+  const TypeOopPtr* etype = TypeOopPtr::make_from_klass(env()->Object_klass());\n+  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3819,0 +3837,2 @@\n+  \/\/ Because we create the scopedValue cache lazily we have to make the\n+  \/\/ type of the result BotPTR.\n@@ -3820,0 +3840,3 @@\n+  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, TypeAryPtr::Offset(0));\n+  return objects_type;\n+}\n@@ -3821,0 +3844,1 @@\n+Node* LibraryCallKit::scopedValueCache_helper() {\n@@ -3833,8 +3857,1 @@\n-  ciKlass *objects_klass = ciObjArrayKlass::make(env()->Object_klass());\n-  const TypeOopPtr *etype = TypeOopPtr::make_from_klass(env()->Object_klass());\n-  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n-\n-  \/\/ Because we create the scopedValue cache lazily we have to make the\n-  \/\/ type of the result BotPTR.\n-  bool xk = etype->klass_is_exact();\n-  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, TypeAryPtr::Offset(0));\n+  const Type* objects_type = scopedValueCache_type();\n@@ -3851,0 +3868,1 @@\n+  const Type* objects_type = scopedValueCache_type();\n@@ -3853,1 +3871,1 @@\n-  access_store_at(nullptr, cache_obj_handle, adr_type, arr, _gvn.type(arr), T_OBJECT, IN_NATIVE | MO_UNORDERED);\n+  access_store_at(nullptr, cache_obj_handle, adr_type, arr, objects_type, T_OBJECT, IN_NATIVE | MO_UNORDERED);\n@@ -5249,0 +5267,14 @@\n+\/\/----------------------inline_unsafe_isFlattenedArray-------------------\n+\/\/ public native boolean Unsafe.isFlattenedArray(Class<?> arrayClass);\n+\/\/ This intrinsic exploits assumptions made by the native implementation\n+\/\/ (arrayClass is neither null nor primitive) to avoid unnecessary null checks.\n+bool LibraryCallKit::inline_unsafe_isFlattenedArray() {\n+  Node* cls = argument(1);\n+  Node* p = basic_plus_adr(cls, java_lang_Class::klass_offset());\n+  Node* kls = _gvn.transform(LoadKlassNode::make(_gvn, nullptr, immutable_memory(), p,\n+                                                 TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT));\n+  Node* result = flat_array_test(kls);\n+  set_result(result);\n+  return true;\n+}\n+\n@@ -5378,2 +5410,2 @@\n-        Node* obj_size  = nullptr;\n-        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &obj_size, \/*deoptimize_on_exception=*\/true);\n+        Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n+        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n@@ -5412,1 +5444,1 @@\n-          copy_to_clone(obj, alloc_obj, obj_size, true);\n+          copy_to_clone(obj, alloc_obj, array_size, true);\n@@ -5449,1 +5481,1 @@\n-      Node* obj_size  = nullptr;\n+      Node* obj_size = nullptr; \/\/ Total object size, including object alignment padding.\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":87,"deletions":55,"binary":false,"changes":142,"status":"modified"},{"patch":"@@ -249,1 +249,1 @@\n-  bool inline_unsafe_access(bool is_store, BasicType type, AccessKind kind, bool is_unaligned);\n+  bool inline_unsafe_access(bool is_store, BasicType type, AccessKind kind, bool is_unaligned, bool is_flat = false);\n@@ -256,0 +256,1 @@\n+  bool inline_unsafe_isFlattenedArray();\n@@ -264,0 +265,1 @@\n+  const Type* scopedValueCache_type();\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -555,1 +555,1 @@\n-      if (sfpt_ctl->is_Proj() && sfpt_ctl->as_Proj()->is_uncommon_trap_proj(Deoptimization::Reason_none)) {\n+      if (sfpt_ctl->is_Proj() && sfpt_ctl->as_Proj()->is_uncommon_trap_proj()) {\n@@ -895,1 +895,1 @@\n-      assert(vk->flat_array(), \"must be flat\");\n+      assert(vk->flat_in_array(), \"must be flat in array\");\n@@ -1082,1 +1082,1 @@\n-        use->as_InlineType()->set_oop(_igvn.zerocon(T_PRIMITIVE_OBJECT));\n+        use->as_InlineType()->set_oop(_igvn.zerocon(T_OBJECT));\n@@ -2731,1 +2731,1 @@\n-    Node* not_subtype_ctrl = Phase::gen_subtype_check(subklass, superklass, &ctrl, nullptr, _igvn);\n+    Node* not_subtype_ctrl = Phase::gen_subtype_check(subklass, superklass, &ctrl, nullptr, _igvn, check->method(), check->bci());\n@@ -2825,1 +2825,1 @@\n-        assert(t->isa_aryklassptr(), \"Unexpected input type\");\n+        assert(t->isa_klassptr(), \"Unexpected input type\");\n@@ -2835,0 +2835,4 @@\n+    Node* m2b = transform_later(new Conv2BNode(masked));\n+    \/\/ The matcher expects the input to If nodes to be produced by a Bool(CmpI..)\n+    \/\/ pattern, but the input to other potential users (e.g. Phi) to be some\n+    \/\/ other pattern (e.g. a Conv2B node, possibly idealized as a CMoveI).\n@@ -2836,1 +2840,9 @@\n-    _igvn.replace_node(old_bol, bol);\n+    for (DUIterator_Last imin, i = old_bol->last_outs(imin); i >= imin; --i) {\n+      Node* user = old_bol->last_out(i);\n+      for (uint j = 0; j < user->req(); j++) {\n+        Node* n = user->in(j);\n+        if (n == old_bol) {\n+          _igvn.replace_input_of(user, j, user->is_If() ? bol : m2b);\n+        }\n+      }\n+    }\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":18,"deletions":6,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -933,1 +933,0 @@\n-  case T_PRIMITIVE_OBJECT:\n@@ -1168,1 +1167,0 @@\n-      assert(memory_type() != T_PRIMITIVE_OBJECT, \"should not be used for inline types\");\n@@ -2705,1 +2703,0 @@\n-  case T_PRIMITIVE_OBJECT:\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -135,0 +135,2 @@\n+class NegNode;\n+class NegVNode;\n@@ -734,1 +736,1 @@\n-        DEFINE_CLASS_ID(Reduction, Vector, 7)\n+        DEFINE_CLASS_ID(Reduction, Vector, 9)\n@@ -736,0 +738,1 @@\n+        DEFINE_CLASS_ID(NegV, Vector, 8)\n@@ -794,0 +797,1 @@\n+    DEFINE_CLASS_ID(Neg,      Node, 19)\n@@ -795,1 +799,1 @@\n-    _max_classes  = ClassMask_LShift\n+    _max_classes  = ClassMask_Neg\n@@ -959,0 +963,2 @@\n+  DEFINE_CLASS_QUERY(Neg)\n+  DEFINE_CLASS_QUERY(NegV)\n@@ -1233,0 +1239,3 @@\n+  \/\/ Whether this is a memory phi node\n+  bool is_memory_phi() const { return is_Phi() && bottom_type() == Type::MEMORY; }\n+\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -272,1 +272,1 @@\n-  C->print_method(CompilerPhaseType::PHASE_MACH_ANALYSIS, 4);\n+  C->print_method(CompilerPhaseType::PHASE_MACH_ANALYSIS, 3);\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -128,1 +128,0 @@\n-  case T_PRIMITIVE_OBJECT:\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -367,1 +367,1 @@\n-  new_vt->set_oop(gvn().zerocon(T_PRIMITIVE_OBJECT));\n+  new_vt->set_oop(gvn().zerocon(T_OBJECT));\n","filename":"src\/hotspot\/share\/opto\/parseHelper.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1884,0 +1884,15 @@\n+Node* FmaVNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  \/\/ We canonicalize the node by converting \"(-a)*b+c\" into \"b*(-a)+c\"\n+  \/\/ This reduces the number of rules in the matcher, as we only need to check\n+  \/\/ for negations on the second argument, and not the symmetric case where\n+  \/\/ the first argument is negated.\n+  \/\/ We cannot do this if the FmaV is masked, since the inactive lanes have to return\n+  \/\/ the first input (i.e. \"-a\"). If we were to swap the inputs, the inactive lanes would\n+  \/\/ incorrectly return \"b\".\n+  if (!is_predicated_vector() && in(1)->is_NegV() && !in(2)->is_NegV()) {\n+    swap_edges(1, 2);\n+    return this;\n+  }\n+  return nullptr;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/vectornode.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -380,0 +380,10 @@\n+\/\/------------------------------FmaVNode--------------------------------------\n+\/\/ Vector fused-multiply-add\n+class FmaVNode : public VectorNode {\n+public:\n+  FmaVNode(Node* in1, Node* in2, Node* in3, const TypeVect* vt) : VectorNode(in1, in2, in3, vt) {\n+    assert(UseFMA, \"Needs FMA instructions support.\");\n+  }\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+};\n+\n@@ -381,2 +391,2 @@\n-\/\/ Vector multiply double\n-class FmaVDNode : public VectorNode {\n+\/\/ Vector fused-multiply-add double\n+class FmaVDNode : public FmaVNode {\n@@ -384,1 +394,1 @@\n-  FmaVDNode(Node* in1, Node* in2, Node* in3, const TypeVect* vt) : VectorNode(in1, in2, in3, vt) {}\n+  FmaVDNode(Node* in1, Node* in2, Node* in3, const TypeVect* vt) : FmaVNode(in1, in2, in3, vt) {}\n@@ -389,2 +399,2 @@\n-\/\/ Vector multiply float\n-class FmaVFNode : public VectorNode {\n+\/\/ Vector fused-multiply-add float\n+class FmaVFNode : public FmaVNode {\n@@ -392,1 +402,1 @@\n-  FmaVFNode(Node* in1, Node* in2, Node* in3, const TypeVect* vt) : VectorNode(in1, in2, in3, vt) {}\n+  FmaVFNode(Node* in1, Node* in2, Node* in3, const TypeVect* vt) : FmaVNode(in1, in2, in3, vt) {}\n@@ -512,1 +522,3 @@\n-  NegVNode(Node* in, const TypeVect* vt) : VectorNode(in, vt) {}\n+  NegVNode(Node* in, const TypeVect* vt) : VectorNode(in, vt) {\n+    init_class_id(Class_NegV);\n+  }\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":19,"deletions":7,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -63,0 +63,1 @@\n+#include \"utilities\/checkedCast.hpp\"\n@@ -1181,1 +1182,1 @@\n-  char quote_c        = 0;\n+  int  quote_c        = 0;\n@@ -1193,1 +1194,1 @@\n-          token[pos++] = c;\n+          token[pos++] = checked_cast<char>(c);\n@@ -1213,1 +1214,1 @@\n-        token[pos++] = c;\n+        token[pos++] = checked_cast<char>(c);\n@@ -1569,1 +1570,1 @@\n-    MaxRAMPercentage = 100.0 \/ MaxRAMFraction;\n+    MaxRAMPercentage = 100.0 \/ (double)MaxRAMFraction;\n@@ -1573,1 +1574,1 @@\n-    MinRAMPercentage = 100.0 \/ MinRAMFraction;\n+    MinRAMPercentage = 100.0 \/ (double)MinRAMFraction;\n@@ -1577,1 +1578,1 @@\n-    InitialRAMPercentage = 100.0 \/ InitialRAMFraction;\n+    InitialRAMPercentage = 100.0 \/ (double)InitialRAMFraction;\n@@ -1583,2 +1584,2 @@\n-    julong reasonable_max = (julong)((phys_mem * MaxRAMPercentage) \/ 100);\n-    const julong reasonable_min = (julong)((phys_mem * MinRAMPercentage) \/ 100);\n+    julong reasonable_max = (julong)(((double)phys_mem * MaxRAMPercentage) \/ 100);\n+    const julong reasonable_min = (julong)(((double)phys_mem * MinRAMPercentage) \/ 100);\n@@ -1668,1 +1669,1 @@\n-      julong reasonable_initial = (julong)((phys_mem * InitialRAMPercentage) \/ 100);\n+      julong reasonable_initial = (julong)(((double)phys_mem * InitialRAMPercentage) \/ 100);\n@@ -1915,2 +1916,5 @@\n-\n-#if !defined(X86) && !defined(AARCH64) && !defined(RISCV64) && !defined(ARM) && !defined(PPC64)\n+  \/\/ Valhalla missing LM_LIGHTWEIGHT support just now\n+  if (EnableValhalla && LockingMode != LM_LEGACY) {\n+    FLAG_SET_CMDLINE(LockingMode, LM_LEGACY);\n+  }\n+#if !defined(X86) && !defined(AARCH64) && !defined(RISCV64) && !defined(ARM) && !defined(PPC64) && !defined(S390)\n@@ -1923,9 +1927,0 @@\n-  if (UseHeavyMonitors) {\n-    if (FLAG_IS_CMDLINE(LockingMode) && LockingMode != LM_MONITOR) {\n-      jio_fprintf(defaultStream::error_stream(),\n-                  \"Conflicting -XX:+UseHeavyMonitors and -XX:LockingMode=%d flags\\n\", LockingMode);\n-      return false;\n-    }\n-    FLAG_SET_CMDLINE(LockingMode, LM_MONITOR);\n-  }\n-\n@@ -1978,4 +1973,4 @@\n-bool Arguments::parse_uintx(const char* value,\n-                            uintx* uintx_arg,\n-                            uintx min_size) {\n-  uintx n;\n+bool Arguments::parse_uint(const char* value,\n+                           uint* uint_arg,\n+                           uint min_size) {\n+  uint n;\n@@ -1986,1 +1981,1 @@\n-    *uintx_arg = n;\n+    *uint_arg = n;\n@@ -2807,2 +2802,2 @@\n-      uintx max_tenuring_thresh = 0;\n-      if (!parse_uintx(tail, &max_tenuring_thresh, 0)) {\n+      uint max_tenuring_thresh = 0;\n+      if (!parse_uint(tail, &max_tenuring_thresh, 0)) {\n@@ -4149,1 +4144,1 @@\n-#ifdef COMPILER2\n+#if COMPILER2_OR_JVMCI\n@@ -4170,1 +4165,1 @@\n-#endif \/\/ COMPILER2\n+#endif \/\/ COMPILER2_OR_JVMCI\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":24,"deletions":29,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -384,4 +384,4 @@\n-  \/\/ parameter passed and returns the value in uintx_arg.  Returns\n-  \/\/ false otherwise, with uintx_arg undefined.\n-  static bool parse_uintx(const char* value, uintx* uintx_arg,\n-                          uintx min_size);\n+  \/\/ parameter passed and returns the value in uint_arg.  Returns\n+  \/\/ false otherwise, with uint_arg undefined.\n+  static bool parse_uint(const char* value, uint* uintx_arg,\n+                         uint min_size);\n","filename":"src\/hotspot\/share\/runtime\/arguments.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -95,0 +95,1 @@\n+#include \"utilities\/checkedCast.hpp\"\n@@ -122,2 +123,1 @@\n-  MutexLocker ml(CompiledMethod_lock->owned_by_self() ? nullptr : CompiledMethod_lock,\n-                 Mutex::_no_safepoint_check_flag);\n+  ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n@@ -144,2 +144,2 @@\n-  MutexLocker ml(CompiledMethod_lock->owned_by_self() ? nullptr : CompiledMethod_lock,\n-                 Mutex::_no_safepoint_check_flag);\n+  ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n+\n@@ -175,2 +175,2 @@\n-      MutexLocker ml(CompiledMethod_lock->owned_by_self() ? nullptr : CompiledMethod_lock,\n-                 Mutex::_no_safepoint_check_flag);\n+      ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n+\n@@ -203,2 +203,2 @@\n-        MutexLocker ml(CompiledMethod_lock->owned_by_self() ? nullptr : CompiledMethod_lock,\n-                       Mutex::_no_safepoint_check_flag);\n+        ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n+\n@@ -254,1 +254,1 @@\n-  int result = _caller_adjustment;\n+  intptr_t result = _caller_adjustment;\n@@ -258,1 +258,1 @@\n-  return result;\n+  return checked_cast<int>(result);\n@@ -1115,1 +1115,1 @@\n-      _high = _low + cache->length() - 1;\n+      _high = checked_cast<PrimitiveType>(_low + cache->length() - 1);\n@@ -1134,1 +1134,1 @@\n-      int offset = value - _low;\n+      int offset = checked_cast<int>(value - _low);\n@@ -1220,6 +1220,6 @@\n-       case T_INT:     return IntegerBoxCache::singleton(THREAD)->lookup_raw(value->get_int(), cache_init_error);\n-       case T_CHAR:    return CharacterBoxCache::singleton(THREAD)->lookup_raw(value->get_int(), cache_init_error);\n-       case T_SHORT:   return ShortBoxCache::singleton(THREAD)->lookup_raw(value->get_int(), cache_init_error);\n-       case T_BYTE:    return ByteBoxCache::singleton(THREAD)->lookup_raw(value->get_int(), cache_init_error);\n-       case T_BOOLEAN: return BooleanBoxCache::singleton(THREAD)->lookup_raw(value->get_int(), cache_init_error);\n-       case T_LONG:    return LongBoxCache::singleton(THREAD)->lookup_raw(value->get_int(), cache_init_error);\n+       case T_INT:     return IntegerBoxCache::singleton(THREAD)->lookup_raw(value->get_intptr(), cache_init_error);\n+       case T_CHAR:    return CharacterBoxCache::singleton(THREAD)->lookup_raw(value->get_intptr(), cache_init_error);\n+       case T_SHORT:   return ShortBoxCache::singleton(THREAD)->lookup_raw(value->get_intptr(), cache_init_error);\n+       case T_BYTE:    return ByteBoxCache::singleton(THREAD)->lookup_raw(value->get_intptr(), cache_init_error);\n+       case T_BOOLEAN: return BooleanBoxCache::singleton(THREAD)->lookup_raw(value->get_intptr(), cache_init_error);\n+       case T_LONG:    return LongBoxCache::singleton(THREAD)->lookup_raw(value->get_intptr(), cache_init_error);\n@@ -1251,2 +1251,1 @@\n-      intptr_t init_value = StackValue::create_stack_value(fr, reg_map, sv->is_init())->get_int();\n-      jint is_init = (jint)*((jint*)&init_value);\n+      jint is_init = StackValue::create_stack_value(fr, reg_map, sv->is_init())->get_jint();\n@@ -1300,1 +1299,1 @@\n-      intptr_t is_larval = StackValue::create_stack_value(fr, reg_map, sv->is_larval())->get_int();\n+      intptr_t is_larval = StackValue::create_stack_value(fr, reg_map, sv->is_larval())->get_jint();\n@@ -1376,1 +1375,1 @@\n-static void byte_array_put(typeArrayOop obj, intptr_t val, int index, int byte_count) {\n+static void byte_array_put(typeArrayOop obj, StackValue* value, int index, int byte_count) {\n@@ -1379,1 +1378,1 @@\n-      obj->byte_at_put(index, (jbyte) *((jint *) &val));\n+      obj->byte_at_put(index, (jbyte) value->get_jint());\n@@ -1382,1 +1381,1 @@\n-      *((jshort *) check_alignment_get_addr(obj, index, 2)) = (jshort) *((jint *) &val);\n+      *((jshort *) check_alignment_get_addr(obj, index, 2)) = (jshort) value->get_jint();\n@@ -1385,1 +1384,1 @@\n-      *((jint *) check_alignment_get_addr(obj, index, 4)) = (jint) *((jint *) &val);\n+      *((jint *) check_alignment_get_addr(obj, index, 4)) = value->get_jint();\n@@ -1388,1 +1387,1 @@\n-      *((jlong *) check_alignment_get_addr(obj, index, 8)) = (jlong) *((jlong *) &val);\n+      *((jlong *) check_alignment_get_addr(obj, index, 8)) = (jlong) value->get_intptr();\n@@ -1400,1 +1399,0 @@\n-  intptr_t val;\n@@ -1410,1 +1408,1 @@\n-      jlong res = (jlong)low->get_int();\n+      jlong res = (jlong)low->get_intptr();\n@@ -1412,1 +1410,1 @@\n-      jlong res = jlong_from((jint)value->get_int(), (jint)low->get_int());\n+      jlong res = jlong_from(value->get_jint(), low->get_jint());\n@@ -1418,1 +1416,0 @@\n-    \/\/ Have to cast to INT (32 bits) pointer to avoid little\/big-endian problem.\n@@ -1439,1 +1436,1 @@\n-        jlong res = (jlong)low->get_int();\n+        jlong res = (jlong)low->get_intptr();\n@@ -1441,1 +1438,1 @@\n-        jlong res = jlong_from((jint)value->get_int(), (jint)low->get_int());\n+        jlong res = jlong_from(value->get_jint(), low->get_jint());\n@@ -1443,2 +1440,2 @@\n-        obj->int_at_put(index, (jint)*((jint*)&res));\n-        obj->int_at_put(++index, (jint)*(((jint*)&res) + 1));\n+        obj->int_at_put(index, *(jint*)&res);\n+        obj->int_at_put(++index, *((jint*)&res + 1));\n@@ -1446,2 +1443,1 @@\n-        val = value->get_int();\n-        obj->int_at_put(index, (jint)*((jint*)&val));\n+        obj->int_at_put(index, value->get_jint());\n@@ -1454,2 +1450,1 @@\n-      val = value->get_int();\n-      obj->short_at_put(index, (jshort)*((jint*)&val));\n+      obj->short_at_put(index, (jshort)value->get_jint());\n@@ -1460,2 +1455,1 @@\n-      val = value->get_int();\n-      obj->char_at_put(index, (jchar)*((jint*)&val));\n+      obj->char_at_put(index, (jchar)value->get_jint());\n@@ -1466,2 +1460,1 @@\n-      \/\/ The value we get is erased as a regular int. We will need to find its actual byte count 'by hand'.\n-      val = value->get_int();\n+      \/\/ The value we get is erased as a regular int. We will need to find its actual byte count 'by hand'.\n@@ -1470,1 +1463,1 @@\n-      byte_array_put(obj, val, index, byte_count);\n+      byte_array_put(obj, value, index, byte_count);\n@@ -1477,1 +1470,1 @@\n-      obj->byte_at_put(index, (jbyte)*((jint*)&val));\n+      obj->byte_at_put(index, (jbyte)value->get_jint());\n@@ -1484,2 +1477,1 @@\n-      val = value->get_int();\n-      obj->bool_at_put(index, (jboolean)*((jint*)&val));\n+      obj->bool_at_put(index, (jboolean)value->get_jint());\n@@ -1609,1 +1601,0 @@\n-      intptr_t val;\n@@ -1643,2 +1634,1 @@\n-            val = value->get_int();\n-            obj->int_field_put(sec_offset, (jint)*((jint*)&val));\n+            obj->int_field_put(sec_offset, value->get_jint());\n@@ -1649,1 +1639,1 @@\n-          \/* no break *\/\n+        \/* no break *\/\n@@ -1655,1 +1645,1 @@\n-          jlong res = (jlong)low->get_int();\n+          jlong res = (jlong)low->get_intptr();\n@@ -1657,1 +1647,1 @@\n-          jlong res = jlong_from((jint)value->get_int(), (jint)low->get_int());\n+          jlong res = jlong_from((jint)value->get_int(), (jint)low->get_jint());\n@@ -1665,2 +1655,1 @@\n-          val = value->get_int();\n-          obj->short_field_put(sec_offset, (jshort)*((jint*)&val));\n+          obj->short_field_put(sec_offset, (jshort)value->get_jint());\n@@ -1671,2 +1660,1 @@\n-          val = value->get_int();\n-          obj->char_field_put(sec_offset, (jchar)*((jint*)&val));\n+          obj->char_field_put(sec_offset, (jchar)value->get_jint());\n@@ -1677,2 +1665,1 @@\n-          val = value->get_int();\n-          obj->byte_field_put(sec_offset, (jbyte)*((jint*)&val));\n+          obj->byte_field_put(sec_offset, (jbyte)value->get_jint());\n@@ -1683,2 +1670,1 @@\n-          val = value->get_int();\n-          obj->bool_field_put(sec_offset, (jboolean)*((jint*)&val));\n+          obj->bool_field_put(sec_offset, (jboolean)value->get_jint());\n@@ -1734,1 +1720,0 @@\n-\n@@ -1800,1 +1785,1 @@\n-  int frame_size = caller.sp() - fr.sp();\n+  int frame_size = pointer_delta_as_int(caller.sp(), fr.sp());\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":47,"deletions":62,"binary":false,"changes":109,"status":"modified"},{"patch":"@@ -245,1 +245,1 @@\n-#if !defined(X86) || !defined(TARGET_COMPILER_gcc) || defined(_WIN64)\n+#if !defined(X86)\n@@ -277,1 +277,1 @@\n-#endif \/\/ !X86 || !TARGET_COMPILER_gcc || _WIN64\n+#endif \/\/ !X86\n@@ -695,1 +695,1 @@\n-    int catch_pco = ret_pc - cm->code_begin();\n+    int catch_pco = pointer_delta_as_int(ret_pc, cm->code_begin());\n@@ -754,1 +754,1 @@\n-  int catch_pco = ret_pc - nm->code_begin();\n+  int catch_pco = pointer_delta_as_int(ret_pc, nm->code_begin());\n@@ -1956,1 +1956,2 @@\n-  if (caller.is_compiled_frame() && !caller.is_deoptimized_frame()) {\n+  if ((caller.is_compiled_frame() && !caller.is_deoptimized_frame()) ||\n+      (caller.is_native_frame() && ((CompiledMethod*)caller.cb())->method()->is_continuation_enter_intrinsic())) {\n@@ -2393,1 +2394,1 @@\n-  return 100.0 * x \/ MAX2(y, (int64_t)1);\n+  return 100.0 * (double)x \/ (double)MAX2(y, (int64_t)1);\n@@ -2429,2 +2430,2 @@\n-    for (int i = 0; i <= n; i++) { sum += histo[i]; weighted_sum += i*histo[i]; }\n-    if (sum >= 1.0) { \/\/ prevent divide by zero or divide overflow\n+    for (int i = 0; i <= n; i++) { sum += (double)histo[i]; weighted_sum += (double)(i*histo[i]); }\n+    if (sum >= 1) { \/\/ prevent divide by zero or divide overflow\n@@ -2434,2 +2435,2 @@\n-        rest -= histo[i];\n-        tty->print_cr(\"%4d: \" UINT64_FORMAT_W(12) \" (%5.1f%%)\", i, histo[i], histo[i] \/ percent);\n+        rest -= (double)histo[i];\n+        tty->print_cr(\"%4d: \" UINT64_FORMAT_W(12) \" (%5.1f%%)\", i, histo[i], (double)histo[i] \/ percent);\n@@ -2514,1 +2515,1 @@\n-    _basic_type_bits = 4,\n+    _basic_type_bits = 5,\n@@ -2594,1 +2595,1 @@\n-          if (bt == T_PRIMITIVE_OBJECT) {\n+          if (bt == T_METADATA) {\n@@ -3105,1 +3106,1 @@\n-              \/\/ Nullable inline type argument, insert InlineTypeNode::IsInit field right after T_PRIMITIVE_OBJECT\n+              \/\/ Nullable inline type argument, insert InlineTypeNode::IsInit field right after T_METADATA delimiter\n@@ -3880,1 +3881,1 @@\n-    if (bt == T_PRIMITIVE_OBJECT) {\n+    if (bt == T_METADATA) {\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":15,"deletions":14,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -54,0 +54,2 @@\n+compiler\/rangechecks\/TestRangeCheckHoistingScaledIV.java 8315969 generic-all\n+\n@@ -74,2 +76,2 @@\n-compiler\/gcbarriers\/TestZGCBarrierElision.java#ZGen 8313737 generic-all\n-compiler\/valhalla\/inlinetypes\/TestArrays.java 8313667 generic-all\n+compiler\/valhalla\/inlinetypes\/TestIntrinsics.java 8323781 generic-all\n+compiler\/valhalla\/inlinetypes\/TestLWorld.java 8323781 generic-all\n@@ -105,2 +107,0 @@\n-runtime\/os\/TestTrimNative.java#trimNative 8312525 linux-all\n-runtime\/os\/TestTrimNative.java#trimNativeLowInterval 8312525 linux-all\n@@ -110,1 +110,3 @@\n-runtime\/ErrorHandling\/TestDwarf.java 8305489 linux-all\n+runtime\/ErrorHandling\/TestDwarf.java#checkDecoder 8305489 linux-all\n+runtime\/ErrorHandling\/MachCodeFramesInErrorFile.java 8313315 linux-ppc64le\n+runtime\/cds\/appcds\/customLoader\/HelloCustom_JFR.java 8241075 linux-all,windows-x64\n@@ -113,1 +115,0 @@\n-applications\/ctw\/modules\/jdk_crypto_ec.java 8312194 generic-all\n@@ -116,1 +117,1 @@\n-containers\/docker\/TestMemoryAwareness.java 8303470 linux-x64\n+containers\/docker\/TestMemoryAwareness.java 8303470 linux-all\n@@ -132,1 +133,1 @@\n-serviceability\/dcmd\/gc\/RunFinalizationTest.java 8227120 linux-all,windows-x64\n+serviceability\/dcmd\/gc\/RunFinalizationTest.java 8227120 linux-all,windows-x64,aix-ppc64\n@@ -141,0 +142,1 @@\n+serviceability\/sa\/ClhsdbDumpclass.java 8316342 generic-all\n@@ -143,0 +145,1 @@\n+serviceability\/jvmti\/RedefineClasses\/RedefineLeakThrowable.java 8316658 generic-all\n@@ -145,0 +148,2 @@\n+serviceability\/jvmti\/Valhalla\/HeapDump\/HeapDump.java 8317416 generic-all\n+\n@@ -191,1 +196,0 @@\n-vmTestbase\/nsk\/jvmti\/AttachOnDemand\/attach002a\/TestDescription.java 8307462 generic-all\n@@ -197,1 +201,1 @@\n-vmTestbase\/nsk\/jvmti\/InterruptThread\/intrpthrd003\/TestDescription.java 8288911 macosx-x64\n+vmTestbase\/nsk\/jvmti\/InterruptThread\/intrpthrd003\/TestDescription.java 8288911 macosx-all\n@@ -217,0 +221,1 @@\n+vmTestbase\/vm\/mlvm\/hiddenloader\/stress\/byteMutation\/Test.java 8317172 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":15,"deletions":10,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -1662,0 +1662,34 @@\n+\n+    \/\/ Test correctness of the Unsafe::isFlattenedArray intrinsic\n+    @Test\n+    public boolean test81(Class<?> cls) {\n+        return U.isFlattenedArray(cls);\n+    }\n+\n+    @Run(test = \"test81\")\n+    public void test81_verifier() {\n+        Asserts.assertEQ(test81(MyValue1[].class), TEST33_FLATTENED_ARRAY, \"test81_1 failed\");\n+        Asserts.assertFalse(test81(String[].class), \"test81_2 failed\");\n+        Asserts.assertFalse(test81(String.class), \"test81_3 failed\");\n+        Asserts.assertFalse(test81(int[].class), \"test81_4 failed\");\n+    }\n+\n+    \/\/ Verify that Unsafe::isFlattenedArray checks with statically known classes\n+    \/\/ are folded\n+    @Test\n+    @IR(failOn = {LOADK})\n+    public boolean test82() {\n+        boolean check1 = U.isFlattenedArray(MyValue1[].class);\n+        if (!TEST33_FLATTENED_ARRAY) {\n+            check1 = !check1;\n+        }\n+        boolean check2 = !U.isFlattenedArray(String[].class);\n+        boolean check3 = !U.isFlattenedArray(String.class);\n+        boolean check4 = !U.isFlattenedArray(int[].class);\n+        return check1 && check2 && check3 && check4;\n+    }\n+\n+    @Run(test = \"test82\")\n+    public void test82_verifier() {\n+        Asserts.assertTrue(test82(), \"test82 failed\");\n+    }\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestIntrinsics.java","additions":35,"deletions":1,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -8,3 +8,1 @@\n-# published by the Free Software Foundation.  Oracle designates this\n-# particular file as subject to the \"Classpath\" exception as provided\n-# by Oracle in the LICENSE file that accompanied this code.\n+# published by the Free Software Foundation.\n","filename":"test\/jdk\/jdk\/incubator\/vector\/gen-tests.sh","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"}]}