{"files":[{"patch":"@@ -711,1 +711,2 @@\n-        int call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_instance), klass);\n+        int call_offset;\n+        call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_instance), klass);\n@@ -744,0 +745,1 @@\n+    case StubId::c1_new_null_free_array_id:\n@@ -751,1 +753,1 @@\n-        } else {\n+        } else if (id == StubId::c1_new_object_array_id) {\n@@ -753,0 +755,2 @@\n+        } else {\n+          __ set_info(\"new_null_free_array\", dont_gc_arguments);\n@@ -762,7 +766,22 @@\n-          int tag = ((id == StubId::c1_new_type_array_id)\n-                     ? Klass::_lh_array_tag_type_value\n-                     : Klass::_lh_array_tag_obj_value);\n-          __ mov(rscratch1, tag);\n-          __ cmpw(t0, rscratch1);\n-          __ br(Assembler::EQ, ok);\n-          __ stop(\"assert(is an array klass)\");\n+          switch (id) {\n+          case StubId::c1_new_type_array_id:\n+            __ cmpw(t0, Klass::_lh_array_tag_type_value);\n+            __ br(Assembler::EQ, ok);\n+            __ stop(\"assert(is a type array klass)\");\n+            break;\n+          case StubId::c1_new_object_array_id:\n+            __ cmpw(t0, Klass::_lh_array_tag_ref_value); \/\/ new \"[Ljava\/lang\/Object;\"\n+            __ br(Assembler::EQ, ok);\n+            __ cmpw(t0, Klass::_lh_array_tag_flat_value);  \/\/ new \"[LVT;\"\n+            __ br(Assembler::EQ, ok);\n+            __ stop(\"assert(is an object or inline type array klass)\");\n+            break;\n+          case StubId::c1_new_null_free_array_id:\n+            __ cmpw(t0, Klass::_lh_array_tag_flat_value);  \/\/ the array can be a flat array.\n+            __ br(Assembler::EQ, ok);\n+            __ cmpw(t0, Klass::_lh_array_tag_ref_value); \/\/ the array cannot be a flat array (due to the InlineArrayElementMaxFlatSize, etc.)\n+            __ br(Assembler::EQ, ok);\n+            __ stop(\"assert(is an object or inline type array klass)\");\n+            break;\n+          default:  ShouldNotReachHere();\n+          }\n@@ -779,1 +798,1 @@\n-        } else {\n+        } else if (id == StubId::c1_new_object_array_id) {\n@@ -781,0 +800,3 @@\n+        } else {\n+          assert(id == StubId::c1_new_null_free_array_id, \"must be\");\n+          call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_null_free_array), klass, length);\n@@ -815,0 +837,87 @@\n+    case StubId::c1_buffer_inline_args_id:\n+    case StubId::c1_buffer_inline_args_no_receiver_id:\n+      {\n+        const char* name = (id == StubId::c1_buffer_inline_args_id) ?\n+          \"buffer_inline_args\" : \"buffer_inline_args_no_receiver\";\n+        StubFrame f(sasm, name, dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm);\n+        Register method = r19;   \/\/ Incoming\n+        address entry = (id == StubId::c1_buffer_inline_args_id) ?\n+          CAST_FROM_FN_PTR(address, buffer_inline_args) :\n+          CAST_FROM_FN_PTR(address, buffer_inline_args_no_receiver);\n+        \/\/ This is called from a C1 method's scalarized entry point\n+        \/\/ where r0-r7 may be holding live argument values so we can't\n+        \/\/ return the result in r0 as the other stubs do. LR is used as\n+        \/\/ a temporary below to avoid the result being clobbered by\n+        \/\/ restore_live_registers. It's saved and restored by\n+        \/\/ StubAssembler::prologue and epilogue anyway.\n+        int call_offset = __ call_RT(lr, noreg, entry, method);\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers(sasm);\n+        __ mov(r20, lr);\n+        __ verify_oop(r20);  \/\/ r20: an array of buffered value objects\n+     }\n+     break;\n+\n+    case StubId::c1_load_flat_array_id:\n+      {\n+        StubFrame f(sasm, \"load_flat_array\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(1, r0); \/\/ r0,: array\n+        f.load_argument(0, r1); \/\/ r1,: index\n+        int call_offset = __ call_RT(r0, noreg, CAST_FROM_FN_PTR(address, load_flat_array), r0, r1);\n+\n+        \/\/ Ensure the stores that initialize the buffer are visible\n+        \/\/ before any subsequent store that publishes this reference.\n+        __ membar(Assembler::StoreStore);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_r0(sasm);\n+\n+        \/\/ r0: loaded element at array[index]\n+        __ verify_oop(r0);\n+      }\n+      break;\n+\n+    case StubId::c1_store_flat_array_id:\n+      {\n+        StubFrame f(sasm, \"store_flat_array\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(2, r0); \/\/ r0: array\n+        f.load_argument(1, r1); \/\/ r1: index\n+        f.load_argument(0, r2); \/\/ r2: value\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, store_flat_array), r0, r1, r2);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_r0(sasm);\n+      }\n+      break;\n+\n+    case StubId::c1_substitutability_check_id:\n+      {\n+        StubFrame f(sasm, \"substitutability_check\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(1, r1); \/\/ r1,: left\n+        f.load_argument(0, r2); \/\/ r2,: right\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, substitutability_check), r1, r2);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_r0(sasm);\n+\n+        \/\/ r0,: are the two operands substitutable\n+      }\n+      break;\n+\n@@ -854,1 +963,1 @@\n-      { StubFrame f(sasm, \"throw_incompatible_class_cast_exception\", dont_gc_arguments, does_not_return);\n+      { StubFrame f(sasm, \"throw_incompatible_class_change_error\", dont_gc_arguments, does_not_return);\n@@ -859,0 +968,12 @@\n+    case StubId::c1_throw_illegal_monitor_state_exception_id:\n+      { StubFrame f(sasm, \"throw_illegal_monitor_state_exception\", dont_gc_arguments);\n+        oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_illegal_monitor_state_exception), false);\n+      }\n+      break;\n+\n+    case StubId::c1_throw_identity_exception_id:\n+      { StubFrame f(sasm, \"throw_identity_exception\", dont_gc_arguments);\n+        oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_identity_exception), true);\n+      }\n+      break;\n+\n@@ -1117,0 +1238,2 @@\n+      \/\/ FIXME: For unhandled trap_id this code fails with assert during vm intialization\n+      \/\/ rather than insert a call to unimplemented_entry\n@@ -1124,0 +1247,2 @@\n+\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Runtime1_aarch64.cpp","additions":136,"deletions":11,"binary":false,"changes":147,"status":"modified"},{"patch":"@@ -52,1 +52,21 @@\n-void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void C2_MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+    Register klass = rscratch1;\n+\n+    mov_metadata(klass, C->method()->holder()->constant_encoding());\n+    clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+\n+    bind(L_skip_barrier);\n+  }\n+\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  bool fp_mode_24b = false;\n+  int stack_bang_size = C->output()->need_stack_bang(bangsize) ? bangsize : 0;\n+\n@@ -97,0 +117,6 @@\n+  if (C->needs_stack_repair()) {\n+    \/\/ Save stack increment just below the saved rbp (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    movptr(Address(rsp, framesize - wordSize), sp_inc + framesize + wordSize);\n+  }\n+\n@@ -115,0 +141,1 @@\n+}\n@@ -116,15 +143,13 @@\n-  if (!is_stub) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n-    Label dummy_slow_path;\n-    Label dummy_continuation;\n-    Label* slow_path = &dummy_slow_path;\n-    Label* continuation = &dummy_continuation;\n-    if (!Compile::current()->output()->in_scratch_emit_size()) {\n-      \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n-      C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n-      Compile::current()->output()->add_stub(stub);\n-      slow_path = &stub->entry();\n-      continuation = &stub->continuation();\n-    }\n-    bs->nmethod_entry_barrier(this, slow_path, continuation);\n+void C2_MacroAssembler::entry_barrier() {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n+  Label dummy_slow_path;\n+  Label dummy_continuation;\n+  Label* slow_path = &dummy_slow_path;\n+  Label* continuation = &dummy_continuation;\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+    C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n+    Compile::current()->output()->add_stub(stub);\n+    slow_path = &stub->entry();\n+    continuation = &stub->continuation();\n@@ -132,0 +157,1 @@\n+  bs->nmethod_entry_barrier(this, slow_path, continuation);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":42,"deletions":16,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);\n+  void verified_entry(Compile* C, int sp_inc = 0);\n@@ -34,0 +34,1 @@\n+  void entry_barrier();\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -46,0 +47,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -47,0 +49,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -56,0 +59,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -59,0 +63,4 @@\n+#include \"vmreg_x86.inline.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/output.hpp\"\n+#endif\n@@ -1306,0 +1314,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2359,0 +2371,107 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  andptr(markword, markWord::inline_type_mask_in_place);\n+  cmpptr(markword, markWord::inline_type_pattern);\n+  jcc(Assembler::equal, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type, bool can_be_null) {\n+  if (can_be_null) {\n+    testptr(object, object);\n+    jcc(Assembler::zero, not_inline_type);\n+  }\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  movptr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  andptr(tmp, is_inline_type_mask);\n+  cmpptr(tmp, is_inline_type_mask);\n+  jcc(Assembler::notEqual, not_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::notEqual, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::equal, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_flat(Register flags, Register temp_reg, Label& is_flat) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_flat_shift);\n+  jcc(Assembler::notEqual, is_flat);\n+}\n+\n+void MacroAssembler::test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::has_null_marker_shift);\n+  jcc(Assembler::notEqual, has_null_marker);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  movptr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  testl(temp_reg, markWord::unlocked_value);\n+  jccb(Assembler::notZero, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  push(rscratch1);\n+  load_prototype_header(temp_reg, oop, rscratch1);\n+  pop(rscratch1);\n+\n+  bind(test_mark_word);\n+  testl(temp_reg, test_bit);\n+  jcc((jmp_set) ? Assembler::notZero : Assembler::zero, jmp_label);\n+}\n+\n+void MacroAssembler::test_flat_array_oop(Register oop, Register temp_reg,\n+                                         Label& is_flat_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flat_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_flat_array_layout(temp_reg, is_flat_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_flat_array_oop(Register oop, Register temp_reg,\n+                                             Label& is_non_flat_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flat_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_flat_array_layout(temp_reg, is_non_flat_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+\n+void MacroAssembler::test_flat_array_layout(Register lh, Label& is_flat_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::notZero, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_layout(Register lh, Label& is_non_flat_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::zero, is_non_flat_array);\n+}\n+\n@@ -3441,0 +3560,118 @@\n+\/\/ Object \/ value buffer allocation...\n+\/\/\n+\/\/ Kills klass and rsi on LP64\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == rax, \"needs to be rax\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  movl(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it is malformed in some way\n+  testl(layout_size, Klass::_lh_instance_slow_path_bit);\n+  jcc(Assembler::notZero, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path.\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  push(klass);\n+  if (UseTLAB) {\n+    tlab_allocate(new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      jmp(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      jmp(initialize_object);\n+    }\n+  } else {\n+    jmp(slow_case);\n+  }\n+\n+  \/\/ If UseTLAB is true, the object is created above and there is an initialize need.\n+  \/\/ Otherwise, skip and go to the slow path.\n+  if (UseTLAB) {\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      if (UseCompactObjectHeaders) {\n+        assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n+        decrement(layout_size, oopDesc::base_offset_in_bytes());\n+      } else {\n+        decrement(layout_size, sizeof(oopDesc));\n+      }\n+      jcc(Assembler::zero, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+      Register zero = klass;\n+      xorl(zero, zero);    \/\/ use zero reg to clear memory (shorter code)\n+      shrl(layout_size, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      \/\/ Ignore partial flag stall after shrl() since it is debug VM\n+      jcc(Assembler::carryClear, L);\n+      stop(\"object size is not multiple of 2 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        bind(loop);\n+        int header_size_bytes = oopDesc::header_size() * HeapWordSize;\n+        assert(is_aligned(header_size_bytes, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n+        movptr(Address(new_obj, layout_size, Address::times_8, header_size_bytes - 1*oopSize), zero);\n+        decrement(layout_size);\n+        jcc(Assembler::notZero, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    if (UseCompactObjectHeaders || Arguments::is_valhalla_enabled()) {\n+      pop(klass);\n+      Register mark_word = t2;\n+      movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+      movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+    } else {\n+     movptr(Address(new_obj, oopDesc::mark_offset_in_bytes()),\n+            (intptr_t)markWord::prototype().value()); \/\/ header\n+     pop(klass);   \/\/ get saved klass back in the register.\n+    }\n+    if (!UseCompactObjectHeaders) {\n+      xorl(rsi, rsi);                 \/\/ use zero reg to clear memory (shorter code)\n+      store_klass_gap(new_obj, rsi);  \/\/ zero klass gap for compressed oops\n+      movptr(t2, klass);         \/\/ preserve klass\n+      store_klass(new_obj, t2, rscratch1);  \/\/ src klass reg is potentially compressed\n+    }\n+    jmp(done);\n+  }\n+\n+  bind(slow_case);\n+  pop(klass);\n+  bind(slow_case_no_pop);\n+  jmp(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -3644,0 +3881,27 @@\n+void MacroAssembler::get_inline_type_field_klass(Register holder_klass, Register index, Register inline_klass) {\n+  inline_layout_info(holder_klass, index, inline_klass);\n+  movptr(inline_klass, Address(inline_klass, InlineLayoutInfo::klass_offset()));\n+}\n+\n+void MacroAssembler::inline_layout_info(Register holder_klass, Register index, Register layout_info) {\n+  movptr(layout_info, Address(holder_klass, InstanceKlass::inline_layout_info_array_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cmpptr(layout_info, 0);\n+    jcc(Assembler::notEqual, done);\n+    stop(\"inline_layout_info_array is null\");\n+    bind(done);\n+  }\n+#endif\n+\n+  InlineLayoutInfo array[2];\n+  int size = (char*)&array[1] - (char*)&array[0]; \/\/ computing size of array elements\n+  if (is_power_of_2(size)) {\n+    shll(index, log2i_exact(size)); \/\/ Scale index by power of 2\n+  } else {\n+    imull(index, index, size); \/\/ Scale the index to be the entry index * array_element_size\n+  }\n+  lea(layout_info, Address(layout_info, index, Address::times_1, Array<InlineLayoutInfo>::base_offset_in_bytes()));\n+}\n+\n@@ -4694,1 +4958,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -4753,1 +5021,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -5147,0 +5419,10 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(dst, src);\n+  } else if (UseCompressedClassPointers) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -5168,0 +5450,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src, Register tmp) {\n+  load_klass(dst, src, tmp);\n+  movptr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -5233,0 +5520,40 @@\n+void MacroAssembler::flat_field_copy(DecoratorSet decorators, Register src, Register dst,\n+                                     Register inline_layout_info) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->flat_field_copy(this, decorators, src, dst, inline_layout_info);\n+}\n+\n+void MacroAssembler::payload_offset(Register inline_klass, Register offset) {\n+  movptr(offset, Address(inline_klass, InlineKlass::adr_members_offset()));\n+  movl(offset, Address(offset, InlineKlass::payload_offset_offset()));\n+}\n+\n+void MacroAssembler::payload_addr(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->payload_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  payload_offset(inline_klass, offset);\n+  if (data == oop) {\n+    addptr(data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert(index != rcx, \"index needs to shift by rcx\");\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rcx, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  movl(rcx, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  shrl(rcx, Klass::_lh_log2_element_size_shift);\n+  andl(rcx, Klass::_lh_log2_element_size_mask);\n+  shlptr(index); \/\/ index << rcx\n+\n+  lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT)));\n+}\n+\n@@ -5589,0 +5916,410 @@\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  testptr(rax, 1);\n+  jcc(Assembler::zero, skip);\n+  int call_offset = -1;\n+\n+#ifdef _LP64\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != nullptr, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(rscratch1, rax); \/\/ save rax for slow_case since *_allocate may corrupt it when allocation failed\n+  if (vk != nullptr) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(rbx, (intptr_t)vk->get_InlineKlass());\n+    jint lh = vk->layout_helper();\n+    assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB && !Klass::layout_helper_needs_slow_path(lh)) {\n+      tlab_allocate(rax, noreg, lh, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. RAX contains ((the InlineKlass* of the return type) | 0x01)\n+    mov(rbx, rax);\n+    andptr(rbx, -2);\n+    if (UseTLAB) {\n+      movl(r14, Address(rbx, Klass::layout_helper_offset()));\n+      testl(r14, Klass::_lh_instance_slow_path_bit);\n+      jcc(Assembler::notZero, slow_case);\n+      tlab_allocate(rax, r14, 0, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = rax;\n+    Register klass = rbx;\n+    if (UseCompactObjectHeaders) {\n+      Register mark_word = r13;\n+      movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+      movptr(Address(buffer_obj, oopDesc::mark_offset_in_bytes()), mark_word);\n+    } else {\n+      movptr(Address(buffer_obj, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::inline_type_prototype().value());\n+      xorl(r13, r13);\n+      store_klass_gap(buffer_obj, r13);\n+      if (vk == nullptr) {\n+        \/\/ store_klass corrupts rbx(klass), so save it in r13 for later use (interpreter case only).\n+        mov(r13, klass);\n+      }\n+      store_klass(buffer_obj, klass, rscratch1);\n+      klass = r13;\n+    }\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != nullptr) {\n+      call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      movptr(rbx, Address(klass, InlineKlass::adr_members_offset()));\n+      movptr(rbx, Address(rbx, InlineKlass::pack_handler_offset()));\n+      call(rbx);\n+    }\n+    jmp(skip);\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(rax, rscratch1);\n+#endif\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to->as_Register(), from->as_Register());\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        Address to_addr = Address(rsp, st_off);\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to_addr, from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to_addr, from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to_addr, from->as_Register());\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(rsp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      if (to->is_reg()) {\n+        if (to->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from_addr);\n+          }\n+        } else {\n+          movq(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(r13, from_addr);\n+        movq(Address(rsp, st_off), r13);\n+      }\n+    }\n+  }\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  \/\/ Two additional slots to account for return address\n+  int sp_inc = (args_on_stack + 2) * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  \/\/ Save the return address, adjust the stack (make sure it is properly\n+  \/\/ 16-byte aligned) and copy the return address to the new top of the stack.\n+  \/\/ The stack will be repaired on return (see MacroAssembler::remove_frame).\n+  assert(sp_inc > 0, \"sanity\");\n+  pop(r13);\n+  subptr(rsp, sp_inc);\n+  push(r13);\n+  return sp_inc;\n+}\n+\n+\/\/ Read all fields from an inline type buffer and store the field values in registers\/stack slots.\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(tmp1, Address(rsp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        testptr(fromReg, fromReg);\n+        jcc(Assembler::zero, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), 1);\n+      } else {\n+        movq(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_XMMRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), dst);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(toReg->as_XMMRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(toReg->as_XMMRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      jmp(L_notNull);\n+      bind(L_null);\n+      \/\/ Set null marker to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+            movq(Address(rsp, st_off), 0);\n+          } else {\n+            xorq(toReg->as_Register(), toReg->as_Register());\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_METADATA, \"should be at delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r14;\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register tmp3 = rbx;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        testb(Address(rsp, ld_off), 1);\n+      } else {\n+        testb(fromReg->as_Register(), 1);\n+      }\n+      jcc(Assembler::notZero, L_notNull);\n+      movptr(val_obj, 0);\n+      jmp(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+    if (!fromReg->is_XMMRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(src, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(dst, fromReg->as_XMMRegister());\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(dst, fromReg->as_XMMRegister());\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();\n+}\n+\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  if (needs_stack_repair) {\n+    \/\/ TODO 8284443 Add a comment drawing the frame like in Aarch64's version of MacroAssembler::remove_frame\n+    movq(rbp, Address(rsp, initial_framesize));\n+    \/\/ The stack increment resides just below the saved rbp\n+    addq(rsp, Address(rsp, initial_framesize - wordSize));\n+  } else {\n+    if (initial_framesize > 0) {\n+      addq(rsp, initial_framesize);\n+    }\n+    pop(rbp);\n+  }\n+}\n+\n@@ -5592,1 +6329,1 @@\n-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, KRegister mask) {\n+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, KRegister mask) {\n@@ -5598,1 +6335,1 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_512bit);\n+    evpbroadcastq(xtmp, val, AVX_512bit);\n@@ -5600,1 +6337,3 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_256bit);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n+    vinserti128_high(xtmp, xtmp);\n@@ -5602,1 +6341,2 @@\n-    pxor(xtmp, xtmp);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n@@ -5625,1 +6365,1 @@\n-    fill64_masked(3, base, 0, xtmp, mask, cnt, rtmp, true);\n+    fill64_masked(3, base, 0, xtmp, mask, cnt, val, true);\n@@ -5644,1 +6384,1 @@\n-    fill32_masked(3, base, 0, xtmp, mask, cnt, rtmp);\n+    fill32_masked(3, base, 0, xtmp, mask, cnt, val);\n@@ -5746,2 +6486,2 @@\n-void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp,\n-                               bool is_large, KRegister mask) {\n+void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp,\n+                               bool is_large, bool word_copy_only, KRegister mask) {\n@@ -5752,1 +6492,1 @@\n-  assert(tmp==rax,   \"tmp register must be eax for rep stos\");\n+  assert(val==rax,   \"val register must be eax for rep stos\");\n@@ -5758,3 +6498,0 @@\n-  if (!is_large || !UseXMMForObjInit) {\n-    xorptr(tmp, tmp);\n-  }\n@@ -5772,1 +6509,1 @@\n-    movptr(Address(base, cnt, Address::times_ptr), tmp);\n+    movptr(Address(base, cnt, Address::times_ptr), val);\n@@ -5781,1 +6518,1 @@\n-  if (UseFastStosb) {\n+  if (UseFastStosb && !word_copy_only) {\n@@ -5785,1 +6522,1 @@\n-    xmm_clear_mem(base, cnt, tmp, xtmp, mask);\n+    xmm_clear_mem(base, cnt, val, xtmp, mask);\n@@ -9705,0 +10442,3 @@\n+  \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+  andptr(reg_rax, ~((int) markWord::inline_type_bit_in_place));\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":757,"deletions":17,"binary":false,"changes":774,"status":"modified"},{"patch":"@@ -1652,0 +1652,4 @@\n+  if (_entry_point == nullptr) {\n+    \/\/ CallLeafNoFPInDirect\n+    return 3; \/\/ callq (register)\n+  }\n@@ -1658,0 +1662,1 @@\n+\n@@ -1883,6 +1888,1 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+  __ verified_entry(C);\n@@ -1890,9 +1890,2 @@\n-    Label L_skip_barrier;\n-    Register klass = rscratch1;\n-\n-    __ mov_metadata(klass, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n-\n-    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n-\n-    __ bind(L_skip_barrier);\n+  if (ra_->C->stub_function() == nullptr) {\n+    __ entry_barrier();\n@@ -1901,1 +1894,3 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != nullptr);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n+  }\n@@ -1913,5 +1908,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n@@ -1965,13 +1955,3 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove word for return adr already pushed\n-  \/\/ and RBP\n-  framesize -= 2*wordSize;\n-\n-  \/\/ Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here\n-\n-  if (framesize) {\n-    __ addq(rsp, framesize);\n-  }\n-\n-  __ popq(rbp);\n+  \/\/ Subtract two words to account for return address and rbp\n+  int initial_framesize = C->output()->frame_size_in_bytes() - 2*wordSize;\n+  __ remove_frame(initial_framesize, C->needs_stack_repair());\n@@ -1996,6 +1976,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -2603,0 +2577,43 @@\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"MachVEPNode\");\n+}\n+#endif\n+\n+void MachVEPNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  CodeBuffer* cbuf = masm->code();\n+  uint insts_size = cbuf->insts_size();\n+  if (!_verified) {\n+    __ ic_check(1);\n+  } else {\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == nullptr) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int initial_framesize = ra_->C->output()->frame_size_in_bytes() - 2*wordSize;\n+      __ remove_frame(initial_framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ jmp(dummy_verified_entry);\n+    } else {\n+      __ jmp(*_verified_entry);\n+    }\n+  }\n+  \/* WARNING these NOPs are critical so that verified entry point is properly\n+     4 bytes aligned for patching by NativeJump::patch_verified_entry() *\/\n+  int nops_cnt = 4 - ((cbuf->insts_size() - insts_size) & 0x3);\n+  nops_cnt &= 0x3; \/\/ Do not add nops if code is aligned.\n+  if (nops_cnt > 0) {\n+    __ nop(nops_cnt);\n+  }\n+}\n+\n@@ -2623,6 +2640,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -4603,0 +4614,33 @@\n+    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic() && _method->return_type()->is_loaded()) {\n+      \/\/ The last return value is not set by the callee but used to pass the null marker to compiled code.\n+      \/\/ Search for the corresponding projection, get the register and emit code that initialized it.\n+      uint con = (tf()->range_cc()->cnt() - 1);\n+      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+        ProjNode* proj = fast_out(i)->as_Proj();\n+        if (proj->_con == con) {\n+          \/\/ Set null marker if rax is non-null (a non-null value is returned buffered or scalarized)\n+          OptoReg::Name optoReg = ra_->get_reg_first(proj);\n+          VMReg reg = OptoReg::as_VMReg(optoReg, ra_->_framesize, OptoReg::reg2stack(ra_->_matcher._new_SP));\n+          Register toReg = reg->is_reg() ? reg->as_Register() : rscratch1;\n+          __ testq(rax, rax);\n+          __ setb(Assembler::notZero, toReg);\n+          __ movzbl(toReg, toReg);\n+          if (reg->is_stack()) {\n+            int st_off = reg->reg2stack() * VMRegImpl::stack_slot_size;\n+            __ movq(Address(rsp, st_off), toReg);\n+          }\n+          break;\n+        }\n+      }\n+      if (return_value_is_used()) {\n+        \/\/ An inline type is returned as fields in multiple registers.\n+        \/\/ Rax either contains an oop if the inline type is buffered or a pointer\n+        \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero rax\n+        \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n+        \/\/ rax &= (rax & 1) - 1\n+        __ movptr(rscratch1, rax);\n+        __ andptr(rscratch1, 0x1);\n+        __ subptr(rscratch1, 0x1);\n+        __ andptr(rax, rscratch1);\n+      }\n+    }\n@@ -5765,0 +5809,16 @@\n+\/\/ Indirect Narrow Oop Operand\n+operand indCompressedOop(rRegN reg) %{\n+  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(DecodeN reg);\n+\n+  op_cost(10);\n+  format %{\"[R12 + $reg << 3] (compressed oop addressing)\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0xc); \/\/ R12\n+    index($reg);\n+    scale(0x3);\n+    disp(0x0);\n+  %}\n+%}\n+\n@@ -6241,1 +6301,1 @@\n-               indCompressedOopOffset,\n+               indCompressedOop, indCompressedOopOffset,\n@@ -8839,0 +8899,26 @@\n+instruct castI2N(rRegN dst, rRegI src)\n+%{\n+  match(Set dst (CastI2N src));\n+\n+  format %{ \"movq    $dst, $src\\t# int -> narrow ptr\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movl($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+instruct castN2X(rRegL dst, rRegN src)\n+%{\n+  match(Set dst (CastP2X src));\n+\n+  format %{ \"movq    $dst, $src\\t# ptr -> long\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n@@ -15085,0 +15171,1 @@\n+\n@@ -15087,1 +15174,1 @@\n-instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -15090,3 +15177,120 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX <= 2));\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                            Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Small non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                       Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  ins_cost(125);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15140,2 +15344,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false, $ktmp$$KRegister);\n@@ -15146,3 +15350,2 @@\n-\/\/ Small non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                       Universe dummy, rFlagsReg cr)\n+instruct rep_stos_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                 Universe dummy, rFlagsReg cr)\n@@ -15150,2 +15353,2 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX > 2));\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -15153,1 +15356,1 @@\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15201,2 +15404,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true, $ktmp$$KRegister);\n@@ -15208,1 +15411,1 @@\n-instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -15211,3 +15414,99 @@\n-  predicate((UseAVX <=2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_large_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                                  Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Large non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                             Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15252,2 +15551,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false, $ktmp$$KRegister);\n@@ -15258,3 +15557,2 @@\n-\/\/ Large non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                             Universe dummy, rFlagsReg cr)\n+instruct rep_stos_large_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                       Universe dummy, rFlagsReg cr)\n@@ -15262,3 +15560,3 @@\n-  predicate((UseAVX > 2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -15303,2 +15601,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true, $ktmp$$KRegister);\n@@ -15310,1 +15608,1 @@\n-instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rRegI zero, kReg ktmp, Universe dummy, rFlagsReg cr)\n+instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rax_RegL val, kReg ktmp, Universe dummy, rFlagsReg cr)\n@@ -15312,2 +15610,3 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (MaxVectorSize >= 32) && VM_Version::supports_avx512vl());\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() &&\n+            ((MaxVectorSize >= 32) && VM_Version::supports_avx512vl()));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -15315,1 +15614,1 @@\n-  effect(TEMP tmp, TEMP zero, TEMP ktmp, KILL cr);\n+  effect(TEMP tmp, USE_KILL val, TEMP ktmp, KILL cr);\n@@ -15318,1 +15617,1 @@\n-   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$constant, $val$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n@@ -17165,0 +17464,16 @@\n+\/\/ Call runtime without safepoint\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPInDirect(rRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == nullptr);\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(300);\n+  format %{ \"call_leaf_nofp,runtime indirect \" %}\n+  ins_encode %{\n+     __ call($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -17168,0 +17483,1 @@\n+  predicate(n->as_Call()->entry_point() != nullptr);\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":396,"deletions":80,"binary":false,"changes":476,"status":"modified"},{"patch":"@@ -80,0 +80,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -506,1 +508,1 @@\n-  soc->do_tag(objArrayOopDesc::base_offset_in_bytes());\n+  soc->do_tag(refArrayOopDesc::base_offset_in_bytes());\n@@ -585,1 +587,8 @@\n-        case atos: new_code = Bytecodes::_fast_agetfield; break;\n+        case atos: {\n+          if (rfe->is_flat()) {\n+            new_code = Bytecodes::_fast_vgetfield;\n+          } else {\n+            new_code = Bytecodes::_fast_agetfield;\n+          }\n+          break;\n+        }\n@@ -610,1 +619,8 @@\n-        case atos: new_code = Bytecodes::_fast_aputfield; break;\n+        case atos: {\n+          if (rfe->is_flat() || rfe->is_null_free_inline_type()) {\n+            new_code = Bytecodes::_fast_vputfield;\n+          } else {\n+            new_code = Bytecodes::_fast_aputfield;\n+          }\n+          break;\n+        }\n","filename":"src\/hotspot\/share\/cds\/aotMetaspace.cpp","additions":19,"deletions":3,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -60,0 +61,3 @@\n+bool CDSConfig::_module_patching_disables_cds = false;\n+bool CDSConfig::_java_base_module_patching_disables_cds = false;\n+\n@@ -146,0 +150,3 @@\n+    if (Arguments::is_valhalla_enabled()) {\n+      tmp.print_raw(\"_valhalla\");\n+    }\n@@ -334,2 +341,1 @@\n-    \"jdk.module.upgrade.path\",\n-    \"jdk.module.patch.0\"\n+    \"jdk.module.upgrade.path\"\n@@ -339,2 +345,1 @@\n-    \"--upgrade-module-path\",\n-    \"--patch-module\"\n+    \"--upgrade-module-path\"\n@@ -363,0 +368,6 @@\n+\n+  if (module_patching_disables_cds()) {\n+    vm_exit_during_initialization(\n+            \"Cannot use the following option when dumping the shared archive\", \"--patch-module\");\n+  }\n+\n@@ -391,0 +402,10 @@\n+\n+  if (module_patching_disables_cds()) {\n+    if (RequireSharedSpaces) {\n+      warning(\"CDS is disabled when the %s option is specified.\", \"--patch-module\");\n+    } else {\n+      log_info(cds)(\"CDS is disabled when the %s option is specified.\", \"--patch-module\");\n+    }\n+    return true;\n+  }\n+\n@@ -625,1 +646,1 @@\n-bool CDSConfig::check_vm_args_consistency(bool patch_mod_javabase, bool mode_flag_cmd_line) {\n+bool CDSConfig::check_vm_args_consistency(bool mode_flag_cmd_line) {\n@@ -700,1 +721,1 @@\n-  if (is_using_archive() && patch_mod_javabase) {\n+  if (is_using_archive() && java_base_module_patching_disables_cds() && module_patching_disables_cds()) {\n","filename":"src\/hotspot\/share\/cds\/cdsConfig.cpp","additions":27,"deletions":6,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -49,0 +49,3 @@\n+  static bool _module_patching_disables_cds;\n+  static bool _java_base_module_patching_disables_cds;\n+\n@@ -100,1 +103,6 @@\n-  static bool check_vm_args_consistency(bool patch_mod_javabase, bool mode_flag_cmd_line) NOT_CDS_RETURN_(true);\n+  static bool check_vm_args_consistency(bool mode_flag_cmd_line) NOT_CDS_RETURN_(true);\n+\n+  static bool module_patching_disables_cds() { return CDS_ONLY(_module_patching_disables_cds) NOT_CDS(false); }\n+  static void set_module_patching_disables_cds() { CDS_ONLY(_module_patching_disables_cds = true;) }\n+  static bool java_base_module_patching_disables_cds() { return CDS_ONLY(_java_base_module_patching_disables_cds) NOT_CDS(false); }\n+  static void set_java_base_module_patching_disables_cds() { CDS_ONLY(_java_base_module_patching_disables_cds = true;) }\n","filename":"src\/hotspot\/share\/cds\/cdsConfig.hpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -94,0 +94,3 @@\n+  \/\/ There should be no oops for ObjArrayKlass but InstanceKlass::array_klasses holds a list of ObjArrayKlass,\n+  \/\/ therefore we need the super of the refined array klass.\n+  Klass* oop_field_klass = oop_field->is_refined_objArray() ? oop_field->klass()->super() : oop_field->klass();\n@@ -97,1 +100,1 @@\n-  } else if (oop_field->klass() != ik && oop_field->klass() != ik->array_klass_or_null()) {\n+  } else if (oop_field_klass != ik && oop_field_klass != ik->array_klass_or_null()) {\n","filename":"src\/hotspot\/share\/cds\/cdsEnumKlass.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -810,0 +810,1 @@\n+      \/\/ For valhalla, the prototype header is the same as markWord::prototype();\n@@ -1517,1 +1518,5 @@\n-      assert(resolved_k == k, \"classes used by archived heap must not be replaced by JVMTI ClassFileLoadHook\");\n+      if (resolved_k->is_array_klass()) {\n+        assert(resolved_k == k || resolved_k == k->super(), \"classes used by archived heap must not be replaced by JVMTI ClassFileLoadHook\");\n+      } else {\n+        assert(resolved_k == k, \"classes used by archived heap must not be replaced by JVMTI ClassFileLoadHook\");\n+      }\n@@ -2271,0 +2276,1 @@\n+\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -57,0 +57,2 @@\n+#include \"oops\/fieldStreams.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -74,0 +76,1 @@\n+#include \"runtime\/os.hpp\"\n@@ -188,0 +191,15 @@\n+\/\/ These migrated value classes are loaded by the bootstrap class loader but are added to the initiating\n+\/\/ loaders automatically so that fields of these types can be found and potentially flattened during\n+\/\/ field layout.\n+static void add_migrated_value_classes(ClassLoaderData* cld) {\n+  JavaThread* current = JavaThread::current();\n+  auto add_klass = [&] (Symbol* classname) {\n+    InstanceKlass* ik = SystemDictionary::find_instance_klass(current, classname, Handle(current, nullptr));\n+    assert(ik != nullptr, \"Must exist\");\n+    SystemDictionary::add_to_initiating_loader(current, ik, cld);\n+  };\n+\n+  MonitorLocker mu1(SystemDictionary_lock);\n+  vmSymbols::migrated_class_names_do(add_klass);\n+}\n+\n@@ -193,2 +211,18 @@\n-    return (class_loader() == nullptr) ? ClassLoaderData::the_null_class_loader_data() :\n-                                      ClassLoaderDataGraph::find_or_create(class_loader);\n+    if (class_loader() == nullptr) {\n+      return ClassLoaderData::the_null_class_loader_data();\n+    } else {\n+      bool created = false;\n+      ClassLoaderData* cld = ClassLoaderDataGraph::find_or_create(class_loader, created);\n+      if (created && Arguments::enable_preview()) {\n+        if (CDSConfig::is_using_aot_linked_classes() && java_system_loader() == nullptr) {\n+          \/\/ We are inside AOTLinkedClassBulkLoader::preload_classes().\n+          \/\/\n+          \/\/ AOTLinkedClassBulkLoader will automatically initiate the loading of all archived\n+          \/\/ public classes from the boot loader into platform\/system loaders, so there's\n+          \/\/ no need to call add_migrated_value_classes().\n+        } else {\n+          add_migrated_value_classes(cld);\n+        }\n+      }\n+      return cld;\n+    }\n@@ -402,1 +436,2 @@\n-\/\/ during class definition to allow class circularity checking\n+\/\/ during class definition, or may be called for inline field layout processing\n+\/\/ to detect class circularity errors.\n@@ -415,0 +450,2 @@\n+\/\/ inline field layout callers:\n+\/\/    The field's class must be loaded to determine layout.\n@@ -418,1 +455,1 @@\n-\/\/ placeholder for the same thread, class, classloader is found.\n+\/\/ placeholder for the same thread, class, and classloader is found.\n@@ -453,1 +490,1 @@\n-    \/\/ Must check ClassCircularity before resolving next_name (superclass or interface).\n+    \/\/ Must check ClassCircularity before resolving next_name (superclass, interface, field types or speculatively preloaded argument types).\n@@ -477,1 +514,1 @@\n-  \/\/ Resolve the superclass or superinterface, check results on return\n+  \/\/ Resolve the superclass, superinterface, field type or speculatively preloaded argument types and check results on return.\n@@ -915,2 +952,1 @@\n-  assert(!ModuleEntryTable::javabase_moduleEntry()->is_patched(),\n-         \"Cannot use sharing if java.base is patched\");\n+  assert(!CDSConfig::module_patching_disables_cds(), \"Cannot use CDS\");\n@@ -992,1 +1028,1 @@\n-        assert(!mod_entry->is_patched(), \"cannot load archived classes for patched module\");\n+        assert(!CDSConfig::module_patching_disables_cds(), \"Cannot use CDS\");\n@@ -1070,0 +1106,69 @@\n+\/\/ Pre-load class referred to in non-static null-free instance field. These fields trigger MANDATORY loading.\n+\/\/ Some pre-loading does not fail fatally\n+bool SystemDictionary::preload_from_null_free_field(InstanceKlass* ik, Handle class_loader, Symbol* sig, int field_index, TRAPS) {\n+  TempNewSymbol name = Signature::strip_envelope(sig);\n+  log_info(class, preload)(\"Preloading of class %s during loading of shared class %s. \"\n+                           \"Cause: a null-free non-static field is declared with this type\",\n+                           name->as_C_string(), ik->name()->as_C_string());\n+  InstanceKlass* real_k = SystemDictionary::resolve_with_circularity_detection(ik->name(), name,\n+                                                                               class_loader, false, CHECK_false);\n+  if (HAS_PENDING_EXCEPTION) {\n+    log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                \"(cause: null-free non-static field) failed: %s\",\n+                                name->as_C_string(), ik->name()->as_C_string(),\n+                                PENDING_EXCEPTION->klass()->name()->as_C_string());\n+    return false; \/\/ Exception is still pending\n+  }\n+\n+  InstanceKlass* k = ik->get_inline_type_field_klass_or_null(field_index);\n+  if (real_k != k) {\n+    \/\/ oops, the app has substituted a different version of k! Does not fail fatally\n+    log_info(class, preload)(\"Preloading of class %s during loading of shared class %s \"\n+                                \"(cause: null-free non-static field) failed : \"\n+                                \"app substituted a different version of %s\",\n+                                name->as_C_string(), ik->name()->as_C_string(),\n+                                name->as_C_string());\n+    return false;\n+  }\n+  log_info(class, preload)(\"Preloading of class %s during loading of shared class %s \"\n+                           \"(cause: null-free non-static field) succeeded\",\n+                           name->as_C_string(), ik->name()->as_C_string());\n+\n+  assert(real_k != nullptr, \"Sanity check\");\n+  InstanceKlass::check_can_be_annotated_with_NullRestricted(real_k, ik->name(), CHECK_false);\n+\n+  return true;\n+}\n+\n+\/\/ Tries to pre-load classes referred to in non-static nullable instance fields if they are found in the\n+\/\/ loadable descriptors attribute. If loading fails, we can fail silently.\n+void SystemDictionary::try_preload_from_loadable_descriptors(InstanceKlass* ik, Handle class_loader, Symbol* sig, int field_index, TRAPS) {\n+  TempNewSymbol name = Signature::strip_envelope(sig);\n+  if (name != ik->name() && ik->is_class_in_loadable_descriptors_attribute(sig)) {\n+    log_info(class, preload)(\"Preloading of class %s during loading of shared class %s. \"\n+                             \"Cause: field type in LoadableDescriptors attribute\",\n+                             name->as_C_string(), ik->name()->as_C_string());\n+    InstanceKlass* real_k = SystemDictionary::resolve_with_circularity_detection(ik->name(), name,\n+                                                                                 class_loader, false, THREAD);\n+    if (HAS_PENDING_EXCEPTION) {\n+      CLEAR_PENDING_EXCEPTION;\n+    }\n+\n+    InstanceKlass* k = ik->get_inline_type_field_klass_or_null(field_index);\n+    if (real_k != k) {\n+      \/\/ oops, the app has substituted a different version of k!\n+      log_info(class, preload)(\"Preloading of class %s during loading of shared class %s \"\n+                                  \"(cause: field type in LoadableDescriptors attribute) failed : \"\n+                                  \"app substituted a different version of %s\",\n+                                  name->as_C_string(), ik->name()->as_C_string(),\n+                                  k->name()->as_C_string());\n+      return;\n+    } else if (real_k != nullptr) {\n+      log_info(class, preload)(\"Preloading of class %s during loading of shared class %s \"\n+                               \"(cause: field type in LoadableDescriptors attribute) succeeded\",\n+                                name->as_C_string(), ik->name()->as_C_string());\n+    }\n+  }\n+}\n+\n+\n@@ -1093,0 +1198,21 @@\n+  if (ik->has_inline_type_fields()) {\n+    for (AllFieldStream fs(ik); !fs.done(); fs.next()) {\n+      if (fs.access_flags().is_static()) continue;\n+\n+      Symbol* sig = fs.signature();\n+      int field_index = fs.index();\n+\n+      if (fs.is_null_free_inline_type()) {\n+        \/\/ A false return means that the class didn't load for other reasons than an exception.\n+        bool check = preload_from_null_free_field(ik, class_loader, sig, field_index, CHECK_NULL);\n+        if (!check) {\n+          ik->set_shared_loading_failed();\n+          return nullptr;\n+        }\n+      } else if (Signature::has_envelope(sig)) {\n+          \/\/ Pending exceptions are cleared so we can fail silently\n+          try_preload_from_loadable_descriptors(ik, class_loader, sig, field_index, CHECK_NULL);\n+      }\n+    }\n+  }\n+\n@@ -1128,0 +1254,1 @@\n+\n@@ -1709,1 +1836,0 @@\n-#if INCLUDE_CDS\n@@ -1712,1 +1838,3 @@\n-\/\/ This API should be used only by AOTLinkedClassBulkLoader\n+\/\/ This API is used by AOTLinkedClassBulkLoader and to register boxing\n+\/\/ classes from java.lang in all class loaders to enable more value\n+\/\/ classes optimizations\n@@ -1716,1 +1844,0 @@\n-  assert(CDSConfig::is_using_aot_linked_classes(), \"must be\");\n@@ -1722,2 +1849,3 @@\n-  assert(dictionary->find_class(current, name) == nullptr, \"sanity\");\n-  dictionary->add_klass(current, name, k);\n+  if (dictionary->find_class(current, name) == nullptr) {\n+    dictionary->add_klass(current, name, k);\n+  }\n@@ -1725,1 +1853,0 @@\n-#endif\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":142,"deletions":15,"binary":false,"changes":157,"status":"modified"},{"patch":"@@ -142,0 +142,1 @@\n+\n@@ -287,1 +288,1 @@\n-                                       ClassLoaderData* loader_data) NOT_CDS_RETURN;\n+                                       ClassLoaderData* loader_data);\n@@ -333,0 +334,2 @@\n+  static bool preload_from_null_free_field(InstanceKlass* ik, Handle class_loader, Symbol* sig, int field_index, TRAPS);\n+  static void try_preload_from_loadable_descriptors(InstanceKlass* ik, Handle class_loader, Symbol* sig, int field_index, TRAPS);\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -130,0 +130,1 @@\n+  do_klass(ValueObjectMethods_klass,                    java_lang_runtime_ValueObjectMethods                  ) \\\n@@ -177,0 +178,24 @@\n+  \/* Other valhalla migrated klasses. *\/                                                                        \\\n+  do_klass(Number_klass,                                java_lang_Number                                      ) \\\n+  do_klass(Optional_klass,                              java_util_Optional                                    ) \\\n+  do_klass(OptionalInt_klass,                           java_util_OptionalInt                                 ) \\\n+  do_klass(OptionalLong_klass,                          java_util_OptionalLong                                ) \\\n+  do_klass(OptionalDouble_klass,                        java_util_OptionalDouble                              ) \\\n+  do_klass(LocalDate_klass,                             java_time_LocalDate                                   ) \\\n+  do_klass(LocalDateTime_klass,                         java_time_LocalDateTime                               ) \\\n+  do_klass(LocalTime_klass,                             java_time_LocalTime                                   ) \\\n+  do_klass(Duration_klass,                              java_time_Duration                                    ) \\\n+  do_klass(Instant_klass,                               java_time_Instant                                     ) \\\n+  do_klass(MonthDay_klass,                              java_time_MonthDay                                    ) \\\n+  do_klass(ZonedDateTime_klass,                         java_time_ZonedDateTime                               ) \\\n+  do_klass(OffsetDateTime_klass,                        java_time_OffsetDateTime                              ) \\\n+  do_klass(OffsetTime_klass,                            java_time_OffsetTime                                  ) \\\n+  do_klass(YearMonth_klass,                             java_time_YearMonth                                   ) \\\n+  do_klass(Year_klass,                                  java_time_Year                                        ) \\\n+  do_klass(Period_klass,                                java_time_Period                                      ) \\\n+  do_klass(chrono_ChronoLocalDateImpl_klass,            java_time_chrono_ChronoLocalDateImpl                  ) \\\n+  do_klass(chrono_MinguoDate_klass,                     java_time_chrono_MinguoDate                           ) \\\n+  do_klass(chrono_HijrahDate_klass,                     java_time_chrono_HijrahDate                           ) \\\n+  do_klass(chrono_JapaneseDate_klass,                   java_time_chrono_JapaneseDate                         ) \\\n+  do_klass(chrono_ThaiBuddhistDate_klass,               java_time_chrono_ThaiBuddhistDate                     ) \\\n+                                                                                                                \\\n","filename":"src\/hotspot\/share\/classfile\/vmClassMacros.hpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -302,1 +302,1 @@\n-        klass->is_objArray_klass()) {\n+        klass->is_refArray_klass()) {\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -66,0 +66,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -72,0 +73,1 @@\n+#include \"oops\/markWord.hpp\"\n@@ -75,0 +77,1 @@\n+#include \"oops\/refArrayKlass.hpp\"\n@@ -152,0 +155,5 @@\n+void InlineLayoutInfo::metaspace_pointers_do(MetaspaceClosure* it) {\n+  log_trace(cds)(\"Iter(InlineFieldInfo): %p\", this);\n+  it->push(&_klass);\n+}\n+\n@@ -173,0 +181,13 @@\n+bool InstanceKlass::field_is_null_free_inline_type(int index) const {\n+  return field(index).field_flags().is_null_free_inline_type();\n+}\n+\n+bool InstanceKlass::is_class_in_loadable_descriptors_attribute(Symbol* name) const {\n+  if (_loadable_descriptors == nullptr) return false;\n+  for (int i = 0; i < _loadable_descriptors->length(); i++) {\n+        Symbol* class_name = _constants->symbol_at(_loadable_descriptors->at(i));\n+        if (class_name == name) return true;\n+  }\n+  return false;\n+}\n+\n@@ -460,1 +481,2 @@\n-                                       parser.is_interface());\n+                                       parser.is_interface(),\n+                                       parser.is_inline_type());\n@@ -482,0 +504,3 @@\n+  } else if (parser.is_inline_type()) {\n+    \/\/ inline type\n+    ik = new (loader_data, size, THREAD) InlineKlass(parser);\n@@ -498,0 +523,6 @@\n+#ifdef ASSERT\n+  ik->bounds_check((address) ik->start_of_vtable(), false, size);\n+  ik->bounds_check((address) ik->start_of_itable(), false, size);\n+  ik->bounds_check((address) ik->end_of_itable(), true, size);\n+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);\n+#endif \/\/ASSERT\n@@ -501,0 +532,23 @@\n+#ifndef PRODUCT\n+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {\n+  const char* bad = nullptr;\n+  address end = nullptr;\n+  if (addr < (address)this) {\n+    bad = \"before\";\n+  } else if (addr == (address)this) {\n+    if (edge_ok)  return true;\n+    bad = \"just before\";\n+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {\n+    if (edge_ok)  return true;\n+    bad = \"just after\";\n+  } else if (addr > end) {\n+    bad = \"after\";\n+  } else {\n+    return true;\n+  }\n+  tty->print_cr(\"%s object bounds: \" INTPTR_FORMAT \" [\" INTPTR_FORMAT \"..\" INTPTR_FORMAT \"]\",\n+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);\n+  Verbose = WizardMode = true; this->print(); \/\/@@\n+  return false;\n+}\n+#endif \/\/PRODUCT\n@@ -528,2 +582,2 @@\n-InstanceKlass::InstanceKlass(const ClassFileParser& parser, KlassKind kind, ReferenceType reference_type) :\n-  Klass(kind),\n+InstanceKlass::InstanceKlass(const ClassFileParser& parser, KlassKind kind, markWord prototype_header, ReferenceType reference_type) :\n+  Klass(kind, prototype_header),\n@@ -540,1 +594,5 @@\n-  _init_thread(nullptr)\n+  _acmp_maps_offset(0),\n+  _init_thread(nullptr),\n+  _inline_layout_info_array(nullptr),\n+  _loadable_descriptors(nullptr),\n+  _adr_inline_klass_members(nullptr)\n@@ -547,0 +605,3 @@\n+  if (parser.has_inline_fields()) {\n+    set_has_inline_type_fields();\n+  }\n@@ -702,0 +763,5 @@\n+  if (inline_layout_info_array() != nullptr) {\n+    MetadataFactory::free_array<InlineLayoutInfo>(loader_data, inline_layout_info_array());\n+  }\n+  set_inline_layout_info_array(nullptr);\n+\n@@ -736,0 +802,7 @@\n+  if (loadable_descriptors() != nullptr &&\n+      loadable_descriptors() != Universe::the_empty_short_array() &&\n+      !loadable_descriptors()->in_aot_cache()) {\n+    MetadataFactory::free_array<jushort>(loader_data, loadable_descriptors());\n+  }\n+  set_loadable_descriptors(nullptr);\n+\n@@ -778,0 +851,22 @@\n+\/\/ Static size helper\n+int InstanceKlass::size(int vtable_length,\n+                        int itable_length,\n+                        int nonstatic_oop_map_size,\n+                        bool is_interface,\n+                        bool is_inline_type) {\n+  return align_metadata_size(header_size() +\n+         vtable_length +\n+         itable_length +\n+         nonstatic_oop_map_size +\n+         (is_interface ? (int)sizeof(Klass*) \/ wordSize : 0) +\n+         (is_inline_type ? (int)sizeof(InlineKlass::Members) \/ wordSize : 0));\n+}\n+\n+int InstanceKlass::size() const {\n+  return size(vtable_length(),\n+              itable_length(),\n+              nonstatic_oop_map_size(),\n+              is_interface(),\n+              is_inline_klass());\n+}\n+\n@@ -930,0 +1025,38 @@\n+static void load_classes_from_loadable_descriptors_attribute(InstanceKlass *ik, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  if (ik->loadable_descriptors() != nullptr && PreloadClasses) {\n+    HandleMark hm(THREAD);\n+    for (int i = 0; i < ik->loadable_descriptors()->length(); i++) {\n+      Symbol* sig = ik->constants()->symbol_at(ik->loadable_descriptors()->at(i));\n+      if (!Signature::has_envelope(sig)) continue;\n+      TempNewSymbol class_name = Signature::strip_envelope(sig);\n+      if (class_name == ik->name()) continue;\n+      log_info(class, preload)(\"Preloading of class %s during linking of class %s \"\n+                               \"because of the class is listed in the LoadableDescriptors attribute\",\n+                               sig->as_C_string(), ik->name()->as_C_string());\n+      oop loader = ik->class_loader();\n+      Klass* klass = SystemDictionary::resolve_or_null(class_name,\n+                                                        Handle(THREAD, loader), THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        CLEAR_PENDING_EXCEPTION;\n+      }\n+      if (klass != nullptr) {\n+        log_info(class, preload)(\"Preloading of class %s during linking of class %s \"\n+                                 \"(cause: LoadableDescriptors attribute) succeeded\",\n+                                 class_name->as_C_string(), ik->name()->as_C_string());\n+        if (!klass->is_inline_klass()) {\n+          \/\/ Non value class are allowed by the current spec, but it could be an indication\n+          \/\/ of an issue so let's log a warning\n+          log_info(class, preload)(\"Preloading of class %s during linking of class %s \"\n+                                      \"(cause: LoadableDescriptors attribute) but loaded class is not a value class\",\n+                                      class_name->as_C_string(), ik->name()->as_C_string());\n+        }\n+      } else {\n+        log_info(class, preload)(\"Preloading of class %s during linking of class %s \"\n+                                    \"(cause: LoadableDescriptors attribute) failed\",\n+                                    class_name->as_C_string(), ik->name()->as_C_string());\n+      }\n+    }\n+  }\n+}\n+\n@@ -1000,0 +1133,7 @@\n+  if (Arguments::is_valhalla_enabled()) {\n+    \/\/ Aggressively preloading all classes from the LoadableDescriptors attribute\n+    \/\/ so inline classes can be scalarized in the calling conventions computed below\n+    load_classes_from_loadable_descriptors_attribute(this, THREAD);\n+    assert(!HAS_PENDING_EXCEPTION, \"Shouldn't have pending exceptions from call above\");\n+  }\n+\n@@ -1323,0 +1463,21 @@\n+  \/\/ Pre-allocating an all-zero value to be used to reset nullable flat storages\n+  if (is_inline_klass()) {\n+      InlineKlass* vk = InlineKlass::cast(this);\n+      if (vk->has_nullable_atomic_layout()) {\n+        oop val = vk->allocate_instance(THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+            Handle e(THREAD, PENDING_EXCEPTION);\n+            CLEAR_PENDING_EXCEPTION;\n+            {\n+                EXCEPTION_MARK;\n+                add_initialization_error(THREAD, e);\n+                \/\/ Locks object, set state, and notify all waiting threads\n+                set_initialization_state_and_notify(initialization_error, THREAD);\n+                CLEAR_PENDING_EXCEPTION;\n+            }\n+            THROW_OOP(e());\n+        }\n+        vk->set_null_reset_value(val);\n+      }\n+  }\n+\n@@ -1355,1 +1516,0 @@\n-\n@@ -1376,0 +1536,30 @@\n+\n+    if (has_strict_static_fields() && !HAS_PENDING_EXCEPTION) {\n+      \/\/ Step 9 also verifies that strict static fields have been initialized.\n+      \/\/ Status bits were set in ClassFileParser::post_process_parsed_stream.\n+      \/\/ After <clinit>, bits must all be clear, or else we must throw an error.\n+      \/\/ This is an extremely fast check, so we won't bother with a timer.\n+      assert(fields_status() != nullptr, \"\");\n+      Symbol* bad_strict_static = nullptr;\n+      for (int index = 0; index < fields_status()->length(); index++) {\n+        \/\/ Very fast loop over single byte array looking for a set bit.\n+        if (fields_status()->adr_at(index)->is_strict_static_unset()) {\n+          \/\/ This strict static field has not been set by the class initializer.\n+          \/\/ Note that in the common no-error case, we read no field metadata.\n+          \/\/ We only unpack it when we need to report an error.\n+          FieldInfo fi = field(index);\n+          bad_strict_static = fi.name(constants());\n+          if (debug_logging_enabled) {\n+            ResourceMark rm(jt);\n+            const char* msg = format_strict_static_message(bad_strict_static);\n+            log_debug(class, init)(\"%s\", msg);\n+          } else {\n+            \/\/ If we are not logging, do not bother to look for a second offense.\n+            break;\n+          }\n+        }\n+      }\n+      if (bad_strict_static != nullptr) {\n+        throw_strict_static_exception(bad_strict_static, \"is unset after initialization of\", THREAD);\n+      }\n+    }\n@@ -1429,0 +1619,68 @@\n+void InstanceKlass::notify_strict_static_access(int field_index, bool is_writing, TRAPS) {\n+  guarantee(field_index >= 0 && field_index < fields_status()->length(), \"valid field index\");\n+  DEBUG_ONLY(FieldInfo debugfi = field(field_index));\n+  assert(debugfi.access_flags().is_strict(), \"\");\n+  assert(debugfi.access_flags().is_static(), \"\");\n+  FieldStatus& fs = *fields_status()->adr_at(field_index);\n+  LogTarget(Trace, class, init) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm(THREAD);\n+    LogStream ls(lt);\n+    FieldInfo fi = field(field_index);\n+    ls.print(\"notify %s %s %s%s \",\n+             external_name(), is_writing? \"Write\" : \"Read\",\n+             fs.is_strict_static_unset() ? \"Unset\" : \"(set)\",\n+             fs.is_strict_static_unread() ? \"+Unread\" : \"\");\n+    fi.print(&ls, constants());\n+  }\n+  if (fs.is_strict_static_unset()) {\n+    assert(fs.is_strict_static_unread(), \"ClassFileParser resp.\");\n+    \/\/ If it is not set, there are only two reasonable things we can do here:\n+    \/\/ - mark it set if this is putstatic\n+    \/\/ - throw an error (Read-Before-Write) if this is getstatic\n+\n+    \/\/ The unset state is (or should be) transient, and observable only in one\n+    \/\/ thread during the execution of <clinit>.  Something is wrong here as this\n+    \/\/ should not be possible\n+    guarantee(is_reentrant_initialization(THREAD), \"unscoped access to strict static\");\n+    if (is_writing) {\n+      \/\/ clear the \"unset\" bit, since the field is actually going to be written\n+      fs.update_strict_static_unset(false);\n+    } else {\n+      \/\/ throw an IllegalStateException, since we are reading before writing\n+      \/\/ see also InstanceKlass::initialize_impl, Step 8 (at end)\n+      Symbol* bad_strict_static = field(field_index).name(constants());\n+      throw_strict_static_exception(bad_strict_static, \"is unset before first read in\", CHECK);\n+    }\n+  } else {\n+    \/\/ Ensure no write after read for final strict statics\n+    FieldInfo fi = field(field_index);\n+    bool is_final = fi.access_flags().is_final();\n+    if (is_final) {\n+      \/\/ no final write after read, so observing a constant freezes it, as if <clinit> ended early\n+      \/\/ (maybe we could trust the constant a little earlier, before <clinit> ends)\n+      if (is_writing && !fs.is_strict_static_unread()) {\n+        Symbol* bad_strict_static = fi.name(constants());\n+        throw_strict_static_exception(bad_strict_static, \"is set after read (as final) in\", CHECK);\n+      } else if (!is_writing && fs.is_strict_static_unread()) {\n+        fs.update_strict_static_unread(false);\n+      }\n+    }\n+  }\n+}\n+\n+void InstanceKlass::throw_strict_static_exception(Symbol* field_name, const char* when, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  const char* msg = format_strict_static_message(field_name, when);\n+  THROW_MSG(vmSymbols::java_lang_IllegalStateException(), msg);\n+}\n+\n+const char* InstanceKlass::format_strict_static_message(Symbol* field_name, const char* when) {\n+  stringStream ss;\n+  ss.print(\"Strict static \\\"%s\\\" %s %s\",\n+           field_name->as_C_string(),\n+           when == nullptr ? \"is unset in\" : when,\n+           external_name());\n+  return ss.as_string();\n+}\n+\n@@ -1679,1 +1937,1 @@\n-  ObjArrayKlass* ak = array_klasses();\n+  ArrayKlass* ak = array_klasses();\n@@ -1686,2 +1944,2 @@\n-  ObjArrayKlass* oak = array_klasses_acquire();\n-  if (oak == nullptr) {\n+  ArrayKlass* ak = array_klasses_acquire();\n+  if (ak == nullptr) {\n@@ -1690,1 +1948,1 @@\n-    return oak->array_klass_or_null(n);\n+    return ak->array_klass_or_null(n);\n@@ -1707,1 +1965,1 @@\n-  if (clinit != nullptr && clinit->has_valid_initializer_flags()) {\n+  if (clinit != nullptr && clinit->is_class_initializer()) {\n@@ -1816,4 +2074,0 @@\n-bool InstanceKlass::contains_field_offset(int offset) {\n-  fieldDescriptor fd;\n-  return find_field_from_offset(offset, false, &fd);\n-}\n@@ -1900,0 +2154,9 @@\n+bool InstanceKlass::contains_field_offset(int offset) {\n+  if (this->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(this);\n+    return offset >= vk->payload_offset() && offset < (vk->payload_offset() + vk->payload_size_in_bytes());\n+  } else {\n+    fieldDescriptor fd;\n+    return find_field_from_offset(offset, false, &fd);\n+  }\n+}\n@@ -2283,0 +2546,3 @@\n+    if (name == vmSymbols::object_initializer_name()) {\n+      break;  \/\/ <init> is never inherited\n+    }\n@@ -2695,0 +2961,1 @@\n+  it->push(&_loadable_descriptors);\n@@ -2696,0 +2963,1 @@\n+  it->push(&_inline_layout_info_array, MetaspaceClosure::_writable);\n@@ -2743,1 +3011,1 @@\n-  \/\/ These are not allocated from metaspace. They are safe to set to null.\n+  \/\/ These are not allocated from metaspace. They are safe to set to nullptr.\n@@ -2834,0 +3102,4 @@\n+  if (is_inline_klass()) {\n+    InlineKlass::cast(this)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -2867,1 +3139,1 @@\n-    assert(this == array_klasses()->bottom_klass(), \"sanity\");\n+    assert(this == ObjArrayKlass::cast(array_klasses())->bottom_klass(), \"sanity\");\n@@ -2870,0 +3142,6 @@\n+    if (class_loader_data() == nullptr) {\n+      ResourceMark rm(THREAD);\n+      log_debug(cds)(\"  loader_data %s \", loader_data == nullptr ? \"nullptr\" : \"non null\");\n+      log_debug(cds)(\"  this %s array_klasses %s \", this->name()->as_C_string(), array_klasses()->name()->as_C_string());\n+    }\n+    assert(!array_klasses()->is_refined_objArray_klass(), \"must be non-refined objarrayklass\");\n@@ -3025,0 +3303,4 @@\n+bool InstanceKlass::supports_inline_types() const {\n+  return major_version() >= Verifier::VALUE_TYPES_MAJOR_VERSION && minor_version() == Verifier::JAVA_PREVIEW_MINOR_VERSION;\n+}\n+\n@@ -3057,0 +3339,2 @@\n+  return signature_name_of_carrier(JVM_SIGNATURE_CLASS);\n+}\n@@ -3058,0 +3342,1 @@\n+const char* InstanceKlass::signature_name_of_carrier(char c) const {\n@@ -3064,1 +3349,1 @@\n-  \/\/ Add L as type indicator\n+  \/\/ Add L or Q as type indicator\n@@ -3066,1 +3351,1 @@\n-  dest[dest_index++] = JVM_SIGNATURE_CLASS;\n+  dest[dest_index++] = c;\n@@ -3347,0 +3632,19 @@\n+void InstanceKlass::check_can_be_annotated_with_NullRestricted(InstanceKlass* type, Symbol* container_klass_name, TRAPS) {\n+  assert(type->is_instance_klass(), \"Sanity check\");\n+  if (type->is_identity_class()) {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+              err_msg(\"Class %s expects class %s to be a value class, but it is an identity class\",\n+              container_klass_name->as_C_string(),\n+              type->external_name()));\n+  }\n+\n+  if (type->is_abstract()) {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+              err_msg(\"Class %s expects class %s to be concrete value type, but it is an abstract class\",\n+              container_klass_name->as_C_string(),\n+              type->external_name()));\n+  }\n+}\n+\n@@ -3413,2 +3717,1 @@\n-  \/\/ Remember to strip ACC_SUPER bit\n-  return (access & (~JVM_ACC_SUPER));\n+  return access;\n@@ -3668,1 +3971,4 @@\n-static void print_vtable(intptr_t* start, int len, outputStream* st) {\n+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {\n+  ResourceMark rm;\n+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);\n+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;\n@@ -3672,0 +3978,5 @@\n+    if (forward_refs[i] != 0) {\n+      int from = forward_refs[i];\n+      int off = (int) start[from];\n+      st->print(\" (offset %d <= [%d])\", off, from);\n+    }\n@@ -3675,0 +3986,6 @@\n+    } else if (self != nullptr && e > 0 && e < 0x10000) {\n+      address location = self + e;\n+      int index = (int)((intptr_t*)location - start);\n+      st->print(\" (offset %d => [%d])\", (int)e, index);\n+      if (index >= 0 && index < len)\n+        forward_refs[index] = i;\n@@ -3681,1 +3998,22 @@\n-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);\n+  return print_vtable(nullptr, reinterpret_cast<intptr_t*>(start), len, st);\n+}\n+\n+template<typename T>\n+ static void print_array_on(outputStream* st, Array<T>* array) {\n+   if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+   array->print_value_on(st); st->cr();\n+   if (Verbose || WizardMode) {\n+     for (int i = 0; i < array->length(); i++) {\n+       st->print(\"%d : \", i); array->at(i)->print_value_on(st); st->cr();\n+     }\n+   }\n+ }\n+\n+static void print_array_on(outputStream* st, Array<int>* array) {\n+  if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+  array->print_value_on(st); st->cr();\n+  if (Verbose || WizardMode) {\n+    for (int i = 0; i < array->length(); i++) {\n+      st->print(\"%d : %d\", i, array->at(i)); st->cr();\n+    }\n+  }\n@@ -3722,8 +4060,2 @@\n-  st->print(BULLET\"methods:           \"); methods()->print_value_on(st);               st->cr();\n-  if (Verbose || WizardMode) {\n-    Array<Method*>* method_array = methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n-  st->print(BULLET\"method ordering:   \"); method_ordering()->print_value_on(st);      st->cr();\n+  st->print(BULLET\"methods:           \"); print_array_on(st, methods());\n+  st->print(BULLET\"method ordering:   \"); print_array_on(st, method_ordering());\n@@ -3731,7 +4063,1 @@\n-    st->print(BULLET\"default_methods:   \"); default_methods()->print_value_on(st);    st->cr();\n-    if (Verbose) {\n-      Array<Method*>* method_array = default_methods();\n-      for (int i = 0; i < method_array->length(); i++) {\n-        st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-      }\n-    }\n+    st->print(BULLET\"default_methods:   \"); print_array_on(st, default_methods());\n@@ -3797,0 +4123,1 @@\n+  st->print(BULLET\"loadable descriptors:     \"); loadable_descriptors()->print_value_on(st); st->cr();\n@@ -3807,1 +4134,1 @@\n-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);\n+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(nullptr, start_of_itable(), itable_length(), st);\n@@ -3839,0 +4166,1 @@\n+  for (int i = 0; i < _indent; i++) _st->print(\"  \");\n@@ -3841,1 +4169,1 @@\n-     fd->print_on(_st);\n+     fd->print_on(_st, _base_offset);\n@@ -3844,2 +4172,2 @@\n-     fd->print_on_for(_st, _obj);\n-     _st->cr();\n+     fd->print_on_for(_st, _obj, _indent, _base_offset);\n+     if (!fd->field_flags().is_flat()) _st->cr();\n@@ -3850,1 +4178,1 @@\n-void InstanceKlass::oop_print_on(oop obj, outputStream* st) {\n+void InstanceKlass::oop_print_on(oop obj, outputStream* st, int indent, int base_offset) {\n@@ -3866,1 +4194,1 @@\n-  FieldPrinter print_field(st, obj);\n+  FieldPrinter print_field(st, obj, indent, base_offset);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":371,"deletions":43,"binary":false,"changes":414,"status":"modified"},{"patch":"@@ -869,0 +869,6 @@\n+  if (tp->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to extract the field being accessed from\n+    \/\/ the address computation\n+    return tp->is_aryptr()->add_field_offset_and_offset(txoffset);\n+  }\n@@ -889,0 +895,6 @@\n+  if (p1->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to extract the field being accessed from\n+    \/\/ the address computation\n+    return p1->is_aryptr()->add_field_offset_and_offset(p2offset);\n+  }\n","filename":"src\/hotspot\/share\/opto\/addnode.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -850,0 +850,6 @@\n+  product(bool, UseArrayLoadStoreProfile, true,                             \\\n+          \"Take advantage of profiling at array load\/store\")                \\\n+                                                                            \\\n+  product(bool, UseACmpProfile, true,                                       \\\n+          \"Take advantage of profiling at acmp\")                            \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-  virtual Node *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node *match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n@@ -93,1 +93,0 @@\n-  static  const TypeTuple *osr_domain();\n@@ -669,1 +668,1 @@\n-class CallProjections : public StackObj {\n+class CallProjections {\n@@ -678,1 +677,19 @@\n-  Node* resproj;\n+  uint nb_resproj;\n+  Node* resproj[1]; \/\/ at least one projection\n+\n+  CallProjections(uint nbres) {\n+    fallthrough_proj      = nullptr;\n+    fallthrough_catchproj = nullptr;\n+    fallthrough_memproj   = nullptr;\n+    fallthrough_ioproj    = nullptr;\n+    catchall_catchproj    = nullptr;\n+    catchall_memproj      = nullptr;\n+    catchall_ioproj       = nullptr;\n+    exobj                 = nullptr;\n+    nb_resproj            = nbres;\n+    resproj[0]            = nullptr;\n+    for (uint i = 1; i < nb_resproj; i++) {\n+      resproj[i]          = nullptr;\n+    }\n+  }\n+\n@@ -700,1 +717,1 @@\n-    : SafePointNode(tf->domain()->cnt(), jvms, adr_type),\n+    : SafePointNode(tf->domain_cc()->cnt(), jvms, adr_type),\n@@ -727,1 +744,1 @@\n-  virtual Node*       match(const ProjNode* proj, const Matcher* m);\n+  virtual Node*       match(const ProjNode* proj, const Matcher* m, const RegMask* mask);\n@@ -742,0 +759,1 @@\n+  bool                has_debug_use(Node* n);\n@@ -748,2 +766,3 @@\n-    const TypeTuple* r = tf()->range();\n-    return (r->cnt() > TypeFunc::Parms &&\n+    const TypeTuple* r = tf()->range_sig();\n+    return (!tf()->returns_inline_type_as_fields() &&\n+            r->cnt() > TypeFunc::Parms &&\n@@ -756,1 +775,1 @@\n-  void extract_projections(CallProjections* projs, bool separate_io_proj, bool do_asserts = true) const;\n+  CallProjections* extract_projections(bool separate_io_proj, bool do_asserts = true) const;\n@@ -822,0 +841,3 @@\n+\n+  bool remove_unknown_flat_array_load(PhaseIterGVN* igvn, Node* ctl, Node* mem, Node* unc_arg);\n+\n@@ -830,0 +852,11 @@\n+    const TypeTuple *r = tf->range_sig();\n+    if (InlineTypeReturnedAsFields &&\n+        method != nullptr &&\n+        method->is_method_handle_intrinsic() &&\n+        r->cnt() > TypeFunc::Parms &&\n+        r->field_at(TypeFunc::Parms)->isa_oopptr() &&\n+        r->field_at(TypeFunc::Parms)->is_oopptr()->can_be_inline_type()) {\n+      \/\/ Make sure this call is processed by PhaseMacroExpand::expand_mh_intrinsic_return\n+      init_flags(Flag_is_macro);\n+      C->add_macro_node(this);\n+    }\n@@ -966,0 +999,1 @@\n+  virtual uint match_edge(uint idx) const;\n@@ -1008,0 +1042,3 @@\n+    InlineType,                       \/\/ InlineTypeNode if this is an inline type allocation\n+    InitValue,                        \/\/ Init value for null-free inline type arrays\n+    RawInitValue,                     \/\/ Same as above but as raw machine word\n@@ -1018,0 +1055,3 @@\n+    fields[InlineType] = Type::BOTTOM;\n+    fields[InitValue] = TypeInstPtr::NOTNULL;\n+    fields[RawInitValue] = TypeX_X;\n@@ -1035,0 +1075,1 @@\n+  bool _larval;\n@@ -1038,1 +1079,2 @@\n-               Node *size, Node *klass_node, Node *initial_test);\n+               Node *size, Node *klass_node, Node *initial_test,\n+               InlineTypeNode* inline_type_node = nullptr);\n@@ -1103,1 +1145,1 @@\n-  Node* make_ideal_mark(PhaseGVN *phase, Node* obj, Node* control, Node* mem);\n+  Node* make_ideal_mark(PhaseGVN* phase, Node* control, Node* mem);\n@@ -1115,1 +1157,2 @@\n-                    Node* initial_test, Node* count_val, Node* valid_length_test)\n+                    Node* initial_test, Node* count_val, Node* valid_length_test,\n+                    Node* init_value, Node* raw_init_value)\n@@ -1120,1 +1163,1 @@\n-    set_req(AllocateNode::ALength,        count_val);\n+    set_req(AllocateNode::ALength, count_val);\n@@ -1122,0 +1165,2 @@\n+    init_req(AllocateNode::InitValue, init_value);\n+    init_req(AllocateNode::RawInitValue, raw_init_value);\n@@ -1123,0 +1168,1 @@\n+  virtual uint size_of() const { return sizeof(*this); }\n","filename":"src\/hotspot\/share\/opto\/callnode.hpp","additions":59,"deletions":13,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -30,0 +30,2 @@\n+#include \"opto\/graphKit.hpp\"\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -33,0 +35,1 @@\n+#include \"opto\/rootnode.hpp\"\n@@ -110,1 +113,1 @@\n-Node* ConstraintCastNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+Node *ConstraintCastNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n@@ -114,0 +117,14 @@\n+\n+  \/\/ Push cast through InlineTypeNode\n+  InlineTypeNode* vt = in(1)->isa_InlineType();\n+  if (vt != nullptr && phase->type(vt)->filter_speculative(_type) != Type::TOP) {\n+    Node* cast = clone();\n+    cast->set_req(1, vt->get_oop());\n+    vt = vt->clone()->as_InlineType();\n+    if (!_type->maybe_null()) {\n+      vt->as_InlineType()->set_null_marker(*phase);\n+    }\n+    vt->set_oop(*phase, phase->transform(cast));\n+    return vt;\n+  }\n+\n@@ -117,0 +134,1 @@\n+\n@@ -414,0 +432,10 @@\n+\/\/=============================================================================\n+\/\/------------------------------Identity---------------------------------------\n+\/\/ If input is already higher or equal to cast type, then this is an identity.\n+Node* CheckCastPPNode::Identity(PhaseGVN* phase) {\n+  if (in(1)->is_InlineType() && _type->isa_instptr() && phase->type(in(1))->inline_klass()->is_subtype_of(_type->is_instptr()->instance_klass())) {\n+    return in(1);\n+  }\n+  return ConstraintCastNode::Identity(phase);\n+}\n+\n@@ -430,0 +458,10 @@\n+    \/\/ TODO 8302672\n+    if (!StressReflectiveCode && my_type->isa_aryptr() && in_type->isa_aryptr()) {\n+      \/\/ Propagate array properties (not flat\/null-free)\n+      \/\/ Don't do this when StressReflectiveCode is enabled because it might lead to\n+      \/\/ a dying data path while the corresponding flat\/null-free check is not folded.\n+      my_type = my_type->is_aryptr()->update_properties(in_type->is_aryptr());\n+      if (my_type == nullptr) {\n+        return Type::TOP; \/\/ Inconsistent properties\n+      }\n+    }\n@@ -434,1 +472,1 @@\n-      result =  my_type->cast_to_ptr_type(my_type->join_ptr(in_ptr));\n+      result = my_type->cast_to_ptr_type(my_type->join_ptr(in_ptr));\n","filename":"src\/hotspot\/share\/opto\/castnode.cpp","additions":40,"deletions":2,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -320,0 +320,1 @@\n+  virtual Node* Identity(PhaseGVN* phase);\n@@ -340,0 +341,11 @@\n+\/\/ Cast an integer to a narrow oop\n+class CastI2NNode : public TypeNode {\n+  public:\n+  CastI2NNode(Node* ctrl, Node* n, const Type* t) : TypeNode(t, 2) {\n+    init_req(0, ctrl);\n+    init_req(1, n);\n+  }\n+  virtual int Opcode() const;\n+  virtual uint ideal_reg() const { return Op_RegN; }\n+};\n+\n@@ -356,2 +368,0 @@\n-\n-\n","filename":"src\/hotspot\/share\/opto\/castnode.hpp","additions":12,"deletions":2,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -523,0 +524,1 @@\n+\n@@ -969,1 +971,2 @@\n-             cmp1->is_SubTypeCheck() || cmp2->is_SubTypeCheck()) {\n+             cmp1->is_SubTypeCheck() || cmp2->is_SubTypeCheck() ||\n+             cmp1->is_FlatArrayCheck() || cmp2->is_FlatArrayCheck()) {\n@@ -1048,1 +1051,1 @@\n-\/\/ note that these functions assume that the _adr_type field is flattened\n+\/\/ note that these functions assume that the _adr_type field is flat\n@@ -1066,1 +1069,1 @@\n-  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at), \"flatten at\");\n+  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at) || (flatten_phi_adr_type(at) == TypeAryPtr::INLINES && Compile::current()->flat_accesses_share_alias()), \"flatten at\");\n@@ -1195,0 +1198,8 @@\n+  \/\/ Flat array element shouldn't get their own memory slice until flat_accesses_share_alias is cleared.\n+  \/\/ It could be the graph has no loads\/stores and flat_accesses_share_alias is never cleared. EA could still\n+  \/\/ creates per element Phis but that wouldn't be a problem as there are no memory accesses for that array.\n+  assert(_adr_type == nullptr || _adr_type->isa_aryptr() == nullptr ||\n+         _adr_type->is_aryptr()->is_known_instance() ||\n+         !_adr_type->is_aryptr()->is_flat() ||\n+         !Compile::current()->flat_accesses_share_alias() ||\n+         _adr_type == TypeAryPtr::INLINES, \"flat array element shouldn't get its own slice yet\");\n@@ -1478,0 +1489,1 @@\n+\n@@ -1532,0 +1544,4 @@\n+  uin = unique_constant_input_recursive(phase);\n+  if (uin != nullptr) {\n+    return uin;\n+  }\n@@ -1631,0 +1647,36 @@\n+\/\/ Find the unique input, try to look recursively through input Phis\n+Node* PhiNode::unique_constant_input_recursive(PhaseGVN* phase) {\n+  if (!phase->is_IterGVN()) {\n+    return nullptr;\n+  }\n+\n+  ResourceMark rm;\n+  Node* unique = nullptr;\n+  Unique_Node_List visited;\n+  visited.push(this);\n+\n+  for (uint visited_idx = 0; visited_idx < visited.size(); visited_idx++) {\n+    Node* current = visited.at(visited_idx);\n+    for (uint i = 1; i < current->req(); i++) {\n+      Node* phi_in = current->in(i);\n+      if (phi_in == nullptr) {\n+        continue;\n+      }\n+\n+      if (phi_in->is_Phi()) {\n+        visited.push(phi_in);\n+      } else {\n+        if (unique == nullptr) {\n+          if (!phi_in->is_Con()) {\n+            return nullptr;\n+          }\n+          unique = phi_in;\n+        } else if (unique != phi_in) {\n+          return nullptr;\n+        }\n+      }\n+    }\n+  }\n+  return unique;\n+}\n+\n@@ -2098,0 +2150,52 @@\n+\/\/ Push inline type input nodes (and null) down through the phi recursively (can handle data loops).\n+InlineTypeNode* PhiNode::push_inline_types_down(PhaseGVN* phase, bool can_reshape, ciInlineKlass* inline_klass) {\n+  assert(inline_klass != nullptr, \"must be\");\n+  InlineTypeNode* vt = InlineTypeNode::make_null(*phase, inline_klass, \/* transform = *\/ false)->clone_with_phis(phase, in(0), nullptr, !_type->maybe_null(), true);\n+  if (can_reshape) {\n+    \/\/ Replace phi right away to be able to use the inline\n+    \/\/ type node when reaching the phi again through data loops.\n+    PhaseIterGVN* igvn = phase->is_IterGVN();\n+    for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+      Node* u = fast_out(i);\n+      igvn->rehash_node_delayed(u);\n+      imax -= u->replace_edge(this, vt);\n+      --i;\n+    }\n+    igvn->rehash_node_delayed(this);\n+    assert(outcnt() == 0, \"should be dead now\");\n+  }\n+  ResourceMark rm;\n+  Node_List casts;\n+  for (uint i = 1; i < req(); ++i) {\n+    Node* n = in(i);\n+    while (n->is_ConstraintCast()) {\n+      casts.push(n);\n+      n = n->in(1);\n+    }\n+    if (phase->type(n)->is_zero_type()) {\n+      n = InlineTypeNode::make_null(*phase, inline_klass);\n+    } else if (n->is_Phi()) {\n+      assert(can_reshape, \"can only handle phis during IGVN\");\n+      n = phase->transform(n->as_Phi()->push_inline_types_down(phase, can_reshape, inline_klass));\n+    }\n+    while (casts.size() != 0) {\n+      \/\/ Push the cast(s) through the InlineTypeNode\n+      \/\/ TODO 8302217 Can we avoid cloning? See InlineTypeNode::clone_if_required\n+      Node* cast = casts.pop()->clone();\n+      cast->set_req_X(1, n->as_InlineType()->get_oop(), phase);\n+      n = n->clone();\n+      n->as_InlineType()->set_oop(*phase, phase->transform(cast));\n+      n = phase->transform(n);\n+      if (n->is_top()) {\n+        break;\n+      }\n+    }\n+    bool transform = !can_reshape && (i == (req()-1)); \/\/ Transform phis on last merge\n+    assert(n->is_top() || n->is_InlineType(), \"Only InlineType or top at this point.\");\n+    if (n->is_InlineType()) {\n+      vt->merge_with(phase, n->as_InlineType(), i, transform);\n+    } \/\/ else nothing to do: phis above vt created by clone_with_phis are initialized to top already.\n+  }\n+  return vt;\n+}\n+\n@@ -2534,0 +2638,2 @@\n+    \/\/ TODO revisit this with JDK-8247216\n+    bool mergemem_only = true;\n@@ -2549,0 +2655,2 @@\n+      } else {\n+        mergemem_only = false;\n@@ -2572,1 +2680,1 @@\n-    if (!split_always_terminates && adr_type() == TypePtr::BOTTOM &&\n+    if (!mergemem_only && !split_always_terminates && adr_type() == TypePtr::BOTTOM &&\n@@ -2607,1 +2715,1 @@\n-      } else if (split_always_terminates) {\n+      } else if (mergemem_only || split_always_terminates) {\n@@ -2643,0 +2751,5 @@\n+            if (igvn) {\n+              \/\/ TODO revisit this with JDK-8247216\n+              \/\/ Put 'n' on the worklist because it might be modified by MergeMemStream::iteration_setup\n+              igvn->_worklist.push(n);\n+            }\n@@ -2685,1 +2798,1 @@\n-        set_req(i, new_in);\n+        set_req_X(i, new_in, phase->is_IterGVN());\n@@ -2753,0 +2866,5 @@\n+  Node* inline_type = try_push_inline_types_down(phase, can_reshape);\n+  if (inline_type != this) {\n+    return inline_type;\n+  }\n+\n@@ -2796,0 +2914,95 @@\n+\/\/ Check recursively if inputs are either an inline type, constant null\n+\/\/ or another Phi (including self references through data loops). If so,\n+\/\/ push the inline types down through the phis to enable folding of loads.\n+Node* PhiNode::try_push_inline_types_down(PhaseGVN* phase, const bool can_reshape) {\n+  if (!can_be_inline_type()) {\n+    return this;\n+  }\n+\n+  ciInlineKlass* inline_klass;\n+  if (can_push_inline_types_down(phase, can_reshape, inline_klass)) {\n+    assert(inline_klass != nullptr, \"must be\");\n+    return push_inline_types_down(phase, can_reshape, inline_klass);\n+  }\n+  return this;\n+}\n+\n+bool PhiNode::can_push_inline_types_down(PhaseGVN* phase, const bool can_reshape, ciInlineKlass*& inline_klass) {\n+  if (req() <= 2) {\n+    \/\/ Dead phi.\n+    return false;\n+  }\n+  inline_klass = nullptr;\n+\n+  \/\/ TODO 8302217 We need to prevent endless pushing through\n+  bool only_phi = (outcnt() != 0);\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    Node* n = fast_out(i);\n+    if (n->is_InlineType() && n->in(1) == this) {\n+      return false;\n+    }\n+    if (!n->is_Phi()) {\n+      only_phi = false;\n+    }\n+  }\n+  if (only_phi) {\n+    return false;\n+  }\n+\n+  ResourceMark rm;\n+  Unique_Node_List worklist;\n+  worklist.push(this);\n+  Node_List casts;\n+\n+  for (uint next = 0; next < worklist.size(); next++) {\n+    Node* phi = worklist.at(next);\n+    for (uint i = 1; i < phi->req(); i++) {\n+      Node* n = phi->in(i);\n+      if (n == nullptr) {\n+        return false;\n+      }\n+      while (n->is_ConstraintCast()) {\n+        if (n->in(0) != nullptr && n->in(0)->is_top()) {\n+          \/\/ Will die, don't optimize\n+          return false;\n+        }\n+        casts.push(n);\n+        n = n->in(1);\n+      }\n+      const Type* type = phase->type(n);\n+      if (n->is_InlineType() && (inline_klass == nullptr || inline_klass == type->inline_klass())) {\n+        inline_klass = type->inline_klass();\n+      } else if (n->is_Phi() && can_reshape && n->bottom_type()->isa_ptr()) {\n+        worklist.push(n);\n+      } else if (!type->is_zero_type()) {\n+        return false;\n+      }\n+    }\n+  }\n+  if (inline_klass == nullptr) {\n+    return false;\n+  }\n+\n+  \/\/ Check if cast nodes can be pushed through\n+  const Type* t = Type::get_const_type(inline_klass);\n+  while (casts.size() != 0 && t != nullptr) {\n+    Node* cast = casts.pop();\n+    if (t->filter(cast->bottom_type()) == Type::TOP) {\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+#ifdef ASSERT\n+bool PhiNode::can_push_inline_types_down(PhaseGVN* phase) {\n+  if (!can_be_inline_type()) {\n+    return false;\n+  }\n+\n+  ciInlineKlass* inline_klass;\n+  return can_push_inline_types_down(phase, true, inline_klass);\n+}\n+#endif \/\/ ASSERT\n+\n@@ -3180,0 +3393,6 @@\n+\n+  \/\/ CheckCastPPNode::Ideal() for inline types reuses the exception\n+  \/\/ paths of a call to perform an allocation: we can see a Phi here.\n+  if (in(1)->is_Phi()) {\n+    return this;\n+  }\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":225,"deletions":6,"binary":false,"changes":231,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -183,0 +184,3 @@\n+  bool can_push_inline_types_down(PhaseGVN* phase, bool can_reshape, ciInlineKlass*& inline_klass);\n+  InlineTypeNode* push_inline_types_down(PhaseGVN* phase, bool can_reshape, ciInlineKlass* inline_klass);\n+\n@@ -235,0 +239,1 @@\n+  Node* unique_constant_input_recursive(PhaseGVN* phase);\n@@ -261,0 +266,7 @@\n+  bool can_be_inline_type() const {\n+    return Arguments::is_valhalla_enabled() && _type->isa_instptr() && _type->is_instptr()->can_be_inline_type();\n+  }\n+\n+  Node* try_push_inline_types_down(PhaseGVN* phase, bool can_reshape);\n+  DEBUG_ONLY(bool can_push_inline_types_down(PhaseGVN* phase);)\n+\n@@ -472,0 +484,2 @@\n+  bool is_flat_array_check(PhaseTransform* phase, Node** array = nullptr);\n+\n@@ -760,0 +774,1 @@\n+    init_class_id(Class_Blackhole);\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1944,1 +1944,1 @@\n-          derived->bottom_type()->make_ptr()->is_ptr()->_offset == 0, \"sanity\");\n+         derived->bottom_type()->make_ptr()->is_ptr()->offset() == 0, \"sanity\");\n@@ -1947,1 +1947,1 @@\n-  if( tj == nullptr || tj->_offset == 0 ) {\n+  if (tj == nullptr || tj->offset() == 0) {\n@@ -2113,1 +2113,1 @@\n-                  derived->bottom_type()->make_ptr()->is_ptr()->_offset == 0, \"sanity\");\n+                 derived->bottom_type()->make_ptr()->is_ptr()->offset() == 0, \"sanity\");\n@@ -2115,1 +2115,1 @@\n-          if( tj && tj->_offset != 0 && tj->isa_oop_ptr() ) {\n+          if (tj && tj->offset() != 0 && tj->isa_oop_ptr()) {\n@@ -2405,1 +2405,1 @@\n-  const TypeTuple *domain = C->tf()->domain();\n+  const TypeTuple *domain = C->tf()->domain_cc();\n@@ -2614,1 +2614,1 @@\n-                  if (is_derived && check->bottom_type()->is_ptr()->_offset != 0) {\n+                  if (is_derived && check->bottom_type()->is_ptr()->offset() != 0) {\n@@ -2618,1 +2618,1 @@\n-                    assert(check->bottom_type()->is_ptr()->_offset == 0, \"Bad base pointer\");\n+                    assert(check->bottom_type()->is_ptr()->offset() == 0, \"Bad base pointer\");\n@@ -2627,1 +2627,1 @@\n-                } else if (check->bottom_type()->is_ptr()->_offset == 0) {\n+                } else if (check->bottom_type()->is_ptr()->offset() == 0) {\n","filename":"src\/hotspot\/share\/opto\/chaitin.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -62,0 +63,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -69,0 +71,1 @@\n+#include \"opto\/movenode.hpp\"\n@@ -70,0 +73,1 @@\n+#include \"opto\/multnode.hpp\"\n@@ -83,0 +87,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -408,0 +413,6 @@\n+  if (dead->is_InlineType()) {\n+    remove_inline_type(dead);\n+  }\n+  if (dead->is_LoadFlat() || dead->is_StoreFlat()) {\n+    remove_flat_access(dead);\n+  }\n@@ -455,0 +466,3 @@\n+    if (n->outcnt() == 0) {\n+      worklist.push(n);\n+    }\n@@ -463,0 +477,7 @@\n+  remove_useless_nodes(_inline_type_nodes,  useful); \/\/ remove useless inline type nodes\n+  remove_useless_nodes(_flat_access_nodes, useful);  \/\/ remove useless flat access nodes\n+#ifdef ASSERT\n+  if (_modified_nodes != nullptr) {\n+    _modified_nodes->remove_useless_nodes(useful.member_set());\n+  }\n+#endif\n@@ -651,0 +672,1 @@\n+      _has_circular_inline_type(false),\n@@ -669,0 +691,2 @@\n+      _inline_type_nodes (comp_arena(), 8, 0, nullptr),\n+      _flat_access_nodes(comp_arena(), 8, 0, nullptr),\n@@ -777,4 +801,2 @@\n-      const TypeTuple *domain = StartOSRNode::osr_domain();\n-      const TypeTuple *range = TypeTuple::make_range(method()->signature());\n-      init_tf(TypeFunc::make(domain, range));\n-      StartNode* s = new StartOSRNode(root(), domain);\n+      init_tf(TypeFunc::make(method(), \/* is_osr_compilation = *\/ true));\n+      StartNode* s = new StartOSRNode(root(), tf()->domain_sig());\n@@ -787,1 +809,1 @@\n-      StartNode* s = new StartNode(root(), tf()->domain());\n+      StartNode* s = new StartNode(root(), tf()->domain_cc());\n@@ -888,0 +910,10 @@\n+  if (needs_stack_repair()) {\n+    \/\/ One extra slot for the special stack increment value\n+    next_slot += 2;\n+  }\n+  \/\/ TODO 8284443 Only reserve extra slot if needed\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ One extra slot to hold the null marker for a nullable\n+    \/\/ inline type return if we run out of registers.\n+    next_slot += 2;\n+  }\n@@ -925,0 +957,1 @@\n+      _has_circular_inline_type(false),\n@@ -1079,0 +1112,4 @@\n+  _has_flat_accesses = false;\n+  _flat_accesses_share_alias = true;\n+  _scalarize_in_safepoints = false;\n+\n@@ -1359,4 +1396,0 @@\n-  if (ta && ta->is_stable()) {\n-    \/\/ Erase stability property for alias analysis.\n-    tj = ta = ta->cast_to_stable(false);\n-  }\n@@ -1373,48 +1406,23 @@\n-    \/\/ For arrays indexed by constant indices, we flatten the alias\n-    \/\/ space to include all of the array body.  Only the header, klass\n-    \/\/ and array length can be accessed un-aliased.\n-    if( offset != Type::OffsetBot ) {\n-      if( ta->const_oop() ) { \/\/ MethodData* or Method*\n-        offset = Type::OffsetBot;   \/\/ Flatten constant access into array body\n-        tj = ta = ta->\n-                remove_speculative()->\n-                cast_to_ptr_type(ptr)->\n-                cast_to_exactness(false)->\n-                with_offset(offset);\n-      } else if( offset == arrayOopDesc::length_offset_in_bytes() ) {\n-        \/\/ range is OK as-is.\n-        tj = ta = TypeAryPtr::RANGE;\n-      } else if( offset == oopDesc::klass_offset_in_bytes() ) {\n-        tj = TypeInstPtr::KLASS; \/\/ all klass loads look alike\n-        ta = TypeAryPtr::RANGE; \/\/ generic ignored junk\n-        ptr = TypePtr::BotPTR;\n-      } else if( offset == oopDesc::mark_offset_in_bytes() ) {\n-        tj = TypeInstPtr::MARK;\n-        ta = TypeAryPtr::RANGE; \/\/ generic ignored junk\n-        ptr = TypePtr::BotPTR;\n-      } else {                  \/\/ Random constant offset into array body\n-        offset = Type::OffsetBot;   \/\/ Flatten constant access into array body\n-        tj = ta = ta->\n-                remove_speculative()->\n-                cast_to_ptr_type(ptr)->\n-                cast_to_exactness(false)->\n-                with_offset(offset);\n-      }\n-    }\n-    \/\/ Arrays of fixed size alias with arrays of unknown size.\n-    if (ta->size() != TypeInt::POS) {\n-      const TypeAry *tary = TypeAry::make(ta->elem(), TypeInt::POS);\n-      tj = ta = ta->\n-              remove_speculative()->\n-              cast_to_ptr_type(ptr)->\n-              with_ary(tary)->\n-              cast_to_exactness(false);\n-    }\n-    \/\/ Arrays of known objects become arrays of unknown objects.\n-    if (ta->elem()->isa_narrowoop() && ta->elem() != TypeNarrowOop::BOTTOM) {\n-      const TypeAry *tary = TypeAry::make(TypeNarrowOop::BOTTOM, ta->size());\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n-    }\n-    if (ta->elem()->isa_oopptr() && ta->elem() != TypeInstPtr::BOTTOM) {\n-      const TypeAry *tary = TypeAry::make(TypeInstPtr::BOTTOM, ta->size());\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+    \/\/ Common slices\n+    if (offset == arrayOopDesc::length_offset_in_bytes()) {\n+      return TypeAryPtr::RANGE;\n+    } else if (offset == oopDesc::klass_offset_in_bytes()) {\n+      return TypeInstPtr::KLASS;\n+    } else if (offset == oopDesc::mark_offset_in_bytes()) {\n+      return TypeInstPtr::MARK;\n+    }\n+\n+    \/\/ Remove size and stability\n+    const TypeAry* normalized_ary = TypeAry::make(ta->elem(), TypeInt::POS, false, ta->is_flat(), ta->is_not_flat(), ta->is_not_null_free(), ta->is_atomic());\n+    \/\/ Remove ptr, const_oop, and offset\n+    if (ta->elem() == Type::BOTTOM) {\n+      \/\/ Bottom array (meet of int[] and byte[] for example), accesses to it will be done with\n+      \/\/ Unsafe. This should alias with all arrays. For now just leave it as it is (this is\n+      \/\/ incorrect, see JDK-8331133).\n+      tj = ta = TypeAryPtr::make(TypePtr::BotPTR, nullptr, normalized_ary, nullptr, false, Type::Offset::bottom);\n+    } else if (ta->elem()->make_oopptr() != nullptr) {\n+      \/\/ Object arrays, keep field_offset\n+      tj = ta = TypeAryPtr::make(TypePtr::BotPTR, nullptr, normalized_ary, nullptr, ta->klass_is_exact(), Type::Offset::bottom, Type::Offset(ta->field_offset()));\n+    } else {\n+      \/\/ Primitive arrays\n+      tj = ta = TypeAryPtr::make(TypePtr::BotPTR, nullptr, normalized_ary, ta->exact_klass(), true, Type::Offset::bottom);\n@@ -1422,0 +1430,1 @@\n+\n@@ -1425,13 +1434,17 @@\n-      const TypeAry *tary = TypeAry::make(TypeInt::BYTE, ta->size());\n-      ciKlass* aklass = ciTypeArrayKlass::make(T_BYTE);\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,offset);\n-    }\n-    \/\/ During the 2nd round of IterGVN, NotNull castings are removed.\n-    \/\/ Make sure the Bottom and NotNull variants alias the same.\n-    \/\/ Also, make sure exact and non-exact variants alias the same.\n-    if (ptr == TypePtr::NotNull || ta->klass_is_exact() || ta->speculative() != nullptr) {\n-      tj = ta = ta->\n-              remove_speculative()->\n-              cast_to_ptr_type(TypePtr::BotPTR)->\n-              cast_to_exactness(false)->\n-              with_offset(offset);\n+      tj = ta = TypeAryPtr::BYTES;\n+    }\n+\n+    \/\/ All arrays of references share the same slice\n+    if (!ta->is_flat() && ta->elem()->make_oopptr() != nullptr) {\n+      const TypeAry* tary = TypeAry::make(TypeInstPtr::BOTTOM, TypeInt::POS, false, false, true, true, true);\n+      tj = ta = TypeAryPtr::make(TypePtr::BotPTR, nullptr, tary, nullptr, false, Type::Offset::bottom);\n+    }\n+\n+    if (ta->is_flat()) {\n+      if (_flat_accesses_share_alias) {\n+        \/\/ Initially all flattened array accesses share a single slice\n+        tj = ta = TypeAryPtr::INLINES;\n+      } else {\n+        \/\/ Flat accesses are always exact\n+        tj = ta = ta->cast_to_exactness(true);\n+      }\n@@ -1445,0 +1458,1 @@\n+    tj = to = to->cast_to_maybe_flat_in_array(); \/\/ flatten to maybe flat in array\n@@ -1477,1 +1491,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, Type::Offset(offset));\n@@ -1498,1 +1512,2 @@\n-        assert(tj == TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, offset, instance_id), \"exact type should be canonical type\");\n+        assert(tj == TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, Type::Offset(offset), instance_id,\n+                                       TypePtr::MaybeFlat), \"exact type should be canonical type\");\n@@ -1501,1 +1516,2 @@\n-        tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, offset, instance_id);\n+        tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, Type::Offset(offset), instance_id,\n+                                    TypePtr::MaybeFlat);\n@@ -1516,1 +1532,2 @@\n-                                       offset);\n+                                       Type::Offset(offset),\n+                                       TypePtr::MaybeFlat);\n@@ -1522,1 +1539,1 @@\n-        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), offset);\n+        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), Type::Offset(offset), TypePtr::MaybeFlat);\n@@ -1524,1 +1541,1 @@\n-        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, offset);\n+        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, Type::Offset(offset), tk->is_not_flat(), tk->is_not_null_free(), tk->is_flat(), tk->is_null_free(), tk->is_atomic(), tk->is_aryklassptr()->is_refined_type());\n@@ -1527,1 +1544,0 @@\n-\n@@ -1657,1 +1673,1 @@\n-Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {\n+Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {\n@@ -1662,3 +1678,6 @@\n-  AliasCacheEntry* ace = probe_alias_cache(adr_type);\n-  if (ace->_adr_type == adr_type) {\n-    return alias_type(ace->_index);\n+  AliasCacheEntry* ace = nullptr;\n+  if (!uncached) {\n+    ace = probe_alias_cache(adr_type);\n+    if (ace->_adr_type == adr_type) {\n+      return alias_type(ace->_index);\n+    }\n@@ -1714,0 +1733,1 @@\n+    ciField* field = nullptr;\n@@ -1720,0 +1740,1 @@\n+      const Type* elemtype = flat->is_aryptr()->elem();\n@@ -1721,1 +1742,8 @@\n-        alias_type(idx)->set_element(flat->is_aryptr()->elem());\n+        alias_type(idx)->set_element(elemtype);\n+      }\n+      int field_offset = flat->is_aryptr()->field_offset().get();\n+      if (flat->is_flat() &&\n+          field_offset != Type::OffsetBot) {\n+        ciInlineKlass* vk = elemtype->inline_klass();\n+        field_offset += vk->payload_offset();\n+        field = vk->get_field_by_offset(field_offset, false);\n@@ -1735,0 +1763,2 @@\n+      if (flat->offset() == in_bytes(Klass::layout_helper_offset()))\n+        alias_type(idx)->set_rewritable(false);\n@@ -1751,1 +1781,0 @@\n-      ciField* field;\n@@ -1758,0 +1787,4 @@\n+      } else if (tinst->is_inlinetypeptr()) {\n+        \/\/ Inline type field\n+        ciInlineKlass* vk = tinst->inline_klass();\n+        field = vk->get_field_by_offset(tinst->offset(), false);\n@@ -1762,7 +1795,14 @@\n-      assert(field == nullptr ||\n-             original_field == nullptr ||\n-             (field->holder() == original_field->holder() &&\n-              field->offset_in_bytes() == original_field->offset_in_bytes() &&\n-              field->is_static() == original_field->is_static()), \"wrong field?\");\n-      \/\/ Set field() and is_rewritable() attributes.\n-      if (field != nullptr)  alias_type(idx)->set_field(field);\n+    }\n+    assert(field == nullptr ||\n+           original_field == nullptr ||\n+           (field->holder() == original_field->holder() &&\n+            field->offset_in_bytes() == original_field->offset_in_bytes() &&\n+            field->is_static() == original_field->is_static()), \"wrong field?\");\n+    \/\/ Set field() and is_rewritable() attributes.\n+    if (field != nullptr) {\n+      alias_type(idx)->set_field(field);\n+      if (flat->isa_aryptr()) {\n+        \/\/ Fields of flat arrays are rewritable although they are declared final\n+        assert(flat->is_flat(), \"must be a flat array\");\n+        alias_type(idx)->set_rewritable(true);\n+      }\n@@ -1773,3 +1813,4 @@\n-  ace->_adr_type = adr_type;\n-  ace->_index    = idx;\n-  assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n+  if (!uncached) {\n+    ace->_adr_type = adr_type;\n+    ace->_index    = idx;\n+    assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n@@ -1777,6 +1818,7 @@\n-  \/\/ Might as well try to fill the cache for the flattened version, too.\n-  AliasCacheEntry* face = probe_alias_cache(flat);\n-  if (face->_adr_type == nullptr) {\n-    face->_adr_type = flat;\n-    face->_index    = idx;\n-    assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    \/\/ Might as well try to fill the cache for the flattened version, too.\n+    AliasCacheEntry* face = probe_alias_cache(flat);\n+    if (face->_adr_type == nullptr) {\n+      face->_adr_type = flat;\n+      face->_index    = idx;\n+      assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    }\n@@ -1904,0 +1946,487 @@\n+void Compile::add_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  _inline_type_nodes.push(n);\n+}\n+\n+void Compile::remove_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  if (_inline_type_nodes.contains(n)) {\n+    _inline_type_nodes.remove(n);\n+  }\n+}\n+\n+\/\/ Does the return value keep otherwise useless inline type allocations alive?\n+static bool return_val_keeps_allocations_alive(Node* ret_val) {\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(ret_val);\n+  bool some_allocations = false;\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->outcnt() > 1) {\n+      \/\/ Some other use for the allocation\n+      return false;\n+    } else if (n->is_InlineType()) {\n+      wq.push(n->in(1));\n+    } else if (n->is_Phi()) {\n+      for (uint j = 1; j < n->req(); j++) {\n+        wq.push(n->in(j));\n+      }\n+    } else if (n->is_CheckCastPP() &&\n+               n->in(1)->is_Proj() &&\n+               n->in(1)->in(0)->is_Allocate()) {\n+      some_allocations = true;\n+    } else if (n->is_CheckCastPP()) {\n+      wq.push(n->in(1));\n+    }\n+  }\n+  return some_allocations;\n+}\n+\n+void Compile::process_inline_types(PhaseIterGVN &igvn, bool remove) {\n+  \/\/ Make sure that the return value does not keep an otherwise unused allocation alive\n+  if (tf()->returns_inline_type_as_fields()) {\n+    Node* ret = nullptr;\n+    for (uint i = 1; i < root()->req(); i++) {\n+      Node* in = root()->in(i);\n+      if (in->Opcode() == Op_Return) {\n+        assert(ret == nullptr, \"only one return\");\n+        ret = in;\n+      }\n+    }\n+    if (ret != nullptr) {\n+      Node* ret_val = ret->in(TypeFunc::Parms);\n+      if (igvn.type(ret_val)->isa_oopptr() &&\n+          return_val_keeps_allocations_alive(ret_val)) {\n+        igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)->inline_klass(), igvn));\n+        assert(ret_val->outcnt() == 0, \"should be dead now\");\n+        igvn.remove_dead_node(ret_val);\n+      }\n+    }\n+  }\n+  if (_inline_type_nodes.length() == 0) {\n+    \/\/ keep the graph canonical\n+    igvn.optimize();\n+    return;\n+  }\n+  \/\/ Scalarize inline types in safepoint debug info.\n+  \/\/ Delay this until all inlining is over to avoid getting inconsistent debug info.\n+  set_scalarize_in_safepoints(true);\n+  for (int i = _inline_type_nodes.length()-1; i >= 0; i--) {\n+    InlineTypeNode* vt = _inline_type_nodes.at(i)->as_InlineType();\n+    vt->make_scalar_in_safepoints(&igvn);\n+    igvn.record_for_igvn(vt);\n+  }\n+  if (remove) {\n+    \/\/ Remove inline type nodes by replacing them with their oop input\n+    while (_inline_type_nodes.length() > 0) {\n+      InlineTypeNode* vt = _inline_type_nodes.pop()->as_InlineType();\n+      if (vt->outcnt() == 0) {\n+        igvn.remove_dead_node(vt);\n+        continue;\n+      }\n+      for (DUIterator i = vt->outs(); vt->has_out(i); i++) {\n+        DEBUG_ONLY(bool must_be_buffered = false);\n+        Node* u = vt->out(i);\n+        \/\/ Check if any users are blackholes. If so, rewrite them to use either the\n+        \/\/ allocated buffer, or individual components, instead of the inline type node\n+        \/\/ that goes away.\n+        if (u->is_Blackhole()) {\n+          BlackholeNode* bh = u->as_Blackhole();\n+\n+          \/\/ Unlink the old input\n+          int idx = bh->find_edge(vt);\n+          assert(idx != -1, \"The edge should be there\");\n+          bh->del_req(idx);\n+          --i;\n+\n+          if (vt->is_allocated(&igvn)) {\n+            \/\/ Already has the allocated instance, blackhole that\n+            bh->add_req(vt->get_oop());\n+          } else {\n+            \/\/ Not allocated yet, blackhole the components\n+            for (uint c = 0; c < vt->field_count(); c++) {\n+              bh->add_req(vt->field_value(c));\n+            }\n+          }\n+\n+          \/\/ Node modified, record for IGVN\n+          igvn.record_for_igvn(bh);\n+        }\n+#ifdef ASSERT\n+        \/\/ Verify that inline type is buffered when replacing by oop\n+        else if (u->is_InlineType()) {\n+          \/\/ InlineType uses don't need buffering because they are about to be replaced as well\n+        } else if (u->is_Phi()) {\n+          \/\/ TODO 8302217 Remove this once InlineTypeNodes are reliably pushed through\n+        } else {\n+          must_be_buffered = true;\n+        }\n+        if (must_be_buffered && !vt->is_allocated(&igvn)) {\n+          vt->dump(0);\n+          u->dump(0);\n+          assert(false, \"Should have been buffered\");\n+        }\n+#endif\n+      }\n+      igvn.replace_node(vt, vt->get_oop());\n+    }\n+  }\n+  igvn.optimize();\n+}\n+\n+void Compile::add_flat_access(Node* n) {\n+  assert(n != nullptr && (n->Opcode() == Op_LoadFlat || n->Opcode() == Op_StoreFlat), \"unexpected node %s\", n == nullptr ? \"nullptr\" : n->Name());\n+  assert(!_flat_access_nodes.contains(n), \"duplicate insertion\");\n+  _flat_access_nodes.push(n);\n+}\n+\n+void Compile::remove_flat_access(Node* n) {\n+  assert(n != nullptr && (n->Opcode() == Op_LoadFlat || n->Opcode() == Op_StoreFlat), \"unexpected node %s\", n == nullptr ? \"nullptr\" : n->Name());\n+  _flat_access_nodes.remove_if_existing(n);\n+}\n+\n+void Compile::process_flat_accesses(PhaseIterGVN& igvn) {\n+  assert(igvn._worklist.size() == 0, \"should be empty\");\n+  igvn.set_delay_transform(true);\n+  for (int i = _flat_access_nodes.length() - 1; i >= 0; i--) {\n+    Node* n = _flat_access_nodes.at(i);\n+    assert(n != nullptr, \"unexpected nullptr\");\n+    if (n->is_LoadFlat()) {\n+      n->as_LoadFlat()->expand_atomic(igvn);\n+    } else {\n+      n->as_StoreFlat()->expand_atomic(igvn);\n+    }\n+  }\n+  _flat_access_nodes.clear_and_deallocate();\n+  igvn.set_delay_transform(false);\n+  igvn.optimize();\n+}\n+\n+void Compile::adjust_flat_array_access_aliases(PhaseIterGVN& igvn) {\n+  DEBUG_ONLY(igvn.verify_empty_worklist(nullptr));\n+  if (!_has_flat_accesses) {\n+    return;\n+  }\n+  \/\/ Initially, all flat array accesses share the same slice to\n+  \/\/ keep dependencies with Object[] array accesses (that could be\n+  \/\/ to a flat array) correct. We're done with parsing so we\n+  \/\/ now know all flat array accesses in this compile\n+  \/\/ unit. Let's move flat array accesses to their own slice,\n+  \/\/ one per element field. This should help memory access\n+  \/\/ optimizations.\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(root());\n+\n+  Node_List mergememnodes;\n+  Node_List memnodes;\n+\n+  \/\/ Alias index currently shared by all flat memory accesses\n+  int index = get_alias_index(TypeAryPtr::INLINES);\n+\n+  \/\/ Find MergeMem nodes and flat array accesses\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->is_Mem()) {\n+      const TypePtr* adr_type = nullptr;\n+      adr_type = get_adr_type(get_alias_index(n->adr_type()));\n+      if (adr_type == TypeAryPtr::INLINES) {\n+        memnodes.push(n);\n+      }\n+    } else if (n->is_MergeMem()) {\n+      MergeMemNode* mm = n->as_MergeMem();\n+      if (mm->memory_at(index) != mm->base_memory()) {\n+        mergememnodes.push(n);\n+      }\n+    }\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* m = n->in(j);\n+      if (m != nullptr) {\n+        wq.push(m);\n+      }\n+    }\n+  }\n+\n+  if (memnodes.size() > 0) {\n+    _flat_accesses_share_alias = false;\n+\n+    \/\/ We are going to change the slice for the flat array\n+    \/\/ accesses so we need to clear the cache entries that refer to\n+    \/\/ them.\n+    for (uint i = 0; i < AliasCacheSize; i++) {\n+      AliasCacheEntry* ace = &_alias_cache[i];\n+      if (ace->_adr_type != nullptr &&\n+          ace->_adr_type->is_flat()) {\n+        ace->_adr_type = nullptr;\n+        ace->_index = (i != 0) ? 0 : AliasIdxTop; \/\/ Make sure the nullptr adr_type resolves to AliasIdxTop\n+      }\n+    }\n+\n+    \/\/ Find what aliases we are going to add\n+    int start_alias = num_alias_types()-1;\n+    int stop_alias = 0;\n+\n+    for (uint i = 0; i < memnodes.size(); i++) {\n+      Node* m = memnodes.at(i);\n+      const TypePtr* adr_type = nullptr;\n+      adr_type = m->adr_type();\n+#ifdef ASSERT\n+      m->as_Mem()->set_adr_type(adr_type);\n+#endif\n+      int idx = get_alias_index(adr_type);\n+      start_alias = MIN2(start_alias, idx);\n+      stop_alias = MAX2(stop_alias, idx);\n+    }\n+\n+    assert(stop_alias >= start_alias, \"should have expanded aliases\");\n+\n+    Node_Stack stack(0);\n+#ifdef ASSERT\n+    VectorSet seen(Thread::current()->resource_area());\n+#endif\n+    \/\/ Now let's fix the memory graph so each flat array access\n+    \/\/ is moved to the right slice. Start from the MergeMem nodes.\n+    uint last = unique();\n+    for (uint i = 0; i < mergememnodes.size(); i++) {\n+      MergeMemNode* current = mergememnodes.at(i)->as_MergeMem();\n+      if (current->outcnt() == 0) {\n+        \/\/ This node is killed by a previous iteration\n+        continue;\n+      }\n+\n+      Node* n = current->memory_at(index);\n+      MergeMemNode* mm = nullptr;\n+      do {\n+        \/\/ Follow memory edges through memory accesses, phis and\n+        \/\/ narrow membars and push nodes on the stack. Once we hit\n+        \/\/ bottom memory, we pop element off the stack one at a\n+        \/\/ time, in reverse order, and move them to the right slice\n+        \/\/ by changing their memory edges.\n+        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() ||\n+            (n->adr_type() == TypeAryPtr::INLINES && !n->is_NarrowMemProj())) {\n+          assert(!seen.test_set(n->_idx), \"\");\n+          \/\/ Uses (a load for instance) will need to be moved to the\n+          \/\/ right slice as well and will get a new memory state\n+          \/\/ that we don't know yet. The use could also be the\n+          \/\/ backedge of a loop. We put a place holder node between\n+          \/\/ the memory node and its uses. We replace that place\n+          \/\/ holder with the correct memory state once we know it,\n+          \/\/ i.e. when nodes are popped off the stack. Using the\n+          \/\/ place holder make the logic work in the presence of\n+          \/\/ loops.\n+          if (n->outcnt() > 1) {\n+            Node* place_holder = nullptr;\n+            assert(!n->has_out_with(Op_Node), \"\");\n+            for (DUIterator k = n->outs(); n->has_out(k); k++) {\n+              Node* u = n->out(k);\n+              if (u != current && u->_idx < last) {\n+                bool success = false;\n+                for (uint l = 0; l < u->req(); l++) {\n+                  if (!stack.is_empty() && u == stack.node() && l == stack.index()) {\n+                    continue;\n+                  }\n+                  Node* in = u->in(l);\n+                  if (in == n) {\n+                    if (place_holder == nullptr) {\n+                      place_holder = new Node(1);\n+                      place_holder->init_req(0, n);\n+                    }\n+                    igvn.replace_input_of(u, l, place_holder);\n+                    success = true;\n+                  }\n+                }\n+                if (success) {\n+                  --k;\n+                }\n+              }\n+            }\n+          }\n+          if (n->is_Phi()) {\n+            stack.push(n, 1);\n+            n = n->in(1);\n+          } else if (n->is_Mem()) {\n+            stack.push(n, n->req());\n+            n = n->in(MemNode::Memory);\n+          } else {\n+            assert(n->is_Proj() && n->in(0)->Opcode() == Op_MemBarCPUOrder, \"\");\n+            stack.push(n, n->req());\n+            n = n->in(0)->in(TypeFunc::Memory);\n+          }\n+        } else {\n+          assert(n->adr_type() == TypePtr::BOTTOM || (n->Opcode() == Op_Node && n->_idx >= last) || n->is_NarrowMemProj(), \"\");\n+          \/\/ Build a new MergeMem node to carry the new memory state\n+          \/\/ as we build it. IGVN should fold extraneous MergeMem\n+          \/\/ nodes.\n+          if (n->is_NarrowMemProj()) {\n+            \/\/ We need 1 NarrowMemProj for each slice of this array\n+            InitializeNode* init = n->in(0)->as_Initialize();\n+            AllocateNode* alloc = init->allocation();\n+            Node* klass_node = alloc->in(AllocateNode::KlassNode);\n+            const TypeAryKlassPtr* klass_type = klass_node->bottom_type()->isa_aryklassptr();\n+            assert(klass_type != nullptr, \"must be an array\");\n+            assert(klass_type->klass_is_exact(), \"must be an exact klass\");\n+            ciArrayKlass* klass = klass_type->exact_klass()->as_array_klass();\n+            assert(klass->is_flat_array_klass(), \"must be a flat array\");\n+            ciInlineKlass* elem_klass = klass->element_klass()->as_inline_klass();\n+            const TypeAryPtr* oop_type = klass_type->as_instance_type()->is_aryptr();\n+            assert(oop_type->klass_is_exact(), \"must be an exact klass\");\n+\n+            Node* base = alloc->in(TypeFunc::Memory);\n+            assert(base->bottom_type() == Type::MEMORY, \"the memory input of AllocateNode must be a memory\");\n+            assert(base->adr_type() == TypePtr::BOTTOM, \"the memory input of AllocateNode must be a bottom memory\");\n+            \/\/ Must create a MergeMem with base as the base memory, do not clone if base is a\n+            \/\/ MergeMem because it may not be processed yet\n+            mm = MergeMemNode::make(nullptr);\n+            mm->set_base_memory(base);\n+            for (int j = 0; j < elem_klass->nof_nonstatic_fields(); j++) {\n+              int field_offset = elem_klass->nonstatic_field_at(j)->offset_in_bytes() - elem_klass->payload_offset();\n+              const TypeAryPtr* field_ptr = oop_type->with_offset(Type::OffsetBot)->with_field_offset(field_offset);\n+              int field_alias_idx = get_alias_index(field_ptr);\n+              assert(field_ptr == get_adr_type(field_alias_idx), \"must match\");\n+              Node* new_proj = new NarrowMemProjNode(init, field_ptr);\n+              igvn.register_new_node_with_optimizer(new_proj);\n+              mm->set_memory_at(field_alias_idx, new_proj);\n+            }\n+            if (!klass->is_elem_null_free()) {\n+              int nm_offset = elem_klass->null_marker_offset_in_payload();\n+              const TypeAryPtr* nm_ptr = oop_type->with_offset(Type::OffsetBot)->with_field_offset(nm_offset);\n+              int nm_alias_idx = get_alias_index(nm_ptr);\n+              assert(nm_ptr == get_adr_type(nm_alias_idx), \"must match\");\n+              Node* new_proj = new NarrowMemProjNode(init, nm_ptr);\n+              igvn.register_new_node_with_optimizer(new_proj);\n+              mm->set_memory_at(nm_alias_idx, new_proj);\n+            }\n+\n+            \/\/ Replace all uses of the old NarrowMemProj with the correct state\n+            MergeMemNode* new_n = MergeMemNode::make(mm);\n+            igvn.register_new_node_with_optimizer(new_n);\n+            igvn.replace_node(n, new_n);\n+          } else {\n+            \/\/ Must create a MergeMem with n as the base memory, do not clone if n is a MergeMem\n+            \/\/ because it may not be processed yet\n+            mm = MergeMemNode::make(nullptr);\n+            mm->set_base_memory(n);\n+          }\n+\n+          igvn.register_new_node_with_optimizer(mm);\n+          while (stack.size() > 0) {\n+            Node* m = stack.node();\n+            uint idx = stack.index();\n+            if (m->is_Mem()) {\n+              \/\/ Move memory node to its new slice\n+              const TypePtr* adr_type = m->adr_type();\n+              int alias = get_alias_index(adr_type);\n+              Node* prev = mm->memory_at(alias);\n+              igvn.replace_input_of(m, MemNode::Memory, prev);\n+              mm->set_memory_at(alias, m);\n+            } else if (m->is_Phi()) {\n+              \/\/ We need as many new phis as there are new aliases\n+              Node* new_phi_in = MergeMemNode::make(mm);\n+              igvn.register_new_node_with_optimizer(new_phi_in);\n+              igvn.replace_input_of(m, idx, new_phi_in);\n+              if (idx == m->req()-1) {\n+                Node* r = m->in(0);\n+                for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                  const TypePtr* adr_type = get_adr_type(j);\n+                  if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                    continue;\n+                  }\n+                  Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));\n+                  igvn.register_new_node_with_optimizer(phi);\n+                  for (uint k = 1; k < m->req(); k++) {\n+                    phi->init_req(k, m->in(k)->as_MergeMem()->memory_at(j));\n+                  }\n+                  mm->set_memory_at(j, phi);\n+                }\n+                Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+                igvn.register_new_node_with_optimizer(base_phi);\n+                for (uint k = 1; k < m->req(); k++) {\n+                  base_phi->init_req(k, m->in(k)->as_MergeMem()->base_memory());\n+                }\n+                mm->set_base_memory(base_phi);\n+              }\n+            } else {\n+              \/\/ This is a MemBarCPUOrder node from\n+              \/\/ Parse::array_load()\/Parse::array_store(), in the\n+              \/\/ branch that handles flat arrays hidden under\n+              \/\/ an Object[] array. We also need one new membar per\n+              \/\/ new alias to keep the unknown access that the\n+              \/\/ membars protect properly ordered with accesses to\n+              \/\/ known flat array.\n+              assert(m->is_Proj(), \"projection expected\");\n+              Node* ctrl = m->in(0)->in(TypeFunc::Control);\n+              igvn.replace_input_of(m->in(0), TypeFunc::Control, top());\n+              for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                const TypePtr* adr_type = get_adr_type(j);\n+                if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                  continue;\n+                }\n+                MemBarNode* mb = new MemBarCPUOrderNode(this, j, nullptr);\n+                igvn.register_new_node_with_optimizer(mb);\n+                Node* mem = mm->memory_at(j);\n+                mb->init_req(TypeFunc::Control, ctrl);\n+                mb->init_req(TypeFunc::Memory, mem);\n+                ctrl = new ProjNode(mb, TypeFunc::Control);\n+                igvn.register_new_node_with_optimizer(ctrl);\n+                mem = new ProjNode(mb, TypeFunc::Memory);\n+                igvn.register_new_node_with_optimizer(mem);\n+                mm->set_memory_at(j, mem);\n+              }\n+              igvn.replace_node(m->in(0)->as_Multi()->proj_out(TypeFunc::Control), ctrl);\n+            }\n+            if (idx < m->req()-1) {\n+              idx += 1;\n+              stack.set_index(idx);\n+              n = m->in(idx);\n+              break;\n+            }\n+            \/\/ Take care of place holder nodes\n+            if (m->has_out_with(Op_Node)) {\n+              Node* place_holder = m->find_out_with(Op_Node);\n+              if (place_holder != nullptr) {\n+                Node* mm_clone = mm->clone();\n+                igvn.register_new_node_with_optimizer(mm_clone);\n+                Node* hook = new Node(1);\n+                hook->init_req(0, mm);\n+                igvn.replace_node(place_holder, mm_clone);\n+                hook->destruct(&igvn);\n+              }\n+              assert(!m->has_out_with(Op_Node), \"place holder should be gone now\");\n+            }\n+            stack.pop();\n+          }\n+        }\n+      } while(stack.size() > 0);\n+      \/\/ Fix the memory state at the MergeMem we started from\n+      igvn.rehash_node_delayed(current);\n+      for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+        const TypePtr* adr_type = get_adr_type(j);\n+        if (!adr_type->isa_aryptr() || !adr_type->is_flat()) {\n+          continue;\n+        }\n+        current->set_memory_at(j, mm);\n+      }\n+      current->set_memory_at(index, current->base_memory());\n+    }\n+    igvn.optimize();\n+  }\n+  print_method(PHASE_SPLIT_INLINES_ARRAY, 2);\n+#ifdef ASSERT\n+  if (!_flat_accesses_share_alias) {\n+    wq.clear();\n+    wq.push(root());\n+    for (uint i = 0; i < wq.size(); i++) {\n+      Node* n = wq.at(i);\n+      assert(n->adr_type() != TypeAryPtr::INLINES, \"should have been removed from the graph\");\n+      for (uint j = 0; j < n->req(); j++) {\n+        Node* m = n->in(j);\n+        if (m != nullptr) {\n+          wq.push(m);\n+        }\n+      }\n+    }\n+  }\n+#endif\n+}\n+\n@@ -2022,1 +2551,1 @@\n-        if (!live_locals.at(i) && !local->is_top() && local != lhs && local!= rhs) {\n+        if (!live_locals.at(i) && !local->is_top() && local != lhs && local != rhs) {\n@@ -2037,1 +2566,1 @@\n-    \/\/ keep the mondified trap for late query\n+    \/\/ keep the modified trap for late query\n@@ -2249,1 +2778,4 @@\n-  assert(_modified_nodes == nullptr, \"not allowed\");\n+#ifdef ASSERT\n+  Unique_Node_List* modified_nodes = _modified_nodes;\n+  _modified_nodes = nullptr;\n+#endif\n@@ -2262,0 +2794,1 @@\n+  DEBUG_ONLY( _modified_nodes = modified_nodes; )\n@@ -2405,0 +2938,5 @@\n+  \/\/ Process inline type nodes now that all inlining is over\n+  process_inline_types(igvn);\n+\n+  adjust_flat_array_access_aliases(igvn);\n+\n@@ -2407,0 +2945,11 @@\n+  if (C->macro_count() > 0) {\n+    \/\/ Eliminate some macro nodes before EA to reduce analysis pressure\n+    PhaseMacroExpand mexp(igvn);\n+    mexp.eliminate_macro_nodes(\/* eliminate_locks= *\/ false);\n+    if (failing()) {\n+      return;\n+    }\n+    igvn.set_delay_transform(false);\n+    print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);\n+  }\n+\n@@ -2417,1 +2966,14 @@\n-      if (failing())  return;\n+      if (failing()) {\n+        return;\n+      }\n+      print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);\n+      if (C->macro_count() > 0) {\n+        \/\/ Eliminate some macro nodes before EA to reduce analysis pressure\n+        PhaseMacroExpand mexp(igvn);\n+        mexp.eliminate_macro_nodes(\/* eliminate_locks= *\/ false);\n+        if (failing()) {\n+          return;\n+        }\n+        igvn.set_delay_transform(false);\n+        print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);\n+      }\n@@ -2419,0 +2981,1 @@\n+\n@@ -2420,1 +2983,0 @@\n-    print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);\n@@ -2438,1 +3000,3 @@\n-        if (failing()) return;\n+        if (failing()) {\n+          return;\n+        }\n@@ -2442,3 +3006,0 @@\n-        igvn.optimize();\n-        if (failing()) return;\n-\n@@ -2459,0 +3020,5 @@\n+  process_flat_accesses(igvn);\n+  if (failing()) {\n+    return;\n+  }\n+\n@@ -2535,0 +3101,8 @@\n+  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n+\n+  if (_late_inlines.length() > 0) {\n+    \/\/ More opportunities to optimize virtual and MH calls.\n+    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n+    process_late_inline_calls_no_inline(igvn);\n+  }\n+\n@@ -2537,0 +3111,7 @@\n+    PhaseMacroExpand mex(igvn);\n+    \/\/ Last attempt to eliminate macro nodes.\n+    mex.eliminate_macro_nodes();\n+    if (failing()) {\n+      return;\n+    }\n+\n@@ -2538,1 +3119,0 @@\n-    PhaseMacroExpand  mex(igvn);\n@@ -2558,0 +3138,4 @@\n+  \/\/ Process inline type nodes again and remove them. From here\n+  \/\/ on we don't need to keep track of field values anymore.\n+  process_inline_types(igvn, \/* remove= *\/ true);\n+\n@@ -2574,0 +3158,1 @@\n+  DEBUG_ONLY( _late_inlines.clear(); )\n@@ -2576,9 +3161,0 @@\n-\n-  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n-\n-  if (_late_inlines.length() > 0) {\n-    \/\/ More opportunities to optimize virtual and MH calls.\n-    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n-    process_late_inline_calls_no_inline(igvn);\n-    if (failing())  return;\n-  }\n@@ -3330,1 +3906,1 @@\n-      for (unsigned int i = TypeFunc::Parms; i < call->tf()->domain()->cnt(); i++) {\n+      for (unsigned int i = TypeFunc::Parms; i < call->tf()->domain_sig()->cnt(); i++) {\n@@ -3376,0 +3952,1 @@\n+  case Op_StoreLSpecial:\n@@ -3925,0 +4502,5 @@\n+  case Op_InlineType: {\n+    n->dump(-1);\n+    assert(false, \"inline type node was not removed\");\n+    break;\n+  }\n@@ -4300,2 +4882,2 @@\n-      if (accessing_method->is_static_initializer() ||\n-          accessing_method->is_object_initializer() ||\n+      if (accessing_method->is_class_initializer() ||\n+          accessing_method->is_object_constructor() ||\n@@ -4309,1 +4891,1 @@\n-      if (accessing_method->is_static_initializer()) {\n+      if (accessing_method->is_class_initializer()) {\n@@ -4379,0 +4961,1 @@\n+               (n->is_Allocate() && i >= AllocateNode::InlineType) ||\n@@ -4381,1 +4964,1 @@\n-              \"only region, phi, arraycopy, unlock or membar nodes have null data edges\");\n+              \"only region, phi, arraycopy, allocate, unlock or membar nodes have null data edges\");\n@@ -4532,0 +5115,7 @@\n+\n+    \/\/ Do not fold the subtype check to an array klass pointer comparison for null-able inline type arrays\n+    \/\/ because null-free [LMyValue <: null-able [LMyValue but the klasses are different. Perform a full test.\n+    if (!superk->is_aryklassptr()->is_null_free() && superk->is_aryklassptr()->elem()->isa_instklassptr() &&\n+        superk->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->is_inlinetype()) {\n+      return SSC_full_test;\n+    }\n@@ -5370,0 +5960,2 @@\n+  } else if (bt == T_FLOAT) {\n+    result = new MoveI2FNode(value);\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":715,"deletions":123,"binary":false,"changes":838,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"utilities\/growableArray.hpp\"\n@@ -56,0 +57,1 @@\n+class CallNode;\n@@ -99,0 +101,1 @@\n+class InlineTypeNode;\n@@ -344,0 +347,1 @@\n+  bool                  _has_circular_inline_type; \/\/ True if method loads an inline type with a circular, non-flat field\n@@ -370,0 +374,3 @@\n+  bool                  _has_flat_accesses;     \/\/ Any known flat array accesses?\n+  bool                  _flat_accesses_share_alias; \/\/ Initially all flat array share a single slice\n+  bool                  _scalarize_in_safepoints; \/\/ Scalarize inline types in safepoint debug info\n@@ -388,0 +395,2 @@\n+  GrowableArray<Node*>  _inline_type_nodes;     \/\/ List of InlineType nodes\n+  GrowableArray<Node*>  _flat_access_nodes;     \/\/ List of LoadFlat and StoreFlat nodes\n@@ -623,0 +632,2 @@\n+  bool              has_circular_inline_type() const { return _has_circular_inline_type; }\n+  void          set_has_circular_inline_type(bool z) { _has_circular_inline_type = z; }\n@@ -655,0 +666,10 @@\n+  void          set_flat_accesses()              { _has_flat_accesses = true; }\n+  bool          flat_accesses_share_alias() const { return _flat_accesses_share_alias; }\n+  void          set_flat_accesses_share_alias(bool z) { _flat_accesses_share_alias = z; }\n+  bool          scalarize_in_safepoints() const { return _scalarize_in_safepoints; }\n+  void          set_scalarize_in_safepoints(bool z) { _scalarize_in_safepoints = z; }\n+\n+  \/\/ Support for scalarized inline type calling convention\n+  bool              has_scalarized_args() const  { return _method != nullptr && _method->has_scalarized_args(); }\n+  bool              needs_stack_repair()  const  { return _method != nullptr && _method->get_Method()->c2_needs_stack_repair(); }\n+\n@@ -786,0 +807,18 @@\n+  \/\/ Keep track of inline type nodes for later processing\n+  void add_inline_type(Node* n);\n+  void remove_inline_type(Node* n);\n+  void process_inline_types(PhaseIterGVN &igvn, bool remove = false);\n+\n+  void add_flat_access(Node* n);\n+  void remove_flat_access(Node* n);\n+  void process_flat_accesses(PhaseIterGVN& igvn);\n+\n+  template <class F>\n+  void for_each_flat_access(F consumer) {\n+    for (int i = _flat_access_nodes.length() - 1; i >= 0; i--) {\n+      consumer(_flat_access_nodes.at(i));\n+    }\n+  }\n+\n+  void adjust_flat_array_access_aliases(PhaseIterGVN& igvn);\n+\n@@ -968,1 +1007,1 @@\n-  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = nullptr) { return find_alias_type(adr_type, false, field); }\n+  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = nullptr, bool uncached = false) { return find_alias_type(adr_type, false, field, uncached); }\n@@ -972,1 +1011,1 @@\n-  int               get_alias_index(const TypePtr* at)  { return alias_type(at)->index(); }\n+  int               get_alias_index(const TypePtr* at, bool uncached = false) { return alias_type(at, nullptr, uncached)->index(); }\n@@ -1215,1 +1254,1 @@\n-  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field);\n+  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field, bool uncached = false);\n@@ -1305,1 +1344,1 @@\n-  \/\/ Auxiliary methods for randomized fuzzing\/stressing\n+  \/\/ Auxiliary method for randomized fuzzing\/stressing\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":43,"deletions":4,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -1618,1 +1618,1 @@\n-      tf()->range(),\n+      tf()->range_cc(),\n@@ -1681,1 +1681,1 @@\n-Node *DivModINode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *DivModINode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -1696,1 +1696,1 @@\n-Node *DivModLNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *DivModLNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -1734,1 +1734,1 @@\n-Node* UDivModINode::match( const ProjNode *proj, const Matcher *match ) {\n+Node* UDivModINode::match(const ProjNode* proj, const Matcher* match, const RegMask* mask) {\n@@ -1749,1 +1749,1 @@\n-Node* UDivModLNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node* UDivModLNode::match( const ProjNode* proj, const Matcher* match, const RegMask* mask) {\n","filename":"src\/hotspot\/share\/opto\/divnode.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"classfile\/vmIntrinsics.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"jvm_io.h\"\n@@ -40,0 +42,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -89,0 +92,57 @@\n+static bool arg_can_be_larval(ciMethod* callee, int arg_idx) {\n+  if (callee->is_object_constructor() && arg_idx == 0) {\n+    return true;\n+  }\n+\n+  if (arg_idx != 1 || callee->intrinsic_id() == vmIntrinsicID::_none) {\n+    return false;\n+  }\n+\n+  switch (callee->intrinsic_id()) {\n+    case vmIntrinsicID::_finishPrivateBuffer:\n+    case vmIntrinsicID::_putBoolean:\n+    case vmIntrinsicID::_putBooleanOpaque:\n+    case vmIntrinsicID::_putBooleanRelease:\n+    case vmIntrinsicID::_putBooleanVolatile:\n+    case vmIntrinsicID::_putByte:\n+    case vmIntrinsicID::_putByteOpaque:\n+    case vmIntrinsicID::_putByteRelease:\n+    case vmIntrinsicID::_putByteVolatile:\n+    case vmIntrinsicID::_putChar:\n+    case vmIntrinsicID::_putCharOpaque:\n+    case vmIntrinsicID::_putCharRelease:\n+    case vmIntrinsicID::_putCharUnaligned:\n+    case vmIntrinsicID::_putCharVolatile:\n+    case vmIntrinsicID::_putShort:\n+    case vmIntrinsicID::_putShortOpaque:\n+    case vmIntrinsicID::_putShortRelease:\n+    case vmIntrinsicID::_putShortUnaligned:\n+    case vmIntrinsicID::_putShortVolatile:\n+    case vmIntrinsicID::_putInt:\n+    case vmIntrinsicID::_putIntOpaque:\n+    case vmIntrinsicID::_putIntRelease:\n+    case vmIntrinsicID::_putIntUnaligned:\n+    case vmIntrinsicID::_putIntVolatile:\n+    case vmIntrinsicID::_putLong:\n+    case vmIntrinsicID::_putLongOpaque:\n+    case vmIntrinsicID::_putLongRelease:\n+    case vmIntrinsicID::_putLongUnaligned:\n+    case vmIntrinsicID::_putLongVolatile:\n+    case vmIntrinsicID::_putFloat:\n+    case vmIntrinsicID::_putFloatOpaque:\n+    case vmIntrinsicID::_putFloatRelease:\n+    case vmIntrinsicID::_putFloatVolatile:\n+    case vmIntrinsicID::_putDouble:\n+    case vmIntrinsicID::_putDoubleOpaque:\n+    case vmIntrinsicID::_putDoubleRelease:\n+    case vmIntrinsicID::_putDoubleVolatile:\n+    case vmIntrinsicID::_putReference:\n+    case vmIntrinsicID::_putReferenceOpaque:\n+    case vmIntrinsicID::_putReferenceRelease:\n+    case vmIntrinsicID::_putReferenceVolatile:\n+      return true;\n+    default:\n+      return false;\n+  }\n+}\n+\n@@ -149,1 +209,15 @@\n-  if (allow_inline && allow_intrinsics) {\n+  if (callee->intrinsic_id() == vmIntrinsics::_makePrivateBuffer || callee->intrinsic_id() == vmIntrinsics::_finishPrivateBuffer) {\n+    \/\/ These methods must be inlined so that we don't have larval value objects crossing method\n+    \/\/ boundaries\n+    assert(!call_does_dispatch, \"callee should not be virtual %s\", callee->name()->as_utf8());\n+    CallGenerator* cg = find_intrinsic(callee, call_does_dispatch);\n+\n+    if (cg == nullptr) {\n+      \/\/ This is probably because the intrinsics is disabled from the command line\n+      char reason[256];\n+      jio_snprintf(reason, sizeof(reason), \"cannot find an intrinsics for %s\", callee->name()->as_utf8());\n+      C->record_method_not_compilable(reason);\n+      return nullptr;\n+    }\n+    return cg;\n+  } else if (allow_inline && allow_intrinsics) {\n@@ -617,1 +691,1 @@\n-  if (iter().cur_bc_raw() == Bytecodes::_invokespecial && !orig_callee->is_object_initializer()) {\n+  if (iter().cur_bc_raw() == Bytecodes::_invokespecial && !orig_callee->is_object_constructor()) {\n@@ -645,0 +719,9 @@\n+  \/\/ Scalarize value objects passed into this invocation if we know that they are not larval\n+  for (int arg_idx = 0; arg_idx < nargs; arg_idx++) {\n+    if (arg_can_be_larval(callee, arg_idx)) {\n+      continue;\n+    }\n+\n+    cast_to_non_larval(peek(nargs - 1 - arg_idx));\n+  }\n+\n@@ -659,0 +742,4 @@\n+  if (failing()) {\n+    return;\n+  }\n+  assert(cg != nullptr, \"must find a CallGenerator for callee %s\", callee->name()->as_utf8());\n@@ -745,1 +832,1 @@\n-          \/\/ It's OK for a method  to return a value that is discarded.\n+          \/\/ It's OK for a method to return a value that is discarded.\n@@ -803,0 +890,21 @@\n+\n+    if (!rtype->is_void() && cg->method()->intrinsic_id() != vmIntrinsicID::_makePrivateBuffer) {\n+      Node* retnode = peek();\n+      const Type* rettype = gvn().type(retnode);\n+      if (rettype->is_inlinetypeptr() && !retnode->is_InlineType()) {\n+        retnode = InlineTypeNode::make_from_oop(this, retnode, rettype->inline_klass());\n+        dec_sp(1);\n+        push(retnode);\n+      }\n+    }\n+\n+    if (cg->method()->is_object_constructor() && receiver != nullptr && gvn().type(receiver)->is_inlinetypeptr()) {\n+      InlineTypeNode* non_larval = InlineTypeNode::make_from_oop(this, receiver, gvn().type(receiver)->inline_klass());\n+      \/\/ Relinquish the oop input, we will delay the allocation to the point it is needed, see the\n+      \/\/ comments in InlineTypeNode::Ideal for more details\n+      non_larval = non_larval->clone_if_required(&gvn(), nullptr);\n+      non_larval->set_oop(gvn(), null());\n+      non_larval->set_is_buffered(gvn(), false);\n+      non_larval = gvn().transform(non_larval)->as_InlineType();\n+      map()->replace_edge(receiver, non_larval);\n+    }\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":111,"deletions":3,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -34,0 +36,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -39,0 +42,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -42,0 +46,2 @@\n+#include \"opto\/multnode.hpp\"\n+#include \"opto\/narrowptrnode.hpp\"\n@@ -47,0 +53,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -49,0 +56,1 @@\n+#include \"runtime\/stubRoutines.hpp\"\n@@ -55,1 +63,1 @@\n-GraphKit::GraphKit(JVMState* jvms)\n+GraphKit::GraphKit(JVMState* jvms, PhaseGVN* gvn)\n@@ -58,1 +66,1 @@\n-    _gvn(*C->initial_gvn()),\n+    _gvn((gvn != nullptr) ? *gvn : *C->initial_gvn()),\n@@ -61,0 +69,1 @@\n+  assert(gvn == nullptr || !gvn->is_IterGVN() || gvn->is_IterGVN()->delay_transform(), \"delay transform should be enabled\");\n@@ -64,0 +73,7 @@\n+#ifdef ASSERT\n+  if (_gvn.is_IterGVN() != nullptr) {\n+    assert(_gvn.is_IterGVN()->delay_transform(), \"Transformation must be delayed if IterGVN is used\");\n+    \/\/ Save the initial size of _for_igvn worklist for verification (see ~GraphKit)\n+    _worklist_size = _gvn.C->igvn_worklist()->size();\n+  }\n+#endif\n@@ -348,1 +364,2 @@\n-  assert(ex_jvms->sp() == phi_map->_jvms->sp(), \"matching stack sizes\");\n+  \/\/ TODO 8325632 Re-enable\n+  \/\/ assert(ex_jvms->sp() == phi_map->_jvms->sp(), \"matching stack sizes\");\n@@ -876,1 +893,1 @@\n-           (is_anewarray && code == Bytecodes::_multianewarray);\n+           (is_anewarray && (code == Bytecodes::_multianewarray));\n@@ -944,1 +961,2 @@\n-    assert(out_jvms->sp() >= (uint)inputs, \"not enough operands for reexecution\");\n+    \/\/ TODO 8371125\n+    \/\/ assert(out_jvms->sp() >= (uint)inputs, \"not enough operands for reexecution\");\n@@ -967,0 +985,2 @@\n+\n+  JVMState* callee_jvms = nullptr;\n@@ -992,2 +1012,3 @@\n-      for (j = 0; j < l; j++)\n-        call->set_req(p++, in_map->in(k+j));\n+      for (j = 0; j < l; j++) {\n+        call->set_req(p++, in_map->in(k + j));\n+      }\n@@ -1003,2 +1024,3 @@\n-      for (j = 0; j < l; j++)\n-        call->set_req(p++, in_map->in(k+j));\n+      for (j = 0; j < l; j++) {\n+        call->set_req(p++, in_map->in(k + j));\n+      }\n@@ -1043,0 +1065,1 @@\n+    callee_jvms = out_jvms;\n@@ -1218,1 +1241,1 @@\n-  return _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+  return _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n@@ -1267,1 +1290,2 @@\n-                                  bool speculative) {\n+                                  bool speculative,\n+                                  bool null_marker_check) {\n@@ -1272,0 +1296,23 @@\n+  if (value->is_InlineType()) {\n+    \/\/ Null checking a scalarized but nullable inline type. Check the null marker\n+    \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+    InlineTypeNode* vtptr = value->as_InlineType();\n+    while (vtptr->get_oop()->is_InlineType()) {\n+      vtptr = vtptr->get_oop()->as_InlineType();\n+    }\n+    null_check_common(vtptr->get_null_marker(), T_INT, assert_null, null_control, speculative, true);\n+    if (stopped()) {\n+      return top();\n+    }\n+    if (assert_null) {\n+      \/\/ TODO 8284443 Scalarize here (this currently leads to compilation bailouts)\n+      \/\/ vtptr = InlineTypeNode::make_null(_gvn, vtptr->type()->inline_klass());\n+      \/\/ replace_in_map(value, vtptr);\n+      \/\/ return vtptr;\n+      replace_in_map(value, null());\n+      return null();\n+    }\n+    bool do_replace_in_map = (null_control == nullptr || (*null_control) == top());\n+    return cast_not_null(value, do_replace_in_map);\n+  }\n+\n@@ -1375,1 +1422,1 @@\n-  } else if (type == T_OBJECT) {\n+  } else if (type == T_OBJECT || null_marker_check) {\n@@ -1449,1 +1496,0 @@\n-\n@@ -1453,0 +1499,9 @@\n+  if (obj->is_InlineType()) {\n+    Node* vt = obj->isa_InlineType()->clone_if_required(&gvn(), map(), do_replace_in_map);\n+    vt->as_InlineType()->set_null_marker(_gvn);\n+    vt = _gvn.transform(vt);\n+    if (do_replace_in_map) {\n+      replace_in_map(obj, vt);\n+    }\n+    return vt;\n+  }\n@@ -1469,0 +1524,11 @@\n+Node* GraphKit::cast_to_non_larval(Node* obj) {\n+  const Type* obj_type = gvn().type(obj);\n+  if (obj->is_InlineType() || !obj_type->is_inlinetypeptr()) {\n+    return obj;\n+  }\n+\n+  Node* new_obj = InlineTypeNode::make_from_oop(this, obj, obj_type->inline_klass());\n+  replace_in_map(obj, new_obj);\n+  return new_obj;\n+}\n+\n@@ -1581,0 +1647,1 @@\n+\n@@ -1634,1 +1701,3 @@\n-                                DecoratorSet decorators) {\n+                                DecoratorSet decorators,\n+                                bool safe_for_replace,\n+                                const InlineTypeNode* vt) {\n@@ -1647,0 +1716,7 @@\n+  if (val->is_InlineType()) {\n+    \/\/ Store to non-flat field. Buffer the inline type and make sure\n+    \/\/ the store is re-executed if the allocation triggers deoptimization.\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    val = val->as_InlineType()->buffer(this, safe_for_replace);\n+  }\n@@ -1650,1 +1726,1 @@\n-  C2ParseAccess access(this, decorators | C2_WRITE_ACCESS, bt, obj, addr);\n+  C2ParseAccess access(this, decorators | C2_WRITE_ACCESS, bt, obj, addr, nullptr, vt);\n@@ -1663,1 +1739,2 @@\n-                               DecoratorSet decorators) {\n+                               DecoratorSet decorators,\n+                               Node* ctl) {\n@@ -1669,1 +1746,1 @@\n-  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr);\n+  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr, ctl);\n@@ -1774,2 +1851,15 @@\n-  uint shift  = exact_log2(type2aelembytes(elembt));\n-  uint header = arrayOopDesc::base_offset_in_bytes(elembt);\n+  const TypeAryPtr* arytype = _gvn.type(ary)->is_aryptr();\n+  uint shift;\n+  uint header;\n+  if (arytype->is_flat() && arytype->klass_is_exact()) {\n+    \/\/ We can only determine the flat array layout statically if the klass is exact. Otherwise, we could have different\n+    \/\/ value classes at runtime with a potentially different layout. The caller needs to fall back to call\n+    \/\/ load\/store_unknown_inline_Type() at runtime. We could return a sentinel node for the non-exact case but that\n+    \/\/ might mess with other GVN transformations in between. Thus, we just continue in the else branch normally, even\n+    \/\/ though we don't need the address node in this case and throw it away again.\n+    shift = arytype->flat_log_elem_size();\n+    header = arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT);\n+  } else {\n+    shift = exact_log2(type2aelembytes(elembt));\n+    header = arrayOopDesc::base_offset_in_bytes(elembt);\n+  }\n@@ -1791,0 +1881,33 @@\n+Node* GraphKit::cast_to_flat_array(Node* array, ciInlineKlass* elem_vk) {\n+  assert(elem_vk->maybe_flat_in_array(), \"no flat array for %s\", elem_vk->name()->as_utf8());\n+  if (!elem_vk->has_atomic_layout() && !elem_vk->has_nullable_atomic_layout()) {\n+    return cast_to_flat_array_exact(array, elem_vk, true, false);\n+  } else if (!elem_vk->has_nullable_atomic_layout() && !elem_vk->has_non_atomic_layout()) {\n+    return cast_to_flat_array_exact(array, elem_vk, true, true);\n+  } else if (!elem_vk->has_atomic_layout() && !elem_vk->has_non_atomic_layout()) {\n+    return cast_to_flat_array_exact(array, elem_vk, false, true);\n+  }\n+\n+  bool is_null_free = false;\n+  if (!elem_vk->has_nullable_atomic_layout()) {\n+    \/\/ Element does not have a nullable flat layout, cannot be nullable\n+    is_null_free = true;\n+  }\n+\n+  ciArrayKlass* array_klass = ciObjArrayKlass::make(elem_vk, false);\n+  const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();\n+  arytype = arytype->cast_to_flat(true)->cast_to_null_free(is_null_free);\n+  return _gvn.transform(new CheckCastPPNode(control(), array, arytype, ConstraintCastNode::DependencyType::NonFloatingNarrowing));\n+}\n+\n+Node* GraphKit::cast_to_flat_array_exact(Node* array, ciInlineKlass* elem_vk, bool is_null_free, bool is_atomic) {\n+  assert(is_null_free || is_atomic, \"nullable arrays must be atomic\");\n+  ciArrayKlass* array_klass = ciObjArrayKlass::make(elem_vk, true, is_null_free, is_atomic);\n+  const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();\n+  assert(arytype->klass_is_exact(), \"inconsistency\");\n+  assert(arytype->is_flat(), \"inconsistency\");\n+  assert(arytype->is_null_free() == is_null_free, \"inconsistency\");\n+  assert(arytype->is_not_null_free() == !is_null_free, \"inconsistency\");\n+  return _gvn.transform(new CheckCastPPNode(control(), array, arytype, ConstraintCastNode::DependencyType::NonFloatingNarrowing));\n+}\n+\n@@ -1806,6 +1929,42 @@\n-void GraphKit::set_arguments_for_java_call(CallJavaNode* call) {\n-  \/\/ Add the call arguments:\n-  uint nargs = call->method()->arg_size();\n-  for (uint i = 0; i < nargs; i++) {\n-    Node* arg = argument(i);\n-    call->init_req(i + TypeFunc::Parms, arg);\n+void GraphKit::set_arguments_for_java_call(CallJavaNode* call, bool is_late_inline) {\n+  PreserveReexecuteState preexecs(this);\n+  if (Arguments::is_valhalla_enabled()) {\n+    \/\/ Make sure the call is \"re-executed\", if buffering of inline type arguments triggers deoptimization.\n+    \/\/ At this point, the call hasn't been executed yet, so we will only ever execute the call once.\n+    jvms()->set_should_reexecute(true);\n+    int arg_size = method()->get_declared_signature_at_bci(bci())->arg_size_for_bc(java_bc());\n+    inc_sp(arg_size);\n+  }\n+  \/\/ Add the call arguments\n+  const TypeTuple* domain = call->tf()->domain_sig();\n+  uint nargs = domain->cnt();\n+  int arg_num = 0;\n+  for (uint i = TypeFunc::Parms, idx = TypeFunc::Parms; i < nargs; i++) {\n+    Node* arg = argument(i-TypeFunc::Parms);\n+    const Type* t = domain->field_at(i);\n+    \/\/ TODO 8284443 A static call to a mismatched method should still be scalarized\n+    if (t->is_inlinetypeptr() && !call->method()->get_Method()->mismatch() && call->method()->is_scalarized_arg(arg_num)) {\n+      \/\/ We don't pass inline type arguments by reference but instead pass each field of the inline type\n+      if (!arg->is_InlineType()) {\n+        assert(_gvn.type(arg)->is_zero_type() && !t->inline_klass()->is_null_free(), \"Unexpected argument type\");\n+        arg = InlineTypeNode::make_from_oop(this, arg, t->inline_klass());\n+      }\n+      InlineTypeNode* vt = arg->as_InlineType();\n+      vt->pass_fields(this, call, idx, true, !t->maybe_null());\n+      \/\/ If an inline type argument is passed as fields, attach the Method* to the call site\n+      \/\/ to be able to access the extended signature later via attached_method_before_pc().\n+      \/\/ For example, see CompiledMethod::preserve_callee_argument_oops().\n+      call->set_override_symbolic_info(true);\n+      \/\/ Register an calling convention dependency on the callee method to make sure that this method is deoptimized and\n+      \/\/ re-compiled with a non-scalarized calling convention if the callee method is later marked as mismatched.\n+      C->dependencies()->assert_mismatch_calling_convention(call->method());\n+      arg_num++;\n+      continue;\n+    } else if (arg->is_InlineType()) {\n+      \/\/ Pass inline type argument via oop to callee\n+      arg = arg->as_InlineType()->buffer(this, true);\n+    }\n+    if (t != Type::HALF) {\n+      arg_num++;\n+    }\n+    call->init_req(idx++, arg);\n@@ -1849,7 +2008,0 @@\n-  \/\/ Capture the return value, if any.\n-  Node* ret;\n-  if (call->method() == nullptr ||\n-      call->method()->return_type()->basic_type() == T_VOID)\n-        ret = top();\n-  else  ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n-\n@@ -1868,0 +2020,72 @@\n+\n+  \/\/ Capture the return value, if any.\n+  Node* ret;\n+  if (call->method() == nullptr || call->method()->return_type()->basic_type() == T_VOID) {\n+    ret = top();\n+  } else if (call->tf()->returns_inline_type_as_fields()) {\n+    \/\/ Return of multiple values (inline type fields): we create a\n+    \/\/ InlineType node, each field is a projection from the call.\n+    ciInlineKlass* vk = call->method()->return_type()->as_inline_klass();\n+    uint base_input = TypeFunc::Parms;\n+    ret = InlineTypeNode::make_from_multi(this, call, vk, base_input, false, false);\n+  } else {\n+    ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+    ciType* t = call->method()->return_type();\n+    if (!t->is_loaded() && InlineTypeReturnedAsFields) {\n+      \/\/ The return type is unloaded but the callee might later be C2 compiled and then return\n+      \/\/ in scalarized form when the return type is loaded. Handle this similar to what we do in\n+      \/\/ PhaseMacroExpand::expand_mh_intrinsic_return by calling into the runtime to buffer.\n+      \/\/ It's a bit unfortunate because we will deopt anyway but the interpreter needs an oop.\n+      IdealKit ideal(this);\n+      IdealVariable res(ideal);\n+      ideal.declarations_done();\n+      ideal.if_then(ret, BoolTest::eq, ideal.makecon(TypePtr::NULL_PTR)); {\n+        \/\/ Return value is null\n+        ideal.set(res, makecon(TypePtr::NULL_PTR));\n+      } ideal.else_(); {\n+        \/\/ Return value is non-null\n+        sync_kit(ideal);\n+\n+        \/\/ Change return type of call to scalarized return\n+        const TypeFunc* tf = call->_tf;\n+        const TypeTuple* domain = OptoRuntime::store_inline_type_fields_Type()->domain_cc();\n+        const TypeFunc* new_tf = TypeFunc::make(tf->domain_sig(), tf->domain_cc(), tf->range_sig(), domain);\n+        call->_tf = new_tf;\n+        _gvn.set_type(call, call->Value(&_gvn));\n+        _gvn.set_type(ret, ret->Value(&_gvn));\n+\n+        Node* store_to_buf_call = make_runtime_call(RC_NO_LEAF | RC_NO_IO,\n+                                                    OptoRuntime::store_inline_type_fields_Type(),\n+                                                    StubRoutines::store_inline_type_fields_to_buf(),\n+                                                    nullptr, TypePtr::BOTTOM, ret);\n+\n+        \/\/ We don't know how many values are returned. This assumes the\n+        \/\/ worst case, that all available registers are used.\n+        for (uint i = TypeFunc::Parms+1; i < domain->cnt(); i++) {\n+          if (domain->field_at(i) == Type::HALF) {\n+            store_to_buf_call->init_req(i, top());\n+            continue;\n+          }\n+          Node* proj =_gvn.transform(new ProjNode(call, i));\n+          store_to_buf_call->init_req(i, proj);\n+        }\n+        make_slow_call_ex(store_to_buf_call, env()->Throwable_klass(), false);\n+\n+        Node* buf = _gvn.transform(new ProjNode(store_to_buf_call, TypeFunc::Parms));\n+        const Type* buf_type = TypeOopPtr::make_from_klass(t->as_klass())->join_speculative(TypePtr::NOTNULL);\n+        buf = _gvn.transform(new CheckCastPPNode(control(), buf, buf_type));\n+\n+        ideal.set(res, buf);\n+        ideal.sync_kit(this);\n+      } ideal.end_if();\n+      sync_kit(ideal);\n+      ret = _gvn.transform(ideal.value(res));\n+    }\n+    if (t->is_klass()) {\n+      const Type* type = TypeOopPtr::make_from_klass(t->as_klass());\n+      if (type->is_inlinetypeptr()) {\n+        ret = InlineTypeNode::make_from_oop(this, ret, type->inline_klass());\n+      }\n+    }\n+  }\n+\n@@ -1969,2 +2193,1 @@\n-  CallProjections callprojs;\n-  call->extract_projections(&callprojs, true, do_asserts);\n+  CallProjections* callprojs = call->extract_projections(true, do_asserts);\n@@ -1979,2 +2202,2 @@\n-  if (callprojs.fallthrough_catchproj != nullptr) {\n-    C->gvn_replace_by(callprojs.fallthrough_catchproj, final_ctl);\n+  if (callprojs->fallthrough_catchproj != nullptr) {\n+    C->gvn_replace_by(callprojs->fallthrough_catchproj, final_ctl);\n@@ -1982,1 +2205,1 @@\n-  if (callprojs.fallthrough_memproj != nullptr) {\n+  if (callprojs->fallthrough_memproj != nullptr) {\n@@ -1987,1 +2210,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_memproj,   final_mem);\n+    C->gvn_replace_by(callprojs->fallthrough_memproj,   final_mem);\n@@ -1990,2 +2213,2 @@\n-  if (callprojs.fallthrough_ioproj != nullptr) {\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj,    final_io);\n+  if (callprojs->fallthrough_ioproj != nullptr) {\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj,    final_io);\n@@ -1995,2 +2218,6 @@\n-  if (callprojs.resproj != nullptr && result != nullptr) {\n-    C->gvn_replace_by(callprojs.resproj, result);\n+  if (callprojs->resproj[0] != nullptr && result != nullptr) {\n+    \/\/ If the inlined code is dead, the result projections for an inline type returned as\n+    \/\/ fields have not been replaced. They will go away once the call is replaced by TOP below.\n+    assert(callprojs->nb_resproj == 1 || (call->tf()->returns_inline_type_as_fields() && stopped()),\n+           \"unexpected number of results\");\n+    C->gvn_replace_by(callprojs->resproj[0], result);\n@@ -2001,2 +2228,2 @@\n-    if (callprojs.catchall_catchproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, C->top());\n+    if (callprojs->catchall_catchproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, C->top());\n@@ -2004,2 +2231,2 @@\n-    if (callprojs.catchall_memproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_memproj,   C->top());\n+    if (callprojs->catchall_memproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_memproj,   C->top());\n@@ -2007,2 +2234,2 @@\n-    if (callprojs.catchall_ioproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    C->top());\n+    if (callprojs->catchall_ioproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    C->top());\n@@ -2011,2 +2238,2 @@\n-    if (callprojs.exobj != nullptr) {\n-      C->gvn_replace_by(callprojs.exobj, C->top());\n+    if (callprojs->exobj != nullptr) {\n+      C->gvn_replace_by(callprojs->exobj, C->top());\n@@ -2023,2 +2250,2 @@\n-    if (callprojs.catchall_catchproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, ekit.control());\n+    if (callprojs->catchall_catchproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, ekit.control());\n@@ -2027,1 +2254,1 @@\n-    if (callprojs.catchall_memproj != nullptr) {\n+    if (callprojs->catchall_memproj != nullptr) {\n@@ -2029,1 +2256,1 @@\n-      C->gvn_replace_by(callprojs.catchall_memproj,   ex_mem);\n+      C->gvn_replace_by(callprojs->catchall_memproj,   ex_mem);\n@@ -2032,2 +2259,2 @@\n-    if (callprojs.catchall_ioproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    ekit.i_o());\n+    if (callprojs->catchall_ioproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    ekit.i_o());\n@@ -2037,2 +2264,2 @@\n-    if (callprojs.exobj != nullptr) {\n-      C->gvn_replace_by(callprojs.exobj, ex_oop);\n+    if (callprojs->exobj != nullptr) {\n+      C->gvn_replace_by(callprojs->exobj, ex_oop);\n@@ -2052,1 +2279,1 @@\n-  if (callprojs.fallthrough_catchproj != nullptr && !final_ctl->is_top() && do_replaced_nodes) {\n+  if (callprojs->fallthrough_catchproj != nullptr && !final_ctl->is_top() && do_replaced_nodes) {\n@@ -2252,1 +2479,1 @@\n-    const TypePtr* ptr = (ptr_kind == ProfileMaybeNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n+    const TypePtr* ptr = (ptr_kind != ProfileNeverNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n@@ -2275,1 +2502,1 @@\n-    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::OffsetBot, TypeOopPtr::InstanceBot, speculative);\n+    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::Offset::bottom, TypeOopPtr::InstanceBot, speculative);\n@@ -2309,2 +2536,9 @@\n-      if (!data->as_BitData()->null_seen()) {\n-        ptr_kind = ProfileNeverNull;\n+      if (java_bc() == Bytecodes::_aastore) {\n+        ciKlass* array_type = nullptr;\n+        ciKlass* element_type = nullptr;\n+        ProfilePtrKind element_ptr = ProfileMaybeNull;\n+        bool flat_array = true;\n+        bool null_free_array = true;\n+        method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+        exact_kls = element_type;\n+        ptr_kind = element_ptr;\n@@ -2312,8 +2546,12 @@\n-        if (TypeProfileCasts) {\n-          assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n-          ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n-          uint i = 0;\n-          for (; i < call->row_limit(); i++) {\n-            ciKlass* receiver = call->receiver(i);\n-            if (receiver != nullptr) {\n-              break;\n+        if (!data->as_BitData()->null_seen()) {\n+          ptr_kind = ProfileNeverNull;\n+        } else {\n+          if (TypeProfileCasts) {\n+            assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n+            ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n+            uint i = 0;\n+            for (; i < call->row_limit(); i++) {\n+              ciKlass* receiver = call->receiver(i);\n+              if (receiver != nullptr) {\n+                break;\n+              }\n@@ -2321,0 +2559,1 @@\n+            ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2322,1 +2561,0 @@\n-          ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2342,1 +2580,1 @@\n-  int             nargs = tf->domain()->cnt() - TypeFunc::Parms;\n+  int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;\n@@ -2345,1 +2583,1 @@\n-    const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);\n+    const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);\n@@ -2505,1 +2743,1 @@\n-    uint num_bits = call_type->range()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte;\n+    uint num_bits = call_type->range_sig()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte;\n@@ -2540,1 +2778,1 @@\n-  assert(call->in(call->req()-1) != nullptr, \"must initialize all parms\");\n+  assert(call->in(call->req()-1) != nullptr || (call->req()-1) > (TypeFunc::Parms+7), \"must initialize all parms\");\n@@ -2588,0 +2826,1 @@\n+\n@@ -2684,0 +2923,8 @@\n+  const TypeKlassPtr* klass_ptr_type = gvn.type(superklass)->is_klassptr();\n+  \/\/ For a direct pointer comparison, we need the refined array klass pointer\n+  Node* vm_superklass = superklass;\n+  if (klass_ptr_type->isa_aryklassptr() && klass_ptr_type->klass_is_exact()) {\n+    assert(!klass_ptr_type->is_aryklassptr()->is_refined_type(), \"Unexpected refined array klass pointer\");\n+    vm_superklass = gvn.makecon(klass_ptr_type->is_aryklassptr()->cast_to_refined_array_klass_ptr());\n+  }\n+\n@@ -2721,1 +2968,1 @@\n-        IfNode* iff = gen_subtype_check_compare(*ctrl, subklass, superklass, BoolTest::eq, PROB_STATIC_FREQUENT, gvn, T_ADDRESS);\n+        IfNode* iff = gen_subtype_check_compare(*ctrl, subklass, vm_superklass, BoolTest::eq, PROB_STATIC_FREQUENT, gvn, T_ADDRESS);\n@@ -2799,0 +3046,4 @@\n+        if (klass_t->isa_aryklassptr()) {\n+          \/\/ For a direct pointer comparison, we need the refined array klass pointer\n+          klass_t = klass_t->is_aryklassptr()->cast_to_refined_array_klass_ptr();\n+        }\n@@ -2852,1 +3103,1 @@\n-  IfNode *iff3 = gen_subtype_check_compare(*ctrl, subklass, superklass, BoolTest::eq, PROB_LIKELY(0.36f), gvn, T_ADDRESS);\n+  IfNode *iff3 = gen_subtype_check_compare(*ctrl, subklass, vm_superklass, BoolTest::eq, PROB_LIKELY(0.36f), gvn, T_ADDRESS);\n@@ -2890,0 +3141,5 @@\n+  const Type* sub_t = _gvn.type(obj_or_subklass);\n+  if (sub_t->make_oopptr() != nullptr && sub_t->make_oopptr()->is_inlinetypeptr()) {\n+    sub_t = TypeKlassPtr::make(sub_t->inline_klass());\n+    obj_or_subklass = makecon(sub_t);\n+  }\n@@ -2895,1 +3151,1 @@\n-    if (!_gvn.type(obj_or_subklass)->isa_klassptr()) {\n+    if (!sub_t->isa_klassptr()) {\n@@ -2913,2 +3169,1 @@\n-                                    float prob,\n-                                    Node* *casted_receiver) {\n+                                    float prob, Node* *casted_receiver) {\n@@ -2916,1 +3171,12 @@\n-\n+  Node* fail = top();\n+  const Type* rec_t = _gvn.type(receiver);\n+  if (rec_t->is_inlinetypeptr()) {\n+    if (klass->equals(rec_t->inline_klass())) {\n+      (*casted_receiver) = receiver; \/\/ Always passes\n+    } else {\n+      (*casted_receiver) = top();    \/\/ Always fails\n+      fail = control();\n+      set_control(top());\n+    }\n+    return fail;\n+  }\n@@ -2918,0 +3184,4 @@\n+  if (tklass->isa_aryklassptr()) {\n+    \/\/ For a direct pointer comparison, we need the refined array klass pointer\n+    tklass = tklass->is_aryklassptr()->cast_to_refined_array_klass_ptr();\n+  }\n@@ -2919,6 +3189,1 @@\n-  Node* want_klass = makecon(tklass);\n-  Node* cmp = _gvn.transform(new CmpPNode(recv_klass, want_klass));\n-  Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n-  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n-  set_control( _gvn.transform(new IfTrueNode (iff)));\n-  Node* fail = _gvn.transform(new IfFalseNode(iff));\n+  fail = type_check(recv_klass, tklass, prob);\n@@ -2928,2 +3193,2 @@\n-    const TypeOopPtr* recvx_type = tklass->as_instance_type();\n-    assert(recvx_type->klass_is_exact(), \"\");\n+    const TypeOopPtr* recv_xtype = tklass->as_instance_type();\n+    assert(recv_xtype->klass_is_exact(), \"\");\n@@ -2931,1 +3196,1 @@\n-    if (!receiver_type->higher_equal(recvx_type)) { \/\/ ignore redundant casts\n+    if (!receiver_type->higher_equal(recv_xtype)) { \/\/ ignore redundant casts\n@@ -2934,2 +3199,7 @@\n-      Node* cast = new CheckCastPPNode(control(), receiver, recvx_type);\n-      (*casted_receiver) = _gvn.transform(cast);\n+      Node* cast = new CheckCastPPNode(control(), receiver, recv_xtype);\n+      Node* res = _gvn.transform(cast);\n+      if (recv_xtype->is_inlinetypeptr()) {\n+        assert(!gvn().type(res)->maybe_null(), \"receiver should never be null\");\n+        res = InlineTypeNode::make_from_oop(this, res, recv_xtype->inline_klass());\n+      }\n+      (*casted_receiver) = res;\n@@ -2944,0 +3214,11 @@\n+Node* GraphKit::type_check(Node* recv_klass, const TypeKlassPtr* tklass,\n+                           float prob) {\n+  Node* want_klass = makecon(tklass);\n+  Node* cmp = _gvn.transform(new CmpPNode(recv_klass, want_klass));\n+  Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n+  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n+  set_control(_gvn.transform(new IfTrueNode (iff)));\n+  Node* fail = _gvn.transform(new IfFalseNode(iff));\n+  return fail;\n+}\n+\n@@ -2956,3 +3237,6 @@\n-    if (!receiver_type->higher_equal(recv_type)) { \/\/ ignore redundant casts\n-      Node* cast = new CheckCastPPNode(control(), receiver, recv_type);\n-      (*casted_receiver) = _gvn.transform(cast);\n+    if (receiver_type != nullptr && !receiver_type->higher_equal(recv_type)) { \/\/ ignore redundant casts\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), receiver, recv_type));\n+      if (recv_type->is_inlinetypeptr()) {\n+        cast = InlineTypeNode::make_from_oop(this, cast, recv_type->inline_klass());\n+      }\n+      (*casted_receiver) = cast;\n@@ -3067,1 +3351,14 @@\n-  ciKlass* exact_kls = spec_klass == nullptr ? profile_has_unique_klass() : spec_klass;\n+  ciKlass* exact_kls = spec_klass;\n+  if (exact_kls == nullptr) {\n+    if (java_bc() == Bytecodes::_aastore) {\n+      ciKlass* array_type = nullptr;\n+      ciKlass* element_type = nullptr;\n+      ProfilePtrKind element_ptr = ProfileMaybeNull;\n+      bool flat_array = true;\n+      bool null_free_array = true;\n+      method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+      exact_kls = element_type;\n+    } else {\n+      exact_kls = profile_has_unique_klass();\n+    }\n+  }\n@@ -3197,1 +3494,1 @@\n-    if (subk->is_loaded()) {\n+    if (subk != nullptr && subk->is_loaded()) {\n@@ -3253,2 +3550,1 @@\n-Node* GraphKit::gen_checkcast(Node *obj, Node* superklass,\n-                              Node* *failure_control) {\n+Node* GraphKit::gen_checkcast(Node* obj, Node* superklass, Node* *failure_control, bool null_free, bool maybe_larval) {\n@@ -3257,0 +3553,16 @@\n+  const Type* obj_type = _gvn.type(obj);\n+  if (obj_type->is_inlinetypeptr() && !obj_type->maybe_null() && klass_ptr_type->klass_is_exact() && obj_type->inline_klass() == klass_ptr_type->exact_klass(true)) {\n+    \/\/ Special case: larval inline objects must not be scalarized. They are also generally not\n+    \/\/ allowed to participate in most operations except as the first operand of putfield, or as an\n+    \/\/ argument to a constructor invocation with it being a receiver, Unsafe::putXXX with it being\n+    \/\/ the first argument, or Unsafe::finishPrivateBuffer. This allows us to aggressively scalarize\n+    \/\/ value objects in all other places. This special case comes from the limitation of the Java\n+    \/\/ language, Unsafe::makePrivateBuffer returns an Object that is checkcast-ed to the concrete\n+    \/\/ value type. We must do this first because C->static_subtype_check may do nothing when\n+    \/\/ StressReflectiveCode is set.\n+    return obj;\n+  }\n+\n+  \/\/ Else it must be a non-larval object\n+  obj = cast_to_non_larval(obj);\n+\n@@ -3259,0 +3571,2 @@\n+  bool safe_for_replace = (failure_control == nullptr);\n+  assert(!null_free || toop->can_be_inline_type(), \"must be an inline type pointer\");\n@@ -3267,3 +3581,10 @@\n-    const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();\n-    if (objtp != nullptr) {\n-      switch (C->static_subtype_check(improved_klass_ptr_type, objtp->as_klass_type())) {\n+    const TypeKlassPtr* kptr = nullptr;\n+    if (obj_type->isa_oop_ptr()) {\n+      kptr = obj_type->is_oopptr()->as_klass_type();\n+    } else if (obj->is_InlineType()) {\n+      ciInlineKlass* vk = obj_type->inline_klass();\n+      kptr = TypeInstKlassPtr::make(TypePtr::NotNull, vk, Type::Offset(0));\n+    }\n+\n+    if (kptr != nullptr) {\n+      switch (C->static_subtype_check(improved_klass_ptr_type, kptr)) {\n@@ -3274,1 +3595,7 @@\n-        return record_profiled_receiver_for_speculation(obj);\n+        obj = record_profiled_receiver_for_speculation(obj);\n+        if (null_free) {\n+          assert(safe_for_replace, \"must be\");\n+          obj = null_check(obj);\n+        }\n+        assert(stopped() || !toop->is_inlinetypeptr() || obj->is_InlineType(), \"should have been scalarized\");\n+        return obj;\n@@ -3276,0 +3603,4 @@\n+        if (null_free) {\n+          assert(safe_for_replace, \"must be\");\n+          obj = null_check(obj);\n+        }\n@@ -3277,2 +3608,1 @@\n-        \/\/ A non-null value will always produce an exception.\n-        if (!objtp->maybe_null()) {\n+        if (obj_type->isa_oopptr() != nullptr && !obj_type->is_oopptr()->maybe_null()) {\n@@ -3295,1 +3625,0 @@\n-  bool safe_for_replace = false;\n@@ -3300,2 +3629,3 @@\n-    data = method()->method_data()->bci_to_data(bci());\n-    safe_for_replace = true;\n+    if (method()->method_data()->is_mature()) {\n+      data = method()->method_data()->bci_to_data(bci());\n+    }\n@@ -3308,0 +3638,3 @@\n+  _gvn.set_type(region, Type::CONTROL);\n+  _gvn.set_type(phi, toop);\n+\n@@ -3315,0 +3648,7 @@\n+  if (obj->is_InlineType()) {\n+    \/\/ Re-execute if buffering during triggers deoptimization\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    obj = obj->as_InlineType()->buffer(this, safe_for_replace);\n+  }\n+\n@@ -3317,1 +3657,7 @@\n-  Node* not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  Node* not_null_obj = nullptr;\n+  if (null_free) {\n+    assert(safe_for_replace, \"must be\");\n+    not_null_obj = null_check(obj);\n+  } else {\n+    not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  }\n@@ -3322,0 +3668,3 @@\n+    if (toop->is_inlinetypeptr()) {\n+      return InlineTypeNode::make_null(_gvn, toop->inline_klass());\n+    }\n@@ -3359,0 +3708,3 @@\n+      \/\/ Only improve the super class for constants which allows subsequent sub type checks to possibly be commoned up.\n+      \/\/ The other non-constant cases cannot be improved with a cast node here since they could be folded to top.\n+      \/\/ Additionally, the benefit would only be minor in non-constant cases.\n@@ -3362,1 +3714,0 @@\n-\n@@ -3400,1 +3751,165 @@\n-  return record_profiled_receiver_for_speculation(res);\n+  bool not_inline = !toop->can_be_inline_type();\n+  bool not_flat_in_array = !UseArrayFlattening || not_inline || (toop->is_inlinetypeptr() && !toop->inline_klass()->maybe_flat_in_array());\n+  if (Arguments::is_valhalla_enabled() && (not_inline || not_flat_in_array)) {\n+    \/\/ Check if obj has been loaded from an array\n+    obj = obj->isa_DecodeN() ? obj->in(1) : obj;\n+    Node* array = nullptr;\n+    if (obj->isa_Load()) {\n+      Node* address = obj->in(MemNode::Address);\n+      if (address->isa_AddP()) {\n+        array = address->as_AddP()->in(AddPNode::Base);\n+      }\n+    } else if (obj->is_Phi()) {\n+      Node* region = obj->in(0);\n+      \/\/ TODO make this more robust (see JDK-8231346)\n+      if (region->req() == 3 && region->in(2) != nullptr && region->in(2)->in(0) != nullptr) {\n+        IfNode* iff = region->in(2)->in(0)->isa_If();\n+        if (iff != nullptr) {\n+          iff->is_flat_array_check(&_gvn, &array);\n+        }\n+      }\n+    }\n+    if (array != nullptr) {\n+      const TypeAryPtr* ary_t = _gvn.type(array)->isa_aryptr();\n+      if (ary_t != nullptr) {\n+        if (!ary_t->is_not_null_free() && !ary_t->is_null_free() && not_inline) {\n+          \/\/ Casting array element to a non-inline-type, mark array as not null-free.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_null_free()));\n+          replace_in_map(array, cast);\n+          array = cast;\n+        }\n+        if (!ary_t->is_not_flat() && !ary_t->is_flat() && not_flat_in_array) {\n+          \/\/ Casting array element to a non-flat-in-array type, mark array as not flat.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_flat()));\n+          replace_in_map(array, cast);\n+          array = cast;\n+        }\n+      }\n+    }\n+  }\n+\n+  if (!stopped() && !res->is_InlineType()) {\n+    res = record_profiled_receiver_for_speculation(res);\n+    if (toop->is_inlinetypeptr() && !maybe_larval) {\n+      Node* vt = InlineTypeNode::make_from_oop(this, res, toop->inline_klass());\n+      res = vt;\n+      if (safe_for_replace) {\n+        replace_in_map(obj, vt);\n+        replace_in_map(not_null_obj, vt);\n+        replace_in_map(res, vt);\n+      }\n+    }\n+  }\n+  return res;\n+}\n+\n+Node* GraphKit::mark_word_test(Node* obj, uintptr_t mask_val, bool eq, bool check_lock) {\n+  \/\/ Load markword\n+  Node* mark_adr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+  Node* mark = make_load(nullptr, mark_adr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  if (check_lock && !UseCompactObjectHeaders) {\n+    \/\/ COH: Locking does not override the markword with a tagged pointer. We can directly read from the markword.\n+    \/\/ Check if obj is locked\n+    Node* locked_bit = MakeConX(markWord::unlocked_value);\n+    locked_bit = _gvn.transform(new AndXNode(locked_bit, mark));\n+    Node* cmp = _gvn.transform(new CmpXNode(locked_bit, MakeConX(0)));\n+    Node* is_unlocked = _gvn.transform(new BoolNode(cmp, BoolTest::ne));\n+    IfNode* iff = new IfNode(control(), is_unlocked, PROB_MAX, COUNT_UNKNOWN);\n+    _gvn.transform(iff);\n+    Node* locked_region = new RegionNode(3);\n+    Node* mark_phi = new PhiNode(locked_region, TypeX_X);\n+\n+    \/\/ Unlocked: Use bits from mark word\n+    locked_region->init_req(1, _gvn.transform(new IfTrueNode(iff)));\n+    mark_phi->init_req(1, mark);\n+\n+    \/\/ Locked: Load prototype header from klass\n+    set_control(_gvn.transform(new IfFalseNode(iff)));\n+    \/\/ Make loads control dependent to make sure they are only executed if array is locked\n+    Node* klass_adr = basic_plus_adr(obj, oopDesc::klass_offset_in_bytes());\n+    Node* klass = _gvn.transform(LoadKlassNode::make(_gvn, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+    Node* proto_adr = basic_plus_adr(klass, in_bytes(Klass::prototype_header_offset()));\n+    Node* proto = _gvn.transform(LoadNode::make(_gvn, control(), C->immutable_memory(), proto_adr, proto_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+\n+    locked_region->init_req(2, control());\n+    mark_phi->init_req(2, proto);\n+    set_control(_gvn.transform(locked_region));\n+    record_for_igvn(locked_region);\n+\n+    mark = mark_phi;\n+  }\n+\n+  \/\/ Now check if mark word bits are set\n+  Node* mask = MakeConX(mask_val);\n+  Node* masked = _gvn.transform(new AndXNode(_gvn.transform(mark), mask));\n+  record_for_igvn(masked); \/\/ Give it a chance to be optimized out by IGVN\n+  Node* cmp = _gvn.transform(new CmpXNode(masked, mask));\n+  return _gvn.transform(new BoolNode(cmp, eq ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::inline_type_test(Node* obj, bool is_inline) {\n+  return mark_word_test(obj, markWord::inline_type_pattern, is_inline, \/* check_lock = *\/ false);\n+}\n+\n+Node* GraphKit::flat_array_test(Node* array_or_klass, bool flat) {\n+  \/\/ We can't use immutable memory here because the mark word is mutable.\n+  \/\/ PhaseIdealLoop::move_flat_array_check_out_of_loop will make sure the\n+  \/\/ check is moved out of loops (mainly to enable loop unswitching).\n+  Node* cmp = _gvn.transform(new FlatArrayCheckNode(C, memory(Compile::AliasIdxRaw), array_or_klass));\n+  record_for_igvn(cmp); \/\/ Give it a chance to be optimized out by IGVN\n+  return _gvn.transform(new BoolNode(cmp, flat ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::null_free_array_test(Node* array, bool null_free) {\n+  return mark_word_test(array, markWord::null_free_array_bit_in_place, null_free);\n+}\n+\n+Node* GraphKit::null_free_atomic_array_test(Node* array, ciInlineKlass* vk) {\n+  assert(vk->has_atomic_layout() || vk->has_non_atomic_layout(), \"Can't be null-free and flat\");\n+\n+  \/\/ TODO 8350865 Add a stress flag to always access atomic if layout exists?\n+  if (!vk->has_non_atomic_layout()) {\n+    return intcon(1); \/\/ Always atomic\n+  } else if (!vk->has_atomic_layout()) {\n+    return intcon(0); \/\/ Never atomic\n+  }\n+\n+  Node* array_klass = load_object_klass(array);\n+  int layout_kind_offset = in_bytes(FlatArrayKlass::layout_kind_offset());\n+  Node* layout_kind_addr = basic_plus_adr(array_klass, array_klass, layout_kind_offset);\n+  Node* layout_kind = make_load(nullptr, layout_kind_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+  Node* cmp = _gvn.transform(new CmpINode(layout_kind, intcon((int)LayoutKind::NULL_FREE_ATOMIC_FLAT)));\n+  return _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n+}\n+\n+\/\/ Deoptimize if 'ary' is a null-free inline type array and 'val' is null\n+Node* GraphKit::inline_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace) {\n+  RegionNode* region = new RegionNode(3);\n+  Node* null_ctl = top();\n+  null_check_oop(val, &null_ctl);\n+  if (null_ctl != top()) {\n+    PreserveJVMState pjvms(this);\n+    set_control(null_ctl);\n+    {\n+      \/\/ Deoptimize if null-free array\n+      BuildCutout unless(this, null_free_array_test(ary, \/* null_free = *\/ false), PROB_MAX);\n+      inc_sp(nargs);\n+      uncommon_trap(Deoptimization::Reason_null_check,\n+                    Deoptimization::Action_none);\n+    }\n+    region->init_req(1, control());\n+  }\n+  region->init_req(2, control());\n+  set_control(_gvn.transform(region));\n+  record_for_igvn(region);\n+  if (_gvn.type(val) == TypePtr::NULL_PTR) {\n+    \/\/ Since we were just successfully storing null, the array can't be null free.\n+    const TypeAryPtr* ary_t = _gvn.type(ary)->is_aryptr();\n+    ary_t = ary_t->cast_to_not_null_free();\n+    Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));\n+    if (safe_for_replace) {\n+      replace_in_map(ary, cast);\n+    }\n+    ary = cast;\n+  }\n+  return ary;\n@@ -3532,0 +4047,1 @@\n+  assert(!obj->is_InlineType(), \"should not unlock on inline type\");\n@@ -3572,1 +4088,8 @@\n-    if (xklass || (klass_t->isa_aryklassptr() && klass_t->is_aryklassptr()->elem() != Type::BOTTOM)) {\n+    bool can_be_flat = false;\n+    const TypeAryPtr* ary_type = klass_t->as_instance_type()->isa_aryptr();\n+    if (UseArrayFlattening && !xklass && ary_type != nullptr && !ary_type->is_null_free()) {\n+      \/\/ Don't constant fold if the runtime type might be a flat array but the static type is not.\n+      const TypeOopPtr* elem = ary_type->elem()->make_oopptr();\n+      can_be_flat = ary_type->can_be_inline_array() && (!elem->is_inlinetypeptr() || elem->inline_klass()->maybe_flat_in_array());\n+    }\n+    if (!can_be_flat && (xklass || (klass_t->isa_aryklassptr() && klass_t->is_aryklassptr()->elem() != Type::BOTTOM))) {\n@@ -3574,2 +4097,4 @@\n-      if (klass_t->isa_aryklassptr()) {\n-        BasicType elem = klass_t->as_instance_type()->isa_aryptr()->elem()->array_element_basic_type();\n+      if (klass_t->is_flat()) {\n+        lhelper = ary_type->flat_layout_helper();\n+      } else if (klass_t->isa_aryklassptr()) {\n+        BasicType elem = ary_type->elem()->array_element_basic_type();\n@@ -3604,1 +4129,3 @@\n-  kit.set_memory(init_out_raw, alias_idx);\n+  if (init_out_raw != nullptr) {\n+    kit.set_memory(init_out_raw, alias_idx);\n+  }\n@@ -3643,0 +4170,1 @@\n+    _gvn.set_type(minit_in, Type::MEMORY);\n@@ -3653,0 +4181,5 @@\n+      \/\/ Initially all flat array accesses share a single slice\n+      \/\/ but that changes after parsing. Prepare the memory graph so\n+      \/\/ it can optimize flat array accesses properly once they\n+      \/\/ don't share a single slice.\n+      assert(C->flat_accesses_share_alias(), \"should be set at parse time\");\n@@ -3657,0 +4190,1 @@\n+      set_memory(minit_out, C->get_alias_index(oop_type)); \/\/ mark word\n@@ -3707,1 +4241,2 @@\n-                             bool deoptimize_on_exception) {\n+                             bool deoptimize_on_exception,\n+                             InlineTypeNode* inline_type_node) {\n@@ -3714,1 +4249,1 @@\n-  int   layout_is_con = (layout_val == nullptr);\n+  bool  layout_is_con = (layout_val == nullptr);\n@@ -3765,1 +4300,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3772,1 +4307,1 @@\n-                                         initial_slow_test);\n+                                         initial_slow_test, inline_type_node);\n@@ -3778,1 +4313,1 @@\n-\/\/ helper for both newarray and anewarray\n+\/\/ helper for newarray and anewarray\n@@ -3788,1 +4323,2 @@\n-                          bool deoptimize_on_exception) {\n+                          bool deoptimize_on_exception,\n+                          Node* init_val) {\n@@ -3791,1 +4327,1 @@\n-  int   layout_is_con = (layout_val == nullptr);\n+  bool  layout_is_con = (layout_val == nullptr);\n@@ -3821,3 +4357,1 @@\n-    assert(fast_size_limit == 0 || count_leading_zeros(fast_size_limit) > static_cast<unsigned>(LogBytesPerLong - log2_esize),\n-           \"fast_size_limit (%d) overflow when shifted left by %d\", fast_size_limit, LogBytesPerLong - log2_esize);\n-    fast_size_limit <<= (LogBytesPerLong - log2_esize);\n+    fast_size_limit <<= MAX2(LogBytesPerLong - log2_esize, 0);\n@@ -3840,0 +4374,1 @@\n+    bool is_flat_array = Klass::layout_helper_is_flatArray(layout_con);\n@@ -3842,1 +4377,1 @@\n-    assert((hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n+    assert(is_flat_array || (hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n@@ -3931,1 +4466,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3940,1 +4475,21 @@\n-  const TypeOopPtr* ary_type = _gvn.type(klass_node)->is_klassptr()->as_instance_type();\n+  const TypeKlassPtr* ary_klass = _gvn.type(klass_node)->isa_klassptr();\n+  const TypeOopPtr* ary_type = ary_klass->as_instance_type();\n+\n+  Node* raw_init_value = nullptr;\n+  if (init_val != nullptr) {\n+    \/\/ TODO 8350865 Fast non-zero init not implemented yet for flat, null-free arrays\n+    if (ary_type->is_flat()) {\n+      initial_slow_test = intcon(1);\n+    }\n+\n+    if (UseCompressedOops) {\n+      \/\/ With compressed oops, the 64-bit init value is built from two 32-bit compressed oops\n+      init_val = _gvn.transform(new EncodePNode(init_val, init_val->bottom_type()->make_narrowoop()));\n+      Node* lower = _gvn.transform(new CastP2XNode(control(), init_val));\n+      Node* upper = _gvn.transform(new LShiftLNode(lower, intcon(32)));\n+      raw_init_value = _gvn.transform(new OrLNode(lower, upper));\n+    } else {\n+      raw_init_value = _gvn.transform(new CastP2XNode(control(), init_val));\n+    }\n+  }\n+\n@@ -3955,2 +4510,2 @@\n-                            length, valid_length_test);\n-\n+                            length, valid_length_test,\n+                            init_val, raw_init_value);\n@@ -4091,0 +4646,1 @@\n+  reset_memory();\n@@ -4111,1 +4667,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4114,2 +4670,2 @@\n-                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS),\n-                                                  ciTypeArrayKlass::make(T_BYTE), true, 0);\n+                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS, false, false, true, true, true),\n+                                                  ciTypeArrayKlass::make(T_BYTE), true, Type::Offset(0));\n@@ -4128,1 +4684,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4140,1 +4696,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4150,1 +4706,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4263,1 +4819,7 @@\n-    return makecon(con_type);\n+    Node* con = makecon(con_type);\n+    if (field->type()->is_inlinetype()) {\n+      con = InlineTypeNode::make_from_oop(this, con, field->type()->as_inline_klass());\n+    } else if (con_type->is_inlinetypeptr()) {\n+      con = InlineTypeNode::make_from_oop(this, con, con_type->inline_klass());\n+    }\n+    return con;\n@@ -4269,1 +4831,1 @@\n-  const TypeOopPtr* obj_type = obj->bottom_type()->isa_oopptr();\n+  const Type* obj_type = obj->bottom_type();\n@@ -4271,1 +4833,1 @@\n-  if (obj_type != nullptr && sig_type->is_loaded() && !obj_type->higher_equal(sig_type)) {\n+  if (obj_type->isa_oopptr() && sig_type->is_loaded() && !obj_type->higher_equal(sig_type)) {\n@@ -4274,1 +4836,4 @@\n-    return casted_obj;\n+    obj = casted_obj;\n+  }\n+  if (sig_type->is_inlinetypeptr()) {\n+    obj = InlineTypeNode::make_from_oop(this, obj, sig_type->inline_klass());\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":706,"deletions":141,"binary":false,"changes":847,"status":"modified"},{"patch":"@@ -1277,0 +1277,17 @@\n+\/\/ Returns true if this IfNode belongs to a flat array check\n+\/\/ and returns the corresponding array in the 'array' parameter.\n+bool IfNode::is_flat_array_check(PhaseTransform* phase, Node** array) {\n+  Node* bol = in(1);\n+  if (!bol->is_Bool()) {\n+    return false;\n+  }\n+  Node* cmp = bol->in(1);\n+  if (cmp->isa_FlatArrayCheck()) {\n+    if (array != nullptr) {\n+      *array = cmp->in(FlatArrayCheckNode::ArrayOrKlass);\n+    }\n+    return true;\n+  }\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/ifnode.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -26,0 +26,3 @@\n+#include \"ci\/ciArrayKlass.hpp\"\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInstanceKlass.hpp\"\n@@ -32,0 +35,1 @@\n+#include \"gc\/shared\/c2\/barrierSetC2.hpp\"\n@@ -34,0 +38,1 @@\n+#include \"oops\/accessDecorators.hpp\"\n@@ -35,0 +40,1 @@\n+#include \"oops\/layoutKind.hpp\"\n@@ -43,0 +49,1 @@\n+#include \"opto\/graphKit.hpp\"\n@@ -44,0 +51,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -49,0 +57,1 @@\n+#include \"opto\/opcodes.hpp\"\n@@ -53,0 +62,1 @@\n+#include \"opto\/type.hpp\"\n@@ -57,0 +67,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -62,0 +73,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -322,0 +334,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:        return inline_unsafe_make_private_buffer();\n+  case vmIntrinsics::_finishPrivateBuffer:      return inline_unsafe_finish_private_buffer();\n@@ -412,0 +426,3 @@\n+  case vmIntrinsics::_getFlatValue:             return inline_unsafe_flat_access(!is_store, Relaxed);\n+  case vmIntrinsics::_putFlatValue:             return inline_unsafe_flat_access( is_store, Relaxed);\n+\n@@ -471,0 +488,5 @@\n+  case vmIntrinsics::_arrayInstanceBaseOffset:  return inline_arrayInstanceBaseOffset();\n+  case vmIntrinsics::_arrayInstanceIndexScale:  return inline_arrayInstanceIndexScale();\n+  case vmIntrinsics::_arrayLayout:              return inline_arrayLayout();\n+  case vmIntrinsics::_getFieldMap:              return inline_getFieldMap();\n+\n@@ -519,0 +541,6 @@\n+  case vmIntrinsics::_newNullRestrictedNonAtomicArray: return inline_newArray(\/* null_free *\/ true, \/* atomic *\/ false);\n+  case vmIntrinsics::_newNullRestrictedAtomicArray: return inline_newArray(\/* null_free *\/ true, \/* atomic *\/ true);\n+  case vmIntrinsics::_newNullableAtomicArray:     return inline_newArray(\/* null_free *\/ false, \/* atomic *\/ true);\n+  case vmIntrinsics::_isFlatArray:              return inline_getArrayProperties(IsFlat);\n+  case vmIntrinsics::_isNullRestrictedArray:    return inline_getArrayProperties(IsNullRestricted);\n+  case vmIntrinsics::_isAtomicArray:            return inline_getArrayProperties(IsAtomic);\n@@ -2336,0 +2364,1 @@\n+  bool null_free = false;\n@@ -2341,0 +2370,1 @@\n+      null_free = alias_type->field()->is_null_free();\n@@ -2347,1 +2377,1 @@\n-    if (adr_type->offset() >= objArrayOopDesc::base_offset_in_bytes()) {\n+    if (adr_type->offset() >= refArrayOopDesc::base_offset_in_bytes()) {\n@@ -2349,0 +2379,1 @@\n+      null_free = adr_type->is_aryptr()->is_null_free();\n@@ -2361,0 +2392,3 @@\n+    if (null_free) {\n+      result = result->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n@@ -2489,0 +2523,36 @@\n+\n+  if (base->is_InlineType()) {\n+    assert(!is_store, \"InlineTypeNodes are non-larval value objects\");\n+    InlineTypeNode* vt = base->as_InlineType();\n+    if (offset->is_Con()) {\n+      long off = find_long_con(offset, 0);\n+      ciInlineKlass* vk = vt->type()->inline_klass();\n+      if ((long)(int)off != off || !vk->contains_field_offset(off)) {\n+        return false;\n+      }\n+\n+      ciField* field = vk->get_non_flat_field_by_offset(off);\n+      if (field != nullptr) {\n+        BasicType bt = type2field[field->type()->basic_type()];\n+        if (bt == T_ARRAY || bt == T_NARROWOOP) {\n+          bt = T_OBJECT;\n+        }\n+        if (bt == type && !field->is_flat()) {\n+          Node* value = vt->field_value_by_offset(off, false);\n+          if (value->is_InlineType()) {\n+            value = value->as_InlineType()->adjust_scalarization_depth(this);\n+          }\n+          set_result(value);\n+          return true;\n+        }\n+      }\n+    }\n+    {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      vt = vt->buffer(this);\n+    }\n+    base = vt->get_oop();\n+  }\n+\n@@ -2532,1 +2602,29 @@\n-  BasicType bt = alias_type->basic_type();\n+  BasicType bt = T_ILLEGAL;\n+  ciField* field = nullptr;\n+  if (adr_type->isa_instptr()) {\n+    const TypeInstPtr* instptr = adr_type->is_instptr();\n+    ciInstanceKlass* k = instptr->instance_klass();\n+    int off = instptr->offset();\n+    if (instptr->const_oop() != nullptr &&\n+        k == ciEnv::current()->Class_klass() &&\n+        instptr->offset() >= (k->size_helper() * wordSize)) {\n+      k = instptr->const_oop()->as_instance()->java_lang_Class_klass()->as_instance_klass();\n+      field = k->get_field_by_offset(off, true);\n+    } else {\n+      field = k->get_non_flat_field_by_offset(off);\n+    }\n+    if (field != nullptr) {\n+      bt = type2field[field->type()->basic_type()];\n+    }\n+    if (bt != alias_type->basic_type()) {\n+      \/\/ Type mismatch. Is it an access to a nested flat field?\n+      field = k->get_field_by_offset(off, false);\n+      if (field != nullptr) {\n+        bt = type2field[field->type()->basic_type()];\n+      }\n+    }\n+    assert(bt == alias_type->basic_type(), \"should match\");\n+  } else {\n+    bt = alias_type->basic_type();\n+  }\n+\n@@ -2566,4 +2664,6 @@\n-  if (!is_store && type == T_OBJECT) {\n-    const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n-    if (tjp != nullptr) {\n-      value_type = tjp;\n+  if (!is_store) {\n+    if (type == T_OBJECT) {\n+      const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n+      if (tjp != nullptr) {\n+        value_type = tjp;\n+      }\n@@ -2585,2 +2685,2 @@\n-    ciField* field = alias_type->field();\n-    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !mismatched) {\n+\n+    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !field->is_flat() && !mismatched) {\n@@ -2593,0 +2693,5 @@\n+      const TypeOopPtr* ptr = value_type->make_oopptr();\n+      if (ptr != nullptr && ptr->is_inlinetypeptr()) {\n+        \/\/ Load a non-flattened inline type from memory\n+        p = InlineTypeNode::make_from_oop(this, p, ptr->inline_klass());\n+      }\n@@ -2636,0 +2741,235 @@\n+bool LibraryCallKit::inline_unsafe_flat_access(bool is_store, AccessKind kind) {\n+#ifdef ASSERT\n+  {\n+    ResourceMark rm;\n+    \/\/ Check the signatures.\n+    ciSignature* sig = callee()->signature();\n+    assert(sig->type_at(0)->basic_type() == T_OBJECT, \"base should be object, but is %s\", type2name(sig->type_at(0)->basic_type()));\n+    assert(sig->type_at(1)->basic_type() == T_LONG, \"offset should be long, but is %s\", type2name(sig->type_at(1)->basic_type()));\n+    assert(sig->type_at(2)->basic_type() == T_INT, \"layout kind should be int, but is %s\", type2name(sig->type_at(3)->basic_type()));\n+    assert(sig->type_at(3)->basic_type() == T_OBJECT, \"value klass should be object, but is %s\", type2name(sig->type_at(4)->basic_type()));\n+    if (is_store) {\n+      assert(sig->return_type()->basic_type() == T_VOID, \"putter must not return a value, but returns %s\", type2name(sig->return_type()->basic_type()));\n+      assert(sig->count() == 5, \"flat putter should have 5 arguments, but has %d\", sig->count());\n+      assert(sig->type_at(4)->basic_type() == T_OBJECT, \"put value should be object, but is %s\", type2name(sig->type_at(5)->basic_type()));\n+    } else {\n+      assert(sig->return_type()->basic_type() == T_OBJECT, \"getter must return an object, but returns %s\", type2name(sig->return_type()->basic_type()));\n+      assert(sig->count() == 4, \"flat getter should have 4 arguments, but has %d\", sig->count());\n+    }\n+ }\n+#endif \/\/ ASSERT\n+\n+  assert(kind == Relaxed, \"Only plain accesses for now\");\n+  if (callee()->is_static()) {\n+    \/\/ caller must have the capability!\n+    return false;\n+  }\n+  C->set_has_unsafe_access(true);\n+\n+  const TypeInstPtr* value_klass_node = _gvn.type(argument(5))->isa_instptr();\n+  if (value_klass_node == nullptr || value_klass_node->const_oop() == nullptr) {\n+    \/\/ parameter valueType is not a constant\n+    return false;\n+  }\n+  ciType* mirror_type = value_klass_node->const_oop()->as_instance()->java_mirror_type();\n+  if (!mirror_type->is_inlinetype()) {\n+    \/\/ Dead code\n+    return false;\n+  }\n+  ciInlineKlass* value_klass = mirror_type->as_inline_klass();\n+\n+  const TypeInt* layout_type = _gvn.type(argument(4))->isa_int();\n+  if (layout_type == nullptr || !layout_type->is_con()) {\n+    \/\/ parameter layoutKind is not a constant\n+    return false;\n+  }\n+  assert(layout_type->get_con() >= static_cast<int>(LayoutKind::REFERENCE) &&\n+         layout_type->get_con() <= static_cast<int>(LayoutKind::UNKNOWN),\n+         \"invalid layoutKind %d\", layout_type->get_con());\n+  LayoutKind layout = static_cast<LayoutKind>(layout_type->get_con());\n+  assert(layout == LayoutKind::REFERENCE || layout == LayoutKind::NULL_FREE_NON_ATOMIC_FLAT ||\n+         layout == LayoutKind::NULL_FREE_ATOMIC_FLAT || layout == LayoutKind::NULLABLE_ATOMIC_FLAT,\n+         \"unexpected layoutKind %d\", layout_type->get_con());\n+\n+  null_check(argument(0));\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  Node* base = must_be_not_null(argument(1), true);\n+  Node* offset = argument(2);\n+  const Type* base_type = _gvn.type(base);\n+\n+  Node* ptr;\n+  bool immutable_memory = false;\n+  DecoratorSet decorators = C2_UNSAFE_ACCESS | IN_HEAP | MO_UNORDERED;\n+  if (base_type->isa_instptr()) {\n+    const TypeLong* offset_type = _gvn.type(offset)->isa_long();\n+    if (offset_type == nullptr || !offset_type->is_con()) {\n+      \/\/ Offset into a non-array should be a constant\n+      decorators |= C2_MISMATCHED;\n+    } else {\n+      int offset_con = checked_cast<int>(offset_type->get_con());\n+      ciInstanceKlass* base_klass = base_type->is_instptr()->instance_klass();\n+      ciField* field = base_klass->get_non_flat_field_by_offset(offset_con);\n+      if (field == nullptr) {\n+        assert(!base_klass->is_final(), \"non-existence field at offset %d of class %s\", offset_con, base_klass->name()->as_utf8());\n+        decorators |= C2_MISMATCHED;\n+      } else {\n+        assert(field->type() == value_klass, \"field at offset %d of %s is of type %s, but valueType is %s\",\n+               offset_con, base_klass->name()->as_utf8(), field->type()->name(), value_klass->name()->as_utf8());\n+        immutable_memory = field->is_strict() && field->is_final();\n+\n+        if (base->is_InlineType()) {\n+          assert(!is_store, \"Cannot store into a non-larval value object\");\n+          set_result(base->as_InlineType()->field_value_by_offset(offset_con, false));\n+          return true;\n+        }\n+      }\n+    }\n+\n+    if (base->is_InlineType()) {\n+      assert(!is_store, \"Cannot store into a non-larval value object\");\n+      base = base->as_InlineType()->buffer(this, true);\n+    }\n+    ptr = basic_plus_adr(base, ConvL2X(offset));\n+  } else if (base_type->isa_aryptr()) {\n+    decorators |= IS_ARRAY;\n+    if (layout == LayoutKind::REFERENCE) {\n+      if (!base_type->is_aryptr()->is_not_flat()) {\n+        const TypeAryPtr* array_type = base_type->is_aryptr()->cast_to_not_flat();\n+        Node* new_base = _gvn.transform(new CastPPNode(control(), base, array_type, ConstraintCastNode::DependencyType::NonFloatingNarrowing));\n+        replace_in_map(base, new_base);\n+        base = new_base;\n+      }\n+      ptr = basic_plus_adr(base, ConvL2X(offset));\n+    } else {\n+      if (UseArrayFlattening) {\n+        \/\/ Flat array must have an exact type\n+        bool is_null_free = !LayoutKindHelper::is_nullable_flat(layout);\n+        bool is_atomic = LayoutKindHelper::is_atomic_flat(layout);\n+        Node* new_base = cast_to_flat_array_exact(base, value_klass, is_null_free, is_atomic);\n+        replace_in_map(base, new_base);\n+        base = new_base;\n+        ptr = basic_plus_adr(base, ConvL2X(offset));\n+        const TypeAryPtr* ptr_type = _gvn.type(ptr)->is_aryptr();\n+        if (ptr_type->field_offset().get() != 0) {\n+          ptr = _gvn.transform(new CastPPNode(control(), ptr, ptr_type->with_field_offset(0), ConstraintCastNode::DependencyType::NonFloatingNarrowing));\n+        }\n+      } else {\n+        uncommon_trap(Deoptimization::Reason_intrinsic,\n+                      Deoptimization::Action_none);\n+        return true;\n+      }\n+    }\n+  } else {\n+    decorators |= C2_MISMATCHED;\n+    ptr = basic_plus_adr(base, ConvL2X(offset));\n+  }\n+\n+  if (is_store) {\n+    Node* value = argument(6);\n+    const Type* value_type = _gvn.type(value);\n+    if (!value_type->is_inlinetypeptr()) {\n+      value_type = Type::get_const_type(value_klass)->filter_speculative(value_type);\n+      Node* new_value = _gvn.transform(new CastPPNode(control(), value, value_type, ConstraintCastNode::DependencyType::NonFloatingNarrowing));\n+      new_value = InlineTypeNode::make_from_oop(this, new_value, value_klass);\n+      replace_in_map(value, new_value);\n+      value = new_value;\n+    }\n+\n+    assert(value_type->inline_klass() == value_klass, \"value is of type %s while valueType is %s\", value_type->inline_klass()->name()->as_utf8(), value_klass->name()->as_utf8());\n+    if (layout == LayoutKind::REFERENCE) {\n+      const TypePtr* ptr_type = (decorators & C2_MISMATCHED) != 0 ? TypeRawPtr::BOTTOM : _gvn.type(ptr)->is_ptr();\n+      access_store_at(base, ptr, ptr_type, value, value_type, T_OBJECT, decorators);\n+    } else {\n+      bool atomic = LayoutKindHelper::is_atomic_flat(layout);\n+      bool null_free = !LayoutKindHelper::is_nullable_flat(layout);\n+      value->as_InlineType()->store_flat(this, base, ptr, atomic, immutable_memory, null_free, decorators);\n+    }\n+\n+    return true;\n+  } else {\n+    decorators |= (C2_CONTROL_DEPENDENT_LOAD | C2_UNKNOWN_CONTROL_LOAD);\n+    InlineTypeNode* result;\n+    if (layout == LayoutKind::REFERENCE) {\n+      const TypePtr* ptr_type = (decorators & C2_MISMATCHED) != 0 ? TypeRawPtr::BOTTOM : _gvn.type(ptr)->is_ptr();\n+      Node* oop = access_load_at(base, ptr, ptr_type, Type::get_const_type(value_klass), T_OBJECT, decorators);\n+      result = InlineTypeNode::make_from_oop(this, oop, value_klass);\n+    } else {\n+      bool atomic = LayoutKindHelper::is_atomic_flat(layout);\n+      bool null_free = !LayoutKindHelper::is_nullable_flat(layout);\n+      result = InlineTypeNode::make_from_flat(this, value_klass, base, ptr, atomic, immutable_memory, null_free, decorators);\n+    }\n+\n+    set_result(result);\n+    return true;\n+  }\n+}\n+\n+bool LibraryCallKit::inline_unsafe_make_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* value = argument(1);\n+\n+  const Type* type = gvn().type(value);\n+  if (!type->is_inlinetypeptr()) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::makePrivateBuffer is not of a constant value type\");\n+    return false;\n+  }\n+\n+  null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  value = null_check(value);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  ciInlineKlass* vk = type->inline_klass();\n+  Node* klass = makecon(TypeKlassPtr::make(vk));\n+  Node* obj = new_instance(klass);\n+  AllocateNode::Ideal_allocation(obj)->_larval = true;\n+\n+  assert(value->is_InlineType(), \"must be an InlineTypeNode\");\n+  Node* payload_ptr = basic_plus_adr(obj, vk->payload_offset());\n+  value->as_InlineType()->store_flat(this, obj, payload_ptr, false, true, true, IN_HEAP | MO_UNORDERED);\n+\n+  set_result(obj);\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_finish_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* buffer = argument(1);\n+\n+  const Type* type = gvn().type(buffer);\n+  if (!type->is_inlinetypeptr()) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::finishPrivateBuffer is not of a constant value type\");\n+    return false;\n+  }\n+\n+  AllocateNode* alloc = AllocateNode::Ideal_allocation(buffer);\n+  if (alloc == nullptr) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::finishPrivateBuffer must be allocated by Unsafe::makePrivateBuffer\");\n+    return false;\n+  }\n+\n+  null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  \/\/ Unset the larval bit in the object header\n+  Node* old_header = make_load(control(), buffer, TypeX_X, TypeX_X->basic_type(), MemNode::unordered, LoadNode::Pinned);\n+  Node* new_header = gvn().transform(new AndXNode(old_header, MakeConX(~markWord::larval_bit_in_place)));\n+  access_store_at(buffer, buffer, type->is_ptr(), new_header, TypeX_X, TypeX_X->basic_type(), MO_UNORDERED | IN_HEAP);\n+\n+  \/\/ We must ensure that the buffer is properly published\n+  insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out(AllocateNode::RawAddress));\n+  assert(!type->maybe_null(), \"result of an allocation should not be null\");\n+  set_result(InlineTypeNode::make_from_oop(this, buffer, type->inline_klass()));\n+  return true;\n+}\n+\n@@ -2838,0 +3178,13 @@\n+    if (oldval != nullptr && oldval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      oldval = oldval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+    if (newval != nullptr && newval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      newval = newval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+\n@@ -2906,0 +3259,98 @@\n+\/\/ private native int arrayInstanceBaseOffset0(Object[] array);\n+bool LibraryCallKit::inline_arrayInstanceBaseOffset() {\n+  Node* array = argument(1);\n+  Node* klass_node = load_object_klass(array);\n+\n+  jint  layout_con = Klass::_lh_neutral_value;\n+  Node* layout_val = get_layout_helper(klass_node, layout_con);\n+  int   layout_is_con = (layout_val == nullptr);\n+\n+  Node* header_size = nullptr;\n+  if (layout_is_con) {\n+    int hsize = Klass::layout_helper_header_size(layout_con);\n+    header_size = intcon(hsize);\n+  } else {\n+    Node* hss = intcon(Klass::_lh_header_size_shift);\n+    Node* hsm = intcon(Klass::_lh_header_size_mask);\n+    header_size = _gvn.transform(new URShiftINode(layout_val, hss));\n+    header_size = _gvn.transform(new AndINode(header_size, hsm));\n+  }\n+  set_result(header_size);\n+  return true;\n+}\n+\n+\/\/ private native int arrayInstanceIndexScale0(Object[] array);\n+bool LibraryCallKit::inline_arrayInstanceIndexScale() {\n+  Node* array = argument(1);\n+  Node* klass_node = load_object_klass(array);\n+\n+  jint  layout_con = Klass::_lh_neutral_value;\n+  Node* layout_val = get_layout_helper(klass_node, layout_con);\n+  int   layout_is_con = (layout_val == nullptr);\n+\n+  Node* element_size = nullptr;\n+  if (layout_is_con) {\n+    int log_element_size  = Klass::layout_helper_log2_element_size(layout_con);\n+    int elem_size = 1 << log_element_size;\n+    element_size = intcon(elem_size);\n+  } else {\n+    Node* ess = intcon(Klass::_lh_log2_element_size_shift);\n+    Node* esm = intcon(Klass::_lh_log2_element_size_mask);\n+    Node* log_element_size = _gvn.transform(new URShiftINode(layout_val, ess));\n+    log_element_size = _gvn.transform(new AndINode(log_element_size, esm));\n+    element_size = _gvn.transform(new LShiftINode(intcon(1), log_element_size));\n+  }\n+  set_result(element_size);\n+  return true;\n+}\n+\n+\/\/ private native int arrayLayout0(Object[] array);\n+bool LibraryCallKit::inline_arrayLayout() {\n+  RegionNode* region = new RegionNode(2);\n+  Node* phi = new PhiNode(region, TypeInt::POS);\n+\n+  Node* array = argument(1);\n+  Node* klass_node = load_object_klass(array);\n+  generate_refArray_guard(klass_node, region);\n+  if (region->req() == 3) {\n+    phi->add_req(intcon((jint)LayoutKind::REFERENCE));\n+  }\n+\n+  int layout_kind_offset = in_bytes(FlatArrayKlass::layout_kind_offset());\n+  Node* layout_kind_addr = basic_plus_adr(klass_node, layout_kind_offset);\n+  Node* layout_kind = make_load(nullptr, layout_kind_addr, TypeInt::POS, T_INT, MemNode::unordered);\n+\n+  region->init_req(1, control());\n+  phi->init_req(1, layout_kind);\n+\n+  set_control(_gvn.transform(region));\n+  set_result(_gvn.transform(phi));\n+  return true;\n+}\n+\n+\/\/ private native int[] getFieldMap0(Class <?> c);\n+\/\/   int offset = c._klass._acmp_maps_offset;\n+\/\/   return (int[])c.obj_field(offset);\n+bool LibraryCallKit::inline_getFieldMap() {\n+  if (!UseAltSubstitutabilityMethod) {\n+    return false;\n+  }\n+\n+  Node* mirror = argument(1);\n+  Node* klass = load_klass_from_mirror(mirror, false, nullptr, 0);\n+\n+  int field_map_offset_offset = in_bytes(InstanceKlass::acmp_maps_offset_offset());\n+  Node* field_map_offset_addr = basic_plus_adr(klass, field_map_offset_offset);\n+  Node* field_map_offset = make_load(nullptr, field_map_offset_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+  field_map_offset = _gvn.transform(ConvI2L(field_map_offset));\n+\n+  Node* map_addr = basic_plus_adr(mirror, field_map_offset);\n+  const TypeAryPtr* val_type = TypeAryPtr::INTS->cast_to_ptr_type(TypePtr::NotNull)->with_offset(0);\n+  \/\/ TODO 8350865 Remove this\n+  val_type = val_type->cast_to_not_flat(true)->cast_to_not_null_free(true);\n+  Node* map = access_load_at(mirror, map_addr, TypeAryPtr::INTS, val_type, T_ARRAY, IN_HEAP | MO_UNORDERED);\n+\n+  set_result(map);\n+  return true;\n+}\n+\n@@ -3024,2 +3475,7 @@\n-\n-  Node* obj = new_instance(kls, test);\n+  Node* obj = nullptr;\n+  const TypeInstKlassPtr* tkls = _gvn.type(kls)->isa_instklassptr();\n+  if (tkls != nullptr && tkls->instance_klass()->is_inlinetype()) {\n+    obj = InlineTypeNode::make_all_zero(_gvn, tkls->instance_klass()->as_inline_klass())->buffer(this);\n+  } else {\n+    obj = new_instance(kls, test);\n+  }\n@@ -3840,1 +4296,1 @@\n-  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true, true);\n@@ -3845,1 +4301,1 @@\n-  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, 0);\n+  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, TypeAryPtr::Offset(0));\n@@ -4021,0 +4477,1 @@\n+\n@@ -4146,10 +4603,12 @@\n-    p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));\n-    kls = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT_OR_NULL));\n-    null_ctl = top();\n-    kls = null_check_oop(kls, &null_ctl);\n-    if (null_ctl != top()) {\n-      \/\/ If the guard is taken, Object.superClass is null (both klass and mirror).\n-      region->add_req(null_ctl);\n-      phi   ->add_req(null());\n-    }\n-      query_value = load_mirror_from_klass(kls);\n+      p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));\n+      kls = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT_OR_NULL));\n+      null_ctl = top();\n+      kls = null_check_oop(kls, &null_ctl);\n+      if (null_ctl != top()) {\n+        \/\/ If the guard is taken, Object.superClass is null (both klass and mirror).\n+        region->add_req(null_ctl);\n+        phi   ->add_req(null());\n+      }\n+      if (!stopped()) {\n+        query_value = load_mirror_from_klass(kls);\n+      }\n@@ -4174,0 +4633,1 @@\n+\n@@ -4196,1 +4656,2 @@\n-      int static_res = C->static_subtype_check(TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces), tp->as_klass_type());\n+      const TypeKlassPtr* tklass = TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces);\n+      int static_res = C->static_subtype_check(tklass, tp->as_klass_type());\n@@ -4225,2 +4686,2 @@\n-  \/\/ Not-subtype or the mirror's klass ptr is null (in case it is a primitive).\n-  enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };\n+  \/\/ Not-subtype or the mirror's klass ptr is nullptr (in case it is a primitive).\n+  enum { _bad_type_path = 1, _prim_path = 2, _npe_path = 3, PATH_LIMIT };\n@@ -4236,0 +4697,2 @@\n+  Node* io = i_o();\n+  Node* mem = merged_memory();\n@@ -4237,0 +4700,1 @@\n+\n@@ -4243,1 +4707,2 @@\n-      region->in(_bad_type_path) != top()) {\n+      region->in(_bad_type_path) != top() ||\n+      region->in(_npe_path) != top()) {\n@@ -4247,0 +4712,3 @@\n+    \/\/ Set IO and memory because gen_checkcast may override them when buffering inline types\n+    set_i_o(io);\n+    set_all_memory(mem);\n@@ -4280,0 +4748,1 @@\n+  RegionNode* prim_region = new RegionNode(2);\n@@ -4282,0 +4751,1 @@\n+  record_for_igvn(prim_region);\n@@ -4306,2 +4776,5 @@\n-    int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);\n-    region->init_req(prim_path, null_ctl);\n+    if (which_arg == 0) {\n+      prim_region->init_req(1, null_ctl);\n+    } else {\n+      region->init_req(_prim_1_path, null_ctl);\n+    }\n@@ -4317,1 +4790,0 @@\n-    \/\/ now we have a successful reference subtype check\n@@ -4324,1 +4796,2 @@\n-  set_control(region->in(_prim_0_path)); \/\/ go back to first null check\n+  \/\/ This path is also used if superc is a value mirror.\n+  set_control(_gvn.transform(prim_region));\n@@ -4329,1 +4802,1 @@\n-    generate_guard(bol_eq, region, PROB_FAIR);\n+    generate_fair_guard(bol_eq, region);\n@@ -4360,2 +4833,1 @@\n-Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,\n-                                                  bool obj_array, bool not_array, Node** obj) {\n+Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind, Node** obj) {\n@@ -4367,9 +4839,0 @@\n-  \/\/ If obj_array\/non_array==false\/false:\n-  \/\/ Branch around if the given klass is in fact an array (either obj or prim).\n-  \/\/ If obj_array\/non_array==false\/true:\n-  \/\/ Branch around if the given klass is not an array klass of any kind.\n-  \/\/ If obj_array\/non_array==true\/true:\n-  \/\/ Branch around if the kls is not an oop array (kls is int[], String, etc.)\n-  \/\/ If obj_array\/non_array==true\/false:\n-  \/\/ Branch around if the kls is an oop array (Object[] or subtype)\n-  \/\/\n@@ -4380,4 +4843,11 @@\n-    bool query = (obj_array\n-                  ? Klass::layout_helper_is_objArray(layout_con)\n-                  : Klass::layout_helper_is_array(layout_con));\n-    if (query == not_array) {\n+    bool query = 0;\n+    switch(kind) {\n+      case RefArray:       query = Klass::layout_helper_is_refArray(layout_con); break;\n+      case NonRefArray:    query = !Klass::layout_helper_is_refArray(layout_con); break;\n+      case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;\n+      case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;\n+      case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    if (!query) {\n@@ -4393,0 +4863,21 @@\n+  unsigned int value = 0;\n+  BoolTest::mask btest = BoolTest::illegal;\n+  switch(kind) {\n+    case RefArray:\n+    case NonRefArray: {\n+      value = Klass::_lh_array_tag_ref_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = (kind == RefArray) ? BoolTest::eq : BoolTest::ne;\n+      break;\n+    }\n+    case TypeArray: {\n+      value = Klass::_lh_array_tag_type_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = BoolTest::eq;\n+      break;\n+    }\n+    case AnyArray:    value = Klass::_lh_neutral_value; btest = BoolTest::lt; break;\n+    case NonArray:    value = Klass::_lh_neutral_value; btest = BoolTest::gt; break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -4394,4 +4885,1 @@\n-  jint  nval = (obj_array\n-                ? (jint)(Klass::_lh_array_tag_type_value\n-                   <<    Klass::_lh_array_tag_shift)\n-                : Klass::_lh_neutral_value);\n+  jint nval = (jint)value;\n@@ -4399,3 +4887,0 @@\n-  BoolTest::mask btest = BoolTest::lt;  \/\/ correct for testing is_[obj]array\n-  \/\/ invert the test if we are looking for a non-array\n-  if (not_array)  btest = BoolTest(btest).negate();\n@@ -4404,1 +4889,1 @@\n-  Node* is_array_ctrl = not_array ? control() : ctrl;\n+  Node* is_array_ctrl = kind == NonArray ? control() : ctrl;\n@@ -4413,0 +4898,129 @@\n+\/\/ public static native Object[] ValueClass::newNullRestrictedAtomicArray(Class<?> componentType, int length, Object initVal);\n+\/\/ public static native Object[] ValueClass::newNullRestrictedNonAtomicArray(Class<?> componentType, int length, Object initVal);\n+\/\/ public static native Object[] ValueClass::newNullableAtomicArray(Class<?> componentType, int length);\n+bool LibraryCallKit::inline_newArray(bool null_free, bool atomic) {\n+  assert(null_free || atomic, \"nullable implies atomic\");\n+  Node* componentType = argument(0);\n+  Node* length = argument(1);\n+  Node* init_val = null_free ? argument(2) : nullptr;\n+\n+  const TypeInstPtr* tp = _gvn.type(componentType)->isa_instptr();\n+  if (tp != nullptr) {\n+    ciInstanceKlass* ik = tp->instance_klass();\n+    if (ik == C->env()->Class_klass()) {\n+      ciType* t = tp->java_mirror_type();\n+      if (t != nullptr && t->is_inlinetype()) {\n+\n+        ciArrayKlass* array_klass = ciArrayKlass::make(t, null_free, atomic, true);\n+        assert(array_klass->is_elem_null_free() == null_free, \"inconsistency\");\n+\n+        \/\/ TOOD 8350865 ZGC needs card marks on initializing oop stores\n+        if (UseZGC && null_free && !array_klass->is_flat_array_klass()) {\n+          return false;\n+        }\n+\n+        if (array_klass->is_loaded() && array_klass->element_klass()->as_inline_klass()->is_initialized()) {\n+          const TypeAryKlassPtr* array_klass_type = TypeAryKlassPtr::make(array_klass, Type::trust_interfaces);\n+          if (null_free) {\n+            if (init_val->is_InlineType()) {\n+              if (array_klass_type->is_flat() && init_val->as_InlineType()->is_all_zero(&gvn(), \/* flat *\/ true)) {\n+                \/\/ Zeroing is enough because the init value is the all-zero value\n+                init_val = nullptr;\n+              } else {\n+                init_val = init_val->as_InlineType()->buffer(this);\n+              }\n+            }\n+            \/\/ TODO 8350865 Should we add a check of the init_val type (maybe in debug only + halt)?\n+          }\n+          Node* obj = new_array(makecon(array_klass_type), length, 0, nullptr, false, init_val);\n+          const TypeAryPtr* arytype = gvn().type(obj)->is_aryptr();\n+          assert(arytype->is_null_free() == null_free, \"inconsistency\");\n+          assert(arytype->is_not_null_free() == !null_free, \"inconsistency\");\n+          set_result(obj);\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+\/\/ public static native boolean ValueClass::isFlatArray(Object array);\n+\/\/ public static native boolean ValueClass::isNullRestrictedArray(Object array);\n+\/\/ public static native boolean ValueClass::isAtomicArray(Object array);\n+bool LibraryCallKit::inline_getArrayProperties(ArrayPropertiesCheck check) {\n+  Node* array = argument(0);\n+\n+  Node* bol;\n+  switch(check) {\n+    case IsFlat:\n+      \/\/ TODO 8350865 Use the object version here instead of loading the klass\n+      \/\/ The problem is that PhaseMacroExpand::expand_flatarraycheck_node can only handle some IR shapes and will fail, for example, if the bol is directly wired to a ReturnNode\n+      bol = flat_array_test(load_object_klass(array));\n+      break;\n+    case IsNullRestricted:\n+      bol = null_free_array_test(array);\n+      break;\n+    case IsAtomic:\n+      \/\/ TODO 8350865 Implement this. It's a bit more complicated, see conditions in JVM_IsAtomicArray\n+      \/\/ Enable TestIntrinsics::test87\/88 once this is implemented\n+      \/\/ bol = null_free_atomic_array_test\n+      return false;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+\n+  Node* res = gvn().transform(new CMoveINode(bol, intcon(0), intcon(1), TypeInt::BOOL));\n+  set_result(res);\n+  return true;\n+}\n+\n+\/\/ Load the default refined array klass from an ObjArrayKlass. This relies on the first entry in the\n+\/\/ '_next_refined_array_klass' linked list being the default (see ObjArrayKlass::klass_with_properties).\n+Node* LibraryCallKit::load_default_refined_array_klass(Node* klass_node, bool type_array_guard) {\n+  RegionNode* region = new RegionNode(2);\n+  Node* phi = new PhiNode(region, TypeInstKlassPtr::OBJECT_OR_NULL);\n+\n+  if (type_array_guard) {\n+    generate_typeArray_guard(klass_node, region);\n+    if (region->req() == 3) {\n+      phi->add_req(klass_node);\n+    }\n+  }\n+  Node* adr_refined_klass = basic_plus_adr(klass_node, in_bytes(ObjArrayKlass::next_refined_array_klass_offset()));\n+  Node* refined_klass = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), adr_refined_klass, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT_OR_NULL));\n+\n+  \/\/ Can be null if not initialized yet, just deopt\n+  Node* null_ctl = top();\n+  refined_klass = null_check_oop(refined_klass, &null_ctl, \/* never_see_null= *\/ true);\n+\n+  region->init_req(1, control());\n+  phi->init_req(1, refined_klass);\n+\n+  set_control(_gvn.transform(region));\n+  return _gvn.transform(phi);\n+}\n+\n+\/\/ Load the non-refined array klass from an ObjArrayKlass.\n+Node* LibraryCallKit::load_non_refined_array_klass(Node* klass_node) {\n+  const TypeAryKlassPtr* ary_klass_ptr = _gvn.type(klass_node)->isa_aryklassptr();\n+  if (ary_klass_ptr != nullptr && ary_klass_ptr->klass_is_exact()) {\n+    return _gvn.makecon(ary_klass_ptr->cast_to_refined_array_klass_ptr(false));\n+  }\n+\n+  RegionNode* region = new RegionNode(2);\n+  Node* phi = new PhiNode(region, TypeInstKlassPtr::OBJECT);\n+\n+  generate_typeArray_guard(klass_node, region);\n+  if (region->req() == 3) {\n+    phi->add_req(klass_node);\n+  }\n+  Node* super_adr = basic_plus_adr(klass_node, in_bytes(Klass::super_offset()));\n+  Node* super_klass = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), super_adr, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT));\n+\n+  region->init_req(1, control());\n+  phi->init_req(1, super_klass);\n+\n+  set_control(_gvn.transform(region));\n+  return _gvn.transform(phi);\n+}\n@@ -4415,1 +5029,1 @@\n-\/\/ private static native Object java.lang.reflect.newArray(Class<?> componentType, int length);\n+\/\/ private static native Object java.lang.reflect.Array.newArray(Class<?> componentType, int length);\n@@ -4473,0 +5087,3 @@\n+\n+    klass_node = load_default_refined_array_klass(klass_node);\n+\n@@ -4561,1 +5178,16 @@\n-    Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);\n+    \/\/ Inline type array may have object field that would require a\n+    \/\/ write barrier. Conservatively, go to slow path.\n+    \/\/ TODO 8251971: Optimize for the case when flat src\/dst are later found\n+    \/\/ to not contain oops (i.e., move this check to the macro expansion phase).\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    const TypeAryPtr* orig_t = _gvn.type(original)->isa_aryptr();\n+    const TypeKlassPtr* tklass = _gvn.type(klass_node)->is_klassptr();\n+    bool exclude_flat = UseArrayFlattening && bs->array_copy_requires_gc_barriers(true, T_OBJECT, false, false, BarrierSetC2::Parsing) &&\n+                        \/\/ Can src array be flat and contain oops?\n+                        (orig_t == nullptr || (!orig_t->is_not_flat() && (!orig_t->is_flat() || orig_t->elem()->inline_klass()->contains_oops()))) &&\n+                        \/\/ Can dest array be flat and contain oops?\n+                        tklass->can_be_inline_array() && (!tklass->is_flat() || tklass->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->as_inline_klass()->contains_oops());\n+    Node* not_objArray = exclude_flat ? generate_non_refArray_guard(klass_node, bailout) : generate_typeArray_guard(klass_node, bailout);\n+\n+    Node* refined_klass_node = load_default_refined_array_klass(klass_node, \/* type_array_guard= *\/ false);\n+\n@@ -4565,3 +5197,4 @@\n-      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0\/*offset*\/);\n-      Node* cast = new CastPPNode(control(), klass_node, akls);\n-      klass_node = _gvn.transform(cast);\n+      bool not_flat = !UseArrayFlattening;\n+      const Type* akls = TypeAryKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0), Type::trust_interfaces, not_flat, false, false, false, not_flat, true);\n+      Node* cast = new CastPPNode(control(), refined_klass_node, akls);\n+      refined_klass_node = _gvn.transform(cast);\n@@ -4585,0 +5218,39 @@\n+    \/\/ Handle inline type arrays\n+    bool can_validate = !too_many_traps(Deoptimization::Reason_class_check);\n+    if (!stopped()) {\n+      \/\/ TODO 8251971\n+      if (!orig_t->is_null_free()) {\n+        \/\/ Not statically known to be null free, add a check\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+      orig_t = _gvn.type(original)->isa_aryptr();\n+      if (orig_t != nullptr && orig_t->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (exclude_flat) {\n+          \/\/ Dest can't be flat, bail out\n+          bailout->add_req(control());\n+          set_control(top());\n+        } else {\n+          generate_fair_guard(flat_array_test(refined_klass_node, \/* flat = *\/ false), bailout);\n+        }\n+        \/\/ TODO 8350865 This is not correct anymore. Write tests and fix logic similar to arraycopy.\n+      } else if (UseArrayFlattening && (orig_t == nullptr || !orig_t->is_not_flat()) &&\n+                 \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check if validated).\n+                 ((!tklass->is_flat() && tklass->can_be_inline_array()) || !can_validate)) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        generate_fair_guard(flat_array_test(load_object_klass(original)), bailout);\n+        if (orig_t != nullptr) {\n+          orig_t = orig_t->cast_to_not_flat();\n+          original = _gvn.transform(new CheckCastPPNode(control(), original, orig_t));\n+        }\n+      }\n+      if (!can_validate) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat\/null-free.\n+        generate_fair_guard(flat_array_test(refined_klass_node), bailout);\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+    }\n+\n@@ -4630,1 +5302,1 @@\n-      if (!too_many_traps(Deoptimization::Reason_class_check)) {\n+      if (can_validate) {\n@@ -4644,1 +5316,1 @@\n-        newcopy = new_array(klass_node, length, 0);  \/\/ no arguments to push\n+        newcopy = new_array(refined_klass_node, length, 0);  \/\/ no arguments to push\n@@ -4716,1 +5388,1 @@\n-    const TypeTuple* range = tf->range();\n+    const TypeTuple* range = tf->range_cc();\n@@ -4720,1 +5392,1 @@\n-    tf = TypeFunc::make(tf->domain(), new_range);\n+    tf = TypeFunc::make(tf->domain_cc(), new_range);\n@@ -4777,1 +5449,8 @@\n-  Node* obj = nullptr;\n+  Node* obj = argument(0);\n+\n+  \/\/ Don't intrinsify hashcode on inline types for now.\n+  \/\/ The \"is locked\" runtime check also subsumes the inline type check (as inline types cannot be locked) and goes to the slow path.\n+  if (gvn().type(obj)->is_inlinetypeptr()) {\n+    return false;\n+  }\n+\n@@ -4787,1 +5466,0 @@\n-    obj = argument(0);\n@@ -4828,0 +5506,2 @@\n+    \/\/ We cannot use the inline type mask as this may check bits that are overriden\n+    \/\/ by an object monitor's pointer when inflating locking.\n@@ -4895,1 +5575,10 @@\n-  Node* obj = null_check_receiver();\n+  Node* obj = argument(0);\n+  if (obj->is_InlineType()) {\n+    const Type* t = _gvn.type(obj);\n+    if (t->maybe_null()) {\n+      null_check(obj);\n+    }\n+    set_result(makecon(TypeInstPtr::make(t->inline_klass()->java_mirror())));\n+    return true;\n+  }\n+  obj = null_check_receiver();\n@@ -5317,1 +6006,2 @@\n-    Node* obj = null_check_receiver();\n+    Node* obj = argument(0);\n+    obj = null_check_receiver();\n@@ -5321,0 +6011,6 @@\n+    if (obj_type->is_inlinetypeptr()) {\n+      \/\/ If the object to clone is an inline type, we can simply return it (i.e. a nop) since inline types have\n+      \/\/ no identity.\n+      set_result(obj);\n+      return true;\n+    }\n@@ -5327,1 +6023,2 @@\n-        obj_type->speculative_type()->is_instance_klass()) {\n+        obj_type->speculative_type()->is_instance_klass() &&\n+        !obj_type->speculative_type()->is_inlinetype()) {\n@@ -5357,0 +6054,5 @@\n+    \/\/ We only go to the fast case code if we pass a number of guards.\n+    \/\/ The paths which do not pass are accumulated in the slow_region.\n+    RegionNode* slow_region = new RegionNode(1);\n+    record_for_igvn(slow_region);\n+\n@@ -5363,3 +6065,0 @@\n-      Node* obj_length = load_array_length(array_obj);\n-      Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n-      Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n@@ -5368,20 +6067,7 @@\n-      if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n-        \/\/ If it is an oop array, it requires very special treatment,\n-        \/\/ because gc barriers are required when accessing the array.\n-        Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)nullptr);\n-        if (is_obja != nullptr) {\n-          PreserveJVMState pjvms2(this);\n-          set_control(is_obja);\n-          \/\/ Generate a direct call to the right arraycopy function(s).\n-          \/\/ Clones are always tightly coupled.\n-          ArrayCopyNode* ac = ArrayCopyNode::make(this, true, array_obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n-          ac->set_clone_oop_array();\n-          Node* n = _gvn.transform(ac);\n-          assert(n == ac, \"cannot disappear\");\n-          ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n-\n-          result_reg->init_req(_objArray_path, control());\n-          result_val->init_req(_objArray_path, alloc_obj);\n-          result_i_o ->set_req(_objArray_path, i_o());\n-          result_mem ->set_req(_objArray_path, reset_memory());\n-        }\n+      const TypeAryPtr* ary_ptr = obj_type->isa_aryptr();\n+      if (UseArrayFlattening && bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Expansion) &&\n+          obj_type->can_be_inline_array() &&\n+          (ary_ptr == nullptr || (!ary_ptr->is_not_flat() && (!ary_ptr->is_flat() || ary_ptr->elem()->inline_klass()->contains_oops())))) {\n+        \/\/ Flat inline type array may have object field that would require a\n+        \/\/ write barrier. Conservatively, go to slow path.\n+        generate_fair_guard(flat_array_test(obj_klass), slow_region);\n@@ -5389,7 +6075,0 @@\n-      \/\/ Otherwise, there are no barriers to worry about.\n-      \/\/ (We can dispense with card marks if we know the allocation\n-      \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n-      \/\/  causes the non-eden paths to take compensating steps to\n-      \/\/  simulate a fresh allocation, so that no further\n-      \/\/  card marks are required in compiled code to initialize\n-      \/\/  the object.)\n@@ -5398,7 +6077,43 @@\n-        copy_to_clone(array_obj, alloc_obj, array_size, true);\n-\n-        \/\/ Present the results of the copy.\n-        result_reg->init_req(_array_path, control());\n-        result_val->init_req(_array_path, alloc_obj);\n-        result_i_o ->set_req(_array_path, i_o());\n-        result_mem ->set_req(_array_path, reset_memory());\n+        Node* obj_length = load_array_length(array_obj);\n+        Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n+        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n+\n+        BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+        if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n+          \/\/ If it is an oop array, it requires very special treatment,\n+          \/\/ because gc barriers are required when accessing the array.\n+          Node* is_obja = generate_refArray_guard(obj_klass, (RegionNode*)nullptr);\n+          if (is_obja != nullptr) {\n+            PreserveJVMState pjvms2(this);\n+            set_control(is_obja);\n+            \/\/ Generate a direct call to the right arraycopy function(s).\n+            \/\/ Clones are always tightly coupled.\n+            ArrayCopyNode* ac = ArrayCopyNode::make(this, true, array_obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n+            ac->set_clone_oop_array();\n+            Node* n = _gvn.transform(ac);\n+            assert(n == ac, \"cannot disappear\");\n+            ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n+\n+            result_reg->init_req(_objArray_path, control());\n+            result_val->init_req(_objArray_path, alloc_obj);\n+            result_i_o ->set_req(_objArray_path, i_o());\n+            result_mem ->set_req(_objArray_path, reset_memory());\n+          }\n+        }\n+        \/\/ Otherwise, there are no barriers to worry about.\n+        \/\/ (We can dispense with card marks if we know the allocation\n+        \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n+        \/\/  causes the non-eden paths to take compensating steps to\n+        \/\/  simulate a fresh allocation, so that no further\n+        \/\/  card marks are required in compiled code to initialize\n+        \/\/  the object.)\n+\n+        if (!stopped()) {\n+          copy_to_clone(obj, alloc_obj, array_size, true);\n+\n+          \/\/ Present the results of the copy.\n+          result_reg->init_req(_array_path, control());\n+          result_val->init_req(_array_path, alloc_obj);\n+          result_i_o ->set_req(_array_path, i_o());\n+          result_mem ->set_req(_array_path, reset_memory());\n+        }\n@@ -5408,4 +6123,0 @@\n-    \/\/ We only go to the instance fast case code if we pass a number of guards.\n-    \/\/ The paths which do not pass are accumulated in the slow_region.\n-    RegionNode* slow_region = new RegionNode(1);\n-    record_for_igvn(slow_region);\n@@ -5543,0 +6254,12 @@\n+  int adjustment = 1;\n+  const TypeAryKlassPtr* ary_klass_ptr = alloc->in(AllocateNode::KlassNode)->bottom_type()->is_aryklassptr();\n+  if (ary_klass_ptr->is_null_free()) {\n+    \/\/ A null-free, tightly coupled array allocation can only come from LibraryCallKit::inline_newArray which\n+    \/\/ also requires the componentType and initVal on stack for re-execution.\n+    \/\/ Re-create and push the componentType.\n+    ciArrayKlass* klass = ary_klass_ptr->exact_klass()->as_array_klass();\n+    ciInstance* instance = klass->component_mirror_instance();\n+    const TypeInstPtr* t_instance = TypeInstPtr::make(instance);\n+    sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), makecon(t_instance));\n+    adjustment++;\n+  }\n@@ -5544,5 +6267,16 @@\n-  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), alloc->in(AllocateNode::ALength));\n-  old_jvms->set_sp(old_jvms->sp()+1);\n-  old_jvms->set_monoff(old_jvms->monoff()+1);\n-  old_jvms->set_scloff(old_jvms->scloff()+1);\n-  old_jvms->set_endoff(old_jvms->endoff()+1);\n+  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp() + adjustment - 1, alloc->in(AllocateNode::ALength));\n+  if (ary_klass_ptr->is_null_free()) {\n+    \/\/ Re-create and push the initVal.\n+    Node* init_val = alloc->in(AllocateNode::InitValue);\n+    if (init_val == nullptr) {\n+      init_val = InlineTypeNode::make_all_zero(_gvn, ary_klass_ptr->elem()->is_instklassptr()->instance_klass()->as_inline_klass());\n+    } else if (UseCompressedOops) {\n+      init_val = _gvn.transform(new DecodeNNode(init_val, init_val->bottom_type()->make_ptr()));\n+    }\n+    sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp() + adjustment, init_val);\n+    adjustment++;\n+  }\n+  old_jvms->set_sp(old_jvms->sp() + adjustment);\n+  old_jvms->set_monoff(old_jvms->monoff() + adjustment);\n+  old_jvms->set_scloff(old_jvms->scloff() + adjustment);\n+  old_jvms->set_endoff(old_jvms->endoff() + adjustment);\n@@ -5581,2 +6315,1 @@\n-    CallProjections callprojs;\n-    alloc->extract_projections(&callprojs, true);\n+    CallProjections* callprojs = alloc->extract_projections(true);\n@@ -5585,1 +6318,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -5627,1 +6360,1 @@\n-    set_i_o(callprojs.fallthrough_ioproj);\n+    set_i_o(callprojs->fallthrough_ioproj);\n@@ -5977,1 +6710,1 @@\n-    if (src_elem == dest_elem && src_elem == T_OBJECT) {\n+    if (src_elem == dest_elem && top_src->is_flat() == top_dest->is_flat() && src_elem == T_OBJECT) {\n@@ -6004,0 +6737,2 @@\n+          src_type = _gvn.type(src);\n+          top_src = src_type->isa_aryptr();\n@@ -6007,0 +6742,2 @@\n+          dest_type = _gvn.type(dest);\n+          top_dest = dest_type->isa_aryptr();\n@@ -6022,2 +6759,1 @@\n-      can_emit_guards &&\n-      !src->is_top() && !dest->is_top()) {\n+      can_emit_guards && !src->is_top() && !dest->is_top()) {\n@@ -6064,0 +6800,1 @@\n+    Node* refined_dest_klass = dest_klass;\n@@ -6065,0 +6802,1 @@\n+      dest_klass = load_non_refined_array_klass(refined_dest_klass);\n@@ -6066,8 +6804,1 @@\n-\n-      if (not_subtype_ctrl != top()) {\n-        PreserveJVMState pjvms(this);\n-        set_control(not_subtype_ctrl);\n-        uncommon_trap(Deoptimization::Reason_intrinsic,\n-                      Deoptimization::Action_make_not_entrant);\n-        assert(stopped(), \"Should be stopped\");\n-      }\n+      slow_region->add_req(not_subtype_ctrl);\n@@ -6075,0 +6806,21 @@\n+\n+    \/\/ TODO 8350865 Improve this. What about atomicity? Make sure this is always folded for type arrays.\n+    \/\/ If destination is null-restricted, source must be null-restricted as well: src_null_restricted || !dst_null_restricted\n+    Node* src_klass = load_object_klass(src);\n+    Node* adr_prop_src = basic_plus_adr(src_klass, in_bytes(ArrayKlass::properties_offset()));\n+    Node* prop_src = _gvn.transform(LoadNode::make(_gvn, control(), immutable_memory(), adr_prop_src, TypeRawPtr::BOTTOM, TypeInt::INT, T_INT, MemNode::unordered));\n+    Node* adr_prop_dest = basic_plus_adr(refined_dest_klass, in_bytes(ArrayKlass::properties_offset()));\n+    Node* prop_dest = _gvn.transform(LoadNode::make(_gvn, control(), immutable_memory(), adr_prop_dest, TypeRawPtr::BOTTOM, TypeInt::INT, T_INT, MemNode::unordered));\n+\n+    prop_dest = _gvn.transform(new XorINode(prop_dest, intcon(ArrayKlass::ArrayProperties::NULL_RESTRICTED)));\n+    prop_src = _gvn.transform(new OrINode(prop_dest, prop_src));\n+    prop_src = _gvn.transform(new AndINode(prop_src, intcon(ArrayKlass::ArrayProperties::NULL_RESTRICTED)));\n+\n+    Node* chk = _gvn.transform(new CmpINode(prop_src, intcon(ArrayKlass::ArrayProperties::NULL_RESTRICTED)));\n+    Node* tst = _gvn.transform(new BoolNode(chk, BoolTest::ne));\n+    generate_fair_guard(tst, slow_region);\n+\n+    \/\/ TODO 8350865 This is too strong\n+    generate_fair_guard(flat_array_test(src), slow_region);\n+    generate_fair_guard(flat_array_test(dest), slow_region);\n+\n@@ -6083,2 +6835,10 @@\n-    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n-    const Type *toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n+    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->isa_klassptr();\n+    if (dest_klass_t == nullptr) {\n+      \/\/ refined_dest_klass may not be an array, which leads to dest_klass being top. This means we\n+      \/\/ are in a dead path.\n+      uncommon_trap(Deoptimization::Reason_intrinsic,\n+                    Deoptimization::Action_make_not_entrant);\n+      return true;\n+    }\n+\n+    const Type* toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n@@ -6093,0 +6853,3 @@\n+  Node* dest_klass = load_object_klass(dest);\n+  dest_klass = load_non_refined_array_klass(dest_klass);\n+\n@@ -6097,1 +6860,1 @@\n-                                          load_object_klass(src), load_object_klass(dest),\n+                                          load_object_klass(src), dest_klass,\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":895,"deletions":132,"binary":false,"changes":1027,"status":"modified"},{"patch":"@@ -128,1 +128,6 @@\n-  if (phase->find_unswitch_candidate(this) == nullptr) {\n+\n+  if (head->is_flat_arrays()) {\n+    return false;\n+  }\n+\n+  if (no_unswitch_candidate()) {\n@@ -136,0 +141,7 @@\n+\/\/ Check the absence of any If node that can be used for Loop Unswitching. In that case, no Loop Unswitching can be done.\n+bool IdealLoopTree::no_unswitch_candidate() const {\n+  ResourceMark rm;\n+  Node_List dont_care;\n+  return _phase->find_unswitch_candidates(this, dont_care) == nullptr;\n+}\n+\n@@ -137,2 +149,39 @@\n-\/\/ one in the loop body. Return the \"unswitch candidate\" If to apply Loop Unswitching on.\n-IfNode* PhaseIdealLoop::find_unswitch_candidate(const IdealLoopTree* loop) const {\n+\/\/ one in the loop body as \"unswitch candidate\" to apply Loop Unswitching on.\n+\/\/ Depending on whether we find such a candidate and if we do, whether it's a flat array check, we do the following:\n+\/\/ (1) Candidate is not a flat array check:\n+\/\/     Return the unique unswitch candidate.\n+\/\/ (2) Candidate is a flat array check:\n+\/\/     Collect all remaining non-loop-exiting flat array checks in the loop body in the provided 'flat_array_checks'\n+\/\/     list in order to create an unswitched loop version without any flat array checks and a version with checks\n+\/\/     (i.e. same as original loop). Return the initially found candidate which could be unique if no further flat array\n+\/\/     checks are found.\n+\/\/ (3) No candidate is initially found:\n+\/\/     As in (2), we collect all non-loop-exiting flat array checks in the loop body in the provided 'flat_array_checks'\n+\/\/     list. Pick the first collected flat array check as unswitch candidate, which could be unique, and return it (a).\n+\/\/     If there are no flat array checks, we cannot apply Loop Unswitching (b).\n+\/\/\n+\/\/ Note that for both (2) and (3a), if there are multiple flat array checks, then the candidate's FlatArrayCheckNode is\n+\/\/ later updated in Loop Unswitching to perform a flat array check on all collected flat array checks.\n+IfNode* PhaseIdealLoop::find_unswitch_candidates(const IdealLoopTree* loop, Node_List& flat_array_checks) const {\n+  IfNode* unswitch_candidate = find_unswitch_candidate_from_idoms(loop);\n+  if (unswitch_candidate != nullptr && !unswitch_candidate->is_flat_array_check(&_igvn)) {\n+    \/\/ Case (1)\n+    return unswitch_candidate;\n+  }\n+\n+  collect_flat_array_checks(loop, flat_array_checks);\n+  if (unswitch_candidate != nullptr) {\n+    \/\/ Case (2)\n+    assert(unswitch_candidate->is_flat_array_check(&_igvn), \"is a flat array check\");\n+    return unswitch_candidate;\n+  } else if (flat_array_checks.size() > 0) {\n+    \/\/ Case (3a): Pick first one found as candidate (there could be multiple).\n+    return flat_array_checks[0]->as_If();\n+  }\n+\n+  \/\/ Case (3b): No suitable unswitch candidate found.\n+  return nullptr;\n+}\n+\n+\/\/ Find an unswitch candidate by following the idom chain from the loop back edge.\n+IfNode* PhaseIdealLoop::find_unswitch_candidate_from_idoms(const IdealLoopTree* loop) const {\n@@ -165,0 +214,144 @@\n+\/\/ Collect all flat array checks in the provided 'flat_array_checks' list.\n+void PhaseIdealLoop::collect_flat_array_checks(const IdealLoopTree* loop, Node_List& flat_array_checks) const {\n+  assert(flat_array_checks.size() == 0, \"should be empty initially\");\n+  for (uint i = 0; i < loop->_body.size(); i++) {\n+    Node* next = loop->_body.at(i);\n+    if (next->is_If() && next->as_If()->is_flat_array_check(&_igvn) && loop->is_invariant(next->in(1)) &&\n+        !loop->is_loop_exit(next)) {\n+      flat_array_checks.push(next);\n+    }\n+  }\n+}\n+\n+\/\/ This class represents an \"unswitch candidate\" which is an If that can be used to perform Loop Unswitching on. If the\n+\/\/ candidate is a flat array check candidate, then we also collect all remaining non-loop-exiting flat array checks.\n+\/\/ These are candidates as well. We want to get rid of all these flat array checks in the true-path-loop for the\n+\/\/ following reason:\n+\/\/\n+\/\/ FlatArrayCheckNodes are used with array accesses to switch between a flat and a non-flat array access. We want\n+\/\/ the performance impact on non-flat array accesses to be as small as possible. We therefore create the following\n+\/\/ loops in Loop Unswitching:\n+\/\/ - True-path-loop:  We remove all non-loop-exiting flat array checks to get a loop with only non-flat array accesses\n+\/\/                    (i.e. a fast path loop).\n+\/\/ - False-path-loop: We keep all flat array checks in this loop (i.e. a slow path loop).\n+class UnswitchCandidate : public StackObj {\n+  PhaseIdealLoop* const _phase;\n+  const Node_List& _old_new;\n+  Node* const _original_loop_entry;\n+  \/\/ If _candidate is a flat array check, this list contains all non-loop-exiting flat array checks in the loop body.\n+  Node_List _flat_array_check_candidates;\n+  IfNode* const _candidate;\n+\n+ public:\n+  UnswitchCandidate(IdealLoopTree* loop, const Node_List& old_new)\n+      : _phase(loop->_phase),\n+        _old_new(old_new),\n+        _original_loop_entry(loop->_head->as_Loop()->skip_strip_mined()->in(LoopNode::EntryControl)),\n+        _flat_array_check_candidates(),\n+        _candidate(find_unswitch_candidate(loop)) {}\n+  NONCOPYABLE(UnswitchCandidate);\n+\n+  IfNode* find_unswitch_candidate(IdealLoopTree* loop) {\n+    IfNode* unswitch_candidate = _phase->find_unswitch_candidates(loop, _flat_array_check_candidates);\n+    assert(unswitch_candidate != nullptr, \"guaranteed to exist by policy_unswitching\");\n+    assert(_phase->is_member(loop, unswitch_candidate), \"must be inside original loop\");\n+    return unswitch_candidate;\n+  }\n+\n+  IfNode* candidate() const {\n+    return _candidate;\n+  }\n+\n+  \/\/ Is the candidate a flat array check and are there other flat array checks as well?\n+  bool has_multiple_flat_array_check_candidates() const {\n+    return _flat_array_check_candidates.size() > 1;\n+  }\n+\n+  \/\/ Remove all candidates from the true-path-loop which are now dominated by the loop selector\n+  \/\/ (i.e. 'true_path_loop_proj'). The removed candidates are folded in the next IGVN round.\n+  void update_in_true_path_loop(IfTrueNode* true_path_loop_proj) const {\n+    remove_from_loop(true_path_loop_proj, _candidate);\n+    if (has_multiple_flat_array_check_candidates()) {\n+      remove_flat_array_checks(true_path_loop_proj);\n+    }\n+  }\n+\n+  \/\/ Remove a unique candidate from the false-path-loop which is now dominated by the loop selector\n+  \/\/ (i.e. 'false_path_loop_proj'). The removed candidate is folded in the next IGVN round. If there are multiple\n+  \/\/ candidates (i.e. flat array checks), then we leave them in the false-path-loop and only mark the loop such that it\n+  \/\/ is not unswitched anymore in later loop opts rounds.\n+  void update_in_false_path_loop(IfFalseNode* false_path_loop_proj, LoopNode* false_path_loop) const {\n+    if (has_multiple_flat_array_check_candidates()) {\n+      \/\/ Leave the flat array checks in the false-path-loop and prevent it from being unswitched again based on these\n+      \/\/ checks.\n+      false_path_loop->mark_flat_arrays();\n+    } else {\n+      remove_from_loop(false_path_loop_proj, _old_new[_candidate->_idx]->as_If());\n+    }\n+  }\n+\n+ private:\n+  void remove_from_loop(IfProjNode* dominating_proj, IfNode* candidate) const {\n+    _phase->igvn().rehash_node_delayed(candidate);\n+    _phase->dominated_by(dominating_proj, candidate);\n+  }\n+\n+  void remove_flat_array_checks(IfProjNode* dominating_proj) const {\n+    for (uint i = 0; i < _flat_array_check_candidates.size(); i++) {\n+      IfNode* flat_array_check = _flat_array_check_candidates.at(i)->as_If();\n+      _phase->igvn().rehash_node_delayed(flat_array_check);\n+      _phase->dominated_by(dominating_proj, flat_array_check);\n+    }\n+  }\n+\n+ public:\n+  \/\/ Merge all flat array checks into a single new BoolNode and return it.\n+  BoolNode* merge_flat_array_checks() const {\n+    assert(has_multiple_flat_array_check_candidates(), \"must have multiple flat array checks to merge\");\n+    assert(_candidate->in(1)->as_Bool()->_test._test == BoolTest::ne, \"IfTrue proj must point to flat array\");\n+    BoolNode* merged_flat_array_check_bool = create_bool_node();\n+    create_flat_array_check_node(merged_flat_array_check_bool);\n+    return merged_flat_array_check_bool;\n+  }\n+\n+ private:\n+  BoolNode* create_bool_node() const {\n+    BoolNode* merged_flat_array_check_bool = _candidate->in(1)->clone()->as_Bool();\n+    _phase->register_new_node(merged_flat_array_check_bool, _original_loop_entry);\n+    return merged_flat_array_check_bool;\n+  }\n+\n+  void create_flat_array_check_node(BoolNode* merged_flat_array_check_bool) const {\n+    FlatArrayCheckNode* cloned_flat_array_check = merged_flat_array_check_bool->in(1)->clone()->as_FlatArrayCheck();\n+    _phase->register_new_node(cloned_flat_array_check, _original_loop_entry);\n+    merged_flat_array_check_bool->set_req(1, cloned_flat_array_check);\n+    set_flat_array_check_inputs(cloned_flat_array_check);\n+  }\n+\n+  \/\/ Combine all checks into a single one that fails if one array is flat.\n+  void set_flat_array_check_inputs(FlatArrayCheckNode* cloned_flat_array_check) const {\n+    assert(cloned_flat_array_check->req() == 3, \"unexpected number of inputs for FlatArrayCheck\");\n+    cloned_flat_array_check->add_req_batch(_phase->C->top(), _flat_array_check_candidates.size() - 1);\n+    for (uint i = 0; i < _flat_array_check_candidates.size(); i++) {\n+      Node* array = _flat_array_check_candidates.at(i)->in(1)->in(1)->in(FlatArrayCheckNode::ArrayOrKlass);\n+      cloned_flat_array_check->set_req(FlatArrayCheckNode::ArrayOrKlass + i, array);\n+    }\n+  }\n+\n+ public:\n+#ifndef PRODUCT\n+  void trace_flat_array_checks() const {\n+    if (has_multiple_flat_array_check_candidates()) {\n+      tty->print_cr(\"- Unswitched and Merged Flat Array Checks:\");\n+      for (uint i = 0; i < _flat_array_check_candidates.size(); i++) {\n+        Node* unswitch_iff = _flat_array_check_candidates.at(i);\n+        Node* cloned_unswitch_iff = _old_new[unswitch_iff->_idx];\n+        assert(cloned_unswitch_iff != nullptr, \"must exist\");\n+        tty->print_cr(\"  - %d %s  ->  %d %s\", unswitch_iff->_idx, unswitch_iff->Name(),\n+                      cloned_unswitch_iff->_idx, cloned_unswitch_iff->Name());\n+      }\n+    }\n+  }\n+#endif \/\/ NOT PRODUCT\n+};\n+\n@@ -179,1 +372,3 @@\n-  enum PathToLoop { TRUE_PATH, FALSE_PATH };\n+  enum PathToLoop {\n+    TRUE_PATH, FALSE_PATH\n+  };\n@@ -195,1 +390,1 @@\n-  LoopSelector(IdealLoopTree* loop, IfNode* unswitch_candidate)\n+  LoopSelector(IdealLoopTree* loop, const UnswitchCandidate& unswitch_candidate)\n@@ -206,0 +401,1 @@\n+ private:\n@@ -213,1 +409,2 @@\n-  IfNode* create_unswitching_if(IfNode* unswitch_candidate) {\n+  IfNode* create_unswitching_if(const UnswitchCandidate& unswitch_candidate) {\n+    const uint dom_depth = _phase->dom_depth(_original_loop_entry);\n@@ -215,4 +412,9 @@\n-    BoolNode* unswitch_candidate_bool = unswitch_candidate->in(1)->as_Bool();\n-    IfNode* selector_if = IfNode::make_with_same_profile(unswitch_candidate, _original_loop_entry,\n-                                                         unswitch_candidate_bool);\n-    _phase->register_node(selector_if, _outer_loop, _original_loop_entry, _dom_depth);\n+    IfNode* unswitch_candidate_if = unswitch_candidate.candidate();\n+    BoolNode* selector_bool;\n+    if (unswitch_candidate.has_multiple_flat_array_check_candidates()) {\n+      selector_bool = unswitch_candidate.merge_flat_array_checks();\n+    } else {\n+      selector_bool = unswitch_candidate_if->in(1)->as_Bool();\n+    }\n+    IfNode* selector_if = IfNode::make_with_same_profile(unswitch_candidate_if, _original_loop_entry, selector_bool);\n+    _phase->register_node(selector_if, _outer_loop, _original_loop_entry, dom_depth);\n@@ -222,1 +424,0 @@\n- private:\n@@ -252,1 +453,1 @@\n-  IfNode* const _unswitch_candidate;\n+  const UnswitchCandidate& _unswitch_candidate;\n@@ -256,2 +457,2 @@\n-  UnswitchedLoopSelector(IdealLoopTree* loop)\n-      : _unswitch_candidate(find_unswitch_candidate(loop)),\n+  UnswitchedLoopSelector(IdealLoopTree* loop, const UnswitchCandidate& unswitch_candidate)\n+      : _unswitch_candidate(unswitch_candidate),\n@@ -261,11 +462,2 @@\n- private:\n-  static IfNode* find_unswitch_candidate(IdealLoopTree* loop) {\n-    IfNode* unswitch_candidate = loop->_phase->find_unswitch_candidate(loop);\n-    assert(unswitch_candidate != nullptr, \"guaranteed to exist by policy_unswitching\");\n-    assert(loop->_phase->is_member(loop, unswitch_candidate), \"must be inside original loop\");\n-    return unswitch_candidate;\n-  }\n-\n- public:\n-  IfNode* unswitch_candidate() const {\n-    return _unswitch_candidate;\n+  IfNode* selector_if() const {\n+    return _loop_selector.selector();\n@@ -301,1 +493,0 @@\n-    remove_unswitch_candidate_from_loops(unswitched_loop_selector);\n@@ -366,14 +557,0 @@\n-\n-  \/\/ Remove the unswitch candidate If nodes in both unswitched loop versions which are now dominated by the loop selector\n-  \/\/ If node. Keep the true-path-path in the true-path-loop and the false-path-path in the false-path-loop by setting\n-  \/\/ the bool input accordingly. The unswitch candidate If nodes are folded in the next IGVN round.\n-  void remove_unswitch_candidate_from_loops(const UnswitchedLoopSelector& unswitched_loop_selector) {\n-    const LoopSelector& loop_selector = unswitched_loop_selector.loop_selector();;\n-    IfNode* unswitch_candidate        = unswitched_loop_selector.unswitch_candidate();\n-    _phase->igvn().rehash_node_delayed(unswitch_candidate);\n-    _phase->dominated_by(loop_selector.true_path_loop_proj(), unswitch_candidate);\n-\n-    IfNode* unswitch_candidate_clone = _old_new[unswitch_candidate->_idx]->as_If();\n-    _phase->igvn().rehash_node_delayed(unswitch_candidate_clone);\n-    _phase->dominated_by(loop_selector.false_path_loop_proj(), unswitch_candidate_clone);\n-  }\n@@ -397,1 +574,2 @@\n-  const UnswitchedLoopSelector unswitched_loop_selector(loop);\n+  const UnswitchCandidate unswitch_candidate(loop, old_new);\n+  const UnswitchedLoopSelector unswitched_loop_selector(loop, unswitch_candidate);\n@@ -401,1 +579,4 @@\n-  hoist_invariant_check_casts(loop, old_new, unswitched_loop_selector);\n+  unswitch_candidate.update_in_true_path_loop(unswitched_loop_selector.loop_selector().true_path_loop_proj());\n+  unswitch_candidate.update_in_false_path_loop(unswitched_loop_selector.loop_selector().false_path_loop_proj(),\n+                                               old_new[original_head->_idx]->as_Loop());\n+  hoist_invariant_check_casts(loop, old_new, unswitch_candidate, unswitched_loop_selector.selector_if());\n@@ -407,1 +588,1 @@\n-  NOT_PRODUCT(trace_loop_unswitching_result(unswitched_loop_selector, original_head, new_head);)\n+  NOT_PRODUCT(trace_loop_unswitching_result(unswitched_loop_selector, unswitch_candidate, original_head, new_head);)\n@@ -614,0 +795,1 @@\n+                                                   const UnswitchCandidate& unswitch_candidate,\n@@ -616,2 +798,2 @@\n-    IfNode* unswitch_candidate = unswitched_loop_selector.unswitch_candidate();\n-    IfNode* loop_selector = unswitched_loop_selector.loop_selector().selector();\n+    IfNode* unswitch_candidate_if = unswitch_candidate.candidate();\n+    IfNode* loop_selector = unswitched_loop_selector.selector_if();\n@@ -619,1 +801,1 @@\n-    tty->print_cr(\"- Unswitch-Candidate-If: %d %s\", unswitch_candidate->_idx, unswitch_candidate->Name());\n+    tty->print_cr(\"- Unswitch-Candidate-If: %d %s\", unswitch_candidate_if->_idx, unswitch_candidate_if->Name());\n@@ -623,0 +805,1 @@\n+    unswitch_candidate.trace_flat_array_checks();\n@@ -649,3 +832,2 @@\n-                                                 const UnswitchedLoopSelector& unswitched_loop_selector) {\n-  IfNode* unswitch_candidate = unswitched_loop_selector.unswitch_candidate();\n-  IfNode* loop_selector = unswitched_loop_selector.loop_selector().selector();\n+                                                 const UnswitchCandidate& unswitch_candidate,\n+                                                 const IfNode* loop_selector) {\n@@ -654,2 +836,3 @@\n-  for (DUIterator_Fast imax, i = unswitch_candidate->fast_outs(imax); i < imax; i++) {\n-    IfProjNode* proj = unswitch_candidate->fast_out(i)->as_IfProj();\n+  const IfNode* unswitch_candidate_if = unswitch_candidate.candidate();\n+  for (DUIterator_Fast imax, i = unswitch_candidate_if->fast_outs(imax); i < imax; i++) {\n+    IfProjNode* proj = unswitch_candidate_if->fast_out(i)->as_IfProj();\n@@ -670,3 +853,6 @@\n-      \/\/ Same for the clone\n-      Node* use_clone = old_new[cast->_idx];\n-      _igvn.replace_input_of(use_clone, 1, cast_clone);\n+      \/\/ Same for the false-path-loop if there are not multiple flat array checks (in that case we leave the\n+      \/\/ false-path-loop unchanged).\n+      if (!unswitch_candidate.has_multiple_flat_array_check_candidates()) {\n+        Node* use_clone = old_new[cast->_idx];\n+        _igvn.replace_input_of(use_clone, 1, cast_clone);\n+      }\n@@ -680,1 +866,1 @@\n-  for(int i = loop->_body.size() - 1; i >= 0 ; i--) {\n+  for (int i = loop->_body.size() - 1; i >= 0; i--) {\n@@ -692,1 +878,0 @@\n-\n","filename":"src\/hotspot\/share\/opto\/loopUnswitch.cpp","additions":241,"deletions":56,"binary":false,"changes":297,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+class UnswitchCandidate;\n@@ -88,1 +89,1 @@\n-       };\n+         FlatArrays            = 1<<18};\n@@ -111,0 +112,1 @@\n+  bool is_flat_arrays() const { return _loop_flags & FlatArrays; }\n@@ -124,0 +126,1 @@\n+  void mark_flat_arrays() { _loop_flags |= FlatArrays; }\n@@ -756,0 +759,1 @@\n+  bool no_unswitch_candidate() const;\n@@ -1566,1 +1570,2 @@\n-  IfNode* find_unswitch_candidate(const IdealLoopTree* loop) const;\n+  IfNode* find_unswitch_candidates(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+  IfNode* find_unswitch_candidate_from_idoms(const IdealLoopTree* loop) const;\n@@ -1573,1 +1578,1 @@\n-                                   const UnswitchedLoopSelector& unswitched_loop_selector);\n+                                   const UnswitchCandidate& unswitch_candidate, const IfNode* loop_selector);\n@@ -1581,0 +1586,1 @@\n+                                            const UnswitchCandidate& unswitch_candidate,\n@@ -1778,0 +1784,1 @@\n+  void move_flat_array_check_out_of_loop(Node* n);\n@@ -1779,0 +1786,1 @@\n+  bool flat_array_element_type_check(Node *n);\n@@ -1970,0 +1978,2 @@\n+  void collect_flat_array_checks(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":13,"deletions":3,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -66,0 +67,6 @@\n+  \/\/ Inline types should not be split through Phis because they cannot be merged\n+  \/\/ through Phi nodes but each value input needs to be merged individually.\n+  if (n->is_InlineType()) {\n+    return nullptr;\n+  }\n+\n@@ -795,0 +802,4 @@\n+      if (inp->isa_InlineType()) {\n+        \/\/ TODO 8302217 This prevents PhiNode::push_inline_types_through\n+        return nullptr;\n+      }\n@@ -1120,0 +1131,48 @@\n+\/\/ We can't use immutable memory for the flat array check because we are loading the mark word which is\n+\/\/ mutable. Although the bits we are interested in are immutable (we check for markWord::unlocked_value),\n+\/\/ we need to use raw memory to not break anti dependency analysis. Below code will attempt to still move\n+\/\/ flat array checks out of loops, mainly to enable loop unswitching.\n+void PhaseIdealLoop::move_flat_array_check_out_of_loop(Node* n) {\n+  \/\/ Skip checks for more than one array\n+  if (n->req() > 3) {\n+    return;\n+  }\n+  Node* mem = n->in(FlatArrayCheckNode::Memory);\n+  Node* array = n->in(FlatArrayCheckNode::ArrayOrKlass)->uncast();\n+  IdealLoopTree* check_loop = get_loop(get_ctrl(n));\n+  IdealLoopTree* ary_loop = get_loop(get_ctrl(array));\n+\n+  \/\/ Check if array is loop invariant\n+  if (!check_loop->is_member(ary_loop)) {\n+    \/\/ Walk up memory graph from the check until we leave the loop\n+    VectorSet wq;\n+    wq.set(mem->_idx);\n+    while (check_loop->is_member(get_loop(ctrl_or_self(mem)))) {\n+      if (mem->is_Phi()) {\n+        mem = mem->in(1);\n+      } else if (mem->is_MergeMem()) {\n+        mem = mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw);\n+      } else if (mem->is_Proj()) {\n+        mem = mem->in(0);\n+      } else if (mem->is_MemBar() || mem->is_SafePoint()) {\n+        mem = mem->in(TypeFunc::Memory);\n+      } else if (mem->is_Store() || mem->is_LoadStore() || mem->is_ClearArray()) {\n+        mem = mem->in(MemNode::Memory);\n+      } else {\n+#ifdef ASSERT\n+        mem->dump();\n+#endif\n+        ShouldNotReachHere();\n+      }\n+      if (wq.test_set(mem->_idx)) {\n+        return;\n+      }\n+    }\n+    \/\/ Replace memory input and re-compute ctrl to move the check out of the loop\n+    _igvn.replace_input_of(n, 1, mem);\n+    set_ctrl_and_loop(n, get_early_ctrl(n));\n+    Node* bol = n->unique_out();\n+    set_ctrl_and_loop(bol, get_early_ctrl(bol));\n+  }\n+}\n+\n@@ -1132,0 +1191,6 @@\n+\n+  if (n->isa_FlatArrayCheck()) {\n+    move_flat_array_check_out_of_loop(n);\n+    return n;\n+  }\n+\n@@ -1404,0 +1469,98 @@\n+bool PhaseIdealLoop::flat_array_element_type_check(Node *n) {\n+  \/\/ If the CmpP is a subtype check for a value that has just been\n+  \/\/ loaded from an array, the subtype check guarantees the value\n+  \/\/ can't be stored in a flat array and the load of the value\n+  \/\/ happens with a flat array check then: push the type check\n+  \/\/ through the phi of the flat array check. This needs special\n+  \/\/ logic because the subtype check's input is not a phi but a\n+  \/\/ LoadKlass that must first be cloned through the phi.\n+  if (n->Opcode() != Op_CmpP) {\n+    return false;\n+  }\n+\n+  Node* klassptr = n->in(1);\n+  Node* klasscon = n->in(2);\n+\n+  if (klassptr->is_DecodeNarrowPtr()) {\n+    klassptr = klassptr->in(1);\n+  }\n+\n+  if (klassptr->Opcode() != Op_LoadKlass && klassptr->Opcode() != Op_LoadNKlass) {\n+    return false;\n+  }\n+\n+  if (!klasscon->is_Con()) {\n+    return false;\n+  }\n+\n+  Node* addr = klassptr->in(MemNode::Address);\n+\n+  if (!addr->is_AddP()) {\n+    return false;\n+  }\n+\n+  intptr_t offset;\n+  Node* obj = AddPNode::Ideal_base_and_offset(addr, &_igvn, offset);\n+\n+  if (obj == nullptr) {\n+    return false;\n+  }\n+\n+  assert(obj != nullptr && addr->in(AddPNode::Base) == addr->in(AddPNode::Address), \"malformed AddP?\");\n+  if (obj->Opcode() == Op_CastPP) {\n+    obj = obj->in(1);\n+  }\n+\n+  if (!obj->is_Phi()) {\n+    return false;\n+  }\n+\n+  Node* region = obj->in(0);\n+\n+  Node* phi = PhiNode::make_blank(region, n->in(1));\n+  for (uint i = 1; i < region->req(); i++) {\n+    Node* in = obj->in(i);\n+    Node* ctrl = region->in(i);\n+    if (addr->in(AddPNode::Base) != obj) {\n+      Node* cast = addr->in(AddPNode::Base);\n+      assert(cast->Opcode() == Op_CastPP && cast->in(0) != nullptr, \"inconsistent subgraph\");\n+      Node* cast_clone = cast->clone();\n+      cast_clone->set_req(0, ctrl);\n+      cast_clone->set_req(1, in);\n+      register_new_node(cast_clone, ctrl);\n+      const Type* tcast = cast_clone->Value(&_igvn);\n+      _igvn.set_type(cast_clone, tcast);\n+      cast_clone->as_Type()->set_type(tcast);\n+      in = cast_clone;\n+    }\n+    Node* addr_clone = addr->clone();\n+    addr_clone->set_req(AddPNode::Base, in);\n+    addr_clone->set_req(AddPNode::Address, in);\n+    register_new_node(addr_clone, ctrl);\n+    _igvn.set_type(addr_clone, addr_clone->Value(&_igvn));\n+    Node* klassptr_clone = klassptr->clone();\n+    klassptr_clone->set_req(2, addr_clone);\n+    register_new_node(klassptr_clone, ctrl);\n+    _igvn.set_type(klassptr_clone, klassptr_clone->Value(&_igvn));\n+    if (klassptr != n->in(1)) {\n+      Node* decode = n->in(1);\n+      assert(decode->is_DecodeNarrowPtr(), \"inconsistent subgraph\");\n+      Node* decode_clone = decode->clone();\n+      decode_clone->set_req(1, klassptr_clone);\n+      register_new_node(decode_clone, ctrl);\n+      _igvn.set_type(decode_clone, decode_clone->Value(&_igvn));\n+      klassptr_clone = decode_clone;\n+    }\n+    phi->set_req(i, klassptr_clone);\n+  }\n+  register_new_node(phi, region);\n+  Node* orig = n->in(1);\n+  _igvn.replace_input_of(n, 1, phi);\n+  split_if_with_blocks_post(n);\n+  if (n->outcnt() != 0) {\n+    _igvn.replace_input_of(n, 1, orig);\n+    _igvn.remove_dead_node(phi);\n+  }\n+  return true;\n+}\n+\n@@ -1410,0 +1573,4 @@\n+  if (flat_array_element_type_check(n)) {\n+    return;\n+  }\n+\n@@ -1553,0 +1720,5 @@\n+\n+  \/\/ Remove multiple allocations of the same inline type\n+  if (n->is_InlineType()) {\n+    n->as_InlineType()->remove_redundant_allocations(this);\n+  }\n@@ -2061,1 +2233,9 @@\n-  Node *sample_cmp = sample_bool->in(1);\n+  Node* sample_cmp = sample_bool->in(1);\n+  const Type* t = Type::TOP;\n+  const TypePtr* at = nullptr;\n+  if (sample_cmp->is_FlatArrayCheck()) {\n+    \/\/ Left input of a FlatArrayCheckNode is memory, set the (adr) type of the phi accordingly\n+    assert(sample_cmp->in(1)->bottom_type() == Type::MEMORY, \"unexpected input type\");\n+    t = Type::MEMORY;\n+    at = TypeRawPtr::BOTTOM;\n+  }\n@@ -2064,1 +2244,1 @@\n-  PhiNode *phi1 = new PhiNode(phi->in(0), Type::TOP);\n+  PhiNode *phi1 = new PhiNode(phi->in(0), t, at);\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":182,"deletions":2,"binary":false,"changes":184,"status":"modified"},{"patch":"@@ -25,0 +25,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInstanceKlass.hpp\"\n@@ -38,0 +40,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -46,0 +49,1 @@\n+#include \"opto\/opcodes.hpp\"\n@@ -55,0 +59,2 @@\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -84,11 +90,0 @@\n-void PhaseMacroExpand::migrate_outs(Node *old, Node *target) {\n-  assert(old != nullptr, \"sanity\");\n-  for (DUIterator_Fast imax, i = old->fast_outs(imax); i < imax; i++) {\n-    Node* use = old->fast_out(i);\n-    _igvn.rehash_node_delayed(use);\n-    imax -= replace_input(use, old, target);\n-    \/\/ back up iterator\n-    --i;\n-  }\n-  assert(old->outcnt() == 0, \"all uses must be deleted\");\n-}\n@@ -147,1 +142,1 @@\n-  bs->eliminate_gc_barrier(this, p2x);\n+  bs->eliminate_gc_barrier(&_igvn, p2x);\n@@ -190,0 +185,2 @@\n+      } else if (in->is_LoadFlat() || in->is_StoreFlat()) {\n+        mem = in->in(TypeFunc::Memory);\n@@ -202,1 +199,1 @@\n-        int adr_offset = atype->offset();\n+        int adr_offset = atype->flat_offset();\n@@ -247,1 +244,1 @@\n-   } else if (mem->Opcode() == Op_StrInflatedCopy) {\n+    } else if (mem->Opcode() == Op_StrInflatedCopy) {\n@@ -292,1 +289,5 @@\n-      const TypePtr* adr_type = nullptr;\n+      Node* base = ac->in(ArrayCopyNode::Src);\n+      const TypeAryPtr* adr_type = _igvn.type(base)->is_aryptr();\n+      if (adr_type->is_flat()) {\n+        shift = adr_type->flat_log_elem_size();\n+      }\n@@ -295,2 +296,2 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(off);\n+        adr_type = _igvn.type(adr)->is_aryptr();\n+        assert(adr_type == _igvn.type(base)->is_aryptr()->add_field_offset_and_offset(off), \"incorrect address type\");\n@@ -300,1 +301,1 @@\n-          return value_from_mem(ac->in(TypeFunc::Memory), ctl, ft, ftype, adr_type->isa_oopptr(), alloc);\n+          return value_from_mem(ac->in(TypeFunc::Memory), ctl, ft, ftype, adr_type, alloc);\n@@ -303,0 +304,5 @@\n+        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n+          \/\/ Non constant offset in the array: we can't statically\n+          \/\/ determine the value\n+          return nullptr;\n+        }\n@@ -310,7 +316,5 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(Type::OffsetBot);\n-        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n-          \/\/ Non constant offset in the array: we can't statically\n-          \/\/ determine the value\n-          return nullptr;\n-        }\n+        \/\/ In the case of a flat inline type array, each field has its\n+        \/\/ own slice so we need to extract the field being accessed from\n+        \/\/ the address computation\n+        adr_type = adr_type->add_field_offset_and_offset(offset)->add_offset(Type::OffsetBot)->is_aryptr();\n+        adr = _igvn.transform(new CastPPNode(ctl, adr, adr_type));\n@@ -327,0 +331,1 @@\n+      assert(res->isa_DecodeN(), \"should be narrow oop\");\n@@ -342,1 +347,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flat_offset();\n@@ -378,1 +383,1 @@\n-    } else  {\n+    } else {\n@@ -381,2 +386,14 @@\n-        \/\/ hit a sentinel, return appropriate 0 value\n-        values.at_put(j, _igvn.zerocon(ft));\n+        \/\/ hit a sentinel, return appropriate value\n+        Node* init_value = alloc->in(AllocateNode::InitValue);\n+        if (init_value != nullptr) {\n+          if (val == start_mem) {\n+            \/\/ TODO 8350865 Scalar replacement does not work well for flat arrays.\n+            \/\/ Somehow we ended up with root mem and therefore walked past the alloc. Fix this. Triggered by TestGenerated::test15\n+            \/\/ Don't we need field_value_by_offset?\n+            return nullptr;\n+          }\n+          values.at_put(j, init_value);\n+        } else {\n+          assert(alloc->in(AllocateNode::RawInitValue) == nullptr, \"init value may not be null\");\n+          values.at_put(j, _igvn.zerocon(ft));\n+        }\n@@ -401,2 +418,10 @@\n-      } else if(val->is_Proj() && val->in(0) == alloc) {\n-        values.at_put(j, _igvn.zerocon(ft));\n+      } else if (val->is_Proj() && val->in(0) == alloc) {\n+        Node* init_value = alloc->in(AllocateNode::InitValue);\n+        if (init_value != nullptr) {\n+          \/\/ TODO 8350865 Scalar replacement does not work well for flat arrays.\n+          \/\/ Is this correct for non-all-zero init values? Don't we need field_value_by_offset?\n+          values.at_put(j, init_value);\n+        } else {\n+          assert(alloc->in(AllocateNode::RawInitValue) == nullptr, \"init value may not be null\");\n+          values.at_put(j, _igvn.zerocon(ft));\n+        }\n@@ -450,1 +475,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flat_offset();\n@@ -452,1 +477,0 @@\n-  Node *alloc_ctrl = alloc->in(TypeFunc::Control);\n@@ -469,1 +493,1 @@\n-        done = true; \/\/ Something go wrong.\n+        done = true; \/\/ Something went wrong.\n@@ -479,1 +503,1 @@\n-             atype->is_known_instance_field() && atype->offset() == offset &&\n+             atype->is_known_instance_field() && atype->flat_offset() == offset &&\n@@ -516,1 +540,20 @@\n-      \/\/ hit a sentinel, return appropriate 0 value\n+      \/\/ hit a sentinel, return appropriate value\n+      Node* init_value = alloc->in(AllocateNode::InitValue);\n+      if (init_value != nullptr) {\n+        if (adr_t->is_flat()) {\n+          if (init_value->is_EncodeP()) {\n+            init_value = init_value->in(1);\n+          }\n+          if (!init_value->is_InlineType()) {\n+            return nullptr;\n+          }\n+          assert(adr_t->is_aryptr()->field_offset().get() != Type::OffsetBot, \"Unknown offset\");\n+          offset = adr_t->is_aryptr()->field_offset().get() + init_value->bottom_type()->inline_klass()->payload_offset();\n+          init_value = init_value->as_InlineType()->field_value_by_offset(offset, true);\n+          if (ft == T_NARROWOOP) {\n+            init_value = transform_later(new EncodePNode(init_value, init_value->bottom_type()->make_ptr()));\n+          }\n+        }\n+        return init_value;\n+      }\n+      assert(alloc->in(AllocateNode::RawInitValue) == nullptr, \"init value may not be null\");\n@@ -548,1 +591,1 @@\n-  \/\/ Something go wrong.\n+  \/\/ Something went wrong.\n@@ -552,0 +595,69 @@\n+\/\/ Search the last value stored into the inline type's fields (for flat arrays).\n+Node* PhaseMacroExpand::inline_type_from_mem(ciInlineKlass* vk, const TypeAryPtr* elem_adr_type, int elem_idx, int offset_in_element, bool null_free, AllocateNode* alloc, SafePointNode* sfpt) {\n+  auto report_failure = [&](int field_offset_in_element) {\n+#ifndef PRODUCT\n+    if (PrintEliminateAllocations) {\n+      ciInlineKlass* elem_klass = elem_adr_type->elem()->inline_klass();\n+      int offset = field_offset_in_element + elem_klass->payload_offset();\n+      ciField* flattened_field = elem_klass->get_field_by_offset(offset, false);\n+      assert(flattened_field != nullptr, \"must have a field of type %s at offset %d\", elem_klass->name()->as_utf8(), offset);\n+      tty->print(\"=== At SafePoint node %d can't find value of field [%s] of array element [%d]\", sfpt->_idx, flattened_field->name()->as_utf8(), elem_idx);\n+      tty->print(\", which prevents elimination of: \");\n+      alloc->dump();\n+    }\n+#endif \/\/ PRODUCT\n+  };\n+\n+  \/\/ Create a new InlineTypeNode and retrieve the field values from memory\n+  InlineTypeNode* vt = InlineTypeNode::make_uninitialized(_igvn, vk, false);\n+  transform_later(vt);\n+  if (null_free) {\n+    vt->set_null_marker(_igvn);\n+  } else {\n+    int nm_offset_in_element = offset_in_element + vk->null_marker_offset_in_payload();\n+    const TypeAryPtr* nm_adr_type = elem_adr_type->with_field_offset(nm_offset_in_element);\n+    Node* nm_value = value_from_mem(sfpt->memory(), sfpt->control(), T_BOOLEAN, TypeInt::BOOL, nm_adr_type, alloc);\n+    if (nm_value != nullptr) {\n+      vt->set_null_marker(_igvn, nm_value);\n+    } else {\n+      report_failure(nm_offset_in_element);\n+      return nullptr;\n+    }\n+  }\n+\n+  for (int i = 0; i < vk->nof_declared_nonstatic_fields(); ++i) {\n+    ciType* field_type = vt->field_type(i);\n+    int field_offset_in_element = offset_in_element + vt->field_offset(i) - vk->payload_offset();\n+    Node* field_value = nullptr;\n+    if (vt->field_is_flat(i)) {\n+      field_value = inline_type_from_mem(field_type->as_inline_klass(), elem_adr_type, elem_idx, field_offset_in_element, vt->field_is_null_free(i), alloc, sfpt);\n+    } else {\n+      const Type* ft = Type::get_const_type(field_type);\n+      BasicType bt = type2field[field_type->basic_type()];\n+      if (UseCompressedOops && !is_java_primitive(bt)) {\n+        ft = ft->make_narrowoop();\n+        bt = T_NARROWOOP;\n+      }\n+      \/\/ Each inline type field has its own memory slice\n+      const TypeAryPtr* field_adr_type = elem_adr_type->with_field_offset(field_offset_in_element);\n+      field_value = value_from_mem(sfpt->memory(), sfpt->control(), bt, ft, field_adr_type, alloc);\n+      if (field_value == nullptr) {\n+        report_failure(field_offset_in_element);\n+      } else if (ft->isa_narrowoop()) {\n+        assert(UseCompressedOops, \"unexpected narrow oop\");\n+        if (field_value->is_EncodeP()) {\n+          field_value = field_value->in(1);\n+        } else if (!field_value->is_InlineType()) {\n+          field_value = transform_later(new DecodeNNode(field_value, field_value->get_ptr_type()));\n+        }\n+      }\n+    }\n+    if (field_value != nullptr) {\n+      vt->set_field_value(i, field_value);\n+    } else {\n+      return nullptr;\n+    }\n+  }\n+  return vt;\n+}\n+\n@@ -561,0 +673,1 @@\n+  Unique_Node_List worklist;\n@@ -569,0 +682,1 @@\n+    worklist.push(res);\n@@ -585,1 +699,1 @@\n-  if (can_eliminate && res != nullptr) {\n+  while (can_eliminate && worklist.size() > 0) {\n@@ -587,2 +701,2 @@\n-    for (DUIterator_Fast jmax, j = res->fast_outs(jmax);\n-                               j < jmax && can_eliminate; j++) {\n+    res = worklist.pop();\n+    for (DUIterator_Fast jmax, j = res->fast_outs(jmax); j < jmax && can_eliminate; j++) {\n@@ -603,1 +717,1 @@\n-          if (n->is_Mem() && n->as_Mem()->is_mismatched_access()) {\n+          if ((n->is_Mem() && n->as_Mem()->is_mismatched_access()) || n->is_LoadFlat() || n->is_StoreFlat()) {\n@@ -639,0 +753,1 @@\n+          assert(!res->is_Phi() || !res->as_Phi()->can_be_inline_type(), \"Inline type allocations should not have safepoint uses\");\n@@ -641,0 +756,23 @@\n+      } else if (use->is_InlineType() && use->as_InlineType()->get_oop() == res) {\n+        \/\/ Look at uses\n+        for (DUIterator_Fast kmax, k = use->fast_outs(kmax); k < kmax; k++) {\n+          Node* u = use->fast_out(k);\n+          if (u->is_InlineType()) {\n+            \/\/ Use in flat field can be eliminated\n+            InlineTypeNode* vt = u->as_InlineType();\n+            for (uint i = 0; i < vt->field_count(); ++i) {\n+              if (vt->field_value(i) == use && !vt->field_is_flat(i)) {\n+                can_eliminate = false; \/\/ Use in non-flat field\n+                break;\n+              }\n+            }\n+          } else {\n+            \/\/ Add other uses to the worklist to process individually\n+            worklist.push(use);\n+          }\n+        }\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ Store to mark word of inline type larval buffer\n+        assert(res_type->is_inlinetypeptr(), \"Unexpected store to mark word\");\n+      } else if (res_type->is_inlinetypeptr() && (use->Opcode() == Op_MemBarRelease || use->Opcode() == Op_MemBarStoreStore)) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n@@ -663,0 +801,3 @@\n+      } else {\n+        assert(use->Opcode() == Op_CastP2X, \"should be\");\n+        assert(!use->has_out_with(Op_OrL), \"should have been removed because oop is never null\");\n@@ -675,1 +816,1 @@\n-    } else if (alloc->_is_scalar_replaceable) {\n+    } else {\n@@ -780,1 +921,148 @@\n-SafePointScalarObjectNode* PhaseMacroExpand::create_scalarized_object_description(AllocateNode *alloc, SafePointNode* sfpt) {\n+void PhaseMacroExpand::process_field_value_at_safepoint(const Type* field_type, Node* field_val, SafePointNode* sfpt, Unique_Node_List* value_worklist) {\n+  if (UseCompressedOops && field_type->isa_narrowoop()) {\n+    \/\/ Enable \"DecodeN(EncodeP(Allocate)) --> Allocate\" transformation\n+    \/\/ to be able scalar replace the allocation.\n+    if (field_val->is_EncodeP()) {\n+      field_val = field_val->in(1);\n+    } else if (!field_val->is_InlineType()) {\n+      field_val = transform_later(new DecodeNNode(field_val, field_val->get_ptr_type()));\n+    }\n+  }\n+\n+  \/\/ Keep track of inline types to scalarize them later\n+  if (field_val->is_InlineType()) {\n+    value_worklist->push(field_val);\n+  } else if (field_val->is_Phi()) {\n+    PhiNode* phi = field_val->as_Phi();\n+    \/\/ Eagerly replace inline type phis now since we could be removing an inline type allocation where we must\n+    \/\/ scalarize all its fields in safepoints.\n+    field_val = phi->try_push_inline_types_down(&_igvn, true);\n+    if (field_val->is_InlineType()) {\n+      value_worklist->push(field_val);\n+    }\n+  }\n+  DEBUG_ONLY(verify_type_compatability(field_val->bottom_type(), field_type);)\n+  sfpt->add_req(field_val);\n+}\n+\n+bool PhaseMacroExpand::add_array_elems_to_safepoint(AllocateNode* alloc, const TypeAryPtr* array_type, SafePointNode* sfpt, Unique_Node_List* value_worklist) {\n+  const Type* elem_type = array_type->elem();\n+  BasicType basic_elem_type = elem_type->array_element_basic_type();\n+\n+  intptr_t elem_size;\n+  uint header_size;\n+  if (array_type->is_flat()) {\n+    elem_size = array_type->flat_elem_size();\n+    header_size = arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT);\n+  } else {\n+    elem_size = type2aelembytes(basic_elem_type);\n+    header_size = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n+  }\n+\n+  int n_elems = alloc->in(AllocateNode::ALength)->get_int();\n+  for (int elem_idx = 0; elem_idx < n_elems; elem_idx++) {\n+    intptr_t elem_offset = header_size + elem_idx * elem_size;\n+    const TypeAryPtr* elem_adr_type = array_type->with_offset(elem_offset);\n+    Node* elem_val;\n+    if (array_type->is_flat()) {\n+      ciInlineKlass* elem_klass = elem_type->inline_klass();\n+      assert(elem_klass->maybe_flat_in_array(), \"must be flat in array\");\n+      elem_val = inline_type_from_mem(elem_klass, elem_adr_type, elem_idx, 0, array_type->is_null_free(), alloc, sfpt);\n+    } else {\n+      elem_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, elem_type, elem_adr_type, alloc);\n+#ifndef PRODUCT\n+      if (PrintEliminateAllocations && elem_val == nullptr) {\n+        tty->print(\"=== At SafePoint node %d can't find value of array element [%d]\", sfpt->_idx, elem_idx);\n+        tty->print(\", which prevents elimination of: \");\n+        alloc->dump();\n+      }\n+#endif \/\/ PRODUCT\n+    }\n+    if (elem_val == nullptr) {\n+      return false;\n+    }\n+\n+    process_field_value_at_safepoint(elem_type, elem_val, sfpt, value_worklist);\n+  }\n+\n+  return true;\n+}\n+\n+\/\/ Recursively adds all flattened fields of a type 'iklass' inside 'base' to 'sfpt'.\n+\/\/ 'offset_minus_header' refers to the offset of the payload of 'iklass' inside 'base' minus the\n+\/\/ payload offset of 'iklass'. If 'base' is of type 'iklass' then 'offset_minus_header' == 0.\n+bool PhaseMacroExpand::add_inst_fields_to_safepoint(ciInstanceKlass* iklass, AllocateNode* alloc, Node* base, int offset_minus_header, SafePointNode* sfpt, Unique_Node_List* value_worklist) {\n+  const TypeInstPtr* base_type = _igvn.type(base)->is_instptr();\n+  auto report_failure = [&](int offset) {\n+#ifndef PRODUCT\n+    if (PrintEliminateAllocations) {\n+      ciInstanceKlass* base_klass = base_type->instance_klass();\n+      ciField* flattened_field = base_klass->get_field_by_offset(offset, false);\n+      assert(flattened_field != nullptr, \"must have a field of type %s at offset %d\", base_klass->name()->as_utf8(), offset);\n+      tty->print(\"=== At SafePoint node %d can't find value of field: \", sfpt->_idx);\n+      flattened_field->print();\n+      int field_idx = C->alias_type(flattened_field)->index();\n+      tty->print(\" (alias_idx=%d)\", field_idx);\n+      tty->print(\", which prevents elimination of: \");\n+      base->dump();\n+    }\n+#endif \/\/ PRODUCT\n+  };\n+\n+  for (int i = 0; i < iklass->nof_declared_nonstatic_fields(); i++) {\n+    ciField* field = iklass->declared_nonstatic_field_at(i);\n+    if (field->is_flat()) {\n+      ciInlineKlass* fvk = field->type()->as_inline_klass();\n+      int field_offset_minus_header = offset_minus_header + field->offset_in_bytes() - fvk->payload_offset();\n+      bool success = add_inst_fields_to_safepoint(fvk, alloc, base, field_offset_minus_header, sfpt, value_worklist);\n+      if (!success) {\n+        return false;\n+      }\n+\n+      \/\/ The null marker of a field is added right after we scalarize that field\n+      if (!field->is_null_free()) {\n+        int nm_offset = offset_minus_header + field->null_marker_offset();\n+        Node* null_marker = value_from_mem(sfpt->memory(), sfpt->control(), T_BOOLEAN, TypeInt::BOOL, base_type->with_offset(nm_offset), alloc);\n+        if (null_marker == nullptr) {\n+          report_failure(nm_offset);\n+          return false;\n+        }\n+        process_field_value_at_safepoint(TypeInt::BOOL, null_marker, sfpt, value_worklist);\n+      }\n+\n+      continue;\n+    }\n+\n+    int offset = offset_minus_header + field->offset_in_bytes();\n+    ciType* elem_type = field->type();\n+    BasicType basic_elem_type = field->layout_type();\n+\n+    const Type* field_type;\n+    if (is_reference_type(basic_elem_type)) {\n+      if (!elem_type->is_loaded()) {\n+        field_type = TypeInstPtr::BOTTOM;\n+      } else {\n+        field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n+      }\n+      if (UseCompressedOops) {\n+        field_type = field_type->make_narrowoop();\n+        basic_elem_type = T_NARROWOOP;\n+      }\n+    } else {\n+      field_type = Type::get_const_basic_type(basic_elem_type);\n+    }\n+\n+    const TypeInstPtr* field_addr_type = base_type->add_offset(offset)->isa_instptr();\n+    Node* field_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, field_type, field_addr_type, alloc);\n+    if (field_val == nullptr) {\n+      report_failure(offset);\n+      return false;\n+    }\n+    process_field_value_at_safepoint(field_type, field_val, sfpt, value_worklist);\n+  }\n+\n+  return true;\n+}\n+\n+SafePointScalarObjectNode* PhaseMacroExpand::create_scalarized_object_description(AllocateNode* alloc, SafePointNode* sfpt,\n+                                                                                  Unique_Node_List* value_worklist) {\n@@ -785,2 +1073,0 @@\n-  BasicType basic_elem_type  = T_ILLEGAL;\n-  const Type* field_type     = nullptr;\n@@ -789,2 +1075,0 @@\n-  int array_base             = 0;\n-  int element_size           = 0;\n@@ -796,0 +1080,1 @@\n+  uint before_sfpt_req = sfpt->req();\n@@ -808,4 +1093,10 @@\n-      basic_elem_type = res_type->is_aryptr()->elem()->array_element_basic_type();\n-      array_base = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n-      element_size = type2aelembytes(basic_elem_type);\n-      field_type = res_type->is_aryptr()->elem();\n+    }\n+\n+    if (res->bottom_type()->is_inlinetypeptr()) {\n+      \/\/ Nullable inline types have a null marker field which is added to the safepoint when scalarizing them (see\n+      \/\/ InlineTypeNode::make_scalar_in_safepoint()). When having circular inline types, we stop scalarizing at depth 1\n+      \/\/ to avoid an endless recursion. Therefore, we do not have a SafePointScalarObjectNode node here, yet.\n+      \/\/ We are about to create a SafePointScalarObjectNode as if this is a normal object. Add an additional int input\n+      \/\/ with value 1 which sets the null marker to true to indicate that the object is always non-null. This input is checked\n+      \/\/ later in PhaseOutput::filLocArray() for inline types.\n+      sfpt->add_req(_igvn.intcon(1));\n@@ -819,64 +1110,4 @@\n-  \/\/ Scan object's fields adding an input to the safepoint for each field.\n-  for (int j = 0; j < nfields; j++) {\n-    intptr_t offset;\n-    ciField* field = nullptr;\n-    if (iklass != nullptr) {\n-      field = iklass->nonstatic_field_at(j);\n-      offset = field->offset_in_bytes();\n-      ciType* elem_type = field->type();\n-      basic_elem_type = field->layout_type();\n-\n-      \/\/ The next code is taken from Parse::do_get_xxx().\n-      if (is_reference_type(basic_elem_type)) {\n-        if (!elem_type->is_loaded()) {\n-          field_type = TypeInstPtr::BOTTOM;\n-        } else if (field != nullptr && field->is_static_constant()) {\n-          ciObject* con = field->constant_value().as_object();\n-          \/\/ Do not \"join\" in the previous type; it doesn't add value,\n-          \/\/ and may yield a vacuous result if the field is of interface type.\n-          field_type = TypeOopPtr::make_from_constant(con)->isa_oopptr();\n-          assert(field_type != nullptr, \"field singleton type must be consistent\");\n-        } else {\n-          field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n-        }\n-        if (UseCompressedOops) {\n-          field_type = field_type->make_narrowoop();\n-          basic_elem_type = T_NARROWOOP;\n-        }\n-      } else {\n-        field_type = Type::get_const_basic_type(basic_elem_type);\n-      }\n-    } else {\n-      offset = array_base + j * (intptr_t)element_size;\n-    }\n-\n-    const TypeOopPtr *field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n-\n-    Node *field_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, field_type, field_addr_type, alloc);\n-\n-    \/\/ We weren't able to find a value for this field,\n-    \/\/ give up on eliminating this allocation.\n-    if (field_val == nullptr) {\n-      uint last = sfpt->req() - 1;\n-      for (int k = 0;  k < j; k++) {\n-        sfpt->del_req(last--);\n-      }\n-      _igvn._worklist.push(sfpt);\n-\n-#ifndef PRODUCT\n-      if (PrintEliminateAllocations) {\n-        if (field != nullptr) {\n-          tty->print(\"=== At SafePoint node %d can't find value of field: \", sfpt->_idx);\n-          field->print();\n-          int field_idx = C->get_alias_index(field_addr_type);\n-          tty->print(\" (alias_idx=%d)\", field_idx);\n-        } else { \/\/ Array's element\n-          tty->print(\"=== At SafePoint node %d can't find value of array element [%d]\", sfpt->_idx, j);\n-        }\n-        tty->print(\", which prevents elimination of: \");\n-        if (res == nullptr)\n-          alloc->dump();\n-        else\n-          res->dump();\n-      }\n-#endif\n+  if (res == nullptr) {\n+    sfpt->jvms()->set_endoff(sfpt->req());\n+    return sobj;\n+  }\n@@ -884,2 +1115,6 @@\n-      return nullptr;\n-    }\n+  bool success;\n+  if (iklass == nullptr) {\n+    success = add_array_elems_to_safepoint(alloc, res_type->is_aryptr(), sfpt, value_worklist);\n+  } else {\n+    success = add_inst_fields_to_safepoint(iklass, alloc, res, 0, sfpt, value_worklist);\n+  }\n@@ -887,8 +1122,4 @@\n-    if (UseCompressedOops && field_type->isa_narrowoop()) {\n-      \/\/ Enable \"DecodeN(EncodeP(Allocate)) --> Allocate\" transformation\n-      \/\/ to be able scalar replace the allocation.\n-      if (field_val->is_EncodeP()) {\n-        field_val = field_val->in(1);\n-      } else {\n-        field_val = transform_later(new DecodeNNode(field_val, field_val->get_ptr_type()));\n-      }\n+  \/\/ We weren't able to find a value for this field, remove all the fields added to the safepoint\n+  if (!success) {\n+    for (uint i = sfpt->req() - 1; i >= before_sfpt_req; i--) {\n+      sfpt->del_req(i);\n@@ -896,2 +1127,2 @@\n-    DEBUG_ONLY(verify_type_compatability(field_val->bottom_type(), field_type);)\n-    sfpt->add_req(field_val);\n+    _igvn._worklist.push(sfpt);\n+    return nullptr;\n@@ -901,1 +1132,0 @@\n-\n@@ -910,0 +1140,4 @@\n+  const TypeOopPtr* res_type = nullptr;\n+  if (res != nullptr) { \/\/ Could be null when there are no users\n+    res_type = _igvn.type(res)->isa_oopptr();\n+  }\n@@ -912,0 +1146,1 @@\n+  Unique_Node_List value_worklist;\n@@ -914,1 +1149,1 @@\n-    SafePointScalarObjectNode* sobj = create_scalarized_object_description(alloc, sfpt);\n+    SafePointScalarObjectNode* sobj = create_scalarized_object_description(alloc, sfpt, &value_worklist);\n@@ -930,1 +1165,8 @@\n-\n+  \/\/ Scalarize inline types that were added to the safepoint.\n+  \/\/ Don't allow linking a constant oop (if available) for flat array elements\n+  \/\/ because Deoptimization::reassign_flat_array_elements needs field values.\n+  bool allow_oop = (res_type != nullptr) && !res_type->is_flat();\n+  for (uint i = 0; i < value_worklist.size(); ++i) {\n+    InlineTypeNode* vt = value_worklist.at(i)->as_InlineType();\n+    vt->make_scalar_in_safepoints(&_igvn, allow_oop);\n+  }\n@@ -946,1 +1188,2 @@\n-void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc) {\n+void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc, bool inline_alloc) {\n+  Unique_Node_List worklist;\n@@ -949,0 +1192,4 @@\n+    worklist.push(res);\n+  }\n+  while (worklist.size() > 0) {\n+    res = worklist.pop();\n@@ -958,10 +1205,7 @@\n-#ifdef ASSERT\n-            \/\/ Verify that there is no dependent MemBarVolatile nodes,\n-            \/\/ they should be removed during IGVN, see MemBarNode::Ideal().\n-            for (DUIterator_Fast pmax, p = n->fast_outs(pmax);\n-                                       p < pmax; p++) {\n-              Node* mb = n->fast_out(p);\n-              assert(mb->is_Initialize() || !mb->is_MemBar() ||\n-                     mb->req() <= MemBarNode::Precedent ||\n-                     mb->in(MemBarNode::Precedent) != n,\n-                     \"MemBarVolatile should be eliminated for non-escaping object\");\n+            for (DUIterator_Fast pmax, p = n->fast_outs(pmax); p < pmax; p++) {\n+              MemBarNode* mb = n->fast_out(p)->isa_MemBar();\n+              if (mb != nullptr && mb->req() <= MemBarNode::Precedent && mb->in(MemBarNode::Precedent) == n) {\n+                \/\/ MemBarVolatiles should have been removed by MemBarNode::Ideal() for non-inline allocations\n+                assert(inline_alloc, \"MemBarVolatile should be eliminated for non-escaping object\");\n+                mb->remove(&_igvn);\n+              }\n@@ -969,1 +1213,0 @@\n-#endif\n@@ -993,2 +1236,1 @@\n-          CallProjections callprojs;\n-          ac->extract_projections(&callprojs, true);\n+          CallProjections* callprojs = ac->extract_projections(true);\n@@ -996,3 +1238,3 @@\n-          _igvn.replace_node(callprojs.fallthrough_ioproj, ac->in(TypeFunc::I_O));\n-          _igvn.replace_node(callprojs.fallthrough_memproj, ac->in(TypeFunc::Memory));\n-          _igvn.replace_node(callprojs.fallthrough_catchproj, ac->in(TypeFunc::Control));\n+          _igvn.replace_node(callprojs->fallthrough_ioproj, ac->in(TypeFunc::I_O));\n+          _igvn.replace_node(callprojs->fallthrough_memproj, ac->in(TypeFunc::Memory));\n+          _igvn.replace_node(callprojs->fallthrough_catchproj, ac->in(TypeFunc::Control));\n@@ -1015,0 +1257,24 @@\n+      } else if (use->is_InlineType()) {\n+        assert(use->as_InlineType()->get_oop() == res, \"unexpected inline type ptr use\");\n+        \/\/ Cut off oop input and remove known instance id from type\n+        _igvn.rehash_node_delayed(use);\n+        use->as_InlineType()->set_oop(_igvn, _igvn.zerocon(T_OBJECT));\n+        use->as_InlineType()->set_is_buffered(_igvn, false);\n+        const TypeOopPtr* toop = _igvn.type(use)->is_oopptr()->cast_to_instance_id(TypeOopPtr::InstanceBot);\n+        _igvn.set_type(use, toop);\n+        use->as_InlineType()->set_type(toop);\n+        \/\/ Process users\n+        for (DUIterator_Fast kmax, k = use->fast_outs(kmax); k < kmax; k++) {\n+          Node* u = use->fast_out(k);\n+          if (!u->is_InlineType() && !u->is_StoreFlat()) {\n+            worklist.push(u);\n+          }\n+        }\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ Store to mark word of inline type larval buffer\n+        assert(inline_alloc, \"Unexpected store to mark word\");\n+        _igvn.replace_node(use, use->in(MemNode::Memory));\n+      } else if (use->Opcode() == Op_MemBarRelease || use->Opcode() == Op_MemBarStoreStore) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n+        assert(inline_alloc, \"Unexpected MemBarRelease\");\n+        use->as_MemBar()->remove(&_igvn);\n@@ -1027,1 +1293,1 @@\n-  if (_callprojs.resproj != nullptr && _callprojs.resproj->outcnt() != 0) {\n+  if (_callprojs->resproj[0] != nullptr && _callprojs->resproj[0]->outcnt() != 0) {\n@@ -1031,2 +1297,2 @@\n-    for (DUIterator_Fast jmax, j = _callprojs.resproj->fast_outs(jmax);  j < jmax; j++) {\n-      Node* use = _callprojs.resproj->fast_out(j);\n+    for (DUIterator_Fast jmax, j = _callprojs->resproj[0]->fast_outs(jmax);  j < jmax; j++) {\n+      Node* use = _callprojs->resproj[0]->fast_out(j);\n@@ -1039,3 +1305,3 @@\n-    for (DUIterator_Last jmin, j = _callprojs.resproj->last_outs(jmin); j >= jmin; ) {\n-      Node* use = _callprojs.resproj->last_out(j);\n-      uint oc1 = _callprojs.resproj->outcnt();\n+    for (DUIterator_Last jmin, j = _callprojs->resproj[0]->last_outs(jmin); j >= jmin; ) {\n+      Node* use = _callprojs->resproj[0]->last_out(j);\n+      uint oc1 = _callprojs->resproj[0]->outcnt();\n@@ -1051,1 +1317,1 @@\n-          assert(tmp == nullptr || tmp == _callprojs.fallthrough_catchproj, \"allocation control projection\");\n+          assert(tmp == nullptr || tmp == _callprojs->fallthrough_catchproj, \"allocation control projection\");\n@@ -1058,1 +1324,1 @@\n-            assert(mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw) == _callprojs.fallthrough_memproj, \"allocation memory projection\");\n+            assert(mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw) == _callprojs->fallthrough_memproj, \"allocation memory projection\");\n@@ -1060,1 +1326,1 @@\n-            assert(mem == _callprojs.fallthrough_memproj, \"allocation memory projection\");\n+            assert(mem == _callprojs->fallthrough_memproj, \"allocation memory projection\");\n@@ -1066,0 +1332,4 @@\n+      } else if (use->Opcode() == Op_MemBarStoreStore) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n+        assert(inline_alloc, \"Unexpected MemBarStoreStore\");\n+        use->as_MemBar()->remove(&_igvn);\n@@ -1069,1 +1339,1 @@\n-      j -= (oc1 - _callprojs.resproj->outcnt());\n+      j -= (oc1 - _callprojs->resproj[0]->outcnt());\n@@ -1072,2 +1342,2 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, alloc->in(TypeFunc::Control));\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, alloc->in(TypeFunc::Control));\n@@ -1075,2 +1345,2 @@\n-  if (_callprojs.fallthrough_memproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_memproj, alloc->in(TypeFunc::Memory));\n+  if (_callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_memproj, alloc->in(TypeFunc::Memory));\n@@ -1078,2 +1348,2 @@\n-  if (_callprojs.catchall_memproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_memproj, C->top());\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_memproj, C->top());\n@@ -1081,2 +1351,2 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -1084,2 +1354,2 @@\n-  if (_callprojs.catchall_ioproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_ioproj, C->top());\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_ioproj, C->top());\n@@ -1087,2 +1357,2 @@\n-  if (_callprojs.catchall_catchproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_catchproj, C->top());\n+  if (_callprojs->catchall_catchproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_catchproj, C->top());\n@@ -1098,1 +1368,1 @@\n-  if (!EliminateAllocations || !alloc->_is_non_escaping) {\n+  if (!EliminateAllocations) {\n@@ -1103,1 +1373,8 @@\n-  Node* res = alloc->result_cast();\n+\n+  \/\/ Attempt to eliminate inline type buffer allocations\n+  \/\/ regardless of usage and escape\/replaceable status.\n+  bool inline_alloc = tklass->isa_instklassptr() &&\n+                      tklass->is_instklassptr()->instance_klass()->is_inlinetype();\n+  if (!alloc->_is_non_escaping && !inline_alloc) {\n+    return false;\n+  }\n@@ -1106,1 +1383,2 @@\n-  bool boxing_alloc = C->eliminate_boxing() &&\n+  Node* res = alloc->result_cast();\n+  bool boxing_alloc = (res == nullptr) && C->eliminate_boxing() &&\n@@ -1109,1 +1387,1 @@\n-  if (!alloc->_is_scalar_replaceable && (!boxing_alloc || (res != nullptr))) {\n+  if (!alloc->_is_scalar_replaceable && !boxing_alloc && !inline_alloc) {\n@@ -1113,1 +1391,1 @@\n-  alloc->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = alloc->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1121,1 +1399,1 @@\n-    assert(res == nullptr, \"sanity\");\n+    assert(res == nullptr || inline_alloc, \"sanity\");\n@@ -1146,1 +1424,1 @@\n-  process_users_of_allocation(alloc);\n+  process_users_of_allocation(alloc, inline_alloc);\n@@ -1168,1 +1446,1 @@\n-  boxing->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = boxing->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1170,1 +1448,1 @@\n-  const TypeTuple* r = boxing->tf()->range();\n+  const TypeTuple* r = boxing->tf()->range_sig();\n@@ -1274,0 +1552,1 @@\n+            Node* init_val, \/\/ value to initialize the array with\n@@ -1356,1 +1635,1 @@\n-    Node *toobig_true = new IfTrueNode( toobig_iff );\n+    Node* toobig_true = new IfTrueNode(toobig_iff);\n@@ -1359,1 +1638,1 @@\n-    toobig_false = new IfFalseNode( toobig_iff );\n+    toobig_false = new IfFalseNode(toobig_iff);\n@@ -1398,0 +1677,1 @@\n+\n@@ -1455,0 +1735,6 @@\n+    if (init_val != nullptr) {\n+      call->init_req(TypeFunc::Parms+2, init_val);\n+    }\n+  } else {\n+    \/\/ Let the runtime know if this is a larval allocation\n+    call->init_req(TypeFunc::Parms+1, _igvn.intcon(alloc->_larval));\n@@ -1486,1 +1772,1 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1492,2 +1778,2 @@\n-  if (expand_fast_path && _callprojs.fallthrough_memproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_memproj, result_phi_rawmem);\n+  if (expand_fast_path && _callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_memproj, result_phi_rawmem);\n@@ -1497,4 +1783,4 @@\n-  if (_callprojs.catchall_memproj != nullptr ) {\n-    if (_callprojs.fallthrough_memproj == nullptr) {\n-      _callprojs.fallthrough_memproj = new ProjNode(call, TypeFunc::Memory);\n-      transform_later(_callprojs.fallthrough_memproj);\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    if (_callprojs->fallthrough_memproj == nullptr) {\n+      _callprojs->fallthrough_memproj = new ProjNode(call, TypeFunc::Memory);\n+      transform_later(_callprojs->fallthrough_memproj);\n@@ -1502,2 +1788,2 @@\n-    migrate_outs(_callprojs.catchall_memproj, _callprojs.fallthrough_memproj);\n-    _igvn.remove_dead_node(_callprojs.catchall_memproj);\n+    _igvn.replace_in_uses(_callprojs->catchall_memproj, _callprojs->fallthrough_memproj);\n+    _igvn.remove_dead_node(_callprojs->catchall_memproj);\n@@ -1511,2 +1797,2 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_ioproj, result_phi_i_o);\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_ioproj, result_phi_i_o);\n@@ -1516,4 +1802,4 @@\n-  if (_callprojs.catchall_ioproj != nullptr ) {\n-    if (_callprojs.fallthrough_ioproj == nullptr) {\n-      _callprojs.fallthrough_ioproj = new ProjNode(call, TypeFunc::I_O);\n-      transform_later(_callprojs.fallthrough_ioproj);\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    if (_callprojs->fallthrough_ioproj == nullptr) {\n+      _callprojs->fallthrough_ioproj = new ProjNode(call, TypeFunc::I_O);\n+      transform_later(_callprojs->fallthrough_ioproj);\n@@ -1521,2 +1807,2 @@\n-    migrate_outs(_callprojs.catchall_ioproj, _callprojs.fallthrough_ioproj);\n-    _igvn.remove_dead_node(_callprojs.catchall_ioproj);\n+    _igvn.replace_in_uses(_callprojs->catchall_ioproj, _callprojs->fallthrough_ioproj);\n+    _igvn.remove_dead_node(_callprojs->catchall_ioproj);\n@@ -1541,2 +1827,2 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    ctrl = _callprojs.fallthrough_catchproj->clone();\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    ctrl = _callprojs->fallthrough_catchproj->clone();\n@@ -1544,1 +1830,1 @@\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, result_region);\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, result_region);\n@@ -1549,1 +1835,1 @@\n-  if (_callprojs.resproj == nullptr) {\n+  if (_callprojs->resproj[0] == nullptr) {\n@@ -1553,1 +1839,1 @@\n-    slow_result = _callprojs.resproj->clone();\n+    slow_result = _callprojs->resproj[0]->clone();\n@@ -1555,1 +1841,1 @@\n-    _igvn.replace_node(_callprojs.resproj, result_phi_rawoop);\n+    _igvn.replace_node(_callprojs->resproj[0], result_phi_rawoop);\n@@ -1565,1 +1851,1 @@\n-  result_phi_rawmem->init_req(slow_result_path, _callprojs.fallthrough_memproj);\n+  result_phi_rawmem->init_req(slow_result_path, _callprojs->fallthrough_memproj);\n@@ -1577,4 +1863,4 @@\n-  alloc->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  if (_callprojs.resproj != nullptr) {\n-    for (DUIterator_Fast imax, i = _callprojs.resproj->fast_outs(imax); i < imax; i++) {\n-      Node* use = _callprojs.resproj->fast_out(i);\n+  _callprojs = alloc->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  if (_callprojs->resproj[0] != nullptr) {\n+    for (DUIterator_Fast imax, i = _callprojs->resproj[0]->fast_outs(imax); i < imax; i++) {\n+      Node* use = _callprojs->resproj[0]->fast_out(i);\n@@ -1585,2 +1871,2 @@\n-    assert(_callprojs.resproj->outcnt() == 0, \"all uses must be deleted\");\n-    _igvn.remove_dead_node(_callprojs.resproj);\n+    assert(_callprojs->resproj[0]->outcnt() == 0, \"all uses must be deleted\");\n+    _igvn.remove_dead_node(_callprojs->resproj[0]);\n@@ -1588,3 +1874,3 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_catchproj, ctrl);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_catchproj);\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_catchproj, ctrl);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_catchproj);\n@@ -1592,3 +1878,3 @@\n-  if (_callprojs.catchall_catchproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_catchproj);\n-    _callprojs.catchall_catchproj->set_req(0, top());\n+  if (_callprojs->catchall_catchproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_catchproj);\n+    _callprojs->catchall_catchproj->set_req(0, top());\n@@ -1596,2 +1882,2 @@\n-  if (_callprojs.fallthrough_proj != nullptr) {\n-    Node* catchnode = _callprojs.fallthrough_proj->unique_ctrl_out();\n+  if (_callprojs->fallthrough_proj != nullptr) {\n+    Node* catchnode = _callprojs->fallthrough_proj->unique_ctrl_out();\n@@ -1599,1 +1885,1 @@\n-    _igvn.remove_dead_node(_callprojs.fallthrough_proj);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_proj);\n@@ -1601,3 +1887,3 @@\n-  if (_callprojs.fallthrough_memproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_memproj, mem);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_memproj);\n+  if (_callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_memproj, mem);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_memproj);\n@@ -1605,3 +1891,3 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_ioproj, i_o);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_ioproj);\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_ioproj, i_o);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_ioproj);\n@@ -1609,3 +1895,3 @@\n-  if (_callprojs.catchall_memproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_memproj);\n-    _callprojs.catchall_memproj->set_req(0, top());\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_memproj);\n+    _callprojs->catchall_memproj->set_req(0, top());\n@@ -1613,3 +1899,3 @@\n-  if (_callprojs.catchall_ioproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_ioproj);\n-    _callprojs.catchall_ioproj->set_req(0, top());\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_ioproj);\n+    _callprojs->catchall_ioproj->set_req(0, top());\n@@ -1749,5 +2035,4 @@\n-Node*\n-PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n-                                    Node* control, Node* rawmem, Node* object,\n-                                    Node* klass_node, Node* length,\n-                                    Node* size_in_bytes) {\n+Node* PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n+                                          Node* control, Node* rawmem, Node* object,\n+                                          Node* klass_node, Node* length,\n+                                          Node* size_in_bytes) {\n@@ -1756,1 +2041,1 @@\n-  Node* mark_node = alloc->make_ideal_mark(&_igvn, object, control, rawmem);\n+  Node* mark_node = alloc->make_ideal_mark(&_igvn, control, rawmem);\n@@ -1794,0 +2079,2 @@\n+                                            alloc->in(AllocateNode::InitValue),\n+                                            alloc->in(AllocateNode::RawInitValue),\n@@ -1968,1 +2255,1 @@\n-  expand_allocate_common(alloc, nullptr,\n+  expand_allocate_common(alloc, nullptr, nullptr,\n@@ -1978,0 +2265,1 @@\n+  Node* init_value = alloc->in(AllocateNode::InitValue);\n@@ -1979,0 +2267,3 @@\n+  assert(!ary_klass_t || !ary_klass_t->klass_is_exact() || !ary_klass_t->exact_klass()->is_obj_array_klass() ||\n+         ary_klass_t->is_refined_type(), \"Must be a refined array klass\");\n+  const TypeFunc* slow_call_type;\n@@ -1985,0 +2276,1 @@\n+    slow_call_type = OptoRuntime::new_array_nozero_Type();\n@@ -1987,0 +2279,7 @@\n+    slow_call_type = OptoRuntime::new_array_Type();\n+\n+    if (init_value == nullptr) {\n+      init_value = _igvn.zerocon(T_OBJECT);\n+    } else if (UseCompressedOops) {\n+      init_value = transform_later(new DecodeNNode(init_value, init_value->bottom_type()->make_ptr()));\n+    }\n@@ -1988,2 +2287,2 @@\n-  expand_allocate_common(alloc, length,\n-                         OptoRuntime::new_array_Type(),\n+  expand_allocate_common(alloc, length, init_value,\n+                         slow_call_type,\n@@ -2201,1 +2500,1 @@\n-  alock->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = alock->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -2205,2 +2504,2 @@\n-         _callprojs.fallthrough_proj != nullptr &&\n-         _callprojs.fallthrough_memproj != nullptr,\n+         _callprojs->fallthrough_proj != nullptr &&\n+         _callprojs->fallthrough_memproj != nullptr,\n@@ -2209,2 +2508,2 @@\n-  Node* fallthroughproj = _callprojs.fallthrough_proj;\n-  Node* memproj_fallthrough = _callprojs.fallthrough_memproj;\n+  Node* fallthroughproj = _callprojs->fallthrough_proj;\n+  Node* memproj_fallthrough = _callprojs->fallthrough_memproj;\n@@ -2281,1 +2580,1 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -2287,2 +2586,2 @@\n-  assert(_callprojs.fallthrough_ioproj == nullptr && _callprojs.catchall_ioproj == nullptr &&\n-         _callprojs.catchall_memproj == nullptr && _callprojs.catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n+  assert(_callprojs->fallthrough_ioproj == nullptr && _callprojs->catchall_ioproj == nullptr &&\n+         _callprojs->catchall_memproj == nullptr && _callprojs->catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n@@ -2293,1 +2592,1 @@\n-  Node *slow_ctrl = _callprojs.fallthrough_proj->clone();\n+  Node *slow_ctrl = _callprojs->fallthrough_proj->clone();\n@@ -2295,2 +2594,2 @@\n-  _igvn.hash_delete(_callprojs.fallthrough_proj);\n-  _callprojs.fallthrough_proj->disconnect_inputs(C);\n+  _igvn.hash_delete(_callprojs->fallthrough_proj);\n+  _callprojs->fallthrough_proj->disconnect_inputs(C);\n@@ -2300,1 +2599,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_proj, region);\n+  _igvn.replace_node(_callprojs->fallthrough_proj, region);\n@@ -2308,1 +2607,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, mem_phi);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, mem_phi);\n@@ -2341,3 +2640,3 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  assert(_callprojs.fallthrough_ioproj == nullptr && _callprojs.catchall_ioproj == nullptr &&\n-         _callprojs.catchall_memproj == nullptr && _callprojs.catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  assert(_callprojs->fallthrough_ioproj == nullptr && _callprojs->catchall_ioproj == nullptr &&\n+         _callprojs->catchall_memproj == nullptr && _callprojs->catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n@@ -2349,1 +2648,1 @@\n-  Node *slow_ctrl = _callprojs.fallthrough_proj->clone();\n+  Node *slow_ctrl = _callprojs->fallthrough_proj->clone();\n@@ -2351,2 +2650,2 @@\n-  _igvn.hash_delete(_callprojs.fallthrough_proj);\n-  _callprojs.fallthrough_proj->disconnect_inputs(C);\n+  _igvn.hash_delete(_callprojs->fallthrough_proj);\n+  _callprojs->fallthrough_proj->disconnect_inputs(C);\n@@ -2356,1 +2655,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_proj, region);\n+  _igvn.replace_node(_callprojs->fallthrough_proj, region);\n@@ -2363,1 +2662,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, mem_phi);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, mem_phi);\n@@ -2366,0 +2665,222 @@\n+\/\/ An inline type might be returned from the call but we don't know its\n+\/\/ type. Either we get a buffered inline type (and nothing needs to be done)\n+\/\/ or one of the values being returned is the klass of the inline type\n+\/\/ and we need to allocate an inline type instance of that type and\n+\/\/ initialize it with other values being returned. In that case, we\n+\/\/ first try a fast path allocation and initialize the value with the\n+\/\/ inline klass's pack handler or we fall back to a runtime call.\n+void PhaseMacroExpand::expand_mh_intrinsic_return(CallStaticJavaNode* call) {\n+  assert(call->method()->is_method_handle_intrinsic(), \"must be a method handle intrinsic call\");\n+  Node* ret = call->proj_out_or_null(TypeFunc::Parms);\n+  if (ret == nullptr) {\n+    return;\n+  }\n+  const TypeFunc* tf = call->_tf;\n+  const TypeTuple* domain = OptoRuntime::store_inline_type_fields_Type()->domain_cc();\n+  const TypeFunc* new_tf = TypeFunc::make(tf->domain_sig(), tf->domain_cc(), tf->range_sig(), domain);\n+  call->_tf = new_tf;\n+  \/\/ Make sure the change of type is applied before projections are processed by igvn\n+  _igvn.set_type(call, call->Value(&_igvn));\n+  _igvn.set_type(ret, ret->Value(&_igvn));\n+\n+  \/\/ Before any new projection is added:\n+  CallProjections* projs = call->extract_projections(true, true);\n+\n+  \/\/ Create temporary hook nodes that will be replaced below.\n+  \/\/ Add an input to prevent hook nodes from being dead.\n+  Node* ctl = new Node(call);\n+  Node* mem = new Node(ctl);\n+  Node* io = new Node(ctl);\n+  Node* ex_ctl = new Node(ctl);\n+  Node* ex_mem = new Node(ctl);\n+  Node* ex_io = new Node(ctl);\n+  Node* res = new Node(ctl);\n+\n+  \/\/ Allocate a new buffered inline type only if a new one is not returned\n+  Node* cast = transform_later(new CastP2XNode(ctl, res));\n+  Node* mask = MakeConX(0x1);\n+  Node* masked = transform_later(new AndXNode(cast, mask));\n+  Node* cmp = transform_later(new CmpXNode(masked, mask));\n+  Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+  IfNode* allocation_iff = new IfNode(ctl, bol, PROB_MAX, COUNT_UNKNOWN);\n+  transform_later(allocation_iff);\n+  Node* allocation_ctl = transform_later(new IfTrueNode(allocation_iff));\n+  Node* no_allocation_ctl = transform_later(new IfFalseNode(allocation_iff));\n+  Node* no_allocation_res = transform_later(new CheckCastPPNode(no_allocation_ctl, res, TypeInstPtr::BOTTOM));\n+\n+  \/\/ Try to allocate a new buffered inline instance either from TLAB or eden space\n+  Node* needgc_ctrl = nullptr; \/\/ needgc means slowcase, i.e. allocation failed\n+  CallLeafNoFPNode* handler_call;\n+  const bool alloc_in_place = UseTLAB;\n+  if (alloc_in_place) {\n+    Node* fast_oop_ctrl = nullptr;\n+    Node* fast_oop_rawmem = nullptr;\n+    Node* mask2 = MakeConX(-2);\n+    Node* masked2 = transform_later(new AndXNode(cast, mask2));\n+    Node* rawklassptr = transform_later(new CastX2PNode(masked2));\n+    Node* klass_node = transform_later(new CheckCastPPNode(allocation_ctl, rawklassptr, TypeInstKlassPtr::OBJECT_OR_NULL));\n+    Node* layout_val = make_load(nullptr, mem, klass_node, in_bytes(Klass::layout_helper_offset()), TypeInt::INT, T_INT);\n+    Node* size_in_bytes = ConvI2X(layout_val);\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    Node* fast_oop = bs->obj_allocate(this, mem, allocation_ctl, size_in_bytes, io, needgc_ctrl,\n+                                      fast_oop_ctrl, fast_oop_rawmem,\n+                                      AllocateInstancePrefetchLines);\n+    \/\/ Allocation succeed, initialize buffered inline instance header firstly,\n+    \/\/ and then initialize its fields with an inline class specific handler\n+    Node* mark_word_node;\n+    if (UseCompactObjectHeaders) {\n+      \/\/ COH: We need to load the prototype from the klass at runtime since it encodes the klass pointer already.\n+      mark_word_node = make_load(fast_oop_ctrl, fast_oop_rawmem, klass_node, in_bytes(Klass::prototype_header_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+    } else {\n+      \/\/ Otherwise, use the static prototype.\n+      mark_word_node = makecon(TypeRawPtr::make((address)markWord::inline_type_prototype().value()));\n+    }\n+\n+    fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::mark_offset_in_bytes(), mark_word_node, T_ADDRESS);\n+    if (!UseCompactObjectHeaders) {\n+      \/\/ COH: Everything is encoded in the mark word, so nothing left to do.\n+      fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+      if (UseCompressedClassPointers) {\n+        fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::klass_gap_offset_in_bytes(), intcon(0), T_INT);\n+      }\n+    }\n+    Node* members  = make_load(fast_oop_ctrl, fast_oop_rawmem, klass_node, in_bytes(InlineKlass::adr_members_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+    Node* pack_handler = make_load(fast_oop_ctrl, fast_oop_rawmem, members, in_bytes(InlineKlass::pack_handler_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+    handler_call = new CallLeafNoFPNode(OptoRuntime::pack_inline_type_Type(),\n+                                        nullptr,\n+                                        \"pack handler\",\n+                                        TypeRawPtr::BOTTOM);\n+    handler_call->init_req(TypeFunc::Control, fast_oop_ctrl);\n+    handler_call->init_req(TypeFunc::Memory, fast_oop_rawmem);\n+    handler_call->init_req(TypeFunc::I_O, top());\n+    handler_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+    handler_call->init_req(TypeFunc::ReturnAdr, top());\n+    handler_call->init_req(TypeFunc::Parms, pack_handler);\n+    handler_call->init_req(TypeFunc::Parms+1, fast_oop);\n+  } else {\n+    needgc_ctrl = allocation_ctl;\n+  }\n+\n+  \/\/ Allocation failed, fall back to a runtime call\n+  CallStaticJavaNode* slow_call = new CallStaticJavaNode(OptoRuntime::store_inline_type_fields_Type(),\n+                                                         StubRoutines::store_inline_type_fields_to_buf(),\n+                                                         \"store_inline_type_fields\",\n+                                                         TypePtr::BOTTOM);\n+  slow_call->init_req(TypeFunc::Control, needgc_ctrl);\n+  slow_call->init_req(TypeFunc::Memory, mem);\n+  slow_call->init_req(TypeFunc::I_O, io);\n+  slow_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+  slow_call->init_req(TypeFunc::ReturnAdr, call->in(TypeFunc::ReturnAdr));\n+  slow_call->init_req(TypeFunc::Parms, res);\n+\n+  Node* slow_ctl = transform_later(new ProjNode(slow_call, TypeFunc::Control));\n+  Node* slow_mem = transform_later(new ProjNode(slow_call, TypeFunc::Memory));\n+  Node* slow_io = transform_later(new ProjNode(slow_call, TypeFunc::I_O));\n+  Node* slow_res = transform_later(new ProjNode(slow_call, TypeFunc::Parms));\n+  Node* slow_catc = transform_later(new CatchNode(slow_ctl, slow_io, 2));\n+  Node* slow_norm = transform_later(new CatchProjNode(slow_catc, CatchProjNode::fall_through_index, CatchProjNode::no_handler_bci));\n+  Node* slow_excp = transform_later(new CatchProjNode(slow_catc, CatchProjNode::catch_all_index,    CatchProjNode::no_handler_bci));\n+\n+  Node* ex_r = new RegionNode(3);\n+  Node* ex_mem_phi = new PhiNode(ex_r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* ex_io_phi = new PhiNode(ex_r, Type::ABIO);\n+  ex_r->init_req(1, slow_excp);\n+  ex_mem_phi->init_req(1, slow_mem);\n+  ex_io_phi->init_req(1, slow_io);\n+  ex_r->init_req(2, ex_ctl);\n+  ex_mem_phi->init_req(2, ex_mem);\n+  ex_io_phi->init_req(2, ex_io);\n+  transform_later(ex_r);\n+  transform_later(ex_mem_phi);\n+  transform_later(ex_io_phi);\n+\n+  \/\/ We don't know how many values are returned. This assumes the\n+  \/\/ worst case, that all available registers are used.\n+  for (uint i = TypeFunc::Parms+1; i < domain->cnt(); i++) {\n+    if (domain->field_at(i) == Type::HALF) {\n+      slow_call->init_req(i, top());\n+      if (alloc_in_place) {\n+        handler_call->init_req(i+1, top());\n+      }\n+      continue;\n+    }\n+    Node* proj = transform_later(new ProjNode(call, i));\n+    slow_call->init_req(i, proj);\n+    if (alloc_in_place) {\n+      handler_call->init_req(i+1, proj);\n+    }\n+  }\n+  \/\/ We can safepoint at that new call\n+  slow_call->copy_call_debug_info(&_igvn, call);\n+  transform_later(slow_call);\n+  if (alloc_in_place) {\n+    transform_later(handler_call);\n+  }\n+\n+  Node* fast_ctl = nullptr;\n+  Node* fast_res = nullptr;\n+  MergeMemNode* fast_mem = nullptr;\n+  if (alloc_in_place) {\n+    fast_ctl = transform_later(new ProjNode(handler_call, TypeFunc::Control));\n+    Node* rawmem = transform_later(new ProjNode(handler_call, TypeFunc::Memory));\n+    fast_res = transform_later(new ProjNode(handler_call, TypeFunc::Parms));\n+    fast_mem = MergeMemNode::make(mem);\n+    fast_mem->set_memory_at(Compile::AliasIdxRaw, rawmem);\n+    transform_later(fast_mem);\n+  }\n+\n+  Node* r = new RegionNode(alloc_in_place ? 4 : 3);\n+  Node* mem_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* io_phi = new PhiNode(r, Type::ABIO);\n+  Node* res_phi = new PhiNode(r, TypeInstPtr::BOTTOM);\n+  r->init_req(1, no_allocation_ctl);\n+  mem_phi->init_req(1, mem);\n+  io_phi->init_req(1, io);\n+  res_phi->init_req(1, no_allocation_res);\n+  r->init_req(2, slow_norm);\n+  mem_phi->init_req(2, slow_mem);\n+  io_phi->init_req(2, slow_io);\n+  res_phi->init_req(2, slow_res);\n+  if (alloc_in_place) {\n+    r->init_req(3, fast_ctl);\n+    mem_phi->init_req(3, fast_mem);\n+    io_phi->init_req(3, io);\n+    res_phi->init_req(3, fast_res);\n+  }\n+  transform_later(r);\n+  transform_later(mem_phi);\n+  transform_later(io_phi);\n+  transform_later(res_phi);\n+\n+  \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+  \/\/ store that would make this buffer accessible by other threads.\n+  MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxBot);\n+  transform_later(mb);\n+  mb->init_req(TypeFunc::Memory, mem_phi);\n+  mb->init_req(TypeFunc::Control, r);\n+  r = new ProjNode(mb, TypeFunc::Control);\n+  transform_later(r);\n+  mem_phi = new ProjNode(mb, TypeFunc::Memory);\n+  transform_later(mem_phi);\n+\n+  assert(projs->nb_resproj == 1, \"unexpected number of results\");\n+  _igvn.replace_in_uses(projs->fallthrough_catchproj, r);\n+  _igvn.replace_in_uses(projs->fallthrough_memproj, mem_phi);\n+  _igvn.replace_in_uses(projs->fallthrough_ioproj, io_phi);\n+  _igvn.replace_in_uses(projs->resproj[0], res_phi);\n+  _igvn.replace_in_uses(projs->catchall_catchproj, ex_r);\n+  _igvn.replace_in_uses(projs->catchall_memproj, ex_mem_phi);\n+  _igvn.replace_in_uses(projs->catchall_ioproj, ex_io_phi);\n+  \/\/ The CatchNode should not use the ex_io_phi. Re-connect it to the catchall_ioproj.\n+  Node* cn = projs->fallthrough_catchproj->in(0);\n+  _igvn.replace_input_of(cn, 1, projs->catchall_ioproj);\n+\n+  _igvn.replace_node(ctl, projs->fallthrough_catchproj);\n+  _igvn.replace_node(mem, projs->fallthrough_memproj);\n+  _igvn.replace_node(io, projs->fallthrough_ioproj);\n+  _igvn.replace_node(res, projs->resproj[0]);\n+  _igvn.replace_node(ex_ctl, projs->catchall_catchproj);\n+  _igvn.replace_node(ex_mem, projs->catchall_memproj);\n+  _igvn.replace_node(ex_io, projs->catchall_ioproj);\n+ }\n+\n@@ -2391,1 +2912,1 @@\n-      subklass = _igvn.transform(LoadKlassNode::make(_igvn, C->immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+      subklass = _igvn.transform(LoadKlassNode::make(_igvn, C->immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n@@ -2403,0 +2924,113 @@\n+\/\/ FlatArrayCheckNode (array1 array2 ...) is expanded into:\n+\/\/\n+\/\/ long mark = array1.mark | array2.mark | ...;\n+\/\/ long locked_bit = markWord::unlocked_value & array1.mark & array2.mark & ...;\n+\/\/ if (locked_bit == 0) {\n+\/\/   \/\/ One array is locked, load prototype header from the klass\n+\/\/   mark = array1.klass.proto | array2.klass.proto | ...\n+\/\/ }\n+\/\/ if ((mark & markWord::flat_array_bit_in_place) == 0) {\n+\/\/    ...\n+\/\/ }\n+void PhaseMacroExpand::expand_flatarraycheck_node(FlatArrayCheckNode* check) {\n+  bool array_inputs = _igvn.type(check->in(FlatArrayCheckNode::ArrayOrKlass))->isa_oopptr() != nullptr;\n+  if (array_inputs) {\n+    Node* mark = MakeConX(0);\n+    Node* locked_bit = MakeConX(markWord::unlocked_value);\n+    Node* mem = check->in(FlatArrayCheckNode::Memory);\n+    for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+      Node* ary = check->in(i);\n+      const TypeOopPtr* t = _igvn.type(ary)->isa_oopptr();\n+      assert(t != nullptr, \"Mixing array and klass inputs\");\n+      assert(!t->is_flat() && !t->is_not_flat(), \"Should have been optimized out\");\n+      Node* mark_adr = basic_plus_adr(ary, oopDesc::mark_offset_in_bytes());\n+      Node* mark_load = _igvn.transform(LoadNode::make(_igvn, nullptr, mem, mark_adr, mark_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+      mark = _igvn.transform(new OrXNode(mark, mark_load));\n+      locked_bit = _igvn.transform(new AndXNode(locked_bit, mark_load));\n+    }\n+    assert(!mark->is_Con(), \"Should have been optimized out\");\n+    Node* cmp = _igvn.transform(new CmpXNode(locked_bit, MakeConX(0)));\n+    Node* is_unlocked = _igvn.transform(new BoolNode(cmp, BoolTest::ne));\n+\n+    \/\/ BoolNode might be shared, replace each if user\n+    Node* old_bol = check->unique_out();\n+    assert(old_bol->is_Bool() && old_bol->as_Bool()->_test._test == BoolTest::ne, \"unexpected condition\");\n+    for (DUIterator_Last imin, i = old_bol->last_outs(imin); i >= imin; --i) {\n+      IfNode* old_iff = old_bol->last_out(i)->as_If();\n+      Node* ctrl = old_iff->in(0);\n+      RegionNode* region = new RegionNode(3);\n+      Node* mark_phi = new PhiNode(region, TypeX_X);\n+\n+      \/\/ Check if array is unlocked\n+      IfNode* iff = _igvn.transform(new IfNode(ctrl, is_unlocked, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+\n+      \/\/ Unlocked: Use bits from mark word\n+      region->init_req(1, _igvn.transform(new IfTrueNode(iff)));\n+      mark_phi->init_req(1, mark);\n+\n+      \/\/ Locked: Load prototype header from klass\n+      ctrl = _igvn.transform(new IfFalseNode(iff));\n+      Node* proto = MakeConX(0);\n+      for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+        Node* ary = check->in(i);\n+        \/\/ Make loads control dependent to make sure they are only executed if array is locked\n+        Node* klass_adr = basic_plus_adr(ary, oopDesc::klass_offset_in_bytes());\n+        Node* klass = _igvn.transform(LoadKlassNode::make(_igvn, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+        Node* proto_adr = basic_plus_adr(klass, in_bytes(Klass::prototype_header_offset()));\n+        Node* proto_load = _igvn.transform(LoadNode::make(_igvn, ctrl, C->immutable_memory(), proto_adr, proto_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+        proto = _igvn.transform(new OrXNode(proto, proto_load));\n+      }\n+      region->init_req(2, ctrl);\n+      mark_phi->init_req(2, proto);\n+\n+      \/\/ Check if flat array bits are set\n+      Node* mask = MakeConX(markWord::flat_array_bit_in_place);\n+      Node* masked = _igvn.transform(new AndXNode(_igvn.transform(mark_phi), mask));\n+      cmp = _igvn.transform(new CmpXNode(masked, MakeConX(0)));\n+      Node* is_not_flat = _igvn.transform(new BoolNode(cmp, BoolTest::eq));\n+\n+      ctrl = _igvn.transform(region);\n+      iff = _igvn.transform(new IfNode(ctrl, is_not_flat, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+      _igvn.replace_node(old_iff, iff);\n+    }\n+    _igvn.replace_node(check, C->top());\n+  } else {\n+    \/\/ Fall back to layout helper check\n+    Node* lhs = intcon(0);\n+    for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+      Node* array_or_klass = check->in(i);\n+      Node* klass = nullptr;\n+      const TypePtr* t = _igvn.type(array_or_klass)->is_ptr();\n+      assert(!t->is_flat() && !t->is_not_flat(), \"Should have been optimized out\");\n+      if (t->isa_oopptr() != nullptr) {\n+        Node* klass_adr = basic_plus_adr(array_or_klass, oopDesc::klass_offset_in_bytes());\n+        klass = transform_later(LoadKlassNode::make(_igvn, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+      } else {\n+        assert(t->isa_klassptr(), \"Unexpected input type\");\n+        klass = array_or_klass;\n+      }\n+      Node* lh_addr = basic_plus_adr(klass, in_bytes(Klass::layout_helper_offset()));\n+      Node* lh_val = _igvn.transform(LoadNode::make(_igvn, nullptr, C->immutable_memory(), lh_addr, lh_addr->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));\n+      lhs = _igvn.transform(new OrINode(lhs, lh_val));\n+    }\n+    Node* masked = transform_later(new AndINode(lhs, intcon(Klass::_lh_array_tag_flat_value_bit_inplace)));\n+    Node* cmp = transform_later(new CmpINode(masked, intcon(0)));\n+    Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+    Node* m2b = transform_later(new Conv2BNode(masked));\n+    \/\/ The matcher expects the input to If\/CMove nodes to be produced by a Bool(CmpI..)\n+    \/\/ pattern, but the input to other potential users (e.g. Phi) to be some\n+    \/\/ other pattern (e.g. a Conv2B node, possibly idealized as a CMoveI).\n+    Node* old_bol = check->unique_out();\n+    for (DUIterator_Last imin, i = old_bol->last_outs(imin); i >= imin; --i) {\n+      Node* user = old_bol->last_out(i);\n+      for (uint j = 0; j < user->req(); j++) {\n+        Node* n = user->in(j);\n+        if (n == old_bol) {\n+          _igvn.replace_input_of(user, j, (user->is_If() || user->is_CMove()) ? bol : m2b);\n+        }\n+      }\n+    }\n+    _igvn.replace_node(check, C->top());\n+  }\n+}\n+\n@@ -2415,2 +3049,2 @@\n-void PhaseMacroExpand::eliminate_macro_nodes() {\n-  if (C->macro_count() == 0)\n+void PhaseMacroExpand::eliminate_macro_nodes(bool eliminate_locks) {\n+  if (C->macro_count() == 0) {\n@@ -2418,0 +3052,1 @@\n+  }\n@@ -2424,7 +3059,5 @@\n-  \/\/ Before elimination may re-mark (change to Nested or NonEscObj)\n-  \/\/ all associated (same box and obj) lock and unlock nodes.\n-  int cnt = C->macro_count();\n-  for (int i=0; i < cnt; i++) {\n-    Node *n = C->macro_node(i);\n-    if (n->is_AbstractLock()) { \/\/ Lock and Unlock nodes\n-      mark_eliminated_locking_nodes(n->as_AbstractLock());\n+  int iteration = 0;\n+  while (C->macro_count() > 0) {\n+    if (iteration++ > 100) {\n+      assert(false, \"Too slow convergence of macro elimination\");\n+      break;\n@@ -2432,23 +3065,11 @@\n-  }\n-  \/\/ Re-marking may break consistency of Coarsened locks.\n-  if (!C->coarsened_locks_consistent()) {\n-    return; \/\/ recompile without Coarsened locks if broken\n-  } else {\n-    \/\/ After coarsened locks are eliminated locking regions\n-    \/\/ become unbalanced. We should not execute any more\n-    \/\/ locks elimination optimizations on them.\n-    C->mark_unbalanced_boxes();\n-  }\n-  \/\/ First, attempt to eliminate locks\n-  bool progress = true;\n-  while (progress) {\n-    progress = false;\n-    for (int i = C->macro_count(); i > 0; i = MIN2(i - 1, C->macro_count())) { \/\/ more than 1 element can be eliminated at once\n-      Node* n = C->macro_node(i - 1);\n-      bool success = false;\n-      DEBUG_ONLY(int old_macro_count = C->macro_count();)\n-      if (n->is_AbstractLock()) {\n-        success = eliminate_locking_node(n->as_AbstractLock());\n-#ifndef PRODUCT\n-        if (success && PrintOptoStatistics) {\n-          AtomicAccess::inc(&PhaseMacroExpand::_monitor_objects_removed_counter);\n+    \/\/ Postpone lock elimination to after EA when most allocations are eliminated\n+    \/\/ because they might block lock elimination if their escape state isn't\n+    \/\/ determined yet and we only got one chance at eliminating the lock.\n+    if (eliminate_locks) {\n+      \/\/ Before elimination may re-mark (change to Nested or NonEscObj)\n+      \/\/ all associated (same box and obj) lock and unlock nodes.\n+      int cnt = C->macro_count();\n+      for (int i=0; i < cnt; i++) {\n+        Node *n = C->macro_node(i);\n+        if (n->is_AbstractLock()) { \/\/ Lock and Unlock nodes\n+          mark_eliminated_locking_nodes(n->as_AbstractLock());\n@@ -2457,5 +3078,8 @@\n-#endif\n-      assert(success == (C->macro_count() < old_macro_count), \"elimination reduces macro count\");\n-      progress = progress || success;\n-      if (success) {\n-        C->print_method(PHASE_AFTER_MACRO_ELIMINATION_STEP, 5, n);\n+      \/\/ Re-marking may break consistency of Coarsened locks.\n+      if (!C->coarsened_locks_consistent()) {\n+        return; \/\/ recompile without Coarsened locks if broken\n+      } else {\n+        \/\/ After coarsened locks are eliminated locking regions\n+        \/\/ become unbalanced. We should not execute any more\n+        \/\/ locks elimination optimizations on them.\n+        C->mark_unbalanced_boxes();\n@@ -2465,5 +3089,2 @@\n-  }\n-  \/\/ Next, attempt to eliminate allocations\n-  progress = true;\n-  while (progress) {\n-    progress = false;\n+\n+    bool progress = false;\n@@ -2484,2 +3105,5 @@\n-      case Node::Class_CallStaticJava:\n-        success = eliminate_boxing_node(n->as_CallStaticJava());\n+      case Node::Class_CallStaticJava: {\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          success = eliminate_boxing_node(n->as_CallStaticJava());\n+        }\n@@ -2487,0 +3111,1 @@\n+      }\n@@ -2489,1 +3114,8 @@\n-        assert(!n->as_AbstractLock()->is_eliminated(), \"sanity\");\n+        if (eliminate_locks) {\n+          success = eliminate_locking_node(n->as_AbstractLock());\n+#ifndef PRODUCT\n+          if (success && PrintOptoStatistics) {\n+            AtomicAccess::inc(&PhaseMacroExpand::_monitor_objects_removed_counter);\n+          }\n+#endif\n+        }\n@@ -2499,0 +3131,2 @@\n+      case Node::Class_FlatArrayCheck:\n+        break;\n@@ -2516,0 +3150,16 @@\n+\n+    \/\/ Ensure the graph after PhaseMacroExpand::eliminate_macro_nodes is canonical (no igvn\n+    \/\/ transformation is pending). If an allocation is used only in safepoints, elimination of\n+    \/\/ other macro nodes can remove all these safepoints, allowing the allocation to be removed.\n+    \/\/ Hence after igvn we retry removing macro nodes if some progress that has been made in this\n+    \/\/ iteration.\n+    _igvn.set_delay_transform(false);\n+    _igvn.optimize();\n+    if (C->failing()) {\n+      return;\n+    }\n+    _igvn.set_delay_transform(true);\n+\n+    if (!progress) {\n+      break;\n+    }\n@@ -2544,4 +3194,7 @@\n-        \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n-        C->remove_macro_node(n);\n-        _igvn._worklist.push(n);\n-        success = true;\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n+          C->remove_macro_node(n);\n+          _igvn._worklist.push(n);\n+          success = true;\n+        }\n@@ -2656,0 +3309,7 @@\n+    case Node::Class_CallStaticJava:\n+      expand_mh_intrinsic_return(n->as_CallStaticJava());\n+      C->remove_macro_node(n);\n+      break;\n+    case Node::Class_FlatArrayCheck:\n+      expand_flatarraycheck_node(n->as_FlatArrayCheck());\n+      break;\n@@ -2667,1 +3327,1 @@\n-        for (unsigned int i = 0; i < mod_macro->tf()->domain()->cnt() - TypeFunc::Parms; i++) {\n+        for (unsigned int i = 0; i < mod_macro->tf()->domain_cc()->cnt() - TypeFunc::Parms; i++) {\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":972,"deletions":312,"binary":false,"changes":1284,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -27,0 +29,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -33,0 +36,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -36,0 +40,1 @@\n+#include \"opto\/callnode.hpp\"\n@@ -40,0 +45,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -53,0 +59,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -55,0 +62,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -143,8 +151,105 @@\n-Node *MemNode::optimize_simple_memory_chain(Node *mchain, const TypeOopPtr *t_oop, Node *load, PhaseGVN *phase) {\n-  assert((t_oop != nullptr), \"sanity\");\n-  bool is_instance = t_oop->is_known_instance_field();\n-  bool is_boxed_value_load = t_oop->is_ptr_to_boxed_value() &&\n-                             (load != nullptr) && load->is_Load() &&\n-                             (phase->is_IterGVN() != nullptr);\n-  if (!(is_instance || is_boxed_value_load))\n-    return mchain;  \/\/ don't try to optimize non-instance types\n+\/\/ Find the memory output corresponding to the fall-through path of a call\n+static Node* find_call_fallthrough_mem_output(CallNode* call) {\n+  ResourceMark rm;\n+  CallProjections* projs = call->extract_projections(false, false);\n+  Node* res = projs->fallthrough_memproj;\n+  assert(res != nullptr, \"must have a fallthrough mem output\");\n+  return res;\n+}\n+\n+\/\/ Try to find a better memory input for a load from a strict final field\n+static Node* try_optimize_strict_final_load_memory(PhaseGVN* phase, Node* adr, ProjNode*& base_local) {\n+  intptr_t offset = 0;\n+  Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);\n+  if (base == nullptr) {\n+    return nullptr;\n+  }\n+\n+  Node* base_uncasted = base->uncast();\n+  if (base_uncasted->is_Proj()) {\n+    MultiNode* multi = base_uncasted->in(0)->as_Multi();\n+    if (multi->is_Allocate()) {\n+      base_local = base_uncasted->as_Proj();\n+      return nullptr;\n+    } else if (multi->is_Call()) {\n+      \/\/ The oop is returned from a call, the memory can be the fallthrough output of the call\n+      return find_call_fallthrough_mem_output(multi->as_Call());\n+    } else if (multi->is_Start()) {\n+      \/\/ The oop is a parameter\n+      if (phase->C->method()->is_object_constructor() && base_uncasted->as_Proj()->_con == TypeFunc::Parms) {\n+        \/\/ The receiver of a constructor is similar to the result of an AllocateNode\n+        base_local = base_uncasted->as_Proj();\n+        return nullptr;\n+      } else {\n+        \/\/ Use the start memory otherwise\n+        return multi->proj_out(TypeFunc::Memory);\n+      }\n+    }\n+  }\n+\n+  return nullptr;\n+}\n+\n+\/\/ Whether a call can modify a strict final field, given that the object is allocated inside the\n+\/\/ current compilation unit, or is the first parameter when the compilation root is a constructor.\n+\/\/ This is equivalent to asking whether 'call' is a constructor invocation and the class declaring\n+\/\/ the target method is a subclass of the class declaring 'field'.\n+static bool call_can_modify_local_object(ciField* field, CallNode* call) {\n+  if (!call->is_CallJava()) {\n+    return false;\n+  }\n+\n+  ciMethod* target = call->as_CallJava()->method();\n+  if (target == nullptr || !target->is_object_constructor()) {\n+    return false;\n+  }\n+\n+  \/\/ If 'field' is declared in a class that is a subclass of the one declaring the constructor,\n+  \/\/ then the field is set inside the constructor, else the field must be set before the\n+  \/\/ constructor invocation. E.g. A field Super.x will be set during the execution of Sub::<init>,\n+  \/\/ while a field Sub.y must be set before Super::<init> is invoked.\n+  \/\/ We can try to be more heroic and decide if the receiver of the constructor invocation is the\n+  \/\/ object from which we are loading from. This, however, may be problematic as deciding if 2\n+  \/\/ nodes are definitely different may not be trivial, especially if the graph is not canonical.\n+  \/\/ As a result, it is made more conservative for now.\n+  assert(call->req() > TypeFunc::Parms, \"constructor must have at least 1 argument\");\n+  return target->holder()->is_subclass_of(field->holder());\n+}\n+\n+Node* MemNode::optimize_simple_memory_chain(Node* mchain, const TypeOopPtr* t_oop, Node* load, PhaseGVN* phase) {\n+  assert(t_oop != nullptr, \"sanity\");\n+  bool is_known_instance = t_oop->is_known_instance_field();\n+  bool is_strict_final_load = false;\n+\n+  \/\/ After macro expansion, an allocation may become a call, changing the memory input to the\n+  \/\/ memory output of that call would be illegal. As a result, disallow this transformation after\n+  \/\/ macro expansion.\n+  if (phase->is_IterGVN() && phase->C->allow_macro_nodes() && load != nullptr && load->is_Load() && !load->as_Load()->is_mismatched_access()) {\n+    is_strict_final_load = t_oop->is_ptr_to_strict_final_field();\n+#ifdef ASSERT\n+    if ((t_oop->is_inlinetypeptr() && t_oop->inline_klass()->contains_field_offset(t_oop->offset())) || t_oop->is_ptr_to_boxed_value()) {\n+      assert(is_strict_final_load, \"sanity check for basic cases\");\n+    }\n+#endif \/\/ ASSERT\n+  }\n+\n+  if (!is_known_instance && !is_strict_final_load) {\n+    return mchain;\n+  }\n+\n+  Node* result = mchain;\n+  ProjNode* base_local = nullptr;\n+\n+  ciField* field = nullptr;\n+  if (is_strict_final_load) {\n+    field = phase->C->alias_type(t_oop)->field();\n+    assert(field != nullptr, \"must point to a field\");\n+\n+    Node* adr = load->in(MemNode::Address);\n+    assert(phase->type(adr) == t_oop, \"inconsistent type\");\n+    Node* tmp = try_optimize_strict_final_load_memory(phase, adr, base_local);\n+    if (tmp != nullptr) {\n+      result = tmp;\n+    }\n+  }\n+\n@@ -152,3 +257,2 @@\n-  Node *start_mem = phase->C->start()->proj_out_or_null(TypeFunc::Memory);\n-  Node *prev = nullptr;\n-  Node *result = mchain;\n+  Node* start_mem = phase->C->start()->proj_out_or_null(TypeFunc::Memory);\n+  Node* prev = nullptr;\n@@ -157,2 +261,5 @@\n-    if (result == start_mem)\n-      break;  \/\/ hit one of our sentinels\n+    if (result == start_mem) {\n+      \/\/ start_mem is the earliest memory possible\n+      break;\n+    }\n+\n@@ -161,1 +268,1 @@\n-      Node *proj_in = result->in(0);\n+      Node* proj_in = result->in(0);\n@@ -163,1 +270,2 @@\n-        break;  \/\/ hit one of our sentinels\n+        \/\/ This is the allocation that creates the object from which we are loading from\n+        break;\n@@ -166,2 +274,4 @@\n-        CallNode *call = proj_in->as_Call();\n-        if (!call->may_modify(t_oop, phase)) { \/\/ returns false for instances\n+        CallNode* call = proj_in->as_Call();\n+        if (!call->may_modify(t_oop, phase)) {\n+          result = call->in(TypeFunc::Memory);\n+        } else if (is_strict_final_load && base_local != nullptr && !call_can_modify_local_object(field, call)) {\n@@ -170,0 +280,3 @@\n+      } else if (proj_in->Opcode() == Op_Tuple) {\n+        \/\/ The call will be folded, skip over it.\n+        break;\n@@ -177,1 +290,1 @@\n-        if (is_instance) {\n+        if (is_known_instance) {\n@@ -179,1 +292,1 @@\n-        } else if (is_boxed_value_load) {\n+        } else if (is_strict_final_load) {\n@@ -183,1 +296,5 @@\n-            result = proj_in->in(TypeFunc::Memory); \/\/ not related allocation\n+            \/\/ Allocation of another type, must be another object\n+            result = proj_in->in(TypeFunc::Memory);\n+          } else if (base_local != nullptr && (base_local->is_Parm() || base_local->in(0) != alloc)) {\n+            \/\/ Allocation of another object\n+            result = proj_in->in(TypeFunc::Memory);\n@@ -192,0 +309,5 @@\n+      } else if (proj_in->is_LoadFlat() || proj_in->is_StoreFlat()) {\n+        if (is_strict_final_load) {\n+          \/\/ LoadFlat and StoreFlat cannot happen to strict final fields\n+          result = proj_in->in(TypeFunc::Memory);\n+        }\n@@ -195,1 +317,1 @@\n-        assert(false, \"unexpected projection\");\n+        assert(false, \"unexpected projection of %s\", proj_in->Name());\n@@ -198,1 +320,1 @@\n-      if (!is_instance || !ClearArrayNode::step_through(&result, instance_id, phase)) {\n+      if (!is_known_instance || !ClearArrayNode::step_through(&result, instance_id, phase)) {\n@@ -236,0 +358,2 @@\n+                     ->cast_to_not_flat(t_oop->is_aryptr()->is_not_flat())\n+                     ->cast_to_not_null_free(t_oop->is_aryptr()->is_not_null_free())\n@@ -262,1 +386,1 @@\n-               tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n+        tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n@@ -974,0 +1098,1 @@\n+  case T_ARRAY:\n@@ -987,1 +1112,1 @@\n-    ShouldNotReachHere();\n+    assert(false, \"unexpected basic type %s\", type2name(bt));\n@@ -1022,1 +1147,1 @@\n-    return (eliminate_boxing && non_volatile) || is_stable_ary;\n+    return (eliminate_boxing && non_volatile) || is_stable_ary || tp->is_inlinetypeptr();\n@@ -1078,2 +1203,1 @@\n-      uint header = arrayOopDesc::base_offset_in_bytes(ary_elem);\n-      uint shift  = exact_log2(type2aelembytes(ary_elem));\n+      uint shift  = ary_t->is_flat() ? ary_t->flat_log_elem_size() : exact_log2(type2aelembytes(ary_elem));\n@@ -1103,0 +1227,10 @@\n+static Node* see_through_inline_type(PhaseValues* phase, const MemNode* load, Node* base, int offset) {\n+  if (!load->is_mismatched_access() && base != nullptr && base->is_InlineType() && offset > oopDesc::klass_offset_in_bytes()) {\n+    InlineTypeNode* vt = base->as_InlineType();\n+    Node* value = vt->field_value_by_offset(offset, true);\n+    assert(value != nullptr, \"must see some value\");\n+    return value;\n+  }\n+\n+  return nullptr;\n+}\n@@ -1111,0 +1245,1 @@\n+\/\/ This method may find an unencoded node instead of the corresponding encoded one.\n@@ -1115,0 +1250,9 @@\n+  \/\/ Try to see through an InlineTypeNode\n+  \/\/ LoadN is special because the input is not compressed\n+  if (Opcode() != Op_LoadN) {\n+    Node* value = see_through_inline_type(phase, this, ld_base, ld_off);\n+    if (value != nullptr) {\n+      return value;\n+    }\n+  }\n+\n@@ -1198,1 +1342,1 @@\n-        const TypeVect* out_vt = as_LoadVector()->vect_type();\n+        const TypeVect* out_vt = is_Load() ? as_LoadVector()->vect_type() : as_StoreVector()->vect_type();\n@@ -1216,0 +1360,29 @@\n+      Node* init_value = ld_alloc->in(AllocateNode::InitValue);\n+      if (init_value != nullptr) {\n+        const TypeAryPtr* ld_adr_type = phase->type(ld_adr)->isa_aryptr();\n+        if (ld_adr_type == nullptr) {\n+          return nullptr;\n+        }\n+\n+        \/\/ We know that this is not a flat array, the load should return the whole oop\n+        if (ld_adr_type->is_not_flat()) {\n+          return init_value;\n+        }\n+\n+        \/\/ If this is a flat array, try to see through init_value\n+        if (init_value->is_EncodeP()) {\n+          init_value = init_value->in(1);\n+        }\n+        if (!init_value->is_InlineType() || ld_adr_type->field_offset() == Type::Offset::bottom) {\n+          return nullptr;\n+        }\n+\n+        ciInlineKlass* vk = phase->type(init_value)->inline_klass();\n+        int field_offset_in_payload = ld_adr_type->field_offset().get();\n+        if (field_offset_in_payload == vk->null_marker_offset_in_payload()) {\n+          return init_value->as_InlineType()->get_null_marker();\n+        } else {\n+          return init_value->as_InlineType()->field_value_by_offset(field_offset_in_payload + vk->payload_offset(), true);\n+        }\n+      }\n+      assert(ld_alloc->in(AllocateNode::RawInitValue) == nullptr, \"init value may not be null\");\n@@ -1268,1 +1441,1 @@\n-    \/\/ Only instances and boxed values.\n+    \/\/ Only known instances and immutable fields\n@@ -1270,1 +1443,1 @@\n-        (t_oop->is_ptr_to_boxed_value() ||\n+        (t_oop->is_ptr_to_strict_final_field() ||\n@@ -1298,0 +1471,4 @@\n+\n+    if (phase->type(value)->isa_ptr() && phase->type(this)->isa_narrowoop()) {\n+      return this;\n+    }\n@@ -1319,2 +1496,2 @@\n-         addr_t->is_ptr_to_boxed_value()) {\n-      \/\/ Use _idx of address base (could be Phi node) for boxed values.\n+         addr_t->is_ptr_to_strict_final_field()) {\n+      \/\/ Use _idx of address base (could be Phi node) for immutable fields in unknown instances\n@@ -1876,0 +2053,2 @@\n+        \/\/ TODO 8350865 Can we re-enable this?\n+        && !(phase->type(address)->is_inlinetypeptr() && is_mismatched_access())\n@@ -1972,1 +2151,8 @@\n-  return progress ? this : nullptr;\n+  if (progress) {\n+    return this;\n+  }\n+\n+  if (!can_reshape) {\n+    phase->record_for_igvn(this);\n+  }\n+  return nullptr;\n@@ -2020,2 +2206,6 @@\n-    assert(value->bottom_type()->higher_equal(_type), \"sanity\");\n-    return value->bottom_type();\n+    if (phase->type(value)->isa_ptr() && _type->isa_narrowoop()) {\n+      return phase->type(value)->make_narrowoop();\n+    } else {\n+      assert(value->bottom_type()->higher_equal(_type), \"sanity\");\n+      return phase->type(value);\n+    }\n@@ -2023,1 +2213,0 @@\n-\n@@ -2070,0 +2259,1 @@\n+        && !ary->is_flat()\n@@ -2105,0 +2295,2 @@\n+            \/\/ Default value load\n+            tp->is_instptr()->instance_klass() == ciEnv::current()->Class_klass() ||\n@@ -2110,1 +2302,17 @@\n-    \/\/ Optimize loads from constant fields.\n+    BasicType bt = value_basic_type();\n+\n+    \/\/ Fold loads of the field map\n+    if (UseAltSubstitutabilityMethod && tinst != nullptr) {\n+      ciInstanceKlass* ik = tinst->instance_klass();\n+      int offset = tinst->offset();\n+      if (ik == phase->C->env()->Class_klass()) {\n+        ciType* t = tinst->java_mirror_type();\n+        if (t != nullptr && t->is_inlinetype() && offset == t->as_inline_klass()->field_map_offset()) {\n+          ciConstant map = t->as_inline_klass()->get_field_map();\n+          bool is_narrow_oop = (bt == T_NARROWOOP);\n+          return Type::make_from_constant(map, true, 1, is_narrow_oop);\n+        }\n+      }\n+    }\n+\n+    \/\/ Optimize loads from constant fields.\n@@ -2114,1 +2322,1 @@\n-      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), value_basic_type());\n+      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), bt);\n@@ -2161,6 +2369,19 @@\n-      if (UseCompactObjectHeaders) {\n-        if (tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n-          \/\/ The field is Klass::_prototype_header. Return its (constant) value.\n-          assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n-          return TypeX::make(klass->prototype_header());\n-        }\n+      if (klass->is_inlinetype() && tkls->offset() == in_bytes(InstanceKlass::acmp_maps_offset_offset())) {\n+        return TypeInt::make(klass->as_inline_klass()->field_map_offset());\n+      }\n+      if (klass->is_obj_array_klass() && tkls->offset() == in_bytes(ObjArrayKlass::next_refined_array_klass_offset())) {\n+        \/\/ Fold loads from LibraryCallKit::load_default_refined_array_klass\n+        return tkls->is_aryklassptr()->cast_to_refined_array_klass_ptr();\n+      }\n+      if (klass->is_array_klass() && tkls->offset() == in_bytes(ObjArrayKlass::properties_offset())) {\n+        assert(klass->is_type_array_klass() || tkls->is_aryklassptr()->is_refined_type(), \"Must be a refined array klass pointer\");\n+        return TypeInt::make(klass->as_array_klass()->properties());\n+      }\n+      if (klass->is_flat_array_klass() && tkls->offset() == in_bytes(FlatArrayKlass::layout_kind_offset())) {\n+        assert(Opcode() == Op_LoadI, \"must load an int from _layout_kind\");\n+        return TypeInt::make(static_cast<jint>(klass->as_flat_array_klass()->layout_kind()));\n+      }\n+      if (UseCompactObjectHeaders && tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n+        \/\/ The field is Klass::_prototype_header. Return its (constant) value.\n+        assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n+        return TypeX::make(klass->prototype_header());\n@@ -2240,0 +2461,12 @@\n+      \/\/ TODO 8350865 Scalar replacement does not work well for flat arrays.\n+      \/\/ Escape Analysis assumes that arrays are always zeroed during allocation which is not true for null-free arrays\n+      \/\/ ConnectionGraph::split_unique_types will re-wire the memory of loads from such arrays around the allocation\n+      \/\/ TestArrays::test6 and test152 and TestBasicFunctionality::test20 are affected by this.\n+      if (tp->isa_aryptr() && tp->is_aryptr()->is_flat() && tp->is_aryptr()->is_null_free()) {\n+        intptr_t offset = 0;\n+        Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);\n+        AllocateNode* alloc = AllocateNode::Ideal_allocation(base);\n+        if (alloc != nullptr && alloc->is_AllocateArray() && alloc->in(AllocateNode::InitValue) != nullptr) {\n+          return _type;\n+        }\n+      }\n@@ -2243,1 +2476,0 @@\n-\n@@ -2247,1 +2479,10 @@\n-      return TypeX::make(markWord::prototype().value());\n+      if (Arguments::is_valhalla_enabled()) {\n+        \/\/ The mark word may contain property bits (inline, flat, null-free)\n+        Node* klass_node = alloc->in(AllocateNode::KlassNode);\n+        const TypeKlassPtr* tkls = phase->type(klass_node)->isa_klassptr();\n+        if (tkls != nullptr && tkls->is_loaded() && tkls->klass_is_exact()) {\n+          return TypeX::make(tkls->exact_klass()->prototype_header());\n+        }\n+      } else {\n+        return TypeX::make(markWord::prototype().value());\n+      }\n@@ -2396,0 +2637,20 @@\n+Node* LoadNNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  \/\/ Loading from an InlineType, find the input and make an EncodeP\n+  Node* addr = in(Address);\n+  intptr_t offset;\n+  Node* base = AddPNode::Ideal_base_and_offset(addr, phase, offset);\n+  Node* value = see_through_inline_type(phase, this, base, offset);\n+  if (value != nullptr) {\n+    return new EncodePNode(value, type());\n+  }\n+\n+  \/\/ Can see the corresponding value, may need to add an EncodeP\n+  value = can_see_stored_value(in(Memory), phase);\n+  if (value != nullptr && phase->type(value)->isa_ptr() && type()->isa_narrowoop()) {\n+    return new EncodePNode(value, type());\n+  }\n+\n+  \/\/ Identity call will handle the case where EncodeP is unnecessary\n+  return LoadNode::Ideal(phase, can_reshape);\n+}\n+\n@@ -2469,1 +2730,1 @@\n-  const TypeAryPtr *tary = tp->isa_aryptr();\n+  const TypeAryPtr* tary = tp->isa_aryptr();\n@@ -2472,1 +2733,1 @@\n-    return tary->as_klass_type(true);\n+    return tary->as_klass_type(true)->is_aryklassptr();\n@@ -2490,0 +2751,7 @@\n+    if (tkls->isa_aryklassptr() != nullptr && tkls->klass_is_exact() &&\n+        !tkls->exact_klass()->is_type_array_klass() &&\n+        tkls->offset() == in_bytes(Klass::super_offset())) {\n+      \/\/ We are loading the super klass of a refined array klass, return the non-refined klass pointer\n+      assert(tkls->is_aryklassptr()->is_refined_type(), \"Must be a refined array klass pointer\");\n+      return tkls->is_aryklassptr()->with_offset(0)->cast_to_non_refined();\n+    }\n@@ -2552,0 +2820,4 @@\n+  \/\/\n+  \/\/ This optimization does not apply to arrays because if k is not a\n+  \/\/ constant, it was obtained via load_klass which returns the VM type\n+  \/\/ and '.java_mirror.as_klass' should return the Java type instead.\n@@ -2561,3 +2833,2 @@\n-            && (tkls->isa_instklassptr() || tkls->isa_aryklassptr())\n-            && adr2->is_AddP()\n-           ) {\n+            && ((tkls->isa_instklassptr() && !tkls->is_instklassptr()->might_be_an_array()))\n+            && adr2->is_AddP()) {\n@@ -2708,0 +2979,1 @@\n+  case T_ARRAY:\n@@ -2723,1 +2995,1 @@\n-    ShouldNotReachHere();\n+    assert(false, \"unexpected basic type %s\", type2name(bt));\n@@ -3385,2 +3657,2 @@\n-  \/\/ unsafe if I have intervening uses.\n-  {\n+  \/\/ unsafe if I have intervening uses...\n+  if (phase->C->get_adr_type(phase->C->get_alias_index(adr_type())) != TypeAryPtr::INLINES) {\n@@ -3406,0 +3678,2 @@\n+             (Opcode() == Op_StoreL && st->Opcode() == Op_StoreN) ||\n+             (st->adr_type()->isa_aryptr() && st->adr_type()->is_aryptr()->is_flat()) || \/\/ TODO 8343835\n@@ -3543,2 +3817,1 @@\n-  if (result == this &&\n-      ReduceFieldZeroing && phase->type(val)->is_zero_type()) {\n+  if (result == this && ReduceFieldZeroing) {\n@@ -3546,1 +3819,2 @@\n-    if (mem->is_Proj() && mem->in(0)->is_Allocate()) {\n+    if (mem->is_Proj() && mem->in(0)->is_Allocate() &&\n+        (phase->type(val)->is_zero_type() || mem->in(0)->in(AllocateNode::InitValue) == val)) {\n@@ -3550,1 +3824,1 @@\n-    if (result == this) {\n+    if (result == this && phase->type(val)->is_zero_type()) {\n@@ -4054,1 +4328,1 @@\n-    return new ClearArrayNode(in(0), in(1), in(2), in(3), true);\n+    return new ClearArrayNode(in(0), in(1), in(2), in(3), in(4), true);\n@@ -4072,1 +4346,1 @@\n-  Node *zero = phase->makecon(TypeLong::ZERO);\n+  Node *val = in(4);\n@@ -4074,1 +4348,1 @@\n-  mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+  mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -4079,1 +4353,1 @@\n-    mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+    mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -4113,0 +4387,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -4123,1 +4399,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != nullptr) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == nullptr, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -4130,1 +4412,1 @@\n-  return clear_memory(ctl, mem, dest, phase->MakeConX(offset), end_offset, phase);\n+  return clear_memory(ctl, mem, dest, raw_val, phase->MakeConX(offset), end_offset, phase);\n@@ -4134,0 +4416,1 @@\n+                                   Node* raw_val,\n@@ -4156,1 +4439,4 @@\n-  mem = new ClearArrayNode(ctl, mem, zsize, adr, false);\n+  if (raw_val == nullptr) {\n+    raw_val = phase->MakeConX(0);\n+  }\n+  mem = new ClearArrayNode(ctl, mem, zsize, adr, raw_val, false);\n@@ -4161,0 +4447,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -4175,1 +4463,1 @@\n-    mem = clear_memory(ctl, mem, dest,\n+    mem = clear_memory(ctl, mem, dest, val, raw_val,\n@@ -4182,1 +4470,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != nullptr) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == nullptr, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -4329,1 +4623,1 @@\n-Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {\n+Node *MemBarNode::match(const ProjNode *proj, const Matcher *m, const RegMask* mask) {\n@@ -4616,1 +4910,3 @@\n-  if (init == nullptr || init->is_complete())  return false;\n+  if (init == nullptr || init->is_complete()) {\n+    return false;\n+  }\n@@ -4800,0 +5096,6 @@\n+                if (base->is_Phi()) {\n+                  \/\/ In rare case, base may be a PhiNode and it may read\n+                  \/\/ the same memory slice between InitializeNode and store.\n+                  failed = true;\n+                  break;\n+                }\n@@ -5386,0 +5688,2 @@\n+                                              allocation()->in(AllocateNode::InitValue),\n+                                              allocation()->in(AllocateNode::RawInitValue),\n@@ -5445,0 +5749,2 @@\n+                                            allocation()->in(AllocateNode::InitValue),\n+                                            allocation()->in(AllocateNode::RawInitValue),\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":375,"deletions":69,"binary":false,"changes":444,"status":"modified"},{"patch":"@@ -202,0 +202,13 @@\n+  \/\/ TODO 8350865 Still needed? Yes, I think this is from PhaseMacroExpand::expand_mh_intrinsic_return\n+  \/\/ Code pattern on return from a call that returns an __Value.  Can\n+  \/\/ be optimized away if the return value turns out to be an oop.\n+  if (op == Op_AndX &&\n+      in(1) != nullptr &&\n+      in(1)->Opcode() == Op_CastP2X &&\n+      in(1)->in(1) != nullptr &&\n+      phase->type(in(1)->in(1))->isa_oopptr() &&\n+      t2->isa_intptr_t()->_lo >= 0 &&\n+      t2->isa_intptr_t()->_hi <= MinObjAlignmentInBytesMask) {\n+    return add_id();\n+  }\n+\n@@ -864,0 +877,41 @@\n+  \/\/ Search for GraphKit::mark_word_test patterns and fold the test if the result is statically known\n+  Node* load1 = in(1);\n+  Node* load2 = nullptr;\n+  if (load1->is_Phi() && phase->type(load1)->isa_long()) {\n+    load1 = in(1)->in(1);\n+    load2 = in(1)->in(2);\n+  }\n+  if (load1 != nullptr && load1->is_Load() && phase->type(load1)->isa_long() &&\n+      (load2 == nullptr || (load2->is_Load() && phase->type(load2)->isa_long()))) {\n+    const TypePtr* adr_t1 = phase->type(load1->in(MemNode::Address))->isa_ptr();\n+    const TypePtr* adr_t2 = (load2 != nullptr) ? phase->type(load2->in(MemNode::Address))->isa_ptr() : nullptr;\n+    if (adr_t1 != nullptr && adr_t1->offset() == oopDesc::mark_offset_in_bytes() &&\n+        (load2 == nullptr || (adr_t2 != nullptr && adr_t2->offset() == in_bytes(Klass::prototype_header_offset())))) {\n+      if (mask == markWord::inline_type_pattern) {\n+        if (adr_t1->is_inlinetypeptr()) {\n+          set_req_X(1, in(2), phase);\n+          return this;\n+        } else if (!adr_t1->can_be_inline_type()) {\n+          set_req_X(1, phase->longcon(0), phase);\n+          return this;\n+        }\n+      } else if (mask == markWord::null_free_array_bit_in_place) {\n+        if (adr_t1->is_null_free()) {\n+          set_req_X(1, in(2), phase);\n+          return this;\n+        } else if (adr_t1->is_not_null_free()) {\n+          set_req_X(1, phase->longcon(0), phase);\n+          return this;\n+        }\n+      } else if (mask == markWord::flat_array_bit_in_place) {\n+        if (adr_t1->is_flat()) {\n+          set_req_X(1, in(2), phase);\n+          return this;\n+        } else if (adr_t1->is_not_flat()) {\n+          set_req_X(1, phase->longcon(0), phase);\n+          return this;\n+        }\n+      }\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/mulnode.cpp","additions":54,"deletions":0,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-Node *MultiNode::match( const ProjNode *proj, const Matcher *m ) { return proj->clone(); }\n+Node *MultiNode::match(const ProjNode *proj, const Matcher *m, const RegMask* mask) { return proj->clone(); }\n","filename":"src\/hotspot\/share\/opto\/multnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-  virtual Node *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node *match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n","filename":"src\/hotspot\/share\/opto\/multnode.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -574,0 +575,6 @@\n+  if (n->is_InlineType()) {\n+    C->add_inline_type(n);\n+  }\n+  if (n->is_LoadFlat() || n->is_StoreFlat()) {\n+    C->add_flat_access(n);\n+  }\n@@ -634,0 +641,3 @@\n+  if (is_InlineType()) {\n+    compile->remove_inline_type(this);\n+  }\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -26,0 +27,1 @@\n+#include \"ci\/ciSymbols.hpp\"\n@@ -38,0 +40,2 @@\n+#include \"opto\/idealKit.hpp\"\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -45,0 +49,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -53,0 +58,17 @@\n+Node* Parse::record_profile_for_speculation_at_array_load(Node* ld) {\n+  \/\/ Feed unused profile data to type speculation\n+  if (UseTypeSpeculation && UseArrayLoadStoreProfile) {\n+    ciKlass* array_type = nullptr;\n+    ciKlass* element_type = nullptr;\n+    ProfilePtrKind element_ptr = ProfileMaybeNull;\n+    bool flat_array = true;\n+    bool null_free_array = true;\n+    method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+    if (element_type != nullptr || element_ptr != ProfileMaybeNull) {\n+      ld = record_profile_for_speculation(ld, element_type, element_ptr);\n+    }\n+  }\n+  return ld;\n+}\n+\n+\n@@ -56,1 +78,0 @@\n-  bool big_val = bt == T_DOUBLE || bt == T_LONG;\n@@ -60,2 +81,62 @@\n-  pop();                      \/\/ index (already used)\n-  Node* array = pop();        \/\/ the array itself\n+  Node* array_index = pop();\n+  Node* array = pop();\n+\n+  \/\/ Handle inline type arrays\n+  const TypeOopPtr* element_ptr = elemtype->make_oopptr();\n+  const TypeAryPtr* array_type = _gvn.type(array)->is_aryptr();\n+\n+  if (!array_type->is_not_flat()) {\n+    \/\/ Cannot statically determine if array is a flat array, emit runtime check\n+    assert(UseArrayFlattening && is_reference_type(bt) && element_ptr->can_be_inline_type() &&\n+           (!element_ptr->is_inlinetypeptr() || element_ptr->inline_klass()->maybe_flat_in_array()), \"array can't be flat\");\n+    IdealKit ideal(this);\n+    IdealVariable res(ideal);\n+    ideal.declarations_done();\n+    ideal.if_then(flat_array_test(array, \/* flat = *\/ false)); {\n+      \/\/ Non-flat array\n+      sync_kit(ideal);\n+      if (!array_type->is_flat()) {\n+        assert(array_type->is_flat() || control()->in(0)->as_If()->is_flat_array_check(&_gvn), \"Should be found\");\n+        const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);\n+        DecoratorSet decorator_set = IN_HEAP | IS_ARRAY | C2_CONTROL_DEPENDENT_LOAD;\n+        if (needs_range_check(array_type->size(), array_index)) {\n+          \/\/ We've emitted a RangeCheck but now insert an additional check between the range check and the actual load.\n+          \/\/ We cannot pin the load to two separate nodes. Instead, we pin it conservatively here such that it cannot\n+          \/\/ possibly float above the range check at any point.\n+          decorator_set |= C2_UNKNOWN_CONTROL_LOAD;\n+        }\n+        Node* ld = access_load_at(array, adr, adr_type, element_ptr, bt, decorator_set);\n+        if (element_ptr->is_inlinetypeptr()) {\n+          ld = InlineTypeNode::make_from_oop(this, ld, element_ptr->inline_klass());\n+        }\n+        ideal.set(res, ld);\n+      }\n+      ideal.sync_kit(this);\n+    } ideal.else_(); {\n+      \/\/ Flat array\n+      sync_kit(ideal);\n+      if (!array_type->is_not_flat()) {\n+        if (element_ptr->is_inlinetypeptr()) {\n+          ciInlineKlass* vk = element_ptr->inline_klass();\n+          Node* flat_array = cast_to_flat_array(array, vk);\n+          Node* vt = InlineTypeNode::make_from_flat_array(this, vk, flat_array, array_index);\n+          ideal.set(res, vt);\n+        } else {\n+          \/\/ Element type is unknown, and thus we cannot statically determine the exact flat array layout. Emit a\n+          \/\/ runtime call to correctly load the inline type element from the flat array.\n+          Node* inline_type = load_from_unknown_flat_array(array, array_index, element_ptr);\n+          bool is_null_free = array_type->is_null_free() || !UseNullableValueFlattening;\n+          if (is_null_free) {\n+            inline_type = cast_not_null(inline_type);\n+          }\n+          ideal.set(res, inline_type);\n+        }\n+      }\n+      ideal.sync_kit(this);\n+    } ideal.end_if();\n+    sync_kit(ideal);\n+    Node* ld = _gvn.transform(ideal.value(res));\n+    ld = record_profile_for_speculation_at_array_load(ld);\n+    push_node(bt, ld);\n+    return;\n+  }\n@@ -67,1 +148,0 @@\n-\n@@ -70,4 +150,5 @@\n-  if (big_val) {\n-    push_pair(ld);\n-  } else {\n-    push(ld);\n+  ld = record_profile_for_speculation_at_array_load(ld);\n+  \/\/ Loading an inline type from a non-flat array\n+  if (element_ptr != nullptr && element_ptr->is_inlinetypeptr()) {\n+    assert(!array_type->is_null_free() || !element_ptr->maybe_null(), \"inline type array elements should never be null\");\n+    ld = InlineTypeNode::make_from_oop(this, ld, element_ptr->inline_klass());\n@@ -75,0 +156,1 @@\n+  push_node(bt, ld);\n@@ -77,0 +159,28 @@\n+Node* Parse::load_from_unknown_flat_array(Node* array, Node* array_index, const TypeOopPtr* element_ptr) {\n+  \/\/ Below membars keep this access to an unknown flat array correctly\n+  \/\/ ordered with other unknown and known flat array accesses.\n+  insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+\n+  Node* call = nullptr;\n+  {\n+    \/\/ Re-execute flat array load if runtime call triggers deoptimization\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_bci(_bci);\n+    jvms()->set_should_reexecute(true);\n+    inc_sp(2);\n+    kill_dead_locals();\n+    call = make_runtime_call(RC_NO_LEAF | RC_NO_IO,\n+                             OptoRuntime::load_unknown_inline_Type(),\n+                             OptoRuntime::load_unknown_inline_Java(),\n+                             nullptr, TypeRawPtr::BOTTOM,\n+                             array, array_index);\n+  }\n+  make_slow_call_ex(call, env()->Throwable_klass(), false);\n+  Node* buffer = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+\n+  insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+\n+  \/\/ Keep track of the information that the inline type is in flat arrays\n+  const Type* unknown_value = element_ptr->is_instptr()->cast_to_flat_in_array();\n+  return _gvn.transform(new CheckCastPPNode(control(), buffer, unknown_value));\n+}\n@@ -81,2 +191,1 @@\n-  bool big_val = bt == T_DOUBLE || bt == T_LONG;\n-  Node* adr = array_addressing(bt, big_val ? 2 : 1, elemtype);\n+  Node* adr = array_addressing(bt, type2size[bt], elemtype);\n@@ -84,0 +193,1 @@\n+  Node* stored_value_casted = nullptr;\n@@ -85,1 +195,1 @@\n-    array_store_check();\n+    stored_value_casted = array_store_check(adr, elemtype);\n@@ -90,8 +200,6 @@\n-  Node* val;                  \/\/ Oop to store\n-  if (big_val) {\n-    val = pop_pair();\n-  } else {\n-    val = pop();\n-  }\n-  pop();                      \/\/ index (already used)\n-  Node* array = pop();        \/\/ the array itself\n+  Node* const stored_value = pop_node(bt); \/\/ Value to store\n+  Node* const array_index = pop();         \/\/ Index in the array\n+  Node* array = pop();                     \/\/ The array itself\n+\n+  const TypeAryPtr* array_type = _gvn.type(array)->is_aryptr();\n+  const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);\n@@ -101,2 +209,68 @@\n-  }\n-  const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);\n+  } else if (bt == T_OBJECT) {\n+    elemtype = elemtype->make_oopptr();\n+    const Type* stored_value_casted_type = _gvn.type(stored_value_casted);\n+    \/\/ Based on the value to be stored, try to determine if the array is not null-free and\/or not flat.\n+    \/\/ This is only legal for non-null stores because the array_store_check always passes for null, even\n+    \/\/ if the array is null-free. Null stores are handled in GraphKit::inline_array_null_guard().\n+    bool not_inline = !stored_value_casted_type->maybe_null() && !stored_value_casted_type->is_oopptr()->can_be_inline_type();\n+    bool not_null_free = not_inline;\n+    bool not_flat = not_inline || ( stored_value_casted_type->is_inlinetypeptr() &&\n+                                   !stored_value_casted_type->inline_klass()->maybe_flat_in_array());\n+    if (!array_type->is_not_null_free() && not_null_free) {\n+      \/\/ Storing a non-inline type, mark array as not null-free.\n+      array_type = array_type->cast_to_not_null_free();\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, array_type));\n+      replace_in_map(array, cast);\n+      array = cast;\n+    }\n+    if (!array_type->is_not_flat() && not_flat) {\n+      \/\/ Storing to a non-flat array, mark array as not flat.\n+      array_type = array_type->cast_to_not_flat();\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, array_type));\n+      replace_in_map(array, cast);\n+      array = cast;\n+    }\n+\n+    if (array_type->is_null_free() && elemtype->is_inlinetypeptr() && elemtype->inline_klass()->is_empty()) {\n+      \/\/ Array of null-free empty inline type, there is only 1 state for the elements\n+      assert(!stored_value_casted_type->maybe_null(), \"should be guaranteed by array store check\");\n+      return;\n+    }\n+\n+    if (!array_type->is_not_flat()) {\n+      \/\/ Array might be a flat array, emit runtime checks (for nullptr, a simple inline_array_null_guard is sufficient).\n+      assert(UseArrayFlattening && !not_flat && elemtype->is_oopptr()->can_be_inline_type() &&\n+             (!array_type->klass_is_exact() || array_type->is_flat()), \"array can't be a flat array\");\n+      \/\/ TODO 8350865 Depending on the available layouts, we can avoid this check in below flat\/not-flat branches. Also the safe_for_replace arg is now always true.\n+      array = inline_array_null_guard(array, stored_value_casted, 3, true);\n+      IdealKit ideal(this);\n+      ideal.if_then(flat_array_test(array, \/* flat = *\/ false)); {\n+        \/\/ Non-flat array\n+        if (!array_type->is_flat()) {\n+          sync_kit(ideal);\n+          assert(array_type->is_flat() || ideal.ctrl()->in(0)->as_If()->is_flat_array_check(&_gvn), \"Should be found\");\n+          inc_sp(3);\n+          access_store_at(array, adr, adr_type, stored_value_casted, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY, false);\n+          dec_sp(3);\n+          ideal.sync_kit(this);\n+        }\n+      } ideal.else_(); {\n+        \/\/ Flat array\n+        sync_kit(ideal);\n+        if (!array_type->is_not_flat()) {\n+          \/\/ Try to determine the inline klass type of the stored value\n+          ciInlineKlass* vk = nullptr;\n+          if (stored_value_casted_type->is_inlinetypeptr()) {\n+            vk = stored_value_casted_type->inline_klass();\n+          } else if (elemtype->is_inlinetypeptr()) {\n+            vk = elemtype->inline_klass();\n+          }\n+\n+          if (vk != nullptr) {\n+            \/\/ Element type is known, cast and store to flat array layout.\n+            Node* flat_array = cast_to_flat_array(array, vk);\n+\n+            \/\/ Re-execute flat array store if buffering triggers deoptimization\n+            PreserveReexecuteState preexecs(this);\n+            jvms()->set_should_reexecute(true);\n+            inc_sp(3);\n@@ -104,1 +278,25 @@\n-  access_store_at(array, adr, adr_type, val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY);\n+            if (!stored_value_casted->is_InlineType()) {\n+              assert(_gvn.type(stored_value_casted) == TypePtr::NULL_PTR, \"Unexpected value\");\n+              stored_value_casted = InlineTypeNode::make_null(_gvn, vk);\n+            }\n+\n+            stored_value_casted->as_InlineType()->store_flat_array(this, flat_array, array_index);\n+          } else {\n+            \/\/ Element type is unknown, emit a runtime call since the flat array layout is not statically known.\n+            store_to_unknown_flat_array(array, array_index, stored_value_casted);\n+          }\n+        }\n+        ideal.sync_kit(this);\n+      }\n+      ideal.end_if();\n+      sync_kit(ideal);\n+      return;\n+    } else if (!array_type->is_not_null_free()) {\n+      \/\/ Array is not flat but may be null free\n+      assert(elemtype->is_oopptr()->can_be_inline_type(), \"array can't be null-free\");\n+      array = inline_array_null_guard(array, stored_value_casted, 3, true);\n+    }\n+  }\n+  inc_sp(3);\n+  access_store_at(array, adr, adr_type, stored_value, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY);\n+  dec_sp(3);\n@@ -107,0 +305,25 @@\n+\/\/ Emit a runtime call to store to a flat array whose element type is either unknown (i.e. we do not know the flat\n+\/\/ array layout) or not exact (could have different flat array layouts at runtime).\n+void Parse::store_to_unknown_flat_array(Node* array, Node* const idx, Node* non_null_stored_value) {\n+  \/\/ Below membars keep this access to an unknown flat array correctly\n+  \/\/ ordered with other unknown and known flat array accesses.\n+  insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+\n+  Node* call = nullptr;\n+  {\n+    \/\/ Re-execute flat array store if runtime call triggers deoptimization\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_bci(_bci);\n+    jvms()->set_should_reexecute(true);\n+    inc_sp(3);\n+    kill_dead_locals();\n+    call = make_runtime_call(RC_NO_LEAF | RC_NO_IO,\n+                      OptoRuntime::store_unknown_inline_Type(),\n+                      OptoRuntime::store_unknown_inline_Java(),\n+                      nullptr, TypeRawPtr::BOTTOM,\n+                      non_null_stored_value, array, idx);\n+  }\n+  make_slow_call_ex(call, env()->Throwable_klass(), false);\n+\n+  insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+}\n@@ -135,11 +358,0 @@\n-  \/\/ Check for big class initializers with all constant offsets\n-  \/\/ feeding into a known-size array.\n-  const TypeInt* idxtype = _gvn.type(idx)->is_int();\n-  \/\/ See if the highest idx value is less than the lowest array bound,\n-  \/\/ and if the idx value cannot be negative:\n-  bool need_range_check = true;\n-  if (idxtype->_hi < sizetype->_lo && idxtype->_lo >= 0) {\n-    need_range_check = false;\n-    if (C->log() != nullptr)   C->log()->elem(\"observe that='!need_range_check'\");\n-  }\n-\n@@ -157,12 +369,1 @@\n-  \/\/ Do the range check\n-  if (need_range_check) {\n-    Node* tst;\n-    if (sizetype->_hi <= 0) {\n-      \/\/ The greatest array bound is negative, so we can conclude that we're\n-      \/\/ compiling unreachable code, but the unsigned compare trick used below\n-      \/\/ only works with non-negative lengths.  Instead, hack \"tst\" to be zero so\n-      \/\/ the uncommon_trap path will always be taken.\n-      tst = _gvn.intcon(0);\n-    } else {\n-      \/\/ Range is constant in array-oop, so we can use the original state of mem\n-      Node* len = load_array_length(ary);\n+  ary = create_speculative_inline_type_array_checks(ary, arytype, elemtype);\n@@ -170,31 +371,4 @@\n-      \/\/ Test length vs index (standard trick using unsigned compare)\n-      Node* chk = _gvn.transform( new CmpUNode(idx, len) );\n-      BoolTest::mask btest = BoolTest::lt;\n-      tst = _gvn.transform( new BoolNode(chk, btest) );\n-    }\n-    RangeCheckNode* rc = new RangeCheckNode(control(), tst, PROB_MAX, COUNT_UNKNOWN);\n-    _gvn.set_type(rc, rc->Value(&_gvn));\n-    if (!tst->is_Con()) {\n-      record_for_igvn(rc);\n-    }\n-    set_control(_gvn.transform(new IfTrueNode(rc)));\n-    \/\/ Branch to failure if out of bounds\n-    {\n-      PreserveJVMState pjvms(this);\n-      set_control(_gvn.transform(new IfFalseNode(rc)));\n-      if (C->allow_range_check_smearing()) {\n-        \/\/ Do not use builtin_throw, since range checks are sometimes\n-        \/\/ made more stringent by an optimistic transformation.\n-        \/\/ This creates \"tentative\" range checks at this point,\n-        \/\/ which are not guaranteed to throw exceptions.\n-        \/\/ See IfNode::Ideal, is_range_check, adjust_check.\n-        uncommon_trap(Deoptimization::Reason_range_check,\n-                      Deoptimization::Action_make_not_entrant,\n-                      nullptr, \"range_check\");\n-      } else {\n-        \/\/ If we have already recompiled with the range-check-widening\n-        \/\/ heroic optimization turned off, then we must really be throwing\n-        \/\/ range check exceptions.\n-        builtin_throw(Deoptimization::Reason_range_check);\n-      }\n-    }\n+  if (needs_range_check(sizetype, idx)) {\n+    create_range_check(idx, ary, sizetype);\n+  } else if (C->log() != nullptr) {\n+    C->log()->elem(\"observe that='!need_range_check'\");\n@@ -202,0 +376,1 @@\n+\n@@ -213,0 +388,199 @@\n+\/\/ Check if we need a range check for an array access. This is the case if the index is either negative or if it could\n+\/\/ be greater or equal the smallest possible array size (i.e. out-of-bounds).\n+bool Parse::needs_range_check(const TypeInt* size_type, const Node* index) const {\n+  const TypeInt* index_type = _gvn.type(index)->is_int();\n+  return index_type->_hi >= size_type->_lo || index_type->_lo < 0;\n+}\n+\n+void Parse::create_range_check(Node* idx, Node* ary, const TypeInt* sizetype) {\n+  Node* tst;\n+  if (sizetype->_hi <= 0) {\n+    \/\/ The greatest array bound is negative, so we can conclude that we're\n+    \/\/ compiling unreachable code, but the unsigned compare trick used below\n+    \/\/ only works with non-negative lengths.  Instead, hack \"tst\" to be zero so\n+    \/\/ the uncommon_trap path will always be taken.\n+    tst = _gvn.intcon(0);\n+  } else {\n+    \/\/ Range is constant in array-oop, so we can use the original state of mem\n+    Node* len = load_array_length(ary);\n+\n+    \/\/ Test length vs index (standard trick using unsigned compare)\n+    Node* chk = _gvn.transform(new CmpUNode(idx, len) );\n+    BoolTest::mask btest = BoolTest::lt;\n+    tst = _gvn.transform(new BoolNode(chk, btest) );\n+  }\n+  RangeCheckNode* rc = new RangeCheckNode(control(), tst, PROB_MAX, COUNT_UNKNOWN);\n+  _gvn.set_type(rc, rc->Value(&_gvn));\n+  if (!tst->is_Con()) {\n+    record_for_igvn(rc);\n+  }\n+  set_control(_gvn.transform(new IfTrueNode(rc)));\n+  \/\/ Branch to failure if out of bounds\n+  {\n+    PreserveJVMState pjvms(this);\n+    set_control(_gvn.transform(new IfFalseNode(rc)));\n+    if (C->allow_range_check_smearing()) {\n+      \/\/ Do not use builtin_throw, since range checks are sometimes\n+      \/\/ made more stringent by an optimistic transformation.\n+      \/\/ This creates \"tentative\" range checks at this point,\n+      \/\/ which are not guaranteed to throw exceptions.\n+      \/\/ See IfNode::Ideal, is_range_check, adjust_check.\n+      uncommon_trap(Deoptimization::Reason_range_check,\n+                    Deoptimization::Action_make_not_entrant,\n+                    nullptr, \"range_check\");\n+    } else {\n+      \/\/ If we have already recompiled with the range-check-widening\n+      \/\/ heroic optimization turned off, then we must really be throwing\n+      \/\/ range check exceptions.\n+      builtin_throw(Deoptimization::Reason_range_check);\n+    }\n+  }\n+}\n+\n+\/\/ For inline type arrays, we can use the profiling information for array accesses to speculate on the type, flatness,\n+\/\/ and null-freeness. We can either prepare the speculative type for later uses or emit explicit speculative checks with\n+\/\/ traps now. In the latter case, the speculative type guarantees can avoid additional runtime checks later (e.g.\n+\/\/ non-null-free implies non-flat which allows us to remove flatness checks). This makes the graph simpler.\n+Node* Parse::create_speculative_inline_type_array_checks(Node* array, const TypeAryPtr* array_type,\n+                                                         const Type*& element_type) {\n+  if (!array_type->is_flat() && !array_type->is_not_flat()) {\n+    \/\/ For arrays that might be flat, speculate that the array has the exact type reported in the profile data such that\n+    \/\/ we can rely on a fixed memory layout (i.e. either a flat layout or not).\n+    array = cast_to_speculative_array_type(array, array_type, element_type);\n+  } else if (UseTypeSpeculation && UseArrayLoadStoreProfile) {\n+    \/\/ Array is known to be either flat or not flat. If possible, update the speculative type by using the profile data\n+    \/\/ at this bci.\n+    array = cast_to_profiled_array_type(array);\n+  }\n+\n+  \/\/ Even though the type does not tell us whether we have an inline type array or not, we can still check the profile data\n+  \/\/ whether we have a non-null-free or non-flat array. Speculating on a non-null-free array doesn't help aaload but could\n+  \/\/ be profitable for a subsequent aastore.\n+  if (!array_type->is_null_free() && !array_type->is_not_null_free()) {\n+    array = speculate_non_null_free_array(array, array_type);\n+  }\n+  if (!array_type->is_flat() && !array_type->is_not_flat()) {\n+    array = speculate_non_flat_array(array, array_type);\n+  }\n+  return array;\n+}\n+\n+\/\/ Speculate that the array has the exact type reported in the profile data. We emit a trap when this turns out to be\n+\/\/ wrong. On the fast path, we add a CheckCastPP to use the exact type.\n+Node* Parse::cast_to_speculative_array_type(Node* const array, const TypeAryPtr*& array_type, const Type*& element_type) {\n+  Deoptimization::DeoptReason reason = Deoptimization::Reason_speculate_class_check;\n+  ciKlass* speculative_array_type = array_type->speculative_type();\n+  if (too_many_traps_or_recompiles(reason) || speculative_array_type == nullptr) {\n+    \/\/ No speculative type, check profile data at this bci\n+    speculative_array_type = nullptr;\n+    reason = Deoptimization::Reason_class_check;\n+    if (UseArrayLoadStoreProfile && !too_many_traps_or_recompiles(reason)) {\n+      ciKlass* profiled_element_type = nullptr;\n+      ProfilePtrKind element_ptr = ProfileMaybeNull;\n+      bool flat_array = true;\n+      bool null_free_array = true;\n+      method()->array_access_profiled_type(bci(), speculative_array_type, profiled_element_type, element_ptr, flat_array,\n+                                           null_free_array);\n+    }\n+  }\n+  if (speculative_array_type != nullptr) {\n+    \/\/ Speculate that this array has the exact type reported by profile data\n+    Node* casted_array = nullptr;\n+    DEBUG_ONLY(Node* old_control = control();)\n+    Node* slow_ctl = type_check_receiver(array, speculative_array_type, 1.0, &casted_array);\n+    if (stopped()) {\n+      \/\/ The check always fails and therefore profile information is incorrect. Don't use it.\n+      assert(old_control == slow_ctl, \"type check should have been removed\");\n+      set_control(slow_ctl);\n+    } else if (!slow_ctl->is_top()) {\n+      { PreserveJVMState pjvms(this);\n+        set_control(slow_ctl);\n+        uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+      }\n+      replace_in_map(array, casted_array);\n+      array_type = _gvn.type(casted_array)->is_aryptr();\n+      element_type = array_type->elem();\n+      return casted_array;\n+    }\n+  }\n+  return array;\n+}\n+\n+\/\/ Create a CheckCastPP when the speculative type can improve the current type.\n+Node* Parse::cast_to_profiled_array_type(Node* const array) {\n+  ciKlass* array_type = nullptr;\n+  ciKlass* element_type = nullptr;\n+  ProfilePtrKind element_ptr = ProfileMaybeNull;\n+  bool flat_array = true;\n+  bool null_free_array = true;\n+  method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+  if (array_type != nullptr) {\n+    return record_profile_for_speculation(array, array_type, ProfileMaybeNull);\n+  }\n+  return array;\n+}\n+\n+\/\/ Speculate that the array is non-null-free. We emit a trap when this turns out to be\n+\/\/ wrong. On the fast path, we add a CheckCastPP to use the non-null-free type.\n+Node* Parse::speculate_non_null_free_array(Node* const array, const TypeAryPtr*& array_type) {\n+  bool null_free_array = true;\n+  Deoptimization::DeoptReason reason = Deoptimization::Reason_none;\n+  if (array_type->speculative() != nullptr &&\n+      array_type->speculative()->is_aryptr()->is_not_null_free() &&\n+      !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+    null_free_array = false;\n+    reason = Deoptimization::Reason_speculate_class_check;\n+  } else if (UseArrayLoadStoreProfile && !too_many_traps_or_recompiles(Deoptimization::Reason_class_check)) {\n+    ciKlass* profiled_array_type = nullptr;\n+    ciKlass* profiled_element_type = nullptr;\n+    ProfilePtrKind element_ptr = ProfileMaybeNull;\n+    bool flat_array = true;\n+    method()->array_access_profiled_type(bci(), profiled_array_type, profiled_element_type, element_ptr, flat_array,\n+                                         null_free_array);\n+    reason = Deoptimization::Reason_class_check;\n+  }\n+  if (!null_free_array) {\n+    { \/\/ Deoptimize if null-free array\n+      BuildCutout unless(this, null_free_array_test(array, \/* null_free = *\/ false), PROB_MAX);\n+      uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+    }\n+    assert(!stopped(), \"null-free array should have been caught earlier\");\n+    Node* casted_array = _gvn.transform(new CheckCastPPNode(control(), array, array_type->cast_to_not_null_free()));\n+    replace_in_map(array, casted_array);\n+    array_type = _gvn.type(casted_array)->is_aryptr();\n+    return casted_array;\n+  }\n+  return array;\n+}\n+\n+\/\/ Speculate that the array is non-flat. We emit a trap when this turns out to be wrong.\n+\/\/ On the fast path, we add a CheckCastPP to use the non-flat type.\n+Node* Parse::speculate_non_flat_array(Node* const array, const TypeAryPtr* const array_type) {\n+  bool flat_array = true;\n+  Deoptimization::DeoptReason reason = Deoptimization::Reason_none;\n+  if (array_type->speculative() != nullptr &&\n+      array_type->speculative()->is_aryptr()->is_not_flat() &&\n+      !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+    flat_array = false;\n+    reason = Deoptimization::Reason_speculate_class_check;\n+  } else if (UseArrayLoadStoreProfile && !too_many_traps_or_recompiles(reason)) {\n+    ciKlass* profiled_array_type = nullptr;\n+    ciKlass* profiled_element_type = nullptr;\n+    ProfilePtrKind element_ptr = ProfileMaybeNull;\n+    bool null_free_array = true;\n+    method()->array_access_profiled_type(bci(), profiled_array_type, profiled_element_type, element_ptr, flat_array,\n+                                         null_free_array);\n+    reason = Deoptimization::Reason_class_check;\n+  }\n+  if (!flat_array) {\n+    { \/\/ Deoptimize if flat array\n+      BuildCutout unless(this, flat_array_test(array, \/* flat = *\/ false), PROB_MAX);\n+      uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+    }\n+    assert(!stopped(), \"flat array should have been caught earlier\");\n+    Node* casted_array = _gvn.transform(new CheckCastPPNode(control(), array, array_type->cast_to_not_flat()));\n+    replace_in_map(array, casted_array);\n+    return casted_array;\n+  }\n+  return array;\n+}\n@@ -1449,1 +1823,1 @@\n-void Parse::do_if(BoolTest::mask btest, Node* c) {\n+void Parse::do_if(BoolTest::mask btest, Node* c, bool can_trap, bool new_path, Node** ctrl_taken, Node** stress_count_mem) {\n@@ -1480,0 +1854,3 @@\n+    if (stress_count_mem != nullptr) {\n+      *stress_count_mem = incr_store;\n+    }\n@@ -1540,2 +1917,2 @@\n-      if (C->eliminate_boxing()) {\n-        \/\/ Mark the successor block as parsed\n+      if (C->eliminate_boxing() && !new_path) {\n+        \/\/ Mark the successor block as parsed (if we haven't created a new path)\n@@ -1545,1 +1922,1 @@\n-      adjust_map_after_if(taken_btest, c, prob, branch_block);\n+      adjust_map_after_if(taken_btest, c, prob, branch_block, can_trap);\n@@ -1547,1 +1924,9 @@\n-        merge(target_bci);\n+        if (new_path) {\n+          \/\/ Merge by using a new path\n+          merge_new_path(target_bci);\n+        } else if (ctrl_taken != nullptr) {\n+          \/\/ Don't merge but save taken branch to be wired by caller\n+          *ctrl_taken = control();\n+        } else {\n+          merge(target_bci);\n+        }\n@@ -1556,1 +1941,1 @@\n-  if (stopped()) {\n+  if (stopped() && ctrl_taken == nullptr) {\n@@ -1558,1 +1943,1 @@\n-      \/\/ Mark the successor block as parsed\n+      \/\/ Mark the successor block as parsed (if caller does not re-wire control flow)\n@@ -1562,1 +1947,1 @@\n-    adjust_map_after_if(untaken_btest, c, untaken_prob, next_block);\n+    adjust_map_after_if(untaken_btest, c, untaken_prob, next_block, can_trap);\n@@ -1570,0 +1955,404 @@\n+\n+static ProfilePtrKind speculative_ptr_kind(const TypeOopPtr* t) {\n+  if (t->speculative() == nullptr) {\n+    return ProfileUnknownNull;\n+  }\n+  if (t->speculative_always_null()) {\n+    return ProfileAlwaysNull;\n+  }\n+  if (t->speculative_maybe_null()) {\n+    return ProfileMaybeNull;\n+  }\n+  return ProfileNeverNull;\n+}\n+\n+void Parse::acmp_always_null_input(Node* input, const TypeOopPtr* tinput, BoolTest::mask btest, Node* eq_region) {\n+  inc_sp(2);\n+  Node* cast = null_check_common(input, T_OBJECT, true, nullptr,\n+                                 !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check) &&\n+                                 speculative_ptr_kind(tinput) == ProfileAlwaysNull);\n+  dec_sp(2);\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      replace_in_map(input, cast);\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+  } else {\n+    replace_in_map(input, cast);\n+  }\n+}\n+\n+Node* Parse::acmp_null_check(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, Node*& null_ctl) {\n+  inc_sp(2);\n+  null_ctl = top();\n+  Node* cast = null_check_oop(input, &null_ctl,\n+                              input_ptr == ProfileNeverNull || (input_ptr == ProfileUnknownNull && !too_many_traps_or_recompiles(Deoptimization::Reason_null_check)),\n+                              false,\n+                              speculative_ptr_kind(tinput) == ProfileNeverNull &&\n+                              !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check));\n+  dec_sp(2);\n+  return cast;\n+}\n+\n+void Parse::acmp_known_non_inline_type_input(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, ciKlass* input_type, BoolTest::mask btest, Node* eq_region) {\n+  Node* ne_region = new RegionNode(1);\n+  Node* null_ctl;\n+  Node* cast = acmp_null_check(input, tinput, input_ptr, null_ctl);\n+  ne_region->add_req(null_ctl);\n+\n+  Node* slow_ctl = type_check_receiver(cast, input_type, 1.0, &cast);\n+  {\n+    PreserveJVMState pjvms(this);\n+    inc_sp(2);\n+    set_control(slow_ctl);\n+    Deoptimization::DeoptReason reason;\n+    if (tinput->speculative_type() != nullptr && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+      reason = Deoptimization::Reason_speculate_class_check;\n+    } else {\n+      reason = Deoptimization::Reason_class_check;\n+    }\n+    uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+  }\n+  ne_region->add_req(control());\n+\n+  record_for_igvn(ne_region);\n+  set_control(_gvn.transform(ne_region));\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      if (null_ctl == top()) {\n+        replace_in_map(input, cast);\n+      }\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+  } else {\n+    if (null_ctl == top()) {\n+      replace_in_map(input, cast);\n+    }\n+    set_control(_gvn.transform(ne_region));\n+  }\n+}\n+\n+void Parse::acmp_unknown_non_inline_type_input(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, BoolTest::mask btest, Node* eq_region) {\n+  Node* ne_region = new RegionNode(1);\n+  Node* null_ctl;\n+  Node* cast = acmp_null_check(input, tinput, input_ptr, null_ctl);\n+  ne_region->add_req(null_ctl);\n+\n+  {\n+    BuildCutout unless(this, inline_type_test(cast, \/* is_inline = *\/ false), PROB_MAX);\n+    inc_sp(2);\n+    uncommon_trap_exact(Deoptimization::Reason_class_check, Deoptimization::Action_maybe_recompile);\n+  }\n+\n+  ne_region->add_req(control());\n+\n+  record_for_igvn(ne_region);\n+  set_control(_gvn.transform(ne_region));\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      if (null_ctl == top()) {\n+        replace_in_map(input, cast);\n+      }\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+  } else {\n+    if (null_ctl == top()) {\n+      replace_in_map(input, cast);\n+    }\n+    set_control(_gvn.transform(ne_region));\n+  }\n+}\n+\n+void Parse::do_acmp(BoolTest::mask btest, Node* left, Node* right) {\n+  ciKlass* left_type = nullptr;\n+  ciKlass* right_type = nullptr;\n+  ProfilePtrKind left_ptr = ProfileUnknownNull;\n+  ProfilePtrKind right_ptr = ProfileUnknownNull;\n+  bool left_inline_type = true;\n+  bool right_inline_type = true;\n+\n+  \/\/ Leverage profiling at acmp\n+  if (UseACmpProfile) {\n+    method()->acmp_profiled_type(bci(), left_type, right_type, left_ptr, right_ptr, left_inline_type, right_inline_type);\n+    if (too_many_traps_or_recompiles(Deoptimization::Reason_class_check)) {\n+      left_type = nullptr;\n+      right_type = nullptr;\n+      left_inline_type = true;\n+      right_inline_type = true;\n+    }\n+    if (too_many_traps_or_recompiles(Deoptimization::Reason_null_check)) {\n+      left_ptr = ProfileUnknownNull;\n+      right_ptr = ProfileUnknownNull;\n+    }\n+  }\n+\n+  if (UseTypeSpeculation) {\n+    record_profile_for_speculation(left, left_type, left_ptr);\n+    record_profile_for_speculation(right, right_type, right_ptr);\n+  }\n+\n+  if (!Arguments::is_valhalla_enabled()) {\n+    Node* cmp = CmpP(left, right);\n+    cmp = optimize_cmp_with_klass(cmp);\n+    do_if(btest, cmp);\n+    return;\n+  }\n+\n+  \/\/ Check for equality before potentially allocating\n+  if (left == right) {\n+    do_if(btest, makecon(TypeInt::CC_EQ));\n+    return;\n+  }\n+\n+  \/\/ Allocate inline type operands and re-execute on deoptimization\n+  if (left->is_InlineType()) {\n+    PreserveReexecuteState preexecs(this);\n+    inc_sp(2);\n+    jvms()->set_should_reexecute(true);\n+    left = left->as_InlineType()->buffer(this);\n+  }\n+  if (right->is_InlineType()) {\n+    PreserveReexecuteState preexecs(this);\n+    inc_sp(2);\n+    jvms()->set_should_reexecute(true);\n+    right = right->as_InlineType()->buffer(this);\n+  }\n+\n+  \/\/ First, do a normal pointer comparison\n+  const TypeOopPtr* tleft = _gvn.type(left)->isa_oopptr();\n+  const TypeOopPtr* tright = _gvn.type(right)->isa_oopptr();\n+  Node* cmp = CmpP(left, right);\n+  cmp = optimize_cmp_with_klass(cmp);\n+  if (tleft == nullptr || !tleft->can_be_inline_type() ||\n+      tright == nullptr || !tright->can_be_inline_type()) {\n+    \/\/ This is sufficient, if one of the operands can't be an inline type\n+    do_if(btest, cmp);\n+    return;\n+  }\n+\n+  \/\/ Don't add traps to unstable if branches because additional checks are required to\n+  \/\/ decide if the operands are equal\/substitutable and we therefore shouldn't prune\n+  \/\/ branches for one if based on the profiling of the acmp branches.\n+  \/\/ Also, OptimizeUnstableIf would set an incorrect re-rexecution state because it\n+  \/\/ assumes that there is a 1-1 mapping between the if and the acmp branches and that\n+  \/\/ hitting a trap means that we will take the corresponding acmp branch on re-execution.\n+  const bool can_trap = true;\n+\n+  Node* eq_region = nullptr;\n+  if (btest == BoolTest::eq) {\n+    do_if(btest, cmp, !can_trap, true);\n+    if (stopped()) {\n+      \/\/ Pointers are equal, operands must be equal\n+      return;\n+    }\n+  } else {\n+    assert(btest == BoolTest::ne, \"only eq or ne\");\n+    Node* is_not_equal = nullptr;\n+    eq_region = new RegionNode(3);\n+    {\n+      PreserveJVMState pjvms(this);\n+      \/\/ Pointers are not equal, but more checks are needed to determine if the operands are (not) substitutable\n+      do_if(btest, cmp, !can_trap, false, &is_not_equal);\n+      if (!stopped()) {\n+        eq_region->init_req(1, control());\n+      }\n+    }\n+    if (is_not_equal == nullptr || is_not_equal->is_top()) {\n+      record_for_igvn(eq_region);\n+      set_control(_gvn.transform(eq_region));\n+      return;\n+    }\n+    set_control(is_not_equal);\n+  }\n+\n+  \/\/ Prefer speculative types if available\n+  if (!too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+    if (tleft->speculative_type() != nullptr) {\n+      left_type = tleft->speculative_type();\n+    }\n+    if (tright->speculative_type() != nullptr) {\n+      right_type = tright->speculative_type();\n+    }\n+  }\n+\n+  if (speculative_ptr_kind(tleft) != ProfileMaybeNull && speculative_ptr_kind(tleft) != ProfileUnknownNull) {\n+    ProfilePtrKind speculative_left_ptr = speculative_ptr_kind(tleft);\n+    if (speculative_left_ptr == ProfileAlwaysNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_assert)) {\n+      left_ptr = speculative_left_ptr;\n+    } else if (speculative_left_ptr == ProfileNeverNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check)) {\n+      left_ptr = speculative_left_ptr;\n+    }\n+  }\n+  if (speculative_ptr_kind(tright) != ProfileMaybeNull && speculative_ptr_kind(tright) != ProfileUnknownNull) {\n+    ProfilePtrKind speculative_right_ptr = speculative_ptr_kind(tright);\n+    if (speculative_right_ptr == ProfileAlwaysNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_assert)) {\n+      right_ptr = speculative_right_ptr;\n+    } else if (speculative_right_ptr == ProfileNeverNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check)) {\n+      right_ptr = speculative_right_ptr;\n+    }\n+  }\n+\n+  if (left_ptr == ProfileAlwaysNull) {\n+    \/\/ Comparison with null. Assert the input is indeed null and we're done.\n+    acmp_always_null_input(left, tleft, btest, eq_region);\n+    return;\n+  }\n+  if (right_ptr == ProfileAlwaysNull) {\n+    \/\/ Comparison with null. Assert the input is indeed null and we're done.\n+    acmp_always_null_input(right, tright, btest, eq_region);\n+    return;\n+  }\n+  if (left_type != nullptr && !left_type->is_inlinetype()) {\n+    \/\/ Comparison with an object of known type\n+    acmp_known_non_inline_type_input(left, tleft, left_ptr, left_type, btest, eq_region);\n+    return;\n+  }\n+  if (right_type != nullptr && !right_type->is_inlinetype()) {\n+    \/\/ Comparison with an object of known type\n+    acmp_known_non_inline_type_input(right, tright, right_ptr, right_type, btest, eq_region);\n+    return;\n+  }\n+  if (!left_inline_type) {\n+    \/\/ Comparison with an object known not to be an inline type\n+    acmp_unknown_non_inline_type_input(left, tleft, left_ptr, btest, eq_region);\n+    return;\n+  }\n+  if (!right_inline_type) {\n+    \/\/ Comparison with an object known not to be an inline type\n+    acmp_unknown_non_inline_type_input(right, tright, right_ptr, btest, eq_region);\n+    return;\n+  }\n+\n+  \/\/ Pointers are not equal, check if first operand is non-null\n+  Node* ne_region = new RegionNode(6);\n+  Node* null_ctl = nullptr;\n+  Node* not_null_left = nullptr;\n+  Node* not_null_right = acmp_null_check(right, tright, right_ptr, null_ctl);\n+  ne_region->init_req(1, null_ctl);\n+\n+  if (!stopped()) {\n+    \/\/ First operand is non-null, check if it is an inline type\n+    Node* is_value = inline_type_test(not_null_right);\n+    IfNode* is_value_iff = create_and_map_if(control(), is_value, PROB_FAIR, COUNT_UNKNOWN);\n+    Node* not_value = _gvn.transform(new IfFalseNode(is_value_iff));\n+    ne_region->init_req(2, not_value);\n+    set_control(_gvn.transform(new IfTrueNode(is_value_iff)));\n+\n+    \/\/ The first operand is an inline type, check if the second operand is non-null\n+    not_null_left = acmp_null_check(left, tleft, left_ptr, null_ctl);\n+    ne_region->init_req(3, null_ctl);\n+\n+    if (!stopped()) {\n+      \/\/ Check if both operands are of the same class.\n+      Node* kls_left = load_object_klass(not_null_left);\n+      Node* kls_right = load_object_klass(not_null_right);\n+      Node* kls_cmp = CmpP(kls_left, kls_right);\n+      Node* kls_bol = _gvn.transform(new BoolNode(kls_cmp, BoolTest::ne));\n+      IfNode* kls_iff = create_and_map_if(control(), kls_bol, PROB_FAIR, COUNT_UNKNOWN);\n+      Node* kls_ne = _gvn.transform(new IfTrueNode(kls_iff));\n+      set_control(_gvn.transform(new IfFalseNode(kls_iff)));\n+      ne_region->init_req(4, kls_ne);\n+    }\n+  }\n+\n+  if (stopped()) {\n+    record_for_igvn(ne_region);\n+    set_control(_gvn.transform(ne_region));\n+    if (btest == BoolTest::ne) {\n+      {\n+        PreserveJVMState pjvms(this);\n+        int target_bci = iter().get_dest();\n+        merge(target_bci);\n+      }\n+      record_for_igvn(eq_region);\n+      set_control(_gvn.transform(eq_region));\n+    }\n+    return;\n+  }\n+\n+  \/\/ Both operands are values types of the same class, we need to perform a\n+  \/\/ substitutability test. Delegate to ValueObjectMethods::isSubstitutable().\n+  Node* ne_io_phi = PhiNode::make(ne_region, i_o());\n+  Node* mem = reset_memory();\n+  Node* ne_mem_phi = PhiNode::make(ne_region, mem);\n+\n+  Node* eq_io_phi = nullptr;\n+  Node* eq_mem_phi = nullptr;\n+  if (eq_region != nullptr) {\n+    eq_io_phi = PhiNode::make(eq_region, i_o());\n+    eq_mem_phi = PhiNode::make(eq_region, mem);\n+  }\n+\n+  set_all_memory(mem);\n+\n+  kill_dead_locals();\n+  ciSymbol* subst_method_name = UseAltSubstitutabilityMethod ? ciSymbols::isSubstitutableAlt_name() : ciSymbols::isSubstitutable_name();\n+  ciMethod* subst_method = ciEnv::current()->ValueObjectMethods_klass()->find_method(subst_method_name, ciSymbols::object_object_boolean_signature());\n+  CallStaticJavaNode* call = new CallStaticJavaNode(C, TypeFunc::make(subst_method), SharedRuntime::get_resolve_static_call_stub(), subst_method);\n+  call->set_override_symbolic_info(true);\n+  call->init_req(TypeFunc::Parms, not_null_left);\n+  call->init_req(TypeFunc::Parms+1, not_null_right);\n+  inc_sp(2);\n+  set_edges_for_java_call(call, false, false);\n+  Node* ret = set_results_for_java_call(call, false, true);\n+  dec_sp(2);\n+\n+  \/\/ Test the return value of ValueObjectMethods::isSubstitutable()\n+  \/\/ This is the last check, do_if can emit traps now.\n+  Node* subst_cmp = _gvn.transform(new CmpINode(ret, intcon(1)));\n+  Node* ctl = C->top();\n+  Node* stress_count_mem = nullptr;\n+  if (btest == BoolTest::eq) {\n+    PreserveJVMState pjvms(this);\n+    do_if(btest, subst_cmp, can_trap, false, nullptr, &stress_count_mem);\n+    if (!stopped()) {\n+      ctl = control();\n+    }\n+  } else {\n+    assert(btest == BoolTest::ne, \"only eq or ne\");\n+    PreserveJVMState pjvms(this);\n+    do_if(btest, subst_cmp, can_trap, false, &ctl, &stress_count_mem);\n+    if (!stopped()) {\n+      eq_region->init_req(2, control());\n+      eq_io_phi->init_req(2, i_o());\n+      eq_mem_phi->init_req(2, reset_memory());\n+    }\n+  }\n+  if (stress_count_mem != nullptr) {\n+    set_memory(stress_count_mem, stress_count_mem->adr_type());\n+  }\n+  ne_region->init_req(5, ctl);\n+  ne_io_phi->init_req(5, i_o());\n+  ne_mem_phi->init_req(5, reset_memory());\n+\n+  record_for_igvn(ne_region);\n+  set_control(_gvn.transform(ne_region));\n+  set_i_o(_gvn.transform(ne_io_phi));\n+  set_all_memory(_gvn.transform(ne_mem_phi));\n+\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+    set_i_o(_gvn.transform(eq_io_phi));\n+    set_all_memory(_gvn.transform(eq_mem_phi));\n+  }\n+}\n+\n@@ -1642,1 +2431,1 @@\n-void Parse::adjust_map_after_if(BoolTest::mask btest, Node* c, float prob, Block* path) {\n+void Parse::adjust_map_after_if(BoolTest::mask btest, Node* c, float prob, Block* path, bool can_trap) {\n@@ -1654,1 +2443,1 @@\n-  if (path_is_suitable_for_uncommon_trap(prob)) {\n+  if (can_trap && path_is_suitable_for_uncommon_trap(prob)) {\n@@ -1822,0 +2611,3 @@\n+        if (tboth->is_inlinetypeptr()) {\n+          ccast = InlineTypeNode::make_from_oop(this, ccast, tboth->exact_klass(true)->as_inline_klass());\n+        }\n@@ -1925,0 +2717,4 @@\n+        if (obj->is_InlineType()) {\n+          assert(obj->as_InlineType()->is_allocated(&_gvn), \"must be allocated\");\n+          obj = obj->as_InlineType()->get_oop();\n+        }\n@@ -2664,1 +3460,1 @@\n-    return_current(pop());\n+    return_current(cast_to_non_larval(pop()));\n@@ -2667,2 +3463,0 @@\n-    return_current(pop_pair());\n-    break;\n@@ -2721,15 +3515,21 @@\n-    b = pop();\n-    if (!_gvn.type(b)->speculative_maybe_null() &&\n-        !too_many_traps(Deoptimization::Reason_speculate_null_check)) {\n-      inc_sp(1);\n-      Node* null_ctl = top();\n-      b = null_check_oop(b, &null_ctl, true, true, true);\n-      assert(null_ctl->is_top(), \"no null control here\");\n-      dec_sp(1);\n-    } else if (_gvn.type(b)->speculative_always_null() &&\n-               !too_many_traps(Deoptimization::Reason_speculate_null_assert)) {\n-      inc_sp(1);\n-      b = null_assert(b);\n-      dec_sp(1);\n-    }\n-    c = _gvn.transform( new CmpPNode(b, a) );\n+    b = cast_to_non_larval(pop());\n+    if (b->is_InlineType()) {\n+      \/\/ Null checking a scalarized but nullable inline type. Check the null marker\n+      \/\/ input instead of the oop input to avoid keeping buffer allocations alive\n+      c = _gvn.transform(new CmpINode(b->as_InlineType()->get_null_marker(), zerocon(T_INT)));\n+    } else {\n+      if (!_gvn.type(b)->speculative_maybe_null() &&\n+          !too_many_traps(Deoptimization::Reason_speculate_null_check)) {\n+        inc_sp(1);\n+        Node* null_ctl = top();\n+        b = null_check_oop(b, &null_ctl, true, true, true);\n+        assert(null_ctl->is_top(), \"no null control here\");\n+        dec_sp(1);\n+      } else if (_gvn.type(b)->speculative_always_null() &&\n+                 !too_many_traps(Deoptimization::Reason_speculate_null_assert)) {\n+        inc_sp(1);\n+        b = null_assert(b);\n+        dec_sp(1);\n+      }\n+      c = _gvn.transform( new CmpPNode(b, a) );\n+    }\n@@ -2744,5 +3544,3 @@\n-    a = pop();\n-    b = pop();\n-    c = _gvn.transform( new CmpPNode(b, a) );\n-    c = optimize_cmp_with_klass(c);\n-    do_if(btest, c);\n+    a = cast_to_non_larval(pop());\n+    b = cast_to_non_larval(pop());\n+    do_acmp(btest, b, a);\n@@ -2803,1 +3601,1 @@\n-    do_anewarray();\n+    do_newarray();\n","filename":"src\/hotspot\/share\/opto\/parse2.cpp","additions":908,"deletions":110,"binary":false,"changes":1018,"status":"modified"},{"patch":"@@ -1032,1 +1032,1 @@\n-  const int max_live_nodes_increase_per_iteration = NodeLimitFudgeFactor * 3;\n+  const int max_live_nodes_increase_per_iteration = NodeLimitFudgeFactor * 5;\n@@ -1186,1 +1186,1 @@\n-  n->dump_bfs(1, nullptr, \"\", &ss);\n+  n->dump_bfs(3, nullptr, \"\", &ss);\n@@ -2094,6 +2094,0 @@\n-  if (_delay_transform) {\n-    \/\/ Register the node but don't optimize for now\n-    register_new_node_with_optimizer(n);\n-    return n;\n-  }\n-\n@@ -2106,0 +2100,6 @@\n+  if (_delay_transform) {\n+    \/\/ Add the node to the worklist but don't optimize for now\n+    _worklist.push(n);\n+    return n;\n+  }\n+\n@@ -2378,0 +2378,13 @@\n+void PhaseIterGVN::replace_in_uses(Node* n, Node* m) {\n+  assert(n != nullptr, \"sanity\");\n+  for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+    Node* u = n->fast_out(i);\n+    if (u != n) {\n+      rehash_node_delayed(u);\n+      int nb = u->replace_edge(n, m);\n+      --i, imax -= nb;\n+    }\n+  }\n+  assert(n->outcnt() == 0, \"all uses must be deleted\");\n+}\n+\n@@ -2433,0 +2446,10 @@\n+  \/\/ AndLNode::Ideal folds GraphKit::mark_word_test patterns. Give it a chance to run.\n+  if (n->is_Load() && use->is_Phi()) {\n+    for (DUIterator_Fast imax, i = use->fast_outs(imax); i < imax; i++) {\n+      Node* u = use->fast_out(i);\n+      if (u->Opcode() == Op_AndL) {\n+        worklist.push(u);\n+      }\n+    }\n+  }\n+\n@@ -2530,0 +2553,9 @@\n+  \/\/ Inline type nodes can have other inline types as users. If an input gets\n+  \/\/ updated, make sure that inline type users get a chance for optimization.\n+  if (use->is_InlineType()) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->is_InlineType())\n+        worklist.push(u);\n+    }\n+  }\n@@ -2656,0 +2688,18 @@\n+  if (use_op == Op_CastP2X) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      \/\/ TODO 8350865 Still needed? Yes, I think this is from PhaseMacroExpand::expand_mh_intrinsic_return\n+      if (u->Opcode() == Op_AndX) {\n+        worklist.push(u);\n+      }\n+      \/\/ Search for CmpL(OrL(CastP2X(..), CastP2X(..)), 0L)\n+      if (u->Opcode() == Op_OrL) {\n+        for (DUIterator_Fast i3max, i3 = u->fast_outs(i3max); i3 < i3max; i3++) {\n+          Node* cmp = u->fast_out(i3);\n+          if (cmp->Opcode() == Op_CmpL) {\n+            worklist.push(cmp);\n+          }\n+        }\n+      }\n+    }\n+  }\n@@ -2674,0 +2724,10 @@\n+  \/\/ Give CallStaticJavaNode::remove_useless_allocation a chance to run\n+  if (use->is_Region()) {\n+    Node* c = use;\n+    do {\n+      c = c->unique_ctrl_out_or_null();\n+    } while (c != nullptr && c->is_Region());\n+    if (c != nullptr && c->is_CallStaticJava() && c->as_CallStaticJava()->uncommon_trap_request() != 0) {\n+      worklist.push(c);\n+    }\n+  }\n@@ -2793,1 +2853,1 @@\n-    n->dump(1);\n+    n->dump(3);\n@@ -2957,0 +3017,1 @@\n+  push_cast(worklist, use);\n@@ -3068,0 +3129,13 @@\n+  }\n+}\n+\n+\/\/ TODO 8350865 Still needed? Yes, I think this is from PhaseMacroExpand::expand_mh_intrinsic_return\n+void PhaseCCP::push_cast(Unique_Node_List& worklist, const Node* use) {\n+  uint use_op = use->Opcode();\n+  if (use_op == Op_CastP2X) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->Opcode() == Op_AndX) {\n+        worklist.push(u);\n+      }\n+    }\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":83,"deletions":9,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -482,1 +482,1 @@\n-  virtual void record_for_igvn(Node *n) { }\n+  virtual void record_for_igvn(Node *n) { _worklist.push(n); }\n@@ -558,0 +558,2 @@\n+  void replace_in_uses(Node* n, Node* m);\n+\n@@ -653,0 +655,1 @@\n+  static void push_cast(Unique_Node_List& worklist, const Node* use);\n","filename":"src\/hotspot\/share\/opto\/phaseX.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -344,4 +344,3 @@\n-  CallProjections projs;\n-  call->extract_projections(&projs, false);\n-  if (projs.fallthrough_catchproj != nullptr) {\n-    C->gvn_replace_by(projs.fallthrough_catchproj, call->in(TypeFunc::Control));\n+  CallProjections* projs = call->extract_projections(false);\n+  if (projs->fallthrough_catchproj != nullptr) {\n+    C->gvn_replace_by(projs->fallthrough_catchproj, call->in(TypeFunc::Control));\n@@ -349,2 +348,2 @@\n-  if (projs.fallthrough_memproj != nullptr) {\n-    C->gvn_replace_by(projs.fallthrough_memproj, call->in(TypeFunc::Memory));\n+  if (projs->fallthrough_memproj != nullptr) {\n+    C->gvn_replace_by(projs->fallthrough_memproj, call->in(TypeFunc::Memory));\n@@ -352,2 +351,2 @@\n-  if (projs.catchall_memproj != nullptr) {\n-    C->gvn_replace_by(projs.catchall_memproj, C->top());\n+  if (projs->catchall_memproj != nullptr) {\n+    C->gvn_replace_by(projs->catchall_memproj, C->top());\n@@ -355,2 +354,2 @@\n-  if (projs.fallthrough_ioproj != nullptr) {\n-    C->gvn_replace_by(projs.fallthrough_ioproj, call->in(TypeFunc::I_O));\n+  if (projs->fallthrough_ioproj != nullptr) {\n+    C->gvn_replace_by(projs->fallthrough_ioproj, call->in(TypeFunc::I_O));\n@@ -358,2 +357,2 @@\n-  if (projs.catchall_ioproj != nullptr) {\n-    C->gvn_replace_by(projs.catchall_ioproj, C->top());\n+  if (projs->catchall_ioproj != nullptr) {\n+    C->gvn_replace_by(projs->catchall_ioproj, C->top());\n@@ -361,1 +360,1 @@\n-  if (projs.catchall_catchproj != nullptr) {\n+  if (projs->catchall_catchproj != nullptr) {\n@@ -364,1 +363,1 @@\n-    for (SimpleDUIterator i(projs.catchall_catchproj); i.has_next(); i.next()) {\n+    for (SimpleDUIterator i(projs->catchall_catchproj); i.has_next(); i.next()) {\n@@ -371,1 +370,1 @@\n-    C->gvn_replace_by(projs.catchall_catchproj, C->top());\n+    C->gvn_replace_by(projs->catchall_catchproj, C->top());\n@@ -373,2 +372,3 @@\n-  if (projs.resproj != nullptr) {\n-    C->gvn_replace_by(projs.resproj, C->top());\n+  if (projs->resproj[0] != nullptr) {\n+    assert(projs->nb_resproj == 1, \"unexpected number of results\");\n+    C->gvn_replace_by(projs->resproj[0], C->top());\n@@ -1155,0 +1155,3 @@\n+        if (opc == Op_CheckCastPP) {\n+          worklist.push(use);\n+        }\n","filename":"src\/hotspot\/share\/opto\/stringopts.cpp","additions":20,"deletions":17,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -31,1 +32,0 @@\n-#include \"runtime\/handles.hpp\"\n@@ -147,0 +147,24 @@\n+  class Offset {\n+  private:\n+    int _offset;\n+\n+  public:\n+    explicit Offset(int offset) : _offset(offset) {}\n+\n+    const Offset meet(const Offset other) const;\n+    const Offset dual() const;\n+    const Offset add(intptr_t offset) const;\n+    bool operator==(const Offset& other) const {\n+      return _offset == other._offset;\n+    }\n+    bool operator!=(const Offset& other) const {\n+      return _offset != other._offset;\n+    }\n+    int get() const { return _offset; }\n+\n+    void dump2(outputStream *st) const;\n+\n+    static const Offset top;\n+    static const Offset bottom;\n+  };\n+\n@@ -348,0 +372,3 @@\n+  bool is_inlinetypeptr() const;\n+  virtual ciInlineKlass* inline_klass() const;\n+\n@@ -957,2 +984,2 @@\n-  static const TypeTuple *make_range(ciSignature *sig, InterfaceHandling interface_handling = ignore_interfaces);\n-  static const TypeTuple *make_domain(ciInstanceKlass* recv, ciSignature *sig, InterfaceHandling interface_handling);\n+  static const TypeTuple *make_range(ciSignature* sig, InterfaceHandling interface_handling = ignore_interfaces, bool ret_vt_fields = false);\n+  static const TypeTuple *make_domain(ciMethod* method, InterfaceHandling interface_handling, bool vt_fields_as_args = false);\n@@ -987,2 +1014,2 @@\n-  TypeAry(const Type* elem, const TypeInt* size, bool stable) : Type(Array),\n-      _elem(elem), _size(size), _stable(stable) {}\n+  TypeAry(const Type* elem, const TypeInt* size, bool stable, bool flat, bool not_flat, bool not_null_free, bool atomic) : Type(Array),\n+      _elem(elem), _size(size), _stable(stable), _flat(flat), _not_flat(not_flat), _not_null_free(not_null_free), _atomic(atomic) {}\n@@ -999,0 +1026,7 @@\n+\n+  \/\/ Inline type array properties\n+  const bool _flat;             \/\/ Array is flat\n+  const bool _not_flat;         \/\/ Array is never flat\n+  const bool _not_null_free;    \/\/ Array is never null-free\n+  const bool _atomic;           \/\/ Array is atomic\n+\n@@ -1002,1 +1036,2 @@\n-  static const TypeAry* make(const Type* elem, const TypeInt* size, bool stable = false);\n+  static const TypeAry* make(const Type* elem, const TypeInt* size, bool stable,\n+                             bool flat, bool not_flat, bool not_null_free, bool atomic);\n@@ -1148,0 +1183,19 @@\n+\n+  \/\/ Only applies to TypeInstPtr and TypeInstKlassPtr. Since the common super class is TypePtr, it is defined here.\n+  \/\/\n+  \/\/ FlatInArray defines the following Boolean Lattice structure\n+  \/\/\n+  \/\/     TopFlat\n+  \/\/    \/      \\\n+  \/\/  Flat   NotFlat\n+  \/\/    \\      \/\n+  \/\/   MaybeFlat\n+  \/\/\n+  \/\/ with meet (see TypePtr::meet_flat_in_array()) and join (implemented over dual, see TypePtr::flat_in_array_dual)\n+  enum FlatInArray {\n+    TopFlat,        \/\/ Dedicated top element and dual of MaybeFlat. Result when joining Flat and NotFlat.\n+    Flat,           \/\/ An instance is always flat in an array.\n+    NotFlat,        \/\/ An instance is never flat in an array.\n+    MaybeFlat,      \/\/ We don't know whether an instance is flat in an array.\n+    Uninitialized   \/\/ Used when the flat in array property was not computed, yet - should never actually end up in a type.\n+  };\n@@ -1149,1 +1203,1 @@\n-  TypePtr(TYPES t, PTR ptr, int offset,\n+  TypePtr(TYPES t, PTR ptr, Offset offset,\n@@ -1158,0 +1212,3 @@\n+  static const FlatInArray flat_in_array_dual[Uninitialized];\n+  static const char* const flat_in_array_msg[Uninitialized];\n+\n@@ -1206,0 +1263,2 @@\n+ protected:\n+  static FlatInArray meet_flat_in_array(FlatInArray left, FlatInArray other);\n@@ -1208,1 +1267,1 @@\n-                                                  ciKlass*& res_klass, bool& res_xk);\n+                                                  ciKlass*& res_klass, bool& res_xk, bool &res_flat, bool &res_not_flat, bool &res_not_null_free, bool &res_atomic);\n@@ -1219,1 +1278,1 @@\n-  const int _offset;            \/\/ Offset into oop, with TOP & BOT\n+  const Offset _offset;         \/\/ Offset into oop, with TOP & BOT\n@@ -1222,1 +1281,1 @@\n-  int offset() const { return _offset; }\n+  int offset() const { return _offset.get(); }\n@@ -1225,1 +1284,1 @@\n-  static const TypePtr *make(TYPES t, PTR ptr, int offset,\n+  static const TypePtr* make(TYPES t, PTR ptr, Offset offset,\n@@ -1234,1 +1293,1 @@\n-  int xadd_offset( intptr_t offset ) const;\n+  Type::Offset xadd_offset(intptr_t offset) const;\n@@ -1237,0 +1296,1 @@\n+  virtual int flat_offset() const { return offset(); }\n@@ -1244,2 +1304,2 @@\n-  int meet_offset( int offset ) const;\n-  int dual_offset( ) const;\n+  Offset meet_offset(int offset) const;\n+  Offset dual_offset() const;\n@@ -1273,0 +1333,16 @@\n+  NOT_PRODUCT(static void dump_flat_in_array(FlatInArray flat_in_array, outputStream* st);)\n+\n+  static FlatInArray compute_flat_in_array(ciInstanceKlass* instance_klass, bool is_exact);\n+  FlatInArray compute_flat_in_array_if_unknown(ciInstanceKlass* instance_klass, bool is_exact,\n+                                               FlatInArray old_flat_in_array) const;\n+\n+  virtual bool can_be_inline_type() const { return false; }\n+  virtual bool is_flat_in_array()     const { return flat_in_array() == Flat; }\n+  virtual bool is_not_flat_in_array() const { return flat_in_array() == NotFlat; }\n+  virtual FlatInArray flat_in_array() const { return NotFlat; }\n+  virtual bool is_flat()            const { return false; }\n+  virtual bool is_not_flat()        const { return false; }\n+  virtual bool is_null_free()       const { return false; }\n+  virtual bool is_not_null_free()   const { return false; }\n+  virtual bool is_atomic()          const { return false; }\n+\n@@ -1290,1 +1366,1 @@\n-  TypeRawPtr( PTR ptr, address bits ) : TypePtr(RawPtr,ptr,0), _bits(bits){}\n+  TypeRawPtr(PTR ptr, address bits) : TypePtr(RawPtr,ptr,Offset(0)), _bits(bits){}\n@@ -1326,1 +1402,1 @@\n- TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int offset, int instance_id,\n+ TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset, Offset field_offset, int instance_id,\n@@ -1350,0 +1426,1 @@\n+  bool          _is_ptr_to_strict_final_field;\n@@ -1367,1 +1444,1 @@\n-  virtual ciKlass* klass() const { return _klass;     }\n+  virtual ciKlass* klass() const { return _klass; }\n@@ -1418,1 +1495,1 @@\n-  static const TypeOopPtr* make(PTR ptr, int offset, int instance_id,\n+  static const TypeOopPtr* make(PTR ptr, Offset offset, int instance_id,\n@@ -1435,0 +1512,1 @@\n+  bool is_ptr_to_strict_final_field() const { return _is_ptr_to_strict_final_field; }\n@@ -1437,1 +1515,4 @@\n-  bool is_known_instance_field() const { return is_known_instance() && _offset >= 0; }\n+  bool is_known_instance_field() const { return is_known_instance() && _offset.get() >= 0; }\n+\n+  virtual bool can_be_inline_type() const { return (_klass == nullptr || _klass->can_be_inline_klass(_klass_is_exact)); }\n+  virtual bool can_be_inline_array() const { ShouldNotReachHere(); return false; }\n@@ -1500,2 +1581,6 @@\n-  TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int off, int instance_id,\n-              const TypePtr* speculative, int inline_depth);\n+  \/\/ Can this instance be in a flat array?\n+  FlatInArray _flat_in_array;\n+\n+  TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset,\n+              FlatInArray flat_in_array, int instance_id, const TypePtr* speculative,\n+              int inline_depth);\n@@ -1504,1 +1589,0 @@\n-\n@@ -1523,1 +1607,1 @@\n-    return make(TypePtr::Constant, k, interfaces, true, o, 0, InstanceBot);\n+    return make(TypePtr::Constant, k, interfaces, true, o, Offset(0));\n@@ -1526,1 +1610,1 @@\n-  static const TypeInstPtr *make(ciObject* o, int offset) {\n+  static const TypeInstPtr *make(ciObject* o, Offset offset) {\n@@ -1529,1 +1613,1 @@\n-    return make(TypePtr::Constant, k, interfaces, true, o, offset, InstanceBot);\n+    return make(TypePtr::Constant, k, interfaces, true, o, offset);\n@@ -1535,1 +1619,1 @@\n-    return make(ptr, klass, interfaces, false, nullptr, 0, InstanceBot);\n+    return make(ptr, klass, interfaces, false, nullptr, Offset(0));\n@@ -1541,1 +1625,1 @@\n-    return make(ptr, klass, interfaces, true, nullptr, 0, InstanceBot);\n+    return make(ptr, klass, interfaces, true, nullptr, Offset(0));\n@@ -1545,1 +1629,1 @@\n-  static const TypeInstPtr *make(PTR ptr, ciKlass* klass, int offset) {\n+  static const TypeInstPtr *make(PTR ptr, ciKlass* klass, Offset offset) {\n@@ -1547,1 +1631,1 @@\n-    return make(ptr, klass, interfaces, false, nullptr, offset, InstanceBot);\n+    return make(ptr, klass, interfaces, false, nullptr, offset);\n@@ -1550,1 +1634,3 @@\n-  static const TypeInstPtr *make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int offset,\n+  \/\/ Make a pointer to an oop.\n+  static const TypeInstPtr* make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset,\n+                                 FlatInArray flat_in_array = Uninitialized,\n@@ -1555,1 +1641,2 @@\n-  static const TypeInstPtr *make(PTR ptr, ciKlass* k, bool xk, ciObject* o, int offset, int instance_id = InstanceBot) {\n+  static const TypeInstPtr *make(PTR ptr, ciKlass* k, bool xk, ciObject* o, Offset offset, int instance_id = InstanceBot,\n+                                 FlatInArray flat_in_array = Uninitialized) {\n@@ -1557,1 +1644,1 @@\n-    return make(ptr, k, interfaces, xk, o, offset, instance_id);\n+    return make(ptr, k, interfaces, xk, o, offset, flat_in_array, instance_id);\n@@ -1574,0 +1661,1 @@\n+  virtual bool empty() const;\n@@ -1583,0 +1671,8 @@\n+  virtual const TypeInstPtr* cast_to_flat_in_array() const;\n+  virtual const TypeInstPtr* cast_to_maybe_flat_in_array() const;\n+  virtual FlatInArray flat_in_array() const { return _flat_in_array; }\n+\n+  FlatInArray dual_flat_in_array() const {\n+    return flat_in_array_dual[_flat_in_array];\n+  }\n+\n@@ -1590,0 +1686,2 @@\n+  virtual bool can_be_inline_array() const;\n+\n@@ -1614,0 +1712,1 @@\n+  friend class TypeInstPtr;\n@@ -1616,4 +1715,4 @@\n-  TypeAryPtr( PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk,\n-              int offset, int instance_id, bool is_autobox_cache,\n-              const TypePtr* speculative, int inline_depth)\n-    : TypeOopPtr(AryPtr,ptr,k,_array_interfaces,xk,o,offset, instance_id, speculative, inline_depth),\n+  TypeAryPtr(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk,\n+             Offset offset, Offset field_offset, int instance_id, bool is_autobox_cache,\n+             const TypePtr* speculative, int inline_depth)\n+    : TypeOopPtr(AryPtr, ptr, k, _array_interfaces, xk, o, offset, field_offset, instance_id, speculative, inline_depth),\n@@ -1621,1 +1720,2 @@\n-    _is_autobox_cache(is_autobox_cache)\n+    _is_autobox_cache(is_autobox_cache),\n+    _field_offset(field_offset)\n@@ -1627,2 +1727,2 @@\n-        _offset != 0 && _offset != arrayOopDesc::length_offset_in_bytes() &&\n-        _offset != arrayOopDesc::klass_offset_in_bytes()) {\n+        _offset.get() != 0 && _offset.get() != arrayOopDesc::length_offset_in_bytes() &&\n+        _offset.get() != arrayOopDesc::klass_offset_in_bytes()) {\n@@ -1637,0 +1737,6 @@\n+  \/\/ For flat inline type arrays, each field of the inline type in\n+  \/\/ the array has its own memory slice so we need to keep track of\n+  \/\/ which field is accessed\n+  const Offset _field_offset;\n+  Offset meet_field_offset(const Type::Offset offset) const;\n+  Offset dual_field_offset() const;\n@@ -1664,0 +1770,7 @@\n+  \/\/ Inline type array properties\n+  bool is_flat()          const { return _ary->_flat; }\n+  bool is_not_flat()      const { return _ary->_not_flat; }\n+  bool is_null_free()     const { return _ary->_elem->make_ptr() != nullptr && (_ary->_elem->make_ptr()->ptr() == NotNull || _ary->_elem->make_ptr()->ptr() == AnyNull); }\n+  bool is_not_null_free() const { return _ary->_not_null_free; }\n+  bool is_atomic()        const { return _ary->_atomic; }\n+\n@@ -1666,1 +1779,2 @@\n-  static const TypeAryPtr *make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+  static const TypeAryPtr* make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, Offset offset,\n+                                Offset field_offset = Offset::bottom,\n@@ -1671,1 +1785,2 @@\n-  static const TypeAryPtr *make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+  static const TypeAryPtr* make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, Offset offset,\n+                                Offset field_offset = Offset::bottom,\n@@ -1674,1 +1789,2 @@\n-                                int inline_depth = InlineDepthBottom, bool is_autobox_cache = false);\n+                                int inline_depth = InlineDepthBottom,\n+                                bool is_autobox_cache = false);\n@@ -1693,0 +1809,1 @@\n+  virtual const Type* cleanup_speculative() const;\n@@ -1700,0 +1817,10 @@\n+  \/\/ Inline type array properties\n+  const TypeAryPtr* cast_to_flat(bool flat) const;\n+  const TypeAryPtr* cast_to_not_flat(bool not_flat = true) const;\n+  const TypeAryPtr* cast_to_null_free(bool null_free) const;\n+  const TypeAryPtr* cast_to_not_null_free(bool not_null_free = true) const;\n+  const TypeAryPtr* update_properties(const TypeAryPtr* new_type) const;\n+  jint flat_layout_helper() const;\n+  int flat_elem_size() const;\n+  int flat_log_elem_size() const;\n+\n@@ -1705,1 +1832,8 @@\n-  static jint max_array_length(BasicType etype) ;\n+  static jint max_array_length(BasicType etype);\n+\n+  int flat_offset() const;\n+  const Offset field_offset() const { return _field_offset; }\n+  const TypeAryPtr* with_field_offset(int offset) const;\n+  const TypePtr* add_field_offset_and_offset(intptr_t offset) const;\n+\n+  virtual bool can_be_inline_type() const { return false; }\n@@ -1708,0 +1842,2 @@\n+  virtual bool can_be_inline_array() const;\n+\n@@ -1710,10 +1846,11 @@\n-  static const TypeAryPtr* RANGE;\n-  static const TypeAryPtr* OOPS;\n-  static const TypeAryPtr* NARROWOOPS;\n-  static const TypeAryPtr* BYTES;\n-  static const TypeAryPtr* SHORTS;\n-  static const TypeAryPtr* CHARS;\n-  static const TypeAryPtr* INTS;\n-  static const TypeAryPtr* LONGS;\n-  static const TypeAryPtr* FLOATS;\n-  static const TypeAryPtr* DOUBLES;\n+  static const TypeAryPtr *RANGE;\n+  static const TypeAryPtr *OOPS;\n+  static const TypeAryPtr *NARROWOOPS;\n+  static const TypeAryPtr *BYTES;\n+  static const TypeAryPtr *SHORTS;\n+  static const TypeAryPtr *CHARS;\n+  static const TypeAryPtr *INTS;\n+  static const TypeAryPtr *LONGS;\n+  static const TypeAryPtr *FLOATS;\n+  static const TypeAryPtr *DOUBLES;\n+  static const TypeAryPtr *INLINES;\n@@ -1738,1 +1875,1 @@\n-  TypeMetadataPtr(PTR ptr, ciMetadata* metadata, int offset);\n+  TypeMetadataPtr(PTR ptr, ciMetadata* metadata, Offset offset);\n@@ -1750,1 +1887,1 @@\n-  static const TypeMetadataPtr* make(PTR ptr, ciMetadata* m, int offset);\n+  static const TypeMetadataPtr* make(PTR ptr, ciMetadata* m, Offset offset);\n@@ -1781,1 +1918,1 @@\n-  TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, int offset);\n+  TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, Offset offset);\n@@ -1820,1 +1957,0 @@\n-  static const TypeKlassPtr *make(PTR ptr, ciKlass* klass, int offset, InterfaceHandling interface_handling = ignore_interfaces);\n@@ -1839,0 +1975,2 @@\n+  virtual bool can_be_inline_array() const { ShouldNotReachHere(); return false; }\n+\n@@ -1869,0 +2007,2 @@\n+  \/\/ Can an instance of this class be in a flat array?\n+  const FlatInArray _flat_in_array;\n@@ -1870,2 +2010,3 @@\n-  TypeInstKlassPtr(PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, int offset)\n-    : TypeKlassPtr(InstKlassPtr, ptr, klass, interfaces, offset) {\n+  TypeInstKlassPtr(PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, Offset offset, FlatInArray flat_in_array)\n+    : TypeKlassPtr(InstKlassPtr, ptr, klass, interfaces, offset), _flat_in_array(flat_in_array) {\n+    assert(flat_in_array != Uninitialized, \"must be set now\");\n@@ -1890,0 +2031,2 @@\n+  virtual bool can_be_inline_type() const { return (_klass == nullptr || _klass->can_be_inline_klass(klass_is_exact())); }\n+\n@@ -1892,1 +2035,1 @@\n-    return make(TypePtr::Constant, k, interfaces, 0);\n+    return make(TypePtr::Constant, k, interfaces, Offset(0));\n@@ -1894,2 +2037,4 @@\n-  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, int offset);\n-  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, int offset) {\n+  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, Offset offset,\n+                                      FlatInArray flat_in_array = Uninitialized);\n+\n+  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, Offset offset, FlatInArray flat_in_array = Uninitialized) {\n@@ -1898,1 +2043,1 @@\n-    return make(ptr, k, interfaces, offset);\n+    return make(ptr, k, interfaces, offset, flat_in_array);\n@@ -1910,0 +2055,2 @@\n+\n+  virtual bool empty() const;\n@@ -1917,0 +2064,8 @@\n+  virtual FlatInArray flat_in_array() const { return _flat_in_array; }\n+\n+  FlatInArray dual_flat_in_array() const {\n+    return flat_in_array_dual[_flat_in_array];\n+  }\n+\n+  virtual bool can_be_inline_array() const;\n+\n@@ -1936,0 +2091,6 @@\n+  const bool _not_flat;      \/\/ Array is never flat\n+  const bool _not_null_free; \/\/ Array is never null-free\n+  const bool _flat;\n+  const bool _null_free;\n+  const bool _atomic;\n+  const bool _refined_type;\n@@ -1938,3 +2099,3 @@\n-  TypeAryKlassPtr(PTR ptr, const Type *elem, ciKlass* klass, int offset)\n-    : TypeKlassPtr(AryKlassPtr, ptr, klass, _array_interfaces, offset), _elem(elem) {\n-    assert(klass == nullptr || klass->is_type_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"\");\n+  TypeAryKlassPtr(PTR ptr, const Type *elem, ciKlass* klass, Offset offset, bool not_flat, int not_null_free, bool flat, bool null_free, bool atomic, bool refined_type)\n+    : TypeKlassPtr(AryKlassPtr, ptr, klass, _array_interfaces, offset), _elem(elem), _not_flat(not_flat), _not_null_free(not_null_free), _flat(flat), _null_free(null_free), _atomic(atomic), _refined_type(refined_type) {\n+    assert(klass == nullptr || klass->is_type_array_klass() || klass->is_flat_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"\");\n@@ -1949,0 +2110,24 @@\n+  bool dual_flat() const {\n+    return _flat;\n+  }\n+\n+  bool meet_flat(bool other) const {\n+    return _flat && other;\n+  }\n+\n+  bool dual_null_free() const {\n+    return _null_free;\n+  }\n+\n+  bool meet_null_free(bool other) const {\n+    return _null_free && other;\n+  }\n+\n+  bool dual_atomic() const {\n+    return _atomic;\n+  }\n+\n+  bool meet_atomic(bool other) const {\n+    return _atomic && other;\n+  }\n+\n@@ -1954,1 +2139,1 @@\n-  static const TypeAryKlassPtr *make(PTR ptr, ciKlass* k, int offset, InterfaceHandling interface_handling);\n+  static const TypeAryKlassPtr* make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling, bool not_flat, bool not_null_free, bool flat, bool null_free, bool atomic, bool refined_type);\n@@ -1962,1 +2147,1 @@\n-  static const TypeAryKlassPtr *make(PTR ptr, const Type *elem, ciKlass* k, int offset);\n+  static const TypeAryKlassPtr* make(PTR ptr, const Type* elem, ciKlass* k, Offset offset, bool not_flat, bool not_null_free, bool flat, bool null_free, bool atomic, bool refined_type);\n@@ -1965,0 +2150,3 @@\n+  const TypeAryKlassPtr* cast_to_non_refined() const;\n+  const TypeAryKlassPtr* cast_to_refined_array_klass_ptr(bool refined = true) const;\n+\n@@ -1987,0 +2175,8 @@\n+  bool is_flat()          const { return _flat; }\n+  bool is_not_flat()      const { return _not_flat; }\n+  bool is_null_free()     const { return _null_free; }\n+  bool is_not_null_free() const { return _not_null_free; }\n+  bool is_atomic()        const { return _atomic; }\n+  bool is_refined_type()  const { return _refined_type; }\n+  virtual bool can_be_inline_array() const;\n+\n@@ -2122,1 +2318,2 @@\n-  TypeFunc( const TypeTuple *domain, const TypeTuple *range ) : Type(Function),  _domain(domain), _range(range) {}\n+  TypeFunc(const TypeTuple *domain_sig, const TypeTuple *domain_cc, const TypeTuple *range_sig, const TypeTuple *range_cc)\n+    : Type(Function), _domain_sig(domain_sig), _domain_cc(domain_cc), _range_sig(range_sig), _range_cc(range_cc) {}\n@@ -2128,2 +2325,13 @@\n-  const TypeTuple* const _domain;     \/\/ Domain of inputs\n-  const TypeTuple* const _range;      \/\/ Range of results\n+  \/\/ Domains of inputs: inline type arguments are not passed by\n+  \/\/ reference, instead each field of the inline type is passed as an\n+  \/\/ argument. We maintain 2 views of the argument list here: one\n+  \/\/ based on the signature (with an inline type argument as a single\n+  \/\/ slot), one based on the actual calling convention (with a value\n+  \/\/ type argument as a list of its fields).\n+  const TypeTuple* const _domain_sig;\n+  const TypeTuple* const _domain_cc;\n+  \/\/ Range of results. Similar to domains: an inline type result can be\n+  \/\/ returned in registers in which case range_cc lists all fields and\n+  \/\/ is the actual calling convention.\n+  const TypeTuple* const _range_sig;\n+  const TypeTuple* const _range_cc;\n@@ -2143,5 +2351,8 @@\n-  const TypeTuple* domain() const { return _domain; }\n-  const TypeTuple* range()  const { return _range; }\n-\n-  static const TypeFunc *make(ciMethod* method);\n-  static const TypeFunc *make(ciSignature signature, const Type* extra);\n+  const TypeTuple* domain_sig() const { return _domain_sig; }\n+  const TypeTuple* domain_cc()  const { return _domain_cc; }\n+  const TypeTuple* range_sig()  const { return _range_sig; }\n+  const TypeTuple* range_cc()   const { return _range_cc; }\n+\n+  static const TypeFunc* make(ciMethod* method, bool is_osr_compilation = false);\n+  static const TypeFunc *make(const TypeTuple* domain_sig, const TypeTuple* domain_cc,\n+                              const TypeTuple* range_sig, const TypeTuple* range_cc);\n@@ -2155,0 +2366,2 @@\n+  bool returns_inline_type_as_fields() const { return range_sig() != range_cc(); }\n+\n@@ -2431,0 +2644,8 @@\n+inline bool Type::is_inlinetypeptr() const {\n+  return isa_instptr() != nullptr && is_instptr()->instance_klass()->is_inlinetype();\n+}\n+\n+inline ciInlineKlass* Type::inline_klass() const {\n+  return make_ptr()->is_instptr()->instance_klass()->as_inline_klass();\n+}\n+\n@@ -2476,0 +2697,1 @@\n+#define CmpUXNode    CmpULNode\n@@ -2494,0 +2716,1 @@\n+#define Op_StoreX    Op_StoreL\n@@ -2522,0 +2745,1 @@\n+#define CmpUXNode    CmpUNode\n@@ -2540,0 +2764,1 @@\n+#define Op_StoreX    Op_StoreI\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":301,"deletions":76,"binary":false,"changes":377,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -68,0 +69,1 @@\n+#include \"oops\/refArrayOop.inline.hpp\"\n@@ -430,0 +432,133 @@\n+static void validate_array_arguments(Klass* elmClass, jint len, TRAPS) {\n+  if (len < 0) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Array length is negative\");\n+  }\n+  elmClass->initialize(CHECK);\n+  if (elmClass->is_array_klass() || elmClass->is_identity_class()) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Element class is not a value class\");\n+  }\n+  if (elmClass->is_abstract()) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Element class is abstract\");\n+  }\n+}\n+\n+JVM_ENTRY(jarray, JVM_CopyOfSpecialArray(JNIEnv *env, jarray orig, jint from, jint to))\n+  oop o = JNIHandles::resolve_non_null(orig);\n+  assert(o->is_array(), \"Must be\");\n+  oop array = nullptr;\n+  arrayOop org = (arrayOop)o;\n+  arrayHandle oh(THREAD, org);\n+  ObjArrayKlass* ak = ObjArrayKlass::cast(org->klass());\n+  InlineKlass* vk = InlineKlass::cast(ak->element_klass());\n+  int len = to - from;  \/\/ length of the new array\n+  if (ak->is_null_free_array_klass()) {\n+    if ((len != 0) && (from >= org->length() || to > org->length())) {\n+      THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Copying of null-free array with uninitialized elements\");\n+    }\n+  }\n+  if (org->is_flatArray()) {\n+    FlatArrayKlass* fak = FlatArrayKlass::cast(org->klass());\n+    LayoutKind lk = fak->layout_kind();\n+    ArrayKlass::ArrayProperties props = ArrayKlass::array_properties_from_layout(lk);\n+    array = oopFactory::new_flatArray(vk, len, props, lk, CHECK_NULL);\n+    arrayHandle ah(THREAD, (arrayOop)array);\n+    int end = to < oh()->length() ? to : oh()->length();\n+    for (int i = from; i < end; i++) {\n+      void* src = ((flatArrayOop)oh())->value_at_addr(i, fak->layout_helper());\n+      void* dst = ((flatArrayOop)ah())->value_at_addr(i - from, fak->layout_helper());\n+      vk->copy_payload_to_addr(src, dst, lk, false);\n+    }\n+    array = ah();\n+  } else {\n+    ArrayKlass::ArrayProperties props = org->is_null_free_array() ? ArrayKlass::ArrayProperties::NULL_RESTRICTED : ArrayKlass::ArrayProperties::DEFAULT;\n+    array = oopFactory::new_objArray(vk, len, props,  CHECK_NULL);\n+    int end = to < oh()->length() ? to : oh()->length();\n+    for (int i = from; i < end; i++) {\n+      if (i < ((objArrayOop)oh())->length()) {\n+        ((objArrayOop)array)->obj_at_put(i - from, ((objArrayOop)oh())->obj_at(i));\n+      } else {\n+        assert(!ak->is_null_free_array_klass(), \"Must be a nullable array\");\n+        ((objArrayOop)array)->obj_at_put(i - from, nullptr);\n+      }\n+    }\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jarray, JVM_NewNullRestrictedNonAtomicArray(JNIEnv *env, jclass elmClass, jint len, jobject initVal))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  oop init = JNIHandles::resolve(initVal);\n+  if (init == nullptr) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Initial value cannot be null\");\n+  }\n+  Handle init_h(THREAD, init);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  if (klass != init_h()->klass()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Type mismatch between array and initial value\");\n+  }\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  ArrayKlass::ArrayProperties props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::NON_ATOMIC | ArrayKlass::ArrayProperties::NULL_RESTRICTED);\n+  objArrayOop array = oopFactory::new_objArray(klass, len, props, CHECK_NULL);\n+  for (int i = 0; i < len; i++) {\n+    array->obj_at_put(i, init_h() \/*, CHECK_NULL*\/ );\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jarray, JVM_NewNullRestrictedAtomicArray(JNIEnv *env, jclass elmClass, jint len, jobject initVal))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  oop init = JNIHandles::resolve(initVal);\n+  if (init == nullptr) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Initial value cannot be null\");\n+  }\n+  Handle init_h(THREAD, init);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  if (klass != init_h()->klass()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Type mismatch between array and initial value\");\n+  }\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  ArrayKlass::ArrayProperties props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::NULL_RESTRICTED);\n+  objArrayOop array = oopFactory::new_objArray(klass, len, props, CHECK_NULL);\n+  for (int i = 0; i < len; i++) {\n+    array->obj_at_put(i, init_h() \/*, CHECK_NULL*\/ );\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jarray, JVM_NewNullableAtomicArray(JNIEnv *env, jclass elmClass, jint len))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  klass->initialize(CHECK_NULL);\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  ArrayKlass::ArrayProperties props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::DEFAULT);\n+  objArrayOop array = oopFactory::new_objArray(klass, len, props, CHECK_NULL);\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsFlatArray(JNIEnv *env, jobject obj))\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  return oop->is_flatArray();\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsNullRestrictedArray(JNIEnv *env, jobject obj))\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  return oop->is_null_free_array();\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsAtomicArray(JNIEnv *env, jobject obj))\n+  \/\/ There are multiple cases where an array can\/must support atomic access:\n+  \/\/   - the array is a reference array\n+  \/\/   - the array uses an atomic flat layout: NULLABLE_ATOMIC_FLAT or NULL_FREE_ATOMIC_FLAT\n+  \/\/   - the array is flat and its component type is naturally atomic\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  if (oop->is_refArray()) return true;\n+  if (oop->is_flatArray()) {\n+    FlatArrayKlass* fak = FlatArrayKlass::cast(oop->klass());\n+    if (LayoutKindHelper::is_atomic_flat(fak->layout_kind())) return true;\n+    if (fak->element_klass()->is_naturally_atomic()) return true;\n+  }\n+  return false;\n+JVM_END\n@@ -638,2 +773,23 @@\n-  return handle == nullptr ? 0 :\n-         checked_cast<jint>(ObjectSynchronizer::FastHashCode (THREAD, JNIHandles::resolve_non_null(handle)));\n+  if (handle == nullptr) {\n+    return 0;\n+  }\n+  oop obj = JNIHandles::resolve_non_null(handle);\n+  if (Arguments::is_valhalla_enabled() && obj->klass()->is_inline_klass()) {\n+      JavaValue result(T_INT);\n+      JavaCallArguments args;\n+      Handle ho(THREAD, obj);\n+      args.push_oop(ho);\n+      methodHandle method(THREAD, UseAltSubstitutabilityMethod\n+              ? Universe::value_object_hash_codeAlt_method() : Universe::value_object_hash_code_method());\n+      JavaCalls::call(&result, method, &args, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in hashCode\", e, false);\n+        }\n+      }\n+      return result.get_jint();\n+  } else {\n+    return checked_cast<jint>(ObjectSynchronizer::FastHashCode(THREAD, obj));\n+  }\n@@ -687,0 +843,6 @@\n+  if (klass->is_inline_klass()) {\n+    \/\/ Value instances have no identity, so return the current instance instead of allocating a new one\n+    \/\/ Value classes cannot have finalizers, so the method can return immediately\n+    return JNIHandles::make_local(THREAD, obj());\n+  }\n+\n@@ -1179,1 +1341,2 @@\n-    size = InstanceKlass::cast(klass)->local_interfaces()->length();\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    size = ik->local_interfaces()->length();\n@@ -1683,1 +1846,1 @@\n-    if (want_constructor && !method->is_object_initializer()) {\n+    if (want_constructor && !method->is_object_constructor()) {\n@@ -1687,1 +1850,1 @@\n-        (method->is_object_initializer() || method->is_static_initializer() ||\n+        (method->is_object_constructor() || method->is_class_initializer() ||\n@@ -1715,0 +1878,1 @@\n+        assert(method->is_object_constructor(), \"must be\");\n@@ -1975,1 +2139,1 @@\n-  if (m->is_object_initializer()) {\n+  if (m->is_object_constructor()) {\n@@ -1978,1 +2142,0 @@\n-    \/\/ new_method accepts <clinit> as Method here\n@@ -2425,1 +2588,1 @@\n-  return method->name() == vmSymbols::object_initializer_name();\n+  return method->is_object_constructor();\n@@ -3219,0 +3382,4 @@\n+JVM_LEAF(jboolean, JVM_IsValhallaEnabled(void))\n+  return Arguments::is_valhalla_enabled() ? JNI_TRUE : JNI_FALSE;\n+JVM_END\n+\n@@ -3298,1 +3465,3 @@\n-    objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n+    objArrayHandle args(THREAD, (objArrayOop)JNIHandles::resolve(args0));\n+    assert(args() == nullptr || !args->is_flatArray(), \"args are never flat or are they???\");\n+\n@@ -3318,0 +3487,2 @@\n+  objArrayHandle args(THREAD, (objArrayOop)JNIHandles::resolve(args0));\n+  assert(args() == nullptr || !args->is_flatArray(), \"args are never flat or are they???\");\n@@ -3319,1 +3490,0 @@\n-  objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n@@ -3556,0 +3726,1 @@\n+  refArrayHandle rah(THREAD, (refArrayOop)ah()); \/\/ j.l.Thread is an identity class, arrays are always reference arrays\n@@ -3561,1 +3732,1 @@\n-    oop thread_obj = ah->obj_at(i);\n+    oop thread_obj = rah->obj_at(i);\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":182,"deletions":11,"binary":false,"changes":193,"status":"modified"},{"patch":"@@ -74,0 +74,61 @@\n+\n+\/\/ Helper class to store objects to visit.\n+class JvmtiHeapwalkVisitStack {\n+private:\n+  enum {\n+    initial_visit_stack_size = 4000\n+  };\n+\n+  GrowableArray<JvmtiHeapwalkObject>* _visit_stack;\n+  JVMTIBitSet _bitset;\n+\n+  static GrowableArray<JvmtiHeapwalkObject>* create_visit_stack() {\n+    return new (mtServiceability) GrowableArray<JvmtiHeapwalkObject>(initial_visit_stack_size, mtServiceability);\n+  }\n+\n+public:\n+  JvmtiHeapwalkVisitStack(): _visit_stack(create_visit_stack()) {\n+  }\n+  ~JvmtiHeapwalkVisitStack() {\n+    if (_visit_stack != nullptr) {\n+      delete _visit_stack;\n+    }\n+  }\n+\n+  bool is_empty() const {\n+    return _visit_stack->is_empty();\n+  }\n+\n+  void push(const JvmtiHeapwalkObject& obj) {\n+    _visit_stack->push(obj);\n+  }\n+\n+  \/\/ If the object hasn't been visited then push it onto the visit stack\n+  \/\/ so that it will be visited later.\n+  void check_for_visit(const JvmtiHeapwalkObject& obj) {\n+    if (!is_visited(obj)) {\n+      _visit_stack->push(obj);\n+    }\n+  }\n+\n+  JvmtiHeapwalkObject pop() {\n+    return _visit_stack->pop();\n+  }\n+\n+  bool is_visited(const JvmtiHeapwalkObject& obj) \/*const*\/ { \/\/ TODO: _bitset.is_marked() should be const\n+    \/\/ The method is called only for objects from visit_stack to ensure an object is not visited twice.\n+    \/\/ Flat objects can be added to visit_stack only when we visit their holder object, so we cannot get duplicate reference to it.\n+    if (obj.is_flat()) {\n+      return false;\n+    }\n+    return _bitset.is_marked(obj.obj());\n+  }\n+\n+  void mark_visited(const JvmtiHeapwalkObject& obj) {\n+    if (!obj.is_flat()) {\n+      _bitset.mark_obj(obj.obj());\n+    }\n+  }\n+};\n+\n+\n@@ -81,1 +142,2 @@\n-  _posting_events(false) {\n+  _posting_events(false),\n+  _converting_flat_object(false) {\n@@ -87,0 +149,1 @@\n+  _flat_hashmap = new JvmtiFlatTagMapTable();\n@@ -102,0 +165,1 @@\n+  delete _flat_hashmap;\n@@ -110,0 +174,1 @@\n+  _flat_hashmap->clear();\n@@ -128,6 +193,1 @@\n-\/\/ iterate over all entries in the tag map.\n-void JvmtiTagMap::entry_iterate(JvmtiTagMapKeyClosure* closure) {\n-  hashmap()->entry_iterate(closure);\n-}\n-\n-bool JvmtiTagMap::is_empty() {\n+bool JvmtiTagMap::is_empty() const {\n@@ -136,1 +196,1 @@\n-  return hashmap()->is_empty();\n+  return _hashmap->is_empty() && _flat_hashmap->is_empty();\n@@ -170,5 +230,170 @@\n-\/\/ Return the tag value for an object, or 0 if the object is\n-\/\/ not tagged\n-\/\/\n-static inline jlong tag_for(JvmtiTagMap* tag_map, oop o) {\n-  return tag_map->hashmap()->find(o);\n+\/\/ Converts entries from JvmtiFlatTagMapTable to JvmtiTagMapTable in batches.\n+\/\/   1. (JvmtiTagMap is locked)\n+\/\/      reads entries from JvmtiFlatTagMapTable (describe flat value objects);\n+\/\/   2. (JvmtiTagMap is unlocked)\n+\/\/      creates heap-allocated copies of the flat object;\n+\/\/   3. (JvmtiTagMap is locked)\n+\/\/      ensures source entry still exists, removes it from JvmtiFlatTagMapTable, adds new entry to JvmtiTagMapTable.\n+\/\/ If some error occurs in step 2 (OOM?), the process stops.\n+class JvmtiTagMapFlatEntryConverter: public StackObj {\n+private:\n+  struct Entry {\n+    \/\/ source flat value object\n+    Handle holder;\n+    int offset;\n+    InlineKlass* inline_klass;\n+    LayoutKind layout_kind;\n+    \/\/ converted heap-allocated object\n+    Handle dst;\n+\n+    Entry(): holder(), offset(0), inline_klass(nullptr), dst() {}\n+    Entry(Handle holder, int offset, InlineKlass* inline_klass, LayoutKind lk)\n+      : holder(holder), offset(offset), inline_klass(inline_klass), layout_kind(lk), dst() {}\n+  };\n+\n+  int _batch_size;\n+  GrowableArray<Entry> _entries;\n+  bool _has_error;\n+\n+public:\n+  JvmtiTagMapFlatEntryConverter(int batch_size): _batch_size(batch_size), _entries(batch_size, mtServiceability), _has_error(false) { }\n+  ~JvmtiTagMapFlatEntryConverter() {}\n+\n+  \/\/ returns false if there is nothing to convert\n+  bool import_entries(JvmtiFlatTagMapTable* table) {\n+    if (_has_error) {\n+      \/\/ stop the process to avoid infinite loop\n+      return false;\n+    }\n+\n+    class Importer: public JvmtiFlatTagMapKeyClosure {\n+    private:\n+      GrowableArray<Entry>& _entries;\n+      int _batch_size;\n+    public:\n+      Importer(GrowableArray<Entry>& entries, int batch_size): _entries(entries), _batch_size(batch_size) {}\n+\n+      bool do_entry(JvmtiFlatTagMapKey& key, jlong& tag) {\n+        Entry entry(Handle(Thread::current(), key.holder()), key.offset(), key.inline_klass(), key.layout_kind());\n+        _entries.append(entry);\n+\n+        return _entries.length() < _batch_size;\n+      }\n+    } importer(_entries, _batch_size);\n+    table->entry_iterate(&importer);\n+\n+    return !_entries.is_empty();\n+  }\n+\n+  void convert() {\n+    for (int i = 0; i < _entries.length(); i++) {\n+      EXCEPTION_MARK;\n+      Entry& entry = _entries.at(i);\n+      oop obj = entry.inline_klass->read_payload_from_addr(entry.holder(), entry.offset, entry.layout_kind, JavaThread::current());\n+\n+      if (HAS_PENDING_EXCEPTION) {\n+        tty->print_cr(\"Exception in JvmtiTagMapFlatEntryConverter: \");\n+        java_lang_Throwable::print(PENDING_EXCEPTION, tty);\n+        tty->cr();\n+        CLEAR_PENDING_EXCEPTION;\n+        \/\/ stop the conversion\n+        _has_error = true;\n+      } else {\n+        entry.dst = Handle(Thread::current(), obj);\n+      }\n+    }\n+  }\n+\n+  \/\/ returns number of converted entries\n+  int move(JvmtiFlatTagMapTable* src_table, JvmtiTagMapTable* dst_table) {\n+    int count = 0;\n+    for (int i = 0; i < _entries.length(); i++) {\n+      Entry& entry = _entries.at(i);\n+      if (entry.dst() == nullptr) {\n+        \/\/ some error during conversion, skip the entry\n+        continue;\n+      }\n+      JvmtiHeapwalkObject obj(entry.holder(), entry.offset, entry.inline_klass, entry.layout_kind);\n+      jlong tag = src_table->remove(obj);\n+\n+      if (tag != 0) { \/\/ ensure the entry is still in the src_table\n+        dst_table->add(entry.dst(), tag);\n+        count++;\n+      } else {\n+\n+      }\n+    }\n+    \/\/ and clean the array\n+    _entries.clear();\n+    return count;\n+  }\n+};\n+\n+\n+void JvmtiTagMap::convert_flat_object_entries() {\n+  Thread* current = Thread::current();\n+  assert(current->is_Java_thread(), \"must be executed on JavaThread\");\n+\n+  log_debug(jvmti, table)(\"convert_flat_object_entries, main table size = %d, flat table size = %d\",\n+                          _hashmap->number_of_entries(), _flat_hashmap->number_of_entries());\n+\n+  {\n+    MonitorLocker ml(lock(), Mutex::_no_safepoint_check_flag);\n+    \/\/ If another thread is converting, let it finish.\n+    while (_converting_flat_object) {\n+      ml.wait();\n+    }\n+    if (_flat_hashmap->is_empty()) {\n+      \/\/ nothing to convert\n+      return;\n+    }\n+    _converting_flat_object = true;\n+  }\n+\n+  const int BATCH_SIZE = 1024;\n+  JvmtiTagMapFlatEntryConverter converter(BATCH_SIZE);\n+\n+  int count = 0;\n+  while (true) {\n+    HandleMark hm(current);\n+    {\n+      MonitorLocker ml(lock(), Mutex::_no_safepoint_check_flag);\n+      if (!converter.import_entries(_flat_hashmap)) {\n+        break;\n+      }\n+    }\n+    \/\/ Convert flat objects to heap-allocated without table lock (so agent callbacks can get\/set tags).\n+    converter.convert();\n+    {\n+      MonitorLocker ml(lock(), Mutex::_no_safepoint_check_flag);\n+      count += converter.move(_flat_hashmap, _hashmap);\n+    }\n+  }\n+\n+  log_info(jvmti, table)(\"%d flat value objects are converted, flat table size = %d\",\n+                         count, _flat_hashmap->number_of_entries());\n+  {\n+    MonitorLocker ml(lock(), Mutex::_no_safepoint_check_flag);\n+    _converting_flat_object = false;\n+    ml.notify_all();\n+  }\n+}\n+\n+jlong JvmtiTagMap::find(const JvmtiHeapwalkObject& obj) const {\n+  jlong tag = _hashmap->find(obj);\n+  if (tag == 0 && obj.is_value()) {\n+    tag = _flat_hashmap->find(obj);\n+  }\n+  return tag;\n+}\n+\n+void JvmtiTagMap::add(const JvmtiHeapwalkObject& obj, jlong tag) {\n+  if (obj.is_flat()) {\n+    \/\/ we may have tag for equal (non-flat) object in _hashmap, try to update it 1st\n+    if (!_hashmap->update(obj, tag)) {\n+      \/\/ no entry in _hashmap, add to _flat_hashmap\n+      _flat_hashmap->add(obj, tag);\n+    }\n+  } else {\n+    _hashmap->add(obj, tag);\n+  }\n@@ -177,0 +402,9 @@\n+void JvmtiTagMap::remove(const JvmtiHeapwalkObject& obj) {\n+  if (!_hashmap->remove(obj)) {\n+    if (obj.is_value()) {\n+      _flat_hashmap->remove(obj);\n+    }\n+  }\n+}\n+\n+\n@@ -195,2 +429,1 @@\n-  JvmtiTagMapTable* _hashmap;\n-  oop _o;\n+  const JvmtiHeapwalkObject& _o;\n@@ -205,2 +438,2 @@\n-  void inline post_callback_tag_update(oop o, JvmtiTagMapTable* hashmap,\n-                                       jlong obj_tag);\n+  void inline post_callback_tag_update(const JvmtiHeapwalkObject& o, JvmtiTagMap* tag_map, jlong obj_tag);\n+\n@@ -208,1 +441,3 @@\n-  CallbackWrapper(JvmtiTagMap* tag_map, oop o) {\n+  CallbackWrapper(JvmtiTagMap* tag_map, const JvmtiHeapwalkObject& o)\n+    : _tag_map(tag_map), _o(o)\n+  {\n@@ -212,8 +447,8 @@\n-    \/\/ object to tag\n-    _o = o;\n-\n-    _obj_size = (jlong)_o->size() * wordSize;\n-\n-    \/\/ record the context\n-    _tag_map = tag_map;\n-    _hashmap = tag_map->hashmap();\n+    if (!o.is_flat()) {\n+      \/\/ common case: we have oop\n+      _obj_size = (jlong)o.obj()->size() * wordSize;\n+    } else {\n+      \/\/ flat value object, we know its InstanceKlass\n+      assert(_o.inline_klass() != nullptr, \"must be\");\n+      _obj_size = _o.inline_klass()->size() * wordSize;;\n+    }\n@@ -223,1 +458,1 @@\n-    _obj_tag = _hashmap->find(_o);\n+    _obj_tag = _tag_map->find(_o);\n@@ -228,1 +463,1 @@\n-    _klass_tag = tag_for(tag_map, _o->klass()->java_mirror());\n+    _klass_tag = _tag_map->find(_o.klass()->java_mirror());\n@@ -232,1 +467,1 @@\n-    post_callback_tag_update(_o, _hashmap, _obj_tag);\n+    post_callback_tag_update(_o, _tag_map, _obj_tag);\n@@ -242,2 +477,2 @@\n-void inline CallbackWrapper::post_callback_tag_update(oop o,\n-                                                      JvmtiTagMapTable* hashmap,\n+void inline CallbackWrapper::post_callback_tag_update(const JvmtiHeapwalkObject& o,\n+                                                      JvmtiTagMap* tag_map,\n@@ -247,1 +482,1 @@\n-    hashmap->remove(o);\n+    tag_map->remove(o);\n@@ -252,1 +487,1 @@\n-    hashmap->add(o, obj_tag);\n+    tag_map->add(o, obj_tag);\n@@ -274,0 +509,1 @@\n+  const JvmtiHeapwalkObject& _referrer;\n@@ -275,2 +511,0 @@\n-  JvmtiTagMapTable* _referrer_hashmap;\n-  oop _referrer;\n@@ -284,2 +518,2 @@\n-  TwoOopCallbackWrapper(JvmtiTagMap* tag_map, oop referrer, oop o) :\n-    CallbackWrapper(tag_map, o)\n+  TwoOopCallbackWrapper(JvmtiTagMap* tag_map, const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& o) :\n+    CallbackWrapper(tag_map, o), _referrer(referrer)\n@@ -294,5 +528,1 @@\n-      _referrer = referrer;\n-      \/\/ record the context\n-      _referrer_hashmap = tag_map->hashmap();\n-\n-      _referrer_obj_tag = _referrer_hashmap->find(_referrer);\n+      _referrer_obj_tag = tag_map->find(_referrer);\n@@ -304,1 +534,1 @@\n-      _referrer_klass_tag = tag_for(tag_map, _referrer->klass()->java_mirror());\n+      _referrer_klass_tag = tag_map->find(_referrer.klass()->java_mirror());\n@@ -311,1 +541,1 @@\n-                               _referrer_hashmap,\n+                               tag_map(),\n@@ -339,3 +569,1 @@\n-\n-  JvmtiTagMapTable* hashmap = _hashmap;\n-\n+  JvmtiHeapwalkObject obj(o);\n@@ -345,1 +573,1 @@\n-    hashmap->remove(o);\n+    _hashmap->remove(obj);\n@@ -349,1 +577,1 @@\n-    hashmap->add(o, tag);\n+    add(obj, tag);\n@@ -365,1 +593,1 @@\n-  return tag_for(this, o);\n+  return find(o);\n@@ -378,0 +606,2 @@\n+  InlineKlass* _inline_klass; \/\/ nullptr for heap object\n+  LayoutKind _layout_kind;\n@@ -379,2 +609,12 @@\n-  ClassFieldDescriptor(int index, char type, int offset) :\n-    _field_index(index), _field_offset(offset), _field_type(type) {\n+  ClassFieldDescriptor(int index, const FieldStreamBase& fld) :\n+      _field_index(index), _field_offset(fld.offset()), _field_type(fld.signature()->char_at(0)) {\n+    if (fld.is_flat()) {\n+      const fieldDescriptor& fd = fld.field_descriptor();\n+      InstanceKlass* holder_klass = fd.field_holder();\n+      InlineLayoutInfo* layout_info = holder_klass->inline_layout_info_adr(fd.index());\n+      _inline_klass = layout_info->klass();\n+      _layout_kind = layout_info->kind();\n+    } else {\n+      _inline_klass = nullptr;\n+      _layout_kind = LayoutKind::REFERENCE;\n+    }\n@@ -385,0 +625,3 @@\n+  bool is_flat()     const  { return _inline_klass != nullptr; }\n+  InlineKlass* inline_klass() const { return _inline_klass; }\n+  LayoutKind layout_kind() const { return _layout_kind; }\n@@ -403,1 +646,1 @@\n-  void add(int index, char type, int offset);\n+  void add(int index, const FieldStreamBase& fld);\n@@ -414,1 +657,1 @@\n-  static ClassFieldMap* create_map_of_instance_fields(oop obj);\n+  static ClassFieldMap* create_map_of_instance_fields(Klass* k);\n@@ -439,2 +682,2 @@\n-void ClassFieldMap::add(int index, char type, int offset) {\n-  ClassFieldDescriptor* field = new ClassFieldDescriptor(index, type, offset);\n+void ClassFieldMap::add(int index, const FieldStreamBase& fld) {\n+  ClassFieldDescriptor* field = new ClassFieldDescriptor(index, fld);\n@@ -464,1 +707,1 @@\n-    field_map->add(index, fld.signature()->char_at(0), fld.offset());\n+    field_map->add(index, fld);\n@@ -473,2 +716,2 @@\n-ClassFieldMap* ClassFieldMap::create_map_of_instance_fields(oop obj) {\n-  InstanceKlass* ik = InstanceKlass::cast(obj->klass());\n+ClassFieldMap* ClassFieldMap::create_map_of_instance_fields(Klass* k) {\n+  InstanceKlass* ik = InstanceKlass::cast(k);\n@@ -493,1 +736,1 @@\n-      field_map->add(start_index + index, fld.signature()->char_at(0), fld.offset());\n+      field_map->add(start_index + index, fld);\n@@ -523,1 +766,1 @@\n-  \/\/ returns the field map for a given object (returning map cached\n+  \/\/ returns the field map for a given klass (returning map cached\n@@ -525,1 +768,1 @@\n-  static ClassFieldMap* get_map_of_instance_fields(oop obj);\n+  static ClassFieldMap* get_map_of_instance_fields(Klass* k);\n@@ -577,1 +820,1 @@\n-\/\/ returns the instance field map for the given object\n+\/\/ returns the instance field map for the given klass\n@@ -579,1 +822,1 @@\n-ClassFieldMap* JvmtiCachedClassFieldMap::get_map_of_instance_fields(oop obj) {\n+ClassFieldMap* JvmtiCachedClassFieldMap::get_map_of_instance_fields(Klass *k) {\n@@ -583,1 +826,0 @@\n-  Klass* k = obj->klass();\n@@ -592,1 +834,1 @@\n-    ClassFieldMap* field_map = ClassFieldMap::create_map_of_instance_fields(obj);\n+    ClassFieldMap* field_map = ClassFieldMap::create_map_of_instance_fields(k);\n@@ -644,1 +886,1 @@\n-static inline bool is_filtered_by_klass_filter(oop obj, Klass* klass_filter) {\n+static inline bool is_filtered_by_klass_filter(const JvmtiHeapwalkObject& obj, Klass* klass_filter) {\n@@ -646,1 +888,1 @@\n-    if (obj->klass() != klass_filter) {\n+    if (obj.klass() != klass_filter) {\n@@ -677,1 +919,1 @@\n-                                         oop str,\n+                                         const JvmtiHeapwalkObject& obj,\n@@ -680,0 +922,2 @@\n+  assert(!obj.is_flat(), \"cannot be flat\");\n+  oop str = obj.obj();\n@@ -728,1 +972,1 @@\n-                                                  oop obj,\n+                                                  const JvmtiHeapwalkObject& obj,\n@@ -731,1 +975,2 @@\n-  assert(obj->is_typeArray(), \"not a primitive array\");\n+  assert(!obj.is_flat(), \"cannot be flat\");\n+  assert(obj.obj()->is_typeArray(), \"not a primitive array\");\n@@ -734,1 +979,1 @@\n-  typeArrayOop array = typeArrayOop(obj);\n+  typeArrayOop array = typeArrayOop(obj.obj());\n@@ -824,1 +1069,1 @@\n-  oop obj,\n+  const JvmtiHeapwalkObject& obj,\n@@ -832,1 +1077,1 @@\n-  ClassFieldMap* fields = JvmtiCachedClassFieldMap::get_map_of_instance_fields(obj);\n+  ClassFieldMap* fields = JvmtiCachedClassFieldMap::get_map_of_instance_fields(obj.klass());\n@@ -846,3 +1091,2 @@\n-    \/\/ get offset and field value\n-    int offset = field->field_offset();\n-    address addr = cast_from_oop<address>(obj) + offset;\n+    \/\/ get field value\n+    address addr = cast_from_oop<address>(obj.obj()) + obj.offset() + field->field_offset();\n@@ -962,1 +1206,2 @@\n-  CallbackWrapper wrapper(tag_map(), o);\n+  JvmtiHeapwalkObject wrapper_obj(o);\n+  CallbackWrapper wrapper(tag_map(), wrapper_obj);\n@@ -1014,0 +1259,4 @@\n+  void visit_object(const JvmtiHeapwalkObject& obj);\n+  void visit_flat_fields(const JvmtiHeapwalkObject& obj);\n+  void visit_flat_array_elements(const JvmtiHeapwalkObject& obj);\n+\n@@ -1029,1 +1278,1 @@\n-  void do_object(oop o);\n+  void do_object(oop obj);\n@@ -1038,4 +1287,1 @@\n-  \/\/ apply class filter\n-  if (is_filtered_by_klass_filter(obj, klass())) return;\n-\n-  if (obj->klass()->java_mirror() == nullptr) {\n+  if (obj != nullptr && obj->klass()->java_mirror() == nullptr) {\n@@ -1048,0 +1294,7 @@\n+  visit_object(obj);\n+}\n+\n+void IterateThroughHeapObjectClosure::visit_object(const JvmtiHeapwalkObject& obj) {\n+  \/\/ apply class filter\n+  if (is_filtered_by_klass_filter(obj, klass())) return;\n+\n@@ -1057,2 +1310,2 @@\n-  bool is_array = obj->is_array();\n-  int len = is_array ? arrayOop(obj)->length() : -1;\n+  bool is_array = obj.klass()->is_array_klass();\n+  int len = is_array ? arrayOop(obj.obj())->length() : -1;\n@@ -1072,1 +1325,1 @@\n-  if (callbacks()->primitive_field_callback != nullptr && obj->is_instance()) {\n+  if (callbacks()->primitive_field_callback != nullptr && obj.klass()->is_instance_klass()) {\n@@ -1075,1 +1328,2 @@\n-    if (obj->klass() == vmClasses::Class_klass()) {\n+    if (obj.klass() == vmClasses::Class_klass()) {\n+      assert(!obj.is_flat(), \"Class object cannot be flattened\");\n@@ -1077,3 +1331,3 @@\n-                                                                    obj,\n-                                                                    cb,\n-                                                                    (void*)user_data());\n+                                                              obj.obj(),\n+                                                              cb,\n+                                                              (void*)user_data());\n@@ -1082,3 +1336,3 @@\n-                                                                      obj,\n-                                                                      cb,\n-                                                                      (void*)user_data());\n+                                                                obj,\n+                                                                cb,\n+                                                                (void*)user_data());\n@@ -1092,1 +1346,1 @@\n-      obj->klass() == vmClasses::String_klass()) {\n+      obj.klass() == vmClasses::String_klass()) {\n@@ -1097,1 +1351,1 @@\n-                (void*)user_data() );\n+                (void*)user_data());\n@@ -1104,1 +1358,1 @@\n-      obj->is_typeArray()) {\n+      obj.klass()->is_typeArray_klass()) {\n@@ -1109,1 +1363,1 @@\n-               (void*)user_data() );\n+               (void*)user_data());\n@@ -1112,1 +1366,80 @@\n-};\n+  \/\/ All info for the object is reported.\n+\n+  \/\/ If the object has flat fields, report them as heap objects.\n+  if (obj.klass()->is_instance_klass()) {\n+    if (InstanceKlass::cast(obj.klass())->has_inline_type_fields()) {\n+      visit_flat_fields(obj);\n+      \/\/ check if iteration has been halted\n+      if (is_iteration_aborted()) {\n+        return;\n+      }\n+    }\n+  }\n+  \/\/ If the object is flat array, report all elements as heap objects.\n+  if (is_array && obj.obj()->is_flatArray()) {\n+    assert(!obj.is_flat(), \"Array object cannot be flattened\");\n+    visit_flat_array_elements(obj);\n+  }\n+}\n+\n+void IterateThroughHeapObjectClosure::visit_flat_fields(const JvmtiHeapwalkObject& obj) {\n+  \/\/ iterate over instance fields\n+  ClassFieldMap* fields = JvmtiCachedClassFieldMap::get_map_of_instance_fields(obj.klass());\n+  for (int i = 0; i < fields->field_count(); i++) {\n+    ClassFieldDescriptor* field = fields->field_at(i);\n+    \/\/ skip non-flat and (for safety) primitive fields\n+    if (!field->is_flat() || is_primitive_field_type(field->field_type())) {\n+      continue;\n+    }\n+\n+    int field_offset = field->field_offset();\n+    if (obj.is_flat()) {\n+      \/\/ the object is inlined, its fields are stored without the header\n+      field_offset += obj.offset() - obj.inline_klass()->payload_offset();\n+    }\n+    \/\/ check for possible nulls\n+    if (LayoutKindHelper::is_nullable_flat(field->layout_kind())) {\n+      address payload = cast_from_oop<address>(obj.obj()) + field_offset;\n+      if (field->inline_klass()->is_payload_marked_as_null(payload)) {\n+        continue;\n+      }\n+    }\n+    JvmtiHeapwalkObject field_obj(obj.obj(), field_offset, field->inline_klass(), field->layout_kind());\n+\n+    visit_object(field_obj);\n+\n+    \/\/ check if iteration has been halted\n+    if (is_iteration_aborted()) {\n+      return;\n+    }\n+  }\n+}\n+\n+void IterateThroughHeapObjectClosure::visit_flat_array_elements(const JvmtiHeapwalkObject& obj) {\n+  assert(!obj.is_flat() && obj.obj()->is_flatArray() , \"sanity check\");\n+  flatArrayOop array = flatArrayOop(obj.obj());\n+  FlatArrayKlass* faklass = FlatArrayKlass::cast(array->klass());\n+  InlineKlass* vk = InlineKlass::cast(faklass->element_klass());\n+  bool need_null_check = LayoutKindHelper::is_nullable_flat(faklass->layout_kind());\n+\n+  for (int index = 0; index < array->length(); index++) {\n+    address addr = (address)array->value_at_addr(index, faklass->layout_helper());\n+    \/\/ check for null\n+    if (need_null_check) {\n+      if (vk->is_payload_marked_as_null(addr)) {\n+        continue;\n+      }\n+    }\n+\n+    \/\/ offset in the array oop\n+    int offset = (int)(addr - cast_from_oop<address>(array));\n+    JvmtiHeapwalkObject elem(obj.obj(), offset, vk, faklass->layout_kind());\n+\n+    visit_object(elem);\n+\n+    \/\/ check if iteration has been halted\n+    if (is_iteration_aborted()) {\n+      return;\n+    }\n+  }\n+}\n@@ -1138,0 +1471,2 @@\n+  convert_flat_object_entries();\n+\n@@ -1165,0 +1500,2 @@\n+  convert_flat_object_entries();\n+\n@@ -1178,1 +1515,1 @@\n-    hashmap()->remove_dead_entries(objects);\n+    _hashmap->remove_dead_entries(objects);\n@@ -1333,0 +1670,3 @@\n+  \/\/ ensure flat object conversion is completed\n+  convert_flat_object_entries();\n+\n@@ -1341,1 +1681,1 @@\n-    entry_iterate(&collector);\n+    _hashmap->entry_iterate(&collector);\n@@ -1381,1 +1721,1 @@\n-  oop _last_referrer;\n+  JvmtiHeapwalkObject _last_referrer;\n@@ -1394,1 +1734,1 @@\n-    _last_referrer(nullptr),\n+    _last_referrer(),\n@@ -1403,2 +1743,2 @@\n-  oop last_referrer() const               { return _last_referrer; }\n-  void set_last_referrer(oop referrer)    { _last_referrer = referrer; }\n+  JvmtiHeapwalkObject last_referrer() const    { return _last_referrer; }\n+  void set_last_referrer(const JvmtiHeapwalkObject& referrer) { _last_referrer = referrer; }\n@@ -1477,2 +1817,1 @@\n-  static GrowableArray<oop>* _visit_stack;\n-  static JVMTIBitSet* _bitset;\n+  static JvmtiHeapwalkVisitStack* _visit_stack;\n@@ -1483,1 +1822,1 @@\n-  static GrowableArray<oop>* visit_stack()             { return _visit_stack; }\n+  static JvmtiHeapwalkVisitStack* visit_stack()        { return _visit_stack; }\n@@ -1487,2 +1826,2 @@\n-  static inline bool check_for_visit(oop obj) {\n-    if (!_bitset->is_marked(obj)) visit_stack()->push(obj);\n+  static inline bool check_for_visit(const JvmtiHeapwalkObject&obj) {\n+    visit_stack()->check_for_visit(obj);\n@@ -1492,0 +1831,10 @@\n+  \/\/ return element count if the obj is array, -1 otherwise\n+  static jint get_array_length(const JvmtiHeapwalkObject& obj) {\n+    if (!obj.klass()->is_array_klass()) {\n+      return -1;\n+    }\n+    assert(!obj.is_flat(), \"array cannot be flat\");\n+    return (jint)arrayOop(obj.obj())->length();\n+  }\n+\n+\n@@ -1494,1 +1843,1 @@\n-    (jvmtiHeapRootKind root_kind, oop obj);\n+    (jvmtiHeapRootKind root_kind, const JvmtiHeapwalkObject& obj);\n@@ -1497,1 +1846,1 @@\n-     int slot, oop obj);\n+     int slot, const JvmtiHeapwalkObject& obj);\n@@ -1499,1 +1848,1 @@\n-    (jvmtiObjectReferenceKind ref_kind, oop referrer, oop referree, jint index);\n+    (jvmtiObjectReferenceKind ref_kind, const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint index);\n@@ -1503,1 +1852,1 @@\n-    (jvmtiHeapReferenceKind ref_kind, oop obj);\n+    (jvmtiHeapReferenceKind ref_kind, const JvmtiHeapwalkObject& obj);\n@@ -1506,1 +1855,1 @@\n-     jmethodID method, jlocation bci, jint slot, oop obj);\n+     jmethodID method, jlocation bci, jint slot, const JvmtiHeapwalkObject& obj);\n@@ -1508,1 +1857,1 @@\n-    (jvmtiHeapReferenceKind ref_kind, oop referrer, oop referree, jint index);\n+    (jvmtiHeapReferenceKind ref_kind, const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint index);\n@@ -1512,1 +1861,1 @@\n-    (jvmtiHeapReferenceKind ref_kind, oop obj, jint index, address addr, char type);\n+    (jvmtiHeapReferenceKind ref_kind, const JvmtiHeapwalkObject& obj, jint index, address addr, char type);\n@@ -1517,1 +1866,0 @@\n-                                             GrowableArray<oop>* visit_stack,\n@@ -1520,1 +1868,1 @@\n-                                             JVMTIBitSet* bitset);\n+                                             JvmtiHeapwalkVisitStack* visit_stack);\n@@ -1524,1 +1872,0 @@\n-                                                GrowableArray<oop>* visit_stack,\n@@ -1527,1 +1874,1 @@\n-                                                JVMTIBitSet* bitset);\n+                                                JvmtiHeapwalkVisitStack* visit_stack);\n@@ -1530,1 +1877,1 @@\n-  static inline bool report_simple_root(jvmtiHeapReferenceKind kind, oop o);\n+  static inline bool report_simple_root(jvmtiHeapReferenceKind kind, const JvmtiHeapwalkObject& o);\n@@ -1532,1 +1879,1 @@\n-    jmethodID m, oop o);\n+    jmethodID m, const JvmtiHeapwalkObject& o);\n@@ -1534,1 +1881,1 @@\n-    jmethodID method, jlocation bci, jint slot, oop o);\n+    jmethodID method, jlocation bci, jint slot, const JvmtiHeapwalkObject& o);\n@@ -1537,14 +1884,14 @@\n-  static inline bool report_array_element_reference(oop referrer, oop referree, jint index);\n-  static inline bool report_class_reference(oop referrer, oop referree);\n-  static inline bool report_class_loader_reference(oop referrer, oop referree);\n-  static inline bool report_signers_reference(oop referrer, oop referree);\n-  static inline bool report_protection_domain_reference(oop referrer, oop referree);\n-  static inline bool report_superclass_reference(oop referrer, oop referree);\n-  static inline bool report_interface_reference(oop referrer, oop referree);\n-  static inline bool report_static_field_reference(oop referrer, oop referree, jint slot);\n-  static inline bool report_field_reference(oop referrer, oop referree, jint slot);\n-  static inline bool report_constant_pool_reference(oop referrer, oop referree, jint index);\n-  static inline bool report_primitive_array_values(oop array);\n-  static inline bool report_string_value(oop str);\n-  static inline bool report_primitive_instance_field(oop o, jint index, address value, char type);\n-  static inline bool report_primitive_static_field(oop o, jint index, address value, char type);\n+  static inline bool report_array_element_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint index);\n+  static inline bool report_class_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree);\n+  static inline bool report_class_loader_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree);\n+  static inline bool report_signers_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree);\n+  static inline bool report_protection_domain_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree);\n+  static inline bool report_superclass_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree);\n+  static inline bool report_interface_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree);\n+  static inline bool report_static_field_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint slot);\n+  static inline bool report_field_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint slot);\n+  static inline bool report_constant_pool_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint index);\n+  static inline bool report_primitive_array_values(const JvmtiHeapwalkObject& array);\n+  static inline bool report_string_value(const JvmtiHeapwalkObject& str);\n+  static inline bool report_primitive_instance_field(const JvmtiHeapwalkObject& o, jint index, address value, char type);\n+  static inline bool report_primitive_static_field(const JvmtiHeapwalkObject& o, jint index, address value, char type);\n@@ -1559,2 +1906,1 @@\n-GrowableArray<oop>* CallbackInvoker::_visit_stack;\n-JVMTIBitSet* CallbackInvoker::_bitset;\n+JvmtiHeapwalkVisitStack* CallbackInvoker::_visit_stack;\n@@ -1564,1 +1910,0 @@\n-                                                     GrowableArray<oop>* visit_stack,\n@@ -1567,1 +1912,1 @@\n-                                                     JVMTIBitSet* bitset) {\n+                                                     JvmtiHeapwalkVisitStack* visit_stack) {\n@@ -1569,1 +1914,0 @@\n-  _visit_stack = visit_stack;\n@@ -1574,1 +1918,1 @@\n-  _bitset = bitset;\n+  _visit_stack = visit_stack;\n@@ -1579,1 +1923,0 @@\n-                                                        GrowableArray<oop>* visit_stack,\n@@ -1582,1 +1925,1 @@\n-                                                        JVMTIBitSet* bitset) {\n+                                                        JvmtiHeapwalkVisitStack* visit_stack) {\n@@ -1584,1 +1927,0 @@\n-  _visit_stack = visit_stack;\n@@ -1589,1 +1931,1 @@\n-  _bitset = bitset;\n+  _visit_stack = visit_stack;\n@@ -1594,1 +1936,1 @@\n-inline bool CallbackInvoker::invoke_basic_heap_root_callback(jvmtiHeapRootKind root_kind, oop obj) {\n+inline bool CallbackInvoker::invoke_basic_heap_root_callback(jvmtiHeapRootKind root_kind, const JvmtiHeapwalkObject& obj) {\n@@ -1621,1 +1963,1 @@\n-                                                             oop obj) {\n+                                                             const JvmtiHeapwalkObject& obj) {\n@@ -1648,2 +1990,2 @@\n-                                                                    oop referrer,\n-                                                                    oop referree,\n+                                                                    const JvmtiHeapwalkObject& referrer,\n+                                                                    const JvmtiHeapwalkObject& referree,\n@@ -1660,1 +2002,1 @@\n-    referrer_tag = tag_for(tag_map(), referrer);\n+    referrer_tag = tag_map()->find(referrer);\n@@ -1692,1 +2034,1 @@\n-                                                                oop obj) {\n+                                                                const JvmtiHeapwalkObject& obj) {\n@@ -1717,1 +2059,1 @@\n-  jint len = (jint)(obj->is_array() ? arrayOop(obj)->length() : -1);\n+  jint len = get_array_length(obj);\n@@ -1746,1 +2088,1 @@\n-                                                                oop obj) {\n+                                                                const JvmtiHeapwalkObject& obj) {\n@@ -1780,1 +2122,1 @@\n-  jint len = (jint)(obj->is_array() ? arrayOop(obj)->length() : -1);\n+  jint len = get_array_length(obj);\n@@ -1813,2 +2155,2 @@\n-                                                                       oop referrer,\n-                                                                       oop obj,\n+                                                                       const JvmtiHeapwalkObject& referrer,\n+                                                                       const JvmtiHeapwalkObject& obj,\n@@ -1847,1 +2189,1 @@\n-  jint len = (jint)(obj->is_array() ? arrayOop(obj)->length() : -1);\n+  jint len = get_array_length(obj);\n@@ -1870,1 +2212,1 @@\n-inline bool CallbackInvoker::report_simple_root(jvmtiHeapReferenceKind kind, oop obj) {\n+inline bool CallbackInvoker::report_simple_root(jvmtiHeapReferenceKind kind, const JvmtiHeapwalkObject& obj) {\n@@ -1886,2 +2228,2 @@\n-inline bool CallbackInvoker::report_primitive_array_values(oop obj) {\n-  assert(obj->is_typeArray(), \"not a primitive array\");\n+inline bool CallbackInvoker::report_primitive_array_values(const JvmtiHeapwalkObject& obj) {\n+  assert(obj.klass()->is_typeArray_klass(), \"not a primitive array\");\n@@ -1915,2 +2257,2 @@\n-inline bool CallbackInvoker::report_string_value(oop str) {\n-  assert(str->klass() == vmClasses::String_klass(), \"not a string\");\n+inline bool CallbackInvoker::report_string_value(const JvmtiHeapwalkObject& str) {\n+  assert(str.klass() == vmClasses::String_klass(), \"not a string\");\n@@ -1945,1 +2287,1 @@\n-                                                    oop obj,\n+                                                    const JvmtiHeapwalkObject& obj,\n@@ -1993,1 +2335,1 @@\n-inline bool CallbackInvoker::report_primitive_instance_field(oop obj,\n+inline bool CallbackInvoker::report_primitive_instance_field(const JvmtiHeapwalkObject& obj,\n@@ -2005,1 +2347,1 @@\n-inline bool CallbackInvoker::report_primitive_static_field(oop obj,\n+inline bool CallbackInvoker::report_primitive_static_field(const JvmtiHeapwalkObject& obj,\n@@ -2017,1 +2359,1 @@\n-inline bool CallbackInvoker::report_jni_local_root(jlong thread_tag, jlong tid, jint depth, jmethodID m, oop obj) {\n+inline bool CallbackInvoker::report_jni_local_root(jlong thread_tag, jlong tid, jint depth, jmethodID m, const JvmtiHeapwalkObject& obj) {\n@@ -2044,1 +2386,1 @@\n-                                                   oop obj) {\n+                                                   const JvmtiHeapwalkObject& obj) {\n@@ -2065,1 +2407,1 @@\n-inline bool CallbackInvoker::report_class_reference(oop referrer, oop referree) {\n+inline bool CallbackInvoker::report_class_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree) {\n@@ -2074,1 +2416,1 @@\n-inline bool CallbackInvoker::report_class_loader_reference(oop referrer, oop referree) {\n+inline bool CallbackInvoker::report_class_loader_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree) {\n@@ -2083,1 +2425,1 @@\n-inline bool CallbackInvoker::report_signers_reference(oop referrer, oop referree) {\n+inline bool CallbackInvoker::report_signers_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree) {\n@@ -2092,1 +2434,1 @@\n-inline bool CallbackInvoker::report_protection_domain_reference(oop referrer, oop referree) {\n+inline bool CallbackInvoker::report_protection_domain_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree) {\n@@ -2101,1 +2443,1 @@\n-inline bool CallbackInvoker::report_superclass_reference(oop referrer, oop referree) {\n+inline bool CallbackInvoker::report_superclass_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree) {\n@@ -2111,1 +2453,1 @@\n-inline bool CallbackInvoker::report_interface_reference(oop referrer, oop referree) {\n+inline bool CallbackInvoker::report_interface_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree) {\n@@ -2120,1 +2462,1 @@\n-inline bool CallbackInvoker::report_static_field_reference(oop referrer, oop referree, jint slot) {\n+inline bool CallbackInvoker::report_static_field_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint slot) {\n@@ -2129,1 +2471,1 @@\n-inline bool CallbackInvoker::report_array_element_reference(oop referrer, oop referree, jint index) {\n+inline bool CallbackInvoker::report_array_element_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint index) {\n@@ -2138,1 +2480,1 @@\n-inline bool CallbackInvoker::report_field_reference(oop referrer, oop referree, jint slot) {\n+inline bool CallbackInvoker::report_field_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint slot) {\n@@ -2147,1 +2489,1 @@\n-inline bool CallbackInvoker::report_constant_pool_reference(oop referrer, oop referree, jint index) {\n+inline bool CallbackInvoker::report_constant_pool_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint index) {\n@@ -2307,1 +2649,1 @@\n-  _thread_tag = tag_for(_tag_map, _threadObj);\n+  _thread_tag = _tag_map->find(_threadObj);\n@@ -2440,4 +2782,0 @@\n-  enum {\n-    initial_visit_stack_size = 4000\n-  };\n-\n@@ -2447,3 +2785,1 @@\n-  GrowableArray<oop>* _visit_stack;                 \/\/ the visit stack\n-\n-  JVMTIBitSet _bitset;\n+  JvmtiHeapwalkVisitStack _visit_stack;\n@@ -2460,4 +2796,0 @@\n-  GrowableArray<oop>* create_visit_stack() {\n-    return new (mtServiceability) GrowableArray<oop>(initial_visit_stack_size, mtServiceability);\n-  }\n-\n@@ -2475,1 +2807,1 @@\n-  GrowableArray<oop>* visit_stack() const          { return _visit_stack; }\n+  JvmtiHeapwalkVisitStack* visit_stack()           { return &_visit_stack; }\n@@ -2478,4 +2810,5 @@\n-  inline bool iterate_over_array(oop o);\n-  inline bool iterate_over_type_array(oop o);\n-  inline bool iterate_over_class(oop o);\n-  inline bool iterate_over_object(oop o);\n+  inline bool iterate_over_array(const JvmtiHeapwalkObject& o);\n+  inline bool iterate_over_flat_array(const JvmtiHeapwalkObject& o);\n+  inline bool iterate_over_type_array(const JvmtiHeapwalkObject& o);\n+  inline bool iterate_over_class(const JvmtiHeapwalkObject& o);\n+  inline bool iterate_over_object(const JvmtiHeapwalkObject& o);\n@@ -2490,1 +2823,1 @@\n-  inline bool visit(oop o);\n+  inline bool visit(const JvmtiHeapwalkObject& o);\n@@ -2524,3 +2857,1 @@\n-  _visit_stack = create_visit_stack();\n-\n-  CallbackInvoker::initialize_for_basic_heap_walk(tag_map, _visit_stack, user_data, callbacks, &_bitset);\n+  CallbackInvoker::initialize_for_basic_heap_walk(tag_map, user_data, callbacks, &_visit_stack);\n@@ -2542,2 +2873,1 @@\n-  _visit_stack = create_visit_stack();\n-  CallbackInvoker::initialize_for_advanced_heap_walk(tag_map, _visit_stack, user_data, callbacks, &_bitset);\n+  CallbackInvoker::initialize_for_advanced_heap_walk(tag_map, user_data, callbacks, &_visit_stack);\n@@ -2548,5 +2878,0 @@\n-  if (_following_object_refs) {\n-    assert(_visit_stack != nullptr, \"checking\");\n-    delete _visit_stack;\n-    _visit_stack = nullptr;\n-  }\n@@ -2557,2 +2882,3 @@\n-inline bool VM_HeapWalkOperation::iterate_over_array(oop o) {\n-  objArrayOop array = objArrayOop(o);\n+inline bool VM_HeapWalkOperation::iterate_over_array(const JvmtiHeapwalkObject& o) {\n+  assert(!o.is_flat(), \"Array object cannot be flattened\");\n+  objArrayOop array = objArrayOop(o.obj());\n@@ -2582,0 +2908,38 @@\n+\/\/ similar to iterate_over_array(), but itrates over flat array\n+inline bool VM_HeapWalkOperation::iterate_over_flat_array(const JvmtiHeapwalkObject& o) {\n+  assert(!o.is_flat(), \"Array object cannot be flattened\");\n+  flatArrayOop array = flatArrayOop(o.obj());\n+  FlatArrayKlass* faklass = FlatArrayKlass::cast(array->klass());\n+  InlineKlass* vk = InlineKlass::cast(faklass->element_klass());\n+  bool need_null_check = LayoutKindHelper::is_nullable_flat(faklass->layout_kind());\n+\n+  \/\/ array reference to its class\n+  oop mirror = faklass->java_mirror();\n+  if (!CallbackInvoker::report_class_reference(o, mirror)) {\n+    return false;\n+  }\n+\n+  \/\/ iterate over the array and report each reference to a\n+  \/\/ non-null element\n+  for (int index = 0; index < array->length(); index++) {\n+    address addr = (address)array->value_at_addr(index, faklass->layout_helper());\n+\n+    \/\/ check for null\n+    if (need_null_check) {\n+      if (vk->is_payload_marked_as_null(addr)) {\n+        continue;\n+      }\n+    }\n+\n+    \/\/ offset in the array oop\n+    int offset = (int)(addr - cast_from_oop<address>(array));\n+    JvmtiHeapwalkObject elem(o.obj(), offset, vk, faklass->layout_kind());\n+\n+    \/\/ report the array reference\n+    if (!CallbackInvoker::report_array_element_reference(o, elem, index)) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n@@ -2583,2 +2947,3 @@\n-inline bool VM_HeapWalkOperation::iterate_over_type_array(oop o) {\n-  Klass* k = o->klass();\n+inline bool VM_HeapWalkOperation::iterate_over_type_array(const JvmtiHeapwalkObject& o) {\n+  assert(!o.is_flat(), \"Array object cannot be flattened\");\n+  Klass* k = o.klass();\n@@ -2618,1 +2983,3 @@\n-inline bool VM_HeapWalkOperation::iterate_over_class(oop java_class) {\n+inline bool VM_HeapWalkOperation::iterate_over_class(const JvmtiHeapwalkObject& o) {\n+  assert(!o.is_flat(), \"Klass object cannot be flattened\");\n+  Klass* klass = java_lang_Class::as_Klass(o.obj());\n@@ -2620,1 +2987,0 @@\n-  Klass* klass = java_lang_Class::as_Klass(java_class);\n@@ -2631,1 +2997,2 @@\n-    oop mirror = klass->java_mirror();\n+    oop mirror_oop = klass->java_mirror();\n+    JvmtiHeapwalkObject mirror(mirror_oop);\n@@ -2720,2 +3087,2 @@\n-        oop fld_o = mirror->obj_field(field->field_offset());\n-        assert(verify_static_oop(ik, mirror, field->field_offset()), \"sanity check\");\n+        oop fld_o = mirror_oop->obj_field(field->field_offset());\n+        assert(verify_static_oop(ik, mirror_oop, field->field_offset()), \"sanity check\");\n@@ -2731,1 +3098,1 @@\n-           address addr = cast_from_oop<address>(mirror) + field->field_offset();\n+           address addr = cast_from_oop<address>(mirror_oop) + field->field_offset();\n@@ -2751,1 +3118,1 @@\n-inline bool VM_HeapWalkOperation::iterate_over_object(oop o) {\n+inline bool VM_HeapWalkOperation::iterate_over_object(const JvmtiHeapwalkObject& o) {\n@@ -2753,1 +3120,1 @@\n-  if (!CallbackInvoker::report_class_reference(o, o->klass()->java_mirror())) {\n+  if (!CallbackInvoker::report_class_reference(o, o.klass()->java_mirror())) {\n@@ -2758,1 +3125,1 @@\n-  ClassFieldMap* field_map = JvmtiCachedClassFieldMap::get_map_of_instance_fields(o);\n+  ClassFieldMap* field_map = JvmtiCachedClassFieldMap::get_map_of_instance_fields(o.klass());\n@@ -2762,0 +3129,6 @@\n+    int slot = field->field_index();\n+    int field_offset = field->field_offset();\n+    if (o.is_flat()) {\n+      \/\/ the object is inlined, its fields are stored without the header\n+      field_offset += o.offset() - o.inline_klass()->payload_offset();\n+    }\n@@ -2763,7 +3136,10 @@\n-      oop fld_o = o->obj_field_access<AS_NO_KEEPALIVE | ON_UNKNOWN_OOP_REF>(field->field_offset());\n-      \/\/ ignore any objects that aren't visible to profiler\n-      if (fld_o != nullptr) {\n-        assert(Universe::heap()->is_in(fld_o), \"unsafe code should not \"\n-               \"have references to Klass* anymore\");\n-        int slot = field->field_index();\n-        if (!CallbackInvoker::report_field_reference(o, fld_o, slot)) {\n+      if (field->is_flat()) {\n+        \/\/ check for possible nulls\n+        if (LayoutKindHelper::is_nullable_flat(field->layout_kind())) {\n+          address payload = cast_from_oop<address>(o.obj()) + field_offset;\n+          if (field->inline_klass()->is_payload_marked_as_null(payload)) {\n+            continue;\n+          }\n+        }\n+        JvmtiHeapwalkObject field_obj(o.obj(), field_offset, field->inline_klass(), field->layout_kind());\n+        if (!CallbackInvoker::report_field_reference(o, field_obj, slot)) {\n@@ -2772,0 +3148,9 @@\n+      } else {\n+        oop fld_o = o.obj()->obj_field_access<AS_NO_KEEPALIVE | ON_UNKNOWN_OOP_REF>(field_offset);\n+        \/\/ ignore any objects that aren't visible to profiler\n+        if (fld_o != nullptr) {\n+          assert(Universe::heap()->is_in(fld_o), \"unsafe code should not have references to Klass* anymore\");\n+          if (!CallbackInvoker::report_field_reference(o, fld_o, slot)) {\n+            return false;\n+          }\n+        }\n@@ -2776,2 +3161,1 @@\n-        address addr = cast_from_oop<address>(o) + field->field_offset();\n-        int slot = field->field_index();\n+        address addr = cast_from_oop<address>(o.obj()) + field_offset;\n@@ -2787,1 +3171,1 @@\n-      o->klass() == vmClasses::String_klass()) {\n+      o.klass() == vmClasses::String_klass()) {\n@@ -2856,1 +3240,1 @@\n-    blk->set_context(tag_for(_tag_map, threadObj), java_lang_Thread::thread_id(threadObj), 0, (jmethodID)nullptr);\n+    blk->set_context(_tag_map->find(threadObj), java_lang_Thread::thread_id(threadObj), 0, (jmethodID)nullptr);\n@@ -2956,1 +3340,1 @@\n-bool VM_HeapWalkOperation::visit(oop o) {\n+bool VM_HeapWalkOperation::visit(const JvmtiHeapwalkObject& o) {\n@@ -2958,2 +3342,2 @@\n-  assert(!_bitset.is_marked(o), \"can't visit same object more than once\");\n-  _bitset.mark_obj(o);\n+  assert(!visit_stack()->is_visited(o), \"can't visit same object more than once\");\n+  visit_stack()->mark_visited(o);\n@@ -2961,0 +3345,1 @@\n+  Klass* klass = o.klass();\n@@ -2962,3 +3347,4 @@\n-  if (o->is_instance()) {\n-    if (o->klass() == vmClasses::Class_klass()) {\n-      if (!java_lang_Class::is_primitive(o)) {\n+  if (klass->is_instance_klass()) {\n+    if (klass == vmClasses::Class_klass()) {\n+      assert(!o.is_flat(), \"Class object cannot be flattened\");\n+      if (!java_lang_Class::is_primitive(o.obj())) {\n@@ -2971,2 +3357,3 @@\n-      if (initial_object().is_null() && java_lang_VirtualThread::is_subclass(o->klass())) {\n-        if (!collect_vthread_stack_refs(o)) {\n+      if (initial_object().is_null() && java_lang_VirtualThread::is_subclass(klass)) {\n+        assert(!o.is_flat(), \"VirtualThread object cannot be flattened\");\n+        if (!collect_vthread_stack_refs(o.obj())) {\n@@ -2980,0 +3367,5 @@\n+  \/\/ flat object array\n+  if (klass->is_flatArray_klass()) {\n+      return iterate_over_flat_array(o);\n+  }\n+\n@@ -2981,1 +3373,1 @@\n-  if (o->is_objArray()) {\n+  if (klass->is_objArray_klass()) {\n@@ -2986,1 +3378,1 @@\n-  if (o->is_typeArray()) {\n+  if (klass->is_typeArray_klass()) {\n@@ -3018,2 +3410,2 @@\n-      oop o = visit_stack()->pop();\n-      if (!_bitset.is_marked(o)) {\n+      const JvmtiHeapwalkObject o = visit_stack()->pop();\n+      if (!visit_stack()->is_visited(o)) {\n@@ -3048,0 +3440,2 @@\n+  convert_flat_object_entries();\n+\n@@ -3070,0 +3464,2 @@\n+  convert_flat_object_entries();\n+\n@@ -3102,0 +3498,2 @@\n+  convert_flat_object_entries();\n+\n","filename":"src\/hotspot\/share\/prims\/jvmtiTagMap.cpp","additions":641,"deletions":243,"binary":false,"changes":884,"status":"modified"},{"patch":"@@ -79,0 +79,2 @@\n+#include <string.h>\n+\n@@ -1798,1 +1800,0 @@\n-static unsigned int patch_mod_count = 0;\n@@ -1806,1 +1807,1 @@\n-  if (!CDSConfig::check_vm_args_consistency(patch_mod_javabase, mode_flag_cmd_line)) {\n+  if (!CDSConfig::check_vm_args_consistency(mode_flag_cmd_line)) {\n@@ -1973,0 +1974,4 @@\n+  if (UseAltSubstitutabilityMethod) {\n+    no_shared_spaces(\"Alternate substitutability method doesn't work with CDS yet\");\n+  }\n+\n@@ -2055,1 +2060,1 @@\n-      add_patch_mod_prefix(module_name, module_equal + 1);\n+      add_patch_mod_prefix(module_name, module_equal + 1, false \/* no append *\/, false \/* no cds *\/);\n@@ -2057,3 +2062,0 @@\n-      if (!create_numbered_module_property(\"jdk.module.patch\", patch_mod_tail, patch_mod_count++)) {\n-        return JNI_ENOMEM;\n-      }\n@@ -2067,0 +2069,23 @@\n+\/\/ Finalize --patch-module args and --enable-preview related to value class module patches.\n+\/\/ Create all numbered properties passing module patches.\n+int Arguments::finalize_patch_module() {\n+  \/\/ Create numbered properties for each module that has been patched by --patch-module.\n+  \/\/ Format is \"jdk.module.patch.<n>=<module_name>=<path>\"\n+  if (_patch_mod_prefix != nullptr) {\n+    char * prop_value = AllocateHeap(JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, mtArguments);\n+    unsigned int patch_mod_count = 0;\n+\n+    for (GrowableArrayIterator<ModulePatchPath *> it = _patch_mod_prefix->begin();\n+            it != _patch_mod_prefix->end(); ++it) {\n+      jio_snprintf(prop_value, JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, \"%s=%s\",\n+                   (*it)->module_name(), (*it)->path_string());\n+      if (!create_numbered_module_property(\"jdk.module.patch\", prop_value, patch_mod_count++)) {\n+        FreeHeap(prop_value);\n+        return JNI_ENOMEM;\n+      }\n+    }\n+    FreeHeap(prop_value);\n+  }\n+  return JNI_OK;\n+}\n+\n@@ -2858,10 +2883,5 @@\n-void Arguments::add_patch_mod_prefix(const char* module_name, const char* path) {\n-  \/\/ For java.base check for duplicate --patch-module options being specified on the command line.\n-  \/\/ This check is only required for java.base, all other duplicate module specifications\n-  \/\/ will be checked during module system initialization.  The module system initialization\n-  \/\/ will throw an ExceptionInInitializerError if this situation occurs.\n-  if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n-    if (patch_mod_javabase) {\n-      vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n-    } else {\n-      patch_mod_javabase = true;\n+void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool allow_append, bool allow_cds) {\n+  if (!allow_cds) {\n+    CDSConfig::set_module_patching_disables_cds();\n+    if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+      CDSConfig::set_java_base_module_patching_disables_cds();\n@@ -2876,1 +2896,18 @@\n-  _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  \/\/ Scan patches for matching module\n+  int i = _patch_mod_prefix->find_if([&](ModulePatchPath* patch) {\n+    return (strcmp(module_name, patch->module_name()) == 0);\n+  });\n+  if (i == -1) {\n+    _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  } else {\n+    if (allow_append) {\n+      \/\/ append path to existing module entry\n+      _patch_mod_prefix->at(i)->append_path(path);\n+    } else {\n+      if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+        vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n+      } else {\n+        vm_exit_during_initialization(\"Cannot specify a module more than once to --patch-module\", module_name);\n+      }\n+    }\n+  }\n@@ -2989,1 +3026,4 @@\n-  if (!check_vm_args_consistency()) {\n+  ClassLoader::init_jimage(is_valhalla_enabled());\n+\n+  \/\/ finalize --module-patch.\n+  if (finalize_patch_module() != JNI_OK) {\n@@ -2993,0 +3033,3 @@\n+  if (!check_vm_args_consistency()) {\n+    return JNI_ERR;\n+  }\n@@ -3878,0 +3921,12 @@\n+  if (!is_valhalla_enabled() || (is_interpreter_only() && !CDSConfig::is_dumping_archive() && !UseSharedSpaces)) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported.\n+    \/\/ Also these aren't useful in -Xint. However, don't disable them when dumping or using\n+    \/\/ the CDS archive, as the values must match between dumptime and runtime.\n+    FLAG_SET_DEFAULT(InlineTypePassFieldsAsArgs, false);\n+    FLAG_SET_DEFAULT(InlineTypeReturnedAsFields, false);\n+  }\n+  if (!UseNonAtomicValueFlattening && !UseNullableValueFlattening && !UseAtomicValueFlattening) {\n+    \/\/ Flattening is disabled\n+    FLAG_SET_DEFAULT(UseArrayFlattening, false);\n+    FLAG_SET_DEFAULT(UseFieldFlattening, false);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":73,"deletions":18,"binary":false,"changes":91,"status":"modified"},{"patch":"@@ -93,0 +93,1 @@\n+  inline void append_path(const char* path) { _path->append_value(path); }\n@@ -480,1 +481,3 @@\n-  static void add_patch_mod_prefix(const char *module_name, const char *path);\n+  static void add_patch_mod_prefix(const char *module_name, const char *path, bool allow_append, bool allow_cds);\n+  static int finalize_patch_module();\n+\n@@ -511,0 +514,4 @@\n+  static bool is_valhalla_enabled() {\n+    \/\/ Valhalla is a feature opted-in by --enable-preview\n+    return enable_preview();\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.hpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -318,0 +318,16 @@\n+\/\/ These checks are required for wait, notify and exit to avoid inflating the monitor to\n+\/\/ find out this inline type object cannot be locked.\n+#define CHECK_THROW_NOSYNC_IMSE(obj)  \\\n+  if ((obj)->mark().is_inline_type()) {  \\\n+    JavaThread* THREAD = current;           \\\n+    ResourceMark rm(THREAD);                \\\n+    THROW_MSG(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \\\n+  }\n+\n+#define CHECK_THROW_NOSYNC_IMSE_0(obj)  \\\n+  if ((obj)->mark().is_inline_type()) {  \\\n+    JavaThread* THREAD = current;             \\\n+    ResourceMark rm(THREAD);                  \\\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \\\n+  }\n+\n@@ -344,0 +360,1 @@\n+  assert(!obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -431,0 +448,1 @@\n+  JavaThread* THREAD = current;\n@@ -439,0 +457,10 @@\n+  if (obj->klass()->is_inline_klass()) {\n+    ResourceMark rm(THREAD);\n+    const char* desc = \"Cannot synchronize on an instance of value class \";\n+    const char* className = obj->klass()->external_name();\n+    size_t msglen = strlen(desc) + strlen(className) + 1;\n+    char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+    assert(message != nullptr, \"NEW_RESOURCE_ARRAY should have called vm_exit_out_of_memory and not return nullptr\");\n+    THROW_MSG(vmSymbols::java_lang_IdentityException(), className);\n+  }\n+\n@@ -456,0 +484,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -516,0 +545,1 @@\n+  CHECK_THROW_NOSYNC_IMSE_0(obj);\n@@ -545,0 +575,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -558,0 +589,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -643,0 +675,3 @@\n+  \/\/ VM should be calling bootstrap method.\n+  assert(!obj->klass()->is_inline_klass(), \"FastHashCode should not be called for inline classes\");\n+\n@@ -741,0 +776,3 @@\n+  if (h_obj->mark().is_inline_type()) {\n+    return false;\n+  }\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":38,"deletions":0,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -123,0 +123,1 @@\n+  DCmdFactory::register_DCmdFactory(new DCmdFactoryImpl<PrintClassLayoutDCmd>(full_export));\n@@ -951,1 +952,25 @@\n-#endif\n+\n+PrintClassLayoutDCmd::PrintClassLayoutDCmd(outputStream* output, bool heap) :\n+                                       DCmdWithParser(output, heap),\n+  _classname(\"classname\", \"Name of class whose layout should be printed. \",\n+             \"STRING\", true) {\n+  _dcmdparser.add_dcmd_argument(&_classname);\n+}\n+\n+void PrintClassLayoutDCmd::execute(DCmdSource source, TRAPS) {\n+  VM_PrintClassLayout printClassLayoutOp(output(), _classname.value());\n+  VMThread::execute(&printClassLayoutOp);\n+}\n+\n+int PrintClassLayoutDCmd::num_arguments() {\n+  ResourceMark rm;\n+  PrintClassLayoutDCmd* dcmd = new PrintClassLayoutDCmd(nullptr, false);\n+  if (dcmd != nullptr) {\n+    DCmdMark mark(dcmd);\n+    return dcmd->_dcmdparser.num_arguments();\n+  } else {\n+    return 0;\n+  }\n+}\n+\n+#endif \/\/ INCLUDE_SERVICES\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.cpp","additions":26,"deletions":1,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+import jdk.internal.misc.PreviewFeatures;\n+import jdk.internal.value.DeserializeConstructor;\n@@ -39,1 +41,0 @@\n-import static java.lang.constant.ConstantDescs.CD_int;\n@@ -54,4 +55,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Byte} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -62,0 +71,1 @@\n+@jdk.internal.MigratedValueClass\n@@ -136,5 +146,17 @@\n-     * If a new {@code Byte} instance is not required, this method\n-     * should generally be used in preference to the constructor\n-     * {@link #Byte(byte)}, as this method is likely to yield\n-     * significantly better space and time performance since\n-     * all byte values are cached.\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          <p>\n+     *              - When preview features are NOT enabled, {@code Byte} is an identity class.\n+     *              If a new {@code Byte} instance is not required, this method\n+     *              should generally be used in preference to the constructor\n+     *              {@link #Byte(byte)}, as this method is likely to yield\n+     *              significantly better space and time performance since\n+     *              all byte values are cached.\n+     *          <\/p>\n+     *          <p>\n+     *              - When preview features are enabled, {@code Byte} is a {@linkplain Class#isValue value class}.\n+     *              The {@code valueOf} behavior is the same as invoking the constructor,\n+     *              whether cached or not.\n+     *          <\/p>\n+     *      <\/div>\n+     * <\/div>\n@@ -147,0 +169,1 @@\n+    @DeserializeConstructor\n@@ -149,1 +172,1 @@\n-        return ByteCache.cache[(int)b + offset];\n+        return (!PreviewFeatures.isEnabled()) ? ByteCache.cache[(int)b + offset] : new Byte(b);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Byte.java","additions":34,"deletions":11,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+import jdk.internal.misc.PreviewFeatures;\n+import jdk.internal.value.DeserializeConstructor;\n@@ -172,4 +174,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Character} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -185,0 +195,1 @@\n+@jdk.internal.MigratedValueClass\n@@ -186,2 +197,1 @@\n-public final\n-class Character implements java.io.Serializable, Comparable<Character>, Constable {\n+public final class Character implements java.io.Serializable, Comparable<Character>, Constable {\n@@ -9411,9 +9421,20 @@\n-     * If a new {@code Character} instance is not required, this method\n-     * should generally be used in preference to the constructor\n-     * {@link #Character(char)}, as this method is likely to yield\n-     * significantly better space and time performance by caching\n-     * frequently requested values.\n-     *\n-     * This method will always cache values in the range {@code\n-     * '\\u005Cu0000'} to {@code '\\u005Cu007F'}, inclusive, and may\n-     * cache other values outside of this range.\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          <p>\n+     *              - When preview features are NOT enabled, {@code Character} is an identity class.\n+     *              If a new {@code Character} instance is not required, this method\n+     *              should generally be used in preference to the constructor\n+     *              {@link #Character(char)}, as this method is likely to yield\n+     *              significantly better space and time performance by caching\n+     *              frequently requested values.\n+     *              This method will always cache values in the range {@code\n+     *              '\\u005Cu0000'} to {@code '\\u005Cu007F'}, inclusive, and may\n+     *              cache other values outside of this range.\n+     *          <\/p>\n+     *          <p>\n+     *             - When preview features are enabled, {@code Character} is a {@linkplain Class#isValue value class}.\n+     *              The {@code valueOf} behavior is the same as invoking the constructor,\n+     *              whether cached or not.\n+     *          <\/p>\n+     *      <\/div>\n+     * <\/div>\n@@ -9426,0 +9447,1 @@\n+    @DeserializeConstructor\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Character.java","additions":37,"deletions":15,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+import jdk.internal.misc.PreviewFeatures;\n@@ -31,0 +32,1 @@\n+import jdk.internal.value.DeserializeConstructor;\n@@ -59,4 +61,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Integer} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -73,0 +83,1 @@\n+@jdk.internal.MigratedValueClass\n@@ -974,8 +985,20 @@\n-     * {@code int} value.  If a new {@code Integer} instance is not\n-     * required, this method should generally be used in preference to\n-     * the constructor {@link #Integer(int)}, as this method is likely\n-     * to yield significantly better space and time performance by\n-     * caching frequently requested values.\n-     *\n-     * This method will always cache values in the range -128 to 127,\n-     * inclusive, and may cache other values outside of this range.\n+     * {@code int} value.\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          <p>\n+     *              - When preview features are NOT enabled, {@code Integer} is an identity class.\n+     *              If a new {@code Integer} instance is not\n+     *              required, this method should generally be used in preference to\n+     *              the constructor {@link #Integer(int)}, as this method is likely\n+     *              to yield significantly better space and time performance by\n+     *              caching frequently requested values.\n+     *              This method will always cache values in the range -128 to 127,\n+     *              inclusive, and may cache other values outside of this range.\n+     *          <\/p>\n+     *          <p>\n+     *              - When preview features are enabled, {@code Integer} is a {@linkplain Class#isValue value class}.\n+     *              The {@code valueOf} behavior is the same as invoking the constructor,\n+     *              whether cached or not.\n+     *          <\/p>\n+     *      <\/div>\n+     * <\/div>\n@@ -988,0 +1011,1 @@\n+    @DeserializeConstructor\n@@ -989,2 +1013,4 @@\n-        if (i >= IntegerCache.low && i <= IntegerCache.high)\n-            return IntegerCache.cache[i + (-IntegerCache.low)];\n+        if (!PreviewFeatures.isEnabled()) {\n+            if (i >= IntegerCache.low && i <= IntegerCache.high)\n+                return IntegerCache.cache[i + (-IntegerCache.low)];\n+        }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Integer.java","additions":40,"deletions":14,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -37,0 +37,2 @@\n+import jdk.internal.misc.PreviewFeatures;\n+import jdk.internal.value.DeserializeConstructor;\n@@ -58,4 +60,13 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Long} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n+ *\n@@ -72,0 +83,1 @@\n+@jdk.internal.MigratedValueClass\n@@ -944,8 +956,19 @@\n-     * If a new {@code Long} instance is not required, this method\n-     * should generally be used in preference to the constructor\n-     * {@link #Long(long)}, as this method is likely to yield\n-     * significantly better space and time performance by caching\n-     * frequently requested values.\n-     *\n-     * This method will always cache values in the range -128 to 127,\n-     * inclusive, and may cache other values outside of this range.\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          <p>\n+     *              - When preview features are NOT enabled, {@code Long} is an identity class.\n+     *              If a new {@code Long} instance is not required, this method\n+     *              should generally be used in preference to the constructor\n+     *              {@link #Long(long)}, as this method is likely to yield\n+     *              significantly better space and time performance by caching\n+     *              frequently requested values.\n+     *              This method will always cache values in the range -128 to 127,\n+     *              inclusive, and may cache other values outside of this range.\n+     *          <\/p>\n+     *          <p>\n+     *              - When preview features are enabled, {@code Long} is a {@linkplain Class#isValue value class}.\n+     *              The {@code valueOf} behavior is the same as invoking the constructor,\n+     *              whether cached or not.\n+     *          <\/p>\n+     *      <\/div>\n+     * <\/div>\n@@ -958,0 +981,1 @@\n+    @DeserializeConstructor\n@@ -959,3 +983,5 @@\n-        final int offset = 128;\n-        if (l >= -128 && l <= 127) { \/\/ will cache\n-            return LongCache.cache[(int)l + offset];\n+        if (!PreviewFeatures.isEnabled()) {\n+            if (l >= -128 && l <= 127) { \/\/ will cache\n+                final int offset = 128;\n+                return LongCache.cache[(int) l + offset];\n+            }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Long.java","additions":41,"deletions":15,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+import jdk.internal.misc.PreviewFeatures;\n+import jdk.internal.value.DeserializeConstructor;\n@@ -38,1 +40,0 @@\n-import static java.lang.constant.ConstantDescs.CD_int;\n@@ -54,4 +55,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Short} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -62,0 +71,1 @@\n+@jdk.internal.MigratedValueClass\n@@ -263,8 +273,19 @@\n-     * If a new {@code Short} instance is not required, this method\n-     * should generally be used in preference to the constructor\n-     * {@link #Short(short)}, as this method is likely to yield\n-     * significantly better space and time performance by caching\n-     * frequently requested values.\n-     *\n-     * This method will always cache values in the range -128 to 127,\n-     * inclusive, and may cache other values outside of this range.\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          <p>\n+     *              - When preview features are NOT enabled, {@code Short} is an identity class.\n+     *              If a new {@code Short} instance is not required, this method\n+     *              should generally be used in preference to the constructor\n+     *              {@link #Short(short)}, as this method is likely to yield\n+     *              significantly better space and time performance by caching\n+     *              frequently requested values.\n+     *              This method will always cache values in the range -128 to 127,\n+     *              inclusive, and may cache other values outside of this range.\n+     *          <\/p>\n+     *          <p>\n+     *              - When preview features are enabled, {@code Short} is a {@linkplain Class#isValue value class}.\n+     *              The {@code valueOf} behavior is the same as invoking the constructor,\n+     *              whether cached or not.\n+     *          <\/p>\n+     *      <\/div>\n+     * <\/div>\n@@ -277,0 +298,1 @@\n+    @DeserializeConstructor\n@@ -278,4 +300,6 @@\n-        final int offset = 128;\n-        int sAsInt = s;\n-        if (sAsInt >= -128 && sAsInt <= 127) { \/\/ must cache\n-            return ShortCache.cache[sAsInt + offset];\n+        if (!PreviewFeatures.isEnabled()) {\n+            final int offset = 128;\n+            int sAsInt = s;\n+            if (sAsInt >= -128 && sAsInt <= 127) { \/\/ must cache\n+                return ShortCache.cache[sAsInt + offset];\n+            }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Short.java","additions":41,"deletions":17,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -118,0 +118,1 @@\n+    private final LocalProxyVarsGen localProxyVarsGen;\n@@ -157,0 +158,1 @@\n+        localProxyVarsGen = LocalProxyVarsGen.instance(context);\n@@ -424,39 +426,44 @@\n-        switch ((short)(sym.flags() & AccessFlags)) {\n-        case PRIVATE:\n-            return\n-                (env.enclClass.sym == sym.owner \/\/ fast special case\n-                 ||\n-                 env.enclClass.sym.outermostClass() ==\n-                 sym.owner.outermostClass()\n-                 ||\n-                 privateMemberInPermitsClauseIfAllowed(env, sym))\n-                &&\n-                sym.isInheritedIn(site.tsym, types);\n-        case 0:\n-            return\n-                (env.toplevel.packge == sym.owner.owner \/\/ fast special case\n-                 ||\n-                 env.toplevel.packge == sym.packge())\n-                &&\n-                isAccessible(env, site, checkInner)\n-                &&\n-                sym.isInheritedIn(site.tsym, types)\n-                &&\n-                notOverriddenIn(site, sym);\n-        case PROTECTED:\n-            return\n-                (env.toplevel.packge == sym.owner.owner \/\/ fast special case\n-                 ||\n-                 env.toplevel.packge == sym.packge()\n-                 ||\n-                 isProtectedAccessible(sym, env.enclClass.sym, site)\n-                 ||\n-                 \/\/ OK to select instance method or field from 'super' or type name\n-                 \/\/ (but type names should be disallowed elsewhere!)\n-                 env.info.selectSuper && (sym.flags() & STATIC) == 0 && sym.kind != TYP)\n-                &&\n-                isAccessible(env, site, checkInner)\n-                &&\n-                notOverriddenIn(site, sym);\n-        default: \/\/ this case includes erroneous combinations as well\n-            return isAccessible(env, site, checkInner) && notOverriddenIn(site, sym);\n+        ClassSymbol enclosingCsym = env.enclClass.sym;\n+        try {\n+            switch ((short)(sym.flags() & AccessFlags)) {\n+                case PRIVATE:\n+                    return\n+                            (env.enclClass.sym == sym.owner \/\/ fast special case\n+                                    ||\n+                                    env.enclClass.sym.outermostClass() ==\n+                                    sym.owner.outermostClass()\n+                                    ||\n+                                    privateMemberInPermitsClauseIfAllowed(env, sym))\n+                                &&\n+                                    sym.isInheritedIn(site.tsym, types);\n+                case 0:\n+                    return\n+                            (env.toplevel.packge == sym.owner.owner \/\/ fast special case\n+                                    ||\n+                                    env.toplevel.packge == sym.packge())\n+                                    &&\n+                                    isAccessible(env, site, checkInner)\n+                                    &&\n+                                    sym.isInheritedIn(site.tsym, types)\n+                                    &&\n+                                    notOverriddenIn(site, sym);\n+                case PROTECTED:\n+                    return\n+                            (env.toplevel.packge == sym.owner.owner \/\/ fast special case\n+                                    ||\n+                                    env.toplevel.packge == sym.packge()\n+                                    ||\n+                                    isProtectedAccessible(sym, env.enclClass.sym, site)\n+                                    ||\n+                                    \/\/ OK to select instance method or field from 'super' or type name\n+                                    \/\/ (but type names should be disallowed elsewhere!)\n+                                    env.info.selectSuper && (sym.flags() & STATIC) == 0 && sym.kind != TYP)\n+                                    &&\n+                                    isAccessible(env, site, checkInner)\n+                                    &&\n+                                    notOverriddenIn(site, sym);\n+                default: \/\/ this case includes erroneous combinations as well\n+                    return isAccessible(env, site, checkInner) && notOverriddenIn(site, sym);\n+            }\n+        } finally {\n+            env.enclClass.sym = enclosingCsym;\n@@ -1533,2 +1540,0 @@\n-                    if (env1.info.ctorPrologue && !isAllowedEarlyReference(pos, env1, (VarSymbol)sym))\n-                        return new RefBeforeCtorCalledError(sym);\n@@ -2042,2 +2047,0 @@\n-                        if (env1.info.ctorPrologue && env1 == env)\n-                            return new RefBeforeCtorCalledError(sym);\n@@ -3819,3 +3822,0 @@\n-                    } else if (env1.info.ctorPrologue && !isAllowedEarlyReference(pos, env1, (VarSymbol)sym)) {\n-                        \/\/ early construction context, stop search\n-                        return new RefBeforeCtorCalledError(sym);\n@@ -3884,4 +3884,0 @@\n-                    else if (env1.info.ctorPrologue &&\n-                            !isReceiverParameter(env, tree) &&\n-                            !isAllowedEarlyReference(pos, env1, (VarSymbol)sym))\n-                        sym = new RefBeforeCtorCalledError(sym);\n@@ -3901,2 +3897,0 @@\n-                    if (env.info.ctorPrologue)\n-                        log.error(pos, Errors.CantRefBeforeCtorCalled(name));\n@@ -3938,95 +3932,0 @@\n-    private boolean isReceiverParameter(Env<AttrContext> env, JCFieldAccess tree) {\n-        if (env.tree.getTag() != METHODDEF)\n-            return false;\n-        JCMethodDecl method = (JCMethodDecl)env.tree;\n-        return method.recvparam != null && tree == method.recvparam.nameexpr;\n-    }\n-\n-    \/**\n-     * Determine if an early instance field reference may appear in a constructor prologue.\n-     *\n-     * <p>\n-     * This is only allowed when:\n-     *  - The field is being assigned a value (i.e., written but not read)\n-     *  - The field is not inherited from a superclass\n-     *  - The assignment is not within a lambda, because that would require\n-     *    capturing 'this' which is not allowed prior to super().\n-     *\n-     * <p>\n-     * Note, this method doesn't catch all such scenarios, because this method\n-     * is invoked for symbol \"x\" only for \"x = 42\" but not for \"this.x = 42\".\n-     * We also don't verify that the field has no initializer, which is required.\n-     * To catch those cases, we rely on similar logic in Attr.checkAssignable().\n-     *\/\n-    private boolean isAllowedEarlyReference(DiagnosticPosition pos, Env<AttrContext> env, VarSymbol v) {\n-\n-        \/\/ Check assumptions\n-        Assert.check(env.info.ctorPrologue);\n-        Assert.check((v.flags_field & STATIC) == 0);\n-\n-        \/\/ The symbol must appear in the LHS of an assignment statement\n-        if (!(env.tree instanceof JCAssign assign))\n-            return false;\n-\n-        \/\/ The assignment statement must not be within a lambda\n-        if (env.info.isLambda)\n-            return false;\n-\n-        \/\/ Get the symbol's qualifier, if any\n-        JCExpression lhs = TreeInfo.skipParens(assign.lhs);\n-        JCExpression base;\n-        switch (lhs.getTag()) {\n-        case IDENT:\n-            base = null;\n-            break;\n-        case SELECT:\n-            JCFieldAccess select = (JCFieldAccess)lhs;\n-            base = select.selected;\n-            if (!TreeInfo.isExplicitThisReference(types, (ClassType)env.enclClass.type, base))\n-                return false;\n-            break;\n-        default:\n-            return false;\n-        }\n-\n-        \/\/ If an early reference, the field must not be declared in a superclass\n-        if (isEarlyReference(env, base, v) && v.owner != env.enclClass.sym)\n-            return false;\n-\n-        \/\/ The flexible constructors feature must be enabled\n-        preview.checkSourceLevel(pos, Feature.FLEXIBLE_CONSTRUCTORS);\n-\n-        \/\/ OK\n-        return true;\n-    }\n-\n-    \/**\n-     * Determine if the variable appearance constitutes an early reference to the current class.\n-     *\n-     * <p>\n-     * This means the variable is an instance field of the current class and it appears\n-     * in an early initialization context of it (i.e., one of its constructor prologues).\n-     *\n-     * <p>\n-     * Such a reference is only allowed for assignments to non-initialized fields that are\n-     * not inherited from a superclass, though that is not enforced by this method.\n-     *\n-     * @param env    The current environment\n-     * @param base   Variable qualifier, if any, otherwise null\n-     * @param v      The variable\n-     *\/\n-    public boolean isEarlyReference(Env<AttrContext> env, JCTree base, VarSymbol v) {\n-        if (env.info.ctorPrologue &&\n-                (v.flags() & STATIC) == 0 &&\n-                v.isMemberOf(env.enclClass.sym, types)) {\n-\n-            \/\/ Allow \"Foo.this.x\" when \"Foo\" is (also) an outer class, as this refers to the outer instance\n-            if (base != null) {\n-                return TreeInfo.isExplicitThisReference(types, (ClassType)env.enclClass.type, base);\n-            }\n-\n-            \/\/ It's an early reference to an instance field member of the current instance\n-            return true;\n-        }\n-        return false;\n-    }\n@@ -4342,1 +4241,0 @@\n-                              kindName(ws.owner),\n@@ -5243,4 +5141,0 @@\n-\n-        boolean internal() {\n-            return internalResolution;\n-        }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Resolve.java","additions":46,"deletions":152,"binary":false,"changes":198,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+#if 0 \/\/ FIX: JDK-8374115\n@@ -64,0 +65,1 @@\n+#endif\n","filename":"test\/hotspot\/gtest\/runtime\/test_classPrinter.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -80,0 +80,3 @@\n+compiler\/jvmci\/jdk.vm.ci.hotspot.test\/src\/jdk\/vm\/ci\/hotspot\/test\/MemoryAccessProviderTest.java 8350208 generic-all\n+compiler\/jvmci\/jdk.vm.ci.hotspot.test\/src\/jdk\/vm\/ci\/hotspot\/test\/TestHotSpotResolvedJavaField.java 8350208 generic-all\n+\n@@ -82,0 +85,26 @@\n+# Valhalla\n+compiler\/whitebox\/DeoptimizeRelocatedNMethod.java 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#C1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#C2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#G1C1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#G1C2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ParallelC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ParallelC2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#SerialC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#SerialC2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ZGCC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ZGCC2 8370571 generic-all\n+compiler\/whitebox\/StressNMethodRelocation.java 8370571 generic-all\n+compiler\/valhalla\/inlinetypes\/TestC1.java             8372341 generic-all\n+compiler\/valhalla\/inlinetypes\/TestCastMismatch.java   8372341 generic-all\n+compiler\/valhalla\/inlinetypes\/TestGetfieldChains.java 8372341 generic-all\n+compiler\/valhalla\/inlinetypes\/TestTrivialMethods.java 8373692 generic-all\n+compiler\/c2\/cmove\/TestScalarConditionalMoveCmpObj.java    8374122 generic-all\n+compiler\/codegen\/TestRedundantLea.java#GetAndSet          8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StringEquals       8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StringInflate      8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#RegexFind          8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StoreNSerial       8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StoreNParallel     8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#Spill              8361089 generic-all\n+\n@@ -103,0 +132,1 @@\n+runtime\/cds\/appcds\/redefineClass\/RedefineRunningMethods_Shared.java  8304168 generic-all\n@@ -122,0 +152,10 @@\n+\n+# Valhalla\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictInstanceFieldsTest.java CODETOOLS-7904031 generic-all\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictStaticFieldsTest.java CODETOOLS-7904031 generic-all\n+\n+runtime\/cds\/TestDefaultArchiveLoading.java#coops_nocoh            8366774           generic-all\n+runtime\/cds\/TestDefaultArchiveLoading.java#nocoops_nocoh          8366774           generic-all\n+\n+# Valhalla + AOT\n+runtime\/cds\/appcds\/methodHandles\/MethodHandlesGeneralTest.java#aot              8367408 generic-all\n@@ -147,0 +187,57 @@\n+\n+# Valhalla TODO:\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbDumpclass.java 8190936 generic-all\n+\n+# Array Changes TODO\n+serviceability\/sa\/CDSJMapClstats.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbClasses.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DisableRegistryTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbDumpheap.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbTestConnectArgument.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DebugdConnectTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJhisto.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id1 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackWithConcurrentLock.java 8365722 generic-all\n+serviceability\/sa\/TestJhsdbJstackWithVirtualThread.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackXcompStress.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#process 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#core 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id0 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id1 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#serial 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#parallel 8365722 generic-all\n+serviceability\/sa\/DeadlockDetectionTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id0 8365722 generic-all\n+serviceability\/sa\/TestInstanceKlassSize.java 8365722 generic-all\n+serviceability\/sa\/TestSysProps.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbAttachToDebugServer.java 8365722 generic-all\n+resourcehogs\/serviceability\/sa\/TestHeapDumpForLargeArray.java 8365722 generic-all\n+\n+resourcehogs\/serviceability\/sa\/ClhsdbRegionDetailsScanOopsForG1.java 8190936 generic-all\n+vmTestbase\/nsk\/jvmti\/scenarios\/events\/EM04\/em04t001\/TestDescription.java 8367590 generic-all\n+\n@@ -186,0 +283,2 @@\n+vmTestbase\/vm\/mlvm\/hiddenloader\/stress\/byteMutation\/Test.java 8317172 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":99,"deletions":0,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-  runtime\n+  runtime \\\n@@ -65,0 +65,8 @@\n+hotspot_valhalla = \\\n+  runtime\/valhalla \\\n+  compiler\/valhalla \\\n+  serviceability\/jvmti\/valhalla\n+\n+hotspot_valhalla_runtime = \\\n+  runtime\/valhalla\n+\n@@ -214,0 +222,1 @@\n+  compiler\/valhalla\/ \\\n@@ -255,0 +264,7 @@\n+\n+tier1_compiler_no_valhalla = \\\n+  :tier1_compiler_1 \\\n+  :tier1_compiler_2 \\\n+  :tier1_compiler_3 \\\n+  -compiler\/valhalla\n+\n@@ -408,0 +424,4 @@\n+tier1_runtime_no_valhalla = \\\n+  :tier1_runtime \\\n+  -runtime\/valhalla\n+\n@@ -567,0 +587,1 @@\n+ -runtime\/cds\/appcds\/RewriteBytecodesInlineTest.java \\\n@@ -635,0 +656,4 @@\n+tier1_serviceability_no_valhalla = \\\n+  :tier1_serviceability \\\n+  -serviceability\/jvmti\/valhalla\n+\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":26,"deletions":1,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import compiler.valhalla.inlinetypes.InlineTypeIRNode;\n@@ -90,1 +91,1 @@\n-    private static final String PREFIX = \"_#\";\n+    public static final String PREFIX = \"_#\";\n@@ -153,0 +154,6 @@\n+    \/\/ Valhalla: Make sure that all Valhalla specific IR nodes are also properly initialized. Doing it here also\n+    \/\/           ensures that the Flag VM is able to pick up the correct compile phases.\n+    static {\n+        InlineTypeIRNode.forceStaticInitialization();\n+    }\n+\n@@ -384,2 +391,6 @@\n-        String regex = START + \"Allocate\\\\b\" + MID + \"allocationKlass:.*\\\\b\" + IS_REPLACED + \"\\\\s.*\" + END;\n-        macroNodes(ALLOC_OF, regex);\n+        allocateOfNodes(ALLOC_OF, IS_REPLACED);\n+    }\n+\n+    public static void allocateOfNodes(String irNodePlaceholder, String allocatee) {\n+        String regex = START + \"Allocate\\\\b\" + MID + \"allocationKlass:.*\\\\b\" + allocatee + \"\\\\s.*\" + END;\n+        macroNodes(irNodePlaceholder, regex);\n@@ -396,0 +407,4 @@\n+        allocateArrayOfNodes(ALLOC_ARRAY_OF, IS_REPLACED);\n+    }\n+\n+    public static void allocateArrayOfNodes(String irNodePlaceholder, String allocatee) {\n@@ -414,1 +429,1 @@\n-        String name_part = \"\\\\[+.(\" + partial_name_prefix + \")?\" + IS_REPLACED + \";\";\n+        String name_part = \"\\\\[+.(\" + partial_name_prefix + \")?\" + allocatee + \";\";\n@@ -416,1 +431,1 @@\n-        macroNodes(ALLOC_ARRAY_OF, regex);\n+        macroNodes(irNodePlaceholder, regex);\n@@ -481,1 +496,1 @@\n-        callOfNodes(CALL_OF, \"Call.*\");\n+        callOfNodes(CALL_OF, \"Call.*\", IS_REPLACED + \" \" );\n@@ -486,1 +501,6 @@\n-        callOfNodes(CALL_OF_METHOD, \"Call.*Java\");\n+        callOfNodes(CALL_OF_METHOD, \"Call.*Java\", IS_REPLACED + \" \");\n+    }\n+\n+    public static final String STATIC_CALL = PREFIX + \"STATIC_CALL\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(STATIC_CALL, \"CallStaticJava\");\n@@ -491,1 +511,19 @@\n-        callOfNodes(STATIC_CALL_OF_METHOD, \"CallStaticJava\");\n+        staticCallOfMethodNodes(STATIC_CALL_OF_METHOD, IS_REPLACED + \" \");\n+    }\n+\n+    public static void staticCallOfMethodNodes(String irNodePlaceholder, String calleeRegex) {\n+        callOfNodes(irNodePlaceholder, \"CallStaticJava\", calleeRegex);\n+    }\n+\n+    public static final String CALL_LEAF_NO_FP = PREFIX + \"CALL_LEAF_NO_FP\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(CALL_LEAF_NO_FP, \"CallLeafNoFP\");\n+    }\n+\n+    public static final String CALL_LEAF_NO_FP_OF_METHOD = COMPOSITE_PREFIX + \"CALL_LEAF_NO_FP_OF_METHOD\" + POSTFIX;\n+    static {\n+        callLeafNoFpOfMethodNodes(CALL_LEAF_NO_FP_OF_METHOD, IS_REPLACED);\n+    }\n+\n+    public static void callLeafNoFpOfMethodNodes(String irNodePlaceholder, String calleeRegex) {\n+        callOfNodes(irNodePlaceholder, \"CallLeafNoFP\", calleeRegex);\n@@ -762,1 +800,1 @@\n-        callOfNodes(DYNAMIC_CALL_OF_METHOD, \"CallDynamicJava\");\n+        callOfNodes(DYNAMIC_CALL_OF_METHOD, \"CallDynamicJava\", IS_REPLACED);\n@@ -898,0 +936,5 @@\n+    public static final String INLINE_TYPE = PREFIX + \"INLINE_TYPE\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(INLINE_TYPE, \"InlineType\");\n+    }\n+\n@@ -944,1 +987,5 @@\n-        loadOfNodes(LOAD_OF_CLASS, \"Load(B|UB|S|US|I|L|F|D|P|N)\");\n+        anyLoadOfNodes(LOAD_OF_CLASS, IS_REPLACED);\n+    }\n+\n+    public static void anyLoadOfNodes(String irNodePlaceholder, String fieldHolder) {\n+        loadOfNodes(irNodePlaceholder, \"Load(B|UB|S|US|I|L|F|D|P|N)\", fieldHolder);\n@@ -954,1 +1001,1 @@\n-        loadOfNodes(LOAD_B_OF_CLASS, \"LoadB\");\n+        loadOfNodes(LOAD_B_OF_CLASS, \"LoadB\", IS_REPLACED);\n@@ -964,1 +1011,1 @@\n-        loadOfNodes(LOAD_D_OF_CLASS, \"LoadD\");\n+        loadOfNodes(LOAD_D_OF_CLASS, \"LoadD\", IS_REPLACED);\n@@ -974,1 +1021,1 @@\n-        loadOfNodes(LOAD_F_OF_CLASS, \"LoadF\");\n+        loadOfNodes(LOAD_F_OF_CLASS, \"LoadF\", IS_REPLACED);\n@@ -984,1 +1031,1 @@\n-        loadOfNodes(LOAD_I_OF_CLASS, \"LoadI\");\n+        loadOfNodes(LOAD_I_OF_CLASS, \"LoadI\", IS_REPLACED);\n@@ -1009,1 +1056,1 @@\n-        loadOfNodes(LOAD_L_OF_CLASS, \"LoadL\");\n+        loadOfNodes(LOAD_L_OF_CLASS, \"LoadL\", IS_REPLACED);\n@@ -1019,1 +1066,1 @@\n-        loadOfNodes(LOAD_N_OF_CLASS, \"LoadN\");\n+        loadOfNodes(LOAD_N_OF_CLASS, \"LoadN\", IS_REPLACED);\n@@ -1035,1 +1082,1 @@\n-        loadOfNodes(LOAD_P_OF_CLASS, \"LoadP\");\n+        loadOfNodes(LOAD_P_OF_CLASS, \"LoadP\", IS_REPLACED);\n@@ -1045,1 +1092,1 @@\n-        loadOfNodes(LOAD_S_OF_CLASS, \"LoadS\");\n+        loadOfNodes(LOAD_S_OF_CLASS, \"LoadS\", IS_REPLACED);\n@@ -1055,1 +1102,1 @@\n-        loadOfNodes(LOAD_UB_OF_CLASS, \"LoadUB\");\n+        loadOfNodes(LOAD_UB_OF_CLASS, \"LoadUB\", IS_REPLACED);\n@@ -1065,1 +1112,1 @@\n-        loadOfNodes(LOAD_US_OF_CLASS, \"LoadUS\");\n+        loadOfNodes(LOAD_US_OF_CLASS, \"LoadUS\", IS_REPLACED);\n@@ -2015,1 +2062,1 @@\n-        storeOfNodes(STORE_B_OF_CLASS, \"StoreB\");\n+        storeOfNodes(STORE_B_OF_CLASS, \"StoreB\", IS_REPLACED);\n@@ -2025,1 +2072,1 @@\n-        storeOfNodes(STORE_C_OF_CLASS, \"StoreC\");\n+        storeOfNodes(STORE_C_OF_CLASS, \"StoreC\", IS_REPLACED);\n@@ -2035,1 +2082,1 @@\n-        storeOfNodes(STORE_D_OF_CLASS, \"StoreD\");\n+        storeOfNodes(STORE_D_OF_CLASS, \"StoreD\", IS_REPLACED);\n@@ -2045,1 +2092,1 @@\n-        storeOfNodes(STORE_F_OF_CLASS, \"StoreF\");\n+        storeOfNodes(STORE_F_OF_CLASS, \"StoreF\", IS_REPLACED);\n@@ -2055,1 +2102,1 @@\n-        storeOfNodes(STORE_I_OF_CLASS, \"StoreI\");\n+        storeOfNodes(STORE_I_OF_CLASS, \"StoreI\", IS_REPLACED);\n@@ -2065,1 +2112,1 @@\n-        storeOfNodes(STORE_L_OF_CLASS, \"StoreL\");\n+        storeOfNodes(STORE_L_OF_CLASS, \"StoreL\", IS_REPLACED);\n@@ -2075,1 +2122,1 @@\n-        storeOfNodes(STORE_N_OF_CLASS, \"StoreN\");\n+        storeOfNodes(STORE_N_OF_CLASS, \"StoreN\", IS_REPLACED);\n@@ -2080,1 +2127,5 @@\n-        storeOfNodes(STORE_OF_CLASS, \"Store(B|C|S|I|L|F|D|P|N)\");\n+        anyStoreOfNodes(STORE_OF_CLASS, IS_REPLACED);\n+    }\n+\n+    public static void anyStoreOfNodes(String irNodePlaceholder, String fieldHolder) {\n+        storeOfNodes(irNodePlaceholder, \"Store(B|C|S|I|L|F|D|P|N)\", fieldHolder);\n@@ -2096,1 +2147,1 @@\n-        storeOfNodes(STORE_P_OF_CLASS, \"StoreP\");\n+        storeOfNodes(STORE_P_OF_CLASS, \"StoreP\", IS_REPLACED);\n@@ -2196,1 +2247,2 @@\n-        beforeMatchingNameRegex(SUBTYPE_CHECK, \"SubTypeCheck\");\n+        String regex = START + \"SubTypeCheck\" + MID + END;\n+        macroNodes(SUBTYPE_CHECK, regex);\n@@ -3148,1 +3200,1 @@\n-    private static void beforeMatching(String irNodePlaceholder, String regex) {\n+    public static void beforeMatching(String irNodePlaceholder, String regex) {\n@@ -3184,2 +3236,2 @@\n-    private static void callOfNodes(String irNodePlaceholder, String callRegex) {\n-        String regex = START + callRegex + MID + IS_REPLACED + \" \" +  END;\n+    private static void callOfNodes(String irNodePlaceholder, String callRegex, String calleeRegex) {\n+        String regex = START + callRegex + MID + calleeRegex + END;\n@@ -3193,1 +3245,1 @@\n-    private static void optoOnly(String irNodePlaceholder, String regex) {\n+    public static void optoOnly(String irNodePlaceholder, String regex) {\n@@ -3287,2 +3339,2 @@\n-    private static void loadOfNodes(String irNodePlaceholder, String irNodeRegex) {\n-        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + IS_REPLACED + LOAD_STORE_SUFFIX + END;\n+    private static void loadOfNodes(String irNodePlaceholder, String irNodeRegex, String loadee) {\n+        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + loadee + LOAD_STORE_SUFFIX + END;\n@@ -3292,2 +3344,2 @@\n-    private static void storeOfNodes(String irNodePlaceholder, String irNodeRegex) {\n-        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + IS_REPLACED + LOAD_STORE_SUFFIX + END;\n+    private static void storeOfNodes(String irNodePlaceholder, String irNodeRegex, String storee) {\n+        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + storee + LOAD_STORE_SUFFIX + END;\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":90,"deletions":38,"binary":false,"changes":128,"status":"modified"},{"patch":"@@ -0,0 +1,2175 @@\n+\/*\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.valhalla.inlinetypes;\n+\n+import jdk.internal.misc.Unsafe;\n+import jdk.test.lib.Asserts;\n+import compiler.lib.ir_framework.*;\n+\n+import java.lang.reflect.Array;\n+import java.lang.reflect.Field;\n+import java.util.Arrays;\n+\n+import jdk.internal.value.ValueClass;\n+import jdk.internal.vm.annotation.LooselyConsistentValue;\n+import jdk.internal.vm.annotation.NullRestricted;\n+import jdk.internal.vm.annotation.Strict;\n+import jdk.test.whitebox.WhiteBox;\n+\n+import static compiler.lib.ir_framework.IRNode.STATIC_CALL_OF_METHOD;\n+import static compiler.valhalla.inlinetypes.InlineTypeIRNode.CALL_UNSAFE;\n+import static compiler.valhalla.inlinetypes.InlineTypes.rI;\n+import static compiler.valhalla.inlinetypes.InlineTypes.rL;\n+\n+import static compiler.lib.ir_framework.IRNode.LOAD;\n+import static compiler.lib.ir_framework.IRNode.LOAD_KLASS;\n+import static compiler.valhalla.inlinetypes.InlineTypes.*;\n+\n+\/*\n+ * @test\n+ * @key randomness\n+ * @summary Test intrinsic support for value classes.\n+ * @library \/test\/lib \/\n+ * @requires (os.simpleArch == \"x64\" | os.simpleArch == \"aarch64\")\n+ * @enablePreview\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.base\/jdk.internal.value\n+ *          java.base\/jdk.internal.vm.annotation\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI compiler.valhalla.inlinetypes.TestIntrinsics 0\n+ *\/\n+\n+\/*\n+ * @test\n+ * @key randomness\n+ * @summary Test intrinsic support for value classes.\n+ * @library \/test\/lib \/\n+ * @requires (os.simpleArch == \"x64\" | os.simpleArch == \"aarch64\")\n+ * @enablePreview\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.base\/jdk.internal.value\n+ *          java.base\/jdk.internal.vm.annotation\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI compiler.valhalla.inlinetypes.TestIntrinsics 1\n+ *\/\n+\n+\/*\n+ * @test\n+ * @key randomness\n+ * @summary Test intrinsic support for value classes.\n+ * @library \/test\/lib \/\n+ * @requires (os.simpleArch == \"x64\" | os.simpleArch == \"aarch64\")\n+ * @enablePreview\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.base\/jdk.internal.value\n+ *          java.base\/jdk.internal.vm.annotation\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI compiler.valhalla.inlinetypes.TestIntrinsics 2\n+ *\/\n+\n+\/*\n+ * @test\n+ * @key randomness\n+ * @summary Test intrinsic support for value classes.\n+ * @library \/test\/lib \/\n+ * @requires (os.simpleArch == \"x64\" | os.simpleArch == \"aarch64\")\n+ * @enablePreview\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.base\/jdk.internal.value\n+ *          java.base\/jdk.internal.vm.annotation\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI compiler.valhalla.inlinetypes.TestIntrinsics 3\n+ *\/\n+\n+\/*\n+ * @test\n+ * @key randomness\n+ * @summary Test intrinsic support for value classes.\n+ * @library \/test\/lib \/\n+ * @requires (os.simpleArch == \"x64\" | os.simpleArch == \"aarch64\")\n+ * @enablePreview\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.base\/jdk.internal.value\n+ *          java.base\/jdk.internal.vm.annotation\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI compiler.valhalla.inlinetypes.TestIntrinsics 4\n+ *\/\n+\n+\/*\n+ * @test\n+ * @key randomness\n+ * @summary Test intrinsic support for value classes.\n+ * @library \/test\/lib \/\n+ * @requires (os.simpleArch == \"x64\" | os.simpleArch == \"aarch64\")\n+ * @enablePreview\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.base\/jdk.internal.value\n+ *          java.base\/jdk.internal.vm.annotation\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI compiler.valhalla.inlinetypes.TestIntrinsics 5\n+ *\/\n+\n+\/*\n+ * @test\n+ * @key randomness\n+ * @summary Test intrinsic support for value classes.\n+ * @library \/test\/lib \/\n+ * @requires (os.simpleArch == \"x64\" | os.simpleArch == \"aarch64\")\n+ * @enablePreview\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.base\/jdk.internal.value\n+ *          java.base\/jdk.internal.vm.annotation\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI compiler.valhalla.inlinetypes.TestIntrinsics 6\n+ *\/\n+\n+@ForceCompileClassInitializer\n+public class TestIntrinsics {\n+\n+    private static final WhiteBox WHITEBOX = WhiteBox.getWhiteBox();\n+    private static final boolean UseArrayFlattening = WHITEBOX.getBooleanVMFlag(\"UseArrayFlattening\");\n+    private static final boolean UseFieldFlattening = WHITEBOX.getBooleanVMFlag(\"UseFieldFlattening\");\n+    private static final boolean PreloadClasses = WHITEBOX.getBooleanVMFlag(\"PreloadClasses\");\n+\n+    public static void main(String[] args) {\n+\n+        Scenario[] scenarios = InlineTypes.DEFAULT_SCENARIOS;\n+        scenarios[3].addFlags(\"-XX:-MonomorphicArrayCheck\", \"-XX:+UseArrayFlattening\");\n+        scenarios[4].addFlags(\"-XX:-MonomorphicArrayCheck\", \"-XX:+UnlockExperimentalVMOptions\", \"-XX:PerMethodSpecTrapLimit=0\", \"-XX:PerMethodTrapLimit=0\");\n+\n+        InlineTypes.getFramework()\n+                   .addScenarios(scenarios[Integer.parseInt(args[0])])\n+                   .addFlags(\"-Xbootclasspath\/a:.\", \"-XX:+UnlockDiagnosticVMOptions\", \"-XX:+WhiteBoxAPI\",\n+                             \"-XX:CompileCommand=inline,jdk.internal.misc.Unsafe::*\",\n+                             \"--add-exports\", \"java.base\/jdk.internal.misc=ALL-UNNAMED\",\n+                             \"--add-exports\", \"java.base\/jdk.internal.value=ALL-UNNAMED\",\n+                             \"-XX:+IgnoreUnrecognizedVMOptions -XX:VerifyIterativeGVN=000\",\n+                             \"-XX:+UseAltSubstitutabilityMethod\")\n+                   .addHelperClasses(MyValue1.class,\n+                                     MyValue2.class,\n+                                     MyValue2Inline.class)\n+                   .start();\n+    }\n+\n+    static {\n+        \/\/ Make sure RuntimeException is loaded to prevent uncommon traps in IR verified tests\n+        RuntimeException tmp = new RuntimeException(\"42\");\n+    }\n+\n+    \/\/ Test correctness of the Class::isAssignableFrom intrinsic\n+    @Test\n+    public boolean test1(Class<?> supercls, Class<?> subcls) {\n+        return supercls.isAssignableFrom(subcls);\n+    }\n+\n+    @Run(test = \"test1\")\n+    public void test1_verifier() {\n+        Asserts.assertTrue(test1(java.util.AbstractList.class, java.util.ArrayList.class), \"test1_1 failed\");\n+        Asserts.assertTrue(test1(MyValue1.class, MyValue1.class), \"test1_2 failed\");\n+        Asserts.assertTrue(test1(Object.class, java.util.ArrayList.class), \"test1_3 failed\");\n+        Asserts.assertTrue(test1(Object.class, MyValue1.class), \"test1_4 failed\");\n+        Asserts.assertTrue(!test1(MyValue1.class, Object.class), \"test1_5 failed\");\n+    }\n+\n+    \/\/ Verify that Class::isAssignableFrom checks with statically known classes are folded\n+    @Test\n+    @IR(failOn = {LOAD_KLASS})\n+    public boolean test2() {\n+        boolean check1 = java.util.AbstractList.class.isAssignableFrom(java.util.ArrayList.class);\n+        boolean check2 = MyValue1.class.isAssignableFrom(MyValue1.class);\n+        boolean check3 = Object.class.isAssignableFrom(java.util.ArrayList.class);\n+        boolean check4 = Object.class.isAssignableFrom(MyValue1.class);\n+        boolean check5 = !MyValue1.class.isAssignableFrom(Object.class);\n+        return check1 && check2 && check3 && check4 && check5;\n+    }\n+\n+    @Run(test = \"test2\")\n+    public void test2_verifier() {\n+        Asserts.assertTrue(test2(), \"test2 failed\");\n+    }\n+\n+    \/\/ Test correctness of the Class::getSuperclass intrinsic\n+    @Test\n+    public Class<?> test3(Class<?> cls) {\n+        return cls.getSuperclass();\n+    }\n+\n+    @Run(test = \"test3\")\n+    public void test3_verifier() {\n+        Asserts.assertTrue(test3(Object.class) == null, \"test3_1 failed\");\n+        Asserts.assertTrue(test3(MyValue1.class) == MyAbstract.class, \"test3_2 failed\");\n+        Asserts.assertTrue(test3(MyValue1.class) == MyAbstract.class, \"test3_3 failed\");\n+        Asserts.assertTrue(test3(Class.class) == Object.class, \"test3_4 failed\");\n+    }\n+\n+    \/\/ Verify that Class::getSuperclass checks with statically known classes are folded\n+    @Test\n+    @IR(failOn = {LOAD_KLASS})\n+    public boolean test4() {\n+        boolean check1 = Object.class.getSuperclass() == null;\n+        boolean check2 = MyValue1.class.getSuperclass() == MyAbstract.class;\n+        boolean check3 = MyValue1.class.getSuperclass() == MyAbstract.class;\n+        boolean check4 = Class.class.getSuperclass() == Object.class;\n+        return check1 && check2 && check3 && check4;\n+    }\n+\n+    @Run(test = \"test4\")\n+    public void test4_verifier() {\n+        Asserts.assertTrue(test4(), \"test4 failed\");\n+    }\n+\n+    \/\/ Test toString() method\n+    @Test\n+    public String test5(MyValue1 v) {\n+        return v.toString();\n+    }\n+\n+    @Run(test = \"test5\")\n+    public void test5_verifier() {\n+        MyValue1 v = MyValue1.createDefaultInline();\n+        test5(v);\n+    }\n+\n+    \/\/ Test hashCode() method\n+    @Test\n+    public int test6(MyValue1 v) {\n+        return v.hashCode();\n+    }\n+\n+    @Run(test = \"test6\")\n+    public void test6_verifier() {\n+        MyValue1 v = MyValue1.createWithFieldsInline(rI, rL);\n+        int res = test6(v);\n+        Asserts.assertEQ(res, v.hashCode());\n+    }\n+\n+    \/\/ Test value class array creation via reflection\n+    @Test\n+    public Object[] test7(Class<?> componentType, int len, Object initValue) {\n+        Object[] va = ValueClass.newNullRestrictedNonAtomicArray(componentType, len, initValue);\n+        return va;\n+    }\n+\n+    @Run(test = \"test7\")\n+    public void test7_verifier() {\n+        int len = Math.abs(rI) % 42;\n+        long hash = MyValue1.createDefaultDontInline().hashPrimitive();\n+        Object[] va = test7(MyValue1.class, len, MyValue1.DEFAULT);\n+        for (int i = 0; i < len; ++i) {\n+            Asserts.assertEQ(((MyValue1)va[i]).hashPrimitive(), hash);\n+        }\n+    }\n+\n+    \/\/ Class.isInstance\n+    @Test\n+    public boolean test8(Class c, MyValue1 vt) {\n+        return c.isInstance(vt);\n+    }\n+\n+    @Run(test = \"test8\")\n+    public void test8_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        boolean result = test8(MyValue1.class, vt);\n+        Asserts.assertTrue(result);\n+        result = test8(MyValue1.class, vt);\n+        Asserts.assertTrue(result);\n+    }\n+\n+    @Test\n+    public boolean test9(Class c, MyValue1 vt) {\n+        return c.isInstance(vt);\n+    }\n+\n+    @Run(test = \"test9\")\n+    public void test9_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        boolean result = test9(MyValue2.class, vt);\n+        Asserts.assertFalse(result);\n+        result = test9(MyValue2.class, vt);\n+        Asserts.assertFalse(result);\n+    }\n+\n+    \/\/ Class.cast\n+    @Test\n+    public Object test10(Class c, MyValue1 vt) {\n+        return c.cast(vt);\n+    }\n+\n+    @Run(test = \"test10\")\n+    public void test10_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        Object result = test10(MyValue1.class, vt);\n+        Asserts.assertEQ(((MyValue1)result).hash(), vt.hash());\n+    }\n+\n+    @Test\n+    public Object test11(Class c, MyValue1 vt) {\n+        return c.cast(vt);\n+    }\n+\n+    @Run(test = \"test11\")\n+    public void test11_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        try {\n+            test11(MyValue2.class, vt);\n+            throw new RuntimeException(\"should have thrown\");\n+        } catch (ClassCastException cce) {\n+        }\n+    }\n+\n+    @Test\n+    public Object test12(MyValue1 vt) {\n+        return MyValue1.class.cast(vt);\n+    }\n+\n+    @Run(test = \"test12\")\n+    public void test12_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        Object result = test12(vt);\n+        Asserts.assertEQ(((MyValue1)result).hash(), vt.hash());\n+    }\n+\n+    @Test\n+    public Object test13(MyValue1 vt) {\n+        return MyValue2.class.cast(vt);\n+    }\n+\n+    @Run(test = \"test13\")\n+    public void test13_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        try {\n+            test13(vt);\n+            throw new RuntimeException(\"should have thrown\");\n+        } catch (ClassCastException cce) {\n+        }\n+    }\n+\n+    \/\/ Value class array creation via reflection\n+    @Test\n+    public void test14(int len, long hash) {\n+        Object[] va = ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, len, MyValue1.DEFAULT);\n+        for (int i = 0; i < len; ++i) {\n+            Asserts.assertEQ(((MyValue1)va[i]).hashPrimitive(), hash);\n+        }\n+    }\n+\n+    @Run(test = \"test14\")\n+    public void test14_verifier() {\n+        int len = Math.abs(rI) % 42;\n+        long hash = MyValue1.createDefaultDontInline().hashPrimitive();\n+        test14(len, hash);\n+    }\n+\n+    \/\/ Test hashCode() method\n+    @Test\n+    public int test15(Object v) {\n+        return v.hashCode();\n+    }\n+\n+    @Run(test = \"test15\")\n+    public void test15_verifier() {\n+        MyValue1 v = MyValue1.createWithFieldsInline(rI, rL);\n+        int res = test15(v);\n+        Asserts.assertEQ(res, v.hashCode());\n+    }\n+\n+    @Test\n+    public int test16(Object v) {\n+        return System.identityHashCode(v);\n+    }\n+\n+    @Run(test = \"test16\")\n+    public void test16_verifier() {\n+        MyValue1 v = MyValue1.createWithFieldsInline(rI, rL);\n+        int res = test16(v);\n+        Asserts.assertEQ(res, System.identityHashCode((Object)v));\n+    }\n+\n+    @Test\n+    public int test17(Object v) {\n+        return System.identityHashCode(v);\n+    }\n+\n+    @Run(test = \"test17\")\n+    public void test17_verifier() {\n+        Integer v = Integer.valueOf(rI);\n+        int res = test17(v);\n+        Asserts.assertEQ(res, System.identityHashCode(v));\n+    }\n+\n+    @Test\n+    public int test18(Object v) {\n+        return System.identityHashCode(v);\n+    }\n+\n+    @Run(test = \"test18\")\n+    public void test18_verifier() {\n+        Object v = null;\n+        int res = test18(v);\n+        Asserts.assertEQ(res, System.identityHashCode(v));\n+    }\n+\n+    \/\/ hashCode() and toString() with different value objects\n+    @Test\n+    public int test19(MyValue1 vt1, MyValue1 vt2, boolean b) {\n+        MyValue1 res = b ? vt1 : vt2;\n+        return res.hashCode();\n+    }\n+\n+    @Run(test = \"test19\")\n+    public void test19_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        int res = test19(vt, vt, true);\n+        Asserts.assertEQ(res, vt.hashCode());\n+        res = test19(vt, vt, false);\n+        Asserts.assertEQ(res, vt.hashCode());\n+    }\n+\n+    @Test\n+    public String test20(MyValue1 vt1, MyValue1 vt2, boolean b) {\n+        MyValue1 res = b ? vt1 : vt2;\n+        return res.toString();\n+    }\n+\n+    @Run(test = \"test20\")\n+    public void test20_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        String res = test20(vt, vt, true);\n+        Asserts.assertEQ(res, vt.toString());\n+        res = test20(vt, vt, false);\n+        Asserts.assertEQ(res, vt.toString());\n+    }\n+\n+    private static final Unsafe U = Unsafe.getUnsafe();\n+    private static final long X_OFFSET;\n+    private static final long Y_OFFSET;\n+    private static final long V1_OFFSET;\n+    private static final boolean V1_FLATTENED;\n+    private static final int V1_LAYOUT;\n+\n+    static {\n+        try {\n+            Field xField = MyValue1.class.getDeclaredField(\"x\");\n+            X_OFFSET = U.objectFieldOffset(xField);\n+            Field yField = MyValue1.class.getDeclaredField(\"y\");\n+            Y_OFFSET = U.objectFieldOffset(yField);\n+            Field v1Field = MyValue1.class.getDeclaredField(\"v1\");\n+            V1_OFFSET = U.objectFieldOffset(v1Field);\n+            V1_FLATTENED = U.isFlatField(v1Field);\n+            V1_LAYOUT = U.fieldLayout(v1Field);\n+        } catch (Exception e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public int test21(MyValue1 v) {\n+       return U.getInt(v, X_OFFSET);\n+    }\n+\n+    @Run(test = \"test21\")\n+    public void test21_verifier() {\n+        MyValue1 v = MyValue1.createWithFieldsInline(rI, rL);\n+        int res = test21(v);\n+        Asserts.assertEQ(res, v.x);\n+    }\n+\n+    MyValue1 test22_vt;\n+\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public void test22(MyValue1 v) {\n+        v = U.makePrivateBuffer(v);\n+        U.putInt(v, X_OFFSET, rI);\n+        v = U.finishPrivateBuffer(v);\n+        test22_vt = v;\n+    }\n+\n+    @Run(test = \"test22\")\n+    public void test22_verifier() {\n+        MyValue1 v = MyValue1.createWithFieldsInline(rI, rL);\n+        test22(v.setX(v, 0));\n+        Asserts.assertEQ(test22_vt.hash(), v.hash());\n+    }\n+\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public int test23(MyValue1 v, long offset) {\n+        return U.getInt(v, offset);\n+    }\n+\n+    @Run(test = \"test23\")\n+    public void test23_verifier() {\n+        MyValue1 v = MyValue1.createWithFieldsInline(rI, rL);\n+        int res = test23(v, X_OFFSET);\n+        Asserts.assertEQ(res, v.x);\n+    }\n+\n+    @Strict\n+    @NullRestricted\n+    MyValue1 test24_vt = MyValue1.createWithFieldsInline(rI, rL);\n+\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public int test24(long offset) {\n+        return U.getInt(test24_vt, offset);\n+    }\n+\n+    @Run(test = \"test24\")\n+    public void test24_verifier() {\n+        int res = test24(X_OFFSET);\n+        Asserts.assertEQ(res, test24_vt.x);\n+    }\n+\n+    \/\/ Test copyOf intrinsic with allocated value object in its debug information\n+    @LooselyConsistentValue\n+    static value class Test25Value {\n+        int x;\n+\n+        public Test25Value(int x) {\n+            this.x = x;\n+        }\n+    }\n+\n+    final Test25Value[] test25Array = (Test25Value[])ValueClass.newNullRestrictedNonAtomicArray(Test25Value.class, 10, new Test25Value(0));\n+\n+    @Test\n+    public Test25Value[] test25(Test25Value element) {\n+        Object[] newArray = Arrays.copyOf(test25Array, test25Array.length);\n+        newArray[test25Array.length - 1] = element;\n+        return (Test25Value[]) newArray;\n+    }\n+\n+    @Run(test = \"test25\")\n+    public void test25_verifier() {\n+        Test25Value vt = new Test25Value(42);\n+        test25(vt);\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I) \/\/ Load of the all-zero value should be folded\n+    public Object test26() {\n+        Class<?>[] ca = new Class<?>[1];\n+        for (int i = 0; i < 1; ++i) {\n+          \/\/ Folds during loop opts\n+          ca[i] = MyValue1.class;\n+        }\n+        return ValueClass.newNullRestrictedNonAtomicArray(ca[0], 1, MyValue1.DEFAULT);\n+    }\n+\n+    @Run(test = \"test26\")\n+    public void test26_verifier() {\n+        Object[] res = (Object[])test26();\n+        Asserts.assertEQ(((MyValue1)res[0]).hashPrimitive(), MyValue1.createDefaultInline().hashPrimitive());\n+    }\n+\n+    \/\/ Load non-flattenable value class field with unsafe\n+    MyValue1 test27_vt;\n+    private static final long TEST27_OFFSET;\n+    static {\n+        try {\n+            Field field = TestIntrinsics.class.getDeclaredField(\"test27_vt\");\n+            TEST27_OFFSET = U.objectFieldOffset(field);\n+        } catch (Exception e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public MyValue1 test27() {\n+        return (MyValue1)U.getReference(this, TEST27_OFFSET);\n+    }\n+\n+    @Run(test = \"test27\")\n+    public void test27_verifier() {\n+        test27_vt = null;\n+        MyValue1 res = test27();\n+        Asserts.assertEQ(res, null);\n+        test27_vt = MyValue1.createWithFieldsInline(rI, rL);\n+        res = test27();\n+        Asserts.assertEQ(res.hash(), test24_vt.hash());\n+    }\n+\n+    \/\/ Mismatched type\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public int test28(MyValue1 v) {\n+        return U.getByte(v, X_OFFSET);\n+    }\n+\n+    @Run(test = \"test28\")\n+    public void test28_verifier() {\n+        MyValue1 v = MyValue1.createWithFieldsInline(rI, rL);\n+        int res = test28(v);\n+        if (java.nio.ByteOrder.nativeOrder() == java.nio.ByteOrder.LITTLE_ENDIAN) {\n+            Asserts.assertEQ(res, (int)((byte)v.x));\n+        } else {\n+            Asserts.assertEQ(res, (int)((byte)Integer.reverseBytes(v.x)));\n+        }\n+    }\n+\n+    \/\/ Wrong alignment\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public long test29(MyValue1 v) {\n+        \/\/ Read the field that's guaranteed to not be last in the\n+        \/\/ value class so we don't read out of bounds.\n+        if (X_OFFSET < Y_OFFSET) {\n+            return U.getInt(v, X_OFFSET+1);\n+        }\n+        return U.getLong(v, Y_OFFSET+1);\n+    }\n+\n+    @Run(test = \"test29\")\n+    public void test29_verifier() {\n+        MyValue1 v = MyValue1.createWithFieldsInline(rI, rL);\n+        long res = test29(v);\n+        if (java.nio.ByteOrder.nativeOrder() == java.nio.ByteOrder.LITTLE_ENDIAN) {\n+            if (X_OFFSET < Y_OFFSET) {\n+                Asserts.assertEQ(((int)res) << 8, (v.x >> 8) << 8);\n+            } else {\n+                Asserts.assertEQ(res << 8, (v.y >> 8) << 8);\n+            }\n+        } else {\n+            if (X_OFFSET < Y_OFFSET) {\n+                Asserts.assertEQ(((int)res), v.x >>> 8);\n+            } else {\n+                Asserts.assertEQ(res, v.y >>> 8);\n+            }\n+        }\n+    }\n+\n+    \/\/ getValue to retrieve flattened field from value object\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public MyValue2 test30(MyValue1 v) {\n+        if (V1_FLATTENED) {\n+            return U.getFlatValue(v, V1_OFFSET, V1_LAYOUT, MyValue2.class);\n+        }\n+        return (MyValue2)U.getReference(v, V1_OFFSET);\n+    }\n+\n+    @Run(test = \"test30\")\n+    public void test30_verifier(RunInfo info) {\n+        MyValue1 v = MyValue1.createWithFieldsInline(rI, rL);\n+        MyValue2 res = test30(v);\n+        Asserts.assertEQ(res.hash(), v.v1.hash());\n+    }\n+\n+    MyValue1 test31_vt;\n+    private static final long TEST31_VT_OFFSET;\n+    private static final boolean TEST31_VT_FLATTENED;\n+    private static final int TEST31_VT_LAYOUT;\n+    static {\n+        try {\n+            Field test31_vt_Field = TestIntrinsics.class.getDeclaredField(\"test31_vt\");\n+            TEST31_VT_OFFSET = U.objectFieldOffset(test31_vt_Field);\n+            TEST31_VT_FLATTENED = U.isFlatField(test31_vt_Field);\n+            TEST31_VT_LAYOUT = U.fieldLayout(test31_vt_Field);\n+        } catch (Exception e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    \/\/ getValue to retrieve flattened field from object\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public MyValue1 test31() {\n+        if (TEST31_VT_FLATTENED) {\n+            return U.getFlatValue(this, TEST31_VT_OFFSET, TEST31_VT_LAYOUT, MyValue1.class);\n+        }\n+        return (MyValue1)U.getReference(this, TEST31_VT_OFFSET);\n+    }\n+\n+    @Run(test = \"test31\")\n+    public void test31_verifier() {\n+        test31_vt = MyValue1.createWithFieldsInline(rI, rL);\n+        MyValue1 res = test31();\n+        Asserts.assertEQ(res.hash(), test31_vt.hash());\n+    }\n+\n+    \/\/ putValue to set flattened field in object\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public void test32(MyValue1 vt) {\n+        if (TEST31_VT_FLATTENED) {\n+            U.putFlatValue(this, TEST31_VT_OFFSET, TEST31_VT_LAYOUT, MyValue1.class, vt);\n+        } else {\n+            U.putReference(this, TEST31_VT_OFFSET, vt);\n+        }\n+    }\n+\n+    @Run(test = \"test32\")\n+    public void test32_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        test31_vt = MyValue1.createDefaultInline();\n+        test32(vt);\n+        Asserts.assertEQ(vt.hash(), test31_vt.hash());\n+    }\n+\n+    private static final long TEST33_BASE_OFFSET;\n+    private static final int TEST33_INDEX_SCALE;\n+    private static final MyValue1[] TEST33_ARRAY;\n+    private static final boolean TEST33_FLATTENED_ARRAY;\n+    private static final int TEST33_LAYOUT;\n+    static {\n+        try {\n+            TEST33_ARRAY = (MyValue1[])ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 2, MyValue1.DEFAULT);\n+            TEST33_BASE_OFFSET = U.arrayInstanceBaseOffset(TEST33_ARRAY);\n+            TEST33_INDEX_SCALE = U.arrayInstanceIndexScale(TEST33_ARRAY);\n+            TEST33_FLATTENED_ARRAY = ValueClass.isFlatArray(TEST33_ARRAY);\n+            TEST33_LAYOUT = U.arrayLayout(TEST33_ARRAY);\n+        } catch (Exception e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+    \/\/ getValue to retrieve flattened field from array\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public MyValue1 test33() {\n+        if (TEST33_FLATTENED_ARRAY) {\n+            return U.getFlatValue(TEST33_ARRAY, TEST33_BASE_OFFSET + TEST33_INDEX_SCALE, TEST33_LAYOUT, MyValue1.class);\n+        }\n+        return (MyValue1)U.getReference(TEST33_ARRAY, TEST33_BASE_OFFSET + TEST33_INDEX_SCALE);\n+    }\n+\n+    @Run(test = \"test33\")\n+    public void test33_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        TEST33_ARRAY[1] = vt;\n+        MyValue1 res = test33();\n+        Asserts.assertEQ(res.hash(), vt.hash());\n+    }\n+\n+    \/\/ putValue to set flattened field in array\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public void test34(MyValue1 vt) {\n+        if (TEST33_FLATTENED_ARRAY) {\n+            U.putFlatValue(TEST33_ARRAY, TEST33_BASE_OFFSET + TEST33_INDEX_SCALE, TEST33_LAYOUT, MyValue1.class, vt);\n+        } else {\n+            U.putReference(TEST33_ARRAY, TEST33_BASE_OFFSET + TEST33_INDEX_SCALE, vt);\n+        }\n+    }\n+\n+    @Run(test = \"test34\")\n+    public void test34_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        test34(vt);\n+        Asserts.assertEQ(TEST33_ARRAY[1].hash(), vt.hash());\n+    }\n+\n+    \/\/ getValue to retrieve flattened field from object with unknown\n+    \/\/ container type\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public MyValue1 test35(Object o) {\n+        if (TEST31_VT_FLATTENED) {\n+            return U.getFlatValue(o, TEST31_VT_OFFSET, TEST31_VT_LAYOUT, MyValue1.class);\n+        }\n+        return (MyValue1)U.getReference(o, TEST31_VT_OFFSET);\n+    }\n+\n+    @Run(test = \"test35\")\n+    public void test35_verifier() {\n+        test31_vt = MyValue1.createWithFieldsInline(rI, rL);\n+        MyValue1 res = test35(this);\n+        Asserts.assertEQ(res.hash(), test31_vt.hash());\n+    }\n+\n+    \/\/ getValue to retrieve flattened field from object at unknown\n+    \/\/ offset\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public MyValue1 test36(long offset) {\n+        if (TEST31_VT_FLATTENED) {\n+            return U.getFlatValue(this, offset, TEST31_VT_LAYOUT, MyValue1.class);\n+        }\n+        return (MyValue1)U.getReference(this, offset);\n+    }\n+\n+    @Run(test = \"test36\")\n+    public void test36_verifier() {\n+        test31_vt = MyValue1.createWithFieldsInline(rI, rL);\n+        MyValue1 res = test36(TEST31_VT_OFFSET);\n+        Asserts.assertEQ(res.hash(), test31_vt.hash());\n+    }\n+\n+    \/\/ putValue to set flattened field in object with unknown\n+    \/\/ container\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public void test37(Object o, MyValue1 vt) {\n+        if (TEST31_VT_FLATTENED) {\n+            U.putFlatValue(o, TEST31_VT_OFFSET, TEST31_VT_LAYOUT, MyValue1.class, vt);\n+        } else {\n+            U.putReference(o, TEST31_VT_OFFSET, vt);\n+        }\n+    }\n+\n+    @Run(test = \"test37\")\n+    public void test37_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        test31_vt = MyValue1.createDefaultInline();\n+        test37(this, vt);\n+        Asserts.assertEQ(vt.hash(), test31_vt.hash());\n+    }\n+\n+    \/\/ putValue to set flattened field in object, non inline argument\n+    \/\/ to store\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public void test38(Object o) {\n+        if (TEST31_VT_FLATTENED) {\n+            U.putFlatValue(this, TEST31_VT_OFFSET, TEST31_VT_LAYOUT, MyValue1.class, o);\n+        } else {\n+            U.putReference(this, TEST31_VT_OFFSET, o);\n+        }\n+    }\n+\n+    @Run(test = \"test38\")\n+    public void test38_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        test31_vt = MyValue1.createDefaultInline();\n+        test38(vt);\n+        Asserts.assertEQ(vt.hash(), test31_vt.hash());\n+    }\n+\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public MyValue1 test39(MyValue1 v) {\n+        v = U.makePrivateBuffer(v);\n+        U.putInt(v, X_OFFSET, rI);\n+        v = U.finishPrivateBuffer(v);\n+        return v;\n+    }\n+\n+    @Run(test = \"test39\")\n+    public void test39_verifier() {\n+        MyValue1 v = MyValue1.createWithFieldsInline(rI, rL);\n+        MyValue1 res = test39(v.setX(v, 0));\n+        Asserts.assertEQ(res.hash(), v.hash());\n+    }\n+\n+    \/\/ Test value class array creation via reflection\n+    @Test\n+    public Object[] test40(Class<?> componentType, int len) {\n+        Object[] va = (Object[])Array.newInstance(componentType, len);\n+        return va;\n+    }\n+\n+    @Run(test = \"test40\")\n+    public void test40_verifier() {\n+        int len = Math.abs(rI) % 42;\n+        Object[] va = test40(MyValue1.class, len);\n+        for (int i = 0; i < len; ++i) {\n+            Asserts.assertEQ(va[i], null);\n+        }\n+    }\n+\n+    \/\/ Class.isInstance\n+    @Test\n+    public boolean test41(Class c, MyValue1 vt) {\n+        return c.isInstance(vt);\n+    }\n+\n+    @Run(test = \"test41\")\n+    public void test41_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        boolean result = test41(MyValue1.class, vt);\n+        Asserts.assertTrue(result);\n+        result = test41(MyValue1.class, null);\n+        Asserts.assertFalse(result);\n+        result = test41(MyValue1.class, vt);\n+        Asserts.assertTrue(result);\n+        result = test41(MyValue1.class, null);\n+        Asserts.assertFalse(result);\n+    }\n+\n+    @Test\n+    public boolean test42(Class c, MyValue1 vt) {\n+        return c.isInstance(vt);\n+    }\n+\n+    @Run(test = \"test42\")\n+    public void test42_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        boolean result = test42(MyValue2.class, vt);\n+        Asserts.assertFalse(result);\n+        result = test42(MyValue2.class, null);\n+        Asserts.assertFalse(result);\n+        result = test42(MyValue2.class, vt);\n+        Asserts.assertFalse(result);\n+        result = test42(MyValue2.class, null);\n+        Asserts.assertFalse(result);\n+    }\n+\n+    \/\/ Class.cast\n+    @Test\n+    public Object test43(Class c, MyValue1 vt) {\n+        return c.cast(vt);\n+    }\n+\n+    @Run(test = \"test43\")\n+    public void test43_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        Object result = test43(MyValue1.class, vt);\n+        Asserts.assertEQ(result, vt);\n+        result = test43(MyValue1.class, null);\n+        Asserts.assertEQ(result, null);\n+        result = test43(MyValue1.class, vt);\n+        Asserts.assertEQ(result, vt);\n+        result = test43(NonValueClass.class, null);\n+        Asserts.assertEQ(result, null);\n+    }\n+\n+    @Test\n+    public Object test44(Class c, MyValue1 vt) {\n+        return c.cast(vt);\n+    }\n+\n+    @Run(test = \"test44\")\n+    public void test44_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        try {\n+            test44(MyValue2.class, vt);\n+            throw new RuntimeException(\"should have thrown\");\n+        } catch (ClassCastException cce) {\n+        }\n+        Object res = test44(MyValue2.class, null);\n+        Asserts.assertEQ(res, null);\n+        try {\n+            test44(MyValue2.class, vt);\n+            throw new RuntimeException(\"should have thrown\");\n+        } catch (ClassCastException cce) {\n+        }\n+    }\n+\n+    @Test\n+    public Object test45(MyValue1 vt) {\n+        return MyValue1.class.cast(vt);\n+    }\n+\n+    @Run(test = \"test45\")\n+    public void test45_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        Object result = test45(vt);\n+        Asserts.assertEQ(((MyValue1)result).hash(), vt.hash());\n+        result = test45(null);\n+        Asserts.assertEQ(result, null);\n+    }\n+\n+    @Test\n+    public Object test46(MyValue1 vt) {\n+        return MyValue2.class.cast(vt);\n+    }\n+\n+    @Run(test = \"test46\")\n+    public void test46_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        Object result = test46(null);\n+        Asserts.assertEQ(result, null);\n+        try {\n+            test46(vt);\n+            throw new RuntimeException(\"should have thrown\");\n+        } catch (ClassCastException cce) {\n+        }\n+    }\n+\n+    @Test\n+    public Object test47(MyValue1 vt) {\n+        return MyValue1.class.cast(vt);\n+    }\n+\n+    @Run(test = \"test47\")\n+    public void test47_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        Object result = test47(vt);\n+        Asserts.assertEQ(((MyValue1)result).hash(), vt.hash());\n+        result = test47(null);\n+        Asserts.assertEQ(result, null);\n+    }\n+\n+    @Test\n+    public Object test48(Class c, MyValue1 vt) {\n+        return c.cast(vt);\n+    }\n+\n+    @Run(test = \"test48\")\n+    public void test48_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        Object result = test48(MyValue1.class, vt);\n+        Asserts.assertEQ(((MyValue1)result).hash(), vt.hash());\n+        result = test48(MyValue1.class, null);\n+        Asserts.assertEQ(result, null);\n+    }\n+\n+    @Test\n+    public Object test49(MyValue1 vt) {\n+        return MyValue1.class.cast(vt);\n+    }\n+\n+    @Run(test = \"test49\")\n+    public void test49_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        Object result = test49(vt);\n+        Asserts.assertEQ(((MyValue1)result).hash(), vt.hash());\n+    }\n+\n+    @Test\n+    public Object test50(Class c, Object obj) {\n+        return c.cast(obj);\n+    }\n+\n+    @Run(test = \"test50\")\n+    public void test50_verifier() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        MyValue1[] va  = (MyValue1[])ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 42, MyValue1.DEFAULT);\n+        MyValue1[] vba = new MyValue1[42];\n+        Object result = test50(MyValue1.class, vt);\n+        Asserts.assertEQ(((MyValue1)result).hash(), vt.hash());\n+        result = test50(MyValue1.class, vt);\n+        Asserts.assertEQ(((MyValue1)result).hash(), vt.hash());\n+        result = test50(MyValue1[].class, va);\n+        Asserts.assertEQ(result, va);\n+        result = test50(MyValue1[].class, vba);\n+        Asserts.assertEQ(result, vba);\n+        result = test50(MyValue1[].class, va);\n+        Asserts.assertEQ(result, va);\n+        result = test50(MyValue1.class, null);\n+        Asserts.assertEQ(result, null);\n+        result = test50(va.getClass(), vba);\n+        Asserts.assertEQ(result, vba);\n+    }\n+\n+    \/\/ Value class array creation via reflection\n+    @Test\n+    public void test51(int len) {\n+        Object[] va = (Object[])Array.newInstance(MyValue1.class, len);\n+        for (int i = 0; i < len; ++i) {\n+            Asserts.assertEQ(va[i], null);\n+        }\n+    }\n+\n+    @Run(test = \"test51\")\n+    public void test51_verifier() {\n+        int len = Math.abs(rI) % 42;\n+        test51(len);\n+    }\n+\n+    \/\/ multidimensional value class array creation via reflection\n+    @Test\n+    public Object[][] test52(int len, int val) {\n+        MyValue1[][] va1 = (MyValue1[][])Array.newInstance(MyValue1[].class, len);\n+        MyValue1[][] va2 = (MyValue1[][])Array.newInstance(MyValue1[].class, len);\n+        Object[][] result;\n+        if (val == 1) {\n+            va1[0] = (MyValue1[])ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 1, MyValue1.DEFAULT);\n+            result = va1;\n+        } else {\n+            va2[0] = new MyValue1[1];\n+            result = va2;\n+        }\n+        if (val == 1) {\n+            Asserts.assertEQ(va1[0][0].hash(), ((MyValue1)result[0][0]).hash());\n+        } else {\n+            Asserts.assertEQ(result[0][0], null);\n+            result[0][0] = null;\n+        }\n+        return result;\n+    }\n+\n+    @Run(test = \"test52\")\n+    public void test52_verifier() {\n+        test52(1, 1);\n+        test52(1, 2);\n+    }\n+\n+    @Test\n+    public Object[][] test53(Class<?> c1, Class<?> c2, int len, int val) {\n+        MyValue1[][] va1 = (MyValue1[][])Array.newInstance(MyValue1[].class, len);\n+        MyValue1[][] va2 = (MyValue1[][])Array.newInstance(MyValue1[].class, len);\n+        Object[][] va3 = (Object[][])Array.newInstance(c1, len);\n+        Object[][] va4 = (Object[][])Array.newInstance(c2, len);\n+        for (int i = 0; i < len; ++i) {\n+            Asserts.assertEQ(va1[i], null);\n+            Asserts.assertEQ(va2[i], null);\n+            Asserts.assertEQ(va3[i], null);\n+            Asserts.assertEQ(va4[i], null);\n+            va1[i] = (MyValue1[])ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 1, MyValue1.DEFAULT);\n+            va2[i] = new MyValue1[1];\n+            va3[i] = (MyValue1[])ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 1, MyValue1.DEFAULT);\n+            va4[i] = new MyValue1[1];\n+            Asserts.assertEQ(va1[i][0].hash(), ((MyValue1)va3[i][0]).hash());\n+            Asserts.assertEQ(va2[i][0], null);\n+            Asserts.assertEQ(va4[i][0], null);\n+        }\n+        Object[][] result;\n+        if (val == 1) {\n+            result = va1;\n+        } else if (val == 2) {\n+            result = va2;\n+        } else if (val == 3) {\n+            result = va3;\n+        } else {\n+            result = va4;\n+        }\n+        if ((val == 1 || val == 3) && len > 0) {\n+            Asserts.assertEQ(va1[0][0].hash(), ((MyValue1)result[0][0]).hash());\n+        } else if (len > 0) {\n+            Asserts.assertEQ(result[0][0], null);\n+            result[0][0] = null;\n+        }\n+        return result;\n+    }\n+\n+    @Run(test = \"test53\")\n+    public void test53_verifier() {\n+        int len = Math.abs(rI) % 42;\n+        test53(MyValue1[].class, MyValue1[].class, len, 1);\n+        test53(MyValue1[].class, MyValue1[].class, len, 2);\n+        test53(MyValue1[].class, MyValue1[].class, len, 3);\n+        test53(MyValue1[].class, MyValue1[].class, len, 4);\n+    }\n+\n+    @Strict\n+    @NullRestricted\n+    static final MyValue1 test55_vt = MyValue1.createWithFieldsInline(rI, rL);\n+\n+    \/\/ Same as test30 but with constant field holder\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public MyValue2 test55() {\n+        if (V1_FLATTENED) {\n+            return U.getFlatValue(test55_vt, V1_OFFSET, V1_LAYOUT, MyValue2.class);\n+        }\n+        return (MyValue2)U.getReference(test55_vt, V1_OFFSET);\n+    }\n+\n+    @Run(test = \"test55\")\n+    public void test55_verifier() {\n+        MyValue2 res = test55();\n+        Asserts.assertEQ(res.hash(), test55_vt.v1.hash());\n+    }\n+\n+    \/\/ Test OptimizePtrCompare part of Escape Analysis\n+    @Test\n+    public void test56(int idx) {\n+        Object[] va = ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 1, MyValue1.DEFAULT);\n+        if (va[idx] == null) {\n+            throw new RuntimeException(\"Unexpected null\");\n+        }\n+    }\n+\n+    @Run(test = \"test56\")\n+    public void test56_verifier() {\n+        test56(0);\n+    }\n+\n+    \/\/ Same as test56 but with load from known array index\n+    @Test\n+    public void test57() {\n+        Object[] va = ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 1, MyValue1.DEFAULT);\n+        if (va[0] == null) {\n+            throw new RuntimeException(\"Unexpected null\");\n+        }\n+    }\n+\n+    @Run(test = \"test57\")\n+    public void test57_verifier() {\n+        test57();\n+    }\n+\n+    \/\/ Use a value class without strict fields for Unsafe allocation in below tests\n+    \/\/ because without the constructor, these fields won't be initialized with is illegal.\n+    static value class ValueWithNoStrictFields1 {\n+        int x = rI;\n+    }\n+\n+    static value class ValueWithNoStrictFields2 {\n+        long x = rL;\n+    }\n+\n+    \/\/ Test unsafe allocation\n+    @Test\n+    public boolean test58(Class<?> c1, Class<?> c2) throws Exception {\n+        Object obj1 = U.allocateInstance(c1);\n+        Object obj2 = U.allocateInstance(c2);\n+        return obj1 == obj2;\n+    }\n+\n+    @Run(test = \"test58\")\n+    public void test58_verifier() throws Exception {\n+        boolean res = test58(ValueWithNoStrictFields1.class, ValueWithNoStrictFields1.class);\n+        Asserts.assertTrue(res);\n+        res = test58(Object.class, ValueWithNoStrictFields1.class);\n+        Asserts.assertFalse(res);\n+        res = test58(ValueWithNoStrictFields1.class, Object.class);\n+        Asserts.assertFalse(res);\n+    }\n+\n+    \/\/ Test synchronization on unsafe value object allocation\n+    @Test\n+    public void test59(Class<?> c) throws Exception {\n+        Object obj = U.allocateInstance(c);\n+        synchronized (obj) {\n+\n+        }\n+    }\n+\n+    @Run(test = \"test59\")\n+    public void test59_verifier() throws Exception {\n+        test59(Object.class);\n+        try {\n+            test59(ValueWithNoStrictFields1.class);\n+            throw new RuntimeException(\"test59 failed: synchronization on value object should not succeed\");\n+        } catch (IdentityException e) {\n+\n+        }\n+    }\n+\n+    \/\/ Test mark word load optimization on unsafe value object allocation\n+    @Test\n+    public boolean test60(Class<?> c1, Class<?> c2, boolean b1, boolean b2) throws Exception {\n+        Object obj1 = b1 ? new Object() : U.allocateInstance(c1);\n+        Object obj2 = b2 ? new Object() : U.allocateInstance(c2);\n+        return obj1 == obj2;\n+    }\n+\n+    @Run(test = \"test60\")\n+    public void test60_verifier() throws Exception {\n+        Asserts.assertTrue(test60(ValueWithNoStrictFields1.class, ValueWithNoStrictFields1.class, false, false));\n+        Asserts.assertFalse(test60(ValueWithNoStrictFields1.class, ValueWithNoStrictFields2.class, false, false));\n+        Asserts.assertFalse(test60(ValueWithNoStrictFields1.class, ValueWithNoStrictFields1.class, false, true));\n+        Asserts.assertFalse(test60(ValueWithNoStrictFields1.class, ValueWithNoStrictFields1.class, true, false));\n+        Asserts.assertFalse(test60(ValueWithNoStrictFields1.class, ValueWithNoStrictFields1.class, true, true));\n+    }\n+\n+    static public value class SmallValue {\n+        byte a;\n+        byte b;\n+        static final SmallValue DEFAULT = createDefaultInline();\n+        SmallValue(byte a, byte b) {\n+            this.a = a;\n+            this.b = b;\n+        }\n+\n+        @ForceInline\n+        static SmallValue createDefaultInline() {\n+            return new SmallValue((byte)0, (byte)0);\n+        }\n+\n+        @ForceInline\n+        static SmallValue createWithFieldsInline(int x, long y) {\n+            return new SmallValue((byte)x, (byte)y);\n+        }\n+    }\n+\n+    SmallValue test63_vt;\n+    private static final long TEST63_VT_OFFSET;\n+    private static final boolean TEST63_VT_FLATTENED;\n+    private static final int TEST63_VT_LAYOUT;\n+    static {\n+        try {\n+            Field test63_vt_Field = TestIntrinsics.class.getDeclaredField(\"test63_vt\");\n+            TEST63_VT_OFFSET = U.objectFieldOffset(test63_vt_Field);\n+            TEST63_VT_FLATTENED = U.isFlatField(test63_vt_Field);\n+            TEST63_VT_LAYOUT = U.fieldLayout(test63_vt_Field);\n+        } catch (Exception e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    \/\/ compareAndSet to flattened field in object\n+    @Test\n+    public boolean test63(SmallValue oldVal, SmallValue newVal) {\n+        if (TEST63_VT_FLATTENED) {\n+            Asserts.assertTrue(UseFieldFlattening && PreloadClasses);\n+            return U.compareAndSetFlatValue(this, TEST63_VT_OFFSET, TEST63_VT_LAYOUT, SmallValue.class, oldVal, newVal);\n+        } else {\n+            Asserts.assertFalse(UseFieldFlattening && PreloadClasses);\n+            return U.compareAndSetReference(this, TEST63_VT_OFFSET, oldVal, newVal);\n+        }\n+    }\n+\n+    @Run(test = \"test63\")\n+    public void test63_verifier() {\n+        \/\/ Unsafe::compareAndSetFlatValue needs UseArrayFlattening.\n+        if (UseFieldFlattening && !UseArrayFlattening) return;\n+        SmallValue vt = SmallValue.createWithFieldsInline(rI, rL);\n+        test63_vt = SmallValue.createDefaultInline();\n+\n+        boolean res = test63(test63_vt, vt);\n+        \/\/ Checks are disabled for non-flattened field because reference comparison\n+        \/\/ fails if C2 scalarizes and re-allocates the value class arguments.\n+        if (TEST63_VT_FLATTENED) {\n+            Asserts.assertTrue(res);\n+            Asserts.assertEQ(test63_vt, vt);\n+        }\n+\n+        res = test63(SmallValue.createDefaultInline(), SmallValue.createDefaultInline());\n+        if (TEST63_VT_FLATTENED) {\n+            Asserts.assertFalse(res);\n+            Asserts.assertEQ(test63_vt, vt);\n+        }\n+    }\n+\n+    private static final long TEST64_BASE_OFFSET;\n+    private static final int TEST64_INDEX_SCALE;\n+    private static final SmallValue[] TEST64_ARRAY;\n+    private static final boolean TEST64_FLATTENED_ARRAY;\n+    private static final boolean TEST64_ATOMIC_ARRAY;\n+    private static final int TEST64_LAYOUT;\n+    static {\n+        try {\n+            TEST64_ARRAY = (SmallValue[])ValueClass.newNullRestrictedAtomicArray(SmallValue.class, 2, SmallValue.DEFAULT);\n+            TEST64_BASE_OFFSET = U.arrayInstanceBaseOffset(TEST64_ARRAY);\n+            TEST64_INDEX_SCALE = U.arrayInstanceIndexScale(TEST64_ARRAY);\n+            TEST64_FLATTENED_ARRAY = ValueClass.isFlatArray(TEST64_ARRAY);\n+            TEST64_ATOMIC_ARRAY = ValueClass.isAtomicArray(TEST64_ARRAY);\n+            TEST64_LAYOUT = U.arrayLayout(TEST64_ARRAY);\n+        } catch (Exception e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    \/\/ compareAndSet to flattened field in array\n+    @Test\n+    public boolean test64(SmallValue[] arr, SmallValue oldVal, SmallValue newVal) {\n+        if (TEST64_FLATTENED_ARRAY) {\n+            Asserts.assertTrue(UseArrayFlattening);\n+            return U.compareAndSetFlatValue(arr, TEST64_BASE_OFFSET + TEST64_INDEX_SCALE, TEST64_LAYOUT, SmallValue.class, oldVal, newVal);\n+        } else {\n+            Asserts.assertFalse(UseArrayFlattening);\n+            return U.compareAndSetReference(arr, TEST64_BASE_OFFSET + TEST64_INDEX_SCALE, oldVal, newVal);\n+        }\n+    }\n+\n+    @Run(test = \"test64\")\n+    public void test64_verifier() {\n+        Asserts.assertTrue(TEST64_ATOMIC_ARRAY);\n+        SmallValue[] arr = (SmallValue[])ValueClass.newNullRestrictedAtomicArray(SmallValue.class, 2, SmallValue.DEFAULT);\n+        SmallValue vt = SmallValue.createWithFieldsInline(rI, rL);\n+\n+        boolean res = test64(arr, arr[1], vt);\n+        \/\/ Checks are disabled for non-flattened array because reference comparison\n+        \/\/ fails if C2 scalarizes and re-allocates the value class arguments.\n+        if (TEST64_FLATTENED_ARRAY) {\n+            Asserts.assertTrue(res);\n+            Asserts.assertEQ(arr[1], vt);\n+        }\n+\n+        res = test64(arr, SmallValue.createDefaultInline(), SmallValue.createDefaultInline());\n+        if (TEST64_FLATTENED_ARRAY) {\n+            Asserts.assertFalse(res);\n+            Asserts.assertEQ(arr[1], vt);\n+        }\n+    }\n+\n+    \/\/ compareAndSet to flattened field in object with unknown container\n+    @Test\n+    public boolean test65(Object o, Object oldVal, SmallValue newVal) {\n+        if (TEST63_VT_FLATTENED) {\n+            Asserts.assertTrue(UseFieldFlattening && PreloadClasses);\n+            return U.compareAndSetFlatValue(o, TEST63_VT_OFFSET, TEST63_VT_LAYOUT, SmallValue.class, oldVal, newVal);\n+        } else {\n+            Asserts.assertFalse(UseFieldFlattening && PreloadClasses);\n+            return U.compareAndSetReference(o, TEST63_VT_OFFSET, oldVal, newVal);\n+        }\n+    }\n+\n+    @Run(test = \"test65\")\n+    public void test65_verifier() {\n+        \/\/ Unsafe::compareAndSetFlatValue needs UseArrayFlattening.\n+        if (UseFieldFlattening && !UseArrayFlattening) return;\n+        SmallValue vt = SmallValue.createWithFieldsInline(rI, rL);\n+        test63_vt = SmallValue.createDefaultInline();\n+\n+        boolean res = test65(this, test63_vt, vt);\n+        Asserts.assertTrue(res);\n+        Asserts.assertEQ(test63_vt, vt);\n+\n+        res = test65(this, SmallValue.createDefaultInline(), SmallValue.createDefaultInline());\n+        Asserts.assertFalse(res);\n+        Asserts.assertEQ(test63_vt, vt);\n+    }\n+\n+    \/\/ compareAndSet to flattened field in object, non-inline arguments to compare and set\n+    @Test\n+    public boolean test66(Object oldVal, Object newVal) {\n+        if (TEST63_VT_FLATTENED) {\n+            Asserts.assertTrue(UseFieldFlattening && PreloadClasses);\n+            return U.compareAndSetFlatValue(this, TEST63_VT_OFFSET, TEST63_VT_LAYOUT, SmallValue.class, oldVal, newVal);\n+        } else {\n+            Asserts.assertFalse(UseFieldFlattening && PreloadClasses);\n+            return U.compareAndSetReference(this, TEST63_VT_OFFSET, oldVal, newVal);\n+        }\n+    }\n+\n+    @Run(test = \"test66\")\n+    public void test66_verifier() {\n+        \/\/ Unsafe::compareAndSetFlatValue needs UseArrayFlattening.\n+        if (UseFieldFlattening && !UseArrayFlattening) return;\n+        SmallValue vt = SmallValue.createWithFieldsInline(rI, rL);\n+        test63_vt = SmallValue.createDefaultInline();\n+\n+        boolean res = test66(test63_vt, vt);\n+        Asserts.assertTrue(res);\n+        Asserts.assertEQ(test63_vt, vt);\n+\n+        res = test66(SmallValue.createDefaultInline(), SmallValue.createDefaultInline());\n+        Asserts.assertFalse(res);\n+        Asserts.assertEQ(test63_vt, vt);\n+    }\n+\n+    \/\/ compareAndExchange to flattened field in object\n+    @Test\n+    public Object test67(SmallValue oldVal, SmallValue newVal) {\n+        if (TEST63_VT_FLATTENED) {\n+            Asserts.assertTrue(UseFieldFlattening && PreloadClasses);\n+            return U.compareAndExchangeFlatValue(this, TEST63_VT_OFFSET, TEST63_VT_LAYOUT, SmallValue.class, oldVal, newVal);\n+        } else {\n+            Asserts.assertFalse(UseFieldFlattening && PreloadClasses);\n+            return U.compareAndExchangeReference(this, TEST63_VT_OFFSET, oldVal, newVal);\n+        }\n+    }\n+\n+    @Run(test = \"test67\")\n+    public void test67_verifier() {\n+        \/\/ Unsafe::compareAndExchangeFlatValue needs UseArrayFlattening.\n+        if (UseFieldFlattening && !UseArrayFlattening) return;\n+        SmallValue vt = SmallValue.createWithFieldsInline(rI, rL);\n+        SmallValue oldVal = SmallValue.createDefaultInline();\n+        test63_vt = oldVal;\n+\n+        Object res = test67(test63_vt, vt);\n+        \/\/ Checks are disabled for non-flattened field because reference comparison\n+        \/\/ fails if C2 scalarizes and re-allocates the value class arguments.\n+        if (TEST63_VT_FLATTENED) {\n+            Asserts.assertEQ(res, oldVal);\n+            Asserts.assertEQ(test63_vt, vt);\n+        }\n+\n+        res = test67(SmallValue.createDefaultInline(), SmallValue.createDefaultInline());\n+        if (TEST63_VT_FLATTENED) {\n+            Asserts.assertEQ(res, vt);\n+            Asserts.assertEQ(test63_vt, vt);\n+        }\n+    }\n+\n+    \/\/ compareAndExchange to flattened field in array\n+    @Test\n+    public Object test68(SmallValue[] arr, SmallValue oldVal, Object newVal) {\n+        if (TEST64_FLATTENED_ARRAY) {\n+            Asserts.assertTrue(UseArrayFlattening);\n+            return U.compareAndExchangeFlatValue(arr, TEST64_BASE_OFFSET + TEST64_INDEX_SCALE, TEST64_LAYOUT, SmallValue.class, oldVal, newVal);\n+        } else {\n+            Asserts.assertFalse(UseArrayFlattening);\n+            return U.compareAndExchangeReference(arr, TEST64_BASE_OFFSET + TEST64_INDEX_SCALE, oldVal, newVal);\n+        }\n+    }\n+\n+    @Run(test = \"test68\")\n+    public void test68_verifier() {\n+        Asserts.assertTrue(TEST64_ATOMIC_ARRAY);\n+        SmallValue[] arr = (SmallValue[])ValueClass.newNullRestrictedAtomicArray(SmallValue.class, 2, SmallValue.DEFAULT);\n+        SmallValue vt = SmallValue.createWithFieldsInline(rI, rL);\n+\n+        Object res = test68(arr, arr[1], vt);\n+        \/\/ Checks are disabled for non-flattened array because reference comparison\n+        \/\/ fails if C2 scalarizes and re-allocates the value class arguments.\n+        if (TEST64_FLATTENED_ARRAY) {\n+            Asserts.assertEQ(res, SmallValue.createDefaultInline());\n+            Asserts.assertEQ(arr[1], vt);\n+        }\n+\n+        res = test68(arr, SmallValue.createDefaultInline(), SmallValue.createDefaultInline());\n+        if (TEST64_FLATTENED_ARRAY) {\n+            Asserts.assertEQ(res, vt);\n+            Asserts.assertEQ(arr[1], vt);\n+        }\n+    }\n+\n+    \/\/ compareAndExchange to flattened field in object with unknown container\n+    @Test\n+    public Object test69(Object o, Object oldVal, SmallValue newVal) {\n+        if (TEST63_VT_FLATTENED) {\n+            Asserts.assertTrue(UseFieldFlattening && PreloadClasses);\n+            return U.compareAndExchangeFlatValue(o, TEST63_VT_OFFSET, TEST63_VT_LAYOUT, SmallValue.class, oldVal, newVal);\n+        } else {\n+            Asserts.assertFalse(UseFieldFlattening && PreloadClasses);\n+            return U.compareAndExchangeReference(o, TEST63_VT_OFFSET, oldVal, newVal);\n+        }\n+    }\n+\n+    @Run(test = \"test69\")\n+    public void test69_verifier() {\n+        \/\/ Unsafe::compareAndExchangeFlatValue needs UseArrayFlattening.\n+        if (UseFieldFlattening && !UseArrayFlattening) return;\n+        SmallValue vt = SmallValue.createWithFieldsInline(rI, rL);\n+        SmallValue oldVal = SmallValue.createDefaultInline();\n+        test63_vt = oldVal;\n+\n+        Object res = test69(this, test63_vt, vt);\n+        Asserts.assertEQ(res, oldVal);\n+        Asserts.assertEQ(test63_vt, vt);\n+\n+        res = test69(this, SmallValue.createDefaultInline(), SmallValue.createDefaultInline());\n+        Asserts.assertEQ(res, vt);\n+        Asserts.assertEQ(test63_vt, vt);\n+    }\n+\n+    \/\/ compareAndExchange to flattened field in object, non-inline arguments to compare and set\n+    @Test\n+    public Object test70(Object oldVal, Object newVal) {\n+        if (TEST63_VT_FLATTENED) {\n+            Asserts.assertTrue(UseFieldFlattening && PreloadClasses);\n+            return U.compareAndExchangeFlatValue(this, TEST63_VT_OFFSET, TEST63_VT_LAYOUT, SmallValue.class, oldVal, newVal);\n+        } else {\n+            Asserts.assertFalse(UseFieldFlattening && PreloadClasses);\n+            return U.compareAndExchangeReference(this, TEST63_VT_OFFSET, oldVal, newVal);\n+        }\n+    }\n+\n+    @Run(test = \"test70\")\n+    public void test70_verifier() {\n+        \/\/ Unsafe::compareAndExchangeFlatValue needs UseArrayFlattening.\n+        if (UseFieldFlattening && !UseArrayFlattening) return;\n+        SmallValue vt = SmallValue.createWithFieldsInline(rI, rL);\n+        SmallValue oldVal = SmallValue.createDefaultInline();\n+        test63_vt = oldVal;\n+\n+        Object res = test70(test63_vt, vt);\n+        Asserts.assertEQ(res, oldVal);\n+        Asserts.assertEQ(test63_vt, vt);\n+\n+        res = test70(SmallValue.createDefaultInline(), SmallValue.createDefaultInline());\n+        Asserts.assertEQ(res, vt);\n+        Asserts.assertEQ(test63_vt, vt);\n+    }\n+\n+    \/\/ getValue to retrieve flattened field from (nullable) value class\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public MyValue2 test71(boolean b, MyValue1 v1, MyValue1 v2) {\n+        if (b) {\n+            if (V1_FLATTENED) {\n+                return U.getFlatValue(v1, V1_OFFSET, V1_LAYOUT, MyValue2.class);\n+            }\n+            return (MyValue2)U.getReference(v1, V1_OFFSET);\n+        } else {\n+            if (V1_FLATTENED) {\n+                return U.getFlatValue(v2, V1_OFFSET, V1_LAYOUT, MyValue2.class);\n+            }\n+            return (MyValue2)U.getReference(v2, V1_OFFSET);\n+        }\n+    }\n+\n+    @Run(test = \"test71\")\n+    public void test71_verifier() {\n+        MyValue1 v = MyValue1.createWithFieldsInline(rI, rL);\n+        Asserts.assertEQ(test71(true, v, v), v.v1);\n+        Asserts.assertEQ(test71(false, v, v), v.v1);\n+    }\n+\n+    \/\/ Same as test71 but with non-constant offset\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public MyValue2 test72(boolean b, MyValue1 v1, MyValue1 v2, long offset) {\n+        if (b) {\n+            if (V1_FLATTENED) {\n+                return U.getFlatValue(v1, offset, V1_LAYOUT, MyValue2.class);\n+            }\n+            return (MyValue2)U.getReference(v1, offset);\n+        } else {\n+            if (V1_FLATTENED) {\n+                return U.getFlatValue(v2, offset, V1_LAYOUT, MyValue2.class);\n+            }\n+            return (MyValue2)U.getReference(v2, offset);\n+        }\n+    }\n+\n+    @Run(test = \"test72\")\n+    public void test72_verifier() {\n+        MyValue1 v = MyValue1.createWithFieldsInline(rI, rL);\n+        Asserts.assertEQ(test72(true, v, v, V1_OFFSET), v.v1);\n+        Asserts.assertEQ(test72(false, v, v, V1_OFFSET), v.v1);\n+    }\n+\n+    @Strict\n+    @NullRestricted\n+    static final MyValue1 test73_value1 = MyValue1.createWithFieldsInline(rI, rL);\n+    static final MyValue1 test73_value2 = MyValue1.createWithFieldsInline(rI+1, rL+1);\n+\n+    \/\/ Same as test72 but with constant base\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public MyValue2 test73(boolean b, long offset) {\n+        if (b) {\n+            if (V1_FLATTENED) {\n+                return U.getFlatValue(test73_value1, offset, V1_LAYOUT, MyValue2.class);\n+            }\n+            return (MyValue2)U.getReference(test73_value1, offset);\n+        } else {\n+            if (V1_FLATTENED) {\n+                return U.getFlatValue(test73_value2, offset, V1_LAYOUT, MyValue2.class);\n+            }\n+            return (MyValue2)U.getReference(test73_value2, offset);\n+        }\n+    }\n+\n+    @Run(test = \"test73\")\n+    public void test73_verifier() {\n+        Asserts.assertEQ(test73(true, V1_OFFSET), test73_value1.v1);\n+        Asserts.assertEQ(test73(false, V1_OFFSET), test73_value2.v1);\n+    }\n+\n+    @LooselyConsistentValue\n+    static value class EmptyInline {\n+\n+    }\n+\n+    @LooselyConsistentValue\n+    static value class ByteInline {\n+        byte x = 0;\n+    }\n+\n+    @Test\n+    public void test74(EmptyInline[] emptyArray) {\n+        System.arraycopy(emptyArray, 0, emptyArray, 10, 10);\n+        System.arraycopy(emptyArray, 0, emptyArray, 20, 10);\n+    }\n+\n+    @Run(test = \"test74\")\n+    public void test74_verifier() {\n+        EmptyInline[] emptyArray = (EmptyInline[])ValueClass.newNullRestrictedNonAtomicArray(EmptyInline.class, 100, new EmptyInline());\n+        test74(emptyArray);\n+        for (EmptyInline empty : emptyArray) {\n+            Asserts.assertEQ(empty, new EmptyInline());\n+        }\n+    }\n+\n+    @Test\n+    public void test75(EmptyInline[] emptyArray) {\n+        System.arraycopy(emptyArray, 0, emptyArray, 10, 10);\n+    }\n+\n+    @Run(test = \"test75\")\n+    public void test75_verifier() {\n+        EmptyInline[] emptyArray = (EmptyInline[])ValueClass.newNullRestrictedNonAtomicArray(EmptyInline.class, 100, new EmptyInline());\n+        test75(emptyArray);\n+        for (EmptyInline empty : emptyArray) {\n+            Asserts.assertEQ(empty, new EmptyInline());\n+        }\n+    }\n+\n+    @Test\n+    public void test76(ByteInline[] byteArray) {\n+        System.arraycopy(byteArray, 0, byteArray, 10, 10);\n+        System.arraycopy(byteArray, 0, byteArray, 20, 10);\n+    }\n+\n+    @Run(test = \"test76\")\n+    public void test76_verifier() {\n+        ByteInline[] byteArray = (ByteInline[])ValueClass.newNullRestrictedNonAtomicArray(ByteInline.class, 100, new ByteInline());\n+        test76(byteArray);\n+        for (ByteInline b : byteArray) {\n+            Asserts.assertEQ(b, new ByteInline());\n+        }\n+    }\n+\n+    @Test\n+    public void test77(ByteInline[] byteArray) {\n+        System.arraycopy(byteArray, 0, byteArray, 10, 10);\n+    }\n+\n+    @Run(test = \"test77\")\n+    public void test77_verifier() {\n+        ByteInline[] byteArray = (ByteInline[])ValueClass.newNullRestrictedNonAtomicArray(ByteInline.class, 100, new ByteInline());\n+        test77(byteArray);\n+        for (ByteInline b : byteArray) {\n+            Asserts.assertEQ(b, new ByteInline());\n+        }\n+    }\n+\n+    @Test\n+    public Object test78(MyValue1 vt) {\n+        return NonValueClass.class.cast(vt);\n+    }\n+\n+    @Run(test = \"test78\")\n+    public void test78_verifier() {\n+        Object result = test78(null);\n+        Asserts.assertEQ(result, null);\n+        try {\n+            test78(MyValue1.createWithFieldsInline(rI, rL));\n+            throw new RuntimeException(\"should have thrown\");\n+        } catch (ClassCastException cce) {\n+        }\n+    }\n+\n+    @Test\n+    public Object test79(MyValue1 vt) {\n+        Object tmp = vt;\n+        return (NonValueClass)tmp;\n+    }\n+\n+    @Run(test = \"test79\")\n+    public void test79_verifier() {\n+        Object result = test79(null);\n+        Asserts.assertEQ(result, null);\n+        try {\n+            test79(MyValue1.createWithFieldsInline(rI, rL));\n+            throw new RuntimeException(\"should have thrown\");\n+        } catch (ClassCastException cce) {\n+        }\n+    }\n+\n+    @LooselyConsistentValue\n+    public static value class Test80Value1 {\n+        @Strict\n+        @NullRestricted\n+        Test80Value2 v = new Test80Value2();\n+    }\n+\n+    @LooselyConsistentValue\n+    public static value class Test80Value2 {\n+        long l = rL;\n+        NonValueClass obj = new NonValueClass(rI);\n+    }\n+\n+    \/\/ layout is not a constant\n+    @Test\n+    @IR(counts = {CALL_UNSAFE, \"1\"})\n+    public Test80Value2 test80(Test80Value1 v, boolean flat, int layout, long offset) {\n+        if (flat) {\n+            return U.getFlatValue(v, offset, layout, Test80Value2.class);\n+        } else {\n+            return (Test80Value2)U.getReference(v, offset);\n+        }\n+    }\n+\n+    @Run(test = \"test80\")\n+    public void test80_verifier() throws Exception {\n+        Test80Value1 v = new Test80Value1();\n+        Field field = Test80Value1.class.getDeclaredField(\"v\");\n+        Asserts.assertEQ(test80(v, U.isFlatField(field), U.fieldLayout(field), U.objectFieldOffset(field)), v.v);\n+    }\n+\n+    static value class SimpleValue {\n+        byte x = 1;\n+        byte y = 1;\n+\n+        static SimpleValue DEFAULT = new SimpleValue();\n+    }\n+\n+    private static final SimpleValue[] TEST_ARRAY1 = (SimpleValue[])ValueClass.newNullRestrictedNonAtomicArray(SimpleValue.class, 1, SimpleValue.DEFAULT);\n+    private static final SimpleValue[] TEST_ARRAY2 = (SimpleValue[])ValueClass.newNullRestrictedAtomicArray(SimpleValue.class, 1, SimpleValue.DEFAULT);\n+    private static final SimpleValue[] TEST_ARRAY3 = (SimpleValue[])ValueClass.newNullableAtomicArray(SimpleValue.class, 1);\n+    private static final SimpleValue[] TEST_ARRAY4 = new SimpleValue[1];\n+    private static final boolean TEST_ARRAY1_IS_FLAT = ValueClass.isFlatArray(TEST_ARRAY1);\n+    private static final boolean TEST_ARRAY2_IS_FLAT = ValueClass.isFlatArray(TEST_ARRAY2);\n+    private static final boolean TEST_ARRAY3_IS_FLAT = ValueClass.isFlatArray(TEST_ARRAY3);\n+    private static final boolean TEST_ARRAY4_IS_FLAT = ValueClass.isFlatArray(TEST_ARRAY4);\n+    private static final boolean TEST_ARRAY1_IS_NULL_RESTRICTED = ValueClass.isNullRestrictedArray(TEST_ARRAY1);\n+    private static final boolean TEST_ARRAY2_IS_NULL_RESTRICTED = ValueClass.isNullRestrictedArray(TEST_ARRAY2);\n+    private static final boolean TEST_ARRAY3_IS_NULL_RESTRICTED = ValueClass.isNullRestrictedArray(TEST_ARRAY3);\n+    private static final boolean TEST_ARRAY4_IS_NULL_RESTRICTED = ValueClass.isNullRestrictedArray(TEST_ARRAY4);\n+    private static final boolean TEST_ARRAY1_IS_ATOMIC = ValueClass.isAtomicArray(TEST_ARRAY1);\n+    private static final boolean TEST_ARRAY2_IS_ATOMIC = ValueClass.isAtomicArray(TEST_ARRAY2);\n+    private static final boolean TEST_ARRAY3_IS_ATOMIC = ValueClass.isAtomicArray(TEST_ARRAY3);\n+    private static final boolean TEST_ARRAY4_IS_ATOMIC = ValueClass.isAtomicArray(TEST_ARRAY4);\n+\n+    \/\/ Test correctness of the ValueClass::isFlatArray intrinsic\n+    @Test\n+    @IR(failOn = {STATIC_CALL_OF_METHOD, \"jdk.internal.value.ValueClass::isFlatArray\"})\n+    public boolean test81(Object array) {\n+        return ValueClass.isFlatArray(array);\n+    }\n+\n+    @Run(test = \"test81\")\n+    public void test81_verifier() {\n+        Asserts.assertEQ(test81(TEST_ARRAY1), TEST_ARRAY1_IS_FLAT, \"test81_1 failed\");\n+        Asserts.assertEQ(test81(TEST_ARRAY2), TEST_ARRAY2_IS_FLAT, \"test81_2 failed\");\n+        Asserts.assertEQ(test81(TEST_ARRAY3), TEST_ARRAY3_IS_FLAT, \"test81_3 failed\");\n+        Asserts.assertEQ(test81(TEST_ARRAY4), TEST_ARRAY4_IS_FLAT, \"test81_4 failed\");\n+        Asserts.assertFalse(test81(new String[0]), \"test81_5 failed\");\n+        Asserts.assertFalse(test81(\"test\"), \"test81_6 failed\");\n+        Asserts.assertFalse(test81(new int[0]), \"test81_7 failed\");\n+    }\n+\n+    \/\/ Verify that ValueClass::isFlatArray checks with statically known classes are folded\n+  \/* FIX: JDK-8374116\n+    @Test\n+    @IR(failOn = {LOAD_KLASS, STATIC_CALL_OF_METHOD, \"jdk.internal.value.ValueClass::isFlatArray\"})\n+    public boolean test82() {\n+        boolean check1 = ValueClass.isFlatArray(TEST_ARRAY1);\n+        if (!TEST_ARRAY1_IS_FLAT) {\n+            check1 = !check1;\n+        }\n+        boolean check2 = ValueClass.isFlatArray(TEST_ARRAY2);\n+        if (!TEST_ARRAY2_IS_FLAT) {\n+            check2 = !check2;\n+        }\n+        boolean check3 = ValueClass.isFlatArray(TEST_ARRAY3);\n+        if (!TEST_ARRAY3_IS_FLAT) {\n+            check3 = !check3;\n+        }\n+        boolean check4 = ValueClass.isFlatArray(TEST_ARRAY4);\n+        if (!TEST_ARRAY4_IS_FLAT) {\n+            check4 = !check4;\n+        }\n+        boolean check5 = !ValueClass.isFlatArray(new String[0]);\n+        boolean check6 = !ValueClass.isFlatArray(\"test\");\n+        boolean check7 = !ValueClass.isFlatArray(new int[0]);\n+        return check1 && check2 && check3 && check4 && check5 && check6 && check7;\n+    }\n+\n+    @Run(test = \"test82\")\n+    public void test82_verifier() {\n+        Asserts.assertTrue(test82(), \"test82 failed\");\n+    }\n+  *\/\n+    \/\/ Test that LibraryCallKit::arraycopy_move_allocation_here works as expected\n+    @Test\n+    public MyValue1 test83(Object[] src) {\n+        MyValue1[] dst = (MyValue1[])ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 10, MyValue1.DEFAULT);\n+        System.arraycopy(src, 0, dst, 0, 10);\n+        return dst[0];\n+    }\n+\n+    @Run(test = \"test83\")\n+    public void test83_verifier(RunInfo info) {\n+        if (info.isWarmUp()) {\n+            MyValue1[] src = (MyValue1[])ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 10, MyValue1.DEFAULT);\n+            Asserts.assertEQ(test83(src), src[0]);\n+        } else {\n+            \/\/ Trigger deoptimization to verify that re-execution works\n+            try {\n+                test83(new NonValueClass[10]);\n+                throw new RuntimeException(\"No NullPointerException thrown\");\n+            } catch (NullPointerException npe) {\n+                \/\/ Expected\n+            }\n+        }\n+    }\n+\n+    \/* TODO: 8322547: Unsafe::putInt checks the larval bit which leads to a VM crash\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public MyValue1 test84(MyValue1 v) {\n+        v = U.makePrivateBuffer(v);\n+        for (int i = 0; i < 10; i++) {\n+            U.putInt(v, X_OFFSET, i);\n+        }\n+        U.putInt(v, X_OFFSET, rI);\n+        v = U.finishPrivateBuffer(v);\n+        return v;\n+    }\n+\n+    @Run(test = \"test84\")\n+    public void test84_verifier() {\n+        MyValue1 v1 = MyValue1.createWithFieldsInline(rI, rL);\n+        MyValue1 v2 = test84(MyValue1.setX(v1, 0));\n+        Asserts.assertEQ(v1.hash(), v2.hash());\n+    }\n+    *\/\n+\n+    static value class MyValueClonable implements Cloneable {\n+        int x;\n+\n+        MyValueClonable(int x) {\n+            this.x = x;\n+        }\n+\n+        @Override\n+        public Object clone() throws CloneNotSupportedException {\n+            return super.clone();\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.ALLOC, \"1\"})\n+    public Object testClone() throws CloneNotSupportedException {\n+        MyValueClonable obj = new MyValueClonable(3);\n+        return obj.clone();\n+    }\n+\n+    @Run(test = \"testClone\")\n+    public void testClone_verifier() {\n+        try {\n+            testClone();\n+        } catch (Exception e) {\n+            Asserts.fail(\"testClone() failed\", e);\n+        }\n+    }\n+\n+    \/\/ Test correctness of the ValueClass::isNullRestrictedArray intrinsic\n+    @Test\n+    @IR(failOn = {STATIC_CALL_OF_METHOD, \"jdk.internal.value.ValueClass::isNullRestrictedArray\"})\n+    public boolean test85(Object array) {\n+        return ValueClass.isNullRestrictedArray(array);\n+    }\n+\n+    @Run(test = \"test85\")\n+    public void test85_verifier() {\n+        Asserts.assertEQ(test85(TEST_ARRAY1), TEST_ARRAY1_IS_NULL_RESTRICTED, \"test85_1 failed\");\n+        Asserts.assertEQ(test85(TEST_ARRAY2), TEST_ARRAY2_IS_NULL_RESTRICTED, \"test85_2 failed\");\n+        Asserts.assertEQ(test85(TEST_ARRAY3), TEST_ARRAY3_IS_NULL_RESTRICTED, \"test85_3 failed\");\n+        Asserts.assertEQ(test85(TEST_ARRAY4), TEST_ARRAY4_IS_NULL_RESTRICTED, \"test85_4 failed\");\n+        Asserts.assertFalse(test85(new String[0]), \"test85_5 failed\");\n+        Asserts.assertFalse(test85(\"test\"), \"test85_6 failed\");\n+        Asserts.assertFalse(test85(new int[0]), \"test85_7 failed\");\n+    }\n+\n+    \/\/ Verify that ValueClass::isNullRestrictedArray checks with statically known classes are folded\n+    @Test\n+    @IR(failOn = {LOAD_KLASS, STATIC_CALL_OF_METHOD, \"jdk.internal.value.ValueClass::isNullRestrictedArray\"})\n+    public boolean test86() {\n+        boolean check1 = ValueClass.isNullRestrictedArray(TEST_ARRAY1);\n+        if (!TEST_ARRAY1_IS_NULL_RESTRICTED) {\n+            check1 = !check1;\n+        }\n+        boolean check2 = ValueClass.isNullRestrictedArray(TEST_ARRAY2);\n+        if (!TEST_ARRAY2_IS_NULL_RESTRICTED) {\n+            check2 = !check2;\n+        }\n+        boolean check3 = ValueClass.isNullRestrictedArray(TEST_ARRAY3);\n+        if (!TEST_ARRAY3_IS_NULL_RESTRICTED) {\n+            check3 = !check3;\n+        }\n+        boolean check4 = ValueClass.isNullRestrictedArray(TEST_ARRAY4);\n+        if (!TEST_ARRAY4_IS_NULL_RESTRICTED) {\n+            check4 = !check4;\n+        }\n+        boolean check5 = !ValueClass.isNullRestrictedArray(new String[0]);\n+        boolean check6 = !ValueClass.isNullRestrictedArray(\"test\");\n+        boolean check7 = !ValueClass.isNullRestrictedArray(new int[0]);\n+        return check1 && check2 && check3 && check4 && check5 && check6 && check7;\n+    }\n+\n+    @Run(test = \"test86\")\n+    public void test86_verifier() {\n+        Asserts.assertTrue(test86(), \"test86 failed\");\n+    }\n+\n+    \/\/ Test correctness of the ValueClass::isAtomicArray intrinsic\n+    @Test\n+    \/\/ TODO 8350865 Implemented intrinsic\n+    \/\/ @IR(failOn = {STATIC_CALL_OF_METHOD, \"jdk.internal.value.ValueClass::isAtomicArray\"})\n+    public boolean test87(Object array) {\n+        return ValueClass.isAtomicArray(array);\n+    }\n+\n+    @Run(test = \"test87\")\n+    public void test87_verifier() {\n+        Asserts.assertEQ(test87(TEST_ARRAY1), TEST_ARRAY1_IS_ATOMIC, \"test87_1 failed\");\n+        Asserts.assertEQ(test87(TEST_ARRAY2), TEST_ARRAY2_IS_ATOMIC, \"test87_2 failed\");\n+        Asserts.assertEQ(test87(TEST_ARRAY3), TEST_ARRAY3_IS_ATOMIC, \"test87_3 failed\");\n+        Asserts.assertEQ(test87(TEST_ARRAY4), TEST_ARRAY4_IS_ATOMIC, \"test87_4 failed\");\n+        Asserts.assertTrue(test87(new String[0]), \"test87_5 failed\");\n+        Asserts.assertFalse(test87(\"test\"), \"test87_6 failed\");\n+        Asserts.assertFalse(test87(new int[0]), \"test87_7 failed\");\n+    }\n+\n+    \/\/ Verify that ValueClass::isAtomicArray checks with statically known classes are folded\n+    @Test\n+    \/\/ TODO 8350865 Implemented intrinsic\n+    \/\/ @IR(failOn = {LOAD_KLASS, STATIC_CALL_OF_METHOD, \"jdk.internal.value.ValueClass::isAtomicArray\"})\n+    public boolean test88() {\n+        boolean check1 = ValueClass.isAtomicArray(TEST_ARRAY1);\n+        if (!TEST_ARRAY1_IS_ATOMIC) {\n+            check1 = !check1;\n+        }\n+        boolean check2 = ValueClass.isAtomicArray(TEST_ARRAY2);\n+        if (!TEST_ARRAY2_IS_ATOMIC) {\n+            check2 = !check2;\n+        }\n+        boolean check3 = ValueClass.isAtomicArray(TEST_ARRAY3);\n+        if (!TEST_ARRAY3_IS_ATOMIC) {\n+            check3 = !check3;\n+        }\n+        boolean check4 = ValueClass.isAtomicArray(TEST_ARRAY4);\n+        if (!TEST_ARRAY4_IS_ATOMIC) {\n+            check4 = !check4;\n+        }\n+        boolean check5 = ValueClass.isAtomicArray(new String[0]);\n+        boolean check6 = !ValueClass.isAtomicArray(\"test\");\n+        boolean check7 = !ValueClass.isAtomicArray(new int[0]);\n+        return check1 && check2 && check3 && check4 && check5 && check6 && check7;\n+    }\n+\n+    @Run(test = \"test88\")\n+    public void test88_verifier() {\n+        Asserts.assertTrue(test88(), \"test88 failed\");\n+    }\n+\n+    private static final long BASE_OFF1 = U.arrayInstanceBaseOffset(TEST_ARRAY1);\n+    private static final long BASE_OFF2 = U.arrayInstanceBaseOffset(TEST_ARRAY2);\n+    private static final long BASE_OFF3 = U.arrayInstanceBaseOffset(TEST_ARRAY3);\n+    private static final long BASE_OFF4 = U.arrayInstanceBaseOffset(TEST_ARRAY4);\n+    private static final long BASE_OFF5 = U.arrayInstanceBaseOffset(new Object[1]);\n+\n+    private static final int IDX_SCALE1 = U.arrayInstanceIndexScale(TEST_ARRAY1);\n+    private static final int IDX_SCALE2 = U.arrayInstanceIndexScale(TEST_ARRAY2);\n+    private static final int IDX_SCALE3 = U.arrayInstanceIndexScale(TEST_ARRAY3);\n+    private static final int IDX_SCALE4 = U.arrayInstanceIndexScale(TEST_ARRAY4);\n+    private static final int IDX_SCALE5 = U.arrayInstanceIndexScale(new Object[1]);\n+\n+    private static final int LAYOUT1 = U.arrayLayout(TEST_ARRAY1);\n+    private static final int LAYOUT2 = U.arrayLayout(TEST_ARRAY2);\n+    private static final int LAYOUT3 = U.arrayLayout(TEST_ARRAY3);\n+    private static final int LAYOUT4 = U.arrayLayout(TEST_ARRAY4);\n+    private static final int LAYOUT5 = U.arrayLayout(new Object[1]);\n+\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public long test89(Object[] array) {\n+        return U.arrayInstanceBaseOffset(array);\n+    }\n+\n+    @Run(test = \"test89\")\n+    public void test89_verifier() {\n+        Asserts.assertEquals(test89(TEST_ARRAY1), BASE_OFF1);\n+        Asserts.assertEquals(test89(TEST_ARRAY2), BASE_OFF2);\n+        Asserts.assertEquals(test89(TEST_ARRAY3), BASE_OFF3);\n+        Asserts.assertEquals(test89(TEST_ARRAY4), BASE_OFF4);\n+        Asserts.assertEquals(test89(new Object[1]), BASE_OFF5);\n+    }\n+\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE, LOAD_KLASS, LOAD})\n+    public long test90() {\n+        return U.arrayInstanceBaseOffset(TEST_ARRAY1);\n+    }\n+\n+    @Run(test = \"test90\")\n+    public void test90_verifier() {\n+        Asserts.assertEquals(test90(), BASE_OFF1);\n+    }\n+\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public int test91(Object[] array) {\n+        return U.arrayInstanceIndexScale(array);\n+    }\n+\n+    @Run(test = \"test91\")\n+    public void test91_verifier() {\n+        Asserts.assertEquals(test91(TEST_ARRAY1), IDX_SCALE1);\n+        Asserts.assertEquals(test91(TEST_ARRAY2), IDX_SCALE2);\n+        Asserts.assertEquals(test91(TEST_ARRAY3), IDX_SCALE3);\n+        Asserts.assertEquals(test91(TEST_ARRAY4), IDX_SCALE4);\n+        Asserts.assertEquals(test91(new Object[1]), IDX_SCALE5);\n+    }\n+\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE, LOAD_KLASS, LOAD})\n+    public int test92() {\n+        return U.arrayInstanceIndexScale(TEST_ARRAY1);\n+    }\n+\n+    @Run(test = \"test92\")\n+    public void test92_verifier() {\n+        Asserts.assertEquals(test92(), IDX_SCALE1);\n+    }\n+\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public int test93(Object[] array) {\n+        return U.arrayLayout(array);\n+    }\n+\n+    @Run(test = \"test93\")\n+    public void test93_verifier() {\n+        Asserts.assertEquals(test93(TEST_ARRAY1), LAYOUT1);\n+        Asserts.assertEquals(test93(TEST_ARRAY2), LAYOUT2);\n+        Asserts.assertEquals(test93(TEST_ARRAY3), LAYOUT3);\n+        Asserts.assertEquals(test93(TEST_ARRAY4), LAYOUT4);\n+        Asserts.assertEquals(test93(new Object[1]), LAYOUT5);\n+    }\n+\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE, LOAD_KLASS, LOAD})\n+    public int test94() {\n+        return U.arrayLayout(TEST_ARRAY1);\n+    }\n+\n+    @Run(test = \"test94\")\n+    public void test94_verifier() {\n+        Asserts.assertEquals(test94(), LAYOUT1);\n+    }\n+\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE, LOAD_KLASS, LOAD})\n+    public int test95() {\n+        int res = 0;\n+        int[] map = U.getFieldMap(MyValue1.class);\n+        for (int i = 0; i < map.length; i++) {\n+            res += map[1];\n+        }\n+        return res;\n+    }\n+\n+    static int test95ExpectedResult = -1;\n+\n+    @Run(test = \"test95\")\n+    public void test95_verifier() {\n+        if (test95ExpectedResult == -1) {\n+            test95ExpectedResult = test95();\n+        }\n+        Asserts.assertEQ(test95(), test95ExpectedResult);\n+    }\n+\n+    @Test\n+    @IR(failOn = {CALL_UNSAFE})\n+    public int test96(Object obj) {\n+        int res = 0;\n+        int[] map = U.getFieldMap(obj.getClass());\n+        for (int i = 0; i < map.length; i++) {\n+            res += map[1];\n+        }\n+        return res;\n+    }\n+\n+    static int test96ExpectedResult = -1;\n+\n+    @Run(test = \"test96\")\n+    public void test96_verifier() {\n+        MyValue1 v = MyValue1.createWithFieldsInline(rI, rL);\n+        if (test96ExpectedResult == -1) {\n+            test96ExpectedResult = test96(v);\n+        }\n+        Asserts.assertEQ(test96(v), test96ExpectedResult);\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestIntrinsics.java","additions":2175,"deletions":0,"binary":false,"changes":2175,"status":"added"},{"patch":"@@ -507,0 +507,5 @@\n+java\/lang\/ModuleLayer\/LayerControllerTest.java                  8337048 generic-all\n+java\/lang\/ModuleLayer\/BasicLayerTest.java                       8337048 generic-all\n+\n+java\/lang\/Thread\/virtual\/stress\/Skynet.java#default             8342977 generic-all\n+\n@@ -524,0 +529,1 @@\n+com\/sun\/management\/HotSpotDiagnosticMXBean\/DumpThreadsWithEliminatedLock.java 8360599 generic-all\n@@ -545,0 +551,2 @@\n+java\/net\/CookieHandler\/B6644726.java                            8365811 generic-all\n+\n@@ -684,0 +692,4 @@\n+com\/sun\/jdi\/cds\/CDSBreakpointTest.java                          8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSDeleteAllBkptsTest.java                      8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSFieldWatchpoints.java                        8304168 generic-all\n+\n@@ -778,0 +790,1 @@\n+\n@@ -784,0 +797,16 @@\n+\n+############################################################################\n+\n+# valhalla\n+java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessValue.java 8367346 generic-all\n+\n+jdk\/classfile\/AccessFlagsTest.java 8366270 generic-all\n+jdk\/jfr\/event\/runtime\/TestClassLoaderStatsEvent.java 8366820 generic-all\n+\n+jdk\/internal\/vm\/Continuation\/Fuzz.java#default 8370177 generic-aarch64\n+jdk\/internal\/vm\/Continuation\/Fuzz.java#preserve-fp 8370177 generic-aarch64\n+\n+sun\/tools\/jhsdb\/BasicLauncherTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTestWithActiveProcess.java 8366806 generic-all\n+sun\/tools\/jhsdb\/JShellHeapDumpTest.java 8366806 generic-all\n","filename":"test\/jdk\/ProblemList.txt","additions":29,"deletions":0,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n- * @bug 8246774\n+ * @bug 8246774 8326879\n@@ -29,0 +29,1 @@\n+ * @run testng\/othervm --enable-preview BasicRecordSer\n","filename":"test\/jdk\/java\/io\/Serializable\/records\/BasicRecordSer.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n- * @bug 8246774\n+ * @bug 8246774 8326879\n@@ -29,0 +29,1 @@\n+ * @run testng\/othervm --enable-preview RecordClassTest\n","filename":"test\/jdk\/java\/io\/Serializable\/records\/RecordClassTest.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -64,1 +64,2 @@\n-\n+tools\/javac\/preview\/PreviewAutoSuppress.java                                    8374021    generic-all\n+tools\/javac\/processing\/filer\/TestOriginatingElements.java                       8373825    generic-all\n@@ -70,1 +71,0 @@\n-\n","filename":"test\/langtools\/ProblemList.txt","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"}]}