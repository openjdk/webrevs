{"files":[{"patch":"@@ -1262,1 +1262,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -205,0 +205,6 @@\n+    int base_offset = arrayOopDesc::length_offset_in_bytes() + BytesPerInt;\n+    if (!is_aligned(base_offset, BytesPerWord)) {\n+      assert(is_aligned(base_offset, BytesPerInt), \"must be 4-byte aligned\");\n+      \/\/ Clear gap\/first 4 bytes following the length field.\n+      strw(zr, Address(obj, base_offset));\n+    }\n@@ -284,1 +290,1 @@\n-void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int header_size, int f, Register klass, Label& slow_case) {\n+void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int base_offset_in_bytes, int f, Register klass, Label& slow_case) {\n@@ -297,1 +303,1 @@\n-  mov(arr_size, (int32_t)header_size * BytesPerWord + MinObjAlignmentInBytesMask);\n+  mov(arr_size, (int32_t)base_offset_in_bytes + MinObjAlignmentInBytesMask);\n@@ -305,0 +311,3 @@\n+  \/\/ Align-up to word boundary, because we clear the 4 bytes potentially\n+  \/\/ following the length field in initialize_header().\n+  int base_offset = align_up(base_offset_in_bytes, BytesPerWord);\n@@ -306,1 +315,1 @@\n-  initialize_body(obj, arr_size, header_size * BytesPerWord, t1, t2);\n+  initialize_body(obj, arr_size, base_offset, t1, t2);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":12,"deletions":3,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/g1\/g1HeapRegion.hpp\"\n@@ -32,1 +33,0 @@\n-#include \"gc\/g1\/heapRegion.hpp\"\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -971,1 +971,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -2301,1 +2301,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -63,1 +63,1 @@\n-define_pd_global(intx,  CodeEntryAlignment,    128);\n+define_pd_global(intx,  CodeEntryAlignment,    64);\n","filename":"src\/hotspot\/cpu\/ppc\/globals_ppc.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2385,1 +2385,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1658,1 +1658,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -202,0 +202,9 @@\n+#ifdef _LP64\n+    int base_offset = arrayOopDesc::length_offset_in_bytes() + BytesPerInt;\n+    if (!is_aligned(base_offset, BytesPerWord)) {\n+      assert(is_aligned(base_offset, BytesPerInt), \"must be 4-byte aligned\");\n+      \/\/ Clear gap\/first 4 bytes following the length field.\n+      xorl(t1, t1);\n+      movl(Address(obj, base_offset), t1);\n+    }\n+#endif\n@@ -285,1 +294,1 @@\n-void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int header_size, Address::ScaleFactor f, Register klass, Label& slow_case) {\n+void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int base_offset_in_bytes, Address::ScaleFactor f, Register klass, Label& slow_case) {\n@@ -298,1 +307,1 @@\n-  movptr(arr_size, header_size * BytesPerWord + MinObjAlignmentInBytesMask);\n+  movptr(arr_size, base_offset_in_bytes + MinObjAlignmentInBytesMask);\n@@ -308,1 +317,4 @@\n-  initialize_body(obj, arr_size, header_size * BytesPerWord, len_zero);\n+  \/\/ Align-up to word boundary, because we clear the 4 bytes potentially\n+  \/\/ following the length field in initialize_header().\n+  int base_offset = align_up(base_offset_in_bytes, BytesPerWord);\n+  initialize_body(obj, arr_size, base_offset, len_zero);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":15,"deletions":3,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/g1\/g1HeapRegion.hpp\"\n@@ -32,1 +33,0 @@\n-#include \"gc\/g1\/heapRegion.hpp\"\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4487,1 +4487,1 @@\n-  predicate(UseAVX > 0 && !SuperWord::is_reduction(n));\n+  predicate(UseAVX > 0 && !VLoopReductions::is_reduction(n));\n@@ -4498,1 +4498,1 @@\n-  predicate(UseAVX > 0 && SuperWord::is_reduction(n));\n+  predicate(UseAVX > 0 && VLoopReductions::is_reduction(n));\n@@ -4512,1 +4512,1 @@\n-  predicate(UseAVX > 0 && !SuperWord::is_reduction(n));\n+  predicate(UseAVX > 0 && !VLoopReductions::is_reduction(n));\n@@ -4523,1 +4523,1 @@\n-  predicate(UseAVX > 0 && SuperWord::is_reduction(n));\n+  predicate(UseAVX > 0 && VLoopReductions::is_reduction(n));\n@@ -4537,1 +4537,1 @@\n-  predicate(UseAVX > 0 && !SuperWord::is_reduction(n));\n+  predicate(UseAVX > 0 && !VLoopReductions::is_reduction(n));\n@@ -4548,1 +4548,1 @@\n-  predicate(UseAVX > 0 && SuperWord::is_reduction(n));\n+  predicate(UseAVX > 0 && VLoopReductions::is_reduction(n));\n@@ -4562,1 +4562,1 @@\n-  predicate(UseAVX > 0 && !SuperWord::is_reduction(n));\n+  predicate(UseAVX > 0 && !VLoopReductions::is_reduction(n));\n@@ -4573,1 +4573,1 @@\n-  predicate(UseAVX > 0 && SuperWord::is_reduction(n));\n+  predicate(UseAVX > 0 && VLoopReductions::is_reduction(n));\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -1502,0 +1502,18 @@\n+void InstructForm::forms_do(FormClosure *f) {\n+  if (_cisc_spill_alternate) f->do_form(_cisc_spill_alternate);\n+  if (_short_branch_form) f->do_form(_short_branch_form);\n+  _localNames.forms_do(f);\n+  if (_matrule) f->do_form(_matrule);\n+  if (_opcode) f->do_form(_opcode);\n+  if (_insencode) f->do_form(_insencode);\n+  if (_constant) f->do_form(_constant);\n+  if (_attribs) f->do_form(_attribs);\n+  if (_predicate) f->do_form(_predicate);\n+  _effects.forms_do(f);\n+  if (_exprule) f->do_form(_exprule);\n+  if (_rewrule) f->do_form(_rewrule);\n+  if (_format) f->do_form(_format);\n+  if (_peephole) f->do_form(_peephole);\n+  assert(_components.count() == 0, \"skip components\");\n+}\n+\n@@ -1619,0 +1637,8 @@\n+\n+void EncodeForm::forms_do(FormClosure* f) {\n+  const char *name;\n+  for (_eclasses.reset(); (name = _eclasses.iter()) != nullptr;) {\n+    f->do_form((EncClass*)_encClass[name]);\n+  }\n+}\n+\n@@ -1709,0 +1735,9 @@\n+void EncClass::forms_do(FormClosure *f) {\n+  _parameter_type.reset();\n+  const char *type = _parameter_type.iter();\n+  for ( ; type != nullptr ; type = _parameter_type.iter() ) {\n+    f->do_form_by_name(type);\n+  }\n+  _localNames.forms_do(f);\n+}\n+\n@@ -1839,0 +1874,9 @@\n+void InsEncode::forms_do(FormClosure *f) {\n+  _encoding.reset();\n+  NameAndList *encoding = (NameAndList*)_encoding.iter();\n+  for( ; encoding != nullptr; encoding = (NameAndList*)_encoding.iter() ) {\n+    \/\/ just check name, other operands will be checked as instruction parameters\n+    f->do_form_by_name(encoding->name());\n+  }\n+}\n+\n@@ -1972,0 +2016,13 @@\n+void ExpandRule::forms_do(FormClosure *f) {\n+  NameAndList *expand_instr = nullptr;\n+  \/\/ Iterate over the instructions 'node' expands into\n+  for(reset_instructions(); (expand_instr = iter_instructions()) != nullptr; ) {\n+    f->do_form_by_name(expand_instr->name());\n+  }\n+  _newopers.reset();\n+  const char* oper = _newopers.iter();\n+  for(; oper != nullptr; oper = _newopers.iter()) {\n+    f->do_form_by_name(oper);\n+  }\n+}\n+\n@@ -1988,0 +2045,6 @@\n+void RewriteRule::forms_do(FormClosure *f) {\n+  if (_condition) f->do_form(_condition);\n+  if (_instrs) f->do_form(_instrs);\n+  if (_opers) f->do_form(_opers);\n+}\n+\n@@ -2070,0 +2133,7 @@\n+void OpClassForm::forms_do(FormClosure* f) {\n+  const char *name;\n+  for(_oplst.reset(); (name = _oplst.iter()) != nullptr;) {\n+    f->do_form_by_name(name);\n+  }\n+}\n+\n@@ -2695,0 +2765,16 @@\n+void OperandForm::forms_do(FormClosure* f) {\n+  if (_matrule)    f->do_form(_matrule);\n+  if (_interface)  f->do_form(_interface);\n+  if (_attribs)    f->do_form(_attribs);\n+  if (_predicate)  f->do_form(_predicate);\n+  if (_constraint) f->do_form(_constraint);\n+  if (_construct)  f->do_form(_construct);\n+  if (_format)     f->do_form(_format);\n+  _localNames.forms_do(f);\n+  const char* opclass = nullptr;\n+  for ( _classes.reset(); (opclass = _classes.iter()) != nullptr; ) {\n+    f->do_form_by_name(opclass);\n+  }\n+  assert(_components.count() == 0, \"skip _compnets\");\n+}\n+\n@@ -2716,0 +2802,4 @@\n+void Constraint::forms_do(FormClosure *f) {\n+  f->do_form_by_name(_arg);\n+}\n+\n@@ -3543,0 +3633,6 @@\n+void MatchNode::forms_do(FormClosure *f) {\n+  f->do_form_by_name(_name);\n+  if (_lChild) f->do_form(_lChild);\n+  if (_rChild) f->do_form(_rChild);\n+}\n+\n@@ -3612,0 +3708,1 @@\n+\n@@ -4338,0 +4435,12 @@\n+void MatchRule::forms_do(FormClosure* f) {\n+  \/\/ keep sync with MatchNode::forms_do\n+  f->do_form_by_name(_name);\n+  if (_lChild) f->do_form(_lChild);\n+  if (_rChild) f->do_form(_rChild);\n+\n+  \/\/ handle next rule\n+  if (_next) {\n+    f->do_form(_next);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":109,"deletions":0,"binary":false,"changes":109,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -189,0 +189,3 @@\n+  \/\/ Check defined operands are used\n+  AD.check_usage();\n+\n","filename":"src\/hotspot\/share\/adlc\/main.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -311,10 +311,8 @@\n-\n-static int sort_pairs(BlockPair** a, BlockPair** b) {\n-  if ((*a)->from() == (*b)->from()) {\n-    return (*a)->to()->block_id() - (*b)->to()->block_id();\n-  } else {\n-    return (*a)->from()->block_id() - (*b)->from()->block_id();\n-  }\n-}\n-\n-\n+\/\/ The functionality of this class is to insert a new block between\n+\/\/ the 'from' and 'to' block of a critical edge.\n+\/\/ It first collects the block pairs, and then processes them.\n+\/\/\n+\/\/ Some instructions may introduce more than one edge between two blocks.\n+\/\/ By checking if the current 'to' block sets critical_edge_split_flag\n+\/\/ (all new blocks set this flag) we can avoid repeated processing.\n+\/\/ This is why BlockPair contains the index rather than the original 'to' block.\n@@ -323,1 +321,0 @@\n-  IR*       _ir;\n@@ -326,1 +323,4 @@\n-  CriticalEdgeFinder(IR* ir): _ir(ir) {}\n+  CriticalEdgeFinder(IR* ir) {\n+    ir->iterate_preorder(this);\n+  }\n+\n@@ -334,1 +334,1 @@\n-          blocks.append(new BlockPair(bb, sux));\n+          blocks.append(new BlockPair(bb, i));\n@@ -341,2 +341,0 @@\n-    BlockPair* last_pair = nullptr;\n-    blocks.sort(sort_pairs);\n@@ -345,2 +343,6 @@\n-      if (last_pair != nullptr && pair->is_same(last_pair)) continue;\n-      BlockBegin* to = pair->to();\n+      int index = pair->index();\n+      BlockBegin* to = from->end()->sux_at(index);\n+      if (to->is_set(BlockBegin::critical_edge_split_flag)) {\n+        \/\/ inserted\n+        continue;\n+      }\n@@ -355,1 +357,0 @@\n-      last_pair = pair;\n@@ -362,2 +363,0 @@\n-\n-  iterate_preorder(&cef);\n","filename":"src\/hotspot\/share\/c1\/c1_IR.cpp","additions":20,"deletions":21,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -701,0 +701,2 @@\n+  assert(!sux->is_set(critical_edge_split_flag), \"sanity check\");\n+\n","filename":"src\/hotspot\/share\/c1\/c1_Instruction.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -2541,1 +2541,1 @@\n-  BlockBegin* _to;\n+  int _index; \/\/ sux index of 'to' block\n@@ -2543,1 +2543,1 @@\n-  BlockPair(BlockBegin* from, BlockBegin* to): _from(from), _to(to) {}\n+  BlockPair(BlockBegin* from, int index): _from(from), _index(index) {}\n@@ -2545,5 +2545,1 @@\n-  BlockBegin* to() const   { return _to;   }\n-  bool is_same(BlockBegin* from, BlockBegin* to) const { return  _from == from && _to == to; }\n-  bool is_same(BlockPair* p) const { return  _from == p->from() && _to == p->to(); }\n-  void set_to(BlockBegin* b)   { _to = b; }\n-  void set_from(BlockBegin* b) { _from = b; }\n+  int index() const        { return _index; }\n","filename":"src\/hotspot\/share\/c1\/c1_Instruction.hpp","additions":4,"deletions":8,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-#include \"gc\/g1\/heapRegion.hpp\"\n+#include \"gc\/g1\/g1HeapRegion.hpp\"\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -74,1 +74,1 @@\n-#include \"gc\/g1\/heapRegion.hpp\"\n+#include \"gc\/g1\/g1HeapRegion.hpp\"\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1336,0 +1336,3 @@\n+    \/\/ NMT: fix up the space tags\n+    MemTracker::record_virtual_memory_type(archive_space_rs.base(), mtClassShared);\n+    MemTracker::record_virtual_memory_type(class_space_rs.base(), mtClass);\n@@ -1365,1 +1368,1 @@\n-                                                     ccs_begin_offset);\n+                                                     ccs_begin_offset, mtClassShared, mtClass);\n@@ -1372,3 +1375,0 @@\n-  \/\/ NMT: fix up the space tags\n-  MemTracker::record_virtual_memory_type(archive_space_rs.base(), mtClassShared);\n-  MemTracker::record_virtual_memory_type(class_space_rs.base(), mtClass);\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-#include \"gc\/g1\/heapRegion.hpp\"\n+#include \"gc\/g1\/g1HeapRegion.hpp\"\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -65,0 +65,1 @@\n+  _marking_stats_cache = nullptr;\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCompactionManager.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1973,0 +1973,1 @@\n+    cm->create_marking_stats_cache();\n@@ -2021,0 +2022,7 @@\n+static void flush_marking_stats_cache(const uint num_workers) {\n+  for (uint i = 0; i < num_workers; ++i) {\n+    ParCompactionManager* cm = ParCompactionManager::gc_thread_compaction_manager(i);\n+    cm->flush_and_destroy_marking_stats_cache();\n+  }\n+}\n+\n@@ -2050,0 +2058,6 @@\n+  {\n+    GCTraceTime(Debug, gc, phases) tm(\"Flush Marking Stats\", &_gc_timer);\n+\n+    flush_marking_stats_cache(active_gc_threads);\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -49,3 +49,0 @@\n-class WorkerTask;\n-class AdaptiveSizePolicy;\n-class BarrierSet;\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1734,0 +1734,15 @@\n+bool ShenandoahBarrierC2Support::merge_point_safe(Node* region) {\n+  for (DUIterator_Fast imax, i = region->fast_outs(imax); i < imax; i++) {\n+    Node* n = region->fast_out(i);\n+    if (n->is_LoadStore()) {\n+      \/\/ Splitting a LoadStore node through phi, causes it to lose its SCMemProj: the split if code doesn't have support\n+      \/\/ for a LoadStore at the region the if is split through because that's not expected to happen (LoadStore nodes\n+      \/\/ should be between barrier nodes). It does however happen with Shenandoah though because barriers can get\n+      \/\/ expanded around a LoadStore node.\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+\n@@ -1738,1 +1753,1 @@\n-    if (phase->can_split_if(n_ctrl)) {\n+    if (phase->can_split_if(n_ctrl) && merge_point_safe(n_ctrl)) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.cpp","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -53,1 +53,11 @@\n-  const size_t header = arrayOopDesc::header_size(element_type);\n+\n+  \/\/ Clear leading 32 bits, if necessary.\n+  int base_offset = arrayOopDesc::base_offset_in_bytes(element_type);\n+  if (!is_aligned(base_offset, HeapWordSize)) {\n+    assert(is_aligned(base_offset, BytesPerInt), \"array base must be 32 bit aligned\");\n+    *reinterpret_cast<jint*>(reinterpret_cast<char*>(mem) + base_offset) = 0;\n+    base_offset += BytesPerInt;\n+  }\n+  assert(is_aligned(base_offset, HeapWordSize), \"remaining array base must be 64 bit aligned\");\n+\n+  const size_t header = heap_word_size(base_offset);\n","filename":"src\/hotspot\/share\/gc\/x\/xObjArrayAllocator.cpp","additions":12,"deletions":2,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -54,1 +54,11 @@\n-  const size_t header = arrayOopDesc::header_size(element_type);\n+\n+  \/\/ Clear leading 32 bits, if necessary.\n+  int base_offset = arrayOopDesc::base_offset_in_bytes(element_type);\n+  if (!is_aligned(base_offset, HeapWordSize)) {\n+    assert(is_aligned(base_offset, BytesPerInt), \"array base must be 32 bit aligned\");\n+    *reinterpret_cast<jint*>(reinterpret_cast<char*>(mem) + base_offset) = 0;\n+    base_offset += BytesPerInt;\n+  }\n+  assert(is_aligned(base_offset, HeapWordSize), \"remaining array base must be 64 bit aligned\");\n+\n+  const size_t header = heap_word_size(base_offset);\n","filename":"src\/hotspot\/share\/gc\/z\/zObjArrayAllocator.cpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -185,1 +185,1 @@\n-  JNIEXPORT result_type JNICALL c2v_ ## name signature { \\\n+  result_type JNICALL c2v_ ## name signature {           \\\n@@ -196,1 +196,1 @@\n-  JNIEXPORT result_type JNICALL c2v_ ## name signature { \\\n+  result_type JNICALL c2v_ ## name signature {           \\\n@@ -212,1 +212,1 @@\n-  JNIEXPORT result_type JNICALL c2v_ ## name signature { \\\n+  result_type JNICALL c2v_ ## name signature {           \\\n@@ -2445,1 +2445,1 @@\n-  return arrayOopDesc::header_size(type) * HeapWordSize;\n+  return arrayOopDesc::base_offset_in_bytes(type);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -46,1 +46,1 @@\n-#include \"gc\/g1\/heapRegion.hpp\"\n+#include \"gc\/g1\/g1HeapRegion.hpp\"\n@@ -1029,8 +1029,0 @@\n-extern \"C\" {\n-JNIEXPORT VMStructEntry* jvmciHotSpotVMStructs = JVMCIVMStructs::localHotSpotVMStructs;\n-JNIEXPORT VMTypeEntry* jvmciHotSpotVMTypes = JVMCIVMStructs::localHotSpotVMTypes;\n-JNIEXPORT VMIntConstantEntry* jvmciHotSpotVMIntConstants = JVMCIVMStructs::localHotSpotVMIntConstants;\n-JNIEXPORT VMLongConstantEntry* jvmciHotSpotVMLongConstants = JVMCIVMStructs::localHotSpotVMLongConstants;\n-JNIEXPORT VMAddressEntry* jvmciHotSpotVMAddresses = JVMCIVMStructs::localHotSpotVMAddresses;\n-}\n-\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":2,"deletions":10,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -47,16 +48,1 @@\n-protected:\n-  \/\/ Header size computation.\n-  \/\/ The header is considered the oop part of this type plus the length.\n-  \/\/ Returns the aligned header_size_in_bytes.  This is not equivalent to\n-  \/\/ sizeof(arrayOopDesc) which should not appear in the code.\n-  static int header_size_in_bytes() {\n-    size_t hs = align_up(length_offset_in_bytes() + sizeof(int),\n-                              HeapWordSize);\n-#ifdef ASSERT\n-    \/\/ make sure it isn't called before UseCompressedOops is initialized.\n-    static size_t arrayoopdesc_hs = 0;\n-    if (arrayoopdesc_hs == 0) arrayoopdesc_hs = hs;\n-    assert(arrayoopdesc_hs == hs, \"header size can't change\");\n-#endif \/\/ ASSERT\n-    return (int)hs;\n-  }\n+private:\n@@ -70,3 +56,1 @@\n-  \/\/ Check whether an element of a typeArrayOop with the given type must be\n-  \/\/ aligned 0 mod 8.  The typeArrayOop itself must be aligned at least this\n-  \/\/ strongly.\n+  \/\/ Given a type, return true if elements of that type must be aligned to 64-bit.\n@@ -74,0 +58,5 @@\n+#ifdef _LP64\n+    if (type == T_OBJECT || type == T_ARRAY || type == T_PRIMITIVE_OBJECT) {\n+      return !UseCompressedOops;\n+    }\n+#endif\n@@ -78,0 +67,14 @@\n+  \/\/ Header size computation.\n+  \/\/ The header is considered the oop part of this type plus the length.\n+  \/\/ This is not equivalent to sizeof(arrayOopDesc) which should not appear in the code.\n+  static int header_size_in_bytes() {\n+    size_t hs = length_offset_in_bytes() + sizeof(int);\n+#ifdef ASSERT\n+    \/\/ make sure it isn't called before UseCompressedOops is initialized.\n+    static size_t arrayoopdesc_hs = 0;\n+    if (arrayoopdesc_hs == 0) arrayoopdesc_hs = hs;\n+    assert(arrayoopdesc_hs == hs, \"header size can't change\");\n+#endif \/\/ ASSERT\n+    return (int)hs;\n+  }\n+\n@@ -88,1 +91,2 @@\n-    return header_size(type) * HeapWordSize;\n+    size_t hs = header_size_in_bytes();\n+    return (int)(element_type_should_be_aligned(type) ? align_up(hs, BytesPerLong) : hs);\n@@ -125,12 +129,1 @@\n-  \/\/ Should only be called with constants as argument\n-  \/\/ (will not constant fold otherwise)\n-  \/\/ Returns the header size in words aligned to the requirements of the\n-  \/\/ array object type.\n-  static int header_size(BasicType type) {\n-    size_t typesize_in_bytes = header_size_in_bytes();\n-    return (int)(element_type_should_be_aligned(type)\n-      ? align_object_offset(typesize_in_bytes\/HeapWordSize)\n-      : typesize_in_bytes\/HeapWordSize);\n-  }\n-\n-  \/\/ Return the maximum length of an array of BasicType.  The length can passed\n+  \/\/ Return the maximum length of an array of BasicType.  The length can be passed\n@@ -144,0 +137,4 @@\n+    size_t hdr_size_in_bytes = base_offset_in_bytes(type);\n+    \/\/ This is rounded-up and may overlap with the first array elements.\n+    size_t hdr_size_in_words = align_up(hdr_size_in_bytes, HeapWordSize) \/ HeapWordSize;\n+\n@@ -145,1 +142,1 @@\n-      align_down((SIZE_MAX\/HeapWordSize - header_size(type)), MinObjAlignment);\n+      align_down((SIZE_MAX\/HeapWordSize - hdr_size_in_words), MinObjAlignment);\n@@ -153,1 +150,1 @@\n-      return align_down(max_jint - header_size(type), MinObjAlignment);\n+      return align_down(max_jint - hdr_size_in_words, MinObjAlignment);\n","filename":"src\/hotspot\/share\/oops\/arrayOop.hpp","additions":32,"deletions":35,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -209,1 +209,1 @@\n-  max_size -= arrayOopDesc::header_size(T_PRIMITIVE_OBJECT);\n+  max_size -= (arrayOopDesc::base_offset_in_bytes(T_PRIMITIVE_OBJECT) >> LogHeapWordSize);\n","filename":"src\/hotspot\/share\/oops\/flatArrayKlass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -46,2 +46,0 @@\n-  } else if (*((juint*)this) == badMetaWordVal) {\n-    st->print_cr(\"BAD META WORD\");\n@@ -61,2 +59,0 @@\n-  } else if (*((juint*)this) == badMetaWordVal) {\n-    st->print_cr(\"BAD META WORD\");\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -162,1 +162,4 @@\n-  DecoratorSet decorators = C2_READ_ACCESS | C2_CONTROL_DEPENDENT_LOAD | IN_HEAP | C2_ARRAY_COPY;\n+  \/\/ Pin the load: if this is an array load, it's going to be dependent on a condition that's not a range check for that\n+  \/\/ access. If that condition is replaced by an identical dominating one, then an unpinned load would risk floating\n+  \/\/ above runtime checks that guarantee it is within bounds.\n+  DecoratorSet decorators = C2_READ_ACCESS | C2_CONTROL_DEPENDENT_LOAD | IN_HEAP | C2_ARRAY_COPY | C2_UNKNOWN_CONTROL_LOAD;\n","filename":"src\/hotspot\/share\/opto\/arraycopynode.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2227,1 +2227,1 @@\n-    if (cgr != nullptr && cgr->not_global_escape(obj_node())) {\n+    if (cgr != nullptr && cgr->can_eliminate_lock(this)) {\n@@ -2390,0 +2390,1 @@\n+        box->set_nested();\n@@ -2424,1 +2425,1 @@\n-    if (cgr != nullptr && cgr->not_global_escape(obj_node())) {\n+    if (cgr != nullptr && cgr->can_eliminate_lock(this)) {\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -433,0 +433,3 @@\n+\n+  static IfNode* make_with_same_profile(IfNode* if_node_profile, Node* ctrl, BoolNode* bol);\n+\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+#include \"opto\/locknode.hpp\"\n@@ -5398,0 +5399,2 @@\n+    AbstractLockNode* alock = locks.at(0);\n+    BoxLockNode* box = alock->box_node()->as_BoxLock();\n@@ -5402,0 +5405,13 @@\n+      BoxLockNode* this_box = lock->box_node()->as_BoxLock();\n+      if (this_box != box) {\n+        \/\/ Locking regions (BoxLock) could be Unbalanced here:\n+        \/\/  - its coarsened locks were eliminated in earlier\n+        \/\/    macro nodes elimination followed by loop unroll\n+        \/\/ Preserve Unbalanced status in such cases.\n+        if (!this_box->is_unbalanced()) {\n+          this_box->set_coarsened();\n+        }\n+        if (!box->is_unbalanced()) {\n+          box->set_coarsened();\n+        }\n+      }\n@@ -5479,0 +5495,32 @@\n+\/\/ Mark locking regions (identified by BoxLockNode) as unbalanced if\n+\/\/ locks coarsening optimization removed Lock\/Unlock nodes from them.\n+\/\/ Such regions become unbalanced because coarsening only removes part\n+\/\/ of Lock\/Unlock nodes in region. As result we can't execute other\n+\/\/ locks elimination optimizations which assume all code paths have\n+\/\/ corresponding pair of Lock\/Unlock nodes - they are balanced.\n+void Compile::mark_unbalanced_boxes() const {\n+  int count = coarsened_count();\n+  for (int i = 0; i < count; i++) {\n+    Node_List* locks_list = _coarsened_locks.at(i);\n+    uint size = locks_list->size();\n+    if (size > 0) {\n+      AbstractLockNode* alock = locks_list->at(0)->as_AbstractLock();\n+      BoxLockNode* box = alock->box_node()->as_BoxLock();\n+      if (alock->is_coarsened()) {\n+        \/\/ coarsened_locks_consistent(), which is called before this method, verifies\n+        \/\/ that the rest of Lock\/Unlock nodes on locks_list are also coarsened.\n+        assert(!box->is_eliminated(), \"regions with coarsened locks should not be marked as eliminated\");\n+        for (uint j = 1; j < size; j++) {\n+          assert(locks_list->at(j)->as_AbstractLock()->is_coarsened(), \"only coarsened locks are expected here\");\n+          BoxLockNode* this_box = locks_list->at(j)->as_AbstractLock()->box_node()->as_BoxLock();\n+          if (box != this_box) {\n+            assert(!this_box->is_eliminated(), \"regions with coarsened locks should not be marked as eliminated\");\n+            box->set_unbalanced();\n+            this_box->set_unbalanced();\n+          }\n+        }\n+      }\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":48,"deletions":0,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -802,0 +802,1 @@\n+  void mark_unbalanced_boxes() const;\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"opto\/locknode.hpp\"\n@@ -2596,1 +2597,1 @@\n-          if (not_global_escape(alock->obj_node()) && !obj_type->is_inlinetypeptr()) {\n+          if (can_eliminate_lock(alock) && !obj_type->is_inlinetypeptr()) {\n@@ -2941,0 +2942,15 @@\n+\/\/ Return true if locked object does not escape globally\n+\/\/ and locked code region (identified by BoxLockNode) is balanced:\n+\/\/ all compiled code paths have corresponding Lock\/Unlock pairs.\n+bool ConnectionGraph::can_eliminate_lock(AbstractLockNode* alock) {\n+  BoxLockNode* box = alock->box_node()->as_BoxLock();\n+  if (!box->is_unbalanced() && not_global_escape(alock->obj_node())) {\n+    if (EliminateNestedLocks) {\n+      \/\/ We can mark whole locking region as Local only when only\n+      \/\/ one object is used for locking.\n+      box->set_local();\n+    }\n+    return true;\n+  }\n+  return false;\n+}\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":17,"deletions":1,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -457,0 +457,16 @@\n+IfNode* IfNode::make_with_same_profile(IfNode* if_node_profile, Node* ctrl, BoolNode* bol) {\n+  \/\/ Assert here that we only try to create a clone from an If node with the same profiling if that actually makes sense.\n+  \/\/ Some If node subtypes should not be cloned in this way. In theory, we should not clone BaseCountedLoopEndNodes.\n+  \/\/ But they can end up being used as normal If nodes when peeling a loop - they serve as zero-trip guard.\n+  \/\/ Allow them as well.\n+  assert(if_node_profile->Opcode() == Op_If || if_node_profile->is_RangeCheck()\n+         || if_node_profile->is_BaseCountedLoopEnd(), \"should not clone other nodes\");\n+  if (if_node_profile->is_RangeCheck()) {\n+    \/\/ RangeCheck nodes could be further optimized.\n+    return new RangeCheckNode(ctrl, bol, if_node_profile->_prob, if_node_profile->_fcnt);\n+  } else {\n+    \/\/ Not a RangeCheckNode? Fall back to IfNode.\n+    return new IfNode(ctrl, bol, if_node_profile->_prob, if_node_profile->_fcnt);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/ifnode.cpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -43,1 +43,1 @@\n-                                       _slot(slot), _is_eliminated(false) {\n+                                       _slot(slot), _kind(BoxLockNode::Regular) {\n@@ -54,2 +54,1 @@\n-\/\/-----------------------------hash--------------------------------------------\n-  if (EliminateNestedLocks)\n+  if (EliminateNestedLocks) {\n@@ -58,1 +57,2 @@\n-  return Node::hash() + _slot + (_is_eliminated ? Compile::current()->fixed_slots() : 0);\n+  }\n+  return Node::hash() + _slot + (is_eliminated() ? Compile::current()->fixed_slots() : 0);\n@@ -61,2 +61,1 @@\n-\/\/------------------------------cmp--------------------------------------------\n-  if (EliminateNestedLocks)\n+  if (EliminateNestedLocks) {\n@@ -65,0 +64,1 @@\n+  }\n@@ -66,1 +66,30 @@\n-  return bn._slot == _slot && bn._is_eliminated == _is_eliminated;\n+  return (bn._slot == _slot) && (bn.is_eliminated() == is_eliminated());\n+}\n+\n+Node* BoxLockNode::Identity(PhaseGVN* phase) {\n+  if (!EliminateNestedLocks && !this->is_eliminated()) {\n+    Node* n = phase->hash_find(this);\n+    if (n == nullptr || n == this) {\n+      return this;\n+    }\n+    BoxLockNode* old_box = n->as_BoxLock();\n+    \/\/ Set corresponding status (_kind) when commoning BoxLock nodes.\n+    if (this->_kind != old_box->_kind) {\n+      if (this->is_unbalanced()) {\n+        old_box->set_unbalanced();\n+      }\n+      if (!old_box->is_unbalanced()) {\n+        \/\/ Only Regular or Coarsened status should be here:\n+        \/\/ Nested and Local are set only when EliminateNestedLocks is on.\n+        if (old_box->is_regular()) {\n+          assert(this->is_coarsened(),\"unexpected kind: %s\", _kind_name[(int)this->_kind]);\n+          old_box->set_coarsened();\n+        } else {\n+          assert(this->is_regular(),\"unexpected kind: %s\", _kind_name[(int)this->_kind]);\n+          assert(old_box->is_coarsened(),\"unexpected kind: %s\", _kind_name[(int)old_box->_kind]);\n+        }\n+      }\n+    }\n+    return old_box;\n+  }\n+  return this;\n@@ -93,0 +122,3 @@\n+  if (is_unbalanced()) {\n+    return false;\n+  }\n","filename":"src\/hotspot\/share\/opto\/locknode.cpp","additions":39,"deletions":7,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -36,0 +36,1 @@\n+private:\n@@ -38,1 +39,28 @@\n-  bool _is_eliminated; \/\/ Associated locks were safely eliminated\n+  enum {\n+    Regular = 0,       \/\/ Normal locking region\n+    Local,             \/\/ EA found that local not escaping object is used for locking\n+    Nested,            \/\/ This region is inside other region which use the same object\n+    Coarsened,         \/\/ Some lock\/unlock in region were marked as coarsened\n+    Unbalanced,        \/\/ This region become unbalanced after coarsened lock\/unlock were eliminated\n+                       \/\/ or it is locking region from OSR when locking is done in Interpreter\n+    Eliminated         \/\/ All lock\/unlock in region were eliminated\n+  } _kind;\n+\n+#ifdef ASSERT\n+  const char* _kind_name[6] = {\n+   \"Regular\",\n+   \"Local\",\n+   \"Nested\",\n+   \"Coarsened\",\n+   \"Unbalanced\",\n+   \"Eliminated\"\n+  };\n+#endif\n+\n+  \/\/ Allowed transitions of _kind:\n+  \/\/   Regular -> Local, Nested, Coarsened\n+  \/\/   Local   -> Eliminated\n+  \/\/   Nested  -> Eliminated\n+  \/\/   Coarsened -> Local, Nested, Unbalanced\n+  \/\/ EA and nested lock elimination can overwrite Coarsened kind.\n+  \/\/ Also allow transition to the same kind.\n@@ -52,0 +80,1 @@\n+  virtual Node* Identity(PhaseGVN* phase);\n@@ -60,3 +89,32 @@\n-  bool is_eliminated() const { return _is_eliminated; }\n-  \/\/ mark lock as eliminated.\n-  void set_eliminated()      { _is_eliminated = true; }\n+  bool is_regular()    const { return _kind == Regular; }\n+  bool is_local()      const { return _kind == Local; }\n+  bool is_nested()     const { return _kind == Nested; }\n+  bool is_coarsened()  const { return _kind == Coarsened; }\n+  bool is_eliminated() const { return _kind == Eliminated; }\n+  bool is_unbalanced() const { return _kind == Unbalanced; }\n+\n+  void set_local()      {\n+    assert((_kind == Regular || _kind == Local || _kind == Coarsened),\n+           \"incorrect kind for Local transitioni: %s\", _kind_name[(int)_kind]);\n+    _kind = Local;\n+  }\n+  void set_nested()     {\n+    assert((_kind == Regular || _kind == Nested || _kind == Coarsened),\n+           \"incorrect kind for Nested transition: %s\", _kind_name[(int)_kind]);\n+    _kind = Nested;\n+  }\n+  void set_coarsened()  {\n+    assert((_kind == Regular || _kind == Coarsened),\n+           \"incorrect kind for Coarsened transition: %s\", _kind_name[(int)_kind]);\n+    _kind = Coarsened;\n+  }\n+  void set_eliminated() {\n+    assert((_kind == Local || _kind == Nested),\n+           \"incorrect kind for Eliminated transition: %s\", _kind_name[(int)_kind]);\n+    _kind = Eliminated;\n+  }\n+  void set_unbalanced() {\n+    assert((_kind == Coarsened || _kind == Unbalanced),\n+           \"incorrect kind for Unbalanced transition: %s\", _kind_name[(int)_kind]);\n+    _kind = Unbalanced;\n+  }\n","filename":"src\/hotspot\/share\/opto\/locknode.hpp","additions":63,"deletions":5,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -27,2 +27,2 @@\n-#include \"opto\/mulnode.hpp\"\n-#include \"opto\/addnode.hpp\"\n+#include \"opto\/castnode.hpp\"\n+#include \"opto\/cfgnode.hpp\"\n@@ -36,1 +36,11 @@\n-\/\/================= Loop Unswitching =====================\n+\/\/ Loop Unswitching is a loop optimization to move an invariant, non-loop-exiting test in the loop body before the loop.\n+\/\/ Such a test is either always true or always false in all loop iterations and could therefore only be executed once.\n+\/\/ To achieve that, we duplicate the loop and change the original and cloned loop as follows:\n+\/\/ - Original loop -> true-path-loop:\n+\/\/        The true-path of the invariant, non-loop-exiting test in the original loop\n+\/\/        is kept while the false-path is killed. We call this unswitched loop version\n+\/\/        the true-path-loop.\n+\/\/ - Cloned loop -> false-path-loop:\n+\/\/        The false-path of the invariant, non-loop-exiting test in the cloned loop\n+\/\/        is kept while the true-path is killed. We call this unswitched loop version\n+\/\/        the false-path loop.\n@@ -38,16 +48,4 @@\n-\/\/ orig:                       transformed:\n-\/\/                               if (invariant-test) then\n-\/\/  predicates                     predicates\n-\/\/  loop                           loop\n-\/\/    stmt1                          stmt1\n-\/\/    if (invariant-test) then       stmt2\n-\/\/      stmt2                        stmt4\n-\/\/    else                         endloop\n-\/\/      stmt3                    else\n-\/\/    endif                        predicates [clone]\n-\/\/    stmt4                        loop [clone]\n-\/\/  endloop                          stmt1 [clone]\n-\/\/                                   stmt3\n-\/\/                                   stmt4 [clone]\n-\/\/                                 endloop\n-\/\/                               endif\n+\/\/ The invariant, non-loop-exiting test can now be moved before both loops (to only execute it once) and turned into a\n+\/\/ loop selector If node to select at runtime which unswitched loop version should be executed.\n+\/\/ - Loop selector true?  Execute the true-path-loop.\n+\/\/ - Loop selector false? Execute the false-path-loop.\n@@ -55,7 +53,40 @@\n-\/\/ Note: the \"else\" clause may be empty\n-\n-\n-\/\/------------------------------policy_unswitching-----------------------------\n-\/\/ Return TRUE or FALSE if the loop should be unswitched\n-\/\/ (ie. clone loop with an invariant test that does not exit the loop)\n-bool IdealLoopTree::policy_unswitching( PhaseIdealLoop *phase ) const {\n+\/\/ Note that even though an invariant test that exits the loop could also be optimized with Loop Unswitching, it is more\n+\/\/ efficient to simply peel the loop which achieves the same result in a simpler manner (also see policy_peeling()).\n+\/\/\n+\/\/ The following graphs summarizes the Loop Unswitching optimization.\n+\/\/ We start with the original loop:\n+\/\/\n+\/\/                       [Predicates]\n+\/\/                            |\n+\/\/                       Original Loop\n+\/\/                         stmt1\n+\/\/                         if (invariant-test)\n+\/\/                           if-path\n+\/\/                         else\n+\/\/                           else-path\n+\/\/                         stmt2\n+\/\/                       Endloop\n+\/\/\n+\/\/\n+\/\/ which is unswitched into a true-path-loop and a false-path-loop together with a loop selector:\n+\/\/\n+\/\/\n+\/\/            [Initialized Assertion Predicates]\n+\/\/                            |\n+\/\/                 loop selector If (invariant-test)\n+\/\/                    \/                   \\\n+\/\/                true?                  false?\n+\/\/                \/                         \\\n+\/\/    [Cloned Parse Predicates]         [Cloned Parse Predicates]\n+\/\/    [Cloned Template                  [Cloned Template\n+\/\/     Assertion Predicates]             Assertion Predicates]\n+\/\/          |                                  |\n+\/\/    True-Path-Loop                    False-Path-Loop\n+\/\/      cloned stmt1                      cloned stmt1\n+\/\/      cloned if-path                    cloned else-path\n+\/\/      cloned stmt2                      cloned stmt2\n+\/\/    Endloop                           Endloop\n+\n+\n+\/\/ Return true if the loop should be unswitched or false otherwise.\n+bool IdealLoopTree::policy_unswitching(PhaseIdealLoop* phase) const {\n@@ -87,1 +118,1 @@\n-  if (phase->find_unswitching_candidate(this, unswitch_iffs) == nullptr) {\n+  if (phase->find_unswitch_candidate(this, unswitch_iffs) == nullptr) {\n@@ -95,7 +126,5 @@\n-\/\/------------------------------find_unswitching_candidate-----------------------------\n-\/\/ Find candidate \"if\" for unswitching\n-IfNode* PhaseIdealLoop::find_unswitching_candidate(const IdealLoopTree *loop, Node_List& unswitch_iffs) const {\n-\n-  \/\/ Find first invariant test that doesn't exit the loop\n-  LoopNode *head = loop->_head->as_Loop();\n-  IfNode* unswitch_iff = nullptr;\n+\/\/ Find an invariant test in the loop body that does not exit the loop. If multiple tests are found, we pick the first\n+\/\/ one in the loop body. Return the \"unswitch candidate\" If to apply Loop Unswitching on.\n+IfNode* PhaseIdealLoop::find_unswitch_candidate(const IdealLoopTree* loop, Node_List& unswitch_iffs) const {\n+  LoopNode* head = loop->_head->as_Loop();\n+  IfNode* unswitch_candidate = nullptr;\n@@ -114,1 +143,2 @@\n-              unswitch_iff = iff;\n+              assert(iff->Opcode() == Op_If || iff->is_RangeCheck() || iff->is_BaseCountedLoopEnd(), \"valid ifs\");\n+              unswitch_candidate = iff;\n@@ -122,2 +152,2 @@\n-  if (unswitch_iff != nullptr) {\n-    unswitch_iffs.push(unswitch_iff);\n+  if (unswitch_candidate != nullptr) {\n+    unswitch_iffs.push(unswitch_candidate);\n@@ -128,1 +158,1 @@\n-  if (unswitch_iff == nullptr || unswitch_iff->is_flat_array_check(&_igvn)) {\n+  if (unswitch_candidate == nullptr || unswitch_candidate->is_flat_array_check(&_igvn)) {\n@@ -131,1 +161,1 @@\n-      if (n != nullptr && n != unswitch_iff && n->is_flat_array_check(&_igvn) &&\n+      if (n != nullptr && n != unswitch_candidate && n->is_flat_array_check(&_igvn) &&\n@@ -134,2 +164,2 @@\n-        if (unswitch_iff == nullptr) {\n-          unswitch_iff = n;\n+        if (unswitch_candidate == nullptr) {\n+          unswitch_candidate = n;\n@@ -140,1 +170,1 @@\n-  return unswitch_iff;\n+  return unswitch_candidate;\n@@ -143,8 +173,36 @@\n-\/\/------------------------------do_unswitching-----------------------------\n-\/\/ Clone loop with an invariant test (that does not exit) and\n-\/\/ insert a clone of the test that selects which version to\n-\/\/ execute.\n-void PhaseIdealLoop::do_unswitching(IdealLoopTree *loop, Node_List &old_new) {\n-  LoopNode* head = loop->_head->as_Loop();\n-  if (has_control_dependencies_from_predicates(head)) {\n-    return;\n+\/\/ This class creates an If node (i.e. loop selector) that selects if the true-path-loop or the false-path-loop should be\n+\/\/ executed at runtime. This is done by finding an invariant and non-loop-exiting unswitch candidate If node (guaranteed\n+\/\/ to exist at this point) to perform Loop Unswitching on.\n+class UnswitchedLoopSelector : public StackObj {\n+  PhaseIdealLoop* const _phase;\n+  IdealLoopTree* const _outer_loop;\n+  Node* const _original_loop_entry;\n+  Node_List _unswitch_iffs;\n+  IfNode* const _unswitch_candidate;\n+  const bool _flat_array_checks;\n+  IfNode* const _selector;\n+  IfTrueNode* const _true_path_loop_proj;\n+  IfFalseNode* const _false_path_loop_proj;\n+\n+  enum PathToLoop { TRUE_PATH, FALSE_PATH };\n+\n+ public:\n+  UnswitchedLoopSelector(IdealLoopTree* loop)\n+      : _phase(loop->_phase),\n+        _outer_loop(loop->skip_strip_mined()->_parent),\n+        _original_loop_entry(loop->_head->as_Loop()->skip_strip_mined()->in(LoopNode::EntryControl)),\n+        _unswitch_iffs(),\n+        _unswitch_candidate(find_unswitch_candidate(loop)),\n+        _flat_array_checks(_unswitch_iffs.size() > 1),\n+        _selector(create_selector_if()),\n+        _true_path_loop_proj(create_proj_to_loop(TRUE_PATH)->as_IfTrue()),\n+        _false_path_loop_proj(create_proj_to_loop(FALSE_PATH)->as_IfFalse()) {\n+  }\n+  NONCOPYABLE(UnswitchedLoopSelector);\n+\n+ private:\n+  IfNode* find_unswitch_candidate(IdealLoopTree* loop) {\n+    IfNode* unswitch_candidate = _phase->find_unswitch_candidate(loop, _unswitch_iffs);\n+    assert(unswitch_candidate != nullptr, \"guaranteed to exist by policy_unswitching\");\n+    assert(_phase->is_member(loop, unswitch_candidate), \"must be inside original loop\");\n+    return unswitch_candidate;\n@@ -153,13 +211,25 @@\n-  \/\/ Find first invariant test that doesn't exit the loop\n-  Node_List unswitch_iffs;\n-  IfNode* unswitch_iff = find_unswitching_candidate((const IdealLoopTree *)loop, unswitch_iffs);\n-  assert(unswitch_iff != nullptr && unswitch_iff == unswitch_iffs.at(0), \"should be at least one\");\n-  bool flat_array_checks = unswitch_iffs.size() > 1;\n-\n-#ifndef PRODUCT\n-  if (TraceLoopOpts) {\n-    tty->print(\"Unswitch   %d \", head->unswitch_count()+1);\n-    loop->dump_head();\n-    for (uint i = 0; i < unswitch_iffs.size(); i++) {\n-      unswitch_iffs.at(i)->dump(3);\n-      tty->cr();\n+  IfNode* create_selector_if() const {\n+    IfNode* unswitch_iff = _unswitch_iffs.at(0)->as_If();\n+    BoolNode* bol = unswitch_iff->in(1)->as_Bool();\n+    if (_unswitch_iffs.size() > 1) {\n+      \/\/ Flat array checks are used on array access to switch between\n+      \/\/ a legacy object array access and a flat inline type array\n+      \/\/ access. We want the performance impact on legacy accesses to be\n+      \/\/ as small as possible so we make two copies of the loop: a fast\n+      \/\/ one where all accesses are known to be legacy, a slow one where\n+      \/\/ some accesses are to flat arrays. Flat array checks\n+      \/\/ can be removed from the fast loop (true proj) but not from the\n+      \/\/ slow loop (false proj) as it can have a mix of flat\/legacy accesses.\n+      assert(bol->_test._test == BoolTest::ne, \"IfTrue proj must point to flat array\");\n+      bol = bol->clone()->as_Bool();\n+      _phase->register_new_node(bol, _original_loop_entry);\n+      FlatArrayCheckNode* cmp = bol->in(1)->clone()->as_FlatArrayCheck();\n+      _phase->register_new_node(cmp, _original_loop_entry);\n+      bol->set_req(1, cmp);\n+      \/\/ Combine all checks into a single one that fails if one array is a flat array\n+      assert(cmp->req() == 3, \"unexpected number of inputs for FlatArrayCheck\");\n+      cmp->add_req_batch(_phase->C->top(), _unswitch_iffs.size() - 1);\n+      for (uint i = 0; i < _unswitch_iffs.size(); i++) {\n+        Node* array = _unswitch_iffs.at(i)->in(1)->in(1)->in(FlatArrayCheckNode::ArrayOrKlass);\n+        cmp->set_req(FlatArrayCheckNode::ArrayOrKlass + i, array);\n+      }\n@@ -167,0 +237,6 @@\n+\n+    const uint dom_depth = _phase->dom_depth(_original_loop_entry);\n+    _phase->igvn().rehash_node_delayed(_original_loop_entry);\n+    IfNode* selector_if = IfNode::make_with_same_profile(_unswitch_candidate, _original_loop_entry, bol);\n+    _phase->register_node(selector_if, _outer_loop, _original_loop_entry, dom_depth);\n+    return selector_if;\n@@ -168,44 +244,7 @@\n-#endif\n-  C->print_method(PHASE_BEFORE_LOOP_UNSWITCHING, 4, head);\n-\n-  \/\/ Need to revert back to normal loop\n-  if (head->is_CountedLoop() && !head->as_CountedLoop()->is_normal_loop()) {\n-    head->as_CountedLoop()->set_normal_loop();\n-  }\n-\n-  IfNode* invar_iff = create_slow_version_of_loop(loop, old_new, unswitch_iffs, CloneIncludesStripMined);\n-  ProjNode* proj_true = invar_iff->proj_out(1);\n-  verify_fast_loop(head, proj_true);\n-\n-  \/\/ Increment unswitch count\n-  LoopNode* head_clone = old_new[head->_idx]->as_Loop();\n-  int nct = head->unswitch_count() + 1;\n-  head->set_unswitch_count(nct);\n-  head_clone->set_unswitch_count(nct);\n-\n-  \/\/ Hoist invariant casts out of each loop to the appropriate control projection.\n-  Node_List worklist;\n-  for (uint i = 0; i < unswitch_iffs.size(); i++) {\n-    IfNode* iff = unswitch_iffs.at(i)->as_If();\n-    for (DUIterator_Fast imax, i = iff->fast_outs(imax); i < imax; i++) {\n-      ProjNode* proj = iff->fast_out(i)->as_Proj();\n-      \/\/ Copy to a worklist for easier manipulation\n-      for (DUIterator_Fast jmax, j = proj->fast_outs(jmax); j < jmax; j++) {\n-        Node* use = proj->fast_out(j);\n-        if (use->Opcode() == Op_CheckCastPP && loop->is_invariant(use->in(1))) {\n-          worklist.push(use);\n-        }\n-      }\n-      ProjNode* invar_proj = invar_iff->proj_out(proj->_con)->as_Proj();\n-      while (worklist.size() > 0) {\n-        Node* use = worklist.pop();\n-        Node* nuse = use->clone();\n-        nuse->set_req(0, invar_proj);\n-        _igvn.replace_input_of(use, 1, nuse);\n-        register_new_node(nuse, invar_proj);\n-        \/\/ Same for the clone if we are removing checks from the slow loop\n-        if (!flat_array_checks) {\n-          Node* use_clone = old_new[use->_idx];\n-          _igvn.replace_input_of(use_clone, 1, nuse);\n-        }\n-      }\n+  IfProjNode* create_proj_to_loop(const PathToLoop path_to_loop) {\n+    const uint dom_depth = _phase->dom_depth(_original_loop_entry);\n+    IfProjNode* proj_to_loop;\n+    if (path_to_loop == TRUE_PATH) {\n+      proj_to_loop = new IfTrueNode(_selector);\n+    } else {\n+      proj_to_loop = new IfFalseNode(_selector);\n@@ -214,0 +253,2 @@\n+    _phase->register_node(proj_to_loop, _outer_loop, _selector, dom_depth);\n+    return proj_to_loop;\n@@ -216,5 +257,3 @@\n-  \/\/ Hardwire the control paths in the loops into if(true) and if(false)\n-  for (uint i = 0; i < unswitch_iffs.size(); i++) {\n-    IfNode* iff = unswitch_iffs.at(i)->as_If();\n-    _igvn.rehash_node_delayed(iff);\n-    dominated_by(proj_true->as_IfProj(), iff);\n+ public:\n+  IfNode* unswitch_candidate() const {\n+    return _unswitch_candidate;\n@@ -222,9 +261,3 @@\n-  IfNode* unswitch_iff_clone = old_new[unswitch_iff->_idx]->as_If();\n-  if (!flat_array_checks) {\n-    ProjNode* proj_false = invar_iff->proj_out(0)->as_Proj();\n-    _igvn.rehash_node_delayed(unswitch_iff_clone);\n-    dominated_by(proj_false->as_IfProj(), unswitch_iff_clone);\n-  } else {\n-    \/\/ Leave the flat array checks in the slow loop and\n-    \/\/ prevent it from being unswitched again based on these checks.\n-    head_clone->mark_flat_arrays();\n+\n+  IfNode* selector() const {\n+    return _selector;\n@@ -233,6 +266,2 @@\n-  \/\/ Reoptimize loops\n-  loop->record_for_igvn();\n-  for(int i = loop->_body.size() - 1; i >= 0 ; i--) {\n-    Node *n = loop->_body[i];\n-    Node *n_clone = old_new[n->_idx];\n-    _igvn._worklist.push(n_clone);\n+  IfTrueNode* true_path_loop_proj() const {\n+    return _true_path_loop_proj;\n@@ -241,2 +270,61 @@\n-#ifndef PRODUCT\n-  if (TraceLoopUnswitching) {\n+  IfFalseNode* false_path_loop_proj() const {\n+    return _false_path_loop_proj;\n+  }\n+\n+  bool has_flat_array_checks() const {\n+    return _flat_array_checks;\n+  }\n+\n+  const Node_List& unswitch_iffs() const {\n+    return _unswitch_iffs;\n+  }\n+};\n+\n+\/\/ Class to unswitch the original loop and create Predicates at the new unswitched loop versions. The newly cloned loop\n+\/\/ becomes the false-path-loop while original loop becomes the true-path-loop.\n+class OriginalLoop : public StackObj {\n+  LoopNode* const _loop_head; \/\/ OuterStripMinedLoopNode if loop strip mined, else just the loop head.\n+  IdealLoopTree* const _loop;\n+  Node_List& _old_new;\n+  PhaseIdealLoop* const _phase;\n+\n+ public:\n+  OriginalLoop(IdealLoopTree* loop, Node_List& old_new)\n+      : _loop_head(loop->_head->as_Loop()->skip_strip_mined()),\n+        _loop(loop),\n+        _old_new(old_new),\n+        _phase(loop->_phase) {}\n+  NONCOPYABLE(OriginalLoop);\n+\n+ private:\n+  void fix_loop_entries(IfProjNode* true_path_loop_entry, IfProjNode* false_path_loop_entry) {\n+    _phase->replace_loop_entry(_loop_head, true_path_loop_entry);\n+    LoopNode* false_path_loop_strip_mined_head = old_to_new(_loop_head)->as_Loop();\n+    _phase->replace_loop_entry(false_path_loop_strip_mined_head, false_path_loop_entry);\n+  }\n+\n+  Node* old_to_new(const Node* old) const {\n+    return _old_new[old->_idx];\n+  }\n+\n+#ifdef ASSERT\n+  void verify_unswitched_loop_versions(LoopNode* true_path_loop_head,\n+                                       const UnswitchedLoopSelector& unswitched_loop_selector) const {\n+    verify_unswitched_loop_version(true_path_loop_head, unswitched_loop_selector.true_path_loop_proj());\n+    verify_unswitched_loop_version(old_to_new(true_path_loop_head)->as_Loop(),\n+                                   unswitched_loop_selector.false_path_loop_proj());\n+  }\n+\n+  static void verify_unswitched_loop_version(LoopNode* loop_head, IfProjNode* loop_selector_if_proj) {\n+    Node* entry = loop_head->skip_strip_mined()->in(LoopNode::EntryControl);\n+    const Predicates predicates(entry);\n+    \/\/ When skipping all predicates, we should end up at 'loop_selector_if_proj'.\n+    assert(loop_selector_if_proj == predicates.entry(), \"should end up at loop selector If\");\n+  }\n+#endif \/\/ ASSERT\n+\n+  \/\/ Remove the unswitch candidate If nodes in both unswitched loop versions which are now dominated by the loop selector\n+  \/\/ If node. Keep the true-path-path in the true-path-loop and the false-path-path in the false-path-loop by setting\n+  \/\/ the bool input accordingly. The unswitch candidate If nodes are folded in the next IGVN round.\n+  void remove_unswitch_candidate_from_loops(const UnswitchedLoopSelector& unswitched_loop_selector) {\n+    const Node_List& unswitch_iffs = unswitched_loop_selector.unswitch_iffs();\n@@ -244,3 +332,13 @@\n-      tty->print_cr(\"Loop unswitching orig: %d @ %d  new: %d @ %d\",\n-                    head->_idx,                unswitch_iffs.at(i)->_idx,\n-                    old_new[head->_idx]->_idx, old_new[unswitch_iffs.at(i)->_idx]->_idx);\n+      IfNode* iff = unswitch_iffs.at(i)->as_If();\n+      _phase->igvn().rehash_node_delayed(iff);\n+      _phase->dominated_by(unswitched_loop_selector.true_path_loop_proj(), iff);\n+    }\n+\n+    if (unswitched_loop_selector.has_flat_array_checks()) {\n+      \/\/ Leave the flat array checks in the slow loop and\n+      \/\/ prevent it from being unswitched again based on these checks.\n+      old_to_new(_loop_head)->as_Loop()->mark_flat_arrays();\n+    } else {\n+      IfNode* unswitching_candidate_clone = _old_new[unswitched_loop_selector.unswitch_candidate()->_idx]->as_If();\n+      _phase->igvn().rehash_node_delayed(unswitching_candidate_clone);\n+      _phase->dominated_by(unswitched_loop_selector.false_path_loop_proj(), unswitching_candidate_clone);\n@@ -249,2 +347,38 @@\n-#endif\n-  C->print_method(PHASE_AFTER_LOOP_UNSWITCHING, 4, head_clone);\n+ public:\n+  \/\/ Unswitch the original loop on the invariant loop selector by creating a true-path-loop and a false-path-loop.\n+  \/\/ Remove the unswitch candidate If from both unswitched loop versions which are now covered by the loop selector If.\n+  void unswitch(const UnswitchedLoopSelector& unswitched_loop_selector) {\n+    _phase->clone_loop(_loop, _old_new, _phase->dom_depth(_loop_head),\n+                       PhaseIdealLoop::CloneIncludesStripMined, unswitched_loop_selector.selector());\n+\n+    \/\/ At this point, the selector If projections are the corresponding loop entries.\n+    \/\/ clone_parse_and_assertion_predicates_to_unswitched_loop() could clone additional predicates after the selector\n+    \/\/ If projections. The loop entries are updated accordingly.\n+    IfProjNode* true_path_loop_entry = unswitched_loop_selector.true_path_loop_proj();\n+    IfProjNode* false_path_loop_entry = unswitched_loop_selector.false_path_loop_proj();\n+    _phase->clone_parse_and_assertion_predicates_to_unswitched_loop(_loop, _old_new,\n+                                                                    true_path_loop_entry, false_path_loop_entry);\n+\n+    fix_loop_entries(true_path_loop_entry, false_path_loop_entry);\n+\n+    DEBUG_ONLY(verify_unswitched_loop_versions(_loop->_head->as_Loop(), unswitched_loop_selector);)\n+\n+    _phase->recompute_dom_depth();\n+    remove_unswitch_candidate_from_loops(unswitched_loop_selector);\n+  }\n+};\n+\n+\/\/ See comments below file header for more information about Loop Unswitching.\n+void PhaseIdealLoop::do_unswitching(IdealLoopTree* loop, Node_List& old_new) {\n+  assert(LoopUnswitching, \"LoopUnswitching must be enabled\");\n+\n+  LoopNode* original_head = loop->_head->as_Loop();\n+  if (has_control_dependencies_from_predicates(original_head)) {\n+    NOT_PRODUCT(trace_loop_unswitching_impossible(original_head);)\n+    return;\n+  }\n+\n+  const UnswitchedLoopSelector unswitched_loop_selector(loop);\n+\n+  NOT_PRODUCT(trace_loop_unswitching_count(loop, original_head, unswitched_loop_selector.unswitch_iffs());)\n+  C->print_method(PHASE_BEFORE_LOOP_UNSWITCHING, 4, original_head);\n@@ -253,0 +387,13 @@\n+  revert_to_normal_loop(original_head);\n+\n+  OriginalLoop original_loop(loop, old_new);\n+  original_loop.unswitch(unswitched_loop_selector);\n+\n+  hoist_invariant_check_casts(loop, old_new, unswitched_loop_selector);\n+  add_unswitched_loop_version_bodies_to_igvn(loop, old_new);\n+\n+  LoopNode* new_head = old_new[original_head->_idx]->as_Loop();\n+  increment_unswitch_counts(original_head, new_head);\n+\n+  NOT_PRODUCT(trace_loop_unswitching_result(unswitched_loop_selector, original_head, new_head, old_new);)\n+  C->print_method(PHASE_AFTER_LOOP_UNSWITCHING, 4, new_head);\n@@ -256,1 +403,1 @@\n-bool PhaseIdealLoop::has_control_dependencies_from_predicates(LoopNode* head) const {\n+bool PhaseIdealLoop::has_control_dependencies_from_predicates(LoopNode* head) {\n@@ -258,1 +405,1 @@\n-  Predicates predicates(entry);\n+  const Predicates predicates(entry);\n@@ -271,37 +418,13 @@\n-\/\/-------------------------create_slow_version_of_loop------------------------\n-\/\/ Create a slow version of the loop by cloning the loop\n-\/\/ and inserting an if to select fast-slow versions.\n-\/\/ Return the inserted if.\n-IfNode* PhaseIdealLoop::create_slow_version_of_loop(IdealLoopTree *loop,\n-                                                    Node_List &old_new,\n-                                                    Node_List &unswitch_iffs,\n-                                                    CloneLoopMode mode) {\n-  LoopNode* head  = loop->_head->as_Loop();\n-  bool counted_loop = head->is_CountedLoop();\n-  Node*     entry = head->skip_strip_mined()->in(LoopNode::EntryControl);\n-  _igvn.rehash_node_delayed(entry);\n-  IdealLoopTree* outer_loop = loop->_parent;\n-\n-  head->verify_strip_mined(1);\n-\n-  \/\/ Add test to new \"if\" outside of loop\n-  IfNode* unswitch_iff = unswitch_iffs.at(0)->as_If();\n-  BoolNode* bol = unswitch_iff->in(1)->as_Bool();\n-  if (unswitch_iffs.size() > 1) {\n-    \/\/ Flat array checks are used on array access to switch between\n-    \/\/ a legacy object array access and a flat inline type array\n-    \/\/ access. We want the performance impact on legacy accesses to be\n-    \/\/ as small as possible so we make two copies of the loop: a fast\n-    \/\/ one where all accesses are known to be legacy, a slow one where\n-    \/\/ some accesses are to flat arrays. Flat array checks\n-    \/\/ can be removed from the fast loop (true proj) but not from the\n-    \/\/ slow loop (false proj) as it can have a mix of flat\/legacy accesses.\n-    assert(bol->_test._test == BoolTest::ne, \"IfTrue proj must point to flat array\");\n-    bol = bol->clone()->as_Bool();\n-    register_new_node(bol, entry);\n-    FlatArrayCheckNode* cmp = bol->in(1)->clone()->as_FlatArrayCheck();\n-    register_new_node(cmp, entry);\n-    bol->set_req(1, cmp);\n-    \/\/ Combine all checks into a single one that fails if one array is a flat array\n-    assert(cmp->req() == 3, \"unexpected number of inputs for FlatArrayCheck\");\n-    cmp->add_req_batch(C->top(), unswitch_iffs.size() - 1);\n+#ifndef PRODUCT\n+void PhaseIdealLoop::trace_loop_unswitching_impossible(const LoopNode* original_head) {\n+  if (TraceLoopUnswitching) {\n+    tty->print_cr(\"Loop Unswitching \\\"%d %s\\\" not possible due to control dependencies\",\n+                  original_head->_idx, original_head->Name());\n+  }\n+}\n+\n+void PhaseIdealLoop::trace_loop_unswitching_count(IdealLoopTree* loop, LoopNode* original_head,\n+                                                  const Node_List& unswitch_iffs) {\n+  if (TraceLoopOpts) {\n+    tty->print(\"Unswitch   %d \", original_head->unswitch_count() + 1);\n+    loop->dump_head();\n@@ -309,2 +432,2 @@\n-      Node* array = unswitch_iffs.at(i)->in(1)->in(1)->in(FlatArrayCheckNode::ArrayOrKlass);\n-      cmp->set_req(FlatArrayCheckNode::ArrayOrKlass + i, array);\n+      unswitch_iffs.at(i)->dump(3);\n+      tty->cr();\n@@ -313,0 +436,1 @@\n+}\n@@ -314,25 +438,25 @@\n-  IfNode* iff = (unswitch_iff->Opcode() == Op_RangeCheck) ? new RangeCheckNode(entry, bol, unswitch_iff->_prob, unswitch_iff->_fcnt) :\n-      new IfNode(entry, bol, unswitch_iff->_prob, unswitch_iff->_fcnt);\n-  register_node(iff, outer_loop, entry, dom_depth(entry));\n-  IfProjNode* iffast = new IfTrueNode(iff);\n-  register_node(iffast, outer_loop, iff, dom_depth(iff));\n-  IfProjNode* ifslow = new IfFalseNode(iff);\n-  register_node(ifslow, outer_loop, iff, dom_depth(iff));\n-\n-  \/\/ Clone the loop body.  The clone becomes the slow loop.  The\n-  \/\/ original pre-header will (illegally) have 3 control users\n-  \/\/ (old & new loops & new if).\n-  clone_loop(loop, old_new, dom_depth(head->skip_strip_mined()), mode, iff);\n-  assert(old_new[head->_idx]->is_Loop(), \"\" );\n-\n-  \/\/ Fast (true) and Slow (false) control\n-  IfProjNode* iffast_pred = iffast;\n-  IfProjNode* ifslow_pred = ifslow;\n-  clone_parse_and_assertion_predicates_to_unswitched_loop(loop, old_new, iffast_pred, ifslow_pred);\n-\n-  Node* l = head->skip_strip_mined();\n-  _igvn.replace_input_of(l, LoopNode::EntryControl, iffast_pred);\n-  set_idom(l, iffast_pred, dom_depth(l));\n-  LoopNode* slow_l = old_new[head->_idx]->as_Loop()->skip_strip_mined();\n-  _igvn.replace_input_of(slow_l, LoopNode::EntryControl, ifslow_pred);\n-  set_idom(slow_l, ifslow_pred, dom_depth(l));\n+void PhaseIdealLoop::trace_loop_unswitching_result(const UnswitchedLoopSelector& unswitched_loop_selector,\n+                                                   const LoopNode* original_head, const LoopNode* new_head,\n+                                                   const Node_List& old_new) {\n+  if (TraceLoopUnswitching) {\n+    IfNode* unswitch_candidate = unswitched_loop_selector.unswitch_candidate();\n+    IfNode* loop_selector = unswitched_loop_selector.selector();\n+    tty->print_cr(\"Loop Unswitching:\");\n+    tty->print_cr(\"- Unswitch-Candidate-If: %d %s\", unswitch_candidate->_idx, unswitch_candidate->Name());\n+    tty->print_cr(\"- Loop-Selector-If: %d %s\", loop_selector->_idx, loop_selector->Name());\n+    tty->print_cr(\"- True-Path-Loop (=Orig): %d %s\", original_head->_idx, original_head->Name());\n+    tty->print_cr(\"- False-Path-Loop (=Clone): %d %s\", new_head->_idx, new_head->Name());\n+    if (unswitched_loop_selector.has_flat_array_checks()) {\n+      const Node_List& unswitch_iffs = unswitched_loop_selector.unswitch_iffs();\n+      tty->print_cr(\"- Unswitched Flat Array Checks:\");\n+      for (uint i = 0; i < unswitch_iffs.size(); i++) {\n+        Node* unswitch_iff = unswitch_iffs.at(i);\n+        Node* cloned_unswitch_iff = old_new[unswitch_iff->_idx];\n+        assert(cloned_unswitch_iff != nullptr, \"must exist\");\n+        tty->print_cr(\"  - %d %s  ->  %d %s\", unswitch_iff->_idx, unswitch_iff->Name(),\n+                                             cloned_unswitch_iff->_idx, cloned_unswitch_iff->Name());\n+      }\n+    }\n+  }\n+}\n+#endif\n@@ -340,1 +464,8 @@\n-  recompute_dom_depth();\n+\/\/ When unswitching a counted loop, we need to convert it back to a normal loop since it's not a proper pre, main or,\n+\/\/ post loop anymore after loop unswitching.\n+void PhaseIdealLoop::revert_to_normal_loop(const LoopNode* loop_head) {\n+  CountedLoopNode* cl = loop_head->isa_CountedLoop();\n+  if (cl != nullptr && !cl->is_normal_loop()) {\n+    cl->set_normal_loop();\n+  }\n+}\n@@ -342,1 +473,30 @@\n-  return iff;\n+\/\/ Hoist invariant CheckCastPPNodes out of each unswitched loop version to the appropriate loop selector If projection.\n+void PhaseIdealLoop::hoist_invariant_check_casts(const IdealLoopTree* loop, const Node_List& old_new,\n+                                                 const UnswitchedLoopSelector& unswitched_loop_selector) {\n+  IfNode* unswitch_candidate = unswitched_loop_selector.unswitch_candidate();\n+  IfNode* loop_selector = unswitched_loop_selector.selector();\n+  ResourceMark rm;\n+  GrowableArray<CheckCastPPNode*> loop_invariant_check_casts;\n+  for (DUIterator_Fast imax, i = unswitch_candidate->fast_outs(imax); i < imax; i++) {\n+    IfProjNode* proj = unswitch_candidate->fast_out(i)->as_IfProj();\n+    \/\/ Copy to a worklist for easier manipulation\n+    for (DUIterator_Fast jmax, j = proj->fast_outs(jmax); j < jmax; j++) {\n+      CheckCastPPNode* check_cast = proj->fast_out(j)->isa_CheckCastPP();\n+      if (check_cast != nullptr && loop->is_invariant(check_cast->in(1))) {\n+        loop_invariant_check_casts.push(check_cast);\n+      }\n+    }\n+    IfProjNode* loop_selector_if_proj = loop_selector->proj_out(proj->_con)->as_IfProj();\n+    while (loop_invariant_check_casts.length() > 0) {\n+      CheckCastPPNode* cast = loop_invariant_check_casts.pop();\n+      Node* cast_clone = cast->clone();\n+      cast_clone->set_req(0, loop_selector_if_proj);\n+      _igvn.replace_input_of(cast, 1, cast_clone);\n+      register_new_node(cast_clone, loop_selector_if_proj);\n+      \/\/ Same for the clone\n+      if (!unswitched_loop_selector.has_flat_array_checks()) {\n+        Node* use_clone = old_new[cast->_idx];\n+        _igvn.replace_input_of(use_clone, 1, cast_clone);\n+      }\n+    }\n+  }\n@@ -345,14 +505,7 @@\n-#ifdef ASSERT\n-void PhaseIdealLoop::verify_fast_loop(LoopNode* head, const ProjNode* proj_true) const {\n-  assert(proj_true->is_IfTrue(), \"must be true projection\");\n-  Node* entry = head->skip_strip_mined()->in(LoopNode::EntryControl);\n-  Predicates predicates(entry);\n-  if (!predicates.has_any()) {\n-    \/\/ No Parse Predicate.\n-    Node* uniqc = proj_true->unique_ctrl_out();\n-    assert((uniqc == head && !head->is_strip_mined()) || (uniqc == head->in(LoopNode::EntryControl)\n-                                                          && head->is_strip_mined()), \"must hold by construction if no predicates\");\n-  } else {\n-    \/\/ There is at least one Parse Predicate. When skipping all predicates\/predicate blocks, we should end up\n-    \/\/ at 'proj_true'.\n-    assert(proj_true == predicates.entry(), \"must hold by construction if at least one Parse Predicate\");\n+\/\/ Enable more optimizations possibilities in the next IGVN round.\n+void PhaseIdealLoop::add_unswitched_loop_version_bodies_to_igvn(IdealLoopTree* loop, const Node_List& old_new) {\n+  loop->record_for_igvn();\n+  for(int i = loop->_body.size() - 1; i >= 0 ; i--) {\n+    Node* n = loop->_body[i];\n+    Node* n_clone = old_new[n->_idx];\n+    _igvn._worklist.push(n_clone);\n@@ -361,1 +514,5 @@\n-#endif \/\/ ASSERT\n+void PhaseIdealLoop::increment_unswitch_counts(LoopNode* original_head, LoopNode* new_head) {\n+  const int unswitch_count = original_head->unswitch_count() + 1;\n+  original_head->set_unswitch_count(unswitch_count);\n+  new_head->set_unswitch_count(unswitch_count);\n+}\n","filename":"src\/hotspot\/share\/opto\/loopUnswitch.cpp","additions":374,"deletions":217,"binary":false,"changes":591,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+class UnswitchedLoopSelector;\n@@ -776,0 +777,6 @@\n+  \/\/ Return the parent's IdealLoopTree for a strip mined loop which is the outer strip mined loop.\n+  \/\/ In all other cases, return this.\n+  IdealLoopTree* skip_strip_mined() {\n+    return _head->as_Loop()->is_strip_mined() ? _parent : this;\n+  }\n+\n@@ -1355,0 +1362,5 @@\n+  void replace_loop_entry(LoopNode* loop_head, Node* new_entry) {\n+    _igvn.replace_input_of(loop_head, LoopNode::EntryControl, new_entry);\n+    set_idom(loop_head, new_entry, dom_depth(new_entry));\n+  }\n+\n@@ -1394,2 +1406,0 @@\n-  bool has_control_dependencies_from_predicates(LoopNode* head) const;\n-  void verify_fast_loop(LoopNode* head, const ProjNode* proj_true) const NOT_DEBUG_RETURN;\n@@ -1410,8 +1420,0 @@\n-  \/\/ Create a slow version of the loop by cloning the loop\n-  \/\/ and inserting an if to select fast-slow versions.\n-  \/\/ Return the inserted if.\n-  IfNode* create_slow_version_of_loop(IdealLoopTree *loop,\n-                                      Node_List &old_new,\n-                                      Node_List &unswitch_iffs,\n-                                      CloneLoopMode mode);\n-\n@@ -1421,1 +1423,1 @@\n-  void do_unswitching (IdealLoopTree *loop, Node_List &old_new);\n+  void do_unswitching(IdealLoopTree* loop, Node_List& old_new);\n@@ -1424,1 +1426,20 @@\n-  IfNode* find_unswitching_candidate(const IdealLoopTree *loop, Node_List& unswitch_iffs) const;\n+  IfNode* find_unswitch_candidate(const IdealLoopTree* loop, Node_List& unswitch_iffs) const;\n+\n+ private:\n+  static bool has_control_dependencies_from_predicates(LoopNode* head);\n+  static void revert_to_normal_loop(const LoopNode* loop_head);\n+\n+  void hoist_invariant_check_casts(const IdealLoopTree* loop, const Node_List& old_new,\n+                                   const UnswitchedLoopSelector& unswitched_loop_selector);\n+  void add_unswitched_loop_version_bodies_to_igvn(IdealLoopTree* loop, const Node_List& old_new);\n+  static void increment_unswitch_counts(LoopNode* original_head, LoopNode* new_head);\n+  void remove_unswitch_candidate_from_loops(const Node_List& old_new, const UnswitchedLoopSelector& unswitched_loop_selector);\n+#ifndef PRODUCT\n+  static void trace_loop_unswitching_count(IdealLoopTree* loop, LoopNode* original_head, const Node_List& unswitch_iffs);\n+  static void trace_loop_unswitching_impossible(const LoopNode* original_head);\n+  static void trace_loop_unswitching_result(const UnswitchedLoopSelector& unswitched_loop_selector,\n+                                            const LoopNode* original_head, const LoopNode* new_head,\n+                                            const Node_List& old_new);\n+#endif\n+\n+ public:\n@@ -1633,0 +1654,1 @@\n+ public:\n@@ -1636,0 +1658,1 @@\n+ private:\n@@ -1752,0 +1775,2 @@\n+\n+  void pin_array_access_nodes_dependent_on(Node* ctrl);\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":37,"deletions":12,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -581,2 +581,2 @@\n-  \/\/ but not if I2 is a constant.\n-  if (n_op == Op_AddP) {\n+  \/\/ but not if I2 is a constant. Skip for irreducible loops.\n+  if (n_op == Op_AddP && n_loop->_head->is_Loop()) {\n@@ -1660,1 +1660,10 @@\n-          dominated_by(prevdom->as_IfProj(), n->as_If());\n+          \/\/ Split if: pin array accesses that are control dependent on a range check and moved to a regular if,\n+          \/\/ to prevent an array load from floating above its range check. There are three cases:\n+          \/\/ 1. Move from RangeCheck \"a\" to RangeCheck \"b\": don't need to pin. If we ever remove b, then we pin\n+          \/\/    all its array accesses at that point.\n+          \/\/ 2. We move from RangeCheck \"a\" to regular if \"b\": need to pin. If we ever remove b, then its array\n+          \/\/    accesses would start to float, since we don't pin at that point.\n+          \/\/ 3. If we move from regular if: don't pin. All array accesses are already assumed to be pinned.\n+          bool pin_array_access_nodes =  n->Opcode() == Op_RangeCheck &&\n+                                         prevdom->in(0)->Opcode() != Op_RangeCheck;\n+          dominated_by(prevdom->as_IfProj(), n->as_If(), false, pin_array_access_nodes);\n@@ -1839,1 +1848,14 @@\n-        _igvn.replace_input_of(n, 0, place_outside_loop(n_ctrl, loop_ctrl));\n+        Node* maybe_pinned_n = n;\n+        Node* outside_ctrl = place_outside_loop(n_ctrl, loop_ctrl);\n+        if (n->depends_only_on_test()) {\n+          Node* pinned_clone = n->pin_array_access_node();\n+          if (pinned_clone != nullptr) {\n+            \/\/ Pin array access nodes: if this is an array load, it's going to be dependent on a condition that's not a\n+            \/\/ range check for that access. If that condition is replaced by an identical dominating one, then an\n+            \/\/ unpinned load would risk floating above its range check.\n+            register_new_node(pinned_clone, n_ctrl);\n+            maybe_pinned_n = pinned_clone;\n+            _igvn.replace_node(n, pinned_clone);\n+          }\n+        }\n+        _igvn.replace_input_of(maybe_pinned_n, 0, outside_ctrl);\n@@ -1853,1 +1875,10 @@\n-          Node* x = n->clone(); \/\/ Clone computation\n+          Node* x = nullptr;\n+          if (n->depends_only_on_test()) {\n+            \/\/ Pin array access nodes: if this is an array load, it's going to be dependent on a condition that's not a\n+            \/\/ range check for that access. If that condition is replaced by an identical dominating one, then an\n+            \/\/ unpinned load would risk floating above its range check.\n+            x = n->pin_array_access_node();\n+          }\n+          if (x == nullptr) {\n+            x = n->clone();\n+          }\n@@ -2411,0 +2442,14 @@\n+      if (idx == 0 &&\n+          use->depends_only_on_test()) {\n+        Node* pinned_clone = use->pin_array_access_node();\n+        if (pinned_clone != nullptr) {\n+          \/\/ Pin array access nodes: control is updated here to a region. If, after some transformations, only one path\n+          \/\/ into the region is left, an array load could become dependent on a condition that's not a range check for\n+          \/\/ that access. If that condition is replaced by an identical dominating one, then an unpinned load would risk\n+          \/\/ floating above its range check.\n+          pinned_clone->set_req(0, phi);\n+          register_new_node(pinned_clone, get_ctrl(use));\n+          _igvn.replace_node(use, pinned_clone);\n+          continue;\n+        }\n+      }\n@@ -3050,2 +3095,1 @@\n-  IfNode* new_if = (opcode == Op_If) ? new IfNode(proj2, bol, iff->_prob, iff->_fcnt):\n-    new RangeCheckNode(proj2, bol, iff->_prob, iff->_fcnt);\n+  IfNode* new_if = IfNode::make_with_same_profile(iff, proj2, bol);\n@@ -4046,0 +4090,13 @@\n+      if (n_clone->depends_only_on_test()) {\n+        \/\/ Pin array access nodes: control is updated here to the loop head. If, after some transformations, the\n+        \/\/ backedge is removed, an array load could become dependent on a condition that's not a range check for that\n+        \/\/ access. If that condition is replaced by an identical dominating one, then an unpinned load would risk\n+        \/\/ floating above its range check.\n+        Node* pinned_clone = n_clone->pin_array_access_node();\n+        if (pinned_clone != nullptr) {\n+          register_new_node(pinned_clone, get_ctrl(n_clone));\n+          old_new.map(n->_idx, pinned_clone);\n+          _igvn.replace_node(n_clone, pinned_clone);\n+          n_clone = pinned_clone;\n+        }\n+      }\n@@ -4376,1 +4433,2 @@\n-  fix_data_uses(wq, loop, ControlAroundStripMined, head->is_strip_mined() ? loop->_parent : loop, new_counter, old_new, worklist, split_if_set, split_bool_set, split_cex_set);\n+  fix_data_uses(wq, loop, ControlAroundStripMined, loop->skip_strip_mined(), new_counter, old_new, worklist,\n+                split_if_set, split_bool_set, split_cex_set);\n@@ -4415,1 +4473,6 @@\n-  SuperWord sw(vloop, vshared);\n+  const VLoopAnalyzer vloop_analyzer(vloop, vshared);\n+  if (!vloop_analyzer.success()) {\n+    return AutoVectorizeStatus::TriedAndFailed;\n+  }\n+\n+  SuperWord sw(vloop_analyzer);\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":73,"deletions":10,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -2115,2 +2115,3 @@\n-void PhaseMacroExpand::mark_eliminated_box(Node* oldbox, Node* obj) {\n-  if (oldbox->as_BoxLock()->is_eliminated()) {\n+void PhaseMacroExpand::mark_eliminated_box(Node* box, Node* obj) {\n+  BoxLockNode* oldbox = box->as_BoxLock();\n+  if (oldbox->is_eliminated()) {\n@@ -2119,0 +2120,1 @@\n+  assert(!oldbox->is_unbalanced(), \"this should not be called for unbalanced region\");\n@@ -2126,0 +2128,1 @@\n+    oldbox->set_local();      \/\/ This verifies correct state of BoxLock\n@@ -2127,1 +2130,1 @@\n-    oldbox->as_BoxLock()->set_eliminated(); \/\/ This changes box's hash value\n+    oldbox->set_eliminated(); \/\/ This changes box's hash value\n@@ -2154,0 +2157,1 @@\n+  newbox->set_local(); \/\/ This verifies correct state of BoxLock\n@@ -2209,0 +2213,3 @@\n+  if (alock->box_node()->as_BoxLock()->is_unbalanced()) {\n+    return; \/\/ Can't do any more elimination for this locking region\n+  }\n@@ -2849,0 +2856,5 @@\n+  } else {\n+    \/\/ After coarsened locks are eliminated locking regions\n+    \/\/ become unbalanced. We should not execute any more\n+    \/\/ locks elimination optimizations on them.\n+    C->mark_unbalanced_boxes();\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":15,"deletions":3,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -3504,0 +3504,1 @@\n+          assert(!trailing_load_store(), \"load store node can't be eliminated\");\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,1 +37,1 @@\n-#include \"gc\/g1\/heapRegion.hpp\"\n+#include \"gc\/g1\/g1HeapRegion.hpp\"\n@@ -327,3 +327,2 @@\n-    const size_t hs = arrayOopDesc::header_size(elem_type);\n-    \/\/ Align to next 8 bytes to avoid trashing arrays's length.\n-    const size_t aligned_hs = align_object_offset(hs);\n+    size_t hs_bytes = arrayOopDesc::base_offset_in_bytes(elem_type);\n+    assert(is_aligned(hs_bytes, BytesPerInt), \"must be 4 byte aligned\");\n@@ -331,2 +330,3 @@\n-    if (aligned_hs > hs) {\n-      Copy::zero_to_words(obj+hs, aligned_hs-hs);\n+    if (!is_aligned(hs_bytes, BytesPerLong)) {\n+      *reinterpret_cast<jint*>(reinterpret_cast<char*>(obj) + hs_bytes) = 0;\n+      hs_bytes += BytesPerInt;\n@@ -334,0 +334,1 @@\n+\n@@ -335,0 +336,2 @@\n+    assert(is_aligned(hs_bytes, BytesPerLong), \"must be 8-byte aligned\");\n+    const size_t aligned_hs = hs_bytes \/ BytesPerLong;\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":10,"deletions":7,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -730,0 +730,8 @@\n+  if (iff->Opcode() == Op_RangeCheck) {\n+    \/\/ Pin array access nodes: control is updated here to a region. If, after some transformations, only one path\n+    \/\/ into the region is left, an array load could become dependent on a condition that's not a range check for\n+    \/\/ that access. If that condition is replaced by an identical dominating one, then an unpinned load would risk\n+    \/\/ floating above its range check.\n+    pin_array_access_nodes_dependent_on(new_true);\n+    pin_array_access_nodes_dependent_on(new_false);\n+  }\n@@ -740,0 +748,15 @@\n+\n+void PhaseIdealLoop::pin_array_access_nodes_dependent_on(Node* ctrl) {\n+  for (DUIterator i = ctrl->outs(); ctrl->has_out(i); i++) {\n+    Node* use = ctrl->out(i);\n+    if (!use->depends_only_on_test()) {\n+      continue;\n+    }\n+    Node* pinned_clone = use->pin_array_access_node();\n+    if (pinned_clone != nullptr) {\n+      register_new_node(pinned_clone, get_ctrl(use));\n+      _igvn.replace_node(use, pinned_clone);\n+      --i;\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/share\/opto\/split_if.cpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -5503,1 +5503,2 @@\n-    int header_size = objArrayOopDesc::header_size() * wordSize;\n+    BasicType basic_elem_type = elem()->basic_type();\n+    int header_size = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n@@ -5508,1 +5509,0 @@\n-      BasicType basic_elem_type = elem()->basic_type();\n@@ -5512,2 +5512,1 @@\n-        int array_base = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n-        st->print(\"[%d]\", (offset() - array_base)\/elem_size);\n+        st->print(\"[%d]\", (offset() - header_size)\/elem_size);\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -76,1 +76,1 @@\n-  ( arrayOopDesc::header_size(T_DOUBLE) * HeapWordSize \\\n+  ( arrayOopDesc::base_offset_in_bytes(T_DOUBLE) \\\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -112,2 +112,2 @@\n-#include \"gc\/g1\/heapRegionManager.hpp\"\n-#include \"gc\/g1\/heapRegionRemSet.inline.hpp\"\n+#include \"gc\/g1\/g1HeapRegionManager.hpp\"\n+#include \"gc\/g1\/g1HeapRegionRemSet.inline.hpp\"\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -311,1 +311,1 @@\n-      JvmtiAgentList::load_agent(\"instrument\", \"false\", _libpath.value(), output());\n+      JvmtiAgentList::load_agent(\"instrument\", false, _libpath.value(), output());\n@@ -328,1 +328,1 @@\n-      JvmtiAgentList::load_agent(\"instrument\", \"false\", opt, output());\n+      JvmtiAgentList::load_agent(\"instrument\", false, opt, output());\n@@ -333,1 +333,1 @@\n-    JvmtiAgentList::load_agent(_libpath.value(), \"true\", _option.value(), output());\n+    JvmtiAgentList::load_agent(_libpath.value(), true, _option.value(), output());\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1058,1 +1058,0 @@\n-const juint    badMetaWordVal     = 0xBAADFADE;             \/\/ value used to zap metadata heap after GC\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+import java.nio.charset.StandardCharsets;\n@@ -45,0 +46,1 @@\n+import jdk.internal.access.JavaLangAccess;\n@@ -3025,0 +3027,2 @@\n+        \/** access to internal methods to count ASCII and inflate latin1\/ASCII bytes to char *\/\n+        private static final JavaLangAccess JLA = SharedSecrets.getJavaLangAccess();\n@@ -3701,0 +3705,4 @@\n+            if (!blkmode) {\n+                end = pos = 0;\n+            }\n+\n@@ -3703,0 +3711,20 @@\n+                \/\/ Scan for leading ASCII chars\n+                int avail = end - pos;\n+                int ascii = JLA.countPositives(buf, pos, Math.min(avail, (int)utflen));\n+                if (ascii == utflen) {\n+                    \/\/ Complete match, consume the buf[pos ... pos + ascii] range and return.\n+                    \/\/ Modified UTF-8 and ISO-8859-1 are both ASCII-compatible encodings bytes\n+                    \/\/ thus we can treat the range as ISO-8859-1 and avoid a redundant scan\n+                    \/\/ in the String constructor\n+                    String utf = new String(buf, pos, ascii, StandardCharsets.ISO_8859_1);\n+                    pos += ascii;\n+                    return utf;\n+                }\n+                \/\/ Avoid allocating a StringBuilder if there's enough data in buf and\n+                \/\/ cbuf is large enough\n+                if (avail >= utflen && utflen <= CHAR_BUF_SIZE) {\n+                    JLA.inflateBytesToChars(buf, pos, cbuf, 0, ascii);\n+                    pos += ascii;\n+                    int cbufPos = readUTFSpan(ascii, utflen - ascii);\n+                    return new String(cbuf, 0, cbufPos);\n+                }\n@@ -3710,4 +3738,0 @@\n-            if (!blkmode) {\n-                end = pos = 0;\n-            }\n-\n@@ -3717,1 +3741,5 @@\n-                    utflen -= readUTFSpan(sbuf, utflen);\n+                    int cbufPos = readUTFSpan(0, utflen);\n+                    \/\/ pos has advanced: adjust utflen by the difference in\n+                    \/\/ available bytes\n+                    utflen -= avail - (end - pos);\n+                    sbuf.append(cbuf, 0, cbufPos);\n@@ -3739,3 +3767,3 @@\n-         * (starting at offset pos and ending at or before offset end),\n-         * consuming no more than utflen bytes.  Appends read characters to\n-         * sbuf.  Returns the number of bytes consumed.\n+         * (starting at offset pos), consuming no more than utflen bytes.\n+         * Appends read characters to cbuf. Returns the current position\n+         * in cbuf.\n@@ -3743,1 +3771,1 @@\n-        private long readUTFSpan(StringBuilder sbuf, long utflen)\n+        private int readUTFSpan(int cpos, long utflen)\n@@ -3746,1 +3774,0 @@\n-            int cpos = 0;\n@@ -3750,1 +3777,1 @@\n-            int stop = pos + ((utflen > avail) ? avail - 2 : (int) utflen);\n+            int stop = start + ((utflen > avail) ? avail - 2 : (int) utflen);\n@@ -3795,3 +3822,1 @@\n-\n-            sbuf.append(cbuf, 0, cpos);\n-            return pos - start;\n+            return cpos;\n","filename":"src\/java.base\/share\/classes\/java\/io\/ObjectInputStream.java","additions":39,"deletions":14,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -818,0 +818,1 @@\n+            boolean genericPatternsExpanded = false;\n@@ -827,1 +828,0 @@\n-                    patterns = updatedPatterns;\n@@ -831,0 +831,13 @@\n+                    if (!repeat && !genericPatternsExpanded) {\n+                        \/\/there may be situation like:\n+                        \/\/class B extends S1, S2\n+                        \/\/patterns: R(S1, B), R(S2, S2)\n+                        \/\/this should be joined to R(B, S2),\n+                        \/\/but hashing in reduceNestedPatterns will not allow that\n+                        \/\/attempt to once expand all types to their transitive permitted types,\n+                        \/\/on all depth of nesting:\n+                        updatedPatterns = expandGenericPatterns(updatedPatterns);\n+                        genericPatternsExpanded = true;\n+                        repeat = !updatedPatterns.equals(patterns);\n+                    }\n+                    patterns = updatedPatterns;\n@@ -1133,0 +1146,34 @@\n+        private Set<PatternDescription> expandGenericPatterns(Set<PatternDescription> patterns) {\n+            var newPatterns = new HashSet<PatternDescription>(patterns);\n+            boolean modified;\n+            do {\n+                modified = false;\n+                for (PatternDescription pd : patterns) {\n+                    if (pd instanceof RecordPattern rpOne) {\n+                        for (int i = 0; i < rpOne.nested.length; i++) {\n+                            Set<PatternDescription> toExpand = Set.of(rpOne.nested[i]);\n+                            Set<PatternDescription> expanded = expandGenericPatterns(toExpand);\n+                            if (expanded != toExpand) {\n+                                expanded.removeAll(toExpand);\n+                                for (PatternDescription exp : expanded) {\n+                                    PatternDescription[] newNested = Arrays.copyOf(rpOne.nested, rpOne.nested.length);\n+                                    newNested[i] = exp;\n+                                    modified |= newPatterns.add(new RecordPattern(rpOne.recordType(), rpOne.fullComponentTypes(), newNested));\n+                                }\n+                            }\n+                        }\n+                    } else if (pd instanceof BindingPattern bp) {\n+                        Set<Symbol> permittedSymbols = allPermittedSubTypes((ClassSymbol) bp.type.tsym, cs -> true);\n+\n+                        if (!permittedSymbols.isEmpty()) {\n+                            for (Symbol permitted : permittedSymbols) {\n+                                \/\/TODO infer.instantiatePatternType(selectorType, csym); (?)\n+                                modified |= newPatterns.add(new BindingPattern(permitted.type));\n+                            }\n+                        }\n+                    }\n+                }\n+            } while (modified);\n+            return newPatterns;\n+        }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Flow.java","additions":48,"deletions":1,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -79,0 +79,2 @@\n+compiler\/startup\/StartupOutput.java 8326615 generic-x64\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,2 +27,2 @@\n-#include \"agent_common.h\"\n-#include \"JVMTITools.h\"\n+#include \"agent_common.hpp\"\n+#include \"JVMTITools.hpp\"\n","filename":"test\/hotspot\/jtreg\/vmTestbase\/nsk\/jvmti\/GetObjectMonitorUsage\/objmonusage007\/objmonusage007.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -629,0 +629,1 @@\n+javax\/net\/ssl\/SSLSession\/CertMsgCheck.java                      8326705 generic-all\n","filename":"test\/jdk\/ProblemList.txt","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -656,1 +656,1 @@\n-     * method to return true and allow any flags.\n+     * method to return true or false and allow or reject any flags.\n@@ -662,2 +662,3 @@\n-        if (System.getenv(\"TEST_VM_FLAGLESS\") != null) {\n-            return \"\" + result;\n+        String flagless = System.getenv(\"TEST_VM_FLAGLESS\");\n+        if (flagless != null) {\n+            return \"\" + \"true\".equalsIgnoreCase(flagless);\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -69,0 +69,1 @@\n+tools\/javac\/patterns\/Exhaustiveness.java \t\t\t\t\t8326616    generic-all    intermittently timeout\n","filename":"test\/langtools\/ProblemList.txt","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}