{"files":[{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2026, Oracle and\/or its affiliates. All rights reserved.\n@@ -216,5 +216,2 @@\n-  \/\/ TODO 8366717 This came with 8284161: Implementation of Virtual Threads (Preview) later in May 2022\n-  \/\/ Check if it's sufficient\n-  \/\/__ push_call_clobbered_registers();\n-  assert_different_registers(rscratch1, pre_val); \/\/ push_CPU_state trashes rscratch1\n-  __ push_CPU_state(true);\n+  assert_different_registers(rscratch1, pre_val); \/\/ push_call_clobbered_registers trashes rscratch1\n+  __ push_call_clobbered_registers();\n@@ -241,1 +238,1 @@\n-  __ pop_CPU_state(true);\n+  __ pop_call_clobbered_registers();\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.cpp","additions":4,"deletions":7,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2026, Oracle and\/or its affiliates. All rights reserved.\n@@ -116,9 +116,1 @@\n-      if (tmp3 != noreg) {\n-        \/\/ TODO 8366717 This change is from before the 'tmp3' arg was added to mainline, check if it's still needed. Same on x64. Also, this should be a __ lea\n-        \/\/ Called by MacroAssembler::pack_inline_helper. We cannot corrupt the dst.base() register\n-        __ mov(tmp3, dst.base());\n-        store_check(masm, tmp3, dst);\n-      } else {\n-        \/\/ It's OK to corrupt the dst.base() register.\n-        store_check(masm, dst.base(), dst);\n-      }\n+      store_check(masm, dst.base(), dst);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/cardTableBarrierSetAssembler_aarch64.cpp","additions":2,"deletions":10,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2026, Oracle and\/or its affiliates. All rights reserved.\n@@ -7180,1 +7180,0 @@\n-  \/\/ TODO 8366717 We need to make sure that r14 (and potentially other long-life regs) are kept live in slowpath runtime calls in GC barriers\n@@ -7183,0 +7182,8 @@\n+\n+#ifndef ASSERT\n+  RegSet clobbered_gp_regs = MacroAssembler::call_clobbered_gp_registers();\n+  assert(clobbered_gp_regs.contains(tmp1), \"tmp1 must be saved explicitly if it's not a clobber\");\n+  assert(clobbered_gp_regs.contains(tmp2), \"tmp2 must be saved explicitly if it's not a clobber\");\n+  assert(clobbered_gp_regs.contains(r14), \"r14 must be saved explicitly if it's not a clobber\");\n+#endif\n+\n@@ -7184,1 +7191,1 @@\n-  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, true);\n@@ -7285,1 +7292,0 @@\n-  \/\/ TODO 8366717 This is probably okay but looks fishy because stream is reset in the \"Set null marker to zero\" case just above. Same on x64.\n@@ -7377,1 +7383,4 @@\n-        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+        \/\/ store_heap_oop transitively calls oop_store_at which corrupts to.base(). We need to keep val_obj valid.\n+        mov(tmp3, val_obj);\n+        Address dst_with_tmp3(tmp3, off);\n+        store_heap_oop(dst_with_tmp3, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":14,"deletions":5,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2026, Oracle and\/or its affiliates. All rights reserved.\n@@ -211,1 +211,0 @@\n-  OopMapSet *oop_maps = new OopMapSet();\n@@ -569,0 +568,2 @@\n+      \/\/ store_heap_oop transitively calls oop_store_at which corrupts to.base(). We need to keep it valid.\n+      __ push(to.base(), sp);\n@@ -570,0 +571,1 @@\n+      __ pop(to.base(), sp);\n@@ -623,1 +625,0 @@\n-  \/\/ TODO 8366717 Is the comment about r13 correct? Isn't that r19_sender_sp?\n@@ -626,1 +627,1 @@\n-  \/\/ convention, rmethod (r12), and r13 which holds the outgoing sender\n+  \/\/ convention, rmethod (r12), and r19 which holds the outgoing sender\n@@ -628,1 +629,0 @@\n-  \/\/ TODO 8366717 We need to make sure that buf_array, buf_oop (and potentially other long-life regs) are kept live in slowpath runtime calls in GC barriers\n@@ -635,0 +635,9 @@\n+#ifndef ASSERT\n+  RegSet clobbered_gp_regs = MacroAssembler::call_clobbered_gp_registers();\n+  assert(clobbered_gp_regs.contains(buf_array), \"buf_array must be saved explicitly if it's not a clobber\");\n+  assert(clobbered_gp_regs.contains(buf_oop), \"buf_oop must be saved explicitly if it's not a clobber\");\n+  assert(clobbered_gp_regs.contains(tmp1), \"tmp1 must be saved explicitly if it's not a clobber\");\n+  assert(clobbered_gp_regs.contains(tmp2), \"tmp2 must be saved explicitly if it's not a clobber\");\n+  assert(clobbered_gp_regs.contains(tmp3), \"tmp3 must be saved explicitly if it's not a clobber\");\n+#endif\n+\n@@ -645,1 +654,0 @@\n-      \/\/ TODO 8366717 Do we need to save vectors here? They could be used as arg registers, right? Same on x64.\n@@ -3039,1 +3047,4 @@\n-        __ store_heap_oop(to, val, r15, r16, r17, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+        \/\/ store_heap_oop transitively calls oop_store_at which corrupts to.base(). We need to keep r0 valid.\n+        __ mov(r17, r0);\n+        Address to_with_r17(r17, off);\n+        __ store_heap_oop(to_with_r17, val, r15, r16, r17, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":18,"deletions":7,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2026, Oracle and\/or its affiliates. All rights reserved.\n@@ -239,18 +239,2 @@\n-  if (Arguments::is_valhalla_enabled() && InlineTypePassFieldsAsArgs) {\n-    \/\/ Barriers might be emitted when converting between (scalarized) calling conventions for inline\n-    \/\/ types. Save all argument registers before calling into the runtime.\n-    \/\/ TODO 8366717: use push_set() (see JDK-8283327 push\/pop_call_clobbered_registers & aarch64 )\n-    __ pusha();\n-    __ subptr(rsp, 64);\n-    __ movdbl(Address(rsp, 0),  j_farg0);\n-    __ movdbl(Address(rsp, 8),  j_farg1);\n-    __ movdbl(Address(rsp, 16), j_farg2);\n-    __ movdbl(Address(rsp, 24), j_farg3);\n-    __ movdbl(Address(rsp, 32), j_farg4);\n-    __ movdbl(Address(rsp, 40), j_farg5);\n-    __ movdbl(Address(rsp, 48), j_farg6);\n-    __ movdbl(Address(rsp, 56), j_farg7);\n-  } else {\n-    \/\/ Determine and save the live input values\n-    __ push_call_clobbered_registers();\n-  }\n+  \/\/ Determine and save the live input values\n+  __ push_call_clobbered_registers();\n@@ -283,15 +267,1 @@\n-  if (Arguments::is_valhalla_enabled() && InlineTypePassFieldsAsArgs) {\n-    \/\/ Restore registers\n-    __ movdbl(j_farg0, Address(rsp, 0));\n-    __ movdbl(j_farg1, Address(rsp, 8));\n-    __ movdbl(j_farg2, Address(rsp, 16));\n-    __ movdbl(j_farg3, Address(rsp, 24));\n-    __ movdbl(j_farg4, Address(rsp, 32));\n-    __ movdbl(j_farg5, Address(rsp, 40));\n-    __ movdbl(j_farg6, Address(rsp, 48));\n-    __ movdbl(j_farg7, Address(rsp, 56));\n-    __ addptr(rsp, 64);\n-    __ popa();\n-  } else {\n-    __ pop_call_clobbered_registers();\n-  }\n+  __ pop_call_clobbered_registers();\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.cpp","additions":4,"deletions":34,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2026, Oracle and\/or its affiliates. All rights reserved.\n@@ -183,8 +183,1 @@\n-      if (tmp3 != noreg) {\n-        \/\/ Called by MacroAssembler::pack_inline_helper. We cannot corrupt the dst.base() register\n-        __ movptr(tmp3, dst.base());\n-        store_check(masm, tmp3, dst);\n-      } else {\n-        \/\/ It's OK to corrupt the dst.base() register.\n-        store_check(masm, dst.base(), dst);\n-      }\n+      store_check(masm, dst.base(), dst);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/cardTableBarrierSetAssembler_x86.cpp","additions":2,"deletions":9,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2026, Oracle and\/or its affiliates. All rights reserved.\n@@ -6109,1 +6109,1 @@\n-  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, true);\n@@ -6296,1 +6296,4 @@\n-        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+        \/\/ store_heap_oop transitively calls oop_store_at which corrupts to.base(). We need to keep val_obj valid.\n+        mov(tmp3, val_obj);\n+        Address dst_with_tmp3(tmp3, off);\n+        store_heap_oop(dst_with_tmp3, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2026, Oracle and\/or its affiliates. All rights reserved.\n@@ -852,0 +852,2 @@\n+      \/\/ store_heap_oop transitively calls oop_store_at which corrupts to.base(). We need to keep it valid.\n+      __ push(to.base());\n@@ -853,0 +855,1 @@\n+      __ pop(to.base());\n@@ -921,1 +924,1 @@\n-      OopMap* map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_vectors*\/ false);\n+      OopMap* map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_wide_vectors*\/ false);\n@@ -3828,1 +3831,0 @@\n-    VMReg r_2 = pair.second();\n@@ -3838,1 +3840,4 @@\n-        __ store_heap_oop(to, val, r14, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+        \/\/ store_heap_oop transitively calls oop_store_at which corrupts to.base(). We need to keep rax valid.\n+        __ mov(rbx, rax);\n+        Address to_with_rbx(rbx, off);\n+        __ store_heap_oop(to_with_rbx, val, r14, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2026, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,1 +38,1 @@\n-  int _step;\n+  const int _step;\n@@ -42,2 +42,2 @@\n-  ScalarizedInlineArgsStream(const GrowableArray<SigEntry>* sig, int sig_idx, VMRegPair* regs, int regs_count, int regs_idx, int step = 1)\n-    : _sig(sig), _sig_idx(sig_idx), _regs(regs), _regs_count(regs_count), _regs_idx(regs_idx), _step(step) {\n+  ScalarizedInlineArgsStream(const GrowableArray<SigEntry>* sig, int sig_idx, VMRegPair* regs, int regs_count, int regs_idx, bool reverse = false)\n+    : _sig(sig), _sig_idx(sig_idx), _regs(regs), _regs_count(regs_count), _regs_idx(regs_idx), _step(reverse ? -1 : 1) {\n","filename":"src\/hotspot\/share\/runtime\/signature_cc.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"}]}