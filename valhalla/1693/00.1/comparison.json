{"files":[{"patch":"@@ -473,1 +473,1 @@\n-  JVM_HEAP_LIMIT_64=\"2048\"\n+  JVM_HEAP_LIMIT_64=\"3200\"\n","filename":"make\/autoconf\/boot-jdk.m4","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1479,0 +1479,1 @@\n+        args = concat(args, \"--with-version-pre=\" + version_numbers.get(\"DEFAULT_PROMOTED_VERSION_PRE\"));\n","filename":"make\/conf\/jib-profiles.js","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -54,4 +54,3 @@\n-  JVM_EXCLUDE_FILES += templateInterpreter.cpp \\\n-      templateInterpreterGenerator.cpp bcEscapeAnalyzer.cpp ciTypeFlow.cpp\n-  JVM_CFLAGS_FEATURES += -DZERO \\\n-      -DZERO_LIBARCH='\"$(OPENJDK_TARGET_CPU_LEGACY_LIB)\"' $(LIBFFI_CFLAGS)\n+  JVM_EXCLUDE_FILES += templateInterpreter.cpp templateInterpreterGenerator.cpp \\\n+                       bcEscapeAnalyzer.cpp ciTypeFlow.cpp macroAssembler_common.cpp\n+  JVM_CFLAGS_FEATURES += -DZERO -DZERO_LIBARCH='\"$(OPENJDK_TARGET_CPU_LEGACY_LIB)\"' $(LIBFFI_CFLAGS)\n","filename":"make\/hotspot\/lib\/JvmFeatures.gmk","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+include gensrc\/GensrcValueClasses.gmk\n","filename":"make\/modules\/java.base\/Gensrc.gmk","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -49,0 +49,21 @@\n+void C2_MacroAssembler::entry_barrier() {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ Dummy labels for just measuring the code size\n+  Label dummy_slow_path;\n+  Label dummy_continuation;\n+  Label dummy_guard;\n+  Label* slow_path = &dummy_slow_path;\n+  Label* continuation = &dummy_continuation;\n+  Label* guard = &dummy_guard;\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+    C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n+    Compile::current()->output()->add_stub(stub);\n+    slow_path = &stub->entry();\n+    continuation = &stub->continuation();\n+    guard = &stub->guard();\n+  }\n+  \/\/ In the C2 code, we move the non-hot part of nmethod entry barriers out-of-line to a stub.\n+  bs->nmethod_entry_barrier(this, slow_path, continuation, guard);\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":21,"deletions":0,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -47,0 +47,2 @@\n+  void entry_barrier();\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -135,0 +135,1 @@\n+  assert_different_registers(value, temp1, temp2);\n@@ -208,1 +209,13 @@\n-  __ push_call_clobbered_registers();\n+  \/\/ save the live input values\n+  RegSet saved = RegSet::of(pre_val);\n+  FloatRegSet fsaved;\n+\n+  \/\/ Barriers might be emitted when converting between (scalarized) calling\n+  \/\/ conventions for inline types. Save all argument registers before calling\n+  \/\/ into the runtime.\n+\n+  \/\/ TODO 8366717 This came with 8284161: Implementation of Virtual Threads (Preview) later in May 2022\n+  \/\/ Check if it's sufficient\n+  \/\/__ push_call_clobbered_registers();\n+  assert_different_registers(rscratch1, pre_val); \/\/ push_CPU_state trashes rscratch1\n+  __ push_CPU_state(true);\n@@ -229,1 +242,1 @@\n-  __ pop_call_clobbered_registers();\n+  __ pop_CPU_state(true);\n@@ -370,0 +383,10 @@\n+\n+  bool in_heap = (decorators & IN_HEAP) != 0;\n+  bool as_normal = (decorators & AS_NORMAL) != 0;\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n+\n+  bool needs_pre_barrier = as_normal && !dest_uninitialized;\n+  bool needs_post_barrier = (val != noreg && in_heap);\n+\n+  assert_different_registers(val, tmp1, tmp2, tmp3);\n+\n@@ -379,8 +402,10 @@\n-  g1_write_barrier_pre(masm,\n-                       tmp3 \/* obj *\/,\n-                       tmp2 \/* pre_val *\/,\n-                       rthread \/* thread *\/,\n-                       tmp1  \/* tmp1 *\/,\n-                       rscratch2  \/* tmp2 *\/,\n-                       val != noreg \/* tosca_live *\/,\n-                       false \/* expand_call *\/);\n+  if (needs_pre_barrier) {\n+    g1_write_barrier_pre(masm,\n+                         tmp3 \/* obj *\/,\n+                         tmp2 \/* pre_val *\/,\n+                         rthread \/* thread *\/,\n+                         tmp1  \/* tmp1 *\/,\n+                         rscratch2  \/* tmp2 *\/,\n+                         val != noreg \/* tosca_live *\/,\n+                         false \/* expand_call *\/);\n+  }\n@@ -393,3 +418,5 @@\n-    if (UseCompressedOops) {\n-      new_val = rscratch2;\n-      __ mov(new_val, val);\n+    if (needs_post_barrier) {\n+      if (UseCompressedOops) {\n+        new_val = rscratch2;\n+        __ mov(new_val, val);\n+      }\n@@ -397,0 +424,1 @@\n+\n@@ -398,6 +426,8 @@\n-    g1_write_barrier_post(masm,\n-                          tmp3 \/* store_adr *\/,\n-                          new_val \/* new_val *\/,\n-                          rthread \/* thread *\/,\n-                          tmp1 \/* tmp1 *\/,\n-                          tmp2 \/* tmp2 *\/);\n+    if (needs_post_barrier) {\n+      g1_write_barrier_post(masm,\n+                            tmp3 \/* store_adr *\/,\n+                            new_val \/* new_val *\/,\n+                            rthread \/* thread *\/,\n+                            tmp1 \/* tmp1 *\/,\n+                            tmp2 \/* tmp2 *\/);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.cpp","additions":49,"deletions":19,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -76,0 +76,85 @@\n+\/\/ TODO 8350865 (same applies to g1StoreLSpecialTwoOops)\n+\/\/ - Can we use an unbound register for src?\n+\/\/ - Do no set\/overwrite barrier data here, also handle G1C2BarrierPostNotNull\n+\/\/ - Is the zero-extend really required in all the places?\n+\/\/ - Move this into the .m4?\n+instruct g1StoreLSpecialOneOop(indirect mem, iRegL_R11 src, immI off, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegPNoSp tmp4, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC);\n+  match(Set mem (StoreLSpecial mem (Binary src off)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, KILL cr);\n+  ins_cost(INSN_COST);\n+  format %{ \"str  $src, $mem\\t# g1StoreLSpecialOneOop\" %}\n+  ins_encode %{\n+    ((MachNode*)this)->set_barrier_data(G1C2BarrierPre | G1C2BarrierPost);\n+\n+    \/\/ Adjust address to point to narrow oop\n+    __ add($tmp4$$Register, $mem$$Register, $off$$constant);\n+    write_barrier_pre(masm, this,\n+                      $tmp4$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/);\n+\n+    __ str($src$$Register, $mem$$Register);\n+\n+    \/\/ Shift long value to extract the narrow oop field value and zero-extend it\n+    __ lsr($src$$Register, $src$$Register, $off$$constant << LogBitsPerByte);\n+    __ ubfm($src$$Register, $src$$Register, 0, 31);\n+\n+    write_barrier_post(masm, this,\n+                       $tmp4$$Register \/* store_addr *\/,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+instruct g1StoreLSpecialTwoOops(indirect mem, iRegL_R11 src, iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegPNoSp tmp4, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC);\n+  match(Set mem (StoreLSpecial mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, KILL cr);\n+  ins_cost(INSN_COST);\n+  format %{ \"str  $src, $mem\\t# g1StoreLSpecialTwoOops\" %}\n+  ins_encode %{\n+    ((MachNode*)this)->set_barrier_data(G1C2BarrierPre | G1C2BarrierPost);\n+\n+    write_barrier_pre(masm, this,\n+                      $mem$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register) \/* preserve *\/);\n+    \/\/ Adjust address to point to the second narrow oop in the long value\n+    __ add($tmp4$$Register, $mem$$Register, 4);\n+    write_barrier_pre(masm, this,\n+                      $tmp4$$Register \/* obj *\/,\n+                      $tmp1$$Register \/* pre_val *\/,\n+                      $tmp2$$Register \/* tmp1 *\/,\n+                      $tmp3$$Register \/* tmp2 *\/,\n+                      RegSet::of($mem$$Register, $src$$Register, $tmp4$$Register) \/* preserve *\/);\n+\n+    __ str($src$$Register, $mem$$Register);\n+\n+    \/\/ Zero-extend first narrow oop to long\n+    __ ubfm($tmp1$$Register, $src$$Register, 0, 31);\n+\n+    \/\/ Shift long value to extract the second narrow oop field value\n+    __ lsr($src$$Register, $src$$Register, 32);\n+    write_barrier_post(masm, this,\n+                       $mem$$Register \/* store_addr *\/,\n+                       $tmp1$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+    write_barrier_post(masm, this,\n+                       $tmp4$$Register \/* store_addr *\/,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp2$$Register \/* tmp1 *\/,\n+                       $tmp3$$Register \/* tmp2 *\/);\n+  %}\n+  ins_pipe(istore_reg_mem);\n+%}\n+\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1_aarch64.ad","additions":85,"deletions":0,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -362,0 +363,79 @@\n+\n+const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt, VMRegPair *regs, int total_args_passed) {\n+\n+  \/\/ Create the mapping between argument positions and registers.\n+\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+    r0 \/* j_rarg7 *\/, j_rarg6, j_rarg5, j_rarg4, j_rarg3, j_rarg2, j_rarg1, j_rarg0\n+  };\n+\n+  static const FloatRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3, j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+      \/\/ Should T_METADATA be added to java_calling_convention as well ?\n+    case T_METADATA:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < SharedRuntime::java_return_convention_max_float) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < SharedRuntime::java_return_convention_max_float) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -396,0 +476,107 @@\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+    for (int i = 0; i < sig_extended->length(); i++) {\n+      BasicType bt = sig_extended->at(i)._bt;\n+      if (bt == T_METADATA) {\n+        \/\/ In sig_extended, an inline type argument starts with:\n+        \/\/ T_METADATA, followed by the types of the fields of the\n+        \/\/ inline type and T_VOID to mark the end of the value\n+        \/\/ type. Inline types are flattened so, for instance, in the\n+        \/\/ case of an inline type with an int field and an inline type\n+        \/\/ field that itself has 2 fields, an int and a long:\n+        \/\/ T_METADATA T_INT T_METADATA T_INT T_LONG T_VOID (second\n+        \/\/ slot for the T_LONG) T_VOID (inner inline type) T_VOID\n+        \/\/ (outer inline type)\n+        total_args_passed++;\n+        int vt = 1;\n+        do {\n+          i++;\n+          BasicType bt = sig_extended->at(i)._bt;\n+          BasicType prev_bt = sig_extended->at(i-1)._bt;\n+          if (bt == T_METADATA) {\n+            vt++;\n+          } else if (bt == T_VOID &&\n+                     prev_bt != T_LONG &&\n+                     prev_bt != T_DOUBLE) {\n+            vt--;\n+          }\n+        } while (vt != 0);\n+      } else {\n+        total_args_passed++;\n+      }\n+    }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n+  return total_args_passed;\n+}\n+\n+\n+static void gen_c2i_adapter_helper(MacroAssembler* masm,\n+                                   BasicType bt,\n+                                   BasicType prev_bt,\n+                                   size_t size_in_bytes,\n+                                   const VMRegPair& reg_pair,\n+                                   const Address& to,\n+                                   Register tmp1,\n+                                   Register tmp2,\n+                                   Register tmp3,\n+                                   int extraspace,\n+                                   bool is_oop) {\n+  if (bt == T_VOID) {\n+    assert(prev_bt == T_LONG || prev_bt == T_DOUBLE, \"missing half\");\n+    return;\n+  }\n+\n+  \/\/ Say 4 args:\n+  \/\/ i   st_off\n+  \/\/ 0   32 T_LONG\n+  \/\/ 1   24 T_VOID\n+  \/\/ 2   16 T_OBJECT\n+  \/\/ 3    8 T_BOOL\n+  \/\/ -    0 return address\n+  \/\/\n+  \/\/ However to make thing extra confusing. Because we can fit a Java long\/double in\n+  \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n+  \/\/ leaves one slot empty and only stores to a single slot. In this case the\n+  \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+\n+  bool wide = (size_in_bytes == wordSize);\n+  VMReg r_1 = reg_pair.first();\n+  VMReg r_2 = reg_pair.second();\n+  assert(r_2->is_valid() == wide, \"invalid size\");\n+  if (!r_1->is_valid()) {\n+    assert(!r_2->is_valid(), \"\");\n+    return;\n+  }\n+\n+  if (!r_1->is_FloatRegister()) {\n+    Register val = r25;\n+    if (r_1->is_stack()) {\n+      \/\/ memory to memory use r25 (scratch registers is used by store_heap_oop)\n+      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+      __ load_sized_value(val, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+    } else {\n+      val = r_1->as_Register();\n+    }\n+    assert_different_registers(to.base(), val, tmp1, tmp2, tmp3);\n+    if (is_oop) {\n+      __ store_heap_oop(to, val, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+    } else {\n+      __ store_sized_value(to, val, size_in_bytes);\n+    }\n+  } else {\n+    if (wide) {\n+      __ strd(r_1->as_FloatRegister(), to);\n+    } else {\n+      \/\/ only a float use just part of the slot\n+      __ strs(r_1->as_FloatRegister(), to);\n+    }\n+  }\n+}\n+\n@@ -397,3 +584,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>* sig_extended,\n@@ -401,1 +586,28 @@\n-                            Label& skip_fixup) {\n+                            bool requires_clinit_barrier,\n+                            address& c2i_no_clinit_check_entry,\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n+  if (requires_clinit_barrier && VM_Version::supports_fast_class_init_checks()) {\n+    Label L_skip_barrier;\n+\n+    { \/\/ Bypass the barrier for non-static methods\n+      __ ldrh(rscratch1, Address(rmethod, Method::access_flags_offset()));\n+      __ andsw(zr, rscratch1, JVM_ACC_STATIC);\n+      __ br(Assembler::EQ, L_skip_barrier); \/\/ non-static\n+    }\n+\n+    __ load_method_holder(rscratch2, rmethod);\n+    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n+    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+\n+    __ bind(L_skip_barrier);\n+    c2i_no_clinit_check_entry = __ pc();\n+  }\n+\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->c2i_entry_barrier(masm);\n+\n@@ -411,1 +623,25 @@\n-  int words_pushed = 0;\n+  \/\/ TODO 8366717 Is the comment about r13 correct? Isn't that r19_sender_sp?\n+  \/\/ Name some registers to be used in the following code. We can use\n+  \/\/ anything except r0-r7 which are arguments in the Java calling\n+  \/\/ convention, rmethod (r12), and r13 which holds the outgoing sender\n+  \/\/ SP for the interpreter.\n+  \/\/ TODO 8366717 We need to make sure that buf_array, buf_oop (and potentially other long-life regs) are kept live in slowpath runtime calls in GC barriers\n+  Register buf_array = r10;   \/\/ Array of buffered inline types\n+  Register buf_oop = r11;     \/\/ Buffered inline type oop\n+  Register tmp1 = r15;\n+  Register tmp2 = r16;\n+  Register tmp3 = r17;\n+\n+  if (InlineTypePassFieldsAsArgs) {\n+    \/\/ Is there an inline type argument?\n+    bool has_inline_argument = false;\n+    for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+      has_inline_argument = (sig_extended->at(i)._bt == T_METADATA);\n+    }\n+    if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types\n+      \/\/ Allocate the buffers here with a runtime call.\n+      \/\/ TODO 8366717 Do we need to save vectors here? They could be used as arg registers, right? Same on x64.\n+      RegisterSaver reg_save(true \/* save_vectors *\/);\n+      OopMap* map = reg_save.save_live_registers(masm, 0, &frame_size_in_words);\n@@ -413,2 +649,2 @@\n-  \/\/ Since all args are passed on the stack, total_args_passed *\n-  \/\/ Interpreter::stackElementSize is the space we need.\n+      frame_complete = __ offset();\n+      address the_pc = __ pc();\n@@ -416,1 +652,2 @@\n-  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+      Label retaddr;\n+      __ set_last_Java_frame(sp, noreg, retaddr, rscratch1);\n@@ -418,1 +655,3 @@\n-  __ mov(r19_sender_sp, sp);\n+      __ mov(c_rarg0, rthread);\n+      __ mov(c_rarg1, rmethod);\n+      __ mov(c_rarg2, (int64_t)alloc_inline_receiver);\n@@ -420,2 +659,3 @@\n-  \/\/ stack is aligned, keep it that way\n-  extraspace = align_up(extraspace, 2*wordSize);\n+      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+      __ blr(rscratch1);\n+      __ bind(retaddr);\n@@ -423,2 +663,2 @@\n-  if (extraspace)\n-    __ sub(sp, sp, extraspace);\n+      oop_maps->add_gc_map(__ pc() - start, map);\n+      __ reset_last_Java_frame(false);\n@@ -426,6 +666,1 @@\n-  \/\/ Now write the args into the outgoing interpreter space\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n+      reg_save.restore_live_registers(masm);\n@@ -433,16 +668,3 @@\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n-\n-    \/\/ Say 4 args:\n-    \/\/ i   st_off\n-    \/\/ 0   32 T_LONG\n-    \/\/ 1   24 T_VOID\n-    \/\/ 2   16 T_OBJECT\n-    \/\/ 3    8 T_BOOL\n-    \/\/ -    0 return address\n-    \/\/\n-    \/\/ However to make thing extra confusing. Because we can fit a Java long\/double in\n-    \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n-    \/\/ leaves one slot empty and only stores to a single slot. In this case the\n-    \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+      Label no_exception;\n+      __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));\n+      __ cbz(rscratch1, no_exception);\n@@ -450,5 +672,9 @@\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n-    if (!r_1->is_valid()) {\n-      assert(!r_2->is_valid(), \"\");\n-      continue;\n+      __ str(zr, Address(rthread, JavaThread::vm_result_oop_offset()));\n+      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));\n+      __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result_oop(buf_array, rthread);\n+      __ get_vm_result_metadata(rmethod, rthread); \/\/ TODO: required to keep the callee Method live?\n@@ -456,9 +682,1 @@\n-    if (r_1->is_stack()) {\n-      \/\/ memory to memory use rscratch1\n-      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size\n-                    + extraspace\n-                    + words_pushed * wordSize);\n-      if (!r_2->is_valid()) {\n-        \/\/ sign extend??\n-        __ ldrw(rscratch1, Address(sp, ld_off));\n-        __ str(rscratch1, Address(sp, st_off));\n+  }\n@@ -466,1 +684,11 @@\n-      } else {\n+  \/\/ Since all args are passed on the stack, total_args_passed *\n+  \/\/ Interpreter::stackElementSize is the space we need.\n+\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n+  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+\n+  \/\/ stack is aligned, keep it that way\n+  extraspace = align_up(extraspace, StackAlignmentInBytes);\n+\n+  \/\/ set senderSP value\n+  __ mov(r19_sender_sp, sp);\n@@ -468,1 +696,1 @@\n-        __ ldr(rscratch1, Address(sp, ld_off));\n+  __ sub(sp, sp, extraspace);\n@@ -470,6 +698,26 @@\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ str(rscratch1, Address(sp, next_off));\n+  \/\/ Now write the args into the outgoing interpreter space\n+\n+  \/\/ next_arg_comp is the next argument from the compiler point of\n+  \/\/ view (inline type fields are passed in registers\/on the stack). In\n+  \/\/ sig_extended, an inline type argument starts with: T_METADATA,\n+  \/\/ followed by the types of the fields of the inline type and T_VOID\n+  \/\/ to mark the end of the inline type. ignored counts the number of\n+  \/\/ T_METADATA\/T_VOID. next_vt_arg is the next inline type argument:\n+  \/\/ used to get the buffer for that argument from the pool of buffers\n+  \/\/ we allocated above and want to pass to the\n+  \/\/ interpreter. next_arg_int is the next argument from the\n+  \/\/ interpreter point of view (inline types are passed by reference).\n+  for (int next_arg_comp = 0, ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+       next_arg_comp < sig_extended->length(); next_arg_comp++) {\n+    assert(ignored <= next_arg_comp, \"shouldn't skip over more slots than there are arguments\");\n+    assert(next_arg_int <= total_args_passed, \"more arguments for the interpreter than expected?\");\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    int st_off = (total_args_passed - next_arg_int - 1) * Interpreter::stackElementSize;\n+    if (!InlineTypePassFieldsAsArgs || bt != T_METADATA) {\n+      int next_off = st_off - Interpreter::stackElementSize;\n+      const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+      const VMRegPair reg_pair = regs[next_arg_comp-ignored];\n+      size_t size_in_bytes = reg_pair.second()->is_valid() ? 8 : 4;\n+      gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                             size_in_bytes, reg_pair, Address(sp, offset), tmp1, tmp2, tmp3, extraspace, false);\n+      next_arg_int++;\n@@ -477,7 +725,4 @@\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaaaull);\n-          __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ str(rscratch1, Address(sp, st_off));\n-        }\n+      if (bt == T_LONG || bt == T_DOUBLE) {\n+        \/\/ Overwrite the unused slot with known junk\n+        __ mov(rscratch1, CONST64(0xdeadffffdeadaaaa));\n+        __ str(rscratch1, Address(sp, st_off));\n@@ -485,16 +730,24 @@\n-    } else if (r_1->is_Register()) {\n-      Register r = r_1->as_Register();\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ str(r, Address(sp, st_off));\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ jlong\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaabull);\n-          __ str(rscratch1, Address(sp, st_off));\n-          __ str(r, Address(sp, next_off));\n+    } else {\n+      ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_OBJECT);\n+      __ load_heap_oop(buf_oop, Address(buf_array, index), tmp1, tmp2);\n+      next_vt_arg++; next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of inline\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      Label L_null;\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp - 1)._bt;\n+        if (bt == T_METADATA) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n@@ -503,1 +756,22 @@\n-          __ str(r, Address(sp, st_off));\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          if (off == -1) {\n+            \/\/ Nullable inline type argument, emit null check\n+            VMReg reg = regs[next_arg_comp-ignored].first();\n+            Label L_notNull;\n+            if (reg->is_stack()) {\n+              int ld_off = reg->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+              __ ldrb(tmp1, Address(sp, ld_off));\n+              __ cbnz(tmp1, L_notNull);\n+            } else {\n+              __ cbnz(reg->as_Register(), L_notNull);\n+            }\n+            __ str(zr, Address(sp, st_off));\n+            __ b(L_null);\n+            __ bind(L_notNull);\n+            continue;\n+          }\n+          assert(off > 0, \"offset in object should be positive\");\n+          size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+          bool is_oop = is_reference_type(bt);\n+          gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                                 size_in_bytes, regs[next_arg_comp-ignored], Address(buf_oop, off), tmp1, tmp2, tmp3, extraspace, is_oop);\n@@ -505,14 +779,4 @@\n-      }\n-    } else {\n-      assert(r_1->is_FloatRegister(), \"\");\n-      if (!r_2->is_valid()) {\n-        \/\/ only a float use just part of the slot\n-        __ strs(r_1->as_FloatRegister(), Address(sp, st_off));\n-      } else {\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaacull);\n-        __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        __ strd(r_1->as_FloatRegister(), Address(sp, next_off));\n-      }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ str(buf_oop, Address(sp, st_off));\n+      __ bind(L_null);\n@@ -528,0 +792,1 @@\n+void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm, int comp_args_on_stack, const GrowableArray<SigEntry>* sig, const VMRegPair *regs) {\n@@ -529,5 +794,0 @@\n-void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,\n-                                    int total_args_passed,\n-                                    int comp_args_on_stack,\n-                                    const BasicType *sig_bt,\n-                                    const VMRegPair *regs) {\n@@ -562,1 +822,1 @@\n-  int comp_words_on_stack = align_up(comp_args_on_stack*VMRegImpl::stack_slot_size, wordSize)>>LogBytesPerWord;\n+  int comp_words_on_stack = 0;\n@@ -564,2 +824,3 @@\n-    __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n-    __ andr(sp, rscratch1, -16);\n+     comp_words_on_stack = align_up(comp_args_on_stack * VMRegImpl::stack_slot_size, wordSize) >> LogBytesPerWord;\n+     __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n+     __ andr(sp, rscratch1, -16);\n@@ -570,1 +831,1 @@\n-  __ ldr(rscratch1, Address(rmethod, in_bytes(Method::from_compiled_offset())));\n+  __ ldr(rscratch1, Address(rmethod, in_bytes(Method::from_compiled_inline_offset())));\n@@ -584,0 +845,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -586,2 +849,3 @@\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+    BasicType bt = sig->at(i)._bt;\n+    if (bt == T_VOID) {\n+      assert(i > 0 && (sig->at(i - 1)._bt == T_LONG || sig->at(i - 1)._bt == T_DOUBLE), \"missing half\");\n@@ -592,0 +856,1 @@\n+    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(), \"scrambled load targets?\");\n@@ -593,3 +858,1 @@\n-    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(),\n-            \"scrambled load targets?\");\n-    int ld_off = (total_args_passed - i - 1)*Interpreter::stackElementSize;\n+    int ld_off = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n@@ -610,1 +873,1 @@\n-      int st_off = regs[i].first()->reg2stack()*VMRegImpl::stack_slot_size;\n+      int st_off = regs[i].first()->reg2stack() * VMRegImpl::stack_slot_size;\n@@ -627,2 +890,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n@@ -631,11 +893,28 @@\n-        __ str(rscratch2, Address(sp, st_off));\n-      }\n-    } else if (r_1->is_Register()) {  \/\/ Register argument\n-      Register r = r_1->as_Register();\n-      if (r_2->is_valid()) {\n-        \/\/\n-        \/\/ We are using two VMRegs. This can be either T_OBJECT,\n-        \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n-        \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n-        \/\/ So we must adjust where to pick up the data to match the\n-        \/\/ interpreter.\n+         __ str(rscratch2, Address(sp, st_off));\n+       }\n+     } else if (r_1->is_Register()) {  \/\/ Register argument\n+       Register r = r_1->as_Register();\n+       if (r_2->is_valid()) {\n+         \/\/\n+         \/\/ We are using two VMRegs. This can be either T_OBJECT,\n+         \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n+         \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n+         \/\/ So we must adjust where to pick up the data to match the\n+         \/\/ interpreter.\n+\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n+\n+         \/\/ this can be a misaligned move\n+         __ ldr(r, Address(esp, offset));\n+       } else {\n+         \/\/ sign extend and use a full word?\n+         __ ldrw(r, Address(esp, ld_off));\n+       }\n+     } else {\n+       if (!r_2->is_valid()) {\n+         __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n+       } else {\n+         __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n+       }\n+     }\n+   }\n@@ -643,17 +922,0 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n-\n-        \/\/ this can be a misaligned move\n-        __ ldr(r, Address(esp, offset));\n-      } else {\n-        \/\/ sign extend and use a full word?\n-        __ ldrw(r, Address(esp, ld_off));\n-      }\n-    } else {\n-      if (!r_2->is_valid()) {\n-        __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n-      } else {\n-        __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n-      }\n-    }\n-  }\n@@ -676,1 +938,0 @@\n-\n@@ -680,8 +941,4 @@\n-\/\/ ---------------------------------------------------------------\n-void SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n-                                            int total_args_passed,\n-                                            int comp_args_on_stack,\n-                                            const BasicType *sig_bt,\n-                                            const VMRegPair *regs,\n-                                            address entry_address[AdapterBlob::ENTRY_COUNT]) {\n-  entry_address[AdapterBlob::I2C] = __ pc();\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n+  Register data = rscratch2;\n+  __ ic_check(1 \/* end_alignment *\/);\n+  __ ldr(rmethod, Address(data, CompiledICData::speculated_method_offset()));\n@@ -689,1 +946,7 @@\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  \/\/ Method might have been compiled since the call site was patched to\n+  \/\/ interpreted; if that is the case treat it as a miss so we can get\n+  \/\/ the call site corrected.\n+  __ ldr(rscratch1, Address(rmethod, in_bytes(Method::code_offset())));\n+  __ cbz(rscratch1, skip_fixup);\n+  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+}\n@@ -691,2 +954,12 @@\n-  entry_address[AdapterBlob::C2I_Unverified] = __ pc();\n-  Label skip_fixup;\n+\/\/ ---------------------------------------------------------------\n+void SharedRuntime::generate_i2c2i_adapters(MacroAssembler* masm,\n+                                            int comp_args_on_stack,\n+                                            const GrowableArray<SigEntry>* sig,\n+                                            const VMRegPair* regs,\n+                                            const GrowableArray<SigEntry>* sig_cc,\n+                                            const VMRegPair* regs_cc,\n+                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                            const VMRegPair* regs_cc_ro,\n+                                            address entry_address[AdapterBlob::ENTRY_COUNT],\n+                                            AdapterBlob*& new_adapter,\n+                                            bool allocate_code_blob) {\n@@ -694,3 +967,2 @@\n-  Register data = rscratch2;\n-  Register receiver = j_rarg0;\n-  Register tmp = r10;  \/\/ A call-clobbered register not used for arg passing\n+  entry_address[AdapterBlob::I2C] = __ pc();\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n@@ -707,7 +979,3 @@\n-  {\n-    __ block_comment(\"c2i_unverified_entry {\");\n-    \/\/ Method might have been compiled since the call site was patched to\n-    \/\/ interpreted; if that is the case treat it as a miss so we can get\n-    \/\/ the call site corrected.\n-    __ ic_check(1 \/* end_alignment *\/);\n-    __ ldr(rmethod, Address(data, CompiledICData::speculated_method_offset()));\n+  entry_address[AdapterBlob::C2I_Unverified] = __ pc();\n+  entry_address[AdapterBlob::C2I_Unverified_Inline] = __ pc();\n+  Label skip_fixup;\n@@ -715,5 +983,1 @@\n-    __ ldr(rscratch1, Address(rmethod, in_bytes(Method::code_offset())));\n-    __ cbz(rscratch1, skip_fixup);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-    __ block_comment(\"} c2i_unverified_entry\");\n-  }\n+  gen_inline_cache_check(masm, skip_fixup);\n@@ -721,1 +985,3 @@\n-  entry_address[AdapterBlob::C2I] = __ pc();\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n@@ -723,1 +989,1 @@\n-  \/\/ Class initialization barrier for static methods\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n@@ -725,15 +991,33 @@\n-  if (VM_Version::supports_fast_class_init_checks()) {\n-    Label L_skip_barrier;\n-\n-    { \/\/ Bypass the barrier for non-static methods\n-      __ ldrh(rscratch1, Address(rmethod, Method::access_flags_offset()));\n-      __ andsw(zr, rscratch1, JVM_ACC_STATIC);\n-      __ br(Assembler::EQ, L_skip_barrier); \/\/ non-static\n-    }\n-\n-    __ load_method_holder(rscratch2, rmethod);\n-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n-\n-    __ bind(L_skip_barrier);\n-    entry_address[AdapterBlob::C2I_No_Clinit_Check] = __ pc();\n+  entry_address[AdapterBlob::C2I_Inline_RO] = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    \/\/ No class init barrier needed because method is guaranteed to be non-static\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, \/* requires_clinit_barrier = *\/ false, entry_address[AdapterBlob::C2I_No_Clinit_Check],\n+                    skip_fixup, entry_address[AdapterBlob::I2C], oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n+    skip_fixup.reset();\n+  }\n+\n+  \/\/ Scalarized c2i adapter\n+  entry_address[AdapterBlob::C2I]        = __ pc();\n+  entry_address[AdapterBlob::C2I_Inline] = __ pc();\n+  gen_c2i_adapter(masm, sig_cc, regs_cc, \/* requires_clinit_barrier = *\/ true, entry_address[AdapterBlob::C2I_No_Clinit_Check],\n+                  skip_fixup, entry_address[AdapterBlob::I2C], oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ true);\n+\n+  \/\/ Non-scalarized c2i adapter\n+  if (regs != regs_cc) {\n+    entry_address[AdapterBlob::C2I_Unverified_Inline] = __ pc();\n+    Label inline_entry_skip_fixup;\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n+\n+    entry_address[AdapterBlob::C2I_Inline] = __ pc();\n+    gen_c2i_adapter(masm, sig, regs, \/* requires_clinit_barrier = *\/ true, entry_address[AdapterBlob::C2I_No_Clinit_Check],\n+                    inline_entry_skip_fixup, entry_address[AdapterBlob::I2C], oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n+  }\n+\n+  \/\/ The c2i adapters might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+  if (allocate_code_blob) {\n+    bool caller_must_gc_arguments = (regs != regs_cc);\n+    int entry_offset[AdapterHandlerEntry::ENTRIES_COUNT];\n+    assert(AdapterHandlerEntry::ENTRIES_COUNT == 7, \"sanity\");\n+    AdapterHandlerLibrary::address_to_offset(entry_address, entry_offset);\n+    new_adapter = AdapterBlob::create(masm->code(), entry_offset, frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n@@ -741,6 +1025,0 @@\n-\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->c2i_entry_barrier(masm);\n-\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n-  return;\n@@ -2757,0 +3035,143 @@\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  if (buf == nullptr) {\n+    return nullptr;\n+  }\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler _masm(&buffer);\n+  MacroAssembler* masm = &_masm;\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_jobject_off = __ offset();\n+  \/\/ Resolve pre-allocated buffer from JNI handle.\n+  \/\/ We cannot do this in generate_call_stub() because it requires GC code to be initialized.\n+  Register Rresult = r14;  \/\/ See StubGenerator::generate_call_stub().\n+  __ ldr(r0, Address(Rresult));\n+  __ resolve_jobject(r0 \/* value *\/,\n+                     rthread \/* thread *\/,\n+                     r12 \/* tmp *\/);\n+  __ str(r0, Address(Rresult));\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ strs(r_1->as_FloatRegister(), to);\n+    } else if (bt == T_DOUBLE) {\n+      __ strd(r_1->as_FloatRegister(), to);\n+    } else {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(to.base(), val, r15, r16, r17);\n+      if (is_reference_type(bt)) {\n+        __ store_heap_oop(to, val, r15, r16, r17, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        __ store_sized_value(to, r_1->as_Register(), type2aelembytes(bt));\n+      }\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+  if (vk->has_nullable_atomic_layout()) {\n+    \/\/ Zero the null marker (setting it to 1 would be better but would require an additional register)\n+    __ strb(zr, Address(r0, vk->null_marker_offset()));\n+  }\n+  __ ret(lr);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  Label skip;\n+  Label not_null;\n+  __ cbnz(r0, not_null);\n+\n+  \/\/ Return value is null. Zero oop registers to make the GC happy.\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    if (bt == T_OBJECT || bt == T_ARRAY) {\n+      VMRegPair pair = regs->at(j);\n+      VMReg r_1 = pair.first();\n+      __ mov(r_1->as_Register(), zr);\n+    }\n+    j++;\n+  }\n+  __ b(skip);\n+  __ bind(not_null);\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ ldrs(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ ldrd(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      assert_different_registers(r0, r_1->as_Register());\n+      __ load_heap_oop(r_1->as_Register(), from, rscratch1, rscratch2);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(r0, r_1->as_Register());\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ bind(skip);\n+\n+  __ ret(lr);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":611,"deletions":190,"binary":false,"changes":801,"status":"modified"},{"patch":"@@ -238,2 +238,18 @@\n-  \/\/ Determine and save the live input values\n-  __ push_call_clobbered_registers();\n+  if (EnableValhalla && InlineTypePassFieldsAsArgs) {\n+    \/\/ Barriers might be emitted when converting between (scalarized) calling conventions for inline\n+    \/\/ types. Save all argument registers before calling into the runtime.\n+    \/\/ TODO 8366717: use push_set() (see JDK-8283327 push\/pop_call_clobbered_registers & aarch64 )\n+    __ pusha();\n+    __ subptr(rsp, 64);\n+    __ movdbl(Address(rsp, 0),  j_farg0);\n+    __ movdbl(Address(rsp, 8),  j_farg1);\n+    __ movdbl(Address(rsp, 16), j_farg2);\n+    __ movdbl(Address(rsp, 24), j_farg3);\n+    __ movdbl(Address(rsp, 32), j_farg4);\n+    __ movdbl(Address(rsp, 40), j_farg5);\n+    __ movdbl(Address(rsp, 48), j_farg6);\n+    __ movdbl(Address(rsp, 56), j_farg7);\n+  } else {\n+    \/\/ Determine and save the live input values\n+    __ push_call_clobbered_registers();\n+  }\n@@ -266,1 +282,15 @@\n-  __ pop_call_clobbered_registers();\n+  if (EnableValhalla && InlineTypePassFieldsAsArgs) {\n+    \/\/ Restore registers\n+    __ movdbl(j_farg0, Address(rsp, 0));\n+    __ movdbl(j_farg1, Address(rsp, 8));\n+    __ movdbl(j_farg2, Address(rsp, 16));\n+    __ movdbl(j_farg3, Address(rsp, 24));\n+    __ movdbl(j_farg4, Address(rsp, 32));\n+    __ movdbl(j_farg5, Address(rsp, 40));\n+    __ movdbl(j_farg6, Address(rsp, 48));\n+    __ movdbl(j_farg7, Address(rsp, 56));\n+    __ addptr(rsp, 64);\n+    __ popa();\n+  } else {\n+    __ pop_call_clobbered_registers();\n+  }\n@@ -388,0 +418,1 @@\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n@@ -389,1 +420,1 @@\n-  bool needs_pre_barrier = as_normal;\n+  bool needs_pre_barrier = as_normal && !dest_uninitialized;\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.cpp","additions":35,"deletions":4,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -102,0 +102,81 @@\n+\/\/ TODO 8350865 (same applies to g1StoreLSpecialTwoOops)\n+\/\/ - Can we use an unbound register for src?\n+\/\/ - Do no set\/overwrite barrier data here, also handle G1C2BarrierPostNotNull\n+\/\/ - Is the zero-extend really required in all the places?\n+instruct g1StoreLSpecialOneOop(memory mem, rdx_RegL src, immI off, rRegP tmp1, rRegP tmp2, rRegP tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC);\n+  match(Set mem (StoreLSpecial mem (Binary src off)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, USE_KILL src, KILL cr);\n+  format %{ \"movq    $mem, $src\\t# g1StoreLSpecialOneOop\" %}\n+  ins_encode %{\n+    ((MachNode*)this)->set_barrier_data(G1C2BarrierPre | G1C2BarrierPost);\n+\n+    __ lea($tmp1$$Register, $mem$$Address);\n+    \/\/ Adjust address to point to narrow oop\n+    __ addq($tmp1$$Register, $off$$constant);\n+    write_barrier_pre(masm, this,\n+                      $tmp1$$Register \/* obj *\/,\n+                      $tmp2$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($tmp1$$Register, $src$$Register) \/* preserve *\/);\n+\n+    __ movq(Address($tmp1$$Register, 0), $src$$Register);\n+\n+    \/\/ Shift long value to extract the narrow oop field value and zero-extend it\n+    __ shrq($src$$Register, $off$$constant << LogBitsPerByte);\n+    __ movl($src$$Register, $src$$Register);\n+\n+    write_barrier_post(masm, this,\n+                       $tmp1$$Register \/* store_addr *\/,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp3$$Register \/* tmp1 *\/);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct g1StoreLSpecialTwoOops(memory mem, rdx_RegL src, rRegP tmp1, rRegP tmp2, rRegP tmp3, rRegP tmp4, rFlagsReg cr)\n+%{\n+  predicate(UseG1GC);\n+  match(Set mem (StoreLSpecial mem src));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, KILL cr);\n+  format %{ \"movq    $mem, $src\\t# g1StoreLSpecialTwoOops\" %}\n+  ins_encode %{\n+    ((MachNode*)this)->set_barrier_data(G1C2BarrierPre | G1C2BarrierPost);\n+\n+    __ lea($tmp1$$Register, $mem$$Address);\n+    write_barrier_pre(masm, this,\n+                      $tmp1$$Register \/* obj *\/,\n+                      $tmp2$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($tmp1$$Register, $src$$Register) \/* preserve *\/);\n+    \/\/ Adjust address to point to the second narrow oop in the long value\n+    __ addq($tmp1$$Register, 4);\n+    write_barrier_pre(masm, this,\n+                      $tmp1$$Register \/* obj *\/,\n+                      $tmp2$$Register \/* pre_val *\/,\n+                      $tmp3$$Register \/* tmp *\/,\n+                      RegSet::of($tmp1$$Register, $src$$Register) \/* preserve *\/);\n+    \/\/ Adjust address again to point to the first narrow oop in the long value\n+    __ subq($tmp1$$Register, 4);\n+\n+    __ movq(Address($tmp1$$Register, 0), $src$$Register);\n+\n+    \/\/ Zero-extend first narrow oop to long\n+    __ movl($tmp4$$Register, $src$$Register);\n+\n+    \/\/ Shift long value to extract the second narrow oop field value\n+    __ shrq($src$$Register, 32);\n+    write_barrier_post(masm, this,\n+                       $tmp1$$Register \/* store_addr *\/,\n+                       $tmp4$$Register \/* new_val *\/,\n+                       $tmp3$$Register \/* tmp1 *\/);\n+    __ addq($tmp1$$Register, 4);\n+    write_barrier_post(masm, this,\n+                       $tmp1$$Register \/* store_addr *\/,\n+                       $src$$Register \/* new_val *\/,\n+                       $tmp3$$Register \/* tmp1 *\/);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1_x86_64.ad","additions":81,"deletions":0,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -46,0 +47,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -56,0 +58,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -59,0 +62,4 @@\n+#include \"vmreg_x86.inline.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/output.hpp\"\n+#endif\n@@ -1306,0 +1313,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2359,0 +2370,107 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  andptr(markword, markWord::inline_type_mask_in_place);\n+  cmpptr(markword, markWord::inline_type_pattern);\n+  jcc(Assembler::equal, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type, bool can_be_null) {\n+  if (can_be_null) {\n+    testptr(object, object);\n+    jcc(Assembler::zero, not_inline_type);\n+  }\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  movptr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  andptr(tmp, is_inline_type_mask);\n+  cmpptr(tmp, is_inline_type_mask);\n+  jcc(Assembler::notEqual, not_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::notEqual, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::equal, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_flat(Register flags, Register temp_reg, Label& is_flat) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_flat_shift);\n+  jcc(Assembler::notEqual, is_flat);\n+}\n+\n+void MacroAssembler::test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::has_null_marker_shift);\n+  jcc(Assembler::notEqual, has_null_marker);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  movptr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  testl(temp_reg, markWord::unlocked_value);\n+  jccb(Assembler::notZero, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  push(rscratch1);\n+  load_prototype_header(temp_reg, oop, rscratch1);\n+  pop(rscratch1);\n+\n+  bind(test_mark_word);\n+  testl(temp_reg, test_bit);\n+  jcc((jmp_set) ? Assembler::notZero : Assembler::zero, jmp_label);\n+}\n+\n+void MacroAssembler::test_flat_array_oop(Register oop, Register temp_reg,\n+                                         Label& is_flat_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flat_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_flat_array_layout(temp_reg, is_flat_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_flat_array_oop(Register oop, Register temp_reg,\n+                                             Label& is_non_flat_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flat_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_flat_array_layout(temp_reg, is_non_flat_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+\n+void MacroAssembler::test_flat_array_layout(Register lh, Label& is_flat_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::notZero, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_layout(Register lh, Label& is_non_flat_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::zero, is_non_flat_array);\n+}\n+\n@@ -3449,0 +3567,118 @@\n+\/\/ Object \/ value buffer allocation...\n+\/\/\n+\/\/ Kills klass and rsi on LP64\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == rax, \"needs to be rax\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  movl(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it is malformed in some way\n+  testl(layout_size, Klass::_lh_instance_slow_path_bit);\n+  jcc(Assembler::notZero, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path.\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  push(klass);\n+  if (UseTLAB) {\n+    tlab_allocate(new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      jmp(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      jmp(initialize_object);\n+    }\n+  } else {\n+    jmp(slow_case);\n+  }\n+\n+  \/\/ If UseTLAB is true, the object is created above and there is an initialize need.\n+  \/\/ Otherwise, skip and go to the slow path.\n+  if (UseTLAB) {\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      if (UseCompactObjectHeaders) {\n+        assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n+        decrement(layout_size, oopDesc::base_offset_in_bytes());\n+      } else {\n+        decrement(layout_size, sizeof(oopDesc));\n+      }\n+      jcc(Assembler::zero, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+      Register zero = klass;\n+      xorl(zero, zero);    \/\/ use zero reg to clear memory (shorter code)\n+      shrl(layout_size, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      \/\/ Ignore partial flag stall after shrl() since it is debug VM\n+      jcc(Assembler::carryClear, L);\n+      stop(\"object size is not multiple of 2 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        bind(loop);\n+        int header_size_bytes = oopDesc::header_size() * HeapWordSize;\n+        assert(is_aligned(header_size_bytes, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n+        movptr(Address(new_obj, layout_size, Address::times_8, header_size_bytes - 1*oopSize), zero);\n+        decrement(layout_size);\n+        jcc(Assembler::notZero, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    if (UseCompactObjectHeaders || EnableValhalla) {\n+      pop(klass);\n+      Register mark_word = t2;\n+      movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+      movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+    } else {\n+     movptr(Address(new_obj, oopDesc::mark_offset_in_bytes()),\n+            (intptr_t)markWord::prototype().value()); \/\/ header\n+     pop(klass);   \/\/ get saved klass back in the register.\n+    }\n+    if (!UseCompactObjectHeaders) {\n+      xorl(rsi, rsi);                 \/\/ use zero reg to clear memory (shorter code)\n+      store_klass_gap(new_obj, rsi);  \/\/ zero klass gap for compressed oops\n+      movptr(t2, klass);         \/\/ preserve klass\n+      store_klass(new_obj, t2, rscratch1);  \/\/ src klass reg is potentially compressed\n+    }\n+    jmp(done);\n+  }\n+\n+  bind(slow_case);\n+  pop(klass);\n+  bind(slow_case_no_pop);\n+  jmp(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -3652,0 +3888,27 @@\n+void MacroAssembler::get_inline_type_field_klass(Register holder_klass, Register index, Register inline_klass) {\n+  inline_layout_info(holder_klass, index, inline_klass);\n+  movptr(inline_klass, Address(inline_klass, InlineLayoutInfo::klass_offset()));\n+}\n+\n+void MacroAssembler::inline_layout_info(Register holder_klass, Register index, Register layout_info) {\n+  movptr(layout_info, Address(holder_klass, InstanceKlass::inline_layout_info_array_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cmpptr(layout_info, 0);\n+    jcc(Assembler::notEqual, done);\n+    stop(\"inline_layout_info_array is null\");\n+    bind(done);\n+  }\n+#endif\n+\n+  InlineLayoutInfo array[2];\n+  int size = (char*)&array[1] - (char*)&array[0]; \/\/ computing size of array elements\n+  if (is_power_of_2(size)) {\n+    shll(index, log2i_exact(size)); \/\/ Scale index by power of 2\n+  } else {\n+    imull(index, index, size); \/\/ Scale the index to be the entry index * array_element_size\n+  }\n+  lea(layout_info, Address(layout_info, index, Address::times_1, Array<InlineLayoutInfo>::base_offset_in_bytes()));\n+}\n+\n@@ -3920,0 +4183,1 @@\n+  \/\/ TODO 8370341 For a direct pointer comparison, we need the refined array klass pointer\n@@ -4702,1 +4966,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -4761,1 +5029,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -5155,0 +5427,10 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(dst, src);\n+  } else if (UseCompressedClassPointers) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -5176,0 +5458,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src, Register tmp) {\n+  load_klass(dst, src, tmp);\n+  movptr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -5241,0 +5528,40 @@\n+void MacroAssembler::flat_field_copy(DecoratorSet decorators, Register src, Register dst,\n+                                     Register inline_layout_info) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->flat_field_copy(this, decorators, src, dst, inline_layout_info);\n+}\n+\n+void MacroAssembler::payload_offset(Register inline_klass, Register offset) {\n+  movptr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  movl(offset, Address(offset, InlineKlass::payload_offset_offset()));\n+}\n+\n+void MacroAssembler::payload_addr(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->payload_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  payload_offset(inline_klass, offset);\n+  if (data == oop) {\n+    addptr(data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert(index != rcx, \"index needs to shift by rcx\");\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rcx, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  movl(rcx, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  shrl(rcx, Klass::_lh_log2_element_size_shift);\n+  andl(rcx, Klass::_lh_log2_element_size_mask);\n+  shlptr(index); \/\/ index << rcx\n+\n+  lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT)));\n+}\n+\n@@ -5600,1 +5927,1 @@\n-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, KRegister mask) {\n+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, KRegister mask) {\n@@ -5606,1 +5933,1 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_512bit);\n+    evpbroadcastq(xtmp, val, AVX_512bit);\n@@ -5608,1 +5935,3 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_256bit);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n+    vinserti128_high(xtmp, xtmp);\n@@ -5610,1 +5939,2 @@\n-    pxor(xtmp, xtmp);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n@@ -5633,1 +5963,1 @@\n-    fill64_masked(3, base, 0, xtmp, mask, cnt, rtmp, true);\n+    fill64_masked(3, base, 0, xtmp, mask, cnt, val, true);\n@@ -5652,1 +5982,1 @@\n-    fill32_masked(3, base, 0, xtmp, mask, cnt, rtmp);\n+    fill32_masked(3, base, 0, xtmp, mask, cnt, val);\n@@ -5665,0 +5995,410 @@\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  testptr(rax, 1);\n+  jcc(Assembler::zero, skip);\n+  int call_offset = -1;\n+\n+#ifdef _LP64\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != nullptr, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(rscratch1, rax); \/\/ save rax for slow_case since *_allocate may corrupt it when allocation failed\n+  if (vk != nullptr) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(rbx, (intptr_t)vk->get_InlineKlass());\n+    jint lh = vk->layout_helper();\n+    assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB && !Klass::layout_helper_needs_slow_path(lh)) {\n+      tlab_allocate(rax, noreg, lh, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. RAX contains ((the InlineKlass* of the return type) | 0x01)\n+    mov(rbx, rax);\n+    andptr(rbx, -2);\n+    if (UseTLAB) {\n+      movl(r14, Address(rbx, Klass::layout_helper_offset()));\n+      testl(r14, Klass::_lh_instance_slow_path_bit);\n+      jcc(Assembler::notZero, slow_case);\n+      tlab_allocate(rax, r14, 0, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = rax;\n+    Register klass = rbx;\n+    if (UseCompactObjectHeaders) {\n+      Register mark_word = r13;\n+      movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+      movptr(Address(buffer_obj, oopDesc::mark_offset_in_bytes()), mark_word);\n+    } else {\n+      movptr(Address(buffer_obj, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::inline_type_prototype().value());\n+      xorl(r13, r13);\n+      store_klass_gap(buffer_obj, r13);\n+      if (vk == nullptr) {\n+        \/\/ store_klass corrupts rbx(klass), so save it in r13 for later use (interpreter case only).\n+        mov(r13, klass);\n+      }\n+      store_klass(buffer_obj, klass, rscratch1);\n+      klass = r13;\n+    }\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != nullptr) {\n+      call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      movptr(rbx, Address(klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      movptr(rbx, Address(rbx, InlineKlass::pack_handler_offset()));\n+      call(rbx);\n+    }\n+    jmp(skip);\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(rax, rscratch1);\n+#endif\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to->as_Register(), from->as_Register());\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        Address to_addr = Address(rsp, st_off);\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to_addr, from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to_addr, from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to_addr, from->as_Register());\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(rsp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      if (to->is_reg()) {\n+        if (to->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from_addr);\n+          }\n+        } else {\n+          movq(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(r13, from_addr);\n+        movq(Address(rsp, st_off), r13);\n+      }\n+    }\n+  }\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  \/\/ Two additional slots to account for return address\n+  int sp_inc = (args_on_stack + 2) * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  \/\/ Save the return address, adjust the stack (make sure it is properly\n+  \/\/ 16-byte aligned) and copy the return address to the new top of the stack.\n+  \/\/ The stack will be repaired on return (see MacroAssembler::remove_frame).\n+  assert(sp_inc > 0, \"sanity\");\n+  pop(r13);\n+  subptr(rsp, sp_inc);\n+  push(r13);\n+  return sp_inc;\n+}\n+\n+\/\/ Read all fields from an inline type buffer and store the field values in registers\/stack slots.\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(tmp1, Address(rsp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        testptr(fromReg, fromReg);\n+        jcc(Assembler::zero, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), 1);\n+      } else {\n+        movq(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_XMMRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), dst);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(toReg->as_XMMRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(toReg->as_XMMRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      jmp(L_notNull);\n+      bind(L_null);\n+      \/\/ Set null marker to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+            movq(Address(rsp, st_off), 0);\n+          } else {\n+            xorq(toReg->as_Register(), toReg->as_Register());\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_METADATA, \"should be at delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r14;\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register tmp3 = rbx;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        testb(Address(rsp, ld_off), 1);\n+      } else {\n+        testb(fromReg->as_Register(), 1);\n+      }\n+      jcc(Assembler::notZero, L_notNull);\n+      movptr(val_obj, 0);\n+      jmp(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+    if (!fromReg->is_XMMRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(src, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(dst, fromReg->as_XMMRegister());\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(dst, fromReg->as_XMMRegister());\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();\n+}\n+\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  if (needs_stack_repair) {\n+    \/\/ TODO 8284443 Add a comment drawing the frame like in Aarch64's version of MacroAssembler::remove_frame\n+    movq(rbp, Address(rsp, initial_framesize));\n+    \/\/ The stack increment resides just below the saved rbp\n+    addq(rsp, Address(rsp, initial_framesize - wordSize));\n+  } else {\n+    if (initial_framesize > 0) {\n+      addq(rsp, initial_framesize);\n+    }\n+    pop(rbp);\n+  }\n+}\n+\n@@ -5754,2 +6494,2 @@\n-void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp,\n-                               bool is_large, KRegister mask) {\n+void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp,\n+                               bool is_large, bool word_copy_only, KRegister mask) {\n@@ -5760,1 +6500,1 @@\n-  assert(tmp==rax,   \"tmp register must be eax for rep stos\");\n+  assert(val==rax,   \"val register must be eax for rep stos\");\n@@ -5766,3 +6506,0 @@\n-  if (!is_large || !UseXMMForObjInit) {\n-    xorptr(tmp, tmp);\n-  }\n@@ -5780,1 +6517,1 @@\n-    movptr(Address(base, cnt, Address::times_ptr), tmp);\n+    movptr(Address(base, cnt, Address::times_ptr), val);\n@@ -5789,1 +6526,1 @@\n-  if (UseFastStosb) {\n+  if (UseFastStosb && !word_copy_only) {\n@@ -5793,1 +6530,1 @@\n-    xmm_clear_mem(base, cnt, tmp, xtmp, mask);\n+    xmm_clear_mem(base, cnt, val, xtmp, mask);\n@@ -9680,0 +10417,4 @@\n+  if (EnableValhalla) {\n+    \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+    andptr(reg_rax, ~((int) markWord::inline_type_bit_in_place));\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":758,"deletions":17,"binary":false,"changes":775,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -637,0 +638,81 @@\n+\/\/ Same as java_calling_convention() but for multiple return\n+\/\/ values. There's no way to store them on the stack so if we don't\n+\/\/ have enough registers, multiple values can't be returned.\n+const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j+1;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt,\n+                                          VMRegPair *regs,\n+                                          int total_args_passed) {\n+  \/\/ Create the mapping between argument positions and\n+  \/\/ registers.\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+    rax, j_rarg5, j_rarg4, j_rarg3, j_rarg2, j_rarg1, j_rarg0\n+  };\n+  static const XMMRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3,\n+    j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < Argument::n_int_register_parameters_j+1) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert(sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+    case T_METADATA:\n+      if (int_args < Argument::n_int_register_parameters_j+1) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert(sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -679,0 +761,106 @@\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+    for (int i = 0; i < sig_extended->length(); i++) {\n+      BasicType bt = sig_extended->at(i)._bt;\n+      if (bt == T_METADATA) {\n+        \/\/ In sig_extended, an inline type argument starts with:\n+        \/\/ T_METADATA, followed by the types of the fields of the\n+        \/\/ inline type and T_VOID to mark the end of the value\n+        \/\/ type. Inline types are flattened so, for instance, in the\n+        \/\/ case of an inline type with an int field and an inline type\n+        \/\/ field that itself has 2 fields, an int and a long:\n+        \/\/ T_METADATA T_INT T_METADATA T_INT T_LONG T_VOID (second\n+        \/\/ slot for the T_LONG) T_VOID (inner inline type) T_VOID\n+        \/\/ (outer inline type)\n+        total_args_passed++;\n+        int vt = 1;\n+        do {\n+          i++;\n+          BasicType bt = sig_extended->at(i)._bt;\n+          BasicType prev_bt = sig_extended->at(i-1)._bt;\n+          if (bt == T_METADATA) {\n+            vt++;\n+          } else if (bt == T_VOID &&\n+                     prev_bt != T_LONG &&\n+                     prev_bt != T_DOUBLE) {\n+            vt--;\n+          }\n+        } while (vt != 0);\n+      } else {\n+        total_args_passed++;\n+      }\n+    }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n+  return total_args_passed;\n+}\n+\n+\n+static void gen_c2i_adapter_helper(MacroAssembler* masm,\n+                                   BasicType bt,\n+                                   BasicType prev_bt,\n+                                   size_t size_in_bytes,\n+                                   const VMRegPair& reg_pair,\n+                                   const Address& to,\n+                                   int extraspace,\n+                                   bool is_oop) {\n+  if (bt == T_VOID) {\n+    assert(prev_bt == T_LONG || prev_bt == T_DOUBLE, \"missing half\");\n+    return;\n+  }\n+\n+  \/\/ Say 4 args:\n+  \/\/ i   st_off\n+  \/\/ 0   32 T_LONG\n+  \/\/ 1   24 T_VOID\n+  \/\/ 2   16 T_OBJECT\n+  \/\/ 3    8 T_BOOL\n+  \/\/ -    0 return address\n+  \/\/\n+  \/\/ However to make thing extra confusing. Because we can fit a long\/double in\n+  \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n+  \/\/ leaves one slot empty and only stores to a single slot. In this case the\n+  \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+\n+  bool wide = (size_in_bytes == wordSize);\n+  VMReg r_1 = reg_pair.first();\n+  VMReg r_2 = reg_pair.second();\n+  assert(r_2->is_valid() == wide, \"invalid size\");\n+  if (!r_1->is_valid()) {\n+    assert(!r_2->is_valid(), \"must be invalid\");\n+    return;\n+  }\n+\n+  if (!r_1->is_XMMRegister()) {\n+    Register val = rax;\n+    if (r_1->is_stack()) {\n+      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+      __ load_sized_value(val, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+    } else {\n+      val = r_1->as_Register();\n+    }\n+    assert_different_registers(to.base(), val, rscratch1);\n+    if (is_oop) {\n+      __ push(r13);\n+      __ push(rbx);\n+      __ store_heap_oop(to, val, rscratch1, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      __ pop(rbx);\n+      __ pop(r13);\n+    } else {\n+      __ store_sized_value(to, val, size_in_bytes);\n+    }\n+  } else {\n+    if (wide) {\n+      __ movdbl(to, r_1->as_XMMRegister());\n+    } else {\n+      __ movflt(to, r_1->as_XMMRegister());\n+    }\n+  }\n+}\n+\n@@ -680,3 +868,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>* sig_extended,\n@@ -684,1 +870,32 @@\n-                            Label& skip_fixup) {\n+                            bool requires_clinit_barrier,\n+                            address& c2i_no_clinit_check_entry,\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n+  if (requires_clinit_barrier && VM_Version::supports_fast_class_init_checks()) {\n+    Label L_skip_barrier;\n+    Register method = rbx;\n+\n+    { \/\/ Bypass the barrier for non-static methods\n+      Register flags = rscratch1;\n+      __ load_unsigned_short(flags, Address(method, Method::access_flags_offset()));\n+      __ testl(flags, JVM_ACC_STATIC);\n+      __ jcc(Assembler::zero, L_skip_barrier); \/\/ non-static\n+    }\n+\n+    Register klass = rscratch1;\n+    __ load_method_holder(klass, method);\n+    __ clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+\n+    __ bind(L_skip_barrier);\n+    c2i_no_clinit_check_entry = __ pc();\n+  }\n+\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->c2i_entry_barrier(masm);\n+\n@@ -694,0 +911,42 @@\n+  if (InlineTypePassFieldsAsArgs) {\n+    \/\/ Is there an inline type argument?\n+    bool has_inline_argument = false;\n+    for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+      has_inline_argument = (sig_extended->at(i)._bt == T_METADATA);\n+    }\n+    if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types.\n+      \/\/ Allocate the buffers here with a runtime call.\n+      OopMap* map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_vectors*\/ false);\n+\n+      frame_complete = __ offset();\n+\n+      __ set_last_Java_frame(noreg, noreg, nullptr, rscratch1);\n+\n+      __ mov(c_rarg0, r15_thread);\n+      __ mov(c_rarg1, rbx);\n+      __ mov64(c_rarg2, (int64_t)alloc_inline_receiver);\n+      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+\n+      oop_maps->add_gc_map((int)(__ pc() - start), map);\n+      __ reset_last_Java_frame(false);\n+\n+      RegisterSaver::restore_live_registers(masm);\n+\n+      Label no_exception;\n+      __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n+      __ jcc(Assembler::equal, no_exception);\n+\n+      __ movptr(Address(r15_thread, JavaThread::vm_result_oop_offset()), NULL_WORD);\n+      __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+      __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result_oop(rscratch2); \/\/ Use rscratch2 (r11) as temporary because rscratch1 (r10) is trashed by movptr()\n+      __ get_vm_result_metadata(rbx); \/\/ TODO: required to keep the callee Method live?\n+    }\n+  }\n+\n@@ -696,1 +955,1 @@\n-\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n@@ -731,46 +990,24 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n-\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n-\n-    \/\/ Say 4 args:\n-    \/\/ i   st_off\n-    \/\/ 0   32 T_LONG\n-    \/\/ 1   24 T_VOID\n-    \/\/ 2   16 T_OBJECT\n-    \/\/ 3    8 T_BOOL\n-    \/\/ -    0 return address\n-    \/\/\n-    \/\/ However to make thing extra confusing. Because we can fit a long\/double in\n-    \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n-    \/\/ leaves one slot empty and only stores to a single slot. In this case the\n-    \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n-\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n-    if (!r_1->is_valid()) {\n-      assert(!r_2->is_valid(), \"\");\n-      continue;\n-    }\n-    if (r_1->is_stack()) {\n-      \/\/ memory to memory use rax\n-      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n-      if (!r_2->is_valid()) {\n-        \/\/ sign extend??\n-        __ movl(rax, Address(rsp, ld_off));\n-        __ movptr(Address(rsp, st_off), rax);\n-      } else {\n-\n-        __ movq(rax, Address(rsp, ld_off));\n-\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ movq(Address(rsp, next_off), rax);\n+  \/\/ next_arg_comp is the next argument from the compiler point of\n+  \/\/ view (inline type fields are passed in registers\/on the stack). In\n+  \/\/ sig_extended, an inline type argument starts with: T_METADATA,\n+  \/\/ followed by the types of the fields of the inline type and T_VOID\n+  \/\/ to mark the end of the inline type. ignored counts the number of\n+  \/\/ T_METADATA\/T_VOID. next_vt_arg is the next inline type argument:\n+  \/\/ used to get the buffer for that argument from the pool of buffers\n+  \/\/ we allocated above and want to pass to the\n+  \/\/ interpreter. next_arg_int is the next argument from the\n+  \/\/ interpreter point of view (inline types are passed by reference).\n+  for (int next_arg_comp = 0, ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+       next_arg_comp < sig_extended->length(); next_arg_comp++) {\n+    assert(ignored <= next_arg_comp, \"shouldn't skip over more slots than there are arguments\");\n+    assert(next_arg_int <= total_args_passed, \"more arguments for the interpreter than expected?\");\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    int st_off = (total_args_passed - next_arg_int) * Interpreter::stackElementSize;\n+    if (!InlineTypePassFieldsAsArgs || bt != T_METADATA) {\n+      int next_off = st_off - Interpreter::stackElementSize;\n+      const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+      const VMRegPair reg_pair = regs[next_arg_comp-ignored];\n+      size_t size_in_bytes = reg_pair.second()->is_valid() ? 8 : 4;\n+      gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                             size_in_bytes, reg_pair, Address(rsp, offset), extraspace, false);\n+      next_arg_int++;\n@@ -779,7 +1016,4 @@\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov64(rax, CONST64(0xdeadffffdeadaaaa));\n-          __ movptr(Address(rsp, st_off), rax);\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ movq(Address(rsp, st_off), rax);\n-        }\n+      if (bt == T_LONG || bt == T_DOUBLE) {\n+        \/\/ Overwrite the unused slot with known junk\n+        __ mov64(rax, CONST64(0xdeadffffdeadaaaa));\n+        __ movptr(Address(rsp, st_off), rax);\n@@ -787,16 +1021,26 @@\n-    } else if (r_1->is_Register()) {\n-      Register r = r_1->as_Register();\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ movl(Address(rsp, st_off), r);\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ long\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov64(rax, CONST64(0xdeadffffdeadaaab));\n-          __ movptr(Address(rsp, st_off), rax);\n-          __ movq(Address(rsp, next_off), r);\n+    } else {\n+      ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_OBJECT);\n+      __ load_heap_oop(r14, Address(rscratch2, index));\n+      next_vt_arg++; next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of inline\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      Label L_null;\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp-1)._bt;\n+        if (bt == T_METADATA) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID &&\n+                   prev_bt != T_LONG &&\n+                   prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n@@ -805,1 +1049,22 @@\n-          __ movptr(Address(rsp, st_off), r);\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          if (off == -1) {\n+            \/\/ Nullable inline type argument, emit null check\n+            VMReg reg = regs[next_arg_comp-ignored].first();\n+            Label L_notNull;\n+            if (reg->is_stack()) {\n+              int ld_off = reg->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+              __ testb(Address(rsp, ld_off), 1);\n+            } else {\n+              __ testb(reg->as_Register(), 1);\n+            }\n+            __ jcc(Assembler::notZero, L_notNull);\n+            __ movptr(Address(rsp, st_off), 0);\n+            __ jmp(L_null);\n+            __ bind(L_notNull);\n+            continue;\n+          }\n+          assert(off > 0, \"offset in object should be positive\");\n+          size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+          bool is_oop = is_reference_type(bt);\n+          gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                                 size_in_bytes, regs[next_arg_comp-ignored], Address(r14, off), extraspace, is_oop);\n@@ -807,14 +1072,4 @@\n-      }\n-    } else {\n-      assert(r_1->is_XMMRegister(), \"\");\n-      if (!r_2->is_valid()) {\n-        \/\/ only a float use just part of the slot\n-        __ movflt(Address(rsp, st_off), r_1->as_XMMRegister());\n-      } else {\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov64(rax, CONST64(0xdeadffffdeadaaac));\n-        __ movptr(Address(rsp, st_off), rax);\n-#endif \/* ASSERT *\/\n-        __ movdbl(Address(rsp, next_off), r_1->as_XMMRegister());\n-      }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ movptr(Address(rsp, st_off), r14);\n+      __ bind(L_null);\n@@ -830,2 +1085,1 @@\n-                                    int total_args_passed,\n-                                    const BasicType *sig_bt,\n+                                    const GrowableArray<SigEntry>* sig,\n@@ -888,1 +1142,1 @@\n-  __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_offset())));\n+  __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_inline_offset())));\n@@ -902,0 +1156,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -905,1 +1161,2 @@\n-    if (sig_bt[i] == T_VOID) {\n+    BasicType bt = sig->at(i)._bt;\n+    if (bt == T_VOID) {\n@@ -908,1 +1165,2 @@\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+      BasicType prev_bt = (i > 0) ? sig->at(i-1)._bt : T_ILLEGAL;\n+      assert(i > 0 && (prev_bt == T_LONG || prev_bt == T_DOUBLE), \"missing half\");\n@@ -950,1 +1208,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (bt==T_LONG||bt==T_DOUBLE)?\n@@ -965,1 +1223,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (bt==T_LONG||bt==T_DOUBLE)?\n@@ -998,1 +1256,1 @@\n-  \/\/ only needed because eof c2 resolve stubs return Method* as a result in\n+  \/\/ only needed because of c2 resolve stubs return Method* as a result in\n@@ -1004,0 +1262,13 @@\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n+  Register data = rax;\n+  __ ic_check(1 \/* end_alignment *\/);\n+  __ movptr(rbx, Address(data, CompiledICData::speculated_method_offset()));\n+\n+  \/\/ Method might have been compiled since the call site was patched to\n+  \/\/ interpreted if that is the case treat it as a miss so we can get\n+  \/\/ the call site corrected.\n+  __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), NULL_WORD);\n+  __ jcc(Assembler::equal, skip_fixup);\n+  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+}\n+\n@@ -1005,2 +1276,1 @@\n-void SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n-                                            int total_args_passed,\n+void SharedRuntime::generate_i2c2i_adapters(MacroAssembler* masm,\n@@ -1008,3 +1278,9 @@\n-                                            const BasicType *sig_bt,\n-                                            const VMRegPair *regs,\n-                                            address entry_address[AdapterBlob::ENTRY_COUNT]) {\n+                                            const GrowableArray<SigEntry>* sig,\n+                                            const VMRegPair* regs,\n+                                            const GrowableArray<SigEntry>* sig_cc,\n+                                            const VMRegPair* regs_cc,\n+                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                            const VMRegPair* regs_cc_ro,\n+                                            address entry_address[AdapterBlob::ENTRY_COUNT],\n+                                            AdapterBlob*& new_adapter,\n+                                            bool allocate_code_blob) {\n@@ -1012,2 +1288,1 @@\n-\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n@@ -1025,0 +1300,1 @@\n+  entry_address[AdapterBlob::C2I_Unverified_Inline] = __ pc();\n@@ -1027,3 +1303,1 @@\n-  Register data = rax;\n-  Register receiver = j_rarg0;\n-  Register temp = rbx;\n+  gen_inline_cache_check(masm, skip_fixup);\n@@ -1031,12 +1305,3 @@\n-  {\n-    __ ic_check(1 \/* end_alignment *\/);\n-    __ movptr(rbx, Address(data, CompiledICData::speculated_method_offset()));\n-    \/\/ Method might have been compiled since the call site was patched to\n-    \/\/ interpreted if that is the case treat it as a miss so we can get\n-    \/\/ the call site corrected.\n-    __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), NULL_WORD);\n-    __ jcc(Assembler::equal, skip_fixup);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-  }\n-\n-  entry_address[AdapterBlob::C2I] = __ pc();\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n@@ -1044,1 +1309,1 @@\n-  \/\/ Class initialization barrier for static methods\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n@@ -1046,19 +1311,33 @@\n-  if (VM_Version::supports_fast_class_init_checks()) {\n-    Label L_skip_barrier;\n-    Register method = rbx;\n-\n-    { \/\/ Bypass the barrier for non-static methods\n-      Register flags = rscratch1;\n-      __ load_unsigned_short(flags, Address(method, Method::access_flags_offset()));\n-      __ testl(flags, JVM_ACC_STATIC);\n-      __ jcc(Assembler::zero, L_skip_barrier); \/\/ non-static\n-    }\n-\n-    Register klass = rscratch1;\n-    __ load_method_holder(klass, method);\n-    __ clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n-\n-    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n-\n-    __ bind(L_skip_barrier);\n-    entry_address[AdapterBlob::C2I_No_Clinit_Check] = __ pc();\n+  entry_address[AdapterBlob::C2I_Inline_RO] = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    \/\/ No class init barrier needed because method is guaranteed to be non-static\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, \/* requires_clinit_barrier = *\/ false, entry_address[AdapterBlob::C2I_No_Clinit_Check],\n+                    skip_fixup, entry_address[AdapterBlob::I2C], oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n+    skip_fixup.reset();\n+  }\n+\n+  \/\/ Scalarized c2i adapter\n+  entry_address[AdapterBlob::C2I]        = __ pc();\n+  entry_address[AdapterBlob::C2I_Inline] = __ pc();\n+  gen_c2i_adapter(masm, sig_cc, regs_cc, \/* requires_clinit_barrier = *\/ true, entry_address[AdapterBlob::C2I_No_Clinit_Check],\n+                  skip_fixup, entry_address[AdapterBlob::I2C], oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ true);\n+\n+  \/\/ Non-scalarized c2i adapter\n+  if (regs != regs_cc) {\n+    entry_address[AdapterBlob::C2I_Unverified_Inline] = __ pc();\n+    Label inline_entry_skip_fixup;\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n+\n+    entry_address[AdapterBlob::C2I_Inline] = __ pc();\n+    gen_c2i_adapter(masm, sig, regs, \/* requires_clinit_barrier = *\/ true, entry_address[AdapterBlob::C2I_No_Clinit_Check],\n+                    inline_entry_skip_fixup, entry_address[AdapterBlob::I2C], oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n+  }\n+\n+  \/\/ The c2i adapters might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+  if (allocate_code_blob) {\n+    bool caller_must_gc_arguments = (regs != regs_cc);\n+    int entry_offset[AdapterHandlerEntry::ENTRIES_COUNT];\n+    assert(AdapterHandlerEntry::ENTRIES_COUNT == 7, \"sanity\");\n+    AdapterHandlerLibrary::address_to_offset(entry_address, entry_offset);\n+    new_adapter = AdapterBlob::create(masm->code(), entry_offset, frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n@@ -1066,6 +1345,0 @@\n-\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->c2i_entry_barrier(masm);\n-\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n-  return;\n@@ -3559,0 +3832,141 @@\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  if (buf == nullptr) {\n+    return nullptr;\n+  }\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler* masm = new MacroAssembler(&buffer);\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_jobject_off = __ offset();\n+  \/\/ Resolve pre-allocated buffer from JNI handle.\n+  \/\/ We cannot do this in generate_call_stub() because it requires GC code to be initialized.\n+  __ movptr(rax, Address(r13, 0));\n+  __ resolve_jobject(rax \/* value *\/,\n+                     r12 \/* tmp *\/);\n+  __ movptr(Address(r13, 0), rax);\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(rax, off);\n+    if (bt == T_FLOAT) {\n+      __ movflt(to, r_1->as_XMMRegister());\n+    } else if (bt == T_DOUBLE) {\n+      __ movdbl(to, r_1->as_XMMRegister());\n+    } else {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(to.base(), val, r14, r13, rbx, rscratch1);\n+      if (is_reference_type(bt)) {\n+        __ store_heap_oop(to, val, r14, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        __ store_sized_value(to, r_1->as_Register(), type2aelembytes(bt));\n+      }\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+  if (vk->has_nullable_atomic_layout()) {\n+    \/\/ Set the null marker\n+    __ movb(Address(rax, vk->null_marker_offset()), 1);\n+  }\n+  __ ret(0);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  Label skip;\n+  Label not_null;\n+  __ testptr(rax, rax);\n+  __ jcc(Assembler::notZero, not_null);\n+\n+  \/\/ Return value is null. Zero oop registers to make the GC happy.\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    if (bt == T_OBJECT || bt == T_ARRAY) {\n+      VMRegPair pair = regs->at(j);\n+      VMReg r_1 = pair.first();\n+      __ xorq(r_1->as_Register(), r_1->as_Register());\n+    }\n+    j++;\n+  }\n+  __ jmp(skip);\n+  __ bind(not_null);\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(rax, off);\n+    if (bt == T_FLOAT) {\n+      __ movflt(r_1->as_XMMRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ movdbl(r_1->as_XMMRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      assert_different_registers(rax, r_1->as_Register());\n+      __ load_heap_oop(r_1->as_Register(), from);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(rax, r_1->as_Register());\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ bind(skip);\n+  __ ret(0);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+}\n+\n@@ -3651,1 +4065,0 @@\n-\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":559,"deletions":146,"binary":false,"changes":705,"status":"modified"},{"patch":"@@ -1795,1 +1795,1 @@\n-  if (!UseFastStosb && UseUnalignedLoadStores) {\n+  if (UseUnalignedLoadStores) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -47,0 +47,12 @@\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt,\n+                                           VMRegPair *regs,\n+                                           int total_args_passed) {\n+  Unimplemented();\n+  return 0;\n+}\n+\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  Unimplemented();\n+  return nullptr;\n+}\n+\n@@ -48,4 +60,9 @@\n-                                            int total_args_passed,\n-                                            const BasicType *sig_bt,\n-                                            const VMRegPair *regs,\n-                                            address entry_address[AdapterBlob::ENTRY_COUNT]) {\n+                                            const GrowableArray <SigEntry>* sig,\n+                                            const VMRegPair* regs,\n+                                            const GrowableArray <SigEntry>* sig_cc,\n+                                            const VMRegPair* regs_cc,\n+                                            const GrowableArray <SigEntry>* sig_cc_ro,\n+                                            const VMRegPair* regs_cc_ro,\n+                                            address entry_address[AdapterBlob::ENTRY_COUNT],\n+                                            AdapterBlob*& new_adapter,\n+                                            bool allocate_code_blob) {\n","filename":"src\/hotspot\/cpu\/zero\/sharedRuntime_zero.cpp","additions":21,"deletions":4,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -80,0 +80,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -481,1 +483,1 @@\n-  soc->do_tag(objArrayOopDesc::base_offset_in_bytes());\n+  soc->do_tag(refArrayOopDesc::base_offset_in_bytes());\n@@ -1263,0 +1265,5 @@\n+  if (CDSConfig::is_valhalla_preview()) {\n+    log_info(cds)(\"Archived java heap is not yet supported with Valhalla preview\");\n+    return;\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/aotMetaspace.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -232,1 +232,1 @@\n-  memset(mem, 0, objArrayOopDesc::object_size(element_count));\n+  memset(mem, 0, refArrayOopDesc::object_size(element_count));\n@@ -238,0 +238,1 @@\n+    assert(!EnableValhalla || Universe::objectArrayKlass()->prototype_header() == markWord::prototype(), \"should be the same\");\n@@ -268,1 +269,1 @@\n-  assert(objArrayOopDesc::object_size(max_elem_count)*HeapWordSize == MIN_GC_REGION_ALIGNMENT,\n+  assert(refArrayOopDesc::object_size(max_elem_count)*HeapWordSize == MIN_GC_REGION_ALIGNMENT,\n@@ -383,1 +384,1 @@\n-  size_t byte_size = objArrayOopDesc::object_size(length) * HeapWordSize;\n+  size_t byte_size = refArrayOopDesc::object_size(length) * HeapWordSize;\n@@ -640,1 +641,1 @@\n-  if (!src_obj->fast_no_hash_check()) {\n+  if (!src_obj->fast_no_hash_check() && (!(EnableValhalla && src_obj->mark().is_inline_type()))) {\n@@ -644,0 +645,2 @@\n+    } else if (EnableValhalla) {\n+      fake_oop->set_mark(src_klass->prototype_header().copy_set_hash(src_hash));\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -61,0 +62,3 @@\n+bool CDSConfig::_module_patching_disables_cds = false;\n+bool CDSConfig::_java_base_module_patching_disables_cds = false;\n+\n@@ -147,0 +151,3 @@\n+    if (is_valhalla_preview()) {\n+      tmp.print_raw(\"_valhalla\");\n+    }\n@@ -302,1 +309,1 @@\n-  if (Arguments::is_incompatible_cds_internal_module_property(key)) {\n+  if (Arguments::is_incompatible_cds_internal_module_property(key) && !Arguments::patching_migrated_classes(key, value)) {\n@@ -335,2 +342,1 @@\n-    \"jdk.module.upgrade.path\",\n-    \"jdk.module.patch.0\"\n+    \"jdk.module.upgrade.path\"\n@@ -340,2 +346,1 @@\n-    \"--upgrade-module-path\",\n-    \"--patch-module\"\n+    \"--upgrade-module-path\"\n@@ -364,0 +369,6 @@\n+\n+  if (module_patching_disables_cds()) {\n+    vm_exit_during_initialization(\n+            \"Cannot use the following option when dumping the shared archive\", \"--patch-module\");\n+  }\n+\n@@ -392,0 +403,10 @@\n+\n+  if (module_patching_disables_cds()) {\n+    if (RequireSharedSpaces) {\n+      warning(\"CDS is disabled when the %s option is specified.\", \"--patch-module\");\n+    } else {\n+      log_info(cds)(\"CDS is disabled when the %s option is specified.\", \"--patch-module\");\n+    }\n+    return true;\n+  }\n+\n@@ -626,1 +647,1 @@\n-bool CDSConfig::check_vm_args_consistency(bool patch_mod_javabase, bool mode_flag_cmd_line) {\n+bool CDSConfig::check_vm_args_consistency(bool mode_flag_cmd_line) {\n@@ -701,1 +722,1 @@\n-  if (is_using_archive() && patch_mod_javabase) {\n+  if (is_using_archive() && java_base_module_patching_disables_cds() && module_patching_disables_cds()) {\n@@ -962,0 +983,4 @@\n+  if (is_valhalla_preview()) {\n+    \/\/ Not working yet -- e.g., HeapShared::oop_hash() needs to be implemented for value oops\n+    return false;\n+  }\n","filename":"src\/hotspot\/share\/cds\/cdsConfig.cpp","additions":32,"deletions":7,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -49,0 +50,3 @@\n+  static bool _module_patching_disables_cds;\n+  static bool _java_base_module_patching_disables_cds;\n+\n@@ -100,1 +104,6 @@\n-  static bool check_vm_args_consistency(bool patch_mod_javabase, bool mode_flag_cmd_line) NOT_CDS_RETURN_(true);\n+  static bool check_vm_args_consistency(bool mode_flag_cmd_line) NOT_CDS_RETURN_(true);\n+\n+  static bool module_patching_disables_cds() { return CDS_ONLY(_module_patching_disables_cds) NOT_CDS(false); }\n+  static void set_module_patching_disables_cds() { CDS_ONLY(_module_patching_disables_cds = true;) }\n+  static bool java_base_module_patching_disables_cds() { return CDS_ONLY(_java_base_module_patching_disables_cds) NOT_CDS(false); }\n+  static void set_java_base_module_patching_disables_cds() { CDS_ONLY(_java_base_module_patching_disables_cds = true;) }\n@@ -201,0 +210,4 @@\n+  static bool is_valhalla_preview() {\n+    return Arguments::enable_preview() && EnableValhalla;\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/cdsConfig.hpp","additions":14,"deletions":1,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -95,0 +95,3 @@\n+  \/\/ There should be no oops for ObjArrayKlass but InstanceKlass::array_klasses holds a list of ObjArrayKlass,\n+  \/\/ therefore we need the super of the refined array klass.\n+  Klass* oop_field_klass = oop_field->is_refined_objArray() ? oop_field->klass()->super() : oop_field->klass();\n@@ -98,1 +101,1 @@\n-  } else if (oop_field->klass() != ik && oop_field->klass() != ik->array_klass_or_null()) {\n+  } else if (oop_field_klass != ik && oop_field_klass != ik->array_klass_or_null()) {\n","filename":"src\/hotspot\/share\/cds\/cdsEnumKlass.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -374,0 +374,3 @@\n+      if (oak->is_refined_objArray_klass()) {\n+        oak = ObjArrayKlass::cast(oak->super());\n+      }\n@@ -443,8 +446,10 @@\n-\n-      if (elm->is_instance_klass()) {\n-        assert(InstanceKlass::cast(elm)->array_klasses() == nullptr, \"must be\");\n-        InstanceKlass::cast(elm)->set_array_klasses(oak);\n-      } else {\n-        assert(elm->is_array_klass(), \"sanity\");\n-        assert(ArrayKlass::cast(elm)->higher_dimension() == nullptr, \"must be\");\n-        ArrayKlass::cast(elm)->set_higher_dimension(oak);\n+      \/\/ Higher dimension may have been set when doing setup on ObjArrayKlass\n+      if (!oak->is_refined_objArray_klass()) {\n+        if (elm->is_instance_klass()) {\n+          assert(InstanceKlass::cast(elm)->array_klasses() == nullptr, \"must be\");\n+          InstanceKlass::cast(elm)->set_array_klasses(oak);\n+        } else {\n+          assert(elm->is_array_klass(), \"sanity\");\n+          assert(ArrayKlass::cast(elm)->higher_dimension() == nullptr, \"must be\");\n+          ArrayKlass::cast(elm)->set_higher_dimension(oak);\n+        }\n","filename":"src\/hotspot\/share\/cds\/dynamicArchive.cpp","additions":13,"deletions":8,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -90,0 +90,65 @@\n+inline void CDSMustMatchFlags::do_print(outputStream* st, bool v) {\n+  st->print(\"%s\", v ? \"true\" : \"false\");\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, intx v) {\n+  st->print(\"%zd\", v);\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, uintx v) {\n+  st->print(\"%zu\", v);\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, double v) {\n+  st->print(\"%f\", v);\n+}\n+\n+void CDSMustMatchFlags::init() {\n+  assert(CDSConfig::is_dumping_archive(), \"sanity\");\n+  _max_name_width = 0;\n+\n+#define INIT_CDS_MUST_MATCH_FLAG(n) \\\n+  _v_##n = n; \\\n+  _max_name_width = MAX2(_max_name_width,strlen(#n));\n+  CDS_MUST_MATCH_FLAGS_DO(INIT_CDS_MUST_MATCH_FLAG);\n+#undef INIT_CDS_MUST_MATCH_FLAG\n+}\n+\n+bool CDSMustMatchFlags::runtime_check() const {\n+#define CHECK_CDS_MUST_MATCH_FLAG(n) \\\n+  if (_v_##n != n) { \\\n+    ResourceMark rm; \\\n+    stringStream ss; \\\n+    ss.print(\"VM option %s is different between dumptime (\", #n);  \\\n+    do_print(&ss, _v_ ## n); \\\n+    ss.print(\") and runtime (\"); \\\n+    do_print(&ss, n); \\\n+    ss.print(\")\"); \\\n+    log_info(cds)(\"%s\", ss.as_string()); \\\n+    return false; \\\n+  }\n+  CDS_MUST_MATCH_FLAGS_DO(CHECK_CDS_MUST_MATCH_FLAG);\n+#undef CHECK_CDS_MUST_MATCH_FLAG\n+\n+  return true;\n+}\n+\n+void CDSMustMatchFlags::print_info() const {\n+  LogTarget(Info, cds) lt;\n+  if (lt.is_enabled()) {\n+    LogStream ls(lt);\n+    ls.print_cr(\"Recorded VM flags during dumptime:\");\n+    print(&ls);\n+  }\n+}\n+\n+void CDSMustMatchFlags::print(outputStream* st) const {\n+#define PRINT_CDS_MUST_MATCH_FLAG(n) \\\n+  st->print(\"- %-s \", #n);                   \\\n+  st->sp(int(_max_name_width - strlen(#n))); \\\n+  do_print(st, _v_##n);                      \\\n+  st->cr();\n+  CDS_MUST_MATCH_FLAGS_DO(PRINT_CDS_MUST_MATCH_FLAG);\n+#undef PRINT_CDS_MUST_MATCH_FLAG\n+}\n+\n@@ -249,0 +314,1 @@\n+  _has_valhalla_patched_classes = CDSConfig::is_valhalla_preview();\n@@ -262,0 +328,1 @@\n+  _must_match.init();\n@@ -320,0 +387,2 @@\n+  st->print_cr(\"- has_valhalla_patched_classes    %d\", _has_valhalla_patched_classes);\n+  _must_match.print(st);\n@@ -703,0 +772,4 @@\n+  if (!header()->check_must_match_flags()) {\n+    return false;\n+  }\n+\n@@ -2060,0 +2133,18 @@\n+  if (is_static()) {\n+    const char* err = nullptr;\n+    if (CDSConfig::is_valhalla_preview()) {\n+      if (!_has_valhalla_patched_classes) {\n+        err = \"not created\";\n+      }\n+    } else {\n+      if (_has_valhalla_patched_classes) {\n+        err = \"created\";\n+      }\n+    }\n+    if (err != nullptr) {\n+      log_warning(cds)(\"This archive was %s with --enable-preview -XX:+EnableValhalla. It is \"\n+                         \"incompatible with the current JVM setting\", err);\n+      return false;\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":91,"deletions":0,"binary":false,"changes":91,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -101,0 +102,31 @@\n+#define CDS_MUST_MATCH_FLAGS_DO(f) \\\n+  f(EnableValhalla) \\\n+  f(UseArrayFlattening) \\\n+  f(UseFieldFlattening) \\\n+  f(InlineTypePassFieldsAsArgs) \\\n+  f(InlineTypeReturnedAsFields) \\\n+  f(UseNonAtomicValueFlattening) \\\n+  f(UseAtomicValueFlattening) \\\n+  f(UseNullableValueFlattening)\n+\n+\n+class CDSMustMatchFlags {\n+private:\n+  size_t _max_name_width;\n+#define DECLARE_CDS_MUST_MATCH_FLAG(n) \\\n+  decltype(n) _v_##n;\n+  CDS_MUST_MATCH_FLAGS_DO(DECLARE_CDS_MUST_MATCH_FLAG);\n+#undef DECLARE_CDS_MUST_MATCH_FLAG\n+\n+  inline static void do_print(outputStream* st, bool v);\n+  inline static void do_print(outputStream* st, intx v);\n+  inline static void do_print(outputStream* st, uintx v);\n+  inline static void do_print(outputStream* st, double v);\n+  void print_info() const;\n+\n+public:\n+  void init();\n+  bool runtime_check() const;\n+  void print(outputStream* st) const;\n+};\n+\n@@ -142,0 +174,2 @@\n+  bool   _has_valhalla_patched_classes; \/\/ Is this archived dumped with --enable-preview -XX:+EnableValhalla?\n+  CDSMustMatchFlags _must_match;        \/\/ These flags must be the same between dumptime and runtime\n@@ -250,0 +284,4 @@\n+  bool check_must_match_flags() const {\n+    return _must_match.runtime_check();\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":38,"deletions":0,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -627,0 +627,1 @@\n+      \/\/ For valhalla, the prototype header is the same as markWord::prototype();\n@@ -1342,1 +1343,5 @@\n-      assert(resolved_k == k, \"classes used by archived heap must not be replaced by JVMTI ClassFileLoadHook\");\n+      if (resolved_k->is_array_klass()) {\n+        assert(resolved_k == k || resolved_k == k->super(), \"classes used by archived heap must not be replaced by JVMTI ClassFileLoadHook\");\n+      } else {\n+        assert(resolved_k == k, \"classes used by archived heap must not be replaced by JVMTI ClassFileLoadHook\");\n+      }\n@@ -2086,0 +2091,7 @@\n+\n+    if (CDSConfig::is_valhalla_preview() && strcmp(klass_name, \"jdk\/internal\/module\/ArchivedModuleGraph\") == 0) {\n+      \/\/ FIXME -- ArchivedModuleGraph doesn't work when java.base is patched with valhalla classes.\n+      i++;\n+      continue;\n+    }\n+\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -47,1 +47,3 @@\n-  friend class ciSignature;\n+  friend class ciSignature;\n+  friend class ciFlatArrayKlass;\n+  friend class ciArrayKlass;\n@@ -107,0 +109,8 @@\n+  virtual bool can_be_inline_klass(bool is_exact = false) {\n+    return false;\n+  }\n+\n+  virtual bool can_be_inline_array_klass() {\n+    return EnableValhalla && is_java_lang_Object();\n+  }\n+\n@@ -128,0 +138,2 @@\n+  markWord prototype_header() const;\n+\n","filename":"src\/hotspot\/share\/ci\/ciKlass.hpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -24,0 +24,1 @@\n+\n@@ -53,0 +54,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -87,0 +89,1 @@\n+#include \"utilities\/stringUtils.hpp\"\n@@ -159,0 +162,2 @@\n+#define CONSTANT_CLASS_DESCRIPTORS        70\n+\n@@ -197,1 +202,1 @@\n-      case JVM_CONSTANT_Class : {\n+      case JVM_CONSTANT_Class: {\n@@ -501,0 +506,3 @@\n+\n+        Symbol* const name = cp->symbol_at(class_index);\n+        const unsigned int name_len = name->utf8_length();\n@@ -710,1 +718,1 @@\n-            } else if (!Signature::is_void_method(signature)) { \/\/ must have void signature.\n+            } else if (!Signature::is_void_method(signature)) {  \/\/ must have void signature.\n@@ -730,2 +738,3 @@\n-            if (ref_kind == JVM_REF_newInvokeSpecial) {\n-              if (name != vmSymbols::object_initializer_name()) {\n+\n+            if (name != vmSymbols::object_initializer_name()) { \/\/ !<init>\n+              if (ref_kind == JVM_REF_newInvokeSpecial) {\n@@ -737,2 +746,10 @@\n-            } else {\n-              if (name == vmSymbols::object_initializer_name()) {\n+            } else { \/\/ <init>\n+              \/\/ The allowed invocation mode of <init> depends on its signature.\n+              \/\/ This test corresponds to verify_invoke_instructions in the verifier.\n+              const int signature_ref_index =\n+                cp->signature_ref_index_at(name_and_type_ref_index);\n+              const Symbol* const signature = cp->symbol_at(signature_ref_index);\n+              if (signature->is_void_method_signature()\n+                  && ref_kind == JVM_REF_newInvokeSpecial) {\n+                \/\/ OK, could be a constructor call\n+              } else {\n@@ -790,4 +807,13 @@\n-\/\/ Side-effects: populates the _local_interfaces field\n-void ClassFileParser::parse_interfaces(const ClassFileStream* const stream,\n-                                       const int itfs_len,\n-                                       ConstantPool* const cp,\n+static void check_identity_and_value_modifiers(ClassFileParser* current, const InstanceKlass* super_type, TRAPS) {\n+  assert(super_type != nullptr,\"Method doesn't support null super type\");\n+  if (super_type->access_flags().is_identity_class() && !current->access_flags().is_identity_class()\n+      && super_type->name() != vmSymbols::java_lang_Object()) {\n+      THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                err_msg(\"Value type %s has an identity type as supertype\",\n+                current->class_name()->as_klass_external_name()));\n+  }\n+}\n+\n+void ClassFileParser::parse_interfaces(const ClassFileStream* stream,\n+                                       int itfs_len,\n+                                       ConstantPool* cp,\n@@ -795,0 +821,6 @@\n+                                       \/\/ FIXME: lots of these functions\n+                                       \/\/ declare their parameters as const,\n+                                       \/\/ which adds only noise to the code.\n+                                       \/\/ Remove the spurious const modifiers.\n+                                       \/\/ Many are of the form \"const int x\"\n+                                       \/\/ or \"T* const x\".\n@@ -802,0 +834,1 @@\n+\n@@ -804,3 +837,2 @@\n-    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, nullptr, CHECK);\n-\n-    int index;\n+    _local_interface_indexes = new GrowableArray<u2>(itfs_len);\n+    int index = 0;\n@@ -809,1 +841,0 @@\n-      Klass* interf;\n@@ -814,29 +845,1 @@\n-      if (cp->tag_at(interface_index).is_klass()) {\n-        interf = cp->resolved_klass_at(interface_index);\n-      } else {\n-        Symbol* const unresolved_klass  = cp->klass_name_at(interface_index);\n-\n-        \/\/ Don't need to check legal name because it's checked when parsing constant pool.\n-        \/\/ But need to make sure it's not an array type.\n-        guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,\n-                           \"Bad interface name in class file %s\", CHECK);\n-\n-        \/\/ Call resolve on the interface class name with class circularity checking\n-        interf = SystemDictionary::resolve_super_or_fail(_class_name,\n-                                                         unresolved_klass,\n-                                                         Handle(THREAD, _loader_data->class_loader()),\n-                                                         false, CHECK);\n-      }\n-\n-      if (!interf->is_interface()) {\n-        THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n-                  err_msg(\"class %s can not implement %s, because it is not an interface (%s)\",\n-                          _class_name->as_klass_external_name(),\n-                          interf->external_name(),\n-                          interf->class_in_module_of_loader()));\n-      }\n-\n-      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n-        *has_nonstatic_concrete_methods = true;\n-      }\n-      _local_interfaces->at_put(index, InstanceKlass::cast(interf));\n+      _local_interface_indexes->at_put_grow(index, interface_index);\n@@ -854,2 +857,1 @@\n-      const InstanceKlass* const k = _local_interfaces->at(index);\n-      Symbol* interface_name = k->name();\n+      Symbol* interface_name = cp->klass_name_at(_local_interface_indexes->at(index));\n@@ -942,0 +944,2 @@\n+    _jdk_internal_LooselyConsistentValue,\n+    _jdk_internal_NullRestricted,\n@@ -1368,1 +1372,1 @@\n-                                   bool is_interface,\n+                                   AccessFlags class_access_flags,\n@@ -1381,0 +1385,1 @@\n+  bool is_inline_type = !class_access_flags.is_identity_class() && !class_access_flags.is_abstract();\n@@ -1388,1 +1393,5 @@\n-  const int total_fields = length + num_injected;\n+\n+  \/\/ two more slots are required for inline classes:\n+  \/\/ one for the static field with a reference to the pre-allocated default value\n+  \/\/ one for the field the JVM injects when detecting an empty inline class\n+  const int total_fields = length + num_injected + (is_inline_type ? 2 : 0);\n@@ -1394,0 +1403,1 @@\n+  int instance_fields_count = 0;\n@@ -1399,0 +1409,7 @@\n+    jint recognized_modifiers = JVM_RECOGNIZED_FIELD_MODIFIERS;\n+    if (!supports_inline_types()) {\n+      recognized_modifiers &= ~JVM_ACC_STRICT;\n+    }\n+\n+    const jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+    verify_legal_field_modifiers(flags, class_access_flags, CHECK);\n@@ -1400,2 +1417,0 @@\n-    const jint flags = cfs->get_u2_fast() & JVM_RECOGNIZED_FIELD_MODIFIERS;\n-    verify_legal_field_modifiers(flags, is_interface, CHECK);\n@@ -1418,0 +1433,1 @@\n+    if (!access_flags.is_static()) instance_fields_count++;\n@@ -1425,0 +1441,2 @@\n+    bool is_null_restricted = false;\n+\n@@ -1444,0 +1462,18 @@\n+        if (parsed_annotations.has_annotation(AnnotationCollector::_jdk_internal_NullRestricted)) {\n+          if (!Signature::has_envelope(sig)) {\n+            Exceptions::fthrow(\n+              THREAD_AND_LOCATION,\n+              vmSymbols::java_lang_ClassFormatError(),\n+              \"Illegal use of @jdk.internal.vm.annotation.NullRestricted annotation on field %s.%s with signature %s (primitive types can never be null)\",\n+              class_name()->as_C_string(), name->as_C_string(), sig->as_C_string());\n+          }\n+          const bool is_strict = (flags & JVM_ACC_STRICT) != 0;\n+          if (!is_strict) {\n+            Exceptions::fthrow(\n+              THREAD_AND_LOCATION,\n+              vmSymbols::java_lang_ClassFormatError(),\n+              \"Illegal use of @jdk.internal.vm.annotation.NullRestricted annotation on field %s.%s which doesn't have the @jdk.internal.vm.annotation.Strict annotation\",\n+              class_name()->as_C_string(), name->as_C_string());\n+          }\n+          is_null_restricted = true;\n+        }\n@@ -1466,0 +1502,4 @@\n+    if (is_null_restricted) {\n+      fieldFlags.update_null_free_inline_type(true);\n+    }\n+\n@@ -1482,0 +1522,3 @@\n+    if (access_flags.is_strict() && access_flags.is_static()) {\n+      _has_strict_static_fields = true;\n+    }\n@@ -1486,1 +1529,0 @@\n-  int index = length;\n@@ -1514,3 +1556,2 @@\n-      fi.set_index(index);\n-      _temp_field_info->append(fi);\n-      index++;\n+      int idx = _temp_field_info->append(fi);\n+      _temp_field_info->adr_at(idx)->set_index(idx);\n@@ -1520,1 +1561,17 @@\n-  assert(_temp_field_info->length() == index, \"Must be\");\n+  if (is_inline_type) {\n+    \/\/ Inject static \".null_reset\" field. This is an all-zero value with its null-channel set to zero.\n+    \/\/ IT should never be seen by user code, it is used when writing \"null\" to a nullable flat field\n+    \/\/ The all-zero value ensure that any embedded oop will be set to null, to avoid keeping dead objects\n+    \/\/ alive.\n+    FieldInfo::FieldFlags fflags2(0);\n+    fflags2.update_injected(true);\n+    AccessFlags aflags2(JVM_ACC_STATIC);\n+    FieldInfo fi2(aflags2,\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(null_reset_value_name)),\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(object_signature)),\n+                 0,\n+                 fflags2);\n+    int idx2 = _temp_field_info->append(fi2);\n+    _temp_field_info->adr_at(idx2)->set_index(idx2);\n+    _static_oop_count++;\n+  }\n@@ -1900,0 +1957,8 @@\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_LooselyConsistentValue_signature): {\n+      if (_location != _in_class)   break; \/\/ only allow for classes\n+      return _jdk_internal_LooselyConsistentValue;\n+    }\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_NullRestricted_signature): {\n+      if (_location != _in_field)   break; \/\/ only allow for fields\n+      return _jdk_internal_NullRestricted;\n+    }\n@@ -2136,0 +2201,2 @@\n+                                      bool is_value_class,\n+                                      bool is_abstract_class,\n@@ -2177,1 +2244,1 @@\n-    verify_legal_method_modifiers(flags, is_interface, name, CHECK_NULL);\n+    verify_legal_method_modifiers(flags, access_flags() , name, CHECK_NULL);\n@@ -2185,0 +2252,9 @@\n+  if (EnableValhalla) {\n+    if (((flags & JVM_ACC_SYNCHRONIZED) == JVM_ACC_SYNCHRONIZED)\n+        && ((flags & JVM_ACC_STATIC) == 0 )\n+        && !_access_flags.is_identity_class()) {\n+      classfile_parse_error(\"Invalid synchronized method in non-identity class %s\", THREAD);\n+        return nullptr;\n+    }\n+  }\n+\n@@ -2719,0 +2795,2 @@\n+                                    bool is_value_class,\n+                                    bool is_abstract_type,\n@@ -2743,0 +2821,2 @@\n+                                    is_value_class,\n+                                    is_abstract_type,\n@@ -3008,0 +3088,1 @@\n+\n@@ -3016,0 +3097,1 @@\n+\n@@ -3021,0 +3103,8 @@\n+    if (!supports_inline_types()) {\n+      const bool is_module = (flags & JVM_ACC_MODULE) != 0;\n+      const bool is_interface = (flags & JVM_ACC_INTERFACE) != 0;\n+      if (!is_module && !is_interface) {\n+        flags |= JVM_ACC_IDENTITY;\n+      }\n+    }\n+\n@@ -3126,0 +3216,43 @@\n+u2 ClassFileParser::parse_classfile_loadable_descriptors_attribute(const ClassFileStream* const cfs,\n+                                                                   const u1* const loadable_descriptors_attribute_start,\n+                                                                   TRAPS) {\n+  const u1* const current_mark = cfs->current();\n+  u2 length = 0;\n+  if (loadable_descriptors_attribute_start != nullptr) {\n+    cfs->set_current(loadable_descriptors_attribute_start);\n+    cfs->guarantee_more(2, CHECK_0);  \/\/ length\n+    length = cfs->get_u2_fast();\n+  }\n+  const int size = length;\n+  Array<u2>* const loadable_descriptors = MetadataFactory::new_array<u2>(_loader_data, size, CHECK_0);\n+  _loadable_descriptors = loadable_descriptors;\n+  if (length > 0) {\n+    int index = 0;\n+    cfs->guarantee_more(2 * length, CHECK_0);\n+    for (int n = 0; n < length; n++) {\n+      const u2 descriptor_index = cfs->get_u2_fast();\n+      guarantee_property(\n+        valid_symbol_at(descriptor_index),\n+        \"LoadableDescriptors descriptor_index %u has bad constant type in class file %s\",\n+        descriptor_index, CHECK_0);\n+      Symbol* descriptor = _cp->symbol_at(descriptor_index);\n+      bool valid = legal_field_signature(descriptor, CHECK_0);\n+      if(!valid) {\n+        ResourceMark rm(THREAD);\n+        Exceptions::fthrow(THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_ClassFormatError(),\n+          \"Descriptor from LoadableDescriptors attribute at index \\\"%d\\\" in class %s has illegal signature \\\"%s\\\"\",\n+          descriptor_index, _class_name->as_C_string(), descriptor->as_C_string());\n+        return 0;\n+      }\n+      loadable_descriptors->at_put(index++, descriptor_index);\n+    }\n+    assert(index == size, \"wrong size\");\n+  }\n+\n+  \/\/ Restore buffer's current position.\n+  cfs->set_current(current_mark);\n+\n+  return length;\n+}\n+\n@@ -3391,0 +3524,2 @@\n+  \/\/ Set _loadable_descriptors attribute to default sentinel\n+  _loadable_descriptors = Universe::the_empty_short_array();\n@@ -3397,0 +3532,1 @@\n+  bool parsed_loadable_descriptors_attribute = false;\n@@ -3418,0 +3554,2 @@\n+  const u1* loadable_descriptors_attribute_start = nullptr;\n+  u4  loadable_descriptors_attribute_length = 0;\n@@ -3633,0 +3771,9 @@\n+            if (EnableValhalla && tag == vmSymbols::tag_loadable_descriptors()) {\n+              if (parsed_loadable_descriptors_attribute) {\n+                classfile_parse_error(\"Multiple LoadableDescriptors attributes in class file %s\", CHECK);\n+                return;\n+              }\n+              parsed_loadable_descriptors_attribute = true;\n+              loadable_descriptors_attribute_start = cfs->current();\n+              loadable_descriptors_attribute_length = attribute_length;\n+            }\n@@ -3709,0 +3856,12 @@\n+  if (parsed_loadable_descriptors_attribute) {\n+    const u2 num_classes = parse_classfile_loadable_descriptors_attribute(\n+                            cfs,\n+                            loadable_descriptors_attribute_start,\n+                            CHECK);\n+    if (_need_verify) {\n+      guarantee_property(\n+        loadable_descriptors_attribute_length == sizeof(num_classes) + sizeof(u2) * num_classes,\n+        \"Wrong LoadableDescriptors attribute length in class file %s\", CHECK);\n+    }\n+  }\n+\n@@ -3775,0 +3934,1 @@\n+  this_klass->set_loadable_descriptors(_loadable_descriptors);\n@@ -3778,0 +3938,1 @@\n+  this_klass->set_inline_layout_info_array(_inline_layout_info_array);\n@@ -3816,2 +3977,1 @@\n-                       \"Invalid superclass index %u in class file %s\",\n-                       super_class_index,\n+                       \"Invalid superclass index 0 in class file %s\",\n@@ -4009,0 +4169,6 @@\n+bool ClassFileParser::supports_inline_types() const {\n+  \/\/ Inline types are only supported by class file version 70.65535 and later\n+  return _major_version > JAVA_26_VERSION ||\n+         (_major_version == JAVA_26_VERSION && _minor_version == JAVA_PREVIEW_MINOR_VERSION);\n+}\n+\n@@ -4052,3 +4218,4 @@\n-  } else if (max_transitive_size == local_size) {\n-    \/\/ only local interfaces added, share local interface array\n-    return local_ifs;\n+    \/\/ The three lines below are commented to work around bug JDK-8245487\n+\/\/  } else if (max_transitive_size == local_size) {\n+\/\/    \/\/ only local interfaces added, share local interface array\n+\/\/    return local_ifs;\n@@ -4075,0 +4242,1 @@\n+\n@@ -4105,0 +4273,10 @@\n+    \/\/ The JVMS says that super classes for value types must not have the ACC_IDENTITY\n+    \/\/ flag set. But, java.lang.Object must still be allowed to be a direct super class\n+    \/\/ for a value classes.  So, it is treated as a special case for now.\n+    if (!this_klass->access_flags().is_identity_class() &&\n+        super->name() != vmSymbols::java_lang_Object() &&\n+        super->is_identity_class()) {\n+      classfile_icce_error(\"value class %s cannot inherit from class %s\", super, THREAD);\n+      return;\n+    }\n+\n@@ -4297,1 +4475,1 @@\n-  const bool is_super      = (flags & JVM_ACC_SUPER)      != 0;\n+  const bool is_identity   = (flags & JVM_ACC_IDENTITY)   != 0;\n@@ -4301,0 +4479,2 @@\n+  const bool valid_value_class = is_identity || is_interface ||\n+                                 (supports_inline_types() && (!is_identity && (is_abstract || is_final)));\n@@ -4304,2 +4484,3 @@\n-      (is_interface && major_gte_1_5 && (is_super || is_enum)) ||\n-      (!is_interface && major_gte_1_5 && is_annotation)) {\n+      (is_interface && major_gte_1_5 && (is_identity || is_enum)) ||   \/\/  ACC_SUPER (now ACC_IDENTITY) was illegal for interfaces\n+      (!is_interface && major_gte_1_5 && is_annotation) ||\n+      (!valid_value_class)) {\n@@ -4307,0 +4488,4 @@\n+    const char* class_note = \"\";\n+    if (!valid_value_class) {\n+      class_note = \" (a value class must be final or else abstract)\";\n+    }\n@@ -4400,2 +4585,2 @@\n-void ClassFileParser::verify_legal_field_modifiers(jint flags,\n-                                                   bool is_interface,\n+void ClassFileParser:: verify_legal_field_modifiers(jint flags,\n+                                                   AccessFlags class_access_flags,\n@@ -4413,0 +4598,1 @@\n+  const bool is_strict    = (flags & JVM_ACC_STRICT)    != 0;\n@@ -4415,1 +4601,2 @@\n-  bool is_illegal = false;\n+  const bool is_interface = class_access_flags.is_interface();\n+  const bool is_identity_class = class_access_flags.is_identity_class();\n@@ -4417,9 +4604,30 @@\n-  if (is_interface) {\n-    if (!is_public || !is_static || !is_final || is_private ||\n-        is_protected || is_volatile || is_transient ||\n-        (major_gte_1_5 && is_enum)) {\n-      is_illegal = true;\n-    }\n-  } else { \/\/ not interface\n-    if (has_illegal_visibility(flags) || (is_final && is_volatile)) {\n-      is_illegal = true;\n+  bool is_illegal = false;\n+  const char* error_msg = \"\";\n+\n+  \/\/ There is some overlap in the checks that apply, for example interface fields\n+  \/\/ must be static, static fields can't be strict, and therefore interfaces can't\n+  \/\/ have strict fields. So we don't have to check every possible invalid combination\n+  \/\/ individually as long as all are covered. Once we have found an illegal combination\n+  \/\/ we can stop checking.\n+\n+  if (!is_illegal) {\n+    if (is_interface) {\n+      if (!is_public || !is_static || !is_final || is_private ||\n+          is_protected || is_volatile || is_transient ||\n+          (major_gte_1_5 && is_enum)) {\n+        is_illegal = true;\n+        error_msg = \"interface fields must be public, static and final, and may be synthetic\";\n+      }\n+    } else { \/\/ not interface\n+      if (has_illegal_visibility(flags)) {\n+        is_illegal = true;\n+        error_msg = \"invalid visibility flags for class field\";\n+      } else if (is_final && is_volatile) {\n+        is_illegal = true;\n+        error_msg = \"fields cannot be final and volatile\";\n+      } else if (supports_inline_types()) {\n+        if (!is_identity_class && !is_static && (!is_strict || !is_final)) {\n+          is_illegal = true;\n+          error_msg = \"value class fields must be either non-static final and strict, or static\";\n+        }\n+      }\n@@ -4435,2 +4643,2 @@\n-      \"Illegal field modifiers in class %s: 0x%X\",\n-      _class_name->as_C_string(), flags);\n+      \"Illegal field modifiers (%s) in class %s: 0x%X\",\n+      error_msg, _class_name->as_C_string(), flags);\n@@ -4442,1 +4650,1 @@\n-                                                    bool is_interface,\n+                                                    AccessFlags class_access_flags,\n@@ -4461,0 +4669,4 @@\n+  \/\/ LW401 CR required: removal of value factories support\n+  const bool is_interface    = class_access_flags.is_interface();\n+  const bool is_identity_class = class_access_flags.is_identity_class();\n+  const bool is_abstract_class = class_access_flags.is_abstract();\n@@ -4464,0 +4676,1 @@\n+  const char* class_note = \"\";\n@@ -4503,4 +4716,9 @@\n-        if (is_abstract) {\n-          if ((is_final || is_native || is_private || is_static ||\n-              (major_gte_1_5 && (is_synchronized || (!major_gte_17 && is_strict))))) {\n-            is_illegal = true;\n+        if (!is_identity_class && is_synchronized && !is_static) {\n+          is_illegal = true;\n+          class_note = \" (not an identity class)\";\n+        } else {\n+          if (is_abstract) {\n+            if ((is_final || is_native || is_private || is_static ||\n+                (major_gte_1_5 && (is_synchronized || (!major_gte_17 && is_strict))))) {\n+              is_illegal = true;\n+            }\n@@ -4519,2 +4737,3 @@\n-      \"Method %s in class %s has illegal modifiers: 0x%X\",\n-      name->as_C_string(), _class_name->as_C_string(), flags);\n+      \"Method %s in class %s%s has illegal modifiers: 0x%X\",\n+      name->as_C_string(), _class_name->as_C_string(),\n+      class_note, flags);\n@@ -4578,0 +4797,9 @@\n+bool ClassFileParser::is_class_in_loadable_descriptors_attribute(Symbol *klass) {\n+  if (_loadable_descriptors == nullptr) return false;\n+  for (int i = 0; i < _loadable_descriptors->length(); i++) {\n+        Symbol* class_name = _cp->symbol_at(_loadable_descriptors->at(i));\n+        if (class_name == klass) return true;\n+  }\n+  return false;\n+}\n+\n@@ -4679,1 +4907,2 @@\n-    case JVM_SIGNATURE_CLASS: {\n+    case JVM_SIGNATURE_CLASS:\n+    {\n@@ -4690,1 +4919,1 @@\n-        \/\/ Skip leading 'L' and ignore first appearance of ';'\n+        \/\/ Skip leading 'L' or 'Q' and ignore first appearance of ';'\n@@ -4746,0 +4975,4 @@\n+    } else if ((_major_version >= CONSTANT_CLASS_DESCRIPTORS || _class_name->starts_with(\"jdk\/internal\/reflect\/\"))\n+                   && bytes[length - 1] == ';' ) {\n+      \/\/ Support for L...; descriptors\n+      legal = verify_unqualified_name(bytes + 1, length - 2, LegalClass);\n@@ -4813,1 +5046,2 @@\n-      if (name == vmSymbols::object_initializer_name() || name == vmSymbols::class_initializer_name()) {\n+      if (name == vmSymbols::object_initializer_name() ||\n+          name == vmSymbols::class_initializer_name()) {\n@@ -4840,0 +5074,10 @@\n+bool ClassFileParser::legal_field_signature(const Symbol* signature, TRAPS) const {\n+  const char* const bytes = (const char*)signature->bytes();\n+  const unsigned int length = signature->utf8_length();\n+  const char* const p = skip_over_field_signature(bytes, false, length, CHECK_false);\n+\n+  if (p == nullptr || (p - bytes) != (int)length) {\n+    return false;\n+  }\n+  return true;\n+}\n@@ -4875,3 +5119,3 @@\n-      name->char_at(0) == JVM_SIGNATURE_SPECIAL &&\n-      sig_length > 0 &&\n-      signature->char_at(sig_length - 1) != JVM_SIGNATURE_VOID) {\n+    name->char_at(0) == JVM_SIGNATURE_SPECIAL &&\n+    sig_length > 0 &&\n+    signature->char_at(sig_length - 1) != JVM_SIGNATURE_VOID) {\n@@ -4928,2 +5172,2 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->_static_field_size;\n+  assert(_layout_info != nullptr, \"invariant\");\n+  return _layout_info->_static_field_size;\n@@ -4933,2 +5177,2 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->oop_map_blocks->_nonstatic_oop_map_count;\n+  assert(_layout_info != nullptr, \"invariant\");\n+  return _layout_info->oop_map_blocks->_nonstatic_oop_map_count;\n@@ -4938,2 +5182,2 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->_instance_size;\n+  assert(_layout_info != nullptr, \"invariant\");\n+  return _layout_info->_instance_size;\n@@ -5054,1 +5298,0 @@\n-\n@@ -5077,3 +5320,3 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  assert(ik->static_field_size() == _field_info->_static_field_size, \"sanity\");\n-  assert(ik->nonstatic_oop_map_count() == _field_info->oop_map_blocks->_nonstatic_oop_map_count,\n+  assert(_layout_info != nullptr, \"invariant\");\n+  assert(ik->static_field_size() == _layout_info->_static_field_size, \"sanity\");\n+  assert(ik->nonstatic_oop_map_count() == _layout_info->oop_map_blocks->_nonstatic_oop_map_count,\n@@ -5083,1 +5326,1 @@\n-  assert(ik->size_helper() == _field_info->_instance_size, \"sanity\");\n+  assert(ik->size_helper() == _layout_info->_instance_size, \"sanity\");\n@@ -5089,2 +5332,12 @@\n-  ik->set_nonstatic_field_size(_field_info->_nonstatic_field_size);\n-  ik->set_has_nonstatic_fields(_field_info->_has_nonstatic_fields);\n+  ik->set_nonstatic_field_size(_layout_info->_nonstatic_field_size);\n+  ik->set_has_nonstatic_fields(_layout_info->_has_nonstatic_fields);\n+  ik->set_has_strict_static_fields(_has_strict_static_fields);\n+\n+  if (_layout_info->_is_naturally_atomic) {\n+    ik->set_is_naturally_atomic();\n+  }\n+\n+  if (_layout_info->_must_be_atomic) {\n+    ik->set_must_be_atomic();\n+  }\n+\n@@ -5096,0 +5349,3 @@\n+  if (ik->is_inline_klass()) {\n+    InlineKlass::cast(ik)->init_fixed_block();\n+  }\n@@ -5110,0 +5366,1 @@\n+  assert(nullptr == _loadable_descriptors, \"invariant\");\n@@ -5113,0 +5370,1 @@\n+  assert(nullptr == _inline_layout_info_array, \"invariant\");\n@@ -5189,1 +5447,1 @@\n-  OopMapBlocksBuilder* oop_map_blocks = _field_info->oop_map_blocks;\n+  OopMapBlocksBuilder* oop_map_blocks = _layout_info->oop_map_blocks;\n@@ -5250,0 +5508,15 @@\n+  if (is_inline_type()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    vk->set_payload_alignment(_layout_info->_payload_alignment);\n+    vk->set_payload_offset(_layout_info->_payload_offset);\n+    vk->set_payload_size_in_bytes(_layout_info->_payload_size_in_bytes);\n+    vk->set_non_atomic_size_in_bytes(_layout_info->_non_atomic_size_in_bytes);\n+    vk->set_non_atomic_alignment(_layout_info->_non_atomic_alignment);\n+    vk->set_atomic_size_in_bytes(_layout_info->_atomic_layout_size_in_bytes);\n+    vk->set_nullable_size_in_bytes(_layout_info->_nullable_layout_size_in_bytes);\n+    vk->set_null_marker_offset(_layout_info->_null_marker_offset);\n+    vk->set_null_reset_value_offset(_layout_info->_null_reset_value_offset);\n+    if (_layout_info->_is_empty_inline_klass) vk->set_is_empty_inline_type();\n+    vk->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -5332,0 +5605,1 @@\n+  _loadable_descriptors(nullptr),\n@@ -5334,0 +5608,1 @@\n+  _local_interface_indexes(nullptr),\n@@ -5343,1 +5618,2 @@\n-  _field_info(nullptr),\n+  _layout_info(nullptr),\n+  _inline_layout_info_array(nullptr),\n@@ -5372,0 +5648,5 @@\n+  _has_strict_static_fields(false),\n+  _has_inline_type_fields(false),\n+  _is_naturally_atomic(false),\n+  _must_be_atomic(true),\n+  _has_loosely_consistent_annotation(false),\n@@ -5409,0 +5690,1 @@\n+  _loadable_descriptors = nullptr;\n@@ -5413,0 +5695,1 @@\n+  _inline_layout_info_array = nullptr;\n@@ -5432,0 +5715,4 @@\n+  if (_inline_layout_info_array != nullptr) {\n+    MetadataFactory::free_array<InlineLayoutInfo>(_loader_data, _inline_layout_info_array);\n+  }\n+\n@@ -5454,0 +5741,4 @@\n+  if (_loadable_descriptors != nullptr && _loadable_descriptors != Universe::the_empty_short_array()) {\n+    MetadataFactory::free_array<u2>(_loader_data, _loadable_descriptors);\n+  }\n+\n@@ -5552,0 +5843,9 @@\n+  \/\/ Fixing ACC_SUPER\/ACC_IDENTITY for old class files\n+  if (!supports_inline_types()) {\n+    const bool is_module = (flags & JVM_ACC_MODULE) != 0;\n+    const bool is_interface = (flags & JVM_ACC_INTERFACE) != 0;\n+    if (!is_module && !is_interface) {\n+      flags |= JVM_ACC_IDENTITY;\n+    }\n+  }\n+\n@@ -5655,2 +5955,0 @@\n-  assert(_local_interfaces != nullptr, \"invariant\");\n-\n@@ -5659,1 +5957,1 @@\n-               _access_flags.is_interface(),\n+               _access_flags,\n@@ -5669,1 +5967,3 @@\n-                _access_flags.is_interface(),\n+                is_interface(),\n+                !is_identity_class(),\n+                is_abstract_class(),\n@@ -5757,1 +6057,1 @@\n-    if (_access_flags.is_interface()) {\n+    if (is_interface()) {\n@@ -5779,0 +6079,14 @@\n+    if (_super_klass->is_interface()) {\n+      classfile_icce_error(\"class %s has interface %s as super class\", _super_klass, THREAD);\n+      return;\n+    }\n+\n+    if (_super_klass->is_final()) {\n+      classfile_icce_error(\"class %s cannot inherit from final class %s\", _super_klass, THREAD);\n+      return;\n+    }\n+\n+    if (EnableValhalla) {\n+      check_identity_and_value_modifiers(this, _super_klass, CHECK);\n+    }\n+\n@@ -5782,0 +6096,1 @@\n+  }\n@@ -5783,3 +6098,69 @@\n-    if (_super_klass->is_interface()) {\n-      classfile_icce_error(\"class %s has interface %s as super class\", _super_klass, THREAD);\n-      return;\n+  if (_parsed_annotations->has_annotation(AnnotationCollector::_jdk_internal_LooselyConsistentValue) && _access_flags.is_identity_class()) {\n+    THROW_MSG(vmSymbols::java_lang_ClassFormatError(),\n+          err_msg(\"class %s cannot have annotation jdk.internal.vm.annotation.LooselyConsistentValue, because it is not a value class\",\n+                  _class_name->as_klass_external_name()));\n+  }\n+\n+  \/\/ Determining is the class allows tearing or not (default is not)\n+  if (EnableValhalla && !_access_flags.is_identity_class()) {\n+    if (_parsed_annotations->has_annotation(ClassAnnotationCollector::_jdk_internal_LooselyConsistentValue)\n+        && (_super_klass == vmClasses::Object_klass() || !_super_klass->must_be_atomic())) {\n+      \/\/ Conditions above are not sufficient to determine atomicity requirements,\n+      \/\/ the presence of fields with atomic requirements could force the current class to have atomicy requirements too\n+      \/\/ Marking as not needing atomicity for now, can be updated when computing the fields layout\n+      \/\/ The InstanceKlass must be filled with the value from the FieldLayoutInfo returned by\n+      \/\/ the FieldLayoutBuilder, not with this _must_be_atomic field.\n+      _must_be_atomic = false;\n+    }\n+    \/\/ Apply VM options override\n+    if (*ForceNonTearable != '\\0') {\n+      \/\/ Allow a command line switch to force the same atomicity property:\n+      const char* class_name_str = _class_name->as_C_string();\n+      if (StringUtils::class_list_match(ForceNonTearable, class_name_str)) {\n+        _must_be_atomic = true;\n+      }\n+    }\n+  }\n+\n+  int itfs_len = _local_interface_indexes == nullptr ? 0 : _local_interface_indexes->length();\n+  _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, nullptr, CHECK);\n+  if (_local_interface_indexes != nullptr) {\n+    for (int i = 0; i < _local_interface_indexes->length(); i++) {\n+      u2 interface_index = _local_interface_indexes->at(i);\n+      Klass* interf;\n+      if (cp->tag_at(interface_index).is_klass()) {\n+        interf = cp->resolved_klass_at(interface_index);\n+      } else {\n+        Symbol* const unresolved_klass  = cp->klass_name_at(interface_index);\n+\n+        \/\/ Don't need to check legal name because it's checked when parsing constant pool.\n+        \/\/ But need to make sure it's not an array type.\n+        guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,\n+                            \"Bad interface name in class file %s\", CHECK);\n+\n+        \/\/ Call resolve on the interface class name with class circularity checking\n+        interf = SystemDictionary::resolve_super_or_fail(\n+                                                  _class_name,\n+                                                  unresolved_klass,\n+                                                  Handle(THREAD, _loader_data->class_loader()),\n+                                                  false,\n+                                                  CHECK);\n+      }\n+\n+      if (!interf->is_interface()) {\n+        THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                  err_msg(\"class %s can not implement %s, because it is not an interface (%s)\",\n+                          _class_name->as_klass_external_name(),\n+                          interf->external_name(),\n+                          interf->class_in_module_of_loader()));\n+      }\n+\n+      if (EnableValhalla) {\n+        \/\/ Check modifiers and set carries_identity_modifier\/carries_value_modifier flags\n+        check_identity_and_value_modifiers(this, InstanceKlass::cast(interf), CHECK);\n+      }\n+\n+      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n+        _has_nonstatic_concrete_methods = true;\n+      }\n+      _local_interfaces->at_put(i, InstanceKlass::cast(interf));\n@@ -5788,0 +6169,1 @@\n+  assert(_local_interfaces != nullptr, \"invariant\");\n@@ -5816,1 +6198,1 @@\n-  _itable_size = _access_flags.is_interface() ? 0 :\n+  _itable_size = is_interface() ? 0 :\n@@ -5821,3 +6203,92 @@\n-  _field_info = new FieldLayoutInfo();\n-  FieldLayoutBuilder lb(class_name(), super_klass(), _cp, \/*_fields*\/ _temp_field_info,\n-                        _parsed_annotations->is_contended(), _field_info);\n+  if (EnableValhalla) {\n+    _inline_layout_info_array = MetadataFactory::new_array<InlineLayoutInfo>(_loader_data,\n+                                                   java_fields_count(),\n+                                                   CHECK);\n+    for (GrowableArrayIterator<FieldInfo> it = _temp_field_info->begin(); it != _temp_field_info->end(); ++it) {\n+      FieldInfo fieldinfo = *it;\n+      if (fieldinfo.access_flags().is_static()) continue;  \/\/ Only non-static fields are processed at load time\n+      Symbol* sig = fieldinfo.signature(cp);\n+      if (fieldinfo.field_flags().is_null_free_inline_type()) {\n+        \/\/ Pre-load classes of null-free fields that are candidate for flattening\n+        TempNewSymbol s = Signature::strip_envelope(sig);\n+        if (s == _class_name) {\n+          THROW_MSG(vmSymbols::java_lang_ClassCircularityError(),\n+                    err_msg(\"Class %s cannot have a null-free non-static field of its own type\", _class_name->as_C_string()));\n+        }\n+        log_info(class, preload)(\"Preloading of class %s during loading of class %s. \"\n+                                  \"Cause: a null-free non-static field is declared with this type\",\n+                                  s->as_C_string(), _class_name->as_C_string());\n+        InstanceKlass* klass = SystemDictionary::resolve_with_circularity_detection(_class_name, s,\n+                                                                                    Handle(THREAD,\n+                                                                                    _loader_data->class_loader()),\n+                                                                                    false, THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+          log_warning(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                      \"(cause: null-free non-static field) failed: %s\",\n+                                      s->as_C_string(), _class_name->as_C_string(),\n+                                      PENDING_EXCEPTION->klass()->name()->as_C_string());\n+          return; \/\/ Exception is still pending\n+        }\n+        assert(klass != nullptr, \"Sanity check\");\n+        InstanceKlass::check_can_be_annotated_with_NullRestricted(klass, _class_name, CHECK);\n+        InlineKlass* vk = InlineKlass::cast(klass);\n+        _inline_layout_info_array->adr_at(fieldinfo.index())->set_klass(vk);\n+        log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                 \"(cause: null-free non-static field) succeeded\",\n+                                 s->as_C_string(), _class_name->as_C_string());\n+      } else if (Signature::has_envelope(sig) && PreloadClasses) {\n+        \/\/ Preloading classes for nullable fields that are listed in the LoadableDescriptors attribute\n+        \/\/ Those classes would be required later for the flattening of nullable inline type fields\n+        TempNewSymbol name = Signature::strip_envelope(sig);\n+        if (name != _class_name && is_class_in_loadable_descriptors_attribute(sig)) {\n+          log_info(class, preload)(\"Preloading of class %s during loading of class %s. \"\n+                                   \"Cause: field type in LoadableDescriptors attribute\",\n+                                   name->as_C_string(), _class_name->as_C_string());\n+          oop loader = loader_data()->class_loader();\n+          Klass* klass = SystemDictionary::resolve_super_or_fail(_class_name, name,\n+                                                                 Handle(THREAD, loader),\n+                                                                 false, THREAD);\n+          if (klass != nullptr) {\n+            if (klass->is_inline_klass()) {\n+              _inline_layout_info_array->adr_at(fieldinfo.index())->set_klass(InlineKlass::cast(klass));\n+              log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                       \"(cause: field type in LoadableDescriptors attribute) succeeded\",\n+                                       name->as_C_string(), _class_name->as_C_string());\n+            } else {\n+              \/\/ Non value class are allowed by the current spec, but it could be an indication of an issue so let's log a warning\n+              log_warning(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                          \"(cause: field type in LoadableDescriptors attribute) but loaded class is not a value class\",\n+                                          name->as_C_string(), _class_name->as_C_string());\n+            }\n+          } else {\n+            log_warning(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                        \"(cause: field type in LoadableDescriptors attribute) failed : %s\",\n+                                        name->as_C_string(), _class_name->as_C_string(),\n+                                        PENDING_EXCEPTION->klass()->name()->as_C_string());\n+          }\n+          \/\/ Loads triggered by the LoadableDescriptors attribute are speculative, failures must not impact loading of current class\n+          if (HAS_PENDING_EXCEPTION) {\n+            CLEAR_PENDING_EXCEPTION;\n+          }\n+        } else {\n+          \/\/ Just poking the system dictionary to see if the class has already be loaded. Looking for migrated classes\n+          \/\/ used when --enable-preview when jdk isn't compiled with --enable-preview so doesn't include LoadableDescriptors.\n+          \/\/ This is temporary.\n+          oop loader = loader_data()->class_loader();\n+          InstanceKlass* klass = SystemDictionary::find_instance_klass(THREAD, name, Handle(THREAD, loader));\n+          if (klass != nullptr && klass->is_inline_klass()) {\n+            _inline_layout_info_array->adr_at(fieldinfo.index())->set_klass(InlineKlass::cast(klass));\n+            log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                     \"(cause: field type not in LoadableDescriptors attribute) succeeded\",\n+                                     name->as_C_string(), _class_name->as_C_string());\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  _layout_info = new FieldLayoutInfo();\n+  FieldLayoutBuilder lb(class_name(), loader_data(), super_klass(), _cp, \/*_fields*\/ _temp_field_info,\n+      _parsed_annotations->is_contended(), is_inline_type(),\n+      access_flags().is_abstract() && !access_flags().is_identity_class() && !access_flags().is_interface(),\n+      _must_be_atomic, _layout_info, _inline_layout_info_array);\n@@ -5825,0 +6296,1 @@\n+  _has_inline_type_fields = _layout_info->_has_inline_fields;\n@@ -5834,0 +6306,21 @@\n+\n+  \/\/ Strict static fields track initialization status from the beginning of time.\n+  \/\/ After this class runs <clinit>, they will be verified as being \"not unset\".\n+  \/\/ See Step 8 of InstanceKlass::initialize_impl.\n+  if (_has_strict_static_fields) {\n+    bool found_one = false;\n+    for (int i = 0; i < _temp_field_info->length(); i++) {\n+      FieldInfo& fi = *_temp_field_info->adr_at(i);\n+      if (fi.access_flags().is_strict() && fi.access_flags().is_static()) {\n+        found_one = true;\n+        if (fi.initializer_index() != 0) {\n+          \/\/ skip strict static fields with ConstantValue attributes\n+        } else {\n+          _fields_status->adr_at(fi.index())->update_strict_static_unset(true);\n+          _fields_status->adr_at(fi.index())->update_strict_static_unread(true);\n+        }\n+      }\n+    }\n+    assert(found_one == _has_strict_static_fields,\n+           \"correct prediction = %d\", (int)_has_strict_static_fields);\n+  }\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":610,"deletions":117,"binary":false,"changes":727,"status":"modified"},{"patch":"@@ -77,1 +77,14 @@\n-  bool  _has_nonstatic_fields;\n+  int _payload_alignment;\n+  int _payload_offset;\n+  int _payload_size_in_bytes;\n+  int _non_atomic_size_in_bytes;\n+  int _non_atomic_alignment;\n+  int _atomic_layout_size_in_bytes;\n+  int _nullable_layout_size_in_bytes;\n+  int _null_marker_offset;\n+  int _null_reset_value_offset;\n+  bool _has_nonstatic_fields;\n+  bool _is_naturally_atomic;\n+  bool _must_be_atomic;\n+  bool _has_inline_fields;\n+  bool _is_empty_inline_klass;\n@@ -133,0 +146,1 @@\n+  Array<u2>* _loadable_descriptors;\n@@ -135,0 +149,1 @@\n+  GrowableArray<u2>* _local_interface_indexes;\n@@ -145,1 +160,2 @@\n-  FieldLayoutInfo* _field_info;\n+  FieldLayoutInfo* _layout_info;\n+  Array<InlineLayoutInfo>* _inline_layout_info_array;\n@@ -159,0 +175,1 @@\n+\n@@ -196,0 +213,6 @@\n+  bool _has_strict_static_fields;\n+\n+  bool _has_inline_type_fields;\n+  bool _is_naturally_atomic;\n+  bool _must_be_atomic;\n+  bool _has_loosely_consistent_annotation;\n@@ -262,1 +285,1 @@\n-                    bool is_interface,\n+                    AccessFlags class_access_flags,\n@@ -271,0 +294,2 @@\n+                       bool is_value_class,\n+                       bool is_abstract_class,\n@@ -277,0 +302,2 @@\n+                     bool is_value_class,\n+                     bool is_abstract_class,\n@@ -331,0 +358,4 @@\n+  u2 parse_classfile_loadable_descriptors_attribute(const ClassFileStream* const cfs,\n+                                                    const u1* const loadable_descriptors_attribute_start,\n+                                                    TRAPS);\n+\n@@ -423,0 +454,2 @@\n+  bool legal_field_signature(const Symbol* signature, TRAPS) const;\n+\n@@ -437,1 +470,1 @@\n-  void verify_legal_field_modifiers(jint flags, bool is_interface, TRAPS) const;\n+  void verify_legal_field_modifiers(jint flags, AccessFlags class_access_flags, TRAPS) const;\n@@ -439,1 +472,1 @@\n-                                     bool is_interface,\n+                                     AccessFlags class_access_flags,\n@@ -491,0 +524,3 @@\n+  \/\/ Check if the class file supports inline types\n+  bool supports_inline_types() const;\n+\n@@ -518,0 +554,8 @@\n+  \/\/ Being an inline type means being a concrete value class\n+  bool is_inline_type() const { return !_access_flags.is_identity_class() && !_access_flags.is_interface() && !_access_flags.is_abstract(); }\n+  bool is_abstract_class() const { return _access_flags.is_abstract(); }\n+  bool is_identity_class() const { return _access_flags.is_identity_class(); }\n+  bool has_inline_fields() const { return _has_inline_type_fields; }\n+\n+  u2 java_fields_count() const { return _java_fields_count; }\n+  bool is_abstract() const { return _access_flags.is_abstract(); }\n@@ -531,0 +575,2 @@\n+  bool is_class_in_loadable_descriptors_attribute(Symbol *klass);\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.hpp","additions":51,"deletions":5,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -1047,0 +1047,1 @@\n+  bool is_patched = false;\n@@ -1066,2 +1067,16 @@\n-    assert(!CDSConfig::is_dumping_archive(), \"CDS doesn't support --patch-module during dumping\");\n-    stream = search_module_entries(THREAD, _patch_mod_entries, pkg_entry, file_name);\n+    \/\/ At CDS dump time, the --patch-module entries are ignored. That means a\n+    \/\/ class is still loaded from the runtime image even if it might\n+    \/\/ appear in the _patch_mod_entries. The runtime shared class visibility\n+    \/\/ check will determine if a shared class is visible based on the runtime\n+    \/\/ environment, including the runtime --patch-module setting.\n+    if (!CDSConfig::is_valhalla_preview()) {\n+      \/\/ Dynamic dumping requires UseSharedSpaces to be enabled. Since --patch-module\n+      \/\/ is not supported with UseSharedSpaces, we can never come here during dynamic dumping.\n+      assert(!CDSConfig::is_dumping_archive(), \"CDS doesn't support --patch-module during dumping\");\n+    }\n+    if (CDSConfig::is_valhalla_preview() || !CDSConfig::is_dumping_static_archive()) {\n+      stream = search_module_entries(THREAD, _patch_mod_entries, pkg_entry, file_name);\n+      if (stream != nullptr) {\n+        is_patched = true;\n+      }\n+    }\n@@ -1116,0 +1131,3 @@\n+  if (is_patched) {\n+    result->set_shared_classpath_index(0);\n+  }\n@@ -1192,0 +1210,4 @@\n+  if (ik->shared_classpath_index() == 0 && ik->defined_by_boot_loader()) {\n+    return;\n+  }\n+\n","filename":"src\/hotspot\/share\/classfile\/classLoader.cpp","additions":24,"deletions":2,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -56,0 +56,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -57,1 +59,1 @@\n-#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.inline.hpp\"\n@@ -1107,1 +1109,9 @@\n-    if (k->is_typeArray_klass()) {\n+    if (k->is_flatArray_klass()) {\n+      Klass* element_klass = (Klass*) FlatArrayKlass::cast(k)->element_klass();\n+      assert(element_klass->is_inline_klass(), \"Must be inline type component\");\n+      if (is_scratch) {\n+        comp_mirror = Handle(THREAD, HeapShared::scratch_java_mirror(element_klass));\n+      } else {\n+        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+      }\n+    } else if (k->is_typeArray_klass()) {\n@@ -1116,0 +1126,1 @@\n+      assert(!k->is_refArray_klass() || !k->is_flatArray_klass(), \"Must not have mirror\");\n@@ -1118,0 +1129,1 @@\n+      oop comp_oop = element_klass->java_mirror();\n@@ -1121,1 +1133,1 @@\n-        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+        comp_mirror = Handle(THREAD, comp_oop);\n@@ -1150,1 +1162,0 @@\n-\n@@ -1154,0 +1165,9 @@\n+\n+    if (k->is_refined_objArray_klass()) {\n+      Klass* super_klass = k->super();\n+      assert(super_klass != nullptr, \"Must be\");\n+      Handle mirror(THREAD, super_klass->java_mirror());\n+      k->set_java_mirror(mirror);\n+      return;\n+    }\n+\n@@ -1176,0 +1196,1 @@\n+\n@@ -1199,3 +1220,3 @@\n-  if (k->class_loader() != nullptr &&\n-      k->class_loader() != SystemDictionary::java_platform_loader() &&\n-      k->class_loader() != SystemDictionary::java_system_loader()) {\n+  if ((k->class_loader() != nullptr &&\n+       k->class_loader() != SystemDictionary::java_platform_loader() &&\n+       k->class_loader() != SystemDictionary::java_system_loader())) {\n@@ -1240,1 +1261,0 @@\n-  assert(as_Klass(m) == k, \"must be\");\n@@ -1244,0 +1264,1 @@\n+    assert(as_Klass(m) == k, \"must be\");\n@@ -1253,0 +1274,4 @@\n+  } else {\n+    ObjArrayKlass* objarray_k = (ObjArrayKlass*)as_Klass(m);\n+    \/\/ Mirror should be restored for an ObjArrayKlass or one of its refined array klasses\n+    assert(objarray_k == k || objarray_k->next_refined_array_klass() == k, \"must be\");\n@@ -1431,1 +1456,3 @@\n-  if (is_instance)  st->print(\"L\");\n+  if (is_instance)  {\n+    st->print(\"L\");\n+  }\n@@ -1490,0 +1517,1 @@\n+  assert(!klass->is_refined_objArray_klass(), \"should not be ref or flat array klass\");\n@@ -2868,1 +2896,1 @@\n-      if (method->name() == vmSymbols::object_initializer_name() &&\n+      if (method->is_object_constructor() &&\n@@ -3227,2 +3255,2 @@\n-  if (m->is_object_initializer()) {\n-    flags |= java_lang_invoke_MemberName::MN_IS_CONSTRUCTOR;\n+  if (m->is_object_constructor()) {\n+    flags |= java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR;\n@@ -3619,1 +3647,1 @@\n-int java_lang_reflect_Field::_trusted_final_offset;\n+int java_lang_reflect_Field::_flags_offset;\n@@ -3629,1 +3657,1 @@\n-  macro(_trusted_final_offset,    k, vmSymbols::trusted_final_name(),    bool_signature,       false); \\\n+  macro(_flags_offset,     k, vmSymbols::flags_name(),     int_signature,    false); \\\n@@ -3694,2 +3722,2 @@\n-void java_lang_reflect_Field::set_trusted_final(oop field) {\n-  field->bool_field_put(_trusted_final_offset, true);\n+void java_lang_reflect_Field::set_flags(oop field, int value) {\n+  field->int_field_put(_flags_offset, value);\n@@ -3995,2 +4023,1 @@\n-int java_lang_boxing_object::_value_offset;\n-int java_lang_boxing_object::_long_value_offset;\n+int* java_lang_boxing_object::_offsets;\n@@ -3998,3 +4025,9 @@\n-#define BOXING_FIELDS_DO(macro) \\\n-  macro(_value_offset,      integerKlass, \"value\", int_signature, false); \\\n-  macro(_long_value_offset, longKlass, \"value\", long_signature, false);\n+#define BOXING_FIELDS_DO(macro)                                                                                                    \\\n+  macro(java_lang_boxing_object::_offsets[T_BOOLEAN - T_BOOLEAN], vmClasses::Boolean_klass(),   \"value\", bool_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_CHAR - T_BOOLEAN],    vmClasses::Character_klass(), \"value\", char_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_FLOAT - T_BOOLEAN],   vmClasses::Float_klass(),     \"value\", float_signature,  false); \\\n+  macro(java_lang_boxing_object::_offsets[T_DOUBLE - T_BOOLEAN],  vmClasses::Double_klass(),    \"value\", double_signature, false); \\\n+  macro(java_lang_boxing_object::_offsets[T_BYTE - T_BOOLEAN],    vmClasses::Byte_klass(),      \"value\", byte_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_SHORT - T_BOOLEAN],   vmClasses::Short_klass(),     \"value\", short_signature,  false); \\\n+  macro(java_lang_boxing_object::_offsets[T_INT - T_BOOLEAN],     vmClasses::Integer_klass(),   \"value\", int_signature,    false); \\\n+  macro(java_lang_boxing_object::_offsets[T_LONG - T_BOOLEAN],    vmClasses::Long_klass(),      \"value\", long_signature,   false);\n@@ -4003,2 +4036,2 @@\n-  InstanceKlass* integerKlass = vmClasses::Integer_klass();\n-  InstanceKlass* longKlass = vmClasses::Long_klass();\n+  assert(T_LONG - T_BOOLEAN == 7, \"Sanity check\");\n+  java_lang_boxing_object::_offsets = NEW_C_HEAP_ARRAY(int, 8, mtInternal);\n@@ -4010,0 +4043,4 @@\n+  if (f->reading()) {\n+    assert(T_LONG - T_BOOLEAN == 7, \"Sanity check\");\n+    java_lang_boxing_object::_offsets = NEW_C_HEAP_ARRAY(int, 8, mtInternal);\n+  }\n@@ -4030,1 +4067,1 @@\n-      box->bool_field_put(_value_offset, value->z);\n+      box->bool_field_put(value_offset(type), value->z);\n@@ -4033,1 +4070,1 @@\n-      box->char_field_put(_value_offset, value->c);\n+      box->char_field_put(value_offset(type), value->c);\n@@ -4036,1 +4073,1 @@\n-      box->float_field_put(_value_offset, value->f);\n+      box->float_field_put(value_offset(type), value->f);\n@@ -4039,1 +4076,1 @@\n-      box->double_field_put(_long_value_offset, value->d);\n+      box->double_field_put(value_offset(type), value->d);\n@@ -4042,1 +4079,1 @@\n-      box->byte_field_put(_value_offset, value->b);\n+      box->byte_field_put(value_offset(type), value->b);\n@@ -4045,1 +4082,1 @@\n-      box->short_field_put(_value_offset, value->s);\n+      box->short_field_put(value_offset(type), value->s);\n@@ -4048,1 +4085,1 @@\n-      box->int_field_put(_value_offset, value->i);\n+      box->int_field_put(value_offset(type), value->i);\n@@ -4051,1 +4088,1 @@\n-      box->long_field_put(_long_value_offset, value->j);\n+      box->long_field_put(value_offset(type), value->j);\n@@ -4073,1 +4110,1 @@\n-    value->z = box->bool_field(_value_offset);\n+    value->z = box->bool_field(value_offset(type));\n@@ -4076,1 +4113,1 @@\n-    value->c = box->char_field(_value_offset);\n+    value->c = box->char_field(value_offset(type));\n@@ -4079,1 +4116,1 @@\n-    value->f = box->float_field(_value_offset);\n+    value->f = box->float_field(value_offset(type));\n@@ -4082,1 +4119,1 @@\n-    value->d = box->double_field(_long_value_offset);\n+    value->d = box->double_field(value_offset(type));\n@@ -4085,1 +4122,1 @@\n-    value->b = box->byte_field(_value_offset);\n+    value->b = box->byte_field(value_offset(type));\n@@ -4088,1 +4125,1 @@\n-    value->s = box->short_field(_value_offset);\n+    value->s = box->short_field(value_offset(type));\n@@ -4091,1 +4128,1 @@\n-    value->i = box->int_field(_value_offset);\n+    value->i = box->int_field(value_offset(type));\n@@ -4094,1 +4131,1 @@\n-    value->j = box->long_field(_long_value_offset);\n+    value->j = box->long_field(value_offset(type));\n@@ -4107,1 +4144,1 @@\n-    box->bool_field_put(_value_offset, value->z);\n+    box->bool_field_put(value_offset(type), value->z);\n@@ -4110,1 +4147,1 @@\n-    box->char_field_put(_value_offset, value->c);\n+    box->char_field_put(value_offset(type), value->c);\n@@ -4113,1 +4150,1 @@\n-    box->float_field_put(_value_offset, value->f);\n+    box->float_field_put(value_offset(type), value->f);\n@@ -4116,1 +4153,1 @@\n-    box->double_field_put(_long_value_offset, value->d);\n+    box->double_field_put(value_offset(type), value->d);\n@@ -4119,1 +4156,1 @@\n-    box->byte_field_put(_value_offset, value->b);\n+    box->byte_field_put(value_offset(type), value->b);\n@@ -4122,1 +4159,1 @@\n-    box->short_field_put(_value_offset, value->s);\n+    box->short_field_put(value_offset(type), value->s);\n@@ -4125,1 +4162,1 @@\n-    box->int_field_put(_value_offset, value->i);\n+    box->int_field_put(value_offset(type), value->i);\n@@ -4128,1 +4165,1 @@\n-    box->long_field_put(_long_value_offset, value->j);\n+    box->long_field_put(value_offset(type), value->j);\n@@ -4524,1 +4561,0 @@\n-\n@@ -4534,1 +4570,1 @@\n-  return (flags(mname) & (MN_IS_METHOD | MN_IS_CONSTRUCTOR)) > 0;\n+  return (flags(mname) & (MN_IS_METHOD | MN_IS_OBJECT_CONSTRUCTOR)) > 0;\n@@ -5527,16 +5563,11 @@\n-#define CHECK_OFFSET(klass_name, cpp_klass_name, field_name, field_sig) \\\n-  valid &= check_offset(klass_name, cpp_klass_name :: _##field_name ## _offset, #field_name, field_sig)\n-\n-#define CHECK_LONG_OFFSET(klass_name, cpp_klass_name, field_name, field_sig) \\\n-  valid &= check_offset(klass_name, cpp_klass_name :: _##long_ ## field_name ## _offset, #field_name, field_sig)\n-\n-  \/\/ Boxed primitive objects (java_lang_boxing_object)\n-\n-  CHECK_OFFSET(\"java\/lang\/Boolean\",   java_lang_boxing_object, value, \"Z\");\n-  CHECK_OFFSET(\"java\/lang\/Character\", java_lang_boxing_object, value, \"C\");\n-  CHECK_OFFSET(\"java\/lang\/Float\",     java_lang_boxing_object, value, \"F\");\n-  CHECK_LONG_OFFSET(\"java\/lang\/Double\", java_lang_boxing_object, value, \"D\");\n-  CHECK_OFFSET(\"java\/lang\/Byte\",      java_lang_boxing_object, value, \"B\");\n-  CHECK_OFFSET(\"java\/lang\/Short\",     java_lang_boxing_object, value, \"S\");\n-  CHECK_OFFSET(\"java\/lang\/Integer\",   java_lang_boxing_object, value, \"I\");\n-  CHECK_LONG_OFFSET(\"java\/lang\/Long\", java_lang_boxing_object, value, \"J\");\n+#define CHECK_OFFSET(klass_name, type, field_sig) \\\n+  valid &= check_offset(klass_name, java_lang_boxing_object::value_offset(type), \"value\", field_sig)\n+\n+  CHECK_OFFSET(\"java\/lang\/Boolean\",   T_BOOLEAN, \"Z\");\n+  CHECK_OFFSET(\"java\/lang\/Character\", T_CHAR,    \"C\");\n+  CHECK_OFFSET(\"java\/lang\/Float\",     T_FLOAT,   \"F\");\n+  CHECK_OFFSET(\"java\/lang\/Double\",    T_DOUBLE,  \"D\");\n+  CHECK_OFFSET(\"java\/lang\/Byte\",      T_BYTE,    \"B\");\n+  CHECK_OFFSET(\"java\/lang\/Short\",     T_SHORT,   \"S\");\n+  CHECK_OFFSET(\"java\/lang\/Integer\",   T_INT,     \"I\");\n+  CHECK_OFFSET(\"java\/lang\/Long\",      T_LONG,    \"J\");\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":96,"deletions":65,"binary":false,"changes":161,"status":"modified"},{"patch":"@@ -255,0 +255,1 @@\n+\n@@ -319,0 +320,1 @@\n+\n@@ -833,1 +835,1 @@\n-  static int _trusted_final_offset;\n+  static int _flags_offset;\n@@ -861,1 +863,1 @@\n-  static void set_trusted_final(oop field);\n+  static void set_flags(oop field, int value);\n@@ -983,2 +985,1 @@\n-  static int _value_offset;\n-  static int _long_value_offset;\n+  static int* _offsets;\n@@ -1001,1 +1002,3 @@\n-    return is_double_word_type(type) ? _long_value_offset : _value_offset;\n+    assert(type >= T_BOOLEAN && type <= T_LONG, \"BasicType out of range\");\n+    assert(_offsets != nullptr, \"Uninitialized offsets\");\n+    return _offsets[type - T_BOOLEAN];\n@@ -1363,1 +1366,1 @@\n-    MN_IS_CONSTRUCTOR        = 0x00020000, \/\/ constructor\n+    MN_IS_OBJECT_CONSTRUCTOR = 0x00020000, \/\/ constructor\n@@ -1369,0 +1372,1 @@\n+    MN_NULL_RESTRICTED_FIELD = 0x00800000, \/\/ null-restricted field\n@@ -1370,1 +1374,3 @@\n-    MN_REFERENCE_KIND_MASK   = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT,\n+    MN_REFERENCE_KIND_MASK   = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT, \/\/ 4 bits\n+    MN_LAYOUT_SHIFT          = 28, \/\/ field layout\n+    MN_LAYOUT_MASK           = 0x70000000 >> MN_LAYOUT_SHIFT, \/\/ 3 bits\n@@ -1874,1 +1880,0 @@\n-\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":13,"deletions":8,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -57,0 +57,2 @@\n+#include \"oops\/fieldStreams.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -74,0 +76,1 @@\n+#include \"runtime\/os.hpp\"\n@@ -188,0 +191,15 @@\n+\/\/ These migrated value classes are loaded by the bootstrap class loader but are added to the initiating\n+\/\/ loaders automatically so that fields of these types can be found and potentially flattened during\n+\/\/ field layout.\n+static void add_migrated_value_classes(ClassLoaderData* cld) {\n+  JavaThread* current = JavaThread::current();\n+  auto add_klass = [&] (Symbol* classname) {\n+    InstanceKlass* ik = SystemDictionary::find_instance_klass(current, classname, Handle(current, nullptr));\n+    assert(ik != nullptr, \"Must exist\");\n+    SystemDictionary::add_to_initiating_loader(current, ik, cld);\n+  };\n+\n+  MonitorLocker mu1(SystemDictionary_lock);\n+  vmSymbols::migrated_class_names_do(add_klass);\n+}\n+\n@@ -193,2 +211,10 @@\n-    return (class_loader() == nullptr) ? ClassLoaderData::the_null_class_loader_data() :\n-                                      ClassLoaderDataGraph::find_or_create(class_loader);\n+    if (class_loader() == nullptr) {\n+      return ClassLoaderData::the_null_class_loader_data();\n+    } else {\n+      bool Bug8370217_FIXED = false;\n+      ClassLoaderData* cld = ClassLoaderDataGraph::find_or_create(class_loader);\n+      if (EnableValhalla && Bug8370217_FIXED) {\n+        add_migrated_value_classes(cld);\n+      }\n+      return cld;\n+    }\n@@ -402,1 +428,2 @@\n-\/\/ during class definition to allow class circularity checking\n+\/\/ during class definition, or may be called for inline field layout processing\n+\/\/ to detect class circularity errors.\n@@ -415,0 +442,2 @@\n+\/\/ inline field layout callers:\n+\/\/    The field's class must be loaded to determine layout.\n@@ -418,1 +447,1 @@\n-\/\/ placeholder for the same thread, class, classloader is found.\n+\/\/ placeholder for the same thread, class, and classloader is found.\n@@ -453,1 +482,1 @@\n-    \/\/ Must check ClassCircularity before resolving next_name (superclass or interface).\n+    \/\/ Must check ClassCircularity before resolving next_name (superclass, interface, field types or speculatively preloaded argument types).\n@@ -477,1 +506,1 @@\n-  \/\/ Resolve the superclass or superinterface, check results on return\n+  \/\/ Resolve the superclass, superinterface, field type or speculatively preloaded argument types and check results on return.\n@@ -923,2 +952,1 @@\n-  assert(!ModuleEntryTable::javabase_moduleEntry()->is_patched(),\n-         \"Cannot use sharing if java.base is patched\");\n+  assert(!CDSConfig::module_patching_disables_cds(), \"Cannot use CDS\");\n@@ -1000,1 +1028,1 @@\n-        assert(!mod_entry->is_patched(), \"cannot load archived classes for patched module\");\n+        assert(!CDSConfig::module_patching_disables_cds(), \"Cannot use CDS\");\n@@ -1078,0 +1106,69 @@\n+\/\/ Pre-load class referred to in non-static null-free instance field. These fields trigger MANDATORY loading.\n+\/\/ Some pre-loading does not fail fatally\n+bool SystemDictionary::preload_from_null_free_field(InstanceKlass* ik, Handle class_loader, Symbol* sig, int field_index, TRAPS) {\n+  TempNewSymbol name = Signature::strip_envelope(sig);\n+  log_info(class, preload)(\"Preloading of class %s during loading of shared class %s. \"\n+                           \"Cause: a null-free non-static field is declared with this type\",\n+                           name->as_C_string(), ik->name()->as_C_string());\n+  InstanceKlass* real_k = SystemDictionary::resolve_with_circularity_detection(ik->name(), name,\n+                                                                               class_loader, false, CHECK_false);\n+  if (HAS_PENDING_EXCEPTION) {\n+    log_warning(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                \"(cause: null-free non-static field) failed: %s\",\n+                                name->as_C_string(), ik->name()->as_C_string(),\n+                                PENDING_EXCEPTION->klass()->name()->as_C_string());\n+    return false; \/\/ Exception is still pending\n+  }\n+\n+  InstanceKlass* k = ik->get_inline_type_field_klass_or_null(field_index);\n+  if (real_k != k) {\n+    \/\/ oops, the app has substituted a different version of k! Does not fail fatally\n+    log_warning(class, preload)(\"Preloading of class %s during loading of shared class %s \"\n+                                \"(cause: null-free non-static field) failed : \"\n+                                \"app substituted a different version of %s\",\n+                                name->as_C_string(), ik->name()->as_C_string(),\n+                                name->as_C_string());\n+    return false;\n+  }\n+  log_info(class, preload)(\"Preloading of class %s during loading of shared class %s \"\n+                           \"(cause: null-free non-static field) succeeded\",\n+                           name->as_C_string(), ik->name()->as_C_string());\n+\n+  assert(real_k != nullptr, \"Sanity check\");\n+  InstanceKlass::check_can_be_annotated_with_NullRestricted(real_k, ik->name(), CHECK_false);\n+\n+  return true;\n+}\n+\n+\/\/ Tries to pre-load classes referred to in non-static nullable instance fields if they are found in the\n+\/\/ loadable descriptors attribute. If loading fails, we can fail silently.\n+void SystemDictionary::try_preload_from_loadable_descriptors(InstanceKlass* ik, Handle class_loader, Symbol* sig, int field_index, TRAPS) {\n+  TempNewSymbol name = Signature::strip_envelope(sig);\n+  if (name != ik->name() && ik->is_class_in_loadable_descriptors_attribute(sig)) {\n+    log_info(class, preload)(\"Preloading of class %s during loading of shared class %s. \"\n+                             \"Cause: field type in LoadableDescriptors attribute\",\n+                             name->as_C_string(), ik->name()->as_C_string());\n+    InstanceKlass* real_k = SystemDictionary::resolve_with_circularity_detection(ik->name(), name,\n+                                                                                 class_loader, false, THREAD);\n+    if (HAS_PENDING_EXCEPTION) {\n+      CLEAR_PENDING_EXCEPTION;\n+    }\n+\n+    InstanceKlass* k = ik->get_inline_type_field_klass_or_null(field_index);\n+    if (real_k != k) {\n+      \/\/ oops, the app has substituted a different version of k!\n+      log_warning(class, preload)(\"Preloading of class %s during loading of shared class %s \"\n+                                  \"(cause: field type in LoadableDescriptors attribute) failed : \"\n+                                  \"app substituted a different version of %s\",\n+                                  name->as_C_string(), ik->name()->as_C_string(),\n+                                  k->name()->as_C_string());\n+      return;\n+    } else if (real_k != nullptr) {\n+      log_info(class, preload)(\"Preloading of class %s during loading of shared class %s \"\n+                               \"(cause: field type in LoadableDescriptors attribute) succeeded\",\n+                                name->as_C_string(), ik->name()->as_C_string());\n+    }\n+  }\n+}\n+\n+\n@@ -1101,0 +1198,21 @@\n+  if (ik->has_inline_type_fields()) {\n+    for (AllFieldStream fs(ik); !fs.done(); fs.next()) {\n+      if (fs.access_flags().is_static()) continue;\n+\n+      Symbol* sig = fs.signature();\n+      int field_index = fs.index();\n+\n+      if (fs.is_null_free_inline_type()) {\n+        \/\/ A false return means that the class didn't load for other reasons than an exception.\n+        bool check = preload_from_null_free_field(ik, class_loader, sig, field_index, CHECK_NULL);\n+        if (!check) {\n+          ik->set_shared_loading_failed();\n+          return nullptr;\n+        }\n+      } else if (Signature::has_envelope(sig)) {\n+          \/\/ Pending exceptions are cleared so we can fail silently\n+          try_preload_from_loadable_descriptors(ik, class_loader, sig, field_index, CHECK_NULL);\n+      }\n+    }\n+  }\n+\n@@ -1136,0 +1254,1 @@\n+\n@@ -1710,1 +1829,0 @@\n-#if INCLUDE_CDS\n@@ -1713,1 +1831,3 @@\n-\/\/ This API should be used only by AOTLinkedClassBulkLoader\n+\/\/ This API is used by AOTLinkedClassBulkLoader and to register boxing\n+\/\/ classes from java.lang in all class loaders to enable more value\n+\/\/ classes optimizations\n@@ -1717,1 +1837,0 @@\n-  assert(CDSConfig::is_using_aot_linked_classes(), \"must be\");\n@@ -1723,2 +1842,3 @@\n-  assert(dictionary->find_class(current, name) == nullptr, \"sanity\");\n-  dictionary->add_klass(current, name, k);\n+  if (dictionary->find_class(current, name) == nullptr) {\n+    dictionary->add_klass(current, name, k);\n+  }\n@@ -1726,1 +1846,0 @@\n-#endif\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":134,"deletions":15,"binary":false,"changes":149,"status":"modified"},{"patch":"@@ -142,0 +142,1 @@\n+\n@@ -287,1 +288,1 @@\n-                                       ClassLoaderData* loader_data) NOT_CDS_RETURN;\n+                                       ClassLoaderData* loader_data);\n@@ -334,0 +335,2 @@\n+  static bool preload_from_null_free_field(InstanceKlass* ik, Handle class_loader, Symbol* sig, int field_index, TRAPS);\n+  static void try_preload_from_loadable_descriptors(InstanceKlass* ik, Handle class_loader, Symbol* sig, int field_index, TRAPS);\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1279,0 +1279,1 @@\n+  SET_ADDRESS(_extrs, SharedRuntime::allocate_inline_types);\n@@ -1328,0 +1329,8 @@\n+    SET_ADDRESS(_extrs, Runtime1::new_null_free_array);\n+    SET_ADDRESS(_extrs, Runtime1::load_flat_array);\n+    SET_ADDRESS(_extrs, Runtime1::store_flat_array);\n+    SET_ADDRESS(_extrs, Runtime1::substitutability_check);\n+    SET_ADDRESS(_extrs, Runtime1::buffer_inline_args);\n+    SET_ADDRESS(_extrs, Runtime1::buffer_inline_args_no_receiver);\n+    SET_ADDRESS(_extrs, Runtime1::throw_identity_exception);\n+    SET_ADDRESS(_extrs, Runtime1::throw_illegal_monitor_state_exception);\n@@ -1361,0 +1370,2 @@\n+    SET_ADDRESS(_extrs, OptoRuntime::load_unknown_inline_C);\n+    SET_ADDRESS(_extrs, OptoRuntime::store_unknown_inline_C);\n@@ -1387,0 +1398,4 @@\n+  if (UseCompressedOops) {\n+    SET_ADDRESS(_extrs, CompressedOops::base_addr());\n+  }\n+\n","filename":"src\/hotspot\/share\/code\/aotCodeCache.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -66,0 +66,1 @@\n+static_assert(!std::is_polymorphic<BufferedInlineTypeBlob>::value,   \"no virtual methods are allowed in code blobs\");\n@@ -94,0 +95,1 @@\n+      &BufferedInlineTypeBlob::_vpntr,\n@@ -429,1 +431,1 @@\n-    blob = new (size) BufferBlob(name, CodeBlobKind::Buffer, cb, size);\n+    blob = new (size) BufferBlob(name, CodeBlobKind::Buffer, cb, size, sizeof(BufferBlob));\n@@ -445,0 +447,4 @@\n+BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, uint16_t header_size, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments)\n+  : RuntimeBlob(name, kind, cb, size, header_size, frame_complete, frame_size, oop_maps, caller_must_gc_arguments)\n+{}\n+\n@@ -449,3 +455,2 @@\n-AdapterBlob::AdapterBlob(int size, CodeBuffer* cb, int entry_offset[AdapterBlob::ENTRY_COUNT]) :\n-  BufferBlob(\"I2C\/C2I adapters\", CodeBlobKind::Adapter, cb, size, sizeof(AdapterBlob)) {\n-  assert(entry_offset[I2C] == 0, \"sanity check\");\n+AdapterBlob::AdapterBlob(int size, CodeBuffer* cb, int entry_offset[AdapterBlob::ENTRY_COUNT], int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) :\n+  BufferBlob(\"I2C\/C2I adapters\", CodeBlobKind::Adapter, cb, size, sizeof(AdapterBlob), frame_complete, frame_size, oop_maps, caller_must_gc_arguments) {\n@@ -453,0 +458,1 @@\n+  assert(entry_offset[I2C] == 0, \"sanity check\");\n@@ -462,0 +468,2 @@\n+  _c2i_inline_offset = entry_offset[C2I_Inline];\n+  _c2i_inline_ro_offset = entry_offset[C2I_Inline_RO];\n@@ -463,0 +471,1 @@\n+  _c2i_unverified_inline_offset = entry_offset[C2I_Unverified_Inline];\n@@ -467,1 +476,1 @@\n-AdapterBlob* AdapterBlob::create(CodeBuffer* cb, int entry_offset[AdapterBlob::ENTRY_COUNT]) {\n+AdapterBlob* AdapterBlob::create(CodeBuffer* cb, int entry_offset[AdapterBlob::ENTRY_COUNT], int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) {\n@@ -476,1 +485,1 @@\n-    blob = new (size) AdapterBlob(size, cb, entry_offset);\n+    blob = new (size) AdapterBlob(size, cb, entry_offset, frame_complete, frame_size, oop_maps, caller_must_gc_arguments);\n@@ -558,0 +567,25 @@\n+\/\/ Implementation of BufferedInlineTypeBlob\n+BufferedInlineTypeBlob::BufferedInlineTypeBlob(int size, CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off) :\n+  BufferBlob(\"buffered inline type\", CodeBlobKind::BufferedInlineType, cb, size, sizeof(BufferedInlineTypeBlob)),\n+  _pack_fields_off(pack_fields_off),\n+  _pack_fields_jobject_off(pack_fields_jobject_off),\n+  _unpack_fields_off(unpack_fields_off) {\n+  CodeCache::commit(this);\n+}\n+\n+BufferedInlineTypeBlob* BufferedInlineTypeBlob::create(CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off) {\n+  ThreadInVMfromUnknown __tiv;  \/\/ get to VM state in case we block on CodeCache_lock\n+\n+  BufferedInlineTypeBlob* blob = nullptr;\n+  unsigned int size = CodeBlob::allocation_size(cb, sizeof(BufferedInlineTypeBlob));\n+  {\n+    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+    blob = new (size) BufferedInlineTypeBlob(size, cb, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+  }\n+  \/\/ Track memory usage statistic after releasing CodeCache_lock\n+  MemoryService::track_code_cache_memory_usage();\n+\n+  return blob;\n+}\n+\n+\/\/----------------------------------------------------------------------------------------------------\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":40,"deletions":6,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+\/\/    BufferedInlineTypeBlob   : used for pack\/unpack handlers\n@@ -86,0 +87,1 @@\n+  BufferedInlineType,\n@@ -204,0 +206,1 @@\n+  bool is_buffered_inline_type_blob() const   { return _kind == CodeBlobKind::BufferedInlineType; }\n@@ -370,0 +373,1 @@\n+  friend class BufferedInlineTypeBlob;\n@@ -377,0 +381,1 @@\n+  BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, uint16_t header_size, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments = false);\n@@ -411,0 +416,2 @@\n+    C2I_Inline,\n+    C2I_Inline_RO,\n@@ -412,0 +419,1 @@\n+    C2I_Unverified_Inline,\n@@ -416,1 +424,2 @@\n-  AdapterBlob(int size, CodeBuffer* cb, int entry_offset[ENTRY_COUNT]);\n+  AdapterBlob(int size, CodeBuffer* cb, int entry_offset[ENTRY_COUNT], int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments = false);\n+\n@@ -419,0 +428,2 @@\n+  int _c2i_inline_offset;\n+  int _c2i_inline_ro_offset;\n@@ -420,0 +431,1 @@\n+  int _c2i_unverified_inline_offset;\n@@ -423,0 +435,8 @@\n+  static AdapterBlob* create(CodeBuffer* cb,\n+                             int entry_offset[ENTRY_COUNT],\n+                             int frame_complete,\n+                             int frame_size,\n+                             OopMapSet* oop_maps,\n+                             bool caller_must_gc_arguments = false);\n+\n+  bool caller_must_gc_arguments(JavaThread* thread) const { return true; }\n@@ -426,0 +446,2 @@\n+  address c2i_inline_entry() { return i2c_entry() + _c2i_inline_offset; }\n+  address c2i_inline_ro_entry() { return i2c_entry() + _c2i_inline_ro_offset; }\n@@ -427,0 +449,1 @@\n+  address c2i_unverified_inline_entry() { return i2c_entry() + _c2i_unverified_inline_offset; }\n@@ -454,0 +477,19 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ BufferedInlineTypeBlob : used for pack\/unpack handlers\n+\n+class BufferedInlineTypeBlob: public BufferBlob {\n+private:\n+  const int _pack_fields_off;\n+  const int _pack_fields_jobject_off;\n+  const int _unpack_fields_off;\n+\n+  BufferedInlineTypeBlob(int size, CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off);\n+\n+public:\n+  \/\/ Creation\n+  static BufferedInlineTypeBlob* create(CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off);\n+\n+  address pack_fields() const { return code_begin() + _pack_fields_off; }\n+  address pack_fields_jobject() const { return code_begin() + _pack_fields_jobject_off; }\n+  address unpack_fields() const { return code_begin() + _unpack_fields_off; }\n+};\n","filename":"src\/hotspot\/share\/code\/codeBlob.hpp","additions":43,"deletions":1,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -1289,1 +1289,1 @@\n-          if (vfst.method()->is_static_initializer() ||\n+        if (vfst.method()->is_class_initializer() ||\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -215,1 +215,1 @@\n-void G1BarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+void G1BarrierSetC2::eliminate_gc_barrier(PhaseIterGVN* igvn, Node* node) const {\n@@ -218,1 +218,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -97,1 +97,1 @@\n-  virtual void eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const;\n+  virtual void eliminate_gc_barrier(PhaseIterGVN* igvn, Node* node) const;\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -241,1 +241,2 @@\n-  to_array->oop_iterate_range(&_scanner,\n+  assert(to_array->is_refArray(), \"Must be\");\n+  refArrayOop(to_array)->oop_iterate_range(&_scanner,\n@@ -263,1 +264,2 @@\n-  to_array->oop_iterate_range(&_scanner, 0, checked_cast<int>(initial_chunk_size));\n+  assert(to_array->is_refArray(), \"Must be\");\n+  refArrayOop(to_array)->oop_iterate_range(&_scanner, 0, checked_cast<int>(initial_chunk_size));\n@@ -434,1 +436,1 @@\n-    if (klass->is_array_klass()) {\n+    if (klass->is_array_klass() && !klass->is_flatArray_klass()) {\n@@ -437,1 +439,1 @@\n-      if (klass->is_objArray_klass()) {\n+      if (klass->is_refArray_klass()) {\n@@ -443,1 +445,1 @@\n-        assert(klass->is_typeArray_klass(), \"invariant\");\n+        assert(klass->is_typeArray_klass() || klass->is_objArray_klass(), \"invariant\");\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -78,0 +78,1 @@\n+#include \"oops\/flatArrayKlass.inline.hpp\"\n@@ -2329,1 +2330,1 @@\n-    cast_to_oop(copy_destination())->init_mark();\n+    cast_to_oop(copy_destination())->reinit_mark();\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -385,0 +385,1 @@\n+  assert(obj->is_refArray(), \"Must be\");\n@@ -400,1 +401,1 @@\n-  if (obj->is_objArray()) {\n+  if (obj->is_refArray()) {\n@@ -417,1 +418,1 @@\n-  array->oop_iterate_range(&mark_and_push_closure, beg_index, end_index);\n+  refArrayOop(array)->oop_iterate_range(&mark_and_push_closure, beg_index, end_index);\n","filename":"src\/hotspot\/share\/gc\/serial\/serialFullGC.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -51,0 +51,4 @@\n+Node* C2ParseAccess::control() const {\n+  return _ctl == nullptr ? _kit->control() : _ctl;\n+}\n+\n@@ -204,1 +208,1 @@\n-    Node* control = control_dependent ? kit->control() : nullptr;\n+    Node* control = control_dependent ? parse_access.control() : nullptr;\n@@ -865,1 +869,1 @@\n-  phase->igvn().replace_node(ac, call);\n+  phase->replace_node(ac, call);\n@@ -889,1 +893,1 @@\n-  phase->igvn().replace_node(ac, call);\n+  phase->replace_node(ac, call);\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -296,0 +296,1 @@\n+  oop obj_buffer_allocate(Klass* klass, size_t size, TRAPS); \/\/ doesn't clear memory\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrier.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -68,0 +68,6 @@\n+  enum OopCopyCheckStatus {\n+    oop_copy_check_ok = 0,         \/\/ oop array copy sucessful\n+    oop_copy_check_class_cast = 1, \/\/ oop array copy failed subtype check (ARRAYCOPY_CHECKCAST)\n+    oop_copy_check_null = 2        \/\/ oop array copy failed null check (ARRAYCOPY_NOTNULL)\n+  };\n+\n@@ -101,2 +107,2 @@\n-    static bool oop_copy_one_check_cast(zpointer* dst, zpointer* src, Klass* dst_klass);\n-    static void oop_copy_one(zpointer* dst, zpointer* src);\n+    static OopCopyCheckStatus oop_copy_one_check_cast(zpointer* dst, zpointer* src, Klass* dst_klass);\n+    static OopCopyCheckStatus oop_copy_one(zpointer* dst, zpointer* src);\n@@ -104,2 +110,2 @@\n-    static bool oop_arraycopy_in_heap_check_cast(zpointer* dst, zpointer* src, size_t length, Klass* dst_klass);\n-    static bool oop_arraycopy_in_heap_no_check_cast(zpointer* dst, zpointer* src, size_t length);\n+    static OopCopyCheckStatus oop_arraycopy_in_heap_check_cast(zpointer* dst, zpointer* src, size_t length, Klass* dst_klass);\n+    static OopCopyCheckStatus oop_arraycopy_in_heap_no_check_cast(zpointer* dst, zpointer* src, size_t length);\n@@ -137,1 +143,1 @@\n-    static bool oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, zpointer* src_raw,\n+    static void oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, zpointer* src_raw,\n@@ -140,1 +146,1 @@\n-    static bool oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, oop* src_raw,\n+    static void oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, oop* src_raw,\n@@ -143,3 +149,3 @@\n-      return oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, (zpointer*)src_raw,\n-                                   dst_obj, dst_offset_in_bytes, (zpointer*)dst_raw,\n-                                   length);\n+      oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, (zpointer*)src_raw,\n+                            dst_obj, dst_offset_in_bytes, (zpointer*)dst_raw,\n+                            length);\n@@ -147,1 +153,1 @@\n-    static bool oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, narrowOop* src_raw,\n+    static void oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, narrowOop* src_raw,\n@@ -149,1 +155,1 @@\n-                                      size_t length) { unsupported(); return false; }\n+                                      size_t length) { unsupported(); }\n@@ -153,0 +159,2 @@\n+    static void value_copy_in_heap(void* src, void* dst, InlineKlass* md, LayoutKind lk);\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSet.hpp","additions":19,"deletions":11,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -33,0 +34,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -34,0 +36,1 @@\n+#include \"utilities\/copy.hpp\"\n@@ -331,1 +334,1 @@\n-inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_copy_one(zpointer* dst, zpointer* src) {\n+inline ZBarrierSet::OopCopyCheckStatus ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_copy_one(zpointer* dst, zpointer* src) {\n@@ -334,0 +337,4 @@\n+  if (HasDecorator<decorators, ARRAYCOPY_NOTNULL>::value && is_null(obj)) {\n+    return oop_copy_check_null;\n+  }\n+\n@@ -335,0 +342,1 @@\n+  return oop_copy_check_ok;\n@@ -338,1 +346,1 @@\n-inline bool ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_copy_one_check_cast(zpointer* dst, zpointer* src, Klass* dst_klass) {\n+inline ZBarrierSet::OopCopyCheckStatus ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_copy_one_check_cast(zpointer* dst, zpointer* src, Klass* dst_klass) {\n@@ -340,0 +348,1 @@\n+  const bool null_check = HasDecorator<decorators, ARRAYCOPY_NOTNULL>::value;\n@@ -341,1 +350,4 @@\n-  if (!oopDesc::is_instanceof_or_null(to_oop(obj), dst_klass)) {\n+  if (null_check && is_null(obj)) {\n+    return oop_copy_check_null;\n+  }\n+  else if (!oopDesc::is_instanceof_or_null(to_oop(obj), dst_klass)) {\n@@ -343,1 +355,1 @@\n-    return false;\n+    return oop_copy_check_class_cast;\n@@ -348,1 +360,1 @@\n-  return true;\n+  return oop_copy_check_ok;\n@@ -351,1 +363,1 @@\n-inline bool ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap_check_cast(zpointer* dst, zpointer* src, size_t length, Klass* dst_klass) {\n+inline ZBarrierSet::OopCopyCheckStatus ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap_check_cast(zpointer* dst, zpointer* src, size_t length, Klass* dst_klass) {\n@@ -354,5 +366,3 @@\n-  for (const zpointer* const end = src + length; src < end; src++, dst++) {\n-    if (!oop_copy_one_check_cast(dst, src, dst_klass)) {\n-      \/\/ Check cast failed\n-      return false;\n-    }\n+  OopCopyCheckStatus check_status = oop_copy_check_ok;\n+  for (const zpointer* const end = src + length; (check_status == oop_copy_check_ok) && (src < end); src++, dst++) {\n+    check_status = oop_copy_one_check_cast(dst, src, dst_klass);\n@@ -360,2 +370,1 @@\n-\n-  return true;\n+  return check_status;\n@@ -365,1 +374,1 @@\n-inline bool ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap_no_check_cast(zpointer* dst, zpointer* src, size_t length) {\n+inline ZBarrierSet::OopCopyCheckStatus ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap_no_check_cast(zpointer* dst, zpointer* src, size_t length) {\n@@ -367,1 +376,1 @@\n-\n+  OopCopyCheckStatus check_status = oop_copy_check_ok;\n@@ -369,2 +378,2 @@\n-    for (const zpointer* const end = src + length; src < end; src++, dst++) {\n-      oop_copy_one(dst, src);\n+    for (const zpointer* const end = src + length; (check_status == oop_copy_check_ok) && (src < end); src++, dst++) {\n+      check_status = oop_copy_one(dst, src);\n@@ -372,1 +381,1 @@\n-    return true;\n+    return check_status;\n@@ -379,2 +388,2 @@\n-    for ( ; src >= end; src--, dst--) {\n-      oop_copy_one(dst, src);\n+    for ( ; (check_status == oop_copy_check_ok) && (src >= end); src--, dst--) {\n+      check_status = oop_copy_one(dst, src);\n@@ -382,1 +391,1 @@\n-    return true;\n+    return check_status;\n@@ -386,1 +395,1 @@\n-  return true;\n+  return check_status;\n@@ -390,1 +399,1 @@\n-inline bool ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, zpointer* src_raw,\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, zpointer* src_raw,\n@@ -395,0 +404,1 @@\n+  OopCopyCheckStatus check_status;\n@@ -398,1 +408,3 @@\n-    return oop_arraycopy_in_heap_check_cast(dst, src, length, dst_klass);\n+    check_status = oop_arraycopy_in_heap_check_cast(dst, src, length, dst_klass);\n+  } else {\n+    check_status = oop_arraycopy_in_heap_no_check_cast(dst, src, length);\n@@ -401,1 +413,13 @@\n-  return oop_arraycopy_in_heap_no_check_cast(dst, src, length);\n+  switch (check_status) {\n+  case oop_copy_check_ok:\n+    return;\n+  case oop_copy_check_class_cast:\n+    throw_array_store_exception(src_obj, dst_obj, JavaThread::current());\n+    break;\n+  case oop_copy_check_null:\n+    throw_array_null_pointer_store_exception(src_obj, dst_obj, JavaThread::current());\n+    break;\n+  default:\n+    ShouldNotReachHere();\n+    return;\n+  }\n@@ -408,1 +432,1 @@\n-  if (dst->is_objArray()) {\n+  if (dst->is_refArray()) {\n@@ -429,0 +453,49 @@\n+static inline void copy_primitive_payload(const void* src, const void* dst, const size_t payload_size_bytes, size_t& copied_bytes) {\n+  if (payload_size_bytes == 0) {\n+    return;\n+  }\n+  void* src_payload = (void*)(address(src) + copied_bytes);\n+  void* dst_payload = (void*)(address(dst) + copied_bytes);\n+  Copy::copy_value_content(src_payload, dst_payload, payload_size_bytes);\n+  copied_bytes += payload_size_bytes;\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::value_copy_in_heap(void* src, void* dst, InlineKlass* md, LayoutKind lk) {\n+  if (md->contains_oops()) {\n+    \/\/ Iterate over each oop map, performing:\n+    \/\/   1) possibly raw copy for any primitive payload before each map\n+    \/\/   2) load and store barrier for each oop\n+    \/\/   3) possibly raw copy for any primitive payload trailer\n+\n+    \/\/ src\/dst may not be oops, need offset to adjust oop map offset\n+    const address src_oop_addr_offset = ((address) src) - md->payload_offset();\n+    OopMapBlock* map = md->start_of_nonstatic_oop_maps();\n+    const OopMapBlock* const end = map + md->nonstatic_oop_map_count();\n+    size_t size_in_bytes = md->layout_size_in_bytes(lk);\n+    size_t copied_bytes = 0;\n+    while (map != end) {\n+      zpointer *src_p = (zpointer*)(src_oop_addr_offset + map->offset());\n+      const uintptr_t oop_offset = uintptr_t(src_p) - uintptr_t(src);\n+      zpointer *dst_p = (zpointer*)(uintptr_t(dst) + oop_offset);\n+\n+      \/\/ Copy any leading primitive payload before every cluster of oops\n+      assert(copied_bytes < oop_offset || copied_bytes == oop_offset, \"Negative sized leading payload segment\");\n+      copy_primitive_payload(src, dst, oop_offset - copied_bytes, copied_bytes);\n+\n+      \/\/ Copy a cluster of oops\n+      for (const zpointer* const src_end = src_p + map->count(); src_p < src_end; src_p++, dst_p++) {\n+        oop_copy_one(dst_p, src_p);\n+        copied_bytes += sizeof(zpointer);\n+      }\n+      map++;\n+    }\n+\n+    \/\/ Copy trailing primitive payload after potential oops\n+    assert(copied_bytes < size_in_bytes || copied_bytes == size_in_bytes, \"Negative sized trailing payload segment\");\n+    copy_primitive_payload(src, dst, size_in_bytes - copied_bytes, copied_bytes);\n+  } else {\n+    Raw::value_copy_in_heap(src, dst, md, lk);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSet.inline.hpp","additions":99,"deletions":26,"binary":false,"changes":125,"status":"modified"},{"patch":"@@ -465,1 +465,1 @@\n-  if (obj->is_objArray()) {\n+  if (obj->is_refArray()) {\n","filename":"src\/hotspot\/share\/gc\/z\/zHeapIterator.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -173,1 +173,1 @@\n-  return to_oop(addr)->is_objArray();\n+  return to_oop(addr)->is_refArray();\n","filename":"src\/hotspot\/share\/gc\/z\/zMark.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -53,1 +54,1 @@\n-  if (_word_size <= segment_max) {\n+  if (_word_size <= segment_max || ArrayKlass::cast(_klass)->is_flatArray_klass()) {\n@@ -69,1 +70,5 @@\n-    arrayOopDesc::set_mark(mem, markWord::prototype().set_marked());\n+    if (EnableValhalla) {\n+      arrayOopDesc::set_mark(mem, _klass->prototype_header().set_marked());\n+    } else {\n+      arrayOopDesc::set_mark(mem, markWord::prototype().set_marked());\n+    }\n@@ -159,1 +164,1 @@\n-  if (UseCompactObjectHeaders) {\n+  if (UseCompactObjectHeaders || EnableValhalla) {\n","filename":"src\/hotspot\/share\/gc\/z\/zObjArrayAllocator.cpp","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -224,1 +224,1 @@\n-  nonstatic_field(InstanceKlass,               _misc_flags._flags,                            u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _misc_flags._flags,                            u4)                                    \\\n@@ -738,0 +738,3 @@\n+  declare_constant(DataLayout::array_store_data_tag)                      \\\n+  declare_constant(DataLayout::array_load_data_tag)                       \\\n+  declare_constant(DataLayout::acmp_data_tag)                             \\\n@@ -818,1 +821,1 @@\n-  declare_constant(Klass::_lh_array_tag_obj_value)                        \\\n+  declare_constant(Klass::_lh_array_tag_ref_value)                        \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -57,0 +57,2 @@\n+  virtual void do_oop_no_buffering(oop* o) { do_oop(o); }\n+  virtual void do_oop_no_buffering(narrowOop* o) { do_oop(o); }\n@@ -139,0 +141,5 @@\n+class BufferedValueClosure : public Closure {\n+public:\n+  virtual void do_buffered_value(oop* p) = 0;\n+};\n+\n","filename":"src\/hotspot\/share\/memory\/iterator.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"oops\/flatArrayKlass.inline.hpp\"\n@@ -41,0 +42,1 @@\n+#include \"oops\/refArrayKlass.inline.hpp\"\n@@ -145,0 +147,1 @@\n+      set_init_function<InlineKlass>();\n@@ -151,0 +154,2 @@\n+      set_init_function<FlatArrayKlass>();\n+      set_init_function<RefArrayKlass>();\n@@ -208,0 +213,1 @@\n+      set_init_function<InlineKlass>();\n@@ -214,0 +220,2 @@\n+      set_init_function<FlatArrayKlass>();\n+      set_init_function<RefArrayKlass>();\n@@ -271,0 +279,1 @@\n+      set_init_function<InlineKlass>();\n@@ -277,0 +286,2 @@\n+      set_init_function<FlatArrayKlass>();\n+      set_init_function<RefArrayKlass>();\n","filename":"src\/hotspot\/share\/memory\/iterator.inline.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -70,0 +70,1 @@\n+#include \"oops\/refArrayKlass.hpp\"\n@@ -117,0 +118,2 @@\n+static LatestMethodCache _is_substitutable_cache;           \/\/ ValueObjectMethods.isSubstitutable()\n+static LatestMethodCache _value_object_hash_code_cache;     \/\/ ValueObjectMethods.valueObjectHashCode()\n@@ -462,0 +465,1 @@\n+\n@@ -509,2 +513,6 @@\n-    Klass* oak = vmClasses::Object_klass()->array_klass(CHECK);\n-    _objectArrayKlass = ObjArrayKlass::cast(oak);\n+    ArrayKlass* oak = vmClasses::Object_klass()->array_klass(CHECK);\n+    oak->append_to_sibling_list();\n+\n+    \/\/ Create a RefArrayKlass (which is the default) and initialize.\n+    ObjArrayKlass* rak = ObjArrayKlass::cast(oak)->klass_with_properties(ArrayKlass::ArrayProperties::DEFAULT, THREAD);\n+    _objectArrayKlass = rak;\n@@ -512,7 +520,0 @@\n-  \/\/ OLD\n-  \/\/ Add the class to the class hierarchy manually to make sure that\n-  \/\/ its vtable is initialized after core bootstrapping is completed.\n-  \/\/ ---\n-  \/\/ New\n-  \/\/ Have already been initialized.\n-  _objectArrayKlass->append_to_sibling_list();\n@@ -655,0 +656,3 @@\n+\n+  \/\/ This isn't added to the subclass list, so need to reinitialize vtables directly.\n+  Universe::objectArrayKlass()->vtable().initialize_vtable();\n@@ -900,1 +904,0 @@\n-\n@@ -1064,0 +1067,2 @@\n+Method* Universe::is_substitutable_method()       { return _is_substitutable_cache.get_method(); }\n+Method* Universe::value_object_hash_code_method() { return _value_object_hash_code_cache.get_method(); }\n@@ -1093,0 +1098,11 @@\n+\n+  \/\/ Set up substitutability testing\n+  ResourceMark rm(current);\n+  _is_substitutable_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::isSubstitutable_name()->as_C_string(),\n+                          vmSymbols::object_object_boolean_signature(), true);\n+  _value_object_hash_code_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::valueObjectHashCode_name()->as_C_string(),\n+                          vmSymbols::object_int_signature(), true);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":26,"deletions":10,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -119,0 +119,1 @@\n+\n@@ -258,0 +259,3 @@\n+  static Method*      is_substitutable_method();\n+  static Method*      value_object_hash_code_method();\n+\n","filename":"src\/hotspot\/share\/memory\/universe.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -40,0 +41,1 @@\n+#include \"oops\/objArrayKlass.hpp\"\n@@ -42,0 +44,1 @@\n+#include \"oops\/refArrayKlass.hpp\"\n@@ -91,2 +94,2 @@\n-ArrayKlass::ArrayKlass(Symbol* name, KlassKind kind) :\n-  Klass(kind),\n+ArrayKlass::ArrayKlass(Symbol* name, KlassKind kind, ArrayProperties props, markWord prototype_header) :\n+Klass(kind, prototype_header),\n@@ -94,0 +97,1 @@\n+  _properties(props),\n@@ -107,0 +111,19 @@\n+Symbol* ArrayKlass::create_element_klass_array_name(Klass* element_klass, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  Symbol* name = nullptr;\n+  char *name_str = element_klass->name()->as_C_string();\n+  int len = element_klass->name()->utf8_length();\n+  char *new_str = NEW_RESOURCE_ARRAY(char, len + 4);\n+  int idx = 0;\n+  new_str[idx++] = JVM_SIGNATURE_ARRAY;\n+  if (element_klass->is_instance_klass()) { \/\/ it could be an array or simple type\n+    new_str[idx++] = JVM_SIGNATURE_CLASS;\n+  }\n+  memcpy(&new_str[idx], name_str, len * sizeof(char));\n+  idx += len;\n+  if (element_klass->is_instance_klass()) {\n+    new_str[idx++] = JVM_SIGNATURE_ENDCLASS;\n+  }\n+  new_str[idx++] = '\\0';\n+  return SymbolTable::new_symbol(new_str);\n+}\n@@ -120,1 +143,10 @@\n-  java_lang_Class::create_mirror(k, Handle(THREAD, k->class_loader()), Handle(THREAD, module_oop), Handle(), Handle(), CHECK);\n+\n+  if (k->is_refArray_klass() || k->is_flatArray_klass()) {\n+    assert(super_klass != nullptr, \"Must be\");\n+    assert(k->super() != nullptr, \"Must be\");\n+    assert(k->super() == super_klass, \"Must be\");\n+    Handle mirror(THREAD, super_klass->java_mirror());\n+    k->set_java_mirror(mirror);\n+  } else {\n+    java_lang_Class::create_mirror(k, Handle(THREAD, k->class_loader()), Handle(THREAD, module_oop), Handle(), Handle(), CHECK);\n+  }\n@@ -137,2 +169,1 @@\n-      ObjArrayKlass* ak =\n-          ObjArrayKlass::allocate_objArray_klass(class_loader_data(), dim + 1, this, CHECK_NULL);\n+      ObjArrayKlass* ak = RefArrayKlass::allocate_objArray_klass(class_loader_data(), dim + 1, this, CHECK_NULL);\n@@ -186,0 +217,4 @@\n+oop ArrayKlass::component_mirror() const {\n+  return java_lang_Class::component_mirror(java_mirror());\n+}\n+\n@@ -225,1 +260,1 @@\n-    ArrayKlass *ak = higher_dimension();\n+    ObjArrayKlass *ak = higher_dimension();\n","filename":"src\/hotspot\/share\/oops\/arrayKlass.cpp","additions":41,"deletions":6,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -38,0 +38,15 @@\n+\n+ public:\n+  enum ArrayProperties : uint32_t {\n+    DEFAULT         = 0,\n+    NULL_RESTRICTED = 1 << 0,\n+    NON_ATOMIC      = 1 << 1,\n+    \/\/ FINAL           = 1 << 2,\n+    \/\/ VOLATILE        = 1 << 3\n+    INVALID         = 1 << 4,\n+    DUMMY           = 1 << 5      \/\/ Just to transition the code, to be removed ASAP\n+  };\n+\n+  static bool is_null_restricted(ArrayProperties props) { return (props & NULL_RESTRICTED) != 0; }\n+  static bool is_non_atomic(ArrayProperties props) { return (props & NON_ATOMIC) != 0; }\n+\n@@ -42,0 +57,1 @@\n+  ArrayProperties _properties;\n@@ -49,1 +65,1 @@\n-  ArrayKlass(Symbol* name, KlassKind kind);\n+  ArrayKlass(Symbol* name, KlassKind kind, ArrayProperties props, markWord prototype_header = markWord::prototype());\n@@ -52,0 +68,4 @@\n+  \/\/ Create array_name for element klass\n+  static Symbol* create_element_klass_array_name(Klass* element_klass, TRAPS);\n+\n+\n@@ -68,0 +88,3 @@\n+  ArrayProperties properties() const { return _properties; }\n+  void set_properties(ArrayProperties props) { _properties = props; }\n+\n@@ -110,0 +133,2 @@\n+  oop component_mirror() const;\n+\n@@ -142,0 +167,8 @@\n+class ArrayDescription : public StackObj {\n+  public:\n+   Klass::KlassKind _kind;\n+   ArrayKlass::ArrayProperties _properties;\n+   LayoutKind _layout_kind;\n+   ArrayDescription(Klass::KlassKind k, ArrayKlass::ArrayProperties p, LayoutKind lk) { _kind = k; _properties = p; _layout_kind = lk; }\n+ };\n+\n","filename":"src\/hotspot\/share\/oops\/arrayKlass.hpp","additions":34,"deletions":1,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -66,0 +66,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -72,0 +73,1 @@\n+#include \"oops\/markWord.hpp\"\n@@ -75,0 +77,1 @@\n+#include \"oops\/refArrayKlass.hpp\"\n@@ -152,0 +155,5 @@\n+void InlineLayoutInfo::metaspace_pointers_do(MetaspaceClosure* it) {\n+  log_trace(cds)(\"Iter(InlineFieldInfo): %p\", this);\n+  it->push(&_klass);\n+}\n+\n@@ -173,0 +181,13 @@\n+bool InstanceKlass::field_is_null_free_inline_type(int index) const {\n+  return field(index).field_flags().is_null_free_inline_type();\n+}\n+\n+bool InstanceKlass::is_class_in_loadable_descriptors_attribute(Symbol* name) const {\n+  if (_loadable_descriptors == nullptr) return false;\n+  for (int i = 0; i < _loadable_descriptors->length(); i++) {\n+        Symbol* class_name = _constants->symbol_at(_loadable_descriptors->at(i));\n+        if (class_name == name) return true;\n+  }\n+  return false;\n+}\n+\n@@ -462,1 +483,2 @@\n-                                       parser.is_interface());\n+                                       parser.is_interface(),\n+                                       parser.is_inline_type());\n@@ -484,0 +506,3 @@\n+  } else if (parser.is_inline_type()) {\n+    \/\/ inline type\n+    ik = new (loader_data, size, THREAD) InlineKlass(parser);\n@@ -500,0 +525,6 @@\n+#ifdef ASSERT\n+  ik->bounds_check((address) ik->start_of_vtable(), false, size);\n+  ik->bounds_check((address) ik->start_of_itable(), false, size);\n+  ik->bounds_check((address) ik->end_of_itable(), true, size);\n+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);\n+#endif \/\/ASSERT\n@@ -503,0 +534,23 @@\n+#ifndef PRODUCT\n+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {\n+  const char* bad = nullptr;\n+  address end = nullptr;\n+  if (addr < (address)this) {\n+    bad = \"before\";\n+  } else if (addr == (address)this) {\n+    if (edge_ok)  return true;\n+    bad = \"just before\";\n+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {\n+    if (edge_ok)  return true;\n+    bad = \"just after\";\n+  } else if (addr > end) {\n+    bad = \"after\";\n+  } else {\n+    return true;\n+  }\n+  tty->print_cr(\"%s object bounds: \" INTPTR_FORMAT \" [\" INTPTR_FORMAT \"..\" INTPTR_FORMAT \"]\",\n+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);\n+  Verbose = WizardMode = true; this->print(); \/\/@@\n+  return false;\n+}\n+#endif \/\/PRODUCT\n@@ -530,2 +584,2 @@\n-InstanceKlass::InstanceKlass(const ClassFileParser& parser, KlassKind kind, ReferenceType reference_type) :\n-  Klass(kind),\n+InstanceKlass::InstanceKlass(const ClassFileParser& parser, KlassKind kind, markWord prototype_header, ReferenceType reference_type) :\n+  Klass(kind, prototype_header),\n@@ -542,1 +596,4 @@\n-  _init_thread(nullptr)\n+  _init_thread(nullptr),\n+  _inline_layout_info_array(nullptr),\n+  _loadable_descriptors(nullptr),\n+  _adr_inlineklass_fixed_block(nullptr)\n@@ -549,0 +606,3 @@\n+  if (parser.has_inline_fields()) {\n+    set_has_inline_type_fields();\n+  }\n@@ -693,0 +753,5 @@\n+  if (inline_layout_info_array() != nullptr) {\n+    MetadataFactory::free_array<InlineLayoutInfo>(loader_data, inline_layout_info_array());\n+  }\n+  set_inline_layout_info_array(nullptr);\n+\n@@ -727,0 +792,7 @@\n+  if (loadable_descriptors() != nullptr &&\n+      loadable_descriptors() != Universe::the_empty_short_array() &&\n+      !loadable_descriptors()->in_aot_cache()) {\n+    MetadataFactory::free_array<jushort>(loader_data, loadable_descriptors());\n+  }\n+  set_loadable_descriptors(nullptr);\n+\n@@ -895,0 +967,38 @@\n+static void load_classes_from_loadable_descriptors_attribute(InstanceKlass *ik, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  if (ik->loadable_descriptors() != nullptr && PreloadClasses) {\n+    HandleMark hm(THREAD);\n+    for (int i = 0; i < ik->loadable_descriptors()->length(); i++) {\n+      Symbol* sig = ik->constants()->symbol_at(ik->loadable_descriptors()->at(i));\n+      if (!Signature::has_envelope(sig)) continue;\n+      TempNewSymbol class_name = Signature::strip_envelope(sig);\n+      if (class_name == ik->name()) continue;\n+      log_info(class, preload)(\"Preloading of class %s during linking of class %s \"\n+                               \"because of the class is listed in the LoadableDescriptors attribute\",\n+                               sig->as_C_string(), ik->name()->as_C_string());\n+      oop loader = ik->class_loader();\n+      Klass* klass = SystemDictionary::resolve_or_null(class_name,\n+                                                        Handle(THREAD, loader), THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        CLEAR_PENDING_EXCEPTION;\n+      }\n+      if (klass != nullptr) {\n+        log_info(class, preload)(\"Preloading of class %s during linking of class %s \"\n+                                 \"(cause: LoadableDescriptors attribute) succeeded\",\n+                                 class_name->as_C_string(), ik->name()->as_C_string());\n+        if (!klass->is_inline_klass()) {\n+          \/\/ Non value class are allowed by the current spec, but it could be an indication\n+          \/\/ of an issue so let's log a warning\n+          log_warning(class, preload)(\"Preloading of class %s during linking of class %s \"\n+                                      \"(cause: LoadableDescriptors attribute) but loaded class is not a value class\",\n+                                      class_name->as_C_string(), ik->name()->as_C_string());\n+        }\n+      } else {\n+        log_warning(class, preload)(\"Preloading of class %s during linking of class %s \"\n+                                    \"(cause: LoadableDescriptors attribute) failed\",\n+                                    class_name->as_C_string(), ik->name()->as_C_string());\n+      }\n+    }\n+  }\n+}\n+\n@@ -965,0 +1075,7 @@\n+  if (EnableValhalla) {\n+    \/\/ Aggressively preloading all classes from the LoadableDescriptors attribute\n+    \/\/ so inline classes can be scalarized in the calling conventions computed below\n+    load_classes_from_loadable_descriptors_attribute(this, THREAD);\n+    assert(!HAS_PENDING_EXCEPTION, \"Shouldn't have pending exceptions from call above\");\n+  }\n+\n@@ -1269,0 +1386,21 @@\n+  \/\/ Pre-allocating an all-zero value to be used to reset nullable flat storages\n+  if (is_inline_klass()) {\n+      InlineKlass* vk = InlineKlass::cast(this);\n+      if (vk->has_nullable_atomic_layout()) {\n+        oop val = vk->allocate_instance(THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+            Handle e(THREAD, PENDING_EXCEPTION);\n+            CLEAR_PENDING_EXCEPTION;\n+            {\n+                EXCEPTION_MARK;\n+                add_initialization_error(THREAD, e);\n+                \/\/ Locks object, set state, and notify all waiting threads\n+                set_initialization_state_and_notify(initialization_error, THREAD);\n+                CLEAR_PENDING_EXCEPTION;\n+            }\n+            THROW_OOP(e());\n+        }\n+        vk->set_null_reset_value(val);\n+      }\n+  }\n+\n@@ -1301,1 +1439,0 @@\n-\n@@ -1322,0 +1459,30 @@\n+\n+    if (has_strict_static_fields() && !HAS_PENDING_EXCEPTION) {\n+      \/\/ Step 9 also verifies that strict static fields have been initialized.\n+      \/\/ Status bits were set in ClassFileParser::post_process_parsed_stream.\n+      \/\/ After <clinit>, bits must all be clear, or else we must throw an error.\n+      \/\/ This is an extremely fast check, so we won't bother with a timer.\n+      assert(fields_status() != nullptr, \"\");\n+      Symbol* bad_strict_static = nullptr;\n+      for (int index = 0; index < fields_status()->length(); index++) {\n+        \/\/ Very fast loop over single byte array looking for a set bit.\n+        if (fields_status()->adr_at(index)->is_strict_static_unset()) {\n+          \/\/ This strict static field has not been set by the class initializer.\n+          \/\/ Note that in the common no-error case, we read no field metadata.\n+          \/\/ We only unpack it when we need to report an error.\n+          FieldInfo fi = field(index);\n+          bad_strict_static = fi.name(constants());\n+          if (debug_logging_enabled) {\n+            ResourceMark rm(jt);\n+            const char* msg = format_strict_static_message(bad_strict_static);\n+            log_debug(class, init)(\"%s\", msg);\n+          } else {\n+            \/\/ If we are not logging, do not bother to look for a second offense.\n+            break;\n+          }\n+        }\n+      }\n+      if (bad_strict_static != nullptr) {\n+        throw_strict_static_exception(bad_strict_static, \"is unset after initialization of\", THREAD);\n+      }\n+    }\n@@ -1375,0 +1542,68 @@\n+void InstanceKlass::notify_strict_static_access(int field_index, bool is_writing, TRAPS) {\n+  guarantee(field_index >= 0 && field_index < fields_status()->length(), \"valid field index\");\n+  DEBUG_ONLY(FieldInfo debugfi = field(field_index));\n+  assert(debugfi.access_flags().is_strict(), \"\");\n+  assert(debugfi.access_flags().is_static(), \"\");\n+  FieldStatus& fs = *fields_status()->adr_at(field_index);\n+  LogTarget(Trace, class, init) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm(THREAD);\n+    LogStream ls(lt);\n+    FieldInfo fi = field(field_index);\n+    ls.print(\"notify %s %s %s%s \",\n+             external_name(), is_writing? \"Write\" : \"Read\",\n+             fs.is_strict_static_unset() ? \"Unset\" : \"(set)\",\n+             fs.is_strict_static_unread() ? \"+Unread\" : \"\");\n+    fi.print(&ls, constants());\n+  }\n+  if (fs.is_strict_static_unset()) {\n+    assert(fs.is_strict_static_unread(), \"ClassFileParser resp.\");\n+    \/\/ If it is not set, there are only two reasonable things we can do here:\n+    \/\/ - mark it set if this is putstatic\n+    \/\/ - throw an error (Read-Before-Write) if this is getstatic\n+\n+    \/\/ The unset state is (or should be) transient, and observable only in one\n+    \/\/ thread during the execution of <clinit>.  Something is wrong here as this\n+    \/\/ should not be possible\n+    guarantee(is_reentrant_initialization(THREAD), \"unscoped access to strict static\");\n+    if (is_writing) {\n+      \/\/ clear the \"unset\" bit, since the field is actually going to be written\n+      fs.update_strict_static_unset(false);\n+    } else {\n+      \/\/ throw an IllegalStateException, since we are reading before writing\n+      \/\/ see also InstanceKlass::initialize_impl, Step 8 (at end)\n+      Symbol* bad_strict_static = field(field_index).name(constants());\n+      throw_strict_static_exception(bad_strict_static, \"is unset before first read in\", CHECK);\n+    }\n+  } else {\n+    \/\/ Ensure no write after read for final strict statics\n+    FieldInfo fi = field(field_index);\n+    bool is_final = fi.access_flags().is_final();\n+    if (is_final) {\n+      \/\/ no final write after read, so observing a constant freezes it, as if <clinit> ended early\n+      \/\/ (maybe we could trust the constant a little earlier, before <clinit> ends)\n+      if (is_writing && !fs.is_strict_static_unread()) {\n+        Symbol* bad_strict_static = fi.name(constants());\n+        throw_strict_static_exception(bad_strict_static, \"is set after read (as final) in\", CHECK);\n+      } else if (!is_writing && fs.is_strict_static_unread()) {\n+        fs.update_strict_static_unread(false);\n+      }\n+    }\n+  }\n+}\n+\n+void InstanceKlass::throw_strict_static_exception(Symbol* field_name, const char* when, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  const char* msg = format_strict_static_message(field_name, when);\n+  THROW_MSG(vmSymbols::java_lang_IllegalStateException(), msg);\n+}\n+\n+const char* InstanceKlass::format_strict_static_message(Symbol* field_name, const char* when) {\n+  stringStream ss;\n+  ss.print(\"Strict static \\\"%s\\\" %s %s\",\n+           field_name->as_C_string(),\n+           when == nullptr ? \"is unset in\" : when,\n+           external_name());\n+  return ss.as_string();\n+}\n+\n@@ -1625,1 +1860,1 @@\n-  ObjArrayKlass* ak = array_klasses();\n+  ArrayKlass* ak = array_klasses();\n@@ -1632,2 +1867,2 @@\n-  ObjArrayKlass* oak = array_klasses_acquire();\n-  if (oak == nullptr) {\n+  ArrayKlass* ak = array_klasses_acquire();\n+  if (ak == nullptr) {\n@@ -1636,1 +1871,1 @@\n-    return oak->array_klass_or_null(n);\n+    return ak->array_klass_or_null(n);\n@@ -1653,1 +1888,1 @@\n-  if (clinit != nullptr && clinit->has_valid_initializer_flags()) {\n+  if (clinit != nullptr && clinit->is_class_initializer()) {\n@@ -1762,4 +1997,0 @@\n-bool InstanceKlass::contains_field_offset(int offset) {\n-  fieldDescriptor fd;\n-  return find_field_from_offset(offset, false, &fd);\n-}\n@@ -1846,0 +2077,9 @@\n+bool InstanceKlass::contains_field_offset(int offset) {\n+  if (this->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(this);\n+    return offset >= vk->payload_offset() && offset < (vk->payload_offset() + vk->payload_size_in_bytes());\n+  } else {\n+    fieldDescriptor fd;\n+    return find_field_from_offset(offset, false, &fd);\n+  }\n+}\n@@ -2229,0 +2469,3 @@\n+    if (name == vmSymbols::object_initializer_name()) {\n+      break;  \/\/ <init> is never inherited\n+    }\n@@ -2641,0 +2884,1 @@\n+  it->push(&_loadable_descriptors);\n@@ -2642,0 +2886,1 @@\n+  it->push(&_inline_layout_info_array, MetaspaceClosure::_writable);\n@@ -2689,1 +2934,1 @@\n-  \/\/ These are not allocated from metaspace. They are safe to set to null.\n+  \/\/ These are not allocated from metaspace. They are safe to set to nullptr.\n@@ -2780,0 +3025,4 @@\n+  if (is_inline_klass()) {\n+    InlineKlass::cast(this)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -2813,1 +3062,1 @@\n-    assert(this == array_klasses()->bottom_klass(), \"sanity\");\n+    assert(this == ObjArrayKlass::cast(array_klasses())->bottom_klass(), \"sanity\");\n@@ -2816,0 +3065,6 @@\n+    if (class_loader_data() == nullptr) {\n+      ResourceMark rm(THREAD);\n+      log_debug(cds)(\"  loader_data %s \", loader_data == nullptr ? \"nullptr\" : \"non null\");\n+      log_debug(cds)(\"  this %s array_klasses %s \", this->name()->as_C_string(), array_klasses()->name()->as_C_string());\n+    }\n+    assert(!array_klasses()->is_refined_objArray_klass(), \"must be non-refined objarrayklass\");\n@@ -2971,0 +3226,4 @@\n+bool InstanceKlass::supports_inline_types() const {\n+  return major_version() >= Verifier::VALUE_TYPES_MAJOR_VERSION && minor_version() == Verifier::JAVA_PREVIEW_MINOR_VERSION;\n+}\n+\n@@ -3003,0 +3262,2 @@\n+  return signature_name_of_carrier(JVM_SIGNATURE_CLASS);\n+}\n@@ -3004,0 +3265,1 @@\n+const char* InstanceKlass::signature_name_of_carrier(char c) const {\n@@ -3010,1 +3272,1 @@\n-  \/\/ Add L as type indicator\n+  \/\/ Add L or Q as type indicator\n@@ -3012,1 +3274,1 @@\n-  dest[dest_index++] = JVM_SIGNATURE_CLASS;\n+  dest[dest_index++] = c;\n@@ -3293,0 +3555,19 @@\n+void InstanceKlass::check_can_be_annotated_with_NullRestricted(InstanceKlass* type, Symbol* container_klass_name, TRAPS) {\n+  assert(type->is_instance_klass(), \"Sanity check\");\n+  if (type->is_identity_class()) {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+              err_msg(\"Class %s expects class %s to be a value class, but it is an identity class\",\n+              container_klass_name->as_C_string(),\n+              type->external_name()));\n+  }\n+\n+  if (type->is_abstract()) {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+              err_msg(\"Class %s expects class %s to be concrete value type, but it is an abstract class\",\n+              container_klass_name->as_C_string(),\n+              type->external_name()));\n+  }\n+}\n+\n@@ -3359,2 +3640,1 @@\n-  \/\/ Remember to strip ACC_SUPER bit\n-  return (access & (~JVM_ACC_SUPER));\n+  return access;\n@@ -3614,1 +3894,4 @@\n-static void print_vtable(intptr_t* start, int len, outputStream* st) {\n+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {\n+  ResourceMark rm;\n+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);\n+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;\n@@ -3618,0 +3901,5 @@\n+    if (forward_refs[i] != 0) {\n+      int from = forward_refs[i];\n+      int off = (int) start[from];\n+      st->print(\" (offset %d <= [%d])\", off, from);\n+    }\n@@ -3621,0 +3909,6 @@\n+    } else if (self != nullptr && e > 0 && e < 0x10000) {\n+      address location = self + e;\n+      int index = (int)((intptr_t*)location - start);\n+      st->print(\" (offset %d => [%d])\", (int)e, index);\n+      if (index >= 0 && index < len)\n+        forward_refs[index] = i;\n@@ -3627,1 +3921,22 @@\n-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);\n+  return print_vtable(nullptr, reinterpret_cast<intptr_t*>(start), len, st);\n+}\n+\n+template<typename T>\n+ static void print_array_on(outputStream* st, Array<T>* array) {\n+   if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+   array->print_value_on(st); st->cr();\n+   if (Verbose || WizardMode) {\n+     for (int i = 0; i < array->length(); i++) {\n+       st->print(\"%d : \", i); array->at(i)->print_value_on(st); st->cr();\n+     }\n+   }\n+ }\n+\n+static void print_array_on(outputStream* st, Array<int>* array) {\n+  if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+  array->print_value_on(st); st->cr();\n+  if (Verbose || WizardMode) {\n+    for (int i = 0; i < array->length(); i++) {\n+      st->print(\"%d : %d\", i, array->at(i)); st->cr();\n+    }\n+  }\n@@ -3668,8 +3983,2 @@\n-  st->print(BULLET\"methods:           \"); methods()->print_value_on(st);               st->cr();\n-  if (Verbose || WizardMode) {\n-    Array<Method*>* method_array = methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n-  st->print(BULLET\"method ordering:   \"); method_ordering()->print_value_on(st);      st->cr();\n+  st->print(BULLET\"methods:           \"); print_array_on(st, methods());\n+  st->print(BULLET\"method ordering:   \"); print_array_on(st, method_ordering());\n@@ -3677,7 +3986,1 @@\n-    st->print(BULLET\"default_methods:   \"); default_methods()->print_value_on(st);    st->cr();\n-    if (Verbose) {\n-      Array<Method*>* method_array = default_methods();\n-      for (int i = 0; i < method_array->length(); i++) {\n-        st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-      }\n-    }\n+    st->print(BULLET\"default_methods:   \"); print_array_on(st, default_methods());\n@@ -3743,0 +4046,1 @@\n+  st->print(BULLET\"loadable descriptors:     \"); loadable_descriptors()->print_value_on(st); st->cr();\n@@ -3753,1 +4057,1 @@\n-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);\n+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(nullptr, start_of_itable(), itable_length(), st);\n@@ -3785,0 +4089,1 @@\n+  for (int i = 0; i < _indent; i++) _st->print(\"  \");\n@@ -3787,1 +4092,1 @@\n-     fd->print_on(_st);\n+     fd->print_on(_st, _base_offset);\n@@ -3790,2 +4095,2 @@\n-     fd->print_on_for(_st, _obj);\n-     _st->cr();\n+     fd->print_on_for(_st, _obj, _indent, _base_offset);\n+     if (!fd->field_flags().is_flat()) _st->cr();\n@@ -3796,1 +4101,1 @@\n-void InstanceKlass::oop_print_on(oop obj, outputStream* st) {\n+void InstanceKlass::oop_print_on(oop obj, outputStream* st, int indent, int base_offset) {\n@@ -3812,1 +4117,1 @@\n-  FieldPrinter print_field(st, obj);\n+  FieldPrinter print_field(st, obj, indent, base_offset);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":348,"deletions":43,"binary":false,"changes":391,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"code\/vmreg.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"oops\/refArrayKlass.hpp\"\n@@ -59,0 +61,1 @@\n+\/\/    [EMBEDDED InlineKlassFixedBlock] only if is an InlineKlass instance\n@@ -74,0 +77,1 @@\n+class BufferedInlineTypeBlob;\n@@ -89,0 +93,2 @@\n+   int _indent;\n+   int _base_offset;\n@@ -90,1 +96,2 @@\n-   FieldPrinter(outputStream* st, oop obj = nullptr) : _obj(obj), _st(st) {}\n+   FieldPrinter(outputStream* st, oop obj = nullptr, int indent = 0, int base_offset = 0) :\n+                 _obj(obj), _st(st), _indent(indent), _base_offset(base_offset) {}\n@@ -134,0 +141,54 @@\n+class SigEntry;\n+\n+class InlineKlassFixedBlock {\n+  Array<SigEntry>** _extended_sig;\n+  Array<VMRegPair>** _return_regs;\n+  address* _pack_handler;\n+  address* _pack_handler_jobject;\n+  address* _unpack_handler;\n+  int* _null_reset_value_offset;\n+  int _payload_offset;          \/\/ offset of the begining of the payload in a heap buffered instance\n+  int _payload_size_in_bytes;   \/\/ size of payload layout\n+  int _payload_alignment;       \/\/ alignment required for payload\n+  int _non_atomic_size_in_bytes; \/\/ size of null-free non-atomic flat layout\n+  int _non_atomic_alignment;    \/\/ alignment requirement for null-free non-atomic layout\n+  int _atomic_size_in_bytes;    \/\/ size and alignment requirement for a null-free atomic layout, -1 if no atomic flat layout is possible\n+  int _nullable_size_in_bytes;  \/\/ size and alignment requirement for a nullable layout (always atomic), -1 if no nullable flat layout is possible\n+  int _null_marker_offset;      \/\/ expressed as an offset from the beginning of the object for a heap buffered value\n+                                \/\/ payload_offset must be subtracted to get the offset from the beginning of the payload\n+\n+  friend class InlineKlass;\n+};\n+\n+class InlineLayoutInfo : public MetaspaceObj {\n+  InlineKlass* _klass;\n+  LayoutKind _kind;\n+  int _null_marker_offset; \/\/ null marker offset for this field, relative to the beginning of the current container\n+\n+ public:\n+  InlineLayoutInfo(): _klass(nullptr), _kind(LayoutKind::UNKNOWN), _null_marker_offset(-1)  {}\n+  InlineLayoutInfo(InlineKlass* ik, LayoutKind kind, int size, int nm_offset):\n+    _klass(ik), _kind(kind), _null_marker_offset(nm_offset) {}\n+\n+  InlineKlass* klass() const { return _klass; }\n+  void set_klass(InlineKlass* k) { _klass = k; }\n+\n+  LayoutKind kind() const {\n+    assert(_kind != LayoutKind::UNKNOWN, \"Not set\");\n+    return _kind;\n+  }\n+  void set_kind(LayoutKind lk) { _kind = lk; }\n+\n+  int null_marker_offset() const {\n+    assert(_null_marker_offset != -1, \"Not set\");\n+    return _null_marker_offset;\n+  }\n+  void set_null_marker_offset(int o) { _null_marker_offset = o; }\n+\n+  void metaspace_pointers_do(MetaspaceClosure* it);\n+  MetaspaceObj::Type type() const { return InlineLayoutInfoType; }\n+\n+  static ByteSize klass_offset() { return in_ByteSize(offset_of(InlineLayoutInfo, _klass)); }\n+  static ByteSize null_marker_offset_offset() { return in_ByteSize(offset_of(InlineLayoutInfo, _null_marker_offset)); }\n+};\n+\n@@ -139,0 +200,1 @@\n+  friend class TemplateTable;\n@@ -144,1 +206,1 @@\n-  InstanceKlass(const ClassFileParser& parser, KlassKind kind = Kind, ReferenceType reference_type = REF_NONE);\n+  InstanceKlass(const ClassFileParser& parser, KlassKind kind = Kind, markWord prototype = markWord::prototype(), ReferenceType reference_type = REF_NONE);\n@@ -280,0 +342,4 @@\n+  Array<InlineLayoutInfo>* _inline_layout_info_array;\n+  Array<u2>* _loadable_descriptors;\n+  const InlineKlassFixedBlock* _adr_inlineklass_fixed_block;\n+\n@@ -328,0 +394,15 @@\n+  bool has_inline_type_fields() const { return _misc_flags.has_inline_type_fields(); }\n+  void set_has_inline_type_fields()   { _misc_flags.set_has_inline_type_fields(true); }\n+\n+  bool is_naturally_atomic() const  { return _misc_flags.is_naturally_atomic(); }\n+  void set_is_naturally_atomic()    { _misc_flags.set_is_naturally_atomic(true); }\n+\n+  \/\/ Query if this class has atomicity requirements (default is yes)\n+  \/\/ This bit can occur anywhere, but is only significant\n+  \/\/ for inline classes *and* their super types.\n+  \/\/ It inherits from supers.\n+  \/\/ Its value depends on the ForceNonTearable VM option, the LooselyConsistentValue annotation\n+  \/\/ and the presence of flat fields with atomicity requirements\n+  bool must_be_atomic() const { return _misc_flags.must_be_atomic(); }\n+  void set_must_be_atomic()   { _misc_flags.set_must_be_atomic(true); }\n+\n@@ -392,0 +473,6 @@\n+  bool field_is_flat(int index) const { return field_flags(index).is_flat(); }\n+  bool field_has_null_marker(int index) const { return field_flags(index).has_null_marker(); }\n+  bool field_is_null_free_inline_type(int index) const;\n+  bool is_class_in_loadable_descriptors_attribute(Symbol* name) const;\n+\n+  int null_marker_offset(int index) const { return inline_layout_info(index).null_marker_offset(); }\n@@ -406,0 +493,3 @@\n+  Array<u2>* loadable_descriptors() const { return _loadable_descriptors; }\n+  void set_loadable_descriptors(Array<u2>* c) { _loadable_descriptors = c; }\n+\n@@ -501,0 +591,3 @@\n+  \/\/ Check if this klass can be null-free\n+  static void check_can_be_annotated_with_NullRestricted(InstanceKlass* type, Symbol* container_klass_name, TRAPS);\n+\n@@ -536,0 +629,3 @@\n+  static ByteSize kind_offset() { return in_ByteSize(offset_of(InstanceKlass, _kind)); }\n+  static ByteSize misc_flags_offset() { return in_ByteSize(offset_of(InstanceKlass, _misc_flags)); }\n+\n@@ -663,0 +759,2 @@\n+  bool supports_inline_types() const;\n+\n@@ -769,0 +867,7 @@\n+  \/\/ runtime support for strict statics\n+  bool has_strict_static_fields() const     { return _misc_flags.has_strict_static_fields(); }\n+  void set_has_strict_static_fields(bool b) { _misc_flags.set_has_strict_static_fields(b); }\n+  void notify_strict_static_access(int field_index, bool is_writing, TRAPS);\n+  const char* format_strict_static_message(Symbol* field_name, const char* doing_what = nullptr);\n+  void throw_strict_static_exception(Symbol* field_name, const char* when, TRAPS);\n+\n@@ -862,0 +967,3 @@\n+  static ByteSize inline_layout_info_array_offset() { return in_ByteSize(offset_of(InstanceKlass, _inline_layout_info_array)); }\n+  static ByteSize adr_inlineklass_fixed_block_offset() { return in_ByteSize(offset_of(InstanceKlass, _adr_inlineklass_fixed_block)); }\n+\n@@ -925,1 +1033,2 @@\n-                  bool is_interface) {\n+                  bool is_interface,\n+                  bool is_inline_type) {\n@@ -930,1 +1039,2 @@\n-           (is_interface ? (int)sizeof(Klass*)\/wordSize : 0));\n+           (is_interface ? (int)sizeof(Klass*)\/wordSize : 0) +\n+           (is_inline_type ? (int)sizeof(InlineKlassFixedBlock) : 0));\n@@ -936,1 +1046,2 @@\n-                                               is_interface());\n+                                               is_interface(),\n+                                               is_inline_klass());\n@@ -943,0 +1054,1 @@\n+  bool bounds_check(address addr, bool edge_ok = false, intptr_t size_in_bytes = -1) const PRODUCT_RETURN0;\n@@ -949,0 +1061,18 @@\n+  void set_inline_layout_info_array(Array<InlineLayoutInfo>* array) { _inline_layout_info_array = array; }\n+  Array<InlineLayoutInfo>* inline_layout_info_array() const { return _inline_layout_info_array; }\n+  void set_inline_layout_info(int index, InlineLayoutInfo *info) {\n+    assert(_inline_layout_info_array != nullptr ,\"Array not created\");\n+    _inline_layout_info_array->at_put(index, *info);\n+  }\n+  InlineLayoutInfo inline_layout_info(int index) const {\n+    assert(_inline_layout_info_array != nullptr ,\"Array not created\");\n+    return _inline_layout_info_array->at(index);\n+  }\n+  InlineLayoutInfo* inline_layout_info_adr(int index) {\n+    assert(_inline_layout_info_array != nullptr ,\"Array not created\");\n+    return _inline_layout_info_array->adr_at(index);\n+  }\n+\n+  inline InlineKlass* get_inline_type_field_klass(int idx) const ;\n+  inline InlineKlass* get_inline_type_field_klass_or_null(int idx) const;\n+\n@@ -950,1 +1080,1 @@\n-  int size_helper() const {\n+  virtual int size_helper() const {\n@@ -1001,0 +1131,1 @@\n+  const char* signature_name_of_carrier(char c) const;\n@@ -1126,1 +1257,1 @@\n-  void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);\n+  virtual void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);\n@@ -1153,1 +1284,2 @@\n-  void oop_print_on      (oop obj, outputStream* st);\n+  void oop_print_on      (oop obj, outputStream* st) { oop_print_on(obj, st, 0, 0); }\n+  void oop_print_on      (oop obj, outputStream* st, int indent = 0, int base_offset = 0);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":140,"deletions":8,"binary":false,"changes":148,"status":"modified"},{"patch":"@@ -277,14 +277,0 @@\n-static markWord make_prototype(const Klass* kls) {\n-  markWord prototype = markWord::prototype();\n-#ifdef _LP64\n-  if (UseCompactObjectHeaders) {\n-    \/\/ With compact object headers, the narrow Klass ID is part of the mark word.\n-    \/\/ We therefore seed the mark word with the narrow Klass ID.\n-    precond(CompressedKlassPointers::is_encodable(kls));\n-    const narrowKlass nk = CompressedKlassPointers::encode(const_cast<Klass*>(kls));\n-    prototype = prototype.set_narrow_klass(nk);\n-  }\n-#endif\n-  return prototype;\n-}\n-\n@@ -303,2 +289,1 @@\n-Klass::Klass(KlassKind kind) : _kind(kind),\n-                               _prototype_header(make_prototype(this)),\n+Klass::Klass(KlassKind kind, markWord prototype_header) : _kind(kind),\n@@ -306,0 +291,1 @@\n+  set_prototype_header(make_prototype_header(this, prototype_header));\n@@ -318,2 +304,2 @@\n-  int  tag   =  isobj ? _lh_array_tag_obj_value : _lh_array_tag_type_value;\n-  int lh = array_layout_helper(tag, hsize, etype, exact_log2(esize));\n+  int  tag   =  isobj ? _lh_array_tag_ref_value : _lh_array_tag_type_value;\n+  int lh = array_layout_helper(tag, false, hsize, etype, exact_log2(esize));\n@@ -323,1 +309,1 @@\n-  assert(layout_helper_is_objArray(lh) == isobj, \"correct kind\");\n+  assert(layout_helper_is_refArray(lh) == isobj, \"correct kind\");\n@@ -1037,4 +1023,2 @@\n-     if (UseCompactObjectHeaders) {\n-       st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n-       st->cr();\n-     }\n+     st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n+     st->cr();\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":7,"deletions":23,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -69,12 +69,16 @@\n-  enum KlassKind : u2 {\n-    InstanceKlassKind,\n-    InstanceRefKlassKind,\n-    InstanceMirrorKlassKind,\n-    InstanceClassLoaderKlassKind,\n-    InstanceStackChunkKlassKind,\n-    TypeArrayKlassKind,\n-    ObjArrayKlassKind,\n-    UnknownKlassKind\n-  };\n-\n-  static const uint KLASS_KIND_COUNT = ObjArrayKlassKind + 1;\n+   enum KlassKind : u2\n+   {\n+     InstanceKlassKind,\n+     InlineKlassKind,\n+     InstanceRefKlassKind,\n+     InstanceMirrorKlassKind,\n+     InstanceClassLoaderKlassKind,\n+     InstanceStackChunkKlassKind,\n+     TypeArrayKlassKind,\n+     ObjArrayKlassKind,\n+     RefArrayKlassKind,\n+     FlatArrayKlassKind,\n+     UnknownKlassKind\n+   };\n+\n+   static const uint KLASS_KIND_COUNT = FlatArrayKlassKind + 1;\n@@ -102,1 +106,1 @@\n-  \/\/    tag is 0x80 if the elements are oops, 0xC0 if non-oops\n+  \/\/    tag is 0x80 if the elements are oops, 0xC0 if non-oops, 0xA0 if value types\n@@ -207,1 +211,1 @@\n-  Klass(KlassKind kind);\n+  Klass(KlassKind kind, markWord prototype_header = markWord::prototype());\n@@ -475,1 +479,1 @@\n-  static const int _lh_array_tag_bits          = 2;\n+  static const int _lh_array_tag_bits          = 4;\n@@ -477,2 +481,9 @@\n-  static const int _lh_array_tag_obj_value     = ~0x01;   \/\/ 0x80000000 >> 30\n-  static const unsigned int _lh_array_tag_type_value = 0Xffffffff; \/\/ ~0x00,  \/\/ 0xC0000000 >> 30\n+  static const unsigned int _lh_array_tag_type_value = 0Xfffffffc;\n+  static const unsigned int _lh_array_tag_flat_value = 0Xfffffffa;\n+  static const unsigned int _lh_array_tag_ref_value  = 0Xfffffff8;\n+\n+  \/\/ null-free array flag bit under the array tag bits, shift one more to get array tag value\n+  static const int _lh_null_free_shift = _lh_array_tag_shift - 1;\n+  static const int _lh_null_free_mask  = 1;\n+\n+  static const jint _lh_array_tag_flat_value_bit_inplace = (jint) (1 << (_lh_array_tag_shift + 1));\n@@ -496,2 +507,1 @@\n-    \/\/ _lh_array_tag_type_value == (lh >> _lh_array_tag_shift);\n-    return (juint)lh >= (juint)(_lh_array_tag_type_value << _lh_array_tag_shift);\n+    return (juint) _lh_array_tag_type_value == (juint)(lh >> _lh_array_tag_shift);\n@@ -499,3 +509,14 @@\n-  static bool layout_helper_is_objArray(jint lh) {\n-    \/\/ _lh_array_tag_obj_value == (lh >> _lh_array_tag_shift);\n-    return (jint)lh < (jint)(_lh_array_tag_type_value << _lh_array_tag_shift);\n+  static bool layout_helper_is_refArray(jint lh) {\n+    return (juint)_lh_array_tag_ref_value == (juint)(lh >> _lh_array_tag_shift);\n+  }\n+  static bool layout_helper_is_flatArray(jint lh) {\n+    return (juint)_lh_array_tag_flat_value == (juint)(lh >> _lh_array_tag_shift);\n+  }\n+  static bool layout_helper_is_null_free(jint lh) {\n+    assert(layout_helper_is_flatArray(lh) || layout_helper_is_refArray(lh), \"must be array of inline types\");\n+    return ((lh >> _lh_null_free_shift) & _lh_null_free_mask);\n+  }\n+  static jint layout_helper_set_null_free(jint lh) {\n+    lh |= (_lh_null_free_mask << _lh_null_free_shift);\n+    assert(layout_helper_is_null_free(lh), \"Bad encoding\");\n+    return lh;\n@@ -512,1 +533,1 @@\n-    assert(btvalue >= T_BOOLEAN && btvalue <= T_OBJECT, \"sanity\");\n+    assert((btvalue >= T_BOOLEAN && btvalue <= T_OBJECT) || btvalue == T_FLAT_ELEMENT, \"sanity\");\n@@ -533,1 +554,1 @@\n-    assert(l2esz <= LogBytesPerLong,\n+    assert(layout_helper_element_type(lh) == T_FLAT_ELEMENT || l2esz <= LogBytesPerLong,\n@@ -537,1 +558,1 @@\n-  static jint array_layout_helper(jint tag, int hsize, BasicType etype, int log2_esize) {\n+  static jint array_layout_helper(jint tag, bool null_free, int hsize, BasicType etype, int log2_esize) {\n@@ -539,0 +560,1 @@\n+      |    ((null_free ? 1 : 0) <<  _lh_null_free_shift)\n@@ -681,0 +703,1 @@\n+  virtual bool is_refArray_klass_slow()     const { return false; }\n@@ -682,0 +705,1 @@\n+  virtual bool is_flatArray_klass_slow()    const { return false; }\n@@ -683,0 +707,2 @@\n+  \/\/ current implementation uses this method even in non debug builds\n+  virtual bool is_inline_klass_slow()       const { return false; }\n@@ -698,2 +724,1 @@\n-  \/\/ Other is anything that is not one of the more specialized kinds of InstanceKlass.\n-  bool is_other_instance_klass()        const { return _kind == InstanceKlassKind; }\n+  bool is_inline_klass()                const { return assert_same_query(_kind == InlineKlassKind, is_inline_klass_slow()); }\n@@ -705,1 +730,3 @@\n-  bool is_objArray_klass()              const { return assert_same_query( _kind == ObjArrayKlassKind,  is_objArray_klass_slow()); }\n+  bool is_flatArray_klass()             const { return assert_same_query( _kind == FlatArrayKlassKind, is_flatArray_klass_slow()); }\n+  bool is_objArray_klass()              const { return assert_same_query( _kind == ObjArrayKlassKind || _kind == RefArrayKlassKind || _kind == FlatArrayKlassKind,  is_objArray_klass_slow()); }\n+  bool is_refArray_klass()              const { return assert_same_query( _kind == RefArrayKlassKind, is_refArray_klass_slow()); }\n@@ -707,0 +734,1 @@\n+  bool is_refined_objArray_klass()      const { return is_refArray_klass() || is_flatArray_klass(); }\n@@ -709,0 +737,2 @@\n+  inline bool is_null_free_array_klass() const { return !is_typeArray_klass() && layout_helper_is_null_free(layout_helper()); }\n+\n@@ -717,1 +747,1 @@\n-  bool is_super() const                 { return _access_flags.is_super(); }\n+  bool is_identity_class() const        { assert(is_instance_klass(), \"only for instanceKlass\"); return _access_flags.is_identity_class(); }\n@@ -734,0 +764,1 @@\n+  static inline markWord make_prototype_header(const Klass* kls, markWord prototype = markWord::prototype());\n@@ -737,0 +768,1 @@\n+  inline void set_prototype_header_klass(narrowKlass klass);\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":61,"deletions":29,"binary":false,"changes":90,"status":"modified"},{"patch":"@@ -62,0 +62,15 @@\n+inline markWord Klass::make_prototype_header(const Klass* kls, markWord prototype) {\n+  if (UseCompactObjectHeaders) {\n+    \/\/ With compact object headers, the narrow Klass ID is part of the mark word.\n+    \/\/ We therefore seed the mark word with the narrow Klass ID.\n+    precond(CompressedKlassPointers::is_encodable(kls));\n+    const narrowKlass nk = CompressedKlassPointers::encode(const_cast<Klass*>(kls));\n+    prototype = prototype.set_narrow_klass(nk);\n+  }\n+  return prototype;\n+}\n+\n+inline void Klass::set_prototype_header(markWord header) {\n+  _prototype_header = header;\n+}\n+\n@@ -68,2 +83,0 @@\n-  assert(UseCompactObjectHeaders, \"only use with compact object headers\");\n-#ifdef _LP64\n@@ -72,1 +85,1 @@\n-  assert(_prototype_header.narrow_klass() > 0, \"Klass \" PTR_FORMAT \": invalid prototype (\" PTR_FORMAT \")\",\n+  assert(!UseCompactObjectHeaders || _prototype_header.narrow_klass() > 0, \"Klass \" PTR_FORMAT \": invalid prototype (\" PTR_FORMAT \")\",\n@@ -74,1 +87,0 @@\n-#endif\n@@ -78,5 +90,3 @@\n-\/\/ This is only used when dumping the archive. In other cases,\n-\/\/ the _prototype_header is already initialized to the right thing.\n-inline void Klass::set_prototype_header(markWord header) {\n-  assert(UseCompactObjectHeaders, \"only with compact headers\");\n-  _prototype_header = header;\n+inline void Klass::set_prototype_header_klass(narrowKlass klass) {\n+  \/\/ Merge narrowKlass in existing prototype header.\n+  _prototype_header = _prototype_header.set_narrow_klass(klass);\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":19,"deletions":9,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -125,1 +126,0 @@\n-\n@@ -170,0 +170,5 @@\n+address Method::get_c2i_inline_entry() {\n+  assert(adapter() != nullptr, \"must have\");\n+  return adapter()->get_c2i_inline_entry();\n+}\n+\n@@ -175,0 +180,5 @@\n+address Method::get_c2i_unverified_inline_entry() {\n+  assert(adapter() != nullptr, \"must have\");\n+  return adapter()->get_c2i_unverified_inline_entry();\n+}\n+\n@@ -400,1 +410,1 @@\n-  if (!method_holder()->is_rewritten()) {\n+  if (!method_holder()->is_rewritten() || CDSConfig::is_valhalla_preview()) {\n@@ -443,0 +453,2 @@\n+    _from_compiled_inline_entry = _adapter->get_c2i_inline_entry();\n+    _from_compiled_inline_ro_entry = _adapter->get_c2i_inline_ro_entry();\n@@ -732,0 +744,14 @@\n+\/\/ InlineKlass the method is declared to return. This must not\n+\/\/ safepoint as it is called with references live on the stack at\n+\/\/ locations the GC is unaware of.\n+InlineKlass* Method::returns_inline_type() const {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  if (is_native()) {\n+    return nullptr;\n+  }\n+  NoSafepointVerifier nsv;\n+  SignatureStream ss(signature());\n+  ss.skip_to_return_type();\n+  return ss.as_inline_klass(method_holder());\n+}\n+\n@@ -880,0 +906,5 @@\n+  if (has_scalarized_return()) {\n+    \/\/ Don't treat this as (trivial) getter method because the\n+    \/\/ inline type should be returned in a scalarized form.\n+    return false;\n+  }\n@@ -901,0 +932,5 @@\n+  if (has_scalarized_args()) {\n+    \/\/ Don't treat this as (trivial) setter method because the\n+    \/\/ inline type argument should be passed in a scalarized form.\n+    return false;\n+  }\n@@ -911,6 +947,2 @@\n-          Bytecodes::is_return(java_code_at(last_index)));\n-}\n-\n-bool Method::has_valid_initializer_flags() const {\n-  return (is_static() ||\n-          method_holder()->major_version() < 51);\n+          Bytecodes::is_return(java_code_at(last_index)) &&\n+          !has_scalarized_args());\n@@ -919,1 +951,1 @@\n-bool Method::is_static_initializer() const {\n+bool Method::is_class_initializer() const {\n@@ -923,2 +955,3 @@\n-  return name() == vmSymbols::class_initializer_name() &&\n-         has_valid_initializer_flags();\n+  return (name() == vmSymbols::class_initializer_name() &&\n+          (is_static() ||\n+           method_holder()->major_version() < 51));\n@@ -927,2 +960,3 @@\n-bool Method::is_object_initializer() const {\n-   return name() == vmSymbols::object_initializer_name();\n+\/\/ A method named <init>, is a classic object constructor.\n+bool Method::is_object_constructor() const {\n+  return name() == vmSymbols::object_initializer_name();\n@@ -991,1 +1025,1 @@\n-  if( constants()->tag_at(klass_index).is_unresolved_klass() ) {\n+  if( constants()->tag_at(klass_index).is_unresolved_klass()) {\n@@ -1006,1 +1040,3 @@\n-    if (constants()->tag_at(klass_index).is_unresolved_klass()) return false;\n+    if (constants()->tag_at(klass_index).is_unresolved_klass()) {\n+      return false;\n+    }\n@@ -1174,1 +1210,3 @@\n-    _from_compiled_entry = nullptr;\n+    _from_compiled_entry    = nullptr;\n+    _from_compiled_inline_entry = nullptr;\n+    _from_compiled_inline_ro_entry = nullptr;\n@@ -1176,1 +1214,3 @@\n-    _from_compiled_entry = adapter()->get_c2i_entry();\n+    _from_compiled_entry    = adapter()->get_c2i_entry();\n+    _from_compiled_inline_entry = adapter()->get_c2i_inline_entry();\n+    _from_compiled_inline_ro_entry = adapter()->get_c2i_inline_ro_entry();\n@@ -1210,0 +1250,2 @@\n+  _from_compiled_inline_entry = nullptr;\n+  _from_compiled_inline_ro_entry = nullptr;\n@@ -1241,0 +1283,2 @@\n+  set_has_scalarized_args(false);\n+  set_has_scalarized_return(false);\n@@ -1277,0 +1321,3 @@\n+  if (InlineTypeReturnedAsFields && returns_inline_type() && !has_scalarized_return()) {\n+    set_has_scalarized_return();\n+  }\n@@ -1287,1 +1334,4 @@\n-    h_method->_from_compiled_entry = SharedRuntime::get_handle_wrong_method_abstract_stub();\n+    address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();\n+    h_method->_from_compiled_entry = wrong_method_abstract;\n+    h_method->_from_compiled_inline_entry = wrong_method_abstract;\n+    h_method->_from_compiled_inline_ro_entry = wrong_method_abstract;\n@@ -1294,0 +1344,2 @@\n+    h_method->_from_compiled_inline_entry = adapter()->get_c2i_inline_entry();\n+    h_method->_from_compiled_inline_ro_entry = adapter()->get_c2i_inline_ro_entry();\n@@ -1349,0 +1401,12 @@\n+address Method::verified_inline_code_entry() {\n+  DEBUG_ONLY(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_entry != nullptr, \"must be set\");\n+  return _from_compiled_inline_entry;\n+}\n+\n+address Method::verified_inline_ro_code_entry() {\n+  DEBUG_ONLY(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_ro_entry != nullptr, \"must be set\");\n+  return _from_compiled_inline_ro_entry;\n+}\n+\n@@ -1380,0 +1444,2 @@\n+  mh->_from_compiled_inline_entry = code->verified_inline_entry_point();\n+  mh->_from_compiled_inline_ro_entry = code->verified_inline_ro_entry_point();\n@@ -1572,0 +1638,2 @@\n+    m->set_from_compiled_inline_entry(m->adapter()->get_c2i_inline_entry());\n+    m->set_from_compiled_inline_ro_entry(m->adapter()->get_c2i_inline_ro_entry());\n@@ -2189,0 +2257,25 @@\n+bool Method::is_scalarized_arg(int idx) const {\n+  if (!has_scalarized_args()) {\n+    return false;\n+  }\n+  \/\/ Search through signature and check if argument is wrapped in T_METADATA\/T_VOID\n+  int depth = 0;\n+  const GrowableArray<SigEntry>* sig = adapter()->get_sig_cc();\n+  for (int i = 0; i < sig->length(); i++) {\n+    BasicType bt = sig->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      depth++;\n+    }\n+    if (idx == 0) {\n+      break; \/\/ Argument found\n+    }\n+    if (bt == T_VOID && (sig->at(i-1)._bt != T_LONG && sig->at(i-1)._bt != T_DOUBLE)) {\n+      depth--;\n+    }\n+    if (depth == 0 && bt != T_LONG && bt != T_DOUBLE) {\n+      idx--; \/\/ Advance to next argument\n+    }\n+  }\n+  return depth != 0;\n+}\n+\n@@ -2214,0 +2307,4 @@\n+#ifdef ASSERT\n+  if (valid_itable_index())\n+    st->print_cr(\" - itable index:      %d\",   itable_index());\n+#endif\n@@ -2221,1 +2318,3 @@\n-  st->print_cr(\" - compiled entry     \" PTR_FORMAT, p2i(from_compiled_entry()));\n+  st->print_cr(\" - compiled entry           \" PTR_FORMAT, p2i(from_compiled_entry()));\n+  st->print_cr(\" - compiled inline entry    \" PTR_FORMAT, p2i(from_compiled_inline_entry()));\n+  st->print_cr(\" - compiled inline ro entry \" PTR_FORMAT, p2i(from_compiled_inline_ro_entry()));\n@@ -2291,0 +2390,1 @@\n+  if (WizardMode) access_flags().print_on(st);\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":119,"deletions":19,"binary":false,"changes":138,"status":"modified"},{"patch":"@@ -47,1 +47,3 @@\n-  Klass* _element_klass;            \/\/ The klass of the elements of this array type\n+ protected:\n+  Klass* _element_klass;            \/\/ The klass of the elements of this array type\n+  ObjArrayKlass* _next_refined_array_klass;\n@@ -50,0 +52,1 @@\n+ protected:\n@@ -51,2 +54,2 @@\n-  ObjArrayKlass(int n, Klass* element_klass, Symbol* name);\n-  static ObjArrayKlass* allocate_klass(ClassLoaderData* loader_data, int n, Klass* k, Symbol* name, TRAPS);\n+  ObjArrayKlass(int n, Klass* element_klass, Symbol* name, KlassKind kind, ArrayKlass::ArrayProperties props, markWord mw);\n+  static ObjArrayKlass* allocate_klass(ClassLoaderData* loader_data, int n, Klass* k, Symbol* name, ArrayKlass::ArrayProperties props, TRAPS);\n@@ -54,1 +57,3 @@\n-  objArrayOop allocate_instance(int length, TRAPS);\n+  static ArrayDescription array_layout_selection(Klass* element, ArrayProperties properties);\n+  ObjArrayKlass* allocate_klass_with_properties(ArrayKlass::ArrayProperties props, TRAPS);\n+  virtual objArrayOop allocate_instance(int length, ArrayProperties props, TRAPS);\n@@ -56,2 +61,1 @@\n- protected:\n-  \/\/ Create array_name for element klass\n+   \/\/ Create array_name for element klass\n@@ -64,3 +68,10 @@\n-  \/\/ Instance variables\n-  Klass* element_klass() const      { return _element_klass; }\n-  void set_element_klass(Klass* k)  { _element_klass = k; }\n+  virtual Klass* element_klass() const      { return _element_klass; }\n+  virtual void set_element_klass(Klass* k)  { _element_klass = k; }\n+\n+\n+  ObjArrayKlass* next_refined_array_klass() const      { return _next_refined_array_klass; }\n+  inline ObjArrayKlass* next_refined_array_klass_acquire() const;\n+  void set_next_refined_klass_klass(ObjArrayKlass* ak) { _next_refined_array_klass = ak; }\n+  inline void release_set_next_refined_klass(ObjArrayKlass* ak);\n+  ObjArrayKlass* klass_with_properties(ArrayKlass::ArrayProperties properties, TRAPS);\n+  static ByteSize next_refined_array_klass_offset() { return byte_offset_of(ObjArrayKlass, _next_refined_array_klass); }\n@@ -99,6 +110,6 @@\n- private:\n-  \/\/ Either oop or narrowOop depending on UseCompressedOops.\n-  \/\/ must be called from within ObjArrayKlass.cpp\n-  void do_copy(arrayOop s, size_t src_offset,\n-               arrayOop d, size_t dst_offset,\n-               int length, TRAPS);\n+#if INCLUDE_CDS\n+  virtual void remove_unshareable_info();\n+  virtual void remove_java_mirror();\n+  void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, TRAPS);\n+#endif\n+\n@@ -160,1 +171,1 @@\n-  void oop_print_value_on(oop obj, outputStream* st);\n+  virtual void oop_print_value_on(oop obj, outputStream* st);\n@@ -162,1 +173,1 @@\n-  void oop_print_on      (oop obj, outputStream* st);\n+  virtual void oop_print_on      (oop obj, outputStream* st);\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.hpp","additions":28,"deletions":17,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -142,6 +142,9 @@\n-bool oopDesc::is_instance_noinline()    const { return is_instance();    }\n-bool oopDesc::is_instanceRef_noinline() const { return is_instanceRef(); }\n-bool oopDesc::is_stackChunk_noinline()  const { return is_stackChunk();  }\n-bool oopDesc::is_array_noinline()       const { return is_array();       }\n-bool oopDesc::is_objArray_noinline()    const { return is_objArray();    }\n-bool oopDesc::is_typeArray_noinline()   const { return is_typeArray();   }\n+bool oopDesc::is_instance_noinline()        const { return is_instance();         }\n+bool oopDesc::is_instanceRef_noinline()     const { return is_instanceRef();      }\n+bool oopDesc::is_stackChunk_noinline()      const { return is_stackChunk();       }\n+bool oopDesc::is_array_noinline()           const { return is_array();            }\n+bool oopDesc::is_objArray_noinline()        const { return is_objArray();         }\n+bool oopDesc::is_refArray_noinline()        const { return is_refArray();         }\n+bool oopDesc::is_typeArray_noinline()       const { return is_typeArray();        }\n+bool oopDesc::is_flatArray_noinline()       const { return is_flatArray();        }\n+bool oopDesc::is_null_free_array_noinline() const { return is_null_free_array();  }\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":9,"deletions":6,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1884,1 +1884,1 @@\n-          derived->bottom_type()->make_ptr()->is_ptr()->_offset == 0, \"sanity\");\n+         derived->bottom_type()->make_ptr()->is_ptr()->offset() == 0, \"sanity\");\n@@ -1887,1 +1887,1 @@\n-  if( tj == nullptr || tj->_offset == 0 ) {\n+  if (tj == nullptr || tj->offset() == 0) {\n@@ -2053,1 +2053,1 @@\n-                  derived->bottom_type()->make_ptr()->is_ptr()->_offset == 0, \"sanity\");\n+                 derived->bottom_type()->make_ptr()->is_ptr()->offset() == 0, \"sanity\");\n@@ -2055,1 +2055,1 @@\n-          if( tj && tj->_offset != 0 && tj->isa_oop_ptr() ) {\n+          if (tj && tj->offset() != 0 && tj->isa_oop_ptr()) {\n@@ -2345,1 +2345,1 @@\n-  const TypeTuple *domain = C->tf()->domain();\n+  const TypeTuple *domain = C->tf()->domain_cc();\n@@ -2554,1 +2554,1 @@\n-                  if (is_derived && check->bottom_type()->is_ptr()->_offset != 0) {\n+                  if (is_derived && check->bottom_type()->is_ptr()->offset() != 0) {\n@@ -2558,1 +2558,1 @@\n-                    assert(check->bottom_type()->is_ptr()->_offset == 0, \"Bad base pointer\");\n+                    assert(check->bottom_type()->is_ptr()->offset() == 0, \"Bad base pointer\");\n@@ -2567,1 +2567,1 @@\n-                } else if (check->bottom_type()->is_ptr()->_offset == 0) {\n+                } else if (check->bottom_type()->is_ptr()->offset() == 0) {\n","filename":"src\/hotspot\/share\/opto\/chaitin.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -69,0 +70,1 @@\n+#include \"opto\/movenode.hpp\"\n@@ -408,0 +410,3 @@\n+  if (dead->is_InlineType()) {\n+    remove_inline_type(dead);\n+  }\n@@ -455,0 +460,3 @@\n+    if (n->outcnt() == 0) {\n+      worklist.push(n);\n+    }\n@@ -463,0 +471,6 @@\n+  remove_useless_nodes(_inline_type_nodes,  useful); \/\/ remove useless inline type nodes\n+#ifdef ASSERT\n+  if (_modified_nodes != nullptr) {\n+    _modified_nodes->remove_useless_nodes(useful.member_set());\n+  }\n+#endif\n@@ -649,0 +663,1 @@\n+      _has_circular_inline_type(false),\n@@ -668,0 +683,1 @@\n+      _inline_type_nodes (comp_arena(), 8, 0, nullptr),\n@@ -776,4 +792,2 @@\n-      const TypeTuple *domain = StartOSRNode::osr_domain();\n-      const TypeTuple *range = TypeTuple::make_range(method()->signature());\n-      init_tf(TypeFunc::make(domain, range));\n-      StartNode* s = new StartOSRNode(root(), domain);\n+      init_tf(TypeFunc::make(method(), \/* is_osr_compilation = *\/ true));\n+      StartNode* s = new StartOSRNode(root(), tf()->domain_sig());\n@@ -786,1 +800,1 @@\n-      StartNode* s = new StartNode(root(), tf()->domain());\n+      StartNode* s = new StartNode(root(), tf()->domain_cc());\n@@ -887,0 +901,10 @@\n+  if (needs_stack_repair()) {\n+    \/\/ One extra slot for the special stack increment value\n+    next_slot += 2;\n+  }\n+  \/\/ TODO 8284443 Only reserve extra slot if needed\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ One extra slot to hold the null marker for a nullable\n+    \/\/ inline type return if we run out of registers.\n+    next_slot += 2;\n+  }\n@@ -924,0 +948,1 @@\n+      _has_circular_inline_type(false),\n@@ -1079,0 +1104,4 @@\n+  _has_flat_accesses = false;\n+  _flat_accesses_share_alias = true;\n+  _scalarize_in_safepoints = false;\n+\n@@ -1363,0 +1392,9 @@\n+  if (ta && ta->is_not_flat()) {\n+    \/\/ Erase not flat property for alias analysis.\n+    tj = ta = ta->cast_to_not_flat(false);\n+  }\n+  if (ta && ta->is_not_null_free()) {\n+    \/\/ Erase not null free property for alias analysis.\n+    tj = ta = ta->cast_to_not_null_free(false);\n+  }\n+\n@@ -1376,0 +1414,2 @@\n+    \/\/ For flat inline type array, each field has its own slice so\n+    \/\/ we must include the field offset.\n@@ -1416,1 +1456,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n@@ -1420,1 +1460,6 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n+    }\n+    \/\/ Initially all flattened array accesses share a single slice\n+    if (ta->is_flat() && ta->elem() != TypeInstPtr::BOTTOM && _flat_accesses_share_alias) {\n+      const TypeAry* tary = TypeAry::make(TypeInstPtr::BOTTOM, ta->size(), \/* stable= *\/ false, \/* flat= *\/ true);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));\n@@ -1427,1 +1472,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,Type::Offset(offset), ta->field_offset());\n@@ -1477,1 +1522,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, Type::Offset(offset));\n@@ -1498,1 +1543,1 @@\n-        assert(tj == TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, offset, instance_id), \"exact type should be canonical type\");\n+        assert(tj == TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, Type::Offset(offset), instance_id), \"exact type should be canonical type\");\n@@ -1501,1 +1546,1 @@\n-        tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, offset, instance_id);\n+        tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, Type::Offset(offset), instance_id);\n@@ -1516,1 +1561,1 @@\n-                                       offset);\n+                                       Type::Offset(offset));\n@@ -1522,1 +1567,1 @@\n-        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), offset);\n+        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), Type::Offset(offset));\n@@ -1524,1 +1569,1 @@\n-        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, offset);\n+        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, Type::Offset(offset), tk->is_not_flat(), tk->is_not_null_free(), tk->is_flat(), tk->is_null_free(), tk->is_atomic(), tk->is_aryklassptr()->is_vm_type());\n@@ -1527,1 +1572,0 @@\n-\n@@ -1657,1 +1701,1 @@\n-Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {\n+Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {\n@@ -1662,3 +1706,6 @@\n-  AliasCacheEntry* ace = probe_alias_cache(adr_type);\n-  if (ace->_adr_type == adr_type) {\n-    return alias_type(ace->_index);\n+  AliasCacheEntry* ace = nullptr;\n+  if (!uncached) {\n+    ace = probe_alias_cache(adr_type);\n+    if (ace->_adr_type == adr_type) {\n+      return alias_type(ace->_index);\n+    }\n@@ -1714,0 +1761,1 @@\n+    ciField* field = nullptr;\n@@ -1720,0 +1768,1 @@\n+      const Type* elemtype = flat->is_aryptr()->elem();\n@@ -1721,1 +1770,8 @@\n-        alias_type(idx)->set_element(flat->is_aryptr()->elem());\n+        alias_type(idx)->set_element(elemtype);\n+      }\n+      int field_offset = flat->is_aryptr()->field_offset().get();\n+      if (flat->is_flat() &&\n+          field_offset != Type::OffsetBot) {\n+        ciInlineKlass* vk = elemtype->inline_klass();\n+        field_offset += vk->payload_offset();\n+        field = vk->get_field_by_offset(field_offset, false);\n@@ -1737,0 +1793,2 @@\n+      if (flat->offset() == in_bytes(Klass::layout_helper_offset()))\n+        alias_type(idx)->set_rewritable(false);\n@@ -1747,1 +1805,0 @@\n-      ciField* field;\n@@ -1754,0 +1811,4 @@\n+      } else if (tinst->is_inlinetypeptr()) {\n+        \/\/ Inline type field\n+        ciInlineKlass* vk = tinst->inline_klass();\n+        field = vk->get_field_by_offset(tinst->offset(), false);\n@@ -1758,7 +1819,14 @@\n-      assert(field == nullptr ||\n-             original_field == nullptr ||\n-             (field->holder() == original_field->holder() &&\n-              field->offset_in_bytes() == original_field->offset_in_bytes() &&\n-              field->is_static() == original_field->is_static()), \"wrong field?\");\n-      \/\/ Set field() and is_rewritable() attributes.\n-      if (field != nullptr)  alias_type(idx)->set_field(field);\n+    }\n+    assert(field == nullptr ||\n+           original_field == nullptr ||\n+           (field->holder() == original_field->holder() &&\n+            field->offset_in_bytes() == original_field->offset_in_bytes() &&\n+            field->is_static() == original_field->is_static()), \"wrong field?\");\n+    \/\/ Set field() and is_rewritable() attributes.\n+    if (field != nullptr) {\n+      alias_type(idx)->set_field(field);\n+      if (flat->isa_aryptr()) {\n+        \/\/ Fields of flat arrays are rewritable although they are declared final\n+        assert(flat->is_flat(), \"must be a flat array\");\n+        alias_type(idx)->set_rewritable(true);\n+      }\n@@ -1769,3 +1837,4 @@\n-  ace->_adr_type = adr_type;\n-  ace->_index    = idx;\n-  assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n+  if (!uncached) {\n+    ace->_adr_type = adr_type;\n+    ace->_index    = idx;\n+    assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n@@ -1773,6 +1842,7 @@\n-  \/\/ Might as well try to fill the cache for the flattened version, too.\n-  AliasCacheEntry* face = probe_alias_cache(flat);\n-  if (face->_adr_type == nullptr) {\n-    face->_adr_type = flat;\n-    face->_index    = idx;\n-    assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    \/\/ Might as well try to fill the cache for the flattened version, too.\n+    AliasCacheEntry* face = probe_alias_cache(flat);\n+    if (face->_adr_type == nullptr) {\n+      face->_adr_type = flat;\n+      face->_index    = idx;\n+      assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    }\n@@ -1900,0 +1970,398 @@\n+void Compile::add_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  _inline_type_nodes.push(n);\n+}\n+\n+void Compile::remove_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  if (_inline_type_nodes.contains(n)) {\n+    _inline_type_nodes.remove(n);\n+  }\n+}\n+\n+\/\/ Does the return value keep otherwise useless inline type allocations alive?\n+static bool return_val_keeps_allocations_alive(Node* ret_val) {\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(ret_val);\n+  bool some_allocations = false;\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->outcnt() > 1) {\n+      \/\/ Some other use for the allocation\n+      return false;\n+    } else if (n->is_InlineType()) {\n+      wq.push(n->in(1));\n+    } else if (n->is_Phi()) {\n+      for (uint j = 1; j < n->req(); j++) {\n+        wq.push(n->in(j));\n+      }\n+    } else if (n->is_CheckCastPP() &&\n+               n->in(1)->is_Proj() &&\n+               n->in(1)->in(0)->is_Allocate()) {\n+      some_allocations = true;\n+    } else if (n->is_CheckCastPP()) {\n+      wq.push(n->in(1));\n+    }\n+  }\n+  return some_allocations;\n+}\n+\n+void Compile::process_inline_types(PhaseIterGVN &igvn, bool remove) {\n+  \/\/ Make sure that the return value does not keep an otherwise unused allocation alive\n+  if (tf()->returns_inline_type_as_fields()) {\n+    Node* ret = nullptr;\n+    for (uint i = 1; i < root()->req(); i++) {\n+      Node* in = root()->in(i);\n+      if (in->Opcode() == Op_Return) {\n+        assert(ret == nullptr, \"only one return\");\n+        ret = in;\n+      }\n+    }\n+    if (ret != nullptr) {\n+      Node* ret_val = ret->in(TypeFunc::Parms);\n+      if (igvn.type(ret_val)->isa_oopptr() &&\n+          return_val_keeps_allocations_alive(ret_val)) {\n+        igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)->inline_klass(), igvn));\n+        assert(ret_val->outcnt() == 0, \"should be dead now\");\n+        igvn.remove_dead_node(ret_val);\n+      }\n+    }\n+  }\n+  if (_inline_type_nodes.length() == 0) {\n+    return;\n+  }\n+  \/\/ Scalarize inline types in safepoint debug info.\n+  \/\/ Delay this until all inlining is over to avoid getting inconsistent debug info.\n+  set_scalarize_in_safepoints(true);\n+  for (int i = _inline_type_nodes.length()-1; i >= 0; i--) {\n+    InlineTypeNode* vt = _inline_type_nodes.at(i)->as_InlineType();\n+    vt->make_scalar_in_safepoints(&igvn);\n+    igvn.record_for_igvn(vt);\n+  }\n+  if (remove) {\n+    \/\/ Remove inline type nodes by replacing them with their oop input\n+    while (_inline_type_nodes.length() > 0) {\n+      InlineTypeNode* vt = _inline_type_nodes.pop()->as_InlineType();\n+      if (vt->outcnt() == 0) {\n+        igvn.remove_dead_node(vt);\n+        continue;\n+      }\n+      for (DUIterator i = vt->outs(); vt->has_out(i); i++) {\n+        DEBUG_ONLY(bool must_be_buffered = false);\n+        Node* u = vt->out(i);\n+        \/\/ Check if any users are blackholes. If so, rewrite them to use either the\n+        \/\/ allocated buffer, or individual components, instead of the inline type node\n+        \/\/ that goes away.\n+        if (u->is_Blackhole()) {\n+          BlackholeNode* bh = u->as_Blackhole();\n+\n+          \/\/ Unlink the old input\n+          int idx = bh->find_edge(vt);\n+          assert(idx != -1, \"The edge should be there\");\n+          bh->del_req(idx);\n+          --i;\n+\n+          if (vt->is_allocated(&igvn)) {\n+            \/\/ Already has the allocated instance, blackhole that\n+            bh->add_req(vt->get_oop());\n+          } else {\n+            \/\/ Not allocated yet, blackhole the components\n+            for (uint c = 0; c < vt->field_count(); c++) {\n+              bh->add_req(vt->field_value(c));\n+            }\n+          }\n+\n+          \/\/ Node modified, record for IGVN\n+          igvn.record_for_igvn(bh);\n+        }\n+#ifdef ASSERT\n+        \/\/ Verify that inline type is buffered when replacing by oop\n+        else if (u->is_InlineType()) {\n+          \/\/ InlineType uses don't need buffering because they are about to be replaced as well\n+        } else if (u->is_Phi()) {\n+          \/\/ TODO 8302217 Remove this once InlineTypeNodes are reliably pushed through\n+        } else {\n+          must_be_buffered = true;\n+        }\n+        if (must_be_buffered && !vt->is_allocated(&igvn)) {\n+          vt->dump(0);\n+          u->dump(0);\n+          assert(false, \"Should have been buffered\");\n+        }\n+#endif\n+      }\n+      igvn.replace_node(vt, vt->get_oop());\n+    }\n+  }\n+  igvn.optimize();\n+}\n+\n+void Compile::adjust_flat_array_access_aliases(PhaseIterGVN& igvn) {\n+  if (!_has_flat_accesses) {\n+    return;\n+  }\n+  \/\/ Initially, all flat array accesses share the same slice to\n+  \/\/ keep dependencies with Object[] array accesses (that could be\n+  \/\/ to a flat array) correct. We're done with parsing so we\n+  \/\/ now know all flat array accesses in this compile\n+  \/\/ unit. Let's move flat array accesses to their own slice,\n+  \/\/ one per element field. This should help memory access\n+  \/\/ optimizations.\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(root());\n+\n+  Node_List mergememnodes;\n+  Node_List memnodes;\n+\n+  \/\/ Alias index currently shared by all flat memory accesses\n+  int index = get_alias_index(TypeAryPtr::INLINES);\n+\n+  \/\/ Find MergeMem nodes and flat array accesses\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->is_Mem()) {\n+      const TypePtr* adr_type = nullptr;\n+      adr_type = get_adr_type(get_alias_index(n->adr_type()));\n+      if (adr_type == TypeAryPtr::INLINES) {\n+        memnodes.push(n);\n+      }\n+    } else if (n->is_MergeMem()) {\n+      MergeMemNode* mm = n->as_MergeMem();\n+      if (mm->memory_at(index) != mm->base_memory()) {\n+        mergememnodes.push(n);\n+      }\n+    }\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* m = n->in(j);\n+      if (m != nullptr) {\n+        wq.push(m);\n+      }\n+    }\n+  }\n+\n+  if (memnodes.size() > 0) {\n+    _flat_accesses_share_alias = false;\n+\n+    \/\/ We are going to change the slice for the flat array\n+    \/\/ accesses so we need to clear the cache entries that refer to\n+    \/\/ them.\n+    for (uint i = 0; i < AliasCacheSize; i++) {\n+      AliasCacheEntry* ace = &_alias_cache[i];\n+      if (ace->_adr_type != nullptr &&\n+          ace->_adr_type->is_flat()) {\n+        ace->_adr_type = nullptr;\n+        ace->_index = (i != 0) ? 0 : AliasIdxTop; \/\/ Make sure the nullptr adr_type resolves to AliasIdxTop\n+      }\n+    }\n+\n+    \/\/ Find what aliases we are going to add\n+    int start_alias = num_alias_types()-1;\n+    int stop_alias = 0;\n+\n+    for (uint i = 0; i < memnodes.size(); i++) {\n+      Node* m = memnodes.at(i);\n+      const TypePtr* adr_type = nullptr;\n+      adr_type = m->adr_type();\n+#ifdef ASSERT\n+      m->as_Mem()->set_adr_type(adr_type);\n+#endif\n+      int idx = get_alias_index(adr_type);\n+      start_alias = MIN2(start_alias, idx);\n+      stop_alias = MAX2(stop_alias, idx);\n+    }\n+\n+    assert(stop_alias >= start_alias, \"should have expanded aliases\");\n+\n+    Node_Stack stack(0);\n+#ifdef ASSERT\n+    VectorSet seen(Thread::current()->resource_area());\n+#endif\n+    \/\/ Now let's fix the memory graph so each flat array access\n+    \/\/ is moved to the right slice. Start from the MergeMem nodes.\n+    uint last = unique();\n+    for (uint i = 0; i < mergememnodes.size(); i++) {\n+      MergeMemNode* current = mergememnodes.at(i)->as_MergeMem();\n+      Node* n = current->memory_at(index);\n+      MergeMemNode* mm = nullptr;\n+      do {\n+        \/\/ Follow memory edges through memory accesses, phis and\n+        \/\/ narrow membars and push nodes on the stack. Once we hit\n+        \/\/ bottom memory, we pop element off the stack one at a\n+        \/\/ time, in reverse order, and move them to the right slice\n+        \/\/ by changing their memory edges.\n+        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() || n->adr_type() == TypeAryPtr::INLINES) {\n+          assert(!seen.test_set(n->_idx), \"\");\n+          \/\/ Uses (a load for instance) will need to be moved to the\n+          \/\/ right slice as well and will get a new memory state\n+          \/\/ that we don't know yet. The use could also be the\n+          \/\/ backedge of a loop. We put a place holder node between\n+          \/\/ the memory node and its uses. We replace that place\n+          \/\/ holder with the correct memory state once we know it,\n+          \/\/ i.e. when nodes are popped off the stack. Using the\n+          \/\/ place holder make the logic work in the presence of\n+          \/\/ loops.\n+          if (n->outcnt() > 1) {\n+            Node* place_holder = nullptr;\n+            assert(!n->has_out_with(Op_Node), \"\");\n+            for (DUIterator k = n->outs(); n->has_out(k); k++) {\n+              Node* u = n->out(k);\n+              if (u != current && u->_idx < last) {\n+                bool success = false;\n+                for (uint l = 0; l < u->req(); l++) {\n+                  if (!stack.is_empty() && u == stack.node() && l == stack.index()) {\n+                    continue;\n+                  }\n+                  Node* in = u->in(l);\n+                  if (in == n) {\n+                    if (place_holder == nullptr) {\n+                      place_holder = new Node(1);\n+                      place_holder->init_req(0, n);\n+                    }\n+                    igvn.replace_input_of(u, l, place_holder);\n+                    success = true;\n+                  }\n+                }\n+                if (success) {\n+                  --k;\n+                }\n+              }\n+            }\n+          }\n+          if (n->is_Phi()) {\n+            stack.push(n, 1);\n+            n = n->in(1);\n+          } else if (n->is_Mem()) {\n+            stack.push(n, n->req());\n+            n = n->in(MemNode::Memory);\n+          } else {\n+            assert(n->is_Proj() && n->in(0)->Opcode() == Op_MemBarCPUOrder, \"\");\n+            stack.push(n, n->req());\n+            n = n->in(0)->in(TypeFunc::Memory);\n+          }\n+        } else {\n+          assert(n->adr_type() == TypePtr::BOTTOM || (n->Opcode() == Op_Node && n->_idx >= last) || (n->is_Proj() && n->in(0)->is_Initialize()), \"\");\n+          \/\/ Build a new MergeMem node to carry the new memory state\n+          \/\/ as we build it. IGVN should fold extraneous MergeMem\n+          \/\/ nodes.\n+          mm = MergeMemNode::make(n);\n+          igvn.register_new_node_with_optimizer(mm);\n+          while (stack.size() > 0) {\n+            Node* m = stack.node();\n+            uint idx = stack.index();\n+            if (m->is_Mem()) {\n+              \/\/ Move memory node to its new slice\n+              const TypePtr* adr_type = m->adr_type();\n+              int alias = get_alias_index(adr_type);\n+              Node* prev = mm->memory_at(alias);\n+              igvn.replace_input_of(m, MemNode::Memory, prev);\n+              mm->set_memory_at(alias, m);\n+            } else if (m->is_Phi()) {\n+              \/\/ We need as many new phis as there are new aliases\n+              igvn.replace_input_of(m, idx, mm);\n+              if (idx == m->req()-1) {\n+                Node* r = m->in(0);\n+                for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                  const TypePtr* adr_type = get_adr_type(j);\n+                  if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                    continue;\n+                  }\n+                  Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));\n+                  igvn.register_new_node_with_optimizer(phi);\n+                  for (uint k = 1; k < m->req(); k++) {\n+                    phi->init_req(k, m->in(k)->as_MergeMem()->memory_at(j));\n+                  }\n+                  mm->set_memory_at(j, phi);\n+                }\n+                Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+                igvn.register_new_node_with_optimizer(base_phi);\n+                for (uint k = 1; k < m->req(); k++) {\n+                  base_phi->init_req(k, m->in(k)->as_MergeMem()->base_memory());\n+                }\n+                mm->set_base_memory(base_phi);\n+              }\n+            } else {\n+              \/\/ This is a MemBarCPUOrder node from\n+              \/\/ Parse::array_load()\/Parse::array_store(), in the\n+              \/\/ branch that handles flat arrays hidden under\n+              \/\/ an Object[] array. We also need one new membar per\n+              \/\/ new alias to keep the unknown access that the\n+              \/\/ membars protect properly ordered with accesses to\n+              \/\/ known flat array.\n+              assert(m->is_Proj(), \"projection expected\");\n+              Node* ctrl = m->in(0)->in(TypeFunc::Control);\n+              igvn.replace_input_of(m->in(0), TypeFunc::Control, top());\n+              for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                const TypePtr* adr_type = get_adr_type(j);\n+                if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                  continue;\n+                }\n+                MemBarNode* mb = new MemBarCPUOrderNode(this, j, nullptr);\n+                igvn.register_new_node_with_optimizer(mb);\n+                Node* mem = mm->memory_at(j);\n+                mb->init_req(TypeFunc::Control, ctrl);\n+                mb->init_req(TypeFunc::Memory, mem);\n+                ctrl = new ProjNode(mb, TypeFunc::Control);\n+                igvn.register_new_node_with_optimizer(ctrl);\n+                mem = new ProjNode(mb, TypeFunc::Memory);\n+                igvn.register_new_node_with_optimizer(mem);\n+                mm->set_memory_at(j, mem);\n+              }\n+              igvn.replace_node(m->in(0)->as_Multi()->proj_out(TypeFunc::Control), ctrl);\n+            }\n+            if (idx < m->req()-1) {\n+              idx += 1;\n+              stack.set_index(idx);\n+              n = m->in(idx);\n+              break;\n+            }\n+            \/\/ Take care of place holder nodes\n+            if (m->has_out_with(Op_Node)) {\n+              Node* place_holder = m->find_out_with(Op_Node);\n+              if (place_holder != nullptr) {\n+                Node* mm_clone = mm->clone();\n+                igvn.register_new_node_with_optimizer(mm_clone);\n+                Node* hook = new Node(1);\n+                hook->init_req(0, mm);\n+                igvn.replace_node(place_holder, mm_clone);\n+                hook->destruct(&igvn);\n+              }\n+              assert(!m->has_out_with(Op_Node), \"place holder should be gone now\");\n+            }\n+            stack.pop();\n+          }\n+        }\n+      } while(stack.size() > 0);\n+      \/\/ Fix the memory state at the MergeMem we started from\n+      igvn.rehash_node_delayed(current);\n+      for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+        const TypePtr* adr_type = get_adr_type(j);\n+        if (!adr_type->isa_aryptr() || !adr_type->is_flat()) {\n+          continue;\n+        }\n+        current->set_memory_at(j, mm);\n+      }\n+      current->set_memory_at(index, current->base_memory());\n+    }\n+    igvn.optimize();\n+  }\n+  print_method(PHASE_SPLIT_INLINES_ARRAY, 2);\n+#ifdef ASSERT\n+  if (!_flat_accesses_share_alias) {\n+    wq.clear();\n+    wq.push(root());\n+    for (uint i = 0; i < wq.size(); i++) {\n+      Node* n = wq.at(i);\n+      assert(n->adr_type() != TypeAryPtr::INLINES, \"should have been removed from the graph\");\n+      for (uint j = 0; j < n->req(); j++) {\n+        Node* m = n->in(j);\n+        if (m != nullptr) {\n+          wq.push(m);\n+        }\n+      }\n+    }\n+  }\n+#endif\n+}\n+\n@@ -2018,1 +2486,1 @@\n-        if (!live_locals.at(i) && !local->is_top() && local != lhs && local!= rhs) {\n+        if (!live_locals.at(i) && !local->is_top() && local != lhs && local != rhs) {\n@@ -2033,1 +2501,1 @@\n-    \/\/ keep the mondified trap for late query\n+    \/\/ keep the modified trap for late query\n@@ -2239,1 +2707,4 @@\n-  assert(_modified_nodes == nullptr, \"not allowed\");\n+#ifdef ASSERT\n+  Unique_Node_List* modified_nodes = _modified_nodes;\n+  _modified_nodes = nullptr;\n+#endif\n@@ -2252,0 +2723,1 @@\n+  DEBUG_ONLY( _modified_nodes = modified_nodes; )\n@@ -2395,0 +2867,5 @@\n+  \/\/ Process inline type nodes now that all inlining is over\n+  process_inline_types(igvn);\n+\n+  adjust_flat_array_access_aliases(igvn);\n+\n@@ -2397,0 +2874,11 @@\n+  if (C->macro_count() > 0) {\n+    \/\/ Eliminate some macro nodes before EA to reduce analysis pressure\n+    PhaseMacroExpand mexp(igvn);\n+    mexp.eliminate_macro_nodes(\/* eliminate_locks= *\/ false);\n+    if (failing()) {\n+      return;\n+    }\n+    igvn.set_delay_transform(false);\n+    print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);\n+  }\n+\n@@ -2407,1 +2895,14 @@\n-      if (failing())  return;\n+      if (failing()) {\n+        return;\n+      }\n+      print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);\n+      if (C->macro_count() > 0) {\n+        \/\/ Eliminate some macro nodes before EA to reduce analysis pressure\n+        PhaseMacroExpand mexp(igvn);\n+        mexp.eliminate_macro_nodes(\/* eliminate_locks= *\/ false);\n+        if (failing()) {\n+          return;\n+        }\n+        igvn.set_delay_transform(false);\n+        print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);\n+      }\n@@ -2409,0 +2910,1 @@\n+\n@@ -2410,1 +2912,0 @@\n-    print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);\n@@ -2428,1 +2929,3 @@\n-        if (failing()) return;\n+        if (failing()) {\n+          return;\n+        }\n@@ -2432,3 +2935,0 @@\n-        igvn.optimize();\n-        if (failing()) return;\n-\n@@ -2525,0 +3025,8 @@\n+  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n+\n+  if (_late_inlines.length() > 0) {\n+    \/\/ More opportunities to optimize virtual and MH calls.\n+    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n+    process_late_inline_calls_no_inline(igvn);\n+  }\n+\n@@ -2527,0 +3035,7 @@\n+    PhaseMacroExpand mex(igvn);\n+    \/\/ Last attempt to eliminate macro nodes.\n+    mex.eliminate_macro_nodes();\n+    if (failing()) {\n+      return;\n+    }\n+\n@@ -2528,1 +3043,0 @@\n-    PhaseMacroExpand  mex(igvn);\n@@ -2548,0 +3062,4 @@\n+  \/\/ Process inline type nodes again and remove them. From here\n+  \/\/ on we don't need to keep track of field values anymore.\n+  process_inline_types(igvn, \/* remove= *\/ true);\n+\n@@ -2564,0 +3082,1 @@\n+  DEBUG_ONLY( _late_inlines.clear(); )\n@@ -2566,9 +3085,0 @@\n-\n-  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n-\n-  if (_late_inlines.length() > 0) {\n-    \/\/ More opportunities to optimize virtual and MH calls.\n-    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n-    process_late_inline_calls_no_inline(igvn);\n-    if (failing())  return;\n-  }\n@@ -3320,1 +3830,1 @@\n-      for (unsigned int i = TypeFunc::Parms; i < call->tf()->domain()->cnt(); i++) {\n+      for (unsigned int i = TypeFunc::Parms; i < call->tf()->domain_sig()->cnt(); i++) {\n@@ -3366,0 +3876,1 @@\n+  case Op_StoreLSpecial:\n@@ -3918,0 +4429,5 @@\n+  case Op_InlineType: {\n+    n->dump(-1);\n+    assert(false, \"inline type node was not removed\");\n+    break;\n+  }\n@@ -4293,2 +4809,2 @@\n-      if (accessing_method->is_static_initializer() ||\n-          accessing_method->is_object_initializer() ||\n+      if (accessing_method->is_class_initializer() ||\n+          accessing_method->is_object_constructor() ||\n@@ -4302,1 +4818,1 @@\n-      if (accessing_method->is_static_initializer()) {\n+      if (accessing_method->is_class_initializer()) {\n@@ -4372,0 +4888,1 @@\n+               (n->is_Allocate() && i >= AllocateNode::InlineType) ||\n@@ -4374,1 +4891,1 @@\n-              \"only region, phi, arraycopy, unlock or membar nodes have null data edges\");\n+              \"only region, phi, arraycopy, allocate, unlock or membar nodes have null data edges\");\n@@ -4525,0 +5042,7 @@\n+\n+    \/\/ Do not fold the subtype check to an array klass pointer comparison for null-able inline type arrays\n+    \/\/ because null-free [LMyValue <: null-able [LMyValue but the klasses are different. Perform a full test.\n+    if (!superk->is_aryklassptr()->is_null_free() && superk->is_aryklassptr()->elem()->isa_instklassptr() &&\n+        superk->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->is_inlinetype()) {\n+      return SSC_full_test;\n+    }\n@@ -5008,0 +5532,21 @@\n+Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {\n+  const TypeInstPtr* ta = phase->type(a)->isa_instptr();\n+  const TypeInstPtr* tb = phase->type(b)->isa_instptr();\n+  if (!EnableValhalla || ta == nullptr || tb == nullptr ||\n+      ta->is_zero_type() || tb->is_zero_type() ||\n+      !ta->can_be_inline_type() || !tb->can_be_inline_type()) {\n+    \/\/ Use old acmp if one operand is null or not an inline type\n+    return new CmpPNode(a, b);\n+  } else if (ta->is_inlinetypeptr() || tb->is_inlinetypeptr()) {\n+    \/\/ We know that one operand is an inline type. Therefore,\n+    \/\/ new acmp will only return true if both operands are nullptr.\n+    \/\/ Check if both operands are null by or'ing the oops.\n+    a = phase->transform(new CastP2XNode(nullptr, a));\n+    b = phase->transform(new CastP2XNode(nullptr, b));\n+    a = phase->transform(new OrXNode(a, b));\n+    return new CmpXNode(a, phase->MakeConX(0));\n+  }\n+  \/\/ Use new acmp\n+  return nullptr;\n+}\n+\n@@ -5363,0 +5908,2 @@\n+  } else if (bt == T_FLOAT) {\n+    result = new MoveI2FNode(value);\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":608,"deletions":61,"binary":false,"changes":669,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+class CallNode;\n@@ -99,0 +100,1 @@\n+class InlineTypeNode;\n@@ -335,0 +337,1 @@\n+  bool                  _has_circular_inline_type; \/\/ True if method loads an inline type with a circular, non-flat field\n@@ -363,0 +366,3 @@\n+  bool                  _has_flat_accesses;     \/\/ Any known flat array accesses?\n+  bool                  _flat_accesses_share_alias; \/\/ Initially all flat array share a single slice\n+  bool                  _scalarize_in_safepoints; \/\/ Scalarize inline types in safepoint debug info\n@@ -381,0 +387,1 @@\n+  GrowableArray<Node*>  _inline_type_nodes;     \/\/ List of InlineType nodes\n@@ -614,0 +621,2 @@\n+  bool              has_circular_inline_type() const { return _has_circular_inline_type; }\n+  void          set_has_circular_inline_type(bool z) { _has_circular_inline_type = z; }\n@@ -646,0 +655,10 @@\n+  void          set_flat_accesses()              { _has_flat_accesses = true; }\n+  bool          flat_accesses_share_alias() const { return _flat_accesses_share_alias; }\n+  void          set_flat_accesses_share_alias(bool z) { _flat_accesses_share_alias = z; }\n+  bool          scalarize_in_safepoints() const { return _scalarize_in_safepoints; }\n+  void          set_scalarize_in_safepoints(bool z) { _scalarize_in_safepoints = z; }\n+\n+  \/\/ Support for scalarized inline type calling convention\n+  bool              has_scalarized_args() const  { return _method != nullptr && _method->has_scalarized_args(); }\n+  bool              needs_stack_repair()  const  { return _method != nullptr && _method->get_Method()->c2_needs_stack_repair(); }\n+\n@@ -781,0 +800,7 @@\n+  \/\/ Keep track of inline type nodes for later processing\n+  void add_inline_type(Node* n);\n+  void remove_inline_type(Node* n);\n+  void process_inline_types(PhaseIterGVN &igvn, bool remove = false);\n+\n+  void adjust_flat_array_access_aliases(PhaseIterGVN& igvn);\n+\n@@ -963,1 +989,1 @@\n-  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = nullptr) { return find_alias_type(adr_type, false, field); }\n+  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = nullptr, bool uncached = false) { return find_alias_type(adr_type, false, field, uncached); }\n@@ -967,1 +993,1 @@\n-  int               get_alias_index(const TypePtr* at)  { return alias_type(at)->index(); }\n+  int               get_alias_index(const TypePtr* at, bool uncached = false) { return alias_type(at, nullptr, uncached)->index(); }\n@@ -1209,1 +1235,1 @@\n-  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field);\n+  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field, bool uncached = false);\n@@ -1299,1 +1325,3 @@\n-  \/\/ Auxiliary methods for randomized fuzzing\/stressing\n+  Node* optimize_acmp(PhaseGVN* phase, Node* a, Node* b);\n+\n+  \/\/ Auxiliary method for randomized fuzzing\/stressing\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":32,"deletions":4,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -236,1 +236,1 @@\n-      for (int i = node->req()-1; i >= 0; --i) {\n+      for (int i = node->len()-1; i >= 0; --i) {\n@@ -1593,0 +1593,3 @@\n+      case Op_CastI2N:\n+        early->add_inst(self);\n+        continue;\n","filename":"src\/hotspot\/share\/opto\/gcm.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -229,0 +229,1 @@\n+    case Op_StoreLSpecial:\n@@ -318,1 +319,1 @@\n-        if (offset == Type::OffsetBot || tptr->_offset == Type::OffsetBot)\n+        if (offset == Type::OffsetBot || tptr->offset() == Type::OffsetBot)\n@@ -320,1 +321,1 @@\n-        offset += tptr->_offset; \/\/ correct if base is offsetted\n+        offset += tptr->offset(); \/\/ correct if base is offsetted\n@@ -365,1 +366,5 @@\n-      Block *inb = get_block_for_node(mach->in(j));\n+      Block* inb = get_block_for_node(mach->in(j));\n+      if (mach->in(j)->is_Con() && mach->in(j)->req() == 1 && inb == get_block_for_node(mach)) {\n+        \/\/ Ignore constant loads scheduled in the same block (we can simply hoist them as well)\n+        continue;\n+      }\n@@ -438,0 +443,21 @@\n+  \/\/ Hoist constant load inputs as well.\n+  for (uint i = 1; i < best->req(); ++i) {\n+    Node* n = best->in(i);\n+    if (n->is_Con() && get_block_for_node(n) == get_block_for_node(best)) {\n+      get_block_for_node(n)->find_remove(n);\n+      block->add_inst(n);\n+      map_node_to_block(n, block);\n+      \/\/ Constant loads may kill flags (for example, when XORing a register).\n+      \/\/ Check for flag-killing projections that also need to be hoisted.\n+      for (DUIterator_Fast jmax, j = n->fast_outs(jmax); j < jmax; j++) {\n+        Node* proj = n->fast_out(j);\n+        if (proj->is_MachProj()) {\n+          get_block_for_node(proj)->find_remove(proj);\n+          block->add_inst(proj);\n+          map_node_to_block(proj, block);\n+        }\n+      }\n+    }\n+  }\n+\n+\n@@ -746,0 +772,1 @@\n+        case Op_StoreLSpecial:\n@@ -913,1 +940,1 @@\n-  uint r_cnt = mcall->tf()->range()->cnt();\n+  uint r_cnt = mcall->tf()->range_cc()->cnt();\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":31,"deletions":4,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -193,0 +193,12 @@\n+const Type* FastLockNode::Value(PhaseGVN* phase) const {\n+  const Type* in1_t = phase->type(in(1));\n+  if (in1_t == Type::TOP) {\n+    return Type::TOP;\n+  }\n+  if (in1_t->is_inlinetypeptr()) {\n+    \/\/ Locking on inline types always fails\n+    return TypeInt::CC_GT;\n+  }\n+  return TypeInt::CC;\n+}\n+\n@@ -212,0 +224,6 @@\n+  {\n+    \/\/ Synchronizing on an inline type is not allowed\n+    BuildCutout unless(this, inline_type_test(obj, \/* is_inline = *\/ false), PROB_MAX);\n+    uncommon_trap_exact(Deoptimization::Reason_class_check, Deoptimization::Action_none);\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/locknode.cpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -96,1 +96,1 @@\n-           \"incorrect kind for Local transitioni: %s\", _kind_name[(int)_kind]);\n+           \"incorrect kind for Local transition: %s\", _kind_name[(int)_kind]);\n@@ -148,1 +148,1 @@\n-  virtual const Type* Value(PhaseGVN* phase) const { return TypeInt::CC; }\n+  virtual const Type* Value(PhaseGVN* phase) const;\n","filename":"src\/hotspot\/share\/opto\/locknode.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -419,0 +419,16 @@\n+  if (tp->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to extract the field being accessed from\n+    \/\/ the address computation\n+    if (offset == Type::OffsetBot) {\n+      Node* base;\n+      Node* index;\n+      const MachOper* oper = memory_inputs(base, index);\n+      if (oper != (MachOper*)-1) {\n+        offset = oper->constant_disp();\n+        return tp->is_aryptr()->add_field_offset_and_offset(offset)->add_offset(Type::OffsetBot);\n+      }\n+    }\n+    return tp->is_aryptr()->add_field_offset_and_offset(offset);\n+  }\n+\n@@ -483,0 +499,5 @@\n+  \/\/ Never rematerialize CastI2N because it might \"hide\" narrow oops from a safepoint\n+  if (ideal_Opcode() == Op_CastI2N) {\n+    return false;\n+  }\n+\n@@ -712,2 +733,2 @@\n-const Type *MachCallNode::bottom_type() const { return tf()->range(); }\n-const Type* MachCallNode::Value(PhaseGVN* phase) const { return tf()->range(); }\n+const Type *MachCallNode::bottom_type() const { return tf()->range_cc(); }\n+const Type* MachCallNode::Value(PhaseGVN* phase) const { return tf()->range_cc(); }\n@@ -724,2 +745,1 @@\n-#ifndef _LP64\n-  if (tf()->range()->cnt() == TypeFunc::Parms) {\n+  if (tf()->range_sig()->cnt() == TypeFunc::Parms) {\n@@ -741,1 +761,0 @@\n-#endif\n@@ -747,1 +766,1 @@\n-  const TypeTuple *r = tf()->range();\n+  const TypeTuple *r = tf()->range_sig();\n@@ -752,0 +771,4 @@\n+bool MachCallNode::returns_scalarized() const {\n+  return tf()->returns_inline_type_as_fields();\n+}\n+\n@@ -756,1 +779,6 @@\n-  if (idx < tf()->domain()->cnt()) {\n+  if (entry_point() == nullptr && idx == TypeFunc::Parms) {\n+    \/\/ Null entry point is a special cast where the target of the call\n+    \/\/ is in a register.\n+    return MachNode::in_RegMask(idx);\n+  }\n+  if (idx < tf()->domain_sig()->cnt()) {\n@@ -789,1 +817,1 @@\n-  if (idx < tf()->domain()->cnt()) {\n+  if (idx < tf()->domain_cc()->cnt()) {\n","filename":"src\/hotspot\/share\/opto\/machnode.cpp","additions":36,"deletions":8,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+class MachVEPNode;\n@@ -512,0 +513,30 @@\n+\/\/------------------------------MachVEPNode-----------------------------------\n+\/\/ Machine Inline Type Entry Point Node\n+class MachVEPNode : public MachIdealNode {\n+public:\n+  Label* _verified_entry;\n+\n+  MachVEPNode(Label* verified_entry, bool verified, bool receiver_only) :\n+    _verified_entry(verified_entry),\n+    _verified(verified),\n+    _receiver_only(receiver_only) {\n+    init_class_id(Class_MachVEP);\n+  }\n+  virtual bool cmp(const Node &n) const {\n+    return (_verified_entry == ((MachVEPNode&)n)._verified_entry) &&\n+           (_verified == ((MachVEPNode&)n)._verified) &&\n+           (_receiver_only == ((MachVEPNode&)n)._receiver_only) &&\n+           MachIdealNode::cmp(n);\n+  }\n+  virtual uint size_of() const { return sizeof(*this); }\n+  virtual void emit(C2_MacroAssembler *masm, PhaseRegAlloc* ra_) const;\n+\n+#ifndef PRODUCT\n+  virtual const char* Name() const { return \"InlineType Entry-Point\"; }\n+  virtual void format(PhaseRegAlloc*, outputStream* st) const;\n+#endif\n+private:\n+  bool   _verified;\n+  bool   _receiver_only;\n+};\n+\n@@ -518,1 +549,0 @@\n-  virtual uint size(PhaseRegAlloc *ra_) const;\n@@ -530,1 +560,9 @@\n-  MachPrologNode( ) {}\n+  Label* _verified_entry;\n+\n+  MachPrologNode(Label* verified_entry) : _verified_entry(verified_entry) {\n+    init_class_id(Class_MachProlog);\n+  }\n+  virtual bool cmp(const Node &n) const {\n+    return (_verified_entry == ((MachPrologNode&)n)._verified_entry) && MachIdealNode::cmp(n);\n+  }\n+  virtual uint size_of() const { return sizeof(*this); }\n@@ -532,1 +570,0 @@\n-  virtual uint size(PhaseRegAlloc *ra_) const;\n@@ -549,1 +586,0 @@\n-  virtual uint size(PhaseRegAlloc *ra_) const;\n@@ -941,1 +977,1 @@\n-  NOT_LP64(bool return_value_is_used() const;)\n+  bool return_value_is_used() const;\n@@ -945,0 +981,1 @@\n+  bool returns_scalarized() const;\n","filename":"src\/hotspot\/share\/opto\/machnode.hpp","additions":42,"deletions":5,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -184,0 +184,45 @@\n+\/\/ Array of RegMask, one per returned values (inline type instances can\n+\/\/ be returned as multiple return values, one per field)\n+RegMask* Matcher::return_values_mask(const TypeFunc* tf) {\n+  const TypeTuple* range = tf->range_cc();\n+  uint cnt = range->cnt() - TypeFunc::Parms;\n+  if (cnt == 0) {\n+    return nullptr;\n+  }\n+  RegMask* mask = NEW_RESOURCE_ARRAY(RegMask, cnt);\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, cnt);\n+  VMRegPair* vm_parm_regs = NEW_RESOURCE_ARRAY(VMRegPair, cnt);\n+  for (uint i = 0; i < cnt; i++) {\n+    sig_bt[i] = range->field_at(i+TypeFunc::Parms)->basic_type();\n+    new (mask + i) RegMask();\n+  }\n+\n+  int regs = SharedRuntime::java_return_convention(sig_bt, vm_parm_regs, cnt);\n+  if (regs <= 0) {\n+    \/\/ We ran out of registers to store the null marker for a nullable inline type return.\n+    \/\/ Since it is only set in the 'call_epilog', we can simply put it on the stack.\n+    assert(tf->returns_inline_type_as_fields(), \"should have been tested during graph construction\");\n+    \/\/ TODO 8284443 Can we teach the register allocator to reserve a stack slot instead?\n+    \/\/ mask[--cnt] = STACK_ONLY_mask does not work (test with -XX:+StressGCM)\n+    int slot = C->fixed_slots() - 2;\n+    if (C->needs_stack_repair()) {\n+      slot -= 2; \/\/ Account for stack increment value\n+    }\n+    mask[--cnt].Clear();\n+    mask[cnt].Insert(OptoReg::stack2reg(slot));\n+  }\n+  for (uint i = 0; i < cnt; i++) {\n+    mask[i].Clear();\n+\n+    OptoReg::Name reg1 = OptoReg::as_OptoReg(vm_parm_regs[i].first());\n+    if (OptoReg::is_valid(reg1)) {\n+      mask[i].Insert(reg1);\n+    }\n+    OptoReg::Name reg2 = OptoReg::as_OptoReg(vm_parm_regs[i].second());\n+    if (OptoReg::is_valid(reg2)) {\n+      mask[i].Insert(reg2);\n+    }\n+  }\n+\n+  return mask;\n+}\n@@ -204,15 +249,3 @@\n-  \/\/ Map a Java-signature return type into return register-value\n-  \/\/ machine registers for 0, 1 and 2 returned values.\n-  const TypeTuple *range = C->tf()->range();\n-  if( range->cnt() > TypeFunc::Parms ) { \/\/ If not a void function\n-    \/\/ Get ideal-register return type\n-    uint ireg = range->field_at(TypeFunc::Parms)->ideal_reg();\n-    \/\/ Get machine return register\n-    uint sop = C->start()->Opcode();\n-    OptoRegPair regs = return_value(ireg);\n-\n-    \/\/ And mask for same\n-    _return_value_mask = RegMask(regs.first());\n-    if( OptoReg::is_valid(regs.second()) )\n-      _return_value_mask.Insert(regs.second());\n-  }\n+  \/\/ Map Java-signature return types into return register-value\n+  \/\/ machine registers.\n+  _return_values_mask = return_values_mask(C->tf());\n@@ -226,1 +259,1 @@\n-  const TypeTuple *domain = C->tf()->domain();\n+  const TypeTuple *domain = C->tf()->domain_cc();\n@@ -537,0 +570,1 @@\n+\n@@ -788,1 +822,1 @@\n-  uint ret_edge_cnt = TypeFunc::Parms + ((C->tf()->range()->cnt() == TypeFunc::Parms) ? 0 : 1);\n+  uint ret_edge_cnt = C->tf()->range_cc()->cnt();\n@@ -790,4 +824,3 @@\n-  \/\/ Returns have 0 or 1 returned values depending on call signature.\n-  \/\/ Return register is specified by return_value in the AD file.\n-  if (ret_edge_cnt > TypeFunc::Parms)\n-    ret_rms[TypeFunc::Parms+0] = _return_value_mask;\n+  for (i = TypeFunc::Parms; i < ret_edge_cnt; i++) {\n+    ret_rms[i] = _return_values_mask[i-TypeFunc::Parms];\n+  }\n@@ -865,1 +898,1 @@\n-  int proj_cnt = C->tf()->domain()->cnt();\n+  int proj_cnt = C->tf()->domain_cc()->cnt();\n@@ -1142,1 +1175,5 @@\n-              m = n->in(0)->as_Multi()->match( n->as_Proj(), this );\n+              RegMask* mask = nullptr;\n+              if (n->in(0)->is_Call() && n->in(0)->as_Call()->tf()->returns_inline_type_as_fields()) {\n+                mask = return_values_mask(n->in(0)->as_Call()->tf());\n+              }\n+              m = n->in(0)->as_Multi()->match(n->as_Proj(), this, mask);\n@@ -1278,1 +1315,1 @@\n-    domain = call->tf()->domain();\n+    domain = call->tf()->domain_cc();\n@@ -1359,1 +1396,4 @@\n-  int argcnt = cnt - TypeFunc::Parms;\n+  \/\/ Null entry point is a special cast where the target of the call\n+  \/\/ is in a register.\n+  int adj = (call != nullptr && call->entry_point() == nullptr) ? 1 : 0;\n+  int argcnt = cnt - TypeFunc::Parms - adj;\n@@ -1365,1 +1405,1 @@\n-      sig_bt[i] = domain->field_at(i+TypeFunc::Parms)->basic_type();\n+      sig_bt[i] = domain->field_at(i+TypeFunc::Parms+adj)->basic_type();\n@@ -1406,1 +1446,1 @@\n-      RegMask *rm = &mcall->_in_rms[i+TypeFunc::Parms];\n+      RegMask *rm = &mcall->_in_rms[i+TypeFunc::Parms+adj];\n@@ -1424,1 +1464,1 @@\n-      if (OptoReg::is_valid(reg1))\n+      if (OptoReg::is_valid(reg1)) {\n@@ -1426,0 +1466,1 @@\n+      }\n@@ -1428,1 +1469,1 @@\n-      if (OptoReg::is_valid(reg2))\n+      if (OptoReg::is_valid(reg2)) {\n@@ -1430,0 +1471,1 @@\n+      }\n@@ -1445,1 +1487,1 @@\n-    uint r_cnt = mcall->tf()->range()->cnt();\n+    uint r_cnt = mcall->tf()->range_sig()->cnt();\n@@ -1463,1 +1505,1 @@\n-         (mcall->jvms()->debug_start() + mcall->_jvmadj == mcall->tf()->domain()->cnt()), \"\");\n+         (mcall->jvms()->debug_start() + mcall->_jvmadj == mcall->tf()->domain_cc()->cnt()), \"\");\n@@ -2152,1 +2194,1 @@\n-      for (int i = n->req() - 1; i >= 0; --i) { \/\/ For my children\n+      for (int i = n->len() - 1; i >= 0; --i) { \/\/ For my children\n@@ -2463,0 +2505,7 @@\n+    case Op_ClearArray: {\n+      Node* pair = new BinaryNode(n->in(2), n->in(3));\n+      n->set_req(2, pair);\n+      n->set_req(3, n->in(4));\n+      n->del_req(4);\n+      break;\n+    }\n@@ -2512,0 +2561,8 @@\n+    case Op_StoreLSpecial: {\n+      if (n->req() > (MemNode::ValueIn + 1) && n->in(MemNode::ValueIn + 1) != nullptr) {\n+        Node* pair = new BinaryNode(n->in(MemNode::ValueIn), n->in(MemNode::ValueIn + 1));\n+        n->set_req(MemNode::ValueIn, pair);\n+        n->del_req(MemNode::ValueIn + 1);\n+      }\n+      break;\n+    }\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":89,"deletions":32,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -574,0 +575,3 @@\n+  if (n->is_InlineType()) {\n+    C->add_inline_type(n);\n+  }\n@@ -634,0 +638,3 @@\n+  if (is_InlineType()) {\n+    compile->remove_inline_type(this);\n+  }\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -68,0 +69,1 @@\n+#include \"oops\/refArrayOop.inline.hpp\"\n@@ -416,0 +418,148 @@\n+static void validate_array_arguments(Klass* elmClass, jint len, TRAPS) {\n+  if (len < 0) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Array length is negative\");\n+  }\n+  elmClass->initialize(CHECK);\n+  if (elmClass->is_array_klass() || elmClass->is_identity_class()) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Element class is not a value class\");\n+  }\n+  if (elmClass->is_abstract()) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Element class is abstract\");\n+  }\n+}\n+\n+JVM_ENTRY(jarray, JVM_CopyOfSpecialArray(JNIEnv *env, jarray orig, jint from, jint to))\n+  oop o = JNIHandles::resolve_non_null(orig);\n+  assert(o->is_array(), \"Must be\");\n+  oop array = nullptr;\n+  arrayOop org = (arrayOop)o;\n+  arrayHandle oh(THREAD, org);\n+  ObjArrayKlass* ak = ObjArrayKlass::cast(org->klass());\n+  InlineKlass* vk = InlineKlass::cast(ak->element_klass());\n+  int len = to - from;  \/\/ length of the new array\n+  if (ak->is_null_free_array_klass()) {\n+    if ((len != 0) && (from >= org->length() || to > org->length())) {\n+      THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Copying of null-free array with uninitialized elements\");\n+    }\n+  }\n+  if (org->is_flatArray()) {\n+    FlatArrayKlass* fak = FlatArrayKlass::cast(org->klass());\n+    LayoutKind lk = fak->layout_kind();\n+    ArrayKlass::ArrayProperties props = ArrayKlass::ArrayProperties::DEFAULT;\n+    switch(lk) {\n+      case LayoutKind::ATOMIC_FLAT:\n+        props = ArrayKlass::ArrayProperties::NULL_RESTRICTED;\n+      break;\n+      case LayoutKind::NON_ATOMIC_FLAT:\n+        props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::NULL_RESTRICTED | ArrayKlass::ArrayProperties::NON_ATOMIC);\n+      break;\n+      case LayoutKind::NULLABLE_ATOMIC_FLAT:\n+      props = ArrayKlass::ArrayProperties::NON_ATOMIC;\n+      break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    array = oopFactory::new_flatArray(vk, len, props, lk, CHECK_NULL);\n+    arrayHandle ah(THREAD, (arrayOop)array);\n+    int end = to < oh()->length() ? to : oh()->length();\n+    for (int i = from; i < end; i++) {\n+      void* src = ((flatArrayOop)oh())->value_at_addr(i, fak->layout_helper());\n+      void* dst = ((flatArrayOop)ah())->value_at_addr(i - from, fak->layout_helper());\n+      vk->copy_payload_to_addr(src, dst, lk, false);\n+    }\n+    array = ah();\n+  } else {\n+    ArrayKlass::ArrayProperties props = org->is_null_free_array() ? ArrayKlass::ArrayProperties::NULL_RESTRICTED : ArrayKlass::ArrayProperties::DEFAULT;\n+    array = oopFactory::new_objArray(vk, len, props,  CHECK_NULL);\n+    int end = to < oh()->length() ? to : oh()->length();\n+    for (int i = from; i < end; i++) {\n+      if (i < ((objArrayOop)oh())->length()) {\n+        ((objArrayOop)array)->obj_at_put(i - from, ((objArrayOop)oh())->obj_at(i));\n+      } else {\n+        assert(!ak->is_null_free_array_klass(), \"Must be a nullable array\");\n+        ((objArrayOop)array)->obj_at_put(i - from, nullptr);\n+      }\n+    }\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jarray, JVM_NewNullRestrictedNonAtomicArray(JNIEnv *env, jclass elmClass, jint len, jobject initVal))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  oop init = JNIHandles::resolve(initVal);\n+  if (init == nullptr) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Initial value cannot be null\");\n+  }\n+  Handle init_h(THREAD, init);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  if (klass != init_h()->klass()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Type mismatch between array and initial value\");\n+  }\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  ArrayKlass::ArrayProperties props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::NON_ATOMIC | ArrayKlass::ArrayProperties::NULL_RESTRICTED);\n+  objArrayOop array = oopFactory::new_objArray(klass, len, props, CHECK_NULL);\n+  for (int i = 0; i < len; i++) {\n+    array->obj_at_put(i, init_h() \/*, CHECK_NULL*\/ );\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jarray, JVM_NewNullRestrictedAtomicArray(JNIEnv *env, jclass elmClass, jint len, jobject initVal))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  oop init = JNIHandles::resolve(initVal);\n+  if (init == nullptr) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Initial value cannot be null\");\n+  }\n+  Handle init_h(THREAD, init);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  if (klass != init_h()->klass()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Type mismatch between array and initial value\");\n+  }\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  ArrayKlass::ArrayProperties props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::NULL_RESTRICTED);\n+  objArrayOop array = oopFactory::new_objArray(klass, len, props, CHECK_NULL);\n+  for (int i = 0; i < len; i++) {\n+    array->obj_at_put(i, init_h() \/*, CHECK_NULL*\/ );\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jarray, JVM_NewNullableAtomicArray(JNIEnv *env, jclass elmClass, jint len))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  klass->initialize(CHECK_NULL);\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  ArrayKlass::ArrayProperties props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::DEFAULT);\n+  objArrayOop array = oopFactory::new_objArray(klass, len, props, CHECK_NULL);\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsFlatArray(JNIEnv *env, jobject obj))\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  return oop->is_flatArray();\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsNullRestrictedArray(JNIEnv *env, jobject obj))\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  return oop->is_null_free_array();\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsAtomicArray(JNIEnv *env, jobject obj))\n+  \/\/ There are multiple cases where an array can\/must support atomic access:\n+  \/\/   - the array is a reference array\n+  \/\/   - the array uses an atomic flat layout: NULLABLE_ATOMIC_FLAT or ATOMIC_FLAT\n+  \/\/   - the array is flat and its component type is naturally atomic\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  if (oop->is_refArray()) return true;\n+  if (oop->is_flatArray()) {\n+    FlatArrayKlass* fak = FlatArrayKlass::cast(oop->klass());\n+    if (fak->layout_kind() == LayoutKind::ATOMIC_FLAT || fak->layout_kind() == LayoutKind::NULLABLE_ATOMIC_FLAT) {\n+      return true;\n+    }\n+    if (fak->element_klass()->is_naturally_atomic()) return true;\n+  }\n+  return false;\n+JVM_END\n@@ -624,2 +774,22 @@\n-  return handle == nullptr ? 0 :\n-         checked_cast<jint>(ObjectSynchronizer::FastHashCode (THREAD, JNIHandles::resolve_non_null(handle)));\n+  if (handle == nullptr) {\n+    return 0;\n+  }\n+  oop obj = JNIHandles::resolve_non_null(handle);\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+      JavaValue result(T_INT);\n+      JavaCallArguments args;\n+      Handle ho(THREAD, obj);\n+      args.push_oop(ho);\n+      methodHandle method(THREAD, Universe::value_object_hash_code_method());\n+      JavaCalls::call(&result, method, &args, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in hashCode\", e, false);\n+        }\n+      }\n+      return result.get_jint();\n+  } else {\n+    return checked_cast<jint>(ObjectSynchronizer::FastHashCode(THREAD, obj));\n+  }\n@@ -673,0 +843,6 @@\n+  if (klass->is_inline_klass()) {\n+    \/\/ Value instances have no identity, so return the current instance instead of allocating a new one\n+    \/\/ Value classes cannot have finalizers, so the method can return immediately\n+    return JNIHandles::make_local(THREAD, obj());\n+  }\n+\n@@ -1169,1 +1345,2 @@\n-    size = InstanceKlass::cast(klass)->local_interfaces()->length();\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    size = ik->local_interfaces()->length();\n@@ -1203,1 +1380,0 @@\n-\n@@ -1684,1 +1860,1 @@\n-    if (want_constructor && !method->is_object_initializer()) {\n+    if (want_constructor && !method->is_object_constructor()) {\n@@ -1688,1 +1864,1 @@\n-        (method->is_object_initializer() || method->is_static_initializer() ||\n+        (method->is_object_constructor() || method->is_class_initializer() ||\n@@ -1716,0 +1892,1 @@\n+        assert(method->is_object_constructor(), \"must be\");\n@@ -1976,1 +2153,1 @@\n-  if (m->is_object_initializer()) {\n+  if (m->is_object_constructor()) {\n@@ -1979,1 +2156,0 @@\n-    \/\/ new_method accepts <clinit> as Method here\n@@ -2426,1 +2602,1 @@\n-  return method->name() == vmSymbols::object_initializer_name();\n+  return method->is_object_constructor();\n@@ -3220,0 +3396,4 @@\n+JVM_LEAF(jboolean, JVM_IsValhallaEnabled(void))\n+  return EnableValhalla ? JNI_TRUE : JNI_FALSE;\n+JVM_END\n+\n@@ -3299,1 +3479,3 @@\n-    objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n+    objArrayHandle args(THREAD, (objArrayOop)JNIHandles::resolve(args0));\n+    assert(args() == nullptr || !args->is_flatArray(), \"args are never flat or are they???\");\n+\n@@ -3319,0 +3501,2 @@\n+  objArrayHandle args(THREAD, (objArrayOop)JNIHandles::resolve(args0));\n+  assert(args() == nullptr || !args->is_flatArray(), \"args are never flat or are they???\");\n@@ -3320,1 +3504,0 @@\n-  objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n@@ -3557,0 +3740,1 @@\n+  refArrayHandle rah(THREAD, (refArrayOop)ah()); \/\/ j.l.Thread is an identity class, arrays are always reference arrays\n@@ -3562,1 +3746,1 @@\n-    oop thread_obj = ah->obj_at(i);\n+    oop thread_obj = rah->obj_at(i);\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":196,"deletions":12,"binary":false,"changes":208,"status":"modified"},{"patch":"@@ -618,2 +618,1 @@\n-    \/\/ At this stage JVM_CONSTANT_UnresolvedClassInError should not be\n-    \/\/ here\n+    \/\/ At this stage JVM_CONSTANT_UnresolvedClassInError should not be here\n@@ -1938,0 +1937,6 @@\n+  \/\/ rewrite constant pool references in the LoadableDescriptors attribute:\n+  if (!rewrite_cp_refs_in_loadable_descriptors_attribute(scratch_class)) {\n+    \/\/ propagate failure back to caller\n+    return false;\n+  }\n+\n@@ -2086,0 +2091,13 @@\n+\/\/ Rewrite constant pool references in the LoadableDescriptors attribute.\n+bool VM_RedefineClasses::rewrite_cp_refs_in_loadable_descriptors_attribute(\n+       InstanceKlass* scratch_class) {\n+\n+  Array<u2>* loadable_descriptors = scratch_class->loadable_descriptors();\n+  assert(loadable_descriptors != nullptr, \"unexpected null loadable_descriptors\");\n+  for (int i = 0; i < loadable_descriptors->length(); i++) {\n+    u2 cp_index = loadable_descriptors->at(i);\n+    loadable_descriptors->at_put(i, find_new_index(cp_index));\n+  }\n+  return true;\n+}\n+\n@@ -3273,0 +3291,8 @@\n+   if (frame_type == 246) {  \/\/ EARLY_LARVAL\n+     \/\/ rewrite_cp_refs in  unset fields and fall through.\n+     rewrite_cp_refs_in_early_larval_stackmaps(stackmap_p, stackmap_end, calc_number_of_entries, frame_type);\n+     \/\/ The larval frames point to the next frame, so advance to the next frame and fall through.\n+     frame_type = *stackmap_p;\n+     stackmap_p++;\n+   }\n+\n@@ -3482,0 +3508,23 @@\n+void VM_RedefineClasses::rewrite_cp_refs_in_early_larval_stackmaps(\n+       address& stackmap_p_ref, address stackmap_end, u2 frame_i,\n+       u1 frame_type) {\n+\n+    u2 num_early_larval_stackmaps = Bytes::get_Java_u2(stackmap_p_ref);\n+    stackmap_p_ref += 2;\n+\n+    for (u2 i = 0; i < num_early_larval_stackmaps; i++) {\n+\n+      u2 name_and_ref_index = Bytes::get_Java_u2(stackmap_p_ref);\n+      u2 new_cp_index = find_new_index(name_and_ref_index);\n+      if (new_cp_index != 0) {\n+        log_debug(redefine, class, stackmap)(\"mapped old name_and_ref_index=%d\", name_and_ref_index);\n+        Bytes::put_Java_u2(stackmap_p_ref, new_cp_index);\n+        name_and_ref_index = new_cp_index;\n+      }\n+      log_debug(redefine, class, stackmap)\n+        (\"frame_i=%u, frame_type=%u, name_and_ref_index=%d\", frame_i, frame_type, name_and_ref_index);\n+\n+      stackmap_p_ref += 2;\n+    }\n+} \/\/ rewrite_cp_refs_in_early_larval_stackmaps\n+\n","filename":"src\/hotspot\/share\/prims\/jvmtiRedefineClasses.cpp","additions":51,"deletions":2,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -62,0 +63,1 @@\n+#include \"oops\/access.hpp\"\n@@ -64,0 +66,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -89,0 +92,1 @@\n+#include \"runtime\/keepStackGCProcessed.hpp\"\n@@ -1949,0 +1953,103 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return nullptr;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(ih->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return nullptr;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayHandle result_array =\n+      oopFactory::new_objArray_handle(vmClasses::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ih->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+WB_END\n+\n+\/\/ Collect Object oops but not value objects...loaded from heap\n+class CollectObjectOops : public BasicOopIterateClosure {\n+  public:\n+  GrowableArray<Handle>* _array;\n+\n+  CollectObjectOops() {\n+      _array = new GrowableArray<Handle>(128);\n+  }\n+\n+  void add_oop(oop o) {\n+    Handle oh = Handle(Thread::current(), o);\n+    if (oh != nullptr && oh->is_inline_type()) {\n+      oh->oop_iterate(this);\n+    } else {\n+      _array->append(oh);\n+    }\n+  }\n+\n+  template <class T> inline void add_oop(T* p) { add_oop(HeapAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    objArrayHandle result_array =\n+        oopFactory::new_objArray_handle(vmClasses::Object_klass(), _array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < _array->length(); i++) {\n+      result_array->obj_at_put(i, _array->at(i)());\n+    }\n+    return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+  }\n+};\n+\n+\/\/ Collect Object oops but not value objects...loaded from frames\n+class CollectFrameObjectOops : public BasicOopIterateClosure {\n+ public:\n+  CollectObjectOops _collect;\n+\n+  template <class T> inline void add_oop(T* p) { _collect.add_oop(RawAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    return _collect.create_jni_result(env, THREAD);\n+  }\n+};\n+\n+\/\/ Collect Object oops for the given oop, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(thread);\n+  Handle objh(thread, JNIHandles::resolve(thing));\n+  CollectObjectOops collectOops;\n+  objh->oop_iterate(&collectOops);\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\/\/ Collect Object oops for the given frame deep, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  KeepStackGCProcessedMark ksgcpm(THREAD);\n+  ResourceMark rm(THREAD);\n+  CollectFrameObjectOops collectOops;\n+  StackFrameStream sfs(thread, true \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, nullptr, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n@@ -2950,0 +3057,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":113,"deletions":0,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -78,0 +78,1 @@\n+#include <string.h>\n@@ -365,0 +366,12 @@\n+bool Arguments::patching_migrated_classes(const char* property, const char* value) {\n+  if (strncmp(property, MODULE_PROPERTY_PREFIX, MODULE_PROPERTY_PREFIX_LEN) == 0) {\n+    const char* property_suffix = property + MODULE_PROPERTY_PREFIX_LEN;\n+    if (matches_property_suffix(property_suffix, PATCH, PATCH_LEN)) {\n+      if (strcmp(value, \"java.base-valueclasses.jar\")) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -1806,1 +1819,0 @@\n-static unsigned int patch_mod_count = 0;\n@@ -1813,1 +1825,1 @@\n-  if (!CDSConfig::check_vm_args_consistency(patch_mod_javabase, mode_flag_cmd_line)) {\n+  if (!CDSConfig::check_vm_args_consistency(mode_flag_cmd_line)) {\n@@ -2062,1 +2074,1 @@\n-      add_patch_mod_prefix(module_name, module_equal + 1);\n+      add_patch_mod_prefix(module_name, module_equal + 1, false \/* no append *\/, false \/* no cds *\/);\n@@ -2064,3 +2076,0 @@\n-      if (!create_numbered_module_property(\"jdk.module.patch\", patch_mod_tail, patch_mod_count++)) {\n-        return JNI_ENOMEM;\n-      }\n@@ -2074,0 +2083,64 @@\n+\/\/ VALUECLASS_STR must match string used in the build\n+#define VALUECLASS_STR \"valueclasses\"\n+#define VALUECLASS_JAR \"-\" VALUECLASS_STR \".jar\"\n+\n+\/\/ Finalize --patch-module args and --enable-preview related to value class module patches.\n+\/\/ Create all numbered properties passing module patches.\n+int Arguments::finalize_patch_module() {\n+  \/\/ If --enable-preview and EnableValhalla is true, each module may have value classes that\n+  \/\/ are to be patched into the module.\n+  \/\/ For each <module>-valueclasses.jar in <JAVA_HOME>\/lib\/valueclasses\/\n+  \/\/ appends the equivalent of --patch-module <module>=<JAVA_HOME>\/lib\/valueclasses\/<module>-valueclasses.jar\n+  if (enable_preview() && EnableValhalla) {\n+    char * valueclasses_dir = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+    const char * fileSep = os::file_separator();\n+\n+    jio_snprintf(valueclasses_dir, JVM_MAXPATHLEN, \"%s%slib%s\" VALUECLASS_STR \"%s\",\n+                 Arguments::get_java_home(), fileSep, fileSep, fileSep);\n+    DIR* dir = os::opendir(valueclasses_dir);\n+    if (dir != nullptr) {\n+      char * module_name = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+      char * path = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+\n+      for (dirent * entry = os::readdir(dir); entry != nullptr; entry = os::readdir(dir)) {\n+        \/\/ Test if file ends-with \"-valueclasses.jar\"\n+        int len = (int)strlen(entry->d_name) - (sizeof(VALUECLASS_JAR) - 1);\n+        if (len <= 0 || strcmp(&entry->d_name[len], VALUECLASS_JAR) != 0) {\n+          continue;         \/\/ too short or not the expected suffix\n+        }\n+\n+        strcpy(module_name, entry->d_name);\n+        module_name[len] = '\\0';     \/\/ truncate to just module-name\n+\n+        jio_snprintf(path, JVM_MAXPATHLEN, \"%s%s\", valueclasses_dir, &entry->d_name);\n+        add_patch_mod_prefix(module_name, path, true \/* append *\/, true \/* cds OK*\/);\n+        log_info(class)(\"--enable-preview appending value classes for module %s: %s\", module_name, entry->d_name);\n+      }\n+      FreeHeap(module_name);\n+      FreeHeap(path);\n+      os::closedir(dir);\n+    }\n+    FreeHeap(valueclasses_dir);\n+  }\n+\n+  \/\/ Create numbered properties for each module that has been patched either\n+  \/\/ by --patch-module or --enable-preview\n+  \/\/ Format is \"jdk.module.patch.<n>=<module_name>=<path>\"\n+  if (_patch_mod_prefix != nullptr) {\n+    char * prop_value = AllocateHeap(JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, mtArguments);\n+    unsigned int patch_mod_count = 0;\n+\n+    for (GrowableArrayIterator<ModulePatchPath *> it = _patch_mod_prefix->begin();\n+            it != _patch_mod_prefix->end(); ++it) {\n+      jio_snprintf(prop_value, JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, \"%s=%s\",\n+                   (*it)->module_name(), (*it)->path_string());\n+      if (!create_numbered_module_property(\"jdk.module.patch\", prop_value, patch_mod_count++)) {\n+        FreeHeap(prop_value);\n+        return JNI_ENOMEM;\n+      }\n+    }\n+    FreeHeap(prop_value);\n+  }\n+  return JNI_OK;\n+}\n+\n@@ -2342,0 +2415,4 @@\n+      \/\/ --enable-preview enables Valhalla, EnableValhalla VM option will eventually be removed before integration\n+      if (FLAG_SET_CMDLINE(EnableValhalla, true) != JVMFlag::SUCCESS) {\n+        return JNI_EINVAL;\n+      }\n@@ -2845,10 +2922,5 @@\n-void Arguments::add_patch_mod_prefix(const char* module_name, const char* path) {\n-  \/\/ For java.base check for duplicate --patch-module options being specified on the command line.\n-  \/\/ This check is only required for java.base, all other duplicate module specifications\n-  \/\/ will be checked during module system initialization.  The module system initialization\n-  \/\/ will throw an ExceptionInInitializerError if this situation occurs.\n-  if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n-    if (patch_mod_javabase) {\n-      vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n-    } else {\n-      patch_mod_javabase = true;\n+void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool allow_append, bool allow_cds) {\n+  if (!allow_cds) {\n+    CDSConfig::set_module_patching_disables_cds();\n+    if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+      CDSConfig::set_java_base_module_patching_disables_cds();\n@@ -2863,1 +2935,18 @@\n-  _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  \/\/ Scan patches for matching module\n+  int i = _patch_mod_prefix->find_if([&](ModulePatchPath* patch) {\n+    return (strcmp(module_name, patch->module_name()) == 0);\n+  });\n+  if (i == -1) {\n+    _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  } else {\n+    if (allow_append) {\n+      \/\/ append path to existing module entry\n+      _patch_mod_prefix->at(i)->append_path(path);\n+    } else {\n+      if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+        vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n+      } else {\n+        vm_exit_during_initialization(\"Cannot specify a module more than once to --patch-module\", module_name);\n+      }\n+    }\n+  }\n@@ -2976,1 +3065,2 @@\n-  if (!check_vm_args_consistency()) {\n+  \/\/ finalize --module-patch and related --enable-preview\n+  if (finalize_patch_module() != JNI_OK) {\n@@ -2980,0 +3070,3 @@\n+  if (!check_vm_args_consistency()) {\n+    return JNI_ERR;\n+  }\n@@ -3865,0 +3958,12 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !CDSConfig::is_dumping_archive() && !UseSharedSpaces)) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported.\n+    \/\/ Also these aren't useful in -Xint. However, don't disable them when dumping or using\n+    \/\/ the CDS archive, as the values must match between dumptime and runtime.\n+    FLAG_SET_DEFAULT(InlineTypePassFieldsAsArgs, false);\n+    FLAG_SET_DEFAULT(InlineTypeReturnedAsFields, false);\n+  }\n+  if (!UseNonAtomicValueFlattening && !UseNullableValueFlattening && !UseAtomicValueFlattening) {\n+    \/\/ Flattening is disabled\n+    FLAG_SET_DEFAULT(UseArrayFlattening, false);\n+    FLAG_SET_DEFAULT(UseFieldFlattening, false);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":123,"deletions":18,"binary":false,"changes":141,"status":"modified"},{"patch":"@@ -816,1 +816,1 @@\n-  develop(bool, PrintFieldLayout, false,                                    \\\n+  product(bool, PrintFieldLayout, false, DIAGNOSTIC,                        \\\n@@ -819,0 +819,27 @@\n+  product(bool, PrintInlineLayout, false, DIAGNOSTIC,                       \\\n+          \"Print field layout for each inline type or class with inline fields\") \\\n+                                                                            \\\n+  product(bool, PrintFlatArrayLayout, false, DIAGNOSTIC,                    \\\n+          \"Print array layout for each inline type array\")                  \\\n+                                                                            \\\n+  product(bool, UseArrayFlattening, true,                                   \\\n+          \"Allow the VM to flatten arrays\")                                 \\\n+                                                                            \\\n+  product(bool, UseFieldFlattening, true,                                   \\\n+          \"Allow the VM to flatten value fields\")                           \\\n+                                                                            \\\n+  product(bool, UseNonAtomicValueFlattening, true,                          \\\n+          \"Allow the JVM to flatten some non-atomic null-free values\")      \\\n+                                                                            \\\n+  product(bool, UseNullableValueFlattening, true,                           \\\n+          \"Allow the JVM to flatten some nullable values\")                  \\\n+                                                                            \\\n+  product(bool, UseAtomicValueFlattening, true,                             \\\n+          \"Allow the JVM to flatten some atomic values\")                    \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxOops, 4,                                 \\\n+          \"Max nof embedded object references in an inline type to flatten, <0 no limit\")  \\\n+                                                                            \\\n+  develop(ccstrlist, PrintInlineKlassFields, \"\",                            \\\n+          \"Print fields collected by InlineKlass::collect_fields\")          \\\n+                                                                            \\\n@@ -1774,0 +1801,3 @@\n+  product(bool, IgnoreAssertUnsetFields, false, DIAGNOSTIC,                           \\\n+          \"Ignore assert_unset_fields\")                                     \\\n+                                                                            \\\n@@ -1945,0 +1975,20 @@\n+  product(bool, EnableValhalla, true,                                       \\\n+          \"Enable experimental Valhalla features\")                          \\\n+                                                                            \\\n+  product_pd(bool, InlineTypePassFieldsAsArgs,                              \\\n+          \"Pass each inline type field as an argument at calls\")            \\\n+                                                                            \\\n+  product_pd(bool, InlineTypeReturnedAsFields,                              \\\n+          \"Return fields instead of an inline type reference\")              \\\n+                                                                            \\\n+  develop(bool, StressCallingConvention, false,                             \\\n+          \"Stress the scalarized calling convention.\")                      \\\n+                                                                            \\\n+  develop(bool, PreloadClasses, true,                                       \\\n+          \"Preloading all classes from the LoadableDescriptors attribute\")  \\\n+                                                                            \\\n+  product(ccstrlist, ForceNonTearable, \"\", DIAGNOSTIC,                      \\\n+          \"List of inline classes which are forced to be atomic \"           \\\n+          \"(whitespace and commas separate names, \"                         \\\n+          \"and leading and trailing stars '*' are wildcards)\")              \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":51,"deletions":1,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"memory\/oopFactory.hpp\"\n@@ -51,0 +52,3 @@\n+#include \"oops\/access.hpp\"\n+#include \"oops\/fieldStreams.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -54,0 +58,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -72,0 +77,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -1197,0 +1203,15 @@\n+  \/\/ Substitutability test implementation piggy backs on static call resolution\n+  Bytecodes::Code code = caller->java_code_at(bci);\n+  if (code == Bytecodes::_if_acmpeq || code == Bytecodes::_if_acmpne) {\n+    bc = Bytecodes::_invokestatic;\n+    methodHandle attached_method(THREAD, extract_attached_method(vfst));\n+    assert(attached_method.not_null(), \"must have attached method\");\n+    vmClasses::ValueObjectMethods_klass()->initialize(CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, false, CHECK_NH);\n+#ifdef ASSERT\n+    Method* is_subst = vmClasses::ValueObjectMethods_klass()->find_method(vmSymbols::isSubstitutable_name(), vmSymbols::object_object_boolean_signature());\n+    assert(callinfo.selected_method() == is_subst, \"must be isSubstitutable method\");\n+#endif\n+    return receiver;\n+  }\n+\n@@ -1232,0 +1253,6 @@\n+    } else {\n+      assert(attached_method->has_scalarized_args(), \"invalid use of attached method\");\n+      if (!attached_method->method_holder()->is_inline_klass()) {\n+        \/\/ Ignore the attached method in this case to not confuse below code\n+        attached_method = methodHandle(current, nullptr);\n+      }\n@@ -1240,0 +1267,1 @@\n+  bool check_null_and_abstract = true;\n@@ -1253,2 +1281,3 @@\n-    if (attached_method.is_null()) {\n-      Method* callee = bytecode.static_target(CHECK_NH);\n+    Method* callee = attached_method();\n+    if (callee == nullptr) {\n+      callee = bytecode.static_target(CHECK_NH);\n@@ -1259,7 +1288,17 @@\n-\n-    \/\/ Retrieve from a compiled argument list\n-    receiver = Handle(current, callerFrame.retrieve_receiver(&reg_map2));\n-    assert(oopDesc::is_oop_or_null(receiver()), \"\");\n-\n-    if (receiver.is_null()) {\n-      THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+    bool caller_is_c1 = callerFrame.is_compiled_frame() && callerFrame.cb()->as_nmethod()->is_compiled_by_c1();\n+    if (!caller_is_c1 && callee->is_scalarized_arg(0)) {\n+      \/\/ If the receiver is an inline type that is passed as fields, no oop is available\n+      \/\/ Resolve the call without receiver null checking.\n+      assert(!callee->mismatch(), \"calls with inline type receivers should never mismatch\");\n+      assert(attached_method.not_null() && !attached_method->is_abstract(), \"must have non-abstract attached method\");\n+      if (bc == Bytecodes::_invokeinterface) {\n+        bc = Bytecodes::_invokevirtual; \/\/ C2 optimistically replaces interface calls by virtual calls\n+      }\n+      check_null_and_abstract = false;\n+    } else {\n+      \/\/ Retrieve from a compiled argument list\n+      receiver = Handle(current, callerFrame.retrieve_receiver(&reg_map2));\n+      assert(oopDesc::is_oop_or_null(receiver()), \"\");\n+      if (receiver.is_null()) {\n+        THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+      }\n@@ -1272,1 +1311,1 @@\n-    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, check_null_and_abstract, CHECK_NH);\n@@ -1281,1 +1320,1 @@\n-  if (has_receiver) {\n+  if (has_receiver && check_null_and_abstract) {\n@@ -1309,1 +1348,1 @@\n-methodHandle SharedRuntime::find_callee_method(TRAPS) {\n+methodHandle SharedRuntime::find_callee_method(bool& caller_does_not_scalarize, TRAPS) {\n@@ -1335,0 +1374,4 @@\n+    \/\/ Calls via mismatching methods are always non-scalarized\n+    if (callinfo.resolved_method()->mismatch()) {\n+      caller_does_not_scalarize = true;\n+    }\n@@ -1342,1 +1385,1 @@\n-methodHandle SharedRuntime::resolve_helper(bool is_virtual, bool is_optimized, TRAPS) {\n+methodHandle SharedRuntime::resolve_helper(bool is_virtual, bool is_optimized, bool& caller_does_not_scalarize, TRAPS) {\n@@ -1365,0 +1408,4 @@\n+  \/\/ Calls via mismatching methods are always non-scalarized\n+  if (caller_nm->is_compiled_by_c1() || call_info.resolved_method()->mismatch()) {\n+    caller_does_not_scalarize = true;\n+  }\n@@ -1383,1 +1430,1 @@\n-    tty->print(\"resolving %s%s (%s) call to\",\n+    tty->print(\"resolving %s%s (%s) %s call to\",\n@@ -1385,1 +1432,1 @@\n-               Bytecodes::name(invoke_code));\n+               Bytecodes::name(invoke_code), (caller_does_not_scalarize) ? \"non-scalar\" : \"\");\n@@ -1424,1 +1471,1 @@\n-    inline_cache->update(&call_info, receiver->klass());\n+    inline_cache->update(&call_info, receiver->klass(), caller_does_not_scalarize);\n@@ -1428,1 +1475,1 @@\n-    callsite->set(callee_method);\n+    callsite->set(callee_method, caller_does_not_scalarize);\n@@ -1448,0 +1495,2 @@\n+  const bool is_optimized = false;\n+  bool caller_does_not_scalarize = false;\n@@ -1449,1 +1498,1 @@\n-    callee_method = SharedRuntime::handle_ic_miss_helper(CHECK_NULL);\n+    callee_method = SharedRuntime::handle_ic_miss_helper(caller_does_not_scalarize, CHECK_NULL);\n@@ -1454,1 +1503,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, is_optimized, caller_does_not_scalarize);\n@@ -1495,1 +1544,5 @@\n-      return callee->get_c2i_entry();\n+      if (caller_frame.is_interpreted_frame()) {\n+        return callee->get_c2i_inline_entry();\n+      } else {\n+        return callee->get_c2i_entry();\n+      }\n@@ -1501,0 +1554,3 @@\n+  bool is_static_call = false;\n+  bool is_optimized = false;\n+  bool caller_does_not_scalarize = false;\n@@ -1503,1 +1559,1 @@\n-    callee_method = SharedRuntime::reresolve_call_site(CHECK_NULL);\n+    callee_method = SharedRuntime::reresolve_call_site(is_static_call, is_optimized, caller_does_not_scalarize, CHECK_NULL);\n@@ -1507,1 +1563,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, is_static_call, is_optimized, caller_does_not_scalarize);\n@@ -1546,1 +1602,2 @@\n-address SharedRuntime::get_resolved_entry(JavaThread* current, methodHandle callee_method) {\n+address SharedRuntime::get_resolved_entry(JavaThread* current, methodHandle callee_method,\n+                                          bool is_static_call, bool is_optimized, bool caller_does_not_scalarize) {\n@@ -1552,2 +1609,11 @@\n-  assert(callee_method->verified_code_entry() != nullptr, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+\n+  if (caller_does_not_scalarize) {\n+    assert(callee_method->verified_inline_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_inline_code_entry();\n+  } else if (is_static_call || is_optimized) {\n+    assert(callee_method->verified_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_code_entry();\n+  } else {\n+    assert(callee_method->verified_inline_ro_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_inline_ro_code_entry();\n+  }\n@@ -1559,0 +1625,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1561,1 +1628,1 @@\n-    callee_method = SharedRuntime::resolve_helper(false, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(false, false, caller_does_not_scalarize, CHECK_NULL);\n@@ -1565,1 +1632,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, true, false, caller_does_not_scalarize);\n@@ -1571,0 +1638,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1572,1 +1640,1 @@\n-    callee_method = SharedRuntime::resolve_helper(true, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(true, false, caller_does_not_scalarize, CHECK_NULL);\n@@ -1576,1 +1644,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, false, caller_does_not_scalarize);\n@@ -1584,0 +1652,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1585,1 +1654,1 @@\n-    callee_method = SharedRuntime::resolve_helper(true, true, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(true, true, caller_does_not_scalarize, CHECK_NULL);\n@@ -1589,1 +1658,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, true, caller_does_not_scalarize);\n@@ -1592,1 +1661,3 @@\n-methodHandle SharedRuntime::handle_ic_miss_helper(TRAPS) {\n+\n+\n+methodHandle SharedRuntime::handle_ic_miss_helper(bool& caller_does_not_scalarize, TRAPS) {\n@@ -1610,1 +1681,1 @@\n-    tty->print(\"IC miss (%s) call to\", Bytecodes::name(bc));\n+    tty->print(\"IC miss (%s) %s call to\", Bytecodes::name(bc), (caller_does_not_scalarize) ? \"non-scalar\" : \"\");\n@@ -1642,0 +1713,4 @@\n+  \/\/ Calls via mismatching methods are always non-scalarized\n+  if (caller_nm->is_compiled_by_c1() || call_info.resolved_method()->mismatch()) {\n+    caller_does_not_scalarize = true;\n+  }\n@@ -1645,1 +1720,1 @@\n-  inline_cache->update(&call_info, receiver()->klass());\n+  inline_cache->update(&call_info, receiver()->klass(), caller_does_not_scalarize);\n@@ -1656,1 +1731,1 @@\n-methodHandle SharedRuntime::reresolve_call_site(TRAPS) {\n+methodHandle SharedRuntime::reresolve_call_site(bool& is_static_call, bool& is_optimized, bool& caller_does_not_scalarize, TRAPS) {\n@@ -1666,0 +1741,3 @@\n+  if (caller.is_compiled_frame()) {\n+    caller_does_not_scalarize = caller.cb()->as_nmethod()->is_compiled_by_c1();\n+  }\n@@ -1706,0 +1784,2 @@\n+        is_static_call = false;\n+        is_optimized = false;\n@@ -1708,0 +1788,1 @@\n+            is_static_call = true;\n@@ -1709,0 +1790,1 @@\n+            is_optimized = (iter.type() == relocInfo::opt_virtual_call_type);\n@@ -1713,1 +1795,0 @@\n-\n@@ -1727,2 +1808,1 @@\n-  methodHandle callee_method = find_callee_method(CHECK_(methodHandle()));\n-\n+  methodHandle callee_method = find_callee_method(caller_does_not_scalarize, CHECK_(methodHandle()));\n@@ -1735,1 +1815,1 @@\n-    tty->print(\"handle_wrong_method reresolving call to\");\n+    tty->print(\"handle_wrong_method reresolving %s call to\", (caller_does_not_scalarize) ? \"non-scalar\" : \"\");\n@@ -1941,0 +2021,15 @@\n+char* SharedRuntime::generate_identity_exception_message(JavaThread* current, Klass* klass) {\n+  assert(klass->is_inline_klass(), \"Must be a concrete value class\");\n+  const char* desc = \"Cannot synchronize on an instance of value class \";\n+  const char* className = klass->external_name();\n+  size_t msglen = strlen(desc) + strlen(className) + 1;\n+  char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+  if (nullptr == message) {\n+    \/\/ Out of memory: can't create detailed error message\n+    message = const_cast<char*>(klass->external_name());\n+  } else {\n+    jio_snprintf(message, msglen, \"%s%s\", desc, className);\n+  }\n+  return message;\n+}\n+\n@@ -2189,5 +2284,30 @@\n- private:\n-  enum {\n-    _basic_type_bits = 4,\n-    _basic_type_mask = right_n_bits(_basic_type_bits),\n-    _basic_types_per_int = BitsPerInt \/ _basic_type_bits,\n+public:\n+  class Element {\n+  private:\n+    \/\/ The highest byte is the type of the argument. The remaining bytes contain the offset of the\n+    \/\/ field if it is flattened in the calling convention, -1 otherwise.\n+    juint _payload;\n+\n+    static constexpr int offset_bit_width = 24;\n+    static constexpr juint offset_bit_mask = (1 << offset_bit_width) - 1;\n+  public:\n+    Element(BasicType bt, int offset) : _payload((static_cast<juint>(bt) << offset_bit_width) | (juint(offset) & offset_bit_mask)) {\n+      assert(offset >= -1 && offset < jint(offset_bit_mask), \"invalid offset %d\", offset);\n+    }\n+\n+    BasicType bt() const {\n+      return static_cast<BasicType>(_payload >> offset_bit_width);\n+    }\n+\n+    int offset() const {\n+      juint res = _payload & offset_bit_mask;\n+      return res == offset_bit_mask ? -1 : res;\n+    }\n+\n+    juint hash() const {\n+      return _payload;\n+    }\n+\n+    bool operator!=(const Element& other) const {\n+      return _payload != other._payload;\n+    }\n@@ -2195,3 +2315,3 @@\n-  \/\/ TO DO:  Consider integrating this with a more global scheme for compressing signatures.\n-  \/\/ For now, 4 bits per components (plus T_VOID gaps after double\/long) is not excessive.\n-  int _length;\n+private:\n+  const bool _has_ro_adapter;\n+  const int _length;\n@@ -2201,2 +2321,8 @@\n-  int* data_pointer() {\n-    return (int*)((address)this + data_offset());\n+  Element* data_pointer() {\n+    return reinterpret_cast<Element*>(reinterpret_cast<address>(this) + data_offset());\n+  }\n+\n+  const Element& element_at(int index) {\n+    assert(index < length(), \"index %d out of bounds for length %d\", index, length());\n+    Element* data = data_pointer();\n+    return data[index];\n@@ -2206,6 +2332,5 @@\n-  AdapterFingerPrint(int total_args_passed, BasicType* sig_bt, int len) {\n-    int* data = data_pointer();\n-    \/\/ Pack the BasicTypes with 8 per int\n-    assert(len == length(total_args_passed), \"sanity\");\n-    _length = len;\n-    int sig_index = 0;\n+  AdapterFingerPrint(const GrowableArray<SigEntry>* sig, bool has_ro_adapter)\n+    : _has_ro_adapter(has_ro_adapter), _length(total_args_passed_in_sig(sig)) {\n+    Element* data = data_pointer();\n+    BasicType prev_bt = T_ILLEGAL;\n+    int vt_count = 0;\n@@ -2213,5 +2338,15 @@\n-      int value = 0;\n-      for (int byte = 0; sig_index < total_args_passed && byte < _basic_types_per_int; byte++) {\n-        int bt = adapter_encoding(sig_bt[sig_index++]);\n-        assert((bt & _basic_type_mask) == bt, \"must fit in 4 bits\");\n-        value = (value << _basic_type_bits) | bt;\n+      const SigEntry& sig_entry = sig->at(index);\n+      BasicType bt = sig_entry._bt;\n+      if (bt == T_METADATA) {\n+        \/\/ Found start of inline type in signature\n+        assert(InlineTypePassFieldsAsArgs, \"unexpected start of inline type\");\n+        vt_count++;\n+      } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+        \/\/ Found end of inline type in signature\n+        assert(InlineTypePassFieldsAsArgs, \"unexpected end of inline type\");\n+        vt_count--;\n+        assert(vt_count >= 0, \"invalid vt_count\");\n+      } else if (vt_count == 0) {\n+        \/\/ Widen fields that are not part of a scalarized inline type argument\n+        assert(sig_entry._offset == -1, \"invalid offset for argument that is not a flattened field %d\", sig_entry._offset);\n+        bt = adapter_encoding(bt);\n@@ -2219,1 +2354,3 @@\n-      data[index] = value;\n+\n+      ::new(&data[index]) Element(bt, sig_entry._offset);\n+      prev_bt = bt;\n@@ -2221,0 +2358,1 @@\n+    assert(vt_count == 0, \"invalid vt_count\");\n@@ -2228,2 +2366,2 @@\n-  static int length(int total_args) {\n-    return (total_args + (_basic_types_per_int-1)) \/ _basic_types_per_int;\n+  static int total_args_passed_in_sig(const GrowableArray<SigEntry>* sig) {\n+    return (sig != nullptr) ? sig->length() : 0;\n@@ -2233,1 +2371,1 @@\n-    return (int)heap_word_size(sizeof(AdapterFingerPrint) + (len * sizeof(int)));\n+    return (int)heap_word_size(sizeof(AdapterFingerPrint) + (len * sizeof(Element)));\n@@ -2239,1 +2377,1 @@\n-  static int adapter_encoding(BasicType in) {\n+  static BasicType adapter_encoding(BasicType in) {\n@@ -2245,1 +2383,1 @@\n-        \/\/ There are all promoted to T_INT in the calling convention\n+        \/\/ They are all promoted to T_INT in the calling convention\n@@ -2278,0 +2416,1 @@\n+public:\n@@ -2281,10 +2420,1 @@\n-      unsigned val = (unsigned)value(i);\n-      \/\/ args are packed so that first\/lower arguments are in the highest\n-      \/\/ bits of each int value, so iterate from highest to the lowest\n-      for (int j = 32 - _basic_type_bits; j >= 0; j -= _basic_type_bits) {\n-        unsigned v = (val >> j) & _basic_type_mask;\n-        if (v == 0) {\n-          continue;\n-        }\n-        function(v);\n-      }\n+      function(element_at(i));\n@@ -2294,3 +2424,2 @@\n- public:\n-  static AdapterFingerPrint* allocate(int total_args_passed, BasicType* sig_bt) {\n-    int len = length(total_args_passed);\n+  static AdapterFingerPrint* allocate(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false) {\n+    int len = total_args_passed_in_sig(sig);\n@@ -2298,1 +2427,1 @@\n-    AdapterFingerPrint* afp = new (size_in_bytes) AdapterFingerPrint(total_args_passed, sig_bt, len);\n+    AdapterFingerPrint* afp = new (size_in_bytes) AdapterFingerPrint(sig, has_ro_adapter);\n@@ -2307,3 +2436,2 @@\n-  int value(int index) {\n-    int* data = data_pointer();\n-    return data[index];\n+  bool has_ro_adapter() const {\n+    return _has_ro_adapter;\n@@ -2312,1 +2440,1 @@\n-  int length() {\n+  int length() const {\n@@ -2319,1 +2447,1 @@\n-      int v = value(i);\n+      const Element& v = element_at(i);\n@@ -2321,1 +2449,1 @@\n-      hash = ((hash << 8) ^ v ^ (hash >> 5)) + 3;\n+      hash = ((hash << 8) ^ v.hash() ^ (hash >> 5)) + 3;\n@@ -2328,1 +2456,6 @@\n-    st.print(\"0x\");\n+    st.print(\"{\");\n+    if (_has_ro_adapter) {\n+      st.print(\"has_ro_adapter\");\n+    } else {\n+      st.print(\"no_ro_adapter\");\n+    }\n@@ -2330,1 +2463,3 @@\n-      st.print(\"%x\", value(i));\n+      st.print(\", \");\n+      const Element& elem = element_at(i);\n+      st.print(\"{%s, %d}\", type2name(elem.bt()), elem.offset());\n@@ -2332,0 +2467,1 @@\n+    st.print(\"}\");\n@@ -2338,1 +2474,1 @@\n-    iterate_args([&] (int arg) {\n+    iterate_args([&] (const Element& arg) {\n@@ -2341,1 +2477,1 @@\n-        if (arg == T_VOID) {\n+        if (arg.bt() == T_VOID) {\n@@ -2347,7 +2483,4 @@\n-      switch (arg) {\n-        case T_INT:    st.print(\"I\");    break;\n-        case T_LONG:   long_prev = true; break;\n-        case T_FLOAT:  st.print(\"F\");    break;\n-        case T_DOUBLE: st.print(\"D\");    break;\n-        case T_VOID:   break;\n-        default: ShouldNotReachHere();\n+      if (arg.bt() == T_LONG) {\n+        long_prev = true;\n+      } else if (arg.bt() != T_VOID) {\n+        st.print(\"%c\", type2char(arg.bt()));\n@@ -2362,52 +2495,3 @@\n-  BasicType* as_basic_type(int& nargs) {\n-    nargs = 0;\n-    GrowableArray<BasicType> btarray;\n-    bool long_prev = false;\n-\n-    iterate_args([&] (int arg) {\n-      if (long_prev) {\n-        long_prev = false;\n-        if (arg == T_VOID) {\n-          btarray.append(T_LONG);\n-        } else {\n-          btarray.append(T_OBJECT); \/\/ it could be T_ARRAY; it shouldn't matter\n-        }\n-      }\n-      switch (arg) {\n-        case T_INT: \/\/ fallthrough\n-        case T_FLOAT: \/\/ fallthrough\n-        case T_DOUBLE:\n-        case T_VOID:\n-          btarray.append((BasicType)arg);\n-          break;\n-        case T_LONG:\n-          long_prev = true;\n-          break;\n-        default: ShouldNotReachHere();\n-      }\n-    });\n-\n-    if (long_prev) {\n-      btarray.append(T_OBJECT);\n-    }\n-\n-    nargs = btarray.length();\n-    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, nargs);\n-    int index = 0;\n-    GrowableArrayIterator<BasicType> iter = btarray.begin();\n-    while (iter != btarray.end()) {\n-      sig_bt[index++] = *iter;\n-      ++iter;\n-    }\n-    assert(index == btarray.length(), \"sanity check\");\n-#ifdef ASSERT\n-    {\n-      AdapterFingerPrint* compare_fp = AdapterFingerPrint::allocate(nargs, sig_bt);\n-      assert(this->equals(compare_fp), \"sanity check\");\n-      AdapterFingerPrint::deallocate(compare_fp);\n-    }\n-#endif\n-    return sig_bt;\n-  }\n-\n-    if (other->_length != _length) {\n+    if (other->_has_ro_adapter != _has_ro_adapter) {\n+      return false;\n+    } else if (other->_length != _length) {\n@@ -2418,1 +2502,1 @@\n-        if (value(i) != other->value(i)) {\n+        if (element_at(i) != other->element_at(i)) {\n@@ -2461,1 +2545,1 @@\n-AdapterHandlerEntry* AdapterHandlerLibrary::lookup(int total_args_passed, BasicType* sig_bt) {\n+AdapterHandlerEntry* AdapterHandlerLibrary::lookup(const GrowableArray<SigEntry>* sig, bool has_ro_adapter) {\n@@ -2464,1 +2548,1 @@\n-  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(total_args_passed, sig_bt);\n+  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(sig, has_ro_adapter);\n@@ -2521,1 +2605,1 @@\n-static const int AdapterHandlerLibrary_size = 16*K;\n+static const int AdapterHandlerLibrary_size = 48*K;\n@@ -2568,1 +2652,3 @@\n-    _no_arg_handler = create_adapter(0, nullptr);\n+    CompiledEntrySignature no_args;\n+    no_args.compute_calling_conventions();\n+    _no_arg_handler = create_adapter(no_args, true);\n@@ -2570,2 +2656,4 @@\n-    BasicType obj_args[] = { T_OBJECT };\n-    _obj_arg_handler = create_adapter(1, obj_args);\n+    CompiledEntrySignature obj_args;\n+    SigEntry::add_entry(obj_args.sig(), T_OBJECT);\n+    obj_args.compute_calling_conventions();\n+    _obj_arg_handler = create_adapter(obj_args, true);\n@@ -2573,2 +2661,4 @@\n-    BasicType int_args[] = { T_INT };\n-    _int_arg_handler = create_adapter(1, int_args);\n+    CompiledEntrySignature int_args;\n+    SigEntry::add_entry(int_args.sig(), T_INT);\n+    int_args.compute_calling_conventions();\n+    _int_arg_handler = create_adapter(int_args, true);\n@@ -2576,2 +2666,5 @@\n-    BasicType obj_int_args[] = { T_OBJECT, T_INT };\n-    _obj_int_arg_handler = create_adapter(2, obj_int_args);\n+    CompiledEntrySignature obj_int_args;\n+    SigEntry::add_entry(obj_int_args.sig(), T_OBJECT);\n+    SigEntry::add_entry(obj_int_args.sig(), T_INT);\n+    obj_int_args.compute_calling_conventions();\n+    _obj_int_arg_handler = create_adapter(obj_int_args, true);\n@@ -2579,2 +2672,5 @@\n-    BasicType obj_obj_args[] = { T_OBJECT, T_OBJECT };\n-    _obj_obj_arg_handler = create_adapter(2, obj_obj_args);\n+    CompiledEntrySignature obj_obj_args;\n+    SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+    SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+    obj_obj_args.compute_calling_conventions();\n+    _obj_obj_arg_handler = create_adapter(obj_obj_args, true);\n@@ -2612,0 +2708,3 @@\n+      if (InlineTypePassFieldsAsArgs && method->method_holder()->is_inline_klass()) {\n+        return nullptr;\n+      }\n@@ -2615,1 +2714,10 @@\n-      case JVM_SIGNATURE_CLASS:\n+      case JVM_SIGNATURE_CLASS: {\n+        if (InlineTypePassFieldsAsArgs) {\n+          SignatureStream ss(method->signature());\n+          InlineKlass* vk = ss.as_inline_klass(method->method_holder());\n+          if (vk != nullptr) {\n+            return nullptr;\n+          }\n+        }\n+        return _obj_arg_handler;\n+      }\n@@ -2626,1 +2734,1 @@\n-             !method->is_static()) {\n+             !method->is_static() && (!InlineTypePassFieldsAsArgs || !method->method_holder()->is_inline_klass())) {\n@@ -2628,1 +2736,10 @@\n-      case JVM_SIGNATURE_CLASS:\n+      case JVM_SIGNATURE_CLASS: {\n+        if (InlineTypePassFieldsAsArgs) {\n+          SignatureStream ss(method->signature());\n+          InlineKlass* vk = ss.as_inline_klass(method->method_holder());\n+          if (vk != nullptr) {\n+            return nullptr;\n+          }\n+        }\n+        return _obj_obj_arg_handler;\n+      }\n@@ -2642,5 +2759,9 @@\n-class AdapterSignatureIterator : public SignatureIterator {\n- private:\n-  BasicType stack_sig_bt[16];\n-  BasicType* sig_bt;\n-  int index;\n+CompiledEntrySignature::CompiledEntrySignature(Method* method) :\n+  _method(method), _num_inline_args(0), _has_inline_recv(false),\n+  _regs(nullptr), _regs_cc(nullptr), _regs_cc_ro(nullptr),\n+  _args_on_stack(0), _args_on_stack_cc(0), _args_on_stack_cc_ro(0),\n+  _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _supers(nullptr) {\n+  _sig = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+  _sig_cc = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+  _sig_cc_ro = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+}\n@@ -2648,11 +2769,24 @@\n- public:\n-  AdapterSignatureIterator(Symbol* signature,\n-                           fingerprint_t fingerprint,\n-                           bool is_static,\n-                           int total_args_passed) :\n-    SignatureIterator(signature, fingerprint),\n-    index(0)\n-  {\n-    sig_bt = (total_args_passed <= 16) ? stack_sig_bt : NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n-    if (!is_static) { \/\/ Pass in receiver first\n-      sig_bt[index++] = T_OBJECT;\n+\/\/ See if we can save space by sharing the same entry for VIEP and VIEP(RO),\n+\/\/ or the same entry for VEP and VIEP(RO).\n+CodeOffsets::Entries CompiledEntrySignature::c1_inline_ro_entry_type() const {\n+  if (!has_scalarized_args()) {\n+    \/\/ VEP\/VIEP\/VIEP(RO) all share the same entry. There's no packing.\n+    return CodeOffsets::Verified_Entry;\n+  }\n+  if (_method->is_static()) {\n+    \/\/ Static methods don't need VIEP(RO)\n+    return CodeOffsets::Verified_Entry;\n+  }\n+\n+  if (has_inline_recv()) {\n+    if (num_inline_args() == 1) {\n+      \/\/ Share same entry for VIEP and VIEP(RO).\n+      \/\/ This is quite common: we have an instance method in an InlineKlass that has\n+      \/\/ no inline type args other than <this>.\n+      return CodeOffsets::Verified_Inline_Entry;\n+    } else {\n+      assert(num_inline_args() > 1, \"must be\");\n+      \/\/ No sharing:\n+      \/\/   VIEP(RO) -- <this> is passed as object\n+      \/\/   VEP      -- <this> is passed as fields\n+      return CodeOffsets::Verified_Inline_Entry_RO;\n@@ -2660,1 +2794,0 @@\n-    do_parameters_on(this);\n@@ -2663,2 +2796,9 @@\n-  BasicType* basic_types() {\n-    return sig_bt;\n+  \/\/ Either a static method, or <this> is not an inline type\n+  if (args_on_stack_cc() != args_on_stack_cc_ro()) {\n+    \/\/ No sharing:\n+    \/\/ Some arguments are passed on the stack, and we have inserted reserved entries\n+    \/\/ into the VEP, but we never insert reserved entries into the VIEP(RO).\n+    return CodeOffsets::Verified_Inline_Entry_RO;\n+  } else {\n+    \/\/ Share same entry for VEP and VIEP(RO).\n+    return CodeOffsets::Verified_Entry;\n@@ -2666,0 +2806,1 @@\n+}\n@@ -2667,3 +2808,39 @@\n-#ifdef ASSERT\n-  int slots() {\n-    return index;\n+\/\/ Returns all super methods (transitive) in classes and interfaces that are overridden by the current method.\n+GrowableArray<Method*>* CompiledEntrySignature::get_supers() {\n+  if (_supers != nullptr) {\n+    return _supers;\n+  }\n+  _supers = new GrowableArray<Method*>();\n+  \/\/ Skip private, static, and <init> methods\n+  if (_method->is_private() || _method->is_static() || _method->is_object_constructor()) {\n+    return _supers;\n+  }\n+  Symbol* name = _method->name();\n+  Symbol* signature = _method->signature();\n+  const Klass* holder = _method->method_holder()->super();\n+  Symbol* holder_name = holder->name();\n+  ThreadInVMfromUnknown tiv;\n+  JavaThread* current = JavaThread::current();\n+  HandleMark hm(current);\n+  Handle loader(current, _method->method_holder()->class_loader());\n+\n+  \/\/ Walk up the class hierarchy and search for super methods\n+  while (holder != nullptr) {\n+    Method* super_method = holder->lookup_method(name, signature);\n+    if (super_method == nullptr) {\n+      break;\n+    }\n+    if (!super_method->is_static() && !super_method->is_private() &&\n+        (!super_method->is_package_private() ||\n+         super_method->method_holder()->is_same_class_package(loader(), holder_name))) {\n+      _supers->push(super_method);\n+    }\n+    holder = super_method->method_holder()->super();\n+  }\n+  \/\/ Search interfaces for super methods\n+  Array<InstanceKlass*>* interfaces = _method->method_holder()->transitive_interfaces();\n+  for (int i = 0; i < interfaces->length(); ++i) {\n+    Method* m = interfaces->at(i)->lookup_method(name, signature);\n+    if (m != nullptr && !m->is_static() && m->is_public()) {\n+      _supers->push(m);\n+    }\n@@ -2671,0 +2848,50 @@\n+  return _supers;\n+}\n+\n+\/\/ Iterate over arguments and compute scalarized and non-scalarized signatures\n+void CompiledEntrySignature::compute_calling_conventions(bool init) {\n+  bool has_scalarized = false;\n+  if (_method != nullptr) {\n+    InstanceKlass* holder = _method->method_holder();\n+    int arg_num = 0;\n+    if (!_method->is_static()) {\n+      \/\/ We shouldn't scalarize 'this' in a value class constructor\n+      if (holder->is_inline_klass() && InlineKlass::cast(holder)->can_be_passed_as_fields() && !_method->is_object_constructor() &&\n+          (init || _method->is_scalarized_arg(arg_num))) {\n+        _sig_cc->appendAll(InlineKlass::cast(holder)->extended_sig());\n+        has_scalarized = true;\n+        _has_inline_recv = true;\n+        _num_inline_args++;\n+      } else {\n+        SigEntry::add_entry(_sig_cc, T_OBJECT, holder->name());\n+      }\n+      SigEntry::add_entry(_sig, T_OBJECT, holder->name());\n+      SigEntry::add_entry(_sig_cc_ro, T_OBJECT, holder->name());\n+      arg_num++;\n+    }\n+    for (SignatureStream ss(_method->signature()); !ss.at_return_type(); ss.next()) {\n+      BasicType bt = ss.type();\n+      if (bt == T_OBJECT) {\n+        InlineKlass* vk = ss.as_inline_klass(holder);\n+        if (vk != nullptr && vk->can_be_passed_as_fields() && (init || _method->is_scalarized_arg(arg_num))) {\n+          \/\/ Check for a calling convention mismatch with super method(s)\n+          bool scalar_super = false;\n+          bool non_scalar_super = false;\n+          GrowableArray<Method*>* supers = get_supers();\n+          for (int i = 0; i < supers->length(); ++i) {\n+            Method* super_method = supers->at(i);\n+            if (super_method->is_scalarized_arg(arg_num)) {\n+              scalar_super = true;\n+            } else {\n+              non_scalar_super = true;\n+            }\n+          }\n+#ifdef ASSERT\n+          \/\/ Randomly enable below code paths for stress testing\n+          bool stress = init && StressCallingConvention;\n+          if (stress && (os::random() & 1) == 1) {\n+            non_scalar_super = true;\n+            if ((os::random() & 1) == 1) {\n+              scalar_super = true;\n+            }\n+          }\n@@ -2672,0 +2899,50 @@\n+          if (non_scalar_super) {\n+            \/\/ Found a super method with a non-scalarized argument. Fall back to the non-scalarized calling convention.\n+            if (scalar_super) {\n+              \/\/ Found non-scalar *and* scalar super methods. We can't handle both.\n+              \/\/ Mark the scalar method as mismatch and re-compile call sites to use non-scalarized calling convention.\n+              for (int i = 0; i < supers->length(); ++i) {\n+                Method* super_method = supers->at(i);\n+                if (super_method->is_scalarized_arg(arg_num) DEBUG_ONLY(|| (stress && (os::random() & 1) == 1))) {\n+                  super_method->set_mismatch();\n+                  MutexLocker ml(Compile_lock, Mutex::_safepoint_check_flag);\n+                  JavaThread* thread = JavaThread::current();\n+                  HandleMark hm(thread);\n+                  methodHandle mh(thread, super_method);\n+                  DeoptimizationScope deopt_scope;\n+                  CodeCache::mark_for_deoptimization(&deopt_scope, mh());\n+                  deopt_scope.deoptimize_marked();\n+                }\n+              }\n+            }\n+            \/\/ Fall back to non-scalarized calling convention\n+            SigEntry::add_entry(_sig_cc, T_OBJECT, ss.as_symbol());\n+            SigEntry::add_entry(_sig_cc_ro, T_OBJECT, ss.as_symbol());\n+          } else {\n+            _num_inline_args++;\n+            has_scalarized = true;\n+            int last = _sig_cc->length();\n+            int last_ro = _sig_cc_ro->length();\n+            _sig_cc->appendAll(vk->extended_sig());\n+            _sig_cc_ro->appendAll(vk->extended_sig());\n+            if (bt == T_OBJECT) {\n+              \/\/ Nullable inline type argument, insert InlineTypeNode::NullMarker field right after T_METADATA delimiter\n+              _sig_cc->insert_before(last+1, SigEntry(T_BOOLEAN, -1, nullptr, true));\n+              _sig_cc_ro->insert_before(last_ro+1, SigEntry(T_BOOLEAN, -1, nullptr, true));\n+            }\n+          }\n+        } else {\n+          SigEntry::add_entry(_sig_cc, T_OBJECT, ss.as_symbol());\n+          SigEntry::add_entry(_sig_cc_ro, T_OBJECT, ss.as_symbol());\n+        }\n+        bt = T_OBJECT;\n+      } else {\n+        SigEntry::add_entry(_sig_cc, ss.type(), ss.as_symbol());\n+        SigEntry::add_entry(_sig_cc_ro, ss.type(), ss.as_symbol());\n+      }\n+      SigEntry::add_entry(_sig, bt, ss.as_symbol());\n+      if (bt != T_VOID) {\n+        arg_num++;\n+      }\n+    }\n+  }\n@@ -2673,1 +2950,14 @@\n- private:\n+  \/\/ Compute the non-scalarized calling convention\n+  _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig->length());\n+  _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);\n+\n+  \/\/ Compute the scalarized calling conventions if there are scalarized inline types in the signature\n+  if (has_scalarized && !_method->is_native()) {\n+    _regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc->length());\n+    _args_on_stack_cc = SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);\n+\n+    _regs_cc_ro = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc_ro->length());\n+    _args_on_stack_cc_ro = SharedRuntime::java_calling_convention(_sig_cc_ro, _regs_cc_ro);\n+\n+    _c1_needs_stack_repair = (_args_on_stack_cc < _args_on_stack) || (_args_on_stack_cc_ro < _args_on_stack);\n+    _c2_needs_stack_repair = (_args_on_stack_cc > _args_on_stack) || (_args_on_stack_cc > _args_on_stack_cc_ro);\n@@ -2675,5 +2965,5 @@\n-  friend class SignatureIterator;  \/\/ so do_parameters_on can call do_type\n-  void do_type(BasicType type) {\n-    sig_bt[index++] = type;\n-    if (type == T_LONG || type == T_DOUBLE) {\n-      sig_bt[index++] = T_VOID; \/\/ Longs & doubles take 2 Java slots\n+    \/\/ Upper bound on stack arguments to avoid hitting the argument limit and\n+    \/\/ bailing out of compilation (\"unsupported incoming calling sequence\").\n+    \/\/ TODO we need a reasonable limit (flag?) here\n+    if (MAX2(_args_on_stack_cc, _args_on_stack_cc_ro) <= 60) {\n+      return; \/\/ Success\n@@ -2682,1 +2972,130 @@\n-};\n+  \/\/ No scalarized args\n+  _sig_cc = _sig;\n+  _regs_cc = _regs;\n+  _args_on_stack_cc = _args_on_stack;\n+\n+  _sig_cc_ro = _sig;\n+  _regs_cc_ro = _regs;\n+  _args_on_stack_cc_ro = _args_on_stack;\n+}\n+\n+void CompiledEntrySignature::initialize_from_fingerprint(AdapterFingerPrint* fingerprint) {\n+  _has_inline_recv = fingerprint->has_ro_adapter();\n+\n+  int value_object_count = 0;\n+  BasicType prev_bt = T_ILLEGAL;\n+  bool has_scalarized_arguments = false;\n+  bool long_prev = false;\n+  int long_prev_offset = -1;\n+\n+  fingerprint->iterate_args([&] (const AdapterFingerPrint::Element& arg) {\n+    BasicType bt = arg.bt();\n+    int offset = arg.offset();\n+\n+    if (long_prev) {\n+      long_prev = false;\n+      BasicType bt_to_add;\n+      if (bt == T_VOID) {\n+        bt_to_add = T_LONG;\n+      } else {\n+        bt_to_add = T_OBJECT;\n+      }\n+      if (value_object_count == 0) {\n+        SigEntry::add_entry(_sig, bt_to_add);\n+      }\n+      SigEntry::add_entry(_sig_cc, bt_to_add, nullptr, long_prev_offset);\n+      SigEntry::add_entry(_sig_cc_ro, bt_to_add, nullptr, long_prev_offset);\n+    }\n+\n+    switch (bt) {\n+      case T_VOID:\n+        if (prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+          assert(InlineTypePassFieldsAsArgs, \"unexpected end of inline type\");\n+          value_object_count--;\n+          SigEntry::add_entry(_sig_cc, T_VOID, nullptr, offset);\n+          SigEntry::add_entry(_sig_cc_ro, T_VOID, nullptr, offset);\n+          assert(value_object_count >= 0, \"invalid value object count\");\n+        } else {\n+          \/\/ Nothing to add for _sig: We already added an addition T_VOID in add_entry() when adding T_LONG or T_DOUBLE.\n+        }\n+        break;\n+      case T_INT:\n+      case T_FLOAT:\n+      case T_DOUBLE:\n+        if (value_object_count == 0) {\n+          SigEntry::add_entry(_sig, bt);\n+        }\n+        SigEntry::add_entry(_sig_cc, bt, nullptr, offset);\n+        SigEntry::add_entry(_sig_cc_ro, bt, nullptr, offset);\n+        break;\n+      case T_LONG:\n+        long_prev = true;\n+        long_prev_offset = offset;\n+        break;\n+      case T_BOOLEAN:\n+      case T_CHAR:\n+      case T_BYTE:\n+      case T_SHORT:\n+      case T_OBJECT:\n+      case T_ARRAY:\n+        assert(value_object_count > 0, \"must be value object field\");\n+        SigEntry::add_entry(_sig_cc, bt, nullptr, offset);\n+        SigEntry::add_entry(_sig_cc_ro, bt, nullptr, offset);\n+        break;\n+      case T_METADATA:\n+        assert(InlineTypePassFieldsAsArgs, \"unexpected start of inline type\");\n+        if (value_object_count == 0) {\n+          SigEntry::add_entry(_sig, T_OBJECT);\n+        }\n+        SigEntry::add_entry(_sig_cc, T_METADATA, nullptr, offset);\n+        SigEntry::add_entry(_sig_cc_ro, T_METADATA, nullptr, offset);\n+        value_object_count++;\n+        has_scalarized_arguments = true;\n+        break;\n+      default: {\n+        fatal(\"Unexpected BasicType: %s\", basictype_to_str(bt));\n+      }\n+    }\n+    prev_bt = bt;\n+  });\n+\n+  if (long_prev) {\n+    \/\/ If previous bt was T_LONG and we reached the end of the signature, we know that it must be a T_OBJECT.\n+    SigEntry::add_entry(_sig, T_OBJECT);\n+    SigEntry::add_entry(_sig_cc, T_OBJECT);\n+    SigEntry::add_entry(_sig_cc_ro, T_OBJECT);\n+  }\n+  assert(value_object_count == 0, \"invalid value object count\");\n+\n+  _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig->length());\n+  _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);\n+\n+  \/\/ Compute the scalarized calling conventions if there are scalarized inline types in the signature\n+  if (has_scalarized_arguments) {\n+    _regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc->length());\n+    _args_on_stack_cc = SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);\n+\n+    _regs_cc_ro = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc_ro->length());\n+    _args_on_stack_cc_ro = SharedRuntime::java_calling_convention(_sig_cc_ro, _regs_cc_ro);\n+\n+    _c1_needs_stack_repair = (_args_on_stack_cc < _args_on_stack) || (_args_on_stack_cc_ro < _args_on_stack);\n+    _c2_needs_stack_repair = (_args_on_stack_cc > _args_on_stack) || (_args_on_stack_cc > _args_on_stack_cc_ro);\n+  } else {\n+    \/\/ No scalarized args\n+    _sig_cc = _sig;\n+    _regs_cc = _regs;\n+    _args_on_stack_cc = _args_on_stack;\n+\n+    _sig_cc_ro = _sig;\n+    _regs_cc_ro = _regs;\n+    _args_on_stack_cc_ro = _args_on_stack;\n+  }\n+\n+#ifdef ASSERT\n+  {\n+    AdapterFingerPrint* compare_fp = AdapterFingerPrint::allocate(_sig_cc, _has_inline_recv);\n+    assert(fingerprint->equals(compare_fp), \"%s - %s\", fingerprint->as_string(), compare_fp->as_string());\n+    AdapterFingerPrint::deallocate(compare_fp);\n+  }\n+#endif\n+}\n@@ -2690,1 +3109,1 @@\n-void AdapterHandlerLibrary::verify_adapter_sharing(int total_args_passed, BasicType* sig_bt, AdapterHandlerEntry* cached_entry) {\n+void AdapterHandlerLibrary::verify_adapter_sharing(CompiledEntrySignature& ces, AdapterHandlerEntry* cached_entry) {\n@@ -2693,1 +3112,1 @@\n-  AdapterHandlerEntry* comparison_entry = create_adapter(total_args_passed, sig_bt, true);\n+  AdapterHandlerEntry* comparison_entry = create_adapter(ces, false, true);\n@@ -2718,2 +3137,13 @@\n-  \/\/ Fill in the signature array, for the calling-convention call.\n-  int total_args_passed = method->size_of_parameters(); \/\/ All args on stack\n+  CompiledEntrySignature ces(method());\n+  ces.compute_calling_conventions();\n+  if (ces.has_scalarized_args()) {\n+    if (!method->has_scalarized_args()) {\n+      method->set_has_scalarized_args();\n+    }\n+    if (ces.c1_needs_stack_repair()) {\n+      method->set_c1_needs_stack_repair();\n+    }\n+    if (ces.c2_needs_stack_repair() && !method->c2_needs_stack_repair()) {\n+      method->set_c2_needs_stack_repair();\n+    }\n+  }\n@@ -2721,4 +3151,0 @@\n-  AdapterSignatureIterator si(method->signature(), method->constMethod()->fingerprint(),\n-                              method->is_static(), total_args_passed);\n-  assert(si.slots() == total_args_passed, \"\");\n-  BasicType* sig_bt = si.basic_types();\n@@ -2728,1 +3154,1 @@\n-    entry = lookup(total_args_passed, sig_bt);\n+    entry = lookup(ces.sig_cc(), ces.has_inline_recv());\n@@ -2737,1 +3163,1 @@\n-        verify_adapter_sharing(total_args_passed, sig_bt, entry);\n+        verify_adapter_sharing(ces, entry);\n@@ -2741,1 +3167,1 @@\n-      entry = create_adapter(total_args_passed, sig_bt);\n+      entry = create_adapter(ces, \/* allocate_code_blob *\/ true);\n@@ -2796,0 +3222,2 @@\n+  entry_offset[AdapterBlob::C2I_Inline] = entry_address[AdapterBlob::C2I_Inline] - entry_address[AdapterBlob::I2C];\n+  entry_offset[AdapterBlob::C2I_Inline_RO] = entry_address[AdapterBlob::C2I_Inline_RO] - entry_address[AdapterBlob::I2C];\n@@ -2797,0 +3225,1 @@\n+  entry_offset[AdapterBlob::C2I_Unverified_Inline] = entry_address[AdapterBlob::C2I_Unverified_Inline] - entry_address[AdapterBlob::I2C];\n@@ -2805,2 +3234,2 @@\n-                                                  int total_args_passed,\n-                                                  BasicType* sig_bt,\n+                                                  CompiledEntrySignature& ces,\n+                                                  bool allocate_code_blob,\n@@ -2813,0 +3242,1 @@\n+  AdapterBlob* adapter_blob = nullptr;\n@@ -2819,2 +3249,1 @@\n-  VMRegPair stack_regs[16];\n-  VMRegPair* regs = (total_args_passed <= 16) ? stack_regs : NEW_RESOURCE_ARRAY(VMRegPair, total_args_passed);\n+  address entry_address[AdapterBlob::ENTRY_COUNT];\n@@ -2823,7 +3252,17 @@\n-  int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed);\n-  address entry_address[AdapterBlob::ENTRY_COUNT];\n-                                         total_args_passed,\n-                                         comp_args_on_stack,\n-                                         sig_bt,\n-                                         regs,\n-                                         entry_address);\n+                                         ces.args_on_stack(),\n+                                         ces.sig(),\n+                                         ces.regs(),\n+                                         ces.sig_cc(),\n+                                         ces.regs_cc(),\n+                                         ces.sig_cc_ro(),\n+                                         ces.regs_cc_ro(),\n+                                         entry_address,\n+                                         adapter_blob,\n+                                         allocate_code_blob);\n+\n+  if (ces.has_scalarized_args()) {\n+    \/\/ Save a C heap allocated version of the scalarized signature and store it in the adapter\n+    GrowableArray<SigEntry>* heap_sig = new (mtInternal) GrowableArray<SigEntry>(ces.sig_cc()->length(), mtInternal);\n+    heap_sig->appendAll(ces.sig_cc());\n+    handler->set_sig_cc(heap_sig);\n+  }\n@@ -2843,1 +3282,0 @@\n-  AdapterBlob* adapter_blob = AdapterBlob::create(&buffer, entry_offset);\n@@ -2870,2 +3308,2 @@\n-AdapterHandlerEntry* AdapterHandlerLibrary::create_adapter(int total_args_passed,\n-                                                           BasicType* sig_bt,\n+AdapterHandlerEntry* AdapterHandlerLibrary::create_adapter(CompiledEntrySignature& ces,\n+                                                           bool allocate_code_blob,\n@@ -2873,1 +3311,6 @@\n-  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(total_args_passed, sig_bt);\n+  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(ces.sig_cc(), ces.has_inline_recv());\n+#ifdef ASSERT\n+  \/\/ Verify that we can successfully restore the compiled entry signature object.\n+  CompiledEntrySignature ces_verify;\n+  ces_verify.initialize_from_fingerprint(fp);\n+#endif\n@@ -2875,1 +3318,1 @@\n-  if (!generate_adapter_code(handler, total_args_passed, sig_bt, is_transient)) {\n+  if (!generate_adapter_code(handler, ces, allocate_code_blob, is_transient)) {\n@@ -2978,3 +3421,3 @@\n-    int nargs;\n-    BasicType* bt = _fingerprint->as_basic_type(nargs);\n-    if (!AdapterHandlerLibrary::generate_adapter_code(this, nargs, bt, \/* is_transient *\/ false)) {\n+    CompiledEntrySignature ces;\n+    ces.initialize_from_fingerprint(_fingerprint);\n+    if (!AdapterHandlerLibrary::generate_adapter_code(this, ces, true, false)) {\n@@ -3007,13 +3450,26 @@\n-  _no_arg_handler = lookup(0, nullptr);\n-\n-  BasicType obj_args[] = { T_OBJECT };\n-  _obj_arg_handler = lookup(1, obj_args);\n-\n-  BasicType int_args[] = { T_INT };\n-  _int_arg_handler = lookup(1, int_args);\n-\n-  BasicType obj_int_args[] = { T_OBJECT, T_INT };\n-  _obj_int_arg_handler = lookup(2, obj_int_args);\n-\n-  BasicType obj_obj_args[] = { T_OBJECT, T_OBJECT };\n-  _obj_obj_arg_handler = lookup(2, obj_obj_args);\n+  ResourceMark rm;\n+  CompiledEntrySignature no_args;\n+  no_args.compute_calling_conventions();\n+  _no_arg_handler = lookup(no_args.sig_cc(), no_args.has_inline_recv());\n+\n+  CompiledEntrySignature obj_args;\n+  SigEntry::add_entry(obj_args.sig(), T_OBJECT);\n+  obj_args.compute_calling_conventions();\n+  _obj_arg_handler = lookup(obj_args.sig_cc(), obj_args.has_inline_recv());\n+\n+  CompiledEntrySignature int_args;\n+  SigEntry::add_entry(int_args.sig(), T_INT);\n+  int_args.compute_calling_conventions();\n+  _int_arg_handler = lookup(int_args.sig_cc(), int_args.has_inline_recv());\n+\n+  CompiledEntrySignature obj_int_args;\n+  SigEntry::add_entry(obj_int_args.sig(), T_OBJECT);\n+  SigEntry::add_entry(obj_int_args.sig(), T_INT);\n+  obj_int_args.compute_calling_conventions();\n+  _obj_int_arg_handler = lookup(obj_int_args.sig_cc(), obj_int_args.has_inline_recv());\n+\n+  CompiledEntrySignature obj_obj_args;\n+  SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+  SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+  obj_obj_args.compute_calling_conventions();\n+  _obj_obj_arg_handler = lookup(obj_obj_args.sig_cc(), obj_obj_args.has_inline_recv());\n@@ -3048,0 +3504,3 @@\n+  if (_sig_cc != nullptr) {\n+    delete _sig_cc;\n+  }\n@@ -3135,0 +3594,1 @@\n+      BasicType stack_sig_bt[16];\n@@ -3136,0 +3596,1 @@\n+      BasicType* sig_bt = (total_args_passed <= 16) ? stack_sig_bt : NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n@@ -3138,5 +3599,13 @@\n-      AdapterSignatureIterator si(method->signature(), method->constMethod()->fingerprint(),\n-                              method->is_static(), total_args_passed);\n-      BasicType* sig_bt = si.basic_types();\n-      assert(si.slots() == total_args_passed, \"\");\n-      BasicType ret_type = si.return_type();\n+      int i = 0;\n+      if (!method->is_static()) {  \/\/ Pass in receiver first\n+        sig_bt[i++] = T_OBJECT;\n+      }\n+      SignatureStream ss(method->signature());\n+      for (; !ss.at_return_type(); ss.next()) {\n+        sig_bt[i++] = ss.type();  \/\/ Collect remaining bits of signature\n+        if (ss.type() == T_LONG || ss.type() == T_DOUBLE) {\n+          sig_bt[i++] = T_VOID;   \/\/ Longs & doubles take 2 Java slots\n+        }\n+      }\n+      assert(i == total_args_passed, \"\");\n+      BasicType ret_type = ss.type();\n@@ -3410,1 +3879,4 @@\n-    st->print(\" c2iUV: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+    st->print(\" c2iVE: \" INTPTR_FORMAT, p2i(get_c2i_inline_entry()));\n+    st->print(\" c2iVROE: \" INTPTR_FORMAT, p2i(get_c2i_inline_ro_entry()));\n+    st->print(\" c2iUE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+    st->print(\" c2iUVE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_inline_entry()));\n@@ -3498,0 +3970,195 @@\n+\n+\/\/ We are at a compiled code to interpreter call. We need backing\n+\/\/ buffers for all inline type arguments. Allocate an object array to\n+\/\/ hold them (convenient because once we're done with it we don't have\n+\/\/ to worry about freeing it).\n+oop SharedRuntime::allocate_inline_types_impl(JavaThread* current, methodHandle callee, bool allocate_receiver, TRAPS) {\n+  assert(InlineTypePassFieldsAsArgs, \"no reason to call this\");\n+  ResourceMark rm;\n+\n+  int nb_slots = 0;\n+  InstanceKlass* holder = callee->method_holder();\n+  allocate_receiver &= !callee->is_static() && holder->is_inline_klass() && callee->is_scalarized_arg(0);\n+  if (allocate_receiver) {\n+    nb_slots++;\n+  }\n+  int arg_num = callee->is_static() ? 0 : 1;\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    BasicType bt = ss.type();\n+    if (bt == T_OBJECT && callee->is_scalarized_arg(arg_num)) {\n+      nb_slots++;\n+    }\n+    if (bt != T_VOID) {\n+      arg_num++;\n+    }\n+  }\n+  objArrayOop array_oop = oopFactory::new_objectArray(nb_slots, CHECK_NULL);\n+  objArrayHandle array(THREAD, array_oop);\n+  arg_num = callee->is_static() ? 0 : 1;\n+  int i = 0;\n+  if (allocate_receiver) {\n+    InlineKlass* vk = InlineKlass::cast(holder);\n+    oop res = vk->allocate_instance(CHECK_NULL);\n+    array->obj_at_put(i++, res);\n+  }\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    BasicType bt = ss.type();\n+    if (bt == T_OBJECT && callee->is_scalarized_arg(arg_num)) {\n+      InlineKlass* vk = ss.as_inline_klass(holder);\n+      assert(vk != nullptr, \"Unexpected klass\");\n+      oop res = vk->allocate_instance(CHECK_NULL);\n+      array->obj_at_put(i++, res);\n+    }\n+    if (bt != T_VOID) {\n+      arg_num++;\n+    }\n+  }\n+  return array();\n+}\n+\n+JRT_ENTRY(void, SharedRuntime::allocate_inline_types(JavaThread* current, Method* callee_method, bool allocate_receiver))\n+  methodHandle callee(current, callee_method);\n+  oop array = SharedRuntime::allocate_inline_types_impl(current, callee, allocate_receiver, CHECK);\n+  current->set_vm_result_oop(array);\n+  current->set_vm_result_metadata(callee()); \/\/ TODO: required to keep callee live?\n+JRT_END\n+\n+\/\/ We're returning from an interpreted method: load each field into a\n+\/\/ register following the calling convention\n+JRT_LEAF(void, SharedRuntime::load_inline_type_fields_in_regs(JavaThread* current, oopDesc* res))\n+{\n+  assert(res->klass()->is_inline_klass(), \"only inline types here\");\n+  ResourceMark rm;\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  frame stubFrame = current->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+  assert(callerFrame.is_interpreted_frame(), \"should be coming from interpreter\");\n+\n+  InlineKlass* vk = InlineKlass::cast(res->klass());\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  if (regs == nullptr) {\n+    \/\/ The fields of the inline klass don't fit in registers, bail out\n+    return;\n+  }\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    address loc = reg_map.location(pair.first(), nullptr);\n+    switch(bt) {\n+    case T_BOOLEAN:\n+      *(jboolean*)loc = res->bool_field(off);\n+      break;\n+    case T_CHAR:\n+      *(jchar*)loc = res->char_field(off);\n+      break;\n+    case T_BYTE:\n+      *(jbyte*)loc = res->byte_field(off);\n+      break;\n+    case T_SHORT:\n+      *(jshort*)loc = res->short_field(off);\n+      break;\n+    case T_INT: {\n+      *(jint*)loc = res->int_field(off);\n+      break;\n+    }\n+    case T_LONG:\n+#ifdef _LP64\n+      *(intptr_t*)loc = res->long_field(off);\n+#else\n+      Unimplemented();\n+#endif\n+      break;\n+    case T_OBJECT:\n+    case T_ARRAY: {\n+      *(oop*)loc = res->obj_field(off);\n+      break;\n+    }\n+    case T_FLOAT:\n+      *(jfloat*)loc = res->float_field(off);\n+      break;\n+    case T_DOUBLE:\n+      *(jdouble*)loc = res->double_field(off);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+#ifdef ASSERT\n+  VMRegPair pair = regs->at(0);\n+  address loc = reg_map.location(pair.first(), nullptr);\n+  assert(*(oopDesc**)loc == res, \"overwritten object\");\n+#endif\n+\n+  current->set_vm_result_oop(res);\n+}\n+JRT_END\n+\n+\/\/ We've returned to an interpreted method, the interpreter needs a\n+\/\/ reference to an inline type instance. Allocate it and initialize it\n+\/\/ from field's values in registers.\n+JRT_BLOCK_ENTRY(void, SharedRuntime::store_inline_type_fields_to_buf(JavaThread* current, intptr_t res))\n+{\n+  ResourceMark rm;\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  frame stubFrame = current->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+\n+#ifdef ASSERT\n+  InlineKlass* verif_vk = InlineKlass::returned_inline_klass(reg_map);\n+#endif\n+\n+  if (!is_set_nth_bit(res, 0)) {\n+    \/\/ We're not returning with inline type fields in registers (the\n+    \/\/ calling convention didn't allow it for this inline klass)\n+    assert(!Metaspace::contains((void*)res), \"should be oop or pointer in buffer area\");\n+    current->set_vm_result_oop((oopDesc*)res);\n+    assert(verif_vk == nullptr, \"broken calling convention\");\n+    return;\n+  }\n+\n+  clear_nth_bit(res, 0);\n+  InlineKlass* vk = (InlineKlass*)res;\n+  assert(verif_vk == vk, \"broken calling convention\");\n+  assert(Metaspace::contains((void*)res), \"should be klass\");\n+\n+  \/\/ Allocate handles for every oop field so they are safe in case of\n+  \/\/ a safepoint when allocating\n+  GrowableArray<Handle> handles;\n+  vk->save_oop_fields(reg_map, handles);\n+\n+  \/\/ It's unsafe to safepoint until we are here\n+  JRT_BLOCK;\n+  {\n+    JavaThread* THREAD = current;\n+    oop vt = vk->realloc_result(reg_map, handles, CHECK);\n+    current->set_vm_result_oop(vt);\n+  }\n+  JRT_BLOCK_END;\n+}\n+JRT_END\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":910,"deletions":243,"binary":false,"changes":1153,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"asm\/codeBuffer.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -41,0 +43,1 @@\n+class SigEntry;\n@@ -378,0 +381,2 @@\n+  static char* generate_identity_exception_message(JavaThread* thr, Klass* klass);\n+\n@@ -380,1 +385,1 @@\n-  static methodHandle resolve_helper(bool is_virtual, bool is_optimized, TRAPS);\n+  static methodHandle resolve_helper(bool is_virtual, bool is_optimized, bool& caller_is_c1, TRAPS);\n@@ -388,1 +393,1 @@\n-                                             bool& needs_ic_stub_refill, TRAPS);\n+                                             bool& needs_ic_stub_refill, bool& is_optimized, bool caller_is_c1, TRAPS);\n@@ -394,1 +399,1 @@\n-  static methodHandle reresolve_call_site(TRAPS);\n+  static methodHandle reresolve_call_site(bool& is_static_call, bool& is_optimized, bool& caller_is_c1, TRAPS);\n@@ -398,1 +403,1 @@\n-  static methodHandle handle_ic_miss_helper(TRAPS);\n+  static methodHandle handle_ic_miss_helper(bool& caller_is_c1, TRAPS);\n@@ -401,1 +406,1 @@\n-  static methodHandle find_callee_method(TRAPS);\n+  static methodHandle find_callee_method(bool& caller_is_c1, TRAPS);\n@@ -432,0 +437,8 @@\n+  static int java_calling_convention(const GrowableArray<SigEntry>* sig, VMRegPair* regs) {\n+    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig->length());\n+    int total_args_passed = SigEntry::fill_sig_bt(sig, sig_bt);\n+    return java_calling_convention(sig_bt, regs, total_args_passed);\n+  }\n+  static int java_return_convention(const BasicType* sig_bt, VMRegPair* regs, int total_args_passed);\n+  static const uint java_return_convention_max_int;\n+  static const uint java_return_convention_max_float;\n@@ -478,1 +491,1 @@\n-  static void generate_i2c2i_adapters(MacroAssembler *_masm,\n+  static void generate_i2c2i_adapters(MacroAssembler* _masm,\n@@ -480,4 +493,9 @@\n-                                      int max_arg,\n-                                      const BasicType *sig_bt,\n-                                      const VMRegPair *regs,\n-                                      address entry_address[AdapterBlob::ENTRY_COUNT]);\n+                                      const GrowableArray<SigEntry>* sig,\n+                                      const VMRegPair* regs,\n+                                      const GrowableArray<SigEntry>* sig_cc,\n+                                      const VMRegPair* regs_cc,\n+                                      const GrowableArray<SigEntry>* sig_cc_ro,\n+                                      const VMRegPair* regs_cc_ro,\n+                                      address entry_address[AdapterBlob::ENTRY_COUNT],\n+                                      AdapterBlob*& new_adapter,\n+                                      bool allocate_code_blob);\n@@ -486,2 +504,1 @@\n-                              int total_args_passed,\n-                              const BasicType *sig_bt,\n+                              const GrowableArray<SigEntry>* sig,\n@@ -561,1 +578,2 @@\n-  static address get_resolved_entry        (JavaThread* current, methodHandle callee_method);\n+  static address get_resolved_entry        (JavaThread* current, methodHandle callee_method,\n+                                            bool is_static_call, bool is_optimized, bool caller_is_c1);\n@@ -566,0 +584,3 @@\n+  static void load_inline_type_fields_in_regs(JavaThread* current, oopDesc* res);\n+  static void store_inline_type_fields_to_buf(JavaThread* current, intptr_t res);\n+\n@@ -576,0 +597,2 @@\n+  static void allocate_inline_types(JavaThread* current, Method* callee, bool allocate_receiver);\n+  static oop allocate_inline_types_impl(JavaThread* current, methodHandle callee, bool allocate_receiver, TRAPS);\n@@ -579,0 +602,1 @@\n+  static BufferedInlineTypeBlob* generate_buffered_inline_type_adapter(const InlineKlass* vk);\n@@ -684,1 +708,1 @@\n-  static const int ENTRIES_COUNT = 4;\n+  static const int ENTRIES_COUNT = 7;\n@@ -693,0 +717,3 @@\n+  \/\/ Support for scalarized inline type calling convention\n+  const GrowableArray<SigEntry>* _sig_cc;\n+\n@@ -703,1 +730,2 @@\n-    _linked(false)\n+    _linked(false),\n+    _sig_cc(nullptr)\n@@ -754,0 +782,18 @@\n+  address get_c2i_inline_entry() const {\n+#ifndef ZERO\n+    assert(_adapter_blob != nullptr, \"must be\");\n+    return _adapter_blob->c2i_inline_entry();\n+#else\n+    return nullptr;\n+#endif \/\/ ZERO\n+  }\n+\n+  address get_c2i_inline_ro_entry() const {\n+#ifndef ZERO\n+    assert(_adapter_blob != nullptr, \"must be\");\n+    return _adapter_blob->c2i_inline_ro_entry();\n+#else\n+    return nullptr;\n+#endif \/\/ ZERO\n+  }\n+\n@@ -763,0 +809,9 @@\n+  address get_c2i_unverified_inline_entry() const {\n+#ifndef ZERO\n+    assert(_adapter_blob != nullptr, \"must be\");\n+    return _adapter_blob->c2i_unverified_inline_entry();\n+#else\n+    return nullptr;\n+#endif \/\/ ZERO\n+  }\n+\n@@ -774,0 +829,4 @@\n+  \/\/ Support for scalarized inline type calling convention\n+  void set_sig_cc(const GrowableArray<SigEntry>* sig)  { _sig_cc = sig; }\n+  const GrowableArray<SigEntry>* get_sig_cc()    const { return _sig_cc; }\n+\n@@ -798,0 +857,2 @@\n+class CompiledEntrySignature;\n+\n@@ -815,2 +876,2 @@\n-  static AdapterHandlerEntry* create_adapter(int total_args_passed,\n-                                             BasicType* sig_bt,\n+  static AdapterHandlerEntry* create_adapter(CompiledEntrySignature& ces,\n+                                             bool allocate_code_blob,\n@@ -827,1 +888,1 @@\n-  static AdapterHandlerEntry* lookup(int total_args_passed, BasicType* sig_bt);\n+  static AdapterHandlerEntry* lookup(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false);\n@@ -829,2 +890,2 @@\n-                                    int total_args_passed,\n-                                    BasicType* sig_bt,\n+                                    CompiledEntrySignature& ces,\n+                                    bool allocate_code_blob,\n@@ -834,1 +895,1 @@\n-  static void verify_adapter_sharing(int total_args_passed, BasicType* sig_bt, AdapterHandlerEntry* cached);\n+  static void verify_adapter_sharing(CompiledEntrySignature& ces, AdapterHandlerEntry* cached_entry);\n@@ -853,0 +914,60 @@\n+\/\/ Utility class for computing the calling convention of the 3 types\n+\/\/ of compiled method entries:\n+\/\/     Method::_from_compiled_entry               - sig_cc\n+\/\/     Method::_from_compiled_inline_ro_entry     - sig_cc_ro\n+\/\/     Method::_from_compiled_inline_entry        - sig\n+class CompiledEntrySignature : public StackObj {\n+  Method* _method;\n+  int  _num_inline_args;\n+  bool _has_inline_recv;\n+  GrowableArray<SigEntry>* _sig;\n+  GrowableArray<SigEntry>* _sig_cc;\n+  GrowableArray<SigEntry>* _sig_cc_ro;\n+  VMRegPair* _regs;\n+  VMRegPair* _regs_cc;\n+  VMRegPair* _regs_cc_ro;\n+\n+  int _args_on_stack;\n+  int _args_on_stack_cc;\n+  int _args_on_stack_cc_ro;\n+\n+  bool _c1_needs_stack_repair;\n+  bool _c2_needs_stack_repair;\n+\n+  GrowableArray<Method*>* _supers;\n+\n+public:\n+  Method* method()                     const { return _method; }\n+\n+  \/\/ Used by Method::_from_compiled_inline_entry\n+  GrowableArray<SigEntry>* sig()       const { return _sig; }\n+\n+  \/\/ Used by Method::_from_compiled_entry\n+  GrowableArray<SigEntry>* sig_cc()    const { return _sig_cc; }\n+\n+  \/\/ Used by Method::_from_compiled_inline_ro_entry\n+  GrowableArray<SigEntry>* sig_cc_ro() const { return _sig_cc_ro; }\n+\n+  VMRegPair* regs()                    const { return _regs; }\n+  VMRegPair* regs_cc()                 const { return _regs_cc; }\n+  VMRegPair* regs_cc_ro()              const { return _regs_cc_ro; }\n+\n+  int args_on_stack()                  const { return _args_on_stack; }\n+  int args_on_stack_cc()               const { return _args_on_stack_cc; }\n+  int args_on_stack_cc_ro()            const { return _args_on_stack_cc_ro; }\n+\n+  int  num_inline_args()               const { return _num_inline_args; }\n+  bool has_inline_recv()               const { return _has_inline_recv; }\n+\n+  bool has_scalarized_args()           const { return _sig != _sig_cc; }\n+  bool c1_needs_stack_repair()         const { return _c1_needs_stack_repair; }\n+  bool c2_needs_stack_repair()         const { return _c2_needs_stack_repair; }\n+  CodeOffsets::Entries c1_inline_ro_entry_type() const;\n+\n+  GrowableArray<Method*>* get_supers();\n+\n+  CompiledEntrySignature(Method* method = nullptr);\n+  void compute_calling_conventions(bool init = true);\n+  void initialize_from_fingerprint(AdapterFingerPrint* fingerprint);\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":142,"deletions":21,"binary":false,"changes":163,"status":"modified"},{"patch":"@@ -40,7 +40,0 @@\n-template StackValue* StackValue::create_stack_value(const frame* fr, const RegisterMap* reg_map, ScopeValue* sv);\n-template StackValue* StackValue::create_stack_value(const frame* fr, const SmallRegisterMap* reg_map, ScopeValue* sv);\n-\n-template<typename RegisterMapT>\n-StackValue* StackValue::create_stack_value(const frame* fr, const RegisterMapT* reg_map, ScopeValue* sv) {\n-  return create_stack_value(sv, stack_value_address(fr, reg_map, sv), reg_map);\n-}\n@@ -147,0 +140,4 @@\n+\n+template StackValue* StackValue::create_stack_value(const frame* fr, const RegisterMap* reg_map, ScopeValue* sv);\n+template StackValue* StackValue::create_stack_value(const frame* fr, const SmallRegisterMap* reg_map, ScopeValue* sv);\n+\n@@ -148,1 +145,2 @@\n-StackValue* StackValue::create_stack_value(ScopeValue* sv, address value_addr, const RegisterMapT* reg_map) {\n+StackValue* StackValue::create_stack_value(const frame* fr, const RegisterMapT* reg_map, ScopeValue* sv) {\n+  address value_addr = stack_value_address(fr, reg_map, sv);\n@@ -249,1 +247,10 @@\n-    return new StackValue(hdl, hdl.is_null() && ov->is_scalar_replaced() ? 1 : 0);\n+    bool scalar_replaced = hdl.is_null() && ov->is_scalar_replaced();\n+    if (ov->has_properties()) {\n+      Klass* k = java_lang_Class::as_Klass(ov->klass()->as_ConstantOopReadValue()->value()());\n+      if (!k->is_array_klass()) {\n+        \/\/ Don't treat inline type as scalar replaced if it is null\n+        jint null_marker = StackValue::create_stack_value(fr, reg_map, ov->properties())->get_jint();\n+        scalar_replaced &= (null_marker != 0);\n+      }\n+    }\n+    return new StackValue(hdl, scalar_replaced ? 1 : 0);\n","filename":"src\/hotspot\/share\/runtime\/stackValue.cpp","additions":16,"deletions":9,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -411,0 +411,1 @@\n+  initialize_class(vmSymbols::java_lang_IdentityException(), CHECK);\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -117,0 +117,1 @@\n+  template(ClassPrintLayout)                      \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -606,0 +606,9 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Prototyping\n+\/\/ \"Code Missing Here\" macro, un-define when integrating back from prototyping stage and break\n+\/\/ compilation on purpose (i.e. \"forget me not\")\n+#define PROTOTYPE\n+#ifdef PROTOTYPE\n+#define CMH(m)\n+#endif\n+\n@@ -684,5 +693,6 @@\n-  T_ADDRESS     = 15,\n-  T_NARROWOOP   = 16,\n-  T_METADATA    = 17,\n-  T_NARROWKLASS = 18,\n-  T_CONFLICT    = 19, \/\/ for stack value type with conflicting contents\n+  T_FLAT_ELEMENT = 15, \/\/ Not a true BasicType, only used in layout helpers of flat arrays\n+  T_ADDRESS     = 16,\n+  T_NARROWOOP   = 17,\n+  T_METADATA    = 18,\n+  T_NARROWKLASS = 19,\n+  T_CONFLICT    = 20, \/\/ for stack value type with conflicting contents\n@@ -732,0 +742,1 @@\n+  assert(t != T_FLAT_ELEMENT, \"\");  \/\/ Strong assert to detect misuses of T_FLAT_ELEMENT\n@@ -806,1 +817,2 @@\n-  T_VOID_size        = 0\n+  T_VOID_size        = 0,\n+  T_FLAT_ELEMENT_size = 0\n@@ -842,1 +854,2 @@\n-  T_VOID_aelem_bytes        = 0\n+  T_VOID_aelem_bytes        = 0,\n+  T_FLAT_ELEMENT_aelem_bytes = 0\n@@ -932,1 +945,1 @@\n-  vtos = 9,             \/\/ tos not cached\n+  vtos = 9,             \/\/ tos not cached,\n@@ -949,1 +962,1 @@\n-    case T_ARRAY  : \/\/ fall through\n+    case T_ARRAY  :   \/\/ fall through\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":22,"deletions":9,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -41,0 +41,2 @@\n+import static java.io.ObjectInputStream.TRACE;\n+\n@@ -135,0 +137,6 @@\n+ * Value classes implementing {@link Externalizable} cannot be serialized\n+ * or deserialized, the value object is immutable and the state cannot be restored.\n+ * Use {@link Serializable} {@code writeReplace} to delegate to another serializable\n+ * object such as a record.\n+ *\n+ * Value objects cannot be {@code java.io.Externalizable}.\n@@ -160,0 +168,1 @@\n+ * <a id=\"record-serialization\"><\/a>\n@@ -163,0 +172,26 @@\n+ * <a id=\"valueclass-serialization\"><\/a>\n+ * <p>Value classes are {@linkplain Serializable} through the use of the serialization proxy pattern.\n+ * The serialization protocol does not support a standard serialized form for value classes.\n+ * The value class delegates to a serialization proxy by supplying an alternate\n+ * record or object to be serialized instead of the value class.\n+ * When the proxy is deserialized it re-constructs the value object and returns the value object.\n+ * For example,\n+ * {@snippet lang=\"java\" :\n+ * value class ZipCode implements Serializable {    \/\/ @highlight substring=\"value class\"\n+ *     private static final long serialVersionUID = 1L;\n+ *     private int zipCode;\n+ *     public ZipCode(int zip) { this.zipCode = zip; }\n+ *     public int zipCode() { return zipCode; }\n+ *\n+ *     public Object writeReplace() {    \/\/ @highlight substring=\"writeReplace\"\n+ *         return new ZipCodeProxy(zipCode);\n+ *     }\n+ *\n+ *     private record ZipCodeProxy(int zipCode) implements Serializable {\n+ *         public Object readResolve() {    \/\/ @highlight substring=\"readResolve\"\n+ *             return new ZipCode(zipCode);\n+ *         }\n+ *     }\n+ * }\n+ * }\n+ *\n@@ -309,0 +344,3 @@\n+     * <p>Serialization and deserialization of value classes is described in\n+     * {@linkplain ObjectOutputStream##valueclass-serialization value class serialization}.\n+     *\n@@ -1317,0 +1355,3 @@\n+                if (desc.isValue())\n+                    throw new InvalidClassException(\"Externalizable not valid for value class \"\n+                            + desc.forClass().getName());\n@@ -1365,2 +1406,2 @@\n-        ObjectStreamClass.ClassDataSlot[] slots = desc.getClassDataLayout();\n-        if (slots.length != 1) {\n+        List<ObjectStreamClass.ClassDataSlot> slots = desc.getClassDataLayout();\n+        if (slots.size() != 1) {\n@@ -1368,1 +1409,1 @@\n-                    \"expected a single record slot length, but found: \" + slots.length);\n+                    \"expected a single record slot length, but found: \" + slots.size());\n@@ -1381,3 +1422,3 @@\n-        ObjectStreamClass.ClassDataSlot[] slots = desc.getClassDataLayout();\n-        for (int i = 0; i < slots.length; i++) {\n-            ObjectStreamClass slotDesc = slots[i].desc;\n+       List<ObjectStreamClass.ClassDataSlot> slots = desc.getClassDataLayout();\n+        for (int i = 0; i < slots.size(); i++) {\n+            ObjectStreamClass slotDesc = slots.get(i).desc;\n","filename":"src\/java.base\/share\/classes\/java\/io\/ObjectOutputStream.java","additions":47,"deletions":6,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+import jdk.internal.value.DeserializeConstructor;\n@@ -51,4 +52,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Double} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -360,0 +369,1 @@\n+@jdk.internal.MigratedValueClass\n@@ -950,0 +960,1 @@\n+    @DeserializeConstructor\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Double.java","additions":15,"deletions":4,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+import jdk.internal.value.DeserializeConstructor;\n@@ -51,4 +52,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Float} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -78,0 +87,1 @@\n+@jdk.internal.MigratedValueClass\n@@ -577,0 +587,1 @@\n+    @DeserializeConstructor\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Float.java","additions":15,"deletions":4,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+import java.lang.reflect.ClassFileFormatVersion;\n@@ -490,0 +491,15 @@\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          The \"identity hash code\" of a {@linkplain Class#isValue() value object}\n+     *          is computed by combining the identity hash codes of the value object's fields recursively.\n+     *      <\/div>\n+     * <\/div>\n+     * @apiNote\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          Note that, like ==, this hash code exposes information about a value object's\n+     *          private fields that might otherwise be hidden by an identity object.\n+     *          Developers should be cautious about storing sensitive secrets in value object fields.\n+     *      <\/div>\n+     * <\/div>\n+     *\n@@ -2313,0 +2329,4 @@\n+            public int classFileFormatVersion(Class<?> clazz) {\n+                return clazz.getClassFileVersion();\n+            }\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/System.java","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+import java.lang.reflect.ClassFileFormatVersion;\n@@ -606,0 +607,6 @@\n+\n+    \/**\n+     * Returns the class file format version of the class.\n+     *\/\n+    int classFileFormatVersion(Class<?> klass);\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/access\/JavaLangAccess.java","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+import java.lang.classfile.ClassModel;\n@@ -35,0 +36,1 @@\n+import java.util.HashSet;\n@@ -40,0 +42,1 @@\n+import static java.lang.classfile.ClassFile.*;\n@@ -50,0 +53,3 @@\n+    private WritableField.UnsetField[] strictInstanceFields; \/\/ do not modify array contents\n+    private ClassModel lastStrictCheckClass; \/\/ buf writer has short life, so do not need weak here\n+    private boolean lastStrictCheckResult;\n@@ -72,0 +78,27 @@\n+    public boolean strictFieldsMatch(ClassModel cm) {\n+        \/\/ We have a cache because this check will be called multiple times\n+        \/\/ if a MethodModel is sent wholesale\n+        if (lastStrictCheckClass == cm) {\n+            return lastStrictCheckResult;\n+        }\n+\n+        var result = doStrictFieldsMatchCheck(cm);\n+        lastStrictCheckClass = cm;\n+        lastStrictCheckResult = result;\n+        return result;\n+    }\n+\n+    private boolean doStrictFieldsMatchCheck(ClassModel cm) {\n+        \/\/ TODO only check for preview class files?\n+        \/\/ UTF8 Entry can be used as equality objects\n+        var checks = new HashSet<>(Arrays.asList(getStrictInstanceFields()));\n+        for (var f : cm.fields()) {\n+            if ((f.flags().flagsMask() & (ACC_STATIC | ACC_STRICT_INIT)) == ACC_STRICT_INIT) {\n+                if (!checks.remove(new WritableField.UnsetField(f.fieldName(), f.fieldType()))) {\n+                    return false; \/\/ Field mismatch!\n+                }\n+            }\n+        }\n+        return checks.isEmpty();\n+    }\n+\n@@ -86,0 +119,10 @@\n+    public WritableField.UnsetField[] getStrictInstanceFields() {\n+        assert strictInstanceFields != null : \"should access only after setter call in DirectClassBuilder\";\n+        return strictInstanceFields;\n+    }\n+\n+    public void setStrictInstanceFields(WritableField.UnsetField[] strictInstanceFields) {\n+        this.strictInstanceFields = strictInstanceFields;\n+    }\n+\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/classfile\/impl\/BufWriterImpl.java","additions":43,"deletions":0,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -87,0 +87,2 @@\n+            case LoadableDescriptorsAttribute pa ->\n+                clb.with(LoadableDescriptorsAttribute.of(pa.loadableDescriptors()));\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/classfile\/impl\/ClassRemapperImpl.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -138,1 +138,0 @@\n-\n@@ -157,1 +156,2 @@\n-        jdk.compiler;\n+        jdk.compiler,\n+        jdk.jdeps;\n@@ -263,0 +263,2 @@\n+    exports jdk.internal.value to  \/\/ Needed by Unsafe\n+        jdk.unsupported;\n","filename":"src\/java.base\/share\/classes\/module-info.java","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -100,1 +100,1 @@\n-    final int instanceKlassMiscFlagsOffset = getFieldOffset(\"InstanceKlass::_misc_flags._flags\", Integer.class, \"u2\");\n+    final int instanceKlassMiscFlagsOffset = getFieldOffset(\"InstanceKlass::_misc_flags._flags\", Integer.class, \"u4\");\n@@ -307,0 +307,3 @@\n+    final int dataLayoutArrayLoadDataTag = getConstant(\"DataLayout::array_load_data_tag\", Integer.class);\n+    final int dataLayoutArrayStoreDataTag = getConstant(\"DataLayout::array_store_data_tag\", Integer.class);\n+    final int dataLayoutACmpDataTag = getConstant(\"DataLayout::acmp_data_tag\", Integer.class);\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -116,0 +116,1 @@\n+compiler\/jvmci\/jdk.vm.ci.code.test\/src\/jdk\/vm\/ci\/code\/test\/MethodTagTest.java                   8343233 generic-aarch64\n","filename":"test\/hotspot\/jtreg\/ProblemList-zgc.txt","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -82,0 +82,8 @@\n+compiler\/jvmci\/jdk.vm.ci.hotspot.test\/src\/jdk\/vm\/ci\/hotspot\/test\/MemoryAccessProviderTest.java 8350208 generic-all\n+compiler\/jvmci\/jdk.vm.ci.hotspot.test\/src\/jdk\/vm\/ci\/hotspot\/test\/TestHotSpotResolvedJavaField.java 8350208 generic-all\n+\n+# Valhalla\n+compiler\/regalloc\/TestVerifyRegisterAllocator.java 8365895 windows-x64\n+compiler\/types\/TestArrayManyDimensions.java 8365895 windows-x64\n+compiler\/types\/correctness\/OffTest.java 8365895 windows-x64\n+\n@@ -100,0 +108,1 @@\n+runtime\/cds\/appcds\/redefineClass\/RedefineRunningMethods_Shared.java  8304168 generic-all\n@@ -120,0 +129,12 @@\n+\n+# Valhalla\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictInstanceFieldsTest.java CODETOOLS-7904031 generic-all\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictStaticFieldsTest.java CODETOOLS-7904031 generic-all\n+\n+runtime\/cds\/TestDefaultArchiveLoading.java#coops_nocoh            8366774           generic-all\n+runtime\/cds\/TestDefaultArchiveLoading.java#nocoops_nocoh          8366774           generic-all\n+\n+# Valhalla + AOT\n+runtime\/cds\/appcds\/aotCache\/HelloAOTCache.java                                  8369043 generic-aarch64\n+runtime\/cds\/appcds\/methodHandles\/MethodHandlesGeneralTest.java#aot              8367408 generic-all\n+\n@@ -149,0 +170,58 @@\n+# Valhalla TODO:\n+serviceability\/jvmti\/valhalla\/HeapDump\/HeapDump.java 8317416 generic-all\n+\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbDumpclass.java 8190936 generic-all\n+\n+# Array Changes TODO\n+serviceability\/sa\/CDSJMapClstats.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbClasses.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DisableRegistryTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbDumpheap.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbTestConnectArgument.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DebugdConnectTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJhisto.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id1 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackWithConcurrentLock.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackXcompStress.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#process 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#core 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id0 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id1 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#serial 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#parallel 8365722 generic-all\n+serviceability\/sa\/DeadlockDetectionTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id0 8365722 generic-all\n+serviceability\/sa\/TestInstanceKlassSize.java 8365722 generic-all\n+serviceability\/sa\/TestSysProps.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbAttachToDebugServer.java 8365722 generic-all\n+resourcehogs\/serviceability\/sa\/TestHeapDumpForLargeArray.java 8365722 generic-all\n+serviceability\/HeapDump\/DuplicateArrayClassesTest.java 8365722 generic-all\n+\n+resourcehogs\/serviceability\/sa\/ClhsdbRegionDetailsScanOopsForG1.java 8190936 generic-all\n+vmTestbase\/nsk\/jvmti\/scenarios\/events\/EM04\/em04t001\/TestDescription.java 8367590 generic-all\n+\n@@ -187,0 +266,2 @@\n+vmTestbase\/vm\/mlvm\/hiddenloader\/stress\/byteMutation\/Test.java 8317172 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":81,"deletions":0,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-  runtime\n+  runtime \\\n@@ -65,0 +65,8 @@\n+hotspot_valhalla = \\\n+  runtime\/valhalla \\\n+  compiler\/valhalla \\\n+  serviceability\/jvmti\/valhalla\n+\n+hotspot_valhalla_runtime = \\\n+  runtime\/valhalla\n+\n@@ -213,0 +221,1 @@\n+  compiler\/valhalla\/ \\\n@@ -254,0 +263,7 @@\n+\n+tier1_compiler_no_valhalla = \\\n+  :tier1_compiler_1 \\\n+  :tier1_compiler_2 \\\n+  :tier1_compiler_3 \\\n+  -compiler\/valhalla\n+\n@@ -406,0 +422,4 @@\n+tier1_runtime_no_valhalla = \\\n+  :tier1_runtime \\\n+  -runtime\/valhalla\n+\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":21,"deletions":1,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -552,0 +552,1 @@\n+    @Warmup(5000)\n@@ -731,1 +732,1 @@\n-        counts = {IRNode.G1_GET_AND_SET_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        counts = {IRNode.G1_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n@@ -734,1 +735,1 @@\n-        counts = {IRNode.G1_GET_AND_SET_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        counts = {IRNode.G1_COMPARE_AND_SWAP_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n@@ -789,1 +790,1 @@\n-        counts = {IRNode.G1_GET_AND_SET_P_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n+        counts = {IRNode.G1_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n@@ -792,1 +793,1 @@\n-        counts = {IRNode.G1_GET_AND_SET_N_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n+        counts = {IRNode.G1_COMPARE_AND_SWAP_N_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n@@ -795,1 +796,1 @@\n-        failOn = {IRNode.G1_GET_AND_SET_P_WITH_BARRIER_FLAG, ANY},\n+        failOn = {IRNode.G1_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, ANY},\n@@ -798,1 +799,1 @@\n-        failOn = {IRNode.G1_GET_AND_SET_N_WITH_BARRIER_FLAG, ANY},\n+        failOn = {IRNode.G1_COMPARE_AND_SWAP_N_WITH_BARRIER_FLAG, ANY},\n@@ -808,1 +809,1 @@\n-        counts = {IRNode.G1_GET_AND_SET_P_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n+        counts = {IRNode.G1_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n@@ -811,1 +812,1 @@\n-        counts = {IRNode.G1_GET_AND_SET_N_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n+        counts = {IRNode.G1_COMPARE_AND_SWAP_N_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n@@ -814,1 +815,1 @@\n-        failOn = {IRNode.G1_GET_AND_SET_P_WITH_BARRIER_FLAG, ANY},\n+        failOn = {IRNode.G1_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, ANY},\n@@ -817,1 +818,1 @@\n-        failOn = {IRNode.G1_GET_AND_SET_N_WITH_BARRIER_FLAG, ANY},\n+        failOn = {IRNode.G1_COMPARE_AND_SWAP_N_WITH_BARRIER_FLAG, ANY},\n@@ -830,1 +831,1 @@\n-        counts = {IRNode.G1_GET_AND_SET_P_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n+        counts = {IRNode.G1_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n@@ -833,1 +834,1 @@\n-        counts = {IRNode.G1_GET_AND_SET_N_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n+        counts = {IRNode.G1_COMPARE_AND_SWAP_N_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n@@ -836,1 +837,1 @@\n-        failOn = {IRNode.G1_GET_AND_SET_P_WITH_BARRIER_FLAG, ANY},\n+        failOn = {IRNode.G1_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, ANY},\n@@ -839,1 +840,1 @@\n-        failOn = {IRNode.G1_GET_AND_SET_N_WITH_BARRIER_FLAG, ANY},\n+        failOn = {IRNode.G1_COMPARE_AND_SWAP_N_WITH_BARRIER_FLAG, ANY},\n@@ -852,1 +853,1 @@\n-        counts = {IRNode.G1_GET_AND_SET_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        counts = {IRNode.G1_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n@@ -855,1 +856,1 @@\n-        counts = {IRNode.G1_GET_AND_SET_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        counts = {IRNode.G1_COMPARE_AND_SWAP_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n","filename":"test\/hotspot\/jtreg\/compiler\/gcbarriers\/TestG1BarrierGeneration.java","additions":17,"deletions":16,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import compiler.valhalla.inlinetypes.InlineTypeIRNode;\n@@ -90,1 +91,1 @@\n-    private static final String PREFIX = \"_#\";\n+    public static final String PREFIX = \"_#\";\n@@ -153,0 +154,6 @@\n+    \/\/ Valhalla: Make sure that all Valhalla specific IR nodes are also properly initialized. Doing it here also\n+    \/\/           ensures that the Flag VM is able to pick up the correct compile phases.\n+    static {\n+        InlineTypeIRNode.forceStaticInitialization();\n+    }\n+\n@@ -384,2 +391,6 @@\n-        String regex = START + \"Allocate\\\\b\" + MID + \"allocationKlass:.*\\\\b\" + IS_REPLACED + \"\\\\s.*\" + END;\n-        macroNodes(ALLOC_OF, regex);\n+        allocateOfNodes(ALLOC_OF, IS_REPLACED);\n+    }\n+\n+    public static void allocateOfNodes(String irNodePlaceholder, String allocatee) {\n+        String regex = START + \"Allocate\\\\b\" + MID + \"allocationKlass:.*\\\\b\" + allocatee + \"\\\\s.*\" + END;\n+        macroNodes(irNodePlaceholder, regex);\n@@ -396,0 +407,4 @@\n+        allocateArrayOfNodes(ALLOC_ARRAY_OF, IS_REPLACED);\n+    }\n+\n+    public static void allocateArrayOfNodes(String irNodePlaceholder, String allocatee) {\n@@ -414,1 +429,1 @@\n-        String name_part = \"\\\\[+.(\" + partial_name_prefix + \")?\" + IS_REPLACED + \";\";\n+        String name_part = \"\\\\[+.(\" + partial_name_prefix + \")?\" + allocatee + \";\";\n@@ -416,1 +431,1 @@\n-        macroNodes(ALLOC_ARRAY_OF, regex);\n+        macroNodes(irNodePlaceholder, regex);\n@@ -481,1 +496,1 @@\n-        callOfNodes(CALL_OF, \"Call.*\");\n+        callOfNodes(CALL_OF, \"Call.*\", IS_REPLACED + \" \" );\n@@ -486,1 +501,6 @@\n-        callOfNodes(CALL_OF_METHOD, \"Call.*Java\");\n+        callOfNodes(CALL_OF_METHOD, \"Call.*Java\", IS_REPLACED + \" \");\n+    }\n+\n+    public static final String STATIC_CALL = PREFIX + \"STATIC_CALL\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(STATIC_CALL, \"CallStaticJava\");\n@@ -491,1 +511,19 @@\n-        callOfNodes(STATIC_CALL_OF_METHOD, \"CallStaticJava\");\n+        staticCallOfMethodNodes(STATIC_CALL_OF_METHOD, IS_REPLACED + \" \");\n+    }\n+\n+    public static void staticCallOfMethodNodes(String irNodePlaceholder, String calleeRegex) {\n+        callOfNodes(irNodePlaceholder, \"CallStaticJava\", calleeRegex);\n+    }\n+\n+    public static final String CALL_LEAF_NO_FP = PREFIX + \"CALL_LEAF_NO_FP\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(CALL_LEAF_NO_FP, \"CallLeafNoFP\");\n+    }\n+\n+    public static final String CALL_LEAF_NO_FP_OF_METHOD = COMPOSITE_PREFIX + \"CALL_LEAF_NO_FP_OF_METHOD\" + POSTFIX;\n+    static {\n+        callLeafNoFpOfMethodNodes(CALL_LEAF_NO_FP_OF_METHOD, IS_REPLACED);\n+    }\n+\n+    public static void callLeafNoFpOfMethodNodes(String irNodePlaceholder, String calleeRegex) {\n+        callOfNodes(irNodePlaceholder, \"CallLeafNoFP\", calleeRegex);\n@@ -588,0 +626,5 @@\n+    public static final String CMP_N = PREFIX + \"CMP_N\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(CMP_N, \"CmpN\");\n+    }\n+\n@@ -722,1 +765,1 @@\n-        callOfNodes(DYNAMIC_CALL_OF_METHOD, \"CallDynamicJava\");\n+        callOfNodes(DYNAMIC_CALL_OF_METHOD, \"CallDynamicJava\", IS_REPLACED);\n@@ -858,0 +901,5 @@\n+    public static final String INLINE_TYPE = PREFIX + \"INLINE_TYPE\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(INLINE_TYPE, \"InlineType\");\n+    }\n+\n@@ -904,1 +952,5 @@\n-        loadOfNodes(LOAD_OF_CLASS, \"Load(B|UB|S|US|I|L|F|D|P|N)\");\n+        anyLoadOfNodes(LOAD_OF_CLASS, IS_REPLACED);\n+    }\n+\n+    public static void anyLoadOfNodes(String irNodePlaceholder, String fieldHolder) {\n+        loadOfNodes(irNodePlaceholder, \"Load(B|UB|S|US|I|L|F|D|P|N)\", fieldHolder);\n@@ -914,1 +966,1 @@\n-        loadOfNodes(LOAD_B_OF_CLASS, \"LoadB\");\n+        loadOfNodes(LOAD_B_OF_CLASS, \"LoadB\", IS_REPLACED);\n@@ -924,1 +976,1 @@\n-        loadOfNodes(LOAD_D_OF_CLASS, \"LoadD\");\n+        loadOfNodes(LOAD_D_OF_CLASS, \"LoadD\", IS_REPLACED);\n@@ -934,1 +986,1 @@\n-        loadOfNodes(LOAD_F_OF_CLASS, \"LoadF\");\n+        loadOfNodes(LOAD_F_OF_CLASS, \"LoadF\", IS_REPLACED);\n@@ -944,1 +996,1 @@\n-        loadOfNodes(LOAD_I_OF_CLASS, \"LoadI\");\n+        loadOfNodes(LOAD_I_OF_CLASS, \"LoadI\", IS_REPLACED);\n@@ -969,1 +1021,1 @@\n-        loadOfNodes(LOAD_L_OF_CLASS, \"LoadL\");\n+        loadOfNodes(LOAD_L_OF_CLASS, \"LoadL\", IS_REPLACED);\n@@ -979,1 +1031,1 @@\n-        loadOfNodes(LOAD_N_OF_CLASS, \"LoadN\");\n+        loadOfNodes(LOAD_N_OF_CLASS, \"LoadN\", IS_REPLACED);\n@@ -995,1 +1047,1 @@\n-        loadOfNodes(LOAD_P_OF_CLASS, \"LoadP\");\n+        loadOfNodes(LOAD_P_OF_CLASS, \"LoadP\", IS_REPLACED);\n@@ -1005,1 +1057,1 @@\n-        loadOfNodes(LOAD_S_OF_CLASS, \"LoadS\");\n+        loadOfNodes(LOAD_S_OF_CLASS, \"LoadS\", IS_REPLACED);\n@@ -1015,1 +1067,1 @@\n-        loadOfNodes(LOAD_UB_OF_CLASS, \"LoadUB\");\n+        loadOfNodes(LOAD_UB_OF_CLASS, \"LoadUB\", IS_REPLACED);\n@@ -1025,1 +1077,1 @@\n-        loadOfNodes(LOAD_US_OF_CLASS, \"LoadUS\");\n+        loadOfNodes(LOAD_US_OF_CLASS, \"LoadUS\", IS_REPLACED);\n@@ -1929,1 +1981,1 @@\n-        storeOfNodes(STORE_B_OF_CLASS, \"StoreB\");\n+        storeOfNodes(STORE_B_OF_CLASS, \"StoreB\", IS_REPLACED);\n@@ -1939,1 +1991,1 @@\n-        storeOfNodes(STORE_C_OF_CLASS, \"StoreC\");\n+        storeOfNodes(STORE_C_OF_CLASS, \"StoreC\", IS_REPLACED);\n@@ -1949,1 +2001,1 @@\n-        storeOfNodes(STORE_D_OF_CLASS, \"StoreD\");\n+        storeOfNodes(STORE_D_OF_CLASS, \"StoreD\", IS_REPLACED);\n@@ -1959,1 +2011,1 @@\n-        storeOfNodes(STORE_F_OF_CLASS, \"StoreF\");\n+        storeOfNodes(STORE_F_OF_CLASS, \"StoreF\", IS_REPLACED);\n@@ -1969,1 +2021,1 @@\n-        storeOfNodes(STORE_I_OF_CLASS, \"StoreI\");\n+        storeOfNodes(STORE_I_OF_CLASS, \"StoreI\", IS_REPLACED);\n@@ -1979,1 +2031,1 @@\n-        storeOfNodes(STORE_L_OF_CLASS, \"StoreL\");\n+        storeOfNodes(STORE_L_OF_CLASS, \"StoreL\", IS_REPLACED);\n@@ -1989,1 +2041,1 @@\n-        storeOfNodes(STORE_N_OF_CLASS, \"StoreN\");\n+        storeOfNodes(STORE_N_OF_CLASS, \"StoreN\", IS_REPLACED);\n@@ -1994,1 +2046,5 @@\n-        storeOfNodes(STORE_OF_CLASS, \"Store(B|C|S|I|L|F|D|P|N)\");\n+        anyStoreOfNodes(STORE_OF_CLASS, IS_REPLACED);\n+    }\n+\n+    public static void anyStoreOfNodes(String irNodePlaceholder, String fieldHolder) {\n+        storeOfNodes(irNodePlaceholder, \"Store(B|C|S|I|L|F|D|P|N)\", fieldHolder);\n@@ -2010,1 +2066,1 @@\n-        storeOfNodes(STORE_P_OF_CLASS, \"StoreP\");\n+        storeOfNodes(STORE_P_OF_CLASS, \"StoreP\", IS_REPLACED);\n@@ -2100,1 +2156,2 @@\n-        beforeMatchingNameRegex(SUBTYPE_CHECK, \"SubTypeCheck\");\n+        String regex = START + \"SubTypeCheck\" + MID + END;\n+        macroNodes(SUBTYPE_CHECK, regex);\n@@ -2962,1 +3019,1 @@\n-    private static void beforeMatching(String irNodePlaceholder, String regex) {\n+    public static void beforeMatching(String irNodePlaceholder, String regex) {\n@@ -2998,2 +3055,2 @@\n-    private static void callOfNodes(String irNodePlaceholder, String callRegex) {\n-        String regex = START + callRegex + MID + IS_REPLACED + \" \" +  END;\n+    private static void callOfNodes(String irNodePlaceholder, String callRegex, String calleeRegex) {\n+        String regex = START + callRegex + MID + calleeRegex + END;\n@@ -3007,1 +3064,1 @@\n-    private static void optoOnly(String irNodePlaceholder, String regex) {\n+    public static void optoOnly(String irNodePlaceholder, String regex) {\n@@ -3092,2 +3149,2 @@\n-    private static void loadOfNodes(String irNodePlaceholder, String irNodeRegex) {\n-        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + IS_REPLACED + LOAD_STORE_SUFFIX + END;\n+    private static void loadOfNodes(String irNodePlaceholder, String irNodeRegex, String loadee) {\n+        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + loadee + LOAD_STORE_SUFFIX + END;\n@@ -3097,2 +3154,2 @@\n-    private static void storeOfNodes(String irNodePlaceholder, String irNodeRegex) {\n-        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + IS_REPLACED + LOAD_STORE_SUFFIX + END;\n+    private static void storeOfNodes(String irNodePlaceholder, String irNodeRegex, String storee) {\n+        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + storee + LOAD_STORE_SUFFIX + END;\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":95,"deletions":38,"binary":false,"changes":133,"status":"modified"},{"patch":"@@ -0,0 +1,4778 @@\n+\/*\n+ * Copyright (c) 2019, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.valhalla.inlinetypes;\n+\n+import compiler.lib.ir_framework.*;\n+import jdk.test.lib.Asserts;\n+import test.java.lang.invoke.lib.InstructionHelper;\n+\n+import java.lang.invoke.MethodHandle;\n+import java.lang.invoke.MethodHandles;\n+import java.lang.invoke.MethodType;\n+import java.lang.reflect.Method;\n+import java.util.Arrays;\n+import java.util.Objects;\n+\n+import jdk.internal.value.ValueClass;\n+import jdk.internal.vm.annotation.LooselyConsistentValue;\n+import jdk.internal.vm.annotation.NullRestricted;\n+import jdk.internal.vm.annotation.Strict;\n+\n+import static compiler.valhalla.inlinetypes.InlineTypeIRNode.ALLOC_OF_MYVALUE_KLASS;\n+import static compiler.valhalla.inlinetypes.InlineTypeIRNode.INLINE_ARRAY_NULL_GUARD;\n+import static compiler.valhalla.inlinetypes.InlineTypeIRNode.LOAD_OF_ANY_KLASS;\n+import static compiler.valhalla.inlinetypes.InlineTypeIRNode.LOAD_UNKNOWN_INLINE;\n+import static compiler.valhalla.inlinetypes.InlineTypeIRNode.STORE_OF_ANY_KLASS;\n+import static compiler.valhalla.inlinetypes.InlineTypeIRNode.STORE_UNKNOWN_INLINE;\n+import static compiler.valhalla.inlinetypes.InlineTypeIRNode.SUBSTITUTABILITY_TEST;\n+import static compiler.valhalla.inlinetypes.InlineTypes.*;\n+\n+import static compiler.lib.ir_framework.IRNode.ALLOC;\n+import static compiler.lib.ir_framework.IRNode.CLASS_CHECK_TRAP;\n+import static compiler.lib.ir_framework.IRNode.COUNTED_LOOP;\n+import static compiler.lib.ir_framework.IRNode.COUNTED_LOOP_MAIN;\n+import static compiler.lib.ir_framework.IRNode.FIELD_ACCESS;\n+import static compiler.lib.ir_framework.IRNode.LOAD_P;\n+import static compiler.lib.ir_framework.IRNode.LOOP;\n+import static compiler.lib.ir_framework.IRNode.MEMBAR;\n+import static compiler.lib.ir_framework.IRNode.NULL_CHECK_TRAP;\n+import static compiler.lib.ir_framework.IRNode.PREDICATE_TRAP;\n+import static compiler.lib.ir_framework.IRNode.UNSTABLE_IF_TRAP;\n+\n+\/*\n+ * @test\n+ * @key randomness\n+ * @summary Test inline types in LWorld.\n+ * @library \/test\/lib \/test\/jdk\/java\/lang\/invoke\/common \/\n+ * @requires (os.simpleArch == \"x64\" | os.simpleArch == \"aarch64\")\n+ * @enablePreview\n+ * @modules java.base\/jdk.internal.value\n+ *          java.base\/jdk.internal.vm.annotation\n+ * @build test.java.lang.invoke.lib.InstructionHelper\n+ * @run main\/timeout=600 compiler.valhalla.inlinetypes.TestLWorld\n+ *\/\n+\n+@ForceCompileClassInitializer\n+public class TestLWorld {\n+\n+    public static void main(String[] args) {\n+        \/\/ Make sure Test140Value is loaded but not linked\n+        Class<?> class1 = Test140Value.class;\n+        \/\/ Make sure Test141Value is linked but not initialized\n+        Class<?> class2 = Test141Value.class;\n+        class2.getDeclaredFields();\n+\n+        Scenario[] scenarios = InlineTypes.DEFAULT_SCENARIOS;\n+        scenarios[3].addFlags(\"-XX:-MonomorphicArrayCheck\", \"-XX:+UseArrayFlattening\");\n+        scenarios[4].addFlags(\"-XX:-MonomorphicArrayCheck\");\n+\n+        InlineTypes.getFramework()\n+                   .addScenarios(scenarios)\n+                   .addHelperClasses(MyValue1.class,\n+                                     MyValue2.class,\n+                                     MyValue2Inline.class,\n+                                     MyValue3.class,\n+                                     MyValue3Inline.class)\n+                   .start();\n+    }\n+\n+    static {\n+        \/\/ Make sure RuntimeException is loaded to prevent uncommon traps in IR verified tests\n+        RuntimeException tmp = new RuntimeException(\"42\");\n+    }\n+\n+    \/\/ Helper methods\n+\n+    @Strict\n+    @NullRestricted\n+    private static final MyValue1 testValue1 = MyValue1.createWithFieldsInline(rI, rL);\n+    @Strict\n+    @NullRestricted\n+    private static final MyValue2 testValue2 = MyValue2.createWithFieldsInline(rI, rD);\n+\n+    protected long hash() {\n+        return testValue1.hash();\n+    }\n+\n+    \/\/ Test passing an inline type as an Object\n+    @DontInline\n+    public Object test1_dontinline1(Object o) {\n+        return o;\n+    }\n+\n+    @DontInline\n+    public MyValue1 test1_dontinline2(Object o) {\n+        return (MyValue1)o;\n+    }\n+\n+    @ForceInline\n+    public Object test1_inline1(Object o) {\n+        return o;\n+    }\n+\n+    @ForceInline\n+    public MyValue1 test1_inline2(Object o) {\n+        return (MyValue1)o;\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public MyValue1 test1() {\n+        MyValue1 vt = testValue1;\n+        vt = (MyValue1)test1_dontinline1(vt);\n+        vt =           test1_dontinline2(vt);\n+        vt = (MyValue1)test1_inline1(vt);\n+        vt =           test1_inline2(vt);\n+        return vt;\n+    }\n+\n+    @Run(test = \"test1\")\n+    public void test1_verifier() {\n+        Asserts.assertEQ(test1().hash(), hash());\n+    }\n+\n+    \/\/ Test storing\/loading inline types to\/from Object and inline type fields\n+    Object objectField1 = null;\n+    Object objectField2 = null;\n+    Object objectField3 = null;\n+    Object objectField4 = null;\n+    Object objectField5 = null;\n+    Object objectField6 = null;\n+\n+    @Strict\n+    @NullRestricted\n+    MyValue1 valueField1 = testValue1;\n+    @Strict\n+    @NullRestricted\n+    MyValue1 valueField2 = testValue1;\n+    MyValue1 valueField3 = testValue1;\n+    @Strict\n+    @NullRestricted\n+    MyValue1 valueField4 = MyValue1.DEFAULT;\n+    MyValue1 valueField5;\n+\n+    static MyValue1 staticValueField1 = testValue1;\n+    @Strict\n+    @NullRestricted\n+    static MyValue1 staticValueField2 = testValue1;\n+    @Strict\n+    @NullRestricted\n+    static MyValue1 staticValueField3 = MyValue1.DEFAULT;\n+    static MyValue1 staticValueField4;\n+\n+    @DontInline\n+    public Object readValueField5() {\n+        return (Object)valueField5;\n+    }\n+\n+    @DontInline\n+    public Object readStaticValueField4() {\n+        return (Object)staticValueField4;\n+    }\n+\n+    @Test\n+    public long test2(MyValue1 vt1, Object vt2) {\n+        objectField1 = vt1;\n+        objectField2 = (MyValue1)vt2;\n+        objectField3 = testValue1;\n+        objectField4 = MyValue1.createWithFieldsDontInline(rI, rL);\n+        objectField5 = valueField1;\n+        objectField6 = valueField3;\n+        valueField1 = (MyValue1)objectField1;\n+        valueField2 = (MyValue1)vt2;\n+        valueField3 = (MyValue1)vt2;\n+        staticValueField1 = (MyValue1)objectField1;\n+        staticValueField2 = (MyValue1)vt1;\n+        \/\/ Don't inline these methods because reading NULL will trigger a deoptimization\n+        if (readValueField5() != null || readStaticValueField4() != null) {\n+            throw new RuntimeException(\"Should be null\");\n+        }\n+        return ((MyValue1)objectField1).hash() + ((MyValue1)objectField2).hash() +\n+               ((MyValue1)objectField3).hash() + ((MyValue1)objectField4).hash() +\n+               ((MyValue1)objectField5).hash() + ((MyValue1)objectField6).hash() +\n+                valueField1.hash() + valueField2.hash() + valueField3.hash() + valueField4.hashPrimitive() +\n+                staticValueField1.hash() + staticValueField2.hash() + staticValueField3.hashPrimitive();\n+    }\n+\n+    @Run(test = \"test2\")\n+    public void test2_verifier() {\n+        MyValue1 vt = testValue1;\n+        MyValue1 def = MyValue1.createDefaultDontInline();\n+        long result = test2(vt, vt);\n+        Asserts.assertEQ(result, 11*vt.hash() + 2*def.hashPrimitive());\n+    }\n+\n+    \/\/ Test merging inline types and objects\n+    @Test\n+    public Object test3(int state) {\n+        Object res = null;\n+        if (state == 0) {\n+            res = new NonValueClass(rI);\n+        } else if (state == 1) {\n+            res = MyValue1.createWithFieldsInline(rI, rL);\n+        } else if (state == 2) {\n+            res = MyValue1.createWithFieldsDontInline(rI, rL);\n+        } else if (state == 3) {\n+            res = (MyValue1)objectField1;\n+        } else if (state == 4) {\n+            res = valueField1;\n+        } else if (state == 5) {\n+            res = null;\n+        } else if (state == 6) {\n+            res = MyValue2.createWithFieldsInline(rI, rD);\n+        } else if (state == 7) {\n+            res = testValue2;\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test3\")\n+    public void test3_verifier() {\n+        objectField1 = valueField1;\n+        Object result = null;\n+        result = test3(0);\n+        Asserts.assertEQ(((NonValueClass)result).x, rI);\n+        result = test3(1);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test3(2);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test3(3);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test3(4);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test3(5);\n+        Asserts.assertEQ(result, null);\n+        result = test3(6);\n+        Asserts.assertEQ(((MyValue2)result).hash(), testValue2.hash());\n+        result = test3(7);\n+        Asserts.assertEQ(((MyValue2)result).hash(), testValue2.hash());\n+    }\n+\n+    \/\/ Test merging inline types and objects in loops\n+    @Test\n+    public Object test4(int iters) {\n+        Object res = new NonValueClass(rI);\n+        for (int i = 0; i < iters; ++i) {\n+            if (res instanceof NonValueClass) {\n+                res = MyValue1.createWithFieldsInline(rI, rL);\n+            } else {\n+                res = MyValue1.createWithFieldsInline(((MyValue1)res).x + 1, rL);\n+            }\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test4\")\n+    public void test4_verifier() {\n+        NonValueClass result1 = (NonValueClass)test4(0);\n+        Asserts.assertEQ(result1.x, rI);\n+        int iters = (Math.abs(rI) % 10) + 1;\n+        MyValue1 result2 = (MyValue1)test4(iters);\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI + iters - 1, rL);\n+        Asserts.assertEQ(result2.hash(), vt.hash());\n+    }\n+\n+    \/\/ Test inline types in object variables that are live at safepoint\n+    @Test\n+    @IR(failOn = {ALLOC_OF_MYVALUE_KLASS, STORE_OF_ANY_KLASS, LOOP})\n+    public long test5(MyValue1 arg, boolean deopt, Method m) {\n+        Object vt1 = MyValue1.createWithFieldsInline(rI, rL);\n+        Object vt2 = MyValue1.createWithFieldsDontInline(rI, rL);\n+        Object vt3 = arg;\n+        Object vt4 = valueField1;\n+        if (deopt) {\n+            \/\/ uncommon trap\n+            TestFramework.deoptimize(m);\n+        }\n+        return ((MyValue1)vt1).hash() + ((MyValue1)vt2).hash() +\n+               ((MyValue1)vt3).hash() + ((MyValue1)vt4).hash();\n+    }\n+\n+    @Run(test = \"test5\")\n+    public void test5_verifier(RunInfo info) {\n+        long result = test5(valueField1, !info.isWarmUp(), info.getTest());\n+        Asserts.assertEQ(result, 4*hash());\n+    }\n+\n+    \/\/ Test comparing inline types with objects\n+    @Test\n+    public boolean test6(Object arg) {\n+        Object vt = MyValue1.createWithFieldsInline(rI, rL);\n+        if (vt == arg || vt == (Object)valueField1 || vt == objectField1 || vt == null ||\n+            arg == vt || (Object)valueField1 == vt || objectField1 == vt || null == vt) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    @Run(test = \"test6\")\n+    public void test6_verifier() {\n+        boolean result = test6(null);\n+        Asserts.assertFalse(result);\n+    }\n+\n+    \/\/ merge of inline type and non-inline type\n+    @Test\n+    public Object test7(boolean flag) {\n+        Object res = null;\n+        if (flag) {\n+            res = valueField1;\n+        } else {\n+            res = objectField1;\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test7\")\n+    public void test7_verifier() {\n+        Asserts.assertEQ(test7(true), valueField1);\n+        Asserts.assertEQ(test7(false), objectField1);\n+    }\n+\n+    @Test\n+    public Object test8(boolean flag) {\n+        Object res = null;\n+        if (flag) {\n+            res = objectField1;\n+        } else {\n+            res = valueField1;\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test8\")\n+    public void test8_verifier() {\n+        Asserts.assertEQ(test8(true), objectField1);\n+        Asserts.assertEQ(test8(false), valueField1);\n+    }\n+\n+    \/\/ merge of inline types in a loop, stored in an object local\n+    @Test\n+    public Object test9() {\n+        Object o = valueField1;\n+        for (int i = 1; i < 100; i *= 2) {\n+            MyValue1 v = (MyValue1)o;\n+            o = MyValue1.setX(v, v.x + 1);\n+        }\n+        return o;\n+    }\n+\n+    @Run(test = \"test9\")\n+    public void test9_verifier() {\n+        Asserts.assertEQ(test9(), MyValue1.setX(valueField1, valueField1.x + 7));\n+    }\n+\n+    \/\/ merge of inline types in an object local\n+    @ForceInline\n+    public Object test10_helper() {\n+        return valueField1;\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public void test10(boolean flag) {\n+        Object o = null;\n+        if (flag) {\n+            o = valueField1;\n+        } else {\n+            o = test10_helper();\n+        }\n+        valueField1 = (MyValue1)o;\n+    }\n+\n+    @Run(test = \"test10\")\n+    public void test10_verifier() {\n+        test10(true);\n+        test10(false);\n+    }\n+\n+    \/\/ Interface tests\n+\n+    @DontInline\n+    public MyInterface test11_dontinline1(MyInterface o) {\n+        return o;\n+    }\n+\n+    @DontInline\n+    public MyValue1 test11_dontinline2(MyInterface o) {\n+        return (MyValue1)o;\n+    }\n+\n+    @ForceInline\n+    public MyInterface test11_inline1(MyInterface o) {\n+        return o;\n+    }\n+\n+    @ForceInline\n+    public MyValue1 test11_inline2(MyInterface o) {\n+        return (MyValue1)o;\n+    }\n+\n+    @Test\n+    public MyValue1 test11() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        vt = (MyValue1)test11_dontinline1(vt);\n+        vt =           test11_dontinline2(vt);\n+        vt = (MyValue1)test11_inline1(vt);\n+        vt =           test11_inline2(vt);\n+        return vt;\n+    }\n+\n+    @Run(test = \"test11\")\n+    public void test11_verifier() {\n+        Asserts.assertEQ(test11().hash(), hash());\n+    }\n+\n+    \/\/ Test storing\/loading inline types to\/from interface and inline type fields\n+    MyInterface interfaceField1 = null;\n+    MyInterface interfaceField2 = null;\n+    MyInterface interfaceField3 = null;\n+    MyInterface interfaceField4 = null;\n+    MyInterface interfaceField5 = null;\n+    MyInterface interfaceField6 = null;\n+\n+    @DontInline\n+    public MyInterface readValueField5AsInterface() {\n+        return (MyInterface)valueField5;\n+    }\n+\n+    @DontInline\n+    public MyInterface readStaticValueField4AsInterface() {\n+        return (MyInterface)staticValueField4;\n+    }\n+\n+    @Test\n+    public long test12(MyValue1 vt1, MyInterface vt2) {\n+        interfaceField1 = vt1;\n+        interfaceField2 = (MyValue1)vt2;\n+        interfaceField3 = MyValue1.createWithFieldsInline(rI, rL);\n+        interfaceField4 = MyValue1.createWithFieldsDontInline(rI, rL);\n+        interfaceField5 = valueField1;\n+        interfaceField6 = valueField3;\n+        valueField1 = (MyValue1)interfaceField1;\n+        valueField2 = (MyValue1)vt2;\n+        valueField3 = (MyValue1)vt2;\n+        staticValueField1 = (MyValue1)interfaceField1;\n+        staticValueField2 = (MyValue1)vt1;\n+        \/\/ Don't inline these methods because reading NULL will trigger a deoptimization\n+        if (readValueField5AsInterface() != null || readStaticValueField4AsInterface() != null) {\n+            throw new RuntimeException(\"Should be null\");\n+        }\n+        return ((MyValue1)interfaceField1).hash() + ((MyValue1)interfaceField2).hash() +\n+               ((MyValue1)interfaceField3).hash() + ((MyValue1)interfaceField4).hash() +\n+               ((MyValue1)interfaceField5).hash() + ((MyValue1)interfaceField6).hash() +\n+                valueField1.hash() + valueField2.hash() + valueField3.hash() + valueField4.hashPrimitive() +\n+                staticValueField1.hash() + staticValueField2.hash() + staticValueField3.hashPrimitive();\n+    }\n+\n+    @Run(test = \"test12\")\n+    public void test12_verifier() {\n+        MyValue1 vt = testValue1;\n+        MyValue1 def = MyValue1.createDefaultDontInline();\n+        long result = test12(vt, vt);\n+        Asserts.assertEQ(result, 11*vt.hash() + 2*def.hashPrimitive());\n+    }\n+\n+    class MyObject1 implements MyInterface {\n+        public int x;\n+\n+        public MyObject1(int x) {\n+            this.x = x;\n+        }\n+\n+        @ForceInline\n+        public long hash() {\n+            return x;\n+        }\n+    }\n+\n+    \/\/ Test merging inline types and interfaces\n+    @Test\n+    public MyInterface test13(int state) {\n+        MyInterface res = null;\n+        if (state == 0) {\n+            res = new MyObject1(rI);\n+        } else if (state == 1) {\n+            res = MyValue1.createWithFieldsInline(rI, rL);\n+        } else if (state == 2) {\n+            res = MyValue1.createWithFieldsDontInline(rI, rL);\n+        } else if (state == 3) {\n+            res = (MyValue1)objectField1;\n+        } else if (state == 4) {\n+            res = valueField1;\n+        } else if (state == 5) {\n+            res = null;\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test13\")\n+    public void test13_verifier() {\n+        objectField1 = valueField1;\n+        MyInterface result = null;\n+        result = test13(0);\n+        Asserts.assertEQ(((MyObject1)result).x, rI);\n+        result = test13(1);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test13(2);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test13(3);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test13(4);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test13(5);\n+        Asserts.assertEQ(result, null);\n+    }\n+\n+    \/\/ Test merging inline types and interfaces in loops\n+    @Test\n+    public MyInterface test14(int iters) {\n+        MyInterface res = new MyObject1(rI);\n+        for (int i = 0; i < iters; ++i) {\n+            if (res instanceof MyObject1) {\n+                res = MyValue1.createWithFieldsInline(rI, rL);\n+            } else {\n+                res = MyValue1.createWithFieldsInline(((MyValue1)res).x + 1, rL);\n+            }\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test14\")\n+    public void test14_verifier() {\n+        MyObject1 result1 = (MyObject1)test14(0);\n+        Asserts.assertEQ(result1.x, rI);\n+        int iters = (Math.abs(rI) % 10) + 1;\n+        MyValue1 result2 = (MyValue1)test14(iters);\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI + iters - 1, rL);\n+        Asserts.assertEQ(result2.hash(), vt.hash());\n+    }\n+\n+    \/\/ Test inline types in interface variables that are live at safepoint\n+    @Test\n+    @IR(failOn = {ALLOC_OF_MYVALUE_KLASS, STORE_OF_ANY_KLASS, LOOP})\n+    public long test15(MyValue1 arg, boolean deopt, Method m) {\n+        MyInterface vt1 = MyValue1.createWithFieldsInline(rI, rL);\n+        MyInterface vt2 = MyValue1.createWithFieldsDontInline(rI, rL);\n+        MyInterface vt3 = arg;\n+        MyInterface vt4 = valueField1;\n+        return ((MyValue1)vt1).hash() + ((MyValue1)vt2).hash() +\n+               ((MyValue1)vt3).hash() + ((MyValue1)vt4).hash();\n+    }\n+\n+    @Run(test = \"test15\")\n+    public void test15_verifier(RunInfo info) {\n+        long result = test15(valueField1, !info.isWarmUp(), info.getTest());\n+        Asserts.assertEQ(result, 4*hash());\n+    }\n+\n+    \/\/ Test comparing inline types with interfaces\n+    @Test\n+    public boolean test16(Object arg) {\n+        MyInterface vt = MyValue1.createWithFieldsInline(rI, rL);\n+        if (vt == arg || vt == (MyInterface)valueField1 || vt == interfaceField1 || vt == null ||\n+            arg == vt || (MyInterface)valueField1 == vt || interfaceField1 == vt || null == vt) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    @Run(test = \"test16\")\n+    public void test16_verifier() {\n+        boolean result = test16(null);\n+        Asserts.assertFalse(result);\n+    }\n+\n+    \/\/ Test subtype check when casting to inline type\n+    @Test\n+    @IR(failOn = {ALLOC_OF_MYVALUE_KLASS})\n+    public MyValue1 test17(MyValue1 vt, Object obj) {\n+        try {\n+            vt = (MyValue1)obj;\n+            throw new RuntimeException(\"ClassCastException expected\");\n+        } catch (ClassCastException e) {\n+            \/\/ Expected\n+        }\n+        return vt;\n+    }\n+\n+    @Run(test = \"test17\")\n+    public void test17_verifier() {\n+        MyValue1 vt = testValue1;\n+        MyValue1 result = test17(vt, new NonValueClass(rI));\n+        Asserts.assertEquals(result.hash(), vt.hash());\n+    }\n+\n+    @Test\n+    public MyValue1 test18(MyValue1 vt) {\n+        Object obj = vt;\n+        vt = (MyValue1)obj;\n+        return vt;\n+    }\n+\n+    @Run(test = \"test18\")\n+    public void test18_verifier() {\n+        MyValue1 vt = testValue1;\n+        MyValue1 result = test18(vt);\n+        Asserts.assertEquals(result.hash(), vt.hash());\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public void test19(MyValue1 vt) {\n+        if (vt == null) {\n+            return;\n+        }\n+        Object obj = vt;\n+        try {\n+            MyValue2 vt2 = (MyValue2)obj;\n+            throw new RuntimeException(\"ClassCastException expected\");\n+        } catch (ClassCastException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    @Run(test = \"test19\")\n+    public void test19_verifier() {\n+        test19(valueField1);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public void test20(MyValue1 vt) {\n+        if (vt == null) {\n+            return;\n+        }\n+        Object obj = vt;\n+        try {\n+            NonValueClass i = (NonValueClass)obj;\n+            throw new RuntimeException(\"ClassCastException expected\");\n+        } catch (ClassCastException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    @Run(test = \"test20\")\n+    public void test20_verifier() {\n+        test20(valueField1);\n+    }\n+\n+    \/\/ Array tests\n+\n+    private static final MyValue1[] testValue1Array = (MyValue1[])ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 3, MyValue1.DEFAULT);\n+    static {\n+        for (int i = 0; i < 3; ++i) {\n+            testValue1Array[i] = testValue1;\n+        }\n+    }\n+\n+    private static final MyValue1[][] testValue1Array2 = new MyValue1[][] {testValue1Array,\n+                                                                           testValue1Array,\n+                                                                           testValue1Array};\n+\n+    private static final MyValue2[] testValue2Array = (MyValue2[])ValueClass.newNullRestrictedNonAtomicArray(MyValue2.class, 3, MyValue2.DEFAULT);\n+    static {\n+        for (int i = 0; i < 3; ++i) {\n+            testValue2Array[i] = testValue2;\n+        }\n+    }\n+\n+    private static final NonValueClass[] testNonValueArray = new NonValueClass[42];\n+\n+    \/\/ Test load from (flattened) inline type array disguised as object array\n+    @Test\n+    public Object test21(Object[] oa, int index) {\n+        return oa[index];\n+    }\n+\n+    @Run(test = \"test21\")\n+    public void test21_verifier() {\n+        MyValue1 result = (MyValue1)test21(testValue1Array, Math.abs(rI) % 3);\n+        Asserts.assertEQ(result.hash(), hash());\n+    }\n+\n+    \/\/ Test load from (flattened) inline type array disguised as interface array\n+    @Test\n+    public Object test22Interface(MyInterface[] ia, int index) {\n+        return ia[index];\n+    }\n+\n+    @Run(test = \"test22Interface\")\n+    public void test22Interface_verifier() {\n+        MyValue1 result = (MyValue1)test22Interface(testValue1Array, Math.abs(rI) % 3);\n+        Asserts.assertEQ(result.hash(), hash());\n+    }\n+\n+    \/\/ Test load from (flattened) inline type array disguised as abstract array\n+    @Test\n+    public Object test22Abstract(MyAbstract[] ia, int index) {\n+        return ia[index];\n+    }\n+\n+    @Run(test = \"test22Abstract\")\n+    public void test22Abstract_verifier() {\n+        MyValue1 result = (MyValue1)test22Abstract(testValue1Array, Math.abs(rI) % 3);\n+        Asserts.assertEQ(result.hash(), hash());\n+    }\n+\n+    \/\/ Test inline store to (flattened) inline type array disguised as object array\n+    @ForceInline\n+    public void test23_inline(Object[] oa, Object o, int index) {\n+        oa[index] = o;\n+    }\n+\n+    @Test\n+    public void test23(Object[] oa, MyValue1 vt, int index) {\n+        test23_inline(oa, vt, index);\n+    }\n+\n+    @Run(test = \"test23\")\n+    public void test23_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI + 1, rL + 1);\n+        test23(testValue1Array, vt, index);\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt.hash());\n+        testValue1Array[index] = testValue1;\n+        try {\n+            test23(testValue2Array, vt, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue2Array[index].hash(), testValue2.hash());\n+    }\n+\n+    @ForceInline\n+    public void test24_inline(Object[] oa, Object o, int index) {\n+        oa[index] = o;\n+    }\n+\n+    @Test\n+    public void test24(Object[] oa, MyValue1 vt, int index) {\n+        test24_inline(oa, vt, index);\n+    }\n+\n+    @Run(test = \"test24\")\n+    public void test24_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test24(testNonValueArray, testValue1, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    @ForceInline\n+    public void test25_inline(Object[] oa, Object o, int index) {\n+        oa[index] = o;\n+    }\n+\n+    @Test\n+    public void test25(Object[] oa, MyValue1 vt, int index) {\n+        test25_inline(oa, vt, index);\n+    }\n+\n+    @Run(test = \"test25\")\n+    public void test25_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test25(null, testValue1, index);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test inline store to (flattened) inline type array disguised as interface array\n+    @ForceInline\n+    public void test26Interface_inline(MyInterface[] ia, MyInterface i, int index) {\n+        ia[index] = i;\n+    }\n+\n+    @Test\n+    public void test26Interface(MyInterface[] ia, MyValue1 vt, int index) {\n+      test26Interface_inline(ia, vt, index);\n+    }\n+\n+    @Run(test = \"test26Interface\")\n+    public void test26Interface_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI + 1, rL + 1);\n+        test26Interface(testValue1Array, vt, index);\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt.hash());\n+        testValue1Array[index] = testValue1;\n+        try {\n+            test26Interface(testValue2Array, vt, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue2Array[index].hash(), testValue2.hash());\n+    }\n+\n+    @ForceInline\n+    public void test27Interface_inline(MyInterface[] ia, MyInterface i, int index) {\n+        ia[index] = i;\n+    }\n+\n+    @Test\n+    public void test27Interface(MyInterface[] ia, MyValue1 vt, int index) {\n+        test27Interface_inline(ia, vt, index);\n+    }\n+\n+    @Run(test = \"test27Interface\")\n+    public void test27Interface_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test27Interface(null, testValue1, index);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test inline store to (flattened) inline type array disguised as abstract array\n+    @ForceInline\n+    public void test26Abstract_inline(MyAbstract[] ia, MyAbstract i, int index) {\n+        ia[index] = i;\n+    }\n+\n+    @Test\n+    public void test26Abstract(MyAbstract[] ia, MyValue1 vt, int index) {\n+      test26Abstract_inline(ia, vt, index);\n+    }\n+\n+    @Run(test = \"test26Abstract\")\n+    public void test26Abstract_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI + 1, rL + 1);\n+        test26Abstract(testValue1Array, vt, index);\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt.hash());\n+        testValue1Array[index] = testValue1;\n+        try {\n+            test26Abstract(testValue2Array, vt, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue2Array[index].hash(), testValue2.hash());\n+    }\n+\n+    @ForceInline\n+    public void test27Abstract_inline(MyAbstract[] ia, MyAbstract i, int index) {\n+        ia[index] = i;\n+    }\n+\n+    @Test\n+    public void test27Abstract(MyAbstract[] ia, MyValue1 vt, int index) {\n+        test27Abstract_inline(ia, vt, index);\n+    }\n+\n+    @Run(test = \"test27Abstract\")\n+    public void test27Abstract_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test27Abstract(null, testValue1, index);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test object store to (flattened) inline type array disguised as object array\n+    @ForceInline\n+    public void test28_inline(Object[] oa, Object o, int index) {\n+        oa[index] = o;\n+    }\n+\n+    @Test\n+    public void test28(Object[] oa, Object o, int index) {\n+        test28_inline(oa, o, index);\n+    }\n+\n+    @Run(test = \"test28\")\n+    public void test28_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        MyValue1 vt1 = MyValue1.createWithFieldsInline(rI + 1, rL + 1);\n+        test28(testValue1Array, vt1, index);\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt1.hash());\n+        try {\n+            test28(testValue1Array, testValue2, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt1.hash());\n+        testValue1Array[index] = testValue1;\n+    }\n+\n+    @ForceInline\n+    public void test29_inline(Object[] oa, Object o, int index) {\n+        oa[index] = o;\n+    }\n+\n+    @Test\n+    public void test29(Object[] oa, Object o, int index) {\n+        test29_inline(oa, o, index);\n+    }\n+\n+    @Run(test = \"test29\")\n+    public void test29_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test29(testValue2Array, testValue1, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue2Array[index].hash(), testValue2.hash());\n+    }\n+\n+    @ForceInline\n+    public void test30_inline(Object[] oa, Object o, int index) {\n+        oa[index] = o;\n+    }\n+\n+    @Test\n+    public void test30(Object[] oa, Object o, int index) {\n+        test30_inline(oa, o, index);\n+    }\n+\n+    @Run(test = \"test30\")\n+    public void test30_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test30(testNonValueArray, testValue1, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test inline store to (flattened) inline type array disguised as interface array\n+    @ForceInline\n+    public void test31Interface_inline(MyInterface[] ia, MyInterface i, int index) {\n+        ia[index] = i;\n+    }\n+\n+    @Test\n+    public void test31Interface(MyInterface[] ia, MyInterface i, int index) {\n+        test31Interface_inline(ia, i, index);\n+    }\n+\n+    @Run(test = \"test31Interface\")\n+    public void test31Interface_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        MyValue1 vt1 = MyValue1.createWithFieldsInline(rI + 1, rL + 1);\n+        test31Interface(testValue1Array, vt1, index);\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt1.hash());\n+        try {\n+            test31Interface(testValue1Array, testValue2, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt1.hash());\n+        testValue1Array[index] = testValue1;\n+    }\n+\n+    @ForceInline\n+    public void test32Interface_inline(MyInterface[] ia, MyInterface i, int index) {\n+        ia[index] = i;\n+    }\n+\n+    @Test\n+    public void test32Interface(MyInterface[] ia, MyInterface i, int index) {\n+        test32Interface_inline(ia, i, index);\n+    }\n+\n+    @Run(test = \"test32Interface\")\n+    public void test32Interface_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test32Interface(testValue2Array, testValue1, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test inline store to (flattened) inline type array disguised as abstract array\n+    @ForceInline\n+    public void test31Abstract_inline(MyAbstract[] ia, MyAbstract i, int index) {\n+        ia[index] = i;\n+    }\n+\n+    @Test\n+    public void test31Abstract(MyAbstract[] ia, MyAbstract i, int index) {\n+        test31Abstract_inline(ia, i, index);\n+    }\n+\n+    @Run(test = \"test31Abstract\")\n+    public void test31Abstract_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        MyValue1 vt1 = MyValue1.createWithFieldsInline(rI + 1, rL + 1);\n+        test31Abstract(testValue1Array, vt1, index);\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt1.hash());\n+        try {\n+            test31Abstract(testValue1Array, testValue2, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt1.hash());\n+        testValue1Array[index] = testValue1;\n+    }\n+\n+    @ForceInline\n+    public void test32Abstract_inline(MyAbstract[] ia, MyAbstract i, int index) {\n+        ia[index] = i;\n+    }\n+\n+    @Test\n+    public void test32Abstract(MyAbstract[] ia, MyAbstract i, int index) {\n+        test32Abstract_inline(ia, i, index);\n+    }\n+\n+    @Run(test = \"test32Abstract\")\n+    public void test32Abstract_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test32Abstract(testValue2Array, testValue1, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test writing null to a (flattened) inline type array disguised as object array\n+    @ForceInline\n+    public void test33_inline(Object[] oa, Object o, int index) {\n+        oa[index] = o;\n+    }\n+\n+    @Test\n+    public void test33(Object[] oa, Object o, int index) {\n+        test33_inline(oa, o, index);\n+    }\n+\n+    @Run(test = \"test33\")\n+    public void test33_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test33(testValue1Array, null, index);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue1Array[index].hash(), hash());\n+    }\n+\n+    \/\/ Test writing constant null to a (flattened) inline type array disguised as object array\n+\n+    @ForceInline\n+    public void test34_inline(Object[] oa, Object o, int index) {\n+        oa[index] = o;\n+    }\n+\n+    @Test\n+    public void test34(Object[] oa, int index) {\n+        test34_inline(oa, null, index);\n+    }\n+\n+    @Run(test = \"test34\")\n+    public void test34_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test34(testValue1Array, index);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue1Array[index].hash(), hash());\n+    }\n+\n+    \/\/ Test writing constant null to a (flattened) inline type array\n+\n+    private static final MethodHandle setArrayElementNull = InstructionHelper.buildMethodHandle(MethodHandles.lookup(),\n+        \"setArrayElementNull\",\n+        MethodType.methodType(void.class, TestLWorld.class, MyValue1[].class, int.class),\n+        CODE -> {\n+            CODE.\n+            aload(1).\n+            iload(2).\n+            aconst_null().\n+            aastore().\n+            return_();\n+        });\n+\n+    @Test\n+    public void test35(MyValue1[] va, int index) throws Throwable {\n+        setArrayElementNull.invoke(this, va, index);\n+    }\n+\n+    @Run(test = \"test35\")\n+    @Warmup(10000)\n+    public void test35_verifier() throws Throwable {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test35(testValue1Array, index);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue1Array[index].hash(), hash());\n+    }\n+\n+    \/\/ Test writing an inline type to a null inline type array\n+    @Test\n+    public void test36(MyValue1[] va, MyValue1 vt, int index) {\n+        va[index] = vt;\n+    }\n+\n+    @Run(test = \"test36\")\n+    public void test36_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test36(null, testValue1Array[index], index);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test incremental inlining\n+    @ForceInline\n+    public void test37_inline(Object[] oa, Object o, int index) {\n+        oa[index] = o;\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public void test37(MyValue1[] va, Object o, int index) {\n+        test37_inline(va, o, index);\n+    }\n+\n+    @Run(test = \"test37\")\n+    public void test37_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        MyValue1 vt1 = MyValue1.createWithFieldsInline(rI + 1, rL + 1);\n+        test37(testValue1Array, vt1, index);\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt1.hash());\n+        try {\n+            test37(testValue1Array, testValue2, index);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue1Array[index].hash(), vt1.hash());\n+        testValue1Array[index] = testValue1;\n+    }\n+\n+    \/\/ Test merging of inline type arrays\n+\n+    @ForceInline\n+    public Object[] test38_inline() {\n+        return (MyValue1[])ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 42, MyValue1.DEFAULT);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_OF_MYVALUE_KLASS})\n+    public Object[] test38(Object[] oa, Object o, int i1, int i2, int num) {\n+        Object[] result = null;\n+        switch (num) {\n+        case 0:\n+            result = test38_inline();\n+            break;\n+        case 1:\n+            result = oa;\n+            break;\n+        case 2:\n+            result = testValue1Array;\n+            break;\n+        case 3:\n+            result = testValue2Array;\n+            break;\n+        case 4:\n+            result = testNonValueArray;\n+            break;\n+        case 5:\n+            result = null;\n+            break;\n+        case 6:\n+            result = testValue1Array2;\n+            break;\n+        }\n+        result[i1] = result[i2];\n+        result[i2] = o;\n+        return result;\n+    }\n+\n+    @Run(test = \"test38\")\n+    public void test38_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        MyValue1[] va = (MyValue1[])ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 42, MyValue1.DEFAULT);\n+        Object[] result = test38(null, testValue1, index, index, 0);\n+        Asserts.assertEQ(((MyValue1)result[index]).hash(), testValue1.hash());\n+        result = test38(testValue1Array, testValue1, index, index, 1);\n+        Asserts.assertEQ(((MyValue1)result[index]).hash(), testValue1.hash());\n+        result = test38(null, testValue1, index, index, 2);\n+        Asserts.assertEQ(((MyValue1)result[index]).hash(), testValue1.hash());\n+        result = test38(null, testValue2, index, index, 3);\n+        Asserts.assertEQ(((MyValue2)result[index]).hash(), testValue2.hash());\n+        try {\n+            result = test38(null, null, index, index, 3);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+        result = test38(null, null, index, index, 4);\n+        try {\n+            result = test38(null, testValue1, index, index, 4);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        try {\n+            result = test38(null, testValue1, index, index, 5);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+        result = test38(null, testValue1Array, index, index, 6);\n+        Asserts.assertEQ(((MyValue1[][])result)[index][index].hash(), testValue1.hash());\n+    }\n+\n+    @ForceInline\n+    public Object test39_inline() {\n+        return (MyValue1[])ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 42, MyValue1.DEFAULT);\n+    }\n+\n+    \/\/ Same as above but merging into Object instead of Object[]\n+    @Test\n+    public Object test39(Object oa, Object o, int i1, int i2, int num) {\n+        Object result = null;\n+        switch (num) {\n+        case 0:\n+            result = test39_inline();\n+            break;\n+        case 1:\n+            result = oa;\n+            break;\n+        case 2:\n+            result = testValue1Array;\n+            break;\n+        case 3:\n+            result = testValue2Array;\n+            break;\n+        case 4:\n+            result = testNonValueArray;\n+            break;\n+        case 5:\n+            result = null;\n+            break;\n+        case 6:\n+            result = testValue1;\n+            break;\n+        case 7:\n+            result = testValue2;\n+            break;\n+        case 8:\n+            result = MyValue1.createWithFieldsInline(rI, rL);\n+            break;\n+        case 9:\n+            result = new NonValueClass(42);\n+            break;\n+        case 10:\n+            result = testValue1Array2;\n+            break;\n+        }\n+        if (result instanceof Object[]) {\n+            ((Object[])result)[i1] = ((Object[])result)[i2];\n+            ((Object[])result)[i2] = o;\n+        }\n+        return result;\n+    }\n+\n+    @Run(test = \"test39\")\n+    public void test39_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        MyValue1[] va = (MyValue1[])ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 42, MyValue1.DEFAULT);\n+        Object result = test39(null, testValue1, index, index, 0);\n+        Asserts.assertEQ(((MyValue1[])result)[index].hash(), testValue1.hash());\n+        result = test39(testValue1Array, testValue1, index, index, 1);\n+        Asserts.assertEQ(((MyValue1[])result)[index].hash(), testValue1.hash());\n+        result = test39(null, testValue1, index, index, 2);\n+        Asserts.assertEQ(((MyValue1[])result)[index].hash(), testValue1.hash());\n+        result = test39(null, testValue2, index, index, 3);\n+        Asserts.assertEQ(((MyValue2[])result)[index].hash(), testValue2.hash());\n+        try {\n+            result = test39(null, null, index, index, 3);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+        result = test39(null, null, index, index, 4);\n+        try {\n+            result = test39(null, testValue1, index, index, 4);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        result = test39(null, testValue1, index, index, 5);\n+        Asserts.assertEQ(result, null);\n+        result = test39(null, testValue1, index, index, 6);\n+        Asserts.assertEQ(((MyValue1)result).hash(), testValue1.hash());\n+        result = test39(null, testValue1, index, index, 7);\n+        Asserts.assertEQ(((MyValue2)result).hash(), testValue2.hash());\n+        result = test39(null, testValue1, index, index, 8);\n+        Asserts.assertEQ(((MyValue1)result).hash(), testValue1.hash());\n+        result = test39(null, testValue1, index, index, 9);\n+        Asserts.assertEQ(((NonValueClass)result).x, 42);\n+        result = test39(null, testValue1Array, index, index, 10);\n+        Asserts.assertEQ(((MyValue1[][])result)[index][index].hash(), testValue1.hash());\n+    }\n+\n+    \/\/ Test instanceof with inline types and arrays\n+    @Test\n+    @IR(applyIf = {\"InlineTypePassFieldsAsArgs\", \"true\"},\n+        failOn = {ALLOC})\n+    public long test40(Object o, int index) {\n+        if (o instanceof MyValue1) {\n+          return ((MyValue1)o).hashInterpreted();\n+        } else if (o instanceof MyValue1[]) {\n+          return ((MyValue1[])o)[index].hashInterpreted();\n+        } else if (o instanceof MyValue2) {\n+          return ((MyValue2)o).hash();\n+        } else if (o instanceof MyValue2[]) {\n+          return ((MyValue2[])o)[index].hash();\n+        } else if (o instanceof MyValue1[][]) {\n+          return ((MyValue1[][])o)[index][index].hash();\n+        } else if (o instanceof Long) {\n+          return (long)o;\n+        }\n+        return 0;\n+    }\n+\n+    @Run(test = \"test40\")\n+    public void test40_verifier() {\n+        int index = Math.abs(rI) % 3;\n+        long result = test40(testValue1, 0);\n+        Asserts.assertEQ(result, testValue1.hashInterpreted());\n+        result = test40(testValue1Array, index);\n+        Asserts.assertEQ(result, testValue1.hashInterpreted());\n+        result = test40(testValue2, index);\n+        Asserts.assertEQ(result, testValue2.hash());\n+        result = test40(testValue2Array, index);\n+        Asserts.assertEQ(result, testValue2.hash());\n+        result = test40(testValue1Array2, index);\n+        Asserts.assertEQ(result, testValue1.hash());\n+        result = test40(Long.valueOf(42), index);\n+        Asserts.assertEQ(result, 42L);\n+    }\n+\n+    \/\/ Test for bug in Escape Analysis\n+    @DontInline\n+    public void test41_dontinline(Object o) {\n+        Asserts.assertEQ(o, rI);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_OF_MYVALUE_KLASS})\n+    public void test41() {\n+        MyValue1[] vals = (MyValue1[])ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 1, MyValue1.DEFAULT);\n+        vals[0] = testValue1;\n+        test41_dontinline(vals[0].oa[0]);\n+        test41_dontinline(vals[0].oa[0]);\n+    }\n+\n+    @Run(test = \"test41\")\n+    public void test41_verifier() {\n+        test41();\n+    }\n+\n+    \/\/ Test for bug in Escape Analysis\n+    private static final MyValue1 test42VT1 = MyValue1.createWithFieldsInline(rI, rL);\n+    private static final MyValue1 test42VT2 = MyValue1.createWithFieldsInline(rI + 1, rL + 1);\n+\n+    @Test\n+    @IR(failOn = {ALLOC_OF_MYVALUE_KLASS})\n+    public void test42() {\n+        MyValue1[] vals = (MyValue1[])ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 2, MyValue1.DEFAULT);\n+        vals[0] = test42VT1;\n+        vals[1] = test42VT2;\n+        Asserts.assertEQ(vals[0].hash(), test42VT1.hash());\n+        Asserts.assertEQ(vals[1].hash(), test42VT2.hash());\n+    }\n+\n+    @Run(test = \"test42\")\n+    public void test42_verifier(RunInfo info) {\n+        if (!info.isWarmUp()) test42(); \/\/ We need -Xcomp behavior\n+    }\n+\n+    \/\/ Test for bug in Escape Analysis\n+    @Test\n+    @IR(failOn = {ALLOC_OF_MYVALUE_KLASS})\n+    public long test43(boolean deopt, Method m) {\n+        MyValue1[] vals = (MyValue1[])ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 2, MyValue1.DEFAULT);\n+        vals[0] = test42VT1;\n+        vals[1] = test42VT2;\n+\n+        if (deopt) {\n+            \/\/ uncommon trap\n+            TestFramework.deoptimize(m);\n+            Asserts.assertEQ(vals[0].hash(), test42VT1.hash());\n+            Asserts.assertEQ(vals[1].hash(), test42VT2.hash());\n+        }\n+\n+        return vals[0].hash();\n+    }\n+\n+    @Run(test = \"test43\")\n+    public void test43_verifier(RunInfo info) {\n+        test43(!info.isWarmUp(), info.getTest());\n+    }\n+\n+    \/\/ Tests writing an array element with a (statically known) incompatible type\n+    private static final MethodHandle setArrayElementIncompatible = InstructionHelper.buildMethodHandle(MethodHandles.lookup(),\n+        \"setArrayElementIncompatible\",\n+        MethodType.methodType(void.class, TestLWorld.class, MyValue1[].class, int.class, MyValue2.class),\n+        CODE -> {\n+            CODE.\n+            aload(1).\n+            iload(2).\n+            aload(3).\n+            aastore().\n+            return_();\n+        });\n+\n+    @Test\n+    public void test44(MyValue1[] va, int index, MyValue2 v) throws Throwable {\n+        setArrayElementIncompatible.invoke(this, va, index, v);\n+    }\n+\n+    @Run(test = \"test44\")\n+    @Warmup(10000)\n+    public void test44_verifier() throws Throwable {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test44(testValue1Array, index, testValue2);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue1Array[index].hash(), hash());\n+    }\n+\n+    \/\/ Tests writing an array element with a (statically known) incompatible type\n+    @ForceInline\n+    public void test45_inline(Object[] oa, Object o, int index) {\n+        oa[index] = o;\n+    }\n+\n+    @Test\n+    public void test45(MyValue1[] va, int index, MyValue2 v) throws Throwable {\n+        test45_inline(va, v, index);\n+    }\n+\n+    @Run(test = \"test45\")\n+    public void test45_verifier() throws Throwable {\n+        int index = Math.abs(rI) % 3;\n+        try {\n+            test45(testValue1Array, index, testValue2);\n+            throw new RuntimeException(\"No ArrayStoreException thrown\");\n+        } catch (ArrayStoreException e) {\n+            \/\/ Expected\n+        }\n+        Asserts.assertEQ(testValue1Array[index].hash(), hash());\n+    }\n+\n+    \/\/ instanceof tests with inline types\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public boolean test46(MyValue1 vt) {\n+        Object obj = vt;\n+        return obj instanceof MyValue1;\n+    }\n+\n+    @Run(test = \"test46\")\n+    public void test46_verifier() {\n+        MyValue1 vt = testValue1;\n+        boolean result = test46(vt);\n+        Asserts.assertTrue(result);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public boolean test47(MyValue1 vt) {\n+        Object obj = vt;\n+        return obj instanceof MyValue2;\n+    }\n+\n+    @Run(test = \"test47\")\n+    public void test47_verifier() {\n+        MyValue1 vt = testValue1;\n+        boolean result = test47(vt);\n+        Asserts.assertFalse(result);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public boolean test48(Object obj) {\n+        return obj instanceof MyValue1;\n+    }\n+\n+    @Run(test = \"test48\")\n+    public void test48_verifier() {\n+        MyValue1 vt = testValue1;\n+        boolean result = test48(vt);\n+        Asserts.assertTrue(result);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public boolean test49(Object obj) {\n+        return obj instanceof MyValue2;\n+    }\n+\n+    @Run(test = \"test49\")\n+    public void test49_verifier() {\n+        MyValue1 vt = testValue1;\n+        boolean result = test49(vt);\n+        Asserts.assertFalse(result);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public boolean test50(Object obj) {\n+        return obj instanceof MyValue1;\n+    }\n+\n+    @Run(test = \"test50\")\n+    public void test50_verifier() {\n+        Asserts.assertFalse(test49(new NonValueClass(42)));\n+    }\n+\n+    \/\/ Inline type with some non-flattened fields\n+    @LooselyConsistentValue\n+    static value class Test51Value {\n+        Object objectField1;\n+        Object objectField2;\n+        Object objectField3;\n+        Object objectField4;\n+        Object objectField5;\n+        Object objectField6;\n+\n+        @Strict\n+        @NullRestricted\n+        MyValue1 valueField1;\n+        @Strict\n+        @NullRestricted\n+        MyValue1 valueField2;\n+        MyValue1 valueField3;\n+        @Strict\n+        @NullRestricted\n+        MyValue1 valueField4;\n+        MyValue1 valueField5;\n+\n+        public Test51Value() {\n+            objectField1 = null;\n+            objectField2 = null;\n+            objectField3 = null;\n+            objectField4 = null;\n+            objectField5 = null;\n+            objectField6 = null;\n+            valueField1 = testValue1;\n+            valueField2 = testValue1;\n+            valueField3 = testValue1;\n+            valueField4 = MyValue1.createDefaultDontInline();\n+            valueField5 = MyValue1.createDefaultDontInline();\n+        }\n+\n+        public Test51Value(Object o1, Object o2, Object o3, Object o4, Object o5, Object o6,\n+                           MyValue1 vt1, MyValue1 vt2, MyValue1 vt3, MyValue1 vt4, MyValue1 vt5) {\n+            objectField1 = o1;\n+            objectField2 = o2;\n+            objectField3 = o3;\n+            objectField4 = o4;\n+            objectField5 = o5;\n+            objectField6 = o6;\n+            valueField1 = vt1;\n+            valueField2 = vt2;\n+            valueField3 = vt3;\n+            valueField4 = vt4;\n+            valueField5 = vt5;\n+        }\n+\n+        @ForceInline\n+        public long test(Test51Value holder, MyValue1 vt1, Object vt2) {\n+            holder = new Test51Value(vt1, holder.objectField2, holder.objectField3, holder.objectField4, holder.objectField5, holder.objectField6,\n+                                     holder.valueField1, holder.valueField2, holder.valueField3, holder.valueField4, holder.valueField5);\n+            holder = new Test51Value(holder.objectField1, (MyValue1)vt2, holder.objectField3, holder.objectField4, holder.objectField5, holder.objectField6,\n+                                     holder.valueField1, holder.valueField2, holder.valueField3, holder.valueField4, holder.valueField5);\n+            holder = new Test51Value(holder.objectField1, holder.objectField2, testValue1, holder.objectField4, holder.objectField5, holder.objectField6,\n+                                     holder.valueField1, holder.valueField2, holder.valueField3, holder.valueField4, holder.valueField5);\n+            holder = new Test51Value(holder.objectField1, holder.objectField2, holder.objectField3, MyValue1.createWithFieldsDontInline(rI, rL), holder.objectField5, holder.objectField6,\n+                                     holder.valueField1, holder.valueField2, holder.valueField3, holder.valueField4, holder.valueField5);\n+            holder = new Test51Value(holder.objectField1, holder.objectField2, holder.objectField3, holder.objectField4, holder.valueField1, holder.objectField6,\n+                                     holder.valueField1, holder.valueField2, holder.valueField3, holder.valueField4, holder.valueField5);\n+            holder = new Test51Value(holder.objectField1, holder.objectField2, holder.objectField3, holder.objectField4, holder.objectField5, holder.valueField3,\n+                                     holder.valueField1, holder.valueField2, holder.valueField3, holder.valueField4, holder.valueField5);\n+            holder = new Test51Value(holder.objectField1, holder.objectField2, holder.objectField3, holder.objectField4, holder.objectField5, holder.objectField6,\n+                                     (MyValue1)holder.objectField1, holder.valueField2, holder.valueField3, holder.valueField4, holder.valueField5);\n+            holder = new Test51Value(holder.objectField1, holder.objectField2, holder.objectField3, holder.objectField4, holder.objectField5, holder.objectField6,\n+                                     holder.valueField1, (MyValue1)vt2, holder.valueField3, holder.valueField4, holder.valueField5);\n+            holder = new Test51Value(holder.objectField1, holder.objectField2, holder.objectField3, holder.objectField4, holder.objectField5, holder.objectField6,\n+                                     holder.valueField1, holder.valueField2, (MyValue1)vt2, holder.valueField4, holder.valueField5);\n+\n+            return ((MyValue1)holder.objectField1).hash() +\n+                   ((MyValue1)holder.objectField2).hash() +\n+                   ((MyValue1)holder.objectField3).hash() +\n+                   ((MyValue1)holder.objectField4).hash() +\n+                   ((MyValue1)holder.objectField5).hash() +\n+                   ((MyValue1)holder.objectField6).hash() +\n+                   holder.valueField1.hash() +\n+                   holder.valueField2.hash() +\n+                   holder.valueField3.hash() +\n+                   holder.valueField4.hashPrimitive();\n+        }\n+    }\n+\n+    \/\/ Pass arguments via fields to avoid excessive spilling leading to compilation bailouts\n+    @Strict\n+    @NullRestricted\n+    static Test51Value test51_arg1 = new Test51Value();\n+    @Strict\n+    @NullRestricted\n+    static MyValue1 test51_arg2 = MyValue1.DEFAULT;\n+    static Object test51_arg3;\n+\n+    \/\/ Same as test2 but with field holder being an inline type\n+    @Test\n+    public long test51() {\n+        return test51_arg1.test(test51_arg1, test51_arg2, test51_arg3);\n+    }\n+\n+    @Run(test = \"test51\")\n+    public void test51_verifier() {\n+        MyValue1 vt = testValue1;\n+        MyValue1 def = MyValue1.createDefaultDontInline();\n+        Test51Value holder = new Test51Value();\n+        Asserts.assertEQ(testValue1.hash(), vt.hash());\n+        Asserts.assertEQ(holder.valueField1.hash(), vt.hash());\n+        test51_arg1 = holder;\n+        test51_arg2 = vt;\n+        test51_arg3 = vt;\n+        long result = test51();\n+        Asserts.assertEQ(result, 9*vt.hash() + def.hashPrimitive());\n+    }\n+\n+    \/\/ Access non-flattened, uninitialized inline type field with inline type holder\n+    @Test\n+    @IR(failOn = {ALLOC_OF_MYVALUE_KLASS})\n+    public void test52(Test51Value holder) {\n+        if ((Object)holder.valueField5 != null) {\n+            throw new RuntimeException(\"Should be null\");\n+        }\n+    }\n+\n+    @Run(test = \"test52\")\n+    public void test52_verifier() {\n+        Test51Value vt = new Test51Value(null, null, null, null, null, null,\n+                                         MyValue1.createDefaultInline(), MyValue1.createDefaultInline(), null, MyValue1.createDefaultInline(), null);\n+        test52(vt);\n+    }\n+\n+    \/\/ Merging inline types of different types\n+    @Test\n+    public Object test53(Object o, boolean b) {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        return b ? vt : o;\n+    }\n+\n+    @Run(test = \"test53\")\n+    public void test53_verifier() {\n+        test53(new Object(), false);\n+        MyValue1 result = (MyValue1)test53(new Object(), true);\n+        Asserts.assertEQ(result.hash(), hash());\n+    }\n+\n+    @Test\n+    public Object test54(boolean b) {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        return b ? vt : testValue2;\n+    }\n+\n+    @Run(test = \"test54\")\n+    public void test54_verifier() {\n+        MyValue1 result1 = (MyValue1)test54(true);\n+        Asserts.assertEQ(result1.hash(), hash());\n+        MyValue2 result2 = (MyValue2)test54(false);\n+        Asserts.assertEQ(result2.hash(), testValue2.hash());\n+    }\n+\n+    @Test\n+    public Object test55(boolean b) {\n+        MyValue1 vt1 = MyValue1.createWithFieldsInline(rI, rL);\n+        MyValue2 vt2 = MyValue2.createWithFieldsInline(rI, rD);\n+        return b ? vt1 : vt2;\n+    }\n+\n+    @Run(test = \"test55\")\n+    public void test55_verifier() {\n+        MyValue1 result1 = (MyValue1)test55(true);\n+        Asserts.assertEQ(result1.hash(), hash());\n+        MyValue2 result2 = (MyValue2)test55(false);\n+        Asserts.assertEQ(result2.hash(), testValue2.hash());\n+    }\n+\n+    \/\/ Test synchronization on inline types\n+    @Test\n+    public void test56(Object vt) {\n+        synchronized (vt) {\n+            throw new RuntimeException(\"test56 failed: synchronization on inline type should not succeed\");\n+        }\n+    }\n+\n+    @Run(test = \"test56\")\n+    public void test56_verifier() {\n+        try {\n+            test56(testValue1);\n+            throw new RuntimeException(\"test56 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    @ForceInline\n+    public void test57_inline(Object vt) {\n+        synchronized (vt) {\n+            throw new RuntimeException(\"test57 failed: synchronization on inline type should not succeed\");\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public void test57(MyValue1 vt) {\n+        test57_inline(vt);\n+    }\n+\n+    @Run(test = \"test57\")\n+    public void test57_verifier() {\n+        try {\n+            test57(testValue1);\n+            throw new RuntimeException(\"test57 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    @ForceInline\n+    public void test58_inline(Object vt) {\n+        synchronized (vt) {\n+            throw new RuntimeException(\"test58 failed: synchronization on inline type should not succeed\");\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_OF_MYVALUE_KLASS})\n+    public void test58() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        test58_inline(vt);\n+    }\n+\n+    @Run(test = \"test58\")\n+    public void test58_verifier() {\n+        try {\n+            test58();\n+            throw new RuntimeException(\"test58 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    @Test\n+    public void test59(Object o, boolean b) {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        Object sync = b ? vt : o;\n+        synchronized (sync) {\n+            if (b) {\n+                throw new RuntimeException(\"test59 failed: synchronization on inline type should not succeed\");\n+            }\n+        }\n+    }\n+\n+    @Run(test = \"test59\")\n+    public void test59_verifier() {\n+        test59(new Object(), false);\n+        try {\n+            test59(new Object(), true);\n+            throw new RuntimeException(\"test59 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    @Test\n+    public void test60(boolean b) {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        Object sync = b ? vt : testValue2;\n+        synchronized (sync) {\n+            throw new RuntimeException(\"test60 failed: synchronization on inline type should not succeed\");\n+        }\n+    }\n+\n+    @Run(test = \"test60\")\n+    public void test60_verifier() {\n+        try {\n+            test60(false);\n+            throw new RuntimeException(\"test60 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+        try {\n+            test60(true);\n+            throw new RuntimeException(\"test60 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test catching the IdentityException in compiled code\n+    @Test\n+    public void test61(Object vt) {\n+        boolean thrown = false;\n+        try {\n+            synchronized (vt) {\n+                throw new RuntimeException(\"test61 failed: no exception thrown\");\n+            }\n+        } catch (IdentityException ex) {\n+            thrown = true;\n+        }\n+        if (!thrown) {\n+            throw new RuntimeException(\"test61 failed: no exception thrown\");\n+        }\n+    }\n+\n+    @Run(test = \"test61\")\n+    public void test61_verifier() {\n+        test61(testValue1);\n+    }\n+\n+    @Test\n+    public void test62(Object o) {\n+        try {\n+            synchronized (o) { }\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+            return;\n+        }\n+        throw new RuntimeException(\"test62 failed: no exception thrown\");\n+    }\n+\n+    @Run(test = \"test62\")\n+    public void test62_verifier() {\n+        test62(testValue1);\n+    }\n+\n+    \/\/ Test synchronization without any instructions in the synchronized block\n+    @Test\n+    public void test63(Object o) {\n+        synchronized (o) { }\n+    }\n+\n+    @Run(test = \"test63\")\n+    public void test63_verifier() {\n+        try {\n+            test63(testValue1);\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+            return;\n+        }\n+        throw new RuntimeException(\"test63 failed: no exception thrown\");\n+    }\n+\n+    \/\/ type system test with interface and inline type\n+    @ForceInline\n+    public MyInterface test64Interface_helper(MyValue1 vt) {\n+        return vt;\n+    }\n+\n+    @Test\n+    public MyInterface test64Interface(MyValue1 vt) {\n+        return test64Interface_helper(vt);\n+    }\n+\n+    @Run(test = \"test64Interface\")\n+    public void test64Interface_verifier() {\n+        test64Interface(testValue1);\n+    }\n+\n+    \/\/ type system test with abstract and inline type\n+    @ForceInline\n+    public MyAbstract test64Abstract_helper(MyValue1 vt) {\n+        return vt;\n+    }\n+\n+    @Test\n+    public MyAbstract test64Abstract(MyValue1 vt) {\n+        return test64Abstract_helper(vt);\n+    }\n+\n+    @Run(test = \"test64Abstract\")\n+    public void test64Abstract_verifier() {\n+        test64Abstract(testValue1);\n+    }\n+\n+    \/\/ Array store tests\n+    @Test\n+    public void test65(Object[] array, MyValue1 vt) {\n+        array[0] = vt;\n+    }\n+\n+    @Run(test = \"test65\")\n+    public void test65_verifier() {\n+        Object[] array = new Object[1];\n+        test65(array, testValue1);\n+        Asserts.assertEQ(((MyValue1)array[0]).hash(), testValue1.hash());\n+    }\n+\n+    @Test\n+    public void test66(Object[] array, MyValue1 vt) {\n+        array[0] = vt;\n+    }\n+\n+    @Run(test = \"test66\")\n+    public void test66_verifier() {\n+        MyValue1[] array = (MyValue1[])ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 1, MyValue1.DEFAULT);\n+        test66(array, testValue1);\n+        Asserts.assertEQ(array[0].hash(), testValue1.hash());\n+    }\n+\n+    @Test\n+    public void test67(Object[] array, Object vt) {\n+        array[0] = vt;\n+    }\n+\n+    @Run(test = \"test67\")\n+    public void test67_verifier() {\n+        MyValue1[] array = (MyValue1[])ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 1, MyValue1.DEFAULT);\n+        test67(array, testValue1);\n+        Asserts.assertEQ(array[0].hash(), testValue1.hash());\n+    }\n+\n+    @Test\n+    public void test68(Object[] array, NonValueClass o) {\n+        array[0] = o;\n+    }\n+\n+    @Run(test = \"test68\")\n+    public void test68_verifier() {\n+        NonValueClass[] array = new NonValueClass[1];\n+        NonValueClass obj = new NonValueClass(1);\n+        test68(array, obj);\n+        Asserts.assertEQ(array[0], obj);\n+    }\n+\n+    \/\/ Test conversion between an inline type and java.lang.Object without an allocation\n+    @ForceInline\n+    public Object test69_sum(Object a, Object b) {\n+        int sum = ((MyValue1)a).x + ((MyValue1)b).x;\n+        return MyValue1.setX(((MyValue1)a), sum);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC, STORE_OF_ANY_KLASS})\n+    public int test69(MyValue1[] array) {\n+        MyValue1 result = MyValue1.createDefaultInline();\n+        for (int i = 0; i < array.length; ++i) {\n+            result = (MyValue1)test69_sum(result, array[i]);\n+        }\n+        return result.x;\n+    }\n+\n+    @Run(test = \"test69\")\n+    public void test69_verifier() {\n+        int result = test69(testValue1Array);\n+        Asserts.assertEQ(result, rI * testValue1Array.length);\n+    }\n+\n+    \/\/ Same as test69 but with an Interface\n+    @ForceInline\n+    public MyInterface test70Interface_sum(MyInterface a, MyInterface b) {\n+        int sum = ((MyValue1)a).x + ((MyValue1)b).x;\n+        return MyValue1.setX(((MyValue1)a), sum);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC, STORE_OF_ANY_KLASS})\n+    public int test70Interface(MyValue1[] array) {\n+        MyValue1 result = MyValue1.createDefaultInline();\n+        for (int i = 0; i < array.length; ++i) {\n+            result = (MyValue1)test70Interface_sum(result, array[i]);\n+        }\n+        return result.x;\n+    }\n+\n+    @Run(test = \"test70Interface\")\n+    public void test70Interface_verifier() {\n+        int result = test70Interface(testValue1Array);\n+        Asserts.assertEQ(result, rI * testValue1Array.length);\n+    }\n+\n+    \/\/ Same as test69 but with an Abstract\n+    @ForceInline\n+    public MyAbstract test70Abstract_sum(MyAbstract a, MyAbstract b) {\n+        int sum = ((MyValue1)a).x + ((MyValue1)b).x;\n+        return MyValue1.setX(((MyValue1)a), sum);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC, STORE_OF_ANY_KLASS})\n+    public int test70Abstract(MyValue1[] array) {\n+        MyValue1 result = MyValue1.createDefaultInline();\n+        for (int i = 0; i < array.length; ++i) {\n+            result = (MyValue1)test70Abstract_sum(result, array[i]);\n+        }\n+        return result.x;\n+    }\n+\n+    @Run(test = \"test70Abstract\")\n+    public void test70Abstract_verifier() {\n+        int result = test70Abstract(testValue1Array);\n+        Asserts.assertEQ(result, rI * testValue1Array.length);\n+    }\n+\n+    \/\/ Test that allocated inline type is not used in non-dominated path\n+    public MyValue1 test71_inline(Object obj) {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        try {\n+            vt = (MyValue1)Objects.requireNonNull(obj);\n+            throw new RuntimeException(\"NullPointerException expected\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+        return vt;\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC_OF_MYVALUE_KLASS})\n+    public MyValue1 test71() {\n+        return test71_inline(null);\n+    }\n+\n+    @Run(test = \"test71\")\n+    public void test71_verifier() {\n+        MyValue1 vt = test71();\n+        Asserts.assertEquals(vt.hash(), hash());\n+    }\n+\n+    \/\/ Test calling a method on an uninitialized inline type\n+    @LooselyConsistentValue\n+    value class Test72Value {\n+        int x = 0;\n+\n+        public int get() {\n+            return x;\n+        }\n+    }\n+\n+    \/\/ Make sure Test72Value is loaded but not initialized\n+    public void unused(Test72Value vt) { }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public int test72() {\n+        Test72Value vt = new Test72Value();\n+        return vt.get();\n+    }\n+\n+    @Run(test = \"test72\")\n+    @Warmup(0)\n+    public void test72_verifier() {\n+        int result = test72();\n+        Asserts.assertEquals(result, 0);\n+    }\n+\n+    \/\/ Tests for loading\/storing unknown values\n+    @Test\n+    public Object test73(Object[] va) {\n+        return va[0];\n+    }\n+\n+    @Run(test = \"test73\")\n+    public void test73_verifier() {\n+        MyValue1 vt = (MyValue1)test73(testValue1Array);\n+        Asserts.assertEquals(testValue1Array[0].hash(), vt.hash());\n+    }\n+\n+    @Test\n+    public void test74(Object[] va, Object vt) {\n+        va[0] = vt;\n+    }\n+\n+    @Run(test = \"test74\")\n+    public void test74_verifier() {\n+        MyValue1[] va = (MyValue1[])ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 1, MyValue1.DEFAULT);\n+        test74(va, testValue1);\n+        Asserts.assertEquals(va[0].hash(), testValue1.hash());\n+    }\n+\n+    \/\/ Verify that mixing instances and arrays with the clone api\n+    \/\/ doesn't break anything\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public Object test75(Object o) {\n+        MyValue1[] va = (MyValue1[])ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 1, MyValue1.DEFAULT);\n+        Object[] next = va;\n+        Object[] arr = va;\n+        for (int i = 0; i < 10; i++) {\n+            arr = next;\n+            next = new NonValueClass[1];\n+        }\n+        return arr[0];\n+    }\n+\n+    @Run(test = \"test75\")\n+    public void test75_verifier() {\n+        test75(42);\n+    }\n+\n+    \/\/ Casting an NonValueClass to a inline type should throw a ClassCastException\n+    @ForceInline\n+    public MyValue1 test77_helper(Object o) {\n+        return (MyValue1)o;\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public MyValue1 test77(NonValueClass obj) throws Throwable {\n+        return test77_helper(obj);\n+    }\n+\n+    @Run(test = \"test77\")\n+    public void test77_verifier() throws Throwable {\n+        try {\n+            test77(new NonValueClass(42));\n+            throw new RuntimeException(\"ClassCastException expected\");\n+        } catch (ClassCastException e) {\n+            \/\/ Expected\n+        } catch (Exception e) {\n+            throw new RuntimeException(\"test77 failed: unexpected exception\", e);\n+        }\n+    }\n+\n+    \/\/ Casting a null NonValueClass to a nullable inline type should not throw\n+    @ForceInline\n+    public MyValue1 test78_helper(Object o) {\n+        return (MyValue1)o;\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public MyValue1 test78(NonValueClass obj) throws Throwable {\n+        return test78_helper(obj);\n+    }\n+\n+    @Run(test = \"test78\")\n+    public void test78_verifier() throws Throwable {\n+        try {\n+            test78(null); \/\/ Should not throw\n+        } catch (Exception e) {\n+            throw new RuntimeException(\"test78 failed: unexpected exception\", e);\n+        }\n+    }\n+\n+    \/\/ Casting an NonValueClass to a nullable inline type should throw a ClassCastException\n+    @ForceInline\n+    public MyValue1 test79_helper(Object o) {\n+        return (MyValue1)o;\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public MyValue1 test79(NonValueClass obj) throws Throwable {\n+        return test79_helper(obj);\n+    }\n+\n+    @Run(test = \"test79\")\n+    public void test79_verifier() throws Throwable {\n+        try {\n+            test79(new NonValueClass(42));\n+            throw new RuntimeException(\"ClassCastException expected\");\n+        } catch (ClassCastException e) {\n+            \/\/ Expected\n+        } catch (Exception e) {\n+            throw new RuntimeException(\"test79 failed: unexpected exception\", e);\n+        }\n+    }\n+\n+    \/\/ Test flattened field with non-flattened (but flattenable) inline type field\n+    @LooselyConsistentValue\n+    static value class Small {\n+        int i;\n+        @Strict\n+        @NullRestricted\n+        Big big; \/\/ Too big to be flattened\n+\n+        private Small() {\n+            i = rI;\n+            big = new Big();\n+        }\n+\n+        private Small(int i, Big big) {\n+            this.i = i;\n+            this.big = big;\n+        }\n+    }\n+\n+    @LooselyConsistentValue\n+    static value class Big {\n+        long l0,l1,l2,l3,l4,l5,l6,l7,l8,l9;\n+        long l10,l11,l12,l13,l14,l15,l16,l17,l18,l19;\n+        long l20,l21,l22,l23,l24,l25,l26,l27,l28,l29;\n+\n+        private Big() {\n+            l0 = l1 = l2 = l3 = l4 = l5 = l6 = l7 = l8 = l9 = rL;\n+            l10 = l11 = l12 = l13 = l14 = l15 = l16 = l17 = l18 = l19 = rL+1;\n+            l20 = l21 = l22 = l23 = l24 = l25 = l26 = l27 = l28 = l29 = rL+2;\n+        }\n+\n+        private Big(long l) {\n+            l0 = l1 = l2 = l3 = l4 = l5 = l6 = l7 = l8 = l9 = l10 =\n+            l11 = l12 = l13 = l14 = l15 = l16 = l17 = l18 = l19 = l20 =\n+            l21 = l22 = l23 = l24 = l25 = l26 = l27 = l28 = l29 = 0;\n+        }\n+    }\n+\n+    @Strict\n+    @NullRestricted\n+    Small small = new Small();\n+    @Strict\n+    @NullRestricted\n+    Small smallDefault = new Small(0, new Big(0));\n+    @Strict\n+    @NullRestricted\n+    Big big = new Big();\n+    @Strict\n+    @NullRestricted\n+    Big bigDefault = new Big(0);\n+\n+    @Test\n+    public long test80() {\n+        return small.i + small.big.l0 + smallDefault.i + smallDefault.big.l29 + big.l0 + bigDefault.l29;\n+    }\n+\n+    @Run(test = \"test80\")\n+    public void test80_verifier() throws Throwable {\n+        long result = test80();\n+        Asserts.assertEQ(result, rI + 2*rL);\n+    }\n+\n+    \/\/ Test scalarization with exceptional control flow\n+    public int test81Callee(MyValue1 vt)  {\n+        return vt.x;\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC, LOAD_OF_ANY_KLASS, STORE_OF_ANY_KLASS})\n+    public int test81()  {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        int result = 0;\n+        for (int i = 0; i < 10; i++) {\n+            try {\n+                result += test81Callee(vt);\n+            } catch (NullPointerException npe) {\n+                result += rI;\n+            }\n+        }\n+        return result;\n+    }\n+\n+    @Run(test = \"test81\")\n+    public void test81_verifier() {\n+        int result = test81();\n+        Asserts.assertEQ(result, 10*rI);\n+    }\n+\n+    \/\/ Test check for null free array when storing to inline type array\n+    @Test\n+    public void test82(Object[] dst, Object v) {\n+        dst[0] = v;\n+    }\n+\n+    @Run(test = \"test82\")\n+    public void test82_verifier(RunInfo info) {\n+        MyValue2[] dst = (MyValue2[])ValueClass.newNullRestrictedNonAtomicArray(MyValue2.class, 1, MyValue2.DEFAULT);\n+        test82(dst, testValue2);\n+        if (!info.isWarmUp()) {\n+            try {\n+                test82(dst, null);\n+                throw new RuntimeException(\"No ArrayStoreException thrown\");\n+            } catch (NullPointerException e) {\n+                \/\/ Expected\n+            }\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public void test83(Object[] dst, Object v, boolean flag) {\n+        if (dst == null) { \/\/ null check\n+        }\n+        if (flag) {\n+            if (dst.getClass() == MyValue1[].class) { \/\/ trigger split if\n+            }\n+        } else {\n+            dst = (MyValue2[])ValueClass.newNullRestrictedNonAtomicArray(MyValue2.class, 1, MyValue2.DEFAULT); \/\/ constant null free property\n+        }\n+        dst[0] = v;\n+    }\n+\n+    @Run(test = \"test83\")\n+    @Warmup(10000)\n+    public void test83_verifier(RunInfo info) {\n+        MyValue2[] dst = (MyValue2[])ValueClass.newNullRestrictedNonAtomicArray(MyValue2.class, 1, MyValue2.DEFAULT);\n+        test83(dst, testValue2, false);\n+        test83(dst, testValue2, true);\n+        if (!info.isWarmUp()) {\n+            try {\n+                test83(dst, null, true);\n+                throw new RuntimeException(\"No ArrayStoreException thrown\");\n+            } catch (NullPointerException e) {\n+                \/\/ Expected\n+            }\n+        }\n+    }\n+\n+    private void rerun_and_recompile_for(Method m, int num, Runnable test) {\n+        for (int i = 1; i < num; i++) {\n+            test.run();\n+\n+            if (!TestFramework.isCompiled(m)) {\n+                TestFramework.compile(m, CompLevel.C2);\n+            }\n+        }\n+    }\n+\n+    \/\/ Tests for the Loop Unswitching optimization\n+    \/\/ Should make 2 copies of the loop, one for non flattened arrays, one for other cases.\n+    @Test\n+    @IR(applyIf = {\"UseArrayFlattening\", \"true\"},\n+        counts = {COUNTED_LOOP_MAIN, \"= 2\"})\n+    @IR(applyIf = {\"UseArrayFlattening\", \"false\"},\n+        counts = {COUNTED_LOOP_MAIN, \"= 1\"})\n+    public void test84(Object[] src, Object[] dst) {\n+        for (int i = 0; i < src.length; i++) {\n+            dst[i] = src[i];\n+        }\n+    }\n+\n+    @Run(test = \"test84\")\n+    @Warmup(0)\n+    public void test84_verifier(RunInfo info) {\n+        MyValue2[] src = (MyValue2[])ValueClass.newNullRestrictedNonAtomicArray(MyValue2.class, 100, MyValue2.DEFAULT);\n+        Arrays.fill(src, testValue2);\n+        MyValue2[] dst = (MyValue2[])ValueClass.newNullRestrictedNonAtomicArray(MyValue2.class, 100, MyValue2.DEFAULT);\n+        rerun_and_recompile_for(info.getTest(), 10,\n+                                () ->  { test84(src, dst);\n+                                         Asserts.assertTrue(Arrays.equals(src, dst)); });\n+    }\n+\n+    @Test\n+    @IR(applyIfAnd = {\"UseArrayFlattening\", \"true\", \"OnError\", \"JDK-8370070-IsFixed\"},\n+        counts = {COUNTED_LOOP, \"= 2\", LOAD_UNKNOWN_INLINE, \"= 1\"})\n+    public void test85(Object[] src, Object[] dst) {\n+        for (int i = 0; i < src.length; i++) {\n+            dst[i] = src[i];\n+        }\n+    }\n+\n+    @Run(test = \"test85\")\n+    @Warmup(0)\n+    public void test85_verifier(RunInfo info) {\n+        Object[] src = new Object[100];\n+        Arrays.fill(src, new Object());\n+        src[0] = null;\n+        Object[] dst = new Object[100];\n+        rerun_and_recompile_for(info.getTest(), 10,\n+                                () -> { test85(src, dst);\n+                                        Asserts.assertTrue(Arrays.equals(src, dst)); });\n+    }\n+\n+    @Test\n+    @IR(applyIfAnd = {\"UseArrayFlattening\", \"true\", \"OnError\", \"JDK-8370070-IsFixed\"},\n+        counts = {COUNTED_LOOP, \"= 2\"})\n+    public void test86(Object[] src, Object[] dst) {\n+        for (int i = 0; i < src.length; i++) {\n+            dst[i] = src[i];\n+        }\n+    }\n+\n+    @Run(test = \"test86\")\n+    @Warmup(0)\n+    public void test86_verifier(RunInfo info) {\n+        MyValue2[] src = (MyValue2[])ValueClass.newNullRestrictedNonAtomicArray(MyValue2.class, 100, MyValue2.DEFAULT);\n+        Arrays.fill(src, testValue2);\n+        Object[] dst = new Object[100];\n+        rerun_and_recompile_for(info.getTest(), 10,\n+                                () -> { test86(src, dst);\n+                                        Asserts.assertTrue(Arrays.equals(src, dst)); });\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseArrayFlattening\", \"true\"},\n+        counts = {COUNTED_LOOP_MAIN, \"= 2\"})\n+    @IR(applyIf = {\"UseArrayFlattening\", \"false\"},\n+        counts = {COUNTED_LOOP_MAIN, \"= 1\"})\n+    public void test87(Object[] src, Object[] dst) {\n+        for (int i = 0; i < src.length; i++) {\n+            dst[i] = src[i];\n+        }\n+    }\n+\n+    @Run(test = \"test87\")\n+    @Warmup(0)\n+    public void test87_verifier(RunInfo info) {\n+        Object[] src = new Object[100];\n+        Arrays.fill(src, testValue2);\n+        MyValue2[] dst = (MyValue2[])ValueClass.newNullRestrictedNonAtomicArray(MyValue2.class, 100, MyValue2.DEFAULT);\n+\n+        rerun_and_recompile_for(info.getTest(), 10,\n+                                () -> { test87(src, dst);\n+                                        Asserts.assertTrue(Arrays.equals(src, dst)); });\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseArrayFlattening\", \"true\"},\n+        counts = {COUNTED_LOOP_MAIN, \"= 2\"})\n+    @IR(applyIf = {\"UseArrayFlattening\", \"false\"},\n+        counts = {COUNTED_LOOP_MAIN, \"= 0\"})\n+    public void test88(Object[] src1, Object[] dst1, Object[] src2, Object[] dst2) {\n+        for (int i = 0; i < src1.length; i++) {\n+            dst1[i] = src1[i];\n+            dst2[i] = src2[i];\n+        }\n+    }\n+\n+    @Run(test = \"test88\")\n+    @Warmup(0)\n+    public void test88_verifier(RunInfo info) {\n+        MyValue2[] src1 = (MyValue2[])ValueClass.newNullRestrictedNonAtomicArray(MyValue2.class, 100, MyValue2.DEFAULT);\n+        Arrays.fill(src1, testValue2);\n+        MyValue2[] dst1 = (MyValue2[])ValueClass.newNullRestrictedNonAtomicArray(MyValue2.class, 100, MyValue2.DEFAULT);\n+        Object[] src2 = new Object[100];\n+        Arrays.fill(src2, new Object());\n+        Object[] dst2 = new Object[100];\n+\n+        rerun_and_recompile_for(info.getTest(), 10,\n+                                () -> { test88(src1, dst1, src2, dst2);\n+                                        Asserts.assertTrue(Arrays.equals(src1, dst1));\n+                                        Asserts.assertTrue(Arrays.equals(src2, dst2)); });\n+    }\n+\n+    @Test\n+    public boolean test89(Object obj) {\n+        return obj.getClass() == NonValueClass.class;\n+    }\n+\n+    @Run(test = \"test89\")\n+    public void test89_verifier() {\n+        Asserts.assertTrue(test89(new NonValueClass(42)));\n+        Asserts.assertFalse(test89(new Object()));\n+    }\n+\n+    @Test\n+    public NonValueClass test90(Object obj) {\n+        return (NonValueClass)obj;\n+    }\n+\n+    @Run(test = \"test90\")\n+    public void test90_verifier() {\n+        test90(new NonValueClass(42));\n+        try {\n+            test90(new Object());\n+            throw new RuntimeException(\"ClassCastException expected\");\n+        } catch (ClassCastException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    @Test\n+    public boolean test91(Object obj) {\n+        return obj.getClass() == MyValue2[].class;\n+    }\n+\n+    @Run(test = \"test91\")\n+    public void test91_verifier() {\n+        Asserts.assertTrue(test91((MyValue2[])ValueClass.newNullRestrictedNonAtomicArray(MyValue2.class, 1, MyValue2.DEFAULT)));\n+        Asserts.assertTrue(test91(new MyValue2[1]));\n+        Asserts.assertFalse(test91(new Object()));\n+    }\n+\n+    @LooselyConsistentValue\n+    static value class Test92Value {\n+        int field;\n+\n+        public Test92Value() {\n+            field = 0x42;\n+        }\n+    }\n+\n+    @Test\n+    \/\/ TODO 8355382 The optimization only applies to null-free, flat arrays\n+    @IR(applyIfAnd = {\"UseArrayFlattening\", \"true\", \"UseNullableValueFlattening\", \"false\"},\n+        counts = {CLASS_CHECK_TRAP, \"= 2\"},\n+        failOn = {LOAD_UNKNOWN_INLINE, ALLOC, MEMBAR})\n+    public Object test92(Object[] array) {\n+        \/\/ Dummy loops to ensure we run enough passes of split if\n+        for (int i = 0; i < 2; i++) {\n+            for (int j = 0; j < 2; j++) {\n+                for (int k = 0; k < 2; k++) {\n+                }\n+            }\n+        }\n+        return (NonValueClass)array[0];\n+    }\n+\n+    @Run(test = \"test92\")\n+    @Warmup(10000)\n+    public void test92_verifier() {\n+        Object[] array = new Object[1];\n+        Object obj = new NonValueClass(rI);\n+        array[0] = obj;\n+        Object result = test92(array);\n+        Asserts.assertEquals(result, obj);\n+    }\n+\n+    \/\/ If the class check succeeds, the flattened array check that\n+    \/\/ precedes will never succeed and the flat array branch should\n+    \/\/ trigger an uncommon trap.\n+    @Test\n+    public Object test93(Object[] array) {\n+        for (int i = 0; i < 2; i++) {\n+            for (int j = 0; j < 2; j++) {\n+            }\n+        }\n+\n+        return (NonValueClass)array[0];\n+    }\n+\n+    @Run(test = \"test93\")\n+    @Warmup(10000)\n+    public void test93_verifier(RunInfo info) {\n+        if (info.isWarmUp()) {\n+            Object[] array = new Object[1];\n+            array[0] = new NonValueClass(42);\n+            Object result = test93(array);\n+            Asserts.assertEquals(((NonValueClass)result).x, 42);\n+        } else {\n+            Object[] array = (Test92Value[])ValueClass.newNullRestrictedNonAtomicArray(Test92Value.class, 1, new Test92Value());\n+            Method m = info.getTest();\n+            int extra = 3;\n+            for (int j = 0; j < extra; j++) {\n+                for (int i = 0; i < 10; i++) {\n+                    try {\n+                        test93(array);\n+                    } catch (ClassCastException cce) {\n+                    }\n+                }\n+                boolean compiled = TestFramework.isCompiled(m);\n+                boolean compilationSkipped = info.isCompilationSkipped();\n+                Asserts.assertTrue(compilationSkipped || compiled || (j != extra-1));\n+                if (!compilationSkipped && !compiled) {\n+                    TestFramework.compile(m, CompLevel.ANY);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Test\n+    \/\/ TODO 8355382 The optimization only applies to null-free, flat arrays\n+    @IR(applyIfAnd = {\"UseArrayFlattening\", \"true\", \"UseNullableValueFlattening\", \"false\"},\n+        counts = {CLASS_CHECK_TRAP, \"= 2\", LOOP, \"= 1\"},\n+        failOn = {LOAD_UNKNOWN_INLINE, ALLOC, MEMBAR})\n+    public int test94(Object[] array) {\n+        int res = 0;\n+        for (int i = 1; i < 4; i *= 2) {\n+            Object v = array[i];\n+            res += ((NonValueClass)v).x;\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test94\")\n+    @Warmup(10000)\n+    public void test94_verifier() {\n+        Object[] array = new Object[4];\n+        Object obj = new NonValueClass(rI);\n+        array[0] = obj;\n+        array[1] = obj;\n+        array[2] = obj;\n+        array[3] = obj;\n+        int result = test94(array);\n+        Asserts.assertEquals(result, rI * 2);\n+    }\n+\n+    @Test\n+    public boolean test95(Object o1, Object o2) {\n+        return o1 == o2;\n+    }\n+\n+    @Run(test = \"test95\")\n+    @Warmup(10000)\n+    public void test95_verifier() {\n+        Object o1 = new Object();\n+        Object o2 = new Object();\n+        Asserts.assertTrue(test95(o1, o1));\n+        Asserts.assertTrue(test95(null, null));\n+        Asserts.assertFalse(test95(o1, null));\n+        Asserts.assertFalse(test95(o1, o2));\n+    }\n+\n+    @Test\n+    public boolean test96(Object o1, Object o2) {\n+        return o1 == o2;\n+    }\n+\n+    @Run(test = \"test96\")\n+    @Warmup(10000)\n+    public void test96_verifier(RunInfo info) {\n+        Object o1 = new Object();\n+        Object o2 = new Object();\n+        Asserts.assertTrue(test96(o1, o1));\n+        Asserts.assertFalse(test96(o1, o2));\n+        if (!info.isWarmUp()) {\n+            Asserts.assertTrue(test96(null, null));\n+            Asserts.assertFalse(test96(o1, null));\n+        }\n+    }\n+\n+    \/\/ Abstract class tests\n+\n+    @DontInline\n+    public MyAbstract test97_dontinline1(MyAbstract o) {\n+        return o;\n+    }\n+\n+    @DontInline\n+    public MyValue1 test97_dontinline2(MyAbstract o) {\n+        return (MyValue1)o;\n+    }\n+\n+    @ForceInline\n+    public MyAbstract test97_inline1(MyAbstract o) {\n+        return o;\n+    }\n+\n+    @ForceInline\n+    public MyValue1 test97_inline2(MyAbstract o) {\n+        return (MyValue1)o;\n+    }\n+\n+    @Test\n+    public MyValue1 test97() {\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI, rL);\n+        vt = (MyValue1)test97_dontinline1(vt);\n+        vt =           test97_dontinline2(vt);\n+        vt = (MyValue1)test97_inline1(vt);\n+        vt =           test97_inline2(vt);\n+        return vt;\n+    }\n+\n+    @Run(test = \"test97\")\n+    public void test97_verifier() {\n+        Asserts.assertEQ(test97().hash(), hash());\n+    }\n+\n+    \/\/ Test storing\/loading inline types to\/from abstract and inline type fields\n+    MyAbstract abstractField1 = null;\n+    MyAbstract abstractField2 = null;\n+    MyAbstract abstractField3 = null;\n+    MyAbstract abstractField4 = null;\n+    MyAbstract abstractField5 = null;\n+    MyAbstract abstractField6 = null;\n+\n+    @DontInline\n+    public MyAbstract readValueField5AsAbstract() {\n+        return (MyAbstract)valueField5;\n+    }\n+\n+    @DontInline\n+    public MyAbstract readStaticValueField4AsAbstract() {\n+        return (MyAbstract)staticValueField4;\n+    }\n+\n+    @Test\n+    public long test98(MyValue1 vt1, MyAbstract vt2) {\n+        abstractField1 = vt1;\n+        abstractField2 = (MyValue1)vt2;\n+        abstractField3 = MyValue1.createWithFieldsInline(rI, rL);\n+        abstractField4 = MyValue1.createWithFieldsDontInline(rI, rL);\n+        abstractField5 = valueField1;\n+        abstractField6 = valueField3;\n+        valueField1 = (MyValue1)abstractField1;\n+        valueField2 = (MyValue1)vt2;\n+        valueField3 = (MyValue1)vt2;\n+        staticValueField1 = (MyValue1)abstractField1;\n+        staticValueField2 = (MyValue1)vt1;\n+        \/\/ Don't inline these methods because reading NULL will trigger a deoptimization\n+        if (readValueField5AsAbstract() != null || readStaticValueField4AsAbstract() != null) {\n+            throw new RuntimeException(\"Should be null\");\n+        }\n+        return ((MyValue1)abstractField1).hash() + ((MyValue1)abstractField2).hash() +\n+               ((MyValue1)abstractField3).hash() + ((MyValue1)abstractField4).hash() +\n+               ((MyValue1)abstractField5).hash() + ((MyValue1)abstractField6).hash() +\n+                valueField1.hash() + valueField2.hash() + valueField3.hash() + valueField4.hashPrimitive() +\n+                staticValueField1.hash() + staticValueField2.hash() + staticValueField3.hashPrimitive();\n+    }\n+\n+    @Run(test = \"test98\")\n+    public void test98_verifier() {\n+        MyValue1 vt = testValue1;\n+        MyValue1 def = MyValue1.createDefaultDontInline();\n+        long result = test98(vt, vt);\n+        Asserts.assertEQ(result, 11*vt.hash() + 2*def.hashPrimitive());\n+    }\n+\n+    value class MyObject2 extends MyAbstract {\n+        public int x;\n+\n+        public MyObject2(int x) {\n+            this.x = x;\n+        }\n+\n+        @ForceInline\n+        public long hash() {\n+            return x;\n+        }\n+    }\n+\n+    \/\/ Test merging inline types and abstract classes\n+    @Test\n+    public MyAbstract test99(int state) {\n+        MyAbstract res = null;\n+        if (state == 0) {\n+            res = new MyObject2(rI);\n+        } else if (state == 1) {\n+            res = MyValue1.createWithFieldsInline(rI, rL);\n+        } else if (state == 2) {\n+            res = MyValue1.createWithFieldsDontInline(rI, rL);\n+        } else if (state == 3) {\n+            res = (MyValue1)objectField1;\n+        } else if (state == 4) {\n+            res = valueField1;\n+        } else if (state == 5) {\n+            res = null;\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test99\")\n+    public void test99_verifier() {\n+        objectField1 = valueField1;\n+        MyAbstract result = null;\n+        result = test99(0);\n+        Asserts.assertEQ(((MyObject2)result).x, rI);\n+        result = test99(1);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test99(2);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test99(3);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test99(4);\n+        Asserts.assertEQ(((MyValue1)result).hash(), hash());\n+        result = test99(5);\n+        Asserts.assertEQ(result, null);\n+    }\n+\n+    \/\/ Test merging inline types and abstract classes in loops\n+    @Test\n+    public MyAbstract test100(int iters) {\n+        MyAbstract res = new MyObject2(rI);\n+        for (int i = 0; i < iters; ++i) {\n+            if (res instanceof MyObject2) {\n+                res = MyValue1.createWithFieldsInline(rI, rL);\n+            } else {\n+                res = MyValue1.createWithFieldsInline(((MyValue1)res).x + 1, rL);\n+            }\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test100\")\n+    public void test100_verifier() {\n+        MyObject2 result1 = (MyObject2)test100(0);\n+        Asserts.assertEQ(result1.x, rI);\n+        int iters = (Math.abs(rI) % 10) + 1;\n+        MyValue1 result2 = (MyValue1)test100(iters);\n+        MyValue1 vt = MyValue1.createWithFieldsInline(rI + iters - 1, rL);\n+        Asserts.assertEQ(result2.hash(), vt.hash());\n+    }\n+\n+    \/\/ Test inline types in abstract class variables that are live at safepoint\n+    @Test\n+    @IR(failOn = {ALLOC_OF_MYVALUE_KLASS, STORE_OF_ANY_KLASS, LOOP})\n+    public long test101(MyValue1 arg, boolean deopt, Method m) {\n+        MyAbstract vt1 = MyValue1.createWithFieldsInline(rI, rL);\n+        MyAbstract vt2 = MyValue1.createWithFieldsDontInline(rI, rL);\n+        MyAbstract vt3 = arg;\n+        MyAbstract vt4 = valueField1;\n+        if (deopt) {\n+            \/\/ uncommon trap\n+            TestFramework.deoptimize(m);\n+        }\n+        return ((MyValue1)vt1).hash() + ((MyValue1)vt2).hash() +\n+               ((MyValue1)vt3).hash() + ((MyValue1)vt4).hash();\n+    }\n+\n+    @Run(test = \"test101\")\n+    public void test101_verifier(RunInfo info) {\n+        long result = test101(valueField1, !info.isWarmUp(), info.getTest());\n+        Asserts.assertEQ(result, 4*hash());\n+    }\n+\n+    \/\/ Test comparing inline types with abstract classes\n+    @Test\n+    public boolean test102(Object arg) {\n+        MyAbstract vt = MyValue1.createWithFieldsInline(rI, rL);\n+        if (vt == arg || vt == (MyAbstract)valueField1 || vt == abstractField1 || vt == null ||\n+            arg == vt || (MyAbstract)valueField1 == vt || abstractField1 == vt || null == vt) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    @Run(test = \"test102\")\n+    public void test102_verifier() {\n+        boolean result = test102(null);\n+        Asserts.assertFalse(result);\n+    }\n+\n+    \/\/ An abstract class with a non-static field can never be implemented by an inline type\n+    abstract class NoValueImplementors1 {\n+        int field = 42;\n+    }\n+\n+    class MyObject3 extends NoValueImplementors1 {\n+\n+    }\n+\n+    class MyObject4 extends NoValueImplementors1 {\n+\n+    }\n+\n+    \/\/ Loading from an abstract class array does not require a flatness check if the abstract class has a non-static field\n+    @Test\n+    @IR(failOn = {ALLOC, MEMBAR, LOAD_UNKNOWN_INLINE, STORE_UNKNOWN_INLINE, INLINE_ARRAY_NULL_GUARD})\n+    public NoValueImplementors1 test103(NoValueImplementors1[] array, int i) {\n+        return array[i];\n+    }\n+\n+    @Run(test = \"test103\")\n+    public void test103_verifier() {\n+        NoValueImplementors1[] array1 = new NoValueImplementors1[3];\n+        MyObject3[] array2 = new MyObject3[3];\n+        MyObject4[] array3 = new MyObject4[3];\n+        NoValueImplementors1 result = test103(array1, 0);\n+        Asserts.assertEquals(result, array1[0]);\n+\n+        result = test103(array2, 1);\n+        Asserts.assertEquals(result, array1[1]);\n+\n+        result = test103(array3, 2);\n+        Asserts.assertEquals(result, array1[2]);\n+    }\n+\n+    \/\/ Storing to an abstract class array does not require a flatness\/null check if the abstract class has a non-static field\n+    @Test\n+    @IR(failOn = {ALLOC, LOAD_UNKNOWN_INLINE, STORE_UNKNOWN_INLINE, INLINE_ARRAY_NULL_GUARD})\n+    public NoValueImplementors1 test104(NoValueImplementors1[] array, NoValueImplementors1 v, MyObject3 o, int i) {\n+        array[0] = v;\n+        array[1] = array[0];\n+        array[2] = o;\n+        return array[i];\n+    }\n+\n+    @Run(test = \"test104\")\n+    public void test104_verifier() {\n+        MyObject4 v = new MyObject4();\n+        MyObject3 o = new MyObject3();\n+        NoValueImplementors1[] array1 = new NoValueImplementors1[3];\n+        MyObject3[] array2 = new MyObject3[3];\n+        MyObject4[] array3 = new MyObject4[3];\n+        NoValueImplementors1 result = test104(array1, v, o, 0);\n+        Asserts.assertEquals(array1[0], v);\n+        Asserts.assertEquals(array1[1], v);\n+        Asserts.assertEquals(array1[2], o);\n+        Asserts.assertEquals(result, v);\n+\n+        result = test104(array2, o, o, 1);\n+        Asserts.assertEquals(array2[0], o);\n+        Asserts.assertEquals(array2[1], o);\n+        Asserts.assertEquals(array2[2], o);\n+        Asserts.assertEquals(result, o);\n+\n+        result = test104(array3, v, null, 1);\n+        Asserts.assertEquals(array3[0], v);\n+        Asserts.assertEquals(array3[1], v);\n+        Asserts.assertEquals(array3[2], null);\n+        Asserts.assertEquals(result, v);\n+    }\n+\n+    \/\/ An abstract class with a single, non-inline implementor\n+    abstract class NoValueImplementors2 {\n+\n+    }\n+\n+    class MyObject5 extends NoValueImplementors2 {\n+\n+    }\n+\n+    \/\/ Loading from an abstract class array does not require a flatness check if the abstract class has no inline implementor\n+    @Test\n+    @IR(failOn = {ALLOC, MEMBAR, LOAD_UNKNOWN_INLINE, STORE_UNKNOWN_INLINE, INLINE_ARRAY_NULL_GUARD})\n+    public NoValueImplementors2 test105(NoValueImplementors2[] array, int i) {\n+        return array[i];\n+    }\n+\n+    @Run(test = \"test105\")\n+    public void test105_verifier() {\n+        NoValueImplementors2[] array1 = new NoValueImplementors2[3];\n+        MyObject5[] array2 = new MyObject5[3];\n+        NoValueImplementors2 result = test105(array1, 0);\n+        Asserts.assertEquals(result, array1[0]);\n+\n+        result = test105(array2, 1);\n+        Asserts.assertEquals(result, array1[1]);\n+    }\n+\n+    \/\/ Storing to an abstract class array does not require a flatness\/null check if the abstract class has no inline implementor\n+    @Test\n+    @IR(failOn = {ALLOC, LOAD_UNKNOWN_INLINE, STORE_UNKNOWN_INLINE, INLINE_ARRAY_NULL_GUARD})\n+    public NoValueImplementors2 test106(NoValueImplementors2[] array, NoValueImplementors2 v, MyObject5 o, int i) {\n+        array[0] = v;\n+        array[1] = array[0];\n+        array[2] = o;\n+        return array[i];\n+    }\n+\n+    @Run(test = \"test106\")\n+    public void test106_verifier() {\n+        MyObject5 v = new MyObject5();\n+        NoValueImplementors2[] array1 = new NoValueImplementors2[3];\n+        MyObject5[] array2 = new MyObject5[3];\n+        NoValueImplementors2 result = test106(array1, v, null, 0);\n+        Asserts.assertEquals(array1[0], v);\n+        Asserts.assertEquals(array1[1], v);\n+        Asserts.assertEquals(array1[2], null);\n+        Asserts.assertEquals(result, v);\n+\n+        result = test106(array2, v, v, 1);\n+        Asserts.assertEquals(array2[0], v);\n+        Asserts.assertEquals(array2[1], v);\n+        Asserts.assertEquals(array2[2], v);\n+        Asserts.assertEquals(result, v);\n+    }\n+\n+    \/\/ More tests for the Loop Unswitching optimization (similar to test84 and following)\n+    Object oFld1, oFld2;\n+\n+    @Test\n+    @IR(applyIf = {\"UseArrayFlattening\", \"true\"},\n+        failOn = {STORE_UNKNOWN_INLINE, INLINE_ARRAY_NULL_GUARD},\n+        counts = {COUNTED_LOOP, \"= 2\", LOAD_UNKNOWN_INLINE, \"= 2\"},\n+         \/\/ Match on CCP since we are removing one of the unswitched loop versions later due to being empty\n+        phase = {CompilePhase.CCP1})\n+    public void test107(Object[] src1, Object[] src2) {\n+        for (int i = 0; i < src1.length; i++) {\n+            oFld1 = src1[i];\n+            oFld2 = src2[i];\n+        }\n+    }\n+\n+    @Run(test = \"test107\")\n+    @Warmup(0)\n+    public void test107_verifier(RunInfo info) {\n+        MyValue2[] src1 = (MyValue2[])ValueClass.newNullRestrictedNonAtomicArray(MyValue2.class, 100, MyValue2.DEFAULT);\n+        Arrays.fill(src1, testValue2);\n+        Object[] src2 = new Object[100];\n+        Object obj = new Object();\n+        Arrays.fill(src2, obj);\n+        rerun_and_recompile_for(info.getTest(), 10,\n+                                () -> { test107(src1, src2);\n+                                        Asserts.assertEquals(oFld1, testValue2);\n+                                        Asserts.assertEquals(oFld2, obj);\n+                                        test107(src2, src1);\n+                                        Asserts.assertEquals(oFld1, obj);\n+                                        Asserts.assertEquals(oFld2, testValue2);  });\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseArrayFlattening\", \"true\"},\n+        failOn = {LOAD_UNKNOWN_INLINE, INLINE_ARRAY_NULL_GUARD},\n+        counts = {COUNTED_LOOP, \"= 4\", STORE_UNKNOWN_INLINE, \"= 9\"})\n+    public void test108(Object[] dst1, Object[] dst2, Object o1, Object o2) {\n+        for (int i = 0; i < dst1.length; i++) {\n+            dst1[i] = o1;\n+            dst2[i] = o2;\n+        }\n+    }\n+\n+    @Run(test = \"test108\")\n+    @Warmup(0)\n+    public void test108_verifier(RunInfo info) {\n+        MyValue2[] dst1 = (MyValue2[])ValueClass.newNullRestrictedNonAtomicArray(MyValue2.class, 100, MyValue2.DEFAULT);\n+        Object[] dst2 = new Object[100];\n+        Object o1 = new Object();\n+        rerun_and_recompile_for(info.getTest(), 10,\n+                                () -> { test108(dst1, dst2, testValue2, o1);\n+                                        for (int i = 0; i < dst1.length; i++) {\n+                                            Asserts.assertEquals(dst1[i], testValue2);\n+                                            Asserts.assertEquals(dst2[i], o1);\n+                                        }\n+                                        test108(dst2, dst1, o1, testValue2);\n+                                        for (int i = 0; i < dst1.length; i++) {\n+                                            Asserts.assertEquals(dst1[i], testValue2);\n+                                            Asserts.assertEquals(dst2[i], o1);\n+                                        } });\n+    }\n+\n+    \/\/ Escape analysis tests\n+\n+    static interface WrapperInterface {\n+        long value();\n+\n+        final static WrapperInterface ZERO = new LongWrapper(0);\n+\n+        @ForceInline\n+        static WrapperInterface wrap(long val) {\n+            return (val == 0L) ? ZERO : new LongWrapper(val);\n+        }\n+    }\n+\n+    @ForceCompileClassInitializer\n+    @LooselyConsistentValue\n+    static value class LongWrapper implements WrapperInterface {\n+        @Strict\n+        @NullRestricted\n+        final static LongWrapper ZERO = new LongWrapper(0);\n+        private long val;\n+\n+        @ForceInline\n+        LongWrapper(long val) {\n+            this.val = val;\n+        }\n+\n+        @ForceInline\n+        static LongWrapper wrap(long val) {\n+            return (val == 0L) ? ZERO : new LongWrapper(val);\n+        }\n+\n+        @ForceInline\n+        public long value() {\n+            return val;\n+        }\n+    }\n+\n+    static class InterfaceBox {\n+        WrapperInterface content;\n+\n+        @ForceInline\n+        InterfaceBox(WrapperInterface content) {\n+            this.content = content;\n+        }\n+\n+        @ForceInline\n+        static InterfaceBox box_sharp(long val) {\n+            return new InterfaceBox(LongWrapper.wrap(val));\n+        }\n+\n+        @ForceInline\n+        static InterfaceBox box(long val) {\n+            return new InterfaceBox(WrapperInterface.wrap(val));\n+        }\n+    }\n+\n+    static class ObjectBox {\n+        Object content;\n+\n+        @ForceInline\n+        ObjectBox(Object content) {\n+            this.content = content;\n+        }\n+\n+        @ForceInline\n+        static ObjectBox box_sharp(long val) {\n+            return new ObjectBox(LongWrapper.wrap(val));\n+        }\n+\n+        @ForceInline\n+        static ObjectBox box(long val) {\n+            return new ObjectBox(WrapperInterface.wrap(val));\n+        }\n+    }\n+\n+    static class RefBox {\n+        LongWrapper content;\n+\n+        @ForceInline\n+        RefBox(LongWrapper content) {\n+            this.content = content;\n+        }\n+\n+        @ForceInline\n+        static RefBox box_sharp(long val) {\n+            return new RefBox(LongWrapper.wrap(val));\n+        }\n+\n+        @ForceInline\n+        static RefBox box(long val) {\n+            return new RefBox((LongWrapper)WrapperInterface.wrap(val));\n+        }\n+    }\n+\n+    static class InlineBox {\n+        @Strict\n+        @NullRestricted\n+        LongWrapper content;\n+\n+        @ForceInline\n+        InlineBox(long val) {\n+            this.content = LongWrapper.wrap(val);\n+        }\n+\n+        @ForceInline\n+        static InlineBox box(long val) {\n+            return new InlineBox(val);\n+        }\n+    }\n+\n+    static class GenericBox<T> {\n+        T content;\n+\n+        @ForceInline\n+        static GenericBox<LongWrapper> box_sharp(long val) {\n+            GenericBox<LongWrapper> res = new GenericBox<>();\n+            res.content = LongWrapper.wrap(val);\n+            return res;\n+        }\n+\n+        @ForceInline\n+        static GenericBox<WrapperInterface> box(long val) {\n+            GenericBox<WrapperInterface> res = new GenericBox<>();\n+            res.content = WrapperInterface.wrap(val);\n+            return res;\n+        }\n+    }\n+\n+    long[] lArr = {0L, rL, 0L, rL, 0L, rL, 0L, rL, 0L, rL};\n+\n+    \/\/ Test removal of allocations when inline type instance is wrapped into box object\n+    @Test\n+    @IR(failOn = {ALLOC, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    public long test109() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += InterfaceBox.box(lArr[i]).content.value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test109\")\n+    @Warmup(10000) \/\/ Make sure interface calls are inlined\n+    public void test109_verifier() {\n+        long res = test109();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    @IR(failOn = {ALLOC, MEMBAR})\n+    public long test109_sharp() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += InterfaceBox.box_sharp(lArr[i]).content.value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test109_sharp\")\n+    @Warmup(10000) \/\/ Make sure interface calls are inlined\n+    public void test109_sharp_verifier() {\n+        long res = test109_sharp();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    \/\/ Same as test109 but with ObjectBox\n+    @Test\n+    @IR(failOn = {ALLOC, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    public long test110() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += ((WrapperInterface)ObjectBox.box(lArr[i]).content).value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test110\")\n+    @Warmup(10000) \/\/ Make sure interface calls are inlined\n+    public void test110_verifier() {\n+        long res = test110();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    @IR(failOn = {ALLOC, MEMBAR})\n+    public long test110_sharp() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += ((WrapperInterface)ObjectBox.box_sharp(lArr[i]).content).value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test110_sharp\")\n+    @Warmup(10000) \/\/ Make sure interface calls are inlined\n+    public void test110_sharp_verifier() {\n+        long res = test110_sharp();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    \/\/ Same as test109 but with RefBox\n+    @Test\n+    @IR(failOn = {ALLOC, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    public long test111() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += RefBox.box(lArr[i]).content.value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test111\")\n+    public void test111_verifier() {\n+        long res = test111();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    public long test111_sharp() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += RefBox.box_sharp(lArr[i]).content.value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test111_sharp\")\n+    public void test111_sharp_verifier() {\n+        long res = test111_sharp();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    \/\/ Same as test109 but with InlineBox\n+    @Test\n+    @IR(failOn = {ALLOC, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    public long test112() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += InlineBox.box(lArr[i]).content.value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test112\")\n+    public void test112_verifier() {\n+        long res = test112();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    \/\/ Same as test109 but with GenericBox\n+    @Test\n+    @IR(failOn = {ALLOC, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    public long test113() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += GenericBox.box(lArr[i]).content.value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test113\")\n+    @Warmup(10000) \/\/ Make sure interface calls are inlined\n+    public void test113_verifier() {\n+        long res = test113();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    public long test113_sharp() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += GenericBox.box_sharp(lArr[i]).content.value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test113_sharp\")\n+    @Warmup(10000) \/\/ Make sure interface calls are inlined\n+    public void test113_sharp_verifier() {\n+        long res = test113_sharp();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    static interface WrapperInterface2 {\n+        public long value();\n+\n+        static final InlineWrapper ZERO = new InlineWrapper(0);\n+\n+        @ForceInline\n+        public static WrapperInterface2 wrap(long val) {\n+            return (val == 0) ? ZERO.content : new LongWrapper2(val);\n+        }\n+\n+        @ForceInline\n+        public static WrapperInterface2 wrap_dynamic(long val) {\n+            return (val == 0) ? new LongWrapper2(0) : new LongWrapper2(val);\n+        }\n+    }\n+\n+    @LooselyConsistentValue\n+    static value class LongWrapper2 implements WrapperInterface2 {\n+        private long val;\n+\n+        @ForceInline\n+        public LongWrapper2(long val) {\n+            this.val = val;\n+        }\n+\n+        @ForceInline\n+        public long value() {\n+            return val;\n+        }\n+    }\n+\n+    @LooselyConsistentValue\n+    static value class InlineWrapper {\n+        WrapperInterface2 content;\n+\n+        @ForceInline\n+        public InlineWrapper(long val) {\n+            content = new LongWrapper2(val);\n+        }\n+    }\n+\n+    static class InterfaceBox2 {\n+        WrapperInterface2 content;\n+\n+        @ForceInline\n+        public InterfaceBox2(long val, boolean def) {\n+            this.content = def ? WrapperInterface2.wrap_dynamic(val) : WrapperInterface2.wrap(val);\n+        }\n+\n+        @ForceInline\n+        static InterfaceBox2 box(long val) {\n+            return new InterfaceBox2(val, false);\n+        }\n+\n+        @ForceInline\n+        static InterfaceBox2 box_dynamic(long val) {\n+            return new InterfaceBox2(val, true);\n+        }\n+    }\n+\n+    \/\/ Same as tests above but with ZERO hidden in field of another inline type\n+    @Test\n+    @IR(failOn = {ALLOC, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    public long test114() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += InterfaceBox2.box(lArr[i]).content.value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test114\")\n+    @Warmup(10000)\n+    public void test114_verifier() {\n+        long res = test114();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    \/\/ Same as test114 but with dynamic instead of constant ZERO field\n+    @Test\n+    @IR(failOn = {ALLOC, MEMBAR},\n+        counts = {PREDICATE_TRAP, \"= 1\"})\n+    public long test115() {\n+        long res = 0;\n+        for (int i = 0; i < lArr.length; i++) {\n+            res += InterfaceBox2.box_dynamic(lArr[i]).content.value();\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"test115\")\n+    @Warmup(10000)\n+    public void test115_verifier() {\n+        long res = test115();\n+        Asserts.assertEquals(res, 5*rL);\n+    }\n+\n+    @Strict\n+    @NullRestricted\n+    static MyValueEmpty fEmpty1 = new MyValueEmpty();\n+    static MyValueEmpty fEmpty2 = new MyValueEmpty();\n+    @Strict\n+    @NullRestricted\n+           MyValueEmpty fEmpty3 = new MyValueEmpty();\n+           MyValueEmpty fEmpty4 = new MyValueEmpty();\n+\n+    \/\/ Test fields loads\/stores with empty inline types\n+    @Test\n+    public void test116() {\n+        fEmpty1 = fEmpty4;\n+        fEmpty2 = fEmpty1;\n+        fEmpty3 = fEmpty2;\n+        fEmpty4 = fEmpty3;\n+    }\n+\n+    @Run(test = \"test116\")\n+    public void test116_verifier() {\n+        test116();\n+        Asserts.assertEquals(fEmpty1, fEmpty2);\n+        Asserts.assertEquals(fEmpty2, fEmpty3);\n+        Asserts.assertEquals(fEmpty3, fEmpty4);\n+    }\n+\n+    \/\/ Test array loads\/stores with empty inline types\n+    @Test\n+    public MyValueEmpty test117(MyValueEmpty[] arr1, MyValueEmpty[] arr2) {\n+        arr1[0] = arr2[0];\n+        arr2[0] = new MyValueEmpty();\n+        return arr1[0];\n+    }\n+\n+    @Run(test = \"test117\")\n+    public void test117_verifier() {\n+        MyValueEmpty[] arr1 = new MyValueEmpty[] { new MyValueEmpty() };\n+        MyValueEmpty res = test117(arr1, arr1);\n+        Asserts.assertEquals(res, new MyValueEmpty());\n+        Asserts.assertEquals(arr1[0], new MyValueEmpty());\n+    }\n+\n+    \/\/ Test acmp with empty inline types\n+    @Test\n+    public boolean test118(MyValueEmpty v1, MyValueEmpty v2, Object o1) {\n+        return (v1 == v2) && (v2 == o1);\n+    }\n+\n+    @Run(test = \"test118\")\n+    public void test118_verifier() {\n+        boolean res = test118(new MyValueEmpty(), new MyValueEmpty(), new MyValueEmpty());\n+        Asserts.assertTrue(res);\n+    }\n+\n+    @LooselyConsistentValue\n+    static value class EmptyContainer {\n+        @Strict\n+        @NullRestricted\n+        private MyValueEmpty empty = new MyValueEmpty();\n+    }\n+\n+    @LooselyConsistentValue\n+    static value class MixedContainer {\n+        public int val = 0;\n+        @Strict\n+        @NullRestricted\n+        private EmptyContainer empty = new EmptyContainer();\n+    }\n+\n+    @Strict\n+    @NullRestricted\n+    static final MyValueEmpty empty = new MyValueEmpty();\n+\n+    @Strict\n+    @NullRestricted\n+    static final EmptyContainer emptyC = new EmptyContainer();\n+\n+    @Strict\n+    @NullRestricted\n+    static final MixedContainer mixedContainer = new MixedContainer();\n+\n+    \/\/ Test re-allocation of empty inline type array during deoptimization\n+    @Test\n+    public void test119(boolean deopt, Method m) {\n+        MyValueEmpty[]   array1 = new MyValueEmpty[] { empty, null };\n+        EmptyContainer[] array2 = (EmptyContainer[])ValueClass.newNullRestrictedNonAtomicArray(EmptyContainer.class, 1, emptyC);\n+        array2[0] = emptyC;\n+        MixedContainer[] array3 = (MixedContainer[])ValueClass.newNullRestrictedNonAtomicArray(MixedContainer.class, 1, mixedContainer);\n+        array3[0] = mixedContainer;\n+        if (deopt) {\n+            \/\/ uncommon trap\n+            TestFramework.deoptimize(m);\n+        }\n+        Asserts.assertEquals(array1[0], empty);\n+        Asserts.assertEquals(array1[1], null);\n+        Asserts.assertEquals(array2[0], emptyC);\n+        Asserts.assertEquals(array3[0], mixedContainer);\n+    }\n+\n+    @Run(test = \"test119\")\n+    public void test119_verifier(RunInfo info) {\n+        test119(!info.isWarmUp(), info.getTest());\n+    }\n+\n+    \/\/ Test optimization of empty inline type field stores\n+    @Test\n+    @IR(failOn = {ALLOC, LOAD_OF_ANY_KLASS, STORE_OF_ANY_KLASS, NULL_CHECK_TRAP, UNSTABLE_IF_TRAP, PREDICATE_TRAP})\n+    public void test120() {\n+        fEmpty1 = empty;\n+        fEmpty3 = empty;\n+        \/\/ fEmpty2 and fEmpty4 could be null, store can't be removed\n+    }\n+\n+    @Run(test = \"test120\")\n+    public void test120_verifier() {\n+        test120();\n+        Asserts.assertEquals(fEmpty1, empty);\n+    }\n+\n+    \/\/ Test removal of empty inline type field loads\n+    @Test\n+    @IR(failOn = {LOAD_OF_ANY_KLASS, STORE_OF_ANY_KLASS, FIELD_ACCESS, NULL_CHECK_TRAP, UNSTABLE_IF_TRAP, PREDICATE_TRAP})\n+    public boolean test121() {\n+        return fEmpty1.equals(fEmpty3);\n+        \/\/ fEmpty2 and fEmpty4 could be null, load can't be removed\n+    }\n+\n+    @Run(test = \"test121\")\n+    public void test121_verifier() {\n+        boolean res = test121();\n+        Asserts.assertTrue(res);\n+    }\n+\n+    \/\/ Verify that empty inline type field loads check for null holder\n+    @Test\n+    @IR(applyIf = {\"InlineTypeReturnedAsFields\", \"true\"},\n+        failOn = {ALLOC})\n+    public MyValueEmpty test122(TestLWorld t) {\n+        return t.fEmpty3;\n+    }\n+\n+    @Run(test = \"test122\")\n+    public void test122_verifier() {\n+        MyValueEmpty res = test122(this);\n+        Asserts.assertEquals(res, new MyValueEmpty());\n+        try {\n+            test122(null);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Verify that empty inline type field stores check for null holder\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public void test123(TestLWorld t) {\n+        t.fEmpty3 = new MyValueEmpty();\n+    }\n+\n+    @Run(test = \"test123\")\n+    public void test123_verifier() {\n+        test123(this);\n+        Asserts.assertEquals(fEmpty3, new MyValueEmpty());\n+        try {\n+            test123(null);\n+            throw new RuntimeException(\"No NPE thrown\");\n+        } catch (NullPointerException e) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ acmp doesn't need substitutability test when one input is known\n+    \/\/ not to be a value type\n+    @Test\n+    @IR(failOn = SUBSTITUTABILITY_TEST)\n+    public boolean test124(NonValueClass o1, Object o2) {\n+        return o1 == o2;\n+    }\n+\n+    @Run(test = \"test124\")\n+    public void test124_verifier() {\n+        NonValueClass obj = new NonValueClass(rI);\n+        test124(obj, obj);\n+        test124(obj, testValue1);\n+    }\n+\n+    \/\/ acmp doesn't need substitutability test when one input is null\n+    @Test\n+    @IR(failOn = {SUBSTITUTABILITY_TEST})\n+    public boolean test125(Object o1) {\n+        Object o2 = null;\n+        return o1 == o2;\n+    }\n+\n+    @Run(test = \"test125\")\n+    public void test125_verifier() {\n+        test125(testValue1);\n+        test125(null);\n+    }\n+\n+    \/\/ Test inline type that can only be scalarized after loop opts\n+    @Test\n+    @IR(failOn = {ALLOC, LOAD_OF_ANY_KLASS, STORE_OF_ANY_KLASS})\n+    public long test126(boolean trap) {\n+        MyValue2 nonNull = MyValue2.createWithFieldsInline(rI, rD);\n+        MyValue2 val = null;\n+\n+        for (int i = 0; i < 4; i++) {\n+            if ((i % 2) == 0) {\n+                val = nonNull;\n+            }\n+        }\n+        \/\/ 'val' is always non-null here but that's only known after loop opts\n+        if (trap) {\n+            \/\/ Uncommon trap with an inline input that can only be scalarized after loop opts\n+            return val.hash();\n+        }\n+        return 0;\n+    }\n+\n+    @Run(test = \"test126\")\n+    @Warmup(10000)\n+    public void test126_verifier(RunInfo info) {\n+        long res = test126(false);\n+        Asserts.assertEquals(res, 0L);\n+        if (!info.isWarmUp()) {\n+            res = test126(true);\n+            Asserts.assertEquals(res, testValue2.hash());\n+        }\n+    }\n+\n+    \/\/ Same as test126 but with interface type\n+    @Test\n+    @IR(failOn = {ALLOC, LOAD_OF_ANY_KLASS, STORE_OF_ANY_KLASS})\n+    public long test127(boolean trap) {\n+        MyValue2 nonNull = MyValue2.createWithFieldsInline(rI, rD);\n+        MyInterface val = null;\n+\n+        for (int i = 0; i < 4; i++) {\n+            if ((i % 2) == 0) {\n+                val = nonNull;\n+            }\n+        }\n+        \/\/ 'val' is always non-null here but that's only known after loop opts\n+        if (trap) {\n+            \/\/ Uncommon trap with an inline input that can only be scalarized after loop opts\n+            return val.hash();\n+        }\n+        return 0;\n+    }\n+\n+    @Run(test = \"test127\")\n+    @Warmup(10000)\n+    public void test127_verifier(RunInfo info) {\n+        long res = test127(false);\n+        Asserts.assertEquals(res, 0L);\n+        if (!info.isWarmUp()) {\n+            res = test127(true);\n+            Asserts.assertEquals(res, testValue2.hash());\n+        }\n+    }\n+\n+    \/\/ Test inline type that can only be scalarized after CCP\n+    @Test\n+    @IR(failOn = {ALLOC, LOAD_OF_ANY_KLASS, STORE_OF_ANY_KLASS})\n+    public long test128(boolean trap) {\n+        MyValue2 nonNull = MyValue2.createWithFieldsInline(rI, rD);\n+        MyValue2 val = null;\n+\n+        int limit = 2;\n+        for (; limit < 4; limit *= 2);\n+        for (int i = 2; i < limit; i++) {\n+            val = nonNull;\n+        }\n+        \/\/ 'val' is always non-null here but that's only known after CCP\n+        if (trap) {\n+            \/\/ Uncommon trap with an inline input that can only be scalarized after CCP\n+            return val.hash();\n+        }\n+        return 0;\n+    }\n+\n+    @Run(test = \"test128\")\n+    @Warmup(10000)\n+    public void test128_verifier(RunInfo info) {\n+        long res = test128(false);\n+        Asserts.assertEquals(res, 0L);\n+        if (!info.isWarmUp()) {\n+            res = test128(true);\n+            Asserts.assertEquals(res, testValue2.hash());\n+        }\n+    }\n+\n+    \/\/ Same as test128 but with interface type\n+    @Test\n+    @IR(failOn = {ALLOC, LOAD_OF_ANY_KLASS, STORE_OF_ANY_KLASS})\n+    public long test129(boolean trap) {\n+        MyValue2 nonNull = MyValue2.createWithFieldsInline(rI, rD);\n+        MyInterface val = null;\n+\n+        int limit = 2;\n+        for (; limit < 4; limit *= 2);\n+        for (int i = 0; i < limit; i++) {\n+            val = nonNull;\n+        }\n+        \/\/ 'val' is always non-null here but that's only known after CCP\n+        if (trap) {\n+            \/\/ Uncommon trap with an inline input that can only be scalarized after CCP\n+            return val.hash();\n+        }\n+        return 0;\n+    }\n+\n+    @Run(test = \"test129\")\n+    @Warmup(10000)\n+    public void test129_verifier(RunInfo info) {\n+        long res = test129(false);\n+        Asserts.assertEquals(res, 0L);\n+        if (!info.isWarmUp()) {\n+            res = test129(true);\n+            Asserts.assertEquals(res, testValue2.hash());\n+        }\n+    }\n+\n+    \/\/ Lock on inline type (known after inlining)\n+    @ForceInline\n+    public Object test130_inlinee() {\n+        return MyValue1.createWithFieldsInline(rI, rL);\n+    }\n+\n+    @Test\n+    @IR(failOn = {LOAD_OF_ANY_KLASS},\n+        \/\/ LockNode keeps MyValue1 allocation alive up until macro expansion which in turn keeps MyValue2\n+        \/\/ alloc alive. Although the MyValue1 allocation is removed (unused), MyValue2 is expanded first\n+        \/\/ and therefore stays.\n+        counts = {ALLOC_OF_MYVALUE_KLASS, \"<= 1\", STORE_OF_ANY_KLASS, \"<= 1\"})\n+    public void test130() {\n+        Object obj = test130_inlinee();\n+        synchronized (obj) {\n+            throw new RuntimeException(\"test130 failed: synchronization on inline type should not succeed\");\n+        }\n+    }\n+\n+    @Run(test = \"test130\")\n+    public void test130_verifier() {\n+        try {\n+            test130();\n+            throw new RuntimeException(\"test130 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Same as test130 but with field load instead of allocation\n+    @ForceInline\n+    public Object test131_inlinee() {\n+        return testValue1;\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public void test131() {\n+        Object obj = test131_inlinee();\n+        synchronized (obj) {\n+            throw new RuntimeException(\"test131 failed: synchronization on inline type should not succeed\");\n+        }\n+    }\n+\n+    @Run(test = \"test131\")\n+    public void test131_verifier() {\n+        try {\n+            test131();\n+            throw new RuntimeException(\"test131 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test locking on object that is known to be an inline type only after CCP\n+    @Test\n+    @IR(failOn = {ALLOC_OF_MYVALUE_KLASS, LOAD_OF_ANY_KLASS, STORE_OF_ANY_KLASS})\n+    public void test132() {\n+        MyValue2 vt = MyValue2.createWithFieldsInline(rI, rD);\n+        Object obj = new NonValueClass(42);\n+\n+        int limit = 2;\n+        for (; limit < 4; limit *= 2);\n+        for (int i = 2; i < limit; i++) {\n+            obj = vt;\n+        }\n+        synchronized (obj) {\n+            throw new RuntimeException(\"test132 failed: synchronization on inline type should not succeed\");\n+        }\n+    }\n+\n+    @Run(test = \"test132\")\n+    @Warmup(10000)\n+    public void test132_verifier() {\n+        try {\n+            test132();\n+            throw new RuntimeException(\"test132 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test conditional locking on inline type and non-escaping object\n+    @Test\n+    public void test133(boolean b) {\n+        Object obj = b ? new NonValueClass(rI) : MyValue2.createWithFieldsInline(rI, rD);\n+        synchronized (obj) {\n+            if (!b) {\n+                throw new RuntimeException(\"test133 failed: synchronization on inline type should not succeed\");\n+            }\n+        }\n+    }\n+\n+    @Run(test = \"test133\")\n+    public void test133_verifier() {\n+        test133(true);\n+        try {\n+            test133(false);\n+            throw new RuntimeException(\"test133 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Variant with non-scalarized inline type\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public void test134(boolean b) {\n+        Object obj = null;\n+        if (b) {\n+            obj = MyValue2.createWithFieldsInline(rI, rD);\n+        }\n+        synchronized (obj) {\n+\n+        }\n+    }\n+\n+    @Run(test = \"test134\")\n+    public void test134_verifier() {\n+        try {\n+            test134(true);\n+            throw new RuntimeException(\"test134 failed: no exception thrown\");\n+        } catch (IdentityException ex) {\n+            \/\/ Expected\n+        }\n+    }\n+\n+    \/\/ Test that acmp of the same inline object is removed\n+    @Test\n+    @IR(failOn = {ALLOC, LOAD_OF_ANY_KLASS, STORE_OF_ANY_KLASS, NULL_CHECK_TRAP, UNSTABLE_IF_TRAP, PREDICATE_TRAP})\n+    public boolean test135() {\n+        MyValue1 val = MyValue1.createWithFieldsInline(rI, rL);\n+        return val == val;\n+    }\n+\n+    @Run(test = \"test135\")\n+    public void test135_verifier() {\n+        Asserts.assertTrue(test135());\n+    }\n+\n+    \/\/ Same as test135 but with null\n+    @Test\n+    @IR(failOn = {ALLOC, LOAD_OF_ANY_KLASS, STORE_OF_ANY_KLASS, NULL_CHECK_TRAP, UNSTABLE_IF_TRAP, PREDICATE_TRAP})\n+    public boolean test136(boolean b) {\n+        MyValue1 val = MyValue1.createWithFieldsInline(rI, rL);\n+        if (b) {\n+            val = null;\n+        }\n+        return val == val;\n+    }\n+\n+    @Run(test = \"test136\")\n+    public void test136_verifier() {\n+        Asserts.assertTrue(test136(false));\n+        Asserts.assertTrue(test136(true));\n+    }\n+\n+    \/\/ Test that acmp of different inline objects with same content is removed\n+    @Test\n+    \/\/ TODO 8228361\n+    \/\/ @IR(failOn = {ALLOC, LOAD_OF_ANY_KLASS, STORE_OF_ANY_KLASS, NULL_CHECK_TRAP, UNSTABLE_IF_TRAP, PREDICATE_TRAP})\n+    public boolean test137(int i) {\n+        MyValue2 val1 = MyValue2.createWithFieldsInline(i, rD);\n+        MyValue2 val2 = MyValue2.createWithFieldsInline(i, rD);\n+        return val1 == val2;\n+    }\n+\n+    @Run(test = \"test137\")\n+    public void test137_verifier() {\n+        Asserts.assertTrue(test137(rI));\n+    }\n+\n+    \/\/ Same as test137 but with null\n+    @Test\n+    \/\/ TODO 8228361\n+    \/\/ @IR(failOn = {ALLOC, LOAD_OF_ANY_KLASS, STORE_OF_ANY_KLASS, NULL_CHECK_TRAP, UNSTABLE_IF_TRAP, PREDICATE_TRAP})\n+    public boolean test138(int i, boolean b) {\n+        MyValue2 val1 = MyValue2.createWithFieldsInline(i, rD);\n+        MyValue2 val2 = MyValue2.createWithFieldsInline(i, rD);\n+        if (b) {\n+            val1 = null;\n+            val2 = null;\n+        }\n+        return val1 == val2;\n+    }\n+\n+    @Run(test = \"test138\")\n+    public void test138_verifier() {\n+        Asserts.assertTrue(test138(rI, false));\n+        Asserts.assertTrue(test138(rI, true));\n+    }\n+\n+    @LooselyConsistentValue\n+    static value class Test139Value {\n+        Object obj = null;\n+        @Strict\n+        @NullRestricted\n+        MyValueEmpty empty = new MyValueEmpty();\n+    }\n+\n+    @LooselyConsistentValue\n+    static value class Test139Wrapper {\n+        @Strict\n+        @NullRestricted\n+        Test139Value value = new Test139Value();\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"InlineTypeReturnedAsFields\", \"true\"},\n+        failOn = {ALLOC})\n+    @IR(failOn = {LOAD_OF_ANY_KLASS, STORE_OF_ANY_KLASS, UNSTABLE_IF_TRAP, PREDICATE_TRAP})\n+    public MyValueEmpty test139() {\n+        Test139Wrapper w = new Test139Wrapper();\n+        return w.value.empty;\n+    }\n+\n+    @Run(test = \"test139\")\n+    public void test139_verifier() {\n+        MyValueEmpty empty = test139();\n+        Asserts.assertEquals(empty, new MyValueEmpty());\n+    }\n+\n+    \/\/ Test calling a method on a loaded but not linked inline type\n+    @LooselyConsistentValue\n+    value class Test140Value {\n+        int x = 0;\n+\n+        public int get() {\n+            return x;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public int test140() {\n+        Test140Value vt = new Test140Value();\n+        return vt.get();\n+    }\n+\n+    @Run(test = \"test140\")\n+    @Warmup(0)\n+    public void test140_verifier() {\n+        int result = test140();\n+        Asserts.assertEquals(result, 0);\n+    }\n+\n+    \/\/ Test calling a method on a linked but not initialized inline type\n+    @LooselyConsistentValue\n+    value class Test141Value {\n+        int x = 0;\n+\n+        public int get() {\n+            return x;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public int test141() {\n+        Test141Value vt = new Test141Value();\n+        return vt.get();\n+    }\n+\n+    @Run(test = \"test141\")\n+    @Warmup(0)\n+    public void test141_verifier() {\n+        int result = test141();\n+        Asserts.assertEquals(result, 0);\n+    }\n+\n+    \/\/ Test that virtual calls on inline type receivers are properly inlined\n+    @Test\n+    @IR(failOn = {ALLOC, LOAD_OF_ANY_KLASS, STORE_OF_ANY_KLASS})\n+    public long test142() {\n+        MyValue2 nonNull = MyValue2.createWithFieldsInline(rI, rD);\n+        MyInterface val = null;\n+\n+        for (int i = 0; i < 4; i++) {\n+            if ((i % 2) == 0) {\n+                val = nonNull;\n+            }\n+        }\n+        return val.hash();\n+    }\n+\n+    @Run(test = \"test142\")\n+    public void test142_verifier() {\n+        long res = test142();\n+        Asserts.assertEquals(res, testValue2.hash());\n+    }\n+\n+    \/\/ Test merging of buffered inline types\n+    @Test\n+    public Object test144(int i) {\n+        if (i == 0) {\n+            return MyValue1.createDefaultInline();\n+        } else if (i == 1) {\n+            return testValue1;\n+        } else {\n+            return MyValue1.createDefaultInline();\n+        }\n+    }\n+\n+    @Run(test = \"test144\")\n+    public void test144_verifier() {\n+        Asserts.assertEquals(test144(0), MyValue1.createDefaultInline());\n+        Asserts.assertEquals(test144(1), testValue1);\n+        Asserts.assertEquals(test144(2), MyValue1.createDefaultInline());\n+    }\n+\n+    \/\/ Tests writing an array element with a (statically known) incompatible type\n+    private static final MethodHandle setArrayElementIncompatibleRef = InstructionHelper.buildMethodHandle(MethodHandles.lookup(),\n+        \"setArrayElementIncompatibleRef\",\n+        MethodType.methodType(void.class, TestLWorld.class, MyValue1[].class, int.class, MyValue2.class),\n+        CODE -> {\n+            CODE.\n+            aload(1).\n+            iload(2).\n+            aload(3).\n+            aastore().\n+            return_();\n+        });\n+\n+    \/\/ Test inline type connected to result node\n+    @Test\n+    @IR(failOn = {ALLOC})\n+    public MyValue1 test146(Object obj) {\n+        return (MyValue1)obj;\n+    }\n+\n+    @Run(test = \"test146\")\n+    @Warmup(10000)\n+    public void test146_verifier() {\n+        Asserts.assertEQ(test146(testValue1), testValue1);\n+    }\n+\n+    @ForceInline\n+    public Object test148_helper(Object obj) {\n+        return (MyValue1)obj;\n+    }\n+\n+    \/\/ Same as test146 but with helper method\n+    @Test\n+    public Object test148(Object obj) {\n+        return test148_helper(obj);\n+    }\n+\n+    @Run(test = \"test148\")\n+    @Warmup(10000)\n+    public void test148_verifier() {\n+        Asserts.assertEQ(test148(testValue1), testValue1);\n+    }\n+\n+    @ForceInline\n+    public Object test149_helper(Object obj) {\n+        return (MyValue1)obj;\n+    }\n+\n+    \/\/ Same as test147 but with helper method\n+    @Test\n+    public Object test149(Object obj) {\n+        return test149_helper(obj);\n+    }\n+\n+    @Run(test = \"test149\")\n+    @Warmup(10000)\n+    public void test149_verifier() {\n+        Asserts.assertEQ(test149(testValue1), testValue1);\n+        Asserts.assertEQ(test149(null), null);\n+    }\n+\n+    \/\/ Test post-parse call devirtualization with inline type receiver\n+    @Test\n+    @IR(applyIf = {\"InlineTypePassFieldsAsArgs\", \"true\"},\n+        failOn = {ALLOC_OF_MYVALUE_KLASS})\n+    @IR(failOn = {compiler.lib.ir_framework.IRNode.DYNAMIC_CALL_OF_METHOD, \"MyValue2::hash\"},\n+        counts = {compiler.lib.ir_framework.IRNode.STATIC_CALL_OF_METHOD, \"MyValue2::hash\", \"= 1\"})\n+    public long test150() {\n+        MyValue2 val = MyValue2.createWithFieldsInline(rI, rD);\n+        MyInterface receiver = MyValue1.createWithFieldsInline(rI, rL);\n+\n+        for (int i = 0; i < 4; i++) {\n+            if ((i % 2) == 0) {\n+                receiver = val;\n+            }\n+        }\n+        \/\/ Trigger post parse call devirtualization (strength-reducing\n+        \/\/ virtual calls to direct calls).\n+        return receiver.hash();\n+    }\n+\n+    @Run(test = \"test150\")\n+    public void test150_verifier() {\n+        Asserts.assertEquals(test150(), testValue2.hash());\n+    }\n+\n+    \/\/ Same as test150 but with a real loop and val not being allocated in the scope of the method\n+    @Test\n+    \/\/ Dynamic call does not null check the receiver, so it cannot be strength reduced to a static\n+    \/\/ call without an explicit null check\n+    @IR(failOn = {compiler.lib.ir_framework.IRNode.DYNAMIC_CALL_OF_METHOD, \"MyValue2::hash\"},\n+        counts = {compiler.lib.ir_framework.IRNode.STATIC_CALL_OF_METHOD, \"MyValue2::hash\", \"= 1\"})\n+    public long test151(MyValue2 val) {\n+        val = Objects.requireNonNull(val);\n+        MyAbstract receiver = MyValue1.createWithFieldsInline(rI, rL);\n+\n+        for (int i = 0; i < 100; i++) {\n+            if ((i % 2) == 0) {\n+                receiver = val;\n+            }\n+        }\n+        \/\/ Trigger post parse call devirtualization (strength-reducing\n+        \/\/ virtual calls to direct calls).\n+        return receiver.hash();\n+    }\n+\n+    @Run(test = \"test151\")\n+    @Warmup(0) \/\/ Make sure there is no receiver type profile\n+    public void test151_verifier() {\n+        Asserts.assertEquals(test151(testValue2), testValue2.hash());\n+    }\n+\n+    static interface MyInterface2 {\n+        public int val();\n+    }\n+\n+    static abstract value class MyAbstract2 implements MyInterface2 {\n+\n+    }\n+\n+    static class MyClass152 extends MyAbstract2 {\n+        private int val;\n+\n+        @ForceInline\n+        public MyClass152(int val) {\n+            this.val = val;\n+        }\n+\n+        @Override\n+        public int val() {\n+            return val;\n+        }\n+    }\n+\n+    @LooselyConsistentValue\n+    static value class MyValue152 extends MyAbstract2 {\n+        private int unused = 0; \/\/ Make sure sub-offset of val is field non-zero\n+        private int val;\n+\n+        @ForceInline\n+        public MyValue152(int val) {\n+            this.val = val;\n+        }\n+\n+        @Override\n+        public int val() {\n+            return val;\n+        }\n+    }\n+\n+    @LooselyConsistentValue\n+    static value class MyWrapper152 {\n+        private int unused = 0; \/\/ Make sure sub-offset of val field is non-zero\n+        @Strict\n+        @NullRestricted\n+        MyValue152 val;\n+\n+        @ForceInline\n+        public MyWrapper152(MyInterface2 val) {\n+            this.val = (MyValue152)val;\n+        }\n+    }\n+\n+    \/\/ Test that checkcast with speculative type does not break scalarization in return\n+    @Test\n+    public MyWrapper152 test152(MyInterface2 val) {\n+        return new MyWrapper152(val);\n+    }\n+\n+    @Run(test = \"test152\")\n+    @Warmup(10000) \/\/ Make sure profile information is available at cast\n+    public void test152_verifier() {\n+        MyClass152 unused = new MyClass152(rI);\n+        MyValue152 val = new MyValue152(rI);\n+        Asserts.assertEquals(test152(val).val, val);\n+    }\n+\n+    @DontInline\n+    static void test153_helper(MyWrapper152 arg) {\n+\n+    }\n+\n+    \/\/ Test that checkcast with speculative type does not prevent scalarization in args\n+    @Test\n+    public void test153(MyInterface2 val) {\n+        test153_helper(new MyWrapper152(val));\n+    }\n+\n+    @Run(test = \"test153\")\n+    @Warmup(10000) \/\/ Make sure profile information is available at cast\n+    public void test153_verifier() {\n+        MyClass152 unused = new MyClass152(rI);\n+        MyValue152 val = new MyValue152(rI);\n+        test153(val);\n+    }\n+\n+    \/\/ Test that checkcast with speculative type enables scalarization\n+    @Test\n+    @IR(failOn = {ALLOC, STORE_OF_ANY_KLASS})\n+    public int test154(Method m, MyInterface2 val, boolean b1, boolean b2) {\n+        MyInterface2 obj = new MyValue152(rI);\n+        if (b1) {\n+            \/\/ Speculative cast to MyValue152 enables scalarization\n+            obj = (MyAbstract2)val;\n+        }\n+        if (b2) {\n+            \/\/ Uncommon trap\n+            TestFramework.deoptimize(m);\n+            return obj.val();\n+        }\n+        return -1;\n+    }\n+\n+    @Run(test = \"test154\")\n+    @Warmup(10000) \/\/ Make sure profile information is available at cast\n+    public void test154_verifier(RunInfo info) {\n+        MyClass152 unused = new MyClass152(rI);\n+        MyValue152 val = new MyValue152(rI);\n+        Asserts.assertEquals(test154(info.getTest(), val, false, false), -1);\n+        Asserts.assertEquals(test154(info.getTest(), val, true, false), -1);\n+        if (!info.isWarmUp()) {\n+            Asserts.assertEquals(test154(info.getTest(), val, false, true), rI);\n+        }\n+    }\n+\n+    \/\/ Same as test154 but with null val\n+    @Test\n+    @IR(failOn = {ALLOC, STORE_OF_ANY_KLASS})\n+    public int test155(Method m, MyInterface2 val, boolean b1, boolean b2) {\n+        MyInterface2 obj = new MyValue152(rI);\n+        if (b1) {\n+            \/\/ Speculative cast to MyValue152 enables scalarization\n+            obj = (MyAbstract2)val;\n+        }\n+        if (b2) {\n+            \/\/ Uncommon trap\n+            TestFramework.deoptimize(m);\n+            return obj.val();\n+        }\n+        return -1;\n+    }\n+\n+    @Run(test = \"test155\")\n+    @Warmup(10000) \/\/ Make sure profile information is available at cast\n+    public void test155_verifier(RunInfo info) {\n+        MyClass152 unused = new MyClass152(rI);\n+        MyValue152 val = new MyValue152(rI);\n+        Asserts.assertEquals(test155(info.getTest(), val, false, false), -1);\n+        Asserts.assertEquals(test155(info.getTest(), val, true, false), -1);\n+        Asserts.assertEquals(test155(info.getTest(), null, true, false), -1);\n+        if (!info.isWarmUp()) {\n+            Asserts.assertEquals(test155(info.getTest(), val, false, true), rI);\n+        }\n+    }\n+\n+    @Strict\n+    @NullRestricted\n+    final static MyValue1 test157Cache = MyValue1.createWithFieldsInline(rI, 0);\n+\n+    \/\/ Test merging buffered inline type from field load with non-buffered inline type\n+    @Test\n+    public MyValue1 test157(long val) {\n+        return (val == 0L) ? test157Cache : MyValue1.createWithFieldsInline(rI, val);\n+    }\n+\n+    @Run(test = \"test157\")\n+    public void test157_verifier() {\n+        Asserts.assertEquals(test157(0), test157Cache);\n+        Asserts.assertEquals(test157(rL).hash(), testValue1.hash());\n+    }\n+\n+    @Strict\n+    @NullRestricted\n+    static MyValue1 test158Cache = MyValue1.createWithFieldsInline(rI, 0);\n+\n+    \/\/ Same as test157 but with non-final field load\n+    @Test\n+    public MyValue1 test158(long val) {\n+        return (val == 0L) ? test158Cache : MyValue1.createWithFieldsInline(rI, val);\n+    }\n+\n+    @Run(test = \"test158\")\n+    public void test158_verifier() {\n+        Asserts.assertEquals(test158(0), test158Cache);\n+        Asserts.assertEquals(test158(rL).hash(), testValue1.hash());\n+    }\n+\n+    \/\/ Verify that cast that with incompatible types is properly handled\n+    @Test\n+    public void test160(NonValueClass arg) {\n+        Object tmp = arg;\n+        MyValue1 res = (MyValue1)tmp;\n+    }\n+\n+    @Run(test = \"test160\")\n+    @Warmup(10000)\n+    public void test160_verifier(RunInfo info) {\n+        try {\n+            test160(new NonValueClass(42));\n+            throw new RuntimeException(\"No CCE thrown\");\n+        } catch (ClassCastException e) {\n+            \/\/ Expected\n+        }\n+        test160(null);\n+    }\n+\n+    abstract value static class AbstractValueClassSingleSubclass {\n+    }\n+\n+    value static class UniqueValueSubClass extends AbstractValueClassSingleSubclass {\n+        int x = 34;\n+    }\n+\n+    static AbstractValueClassSingleSubclass abstractValueClassSingleSubclass = new UniqueValueSubClass();\n+\n+    @Test\n+    public void testUniqueConcreteValueSubKlass(boolean flag) {\n+        \/\/ C2 should recognize that even though we do not know the exact layout of the underlying inline type of the\n+        \/\/ abstract field abstractValueClassSingleSubclass (i.e. cannot scalarize), we only have a unique concrete sub\n+        \/\/ class from which we know at compile time whether it can be scalarized or not. This unique sub class\n+        \/\/ optimization was missing, resulting in a missing InlineTypeNode assertion failure.\n+        doNothing(abstractValueClassSingleSubclass, flag ? 23 : 34);\n+    }\n+\n+    void doNothing(Object a, int i) {}\n+\n+    @Run(test = \"testUniqueConcreteValueSubKlass\")\n+    public void testUniqueConcreteValueSubKlass_verifier() {\n+        testUniqueConcreteValueSubKlass(true);\n+    }\n+\n+    static value class MyValueContainer {\n+        private final Object value;\n+\n+        private MyValueContainer(Object value) {\n+            this.value = value;\n+        }\n+    }\n+\n+    static value class MyValue161 {\n+        int x = 0;\n+    }\n+\n+    \/\/ Test merging value classes with Object fields\n+    @Test\n+    public MyValueContainer test161(boolean b) {\n+        MyValueContainer res = b ? new MyValueContainer(new MyValue161()) : null;\n+        \/\/ Cast to verify that merged values are of correct type\n+        Object obj = b ? (MyValue161)res.value : null;\n+        return res;\n+    }\n+\n+    @Run(test = \"test161\")\n+    public void test161_verifier() {\n+        Asserts.assertEquals(test161(true), new MyValueContainer(new MyValue161()));\n+        Asserts.assertEquals(test161(false), null);\n+    }\n+\n+    @Test\n+    public MyValueContainer test162(boolean b) {\n+        MyValueContainer res = b ? null : new MyValueContainer(new MyValue161());\n+        \/\/ Cast to verify that merged values are of correct type\n+        Object obj = b ? null : (MyValue161)res.value;\n+        return res;\n+    }\n+\n+    @Run(test = \"test162\")\n+    public void test162_verifier() {\n+        Asserts.assertEquals(test162(true), null);\n+        Asserts.assertEquals(test162(false), new MyValueContainer(new MyValue161()));\n+    }\n+\n+    @Test\n+    public MyValueContainer test163(boolean b) {\n+        MyValueContainer res = b ? new MyValueContainer(new MyValue161()) : new MyValueContainer(null);\n+        \/\/ Cast to verify that merged values are of correct type\n+        Object obj = b ? (MyValue161)res.value : (MyValue161)res.value;\n+        return res;\n+    }\n+\n+    @Run(test = \"test163\")\n+    public void test163_verifier() {\n+        Asserts.assertEquals(test163(true), new MyValueContainer(new MyValue161()));\n+        Asserts.assertEquals(test163(false), new MyValueContainer(null));\n+    }\n+\n+    @Test\n+    public MyValueContainer test164(boolean b) {\n+        MyValueContainer res = b ? new MyValueContainer(null) : new MyValueContainer(new MyValue161());\n+        \/\/ Cast to verify that merged values are of correct type\n+        Object obj = b ? (MyValue161)res.value : (MyValue161)res.value;\n+        return res;\n+    }\n+\n+    @Run(test = \"test164\")\n+    public void test164_verifier() {\n+        Asserts.assertEquals(test164(true), new MyValueContainer(null));\n+        Asserts.assertEquals(test164(false), new MyValueContainer(new MyValue161()));\n+    }\n+\n+    @Test\n+    public MyValueContainer test165(boolean b) {\n+        MyValueContainer res = b ? new MyValueContainer(new MyValue161()) : new MyValueContainer(42);\n+        \/\/ Cast to verify that merged values are of correct type\n+        Object obj = b ? (MyValue161)res.value : (Integer)res.value;\n+        return res;\n+    }\n+\n+    @Run(test = \"test165\")\n+    public void test165_verifier() {\n+        Asserts.assertEquals(test165(true), new MyValueContainer(new MyValue161()));\n+        Asserts.assertEquals(test165(false), new MyValueContainer(42));\n+    }\n+\n+    @Test\n+    public MyValueContainer test166(boolean b) {\n+        MyValueContainer res = b ? new MyValueContainer(42) : new MyValueContainer(new MyValue161());\n+        \/\/ Cast to verify that merged values are of correct type\n+        Object obj = b ? (Integer)res.value : (MyValue161)res.value;\n+        return res;\n+    }\n+\n+    @Run(test = \"test166\")\n+    public void test166_verifier() {\n+        Asserts.assertEquals(test166(true), new MyValueContainer(42));\n+        Asserts.assertEquals(test166(false), new MyValueContainer(new MyValue161()));\n+    }\n+\n+    \/\/ Verify that monitor information in JVMState is correct at method exit\n+    @Test\n+    public synchronized Object test167() {\n+        return MyValue1.createWithFieldsInline(rI, rL); \/\/ Might trigger buffering which requires JVMState\n+    }\n+\n+    @Run(test = \"test167\")\n+    public void test167_verifier() {\n+        Asserts.assertEquals(((MyValue1)test167()).hash(), hash());\n+    }\n+\n+    @LooselyConsistentValue\n+    static value class ValueClassWithInt {\n+        int i;\n+\n+        ValueClassWithInt(int i) {\n+            this.i = i;\n+        }\n+    }\n+\n+    @LooselyConsistentValue\n+    static value class ValueClassWithDouble {\n+        double d;\n+\n+        ValueClassWithDouble(double d) {\n+            this.d = d;\n+        }\n+    }\n+\n+    @LooselyConsistentValue\n+    static abstract value class AbstractValueClassWithByte {\n+        byte b;\n+\n+        AbstractValueClassWithByte(byte b) {\n+            this.b = b;\n+        }\n+    }\n+\n+    @LooselyConsistentValue\n+    static value class SubValueClassWithInt extends AbstractValueClassWithByte {\n+        int i;\n+\n+        SubValueClassWithInt(int i) {\n+            this.i = i;\n+            super((byte)(i + 1));\n+        }\n+    }\n+\n+    @LooselyConsistentValue\n+    static value class SubValueClassWithDouble extends AbstractValueClassWithByte {\n+        double d;\n+\n+        SubValueClassWithDouble(double d) {\n+            this.d = d;\n+            super((byte)(d + 1));\n+        }\n+    }\n+\n+    \/\/ TODO 8350865 We need more copies of these tests for all ValueClass array factories\n+    static final ValueClassWithInt[] VALUE_CLASS_WITH_INT_ARRAY = (ValueClassWithInt[]) ValueClass.newNullRestrictedNonAtomicArray(ValueClassWithInt.class, 2, new ValueClassWithInt(0));\n+    static final ValueClassWithDouble[] VALUE_CLASS_WITH_DOUBLE_ARRAY = (ValueClassWithDouble[]) ValueClass.newNullRestrictedNonAtomicArray(ValueClassWithDouble.class, 2, new ValueClassWithDouble(0));\n+    static final SubValueClassWithInt[] SUB_VALUE_CLASS_WITH_INT_ARRAY = (SubValueClassWithInt[]) ValueClass.newNullRestrictedNonAtomicArray(SubValueClassWithInt.class, 2, new SubValueClassWithInt(0));\n+    static final SubValueClassWithDouble[] SUB_VALUE_CLASS_WITH_DOUBLE_ARRAY = (SubValueClassWithDouble[]) ValueClass.newNullRestrictedNonAtomicArray(SubValueClassWithDouble.class, 2, new SubValueClassWithDouble(0));\n+\n+\/\/ TODO: Can only be enabled once JDK-8343835 is fixed. Otherwise, we hit the mismatched stores assert.\n+\/\/    static {\n+\/\/        VALUE_CLASS_WITH_INT_ARRAY[0] = new ValueClassWithInt(5);\n+\/\/        VALUE_CLASS_WITH_DOUBLE_ARRAY[0] = new ValueClassWithDouble(6);\n+\/\/        SUB_VALUE_CLASS_WITH_INT_ARRAY[0] = new SubValueClassWithInt(7);\n+\/\/        SUB_VALUE_CLASS_WITH_DOUBLE_ARRAY[0] = new SubValueClassWithDouble(8);\n+\/\/    }\n+\n+    @Test\n+    static void testFlatArrayInexactObjectStore(Object o, boolean flag) {\n+        Object[] oArr;\n+        if (flag) {\n+            oArr = VALUE_CLASS_WITH_INT_ARRAY; \/\/ VALUE_CLASS_WITH_INT_ARRAY is statically known to be flat.\n+        } else {\n+            oArr = VALUE_CLASS_WITH_DOUBLE_ARRAY; \/\/ VALUE_CLASS_WITH_DOUBLE_ARRAY is statically known to be flat.\n+        }\n+        \/\/ The type of 'oArr' is inexact here because we merge two arrays. Since both arrays are flat, 'oArr' is also flat:\n+        \/\/     Type: flat:narrowoop: java\/lang\/Object:NotNull * (flat in array)[int:2]\n+        \/\/ Since the type is inexact, we do not know the exact flat array layout statically and thus need to fall back\n+        \/\/ to call \"store_unknown_inline_Type()\" at runtime where we know the flat array layout\n+        oArr[0] = o;\n+    }\n+\n+    @Test\n+    static Object testFlatArrayInexactObjectLoad(boolean flag) {\n+        Object[] oArr;\n+        if (flag) {\n+            oArr = VALUE_CLASS_WITH_INT_ARRAY; \/\/ VALUE_CLASS_WITH_INT_ARRAY is statically known to be flat.\n+        } else {\n+            oArr = VALUE_CLASS_WITH_DOUBLE_ARRAY; \/\/ VALUE_CLASS_WITH_DOUBLE_ARRAY is statically known to be flat.\n+        }\n+        \/\/ The type of 'oArr' is inexact here because we merge two arrays. Since both arrays are flat, 'oArr' is also flat:\n+        \/\/     Type: flat:narrowoop: java\/lang\/Object:NotNull * (flat in array)[int:2]\n+        \/\/ Since the type is inexact, we do not know the exact flat array layout statically and thus need to fall back\n+        \/\/ to call \"load_unknown_inline_Type()\" at runtime where we know the flat array layout\n+        return oArr[0];\n+    }\n+\n+    @Test\n+    static void testFlatArrayInexactAbstractValueClassStore(AbstractValueClassWithByte abstractValueClassWithByte,\n+                                                            boolean flag) {\n+        AbstractValueClassWithByte[] avArr;\n+        if (flag) {\n+            avArr = SUB_VALUE_CLASS_WITH_INT_ARRAY;\n+        } else {\n+            avArr = SUB_VALUE_CLASS_WITH_DOUBLE_ARRAY;\n+        }\n+        \/\/ Same as testFlatArrayInexactObjectStore() but the inexact type is with an abstract value class:\n+        \/\/    flat:narrowoop: compiler\/valhalla\/inlinetypes\/TestLWorld$AbstractValueClassWithByte:NotNull * (flat in array)[int:2]\n+        avArr[0] = abstractValueClassWithByte;\n+    }\n+\n+    @Test\n+    static AbstractValueClassWithByte testFlatArrayInexactAbstractValueClassLoad(boolean flag) {\n+        AbstractValueClassWithByte[] avArr;\n+        if (flag) {\n+            avArr = SUB_VALUE_CLASS_WITH_INT_ARRAY;\n+        } else {\n+            avArr = SUB_VALUE_CLASS_WITH_DOUBLE_ARRAY;\n+        }\n+        \/\/ Same as testFlatArrayInexactObjectLoad() but the inexact type is with an abstract value class:\n+        \/\/    flat:narrowoop: compiler\/valhalla\/inlinetypes\/TestLWorld$AbstractValueClassWithByte:NotNull * (flat in array)[int:2]\n+        return avArr[0];\n+    }\n+\n+    @Run(test = {\"testFlatArrayInexactObjectStore\",\n+                 \"testFlatArrayInexactObjectLoad\",\n+                 \"testFlatArrayInexactAbstractValueClassStore\",\n+                 \"testFlatArrayInexactAbstractValueClassLoad\"})\n+    static void runFlatArrayInexactLoadAndStore() {\n+        \/\/ TODO: Remove these again once JDK-8343835 is fixed and uncomment static initializer above\n+        VALUE_CLASS_WITH_INT_ARRAY[0] = new ValueClassWithInt(5);\n+        VALUE_CLASS_WITH_DOUBLE_ARRAY[0] = new ValueClassWithDouble(6);\n+        SUB_VALUE_CLASS_WITH_INT_ARRAY[0] = new SubValueClassWithInt(7);\n+        SUB_VALUE_CLASS_WITH_DOUBLE_ARRAY[0] = new SubValueClassWithDouble(8);\n+\n+        boolean flag = true;\n+        ValueClassWithInt valueClassWithInt = new ValueClassWithInt(15);\n+        ValueClassWithDouble valueClassWithDouble = new ValueClassWithDouble(16);\n+\n+        testFlatArrayInexactObjectStore(valueClassWithInt, true);\n+        Asserts.assertEQ(valueClassWithInt, VALUE_CLASS_WITH_INT_ARRAY[0]);\n+        testFlatArrayInexactObjectStore(valueClassWithDouble, false);\n+        Asserts.assertEQ(valueClassWithDouble, VALUE_CLASS_WITH_DOUBLE_ARRAY[0]);\n+\n+        Asserts.assertEQ(valueClassWithInt, testFlatArrayInexactObjectLoad(true));\n+        Asserts.assertEQ(valueClassWithDouble, testFlatArrayInexactObjectLoad(false));\n+\n+        SubValueClassWithInt subValueClassWithInt = new SubValueClassWithInt(17);\n+        SubValueClassWithDouble subValueClassWithDouble = new SubValueClassWithDouble(18);\n+\n+        testFlatArrayInexactAbstractValueClassStore(subValueClassWithInt, true);\n+        Asserts.assertEQ(subValueClassWithInt, SUB_VALUE_CLASS_WITH_INT_ARRAY[0]);\n+        testFlatArrayInexactAbstractValueClassStore(subValueClassWithDouble, false);\n+        Asserts.assertEQ(subValueClassWithDouble, SUB_VALUE_CLASS_WITH_DOUBLE_ARRAY[0]);\n+\n+        Asserts.assertEQ(subValueClassWithInt, testFlatArrayInexactAbstractValueClassLoad(true));\n+        Asserts.assertEQ(subValueClassWithDouble, testFlatArrayInexactAbstractValueClassLoad(false));\n+    }\n+\n+    \/\/ Check that comparisons between Java mirrors are optimized to comparisons of the klass\n+    @Test\n+    @IR(failOn = {LOAD_P})\n+    public boolean test168(Object o) {\n+        return o.getClass() == NonValueClass.class;\n+    }\n+\n+    @Run(test = \"test168\")\n+    public void test168_verifier() {\n+        Asserts.assertTrue(test168(new NonValueClass(rI)));\n+        Asserts.assertFalse(test168(new NonValueClass[0]));\n+        Asserts.assertFalse(test168(42));\n+        Asserts.assertFalse(test168(new int[0]));\n+    }\n+\n+    @Test\n+    @IR(failOn = {LOAD_P})\n+    public boolean test169(Object o) {\n+        return o.getClass() == NonValueClass[].class;\n+    }\n+\n+    @Run(test = \"test169\")\n+    public void test169_verifier() {\n+        Asserts.assertFalse(test169(new NonValueClass(rI)));\n+        Asserts.assertTrue(test169(new NonValueClass[0]));\n+        Asserts.assertFalse(test169(42));\n+        Asserts.assertFalse(test169(new int[0]));\n+    }\n+\n+    @Test\n+    @IR(counts = {LOAD_P, \"= 2\"}) \/\/ Can't be optimized because o could be an array\n+    public boolean test170(Object o) {\n+        return o.getClass() == MyValue1[].class;\n+    }\n+\n+    @Run(test = \"test170\")\n+    public void test170_verifier() {\n+        Asserts.assertFalse(test170(new NonValueClass(rI)));\n+        Asserts.assertTrue(test170(new MyValue1[0]));\n+        Asserts.assertTrue(test170(ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 0, MyValue1.DEFAULT)));\n+        Asserts.assertTrue(test170(ValueClass.newNullRestrictedAtomicArray(MyValue1.class, 0, MyValue1.DEFAULT)));\n+        Asserts.assertTrue(test170(ValueClass.newNullableAtomicArray(MyValue1.class, 0)));\n+        Asserts.assertFalse(test170(42));\n+        Asserts.assertFalse(test170(new int[0]));\n+    }\n+\n+    @Test\n+    @IR(counts = {LOAD_P, \"= 4\"}) \/\/ Can't be optimized because o1 and o2 could be arrays\n+    public boolean test171(Object o1, Object o2) {\n+        return o1.getClass() == o2.getClass();\n+    }\n+\n+    @Run(test = \"test171\")\n+    public void test171_verifier() {\n+        Asserts.assertTrue(test171(new NonValueClass(rI), new NonValueClass(rI)));\n+        Asserts.assertTrue(test171(new NonValueClass[0], new NonValueClass[0]));\n+        Asserts.assertTrue(test171(ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 0, MyValue1.DEFAULT), new MyValue1[0]));\n+        Asserts.assertTrue(test171(ValueClass.newNullRestrictedAtomicArray(MyValue1.class, 0, MyValue1.DEFAULT), new MyValue1[0]));\n+        Asserts.assertTrue(test171(ValueClass.newNullableAtomicArray(MyValue1.class, 0), new MyValue1[0]));\n+        Asserts.assertTrue(test171(ValueClass.newNullRestrictedAtomicArray(MyValue1.class, 0, MyValue1.DEFAULT), ValueClass.newNullableAtomicArray(MyValue1.class, 0)));\n+        Asserts.assertFalse(test171(42, new int[0]));\n+        Asserts.assertFalse(test171(new NonValueClass(rI), 42));\n+    }\n+\n+    @Test\n+    @IR(failOn = {LOAD_P})\n+    public boolean test172(NonValueClass o1, Object o2) {\n+        return o1.getClass() == o2.getClass();\n+    }\n+\n+    @Run(test = \"test172\")\n+    public void test172_verifier() {\n+        Asserts.assertTrue(test172(new NonValueClass(rI), new NonValueClass(rI)));\n+        Asserts.assertFalse(test172(new NonValueClass(rI), new NonValueClass[0]));\n+        Asserts.assertFalse(test172(new NonValueClass(rI), new MyValue1[0]));\n+        Asserts.assertFalse(test172(new NonValueClass(rI), 42));\n+    }\n+\n+    @Test\n+    @IR(counts = {LOAD_P, \"= 4\"}) \/\/ Can't be optimized because o1 and o2 could be arrays\n+    public boolean test173(Cloneable o1, Object o2) {\n+        return o1.getClass() == o2.getClass();\n+    }\n+\n+    @Run(test = \"test173\")\n+    public void test173_verifier() {\n+        Asserts.assertTrue(test173(new NonValueClass[0], new NonValueClass[0]));\n+        Asserts.assertTrue(test173(ValueClass.newNullRestrictedNonAtomicArray(MyValue1.class, 0, MyValue1.DEFAULT), new MyValue1[0]));\n+        Asserts.assertTrue(test173(ValueClass.newNullRestrictedAtomicArray(MyValue1.class, 0, MyValue1.DEFAULT), new MyValue1[0]));\n+        Asserts.assertTrue(test173(ValueClass.newNullableAtomicArray(MyValue1.class, 0), new MyValue1[0]));\n+        Asserts.assertTrue(test173(ValueClass.newNullRestrictedAtomicArray(MyValue1.class, 0, MyValue1.DEFAULT), ValueClass.newNullableAtomicArray(MyValue1.class, 0)));\n+        Asserts.assertFalse(test173(new boolean[0], new int[0]));\n+    }\n+}\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestLWorld.java","additions":4778,"deletions":0,"binary":false,"changes":4778,"status":"added"},{"patch":"@@ -777,2 +777,11 @@\n-    @IR(counts = {IRNode.STORE_OF_FIELD, \"myClassEmpty\", \"1\", IRNode.STORE_OF_CLASS, \"GoodCount\", \"1\",\n-                  IRNode.STORE_OF_CLASS, \"\/GoodCount\", \"1\", IRNode.STORE_OF_CLASS, \"MyClassEmpty\", \"0\"},\n+    @IR(counts = {\n+            IRNode.STORE_OF_FIELD, \"myClassEmpty\", \"1\",\n+            IRNode.STORE_OF_CLASS, \"oodCount\", \"0\",\n+            IRNode.STORE_OF_CLASS, \"GoodCount\", \"1\",\n+            IRNode.STORE_OF_CLASS, \"\/GoodCount\", \"1\",\n+            IRNode.STORE_OF_CLASS, \"tests\/GoodCount\", \"1\",\n+            IRNode.STORE_OF_CLASS, \"\/tests\/GoodCount\", \"1\",\n+            IRNode.STORE_OF_CLASS, \"ir_framework\/tests\/GoodCount\", \"1\",\n+            IRNode.STORE_OF_CLASS, \"\/ir_framework\/tests\/GoodCount\", \"0\",\n+            IRNode.STORE_OF_CLASS, \"MyClassEmpty\", \"0\"\n+        },\n","filename":"test\/hotspot\/jtreg\/testlibrary_tests\/ir_framework\/tests\/TestIRMatching.java","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -68,0 +68,4 @@\n+\n+# Valhalla\n+java\/lang\/Thread\/virtual\/stress\/PingPong.java 8314996 macosx-all\n+java\/lang\/Thread\/virtual\/stress\/Skynet.java 8342977 generic-all\n","filename":"test\/jdk\/ProblemList-Virtual.txt","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -533,0 +533,5 @@\n+java\/lang\/ModuleLayer\/LayerControllerTest.java                  8337048 generic-all\n+java\/lang\/ModuleLayer\/BasicLayerTest.java                       8337048 generic-all\n+\n+java\/lang\/Thread\/virtual\/stress\/Skynet.java#default             8342977 generic-all\n+\n@@ -550,0 +555,1 @@\n+com\/sun\/management\/HotSpotDiagnosticMXBean\/DumpThreadsWithEliminatedLock.java 8360599 generic-all\n@@ -574,0 +580,2 @@\n+java\/net\/CookieHandler\/B6644726.java                            8365811 generic-all\n+\n@@ -725,0 +733,4 @@\n+com\/sun\/jdi\/cds\/CDSBreakpointTest.java                          8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSDeleteAllBkptsTest.java                      8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSFieldWatchpoints.java                        8304168 generic-all\n+\n@@ -733,0 +745,1 @@\n+java\/util\/logging\/LoggingDeadlock2.java       8368801 generic-all\n@@ -819,0 +832,1 @@\n+\n@@ -824,0 +838,14 @@\n+\n+############################################################################\n+\n+# valhalla\n+java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessValue.java 8367346 generic-all\n+\n+jdk\/classfile\/AccessFlagsTest.java 8366270 generic-all\n+jdk\/internal\/misc\/Unsafe\/AddressComputationContractTest.java 8368933 generic-all\n+jdk\/jfr\/event\/runtime\/TestClassLoaderStatsEvent.java 8366820 generic-all\n+\n+sun\/tools\/jhsdb\/BasicLauncherTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTestWithActiveProcess.java 8366806 generic-all\n+sun\/tools\/jhsdb\/JShellHeapDumpTest.java 8366806 generic-all\n","filename":"test\/jdk\/ProblemList.txt","additions":28,"deletions":0,"binary":false,"changes":28,"status":"modified"}]}