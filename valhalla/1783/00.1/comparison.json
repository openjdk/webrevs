{"files":[{"patch":"@@ -44,0 +44,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -470,0 +471,5 @@\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    __ store_inline_type_fields_to_buf(nullptr, true);\n+  }\n+\n@@ -1660,1 +1666,1 @@\n-address TemplateInterpreterGenerator::generate_normal_entry(bool synchronized) {\n+address TemplateInterpreterGenerator::generate_normal_entry(bool synchronized, bool object_init) {\n@@ -1787,0 +1793,6 @@\n+  \/\/ Issue a StoreStore barrier on entry to Object_init if the\n+  \/\/ class has strict field fields.  Be lazy, always do it.\n+  if (object_init) {\n+    __ membar(MacroAssembler::StoreStore);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -1539,0 +1539,6 @@\n+  \/\/ Issue a StoreStore barrier on entry to Object_init if the\n+  \/\/ class has strict field fields.  Be lazy, always do it.\n+  if (object_init) {\n+    __ membar(MacroAssembler::StoreStore);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/templateInterpreterGenerator_riscv.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -297,1 +297,1 @@\n-  memset(mem, 0, objArrayOopDesc::object_size(element_count));\n+  memset(mem, 0, refArrayOopDesc::object_size(element_count));\n@@ -303,0 +303,1 @@\n+    assert(!EnableValhalla || Universe::objectArrayKlass()->prototype_header() == markWord::prototype(), \"should be the same\");\n@@ -333,1 +334,1 @@\n-  assert(objArrayOopDesc::object_size(max_elem_count)*HeapWordSize == MIN_GC_REGION_ALIGNMENT,\n+  assert(refArrayOopDesc::object_size(max_elem_count)*HeapWordSize == MIN_GC_REGION_ALIGNMENT,\n@@ -448,1 +449,1 @@\n-  size_t byte_size = objArrayOopDesc::object_size(length) * HeapWordSize;\n+  size_t byte_size = refArrayOopDesc::object_size(length) * HeapWordSize;\n@@ -741,1 +742,1 @@\n-  if (!src_obj->fast_no_hash_check()) {\n+  if (!src_obj->fast_no_hash_check() && (!(EnableValhalla && src_obj->mark().is_inline_type()))) {\n@@ -745,0 +746,2 @@\n+    } else if (EnableValhalla) {\n+      fake_oop->set_mark(src_klass->prototype_header().copy_set_hash(src_hash));\n","filename":"src\/hotspot\/share\/cds\/aotMappedHeapWriter.cpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -80,0 +80,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -504,1 +506,1 @@\n-  soc->do_tag(objArrayOopDesc::base_offset_in_bytes());\n+  soc->do_tag(refArrayOopDesc::base_offset_in_bytes());\n@@ -1374,0 +1376,5 @@\n+  if (CDSConfig::is_valhalla_preview()) {\n+    log_info(cds)(\"Archived java heap is not yet supported with Valhalla preview\");\n+    return;\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/aotMetaspace.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -60,0 +61,3 @@\n+bool CDSConfig::_module_patching_disables_cds = false;\n+bool CDSConfig::_java_base_module_patching_disables_cds = false;\n+\n@@ -146,0 +150,3 @@\n+    if (is_valhalla_preview()) {\n+      tmp.print_raw(\"_valhalla\");\n+    }\n@@ -301,1 +308,1 @@\n-  if (Arguments::is_incompatible_cds_internal_module_property(key)) {\n+  if (Arguments::is_incompatible_cds_internal_module_property(key) && !Arguments::patching_migrated_classes(key, value)) {\n@@ -334,2 +341,1 @@\n-    \"jdk.module.upgrade.path\",\n-    \"jdk.module.patch.0\"\n+    \"jdk.module.upgrade.path\"\n@@ -339,2 +345,1 @@\n-    \"--upgrade-module-path\",\n-    \"--patch-module\"\n+    \"--upgrade-module-path\"\n@@ -363,0 +368,6 @@\n+\n+  if (module_patching_disables_cds()) {\n+    vm_exit_during_initialization(\n+            \"Cannot use the following option when dumping the shared archive\", \"--patch-module\");\n+  }\n+\n@@ -391,0 +402,10 @@\n+\n+  if (module_patching_disables_cds()) {\n+    if (RequireSharedSpaces) {\n+      warning(\"CDS is disabled when the %s option is specified.\", \"--patch-module\");\n+    } else {\n+      log_info(cds)(\"CDS is disabled when the %s option is specified.\", \"--patch-module\");\n+    }\n+    return true;\n+  }\n+\n@@ -625,1 +646,1 @@\n-bool CDSConfig::check_vm_args_consistency(bool patch_mod_javabase, bool mode_flag_cmd_line) {\n+bool CDSConfig::check_vm_args_consistency(bool mode_flag_cmd_line) {\n@@ -700,1 +721,1 @@\n-  if (is_using_archive() && patch_mod_javabase) {\n+  if (is_using_archive() && java_base_module_patching_disables_cds() && module_patching_disables_cds()) {\n@@ -957,0 +978,4 @@\n+  if (is_valhalla_preview()) {\n+    \/\/ Not working yet -- e.g., HeapShared::oop_hash() needs to be implemented for value oops\n+    return false;\n+  }\n","filename":"src\/hotspot\/share\/cds\/cdsConfig.cpp","additions":32,"deletions":7,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -374,0 +374,3 @@\n+      if (oak->is_refined_objArray_klass()) {\n+        oak = ObjArrayKlass::cast(oak->super());\n+      }\n@@ -443,8 +446,10 @@\n-\n-      if (elm->is_instance_klass()) {\n-        assert(InstanceKlass::cast(elm)->array_klasses() == nullptr, \"must be\");\n-        InstanceKlass::cast(elm)->set_array_klasses(oak);\n-      } else {\n-        assert(elm->is_array_klass(), \"sanity\");\n-        assert(ArrayKlass::cast(elm)->higher_dimension() == nullptr, \"must be\");\n-        ArrayKlass::cast(elm)->set_higher_dimension(oak);\n+      \/\/ Higher dimension may have been set when doing setup on ObjArrayKlass\n+      if (!oak->is_refined_objArray_klass()) {\n+        if (elm->is_instance_klass()) {\n+          assert(InstanceKlass::cast(elm)->array_klasses() == nullptr, \"must be\");\n+          InstanceKlass::cast(elm)->set_array_klasses(oak);\n+        } else {\n+          assert(elm->is_array_klass(), \"sanity\");\n+          assert(ArrayKlass::cast(elm)->higher_dimension() == nullptr, \"must be\");\n+          ArrayKlass::cast(elm)->set_higher_dimension(oak);\n+        }\n","filename":"src\/hotspot\/share\/cds\/dynamicArchive.cpp","additions":13,"deletions":8,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -90,0 +90,65 @@\n+inline void CDSMustMatchFlags::do_print(outputStream* st, bool v) {\n+  st->print(\"%s\", v ? \"true\" : \"false\");\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, intx v) {\n+  st->print(\"%zd\", v);\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, uintx v) {\n+  st->print(\"%zu\", v);\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, double v) {\n+  st->print(\"%f\", v);\n+}\n+\n+void CDSMustMatchFlags::init() {\n+  assert(CDSConfig::is_dumping_archive(), \"sanity\");\n+  _max_name_width = 0;\n+\n+#define INIT_CDS_MUST_MATCH_FLAG(n) \\\n+  _v_##n = n; \\\n+  _max_name_width = MAX2(_max_name_width,strlen(#n));\n+  CDS_MUST_MATCH_FLAGS_DO(INIT_CDS_MUST_MATCH_FLAG);\n+#undef INIT_CDS_MUST_MATCH_FLAG\n+}\n+\n+bool CDSMustMatchFlags::runtime_check() const {\n+#define CHECK_CDS_MUST_MATCH_FLAG(n) \\\n+  if (_v_##n != n) { \\\n+    ResourceMark rm; \\\n+    stringStream ss; \\\n+    ss.print(\"VM option %s is different between dumptime (\", #n);  \\\n+    do_print(&ss, _v_ ## n); \\\n+    ss.print(\") and runtime (\"); \\\n+    do_print(&ss, n); \\\n+    ss.print(\")\"); \\\n+    log_info(cds)(\"%s\", ss.as_string()); \\\n+    return false; \\\n+  }\n+  CDS_MUST_MATCH_FLAGS_DO(CHECK_CDS_MUST_MATCH_FLAG);\n+#undef CHECK_CDS_MUST_MATCH_FLAG\n+\n+  return true;\n+}\n+\n+void CDSMustMatchFlags::print_info() const {\n+  LogTarget(Info, cds) lt;\n+  if (lt.is_enabled()) {\n+    LogStream ls(lt);\n+    ls.print_cr(\"Recorded VM flags during dumptime:\");\n+    print(&ls);\n+  }\n+}\n+\n+void CDSMustMatchFlags::print(outputStream* st) const {\n+#define PRINT_CDS_MUST_MATCH_FLAG(n) \\\n+  st->print(\"- %-s \", #n);                   \\\n+  st->sp(int(_max_name_width - strlen(#n))); \\\n+  do_print(st, _v_##n);                      \\\n+  st->cr();\n+  CDS_MUST_MATCH_FLAGS_DO(PRINT_CDS_MUST_MATCH_FLAG);\n+#undef PRINT_CDS_MUST_MATCH_FLAG\n+}\n+\n@@ -251,0 +316,1 @@\n+  _has_valhalla_patched_classes = CDSConfig::is_valhalla_preview();\n@@ -264,0 +330,1 @@\n+  _must_match.init();\n@@ -333,0 +400,2 @@\n+  st->print_cr(\"- has_valhalla_patched_classes              %d\", _has_valhalla_patched_classes);\n+  _must_match.print(st);\n@@ -716,0 +785,4 @@\n+  if (!header()->check_must_match_flags()) {\n+    return false;\n+  }\n+\n@@ -1921,0 +1994,18 @@\n+  if (is_static()) {\n+    const char* err = nullptr;\n+    if (CDSConfig::is_valhalla_preview()) {\n+      if (!_has_valhalla_patched_classes) {\n+        err = \"not created\";\n+      }\n+    } else {\n+      if (_has_valhalla_patched_classes) {\n+        err = \"created\";\n+      }\n+    }\n+    if (err != nullptr) {\n+      log_warning(cds)(\"This archive was %s with --enable-preview -XX:+EnableValhalla. It is \"\n+                         \"incompatible with the current JVM setting\", err);\n+      return false;\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":91,"deletions":0,"binary":false,"changes":91,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -102,0 +103,31 @@\n+#define CDS_MUST_MATCH_FLAGS_DO(f) \\\n+  f(EnableValhalla) \\\n+  f(UseArrayFlattening) \\\n+  f(UseFieldFlattening) \\\n+  f(InlineTypePassFieldsAsArgs) \\\n+  f(InlineTypeReturnedAsFields) \\\n+  f(UseNonAtomicValueFlattening) \\\n+  f(UseAtomicValueFlattening) \\\n+  f(UseNullableValueFlattening)\n+\n+\n+class CDSMustMatchFlags {\n+private:\n+  size_t _max_name_width;\n+#define DECLARE_CDS_MUST_MATCH_FLAG(n) \\\n+  decltype(n) _v_##n;\n+  CDS_MUST_MATCH_FLAGS_DO(DECLARE_CDS_MUST_MATCH_FLAG);\n+#undef DECLARE_CDS_MUST_MATCH_FLAG\n+\n+  inline static void do_print(outputStream* st, bool v);\n+  inline static void do_print(outputStream* st, intx v);\n+  inline static void do_print(outputStream* st, uintx v);\n+  inline static void do_print(outputStream* st, double v);\n+  void print_info() const;\n+\n+public:\n+  void init();\n+  bool runtime_check() const;\n+  void print(outputStream* st) const;\n+};\n+\n@@ -144,0 +176,2 @@\n+  bool   _has_valhalla_patched_classes; \/\/ Is this archived dumped with --enable-preview -XX:+EnableValhalla?\n+  CDSMustMatchFlags _must_match;        \/\/ These flags must be the same between dumptime and runtime\n@@ -255,0 +289,4 @@\n+  bool check_must_match_flags() const {\n+    return _must_match.runtime_check();\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":38,"deletions":0,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -804,0 +804,1 @@\n+      \/\/ For valhalla, the prototype header is the same as markWord::prototype();\n@@ -1505,1 +1506,5 @@\n-      assert(resolved_k == k, \"classes used by archived heap must not be replaced by JVMTI ClassFileLoadHook\");\n+      if (resolved_k->is_array_klass()) {\n+        assert(resolved_k == k || resolved_k == k->super(), \"classes used by archived heap must not be replaced by JVMTI ClassFileLoadHook\");\n+      } else {\n+        assert(resolved_k == k, \"classes used by archived heap must not be replaced by JVMTI ClassFileLoadHook\");\n+      }\n@@ -2257,0 +2262,7 @@\n+\n+    if (CDSConfig::is_valhalla_preview() && strcmp(klass_name, \"jdk\/internal\/module\/ArchivedModuleGraph\") == 0) {\n+      \/\/ FIXME -- ArchivedModuleGraph doesn't work when java.base is patched with valhalla classes.\n+      i++;\n+      continue;\n+    }\n+\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciConstant.hpp\"\n@@ -26,0 +27,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -27,0 +29,1 @@\n+#include \"ci\/ciSymbol.hpp\"\n@@ -33,0 +36,1 @@\n+#include \"jvm_io.h\"\n@@ -38,0 +42,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -73,1 +78,1 @@\n-    _known_to_link_with_put(nullptr), _known_to_link_with_get(nullptr) {\n+  _original_holder(nullptr), _is_flat(false), _known_to_link_with_put(nullptr), _known_to_link_with_get(nullptr) {\n@@ -105,0 +110,3 @@\n+  _is_null_free = false;\n+  _null_marker_offset = -1;\n+\n@@ -216,0 +224,55 @@\n+\/\/ Special copy constructor used to flatten inline type fields by\n+\/\/ copying the fields of the inline type to a new holder klass.\n+ciField::ciField(ciField* declared_field, ciField* subfield) {\n+  assert(subfield->holder()->is_inlinetype() || subfield->holder()->is_abstract(), \"should only be used for inline type field flattening\");\n+  assert(!subfield->is_flat(), \"subfield must not be flat\");\n+  assert(declared_field->is_flat(), \"declared field must be flat\");\n+\n+  _flags = declared_field->flags();\n+  _holder = declared_field->holder();\n+  _offset = declared_field->offset_in_bytes() + (subfield->offset_in_bytes() - declared_field->type()->as_inline_klass()->payload_offset());\n+\n+  char buffer[256];\n+  jio_snprintf(buffer, sizeof(buffer), \"%s.%s\", declared_field->name()->as_utf8(), subfield->name()->as_utf8());\n+  _name = ciSymbol::make(buffer);\n+\n+  _signature = subfield->_signature;\n+  _type = subfield->_type;\n+  _is_constant = declared_field->is_strict() && declared_field->is_final();\n+  _known_to_link_with_put = subfield->_known_to_link_with_put;\n+  _known_to_link_with_get = subfield->_known_to_link_with_get;\n+  _constant_value = ciConstant();\n+\n+  _is_flat = false;\n+  _is_null_free = false;\n+  _null_marker_offset = -1;\n+  _original_holder = (subfield->_original_holder != nullptr) ? subfield->_original_holder : subfield->_holder;\n+}\n+\n+\/\/ Constructor for the ciField of a null marker\n+ciField::ciField(ciField* declared_field) {\n+  assert(declared_field->is_flat(), \"declared field must be flat\");\n+  assert(!declared_field->is_null_free(), \"must have a null marker\");\n+\n+  _flags = declared_field->flags();\n+  _holder = declared_field->holder();\n+  _offset = declared_field->null_marker_offset();\n+\n+  char buffer[256];\n+  jio_snprintf(buffer, sizeof(buffer), \"%s.$nullMarker$\", declared_field->name()->as_utf8());\n+  _name = ciSymbol::make(buffer);\n+\n+  _signature = ciSymbols::bool_signature();\n+  _type = ciType::make(T_BOOLEAN);\n+\n+  _is_constant = declared_field->is_strict() && declared_field->is_final();\n+  _known_to_link_with_put = nullptr;\n+  _known_to_link_with_get = nullptr;\n+  _constant_value = ciConstant();\n+\n+  _is_flat = false;\n+  _is_null_free = false;\n+  _null_marker_offset = -1;\n+  _original_holder = nullptr;\n+}\n+\n@@ -230,0 +293,3 @@\n+  \/\/ Trust final fields in inline type buffers\n+  if (holder->is_inlinetype())\n+    return true;\n@@ -248,1 +314,1 @@\n-  Klass* field_holder = fd->field_holder();\n+  InstanceKlass* field_holder = fd->field_holder();\n@@ -251,0 +317,9 @@\n+  _is_flat = fd->is_flat();\n+  _is_null_free = fd->is_null_free_inline_type();\n+  if (fd->has_null_marker()) {\n+    InlineLayoutInfo* li = field_holder->inline_layout_info_adr(fd->index());\n+    _null_marker_offset = li->null_marker_offset();\n+  } else {\n+    _null_marker_offset = -1;\n+  }\n+  _original_holder = nullptr;\n@@ -323,1 +398,3 @@\n-  ciKlass* type = CURRENT_ENV->get_klass_by_name_impl(_holder, constantPoolHandle(), _signature, false);\n+  \/\/ Use original holder for fields that came in through flattening\n+  ciKlass* accessing_klass = (_original_holder != nullptr) ? _original_holder : _holder;\n+  ciKlass* type = CURRENT_ENV->get_klass_by_name_impl(accessing_klass, constantPoolHandle(), _signature, false);\n@@ -386,0 +463,12 @@\n+  \/\/ Strict statics may require tracking if their class is not fully initialized.\n+  \/\/ For now we can bail out of the compiler and let the interpreter handle it.\n+  if (is_static && result.is_strict_static_unset()) {\n+    \/\/ If we left out this logic, we would get (a) spurious <clinit>\n+    \/\/ failures for C2 code because compiled putstatic would not write\n+    \/\/ the \"unset\" bits, and (b) missed failures for too-early reads,\n+    \/\/ since the compiled getstatic would not check the \"unset\" bits.\n+    \/\/ Test C1 on <clinit> with \"-XX:TieredStopAtLevel=2 -Xcomp -Xbatch\".\n+    \/\/ Test C2 on <clinit> with \"-XX:-TieredCompilation -Xcomp -Xbatch\".\n+    return false;\n+  }\n+\n@@ -436,0 +525,3 @@\n+  tty->print(\" is_flat=%s\", bool_to_str(_is_flat));\n+  tty->print(\" is_null_free=%s\", bool_to_str(_is_null_free));\n+  tty->print(\" null_marker_offset=%d\", _null_marker_offset);\n","filename":"src\/hotspot\/share\/ci\/ciField.cpp","additions":95,"deletions":3,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -35,0 +37,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -68,1 +71,2 @@\n-  _nonstatic_fields = nullptr; \/\/ initialized lazily by compute_nonstatic_fields:\n+  _declared_nonstatic_fields = nullptr; \/\/ initialized lazily by compute_nonstatic_fields\n+  _nonstatic_fields = nullptr;          \/\/ initialized lazily by compute_nonstatic_fields\n@@ -70,1 +74,1 @@\n-  _implementor = nullptr; \/\/ we will fill these lazily\n+  _implementor = nullptr;               \/\/ we will fill these lazily\n@@ -117,2 +121,3 @@\n-                                 jobject loader)\n-  : ciKlass(name, T_OBJECT)\n+                                 jobject loader,\n+                                 BasicType bt)\n+  : ciKlass(name, bt)\n@@ -123,1 +128,2 @@\n-  _nonstatic_fields = nullptr;\n+  _declared_nonstatic_fields = nullptr; \/\/ initialized lazily by compute_nonstatic_fields\n+  _nonstatic_fields = nullptr;          \/\/ initialized lazily by compute_nonstatic_fields\n@@ -321,1 +327,1 @@\n-    _flags.print_klass_flags();\n+    _flags.print_klass_flags(st);\n@@ -325,1 +331,1 @@\n-      _super->print_name();\n+      _super->print_name_on(st);\n@@ -396,1 +402,1 @@\n-    ciField* field = _nonstatic_fields->at(i);\n+    ciField* field = nonstatic_field_at(i);\n@@ -398,1 +404,1 @@\n-    if (field_off == field_offset)\n+    if (field_off == field_offset) {\n@@ -400,0 +406,1 @@\n+    }\n@@ -410,0 +417,1 @@\n+\n@@ -420,0 +428,34 @@\n+ciField* ciInstanceKlass::get_non_flat_field_by_offset(int field_offset) {\n+  for (int i = 0, len = nof_declared_nonstatic_fields(); i < len; i++) {\n+    ciField* field = declared_nonstatic_field_at(i);\n+    int field_off = field->offset_in_bytes();\n+    if (field_off == field_offset) {\n+      return field;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+int ciInstanceKlass::field_index_by_offset(int offset) {\n+  assert(contains_field_offset(offset), \"invalid field offset\");\n+  int best_offset = 0;\n+  int best_index = -1;\n+  \/\/ Search the field with the given offset\n+  for (int i = 0; i < nof_declared_nonstatic_fields(); ++i) {\n+    int field_offset = declared_nonstatic_field_at(i)->offset_in_bytes();\n+    if (field_offset == offset) {\n+      \/\/ Exact match\n+      return i;\n+    } else if (field_offset < offset && field_offset > best_offset) {\n+      \/\/ No exact match. Save the index of the field with the closest offset that\n+      \/\/ is smaller than the given field offset. This index corresponds to the\n+      \/\/ flat field that holds the field we are looking for.\n+      best_offset = field_offset;\n+      best_index = i;\n+    }\n+  }\n+  assert(best_index >= 0, \"field not found\");\n+  assert(best_offset == offset || declared_nonstatic_field_at(best_index)->type()->is_inlinetype(), \"offset should match for non-inline types\");\n+  return best_index;\n+}\n+\n@@ -434,0 +476,2 @@\n+const GrowableArray<ciField*> empty_field_array(0, MemTag::mtCompiler);\n+\n@@ -459,3 +503,1 @@\n-\/\/ ------------------------------------------------------------------\n-\/\/ ciInstanceKlass::compute_nonstatic_fields\n-int ciInstanceKlass::compute_nonstatic_fields() {\n+void ciInstanceKlass::compute_nonstatic_fields() {\n@@ -464,2 +506,4 @@\n-  if (_nonstatic_fields != nullptr)\n-    return _nonstatic_fields->length();\n+  if (_nonstatic_fields != nullptr) {\n+    assert(_declared_nonstatic_fields != nullptr, \"must be initialized at the same time, class %s\", name()->as_utf8());\n+    return;\n+  }\n@@ -468,3 +512,3 @@\n-    Arena* arena = CURRENT_ENV->arena();\n-    _nonstatic_fields = new (arena) GrowableArray<ciField*>(arena, 0, 0, nullptr);\n-    return 0;\n+    _declared_nonstatic_fields = &empty_field_array;\n+    _nonstatic_fields = &empty_field_array;\n+    return;\n@@ -475,6 +519,5 @@\n-  GrowableArray<ciField*>* super_fields = nullptr;\n-  if (super != nullptr && super->has_nonstatic_fields()) {\n-    int super_flen   = super->nof_nonstatic_fields();\n-    super_fields = super->_nonstatic_fields;\n-    assert(super_flen == 0 || super_fields != nullptr, \"first get nof_fields\");\n-  }\n+  assert(super != nullptr, \"must have a super class, current class: %s\", name()->as_utf8());\n+  super->compute_nonstatic_fields();\n+  const GrowableArray<ciField*>* super_declared_fields = super->_declared_nonstatic_fields;;\n+  const GrowableArray<ciField*>* super_fields = super->_nonstatic_fields;\n+  assert(super_declared_fields != nullptr && super_fields != nullptr, \"must have been initialized, current class: %s, super class: %s\", name()->as_utf8(), super->name()->as_utf8());\n@@ -482,18 +525,2 @@\n-  GrowableArray<ciField*>* fields = nullptr;\n-      fields = compute_nonstatic_fields_impl(super_fields);\n-    });\n-\n-  if (fields == nullptr) {\n-    \/\/ This can happen if this class (java.lang.Class) has invisible fields.\n-    if (super_fields != nullptr) {\n-      _nonstatic_fields = super_fields;\n-      return super_fields->length();\n-    } else {\n-      return 0;\n-    }\n-  }\n-\n-  int flen = fields->length();\n-\n-  _nonstatic_fields = fields;\n-  return flen;\n+    compute_nonstatic_fields_impl(super_declared_fields, super_fields);\n+  });\n@@ -503,3 +530,2 @@\n-GrowableArray<ciField*>*\n-ciInstanceKlass::compute_nonstatic_fields_impl(GrowableArray<ciField*>*\n-                                               super_fields) {\n+void ciInstanceKlass::compute_nonstatic_fields_impl(const GrowableArray<ciField*>* super_declared_fields, const GrowableArray<ciField*>* super_fields) {\n+  assert(_declared_nonstatic_fields == nullptr && _nonstatic_fields == nullptr, \"initialized already\");\n@@ -508,10 +534,19 @@\n-  int flen = 0;\n-  GrowableArray<ciField*>* fields = nullptr;\n-  InstanceKlass* k = get_instanceKlass();\n-  for (JavaFieldStream fs(k); !fs.done(); fs.next()) {\n-    if (fs.access_flags().is_static())  continue;\n-    flen += 1;\n-  }\n-  \/\/ allocate the array:\n-  if (flen == 0) {\n-    return nullptr;  \/\/ return nothing if none are locally declared\n+  InstanceKlass* this_klass = get_instanceKlass();\n+  int declared_field_num = 0;\n+  int field_num = 0;\n+  for (JavaFieldStream fs(this_klass); !fs.done(); fs.next()) {\n+    if (fs.access_flags().is_static()) {\n+      continue;\n+    }\n+\n+    declared_field_num++;\n+\n+    fieldDescriptor& fd = fs.field_descriptor();\n+    if (fd.is_flat()) {\n+      InlineKlass* k = this_klass->get_inline_type_field_klass(fd.index());\n+      ciInlineKlass* vk = CURRENT_ENV->get_klass(k)->as_inline_klass();\n+      field_num += vk->nof_nonstatic_fields();\n+      field_num += fd.has_null_marker() ? 1 : 0;\n+    } else {\n+      field_num++;\n+    }\n@@ -520,2 +555,5 @@\n-  if (super_fields != nullptr) {\n-    flen += super_fields->length();\n+\n+  GrowableArray<ciField*>* tmp_declared_fields = nullptr;\n+  if (declared_field_num != 0) {\n+    tmp_declared_fields = new (arena) GrowableArray<ciField*>(arena, declared_field_num + super_declared_fields->length(), 0, nullptr);\n+    tmp_declared_fields->appendAll(super_declared_fields);\n@@ -523,3 +561,5 @@\n-  fields = new (arena) GrowableArray<ciField*>(arena, flen, 0, nullptr);\n-  if (super_fields != nullptr) {\n-    fields->appendAll(super_fields);\n+\n+  GrowableArray<ciField*>* tmp_fields = nullptr;\n+  if (field_num != 0) {\n+    tmp_fields = new (arena) GrowableArray<ciField*>(arena, field_num + super_fields->length(), 0, nullptr);\n+    tmp_fields->appendAll(super_fields);\n@@ -528,2 +568,9 @@\n-  for (JavaFieldStream fs(k); !fs.done(); fs.next()) {\n-    if (fs.access_flags().is_static())  continue;\n+  \/\/ For later assertion\n+  declared_field_num += super_declared_fields->length();\n+  field_num += super_fields->length();\n+\n+  for (JavaFieldStream fs(this_klass); !fs.done(); fs.next()) {\n+    if (fs.access_flags().is_static()) {\n+      continue;\n+    }\n+\n@@ -531,2 +578,38 @@\n-    ciField* field = new (arena) ciField(&fd);\n-    fields->append(field);\n+    ciField* declared_field = new (arena) ciField(&fd);\n+    assert(tmp_declared_fields != nullptr, \"should be initialized\");\n+    tmp_declared_fields->append(declared_field);\n+\n+    if (fd.is_flat()) {\n+      \/\/ Flat fields are embedded\n+      Klass* k = get_instanceKlass()->get_inline_type_field_klass(fd.index());\n+      ciInlineKlass* vk = CURRENT_ENV->get_klass(k)->as_inline_klass();\n+      \/\/ Iterate over fields of the flat inline type and copy them to 'this'\n+      for (int i = 0; i < vk->nof_nonstatic_fields(); ++i) {\n+        assert(tmp_fields != nullptr, \"should be initialized\");\n+        tmp_fields->append(new (arena) ciField(declared_field, vk->nonstatic_field_at(i)));\n+      }\n+      if (fd.has_null_marker()) {\n+        assert(tmp_fields != nullptr, \"should be initialized\");\n+        tmp_fields->append(new (arena) ciField(declared_field));\n+      }\n+    } else {\n+      assert(tmp_fields != nullptr, \"should be initialized\");\n+      tmp_fields->append(declared_field);\n+    }\n+  }\n+\n+  \/\/ Now sort them by offset, ascending. In principle, they could mix with superclass fields.\n+  if (tmp_declared_fields != nullptr) {\n+    assert(tmp_declared_fields->length() == declared_field_num, \"sanity check failed for class: %s, number of declared fields: %d, expected: %d\",\n+           name()->as_utf8(), tmp_declared_fields->length(), declared_field_num);\n+    _declared_nonstatic_fields = tmp_declared_fields;\n+  } else {\n+    _declared_nonstatic_fields = super_declared_fields;\n+  }\n+\n+  if (tmp_fields != nullptr) {\n+    assert(tmp_fields->length() == field_num, \"sanity check failed for class: %s, number of fields: %d, expected: %d\",\n+           name()->as_utf8(), tmp_fields->length(), field_num);\n+    _nonstatic_fields = tmp_fields;\n+  } else {\n+    _nonstatic_fields = super_fields;\n@@ -534,2 +617,0 @@\n-  assert(fields->length() == flen, \"sanity\");\n-  return fields;\n@@ -647,0 +728,17 @@\n+bool ciInstanceKlass::can_be_inline_klass(bool is_exact) {\n+  if (!EnableValhalla) {\n+    return false;\n+  }\n+  if (!is_loaded() || is_inlinetype()) {\n+    \/\/ Not loaded or known to be an inline klass\n+    return true;\n+  }\n+  if (!is_exact) {\n+    \/\/ Not exact, check if this is a valid super for an inline klass\n+    GUARDED_VM_ENTRY(\n+      return !get_instanceKlass()->access_flags().is_identity_class() || is_java_lang_Object();\n+    )\n+  }\n+  return false;\n+}\n+\n@@ -655,1 +753,2 @@\n-class StaticFinalFieldPrinter : public FieldClosure {\n+class StaticFieldPrinter : public FieldClosure {\n+protected:\n@@ -657,0 +756,8 @@\n+public:\n+  StaticFieldPrinter(outputStream* out) :\n+    _out(out) {\n+  }\n+  void do_field_helper(fieldDescriptor* fd, oop obj, bool is_flat);\n+};\n+\n+class StaticFinalFieldPrinter : public StaticFieldPrinter {\n@@ -660,2 +767,1 @@\n-    _out(out),\n-    _holder(holder) {\n+    StaticFieldPrinter(out), _holder(holder) {\n@@ -666,46 +772,59 @@\n-      oop mirror = fd->field_holder()->java_mirror();\n-      _out->print(\"staticfield %s %s %s \", _holder, fd->name()->as_quoted_ascii(), fd->signature()->as_quoted_ascii());\n-      BasicType field_type = fd->field_type();\n-      switch (field_type) {\n-        case T_BYTE:    _out->print_cr(\"%d\", mirror->byte_field(fd->offset()));   break;\n-        case T_BOOLEAN: _out->print_cr(\"%d\", mirror->bool_field(fd->offset()));   break;\n-        case T_SHORT:   _out->print_cr(\"%d\", mirror->short_field(fd->offset()));  break;\n-        case T_CHAR:    _out->print_cr(\"%d\", mirror->char_field(fd->offset()));   break;\n-        case T_INT:     _out->print_cr(\"%d\", mirror->int_field(fd->offset()));    break;\n-        case T_LONG:    _out->print_cr(INT64_FORMAT, (int64_t)(mirror->long_field(fd->offset())));   break;\n-        case T_FLOAT: {\n-          float f = mirror->float_field(fd->offset());\n-          _out->print_cr(\"%d\", *(int*)&f);\n-          break;\n-        }\n-        case T_DOUBLE: {\n-          double d = mirror->double_field(fd->offset());\n-          _out->print_cr(INT64_FORMAT, *(int64_t*)&d);\n-          break;\n-        }\n-        case T_ARRAY:  \/\/ fall-through\n-        case T_OBJECT: {\n-          oop value =  mirror->obj_field_acquire(fd->offset());\n-          if (value == nullptr) {\n-            if (field_type == T_ARRAY) {\n-              _out->print(\"%d\", -1);\n-            }\n-            _out->cr();\n-          } else if (value->is_instance()) {\n-            assert(field_type == T_OBJECT, \"\");\n-            if (value->is_a(vmClasses::String_klass())) {\n-              const char* ascii_value = java_lang_String::as_quoted_ascii(value);\n-              _out->print_cr(\"\\\"%s\\\"\", (ascii_value != nullptr) ? ascii_value : \"\");\n-            } else {\n-              const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n-              _out->print_cr(\"%s\", klass_name);\n-            }\n-          } else if (value->is_array()) {\n-            typeArrayOop ta = (typeArrayOop)value;\n-            _out->print(\"%d\", ta->length());\n-            if (value->is_objArray()) {\n-              objArrayOop oa = (objArrayOop)value;\n-              const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n-              _out->print(\" %s\", klass_name);\n-            }\n-            _out->cr();\n+      InstanceKlass* holder = fd->field_holder();\n+      oop mirror = holder->java_mirror();\n+      _out->print(\"staticfield %s %s \", _holder, fd->name()->as_quoted_ascii());\n+      BasicType bt = fd->field_type();\n+      if (bt != T_OBJECT && bt != T_ARRAY) {\n+        _out->print(\"%s \", fd->signature()->as_quoted_ascii());\n+      }\n+      do_field_helper(fd, mirror, false);\n+      _out->cr();\n+    }\n+  }\n+};\n+\n+class InlineTypeFieldPrinter : public StaticFieldPrinter {\n+  oop _obj;\n+public:\n+  InlineTypeFieldPrinter(outputStream* out, oop obj) :\n+    StaticFieldPrinter(out), _obj(obj) {\n+  }\n+  void do_field(fieldDescriptor* fd) {\n+    do_field_helper(fd, _obj, true);\n+    _out->print(\" \");\n+  }\n+};\n+\n+void StaticFieldPrinter::do_field_helper(fieldDescriptor* fd, oop mirror, bool is_flat) {\n+  BasicType field_type = fd->field_type();\n+  switch (field_type) {\n+    case T_BYTE:    _out->print(\"%d\", mirror->byte_field(fd->offset()));   break;\n+    case T_BOOLEAN: _out->print(\"%d\", mirror->bool_field(fd->offset()));   break;\n+    case T_SHORT:   _out->print(\"%d\", mirror->short_field(fd->offset()));  break;\n+    case T_CHAR:    _out->print(\"%d\", mirror->char_field(fd->offset()));   break;\n+    case T_INT:     _out->print(\"%d\", mirror->int_field(fd->offset()));    break;\n+    case T_LONG:    _out->print(INT64_FORMAT, (int64_t)(mirror->long_field(fd->offset())));   break;\n+    case T_FLOAT: {\n+      float f = mirror->float_field(fd->offset());\n+      _out->print(\"%d\", *(int*)&f);\n+      break;\n+    }\n+    case T_DOUBLE: {\n+      double d = mirror->double_field(fd->offset());\n+      _out->print(INT64_FORMAT, *(int64_t*)&d);\n+      break;\n+    }\n+    case T_ARRAY:  \/\/ fall-through\n+    case T_OBJECT:\n+      if (!fd->is_null_free_inline_type()) {\n+        _out->print(\"%s \", fd->signature()->as_quoted_ascii());\n+        oop value =  mirror->obj_field_acquire(fd->offset());\n+        if (value == nullptr) {\n+          if (field_type == T_ARRAY) {\n+            _out->print(\"%d\", -1);\n+          }\n+          _out->cr();\n+        } else if (value->is_instance()) {\n+          assert(field_type == T_OBJECT, \"\");\n+          if (value->is_a(vmClasses::String_klass())) {\n+            const char* ascii_value = java_lang_String::as_quoted_ascii(value);\n+            _out->print(\"\\\"%s\\\"\", (ascii_value != nullptr) ? ascii_value : \"\");\n@@ -713,1 +832,2 @@\n-            ShouldNotReachHere();\n+            const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n+            _out->print(\"%s\", klass_name);\n@@ -715,3 +835,9 @@\n-          break;\n-        }\n-        default:\n+        } else if (value->is_array()) {\n+          typeArrayOop ta = (typeArrayOop)value;\n+          _out->print(\"%d\", ta->length());\n+          if (value->is_objArray() || value->is_flatArray()) {\n+            objArrayOop oa = (objArrayOop)value;\n+            const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n+            _out->print(\" %s\", klass_name);\n+          }\n+        } else {\n@@ -720,1 +846,26 @@\n-    }\n+        break;\n+      } else {\n+        \/\/ handling of null free inline type\n+        ResetNoHandleMark rnhm;\n+        Thread* THREAD = Thread::current();\n+        SignatureStream ss(fd->signature(), false);\n+        Symbol* name = ss.as_symbol();\n+        assert(!HAS_PENDING_EXCEPTION, \"can resolve klass?\");\n+        InstanceKlass* holder = fd->field_holder();\n+        InstanceKlass* k = SystemDictionary::find_instance_klass(THREAD, name,\n+                                                                 Handle(THREAD, holder->class_loader()));\n+        assert(k != nullptr && !HAS_PENDING_EXCEPTION, \"can resolve klass?\");\n+        InlineKlass* vk = InlineKlass::cast(k);\n+        oop obj;\n+        if (is_flat) {\n+          int field_offset = fd->offset() - vk->payload_offset();\n+          obj = cast_to_oop(cast_from_oop<address>(mirror) + field_offset);\n+        } else {\n+          obj = mirror->obj_field_acquire(fd->offset());\n+        }\n+        InlineTypeFieldPrinter print_field(_out, obj);\n+        vk->do_nonstatic_fields(&print_field);\n+        break;\n+      }\n+    default:\n+      ShouldNotReachHere();\n@@ -722,1 +873,1 @@\n-};\n+}\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.cpp","additions":271,"deletions":120,"binary":false,"changes":391,"status":"modified"},{"patch":"@@ -71,1 +71,14 @@\n-  GrowableArray<ciField*>* _nonstatic_fields;  \/\/ ordered by JavaFieldStream\n+\n+  \/\/ Fields declared in the bytecode (without nested fields in flat fields),\n+  \/\/ ordered in JavaFieldStream order, with superclasses first (i.e. from lang.java.Object\n+  \/\/ to most derived class).\n+  const GrowableArray<ciField*>* _declared_nonstatic_fields;\n+\n+  \/\/ Fields laid out in memory (flat fields are expanded into their components). The ciField object\n+  \/\/ for each primitive component has the holder being this ciInstanceKlass or one of its\n+  \/\/ superclasses.\n+  \/\/ Fields are in the same order as in _declared_nonstatic_fields, but flat fields are replaced by\n+  \/\/ the list of their own fields, ordered the same way (hierarchy traversed top-down, in\n+  \/\/ JavaFieldStream order).\n+  const GrowableArray<ciField*>* _nonstatic_fields;\n+\n@@ -89,1 +102,1 @@\n-  ciInstanceKlass(ciSymbol* name, jobject loader);\n+  ciInstanceKlass(ciSymbol* name, jobject loader, BasicType bt = T_OBJECT); \/\/ for unloaded klasses\n@@ -110,2 +123,2 @@\n-  int  compute_nonstatic_fields();\n-  GrowableArray<ciField*>* compute_nonstatic_fields_impl(GrowableArray<ciField*>* super_fields);\n+  void compute_nonstatic_fields();\n+  void compute_nonstatic_fields_impl(const GrowableArray<ciField*>* super_declared_fields, const GrowableArray<ciField*>* super_fields);\n@@ -209,0 +222,12 @@\n+  \/\/ Get field descriptor at field_offset ignoring flattening\n+  ciField* get_non_flat_field_by_offset(int field_offset);\n+  \/\/ Get the index of the declared field that contains this offset\n+  int field_index_by_offset(int offset);\n+\n+  \/\/ Total number of nonstatic fields (including inherited)\n+  int nof_declared_nonstatic_fields() {\n+    if (_declared_nonstatic_fields == nullptr) {\n+      compute_nonstatic_fields();\n+    }\n+    return _declared_nonstatic_fields->length();\n+  }\n@@ -211,5 +236,4 @@\n-  \/\/ total number of nonstatic fields (including inherited):\n-    if (_nonstatic_fields == nullptr)\n-      return compute_nonstatic_fields();\n-    else\n-      return _nonstatic_fields->length();\n+    if (_nonstatic_fields == nullptr) {\n+      compute_nonstatic_fields();\n+    }\n+    return _nonstatic_fields->length();\n@@ -228,1 +252,5 @@\n-  \/\/ nth nonstatic field (presented by ascending address)\n+  ciField* declared_nonstatic_field_at(int i) {\n+    assert(_declared_nonstatic_fields != nullptr, \"should be initialized\");\n+    return _declared_nonstatic_fields->at(i);\n+  }\n+\n@@ -249,1 +277,0 @@\n-  bool is_super       () { return flags().is_super(); }\n@@ -252,0 +279,1 @@\n+  bool is_abstract_value_klass() { return is_abstract() && !flags().is_identity(); }\n@@ -267,0 +295,2 @@\n+  virtual bool can_be_inline_klass(bool is_exact = false);\n+\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.hpp","additions":41,"deletions":11,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -55,0 +55,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -56,1 +58,1 @@\n-#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.inline.hpp\"\n@@ -871,0 +873,1 @@\n+int java_lang_Class::_is_identity_offset;\n@@ -1098,0 +1101,1 @@\n+  set_is_identity(mirror(), k->is_array_klass() || k->is_identity_class());\n@@ -1106,1 +1110,9 @@\n-    if (k->is_typeArray_klass()) {\n+    if (k->is_flatArray_klass()) {\n+      Klass* element_klass = (Klass*) FlatArrayKlass::cast(k)->element_klass();\n+      assert(element_klass->is_inline_klass(), \"Must be inline type component\");\n+      if (is_scratch) {\n+        comp_mirror = Handle(THREAD, HeapShared::scratch_java_mirror(element_klass));\n+      } else {\n+        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+      }\n+    } else if (k->is_typeArray_klass()) {\n@@ -1115,0 +1127,1 @@\n+      assert(!k->is_refArray_klass() || !k->is_flatArray_klass(), \"Must not have mirror\");\n@@ -1117,0 +1130,1 @@\n+      oop comp_oop = element_klass->java_mirror();\n@@ -1120,1 +1134,1 @@\n-        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+        comp_mirror = Handle(THREAD, comp_oop);\n@@ -1149,1 +1163,0 @@\n-\n@@ -1153,0 +1166,9 @@\n+\n+    if (k->is_refined_objArray_klass()) {\n+      Klass* super_klass = k->super();\n+      assert(super_klass != nullptr, \"Must be\");\n+      Handle mirror(THREAD, super_klass->java_mirror());\n+      k->set_java_mirror(mirror);\n+      return;\n+    }\n+\n@@ -1175,0 +1197,1 @@\n+\n@@ -1198,3 +1221,3 @@\n-  if (k->class_loader() != nullptr &&\n-      k->class_loader() != SystemDictionary::java_platform_loader() &&\n-      k->class_loader() != SystemDictionary::java_system_loader()) {\n+  if ((k->class_loader() != nullptr &&\n+       k->class_loader() != SystemDictionary::java_platform_loader() &&\n+       k->class_loader() != SystemDictionary::java_system_loader())) {\n@@ -1239,1 +1262,0 @@\n-  assert(as_Klass(m) == k, \"must be\");\n@@ -1243,0 +1265,1 @@\n+    assert(as_Klass(m) == k, \"must be\");\n@@ -1249,0 +1272,4 @@\n+  } else {\n+    ObjArrayKlass* objarray_k = (ObjArrayKlass*)as_Klass(m);\n+    \/\/ Mirror should be restored for an ObjArrayKlass or one of its refined array klasses\n+    assert(objarray_k == k || objarray_k->next_refined_array_klass() == k, \"must be\");\n@@ -1381,0 +1408,4 @@\n+void java_lang_Class::set_is_identity(oop java_class, bool value) {\n+  assert(_is_identity_offset != 0, \"must be set\");\n+  java_class->bool_field_put(_is_identity_offset, value);\n+}\n@@ -1422,1 +1453,3 @@\n-  if (is_instance)  st->print(\"L\");\n+  if (is_instance)  {\n+    st->print(\"L\");\n+  }\n@@ -1480,0 +1513,1 @@\n+  assert(!klass->is_refined_objArray_klass(), \"should not be ref or flat array klass\");\n@@ -1539,1 +1573,2 @@\n-  macro(_is_primitive_offset,        k, \"primitive\",           bool_signature,         false);\n+  macro(_is_primitive_offset,        k, \"primitive\",           bool_signature,         false); \\\n+  macro(_is_identity_offset,         k, \"identity\",            bool_signature,         false);\n@@ -2859,1 +2894,1 @@\n-      if (method->name() == vmSymbols::object_initializer_name() &&\n+      if (method->is_object_constructor() &&\n@@ -3218,2 +3253,2 @@\n-  if (m->is_object_initializer()) {\n-    flags |= java_lang_invoke_MemberName::MN_IS_CONSTRUCTOR;\n+  if (m->is_object_constructor()) {\n+    flags |= java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR;\n@@ -3610,1 +3645,1 @@\n-int java_lang_reflect_Field::_trusted_final_offset;\n+int java_lang_reflect_Field::_flags_offset;\n@@ -3620,1 +3655,1 @@\n-  macro(_trusted_final_offset,    k, vmSymbols::trusted_final_name(),    bool_signature,       false); \\\n+  macro(_flags_offset,     k, vmSymbols::flags_name(),     int_signature,    false); \\\n@@ -3685,2 +3720,2 @@\n-void java_lang_reflect_Field::set_trusted_final(oop field) {\n-  field->bool_field_put(_trusted_final_offset, true);\n+void java_lang_reflect_Field::set_flags(oop field, int value) {\n+  field->int_field_put(_flags_offset, value);\n@@ -3986,2 +4021,1 @@\n-int java_lang_boxing_object::_value_offset;\n-int java_lang_boxing_object::_long_value_offset;\n+int* java_lang_boxing_object::_offsets;\n@@ -3989,3 +4023,9 @@\n-#define BOXING_FIELDS_DO(macro) \\\n-  macro(_value_offset,      integerKlass, \"value\", int_signature, false); \\\n-  macro(_long_value_offset, longKlass, \"value\", long_signature, false);\n+#define BOXING_FIELDS_DO(macro)                                                                                                    \\\n+  macro(java_lang_boxing_object::_offsets[T_BOOLEAN - T_BOOLEAN], vmClasses::Boolean_klass(),   \"value\", bool_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_CHAR - T_BOOLEAN],    vmClasses::Character_klass(), \"value\", char_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_FLOAT - T_BOOLEAN],   vmClasses::Float_klass(),     \"value\", float_signature,  false); \\\n+  macro(java_lang_boxing_object::_offsets[T_DOUBLE - T_BOOLEAN],  vmClasses::Double_klass(),    \"value\", double_signature, false); \\\n+  macro(java_lang_boxing_object::_offsets[T_BYTE - T_BOOLEAN],    vmClasses::Byte_klass(),      \"value\", byte_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_SHORT - T_BOOLEAN],   vmClasses::Short_klass(),     \"value\", short_signature,  false); \\\n+  macro(java_lang_boxing_object::_offsets[T_INT - T_BOOLEAN],     vmClasses::Integer_klass(),   \"value\", int_signature,    false); \\\n+  macro(java_lang_boxing_object::_offsets[T_LONG - T_BOOLEAN],    vmClasses::Long_klass(),      \"value\", long_signature,   false);\n@@ -3994,2 +4034,2 @@\n-  InstanceKlass* integerKlass = vmClasses::Integer_klass();\n-  InstanceKlass* longKlass = vmClasses::Long_klass();\n+  assert(T_LONG - T_BOOLEAN == 7, \"Sanity check\");\n+  java_lang_boxing_object::_offsets = NEW_C_HEAP_ARRAY(int, 8, mtInternal);\n@@ -4001,0 +4041,4 @@\n+  if (f->reading()) {\n+    assert(T_LONG - T_BOOLEAN == 7, \"Sanity check\");\n+    java_lang_boxing_object::_offsets = NEW_C_HEAP_ARRAY(int, 8, mtInternal);\n+  }\n@@ -4021,1 +4065,1 @@\n-      box->bool_field_put(_value_offset, value->z);\n+      box->bool_field_put(value_offset(type), value->z);\n@@ -4024,1 +4068,1 @@\n-      box->char_field_put(_value_offset, value->c);\n+      box->char_field_put(value_offset(type), value->c);\n@@ -4027,1 +4071,1 @@\n-      box->float_field_put(_value_offset, value->f);\n+      box->float_field_put(value_offset(type), value->f);\n@@ -4030,1 +4074,1 @@\n-      box->double_field_put(_long_value_offset, value->d);\n+      box->double_field_put(value_offset(type), value->d);\n@@ -4033,1 +4077,1 @@\n-      box->byte_field_put(_value_offset, value->b);\n+      box->byte_field_put(value_offset(type), value->b);\n@@ -4036,1 +4080,1 @@\n-      box->short_field_put(_value_offset, value->s);\n+      box->short_field_put(value_offset(type), value->s);\n@@ -4039,1 +4083,1 @@\n-      box->int_field_put(_value_offset, value->i);\n+      box->int_field_put(value_offset(type), value->i);\n@@ -4042,1 +4086,1 @@\n-      box->long_field_put(_long_value_offset, value->j);\n+      box->long_field_put(value_offset(type), value->j);\n@@ -4064,1 +4108,1 @@\n-    value->z = box->bool_field(_value_offset);\n+    value->z = box->bool_field(value_offset(type));\n@@ -4067,1 +4111,1 @@\n-    value->c = box->char_field(_value_offset);\n+    value->c = box->char_field(value_offset(type));\n@@ -4070,1 +4114,1 @@\n-    value->f = box->float_field(_value_offset);\n+    value->f = box->float_field(value_offset(type));\n@@ -4073,1 +4117,1 @@\n-    value->d = box->double_field(_long_value_offset);\n+    value->d = box->double_field(value_offset(type));\n@@ -4076,1 +4120,1 @@\n-    value->b = box->byte_field(_value_offset);\n+    value->b = box->byte_field(value_offset(type));\n@@ -4079,1 +4123,1 @@\n-    value->s = box->short_field(_value_offset);\n+    value->s = box->short_field(value_offset(type));\n@@ -4082,1 +4126,1 @@\n-    value->i = box->int_field(_value_offset);\n+    value->i = box->int_field(value_offset(type));\n@@ -4085,1 +4129,1 @@\n-    value->j = box->long_field(_long_value_offset);\n+    value->j = box->long_field(value_offset(type));\n@@ -4098,1 +4142,1 @@\n-    box->bool_field_put(_value_offset, value->z);\n+    box->bool_field_put(value_offset(type), value->z);\n@@ -4101,1 +4145,1 @@\n-    box->char_field_put(_value_offset, value->c);\n+    box->char_field_put(value_offset(type), value->c);\n@@ -4104,1 +4148,1 @@\n-    box->float_field_put(_value_offset, value->f);\n+    box->float_field_put(value_offset(type), value->f);\n@@ -4107,1 +4151,1 @@\n-    box->double_field_put(_long_value_offset, value->d);\n+    box->double_field_put(value_offset(type), value->d);\n@@ -4110,1 +4154,1 @@\n-    box->byte_field_put(_value_offset, value->b);\n+    box->byte_field_put(value_offset(type), value->b);\n@@ -4113,1 +4157,1 @@\n-    box->short_field_put(_value_offset, value->s);\n+    box->short_field_put(value_offset(type), value->s);\n@@ -4116,1 +4160,1 @@\n-    box->int_field_put(_value_offset, value->i);\n+    box->int_field_put(value_offset(type), value->i);\n@@ -4119,1 +4163,1 @@\n-    box->long_field_put(_long_value_offset, value->j);\n+    box->long_field_put(value_offset(type), value->j);\n@@ -4515,1 +4559,0 @@\n-\n@@ -4525,1 +4568,1 @@\n-  return (flags(mname) & (MN_IS_METHOD | MN_IS_CONSTRUCTOR)) > 0;\n+  return (flags(mname) & (MN_IS_METHOD | MN_IS_OBJECT_CONSTRUCTOR)) > 0;\n@@ -5518,16 +5561,11 @@\n-#define CHECK_OFFSET(klass_name, cpp_klass_name, field_name, field_sig) \\\n-  valid &= check_offset(klass_name, cpp_klass_name :: _##field_name ## _offset, #field_name, field_sig)\n-\n-#define CHECK_LONG_OFFSET(klass_name, cpp_klass_name, field_name, field_sig) \\\n-  valid &= check_offset(klass_name, cpp_klass_name :: _##long_ ## field_name ## _offset, #field_name, field_sig)\n-\n-  \/\/ Boxed primitive objects (java_lang_boxing_object)\n-\n-  CHECK_OFFSET(\"java\/lang\/Boolean\",   java_lang_boxing_object, value, \"Z\");\n-  CHECK_OFFSET(\"java\/lang\/Character\", java_lang_boxing_object, value, \"C\");\n-  CHECK_OFFSET(\"java\/lang\/Float\",     java_lang_boxing_object, value, \"F\");\n-  CHECK_LONG_OFFSET(\"java\/lang\/Double\", java_lang_boxing_object, value, \"D\");\n-  CHECK_OFFSET(\"java\/lang\/Byte\",      java_lang_boxing_object, value, \"B\");\n-  CHECK_OFFSET(\"java\/lang\/Short\",     java_lang_boxing_object, value, \"S\");\n-  CHECK_OFFSET(\"java\/lang\/Integer\",   java_lang_boxing_object, value, \"I\");\n-  CHECK_LONG_OFFSET(\"java\/lang\/Long\", java_lang_boxing_object, value, \"J\");\n+#define CHECK_OFFSET(klass_name, type, field_sig) \\\n+  valid &= check_offset(klass_name, java_lang_boxing_object::value_offset(type), \"value\", field_sig)\n+\n+  CHECK_OFFSET(\"java\/lang\/Boolean\",   T_BOOLEAN, \"Z\");\n+  CHECK_OFFSET(\"java\/lang\/Character\", T_CHAR,    \"C\");\n+  CHECK_OFFSET(\"java\/lang\/Float\",     T_FLOAT,   \"F\");\n+  CHECK_OFFSET(\"java\/lang\/Double\",    T_DOUBLE,  \"D\");\n+  CHECK_OFFSET(\"java\/lang\/Byte\",      T_BYTE,    \"B\");\n+  CHECK_OFFSET(\"java\/lang\/Short\",     T_SHORT,   \"S\");\n+  CHECK_OFFSET(\"java\/lang\/Integer\",   T_INT,     \"I\");\n+  CHECK_OFFSET(\"java\/lang\/Long\",      T_LONG,    \"J\");\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":104,"deletions":66,"binary":false,"changes":170,"status":"modified"},{"patch":"@@ -216,0 +216,1 @@\n+  static inline bool is_instance_without_asserts(oop obj);\n@@ -255,0 +256,1 @@\n+\n@@ -262,0 +264,1 @@\n+  static int _is_identity_offset;\n@@ -276,0 +279,1 @@\n+  static void set_is_identity(oop java_class, bool value);\n@@ -323,0 +327,1 @@\n+\n@@ -834,1 +839,1 @@\n-  static int _trusted_final_offset;\n+  static int _flags_offset;\n@@ -862,1 +867,1 @@\n-  static void set_trusted_final(oop field);\n+  static void set_flags(oop field, int value);\n@@ -984,2 +989,1 @@\n-  static int _value_offset;\n-  static int _long_value_offset;\n+  static int* _offsets;\n@@ -1002,1 +1006,3 @@\n-    return is_double_word_type(type) ? _long_value_offset : _value_offset;\n+    assert(type >= T_BOOLEAN && type <= T_LONG, \"BasicType out of range\");\n+    assert(_offsets != nullptr, \"Uninitialized offsets\");\n+    return _offsets[type - T_BOOLEAN];\n@@ -1364,1 +1370,1 @@\n-    MN_IS_CONSTRUCTOR        = 0x00020000, \/\/ constructor\n+    MN_IS_OBJECT_CONSTRUCTOR = 0x00020000, \/\/ constructor\n@@ -1370,0 +1376,1 @@\n+    MN_NULL_RESTRICTED_FIELD = 0x00800000, \/\/ null-restricted field\n@@ -1371,1 +1378,3 @@\n-    MN_REFERENCE_KIND_MASK   = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT,\n+    MN_REFERENCE_KIND_MASK   = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT, \/\/ 4 bits\n+    MN_LAYOUT_SHIFT          = 28, \/\/ field layout\n+    MN_LAYOUT_MASK           = 0x70000000 >> MN_LAYOUT_SHIFT, \/\/ 3 bits\n@@ -1875,1 +1884,0 @@\n-\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":16,"deletions":8,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -295,0 +295,1 @@\n+  oop obj_buffer_allocate(Klass* klass, size_t size, TRAPS); \/\/ doesn't clear memory\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -316,0 +316,1 @@\n+  f(InlineLayoutInfo) \\\n","filename":"src\/hotspot\/share\/memory\/allocation.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -69,0 +69,1 @@\n+#include \"oops\/refArrayKlass.hpp\"\n@@ -116,0 +117,4 @@\n+static LatestMethodCache _is_substitutable_cache;           \/\/ ValueObjectMethods.isSubstitutable()\n+static LatestMethodCache _value_object_hash_code_cache;     \/\/ ValueObjectMethods.valueObjectHashCode()\n+static LatestMethodCache _is_substitutable_alt_cache;       \/\/ ValueObjectMethods.isSubstitutableAlt()\n+static LatestMethodCache _value_object_hash_code_alt_cache; \/\/ ValueObjectMethods.valueObjectHashCodeAlt()\n@@ -513,2 +518,6 @@\n-    Klass* oak = vmClasses::Object_klass()->array_klass(CHECK);\n-    _objectArrayKlass = ObjArrayKlass::cast(oak);\n+    ArrayKlass* oak = vmClasses::Object_klass()->array_klass(CHECK);\n+    oak->append_to_sibling_list();\n+\n+    \/\/ Create a RefArrayKlass (which is the default) and initialize.\n+    ObjArrayKlass* rak = ObjArrayKlass::cast(oak)->klass_with_properties(ArrayKlass::ArrayProperties::DEFAULT, THREAD);\n+    _objectArrayKlass = rak;\n@@ -516,7 +525,0 @@\n-  \/\/ OLD\n-  \/\/ Add the class to the class hierarchy manually to make sure that\n-  \/\/ its vtable is initialized after core bootstrapping is completed.\n-  \/\/ ---\n-  \/\/ New\n-  \/\/ Have already been initialized.\n-  _objectArrayKlass->append_to_sibling_list();\n@@ -657,0 +659,3 @@\n+\n+  \/\/ This isn't added to the subclass list, so need to reinitialize vtables directly.\n+  Universe::objectArrayKlass()->vtable().initialize_vtable();\n@@ -1081,5 +1086,9 @@\n-Method* Universe::finalizer_register_method()     { return _finalizer_register_cache.get_method(); }\n-Method* Universe::loader_addClass_method()        { return _loader_addClass_cache.get_method(); }\n-Method* Universe::throw_illegal_access_error()    { return _throw_illegal_access_error_cache.get_method(); }\n-Method* Universe::throw_no_such_method_error()    { return _throw_no_such_method_error_cache.get_method(); }\n-Method* Universe::do_stack_walk_method()          { return _do_stack_walk_cache.get_method(); }\n+Method* Universe::finalizer_register_method()        { return _finalizer_register_cache.get_method(); }\n+Method* Universe::loader_addClass_method()           { return _loader_addClass_cache.get_method(); }\n+Method* Universe::throw_illegal_access_error()       { return _throw_illegal_access_error_cache.get_method(); }\n+Method* Universe::throw_no_such_method_error()       { return _throw_no_such_method_error_cache.get_method(); }\n+Method* Universe::do_stack_walk_method()             { return _do_stack_walk_cache.get_method(); }\n+Method* Universe::is_substitutable_method()          { return _is_substitutable_cache.get_method(); }\n+Method* Universe::value_object_hash_code_method()    { return _value_object_hash_code_cache.get_method(); }\n+Method* Universe::is_substitutableAlt_method()       { return _is_substitutable_alt_cache.get_method(); }\n+Method* Universe::value_object_hash_codeAlt_method() { return _value_object_hash_code_alt_cache.get_method(); }\n@@ -1115,0 +1124,19 @@\n+\n+  \/\/ Set up substitutability testing\n+  ResourceMark rm(current);\n+  _is_substitutable_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::isSubstitutable_name()->as_C_string(),\n+                          vmSymbols::object_object_boolean_signature(), true);\n+  _value_object_hash_code_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::valueObjectHashCode_name()->as_C_string(),\n+                          vmSymbols::object_int_signature(), true);\n+  _is_substitutable_alt_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::isSubstitutableAlt_name()->as_C_string(),\n+                          vmSymbols::object_object_boolean_signature(), true);\n+  _value_object_hash_code_alt_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::valueObjectHashCodeAlt_name()->as_C_string(),\n+                          vmSymbols::object_int_signature(), true);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":42,"deletions":14,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -66,0 +66,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -72,0 +73,1 @@\n+#include \"oops\/markWord.hpp\"\n@@ -75,0 +77,1 @@\n+#include \"oops\/refArrayKlass.hpp\"\n@@ -152,0 +155,5 @@\n+void InlineLayoutInfo::metaspace_pointers_do(MetaspaceClosure* it) {\n+  log_trace(cds)(\"Iter(InlineFieldInfo): %p\", this);\n+  it->push(&_klass);\n+}\n+\n@@ -173,0 +181,13 @@\n+bool InstanceKlass::field_is_null_free_inline_type(int index) const {\n+  return field(index).field_flags().is_null_free_inline_type();\n+}\n+\n+bool InstanceKlass::is_class_in_loadable_descriptors_attribute(Symbol* name) const {\n+  if (_loadable_descriptors == nullptr) return false;\n+  for (int i = 0; i < _loadable_descriptors->length(); i++) {\n+        Symbol* class_name = _constants->symbol_at(_loadable_descriptors->at(i));\n+        if (class_name == name) return true;\n+  }\n+  return false;\n+}\n+\n@@ -462,1 +483,2 @@\n-                                       parser.is_interface());\n+                                       parser.is_interface(),\n+                                       parser.is_inline_type());\n@@ -484,0 +506,3 @@\n+  } else if (parser.is_inline_type()) {\n+    \/\/ inline type\n+    ik = new (loader_data, size, THREAD) InlineKlass(parser);\n@@ -500,0 +525,6 @@\n+#ifdef ASSERT\n+  ik->bounds_check((address) ik->start_of_vtable(), false, size);\n+  ik->bounds_check((address) ik->start_of_itable(), false, size);\n+  ik->bounds_check((address) ik->end_of_itable(), true, size);\n+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);\n+#endif \/\/ASSERT\n@@ -503,0 +534,23 @@\n+#ifndef PRODUCT\n+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {\n+  const char* bad = nullptr;\n+  address end = nullptr;\n+  if (addr < (address)this) {\n+    bad = \"before\";\n+  } else if (addr == (address)this) {\n+    if (edge_ok)  return true;\n+    bad = \"just before\";\n+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {\n+    if (edge_ok)  return true;\n+    bad = \"just after\";\n+  } else if (addr > end) {\n+    bad = \"after\";\n+  } else {\n+    return true;\n+  }\n+  tty->print_cr(\"%s object bounds: \" INTPTR_FORMAT \" [\" INTPTR_FORMAT \"..\" INTPTR_FORMAT \"]\",\n+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);\n+  Verbose = WizardMode = true; this->print(); \/\/@@\n+  return false;\n+}\n+#endif \/\/PRODUCT\n@@ -530,2 +584,2 @@\n-InstanceKlass::InstanceKlass(const ClassFileParser& parser, KlassKind kind, ReferenceType reference_type) :\n-  Klass(kind),\n+InstanceKlass::InstanceKlass(const ClassFileParser& parser, KlassKind kind, markWord prototype_header, ReferenceType reference_type) :\n+  Klass(kind, prototype_header),\n@@ -542,1 +596,5 @@\n-  _init_thread(nullptr)\n+  _acmp_maps_offset(0),\n+  _init_thread(nullptr),\n+  _inline_layout_info_array(nullptr),\n+  _loadable_descriptors(nullptr),\n+  _adr_inlineklass_fixed_block(nullptr)\n@@ -549,0 +607,3 @@\n+  if (parser.has_inline_fields()) {\n+    set_has_inline_type_fields();\n+  }\n@@ -693,0 +754,5 @@\n+  if (inline_layout_info_array() != nullptr) {\n+    MetadataFactory::free_array<InlineLayoutInfo>(loader_data, inline_layout_info_array());\n+  }\n+  set_inline_layout_info_array(nullptr);\n+\n@@ -727,0 +793,7 @@\n+  if (loadable_descriptors() != nullptr &&\n+      loadable_descriptors() != Universe::the_empty_short_array() &&\n+      !loadable_descriptors()->in_aot_cache()) {\n+    MetadataFactory::free_array<jushort>(loader_data, loadable_descriptors());\n+  }\n+  set_loadable_descriptors(nullptr);\n+\n@@ -921,0 +994,38 @@\n+static void load_classes_from_loadable_descriptors_attribute(InstanceKlass *ik, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  if (ik->loadable_descriptors() != nullptr && PreloadClasses) {\n+    HandleMark hm(THREAD);\n+    for (int i = 0; i < ik->loadable_descriptors()->length(); i++) {\n+      Symbol* sig = ik->constants()->symbol_at(ik->loadable_descriptors()->at(i));\n+      if (!Signature::has_envelope(sig)) continue;\n+      TempNewSymbol class_name = Signature::strip_envelope(sig);\n+      if (class_name == ik->name()) continue;\n+      log_info(class, preload)(\"Preloading of class %s during linking of class %s \"\n+                               \"because of the class is listed in the LoadableDescriptors attribute\",\n+                               sig->as_C_string(), ik->name()->as_C_string());\n+      oop loader = ik->class_loader();\n+      Klass* klass = SystemDictionary::resolve_or_null(class_name,\n+                                                        Handle(THREAD, loader), THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        CLEAR_PENDING_EXCEPTION;\n+      }\n+      if (klass != nullptr) {\n+        log_info(class, preload)(\"Preloading of class %s during linking of class %s \"\n+                                 \"(cause: LoadableDescriptors attribute) succeeded\",\n+                                 class_name->as_C_string(), ik->name()->as_C_string());\n+        if (!klass->is_inline_klass()) {\n+          \/\/ Non value class are allowed by the current spec, but it could be an indication\n+          \/\/ of an issue so let's log a warning\n+          log_info(class, preload)(\"Preloading of class %s during linking of class %s \"\n+                                      \"(cause: LoadableDescriptors attribute) but loaded class is not a value class\",\n+                                      class_name->as_C_string(), ik->name()->as_C_string());\n+        }\n+      } else {\n+        log_info(class, preload)(\"Preloading of class %s during linking of class %s \"\n+                                    \"(cause: LoadableDescriptors attribute) failed\",\n+                                    class_name->as_C_string(), ik->name()->as_C_string());\n+      }\n+    }\n+  }\n+}\n+\n@@ -991,0 +1102,7 @@\n+  if (EnableValhalla) {\n+    \/\/ Aggressively preloading all classes from the LoadableDescriptors attribute\n+    \/\/ so inline classes can be scalarized in the calling conventions computed below\n+    load_classes_from_loadable_descriptors_attribute(this, THREAD);\n+    assert(!HAS_PENDING_EXCEPTION, \"Shouldn't have pending exceptions from call above\");\n+  }\n+\n@@ -1314,0 +1432,21 @@\n+  \/\/ Pre-allocating an all-zero value to be used to reset nullable flat storages\n+  if (is_inline_klass()) {\n+      InlineKlass* vk = InlineKlass::cast(this);\n+      if (vk->has_nullable_atomic_layout()) {\n+        oop val = vk->allocate_instance(THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+            Handle e(THREAD, PENDING_EXCEPTION);\n+            CLEAR_PENDING_EXCEPTION;\n+            {\n+                EXCEPTION_MARK;\n+                add_initialization_error(THREAD, e);\n+                \/\/ Locks object, set state, and notify all waiting threads\n+                set_initialization_state_and_notify(initialization_error, THREAD);\n+                CLEAR_PENDING_EXCEPTION;\n+            }\n+            THROW_OOP(e());\n+        }\n+        vk->set_null_reset_value(val);\n+      }\n+  }\n+\n@@ -1346,1 +1485,0 @@\n-\n@@ -1367,0 +1505,30 @@\n+\n+    if (has_strict_static_fields() && !HAS_PENDING_EXCEPTION) {\n+      \/\/ Step 9 also verifies that strict static fields have been initialized.\n+      \/\/ Status bits were set in ClassFileParser::post_process_parsed_stream.\n+      \/\/ After <clinit>, bits must all be clear, or else we must throw an error.\n+      \/\/ This is an extremely fast check, so we won't bother with a timer.\n+      assert(fields_status() != nullptr, \"\");\n+      Symbol* bad_strict_static = nullptr;\n+      for (int index = 0; index < fields_status()->length(); index++) {\n+        \/\/ Very fast loop over single byte array looking for a set bit.\n+        if (fields_status()->adr_at(index)->is_strict_static_unset()) {\n+          \/\/ This strict static field has not been set by the class initializer.\n+          \/\/ Note that in the common no-error case, we read no field metadata.\n+          \/\/ We only unpack it when we need to report an error.\n+          FieldInfo fi = field(index);\n+          bad_strict_static = fi.name(constants());\n+          if (debug_logging_enabled) {\n+            ResourceMark rm(jt);\n+            const char* msg = format_strict_static_message(bad_strict_static);\n+            log_debug(class, init)(\"%s\", msg);\n+          } else {\n+            \/\/ If we are not logging, do not bother to look for a second offense.\n+            break;\n+          }\n+        }\n+      }\n+      if (bad_strict_static != nullptr) {\n+        throw_strict_static_exception(bad_strict_static, \"is unset after initialization of\", THREAD);\n+      }\n+    }\n@@ -1420,0 +1588,68 @@\n+void InstanceKlass::notify_strict_static_access(int field_index, bool is_writing, TRAPS) {\n+  guarantee(field_index >= 0 && field_index < fields_status()->length(), \"valid field index\");\n+  DEBUG_ONLY(FieldInfo debugfi = field(field_index));\n+  assert(debugfi.access_flags().is_strict(), \"\");\n+  assert(debugfi.access_flags().is_static(), \"\");\n+  FieldStatus& fs = *fields_status()->adr_at(field_index);\n+  LogTarget(Trace, class, init) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm(THREAD);\n+    LogStream ls(lt);\n+    FieldInfo fi = field(field_index);\n+    ls.print(\"notify %s %s %s%s \",\n+             external_name(), is_writing? \"Write\" : \"Read\",\n+             fs.is_strict_static_unset() ? \"Unset\" : \"(set)\",\n+             fs.is_strict_static_unread() ? \"+Unread\" : \"\");\n+    fi.print(&ls, constants());\n+  }\n+  if (fs.is_strict_static_unset()) {\n+    assert(fs.is_strict_static_unread(), \"ClassFileParser resp.\");\n+    \/\/ If it is not set, there are only two reasonable things we can do here:\n+    \/\/ - mark it set if this is putstatic\n+    \/\/ - throw an error (Read-Before-Write) if this is getstatic\n+\n+    \/\/ The unset state is (or should be) transient, and observable only in one\n+    \/\/ thread during the execution of <clinit>.  Something is wrong here as this\n+    \/\/ should not be possible\n+    guarantee(is_reentrant_initialization(THREAD), \"unscoped access to strict static\");\n+    if (is_writing) {\n+      \/\/ clear the \"unset\" bit, since the field is actually going to be written\n+      fs.update_strict_static_unset(false);\n+    } else {\n+      \/\/ throw an IllegalStateException, since we are reading before writing\n+      \/\/ see also InstanceKlass::initialize_impl, Step 8 (at end)\n+      Symbol* bad_strict_static = field(field_index).name(constants());\n+      throw_strict_static_exception(bad_strict_static, \"is unset before first read in\", CHECK);\n+    }\n+  } else {\n+    \/\/ Ensure no write after read for final strict statics\n+    FieldInfo fi = field(field_index);\n+    bool is_final = fi.access_flags().is_final();\n+    if (is_final) {\n+      \/\/ no final write after read, so observing a constant freezes it, as if <clinit> ended early\n+      \/\/ (maybe we could trust the constant a little earlier, before <clinit> ends)\n+      if (is_writing && !fs.is_strict_static_unread()) {\n+        Symbol* bad_strict_static = fi.name(constants());\n+        throw_strict_static_exception(bad_strict_static, \"is set after read (as final) in\", CHECK);\n+      } else if (!is_writing && fs.is_strict_static_unread()) {\n+        fs.update_strict_static_unread(false);\n+      }\n+    }\n+  }\n+}\n+\n+void InstanceKlass::throw_strict_static_exception(Symbol* field_name, const char* when, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  const char* msg = format_strict_static_message(field_name, when);\n+  THROW_MSG(vmSymbols::java_lang_IllegalStateException(), msg);\n+}\n+\n+const char* InstanceKlass::format_strict_static_message(Symbol* field_name, const char* when) {\n+  stringStream ss;\n+  ss.print(\"Strict static \\\"%s\\\" %s %s\",\n+           field_name->as_C_string(),\n+           when == nullptr ? \"is unset in\" : when,\n+           external_name());\n+  return ss.as_string();\n+}\n+\n@@ -1670,1 +1906,1 @@\n-  ObjArrayKlass* ak = array_klasses();\n+  ArrayKlass* ak = array_klasses();\n@@ -1677,2 +1913,2 @@\n-  ObjArrayKlass* oak = array_klasses_acquire();\n-  if (oak == nullptr) {\n+  ArrayKlass* ak = array_klasses_acquire();\n+  if (ak == nullptr) {\n@@ -1681,1 +1917,1 @@\n-    return oak->array_klass_or_null(n);\n+    return ak->array_klass_or_null(n);\n@@ -1698,1 +1934,1 @@\n-  if (clinit != nullptr && clinit->has_valid_initializer_flags()) {\n+  if (clinit != nullptr && clinit->is_class_initializer()) {\n@@ -1807,4 +2043,0 @@\n-bool InstanceKlass::contains_field_offset(int offset) {\n-  fieldDescriptor fd;\n-  return find_field_from_offset(offset, false, &fd);\n-}\n@@ -1891,0 +2123,9 @@\n+bool InstanceKlass::contains_field_offset(int offset) {\n+  if (this->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(this);\n+    return offset >= vk->payload_offset() && offset < (vk->payload_offset() + vk->payload_size_in_bytes());\n+  } else {\n+    fieldDescriptor fd;\n+    return find_field_from_offset(offset, false, &fd);\n+  }\n+}\n@@ -2274,0 +2515,3 @@\n+    if (name == vmSymbols::object_initializer_name()) {\n+      break;  \/\/ <init> is never inherited\n+    }\n@@ -2686,0 +2930,1 @@\n+  it->push(&_loadable_descriptors);\n@@ -2687,0 +2932,1 @@\n+  it->push(&_inline_layout_info_array, MetaspaceClosure::_writable);\n@@ -2734,1 +2980,1 @@\n-  \/\/ These are not allocated from metaspace. They are safe to set to null.\n+  \/\/ These are not allocated from metaspace. They are safe to set to nullptr.\n@@ -2825,0 +3071,4 @@\n+  if (is_inline_klass()) {\n+    InlineKlass::cast(this)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -2858,1 +3108,1 @@\n-    assert(this == array_klasses()->bottom_klass(), \"sanity\");\n+    assert(this == ObjArrayKlass::cast(array_klasses())->bottom_klass(), \"sanity\");\n@@ -2861,0 +3111,6 @@\n+    if (class_loader_data() == nullptr) {\n+      ResourceMark rm(THREAD);\n+      log_debug(cds)(\"  loader_data %s \", loader_data == nullptr ? \"nullptr\" : \"non null\");\n+      log_debug(cds)(\"  this %s array_klasses %s \", this->name()->as_C_string(), array_klasses()->name()->as_C_string());\n+    }\n+    assert(!array_klasses()->is_refined_objArray_klass(), \"must be non-refined objarrayklass\");\n@@ -3016,0 +3272,4 @@\n+bool InstanceKlass::supports_inline_types() const {\n+  return major_version() >= Verifier::VALUE_TYPES_MAJOR_VERSION && minor_version() == Verifier::JAVA_PREVIEW_MINOR_VERSION;\n+}\n+\n@@ -3048,0 +3308,2 @@\n+  return signature_name_of_carrier(JVM_SIGNATURE_CLASS);\n+}\n@@ -3049,0 +3311,1 @@\n+const char* InstanceKlass::signature_name_of_carrier(char c) const {\n@@ -3055,1 +3318,1 @@\n-  \/\/ Add L as type indicator\n+  \/\/ Add L or Q as type indicator\n@@ -3057,1 +3320,1 @@\n-  dest[dest_index++] = JVM_SIGNATURE_CLASS;\n+  dest[dest_index++] = c;\n@@ -3338,0 +3601,19 @@\n+void InstanceKlass::check_can_be_annotated_with_NullRestricted(InstanceKlass* type, Symbol* container_klass_name, TRAPS) {\n+  assert(type->is_instance_klass(), \"Sanity check\");\n+  if (type->is_identity_class()) {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+              err_msg(\"Class %s expects class %s to be a value class, but it is an identity class\",\n+              container_klass_name->as_C_string(),\n+              type->external_name()));\n+  }\n+\n+  if (type->is_abstract()) {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+              err_msg(\"Class %s expects class %s to be concrete value type, but it is an abstract class\",\n+              container_klass_name->as_C_string(),\n+              type->external_name()));\n+  }\n+}\n+\n@@ -3404,2 +3686,1 @@\n-  \/\/ Remember to strip ACC_SUPER bit\n-  return (access & (~JVM_ACC_SUPER));\n+  return access;\n@@ -3659,1 +3940,4 @@\n-static void print_vtable(intptr_t* start, int len, outputStream* st) {\n+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {\n+  ResourceMark rm;\n+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);\n+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;\n@@ -3663,0 +3947,5 @@\n+    if (forward_refs[i] != 0) {\n+      int from = forward_refs[i];\n+      int off = (int) start[from];\n+      st->print(\" (offset %d <= [%d])\", off, from);\n+    }\n@@ -3666,0 +3955,6 @@\n+    } else if (self != nullptr && e > 0 && e < 0x10000) {\n+      address location = self + e;\n+      int index = (int)((intptr_t*)location - start);\n+      st->print(\" (offset %d => [%d])\", (int)e, index);\n+      if (index >= 0 && index < len)\n+        forward_refs[index] = i;\n@@ -3672,1 +3967,22 @@\n-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);\n+  return print_vtable(nullptr, reinterpret_cast<intptr_t*>(start), len, st);\n+}\n+\n+template<typename T>\n+ static void print_array_on(outputStream* st, Array<T>* array) {\n+   if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+   array->print_value_on(st); st->cr();\n+   if (Verbose || WizardMode) {\n+     for (int i = 0; i < array->length(); i++) {\n+       st->print(\"%d : \", i); array->at(i)->print_value_on(st); st->cr();\n+     }\n+   }\n+ }\n+\n+static void print_array_on(outputStream* st, Array<int>* array) {\n+  if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+  array->print_value_on(st); st->cr();\n+  if (Verbose || WizardMode) {\n+    for (int i = 0; i < array->length(); i++) {\n+      st->print(\"%d : %d\", i, array->at(i)); st->cr();\n+    }\n+  }\n@@ -3713,8 +4029,2 @@\n-  st->print(BULLET\"methods:           \"); methods()->print_value_on(st);               st->cr();\n-  if (Verbose || WizardMode) {\n-    Array<Method*>* method_array = methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n-  st->print(BULLET\"method ordering:   \"); method_ordering()->print_value_on(st);      st->cr();\n+  st->print(BULLET\"methods:           \"); print_array_on(st, methods());\n+  st->print(BULLET\"method ordering:   \"); print_array_on(st, method_ordering());\n@@ -3722,7 +4032,1 @@\n-    st->print(BULLET\"default_methods:   \"); default_methods()->print_value_on(st);    st->cr();\n-    if (Verbose) {\n-      Array<Method*>* method_array = default_methods();\n-      for (int i = 0; i < method_array->length(); i++) {\n-        st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-      }\n-    }\n+    st->print(BULLET\"default_methods:   \"); print_array_on(st, default_methods());\n@@ -3788,0 +4092,1 @@\n+  st->print(BULLET\"loadable descriptors:     \"); loadable_descriptors()->print_value_on(st); st->cr();\n@@ -3798,1 +4103,1 @@\n-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);\n+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(nullptr, start_of_itable(), itable_length(), st);\n@@ -3830,0 +4135,1 @@\n+  for (int i = 0; i < _indent; i++) _st->print(\"  \");\n@@ -3832,1 +4138,1 @@\n-     fd->print_on(_st);\n+     fd->print_on(_st, _base_offset);\n@@ -3835,2 +4141,2 @@\n-     fd->print_on_for(_st, _obj);\n-     _st->cr();\n+     fd->print_on_for(_st, _obj, _indent, _base_offset);\n+     if (!fd->field_flags().is_flat()) _st->cr();\n@@ -3841,1 +4147,1 @@\n-void InstanceKlass::oop_print_on(oop obj, outputStream* st) {\n+void InstanceKlass::oop_print_on(oop obj, outputStream* st, int indent, int base_offset) {\n@@ -3857,1 +4163,1 @@\n-  FieldPrinter print_field(st, obj);\n+  FieldPrinter print_field(st, obj, indent, base_offset);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":349,"deletions":43,"binary":false,"changes":392,"status":"modified"},{"patch":"@@ -27,0 +27,2 @@\n+#include \"interpreter\/bytecodes.hpp\"\n+#include \"oops\/instanceOop.hpp\"\n@@ -28,0 +30,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -51,0 +54,3 @@\n+  st->print_cr(\" - Is Flat: %d\", is_flat());\n+  st->print_cr(\" - Is Null Free Inline Type: %d\", is_null_free_inline_type());\n+  st->print_cr(\" - Has null marker: %d\", has_null_marker());\n@@ -55,0 +61,9 @@\n+bool ResolvedFieldEntry::is_valid() const {\n+  return field_holder()->is_instance_klass() &&\n+    field_offset() >= instanceOopDesc::base_offset_in_bytes() && field_offset() < 0x7fffffff &&\n+    as_BasicType((TosState)tos_state()) != T_ILLEGAL &&\n+    _flags < (1 << (max_flag_shift + 1)) &&\n+    (get_code() == 0 || get_code() == Bytecodes::_getstatic || get_code() == Bytecodes::_getfield) &&\n+    (put_code() == 0 || put_code() == Bytecodes::_putstatic || put_code() == Bytecodes::_putfield);\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/resolvedFieldEntry.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -58,1 +58,1 @@\n-  u1 _flags;                    \/\/ Flags: [0000|00|is_final|is_volatile]\n+  u1 _flags;                    \/\/ Flags: [000|has_null_marker|is_null_free_inline_type|is_flat|is_final|is_volatile]\n@@ -87,0 +87,4 @@\n+      is_flat_shift         = 2,\n+      is_null_free_inline_type_shift = 3,\n+      has_null_marker_shift = 4,\n+      max_flag_shift = has_null_marker_shift\n@@ -99,0 +103,3 @@\n+  bool is_flat()                const { return (_flags & (1 << is_flat_shift))     != 0; }\n+  bool is_null_free_inline_type() const { return (_flags & (1 << is_null_free_inline_type_shift)) != 0; }\n+  bool has_null_marker()        const { return (_flags & (1 << has_null_marker_shift)) != 0; }\n@@ -116,2 +123,6 @@\n-  void set_flags(bool is_final_flag, bool is_volatile_flag) {\n-    int new_flags = (is_final_flag << is_final_shift) | static_cast<int>(is_volatile_flag);\n+  void set_flags(bool is_final_flag, bool is_volatile_flag, bool is_flat_flag, bool is_null_free_inline_type_flag,\n+                 bool has_null_marker_flag) {\n+    u1 new_flags = ((is_final_flag ? 1 : 0) << is_final_shift) | static_cast<int>(is_volatile_flag) |\n+      ((is_flat_flag ? 1 : 0) << is_flat_shift) |\n+      ((is_null_free_inline_type_flag ? 1 : 0) << is_null_free_inline_type_shift) |\n+      ((has_null_marker_flag ? 1 : 0) << has_null_marker_shift);\n@@ -121,0 +132,3 @@\n+    assert(is_flat() == is_flat_flag, \"Must be\");\n+    assert(is_null_free_inline_type() == is_null_free_inline_type_flag, \"Must be\");\n+    assert(has_null_marker() == has_null_marker_flag, \"Must be\");\n@@ -132,1 +146,1 @@\n-  \/\/ Populate the strucutre with resolution information\n+  \/\/ Populate the structure with resolution information\n@@ -142,0 +156,1 @@\n+    assert(is_valid(), \"invalid\");\n@@ -159,0 +174,2 @@\n+  \/\/ Debug help\n+  bool is_valid() const;\n","filename":"src\/hotspot\/share\/oops\/resolvedFieldEntry.hpp","additions":21,"deletions":4,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -116,3 +118,7 @@\n-  Node* src = in(ArrayCopyNode::Src);\n-  const Type* src_type = phase->type(src);\n-\n+    Node* src = in(ArrayCopyNode::Src);\n+    const Type* src_type = phase->type(src);\n+\n+    if (src_type == Type::TOP) {\n+      return -1;\n+    }\n+\n@@ -142,0 +148,1 @@\n+             (UseArrayFlattening && ary_src->elem()->make_oopptr() != nullptr && ary_src->elem()->make_oopptr()->can_be_inline_type()) ||\n@@ -195,0 +202,1 @@\n+  phase->record_for_igvn(mem);\n@@ -294,1 +302,2 @@\n-    if (src_elem != dest_elem || dest_elem == T_VOID) {\n+    \/\/ TODO 8350865 What about atomicity?\n+    if (src_elem != dest_elem || ary_src->is_null_free() != ary_dest->is_null_free() || ary_src->is_flat() != ary_dest->is_flat() || dest_elem == T_VOID) {\n@@ -300,3 +309,4 @@\n-    if (bs->array_copy_requires_gc_barriers(is_alloc_tightly_coupled(), dest_elem, false, false, BarrierSetC2::Optimization)) {\n-      \/\/ It's an object array copy but we can't emit the card marking\n-      \/\/ that is needed\n+    if ((!ary_dest->is_flat() && bs->array_copy_requires_gc_barriers(is_alloc_tightly_coupled(), dest_elem, false, false, BarrierSetC2::Optimization)) ||\n+        (ary_dest->is_flat() && ary_src->elem()->inline_klass()->contains_oops() &&\n+         bs->array_copy_requires_gc_barriers(is_alloc_tightly_coupled(), T_OBJECT, false, false, BarrierSetC2::Optimization))) {\n+      \/\/ It's an object array copy but we can't emit the card marking that is needed\n@@ -309,1 +319,8 @@\n-    uint header = arrayOopDesc::base_offset_in_bytes(dest_elem);\n+    if (ary_dest->is_flat()) {\n+      assert(ary_src->is_flat(), \"src and dest must be flat\");\n+      shift = ary_src->flat_log_elem_size();\n+      src_elem = T_FLAT_ELEMENT;\n+      dest_elem = T_FLAT_ELEMENT;\n+    }\n+\n+    const uint header = arrayOopDesc::base_offset_in_bytes(dest_elem);\n@@ -348,0 +365,5 @@\n+    if (ary_src->elem()->make_oopptr() != nullptr &&\n+        ary_src->elem()->make_oopptr()->can_be_inline_type()) {\n+      return false;\n+    }\n+\n@@ -354,1 +376,4 @@\n-    if (bs->array_copy_requires_gc_barriers(true, elem, true, is_clone_inst(), BarrierSetC2::Optimization)) {\n+    if ((!ary_src->is_flat() && bs->array_copy_requires_gc_barriers(true, elem, true, is_clone_inst(), BarrierSetC2::Optimization)) ||\n+        (ary_src->is_flat() && ary_src->elem()->inline_klass()->contains_oops() &&\n+         bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, is_clone_inst(), BarrierSetC2::Optimization))) {\n+      \/\/ It's an object array copy but we can't emit the card marking that is needed\n@@ -378,1 +403,1 @@\n-const TypePtr* ArrayCopyNode::get_address_type(PhaseGVN* phase, const TypePtr* atp, Node* n) {\n+const TypeAryPtr* ArrayCopyNode::get_address_type(PhaseGVN* phase, const TypePtr* atp, Node* n) {\n@@ -383,1 +408,1 @@\n-  return atp->add_offset(Type::OffsetBot);\n+  return atp->add_offset(Type::OffsetBot)->is_aryptr();\n@@ -386,2 +411,2 @@\n-void ArrayCopyNode::array_copy_test_overlap(PhaseGVN *phase, bool can_reshape, bool disjoint_bases, int count, Node*& forward_ctl, Node*& backward_ctl) {\n-  Node* ctl = in(TypeFunc::Control);\n+void ArrayCopyNode::array_copy_test_overlap(GraphKit& kit, bool disjoint_bases, int count, Node*& backward_ctl) {\n+  Node* ctl = kit.control();\n@@ -389,0 +414,1 @@\n+    PhaseGVN& gvn = kit.gvn();\n@@ -392,2 +418,2 @@\n-    Node* cmp = phase->transform(new CmpINode(src_offset, dest_offset));\n-    Node *bol = phase->transform(new BoolNode(cmp, BoolTest::lt));\n+    Node* cmp = gvn.transform(new CmpINode(src_offset, dest_offset));\n+    Node *bol = gvn.transform(new BoolNode(cmp, BoolTest::lt));\n@@ -396,1 +422,1 @@\n-    phase->transform(iff);\n+    gvn.transform(iff);\n@@ -398,2 +424,34 @@\n-    forward_ctl = phase->transform(new IfFalseNode(iff));\n-    backward_ctl = phase->transform(new IfTrueNode(iff));\n+    kit.set_control(gvn.transform(new IfFalseNode(iff)));\n+    backward_ctl = gvn.transform(new IfTrueNode(iff));\n+  }\n+}\n+\n+void ArrayCopyNode::copy(GraphKit& kit,\n+                         const TypeAryPtr* atp_src,\n+                         const TypeAryPtr* atp_dest,\n+                         int i,\n+                         Node* base_src,\n+                         Node* base_dest,\n+                         Node* adr_src,\n+                         Node* adr_dest,\n+                         BasicType copy_type,\n+                         const Type* value_type) {\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  Node* ctl = kit.control();\n+  if (atp_dest->is_flat()) {\n+    ciInlineKlass* vk = atp_src->elem()->inline_klass();\n+    for (int j = 0; j < vk->nof_nonstatic_fields(); j++) {\n+      ciField* field = vk->nonstatic_field_at(j);\n+      int off_in_vt = field->offset_in_bytes() - vk->payload_offset();\n+      Node* off  = kit.MakeConX(off_in_vt + i * atp_src->flat_elem_size());\n+      ciType* ft = field->type();\n+      BasicType bt = type2field[ft->basic_type()];\n+      assert(!field->is_flat(), \"flat field encountered\");\n+      const Type* rt = Type::get_const_type(ft);\n+      const TypePtr* adr_type = atp_src->with_field_offset(off_in_vt)->add_offset(Type::OffsetBot);\n+      assert(!bs->array_copy_requires_gc_barriers(is_alloc_tightly_coupled(), bt, false, false, BarrierSetC2::Optimization), \"GC barriers required\");\n+      Node* next_src = kit.gvn().transform(new AddPNode(base_src, adr_src, off));\n+      Node* next_dest = kit.gvn().transform(new AddPNode(base_dest, adr_dest, off));\n+      Node* v = load(bs, &kit.gvn(), ctl, kit.merged_memory(), next_src, adr_type, rt, bt);\n+      store(bs, &kit.gvn(), ctl, kit.merged_memory(), next_dest, adr_type, v, rt, bt);\n+    }\n@@ -401,1 +459,5 @@\n-    forward_ctl = ctl;\n+    Node* off = kit.MakeConX(type2aelembytes(copy_type) * i);\n+    Node* next_src = kit.gvn().transform(new AddPNode(base_src, adr_src, off));\n+    Node* next_dest = kit.gvn().transform(new AddPNode(base_dest, adr_dest, off));\n+    Node* v = load(bs, &kit.gvn(), ctl, kit.merged_memory(), next_src, atp_src, value_type, copy_type);\n+    store(bs, &kit.gvn(), ctl, kit.merged_memory(), next_dest, atp_dest, v, value_type, copy_type);\n@@ -403,0 +465,1 @@\n+  kit.set_control(ctl);\n@@ -405,16 +468,13 @@\n-Node* ArrayCopyNode::array_copy_forward(PhaseGVN *phase,\n-                                        bool can_reshape,\n-                                        Node*& forward_ctl,\n-                                        Node* mem,\n-                                        const TypePtr* atp_src,\n-                                        const TypePtr* atp_dest,\n-                                        Node* adr_src,\n-                                        Node* base_src,\n-                                        Node* adr_dest,\n-                                        Node* base_dest,\n-                                        BasicType copy_type,\n-                                        const Type* value_type,\n-                                        int count) {\n-  if (!forward_ctl->is_top()) {\n-    \/\/ copy forward\n-    MergeMemNode* mm = MergeMemNode::make(mem);\n+void ArrayCopyNode::array_copy_forward(GraphKit& kit,\n+                                       bool can_reshape,\n+                                       const TypeAryPtr* atp_src,\n+                                       const TypeAryPtr* atp_dest,\n+                                       Node* adr_src,\n+                                       Node* base_src,\n+                                       Node* adr_dest,\n+                                       Node* base_dest,\n+                                       BasicType copy_type,\n+                                       const Type* value_type,\n+                                       int count) {\n+  if (!kit.stopped()) {\n+    \/\/ copy forward\n@@ -423,9 +483,2 @@\n-      BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n-      Node* v = load(bs, phase, forward_ctl, mm, adr_src, atp_src, value_type, copy_type);\n-      store(bs, phase, forward_ctl, mm, adr_dest, atp_dest, v, value_type, copy_type);\n-      for (int i = 1; i < count; i++) {\n-        Node* off  = phase->MakeConX(type2aelembytes(copy_type) * i);\n-        Node* next_src = phase->transform(new AddPNode(base_src,adr_src,off));\n-        Node* next_dest = phase->transform(new AddPNode(base_dest,adr_dest,off));\n-        v = load(bs, phase, forward_ctl, mm, next_src, atp_src, value_type, copy_type);\n-        store(bs, phase, forward_ctl, mm, next_dest, atp_dest, v, value_type, copy_type);\n+      for (int i = 0; i < count; i++) {\n+        copy(kit, atp_src, atp_dest, i, base_src, base_dest, adr_src, adr_dest, copy_type, value_type);\n@@ -434,3 +487,4 @@\n-      PhaseIterGVN* igvn = phase->is_IterGVN();\n-      igvn->_worklist.push(adr_src);\n-      igvn->_worklist.push(adr_dest);\n+      PhaseGVN& gvn = kit.gvn();\n+      assert(gvn.is_IterGVN(), \"\");\n+      gvn.record_for_igvn(adr_src);\n+      gvn.record_for_igvn(adr_dest);\n@@ -438,2 +492,0 @@\n-    return mm;\n-  return phase->C->top();\n@@ -443,14 +495,12 @@\n-Node* ArrayCopyNode::array_copy_backward(PhaseGVN *phase,\n-                                         bool can_reshape,\n-                                         Node*& backward_ctl,\n-                                         Node* mem,\n-                                         const TypePtr* atp_src,\n-                                         const TypePtr* atp_dest,\n-                                         Node* adr_src,\n-                                         Node* base_src,\n-                                         Node* adr_dest,\n-                                         Node* base_dest,\n-                                         BasicType copy_type,\n-                                         const Type* value_type,\n-                                         int count) {\n-  if (!backward_ctl->is_top()) {\n+void ArrayCopyNode::array_copy_backward(GraphKit& kit,\n+                                        bool can_reshape,\n+                                        const TypeAryPtr* atp_src,\n+                                        const TypeAryPtr* atp_dest,\n+                                        Node* adr_src,\n+                                        Node* base_src,\n+                                        Node* adr_dest,\n+                                        Node* base_dest,\n+                                        BasicType copy_type,\n+                                        const Type* value_type,\n+                                        int count) {\n+  if (!kit.stopped()) {\n@@ -458,4 +508,1 @@\n-    MergeMemNode* mm = MergeMemNode::make(mem);\n-\n-    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n-    assert(copy_type != T_OBJECT || !bs->array_copy_requires_gc_barriers(false, T_OBJECT, false, false, BarrierSetC2::Optimization), \"only tightly coupled allocations for object arrays\");\n+    PhaseGVN& gvn = kit.gvn();\n@@ -464,6 +511,2 @@\n-      for (int i = count-1; i >= 1; i--) {\n-        Node* off  = phase->MakeConX(type2aelembytes(copy_type) * i);\n-        Node* next_src = phase->transform(new AddPNode(base_src,adr_src,off));\n-        Node* next_dest = phase->transform(new AddPNode(base_dest,adr_dest,off));\n-        Node* v = load(bs, phase, backward_ctl, mm, next_src, atp_src, value_type, copy_type);\n-        store(bs, phase, backward_ctl, mm, next_dest, atp_dest, v, value_type, copy_type);\n+      for (int i = count-1; i >= 0; i--) {\n+        copy(kit, atp_src, atp_dest, i, base_src, base_dest, adr_src, adr_dest, copy_type, value_type);\n@@ -471,6 +514,5 @@\n-      Node* v = load(bs, phase, backward_ctl, mm, adr_src, atp_src, value_type, copy_type);\n-      store(bs, phase, backward_ctl, mm, adr_dest, atp_dest, v, value_type, copy_type);\n-    } else if (can_reshape) {\n-      PhaseIterGVN* igvn = phase->is_IterGVN();\n-      igvn->_worklist.push(adr_src);\n-      igvn->_worklist.push(adr_dest);\n+    } else if(can_reshape) {\n+      PhaseGVN& gvn = kit.gvn();\n+      assert(gvn.is_IterGVN(), \"\");\n+      gvn.record_for_igvn(adr_src);\n+      gvn.record_for_igvn(adr_dest);\n@@ -478,2 +520,0 @@\n-    return phase->transform(mm);\n-  return phase->C->top();\n@@ -505,2 +545,1 @@\n-      CallProjections callprojs;\n-      extract_projections(&callprojs, true, false);\n+      CallProjections* callprojs = extract_projections(true, false);\n@@ -508,2 +547,2 @@\n-      if (callprojs.fallthrough_ioproj != nullptr) {\n-        igvn->replace_node(callprojs.fallthrough_ioproj, in(TypeFunc::I_O));\n+      if (callprojs->fallthrough_ioproj != nullptr) {\n+        igvn->replace_node(callprojs->fallthrough_ioproj, in(TypeFunc::I_O));\n@@ -511,2 +550,2 @@\n-      if (callprojs.fallthrough_memproj != nullptr) {\n-        igvn->replace_node(callprojs.fallthrough_memproj, mem);\n+      if (callprojs->fallthrough_memproj != nullptr) {\n+        igvn->replace_node(callprojs->fallthrough_memproj, mem);\n@@ -514,2 +553,2 @@\n-      if (callprojs.fallthrough_catchproj != nullptr) {\n-        igvn->replace_node(callprojs.fallthrough_catchproj, ctl);\n+      if (callprojs->fallthrough_catchproj != nullptr) {\n+        igvn->replace_node(callprojs->fallthrough_catchproj, ctl);\n@@ -540,1 +579,5 @@\n-  if (remove_dead_region(phase, can_reshape))  return this;\n+  \/\/ Perform any generic optimizations first\n+  Node* result = SafePointNode::Ideal(phase, can_reshape);\n+  if (result != nullptr) {\n+    return result;\n+  }\n@@ -582,0 +625,11 @@\n+  Node* src = in(ArrayCopyNode::Src);\n+  Node* dest = in(ArrayCopyNode::Dest);\n+  const Type* src_type = phase->type(src);\n+  const Type* dest_type = phase->type(dest);\n+\n+  if (src_type->isa_aryptr() && dest_type->isa_instptr()) {\n+    \/\/ clone used for load of unknown inline type can't be optimized at\n+    \/\/ this point\n+    return nullptr;\n+  }\n+\n@@ -603,5 +657,21 @@\n-  Node* src = in(ArrayCopyNode::Src);\n-  Node* dest = in(ArrayCopyNode::Dest);\n-  const TypePtr* atp_src = get_address_type(phase, _src_type, src);\n-  const TypePtr* atp_dest = get_address_type(phase, _dest_type, dest);\n-  Node* in_mem = in(TypeFunc::Memory);\n+  JVMState* new_jvms = nullptr;\n+  SafePointNode* new_map = nullptr;\n+  if (!is_clonebasic()) {\n+    new_jvms = jvms()->clone_shallow(phase->C);\n+    new_map = new SafePointNode(req(), new_jvms);\n+    for (uint i = TypeFunc::FramePtr; i < req(); i++) {\n+      new_map->init_req(i, in(i));\n+    }\n+    new_jvms->set_map(new_map);\n+  } else {\n+    new_jvms = new (phase->C) JVMState(0);\n+    new_map = new SafePointNode(TypeFunc::Parms, new_jvms);\n+    new_jvms->set_map(new_map);\n+  }\n+  new_map->set_control(in(TypeFunc::Control));\n+  new_map->set_memory(MergeMemNode::make(in(TypeFunc::Memory)));\n+  new_map->set_i_o(in(TypeFunc::I_O));\n+  phase->record_for_igvn(new_map);\n+\n+  const TypeAryPtr* atp_src = get_address_type(phase, _src_type, src);\n+  const TypeAryPtr* atp_dest = get_address_type(phase, _dest_type, dest);\n@@ -614,0 +684,4 @@\n+  GraphKit kit(new_jvms, phase);\n+\n+  SafePointNode* backward_map = nullptr;\n+  SafePointNode* forward_map = nullptr;\n@@ -615,36 +689,36 @@\n-  Node* forward_ctl = phase->C->top();\n-  array_copy_test_overlap(phase, can_reshape, disjoint_bases, count, forward_ctl, backward_ctl);\n-\n-  Node* forward_mem = array_copy_forward(phase, can_reshape, forward_ctl,\n-                                         in_mem,\n-                                         atp_src, atp_dest,\n-                                         adr_src, base_src, adr_dest, base_dest,\n-                                         copy_type, value_type, count);\n-\n-  Node* backward_mem = array_copy_backward(phase, can_reshape, backward_ctl,\n-                                           in_mem,\n-                                           atp_src, atp_dest,\n-                                           adr_src, base_src, adr_dest, base_dest,\n-                                           copy_type, value_type, count);\n-\n-  Node* ctl = nullptr;\n-  if (!forward_ctl->is_top() && !backward_ctl->is_top()) {\n-    ctl = new RegionNode(3);\n-    ctl->init_req(1, forward_ctl);\n-    ctl->init_req(2, backward_ctl);\n-    ctl = phase->transform(ctl);\n-    MergeMemNode* forward_mm = forward_mem->as_MergeMem();\n-    MergeMemNode* backward_mm = backward_mem->as_MergeMem();\n-    for (MergeMemStream mms(forward_mm, backward_mm); mms.next_non_empty2(); ) {\n-      if (mms.memory() != mms.memory2()) {\n-        Node* phi = new PhiNode(ctl, Type::MEMORY, phase->C->get_adr_type(mms.alias_idx()));\n-        phi->init_req(1, mms.memory());\n-        phi->init_req(2, mms.memory2());\n-        phi = phase->transform(phi);\n-        mms.set_memory(phi);\n-      }\n-    }\n-    mem = forward_mem;\n-  } else if (!forward_ctl->is_top()) {\n-    ctl = forward_ctl;\n-    mem = forward_mem;\n+\n+  array_copy_test_overlap(kit, disjoint_bases, count, backward_ctl);\n+\n+  {\n+    PreserveJVMState pjvms(&kit);\n+\n+    array_copy_forward(kit, can_reshape,\n+                       atp_src, atp_dest,\n+                       adr_src, base_src, adr_dest, base_dest,\n+                       copy_type, value_type, count);\n+\n+    forward_map = kit.stop();\n+  }\n+\n+  kit.set_control(backward_ctl);\n+  array_copy_backward(kit, can_reshape,\n+                      atp_src, atp_dest,\n+                      adr_src, base_src, adr_dest, base_dest,\n+                      copy_type, value_type, count);\n+\n+  backward_map = kit.stop();\n+\n+  if (!forward_map->control()->is_top() && !backward_map->control()->is_top()) {\n+    assert(forward_map->i_o() == backward_map->i_o(), \"need a phi on IO?\");\n+    Node* ctl = new RegionNode(3);\n+    Node* mem = new PhiNode(ctl, Type::MEMORY, TypePtr::BOTTOM);\n+    kit.set_map(forward_map);\n+    ctl->init_req(1, kit.control());\n+    mem->init_req(1, kit.reset_memory());\n+    kit.set_map(backward_map);\n+    ctl->init_req(2, kit.control());\n+    mem->init_req(2, kit.reset_memory());\n+    kit.set_control(phase->transform(ctl));\n+    kit.set_all_memory(phase->transform(mem));\n+  } else if (!forward_map->control()->is_top()) {\n+    kit.set_map(forward_map);\n@@ -652,3 +726,2 @@\n-    assert(!backward_ctl->is_top(), \"no copy?\");\n-    ctl = backward_ctl;\n-    mem = backward_mem;\n+    assert(!backward_map->control()->is_top(), \"no copy?\");\n+    kit.set_map(backward_map);\n@@ -662,2 +735,5 @@\n-  if (!finish_transform(phase, can_reshape, ctl, mem)) {\n-    if (can_reshape) {\n+  mem = kit.map()->memory();\n+  if (!finish_transform(phase, can_reshape, kit.control(), mem)) {\n+    if (!can_reshape) {\n+      phase->record_for_igvn(this);\n+    } else {\n@@ -760,2 +836,9 @@\n-  uint header = arrayOopDesc::base_offset_in_bytes(ary_elem);\n-  uint elemsize = type2aelembytes(ary_elem);\n+  uint header;\n+  uint elem_size;\n+  if (ary_t->is_flat()) {\n+    header = arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT);\n+    elem_size = ary_t->flat_elem_size();\n+  } else {\n+    header = arrayOopDesc::base_offset_in_bytes(ary_elem);\n+    elem_size = type2aelembytes(ary_elem);\n+  }\n@@ -763,4 +846,4 @@\n-  jlong dest_pos_plus_len_lo = (((jlong)dest_pos_t->_lo) + len_t->_lo) * elemsize + header;\n-  jlong dest_pos_plus_len_hi = (((jlong)dest_pos_t->_hi) + len_t->_hi) * elemsize + header;\n-  jlong dest_pos_lo = ((jlong)dest_pos_t->_lo) * elemsize + header;\n-  jlong dest_pos_hi = ((jlong)dest_pos_t->_hi) * elemsize + header;\n+  jlong dest_pos_plus_len_lo = (((jlong)dest_pos_t->_lo) + len_t->_lo) * elem_size + header;\n+  jlong dest_pos_plus_len_hi = (((jlong)dest_pos_t->_hi) + len_t->_hi) * elem_size + header;\n+  jlong dest_pos_lo = ((jlong)dest_pos_t->_lo) * elem_size + header;\n+  jlong dest_pos_hi = ((jlong)dest_pos_t->_hi) * elem_size + header;\n","filename":"src\/hotspot\/share\/opto\/arraycopynode.cpp","additions":225,"deletions":142,"binary":false,"changes":367,"status":"modified"},{"patch":"@@ -103,1 +103,1 @@\n-  static const TypePtr* get_address_type(PhaseGVN* phase, const TypePtr* atp, Node* n);\n+  static const TypeAryPtr* get_address_type(PhaseGVN* phase, const TypePtr* atp, Node* n);\n@@ -109,1 +109,1 @@\n-  void array_copy_test_overlap(PhaseGVN *phase, bool can_reshape,\n+  void array_copy_test_overlap(GraphKit& kit,\n@@ -111,4 +111,7 @@\n-                               Node*& forward_ctl, Node*& backward_ctl);\n-  Node* array_copy_forward(PhaseGVN *phase, bool can_reshape, Node*& ctl,\n-                           Node* mem,\n-                           const TypePtr* atp_src, const TypePtr* atp_dest,\n+                               Node*& backward_ctl);\n+  void array_copy_forward(GraphKit& kit, bool can_reshape,\n+                          const TypeAryPtr* atp_src, const TypeAryPtr* atp_dest,\n+                          Node* adr_src, Node* base_src, Node* adr_dest, Node* base_dest,\n+                          BasicType copy_type, const Type* value_type, int count);\n+  void array_copy_backward(GraphKit& kit, bool can_reshape,\n+                           const TypeAryPtr* atp_src, const TypeAryPtr* atp_dest,\n@@ -117,5 +120,0 @@\n-  Node* array_copy_backward(PhaseGVN *phase, bool can_reshape, Node*& ctl,\n-                            Node* mem,\n-                            const TypePtr* atp_src, const TypePtr* atp_dest,\n-                            Node* adr_src, Node* base_src, Node* adr_dest, Node* base_dest,\n-                            BasicType copy_type, const Type* value_type, int count);\n@@ -124,0 +122,4 @@\n+  void copy(GraphKit& kit, const TypeAryPtr* atp_src, const TypeAryPtr* atp_dest, int i,\n+            Node* base_src, Node* base_dest, Node* adr_src, Node* adr_dest,\n+            BasicType copy_type, const Type* value_type);\n+\n","filename":"src\/hotspot\/share\/opto\/arraycopynode.hpp","additions":13,"deletions":11,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"classfile\/vmIntrinsics.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"jvm_io.h\"\n@@ -40,0 +42,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -89,0 +92,58 @@\n+static bool arg_can_be_larval(ciMethod* callee, int arg_idx) {\n+  if (callee->is_object_constructor() && arg_idx == 0) {\n+    return true;\n+  }\n+\n+  if (arg_idx != 1 || callee->intrinsic_id() == vmIntrinsicID::_none) {\n+    return false;\n+  }\n+\n+  switch (callee->intrinsic_id()) {\n+    case vmIntrinsicID::_finishPrivateBuffer:\n+    case vmIntrinsicID::_putBoolean:\n+    case vmIntrinsicID::_putBooleanOpaque:\n+    case vmIntrinsicID::_putBooleanRelease:\n+    case vmIntrinsicID::_putBooleanVolatile:\n+    case vmIntrinsicID::_putByte:\n+    case vmIntrinsicID::_putByteOpaque:\n+    case vmIntrinsicID::_putByteRelease:\n+    case vmIntrinsicID::_putByteVolatile:\n+    case vmIntrinsicID::_putChar:\n+    case vmIntrinsicID::_putCharOpaque:\n+    case vmIntrinsicID::_putCharRelease:\n+    case vmIntrinsicID::_putCharUnaligned:\n+    case vmIntrinsicID::_putCharVolatile:\n+    case vmIntrinsicID::_putShort:\n+    case vmIntrinsicID::_putShortOpaque:\n+    case vmIntrinsicID::_putShortRelease:\n+    case vmIntrinsicID::_putShortUnaligned:\n+    case vmIntrinsicID::_putShortVolatile:\n+    case vmIntrinsicID::_putInt:\n+    case vmIntrinsicID::_putIntOpaque:\n+    case vmIntrinsicID::_putIntRelease:\n+    case vmIntrinsicID::_putIntUnaligned:\n+    case vmIntrinsicID::_putIntVolatile:\n+    case vmIntrinsicID::_putLong:\n+    case vmIntrinsicID::_putLongOpaque:\n+    case vmIntrinsicID::_putLongRelease:\n+    case vmIntrinsicID::_putLongUnaligned:\n+    case vmIntrinsicID::_putLongVolatile:\n+    case vmIntrinsicID::_putFloat:\n+    case vmIntrinsicID::_putFloatOpaque:\n+    case vmIntrinsicID::_putFloatRelease:\n+    case vmIntrinsicID::_putFloatVolatile:\n+    case vmIntrinsicID::_putDouble:\n+    case vmIntrinsicID::_putDoubleOpaque:\n+    case vmIntrinsicID::_putDoubleRelease:\n+    case vmIntrinsicID::_putDoubleVolatile:\n+    case vmIntrinsicID::_putReference:\n+    case vmIntrinsicID::_putReferenceOpaque:\n+    case vmIntrinsicID::_putReferenceRelease:\n+    case vmIntrinsicID::_putReferenceVolatile:\n+    case vmIntrinsicID::_putValue:\n+      return true;\n+    default:\n+      return false;\n+  }\n+}\n+\n@@ -149,1 +210,15 @@\n-  if (allow_inline && allow_intrinsics) {\n+  if (callee->intrinsic_id() == vmIntrinsics::_makePrivateBuffer || callee->intrinsic_id() == vmIntrinsics::_finishPrivateBuffer) {\n+    \/\/ These methods must be inlined so that we don't have larval value objects crossing method\n+    \/\/ boundaries\n+    assert(!call_does_dispatch, \"callee should not be virtual %s\", callee->name()->as_utf8());\n+    CallGenerator* cg = find_intrinsic(callee, call_does_dispatch);\n+\n+    if (cg == nullptr) {\n+      \/\/ This is probably because the intrinsics is disabled from the command line\n+      char reason[256];\n+      jio_snprintf(reason, sizeof(reason), \"cannot find an intrinsics for %s\", callee->name()->as_utf8());\n+      C->record_method_not_compilable(reason);\n+      return nullptr;\n+    }\n+    return cg;\n+  } else if (allow_inline && allow_intrinsics) {\n@@ -617,1 +692,1 @@\n-  if (iter().cur_bc_raw() == Bytecodes::_invokespecial && !orig_callee->is_object_initializer()) {\n+  if (iter().cur_bc_raw() == Bytecodes::_invokespecial && !orig_callee->is_object_constructor()) {\n@@ -645,0 +720,9 @@\n+  \/\/ Scalarize value objects passed into this invocation if we know that they are not larval\n+  for (int arg_idx = 0; arg_idx < nargs; arg_idx++) {\n+    if (arg_can_be_larval(callee, arg_idx)) {\n+      continue;\n+    }\n+\n+    cast_to_non_larval(peek(nargs - 1 - arg_idx));\n+  }\n+\n@@ -659,0 +743,4 @@\n+  if (failing()) {\n+    return;\n+  }\n+  assert(cg != nullptr, \"must find a CallGenerator for callee %s\", callee->name()->as_utf8());\n@@ -745,1 +833,1 @@\n-          \/\/ It's OK for a method  to return a value that is discarded.\n+          \/\/ It's OK for a method to return a value that is discarded.\n@@ -803,0 +891,21 @@\n+\n+    if (!rtype->is_void() && cg->method()->intrinsic_id() != vmIntrinsicID::_makePrivateBuffer) {\n+      Node* retnode = peek();\n+      const Type* rettype = gvn().type(retnode);\n+      if (rettype->is_inlinetypeptr() && !retnode->is_InlineType()) {\n+        retnode = InlineTypeNode::make_from_oop(this, retnode, rettype->inline_klass());\n+        dec_sp(1);\n+        push(retnode);\n+      }\n+    }\n+\n+    if (cg->method()->is_object_constructor() && receiver != nullptr && gvn().type(receiver)->is_inlinetypeptr()) {\n+      InlineTypeNode* non_larval = InlineTypeNode::make_from_oop(this, receiver, gvn().type(receiver)->inline_klass());\n+      \/\/ Relinquish the oop input, we will delay the allocation to the point it is needed, see the\n+      \/\/ comments in InlineTypeNode::Ideal for more details\n+      non_larval = non_larval->clone_if_required(&gvn(), nullptr);\n+      non_larval->set_oop(gvn(), null());\n+      non_larval->set_is_buffered(gvn(), false);\n+      non_larval = gvn().transform(non_larval)->as_InlineType();\n+      map()->replace_edge(receiver, non_larval);\n+    }\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":112,"deletions":3,"binary":false,"changes":115,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/metaspace.hpp\"\n@@ -39,0 +40,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -170,0 +172,10 @@\n+    if ((n->Opcode() == Op_LoadX || n->Opcode() == Op_StoreX) &&\n+        !n->in(MemNode::Address)->is_AddP() &&\n+        _igvn->type(n->in(MemNode::Address))->isa_oopptr()) {\n+      \/\/ Load\/Store at mark work address is at offset 0 so has no AddP which confuses EA\n+      Node* addp = new AddPNode(n->in(MemNode::Address), n->in(MemNode::Address), _igvn->MakeConX(0));\n+      _igvn->register_new_node_with_optimizer(addp);\n+      _igvn->replace_input_of(n, MemNode::Address, addp);\n+      ideal_nodes.push(addp);\n+      _nodes.at_put_grow(addp->_idx, nullptr, nullptr);\n+    }\n@@ -427,0 +439,8 @@\n+  \/\/ 6. Expand flat accesses if the object does not escape. This adds nodes to\n+  \/\/ the graph, so it has to be after split_unique_types. This expands atomic\n+  \/\/ mismatched accesses (though encapsulated in LoadFlats and StoreFlats) into\n+  \/\/ non-mismatched accesses, so it is better before reduce allocation merges.\n+  if (has_non_escaping_obj) {\n+    optimize_flat_accesses(sfn_worklist);\n+  }\n+\n@@ -429,1 +449,1 @@\n-  \/\/ 6. Reduce allocation merges used as debug information. This is done after\n+  \/\/ 7. Reduce allocation merges used as debug information. This is done after\n@@ -1270,1 +1290,9 @@\n-      SafePointScalarObjectNode* sobj = mexp.create_scalarized_object_description(alloc, sfpt);\n+      Unique_Node_List value_worklist;\n+#ifdef ASSERT\n+      const Type* res_type = alloc->result_cast()->bottom_type();\n+      if (res_type->is_inlinetypeptr() && !Compile::current()->has_circular_inline_type()) {\n+        PhiNode* phi = ophi->as_Phi();\n+        assert(!ophi->as_Phi()->can_push_inline_types_down(_igvn), \"missed earlier scalarization opportunity\");\n+      }\n+#endif\n+      SafePointScalarObjectNode* sobj = mexp.create_scalarized_object_description(alloc, sfpt, &value_worklist);\n@@ -1272,0 +1300,1 @@\n+        _compile->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n@@ -1282,0 +1311,9 @@\n+\n+      \/\/ Scalarize inline types that were added to the safepoint.\n+      \/\/ Don't allow linking a constant oop (if available) for flat array elements\n+      \/\/ because Deoptimization::reassign_flat_array_elements needs field values.\n+      const bool allow_oop = !merge_t->is_flat();\n+      for (uint j = 0; j < value_worklist.size(); ++j) {\n+        InlineTypeNode* vt = value_worklist.at(j)->as_InlineType();\n+        vt->make_scalar_in_safepoints(_igvn, allow_oop);\n+      }\n@@ -1484,1 +1522,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_sig();\n@@ -1558,0 +1596,11 @@\n+      } else if (n->as_Call()->tf()->returns_inline_type_as_fields()) {\n+        bool returns_oop = false;\n+        for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax && !returns_oop; i++) {\n+          ProjNode* pn = n->fast_out(i)->as_Proj();\n+          if (pn->_con >= TypeFunc::Parms && pn->bottom_type()->isa_ptr()) {\n+            returns_oop = true;\n+          }\n+        }\n+        if (returns_oop) {\n+          add_call_node(n->as_Call());\n+        }\n@@ -1585,1 +1634,2 @@\n-    case Op_CastX2P: {\n+    case Op_CastX2P:\n+    case Op_CastI2N: {\n@@ -1589,0 +1639,1 @@\n+    case Op_InlineType:\n@@ -1658,0 +1709,8 @@\n+    case Op_LoadFlat:\n+      \/\/ Treat LoadFlat similar to an unknown call that receives nothing and produces its results\n+      map_ideal_node(n, phantom_obj);\n+      break;\n+    case Op_StoreFlat:\n+      \/\/ Treat StoreFlat similar to a call that escapes the stored flattened fields\n+      delayed_worklist->push(n);\n+      break;\n@@ -1660,2 +1719,7 @@\n-      if (n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-          n->in(0)->as_Call()->returns_pointer()) {\n+      if (n->as_Proj()->_con >= TypeFunc::Parms && n->in(0)->is_Call() &&\n+          (n->in(0)->as_Call()->returns_pointer() || n->bottom_type()->isa_ptr())) {\n+        assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+               n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n+        add_local_var_and_edge(n, PointsToNode::NoEscape, n->in(0), delayed_worklist);\n+      } else if (n->as_Proj()->_con >= TypeFunc::Parms && n->in(0)->is_LoadFlat() && igvn->type(n)->isa_ptr()) {\n+        \/\/ Treat LoadFlat outputs similar to a call return value\n@@ -1747,1 +1811,1 @@\n-  assert(n->is_Store() || n->is_LoadStore() ||\n+  assert(n->is_Store() || n->is_LoadStore() || n->is_StoreFlat() ||\n@@ -1763,0 +1827,1 @@\n+    case Op_InlineType:\n@@ -1815,0 +1880,16 @@\n+    case Op_StoreFlat: {\n+      \/\/ StoreFlat globally escapes its stored flattened fields\n+      InlineTypeNode* value = n->as_StoreFlat()->value();\n+      ciInlineKlass* vk = _igvn->type(value)->inline_klass();\n+      for (int i = 0; i < vk->nof_nonstatic_fields(); i++) {\n+        ciField* field = vk->nonstatic_field_at(i);\n+        if (field->type()->is_primitive_type()) {\n+          continue;\n+        }\n+\n+        Node* field_value = value->field_value_by_offset(field->offset_in_bytes(), true);\n+        PointsToNode* field_value_ptn = ptnode_adr(field_value->_idx);\n+        set_escape_state(field_value_ptn, PointsToNode::GlobalEscape NOT_PRODUCT(COMMA \"store into a flat field\"));\n+      }\n+      break;\n+    }\n@@ -1816,4 +1897,9 @@\n-      \/\/ we are only interested in the oop result projection from a call\n-      assert(n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-             n->in(0)->as_Call()->returns_pointer(), \"Unexpected node type\");\n-      add_local_var_and_edge(n, PointsToNode::NoEscape, n->in(0), nullptr);\n+      if (n->in(0)->is_Call()) {\n+        \/\/ we are only interested in the oop result projection from a call\n+        assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+              n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n+        add_local_var_and_edge(n, PointsToNode::NoEscape, n->in(0), nullptr);\n+      } else if (n->in(0)->is_LoadFlat()) {\n+        \/\/ Treat LoadFlat outputs similar to a call return value\n+        add_local_var_and_edge(n, PointsToNode::NoEscape, n->in(0), nullptr);\n+      }\n@@ -1994,1 +2080,1 @@\n-  assert(call->returns_pointer(), \"only for call which returns pointer\");\n+  assert(call->returns_pointer() || call->tf()->returns_inline_type_as_fields(), \"only for call which returns pointer\");\n@@ -2070,1 +2156,3 @@\n-      assert(strncmp(name, \"C2 Runtime multianewarray\", 25) == 0, \"TODO: add failed case check\");\n+      assert(strncmp(name, \"C2 Runtime multianewarray\", 25) == 0 ||\n+             strncmp(name, \"C2 Runtime load_unknown_inline\", 30) == 0 ||\n+             strncmp(name, \"store_inline_type_fields_to_buf\", 31) == 0, \"TODO: add failed case check\");\n@@ -2101,1 +2189,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -2149,1 +2237,1 @@\n-      const TypeTuple * d = call->tf()->domain();\n+      const TypeTuple * d = call->tf()->domain_sig();\n@@ -2180,1 +2268,4 @@\n-                               (aat->isa_aryptr() && (aat->isa_aryptr()->elem() == Type::BOTTOM || aat->isa_aryptr()->elem()->make_oopptr() != nullptr)));\n+                               (aat->isa_aryptr() && (aat->isa_aryptr()->elem() == Type::BOTTOM || aat->isa_aryptr()->elem()->make_oopptr() != nullptr)) ||\n+                               (aat->isa_aryptr() && aat->isa_aryptr()->elem() != nullptr &&\n+                                                               aat->isa_aryptr()->is_flat() &&\n+                                                               aat->isa_aryptr()->elem()->inline_klass()->contains_oops()));\n@@ -2244,0 +2335,4 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"vectorizedMismatch\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"load_unknown_inline\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"store_unknown_inline\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"store_inline_type_fields_to_buf\") == 0 ||\n@@ -2306,1 +2401,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -2350,1 +2445,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_cc();\n@@ -2768,0 +2863,1 @@\n+  PointsToNode* init_val = phantom_obj;\n@@ -2773,1 +2869,7 @@\n-    return 0;\n+    if (alloc->as_Allocate()->in(AllocateNode::InitValue) != nullptr) {\n+      \/\/ Null-free inline type arrays are initialized with an init value instead of null\n+      init_val = ptnode_adr(alloc->as_Allocate()->in(AllocateNode::InitValue)->_idx);\n+      assert(init_val != nullptr, \"init value should be registered\");\n+    } else {\n+      return 0;\n+    }\n@@ -2775,1 +2877,2 @@\n-  assert(pta->arraycopy_dst() || alloc->as_CallStaticJava(), \"sanity\");\n+  \/\/ Non-escaped allocation returned from Java or runtime call has unknown values in fields.\n+  assert(pta->arraycopy_dst() || alloc->is_CallStaticJava() || init_val != phantom_obj, \"sanity\");\n@@ -2777,1 +2880,1 @@\n-  if (!pta->arraycopy_dst() && alloc->as_CallStaticJava()->method() == nullptr) {\n+  if (alloc->is_CallStaticJava() && alloc->as_CallStaticJava()->method() == nullptr) {\n@@ -2779,1 +2882,3 @@\n-    assert(strncmp(name, \"C2 Runtime multianewarray\", 25) == 0, \"sanity\");\n+    assert(strncmp(name, \"C2 Runtime multianewarray\", 25) == 0 ||\n+           strncmp(name, \"C2 Runtime load_unknown_inline\", 30) == 0 ||\n+           strncmp(name, \"store_inline_type_fields_to_buf\", 31) == 0, \"sanity\");\n@@ -2787,1 +2892,1 @@\n-      if (add_edge(field, phantom_obj)) {\n+      if (add_edge(field, init_val)) {\n@@ -2802,1 +2907,1 @@\n-  if (!alloc->is_Allocate()) {\n+  if (!alloc->is_Allocate() || alloc->as_Allocate()->in(AllocateNode::InitValue) != nullptr) {\n@@ -2888,1 +2993,1 @@\n-                tty->print_cr(\"----------missed referernce to object-----------\");\n+                tty->print_cr(\"----------missed reference to object------------\");\n@@ -2890,1 +2995,1 @@\n-                tty->print_cr(\"----------object referernced by init store -----\");\n+                tty->print_cr(\"----------object referenced by init store-------\");\n@@ -3255,1 +3360,2 @@\n-          if (can_eliminate_lock(alock)) {\n+          const Type* obj_type = igvn->type(alock->obj_node());\n+          if (can_eliminate_lock(alock) && !obj_type->is_inlinetypeptr()) {\n@@ -3297,5 +3403,10 @@\n-      MemBarNode* mb = MemBarNode::make(C, Op_MemBarCPUOrder, Compile::AliasIdxBot);\n-      mb->init_req(TypeFunc::Memory,  storestore->in(TypeFunc::Memory));\n-      mb->init_req(TypeFunc::Control, storestore->in(TypeFunc::Control));\n-      igvn->register_new_node_with_optimizer(mb);\n-      igvn->replace_node(storestore, mb);\n+      if (alloc->in(AllocateNode::InlineType) != nullptr) {\n+        \/\/ Non-escaping inline type buffer allocations don't require a membar\n+        storestore->as_MemBar()->remove(_igvn);\n+      } else {\n+        MemBarNode* mb = MemBarNode::make(C, Op_MemBarCPUOrder, Compile::AliasIdxBot);\n+        mb->init_req(TypeFunc::Memory,  storestore->in(TypeFunc::Memory));\n+        mb->init_req(TypeFunc::Control, storestore->in(TypeFunc::Control));\n+        igvn->register_new_node_with_optimizer(mb);\n+        igvn->replace_node(storestore, mb);\n+      }\n@@ -3306,0 +3417,25 @@\n+\/\/ Atomic flat accesses on non-escaping objects can be optimized to non-atomic accesses\n+void ConnectionGraph::optimize_flat_accesses(GrowableArray<SafePointNode*>& sfn_worklist) {\n+  PhaseIterGVN& igvn = *_igvn;\n+  bool delay = igvn.delay_transform();\n+  igvn.set_delay_transform(true);\n+  igvn.C->for_each_flat_access([&](Node* n) {\n+    Node* base = n->is_LoadFlat() ? n->as_LoadFlat()->base() : n->as_StoreFlat()->base();\n+    if (!not_global_escape(base)) {\n+      return;\n+    }\n+\n+    bool expanded;\n+    if (n->is_LoadFlat()) {\n+      expanded = n->as_LoadFlat()->expand_non_atomic(igvn);\n+    } else {\n+      expanded = n->as_StoreFlat()->expand_non_atomic(igvn);\n+    }\n+    if (expanded) {\n+      sfn_worklist.remove(n->as_SafePoint());\n+      igvn.C->remove_flat_access(n);\n+    }\n+  });\n+  igvn.set_delay_transform(delay);\n+}\n+\n@@ -3465,0 +3601,1 @@\n+  int field_offset = adr_type->isa_aryptr() ? adr_type->isa_aryptr()->field_offset().get() : Type::OffsetBot;\n@@ -3466,1 +3603,1 @@\n-  if (offset == Type::OffsetBot) {\n+  if (offset == Type::OffsetBot && field_offset == Type::OffsetBot) {\n@@ -3478,1 +3615,1 @@\n-      ciField* field = _compile->alias_type(adr_type->isa_instptr())->field();\n+      ciField* field = _compile->alias_type(adr_type->is_ptr())->field();\n@@ -3497,2 +3634,14 @@\n-        const Type* elemtype = adr_type->isa_aryptr()->elem();\n-        bt = elemtype->array_element_basic_type();\n+        const Type* elemtype = adr_type->is_aryptr()->elem();\n+        if (adr_type->is_aryptr()->is_flat() && field_offset != Type::OffsetBot) {\n+          ciInlineKlass* vk = elemtype->inline_klass();\n+          field_offset += vk->payload_offset();\n+          ciField* field = vk->get_field_by_offset(field_offset, false);\n+          if (field != nullptr) {\n+            bt = field->layout_type();\n+          } else {\n+            assert(field_offset == vk->payload_offset() + vk->null_marker_offset_in_payload(), \"no field or null marker of %s at offset %d\", vk->name()->as_utf8(), field_offset);\n+            bt = T_BOOLEAN;\n+          }\n+        } else {\n+          bt = elemtype->array_element_basic_type();\n+        }\n@@ -3695,3 +3844,1 @@\n-  const TypePtr *t_ptr = adr_type->isa_ptr();\n-  assert(t_ptr != nullptr, \"must be a pointer type\");\n-  return t_ptr->offset();\n+  return adr_type->is_ptr()->flat_offset();\n@@ -3851,1 +3998,8 @@\n-    t = base_t->add_offset(offs)->is_oopptr();\n+    if (base_t->isa_aryptr() != nullptr) {\n+      \/\/ In the case of a flat inline type array, each field has its\n+      \/\/ own slice so we need to extract the field being accessed from\n+      \/\/ the address computation\n+      t = base_t->isa_aryptr()->add_field_offset_and_offset(offs)->is_oopptr();\n+    } else {\n+      t = base_t->add_offset(offs)->is_oopptr();\n+    }\n@@ -3853,1 +4007,1 @@\n-  int inst_id =  base_t->instance_id();\n+  int inst_id = base_t->instance_id();\n@@ -3867,1 +4021,1 @@\n-  \/\/ It could happened when CHA type is different from MDO type on a dead path\n+  \/\/ It could happen when CHA type is different from MDO type on a dead path\n@@ -3877,1 +4031,12 @@\n-  const TypeOopPtr *tinst = base_t->add_offset(t->offset())->is_oopptr();\n+  const TypePtr* tinst = base_t->add_offset(t->offset());\n+  if (tinst->isa_aryptr() && t->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to keep track of the field being accessed.\n+    tinst = tinst->is_aryptr()->with_field_offset(t->is_aryptr()->field_offset().get());\n+    \/\/ Keep array properties (not flat\/null-free)\n+    tinst = tinst->is_aryptr()->update_properties(t->is_aryptr());\n+    if (tinst == nullptr) {\n+      return false; \/\/ Skip dead path with inconsistent properties\n+    }\n+  }\n+\n@@ -4181,0 +4346,1 @@\n+#if 0  \/\/ TODO: Fix 8372259\n@@ -4187,0 +4353,3 @@\n+#else\n+        }\n+#endif\n@@ -4604,0 +4773,7 @@\n+          if (tn_t->isa_aryptr()) {\n+            \/\/ Keep array properties (not flat\/null-free)\n+            tinst = tinst->is_aryptr()->update_properties(tn_t->is_aryptr());\n+            if (tinst == nullptr) {\n+              continue; \/\/ Skip dead path with inconsistent properties\n+            }\n+          }\n@@ -4629,1 +4805,1 @@\n-      if(use->is_Mem() && use->in(MemNode::Address) == n) {\n+      if (use->is_Mem() && use->in(MemNode::Address) == n) {\n@@ -4665,0 +4841,3 @@\n+      } else if (use->Opcode() == Op_Return) {\n+        \/\/ Allocation is referenced by field of returned inline type\n+        assert(_compile->tf()->returns_inline_type_as_fields(), \"EA: unexpected reference by ReturnNode\");\n@@ -4678,1 +4857,1 @@\n-              op == Op_SubTypeCheck ||\n+              op == Op_SubTypeCheck || op == Op_InlineType || op == Op_FlatArrayCheck ||\n@@ -4786,0 +4965,3 @@\n+    } else if (n->is_CallLeaf() && n->as_CallLeaf()->_name != nullptr &&\n+               strcmp(n->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+      n = n->as_CallLeaf()->proj_out(TypeFunc::Memory);\n@@ -4837,1 +5019,1 @@\n-      } else if(use->is_Mem()) {\n+      } else if (use->is_Mem()) {\n@@ -4846,0 +5028,4 @@\n+      } else if (use->is_CallLeaf() && use->as_CallLeaf()->_name != nullptr &&\n+                 strcmp(use->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+        \/\/ store_unknown_inline overwrites destination array\n+        memnode_worklist.append_if_missing(use);\n@@ -4855,1 +5041,1 @@\n-              op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar)) {\n+              op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar || op == Op_FlatArrayCheck)) {\n@@ -4958,1 +5144,1 @@\n-  \/\/ chains as is done in split_memory_phi() since they  will\n+  \/\/ chains as is done in split_memory_phi() since they will\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":234,"deletions":48,"binary":false,"changes":282,"status":"modified"},{"patch":"@@ -488,0 +488,2 @@\n+  \/\/ Expand flat accesses to accesses to each component if the object does not escape\n+  void optimize_flat_accesses(GrowableArray<SafePointNode*>& sfn_worklist);\n","filename":"src\/hotspot\/share\/opto\/escape.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+class UnswitchCandidate;\n@@ -88,1 +89,1 @@\n-       };\n+         FlatArrays            = 1<<18};\n@@ -111,0 +112,1 @@\n+  bool is_flat_arrays() const { return _loop_flags & FlatArrays; }\n@@ -124,0 +126,1 @@\n+  void mark_flat_arrays() { _loop_flags |= FlatArrays; }\n@@ -756,0 +759,1 @@\n+  bool no_unswitch_candidate() const;\n@@ -1566,1 +1570,2 @@\n-  IfNode* find_unswitch_candidate(const IdealLoopTree* loop) const;\n+  IfNode* find_unswitch_candidates(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+  IfNode* find_unswitch_candidate_from_idoms(const IdealLoopTree* loop) const;\n@@ -1573,1 +1578,1 @@\n-                                   const UnswitchedLoopSelector& unswitched_loop_selector);\n+                                   const UnswitchCandidate& unswitch_candidate, const IfNode* loop_selector);\n@@ -1581,0 +1586,1 @@\n+                                            const UnswitchCandidate& unswitch_candidate,\n@@ -1778,0 +1784,1 @@\n+  void move_flat_array_check_out_of_loop(Node* n);\n@@ -1779,0 +1786,1 @@\n+  bool flat_array_element_type_check(Node *n);\n@@ -1970,0 +1978,2 @@\n+  void collect_flat_array_checks(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":13,"deletions":3,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -66,0 +67,6 @@\n+  \/\/ Inline types should not be split through Phis because they cannot be merged\n+  \/\/ through Phi nodes but each value input needs to be merged individually.\n+  if (n->is_InlineType()) {\n+    return nullptr;\n+  }\n+\n@@ -795,0 +802,4 @@\n+      if (inp->isa_InlineType()) {\n+        \/\/ TODO 8302217 This prevents PhiNode::push_inline_types_through\n+        return nullptr;\n+      }\n@@ -1120,0 +1131,48 @@\n+\/\/ We can't use immutable memory for the flat array check because we are loading the mark word which is\n+\/\/ mutable. Although the bits we are interested in are immutable (we check for markWord::unlocked_value),\n+\/\/ we need to use raw memory to not break anti dependency analysis. Below code will attempt to still move\n+\/\/ flat array checks out of loops, mainly to enable loop unswitching.\n+void PhaseIdealLoop::move_flat_array_check_out_of_loop(Node* n) {\n+  \/\/ Skip checks for more than one array\n+  if (n->req() > 3) {\n+    return;\n+  }\n+  Node* mem = n->in(FlatArrayCheckNode::Memory);\n+  Node* array = n->in(FlatArrayCheckNode::ArrayOrKlass)->uncast();\n+  IdealLoopTree* check_loop = get_loop(get_ctrl(n));\n+  IdealLoopTree* ary_loop = get_loop(get_ctrl(array));\n+\n+  \/\/ Check if array is loop invariant\n+  if (!check_loop->is_member(ary_loop)) {\n+    \/\/ Walk up memory graph from the check until we leave the loop\n+    VectorSet wq;\n+    wq.set(mem->_idx);\n+    while (check_loop->is_member(get_loop(ctrl_or_self(mem)))) {\n+      if (mem->is_Phi()) {\n+        mem = mem->in(1);\n+      } else if (mem->is_MergeMem()) {\n+        mem = mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw);\n+      } else if (mem->is_Proj()) {\n+        mem = mem->in(0);\n+      } else if (mem->is_MemBar() || mem->is_SafePoint()) {\n+        mem = mem->in(TypeFunc::Memory);\n+      } else if (mem->is_Store() || mem->is_LoadStore() || mem->is_ClearArray()) {\n+        mem = mem->in(MemNode::Memory);\n+      } else {\n+#ifdef ASSERT\n+        mem->dump();\n+#endif\n+        ShouldNotReachHere();\n+      }\n+      if (wq.test_set(mem->_idx)) {\n+        return;\n+      }\n+    }\n+    \/\/ Replace memory input and re-compute ctrl to move the check out of the loop\n+    _igvn.replace_input_of(n, 1, mem);\n+    set_ctrl_and_loop(n, get_early_ctrl(n));\n+    Node* bol = n->unique_out();\n+    set_ctrl_and_loop(bol, get_early_ctrl(bol));\n+  }\n+}\n+\n@@ -1132,0 +1191,6 @@\n+\n+  if (n->isa_FlatArrayCheck()) {\n+    move_flat_array_check_out_of_loop(n);\n+    return n;\n+  }\n+\n@@ -1404,0 +1469,98 @@\n+bool PhaseIdealLoop::flat_array_element_type_check(Node *n) {\n+  \/\/ If the CmpP is a subtype check for a value that has just been\n+  \/\/ loaded from an array, the subtype check guarantees the value\n+  \/\/ can't be stored in a flat array and the load of the value\n+  \/\/ happens with a flat array check then: push the type check\n+  \/\/ through the phi of the flat array check. This needs special\n+  \/\/ logic because the subtype check's input is not a phi but a\n+  \/\/ LoadKlass that must first be cloned through the phi.\n+  if (n->Opcode() != Op_CmpP) {\n+    return false;\n+  }\n+\n+  Node* klassptr = n->in(1);\n+  Node* klasscon = n->in(2);\n+\n+  if (klassptr->is_DecodeNarrowPtr()) {\n+    klassptr = klassptr->in(1);\n+  }\n+\n+  if (klassptr->Opcode() != Op_LoadKlass && klassptr->Opcode() != Op_LoadNKlass) {\n+    return false;\n+  }\n+\n+  if (!klasscon->is_Con()) {\n+    return false;\n+  }\n+\n+  Node* addr = klassptr->in(MemNode::Address);\n+\n+  if (!addr->is_AddP()) {\n+    return false;\n+  }\n+\n+  intptr_t offset;\n+  Node* obj = AddPNode::Ideal_base_and_offset(addr, &_igvn, offset);\n+\n+  if (obj == nullptr) {\n+    return false;\n+  }\n+\n+  assert(obj != nullptr && addr->in(AddPNode::Base) == addr->in(AddPNode::Address), \"malformed AddP?\");\n+  if (obj->Opcode() == Op_CastPP) {\n+    obj = obj->in(1);\n+  }\n+\n+  if (!obj->is_Phi()) {\n+    return false;\n+  }\n+\n+  Node* region = obj->in(0);\n+\n+  Node* phi = PhiNode::make_blank(region, n->in(1));\n+  for (uint i = 1; i < region->req(); i++) {\n+    Node* in = obj->in(i);\n+    Node* ctrl = region->in(i);\n+    if (addr->in(AddPNode::Base) != obj) {\n+      Node* cast = addr->in(AddPNode::Base);\n+      assert(cast->Opcode() == Op_CastPP && cast->in(0) != nullptr, \"inconsistent subgraph\");\n+      Node* cast_clone = cast->clone();\n+      cast_clone->set_req(0, ctrl);\n+      cast_clone->set_req(1, in);\n+      register_new_node(cast_clone, ctrl);\n+      const Type* tcast = cast_clone->Value(&_igvn);\n+      _igvn.set_type(cast_clone, tcast);\n+      cast_clone->as_Type()->set_type(tcast);\n+      in = cast_clone;\n+    }\n+    Node* addr_clone = addr->clone();\n+    addr_clone->set_req(AddPNode::Base, in);\n+    addr_clone->set_req(AddPNode::Address, in);\n+    register_new_node(addr_clone, ctrl);\n+    _igvn.set_type(addr_clone, addr_clone->Value(&_igvn));\n+    Node* klassptr_clone = klassptr->clone();\n+    klassptr_clone->set_req(2, addr_clone);\n+    register_new_node(klassptr_clone, ctrl);\n+    _igvn.set_type(klassptr_clone, klassptr_clone->Value(&_igvn));\n+    if (klassptr != n->in(1)) {\n+      Node* decode = n->in(1);\n+      assert(decode->is_DecodeNarrowPtr(), \"inconsistent subgraph\");\n+      Node* decode_clone = decode->clone();\n+      decode_clone->set_req(1, klassptr_clone);\n+      register_new_node(decode_clone, ctrl);\n+      _igvn.set_type(decode_clone, decode_clone->Value(&_igvn));\n+      klassptr_clone = decode_clone;\n+    }\n+    phi->set_req(i, klassptr_clone);\n+  }\n+  register_new_node(phi, region);\n+  Node* orig = n->in(1);\n+  _igvn.replace_input_of(n, 1, phi);\n+  split_if_with_blocks_post(n);\n+  if (n->outcnt() != 0) {\n+    _igvn.replace_input_of(n, 1, orig);\n+    _igvn.remove_dead_node(phi);\n+  }\n+  return true;\n+}\n+\n@@ -1410,0 +1573,4 @@\n+  if (flat_array_element_type_check(n)) {\n+    return;\n+  }\n+\n@@ -1553,0 +1720,5 @@\n+\n+  \/\/ Remove multiple allocations of the same inline type\n+  if (n->is_InlineType()) {\n+    n->as_InlineType()->remove_redundant_allocations(this);\n+  }\n@@ -2061,1 +2233,9 @@\n-  Node *sample_cmp = sample_bool->in(1);\n+  Node* sample_cmp = sample_bool->in(1);\n+  const Type* t = Type::TOP;\n+  const TypePtr* at = nullptr;\n+  if (sample_cmp->is_FlatArrayCheck()) {\n+    \/\/ Left input of a FlatArrayCheckNode is memory, set the (adr) type of the phi accordingly\n+    assert(sample_cmp->in(1)->bottom_type() == Type::MEMORY, \"unexpected input type\");\n+    t = Type::MEMORY;\n+    at = TypeRawPtr::BOTTOM;\n+  }\n@@ -2064,1 +2244,1 @@\n-  PhiNode *phi1 = new PhiNode(phi->in(0), Type::TOP);\n+  PhiNode *phi1 = new PhiNode(phi->in(0), t, at);\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":182,"deletions":2,"binary":false,"changes":184,"status":"modified"},{"patch":"@@ -25,0 +25,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInstanceKlass.hpp\"\n@@ -38,0 +40,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -46,0 +49,1 @@\n+#include \"opto\/opcodes.hpp\"\n@@ -55,0 +59,2 @@\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -84,11 +90,0 @@\n-void PhaseMacroExpand::migrate_outs(Node *old, Node *target) {\n-  assert(old != nullptr, \"sanity\");\n-  for (DUIterator_Fast imax, i = old->fast_outs(imax); i < imax; i++) {\n-    Node* use = old->fast_out(i);\n-    _igvn.rehash_node_delayed(use);\n-    imax -= replace_input(use, old, target);\n-    \/\/ back up iterator\n-    --i;\n-  }\n-  assert(old->outcnt() == 0, \"all uses must be deleted\");\n-}\n@@ -147,1 +142,1 @@\n-  bs->eliminate_gc_barrier(this, p2x);\n+  bs->eliminate_gc_barrier(&_igvn, p2x);\n@@ -190,0 +185,2 @@\n+      } else if (in->is_LoadFlat() || in->is_StoreFlat()) {\n+        mem = in->in(TypeFunc::Memory);\n@@ -202,1 +199,1 @@\n-        int adr_offset = atype->offset();\n+        int adr_offset = atype->flat_offset();\n@@ -247,1 +244,1 @@\n-   } else if (mem->Opcode() == Op_StrInflatedCopy) {\n+    } else if (mem->Opcode() == Op_StrInflatedCopy) {\n@@ -292,1 +289,5 @@\n-      const TypePtr* adr_type = nullptr;\n+      Node* base = ac->in(ArrayCopyNode::Src);\n+      const TypeAryPtr* adr_type = _igvn.type(base)->is_aryptr();\n+      if (adr_type->is_flat()) {\n+        shift = adr_type->flat_log_elem_size();\n+      }\n@@ -295,2 +296,2 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(off);\n+        adr_type = _igvn.type(adr)->is_aryptr();\n+        assert(adr_type == _igvn.type(base)->is_aryptr()->add_field_offset_and_offset(off), \"incorrect address type\");\n@@ -300,1 +301,1 @@\n-          return value_from_mem(ac->in(TypeFunc::Memory), ctl, ft, ftype, adr_type->isa_oopptr(), alloc);\n+          return value_from_mem(ac->in(TypeFunc::Memory), ctl, ft, ftype, adr_type, alloc);\n@@ -303,0 +304,5 @@\n+        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n+          \/\/ Non constant offset in the array: we can't statically\n+          \/\/ determine the value\n+          return nullptr;\n+        }\n@@ -310,7 +316,5 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(Type::OffsetBot);\n-        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n-          \/\/ Non constant offset in the array: we can't statically\n-          \/\/ determine the value\n-          return nullptr;\n-        }\n+        \/\/ In the case of a flat inline type array, each field has its\n+        \/\/ own slice so we need to extract the field being accessed from\n+        \/\/ the address computation\n+        adr_type = adr_type->add_field_offset_and_offset(offset)->add_offset(Type::OffsetBot)->is_aryptr();\n+        adr = _igvn.transform(new CastPPNode(ctl, adr, adr_type));\n@@ -327,0 +331,1 @@\n+      assert(res->isa_DecodeN(), \"should be narrow oop\");\n@@ -342,1 +347,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flat_offset();\n@@ -378,1 +383,1 @@\n-    } else  {\n+    } else {\n@@ -381,2 +386,14 @@\n-        \/\/ hit a sentinel, return appropriate 0 value\n-        values.at_put(j, _igvn.zerocon(ft));\n+        \/\/ hit a sentinel, return appropriate value\n+        Node* init_value = alloc->in(AllocateNode::InitValue);\n+        if (init_value != nullptr) {\n+          if (val == start_mem) {\n+            \/\/ TODO 8350865 Scalar replacement does not work well for flat arrays.\n+            \/\/ Somehow we ended up with root mem and therefore walked past the alloc. Fix this. Triggered by TestGenerated::test15\n+            \/\/ Don't we need field_value_by_offset?\n+            return nullptr;\n+          }\n+          values.at_put(j, init_value);\n+        } else {\n+          assert(alloc->in(AllocateNode::RawInitValue) == nullptr, \"init value may not be null\");\n+          values.at_put(j, _igvn.zerocon(ft));\n+        }\n@@ -401,2 +418,10 @@\n-      } else if(val->is_Proj() && val->in(0) == alloc) {\n-        values.at_put(j, _igvn.zerocon(ft));\n+      } else if (val->is_Proj() && val->in(0) == alloc) {\n+        Node* init_value = alloc->in(AllocateNode::InitValue);\n+        if (init_value != nullptr) {\n+          \/\/ TODO 8350865 Scalar replacement does not work well for flat arrays.\n+          \/\/ Is this correct for non-all-zero init values? Don't we need field_value_by_offset?\n+          values.at_put(j, init_value);\n+        } else {\n+          assert(alloc->in(AllocateNode::RawInitValue) == nullptr, \"init value may not be null\");\n+          values.at_put(j, _igvn.zerocon(ft));\n+        }\n@@ -450,1 +475,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flat_offset();\n@@ -452,1 +477,0 @@\n-  Node *alloc_ctrl = alloc->in(TypeFunc::Control);\n@@ -469,1 +493,1 @@\n-        done = true; \/\/ Something go wrong.\n+        done = true; \/\/ Something went wrong.\n@@ -479,1 +503,1 @@\n-             atype->is_known_instance_field() && atype->offset() == offset &&\n+             atype->is_known_instance_field() && atype->flat_offset() == offset &&\n@@ -516,1 +540,17 @@\n-      \/\/ hit a sentinel, return appropriate 0 value\n+      \/\/ hit a sentinel, return appropriate value\n+      Node* init_value = alloc->in(AllocateNode::InitValue);\n+      if (init_value != nullptr) {\n+        if (adr_t->is_flat()) {\n+          if (init_value->is_EncodeP()) {\n+            init_value = init_value->in(1);\n+          }\n+          assert(adr_t->is_aryptr()->field_offset().get() != Type::OffsetBot, \"Unknown offset\");\n+          offset = adr_t->is_aryptr()->field_offset().get() + init_value->bottom_type()->inline_klass()->payload_offset();\n+          init_value = init_value->as_InlineType()->field_value_by_offset(offset, true);\n+          if (ft == T_NARROWOOP) {\n+            init_value = transform_later(new EncodePNode(init_value, init_value->bottom_type()->make_ptr()));\n+          }\n+        }\n+        return init_value;\n+      }\n+      assert(alloc->in(AllocateNode::RawInitValue) == nullptr, \"init value may not be null\");\n@@ -548,1 +588,1 @@\n-  \/\/ Something go wrong.\n+  \/\/ Something went wrong.\n@@ -552,0 +592,69 @@\n+\/\/ Search the last value stored into the inline type's fields (for flat arrays).\n+Node* PhaseMacroExpand::inline_type_from_mem(ciInlineKlass* vk, const TypeAryPtr* elem_adr_type, int elem_idx, int offset_in_element, bool null_free, AllocateNode* alloc, SafePointNode* sfpt) {\n+  auto report_failure = [&](int field_offset_in_element) {\n+#ifndef PRODUCT\n+    if (PrintEliminateAllocations) {\n+      ciInlineKlass* elem_klass = elem_adr_type->elem()->inline_klass();\n+      int offset = field_offset_in_element + elem_klass->payload_offset();\n+      ciField* flattened_field = elem_klass->get_field_by_offset(offset, false);\n+      assert(flattened_field != nullptr, \"must have a field of type %s at offset %d\", elem_klass->name()->as_utf8(), offset);\n+      tty->print(\"=== At SafePoint node %d can't find value of field [%s] of array element [%d]\", sfpt->_idx, flattened_field->name()->as_utf8(), elem_idx);\n+      tty->print(\", which prevents elimination of: \");\n+      alloc->dump();\n+    }\n+#endif \/\/ PRODUCT\n+  };\n+\n+  \/\/ Create a new InlineTypeNode and retrieve the field values from memory\n+  InlineTypeNode* vt = InlineTypeNode::make_uninitialized(_igvn, vk, false);\n+  transform_later(vt);\n+  if (null_free) {\n+    vt->set_null_marker(_igvn);\n+  } else {\n+    int nm_offset_in_element = offset_in_element + vk->null_marker_offset_in_payload();\n+    const TypeAryPtr* nm_adr_type = elem_adr_type->with_field_offset(nm_offset_in_element);\n+    Node* nm_value = value_from_mem(sfpt->memory(), sfpt->control(), T_BOOLEAN, TypeInt::BOOL, nm_adr_type, alloc);\n+    if (nm_value != nullptr) {\n+      vt->set_null_marker(_igvn, nm_value);\n+    } else {\n+      report_failure(nm_offset_in_element);\n+      return nullptr;\n+    }\n+  }\n+\n+  for (int i = 0; i < vk->nof_declared_nonstatic_fields(); ++i) {\n+    ciType* field_type = vt->field_type(i);\n+    int field_offset_in_element = offset_in_element + vt->field_offset(i) - vk->payload_offset();\n+    Node* field_value = nullptr;\n+    if (vt->field_is_flat(i)) {\n+      field_value = inline_type_from_mem(field_type->as_inline_klass(), elem_adr_type, elem_idx, field_offset_in_element, vt->field_is_null_free(i), alloc, sfpt);\n+    } else {\n+      const Type* ft = Type::get_const_type(field_type);\n+      BasicType bt = type2field[field_type->basic_type()];\n+      if (UseCompressedOops && !is_java_primitive(bt)) {\n+        ft = ft->make_narrowoop();\n+        bt = T_NARROWOOP;\n+      }\n+      \/\/ Each inline type field has its own memory slice\n+      const TypeAryPtr* field_adr_type = elem_adr_type->with_field_offset(field_offset_in_element);\n+      field_value = value_from_mem(sfpt->memory(), sfpt->control(), bt, ft, field_adr_type, alloc);\n+      if (field_value == nullptr) {\n+        report_failure(field_offset_in_element);\n+      } else if (ft->isa_narrowoop()) {\n+        assert(UseCompressedOops, \"unexpected narrow oop\");\n+        if (field_value->is_EncodeP()) {\n+          field_value = field_value->in(1);\n+        } else if (!field_value->is_InlineType()) {\n+          field_value = transform_later(new DecodeNNode(field_value, field_value->get_ptr_type()));\n+        }\n+      }\n+    }\n+    if (field_value != nullptr) {\n+      vt->set_field_value(i, field_value);\n+    } else {\n+      return nullptr;\n+    }\n+  }\n+  return vt;\n+}\n+\n@@ -561,0 +670,1 @@\n+  Unique_Node_List worklist;\n@@ -569,0 +679,1 @@\n+    worklist.push(res);\n@@ -585,1 +696,1 @@\n-  if (can_eliminate && res != nullptr) {\n+  while (can_eliminate && worklist.size() > 0) {\n@@ -587,2 +698,2 @@\n-    for (DUIterator_Fast jmax, j = res->fast_outs(jmax);\n-                               j < jmax && can_eliminate; j++) {\n+    res = worklist.pop();\n+    for (DUIterator_Fast jmax, j = res->fast_outs(jmax); j < jmax && can_eliminate; j++) {\n@@ -603,1 +714,1 @@\n-          if (n->is_Mem() && n->as_Mem()->is_mismatched_access()) {\n+          if ((n->is_Mem() && n->as_Mem()->is_mismatched_access()) || n->is_LoadFlat() || n->is_StoreFlat()) {\n@@ -639,0 +750,1 @@\n+          assert(!res->is_Phi() || !res->as_Phi()->can_be_inline_type(), \"Inline type allocations should not have safepoint uses\");\n@@ -641,0 +753,23 @@\n+      } else if (use->is_InlineType() && use->as_InlineType()->get_oop() == res) {\n+        \/\/ Look at uses\n+        for (DUIterator_Fast kmax, k = use->fast_outs(kmax); k < kmax; k++) {\n+          Node* u = use->fast_out(k);\n+          if (u->is_InlineType()) {\n+            \/\/ Use in flat field can be eliminated\n+            InlineTypeNode* vt = u->as_InlineType();\n+            for (uint i = 0; i < vt->field_count(); ++i) {\n+              if (vt->field_value(i) == use && !vt->field_is_flat(i)) {\n+                can_eliminate = false; \/\/ Use in non-flat field\n+                break;\n+              }\n+            }\n+          } else {\n+            \/\/ Add other uses to the worklist to process individually\n+            worklist.push(use);\n+          }\n+        }\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ Store to mark word of inline type larval buffer\n+        assert(res_type->is_inlinetypeptr(), \"Unexpected store to mark word\");\n+      } else if (res_type->is_inlinetypeptr() && (use->Opcode() == Op_MemBarRelease || use->Opcode() == Op_MemBarStoreStore)) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n@@ -663,0 +798,3 @@\n+      } else {\n+        assert(use->Opcode() == Op_CastP2X, \"should be\");\n+        assert(!use->has_out_with(Op_OrL), \"should have been removed because oop is never null\");\n@@ -675,1 +813,1 @@\n-    } else if (alloc->_is_scalar_replaceable) {\n+    } else {\n@@ -780,1 +918,148 @@\n-SafePointScalarObjectNode* PhaseMacroExpand::create_scalarized_object_description(AllocateNode *alloc, SafePointNode* sfpt) {\n+void PhaseMacroExpand::process_field_value_at_safepoint(const Type* field_type, Node* field_val, SafePointNode* sfpt, Unique_Node_List* value_worklist) {\n+  if (UseCompressedOops && field_type->isa_narrowoop()) {\n+    \/\/ Enable \"DecodeN(EncodeP(Allocate)) --> Allocate\" transformation\n+    \/\/ to be able scalar replace the allocation.\n+    if (field_val->is_EncodeP()) {\n+      field_val = field_val->in(1);\n+    } else if (!field_val->is_InlineType()) {\n+      field_val = transform_later(new DecodeNNode(field_val, field_val->get_ptr_type()));\n+    }\n+  }\n+\n+  \/\/ Keep track of inline types to scalarize them later\n+  if (field_val->is_InlineType()) {\n+    value_worklist->push(field_val);\n+  } else if (field_val->is_Phi()) {\n+    PhiNode* phi = field_val->as_Phi();\n+    \/\/ Eagerly replace inline type phis now since we could be removing an inline type allocation where we must\n+    \/\/ scalarize all its fields in safepoints.\n+    field_val = phi->try_push_inline_types_down(&_igvn, true);\n+    if (field_val->is_InlineType()) {\n+      value_worklist->push(field_val);\n+    }\n+  }\n+  DEBUG_ONLY(verify_type_compatability(field_val->bottom_type(), field_type);)\n+  sfpt->add_req(field_val);\n+}\n+\n+bool PhaseMacroExpand::add_array_elems_to_safepoint(AllocateNode* alloc, const TypeAryPtr* array_type, SafePointNode* sfpt, Unique_Node_List* value_worklist) {\n+  const Type* elem_type = array_type->elem();\n+  BasicType basic_elem_type = elem_type->array_element_basic_type();\n+\n+  intptr_t elem_size;\n+  uint header_size;\n+  if (array_type->is_flat()) {\n+    elem_size = array_type->flat_elem_size();\n+    header_size = arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT);\n+  } else {\n+    elem_size = type2aelembytes(basic_elem_type);\n+    header_size = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n+  }\n+\n+  int n_elems = alloc->in(AllocateNode::ALength)->get_int();\n+  for (int elem_idx = 0; elem_idx < n_elems; elem_idx++) {\n+    intptr_t elem_offset = header_size + elem_idx * elem_size;\n+    const TypeAryPtr* elem_adr_type = array_type->with_offset(elem_offset);\n+    Node* elem_val;\n+    if (array_type->is_flat()) {\n+      ciInlineKlass* elem_klass = elem_type->inline_klass();\n+      assert(elem_klass->maybe_flat_in_array(), \"must be flat in array\");\n+      elem_val = inline_type_from_mem(elem_klass, elem_adr_type, elem_idx, 0, array_type->is_null_free(), alloc, sfpt);\n+    } else {\n+      elem_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, elem_type, elem_adr_type, alloc);\n+#ifndef PRODUCT\n+      if (PrintEliminateAllocations && elem_val == nullptr) {\n+        tty->print(\"=== At SafePoint node %d can't find value of array element [%d]\", sfpt->_idx, elem_idx);\n+        tty->print(\", which prevents elimination of: \");\n+        alloc->dump();\n+      }\n+#endif \/\/ PRODUCT\n+    }\n+    if (elem_val == nullptr) {\n+      return false;\n+    }\n+\n+    process_field_value_at_safepoint(elem_type, elem_val, sfpt, value_worklist);\n+  }\n+\n+  return true;\n+}\n+\n+\/\/ Recursively adds all flattened fields of a type 'iklass' inside 'base' to 'sfpt'.\n+\/\/ 'offset_minus_header' refers to the offset of the payload of 'iklass' inside 'base' minus the\n+\/\/ payload offset of 'iklass'. If 'base' is of type 'iklass' then 'offset_minus_header' == 0.\n+bool PhaseMacroExpand::add_inst_fields_to_safepoint(ciInstanceKlass* iklass, AllocateNode* alloc, Node* base, int offset_minus_header, SafePointNode* sfpt, Unique_Node_List* value_worklist) {\n+  const TypeInstPtr* base_type = _igvn.type(base)->is_instptr();\n+  auto report_failure = [&](int offset) {\n+#ifndef PRODUCT\n+    if (PrintEliminateAllocations) {\n+      ciInstanceKlass* base_klass = base_type->instance_klass();\n+      ciField* flattened_field = base_klass->get_field_by_offset(offset, false);\n+      assert(flattened_field != nullptr, \"must have a field of type %s at offset %d\", base_klass->name()->as_utf8(), offset);\n+      tty->print(\"=== At SafePoint node %d can't find value of field: \", sfpt->_idx);\n+      flattened_field->print();\n+      int field_idx = C->alias_type(flattened_field)->index();\n+      tty->print(\" (alias_idx=%d)\", field_idx);\n+      tty->print(\", which prevents elimination of: \");\n+      base->dump();\n+    }\n+#endif \/\/ PRODUCT\n+  };\n+\n+  for (int i = 0; i < iklass->nof_declared_nonstatic_fields(); i++) {\n+    ciField* field = iklass->declared_nonstatic_field_at(i);\n+    if (field->is_flat()) {\n+      ciInlineKlass* fvk = field->type()->as_inline_klass();\n+      int field_offset_minus_header = offset_minus_header + field->offset_in_bytes() - fvk->payload_offset();\n+      bool success = add_inst_fields_to_safepoint(fvk, alloc, base, field_offset_minus_header, sfpt, value_worklist);\n+      if (!success) {\n+        return false;\n+      }\n+\n+      \/\/ The null marker of a field is added right after we scalarize that field\n+      if (!field->is_null_free()) {\n+        int nm_offset = offset_minus_header + field->null_marker_offset();\n+        Node* null_marker = value_from_mem(sfpt->memory(), sfpt->control(), T_BOOLEAN, TypeInt::BOOL, base_type->with_offset(nm_offset), alloc);\n+        if (null_marker == nullptr) {\n+          report_failure(nm_offset);\n+          return false;\n+        }\n+        process_field_value_at_safepoint(TypeInt::BOOL, null_marker, sfpt, value_worklist);\n+      }\n+\n+      continue;\n+    }\n+\n+    int offset = offset_minus_header + field->offset_in_bytes();\n+    ciType* elem_type = field->type();\n+    BasicType basic_elem_type = field->layout_type();\n+\n+    const Type* field_type;\n+    if (is_reference_type(basic_elem_type)) {\n+      if (!elem_type->is_loaded()) {\n+        field_type = TypeInstPtr::BOTTOM;\n+      } else {\n+        field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n+      }\n+      if (UseCompressedOops) {\n+        field_type = field_type->make_narrowoop();\n+        basic_elem_type = T_NARROWOOP;\n+      }\n+    } else {\n+      field_type = Type::get_const_basic_type(basic_elem_type);\n+    }\n+\n+    const TypeInstPtr* field_addr_type = base_type->add_offset(offset)->isa_instptr();\n+    Node* field_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, field_type, field_addr_type, alloc);\n+    if (field_val == nullptr) {\n+      report_failure(offset);\n+      return false;\n+    }\n+    process_field_value_at_safepoint(field_type, field_val, sfpt, value_worklist);\n+  }\n+\n+  return true;\n+}\n+\n+SafePointScalarObjectNode* PhaseMacroExpand::create_scalarized_object_description(AllocateNode* alloc, SafePointNode* sfpt,\n+                                                                                  Unique_Node_List* value_worklist) {\n@@ -785,2 +1070,0 @@\n-  BasicType basic_elem_type  = T_ILLEGAL;\n-  const Type* field_type     = nullptr;\n@@ -789,2 +1072,0 @@\n-  int array_base             = 0;\n-  int element_size           = 0;\n@@ -796,0 +1077,1 @@\n+  uint before_sfpt_req = sfpt->req();\n@@ -808,4 +1090,10 @@\n-      basic_elem_type = res_type->is_aryptr()->elem()->array_element_basic_type();\n-      array_base = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n-      element_size = type2aelembytes(basic_elem_type);\n-      field_type = res_type->is_aryptr()->elem();\n+    }\n+\n+    if (res->bottom_type()->is_inlinetypeptr()) {\n+      \/\/ Nullable inline types have a null marker field which is added to the safepoint when scalarizing them (see\n+      \/\/ InlineTypeNode::make_scalar_in_safepoint()). When having circular inline types, we stop scalarizing at depth 1\n+      \/\/ to avoid an endless recursion. Therefore, we do not have a SafePointScalarObjectNode node here, yet.\n+      \/\/ We are about to create a SafePointScalarObjectNode as if this is a normal object. Add an additional int input\n+      \/\/ with value 1 which sets the null marker to true to indicate that the object is always non-null. This input is checked\n+      \/\/ later in PhaseOutput::filLocArray() for inline types.\n+      sfpt->add_req(_igvn.intcon(1));\n@@ -819,64 +1107,4 @@\n-  \/\/ Scan object's fields adding an input to the safepoint for each field.\n-  for (int j = 0; j < nfields; j++) {\n-    intptr_t offset;\n-    ciField* field = nullptr;\n-    if (iklass != nullptr) {\n-      field = iklass->nonstatic_field_at(j);\n-      offset = field->offset_in_bytes();\n-      ciType* elem_type = field->type();\n-      basic_elem_type = field->layout_type();\n-\n-      \/\/ The next code is taken from Parse::do_get_xxx().\n-      if (is_reference_type(basic_elem_type)) {\n-        if (!elem_type->is_loaded()) {\n-          field_type = TypeInstPtr::BOTTOM;\n-        } else if (field != nullptr && field->is_static_constant()) {\n-          ciObject* con = field->constant_value().as_object();\n-          \/\/ Do not \"join\" in the previous type; it doesn't add value,\n-          \/\/ and may yield a vacuous result if the field is of interface type.\n-          field_type = TypeOopPtr::make_from_constant(con)->isa_oopptr();\n-          assert(field_type != nullptr, \"field singleton type must be consistent\");\n-        } else {\n-          field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n-        }\n-        if (UseCompressedOops) {\n-          field_type = field_type->make_narrowoop();\n-          basic_elem_type = T_NARROWOOP;\n-        }\n-      } else {\n-        field_type = Type::get_const_basic_type(basic_elem_type);\n-      }\n-    } else {\n-      offset = array_base + j * (intptr_t)element_size;\n-    }\n-\n-    const TypeOopPtr *field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n-\n-    Node *field_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, field_type, field_addr_type, alloc);\n-\n-    \/\/ We weren't able to find a value for this field,\n-    \/\/ give up on eliminating this allocation.\n-    if (field_val == nullptr) {\n-      uint last = sfpt->req() - 1;\n-      for (int k = 0;  k < j; k++) {\n-        sfpt->del_req(last--);\n-      }\n-      _igvn._worklist.push(sfpt);\n-\n-#ifndef PRODUCT\n-      if (PrintEliminateAllocations) {\n-        if (field != nullptr) {\n-          tty->print(\"=== At SafePoint node %d can't find value of field: \", sfpt->_idx);\n-          field->print();\n-          int field_idx = C->get_alias_index(field_addr_type);\n-          tty->print(\" (alias_idx=%d)\", field_idx);\n-        } else { \/\/ Array's element\n-          tty->print(\"=== At SafePoint node %d can't find value of array element [%d]\", sfpt->_idx, j);\n-        }\n-        tty->print(\", which prevents elimination of: \");\n-        if (res == nullptr)\n-          alloc->dump();\n-        else\n-          res->dump();\n-      }\n-#endif\n+  if (res == nullptr) {\n+    sfpt->jvms()->set_endoff(sfpt->req());\n+    return sobj;\n+  }\n@@ -884,2 +1112,6 @@\n-      return nullptr;\n-    }\n+  bool success;\n+  if (iklass == nullptr) {\n+    success = add_array_elems_to_safepoint(alloc, res_type->is_aryptr(), sfpt, value_worklist);\n+  } else {\n+    success = add_inst_fields_to_safepoint(iklass, alloc, res, 0, sfpt, value_worklist);\n+  }\n@@ -887,8 +1119,4 @@\n-    if (UseCompressedOops && field_type->isa_narrowoop()) {\n-      \/\/ Enable \"DecodeN(EncodeP(Allocate)) --> Allocate\" transformation\n-      \/\/ to be able scalar replace the allocation.\n-      if (field_val->is_EncodeP()) {\n-        field_val = field_val->in(1);\n-      } else {\n-        field_val = transform_later(new DecodeNNode(field_val, field_val->get_ptr_type()));\n-      }\n+  \/\/ We weren't able to find a value for this field, remove all the fields added to the safepoint\n+  if (!success) {\n+    for (uint i = sfpt->req() - 1; i >= before_sfpt_req; i--) {\n+      sfpt->del_req(i);\n@@ -896,2 +1124,2 @@\n-    DEBUG_ONLY(verify_type_compatability(field_val->bottom_type(), field_type);)\n-    sfpt->add_req(field_val);\n+    _igvn._worklist.push(sfpt);\n+    return nullptr;\n@@ -901,1 +1129,0 @@\n-\n@@ -910,0 +1137,4 @@\n+  const TypeOopPtr* res_type = nullptr;\n+  if (res != nullptr) { \/\/ Could be null when there are no users\n+    res_type = _igvn.type(res)->isa_oopptr();\n+  }\n@@ -912,0 +1143,1 @@\n+  Unique_Node_List value_worklist;\n@@ -914,1 +1146,1 @@\n-    SafePointScalarObjectNode* sobj = create_scalarized_object_description(alloc, sfpt);\n+    SafePointScalarObjectNode* sobj = create_scalarized_object_description(alloc, sfpt, &value_worklist);\n@@ -930,1 +1162,8 @@\n-\n+  \/\/ Scalarize inline types that were added to the safepoint.\n+  \/\/ Don't allow linking a constant oop (if available) for flat array elements\n+  \/\/ because Deoptimization::reassign_flat_array_elements needs field values.\n+  bool allow_oop = (res_type != nullptr) && !res_type->is_flat();\n+  for (uint i = 0; i < value_worklist.size(); ++i) {\n+    InlineTypeNode* vt = value_worklist.at(i)->as_InlineType();\n+    vt->make_scalar_in_safepoints(&_igvn, allow_oop);\n+  }\n@@ -946,1 +1185,2 @@\n-void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc) {\n+void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc, bool inline_alloc) {\n+  Unique_Node_List worklist;\n@@ -949,0 +1189,4 @@\n+    worklist.push(res);\n+  }\n+  while (worklist.size() > 0) {\n+    res = worklist.pop();\n@@ -958,10 +1202,7 @@\n-#ifdef ASSERT\n-            \/\/ Verify that there is no dependent MemBarVolatile nodes,\n-            \/\/ they should be removed during IGVN, see MemBarNode::Ideal().\n-            for (DUIterator_Fast pmax, p = n->fast_outs(pmax);\n-                                       p < pmax; p++) {\n-              Node* mb = n->fast_out(p);\n-              assert(mb->is_Initialize() || !mb->is_MemBar() ||\n-                     mb->req() <= MemBarNode::Precedent ||\n-                     mb->in(MemBarNode::Precedent) != n,\n-                     \"MemBarVolatile should be eliminated for non-escaping object\");\n+            for (DUIterator_Fast pmax, p = n->fast_outs(pmax); p < pmax; p++) {\n+              MemBarNode* mb = n->fast_out(p)->isa_MemBar();\n+              if (mb != nullptr && mb->req() <= MemBarNode::Precedent && mb->in(MemBarNode::Precedent) == n) {\n+                \/\/ MemBarVolatiles should have been removed by MemBarNode::Ideal() for non-inline allocations\n+                assert(inline_alloc, \"MemBarVolatile should be eliminated for non-escaping object\");\n+                mb->remove(&_igvn);\n+              }\n@@ -969,1 +1210,0 @@\n-#endif\n@@ -993,2 +1233,1 @@\n-          CallProjections callprojs;\n-          ac->extract_projections(&callprojs, true);\n+          CallProjections* callprojs = ac->extract_projections(true);\n@@ -996,3 +1235,3 @@\n-          _igvn.replace_node(callprojs.fallthrough_ioproj, ac->in(TypeFunc::I_O));\n-          _igvn.replace_node(callprojs.fallthrough_memproj, ac->in(TypeFunc::Memory));\n-          _igvn.replace_node(callprojs.fallthrough_catchproj, ac->in(TypeFunc::Control));\n+          _igvn.replace_node(callprojs->fallthrough_ioproj, ac->in(TypeFunc::I_O));\n+          _igvn.replace_node(callprojs->fallthrough_memproj, ac->in(TypeFunc::Memory));\n+          _igvn.replace_node(callprojs->fallthrough_catchproj, ac->in(TypeFunc::Control));\n@@ -1015,0 +1254,24 @@\n+      } else if (use->is_InlineType()) {\n+        assert(use->as_InlineType()->get_oop() == res, \"unexpected inline type ptr use\");\n+        \/\/ Cut off oop input and remove known instance id from type\n+        _igvn.rehash_node_delayed(use);\n+        use->as_InlineType()->set_oop(_igvn, _igvn.zerocon(T_OBJECT));\n+        use->as_InlineType()->set_is_buffered(_igvn, false);\n+        const TypeOopPtr* toop = _igvn.type(use)->is_oopptr()->cast_to_instance_id(TypeOopPtr::InstanceBot);\n+        _igvn.set_type(use, toop);\n+        use->as_InlineType()->set_type(toop);\n+        \/\/ Process users\n+        for (DUIterator_Fast kmax, k = use->fast_outs(kmax); k < kmax; k++) {\n+          Node* u = use->fast_out(k);\n+          if (!u->is_InlineType() && !u->is_StoreFlat()) {\n+            worklist.push(u);\n+          }\n+        }\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ Store to mark word of inline type larval buffer\n+        assert(inline_alloc, \"Unexpected store to mark word\");\n+        _igvn.replace_node(use, use->in(MemNode::Memory));\n+      } else if (use->Opcode() == Op_MemBarRelease || use->Opcode() == Op_MemBarStoreStore) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n+        assert(inline_alloc, \"Unexpected MemBarRelease\");\n+        use->as_MemBar()->remove(&_igvn);\n@@ -1027,1 +1290,1 @@\n-  if (_callprojs.resproj != nullptr && _callprojs.resproj->outcnt() != 0) {\n+  if (_callprojs->resproj[0] != nullptr && _callprojs->resproj[0]->outcnt() != 0) {\n@@ -1031,2 +1294,2 @@\n-    for (DUIterator_Fast jmax, j = _callprojs.resproj->fast_outs(jmax);  j < jmax; j++) {\n-      Node* use = _callprojs.resproj->fast_out(j);\n+    for (DUIterator_Fast jmax, j = _callprojs->resproj[0]->fast_outs(jmax);  j < jmax; j++) {\n+      Node* use = _callprojs->resproj[0]->fast_out(j);\n@@ -1039,3 +1302,3 @@\n-    for (DUIterator_Last jmin, j = _callprojs.resproj->last_outs(jmin); j >= jmin; ) {\n-      Node* use = _callprojs.resproj->last_out(j);\n-      uint oc1 = _callprojs.resproj->outcnt();\n+    for (DUIterator_Last jmin, j = _callprojs->resproj[0]->last_outs(jmin); j >= jmin; ) {\n+      Node* use = _callprojs->resproj[0]->last_out(j);\n+      uint oc1 = _callprojs->resproj[0]->outcnt();\n@@ -1051,1 +1314,1 @@\n-          assert(tmp == nullptr || tmp == _callprojs.fallthrough_catchproj, \"allocation control projection\");\n+          assert(tmp == nullptr || tmp == _callprojs->fallthrough_catchproj, \"allocation control projection\");\n@@ -1058,1 +1321,1 @@\n-            assert(mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw) == _callprojs.fallthrough_memproj, \"allocation memory projection\");\n+            assert(mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw) == _callprojs->fallthrough_memproj, \"allocation memory projection\");\n@@ -1060,1 +1323,1 @@\n-            assert(mem == _callprojs.fallthrough_memproj, \"allocation memory projection\");\n+            assert(mem == _callprojs->fallthrough_memproj, \"allocation memory projection\");\n@@ -1066,0 +1329,4 @@\n+      } else if (use->Opcode() == Op_MemBarStoreStore) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n+        assert(inline_alloc, \"Unexpected MemBarStoreStore\");\n+        use->as_MemBar()->remove(&_igvn);\n@@ -1069,1 +1336,1 @@\n-      j -= (oc1 - _callprojs.resproj->outcnt());\n+      j -= (oc1 - _callprojs->resproj[0]->outcnt());\n@@ -1072,2 +1339,2 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, alloc->in(TypeFunc::Control));\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, alloc->in(TypeFunc::Control));\n@@ -1075,2 +1342,2 @@\n-  if (_callprojs.fallthrough_memproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_memproj, alloc->in(TypeFunc::Memory));\n+  if (_callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_memproj, alloc->in(TypeFunc::Memory));\n@@ -1078,2 +1345,2 @@\n-  if (_callprojs.catchall_memproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_memproj, C->top());\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_memproj, C->top());\n@@ -1081,2 +1348,2 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -1084,2 +1351,2 @@\n-  if (_callprojs.catchall_ioproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_ioproj, C->top());\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_ioproj, C->top());\n@@ -1087,2 +1354,2 @@\n-  if (_callprojs.catchall_catchproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_catchproj, C->top());\n+  if (_callprojs->catchall_catchproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_catchproj, C->top());\n@@ -1098,1 +1365,1 @@\n-  if (!EliminateAllocations || !alloc->_is_non_escaping) {\n+  if (!EliminateAllocations) {\n@@ -1103,1 +1370,8 @@\n-  Node* res = alloc->result_cast();\n+\n+  \/\/ Attempt to eliminate inline type buffer allocations\n+  \/\/ regardless of usage and escape\/replaceable status.\n+  bool inline_alloc = tklass->isa_instklassptr() &&\n+                      tklass->is_instklassptr()->instance_klass()->is_inlinetype();\n+  if (!alloc->_is_non_escaping && !inline_alloc) {\n+    return false;\n+  }\n@@ -1106,1 +1380,2 @@\n-  bool boxing_alloc = C->eliminate_boxing() &&\n+  Node* res = alloc->result_cast();\n+  bool boxing_alloc = (res == nullptr) && C->eliminate_boxing() &&\n@@ -1109,1 +1384,1 @@\n-  if (!alloc->_is_scalar_replaceable && (!boxing_alloc || (res != nullptr))) {\n+  if (!alloc->_is_scalar_replaceable && !boxing_alloc && !inline_alloc) {\n@@ -1113,1 +1388,1 @@\n-  alloc->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = alloc->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1121,1 +1396,1 @@\n-    assert(res == nullptr, \"sanity\");\n+    assert(res == nullptr || inline_alloc, \"sanity\");\n@@ -1146,1 +1421,1 @@\n-  process_users_of_allocation(alloc);\n+  process_users_of_allocation(alloc, inline_alloc);\n@@ -1168,1 +1443,1 @@\n-  boxing->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = boxing->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1170,1 +1445,1 @@\n-  const TypeTuple* r = boxing->tf()->range();\n+  const TypeTuple* r = boxing->tf()->range_sig();\n@@ -1274,0 +1549,1 @@\n+            Node* init_val, \/\/ value to initialize the array with\n@@ -1356,1 +1632,1 @@\n-    Node *toobig_true = new IfTrueNode( toobig_iff );\n+    Node* toobig_true = new IfTrueNode(toobig_iff);\n@@ -1359,1 +1635,1 @@\n-    toobig_false = new IfFalseNode( toobig_iff );\n+    toobig_false = new IfFalseNode(toobig_iff);\n@@ -1398,0 +1674,1 @@\n+\n@@ -1455,0 +1732,6 @@\n+    if (init_val != nullptr) {\n+      call->init_req(TypeFunc::Parms+2, init_val);\n+    }\n+  } else {\n+    \/\/ Let the runtime know if this is a larval allocation\n+    call->init_req(TypeFunc::Parms+1, _igvn.intcon(alloc->_larval));\n@@ -1486,1 +1769,1 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1492,2 +1775,2 @@\n-  if (expand_fast_path && _callprojs.fallthrough_memproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_memproj, result_phi_rawmem);\n+  if (expand_fast_path && _callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_memproj, result_phi_rawmem);\n@@ -1497,4 +1780,4 @@\n-  if (_callprojs.catchall_memproj != nullptr ) {\n-    if (_callprojs.fallthrough_memproj == nullptr) {\n-      _callprojs.fallthrough_memproj = new ProjNode(call, TypeFunc::Memory);\n-      transform_later(_callprojs.fallthrough_memproj);\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    if (_callprojs->fallthrough_memproj == nullptr) {\n+      _callprojs->fallthrough_memproj = new ProjNode(call, TypeFunc::Memory);\n+      transform_later(_callprojs->fallthrough_memproj);\n@@ -1502,2 +1785,2 @@\n-    migrate_outs(_callprojs.catchall_memproj, _callprojs.fallthrough_memproj);\n-    _igvn.remove_dead_node(_callprojs.catchall_memproj);\n+    _igvn.replace_in_uses(_callprojs->catchall_memproj, _callprojs->fallthrough_memproj);\n+    _igvn.remove_dead_node(_callprojs->catchall_memproj);\n@@ -1511,2 +1794,2 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_ioproj, result_phi_i_o);\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_ioproj, result_phi_i_o);\n@@ -1516,4 +1799,4 @@\n-  if (_callprojs.catchall_ioproj != nullptr ) {\n-    if (_callprojs.fallthrough_ioproj == nullptr) {\n-      _callprojs.fallthrough_ioproj = new ProjNode(call, TypeFunc::I_O);\n-      transform_later(_callprojs.fallthrough_ioproj);\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    if (_callprojs->fallthrough_ioproj == nullptr) {\n+      _callprojs->fallthrough_ioproj = new ProjNode(call, TypeFunc::I_O);\n+      transform_later(_callprojs->fallthrough_ioproj);\n@@ -1521,2 +1804,2 @@\n-    migrate_outs(_callprojs.catchall_ioproj, _callprojs.fallthrough_ioproj);\n-    _igvn.remove_dead_node(_callprojs.catchall_ioproj);\n+    _igvn.replace_in_uses(_callprojs->catchall_ioproj, _callprojs->fallthrough_ioproj);\n+    _igvn.remove_dead_node(_callprojs->catchall_ioproj);\n@@ -1541,2 +1824,2 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    ctrl = _callprojs.fallthrough_catchproj->clone();\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    ctrl = _callprojs->fallthrough_catchproj->clone();\n@@ -1544,1 +1827,1 @@\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, result_region);\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, result_region);\n@@ -1549,1 +1832,1 @@\n-  if (_callprojs.resproj == nullptr) {\n+  if (_callprojs->resproj[0] == nullptr) {\n@@ -1553,1 +1836,1 @@\n-    slow_result = _callprojs.resproj->clone();\n+    slow_result = _callprojs->resproj[0]->clone();\n@@ -1555,1 +1838,1 @@\n-    _igvn.replace_node(_callprojs.resproj, result_phi_rawoop);\n+    _igvn.replace_node(_callprojs->resproj[0], result_phi_rawoop);\n@@ -1565,1 +1848,1 @@\n-  result_phi_rawmem->init_req(slow_result_path, _callprojs.fallthrough_memproj);\n+  result_phi_rawmem->init_req(slow_result_path, _callprojs->fallthrough_memproj);\n@@ -1577,4 +1860,4 @@\n-  alloc->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  if (_callprojs.resproj != nullptr) {\n-    for (DUIterator_Fast imax, i = _callprojs.resproj->fast_outs(imax); i < imax; i++) {\n-      Node* use = _callprojs.resproj->fast_out(i);\n+  _callprojs = alloc->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  if (_callprojs->resproj[0] != nullptr) {\n+    for (DUIterator_Fast imax, i = _callprojs->resproj[0]->fast_outs(imax); i < imax; i++) {\n+      Node* use = _callprojs->resproj[0]->fast_out(i);\n@@ -1585,2 +1868,2 @@\n-    assert(_callprojs.resproj->outcnt() == 0, \"all uses must be deleted\");\n-    _igvn.remove_dead_node(_callprojs.resproj);\n+    assert(_callprojs->resproj[0]->outcnt() == 0, \"all uses must be deleted\");\n+    _igvn.remove_dead_node(_callprojs->resproj[0]);\n@@ -1588,3 +1871,3 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_catchproj, ctrl);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_catchproj);\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_catchproj, ctrl);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_catchproj);\n@@ -1592,3 +1875,3 @@\n-  if (_callprojs.catchall_catchproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_catchproj);\n-    _callprojs.catchall_catchproj->set_req(0, top());\n+  if (_callprojs->catchall_catchproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_catchproj);\n+    _callprojs->catchall_catchproj->set_req(0, top());\n@@ -1596,2 +1879,2 @@\n-  if (_callprojs.fallthrough_proj != nullptr) {\n-    Node* catchnode = _callprojs.fallthrough_proj->unique_ctrl_out();\n+  if (_callprojs->fallthrough_proj != nullptr) {\n+    Node* catchnode = _callprojs->fallthrough_proj->unique_ctrl_out();\n@@ -1599,1 +1882,1 @@\n-    _igvn.remove_dead_node(_callprojs.fallthrough_proj);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_proj);\n@@ -1601,3 +1884,3 @@\n-  if (_callprojs.fallthrough_memproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_memproj, mem);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_memproj);\n+  if (_callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_memproj, mem);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_memproj);\n@@ -1605,3 +1888,3 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_ioproj, i_o);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_ioproj);\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_ioproj, i_o);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_ioproj);\n@@ -1609,3 +1892,3 @@\n-  if (_callprojs.catchall_memproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_memproj);\n-    _callprojs.catchall_memproj->set_req(0, top());\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_memproj);\n+    _callprojs->catchall_memproj->set_req(0, top());\n@@ -1613,3 +1896,3 @@\n-  if (_callprojs.catchall_ioproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_ioproj);\n-    _callprojs.catchall_ioproj->set_req(0, top());\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_ioproj);\n+    _callprojs->catchall_ioproj->set_req(0, top());\n@@ -1749,5 +2032,4 @@\n-Node*\n-PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n-                                    Node* control, Node* rawmem, Node* object,\n-                                    Node* klass_node, Node* length,\n-                                    Node* size_in_bytes) {\n+Node* PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n+                                          Node* control, Node* rawmem, Node* object,\n+                                          Node* klass_node, Node* length,\n+                                          Node* size_in_bytes) {\n@@ -1756,1 +2038,1 @@\n-  Node* mark_node = alloc->make_ideal_mark(&_igvn, object, control, rawmem);\n+  Node* mark_node = alloc->make_ideal_mark(&_igvn, control, rawmem);\n@@ -1794,0 +2076,2 @@\n+                                            alloc->in(AllocateNode::InitValue),\n+                                            alloc->in(AllocateNode::RawInitValue),\n@@ -1967,1 +2251,1 @@\n-  expand_allocate_common(alloc, nullptr,\n+  expand_allocate_common(alloc, nullptr, nullptr,\n@@ -1977,0 +2261,1 @@\n+  Node* init_value = alloc->in(AllocateNode::InitValue);\n@@ -1978,0 +2263,3 @@\n+  assert(!ary_klass_t || !ary_klass_t->klass_is_exact() || !ary_klass_t->exact_klass()->is_obj_array_klass() ||\n+         ary_klass_t->is_refined_type(), \"Must be a refined array klass\");\n+  const TypeFunc* slow_call_type;\n@@ -1984,0 +2272,1 @@\n+    slow_call_type = OptoRuntime::new_array_nozero_Type();\n@@ -1986,0 +2275,7 @@\n+    slow_call_type = OptoRuntime::new_array_Type();\n+\n+    if (init_value == nullptr) {\n+      init_value = _igvn.zerocon(T_OBJECT);\n+    } else if (UseCompressedOops) {\n+      init_value = transform_later(new DecodeNNode(init_value, init_value->bottom_type()->make_ptr()));\n+    }\n@@ -1987,2 +2283,2 @@\n-  expand_allocate_common(alloc, length,\n-                         OptoRuntime::new_array_Type(),\n+  expand_allocate_common(alloc, length, init_value,\n+                         slow_call_type,\n@@ -2200,1 +2496,1 @@\n-  alock->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = alock->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -2204,2 +2500,2 @@\n-         _callprojs.fallthrough_proj != nullptr &&\n-         _callprojs.fallthrough_memproj != nullptr,\n+         _callprojs->fallthrough_proj != nullptr &&\n+         _callprojs->fallthrough_memproj != nullptr,\n@@ -2208,2 +2504,2 @@\n-  Node* fallthroughproj = _callprojs.fallthrough_proj;\n-  Node* memproj_fallthrough = _callprojs.fallthrough_memproj;\n+  Node* fallthroughproj = _callprojs->fallthrough_proj;\n+  Node* memproj_fallthrough = _callprojs->fallthrough_memproj;\n@@ -2280,1 +2576,1 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -2286,2 +2582,2 @@\n-  assert(_callprojs.fallthrough_ioproj == nullptr && _callprojs.catchall_ioproj == nullptr &&\n-         _callprojs.catchall_memproj == nullptr && _callprojs.catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n+  assert(_callprojs->fallthrough_ioproj == nullptr && _callprojs->catchall_ioproj == nullptr &&\n+         _callprojs->catchall_memproj == nullptr && _callprojs->catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n@@ -2292,1 +2588,1 @@\n-  Node *slow_ctrl = _callprojs.fallthrough_proj->clone();\n+  Node *slow_ctrl = _callprojs->fallthrough_proj->clone();\n@@ -2294,2 +2590,2 @@\n-  _igvn.hash_delete(_callprojs.fallthrough_proj);\n-  _callprojs.fallthrough_proj->disconnect_inputs(C);\n+  _igvn.hash_delete(_callprojs->fallthrough_proj);\n+  _callprojs->fallthrough_proj->disconnect_inputs(C);\n@@ -2299,1 +2595,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_proj, region);\n+  _igvn.replace_node(_callprojs->fallthrough_proj, region);\n@@ -2307,1 +2603,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, mem_phi);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, mem_phi);\n@@ -2340,3 +2636,3 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  assert(_callprojs.fallthrough_ioproj == nullptr && _callprojs.catchall_ioproj == nullptr &&\n-         _callprojs.catchall_memproj == nullptr && _callprojs.catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  assert(_callprojs->fallthrough_ioproj == nullptr && _callprojs->catchall_ioproj == nullptr &&\n+         _callprojs->catchall_memproj == nullptr && _callprojs->catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n@@ -2348,1 +2644,1 @@\n-  Node *slow_ctrl = _callprojs.fallthrough_proj->clone();\n+  Node *slow_ctrl = _callprojs->fallthrough_proj->clone();\n@@ -2350,2 +2646,2 @@\n-  _igvn.hash_delete(_callprojs.fallthrough_proj);\n-  _callprojs.fallthrough_proj->disconnect_inputs(C);\n+  _igvn.hash_delete(_callprojs->fallthrough_proj);\n+  _callprojs->fallthrough_proj->disconnect_inputs(C);\n@@ -2355,1 +2651,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_proj, region);\n+  _igvn.replace_node(_callprojs->fallthrough_proj, region);\n@@ -2362,1 +2658,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, mem_phi);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, mem_phi);\n@@ -2365,0 +2661,222 @@\n+\/\/ An inline type might be returned from the call but we don't know its\n+\/\/ type. Either we get a buffered inline type (and nothing needs to be done)\n+\/\/ or one of the values being returned is the klass of the inline type\n+\/\/ and we need to allocate an inline type instance of that type and\n+\/\/ initialize it with other values being returned. In that case, we\n+\/\/ first try a fast path allocation and initialize the value with the\n+\/\/ inline klass's pack handler or we fall back to a runtime call.\n+void PhaseMacroExpand::expand_mh_intrinsic_return(CallStaticJavaNode* call) {\n+  assert(call->method()->is_method_handle_intrinsic(), \"must be a method handle intrinsic call\");\n+  Node* ret = call->proj_out_or_null(TypeFunc::Parms);\n+  if (ret == nullptr) {\n+    return;\n+  }\n+  const TypeFunc* tf = call->_tf;\n+  const TypeTuple* domain = OptoRuntime::store_inline_type_fields_Type()->domain_cc();\n+  const TypeFunc* new_tf = TypeFunc::make(tf->domain_sig(), tf->domain_cc(), tf->range_sig(), domain);\n+  call->_tf = new_tf;\n+  \/\/ Make sure the change of type is applied before projections are processed by igvn\n+  _igvn.set_type(call, call->Value(&_igvn));\n+  _igvn.set_type(ret, ret->Value(&_igvn));\n+\n+  \/\/ Before any new projection is added:\n+  CallProjections* projs = call->extract_projections(true, true);\n+\n+  \/\/ Create temporary hook nodes that will be replaced below.\n+  \/\/ Add an input to prevent hook nodes from being dead.\n+  Node* ctl = new Node(call);\n+  Node* mem = new Node(ctl);\n+  Node* io = new Node(ctl);\n+  Node* ex_ctl = new Node(ctl);\n+  Node* ex_mem = new Node(ctl);\n+  Node* ex_io = new Node(ctl);\n+  Node* res = new Node(ctl);\n+\n+  \/\/ Allocate a new buffered inline type only if a new one is not returned\n+  Node* cast = transform_later(new CastP2XNode(ctl, res));\n+  Node* mask = MakeConX(0x1);\n+  Node* masked = transform_later(new AndXNode(cast, mask));\n+  Node* cmp = transform_later(new CmpXNode(masked, mask));\n+  Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+  IfNode* allocation_iff = new IfNode(ctl, bol, PROB_MAX, COUNT_UNKNOWN);\n+  transform_later(allocation_iff);\n+  Node* allocation_ctl = transform_later(new IfTrueNode(allocation_iff));\n+  Node* no_allocation_ctl = transform_later(new IfFalseNode(allocation_iff));\n+  Node* no_allocation_res = transform_later(new CheckCastPPNode(no_allocation_ctl, res, TypeInstPtr::BOTTOM));\n+\n+  \/\/ Try to allocate a new buffered inline instance either from TLAB or eden space\n+  Node* needgc_ctrl = nullptr; \/\/ needgc means slowcase, i.e. allocation failed\n+  CallLeafNoFPNode* handler_call;\n+  const bool alloc_in_place = UseTLAB;\n+  if (alloc_in_place) {\n+    Node* fast_oop_ctrl = nullptr;\n+    Node* fast_oop_rawmem = nullptr;\n+    Node* mask2 = MakeConX(-2);\n+    Node* masked2 = transform_later(new AndXNode(cast, mask2));\n+    Node* rawklassptr = transform_later(new CastX2PNode(masked2));\n+    Node* klass_node = transform_later(new CheckCastPPNode(allocation_ctl, rawklassptr, TypeInstKlassPtr::OBJECT_OR_NULL));\n+    Node* layout_val = make_load(nullptr, mem, klass_node, in_bytes(Klass::layout_helper_offset()), TypeInt::INT, T_INT);\n+    Node* size_in_bytes = ConvI2X(layout_val);\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    Node* fast_oop = bs->obj_allocate(this, mem, allocation_ctl, size_in_bytes, io, needgc_ctrl,\n+                                      fast_oop_ctrl, fast_oop_rawmem,\n+                                      AllocateInstancePrefetchLines);\n+    \/\/ Allocation succeed, initialize buffered inline instance header firstly,\n+    \/\/ and then initialize its fields with an inline class specific handler\n+    Node* mark_word_node;\n+    if (UseCompactObjectHeaders) {\n+      \/\/ COH: We need to load the prototype from the klass at runtime since it encodes the klass pointer already.\n+      mark_word_node = make_load(fast_oop_ctrl, fast_oop_rawmem, klass_node, in_bytes(Klass::prototype_header_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+    } else {\n+      \/\/ Otherwise, use the static prototype.\n+      mark_word_node = makecon(TypeRawPtr::make((address)markWord::inline_type_prototype().value()));\n+    }\n+\n+    fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::mark_offset_in_bytes(), mark_word_node, T_ADDRESS);\n+    if (!UseCompactObjectHeaders) {\n+      \/\/ COH: Everything is encoded in the mark word, so nothing left to do.\n+      fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+      if (UseCompressedClassPointers) {\n+        fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::klass_gap_offset_in_bytes(), intcon(0), T_INT);\n+      }\n+    }\n+    Node* fixed_block  = make_load(fast_oop_ctrl, fast_oop_rawmem, klass_node, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+    Node* pack_handler = make_load(fast_oop_ctrl, fast_oop_rawmem, fixed_block, in_bytes(InlineKlass::pack_handler_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+    handler_call = new CallLeafNoFPNode(OptoRuntime::pack_inline_type_Type(),\n+                                        nullptr,\n+                                        \"pack handler\",\n+                                        TypeRawPtr::BOTTOM);\n+    handler_call->init_req(TypeFunc::Control, fast_oop_ctrl);\n+    handler_call->init_req(TypeFunc::Memory, fast_oop_rawmem);\n+    handler_call->init_req(TypeFunc::I_O, top());\n+    handler_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+    handler_call->init_req(TypeFunc::ReturnAdr, top());\n+    handler_call->init_req(TypeFunc::Parms, pack_handler);\n+    handler_call->init_req(TypeFunc::Parms+1, fast_oop);\n+  } else {\n+    needgc_ctrl = allocation_ctl;\n+  }\n+\n+  \/\/ Allocation failed, fall back to a runtime call\n+  CallStaticJavaNode* slow_call = new CallStaticJavaNode(OptoRuntime::store_inline_type_fields_Type(),\n+                                                         StubRoutines::store_inline_type_fields_to_buf(),\n+                                                         \"store_inline_type_fields\",\n+                                                         TypePtr::BOTTOM);\n+  slow_call->init_req(TypeFunc::Control, needgc_ctrl);\n+  slow_call->init_req(TypeFunc::Memory, mem);\n+  slow_call->init_req(TypeFunc::I_O, io);\n+  slow_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+  slow_call->init_req(TypeFunc::ReturnAdr, call->in(TypeFunc::ReturnAdr));\n+  slow_call->init_req(TypeFunc::Parms, res);\n+\n+  Node* slow_ctl = transform_later(new ProjNode(slow_call, TypeFunc::Control));\n+  Node* slow_mem = transform_later(new ProjNode(slow_call, TypeFunc::Memory));\n+  Node* slow_io = transform_later(new ProjNode(slow_call, TypeFunc::I_O));\n+  Node* slow_res = transform_later(new ProjNode(slow_call, TypeFunc::Parms));\n+  Node* slow_catc = transform_later(new CatchNode(slow_ctl, slow_io, 2));\n+  Node* slow_norm = transform_later(new CatchProjNode(slow_catc, CatchProjNode::fall_through_index, CatchProjNode::no_handler_bci));\n+  Node* slow_excp = transform_later(new CatchProjNode(slow_catc, CatchProjNode::catch_all_index,    CatchProjNode::no_handler_bci));\n+\n+  Node* ex_r = new RegionNode(3);\n+  Node* ex_mem_phi = new PhiNode(ex_r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* ex_io_phi = new PhiNode(ex_r, Type::ABIO);\n+  ex_r->init_req(1, slow_excp);\n+  ex_mem_phi->init_req(1, slow_mem);\n+  ex_io_phi->init_req(1, slow_io);\n+  ex_r->init_req(2, ex_ctl);\n+  ex_mem_phi->init_req(2, ex_mem);\n+  ex_io_phi->init_req(2, ex_io);\n+  transform_later(ex_r);\n+  transform_later(ex_mem_phi);\n+  transform_later(ex_io_phi);\n+\n+  \/\/ We don't know how many values are returned. This assumes the\n+  \/\/ worst case, that all available registers are used.\n+  for (uint i = TypeFunc::Parms+1; i < domain->cnt(); i++) {\n+    if (domain->field_at(i) == Type::HALF) {\n+      slow_call->init_req(i, top());\n+      if (alloc_in_place) {\n+        handler_call->init_req(i+1, top());\n+      }\n+      continue;\n+    }\n+    Node* proj = transform_later(new ProjNode(call, i));\n+    slow_call->init_req(i, proj);\n+    if (alloc_in_place) {\n+      handler_call->init_req(i+1, proj);\n+    }\n+  }\n+  \/\/ We can safepoint at that new call\n+  slow_call->copy_call_debug_info(&_igvn, call);\n+  transform_later(slow_call);\n+  if (alloc_in_place) {\n+    transform_later(handler_call);\n+  }\n+\n+  Node* fast_ctl = nullptr;\n+  Node* fast_res = nullptr;\n+  MergeMemNode* fast_mem = nullptr;\n+  if (alloc_in_place) {\n+    fast_ctl = transform_later(new ProjNode(handler_call, TypeFunc::Control));\n+    Node* rawmem = transform_later(new ProjNode(handler_call, TypeFunc::Memory));\n+    fast_res = transform_later(new ProjNode(handler_call, TypeFunc::Parms));\n+    fast_mem = MergeMemNode::make(mem);\n+    fast_mem->set_memory_at(Compile::AliasIdxRaw, rawmem);\n+    transform_later(fast_mem);\n+  }\n+\n+  Node* r = new RegionNode(alloc_in_place ? 4 : 3);\n+  Node* mem_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* io_phi = new PhiNode(r, Type::ABIO);\n+  Node* res_phi = new PhiNode(r, TypeInstPtr::BOTTOM);\n+  r->init_req(1, no_allocation_ctl);\n+  mem_phi->init_req(1, mem);\n+  io_phi->init_req(1, io);\n+  res_phi->init_req(1, no_allocation_res);\n+  r->init_req(2, slow_norm);\n+  mem_phi->init_req(2, slow_mem);\n+  io_phi->init_req(2, slow_io);\n+  res_phi->init_req(2, slow_res);\n+  if (alloc_in_place) {\n+    r->init_req(3, fast_ctl);\n+    mem_phi->init_req(3, fast_mem);\n+    io_phi->init_req(3, io);\n+    res_phi->init_req(3, fast_res);\n+  }\n+  transform_later(r);\n+  transform_later(mem_phi);\n+  transform_later(io_phi);\n+  transform_later(res_phi);\n+\n+  \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+  \/\/ store that would make this buffer accessible by other threads.\n+  MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxBot);\n+  transform_later(mb);\n+  mb->init_req(TypeFunc::Memory, mem_phi);\n+  mb->init_req(TypeFunc::Control, r);\n+  r = new ProjNode(mb, TypeFunc::Control);\n+  transform_later(r);\n+  mem_phi = new ProjNode(mb, TypeFunc::Memory);\n+  transform_later(mem_phi);\n+\n+  assert(projs->nb_resproj == 1, \"unexpected number of results\");\n+  _igvn.replace_in_uses(projs->fallthrough_catchproj, r);\n+  _igvn.replace_in_uses(projs->fallthrough_memproj, mem_phi);\n+  _igvn.replace_in_uses(projs->fallthrough_ioproj, io_phi);\n+  _igvn.replace_in_uses(projs->resproj[0], res_phi);\n+  _igvn.replace_in_uses(projs->catchall_catchproj, ex_r);\n+  _igvn.replace_in_uses(projs->catchall_memproj, ex_mem_phi);\n+  _igvn.replace_in_uses(projs->catchall_ioproj, ex_io_phi);\n+  \/\/ The CatchNode should not use the ex_io_phi. Re-connect it to the catchall_ioproj.\n+  Node* cn = projs->fallthrough_catchproj->in(0);\n+  _igvn.replace_input_of(cn, 1, projs->catchall_ioproj);\n+\n+  _igvn.replace_node(ctl, projs->fallthrough_catchproj);\n+  _igvn.replace_node(mem, projs->fallthrough_memproj);\n+  _igvn.replace_node(io, projs->fallthrough_ioproj);\n+  _igvn.replace_node(res, projs->resproj[0]);\n+  _igvn.replace_node(ex_ctl, projs->catchall_catchproj);\n+  _igvn.replace_node(ex_mem, projs->catchall_memproj);\n+  _igvn.replace_node(ex_io, projs->catchall_ioproj);\n+ }\n+\n@@ -2390,1 +2908,1 @@\n-      subklass = _igvn.transform(LoadKlassNode::make(_igvn, C->immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+      subklass = _igvn.transform(LoadKlassNode::make(_igvn, C->immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n@@ -2402,0 +2920,113 @@\n+\/\/ FlatArrayCheckNode (array1 array2 ...) is expanded into:\n+\/\/\n+\/\/ long mark = array1.mark | array2.mark | ...;\n+\/\/ long locked_bit = markWord::unlocked_value & array1.mark & array2.mark & ...;\n+\/\/ if (locked_bit == 0) {\n+\/\/   \/\/ One array is locked, load prototype header from the klass\n+\/\/   mark = array1.klass.proto | array2.klass.proto | ...\n+\/\/ }\n+\/\/ if ((mark & markWord::flat_array_bit_in_place) == 0) {\n+\/\/    ...\n+\/\/ }\n+void PhaseMacroExpand::expand_flatarraycheck_node(FlatArrayCheckNode* check) {\n+  bool array_inputs = _igvn.type(check->in(FlatArrayCheckNode::ArrayOrKlass))->isa_oopptr() != nullptr;\n+  if (array_inputs) {\n+    Node* mark = MakeConX(0);\n+    Node* locked_bit = MakeConX(markWord::unlocked_value);\n+    Node* mem = check->in(FlatArrayCheckNode::Memory);\n+    for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+      Node* ary = check->in(i);\n+      const TypeOopPtr* t = _igvn.type(ary)->isa_oopptr();\n+      assert(t != nullptr, \"Mixing array and klass inputs\");\n+      assert(!t->is_flat() && !t->is_not_flat(), \"Should have been optimized out\");\n+      Node* mark_adr = basic_plus_adr(ary, oopDesc::mark_offset_in_bytes());\n+      Node* mark_load = _igvn.transform(LoadNode::make(_igvn, nullptr, mem, mark_adr, mark_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+      mark = _igvn.transform(new OrXNode(mark, mark_load));\n+      locked_bit = _igvn.transform(new AndXNode(locked_bit, mark_load));\n+    }\n+    assert(!mark->is_Con(), \"Should have been optimized out\");\n+    Node* cmp = _igvn.transform(new CmpXNode(locked_bit, MakeConX(0)));\n+    Node* is_unlocked = _igvn.transform(new BoolNode(cmp, BoolTest::ne));\n+\n+    \/\/ BoolNode might be shared, replace each if user\n+    Node* old_bol = check->unique_out();\n+    assert(old_bol->is_Bool() && old_bol->as_Bool()->_test._test == BoolTest::ne, \"unexpected condition\");\n+    for (DUIterator_Last imin, i = old_bol->last_outs(imin); i >= imin; --i) {\n+      IfNode* old_iff = old_bol->last_out(i)->as_If();\n+      Node* ctrl = old_iff->in(0);\n+      RegionNode* region = new RegionNode(3);\n+      Node* mark_phi = new PhiNode(region, TypeX_X);\n+\n+      \/\/ Check if array is unlocked\n+      IfNode* iff = _igvn.transform(new IfNode(ctrl, is_unlocked, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+\n+      \/\/ Unlocked: Use bits from mark word\n+      region->init_req(1, _igvn.transform(new IfTrueNode(iff)));\n+      mark_phi->init_req(1, mark);\n+\n+      \/\/ Locked: Load prototype header from klass\n+      ctrl = _igvn.transform(new IfFalseNode(iff));\n+      Node* proto = MakeConX(0);\n+      for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+        Node* ary = check->in(i);\n+        \/\/ Make loads control dependent to make sure they are only executed if array is locked\n+        Node* klass_adr = basic_plus_adr(ary, oopDesc::klass_offset_in_bytes());\n+        Node* klass = _igvn.transform(LoadKlassNode::make(_igvn, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+        Node* proto_adr = basic_plus_adr(klass, in_bytes(Klass::prototype_header_offset()));\n+        Node* proto_load = _igvn.transform(LoadNode::make(_igvn, ctrl, C->immutable_memory(), proto_adr, proto_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+        proto = _igvn.transform(new OrXNode(proto, proto_load));\n+      }\n+      region->init_req(2, ctrl);\n+      mark_phi->init_req(2, proto);\n+\n+      \/\/ Check if flat array bits are set\n+      Node* mask = MakeConX(markWord::flat_array_bit_in_place);\n+      Node* masked = _igvn.transform(new AndXNode(_igvn.transform(mark_phi), mask));\n+      cmp = _igvn.transform(new CmpXNode(masked, MakeConX(0)));\n+      Node* is_not_flat = _igvn.transform(new BoolNode(cmp, BoolTest::eq));\n+\n+      ctrl = _igvn.transform(region);\n+      iff = _igvn.transform(new IfNode(ctrl, is_not_flat, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+      _igvn.replace_node(old_iff, iff);\n+    }\n+    _igvn.replace_node(check, C->top());\n+  } else {\n+    \/\/ Fall back to layout helper check\n+    Node* lhs = intcon(0);\n+    for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+      Node* array_or_klass = check->in(i);\n+      Node* klass = nullptr;\n+      const TypePtr* t = _igvn.type(array_or_klass)->is_ptr();\n+      assert(!t->is_flat() && !t->is_not_flat(), \"Should have been optimized out\");\n+      if (t->isa_oopptr() != nullptr) {\n+        Node* klass_adr = basic_plus_adr(array_or_klass, oopDesc::klass_offset_in_bytes());\n+        klass = transform_later(LoadKlassNode::make(_igvn, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+      } else {\n+        assert(t->isa_klassptr(), \"Unexpected input type\");\n+        klass = array_or_klass;\n+      }\n+      Node* lh_addr = basic_plus_adr(klass, in_bytes(Klass::layout_helper_offset()));\n+      Node* lh_val = _igvn.transform(LoadNode::make(_igvn, nullptr, C->immutable_memory(), lh_addr, lh_addr->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));\n+      lhs = _igvn.transform(new OrINode(lhs, lh_val));\n+    }\n+    Node* masked = transform_later(new AndINode(lhs, intcon(Klass::_lh_array_tag_flat_value_bit_inplace)));\n+    Node* cmp = transform_later(new CmpINode(masked, intcon(0)));\n+    Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+    Node* m2b = transform_later(new Conv2BNode(masked));\n+    \/\/ The matcher expects the input to If\/CMove nodes to be produced by a Bool(CmpI..)\n+    \/\/ pattern, but the input to other potential users (e.g. Phi) to be some\n+    \/\/ other pattern (e.g. a Conv2B node, possibly idealized as a CMoveI).\n+    Node* old_bol = check->unique_out();\n+    for (DUIterator_Last imin, i = old_bol->last_outs(imin); i >= imin; --i) {\n+      Node* user = old_bol->last_out(i);\n+      for (uint j = 0; j < user->req(); j++) {\n+        Node* n = user->in(j);\n+        if (n == old_bol) {\n+          _igvn.replace_input_of(user, j, (user->is_If() || user->is_CMove()) ? bol : m2b);\n+        }\n+      }\n+    }\n+    _igvn.replace_node(check, C->top());\n+  }\n+}\n+\n@@ -2414,2 +3045,2 @@\n-void PhaseMacroExpand::eliminate_macro_nodes() {\n-  if (C->macro_count() == 0)\n+void PhaseMacroExpand::eliminate_macro_nodes(bool eliminate_locks) {\n+  if (C->macro_count() == 0) {\n@@ -2417,0 +3048,1 @@\n+  }\n@@ -2423,7 +3055,5 @@\n-  \/\/ Before elimination may re-mark (change to Nested or NonEscObj)\n-  \/\/ all associated (same box and obj) lock and unlock nodes.\n-  int cnt = C->macro_count();\n-  for (int i=0; i < cnt; i++) {\n-    Node *n = C->macro_node(i);\n-    if (n->is_AbstractLock()) { \/\/ Lock and Unlock nodes\n-      mark_eliminated_locking_nodes(n->as_AbstractLock());\n+  int iteration = 0;\n+  while (C->macro_count() > 0) {\n+    if (iteration++ > 100) {\n+      assert(false, \"Too slow convergence of macro elimination\");\n+      break;\n@@ -2431,23 +3061,11 @@\n-  }\n-  \/\/ Re-marking may break consistency of Coarsened locks.\n-  if (!C->coarsened_locks_consistent()) {\n-    return; \/\/ recompile without Coarsened locks if broken\n-  } else {\n-    \/\/ After coarsened locks are eliminated locking regions\n-    \/\/ become unbalanced. We should not execute any more\n-    \/\/ locks elimination optimizations on them.\n-    C->mark_unbalanced_boxes();\n-  }\n-  \/\/ First, attempt to eliminate locks\n-  bool progress = true;\n-  while (progress) {\n-    progress = false;\n-    for (int i = C->macro_count(); i > 0; i = MIN2(i - 1, C->macro_count())) { \/\/ more than 1 element can be eliminated at once\n-      Node* n = C->macro_node(i - 1);\n-      bool success = false;\n-      DEBUG_ONLY(int old_macro_count = C->macro_count();)\n-      if (n->is_AbstractLock()) {\n-        success = eliminate_locking_node(n->as_AbstractLock());\n-#ifndef PRODUCT\n-        if (success && PrintOptoStatistics) {\n-          AtomicAccess::inc(&PhaseMacroExpand::_monitor_objects_removed_counter);\n+    \/\/ Postpone lock elimination to after EA when most allocations are eliminated\n+    \/\/ because they might block lock elimination if their escape state isn't\n+    \/\/ determined yet and we only got one chance at eliminating the lock.\n+    if (eliminate_locks) {\n+      \/\/ Before elimination may re-mark (change to Nested or NonEscObj)\n+      \/\/ all associated (same box and obj) lock and unlock nodes.\n+      int cnt = C->macro_count();\n+      for (int i=0; i < cnt; i++) {\n+        Node *n = C->macro_node(i);\n+        if (n->is_AbstractLock()) { \/\/ Lock and Unlock nodes\n+          mark_eliminated_locking_nodes(n->as_AbstractLock());\n@@ -2456,5 +3074,8 @@\n-#endif\n-      assert(success == (C->macro_count() < old_macro_count), \"elimination reduces macro count\");\n-      progress = progress || success;\n-      if (success) {\n-        C->print_method(PHASE_AFTER_MACRO_ELIMINATION_STEP, 5, n);\n+      \/\/ Re-marking may break consistency of Coarsened locks.\n+      if (!C->coarsened_locks_consistent()) {\n+        return; \/\/ recompile without Coarsened locks if broken\n+      } else {\n+        \/\/ After coarsened locks are eliminated locking regions\n+        \/\/ become unbalanced. We should not execute any more\n+        \/\/ locks elimination optimizations on them.\n+        C->mark_unbalanced_boxes();\n@@ -2464,5 +3085,2 @@\n-  }\n-  \/\/ Next, attempt to eliminate allocations\n-  progress = true;\n-  while (progress) {\n-    progress = false;\n+\n+    bool progress = false;\n@@ -2483,2 +3101,5 @@\n-      case Node::Class_CallStaticJava:\n-        success = eliminate_boxing_node(n->as_CallStaticJava());\n+      case Node::Class_CallStaticJava: {\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          success = eliminate_boxing_node(n->as_CallStaticJava());\n+        }\n@@ -2486,0 +3107,1 @@\n+      }\n@@ -2488,1 +3110,8 @@\n-        assert(!n->as_AbstractLock()->is_eliminated(), \"sanity\");\n+        if (eliminate_locks) {\n+          success = eliminate_locking_node(n->as_AbstractLock());\n+#ifndef PRODUCT\n+          if (success && PrintOptoStatistics) {\n+            AtomicAccess::inc(&PhaseMacroExpand::_monitor_objects_removed_counter);\n+          }\n+#endif\n+        }\n@@ -2498,0 +3127,2 @@\n+      case Node::Class_FlatArrayCheck:\n+        break;\n@@ -2515,0 +3146,16 @@\n+\n+    \/\/ Ensure the graph after PhaseMacroExpand::eliminate_macro_nodes is canonical (no igvn\n+    \/\/ transformation is pending). If an allocation is used only in safepoints, elimination of\n+    \/\/ other macro nodes can remove all these safepoints, allowing the allocation to be removed.\n+    \/\/ Hence after igvn we retry removing macro nodes if some progress that has been made in this\n+    \/\/ iteration.\n+    _igvn.set_delay_transform(false);\n+    _igvn.optimize();\n+    if (C->failing()) {\n+      return;\n+    }\n+    _igvn.set_delay_transform(true);\n+\n+    if (!progress) {\n+      break;\n+    }\n@@ -2543,4 +3190,7 @@\n-        \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n-        C->remove_macro_node(n);\n-        _igvn._worklist.push(n);\n-        success = true;\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n+          C->remove_macro_node(n);\n+          _igvn._worklist.push(n);\n+          success = true;\n+        }\n@@ -2655,0 +3305,7 @@\n+    case Node::Class_CallStaticJava:\n+      expand_mh_intrinsic_return(n->as_CallStaticJava());\n+      C->remove_macro_node(n);\n+      break;\n+    case Node::Class_FlatArrayCheck:\n+      expand_flatarraycheck_node(n->as_FlatArrayCheck());\n+      break;\n@@ -2667,1 +3324,1 @@\n-        for (unsigned int i = 0; i < mod_macro->tf()->domain()->cnt() - TypeFunc::Parms; i++) {\n+        for (unsigned int i = 0; i < mod_macro->tf()->domain_cc()->cnt() - TypeFunc::Parms; i++) {\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":969,"deletions":312,"binary":false,"changes":1281,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -60,0 +61,5 @@\n+  const TypeAryPtr* array_type = _igvn.type(ary)->isa_aryptr();\n+  if (array_type != nullptr && array_type->is_aryptr()->is_flat()) {\n+    \/\/ Use T_FLAT_ELEMENT to get proper alignment with COH when fetching the array element address.\n+    elembt = T_FLAT_ELEMENT;\n+  }\n@@ -148,1 +154,1 @@\n-inline Node* PhaseMacroExpand::generate_slow_guard(Node** ctrl, Node* test, RegionNode* region) {\n+Node* PhaseMacroExpand::generate_slow_guard(Node** ctrl, Node* test, RegionNode* region) {\n@@ -152,0 +158,4 @@\n+inline Node* PhaseMacroExpand::generate_fair_guard(Node** ctrl, Node* test, RegionNode* region) {\n+  return generate_guard(ctrl, test, region, PROB_FAIR);\n+}\n+\n@@ -285,0 +295,43 @@\n+Node* PhaseMacroExpand::mark_word_test(Node** ctrl, Node* obj, MergeMemNode* mem, uintptr_t mask_val, RegionNode* region) {\n+  \/\/ Load markword and check if obj is locked\n+  Node* mark = make_load(nullptr, mem->memory_at(Compile::AliasIdxRaw), obj, oopDesc::mark_offset_in_bytes(), TypeX_X, TypeX_X->basic_type());\n+  Node* locked_bit = MakeConX(markWord::unlocked_value);\n+  locked_bit = transform_later(new AndXNode(locked_bit, mark));\n+  Node* cmp = transform_later(new CmpXNode(locked_bit, MakeConX(0)));\n+  Node* is_unlocked = transform_later(new BoolNode(cmp, BoolTest::ne));\n+  IfNode* iff = transform_later(new IfNode(*ctrl, is_unlocked, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+  Node* locked_region = transform_later(new RegionNode(3));\n+  Node* mark_phi = transform_later(new PhiNode(locked_region, TypeX_X));\n+\n+  \/\/ Unlocked: Use bits from mark word\n+  locked_region->init_req(1, transform_later(new IfTrueNode(iff)));\n+  mark_phi->init_req(1, mark);\n+\n+  \/\/ Locked: Load prototype header from klass\n+  *ctrl = transform_later(new IfFalseNode(iff));\n+  \/\/ Make loads control dependent to make sure they are only executed if array is locked\n+  Node* klass_adr = basic_plus_adr(obj, oopDesc::klass_offset_in_bytes());\n+  Node* klass = transform_later(LoadKlassNode::make(_igvn, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+  Node* proto_adr = basic_plus_adr(klass, in_bytes(Klass::prototype_header_offset()));\n+  Node* proto = transform_later(LoadNode::make(_igvn, *ctrl, C->immutable_memory(), proto_adr, proto_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+\n+  locked_region->init_req(2, *ctrl);\n+  mark_phi->init_req(2, proto);\n+  *ctrl = locked_region;\n+\n+  \/\/ Now check if mark word bits are set\n+  Node* mask = MakeConX(mask_val);\n+  Node* masked = transform_later(new AndXNode(mark_phi, mask));\n+  cmp = transform_later(new CmpXNode(masked, mask));\n+  Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+  return generate_fair_guard(ctrl, bol, region);\n+}\n+\n+Node* PhaseMacroExpand::generate_flat_array_guard(Node** ctrl, Node* array, MergeMemNode* mem, RegionNode* region) {\n+  return mark_word_test(ctrl, array, mem, markWord::flat_array_bit_in_place, region);\n+}\n+\n+Node* PhaseMacroExpand::generate_null_free_array_guard(Node** ctrl, Node* array, MergeMemNode* mem, RegionNode* region) {\n+  return mark_word_test(ctrl, array, mem, markWord::null_free_array_bit_in_place, region);\n+}\n+\n@@ -379,0 +432,1 @@\n+                                           Node* dest_length,\n@@ -390,0 +444,2 @@\n+  Node* init_value = nullptr;\n+  Node* raw_init_value = nullptr;\n@@ -420,0 +476,2 @@\n+      init_value = alloc->in(AllocateNode::InitValue);\n+      raw_init_value = alloc->in(AllocateNode::RawInitValue);\n@@ -489,1 +547,0 @@\n-      Node* dest_length = alloc->in(AllocateNode::ALength);\n@@ -496,1 +553,3 @@\n-                             adr_type, dest, basic_elem_type,\n+                             adr_type, dest,\n+                             init_value, raw_init_value,\n+                             basic_elem_type,\n@@ -527,1 +586,0 @@\n-    Node* dest_length = alloc->in(AllocateNode::ALength);\n@@ -533,1 +591,3 @@\n-                           adr_type, dest, basic_elem_type,\n+                           adr_type, dest,\n+                           init_value, raw_init_value,\n+                           basic_elem_type,\n@@ -582,1 +642,3 @@\n-                             adr_type, dest, basic_elem_type,\n+                             adr_type, dest,\n+                             init_value, raw_init_value,\n+                             basic_elem_type,\n@@ -592,1 +654,3 @@\n-                             adr_type, dest, basic_elem_type,\n+                             adr_type, dest,\n+                             init_value, raw_init_value,\n+                             basic_elem_type,\n@@ -769,1 +833,3 @@\n-                           adr_type, dest, basic_elem_type,\n+                           adr_type, dest,\n+                           init_value, raw_init_value,\n+                           basic_elem_type,\n@@ -838,3 +904,3 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, out_mem);\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_ioproj, *io);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, out_mem);\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_ioproj, *io);\n@@ -842,1 +908,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_catchproj, *ctrl);\n+  _igvn.replace_node(_callprojs->fallthrough_catchproj, *ctrl);\n@@ -882,0 +948,2 @@\n+                                            Node* val,\n+                                            Node* raw_val,\n@@ -920,1 +988,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, val, raw_val,\n@@ -925,1 +993,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, val, raw_val,\n@@ -938,1 +1006,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, val, raw_val,\n@@ -967,1 +1035,7 @@\n-        mem = StoreNode::make(_igvn, ctrl, mem, p1, adr_type, intcon(0), T_INT, MemNode::unordered);\n+        if (val == nullptr) {\n+          assert(raw_val == nullptr, \"val may not be null\");\n+          mem = StoreNode::make(_igvn, ctrl, mem, p1, adr_type, intcon(0), T_INT, MemNode::unordered);\n+        } else {\n+          assert(_igvn.type(val)->isa_narrowoop(), \"should be narrow oop\");\n+          mem = new StoreNNode(ctrl, mem, p1, adr_type, val, MemNode::unordered);\n+        }\n@@ -972,1 +1046,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, raw_val,\n@@ -1088,2 +1162,2 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  *ctrl = _callprojs.fallthrough_catchproj->clone();\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  *ctrl = _callprojs->fallthrough_catchproj->clone();\n@@ -1092,1 +1166,1 @@\n-  Node* m = _callprojs.fallthrough_memproj->clone();\n+  Node* m = _callprojs->fallthrough_memproj->clone();\n@@ -1106,3 +1180,3 @@\n-  \/\/ could be null. Skip clone and update null fallthrough_ioproj.\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    *io = _callprojs.fallthrough_ioproj->clone();\n+  \/\/ could be nullptr. Skip clone and update nullptr fallthrough_ioproj.\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    *io = _callprojs->fallthrough_ioproj->clone();\n@@ -1240,0 +1314,36 @@\n+const TypePtr* PhaseMacroExpand::adjust_for_flat_array(const TypeAryPtr* top_dest, Node*& src_offset,\n+                                                       Node*& dest_offset, Node*& length, BasicType& dest_elem,\n+                                                       Node*& dest_length) {\n+#ifdef ASSERT\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  bool needs_barriers = top_dest->elem()->inline_klass()->contains_oops() &&\n+    bs->array_copy_requires_gc_barriers(dest_length != nullptr, T_OBJECT, false, false, BarrierSetC2::Optimization);\n+  assert(!needs_barriers || StressReflectiveCode, \"Flat arracopy would require GC barriers\");\n+#endif\n+  int elem_size = top_dest->flat_elem_size();\n+  if (elem_size >= 8) {\n+    if (elem_size > 8) {\n+      \/\/ treat as array of long but scale length, src offset and dest offset\n+      assert((elem_size % 8) == 0, \"not a power of 2?\");\n+      int factor = elem_size \/ 8;\n+      length = transform_later(new MulINode(length, intcon(factor)));\n+      src_offset = transform_later(new MulINode(src_offset, intcon(factor)));\n+      dest_offset = transform_later(new MulINode(dest_offset, intcon(factor)));\n+      if (dest_length != nullptr) {\n+        dest_length = transform_later(new MulINode(dest_length, intcon(factor)));\n+      }\n+      elem_size = 8;\n+    }\n+    dest_elem = T_LONG;\n+  } else if (elem_size == 4) {\n+    dest_elem = T_INT;\n+  } else if (elem_size == 2) {\n+    dest_elem = T_CHAR;\n+  } else if (elem_size == 1) {\n+    dest_elem = T_BYTE;\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+  return TypeRawPtr::BOTTOM;\n+}\n+\n@@ -1257,3 +1367,16 @@\n-    Node* mem = ac->in(TypeFunc::Memory);\n-    merge_mem = MergeMemNode::make(mem);\n-    transform_later(merge_mem);\n+    const Type* src_type = _igvn.type(src);\n+    const Type* dest_type = _igvn.type(dest);\n+    const TypeAryPtr* top_src = src_type->isa_aryptr();\n+    \/\/ Note: The destination could have type Object (i.e. non-array) when directly invoking the protected method\n+    \/\/       Object::clone() with reflection on a declared Object that is an array at runtime. top_dest is then null.\n+    const TypeAryPtr* top_dest = dest_type->isa_aryptr();\n+    BasicType dest_elem = T_OBJECT;\n+    if (top_dest != nullptr && top_dest->elem() != Type::BOTTOM) {\n+      dest_elem = top_dest->elem()->array_element_basic_type();\n+    }\n+    if (is_reference_type(dest_elem, true)) dest_elem = T_OBJECT;\n+\n+    if (top_src != nullptr && top_src->is_flat()) {\n+      \/\/ If src is flat, dest is guaranteed to be flat as well\n+      top_dest = top_src;\n+    }\n@@ -1262,0 +1385,1 @@\n+    Node* dest_length = nullptr;\n@@ -1265,0 +1389,1 @@\n+      dest_length = alloc->in(AllocateNode::ALength);\n@@ -1267,3 +1392,20 @@\n-    const TypePtr* adr_type = _igvn.type(dest)->is_oopptr()->add_offset(Type::OffsetBot);\n-    if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n-      adr_type = ac->_dest_type->add_offset(Type::OffsetBot)->is_ptr();\n+    Node* mem = ac->in(TypeFunc::Memory);\n+    const TypePtr* adr_type = nullptr;\n+    if (top_dest != nullptr && top_dest->is_flat()) {\n+      assert(dest_length != nullptr || StressReflectiveCode, \"must be tightly coupled\");\n+      \/\/ Copy to a flat array modifies multiple memory slices. Conservatively insert a barrier\n+      \/\/ on all slices to prevent writes into the source from floating below the arraycopy.\n+      int mem_bar_alias_idx = Compile::AliasIdxBot;\n+      if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n+        mem_bar_alias_idx = C->get_alias_index(ac->_dest_type->add_offset(Type::OffsetBot)->is_ptr());\n+      }\n+      insert_mem_bar(&ctrl, &mem, Op_MemBarCPUOrder, mem_bar_alias_idx);\n+      adr_type = adjust_for_flat_array(top_dest, src_offset, dest_offset, length, dest_elem, dest_length);\n+    } else {\n+      adr_type = dest_type->is_oopptr()->add_offset(Type::OffsetBot);\n+      if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n+        adr_type = ac->_dest_type->add_offset(Type::OffsetBot)->is_ptr();\n+      }\n+      if (ac->_src_type != ac->_dest_type) {\n+        adr_type = TypeRawPtr::BOTTOM;\n+      }\n@@ -1271,0 +1413,3 @@\n+    merge_mem = MergeMemNode::make(mem);\n+    transform_later(merge_mem);\n+\n@@ -1272,1 +1417,1 @@\n-                       adr_type, T_OBJECT,\n+                       adr_type, dest_elem,\n@@ -1274,0 +1419,1 @@\n+                       dest_length,\n@@ -1309,3 +1455,1 @@\n-  if (ac->is_arraycopy_validated() &&\n-      dest_elem != T_CONFLICT &&\n-      src_elem == T_CONFLICT) {\n+  if (ac->is_arraycopy_validated() && dest_elem != T_CONFLICT && src_elem == T_CONFLICT) {\n@@ -1327,6 +1471,7 @@\n-    Node* mem = generate_arraycopy(ac, nullptr, &ctrl, merge_mem, &io,\n-                                   TypeRawPtr::BOTTOM, T_CONFLICT,\n-                                   src, src_offset, dest, dest_offset, length,\n-                                   \/\/ If a  negative length guard was generated for the ArrayCopyNode,\n-                                   \/\/ the length of the array can never be negative.\n-                                   false, ac->has_negative_length_guard());\n+    generate_arraycopy(ac, nullptr, &ctrl, merge_mem, &io,\n+                       TypeRawPtr::BOTTOM, T_CONFLICT,\n+                       src, src_offset, dest, dest_offset, length,\n+                       nullptr,\n+                       \/\/ If a  negative length guard was generated for the ArrayCopyNode,\n+                       \/\/ the length of the array can never be negative.\n+                       false, ac->has_negative_length_guard());\n@@ -1340,1 +1485,8 @@\n-  if (src_elem != dest_elem || dest_elem == T_VOID) {\n+  \/\/\n+  \/\/ We have no stub to copy flat inline type arrays with oop\n+  \/\/ fields if we need to emit write barriers.\n+  \/\/\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  if (src_elem != dest_elem || top_src->is_flat() != top_dest->is_flat() || dest_elem == T_VOID ||\n+      (top_src->is_flat() && top_dest->elem()->inline_klass()->contains_oops() &&\n+       bs->array_copy_requires_gc_barriers(alloc != nullptr, T_OBJECT, false, false, BarrierSetC2::Optimization))) {\n@@ -1348,3 +1500,3 @@\n-    _igvn.replace_node(_callprojs.fallthrough_memproj, merge_mem);\n-    if (_callprojs.fallthrough_ioproj != nullptr) {\n-      _igvn.replace_node(_callprojs.fallthrough_ioproj, io);\n+    _igvn.replace_node(_callprojs->fallthrough_memproj, merge_mem);\n+    if (_callprojs->fallthrough_ioproj != nullptr) {\n+      _igvn.replace_node(_callprojs->fallthrough_ioproj, io);\n@@ -1352,1 +1504,1 @@\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, ctrl);\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, ctrl);\n@@ -1369,4 +1521,9 @@\n-  {\n-    Node* mem = ac->in(TypeFunc::Memory);\n-    merge_mem = MergeMemNode::make(mem);\n-    transform_later(merge_mem);\n+  Node* mem = ac->in(TypeFunc::Memory);\n+  if (top_dest->is_flat()) {\n+    \/\/ Copy to a flat array modifies multiple memory slices. Conservatively insert a barrier\n+    \/\/ on all slices to prevent writes into the source from floating below the arraycopy.\n+    int mem_bar_alias_idx = Compile::AliasIdxBot;\n+    if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n+      mem_bar_alias_idx = C->get_alias_index(ac->_dest_type->add_offset(Type::OffsetBot)->is_ptr());\n+    }\n+    insert_mem_bar(&ctrl, &mem, Op_MemBarCPUOrder, mem_bar_alias_idx);\n@@ -1374,0 +1531,2 @@\n+  merge_mem = MergeMemNode::make(mem);\n+  transform_later(merge_mem);\n@@ -1414,0 +1573,8 @@\n+\n+    \/\/ TODO 8350865 This is too strong\n+    \/\/ We need to be careful here because 'adjust_for_flat_array' will adjust offsets\/length etc. which then does not work anymore for the slow call to SharedRuntime::slow_arraycopy_C.\n+    if (!(top_src->is_flat() && top_dest->is_flat() && top_src->is_null_free() == top_dest->is_null_free())) {\n+      generate_flat_array_guard(&ctrl, src, merge_mem, slow_region);\n+      generate_flat_array_guard(&ctrl, dest, merge_mem, slow_region);\n+      generate_null_free_array_guard(&ctrl, dest, merge_mem, slow_region);\n+    }\n@@ -1415,0 +1582,1 @@\n+\n@@ -1417,1 +1585,6 @@\n-  if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n+  Node* dest_length = (alloc != nullptr) ? alloc->in(AllocateNode::ALength) : nullptr;\n+\n+  if (top_src->is_flat() && top_dest->is_flat() &&\n+      top_src->is_null_free() == top_dest->is_null_free()) {\n+    adr_type = adjust_for_flat_array(top_dest, src_offset, dest_offset, length, dest_elem, dest_length);\n+  } else if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n@@ -1426,0 +1599,1 @@\n+                     dest_length,\n@@ -1428,1 +1602,2 @@\n-                     false, ac->has_negative_length_guard(), slow_region);\n+                     false, ac->has_negative_length_guard(),\n+                     slow_region);\n","filename":"src\/hotspot\/share\/opto\/macroArrayCopy.cpp","additions":225,"deletions":50,"binary":false,"changes":275,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -27,0 +28,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -33,0 +35,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -40,0 +43,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -55,0 +59,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -143,8 +148,105 @@\n-Node *MemNode::optimize_simple_memory_chain(Node *mchain, const TypeOopPtr *t_oop, Node *load, PhaseGVN *phase) {\n-  assert((t_oop != nullptr), \"sanity\");\n-  bool is_instance = t_oop->is_known_instance_field();\n-  bool is_boxed_value_load = t_oop->is_ptr_to_boxed_value() &&\n-                             (load != nullptr) && load->is_Load() &&\n-                             (phase->is_IterGVN() != nullptr);\n-  if (!(is_instance || is_boxed_value_load))\n-    return mchain;  \/\/ don't try to optimize non-instance types\n+\/\/ Find the memory output corresponding to the fall-through path of a call\n+static Node* find_call_fallthrough_mem_output(CallNode* call) {\n+  ResourceMark rm;\n+  CallProjections* projs = call->extract_projections(false, false);\n+  Node* res = projs->fallthrough_memproj;\n+  assert(res != nullptr, \"must have a fallthrough mem output\");\n+  return res;\n+}\n+\n+\/\/ Try to find a better memory input for a load from a strict final field\n+static Node* try_optimize_strict_final_load_memory(PhaseGVN* phase, Node* adr, ProjNode*& base_local) {\n+  intptr_t offset = 0;\n+  Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);\n+  if (base == nullptr) {\n+    return nullptr;\n+  }\n+\n+  Node* base_uncasted = base->uncast();\n+  if (base_uncasted->is_Proj()) {\n+    MultiNode* multi = base_uncasted->in(0)->as_Multi();\n+    if (multi->is_Allocate()) {\n+      base_local = base_uncasted->as_Proj();\n+      return nullptr;\n+    } else if (multi->is_Call()) {\n+      \/\/ The oop is returned from a call, the memory can be the fallthrough output of the call\n+      return find_call_fallthrough_mem_output(multi->as_Call());\n+    } else if (multi->is_Start()) {\n+      \/\/ The oop is a parameter\n+      if (phase->C->method()->is_object_constructor() && base_uncasted->as_Proj()->_con == TypeFunc::Parms) {\n+        \/\/ The receiver of a constructor is similar to the result of an AllocateNode\n+        base_local = base_uncasted->as_Proj();\n+        return nullptr;\n+      } else {\n+        \/\/ Use the start memory otherwise\n+        return multi->proj_out(TypeFunc::Memory);\n+      }\n+    }\n+  }\n+\n+  return nullptr;\n+}\n+\n+\/\/ Whether a call can modify a strict final field, given that the object is allocated inside the\n+\/\/ current compilation unit, or is the first parameter when the compilation root is a constructor.\n+\/\/ This is equivalent to asking whether 'call' is a constructor invocation and the class declaring\n+\/\/ the target method is a subclass of the class declaring 'field'.\n+static bool call_can_modify_local_object(ciField* field, CallNode* call) {\n+  if (!call->is_CallJava()) {\n+    return false;\n+  }\n+\n+  ciMethod* target = call->as_CallJava()->method();\n+  if (target == nullptr || !target->is_object_constructor()) {\n+    return false;\n+  }\n+\n+  \/\/ If 'field' is declared in a class that is a subclass of the one declaring the constructor,\n+  \/\/ then the field is set inside the constructor, else the field must be set before the\n+  \/\/ constructor invocation. E.g. A field Super.x will be set during the execution of Sub::<init>,\n+  \/\/ while a field Sub.y must be set before Super::<init> is invoked.\n+  \/\/ We can try to be more heroic and decide if the receiver of the constructor invocation is the\n+  \/\/ object from which we are loading from. This, however, may be problematic as deciding if 2\n+  \/\/ nodes are definitely different may not be trivial, especially if the graph is not canonical.\n+  \/\/ As a result, it is made more conservative for now.\n+  assert(call->req() > TypeFunc::Parms, \"constructor must have at least 1 argument\");\n+  return target->holder()->is_subclass_of(field->holder());\n+}\n+\n+Node* MemNode::optimize_simple_memory_chain(Node* mchain, const TypeOopPtr* t_oop, Node* load, PhaseGVN* phase) {\n+  assert(t_oop != nullptr, \"sanity\");\n+  bool is_known_instance = t_oop->is_known_instance_field();\n+  bool is_strict_final_load = false;\n+\n+  \/\/ After macro expansion, an allocation may become a call, changing the memory input to the\n+  \/\/ memory output of that call would be illegal. As a result, disallow this transformation after\n+  \/\/ macro expansion.\n+  if (phase->is_IterGVN() && phase->C->allow_macro_nodes() && load != nullptr && load->is_Load() && !load->as_Load()->is_mismatched_access()) {\n+    is_strict_final_load = t_oop->is_ptr_to_strict_final_field();\n+#ifdef ASSERT\n+    if ((t_oop->is_inlinetypeptr() && t_oop->inline_klass()->contains_field_offset(t_oop->offset())) || t_oop->is_ptr_to_boxed_value()) {\n+      assert(is_strict_final_load, \"sanity check for basic cases\");\n+    }\n+#endif \/\/ ASSERT\n+  }\n+\n+  if (!is_known_instance && !is_strict_final_load) {\n+    return mchain;\n+  }\n+\n+  Node* result = mchain;\n+  ProjNode* base_local = nullptr;\n+\n+  ciField* field = nullptr;\n+  if (is_strict_final_load) {\n+    field = phase->C->alias_type(t_oop)->field();\n+    assert(field != nullptr, \"must point to a field\");\n+\n+    Node* adr = load->in(MemNode::Address);\n+    assert(phase->type(adr) == t_oop, \"inconsistent type\");\n+    Node* tmp = try_optimize_strict_final_load_memory(phase, adr, base_local);\n+    if (tmp != nullptr) {\n+      result = tmp;\n+    }\n+  }\n+\n@@ -152,3 +254,2 @@\n-  Node *start_mem = phase->C->start()->proj_out_or_null(TypeFunc::Memory);\n-  Node *prev = nullptr;\n-  Node *result = mchain;\n+  Node* start_mem = phase->C->start()->proj_out_or_null(TypeFunc::Memory);\n+  Node* prev = nullptr;\n@@ -157,2 +258,5 @@\n-    if (result == start_mem)\n-      break;  \/\/ hit one of our sentinels\n+    if (result == start_mem) {\n+      \/\/ start_mem is the earliest memory possible\n+      break;\n+    }\n+\n@@ -161,1 +265,1 @@\n-      Node *proj_in = result->in(0);\n+      Node* proj_in = result->in(0);\n@@ -163,1 +267,2 @@\n-        break;  \/\/ hit one of our sentinels\n+        \/\/ This is the allocation that creates the object from which we are loading from\n+        break;\n@@ -166,2 +271,4 @@\n-        CallNode *call = proj_in->as_Call();\n-        if (!call->may_modify(t_oop, phase)) { \/\/ returns false for instances\n+        CallNode* call = proj_in->as_Call();\n+        if (!call->may_modify(t_oop, phase)) {\n+          result = call->in(TypeFunc::Memory);\n+        } else if (is_strict_final_load && base_local != nullptr && !call_can_modify_local_object(field, call)) {\n@@ -177,1 +284,1 @@\n-        if (is_instance) {\n+        if (is_known_instance) {\n@@ -179,1 +286,1 @@\n-        } else if (is_boxed_value_load) {\n+        } else if (is_strict_final_load) {\n@@ -183,1 +290,5 @@\n-            result = proj_in->in(TypeFunc::Memory); \/\/ not related allocation\n+            \/\/ Allocation of another type, must be another object\n+            result = proj_in->in(TypeFunc::Memory);\n+          } else if (base_local != nullptr && (base_local->is_Parm() || base_local->in(0) != alloc)) {\n+            \/\/ Allocation of another object\n+            result = proj_in->in(TypeFunc::Memory);\n@@ -192,0 +303,5 @@\n+      } else if (proj_in->is_LoadFlat() || proj_in->is_StoreFlat()) {\n+        if (is_strict_final_load) {\n+          \/\/ LoadFlat and StoreFlat cannot happen to strict final fields\n+          result = proj_in->in(TypeFunc::Memory);\n+        }\n@@ -195,1 +311,1 @@\n-        assert(false, \"unexpected projection\");\n+        assert(false, \"unexpected projection of %s\", proj_in->Name());\n@@ -198,1 +314,1 @@\n-      if (!is_instance || !ClearArrayNode::step_through(&result, instance_id, phase)) {\n+      if (!is_known_instance || !ClearArrayNode::step_through(&result, instance_id, phase)) {\n@@ -236,0 +352,2 @@\n+                     ->cast_to_not_flat(t_oop->is_aryptr()->is_not_flat())\n+                     ->cast_to_not_null_free(t_oop->is_aryptr()->is_not_null_free())\n@@ -262,1 +380,1 @@\n-               tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n+        tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n@@ -974,0 +1092,1 @@\n+  case T_ARRAY:\n@@ -987,1 +1106,1 @@\n-    ShouldNotReachHere();\n+    assert(false, \"unexpected basic type %s\", type2name(bt));\n@@ -1022,1 +1141,1 @@\n-    return (eliminate_boxing && non_volatile) || is_stable_ary;\n+    return (eliminate_boxing && non_volatile) || is_stable_ary || tp->is_inlinetypeptr();\n@@ -1078,2 +1197,1 @@\n-      uint header = arrayOopDesc::base_offset_in_bytes(ary_elem);\n-      uint shift  = exact_log2(type2aelembytes(ary_elem));\n+      uint shift  = ary_t->is_flat() ? ary_t->flat_log_elem_size() : exact_log2(type2aelembytes(ary_elem));\n@@ -1103,0 +1221,10 @@\n+static Node* see_through_inline_type(PhaseValues* phase, const MemNode* load, Node* base, int offset) {\n+  if (!load->is_mismatched_access() && base != nullptr && base->is_InlineType() && offset > oopDesc::klass_offset_in_bytes()) {\n+    InlineTypeNode* vt = base->as_InlineType();\n+    Node* value = vt->field_value_by_offset(offset, true);\n+    assert(value != nullptr, \"must see some value\");\n+    return value;\n+  }\n+\n+  return nullptr;\n+}\n@@ -1115,0 +1243,9 @@\n+  \/\/ Try to see through an InlineTypeNode\n+  \/\/ LoadN is special because the input is not compressed\n+  if (Opcode() != Op_LoadN) {\n+    Node* value = see_through_inline_type(phase, this, ld_base, ld_off);\n+    if (value != nullptr) {\n+      return value;\n+    }\n+  }\n+\n@@ -1198,1 +1335,1 @@\n-        const TypeVect* out_vt = as_LoadVector()->vect_type();\n+        const TypeVect* out_vt = is_Load() ? as_LoadVector()->vect_type() : as_StoreVector()->vect_type();\n@@ -1216,0 +1353,7 @@\n+      Node* init_value = ld_alloc->in(AllocateNode::InitValue);\n+      if (init_value != nullptr) {\n+        \/\/ TODO 8350865 Scalar replacement does not work well for flat arrays.\n+        \/\/ Is this correct for non-all-zero init values? Don't we need field_value_by_offset?\n+        return init_value;\n+      }\n+      assert(ld_alloc->in(AllocateNode::RawInitValue) == nullptr, \"init value may not be null\");\n@@ -1268,1 +1412,1 @@\n-    \/\/ Only instances and boxed values.\n+    \/\/ Only known instances and immutable fields\n@@ -1270,1 +1414,1 @@\n-        (t_oop->is_ptr_to_boxed_value() ||\n+        (t_oop->is_ptr_to_strict_final_field() ||\n@@ -1319,2 +1463,2 @@\n-         addr_t->is_ptr_to_boxed_value()) {\n-      \/\/ Use _idx of address base (could be Phi node) for boxed values.\n+         addr_t->is_ptr_to_strict_final_field()) {\n+      \/\/ Use _idx of address base (could be Phi node) for immutable fields in unknown instances\n@@ -1876,0 +2020,1 @@\n+        && !(phase->type(address)->is_inlinetypeptr() && is_mismatched_access())\n@@ -1972,1 +2117,8 @@\n-  return progress ? this : nullptr;\n+  if (progress) {\n+    return this;\n+  }\n+\n+  if (!can_reshape) {\n+    phase->record_for_igvn(this);\n+  }\n+  return nullptr;\n@@ -2068,0 +2220,1 @@\n+        && !ary->is_flat()\n@@ -2103,0 +2256,2 @@\n+            \/\/ Default value load\n+            tp->is_instptr()->instance_klass() == ciEnv::current()->Class_klass() ||\n@@ -2108,1 +2263,3 @@\n-    \/\/ Optimize loads from constant fields.\n+    BasicType bt = value_basic_type();\n+\n+    \/\/ Optimize loads from constant fields.\n@@ -2112,1 +2269,1 @@\n-      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), value_basic_type());\n+      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), bt);\n@@ -2159,6 +2316,16 @@\n-      if (UseCompactObjectHeaders) {\n-        if (tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n-          \/\/ The field is Klass::_prototype_header. Return its (constant) value.\n-          assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n-          return TypeX::make(klass->prototype_header());\n-        }\n+      if (tkls->offset() == in_bytes(ObjArrayKlass::next_refined_array_klass_offset()) && klass->is_obj_array_klass()) {\n+        \/\/ Fold loads from LibraryCallKit::load_default_refined_array_klass\n+        return tkls->is_aryklassptr()->cast_to_refined_array_klass_ptr();\n+      }\n+      if (klass->is_array_klass() && tkls->offset() == in_bytes(ObjArrayKlass::properties_offset())) {\n+        assert(klass->is_type_array_klass() || tkls->is_aryklassptr()->is_refined_type(), \"Must be a refined array klass pointer\");\n+        return TypeInt::make(klass->as_array_klass()->properties());\n+      }\n+      if (klass->is_flat_array_klass() && tkls->offset() == in_bytes(FlatArrayKlass::layout_kind_offset())) {\n+        assert(Opcode() == Op_LoadI, \"must load an int from _layout_kind\");\n+        return TypeInt::make(static_cast<jint>(klass->as_flat_array_klass()->layout_kind()));\n+      }\n+      if (UseCompactObjectHeaders && tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n+        \/\/ The field is Klass::_prototype_header. Return its (constant) value.\n+        assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n+        return TypeX::make(klass->prototype_header());\n@@ -2238,0 +2405,12 @@\n+      \/\/ TODO 8350865 Scalar replacement does not work well for flat arrays.\n+      \/\/ Escape Analysis assumes that arrays are always zeroed during allocation which is not true for null-free arrays\n+      \/\/ ConnectionGraph::split_unique_types will re-wire the memory of loads from such arrays around the allocation\n+      \/\/ TestArrays::test6 and test152 and TestBasicFunctionality::test20 are affected by this.\n+      if (tp->isa_aryptr() && tp->is_aryptr()->is_flat() && tp->is_aryptr()->is_null_free()) {\n+        intptr_t offset = 0;\n+        Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);\n+        AllocateNode* alloc = AllocateNode::Ideal_allocation(base);\n+        if (alloc != nullptr && alloc->is_AllocateArray() && alloc->in(AllocateNode::InitValue) != nullptr) {\n+          return _type;\n+        }\n+      }\n@@ -2241,1 +2420,0 @@\n-\n@@ -2245,1 +2423,10 @@\n-      return TypeX::make(markWord::prototype().value());\n+      if (EnableValhalla) {\n+        \/\/ The mark word may contain property bits (inline, flat, null-free)\n+        Node* klass_node = alloc->in(AllocateNode::KlassNode);\n+        const TypeKlassPtr* tkls = phase->type(klass_node)->isa_klassptr();\n+        if (tkls != nullptr && tkls->is_loaded() && tkls->klass_is_exact()) {\n+          return TypeX::make(tkls->exact_klass()->prototype_header());\n+        }\n+      } else {\n+        return TypeX::make(markWord::prototype().value());\n+      }\n@@ -2394,0 +2581,13 @@\n+Node* LoadNNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  \/\/ Loading from an InlineType, find the input and make an EncodeP\n+  Node* addr = in(Address);\n+  intptr_t offset;\n+  Node* base = AddPNode::Ideal_base_and_offset(addr, phase, offset);\n+  Node* value = see_through_inline_type(phase, this, base, offset);\n+  if (value != nullptr) {\n+    return new EncodePNode(value, type());\n+  }\n+\n+  return LoadNode::Ideal(phase, can_reshape);\n+}\n+\n@@ -2467,1 +2667,1 @@\n-  const TypeAryPtr *tary = tp->isa_aryptr();\n+  const TypeAryPtr* tary = tp->isa_aryptr();\n@@ -2470,1 +2670,3 @@\n-    return tary->as_klass_type(true);\n+    const TypeAryKlassPtr* res = tary->as_klass_type(true)->is_aryklassptr();\n+    \/\/ The klass of an array object must be a refined array klass\n+    return res->cast_to_refined_array_klass_ptr();\n@@ -2488,0 +2690,7 @@\n+    if (tkls->isa_aryklassptr() != nullptr && tkls->klass_is_exact() &&\n+        !tkls->exact_klass()->is_type_array_klass() &&\n+        tkls->offset() == in_bytes(Klass::super_offset())) {\n+      \/\/ We are loading the super klass of a refined array klass, return the non-refined klass pointer\n+      assert(tkls->is_aryklassptr()->is_refined_type(), \"Must be a refined array klass pointer\");\n+      return tkls->is_aryklassptr()->cast_to_refined_array_klass_ptr(false);\n+    }\n@@ -2550,0 +2759,4 @@\n+  \/\/\n+  \/\/ This optimization does not apply to arrays because if k is not a\n+  \/\/ constant, it was obtained via load_klass which returns the VM type\n+  \/\/ and '.java_mirror.as_klass' should return the Java type instead.\n@@ -2559,3 +2772,2 @@\n-            && (tkls->isa_instklassptr() || tkls->isa_aryklassptr())\n-            && adr2->is_AddP()\n-           ) {\n+            && ((tkls->isa_instklassptr() && !tkls->is_instklassptr()->might_be_an_array()))\n+            && adr2->is_AddP()) {\n@@ -2706,0 +2918,1 @@\n+  case T_ARRAY:\n@@ -2721,1 +2934,1 @@\n-    ShouldNotReachHere();\n+    assert(false, \"unexpected basic type %s\", type2name(bt));\n@@ -3383,2 +3596,2 @@\n-  \/\/ unsafe if I have intervening uses.\n-  {\n+  \/\/ unsafe if I have intervening uses...\n+  if (phase->C->get_adr_type(phase->C->get_alias_index(adr_type())) != TypeAryPtr::INLINES) {\n@@ -3404,0 +3617,2 @@\n+             (Opcode() == Op_StoreL && st->Opcode() == Op_StoreN) ||\n+             (st->adr_type()->isa_aryptr() && st->adr_type()->is_aryptr()->is_flat()) || \/\/ TODO 8343835\n@@ -3541,2 +3756,1 @@\n-  if (result == this &&\n-      ReduceFieldZeroing && phase->type(val)->is_zero_type()) {\n+  if (result == this && ReduceFieldZeroing) {\n@@ -3544,1 +3758,2 @@\n-    if (mem->is_Proj() && mem->in(0)->is_Allocate()) {\n+    if (mem->is_Proj() && mem->in(0)->is_Allocate() &&\n+        (phase->type(val)->is_zero_type() || mem->in(0)->in(AllocateNode::InitValue) == val)) {\n@@ -3548,1 +3763,1 @@\n-    if (result == this) {\n+    if (result == this && phase->type(val)->is_zero_type()) {\n@@ -4052,1 +4267,1 @@\n-    return new ClearArrayNode(in(0), in(1), in(2), in(3), true);\n+    return new ClearArrayNode(in(0), in(1), in(2), in(3), in(4), true);\n@@ -4070,1 +4285,1 @@\n-  Node *zero = phase->makecon(TypeLong::ZERO);\n+  Node *val = in(4);\n@@ -4072,1 +4287,1 @@\n-  mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+  mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -4077,1 +4292,1 @@\n-    mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+    mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -4111,0 +4326,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -4121,1 +4338,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != nullptr) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == nullptr, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -4128,1 +4351,1 @@\n-  return clear_memory(ctl, mem, dest, phase->MakeConX(offset), end_offset, phase);\n+  return clear_memory(ctl, mem, dest, raw_val, phase->MakeConX(offset), end_offset, phase);\n@@ -4132,0 +4355,1 @@\n+                                   Node* raw_val,\n@@ -4154,1 +4378,4 @@\n-  mem = new ClearArrayNode(ctl, mem, zsize, adr, false);\n+  if (raw_val == nullptr) {\n+    raw_val = phase->MakeConX(0);\n+  }\n+  mem = new ClearArrayNode(ctl, mem, zsize, adr, raw_val, false);\n@@ -4159,0 +4386,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -4173,1 +4402,1 @@\n-    mem = clear_memory(ctl, mem, dest,\n+    mem = clear_memory(ctl, mem, dest, val, raw_val,\n@@ -4180,1 +4409,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != nullptr) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == nullptr, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -4327,1 +4562,1 @@\n-Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {\n+Node *MemBarNode::match(const ProjNode *proj, const Matcher *m, const RegMask* mask) {\n@@ -4614,1 +4849,3 @@\n-  if (init == nullptr || init->is_complete())  return false;\n+  if (init == nullptr || init->is_complete()) {\n+    return false;\n+  }\n@@ -4798,0 +5035,6 @@\n+                if (base->is_Phi()) {\n+                  \/\/ In rare case, base may be a PhiNode and it may read\n+                  \/\/ the same memory slice between InitializeNode and store.\n+                  failed = true;\n+                  break;\n+                }\n@@ -5384,0 +5627,2 @@\n+                                              allocation()->in(AllocateNode::InitValue),\n+                                              allocation()->in(AllocateNode::RawInitValue),\n@@ -5443,0 +5688,2 @@\n+                                            allocation()->in(AllocateNode::InitValue),\n+                                            allocation()->in(AllocateNode::RawInitValue),\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":313,"deletions":66,"binary":false,"changes":379,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -574,0 +575,6 @@\n+  if (n->is_InlineType()) {\n+    C->add_inline_type(n);\n+  }\n+  if (n->is_LoadFlat() || n->is_StoreFlat()) {\n+    C->add_flat_access(n);\n+  }\n@@ -634,0 +641,3 @@\n+  if (is_InlineType()) {\n+    compile->remove_inline_type(this);\n+  }\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1184,1 +1184,1 @@\n-  n->dump_bfs(1, nullptr, \"\", &ss);\n+  n->dump_bfs(3, nullptr, \"\", &ss);\n@@ -2077,6 +2077,0 @@\n-  if (_delay_transform) {\n-    \/\/ Register the node but don't optimize for now\n-    register_new_node_with_optimizer(n);\n-    return n;\n-  }\n-\n@@ -2089,0 +2083,6 @@\n+  if (_delay_transform) {\n+    \/\/ Add the node to the worklist but don't optimize for now\n+    _worklist.push(n);\n+    return n;\n+  }\n+\n@@ -2361,0 +2361,13 @@\n+void PhaseIterGVN::replace_in_uses(Node* n, Node* m) {\n+  assert(n != nullptr, \"sanity\");\n+  for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+    Node* u = n->fast_out(i);\n+    if (u != n) {\n+      rehash_node_delayed(u);\n+      int nb = u->replace_edge(n, m);\n+      --i, imax -= nb;\n+    }\n+  }\n+  assert(n->outcnt() == 0, \"all uses must be deleted\");\n+}\n+\n@@ -2416,0 +2429,10 @@\n+  \/\/ AndLNode::Ideal folds GraphKit::mark_word_test patterns. Give it a chance to run.\n+  if (n->is_Load() && use->is_Phi()) {\n+    for (DUIterator_Fast imax, i = use->fast_outs(imax); i < imax; i++) {\n+      Node* u = use->fast_out(i);\n+      if (u->Opcode() == Op_AndL) {\n+        worklist.push(u);\n+      }\n+    }\n+  }\n+\n@@ -2513,0 +2536,9 @@\n+  \/\/ Inline type nodes can have other inline types as users. If an input gets\n+  \/\/ updated, make sure that inline type users get a chance for optimization.\n+  if (use->is_InlineType()) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->is_InlineType())\n+        worklist.push(u);\n+    }\n+  }\n@@ -2639,0 +2671,8 @@\n+  if (use_op == Op_CastP2X) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->Opcode() == Op_AndX) {\n+        worklist.push(u);\n+      }\n+    }\n+  }\n@@ -2657,0 +2697,10 @@\n+  \/\/ Give CallStaticJavaNode::remove_useless_allocation a chance to run\n+  if (use->is_Region()) {\n+    Node* c = use;\n+    do {\n+      c = c->unique_ctrl_out_or_null();\n+    } while (c != nullptr && c->is_Region());\n+    if (c != nullptr && c->is_CallStaticJava() && c->as_CallStaticJava()->uncommon_trap_request() != 0) {\n+      worklist.push(c);\n+    }\n+  }\n@@ -2776,1 +2826,1 @@\n-    n->dump(1);\n+    n->dump(3);\n@@ -2898,0 +2948,1 @@\n+  push_cast(worklist, use);\n@@ -3009,0 +3060,12 @@\n+  }\n+}\n+\n+void PhaseCCP::push_cast(Unique_Node_List& worklist, const Node* use) {\n+  uint use_op = use->Opcode();\n+  if (use_op == Op_CastP2X) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->Opcode() == Op_AndX) {\n+        worklist.push(u);\n+      }\n+    }\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":71,"deletions":8,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -146,0 +146,2 @@\n+  flags(SPLIT_INLINES_ARRAY,            \"Split inlines array\") \\\n+  flags(SPLIT_INLINES_ARRAY_IGVN,       \"IGVN after split inlines array\") \\\n","filename":"src\/hotspot\/share\/opto\/phasetype.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -54,0 +54,2 @@\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -421,0 +423,1 @@\n+  bool is_flat = InstanceKlass::cast(k1)->field_is_flat(slot);\n@@ -422,1 +425,1 @@\n-  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset);\n+  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset, is_flat);\n@@ -439,1 +442,1 @@\n-  if (m->is_object_initializer()) {\n+  if (m->is_object_constructor()) {\n@@ -798,1 +801,1 @@\n-    case T_OBJECT:      push_object(va_arg(_ap, jobject)); break;\n+    case T_OBJECT: push_object(va_arg(_ap, jobject)); break;\n@@ -965,1 +968,7 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr || k->is_inline_klass()) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+  instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n@@ -979,1 +988,8 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+\n+  instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n@@ -984,0 +1000,1 @@\n+\n@@ -985,1 +1002,1 @@\n-JNI_END\n+  JNI_END\n@@ -997,1 +1014,8 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+\n+  instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n@@ -1002,0 +1026,1 @@\n+\n@@ -1015,1 +1040,8 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+\n+  instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n@@ -1023,0 +1055,1 @@\n+\n@@ -1773,1 +1806,1 @@\n-  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset());\n+  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset(), fd.is_flat());\n@@ -1783,0 +1816,1 @@\n+  oop res = nullptr;\n@@ -1788,2 +1822,15 @@\n-  oop loaded_obj = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);\n-  jobject ret = JNIHandles::make_local(THREAD, loaded_obj);\n+  if (!jfieldIDWorkaround::is_flat_jfieldID(fieldID)) {\n+    res = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);\n+  } else {\n+    assert(k->is_instance_klass(), \"Only instance can have flat fields\");\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    fieldDescriptor fd;\n+    bool found = ik->find_field_from_offset(offset, false, &fd);  \/\/ performance bottleneck\n+    assert(found, \"Field not found\");\n+    InstanceKlass* holder = fd.field_holder();\n+    assert(holder->field_is_flat(fd.index()), \"Must be\");\n+    InlineLayoutInfo* li = holder->inline_layout_info_adr(fd.index());\n+    InlineKlass* field_vklass = li->klass();\n+    res = field_vklass->read_payload_from_addr(o, ik->field_offset(fd.index()), li->kind(), CHECK_NULL);\n+  }\n+  jobject ret = JNIHandles::make_local(THREAD, res);\n@@ -1794,2 +1841,0 @@\n-\n-\n@@ -1907,1 +1952,22 @@\n-  HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, JNIHandles::resolve(value));\n+  if (!jfieldIDWorkaround::is_flat_jfieldID(fieldID)) {\n+    oop v = JNIHandles::resolve(value);\n+    if (v == nullptr) {\n+      InstanceKlass *ik = InstanceKlass::cast(k);\n+      fieldDescriptor fd;\n+      ik->find_field_from_offset(offset, false, &fd);\n+      if (fd.is_null_free_inline_type()) {\n+        THROW_MSG(vmSymbols::java_lang_NullPointerException(), \"Cannot store null in a null-restricted field\");\n+      }\n+    }\n+    HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, v);\n+  } else {\n+    assert(k->is_instance_klass(), \"Only instances can have flat fields\");\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    fieldDescriptor fd;\n+    ik->find_field_from_offset(offset, false, &fd);\n+    InstanceKlass* holder = fd.field_holder();\n+    InlineLayoutInfo* li = holder->inline_layout_info_adr(fd.index());\n+    InlineKlass* vklass = li->klass();\n+    oop v = JNIHandles::resolve(value);\n+    vklass->write_value_to_addr(v, ((char*)(oopDesc*)o) + offset, li->kind(), true, CHECK);\n+  }\n@@ -2340,1 +2406,3 @@\n-    ret = JNIHandles::make_local(THREAD, a->obj_at(index));\n+    oop res = a->obj_at(index, CHECK_NULL);\n+    assert(res != nullptr || !a->is_null_free_array(), \"Invalid value\");\n+    ret = JNIHandles::make_local(THREAD, res);\n@@ -2357,5 +2425,6 @@\n-  objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n-  oop v = JNIHandles::resolve(value);\n-  if (a->is_within_bounds(index)) {\n-    if (v == nullptr || v->is_a(ObjArrayKlass::cast(a->klass())->element_klass())) {\n-      a->obj_at_put(index, v);\n+   objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n+   oop v = JNIHandles::resolve(value);\n+   if (a->is_within_bounds(index)) {\n+    Klass* ek = a->is_flatArray() ? FlatArrayKlass::cast(a->klass())->element_klass() : RefArrayKlass::cast(a->klass())->element_klass();\n+    if (v == nullptr || v->is_a(ek)) {\n+      a->obj_at_put(index, v, CHECK);\n@@ -2375,6 +2444,6 @@\n-  } else {\n-    ResourceMark rm(THREAD);\n-    stringStream ss;\n-    ss.print(\"Index %d out of bounds for length %d\", index, a->length());\n-    THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n-  }\n+   } else {\n+     ResourceMark rm(THREAD);\n+     stringStream ss;\n+     ss.print(\"Index %d out of bounds for length %d\", index, a->length());\n+     THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n+   }\n@@ -2756,1 +2825,1 @@\n-  ObjectSynchronizer::jni_enter(obj, thread);\n+  ObjectSynchronizer::jni_enter(obj, CHECK_(JNI_ERR));\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":96,"deletions":27,"binary":false,"changes":123,"status":"modified"},{"patch":"@@ -258,1 +258,1 @@\n-      !(fd.field_type() == T_ARRAY && ftype == T_OBJECT)) {\n+      !(fd.field_type() == T_ARRAY && ftype == T_OBJECT))  {\n@@ -361,1 +361,1 @@\n-check_is_obj_array(JavaThread* thr, jarray jArray) {\n+check_is_obj_or_inline_array(JavaThread* thr, jarray jArray) {\n@@ -363,1 +363,1 @@\n-  if (!aOop->is_objArray()) {\n+  if (!aOop->is_objArray() && !aOop->is_flatArray()) {\n@@ -1670,1 +1670,1 @@\n-      check_is_obj_array(thr, array);\n+      check_is_obj_or_inline_array(thr, array);\n@@ -1684,1 +1684,1 @@\n-      check_is_obj_array(thr, array);\n+      check_is_obj_or_inline_array(thr, array);\n","filename":"src\/hotspot\/share\/prims\/jniCheck.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -73,0 +73,61 @@\n+\n+\/\/ Helper class to store objects to visit.\n+class JvmtiHeapwalkVisitStack {\n+private:\n+  enum {\n+    initial_visit_stack_size = 4000\n+  };\n+\n+  GrowableArray<JvmtiHeapwalkObject>* _visit_stack;\n+  JVMTIBitSet _bitset;\n+\n+  static GrowableArray<JvmtiHeapwalkObject>* create_visit_stack() {\n+    return new (mtServiceability) GrowableArray<JvmtiHeapwalkObject>(initial_visit_stack_size, mtServiceability);\n+  }\n+\n+public:\n+  JvmtiHeapwalkVisitStack(): _visit_stack(create_visit_stack()) {\n+  }\n+  ~JvmtiHeapwalkVisitStack() {\n+    if (_visit_stack != nullptr) {\n+      delete _visit_stack;\n+    }\n+  }\n+\n+  bool is_empty() const {\n+    return _visit_stack->is_empty();\n+  }\n+\n+  void push(const JvmtiHeapwalkObject& obj) {\n+    _visit_stack->push(obj);\n+  }\n+\n+  \/\/ If the object hasn't been visited then push it onto the visit stack\n+  \/\/ so that it will be visited later.\n+  void check_for_visit(const JvmtiHeapwalkObject& obj) {\n+    if (!is_visited(obj)) {\n+      _visit_stack->push(obj);\n+    }\n+  }\n+\n+  JvmtiHeapwalkObject pop() {\n+    return _visit_stack->pop();\n+  }\n+\n+  bool is_visited(const JvmtiHeapwalkObject& obj) \/*const*\/ { \/\/ TODO: _bitset.is_marked() should be const\n+    \/\/ The method is called only for objects from visit_stack to ensure an object is not visited twice.\n+    \/\/ Flat objects can be added to visit_stack only when we visit their holder object, so we cannot get duplicate reference to it.\n+    if (obj.is_flat()) {\n+      return false;\n+    }\n+    return _bitset.is_marked(obj.obj());\n+  }\n+\n+  void mark_visited(const JvmtiHeapwalkObject& obj) {\n+    if (!obj.is_flat()) {\n+      _bitset.mark_obj(obj.obj());\n+    }\n+  }\n+};\n+\n+\n@@ -80,1 +141,2 @@\n-  _posting_events(false) {\n+  _posting_events(false),\n+  _converting_flat_object(false) {\n@@ -86,0 +148,1 @@\n+  _flat_hashmap = new JvmtiFlatTagMapTable();\n@@ -101,0 +164,1 @@\n+  delete _flat_hashmap;\n@@ -109,0 +173,1 @@\n+  _flat_hashmap->clear();\n@@ -127,6 +192,1 @@\n-\/\/ iterate over all entries in the tag map.\n-void JvmtiTagMap::entry_iterate(JvmtiTagMapKeyClosure* closure) {\n-  hashmap()->entry_iterate(closure);\n-}\n-\n-bool JvmtiTagMap::is_empty() {\n+bool JvmtiTagMap::is_empty() const {\n@@ -135,1 +195,1 @@\n-  return hashmap()->is_empty();\n+  return _hashmap->is_empty() && _flat_hashmap->is_empty();\n@@ -169,5 +229,170 @@\n-\/\/ Return the tag value for an object, or 0 if the object is\n-\/\/ not tagged\n-\/\/\n-static inline jlong tag_for(JvmtiTagMap* tag_map, oop o) {\n-  return tag_map->hashmap()->find(o);\n+\/\/ Converts entries from JvmtiFlatTagMapTable to JvmtiTagMapTable in batches.\n+\/\/   1. (JvmtiTagMap is locked)\n+\/\/      reads entries from JvmtiFlatTagMapTable (describe flat value objects);\n+\/\/   2. (JvmtiTagMap is unlocked)\n+\/\/      creates heap-allocated copies of the flat object;\n+\/\/   3. (JvmtiTagMap is locked)\n+\/\/      ensures source entry still exists, removes it from JvmtiFlatTagMapTable, adds new entry to JvmtiTagMapTable.\n+\/\/ If some error occurs in step 2 (OOM?), the process stops.\n+class JvmtiTagMapFlatEntryConverter: public StackObj {\n+private:\n+  struct Entry {\n+    \/\/ source flat value object\n+    Handle holder;\n+    int offset;\n+    InlineKlass* inline_klass;\n+    LayoutKind layout_kind;\n+    \/\/ converted heap-allocated object\n+    Handle dst;\n+\n+    Entry(): holder(), offset(0), inline_klass(nullptr), dst() {}\n+    Entry(Handle holder, int offset, InlineKlass* inline_klass, LayoutKind lk)\n+      : holder(holder), offset(offset), inline_klass(inline_klass), layout_kind(lk), dst() {}\n+  };\n+\n+  int _batch_size;\n+  GrowableArray<Entry> _entries;\n+  bool _has_error;\n+\n+public:\n+  JvmtiTagMapFlatEntryConverter(int batch_size): _batch_size(batch_size), _entries(batch_size, mtServiceability), _has_error(false) { }\n+  ~JvmtiTagMapFlatEntryConverter() {}\n+\n+  \/\/ returns false if there is nothing to convert\n+  bool import_entries(JvmtiFlatTagMapTable* table) {\n+    if (_has_error) {\n+      \/\/ stop the process to avoid infinite loop\n+      return false;\n+    }\n+\n+    class Importer: public JvmtiFlatTagMapKeyClosure {\n+    private:\n+      GrowableArray<Entry>& _entries;\n+      int _batch_size;\n+    public:\n+      Importer(GrowableArray<Entry>& entries, int batch_size): _entries(entries), _batch_size(batch_size) {}\n+\n+      bool do_entry(JvmtiFlatTagMapKey& key, jlong& tag) {\n+        Entry entry(Handle(Thread::current(), key.holder()), key.offset(), key.inline_klass(), key.layout_kind());\n+        _entries.append(entry);\n+\n+        return _entries.length() < _batch_size;\n+      }\n+    } importer(_entries, _batch_size);\n+    table->entry_iterate(&importer);\n+\n+    return !_entries.is_empty();\n+  }\n+\n+  void convert() {\n+    for (int i = 0; i < _entries.length(); i++) {\n+      EXCEPTION_MARK;\n+      Entry& entry = _entries.at(i);\n+      oop obj = entry.inline_klass->read_payload_from_addr(entry.holder(), entry.offset, entry.layout_kind, JavaThread::current());\n+\n+      if (HAS_PENDING_EXCEPTION) {\n+        tty->print_cr(\"Exception in JvmtiTagMapFlatEntryConverter: \");\n+        java_lang_Throwable::print(PENDING_EXCEPTION, tty);\n+        tty->cr();\n+        CLEAR_PENDING_EXCEPTION;\n+        \/\/ stop the conversion\n+        _has_error = true;\n+      } else {\n+        entry.dst = Handle(Thread::current(), obj);\n+      }\n+    }\n+  }\n+\n+  \/\/ returns number of converted entries\n+  int move(JvmtiFlatTagMapTable* src_table, JvmtiTagMapTable* dst_table) {\n+    int count = 0;\n+    for (int i = 0; i < _entries.length(); i++) {\n+      Entry& entry = _entries.at(i);\n+      if (entry.dst() == nullptr) {\n+        \/\/ some error during conversion, skip the entry\n+        continue;\n+      }\n+      JvmtiHeapwalkObject obj(entry.holder(), entry.offset, entry.inline_klass, entry.layout_kind);\n+      jlong tag = src_table->remove(obj);\n+\n+      if (tag != 0) { \/\/ ensure the entry is still in the src_table\n+        dst_table->add(entry.dst(), tag);\n+        count++;\n+      } else {\n+\n+      }\n+    }\n+    \/\/ and clean the array\n+    _entries.clear();\n+    return count;\n+  }\n+};\n+\n+\n+void JvmtiTagMap::convert_flat_object_entries() {\n+  Thread* current = Thread::current();\n+  assert(current->is_Java_thread(), \"must be executed on JavaThread\");\n+\n+  log_debug(jvmti, table)(\"convert_flat_object_entries, main table size = %d, flat table size = %d\",\n+                          _hashmap->number_of_entries(), _flat_hashmap->number_of_entries());\n+\n+  {\n+    MonitorLocker ml(lock(), Mutex::_no_safepoint_check_flag);\n+    \/\/ If another thread is converting, let it finish.\n+    while (_converting_flat_object) {\n+      ml.wait();\n+    }\n+    if (_flat_hashmap->is_empty()) {\n+      \/\/ nothing to convert\n+      return;\n+    }\n+    _converting_flat_object = true;\n+  }\n+\n+  const int BATCH_SIZE = 1024;\n+  JvmtiTagMapFlatEntryConverter converter(BATCH_SIZE);\n+\n+  int count = 0;\n+  while (true) {\n+    HandleMark hm(current);\n+    {\n+      MonitorLocker ml(lock(), Mutex::_no_safepoint_check_flag);\n+      if (!converter.import_entries(_flat_hashmap)) {\n+        break;\n+      }\n+    }\n+    \/\/ Convert flat objects to heap-allocated without table lock (so agent callbacks can get\/set tags).\n+    converter.convert();\n+    {\n+      MonitorLocker ml(lock(), Mutex::_no_safepoint_check_flag);\n+      count += converter.move(_flat_hashmap, _hashmap);\n+    }\n+  }\n+\n+  log_info(jvmti, table)(\"%d flat value objects are converted, flat table size = %d\",\n+                         count, _flat_hashmap->number_of_entries());\n+  {\n+    MonitorLocker ml(lock(), Mutex::_no_safepoint_check_flag);\n+    _converting_flat_object = false;\n+    ml.notify_all();\n+  }\n+}\n+\n+jlong JvmtiTagMap::find(const JvmtiHeapwalkObject& obj) const {\n+  jlong tag = _hashmap->find(obj);\n+  if (tag == 0 && obj.is_value()) {\n+    tag = _flat_hashmap->find(obj);\n+  }\n+  return tag;\n+}\n+\n+void JvmtiTagMap::add(const JvmtiHeapwalkObject& obj, jlong tag) {\n+  if (obj.is_flat()) {\n+    \/\/ we may have tag for equal (non-flat) object in _hashmap, try to update it 1st\n+    if (!_hashmap->update(obj, tag)) {\n+      \/\/ no entry in _hashmap, add to _flat_hashmap\n+      _flat_hashmap->add(obj, tag);\n+    }\n+  } else {\n+    _hashmap->add(obj, tag);\n+  }\n@@ -176,0 +401,9 @@\n+void JvmtiTagMap::remove(const JvmtiHeapwalkObject& obj) {\n+  if (!_hashmap->remove(obj)) {\n+    if (obj.is_value()) {\n+      _flat_hashmap->remove(obj);\n+    }\n+  }\n+}\n+\n+\n@@ -194,2 +428,1 @@\n-  JvmtiTagMapTable* _hashmap;\n-  oop _o;\n+  const JvmtiHeapwalkObject& _o;\n@@ -204,2 +437,2 @@\n-  void inline post_callback_tag_update(oop o, JvmtiTagMapTable* hashmap,\n-                                       jlong obj_tag);\n+  void inline post_callback_tag_update(const JvmtiHeapwalkObject& o, JvmtiTagMap* tag_map, jlong obj_tag);\n+\n@@ -207,1 +440,3 @@\n-  CallbackWrapper(JvmtiTagMap* tag_map, oop o) {\n+  CallbackWrapper(JvmtiTagMap* tag_map, const JvmtiHeapwalkObject& o)\n+    : _tag_map(tag_map), _o(o)\n+  {\n@@ -211,8 +446,8 @@\n-    \/\/ object to tag\n-    _o = o;\n-\n-    _obj_size = (jlong)_o->size() * wordSize;\n-\n-    \/\/ record the context\n-    _tag_map = tag_map;\n-    _hashmap = tag_map->hashmap();\n+    if (!o.is_flat()) {\n+      \/\/ common case: we have oop\n+      _obj_size = (jlong)o.obj()->size() * wordSize;\n+    } else {\n+      \/\/ flat value object, we know its InstanceKlass\n+      assert(_o.inline_klass() != nullptr, \"must be\");\n+      _obj_size = _o.inline_klass()->size() * wordSize;;\n+    }\n@@ -222,1 +457,1 @@\n-    _obj_tag = _hashmap->find(_o);\n+    _obj_tag = _tag_map->find(_o);\n@@ -227,1 +462,1 @@\n-    _klass_tag = tag_for(tag_map, _o->klass()->java_mirror());\n+    _klass_tag = _tag_map->find(_o.klass()->java_mirror());\n@@ -231,1 +466,1 @@\n-    post_callback_tag_update(_o, _hashmap, _obj_tag);\n+    post_callback_tag_update(_o, _tag_map, _obj_tag);\n@@ -241,2 +476,2 @@\n-void inline CallbackWrapper::post_callback_tag_update(oop o,\n-                                                      JvmtiTagMapTable* hashmap,\n+void inline CallbackWrapper::post_callback_tag_update(const JvmtiHeapwalkObject& o,\n+                                                      JvmtiTagMap* tag_map,\n@@ -246,1 +481,1 @@\n-    hashmap->remove(o);\n+    tag_map->remove(o);\n@@ -251,1 +486,1 @@\n-    hashmap->add(o, obj_tag);\n+    tag_map->add(o, obj_tag);\n@@ -273,0 +508,1 @@\n+  const JvmtiHeapwalkObject& _referrer;\n@@ -274,2 +510,0 @@\n-  JvmtiTagMapTable* _referrer_hashmap;\n-  oop _referrer;\n@@ -283,2 +517,2 @@\n-  TwoOopCallbackWrapper(JvmtiTagMap* tag_map, oop referrer, oop o) :\n-    CallbackWrapper(tag_map, o)\n+  TwoOopCallbackWrapper(JvmtiTagMap* tag_map, const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& o) :\n+    CallbackWrapper(tag_map, o), _referrer(referrer)\n@@ -293,5 +527,1 @@\n-      _referrer = referrer;\n-      \/\/ record the context\n-      _referrer_hashmap = tag_map->hashmap();\n-\n-      _referrer_obj_tag = _referrer_hashmap->find(_referrer);\n+      _referrer_obj_tag = tag_map->find(_referrer);\n@@ -303,1 +533,1 @@\n-      _referrer_klass_tag = tag_for(tag_map, _referrer->klass()->java_mirror());\n+      _referrer_klass_tag = tag_map->find(_referrer.klass()->java_mirror());\n@@ -310,1 +540,1 @@\n-                               _referrer_hashmap,\n+                               tag_map(),\n@@ -338,3 +568,1 @@\n-\n-  JvmtiTagMapTable* hashmap = _hashmap;\n-\n+  JvmtiHeapwalkObject obj(o);\n@@ -344,1 +572,1 @@\n-    hashmap->remove(o);\n+    _hashmap->remove(obj);\n@@ -348,1 +576,1 @@\n-    hashmap->add(o, tag);\n+    add(obj, tag);\n@@ -364,1 +592,1 @@\n-  return tag_for(this, o);\n+  return find(o);\n@@ -377,0 +605,2 @@\n+  InlineKlass* _inline_klass; \/\/ nullptr for heap object\n+  LayoutKind _layout_kind;\n@@ -378,2 +608,12 @@\n-  ClassFieldDescriptor(int index, char type, int offset) :\n-    _field_index(index), _field_offset(offset), _field_type(type) {\n+  ClassFieldDescriptor(int index, const FieldStreamBase& fld) :\n+      _field_index(index), _field_offset(fld.offset()), _field_type(fld.signature()->char_at(0)) {\n+    if (fld.is_flat()) {\n+      const fieldDescriptor& fd = fld.field_descriptor();\n+      InstanceKlass* holder_klass = fd.field_holder();\n+      InlineLayoutInfo* layout_info = holder_klass->inline_layout_info_adr(fd.index());\n+      _inline_klass = layout_info->klass();\n+      _layout_kind = layout_info->kind();\n+    } else {\n+      _inline_klass = nullptr;\n+      _layout_kind = LayoutKind::REFERENCE;\n+    }\n@@ -384,0 +624,3 @@\n+  bool is_flat()     const  { return _inline_klass != nullptr; }\n+  InlineKlass* inline_klass() const { return _inline_klass; }\n+  LayoutKind layout_kind() const { return _layout_kind; }\n@@ -402,1 +645,1 @@\n-  void add(int index, char type, int offset);\n+  void add(int index, const FieldStreamBase& fld);\n@@ -413,1 +656,1 @@\n-  static ClassFieldMap* create_map_of_instance_fields(oop obj);\n+  static ClassFieldMap* create_map_of_instance_fields(Klass* k);\n@@ -438,2 +681,2 @@\n-void ClassFieldMap::add(int index, char type, int offset) {\n-  ClassFieldDescriptor* field = new ClassFieldDescriptor(index, type, offset);\n+void ClassFieldMap::add(int index, const FieldStreamBase& fld) {\n+  ClassFieldDescriptor* field = new ClassFieldDescriptor(index, fld);\n@@ -463,1 +706,1 @@\n-    field_map->add(index, fld.signature()->char_at(0), fld.offset());\n+    field_map->add(index, fld);\n@@ -472,2 +715,2 @@\n-ClassFieldMap* ClassFieldMap::create_map_of_instance_fields(oop obj) {\n-  InstanceKlass* ik = InstanceKlass::cast(obj->klass());\n+ClassFieldMap* ClassFieldMap::create_map_of_instance_fields(Klass* k) {\n+  InstanceKlass* ik = InstanceKlass::cast(k);\n@@ -492,1 +735,1 @@\n-      field_map->add(start_index + index, fld.signature()->char_at(0), fld.offset());\n+      field_map->add(start_index + index, fld);\n@@ -522,1 +765,1 @@\n-  \/\/ returns the field map for a given object (returning map cached\n+  \/\/ returns the field map for a given klass (returning map cached\n@@ -524,1 +767,1 @@\n-  static ClassFieldMap* get_map_of_instance_fields(oop obj);\n+  static ClassFieldMap* get_map_of_instance_fields(Klass* k);\n@@ -576,1 +819,1 @@\n-\/\/ returns the instance field map for the given object\n+\/\/ returns the instance field map for the given klass\n@@ -578,1 +821,1 @@\n-ClassFieldMap* JvmtiCachedClassFieldMap::get_map_of_instance_fields(oop obj) {\n+ClassFieldMap* JvmtiCachedClassFieldMap::get_map_of_instance_fields(Klass *k) {\n@@ -582,1 +825,0 @@\n-  Klass* k = obj->klass();\n@@ -591,1 +833,1 @@\n-    ClassFieldMap* field_map = ClassFieldMap::create_map_of_instance_fields(obj);\n+    ClassFieldMap* field_map = ClassFieldMap::create_map_of_instance_fields(k);\n@@ -643,1 +885,1 @@\n-static inline bool is_filtered_by_klass_filter(oop obj, Klass* klass_filter) {\n+static inline bool is_filtered_by_klass_filter(const JvmtiHeapwalkObject& obj, Klass* klass_filter) {\n@@ -645,1 +887,1 @@\n-    if (obj->klass() != klass_filter) {\n+    if (obj.klass() != klass_filter) {\n@@ -676,1 +918,1 @@\n-                                         oop str,\n+                                         const JvmtiHeapwalkObject& obj,\n@@ -679,0 +921,2 @@\n+  assert(!obj.is_flat(), \"cannot be flat\");\n+  oop str = obj.obj();\n@@ -727,1 +971,1 @@\n-                                                  oop obj,\n+                                                  const JvmtiHeapwalkObject& obj,\n@@ -730,1 +974,2 @@\n-  assert(obj->is_typeArray(), \"not a primitive array\");\n+  assert(!obj.is_flat(), \"cannot be flat\");\n+  assert(obj.obj()->is_typeArray(), \"not a primitive array\");\n@@ -733,1 +978,1 @@\n-  typeArrayOop array = typeArrayOop(obj);\n+  typeArrayOop array = typeArrayOop(obj.obj());\n@@ -823,1 +1068,1 @@\n-  oop obj,\n+  const JvmtiHeapwalkObject& obj,\n@@ -831,1 +1076,1 @@\n-  ClassFieldMap* fields = JvmtiCachedClassFieldMap::get_map_of_instance_fields(obj);\n+  ClassFieldMap* fields = JvmtiCachedClassFieldMap::get_map_of_instance_fields(obj.klass());\n@@ -845,3 +1090,2 @@\n-    \/\/ get offset and field value\n-    int offset = field->field_offset();\n-    address addr = cast_from_oop<address>(obj) + offset;\n+    \/\/ get field value\n+    address addr = cast_from_oop<address>(obj.obj()) + obj.offset() + field->field_offset();\n@@ -961,1 +1205,2 @@\n-  CallbackWrapper wrapper(tag_map(), o);\n+  JvmtiHeapwalkObject wrapper_obj(o);\n+  CallbackWrapper wrapper(tag_map(), wrapper_obj);\n@@ -1013,0 +1258,4 @@\n+  void visit_object(const JvmtiHeapwalkObject& obj);\n+  void visit_flat_fields(const JvmtiHeapwalkObject& obj);\n+  void visit_flat_array_elements(const JvmtiHeapwalkObject& obj);\n+\n@@ -1028,1 +1277,1 @@\n-  void do_object(oop o);\n+  void do_object(oop obj);\n@@ -1037,4 +1286,1 @@\n-  \/\/ apply class filter\n-  if (is_filtered_by_klass_filter(obj, klass())) return;\n-\n-  if (obj->klass()->java_mirror() == nullptr) {\n+  if (obj != nullptr && obj->klass()->java_mirror() == nullptr) {\n@@ -1047,0 +1293,7 @@\n+  visit_object(obj);\n+}\n+\n+void IterateThroughHeapObjectClosure::visit_object(const JvmtiHeapwalkObject& obj) {\n+  \/\/ apply class filter\n+  if (is_filtered_by_klass_filter(obj, klass())) return;\n+\n@@ -1056,2 +1309,2 @@\n-  bool is_array = obj->is_array();\n-  int len = is_array ? arrayOop(obj)->length() : -1;\n+  bool is_array = obj.klass()->is_array_klass();\n+  int len = is_array ? arrayOop(obj.obj())->length() : -1;\n@@ -1071,1 +1324,1 @@\n-  if (callbacks()->primitive_field_callback != nullptr && obj->is_instance()) {\n+  if (callbacks()->primitive_field_callback != nullptr && obj.klass()->is_instance_klass()) {\n@@ -1074,1 +1327,2 @@\n-    if (obj->klass() == vmClasses::Class_klass()) {\n+    if (obj.klass() == vmClasses::Class_klass()) {\n+      assert(!obj.is_flat(), \"Class object cannot be flattened\");\n@@ -1076,3 +1330,3 @@\n-                                                                    obj,\n-                                                                    cb,\n-                                                                    (void*)user_data());\n+                                                              obj.obj(),\n+                                                              cb,\n+                                                              (void*)user_data());\n@@ -1081,3 +1335,3 @@\n-                                                                      obj,\n-                                                                      cb,\n-                                                                      (void*)user_data());\n+                                                                obj,\n+                                                                cb,\n+                                                                (void*)user_data());\n@@ -1091,1 +1345,1 @@\n-      obj->klass() == vmClasses::String_klass()) {\n+      obj.klass() == vmClasses::String_klass()) {\n@@ -1096,1 +1350,1 @@\n-                (void*)user_data() );\n+                (void*)user_data());\n@@ -1103,1 +1357,1 @@\n-      obj->is_typeArray()) {\n+      obj.klass()->is_typeArray_klass()) {\n@@ -1108,1 +1362,1 @@\n-               (void*)user_data() );\n+               (void*)user_data());\n@@ -1111,1 +1365,80 @@\n-};\n+  \/\/ All info for the object is reported.\n+\n+  \/\/ If the object has flat fields, report them as heap objects.\n+  if (obj.klass()->is_instance_klass()) {\n+    if (InstanceKlass::cast(obj.klass())->has_inline_type_fields()) {\n+      visit_flat_fields(obj);\n+      \/\/ check if iteration has been halted\n+      if (is_iteration_aborted()) {\n+        return;\n+      }\n+    }\n+  }\n+  \/\/ If the object is flat array, report all elements as heap objects.\n+  if (is_array && obj.obj()->is_flatArray()) {\n+    assert(!obj.is_flat(), \"Array object cannot be flattened\");\n+    visit_flat_array_elements(obj);\n+  }\n+}\n+\n+void IterateThroughHeapObjectClosure::visit_flat_fields(const JvmtiHeapwalkObject& obj) {\n+  \/\/ iterate over instance fields\n+  ClassFieldMap* fields = JvmtiCachedClassFieldMap::get_map_of_instance_fields(obj.klass());\n+  for (int i = 0; i < fields->field_count(); i++) {\n+    ClassFieldDescriptor* field = fields->field_at(i);\n+    \/\/ skip non-flat and (for safety) primitive fields\n+    if (!field->is_flat() || is_primitive_field_type(field->field_type())) {\n+      continue;\n+    }\n+\n+    int field_offset = field->field_offset();\n+    if (obj.is_flat()) {\n+      \/\/ the object is inlined, its fields are stored without the header\n+      field_offset += obj.offset() - obj.inline_klass()->payload_offset();\n+    }\n+    \/\/ check for possible nulls\n+    if (LayoutKindHelper::is_nullable_flat(field->layout_kind())) {\n+      address payload = cast_from_oop<address>(obj.obj()) + field_offset;\n+      if (field->inline_klass()->is_payload_marked_as_null(payload)) {\n+        continue;\n+      }\n+    }\n+    JvmtiHeapwalkObject field_obj(obj.obj(), field_offset, field->inline_klass(), field->layout_kind());\n+\n+    visit_object(field_obj);\n+\n+    \/\/ check if iteration has been halted\n+    if (is_iteration_aborted()) {\n+      return;\n+    }\n+  }\n+}\n+\n+void IterateThroughHeapObjectClosure::visit_flat_array_elements(const JvmtiHeapwalkObject& obj) {\n+  assert(!obj.is_flat() && obj.obj()->is_flatArray() , \"sanity check\");\n+  flatArrayOop array = flatArrayOop(obj.obj());\n+  FlatArrayKlass* faklass = FlatArrayKlass::cast(array->klass());\n+  InlineKlass* vk = InlineKlass::cast(faklass->element_klass());\n+  bool need_null_check = LayoutKindHelper::is_nullable_flat(faklass->layout_kind());\n+\n+  for (int index = 0; index < array->length(); index++) {\n+    address addr = (address)array->value_at_addr(index, faklass->layout_helper());\n+    \/\/ check for null\n+    if (need_null_check) {\n+      if (vk->is_payload_marked_as_null(addr)) {\n+        continue;\n+      }\n+    }\n+\n+    \/\/ offset in the array oop\n+    int offset = (int)(addr - cast_from_oop<address>(array));\n+    JvmtiHeapwalkObject elem(obj.obj(), offset, vk, faklass->layout_kind());\n+\n+    visit_object(elem);\n+\n+    \/\/ check if iteration has been halted\n+    if (is_iteration_aborted()) {\n+      return;\n+    }\n+  }\n+}\n@@ -1137,0 +1470,2 @@\n+  convert_flat_object_entries();\n+\n@@ -1164,0 +1499,2 @@\n+  convert_flat_object_entries();\n+\n@@ -1177,1 +1514,1 @@\n-    hashmap()->remove_dead_entries(objects);\n+    _hashmap->remove_dead_entries(objects);\n@@ -1330,0 +1667,3 @@\n+  \/\/ ensure flat object conversion is completed\n+  convert_flat_object_entries();\n+\n@@ -1338,1 +1678,1 @@\n-    entry_iterate(&collector);\n+    _hashmap->entry_iterate(&collector);\n@@ -1378,1 +1718,1 @@\n-  oop _last_referrer;\n+  JvmtiHeapwalkObject _last_referrer;\n@@ -1391,1 +1731,1 @@\n-    _last_referrer(nullptr),\n+    _last_referrer(),\n@@ -1400,2 +1740,2 @@\n-  oop last_referrer() const               { return _last_referrer; }\n-  void set_last_referrer(oop referrer)    { _last_referrer = referrer; }\n+  JvmtiHeapwalkObject last_referrer() const    { return _last_referrer; }\n+  void set_last_referrer(const JvmtiHeapwalkObject& referrer) { _last_referrer = referrer; }\n@@ -1474,2 +1814,1 @@\n-  static GrowableArray<oop>* _visit_stack;\n-  static JVMTIBitSet* _bitset;\n+  static JvmtiHeapwalkVisitStack* _visit_stack;\n@@ -1480,1 +1819,1 @@\n-  static GrowableArray<oop>* visit_stack()             { return _visit_stack; }\n+  static JvmtiHeapwalkVisitStack* visit_stack()        { return _visit_stack; }\n@@ -1484,2 +1823,2 @@\n-  static inline bool check_for_visit(oop obj) {\n-    if (!_bitset->is_marked(obj)) visit_stack()->push(obj);\n+  static inline bool check_for_visit(const JvmtiHeapwalkObject&obj) {\n+    visit_stack()->check_for_visit(obj);\n@@ -1489,0 +1828,10 @@\n+  \/\/ return element count if the obj is array, -1 otherwise\n+  static jint get_array_length(const JvmtiHeapwalkObject& obj) {\n+    if (!obj.klass()->is_array_klass()) {\n+      return -1;\n+    }\n+    assert(!obj.is_flat(), \"array cannot be flat\");\n+    return (jint)arrayOop(obj.obj())->length();\n+  }\n+\n+\n@@ -1491,1 +1840,1 @@\n-    (jvmtiHeapRootKind root_kind, oop obj);\n+    (jvmtiHeapRootKind root_kind, const JvmtiHeapwalkObject& obj);\n@@ -1494,1 +1843,1 @@\n-     int slot, oop obj);\n+     int slot, const JvmtiHeapwalkObject& obj);\n@@ -1496,1 +1845,1 @@\n-    (jvmtiObjectReferenceKind ref_kind, oop referrer, oop referree, jint index);\n+    (jvmtiObjectReferenceKind ref_kind, const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint index);\n@@ -1500,1 +1849,1 @@\n-    (jvmtiHeapReferenceKind ref_kind, oop obj);\n+    (jvmtiHeapReferenceKind ref_kind, const JvmtiHeapwalkObject& obj);\n@@ -1503,1 +1852,1 @@\n-     jmethodID method, jlocation bci, jint slot, oop obj);\n+     jmethodID method, jlocation bci, jint slot, const JvmtiHeapwalkObject& obj);\n@@ -1505,1 +1854,1 @@\n-    (jvmtiHeapReferenceKind ref_kind, oop referrer, oop referree, jint index);\n+    (jvmtiHeapReferenceKind ref_kind, const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint index);\n@@ -1509,1 +1858,1 @@\n-    (jvmtiHeapReferenceKind ref_kind, oop obj, jint index, address addr, char type);\n+    (jvmtiHeapReferenceKind ref_kind, const JvmtiHeapwalkObject& obj, jint index, address addr, char type);\n@@ -1514,1 +1863,0 @@\n-                                             GrowableArray<oop>* visit_stack,\n@@ -1517,1 +1865,1 @@\n-                                             JVMTIBitSet* bitset);\n+                                             JvmtiHeapwalkVisitStack* visit_stack);\n@@ -1521,1 +1869,0 @@\n-                                                GrowableArray<oop>* visit_stack,\n@@ -1524,1 +1871,1 @@\n-                                                JVMTIBitSet* bitset);\n+                                                JvmtiHeapwalkVisitStack* visit_stack);\n@@ -1527,1 +1874,1 @@\n-  static inline bool report_simple_root(jvmtiHeapReferenceKind kind, oop o);\n+  static inline bool report_simple_root(jvmtiHeapReferenceKind kind, const JvmtiHeapwalkObject& o);\n@@ -1529,1 +1876,1 @@\n-    jmethodID m, oop o);\n+    jmethodID m, const JvmtiHeapwalkObject& o);\n@@ -1531,1 +1878,1 @@\n-    jmethodID method, jlocation bci, jint slot, oop o);\n+    jmethodID method, jlocation bci, jint slot, const JvmtiHeapwalkObject& o);\n@@ -1534,14 +1881,14 @@\n-  static inline bool report_array_element_reference(oop referrer, oop referree, jint index);\n-  static inline bool report_class_reference(oop referrer, oop referree);\n-  static inline bool report_class_loader_reference(oop referrer, oop referree);\n-  static inline bool report_signers_reference(oop referrer, oop referree);\n-  static inline bool report_protection_domain_reference(oop referrer, oop referree);\n-  static inline bool report_superclass_reference(oop referrer, oop referree);\n-  static inline bool report_interface_reference(oop referrer, oop referree);\n-  static inline bool report_static_field_reference(oop referrer, oop referree, jint slot);\n-  static inline bool report_field_reference(oop referrer, oop referree, jint slot);\n-  static inline bool report_constant_pool_reference(oop referrer, oop referree, jint index);\n-  static inline bool report_primitive_array_values(oop array);\n-  static inline bool report_string_value(oop str);\n-  static inline bool report_primitive_instance_field(oop o, jint index, address value, char type);\n-  static inline bool report_primitive_static_field(oop o, jint index, address value, char type);\n+  static inline bool report_array_element_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint index);\n+  static inline bool report_class_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree);\n+  static inline bool report_class_loader_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree);\n+  static inline bool report_signers_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree);\n+  static inline bool report_protection_domain_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree);\n+  static inline bool report_superclass_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree);\n+  static inline bool report_interface_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree);\n+  static inline bool report_static_field_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint slot);\n+  static inline bool report_field_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint slot);\n+  static inline bool report_constant_pool_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint index);\n+  static inline bool report_primitive_array_values(const JvmtiHeapwalkObject& array);\n+  static inline bool report_string_value(const JvmtiHeapwalkObject& str);\n+  static inline bool report_primitive_instance_field(const JvmtiHeapwalkObject& o, jint index, address value, char type);\n+  static inline bool report_primitive_static_field(const JvmtiHeapwalkObject& o, jint index, address value, char type);\n@@ -1556,2 +1903,1 @@\n-GrowableArray<oop>* CallbackInvoker::_visit_stack;\n-JVMTIBitSet* CallbackInvoker::_bitset;\n+JvmtiHeapwalkVisitStack* CallbackInvoker::_visit_stack;\n@@ -1561,1 +1907,0 @@\n-                                                     GrowableArray<oop>* visit_stack,\n@@ -1564,1 +1909,1 @@\n-                                                     JVMTIBitSet* bitset) {\n+                                                     JvmtiHeapwalkVisitStack* visit_stack) {\n@@ -1566,1 +1911,0 @@\n-  _visit_stack = visit_stack;\n@@ -1571,1 +1915,1 @@\n-  _bitset = bitset;\n+  _visit_stack = visit_stack;\n@@ -1576,1 +1920,0 @@\n-                                                        GrowableArray<oop>* visit_stack,\n@@ -1579,1 +1922,1 @@\n-                                                        JVMTIBitSet* bitset) {\n+                                                        JvmtiHeapwalkVisitStack* visit_stack) {\n@@ -1581,1 +1924,0 @@\n-  _visit_stack = visit_stack;\n@@ -1586,1 +1928,1 @@\n-  _bitset = bitset;\n+  _visit_stack = visit_stack;\n@@ -1591,1 +1933,1 @@\n-inline bool CallbackInvoker::invoke_basic_heap_root_callback(jvmtiHeapRootKind root_kind, oop obj) {\n+inline bool CallbackInvoker::invoke_basic_heap_root_callback(jvmtiHeapRootKind root_kind, const JvmtiHeapwalkObject& obj) {\n@@ -1618,1 +1960,1 @@\n-                                                             oop obj) {\n+                                                             const JvmtiHeapwalkObject& obj) {\n@@ -1645,2 +1987,2 @@\n-                                                                    oop referrer,\n-                                                                    oop referree,\n+                                                                    const JvmtiHeapwalkObject& referrer,\n+                                                                    const JvmtiHeapwalkObject& referree,\n@@ -1657,1 +1999,1 @@\n-    referrer_tag = tag_for(tag_map(), referrer);\n+    referrer_tag = tag_map()->find(referrer);\n@@ -1689,1 +2031,1 @@\n-                                                                oop obj) {\n+                                                                const JvmtiHeapwalkObject& obj) {\n@@ -1714,1 +2056,1 @@\n-  jint len = (jint)(obj->is_array() ? arrayOop(obj)->length() : -1);\n+  jint len = get_array_length(obj);\n@@ -1743,1 +2085,1 @@\n-                                                                oop obj) {\n+                                                                const JvmtiHeapwalkObject& obj) {\n@@ -1777,1 +2119,1 @@\n-  jint len = (jint)(obj->is_array() ? arrayOop(obj)->length() : -1);\n+  jint len = get_array_length(obj);\n@@ -1810,2 +2152,2 @@\n-                                                                       oop referrer,\n-                                                                       oop obj,\n+                                                                       const JvmtiHeapwalkObject& referrer,\n+                                                                       const JvmtiHeapwalkObject& obj,\n@@ -1844,1 +2186,1 @@\n-  jint len = (jint)(obj->is_array() ? arrayOop(obj)->length() : -1);\n+  jint len = get_array_length(obj);\n@@ -1867,1 +2209,1 @@\n-inline bool CallbackInvoker::report_simple_root(jvmtiHeapReferenceKind kind, oop obj) {\n+inline bool CallbackInvoker::report_simple_root(jvmtiHeapReferenceKind kind, const JvmtiHeapwalkObject& obj) {\n@@ -1883,2 +2225,2 @@\n-inline bool CallbackInvoker::report_primitive_array_values(oop obj) {\n-  assert(obj->is_typeArray(), \"not a primitive array\");\n+inline bool CallbackInvoker::report_primitive_array_values(const JvmtiHeapwalkObject& obj) {\n+  assert(obj.klass()->is_typeArray_klass(), \"not a primitive array\");\n@@ -1912,2 +2254,2 @@\n-inline bool CallbackInvoker::report_string_value(oop str) {\n-  assert(str->klass() == vmClasses::String_klass(), \"not a string\");\n+inline bool CallbackInvoker::report_string_value(const JvmtiHeapwalkObject& str) {\n+  assert(str.klass() == vmClasses::String_klass(), \"not a string\");\n@@ -1942,1 +2284,1 @@\n-                                                    oop obj,\n+                                                    const JvmtiHeapwalkObject& obj,\n@@ -1990,1 +2332,1 @@\n-inline bool CallbackInvoker::report_primitive_instance_field(oop obj,\n+inline bool CallbackInvoker::report_primitive_instance_field(const JvmtiHeapwalkObject& obj,\n@@ -2002,1 +2344,1 @@\n-inline bool CallbackInvoker::report_primitive_static_field(oop obj,\n+inline bool CallbackInvoker::report_primitive_static_field(const JvmtiHeapwalkObject& obj,\n@@ -2014,1 +2356,1 @@\n-inline bool CallbackInvoker::report_jni_local_root(jlong thread_tag, jlong tid, jint depth, jmethodID m, oop obj) {\n+inline bool CallbackInvoker::report_jni_local_root(jlong thread_tag, jlong tid, jint depth, jmethodID m, const JvmtiHeapwalkObject& obj) {\n@@ -2041,1 +2383,1 @@\n-                                                   oop obj) {\n+                                                   const JvmtiHeapwalkObject& obj) {\n@@ -2062,1 +2404,1 @@\n-inline bool CallbackInvoker::report_class_reference(oop referrer, oop referree) {\n+inline bool CallbackInvoker::report_class_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree) {\n@@ -2071,1 +2413,1 @@\n-inline bool CallbackInvoker::report_class_loader_reference(oop referrer, oop referree) {\n+inline bool CallbackInvoker::report_class_loader_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree) {\n@@ -2080,1 +2422,1 @@\n-inline bool CallbackInvoker::report_signers_reference(oop referrer, oop referree) {\n+inline bool CallbackInvoker::report_signers_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree) {\n@@ -2089,1 +2431,1 @@\n-inline bool CallbackInvoker::report_protection_domain_reference(oop referrer, oop referree) {\n+inline bool CallbackInvoker::report_protection_domain_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree) {\n@@ -2098,1 +2440,1 @@\n-inline bool CallbackInvoker::report_superclass_reference(oop referrer, oop referree) {\n+inline bool CallbackInvoker::report_superclass_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree) {\n@@ -2108,1 +2450,1 @@\n-inline bool CallbackInvoker::report_interface_reference(oop referrer, oop referree) {\n+inline bool CallbackInvoker::report_interface_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree) {\n@@ -2117,1 +2459,1 @@\n-inline bool CallbackInvoker::report_static_field_reference(oop referrer, oop referree, jint slot) {\n+inline bool CallbackInvoker::report_static_field_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint slot) {\n@@ -2126,1 +2468,1 @@\n-inline bool CallbackInvoker::report_array_element_reference(oop referrer, oop referree, jint index) {\n+inline bool CallbackInvoker::report_array_element_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint index) {\n@@ -2135,1 +2477,1 @@\n-inline bool CallbackInvoker::report_field_reference(oop referrer, oop referree, jint slot) {\n+inline bool CallbackInvoker::report_field_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint slot) {\n@@ -2144,1 +2486,1 @@\n-inline bool CallbackInvoker::report_constant_pool_reference(oop referrer, oop referree, jint index) {\n+inline bool CallbackInvoker::report_constant_pool_reference(const JvmtiHeapwalkObject& referrer, const JvmtiHeapwalkObject& referree, jint index) {\n@@ -2304,1 +2646,1 @@\n-  _thread_tag = tag_for(_tag_map, _threadObj);\n+  _thread_tag = _tag_map->find(_threadObj);\n@@ -2437,4 +2779,0 @@\n-  enum {\n-    initial_visit_stack_size = 4000\n-  };\n-\n@@ -2444,3 +2782,1 @@\n-  GrowableArray<oop>* _visit_stack;                 \/\/ the visit stack\n-\n-  JVMTIBitSet _bitset;\n+  JvmtiHeapwalkVisitStack _visit_stack;\n@@ -2457,4 +2793,0 @@\n-  GrowableArray<oop>* create_visit_stack() {\n-    return new (mtServiceability) GrowableArray<oop>(initial_visit_stack_size, mtServiceability);\n-  }\n-\n@@ -2472,1 +2804,1 @@\n-  GrowableArray<oop>* visit_stack() const          { return _visit_stack; }\n+  JvmtiHeapwalkVisitStack* visit_stack()           { return &_visit_stack; }\n@@ -2475,4 +2807,5 @@\n-  inline bool iterate_over_array(oop o);\n-  inline bool iterate_over_type_array(oop o);\n-  inline bool iterate_over_class(oop o);\n-  inline bool iterate_over_object(oop o);\n+  inline bool iterate_over_array(const JvmtiHeapwalkObject& o);\n+  inline bool iterate_over_flat_array(const JvmtiHeapwalkObject& o);\n+  inline bool iterate_over_type_array(const JvmtiHeapwalkObject& o);\n+  inline bool iterate_over_class(const JvmtiHeapwalkObject& o);\n+  inline bool iterate_over_object(const JvmtiHeapwalkObject& o);\n@@ -2487,1 +2820,1 @@\n-  inline bool visit(oop o);\n+  inline bool visit(const JvmtiHeapwalkObject& o);\n@@ -2521,3 +2854,1 @@\n-  _visit_stack = create_visit_stack();\n-\n-  CallbackInvoker::initialize_for_basic_heap_walk(tag_map, _visit_stack, user_data, callbacks, &_bitset);\n+  CallbackInvoker::initialize_for_basic_heap_walk(tag_map, user_data, callbacks, &_visit_stack);\n@@ -2539,2 +2870,1 @@\n-  _visit_stack = create_visit_stack();\n-  CallbackInvoker::initialize_for_advanced_heap_walk(tag_map, _visit_stack, user_data, callbacks, &_bitset);\n+  CallbackInvoker::initialize_for_advanced_heap_walk(tag_map, user_data, callbacks, &_visit_stack);\n@@ -2545,5 +2875,0 @@\n-  if (_following_object_refs) {\n-    assert(_visit_stack != nullptr, \"checking\");\n-    delete _visit_stack;\n-    _visit_stack = nullptr;\n-  }\n@@ -2554,2 +2879,3 @@\n-inline bool VM_HeapWalkOperation::iterate_over_array(oop o) {\n-  objArrayOop array = objArrayOop(o);\n+inline bool VM_HeapWalkOperation::iterate_over_array(const JvmtiHeapwalkObject& o) {\n+  assert(!o.is_flat(), \"Array object cannot be flattened\");\n+  objArrayOop array = objArrayOop(o.obj());\n@@ -2579,0 +2905,38 @@\n+\/\/ similar to iterate_over_array(), but itrates over flat array\n+inline bool VM_HeapWalkOperation::iterate_over_flat_array(const JvmtiHeapwalkObject& o) {\n+  assert(!o.is_flat(), \"Array object cannot be flattened\");\n+  flatArrayOop array = flatArrayOop(o.obj());\n+  FlatArrayKlass* faklass = FlatArrayKlass::cast(array->klass());\n+  InlineKlass* vk = InlineKlass::cast(faklass->element_klass());\n+  bool need_null_check = LayoutKindHelper::is_nullable_flat(faklass->layout_kind());\n+\n+  \/\/ array reference to its class\n+  oop mirror = faklass->java_mirror();\n+  if (!CallbackInvoker::report_class_reference(o, mirror)) {\n+    return false;\n+  }\n+\n+  \/\/ iterate over the array and report each reference to a\n+  \/\/ non-null element\n+  for (int index = 0; index < array->length(); index++) {\n+    address addr = (address)array->value_at_addr(index, faklass->layout_helper());\n+\n+    \/\/ check for null\n+    if (need_null_check) {\n+      if (vk->is_payload_marked_as_null(addr)) {\n+        continue;\n+      }\n+    }\n+\n+    \/\/ offset in the array oop\n+    int offset = (int)(addr - cast_from_oop<address>(array));\n+    JvmtiHeapwalkObject elem(o.obj(), offset, vk, faklass->layout_kind());\n+\n+    \/\/ report the array reference\n+    if (!CallbackInvoker::report_array_element_reference(o, elem, index)) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n@@ -2580,2 +2944,3 @@\n-inline bool VM_HeapWalkOperation::iterate_over_type_array(oop o) {\n-  Klass* k = o->klass();\n+inline bool VM_HeapWalkOperation::iterate_over_type_array(const JvmtiHeapwalkObject& o) {\n+  assert(!o.is_flat(), \"Array object cannot be flattened\");\n+  Klass* k = o.klass();\n@@ -2615,1 +2980,3 @@\n-inline bool VM_HeapWalkOperation::iterate_over_class(oop java_class) {\n+inline bool VM_HeapWalkOperation::iterate_over_class(const JvmtiHeapwalkObject& o) {\n+  assert(!o.is_flat(), \"Klass object cannot be flattened\");\n+  Klass* klass = java_lang_Class::as_Klass(o.obj());\n@@ -2617,1 +2984,0 @@\n-  Klass* klass = java_lang_Class::as_Klass(java_class);\n@@ -2628,1 +2994,2 @@\n-    oop mirror = klass->java_mirror();\n+    oop mirror_oop = klass->java_mirror();\n+    JvmtiHeapwalkObject mirror(mirror_oop);\n@@ -2717,2 +3084,2 @@\n-        oop fld_o = mirror->obj_field(field->field_offset());\n-        assert(verify_static_oop(ik, mirror, field->field_offset()), \"sanity check\");\n+        oop fld_o = mirror_oop->obj_field(field->field_offset());\n+        assert(verify_static_oop(ik, mirror_oop, field->field_offset()), \"sanity check\");\n@@ -2728,1 +3095,1 @@\n-           address addr = cast_from_oop<address>(mirror) + field->field_offset();\n+           address addr = cast_from_oop<address>(mirror_oop) + field->field_offset();\n@@ -2748,1 +3115,1 @@\n-inline bool VM_HeapWalkOperation::iterate_over_object(oop o) {\n+inline bool VM_HeapWalkOperation::iterate_over_object(const JvmtiHeapwalkObject& o) {\n@@ -2750,1 +3117,1 @@\n-  if (!CallbackInvoker::report_class_reference(o, o->klass()->java_mirror())) {\n+  if (!CallbackInvoker::report_class_reference(o, o.klass()->java_mirror())) {\n@@ -2755,1 +3122,1 @@\n-  ClassFieldMap* field_map = JvmtiCachedClassFieldMap::get_map_of_instance_fields(o);\n+  ClassFieldMap* field_map = JvmtiCachedClassFieldMap::get_map_of_instance_fields(o.klass());\n@@ -2759,0 +3126,6 @@\n+    int slot = field->field_index();\n+    int field_offset = field->field_offset();\n+    if (o.is_flat()) {\n+      \/\/ the object is inlined, its fields are stored without the header\n+      field_offset += o.offset() - o.inline_klass()->payload_offset();\n+    }\n@@ -2760,7 +3133,10 @@\n-      oop fld_o = o->obj_field_access<AS_NO_KEEPALIVE | ON_UNKNOWN_OOP_REF>(field->field_offset());\n-      \/\/ ignore any objects that aren't visible to profiler\n-      if (fld_o != nullptr) {\n-        assert(Universe::heap()->is_in(fld_o), \"unsafe code should not \"\n-               \"have references to Klass* anymore\");\n-        int slot = field->field_index();\n-        if (!CallbackInvoker::report_field_reference(o, fld_o, slot)) {\n+      if (field->is_flat()) {\n+        \/\/ check for possible nulls\n+        if (LayoutKindHelper::is_nullable_flat(field->layout_kind())) {\n+          address payload = cast_from_oop<address>(o.obj()) + field_offset;\n+          if (field->inline_klass()->is_payload_marked_as_null(payload)) {\n+            continue;\n+          }\n+        }\n+        JvmtiHeapwalkObject field_obj(o.obj(), field_offset, field->inline_klass(), field->layout_kind());\n+        if (!CallbackInvoker::report_field_reference(o, field_obj, slot)) {\n@@ -2769,0 +3145,9 @@\n+      } else {\n+        oop fld_o = o.obj()->obj_field_access<AS_NO_KEEPALIVE | ON_UNKNOWN_OOP_REF>(field_offset);\n+        \/\/ ignore any objects that aren't visible to profiler\n+        if (fld_o != nullptr) {\n+          assert(Universe::heap()->is_in(fld_o), \"unsafe code should not have references to Klass* anymore\");\n+          if (!CallbackInvoker::report_field_reference(o, fld_o, slot)) {\n+            return false;\n+          }\n+        }\n@@ -2773,2 +3158,1 @@\n-        address addr = cast_from_oop<address>(o) + field->field_offset();\n-        int slot = field->field_index();\n+        address addr = cast_from_oop<address>(o.obj()) + field_offset;\n@@ -2784,1 +3168,1 @@\n-      o->klass() == vmClasses::String_klass()) {\n+      o.klass() == vmClasses::String_klass()) {\n@@ -2853,1 +3237,1 @@\n-    blk->set_context(tag_for(_tag_map, threadObj), java_lang_Thread::thread_id(threadObj), 0, (jmethodID)nullptr);\n+    blk->set_context(_tag_map->find(threadObj), java_lang_Thread::thread_id(threadObj), 0, (jmethodID)nullptr);\n@@ -2953,1 +3337,1 @@\n-bool VM_HeapWalkOperation::visit(oop o) {\n+bool VM_HeapWalkOperation::visit(const JvmtiHeapwalkObject& o) {\n@@ -2955,2 +3339,2 @@\n-  assert(!_bitset.is_marked(o), \"can't visit same object more than once\");\n-  _bitset.mark_obj(o);\n+  assert(!visit_stack()->is_visited(o), \"can't visit same object more than once\");\n+  visit_stack()->mark_visited(o);\n@@ -2958,0 +3342,1 @@\n+  Klass* klass = o.klass();\n@@ -2959,3 +3344,4 @@\n-  if (o->is_instance()) {\n-    if (o->klass() == vmClasses::Class_klass()) {\n-      if (!java_lang_Class::is_primitive(o)) {\n+  if (klass->is_instance_klass()) {\n+    if (klass == vmClasses::Class_klass()) {\n+      assert(!o.is_flat(), \"Class object cannot be flattened\");\n+      if (!java_lang_Class::is_primitive(o.obj())) {\n@@ -2968,2 +3354,3 @@\n-      if (initial_object().is_null() && java_lang_VirtualThread::is_subclass(o->klass())) {\n-        if (!collect_vthread_stack_refs(o)) {\n+      if (initial_object().is_null() && java_lang_VirtualThread::is_subclass(klass)) {\n+        assert(!o.is_flat(), \"VirtualThread object cannot be flattened\");\n+        if (!collect_vthread_stack_refs(o.obj())) {\n@@ -2977,0 +3364,5 @@\n+  \/\/ flat object array\n+  if (klass->is_flatArray_klass()) {\n+      return iterate_over_flat_array(o);\n+  }\n+\n@@ -2978,1 +3370,1 @@\n-  if (o->is_objArray()) {\n+  if (klass->is_objArray_klass()) {\n@@ -2983,1 +3375,1 @@\n-  if (o->is_typeArray()) {\n+  if (klass->is_typeArray_klass()) {\n@@ -3015,2 +3407,2 @@\n-      oop o = visit_stack()->pop();\n-      if (!_bitset.is_marked(o)) {\n+      const JvmtiHeapwalkObject o = visit_stack()->pop();\n+      if (!visit_stack()->is_visited(o)) {\n@@ -3045,0 +3437,2 @@\n+  convert_flat_object_entries();\n+\n@@ -3067,0 +3461,2 @@\n+  convert_flat_object_entries();\n+\n@@ -3099,0 +3495,2 @@\n+  convert_flat_object_entries();\n+\n","filename":"src\/hotspot\/share\/prims\/jvmtiTagMap.cpp","additions":641,"deletions":243,"binary":false,"changes":884,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -63,0 +64,1 @@\n+#include \"oops\/access.hpp\"\n@@ -65,0 +67,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -90,0 +93,1 @@\n+#include \"runtime\/keepStackGCProcessed.hpp\"\n@@ -2007,0 +2011,103 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return nullptr;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(ih->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return nullptr;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayHandle result_array =\n+      oopFactory::new_objArray_handle(vmClasses::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ih->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+WB_END\n+\n+\/\/ Collect Object oops but not value objects...loaded from heap\n+class CollectObjectOops : public BasicOopIterateClosure {\n+  public:\n+  GrowableArray<Handle>* _array;\n+\n+  CollectObjectOops() {\n+      _array = new GrowableArray<Handle>(128);\n+  }\n+\n+  void add_oop(oop o) {\n+    Handle oh = Handle(Thread::current(), o);\n+    if (oh != nullptr && oh->is_inline_type()) {\n+      oh->oop_iterate(this);\n+    } else {\n+      _array->append(oh);\n+    }\n+  }\n+\n+  template <class T> inline void add_oop(T* p) { add_oop(HeapAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    objArrayHandle result_array =\n+        oopFactory::new_objArray_handle(vmClasses::Object_klass(), _array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < _array->length(); i++) {\n+      result_array->obj_at_put(i, _array->at(i)());\n+    }\n+    return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+  }\n+};\n+\n+\/\/ Collect Object oops but not value objects...loaded from frames\n+class CollectFrameObjectOops : public BasicOopIterateClosure {\n+ public:\n+  CollectObjectOops _collect;\n+\n+  template <class T> inline void add_oop(T* p) { _collect.add_oop(RawAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    return _collect.create_jni_result(env, THREAD);\n+  }\n+};\n+\n+\/\/ Collect Object oops for the given oop, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(thread);\n+  Handle objh(thread, JNIHandles::resolve(thing));\n+  CollectObjectOops collectOops;\n+  objh->oop_iterate(&collectOops);\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\/\/ Collect Object oops for the given frame deep, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  KeepStackGCProcessedMark ksgcpm(THREAD);\n+  ResourceMark rm(THREAD);\n+  CollectFrameObjectOops collectOops;\n+  StackFrameStream sfs(thread, true \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, nullptr, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n@@ -3023,0 +3130,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":113,"deletions":0,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -79,0 +79,2 @@\n+#include <string.h>\n+\n@@ -371,0 +373,12 @@\n+bool Arguments::patching_migrated_classes(const char* property, const char* value) {\n+  if (strncmp(property, MODULE_PROPERTY_PREFIX, MODULE_PROPERTY_PREFIX_LEN) == 0) {\n+    const char* property_suffix = property + MODULE_PROPERTY_PREFIX_LEN;\n+    if (matches_property_suffix(property_suffix, PATCH, PATCH_LEN)) {\n+      if (strcmp(value, \"java.base-valueclasses.jar\")) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -1816,1 +1830,0 @@\n-static unsigned int patch_mod_count = 0;\n@@ -1824,1 +1837,1 @@\n-  if (!CDSConfig::check_vm_args_consistency(patch_mod_javabase, mode_flag_cmd_line)) {\n+  if (!CDSConfig::check_vm_args_consistency(mode_flag_cmd_line)) {\n@@ -1991,0 +2004,4 @@\n+  if (UseAltSubstitutabilityMethod) {\n+    no_shared_spaces(\"Alternate substitutability method doesn't work with CDS yet\");\n+  }\n+\n@@ -2073,1 +2090,1 @@\n-      add_patch_mod_prefix(module_name, module_equal + 1);\n+      add_patch_mod_prefix(module_name, module_equal + 1, false \/* no append *\/, false \/* no cds *\/);\n@@ -2075,3 +2092,0 @@\n-      if (!create_numbered_module_property(\"jdk.module.patch\", patch_mod_tail, patch_mod_count++)) {\n-        return JNI_ENOMEM;\n-      }\n@@ -2085,0 +2099,82 @@\n+\/\/ Temporary system property to disable preview patching and enable the new preview mode\n+\/\/ feature for testing\/development. Once the preview mode feature is finished, the value\n+\/\/ will be always 'true' and this code, and all related dead-code can be removed.\n+#define DISABLE_PREVIEW_PATCHING_DEFAULT false\n+\n+bool Arguments::disable_preview_patching() {\n+  const char* prop = get_property(\"DISABLE_PREVIEW_PATCHING\");\n+  return (prop != nullptr)\n+      ? strncmp(prop, \"true\", strlen(\"true\")) == 0\n+      : DISABLE_PREVIEW_PATCHING_DEFAULT;\n+}\n+\n+\/\/ VALUECLASS_STR must match string used in the build\n+#define VALUECLASS_STR \"valueclasses\"\n+#define VALUECLASS_JAR \"-\" VALUECLASS_STR \".jar\"\n+\n+\/\/ Finalize --patch-module args and --enable-preview related to value class module patches.\n+\/\/ Create all numbered properties passing module patches.\n+int Arguments::finalize_patch_module() {\n+  \/\/ If --enable-preview and EnableValhalla is true, modules may have preview mode resources.\n+  bool enable_valhalla_preview = enable_preview() && EnableValhalla;\n+  \/\/ Whether to use module patching, or the new preview mode feature for preview resources.\n+  bool disable_patching = disable_preview_patching();\n+\n+  \/\/ This must be called, even with 'false', to enable resource lookup from JImage.\n+  ClassLoader::init_jimage(disable_patching && enable_valhalla_preview);\n+\n+  \/\/ For each <module>-valueclasses.jar in <JAVA_HOME>\/lib\/valueclasses\/\n+  \/\/ appends the equivalent of --patch-module <module>=<JAVA_HOME>\/lib\/valueclasses\/<module>-valueclasses.jar\n+  if (!disable_patching && enable_valhalla_preview) {\n+    char * valueclasses_dir = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+    const char * fileSep = os::file_separator();\n+\n+    jio_snprintf(valueclasses_dir, JVM_MAXPATHLEN, \"%s%slib%s\" VALUECLASS_STR \"%s\",\n+                 Arguments::get_java_home(), fileSep, fileSep, fileSep);\n+    DIR* dir = os::opendir(valueclasses_dir);\n+    if (dir != nullptr) {\n+      char * module_name = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+      char * path = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+\n+      for (dirent * entry = os::readdir(dir); entry != nullptr; entry = os::readdir(dir)) {\n+        \/\/ Test if file ends-with \"-valueclasses.jar\"\n+        int len = (int)strlen(entry->d_name) - (sizeof(VALUECLASS_JAR) - 1);\n+        if (len <= 0 || strcmp(&entry->d_name[len], VALUECLASS_JAR) != 0) {\n+          continue;         \/\/ too short or not the expected suffix\n+        }\n+\n+        strcpy(module_name, entry->d_name);\n+        module_name[len] = '\\0';     \/\/ truncate to just module-name\n+\n+        jio_snprintf(path, JVM_MAXPATHLEN, \"%s%s\", valueclasses_dir, &entry->d_name);\n+        add_patch_mod_prefix(module_name, path, true \/* append *\/, true \/* cds OK*\/);\n+        log_info(class)(\"--enable-preview appending value classes for module %s: %s\", module_name, entry->d_name);\n+      }\n+      FreeHeap(module_name);\n+      FreeHeap(path);\n+      os::closedir(dir);\n+    }\n+    FreeHeap(valueclasses_dir);\n+  }\n+\n+  \/\/ Create numbered properties for each module that has been patched either\n+  \/\/ by --patch-module (or --enable-preview if disable_patching is false).\n+  \/\/ Format is \"jdk.module.patch.<n>=<module_name>=<path>\"\n+  if (_patch_mod_prefix != nullptr) {\n+    char * prop_value = AllocateHeap(JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, mtArguments);\n+    unsigned int patch_mod_count = 0;\n+\n+    for (GrowableArrayIterator<ModulePatchPath *> it = _patch_mod_prefix->begin();\n+            it != _patch_mod_prefix->end(); ++it) {\n+      jio_snprintf(prop_value, JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, \"%s=%s\",\n+                   (*it)->module_name(), (*it)->path_string());\n+      if (!create_numbered_module_property(\"jdk.module.patch\", prop_value, patch_mod_count++)) {\n+        FreeHeap(prop_value);\n+        return JNI_ENOMEM;\n+      }\n+    }\n+    FreeHeap(prop_value);\n+  }\n+  return JNI_OK;\n+}\n+\n@@ -2366,0 +2462,4 @@\n+      \/\/ --enable-preview enables Valhalla, EnableValhalla VM option will eventually be removed before integration\n+      if (FLAG_SET_CMDLINE(EnableValhalla, true) != JVMFlag::SUCCESS) {\n+        return JNI_EINVAL;\n+      }\n@@ -2872,10 +2972,5 @@\n-void Arguments::add_patch_mod_prefix(const char* module_name, const char* path) {\n-  \/\/ For java.base check for duplicate --patch-module options being specified on the command line.\n-  \/\/ This check is only required for java.base, all other duplicate module specifications\n-  \/\/ will be checked during module system initialization.  The module system initialization\n-  \/\/ will throw an ExceptionInInitializerError if this situation occurs.\n-  if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n-    if (patch_mod_javabase) {\n-      vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n-    } else {\n-      patch_mod_javabase = true;\n+void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool allow_append, bool allow_cds) {\n+  if (!allow_cds) {\n+    CDSConfig::set_module_patching_disables_cds();\n+    if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+      CDSConfig::set_java_base_module_patching_disables_cds();\n@@ -2890,1 +2985,18 @@\n-  _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  \/\/ Scan patches for matching module\n+  int i = _patch_mod_prefix->find_if([&](ModulePatchPath* patch) {\n+    return (strcmp(module_name, patch->module_name()) == 0);\n+  });\n+  if (i == -1) {\n+    _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  } else {\n+    if (allow_append) {\n+      \/\/ append path to existing module entry\n+      _patch_mod_prefix->at(i)->append_path(path);\n+    } else {\n+      if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+        vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n+      } else {\n+        vm_exit_during_initialization(\"Cannot specify a module more than once to --patch-module\", module_name);\n+      }\n+    }\n+  }\n@@ -3003,1 +3115,2 @@\n-  if (!check_vm_args_consistency()) {\n+  \/\/ finalize --module-patch and related --enable-preview\n+  if (finalize_patch_module() != JNI_OK) {\n@@ -3007,0 +3120,3 @@\n+  if (!check_vm_args_consistency()) {\n+    return JNI_ERR;\n+  }\n@@ -3892,0 +4008,12 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !CDSConfig::is_dumping_archive() && !UseSharedSpaces)) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported.\n+    \/\/ Also these aren't useful in -Xint. However, don't disable them when dumping or using\n+    \/\/ the CDS archive, as the values must match between dumptime and runtime.\n+    FLAG_SET_DEFAULT(InlineTypePassFieldsAsArgs, false);\n+    FLAG_SET_DEFAULT(InlineTypeReturnedAsFields, false);\n+  }\n+  if (!UseNonAtomicValueFlattening && !UseNullableValueFlattening && !UseAtomicValueFlattening) {\n+    \/\/ Flattening is disabled\n+    FLAG_SET_DEFAULT(UseArrayFlattening, false);\n+    FLAG_SET_DEFAULT(UseFieldFlattening, false);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":146,"deletions":18,"binary":false,"changes":164,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -46,1 +47,2 @@\n-  return is_final() && (is_static() || ik->is_hidden() || ik->is_record());\n+  return is_final() && (is_static() || ik->is_hidden() || ik->is_record() || ik->is_inline_klass()\n+                        || (ik->is_abstract() && !ik->is_identity_class() && !ik->is_interface()));\n@@ -111,1 +113,1 @@\n-void fieldDescriptor::print_on(outputStream* st) const {\n+void fieldDescriptor::print_on(outputStream* st, int base_offset) const {\n@@ -117,1 +119,1 @@\n-  st->print(\" @%d \", offset());\n+  st->print(\" @%d \", offset() + base_offset);\n@@ -135,4 +137,1 @@\n-void fieldDescriptor::print_on_for(outputStream* st, oop obj) {\n-  print_on(st);\n-  st->print(\" \");\n-\n+void fieldDescriptor::print_on_for(outputStream* st, oop obj, int indent, int base_offset) {\n@@ -140,0 +139,3 @@\n+  print_on(st, base_offset);\n+  st->print(\" \");\n+  jint as_int = 0;\n@@ -169,6 +171,27 @@\n-      if (obj->obj_field(offset()) != nullptr) {\n-        obj->obj_field(offset())->print_value_on(st);\n-      } else {\n-        st->print(\"null\");\n-      }\n-      break;\n+      if (is_flat()) { \/\/ only some inline types can be flat\n+        InlineKlass* vk = InlineKlass::cast(field_holder()->get_inline_type_field_klass(index()));\n+        st->print(\"Flat inline type field '%s':\", vk->name()->as_C_string());\n+        if (!is_null_free_inline_type()) {\n+          assert(has_null_marker(), \"should have null marker\");\n+          InlineLayoutInfo* li = field_holder()->inline_layout_info_adr(index());\n+          int nm_offset = li->null_marker_offset();\n+          if (obj->byte_field_acquire(nm_offset) == 0) {\n+            st->print(\" null\");\n+            return;\n+          }\n+        }\n+        st->cr();\n+        \/\/ Print fields of flat field (recursively)\n+        int field_offset = offset() - vk->payload_offset();\n+        obj = cast_to_oop(cast_from_oop<address>(obj) + field_offset);\n+        FieldPrinter print_field(st, obj, indent + 1, base_offset + field_offset );\n+        vk->do_nonstatic_fields(&print_field);\n+        if (this->field_flags().has_null_marker()) {\n+          for (int i = 0; i < indent + 1; i++) st->print(\"  \");\n+          st->print_cr(\" - [null_marker] @%d %s\",\n+                    vk->null_marker_offset() - base_offset + field_offset,\n+                    obj->bool_field(vk->null_marker_offset()) ? \"Field marked as non-null\" : \"Field marked as null\");\n+        }\n+        return; \/\/ Do not print underlying representation\n+      }\n+      \/\/ Not flat inline type field, fall through\n","filename":"src\/hotspot\/share\/runtime\/fieldDescriptor.cpp","additions":36,"deletions":13,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+  LayoutKind layout_kind()        const    { return _fieldinfo.layout_kind(); }\n@@ -77,0 +78,3 @@\n+  \/\/ Unset strict static\n+  inline bool is_strict_static_unset()   const;\n+\n@@ -90,0 +94,4 @@\n+  bool is_strict()                const    { return access_flags().is_strict(); }\n+  inline bool is_flat()           const;\n+  inline bool is_null_free_inline_type() const;\n+  inline bool has_null_marker()   const;\n@@ -112,2 +120,2 @@\n-  void print_on(outputStream* st) const;\n-  void print_on_for(outputStream* st, oop obj);\n+  void print_on(outputStream* st, int base_offset = 0) const;\n+  void print_on_for(outputStream* st, oop obj, int indent = 0, int base_offset = 0);\n","filename":"src\/hotspot\/share\/runtime\/fieldDescriptor.hpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -112,0 +112,1 @@\n+  template(ClassPrintLayout)                      \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -61,0 +61,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -941,1 +943,3 @@\n-           declare_type(ObjArrayKlass, ArrayKlass)                        \\\n+           declare_type(ObjArrayKlass, ArrayKlass)                        \\\n+             declare_type(FlatArrayKlass, ArrayKlass)                     \\\n+             declare_type(RefArrayKlass, ArrayKlass)                      \\\n@@ -944,0 +948,1 @@\n+        declare_type(InlineKlass, InstanceKlass)                          \\\n@@ -1416,1 +1421,1 @@\n-  declare_constant(Klass::_lh_array_tag_obj_value)                        \\\n+  declare_constant(Klass::_lh_array_tag_ref_value)                        \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -613,0 +613,9 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Prototyping\n+\/\/ \"Code Missing Here\" macro, un-define when integrating back from prototyping stage and break\n+\/\/ compilation on purpose (i.e. \"forget me not\")\n+#define PROTOTYPE\n+#ifdef PROTOTYPE\n+#define CMH(m)\n+#endif\n+\n@@ -691,5 +700,6 @@\n-  T_ADDRESS     = 15,\n-  T_NARROWOOP   = 16,\n-  T_METADATA    = 17,\n-  T_NARROWKLASS = 18,\n-  T_CONFLICT    = 19, \/\/ for stack value type with conflicting contents\n+  T_FLAT_ELEMENT = 15, \/\/ Not a true BasicType, only used in layout helpers of flat arrays\n+  T_ADDRESS     = 16,\n+  T_NARROWOOP   = 17,\n+  T_METADATA    = 18,\n+  T_NARROWKLASS = 19,\n+  T_CONFLICT    = 20, \/\/ for stack value type with conflicting contents\n@@ -739,0 +749,1 @@\n+  assert(t != T_FLAT_ELEMENT, \"\");  \/\/ Strong assert to detect misuses of T_FLAT_ELEMENT\n@@ -813,1 +824,2 @@\n-  T_VOID_size        = 0\n+  T_VOID_size        = 0,\n+  T_FLAT_ELEMENT_size = 0\n@@ -849,1 +861,2 @@\n-  T_VOID_aelem_bytes        = 0\n+  T_VOID_aelem_bytes        = 0,\n+  T_FLAT_ELEMENT_aelem_bytes = 0\n@@ -939,1 +952,1 @@\n-  vtos = 9,             \/\/ tos not cached\n+  vtos = 9,             \/\/ tos not cached,\n@@ -956,1 +969,1 @@\n-    case T_ARRAY  : \/\/ fall through\n+    case T_ARRAY  :   \/\/ fall through\n@@ -1034,0 +1047,1 @@\n+const juint    badRegWordVal      = 0xDEADDA7A;             \/\/ value used to zap registers\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":23,"deletions":9,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+import jdk.internal.misc.PreviewFeatures;\n+import jdk.internal.value.DeserializeConstructor;\n@@ -171,4 +173,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Character} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -184,0 +194,1 @@\n+@jdk.internal.MigratedValueClass\n@@ -185,2 +196,1 @@\n-public final\n-class Character implements java.io.Serializable, Comparable<Character>, Constable {\n+public final class Character implements java.io.Serializable, Comparable<Character>, Constable {\n@@ -9408,9 +9418,20 @@\n-     * If a new {@code Character} instance is not required, this method\n-     * should generally be used in preference to the constructor\n-     * {@link #Character(char)}, as this method is likely to yield\n-     * significantly better space and time performance by caching\n-     * frequently requested values.\n-     *\n-     * This method will always cache values in the range {@code\n-     * '\\u005Cu0000'} to {@code '\\u005Cu007F'}, inclusive, and may\n-     * cache other values outside of this range.\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          <p>\n+     *              - When preview features are NOT enabled, {@code Character} is an identity class.\n+     *              If a new {@code Character} instance is not required, this method\n+     *              should generally be used in preference to the constructor\n+     *              {@link #Character(char)}, as this method is likely to yield\n+     *              significantly better space and time performance by caching\n+     *              frequently requested values.\n+     *              This method will always cache values in the range {@code\n+     *              '\\u005Cu0000'} to {@code '\\u005Cu007F'}, inclusive, and may\n+     *              cache other values outside of this range.\n+     *          <\/p>\n+     *          <p>\n+     *             - When preview features are enabled, {@code Character} is a {@linkplain Class#isValue value class}.\n+     *              The {@code valueOf} behavior is the same as invoking the constructor,\n+     *              whether cached or not.\n+     *          <\/p>\n+     *      <\/div>\n+     * <\/div>\n@@ -9423,0 +9444,1 @@\n+    @DeserializeConstructor\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Character.java","additions":37,"deletions":15,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+import jdk.internal.misc.PreviewFeatures;\n@@ -31,0 +32,1 @@\n+import jdk.internal.value.DeserializeConstructor;\n@@ -57,4 +59,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Integer} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -71,0 +81,1 @@\n+@jdk.internal.MigratedValueClass\n@@ -951,8 +962,20 @@\n-     * {@code int} value.  If a new {@code Integer} instance is not\n-     * required, this method should generally be used in preference to\n-     * the constructor {@link #Integer(int)}, as this method is likely\n-     * to yield significantly better space and time performance by\n-     * caching frequently requested values.\n-     *\n-     * This method will always cache values in the range -128 to 127,\n-     * inclusive, and may cache other values outside of this range.\n+     * {@code int} value.\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          <p>\n+     *              - When preview features are NOT enabled, {@code Integer} is an identity class.\n+     *              If a new {@code Integer} instance is not\n+     *              required, this method should generally be used in preference to\n+     *              the constructor {@link #Integer(int)}, as this method is likely\n+     *              to yield significantly better space and time performance by\n+     *              caching frequently requested values.\n+     *              This method will always cache values in the range -128 to 127,\n+     *              inclusive, and may cache other values outside of this range.\n+     *          <\/p>\n+     *          <p>\n+     *              - When preview features are enabled, {@code Integer} is a {@linkplain Class#isValue value class}.\n+     *              The {@code valueOf} behavior is the same as invoking the constructor,\n+     *              whether cached or not.\n+     *          <\/p>\n+     *      <\/div>\n+     * <\/div>\n@@ -965,0 +988,1 @@\n+    @DeserializeConstructor\n@@ -966,2 +990,4 @@\n-        if (i >= IntegerCache.low && i <= IntegerCache.high)\n-            return IntegerCache.cache[i + (-IntegerCache.low)];\n+        if (!PreviewFeatures.isEnabled()) {\n+            if (i >= IntegerCache.low && i <= IntegerCache.high)\n+                return IntegerCache.cache[i + (-IntegerCache.low)];\n+        }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Integer.java","additions":40,"deletions":14,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -37,0 +37,2 @@\n+import jdk.internal.misc.PreviewFeatures;\n+import jdk.internal.value.DeserializeConstructor;\n@@ -57,4 +59,13 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Long} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n+ *\n@@ -71,0 +82,1 @@\n+@jdk.internal.MigratedValueClass\n@@ -942,8 +954,19 @@\n-     * If a new {@code Long} instance is not required, this method\n-     * should generally be used in preference to the constructor\n-     * {@link #Long(long)}, as this method is likely to yield\n-     * significantly better space and time performance by caching\n-     * frequently requested values.\n-     *\n-     * This method will always cache values in the range -128 to 127,\n-     * inclusive, and may cache other values outside of this range.\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          <p>\n+     *              - When preview features are NOT enabled, {@code Long} is an identity class.\n+     *              If a new {@code Long} instance is not required, this method\n+     *              should generally be used in preference to the constructor\n+     *              {@link #Long(long)}, as this method is likely to yield\n+     *              significantly better space and time performance by caching\n+     *              frequently requested values.\n+     *              This method will always cache values in the range -128 to 127,\n+     *              inclusive, and may cache other values outside of this range.\n+     *          <\/p>\n+     *          <p>\n+     *              - When preview features are enabled, {@code Long} is a {@linkplain Class#isValue value class}.\n+     *              The {@code valueOf} behavior is the same as invoking the constructor,\n+     *              whether cached or not.\n+     *          <\/p>\n+     *      <\/div>\n+     * <\/div>\n@@ -956,0 +979,1 @@\n+    @DeserializeConstructor\n@@ -957,3 +981,5 @@\n-        final int offset = 128;\n-        if (l >= -128 && l <= 127) { \/\/ will cache\n-            return LongCache.cache[(int)l + offset];\n+        if (!PreviewFeatures.isEnabled()) {\n+            if (l >= -128 && l <= 127) { \/\/ will cache\n+                final int offset = 128;\n+                return LongCache.cache[(int) l + offset];\n+            }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Long.java","additions":41,"deletions":15,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+import java.lang.reflect.ClassFileFormatVersion;\n@@ -490,0 +491,15 @@\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          The \"identity hash code\" of a {@linkplain Class#isValue() value object}\n+     *          is computed by combining the identity hash codes of the value object's fields recursively.\n+     *      <\/div>\n+     * <\/div>\n+     * @apiNote\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          Note that, like ==, this hash code exposes information about a value object's\n+     *          private fields that might otherwise be hidden by an identity object.\n+     *          Developers should be cautious about storing sensitive secrets in value object fields.\n+     *      <\/div>\n+     * <\/div>\n+     *\n@@ -2328,0 +2344,4 @@\n+            public int classFileFormatVersion(Class<?> clazz) {\n+                return clazz.getClassFileVersion();\n+            }\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/System.java","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2666,0 +2666,2 @@\n+         *\n+         *\n@@ -2679,0 +2681,3 @@\n+            if (type.returnType() != void.class) {\n+                throw new NoSuchMethodException(\"Constructors must have void return type: \" + refc.getName());\n+            }\n@@ -3851,1 +3856,0 @@\n-\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandles.java","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+import jdk.internal.value.ValueClass;\n+import jdk.internal.vm.annotation.LooselyConsistentValue;\n@@ -54,3 +56,30 @@\n-                return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n-                       ? new VarHandleReferences.FieldInstanceReadOnly(refc, foffset, type)\n-                       : new VarHandleReferences.FieldInstanceReadWrite(refc, foffset, type));\n+                if (type.isValue()) {\n+                    int layout = f.getLayout();\n+                    boolean isAtomic = isAtomicFlat(f);\n+                    boolean isFlat = f.isFlat();\n+                    if (isFlat) {\n+                        if (isAtomic) {\n+                            return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n+                                    ? new VarHandleFlatValues.FieldInstanceReadOnly(refc, foffset, type, f.isNullRestricted(), layout)\n+                                    : new VarHandleFlatValues.FieldInstanceReadWrite(refc, foffset, type, f.isNullRestricted(), layout));\n+                        } else {\n+                            return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n+                                    ? new VarHandleNonAtomicFlatValues.FieldInstanceReadOnly(refc, foffset, type, f.isNullRestricted(), layout)\n+                                    : new VarHandleNonAtomicFlatValues.FieldInstanceReadWrite(refc, foffset, type, f.isNullRestricted(), layout));\n+                        }\n+                    } else {\n+                        if (isAtomic) {\n+                            return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n+                                    ? new VarHandleReferences.FieldInstanceReadOnly(refc, foffset, type, f.isNullRestricted())\n+                                    : new VarHandleReferences.FieldInstanceReadWrite(refc, foffset, type, f.isNullRestricted()));\n+                        } else {\n+                            return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n+                                    ? new VarHandleNonAtomicReferences.FieldInstanceReadOnly(refc, foffset, type, f.isNullRestricted())\n+                                    : new VarHandleNonAtomicReferences.FieldInstanceReadWrite(refc, foffset, type, f.isNullRestricted()));\n+                        }\n+                    }\n+                } else {\n+                    return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n+                       ? new VarHandleReferences.FieldInstanceReadOnly(refc, foffset, type, f.isNullRestricted())\n+                       : new VarHandleReferences.FieldInstanceReadWrite(refc, foffset, type, f.isNullRestricted()));\n+                }\n@@ -116,3 +145,16 @@\n-            return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n-                    ? new VarHandleReferences.FieldStaticReadOnly(decl, base, foffset, type)\n-                    : new VarHandleReferences.FieldStaticReadWrite(decl, base, foffset, type));\n+            assert !f.isFlat() : (\"static field is flat in \" + decl + \".\" + f.getName());\n+            if (type.isValue()) {\n+                if (isAtomicFlat(f)) {\n+                    return f.isFinal() && !isWriteAllowedOnFinalFields\n+                            ? new VarHandleReferences.FieldStaticReadOnly(decl, base, foffset, type, f.isNullRestricted())\n+                            : new VarHandleReferences.FieldStaticReadWrite(decl, base, foffset, type, f.isNullRestricted());\n+                } else {\n+                    return maybeAdapt(f.isFinal() && !isWriteAllowedOnFinalFields\n+                            ? new VarHandleNonAtomicReferences.FieldStaticReadOnly(decl, base, foffset, type, f.isNullRestricted())\n+                            : new VarHandleNonAtomicReferences.FieldStaticReadWrite(decl, base, foffset, type, f.isNullRestricted()));\n+                }\n+            } else {\n+                return f.isFinal() && !isWriteAllowedOnFinalFields\n+                        ? new VarHandleReferences.FieldStaticReadOnly(decl, base, foffset, type, f.isNullRestricted())\n+                        : new VarHandleReferences.FieldStaticReadWrite(decl, base, foffset, type, f.isNullRestricted());\n+            }\n@@ -165,0 +207,30 @@\n+    static boolean isAtomicFlat(MemberName field) {\n+        boolean hasAtomicAccess = (field.getModifiers() & Modifier.VOLATILE) != 0 ||\n+                !(field.isNullRestricted()) ||\n+                !field.getFieldType().isAnnotationPresent(LooselyConsistentValue.class);\n+        return hasAtomicAccess && !HAS_OOPS.get(field.getFieldType());\n+    }\n+\n+    static boolean isAtomicFlat(Object[] array) {\n+        Class<?> componentType = array.getClass().componentType();\n+        boolean hasAtomicAccess = ValueClass.isAtomicArray(array) ||\n+                !ValueClass.isNullRestrictedArray(array) ||\n+                !componentType.isAnnotationPresent(LooselyConsistentValue.class);\n+        return hasAtomicAccess && !HAS_OOPS.get(componentType);\n+    }\n+\n+    static final ClassValue<Boolean> HAS_OOPS = new ClassValue<>() {\n+        @Override\n+        protected Boolean computeValue(Class<?> c) {\n+            for (Field f : c.getDeclaredFields()) {\n+                Class<?> ftype = f.getType();\n+                if (UNSAFE.isFlatField(f) && HAS_OOPS.get(ftype)) {\n+                    return true;\n+                } else if (!ftype.isPrimitive()) {\n+                    return true;\n+                }\n+            }\n+            return false;\n+        }\n+    };\n+\n@@ -198,0 +270,7 @@\n+    \/\/ This is invoked by non-flat array var handle code when attempting to access a flat array\n+    public static void checkAtomicFlatArray(Object[] array) {\n+        if (!isAtomicFlat(array)) {\n+            throw new IllegalArgumentException(\"Attempt to perform a non-plain access on a non-atomic array\");\n+        }\n+    }\n+\n@@ -203,6 +282,5 @@\n-\n-        int aoffset = (int) UNSAFE.arrayBaseOffset(arrayClass);\n-        int ascale = UNSAFE.arrayIndexScale(arrayClass);\n-        int ashift = 31 - Integer.numberOfLeadingZeros(ascale);\n-\n-            return maybeAdapt(new VarHandleReferences.Array(aoffset, ashift, arrayClass));\n+            \/\/ Here we always return a reference array element var handle. This is because\n+            \/\/ the access semantics is determined at runtime, when an actual array object is passed\n+            \/\/ to the var handle. The var handle implementation will switch to use flat access\n+            \/\/ primitives if it sees a flat array.\n+            return maybeAdapt(new ArrayVarHandle(arrayClass));\n@@ -212,1 +290,1 @@\n-            return maybeAdapt(new VarHandleBooleans.Array(aoffset, ashift));\n+            return maybeAdapt(VarHandleBooleans.Array.NON_EXACT_INSTANCE);\n@@ -215,1 +293,1 @@\n-            return maybeAdapt(new VarHandleBytes.Array(aoffset, ashift));\n+            return maybeAdapt(VarHandleBytes.Array.NON_EXACT_INSTANCE);\n@@ -218,1 +296,1 @@\n-            return maybeAdapt(new VarHandleShorts.Array(aoffset, ashift));\n+            return maybeAdapt(VarHandleShorts.Array.NON_EXACT_INSTANCE);\n@@ -221,1 +299,1 @@\n-            return maybeAdapt(new VarHandleChars.Array(aoffset, ashift));\n+            return maybeAdapt(VarHandleChars.Array.NON_EXACT_INSTANCE);\n@@ -224,1 +302,1 @@\n-            return maybeAdapt(new VarHandleInts.Array(aoffset, ashift));\n+            return maybeAdapt(VarHandleInts.Array.NON_EXACT_INSTANCE);\n@@ -227,1 +305,1 @@\n-            return maybeAdapt(new VarHandleLongs.Array(aoffset, ashift));\n+            return maybeAdapt(VarHandleLongs.Array.NON_EXACT_INSTANCE);\n@@ -230,1 +308,1 @@\n-            return maybeAdapt(new VarHandleFloats.Array(aoffset, ashift));\n+            return maybeAdapt(VarHandleFloats.Array.NON_EXACT_INSTANCE);\n@@ -233,1 +311,1 @@\n-            return maybeAdapt(new VarHandleDoubles.Array(aoffset, ashift));\n+            return maybeAdapt(VarHandleDoubles.Array.NON_EXACT_INSTANCE);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/VarHandles.java","additions":98,"deletions":20,"binary":false,"changes":118,"status":"modified"},{"patch":"@@ -221,0 +221,1 @@\n+     * The {@code AccessFlags} may depend on the class file format version of the class.\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/Executable.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -83,1 +83,1 @@\n-    private final boolean             trustedFinal;\n+    private final int                 flags;\n@@ -133,1 +133,1 @@\n-          boolean trustedFinal,\n+          int flags,\n@@ -142,1 +142,1 @@\n-        this.trustedFinal = trustedFinal;\n+        this.flags = flags;\n@@ -164,1 +164,1 @@\n-        Field res = new Field(clazz, name, type, modifiers, trustedFinal, slot, signature, annotations);\n+        Field res = new Field(clazz, name, type, modifiers, flags, slot, signature, annotations);\n@@ -243,0 +243,2 @@\n+     * The {@code AccessFlags} may depend on the class file format version of the class.\n+     *\n@@ -813,0 +815,1 @@\n+     * <li>{@code D} is not a {@linkplain Class#isValue() value class}.<\/li>\n@@ -1353,0 +1356,3 @@\n+    private static final int TRUST_FINAL     = 0x0010;\n+    private static final int NULL_RESTRICTED = 0x0020;\n+\n@@ -1354,1 +1360,5 @@\n-        return trustedFinal;\n+        return (flags & TRUST_FINAL) == TRUST_FINAL;\n+    }\n+\n+    \/* package-private *\/ boolean isNullRestricted() {\n+        return (flags & NULL_RESTRICTED) == NULL_RESTRICTED;\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/Field.java","additions":15,"deletions":5,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -76,0 +76,4 @@\n+    public boolean isNullRestrictedField(Field f) {\n+        return f.isNullRestricted();\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/ReflectAccess.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -746,1 +746,1 @@\n-                    clb.withFlags(AccessFlag.FINAL, AccessFlag.SUPER, AccessFlag.SYNTHETIC)\n+                    clb.withFlags(AccessFlag.FINAL, (PreviewFeatures.isEnabled())  ? AccessFlag.IDENTITY : AccessFlag.SUPER, AccessFlag.SYNTHETIC)\n","filename":"src\/java.base\/share\/classes\/java\/lang\/runtime\/SwitchBootstraps.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -132,5 +132,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n- * The {@code equals} method should be used for comparisons.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code LocalDate} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -144,0 +151,1 @@\n+@jdk.internal.MigratedValueClass\n","filename":"src\/java.base\/share\/classes\/java\/time\/LocalDate.java","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -118,5 +118,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n- * The {@code equals} method should be used for comparisons.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code MonthDay} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -130,0 +137,1 @@\n+@jdk.internal.MigratedValueClass\n","filename":"src\/java.base\/share\/classes\/java\/time\/MonthDay.java","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -122,5 +122,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n- * The {@code equals} method should be used for comparisons.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code YearMonth} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -134,0 +141,1 @@\n+@jdk.internal.MigratedValueClass\n","filename":"src\/java.base\/share\/classes\/java\/time\/YearMonth.java","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -108,5 +108,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n- * The {@code equals} method should be used for comparisons.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code HijrahDate} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -120,0 +127,1 @@\n+@jdk.internal.MigratedValueClass\n","filename":"src\/java.base\/share\/classes\/java\/time\/chrono\/HijrahDate.java","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -49,4 +49,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Optional} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -64,0 +72,1 @@\n+@jdk.internal.MigratedValueClass\n","filename":"src\/java.base\/share\/classes\/java\/util\/Optional.java","additions":13,"deletions":4,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+import java.lang.reflect.ClassFileFormatVersion;\n@@ -632,0 +633,6 @@\n+\n+    \/**\n+     * Returns the class file format version of the class.\n+     *\/\n+    int classFileFormatVersion(Class<?> klass);\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/access\/JavaLangAccess.java","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -67,0 +67,4 @@\n+\n+    \/** Tests if this is a null-restricted field *\/\n+    public boolean isNullRestrictedField(Field f);\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/access\/JavaLangReflectAccess.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -67,0 +67,3 @@\n+        @JEP(number=401, title=\"Value Classes and Objects\", status = \"Preview\")\n+        VALUE_OBJECTS,\n+\n@@ -84,1 +87,0 @@\n-\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/javac\/PreviewFeature.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -138,1 +138,0 @@\n-\n@@ -157,1 +156,2 @@\n-        jdk.compiler;\n+        jdk.compiler,\n+        jdk.jdeps;\n@@ -263,0 +263,2 @@\n+    exports jdk.internal.value to  \/\/ Needed by Unsafe\n+        jdk.unsupported;\n","filename":"src\/java.base\/share\/classes\/module-info.java","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+import java.util.function.BiFunction;\n@@ -124,1 +125,6 @@\n-        noWarnings = new Warner(null);\n+        noWarnings = new Warner(null) {\n+            @Override\n+            public String toString() {\n+                return \"NO_WARNINGS\";\n+            }\n+        };\n@@ -2158,0 +2164,4 @@\n+        return makeArrayType(t, 1);\n+    }\n+\n+    public ArrayType makeArrayType(Type t, int dimensions) {\n@@ -2161,1 +2171,5 @@\n-        return new ArrayType(t, syms.arrayClass);\n+        ArrayType result = new ArrayType(t, syms.arrayClass);\n+        for (int i = 1; i < dimensions; i++) {\n+            result = new ArrayType(result, syms.arrayClass);\n+        }\n+        return result;\n@@ -3953,1 +3967,1 @@\n-                                 class1.tsym);\n+                                 class1.tsym, List.nil());\n@@ -4936,0 +4950,1 @@\n+        private boolean encodeTypeSig;\n@@ -4937,1 +4952,1 @@\n-        public UniqueType(Type type, Types types) {\n+        public UniqueType(Type type, Types types, boolean encodeTypeSig) {\n@@ -4940,0 +4955,5 @@\n+            this.encodeTypeSig = encodeTypeSig;\n+        }\n+\n+        public UniqueType(Type type, Types types) {\n+            this(type, types, true);\n@@ -4951,0 +4971,4 @@\n+        public boolean encodeTypeSig() {\n+            return encodeTypeSig;\n+        }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Types.java","additions":28,"deletions":4,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -127,0 +127,1 @@\n+    final LocalProxyVarsGen localProxyVarsGen;\n@@ -167,0 +168,1 @@\n+        localProxyVarsGen = LocalProxyVarsGen.instance(context);\n@@ -189,0 +191,2 @@\n+        allowValueClasses = (!preview.isPreview(Feature.VALUE_CLASSES) || preview.isEnabled()) &&\n+                Feature.VALUE_CLASSES.allowedInSource(source);\n@@ -207,0 +211,4 @@\n+    \/** Are value classes allowed\n+     *\/\n+    private final boolean allowValueClasses;\n+\n@@ -300,3 +308,1 @@\n-            return;\n-        }\n-        if ((v.flags() & FINAL) != 0 &&\n+        } else if ((v.flags() & FINAL) != 0 &&\n@@ -313,17 +319,0 @@\n-            return;\n-        }\n-\n-        \/\/ Check instance field assignments that appear in constructor prologues\n-        if (rs.isEarlyReference(env, base, v)) {\n-\n-            \/\/ Field may not be inherited from a superclass\n-            if (v.owner != env.enclClass.sym) {\n-                log.error(pos, Errors.CantRefBeforeCtorCalled(v));\n-                return;\n-            }\n-\n-            \/\/ Field may not have an initializer\n-            if ((v.flags() & HASINIT) != 0) {\n-                log.error(pos, Errors.CantAssignInitializedBeforeCtorCalled(v));\n-                return;\n-            }\n@@ -1124,1 +1113,2 @@\n-                            if (TreeInfo.hasAnyConstructorCall(tree)) {\n+                            if ((!allowValueClasses || TreeInfo.isCompactConstructor(tree)) &&\n+                                    TreeInfo.hasAnyConstructorCall(tree)) {\n@@ -1202,0 +1192,1 @@\n+                boolean addedSuperInIdentityClass = false;\n@@ -1206,1 +1197,6 @@\n-                        tree.body.stats = tree.body.stats.prepend(supCall);\n+                        if (allowValueClasses && (owner.isValueClass() || owner.hasStrict() || ((owner.flags_field & RECORD) != 0))) {\n+                            tree.body.stats = tree.body.stats.append(supCall);\n+                        } else {\n+                            tree.body.stats = tree.body.stats.prepend(supCall);\n+                            addedSuperInIdentityClass = true;\n+                        }\n@@ -1244,0 +1240,29 @@\n+                if (localEnv.info.ctorPrologue) {\n+                    boolean thisInvocation = false;\n+                    ListBuffer<JCTree> prologueCode = new ListBuffer<>();\n+                    for (JCTree stat : tree.body.stats) {\n+                        prologueCode.add(stat);\n+                        \/* gather all the stats in the body until a `super` or `this` constructor invocation is found,\n+                         * including the constructor invocation, that way we don't need to worry in the visitor below if\n+                         * if we are dealing or not with prologue code\n+                         *\/\n+                        if (stat instanceof JCExpressionStatement expStmt &&\n+                                expStmt.expr instanceof JCMethodInvocation mi &&\n+                                TreeInfo.isConstructorCall(mi)) {\n+                            thisInvocation = TreeInfo.name(mi.meth) == names._this;\n+                            if (!addedSuperInIdentityClass || !allowValueClasses) {\n+                                break;\n+                            }\n+                        }\n+                    }\n+                    if (!prologueCode.isEmpty()) {\n+                        CtorPrologueVisitor ctorPrologueVisitor = new CtorPrologueVisitor(localEnv,\n+                                addedSuperInIdentityClass && allowValueClasses ?\n+                                        PrologueVisitorMode.WARNINGS_ONLY :\n+                                        thisInvocation ?\n+                                                PrologueVisitorMode.THIS_CONSTRUCTOR :\n+                                                PrologueVisitorMode.SUPER_CONSTRUCTOR,\n+                                false);\n+                        ctorPrologueVisitor.scan(prologueCode.toList());\n+                    }\n+                }\n@@ -1255,0 +1280,338 @@\n+    enum PrologueVisitorMode {\n+        WARNINGS_ONLY,\n+        SUPER_CONSTRUCTOR,\n+        THIS_CONSTRUCTOR\n+    }\n+\n+    class CtorPrologueVisitor extends TreeScanner {\n+        Env<AttrContext> localEnv;\n+        PrologueVisitorMode mode;\n+        boolean isInitializer;\n+\n+        CtorPrologueVisitor(Env<AttrContext> localEnv, PrologueVisitorMode mode, boolean isInitializer) {\n+            this.localEnv = localEnv;\n+            currentClassSym = localEnv.enclClass.sym;\n+            this.mode = mode;\n+            this.isInitializer = isInitializer;\n+        }\n+\n+        boolean insideLambdaOrClassDef = false;\n+\n+        @Override\n+        public void visitLambda(JCLambda lambda) {\n+            boolean previousInsideLambdaOrClassDef = insideLambdaOrClassDef;\n+            try {\n+                insideLambdaOrClassDef = true;\n+                super.visitLambda(lambda);\n+            } finally {\n+                insideLambdaOrClassDef = previousInsideLambdaOrClassDef;\n+            }\n+        }\n+\n+        ClassSymbol currentClassSym;\n+\n+        @Override\n+        public void visitClassDef(JCClassDecl classDecl) {\n+            boolean previousInsideLambdaOrClassDef = insideLambdaOrClassDef;\n+            ClassSymbol previousClassSym = currentClassSym;\n+            try {\n+                insideLambdaOrClassDef = true;\n+                currentClassSym = classDecl.sym;\n+                super.visitClassDef(classDecl);\n+            } finally {\n+                insideLambdaOrClassDef = previousInsideLambdaOrClassDef;\n+                currentClassSym = previousClassSym;\n+            }\n+        }\n+\n+        private void reportPrologueError(JCTree tree, Symbol sym) {\n+            reportPrologueError(tree, sym, false);\n+        }\n+\n+        private void reportPrologueError(JCTree tree, Symbol sym, boolean hasInit) {\n+            preview.checkSourceLevel(tree, Feature.FLEXIBLE_CONSTRUCTORS);\n+            if (mode != PrologueVisitorMode.WARNINGS_ONLY) {\n+                if (hasInit) {\n+                    log.error(tree, Errors.CantAssignInitializedBeforeCtorCalled(sym));\n+                } else {\n+                    log.error(tree, Errors.CantRefBeforeCtorCalled(sym));\n+                }\n+            } else if (allowValueClasses) {\n+                \/\/ issue lint warning\n+                log.warning(tree, LintWarnings.WouldNotBeAllowedInPrologue(sym));\n+            }\n+        }\n+\n+        @Override\n+        public void visitApply(JCMethodInvocation tree) {\n+            super.visitApply(tree);\n+            Name name = TreeInfo.name(tree.meth);\n+            boolean isConstructorCall = name == names._this || name == names._super;\n+            Symbol msym = TreeInfo.symbolFor(tree.meth);\n+            \/\/ is this an instance method call or an illegal constructor invocation like: `this.super()`?\n+            if (msym != null && \/\/ for erroneous invocations msym can be null, ignore those\n+                (!isConstructorCall ||\n+                isConstructorCall && tree.meth.hasTag(SELECT))) {\n+                if (isEarlyReference(localEnv, tree.meth, msym))\n+                    reportPrologueError(tree.meth, msym);\n+            }\n+        }\n+\n+        @Override\n+        public void visitIdent(JCIdent tree) {\n+            analyzeSymbol(tree);\n+        }\n+\n+        boolean isIndexed = false;\n+\n+        @Override\n+        public void visitIndexed(JCArrayAccess tree) {\n+            boolean previousIsIndexed = isIndexed;\n+            try {\n+                isIndexed = true;\n+                scan(tree.indexed);\n+            } finally {\n+                isIndexed = previousIsIndexed;\n+            }\n+            scan(tree.index);\n+            if (mode == PrologueVisitorMode.SUPER_CONSTRUCTOR && isInstanceField(tree.indexed)) {\n+                localProxyVarsGen.addFieldReadInPrologue(localEnv.enclMethod, TreeInfo.symbolFor(tree.indexed));\n+            }\n+        }\n+\n+        @Override\n+        public void visitSelect(JCFieldAccess tree) {\n+            SelectScanner ss = new SelectScanner();\n+            ss.scan(tree);\n+            if (ss.scanLater == null) {\n+                Symbol sym = TreeInfo.symbolFor(tree);\n+                \/\/ if this is a field access\n+                if (sym.kind == VAR && sym.owner.kind == TYP) {\n+                    \/\/ Type.super.field or super.field expressions are forbidden in early construction contexts\n+                    for (JCTree subtree : ss.selectorTrees) {\n+                        if (TreeInfo.isSuperOrSelectorDotSuper(subtree)) {\n+                            reportPrologueError(tree, sym);\n+                            return;\n+                        }\n+                    }\n+                }\n+                analyzeSymbol(tree);\n+            } else {\n+                boolean prevLhs = isInLHS;\n+                try {\n+                    isInLHS = false;\n+                    scan(ss.scanLater);\n+                } finally {\n+                    isInLHS = prevLhs;\n+                }\n+            }\n+            if (mode == PrologueVisitorMode.SUPER_CONSTRUCTOR) {\n+                for (JCTree subtree : ss.selectorTrees) {\n+                    if (isInstanceField(subtree)) {\n+                        \/\/ we need to add a proxy for this one\n+                        localProxyVarsGen.addFieldReadInPrologue(localEnv.enclMethod, TreeInfo.symbolFor(subtree));\n+                    }\n+                }\n+            }\n+        }\n+\n+        boolean isInstanceField(JCTree tree) {\n+            Symbol sym = TreeInfo.symbolFor(tree);\n+            return (sym != null &&\n+                    !sym.isStatic() &&\n+                    sym.kind == VAR &&\n+                    sym.owner.kind == TYP &&\n+                    sym.name != names._this &&\n+                    sym.name != names._super &&\n+                    isEarlyReference(localEnv, tree, sym));\n+        }\n+\n+        @Override\n+        public void visitNewClass(JCNewClass tree) {\n+            super.visitNewClass(tree);\n+            checkNewClassAndMethRefs(tree, tree.type);\n+        }\n+\n+        @Override\n+        public void visitReference(JCMemberReference tree) {\n+            super.visitReference(tree);\n+            if (tree.getMode() == JCMemberReference.ReferenceMode.NEW) {\n+                checkNewClassAndMethRefs(tree, tree.expr.type);\n+            }\n+        }\n+\n+        void checkNewClassAndMethRefs(JCTree tree, Type t) {\n+            if (t.tsym.isEnclosedBy(localEnv.enclClass.sym) &&\n+                    !t.tsym.isStatic() &&\n+                    !t.tsym.isDirectlyOrIndirectlyLocal()) {\n+                reportPrologueError(tree, t.getEnclosingType().tsym);\n+            }\n+        }\n+\n+        \/* if a symbol is in the LHS of an assignment expression we won't consider it as a candidate\n+         * for a proxy local variable later on\n+         *\/\n+        boolean isInLHS = false;\n+\n+        @Override\n+        public void visitAssign(JCAssign tree) {\n+            boolean previousIsInLHS = isInLHS;\n+            try {\n+                isInLHS = true;\n+                scan(tree.lhs);\n+            } finally {\n+                isInLHS = previousIsInLHS;\n+            }\n+            scan(tree.rhs);\n+        }\n+\n+        @Override\n+        public void visitMethodDef(JCMethodDecl tree) {\n+            \/\/ ignore any declarative part, mainly to avoid scanning receiver parameters\n+            scan(tree.body);\n+        }\n+\n+        void analyzeSymbol(JCTree tree) {\n+            Symbol sym = TreeInfo.symbolFor(tree);\n+            \/\/ make sure that there is a symbol and it is not static\n+            if (sym == null || sym.isStatic()) {\n+                return;\n+            }\n+            if (isInLHS && !insideLambdaOrClassDef) {\n+                \/\/ Check instance field assignments that appear in constructor prologues\n+                if (isEarlyReference(localEnv, tree, sym)) {\n+                    \/\/ Field may not be inherited from a superclass\n+                    if (sym.owner != localEnv.enclClass.sym) {\n+                        reportPrologueError(tree, sym);\n+                        return;\n+                    }\n+                    \/\/ Field may not have an initializer\n+                    if ((sym.flags() & HASINIT) != 0) {\n+                        if (!localEnv.enclClass.sym.isValueClass() || !sym.type.hasTag(ARRAY) || !isIndexed) {\n+                            reportPrologueError(tree, sym, true);\n+                        }\n+                        return;\n+                    }\n+                    \/\/ cant reference an instance field before a this constructor\n+                    if (allowValueClasses && mode == PrologueVisitorMode.THIS_CONSTRUCTOR) {\n+                        reportPrologueError(tree, sym);\n+                        return;\n+                    }\n+                }\n+                return;\n+            }\n+            tree = TreeInfo.skipParens(tree);\n+            if (sym.kind == VAR && sym.owner.kind == TYP) {\n+                if (sym.name == names._this || sym.name == names._super) {\n+                    \/\/ are we seeing something like `this` or `CurrentClass.this` or `SuperClass.super::foo`?\n+                    if (TreeInfo.isExplicitThisReference(\n+                            types,\n+                            (ClassType)localEnv.enclClass.sym.type,\n+                            tree)) {\n+                        reportPrologueError(tree, sym);\n+                    }\n+                } else if (sym.kind == VAR && sym.owner.kind == TYP) { \/\/ now fields only\n+                    if (sym.owner != localEnv.enclClass.sym) {\n+                        if (localEnv.enclClass.sym.isSubClass(sym.owner, types) &&\n+                                sym.isInheritedIn(localEnv.enclClass.sym, types)) {\n+                            \/* if we are dealing with a field that doesn't belong to the current class, but the\n+                             * field is inherited, this is an error. Unless, the super class is also an outer\n+                             * class and the field's qualifier refers to the outer class\n+                             *\/\n+                            if (tree.hasTag(IDENT) ||\n+                                TreeInfo.isExplicitThisReference(\n+                                        types,\n+                                        (ClassType)localEnv.enclClass.sym.type,\n+                                        ((JCFieldAccess)tree).selected)) {\n+                                reportPrologueError(tree, sym);\n+                            }\n+                        }\n+                    } else if (isEarlyReference(localEnv, tree, sym)) {\n+                        \/* now this is a `proper` instance field of the current class\n+                         * references to fields of identity classes which happen to have initializers are\n+                         * not allowed in the prologue.\n+                         * But it is OK for a field with initializer to refer to another field with initializer,\n+                         * so no warning or error if we are analyzing a field initializer.\n+                         *\/\n+                        if (insideLambdaOrClassDef ||\n+                            (!localEnv.enclClass.sym.isValueClass() &&\n+                             (sym.flags_field & HASINIT) != 0 &&\n+                             !isInitializer))\n+                            reportPrologueError(tree, sym);\n+                        \/\/ we will need to generate a proxy for this field later on\n+                        if (!isInLHS) {\n+                            if (!allowValueClasses) {\n+                                reportPrologueError(tree, sym);\n+                            } else {\n+                                if (mode == PrologueVisitorMode.THIS_CONSTRUCTOR) {\n+                                    reportPrologueError(tree, sym);\n+                                } else if (mode == PrologueVisitorMode.SUPER_CONSTRUCTOR) {\n+                                    localProxyVarsGen.addFieldReadInPrologue(localEnv.enclMethod, sym);\n+                                }\n+                                \/* we do nothing in warnings only mode, as in that mode we are simulating what\n+                                 * the compiler would do in case the constructor code would be in the prologue\n+                                 * phase\n+                                 *\/\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        \/**\n+         * Determine if the symbol appearance constitutes an early reference to the current class.\n+         *\n+         * <p>\n+         * This means the symbol is an instance field, or method, of the current class and it appears\n+         * in an early initialization context of it (i.e., one of its constructor prologues).\n+         *\n+         * @param env    The current environment\n+         * @param tree   the AST referencing the variable\n+         * @param sym    The symbol\n+         *\/\n+        private boolean isEarlyReference(Env<AttrContext> env, JCTree tree, Symbol sym) {\n+            if ((sym.flags() & STATIC) == 0 &&\n+                    (sym.kind == VAR || sym.kind == MTH) &&\n+                    sym.isMemberOf(env.enclClass.sym, types)) {\n+                \/\/ Allow \"Foo.this.x\" when \"Foo\" is (also) an outer class, as this refers to the outer instance\n+                if (tree instanceof JCFieldAccess fa) {\n+                    return TreeInfo.isExplicitThisReference(types, (ClassType)env.enclClass.type, fa.selected);\n+                } else if (currentClassSym != env.enclClass.sym) {\n+                    \/* so we are inside a class, CI, in the prologue of an outer class, CO, and the symbol being\n+                     * analyzed has no qualifier. So if the symbol is a member of CI the reference is allowed,\n+                     * otherwise it is not.\n+                     * It could be that the reference to CI's member happens inside CI's own prologue, but that\n+                     * will be checked separately, when CI's prologue is analyzed.\n+                     *\/\n+                    return !sym.isMemberOf(currentClassSym, types);\n+                }\n+                return true;\n+            }\n+            return false;\n+        }\n+\n+        \/* scanner for a select expression, anything that is not a select or identifier\n+         * will be stored for further analysis\n+         *\/\n+        class SelectScanner extends DeferredAttr.FilterScanner {\n+            JCTree scanLater;\n+            java.util.List<JCTree> selectorTrees = new ArrayList<>();\n+\n+            SelectScanner() {\n+                super(Set.of(IDENT, SELECT, PARENS));\n+            }\n+\n+            @Override\n+            public void visitSelect(JCFieldAccess tree) {\n+                super.visitSelect(tree);\n+                selectorTrees.add(tree.selected);\n+            }\n+\n+            @Override\n+            void skip(JCTree tree) {\n+                scanLater = tree;\n+            }\n+        }\n+    }\n+\n@@ -1303,0 +1666,1 @@\n+                Env<AttrContext> initEnv = memberEnter.initEnv(tree, env);\n@@ -1309,1 +1673,0 @@\n-                    Env<AttrContext> initEnv = memberEnter.initEnv(tree, env);\n@@ -1315,4 +1678,13 @@\n-                    attribExpr(tree.init, initEnv, v.type);\n-                    if (tree.isImplicitlyTyped()) {\n-                        \/\/fixup local variable type\n-                        v.type = chk.checkLocalVarType(tree, tree.init.type, tree.name);\n+                    boolean previousCtorPrologue = initEnv.info.ctorPrologue;\n+                    try {\n+                        if (v.owner.kind == TYP && !v.isStatic() && v.isStrict()) {\n+                            \/\/ strict instance initializer in a value class\n+                            initEnv.info.ctorPrologue = true;\n+                        }\n+                        attribExpr(tree.init, initEnv, v.type);\n+                        if (tree.isImplicitlyTyped()) {\n+                            \/\/fixup local variable type\n+                            v.type = chk.checkLocalVarType(tree, tree.init.type, tree.name);\n+                        }\n+                    } finally {\n+                        initEnv.info.ctorPrologue = previousCtorPrologue;\n@@ -1321,0 +1693,7 @@\n+                if (allowValueClasses && v.owner.kind == TYP && !v.isStatic()) {\n+                    \/\/ strict field initializers are inlined in constructor's prologues\n+                    CtorPrologueVisitor ctorPrologueVisitor = new CtorPrologueVisitor(initEnv,\n+                            !v.isStrict() ? PrologueVisitorMode.WARNINGS_ONLY : PrologueVisitorMode.SUPER_CONSTRUCTOR,\n+                            true);\n+                    ctorPrologueVisitor.scan(tree.init);\n+                }\n@@ -1439,1 +1818,5 @@\n-            if ((tree.flags & STATIC) != 0) localEnv.info.staticLevel++;\n+            if ((tree.flags & STATIC) != 0) {\n+                localEnv.info.staticLevel++;\n+            } else {\n+                localEnv.info.instanceInitializerBlock = true;\n+            }\n@@ -1952,2 +2335,2 @@\n-        chk.checkRefType(tree.pos(), attribExpr(tree.lock, env));\n-        if (tree.lock.type != null && tree.lock.type.isValueBased()) {\n+        boolean identityType = chk.checkIdentityType(tree.pos(), attribExpr(tree.lock, env));\n+        if (identityType && tree.lock.type != null && tree.lock.type.isValueBased()) {\n@@ -4409,0 +4792,1 @@\n+        Assert.check(site == tree.selected.type);\n@@ -5520,1 +5904,1 @@\n-                } else {\n+                } else if ((c.flags_field & Flags.COMPOUND) == 0) {\n@@ -5557,0 +5941,5 @@\n+                if (c.isValueClass()) {\n+                    Assert.check(env.tree.hasTag(CLASSDEF));\n+                    chk.checkConstraintsOfValueClass((JCClassDecl) env.tree, c);\n+                }\n+\n@@ -5699,1 +6088,1 @@\n-            chk.checkSerialStructure(tree, c);\n+            chk.checkSerialStructure(env, tree, c);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":421,"deletions":32,"binary":false,"changes":453,"status":"modified"},{"patch":"@@ -170,0 +170,2 @@\n+        allowValueClasses = (!preview.isPreview(Feature.VALUE_CLASSES) || preview.isEnabled()) &&\n+                Feature.VALUE_CLASSES.allowedInSource(source);\n@@ -197,0 +199,4 @@\n+    \/** Are value classes allowed\n+     *\/\n+    private final boolean allowValueClasses;\n+\n@@ -674,0 +680,25 @@\n+    void checkConstraintsOfValueClass(JCClassDecl tree, ClassSymbol c) {\n+        DiagnosticPosition pos = tree.pos();\n+        for (Type st : types.closure(c.type)) {\n+            if (st == null || st.tsym == null || st.tsym.kind == ERR)\n+                continue;\n+            if  (st.tsym == syms.objectType.tsym || st.tsym == syms.recordType.tsym || st.isInterface())\n+                continue;\n+            if (!st.tsym.isAbstract()) {\n+                if (c != st.tsym) {\n+                    log.error(pos, Errors.ConcreteSupertypeForValueClass(c, st));\n+                }\n+                continue;\n+            }\n+            \/\/ dealing with an abstract value or value super class below.\n+            for (Symbol s : st.tsym.members().getSymbols(NON_RECURSIVE)) {\n+                if (s.kind == MTH) {\n+                    if ((s.flags() & (SYNCHRONIZED | STATIC)) == SYNCHRONIZED) {\n+                        log.error(pos, Errors.SuperClassMethodCannotBeSynchronized(s, c, st));\n+                    }\n+                    break;\n+                }\n+            }\n+        }\n+    }\n+\n@@ -731,0 +762,26 @@\n+    \/** Check that type is an identity type, i.e. not a value type.\n+     *  When not discernible statically, give it the benefit of doubt\n+     *  and defer to runtime.\n+     *\n+     *  @param pos           Position to be used for error reporting.\n+     *  @param t             The type to be checked.\n+     *\/\n+    boolean checkIdentityType(DiagnosticPosition pos, Type t) {\n+        if (t.hasTag(TYPEVAR)) {\n+            t = types.skipTypeVars(t, false);\n+        }\n+        if (t.isIntersection()) {\n+            IntersectionClassType ict = (IntersectionClassType)t;\n+            boolean result = true;\n+            for (Type component : ict.getExplicitComponents()) {\n+                result &= checkIdentityType(pos, component);\n+            }\n+            return result;\n+        }\n+        if (t.isPrimitive() || (t.isValueClass() && !t.tsym.isAbstract())) {\n+            typeTagError(pos, diags.fragment(Fragments.TypeReqIdentity), t);\n+            return false;\n+        }\n+        return true;\n+    }\n+\n@@ -1122,2 +1179,11 @@\n-            else\n-                mask = VarFlags;\n+            else {\n+                boolean isInstanceField = (flags & STATIC) == 0;\n+                boolean isInstanceFieldOfValueClass = isInstanceField && sym.owner.type.isValueClass();\n+                boolean isRecordField = isInstanceField && (sym.owner.flags_field & RECORD) != 0;\n+                if (allowValueClasses && (isInstanceFieldOfValueClass || isRecordField)) {\n+                    implicit |= FINAL | STRICT;\n+                    mask = ValueFieldFlags;\n+                } else {\n+                    mask = VarFlags;\n+                }\n+            }\n@@ -1149,1 +1215,2 @@\n-                mask = RecordMethodFlags;\n+                mask = ((sym.owner.flags_field & VALUE_CLASS) != 0 && (flags & Flags.STATIC) == 0) ?\n+                        RecordMethodFlags & ~SYNCHRONIZED : RecordMethodFlags;\n@@ -1151,1 +1218,3 @@\n-                mask = MethodFlags;\n+                \/\/ value objects do not have an associated monitor\/lock\n+                mask = ((sym.owner.flags_field & VALUE_CLASS) != 0 && (flags & Flags.STATIC) == 0) ?\n+                        MethodFlags & ~SYNCHRONIZED : MethodFlags;\n@@ -1168,1 +1237,1 @@\n-                mask = staticOrImplicitlyStatic && allowRecords && (flags & ANNOTATION) == 0 ? StaticLocalFlags : LocalClassFlags;\n+                mask = staticOrImplicitlyStatic && allowRecords && (flags & ANNOTATION) == 0 ? ExtendedStaticLocalClassFlags : ExtendedLocalClassFlags;\n@@ -1184,0 +1253,4 @@\n+            if ((flags & (VALUE_CLASS | SEALED | ABSTRACT)) == (VALUE_CLASS | SEALED) ||\n+                (flags & (VALUE_CLASS | NON_SEALED | ABSTRACT)) == (VALUE_CLASS | NON_SEALED)) {\n+                log.error(pos, Errors.NonAbstractValueClassCantBeSealedOrNonSealed);\n+            }\n@@ -1187,0 +1260,4 @@\n+            if ((flags & (INTERFACE | VALUE_CLASS)) == 0) {\n+                implicit |= IDENTITY_TYPE;\n+            }\n+\n@@ -1188,2 +1265,2 @@\n-                \/\/ enums can't be declared abstract, final, sealed or non-sealed\n-                mask &= ~(ABSTRACT | FINAL | SEALED | NON_SEALED);\n+                \/\/ enums can't be declared abstract, final, sealed or non-sealed or value\n+                mask &= ~(ABSTRACT | FINAL | SEALED | NON_SEALED | VALUE_CLASS);\n@@ -1202,0 +1279,5 @@\n+\n+            \/\/ concrete value classes are implicitly final\n+            if ((flags & (ABSTRACT | INTERFACE | VALUE_CLASS)) == VALUE_CLASS) {\n+                implicit |= FINAL;\n+            }\n@@ -1216,2 +1298,1 @@\n-        }\n-        else if ((sym.kind == TYP ||\n+        } else if ((sym.kind == TYP ||\n@@ -1240,1 +1321,2 @@\n-                 checkDisjoint(pos, flags,\n+                 \/\/ we are using `implicit` here as instance fields of value classes are implicitly final\n+                 checkDisjoint(pos, flags | implicit,\n@@ -1256,1 +1338,7 @@\n-                                ANNOTATION)) {\n+                                ANNOTATION)\n+                && checkDisjoint(pos, flags,\n+                                VALUE_CLASS,\n+                                ANNOTATION)\n+                && checkDisjoint(pos, flags,\n+                                VALUE_CLASS,\n+                                INTERFACE) ) {\n@@ -2053,0 +2141,5 @@\n+        if (allowValueClasses && origin.isValueClass() && names.finalize.equals(m.name)) {\n+            if (m.overrides(syms.objectFinalize, origin, types, false)) {\n+                log.warning(tree.pos(), Warnings.ValueFinalize);\n+            }\n+        }\n@@ -2488,0 +2581,12 @@\n+\n+        Type identitySuper = null;\n+        for (Type t : types.closure(c)) {\n+            if (t != c) {\n+                if (t.isIdentityClass() && (t.tsym.flags() & VALUE_BASED) == 0)\n+                    identitySuper = t;\n+                if (c.isValueClass() && identitySuper != null && identitySuper.tsym != syms.objectType.tsym) { \/\/ Object is special\n+                    log.error(pos, Errors.ValueTypeHasIdentitySuperType(c, identitySuper));\n+                    break;\n+                }\n+            }\n+        }\n@@ -4857,2 +4962,2 @@\n-    public void checkSerialStructure(JCClassDecl tree, ClassSymbol c) {\n-        (new SerialTypeVisitor()).visit(c, tree);\n+    public void checkSerialStructure(Env<AttrContext> env, JCClassDecl tree, ClassSymbol c) {\n+        (new SerialTypeVisitor(env)).visit(c, tree);\n@@ -4889,1 +4994,2 @@\n-        SerialTypeVisitor() {\n+        Env<AttrContext> env;\n+        SerialTypeVisitor(Env<AttrContext> env) {\n@@ -4891,0 +4997,1 @@\n+            this.env = env;\n@@ -4950,0 +5057,1 @@\n+            final boolean[] hasWriteReplace = {false};\n@@ -5024,1 +5132,1 @@\n-                            case \"writeReplace\"     -> checkWriteReplace(tree,e, method);\n+                            case \"writeReplace\"     -> {hasWriteReplace[0] = true; hasAppropriateWriteReplace(tree, method, true);}\n@@ -5035,1 +5143,20 @@\n-\n+            if (!hasWriteReplace[0] &&\n+                    (c.isValueClass() || hasAbstractValueSuperClass(c, Set.of(syms.numberType.tsym))) &&\n+                    !c.isAbstract() && !c.isRecord() &&\n+                    types.unboxedType(c.type) == Type.noType) {\n+                \/\/ we need to check if the class is inheriting an appropriate writeReplace method\n+                MethodSymbol ms = null;\n+                Log.DiagnosticHandler discardHandler = log.new DiscardDiagnosticHandler();\n+                try {\n+                    ms = rs.resolveInternalMethod(env.tree, env, c.type, names.writeReplace, List.nil(), List.nil());\n+                } catch (FatalError fe) {\n+                    \/\/ ignore no method was found\n+                } finally {\n+                    log.popDiagnosticHandler(discardHandler);\n+                }\n+                if (ms == null || !hasAppropriateWriteReplace(p, ms, false)) {\n+                    log.warning(p.pos(),\n+                            c.isValueClass() ? LintWarnings.SerializableValueClassWithoutWriteReplace1 :\n+                                    LintWarnings.SerializableValueClassWithoutWriteReplace2);\n+                }\n+            }\n@@ -5043,0 +5170,16 @@\n+        private boolean hasAbstractValueSuperClass(Symbol c, Set<Symbol> excluding) {\n+            while (c.getKind() == ElementKind.CLASS) {\n+                Type sup = ((ClassSymbol)c).getSuperclass();\n+                if (!sup.hasTag(CLASS) || sup.isErroneous() ||\n+                        sup.tsym == syms.objectType.tsym) {\n+                    return false;\n+                }\n+                \/\/ if it is a value super class it has to be abstract\n+                if (sup.isValueClass() && !excluding.contains(sup.tsym)) {\n+                    return true;\n+                }\n+                c = sup.tsym;\n+            }\n+            return false;\n+        }\n+\n@@ -5166,1 +5309,1 @@\n-            checkReturnType(tree, e, method, syms.voidType);\n+            isExpectedReturnType(tree, method, syms.voidType, true);\n@@ -5168,1 +5311,1 @@\n-            checkExceptions(tree, e, method, syms.ioExceptionType);\n+            hasExpectedExceptions(tree, method, true, syms.ioExceptionType);\n@@ -5172,1 +5315,1 @@\n-        private void checkWriteReplace(JCClassDecl tree, Element e, MethodSymbol method) {\n+        private boolean hasAppropriateWriteReplace(JCClassDecl tree, MethodSymbol method, boolean warn) {\n@@ -5178,4 +5321,4 @@\n-            checkConcreteInstanceMethod(tree, e, method);\n-            checkReturnType(tree, e, method, syms.objectType);\n-            checkNoArgs(tree, e, method);\n-            checkExceptions(tree, e, method, syms.objectStreamExceptionType);\n+            return isConcreteInstanceMethod(tree, method, warn) &&\n+                    isExpectedReturnType(tree, method, syms.objectType, warn) &&\n+                    hasNoArgs(tree, method, warn) &&\n+                    hasExpectedExceptions(tree, method, warn, syms.objectStreamExceptionType);\n@@ -5192,1 +5335,1 @@\n-            checkReturnType(tree, e, method, syms.voidType);\n+            isExpectedReturnType(tree, method, syms.voidType, true);\n@@ -5194,1 +5337,1 @@\n-            checkExceptions(tree, e, method, syms.ioExceptionType, syms.classNotFoundExceptionType);\n+            hasExpectedExceptions(tree, method, true, syms.ioExceptionType, syms.classNotFoundExceptionType);\n@@ -5201,3 +5344,3 @@\n-            checkReturnType(tree, e, method, syms.voidType);\n-            checkNoArgs(tree, e, method);\n-            checkExceptions(tree, e, method, syms.objectStreamExceptionType);\n+            isExpectedReturnType(tree, method, syms.voidType, true);\n+            hasNoArgs(tree, method, true);\n+            hasExpectedExceptions(tree, method, true, syms.objectStreamExceptionType);\n@@ -5213,4 +5356,4 @@\n-            checkConcreteInstanceMethod(tree, e, method);\n-            checkReturnType(tree,e, method, syms.objectType);\n-            checkNoArgs(tree, e, method);\n-            checkExceptions(tree, e, method, syms.objectStreamExceptionType);\n+            isConcreteInstanceMethod(tree, method, true);\n+            isExpectedReturnType(tree, method, syms.objectType, true);\n+            hasNoArgs(tree, method, true);\n+            hasExpectedExceptions(tree, method, true, syms.objectStreamExceptionType);\n@@ -5466,1 +5609,1 @@\n-                        case \"writeReplace\" -> checkWriteReplace(tree, e, method);\n+                        case \"writeReplace\" -> hasAppropriateWriteReplace(tree, method, true);\n@@ -5484,3 +5627,3 @@\n-        void checkConcreteInstanceMethod(JCClassDecl tree,\n-                                         Element enclosing,\n-                                         MethodSymbol method) {\n+        boolean isConcreteInstanceMethod(JCClassDecl tree,\n+                                         MethodSymbol method,\n+                                         boolean warn) {\n@@ -5488,0 +5631,1 @@\n+                if (warn) {\n@@ -5491,0 +5635,2 @@\n+                }\n+                return false;\n@@ -5492,0 +5638,1 @@\n+            return true;\n@@ -5494,4 +5641,4 @@\n-        private void checkReturnType(JCClassDecl tree,\n-                                     Element enclosing,\n-                                     MethodSymbol method,\n-                                     Type expectedReturnType) {\n+        private boolean isExpectedReturnType(JCClassDecl tree,\n+                                          MethodSymbol method,\n+                                          Type expectedReturnType,\n+                                          boolean warn) {\n@@ -5505,2 +5652,3 @@\n-                log.warning(\n-                        TreeInfo.diagnosticPositionFor(method, tree),\n+                if (warn) {\n+                    log.warning(\n+                            TreeInfo.diagnosticPositionFor(method, tree),\n@@ -5509,0 +5657,2 @@\n+                }\n+                return false;\n@@ -5510,0 +5660,1 @@\n+            return true;\n@@ -5547,1 +5698,1 @@\n-        private void checkNoArgs(JCClassDecl tree, Element enclosing, MethodSymbol method) {\n+        boolean hasNoArgs(JCClassDecl tree, MethodSymbol method, boolean warn) {\n@@ -5550,2 +5701,3 @@\n-                log.warning(\n-                        TreeInfo.diagnosticPositionFor(parameters.get(0), tree),\n+                if (warn) {\n+                    log.warning(\n+                            TreeInfo.diagnosticPositionFor(parameters.get(0), tree),\n@@ -5553,0 +5705,2 @@\n+                }\n+                return false;\n@@ -5554,0 +5708,1 @@\n+            return true;\n@@ -5566,4 +5721,4 @@\n-        private void checkExceptions(JCClassDecl tree,\n-                                     Element enclosing,\n-                                     MethodSymbol method,\n-                                     Type... declaredExceptions) {\n+        private boolean hasExpectedExceptions(JCClassDecl tree,\n+                                              MethodSymbol method,\n+                                              boolean warn,\n+                                              Type... declaredExceptions) {\n@@ -5588,2 +5743,3 @@\n-                        log.warning(\n-                                TreeInfo.diagnosticPositionFor(method, tree),\n+                        if (warn) {\n+                            log.warning(\n+                                    TreeInfo.diagnosticPositionFor(method, tree),\n@@ -5592,0 +5748,2 @@\n+                        }\n+                        return false;\n@@ -5595,1 +5753,1 @@\n-            return;\n+            return true;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Check.java","additions":210,"deletions":52,"binary":false,"changes":262,"status":"modified"},{"patch":"@@ -110,0 +110,1 @@\n+    private final boolean allowValueClasses;\n@@ -145,0 +146,2 @@\n+        this.allowValueClasses = (!preview.isPreview(Feature.VALUE_CLASSES) || preview.isEnabled()) &&\n+                Feature.VALUE_CLASSES.allowedInSource(source);\n@@ -201,0 +204,4 @@\n+    \/** A hash table mapping local classes to a set of outer this fields\n+     *\/\n+    public Map<ClassSymbol, Set<JCExpression>> initializerOuterThis = new WeakHashMap<>();\n+\n@@ -779,1 +786,2 @@\n-            JCClassDecl cdec = makeEmptyClass(STATIC | SYNTHETIC,\n+            \/\/ IDENTITY_TYPE will be interpreted as ACC_SUPER for older class files so we are fine\n+            JCClassDecl cdec = makeEmptyClass(STATIC | SYNTHETIC | IDENTITY_TYPE,\n@@ -1251,1 +1259,2 @@\n-                ctag = makeEmptyClass(STATIC | SYNTHETIC, topClass).sym;\n+                \/\/ IDENTITY_TYPE will be interpreted as ACC_SUPER for older class files so we are fine\n+                ctag = makeEmptyClass(STATIC | SYNTHETIC | IDENTITY_TYPE, topClass).sym;\n@@ -1407,3 +1416,4 @@\n-     *  @param pos        The source code position of the definition.\n-     *  @param freevars   The free variables.\n-     *  @param owner      The class in which the definitions go.\n+     *  @param pos               The source code position of the definition.\n+     *  @param freevars          The free variables.\n+     *  @param owner             The class in which the definitions go.\n+     *  @param additionalFlags   Any additional flags\n@@ -1500,1 +1510,1 @@\n-        VarSymbol outerThis = makeOuterThisVarSymbol(owner, FINAL | SYNTHETIC);\n+        VarSymbol outerThis = makeOuterThisVarSymbol(owner, FINAL | SYNTHETIC | (allowValueClasses && owner.isValueClass() ? STRICT : 0));\n@@ -1840,1 +1850,2 @@\n-        return makeEmptyClass(STATIC | SYNTHETIC, clazz).sym;\n+        \/\/ IDENTITY_TYPE will be interpreted as ACC_SUPER for older class files so we are fine\n+        return makeEmptyClass(STATIC | SYNTHETIC | IDENTITY_TYPE, clazz).sym;\n@@ -1892,1 +1903,2 @@\n-        assertionsDisabledClassCache = makeEmptyClass(STATIC | SYNTHETIC, outermostClassDef.sym).sym;\n+        \/\/ IDENTITY_TYPE will be interpreted as ACC_SUPER for older class files so we are fine\n+        assertionsDisabledClassCache = makeEmptyClass(STATIC | SYNTHETIC | IDENTITY_TYPE, outermostClassDef.sym).sym;\n@@ -2180,1 +2192,1 @@\n-            tree.pos, freevars(currentClass), currentClass);\n+            tree.pos, freevars(currentClass), currentClass, allowValueClasses && currentClass.isValueClass() ? STRICT : LOCAL_CAPTURE_FIELD);\n@@ -2761,0 +2773,1 @@\n+            ListBuffer<JCStatement> initializers = new ListBuffer<>();\n@@ -2765,6 +2778,4 @@\n-                    tree.body.stats = tree.body.stats.append(\n-                            make.Exec(\n-                                    make.Assign(\n-                                            make.Select(make.This(field.owner.erasure(types)), field),\n-                                            make.Ident(param)).setType(field.erasure(types))));\n-                    \/\/ we don't need the flag at the field anymore\n+                    initializers.add(make.Exec(\n+                            make.Assign(\n+                                    make.Select(make.This(field.owner.erasure(types)), field),\n+                                    make.Ident(param)).setType(field.erasure(types))));\n@@ -2774,0 +2785,7 @@\n+            if (initializers.nonEmpty()) {\n+                if (allowValueClasses && (tree.sym.owner.isValueClass() || tree.sym.owner.hasStrict() || ((ClassSymbol)tree.sym.owner).isRecord())) {\n+                    TreeInfo.mapSuperCalls(tree.body, supercall -> make.Block(0, initializers.toList().append(supercall)));\n+                } else {\n+                    tree.body.stats = tree.body.stats.appendList(initializers);\n+                }\n+            }\n@@ -2997,0 +3015,11 @@\n+                if (currentMethodSym != null &&\n+                        ((currentMethodSym.flags_field & (STATIC | BLOCK)) == BLOCK) &&\n+                        currentMethodSym.owner.isValueClass()) {\n+                    \/\/ instance initializer in a value class\n+                    Set<JCExpression> outerThisSet = initializerOuterThis.get(currentClass);\n+                    if (outerThisSet == null) {\n+                        outerThisSet = new HashSet<>();\n+                    }\n+                    outerThisSet.add(thisArg);\n+                    initializerOuterThis.put(currentClass, outerThisSet);\n+                }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Lower.java","additions":44,"deletions":15,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -35,1 +35,0 @@\n-import java.util.function.ToIntFunction;\n@@ -44,11 +43,0 @@\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_Class;\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_Double;\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_Fieldref;\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_Float;\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_Integer;\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_InterfaceMethodref;\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_Long;\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_MethodHandle;\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_MethodType;\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_Methodref;\n-import static com.sun.tools.javac.jvm.ClassFile.CONSTANT_String;\n@@ -58,0 +46,3 @@\n+import java.util.Map;\n+import java.util.HashMap;\n+import java.util.Set;\n@@ -202,0 +193,8 @@\n+    private Map<Integer, Set<VarSymbol>> cpToUnsetFieldsMap = new HashMap<>();\n+\n+    public Set<VarSymbol> initialUnsetFields;\n+\n+    public Set<VarSymbol> currentUnsetFields;\n+\n+    boolean generateEarlyLarvalFrame;\n+\n@@ -214,1 +213,2 @@\n-                PoolWriter poolWriter) {\n+                PoolWriter poolWriter,\n+                boolean generateEarlyLarvalFrame) {\n@@ -236,0 +236,1 @@\n+        this.generateEarlyLarvalFrame = generateEarlyLarvalFrame;\n@@ -1086,1 +1087,0 @@\n-\n@@ -1233,0 +1233,1 @@\n+        Set<VarSymbol> unsetFields;\n@@ -1328,0 +1329,1 @@\n+        boolean hasUninitalizedThis = false;\n@@ -1333,1 +1335,1 @@\n-                if (!(vtype instanceof UninitializedType))\n+                if (!(vtype instanceof UninitializedType)) {\n@@ -1335,0 +1337,3 @@\n+                } else if (vtype.hasTag(TypeTag.UNINITIALIZED_THIS)) {\n+                    hasUninitalizedThis = true;\n+                }\n@@ -1360,0 +1365,4 @@\n+        Set<VarSymbol> unsetFieldsAtPC = cpToUnsetFieldsMap.get(pc);\n+        boolean encloseWithEarlyLarvalFrame = unsetFieldsAtPC != null && generateEarlyLarvalFrame && hasUninitalizedThis\n+                && !lastFrame.unsetFields.equals(unsetFieldsAtPC);\n+\n@@ -1367,2 +1376,9 @@\n-        stackMapTableBuffer[stackMapBufferSize++] =\n-                StackMapTableFrame.getInstance(frame, lastFrame.pc, lastFrame.locals, types);\n+\n+        StackMapTableFrame tableFrame = StackMapTableFrame.getInstance(frame, lastFrame, types, pc);\n+        if (encloseWithEarlyLarvalFrame) {\n+            tableFrame = new StackMapTableFrame.EarlyLarvalFrame(tableFrame, unsetFieldsAtPC);\n+            frame.unsetFields = unsetFieldsAtPC;\n+        } else {\n+            frame.unsetFields = lastFrame.unsetFields;\n+        }\n+        stackMapTableBuffer[stackMapBufferSize++] = tableFrame;\n@@ -1374,0 +1390,4 @@\n+    public void addUnsetFieldsAtPC(int pc, Set<VarSymbol> unsetFields) {\n+        cpToUnsetFieldsMap.put(pc, unsetFields);\n+    }\n+\n@@ -1395,0 +1415,1 @@\n+        frame.unsetFields = initialUnsetFields;\n@@ -1473,0 +1494,3 @@\n+            if (currentUnsetFields != null) {\n+                addUnsetFieldsAtPC(result.pc, currentUnsetFields);\n+            }\n@@ -1484,0 +1508,1 @@\n+        int originalTarget = target;\n@@ -1510,1 +1535,1 @@\n-                if (fatcode)\n+                if (fatcode) {\n@@ -1512,0 +1537,4 @@\n+                    if (cpToUnsetFieldsMap.get(chain.pc) != null) {\n+                        addUnsetFieldsAtPC(originalTarget, cpToUnsetFieldsMap.get(chain.pc));\n+                    }\n+                }\n@@ -1515,1 +1544,1 @@\n-                else\n+                else {\n@@ -1517,0 +1546,4 @@\n+                    if (cpToUnsetFieldsMap.get(chain.pc) != null) {\n+                        addUnsetFieldsAtPC(originalTarget, cpToUnsetFieldsMap.get(chain.pc));\n+                    }\n+                }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/Code.java","additions":52,"deletions":19,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -786,1 +786,1 @@\n-    improperly formed type, some parameters are missing\n+    improperly formed type, some parameters are missing or misplaced\n@@ -2139,0 +2139,8 @@\n+# lint: serial\n+compiler.warn.serializable.value.class.without.write.replace.1=\\\n+    serializable value class does not declare, or inherits, a writeReplace method\n+\n+# lint: serial\n+compiler.warn.serializable.value.class.without.write.replace.2=\\\n+    serializable class does not declare, or inherits, a writeReplace method\n+\n@@ -2886,0 +2894,3 @@\n+compiler.misc.type.req.identity=\\\n+    a type with identity\n+\n@@ -3946,0 +3957,3 @@\n+compiler.misc.bad.access.flags=\\\n+    bad access flags combination: {0}\n+\n@@ -4287,0 +4301,22 @@\n+compiler.misc.feature.value.classes=\\\n+    value classes\n+\n+# 0: type, 1: type\n+compiler.err.value.type.has.identity.super.type=\\\n+    The identity type {1} cannot be a supertype of the value type {0}\n+\n+# 0: symbol, 1: type\n+compiler.err.concrete.supertype.for.value.class=\\\n+    The concrete class {1} is not allowed to be a super class of the value class {0} either directly or indirectly\n+\n+# 0: symbol, 1: symbol, 2: type\n+compiler.err.super.class.method.cannot.be.synchronized=\\\n+    The method {0} in the super class {2} of the value class {1} is synchronized. This is disallowed\n+\n+compiler.err.non.abstract.value.class.cant.be.sealed.or.non.sealed=\\\n+    ''sealed'' or ''non-sealed'' modifiers are only applicable to abstract value classes\n+\n+# 0: symbol\n+compiler.err.strict.field.not.have.been.initialized.before.super=\\\n+    strict field {0} is not initialized before the supertype constructor has been called\n+\n@@ -4314,0 +4350,5 @@\n+# 0: symbol or name\n+# lint: initialization\n+compiler.warn.would.not.be.allowed.in.prologue=\\\n+    reference to {0} would not be allowed in the prologue phase\n+\n@@ -4318,0 +4359,3 @@\n+\n+compiler.warn.value.finalize=\\\n+    value classes should not have finalize methods, they are not invoked\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/compiler.properties","additions":45,"deletions":1,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -230,0 +230,4 @@\n+javac.opt.Xlint.desc.initialization=\\\n+    Warn about code in identity classes that wouldn''t be allowed in early\\n\\\n+\\                         construction due to a this dependency.\n+\n@@ -236,0 +240,3 @@\n+javac.opt.Xlint.desc.migration=\\\n+    Warn about issues related to migration of JDK classes.\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/javac.properties","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -168,0 +168,2 @@\n+ * <tr><th scope=\"row\">{@code initialization}       <td>code in identity classes that wouldn't be allowed in early\n+ *                                                      construction due to a {@code this} dependency.\n@@ -262,0 +264,1 @@\n+        jdk.jdeps,\n@@ -271,0 +274,1 @@\n+        jdk.jdeps,\n","filename":"src\/jdk.compiler\/share\/classes\/module-info.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -613,0 +613,3 @@\n+    -   `initialization`: Warns about code in identity classes that wouldn't be\n+        allowed in early construction due to a `this` dependency.\n+\n","filename":"src\/jdk.compiler\/share\/man\/javac.md","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -80,0 +80,3 @@\n+compiler\/jvmci\/jdk.vm.ci.hotspot.test\/src\/jdk\/vm\/ci\/hotspot\/test\/MemoryAccessProviderTest.java 8350208 generic-all\n+compiler\/jvmci\/jdk.vm.ci.hotspot.test\/src\/jdk\/vm\/ci\/hotspot\/test\/TestHotSpotResolvedJavaField.java 8350208 generic-all\n+\n@@ -82,0 +85,24 @@\n+# Valhalla\n+compiler\/whitebox\/DeoptimizeRelocatedNMethod.java 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#C1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#C2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#G1C1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#G1C2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ParallelC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ParallelC2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#SerialC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#SerialC2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ZGCC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ZGCC2 8370571 generic-all\n+compiler\/whitebox\/StressNMethodRelocation.java 8370571 generic-all\n+compiler\/valhalla\/inlinetypes\/TestC1.java             8372341 generic-all\n+compiler\/valhalla\/inlinetypes\/TestCastMismatch.java   8372341 generic-all\n+compiler\/valhalla\/inlinetypes\/TestGetfieldChains.java 8372341 generic-all\n+compiler\/codegen\/TestRedundantLea.java#GetAndSet          8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StringEquals       8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StringInflate      8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#RegexFind          8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StoreNSerial       8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StoreNParallel     8361089 generic-all\n+compiler\/codegen\/TestRedundantLea.java#Spill              8361089 generic-all\n+\n@@ -103,0 +130,1 @@\n+runtime\/cds\/appcds\/redefineClass\/RedefineRunningMethods_Shared.java  8304168 generic-all\n@@ -122,0 +150,16 @@\n+\n+# Valhalla\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictInstanceFieldsTest.java CODETOOLS-7904031 generic-all\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictStaticFieldsTest.java CODETOOLS-7904031 generic-all\n+\n+runtime\/cds\/TestDefaultArchiveLoading.java#coops_nocoh            8366774           generic-all\n+runtime\/cds\/TestDefaultArchiveLoading.java#nocoops_nocoh          8366774           generic-all\n+\n+# Valhalla + AOT\n+runtime\/cds\/appcds\/aotCache\/HelloAOTCache.java                                  8369043 generic-aarch64\n+runtime\/cds\/appcds\/aotCode\/AOTCodeFlags.java                                    8369043 generic-aarch64\n+runtime\/cds\/appcds\/methodHandles\/MethodHandlesGeneralTest.java#aot              8367408 generic-all\n+runtime\/cds\/appcds\/resolvedConstants\/ResolvedConstants.java#aot                 8371456 generic-all\n+runtime\/cds\/appcds\/resolvedConstants\/ResolvedConstants.java#static              8371456 generic-all\n+runtime\/cds\/appcds\/dynamicArchive\/DynamicArchiveRelocationTest.java             8372265 generic-all\n+runtime\/cds\/appcds\/dynamicArchive\/HelloDynamicInlineClass.java                  8372265 generic-all\n@@ -148,0 +192,57 @@\n+\n+# Valhalla TODO:\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbDumpclass.java 8190936 generic-all\n+\n+# Array Changes TODO\n+serviceability\/sa\/CDSJMapClstats.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbClasses.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DisableRegistryTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbDumpheap.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbTestConnectArgument.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DebugdConnectTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJhisto.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id1 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackWithConcurrentLock.java 8365722 generic-all\n+serviceability\/sa\/TestJhsdbJstackWithVirtualThread.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackXcompStress.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#process 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#core 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id0 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id1 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#serial 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#parallel 8365722 generic-all\n+serviceability\/sa\/DeadlockDetectionTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id0 8365722 generic-all\n+serviceability\/sa\/TestInstanceKlassSize.java 8365722 generic-all\n+serviceability\/sa\/TestSysProps.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbAttachToDebugServer.java 8365722 generic-all\n+resourcehogs\/serviceability\/sa\/TestHeapDumpForLargeArray.java 8365722 generic-all\n+\n+resourcehogs\/serviceability\/sa\/ClhsdbRegionDetailsScanOopsForG1.java 8190936 generic-all\n+vmTestbase\/nsk\/jvmti\/scenarios\/events\/EM04\/em04t001\/TestDescription.java 8367590 generic-all\n+\n@@ -186,0 +287,2 @@\n+vmTestbase\/vm\/mlvm\/hiddenloader\/stress\/byteMutation\/Test.java 8317172 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":103,"deletions":0,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n- *          4802647 7123424 8024709 8193128 8327858 8368178 8371164\n+ *          4802647 7123424 8024709 8193128 8327858 8368178 8346307 8371164\n@@ -35,0 +35,1 @@\n+ * @run main MOAT --enable-preview\n","filename":"test\/jdk\/java\/util\/Collection\/MOAT.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+ * @enablePreview\n@@ -50,0 +51,1 @@\n+ * @enablePreview\n@@ -86,0 +88,1 @@\n+    static final Random RAND = Utils.getRandomInstance();\n@@ -115,1 +118,1 @@\n-        if (RANDOM) testRandom(System.currentTimeMillis(), 50);\n+        if (RANDOM) testRandom(RAND.nextLong(), 50);\n@@ -121,2 +124,2 @@\n-        CALL_I_INT, CALL_I_DBL, CALL_I_MANY,\n-        CALL_C_INT, CALL_C_DBL, CALL_C_MANY,\n+        CALL_I_INT, CALL_I_DBL, CALL_I_MANY, CALL_I_VAL,\n+        CALL_C_INT, CALL_C_DBL, CALL_C_MANY, CALL_C_VAL,\n@@ -285,1 +288,1 @@\n-    static final Op[] WARMUP_TRACE = {Op.MH_C_INT, Op.MH_C_MANY, Op.REF_C_INT, Op.REF_C_MANY, Op.CALL_C_INT};\n+    static final Op[] WARMUP_TRACE = {Op.MH_C_INT, Op.MH_C_MANY, Op.REF_C_INT, Op.REF_C_MANY, Op.CALL_C_INT, Op.CALL_C_VAL};\n@@ -605,0 +608,112 @@\n+    \/\/\/\/ Value Classes\n+\n+    static abstract value class BaseValue {\n+        public abstract int res();\n+    };\n+\n+    static value class SmallValue extends BaseValue {\n+        int x1;\n+        int x2;\n+\n+        public SmallValue(int i) {\n+            x1 = i;\n+            x2 = i;\n+        }\n+\n+        public int res() {\n+            return x1 + x2;\n+        }\n+    };\n+\n+    static value class LargeValue extends BaseValue {\n+        int x1;\n+        int x2;\n+        int x3;\n+        int x4;\n+        int x5;\n+        int x6;\n+        int x7;\n+\n+        public LargeValue(int i) {\n+            x1 = i;\n+            x2 = i;\n+            x3 = i;\n+            x4 = i;\n+            x5 = i;\n+            x6 = i;\n+            x7 = i;\n+        }\n+\n+        public int res() {\n+            return x1 + x2 + x3 + x4 + x5 + x6 + x7;\n+        }\n+    };\n+\n+    static value class OopsValue extends BaseValue {\n+        Object x1;\n+        Object x2;\n+        Object x3;\n+        Object x4;\n+        Object x5;\n+        int x6;\n+\n+        public OopsValue(int i) {\n+            x1 = new Object();\n+            x2 = new Object();\n+            x3 = new Object();\n+            x4 = new Object();\n+            x5 = new Object();\n+            x6 = i;\n+        }\n+\n+        public int res() {\n+            return x6;\n+        }\n+    };\n+\n+    public static value class DoubleValue extends BaseValue {\n+        double d1;\n+        double d2;\n+        double d3;\n+        double d4;\n+        double d5;\n+        double d6;\n+        double d7;\n+\n+        public DoubleValue(double d) {\n+            d1 = d;\n+            d2 = d + 1;\n+            d3 = d + 2;\n+            d4 = d + 3;\n+            d5 = d + 4;\n+            d6 = d + 4;\n+            d7 = d + 4;\n+        }\n+\n+        public int res() {\n+            return (int)(d1 + d2 + d3 + d4 + d5 + d6 + d7);\n+        }\n+    };\n+\n+    static value class MixedValue extends BaseValue {\n+        byte x1;\n+        short x2;\n+        int x3;\n+        long x4;\n+        double x5;\n+        boolean x6;\n+\n+        public MixedValue(int i) {\n+            x1 = (byte)i;\n+            x2 = (short)i;\n+            x3 = i;\n+            x4 = i;\n+            x5 = i;\n+            x6 = (i % 2) == 0;\n+        }\n+\n+        public int res() {\n+            return (int)x1 + (int)x2 + (int)x3 + (int)x4 + (int)x5 + (x6 ? 1 : 0);\n+        }\n+    };\n+\n@@ -640,0 +755,2 @@\n+    static final Class<?>[] val_sig = new Class<?>[]{int.class, SmallValue.class,\n+        LargeValue.class, OopsValue.class, DoubleValue.class, MixedValue.class};\n@@ -665,0 +782,2 @@\n+            method.put(Op.CALL_I_VAL,  Fuzz.class.getDeclaredMethod(\"int_val\", val_sig));\n+            method.put(Op.CALL_C_VAL,  Fuzz.class.getDeclaredMethod(\"com_val\", val_sig));\n@@ -709,0 +828,5 @@\n+        SmallValue sv = new SmallValue(res);\n+        LargeValue lv = new LargeValue(res);\n+        OopsValue ov = new OopsValue(res);\n+        DoubleValue dv = new DoubleValue((double)res);\n+        MixedValue mv = new MixedValue(res);\n@@ -720,0 +844,2 @@\n+            case CALL_I_VAL  -> res += int_val(depth+1, sv, lv, ov, dv, mv).res();\n+            case CALL_C_VAL  -> res += com_val(depth+1, sv, lv, ov, dv, mv).res();\n@@ -746,0 +872,5 @@\n+        SmallValue sv = new SmallValue(x);\n+        LargeValue lv = new LargeValue(x);\n+        OopsValue ov = new OopsValue(x);\n+        DoubleValue dv = new DoubleValue((double)x);\n+        MixedValue mv = new MixedValue(x);\n@@ -757,0 +888,2 @@\n+            case CALL_I_VAL  -> res += int_val(depth+1, sv, lv, ov, dv, mv).res();\n+            case CALL_C_VAL  -> res += com_val(depth+1, sv, lv, ov, dv, mv).res();\n@@ -783,0 +916,5 @@\n+        SmallValue sv = new SmallValue(x);\n+        LargeValue lv = new LargeValue(x);\n+        OopsValue ov = new OopsValue(x);\n+        DoubleValue dv = new DoubleValue((double)x);\n+        MixedValue mv = new MixedValue(x);\n@@ -794,0 +932,2 @@\n+            case CALL_I_VAL  -> res += int_val(depth+1, sv, lv, ov, dv, mv).res();\n+            case CALL_C_VAL  -> res += com_val(depth+1, sv, lv, ov, dv, mv).res();\n@@ -820,0 +960,5 @@\n+        SmallValue sv = new SmallValue(x1);\n+        LargeValue lv = new LargeValue(x1);\n+        OopsValue ov = new OopsValue(x1);\n+        DoubleValue dv = new DoubleValue((double)x1);\n+        MixedValue mv = new MixedValue(x1);\n@@ -831,0 +976,2 @@\n+            case CALL_I_VAL  -> res += int_val(depth+1, sv, lv, ov, dv, mv).res();\n+            case CALL_C_VAL  -> res += com_val(depth+1, sv, lv, ov, dv, mv).res();\n@@ -857,0 +1004,5 @@\n+        SmallValue sv = new SmallValue(x1);\n+        LargeValue lv = new LargeValue(x1);\n+        OopsValue ov = new OopsValue(x1);\n+        DoubleValue dv = new DoubleValue((double)x1);\n+        MixedValue mv = new MixedValue(x1);\n@@ -868,0 +1020,2 @@\n+            case CALL_I_VAL  -> res += int_val(depth+1, sv, lv, ov, dv, mv).res();\n+            case CALL_C_VAL  -> res += com_val(depth+1, sv, lv, ov, dv, mv).res();\n@@ -885,0 +1039,104 @@\n+    @DontInline\n+    BaseValue int_val(final int depth, SmallValue x1, LargeValue x2, OopsValue x3, DoubleValue x4, MixedValue x5) {\n+        int res = x1.res();\n+\n+        int x11 = (int)res, x12 = (int)res, x13 = (int)res, x14 = (int)res;\n+        double d1 = (double)res, d2 = (double)res, d3 = (double)res, d4 = (double)res;\n+        long l1 = (long)res, l2 = (long)res, l3 = (long)res, l4 = (long)res;\n+        float f1 = (float)res, f2 = (float)res, f3 = (float)res, f4 = (float)res;\n+        Object o1 = res, o2 = res, o3 = res, o4 = res;\n+        SmallValue sv = new SmallValue(res);\n+        LargeValue lv = new LargeValue(res);\n+        OopsValue ov = new OopsValue(res);\n+        DoubleValue dv = new DoubleValue((double)res);\n+        MixedValue mv = new MixedValue(res);\n+\n+        for (int c = 1, index0 = index; c > 0; c--, maybeResetIndex(index0)) { \/\/ index0 is the index to which we return when we loop\n+            switch (next(c)) {\n+            case THROW -> throwException();\n+            case LOOP  -> { c += 2; index0 = index; }\n+            case YIELD -> { preYield(); boolean y = Continuation.yield(SCOPE); postYield(y); c++; }\n+            case DONE  -> { break; }\n+            case CALL_I_INT  -> res += int_int(depth+1, (int)res);\n+            case CALL_C_INT  -> res += com_int(depth+1, (int)res);\n+            case CALL_I_DBL  -> res += (int)int_dbl(depth+1, res);\n+            case CALL_C_DBL  -> res += (int)com_dbl(depth+1, res);\n+            case CALL_I_VAL  -> res += int_val(depth+1, sv, lv, ov, dv, mv).res();\n+            case CALL_C_VAL  -> res += com_val(depth+1, sv, lv, ov, dv, mv).res();\n+            case CALL_I_PIN  -> res += int_pin(depth+1, (int)res);\n+            case CALL_C_PIN  -> res += com_pin(depth+1, (int)res);\n+            case CALL_I_MANY -> res += int_mny(depth+1, x11, d1, l1, f1, o1, x12, d2, l2, f2, o2, x13, d3, l3, f3, o3, x14, d4, l4, f4, o4);\n+            case CALL_C_MANY -> res += com_mny(depth+1, x11, d1, l1, f1, o1, x12, d2, l2, f2, o2, x13, d3, l3, f3, o3, x14, d4, l4, f4, o4);\n+            case CALL_I_CTCH -> {try { res += int_int(depth+1, (int)res); } catch (FuzzException e) {}}\n+            case CALL_C_CTCH -> {try { res += com_int(depth+1, (int)res); } catch (FuzzException e) {}}\n+            case MH_I_INT, MH_C_INT     -> {try { res += (int)handle(current()).invokeExact(this, depth+1, (int)res);  } catch (Throwable e) { rethrow(e); }}\n+            case MH_I_MANY, MH_C_MANY   -> {try { res += (int)handle(current()).invokeExact(this, depth+1, x11, d1, l1, f1, o1, x12, d2, l2, f2, o2, x13, d3, l3, f3, o3, x14, d4, l4, f4, o4); } catch (Throwable e) { rethrow(e); }}\n+            case REF_I_INT,  REF_C_INT  -> {try { res += (int)method(current()).invoke(this, depth+1, (int)res); } catch (InvocationTargetException e) { rethrow(e.getCause()); } catch (IllegalAccessException e) { assert false; }}\n+            case REF_I_MANY, REF_C_MANY -> {try { res += (int)method(current()).invoke(this, depth+1, x11, d1, l1, f1, o1, x12, d2, l2, f2, o2, x13, d3, l3, f3, o3, x14, d4, l4, f4, o4); } catch (InvocationTargetException e) { rethrow(e.getCause()); } catch (IllegalAccessException e) { assert false; }}\n+            default -> throw new AssertionError(\"Unknown op: \" + current());\n+            }\n+        }\n+\n+        int positiveRes = (res == Integer.MIN_VALUE) ? Integer.MAX_VALUE : Math.abs(res);\n+        switch (positiveRes % 5) {\n+            case 0 -> { return log(new SmallValue(res)); }\n+            case 1 -> { return log(new LargeValue(res)); }\n+            case 2 -> { return log(new OopsValue(res)); }\n+            case 3 -> { return log(new DoubleValue((double)res)); }\n+            case 4 -> { return log(new MixedValue(res)); }\n+            default -> throw new AssertionError(\"Invalid case\");\n+        }\n+    }\n+\n+    @DontInline\n+    BaseValue com_val(final int depth, SmallValue x1, LargeValue x2, OopsValue x3, DoubleValue x4, MixedValue x5) {\n+        int res = x1.res();\n+\n+        int x11 = (int)res, x12 = (int)res, x13 = (int)res, x14 = (int)res;\n+        double d1 = (double)res, d2 = (double)res, d3 = (double)res, d4 = (double)res;\n+        long l1 = (long)res, l2 = (long)res, l3 = (long)res, l4 = (long)res;\n+        float f1 = (float)res, f2 = (float)res, f3 = (float)res, f4 = (float)res;\n+        Object o1 = res, o2 = res, o3 = res, o4 = res;\n+        SmallValue sv = new SmallValue(res);\n+        LargeValue lv = new LargeValue(res);\n+        OopsValue ov = new OopsValue(res);\n+        DoubleValue dv = new DoubleValue((double)res);\n+        MixedValue mv = new MixedValue(res);\n+\n+        for (int c = 1, index0 = index; c > 0; c--, maybeResetIndex(index0)) { \/\/ index0 is the index to which we return when we loop\n+            switch (next(c)) {\n+            case THROW -> throwException();\n+            case LOOP  -> { c += 2; index0 = index; }\n+            case YIELD -> { preYield(); boolean y = Continuation.yield(SCOPE); postYield(y); c++; }\n+            case DONE  -> { break; }\n+            case CALL_I_INT  -> res += int_int(depth+1, (int)res);\n+            case CALL_C_INT  -> res += com_int(depth+1, (int)res);\n+            case CALL_I_DBL  -> res += (int)int_dbl(depth+1, res);\n+            case CALL_C_DBL  -> res += (int)com_dbl(depth+1, res);\n+            case CALL_I_VAL  -> res += int_val(depth+1, sv, lv, ov, dv, mv).res();\n+            case CALL_C_VAL  -> res += com_val(depth+1, sv, lv, ov, dv, mv).res();\n+            case CALL_I_PIN  -> res += int_pin(depth+1, (int)res);\n+            case CALL_C_PIN  -> res += com_pin(depth+1, (int)res);\n+            case CALL_I_MANY -> res += int_mny(depth+1, x11, d1, l1, f1, o1, x12, d2, l2, f2, o2, x13, d3, l3, f3, o3, x14, d4, l4, f4, o4);\n+            case CALL_C_MANY -> res += com_mny(depth+1, x11, d1, l1, f1, o1, x12, d2, l2, f2, o2, x13, d3, l3, f3, o3, x14, d4, l4, f4, o4);\n+            case CALL_I_CTCH -> {try { res += int_int(depth+1, (int)res); } catch (FuzzException e) {}}\n+            case CALL_C_CTCH -> {try { res += com_int(depth+1, (int)res); } catch (FuzzException e) {}}\n+            case MH_I_INT, MH_C_INT     -> {try { res += (int)handle(current()).invokeExact(this, depth+1, (int)res);  } catch (Throwable e) { rethrow(e); }}\n+            case MH_I_MANY, MH_C_MANY   -> {try { res += (int)handle(current()).invokeExact(this, depth+1, x11, d1, l1, f1, o1, x12, d2, l2, f2, o2, x13, d3, l3, f3, o3, x14, d4, l4, f4, o4); } catch (Throwable e) { rethrow(e); }}\n+            case REF_I_INT,  REF_C_INT  -> {try { res += (int)method(current()).invoke(this, depth+1, (int)res); } catch (InvocationTargetException e) { rethrow(e.getCause()); } catch (IllegalAccessException e) { assert false; }}\n+            case REF_I_MANY, REF_C_MANY -> {try { res += (int)method(current()).invoke(this, depth+1, x11, d1, l1, f1, o1, x12, d2, l2, f2, o2, x13, d3, l3, f3, o3, x14, d4, l4, f4, o4); } catch (InvocationTargetException e) { rethrow(e.getCause()); } catch (IllegalAccessException e) { assert false; }}\n+            default -> throw new AssertionError(\"Unknown op: \" + current());\n+            }\n+        }\n+\n+        int positiveRes = (res == Integer.MIN_VALUE) ? Integer.MAX_VALUE : Math.abs(res);\n+        switch (positiveRes % 5) {\n+            case 0 -> { return log(new SmallValue(res)); }\n+            case 1 -> { return log(new LargeValue(res)); }\n+            case 2 -> { return log(new OopsValue(res)); }\n+            case 3 -> { return log(new DoubleValue((double)res)); }\n+            case 4 -> { return log(new MixedValue(res)); }\n+            default -> throw new AssertionError(\"Invalid case\");\n+        }\n+    }\n+\n@@ -894,0 +1152,5 @@\n+        SmallValue sv = new SmallValue(x);\n+        LargeValue lv = new LargeValue(x);\n+        OopsValue ov = new OopsValue(x);\n+        DoubleValue dv = new DoubleValue((double)x);\n+        MixedValue mv = new MixedValue(x);\n@@ -907,0 +1170,2 @@\n+            case CALL_I_VAL  -> res += int_val(depth+1, sv, lv, ov, dv, mv).res();\n+            case CALL_C_VAL  -> res += com_val(depth+1, sv, lv, ov, dv, mv).res();\n@@ -935,0 +1200,5 @@\n+        SmallValue sv = new SmallValue(x);\n+        LargeValue lv = new LargeValue(x);\n+        OopsValue ov = new OopsValue(x);\n+        DoubleValue dv = new DoubleValue((double)x);\n+        MixedValue mv = new MixedValue(x);\n@@ -948,0 +1218,2 @@\n+            case CALL_I_VAL  -> res += int_val(depth+1, sv, lv, ov, dv, mv).res();\n+            case CALL_C_VAL  -> res += com_val(depth+1, sv, lv, ov, dv, mv).res();\n@@ -975,0 +1247,5 @@\n+        SmallValue sv = new SmallValue(x1);\n+        LargeValue lv = new LargeValue(x1);\n+        OopsValue ov = new OopsValue(x1);\n+        DoubleValue dv = new DoubleValue((double)x1);\n+        MixedValue mv = new MixedValue(x1);\n@@ -986,0 +1263,2 @@\n+            case CALL_I_VAL  -> res += int_val(depth+1, sv, lv, ov, dv, mv).res();\n+            case CALL_C_VAL  -> res += com_val(depth+1, sv, lv, ov, dv, mv).res();\n@@ -1011,0 +1290,5 @@\n+        SmallValue sv = new SmallValue(x1);\n+        LargeValue lv = new LargeValue(x1);\n+        OopsValue ov = new OopsValue(x1);\n+        DoubleValue dv = new DoubleValue((double)x1);\n+        MixedValue mv = new MixedValue(x1);\n@@ -1022,0 +1306,2 @@\n+            case CALL_I_VAL  -> res += int_val(depth+1, sv, lv, ov, dv, mv).res();\n+            case CALL_C_VAL  -> res += com_val(depth+1, sv, lv, ov, dv, mv).res();\n","filename":"test\/jdk\/jdk\/internal\/vm\/Continuation\/Fuzz.java","additions":290,"deletions":4,"binary":false,"changes":294,"status":"modified"}]}