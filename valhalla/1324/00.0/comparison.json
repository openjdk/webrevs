{"files":[{"patch":"@@ -52,1 +52,0 @@\n-MICROBENCHMARK_INDIFY_DONE := $(MICROBENCHMARK_CLASSES)\/_indify.marker\n@@ -74,11 +73,0 @@\n-#### Compile Indify tool\n-\n-$(eval $(call SetupJavaCompilation, BUILD_INDIFY, \\\n-    TARGET_RELEASE := $(TARGET_RELEASE_BOOTJDK), \\\n-    SRC := $(TOPDIR)\/test\/jdk\/java\/lang\/invoke, \\\n-    INCLUDE_FILES := indify\/Indify.java, \\\n-    DISABLED_WARNINGS := this-escape rawtypes serial options, \\\n-    BIN := $(MICROBENCHMARK_TOOLS_CLASSES), \\\n-    JAVAC_FLAGS := -XDstringConcat=inline -Xprefer:newer, \\\n-))\n-\n@@ -127,8 +115,0 @@\n-# Run Indify\n-$(MICROBENCHMARK_INDIFY_DONE): $(BUILD_INDIFY) $(BUILD_JDK_MICROBENCHMARK)\n-\t$(call LogWarn, Running Indify on microbenchmark classes)\n-\t$(JAVA_SMALL) -cp $(MICROBENCHMARK_TOOLS_CLASSES) \\\n-\t    indify.Indify --overwrite $(MICROBENCHMARK_CLASSES) \\\n-\t    $(LOG_DEBUG) 2>&1\n-\t$(TOUCH) $@\n-\n@@ -147,2 +127,1 @@\n-    DEPENDENCIES := $(BUILD_JDK_MICROBENCHMARK) $(JMH_UNPACKED_JARS_DONE) \\\n-        $(MICROBENCHMARK_INDIFY_DONE), \\\n+    DEPENDENCIES := $(BUILD_JDK_MICROBENCHMARK) $(JMH_UNPACKED_JARS_DONE), \\\n","filename":"make\/test\/BuildMicrobenchmark.gmk","additions":1,"deletions":22,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -2302,2 +2302,1 @@\n-\/\/ Vector calling convention not yet implemented.\n-  return false;\n+  return EnableVectorSupport && UseVectorStubs;\n@@ -2308,2 +2307,7 @@\n-  Unimplemented();\n-  return OptoRegPair(0, 0);\n+  assert(EnableVectorSupport && UseVectorStubs, \"sanity\");\n+  int lo = V0_num;\n+  int hi = V0_H_num;\n+  if (ideal_reg == Op_VecX || ideal_reg == Op_VecA) {\n+    hi = V0_K_num;\n+  }\n+  return OptoRegPair(hi, lo);\n@@ -5050,0 +5054,18 @@\n+operand vRegD_V12()\n+%{\n+  constraint(ALLOC_IN_RC(v12_reg));\n+  match(RegD);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand vRegD_V13()\n+%{\n+  constraint(ALLOC_IN_RC(v13_reg));\n+  match(RegD);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n@@ -16167,0 +16189,16 @@\n+\/\/ Call Runtime Instruction without safepoint and with vector arguments\n+instruct CallLeafDirectVector(method meth)\n+%{\n+  match(CallLeafVector);\n+\n+  effect(USE meth);\n+\n+  ins_cost(CALL_COST);\n+\n+  format %{ \"CALL, runtime leaf vector $meth\" %}\n+\n+  ins_encode(aarch64_enc_java_to_runtime(meth));\n+\n+  ins_pipe(pipe_class_call);\n+%}\n+\n@@ -16321,0 +16359,1 @@\n+  predicate(!UseSecondarySupersTable);\n@@ -16323,1 +16362,1 @@\n-  ins_cost(1100);  \/\/ slightly larger than the next version\n+  ins_cost(20 * INSN_COST);  \/\/ slightly larger than the next version\n@@ -16333,0 +16372,28 @@\n+\/\/ Two versions of partialSubtypeCheck, both used when we need to\n+\/\/ search for a super class in the secondary supers array. The first\n+\/\/ is used when we don't know _a priori_ the class being searched\n+\/\/ for. The second, far more common, is used when we do know: this is\n+\/\/ used for instanceof, checkcast, and any case where C2 can determine\n+\/\/ it by constant propagation.\n+\n+instruct partialSubtypeCheckVarSuper(iRegP_R4 sub, iRegP_R0 super, vRegD_V0 vtemp, iRegP_R5 result,\n+                                     iRegP_R1 tempR1, iRegP_R2 tempR2, iRegP_R3 tempR3,\n+                                     rFlagsReg cr)\n+%{\n+  match(Set result (PartialSubtypeCheck sub super));\n+  predicate(UseSecondarySupersTable);\n+  effect(KILL cr, TEMP tempR1, TEMP tempR2, TEMP tempR3, TEMP vtemp);\n+\n+  ins_cost(10 * INSN_COST);  \/\/ slightly larger than the next version\n+  format %{ \"partialSubtypeCheck $result, $sub, $super\" %}\n+\n+  ins_encode %{\n+    __ lookup_secondary_supers_table_var($sub$$Register, $super$$Register,\n+                                         $tempR1$$Register, $tempR2$$Register, $tempR3$$Register,\n+                                         $vtemp$$FloatRegister,\n+                                         $result$$Register, \/*L_success*\/nullptr);\n+  %}\n+\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n@@ -16341,1 +16408,1 @@\n-  ins_cost(700);  \/\/ smaller than the next version\n+  ins_cost(5 * INSN_COST);  \/\/ smaller than the next version\n@@ -16348,5 +16415,6 @@\n-      success = __ lookup_secondary_supers_table($sub$$Register, $super_reg$$Register,\n-                                                 $tempR1$$Register, $tempR2$$Register, $tempR3$$Register,\n-                                                 $vtemp$$FloatRegister,\n-                                                 $result$$Register,\n-                                                 super_klass_slot);\n+      success =\n+        __ lookup_secondary_supers_table_const($sub$$Register, $super_reg$$Register,\n+                                               $tempR1$$Register, $tempR2$$Register, $tempR3$$Register,\n+                                               $vtemp$$FloatRegister,\n+                                               $result$$Register,\n+                                               super_klass_slot);\n@@ -16366,15 +16434,0 @@\n-instruct partialSubtypeCheckVsZero(iRegP_R4 sub, iRegP_R0 super, iRegP_R2 temp, iRegP_R5 result, immP0 zero, rFlagsReg cr)\n-%{\n-  match(Set cr (CmpP (PartialSubtypeCheck sub super) zero));\n-  effect(KILL temp, KILL result);\n-\n-  ins_cost(1100);  \/\/ slightly larger than the next version\n-  format %{ \"partialSubtypeCheck $result, $sub, $super == 0\" %}\n-\n-  ins_encode(aarch64_enc_partial_subtype_check(sub, super, temp, result));\n-\n-  opcode(0x0); \/\/ Don't zero result reg on hit\n-\n-  ins_pipe(pipe_class_memory);\n-%}\n-\n@@ -16828,0 +16881,26 @@\n+instruct arrays_hashcode(iRegP_R1 ary, iRegI_R2 cnt, iRegI_R0 result, immI basic_type,\n+                         vRegD_V0 vtmp0, vRegD_V1 vtmp1, vRegD_V2 vtmp2, vRegD_V3 vtmp3,\n+                         vRegD_V4 vtmp4, vRegD_V5 vtmp5, vRegD_V6 vtmp6, vRegD_V7 vtmp7,\n+                         vRegD_V12 vtmp8, vRegD_V13 vtmp9, rFlagsReg cr)\n+%{\n+  match(Set result (VectorizedHashCode (Binary ary cnt) (Binary result basic_type)));\n+  effect(TEMP vtmp0, TEMP vtmp1, TEMP vtmp2, TEMP vtmp3, TEMP vtmp4, TEMP vtmp5, TEMP vtmp6,\n+         TEMP vtmp7, TEMP vtmp8, TEMP vtmp9, USE_KILL ary, USE_KILL cnt, USE basic_type, KILL cr);\n+\n+  format %{ \"Array HashCode array[] $ary,$cnt,$result,$basic_type -> $result   \/\/ KILL all\" %}\n+  ins_encode %{\n+    address tpc = __ arrays_hashcode($ary$$Register, $cnt$$Register, $result$$Register,\n+                                     $vtmp3$$FloatRegister, $vtmp2$$FloatRegister,\n+                                     $vtmp1$$FloatRegister, $vtmp0$$FloatRegister,\n+                                     $vtmp4$$FloatRegister, $vtmp5$$FloatRegister,\n+                                     $vtmp6$$FloatRegister, $vtmp7$$FloatRegister,\n+                                     $vtmp8$$FloatRegister, $vtmp9$$FloatRegister,\n+                                     (BasicType)$basic_type$$constant);\n+    if (tpc == nullptr) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":105,"deletions":26,"binary":false,"changes":131,"status":"modified"},{"patch":"@@ -965,1 +965,7 @@\n-        __ check_klass_subtype_slow_path(r4, r0, r2, r5, nullptr, &miss);\n+        __ check_klass_subtype_slow_path(\/*sub_klass*\/r4,\n+                                         \/*super_klass*\/r0,\n+                                         \/*temp_reg*\/r2,\n+                                         \/*temp2_reg*\/r5,\n+                                         \/*L_success*\/nullptr,\n+                                         \/*L_failure*\/&miss);\n+        \/\/ Need extras for table lookup: r1, r3, vtemp\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Runtime1_aarch64.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"utilities\/powerOfTwo.hpp\"\n@@ -72,0 +73,95 @@\n+\/\/ jdk.internal.util.ArraysSupport.vectorizedHashCode\n+address C2_MacroAssembler::arrays_hashcode(Register ary, Register cnt, Register result,\n+                                           FloatRegister vdata0, FloatRegister vdata1,\n+                                           FloatRegister vdata2, FloatRegister vdata3,\n+                                           FloatRegister vmul0, FloatRegister vmul1,\n+                                           FloatRegister vmul2, FloatRegister vmul3,\n+                                           FloatRegister vpow, FloatRegister vpowm,\n+                                           BasicType eltype) {\n+  ARRAYS_HASHCODE_REGISTERS;\n+\n+  Register tmp1 = rscratch1, tmp2 = rscratch2;\n+\n+  Label TAIL, STUB_SWITCH, STUB_SWITCH_OUT, LOOP, BR_BASE, LARGE, DONE;\n+\n+  \/\/ Vectorization factor. Number of array elements loaded to one SIMD&FP registers by the stubs. We\n+  \/\/ use 8H load arrangements for chars and shorts and 8B for booleans and bytes. It's possible to\n+  \/\/ use 4H for chars and shorts instead, but using 8H gives better performance.\n+  const size_t vf = eltype == T_BOOLEAN || eltype == T_BYTE ? 8\n+                    : eltype == T_CHAR || eltype == T_SHORT ? 8\n+                    : eltype == T_INT                       ? 4\n+                                                            : 0;\n+  guarantee(vf, \"unsupported eltype\");\n+\n+  \/\/ Unroll factor for the scalar loop below. The value is chosen based on performance analysis.\n+  const size_t unroll_factor = 4;\n+\n+  switch (eltype) {\n+  case T_BOOLEAN:\n+    BLOCK_COMMENT(\"arrays_hashcode(unsigned byte) {\");\n+    break;\n+  case T_CHAR:\n+    BLOCK_COMMENT(\"arrays_hashcode(char) {\");\n+    break;\n+  case T_BYTE:\n+    BLOCK_COMMENT(\"arrays_hashcode(byte) {\");\n+    break;\n+  case T_SHORT:\n+    BLOCK_COMMENT(\"arrays_hashcode(short) {\");\n+    break;\n+  case T_INT:\n+    BLOCK_COMMENT(\"arrays_hashcode(int) {\");\n+    break;\n+  default:\n+    ShouldNotReachHere();\n+  }\n+\n+  \/\/ large_arrays_hashcode(T_INT) performs worse than the scalar loop below when the Neon loop\n+  \/\/ implemented by the stub executes just once. Call the stub only if at least two iterations will\n+  \/\/ be executed.\n+  const size_t large_threshold = eltype == T_INT ? vf * 2 : vf;\n+  cmpw(cnt, large_threshold);\n+  br(Assembler::HS, LARGE);\n+\n+  bind(TAIL);\n+\n+  \/\/ The andr performs cnt % uf where uf = unroll_factor. The subtract shifted by 3 offsets past\n+  \/\/ uf - (cnt % uf) pairs of load + madd insns i.e. it only executes cnt % uf load + madd pairs.\n+  \/\/ Iteration eats up the remainder, uf elements at a time.\n+  assert(is_power_of_2(unroll_factor), \"can't use this value to calculate the jump target PC\");\n+  andr(tmp2, cnt, unroll_factor - 1);\n+  adr(tmp1, BR_BASE);\n+  sub(tmp1, tmp1, tmp2, ext::sxtw, 3);\n+  movw(tmp2, 0x1f);\n+  br(tmp1);\n+\n+  bind(LOOP);\n+  for (size_t i = 0; i < unroll_factor; ++i) {\n+    load(tmp1, Address(post(ary, type2aelembytes(eltype))), eltype);\n+    maddw(result, result, tmp2, tmp1);\n+  }\n+  bind(BR_BASE);\n+  subsw(cnt, cnt, unroll_factor);\n+  br(Assembler::HS, LOOP);\n+\n+  b(DONE);\n+\n+  bind(LARGE);\n+\n+  RuntimeAddress stub = RuntimeAddress(StubRoutines::aarch64::large_arrays_hashcode(eltype));\n+  assert(stub.target() != nullptr, \"array_hashcode stub has not been generated\");\n+  address tpc = trampoline_call(stub);\n+  if (tpc == nullptr) {\n+    DEBUG_ONLY(reset_labels(TAIL, BR_BASE));\n+    postcond(pc() == badAddress);\n+    return nullptr;\n+  }\n+\n+  bind(DONE);\n+\n+  BLOCK_COMMENT(\"} \/\/ arrays_hashcode\");\n+\n+  postcond(pc() != badAddress);\n+  return pc();\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":96,"deletions":0,"binary":false,"changes":96,"status":"modified"},{"patch":"@@ -40,0 +40,7 @@\n+  \/\/ jdk.internal.util.ArraysSupport.vectorizedHashCode\n+  address arrays_hashcode(Register ary, Register cnt, Register result, FloatRegister vdata0,\n+                          FloatRegister vdata1, FloatRegister vdata2, FloatRegister vdata3,\n+                          FloatRegister vmul0, FloatRegister vmul1, FloatRegister vmul2,\n+                          FloatRegister vmul3, FloatRegister vpow, FloatRegister vpowm,\n+                          BasicType eltype);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1430,7 +1430,4 @@\n-                                        RegisterOrConstant super_check_offset) {\n-  assert_different_registers(sub_klass, super_klass, temp_reg);\n-  bool must_load_sco = (super_check_offset.constant_or_zero() == -1);\n-  if (super_check_offset.is_register()) {\n-    assert_different_registers(sub_klass, super_klass,\n-                               super_check_offset.as_register());\n-  } else if (must_load_sco) {\n+                                                   Register super_check_offset) {\n+  assert_different_registers(sub_klass, super_klass, temp_reg, super_check_offset);\n+  bool must_load_sco = ! super_check_offset->is_valid();\n+  if (must_load_sco) {\n@@ -1447,1 +1444,0 @@\n-  int sc_offset = in_bytes(Klass::secondary_super_cache_offset());\n@@ -1469,1 +1465,1 @@\n-    super_check_offset = RegisterOrConstant(temp_reg);\n+    super_check_offset = temp_reg;\n@@ -1471,0 +1467,1 @@\n+\n@@ -1474,0 +1471,1 @@\n+  br(Assembler::EQ, *L_success);\n@@ -1486,17 +1484,3 @@\n-  if (super_check_offset.is_register()) {\n-    br(Assembler::EQ, *L_success);\n-    subs(zr, super_check_offset.as_register(), sc_offset);\n-    if (L_failure == &L_fallthrough) {\n-      br(Assembler::EQ, *L_slow_path);\n-    } else {\n-      br(Assembler::NE, *L_failure);\n-      final_jmp(*L_slow_path);\n-    }\n-  } else if (super_check_offset.as_constant() == sc_offset) {\n-    \/\/ Need a slow path; fast failure is impossible.\n-    if (L_slow_path == &L_fallthrough) {\n-      br(Assembler::EQ, *L_success);\n-    } else {\n-      br(Assembler::NE, *L_slow_path);\n-      final_jmp(*L_success);\n-    }\n+  sub(rscratch1, super_check_offset, in_bytes(Klass::secondary_super_cache_offset()));\n+  if (L_failure == &L_fallthrough) {\n+    cbz(rscratch1, *L_slow_path);\n@@ -1504,7 +1488,2 @@\n-    \/\/ No slow path; it's a fast decision.\n-    if (L_failure == &L_fallthrough) {\n-      br(Assembler::EQ, *L_success);\n-    } else {\n-      br(Assembler::NE, *L_failure);\n-      final_jmp(*L_success);\n-    }\n+    cbnz(rscratch1, *L_failure);\n+    final_jmp(*L_slow_path);\n@@ -1550,7 +1529,7 @@\n-void MacroAssembler::check_klass_subtype_slow_path(Register sub_klass,\n-                                                   Register super_klass,\n-                                                   Register temp_reg,\n-                                                   Register temp2_reg,\n-                                                   Label* L_success,\n-                                                   Label* L_failure,\n-                                                   bool set_cond_codes) {\n+void MacroAssembler::check_klass_subtype_slow_path_linear(Register sub_klass,\n+                                                          Register super_klass,\n+                                                          Register temp_reg,\n+                                                          Register temp2_reg,\n+                                                          Label* L_success,\n+                                                          Label* L_failure,\n+                                                          bool set_cond_codes) {\n@@ -1624,1 +1603,4 @@\n-  str(super_klass, super_cache_addr);\n+\n+  if (UseSecondarySupersCache) {\n+    str(super_klass, super_cache_addr);\n+  }\n@@ -1635,0 +1617,96 @@\n+\/\/ If Register r is invalid, remove a new register from\n+\/\/ available_regs, and add new register to regs_to_push.\n+Register MacroAssembler::allocate_if_noreg(Register r,\n+                                  RegSetIterator<Register> &available_regs,\n+                                  RegSet &regs_to_push) {\n+  if (!r->is_valid()) {\n+    r = *available_regs++;\n+    regs_to_push += r;\n+  }\n+  return r;\n+}\n+\n+\/\/ check_klass_subtype_slow_path_table() looks for super_klass in the\n+\/\/ hash table belonging to super_klass, branching to L_success or\n+\/\/ L_failure as appropriate. This is essentially a shim which\n+\/\/ allocates registers as necessary then calls\n+\/\/ lookup_secondary_supers_table() to do the work. Any of the temp\n+\/\/ regs may be noreg, in which case this logic will chooses some\n+\/\/ registers push and pop them from the stack.\n+void MacroAssembler::check_klass_subtype_slow_path_table(Register sub_klass,\n+                                                         Register super_klass,\n+                                                         Register temp_reg,\n+                                                         Register temp2_reg,\n+                                                         Register temp3_reg,\n+                                                         Register result_reg,\n+                                                         FloatRegister vtemp,\n+                                                         Label* L_success,\n+                                                         Label* L_failure,\n+                                                         bool set_cond_codes) {\n+  RegSet temps = RegSet::of(temp_reg, temp2_reg, temp3_reg);\n+\n+  assert_different_registers(sub_klass, super_klass, temp_reg, temp2_reg, rscratch1);\n+\n+  Label L_fallthrough;\n+  int label_nulls = 0;\n+  if (L_success == nullptr)   { L_success   = &L_fallthrough; label_nulls++; }\n+  if (L_failure == nullptr)   { L_failure   = &L_fallthrough; label_nulls++; }\n+  assert(label_nulls <= 1, \"at most one null in the batch\");\n+\n+  BLOCK_COMMENT(\"check_klass_subtype_slow_path\");\n+\n+  RegSetIterator<Register> available_regs\n+    = (RegSet::range(r0, r15) - temps - sub_klass - super_klass).begin();\n+\n+  RegSet pushed_regs;\n+\n+  temp_reg = allocate_if_noreg(temp_reg, available_regs, pushed_regs);\n+  temp2_reg = allocate_if_noreg(temp2_reg, available_regs, pushed_regs);\n+  temp3_reg = allocate_if_noreg(temp3_reg, available_regs, pushed_regs);\n+  result_reg = allocate_if_noreg(result_reg, available_regs, pushed_regs);\n+\n+  push(pushed_regs, sp);\n+\n+  lookup_secondary_supers_table_var(sub_klass,\n+                                    super_klass,\n+                                    temp_reg, temp2_reg, temp3_reg, vtemp, result_reg,\n+                                    nullptr);\n+  cmp(result_reg, zr);\n+\n+  \/\/ Unspill the temp. registers:\n+  pop(pushed_regs, sp);\n+\n+  \/\/ NB! Callers may assume that, when set_cond_codes is true, this\n+  \/\/ code sets temp2_reg to a nonzero value.\n+  if (set_cond_codes) {\n+    mov(temp2_reg, 1);\n+  }\n+\n+  br(Assembler::NE, *L_failure);\n+\n+  if (L_success != &L_fallthrough) {\n+    b(*L_success);\n+  }\n+\n+  bind(L_fallthrough);\n+}\n+\n+void MacroAssembler::check_klass_subtype_slow_path(Register sub_klass,\n+                                                   Register super_klass,\n+                                                   Register temp_reg,\n+                                                   Register temp2_reg,\n+                                                   Label* L_success,\n+                                                   Label* L_failure,\n+                                                   bool set_cond_codes) {\n+  if (UseSecondarySupersTable) {\n+    check_klass_subtype_slow_path_table\n+      (sub_klass, super_klass, temp_reg, temp2_reg, \/*temp3*\/noreg, \/*result*\/noreg,\n+       \/*vtemp*\/fnoreg,\n+       L_success, L_failure, set_cond_codes);\n+  } else {\n+    check_klass_subtype_slow_path_linear\n+      (sub_klass, super_klass, temp_reg, temp2_reg, L_success, L_failure, set_cond_codes);\n+  }\n+}\n+\n+\n@@ -1647,10 +1725,9 @@\n-\/\/ Return true: we succeeded in generating this code\n-bool MacroAssembler::lookup_secondary_supers_table(Register r_sub_klass,\n-                                                   Register r_super_klass,\n-                                                   Register temp1,\n-                                                   Register temp2,\n-                                                   Register temp3,\n-                                                   FloatRegister vtemp,\n-                                                   Register result,\n-                                                   u1 super_klass_slot,\n-                                                   bool stub_is_near) {\n+bool MacroAssembler::lookup_secondary_supers_table_const(Register r_sub_klass,\n+                                                         Register r_super_klass,\n+                                                         Register temp1,\n+                                                         Register temp2,\n+                                                         Register temp3,\n+                                                         FloatRegister vtemp,\n+                                                         Register result,\n+                                                         u1 super_klass_slot,\n+                                                         bool stub_is_near) {\n@@ -1678,1 +1755,1 @@\n-  ldr(r_bitmap, Address(r_sub_klass, Klass::bitmap_offset()));\n+  ldr(r_bitmap, Address(r_sub_klass, Klass::secondary_supers_bitmap_offset()));\n@@ -1680,1 +1757,1 @@\n-    ldrd(vtemp, Address(r_sub_klass, Klass::bitmap_offset()));\n+    ldrd(vtemp, Address(r_sub_klass, Klass::secondary_supers_bitmap_offset()));\n@@ -1744,0 +1821,107 @@\n+\/\/ At runtime, return 0 in result if r_super_klass is a superclass of\n+\/\/ r_sub_klass, otherwise return nonzero. Use this version of\n+\/\/ lookup_secondary_supers_table() if you don't know ahead of time\n+\/\/ which superclass will be searched for. Used by interpreter and\n+\/\/ runtime stubs. It is larger and has somewhat greater latency than\n+\/\/ the version above, which takes a constant super_klass_slot.\n+void MacroAssembler::lookup_secondary_supers_table_var(Register r_sub_klass,\n+                                                       Register r_super_klass,\n+                                                       Register temp1,\n+                                                       Register temp2,\n+                                                       Register temp3,\n+                                                       FloatRegister vtemp,\n+                                                       Register result,\n+                                                       Label *L_success) {\n+  assert_different_registers(r_sub_klass, temp1, temp2, temp3, result, rscratch1, rscratch2);\n+\n+  Label L_fallthrough;\n+\n+  BLOCK_COMMENT(\"lookup_secondary_supers_table {\");\n+\n+  const Register\n+    r_array_index = temp3,\n+    slot          = rscratch1,\n+    r_bitmap      = rscratch2;\n+\n+  ldrb(slot, Address(r_super_klass, Klass::hash_slot_offset()));\n+\n+  \/\/ Make sure that result is nonzero if the test below misses.\n+  mov(result, 1);\n+\n+  ldr(r_bitmap, Address(r_sub_klass, Klass::secondary_supers_bitmap_offset()));\n+\n+  \/\/ First check the bitmap to see if super_klass might be present. If\n+  \/\/ the bit is zero, we are certain that super_klass is not one of\n+  \/\/ the secondary supers.\n+\n+  \/\/ This next instruction is equivalent to:\n+  \/\/ mov(tmp_reg, (u1)(Klass::SECONDARY_SUPERS_TABLE_SIZE - 1));\n+  \/\/ sub(temp2, tmp_reg, slot);\n+  eor(temp2, slot, (u1)(Klass::SECONDARY_SUPERS_TABLE_SIZE - 1));\n+  lslv(temp2, r_bitmap, temp2);\n+  tbz(temp2, Klass::SECONDARY_SUPERS_TABLE_SIZE - 1, L_fallthrough);\n+\n+  bool must_save_v0 = (vtemp == fnoreg);\n+  if (must_save_v0) {\n+    \/\/ temp1 and result are free, so use them to preserve vtemp\n+    vtemp = v0;\n+    mov(temp1,  vtemp, D, 0);\n+    mov(result, vtemp, D, 1);\n+  }\n+\n+  \/\/ Get the first array index that can contain super_klass into r_array_index.\n+  mov(vtemp, D, 0, temp2);\n+  cnt(vtemp, T8B, vtemp);\n+  addv(vtemp, T8B, vtemp);\n+  mov(r_array_index, vtemp, D, 0);\n+\n+  if (must_save_v0) {\n+    mov(vtemp, D, 0, temp1 );\n+    mov(vtemp, D, 1, result);\n+  }\n+\n+  \/\/ NB! r_array_index is off by 1. It is compensated by keeping r_array_base off by 1 word.\n+\n+  const Register\n+    r_array_base   = temp1,\n+    r_array_length = temp2;\n+\n+  \/\/ The value i in r_array_index is >= 1, so even though r_array_base\n+  \/\/ points to the length, we don't need to adjust it to point to the\n+  \/\/ data.\n+  assert(Array<Klass*>::base_offset_in_bytes() == wordSize, \"Adjust this code\");\n+  assert(Array<Klass*>::length_offset_in_bytes() == 0, \"Adjust this code\");\n+\n+  \/\/ We will consult the secondary-super array.\n+  ldr(r_array_base, Address(r_sub_klass, in_bytes(Klass::secondary_supers_offset())));\n+\n+  ldr(result, Address(r_array_base, r_array_index, Address::lsl(LogBytesPerWord)));\n+  eor(result, result, r_super_klass);\n+  cbz(result, L_success ? *L_success : L_fallthrough); \/\/ Found a match\n+\n+  \/\/ Is there another entry to check? Consult the bitmap.\n+  rorv(r_bitmap, r_bitmap, slot);\n+  \/\/ rol(r_bitmap, r_bitmap, 1);\n+  tbz(r_bitmap, 1, L_fallthrough);\n+\n+  \/\/ The slot we just inspected is at secondary_supers[r_array_index - 1].\n+  \/\/ The next slot to be inspected, by the logic we're about to call,\n+  \/\/ is secondary_supers[r_array_index]. Bits 0 and 1 in the bitmap\n+  \/\/ have been checked.\n+  lookup_secondary_supers_table_slow_path(r_super_klass, r_array_base, r_array_index,\n+                                          r_bitmap, r_array_length, result, \/*is_stub*\/false);\n+\n+  BLOCK_COMMENT(\"} lookup_secondary_supers_table\");\n+\n+  bind(L_fallthrough);\n+\n+  if (VerifySecondarySupers) {\n+    verify_secondary_supers_table(r_sub_klass, r_super_klass, \/\/ r4, r0\n+                                  temp1, temp2, result);      \/\/ r1, r2, r5\n+  }\n+\n+  if (L_success) {\n+    cbz(result, *L_success);\n+  }\n+}\n+\n@@ -1752,1 +1936,2 @@\n-                                                             Register result) {\n+                                                             Register result,\n+                                                             bool is_stub) {\n@@ -1759,1 +1944,3 @@\n-  LOOKUP_SECONDARY_SUPERS_TABLE_REGISTERS;\n+  if (is_stub) {\n+    LOOKUP_SECONDARY_SUPERS_TABLE_REGISTERS;\n+  }\n@@ -1784,2 +1971,4 @@\n-    \/\/ The check above guarantees there are 0s in the bitmap, so the loop\n-    \/\/ eventually terminates.\n+    \/\/ As long as the bitmap is not completely full,\n+    \/\/ array_length == popcount(bitmap). The array_length check above\n+    \/\/ guarantees there are 0s in the bitmap, so the loop eventually\n+    \/\/ terminates.\n@@ -1831,2 +2020,0 @@\n-  LOOKUP_SECONDARY_SUPERS_TABLE_REGISTERS;\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":246,"deletions":59,"binary":false,"changes":305,"status":"modified"},{"patch":"@@ -1061,1 +1061,1 @@\n-                RegisterOrConstant super_check_offset = RegisterOrConstant(-1));\n+                                     Register super_check_offset = noreg);\n@@ -1076,0 +1076,37 @@\n+  void check_klass_subtype_slow_path_linear(Register sub_klass,\n+                                            Register super_klass,\n+                                            Register temp_reg,\n+                                            Register temp2_reg,\n+                                            Label* L_success,\n+                                            Label* L_failure,\n+                                            bool set_cond_codes = false);\n+\n+  void check_klass_subtype_slow_path_table(Register sub_klass,\n+                                           Register super_klass,\n+                                           Register temp_reg,\n+                                           Register temp2_reg,\n+                                           Register temp3_reg,\n+                                           Register result_reg,\n+                                           FloatRegister vtemp_reg,\n+                                           Label* L_success,\n+                                           Label* L_failure,\n+                                           bool set_cond_codes = false);\n+\n+  \/\/ If r is valid, return r.\n+  \/\/ If r is invalid, remove a register r2 from available_regs, add r2\n+  \/\/ to regs_to_push, then return r2.\n+  Register allocate_if_noreg(const Register r,\n+                             RegSetIterator<Register> &available_regs,\n+                             RegSet &regs_to_push);\n+\n+  \/\/ Secondary subtype checking\n+  void lookup_secondary_supers_table_var(Register sub_klass,\n+                                         Register r_super_klass,\n+                                         Register temp1,\n+                                         Register temp2,\n+                                         Register temp3,\n+                                         FloatRegister vtemp,\n+                                         Register result,\n+                                         Label *L_success);\n+\n+\n@@ -1078,9 +1115,9 @@\n-  bool lookup_secondary_supers_table(Register r_sub_klass,\n-                                     Register r_super_klass,\n-                                     Register temp1,\n-                                     Register temp2,\n-                                     Register temp3,\n-                                     FloatRegister vtemp,\n-                                     Register result,\n-                                     u1 super_klass_slot,\n-                                     bool stub_is_near = false);\n+  bool lookup_secondary_supers_table_const(Register r_sub_klass,\n+                                           Register r_super_klass,\n+                                           Register temp1,\n+                                           Register temp2,\n+                                           Register temp3,\n+                                           FloatRegister vtemp,\n+                                           Register result,\n+                                           u1 super_klass_slot,\n+                                           bool stub_is_near = false);\n@@ -1099,1 +1136,2 @@\n-                                               Register result);\n+                                               Register result,\n+                                               bool is_stub = true);\n@@ -1523,0 +1561,18 @@\n+\/\/ Ensure that the inline code and the stub use the same registers.\n+#define ARRAYS_HASHCODE_REGISTERS \\\n+  do {                      \\\n+    assert(result == r0  && \\\n+           ary    == r1  && \\\n+           cnt    == r2  && \\\n+           vdata0 == v3  && \\\n+           vdata1 == v2  && \\\n+           vdata2 == v1  && \\\n+           vdata3 == v0  && \\\n+           vmul0  == v4  && \\\n+           vmul1  == v5  && \\\n+           vmul2  == v6  && \\\n+           vmul3  == v7  && \\\n+           vpow   == v12 && \\\n+           vpowm  == v13, \"registers must match aarch64.ad\"); \\\n+  } while (0)\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":67,"deletions":11,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -1149,1 +1149,14 @@\n-  Unimplemented();\n+  \/\/ More than 8 argument inputs are not supported now.\n+  assert(total_args_passed <= Argument::n_float_register_parameters_c, \"unsupported\");\n+  assert(num_bits >= 64 && num_bits <= 2048 && is_power_of_2(num_bits), \"unsupported\");\n+\n+  static const FloatRegister VEC_ArgReg[Argument::n_float_register_parameters_c] = {\n+    v0, v1, v2, v3, v4, v5, v6, v7\n+  };\n+\n+  \/\/ On SVE, we use the same vector registers with 128-bit vector registers on NEON.\n+  int next_reg_val = num_bits == 64 ? 1 : 3;\n+  for (uint i = 0; i < total_args_passed; i++) {\n+    VMReg vmreg = VEC_ArgReg[i]->as_VMReg();\n+    regs[i].set_pair(vmreg->next(next_reg_val), vmreg);\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":14,"deletions":1,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -56,0 +57,1 @@\n+#include \"utilities\/debug.hpp\"\n@@ -57,0 +59,1 @@\n+#include \"utilities\/intpow.hpp\"\n@@ -1858,0 +1861,3 @@\n+                           Register temp1,\n+                           Register temp2,\n+                           Register result,\n@@ -1867,1 +1873,1 @@\n-    __ check_klass_subtype_slow_path(sub_klass, super_klass, noreg, noreg, &L_success, nullptr);\n+    __ check_klass_subtype_slow_path(sub_klass, super_klass, temp1, temp2, &L_success, nullptr);\n@@ -2003,1 +2009,11 @@\n-    generate_type_check(r19_klass, ckoff, ckval, L_store_element);\n+\n+    BLOCK_COMMENT(\"type_check:\");\n+    generate_type_check(\/*sub_klass*\/r19_klass,\n+                        \/*super_check_offset*\/ckoff,\n+                        \/*super_klass*\/ckval,\n+                        \/*r_array_base*\/gct1,\n+                        \/*temp2*\/gct2,\n+                        \/*result*\/r10, L_store_element);\n+\n+    \/\/ Fall through on failure!\n+\n@@ -2012,1 +2028,1 @@\n-    __ eon(count, count, zr);                   \/\/ report (-1^K) to caller\n+    __ eon(count, count, zr);              \/\/ report (-1^K) to caller\n@@ -2385,1 +2401,2 @@\n-      generate_type_check(scratch_src_klass, sco_temp, dst_klass, L_plain_copy);\n+      generate_type_check(scratch_src_klass, sco_temp, dst_klass, \/*temps*\/ noreg, noreg, noreg,\n+                          L_plain_copy);\n@@ -5344,0 +5361,301 @@\n+  \/\/ result = r0 - return value. Contains initial hashcode value on entry.\n+  \/\/ ary = r1 - array address\n+  \/\/ cnt = r2 - elements count\n+  \/\/ Clobbers: v0-v13, rscratch1, rscratch2\n+  address generate_large_arrays_hashcode(BasicType eltype) {\n+    const Register result = r0, ary = r1, cnt = r2;\n+    const FloatRegister vdata0 = v3, vdata1 = v2, vdata2 = v1, vdata3 = v0;\n+    const FloatRegister vmul0 = v4, vmul1 = v5, vmul2 = v6, vmul3 = v7;\n+    const FloatRegister vpow = v12;  \/\/ powers of 31: <31^3, ..., 31^0>\n+    const FloatRegister vpowm = v13;\n+\n+    ARRAYS_HASHCODE_REGISTERS;\n+\n+    Label SMALL_LOOP, LARGE_LOOP_PREHEADER, LARGE_LOOP, TAIL, TAIL_SHORTCUT, BR_BASE;\n+\n+    unsigned int vf; \/\/ vectorization factor\n+    bool multiply_by_halves;\n+    Assembler::SIMD_Arrangement load_arrangement;\n+    switch (eltype) {\n+    case T_BOOLEAN:\n+    case T_BYTE:\n+      load_arrangement = Assembler::T8B;\n+      multiply_by_halves = true;\n+      vf = 8;\n+      break;\n+    case T_CHAR:\n+    case T_SHORT:\n+      load_arrangement = Assembler::T8H;\n+      multiply_by_halves = true;\n+      vf = 8;\n+      break;\n+    case T_INT:\n+      load_arrangement = Assembler::T4S;\n+      multiply_by_halves = false;\n+      vf = 4;\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+\n+    \/\/ Unroll factor\n+    const unsigned uf = 4;\n+\n+    \/\/ Effective vectorization factor\n+    const unsigned evf = vf * uf;\n+\n+    __ align(CodeEntryAlignment);\n+\n+    const char *mark_name = \"\";\n+    switch (eltype) {\n+    case T_BOOLEAN:\n+      mark_name = \"_large_arrays_hashcode_boolean\";\n+      break;\n+    case T_BYTE:\n+      mark_name = \"_large_arrays_hashcode_byte\";\n+      break;\n+    case T_CHAR:\n+      mark_name = \"_large_arrays_hashcode_char\";\n+      break;\n+    case T_SHORT:\n+      mark_name = \"_large_arrays_hashcode_short\";\n+      break;\n+    case T_INT:\n+      mark_name = \"_large_arrays_hashcode_int\";\n+      break;\n+    default:\n+      mark_name = \"_large_arrays_hashcode_incorrect_type\";\n+      __ should_not_reach_here();\n+    };\n+\n+    StubCodeMark mark(this, \"StubRoutines\", mark_name);\n+\n+    address entry = __ pc();\n+    __ enter();\n+\n+    \/\/ Put 0-3'th powers of 31 into a single SIMD register together. The register will be used in\n+    \/\/ the SMALL and LARGE LOOPS' epilogues. The initialization is hoisted here and the register's\n+    \/\/ value shouldn't change throughout both loops.\n+    __ movw(rscratch1, intpow(31U, 3));\n+    __ mov(vpow, Assembler::S, 0, rscratch1);\n+    __ movw(rscratch1, intpow(31U, 2));\n+    __ mov(vpow, Assembler::S, 1, rscratch1);\n+    __ movw(rscratch1, intpow(31U, 1));\n+    __ mov(vpow, Assembler::S, 2, rscratch1);\n+    __ movw(rscratch1, intpow(31U, 0));\n+    __ mov(vpow, Assembler::S, 3, rscratch1);\n+\n+    __ mov(vmul0, Assembler::T16B, 0);\n+    __ mov(vmul0, Assembler::S, 3, result);\n+\n+    __ andr(rscratch2, cnt, (uf - 1) * vf);\n+    __ cbz(rscratch2, LARGE_LOOP_PREHEADER);\n+\n+    __ movw(rscratch1, intpow(31U, multiply_by_halves ? vf \/ 2 : vf));\n+    __ mov(vpowm, Assembler::S, 0, rscratch1);\n+\n+    \/\/ SMALL LOOP\n+    __ bind(SMALL_LOOP);\n+\n+    __ ld1(vdata0, load_arrangement, Address(__ post(ary, vf * type2aelembytes(eltype))));\n+    __ mulvs(vmul0, Assembler::T4S, vmul0, vpowm, 0);\n+    __ subsw(rscratch2, rscratch2, vf);\n+\n+    if (load_arrangement == Assembler::T8B) {\n+      \/\/ Extend 8B to 8H to be able to use vector multiply\n+      \/\/ instructions\n+      assert(load_arrangement == Assembler::T8B, \"expected to extend 8B to 8H\");\n+      if (is_signed_subword_type(eltype)) {\n+        __ sxtl(vdata0, Assembler::T8H, vdata0, load_arrangement);\n+      } else {\n+        __ uxtl(vdata0, Assembler::T8H, vdata0, load_arrangement);\n+      }\n+    }\n+\n+    switch (load_arrangement) {\n+    case Assembler::T4S:\n+      __ addv(vmul0, load_arrangement, vmul0, vdata0);\n+      break;\n+    case Assembler::T8B:\n+    case Assembler::T8H:\n+      assert(is_subword_type(eltype), \"subword type expected\");\n+      if (is_signed_subword_type(eltype)) {\n+        __ saddwv(vmul0, vmul0, Assembler::T4S, vdata0, Assembler::T4H);\n+      } else {\n+        __ uaddwv(vmul0, vmul0, Assembler::T4S, vdata0, Assembler::T4H);\n+      }\n+      break;\n+    default:\n+      __ should_not_reach_here();\n+    }\n+\n+    \/\/ Process the upper half of a vector\n+    if (load_arrangement == Assembler::T8B || load_arrangement == Assembler::T8H) {\n+      __ mulvs(vmul0, Assembler::T4S, vmul0, vpowm, 0);\n+      if (is_signed_subword_type(eltype)) {\n+        __ saddwv2(vmul0, vmul0, Assembler::T4S, vdata0, Assembler::T8H);\n+      } else {\n+        __ uaddwv2(vmul0, vmul0, Assembler::T4S, vdata0, Assembler::T8H);\n+      }\n+    }\n+\n+    __ br(Assembler::HI, SMALL_LOOP);\n+\n+    \/\/ SMALL LOOP'S EPILOQUE\n+    __ lsr(rscratch2, cnt, exact_log2(evf));\n+    __ cbnz(rscratch2, LARGE_LOOP_PREHEADER);\n+\n+    __ mulv(vmul0, Assembler::T4S, vmul0, vpow);\n+    __ addv(vmul0, Assembler::T4S, vmul0);\n+    __ umov(result, vmul0, Assembler::S, 0);\n+\n+    \/\/ TAIL\n+    __ bind(TAIL);\n+\n+    \/\/ The andr performs cnt % vf. The subtract shifted by 3 offsets past vf - 1 - (cnt % vf) pairs\n+    \/\/ of load + madd insns i.e. it only executes cnt % vf load + madd pairs.\n+    assert(is_power_of_2(vf), \"can't use this value to calculate the jump target PC\");\n+    __ andr(rscratch2, cnt, vf - 1);\n+    __ bind(TAIL_SHORTCUT);\n+    __ adr(rscratch1, BR_BASE);\n+    __ sub(rscratch1, rscratch1, rscratch2, ext::uxtw, 3);\n+    __ movw(rscratch2, 0x1f);\n+    __ br(rscratch1);\n+\n+    for (size_t i = 0; i < vf - 1; ++i) {\n+      __ load(rscratch1, Address(__ post(ary, type2aelembytes(eltype))),\n+                                   eltype);\n+      __ maddw(result, result, rscratch2, rscratch1);\n+    }\n+    __ bind(BR_BASE);\n+\n+    __ leave();\n+    __ ret(lr);\n+\n+    \/\/ LARGE LOOP\n+    __ bind(LARGE_LOOP_PREHEADER);\n+\n+    __ lsr(rscratch2, cnt, exact_log2(evf));\n+\n+    if (multiply_by_halves) {\n+      \/\/ 31^4 - multiplier between lower and upper parts of a register\n+      __ movw(rscratch1, intpow(31U, vf \/ 2));\n+      __ mov(vpowm, Assembler::S, 1, rscratch1);\n+      \/\/ 31^28 - remainder of the iteraion multiplier, 28 = 32 - 4\n+      __ movw(rscratch1, intpow(31U, evf - vf \/ 2));\n+      __ mov(vpowm, Assembler::S, 0, rscratch1);\n+    } else {\n+      \/\/ 31^16\n+      __ movw(rscratch1, intpow(31U, evf));\n+      __ mov(vpowm, Assembler::S, 0, rscratch1);\n+    }\n+\n+    __ mov(vmul3, Assembler::T16B, 0);\n+    __ mov(vmul2, Assembler::T16B, 0);\n+    __ mov(vmul1, Assembler::T16B, 0);\n+\n+    __ bind(LARGE_LOOP);\n+\n+    __ mulvs(vmul3, Assembler::T4S, vmul3, vpowm, 0);\n+    __ mulvs(vmul2, Assembler::T4S, vmul2, vpowm, 0);\n+    __ mulvs(vmul1, Assembler::T4S, vmul1, vpowm, 0);\n+    __ mulvs(vmul0, Assembler::T4S, vmul0, vpowm, 0);\n+\n+    __ ld1(vdata3, vdata2, vdata1, vdata0, load_arrangement,\n+           Address(__ post(ary, evf * type2aelembytes(eltype))));\n+\n+    if (load_arrangement == Assembler::T8B) {\n+      \/\/ Extend 8B to 8H to be able to use vector multiply\n+      \/\/ instructions\n+      assert(load_arrangement == Assembler::T8B, \"expected to extend 8B to 8H\");\n+      if (is_signed_subword_type(eltype)) {\n+        __ sxtl(vdata3, Assembler::T8H, vdata3, load_arrangement);\n+        __ sxtl(vdata2, Assembler::T8H, vdata2, load_arrangement);\n+        __ sxtl(vdata1, Assembler::T8H, vdata1, load_arrangement);\n+        __ sxtl(vdata0, Assembler::T8H, vdata0, load_arrangement);\n+      } else {\n+        __ uxtl(vdata3, Assembler::T8H, vdata3, load_arrangement);\n+        __ uxtl(vdata2, Assembler::T8H, vdata2, load_arrangement);\n+        __ uxtl(vdata1, Assembler::T8H, vdata1, load_arrangement);\n+        __ uxtl(vdata0, Assembler::T8H, vdata0, load_arrangement);\n+      }\n+    }\n+\n+    switch (load_arrangement) {\n+    case Assembler::T4S:\n+      __ addv(vmul3, load_arrangement, vmul3, vdata3);\n+      __ addv(vmul2, load_arrangement, vmul2, vdata2);\n+      __ addv(vmul1, load_arrangement, vmul1, vdata1);\n+      __ addv(vmul0, load_arrangement, vmul0, vdata0);\n+      break;\n+    case Assembler::T8B:\n+    case Assembler::T8H:\n+      assert(is_subword_type(eltype), \"subword type expected\");\n+      if (is_signed_subword_type(eltype)) {\n+        __ saddwv(vmul3, vmul3, Assembler::T4S, vdata3, Assembler::T4H);\n+        __ saddwv(vmul2, vmul2, Assembler::T4S, vdata2, Assembler::T4H);\n+        __ saddwv(vmul1, vmul1, Assembler::T4S, vdata1, Assembler::T4H);\n+        __ saddwv(vmul0, vmul0, Assembler::T4S, vdata0, Assembler::T4H);\n+      } else {\n+        __ uaddwv(vmul3, vmul3, Assembler::T4S, vdata3, Assembler::T4H);\n+        __ uaddwv(vmul2, vmul2, Assembler::T4S, vdata2, Assembler::T4H);\n+        __ uaddwv(vmul1, vmul1, Assembler::T4S, vdata1, Assembler::T4H);\n+        __ uaddwv(vmul0, vmul0, Assembler::T4S, vdata0, Assembler::T4H);\n+      }\n+      break;\n+    default:\n+      __ should_not_reach_here();\n+    }\n+\n+    \/\/ Process the upper half of a vector\n+    if (load_arrangement == Assembler::T8B || load_arrangement == Assembler::T8H) {\n+      __ mulvs(vmul3, Assembler::T4S, vmul3, vpowm, 1);\n+      __ mulvs(vmul2, Assembler::T4S, vmul2, vpowm, 1);\n+      __ mulvs(vmul1, Assembler::T4S, vmul1, vpowm, 1);\n+      __ mulvs(vmul0, Assembler::T4S, vmul0, vpowm, 1);\n+      if (is_signed_subword_type(eltype)) {\n+        __ saddwv2(vmul3, vmul3, Assembler::T4S, vdata3, Assembler::T8H);\n+        __ saddwv2(vmul2, vmul2, Assembler::T4S, vdata2, Assembler::T8H);\n+        __ saddwv2(vmul1, vmul1, Assembler::T4S, vdata1, Assembler::T8H);\n+        __ saddwv2(vmul0, vmul0, Assembler::T4S, vdata0, Assembler::T8H);\n+      } else {\n+        __ uaddwv2(vmul3, vmul3, Assembler::T4S, vdata3, Assembler::T8H);\n+        __ uaddwv2(vmul2, vmul2, Assembler::T4S, vdata2, Assembler::T8H);\n+        __ uaddwv2(vmul1, vmul1, Assembler::T4S, vdata1, Assembler::T8H);\n+        __ uaddwv2(vmul0, vmul0, Assembler::T4S, vdata0, Assembler::T8H);\n+      }\n+    }\n+\n+    __ subsw(rscratch2, rscratch2, 1);\n+    __ br(Assembler::HI, LARGE_LOOP);\n+\n+    __ mulv(vmul3, Assembler::T4S, vmul3, vpow);\n+    __ addv(vmul3, Assembler::T4S, vmul3);\n+    __ umov(result, vmul3, Assembler::S, 0);\n+\n+    __ mov(rscratch2, intpow(31U, vf));\n+\n+    __ mulv(vmul2, Assembler::T4S, vmul2, vpow);\n+    __ addv(vmul2, Assembler::T4S, vmul2);\n+    __ umov(rscratch1, vmul2, Assembler::S, 0);\n+    __ maddw(result, result, rscratch2, rscratch1);\n+\n+    __ mulv(vmul1, Assembler::T4S, vmul1, vpow);\n+    __ addv(vmul1, Assembler::T4S, vmul1);\n+    __ umov(rscratch1, vmul1, Assembler::S, 0);\n+    __ maddw(result, result, rscratch2, rscratch1);\n+\n+    __ mulv(vmul0, Assembler::T4S, vmul0, vpow);\n+    __ addv(vmul0, Assembler::T4S, vmul0);\n+    __ umov(rscratch1, vmul0, Assembler::S, 0);\n+    __ maddw(result, result, rscratch2, rscratch1);\n+\n+    __ andr(rscratch2, cnt, vf - 1);\n+    __ cbnz(rscratch2, TAIL_SHORTCUT);\n+\n+    __ leave();\n+    __ ret(lr);\n+\n+    return entry;\n+  }\n+\n@@ -6818,4 +7136,4 @@\n-    __ lookup_secondary_supers_table(r_sub_klass, r_super_klass,\n-                                     r_array_base, r_array_length, r_array_index,\n-                                     vtemp, result, super_klass_index,\n-                                     \/*stub_is_near*\/true);\n+    __ lookup_secondary_supers_table_const(r_sub_klass, r_super_klass,\n+                                           r_array_base, r_array_length, r_array_index,\n+                                           vtemp, result, super_klass_index,\n+                                           \/*stub_is_near*\/true);\n@@ -8201,0 +8519,72 @@\n+  void generate_vector_math_stubs() {\n+    \/\/ Get native vector math stub routine addresses\n+    void* libsleef = nullptr;\n+    char ebuf[1024];\n+    char dll_name[JVM_MAXPATHLEN];\n+    if (os::dll_locate_lib(dll_name, sizeof(dll_name), Arguments::get_dll_dir(), \"sleef\")) {\n+      libsleef = os::dll_load(dll_name, ebuf, sizeof ebuf);\n+    }\n+    if (libsleef == nullptr) {\n+      log_info(library)(\"Failed to load native vector math library, %s!\", ebuf);\n+      return;\n+    }\n+    \/\/ Method naming convention\n+    \/\/   All the methods are named as <OP><T><N>_<U><suffix>\n+    \/\/   Where:\n+    \/\/     <OP>     is the operation name, e.g. sin\n+    \/\/     <T>      is optional to indicate float\/double\n+    \/\/              \"f\/d\" for vector float\/double operation\n+    \/\/     <N>      is the number of elements in the vector\n+    \/\/              \"2\/4\" for neon, and \"x\" for sve\n+    \/\/     <U>      is the precision level\n+    \/\/              \"u10\/u05\" represents 1.0\/0.5 ULP error bounds\n+    \/\/               We use \"u10\" for all operations by default\n+    \/\/               But for those functions do not have u10 support, we use \"u05\" instead\n+    \/\/     <suffix> indicates neon\/sve\n+    \/\/              \"sve\/advsimd\" for sve\/neon implementations\n+    \/\/     e.g. sinfx_u10sve is the method for computing vector float sin using SVE instructions\n+    \/\/          cosd2_u10advsimd is the method for computing 2 elements vector double cos using NEON instructions\n+    \/\/\n+    log_info(library)(\"Loaded library %s, handle \" INTPTR_FORMAT, JNI_LIB_PREFIX \"sleef\" JNI_LIB_SUFFIX, p2i(libsleef));\n+\n+    \/\/ Math vector stubs implemented with SVE for scalable vector size.\n+    if (UseSVE > 0) {\n+      for (int op = 0; op < VectorSupport::NUM_VECTOR_OP_MATH; op++) {\n+        int vop = VectorSupport::VECTOR_OP_MATH_START + op;\n+        \/\/ Skip \"tanh\" because there is performance regression\n+        if (vop == VectorSupport::VECTOR_OP_TANH) {\n+          continue;\n+        }\n+\n+        \/\/ The native library does not support u10 level of \"hypot\".\n+        const char* ulf = (vop == VectorSupport::VECTOR_OP_HYPOT) ? \"u05\" : \"u10\";\n+\n+        snprintf(ebuf, sizeof(ebuf), \"%sfx_%ssve\", VectorSupport::mathname[op], ulf);\n+        StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_SCALABLE][op] = (address)os::dll_lookup(libsleef, ebuf);\n+\n+        snprintf(ebuf, sizeof(ebuf), \"%sdx_%ssve\", VectorSupport::mathname[op], ulf);\n+        StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_SCALABLE][op] = (address)os::dll_lookup(libsleef, ebuf);\n+      }\n+    }\n+\n+    \/\/ Math vector stubs implemented with NEON for 64\/128 bits vector size.\n+    for (int op = 0; op < VectorSupport::NUM_VECTOR_OP_MATH; op++) {\n+      int vop = VectorSupport::VECTOR_OP_MATH_START + op;\n+      \/\/ Skip \"tanh\" because there is performance regression\n+      if (vop == VectorSupport::VECTOR_OP_TANH) {\n+        continue;\n+      }\n+\n+      \/\/ The native library does not support u10 level of \"hypot\".\n+      const char* ulf = (vop == VectorSupport::VECTOR_OP_HYPOT) ? \"u05\" : \"u10\";\n+\n+      snprintf(ebuf, sizeof(ebuf), \"%sf4_%sadvsimd\", VectorSupport::mathname[op], ulf);\n+      StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_64][op] = (address)os::dll_lookup(libsleef, ebuf);\n+\n+      snprintf(ebuf, sizeof(ebuf), \"%sf4_%sadvsimd\", VectorSupport::mathname[op], ulf);\n+      StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_128][op] = (address)os::dll_lookup(libsleef, ebuf);\n+\n+      snprintf(ebuf, sizeof(ebuf), \"%sd2_%sadvsimd\", VectorSupport::mathname[op], ulf);\n+      StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_128][op] = (address)os::dll_lookup(libsleef, ebuf);\n+    }\n+  }\n@@ -8449,0 +8839,7 @@\n+    \/\/ arrays_hascode stub for large arrays.\n+    StubRoutines::aarch64::_large_arrays_hashcode_boolean = generate_large_arrays_hashcode(T_BOOLEAN);\n+    StubRoutines::aarch64::_large_arrays_hashcode_byte = generate_large_arrays_hashcode(T_BYTE);\n+    StubRoutines::aarch64::_large_arrays_hashcode_char = generate_large_arrays_hashcode(T_CHAR);\n+    StubRoutines::aarch64::_large_arrays_hashcode_int = generate_large_arrays_hashcode(T_INT);\n+    StubRoutines::aarch64::_large_arrays_hashcode_short = generate_large_arrays_hashcode(T_SHORT);\n+\n@@ -8490,0 +8887,3 @@\n+\n+    generate_vector_math_stubs();\n+\n@@ -8545,0 +8945,1 @@\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":409,"deletions":8,"binary":false,"changes":417,"status":"modified"},{"patch":"@@ -995,13 +995,9 @@\n-  \/\/ T1 = -1\n-  vpcmpeqq(xtmp1, xtmp1, xtmp1, vlen_enc);\n-  \/\/ T1 = -1 << 63\n-  vpsllq(xtmp1, xtmp1, 63, vlen_enc);\n-  \/\/ Convert SRC2 to signed value i.e. T2 = T1 + SRC2\n-  vpaddq(xtmp2, xtmp1, src2, vlen_enc);\n-  \/\/ Convert SRC1 to signed value i.e. T1 = T1 + SRC1\n-  vpaddq(xtmp1, xtmp1, src1, vlen_enc);\n-  \/\/ Mask = T2 > T1\n-  vpcmpgtq(xtmp1, xtmp2, xtmp1, vlen_enc);\n-  if (opcode == Op_UMaxV) {\n-    \/\/ Res = Mask ? Src2 : Src1\n-    vpblendvb(dst, src1, src2, xtmp1, vlen_enc);\n+  \/\/ For optimality, leverage a full vector width of 512 bits\n+  \/\/ for operations over smaller vector sizes on AVX512 targets.\n+  if (VM_Version::supports_evex() && !VM_Version::supports_avx512vl()) {\n+    if (opcode == Op_UMaxV) {\n+      evpmaxuq(dst, k0, src1, src2, false, Assembler::AVX_512bit);\n+    } else {\n+      assert(opcode == Op_UMinV, \"required\");\n+      evpminuq(dst, k0, src1, src2, false, Assembler::AVX_512bit);\n+    }\n@@ -1009,2 +1005,17 @@\n-    \/\/ Res = Mask ? Src1 : Src2\n-    vpblendvb(dst, src2, src1, xtmp1, vlen_enc);\n+    \/\/ T1 = -1\n+    vpcmpeqq(xtmp1, xtmp1, xtmp1, vlen_enc);\n+    \/\/ T1 = -1 << 63\n+    vpsllq(xtmp1, xtmp1, 63, vlen_enc);\n+    \/\/ Convert SRC2 to signed value i.e. T2 = T1 + SRC2\n+    vpaddq(xtmp2, xtmp1, src2, vlen_enc);\n+    \/\/ Convert SRC1 to signed value i.e. T1 = T1 + SRC1\n+    vpaddq(xtmp1, xtmp1, src1, vlen_enc);\n+    \/\/ Mask = T2 > T1\n+    vpcmpgtq(xtmp1, xtmp2, xtmp1, vlen_enc);\n+    if (opcode == Op_UMaxV) {\n+      \/\/ Res = Mask ? Src2 : Src1\n+      vpblendvb(dst, src1, src2, xtmp1, vlen_enc);\n+    } else {\n+      \/\/ Res = Mask ? Src1 : Src2\n+      vpblendvb(dst, src2, src1, xtmp1, vlen_enc);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":26,"deletions":15,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -45,10 +45,0 @@\n-#ifdef AMD64\n-#ifdef _WIN64\n-  _num_args = (method->is_static() ? 1 : 0);\n-  _stack_offset = (Argument::n_int_register_parameters_c+1)* wordSize; \/\/ don't overwrite return address\n-#else\n-  _num_int_args = (method->is_static() ? 1 : 0);\n-  _num_fp_args = 0;\n-  _stack_offset = wordSize; \/\/ don't overwrite return address\n-#endif \/\/ _WIN64\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/interpreterRT_x86_32.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -4976,7 +4976,7 @@\n-void MacroAssembler::check_klass_subtype_slow_path(Register sub_klass,\n-                                                   Register super_klass,\n-                                                   Register temp_reg,\n-                                                   Register temp2_reg,\n-                                                   Label* L_success,\n-                                                   Label* L_failure,\n-                                                   bool set_cond_codes) {\n+void MacroAssembler::check_klass_subtype_slow_path_linear(Register sub_klass,\n+                                                          Register super_klass,\n+                                                          Register temp_reg,\n+                                                          Register temp2_reg,\n+                                                          Label* L_success,\n+                                                          Label* L_failure,\n+                                                          bool set_cond_codes) {\n@@ -5068,1 +5068,122 @@\n-#ifdef _LP64\n+#ifndef _LP64\n+\n+\/\/ 32-bit x86 only: always use the linear search.\n+void MacroAssembler::check_klass_subtype_slow_path(Register sub_klass,\n+                                                   Register super_klass,\n+                                                   Register temp_reg,\n+                                                   Register temp2_reg,\n+                                                   Label* L_success,\n+                                                   Label* L_failure,\n+                                                   bool set_cond_codes) {\n+  check_klass_subtype_slow_path_linear\n+    (sub_klass, super_klass, temp_reg, temp2_reg, L_success, L_failure, set_cond_codes);\n+}\n+\n+#else \/\/ _LP64\n+\n+void MacroAssembler::check_klass_subtype_slow_path(Register sub_klass,\n+                                                   Register super_klass,\n+                                                   Register temp_reg,\n+                                                   Register temp2_reg,\n+                                                   Label* L_success,\n+                                                   Label* L_failure,\n+                                                   bool set_cond_codes) {\n+  assert(set_cond_codes == false, \"must be false on 64-bit x86\");\n+  check_klass_subtype_slow_path\n+    (sub_klass, super_klass, temp_reg, temp2_reg, noreg, noreg,\n+     L_success, L_failure);\n+}\n+\n+void MacroAssembler::check_klass_subtype_slow_path(Register sub_klass,\n+                                                   Register super_klass,\n+                                                   Register temp_reg,\n+                                                   Register temp2_reg,\n+                                                   Register temp3_reg,\n+                                                   Register temp4_reg,\n+                                                   Label* L_success,\n+                                                   Label* L_failure) {\n+  if (UseSecondarySupersTable) {\n+    check_klass_subtype_slow_path_table\n+      (sub_klass, super_klass, temp_reg, temp2_reg, temp3_reg, temp4_reg,\n+       L_success, L_failure);\n+  } else {\n+    check_klass_subtype_slow_path_linear\n+      (sub_klass, super_klass, temp_reg, temp2_reg, L_success, L_failure, \/*set_cond_codes*\/false);\n+  }\n+}\n+\n+Register MacroAssembler::allocate_if_noreg(Register r,\n+                                  RegSetIterator<Register> &available_regs,\n+                                  RegSet &regs_to_push) {\n+  if (!r->is_valid()) {\n+    r = *available_regs++;\n+    regs_to_push += r;\n+  }\n+  return r;\n+}\n+\n+void MacroAssembler::check_klass_subtype_slow_path_table(Register sub_klass,\n+                                                         Register super_klass,\n+                                                         Register temp_reg,\n+                                                         Register temp2_reg,\n+                                                         Register temp3_reg,\n+                                                         Register result_reg,\n+                                                         Label* L_success,\n+                                                         Label* L_failure) {\n+  \/\/ NB! Callers may assume that, when temp2_reg is a valid register,\n+  \/\/ this code sets it to a nonzero value.\n+  bool temp2_reg_was_valid = temp2_reg->is_valid();\n+\n+  RegSet temps = RegSet::of(temp_reg, temp2_reg, temp3_reg);\n+\n+  Label L_fallthrough;\n+  int label_nulls = 0;\n+  if (L_success == nullptr)   { L_success   = &L_fallthrough; label_nulls++; }\n+  if (L_failure == nullptr)   { L_failure   = &L_fallthrough; label_nulls++; }\n+  assert(label_nulls <= 1, \"at most one null in the batch\");\n+\n+  BLOCK_COMMENT(\"check_klass_subtype_slow_path_table\");\n+\n+  RegSetIterator<Register> available_regs\n+    = (RegSet::of(rax, rcx, rdx, r8) + r9 + r10 + r11 + r12 - temps - sub_klass - super_klass).begin();\n+\n+  RegSet pushed_regs;\n+\n+  temp_reg = allocate_if_noreg(temp_reg, available_regs, pushed_regs);\n+  temp2_reg = allocate_if_noreg(temp2_reg, available_regs, pushed_regs);\n+  temp3_reg = allocate_if_noreg(temp3_reg, available_regs, pushed_regs);\n+  result_reg = allocate_if_noreg(result_reg, available_regs, pushed_regs);\n+  Register temp4_reg = allocate_if_noreg(noreg, available_regs, pushed_regs);\n+\n+  assert_different_registers(sub_klass, super_klass, temp_reg, temp2_reg, temp3_reg, result_reg);\n+\n+  {\n+\n+    int register_push_size = pushed_regs.size() * Register::max_slots_per_register * VMRegImpl::stack_slot_size;\n+    int aligned_size = align_up(register_push_size, StackAlignmentInBytes);\n+    subptr(rsp, aligned_size);\n+    push_set(pushed_regs, 0);\n+\n+    lookup_secondary_supers_table_var(sub_klass,\n+                                      super_klass,\n+                                      temp_reg, temp2_reg, temp3_reg, temp4_reg, result_reg);\n+    cmpq(result_reg, 0);\n+\n+    \/\/ Unspill the temp. registers:\n+    pop_set(pushed_regs, 0);\n+    \/\/ Increment SP but do not clobber flags.\n+    lea(rsp, Address(rsp, aligned_size));\n+  }\n+\n+  if (temp2_reg_was_valid) {\n+    movq(temp2_reg, 1);\n+  }\n+\n+  jcc(Assembler::notEqual, *L_failure);\n+\n+  if (L_success != &L_fallthrough) {\n+    jmp(*L_success);\n+  }\n+\n+  bind(L_fallthrough);\n+}\n@@ -5115,8 +5236,38 @@\n-void MacroAssembler::lookup_secondary_supers_table(Register r_sub_klass,\n-                                                   Register r_super_klass,\n-                                                   Register temp1,\n-                                                   Register temp2,\n-                                                   Register temp3,\n-                                                   Register temp4,\n-                                                   Register result,\n-                                                   u1 super_klass_slot) {\n+\/\/ Versions of salq and rorq that don't need count to be in rcx\n+\n+void MacroAssembler::salq(Register dest, Register count) {\n+  if (count == rcx) {\n+    Assembler::salq(dest);\n+  } else {\n+    assert_different_registers(rcx, dest);\n+    xchgq(rcx, count);\n+    Assembler::salq(dest);\n+    xchgq(rcx, count);\n+  }\n+}\n+\n+void MacroAssembler::rorq(Register dest, Register count) {\n+  if (count == rcx) {\n+    Assembler::rorq(dest);\n+  } else {\n+    assert_different_registers(rcx, dest);\n+    xchgq(rcx, count);\n+    Assembler::rorq(dest);\n+    xchgq(rcx, count);\n+  }\n+}\n+\n+\/\/ Return true: we succeeded in generating this code\n+\/\/\n+\/\/ At runtime, return 0 in result if r_super_klass is a superclass of\n+\/\/ r_sub_klass, otherwise return nonzero. Use this if you know the\n+\/\/ super_klass_slot of the class you're looking for. This is always\n+\/\/ the case for instanceof and checkcast.\n+void MacroAssembler::lookup_secondary_supers_table_const(Register r_sub_klass,\n+                                                         Register r_super_klass,\n+                                                         Register temp1,\n+                                                         Register temp2,\n+                                                         Register temp3,\n+                                                         Register temp4,\n+                                                         Register result,\n+                                                         u1 super_klass_slot) {\n@@ -5139,1 +5290,1 @@\n-  movq(r_bitmap, Address(r_sub_klass, Klass::bitmap_offset()));\n+  movq(r_bitmap, Address(r_sub_klass, Klass::secondary_supers_bitmap_offset()));\n@@ -5212,0 +5363,116 @@\n+\/\/ At runtime, return 0 in result if r_super_klass is a superclass of\n+\/\/ r_sub_klass, otherwise return nonzero. Use this version of\n+\/\/ lookup_secondary_supers_table() if you don't know ahead of time\n+\/\/ which superclass will be searched for. Used by interpreter and\n+\/\/ runtime stubs. It is larger and has somewhat greater latency than\n+\/\/ the version above, which takes a constant super_klass_slot.\n+void MacroAssembler::lookup_secondary_supers_table_var(Register r_sub_klass,\n+                                                       Register r_super_klass,\n+                                                       Register temp1,\n+                                                       Register temp2,\n+                                                       Register temp3,\n+                                                       Register temp4,\n+                                                       Register result) {\n+  assert_different_registers(r_sub_klass, r_super_klass, temp1, temp2, temp3, temp4, result);\n+  assert_different_registers(r_sub_klass, r_super_klass, rcx);\n+  RegSet temps = RegSet::of(temp1, temp2, temp3, temp4);\n+\n+  Label L_fallthrough, L_success, L_failure;\n+\n+  BLOCK_COMMENT(\"lookup_secondary_supers_table {\");\n+\n+  RegSetIterator<Register> available_regs = (temps - rcx).begin();\n+\n+  \/\/ FIXME. Once we are sure that all paths reaching this point really\n+  \/\/ do pass rcx as one of our temps we can get rid of the following\n+  \/\/ workaround.\n+  assert(temps.contains(rcx), \"fix this code\");\n+\n+  \/\/ We prefer to have our shift count in rcx. If rcx is one of our\n+  \/\/ temps, use it for slot. If not, pick any of our temps.\n+  Register slot;\n+  if (!temps.contains(rcx)) {\n+    slot = *available_regs++;\n+  } else {\n+    slot = rcx;\n+  }\n+\n+  const Register r_array_index = *available_regs++;\n+  const Register r_bitmap      = *available_regs++;\n+\n+  \/\/ The logic above guarantees this property, but we state it here.\n+  assert_different_registers(r_array_index, r_bitmap, rcx);\n+\n+  movq(r_bitmap, Address(r_sub_klass, Klass::secondary_supers_bitmap_offset()));\n+  movq(r_array_index, r_bitmap);\n+\n+  \/\/ First check the bitmap to see if super_klass might be present. If\n+  \/\/ the bit is zero, we are certain that super_klass is not one of\n+  \/\/ the secondary supers.\n+  movb(slot, Address(r_super_klass, Klass::hash_slot_offset()));\n+  xorl(slot, (u1)(Klass::SECONDARY_SUPERS_TABLE_SIZE - 1)); \/\/ slot ^ 63 === 63 - slot (mod 64)\n+  salq(r_array_index, slot);\n+\n+  testq(r_array_index, r_array_index);\n+  \/\/ We test the MSB of r_array_index, i.e. its sign bit\n+  jcc(Assembler::positive, L_failure);\n+\n+  const Register r_array_base = *available_regs++;\n+\n+  \/\/ Get the first array index that can contain super_klass into r_array_index.\n+  population_count(r_array_index, r_array_index, \/*temp2*\/r_array_base, \/*temp3*\/slot);\n+\n+  \/\/ NB! r_array_index is off by 1. It is compensated by keeping r_array_base off by 1 word.\n+\n+  \/\/ We will consult the secondary-super array.\n+  movptr(r_array_base, Address(r_sub_klass, in_bytes(Klass::secondary_supers_offset())));\n+\n+  \/\/ We're asserting that the first word in an Array<Klass*> is the\n+  \/\/ length, and the second word is the first word of the data. If\n+  \/\/ that ever changes, r_array_base will have to be adjusted here.\n+  assert(Array<Klass*>::base_offset_in_bytes() == wordSize, \"Adjust this code\");\n+  assert(Array<Klass*>::length_offset_in_bytes() == 0, \"Adjust this code\");\n+\n+  cmpq(r_super_klass, Address(r_array_base, r_array_index, Address::times_8));\n+  jccb(Assembler::equal, L_success);\n+\n+  \/\/ Restore slot to its true value\n+  xorl(slot, (u1)(Klass::SECONDARY_SUPERS_TABLE_SIZE - 1)); \/\/ slot ^ 63 === 63 - slot (mod 64)\n+\n+  \/\/ Linear probe. Rotate the bitmap so that the next bit to test is\n+  \/\/ in Bit 1.\n+  rorq(r_bitmap, slot);\n+\n+  \/\/ Is there another entry to check? Consult the bitmap.\n+  btq(r_bitmap, 1);\n+  jccb(Assembler::carryClear, L_failure);\n+\n+  \/\/ Calls into the stub generated by lookup_secondary_supers_table_slow_path.\n+  \/\/ Arguments: r_super_klass, r_array_base, r_array_index, r_bitmap.\n+  \/\/ Kills: r_array_length.\n+  \/\/ Returns: result.\n+  lookup_secondary_supers_table_slow_path(r_super_klass,\n+                                          r_array_base,\n+                                          r_array_index,\n+                                          r_bitmap,\n+                                          \/*temp1*\/result,\n+                                          \/*temp2*\/slot,\n+                                          &L_success,\n+                                          nullptr);\n+\n+  bind(L_failure);\n+  movq(result, 1);\n+  jmpb(L_fallthrough);\n+\n+  bind(L_success);\n+  xorq(result, result); \/\/ = 0\n+\n+  bind(L_fallthrough);\n+  BLOCK_COMMENT(\"} lookup_secondary_supers_table\");\n+\n+  if (VerifySecondarySupers) {\n+    verify_secondary_supers_table(r_sub_klass, r_super_klass, result,\n+                                  temp1, temp2, temp3);\n+  }\n+}\n+\n@@ -5252,2 +5519,0 @@\n-  LOOKUP_SECONDARY_SUPERS_TABLE_REGISTERS;\n-\n@@ -5350,2 +5615,0 @@\n-  LOOKUP_SECONDARY_SUPERS_TABLE_REGISTERS;\n-\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":284,"deletions":21,"binary":false,"changes":305,"status":"modified"},{"patch":"@@ -711,1 +711,4 @@\n-  void hashed_check_klass_subtype_slow_path(Register sub_klass,\n+\n+#ifdef _LP64\n+  \/\/ The 64-bit version, which may do a hashed subclass lookup.\n+  void check_klass_subtype_slow_path(Register sub_klass,\n@@ -715,0 +718,2 @@\n+                                     Register temp3_reg,\n+                                     Register temp4_reg,\n@@ -716,2 +721,26 @@\n-                                     Label* L_failure,\n-                                     bool set_cond_codes = false);\n+                                     Label* L_failure);\n+#endif\n+\n+  \/\/ Three parts of a hashed subclass lookup: a simple linear search,\n+  \/\/ a table lookup, and a fallback that does linear probing in the\n+  \/\/ event of a hash collision.\n+  void check_klass_subtype_slow_path_linear(Register sub_klass,\n+                                            Register super_klass,\n+                                            Register temp_reg,\n+                                            Register temp2_reg,\n+                                            Label* L_success,\n+                                            Label* L_failure,\n+                                            bool set_cond_codes = false);\n+  void check_klass_subtype_slow_path_table(Register sub_klass,\n+                                           Register super_klass,\n+                                           Register temp_reg,\n+                                           Register temp2_reg,\n+                                           Register temp3_reg,\n+                                           Register result_reg,\n+                                           Label* L_success,\n+                                           Label* L_failure);\n+  void hashed_check_klass_subtype_slow_path(Register sub_klass,\n+                                            Register super_klass,\n+                                            Register temp_reg,\n+                                            Label* L_success,\n+                                            Label* L_failure);\n@@ -721,8 +750,21 @@\n-  void lookup_secondary_supers_table(Register sub_klass,\n-                                     Register super_klass,\n-                                     Register temp1,\n-                                     Register temp2,\n-                                     Register temp3,\n-                                     Register temp4,\n-                                     Register result,\n-                                     u1 super_klass_slot);\n+  void lookup_secondary_supers_table_const(Register sub_klass,\n+                                           Register super_klass,\n+                                           Register temp1,\n+                                           Register temp2,\n+                                           Register temp3,\n+                                           Register temp4,\n+                                           Register result,\n+                                           u1 super_klass_slot);\n+\n+#ifdef _LP64\n+  using Assembler::salq;\n+  void salq(Register dest, Register count);\n+  using Assembler::rorq;\n+  void rorq(Register dest, Register count);\n+  void lookup_secondary_supers_table_var(Register sub_klass,\n+                                         Register super_klass,\n+                                         Register temp1,\n+                                         Register temp2,\n+                                         Register temp3,\n+                                         Register temp4,\n+                                         Register result);\n@@ -745,0 +787,1 @@\n+#endif\n@@ -750,1 +793,8 @@\n-    \/\/ Simplified, combined version, good for typical uses.\n+  \/\/ If r is valid, return r.\n+  \/\/ If r is invalid, remove a register r2 from available_regs, add r2\n+  \/\/ to regs_to_push, then return r2.\n+  Register allocate_if_noreg(const Register r,\n+                             RegSetIterator<Register> &available_regs,\n+                             RegSet &regs_to_push);\n+\n+  \/\/ Simplified, combined version, good for typical uses.\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":62,"deletions":12,"binary":false,"changes":74,"status":"modified"},{"patch":"@@ -3850,4 +3850,4 @@\n-  __ lookup_secondary_supers_table(r_sub_klass, r_super_klass,\n-                                   rdx, rcx, rbx, r11, \/\/ temps\n-                                   result,\n-                                   super_klass_index);\n+  __ lookup_secondary_supers_table_const(r_sub_klass, r_super_klass,\n+                                         rdx, rcx, rbx, r11, \/\/ temps\n+                                         result,\n+                                         super_klass_index);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2141,2 +2141,0 @@\n-    case Op_MaxV:\n-    case Op_MinV:\n@@ -2145,0 +2143,5 @@\n+      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      } \/\/ fallthrough\n+    case Op_MaxV:\n+    case Op_MinV:\n@@ -2512,1 +2515,1 @@\n-    \/\/ Intel can handle 2 adds in addressing mode\n+    \/\/ Intel can handle 2 adds in addressing mode, with one of them using an immediate offset.\n@@ -2517,0 +2520,1 @@\n+        !adr->in(AddPNode::Offset)->is_Con() &&\n@@ -6600,4 +6604,4 @@\n-instruct vector_uminmax_reg_masked(vec dst, vec src1, vec src2, kReg mask) %{\n-  match(Set dst (UMinV (Binary src1 src2) mask));\n-  match(Set dst (UMaxV (Binary src1 src2) mask));\n-  format %{ \"vector_uminmax_masked $dst, $src1, $src2, $mask\\t! umin\/max masked operation\" %}\n+instruct vector_uminmax_reg_masked(vec dst, vec src2, kReg mask) %{\n+  match(Set dst (UMinV (Binary dst src2) mask));\n+  match(Set dst (UMaxV (Binary dst src2) mask));\n+  format %{ \"vector_uminmax_masked $dst, $dst, $src2, $mask\\t! umin\/max masked operation\" %}\n@@ -6614,3 +6618,3 @@\n-instruct vector_uminmax_mem_masked(vec dst, vec src1, memory src2, kReg mask) %{\n-  match(Set dst (UMinV (Binary src1 (LoadVector src2)) mask));\n-  match(Set dst (UMaxV (Binary src1 (LoadVector src2)) mask));\n+instruct vector_uminmax_mem_masked(vec dst, memory src2, kReg mask) %{\n+  match(Set dst (UMinV (Binary dst (LoadVector src2)) mask));\n+  match(Set dst (UMaxV (Binary dst (LoadVector src2)) mask));\n@@ -6623,1 +6627,1 @@\n-                   $src1$$XMMRegister, $src2$$Address, true, vlen_enc);\n+                   $dst$$XMMRegister, $src2$$Address, true, vlen_enc);\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":15,"deletions":11,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -1717,1 +1717,3 @@\n-    __ check_klass_subtype_slow_path(Resi, Reax, Recx, Redi,\n+    \/\/ NB: Callers may assume that, when $result is a valid register,\n+    \/\/ check_klass_subtype_slow_path sets it to a nonzero value.\n+     __ check_klass_subtype_slow_path(Resi, Reax, Recx, Redi,\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1845,18 +1845,0 @@\n-  enc_class enc_PartialSubtypeCheck()\n-  %{\n-    Register Rrdi = as_Register(RDI_enc); \/\/ result register\n-    Register Rrax = as_Register(RAX_enc); \/\/ super class\n-    Register Rrcx = as_Register(RCX_enc); \/\/ killed\n-    Register Rrsi = as_Register(RSI_enc); \/\/ sub class\n-    Label miss;\n-    const bool set_cond_codes = true;\n-\n-    __ check_klass_subtype_slow_path(Rrsi, Rrax, Rrcx, Rrdi,\n-                                     nullptr, &miss,\n-                                     \/*set_cond_codes:*\/ true);\n-    if ($primary) {\n-      __ xorptr(Rrdi, Rrdi);\n-    }\n-    __ bind(miss);\n-  %}\n-\n@@ -12395,0 +12377,1 @@\n+  predicate(!UseSecondarySupersTable);\n@@ -12407,2 +12390,40 @@\n-  opcode(0x1); \/\/ Force a XOR of RDI\n-  ins_encode(enc_PartialSubtypeCheck());\n+  ins_encode %{\n+    Label miss;\n+    \/\/ NB: Callers may assume that, when $result is a valid register,\n+    \/\/ check_klass_subtype_slow_path_linear sets it to a nonzero\n+    \/\/ value.\n+    __ check_klass_subtype_slow_path_linear($sub$$Register, $super$$Register,\n+                                            $rcx$$Register, $result$$Register,\n+                                            nullptr, &miss,\n+                                            \/*set_cond_codes:*\/ true);\n+    __ xorptr($result$$Register, $result$$Register);\n+    __ bind(miss);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ Two versions of hashtable-based partialSubtypeCheck, both used when\n+\/\/ we need to search for a super class in the secondary supers array.\n+\/\/ The first is used when we don't know _a priori_ the class being\n+\/\/ searched for. The second, far more common, is used when we do know:\n+\/\/ this is used for instanceof, checkcast, and any case where C2 can\n+\/\/ determine it by constant propagation.\n+\n+instruct partialSubtypeCheckVarSuper(rsi_RegP sub, rax_RegP super, rdi_RegP result,\n+                                       rdx_RegL temp1, rcx_RegL temp2, rbx_RegP temp3, r11_RegL temp4,\n+                                       rFlagsReg cr)\n+%{\n+  match(Set result (PartialSubtypeCheck sub super));\n+  predicate(UseSecondarySupersTable);\n+  effect(KILL cr, TEMP temp1, TEMP temp2, TEMP temp3, TEMP temp4);\n+\n+  ins_cost(1000);\n+  format %{ \"partialSubtypeCheck $result, $sub, $super\" %}\n+\n+  ins_encode %{\n+    __ lookup_secondary_supers_table_var($sub$$Register, $super$$Register, $temp1$$Register, $temp2$$Register,\n+\t\t\t\t\t $temp3$$Register, $temp4$$Register, $result$$Register);\n+  %}\n+\n@@ -12426,1 +12447,1 @@\n-      __ lookup_secondary_supers_table($sub$$Register, $super_reg$$Register, $temp1$$Register, $temp2$$Register,\n+      __ lookup_secondary_supers_table_const($sub$$Register, $super_reg$$Register, $temp1$$Register, $temp2$$Register,\n@@ -12437,22 +12458,0 @@\n-instruct partialSubtypeCheck_vs_Zero(rFlagsReg cr,\n-                                     rsi_RegP sub, rax_RegP super, rcx_RegI rcx,\n-                                     immP0 zero,\n-                                     rdi_RegP result)\n-%{\n-  match(Set cr (CmpP (PartialSubtypeCheck sub super) zero));\n-  effect(KILL rcx, KILL result);\n-\n-  ins_cost(1000);\n-  format %{ \"movq    rdi, [$sub + in_bytes(Klass::secondary_supers_offset())]\\n\\t\"\n-            \"movl    rcx, [rdi + Array<Klass*>::length_offset_in_bytes()]\\t# length to scan\\n\\t\"\n-            \"addq    rdi, Array<Klass*>::base_offset_in_bytes()\\t# Skip to start of data; set NZ in case count is zero\\n\\t\"\n-            \"repne   scasq\\t# Scan *rdi++ for a match with rax while cx-- != 0\\n\\t\"\n-            \"jne,s   miss\\t\\t# Missed: flags nz\\n\\t\"\n-            \"movq    [$sub + in_bytes(Klass::secondary_super_cache_offset())], $super\\t# Hit: update cache\\n\\t\"\n-    \"miss:\\t\" %}\n-\n-  opcode(0x0); \/\/ No need to XOR RDI\n-  ins_encode(enc_PartialSubtypeCheck());\n-  ins_pipe(pipe_slow);\n-%}\n-\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":42,"deletions":43,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -561,0 +561,3 @@\n+  if (src_obj == nullptr) {\n+    return;\n+  }\n@@ -563,1 +566,1 @@\n-  if (src_obj != nullptr && !src_obj->fast_no_hash_check() && (!(EnableValhalla && src_obj->mark().is_inline_type()))) {\n+  if (!src_obj->fast_no_hash_check() && (!(EnableValhalla && src_obj->mark().is_inline_type()))) {\n@@ -571,0 +574,2 @@\n+  \/\/ Strip age bits.\n+  fake_oop->set_mark(fake_oop->mark().set_age(0));\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -252,1 +252,3 @@\n-  if (Arguments::is_internal_module_property(key) && !Arguments::is_module_path_property(key)) {\n+  if (Arguments::is_internal_module_property(key) &&\n+      !Arguments::is_module_path_property(key) &&\n+      !Arguments::is_add_modules_property(key)) {\n","filename":"src\/hotspot\/share\/cds\/cdsConfig.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -279,1 +279,0 @@\n-  _use_secondary_supers_table = UseSecondarySupersTable;\n@@ -343,1 +342,0 @@\n-  st->print_cr(\"- use_secondary_supers_table:     %d\", _use_secondary_supers_table);\n@@ -1005,1 +1003,1 @@\n-  module_paths->sort(ClassLoaderExt::compare_module_path_by_name);\n+  module_paths->sort(ClassLoaderExt::compare_module_names);\n@@ -2584,5 +2582,0 @@\n-  if (! _use_secondary_supers_table && UseSecondarySupersTable) {\n-    log_warning(cds)(\"The shared archive was created without UseSecondarySupersTable.\");\n-    return false;\n-  }\n-\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":1,"deletions":8,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -226,1 +226,0 @@\n-  bool    _use_secondary_supers_table;            \/\/ save the flag UseSecondarySupersTable\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -408,0 +408,1 @@\n+  CDS_JAVA_HEAP_ONLY(Modules::serialize_addmods_names(soc);)\n@@ -507,0 +508,2 @@\n+  \/\/ Write module names from --add-modules into archive\n+  CDS_JAVA_HEAP_ONLY(Modules::dump_addmods_names();)\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -617,1 +617,0 @@\n-  do_intrinsic(_notifyJvmtiVThreadHideFrames, java_lang_VirtualThread, notifyJvmtiHideFrames_name, bool_void_signature, F_SN) \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -405,1 +405,0 @@\n-  template(notifyJvmtiHideFrames_name,                \"notifyJvmtiHideFrames\")                    \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1026,0 +1026,16 @@\n+  int old_c2_count = 0, new_c2_count = 0, old_c1_count = 0, new_c1_count = 0;\n+  const int c2_tasks_per_thread = 2, c1_tasks_per_thread = 4;\n+\n+  \/\/ Quick check if we already have enough compiler threads without taking the lock.\n+  \/\/ Numbers may change concurrently, so we read them again after we have the lock.\n+  if (_c2_compile_queue != nullptr) {\n+    old_c2_count = get_c2_thread_count();\n+    new_c2_count = MIN2(_c2_count, _c2_compile_queue->size() \/ c2_tasks_per_thread);\n+  }\n+  if (_c1_compile_queue != nullptr) {\n+    old_c1_count = get_c1_thread_count();\n+    new_c1_count = MIN2(_c1_count, _c1_compile_queue->size() \/ c1_tasks_per_thread);\n+  }\n+  if (new_c2_count <= old_c2_count && new_c1_count <= old_c1_count) return;\n+\n+  \/\/ Now, we do the more expensive operations.\n@@ -1028,2 +1044,2 @@\n-  size_t available_cc_np  = CodeCache::unallocated_capacity(CodeBlobType::MethodNonProfiled),\n-         available_cc_p   = CodeCache::unallocated_capacity(CodeBlobType::MethodProfiled);\n+  size_t available_cc_np = CodeCache::unallocated_capacity(CodeBlobType::MethodNonProfiled),\n+         available_cc_p  = CodeCache::unallocated_capacity(CodeBlobType::MethodProfiled);\n@@ -1031,1 +1047,1 @@\n-  \/\/ Only do attempt to start additional threads if the lock is free.\n+  \/\/ Only attempt to start additional threads if the lock is free.\n@@ -1035,3 +1051,3 @@\n-    int old_c2_count = _compilers[1]->num_compiler_threads();\n-    int new_c2_count = MIN4(_c2_count,\n-        _c2_compile_queue->size() \/ 2,\n+    old_c2_count = get_c2_thread_count();\n+    new_c2_count = MIN4(_c2_count,\n+        _c2_compile_queue->size() \/ c2_tasks_per_thread,\n@@ -1073,1 +1089,1 @@\n-        if (_compilers[1]->num_compiler_threads() != i) break;\n+        if (get_c2_thread_count() != i) break;\n@@ -1096,3 +1112,3 @@\n-    int old_c1_count = _compilers[0]->num_compiler_threads();\n-    int new_c1_count = MIN4(_c1_count,\n-        _c1_compile_queue->size() \/ 4,\n+    old_c1_count = get_c1_thread_count();\n+    new_c1_count = MIN4(_c1_count,\n+        _c1_compile_queue->size() \/ c1_tasks_per_thread,\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":26,"deletions":10,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -1916,4 +1916,2 @@\n-    if (!c->completed()) {\n-      log_warning(gc)(\"region \" SIZE_FORMAT \" not filled: destination_count=%u\",\n-                      cur_region, c->destination_count());\n-    }\n+    assert(c->completed(), \"region %zu not filled: destination_count=%u\",\n+           cur_region, c->destination_count());\n@@ -1924,4 +1922,2 @@\n-    if (!c->available()) {\n-      log_warning(gc)(\"region \" SIZE_FORMAT \" not empty: destination_count=%u\",\n-                      cur_region, c->destination_count());\n-    }\n+    assert(c->available(), \"region %zu not empty: destination_count=%u\",\n+           cur_region, c->destination_count());\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":4,"deletions":8,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1160,3 +1160,0 @@\n-JNIEXPORT void JNICALL\n-JVM_VirtualThreadHideFrames(JNIEnv* env, jclass clazz, jboolean hide);\n-\n","filename":"src\/hotspot\/share\/include\/jvm.h","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -187,1 +187,2 @@\n-  CompilerThreadCanCallJava ccj(thread, __is_hotspot);     \\\n+  bool __block_can_call_java = __is_hotspot || !thread->is_Compiler_thread() || CompilerThread::cast(thread)->can_call_java(); \\\n+  CompilerThreadCanCallJava ccj(thread, __block_can_call_java); \\\n@@ -404,0 +405,5 @@\n+C2V_VMENTRY_PREFIX(jboolean, updateCompilerThreadCanCallJava, (JNIEnv* env, jobject, jboolean newState))\n+  return CompilerThreadCanCallJava::update(thread, newState) != nullptr;\n+C2V_END\n+\n+\n@@ -3390,0 +3396,1 @@\n+  {CC \"updateCompilerThreadCanCallJava\",              CC \"(Z)Z\",                                                                            FN_PTR(updateCompilerThreadCanCallJava)},\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -251,1 +251,0 @@\n-  JVMTI_ONLY(nonstatic_field(JavaThread,       _is_in_tmp_VTMS_transition,                    bool))                                 \\\n@@ -276,1 +275,1 @@\n-  nonstatic_field(Klass,                       _bitmap,                                       uintx)                                 \\\n+  nonstatic_field(Klass,                       _secondary_supers_bitmap,                      uintx)                                 \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -460,4 +460,2 @@\n-    if (UseSecondarySupersTable) {\n-      Universe::_the_array_interfaces_bitmap = Klass::compute_secondary_supers_bitmap(_the_array_interfaces_array);\n-      Universe::_the_empty_klass_bitmap      = Klass::compute_secondary_supers_bitmap(_the_empty_klass_array);\n-    }\n+    _the_array_interfaces_bitmap = Klass::compute_secondary_supers_bitmap(_the_array_interfaces_array);\n+    _the_empty_klass_bitmap      = Klass::compute_secondary_supers_bitmap(_the_empty_klass_array);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+#include \"klass.inline.hpp\"\n@@ -713,1 +714,1 @@\n-  set_secondary_supers(nullptr);\n+  set_secondary_supers(nullptr, SECONDARY_SUPERS_BITMAP_EMPTY);\n@@ -1659,15 +1660,6 @@\n-  } else if (num_extra_slots == 0) {\n-    \/\/ The secondary super list is exactly the same as the transitive interfaces, so\n-    \/\/ let's use it instead of making a copy.\n-    \/\/ Redefine classes has to be careful not to delete this!\n-    if (!UseSecondarySupersTable) {\n-      set_secondary_supers(interfaces);\n-      return nullptr;\n-    } else if (num_extra_slots == 0 && interfaces->length() <= 1) {\n-      \/\/ We will reuse the transitive interfaces list if we're certain\n-      \/\/ it's in hash order.\n-      uintx bitmap = compute_secondary_supers_bitmap(interfaces);\n-      set_secondary_supers(interfaces, bitmap);\n-      return nullptr;\n-    }\n-    \/\/ ... fall through if that didn't work.\n+  } else if (num_extra_slots == 0 && interfaces->length() <= 1) {\n+    \/\/ We will reuse the transitive interfaces list if we're certain\n+    \/\/ it's in hash order.\n+    uintx bitmap = compute_secondary_supers_bitmap(interfaces);\n+    set_secondary_supers(interfaces, bitmap);\n+    return nullptr;\n@@ -3811,4 +3803,4 @@\n-  if (UseSecondarySupersTable) {\n-    st->print(BULLET\"hash_slot:         %d\", hash_slot()); st->cr();\n-    st->print(BULLET\"bitmap:            \" UINTX_FORMAT_X_0, _bitmap); st->cr();\n-  }\n+\n+  st->print(BULLET\"hash_slot:         %d\", hash_slot()); st->cr();\n+  st->print(BULLET\"secondary bitmap: \" UINTX_FORMAT_X_0, _secondary_supers_bitmap); st->cr();\n+\n@@ -3817,1 +3809,1 @@\n-      bool is_hashed = UseSecondarySupersTable && (_bitmap != SECONDARY_SUPERS_BITMAP_FULL);\n+      bool is_hashed = (_secondary_supers_bitmap != SECONDARY_SUPERS_BITMAP_FULL);\n@@ -3824,1 +3816,1 @@\n-          int home_slot = compute_home_slot(secondary_super, _bitmap);\n+          int home_slot = compute_home_slot(secondary_super, _secondary_supers_bitmap);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":14,"deletions":22,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -131,1 +131,1 @@\n-  if (UseSecondarySupersTable) {\n+  {\n@@ -166,7 +166,1 @@\n-bool Klass::search_secondary_supers(Klass* k) const {\n-  \/\/ Put some extra logic here out-of-line, before the search proper.\n-  \/\/ This cuts down the size of the inline method.\n-\n-  \/\/ This is necessary, since I am never in my own secondary_super list.\n-  if (this == k)\n-    return true;\n+bool Klass::linear_search_secondary_supers(const Klass* k) const {\n@@ -174,0 +168,3 @@\n+  \/\/ FIXME: We could do something smarter here, maybe a vectorized\n+  \/\/ comparison or a binary search, but is that worth any added\n+  \/\/ complexity?\n@@ -177,1 +174,0 @@\n-      ((Klass*)this)->set_secondary_super_cache(k);\n@@ -184,0 +180,31 @@\n+\/\/ Given a secondary superklass k, an initial array index, and an\n+\/\/ occupancy bitmap rotated such that Bit 1 is the next bit to test,\n+\/\/ search for k.\n+bool Klass::fallback_search_secondary_supers(const Klass* k, int index, uintx rotated_bitmap) const {\n+  \/\/ Once the occupancy bitmap is almost full, it's faster to use a\n+  \/\/ linear search.\n+  if (secondary_supers()->length() > SECONDARY_SUPERS_TABLE_SIZE - 2) {\n+    return linear_search_secondary_supers(k);\n+  }\n+\n+  \/\/ This is conventional linear probing, but instead of terminating\n+  \/\/ when a null entry is found in the table, we maintain a bitmap\n+  \/\/ in which a 0 indicates missing entries.\n+\n+  precond((int)population_count(rotated_bitmap) == secondary_supers()->length());\n+\n+  \/\/ The check for secondary_supers()->length() <= SECONDARY_SUPERS_TABLE_SIZE - 2\n+  \/\/ at the start of this function guarantees there are 0s in the\n+  \/\/ bitmap, so this loop eventually terminates.\n+  while ((rotated_bitmap & 2) != 0) {\n+    if (++index == secondary_supers()->length()) {\n+      index = 0;\n+    }\n+    if (secondary_supers()->at(index) == k) {\n+      return true;\n+    }\n+    rotated_bitmap = rotate_right(rotated_bitmap, 1);\n+  }\n+  return false;\n+}\n+\n@@ -296,5 +323,0 @@\n-void Klass::set_secondary_supers(Array<Klass*>* secondaries) {\n-  assert(!UseSecondarySupersTable || secondaries == nullptr, \"\");\n-  set_secondary_supers(secondaries, SECONDARY_SUPERS_BITMAP_EMPTY);\n-}\n-\n@@ -303,1 +325,1 @@\n-  if (UseSecondarySupersTable && secondaries != nullptr) {\n+  if (secondaries != nullptr) {\n@@ -309,1 +331,1 @@\n-  _bitmap = bitmap;\n+  _secondary_supers_bitmap = bitmap;\n@@ -386,0 +408,1 @@\n+    postcond((int)population_count(bitmap) == secondaries->length());\n@@ -446,5 +469,1 @@\n-  if (UseSecondarySupersTable) {\n-    bitmap = hash_secondary_supers(secondary_supers, \/*rewrite=*\/true); \/\/ rewrites freshly allocated array\n-  } else {\n-    bitmap = SECONDARY_SUPERS_BITMAP_EMPTY;\n-  }\n+  bitmap = hash_secondary_supers(secondary_supers, \/*rewrite=*\/true); \/\/ rewrites freshly allocated array\n@@ -774,1 +793,1 @@\n-  \/\/assert(compute_secondary_supers_bitmap(secondary_supers()) == _bitmap, \"broken table\");\n+  \/\/assert(compute_secondary_supers_bitmap(secondary_supers()) == _secondary_supers_bitmap, \"broken table\");\n@@ -790,1 +809,1 @@\n-  assert(secondary_supers()->length() >= (int)population_count(_bitmap), \"must be\");\n+  assert(secondary_supers()->length() >= (int)population_count(_secondary_supers_bitmap), \"must be\");\n@@ -1259,8 +1278,7 @@\n-    if (UseSecondarySupersTable) {\n-      st->print(\"  - \"); st->print(\"%d elements;\", _secondary_supers->length());\n-      st->print_cr(\" bitmap: \" UINTX_FORMAT_X_0 \";\", _bitmap);\n-      if (_bitmap != SECONDARY_SUPERS_BITMAP_EMPTY &&\n-          _bitmap != SECONDARY_SUPERS_BITMAP_FULL) {\n-        st->print(\"  - \"); print_positive_lookup_stats(secondary_supers(), _bitmap, st); st->cr();\n-        st->print(\"  - \"); print_negative_lookup_stats(_bitmap, st); st->cr();\n-      }\n+    st->print(\"  - \"); st->print(\"%d elements;\", _secondary_supers->length());\n+    st->print_cr(\" bitmap: \" UINTX_FORMAT_X_0 \";\", _secondary_supers_bitmap);\n+    if (_secondary_supers_bitmap != SECONDARY_SUPERS_BITMAP_EMPTY &&\n+        _secondary_supers_bitmap != SECONDARY_SUPERS_BITMAP_FULL) {\n+      st->print(\"  - \"); print_positive_lookup_stats(secondary_supers(),\n+                                                     _secondary_supers_bitmap, st); st->cr();\n+      st->print(\"  - \"); print_negative_lookup_stats(_secondary_supers_bitmap, st); st->cr();\n@@ -1277,3 +1295,2 @@\n-  fatal(\"%s: %s implements %s: is_subtype_of: %d; linear_search: %d; table_lookup: %d\",\n-        msg, sub->external_name(), super->external_name(),\n-        sub->is_subtype_of(super), linear_result, table_result);\n+  fatal(\"%s: %s implements %s: linear_search: %d; table_lookup: %d\",\n+        msg, sub->external_name(), super->external_name(), linear_result, table_result);\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":51,"deletions":34,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -68,0 +68,1 @@\n+\n@@ -165,0 +166,3 @@\n+  \/\/ Bitmap and hash code used by hashed secondary supers.\n+  uintx    _secondary_supers_bitmap;\n+  uint8_t  _hash_slot;\n@@ -177,4 +181,0 @@\n-  \/\/ Bitmap and hash code used by hashed secondary supers.\n-  uintx    _bitmap;\n-  uint8_t  _hash_slot;\n-\n@@ -245,1 +245,0 @@\n-  void set_secondary_supers(Array<Klass*>* k);\n@@ -404,0 +403,5 @@\n+  bool search_secondary_supers(Klass* k) const;\n+  bool lookup_secondary_supers_table(Klass *k) const;\n+  bool linear_search_secondary_supers(const Klass* k) const;\n+  bool fallback_search_secondary_supers(const Klass* k, int index, uintx rotated_bitmap) const;\n+\n@@ -415,1 +419,1 @@\n-  static constexpr int SECONDARY_SUPERS_TABLE_SIZE = sizeof(_bitmap) * 8;\n+  static constexpr int SECONDARY_SUPERS_TABLE_SIZE = sizeof(_secondary_supers_bitmap) * 8;\n@@ -436,1 +440,3 @@\n-  static ByteSize bitmap_offset()                { return byte_offset_of(Klass, _bitmap); }\n+  static ByteSize secondary_supers_bitmap_offset()\n+                                                 { return byte_offset_of(Klass, _secondary_supers_bitmap); }\n+  static ByteSize hash_slot_offset()             { return byte_offset_of(Klass, _hash_slot); }\n@@ -555,14 +561,2 @@\n-  \/\/ subtype check: true if is_subclass_of, or if k is interface and receiver implements it\n-  bool is_subtype_of(Klass* k) const {\n-    juint    off = k->super_check_offset();\n-    Klass* sup = *(Klass**)( (address)this + off );\n-    const juint secondary_offset = in_bytes(secondary_super_cache_offset());\n-    if (sup == k) {\n-      return true;\n-    } else if (off != secondary_offset) {\n-      return false;\n-    } else {\n-      return search_secondary_supers(k);\n-    }\n-  }\n-  bool search_secondary_supers(Klass* k) const;\n+  \/\/ subtype check: true if is_subclass_of, or if k is interface and receiver implements it\n+  bool is_subtype_of(Klass* k) const;\n@@ -571,0 +565,1 @@\n+public:\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":16,"deletions":21,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"utilities\/rotate_bits.hpp\"\n@@ -96,0 +97,68 @@\n+\/\/ subtype check: true if is_subclass_of, or if k is interface and receiver implements it\n+inline bool Klass::is_subtype_of(Klass* k) const {\n+  assert(secondary_supers() != nullptr, \"must be\");\n+  const juint off = k->super_check_offset();\n+  const juint secondary_offset = in_bytes(secondary_super_cache_offset());\n+  if (off == secondary_offset) {\n+    return search_secondary_supers(k);\n+  } else {\n+    Klass* sup = *(Klass**)( (address)this + off );\n+    return (sup == k);\n+  }\n+}\n+\n+\/\/ Hashed search for secondary super k.\n+inline bool Klass::lookup_secondary_supers_table(Klass* k) const {\n+  uintx bitmap = _secondary_supers_bitmap;\n+\n+  constexpr int highest_bit_number = SECONDARY_SUPERS_TABLE_SIZE - 1;\n+  uint8_t slot = k->_hash_slot;\n+  uintx shifted_bitmap = bitmap << (highest_bit_number - slot);\n+\n+  precond((int)population_count(bitmap) <= secondary_supers()->length());\n+\n+  \/\/ First check the bitmap to see if super_klass might be present. If\n+  \/\/ the bit is zero, we are certain that super_klass is not one of\n+  \/\/ the secondary supers.\n+  if (((shifted_bitmap >> highest_bit_number) & 1) == 0) {\n+    return false;\n+  }\n+\n+  \/\/ Calculate the initial hash probe\n+  int index = population_count(shifted_bitmap) - 1;\n+  if (secondary_supers()->at(index) == k) {\n+    \/\/ Yes! It worked the first time.\n+    return true;\n+  }\n+\n+  \/\/ Is there another entry to check? Consult the bitmap. If Bit 1,\n+  \/\/ the next bit to test, is zero, we are certain that super_klass is\n+  \/\/ not one of the secondary supers.\n+  bitmap = rotate_right(bitmap, slot);\n+  if ((bitmap & 2) == 0) {\n+    return false;\n+  }\n+\n+  \/\/ Continue probing the hash table\n+  return fallback_search_secondary_supers(k, index, bitmap);\n+}\n+\n+inline bool Klass::search_secondary_supers(Klass *k) const {\n+  \/\/ This is necessary because I am never in my own secondary_super list.\n+  if (this == k)\n+    return true;\n+\n+  bool result = lookup_secondary_supers_table(k);\n+\n+#ifndef PRODUCT\n+  if (VerifySecondarySupers) {\n+    bool linear_result = linear_search_secondary_supers(k);\n+    if (linear_result != result) {\n+      on_secondary_supers_verification_failure((Klass*)this, k, linear_result, result, \"mismatch\");\n+    }\n+  }\n+#endif \/\/ PRODUCT\n+\n+  return result;\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":69,"deletions":0,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -370,3 +370,0 @@\n-  develop(bool, TraceMergeStores, false,                                    \\\n-          \"Trace creation of merged stores\")                                \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -851,1 +851,0 @@\n-  case vmIntrinsics::_notifyJvmtiVThreadHideFrames:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -4272,0 +4272,6 @@\n+  case Op_ConNKlass: {\n+    const TypePtr* tp = n->as_Type()->type()->make_ptr();\n+    ciKlass* klass = tp->is_klassptr()->exact_klass();\n+    assert(klass->is_in_encoding_range(), \"klass cannot be compressed\");\n+    break;\n+  }\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -332,1 +332,1 @@\n-    find_scalar_replaceable_allocs(jobj_worklist);\n+    find_scalar_replaceable_allocs(jobj_worklist, reducible_merges);\n@@ -589,1 +589,2 @@\n-          \/\/ We may have an OpaqueNotNull node between If and Bool nodes. Bail out in such case.\n+          \/\/ We may have an OpaqueNotNull node between If and Bool nodes. But we could also have a sub class of IfNode,\n+          \/\/ for example, an OuterStripMinedLoopEnd or a Parse Predicate. Bail out in all these cases.\n@@ -595,2 +596,0 @@\n-          } else {\n-            assert(iff->in(1)->is_OpaqueNotNull(), \"must be OpaqueNotNull\");\n@@ -3117,0 +3116,35 @@\n+void ConnectionGraph::revisit_reducible_phi_status(JavaObjectNode* jobj, Unique_Node_List& reducible_merges) {\n+  assert(jobj != nullptr && !jobj->scalar_replaceable(), \"jobj should be set as NSR before calling this function.\");\n+\n+  \/\/ Look for 'phis' that refer to 'jobj' as the last\n+  \/\/ remaining scalar replaceable input.\n+  uint reducible_merges_cnt = reducible_merges.size();\n+  for (uint i = 0; i < reducible_merges_cnt; i++) {\n+    Node* phi = reducible_merges.at(i);\n+\n+    \/\/ This 'Phi' will be a 'good' if it still points to\n+    \/\/ at least one scalar replaceable object. Note that 'obj'\n+    \/\/ was\/should be marked as NSR before calling this function.\n+    bool good_phi = false;\n+\n+    for (uint j = 1; j < phi->req(); j++) {\n+      JavaObjectNode* phi_in_obj = unique_java_object(phi->in(j));\n+      if (phi_in_obj != nullptr && phi_in_obj->scalar_replaceable()) {\n+        good_phi = true;\n+        break;\n+      }\n+    }\n+\n+    if (!good_phi) {\n+      NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Phi %d became non-reducible after node %d became NSR.\", phi->_idx, jobj->ideal_node()->_idx);)\n+      reducible_merges.remove(i);\n+\n+      \/\/ Decrement the index because the 'remove' call above actually\n+      \/\/ moves the last entry of the list to position 'i'.\n+      i--;\n+\n+      reducible_merges_cnt--;\n+    }\n+  }\n+}\n+\n@@ -3118,1 +3152,1 @@\n-void ConnectionGraph::find_scalar_replaceable_allocs(GrowableArray<JavaObjectNode*>& jobj_worklist) {\n+void ConnectionGraph::find_scalar_replaceable_allocs(GrowableArray<JavaObjectNode*>& jobj_worklist, Unique_Node_List &reducible_merges) {\n@@ -3137,0 +3171,4 @@\n+              \/\/ Any merge that had only 'jobj' as scalar-replaceable will now be non-reducible,\n+              \/\/ because there is no point in reducing a Phi that won't improve the number of SR\n+              \/\/ objects.\n+              revisit_reducible_phi_status(jobj, reducible_merges);\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":43,"deletions":5,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -503,1 +503,0 @@\n-  case vmIntrinsics::_notifyJvmtiVThreadHideFrames:     return inline_native_notify_jvmti_hide();\n@@ -3181,23 +3180,0 @@\n-\/\/ Always update the temporary VTMS transition bit.\n-bool LibraryCallKit::inline_native_notify_jvmti_hide() {\n-  if (!DoJVMTIVirtualThreadTransitions) {\n-    return true;\n-  }\n-  IdealKit ideal(this);\n-\n-  {\n-    \/\/ unconditionally update the temporary VTMS transition bit in current JavaThread\n-    Node* thread = ideal.thread();\n-    Node* hide = _gvn.transform(argument(0)); \/\/ hide argument for temporary VTMS transition notification\n-    Node* addr = basic_plus_adr(thread, in_bytes(JavaThread::is_in_tmp_VTMS_transition_offset()));\n-    const TypePtr *addr_type = _gvn.type(addr)->isa_ptr();\n-\n-    sync_kit(ideal);\n-    access_store_at(nullptr, addr, addr_type, hide, _gvn.type(hide), T_BOOLEAN, IN_NATIVE | MO_UNORDERED);\n-    ideal.sync_kit(this);\n-  }\n-  final_sync(ideal);\n-\n-  return true;\n-}\n-\n@@ -7789,1 +7765,1 @@\n-#if defined(PPC64) || defined(S390)\n+#if defined(PPC64) || defined(S390) || defined(RISCV64)\n@@ -7793,1 +7769,1 @@\n-  \/\/ The ppc64 stubs of encryption and decryption use the same round keys (sessionK[0]).\n+  \/\/ The ppc64 and riscv64 stubs of encryption and decryption use the same round keys (sessionK[0]).\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":2,"deletions":26,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -947,1 +947,1 @@\n-  void ensure_zero_trip_guard_proj(Node* node, bool is_main_loop);\n+  static void ensure_zero_trip_guard_proj(Node* node, bool is_main_loop);\n@@ -949,11 +949,0 @@\n-  void copy_assertion_predicates_to_main_loop_helper(const PredicateBlock* predicate_block, Node* init, Node* stride,\n-                                                     IdealLoopTree* outer_loop, LoopNode* outer_main_head,\n-                                                     uint dd_main_head, uint idx_before_pre_post,\n-                                                     uint idx_after_post_before_pre, Node* zero_trip_guard_proj_main,\n-                                                     Node* zero_trip_guard_proj_post, const Node_List &old_new);\n-  void copy_assertion_predicates_to_main_loop(CountedLoopNode* pre_head, Node* init, Node* stride, IdealLoopTree* outer_loop,\n-                                              LoopNode* outer_main_head, uint dd_main_head, uint idx_before_pre_post,\n-                                              uint idx_after_post_before_pre, Node* zero_trip_guard_proj_main,\n-                                              Node* zero_trip_guard_proj_post, const Node_List& old_new);\n-  Node* clone_template_assertion_predicate(IfNode* iff, Node* new_init, Node* predicate, Node* uncommon_proj, Node* control,\n-                                           IdealLoopTree* outer_loop, Node* new_control);\n@@ -963,0 +952,1 @@\n+  DEBUG_ONLY(static bool assertion_predicate_has_loop_opaque_node(IfNode* iff);)\n@@ -965,1 +955,0 @@\n-  DEBUG_ONLY(static bool assertion_predicate_has_loop_opaque_node(IfNode* iff);)\n@@ -968,2 +957,0 @@\n-  void copy_assertion_predicates_to_post_loop(LoopNode* main_loop_head, CountedLoopNode* post_loop_head,\n-                                              Node* stride);\n@@ -974,0 +961,6 @@\n+  void initialize_assertion_predicates_for_main_loop(CountedLoopNode* pre_loop_head,\n+                                                     CountedLoopNode* main_loop_head,\n+                                                     uint first_node_index_in_cloned_loop_body,\n+                                                     const Node_List& old_new);\n+  void initialize_assertion_predicates_for_post_loop(CountedLoopNode* main_loop_head, CountedLoopNode* post_loop_head,\n+                                                     uint first_node_index_in_cloned_loop_body);\n@@ -975,1 +968,1 @@\n-                                           const NodeInLoopBody& _node_in_loop_body);\n+                                           const NodeInLoopBody& _node_in_loop_body, bool clone_template);\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":9,"deletions":16,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -173,1 +173,3 @@\n-    visited.set(n->_idx);\n+    if (visited.test_set(n->_idx)) {\n+      continue;\n+    }\n@@ -178,4 +180,1 @@\n-        assert(C->node_arena()->contains(in), \"dead node\");\n-        if (!visited.test(in->_idx)) {\n-          worklist.push(in);\n-        }\n+        worklist.push(in);\n@@ -184,0 +183,3 @@\n+    for (DUIterator_Fast jmax, j = n->fast_outs(jmax); j < jmax; j++) {\n+      worklist.push(n->fast_out(j));\n+    }\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"opto\/mempointer.hpp\"\n@@ -54,0 +55,1 @@\n+#include \"opto\/traceMergeStoresTag.hpp\"\n@@ -2791,178 +2793,0 @@\n-\/\/ Class to parse array pointers, and determine if they are adjacent. We parse the form:\n-\/\/\n-\/\/   pointer =   base\n-\/\/             + constant_offset\n-\/\/             + LShiftL( ConvI2L(int_offset + int_con), int_offset_shift)\n-\/\/             + sum(other_offsets)\n-\/\/\n-\/\/\n-\/\/ Note: we accumulate all constant offsets into constant_offset, even the int constant behind\n-\/\/       the \"LShiftL(ConvI2L(...))\" pattern. We convert \"ConvI2L(int_offset + int_con)\" to\n-\/\/       \"ConvI2L(int_offset) + int_con\", which is only safe if we can assume that either all\n-\/\/       compared addresses have an overflow for \"int_offset + int_con\" or none.\n-\/\/       For loads and stores on arrays, we know that if one overflows and the other not, then\n-\/\/       the two addresses lay almost max_int indices apart, but the maximal array size is\n-\/\/       only about half of that. Therefore, the RangeCheck on at least one of them must have\n-\/\/       failed.\n-\/\/\n-\/\/   constant_offset += LShiftL( ConvI2L(int_con), int_offset_shift)\n-\/\/\n-\/\/   pointer =   base\n-\/\/             + constant_offset\n-\/\/             + LShiftL( ConvI2L(int_offset), int_offset_shift)\n-\/\/             + sum(other_offsets)\n-\/\/\n-class ArrayPointer {\n-private:\n-  const Node* _pointer;          \/\/ The final pointer to the position in the array\n-  const Node* _base;             \/\/ Base address of the array\n-  const jlong _constant_offset;  \/\/ Sum of collected constant offsets\n-  const Node* _int_offset;       \/\/ (optional) Offset behind LShiftL and ConvI2L\n-  const GrowableArray<Node*>* _other_offsets; \/\/ List of other AddP offsets\n-  const jint _int_offset_shift; \/\/ (optional) Shift value for int_offset\n-  const bool _is_valid;          \/\/ The parsing succeeded\n-\n-  ArrayPointer(const bool is_valid,\n-               const Node* pointer,\n-               const Node* base,\n-               const jlong constant_offset,\n-               const Node* int_offset,\n-               const jint int_offset_shift,\n-               const GrowableArray<Node*>* other_offsets) :\n-      _pointer(pointer),\n-      _base(base),\n-      _constant_offset(constant_offset),\n-      _int_offset(int_offset),\n-      _other_offsets(other_offsets),\n-      _int_offset_shift(int_offset_shift),\n-      _is_valid(is_valid)\n-  {\n-    assert(_pointer != nullptr, \"must always have pointer\");\n-    assert(is_valid == (_base != nullptr), \"have base exactly if valid\");\n-    assert(is_valid == (_other_offsets != nullptr), \"have other_offsets exactly if valid\");\n-  }\n-\n-  static ArrayPointer make_invalid(const Node* pointer) {\n-    return ArrayPointer(false, pointer, nullptr, 0, nullptr, 0, nullptr);\n-  }\n-\n-  static bool parse_int_offset(Node* offset, Node*& int_offset, jint& int_offset_shift) {\n-    \/\/ offset = LShiftL( ConvI2L(int_offset), int_offset_shift)\n-    if (offset->Opcode() == Op_LShiftL &&\n-        offset->in(1)->Opcode() == Op_ConvI2L &&\n-        offset->in(2)->Opcode() == Op_ConI) {\n-      int_offset = offset->in(1)->in(1); \/\/ LShiftL -> ConvI2L -> int_offset\n-      int_offset_shift = offset->in(2)->get_int(); \/\/ LShiftL -> int_offset_shift\n-      return true;\n-    }\n-\n-    \/\/ offset = ConvI2L(int_offset) = LShiftL( ConvI2L(int_offset), 0)\n-    if (offset->Opcode() == Op_ConvI2L) {\n-      int_offset = offset->in(1);\n-      int_offset_shift = 0;\n-      return true;\n-    }\n-\n-    \/\/ parse failed\n-    return false;\n-  }\n-\n-public:\n-  \/\/ Parse the structure above the pointer\n-  static ArrayPointer make(PhaseGVN* phase, const Node* pointer) {\n-    assert(phase->type(pointer)->isa_aryptr() != nullptr, \"must be array pointer\");\n-    if (!pointer->is_AddP()) { return ArrayPointer::make_invalid(pointer); }\n-\n-    const Node* base = pointer->in(AddPNode::Base);\n-    if (base == nullptr) { return ArrayPointer::make_invalid(pointer); }\n-\n-    const int search_depth = 5;\n-    Node* offsets[search_depth];\n-    int count = pointer->as_AddP()->unpack_offsets(offsets, search_depth);\n-\n-    \/\/ We expect at least a constant each\n-    if (count <= 0) { return ArrayPointer::make_invalid(pointer); }\n-\n-    \/\/ We extract the form:\n-    \/\/\n-    \/\/   pointer =   base\n-    \/\/             + constant_offset\n-    \/\/             + LShiftL( ConvI2L(int_offset + int_con), int_offset_shift)\n-    \/\/             + sum(other_offsets)\n-    \/\/\n-    jlong constant_offset = 0;\n-    Node* int_offset = nullptr;\n-    jint int_offset_shift = 0;\n-    GrowableArray<Node*>* other_offsets = new GrowableArray<Node*>(count);\n-\n-    for (int i = 0; i < count; i++) {\n-      Node* offset = offsets[i];\n-      if (offset->Opcode() == Op_ConI) {\n-        \/\/ Constant int offset\n-        constant_offset += offset->get_int();\n-      } else if (offset->Opcode() == Op_ConL) {\n-        \/\/ Constant long offset\n-        constant_offset += offset->get_long();\n-      } else if(int_offset == nullptr && parse_int_offset(offset, int_offset, int_offset_shift)) {\n-        \/\/ LShiftL( ConvI2L(int_offset), int_offset_shift)\n-        int_offset = int_offset->uncast();\n-        if (int_offset->Opcode() == Op_AddI && int_offset->in(2)->Opcode() == Op_ConI) {\n-          \/\/ LShiftL( ConvI2L(int_offset + int_con), int_offset_shift)\n-          constant_offset += ((jlong)int_offset->in(2)->get_int()) << int_offset_shift;\n-          int_offset = int_offset->in(1);\n-        }\n-      } else {\n-        \/\/ All others\n-        other_offsets->append(offset);\n-      }\n-    }\n-\n-    return ArrayPointer(true, pointer, base, constant_offset, int_offset, int_offset_shift, other_offsets);\n-  }\n-\n-  bool is_adjacent_to_and_before(const ArrayPointer& other, const jlong data_size) const {\n-    if (!_is_valid || !other._is_valid) { return false; }\n-\n-    \/\/ Offset adjacent?\n-    if (this->_constant_offset + data_size != other._constant_offset) { return false; }\n-\n-    \/\/ All other components identical?\n-    if (this->_base != other._base ||\n-        this->_int_offset != other._int_offset ||\n-        this->_int_offset_shift != other._int_offset_shift ||\n-        this->_other_offsets->length() != other._other_offsets->length()) {\n-      return false;\n-    }\n-\n-    for (int i = 0; i < this->_other_offsets->length(); i++) {\n-      Node* o1 = this->_other_offsets->at(i);\n-      Node* o2 = other._other_offsets->at(i);\n-      if (o1 != o2) { return false; }\n-    }\n-\n-    return true;\n-  }\n-\n-#ifndef PRODUCT\n-  void dump() {\n-    if (!_is_valid) {\n-      tty->print(\"ArrayPointer[%d %s, invalid]\", _pointer->_idx, _pointer->Name());\n-      return;\n-    }\n-    tty->print(\"ArrayPointer[%d %s, base[%d %s] + %lld\",\n-               _pointer->_idx, _pointer->Name(),\n-               _base->_idx, _base->Name(),\n-               (long long)_constant_offset);\n-    if (_int_offset != nullptr) {\n-      tty->print(\" + I2L[%d %s] << %d\",\n-                 _int_offset->_idx, _int_offset->Name(), _int_offset_shift);\n-    }\n-    for (int i = 0; i < _other_offsets->length(); i++) {\n-      Node* n = _other_offsets->at(i);\n-      tty->print(\" + [%d %s]\", n->_idx, n->Name());\n-    }\n-    tty->print_cr(\"]\");\n-  }\n-#endif\n-};\n-\n@@ -3004,1 +2828,1 @@\n-class MergePrimitiveArrayStores : public StackObj {\n+class MergePrimitiveStores : public StackObj {\n@@ -3006,2 +2830,4 @@\n-  PhaseGVN* _phase;\n-  StoreNode* _store;\n+  PhaseGVN* const _phase;\n+  StoreNode* const _store;\n+\n+  NOT_PRODUCT( const CHeapBitMap &_trace_tags; )\n@@ -3010,1 +2836,4 @@\n-  MergePrimitiveArrayStores(PhaseGVN* phase, StoreNode* store) : _phase(phase), _store(store) {}\n+  MergePrimitiveStores(PhaseGVN* phase, StoreNode* store) :\n+    _phase(phase), _store(store)\n+    NOT_PRODUCT( COMMA _trace_tags(Compile::current()->directive()->trace_merge_stores_tags()) )\n+    {}\n@@ -3041,0 +2870,11 @@\n+\n+#ifndef PRODUCT\n+    void print_on(outputStream* st) const {\n+      if (_found_store == nullptr) {\n+        st->print_cr(\"None\");\n+      } else {\n+        st->print_cr(\"Found[%d %s, %s]\", _found_store->_idx, _found_store->Name(),\n+                                         _found_range_check ? \"RC\" : \"no-RC\");\n+      }\n+    }\n+#endif\n@@ -3054,2 +2894,5 @@\n-  DEBUG_ONLY( void trace(const Node_List& merge_list, const Node* merged_input_value, const StoreNode* merged_store) const; )\n-};\n+#ifndef PRODUCT\n+  \/\/ Access to TraceMergeStores tags\n+  bool is_trace(TraceMergeStores::Tag tag) const {\n+    return _trace_tags.at(tag);\n+  }\n@@ -3057,5 +2900,2 @@\n-StoreNode* MergePrimitiveArrayStores::run() {\n-  \/\/ Check for B\/S\/C\/I\n-  int opc = _store->Opcode();\n-  if (opc != Op_StoreB && opc != Op_StoreC && opc != Op_StoreI) {\n-    return nullptr;\n+  bool is_trace_basic() const {\n+    return is_trace(TraceMergeStores::Tag::BASIC);\n@@ -3064,4 +2904,2 @@\n-  \/\/ Only merge stores on arrays, and the stores must have the same size as the elements.\n-  const TypePtr* ptr_t = _store->adr_type();\n-  if (ptr_t == nullptr) {\n-    return nullptr;\n+  bool is_trace_pointer() const {\n+    return is_trace(TraceMergeStores::Tag::POINTER);\n@@ -3069,3 +2907,3 @@\n-  const TypeAryPtr* aryptr_t = ptr_t->isa_aryptr();\n-  if (aryptr_t == nullptr) {\n-    return nullptr;\n+\n+  bool is_trace_aliasing() const {\n+    return is_trace(TraceMergeStores::Tag::ALIASING);\n@@ -3073,4 +2911,7 @@\n-  BasicType bt = aryptr_t->elem()->array_element_basic_type();\n-  if (!is_java_primitive(bt) ||\n-      type2aelembytes(bt) != _store->memory_size()) {\n-    return nullptr;\n+\n+  bool is_trace_adjacency() const {\n+    return is_trace(TraceMergeStores::Tag::ADJACENCY);\n+  }\n+\n+  bool is_trace_success() const {\n+    return is_trace(TraceMergeStores::Tag::SUCCESS);\n@@ -3078,1 +2919,9 @@\n-  if (_store->is_unsafe_access()) {\n+#endif\n+\n+  NOT_PRODUCT( void trace(const Node_List& merge_list, const Node* merged_input_value, const StoreNode* merged_store) const; )\n+};\n+\n+StoreNode* MergePrimitiveStores::run() {\n+  \/\/ Check for B\/S\/C\/I\n+  int opc = _store->Opcode();\n+  if (opc != Op_StoreB && opc != Op_StoreC && opc != Op_StoreI) {\n@@ -3082,0 +2931,2 @@\n+  NOT_PRODUCT( if (is_trace_basic()) { tty->print(\"[TraceMergeStores] MergePrimitiveStores::run: \"); _store->dump(); })\n+\n@@ -3085,0 +2936,1 @@\n+  NOT_PRODUCT( if (is_trace_basic()) { tty->print(\"[TraceMergeStores] expect no use: \"); status_use.print_on(tty); })\n@@ -3091,0 +2943,1 @@\n+  NOT_PRODUCT( if (is_trace_basic()) { tty->print(\"[TraceMergeStores] expect def: \"); status_def.print_on(tty); })\n@@ -3104,1 +2957,1 @@\n-  DEBUG_ONLY( if(TraceMergeStores) { trace(merge_list, merged_input_value, merged_store); } )\n+  NOT_PRODUCT( if (is_trace_success()) { trace(merge_list, merged_input_value, merged_store); } )\n@@ -3110,1 +2963,1 @@\n-bool MergePrimitiveArrayStores::is_compatible_store(const StoreNode* other_store) const {\n+bool MergePrimitiveStores::is_compatible_store(const StoreNode* other_store) const {\n@@ -3113,2 +2966,0 @@\n-  assert(_store->adr_type()->isa_aryptr() != nullptr, \"must be array store\");\n-  assert(!_store->is_unsafe_access(), \"no unsafe accesses\");\n@@ -3117,4 +2968,1 @@\n-      _store->Opcode() != other_store->Opcode() ||\n-      other_store->adr_type() == nullptr ||\n-      other_store->adr_type()->isa_aryptr() == nullptr ||\n-      other_store->is_unsafe_access()) {\n+      _store->Opcode() != other_store->Opcode()) {\n@@ -3124,15 +2972,0 @@\n-  \/\/ Check that the size of the stores, and the array elements are all the same.\n-  const TypeAryPtr* aryptr_t1 = _store->adr_type()->is_aryptr();\n-  const TypeAryPtr* aryptr_t2 = other_store->adr_type()->is_aryptr();\n-  BasicType aryptr_bt1 = aryptr_t1->elem()->array_element_basic_type();\n-  BasicType aryptr_bt2 = aryptr_t2->elem()->array_element_basic_type();\n-  if (!is_java_primitive(aryptr_bt1) || !is_java_primitive(aryptr_bt2)) {\n-    return false;\n-  }\n-  int size1 = type2aelembytes(aryptr_bt1);\n-  int size2 = type2aelembytes(aryptr_bt2);\n-  if (size1 != size2 ||\n-      size1 != _store->memory_size() ||\n-      _store->memory_size() != other_store->memory_size()) {\n-    return false;\n-  }\n@@ -3142,1 +2975,1 @@\n-bool MergePrimitiveArrayStores::is_adjacent_pair(const StoreNode* use_store, const StoreNode* def_store) const {\n+bool MergePrimitiveStores::is_adjacent_pair(const StoreNode* use_store, const StoreNode* def_store) const {\n@@ -3150,7 +2983,8 @@\n-  ArrayPointer array_pointer_use = ArrayPointer::make(_phase, use_store->in(MemNode::Address));\n-  ArrayPointer array_pointer_def = ArrayPointer::make(_phase, def_store->in(MemNode::Address));\n-  if (!array_pointer_def.is_adjacent_to_and_before(array_pointer_use, use_store->memory_size())) {\n-    return false;\n-  }\n-\n-  return true;\n+#ifndef PRODUCT\n+  const TraceMemPointer trace(is_trace_pointer(),\n+                              is_trace_aliasing(),\n+                              is_trace_adjacency());\n+#endif\n+  const MemPointer pointer_use(use_store NOT_PRODUCT( COMMA trace ));\n+  const MemPointer pointer_def(def_store NOT_PRODUCT( COMMA trace ));\n+  return pointer_def.is_adjacent_to_and_before(pointer_use);\n@@ -3159,1 +2993,1 @@\n-bool MergePrimitiveArrayStores::is_adjacent_input_pair(const Node* n1, const Node* n2, const int memory_size) const {\n+bool MergePrimitiveStores::is_adjacent_input_pair(const Node* n1, const Node* n2, const int memory_size) const {\n@@ -3201,1 +3035,1 @@\n-bool MergePrimitiveArrayStores::is_con_RShift(const Node* n, Node const*& base_out, jint& shift_out) {\n+bool MergePrimitiveStores::is_con_RShift(const Node* n, Node const*& base_out, jint& shift_out) {\n@@ -3224,1 +3058,1 @@\n-MergePrimitiveArrayStores::CFGStatus MergePrimitiveArrayStores::cfg_status_for_pair(const StoreNode* use_store, const StoreNode* def_store) {\n+MergePrimitiveStores::CFGStatus MergePrimitiveStores::cfg_status_for_pair(const StoreNode* use_store, const StoreNode* def_store) {\n@@ -3269,1 +3103,1 @@\n-MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_adjacent_use_store(const StoreNode* def_store) const {\n+MergePrimitiveStores::Status MergePrimitiveStores::find_adjacent_use_store(const StoreNode* def_store) const {\n@@ -3278,1 +3112,1 @@\n-MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_adjacent_def_store(const StoreNode* use_store) const {\n+MergePrimitiveStores::Status MergePrimitiveStores::find_adjacent_def_store(const StoreNode* use_store) const {\n@@ -3287,1 +3121,1 @@\n-MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_use_store(const StoreNode* def_store) const {\n+MergePrimitiveStores::Status MergePrimitiveStores::find_use_store(const StoreNode* def_store) const {\n@@ -3303,1 +3137,1 @@\n-MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_def_store(const StoreNode* use_store) const {\n+MergePrimitiveStores::Status MergePrimitiveStores::find_def_store(const StoreNode* use_store) const {\n@@ -3319,1 +3153,1 @@\n-MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_use_store_unidirectional(const StoreNode* def_store) const {\n+MergePrimitiveStores::Status MergePrimitiveStores::find_use_store_unidirectional(const StoreNode* def_store) const {\n@@ -3332,1 +3166,1 @@\n-MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_def_store_unidirectional(const StoreNode* use_store) const {\n+MergePrimitiveStores::Status MergePrimitiveStores::find_def_store_unidirectional(const StoreNode* use_store) const {\n@@ -3343,1 +3177,1 @@\n-void MergePrimitiveArrayStores::collect_merge_list(Node_List& merge_list) const {\n+void MergePrimitiveStores::collect_merge_list(Node_List& merge_list) const {\n@@ -3356,0 +3190,2 @@\n+    NOT_PRODUCT( if (is_trace_basic()) { tty->print(\"[TraceMergeStores] find def: \"); status.print_on(tty); })\n+\n@@ -3362,0 +3198,1 @@\n+        NOT_PRODUCT( if (is_trace_basic()) { tty->print_cr(\"[TraceMergeStores] found RangeCheck, stop traversal.\"); })\n@@ -3367,0 +3204,2 @@\n+  NOT_PRODUCT( if (is_trace_basic()) { tty->print_cr(\"[TraceMergeStores] found:\"); merge_list.dump(); })\n+\n@@ -3371,0 +3210,2 @@\n+\n+  NOT_PRODUCT( if (is_trace_basic()) { tty->print_cr(\"[TraceMergeStores] truncated:\"); merge_list.dump(); })\n@@ -3374,1 +3215,1 @@\n-Node* MergePrimitiveArrayStores::make_merged_input_value(const Node_List& merge_list) {\n+Node* MergePrimitiveStores::make_merged_input_value(const Node_List& merge_list) {\n@@ -3460,1 +3301,1 @@\n-StoreNode* MergePrimitiveArrayStores::make_merged_store(const Node_List& merge_list, Node* merged_input_value) {\n+StoreNode* MergePrimitiveStores::make_merged_store(const Node_List& merge_list, Node* merged_input_value) {\n@@ -3489,2 +3330,2 @@\n-#ifdef ASSERT\n-void MergePrimitiveArrayStores::trace(const Node_List& merge_list, const Node* merged_input_value, const StoreNode* merged_store) const {\n+#ifndef PRODUCT\n+void MergePrimitiveStores::trace(const Node_List& merge_list, const Node* merged_input_value, const StoreNode* merged_store) const {\n@@ -3589,1 +3430,1 @@\n-      MergePrimitiveArrayStores merge(phase, this);\n+      MergePrimitiveStores merge(phase, this);\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":88,"deletions":247,"binary":false,"changes":335,"status":"modified"},{"patch":"@@ -857,1 +857,8 @@\n-        (void)FillLocArray(mv->possible_objects()->length(), sfpt, obj_node, mv->possible_objects(), objs);\n+        int idx = mv->possible_objects()->length();\n+        (void)FillLocArray(idx, sfpt, obj_node, mv->possible_objects(), objs);\n+\n+        \/\/ By default ObjectValues that are in 'possible_objects' are not root objects.\n+        \/\/ They will be marked as root later if they are directly referenced in a JVMS.\n+        assert(mv->possible_objects()->length() > idx, \"Didn't add entry to possible_objects?!\");\n+        assert(mv->possible_objects()->at(idx)->is_object(), \"Entries in possible_objects should be ObjectValue.\");\n+        mv->possible_objects()->at(idx)->as_ObjectValue()->set_root(false);\n@@ -1196,1 +1203,8 @@\n-            FillLocArray(mv->possible_objects()->length(), sfn, obj_node, mv->possible_objects(), objs);\n+            int idx = mv->possible_objects()->length();\n+            (void)FillLocArray(idx, sfn, obj_node, mv->possible_objects(), objs);\n+\n+            \/\/ By default ObjectValues that are in 'possible_objects' are not root objects.\n+            \/\/ They will be marked as root later if they are directly referenced in a JVMS.\n+            assert(mv->possible_objects()->length() > idx, \"Didn't add entry to possible_objects?!\");\n+            assert(mv->possible_objects()->at(idx)->is_object(), \"Entries in possible_objects should be ObjectValue.\");\n+            mv->possible_objects()->at(idx)->as_ObjectValue()->set_root(false);\n@@ -1227,5 +1241,11 @@\n-          bool is_root = locarray->contains(ov) ||\n-                         exparray->contains(ov) ||\n-                         contains_as_owner(monarray, ov) ||\n-                         contains_as_scalarized_obj(jvms, sfn, objs, ov);\n-          ov->set_root(is_root);\n+          if (ov->is_root()) {\n+            \/\/ Already flagged as 'root' by something else. We shouldn't change it\n+            \/\/ to non-root in a younger JVMS because it may need to be alive in\n+            \/\/ a younger JVMS.\n+          } else {\n+            bool is_root = locarray->contains(ov) ||\n+                           exparray->contains(ov) ||\n+                           contains_as_owner(monarray, ov) ||\n+                           contains_as_scalarized_obj(jvms, sfn, objs, ov);\n+            ov->set_root(is_root);\n+          }\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":27,"deletions":7,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -1748,0 +1748,8 @@\n+  if (use->Opcode() == Op_AddX) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->Opcode() == Op_CastX2P) {\n+        worklist.push(u);\n+      }\n+    }\n+  }\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -3948,0 +3948,3 @@\n+  \/\/ Want this inside 'attaching via jni'.\n+  JFR_ONLY(Jfr::on_thread_start(thread);)\n+\n@@ -3962,2 +3965,0 @@\n-  JFR_ONLY(Jfr::on_thread_start(thread);)\n-\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -4059,13 +4059,0 @@\n-\/\/ Always update the temporary VTMS transition bit.\n-JVM_ENTRY(void, JVM_VirtualThreadHideFrames(JNIEnv* env, jclass clazz, jboolean hide))\n-#if INCLUDE_JVMTI\n-  if (!DoJVMTIVirtualThreadTransitions) {\n-    assert(!JvmtiExport::can_support_virtual_threads(), \"sanity check\");\n-    return;\n-  }\n-  assert(!thread->is_in_VTMS_transition(), \"sanity check\");\n-  assert(thread->is_in_tmp_VTMS_transition() != (bool)hide, \"sanity check\");\n-  thread->toggle_is_in_tmp_VTMS_transition();\n-#endif\n-JVM_END\n-\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":0,"deletions":13,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -1088,0 +1088,4 @@\n+  if (method->is_abstract()) {\n+    tty->print_cr(\"WB error: request to compile abstract method\");\n+    return false;\n+  }\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -89,0 +89,1 @@\n+unsigned int Arguments::_addmods_count          = 0;\n@@ -340,0 +341,4 @@\n+bool Arguments::is_add_modules_property(const char* key) {\n+  return (strcmp(key, MODULE_PROPERTY_PREFIX ADDMODS) == 0);\n+}\n+\n@@ -1777,1 +1782,0 @@\n-unsigned int addmods_count = 0;\n@@ -1802,1 +1806,1 @@\n-      if (!create_numbered_module_property(\"jdk.module.addmods\", \"jdk.internal.vm.ci\", addmods_count++)) {\n+      if (!create_numbered_module_property(\"jdk.module.addmods\", \"jdk.internal.vm.ci\", _addmods_count++)) {\n@@ -1811,1 +1815,1 @@\n-    if (!create_numbered_module_property(\"jdk.module.addmods\", \"jdk.jfr\", addmods_count++)) {\n+    if (!create_numbered_module_property(\"jdk.module.addmods\", \"jdk.jfr\", _addmods_count++)) {\n@@ -2297,1 +2301,1 @@\n-      if (!create_numbered_module_property(\"jdk.module.addmods\", tail, addmods_count++)) {\n+      if (!create_numbered_module_property(\"jdk.module.addmods\", tail, _addmods_count++)) {\n@@ -2384,1 +2388,1 @@\n-        if (!create_numbered_module_property(\"jdk.module.addmods\", \"java.instrument\", addmods_count++)) {\n+        if (!create_numbered_module_property(\"jdk.module.addmods\", \"java.instrument\", _addmods_count++)) {\n@@ -2569,1 +2573,1 @@\n-        if (!create_numbered_module_property(\"jdk.module.addmods\", \"jdk.management.agent\", addmods_count++)) {\n+        if (!create_numbered_module_property(\"jdk.module.addmods\", \"jdk.management.agent\", _addmods_count++)) {\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":10,"deletions":6,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -200,0 +200,2 @@\n+  \/\/ number of unique modules specified in the --add-modules option\n+  static unsigned int _addmods_count;\n@@ -465,0 +467,2 @@\n+  static bool is_add_modules_property(const char* key);\n+  static unsigned int addmods_count() { return  _addmods_count; }\n","filename":"src\/hotspot\/share\/runtime\/arguments.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -450,1 +450,0 @@\n-  _is_in_tmp_VTMS_transition(false),\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -315,1 +315,0 @@\n-  bool                  _is_in_tmp_VTMS_transition;      \/\/ thread is in temporary virtual thread mount state transition\n@@ -679,4 +678,0 @@\n-  bool is_in_tmp_VTMS_transition() const         { return _is_in_tmp_VTMS_transition; }\n-  bool is_in_any_VTMS_transition() const         { return _is_in_VTMS_transition || _is_in_tmp_VTMS_transition; }\n-\n-  void toggle_is_in_tmp_VTMS_transition()        { _is_in_tmp_VTMS_transition = !_is_in_tmp_VTMS_transition; };\n@@ -859,1 +854,0 @@\n-  static ByteSize is_in_tmp_VTMS_transition_offset() { return byte_offset_of(JavaThread, _is_in_tmp_VTMS_transition); }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -169,0 +169,3 @@\n+  JFR_ONLY(assert(JFR_JVM_THREAD_ID(thread) == static_cast<traceid>(java_lang_Thread::thread_id(thread_oop())),\n+             \"initial tid mismatch\");)\n+\n@@ -536,1 +539,1 @@\n-  if (!main_thread->set_as_starting_thread()) {\n+  if (!Thread::set_as_starting_thread(main_thread)) {\n@@ -544,0 +547,2 @@\n+  JFR_ONLY(Jfr::initialize_main_thread(main_thread);)\n+\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -121,1 +121,1 @@\n-            if (archivedCache == null || archivedCache.length != size) {\n+            if (archivedCache == null) {\n@@ -130,0 +130,1 @@\n+            assert cache.length == size;\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Byte.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -8988,1 +8988,1 @@\n-            if (archivedCache == null || archivedCache.length != size) {\n+            if (archivedCache == null) {\n@@ -8996,0 +8996,1 @@\n+            assert cache.length == size;\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Character.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -155,1 +155,1 @@\n- * is situated in a <dfn>{@index \"nest\"}<\/dfn>. A <a id=\"nest\">nest<\/a> is a set of\n+ * is situated in a <dfn>{@index \"nest\"}<\/dfn>. A nest is a set of\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Class.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -965,1 +965,11 @@\n-                for(int i = 0; i < c.length; i++) {\n+                \/\/ If archive has Integer cache, we must use all instances from it.\n+                \/\/ Otherwise, the identity checks between archived Integers and\n+                \/\/ runtime-cached Integers would fail.\n+                int archivedSize = (archivedCache == null) ? 0 : archivedCache.length;\n+                for (int i = 0; i < archivedSize; i++) {\n+                    c[i] = archivedCache[i];\n+                    assert j == archivedCache[i];\n+                    j++;\n+                }\n+                \/\/ Fill the rest of the cache.\n+                for (int i = archivedSize; i < size; i++) {\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Integer.java","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -967,1 +967,1 @@\n-            if (archivedCache == null || archivedCache.length != size) {\n+            if (archivedCache == null) {\n@@ -976,0 +976,1 @@\n+            assert cache.length == size;\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Long.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -248,1 +248,1 @@\n-            if (archivedCache == null || archivedCache.length != size) {\n+            if (archivedCache == null) {\n@@ -257,0 +257,1 @@\n+            assert cache.length == size;\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Short.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1996, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1996, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -79,15 +79,11 @@\n-    \/\/ generic info repository; lazily initialized\n-    private transient volatile FieldRepository genericInfo;\n-    \/\/ Cached field accessor created without override\n-    @Stable\n-    private FieldAccessor fieldAccessor;\n-    \/\/ Cached field accessor created with override\n-    @Stable\n-    private FieldAccessor overrideFieldAccessor;\n-    \/\/ For sharing of FieldAccessors. This branching structure is\n-    \/\/ currently only two levels deep (i.e., one root Field and\n-    \/\/ potentially many Field objects pointing to it.)\n-    \/\/\n-    \/\/ If this branching structure would ever contain cycles, deadlocks can\n-    \/\/ occur in annotation code.\n-    private Field               root;\n+\n+    \/**\n+     * Fields are mutable due to {@link AccessibleObject#setAccessible(boolean)}.\n+     * Thus, we return a new copy of a root each time a field is returned.\n+     * Some lazily initialized immutable states can be stored on root and shared to the copies.\n+     *\/\n+    private Field root;\n+    private transient volatile FieldRepository genericInfo;\n+    private @Stable FieldAccessor fieldAccessor; \/\/ access control enabled\n+    private @Stable FieldAccessor overrideFieldAccessor; \/\/ access control suppressed\n+    \/\/ End shared states\n@@ -110,4 +106,6 @@\n-        \/\/ lazily initialize repository if necessary\n-            \/\/ create and cache generic info repository\n-            genericInfo = FieldRepository.make(getGenericSignature(),\n-                                               getFactory());\n+            var root = this.root;\n+            if (root != null) {\n+                genericInfo = root.getGenericInfo();\n+            } else {\n+                genericInfo = FieldRepository.make(getGenericSignature(), getFactory());\n+            }\n@@ -117,1 +115,1 @@\n-        return genericInfo; \/\/return cached repository\n+        return genericInfo;\n@@ -120,1 +118,0 @@\n-\n@@ -165,0 +162,1 @@\n+        res.genericInfo = genericInfo;\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/Field.java","additions":20,"deletions":22,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -75,1 +75,1 @@\n-        @JEP(number=481, title=\"Scoped Values\", status=\"Third Preview\")\n+        @JEP(number=487, title=\"Scoped Values\", status=\"Fourth Preview\")\n@@ -85,0 +85,2 @@\n+        @JEP(number=478, title=\"Key Derivation Function API\", status=\"Preview\")\n+        KEY_DERIVATION,\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/javac\/PreviewFeature.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -279,0 +279,1 @@\n+        PRIVATE_MEMBERS_IN_PERMITS_CLAUSE(JDK19),\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Source.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -95,0 +95,4 @@\n+    \/** Are we attributing a permits clause?\n+     *\/\n+    boolean isPermitsClause = false;\n+\n@@ -155,0 +159,1 @@\n+        info.isPermitsClause = isPermitsClause;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/AttrContext.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -114,0 +114,1 @@\n+    private final boolean allowPrivateMembersInPermitsClause;\n@@ -150,0 +151,1 @@\n+        allowPrivateMembersInPermitsClause = Feature.PRIVATE_MEMBERS_IN_PERMITS_CLAUSE.allowedInSource(source);\n@@ -430,2 +432,4 @@\n-                                            sym.owner.outermostClass())\n-                                    &&\n+                                    sym.owner.outermostClass()\n+                                    ||\n+                                    privateMemberInPermitsClauseIfAllowed(env, sym))\n+                                &&\n@@ -466,0 +470,7 @@\n+\n+    private boolean privateMemberInPermitsClauseIfAllowed(Env<AttrContext> env, Symbol sym) {\n+        return allowPrivateMembersInPermitsClause &&\n+            env.info.isPermitsClause &&\n+            ((JCClassDecl) env.tree).sym.outermostClass() == sym.owner.outermostClass();\n+    }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Resolve.java","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -983,3 +983,10 @@\n-                for (JCExpression permitted : permittedTrees) {\n-                    Type pt = attr.attribBase(permitted, baseEnv, false, false, false);\n-                    permittedSubtypeSymbols.append(pt.tsym);\n+                var isPermitsClause = baseEnv.info.isPermitsClause;\n+                try {\n+                    baseEnv.info.isPermitsClause = true;\n+                    for (JCExpression permitted : permittedTrees) {\n+                        Type pt = attr.attribBase(permitted, baseEnv, false, false, false);\n+                        permittedSubtypeSymbols.append(pt.tsym);\n+                    }\n+                    sym.setPermittedSubclasses(permittedSubtypeSymbols.toList());\n+                } finally {\n+                    baseEnv.info.isPermitsClause = isPermitsClause;\n@@ -987,1 +994,0 @@\n-                sym.setPermittedSubclasses(permittedSubtypeSymbols.toList());\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/TypeEnter.java","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -151,1 +151,1 @@\n-    Don't accept generics in the language\n+    Don''t accept generics in the language\n@@ -181,1 +181,1 @@\n-    Precede a key by '-' to disable the specified warning.\\n\\\n+    Precede a key by ''-'' to disable the specified warning.\\n\\\n@@ -319,1 +319,1 @@\n-    the qualified name of a package or a package name prefix followed by '.*',\\n\\\n+    a qualified package name or a package name prefix followed by ''.*'',\\n\\\n@@ -321,1 +321,1 @@\n-    can be prefixed with '-' to disable checks for the specified package(s).\n+    can be prefixed with ''-'' to disable checks for the specified package(s).\n@@ -388,1 +388,1 @@\n-    Disable support for documentation comments with lines beginning '\/\/\/'\n+    Disable support for documentation comments with lines beginning ''\/\/\/''\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/javac.properties","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -108,1 +108,1 @@\n-    if ( JNI_FUNC_PTR(env,ExceptionOccurred)(env) ) {\n+    if ( JNI_FUNC_PTR(env,ExceptionCheck)(env) ) {\n@@ -138,1 +138,1 @@\n-    if ( JNI_FUNC_PTR(env,ExceptionOccurred)(env) ) {\n+    if ( JNI_FUNC_PTR(env,ExceptionCheck)(env) ) {\n@@ -169,1 +169,1 @@\n-    if ( JNI_FUNC_PTR(env,ExceptionOccurred)(env) ) {\n+    if ( JNI_FUNC_PTR(env,ExceptionCheck)(env) ) {\n@@ -269,1 +269,1 @@\n-            if (JNI_FUNC_PTR(env,ExceptionOccurred)(env)) {\n+            if (JNI_FUNC_PTR(env,ExceptionCheck)(env)) {\n@@ -279,1 +279,1 @@\n-            if (JNI_FUNC_PTR(env,ExceptionOccurred)(env)) {\n+            if (JNI_FUNC_PTR(env,ExceptionCheck)(env)) {\n@@ -857,1 +857,1 @@\n-        if (JNI_FUNC_PTR(env,ExceptionOccurred)(env)) {\n+        if (JNI_FUNC_PTR(env,ExceptionCheck)(env)) {\n@@ -866,1 +866,1 @@\n-        if (JNI_FUNC_PTR(env,ExceptionOccurred)(env)) {\n+        if (JNI_FUNC_PTR(env,ExceptionCheck)(env)) {\n@@ -877,1 +877,1 @@\n-        if (JNI_FUNC_PTR(env,ExceptionOccurred)(env)) {\n+        if (JNI_FUNC_PTR(env,ExceptionCheck)(env)) {\n@@ -1595,1 +1595,1 @@\n-    if (JNI_FUNC_PTR(env,ExceptionOccurred)(env)) {\n+    if (JNI_FUNC_PTR(env,ExceptionCheck)(env)) {\n@@ -1602,1 +1602,1 @@\n-        if (JNI_FUNC_PTR(env,ExceptionOccurred)(env)) {\n+        if (JNI_FUNC_PTR(env,ExceptionCheck)(env)) {\n@@ -1649,1 +1649,1 @@\n-    if (JNI_FUNC_PTR(env,ExceptionOccurred)(env)) {\n+    if (JNI_FUNC_PTR(env,ExceptionCheck)(env)) {\n","filename":"src\/jdk.jdwp.agent\/share\/native\/libjdwp\/util.c","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -120,0 +120,8 @@\n+compiler\/jvmci\/jdk.vm.ci.code.test\/src\/jdk\/vm\/ci\/code\/test\/DataPatchTest.java                   8343233 generic-all\n+compiler\/jvmci\/jdk.vm.ci.code.test\/src\/jdk\/vm\/ci\/code\/test\/InterpreterFrameSizeTest.java        8343233 generic-aarch64\n+compiler\/jvmci\/jdk.vm.ci.code.test\/src\/jdk\/vm\/ci\/code\/test\/MaxOopMapStackOffsetTest.java        8343233 generic-aarch64\n+compiler\/jvmci\/jdk.vm.ci.code.test\/src\/jdk\/vm\/ci\/code\/test\/NativeCallTest.java                  8343233 generic-aarch64\n+compiler\/jvmci\/jdk.vm.ci.code.test\/src\/jdk\/vm\/ci\/code\/test\/SimpleCodeInstallationTest.java      8343233 generic-aarch64\n+compiler\/jvmci\/jdk.vm.ci.code.test\/src\/jdk\/vm\/ci\/code\/test\/SimpleDebugInfoTest.java             8343233 generic-aarch64\n+compiler\/jvmci\/jdk.vm.ci.code.test\/src\/jdk\/vm\/ci\/code\/test\/VirtualObjectDebugInfoTest.java      8343233 generic-aarch64\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList-zgc.txt","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -58,1 +58,0 @@\n-compiler\/codecache\/CheckLargePages.java 8332654 linux-x64\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -45,5 +45,0 @@\n-hotspot_compiler_all_gcs = \\\n-  :hotspot_compiler \\\n-  -compiler\/valhalla\/inlinetypes\/TestWrongFlatArrayCopyStubWithZGC.java \\\n-  -compiler\/jvmci\n-\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -229,0 +229,5 @@\n+    public static final String ADD_P = PREFIX + \"ADD_P\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(ADD_P, \"AddP\");\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+ * @requires vm.gc.Z\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestWrongFlatArrayCopyStubWithZGC.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -73,0 +73,3 @@\n+# Direct buffer memory allocated before test launch\n+java\/nio\/Buffer\/LimitDirectMemory.java 8342849 generic-all\n+\n","filename":"test\/jdk\/ProblemList-Virtual.txt","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -592,2 +592,0 @@\n-java\/nio\/Buffer\/LimitDirectMemory.java                          8342849 generic-all\n-\n","filename":"test\/jdk\/ProblemList.txt","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -527,1 +527,1 @@\n-        for (String name : new String[]{\"compiler\", \"launcher\"}) {\n+        for (String name : new String[]{\"javac\", \"compiler\", \"launcher\"}) {\n","filename":"test\/langtools\/tools\/javac\/diags\/CheckResourceKeys.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}