{"files":[{"patch":"@@ -143,3 +143,0 @@\n-BUILD_HOTSPOT_JTREG_LIBRARIES_CFLAGS := -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS $(JVMTI_COMMON_INCLUDES)\n-BUILD_HOTSPOT_JTREG_EXECUTABLES_CFLAGS := -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS $(JVMTI_COMMON_INCLUDES)\n-\n@@ -1532,0 +1529,4 @@\n+# These apply to all tests\n+BUILD_HOTSPOT_JTREG_LIBRARIES_CFLAGS := -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS $(JVMTI_COMMON_INCLUDES)\n+BUILD_HOTSPOT_JTREG_EXECUTABLES_CFLAGS := -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS $(JVMTI_COMMON_INCLUDES)\n+\n","filename":"make\/test\/JtregNativeHotspot.gmk","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -228,1 +228,1 @@\n-    CompiledMethod* nm = sender_blob->as_compiled_method_or_null();\n+    nmethod* nm = sender_blob->as_nmethod_or_null();\n@@ -240,1 +240,1 @@\n-      assert(!sender_blob->is_compiled(), \"should count return address at least\");\n+      assert(!sender_blob->is_nmethod(), \"should count return address at least\");\n@@ -249,1 +249,1 @@\n-    if (!sender_blob->is_compiled()) {\n+    if (!sender_blob->is_nmethod()) {\n@@ -303,1 +303,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = get_deopt_original_pc();\n@@ -432,1 +432,1 @@\n-void frame::verify_deopt_original_pc(CompiledMethod* nm, intptr_t* unextended_sp) {\n+void frame::verify_deopt_original_pc(nmethod* nm, intptr_t* unextended_sp) {\n@@ -455,2 +455,2 @@\n-    CompiledMethod* sender_cm = _cb->as_compiled_method_or_null();\n-    if (sender_cm != nullptr) {\n+    nmethod* sender_nm = _cb->as_nmethod_or_null();\n+    if (sender_nm != nullptr) {\n@@ -458,3 +458,3 @@\n-      if (sender_cm->is_deopt_entry(_pc) ||\n-          sender_cm->is_deopt_mh_entry(_pc)) {\n-        verify_deopt_original_pc(sender_cm, _unextended_sp);\n+      if (sender_nm->is_deopt_entry(_pc) ||\n+          sender_nm->is_deopt_mh_entry(_pc)) {\n+        verify_deopt_original_pc(sender_nm, _unextended_sp);\n@@ -797,2 +797,2 @@\n-  CompiledMethod* cm = _cb->as_compiled_method_or_null();\n-  if (cm != nullptr && cm->needs_stack_repair()) {\n+  nmethod* nm = _cb->as_nmethod_or_null();\n+  if (nm != nullptr && nm->needs_stack_repair()) {\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.cpp","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -159,1 +159,1 @@\n-  static void verify_deopt_original_pc(   CompiledMethod* nm, intptr_t* unextended_sp);\n+  static void verify_deopt_original_pc(nmethod* nm, intptr_t* unextended_sp);\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -77,1 +77,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = get_deopt_original_pc();\n@@ -81,1 +81,1 @@\n-    assert(_cb == nullptr || _cb->as_compiled_method()->insts_contains_inclusive(_pc),\n+    assert(_cb == nullptr || _cb->as_nmethod()->insts_contains_inclusive(_pc),\n@@ -184,1 +184,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = get_deopt_original_pc();\n@@ -246,2 +246,2 @@\n-  assert(cb()->is_compiled(), \"\");\n-  return (cb()->as_compiled_method()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord;\n+  assert(cb()->is_nmethod(), \"\");\n+  return (cb()->as_nmethod()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord;\n@@ -451,1 +451,1 @@\n-    if (!_cb->is_compiled() || c1_buffering) { \/\/ compiled frames do not use callee-saved registers\n+    if (!_cb->is_nmethod() || c1_buffering) { \/\/ compiled frames do not use callee-saved registers\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.inline.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -91,2 +91,0 @@\n-  product(bool, UseNeon, false,                                         \\\n-          \"Use Neon for CRC32 computation\")                             \\\n","filename":"src\/hotspot\/cpu\/aarch64\/globals_aarch64.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -874,1 +874,1 @@\n-          !CodeCache::find_blob(target)->is_compiled(),\n+          !CodeCache::find_blob(target)->is_nmethod(),\n@@ -1241,1 +1241,1 @@\n-  \/\/ %%% Could store the aligned, prescaled offset in the klassoop.\n+  \/\/ Could store the aligned, prescaled offset in the klass.\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1054,2 +1054,3 @@\n-  jccb(Assembler::zero, zf_correct);\n-  stop(\"Fast Lock ZF != 1\");\n+  Label zf_bad_zero;\n+  jcc(Assembler::zero, zf_correct);\n+  jmp(zf_bad_zero);\n@@ -1061,1 +1062,1 @@\n-  jccb(Assembler::notZero, zf_correct);\n+  jcc(Assembler::notZero, zf_correct);\n@@ -1063,0 +1064,2 @@\n+  bind(zf_bad_zero);\n+  stop(\"Fast Lock ZF != 1\");\n@@ -1193,1 +1196,1 @@\n-  jccb(Assembler::zero, zf_correct);\n+  jcc(Assembler::zero, zf_correct);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -98,1 +98,1 @@\n-      if (_cb->is_compiled() || _cb->is_adapter_blob() || _cb->is_runtime_stub()) {\n+      if (_cb->is_nmethod() || _cb->is_adapter_blob() || _cb->is_runtime_stub()) {\n@@ -219,1 +219,1 @@\n-    CompiledMethod* nm = sender_blob->as_compiled_method_or_null();\n+    nmethod* nm = sender_blob->as_nmethod_or_null();\n@@ -231,1 +231,1 @@\n-      assert(!sender_blob->is_compiled(), \"should count return address at least\");\n+      assert(!sender_blob->is_nmethod(), \"should count return address at least\");\n@@ -240,1 +240,1 @@\n-    if (!sender_blob->is_compiled()) {\n+    if (!sender_blob->is_nmethod()) {\n@@ -289,1 +289,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = get_deopt_original_pc();\n@@ -297,1 +297,1 @@\n-  assert(!is_compiled_frame() || !_cb->as_compiled_method()->is_deopt_entry(_pc), \"must be\");\n+  assert(!is_compiled_frame() || !_cb->as_nmethod()->is_deopt_entry(_pc), \"must be\");\n@@ -421,1 +421,1 @@\n-void frame::verify_deopt_original_pc(CompiledMethod* nm, intptr_t* unextended_sp) {\n+void frame::verify_deopt_original_pc(nmethod* nm, intptr_t* unextended_sp) {\n@@ -444,2 +444,2 @@\n-    CompiledMethod* sender_cm = _cb->as_compiled_method_or_null();\n-    if (sender_cm != nullptr) {\n+    nmethod* sender_nm = _cb->as_nmethod_or_null();\n+    if (sender_nm != nullptr) {\n@@ -447,3 +447,3 @@\n-      if (sender_cm->is_deopt_entry(_pc) ||\n-          sender_cm->is_deopt_mh_entry(_pc)) {\n-        verify_deopt_original_pc(sender_cm, _unextended_sp);\n+      if (sender_nm->is_deopt_entry(_pc) ||\n+          sender_nm->is_deopt_mh_entry(_pc)) {\n+        verify_deopt_original_pc(sender_nm, _unextended_sp);\n@@ -671,2 +671,2 @@\n-  CompiledMethod* cm = _cb->as_compiled_method_or_null();\n-  if (cm != nullptr && cm->needs_stack_repair()) {\n+  nmethod* nm = _cb->as_nmethod_or_null();\n+  if (nm != nullptr && nm->needs_stack_repair()) {\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.cpp","additions":15,"deletions":15,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -152,1 +152,1 @@\n-  static void verify_deopt_original_pc(CompiledMethod* nm, intptr_t* unextended_sp);\n+  static void verify_deopt_original_pc(nmethod* nm, intptr_t* unextended_sp);\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -72,1 +72,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = get_deopt_original_pc();\n@@ -76,1 +76,1 @@\n-    assert(_cb == nullptr || _cb->as_compiled_method()->insts_contains_inclusive(_pc),\n+    assert(_cb == nullptr || _cb->as_nmethod()->insts_contains_inclusive(_pc),\n@@ -170,1 +170,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = get_deopt_original_pc();\n@@ -232,2 +232,2 @@\n-  assert(cb()->is_compiled(), \"\");\n-  return (cb()->as_compiled_method()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord;\n+  assert(cb()->is_nmethod(), \"\");\n+  return (cb()->as_nmethod()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord;\n@@ -437,1 +437,1 @@\n-    if (!_cb->is_compiled() || c1_buffering) { \/\/ compiled frames do not use callee-saved registers\n+    if (!_cb->is_nmethod() || c1_buffering) { \/\/ compiled frames do not use callee-saved registers\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.inline.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -301,2 +301,0 @@\n-  \/\/ Generated code assumes that buffer index is pointer sized.\n-  STATIC_ASSERT(in_bytes(SATBMarkQueue::byte_width_of_index()) == sizeof(intptr_t));\n@@ -353,0 +351,3 @@\n+  \/\/ The code below assumes that buffer index is pointer sized.\n+  STATIC_ASSERT(in_bytes(G1DirtyCardQueue::byte_width_of_index()) == sizeof(intptr_t));\n+\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -4642,1 +4642,1 @@\n-  \/\/ %%% Could store the aligned, prescaled offset in the klassoop.\n+  \/\/ Could store the aligned, prescaled offset in the klass.\n@@ -6546,1 +6546,1 @@\n-  assert(UseAVX > 2 && VM_Version::supports_avx512vlbw(), \"\");\n+  assert(UseAVX > 2 && VM_Version::supports_avx512vl(), \"\");\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2954,0 +2954,2 @@\n+    if (sef_cpuid7_ecx.bits.gfni != 0)\n+        result |= CPU_GFNI;\n@@ -2979,2 +2981,0 @@\n-      if (sef_cpuid7_ecx.bits.gfni != 0)\n-        result |= CPU_GFNI;\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1753,1 +1753,0 @@\n-    case Op_ClearArray:\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -11470,2 +11470,2 @@\n-\/\/ fast clearing of an array\n-\/\/ Small ClearArray non-AVX512.\n+\/\/ Fast clearing of an array\n+\/\/ Small non-constant length ClearArray for non-AVX512 targets.\n@@ -11531,1 +11531,1 @@\n-\/\/ Small ClearArray AVX512 non-constant length.\n+\/\/ Small non-constant length ClearArray for AVX512 targets.\n@@ -11592,1 +11592,1 @@\n-\/\/ Large ClearArray non-AVX512.\n+\/\/ Large non-constant length ClearArray for non-AVX512 targets.\n@@ -11642,1 +11642,1 @@\n-\/\/ Large ClearArray AVX512.\n+\/\/ Large non-constant length ClearArray for AVX512 targets.\n@@ -11692,1 +11692,1 @@\n-\/\/ Small ClearArray AVX512 constant length.\n+\/\/ Small constant length ClearArray for AVX512 targets.\n@@ -11695,2 +11695,1 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() &&\n-               ((UseAVX > 2) && VM_Version::supports_avx512vlbw()));\n+  predicate(!((ClearArrayNode*)n)->is_large() && (MaxVectorSize >= 32) && VM_Version::supports_avx512vl());\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":7,"deletions":8,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -5690,1 +5690,1 @@\n-instruct bytes_reversebit_int_gfni(rRegI dst, rRegI src, regF xtmp1, regF xtmp2, rRegL rtmp, rFlagsReg cr) %{\n+instruct bytes_reversebit_int_gfni(rRegI dst, rRegI src, vlRegF xtmp1, vlRegF xtmp2, rRegL rtmp, rFlagsReg cr) %{\n@@ -5712,1 +5712,1 @@\n-instruct bytes_reversebit_long_gfni(rRegL dst, rRegL src, regD xtmp1, regD xtmp2, rRegL rtmp, rFlagsReg cr) %{\n+instruct bytes_reversebit_long_gfni(rRegL dst, rRegL src, vlRegD xtmp1, vlRegD xtmp2, rRegL rtmp, rFlagsReg cr) %{\n@@ -10000,0 +10000,1 @@\n+  predicate(UseAVX == 0);\n@@ -10022,0 +10023,1 @@\n+  predicate(UseAVX == 0);\n@@ -10098,1 +10100,1 @@\n-instruct convI2F_reg_reg(regF dst, rRegI src)\n+instruct convI2F_reg_reg(vlRegF dst, rRegI src)\n@@ -10105,0 +10107,3 @@\n+    if (UseAVX > 0) {\n+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n+    }\n@@ -10112,0 +10117,1 @@\n+  predicate(UseAVX == 0);\n@@ -10121,1 +10127,1 @@\n-instruct convI2D_reg_reg(regD dst, rRegI src)\n+instruct convI2D_reg_reg(vlRegD dst, rRegI src)\n@@ -10128,0 +10134,3 @@\n+    if (UseAVX > 0) {\n+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n+    }\n@@ -10135,0 +10144,1 @@\n+  predicate(UseAVX == 0);\n@@ -10172,1 +10182,1 @@\n-instruct convL2F_reg_reg(regF dst, rRegL src)\n+instruct convL2F_reg_reg(vlRegF dst, rRegL src)\n@@ -10178,0 +10188,3 @@\n+    if (UseAVX > 0) {\n+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n+    }\n@@ -10185,0 +10198,1 @@\n+  predicate(UseAVX == 0);\n@@ -10194,1 +10208,1 @@\n-instruct convL2D_reg_reg(regD dst, rRegL src)\n+instruct convL2D_reg_reg(vlRegD dst, rRegL src)\n@@ -10200,0 +10214,3 @@\n+    if (UseAVX > 0) {\n+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n+    }\n@@ -10207,0 +10224,1 @@\n+  predicate(UseAVX == 0);\n@@ -10434,1 +10452,1 @@\n-\/\/ Small ClearArray non-AVX512.\n+\/\/ Small non-constant lenght ClearArray for non-AVX512 targets.\n@@ -10550,1 +10568,1 @@\n-\/\/ Small ClearArray AVX512 non-constant length.\n+\/\/ Small non-constant length ClearArray for AVX512 targets.\n@@ -10671,1 +10689,1 @@\n-\/\/ Large ClearArray non-AVX512.\n+\/\/ Large non-constant length ClearArray for non-AVX512 targets.\n@@ -10767,1 +10785,1 @@\n-\/\/ Large ClearArray AVX512.\n+\/\/ Large non-constant length ClearArray for AVX512 targets.\n@@ -10868,1 +10886,1 @@\n-\/\/ Small ClearArray AVX512 constant length.\n+\/\/ Small constant length ClearArray for AVX512 targets.\n@@ -10872,1 +10890,1 @@\n-            ((UseAVX > 2) && VM_Version::supports_avx512vlbw()));\n+            ((MaxVectorSize >= 32) && VM_Version::supports_avx512vl()));\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":30,"deletions":12,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -1355,4 +1355,4 @@\n-  Klass* univ_klass_obj = Universe::byteArrayKlassObj();\n-  assert(univ_klass_obj->modifier_flags() == (JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC\n-                                              | (Arguments::enable_preview() ? JVM_ACC_IDENTITY : 0)), \"Sanity\");\n-  LIR_Opr prim_klass = LIR_OprFact::metadataConst(univ_klass_obj);\n+  Klass* univ_klass = Universe::byteArrayKlass();\n+  assert(univ_klass->modifier_flags() == (JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC\n+                                          | (Arguments::enable_preview() ? JVM_ACC_IDENTITY : 0)), \"Sanity\");\n+  LIR_Opr prim_klass = LIR_OprFact::metadataConst(univ_klass);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -44,1 +44,0 @@\n-\n@@ -65,1 +64,1 @@\n-GrowableArrayCHeap<int, mtClassShared>* ArchiveHeapWriter::_source_objs_order;\n+GrowableArrayCHeap<ArchiveHeapWriter::HeapObjOrder, mtClassShared>* ArchiveHeapWriter::_source_objs_order;\n@@ -82,1 +81,1 @@\n-    _buffer_offset_to_source_obj_table = new BufferOffsetToSourceObjectTable();\n+    _buffer_offset_to_source_obj_table = new BufferOffsetToSourceObjectTable(\/*size (prime)*\/36137, \/*max size*\/1 * M);\n@@ -89,1 +88,0 @@\n-    _source_objs_order = new GrowableArrayCHeap<int, mtClassShared>(10000);\n@@ -97,1 +95,0 @@\n-  _source_objs_order->append(_source_objs->length());\n@@ -191,1 +188,1 @@\n-  Klass* k = Universe::objectArrayKlassObj(); \/\/ already relocated to point to archived klass\n+  Klass* k = Universe::objectArrayKlass(); \/\/ already relocated to point to archived klass\n@@ -234,2 +231,2 @@\n-  bool has_o_ptr = HeapShared::has_oop_pointers(o);\n-  bool has_n_ptr = HeapShared::has_native_pointers(o);\n+  bool has_oop_ptr, has_native_ptr;\n+  HeapShared::get_pointer_info(o, has_oop_ptr, has_native_ptr);\n@@ -237,2 +234,2 @@\n-  if (!has_o_ptr) {\n-    if (!has_n_ptr) {\n+  if (!has_oop_ptr) {\n+    if (!has_native_ptr) {\n@@ -244,1 +241,1 @@\n-    if (has_n_ptr) {\n+    if (has_native_ptr) {\n@@ -257,6 +254,3 @@\n-int ArchiveHeapWriter::compare_objs_by_oop_fields(int* a, int* b) {\n-  oop oa = _source_objs->at(*a);\n-  oop ob = _source_objs->at(*b);\n-\n-  int rank_a = oop_sorting_rank(oa);\n-  int rank_b = oop_sorting_rank(ob);\n+int ArchiveHeapWriter::compare_objs_by_oop_fields(HeapObjOrder* a, HeapObjOrder* b) {\n+  int rank_a = a->_rank;\n+  int rank_b = b->_rank;\n@@ -268,1 +262,1 @@\n-    return *a - *b;\n+    return a->_index - b->_index;\n@@ -273,0 +267,11 @@\n+  log_info(cds)(\"sorting heap objects\");\n+  int len = _source_objs->length();\n+  _source_objs_order = new GrowableArrayCHeap<HeapObjOrder, mtClassShared>(len);\n+\n+  for (int i = 0; i < len; i++) {\n+    oop o = _source_objs->at(i);\n+    int rank = oop_sorting_rank(o);\n+    HeapObjOrder os = {i, rank};\n+    _source_objs_order->append(os);\n+  }\n+  log_info(cds)(\"computed ranks\");\n@@ -274,0 +279,1 @@\n+  log_info(cds)(\"sorting heap objects done\");\n@@ -279,1 +285,1 @@\n-    int src_obj_index = _source_objs_order->at(i);\n+    int src_obj_index = _source_objs_order->at(i)._index;\n@@ -286,1 +292,2 @@\n-    _buffer_offset_to_source_obj_table->put(buffer_offset, src_obj);\n+    _buffer_offset_to_source_obj_table->put_when_absent(buffer_offset, src_obj);\n+    _buffer_offset_to_source_obj_table->maybe_grow();\n@@ -318,1 +325,1 @@\n-  Klass* oak = Universe::objectArrayKlassObj(); \/\/ already relocated to point to archived klass\n+  Klass* oak = Universe::objectArrayKlass(); \/\/ already relocated to point to archived klass\n@@ -583,1 +590,1 @@\n-    int src_obj_index = _source_objs_order->at(i);\n+    int src_obj_index = _source_objs_order->at(i)._index;\n@@ -597,1 +604,1 @@\n-  update_header_for_requested_obj(requested_roots, nullptr, Universe::objectArrayKlassObj());\n+  update_header_for_requested_obj(requested_roots, nullptr, Universe::objectArrayKlass());\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":30,"deletions":23,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -2302,2 +2302,2 @@\n-    \/\/ Populate the archive regions' G1BlockOffsetTableParts. That ensures\n-    \/\/ fast G1BlockOffsetTablePart::block_start operations for any given address\n+    \/\/ Populate the archive regions' G1BlockOffsetTables. That ensures\n+    \/\/ fast G1BlockOffsetTable::block_start operations for any given address\n@@ -2306,1 +2306,1 @@\n-    G1CollectedHeap::heap()->populate_archive_regions_bot_part(_mapped_heap_memregion);\n+    G1CollectedHeap::heap()->populate_archive_regions_bot(_mapped_heap_memregion);\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -290,1 +290,2 @@\n-    archived_object_cache()->put(obj, info);\n+    archived_object_cache()->put_when_absent(obj, info);\n+    archived_object_cache()->maybe_grow();\n@@ -448,1 +449,1 @@\n-bool HeapShared::has_oop_pointers(oop src_obj) {\n+void HeapShared::get_pointer_info(oop src_obj, bool& has_oop_pointers, bool& has_native_pointers) {\n@@ -451,7 +452,2 @@\n-  return info->has_oop_pointers();\n-}\n-\n-bool HeapShared::has_native_pointers(oop src_obj) {\n-  CachedOopInfo* info = archived_object_cache()->get(src_obj);\n-  assert(info != nullptr, \"must be\");\n-  return info->has_native_pointers();\n+  has_oop_pointers = info->has_oop_pointers();\n+  has_native_pointers = info->has_native_pointers();\n@@ -696,1 +692,1 @@\n-    if (buffered_k == Universe::objectArrayKlassObj()) {\n+    if (buffered_k == Universe::objectArrayKlass()) {\n@@ -1429,1 +1425,2 @@\n-  _seen_objects_table->put(obj, true);\n+  _seen_objects_table->put_when_absent(obj, true);\n+  _seen_objects_table->maybe_grow();\n@@ -1635,1 +1632,1 @@\n-    _dumped_interned_strings = new (mtClass)DumpedInternedStrings();\n+    _dumped_interned_strings = new (mtClass)DumpedInternedStrings(INITIAL_TABLE_SIZE, MAX_TABLE_SIZE);\n@@ -1708,0 +1705,3 @@\n+  if (created) {\n+    _dumped_interned_strings->maybe_grow();\n+  }\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1151,1 +1151,1 @@\n-        CompiledMethod* old = method->code();\n+        nmethod* old = method->code();\n@@ -1169,1 +1169,1 @@\n-        MutexLocker ml(CompiledMethod_lock, Mutex::_no_safepoint_check_flag);\n+        MutexLocker ml(NMethodState_lock, Mutex::_no_safepoint_check_flag);\n@@ -1181,1 +1181,1 @@\n-        MutexLocker ml(CompiledMethod_lock, Mutex::_no_safepoint_check_flag);\n+        MutexLocker ml(NMethodState_lock, Mutex::_no_safepoint_check_flag);\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1190,1 +1190,1 @@\n-      CompiledMethod* code = get_Method()->code();\n+      nmethod* code = get_Method()->code();\n@@ -1206,1 +1206,1 @@\n-    CompiledMethod* code = get_Method()->code();\n+    nmethod* code = get_Method()->code();\n","filename":"src\/hotspot\/share\/ci\/ciMethod.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -186,8 +186,8 @@\n-  get_metadata(Universe::boolArrayKlassObj());\n-  get_metadata(Universe::charArrayKlassObj());\n-  get_metadata(Universe::floatArrayKlassObj());\n-  get_metadata(Universe::doubleArrayKlassObj());\n-  get_metadata(Universe::byteArrayKlassObj());\n-  get_metadata(Universe::shortArrayKlassObj());\n-  get_metadata(Universe::intArrayKlassObj());\n-  get_metadata(Universe::longArrayKlassObj());\n+  get_metadata(Universe::boolArrayKlass());\n+  get_metadata(Universe::charArrayKlass());\n+  get_metadata(Universe::floatArrayKlass());\n+  get_metadata(Universe::doubleArrayKlass());\n+  get_metadata(Universe::byteArrayKlass());\n+  get_metadata(Universe::shortArrayKlass());\n+  get_metadata(Universe::intArrayKlass());\n+  get_metadata(Universe::longArrayKlass());\n","filename":"src\/hotspot\/share\/ci\/ciObjectFactory.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -808,1 +808,1 @@\n-    CompiledMethod* nm = (entry_bci != InvocationEntryBci) ? method->lookup_osr_nmethod_for(entry_bci, comp_level, true) : method->code();\n+    nmethod* nm = (entry_bci != InvocationEntryBci) ? method->lookup_osr_nmethod_for(entry_bci, comp_level, true) : method->code();\n","filename":"src\/hotspot\/share\/ci\/ciReplay.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1275,1 +1275,1 @@\n-    Klass* aklass = Universe::typeArrayKlassObj(type);\n+    Klass* aklass = Universe::typeArrayKlass(type);\n@@ -2439,1 +2439,1 @@\n-      CompiledMethod* nm = method->code();\n+      nmethod* nm = method->code();\n@@ -2565,1 +2565,1 @@\n-  CompiledMethod* nm = nullptr;\n+  nmethod* nm = nullptr;\n@@ -2609,1 +2609,1 @@\n-        if (cb == nullptr || !cb->is_compiled()) {\n+        if (cb == nullptr || !cb->is_nmethod()) {\n@@ -2612,1 +2612,1 @@\n-        nm = cb->as_compiled_method();\n+        nm = cb->as_nmethod();\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -373,2 +373,2 @@\n-    k = Universe::typeArrayKlassObj(t);\n-    k = TypeArrayKlass::cast(k)->array_klass(ndims, CHECK_NULL);\n+    k = Universe::typeArrayKlass(t);\n+    k = k->array_klass(ndims, CHECK_NULL);\n@@ -788,1 +788,1 @@\n-      k = Universe::typeArrayKlassObj(t);\n+      k = Universe::typeArrayKlass(t);\n@@ -1765,1 +1765,1 @@\n-      klass = Universe::typeArrayKlassObj(t);\n+      klass = Universe::typeArrayKlass(t);\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -57,3 +57,0 @@\n-const char* CodeBlob::compiler_name() const {\n-  return compilertype2name(_type);\n-}\n@@ -67,1 +64,0 @@\n-\n@@ -80,7 +76,17 @@\n-CodeBlob::CodeBlob(const char* name, CompilerType type, const CodeBlobLayout& layout, int frame_complete_offset, int frame_size, ImmutableOopMapSet* oop_maps, bool caller_must_gc_arguments, bool compiled) :\n-  _code_begin(layout.code_begin()),\n-  _code_end(layout.code_end()),\n-  _content_begin(layout.content_begin()),\n-  _data_end(layout.data_end()),\n-  _relocation_begin(layout.relocation_begin()),\n-  _relocation_end(layout.relocation_end()),\n+#ifdef ASSERT\n+void CodeBlob::verify_parameters() {\n+  assert(is_aligned(_size,            oopSize), \"unaligned size\");\n+  assert(is_aligned(_header_size,     oopSize), \"unaligned size\");\n+  assert(is_aligned(_relocation_size, oopSize), \"unaligned size\");\n+  assert(_data_offset <= size(), \"codeBlob is too small\");\n+  assert(code_end() == content_end(), \"must be the same - see code_end()\");\n+#ifdef COMPILER1\n+  \/\/ probably wrong for tiered\n+  assert(frame_size() >= -1, \"must use frame size or -1 for runtime stubs\");\n+#endif \/\/ COMPILER1\n+}\n+#endif\n+\n+CodeBlob::CodeBlob(const char* name, CodeBlobKind kind, int size, int header_size, int relocation_size,\n+                   int content_offset, int code_offset, int frame_complete_offset, int data_offset,\n+                   int frame_size, ImmutableOopMapSet* oop_maps, bool caller_must_gc_arguments) :\n@@ -89,2 +95,5 @@\n-  _size(layout.size()),\n-  _header_size(layout.header_size()),\n+  _size(size),\n+  _header_size(header_size),\n+  _relocation_size(relocation_size),\n+  _content_offset(content_offset),\n+  _code_offset(code_offset),\n@@ -92,1 +101,1 @@\n-  _data_offset(layout.data_offset()),\n+  _data_offset(data_offset),\n@@ -94,3 +103,3 @@\n-  _caller_must_gc_arguments(caller_must_gc_arguments),\n-  _is_compiled(compiled),\n-  _type(type)\n+  S390_ONLY(_ctable_offset(0) COMMA)\n+  _kind(kind),\n+  _caller_must_gc_arguments(caller_must_gc_arguments)\n@@ -98,9 +107,1 @@\n-  assert(is_aligned(layout.size(),            oopSize), \"unaligned size\");\n-  assert(is_aligned(layout.header_size(),     oopSize), \"unaligned size\");\n-  assert(is_aligned(layout.relocation_size(), oopSize), \"unaligned size\");\n-  assert(layout.code_end() == layout.content_end(), \"must be the same - see code_end()\");\n-#ifdef COMPILER1\n-  \/\/ probably wrong for tiered\n-  assert(_frame_size >= -1, \"must use frame size or -1 for runtime stubs\");\n-#endif \/\/ COMPILER1\n-  S390_ONLY(_ctable_offset = 0;) \/\/ avoid uninitialized fields\n+  DEBUG_ONLY( verify_parameters(); )\n@@ -109,7 +110,3 @@\n-CodeBlob::CodeBlob(const char* name, CompilerType type, const CodeBlobLayout& layout, CodeBuffer* cb \/*UNUSED*\/, int frame_complete_offset, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments, bool compiled) :\n-  _code_begin(layout.code_begin()),\n-  _code_end(layout.code_end()),\n-  _content_begin(layout.content_begin()),\n-  _data_end(layout.data_end()),\n-  _relocation_begin(layout.relocation_begin()),\n-  _relocation_end(layout.relocation_end()),\n+CodeBlob::CodeBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int header_size,\n+                   int frame_complete_offset, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) :\n+  _oop_maps(nullptr), \/\/ will be set by set_oop_maps() call\n@@ -117,2 +114,5 @@\n-  _size(layout.size()),\n-  _header_size(layout.header_size()),\n+  _size(size),\n+  _header_size(header_size),\n+  _relocation_size(align_up(cb->total_relocation_size(), oopSize)),\n+  _content_offset(CodeBlob::align_code_offset(_header_size + _relocation_size)),\n+  _code_offset(_content_offset + cb->total_offset_of(cb->insts())),\n@@ -120,1 +120,1 @@\n-  _data_offset(layout.data_offset()),\n+  _data_offset(_content_offset + align_up(cb->total_content_size(), oopSize)),\n@@ -122,3 +122,3 @@\n-  _caller_must_gc_arguments(caller_must_gc_arguments),\n-  _is_compiled(compiled),\n-  _type(type)\n+  S390_ONLY(_ctable_offset(0) COMMA)\n+  _kind(kind),\n+  _caller_must_gc_arguments(caller_must_gc_arguments)\n@@ -126,4 +126,1 @@\n-  assert(is_aligned(_size,        oopSize), \"unaligned size\");\n-  assert(is_aligned(_header_size, oopSize), \"unaligned size\");\n-  assert(_data_offset <= _size, \"codeBlob is too small\");\n-  assert(layout.code_end() == layout.content_end(), \"must be the same - see code_end()\");\n+  DEBUG_ONLY( verify_parameters(); )\n@@ -132,5 +129,0 @@\n-#ifdef COMPILER1\n-  \/\/ probably wrong for tiered\n-  assert(_frame_size >= -1, \"must use frame size or -1 for runtime stubs\");\n-#endif \/\/ COMPILER1\n-  S390_ONLY(_ctable_offset = 0;) \/\/ avoid uninitialized fields\n@@ -139,4 +131,15 @@\n-\n-\/\/ Creates a simple CodeBlob. Sets up the size of the different regions.\n-RuntimeBlob::RuntimeBlob(const char* name, int header_size, int size, int frame_complete, int locs_size)\n-  : CodeBlob(name, compiler_none, CodeBlobLayout((address) this, size, header_size, locs_size, size), frame_complete, 0, nullptr, false \/* caller_must_gc_arguments *\/)\n+\/\/ Simple CodeBlob used for simple BufferBlob.\n+CodeBlob::CodeBlob(const char* name, CodeBlobKind kind, int size, int header_size) :\n+  _oop_maps(nullptr),\n+  _name(name),\n+  _size(size),\n+  _header_size(header_size),\n+  _relocation_size(0),\n+  _content_offset(CodeBlob::align_code_offset(header_size)),\n+  _code_offset(_content_offset),\n+  _frame_complete_offset(CodeOffsets::frame_never_safe),\n+  _data_offset(size),\n+  _frame_size(0),\n+  S390_ONLY(_ctable_offset(0) COMMA)\n+  _kind(kind),\n+  _caller_must_gc_arguments(false)\n@@ -144,1 +147,2 @@\n-  assert(is_aligned(locs_size, oopSize), \"unaligned size\");\n+  assert(is_aligned(size,            oopSize), \"unaligned size\");\n+  assert(is_aligned(header_size,     oopSize), \"unaligned size\");\n@@ -147,0 +151,31 @@\n+void CodeBlob::purge(bool free_code_cache_data, bool unregister_nmethod) {\n+  if (_oop_maps != nullptr) {\n+    delete _oop_maps;\n+    _oop_maps = nullptr;\n+  }\n+  NOT_PRODUCT(_asm_remarks.clear());\n+  NOT_PRODUCT(_dbg_strings.clear());\n+}\n+\n+void CodeBlob::set_oop_maps(OopMapSet* p) {\n+  \/\/ Danger Will Robinson! This method allocates a big\n+  \/\/ chunk of memory, its your job to free it.\n+  if (p != nullptr) {\n+    _oop_maps = ImmutableOopMapSet::build_from(p);\n+  } else {\n+    _oop_maps = nullptr;\n+  }\n+}\n+\n+const ImmutableOopMap* CodeBlob::oop_map_for_return_address(address return_address) const {\n+  assert(_oop_maps != nullptr, \"nope\");\n+  return _oop_maps->find_map_at_offset((intptr_t) return_address - (intptr_t) code_begin());\n+}\n+\n+void CodeBlob::print_code_on(outputStream* st) {\n+  ResourceMark m;\n+  Disassembler::decode(this, st);\n+}\n+\n+\/\/-----------------------------------------------------------------------------------------\n+\/\/ Creates a RuntimeBlob from a CodeBuffer and copy code and relocation info.\n@@ -148,2 +183,0 @@\n-\/\/ Creates a RuntimeBlob from a CodeBuffer\n-\/\/ and copy code and relocation info.\n@@ -152,0 +185,1 @@\n+  CodeBlobKind kind,\n@@ -153,1 +187,1 @@\n-  int         header_size,\n+  int         header_size,\n@@ -158,2 +192,3 @@\n-  bool        caller_must_gc_arguments\n-) : CodeBlob(name, compiler_none, CodeBlobLayout((address) this, size, header_size, cb), cb, frame_complete, frame_size, oop_maps, caller_must_gc_arguments) {\n+  bool        caller_must_gc_arguments)\n+  : CodeBlob(name, kind, cb, size, header_size, frame_complete, frame_size, oop_maps, caller_must_gc_arguments)\n+{\n@@ -175,19 +210,0 @@\n-void CodeBlob::purge(bool free_code_cache_data, bool unregister_nmethod) {\n-  if (_oop_maps != nullptr) {\n-    delete _oop_maps;\n-    _oop_maps = nullptr;\n-  }\n-  NOT_PRODUCT(_asm_remarks.clear());\n-  NOT_PRODUCT(_dbg_strings.clear());\n-}\n-\n-void CodeBlob::set_oop_maps(OopMapSet* p) {\n-  \/\/ Danger Will Robinson! This method allocates a big\n-  \/\/ chunk of memory, its your job to free it.\n-  if (p != nullptr) {\n-    _oop_maps = ImmutableOopMapSet::build_from(p);\n-  } else {\n-    _oop_maps = nullptr;\n-  }\n-}\n-\n@@ -233,10 +249,0 @@\n-const ImmutableOopMap* CodeBlob::oop_map_for_return_address(address return_address) const {\n-  assert(_oop_maps != nullptr, \"nope\");\n-  return _oop_maps->find_map_at_offset((intptr_t) return_address - (intptr_t) code_begin());\n-}\n-\n-void CodeBlob::print_code_on(outputStream* st) {\n-  ResourceMark m;\n-  Disassembler::decode(this, st);\n-}\n-\n@@ -246,3 +252,2 @@\n-\n-BufferBlob::BufferBlob(const char* name, int size)\n-: RuntimeBlob(name, sizeof(BufferBlob), size, CodeOffsets::frame_never_safe, \/*locs_size:*\/ 0)\n+BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, int size)\n+: RuntimeBlob(name, kind, size, sizeof(BufferBlob))\n@@ -262,1 +267,1 @@\n-    blob = new (size) BufferBlob(name, size);\n+    blob = new (size) BufferBlob(name, CodeBlobKind::Buffer, size);\n@@ -271,2 +276,2 @@\n-BufferBlob::BufferBlob(const char* name, int header_size, int size, CodeBuffer* cb)\n-  : RuntimeBlob(name, cb, header_size, size, CodeOffsets::frame_never_safe, 0, nullptr)\n+BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int header_size)\n+  : RuntimeBlob(name, kind, cb, size, header_size, CodeOffsets::frame_never_safe, 0, nullptr)\n@@ -275,0 +280,1 @@\n+\/\/ Used by gtest\n@@ -283,1 +289,1 @@\n-    blob = new (size) BufferBlob(name, sizeof(BufferBlob), size, cb);\n+    blob = new (size) BufferBlob(name, CodeBlobKind::Buffer, cb, size, sizeof(BufferBlob));\n@@ -299,2 +305,2 @@\n-BufferBlob::BufferBlob(const char* name, int size, CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments)\n-  : RuntimeBlob(name, cb, sizeof(BufferBlob), size, frame_complete, frame_size, oop_maps, caller_must_gc_arguments)\n+BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments)\n+  : RuntimeBlob(name, kind, cb, size, sizeof(BufferBlob), frame_complete, frame_size, oop_maps, caller_must_gc_arguments)\n@@ -308,1 +314,1 @@\n-  BufferBlob(\"I2C\/C2I adapters\", size, cb, frame_complete, frame_size, oop_maps, caller_must_gc_arguments) {\n+  BufferBlob(\"I2C\/C2I adapters\", CodeBlobKind::Adapter, cb, size, frame_complete, frame_size, oop_maps, caller_must_gc_arguments) {\n@@ -329,0 +335,3 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Implementation of VtableBlob\n+\n@@ -340,1 +349,1 @@\n-  BufferBlob(name, size) {\n+  BufferBlob(name, CodeBlobKind::Vtable, size) {\n@@ -402,1 +411,1 @@\n-  BufferBlob(\"buffered inline type\", sizeof(BufferedInlineTypeBlob), size, cb),\n+  BufferBlob(\"buffered inline type\", CodeBlobKind::BufferedInlineType, cb, size, sizeof(BufferedInlineTypeBlob)),\n@@ -436,1 +445,2 @@\n-: RuntimeBlob(name, cb, sizeof(RuntimeStub), size, frame_complete, frame_size, oop_maps, caller_must_gc_arguments)\n+: RuntimeBlob(name, CodeBlobKind::Runtime_Stub, cb, size, sizeof(RuntimeStub),\n+              frame_complete, frame_size, oop_maps, caller_must_gc_arguments)\n@@ -492,1 +502,2 @@\n-: SingletonBlob(\"DeoptimizationBlob\", cb, sizeof(DeoptimizationBlob), size, frame_size, oop_maps)\n+: SingletonBlob(\"DeoptimizationBlob\", CodeBlobKind::Deoptimization, cb,\n+                size, sizeof(DeoptimizationBlob), frame_size, oop_maps)\n@@ -541,1 +552,2 @@\n-: SingletonBlob(\"UncommonTrapBlob\", cb, sizeof(UncommonTrapBlob), size, frame_size, oop_maps)\n+: SingletonBlob(\"UncommonTrapBlob\", CodeBlobKind::Uncommon_Trap, cb,\n+                size, sizeof(UncommonTrapBlob), frame_size, oop_maps)\n@@ -577,1 +589,2 @@\n-: SingletonBlob(\"ExceptionBlob\", cb, sizeof(ExceptionBlob), size, frame_size, oop_maps)\n+: SingletonBlob(\"ExceptionBlob\", CodeBlobKind::Exception, cb,\n+                size, sizeof(ExceptionBlob), frame_size, oop_maps)\n@@ -612,1 +625,2 @@\n-: SingletonBlob(\"SafepointBlob\", cb, sizeof(SafepointBlob), size, frame_size, oop_maps)\n+: SingletonBlob(\"SafepointBlob\", CodeBlobKind::Safepoint, cb,\n+                size, sizeof(SafepointBlob), frame_size, oop_maps)\n@@ -634,0 +648,55 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Implementation of UpcallStub\n+\n+UpcallStub::UpcallStub(const char* name, CodeBuffer* cb, int size, jobject receiver, ByteSize frame_data_offset) :\n+  RuntimeBlob(name, CodeBlobKind::Upcall, cb, size, sizeof(UpcallStub),\n+              CodeOffsets::frame_never_safe, 0 \/* no frame size *\/,\n+              \/* oop maps = *\/ nullptr, \/* caller must gc arguments = *\/ false),\n+  _receiver(receiver),\n+  _frame_data_offset(frame_data_offset)\n+{\n+  CodeCache::commit(this);\n+}\n+\n+void* UpcallStub::operator new(size_t s, unsigned size) throw() {\n+  return CodeCache::allocate(size, CodeBlobType::NonNMethod);\n+}\n+\n+UpcallStub* UpcallStub::create(const char* name, CodeBuffer* cb, jobject receiver, ByteSize frame_data_offset) {\n+  ThreadInVMfromUnknown __tiv;  \/\/ get to VM state in case we block on CodeCache_lock\n+\n+  UpcallStub* blob = nullptr;\n+  unsigned int size = CodeBlob::allocation_size(cb, sizeof(UpcallStub));\n+  {\n+    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+    blob = new (size) UpcallStub(name, cb, size, receiver, frame_data_offset);\n+  }\n+  if (blob == nullptr) {\n+    return nullptr; \/\/ caller must handle this\n+  }\n+\n+  \/\/ Track memory usage statistic after releasing CodeCache_lock\n+  MemoryService::track_code_cache_memory_usage();\n+\n+  trace_new_stub(blob, \"UpcallStub\");\n+\n+  return blob;\n+}\n+\n+void UpcallStub::oops_do(OopClosure* f, const frame& frame) {\n+  frame_data_for_frame(frame)->old_handles->oops_do(f);\n+}\n+\n+JavaFrameAnchor* UpcallStub::jfa_for_frame(const frame& frame) const {\n+  return &frame_data_for_frame(frame)->jfa;\n+}\n+\n+void UpcallStub::free(UpcallStub* blob) {\n+  assert(blob != nullptr, \"caller must check for nullptr\");\n+  JNIHandles::destroy_global(blob->receiver());\n+  RuntimeBlob::free(blob);\n+}\n+\n+void UpcallStub::preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f) {\n+  ShouldNotReachHere(); \/\/ caller should never have to gc arguments\n+}\n@@ -710,4 +779,0 @@\n-void RuntimeBlob::verify() {\n-  ShouldNotReachHere();\n-}\n-\n@@ -762,54 +827,0 @@\n-\/\/ Implementation of UpcallStub\n-\n-UpcallStub::UpcallStub(const char* name, CodeBuffer* cb, int size, jobject receiver, ByteSize frame_data_offset) :\n-  RuntimeBlob(name, cb, sizeof(UpcallStub), size, CodeOffsets::frame_never_safe, 0 \/* no frame size *\/,\n-              \/* oop maps = *\/ nullptr, \/* caller must gc arguments = *\/ false),\n-  _receiver(receiver),\n-  _frame_data_offset(frame_data_offset) {\n-  CodeCache::commit(this);\n-}\n-\n-void* UpcallStub::operator new(size_t s, unsigned size) throw() {\n-  return CodeCache::allocate(size, CodeBlobType::NonNMethod);\n-}\n-\n-UpcallStub* UpcallStub::create(const char* name, CodeBuffer* cb, jobject receiver, ByteSize frame_data_offset) {\n-  ThreadInVMfromUnknown __tiv;  \/\/ get to VM state in case we block on CodeCache_lock\n-\n-  UpcallStub* blob = nullptr;\n-  unsigned int size = CodeBlob::allocation_size(cb, sizeof(UpcallStub));\n-  {\n-    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-    blob = new (size) UpcallStub(name, cb, size, receiver, frame_data_offset);\n-  }\n-  if (blob == nullptr) {\n-    return nullptr; \/\/ caller must handle this\n-  }\n-\n-  \/\/ Track memory usage statistic after releasing CodeCache_lock\n-  MemoryService::track_code_cache_memory_usage();\n-\n-  trace_new_stub(blob, \"UpcallStub\");\n-\n-  return blob;\n-}\n-\n-void UpcallStub::oops_do(OopClosure* f, const frame& frame) {\n-  frame_data_for_frame(frame)->old_handles->oops_do(f);\n-}\n-\n-JavaFrameAnchor* UpcallStub::jfa_for_frame(const frame& frame) const {\n-  return &frame_data_for_frame(frame)->jfa;\n-}\n-\n-void UpcallStub::free(UpcallStub* blob) {\n-  assert(blob != nullptr, \"caller must check for nullptr\");\n-  JNIHandles::destroy_global(blob->receiver());\n-  RuntimeBlob::free(blob);\n-}\n-\n-void UpcallStub::preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f) {\n-  ShouldNotReachHere(); \/\/ caller should never have to gc arguments\n-}\n-\n-\/\/ Misc.\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":173,"deletions":162,"binary":false,"changes":335,"status":"modified"},{"patch":"@@ -55,2 +55,1 @@\n-\/\/  CompiledMethod       : Compiled Java methods (include method that calls to native code)\n-\/\/   nmethod             : JIT Compiled Java methods\n+\/\/  nmethod              : JIT Compiled Java methods\n@@ -62,0 +61,1 @@\n+\/\/    BufferedInlineTypeBlob   : used for pack\/unpack handlers\n@@ -78,0 +78,16 @@\n+enum class CodeBlobKind : u1 {\n+  None,\n+  Nmethod,\n+  Buffer,\n+  Adapter,\n+  Vtable,\n+  MH_Adapter,\n+  BufferedInlineType,\n+  Runtime_Stub,\n+  Deoptimization,\n+  Exception,\n+  Safepoint,\n+  Uncommon_Trap,\n+  Upcall,\n+  Number_Of_Kinds\n+};\n@@ -79,1 +95,0 @@\n-class CodeBlobLayout;\n@@ -90,11 +105,0 @@\n-\n-  address    _code_begin;\n-  address    _code_end;\n-  address    _content_begin;                     \/\/ address to where content region begins (this includes consts, insts, stubs)\n-                                                 \/\/ address    _content_end - not required, for all CodeBlobs _code_end == _content_end for now\n-  address    _data_end;\n-  address    _relocation_begin;\n-  address    _relocation_end;\n-\n-\n-  S390_ONLY(int       _ctable_offset;)\n@@ -107,0 +111,3 @@\n+  int        _relocation_size;                   \/\/ size of relocation\n+  int        _content_offset;                    \/\/ offset to where content region begins (this includes consts, insts, stubs)\n+  int        _code_offset;                       \/\/ offset to where instructions region begins (this includes insts, stubs)\n@@ -114,1 +121,3 @@\n-  bool                _caller_must_gc_arguments;\n+  S390_ONLY(int       _ctable_offset;)\n+\n+  CodeBlobKind        _kind;                     \/\/ Kind of this code blob\n@@ -116,2 +125,1 @@\n-  bool                _is_compiled;\n-  const CompilerType  _type;                     \/\/ CompilerType\n+  bool                _caller_must_gc_arguments;\n@@ -124,6 +132,11 @@\n-  CodeBlob(const char* name, CompilerType type, const CodeBlobLayout& layout, int frame_complete_offset,\n-           int frame_size, ImmutableOopMapSet* oop_maps,\n-           bool caller_must_gc_arguments, bool compiled = false);\n-  CodeBlob(const char* name, CompilerType type, const CodeBlobLayout& layout, CodeBuffer* cb, int frame_complete_offset,\n-           int frame_size, OopMapSet* oop_maps,\n-           bool caller_must_gc_arguments, bool compiled = false);\n+  DEBUG_ONLY( void verify_parameters() );\n+\n+  CodeBlob(const char* name, CodeBlobKind kind, int size, int header_size, int relocation_size,\n+           int content_offset, int code_offset, int data_offset, int frame_complete_offset,\n+           int frame_size, ImmutableOopMapSet* oop_maps, bool caller_must_gc_arguments);\n+\n+  CodeBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int header_size,\n+           int frame_complete_offset, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments);\n+\n+  \/\/ Simple CodeBlob used for simple BufferBlob.\n+  CodeBlob(const char* name, CodeBlobKind kind, int size, int header_size);\n@@ -134,2 +147,0 @@\n-  \/\/ Only used by unit test.\n-  CodeBlob() : _type(compiler_none) {}\n@@ -149,20 +160,12 @@\n-  virtual bool is_buffer_blob() const                 { return false; }\n-  virtual bool is_nmethod() const                     { return false; }\n-  virtual bool is_runtime_stub() const                { return false; }\n-  virtual bool is_deoptimization_stub() const         { return false; }\n-  virtual bool is_uncommon_trap_stub() const          { return false; }\n-  virtual bool is_exception_stub() const              { return false; }\n-  virtual bool is_safepoint_stub() const              { return false; }\n-  virtual bool is_adapter_blob() const                { return false; }\n-  virtual bool is_vtable_blob() const                 { return false; }\n-  virtual bool is_method_handles_adapter_blob() const { return false; }\n-  virtual bool is_upcall_stub() const                 { return false; }\n-  bool is_compiled() const                            { return _is_compiled; }\n-  const bool* is_compiled_addr() const                { return &_is_compiled; }\n-  virtual bool is_buffered_inline_type_blob() const   { return false; }\n-\n-  inline bool is_compiled_by_c1() const    { return _type == compiler_c1; };\n-  inline bool is_compiled_by_c2() const    { return _type == compiler_c2; };\n-  inline bool is_compiled_by_jvmci() const { return _type == compiler_jvmci; };\n-  const char* compiler_name() const;\n-  CompilerType compiler_type() const { return _type; }\n+  bool is_nmethod() const                     { return _kind == CodeBlobKind::Nmethod; }\n+  bool is_buffer_blob() const                 { return _kind == CodeBlobKind::Buffer; }\n+  bool is_runtime_stub() const                { return _kind == CodeBlobKind::Runtime_Stub; }\n+  bool is_deoptimization_stub() const         { return _kind == CodeBlobKind::Deoptimization; }\n+  bool is_uncommon_trap_stub() const          { return _kind == CodeBlobKind::Uncommon_Trap; }\n+  bool is_exception_stub() const              { return _kind == CodeBlobKind::Exception; }\n+  bool is_safepoint_stub() const              { return _kind == CodeBlobKind::Safepoint; }\n+  bool is_adapter_blob() const                { return _kind == CodeBlobKind::Adapter; }\n+  bool is_vtable_blob() const                 { return _kind == CodeBlobKind::Vtable; }\n+  bool is_method_handles_adapter_blob() const { return _kind == CodeBlobKind::MH_Adapter; }\n+  bool is_buffered_inline_type_blob() const   { return _kind == CodeBlobKind::BufferedInlineType; }\n+  bool is_upcall_stub() const                 { return _kind == CodeBlobKind::Upcall; }\n@@ -171,7 +174,5 @@\n-  nmethod* as_nmethod_or_null()                { return is_nmethod() ? (nmethod*) this : nullptr; }\n-  nmethod* as_nmethod()                        { assert(is_nmethod(), \"must be nmethod\"); return (nmethod*) this; }\n-  CompiledMethod* as_compiled_method_or_null() { return is_compiled() ? (CompiledMethod*) this : nullptr; }\n-  CompiledMethod* as_compiled_method()         { assert(is_compiled(), \"must be compiled\"); return (CompiledMethod*) this; }\n-  CodeBlob* as_codeblob_or_null() const        { return (CodeBlob*) this; }\n-  UpcallStub* as_upcall_stub() const           { assert(is_upcall_stub(), \"must be upcall stub\"); return (UpcallStub*) this; }\n-  RuntimeStub* as_runtime_stub() const         { assert(is_runtime_stub(), \"must be runtime blob\"); return (RuntimeStub*) this; }\n+  nmethod* as_nmethod_or_null()               { return is_nmethod() ? (nmethod*) this : nullptr; }\n+  nmethod* as_nmethod()                       { assert(is_nmethod(), \"must be nmethod\"); return (nmethod*) this; }\n+  CodeBlob* as_codeblob_or_null() const       { return (CodeBlob*) this; }\n+  UpcallStub* as_upcall_stub() const          { assert(is_upcall_stub(), \"must be upcall stub\"); return (UpcallStub*) this; }\n+  RuntimeStub* as_runtime_stub() const        { assert(is_runtime_stub(), \"must be runtime blob\"); return (RuntimeStub*) this; }\n@@ -180,8 +181,16 @@\n-  address header_begin() const        { return (address) this; }\n-  relocInfo* relocation_begin() const { return (relocInfo*) _relocation_begin; };\n-  relocInfo* relocation_end() const   { return (relocInfo*) _relocation_end; }\n-  address content_begin() const       { return _content_begin; }\n-  address content_end() const         { return _code_end; } \/\/ _code_end == _content_end is true for all types of blobs for now, it is also checked in the constructor\n-  address code_begin() const          { return _code_begin;    }\n-  address code_end() const            { return _code_end; }\n-  address data_end() const            { return _data_end;      }\n+  address    header_begin() const             { return (address)    this; }\n+  address    header_end() const               { return ((address)   this) + _header_size; }\n+  relocInfo* relocation_begin() const         { return (relocInfo*) header_end(); }\n+  relocInfo* relocation_end() const           { return (relocInfo*)(header_end()   + _relocation_size); }\n+  address    content_begin() const            { return (address)    header_begin() + _content_offset; }\n+  address    content_end() const              { return (address)    header_begin() + _data_offset; }\n+  address    code_begin() const               { return (address)    header_begin() + _code_offset; }\n+  \/\/ code_end == content_end is true for all types of blobs for now, it is also checked in the constructor\n+  address    code_end() const                 { return (address)    header_begin() + _data_offset; }\n+  address    data_begin() const               { return (address)    header_begin() + _data_offset; }\n+  address    data_end() const                 { return (address)    header_begin() + _size; }\n+\n+  \/\/ Offsets\n+  int content_offset() const                  { return _content_offset; }\n+  int code_offset() const                     { return _code_offset; }\n+  int data_offset() const                     { return _data_offset; }\n@@ -196,5 +205,6 @@\n-  int size() const                               { return _size; }\n-  int header_size() const                        { return _header_size; }\n-  int relocation_size() const                    { return pointer_delta_as_int((address) relocation_end(), (address) relocation_begin()); }\n-  int content_size() const                       { return pointer_delta_as_int(content_end(), content_begin()); }\n-  int code_size() const                          { return pointer_delta_as_int(code_end(), code_begin()); }\n+  int size() const               { return _size; }\n+  int header_size() const        { return _header_size; }\n+  int relocation_size() const    { return pointer_delta_as_int((address) relocation_end(), (address) relocation_begin()); }\n+  int content_size() const       { return pointer_delta_as_int(content_end(), content_begin()); }\n+  int code_size() const          { return pointer_delta_as_int(code_end(), code_begin()); }\n+\n@@ -205,2 +215,0 @@\n-    _code_end = (address)this + used;\n-    _data_end = (address)this + used;\n@@ -217,2 +225,0 @@\n-  virtual bool is_not_entrant() const            { return false; }\n-\n@@ -264,91 +270,2 @@\n-class CodeBlobLayout : public StackObj {\n-private:\n-  int _size;\n-  int _header_size;\n-  int _relocation_size;\n-  int _content_offset;\n-  int _code_offset;\n-  int _data_offset;\n-  address _code_begin;\n-  address _code_end;\n-  address _content_begin;\n-  address _content_end;\n-  address _data_end;\n-  address _relocation_begin;\n-  address _relocation_end;\n-\n-public:\n-  CodeBlobLayout(address code_begin, address code_end, address content_begin, address content_end, address data_end, address relocation_begin, address relocation_end) :\n-    _size(0),\n-    _header_size(0),\n-    _relocation_size(0),\n-    _content_offset(0),\n-    _code_offset(0),\n-    _data_offset(0),\n-    _code_begin(code_begin),\n-    _code_end(code_end),\n-    _content_begin(content_begin),\n-    _content_end(content_end),\n-    _data_end(data_end),\n-    _relocation_begin(relocation_begin),\n-    _relocation_end(relocation_end)\n-  {\n-  }\n-\n-  CodeBlobLayout(const address start, int size, int header_size, int relocation_size, int data_offset) :\n-    _size(size),\n-    _header_size(header_size),\n-    _relocation_size(relocation_size),\n-    _content_offset(CodeBlob::align_code_offset(_header_size + _relocation_size)),\n-    _code_offset(_content_offset),\n-    _data_offset(data_offset)\n-  {\n-    assert(is_aligned(_relocation_size, oopSize), \"unaligned size\");\n-\n-    _code_begin = (address) start + _code_offset;\n-    _code_end = (address) start + _data_offset;\n-\n-    _content_begin = (address) start + _content_offset;\n-    _content_end = (address) start + _data_offset;\n-\n-    _data_end = (address) start + _size;\n-    _relocation_begin = (address) start + _header_size;\n-    _relocation_end = _relocation_begin + _relocation_size;\n-  }\n-\n-  CodeBlobLayout(const address start, int size, int header_size, const CodeBuffer* cb) :\n-    _size(size),\n-    _header_size(header_size),\n-    _relocation_size(align_up(cb->total_relocation_size(), oopSize)),\n-    _content_offset(CodeBlob::align_code_offset(_header_size + _relocation_size)),\n-    _code_offset(_content_offset + cb->total_offset_of(cb->insts())),\n-    _data_offset(_content_offset + align_up(cb->total_content_size(), oopSize))\n-  {\n-    assert(is_aligned(_relocation_size, oopSize), \"unaligned size\");\n-\n-    _code_begin = (address) start + _code_offset;\n-    _code_end = (address) start + _data_offset;\n-\n-    _content_begin = (address) start + _content_offset;\n-    _content_end = (address) start + _data_offset;\n-\n-    _data_end = (address) start + _size;\n-    _relocation_begin = (address) start + _header_size;\n-    _relocation_end = _relocation_begin + _relocation_size;\n-  }\n-\n-  int size() const { return _size; }\n-  int header_size() const { return _header_size; }\n-  int relocation_size() const { return _relocation_size; }\n-  int content_offset() const { return _content_offset; }\n-  int code_offset() const { return _code_offset; }\n-  int data_offset() const { return _data_offset; }\n-  address code_begin() const { return _code_begin; }\n-  address code_end() const { return _code_end; }\n-  address data_end() const { return _data_end; }\n-  address relocation_begin() const { return _relocation_begin; }\n-  address relocation_end() const { return _relocation_end; }\n-  address content_begin() const { return _content_begin; }\n-  address content_end() const { return _content_end; }\n-};\n-\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ RuntimeBlob: used for non-compiled method code (adapters, stubs, blobs)\n@@ -362,3 +279,3 @@\n-  \/\/ frame_complete is the offset from the beginning of the instructions\n-  \/\/ to where the frame setup (from stackwalk viewpoint) is complete.\n-  RuntimeBlob(const char* name, int header_size, int size, int frame_complete, int locs_size);\n+  RuntimeBlob(const char* name, CodeBlobKind kind, int size, int header_size)\n+    : CodeBlob(name, kind, size, header_size)\n+  {}\n@@ -367,0 +284,2 @@\n+  \/\/ frame_complete is the offset from the beginning of the instructions\n+  \/\/ to where the frame setup (from stackwalk viewpoint) is complete.\n@@ -369,0 +288,1 @@\n+    CodeBlobKind kind,\n@@ -370,1 +290,1 @@\n-    int         header_size,\n+    int         header_size,\n@@ -380,9 +300,0 @@\n-  void verify();\n-\n-  \/\/ OopMap for frame\n-  virtual void preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f)  { ShouldNotReachHere(); }\n-\n-  \/\/ Debugging\n-  virtual void print_on(outputStream* st) const { CodeBlob::print_on(st); }\n-  virtual void print_value_on(outputStream* st) const { CodeBlob::print_value_on(st); }\n-\n@@ -408,3 +319,3 @@\n-  BufferBlob(const char* name, int size);\n-  BufferBlob(const char* name, int header_size, int size, CodeBuffer* cb);\n-  BufferBlob(const char* name, int size, CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments = false);\n+  BufferBlob(const char* name, CodeBlobKind kind, int size);\n+  BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int header_size);\n+  BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments = false);\n@@ -421,4 +332,1 @@\n-  \/\/ Typing\n-  virtual bool is_buffer_blob() const            { return true; }\n-\n-  void preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f)  { \/* nothing to do *\/ }\n+  void preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f) override { \/* nothing to do *\/ }\n@@ -427,3 +335,3 @@\n-  void verify();\n-  void print_on(outputStream* st) const;\n-  void print_value_on(outputStream* st) const;\n+  void verify() override;\n+  void print_on(outputStream* st) const override;\n+  void print_value_on(outputStream* st) const override;\n@@ -448,3 +356,0 @@\n-  \/\/ Typing\n-  virtual bool is_adapter_blob() const { return true; }\n-\n@@ -464,3 +369,0 @@\n-\n-  \/\/ Typing\n-  virtual bool is_vtable_blob() const { return true; }\n@@ -474,1 +376,1 @@\n-  MethodHandlesAdapterBlob(int size): BufferBlob(\"MethodHandles adapters\", size) {}\n+  MethodHandlesAdapterBlob(int size): BufferBlob(\"MethodHandles adapters\", CodeBlobKind::MH_Adapter, size) {}\n@@ -479,3 +381,0 @@\n-\n-  \/\/ Typing\n-  virtual bool is_method_handles_adapter_blob() const { return true; }\n@@ -502,3 +401,0 @@\n-\n-  \/\/ Typing\n-  virtual bool is_buffered_inline_type_blob() const { return true; }\n@@ -540,3 +436,0 @@\n-  \/\/ Typing\n-  bool is_runtime_stub() const                   { return true; }\n-\n@@ -546,1 +439,1 @@\n-  void preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f)  { \/* nothing to do *\/ }\n+  void preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f) override { \/* nothing to do *\/ }\n@@ -548,3 +441,3 @@\n-  void verify();\n-  void print_on(outputStream* st) const;\n-  void print_value_on(outputStream* st) const;\n+  void verify() override;\n+  void print_on(outputStream* st) const override;\n+  void print_value_on(outputStream* st) const override;\n@@ -565,6 +458,7 @@\n-     const char* name,\n-     CodeBuffer* cb,\n-     int         header_size,\n-     int         size,\n-     int         frame_size,\n-     OopMapSet*  oop_maps\n+     const char*  name,\n+     CodeBlobKind kind,\n+     CodeBuffer*  cb,\n+     int          size,\n+     int          header_size,\n+     int          frame_size,\n+     OopMapSet*   oop_maps\n@@ -572,1 +466,1 @@\n-   : RuntimeBlob(name, cb, header_size, size, CodeOffsets::frame_never_safe, frame_size, oop_maps)\n+   : RuntimeBlob(name, kind, cb, size, header_size, CodeOffsets::frame_never_safe, frame_size, oop_maps)\n@@ -578,4 +472,4 @@\n-  void preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f)  { \/* nothing to do *\/ }\n-  void verify(); \/\/ does nothing\n-  void print_on(outputStream* st) const;\n-  void print_value_on(outputStream* st) const;\n+  void preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f) override { \/* nothing to do *\/ }\n+  void verify() override; \/\/ does nothing\n+  void print_on(outputStream* st) const override;\n+  void print_value_on(outputStream* st) const override;\n@@ -626,7 +520,1 @@\n-  \/\/ Typing\n-  bool is_deoptimization_stub() const { return true; }\n-\n-  \/\/ GC for args\n-  void preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f) { \/* Nothing to do *\/ }\n-\n-  void print_value_on(outputStream* st) const;\n+  void print_value_on(outputStream* st) const override;\n@@ -690,6 +578,0 @@\n-\n-  \/\/ GC for args\n-  void preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f)  { \/* nothing to do *\/ }\n-\n-  \/\/ Typing\n-  bool is_uncommon_trap_stub() const             { return true; }\n@@ -720,6 +602,0 @@\n-\n-  \/\/ GC for args\n-  void preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f)  { \/* nothing to do *\/ }\n-\n-  \/\/ Typing\n-  bool is_exception_stub() const                 { return true; }\n@@ -751,6 +627,0 @@\n-\n-  \/\/ GC for args\n-  void preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f)  { \/* nothing to do *\/ }\n-\n-  \/\/ Typing\n-  bool is_safepoint_stub() const                 { return true; }\n@@ -793,3 +663,0 @@\n-  \/\/ Typing\n-  virtual bool is_upcall_stub() const override { return true; }\n-\n@@ -798,2 +665,2 @@\n-  virtual void preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f) override;\n-  virtual void verify() override;\n+  void preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f) override;\n+  void verify() override;\n@@ -802,2 +669,2 @@\n-  virtual void print_on(outputStream* st) const override;\n-  virtual void print_value_on(outputStream* st) const override;\n+  void print_on(outputStream* st) const override;\n+  void print_value_on(outputStream* st) const override;\n","filename":"src\/hotspot\/share\/code\/codeBlob.hpp","additions":113,"deletions":246,"binary":false,"changes":359,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -47,1 +47,1 @@\n-CompiledICLocker::CompiledICLocker(CompiledMethod* method)\n+CompiledICLocker::CompiledICLocker(nmethod* method)\n@@ -59,1 +59,1 @@\n-bool CompiledICLocker::is_safe(CompiledMethod* method) {\n+bool CompiledICLocker::is_safe(nmethod* method) {\n@@ -65,3 +65,3 @@\n-  assert(cb != nullptr && cb->is_compiled(), \"must be compiled\");\n-  CompiledMethod* cm = cb->as_compiled_method();\n-  return CompiledICProtectionBehaviour::current()->is_safe(cm);\n+  assert(cb != nullptr && cb->is_nmethod(), \"must be compiled\");\n+  nmethod* nm = cb->as_nmethod();\n+  return CompiledICProtectionBehaviour::current()->is_safe(nm);\n@@ -170,1 +170,1 @@\n-CompiledIC* CompiledIC_before(CompiledMethod* nm, address return_addr) {\n+CompiledIC* CompiledIC_before(nmethod* nm, address return_addr) {\n@@ -175,1 +175,1 @@\n-CompiledIC* CompiledIC_at(CompiledMethod* nm, address call_site) {\n+CompiledIC* CompiledIC_at(nmethod* nm, address call_site) {\n@@ -183,2 +183,2 @@\n-  CompiledMethod* cm = CodeCache::find_blob(call_reloc->addr())->as_compiled_method();\n-  return CompiledIC_at(cm, call_site);\n+  nmethod* nm = CodeCache::find_blob(call_reloc->addr())->as_nmethod();\n+  return CompiledIC_at(nm, call_site);\n@@ -207,1 +207,1 @@\n-  CompiledMethod* code = method->code();\n+  nmethod* code = method->code();\n@@ -324,1 +324,1 @@\n-  \/\/ in_use is unused but needed to match template function in CompiledMethod\n+  \/\/ in_use is unused but needed to match template function in nmethod\n@@ -346,2 +346,3 @@\n-  CompiledMethod* code = callee_method->code();\n-  CompiledMethod* caller = CodeCache::find_compiled(instruction_address());\n+  nmethod* code = callee_method->code();\n+  nmethod* caller = CodeCache::find_nmethod(instruction_address());\n+  assert(caller != nullptr, \"did not find caller nmethod\");\n@@ -380,2 +381,3 @@\n-  CompiledMethod* cm = CodeCache::find_compiled(instruction_address());\n-  return cm->stub_contains(destination());\n+  nmethod* nm = CodeCache::find_nmethod(instruction_address());\n+  assert(nm != nullptr, \"did not find nmethod\");\n+  return nm->stub_contains(destination());\n@@ -385,1 +387,2 @@\n-  CompiledMethod* caller = CodeCache::find_compiled(instruction_address());\n+  nmethod* caller = CodeCache::find_nmethod(instruction_address());\n+  assert(caller != nullptr, \"did not find caller nmethod\");\n@@ -387,1 +390,1 @@\n-  return !caller->stub_contains(destination()) && dest_cb->is_compiled();\n+  return !caller->stub_contains(destination()) && dest_cb->is_nmethod();\n","filename":"src\/hotspot\/share\/code\/compiledIC.cpp","additions":21,"deletions":18,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,1 +42,1 @@\n-class CompiledMethod;\n+class nmethod;\n@@ -45,1 +45,1 @@\n-  CompiledMethod* _method;\n+  nmethod* _method;\n@@ -51,1 +51,1 @@\n-  CompiledICLocker(CompiledMethod* method);\n+  CompiledICLocker(nmethod* method);\n@@ -53,1 +53,1 @@\n-  static bool is_safe(CompiledMethod* method);\n+  static bool is_safe(nmethod* method);\n@@ -101,1 +101,1 @@\n-  CompiledMethod* _method;\n+  nmethod* _method;\n@@ -117,2 +117,2 @@\n-  friend CompiledIC* CompiledIC_before(CompiledMethod* nm, address return_addr);\n-  friend CompiledIC* CompiledIC_at(CompiledMethod* nm, address call_site);\n+  friend CompiledIC* CompiledIC_before(nmethod* nm, address return_addr);\n+  friend CompiledIC* CompiledIC_at(nmethod* nm, address call_site);\n@@ -149,2 +149,2 @@\n-CompiledIC* CompiledIC_before(CompiledMethod* nm, address return_addr);\n-CompiledIC* CompiledIC_at(CompiledMethod* nm, address call_site);\n+CompiledIC* CompiledIC_before(nmethod* nm, address return_addr);\n+CompiledIC* CompiledIC_at(nmethod* nm, address call_site);\n","filename":"src\/hotspot\/share\/code\/compiledIC.hpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -56,9 +56,3 @@\n-  nmethod* nm = const_cast<CompiledMethod*>(code())->as_nmethod_or_null();\n-  oop o;\n-  if (nm != nullptr) {\n-    \/\/ Despite these oops being found inside nmethods that are on-stack,\n-    \/\/ they are not kept alive by all GCs (e.g. G1 and Shenandoah).\n-    o = nm->oop_at_phantom(read_int());\n-  } else {\n-    o = code()->oop_at(read_int());\n-  }\n+  \/\/ Despite these oops being found inside nmethods that are on-stack,\n+  \/\/ they are not kept alive by all GCs (e.g. G1 and Shenandoah).\n+  oop o = code()->oop_at_phantom(read_int());\n@@ -259,1 +253,1 @@\n-    _selected = new ObjectValue(id());\n+    _selected = new ObjectValue(id(), nullptr, false);\n@@ -265,2 +259,1 @@\n-    \/\/ No need to rematerialize\n-    return nullptr;\n+    return _selected;\n","filename":"src\/hotspot\/share\/code\/debugInfo.cpp","additions":6,"deletions":13,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -137,0 +137,2 @@\n+  bool                       _is_scalar_replaced;      \/\/ Whether this ObjectValue describes an object scalar replaced or just\n+                                                       \/\/ an object (possibly null) participating in an allocation merge.\n@@ -142,1 +144,1 @@\n-  ObjectValue(int id, ScopeValue* klass, ScopeValue* is_init = nullptr)\n+  ObjectValue(int id, ScopeValue* klass = nullptr, bool is_scalar_replaced = true, ScopeValue* is_init = nullptr)\n@@ -145,1 +147,1 @@\n-     , _is_init(is_init)\n+     , _is_init((is_init == nullptr) ? new MarkerValue() : is_init)\n@@ -149,0 +151,1 @@\n+     , _is_scalar_replaced(is_scalar_replaced)\n@@ -150,1 +153,1 @@\n-    assert(klass->is_constant_oop(), \"should be constant java mirror oop\");\n+    assert(klass == nullptr || klass->is_constant_oop(), \"should be constant java mirror oop\");\n@@ -153,9 +156,0 @@\n-  ObjectValue(int id)\n-     : _id(id)\n-     , _klass(nullptr)\n-     , _is_init(new MarkerValue())\n-     , _field_values()\n-     , _value()\n-     , _visited(false)\n-     , _is_root(true) {}\n-\n@@ -172,0 +166,1 @@\n+  bool                        is_scalar_replaced() const  { return _is_scalar_replaced; }\n@@ -175,1 +170,1 @@\n-  void                        set_id(int id)              { _id = id; }\n+  void                        set_id(int id)                   { _id = id; }\n@@ -177,2 +172,3 @@\n-  void                        set_visited(bool visited)   { _visited = visited; }\n-  void                        set_root(bool root)         { _is_root = root; }\n+  void                        set_visited(bool visited)        { _visited = visited; }\n+  void                        set_is_scalar_replaced(bool scd) { _is_scalar_replaced = scd; }\n+  void                        set_root(bool root)              { _is_root = root; }\n@@ -216,1 +212,1 @@\n-     : ObjectValue(id)\n+     : ObjectValue(id, nullptr, false)\n@@ -223,1 +219,1 @@\n-     : ObjectValue(id)\n+     : ObjectValue(id, nullptr, false)\n@@ -380,2 +376,2 @@\n-  const CompiledMethod* _code;\n-  const CompiledMethod* code() const { return _code; }\n+  const nmethod* _code;\n+  const nmethod* code() const { return _code; }\n@@ -384,1 +380,1 @@\n-  DebugInfoReadStream(const CompiledMethod* code, int offset, GrowableArray<ScopeValue*>* obj_pool = nullptr) :\n+  DebugInfoReadStream(const nmethod* code, int offset, GrowableArray<ScopeValue*>* obj_pool = nullptr) :\n","filename":"src\/hotspot\/share\/code\/debugInfo.hpp","additions":17,"deletions":21,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -178,1 +178,1 @@\n-         \"must specify a new, larger pc offset\");\n+         \"must specify a new, larger pc offset: %d >= %d\", last_pc()->pc_offset(), pc_offset);\n","filename":"src\/hotspot\/share\/code\/debugInfoRec.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"code\/compiledMethod.inline.hpp\"\n@@ -32,1 +31,1 @@\n-#include \"code\/nmethod.hpp\"\n+#include \"code\/nmethod.inline.hpp\"\n@@ -47,1 +46,1 @@\n-#include \"interpreter\/bytecode.hpp\"\n+#include \"interpreter\/bytecode.inline.hpp\"\n@@ -101,3 +100,3 @@\n-        (char *) klass_name->bytes(), klass_name->utf8_length(),                   \\\n-        (char *) name->bytes(), name->utf8_length(),                               \\\n-        (char *) signature->bytes(), signature->utf8_length());                    \\\n+        (char *) klass_name->bytes(), klass_name->utf8_length(),          \\\n+        (char *) name->bytes(), name->utf8_length(),                      \\\n+        (char *) signature->bytes(), signature->utf8_length());           \\\n@@ -141,0 +140,3 @@\n+  uint size_gt_32k;\n+  int size_max;\n+\n@@ -159,0 +161,3 @@\n+    int short_pos_max = ((1<<15) - 1);\n+    if (nm->size() > short_pos_max) size_gt_32k++;\n+    if (nm->size() > size_max) size_max = nm->size();\n@@ -163,13 +168,14 @@\n-    if (total_size != 0)          tty->print_cr(\" total in heap  = %u\", total_size);\n-    if (nmethod_count != 0)       tty->print_cr(\" header         = \" SIZE_FORMAT, nmethod_count * sizeof(nmethod));\n-    if (relocation_size != 0)     tty->print_cr(\" relocation     = %u\", relocation_size);\n-    if (consts_size != 0)         tty->print_cr(\" constants      = %u\", consts_size);\n-    if (insts_size != 0)          tty->print_cr(\" main code      = %u\", insts_size);\n-    if (stub_size != 0)           tty->print_cr(\" stub code      = %u\", stub_size);\n-    if (oops_size != 0)           tty->print_cr(\" oops           = %u\", oops_size);\n-    if (metadata_size != 0)       tty->print_cr(\" metadata       = %u\", metadata_size);\n-    if (scopes_data_size != 0)    tty->print_cr(\" scopes data    = %u\", scopes_data_size);\n-    if (scopes_pcs_size != 0)     tty->print_cr(\" scopes pcs     = %u\", scopes_pcs_size);\n-    if (dependencies_size != 0)   tty->print_cr(\" dependencies   = %u\", dependencies_size);\n-    if (handler_table_size != 0)  tty->print_cr(\" handler table  = %u\", handler_table_size);\n-    if (nul_chk_table_size != 0)  tty->print_cr(\" nul chk table  = %u\", nul_chk_table_size);\n+    if (total_size != 0)          tty->print_cr(\" total in heap  = %u (100%%)\", total_size);\n+    uint header_size = (uint)(nmethod_count * sizeof(nmethod));\n+    if (nmethod_count != 0)       tty->print_cr(\" header         = %u (%f%%)\", header_size, (header_size * 100.0f)\/total_size);\n+    if (relocation_size != 0)     tty->print_cr(\" relocation     = %u (%f%%)\", relocation_size, (relocation_size * 100.0f)\/total_size);\n+    if (consts_size != 0)         tty->print_cr(\" constants      = %u (%f%%)\", consts_size, (consts_size * 100.0f)\/total_size);\n+    if (insts_size != 0)          tty->print_cr(\" main code      = %u (%f%%)\", insts_size, (insts_size * 100.0f)\/total_size);\n+    if (stub_size != 0)           tty->print_cr(\" stub code      = %u (%f%%)\", stub_size, (stub_size * 100.0f)\/total_size);\n+    if (oops_size != 0)           tty->print_cr(\" oops           = %u (%f%%)\", oops_size, (oops_size * 100.0f)\/total_size);\n+    if (metadata_size != 0)       tty->print_cr(\" metadata       = %u (%f%%)\", metadata_size, (metadata_size * 100.0f)\/total_size);\n+    if (scopes_data_size != 0)    tty->print_cr(\" scopes data    = %u (%f%%)\", scopes_data_size, (scopes_data_size * 100.0f)\/total_size);\n+    if (scopes_pcs_size != 0)     tty->print_cr(\" scopes pcs     = %u (%f%%)\", scopes_pcs_size, (scopes_pcs_size * 100.0f)\/total_size);\n+    if (dependencies_size != 0)   tty->print_cr(\" dependencies   = %u (%f%%)\", dependencies_size, (dependencies_size * 100.0f)\/total_size);\n+    if (handler_table_size != 0)  tty->print_cr(\" handler table  = %u (%f%%)\", handler_table_size, (handler_table_size * 100.0f)\/total_size);\n+    if (nul_chk_table_size != 0)  tty->print_cr(\" nul chk table  = %u (%f%%)\", nul_chk_table_size, (nul_chk_table_size * 100.0f)\/total_size);\n@@ -177,2 +183,2 @@\n-    if (speculations_size != 0)   tty->print_cr(\" speculations   = %u\", speculations_size);\n-    if (jvmci_data_size != 0)     tty->print_cr(\" JVMCI data     = %u\", jvmci_data_size);\n+    if (speculations_size != 0)   tty->print_cr(\" speculations   = %u (%f%%)\", speculations_size, (speculations_size * 100.0f)\/total_size);\n+    if (jvmci_data_size != 0)     tty->print_cr(\" JVMCI data     = %u (%f%%)\", jvmci_data_size, (jvmci_data_size * 100.0f)\/total_size);\n@@ -180,0 +186,2 @@\n+    if (size_gt_32k != 0)         tty->print_cr(\" size > 32k     = %u\", size_gt_32k);\n+    if (size_max != 0)            tty->print_cr(\" max size       = %d\", size_max);\n@@ -420,0 +428,563 @@\n+bool nmethod::is_method_handle_return(address return_pc) {\n+  if (!has_method_handle_invokes())  return false;\n+  PcDesc* pd = pc_desc_at(return_pc);\n+  if (pd == nullptr)\n+    return false;\n+  return pd->is_method_handle_invoke();\n+}\n+\n+\/\/ Returns a string version of the method state.\n+const char* nmethod::state() const {\n+  int state = get_state();\n+  switch (state) {\n+  case not_installed:\n+    return \"not installed\";\n+  case in_use:\n+    return \"in use\";\n+  case not_entrant:\n+    return \"not_entrant\";\n+  default:\n+    fatal(\"unexpected method state: %d\", state);\n+    return nullptr;\n+  }\n+}\n+\n+void nmethod::set_deoptimized_done() {\n+  ConditionalMutexLocker ml(NMethodState_lock, !NMethodState_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n+  if (_deoptimization_status != deoptimize_done) { \/\/ can't go backwards\n+    Atomic::store(&_deoptimization_status, deoptimize_done);\n+  }\n+}\n+\n+ExceptionCache* nmethod::exception_cache_acquire() const {\n+  return Atomic::load_acquire(&_exception_cache);\n+}\n+\n+void nmethod::add_exception_cache_entry(ExceptionCache* new_entry) {\n+  assert(ExceptionCache_lock->owned_by_self(),\"Must hold the ExceptionCache_lock\");\n+  assert(new_entry != nullptr,\"Must be non null\");\n+  assert(new_entry->next() == nullptr, \"Must be null\");\n+\n+  for (;;) {\n+    ExceptionCache *ec = exception_cache();\n+    if (ec != nullptr) {\n+      Klass* ex_klass = ec->exception_type();\n+      if (!ex_klass->is_loader_alive()) {\n+        \/\/ We must guarantee that entries are not inserted with new next pointer\n+        \/\/ edges to ExceptionCache entries with dead klasses, due to bad interactions\n+        \/\/ with concurrent ExceptionCache cleanup. Therefore, the inserts roll\n+        \/\/ the head pointer forward to the first live ExceptionCache, so that the new\n+        \/\/ next pointers always point at live ExceptionCaches, that are not removed due\n+        \/\/ to concurrent ExceptionCache cleanup.\n+        ExceptionCache* next = ec->next();\n+        if (Atomic::cmpxchg(&_exception_cache, ec, next) == ec) {\n+          CodeCache::release_exception_cache(ec);\n+        }\n+        continue;\n+      }\n+      ec = exception_cache();\n+      if (ec != nullptr) {\n+        new_entry->set_next(ec);\n+      }\n+    }\n+    if (Atomic::cmpxchg(&_exception_cache, ec, new_entry) == ec) {\n+      return;\n+    }\n+  }\n+}\n+\n+void nmethod::clean_exception_cache() {\n+  \/\/ For each nmethod, only a single thread may call this cleanup function\n+  \/\/ at the same time, whether called in STW cleanup or concurrent cleanup.\n+  \/\/ Note that if the GC is processing exception cache cleaning in a concurrent phase,\n+  \/\/ then a single writer may contend with cleaning up the head pointer to the\n+  \/\/ first ExceptionCache node that has a Klass* that is alive. That is fine,\n+  \/\/ as long as there is no concurrent cleanup of next pointers from concurrent writers.\n+  \/\/ And the concurrent writers do not clean up next pointers, only the head.\n+  \/\/ Also note that concurrent readers will walk through Klass* pointers that are not\n+  \/\/ alive. That does not cause ABA problems, because Klass* is deleted after\n+  \/\/ a handshake with all threads, after all stale ExceptionCaches have been\n+  \/\/ unlinked. That is also when the CodeCache::exception_cache_purge_list()\n+  \/\/ is deleted, with all ExceptionCache entries that were cleaned concurrently.\n+  \/\/ That similarly implies that CAS operations on ExceptionCache entries do not\n+  \/\/ suffer from ABA problems as unlinking and deletion is separated by a global\n+  \/\/ handshake operation.\n+  ExceptionCache* prev = nullptr;\n+  ExceptionCache* curr = exception_cache_acquire();\n+\n+  while (curr != nullptr) {\n+    ExceptionCache* next = curr->next();\n+\n+    if (!curr->exception_type()->is_loader_alive()) {\n+      if (prev == nullptr) {\n+        \/\/ Try to clean head; this is contended by concurrent inserts, that\n+        \/\/ both lazily clean the head, and insert entries at the head. If\n+        \/\/ the CAS fails, the operation is restarted.\n+        if (Atomic::cmpxchg(&_exception_cache, curr, next) != curr) {\n+          prev = nullptr;\n+          curr = exception_cache_acquire();\n+          continue;\n+        }\n+      } else {\n+        \/\/ It is impossible to during cleanup connect the next pointer to\n+        \/\/ an ExceptionCache that has not been published before a safepoint\n+        \/\/ prior to the cleanup. Therefore, release is not required.\n+        prev->set_next(next);\n+      }\n+      \/\/ prev stays the same.\n+\n+      CodeCache::release_exception_cache(curr);\n+    } else {\n+      prev = curr;\n+    }\n+\n+    curr = next;\n+  }\n+}\n+\n+\/\/ public method for accessing the exception cache\n+\/\/ These are the public access methods.\n+address nmethod::handler_for_exception_and_pc(Handle exception, address pc) {\n+  \/\/ We never grab a lock to read the exception cache, so we may\n+  \/\/ have false negatives. This is okay, as it can only happen during\n+  \/\/ the first few exception lookups for a given nmethod.\n+  ExceptionCache* ec = exception_cache_acquire();\n+  while (ec != nullptr) {\n+    address ret_val;\n+    if ((ret_val = ec->match(exception,pc)) != nullptr) {\n+      return ret_val;\n+    }\n+    ec = ec->next();\n+  }\n+  return nullptr;\n+}\n+\n+void nmethod::add_handler_for_exception_and_pc(Handle exception, address pc, address handler) {\n+  \/\/ There are potential race conditions during exception cache updates, so we\n+  \/\/ must own the ExceptionCache_lock before doing ANY modifications. Because\n+  \/\/ we don't lock during reads, it is possible to have several threads attempt\n+  \/\/ to update the cache with the same data. We need to check for already inserted\n+  \/\/ copies of the current data before adding it.\n+\n+  MutexLocker ml(ExceptionCache_lock);\n+  ExceptionCache* target_entry = exception_cache_entry_for_exception(exception);\n+\n+  if (target_entry == nullptr || !target_entry->add_address_and_handler(pc,handler)) {\n+    target_entry = new ExceptionCache(exception,pc,handler);\n+    add_exception_cache_entry(target_entry);\n+  }\n+}\n+\n+\/\/ private method for handling exception cache\n+\/\/ These methods are private, and used to manipulate the exception cache\n+\/\/ directly.\n+ExceptionCache* nmethod::exception_cache_entry_for_exception(Handle exception) {\n+  ExceptionCache* ec = exception_cache_acquire();\n+  while (ec != nullptr) {\n+    if (ec->match_exception_with_space(exception)) {\n+      return ec;\n+    }\n+    ec = ec->next();\n+  }\n+  return nullptr;\n+}\n+\n+bool nmethod::is_at_poll_return(address pc) {\n+  RelocIterator iter(this, pc, pc+1);\n+  while (iter.next()) {\n+    if (iter.type() == relocInfo::poll_return_type)\n+      return true;\n+  }\n+  return false;\n+}\n+\n+\n+bool nmethod::is_at_poll_or_poll_return(address pc) {\n+  RelocIterator iter(this, pc, pc+1);\n+  while (iter.next()) {\n+    relocInfo::relocType t = iter.type();\n+    if (t == relocInfo::poll_return_type || t == relocInfo::poll_type)\n+      return true;\n+  }\n+  return false;\n+}\n+\n+void nmethod::verify_oop_relocations() {\n+  \/\/ Ensure sure that the code matches the current oop values\n+  RelocIterator iter(this, nullptr, nullptr);\n+  while (iter.next()) {\n+    if (iter.type() == relocInfo::oop_type) {\n+      oop_Relocation* reloc = iter.oop_reloc();\n+      if (!reloc->oop_is_immediate()) {\n+        reloc->verify_oop_relocation();\n+      }\n+    }\n+  }\n+}\n+\n+\n+ScopeDesc* nmethod::scope_desc_at(address pc) {\n+  PcDesc* pd = pc_desc_at(pc);\n+  guarantee(pd != nullptr, \"scope must be present\");\n+  return new ScopeDesc(this, pd);\n+}\n+\n+ScopeDesc* nmethod::scope_desc_near(address pc) {\n+  PcDesc* pd = pc_desc_near(pc);\n+  guarantee(pd != nullptr, \"scope must be present\");\n+  return new ScopeDesc(this, pd);\n+}\n+\n+address nmethod::oops_reloc_begin() const {\n+  \/\/ If the method is not entrant then a JMP is plastered over the\n+  \/\/ first few bytes.  If an oop in the old code was there, that oop\n+  \/\/ should not get GC'd.  Skip the first few bytes of oops on\n+  \/\/ not-entrant methods.\n+  if (frame_complete_offset() != CodeOffsets::frame_never_safe &&\n+      code_begin() + frame_complete_offset() >\n+      verified_entry_point() + NativeJump::instruction_size)\n+  {\n+    \/\/ If we have a frame_complete_offset after the native jump, then there\n+    \/\/ is no point trying to look for oops before that. This is a requirement\n+    \/\/ for being allowed to scan oops concurrently.\n+    return code_begin() + frame_complete_offset();\n+  }\n+\n+  \/\/ It is not safe to read oops concurrently using entry barriers, if their\n+  \/\/ location depend on whether the nmethod is entrant or not.\n+  \/\/ assert(BarrierSet::barrier_set()->barrier_set_nmethod() == nullptr, \"Not safe oop scan\");\n+\n+  address low_boundary = verified_entry_point();\n+  if (!is_in_use()) {\n+    low_boundary += NativeJump::instruction_size;\n+    \/\/ %%% Note:  On SPARC we patch only a 4-byte trap, not a full NativeJump.\n+    \/\/ This means that the low_boundary is going to be a little too high.\n+    \/\/ This shouldn't matter, since oops of non-entrant methods are never used.\n+    \/\/ In fact, why are we bothering to look at oops in a non-entrant method??\n+  }\n+  return low_boundary;\n+}\n+\n+\/\/ Method that knows how to preserve outgoing arguments at call. This method must be\n+\/\/ called with a frame corresponding to a Java invoke\n+void nmethod::preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f) {\n+  if (method() == nullptr) {\n+    return;\n+  }\n+\n+  \/\/ handle the case of an anchor explicitly set in continuation code that doesn't have a callee\n+  JavaThread* thread = reg_map->thread();\n+  if (thread->has_last_Java_frame() && fr.sp() == thread->last_Java_sp()) {\n+    return;\n+  }\n+\n+  if (!method()->is_native()) {\n+    address pc = fr.pc();\n+    bool has_receiver, has_appendix;\n+    Symbol* signature;\n+\n+    \/\/ The method attached by JIT-compilers should be used, if present.\n+    \/\/ Bytecode can be inaccurate in such case.\n+    Method* callee = attached_method_before_pc(pc);\n+    if (callee != nullptr) {\n+      has_receiver = !(callee->access_flags().is_static());\n+      has_appendix = false;\n+      signature    = callee->signature();\n+\n+      \/\/ If inline types are passed as fields, use the extended signature\n+      \/\/ which contains the types of all (oop) fields of the inline type.\n+      if (is_compiled_by_c2() && callee->has_scalarized_args()) {\n+        const GrowableArray<SigEntry>* sig = callee->adapter()->get_sig_cc();\n+        assert(sig != nullptr, \"sig should never be null\");\n+        TempNewSymbol tmp_sig = SigEntry::create_symbol(sig);\n+        has_receiver = false; \/\/ The extended signature contains the receiver type\n+        fr.oops_compiled_arguments_do(tmp_sig, has_receiver, has_appendix, reg_map, f);\n+        return;\n+      }\n+    } else {\n+      SimpleScopeDesc ssd(this, pc);\n+\n+      Bytecode_invoke call(methodHandle(Thread::current(), ssd.method()), ssd.bci());\n+      has_receiver = call.has_receiver();\n+      has_appendix = call.has_appendix();\n+      signature    = call.signature();\n+    }\n+\n+    fr.oops_compiled_arguments_do(signature, has_receiver, has_appendix, reg_map, f);\n+  } else if (method()->is_continuation_enter_intrinsic()) {\n+    \/\/ This method only calls Continuation.enter()\n+    Symbol* signature = vmSymbols::continuationEnter_signature();\n+    fr.oops_compiled_arguments_do(signature, false, false, reg_map, f);\n+  }\n+}\n+\n+Method* nmethod::attached_method(address call_instr) {\n+  assert(code_contains(call_instr), \"not part of the nmethod\");\n+  RelocIterator iter(this, call_instr, call_instr + 1);\n+  while (iter.next()) {\n+    if (iter.addr() == call_instr) {\n+      switch(iter.type()) {\n+        case relocInfo::static_call_type:      return iter.static_call_reloc()->method_value();\n+        case relocInfo::opt_virtual_call_type: return iter.opt_virtual_call_reloc()->method_value();\n+        case relocInfo::virtual_call_type:     return iter.virtual_call_reloc()->method_value();\n+        default:                               break;\n+      }\n+    }\n+  }\n+  return nullptr; \/\/ not found\n+}\n+\n+Method* nmethod::attached_method_before_pc(address pc) {\n+  if (NativeCall::is_call_before(pc)) {\n+    NativeCall* ncall = nativeCall_before(pc);\n+    return attached_method(ncall->instruction_address());\n+  }\n+  return nullptr; \/\/ not a call\n+}\n+\n+void nmethod::clear_inline_caches() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"clearing of IC's only allowed at safepoint\");\n+  RelocIterator iter(this);\n+  while (iter.next()) {\n+    iter.reloc()->clear_inline_cache();\n+  }\n+}\n+\n+#ifdef ASSERT\n+\/\/ Check class_loader is alive for this bit of metadata.\n+class CheckClass : public MetadataClosure {\n+  void do_metadata(Metadata* md) {\n+    Klass* klass = nullptr;\n+    if (md->is_klass()) {\n+      klass = ((Klass*)md);\n+    } else if (md->is_method()) {\n+      klass = ((Method*)md)->method_holder();\n+    } else if (md->is_methodData()) {\n+      klass = ((MethodData*)md)->method()->method_holder();\n+    } else {\n+      md->print();\n+      ShouldNotReachHere();\n+    }\n+    assert(klass->is_loader_alive(), \"must be alive\");\n+  }\n+};\n+#endif \/\/ ASSERT\n+\n+\n+static void clean_ic_if_metadata_is_dead(CompiledIC *ic) {\n+  ic->clean_metadata();\n+}\n+\n+\/\/ Clean references to unloaded nmethods at addr from this one, which is not unloaded.\n+template <typename CallsiteT>\n+static void clean_if_nmethod_is_unloaded(CallsiteT* callsite, nmethod* from,\n+                                         bool clean_all) {\n+  CodeBlob* cb = CodeCache::find_blob(callsite->destination());\n+  if (!cb->is_nmethod()) {\n+    return;\n+  }\n+  nmethod* nm = cb->as_nmethod();\n+  if (clean_all || !nm->is_in_use() || nm->is_unloading() || nm->method()->code() != nm) {\n+    callsite->set_to_clean();\n+  }\n+}\n+\n+\/\/ Cleans caches in nmethods that point to either classes that are unloaded\n+\/\/ or nmethods that are unloaded.\n+\/\/\n+\/\/ Can be called either in parallel by G1 currently or after all\n+\/\/ nmethods are unloaded.  Return postponed=true in the parallel case for\n+\/\/ inline caches found that point to nmethods that are not yet visited during\n+\/\/ the do_unloading walk.\n+void nmethod::unload_nmethod_caches(bool unloading_occurred) {\n+  ResourceMark rm;\n+\n+  \/\/ Exception cache only needs to be called if unloading occurred\n+  if (unloading_occurred) {\n+    clean_exception_cache();\n+  }\n+\n+  cleanup_inline_caches_impl(unloading_occurred, false);\n+\n+#ifdef ASSERT\n+  \/\/ Check that the metadata embedded in the nmethod is alive\n+  CheckClass check_class;\n+  metadata_do(&check_class);\n+#endif\n+}\n+\n+void nmethod::run_nmethod_entry_barrier() {\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  if (bs_nm != nullptr) {\n+    \/\/ We want to keep an invariant that nmethods found through iterations of a Thread's\n+    \/\/ nmethods found in safepoints have gone through an entry barrier and are not armed.\n+    \/\/ By calling this nmethod entry barrier, it plays along and acts\n+    \/\/ like any other nmethod found on the stack of a thread (fewer surprises).\n+    nmethod* nm = this;\n+    if (bs_nm->is_armed(nm)) {\n+      bool alive = bs_nm->nmethod_entry_barrier(nm);\n+      assert(alive, \"should be alive\");\n+    }\n+  }\n+}\n+\n+\/\/ Only called by whitebox test\n+void nmethod::cleanup_inline_caches_whitebox() {\n+  assert_locked_or_safepoint(CodeCache_lock);\n+  CompiledICLocker ic_locker(this);\n+  cleanup_inline_caches_impl(false \/* unloading_occurred *\/, true \/* clean_all *\/);\n+}\n+\n+address* nmethod::orig_pc_addr(const frame* fr) {\n+  return (address*) ((address)fr->unextended_sp() + orig_pc_offset());\n+}\n+\n+\/\/ Called to clean up after class unloading for live nmethods\n+void nmethod::cleanup_inline_caches_impl(bool unloading_occurred, bool clean_all) {\n+  assert(CompiledICLocker::is_safe(this), \"mt unsafe call\");\n+  ResourceMark rm;\n+\n+  \/\/ Find all calls in an nmethod and clear the ones that point to bad nmethods.\n+  RelocIterator iter(this, oops_reloc_begin());\n+  bool is_in_static_stub = false;\n+  while(iter.next()) {\n+\n+    switch (iter.type()) {\n+\n+    case relocInfo::virtual_call_type:\n+      if (unloading_occurred) {\n+        \/\/ If class unloading occurred we first clear ICs where the cached metadata\n+        \/\/ is referring to an unloaded klass or method.\n+        clean_ic_if_metadata_is_dead(CompiledIC_at(&iter));\n+      }\n+\n+      clean_if_nmethod_is_unloaded(CompiledIC_at(&iter), this, clean_all);\n+      break;\n+\n+    case relocInfo::opt_virtual_call_type:\n+    case relocInfo::static_call_type:\n+      clean_if_nmethod_is_unloaded(CompiledDirectCall::at(iter.reloc()), this, clean_all);\n+      break;\n+\n+    case relocInfo::static_stub_type: {\n+      is_in_static_stub = true;\n+      break;\n+    }\n+\n+    case relocInfo::metadata_type: {\n+      \/\/ Only the metadata relocations contained in static\/opt virtual call stubs\n+      \/\/ contains the Method* passed to c2i adapters. It is the only metadata\n+      \/\/ relocation that needs to be walked, as it is the one metadata relocation\n+      \/\/ that violates the invariant that all metadata relocations have an oop\n+      \/\/ in the compiled method (due to deferred resolution and code patching).\n+\n+      \/\/ This causes dead metadata to remain in compiled methods that are not\n+      \/\/ unloading. Unless these slippery metadata relocations of the static\n+      \/\/ stubs are at least cleared, subsequent class redefinition operations\n+      \/\/ will access potentially free memory, and JavaThread execution\n+      \/\/ concurrent to class unloading may call c2i adapters with dead methods.\n+      if (!is_in_static_stub) {\n+        \/\/ The first metadata relocation after a static stub relocation is the\n+        \/\/ metadata relocation of the static stub used to pass the Method* to\n+        \/\/ c2i adapters.\n+        continue;\n+      }\n+      is_in_static_stub = false;\n+      if (is_unloading()) {\n+        \/\/ If the nmethod itself is dying, then it may point at dead metadata.\n+        \/\/ Nobody should follow that metadata; it is strictly unsafe.\n+        continue;\n+      }\n+      metadata_Relocation* r = iter.metadata_reloc();\n+      Metadata* md = r->metadata_value();\n+      if (md != nullptr && md->is_method()) {\n+        Method* method = static_cast<Method*>(md);\n+        if (!method->method_holder()->is_loader_alive()) {\n+          Atomic::store(r->metadata_addr(), (Method*)nullptr);\n+\n+          if (!r->metadata_is_immediate()) {\n+            r->fix_metadata_relocation();\n+          }\n+        }\n+      }\n+      break;\n+    }\n+\n+    default:\n+      break;\n+    }\n+  }\n+}\n+\n+address nmethod::continuation_for_implicit_exception(address pc, bool for_div0_check) {\n+  \/\/ Exception happened outside inline-cache check code => we are inside\n+  \/\/ an active nmethod => use cpc to determine a return address\n+  int exception_offset = int(pc - code_begin());\n+  int cont_offset = ImplicitExceptionTable(this).continuation_offset( exception_offset );\n+#ifdef ASSERT\n+  if (cont_offset == 0) {\n+    Thread* thread = Thread::current();\n+    ResourceMark rm(thread);\n+    CodeBlob* cb = CodeCache::find_blob(pc);\n+    assert(cb != nullptr && cb == this, \"\");\n+\n+    \/\/ Keep tty output consistent. To avoid ttyLocker, we buffer in stream, and print all at once.\n+    stringStream ss;\n+    ss.print_cr(\"implicit exception happened at \" INTPTR_FORMAT, p2i(pc));\n+    print_on(&ss);\n+    method()->print_codes_on(&ss);\n+    print_code_on(&ss);\n+    print_pcs_on(&ss);\n+    tty->print(\"%s\", ss.as_string()); \/\/ print all at once\n+  }\n+#endif\n+  if (cont_offset == 0) {\n+    \/\/ Let the normal error handling report the exception\n+    return nullptr;\n+  }\n+  if (cont_offset == exception_offset) {\n+#if INCLUDE_JVMCI\n+    Deoptimization::DeoptReason deopt_reason = for_div0_check ? Deoptimization::Reason_div0_check : Deoptimization::Reason_null_check;\n+    JavaThread *thread = JavaThread::current();\n+    thread->set_jvmci_implicit_exception_pc(pc);\n+    thread->set_pending_deoptimization(Deoptimization::make_trap_request(deopt_reason,\n+                                                                         Deoptimization::Action_reinterpret));\n+    return (SharedRuntime::deopt_blob()->implicit_exception_uncommon_trap());\n+#else\n+    ShouldNotReachHere();\n+#endif\n+  }\n+  return code_begin() + cont_offset;\n+}\n+\n+class HasEvolDependency : public MetadataClosure {\n+  bool _has_evol_dependency;\n+ public:\n+  HasEvolDependency() : _has_evol_dependency(false) {}\n+  void do_metadata(Metadata* md) {\n+    if (md->is_method()) {\n+      Method* method = (Method*)md;\n+      if (method->is_old()) {\n+        _has_evol_dependency = true;\n+      }\n+    }\n+  }\n+  bool has_evol_dependency() const { return _has_evol_dependency; }\n+};\n+\n+bool nmethod::has_evol_metadata() {\n+  \/\/ Check the metadata in relocIter and CompiledIC and also deoptimize\n+  \/\/ any nmethod that has reference to old methods.\n+  HasEvolDependency check_evol;\n+  metadata_do(&check_evol);\n+  if (check_evol.has_evol_dependency() && log_is_enabled(Debug, redefine, class, nmethod)) {\n+    ResourceMark rm;\n+    log_debug(redefine, class, nmethod)\n+            (\"Found evol dependency of nmethod %s.%s(%s) compile_id=%d on in nmethod metadata\",\n+             _method->method_holder()->external_name(),\n+             _method->name()->as_C_string(),\n+             _method->signature()->as_C_string(),\n+             compile_id());\n+  }\n+  return check_evol.has_evol_dependency();\n+}\n@@ -443,0 +1014,4 @@\n+const char* nmethod::compiler_name() const {\n+  return compilertype2name(_compiler_type);\n+}\n+\n@@ -445,0 +1020,8 @@\n+  \/\/ avoid uninitialized fields, even for short time periods\n+  _exception_cache            = nullptr;\n+\n+  _has_unsafe_access          = 0;\n+  _has_method_handle_invokes  = 0;\n+  _has_wide_vectors           = 0;\n+  _has_monitors               = 0;\n+\n@@ -449,2 +1032,2 @@\n-  _oops_do_mark_link       = nullptr;\n-  _osr_link                = nullptr;\n+  _oops_do_mark_link          = nullptr;\n+  _osr_link                   = nullptr;\n@@ -452,1 +1035,1 @@\n-  _rtm_state               = NoRTM;\n+  _rtm_state                  = NoRTM;\n@@ -642,1 +1225,5 @@\n-  : CompiledMethod(method, \"native nmethod\", type, nmethod_size, sizeof(nmethod), code_buffer, offsets->value(CodeOffsets::Frame_Complete), frame_size, oop_maps, false, true),\n+  : CodeBlob(\"native nmethod\", CodeBlobKind::Nmethod, code_buffer, nmethod_size, sizeof(nmethod),\n+             offsets->value(CodeOffsets::Frame_Complete), frame_size, oop_maps, false),\n+  _deoptimization_generation(0),\n+  _method(method),\n+  _gc_data(nullptr),\n@@ -647,1 +1234,2 @@\n-  _is_unloading_state(0)\n+  _is_unloading_state(0),\n+  _deoptimization_status(not_marked)\n@@ -650,4 +1238,0 @@\n-    int scopes_data_offset   = 0;\n-    int deoptimize_offset    = 0;\n-    int deoptimize_mh_offset = 0;\n-\n@@ -664,0 +1248,2 @@\n+    _deopt_handler_offset    = 0;\n+    _deopt_mh_handler_offset = 0;\n@@ -670,2 +1256,2 @@\n-    scopes_data_offset       = _metadata_offset      + align_up(code_buffer->total_metadata_size(), wordSize);\n-    _scopes_pcs_offset       = scopes_data_offset;\n+    _scopes_data_offset      = _metadata_offset      + align_up(code_buffer->total_metadata_size(), wordSize);\n+    _scopes_pcs_offset       = _scopes_data_offset;\n@@ -684,0 +1270,1 @@\n+    _compiler_type           = type;\n@@ -698,4 +1285,0 @@\n-    _scopes_data_begin = (address) this + scopes_data_offset;\n-    _deopt_handler_begin = (address) this + deoptimize_offset;\n-    _deopt_mh_handler_begin = (address) this + deoptimize_mh_offset;\n-\n@@ -793,1 +1376,5 @@\n-  : CompiledMethod(method, \"nmethod\", type, nmethod_size, sizeof(nmethod), code_buffer, offsets->value(CodeOffsets::Frame_Complete), frame_size, oop_maps, false, true),\n+  : CodeBlob(\"nmethod\", CodeBlobKind::Nmethod, code_buffer, nmethod_size, sizeof(nmethod),\n+             offsets->value(CodeOffsets::Frame_Complete), frame_size, oop_maps, false),\n+  _deoptimization_generation(0),\n+  _method(method),\n+  _gc_data(nullptr),\n@@ -798,1 +1385,2 @@\n-  _is_unloading_state(0)\n+  _is_unloading_state(0),\n+  _deoptimization_status(not_marked)\n@@ -805,8 +1393,6 @@\n-    _deopt_handler_begin = (address) this;\n-    _deopt_mh_handler_begin = (address) this;\n-\n-    _entry_bci               = entry_bci;\n-    _compile_id              = compile_id;\n-    _comp_level              = comp_level;\n-    _orig_pc_offset          = orig_pc_offset;\n-    _gc_epoch                = CodeCache::gc_epoch();\n+    _entry_bci      = entry_bci;\n+    _compile_id     = compile_id;\n+    _compiler_type  = type;\n+    _comp_level     = comp_level;\n+    _orig_pc_offset = orig_pc_offset;\n+    _gc_epoch       = CodeCache::gc_epoch();\n@@ -816,2 +1402,2 @@\n-    _consts_offset           = content_offset()      + code_buffer->total_offset_of(code_buffer->consts());\n-    _stub_offset             = content_offset()      + code_buffer->total_offset_of(code_buffer->stubs());\n+    _consts_offset  = content_offset() + code_buffer->total_offset_of(code_buffer->consts());\n+    _stub_offset    = content_offset() + code_buffer->total_offset_of(code_buffer->stubs());\n@@ -819,1 +1405,1 @@\n-    _skipped_instructions_size      = code_buffer->total_skipped_instructions_size();\n+    _skipped_instructions_size = code_buffer->total_skipped_instructions_size();\n@@ -825,1 +1411,1 @@\n-        _exception_offset        = code_offset()          + offsets->value(CodeOffsets::Exceptions);\n+        _exception_offset        = code_offset() + offsets->value(CodeOffsets::Exceptions);\n@@ -827,1 +1413,1 @@\n-        _exception_offset = -1;\n+        _exception_offset        = -1;\n@@ -830,1 +1416,1 @@\n-        _deopt_handler_begin       = (address) this + code_offset()          + offsets->value(CodeOffsets::Deopt);\n+        _deopt_handler_offset    = code_offset() + offsets->value(CodeOffsets::Deopt);\n@@ -832,1 +1418,1 @@\n-        _deopt_handler_begin = nullptr;\n+        _deopt_handler_offset    = -1;\n@@ -835,1 +1421,1 @@\n-        _deopt_mh_handler_begin  = (address) this + code_offset()          + offsets->value(CodeOffsets::DeoptMH);\n+        _deopt_mh_handler_offset = code_offset() + offsets->value(CodeOffsets::DeoptMH);\n@@ -837,1 +1423,1 @@\n-        _deopt_mh_handler_begin = nullptr;\n+        _deopt_mh_handler_offset = -1;\n@@ -846,2 +1432,2 @@\n-      _exception_offset       = _stub_offset          + offsets->value(CodeOffsets::Exceptions);\n-      _deopt_handler_begin    = (address) this + _stub_offset          + offsets->value(CodeOffsets::Deopt);\n+      _exception_offset          = _stub_offset + offsets->value(CodeOffsets::Exceptions);\n+      _deopt_handler_offset      = _stub_offset + offsets->value(CodeOffsets::Deopt);\n@@ -849,1 +1435,1 @@\n-        _deopt_mh_handler_begin  = (address) this + _stub_offset          + offsets->value(CodeOffsets::DeoptMH);\n+        _deopt_mh_handler_offset = _stub_offset + offsets->value(CodeOffsets::DeoptMH);\n@@ -851,1 +1437,1 @@\n-        _deopt_mh_handler_begin  = nullptr;\n+        _deopt_mh_handler_offset = -1;\n@@ -855,1 +1441,1 @@\n-      _unwind_handler_offset = code_offset()         + offsets->value(CodeOffsets::UnwindHandler);\n+      _unwind_handler_offset = code_offset() + offsets->value(CodeOffsets::UnwindHandler);\n@@ -862,1 +1448,1 @@\n-    int scopes_data_offset   = _metadata_offset      + align_up(code_buffer->total_metadata_size(), wordSize);\n+    _scopes_data_offset      = _metadata_offset      + align_up(code_buffer->total_metadata_size(), wordSize);\n@@ -864,1 +1450,1 @@\n-    _scopes_pcs_offset       = scopes_data_offset    + align_up(debug_info->data_size       (), oopSize);\n+    _scopes_pcs_offset       = _scopes_data_offset   + align_up(debug_info->data_size       (), oopSize);\n@@ -883,1 +1469,0 @@\n-    _scopes_data_begin       = (address) this + scopes_data_offset;\n@@ -1306,1 +1891,1 @@\n-  assert_lock_strong(CompiledMethod_lock);\n+  assert_lock_strong(NMethodState_lock);\n@@ -1369,1 +1954,1 @@\n-    ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n+    ConditionalMutexLocker ml(NMethodState_lock, !NMethodState_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n@@ -1412,1 +1997,1 @@\n-  } \/\/ leave critical region under CompiledMethod_lock\n+  } \/\/ leave critical region under NMethodState_lock\n@@ -1442,1 +2027,1 @@\n-  \/\/ unlink_from_method will take the CompiledMethod_lock.\n+  \/\/ unlink_from_method will take the NMethodState_lock.\n@@ -1521,1 +2106,1 @@\n-    set_has_flushed_dependencies();\n+    set_has_flushed_dependencies(true);\n@@ -2038,1 +2623,1 @@\n-  assert(has_method_handle_invokes() == (_deopt_mh_handler_begin != nullptr), \"must have deopt mh handler\");\n+  assert(has_method_handle_invokes() == (_deopt_mh_handler_offset != -1), \"must have deopt mh handler\");\n@@ -2255,1 +2840,1 @@\n-    fatal(\"findNMethod did not find this nmethod (\" INTPTR_FORMAT \")\", p2i(this));\n+    fatal(\"find_nmethod did not find this nmethod (\" INTPTR_FORMAT \")\", p2i(this));\n@@ -2338,1 +2923,1 @@\n-  RelocIterator iter((nmethod*)this);\n+  RelocIterator iter(this);\n@@ -3004,2 +3589,2 @@\n-  if (JVMCI_ONLY(_exception_offset >= 0 &&) pos == exception_begin())           label = \"[Exception Handler]\";\n-  if (JVMCI_ONLY(_deopt_handler_begin != nullptr &&) pos == deopt_handler_begin()) label = \"[Deopt Handler Code]\";\n+  if (JVMCI_ONLY(_exception_offset >= 0 &&) pos == exception_begin())          label = \"[Exception Handler]\";\n+  if (JVMCI_ONLY(_deopt_handler_offset != -1 &&) pos == deopt_handler_begin()) label = \"[Deopt Handler Code]\";\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":658,"deletions":73,"binary":false,"changes":731,"status":"modified"},{"patch":"@@ -28,1 +28,2 @@\n-#include \"code\/compiledMethod.hpp\"\n+#include \"code\/codeBlob.hpp\"\n+#include \"code\/pcDesc.hpp\"\n@@ -30,0 +31,2 @@\n+#include \"oops\/metadata.hpp\"\n+#include \"oops\/method.hpp\"\n@@ -31,0 +34,3 @@\n+class AbstractCompiler;\n+class CompiledDirectCall;\n+class CompiledIC;\n@@ -34,0 +40,1 @@\n+class Dependencies;\n@@ -36,0 +43,2 @@\n+class ExceptionHandlerTable;\n+class ImplicitExceptionTable;\n@@ -37,0 +46,2 @@\n+class MetadataClosure;\n+class NativeCallWrapper;\n@@ -38,0 +49,96 @@\n+class ScopeDesc;\n+class xmlStream;\n+\n+\/\/ This class is used internally by nmethods, to cache\n+\/\/ exception\/pc\/handler information.\n+\n+class ExceptionCache : public CHeapObj<mtCode> {\n+  friend class VMStructs;\n+ private:\n+  enum { cache_size = 16 };\n+  Klass*   _exception_type;\n+  address  _pc[cache_size];\n+  address  _handler[cache_size];\n+  volatile int _count;\n+  ExceptionCache* volatile _next;\n+  ExceptionCache* _purge_list_next;\n+\n+  inline address pc_at(int index);\n+  void set_pc_at(int index, address a)      { assert(index >= 0 && index < cache_size,\"\"); _pc[index] = a; }\n+\n+  inline address handler_at(int index);\n+  void set_handler_at(int index, address a) { assert(index >= 0 && index < cache_size,\"\"); _handler[index] = a; }\n+\n+  inline int count();\n+  \/\/ increment_count is only called under lock, but there may be concurrent readers.\n+  void increment_count();\n+\n+ public:\n+\n+  ExceptionCache(Handle exception, address pc, address handler);\n+\n+  Klass*    exception_type()                { return _exception_type; }\n+  ExceptionCache* next();\n+  void      set_next(ExceptionCache *ec);\n+  ExceptionCache* purge_list_next()                 { return _purge_list_next; }\n+  void      set_purge_list_next(ExceptionCache *ec) { _purge_list_next = ec; }\n+\n+  address match(Handle exception, address pc);\n+  bool    match_exception_with_space(Handle exception) ;\n+  address test_address(address addr);\n+  bool    add_address_and_handler(address addr, address handler) ;\n+};\n+\n+\/\/ cache pc descs found in earlier inquiries\n+class PcDescCache {\n+  friend class VMStructs;\n+ private:\n+  enum { cache_size = 4 };\n+  \/\/ The array elements MUST be volatile! Several threads may modify\n+  \/\/ and read from the cache concurrently. find_pc_desc_internal has\n+  \/\/ returned wrong results. C++ compiler (namely xlC12) may duplicate\n+  \/\/ C++ field accesses if the elements are not volatile.\n+  typedef PcDesc* PcDescPtr;\n+  volatile PcDescPtr _pc_descs[cache_size]; \/\/ last cache_size pc_descs found\n+ public:\n+  PcDescCache() { debug_only(_pc_descs[0] = nullptr); }\n+  void    reset_to(PcDesc* initial_pc_desc);\n+  PcDesc* find_pc_desc(int pc_offset, bool approximate);\n+  void    add_pc_desc(PcDesc* pc_desc);\n+  PcDesc* last_pc_desc() { return _pc_descs[0]; }\n+};\n+\n+class PcDescSearch {\n+private:\n+  address _code_begin;\n+  PcDesc* _lower;\n+  PcDesc* _upper;\n+public:\n+  PcDescSearch(address code, PcDesc* lower, PcDesc* upper) :\n+    _code_begin(code), _lower(lower), _upper(upper)\n+  {\n+  }\n+\n+  address code_begin() const { return _code_begin; }\n+  PcDesc* scopes_pcs_begin() const { return _lower; }\n+  PcDesc* scopes_pcs_end() const { return _upper; }\n+};\n+\n+class PcDescContainer {\n+private:\n+  PcDescCache _pc_desc_cache;\n+public:\n+  PcDescContainer() {}\n+\n+  PcDesc* find_pc_desc_internal(address pc, bool approximate, const PcDescSearch& search);\n+  void    reset_to(PcDesc* initial_pc_desc) { _pc_desc_cache.reset_to(initial_pc_desc); }\n+\n+  PcDesc* find_pc_desc(address pc, bool approximate, const PcDescSearch& search) {\n+    address base_address = search.code_begin();\n+    PcDesc* desc = _pc_desc_cache.last_pc_desc();\n+    if (desc != nullptr && desc->pc_offset() == pc - base_address) {\n+      return desc;\n+    }\n+    return find_pc_desc_internal(pc, approximate, search);\n+  }\n+};\n@@ -69,1 +176,1 @@\n-class nmethod : public CompiledMethod {\n+class nmethod : public CodeBlob {\n@@ -74,0 +181,1 @@\n+  friend class DeoptimizationScope;\n@@ -77,0 +185,3 @@\n+  \/\/ Used to track in which deoptimize handshake this method will be deoptimized.\n+  uint64_t  _deoptimization_generation;\n+\n@@ -79,0 +190,2 @@\n+  Method*   _method;\n+\n@@ -82,72 +195,2 @@\n-  \/\/ STW two-phase nmethod root processing helpers.\n-  \/\/\n-  \/\/ When determining liveness of a given nmethod to do code cache unloading,\n-  \/\/ some collectors need to do different things depending on whether the nmethods\n-  \/\/ need to absolutely be kept alive during root processing; \"strong\"ly reachable\n-  \/\/ nmethods are known to be kept alive at root processing, but the liveness of\n-  \/\/ \"weak\"ly reachable ones is to be determined later.\n-  \/\/\n-  \/\/ We want to allow strong and weak processing of nmethods by different threads\n-  \/\/ at the same time without heavy synchronization. Additional constraints are\n-  \/\/ to make sure that every nmethod is processed a minimal amount of time, and\n-  \/\/ nmethods themselves are always iterated at most once at a particular time.\n-  \/\/\n-  \/\/ Note that strong processing work must be a superset of weak processing work\n-  \/\/ for this code to work.\n-  \/\/\n-  \/\/ We store state and claim information in the _oops_do_mark_link member, using\n-  \/\/ the two LSBs for the state and the remaining upper bits for linking together\n-  \/\/ nmethods that were already visited.\n-  \/\/ The last element is self-looped, i.e. points to itself to avoid some special\n-  \/\/ \"end-of-list\" sentinel value.\n-  \/\/\n-  \/\/ _oops_do_mark_link special values:\n-  \/\/\n-  \/\/   _oops_do_mark_link == nullptr: the nmethod has not been visited at all yet, i.e.\n-  \/\/      is Unclaimed.\n-  \/\/\n-  \/\/ For other values, its lowest two bits indicate the following states of the nmethod:\n-  \/\/\n-  \/\/   weak_request (WR): the nmethod has been claimed by a thread for weak processing\n-  \/\/   weak_done (WD): weak processing has been completed for this nmethod.\n-  \/\/   strong_request (SR): the nmethod has been found to need strong processing while\n-  \/\/       being weak processed.\n-  \/\/   strong_done (SD): strong processing has been completed for this nmethod .\n-  \/\/\n-  \/\/ The following shows the _only_ possible progressions of the _oops_do_mark_link\n-  \/\/ pointer.\n-  \/\/\n-  \/\/ Given\n-  \/\/   N as the nmethod\n-  \/\/   X the current next value of _oops_do_mark_link\n-  \/\/\n-  \/\/ Unclaimed (C)-> N|WR (C)-> X|WD: the nmethod has been processed weakly by\n-  \/\/   a single thread.\n-  \/\/ Unclaimed (C)-> N|WR (C)-> X|WD (O)-> X|SD: after weak processing has been\n-  \/\/   completed (as above) another thread found that the nmethod needs strong\n-  \/\/   processing after all.\n-  \/\/ Unclaimed (C)-> N|WR (O)-> N|SR (C)-> X|SD: during weak processing another\n-  \/\/   thread finds that the nmethod needs strong processing, marks it as such and\n-  \/\/   terminates. The original thread completes strong processing.\n-  \/\/ Unclaimed (C)-> N|SD (C)-> X|SD: the nmethod has been processed strongly from\n-  \/\/   the beginning by a single thread.\n-  \/\/\n-  \/\/ \"|\" describes the concatenation of bits in _oops_do_mark_link.\n-  \/\/\n-  \/\/ The diagram also describes the threads responsible for changing the nmethod to\n-  \/\/ the next state by marking the _transition_ with (C) and (O), which mean \"current\"\n-  \/\/ and \"other\" thread respectively.\n-  \/\/\n-  struct oops_do_mark_link; \/\/ Opaque data type.\n-\n-  \/\/ States used for claiming nmethods during root processing.\n-  static const uint claim_weak_request_tag = 0;\n-  static const uint claim_weak_done_tag = 1;\n-  static const uint claim_strong_request_tag = 2;\n-  static const uint claim_strong_done_tag = 3;\n-\n-  static oops_do_mark_link* mark_link(nmethod* nm, uint tag) {\n-    assert(tag <= claim_strong_done_tag, \"invalid tag %u\", tag);\n-    assert(is_aligned(nm, 4), \"nmethod pointer must have zero lower two LSB\");\n-    return (oops_do_mark_link*)(((uintptr_t)nm & ~0x3) | tag);\n-  }\n+  PcDescContainer _pc_desc_container;\n+  ExceptionCache* volatile _exception_cache;\n@@ -155,20 +198,1 @@\n-  static uint extract_state(oops_do_mark_link* link) {\n-    return (uint)((uintptr_t)link & 0x3);\n-  }\n-\n-  static nmethod* extract_nmethod(oops_do_mark_link* link) {\n-    return (nmethod*)((uintptr_t)link & ~0x3);\n-  }\n-\n-  void oops_do_log_change(const char* state);\n-\n-  static bool oops_do_has_weak_request(oops_do_mark_link* next) {\n-    return extract_state(next) == claim_weak_request_tag;\n-  }\n-\n-  static bool oops_do_has_any_strong_state(oops_do_mark_link* next) {\n-    return extract_state(next) >= claim_strong_request_tag;\n-  }\n-\n-  \/\/ Attempt Unclaimed -> N|WR transition. Returns true if successful.\n-  bool oops_do_try_claim_weak_request();\n+  void* _gc_data;\n@@ -176,18 +200,2 @@\n-  \/\/ Attempt Unclaimed -> N|SD transition. Returns the current link.\n-  oops_do_mark_link* oops_do_try_claim_strong_done();\n-  \/\/ Attempt N|WR -> X|WD transition. Returns nullptr if successful, X otherwise.\n-  nmethod* oops_do_try_add_to_list_as_weak_done();\n-\n-  \/\/ Attempt X|WD -> N|SR transition. Returns the current link.\n-  oops_do_mark_link* oops_do_try_add_strong_request(oops_do_mark_link* next);\n-  \/\/ Attempt X|WD -> X|SD transition. Returns true if successful.\n-  bool oops_do_try_claim_weak_done_as_strong_done(oops_do_mark_link* next);\n-\n-  \/\/ Do the N|SD -> X|SD transition.\n-  void oops_do_add_to_list_as_strong_done();\n-\n-  \/\/ Sets this nmethod as strongly claimed (as part of N|SD -> X|SD and N|SR -> X|SD\n-  \/\/ transitions).\n-  void oops_do_set_strong_done(nmethod* old_head);\n-\n-  static nmethod* volatile _oops_do_mark_nmethods;\n+  struct oops_do_mark_link; \/\/ Opaque data type.\n+  static nmethod*    volatile _oops_do_mark_nmethods;\n@@ -205,1 +213,0 @@\n-  bool _is_unlinked;\n@@ -212,0 +219,6 @@\n+  \/\/ All deoptee's will resume execution at this location described by\n+  \/\/ this offset.\n+  int _deopt_handler_offset;\n+  \/\/ All deoptee's at a MethodHandle call site will resume execution\n+  \/\/ at this location described by this offset.\n+  int _deopt_mh_handler_offset;\n@@ -229,2 +242,1 @@\n-\n-  int code_offset() const { return int(code_begin() - header_begin()); }\n+  int _skipped_instructions_size;\n@@ -236,1 +248,5 @@\n-  int _compile_id;                           \/\/ which compilation made this nmethod\n+  int _compile_id;                        \/\/ which compilation made this nmethod\n+\n+  CompilerType _compiler_type;            \/\/ which compiler made this nmethod (u1)\n+\n+  bool _is_unlinked;\n@@ -255,1 +271,1 @@\n-  CompLevel _comp_level;               \/\/ compilation level\n+  CompLevel _comp_level;               \/\/ compilation level (s1)\n@@ -260,3 +276,0 @@\n-  \/\/ protected by CodeCache_lock\n-  bool _has_flushed_dependencies;      \/\/ Used for maintenance of dependencies (CodeCache_lock)\n-\n@@ -266,2 +279,16 @@\n-  \/\/ Protected by CompiledMethod_lock\n-  volatile signed char _state;         \/\/ {not_installed, in_use, not_used, not_entrant}\n+  \/\/ Protected by NMethodState_lock\n+  volatile signed char _state;         \/\/ {not_installed, in_use, not_entrant}\n+\n+  \/\/ set during construction\n+  uint8_t _has_unsafe_access:1,        \/\/ May fault due to unsafe access.\n+          _has_method_handle_invokes:1,\/\/ Has this method MethodHandle invokes?\n+          _has_wide_vectors:1,         \/\/ Preserve wide vectors at safepoints\n+          _has_monitors:1,             \/\/ Fastpath monitor detection for continuations\n+          _has_flushed_dependencies:1; \/\/ Used for maintenance of dependencies (under CodeCache_lock)\n+\n+  enum DeoptimizationStatus : u1 {\n+    not_marked,\n+    deoptimize,\n+    deoptimize_noupdate,\n+    deoptimize_done\n+  };\n@@ -269,1 +296,5 @@\n-  int _skipped_instructions_size;\n+  volatile DeoptimizationStatus _deoptimization_status; \/\/ Used for stack deoptimization\n+\n+  DeoptimizationStatus deoptimization_status() const {\n+    return Atomic::load(&_deoptimization_status);\n+  }\n@@ -309,0 +340,1 @@\n+\n@@ -329,3 +361,3 @@\n-  \/\/ Offsets\n-  int content_offset() const                  { return int(content_begin() - header_begin()); }\n-  int data_offset() const                     { return _data_offset; }\n+  PcDesc* find_pc_desc(address pc, bool approximate) {\n+    return _pc_desc_container.find_pc_desc(pc, approximate, PcDescSearch(code_begin(), scopes_pcs_begin(), scopes_pcs_end()));\n+  }\n@@ -333,1 +365,59 @@\n-  address header_end() const                  { return (address)    header_begin() + header_size(); }\n+  \/\/ STW two-phase nmethod root processing helpers.\n+  \/\/\n+  \/\/ When determining liveness of a given nmethod to do code cache unloading,\n+  \/\/ some collectors need to do different things depending on whether the nmethods\n+  \/\/ need to absolutely be kept alive during root processing; \"strong\"ly reachable\n+  \/\/ nmethods are known to be kept alive at root processing, but the liveness of\n+  \/\/ \"weak\"ly reachable ones is to be determined later.\n+  \/\/\n+  \/\/ We want to allow strong and weak processing of nmethods by different threads\n+  \/\/ at the same time without heavy synchronization. Additional constraints are\n+  \/\/ to make sure that every nmethod is processed a minimal amount of time, and\n+  \/\/ nmethods themselves are always iterated at most once at a particular time.\n+  \/\/\n+  \/\/ Note that strong processing work must be a superset of weak processing work\n+  \/\/ for this code to work.\n+  \/\/\n+  \/\/ We store state and claim information in the _oops_do_mark_link member, using\n+  \/\/ the two LSBs for the state and the remaining upper bits for linking together\n+  \/\/ nmethods that were already visited.\n+  \/\/ The last element is self-looped, i.e. points to itself to avoid some special\n+  \/\/ \"end-of-list\" sentinel value.\n+  \/\/\n+  \/\/ _oops_do_mark_link special values:\n+  \/\/\n+  \/\/   _oops_do_mark_link == nullptr: the nmethod has not been visited at all yet, i.e.\n+  \/\/      is Unclaimed.\n+  \/\/\n+  \/\/ For other values, its lowest two bits indicate the following states of the nmethod:\n+  \/\/\n+  \/\/   weak_request (WR): the nmethod has been claimed by a thread for weak processing\n+  \/\/   weak_done (WD): weak processing has been completed for this nmethod.\n+  \/\/   strong_request (SR): the nmethod has been found to need strong processing while\n+  \/\/       being weak processed.\n+  \/\/   strong_done (SD): strong processing has been completed for this nmethod .\n+  \/\/\n+  \/\/ The following shows the _only_ possible progressions of the _oops_do_mark_link\n+  \/\/ pointer.\n+  \/\/\n+  \/\/ Given\n+  \/\/   N as the nmethod\n+  \/\/   X the current next value of _oops_do_mark_link\n+  \/\/\n+  \/\/ Unclaimed (C)-> N|WR (C)-> X|WD: the nmethod has been processed weakly by\n+  \/\/   a single thread.\n+  \/\/ Unclaimed (C)-> N|WR (C)-> X|WD (O)-> X|SD: after weak processing has been\n+  \/\/   completed (as above) another thread found that the nmethod needs strong\n+  \/\/   processing after all.\n+  \/\/ Unclaimed (C)-> N|WR (O)-> N|SR (C)-> X|SD: during weak processing another\n+  \/\/   thread finds that the nmethod needs strong processing, marks it as such and\n+  \/\/   terminates. The original thread completes strong processing.\n+  \/\/ Unclaimed (C)-> N|SD (C)-> X|SD: the nmethod has been processed strongly from\n+  \/\/   the beginning by a single thread.\n+  \/\/\n+  \/\/ \"|\" describes the concatenation of bits in _oops_do_mark_link.\n+  \/\/\n+  \/\/ The diagram also describes the threads responsible for changing the nmethod to\n+  \/\/ the next state by marking the _transition_ with (C) and (O), which mean \"current\"\n+  \/\/ and \"other\" thread respectively.\n+  \/\/\n@@ -335,1 +425,51 @@\n- public:\n+  \/\/ States used for claiming nmethods during root processing.\n+  static const uint claim_weak_request_tag = 0;\n+  static const uint claim_weak_done_tag = 1;\n+  static const uint claim_strong_request_tag = 2;\n+  static const uint claim_strong_done_tag = 3;\n+\n+  static oops_do_mark_link* mark_link(nmethod* nm, uint tag) {\n+    assert(tag <= claim_strong_done_tag, \"invalid tag %u\", tag);\n+    assert(is_aligned(nm, 4), \"nmethod pointer must have zero lower two LSB\");\n+    return (oops_do_mark_link*)(((uintptr_t)nm & ~0x3) | tag);\n+  }\n+\n+  static uint extract_state(oops_do_mark_link* link) {\n+    return (uint)((uintptr_t)link & 0x3);\n+  }\n+\n+  static nmethod* extract_nmethod(oops_do_mark_link* link) {\n+    return (nmethod*)((uintptr_t)link & ~0x3);\n+  }\n+\n+  void oops_do_log_change(const char* state);\n+\n+  static bool oops_do_has_weak_request(oops_do_mark_link* next) {\n+    return extract_state(next) == claim_weak_request_tag;\n+  }\n+\n+  static bool oops_do_has_any_strong_state(oops_do_mark_link* next) {\n+    return extract_state(next) >= claim_strong_request_tag;\n+  }\n+\n+  \/\/ Attempt Unclaimed -> N|WR transition. Returns true if successful.\n+  bool oops_do_try_claim_weak_request();\n+\n+  \/\/ Attempt Unclaimed -> N|SD transition. Returns the current link.\n+  oops_do_mark_link* oops_do_try_claim_strong_done();\n+  \/\/ Attempt N|WR -> X|WD transition. Returns nullptr if successful, X otherwise.\n+  nmethod* oops_do_try_add_to_list_as_weak_done();\n+\n+  \/\/ Attempt X|WD -> N|SR transition. Returns the current link.\n+  oops_do_mark_link* oops_do_try_add_strong_request(oops_do_mark_link* next);\n+  \/\/ Attempt X|WD -> X|SD transition. Returns true if successful.\n+  bool oops_do_try_claim_weak_done_as_strong_done(oops_do_mark_link* next);\n+\n+  \/\/ Do the N|SD -> X|SD transition.\n+  void oops_do_add_to_list_as_strong_done();\n+\n+  \/\/ Sets this nmethod as strongly claimed (as part of N|SD -> X|SD and N|SR -> X|SD\n+  \/\/ transitions).\n+  void oops_do_set_strong_done(nmethod* old_head);\n+\n+public:\n@@ -358,8 +498,0 @@\n-  \/\/ Only used for unit tests.\n-  nmethod()\n-    : CompiledMethod(),\n-      _native_receiver_sp_offset(in_ByteSize(-1)),\n-      _native_basic_lock_sp_offset(in_ByteSize(-1)),\n-      _is_unloading_state(0) {}\n-\n-\n@@ -377,3 +509,17 @@\n-  \/\/ type info\n-  bool is_nmethod() const                         { return true; }\n-  bool is_osr_method() const                      { return _entry_bci != InvocationEntryBci; }\n+  Method* method       () const { return _method; }\n+  bool is_native_method() const { return _method != nullptr && _method->is_native(); }\n+  bool is_java_method  () const { return _method != nullptr && !_method->is_native(); }\n+  bool is_osr_method   () const { return _entry_bci != InvocationEntryBci; }\n+\n+  \/\/ Compiler task identification.  Note that all OSR methods\n+  \/\/ are numbered in an independent sequence if CICountOSR is true,\n+  \/\/ and native method wrappers are also numbered independently if\n+  \/\/ CICountNative is true.\n+  int compile_id() const { return _compile_id; }\n+  const char* compile_kind() const;\n+\n+  inline bool  is_compiled_by_c1   () const { return _compiler_type == compiler_c1; }\n+  inline bool  is_compiled_by_c2   () const { return _compiler_type == compiler_c2; }\n+  inline bool  is_compiled_by_jvmci() const { return _compiler_type == compiler_jvmci; }\n+  CompilerType compiler_type       () const { return _compiler_type; }\n+  const char*  compiler_name       () const;\n@@ -382,22 +528,25 @@\n-  address consts_begin          () const          { return           header_begin() + _consts_offset        ; }\n-  address consts_end            () const          { return           code_begin()                           ; }\n-  address stub_begin            () const          { return           header_begin() + _stub_offset          ; }\n-  address stub_end              () const          { return           header_begin() + _oops_offset          ; }\n-  address exception_begin       () const          { return           header_begin() + _exception_offset     ; }\n-  address unwind_handler_begin  () const          { return _unwind_handler_offset != -1 ? (header_begin() + _unwind_handler_offset) : nullptr; }\n-  oop*    oops_begin            () const          { return (oop*)   (header_begin() + _oops_offset)         ; }\n-  oop*    oops_end              () const          { return (oop*)   (header_begin() + _metadata_offset)     ; }\n-\n-  Metadata** metadata_begin   () const            { return (Metadata**)  (header_begin() + _metadata_offset)     ; }\n-  Metadata** metadata_end     () const            { return (Metadata**)  _scopes_data_begin; }\n-\n-  address scopes_data_end       () const          { return           header_begin() + _scopes_pcs_offset    ; }\n-  PcDesc* scopes_pcs_begin      () const          { return (PcDesc*)(header_begin() + _scopes_pcs_offset   ); }\n-  PcDesc* scopes_pcs_end        () const          { return (PcDesc*)(header_begin() + _dependencies_offset) ; }\n-  address dependencies_begin    () const          { return           header_begin() + _dependencies_offset  ; }\n-  address dependencies_end      () const          { return           header_begin() + _handler_table_offset ; }\n-  address handler_table_begin   () const          { return           header_begin() + _handler_table_offset ; }\n-  address handler_table_end     () const          { return           header_begin() + _nul_chk_table_offset ; }\n-  address nul_chk_table_begin   () const          { return           header_begin() + _nul_chk_table_offset ; }\n-\n-  int skipped_instructions_size () const          { return           _skipped_instructions_size             ; }\n+  address consts_begin          () const { return           header_begin() + _consts_offset           ; }\n+  address consts_end            () const { return           header_begin() +  code_offset()           ; }\n+  address insts_begin           () const { return           header_begin() +  code_offset()           ; }\n+  address insts_end             () const { return           header_begin() + _stub_offset             ; }\n+  address stub_begin            () const { return           header_begin() + _stub_offset             ; }\n+  address stub_end              () const { return           header_begin() + _oops_offset             ; }\n+  address exception_begin       () const { return           header_begin() + _exception_offset        ; }\n+  address deopt_handler_begin   () const { return           header_begin() + _deopt_handler_offset    ; }\n+  address deopt_mh_handler_begin() const { return           header_begin() + _deopt_mh_handler_offset ; }\n+  address unwind_handler_begin  () const { return _unwind_handler_offset != -1 ? (header_begin() + _unwind_handler_offset) : nullptr; }\n+  oop*    oops_begin            () const { return (oop*)   (header_begin() + _oops_offset)            ; }\n+  oop*    oops_end              () const { return (oop*)   (header_begin() + _metadata_offset)        ; }\n+\n+  Metadata** metadata_begin     () const { return (Metadata**) (header_begin() + _metadata_offset)    ; }\n+  Metadata** metadata_end       () const { return (Metadata**) (header_begin() + _scopes_data_offset) ; }\n+\n+  address scopes_data_begin     () const { return           header_begin() + _scopes_data_offset      ; }\n+  address scopes_data_end       () const { return           header_begin() + _scopes_pcs_offset       ; }\n+  PcDesc* scopes_pcs_begin      () const { return (PcDesc*)(header_begin() + _scopes_pcs_offset)      ; }\n+  PcDesc* scopes_pcs_end        () const { return (PcDesc*)(header_begin() + _dependencies_offset)    ; }\n+  address dependencies_begin    () const { return           header_begin() + _dependencies_offset     ; }\n+  address dependencies_end      () const { return           header_begin() + _handler_table_offset    ; }\n+  address handler_table_begin   () const { return           header_begin() + _handler_table_offset    ; }\n+  address handler_table_end     () const { return           header_begin() + _nul_chk_table_offset    ; }\n+  address nul_chk_table_begin   () const { return           header_begin() + _nul_chk_table_offset    ; }\n@@ -406,5 +555,5 @@\n-  address nul_chk_table_end     () const          { return           header_begin() + _speculations_offset  ; }\n-  address speculations_begin    () const          { return           header_begin() + _speculations_offset  ; }\n-  address speculations_end      () const          { return           header_begin() + _jvmci_data_offset   ; }\n-  address jvmci_data_begin      () const          { return           header_begin() + _jvmci_data_offset    ; }\n-  address jvmci_data_end        () const          { return           header_begin() + _nmethod_end_offset   ; }\n+  address nul_chk_table_end     () const { return           header_begin() + _speculations_offset     ; }\n+  address speculations_begin    () const { return           header_begin() + _speculations_offset     ; }\n+  address speculations_end      () const { return           header_begin() + _jvmci_data_offset       ; }\n+  address jvmci_data_begin      () const { return           header_begin() + _jvmci_data_offset       ; }\n+  address jvmci_data_end        () const { return           header_begin() + _nmethod_end_offset      ; }\n@@ -412,1 +561,1 @@\n-  address nul_chk_table_end     () const          { return           header_begin() + _nmethod_end_offset   ; }\n+  address nul_chk_table_end     () const { return           header_begin() + _nmethod_end_offset      ; }\n@@ -416,3 +565,10 @@\n-  int oops_size         () const                  { return int((address)  oops_end         () - (address)  oops_begin         ()); }\n-  int metadata_size     () const                  { return int((address)  metadata_end     () - (address)  metadata_begin     ()); }\n-  int dependencies_size () const                  { return int(           dependencies_end () -            dependencies_begin ()); }\n+  int consts_size       () const { return int(          consts_end       () -           consts_begin       ()); }\n+  int insts_size        () const { return int(          insts_end        () -           insts_begin        ()); }\n+  int stub_size         () const { return int(          stub_end         () -           stub_begin         ()); }\n+  int oops_size         () const { return int((address) oops_end         () - (address) oops_begin         ()); }\n+  int metadata_size     () const { return int((address) metadata_end     () - (address) metadata_begin     ()); }\n+  int scopes_data_size  () const { return int(          scopes_data_end  () -           scopes_data_begin  ()); }\n+  int scopes_pcs_size   () const { return int((intptr_t)scopes_pcs_end   () - (intptr_t)scopes_pcs_begin   ()); }\n+  int dependencies_size () const { return int(          dependencies_end () -           dependencies_begin ()); }\n+  int handler_table_size() const { return int(          handler_table_end() -           handler_table_begin()); }\n+  int nul_chk_table_size() const { return int(          nul_chk_table_end() -           nul_chk_table_begin()); }\n@@ -420,2 +576,2 @@\n-  int speculations_size () const                  { return int(           speculations_end () -            speculations_begin ()); }\n-  int jvmci_data_size   () const                  { return int(           jvmci_data_end   () -            jvmci_data_begin   ()); }\n+  int speculations_size () const { return int(          speculations_end () -           speculations_begin ()); }\n+  int jvmci_data_size   () const { return int(          jvmci_data_end   () -           jvmci_data_begin   ()); }\n@@ -427,1 +583,2 @@\n-  int total_size        () const;\n+  int skipped_instructions_size () const { return _skipped_instructions_size; }\n+  int total_size() const;\n@@ -430,4 +587,12 @@\n-  bool oops_contains         (oop*    addr) const { return oops_begin         () <= addr && addr < oops_end         (); }\n-  bool metadata_contains     (Metadata** addr) const   { return metadata_begin     () <= addr && addr < metadata_end     (); }\n-  bool scopes_data_contains  (address addr) const { return scopes_data_begin  () <= addr && addr < scopes_data_end  (); }\n-  bool scopes_pcs_contains   (PcDesc* addr) const { return scopes_pcs_begin   () <= addr && addr < scopes_pcs_end   (); }\n+  bool consts_contains         (address addr) const { return consts_begin       () <= addr && addr < consts_end       (); }\n+  \/\/ Returns true if a given address is in the 'insts' section. The method\n+  \/\/ insts_contains_inclusive() is end-inclusive.\n+  bool insts_contains          (address addr) const { return insts_begin        () <= addr && addr < insts_end        (); }\n+  bool insts_contains_inclusive(address addr) const { return insts_begin        () <= addr && addr <= insts_end       (); }\n+  bool stub_contains           (address addr) const { return stub_begin         () <= addr && addr < stub_end         (); }\n+  bool oops_contains           (oop*    addr) const { return oops_begin         () <= addr && addr < oops_end         (); }\n+  bool metadata_contains       (Metadata** addr) const { return metadata_begin  () <= addr && addr < metadata_end     (); }\n+  bool scopes_data_contains    (address addr) const { return scopes_data_begin  () <= addr && addr < scopes_data_end  (); }\n+  bool scopes_pcs_contains     (PcDesc* addr) const { return scopes_pcs_begin   () <= addr && addr < scopes_pcs_end   (); }\n+  bool handler_table_contains  (address addr) const { return handler_table_begin() <= addr && addr < handler_table_end(); }\n+  bool nul_chk_table_contains  (address addr) const { return nul_chk_table_begin() <= addr && addr < nul_chk_table_end(); }\n@@ -436,2 +601,2 @@\n-  address entry_point() const                     { return _entry_point;             }        \/\/ normal entry point\n-  address verified_entry_point() const            { return _verified_entry_point;    }        \/\/ normal entry point without class check\n+  address entry_point() const          { return _entry_point;          } \/\/ normal entry point\n+  address verified_entry_point() const { return _verified_entry_point; } \/\/ if klass is correct\n@@ -442,0 +607,6 @@\n+  enum : signed char { not_installed = -1, \/\/ in construction, only the owner doing the construction is\n+                                           \/\/ allowed to advance state\n+                       in_use        = 0,  \/\/ executable nmethod\n+                       not_entrant   = 1   \/\/ marked for deoptimization but activations may still exist\n+  };\n+\n@@ -443,3 +614,4 @@\n-  bool  is_not_installed() const                  { return _state == not_installed; }\n-  bool  is_in_use() const                         { return _state <= in_use; }\n-  bool  is_not_entrant() const                    { return _state == not_entrant; }\n+  bool is_not_installed() const        { return _state == not_installed; }\n+  bool is_in_use() const               { return _state <= in_use; }\n+  bool is_not_entrant() const          { return _state == not_entrant; }\n+  int  get_state() const               { return _state; }\n@@ -450,2 +622,2 @@\n-  virtual bool is_unloading();\n-  virtual void do_unloading(bool unloading_occurred);\n+  bool is_unloading();\n+  void do_unloading(bool unloading_occurred);\n@@ -453,2 +625,2 @@\n-  bool is_unlinked() const                        { return _is_unlinked; }\n-  void set_is_unlinked()                          { assert(!_is_unlinked, \"already unlinked\"); _is_unlinked = true; }\n+  bool is_unlinked() const             { return _is_unlinked; }\n+  void set_is_unlinked()               { assert(!_is_unlinked, \"already unlinked\"); _is_unlinked = true; }\n@@ -458,2 +630,2 @@\n-  RTMState  rtm_state() const                     { return _rtm_state; }\n-  void set_rtm_state(RTMState state)              { _rtm_state = state; }\n+  RTMState  rtm_state() const          { return _rtm_state; }\n+  void set_rtm_state(RTMState state)   { _rtm_state = state; }\n@@ -472,2 +644,10 @@\n-  int get_state() const {\n-    return _state;\n+  bool  is_marked_for_deoptimization() const { return deoptimization_status() != not_marked; }\n+  bool  has_been_deoptimized() const { return deoptimization_status() == deoptimize_done; }\n+  void  set_deoptimized_done();\n+\n+  bool update_recompile_counts() const {\n+    \/\/ Update recompile counts when either the update is explicitly requested (deoptimize)\n+    \/\/ or the nmethod is not marked for deoptimization at all (not_marked).\n+    \/\/ The latter happens during uncommon traps when deoptimized nmethod is made not entrant.\n+    DeoptimizationStatus status = deoptimization_status();\n+    return status != deoptimize_noupdate && status != deoptimize_done;\n@@ -476,0 +656,4 @@\n+  \/\/ tells whether frames described by this nmethod can be deoptimized\n+  \/\/ note: native wrappers cannot be deoptimized.\n+  bool can_be_deoptimized() const { return is_java_method(); }\n+\n@@ -479,2 +663,30 @@\n-  bool has_flushed_dependencies()                 { return _has_flushed_dependencies; }\n-  void set_has_flushed_dependencies()             {\n+\n+  template<typename T>\n+  T* gc_data() const                              { return reinterpret_cast<T*>(_gc_data); }\n+  template<typename T>\n+  void set_gc_data(T* gc_data)                    { _gc_data = reinterpret_cast<void*>(gc_data); }\n+\n+  bool  has_unsafe_access() const                 { return _has_unsafe_access; }\n+  void  set_has_unsafe_access(bool z)             { _has_unsafe_access = z; }\n+\n+  bool  has_monitors() const                      { return _has_monitors; }\n+  void  set_has_monitors(bool z)                  { _has_monitors = z; }\n+\n+  bool  has_method_handle_invokes() const         { return _has_method_handle_invokes; }\n+  void  set_has_method_handle_invokes(bool z)     { _has_method_handle_invokes = z; }\n+\n+  bool  has_wide_vectors() const                  { return _has_wide_vectors; }\n+  void  set_has_wide_vectors(bool z)              { _has_wide_vectors = z; }\n+\n+  bool  needs_stack_repair() const {\n+    if (is_compiled_by_c1()) {\n+      return method()->c1_needs_stack_repair();\n+    } else if (is_compiled_by_c2()) {\n+      return method()->c2_needs_stack_repair();\n+    } else {\n+      return false;\n+    }\n+  }\n+\n+  bool  has_flushed_dependencies() const          { return _has_flushed_dependencies; }\n+  void  set_has_flushed_dependencies(bool z)      {\n@@ -482,1 +694,1 @@\n-    _has_flushed_dependencies = 1;\n+    _has_flushed_dependencies = z;\n@@ -487,2 +699,0 @@\n-  void unlink_from_method();\n-\n@@ -501,1 +711,1 @@\n-  Metadata*     metadata_at(int index) const      { return index == 0 ? nullptr: *metadata_addr_at(index); }\n+  Metadata*   metadata_at(int index) const      { return index == 0 ? nullptr: *metadata_addr_at(index); }\n@@ -516,0 +726,3 @@\n+protected:\n+  address oops_reloc_begin() const;\n+\n@@ -520,0 +733,72 @@\n+  bool is_at_poll_return(address pc);\n+  bool is_at_poll_or_poll_return(address pc);\n+\n+protected:\n+  \/\/ Exception cache support\n+  \/\/ Note: _exception_cache may be read and cleaned concurrently.\n+  ExceptionCache* exception_cache() const         { return _exception_cache; }\n+  ExceptionCache* exception_cache_acquire() const;\n+  void set_exception_cache(ExceptionCache *ec)    { _exception_cache = ec; }\n+\n+public:\n+  address handler_for_exception_and_pc(Handle exception, address pc);\n+  void add_handler_for_exception_and_pc(Handle exception, address pc, address handler);\n+  void clean_exception_cache();\n+\n+  void add_exception_cache_entry(ExceptionCache* new_entry);\n+  ExceptionCache* exception_cache_entry_for_exception(Handle exception);\n+\n+\n+  \/\/ MethodHandle\n+  bool is_method_handle_return(address return_pc);\n+  \/\/ Deopt\n+  \/\/ Return true is the PC is one would expect if the frame is being deopted.\n+  inline bool is_deopt_pc(address pc);\n+  inline bool is_deopt_mh_entry(address pc);\n+  inline bool is_deopt_entry(address pc);\n+\n+  \/\/ Accessor\/mutator for the original pc of a frame before a frame was deopted.\n+  address get_original_pc(const frame* fr) { return *orig_pc_addr(fr); }\n+  void    set_original_pc(const frame* fr, address pc) { *orig_pc_addr(fr) = pc; }\n+\n+  const char* state() const;\n+\n+  bool inlinecache_check_contains(address addr) const {\n+    return (addr >= code_begin() && addr < verified_entry_point());\n+  }\n+\n+  void preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f) override;\n+\n+  \/\/ implicit exceptions support\n+  address continuation_for_implicit_div0_exception(address pc) { return continuation_for_implicit_exception(pc, true); }\n+  address continuation_for_implicit_null_exception(address pc) { return continuation_for_implicit_exception(pc, false); }\n+\n+  \/\/ Inline cache support for class unloading and nmethod unloading\n+ private:\n+  void cleanup_inline_caches_impl(bool unloading_occurred, bool clean_all);\n+\n+  address continuation_for_implicit_exception(address pc, bool for_div0_check);\n+\n+ public:\n+  \/\/ Serial version used by whitebox test\n+  void cleanup_inline_caches_whitebox();\n+\n+  void clear_inline_caches();\n+\n+  \/\/ Execute nmethod barrier code, as if entering through nmethod call.\n+  void run_nmethod_entry_barrier();\n+\n+  void verify_oop_relocations();\n+\n+  bool has_evol_metadata();\n+\n+  Method* attached_method(address call_pc);\n+  Method* attached_method_before_pc(address pc);\n+\n+  \/\/ GC unloading support\n+  \/\/ Cleans unloaded klasses and unloaded nmethods in inline caches\n+\n+  void unload_nmethod_caches(bool class_unloading_occurred);\n+\n+  void unlink_from_method();\n+\n@@ -534,1 +819,1 @@\n-  void purge(bool free_code_cache_data, bool unregister_nmethod);\n+  void purge(bool free_code_cache_data, bool unregister_nmethod) override;\n@@ -559,1 +844,0 @@\n- public:\n@@ -601,0 +885,9 @@\n+  \/\/ ScopeDesc retrieval operation\n+  PcDesc* pc_desc_at(address pc)   { return find_pc_desc(pc, false); }\n+  \/\/ pc_desc_near returns the first PcDesc at or after the given pc.\n+  PcDesc* pc_desc_near(address pc) { return find_pc_desc(pc, true); }\n+\n+  \/\/ ScopeDesc for an instruction\n+  ScopeDesc* scope_desc_at(address pc);\n+  ScopeDesc* scope_desc_near(address pc);\n+\n@@ -614,1 +907,1 @@\n-  void verify();\n+  void verify() override;\n@@ -626,2 +919,2 @@\n-  void print()                          const;\n-  void print(outputStream* st)          const;\n+  void print()                 const override;\n+  void print(outputStream* st) const;\n@@ -636,1 +929,1 @@\n-  void print_value_on(outputStream* st) const;\n+  void print_value_on(outputStream* st) const override;\n@@ -656,1 +949,1 @@\n-  virtual void print_on(outputStream* st) const { CodeBlob::print_on(st); }\n+  void print_on(outputStream* st) const override { CodeBlob::print_on(st); }\n@@ -665,1 +958,1 @@\n-  virtual void print_block_comment(outputStream* stream, address block_begin) const {\n+  void print_block_comment(outputStream* stream, address block_begin) const override {\n@@ -680,7 +973,0 @@\n-  \/\/ Compiler task identification.  Note that all OSR methods\n-  \/\/ are numbered in an independent sequence if CICountOSR is true,\n-  \/\/ and native method wrappers are also numbered independently if\n-  \/\/ CICountNative is true.\n-  virtual int compile_id() const { return _compile_id; }\n-  const char* compile_kind() const;\n-\n@@ -694,1 +980,1 @@\n-  virtual bool is_dependent_on_method(Method* dependee);\n+  bool is_dependent_on_method(Method* dependee);\n@@ -709,1 +995,1 @@\n-  virtual void metadata_do(MetadataClosure* f);\n+  void metadata_do(MetadataClosure* f);\n@@ -713,1 +999,1 @@\n-  virtual void  make_deoptimized();\n+  void make_deoptimized();\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":500,"deletions":214,"binary":false,"changes":714,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,1 @@\n-class CompiledMethod;\n+class nmethod;\n@@ -108,1 +108,1 @@\n-  address real_pc(const CompiledMethod* code) const;\n+  address real_pc(const nmethod* code) const;\n@@ -110,3 +110,3 @@\n-  void print(CompiledMethod* code) { print_on(tty, code); }\n-  void print_on(outputStream* st, CompiledMethod* code);\n-  bool verify(CompiledMethod* code);\n+  void print(nmethod* code) { print_on(tty, code); }\n+  void print_on(outputStream* st, nmethod* code);\n+  bool verify(nmethod* code);\n","filename":"src\/hotspot\/share\/code\/pcDesc.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,1 @@\n-ScopeDesc::ScopeDesc(const CompiledMethod* code, PcDesc* pd, bool ignore_objects) {\n+ScopeDesc::ScopeDesc(const nmethod* code, PcDesc* pd, bool ignore_objects) {\n@@ -153,3 +153,3 @@\n-      \/\/ If select() returns nullptr, then the object doesn't need to be\n-      \/\/ rematerialized.\n-      if (sv == nullptr) {\n+      \/\/ 'select(...)' may return an ObjectValue that actually represents a\n+      \/\/ non-scalar replaced object participating in a merge.\n+      if (!sv->is_scalar_replaced()) {\n","filename":"src\/hotspot\/share\/code\/scopeDesc.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -44,1 +44,1 @@\n-  SimpleScopeDesc(CompiledMethod* code, address pc) {\n+  SimpleScopeDesc(nmethod* code, address pc) {\n@@ -64,1 +64,1 @@\n-  ScopeDesc(const CompiledMethod* code, PcDesc* pd, bool ignore_objects = false);\n+  ScopeDesc(const nmethod* code, PcDesc* pd, bool ignore_objects = false);\n@@ -124,1 +124,1 @@\n-  const CompiledMethod* _code;\n+  const nmethod* _code;\n","filename":"src\/hotspot\/share\/code\/scopeDesc.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1379,3 +1379,2 @@\n-    CompiledMethod* method_code = method->code();\n-    if (method_code != nullptr && method_code->is_nmethod()\n-                      && (compile_reason != CompileTask::Reason_DirectivesChanged)) {\n+    nmethod* method_code = method->code();\n+    if (method_code != nullptr && (compile_reason != CompileTask::Reason_DirectivesChanged)) {\n@@ -1383,1 +1382,1 @@\n-        return (nmethod*) method_code;\n+        return method_code;\n@@ -1484,6 +1483,1 @@\n-    CompiledMethod* code = method->code();\n-    if (code == nullptr) {\n-      return (nmethod*) code;\n-    } else {\n-      return code->as_nmethod_or_null();\n-    }\n+    return method->code();\n@@ -1514,1 +1508,1 @@\n-      CompiledMethod* result = method->code();\n+      nmethod* result = method->code();\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":5,"deletions":11,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -502,1 +502,0 @@\n-      \/\/DEBUG_ONLY(nof_callee++;)\n@@ -524,8 +523,0 @@\n-  DEBUG_ONLY(int nof_callee = 0;)\n-\n-  \/\/ Check that runtime stubs save all callee-saved registers\n-#ifdef COMPILER2\n-  assert(cb == nullptr || cb->is_compiled_by_c1() || cb->is_compiled_by_jvmci() || !cb->is_runtime_stub() ||\n-         (nof_callee >= SAVED_ON_ENTRY_REG_COUNT || nof_callee >= C_SAVED_ON_ENTRY_REG_COUNT),\n-         \"must save all\");\n-#endif \/\/ COMPILER2\n","filename":"src\/hotspot\/share\/compiler\/oopMap.cpp","additions":1,"deletions":10,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -917,6 +917,0 @@\n-  \/\/ Verify object start arrays\n-  if (VerifyObjectStartArray &&\n-      VerifyBeforeGC) {\n-    heap->old_gen()->verify_object_start_array();\n-  }\n-\n@@ -1418,1 +1412,1 @@\n-                              false \/* lock_codeblob_free_separately *\/);\n+                              false \/* lock_nmethod_free_separately *\/);\n@@ -1431,1 +1425,1 @@\n-    \/\/ adjust_roots() updates Universe::_intArrayKlassObj which is\n+    \/\/ adjust_roots() updates Universe::_intArrayKlass which is\n@@ -1539,6 +1533,0 @@\n-  \/\/ Re-verify object start arrays\n-  if (VerifyObjectStartArray &&\n-      VerifyAfterGC) {\n-    old_gen->verify_object_start_array();\n-  }\n-\n@@ -1576,1 +1564,1 @@\n-    MarkingCodeBlobClosure mark_and_push_in_blobs(&mark_and_push_closure, !CodeBlobToOopClosure::FixRelocations, true \/* keepalive nmethods *\/);\n+    MarkingNMethodClosure mark_and_push_in_blobs(&mark_and_push_closure, !NMethodToOopClosure::FixRelocations, true \/* keepalive nmethods *\/);\n@@ -1747,1 +1735,1 @@\n-      ctx->free_code_blobs();\n+      ctx->free_nmethods();\n@@ -1813,2 +1801,2 @@\n-      CodeBlobToOopClosure adjust_code(&adjust, CodeBlobToOopClosure::FixRelocations);\n-      CodeCache::blobs_do(&adjust_code);\n+      NMethodToOopClosure adjust_code(&adjust, NMethodToOopClosure::FixRelocations);\n+      CodeCache::nmethods_do(&adjust_code);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":6,"deletions":18,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -149,1 +149,0 @@\n-public:\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSet.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -488,2 +488,2 @@\n-  bool isLongArray = klass == Universe::longArrayKlassObj();\n-  bool isByteArray = klass == Universe::byteArrayKlassObj();\n+  bool isLongArray = klass == Universe::longArrayKlass();\n+  bool isByteArray = klass == Universe::byteArrayKlass();\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -639,1 +639,1 @@\n-        resolved_klass = TypeArrayKlass::cast(Universe::typeArrayKlassObj(ss.type()))->array_klass(ndim, CHECK_NULL);\n+        resolved_klass = Universe::typeArrayKlass(ss.type())->array_klass(ndim, CHECK_NULL);\n@@ -659,1 +659,1 @@\n-    array_klass = Universe::typeArrayKlassObj(type);\n+    array_klass = Universe::typeArrayKlass(type);\n@@ -1308,1 +1308,1 @@\n-  CompiledMethod* code = method->code();\n+  nmethod* code = method->code();\n@@ -1753,2 +1753,1 @@\n-    assert(fst.current()->cb()->is_nmethod(), \"nmethod expected\");\n-    ((nmethod*) fst.current()->cb())->make_not_entrant();\n+    fst.current()->cb()->as_nmethod()->make_not_entrant();\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -269,1 +269,1 @@\n-  volatile_nonstatic_field(Method,             _code,                                         CompiledMethod*)                       \\\n+  volatile_nonstatic_field(Method,             _code,                                         nmethod*)                              \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -107,0 +107,1 @@\n+  LOG_TAG(jmethod) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -90,8 +90,8 @@\n-    if (_klass == Universe::boolArrayKlassObj())         name = \"<boolArrayKlass>\";         else\n-    if (_klass == Universe::charArrayKlassObj())         name = \"<charArrayKlass>\";         else\n-    if (_klass == Universe::floatArrayKlassObj())        name = \"<floatArrayKlass>\";        else\n-    if (_klass == Universe::doubleArrayKlassObj())       name = \"<doubleArrayKlass>\";       else\n-    if (_klass == Universe::byteArrayKlassObj())         name = \"<byteArrayKlass>\";         else\n-    if (_klass == Universe::shortArrayKlassObj())        name = \"<shortArrayKlass>\";        else\n-    if (_klass == Universe::intArrayKlassObj())          name = \"<intArrayKlass>\";          else\n-    if (_klass == Universe::longArrayKlassObj())         name = \"<longArrayKlass>\";         else\n+    if (_klass == Universe::boolArrayKlass())         name = \"<boolArrayKlass>\";         else\n+    if (_klass == Universe::charArrayKlass())         name = \"<charArrayKlass>\";         else\n+    if (_klass == Universe::floatArrayKlass())        name = \"<floatArrayKlass>\";        else\n+    if (_klass == Universe::doubleArrayKlass())       name = \"<doubleArrayKlass>\";       else\n+    if (_klass == Universe::byteArrayKlass())         name = \"<byteArrayKlass>\";         else\n+    if (_klass == Universe::shortArrayKlass())        name = \"<shortArrayKlass>\";        else\n+    if (_klass == Universe::intArrayKlass())          name = \"<intArrayKlass>\";          else\n+    if (_klass == Universe::longArrayKlass())         name = \"<longArrayKlass>\";         else\n@@ -177,1 +177,1 @@\n-  _ref = (HeapWord*) Universe::boolArrayKlassObj();\n+  _ref = (uintptr_t) Universe::boolArrayKlass();\n@@ -203,1 +203,1 @@\n-  return (uint)(((uintptr_t)p - (uintptr_t)_ref) >> 2);\n+  return (uint)(((uintptr_t)p - _ref) >> 2);\n","filename":"src\/hotspot\/share\/memory\/heapInspection.cpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -111,3 +111,2 @@\n-  \/\/ address in the perm gen) used for hashing klass\n-  \/\/ objects.\n-  HeapWord* _ref;\n+  \/\/ address in the metaspace) used for hashing klasses.\n+  uintptr_t _ref;\n","filename":"src\/hotspot\/share\/memory\/heapInspection.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -242,1 +242,1 @@\n-\/\/ CodeBlobClosure is used for iterating through code blobs\n+\/\/ NMethodClosure is used for iterating through nmethods\n@@ -245,1 +245,1 @@\n-class CodeBlobClosure : public Closure {\n+class NMethodClosure : public Closure {\n@@ -247,2 +247,1 @@\n-  \/\/ Called for each code blob.\n-  virtual void do_code_blob(CodeBlob* cb) = 0;\n+  virtual void do_nmethod(nmethod* n) = 0;\n@@ -251,1 +250,1 @@\n-\/\/ Applies an oop closure to all ref fields in code blobs\n+\/\/ Applies an oop closure to all ref fields in nmethods\n@@ -253,1 +252,1 @@\n-class CodeBlobToOopClosure : public CodeBlobClosure {\n+class NMethodToOopClosure : public NMethodClosure {\n@@ -257,1 +256,0 @@\n-  void do_nmethod(nmethod* nm);\n@@ -261,2 +259,2 @@\n-  CodeBlobToOopClosure(OopClosure* cl, bool fix_relocations) : _cl(cl), _fix_relocations(fix_relocations) {}\n-  virtual void do_code_blob(CodeBlob* cb);\n+  NMethodToOopClosure(OopClosure* cl, bool fix_relocations) : _cl(cl), _fix_relocations(fix_relocations) {}\n+  void do_nmethod(nmethod* nm) override;\n@@ -268,1 +266,1 @@\n-class MarkingCodeBlobClosure : public CodeBlobToOopClosure {\n+class MarkingNMethodClosure : public NMethodToOopClosure {\n@@ -272,2 +270,2 @@\n-  MarkingCodeBlobClosure(OopClosure* cl, bool fix_relocations, bool keepalive_nmethods) :\n-      CodeBlobToOopClosure(cl, fix_relocations),\n+  MarkingNMethodClosure(OopClosure* cl, bool fix_relocations, bool keepalive_nmethods) :\n+      NMethodToOopClosure(cl, fix_relocations),\n@@ -276,16 +274,2 @@\n-  \/\/ Called for each code blob, but at most once per unique blob.\n-  virtual void do_code_blob(CodeBlob* cb);\n-};\n-\n-class NMethodClosure : public Closure {\n- public:\n-  virtual void do_nmethod(nmethod* n) = 0;\n-};\n-\n-class CodeBlobToNMethodClosure : public CodeBlobClosure {\n-  NMethodClosure* const _nm_cl;\n-\n- public:\n-  CodeBlobToNMethodClosure(NMethodClosure* nm_cl) : _nm_cl(nm_cl) {}\n-\n-  virtual void do_code_blob(CodeBlob* cb);\n+  \/\/ Called for each nmethod.\n+  virtual void do_nmethod(nmethod* nm);\n","filename":"src\/hotspot\/share\/memory\/iterator.hpp","additions":12,"deletions":28,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-  return TypeArrayKlass::cast(Universe::boolArrayKlassObj())->allocate(length, THREAD);\n+  return Universe::boolArrayKlass()->allocate(length, THREAD);\n@@ -52,1 +52,1 @@\n-  return TypeArrayKlass::cast(Universe::charArrayKlassObj())->allocate(length, THREAD);\n+  return Universe::charArrayKlass()->allocate(length, THREAD);\n@@ -56,1 +56,1 @@\n-  return TypeArrayKlass::cast(Universe::floatArrayKlassObj())->allocate(length, THREAD);\n+  return Universe::floatArrayKlass()->allocate(length, THREAD);\n@@ -60,1 +60,1 @@\n-  return TypeArrayKlass::cast(Universe::doubleArrayKlassObj())->allocate(length, THREAD);\n+  return Universe::doubleArrayKlass()->allocate(length, THREAD);\n@@ -64,1 +64,1 @@\n-  return TypeArrayKlass::cast(Universe::byteArrayKlassObj())->allocate(length, THREAD);\n+  return Universe::byteArrayKlass()->allocate(length, THREAD);\n@@ -68,1 +68,1 @@\n-  return TypeArrayKlass::cast(Universe::shortArrayKlassObj())->allocate(length, THREAD);\n+  return Universe::shortArrayKlass()->allocate(length, THREAD);\n@@ -72,1 +72,1 @@\n-  return TypeArrayKlass::cast(Universe::intArrayKlassObj())->allocate(length, THREAD);\n+  return Universe::intArrayKlass()->allocate(length, THREAD);\n@@ -76,1 +76,1 @@\n-  return TypeArrayKlass::cast(Universe::longArrayKlassObj())->allocate(length, THREAD);\n+  return Universe::longArrayKlass()->allocate(length, THREAD);\n@@ -81,2 +81,2 @@\n-  assert(Universe::objectArrayKlassObj() != nullptr, \"Too early?\");\n-  return ObjArrayKlass::cast(Universe::objectArrayKlassObj())->allocate(length, THREAD);\n+  assert(Universe::objectArrayKlass() != nullptr, \"Too early?\");\n+  return Universe::objectArrayKlass()->allocate(length, THREAD);\n@@ -95,4 +95,2 @@\n-  Klass* type_asKlassOop = Universe::typeArrayKlassObj(type);\n-  TypeArrayKlass* type_asArrayKlass = TypeArrayKlass::cast(type_asKlassOop);\n-  typeArrayOop result = type_asArrayKlass->allocate(length, THREAD);\n-  return result;\n+  TypeArrayKlass* klass = Universe::typeArrayKlass(type);\n+  return klass->allocate(length, THREAD);\n@@ -107,4 +105,1 @@\n-  Klass* type_asKlassOop = Universe::typeArrayKlassObj(type);\n-  TypeArrayKlass* type_asArrayKlass = TypeArrayKlass::cast(type_asKlassOop);\n-  typeArrayOop result = type_asArrayKlass->allocate(length, THREAD);\n-  return result;\n+  return new_typeArray(type, length, THREAD);\n@@ -114,4 +109,2 @@\n-  Klass* type_asKlassOop = Universe::typeArrayKlassObj(type);\n-  TypeArrayKlass* type_asArrayKlass = TypeArrayKlass::cast(type_asKlassOop);\n-  typeArrayOop result = type_asArrayKlass->allocate_common(length, false, THREAD);\n-  return result;\n+  TypeArrayKlass* klass = Universe::typeArrayKlass(type);\n+  return klass->allocate_common(length, false, THREAD);\n","filename":"src\/hotspot\/share\/memory\/oopFactory.cpp","additions":15,"deletions":22,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -116,3 +116,3 @@\n-Klass* Universe::_typeArrayKlassObjs[T_LONG+1]        = { nullptr \/*, nullptr...*\/ };\n-Klass* Universe::_objectArrayKlassObj                 = nullptr;\n-Klass* Universe::_fillerArrayKlassObj                 = nullptr;\n+TypeArrayKlass* Universe::_typeArrayKlasses[T_LONG+1] = { nullptr \/*, nullptr...*\/ };\n+ObjArrayKlass* Universe::_objectArrayKlass            = nullptr;\n+Klass* Universe::_fillerArrayKlass                    = nullptr;\n@@ -270,1 +270,1 @@\n-    closure->do_klass(_typeArrayKlassObjs[i]);\n+    closure->do_klass(_typeArrayKlasses[i]);\n@@ -273,1 +273,1 @@\n-  \/\/ _fillerArrayKlassObj is used only by GC, which doesn't need to see\n+  \/\/ _fillerArrayKlass is used only by GC, which doesn't need to see\n@@ -276,1 +276,1 @@\n-  \/\/ closure->do_klass(_fillerArrayKlassObj);\n+  \/\/ closure->do_klass(_fillerArrayKlass);\n@@ -280,1 +280,1 @@\n-  it->push(&_fillerArrayKlassObj);\n+  it->push(&_fillerArrayKlass);\n@@ -282,1 +282,1 @@\n-    it->push(&_typeArrayKlassObjs[i]);\n+    it->push(&_typeArrayKlasses[i]);\n@@ -284,1 +284,1 @@\n-  it->push(&_objectArrayKlassObj);\n+  it->push(&_objectArrayKlass);\n@@ -339,1 +339,1 @@\n-  f->do_ptr(&_fillerArrayKlassObj);\n+  f->do_ptr(&_fillerArrayKlass);\n@@ -341,1 +341,1 @@\n-    f->do_ptr(&_typeArrayKlassObjs[i]);\n+    f->do_ptr(&_typeArrayKlasses[i]);\n@@ -344,1 +344,1 @@\n-  f->do_ptr(&_objectArrayKlassObj);\n+  f->do_ptr(&_objectArrayKlass);\n@@ -401,1 +401,1 @@\n-      _fillerArrayKlassObj = TypeArrayKlass::create_klass(T_INT, \"[Ljdk\/internal\/vm\/FillerElement;\", CHECK);\n+      _fillerArrayKlass = TypeArrayKlass::create_klass(T_INT, \"[Ljdk\/internal\/vm\/FillerElement;\", CHECK);\n@@ -403,1 +403,1 @@\n-        _typeArrayKlassObjs[i] = TypeArrayKlass::create_klass((BasicType)i, CHECK);\n+        _typeArrayKlasses[i] = TypeArrayKlass::create_klass((BasicType)i, CHECK);\n@@ -443,1 +443,1 @@\n-    initialize_basic_type_klass(_fillerArrayKlassObj, CHECK);\n+    initialize_basic_type_klass(_fillerArrayKlass, CHECK);\n@@ -445,8 +445,8 @@\n-    initialize_basic_type_klass(boolArrayKlassObj(), CHECK);\n-    initialize_basic_type_klass(charArrayKlassObj(), CHECK);\n-    initialize_basic_type_klass(floatArrayKlassObj(), CHECK);\n-    initialize_basic_type_klass(doubleArrayKlassObj(), CHECK);\n-    initialize_basic_type_klass(byteArrayKlassObj(), CHECK);\n-    initialize_basic_type_klass(shortArrayKlassObj(), CHECK);\n-    initialize_basic_type_klass(intArrayKlassObj(), CHECK);\n-    initialize_basic_type_klass(longArrayKlassObj(), CHECK);\n+    initialize_basic_type_klass(boolArrayKlass(), CHECK);\n+    initialize_basic_type_klass(charArrayKlass(), CHECK);\n+    initialize_basic_type_klass(floatArrayKlass(), CHECK);\n+    initialize_basic_type_klass(doubleArrayKlass(), CHECK);\n+    initialize_basic_type_klass(byteArrayKlass(), CHECK);\n+    initialize_basic_type_klass(shortArrayKlass(), CHECK);\n+    initialize_basic_type_klass(intArrayKlass(), CHECK);\n+    initialize_basic_type_klass(longArrayKlass(), CHECK);\n@@ -454,1 +454,1 @@\n-    assert(_fillerArrayKlassObj != intArrayKlassObj(),\n+    assert(_fillerArrayKlass != intArrayKlass(),\n@@ -478,2 +478,4 @@\n-  _objectArrayKlassObj = InstanceKlass::\n-    cast(vmClasses::Object_klass())->array_klass(1, CHECK);\n+  {\n+    Klass* oak = vmClasses::Object_klass()->array_klass(CHECK);\n+    _objectArrayKlass = ObjArrayKlass::cast(oak);\n+  }\n@@ -486,1 +488,1 @@\n-  _objectArrayKlassObj->append_to_sibling_list();\n+  _objectArrayKlass->append_to_sibling_list();\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":29,"deletions":27,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-  friend class MarkSweep;\n+  friend class SerialFullGC;\n@@ -68,2 +68,2 @@\n-  static Klass* _typeArrayKlassObjs[T_LONG+1];\n-  static Klass* _objectArrayKlassObj;\n+  static TypeArrayKlass* _typeArrayKlasses[T_LONG+1];\n+  static ObjArrayKlass* _objectArrayKlass;\n@@ -72,1 +72,1 @@\n-  static Klass* _fillerArrayKlassObj;\n+  static Klass* _fillerArrayKlass;\n@@ -178,8 +178,8 @@\n-  static Klass* boolArrayKlassObj()                 { return typeArrayKlassObj(T_BOOLEAN); }\n-  static Klass* byteArrayKlassObj()                 { return typeArrayKlassObj(T_BYTE); }\n-  static Klass* charArrayKlassObj()                 { return typeArrayKlassObj(T_CHAR); }\n-  static Klass* intArrayKlassObj()                  { return typeArrayKlassObj(T_INT); }\n-  static Klass* shortArrayKlassObj()                { return typeArrayKlassObj(T_SHORT); }\n-  static Klass* longArrayKlassObj()                 { return typeArrayKlassObj(T_LONG); }\n-  static Klass* floatArrayKlassObj()                { return typeArrayKlassObj(T_FLOAT); }\n-  static Klass* doubleArrayKlassObj()               { return typeArrayKlassObj(T_DOUBLE); }\n+  static TypeArrayKlass* boolArrayKlass()        { return typeArrayKlass(T_BOOLEAN); }\n+  static TypeArrayKlass* byteArrayKlass()        { return typeArrayKlass(T_BYTE); }\n+  static TypeArrayKlass* charArrayKlass()        { return typeArrayKlass(T_CHAR); }\n+  static TypeArrayKlass* intArrayKlass()         { return typeArrayKlass(T_INT); }\n+  static TypeArrayKlass* shortArrayKlass()       { return typeArrayKlass(T_SHORT); }\n+  static TypeArrayKlass* longArrayKlass()        { return typeArrayKlass(T_LONG); }\n+  static TypeArrayKlass* floatArrayKlass()       { return typeArrayKlass(T_FLOAT); }\n+  static TypeArrayKlass* doubleArrayKlass()      { return typeArrayKlass(T_DOUBLE); }\n@@ -187,1 +187,1 @@\n-  static Klass* objectArrayKlassObj()               { return _objectArrayKlassObj; }\n+  static ObjArrayKlass* objectArrayKlass()       { return _objectArrayKlass; }\n@@ -189,1 +189,1 @@\n-  static Klass* fillerArrayKlassObj()               { return _fillerArrayKlassObj; }\n+  static Klass* fillerArrayKlass()               { return _fillerArrayKlass; }\n@@ -191,1 +191,1 @@\n-  static Klass* typeArrayKlassObj(BasicType t) {\n+  static TypeArrayKlass* typeArrayKlass(BasicType t) {\n@@ -194,2 +194,2 @@\n-    assert(_typeArrayKlassObjs[t] != nullptr, \"domain check\");\n-    return _typeArrayKlassObjs[t];\n+    assert(_typeArrayKlasses[t] != nullptr, \"domain check\");\n+    return _typeArrayKlasses[t];\n","filename":"src\/hotspot\/share\/memory\/universe.hpp","additions":17,"deletions":17,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -2502,0 +2502,11 @@\n+jmethodID InstanceKlass::update_jmethod_id(jmethodID* jmeths, Method* method, int idnum) {\n+  if (method->is_old() && !method->is_obsolete()) {\n+    \/\/ If the method passed in is old (but not obsolete), use the current version.\n+    method = method_with_idnum((int)idnum);\n+    assert(method != nullptr, \"old and but not obsolete, so should exist\");\n+  }\n+  jmethodID new_id = Method::make_jmethod_id(class_loader_data(), method);\n+  Atomic::release_store(&jmeths[idnum + 1], new_id);\n+  return new_id;\n+}\n+\n@@ -2508,1 +2519,2 @@\n-  size_t idnum = (size_t)method_h->method_idnum();\n+  Method* method = method_h();\n+  int idnum = method->method_idnum();\n@@ -2510,2 +2522,0 @@\n-  size_t length = 0;\n-  jmethodID id = nullptr;\n@@ -2523,1 +2533,1 @@\n-  \/\/ generally acquired in those two cases.\n+  \/\/ acquired in those two cases.\n@@ -2525,38 +2535,12 @@\n-  \/\/ If the RedefineClasses() API has been used, then this cache can\n-  \/\/ grow and we'll have transitions from non-null to bigger non-null.\n-  \/\/ Cache creation requires no leaks and we require safety between all\n-  \/\/ cache accesses and freeing of the old cache so a lock is generally\n-  \/\/ acquired when the RedefineClasses() API has been used.\n-\n-  if (jmeths != nullptr) {\n-    \/\/ the cache already exists\n-    if (!idnum_can_increment()) {\n-      \/\/ the cache can't grow so we can just get the current values\n-      get_jmethod_id_length_value(jmeths, idnum, &length, &id);\n-    } else {\n-      MutexLocker ml(JmethodIdCreation_lock, Mutex::_no_safepoint_check_flag);\n-      get_jmethod_id_length_value(jmeths, idnum, &length, &id);\n-    }\n-  }\n-  \/\/ implied else:\n-  \/\/ we need to allocate a cache so default length and id values are good\n-\n-  if (jmeths == nullptr ||   \/\/ no cache yet\n-      length <= idnum ||     \/\/ cache is too short\n-      id == nullptr) {       \/\/ cache doesn't contain entry\n-\n-    \/\/ This function can be called by the VMThread or GC worker threads so we\n-    \/\/ have to do all things that might block on a safepoint before grabbing the lock.\n-    \/\/ Otherwise, we can deadlock with the VMThread or have a cache\n-    \/\/ consistency issue. These vars keep track of what we might have\n-    \/\/ to free after the lock is dropped.\n-    jmethodID  to_dealloc_id     = nullptr;\n-    jmethodID* to_dealloc_jmeths = nullptr;\n-\n-    \/\/ may not allocate new_jmeths or use it if we allocate it\n-    jmethodID* new_jmeths = nullptr;\n-    if (length <= idnum) {\n-      \/\/ allocate a new cache that might be used\n-      size_t size = MAX2(idnum+1, (size_t)idnum_allocated_count());\n-      new_jmeths = NEW_C_HEAP_ARRAY(jmethodID, size+1, mtClass);\n-      memset(new_jmeths, 0, (size+1)*sizeof(jmethodID));\n+  \/\/ If the RedefineClasses() API has been used, then this cache grows\n+  \/\/ in the redefinition safepoint.\n+\n+  if (jmeths == nullptr) {\n+    MutexLocker ml(JmethodIdCreation_lock, Mutex::_no_safepoint_check_flag);\n+    jmeths = methods_jmethod_ids_acquire();\n+    \/\/ Still null?\n+    if (jmeths == nullptr) {\n+      size_t size = idnum_allocated_count();\n+      assert(size > (size_t)idnum, \"should already have space\");\n+      jmeths = NEW_C_HEAP_ARRAY(jmethodID, size + 1, mtClass);\n+      memset(jmeths, 0, (size + 1) * sizeof(jmethodID));\n@@ -2564,17 +2548,2 @@\n-      new_jmeths[0] = (jmethodID)size;\n-    }\n-\n-    \/\/ allocate a new jmethodID that might be used\n-    {\n-      MutexLocker ml(JmethodIdCreation_lock, Mutex::_no_safepoint_check_flag);\n-      jmethodID new_id = nullptr;\n-      if (method_h->is_old() && !method_h->is_obsolete()) {\n-        \/\/ The method passed in is old (but not obsolete), we need to use the current version\n-        Method* current_method = method_with_idnum((int)idnum);\n-        assert(current_method != nullptr, \"old and but not obsolete, so should exist\");\n-        new_id = Method::make_jmethod_id(class_loader_data(), current_method);\n-      } else {\n-        \/\/ It is the current version of the method or an obsolete method,\n-        \/\/ use the version passed in\n-        new_id = Method::make_jmethod_id(class_loader_data(), method_h());\n-      }\n+      jmeths[0] = (jmethodID)size;\n+      jmethodID new_id = update_jmethod_id(jmeths, method, idnum);\n@@ -2582,2 +2551,3 @@\n-      id = get_jmethod_id_fetch_or_update(idnum, new_id, new_jmeths,\n-                                          &to_dealloc_id, &to_dealloc_jmeths);\n+      \/\/ publish jmeths\n+      release_set_methods_jmethod_ids(jmeths);\n+      return new_id;\n@@ -2585,0 +2555,1 @@\n+  }\n@@ -2586,8 +2557,7 @@\n-    \/\/ The lock has been dropped so we can free resources.\n-    \/\/ Free up either the old cache or the new cache if we allocated one.\n-    if (to_dealloc_jmeths != nullptr) {\n-      FreeHeap(to_dealloc_jmeths);\n-    }\n-    \/\/ free up the new ID since it wasn't needed\n-    if (to_dealloc_id != nullptr) {\n-      Method::destroy_jmethod_id(class_loader_data(), to_dealloc_id);\n+  jmethodID id = Atomic::load_acquire(&jmeths[idnum + 1]);\n+  if (id == nullptr) {\n+    MutexLocker ml(JmethodIdCreation_lock, Mutex::_no_safepoint_check_flag);\n+    id = jmeths[idnum + 1];\n+    \/\/ Still null?\n+    if (id == nullptr) {\n+      return update_jmethod_id(jmeths, method, idnum);\n@@ -2599,0 +2569,23 @@\n+void InstanceKlass::update_methods_jmethod_cache() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"only called at safepoint\");\n+  jmethodID* cache = _methods_jmethod_ids;\n+  if (cache != nullptr) {\n+    size_t size = idnum_allocated_count();\n+    size_t old_size = (size_t)cache[0];\n+    if (old_size < size + 1) {\n+      \/\/ Allocate a larger one and copy entries to the new one.\n+      \/\/ They've already been updated to point to new methods where applicable (i.e., not obsolete).\n+      jmethodID* new_cache = NEW_C_HEAP_ARRAY(jmethodID, size + 1, mtClass);\n+      memset(new_cache, 0, (size + 1) * sizeof(jmethodID));\n+      \/\/ The cache size is stored in element[0]; the other elements are offset by one.\n+      new_cache[0] = (jmethodID)size;\n+\n+      for (int i = 1; i <= (int)old_size; i++) {\n+        new_cache[i] = cache[i];\n+      }\n+      _methods_jmethod_ids = new_cache;\n+      FREE_C_HEAP_ARRAY(jmethodID, cache);\n+    }\n+  }\n+}\n+\n@@ -2619,71 +2612,0 @@\n-\/\/ Common code to fetch the jmethodID from the cache or update the\n-\/\/ cache with the new jmethodID. This function should never do anything\n-\/\/ that causes the caller to go to a safepoint or we can deadlock with\n-\/\/ the VMThread or have cache consistency issues.\n-\/\/\n-jmethodID InstanceKlass::get_jmethod_id_fetch_or_update(\n-            size_t idnum, jmethodID new_id,\n-            jmethodID* new_jmeths, jmethodID* to_dealloc_id_p,\n-            jmethodID** to_dealloc_jmeths_p) {\n-  assert(new_id != nullptr, \"sanity check\");\n-  assert(to_dealloc_id_p != nullptr, \"sanity check\");\n-  assert(to_dealloc_jmeths_p != nullptr, \"sanity check\");\n-  assert(JmethodIdCreation_lock->owned_by_self(), \"sanity check\");\n-\n-  \/\/ reacquire the cache - we are locked, single threaded or at a safepoint\n-  jmethodID* jmeths = methods_jmethod_ids_acquire();\n-  jmethodID  id     = nullptr;\n-  size_t     length = 0;\n-\n-  if (jmeths == nullptr ||                      \/\/ no cache yet\n-      (length = (size_t)jmeths[0]) <= idnum) {  \/\/ cache is too short\n-    if (jmeths != nullptr) {\n-      \/\/ copy any existing entries from the old cache\n-      for (size_t index = 0; index < length; index++) {\n-        new_jmeths[index+1] = jmeths[index+1];\n-      }\n-      *to_dealloc_jmeths_p = jmeths;  \/\/ save old cache for later delete\n-    }\n-    release_set_methods_jmethod_ids(jmeths = new_jmeths);\n-  } else {\n-    \/\/ fetch jmethodID (if any) from the existing cache\n-    id = jmeths[idnum+1];\n-    *to_dealloc_jmeths_p = new_jmeths;  \/\/ save new cache for later delete\n-  }\n-  if (id == nullptr) {\n-    \/\/ No matching jmethodID in the existing cache or we have a new\n-    \/\/ cache or we just grew the cache. This cache write is done here\n-    \/\/ by the first thread to win the foot race because a jmethodID\n-    \/\/ needs to be unique once it is generally available.\n-    id = new_id;\n-\n-    \/\/ The jmethodID cache can be read while unlocked so we have to\n-    \/\/ make sure the new jmethodID is complete before installing it\n-    \/\/ in the cache.\n-    Atomic::release_store(&jmeths[idnum+1], id);\n-  } else {\n-    *to_dealloc_id_p = new_id; \/\/ save new id for later delete\n-  }\n-  return id;\n-}\n-\n-\n-\/\/ Common code to get the jmethodID cache length and the jmethodID\n-\/\/ value at index idnum if there is one.\n-\/\/\n-void InstanceKlass::get_jmethod_id_length_value(jmethodID* cache,\n-       size_t idnum, size_t *length_p, jmethodID* id_p) {\n-  assert(cache != nullptr, \"sanity check\");\n-  assert(length_p != nullptr, \"sanity check\");\n-  assert(id_p != nullptr, \"sanity check\");\n-\n-  \/\/ cache size is stored in element[0], other elements offset by one\n-  *length_p = (size_t)cache[0];\n-  if (*length_p <= idnum) {  \/\/ cache is too short\n-    *id_p = nullptr;\n-  } else {\n-    *id_p = cache[idnum+1];  \/\/ fetch jmethodID (if any)\n-  }\n-}\n-\n-\n@@ -2692,1 +2614,1 @@\n-  size_t idnum = (size_t)method->method_idnum();\n+  int idnum = method->method_idnum();\n@@ -2694,7 +2616,1 @@\n-  size_t length;                                \/\/ length assigned as debugging crumb\n-  jmethodID id = nullptr;\n-  if (jmeths != nullptr &&                      \/\/ If there is a cache\n-      (length = (size_t)jmeths[0]) > idnum) {   \/\/ and if it is long enough,\n-    id = jmeths[idnum+1];                       \/\/ Look up the id (may be null)\n-  }\n-  return id;\n+  return (jmeths != nullptr) ? jmeths[idnum + 1] : nullptr;\n@@ -3127,1 +3043,1 @@\n-  if (jmeths != (jmethodID*)nullptr) {\n+  if (jmeths != nullptr) {\n@@ -3684,1 +3600,1 @@\n-  assert_lock_strong(CompiledMethod_lock);\n+  assert_lock_strong(NMethodState_lock);\n@@ -3709,1 +3625,1 @@\n-  ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n+  ConditionalMutexLocker ml(NMethodState_lock, !NMethodState_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n@@ -3750,1 +3666,1 @@\n-  ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n+  ConditionalMutexLocker ml(NMethodState_lock, !NMethodState_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n@@ -3765,1 +3681,1 @@\n-  ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n+  ConditionalMutexLocker ml(NMethodState_lock, !NMethodState_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":69,"deletions":153,"binary":false,"changes":222,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -262,3 +262,3 @@\n-  JNIid*          _jni_ids;              \/\/ First JNI identifier for static fields in this class\n-  jmethodID*      volatile _methods_jmethod_ids;  \/\/ jmethodIDs corresponding to method_idnum, or null if none\n-  nmethodBucket*  volatile _dep_context;          \/\/ packed DependencyContext structure\n+  JNIid*          _jni_ids;                  \/\/ First JNI identifier for static fields in this class\n+  jmethodID* volatile _methods_jmethod_ids;  \/\/ jmethodIDs corresponding to method_idnum, or null if none\n+  nmethodBucket*  volatile _dep_context;     \/\/ packed DependencyContext structure\n@@ -858,6 +858,0 @@\n-  jmethodID get_jmethod_id_fetch_or_update(size_t idnum,\n-                     jmethodID new_id, jmethodID* new_jmeths,\n-                     jmethodID* to_dealloc_id_p,\n-                     jmethodID** to_dealloc_jmeths_p);\n-  static void get_jmethod_id_length_value(jmethodID* cache, size_t idnum,\n-                size_t *length_p, jmethodID* id_p);\n@@ -866,0 +860,1 @@\n+  void update_methods_jmethod_cache();\n@@ -1157,5 +1152,0 @@\n-  \/\/ The RedefineClasses() API can cause new method idnums to be needed\n-  \/\/ which will cause the caches to grow. Safety requires different\n-  \/\/ cache management logic if the caches can grow instead of just\n-  \/\/ going from null to non-null.\n-  bool idnum_can_increment() const      { return has_been_redefined(); }\n@@ -1166,0 +1156,1 @@\n+  jmethodID update_jmethod_id(jmethodID* jmeths, Method* method, int idnum);\n@@ -1167,1 +1158,0 @@\n-  \/\/ Lock during initialization\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":6,"deletions":16,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -254,1 +254,1 @@\n-  bool is_neutral()  const {\n+  bool is_neutral()  const {  \/\/ Not locked, or marked - a \"clean\" neutral state\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1047,1 +1047,1 @@\n-  CompiledMethod* nm = code(); \/\/ Put it into local variable to guard against concurrent updates\n+  nmethod* nm = code(); \/\/ Put it into local variable to guard against concurrent updates\n@@ -1199,2 +1199,2 @@\n-void Method::unlink_code(CompiledMethod *compare) {\n-  ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n+void Method::unlink_code(nmethod *compare) {\n+  ConditionalMutexLocker ml(NMethodState_lock, !NMethodState_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n@@ -1211,1 +1211,1 @@\n-  ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n+  ConditionalMutexLocker ml(NMethodState_lock, !NMethodState_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n@@ -1364,1 +1364,1 @@\n-  CompiledMethod *code = Atomic::load_acquire(&_code);\n+  nmethod *code = Atomic::load_acquire(&_code);\n@@ -1369,2 +1369,2 @@\n-void Method::set_code(const methodHandle& mh, CompiledMethod *code) {\n-  assert_lock_strong(CompiledMethod_lock);\n+void Method::set_code(const methodHandle& mh, nmethod *code) {\n+  assert_lock_strong(NMethodState_lock);\n@@ -2206,8 +2206,0 @@\n-  \/\/ Doesn't really destroy it, just marks it as free so it can be reused.\n-  void destroy_method(Method** m) {\n-#ifdef ASSERT\n-    assert(contains(m), \"should be a methodID\");\n-#endif \/\/ ASSERT\n-    *m = _free_method;\n-  }\n-\n@@ -2266,0 +2258,3 @@\n+\n+  ResourceMark rm;\n+  log_debug(jmethod)(\"Creating jmethodID for Method %s\", m->external_name());\n@@ -2278,8 +2273,0 @@\n-\/\/ Mark a jmethodID as free.  This is called when there is a data race in\n-\/\/ InstanceKlass while creating the jmethodID cache.\n-void Method::destroy_jmethod_id(ClassLoaderData* cld, jmethodID m) {\n-  Method** ptr = (Method**)m;\n-  assert(cld->jmethod_ids() != nullptr, \"should have method handles\");\n-  cld->jmethod_ids()->destroy_method(ptr);\n-}\n-\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":10,"deletions":23,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -65,1 +65,1 @@\n-class CompiledMethod;\n+class nmethod;\n@@ -96,1 +96,1 @@\n-  volatile address _from_compiled_entry;           \/\/ Cache of: _code ? _code->verified_entry_point()           : _adapter->c2i_entry()\n+  volatile address _from_compiled_entry;           \/\/ Cache of: _code ? _code->entry_point() : _adapter->c2i_entry()\n@@ -104,2 +104,2 @@\n-  CompiledMethod* volatile _code;                       \/\/ Points to the corresponding piece of native code\n-  volatile address           _from_interpreted_entry; \/\/ Cache of _code ? _adapter->i2c_entry() : _i2i_entry\n+  nmethod* volatile _code;                   \/\/ Points to the corresponding piece of native code\n+  volatile address  _from_interpreted_entry; \/\/ Cache of _code ? _adapter->i2c_entry() : _i2i_entry\n@@ -366,1 +366,1 @@\n-  CompiledMethod* code() const;\n+  nmethod* code() const;\n@@ -368,3 +368,3 @@\n-  \/\/ Locks CompiledMethod_lock if not held.\n-  void unlink_code(CompiledMethod *compare);\n-  \/\/ Locks CompiledMethod_lock if not held.\n+  \/\/ Locks NMethodState_lock if not held.\n+  void unlink_code(nmethod *compare);\n+  \/\/ Locks NMethodState_lock if not held.\n@@ -374,1 +374,1 @@\n-  \/\/ Either called with CompiledMethod_lock held or from constructor.\n+  \/\/ Either called with NMethodState_lock held or from constructor.\n@@ -382,1 +382,1 @@\n-  static void set_code(const methodHandle& mh, CompiledMethod* code);\n+  static void set_code(const methodHandle& mh, nmethod* code);\n@@ -714,1 +714,0 @@\n-  static void destroy_jmethod_id(ClassLoaderData* cld, jmethodID mid);\n","filename":"src\/hotspot\/share\/oops\/method.hpp","additions":10,"deletions":11,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -50,1 +50,1 @@\n-inline CompiledMethod* Method::code() const {\n+inline nmethod* Method::code() const {\n","filename":"src\/hotspot\/share\/oops\/method.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -170,10 +170,0 @@\n-void* oopDesc::load_klass_raw(oop obj) {\n-  if (UseCompressedClassPointers) {\n-    narrowKlass narrow_klass = obj->_metadata._compressed_klass;\n-    if (narrow_klass == 0) return nullptr;\n-    return (void*)CompressedKlassPointers::decode_raw(narrow_klass);\n-  } else {\n-    return obj->_metadata._klass;\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -98,2 +98,2 @@\n-  \/\/ Get the raw value without any checks.\n-  inline Klass* klass_raw() const;\n+  \/\/ Get the klass without running any asserts.\n+  inline Klass* klass_without_asserts() const;\n@@ -334,1 +334,0 @@\n-  static void* load_klass_raw(oop obj);\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -114,1 +114,1 @@\n-Klass* oopDesc::klass_raw() const {\n+Klass* oopDesc::klass_without_asserts() const {\n@@ -116,1 +116,1 @@\n-    return CompressedKlassPointers::decode_raw(_metadata._compressed_klass);\n+    return CompressedKlassPointers::decode_without_asserts(_metadata._compressed_klass);\n@@ -274,1 +274,0 @@\n-\/\/ Used only for markSweep, scavenging\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -183,0 +183,1 @@\n+    init_class_id(Class_CastPP);\n","filename":"src\/hotspot\/share\/opto\/castnode.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -5545,10 +5545,1 @@\n-#ifdef ASSERT\n-          if (!in_hash) {\n-            tty->print_cr(\"current graph:\");\n-            n->dump_bfs(MaxNodeLimit, nullptr, \"S$\");\n-            tty->cr();\n-            tty->print_cr(\"erroneous node:\");\n-            n->dump();\n-            assert(false, \"node should be in igvn hash table\");\n-          }\n-#endif\n+          assert(in_hash || n->hash() == Node::NO_HASH, \"node should be in igvn hash table\");\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":1,"deletions":10,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -44,0 +44,2 @@\n+#include \"opto\/narrowptrnode.hpp\"\n+#include \"opto\/castnode.hpp\"\n@@ -424,8 +426,11 @@\n-  \/\/ 6. Remove reducible allocation merges from ideal graph\n-  if (reducible_merges.size() > 0) {\n-    bool delay = _igvn->delay_transform();\n-    _igvn->set_delay_transform(true);\n-    for (uint i = 0; i < reducible_merges.size(); i++ ) {\n-      Node* n = reducible_merges.at(i);\n-      reduce_phi(n->as_Phi());\n-      if (C->failing()) {\n+  \/\/ 6. Reduce allocation merges used as debug information. This is done after\n+  \/\/ split_unique_types because the methods used to create SafePointScalarObject\n+  \/\/ need to traverse the memory graph to find values for object fields. We also\n+  \/\/ set to null the scalarized inputs of reducible Phis so that the Allocate\n+  \/\/ that they point can be later scalar replaced.\n+  bool delay = _igvn->delay_transform();\n+  _igvn->set_delay_transform(true);\n+  for (uint i = 0; i < reducible_merges.size(); i++) {\n+    Node* n = reducible_merges.at(i);\n+    if (n->outcnt() > 0) {\n+      if (!reduce_phi_on_safepoints(n->as_Phi())) {\n@@ -433,0 +438,1 @@\n+        C->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n@@ -435,0 +441,4 @@\n+\n+      \/\/ Now we set the scalar replaceable inputs of ophi to null, which is\n+      \/\/ the last piece that would prevent it from being scalar replaceable.\n+      reset_scalar_replaceable_entries(n->as_Phi());\n@@ -436,1 +446,1 @@\n-    _igvn->set_delay_transform(delay);\n+  _igvn->set_delay_transform(delay);\n@@ -462,2 +472,1 @@\n-\/\/ if at least one scalar replaceable allocation participates in the merge and\n-\/\/ no input to the Phi is nullable.\n+\/\/ if at least one scalar replaceable allocation participates in the merge.\n@@ -465,1 +474,0 @@\n-  \/\/ Check if there is a scalar replaceable allocate in the Phi\n@@ -469,8 +477,0 @@\n-    \/\/ Right now we can't restore a \"null\" pointer during deoptimization\n-    const Type* inp_t = _igvn->type(ophi->in(i));\n-    if (inp_t == nullptr || inp_t->make_oopptr() == nullptr || inp_t->make_oopptr()->maybe_null()) {\n-      NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. Input %d is nullable.\", ophi->_idx, _invocation, i);)\n-      return false;\n-    }\n-\n-    \/\/ We are looking for at least one SR object in the merge\n@@ -479,1 +479,0 @@\n-      assert(ptn->ideal_node() != nullptr && ptn->ideal_node()->is_Allocate(), \"sanity\");\n@@ -482,0 +481,6 @@\n+      \/\/ Don't handle arrays.\n+      if (alloc->Opcode() != Op_Allocate) {\n+        assert(alloc->Opcode() == Op_AllocateArray, \"Unexpected type of allocation.\");\n+        continue;\n+      }\n+\n@@ -485,0 +490,1 @@\n+        NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"%dth input of Phi %d is SR but can't be eliminated.\", i, ophi->_idx);)\n@@ -494,5 +500,42 @@\n-\/\/ Check if we are able to untangle the merge. Right now we only reduce Phis\n-\/\/ which are only used as debug information.\n-bool ConnectionGraph::can_reduce_phi_check_users(PhiNode* ophi) const {\n-  for (DUIterator_Fast imax, i = ophi->fast_outs(imax); i < imax; i++) {\n-    Node* use = ophi->fast_out(i);\n+\/\/ We can reduce the Cmp if it's a comparison between the Phi and a constant.\n+\/\/ I require the 'other' input to be a constant so that I can move the Cmp\n+\/\/ around safely.\n+bool ConnectionGraph::can_reduce_cmp(Node* n, Node* cmp) const {\n+  Node* left = cmp->in(1);\n+  Node* right = cmp->in(2);\n+\n+  return (cmp->Opcode() == Op_CmpP || cmp->Opcode() == Op_CmpN) &&\n+         (left == n || right == n) &&\n+         (left->is_Con() || right->is_Con()) &&\n+         cmp->outcnt() == 1;\n+}\n+\n+\/\/ We are going to check if any of the SafePointScalarMerge entries\n+\/\/ in the SafePoint reference the Phi that we are checking.\n+bool ConnectionGraph::has_been_reduced(PhiNode* n, SafePointNode* sfpt) const {\n+  JVMState *jvms = sfpt->jvms();\n+\n+  for (uint i = jvms->debug_start(); i < jvms->debug_end(); i++) {\n+    Node* sfpt_in = sfpt->in(i);\n+    if (sfpt_in->is_SafePointScalarMerge()) {\n+      SafePointScalarMergeNode* smerge = sfpt_in->as_SafePointScalarMerge();\n+      Node* nsr_ptr = sfpt->in(smerge->merge_pointer_idx(jvms));\n+      if (nsr_ptr == n) {\n+        return true;\n+      }\n+    }\n+  }\n+\n+  return false;\n+}\n+\n+\/\/ Check if we are able to untangle the merge. The following patterns are\n+\/\/ supported:\n+\/\/  - Phi -> SafePoints\n+\/\/  - Phi -> CmpP\/N\n+\/\/  - Phi -> AddP -> Load\n+\/\/  - Phi -> CastPP -> SafePoints\n+\/\/  - Phi -> CastPP -> AddP -> Load\n+bool ConnectionGraph::can_reduce_check_users(Node* n, uint nesting) const {\n+  for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+    Node* use = n->fast_out(i);\n@@ -501,2 +544,5 @@\n-      if (use->is_Call() && use->as_Call()->has_non_debug_use(ophi)) {\n-        NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. Call has non_debug_use().\", ophi->_idx, _invocation);)\n+      if (use->is_Call() && use->as_Call()->has_non_debug_use(n)) {\n+        NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. Call has non_debug_use().\", n->_idx, _invocation);)\n+        return false;\n+      } else if (has_been_reduced(n->is_Phi() ? n->as_Phi() : n->as_CastPP()->in(1)->as_Phi(), use->as_SafePoint())) {\n+        NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. It has already been reduced.\", n->_idx, _invocation);)\n@@ -509,0 +555,2 @@\n+        const Type* load_type = _igvn->type(use_use);\n+\n@@ -510,1 +558,4 @@\n-          NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. AddP user isn't a [splittable] Load(): %s\", ophi->_idx, _invocation, use_use->Name());)\n+          NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. AddP user isn't a [splittable] Load(): %s\", n->_idx, _invocation, use_use->Name());)\n+          return false;\n+        } else if (nesting > 0 && load_type->isa_narrowklass()) {\n+          NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. Nested NarrowKlass Load: %s\", n->_idx, _invocation, use_use->Name());)\n@@ -514,0 +565,35 @@\n+    } else if (nesting > 0) {\n+      NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. Unsupported user %s at nesting level %d.\", n->_idx, _invocation, use->Name(), nesting);)\n+      return false;\n+    \/\/ TODO 8315003 Re-enable\n+    } else if (use->is_CastPP() && false) {\n+      const Type* cast_t = _igvn->type(use);\n+      if (cast_t == nullptr || cast_t->make_ptr()->isa_instptr() == nullptr) {\n+        NOT_PRODUCT(use->dump();)\n+        NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. CastPP is not to an instance.\", n->_idx, _invocation);)\n+        return false;\n+      }\n+\n+      bool is_trivial_control = use->in(0) == nullptr || use->in(0) == n->in(0);\n+      if (!is_trivial_control) {\n+        \/\/ If it's not a trivial control then we check if we can reduce the\n+        \/\/ CmpP\/N used by the If controlling the cast.\n+        if (use->in(0)->is_IfTrue() || use->in(0)->is_IfFalse()) {\n+          Node* iff = use->in(0)->in(0);\n+          Node* iff_cmp = iff->in(1)->in(1); \/\/ if->bool->cmp\n+          if (!can_reduce_cmp(n, iff_cmp)) {\n+            NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. CastPP %d doesn't have simple control.\", n->_idx, _invocation, use->_idx);)\n+            NOT_PRODUCT(n->dump(5);)\n+            return false;\n+          }\n+        }\n+      }\n+\n+      if (!can_reduce_check_users(use, nesting+1)) {\n+        return false;\n+      }\n+    } else if (use->Opcode() == Op_CmpP || use->Opcode() == Op_CmpN) {\n+      if (!can_reduce_cmp(n, use)) {\n+        NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. CmpP\/N %d isn't reducible.\", n->_idx, _invocation, use->_idx);)\n+        return false;\n+      }\n@@ -515,1 +601,1 @@\n-      NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. One of the uses is: %d %s\", ophi->_idx, _invocation, use->_idx, use->Name());)\n+      NOT_PRODUCT(if (TraceReduceAllocationMerges) tty->print_cr(\"Can NOT reduce Phi %d on invocation %d. One of the uses is: %d %s\", n->_idx, _invocation, use->_idx, use->Name());)\n@@ -531,2 +617,1 @@\n-  \/\/ If EliminateAllocations is False, there is no point in reducing merges.\n-  if (!_compile->do_reduce_allocation_merges()) {\n+  if (!_compile->do_reduce_allocation_merges() || ophi->region()->Opcode() != Op_Region) {\n@@ -537,4 +622,3 @@\n-  if (phi_t == nullptr || phi_t->make_ptr() == nullptr ||\n-                          phi_t->make_ptr()->isa_instptr() == nullptr ||\n-                          !phi_t->make_ptr()->isa_instptr()->klass_is_exact()) {\n-    NOT_PRODUCT(if (TraceReduceAllocationMerges) { tty->print_cr(\"Can NOT reduce Phi %d during invocation %d because it's nullable.\", ophi->_idx, _invocation); })\n+  if (phi_t == nullptr ||\n+      phi_t->make_ptr() == nullptr ||\n+      phi_t->make_ptr()->isa_aryptr() != nullptr) {\n@@ -544,1 +628,1 @@\n-  if (!can_reduce_phi_check_inputs(ophi) || !can_reduce_phi_check_users(ophi)) {\n+  if (!can_reduce_phi_check_inputs(ophi) || !can_reduce_check_users(ophi, \/* nesting: *\/ 0)) {\n@@ -552,11 +636,19 @@\n-void ConnectionGraph::reduce_phi_on_field_access(PhiNode* ophi, GrowableArray<Node *>  &alloc_worklist) {\n-  \/\/ We'll pass this to 'split_through_phi' so that it'll do the split even\n-  \/\/ though the load doesn't have an unique instance type.\n-  bool ignore_missing_instance_id = true;\n-\n-#ifdef ASSERT\n-  if (VerifyReduceAllocationMerges && !can_reduce_phi(ophi)) {\n-    TraceReduceAllocationMerges = true;\n-    ophi->dump(2);\n-    ophi->dump(-2);\n-    assert(can_reduce_phi(ophi), \"Sanity: previous reducible Phi is no longer reducible inside reduce_phi_on_field_access.\");\n+\/\/ This method will return a CmpP\/N that we need to use on the If controlling a\n+\/\/ CastPP after it was split. This method is only called on bases that are\n+\/\/ nullable therefore we always need a controlling if for the splitted CastPP.\n+\/\/\n+\/\/ 'curr_ctrl' is the control of the CastPP that we want to split through phi.\n+\/\/ If the CastPP currently doesn't have a control then the CmpP\/N will be\n+\/\/ against the NULL constant, otherwise it will be against the constant input of\n+\/\/ the existing CmpP\/N. It's guaranteed that there will be a CmpP\/N in the later\n+\/\/ case because we have constraints on it and because the CastPP has a control\n+\/\/ input.\n+Node* ConnectionGraph::specialize_cmp(Node* base, Node* curr_ctrl) {\n+  const Type* t = base->bottom_type();\n+  Node* con = nullptr;\n+\n+  if (curr_ctrl == nullptr || curr_ctrl->is_Region()) {\n+    con = _igvn->zerocon(t->basic_type());\n+  } else {\n+    Node* curr_cmp = curr_ctrl->in(0)->in(1)->in(1); \/\/ true\/false -> if -> bool -> cmp\n+    con = curr_cmp->in(1)->is_Con() ? curr_cmp->in(1) : curr_cmp->in(2);\n@@ -564,34 +656,2 @@\n-#endif\n-  \/\/ Iterate over Phi outputs looking for an AddP\n-  for (int j = ophi->outcnt()-1; j >= 0;) {\n-    Node* previous_addp = ophi->raw_out(j);\n-    if (previous_addp->is_AddP()) {\n-      \/\/ All AddPs are present in the connection graph\n-      FieldNode* fn = ptnode_adr(previous_addp->_idx)->as_Field();\n-\n-      \/\/ Iterate over AddP looking for a Load\n-      for (int k = previous_addp->outcnt()-1; k >= 0;) {\n-        Node* previous_load = previous_addp->raw_out(k);\n-        if (previous_load->is_Load()) {\n-          Node* data_phi = previous_load->as_Load()->split_through_phi(_igvn, ignore_missing_instance_id);\n-          _igvn->replace_node(previous_load, data_phi);\n-          assert(data_phi != nullptr, \"Output of split_through_phi is null.\");\n-          assert(data_phi != previous_load, \"Output of split_through_phi is same as input.\");\n-          assert(data_phi->is_Phi(), \"Return of split_through_phi should be a Phi.\");\n-\n-          \/\/ Push the newly created AddP on alloc_worklist and patch\n-          \/\/ the connection graph. Note that the changes in the CG below\n-          \/\/ won't affect the ES of objects since the new nodes have the\n-          \/\/ same status as the old ones.\n-          for (uint i = 1; i < data_phi->req(); i++) {\n-            Node* new_load = data_phi->in(i);\n-            if (new_load->is_Load()) {\n-              Node* new_addp = new_load->in(MemNode::Address);\n-              Node* base = get_addp_base(new_addp);\n-\n-              \/\/ The base might not be something that we can create an unique\n-              \/\/ type for. If that's the case we are done with that input.\n-              PointsToNode* jobj_ptn = unique_java_object(base);\n-              if (jobj_ptn == nullptr || !jobj_ptn->scalar_replaceable()) {\n-                continue;\n-              }\n+  return CmpNode::make(base, con, t->basic_type());\n+}\n@@ -600,20 +660,85 @@\n-              \/\/ Push to alloc_worklist since the base has an unique_type\n-              alloc_worklist.append_if_missing(new_addp);\n-\n-              \/\/ Now let's add the node to the connection graph\n-              _nodes.at_grow(new_addp->_idx, nullptr);\n-              add_field(new_addp, fn->escape_state(), fn->offset());\n-              add_base(ptnode_adr(new_addp->_idx)->as_Field(), ptnode_adr(base->_idx));\n-\n-              \/\/ If the load doesn't load an object then it won't be\n-              \/\/ part of the connection graph\n-              PointsToNode* curr_load_ptn = ptnode_adr(previous_load->_idx);\n-              if (curr_load_ptn != nullptr) {\n-                _nodes.at_grow(new_load->_idx, nullptr);\n-                add_local_var(new_load, curr_load_ptn->escape_state());\n-                add_edge(ptnode_adr(new_load->_idx), ptnode_adr(new_addp->_idx)->as_Field());\n-              }\n-            }\n-          }\n-        }\n-        k = MIN2(--k, (int)previous_addp->outcnt()-1);\n+\/\/ This method 'specializes' the CastPP passed as parameter to the base passed\n+\/\/ as parameter. Note that the existing CastPP input is a Phi. \"Specialize\"\n+\/\/ means that the CastPP now will be specific for a given base instead of a Phi.\n+\/\/ An If-Then-Else-Region block is inserted to control the CastPP. The control\n+\/\/ of the CastPP is a copy of the current one (if there is one) or a check\n+\/\/ against NULL.\n+\/\/\n+\/\/ Before:\n+\/\/\n+\/\/    C1     C2  ... Cn\n+\/\/     \\      |      \/\n+\/\/      \\     |     \/\n+\/\/       \\    |    \/\n+\/\/        \\   |   \/\n+\/\/         \\  |  \/\n+\/\/          \\ | \/\n+\/\/           \\|\/\n+\/\/          Region     B1      B2  ... Bn\n+\/\/            |          \\      |      \/\n+\/\/            |           \\     |     \/\n+\/\/            |            \\    |    \/\n+\/\/            |             \\   |   \/\n+\/\/            |              \\  |  \/\n+\/\/            |               \\ | \/\n+\/\/            ---------------> Phi\n+\/\/                              |\n+\/\/                      X       |\n+\/\/                      |       |\n+\/\/                      |       |\n+\/\/                      ------> CastPP\n+\/\/\n+\/\/ After (only partial illustration; base = B2, current_control = C2):\n+\/\/\n+\/\/                      C2\n+\/\/                      |\n+\/\/                      If\n+\/\/                     \/ \\\n+\/\/                    \/   \\\n+\/\/                   T     F\n+\/\/                  \/\\     \/\n+\/\/                 \/  \\   \/\n+\/\/                \/    \\ \/\n+\/\/      C1    CastPP   Reg        Cn\n+\/\/       |              |          |\n+\/\/       |              |          |\n+\/\/       |              |          |\n+\/\/       -------------- | ----------\n+\/\/                    | | |\n+\/\/                    Region\n+\/\/\n+Node* ConnectionGraph::specialize_castpp(Node* castpp, Node* base, Node* current_control) {\n+  Node* control_successor  = current_control->unique_ctrl_out();\n+  Node* minus_one          = _igvn->transform(ConINode::make(-1));\n+  Node* cmp                = _igvn->transform(specialize_cmp(base, castpp->in(0)));\n+  Node* boll               = _igvn->transform(new BoolNode(cmp, BoolTest::ne));\n+  IfNode* if_ne            = _igvn->transform(new IfNode(current_control, boll, PROB_MIN, COUNT_UNKNOWN))->as_If();\n+  Node* not_eq_control     = _igvn->transform(new IfTrueNode(if_ne));\n+  Node* yes_eq_control     = _igvn->transform(new IfFalseNode(if_ne));\n+  Node* end_region         = _igvn->transform(new RegionNode(3));\n+\n+  \/\/ Insert the new if-else-region block into the graph\n+  end_region->set_req(1, not_eq_control);\n+  end_region->set_req(2, yes_eq_control);\n+  control_successor->replace_edge(current_control, end_region, _igvn);\n+\n+  _igvn->_worklist.push(current_control);\n+  _igvn->_worklist.push(control_successor);\n+\n+  return _igvn->transform(ConstraintCastNode::make_cast_for_type(not_eq_control, base, _igvn->type(castpp), ConstraintCastNode::UnconditionalDependency, nullptr));\n+}\n+\n+Node* ConnectionGraph::split_castpp_load_through_phi(Node* curr_addp, Node* curr_load, Node* region, GrowableArray<Node*>* bases_for_loads, GrowableArray<Node *>  &alloc_worklist) {\n+  const Type* load_type = _igvn->type(curr_load);\n+  Node* nsr_value       = _igvn->zerocon(load_type->basic_type());\n+  Node* data_phi        = _igvn->transform(PhiNode::make(region, nsr_value, load_type));\n+  Node* memory          = curr_load->in(MemNode::Memory);\n+\n+  for (int i = 1; i < bases_for_loads->length(); i++) {\n+    Node* base = bases_for_loads->at(i);\n+    Node* cmp_region = nullptr;\n+    if (base != nullptr) {\n+      if (base->is_CFG()) { \/\/ means that we added a CastPP as child of this CFG node\n+        cmp_region = base->unique_ctrl_out_or_null();\n+        assert(cmp_region != nullptr, \"There should be.\");\n+        base = base->find_out_with(Op_CastPP);\n@@ -622,3 +747,16 @@\n-      \/\/ Remove the old AddP from the processing list because it's dead now\n-      alloc_worklist.remove_if_existing(previous_addp);\n-      _igvn->remove_globally_dead_node(previous_addp);\n+      Node* addr = _igvn->transform(new AddPNode(base, base, curr_addp->in(AddPNode::Offset)));\n+      Node* mem = (memory->is_Phi() && (memory->in(0) == region)) ? memory->in(i) : memory;\n+      Node* load = _igvn->transform(curr_load->clone());\n+      load->set_req(0, nullptr);\n+      load->set_req(1, mem);\n+      load->set_req(2, addr);\n+\n+      if (cmp_region != nullptr) { \/\/ see comment on previous if\n+        Node* intermediate_phi = _igvn->transform(PhiNode::make(cmp_region, nsr_value, load_type));\n+        intermediate_phi->set_req(1, load);\n+        load = intermediate_phi;\n+      }\n+\n+      data_phi->set_req(i, load);\n+    } else {\n+      \/\/ Just use the default, which is already in phi\n@@ -626,1 +764,0 @@\n-    j = MIN2(--j, (int)ophi->outcnt()-1);\n@@ -629,8 +766,109 @@\n-#ifdef ASSERT\n-  if (VerifyReduceAllocationMerges) {\n-    for (uint j = 0; j < ophi->outcnt(); j++) {\n-      Node* use = ophi->raw_out(j);\n-      if (!use->is_SafePoint()) {\n-        ophi->dump(2);\n-        ophi->dump(-2);\n-        assert(false, \"Should be a SafePoint.\");\n+  \/\/ Takes care of updating CG and split_unique_types worklists due to cloned\n+  \/\/ AddP->Load.\n+  updates_after_load_split(data_phi, curr_load, alloc_worklist);\n+\n+  return data_phi;\n+}\n+\n+\/\/ This method only reduces CastPP fields loads; SafePoints are handled\n+\/\/ separately. The idea here is basically to clone the CastPP and place copies\n+\/\/ on each input of the Phi, including non-scalar replaceable inputs.\n+\/\/ Experimentation shows that the resulting IR graph is simpler that way than if\n+\/\/ we just split the cast through scalar-replaceable inputs.\n+\/\/\n+\/\/ The reduction process requires that CastPP's control be one of:\n+\/\/  1) no control,\n+\/\/  2) the same region as Ophi, or\n+\/\/  3) an IfTrue\/IfFalse coming from an CmpP\/N between Ophi and a constant.\n+\/\/\n+\/\/ After splitting the CastPP we'll put it under an If-Then-Else-Region control\n+\/\/ flow. If the CastPP originally had an IfTrue\/False control input then we'll\n+\/\/ use a similar CmpP\/N to control the new If-Then-Else-Region. Otherwise, we'll\n+\/\/ juse use a CmpP\/N against the NULL constant.\n+\/\/\n+\/\/ The If-Then-Else-Region isn't always needed. For instance, if input to\n+\/\/ splitted cast was not nullable (or if it was the NULL constant) then we don't\n+\/\/ need (shouldn't) use a CastPP at all.\n+\/\/\n+\/\/ After the casts are splitted we'll split the AddP->Loads through the Phi and\n+\/\/ connect them to the just split CastPPs.\n+\/\/\n+\/\/ Before (CastPP control is same as Phi):\n+\/\/\n+\/\/          Region     Allocate   Null    Call\n+\/\/            |             \\      |      \/\n+\/\/            |              \\     |     \/\n+\/\/            |               \\    |    \/\n+\/\/            |                \\   |   \/\n+\/\/            |                 \\  |  \/\n+\/\/            |                  \\ | \/\n+\/\/            ------------------> Phi            # Oop Phi\n+\/\/            |                    |\n+\/\/            |                    |\n+\/\/            |                    |\n+\/\/            |                    |\n+\/\/            ----------------> CastPP\n+\/\/                                 |\n+\/\/                               AddP\n+\/\/                                 |\n+\/\/                               Load\n+\/\/\n+\/\/ After (Very much simplified):\n+\/\/\n+\/\/                         Call  NULL\n+\/\/                            \\  \/\n+\/\/                            CmpP\n+\/\/                             |\n+\/\/                           Bool#NE\n+\/\/                             |\n+\/\/                             If\n+\/\/                            \/ \\\n+\/\/                           T   F\n+\/\/                          \/ \\ \/\n+\/\/                         \/   R\n+\/\/                     CastPP  |\n+\/\/                       |     |\n+\/\/                     AddP    |\n+\/\/                       |     |\n+\/\/                     Load    |\n+\/\/                         \\   |   0\n+\/\/            Allocate      \\  |  \/\n+\/\/                \\          \\ | \/\n+\/\/               AddP         Phi\n+\/\/                  \\         \/\n+\/\/                 Load      \/\n+\/\/                    \\  0  \/\n+\/\/                     \\ | \/\n+\/\/                      \\|\/\n+\/\/                      Phi        # \"Field\" Phi\n+\/\/\n+void ConnectionGraph::reduce_phi_on_castpp_field_load(Node* curr_castpp, GrowableArray<Node *>  &alloc_worklist, GrowableArray<Node *>  &memnode_worklist) {\n+  Node* ophi = curr_castpp->in(1);\n+  assert(ophi->is_Phi(), \"Expected this to be a Phi node.\");\n+\n+  \/\/ Identify which base should be used for AddP->Load later when spliting the\n+  \/\/ CastPP->Loads through ophi. Three kind of values may be stored in this\n+  \/\/ array, depending on the nullability status of the corresponding input in\n+  \/\/ ophi.\n+  \/\/\n+  \/\/  - nullptr:    Meaning that the base is actually the NULL constant and therefore\n+  \/\/                we won't try to load from it.\n+  \/\/\n+  \/\/  - CFG Node:   Meaning that the base is a CastPP that was specialized for\n+  \/\/                this input of Ophi. I.e., we added an If->Then->Else-Region\n+  \/\/                that will 'activate' the CastPp only when the input is not Null.\n+  \/\/\n+  \/\/  - Other Node: Meaning that the base is not nullable and therefore we'll try\n+  \/\/                to load directly from it.\n+  GrowableArray<Node*> bases_for_loads(ophi->req(), ophi->req(), nullptr);\n+\n+  for (uint i = 1; i < ophi->req(); i++) {\n+    Node* base = ophi->in(i);\n+    const Type* base_t = _igvn->type(base);\n+\n+    if (base_t->maybe_null()) {\n+      if (base->is_Con()) {\n+        \/\/ Nothing todo as bases_for_loads[i] is already nullptr\n+      } else {\n+        Node* new_castpp = specialize_castpp(curr_castpp, base, ophi->in(0)->in(i));\n+        bases_for_loads.at_put(i, new_castpp->in(0)); \/\/ Use the ctrl of the new node just as a flag\n@@ -638,0 +876,2 @@\n+    } else {\n+      bases_for_loads.at_put(i, base);\n@@ -640,1 +880,25 @@\n-#endif\n+\n+  \/\/ Now let's split the CastPP->Loads through the Phi\n+  for (int i = curr_castpp->outcnt()-1; i >= 0;) {\n+    Node* use = curr_castpp->raw_out(i);\n+    if (use->is_AddP()) {\n+      for (int j = use->outcnt()-1; j >= 0;) {\n+        Node* use_use = use->raw_out(j);\n+        assert(use_use->is_Load(), \"Expected this to be a Load node.\");\n+\n+        \/\/ We can't make an unconditional load from a nullable input. The\n+        \/\/ 'split_castpp_load_through_phi` method will add an\n+        \/\/ 'If-Then-Else-Region` around nullable bases and only load from them\n+        \/\/ when the input is not null.\n+        Node* phi = split_castpp_load_through_phi(use, use_use, ophi->in(0), &bases_for_loads, alloc_worklist);\n+        _igvn->replace_node(use_use, phi);\n+\n+        --j;\n+        j = MIN2(j, (int)use->outcnt()-1);\n+      }\n+\n+      _igvn->remove_dead_node(use);\n+    }\n+    --i;\n+    i = MIN2(i, (int)curr_castpp->outcnt()-1);\n+  }\n@@ -643,7 +907,8 @@\n-\/\/ This method will create a SafePointScalarObjectNode for each combination of\n-\/\/ scalar replaceable allocation in 'ophi' and SafePoint node in 'safepoints'.\n-\/\/ The method will create a SafePointScalarMERGEnode for each combination of\n-\/\/ 'ophi' and SafePoint node in 'safepoints'.\n-\/\/ Each SafePointScalarMergeNode created here may describe multiple scalar\n-\/\/ replaced objects - check detailed description in SafePointScalarMergeNode\n-\/\/ class header.\n+\/\/ This method split a given CmpP\/N through the Phi used in one of its inputs.\n+\/\/ As a result we convert a comparison with a pointer to a comparison with an\n+\/\/ integer.\n+\/\/ The only requirement is that one of the inputs of the CmpP\/N must be a Phi\n+\/\/ while the other must be a constant.\n+\/\/ The splitting process is basically just cloning the CmpP\/N above the input\n+\/\/ Phi.  However, some (most) of the cloned CmpP\/Ns won't be requred because we\n+\/\/ can prove at compile time the result of the comparison.\n@@ -651,8 +916,44 @@\n-\/\/ This method will set entries in the Phi that are scalar replaceable to 'null'.\n-void ConnectionGraph::reduce_phi_on_safepoints(PhiNode* ophi, Unique_Node_List* safepoints) {\n-  Node* minus_one           = _igvn->register_new_node_with_optimizer(ConINode::make(-1));\n-  Node* selector            = _igvn->register_new_node_with_optimizer(PhiNode::make(ophi->region(), minus_one, TypeInt::INT));\n-  Node* null_ptr            = _igvn->makecon(TypePtr::NULL_PTR);\n-  const TypeOopPtr* merge_t = _igvn->type(ophi)->make_oopptr();\n-  uint number_of_sr_objects = 0;\n-  PhaseMacroExpand mexp(*_igvn);\n+\/\/ Before:\n+\/\/\n+\/\/             in1    in2 ... inN\n+\/\/              \\      |      \/\n+\/\/               \\     |     \/\n+\/\/                \\    |    \/\n+\/\/                 \\   |   \/\n+\/\/                  \\  |  \/\n+\/\/                   \\ | \/\n+\/\/                    Phi\n+\/\/                     |   Other\n+\/\/                     |    \/\n+\/\/                     |   \/\n+\/\/                     |  \/\n+\/\/                    CmpP\/N\n+\/\/\n+\/\/ After:\n+\/\/\n+\/\/        in1  Other   in2 Other  inN  Other\n+\/\/         |    |      |   |      |    |\n+\/\/         \\    |      |   |      |    |\n+\/\/          \\  \/       |   \/      |    \/\n+\/\/          CmpP\/N    CmpP\/N     CmpP\/N\n+\/\/          Bool      Bool       Bool\n+\/\/            \\        |        \/\n+\/\/             \\       |       \/\n+\/\/              \\      |      \/\n+\/\/               \\     |     \/\n+\/\/                \\    |    \/\n+\/\/                 \\   |   \/\n+\/\/                  \\  |  \/\n+\/\/                   \\ | \/\n+\/\/                    Phi\n+\/\/                     |\n+\/\/                     |   Zero\n+\/\/                     |    \/\n+\/\/                     |   \/\n+\/\/                     |  \/\n+\/\/                     CmpI\n+\/\/\n+\/\/\n+void ConnectionGraph::reduce_phi_on_cmp(Node* cmp) {\n+  Node* ophi = cmp->in(1)->is_Con() ? cmp->in(2) : cmp->in(1);\n+  assert(ophi->is_Phi(), \"Expected this to be a Phi node.\");\n@@ -660,1 +961,37 @@\n-  _igvn->hash_delete(ophi);\n+  Node* other = cmp->in(1)->is_Con() ? cmp->in(1) : cmp->in(2);\n+  Node* zero = _igvn->intcon(0);\n+  BoolTest::mask mask = cmp->unique_out()->as_Bool()->_test._test;\n+\n+  \/\/ This Phi will merge the result of the Cmps split through the Phi\n+  Node* res_phi  = _igvn->transform(PhiNode::make(ophi->in(0), zero, TypeInt::INT));\n+\n+  for (uint i=1; i<ophi->req(); i++) {\n+    Node* ophi_input = ophi->in(i);\n+    Node* res_phi_input = nullptr;\n+\n+    const TypeInt* tcmp = optimize_ptr_compare(ophi_input, other);\n+    if (tcmp->singleton()) {\n+      res_phi_input = _igvn->makecon(tcmp);\n+    } else {\n+      Node* ncmp = _igvn->transform(cmp->clone());\n+      ncmp->set_req(1, ophi_input);\n+      ncmp->set_req(2, other);\n+      Node* boll = _igvn->transform(new BoolNode(ncmp, mask));\n+      res_phi_input = boll->as_Bool()->as_int_value(_igvn);\n+    }\n+\n+    res_phi->set_req(i, res_phi_input);\n+  }\n+\n+  Node* new_cmp = _igvn->transform(new CmpINode(res_phi, zero));\n+  _igvn->replace_node(cmp, new_cmp);\n+}\n+\n+\/\/ Push the newly created AddP on alloc_worklist and patch\n+\/\/ the connection graph. Note that the changes in the CG below\n+\/\/ won't affect the ES of objects since the new nodes have the\n+\/\/ same status as the old ones.\n+void ConnectionGraph::updates_after_load_split(Node* data_phi, Node* previous_load, GrowableArray<Node *>  &alloc_worklist) {\n+  assert(data_phi != nullptr, \"Output of split_through_phi is null.\");\n+  assert(data_phi != previous_load, \"Output of split_through_phi is same as input.\");\n+  assert(data_phi->is_Phi(), \"Output of split_through_phi isn't a Phi.\");\n@@ -662,5 +999,86 @@\n-  \/\/ Fill in the 'selector' Phi. If index 'i' of the selector is:\n-  \/\/ -> a '-1' constant, the i'th input of the original Phi is NSR.\n-  \/\/ -> a 'x' constant >=0, the i'th input of of original Phi will be SR and the\n-  \/\/    info about the scalarized object will be at index x of\n-  \/\/    ObjectMergeValue::possible_objects\n+  if (data_phi == nullptr || !data_phi->is_Phi()) {\n+    \/\/ Make this a retry?\n+    return ;\n+  }\n+\n+  Node* previous_addp = previous_load->in(MemNode::Address);\n+  FieldNode* fn = ptnode_adr(previous_addp->_idx)->as_Field();\n+  for (uint i = 1; i < data_phi->req(); i++) {\n+    Node* new_load = data_phi->in(i);\n+\n+    if (new_load->is_Phi()) {\n+      \/\/ new_load is currently the \"intermediate_phi\" from an specialized\n+      \/\/ CastPP.\n+      new_load = new_load->in(1);\n+    }\n+\n+    \/\/ \"new_load\" might actually be a constant, parameter, etc.\n+    if (new_load->is_Load()) {\n+      Node* new_addp = new_load->in(MemNode::Address);\n+      Node* base = get_addp_base(new_addp);\n+\n+      \/\/ The base might not be something that we can create an unique\n+      \/\/ type for. If that's the case we are done with that input.\n+      PointsToNode* jobj_ptn = unique_java_object(base);\n+      if (jobj_ptn == nullptr || !jobj_ptn->scalar_replaceable()) {\n+        continue;\n+      }\n+\n+      \/\/ Push to alloc_worklist since the base has an unique_type\n+      alloc_worklist.append_if_missing(new_addp);\n+\n+      \/\/ Now let's add the node to the connection graph\n+      _nodes.at_grow(new_addp->_idx, nullptr);\n+      add_field(new_addp, fn->escape_state(), fn->offset());\n+      add_base(ptnode_adr(new_addp->_idx)->as_Field(), ptnode_adr(base->_idx));\n+\n+      \/\/ If the load doesn't load an object then it won't be\n+      \/\/ part of the connection graph\n+      PointsToNode* curr_load_ptn = ptnode_adr(previous_load->_idx);\n+      if (curr_load_ptn != nullptr) {\n+        _nodes.at_grow(new_load->_idx, nullptr);\n+        add_local_var(new_load, curr_load_ptn->escape_state());\n+        add_edge(ptnode_adr(new_load->_idx), ptnode_adr(new_addp->_idx)->as_Field());\n+      }\n+    }\n+  }\n+}\n+\n+void ConnectionGraph::reduce_phi_on_field_access(Node* previous_addp, GrowableArray<Node *>  &alloc_worklist) {\n+  \/\/ We'll pass this to 'split_through_phi' so that it'll do the split even\n+  \/\/ though the load doesn't have an unique instance type.\n+  bool ignore_missing_instance_id = true;\n+\n+  \/\/ All AddPs are present in the connection graph\n+  FieldNode* fn = ptnode_adr(previous_addp->_idx)->as_Field();\n+\n+  \/\/ Iterate over AddP looking for a Load\n+  for (int k = previous_addp->outcnt()-1; k >= 0;) {\n+    Node* previous_load = previous_addp->raw_out(k);\n+    if (previous_load->is_Load()) {\n+      Node* data_phi = previous_load->as_Load()->split_through_phi(_igvn, ignore_missing_instance_id);\n+\n+      \/\/ Takes care of updating CG and split_unique_types worklists due to cloned\n+      \/\/ AddP->Load.\n+      updates_after_load_split(data_phi, previous_load, alloc_worklist);\n+\n+      _igvn->replace_node(previous_load, data_phi);\n+    }\n+    --k;\n+    k = MIN2(k, (int)previous_addp->outcnt()-1);\n+  }\n+\n+  \/\/ Remove the old AddP from the processing list because it's dead now\n+  assert(previous_addp->outcnt() == 0, \"AddP should be dead now.\");\n+  alloc_worklist.remove_if_existing(previous_addp);\n+}\n+\n+\/\/ Create a 'selector' Phi based on the inputs of 'ophi'. If index 'i' of the\n+\/\/ selector is:\n+\/\/    -> a '-1' constant, the i'th input of the original Phi is NSR.\n+\/\/    -> a 'x' constant >=0, the i'th input of of original Phi will be SR and\n+\/\/       the info about the scalarized object will be at index x of ObjectMergeValue::possible_objects\n+PhiNode* ConnectionGraph::create_selector(PhiNode* ophi) const {\n+  Node* minus_one = _igvn->register_new_node_with_optimizer(ConINode::make(-1));\n+  Node* selector  = _igvn->register_new_node_with_optimizer(PhiNode::make(ophi->region(), minus_one, TypeInt::INT));\n+  uint number_of_sr_objects = 0;\n@@ -668,1 +1086,1 @@\n-    Node* base          = ophi->in(i);\n+    Node* base = ophi->in(i);\n@@ -678,3 +1096,104 @@\n-  \/\/ Update the debug information of all safepoints in turn\n-  for (uint spi = 0; spi < safepoints->size(); spi++) {\n-    SafePointNode* sfpt = safepoints->at(spi)->as_SafePoint();\n+  return selector->as_Phi();\n+}\n+\n+\/\/ Returns true if the AddP node 'n' has at least one base that is a reducible\n+\/\/ merge. If the base is a CastPP\/CheckCastPP then the input of the cast is\n+\/\/ checked instead.\n+bool ConnectionGraph::has_reducible_merge_base(AddPNode* n, Unique_Node_List &reducible_merges) {\n+  PointsToNode* ptn = ptnode_adr(n->_idx);\n+  if (ptn == nullptr || !ptn->is_Field() || ptn->as_Field()->base_count() < 2) {\n+    return false;\n+  }\n+\n+  for (BaseIterator i(ptn->as_Field()); i.has_next(); i.next()) {\n+    Node* base = i.get()->ideal_node();\n+\n+    if (reducible_merges.member(base)) {\n+      return true;\n+    }\n+\n+    if (base->is_CastPP() || base->is_CheckCastPP()) {\n+      base = base->in(1);\n+      if (reducible_merges.member(base)) {\n+        return true;\n+      }\n+    }\n+  }\n+\n+  return false;\n+}\n+\n+\/\/ This method will call its helper method to reduce SafePoint nodes that use\n+\/\/ 'ophi' or a casted version of 'ophi'. All SafePoint nodes using the same\n+\/\/ \"version\" of Phi use the same debug information (regarding the Phi).\n+\/\/ Therefore, I collect all safepoints and patch them all at once.\n+\/\/\n+\/\/ The safepoints using the Phi node have to be processed before safepoints of\n+\/\/ CastPP nodes. The reason is, when reducing a CastPP we add a reference (the\n+\/\/ NSR merge pointer) to the input of the CastPP (i.e., the Phi) in the\n+\/\/ safepoint. If we process CastPP's safepoints before Phi's safepoints the\n+\/\/ algorithm that process Phi's safepoints will think that the added Phi\n+\/\/ reference is a regular reference.\n+bool ConnectionGraph::reduce_phi_on_safepoints(PhiNode* ophi) {\n+  PhiNode* selector = create_selector(ophi);\n+  Unique_Node_List safepoints;\n+  Unique_Node_List casts;\n+\n+  \/\/ Just collect the users of the Phis for later processing\n+  \/\/ in the needed order.\n+  for (uint i = 0; i < ophi->outcnt(); i++) {\n+    Node* use = ophi->raw_out(i);\n+    if (use->is_SafePoint()) {\n+      safepoints.push(use);\n+    } else if (use->is_CastPP()) {\n+      casts.push(use);\n+    } else {\n+      assert(use->outcnt() == 0, \"Only CastPP & SafePoint users should be left.\");\n+    }\n+  }\n+\n+  \/\/ Need to process safepoints using the Phi first\n+  if (!reduce_phi_on_safepoints_helper(ophi, nullptr, selector, safepoints)) {\n+    return false;\n+  }\n+\n+  \/\/ Now process CastPP->safepoints\n+  for (uint i = 0; i < casts.size(); i++) {\n+    Node* cast = casts.at(i);\n+    Unique_Node_List cast_sfpts;\n+\n+    for (DUIterator_Fast jmax, j = cast->fast_outs(jmax); j < jmax; j++) {\n+      Node* use_use = cast->fast_out(j);\n+      if (use_use->is_SafePoint()) {\n+        cast_sfpts.push(use_use);\n+      } else {\n+        assert(use_use->outcnt() == 0, \"Only SafePoint users should be left.\");\n+      }\n+    }\n+\n+    if (!reduce_phi_on_safepoints_helper(ophi, cast, selector, cast_sfpts)) {\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+\/\/ This method will create a SafePointScalarMERGEnode for each SafePoint in\n+\/\/ 'safepoints'. It then will iterate on the inputs of 'ophi' and create a\n+\/\/ SafePointScalarObjectNode for each scalar replaceable input. Each\n+\/\/ SafePointScalarMergeNode may describe multiple scalar replaced objects -\n+\/\/ check detailed description in SafePointScalarMergeNode class header.\n+bool ConnectionGraph::reduce_phi_on_safepoints_helper(Node* ophi, Node* cast, Node* selector, Unique_Node_List& safepoints) {\n+  PhaseMacroExpand mexp(*_igvn);\n+  Node* original_sfpt_parent =  cast != nullptr ? cast : ophi;\n+  const TypeOopPtr* merge_t = _igvn->type(original_sfpt_parent)->make_oopptr();\n+\n+  Node* nsr_merge_pointer = ophi;\n+  if (cast != nullptr) {\n+    const Type* new_t = merge_t->meet(TypePtr::NULL_PTR);\n+    nsr_merge_pointer = _igvn->transform(ConstraintCastNode::make_cast_for_type(cast->in(0), cast->in(1), new_t, ConstraintCastNode::RegularDependency, nullptr));\n+  }\n+\n+  for (uint spi = 0; spi < safepoints.size(); spi++) {\n+    SafePointNode* sfpt = safepoints.at(spi)->as_SafePoint();\n@@ -694,1 +1213,1 @@\n-    sfpt->add_req(ophi);\n+    sfpt->add_req(nsr_merge_pointer);\n@@ -698,1 +1217,1 @@\n-      Node* base          = ophi->in(i);\n+      Node* base = ophi->in(i);\n@@ -710,1 +1229,4 @@\n-      guarantee(value_worklist.size() == 0, \"Unimplemented: Valhalla support for 8287061\");\n+      \/\/ TODO 8315003 Remove this bailout\n+      if (value_worklist.size() != 0) {\n+        return false;\n+      }\n@@ -712,2 +1234,1 @@\n-        _compile->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n-        return;\n+        return false;\n@@ -725,2 +1246,2 @@\n-    \/\/ Replaces debug information references to \"ophi\" in \"sfpt\" with references to \"smerge\"\n-    sfpt->replace_edges_in_range(ophi, smerge, debug_start, jvms->debug_end(), _igvn);\n+    \/\/ Replaces debug information references to \"original_sfpt_parent\" in \"sfpt\" with references to \"smerge\"\n+    sfpt->replace_edges_in_range(original_sfpt_parent, smerge, debug_start, jvms->debug_end(), _igvn);\n@@ -731,1 +1252,1 @@\n-    sfpt->set_req(smerge->merge_pointer_idx(jvms), ophi);\n+    sfpt->set_req(smerge->merge_pointer_idx(jvms), nsr_merge_pointer);\n@@ -735,4 +1256,53 @@\n-  \/\/ Now we can change ophi since we don't need to know the types\n-  \/\/ of the input allocations anymore.\n-  const Type* new_t = merge_t->meet(TypePtr::NULL_PTR);\n-  Node* new_phi = _igvn->register_new_node_with_optimizer(PhiNode::make(ophi->region(), null_ptr, new_t));\n+  return true;\n+}\n+\n+void ConnectionGraph::reduce_phi(PhiNode* ophi, GrowableArray<Node *>  &alloc_worklist, GrowableArray<Node *>  &memnode_worklist) {\n+  bool delay = _igvn->delay_transform();\n+  _igvn->set_delay_transform(true);\n+  _igvn->hash_delete(ophi);\n+\n+  \/\/ Copying all users first because some will be removed and others won't.\n+  \/\/ Ophi also may acquire some new users as part of Cast reduction.\n+  \/\/ CastPPs also need to be processed before CmpPs.\n+  Unique_Node_List castpps;\n+  Unique_Node_List others;\n+  for (DUIterator_Fast imax, i = ophi->fast_outs(imax); i < imax; i++) {\n+    Node* use = ophi->fast_out(i);\n+\n+    if (use->is_CastPP()) {\n+      castpps.push(use);\n+    } else if (use->is_AddP() || use->is_Cmp()) {\n+      others.push(use);\n+    } else if (use->is_SafePoint()) {\n+      \/\/ processed later\n+    } else {\n+      assert(use->is_SafePoint(), \"Unexpected user of reducible Phi %d -> %d:%s:%d\", ophi->_idx, use->_idx, use->Name(), use->outcnt());\n+    }\n+  }\n+\n+  \/\/ CastPPs need to be processed before Cmps because during the process of\n+  \/\/ splitting CastPPs we make reference to the inputs of the Cmp that is used\n+  \/\/ by the If controlling the CastPP.\n+  for (uint i = 0; i < castpps.size(); i++) {\n+    reduce_phi_on_castpp_field_load(castpps.at(i), alloc_worklist, memnode_worklist);\n+  }\n+\n+  for (uint i = 0; i < others.size(); i++) {\n+    Node* use = others.at(i);\n+\n+    if (use->is_AddP()) {\n+      reduce_phi_on_field_access(use, alloc_worklist);\n+    } else if(use->is_Cmp()) {\n+      reduce_phi_on_cmp(use);\n+    }\n+  }\n+\n+  _igvn->set_delay_transform(delay);\n+}\n+\n+void ConnectionGraph::reset_scalar_replaceable_entries(PhiNode* ophi) {\n+  Node* null_ptr            = _igvn->makecon(TypePtr::NULL_PTR);\n+  const TypeOopPtr* merge_t = _igvn->type(ophi)->make_oopptr();\n+  const Type* new_t         = merge_t->meet(TypePtr::NULL_PTR);\n+  Node* new_phi             = _igvn->register_new_node_with_optimizer(PhiNode::make(ophi->region(), null_ptr, new_t));\n+\n@@ -750,4 +1320,2 @@\n-  _igvn->replace_node(ophi, new_phi);\n-  _igvn->hash_insert(ophi);\n-  _igvn->_worklist.push(ophi);\n-}\n+  for (int i = ophi->outcnt()-1; i >= 0;) {\n+    Node* out = ophi->raw_out(i);\n@@ -755,2 +1323,4 @@\n-void ConnectionGraph::reduce_phi(PhiNode* ophi) {\n-  Unique_Node_List safepoints;\n+    if (out->is_ConstraintCast()) {\n+      const Type* out_t = _igvn->type(out)->make_ptr();\n+      const Type* out_new_t = out_t->meet(TypePtr::NULL_PTR);\n+      bool change = out_new_t != out_t;\n@@ -758,2 +1328,7 @@\n-  for (uint i = 0; i < ophi->outcnt(); i++) {\n-    Node* use = ophi->raw_out(i);\n+      for (int j = out->outcnt()-1; change && j >= 0; --j) {\n+        Node* out2 = out->raw_out(j);\n+        if (!out2->is_SafePoint()) {\n+          change = false;\n+          break;\n+        }\n+      }\n@@ -761,13 +1336,5 @@\n-    \/\/ All SafePoint nodes using the same Phi node use the same debug\n-    \/\/ information (regarding the Phi). Furthermore, reducing the Phi used by a\n-    \/\/ SafePoint requires changing the Phi. Therefore, I collect all safepoints\n-    \/\/ and patch them all at once later.\n-    if (use->is_SafePoint()) {\n-      safepoints.push(use->as_SafePoint());\n-    } else {\n-#ifdef ASSERT\n-      ophi->dump(-3);\n-      assert(false, \"Unexpected user of reducible Phi %d -> %d:%s\", ophi->_idx, use->_idx, use->Name());\n-#endif\n-      _compile->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n-      return;\n+      if (change) {\n+        Node* new_cast = ConstraintCastNode::make_cast_for_type(out->in(0), out->in(1), out_new_t, ConstraintCastNode::StrongDependency, nullptr);\n+        _igvn->replace_node(out, new_cast);\n+        _igvn->register_new_node_with_optimizer(new_cast);\n+      }\n@@ -775,3 +1342,2 @@\n-  }\n-  if (safepoints.size() > 0) {\n-    reduce_phi_on_safepoints(ophi, &safepoints);\n+    --i;\n+    i = MIN2(i, (int)ophi->outcnt()-1);\n@@ -780,0 +1346,2 @@\n+\n+  _igvn->replace_node(ophi, new_phi);\n@@ -2376,0 +2944,6 @@\n+        \/\/ These other local vars may point to multiple objects through a Phi\n+        \/\/ In this case we skip them and see if we can reduce the Phi.\n+        if (use_n->is_CastPP() || use_n->is_CheckCastPP()) {\n+          use_n = use_n->in(1);\n+        }\n+\n@@ -2377,4 +2951,1 @@\n-        if (candidates.member(use_n)) {\n-          continue;\n-        } else if (reducible_merges.member(use_n)) {\n-          candidates.push(use_n);\n+        if (candidates.member(use_n) || reducible_merges.member(use_n)) {\n@@ -2454,9 +3025,11 @@\n-      for (BaseIterator i(field); i.has_next(); i.next()) {\n-        PointsToNode* base = i.get();\n-        \/\/ Don't take into account LocalVar nodes which\n-        \/\/ may point to only one object which should be also\n-        \/\/ this field's base by now.\n-        if (base->is_JavaObject() && base != jobj) {\n-          \/\/ Mark all bases.\n-          set_not_scalar_replaceable(jobj NOT_PRODUCT(COMMA \"may point to more than one object\"));\n-          set_not_scalar_replaceable(base NOT_PRODUCT(COMMA \"may point to more than one object\"));\n+      if (has_non_reducible_merge(field, reducible_merges)) {\n+        for (BaseIterator i(field); i.has_next(); i.next()) {\n+          PointsToNode* base = i.get();\n+          \/\/ Don't take into account LocalVar nodes which\n+          \/\/ may point to only one object which should be also\n+          \/\/ this field's base by now.\n+          if (base->is_JavaObject() && base != jobj) {\n+            \/\/ Mark all bases.\n+            set_not_scalar_replaceable(jobj NOT_PRODUCT(COMMA \"may point to more than one object\"));\n+            set_not_scalar_replaceable(base NOT_PRODUCT(COMMA \"may point to more than one object\"));\n+          }\n@@ -2464,3 +3037,3 @@\n-      }\n-      if (!jobj->scalar_replaceable()) {\n-        return;\n+        if (!jobj->scalar_replaceable()) {\n+          return;\n+        }\n@@ -2482,0 +3055,10 @@\n+bool ConnectionGraph::has_non_reducible_merge(FieldNode* field, Unique_Node_List& reducible_merges) {\n+  for (BaseIterator i(field); i.has_next(); i.next()) {\n+    Node* base = i.get()->ideal_node();\n+    if (base->is_Phi() && !reducible_merges.member(base)) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -2615,1 +3198,2 @@\n-      const TypeInt* tcmp = optimize_ptr_compare(n);\n+      assert(n->Opcode() == Op_CmpN || n->Opcode() == Op_CmpP, \"must be\");\n+      const TypeInt* tcmp = optimize_ptr_compare(n->in(1), n->in(2));\n@@ -2653,1 +3237,1 @@\n-const TypeInt* ConnectionGraph::optimize_ptr_compare(Node* n) {\n+const TypeInt* ConnectionGraph::optimize_ptr_compare(Node* left, Node* right) {\n@@ -2655,1 +3239,0 @@\n-  assert(n->Opcode() == Op_CmpN || n->Opcode() == Op_CmpP, \"must be\");\n@@ -2660,4 +3243,13 @@\n-  PointsToNode* ptn1 = ptnode_adr(n->in(1)->_idx);\n-  PointsToNode* ptn2 = ptnode_adr(n->in(2)->_idx);\n-  JavaObjectNode* jobj1 = unique_java_object(n->in(1));\n-  JavaObjectNode* jobj2 = unique_java_object(n->in(2));\n+  PointsToNode* ptn1 = ptnode_adr(left->_idx);\n+  PointsToNode* ptn2 = ptnode_adr(right->_idx);\n+  JavaObjectNode* jobj1 = unique_java_object(left);\n+  JavaObjectNode* jobj2 = unique_java_object(right);\n+\n+  \/\/ The use of this method during allocation merge reduction may cause 'left'\n+  \/\/ or 'right' be something (e.g., a Phi) that isn't in the connection graph or\n+  \/\/ that doesn't reference an unique java object.\n+  if (ptn1 == nullptr || ptn2 == nullptr ||\n+      jobj1 == nullptr || jobj2 == nullptr) {\n+    return UNKNOWN;\n+  }\n+\n@@ -3877,2 +4469,1 @@\n-      Node* addp_base = get_addp_base(n);\n-      if (addp_base != nullptr && reducible_merges.member(addp_base)) {\n+      if (has_reducible_merge_base(n->as_AddP(), reducible_merges)) {\n@@ -3882,0 +4473,1 @@\n+      Node* addp_base = get_addp_base(n);\n@@ -3903,1 +4495,3 @@\n-      \/\/ Reducible Phi's will be removed from the graph after split_unique_types finishes\n+      \/\/ Reducible Phi's will be removed from the graph after split_unique_types\n+      \/\/ finishes. For now we just try to split out the SR inputs of the merge.\n+      Node* parent = n->in(1);\n@@ -3905,2 +4499,1 @@\n-        \/\/ Split loads through phi\n-        reduce_phi_on_field_access(n->as_Phi(), alloc_worklist);\n+        reduce_phi(n->as_Phi(), alloc_worklist, memnode_worklist);\n@@ -3913,0 +4506,4 @@\n+      } else if (reducible_merges.member(parent)) {\n+        \/\/ 'n' is an user of a reducible merge (a Phi). It will be simplified as\n+        \/\/ part of reduce_merge.\n+        continue;\n@@ -4035,1 +4632,0 @@\n-    \/\/ At this point reducible Phis shouldn't have AddP users anymore; only SafePoints.\n@@ -4045,0 +4641,1 @@\n+      \/\/ At this point reducible Phis shouldn't have AddP users anymore; only SafePoints or Casts.\n@@ -4047,1 +4644,1 @@\n-        if (!use->is_SafePoint()) {\n+        if (!use->is_SafePoint() && !use->is_CastPP()) {\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":803,"deletions":206,"binary":false,"changes":1009,"status":"modified"},{"patch":"@@ -958,2 +958,0 @@\n-  Node* create_bool_from_template_assertion_predicate(Node* template_assertion_predicate, Node* new_init, Node* new_stride,\n-                                                      Node* control);\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -863,1 +863,2 @@\n-      assert(C->has_circular_inline_type(), \"non-circular inline types should have been replaced earlier\");\n+      \/\/ TODO 8315003 This starts to fail after JDK-8316991. Fix and re-enable.\n+      \/\/ assert(C->has_circular_inline_type(), \"non-circular inline types should have been replaced earlier\");\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -233,1 +233,1 @@\n-      if (t_oop->is_aryptr()) {\n+      if (t_oop->isa_aryptr()) {\n@@ -952,0 +952,1 @@\n+  case T_NARROWOOP:\n@@ -1578,0 +1579,2 @@\n+\/\/ Some differences from original method (split_through_phi):\n+\/\/  - If base->is_CastPP(): base = base->in(1)\n@@ -1583,2 +1586,5 @@\n-  bool base_is_phi = (base != nullptr) && base->is_Phi();\n-  if (req() > 3 || !base_is_phi) {\n+  if (base->is_CastPP()) {\n+    base = base->in(1);\n+  }\n+\n+  if (req() > 3 || base == nullptr || !base->is_Phi()) {\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -64,0 +64,1 @@\n+class CastPPNode;\n@@ -726,0 +727,1 @@\n+        DEFINE_CLASS_ID(CastPP, ConstraintCast, 6)\n@@ -909,0 +911,1 @@\n+  DEFINE_CLASS_QUERY(CastPP)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -826,1 +826,1 @@\n-                           new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()), is_init);\n+                           new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()), true, is_init);\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"code\/compiledMethod.inline.hpp\"\n@@ -89,1 +88,1 @@\n-\/\/  MarkSweep::invoke(0, \"Debugging\");\n+\/\/  Universe::heap()->collect();\n@@ -1860,3 +1859,2 @@\n-  if (blob->is_compiled()) {\n-    CompiledMethod* cm = blob->as_compiled_method_or_null();\n-    cm->method()->print_value_on(&tempst);\n+  if (blob->is_nmethod()) {\n+    blob->as_nmethod()->method()->print_value_on(&tempst);\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -647,0 +647,8 @@\n+    case T_OBJECT:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+    case T_METADATA:\n+      return new CmpPNode(in1, in2);\n+    case T_NARROWOOP:\n+    case T_NARROWKLASS:\n+      return new CmpNNode(in1, in2);\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -6812,1 +6812,2 @@\n-      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->intersection_with(tp_interfaces)->eq(tp_interfaces) && !tp->klass_is_exact()) {\n+      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) &&\n+          !tp->klass_is_exact()) {\n@@ -6830,1 +6831,2 @@\n-        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->intersection_with(tp_interfaces)->eq(tp_interfaces) && !tp->klass_is_exact()) {\n+        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) &&\n+            !tp->klass_is_exact()) {\n@@ -6864,1 +6866,2 @@\n-    return other->klass() == ciEnv::current()->Object_klass() && other->_interfaces->intersection_with(this_one->_interfaces)->eq(other->_interfaces) && other_exact;\n+    return other->klass() == ciEnv::current()->Object_klass() && this_one->_interfaces->contains(other->_interfaces) &&\n+           other_exact;\n@@ -6933,1 +6936,2 @@\n-    return other->klass()->equals(ciEnv::current()->Object_klass()) && other->_interfaces->intersection_with(this_one->_interfaces)->eq(other->_interfaces);\n+    return other->klass()->equals(ciEnv::current()->Object_klass()) &&\n+           this_one->_interfaces->contains(other->_interfaces);\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1681,1 +1681,0 @@\n-  JvmtiVTMSTransitionDisabler disabler;\n@@ -1684,1 +1683,0 @@\n-\n@@ -1687,1 +1685,0 @@\n-    ThreadsListHandle tlh(current_thread);\n@@ -1690,16 +1687,0 @@\n-    JavaThread *java_thread;\n-    oop thread_obj = nullptr;\n-    err = get_threadOop_and_JavaThread(tlh.list(), thread, &java_thread, &thread_obj);\n-    if (err != JVMTI_ERROR_NONE) {\n-      return err;\n-    }\n-\n-    if (java_lang_VirtualThread::is_instance(thread_obj) && java_thread == nullptr) {\n-      \/\/ Target virtual thread is unmounted.\n-      ResourceMark rm(current_thread);\n-      MultipleStackTracesCollector collector(this, max_frame_count);\n-      collector.fill_frames(thread, java_thread, thread_obj);\n-      collector.allocate_and_fill_stacks(\/* thread_count *\/ 1);\n-      *stack_info_ptr = collector.stack_info();\n-      return collector.result();\n-    }\n@@ -1708,1 +1689,1 @@\n-    Handshake::execute(&op, &tlh, java_thread);\n+    JvmtiHandshake::execute(&op, thread);\n@@ -1714,0 +1695,2 @@\n+    JvmtiVTMSTransitionDisabler disabler;\n+\n@@ -1751,0 +1734,1 @@\n+  Handle thread_handle(current_thread, thread_obj);\n@@ -1777,5 +1761,1 @@\n-  if (self) {\n-    op.doit(java_thread, self);\n-  } else {\n-    Handshake::execute(&op, java_thread);\n-  }\n+  JvmtiHandshake::execute(&op, &tlh, java_thread, thread_handle);\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnv.cpp","additions":5,"deletions":25,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -4371,1 +4371,2 @@\n-  \/\/ Leave arrays of jmethodIDs and itable index cache unchanged\n+  \/\/ Update jmethodID cache if present.\n+  the_class->update_methods_jmethod_cache();\n","filename":"src\/hotspot\/share\/prims\/jvmtiRedefineClasses.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1060,1 +1060,1 @@\n-    if (box.not_null() && box->klass() == Universe::objectArrayKlassObj() && box->length() > 0) {\n+    if (box.not_null() && box->klass() == Universe::objectArrayKlass() && box->length() > 0) {\n@@ -1263,1 +1263,1 @@\n-      index_info_oop->klass() != Universe::intArrayKlassObj() ||\n+      index_info_oop->klass() != Universe::intArrayKlass() ||\n","filename":"src\/hotspot\/share\/prims\/methodHandles.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -774,3 +774,3 @@\n-                CompiledMethod* cm = CodeCache::find_compiled(f->pc());\n-                assert(cm != nullptr, \"sanity check\");\n-                cm->make_not_entrant();\n+                nmethod* nm = CodeCache::find_nmethod(f->pc());\n+                assert(nm != nullptr, \"did not find nmethod\");\n+                nm->make_not_entrant();\n@@ -826,1 +826,1 @@\n-      MutexLocker ml(CompiledMethod_lock, Mutex::_no_safepoint_check_flag);\n+      MutexLocker ml(NMethodState_lock, Mutex::_no_safepoint_check_flag);\n@@ -845,1 +845,1 @@\n-  CompiledMethod* code = is_osr ? mh->lookup_osr_nmethod_for(InvocationEntryBci, CompLevel_none, false) : mh->code();\n+  nmethod* code = is_osr ? mh->lookup_osr_nmethod_for(InvocationEntryBci, CompLevel_none, false) : mh->code();\n@@ -944,1 +944,1 @@\n-  CompiledMethod* code = is_osr ? mh->lookup_osr_nmethod_for(InvocationEntryBci, CompLevel_none, false) : mh->code();\n+  nmethod* code = is_osr ? mh->lookup_osr_nmethod_for(InvocationEntryBci, CompLevel_none, false) : mh->code();\n@@ -1029,1 +1029,1 @@\n-  CompiledMethod* code = mh->lookup_osr_nmethod_for(InvocationEntryBci, CompLevel_none, false);\n+  nmethod* code = mh->lookup_osr_nmethod_for(InvocationEntryBci, CompLevel_none, false);\n@@ -1103,2 +1103,2 @@\n-    CompiledMethod* code = mh->code();\n-    if (code != nullptr && code->as_nmethod_or_null() != nullptr) {\n+    nmethod* code = mh->code();\n+    if (code != nullptr) {\n@@ -1562,1 +1562,1 @@\n-  CompiledMethod* code = is_osr ? mh->lookup_osr_nmethod_for(InvocationEntryBci, CompLevel_none, false) : mh->code();\n+  nmethod* code = is_osr ? mh->lookup_osr_nmethod_for(InvocationEntryBci, CompLevel_none, false) : mh->code();\n@@ -1614,1 +1614,1 @@\n-      ::new (blob) BufferBlob(\"WB::DummyBlob\", full_size);\n+      ::new (blob) BufferBlob(\"WB::DummyBlob\", CodeBlobKind::Buffer, full_size);\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -541,0 +541,1 @@\n+  { \"UseNeon\",                      JDK_Version::undefined(), JDK_Version::jdk(23), JDK_Version::jdk(24) },\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -114,1 +114,1 @@\n-  MutexLocker ml(CompiledMethod_lock, Mutex::_no_safepoint_check_flag);\n+  MutexLocker ml(NMethodState_lock, Mutex::_no_safepoint_check_flag);\n@@ -123,2 +123,2 @@\n-void DeoptimizationScope::mark(CompiledMethod* cm, bool inc_recompile_counts) {\n-  ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n+void DeoptimizationScope::mark(nmethod* nm, bool inc_recompile_counts) {\n+  ConditionalMutexLocker ml(NMethodState_lock, !NMethodState_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n@@ -127,2 +127,2 @@\n-  if (cm->is_marked_for_deoptimization()) {\n-    dependent(cm);\n+  if (nm->is_marked_for_deoptimization()) {\n+    dependent(nm);\n@@ -132,3 +132,3 @@\n-  CompiledMethod::DeoptimizationStatus status =\n-    inc_recompile_counts ? CompiledMethod::deoptimize : CompiledMethod::deoptimize_noupdate;\n-  Atomic::store(&cm->_deoptimization_status, status);\n+  nmethod::DeoptimizationStatus status =\n+    inc_recompile_counts ? nmethod::deoptimize : nmethod::deoptimize_noupdate;\n+  Atomic::store(&nm->_deoptimization_status, status);\n@@ -138,1 +138,1 @@\n-  assert(cm->_deoptimization_generation == 0, \"Is already marked\");\n+  assert(nm->_deoptimization_generation == 0, \"Is already marked\");\n@@ -140,1 +140,1 @@\n-  cm->_deoptimization_generation = DeoptimizationScope::_active_deopt_gen;\n+  nm->_deoptimization_generation = DeoptimizationScope::_active_deopt_gen;\n@@ -144,2 +144,2 @@\n-void DeoptimizationScope::dependent(CompiledMethod* cm) {\n-  ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n+void DeoptimizationScope::dependent(nmethod* nm) {\n+  ConditionalMutexLocker ml(NMethodState_lock, !NMethodState_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n@@ -149,2 +149,2 @@\n-  if (_required_gen < cm->_deoptimization_generation) {\n-    _required_gen = cm->_deoptimization_generation;\n+  if (_required_gen < nm->_deoptimization_generation) {\n+    _required_gen = nm->_deoptimization_generation;\n@@ -176,1 +176,1 @@\n-      ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n+      ConditionalMutexLocker ml(NMethodState_lock, !NMethodState_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n@@ -204,1 +204,1 @@\n-        ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n+        ConditionalMutexLocker ml(NMethodState_lock, !NMethodState_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n@@ -307,1 +307,0 @@\n-    Klass* k = java_lang_Class::as_Klass(sv->klass()->as_ConstantOopReadValue()->value()());\n@@ -310,10 +309,2 @@\n-    st.print(\"     object <\" INTPTR_FORMAT \"> of type \", p2i(sv->value()()));\n-    k->print_value_on(&st);\n-    assert(obj.not_null() || k->is_inline_klass() || realloc_failures, \"reallocation was missed\");\n-      if (k->is_inline_klass()) {\n-        st.print(\" is null\");\n-      } else {\n-        st.print(\" allocation failed\");\n-      }\n-    } else {\n-      st.print(\" allocated (\" SIZE_FORMAT \" bytes)\", obj->size() * HeapWordSize);\n+      st.print_cr(\"     nullptr\");\n+      continue;\n@@ -322,2 +313,7 @@\n-    st.cr();\n-    if (Verbose && !obj.is_null()) {\n+    Klass* k = java_lang_Class::as_Klass(sv->klass()->as_ConstantOopReadValue()->value()());\n+\n+    st.print(\"     object <\" INTPTR_FORMAT \"> of type \", p2i(sv->value()()));\n+    k->print_value_on(&st);\n+    st.print_cr(\" allocated (\" SIZE_FORMAT \" bytes)\", obj->size() * HeapWordSize);\n+\n+    if (Verbose && k != nullptr) {\n@@ -331,1 +327,1 @@\n-static bool rematerialize_objects(JavaThread* thread, int exec_mode, CompiledMethod* compiled_method,\n+static bool rematerialize_objects(JavaThread* thread, int exec_mode, nmethod* compiled_method,\n@@ -473,1 +469,1 @@\n-  CompiledMethod* cm = deoptee.cb()->as_compiled_method_or_null();\n+  nmethod* nm = deoptee.cb()->as_nmethod_or_null();\n@@ -482,1 +478,1 @@\n-    realloc_failures = rematerialize_objects(thread, Unpack_none, cm, deoptee, map, chunk, deoptimized_objects);\n+    realloc_failures = rematerialize_objects(thread, Unpack_none, nm, deoptee, map, chunk, deoptimized_objects);\n@@ -526,2 +522,2 @@\n-  CompiledMethod* cm = deoptee.cb()->as_compiled_method_or_null();\n-  current->set_deopt_compiled_method(cm);\n+  nmethod* nm = deoptee.cb()->as_nmethod_or_null();\n+  current->set_deopt_compiled_method(nm);\n@@ -556,1 +552,1 @@\n-    realloc_failures = rematerialize_objects(current, exec_mode, cm, deoptee, map, chunk, unused);\n+    realloc_failures = rematerialize_objects(current, exec_mode, nm, deoptee, map, chunk, unused);\n@@ -1263,2 +1259,2 @@\n-      CompiledMethod* cm = fr->cb()->as_compiled_method_or_null();\n-      if (cm->is_compiled_by_jvmci() && sv->is_auto_box()) {\n+      nmethod* nm = fr->cb()->as_nmethod_or_null();\n+      if (nm->is_compiled_by_jvmci() && sv->is_auto_box()) {\n@@ -1844,2 +1840,2 @@\n-    CompiledMethod* cm = fr.cb()->as_compiled_method_or_null();\n-    assert(cm != nullptr, \"only compiled methods can deopt\");\n+    nmethod* nm = fr.cb()->as_nmethod_or_null();\n+    assert(nm != nullptr, \"only compiled methods can deopt\");\n@@ -1849,1 +1845,1 @@\n-    cm->log_identity(xtty);\n+    nm->log_identity(xtty);\n@@ -1851,1 +1847,1 @@\n-    for (ScopeDesc* sd = cm->scope_desc_at(fr.pc()); ; sd = sd->sender()) {\n+    for (ScopeDesc* sd = nm->scope_desc_at(fr.pc()); ; sd = sd->sender()) {\n@@ -1879,1 +1875,1 @@\n-address Deoptimization::deoptimize_for_missing_exception_handler(CompiledMethod* cm) {\n+address Deoptimization::deoptimize_for_missing_exception_handler(nmethod* nm) {\n@@ -1881,1 +1877,1 @@\n-  cm->make_not_entrant();\n+  nm->make_not_entrant();\n@@ -1894,1 +1890,1 @@\n-  assert(caller_frame.cb()->as_compiled_method_or_null() == cm, \"expect top frame compiled method\");\n+  assert(caller_frame.cb()->as_nmethod_or_null() == nm, \"expect top frame compiled method\");\n@@ -1912,1 +1908,1 @@\n-  MethodData* trap_mdo = get_method_data(thread, methodHandle(thread, cm->method()), true);\n+  MethodData* trap_mdo = get_method_data(thread, methodHandle(thread, nm->method()), true);\n@@ -2047,1 +2043,1 @@\n-static void post_deoptimization_event(CompiledMethod* nm,\n+static void post_deoptimization_event(nmethod* nm,\n@@ -2076,1 +2072,1 @@\n-static void log_deopt(CompiledMethod* nm, Method* tm, intptr_t pc, frame& fr, int trap_bci,\n+static void log_deopt(nmethod* nm, Method* tm, intptr_t pc, frame& fr, int trap_bci,\n@@ -2138,1 +2134,1 @@\n-    CompiledMethod* nm = cvf->code();\n+    nmethod* nm = cvf->code();\n@@ -2155,1 +2151,1 @@\n-      nm->as_nmethod()->update_speculation(current);\n+      nm->update_speculation(current);\n@@ -2275,2 +2271,2 @@\n-        if (nm->is_nmethod()) {\n-          const char* installed_code_name = nm->as_nmethod()->jvmci_name();\n+        if (nm->is_compiled_by_jvmci()) {\n+          const char* installed_code_name = nm->jvmci_name();\n@@ -2530,1 +2526,1 @@\n-          UseRTMDeopt && (nm->as_nmethod()->rtm_state() != ProfileRTM)) {\n+          UseRTMDeopt && (nm->rtm_state() != ProfileRTM)) {\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":48,"deletions":52,"binary":false,"changes":100,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -60,1 +60,1 @@\n-  void mark(CompiledMethod* cm, bool inc_recompile_counts = true);\n+  void mark(nmethod* nm, bool inc_recompile_counts = true);\n@@ -62,1 +62,1 @@\n-  void dependent(CompiledMethod* cm);\n+  void dependent(nmethod* nm);\n@@ -187,1 +187,1 @@\n-  static address deoptimize_for_missing_exception_handler(CompiledMethod* cm);\n+  static address deoptimize_for_missing_exception_handler(nmethod* nm);\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -212,3 +212,4 @@\n-    CompiledMethod* cm = cb()->as_compiled_method_or_null();\n-    if (cm->is_method_handle_return(pc()))\n-      return cm->deopt_mh_handler_begin() - pc_return_offset;\n+    nmethod* nm = cb()->as_nmethod_or_null();\n+    assert(nm != nullptr, \"only nmethod is expected here\");\n+    if (nm->is_method_handle_return(pc()))\n+      return nm->deopt_mh_handler_begin() - pc_return_offset;\n@@ -216,1 +217,1 @@\n-      return cm->deopt_handler_begin() - pc_return_offset;\n+      return nm->deopt_handler_begin() - pc_return_offset;\n@@ -320,2 +321,2 @@\n-  assert(_cb != nullptr && _cb->is_compiled(), \"must be an nmethod\");\n-  CompiledMethod* nm = (CompiledMethod *)_cb;\n+  assert(_cb != nullptr && _cb->is_nmethod(), \"must be an nmethod\");\n+  nmethod* nm = _cb->as_nmethod();\n@@ -340,1 +341,1 @@\n-  CompiledMethod* nm = (CompiledMethod*)_cb;\n+  nmethod* nm = _cb->as_nmethod();\n@@ -353,1 +354,1 @@\n-  assert(_cb != nullptr && _cb->is_compiled(), \"must be\");\n+  assert(_cb != nullptr && _cb->is_nmethod(), \"must be\");\n@@ -356,4 +357,4 @@\n-  CompiledMethod* cm = (CompiledMethod*) _cb;\n-  address deopt = cm->is_method_handle_return(pc()) ?\n-                        cm->deopt_mh_handler_begin() :\n-                        cm->deopt_handler_begin();\n+  nmethod* nm = _cb->as_nmethod();\n+  address deopt = nm->is_method_handle_return(pc()) ?\n+                        nm->deopt_mh_handler_begin() :\n+                        nm->deopt_handler_begin();\n@@ -364,1 +365,1 @@\n-  cm->set_original_pc(this, pc());\n+  nm->set_original_pc(this, pc());\n@@ -367,2 +368,2 @@\n-  if (cm->is_compiled_by_c1() && cm->method()->has_scalarized_args() &&\n-      pc() < cm->verified_inline_entry_point()) {\n+  if (nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n+      pc() < nm->verified_inline_entry_point()) {\n@@ -700,3 +701,3 @@\n-    } else if (_cb->is_compiled()) {\n-      CompiledMethod* cm = (CompiledMethod*)_cb;\n-      Method* m = cm->method();\n+    } else if (_cb->is_nmethod()) {\n+      nmethod* nm = _cb->as_nmethod();\n+      Method* m = nm->method();\n@@ -704,5 +705,2 @@\n-        if (cm->is_nmethod()) {\n-          nmethod* nm = cm->as_nmethod();\n-          st->print(\"J %d%s\", nm->compile_id(), (nm->is_osr_method() ? \"%\" : \"\"));\n-          st->print(\" %s\", nm->compiler_name());\n-        }\n+        st->print(\"J %d%s\", nm->compile_id(), (nm->is_osr_method() ? \"%\" : \"\"));\n+        st->print(\" %s\", nm->compiler_name());\n@@ -723,6 +721,3 @@\n-        if (cm->is_nmethod()) {\n-          nmethod* nm = cm->as_nmethod();\n-          const char* jvmciName = nm->jvmci_name();\n-          if (jvmciName != nullptr) {\n-            st->print(\" (%s)\", jvmciName);\n-          }\n+        const char* jvmciName = nm->jvmci_name();\n+        if (jvmciName != nullptr) {\n+          st->print(\" (%s)\", jvmciName);\n@@ -1014,1 +1009,1 @@\n-void frame::oops_code_blob_do(OopClosure* f, CodeBlobClosure* cf, DerivedOopClosure* df, DerivedPointerIterationMode derived_mode, const RegisterMap* reg_map) const {\n+void frame::oops_nmethod_do(OopClosure* f, NMethodClosure* cf, DerivedOopClosure* df, DerivedPointerIterationMode derived_mode, const RegisterMap* reg_map) const {\n@@ -1035,2 +1030,2 @@\n-  if (cf != nullptr)\n-    cf->do_code_blob(_cb);\n+  if (cf != nullptr && _cb->is_nmethod())\n+    cf->do_nmethod(_cb->as_nmethod());\n@@ -1180,1 +1175,1 @@\n-void frame::oops_do_internal(OopClosure* f, CodeBlobClosure* cf,\n+void frame::oops_do_internal(OopClosure* f, NMethodClosure* cf,\n@@ -1197,1 +1192,1 @@\n-    oops_code_blob_do(f, cf, df, derived_mode, map);\n+    oops_nmethod_do(f, cf, df, derived_mode, map);\n@@ -1203,1 +1198,1 @@\n-void frame::nmethods_do(CodeBlobClosure* cf) const {\n+void frame::nmethod_do(NMethodClosure* cf) const {\n@@ -1205,1 +1200,1 @@\n-    cf->do_code_blob(_cb);\n+    cf->do_nmethod(_cb->as_nmethod());\n@@ -1447,1 +1442,1 @@\n-  } else if (cb()->is_compiled()) {\n+  } else if (cb()->is_nmethod()) {\n@@ -1449,1 +1444,1 @@\n-    CompiledMethod* cm = cb()->as_compiled_method();\n+    nmethod* nm = cb()->as_nmethod();\n@@ -1452,2 +1447,2 @@\n-                                       p2i(cm),\n-                                       cm->method()->name_and_sig_as_C_string(),\n+                                       p2i(nm),\n+                                       nm->method()->name_and_sig_as_C_string(),\n@@ -1460,1 +1455,1 @@\n-      Method* m = cm->method();\n+      Method* m = nm->method();\n@@ -1462,1 +1457,1 @@\n-      int stack_slot_offset = cm->frame_size() * wordSize; \/\/ offset, in bytes, to caller sp\n+      int stack_slot_offset = nm->frame_size() * wordSize; \/\/ offset, in bytes, to caller sp\n@@ -1513,1 +1508,1 @@\n-      for (ScopeDesc* scope = cm->scope_desc_at(pc()); scope != nullptr; scope = scope->sender(), scope_no++) {\n+      for (ScopeDesc* scope = nm->scope_desc_at(pc()); scope != nullptr; scope = scope->sender(), scope_no++) {\n@@ -1551,1 +1546,1 @@\n-    if (cm->method()->is_continuation_enter_intrinsic()) {\n+    if (nm->method()->is_continuation_enter_intrinsic()) {\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":39,"deletions":44,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,1 +43,0 @@\n-class CompiledMethod;\n@@ -125,0 +124,5 @@\n+  \/\/ Return the original PC for the given PC if:\n+  \/\/ (a) the given PC belongs to an nmethod and\n+  \/\/ (b) it is a deopt PC\n+  address get_deopt_original_pc() const;\n+\n@@ -455,1 +459,1 @@\n-  void oops_do_internal(OopClosure* f, CodeBlobClosure* cf,\n+  void oops_do_internal(OopClosure* f, NMethodClosure* cf,\n@@ -460,3 +464,3 @@\n-  void oops_code_blob_do(OopClosure* f, CodeBlobClosure* cf,\n-                         DerivedOopClosure* df, DerivedPointerIterationMode derived_mode,\n-                         const RegisterMap* map) const;\n+  void oops_nmethod_do(OopClosure* f, NMethodClosure* cf,\n+                       DerivedOopClosure* df, DerivedPointerIterationMode derived_mode,\n+                       const RegisterMap* map) const;\n@@ -465,1 +469,1 @@\n-  void oops_do(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* map) {\n+  void oops_do(OopClosure* f, NMethodClosure* cf, const RegisterMap* map) {\n@@ -476,1 +480,1 @@\n-  void oops_do(OopClosure* f, CodeBlobClosure* cf, DerivedOopClosure* df, const RegisterMap* map) {\n+  void oops_do(OopClosure* f, NMethodClosure* cf, DerivedOopClosure* df, const RegisterMap* map) {\n@@ -480,1 +484,1 @@\n-  void oops_do(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* map,\n+  void oops_do(OopClosure* f, NMethodClosure* cf, const RegisterMap* map,\n@@ -485,1 +489,1 @@\n-  void nmethods_do(CodeBlobClosure* cf) const;\n+  void nmethod_do(NMethodClosure* cf) const;\n","filename":"src\/hotspot\/share\/runtime\/frame.hpp","additions":14,"deletions":10,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -1294,2 +1294,2 @@\n-        address pc = fst.current()->pc();\n-        nmethod* nm =  (nmethod*) fst.current()->cb();\n+        address    pc = fst.current()->pc();\n+        nmethod*   nm = fst.current()->cb()->as_nmethod();\n@@ -1337,0 +1337,1 @@\n+      assert(nm != nullptr, \"did not find nmethod\");\n@@ -1385,1 +1386,1 @@\n-void JavaThread::oops_do_no_frames(OopClosure* f, CodeBlobClosure* cf) {\n+void JavaThread::oops_do_no_frames(OopClosure* f, NMethodClosure* cf) {\n@@ -1443,1 +1444,1 @@\n-void JavaThread::oops_do_frames(OopClosure* f, CodeBlobClosure* cf) {\n+void JavaThread::oops_do_frames(OopClosure* f, NMethodClosure* cf) {\n@@ -1462,1 +1463,1 @@\n-void JavaThread::nmethods_do(CodeBlobClosure* cf) {\n+void JavaThread::nmethods_do(NMethodClosure* cf) {\n@@ -1469,1 +1470,1 @@\n-      fst.current()->nmethods_do(cf);\n+      fst.current()->nmethod_do(cf);\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":7,"deletions":6,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -123,1 +123,1 @@\n-  CompiledMethod*       _deopt_nmethod;         \/\/ CompiledMethod that is currently being deoptimized\n+  nmethod*      _deopt_nmethod;                  \/\/ nmethod that is currently being deoptimized\n@@ -690,2 +690,2 @@\n-  void set_deopt_compiled_method(CompiledMethod* nm)  { _deopt_nmethod = nm; }\n-  CompiledMethod* deopt_compiled_method()        { return _deopt_nmethod; }\n+  void set_deopt_compiled_method(nmethod* nm)    { _deopt_nmethod = nm; }\n+  nmethod* deopt_compiled_method()               { return _deopt_nmethod; }\n@@ -899,2 +899,2 @@\n-  void oops_do_frames(OopClosure* f, CodeBlobClosure* cf);\n-  void oops_do_no_frames(OopClosure* f, CodeBlobClosure* cf);\n+  void oops_do_frames(OopClosure* f, NMethodClosure* cf);\n+  void oops_do_no_frames(OopClosure* f, NMethodClosure* cf);\n@@ -903,1 +903,1 @@\n-  virtual void nmethods_do(CodeBlobClosure* cf);\n+  virtual void nmethods_do(NMethodClosure* cf);\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -338,1 +338,1 @@\n-    return Universe::typeArrayKlassObj(type);\n+    return Universe::typeArrayKlass(type);\n","filename":"src\/hotspot\/share\/runtime\/reflection.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -848,2 +848,2 @@\n-  assert(cb != nullptr && cb->is_compiled(), \"return address should be in nmethod\");\n-  CompiledMethod* nm = (CompiledMethod*)cb;\n+  assert(cb != nullptr && cb->is_nmethod(), \"return address should be in nmethod\");\n+  nmethod* nm = cb->as_nmethod();\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-#include \"code\/compiledMethod.inline.hpp\"\n+#include \"code\/nmethod.inline.hpp\"\n@@ -491,1 +491,1 @@\n-  CompiledMethod* nm = (blob != nullptr) ? blob->as_compiled_method_or_null() : nullptr;\n+  nmethod* nm = (blob != nullptr) ? blob->as_nmethod_or_null() : nullptr;\n@@ -564,1 +564,1 @@\n-  guarantee(cb != nullptr && cb->is_compiled(), \"safepoint polling: pc must refer to an nmethod\");\n+  guarantee(cb != nullptr && cb->is_nmethod(), \"safepoint polling: pc must refer to an nmethod\");\n@@ -567,1 +567,1 @@\n-  assert(((CompiledMethod*)cb)->is_at_poll_or_poll_return(pc),\n+  assert(cb->as_nmethod()->is_at_poll_or_poll_return(pc),\n@@ -578,2 +578,2 @@\n-  bool at_poll_return = ((CompiledMethod*)cb)->is_at_poll_return(pc);\n-  bool has_wide_vectors = ((CompiledMethod*)cb)->has_wide_vectors();\n+  bool at_poll_return = cb->as_nmethod()->is_at_poll_return(pc);\n+  bool has_wide_vectors = cb->as_nmethod()->has_wide_vectors();\n@@ -689,1 +689,1 @@\n-address SharedRuntime::compute_compiled_exc_handler(CompiledMethod* cm, address ret_pc, Handle& exception,\n+address SharedRuntime::compute_compiled_exc_handler(nmethod* nm, address ret_pc, Handle& exception,\n@@ -691,1 +691,1 @@\n-  assert(cm != nullptr, \"must exist\");\n+  assert(nm != nullptr, \"must exist\");\n@@ -695,1 +695,1 @@\n-  if (cm->is_compiled_by_jvmci()) {\n+  if (nm->is_compiled_by_jvmci()) {\n@@ -697,2 +697,2 @@\n-    int catch_pco = pointer_delta_as_int(ret_pc, cm->code_begin());\n-    ExceptionHandlerTable table(cm);\n+    int catch_pco = pointer_delta_as_int(ret_pc, nm->code_begin());\n+    ExceptionHandlerTable table(nm);\n@@ -701,1 +701,1 @@\n-      return cm->code_begin() + t->pco();\n+      return nm->code_begin() + t->pco();\n@@ -703,1 +703,1 @@\n-      return Deoptimization::deoptimize_for_missing_exception_handler(cm);\n+      return Deoptimization::deoptimize_for_missing_exception_handler(nm);\n@@ -708,1 +708,0 @@\n-  nmethod* nm = cm->as_nmethod();\n@@ -919,1 +918,1 @@\n-          if (!cb->is_compiled()) {\n+          if (!cb->is_nmethod()) {\n@@ -931,2 +930,2 @@\n-          CompiledMethod* cm = (CompiledMethod*)cb;\n-          if (cm->inlinecache_check_contains(pc)) {\n+          nmethod* nm = cb->as_nmethod();\n+          if (nm->inlinecache_check_contains(pc)) {\n@@ -941,1 +940,1 @@\n-          if (cm->method()->is_method_handle_intrinsic()) {\n+          if (nm->method()->is_method_handle_intrinsic()) {\n@@ -950,1 +949,1 @@\n-          target_pc = cm->continuation_for_implicit_null_exception(pc);\n+          target_pc = nm->continuation_for_implicit_null_exception(pc);\n@@ -961,2 +960,2 @@\n-        CompiledMethod* cm = CodeCache::find_compiled(pc);\n-        guarantee(cm != nullptr, \"must have containing compiled method for implicit division-by-zero exceptions\");\n+        nmethod* nm = CodeCache::find_nmethod(pc);\n+        guarantee(nm != nullptr, \"must have containing compiled method for implicit division-by-zero exceptions\");\n@@ -966,1 +965,1 @@\n-        target_pc = cm->continuation_for_implicit_div0_exception(pc);\n+        target_pc = nm->continuation_for_implicit_div0_exception(pc);\n@@ -1115,1 +1114,1 @@\n-  CompiledMethod* caller = vfst.nm();\n+  nmethod* caller = vfst.nm();\n@@ -1231,1 +1230,1 @@\n-    bool caller_is_c1 = callerFrame.is_compiled_frame() && callerFrame.cb()->is_compiled_by_c1();\n+    bool caller_is_c1 = callerFrame.is_compiled_frame() && callerFrame.cb()->as_nmethod()->is_compiled_by_c1();\n@@ -1338,2 +1337,2 @@\n-  guarantee(caller_cb != nullptr && caller_cb->is_compiled(), \"must be called from compiled method\");\n-  CompiledMethod* caller_nm = caller_cb->as_compiled_method();\n+  guarantee(caller_cb != nullptr && caller_cb->is_nmethod(), \"must be called from compiled method\");\n+  nmethod* caller_nm = caller_cb->as_nmethod();\n@@ -1561,2 +1560,2 @@\n-      enter_special = caller.cb() != nullptr && caller.cb()->is_compiled()\n-        && caller.cb()->as_compiled_method()->method()->is_continuation_enter_intrinsic();\n+      enter_special = caller.cb() != nullptr && caller.cb()->is_nmethod()\n+        && caller.cb()->as_nmethod()->method()->is_continuation_enter_intrinsic();\n@@ -1668,1 +1667,1 @@\n-  CompiledMethod* caller_nm = cb->as_compiled_method();\n+  nmethod* caller_nm = cb->as_nmethod();\n@@ -1698,1 +1697,1 @@\n-    caller_is_c1 = caller.cb()->is_compiled_by_c1();\n+    caller_is_c1 = caller.cb()->as_nmethod()->is_compiled_by_c1();\n@@ -1706,1 +1705,1 @@\n-      (caller.is_native_frame() && ((CompiledMethod*)caller.cb())->method()->is_continuation_enter_intrinsic())) {\n+      (caller.is_native_frame() && caller.cb()->as_nmethod()->method()->is_continuation_enter_intrinsic())) {\n@@ -1710,1 +1709,2 @@\n-    CompiledMethod* caller_nm = CodeCache::find_compiled(pc);\n+    nmethod* caller_nm = CodeCache::find_nmethod(pc);\n+    assert(caller_nm != nullptr, \"did not find caller nmethod\");\n@@ -1840,1 +1840,1 @@\n-  CompiledMethod* callee = method->code();\n+  nmethod* callee = method->code();\n@@ -1849,1 +1849,1 @@\n-  if (cb == nullptr || !cb->is_compiled() || !callee->is_in_use() || callee->is_unloading()) {\n+  if (cb == nullptr || !cb->is_nmethod() || !callee->is_in_use() || callee->is_unloading()) {\n@@ -1853,2 +1853,2 @@\n-  \/\/ The check above makes sure this is a nmethod.\n-  CompiledMethod* caller = cb->as_compiled_method();\n+  \/\/ The check above makes sure this is an nmethod.\n+  nmethod* caller = cb->as_nmethod();\n@@ -3161,1 +3161,1 @@\n-          MutexLocker pl(CompiledMethod_lock, Mutex::_no_safepoint_check_flag);\n+          MutexLocker pl(NMethodState_lock, Mutex::_no_safepoint_check_flag);\n@@ -3432,1 +3432,1 @@\n-  CompiledMethod* nm = nullptr;\n+  nmethod* nm = nullptr;\n@@ -3455,2 +3455,2 @@\n-      if (cb != nullptr && cb->is_compiled()) {\n-        nm = cb->as_compiled_method();\n+      if (cb != nullptr && cb->is_nmethod()) {\n+        nm = cb->as_nmethod();\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":40,"deletions":40,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -183,1 +183,1 @@\n-  static address compute_compiled_exc_handler(CompiledMethod* nm, address ret_pc, Handle& exception,\n+  static address compute_compiled_exc_handler(nmethod* nm, address ret_pc, Handle& exception,\n@@ -332,1 +332,1 @@\n-  static bool handle_ic_miss_helper_internal(Handle receiver, CompiledMethod* caller_nm, const frame& caller_frame,\n+  static bool handle_ic_miss_helper_internal(Handle receiver, nmethod* caller_nm, const frame& caller_frame,\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -249,2 +249,3 @@\n-    ObjectValue* ov = ((ObjectValue *)sv);\n-    bool scalar_replaced = ov->value().is_null();\n+    ObjectValue* ov = (ObjectValue *)sv;\n+    Handle hdl = ov->value();\n+    bool scalar_replaced = hdl.is_null() && ov->is_scalar_replaced();\n@@ -256,1 +257,1 @@\n-    return new StackValue(ov->value(), scalar_replaced ? 1 : 0);\n+    return new StackValue(hdl, scalar_replaced ? 1 : 0);\n","filename":"src\/hotspot\/share\/runtime\/stackValue.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -625,1 +625,1 @@\n-      while (mark.is_neutral()) {\n+      while (mark.is_unlocked()) {\n@@ -648,1 +648,1 @@\n-      if (mark.is_neutral()) {\n+      if (mark.is_unlocked()) {\n@@ -722,1 +722,1 @@\n-          assert(!mark.is_neutral(), \"invariant\");\n+          assert(!mark.is_unlocked(), \"invariant\");\n@@ -1054,1 +1054,1 @@\n-    if (mark.is_neutral() || (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked())) {\n+    if (mark.is_unlocked() || (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked())) {\n@@ -1187,1 +1187,1 @@\n-  assert(mark.is_neutral(), \"sanity check\");\n+  assert(mark.is_unlocked(), \"sanity check\");\n@@ -1220,1 +1220,1 @@\n-  \/\/ assert(mark.is_neutral(), \"sanity check\");\n+  \/\/ assert(mark.is_unlocked(), \"sanity check\");\n@@ -1474,1 +1474,1 @@\n-    \/\/ *  neutral      - Aggressively inflate the object.\n+    \/\/ *  unlocked     - Aggressively inflate the object.\n@@ -1652,1 +1652,1 @@\n-    \/\/ CASE: neutral\n+    \/\/ CASE: unlocked\n@@ -1661,3 +1661,1 @@\n-    \/\/ Catch if the object's header is not neutral (not locked and\n-    \/\/ not marked is what we care about here).\n-    assert(mark.is_neutral(), \"invariant: header=\" INTPTR_FORMAT, mark.value());\n+    assert(mark.is_unlocked(), \"invariant: header=\" INTPTR_FORMAT, mark.value());\n@@ -1686,1 +1684,1 @@\n-      lsh.print_cr(\"inflate(neutral): object=\" INTPTR_FORMAT \", mark=\"\n+      lsh.print_cr(\"inflate(unlocked): object=\" INTPTR_FORMAT \", mark=\"\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":10,"deletions":12,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -1022,1 +1022,1 @@\n-  Events::log(p, \"Thread added: \" INTPTR_FORMAT, p2i(p));\n+  Events::log(Thread::current(), \"Thread added: \" INTPTR_FORMAT, p2i(p));\n@@ -1085,1 +1085,1 @@\n-  Events::log(p, \"Thread exited: \" INTPTR_FORMAT, p2i(p));\n+  Events::log(Thread::current(), \"Thread exited: \" INTPTR_FORMAT, p2i(p));\n@@ -1095,1 +1095,1 @@\n-void Threads::oops_do(OopClosure* f, CodeBlobClosure* cf) {\n+void Threads::oops_do(OopClosure* f, NMethodClosure* cf) {\n@@ -1152,1 +1152,1 @@\n-  CodeBlobClosure* _cf;\n+  NMethodClosure* _cf;\n@@ -1154,1 +1154,1 @@\n-  ParallelOopsDoThreadClosure(OopClosure* f, CodeBlobClosure* cf) : _f(f), _cf(cf) {}\n+  ParallelOopsDoThreadClosure(OopClosure* f, NMethodClosure* cf) : _f(f), _cf(cf) {}\n@@ -1160,1 +1160,1 @@\n-void Threads::possibly_parallel_oops_do(bool is_par, OopClosure* f, CodeBlobClosure* cf) {\n+void Threads::possibly_parallel_oops_do(bool is_par, OopClosure* f, NMethodClosure* cf) {\n@@ -1340,4 +1340,1 @@\n-  cl.do_thread(VMThread::vm_thread());\n-  Universe::heap()->gc_threads_do(&cl);\n-  cl.do_thread(WatcherThread::watcher_thread());\n-  cl.do_thread(AsyncLogWriter::instance());\n+  non_java_threads_do(&cl);\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":7,"deletions":10,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -50,1 +50,0 @@\n-  template(HeapDumpMerge)                         \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -258,1 +258,0 @@\n-  volatile_nonstatic_field(InstanceKlass,      _methods_jmethod_ids,                          jmethodID*)                            \\\n@@ -312,1 +311,1 @@\n-  volatile_nonstatic_field(Method,             _code,                                         CompiledMethod*)                       \\\n+  volatile_nonstatic_field(Method,             _code,                                         nmethod*)                              \\\n@@ -554,11 +553,11 @@\n-  nonstatic_field(CodeBlob,                 _name,                                   const char*)                                    \\\n-  nonstatic_field(CodeBlob,                 _size,                                   int)                                            \\\n-  nonstatic_field(CodeBlob,                 _header_size,                            int)                                            \\\n-  nonstatic_field(CodeBlob,                 _frame_complete_offset,                  int)                                            \\\n-  nonstatic_field(CodeBlob,                 _data_offset,                            int)                                            \\\n-  nonstatic_field(CodeBlob,                 _frame_size,                             int)                                            \\\n-  nonstatic_field(CodeBlob,                 _oop_maps,                               ImmutableOopMapSet*)                            \\\n-  nonstatic_field(CodeBlob,                 _code_begin,                             address)                                        \\\n-  nonstatic_field(CodeBlob,                 _code_end,                               address)                                        \\\n-  nonstatic_field(CodeBlob,                 _content_begin,                          address)                                        \\\n-  nonstatic_field(CodeBlob,                 _data_end,                               address)                                        \\\n+  nonstatic_field(CodeBlob,                    _name,                                         const char*)                           \\\n+  nonstatic_field(CodeBlob,                    _size,                                         int)                                   \\\n+  nonstatic_field(CodeBlob,                    _header_size,                                  int)                                   \\\n+  nonstatic_field(CodeBlob,                    _relocation_size,                              int)                                   \\\n+  nonstatic_field(CodeBlob,                    _content_offset,                               int)                                   \\\n+  nonstatic_field(CodeBlob,                    _code_offset,                                  int)                                   \\\n+  nonstatic_field(CodeBlob,                    _frame_complete_offset,                        int)                                   \\\n+  nonstatic_field(CodeBlob,                    _data_offset,                                  int)                                   \\\n+  nonstatic_field(CodeBlob,                    _frame_size,                                   int)                                   \\\n+  nonstatic_field(CodeBlob,                    _oop_maps,                                     ImmutableOopMapSet*)                   \\\n+  nonstatic_field(CodeBlob,                    _caller_must_gc_arguments,                     bool)                                  \\\n@@ -568,12 +567,0 @@\n-  nonstatic_field(RuntimeStub,                 _caller_must_gc_arguments,                     bool)                                  \\\n-                                                                                                                                     \\\n-  \/********************************************************\/                                                                         \\\n-  \/* CompiledMethod (NOTE: incomplete, but only a little) *\/                                                                         \\\n-  \/********************************************************\/                                                                         \\\n-                                                                                                                                     \\\n-  nonstatic_field(CompiledMethod,                     _method,                                       Method*)                        \\\n-  volatile_nonstatic_field(CompiledMethod,            _exception_cache,                              ExceptionCache*)                \\\n-  nonstatic_field(CompiledMethod,                     _scopes_data_begin,                            address)                        \\\n-  nonstatic_field(CompiledMethod,                     _deopt_handler_begin,                          address)                        \\\n-  nonstatic_field(CompiledMethod,                     _deopt_mh_handler_begin,                       address)                        \\\n-                                                                                                                                     \\\n@@ -584,0 +571,1 @@\n+  nonstatic_field(nmethod,                     _method,                                       Method*)                               \\\n@@ -588,0 +576,2 @@\n+  nonstatic_field(nmethod,                     _deopt_handler_offset,                         int)                                   \\\n+  nonstatic_field(nmethod,                     _deopt_mh_handler_offset,                      int)                                   \\\n@@ -593,0 +583,1 @@\n+  nonstatic_field(nmethod,                     _scopes_data_offset,                           int)                                   \\\n@@ -603,0 +594,1 @@\n+  volatile_nonstatic_field(nmethod,            _exception_cache,                              ExceptionCache*)                       \\\n@@ -1318,2 +1310,1 @@\n-  declare_type(CompiledMethod,           CodeBlob)                        \\\n-  declare_type(nmethod,                  CompiledMethod)                  \\\n+  declare_type(nmethod,                  CodeBlob)                        \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":18,"deletions":27,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -2676,14 +2676,0 @@\n-\/\/ The VM operation wraps DumpMerger so that it could be performed by VM thread\n-class VM_HeapDumpMerge : public VM_Operation {\n-private:\n-  DumpMerger* _merger;\n-public:\n-  VM_HeapDumpMerge(DumpMerger* merger) : _merger(merger) {}\n-  VMOp_Type type() const { return VMOp_HeapDumpMerge; }\n-  \/\/ heap dump merge could happen outside safepoint\n-  virtual bool evaluate_at_safepoint() const { return false; }\n-  void doit() {\n-    _merger->do_merge();\n-  }\n-};\n-\n@@ -3182,11 +3168,4 @@\n-  Thread* current_thread = Thread::current();\n-  if (current_thread->is_AttachListener_thread()) {\n-    \/\/ perform heapdump file merge operation in the current thread prevents us\n-    \/\/ from occupying the VM Thread, which in turn affects the occurrence of\n-    \/\/ GC and other VM operations.\n-    merger.do_merge();\n-  } else {\n-    \/\/ otherwise, performs it by VM thread\n-    VM_HeapDumpMerge op(&merger);\n-    VMThread::execute(&op);\n-  }\n+  \/\/ Perform heapdump file merge operation in the current thread prevents us\n+  \/\/ from occupying the VM Thread, which in turn affects the occurrence of\n+  \/\/ GC and other VM operations.\n+  merger.do_merge();\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":4,"deletions":25,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -183,1 +183,1 @@\n-     * Default is {@code FAIL_ON_DEAD_LABELS} to throw IllegalStateException\n+     * Default is {@code FAIL_ON_DEAD_LABELS} to throw IllegalArgumentException\n","filename":"src\/java.base\/share\/classes\/java\/lang\/classfile\/ClassFile.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -177,6 +177,2 @@\n- *   <li>{@link java.lang.classfile.ClassFile.StackMapsOption}\n- * -- generate stackmaps (default is {@code STACK_MAPS_WHEN_REQUIRED})<\/li>\n- *   <li>{@link java.lang.classfile.ClassFile.DebugElementsOption}\n- * -- processing of debug information, such as local variable metadata (default is {@code PASS_DEBUG}) <\/li>\n- *   <li>{@link java.lang.classfile.ClassFile.LineNumbersOption}\n- * -- processing of line numbers (default is {@code PASS_LINE_NUMBERS}) <\/li>\n+ *   <li>{@link java.lang.classfile.ClassFile.AttributeMapperOption#of(java.util.function.Function)}\n+ * -- specify format of custom attributes<\/li>\n@@ -185,2 +181,0 @@\n- *   <li>{@link java.lang.classfile.ClassFile.ConstantPoolSharingOption}}\n- * -- share constant pool when transforming (default is {@code SHARED_POOL})<\/li>\n@@ -189,2 +183,14 @@\n- *   <li>{@link java.lang.classfile.ClassFile.AttributeMapperOption#of(java.util.function.Function)}\n- * -- specify format of custom attributes<\/li>\n+ *   <li>{@link java.lang.classfile.ClassFile.ConstantPoolSharingOption}}\n+ * -- share constant pool when transforming (default is {@code SHARED_POOL})<\/li>\n+ *   <li>{@link java.lang.classfile.ClassFile.DeadCodeOption}}\n+ * -- patch out unreachable code (default is {@code PATCH_DEAD_CODE})<\/li>\n+ *   <li>{@link java.lang.classfile.ClassFile.DeadLabelsOption}}\n+ * -- filter unresolved labels (default is {@code FAIL_ON_DEAD_LABELS})<\/li>\n+ *   <li>{@link java.lang.classfile.ClassFile.DebugElementsOption}\n+ * -- processing of debug information, such as local variable metadata (default is {@code PASS_DEBUG}) <\/li>\n+ *   <li>{@link java.lang.classfile.ClassFile.LineNumbersOption}\n+ * -- processing of line numbers (default is {@code PASS_LINE_NUMBERS}) <\/li>\n+ *   <li>{@link java.lang.classfile.ClassFile.ShortJumpsOption}\n+ * -- automatically rewrite short jumps to long when necessary (default is {@code FIX_SHORT_JUMPS})<\/li>\n+ *   <li>{@link java.lang.classfile.ClassFile.StackMapsOption}\n+ * -- generate stackmaps (default is {@code STACK_MAPS_WHEN_REQUIRED})<\/li>\n","filename":"src\/java.base\/share\/classes\/java\/lang\/classfile\/package-info.java","additions":16,"deletions":10,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -820,1 +820,2 @@\n-                        case InvokeDynamicInstruction invd -> in.with(leafs(\n+                        case InvokeDynamicInstruction invd -> {\n+                            in.with(leafs(\n@@ -823,4 +824,5 @@\n-                                \"kind\", invd.bootstrapMethod().kind().name(),\n-                                \"owner\", invd.bootstrapMethod().owner().descriptorString(),\n-                                \"method name\", invd.bootstrapMethod().methodName(),\n-                                \"invocation type\", invd.bootstrapMethod().invocationType().descriptorString()));\n+                                \"bootstrap method\", invd.bootstrapMethod().kind().name()\n+                                     + \" \" + Util.toInternalName(invd.bootstrapMethod().owner())\n+                                     + \"::\" + invd.bootstrapMethod().methodName()));\n+                            in.with(list(\"arguments\", \"arg\", invd.bootstrapArgs().stream()));\n+                        }\n@@ -887,1 +889,3 @@\n-                        return map(\"bm\",\n+                        var bmNode = new MapNodeImpl(FLOW, \"bm\");\n+                        bmNode.with(leafs(\n+                                \"index\", bm.bsmIndex(),\n@@ -890,3 +894,4 @@\n-                                \"owner\", mref.owner().name().stringValue(),\n-                                \"name\", mref.nameAndType().name().stringValue(),\n-                                \"type\", mref.nameAndType().type().stringValue());\n+                                \"owner\", mref.owner().asInternalName(),\n+                                \"name\", mref.nameAndType().name().stringValue()));\n+                        bmNode.with(list(\"args\", \"arg\", bm.arguments().stream().map(LoadableConstantEntry::constantValue)));\n+                        return bmNode;\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/classfile\/impl\/ClassPrinterImpl.java","additions":14,"deletions":9,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -83,1 +83,1 @@\n-        @JEP(number=457, title=\"ClassFile API\", status=\"Preview\")\n+        @JEP(number=466, title=\"ClassFile API\", status=\"Second Preview\")\n@@ -87,0 +87,1 @@\n+        LANGUAGE_MODEL,\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/javac\/PreviewFeature.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -968,4 +968,4 @@\n-                \/\/ If this class appears as an anonymous class in a constructor\n-                \/\/ prologue, disable implicit outer instance from being passed.\n-                \/\/ (This would be an illegal access to \"this before super\").\n-                if (ctorProloguePrev && env.tree.hasTag(NEWCLASS)) {\n+                \/\/ If a class declaration appears in a constructor prologue,\n+                \/\/ that means it's either a local class or an anonymous class.\n+                \/\/ Either way, there is no immediately enclosing instance.\n+                if (ctorProloguePrev) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -235,0 +235,6 @@\n+\n+    \/** Releases prior to JDK 23 expect a less precise SwitchBootstraps.typeSwitch signature on the selectorType\n+     *\/\n+    public boolean usesReferenceOnlySelectorTypes() {\n+        return compareTo(Target.JDK1_23) < 0;\n+    }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/Target.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2011, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2011, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -152,1 +152,1 @@\n-    final int methodCodeOffset = getFieldOffset(\"Method::_code\", Integer.class, \"CompiledMethod*\");\n+    final int methodCodeOffset = getFieldOffset(\"Method::_code\", Integer.class, \"nmethod*\");\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -101,1 +101,1 @@\n-  assert_mark_word_print_pattern(h_obj, \"is_neutral no_hash\");\n+  assert_mark_word_print_pattern(h_obj, \"is_unlocked no_hash\");\n@@ -105,1 +105,1 @@\n-  assert_mark_word_print_pattern(h_obj, \"is_neutral hash=0x\");\n+  assert_mark_word_print_pattern(h_obj, \"is_unlocked hash=0x\");\n","filename":"test\/hotspot\/gtest\/oops\/test_markWord.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -81,0 +81,2 @@\n+compiler\/c2\/irTests\/scalarReplacement\/AllocationMergesTests.java 8315003 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -79,1 +79,0 @@\n-javax\/management\/remote\/mandatory\/subjectDelegation\/SubjectDelegation1Test.java 0000000 generic-all\n","filename":"test\/jdk\/ProblemList-Virtual.txt","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -137,1 +137,0 @@\n-java\/awt\/Frame\/ExceptionOnSetExtendedStateTest\/ExceptionOnSetExtendedStateTest.java 8198237 macosx-all\n@@ -540,2 +539,0 @@\n-javax\/management\/remote\/mandatory\/subjectDelegation\/SubjectDelegation1Test.java 8149084 linux-aarch64\n-\n@@ -669,1 +666,1 @@\n-javax\/swing\/JFileChooser\/8194044\/FileSystemRootTest.java 8320944 windows-all\n+javax\/swing\/JFileChooser\/8194044\/FileSystemRootTest.java 8327236 windows-all\n@@ -816,0 +813,1 @@\n+java\/awt\/Frame\/SizeMinimizedTest.java 8305915 linux-x64\n","filename":"test\/jdk\/ProblemList.txt","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -72,388 +72,9 @@\n-                                    case CodeModel com -> mb.withCode(cb -> cb.transforming(CodeStackTracker.of(), cob -> {\n-                                        var labels = new HashMap<Label, Label>();\n-                                        for (var coe : com) {\n-                                            switch (coe) {\n-                                                case ArrayLoadInstruction i -> {\n-                                                    switch (i.typeKind()) {\n-                                                        case ByteType -> cob.baload();\n-                                                        case ShortType -> cob.saload();\n-                                                        case IntType -> cob.iaload();\n-                                                        case FloatType -> cob.faload();\n-                                                        case LongType -> cob.laload();\n-                                                        case DoubleType -> cob.daload();\n-                                                        case ReferenceType -> cob.aaload();\n-                                                        case CharType -> cob.caload();\n-                                                        default -> throw new AssertionError(\"Should not reach here\");\n-                                                    }\n-                                                }\n-                                                case ArrayStoreInstruction i -> {\n-                                                    switch (i.typeKind()) {\n-                                                        case ByteType -> cob.bastore();\n-                                                        case ShortType -> cob.sastore();\n-                                                        case IntType -> cob.iastore();\n-                                                        case FloatType -> cob.fastore();\n-                                                        case LongType -> cob.lastore();\n-                                                        case DoubleType -> cob.dastore();\n-                                                        case ReferenceType -> cob.aastore();\n-                                                        case CharType -> cob.castore();\n-                                                        default -> throw new AssertionError(\"Should not reach here\");\n-                                                    }\n-                                                }\n-                                                case BranchInstruction i -> {\n-                                                    var target = labels.computeIfAbsent(i.target(), l -> cob.newLabel());\n-                                                    switch (i.opcode()) {\n-                                                        case GOTO -> cob.goto_(target);\n-                                                        case GOTO_W -> cob.goto_w(target);\n-                                                        case IF_ACMPEQ -> cob.if_acmpeq(target);\n-                                                        case IF_ACMPNE -> cob.if_acmpne(target);\n-                                                        case IF_ICMPEQ -> cob.if_icmpeq(target);\n-                                                        case IF_ICMPGE -> cob.if_icmpge(target);\n-                                                        case IF_ICMPGT -> cob.if_icmpgt(target);\n-                                                        case IF_ICMPLE -> cob.if_icmple(target);\n-                                                        case IF_ICMPLT -> cob.if_icmplt(target);\n-                                                        case IF_ICMPNE -> cob.if_icmpne(target);\n-                                                        case IFNONNULL -> cob.if_nonnull(target);\n-                                                        case IFNULL -> cob.if_null(target);\n-                                                        case IFEQ -> cob.ifeq(target);\n-                                                        case IFGE -> cob.ifge(target);\n-                                                        case IFGT -> cob.ifgt(target);\n-                                                        case IFLE -> cob.ifle(target);\n-                                                        case IFLT -> cob.iflt(target);\n-                                                        case IFNE -> cob.ifne(target);\n-                                                        default -> throw new AssertionError(\"Should not reach here\");\n-                                                    }\n-                                                }\n-                                                case ConstantInstruction i -> {\n-                                                    if (i.constantValue() == null)\n-                                                        if (pathSwitch.nextBoolean()) cob.aconst_null();\n-                                                        else cob.constantInstruction(null);\n-                                                    else switch (i.constantValue()) {\n-                                                        case Integer iVal -> {\n-                                                            if (iVal == 1 && pathSwitch.nextBoolean()) cob.iconst_1();\n-                                                            else if (iVal == 2 && pathSwitch.nextBoolean()) cob.iconst_2();\n-                                                            else if (iVal == 3 && pathSwitch.nextBoolean()) cob.iconst_3();\n-                                                            else if (iVal == 4 && pathSwitch.nextBoolean()) cob.iconst_4();\n-                                                            else if (iVal == 5 && pathSwitch.nextBoolean()) cob.iconst_5();\n-                                                            else if (iVal == -1 && pathSwitch.nextBoolean()) cob.iconst_m1();\n-                                                            else if (iVal >= -128 && iVal <= 127 && pathSwitch.nextBoolean()) cob.bipush(iVal);\n-                                                            else if (iVal >= -32768 && iVal <= 32767 && pathSwitch.nextBoolean()) cob.sipush(iVal);\n-                                                            else cob.constantInstruction(iVal);\n-                                                        }\n-                                                        case Long lVal -> {\n-                                                            if (lVal == 0 && pathSwitch.nextBoolean()) cob.lconst_0();\n-                                                            else if (lVal == 1 && pathSwitch.nextBoolean()) cob.lconst_1();\n-                                                            else cob.constantInstruction(lVal);\n-                                                        }\n-                                                        case Float fVal -> {\n-                                                            if (fVal == 0.0 && pathSwitch.nextBoolean()) cob.fconst_0();\n-                                                            else if (fVal == 1.0 && pathSwitch.nextBoolean()) cob.fconst_1();\n-                                                            else if (fVal == 2.0 && pathSwitch.nextBoolean()) cob.fconst_2();\n-                                                            else cob.constantInstruction(fVal);\n-                                                        }\n-                                                        case Double dVal -> {\n-                                                            if (dVal == 0.0d && pathSwitch.nextBoolean()) cob.dconst_0();\n-                                                            else if (dVal == 1.0d && pathSwitch.nextBoolean()) cob.dconst_1();\n-                                                            else cob.constantInstruction(dVal);\n-                                                        }\n-                                                        default -> cob.constantInstruction(i.constantValue());\n-                                                    }\n-                                                }\n-                                                case ConvertInstruction i -> {\n-                                                    switch (i.fromType()) {\n-                                                        case DoubleType -> {\n-                                                            switch (i.toType()) {\n-                                                                case FloatType -> cob.d2f();\n-                                                                case IntType -> cob.d2i();\n-                                                                case LongType -> cob.d2l();\n-                                                                default -> throw new AssertionError(\"Should not reach here\");\n-                                                            }\n-                                                        }\n-                                                        case FloatType -> {\n-                                                            switch (i.toType()) {\n-                                                                case DoubleType -> cob.f2d();\n-                                                                case IntType -> cob.f2i();\n-                                                                case LongType -> cob.f2l();\n-                                                                default -> throw new AssertionError(\"Should not reach here\");\n-                                                            }\n-                                                        }\n-                                                        case IntType -> {\n-                                                            switch (i.toType()) {\n-                                                                case ByteType -> cob.i2b();\n-                                                                case CharType -> cob.i2c();\n-                                                                case DoubleType -> cob.i2d();\n-                                                                case FloatType -> cob.i2f();\n-                                                                case LongType -> cob.i2l();\n-                                                                case ShortType -> cob.i2s();\n-                                                                default -> throw new AssertionError(\"Should not reach here\");\n-                                                            }\n-                                                        }\n-                                                        case LongType -> {\n-                                                            switch (i.toType()) {\n-                                                                case DoubleType -> cob.l2d();\n-                                                                case FloatType -> cob.l2f();\n-                                                                case IntType -> cob.l2i();\n-                                                                default -> throw new AssertionError(\"Should not reach here\");\n-                                                            }\n-                                                        }\n-                                                        default -> throw new AssertionError(\"Should not reach here\");\n-                                                    }\n-                                                }\n-                                                case DiscontinuedInstruction.JsrInstruction i ->\n-                                                    cob.with(DiscontinuedInstruction.JsrInstruction.of(i.opcode(), labels.computeIfAbsent(i.target(), l -> cob.newLabel())));\n-                                                case DiscontinuedInstruction.RetInstruction i ->\n-                                                    cob.with(DiscontinuedInstruction.RetInstruction.of(i.opcode(), i.slot()));\n-                                                case FieldInstruction i -> {\n-                                                    if (pathSwitch.nextBoolean()) {\n-                                                        switch (i.opcode()) {\n-                                                            case GETFIELD -> cob.getfield(i.owner().asSymbol(), i.name().stringValue(), i.typeSymbol());\n-                                                            case GETSTATIC -> cob.getstatic(i.owner().asSymbol(), i.name().stringValue(), i.typeSymbol());\n-                                                            case PUTFIELD -> cob.putfield(i.owner().asSymbol(), i.name().stringValue(), i.typeSymbol());\n-                                                            case PUTSTATIC -> cob.putstatic(i.owner().asSymbol(), i.name().stringValue(), i.typeSymbol());\n-                                                            default -> throw new AssertionError(\"Should not reach here\");\n-                                                        }\n-                                                    } else {\n-                                                        switch (i.opcode()) {\n-                                                            case GETFIELD -> cob.getfield(i.field());\n-                                                            case GETSTATIC -> cob.getstatic(i.field());\n-                                                            case PUTFIELD -> cob.putfield(i.field());\n-                                                            case PUTSTATIC -> cob.putstatic(i.field());\n-                                                            default -> throw new AssertionError(\"Should not reach here\");\n-                                                        }\n-                                                    }\n-                                                }\n-                                                case InvokeDynamicInstruction i -> {\n-                                                    if (pathSwitch.nextBoolean()) cob.invokedynamic(i.invokedynamic().asSymbol());\n-                                                    else cob.invokedynamic(i.invokedynamic());\n-                                                }\n-                                                case InvokeInstruction i -> {\n-                                                    if (pathSwitch.nextBoolean()) {\n-                                                        if (i.isInterface()) {\n-                                                            switch (i.opcode()) {\n-                                                                case INVOKEINTERFACE -> cob.invokeinterface(i.owner().asSymbol(), i.name().stringValue(), i.typeSymbol());\n-                                                                case INVOKESPECIAL -> cob.invokespecial(i.owner().asSymbol(), i.name().stringValue(), i.typeSymbol(), true);\n-                                                                case INVOKESTATIC -> cob.invokestatic(i.owner().asSymbol(), i.name().stringValue(), i.typeSymbol(), true);\n-                                                                default -> throw new AssertionError(\"Should not reach here\");\n-                                                            }\n-                                                        } else {\n-                                                            switch (i.opcode()) {\n-                                                                case INVOKESPECIAL -> cob.invokespecial(i.owner().asSymbol(), i.name().stringValue(), i.typeSymbol());\n-                                                                case INVOKESTATIC -> cob.invokestatic(i.owner().asSymbol(), i.name().stringValue(), i.typeSymbol());\n-                                                                case INVOKEVIRTUAL -> cob.invokevirtual(i.owner().asSymbol(), i.name().stringValue(), i.typeSymbol());\n-                                                                default -> throw new AssertionError(\"Should not reach here\");\n-                                                            }\n-                                                        }\n-                                                    } else {\n-                                                        switch (i.method()) {\n-                                                            case InterfaceMethodRefEntry en -> {\n-                                                                switch (i.opcode()) {\n-                                                                        case INVOKEINTERFACE -> cob.invokeinterface(en);\n-                                                                        case INVOKESPECIAL -> cob.invokespecial(en);\n-                                                                        case INVOKESTATIC -> cob.invokestatic(en);\n-                                                                        default -> throw new AssertionError(\"Should not reach here\");\n-                                                                }\n-                                                            }\n-                                                            case MethodRefEntry en -> {\n-                                                                switch (i.opcode()) {\n-                                                                        case INVOKESPECIAL -> cob.invokespecial(en);\n-                                                                        case INVOKESTATIC -> cob.invokestatic(en);\n-                                                                        case INVOKEVIRTUAL -> cob.invokevirtual(en);\n-                                                                        default -> throw new AssertionError(\"Should not reach here\");\n-                                                                }\n-                                                            }\n-                                                            default -> throw new AssertionError(\"Should not reach here\");\n-                                                        }\n-                                                    }\n-                                                }\n-                                                case LoadInstruction i -> {\n-                                                    switch (i.typeKind()) {\n-                                                        case IntType -> cob.iload(i.slot());\n-                                                        case FloatType -> cob.fload(i.slot());\n-                                                        case LongType -> cob.lload(i.slot());\n-                                                        case DoubleType -> cob.dload(i.slot());\n-                                                        case ReferenceType -> cob.aload(i.slot());\n-                                                        default -> throw new AssertionError(\"Should not reach here\");\n-                                                    }\n-                                                }\n-                                                case StoreInstruction i -> {\n-                                                    switch (i.typeKind()) {\n-                                                        case IntType -> cob.istore(i.slot());\n-                                                        case FloatType -> cob.fstore(i.slot());\n-                                                        case LongType -> cob.lstore(i.slot());\n-                                                        case DoubleType -> cob.dstore(i.slot());\n-                                                        case ReferenceType -> cob.astore(i.slot());\n-                                                        default -> throw new AssertionError(\"Should not reach here\");\n-                                                    }\n-                                                }\n-                                                case IncrementInstruction i ->\n-                                                    cob.iinc(i.slot(), i.constant());\n-                                                case LookupSwitchInstruction i ->\n-                                                    cob.lookupswitch(labels.computeIfAbsent(i.defaultTarget(), l -> cob.newLabel()),\n-                                                                     i.cases().stream().map(sc ->\n-                                                                             SwitchCase.of(sc.caseValue(), labels.computeIfAbsent(sc.target(), l -> cob.newLabel()))).toList());\n-                                                case MonitorInstruction i -> {\n-                                                    switch (i.opcode()) {\n-                                                        case MONITORENTER ->  cob.monitorenter();\n-                                                        case MONITOREXIT ->  cob.monitorexit();\n-                                                        default -> throw new AssertionError(\"Should not reach here\");\n-                                                    }\n-                                                }\n-                                                case NewMultiArrayInstruction i -> {\n-                                                    if (pathSwitch.nextBoolean()) {\n-                                                        cob.multianewarray(i.arrayType().asSymbol(), i.dimensions());\n-                                                    } else {\n-                                                        cob.multianewarray(i.arrayType(), i.dimensions());\n-                                                    }\n-                                                }\n-                                                case NewObjectInstruction i -> {\n-                                                    if (pathSwitch.nextBoolean()) {\n-                                                        cob.new_(i.className().asSymbol());\n-                                                    } else {\n-                                                        cob.new_(i.className());\n-                                                    }\n-                                                }\n-                                                case NewPrimitiveArrayInstruction i ->\n-                                                    cob.newarray(i.typeKind());\n-                                                case NewReferenceArrayInstruction i -> {\n-                                                    if (pathSwitch.nextBoolean()) {\n-                                                        cob.anewarray(i.componentType().asSymbol());\n-                                                    } else {\n-                                                        cob.anewarray(i.componentType());\n-                                                    }\n-                                                }\n-                                                case NopInstruction i ->\n-                                                    cob.nop();\n-                                                case OperatorInstruction i -> {\n-                                                    switch (i.opcode()) {\n-                                                        case IADD -> cob.iadd();\n-                                                        case LADD -> cob.ladd();\n-                                                        case FADD -> cob.fadd();\n-                                                        case DADD -> cob.dadd();\n-                                                        case ISUB -> cob.isub();\n-                                                        case LSUB -> cob.lsub();\n-                                                        case FSUB -> cob.fsub();\n-                                                        case DSUB -> cob.dsub();\n-                                                        case IMUL -> cob.imul();\n-                                                        case LMUL -> cob.lmul();\n-                                                        case FMUL -> cob.fmul();\n-                                                        case DMUL -> cob.dmul();\n-                                                        case IDIV -> cob.idiv();\n-                                                        case LDIV -> cob.ldiv();\n-                                                        case FDIV -> cob.fdiv();\n-                                                        case DDIV -> cob.ddiv();\n-                                                        case IREM -> cob.irem();\n-                                                        case LREM -> cob.lrem();\n-                                                        case FREM -> cob.frem();\n-                                                        case DREM -> cob.drem();\n-                                                        case INEG -> cob.ineg();\n-                                                        case LNEG -> cob.lneg();\n-                                                        case FNEG -> cob.fneg();\n-                                                        case DNEG -> cob.dneg();\n-                                                        case ISHL -> cob.ishl();\n-                                                        case LSHL -> cob.lshl();\n-                                                        case ISHR -> cob.ishr();\n-                                                        case LSHR -> cob.lshr();\n-                                                        case IUSHR -> cob.iushr();\n-                                                        case LUSHR -> cob.lushr();\n-                                                        case IAND -> cob.iand();\n-                                                        case LAND -> cob.land();\n-                                                        case IOR -> cob.ior();\n-                                                        case LOR -> cob.lor();\n-                                                        case IXOR -> cob.ixor();\n-                                                        case LXOR -> cob.lxor();\n-                                                        case LCMP -> cob.lcmp();\n-                                                        case FCMPL -> cob.fcmpl();\n-                                                        case FCMPG -> cob.fcmpg();\n-                                                        case DCMPL -> cob.dcmpl();\n-                                                        case DCMPG -> cob.dcmpg();\n-                                                        case ARRAYLENGTH -> cob.arraylength();\n-                                                        default -> throw new AssertionError(\"Should not reach here\");\n-                                                    }\n-                                                }\n-                                                case ReturnInstruction i -> {\n-                                                    switch (i.typeKind()) {\n-                                                        case IntType -> cob.ireturn();\n-                                                        case FloatType -> cob.freturn();\n-                                                        case LongType -> cob.lreturn();\n-                                                        case DoubleType -> cob.dreturn();\n-                                                        case ReferenceType -> cob.areturn();\n-                                                        case VoidType -> cob.return_();\n-                                                        default -> throw new AssertionError(\"Should not reach here\");\n-                                                    }\n-                                                }\n-                                                case StackInstruction i -> {\n-                                                    switch (i.opcode()) {\n-                                                        case POP -> cob.pop();\n-                                                        case POP2 -> cob.pop2();\n-                                                        case DUP -> cob.dup();\n-                                                        case DUP_X1 -> cob.dup_x1();\n-                                                        case DUP_X2 -> cob.dup_x2();\n-                                                        case DUP2 -> cob.dup2();\n-                                                        case DUP2_X1 -> cob.dup2_x1();\n-                                                        case DUP2_X2 -> cob.dup2_x2();\n-                                                        case SWAP -> cob.swap();\n-                                                        default -> throw new AssertionError(\"Should not reach here\");\n-                                                    }\n-                                                }\n-                                                case TableSwitchInstruction i ->\n-                                                    cob.tableswitch(i.lowValue(), i.highValue(),\n-                                                                    labels.computeIfAbsent(i.defaultTarget(), l -> cob.newLabel()),\n-                                                                    i.cases().stream().map(sc ->\n-                                                                            SwitchCase.of(sc.caseValue(), labels.computeIfAbsent(sc.target(), l -> cob.newLabel()))).toList());\n-                                                case ThrowInstruction i -> cob.athrow();\n-                                                case TypeCheckInstruction i -> {\n-                                                    if (pathSwitch.nextBoolean()) {\n-                                                        switch (i.opcode()) {\n-                                                            case CHECKCAST -> cob.checkcast(i.type().asSymbol());\n-                                                            case INSTANCEOF -> cob.instanceof_(i.type().asSymbol());\n-                                                            default -> throw new AssertionError(\"Should not reach here\");\n-                                                        }\n-                                                    } else {\n-                                                        switch (i.opcode()) {\n-                                                            case CHECKCAST -> cob.checkcast(i.type());\n-                                                            case INSTANCEOF -> cob.instanceof_(i.type());\n-                                                            default -> throw new AssertionError(\"Should not reach here\");\n-                                                        }\n-                                                    }\n-                                                }\n-                                                case CharacterRange pi ->\n-                                                    cob.characterRange(labels.computeIfAbsent(pi.startScope(), l -> cob.newLabel()),\n-                                                                       labels.computeIfAbsent(pi.endScope(), l -> cob.newLabel()),\n-                                                                       pi.characterRangeStart(), pi.characterRangeEnd(), pi.flags());\n-                                                case ExceptionCatch pi ->\n-                                                    pi.catchType().ifPresentOrElse(\n-                                                            catchType -> cob.exceptionCatch(labels.computeIfAbsent(pi.tryStart(), l -> cob.newLabel()),\n-                                                                                            labels.computeIfAbsent(pi.tryEnd(), l -> cob.newLabel()),\n-                                                                                            labels.computeIfAbsent(pi.handler(), l -> cob.newLabel()),\n-                                                                                            catchType.asSymbol()),\n-                                                            () -> cob.exceptionCatchAll(labels.computeIfAbsent(pi.tryStart(), l -> cob.newLabel()),\n-                                                                                        labels.computeIfAbsent(pi.tryEnd(), l -> cob.newLabel()),\n-                                                                                        labels.computeIfAbsent(pi.handler(), l -> cob.newLabel())));\n-                                                case LabelTarget pi ->\n-                                                    cob.labelBinding(labels.computeIfAbsent(pi.label(), l -> cob.newLabel()));\n-                                                case LineNumber pi ->\n-                                                    cob.lineNumber(pi.line());\n-                                                case LocalVariable pi ->\n-                                                    cob.localVariable(pi.slot(), pi.name().stringValue(), pi.typeSymbol(),\n-                                                                      labels.computeIfAbsent(pi.startScope(), l -> cob.newLabel()),\n-                                                                       labels.computeIfAbsent(pi.endScope(), l -> cob.newLabel()));\n-                                                case LocalVariableType pi ->\n-                                                    cob.localVariableType(pi.slot(), pi.name().stringValue(),\n-                                                                          Signature.parseFrom(pi.signatureSymbol().signatureString()),\n-                                                                          labels.computeIfAbsent(pi.startScope(), l -> cob.newLabel()),\n-                                                                          labels.computeIfAbsent(pi.endScope(), l -> cob.newLabel()));\n-                                                case RuntimeInvisibleTypeAnnotationsAttribute a ->\n-                                                    cob.with(RuntimeInvisibleTypeAnnotationsAttribute.of(transformTypeAnnotations(a.annotations(), cob, labels)));\n-                                                case RuntimeVisibleTypeAnnotationsAttribute a ->\n-                                                    cob.with(RuntimeVisibleTypeAnnotationsAttribute.of(transformTypeAnnotations(a.annotations(), cob, labels)));\n-                                                case StackMapTableAttribute a ->\n-                                                    throw new AssertionError(\"Unexpected StackMapTableAttribute here\");\n-                                                case CustomAttribute a ->\n-                                                    throw new AssertionError(\"Unexpected custom attribute: \" + a.attributeName());\n-                                            }\n-                                        }\n-                                        com.findAttribute(Attributes.STACK_MAP_TABLE).ifPresent(smta ->\n-                                                    cob.with(StackMapTableAttribute.of(smta.entries().stream().map(fr ->\n-                                                            StackMapFrameInfo.of(labels.computeIfAbsent(fr.target(), l -> cob.newLabel()),\n-                                                                    transformFrameTypeInfos(fr.locals(), cob, labels),\n-                                                                    transformFrameTypeInfos(fr.stack(), cob, labels))).toList())));\n-                                    }));\n+                                    case CodeModel com -> mb.withCode(cob1 ->\n+                                            cob1.transforming(CodeStackTracker.of(), cob2 ->\n+                                            \/\/ second pass transforms unbound to unbound instructions\n+                                            cob2.transforming(new CodeRebuildingTransform(), cob3 ->\n+                                            \/\/ first pass transforms bound to unbound instructions\n+                                            cob3.transforming(new CodeRebuildingTransform(), cob4 -> {\n+                                                com.forEachElement(cob4::with);\n+                                                com.findAttribute(Attributes.STACK_MAP_TABLE).ifPresent(cob4::with);\n+                                            }))));\n@@ -593,0 +214,390 @@\n+\n+    static class CodeRebuildingTransform implements CodeTransform {\n+\n+        final HashMap<Label, Label> labels = new HashMap<>();\n+\n+        @Override\n+        public void accept(CodeBuilder cob, CodeElement coe) {\n+            switch (coe) {\n+                case ArrayLoadInstruction i -> {\n+                    switch (i.typeKind()) {\n+                        case ByteType -> cob.baload();\n+                        case ShortType -> cob.saload();\n+                        case IntType -> cob.iaload();\n+                        case FloatType -> cob.faload();\n+                        case LongType -> cob.laload();\n+                        case DoubleType -> cob.daload();\n+                        case ReferenceType -> cob.aaload();\n+                        case CharType -> cob.caload();\n+                        default -> throw new AssertionError(\"Should not reach here\");\n+                    }\n+                }\n+                case ArrayStoreInstruction i -> {\n+                    switch (i.typeKind()) {\n+                        case ByteType -> cob.bastore();\n+                        case ShortType -> cob.sastore();\n+                        case IntType -> cob.iastore();\n+                        case FloatType -> cob.fastore();\n+                        case LongType -> cob.lastore();\n+                        case DoubleType -> cob.dastore();\n+                        case ReferenceType -> cob.aastore();\n+                        case CharType -> cob.castore();\n+                        default -> throw new AssertionError(\"Should not reach here\");\n+                    }\n+                }\n+                case BranchInstruction i -> {\n+                    var target = labels.computeIfAbsent(i.target(), l -> cob.newLabel());\n+                    switch (i.opcode()) {\n+                        case GOTO -> cob.goto_(target);\n+                        case GOTO_W -> cob.goto_w(target);\n+                        case IF_ACMPEQ -> cob.if_acmpeq(target);\n+                        case IF_ACMPNE -> cob.if_acmpne(target);\n+                        case IF_ICMPEQ -> cob.if_icmpeq(target);\n+                        case IF_ICMPGE -> cob.if_icmpge(target);\n+                        case IF_ICMPGT -> cob.if_icmpgt(target);\n+                        case IF_ICMPLE -> cob.if_icmple(target);\n+                        case IF_ICMPLT -> cob.if_icmplt(target);\n+                        case IF_ICMPNE -> cob.if_icmpne(target);\n+                        case IFNONNULL -> cob.if_nonnull(target);\n+                        case IFNULL -> cob.if_null(target);\n+                        case IFEQ -> cob.ifeq(target);\n+                        case IFGE -> cob.ifge(target);\n+                        case IFGT -> cob.ifgt(target);\n+                        case IFLE -> cob.ifle(target);\n+                        case IFLT -> cob.iflt(target);\n+                        case IFNE -> cob.ifne(target);\n+                        default -> throw new AssertionError(\"Should not reach here\");\n+                    }\n+                }\n+                case ConstantInstruction i -> {\n+                    if (i.constantValue() == null)\n+                        if (pathSwitch.nextBoolean()) cob.aconst_null();\n+                        else cob.constantInstruction(null);\n+                    else switch (i.constantValue()) {\n+                        case Integer iVal -> {\n+                            if (iVal == 1 && pathSwitch.nextBoolean()) cob.iconst_1();\n+                            else if (iVal == 2 && pathSwitch.nextBoolean()) cob.iconst_2();\n+                            else if (iVal == 3 && pathSwitch.nextBoolean()) cob.iconst_3();\n+                            else if (iVal == 4 && pathSwitch.nextBoolean()) cob.iconst_4();\n+                            else if (iVal == 5 && pathSwitch.nextBoolean()) cob.iconst_5();\n+                            else if (iVal == -1 && pathSwitch.nextBoolean()) cob.iconst_m1();\n+                            else if (iVal >= -128 && iVal <= 127 && pathSwitch.nextBoolean()) cob.bipush(iVal);\n+                            else if (iVal >= -32768 && iVal <= 32767 && pathSwitch.nextBoolean()) cob.sipush(iVal);\n+                            else cob.constantInstruction(iVal);\n+                        }\n+                        case Long lVal -> {\n+                            if (lVal == 0 && pathSwitch.nextBoolean()) cob.lconst_0();\n+                            else if (lVal == 1 && pathSwitch.nextBoolean()) cob.lconst_1();\n+                            else cob.constantInstruction(lVal);\n+                        }\n+                        case Float fVal -> {\n+                            if (fVal == 0.0 && pathSwitch.nextBoolean()) cob.fconst_0();\n+                            else if (fVal == 1.0 && pathSwitch.nextBoolean()) cob.fconst_1();\n+                            else if (fVal == 2.0 && pathSwitch.nextBoolean()) cob.fconst_2();\n+                            else cob.constantInstruction(fVal);\n+                        }\n+                        case Double dVal -> {\n+                            if (dVal == 0.0d && pathSwitch.nextBoolean()) cob.dconst_0();\n+                            else if (dVal == 1.0d && pathSwitch.nextBoolean()) cob.dconst_1();\n+                            else cob.constantInstruction(dVal);\n+                        }\n+                        default -> cob.constantInstruction(i.constantValue());\n+                    }\n+                }\n+                case ConvertInstruction i -> {\n+                    switch (i.fromType()) {\n+                        case DoubleType -> {\n+                            switch (i.toType()) {\n+                                case FloatType -> cob.d2f();\n+                                case IntType -> cob.d2i();\n+                                case LongType -> cob.d2l();\n+                                default -> throw new AssertionError(\"Should not reach here\");\n+                            }\n+                        }\n+                        case FloatType -> {\n+                            switch (i.toType()) {\n+                                case DoubleType -> cob.f2d();\n+                                case IntType -> cob.f2i();\n+                                case LongType -> cob.f2l();\n+                                default -> throw new AssertionError(\"Should not reach here\");\n+                            }\n+                        }\n+                        case IntType -> {\n+                            switch (i.toType()) {\n+                                case ByteType -> cob.i2b();\n+                                case CharType -> cob.i2c();\n+                                case DoubleType -> cob.i2d();\n+                                case FloatType -> cob.i2f();\n+                                case LongType -> cob.i2l();\n+                                case ShortType -> cob.i2s();\n+                                default -> throw new AssertionError(\"Should not reach here\");\n+                            }\n+                        }\n+                        case LongType -> {\n+                            switch (i.toType()) {\n+                                case DoubleType -> cob.l2d();\n+                                case FloatType -> cob.l2f();\n+                                case IntType -> cob.l2i();\n+                                default -> throw new AssertionError(\"Should not reach here\");\n+                            }\n+                        }\n+                        default -> throw new AssertionError(\"Should not reach here\");\n+                    }\n+                }\n+                case DiscontinuedInstruction.JsrInstruction i ->\n+                    cob.with(DiscontinuedInstruction.JsrInstruction.of(i.opcode(), labels.computeIfAbsent(i.target(), l -> cob.newLabel())));\n+                case DiscontinuedInstruction.RetInstruction i ->\n+                    cob.with(DiscontinuedInstruction.RetInstruction.of(i.opcode(), i.slot()));\n+                case FieldInstruction i -> {\n+                    if (pathSwitch.nextBoolean()) {\n+                        switch (i.opcode()) {\n+                            case GETFIELD -> cob.getfield(i.owner().asSymbol(), i.name().stringValue(), i.typeSymbol());\n+                            case GETSTATIC -> cob.getstatic(i.owner().asSymbol(), i.name().stringValue(), i.typeSymbol());\n+                            case PUTFIELD -> cob.putfield(i.owner().asSymbol(), i.name().stringValue(), i.typeSymbol());\n+                            case PUTSTATIC -> cob.putstatic(i.owner().asSymbol(), i.name().stringValue(), i.typeSymbol());\n+                            default -> throw new AssertionError(\"Should not reach here\");\n+                        }\n+                    } else {\n+                        switch (i.opcode()) {\n+                            case GETFIELD -> cob.getfield(i.field());\n+                            case GETSTATIC -> cob.getstatic(i.field());\n+                            case PUTFIELD -> cob.putfield(i.field());\n+                            case PUTSTATIC -> cob.putstatic(i.field());\n+                            default -> throw new AssertionError(\"Should not reach here\");\n+                        }\n+                    }\n+                }\n+                case InvokeDynamicInstruction i -> {\n+                    if (pathSwitch.nextBoolean()) cob.invokedynamic(i.invokedynamic().asSymbol());\n+                    else cob.invokedynamic(i.invokedynamic());\n+                }\n+                case InvokeInstruction i -> {\n+                    if (pathSwitch.nextBoolean()) {\n+                        if (i.isInterface()) {\n+                            switch (i.opcode()) {\n+                                case INVOKEINTERFACE -> cob.invokeinterface(i.owner().asSymbol(), i.name().stringValue(), i.typeSymbol());\n+                                case INVOKESPECIAL -> cob.invokespecial(i.owner().asSymbol(), i.name().stringValue(), i.typeSymbol(), true);\n+                                case INVOKESTATIC -> cob.invokestatic(i.owner().asSymbol(), i.name().stringValue(), i.typeSymbol(), true);\n+                                default -> throw new AssertionError(\"Should not reach here\");\n+                            }\n+                        } else {\n+                            switch (i.opcode()) {\n+                                case INVOKESPECIAL -> cob.invokespecial(i.owner().asSymbol(), i.name().stringValue(), i.typeSymbol());\n+                                case INVOKESTATIC -> cob.invokestatic(i.owner().asSymbol(), i.name().stringValue(), i.typeSymbol());\n+                                case INVOKEVIRTUAL -> cob.invokevirtual(i.owner().asSymbol(), i.name().stringValue(), i.typeSymbol());\n+                                default -> throw new AssertionError(\"Should not reach here\");\n+                            }\n+                        }\n+                    } else {\n+                        switch (i.method()) {\n+                            case InterfaceMethodRefEntry en -> {\n+                                switch (i.opcode()) {\n+                                        case INVOKEINTERFACE -> cob.invokeinterface(en);\n+                                        case INVOKESPECIAL -> cob.invokespecial(en);\n+                                        case INVOKESTATIC -> cob.invokestatic(en);\n+                                        default -> throw new AssertionError(\"Should not reach here\");\n+                                }\n+                            }\n+                            case MethodRefEntry en -> {\n+                                switch (i.opcode()) {\n+                                        case INVOKESPECIAL -> cob.invokespecial(en);\n+                                        case INVOKESTATIC -> cob.invokestatic(en);\n+                                        case INVOKEVIRTUAL -> cob.invokevirtual(en);\n+                                        default -> throw new AssertionError(\"Should not reach here\");\n+                                }\n+                            }\n+                            default -> throw new AssertionError(\"Should not reach here\");\n+                        }\n+                    }\n+                }\n+                case LoadInstruction i -> {\n+                    switch (i.typeKind()) {\n+                        case IntType -> cob.iload(i.slot());\n+                        case FloatType -> cob.fload(i.slot());\n+                        case LongType -> cob.lload(i.slot());\n+                        case DoubleType -> cob.dload(i.slot());\n+                        case ReferenceType -> cob.aload(i.slot());\n+                        default -> throw new AssertionError(\"Should not reach here\");\n+                    }\n+                }\n+                case StoreInstruction i -> {\n+                    switch (i.typeKind()) {\n+                        case IntType -> cob.istore(i.slot());\n+                        case FloatType -> cob.fstore(i.slot());\n+                        case LongType -> cob.lstore(i.slot());\n+                        case DoubleType -> cob.dstore(i.slot());\n+                        case ReferenceType -> cob.astore(i.slot());\n+                        default -> throw new AssertionError(\"Should not reach here\");\n+                    }\n+                }\n+                case IncrementInstruction i ->\n+                    cob.iinc(i.slot(), i.constant());\n+                case LookupSwitchInstruction i ->\n+                    cob.lookupswitch(labels.computeIfAbsent(i.defaultTarget(), l -> cob.newLabel()),\n+                                     i.cases().stream().map(sc ->\n+                                             SwitchCase.of(sc.caseValue(), labels.computeIfAbsent(sc.target(), l -> cob.newLabel()))).toList());\n+                case MonitorInstruction i -> {\n+                    switch (i.opcode()) {\n+                        case MONITORENTER -> cob.monitorenter();\n+                        case MONITOREXIT -> cob.monitorexit();\n+                        default -> throw new AssertionError(\"Should not reach here\");\n+                    }\n+                }\n+                case NewMultiArrayInstruction i -> {\n+                    if (pathSwitch.nextBoolean()) {\n+                        cob.multianewarray(i.arrayType().asSymbol(), i.dimensions());\n+                    } else {\n+                        cob.multianewarray(i.arrayType(), i.dimensions());\n+                    }\n+                }\n+                case NewObjectInstruction i -> {\n+                    if (pathSwitch.nextBoolean()) {\n+                        cob.new_(i.className().asSymbol());\n+                    } else {\n+                        cob.new_(i.className());\n+                    }\n+                }\n+                case NewPrimitiveArrayInstruction i ->\n+                    cob.newarray(i.typeKind());\n+                case NewReferenceArrayInstruction i -> {\n+                    if (pathSwitch.nextBoolean()) {\n+                        cob.anewarray(i.componentType().asSymbol());\n+                    } else {\n+                        cob.anewarray(i.componentType());\n+                    }\n+                }\n+                case NopInstruction i ->\n+                    cob.nop();\n+                case OperatorInstruction i -> {\n+                    switch (i.opcode()) {\n+                        case IADD -> cob.iadd();\n+                        case LADD -> cob.ladd();\n+                        case FADD -> cob.fadd();\n+                        case DADD -> cob.dadd();\n+                        case ISUB -> cob.isub();\n+                        case LSUB -> cob.lsub();\n+                        case FSUB -> cob.fsub();\n+                        case DSUB -> cob.dsub();\n+                        case IMUL -> cob.imul();\n+                        case LMUL -> cob.lmul();\n+                        case FMUL -> cob.fmul();\n+                        case DMUL -> cob.dmul();\n+                        case IDIV -> cob.idiv();\n+                        case LDIV -> cob.ldiv();\n+                        case FDIV -> cob.fdiv();\n+                        case DDIV -> cob.ddiv();\n+                        case IREM -> cob.irem();\n+                        case LREM -> cob.lrem();\n+                        case FREM -> cob.frem();\n+                        case DREM -> cob.drem();\n+                        case INEG -> cob.ineg();\n+                        case LNEG -> cob.lneg();\n+                        case FNEG -> cob.fneg();\n+                        case DNEG -> cob.dneg();\n+                        case ISHL -> cob.ishl();\n+                        case LSHL -> cob.lshl();\n+                        case ISHR -> cob.ishr();\n+                        case LSHR -> cob.lshr();\n+                        case IUSHR -> cob.iushr();\n+                        case LUSHR -> cob.lushr();\n+                        case IAND -> cob.iand();\n+                        case LAND -> cob.land();\n+                        case IOR -> cob.ior();\n+                        case LOR -> cob.lor();\n+                        case IXOR -> cob.ixor();\n+                        case LXOR -> cob.lxor();\n+                        case LCMP -> cob.lcmp();\n+                        case FCMPL -> cob.fcmpl();\n+                        case FCMPG -> cob.fcmpg();\n+                        case DCMPL -> cob.dcmpl();\n+                        case DCMPG -> cob.dcmpg();\n+                        case ARRAYLENGTH -> cob.arraylength();\n+                        default -> throw new AssertionError(\"Should not reach here\");\n+                    }\n+                }\n+                case ReturnInstruction i -> {\n+                    switch (i.typeKind()) {\n+                        case IntType -> cob.ireturn();\n+                        case FloatType -> cob.freturn();\n+                        case LongType -> cob.lreturn();\n+                        case DoubleType -> cob.dreturn();\n+                        case ReferenceType -> cob.areturn();\n+                        case VoidType -> cob.return_();\n+                        default -> throw new AssertionError(\"Should not reach here\");\n+                    }\n+                }\n+                case StackInstruction i -> {\n+                    switch (i.opcode()) {\n+                        case POP -> cob.pop();\n+                        case POP2 -> cob.pop2();\n+                        case DUP -> cob.dup();\n+                        case DUP_X1 -> cob.dup_x1();\n+                        case DUP_X2 -> cob.dup_x2();\n+                        case DUP2 -> cob.dup2();\n+                        case DUP2_X1 -> cob.dup2_x1();\n+                        case DUP2_X2 -> cob.dup2_x2();\n+                        case SWAP -> cob.swap();\n+                        default -> throw new AssertionError(\"Should not reach here\");\n+                    }\n+                }\n+                case TableSwitchInstruction i ->\n+                    cob.tableswitch(i.lowValue(), i.highValue(),\n+                                    labels.computeIfAbsent(i.defaultTarget(), l -> cob.newLabel()),\n+                                    i.cases().stream().map(sc ->\n+                                            SwitchCase.of(sc.caseValue(), labels.computeIfAbsent(sc.target(), l -> cob.newLabel()))).toList());\n+                case ThrowInstruction i -> cob.athrow();\n+                case TypeCheckInstruction i -> {\n+                    if (pathSwitch.nextBoolean()) {\n+                        switch (i.opcode()) {\n+                            case CHECKCAST -> cob.checkcast(i.type().asSymbol());\n+                            case INSTANCEOF -> cob.instanceof_(i.type().asSymbol());\n+                            default -> throw new AssertionError(\"Should not reach here\");\n+                        }\n+                    } else {\n+                        switch (i.opcode()) {\n+                            case CHECKCAST -> cob.checkcast(i.type());\n+                            case INSTANCEOF -> cob.instanceof_(i.type());\n+                            default -> throw new AssertionError(\"Should not reach here\");\n+                        }\n+                    }\n+                }\n+                case CharacterRange pi ->\n+                    cob.characterRange(labels.computeIfAbsent(pi.startScope(), l -> cob.newLabel()),\n+                                       labels.computeIfAbsent(pi.endScope(), l -> cob.newLabel()),\n+                                       pi.characterRangeStart(), pi.characterRangeEnd(), pi.flags());\n+                case ExceptionCatch pi ->\n+                    pi.catchType().ifPresentOrElse(\n+                            catchType -> cob.exceptionCatch(labels.computeIfAbsent(pi.tryStart(), l -> cob.newLabel()),\n+                                                            labels.computeIfAbsent(pi.tryEnd(), l -> cob.newLabel()),\n+                                                            labels.computeIfAbsent(pi.handler(), l -> cob.newLabel()),\n+                                                            catchType.asSymbol()),\n+                            () -> cob.exceptionCatchAll(labels.computeIfAbsent(pi.tryStart(), l -> cob.newLabel()),\n+                                                        labels.computeIfAbsent(pi.tryEnd(), l -> cob.newLabel()),\n+                                                        labels.computeIfAbsent(pi.handler(), l -> cob.newLabel())));\n+                case LabelTarget pi ->\n+                    cob.labelBinding(labels.computeIfAbsent(pi.label(), l -> cob.newLabel()));\n+                case LineNumber pi ->\n+                    cob.lineNumber(pi.line());\n+                case LocalVariable pi ->\n+                    cob.localVariable(pi.slot(), pi.name().stringValue(), pi.typeSymbol(),\n+                                      labels.computeIfAbsent(pi.startScope(), l -> cob.newLabel()),\n+                                       labels.computeIfAbsent(pi.endScope(), l -> cob.newLabel()));\n+                case LocalVariableType pi ->\n+                    cob.localVariableType(pi.slot(), pi.name().stringValue(),\n+                                          Signature.parseFrom(pi.signatureSymbol().signatureString()),\n+                                          labels.computeIfAbsent(pi.startScope(), l -> cob.newLabel()),\n+                                          labels.computeIfAbsent(pi.endScope(), l -> cob.newLabel()));\n+                case RuntimeInvisibleTypeAnnotationsAttribute a ->\n+                    cob.with(RuntimeInvisibleTypeAnnotationsAttribute.of(transformTypeAnnotations(a.annotations(), cob, labels)));\n+                case RuntimeVisibleTypeAnnotationsAttribute a ->\n+                    cob.with(RuntimeVisibleTypeAnnotationsAttribute.of(transformTypeAnnotations(a.annotations(), cob, labels)));\n+                case StackMapTableAttribute a ->\n+                    cob.with(StackMapTableAttribute.of(a.entries().stream().map(fr ->\n+                            StackMapFrameInfo.of(labels.computeIfAbsent(fr.target(), l -> cob.newLabel()),\n+                                    transformFrameTypeInfos(fr.locals(), cob, labels),\n+                                    transformFrameTypeInfos(fr.stack(), cob, labels))).toList()));\n+                case CustomAttribute a ->\n+                    throw new AssertionError(\"Unexpected custom attribute: \" + a.attributeName());\n+            }\n+        }\n+    }\n","filename":"test\/jdk\/jdk\/classfile\/helpers\/RebuildingTransformation.java","additions":400,"deletions":389,"binary":false,"changes":789,"status":"modified"},{"patch":"@@ -411,26 +411,0 @@\n-    \/\/ local class declared before super(), but not used until after super()\n-    public static class Test20 {\n-        public Test20() {\n-            class Foo {\n-                Foo() {\n-                    Test20.this.hashCode();\n-                }\n-            }\n-            super();\n-            new Foo();\n-        }\n-    }\n-\n-    \/\/ local class inside super() parameter list\n-    public static class Test21 extends AtomicReference<Object> {\n-        private int x;\n-        public Test21() {\n-            super(switch (\"foo\".hashCode()) {\n-                default -> {\n-                    class Nested {{ System.out.println(x); }}       \/\/ class is NOT instantiated - OK\n-                    yield \"bar\";\n-                }\n-            });\n-        }\n-    }\n-\n@@ -478,2 +452,0 @@\n-        new Test20();\n-        new Test21();\n","filename":"test\/langtools\/tools\/javac\/SuperInit\/SuperInitGood.java","additions":0,"deletions":28,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -118,1 +118,2 @@\n-    public static abstract class AbstractAnnotationValueVisitor<R, P> extends AbstractAnnotationValueVisitor14<R, P> {\n+    @SuppressWarnings(\"preview\")\n+    public static abstract class AbstractAnnotationValueVisitor<R, P> extends AbstractAnnotationValueVisitorPreview<R, P> {\n@@ -129,1 +130,2 @@\n-    public static abstract class AbstractElementVisitor<R, P> extends AbstractElementVisitor14<R, P> {\n+    @SuppressWarnings(\"preview\")\n+    public static abstract class AbstractElementVisitor<R, P> extends AbstractElementVisitorPreview<R, P> {\n@@ -139,1 +141,2 @@\n-    public static abstract class AbstractTypeVisitor<R, P> extends AbstractTypeVisitor14<R, P> {\n+    @SuppressWarnings(\"preview\")\n+    public static abstract class AbstractTypeVisitor<R, P> extends AbstractTypeVisitorPreview<R, P> {\n@@ -149,1 +152,2 @@\n-    public static class ElementKindVisitor<R, P> extends ElementKindVisitor14<R, P> {\n+    @SuppressWarnings(\"preview\")\n+    public static class ElementKindVisitor<R, P> extends ElementKindVisitorPreview<R, P> {\n@@ -170,1 +174,2 @@\n-    public static class ElementScanner<R, P> extends ElementScanner14<R, P> {\n+    @SuppressWarnings(\"preview\")\n+    public static class ElementScanner<R, P> extends ElementScannerPreview<R, P> {\n@@ -189,1 +194,2 @@\n-    public static class SimpleAnnotationValueVisitor<R, P> extends SimpleAnnotationValueVisitor14<R, P> {\n+    @SuppressWarnings(\"preview\")\n+    public static class SimpleAnnotationValueVisitor<R, P> extends SimpleAnnotationValueVisitorPreview<R, P> {\n@@ -210,1 +216,2 @@\n-    public static class SimpleElementVisitor<R, P> extends SimpleElementVisitor14<R, P> {\n+    @SuppressWarnings(\"preview\")\n+    public static class SimpleElementVisitor<R, P> extends SimpleElementVisitorPreview<R, P> {\n@@ -231,1 +238,2 @@\n-    public static class SimpleTypeVisitor<R, P> extends SimpleTypeVisitor14<R, P> {\n+    @SuppressWarnings(\"preview\")\n+    public static class SimpleTypeVisitor<R, P> extends SimpleTypeVisitorPreview<R, P> {\n@@ -252,1 +260,2 @@\n-    public static class TypeKindVisitor<R, P> extends TypeKindVisitor14<R, P> {\n+    @SuppressWarnings(\"preview\")\n+    public static class TypeKindVisitor<R, P> extends TypeKindVisitorPreview<R, P> {\n","filename":"test\/langtools\/tools\/javac\/lib\/JavacTestingAbstractProcessor.java","additions":18,"deletions":9,"binary":false,"changes":27,"status":"modified"}]}