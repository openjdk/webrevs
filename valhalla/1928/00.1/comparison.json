{"files":[{"patch":"@@ -58,4 +58,4 @@\n-          - arm\n-          - s390x\n-          - ppc64le\n-          - riscv64\n+          # - arm\n+          # - s390x\n+          # - ppc64le\n+          # - riscv64\n@@ -69,25 +69,25 @@\n-          - target-cpu: arm\n-            gnu-arch: arm\n-            debian-arch: armhf\n-            debian-repository: https:\/\/httpredir.debian.org\/debian\/\n-            debian-version: trixie\n-            tolerate-sysroot-errors: false\n-            gnu-abi: eabihf\n-          - target-cpu: s390x\n-            gnu-arch: s390x\n-            debian-arch: s390x\n-            debian-repository: https:\/\/httpredir.debian.org\/debian\/\n-            debian-version: trixie\n-            tolerate-sysroot-errors: false\n-          - target-cpu: ppc64le\n-            gnu-arch: powerpc64le\n-            debian-arch: ppc64el\n-            debian-repository: https:\/\/httpredir.debian.org\/debian\/\n-            debian-version: trixie\n-            tolerate-sysroot-errors: false\n-          - target-cpu: riscv64\n-            gnu-arch: riscv64\n-            debian-arch: riscv64\n-            debian-repository: https:\/\/httpredir.debian.org\/debian\/\n-            debian-version: trixie\n-            tolerate-sysroot-errors: false\n+          # - target-cpu: arm\n+          #   gnu-arch: arm\n+          #   debian-arch: armhf\n+          #   debian-repository: https:\/\/httpredir.debian.org\/debian\/\n+          #   debian-version: trixie\n+          #   tolerate-sysroot-errors: false\n+          #   gnu-abi: eabihf\n+          # - target-cpu: s390x\n+          #   gnu-arch: s390x\n+          #   debian-arch: s390x\n+          #   debian-repository: https:\/\/httpredir.debian.org\/debian\/\n+          #   debian-version: trixie\n+          #   tolerate-sysroot-errors: false\n+          # - target-cpu: ppc64le\n+          #   gnu-arch: powerpc64le\n+          #   debian-arch: ppc64el\n+          #   debian-repository: https:\/\/httpredir.debian.org\/debian\/\n+          #   debian-version: trixie\n+          #   tolerate-sysroot-errors: false\n+          # - target-cpu: riscv64\n+          #   gnu-arch: riscv64\n+          #   debian-arch: riscv64\n+          #   debian-repository: https:\/\/httpredir.debian.org\/debian\/\n+          #   debian-version: trixie\n+          #   tolerate-sysroot-errors: false\n","filename":".github\/workflows\/build-cross-compile.yml","additions":29,"deletions":29,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -146,0 +146,1 @@\n+          --disable-jvm-feature-shenandoahgc\n","filename":".github\/workflows\/build-linux.yml","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -113,0 +113,1 @@\n+          --disable-jvm-feature-shenandoahgc\n","filename":".github\/workflows\/build-macos.yml","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -137,0 +137,1 @@\n+          --disable-jvm-feature-shenandoahgc\n","filename":".github\/workflows\/build-windows.yml","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -219,14 +219,14 @@\n-  build-linux-x64-hs-zero:\n-    name: linux-x64-hs-zero\n-    needs: prepare\n-    uses: .\/.github\/workflows\/build-linux.yml\n-    with:\n-      platform: linux-x64\n-      make-target: 'hotspot'\n-      debug-levels: '[ \"debug\" ]'\n-      gcc-major-version: '10'\n-      extra-conf-options: '--with-jvm-variants=zero --disable-precompiled-headers'\n-      configure-arguments: ${{ github.event.inputs.configure-arguments }}\n-      make-arguments: ${{ github.event.inputs.make-arguments }}\n-      dry-run: ${{ needs.prepare.outputs.dry-run == 'true' }}\n-    if: needs.prepare.outputs.linux-x64-variants == 'true'\n+  # build-linux-x64-hs-zero:\n+  #   name: linux-x64-hs-zero\n+  #   needs: prepare\n+  #   uses: .\/.github\/workflows\/build-linux.yml\n+  #   with:\n+  #     platform: linux-x64\n+  #     make-target: 'hotspot'\n+  #     debug-levels: '[ \"debug\" ]'\n+  #     gcc-major-version: '10'\n+  #     extra-conf-options: '--with-jvm-variants=zero --disable-precompiled-headers'\n+  #     configure-arguments: ${{ github.event.inputs.configure-arguments }}\n+  #     make-arguments: ${{ github.event.inputs.make-arguments }}\n+  #     dry-run: ${{ needs.prepare.outputs.dry-run == 'true' }}\n+  #   if: needs.prepare.outputs.linux-x64-variants == 'true'\n","filename":".github\/workflows\/main.yml","additions":14,"deletions":14,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -97,1 +97,1 @@\n-JAVADOC_OPTIONS := -use -keywords -notimestamp \\\n+JAVADOC_OPTIONS := -XDignore.symbol.file=true -use -keywords -notimestamp \\\n@@ -100,0 +100,1 @@\n+    --enable-preview -source $(JDK_SOURCE_TARGET_VERSION) \\\n","filename":"make\/Docs.gmk","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -66,1 +66,1 @@\n-JAVA_FLAGS_BIG := -Xms64M -Xmx2048M\n+JAVA_FLAGS_BIG := -Xms64M -Xmx3200M\n","filename":"make\/RunTestsPrebuiltSpec.gmk","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,0 +29,3 @@\n+# Valhalla temporarily disabled\n+VALHALLA_TEMP=false\n+\n","filename":"make\/autoconf\/hotspot.m4","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2011, 2025, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2011, 2026, Oracle and\/or its affiliates. All rights reserved.\n@@ -770,0 +770,2 @@\n+# Default disabled within Valhalla until support added (JDK-8348568)\n+#\n","filename":"make\/autoconf\/jdk-options.m4","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -77,0 +77,3 @@\n+  void arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check);\n+  void move(LIR_Opr src, LIR_Opr dst);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -49,0 +49,21 @@\n+void C2_MacroAssembler::entry_barrier() {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ Dummy labels for just measuring the code size\n+  Label dummy_slow_path;\n+  Label dummy_continuation;\n+  Label dummy_guard;\n+  Label* slow_path = &dummy_slow_path;\n+  Label* continuation = &dummy_continuation;\n+  Label* guard = &dummy_guard;\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+    C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n+    Compile::current()->output()->add_stub(stub);\n+    slow_path = &stub->entry();\n+    continuation = &stub->continuation();\n+    guard = &stub->guard();\n+  }\n+  \/\/ In the C2 code, we move the non-hot part of nmethod entry barriers out-of-line to a stub.\n+  bs->nmethod_entry_barrier(this, slow_path, continuation, guard);\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":21,"deletions":0,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -147,2 +147,2 @@\n-inline void ContinuationHelper::Frame::patch_pc(const frame& f, address pc) {\n-  address* pc_addr = &(((address*) f.sp())[-1]);\n+inline void ContinuationHelper::Frame::patch_pc(const frame& f, address pc, bool callee_augmented) {\n+  address* pc_addr = &(((address*) (callee_augmented ? f.unextended_sp() : f.sp()))[-1]);\n","filename":"src\/hotspot\/cpu\/aarch64\/continuationHelper_aarch64.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -38,2 +38,24 @@\n-  int argsize = is_compiled() ? (_cb->as_nmethod()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord : 0;\n-  int frame_size = _cb->frame_size() + argsize;\n+  int frame_size = _cb->frame_size();\n+  if (is_compiled()) {\n+    nmethod* nm = _cb->as_nmethod_or_null();\n+    if (nm->needs_stack_repair() && nm->is_compiled_by_c2()) {\n+      frame f = to_frame();\n+      bool augmented = f.was_augmented_on_entry(frame_size);\n+      if (!augmented) {\n+        \/\/ Fix: C2 caller, so frame was not extended and thus the\n+        \/\/ size read from the frame does not include the arguments.\n+        \/\/ Ideally we have to count the arg size for the scalarized\n+        \/\/ convention. For now we include the size of the caller frame\n+        \/\/ which would at least be equal to that.\n+        RegisterMap map(nullptr,\n+                        RegisterMap::UpdateMap::skip,\n+                        RegisterMap::ProcessFrames::skip,\n+                        RegisterMap::WalkContinuation::skip);\n+        frame caller = to_frame().sender(&map);\n+        assert(caller.is_compiled_frame() && caller.cb()->as_nmethod()->is_compiled_by_c2(), \"needs stack repair but was not extended with c1\/interpreter caller\");\n+        frame_size += (caller.real_fp() - caller.sp());\n+      }\n+    } else {\n+      frame_size += _cb->as_nmethod()->num_stack_arg_slots() * VMRegImpl::stack_slot_size >> LogBytesPerWord;\n+    }\n+  }\n@@ -49,1 +71,7 @@\n-    return frame(sp(), unextended_sp(), fp(), pc(), cb(), _oopmap, true);\n+    frame f = frame(sp(), unextended_sp(), fp(), pc(), cb(), _oopmap, true);\n+    \/\/ If caller tries to get the sender of this frame and PreserveFramePointer\n+    \/\/ is set, fp() will be used which contains the old value at the time of\n+    \/\/ freeze (fp is reconstructed again during thaw). Setting sp as trusted\n+    \/\/ causes the sender code to use _unextended_sp instead (see sender_for_compiled_frame()).\n+    f.set_sp_is_trusted();\n+    return f;\n@@ -57,1 +85,1 @@\n-  return pauth_strip_pointer(*(address*)(_sp - 1));\n+  return pauth_strip_pointer(*(address*)((_callee_augmented ? _unextended_sp : _sp) - 1));\n","filename":"src\/hotspot\/cpu\/aarch64\/stackChunkFrameStream_aarch64.inline.hpp","additions":32,"deletions":4,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -116,1 +116,1 @@\n-inline void ContinuationHelper::Frame::patch_pc(const frame& f, address pc) {\n+inline void ContinuationHelper::Frame::patch_pc(const frame& f, address pc, bool callee_augmented) {\n","filename":"src\/hotspot\/cpu\/ppc\/continuationHelper_ppc.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -134,1 +134,1 @@\n-inline void ContinuationHelper::Frame::patch_pc(const frame& f, address pc) {\n+inline void ContinuationHelper::Frame::patch_pc(const frame& f, address pc, bool callee_augmented) {\n","filename":"src\/hotspot\/cpu\/riscv\/continuationHelper_riscv.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"ci\/ciObjArrayKlass.hpp\"\n@@ -39,0 +41,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -431,1 +434,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -476,0 +479,44 @@\n+  if (InlineTypeReturnedAsFields) {\n+  #ifndef _LP64\n+     Unimplemented();\n+  #endif\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    ciType* return_type = compilation()->method()->return_type();\n+    if (return_type->is_inlinetype()) {\n+      ciInlineKlass* vk = return_type->as_inline_klass();\n+      if (vk->can_be_returned_as_fields()) {\n+        address unpack_handler = vk->unpack_handler();\n+        assert(unpack_handler != nullptr, \"must be\");\n+        __ call(RuntimeAddress(unpack_handler));\n+      }\n+    } else if (return_type->is_instance_klass() && (!return_type->is_loaded() || StressCallingConvention)) {\n+      Label skip;\n+      Label not_null;\n+      __ testptr(rax, rax);\n+      __ jcc(Assembler::notZero, not_null);\n+      \/\/ Returned value is null, zero all return registers because they may belong to oop fields\n+      __ xorq(j_rarg1, j_rarg1);\n+      __ xorq(j_rarg2, j_rarg2);\n+      __ xorq(j_rarg3, j_rarg3);\n+      __ xorq(j_rarg4, j_rarg4);\n+      __ xorq(j_rarg5, j_rarg5);\n+      __ jmp(skip);\n+      __ bind(not_null);\n+\n+      \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+      __ test_oop_is_not_inline_type(rax, rscratch1, skip, \/* can_be_null= *\/ false);\n+\n+      \/\/ Load fields from a buffered value with an inline class specific handler\n+      __ load_klass(rdi, rax, rscratch1);\n+      __ movptr(rdi, Address(rdi, InlineKlass::adr_members_offset()));\n+      __ movptr(rdi, Address(rdi, InlineKlass::unpack_handler_offset()));\n+      \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+      __ testptr(rdi, rdi);\n+      __ jcc(Assembler::zero, skip);\n+      __ call(rdi);\n+\n+      __ bind(skip);\n+    }\n+    \/\/ At this point, rax points to the value object (for interpreter or C1 caller).\n+    \/\/ The fields of the object are copied into registers (for C2 caller).\n+  }\n@@ -478,1 +525,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -494,0 +541,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -1232,1 +1283,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->always_slow_path() ||\n@@ -1311,12 +1362,13 @@\n-  __ testptr(obj, obj);\n-  if (op->should_profile()) {\n-    Label not_null;\n-    Register mdo  = klass_RInfo;\n-    __ mov_metadata(mdo, md->constant_encoding());\n-    __ jccb(Assembler::notEqual, not_null);\n-    \/\/ Object is null; update MDO and exit\n-    Address data_addr(mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n-    int header_bits = BitData::null_seen_byte_constant();\n-    __ orb(data_addr, header_bits);\n-    __ jmp(*obj_is_null);\n-    __ bind(not_null);\n+  if (op->need_null_check()) {\n+    __ testptr(obj, obj);\n+    if (op->should_profile()) {\n+      Label not_null;\n+      Register mdo  = klass_RInfo;\n+      __ mov_metadata(mdo, md->constant_encoding());\n+      __ jccb(Assembler::notEqual, not_null);\n+      \/\/ Object is null; update MDO and exit\n+      Address data_addr(mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n+      int header_bits = BitData::null_seen_byte_constant();\n+      __ orb(data_addr, header_bits);\n+      __ jmp(*obj_is_null);\n+      __ bind(not_null);\n@@ -1327,2 +1379,3 @@\n-  } else {\n-    __ jcc(Assembler::equal, *obj_is_null);\n+    } else {\n+      __ jcc(Assembler::equal, *obj_is_null);\n+    }\n@@ -1339,0 +1392,1 @@\n+    assert(!k->is_loaded() || !k->is_obj_array_klass(), \"Use refined array for a direct pointer comparison\");\n@@ -1363,1 +1417,8 @@\n-        __ cmpptr(klass_RInfo, k_RInfo);\n+        if (k->is_loaded() && k->is_obj_array_klass()) {\n+          \/\/ For a direct pointer comparison, we need the refined array klass pointer\n+          ciKlass* k_refined = ciObjArrayKlass::make(k->as_obj_array_klass()->element_klass());\n+          __ mov_metadata(tmp_load_klass, k_refined->constant_encoding());\n+          __ cmpptr(klass_RInfo, tmp_load_klass);\n+        } else {\n+          __ cmpptr(klass_RInfo, k_RInfo);\n+        }\n@@ -1493,0 +1554,103 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing from\/to an array that *may* be a flat array (the\n+  \/\/ declared type is Object[], abstract[], interface[] or VT.ref[]).\n+  \/\/ If this array is a flat array, take the slow path.\n+  __ test_flat_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+  if (!op->value()->is_illegal()) {\n+    \/\/ TODO 8350865 This is also used for profiling code, right? And in that case we don't care about null but just want to know if the array is flat or not.\n+    \/\/ The array is not a flat array, but it might be null-free. If we are storing\n+    \/\/ a null into a null-free array, take the slow path (which will throw NPE).\n+    Label skip;\n+    __ cmpptr(op->value()->as_register(), NULL_WORD);\n+    __ jcc(Assembler::notEqual, skip);\n+    __ test_null_free_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+    __ bind(skip);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ We are storing into an array that *may* be null-free (the declared type is\n+  \/\/ Object[], abstract[], interface[] or VT.ref[]).\n+  Label test_mark_word;\n+  Register tmp = op->tmp()->as_register();\n+  __ movptr(tmp, Address(op->array()->as_register(), oopDesc::mark_offset_in_bytes()));\n+  __ testl(tmp, markWord::unlocked_value);\n+  __ jccb(Assembler::notZero, test_mark_word);\n+  __ load_prototype_header(tmp, op->array()->as_register(), rscratch1);\n+  __ bind(test_mark_word);\n+  __ testl(tmp, markWord::null_free_array_bit_in_place);\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmpptr(left, right);\n+  __ jcc(Assembler::equal, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  __ testptr(left, right);\n+  __ jcc(Assembler::zero, L_oops_not_equal);\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Inline type check -- if either of the operands is not a inline type,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are inline type\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = op->tmp1()->as_register();\n+    __ movptr(tmp1, (intptr_t)markWord::inline_type_pattern);\n+    __ andptr(tmp1, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(tmp1, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ cmpptr(tmp1, (intptr_t)markWord::inline_type_pattern);\n+    __ jcc(Assembler::notEqual, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ jmp(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedClassPointers) {\n+      __ movl(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ movl(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpl(left_klass_op, right_klass_op);\n+    } else {\n+      __ movptr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ movptr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpptr(left_klass_op, right_klass_op);\n+    }\n+\n+    __ jcc(Assembler::equal, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ jmp(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ jmp(L_end);\n+\n+  \/\/ We've returned from the stub. RAX contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+  __ cmpl(rax, 0);\n+  __ jcc(Assembler::equal, L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+}\n@@ -1538,0 +1702,15 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, nullptr);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n@@ -2148,1 +2327,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2155,1 +2334,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2322,0 +2501,15 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {\n+  if (null_check) {\n+    __ testptr(obj, obj);\n+    __ jcc(Assembler::zero, *slow_path->entry());\n+  }\n+  if (is_dest) {\n+    __ test_null_free_array_oop(obj, tmp, *slow_path->entry());\n+    \/\/ TODO 8350865 Flat no longer implies null-free, so we need to check for flat dest. Can we do better here?\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  } else {\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  }\n+}\n+\n+\n@@ -2341,0 +2535,6 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ jmp(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n@@ -2417,0 +2617,8 @@\n+  \/\/ Handle inline type arrays\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false, (flags & LIR_OpArrayCopy::src_null_check));\n+  }\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true, (flags & LIR_OpArrayCopy::dst_null_check));\n+  }\n+\n@@ -2643,0 +2851,1 @@\n+\n@@ -2956,0 +3165,21 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Register obj = op->obj()->as_register();\n+  Register tmp = op->tmp()->as_pointer_register();\n+  Address mdo_addr = as_Address(op->mdp()->as_address_ptr());\n+  bool not_null = op->not_null();\n+  int flag = op->flag();\n+\n+  Label not_inline_type;\n+  if (!not_null) {\n+    __ testptr(obj, obj);\n+    __ jccb(Assembler::zero, not_inline_type);\n+  }\n+\n+  __ test_oop_is_not_inline_type(obj, tmp, not_inline_type);\n+\n+  __ orb(mdo_addr, flag);\n+\n+  __ bind(not_inline_type);\n+}\n+\n+\n@@ -3141,0 +3371,3 @@\n+void LIR_Assembler::check_orig_pc() {\n+  __ cmpptr(frame_map()->address_for_orig_pc_addr(), NULL_WORD);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":253,"deletions":20,"binary":false,"changes":273,"status":"modified"},{"patch":"@@ -54,0 +54,3 @@\n+  void arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check);\n+  void move(LIR_Opr src, LIR_Opr dst);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -52,1 +52,24 @@\n-void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+\/\/ Beware! This sp_inc is NOT the same as the one mentioned in MacroAssembler::remove_frame but only the size\n+\/\/ of the extension space + the additional copy of the return address. That means, it doesn't contain the\n+\/\/ frame size (where the local and sp_inc are) and the saved RBP.\n+void C2_MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+    Register klass = rscratch1;\n+\n+    mov_metadata(klass, C->method()->holder()->constant_encoding());\n+    clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+\n+    bind(L_skip_barrier);\n+  }\n+\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  bool fp_mode_24b = false;\n+  int stack_bang_size = C->output()->need_stack_bang(bangsize) ? bangsize : 0;\n+\n@@ -97,0 +120,6 @@\n+  if (C->needs_stack_repair()) {\n+    \/\/ Save stack increment just below the saved rbp (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    movptr(Address(rsp, framesize - wordSize), sp_inc + framesize + wordSize);\n+  }\n+\n@@ -115,0 +144,1 @@\n+}\n@@ -116,15 +146,13 @@\n-  if (!is_stub) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n-    Label dummy_slow_path;\n-    Label dummy_continuation;\n-    Label* slow_path = &dummy_slow_path;\n-    Label* continuation = &dummy_continuation;\n-    if (!Compile::current()->output()->in_scratch_emit_size()) {\n-      \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n-      C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n-      Compile::current()->output()->add_stub(stub);\n-      slow_path = &stub->entry();\n-      continuation = &stub->continuation();\n-    }\n-    bs->nmethod_entry_barrier(this, slow_path, continuation);\n+void C2_MacroAssembler::entry_barrier() {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n+  Label dummy_slow_path;\n+  Label dummy_continuation;\n+  Label* slow_path = &dummy_slow_path;\n+  Label* continuation = &dummy_continuation;\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+    C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n+    Compile::current()->output()->add_stub(stub);\n+    slow_path = &stub->entry();\n+    continuation = &stub->continuation();\n@@ -132,0 +160,1 @@\n+  bs->nmethod_entry_barrier(this, slow_path, continuation);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":45,"deletions":16,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -125,2 +125,2 @@\n-inline void ContinuationHelper::Frame::patch_pc(const frame& f, address pc) {\n-  address* pc_addr = &(((address*) f.sp())[-1]);\n+inline void ContinuationHelper::Frame::patch_pc(const frame& f, address pc, bool callee_augmented) {\n+  address* pc_addr = &(((address*) (callee_augmented ? f.unextended_sp() : f.sp()))[-1]);\n","filename":"src\/hotspot\/cpu\/x86\/continuationHelper_x86.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"oops\/constMethodFlags.hpp\"\n@@ -34,0 +35,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -168,1 +170,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -213,1 +215,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n@@ -585,1 +587,2 @@\n-                                                  Label& ok_is_subtype) {\n+                                                  Label& ok_is_subtype,\n+                                                  bool profile) {\n@@ -592,2 +595,3 @@\n-  profile_typecheck(rcx, Rsub_klass); \/\/ blows rcx\n-\n+  if (profile) {\n+    profile_typecheck(rcx, Rsub_klass); \/\/ blows rcx\n+  }\n@@ -886,1 +890,1 @@\n- \/\/ get method access flags\n+  \/\/ get method access flags\n@@ -1025,4 +1029,2 @@\n-  \/\/ remove activation\n-  \/\/ get sender sp\n-  movptr(rbx,\n-         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    movptr(rbx,\n+               Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n@@ -1053,0 +1055,52 @@\n+  \/\/ remove activation\n+  \/\/ get sender sp\n+  movptr(rbx,\n+         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    Label skip;\n+    Label not_null;\n+    testptr(rax, rax);\n+    jcc(Assembler::notZero, not_null);\n+    \/\/ Returned value is null, zero all return registers because they may belong to oop fields\n+    xorq(j_rarg1, j_rarg1);\n+    xorq(j_rarg2, j_rarg2);\n+    xorq(j_rarg3, j_rarg3);\n+    xorq(j_rarg4, j_rarg4);\n+    xorq(j_rarg5, j_rarg5);\n+    jmp(skip);\n+    bind(not_null);\n+\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    test_oop_is_not_inline_type(rax, rscratch1, skip, \/* can_be_null= *\/ false);\n+\n+#ifndef _LP64\n+    super_call_VM_leaf(StubRoutines::load_inline_type_fields_in_regs());\n+#else\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+    load_klass(rdi, rax, rscratch1);\n+    movptr(rdi, Address(rdi, InlineKlass::adr_members_offset()));\n+    movptr(rdi, Address(rdi, InlineKlass::unpack_handler_offset()));\n+    \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+    testptr(rdi, rdi);\n+    jcc(Assembler::zero, skip);\n+    call(rdi);\n+#endif\n+#ifdef ASSERT\n+    \/\/ TODO 8284443 Enable\n+    if (StressCallingConvention && false) {\n+      Label skip_stress;\n+      movptr(rscratch1, Address(rbp, frame::interpreter_frame_method_offset * wordSize));\n+      movl(rscratch1, Address(rscratch1, Method::flags_offset()));\n+      testl(rcx, MethodFlags::has_scalarized_return_flag());\n+      jcc(Assembler::zero, skip_stress);\n+      load_klass(rax, rax, rscratch1);\n+      orptr(rax, 1);\n+      bind(skip_stress);\n+    }\n+#endif\n+    \/\/ call above kills the value in rbx. Reload it.\n+    movptr(rbx, Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    bind(skip);\n+  }\n+\n@@ -1089,0 +1143,89 @@\n+void InterpreterMacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                                  Register t1, Register t2,\n+                                                  bool clear_fields, Label& alloc_failed) {\n+  MacroAssembler::allocate_instance(klass, new_obj, t1, t2, clear_fields, alloc_failed);\n+  if (DTraceAllocProbes) {\n+    \/\/ Trigger dtrace event for fastpath\n+    push(atos);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), new_obj);\n+    pop(atos);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::read_flat_field(Register entry, Register tmp1, Register tmp2, Register obj) {\n+  Label alloc_failed, slow_path, done;\n+  const Register alloc_temp = LP64_ONLY(rscratch1) NOT_LP64(rsi);\n+  const Register dst_temp   = LP64_ONLY(rscratch2) NOT_LP64(rdi);\n+  assert_different_registers(obj, entry, tmp1, tmp2, dst_temp, r8, r9);\n+\n+  \/\/ If the field is nullable, jump to slow path\n+  load_unsigned_byte(tmp1, Address(entry, in_bytes(ResolvedFieldEntry::flags_offset())));\n+  testl(tmp1, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::equal, slow_path);\n+\n+  \/\/ Grap the inline field klass\n+  const Register field_klass = tmp1;\n+  load_unsigned_short(tmp2, Address(entry, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+\n+  movptr(tmp1, Address(entry, ResolvedFieldEntry::field_holder_offset()));\n+  get_inline_type_field_klass(tmp1, tmp2, field_klass);\n+\n+  \/\/ allocate buffer\n+  push(obj);  \/\/ push object being read from\n+  allocate_instance(field_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+\n+  \/\/ Have an oop instance buffer, copy into it\n+  load_unsigned_short(r9, Address(entry, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+  movptr(r8, Address(entry, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+  inline_layout_info(r8, r9, r8); \/\/ holder, index, info => InlineLayoutInfo into r8\n+\n+  payload_addr(obj, dst_temp, field_klass);\n+  pop(alloc_temp);             \/\/ restore object being read from\n+  load_sized_value(tmp2, Address(entry, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n+  lea(tmp2, Address(alloc_temp, tmp2));\n+  \/\/ call_VM_leaf, clobbers a few regs, save restore new obj\n+  push(obj);\n+  flat_field_copy(IS_DEST_UNINITIALIZED, tmp2, dst_temp, r8);\n+  pop(obj);\n+  jmp(done);\n+\n+  bind(alloc_failed);\n+  pop(obj);\n+  bind(slow_path);\n+  call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flat_field),\n+          obj, entry);\n+  get_vm_result_oop(obj);\n+  bind(done);\n+}\n+\n+void InterpreterMacroAssembler::write_flat_field(Register entry, Register tmp1, Register tmp2,\n+                                                 Register obj, Register off, Register value) {\n+  assert_different_registers(entry, tmp1, tmp2, obj, off, value);\n+\n+  Label slow_path, done;\n+\n+  load_unsigned_byte(tmp2, Address(entry, in_bytes(ResolvedFieldEntry::flags_offset())));\n+  test_field_is_not_null_free_inline_type(tmp2, tmp1, slow_path);\n+\n+  null_check(value); \/\/ FIXME JDK-8341120\n+\n+  lea(obj, Address(obj, off, Address::times_1));\n+\n+  load_klass(tmp2, value, tmp1);\n+  payload_addr(value, value, tmp2);\n+\n+  Register idx = tmp1;\n+  load_unsigned_short(idx, Address(entry, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+  movptr(tmp2, Address(entry, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+\n+  Register layout_info = off;\n+  inline_layout_info(tmp2, idx, layout_info);\n+\n+  flat_field_copy(IN_HEAP, value, obj, layout_info);\n+  jmp(done);\n+\n+  bind(slow_path);\n+  call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flat_field), obj, value, entry);\n+\n+  bind(done);\n+}\n@@ -1341,1 +1484,1 @@\n-void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp) {\n+void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp, bool acmp) {\n@@ -1353,1 +1496,1 @@\n-    update_mdp_by_constant(mdp, in_bytes(BranchData::branch_data_size()));\n+    update_mdp_by_constant(mdp, acmp ? in_bytes(ACmpData::acmp_data_size()): in_bytes(BranchData::branch_data_size()));\n@@ -1556,0 +1699,114 @@\n+template <class ArrayData> void InterpreterMacroAssembler::profile_array_type(Register mdp,\n+                                                                              Register array,\n+                                                                              Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, array);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayData::array_offset())));\n+\n+    Label not_flat;\n+    test_non_flat_array_oop(array, tmp, not_flat);\n+\n+    set_mdp_flag_at(mdp, ArrayData::flat_array_byte_constant());\n+\n+    bind(not_flat);\n+\n+    Label not_null_free;\n+    test_non_null_free_array_oop(array, tmp, not_null_free);\n+\n+    set_mdp_flag_at(mdp, ArrayData::null_free_array_byte_constant());\n+\n+    bind(not_null_free);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+template void InterpreterMacroAssembler::profile_array_type<ArrayLoadData>(Register mdp,\n+                                                                           Register array,\n+                                                                           Register tmp);\n+template void InterpreterMacroAssembler::profile_array_type<ArrayStoreData>(Register mdp,\n+                                                                            Register array,\n+                                                                            Register tmp);\n+\n+\n+void InterpreterMacroAssembler::profile_multiple_element_types(Register mdp, Register element, Register tmp, const Register tmp2) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    Label done, update;\n+    testptr(element, element);\n+    jccb(Assembler::notZero, update);\n+    set_mdp_flag_at(mdp, BitData::null_seen_byte_constant());\n+    jmp(done);\n+\n+    bind(update);\n+    load_klass(tmp, element, rscratch1);\n+\n+    \/\/ Record the object type.\n+    profile_receiver_type(tmp, mdp, 0);\n+\n+    bind(done);\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayStoreData::array_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_element_type(Register mdp,\n+                                                     Register element,\n+                                                     Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, element);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadData::element_offset())));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadData::array_load_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_acmp(Register mdp,\n+                                             Register left,\n+                                             Register right,\n+                                             Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, left);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::left_offset())));\n+\n+    Label left_not_inline_type;\n+    test_oop_is_not_inline_type(left, tmp, left_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::left_inline_type_byte_constant());\n+    bind(left_not_inline_type);\n+\n+    mov(tmp, right);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::right_offset())));\n+\n+    Label right_not_inline_type;\n+    test_oop_is_not_inline_type(right, tmp, right_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::right_inline_type_byte_constant());\n+    bind(right_not_inline_type);\n+\n+    bind(profile_continue);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":269,"deletions":12,"binary":false,"changes":281,"status":"modified"},{"patch":"@@ -178,1 +178,1 @@\n-  void gen_subtype_check( Register sub_klass, Label &ok_is_subtype );\n+  void gen_subtype_check(Register sub_klass, Label &ok_is_subtype, bool profile = true);\n@@ -218,0 +218,17 @@\n+  \/\/ Kills t1 and t2, preserves klass, return allocation in new_obj\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n+  \/\/ Allocate instance in \"obj\" and read in the content of the inline field\n+  \/\/ NOTES:\n+  \/\/   - input holder object via \"obj\", which must be rax,\n+  \/\/     will return new instance via the same reg\n+  \/\/   - assumes holder_klass and valueKlass field klass have both been resolved\n+  void read_flat_field(Register entry,\n+                       Register tmp1, Register tmp2,\n+                       Register obj = rax);\n+  void write_flat_field(Register entry,\n+                        Register tmp1, Register tmp2,\n+                        Register obj, Register off, Register value);\n+\n@@ -243,1 +260,1 @@\n-  void profile_not_taken_branch(Register mdp);\n+  void profile_not_taken_branch(Register mdp, bool acmp = false);\n@@ -255,0 +272,5 @@\n+  template <class ArrayData> void profile_array_type(Register mdp, Register array, Register tmp);\n+\n+  void profile_multiple_element_types(Register mdp, Register element, Register tmp, const Register tmp2);\n+  void profile_element_type(Register mdp, Register element, Register tmp);\n+  void profile_acmp(Register mdp, Register left, Register right, Register tmp);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.hpp","additions":24,"deletions":2,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2026, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,0 +31,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -46,0 +47,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -47,0 +49,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -56,0 +59,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -59,0 +63,4 @@\n+#include \"vmreg_x86.inline.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/output.hpp\"\n+#endif\n@@ -1306,0 +1314,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2359,0 +2371,107 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  andptr(markword, markWord::inline_type_mask_in_place);\n+  cmpptr(markword, markWord::inline_type_pattern);\n+  jcc(Assembler::equal, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type, bool can_be_null) {\n+  if (can_be_null) {\n+    testptr(object, object);\n+    jcc(Assembler::zero, not_inline_type);\n+  }\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  movptr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  andptr(tmp, is_inline_type_mask);\n+  cmpptr(tmp, is_inline_type_mask);\n+  jcc(Assembler::notEqual, not_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::notEqual, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::equal, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_flat(Register flags, Register temp_reg, Label& is_flat) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_flat_shift);\n+  jcc(Assembler::notEqual, is_flat);\n+}\n+\n+void MacroAssembler::test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::has_null_marker_shift);\n+  jcc(Assembler::notEqual, has_null_marker);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  movptr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  testl(temp_reg, markWord::unlocked_value);\n+  jccb(Assembler::notZero, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  push(rscratch1);\n+  load_prototype_header(temp_reg, oop, rscratch1);\n+  pop(rscratch1);\n+\n+  bind(test_mark_word);\n+  testl(temp_reg, test_bit);\n+  jcc((jmp_set) ? Assembler::notZero : Assembler::zero, jmp_label);\n+}\n+\n+void MacroAssembler::test_flat_array_oop(Register oop, Register temp_reg,\n+                                         Label& is_flat_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flat_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_flat_array_layout(temp_reg, is_flat_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_flat_array_oop(Register oop, Register temp_reg,\n+                                             Label& is_non_flat_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flat_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_flat_array_layout(temp_reg, is_non_flat_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+\n+void MacroAssembler::test_flat_array_layout(Register lh, Label& is_flat_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::notZero, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_layout(Register lh, Label& is_non_flat_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::zero, is_non_flat_array);\n+}\n+\n@@ -3441,0 +3560,118 @@\n+\/\/ Object \/ value buffer allocation...\n+\/\/\n+\/\/ Kills klass and rsi on LP64\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == rax, \"needs to be rax\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  movl(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it is malformed in some way\n+  testl(layout_size, Klass::_lh_instance_slow_path_bit);\n+  jcc(Assembler::notZero, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path.\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  push(klass);\n+  if (UseTLAB) {\n+    tlab_allocate(new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      jmp(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      jmp(initialize_object);\n+    }\n+  } else {\n+    jmp(slow_case);\n+  }\n+\n+  \/\/ If UseTLAB is true, the object is created above and there is an initialize need.\n+  \/\/ Otherwise, skip and go to the slow path.\n+  if (UseTLAB) {\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      if (UseCompactObjectHeaders) {\n+        assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n+        decrement(layout_size, oopDesc::base_offset_in_bytes());\n+      } else {\n+        decrement(layout_size, sizeof(oopDesc));\n+      }\n+      jcc(Assembler::zero, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+      Register zero = klass;\n+      xorl(zero, zero);    \/\/ use zero reg to clear memory (shorter code)\n+      shrl(layout_size, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      \/\/ Ignore partial flag stall after shrl() since it is debug VM\n+      jcc(Assembler::carryClear, L);\n+      stop(\"object size is not multiple of 2 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        bind(loop);\n+        int header_size_bytes = oopDesc::header_size() * HeapWordSize;\n+        assert(is_aligned(header_size_bytes, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n+        movptr(Address(new_obj, layout_size, Address::times_8, header_size_bytes - 1*oopSize), zero);\n+        decrement(layout_size);\n+        jcc(Assembler::notZero, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    if (UseCompactObjectHeaders || Arguments::is_valhalla_enabled()) {\n+      pop(klass);\n+      Register mark_word = t2;\n+      movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+      movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+    } else {\n+     movptr(Address(new_obj, oopDesc::mark_offset_in_bytes()),\n+            (intptr_t)markWord::prototype().value()); \/\/ header\n+     pop(klass);   \/\/ get saved klass back in the register.\n+    }\n+    if (!UseCompactObjectHeaders) {\n+      xorl(rsi, rsi);                 \/\/ use zero reg to clear memory (shorter code)\n+      store_klass_gap(new_obj, rsi);  \/\/ zero klass gap for compressed oops\n+      movptr(t2, klass);         \/\/ preserve klass\n+      store_klass(new_obj, t2, rscratch1);  \/\/ src klass reg is potentially compressed\n+    }\n+    jmp(done);\n+  }\n+\n+  bind(slow_case);\n+  pop(klass);\n+  bind(slow_case_no_pop);\n+  jmp(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -3644,0 +3881,27 @@\n+void MacroAssembler::get_inline_type_field_klass(Register holder_klass, Register index, Register inline_klass) {\n+  inline_layout_info(holder_klass, index, inline_klass);\n+  movptr(inline_klass, Address(inline_klass, InlineLayoutInfo::klass_offset()));\n+}\n+\n+void MacroAssembler::inline_layout_info(Register holder_klass, Register index, Register layout_info) {\n+  movptr(layout_info, Address(holder_klass, InstanceKlass::inline_layout_info_array_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cmpptr(layout_info, 0);\n+    jcc(Assembler::notEqual, done);\n+    stop(\"inline_layout_info_array is null\");\n+    bind(done);\n+  }\n+#endif\n+\n+  InlineLayoutInfo array[2];\n+  int size = (char*)&array[1] - (char*)&array[0]; \/\/ computing size of array elements\n+  if (is_power_of_2(size)) {\n+    shll(index, log2i_exact(size)); \/\/ Scale index by power of 2\n+  } else {\n+    imull(index, index, size); \/\/ Scale the index to be the entry index * array_element_size\n+  }\n+  lea(layout_info, Address(layout_info, index, Address::times_1, Array<InlineLayoutInfo>::base_offset_in_bytes()));\n+}\n+\n@@ -4694,1 +4958,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -4950,1 +5218,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -5344,0 +5616,10 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(dst, src);\n+  } else if (UseCompressedClassPointers) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -5365,0 +5647,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src, Register tmp) {\n+  load_klass(dst, src, tmp);\n+  movptr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -5430,0 +5717,40 @@\n+void MacroAssembler::flat_field_copy(DecoratorSet decorators, Register src, Register dst,\n+                                     Register inline_layout_info) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->flat_field_copy(this, decorators, src, dst, inline_layout_info);\n+}\n+\n+void MacroAssembler::payload_offset(Register inline_klass, Register offset) {\n+  movptr(offset, Address(inline_klass, InlineKlass::adr_members_offset()));\n+  movl(offset, Address(offset, InlineKlass::payload_offset_offset()));\n+}\n+\n+void MacroAssembler::payload_addr(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->payload_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  payload_offset(inline_klass, offset);\n+  if (data == oop) {\n+    addptr(data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert(index != rcx, \"index needs to shift by rcx\");\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rcx, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  movl(rcx, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  shrl(rcx, Klass::_lh_log2_element_size_shift);\n+  andl(rcx, Klass::_lh_log2_element_size_mask);\n+  shlptr(index); \/\/ index << rcx\n+\n+  lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT)));\n+}\n+\n@@ -5786,0 +6113,476 @@\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  testptr(rax, 1);\n+  jcc(Assembler::zero, skip);\n+  int call_offset = -1;\n+\n+#ifdef _LP64\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != nullptr, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(rscratch1, rax); \/\/ save rax for slow_case since *_allocate may corrupt it when allocation failed\n+  if (vk != nullptr) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(rbx, (intptr_t)vk->get_InlineKlass());\n+    jint lh = vk->layout_helper();\n+    assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB && !Klass::layout_helper_needs_slow_path(lh)) {\n+      tlab_allocate(rax, noreg, lh, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. RAX contains ((the InlineKlass* of the return type) | 0x01)\n+    mov(rbx, rax);\n+    andptr(rbx, -2);\n+    if (UseTLAB) {\n+      movl(r14, Address(rbx, Klass::layout_helper_offset()));\n+      testl(r14, Klass::_lh_instance_slow_path_bit);\n+      jcc(Assembler::notZero, slow_case);\n+      tlab_allocate(rax, r14, 0, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = rax;\n+    Register klass = rbx;\n+    if (UseCompactObjectHeaders) {\n+      Register mark_word = r13;\n+      movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+      movptr(Address(buffer_obj, oopDesc::mark_offset_in_bytes()), mark_word);\n+    } else {\n+      movptr(Address(buffer_obj, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::inline_type_prototype().value());\n+      xorl(r13, r13);\n+      store_klass_gap(buffer_obj, r13);\n+      if (vk == nullptr) {\n+        \/\/ store_klass corrupts rbx(klass), so save it in r13 for later use (interpreter case only).\n+        mov(r13, klass);\n+      }\n+      store_klass(buffer_obj, klass, rscratch1);\n+      klass = r13;\n+    }\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != nullptr) {\n+      call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      movptr(rbx, Address(klass, InlineKlass::adr_members_offset()));\n+      movptr(rbx, Address(rbx, InlineKlass::pack_handler_offset()));\n+      call(rbx);\n+    }\n+    jmp(skip);\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(rax, rscratch1);\n+#endif\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to->as_Register(), from->as_Register());\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        Address to_addr = Address(rsp, st_off);\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to_addr, from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to_addr, from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to_addr, from->as_Register());\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(rsp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      if (to->is_reg()) {\n+        if (to->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from_addr);\n+          }\n+        } else {\n+          movq(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(r13, from_addr);\n+        movq(Address(rsp, st_off), r13);\n+      }\n+    }\n+  }\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer.\n+\/\/\n+\/\/ This extra stack space take into account the copy #2 of the return address,\n+\/\/ but NOT the saved RBP or the normal size of the frame (see MacroAssembler::remove_frame\n+\/\/ for notations).\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  \/\/ Two additional slots to account for return address\n+  int sp_inc = (args_on_stack + 2) * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  \/\/ Save the return address, adjust the stack (make sure it is properly\n+  \/\/ 16-byte aligned) and copy the return address to the new top of the stack.\n+  \/\/ The stack will be repaired on return (see MacroAssembler::remove_frame).\n+  assert(sp_inc > 0, \"sanity\");\n+  pop(r13);\n+  subptr(rsp, sp_inc);\n+#ifdef ASSERT\n+  movl(Address(rsp, -VMRegImpl::stack_slot_size), badRegWordVal);\n+  movl(Address(rsp, -2 * VMRegImpl::stack_slot_size), badRegWordVal);\n+  subptr(rsp, 2 * VMRegImpl::stack_slot_size);\n+#else\n+  push(r13);\n+#endif\n+  return sp_inc;\n+}\n+\n+\/\/ Read all fields from an inline type buffer and store the field values in registers\/stack slots.\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, true);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(tmp1, Address(rsp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        testptr(fromReg, fromReg);\n+        jcc(Assembler::zero, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), 1);\n+      } else {\n+        movq(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_XMMRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), dst);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(toReg->as_XMMRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(toReg->as_XMMRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      jmp(L_notNull);\n+      bind(L_null);\n+      \/\/ Set null marker to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+            movq(Address(rsp, st_off), 0);\n+          } else {\n+            xorq(toReg->as_Register(), toReg->as_Register());\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_METADATA, \"should be at delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r14;\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register tmp3 = rbx;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        testb(Address(rsp, ld_off), 1);\n+      } else {\n+        testb(fromReg->as_Register(), 1);\n+      }\n+      jcc(Assembler::notZero, L_notNull);\n+      movptr(val_obj, 0);\n+      jmp(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+    if (!fromReg->is_XMMRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(src, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        \/\/ store_heap_oop transitively calls oop_store_at which corrupts to.base(). We need to keep val_obj valid.\n+        mov(tmp3, val_obj);\n+        Address dst_with_tmp3(tmp3, off);\n+        store_heap_oop(dst_with_tmp3, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(dst, fromReg->as_XMMRegister());\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(dst, fromReg->as_XMMRegister());\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();\n+}\n+\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  if (needs_stack_repair) {\n+    \/\/ The method has a scalarized entry point (where fields of value object arguments\n+    \/\/ are passed through registers and stack), and a non-scalarized entry point (where\n+    \/\/ value object arguments are given as oops). The non-scalarized entry point will\n+    \/\/ first load each field of value object arguments and store them in registers and on\n+    \/\/ the stack in a way compatible with the scalarized entry point. To do so, some extra\n+    \/\/ stack space might be reserved (if argument registers are not enough). On leaving the\n+    \/\/ method, this space must be freed.\n+    \/\/\n+    \/\/ In case we used the non-scalarized entry point the stack looks like this:\n+    \/\/\n+    \/\/ | Arguments from caller     |\n+    \/\/ |---------------------------|  <-- caller's SP\n+    \/\/ | Return address #1         |\n+    \/\/ |---------------------------|\n+    \/\/ | Extension space for       |\n+    \/\/ |   inline arg (un)packing  |\n+    \/\/ |---------------------------|\n+    \/\/ | Return address #2         |\n+    \/\/ | Saved RBP                 |\n+    \/\/ |---------------------------|  <-- start of this method's frame\n+    \/\/ | sp_inc                    |\n+    \/\/ | method locals             |\n+    \/\/ |---------------------------|  <-- SP\n+    \/\/\n+    \/\/ There is two copies of the return address on the stack. They will be identical at\n+    \/\/ first, but that can change.\n+    \/\/ If the caller has been deoptimized, the copy #1 will be patched to point at the\n+    \/\/ deopt blob, and the copy #2 will still point into the old method. In short\n+    \/\/ the copy #2 is not reliable and should not be used. It is mostly needed to\n+    \/\/ add space between the extension space and the locals, as there would be between\n+    \/\/ the real arguments and the locals if we don't need to do unpacking (from the\n+    \/\/ scalarized entry point).\n+    \/\/\n+    \/\/ When leaving, one must use the copy #1 of the return address, while keeping in mind\n+    \/\/ that from the scalarized entry point, there will be only one copy. Indeed, in the\n+    \/\/ case we used the scalarized calling convention, the stack looks like this:\n+    \/\/\n+    \/\/ | Arguments from caller     |\n+    \/\/ |---------------------------|  <-- caller's SP\n+    \/\/ | Return address            |\n+    \/\/ | Saved RBP                 |\n+    \/\/ |---------------------------|  <-- start of this method's frame\n+    \/\/ | sp_inc                    |\n+    \/\/ | method locals             |\n+    \/\/ |---------------------------|  <-- SP\n+    \/\/\n+    \/\/ The sp_inc stack slot holds the total size of the frame, including the extension\n+    \/\/ space the possible copy #2 of the return address and the saved RBP (but never the\n+    \/\/ copy #1 of the return address). That is how to find the copy #1 of the return address.\n+    \/\/ This size is expressed in bytes. Be careful when using it from C++ in pointer arithmetic;\n+    \/\/ you might need to divide it by wordSize.\n+    \/\/\n+    \/\/ One can find sp_inc since the start the method's frame is SP + initial_framesize.\n+\n+    movq(rbp, Address(rsp, initial_framesize));\n+    \/\/ The stack increment resides just below the saved rbp\n+    addq(rsp, Address(rsp, initial_framesize - wordSize));\n+  } else {\n+    if (initial_framesize > 0) {\n+      addq(rsp, initial_framesize);\n+    }\n+    pop(rbp);\n+  }\n+}\n+\n@@ -5789,1 +6592,1 @@\n-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, KRegister mask) {\n+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, KRegister mask) {\n@@ -5795,1 +6598,1 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_512bit);\n+    evpbroadcastq(xtmp, val, AVX_512bit);\n@@ -5797,1 +6600,3 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_256bit);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n+    vinserti128_high(xtmp, xtmp);\n@@ -5799,1 +6604,2 @@\n-    pxor(xtmp, xtmp);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n@@ -5822,1 +6628,1 @@\n-    fill64_masked(3, base, 0, xtmp, mask, cnt, rtmp, true);\n+    fill64_masked(3, base, 0, xtmp, mask, cnt, val, true);\n@@ -5841,1 +6647,1 @@\n-    fill32_masked(3, base, 0, xtmp, mask, cnt, rtmp);\n+    fill32_masked(3, base, 0, xtmp, mask, cnt, val);\n@@ -5943,2 +6749,2 @@\n-void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp,\n-                               bool is_large, KRegister mask) {\n+void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp,\n+                               bool is_large, bool word_copy_only, KRegister mask) {\n@@ -5949,1 +6755,1 @@\n-  assert(tmp==rax,   \"tmp register must be eax for rep stos\");\n+  assert(val==rax,   \"val register must be eax for rep stos\");\n@@ -5955,3 +6761,0 @@\n-  if (!is_large || !UseXMMForObjInit) {\n-    xorptr(tmp, tmp);\n-  }\n@@ -5969,1 +6772,1 @@\n-    movptr(Address(base, cnt, Address::times_ptr), tmp);\n+    movptr(Address(base, cnt, Address::times_ptr), val);\n@@ -5978,1 +6781,1 @@\n-  if (UseFastStosb) {\n+  if (UseFastStosb && !word_copy_only) {\n@@ -5982,1 +6785,1 @@\n-    xmm_clear_mem(base, cnt, tmp, xtmp, mask);\n+    xmm_clear_mem(base, cnt, val, xtmp, mask);\n@@ -9902,0 +10705,3 @@\n+  \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+  andptr(reg_rax, ~((int) markWord::inline_type_bit_in_place));\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":824,"deletions":18,"binary":false,"changes":842,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -36,0 +37,2 @@\n+class ciInlineKlass;\n+\n@@ -97,0 +100,22 @@\n+  \/\/ markWord tests, kills markWord reg\n+  void test_markword_is_inline_type(Register markword, Label& is_inline_type);\n+\n+  \/\/ inlineKlass queries, kills temp_reg\n+  void test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type, bool can_be_null = true);\n+\n+  void test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free);\n+  void test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free);\n+  void test_field_is_flat(Register flags, Register temp_reg, Label& is_flat);\n+  void test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker);\n+\n+  \/\/ Check oops for special arrays, i.e. flat arrays and\/or null-free arrays\n+  void test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label);\n+  void test_flat_array_oop(Register oop, Register temp_reg, Label& is_flat_array);\n+  void test_non_flat_array_oop(Register oop, Register temp_reg, Label& is_non_flat_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array);\n+  void test_non_null_free_array_oop(Register oop, Register temp_reg, Label& is_non_null_free_array);\n+\n+  \/\/ Check array klass layout helper for flat or null-free arrays...\n+  void test_flat_array_layout(Register lh, Label& is_flat_array);\n+  void test_non_flat_array_layout(Register lh, Label& is_non_flat_array);\n+\n@@ -350,0 +375,3 @@\n+\n+  \/\/ Load oopDesc._metadata without decode (useful for direct Klass* compare from oops)\n+  void load_metadata(Register dst, Register src);\n@@ -367,0 +395,9 @@\n+  void flat_field_copy(DecoratorSet decorators, Register src, Register dst, Register inline_layout_info);\n+\n+  \/\/ inline type data payload offsets...\n+  void payload_offset(Register inline_klass, Register offset);\n+  void payload_addr(Register oop, Register data, Register inline_klass);\n+  \/\/ get data payload ptr a flat value array at index, kills rcx and index\n+  void data_for_value_array_index(Register array, Register array_klass,\n+                                  Register index, Register data);\n+\n@@ -376,0 +413,2 @@\n+  void load_prototype_header(Register dst, Register src, Register tmp);\n+\n@@ -510,0 +549,9 @@\n+\n+  \/\/ Object \/ value buffer allocation...\n+  \/\/ Allocate instance of klass, assumes klass initialized by caller\n+  \/\/ new_obj prefers to be rax\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj (rsi on LP64)\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n@@ -520,0 +568,5 @@\n+  \/\/ For field \"index\" within \"klass\", return inline_klass ...\n+  void get_inline_type_field_klass(Register klass, Register index, Register inline_klass);\n+\n+  void inline_layout_info(Register klass, Register index, Register layout_info);\n+\n@@ -768,0 +821,1 @@\n+  void andptr(Register dst, Address src) { andq(dst, src); }\n@@ -1907,0 +1961,15 @@\n+  \/\/ Inline type specific methods\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                            VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                            RegState reg_state[]);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                          RegState reg_state[], Register val_array);\n+  int extend_stack_for_inline_args(int args_on_stack);\n+  void remove_frame(int initial_framesize, bool needs_stack_repair);\n+  VMReg spill_reg_for(VMReg reg);\n+\n@@ -1909,1 +1978,1 @@\n-  void clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, bool is_large, KRegister mask=knoreg);\n+  void clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only, KRegister mask=knoreg);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":70,"deletions":1,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -37,2 +37,24 @@\n-  int argsize = is_compiled() ? (_cb->as_nmethod()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord : 0;\n-  int frame_size = _cb->frame_size() + argsize;\n+  int frame_size = _cb->frame_size();\n+  if (is_compiled()) {\n+    nmethod* nm = _cb->as_nmethod_or_null();\n+    if (nm->needs_stack_repair() && nm->is_compiled_by_c2()) {\n+      frame f = to_frame();\n+      bool augmented = f.was_augmented_on_entry(frame_size);\n+      if (!augmented) {\n+        \/\/ Fix: C2 caller, so frame was not extended and thus the\n+        \/\/ size read from the frame does not include the arguments.\n+        \/\/ Ideally we have to count the arg size for the scalarized\n+        \/\/ convention. For now we include the size of the caller frame\n+        \/\/ which would at least be equal to that.\n+        RegisterMap map(nullptr,\n+                        RegisterMap::UpdateMap::skip,\n+                        RegisterMap::ProcessFrames::skip,\n+                        RegisterMap::WalkContinuation::skip);\n+        frame caller = to_frame().sender(&map);\n+        assert(caller.is_compiled_frame() && caller.cb()->as_nmethod()->is_compiled_by_c2(), \"needs stack repair but was not extended with c1\/interpreter caller\");\n+        frame_size += (caller.real_fp() - caller.sp());\n+      }\n+    } else {\n+      frame_size += _cb->as_nmethod()->num_stack_arg_slots() * VMRegImpl::stack_slot_size >> LogBytesPerWord;\n+    }\n+  }\n@@ -55,1 +77,1 @@\n-  return *(address*)(_sp - 1);\n+  return *(address*)((_callee_augmented ? _unextended_sp : _sp) - 1);\n","filename":"src\/hotspot\/cpu\/x86\/stackChunkFrameStream_x86.inline.hpp","additions":25,"deletions":3,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -44,0 +45,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -170,0 +172,1 @@\n+  case Bytecodes::_fast_vputfield:\n@@ -778,9 +781,28 @@\n-  \/\/ rax: index\n-  \/\/ rdx: array\n-  index_check(rdx, rax); \/\/ kills rbx\n-  do_oop_load(_masm,\n-              Address(rdx, rax,\n-                      UseCompressedOops ? Address::times_4 : Address::times_ptr,\n-                      arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n-              rax,\n-              IS_ARRAY);\n+  Register array = rdx;\n+  Register index = rax;\n+\n+  index_check(array, index); \/\/ kills rbx\n+  __ profile_array_type<ArrayLoadData>(rbx, array, rcx);\n+  if (UseArrayFlattening) {\n+    Label is_flat_array, done;\n+    __ test_flat_array_oop(array, rbx, is_flat_array);\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+    __ jmp(done);\n+    __ bind(is_flat_array);\n+    __ movptr(rcx, array);\n+    call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::flat_array_load), rcx, index);\n+    __ bind(done);\n+  } else {\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+  }\n+  __ profile_element_type(rbx, rax, rcx);\n@@ -1060,1 +1082,1 @@\n-  Label is_null, ok_is_subtype, done;\n+  Label is_null, is_flat_array, ok_is_subtype, done;\n@@ -1072,0 +1094,4 @@\n+\n+  __ profile_array_type<ArrayStoreData>(rdi, rdx, rbx);\n+  __ profile_multiple_element_types(rdi, rax, rbx, rcx);\n+\n@@ -1075,0 +1101,7 @@\n+  \/\/ Move array class to rdi\n+  __ load_klass(rdi, rdx, rscratch1);\n+  if (UseArrayFlattening) {\n+    __ movl(rbx, Address(rdi, Klass::layout_helper_offset()));\n+    __ test_flat_array_layout(rbx, is_flat_array);\n+  }\n+\n@@ -1077,3 +1110,2 @@\n-  \/\/ Move superklass into rax\n-  __ load_klass(rax, rdx, rscratch1);\n-  __ movptr(rax, Address(rax,\n+  \/\/ Move array element superklass into rax\n+  __ movptr(rax, Address(rdi,\n@@ -1084,1 +1116,2 @@\n-  __ gen_subtype_check(rbx, ok_is_subtype);\n+  \/\/ is \"rbx <: rax\" ? (value subclass <: array element superclass)\n+  __ gen_subtype_check(rbx, ok_is_subtype, false);\n@@ -1102,1 +1135,13 @@\n-  __ profile_null_seen(rbx);\n+  if (Arguments::is_valhalla_enabled()) {\n+    Label write_null_to_null_free_array, store_null;\n+\n+      \/\/ Move array class to rdi\n+    __ load_klass(rdi, rdx, rscratch1);\n+    if (UseArrayFlattening) {\n+      __ movl(rbx, Address(rdi, Klass::layout_helper_offset()));\n+      __ test_flat_array_layout(rbx, is_flat_array);\n+    }\n+\n+    \/\/ No way to store null in null-free array\n+    __ test_null_free_array_oop(rdx, rbx, write_null_to_null_free_array);\n+    __ jmp(store_null);\n@@ -1104,0 +1149,5 @@\n+    __ bind(write_null_to_null_free_array);\n+    __ jump(RuntimeAddress(Interpreter::_throw_NullPointerException_entry));\n+\n+    __ bind(store_null);\n+  }\n@@ -1106,0 +1156,5 @@\n+  __ jmp(done);\n+\n+  if (UseArrayFlattening) {\n+    Label is_type_ok;\n+    __ bind(is_flat_array); \/\/ Store non-null value to flat\n@@ -1107,0 +1162,6 @@\n+    __ movptr(rax, at_tos());\n+    __ movl(rcx, at_tos_p1()); \/\/ index\n+    __ movptr(rdx, at_tos_p2()); \/\/ array\n+\n+    call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::flat_array_store), rax, rdx, rcx);\n+  }\n@@ -1894,1 +1955,1 @@\n-  Label not_taken;\n+  Label taken, not_taken;\n@@ -1896,0 +1957,36 @@\n+\n+  __ profile_acmp(rbx, rdx, rax, rcx);\n+\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  if (Arguments::is_valhalla_enabled()) {\n+    __ cmpoop(rdx, rax);\n+    __ jcc(Assembler::equal, (cc == equal) ? taken : not_taken);\n+\n+    \/\/ might be substitutable, test if either rax or rdx is null\n+    __ testptr(rax, rax);\n+    __ jcc(Assembler::zero, (cc == equal) ? not_taken : taken);\n+    __ testptr(rdx, rdx);\n+    __ jcc(Assembler::zero, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ and both are values ?\n+    __ movptr(rbx, Address(rdx, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, is_inline_type_mask);\n+    __ cmpptr(rbx, is_inline_type_mask);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ same value klass ?\n+    __ load_metadata(rbx, rdx);\n+    __ load_metadata(rcx, rax);\n+    __ cmpptr(rbx, rcx);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ Know both are the same type, let's test for substitutability...\n+    if (cc == equal) {\n+      invoke_is_substitutable(rax, rdx, taken, not_taken);\n+    } else {\n+      invoke_is_substitutable(rax, rdx, not_taken, taken);\n+    }\n+    __ stop(\"Not reachable\");\n+  }\n+\n@@ -1898,0 +1995,1 @@\n+  __ bind(taken);\n@@ -1900,1 +1998,10 @@\n-  __ profile_not_taken_branch(rax);\n+  __ profile_not_taken_branch(rax, true);\n+}\n+\n+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,\n+                                            Label& is_subst, Label& not_subst) {\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);\n+  \/\/ Restored...rax answer, jmp to outcome...\n+  __ testl(rax, rax);\n+  __ jcc(Assembler::zero, not_subst);\n+  __ jmp(is_subst);\n@@ -2154,1 +2261,2 @@\n-  __ remove_activation(state, rbcp);\n+\n+  __ remove_activation(state, rbcp, true, true, true);\n@@ -2526,1 +2634,1 @@\n-  const Register obj   = c_rarg3;\n+  const Register obj   = r9;\n@@ -2538,2 +2646,0 @@\n-  if (!is_static) pop_and_check_object(obj);\n-\n@@ -2550,0 +2656,1 @@\n+  if (!is_static) pop_and_check_object(obj);\n@@ -2563,0 +2670,1 @@\n+  if (!is_static) pop_and_check_object(obj);\n@@ -2576,4 +2684,35 @@\n-  do_oop_load(_masm, field, rax);\n-  __ push(atos);\n-  if (!is_static && rc == may_rewrite) {\n-    patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+  if (!Arguments::is_valhalla_enabled()) {\n+    if (!is_static) pop_and_check_object(obj);\n+    do_oop_load(_masm, field, rax);\n+    __ push(atos);\n+    if (!is_static && rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+    }\n+    __ jmp(Done);\n+  } else {\n+    if (is_static) {\n+      __ load_heap_oop(rax, field);\n+      __ push(atos);\n+      __ jmp(Done);\n+    } else {\n+      Label is_flat, rewrite_inline;\n+      __ test_field_is_flat(flags, rscratch1, is_flat);\n+      pop_and_check_object(obj);\n+      __ load_heap_oop(rax, field);\n+      __ push(atos);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+      }\n+      __ jmp(Done);\n+      __ bind(is_flat);\n+      \/\/ field is flat (null-free or nullable with a null-marker)\n+      pop_and_check_object(rax);\n+      __ read_flat_field(rcx, rdx, rbx, rax);\n+      __ verify_oop(rax);\n+      __ push(atos);\n+      __ bind(rewrite_inline);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_vgetfield, bc, rbx);\n+      }\n+      __ jmp(Done);\n+    }\n@@ -2581,1 +2720,0 @@\n-  __ jmp(Done);\n@@ -2584,0 +2722,3 @@\n+\n+  if (!is_static) pop_and_check_object(obj);\n+\n@@ -2683,1 +2824,0 @@\n-\n@@ -2745,1 +2885,1 @@\n-  const Register flags = rax;\n+  const Register flags = r9;\n@@ -2758,2 +2898,3 @@\n-  __ andl(flags, (1 << ResolvedFieldEntry::is_volatile_shift));\n-  __ testl(flags, flags);\n+  __ movl(rscratch1, flags);\n+  __ andl(rscratch1, (1 << ResolvedFieldEntry::is_volatile_shift));\n+  __ testl(rscratch1, rscratch1);\n@@ -2762,1 +2903,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state, flags);\n@@ -2768,1 +2909,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state, flags);\n@@ -2774,1 +2915,1 @@\n-                                              Register obj, Register off, Register tos_state) {\n+                                              Register obj, Register off, Register tos_state, Register flags) {\n@@ -2821,6 +2962,44 @@\n-    __ pop(atos);\n-    if (!is_static) pop_and_check_object(obj);\n-    \/\/ Store into the field\n-    do_oop_store(_masm, field, rax);\n-    if (!is_static && rc == may_rewrite) {\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+    if (!Arguments::is_valhalla_enabled()) {\n+      __ pop(atos);\n+      if (!is_static) pop_and_check_object(obj);\n+      \/\/ Store into the field\n+      do_oop_store(_masm, field, rax);\n+      if (!is_static && rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+      }\n+      __ jmp(Done);\n+    } else {\n+      __ pop(atos);\n+      if (is_static) {\n+        Label is_nullable;\n+        __ test_field_is_not_null_free_inline_type(flags, rscratch1, is_nullable);\n+        __ null_check(rax);  \/\/ FIXME JDK-8341120\n+        __ bind(is_nullable);\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(Done);\n+      } else {\n+        Label is_flat, null_free_reference, rewrite_inline;\n+        __ test_field_is_flat(flags, rscratch1, is_flat);\n+        __ test_field_is_null_free_inline_type(flags, rscratch1, null_free_reference);\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, rax);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+        }\n+        __ jmp(Done);\n+        __ bind(null_free_reference);\n+        __ null_check(rax);  \/\/ FIXME JDK-8341120\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(rewrite_inline);\n+        __ bind(is_flat);\n+        pop_and_check_object(rscratch2);\n+        __ write_flat_field(rcx, r8, rscratch1, rscratch2, rbx, rax);\n+        __ bind(rewrite_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_vputfield, bc, rbx, true, byte_no);\n+        }\n+        __ jmp(Done);\n+      }\n@@ -2828,1 +3007,0 @@\n-    __ jmp(Done);\n@@ -2965,0 +3143,1 @@\n+    case Bytecodes::_fast_vputfield: \/\/ fall through\n@@ -2988,0 +3167,1 @@\n+    case Bytecodes::_fast_vputfield: \/\/ fall through\n@@ -3006,2 +3186,0 @@\n-  Register cache = rcx;\n-\n@@ -3014,3 +3192,1 @@\n-  load_resolved_field_entry(noreg, cache, rax, rbx, rdx);\n-  \/\/ RBX: field offset, RAX: TOS, RDX: flags\n-  __ andl(rdx, (1 << ResolvedFieldEntry::is_volatile_shift));\n+  load_resolved_field_entry(noreg, rcx, rax, rbx, rdx);\n@@ -3018,0 +3194,1 @@\n+  \/\/ RBX: field offset, RCX: RAX: TOS, RDX: flags\n@@ -3026,1 +3203,3 @@\n-  __ testl(rdx, rdx);\n+  __ movl(rscratch2, rdx);  \/\/ saving flags for is_flat test\n+  __ andl(rscratch2, (1 << ResolvedFieldEntry::is_volatile_shift));\n+  __ testl(rscratch2, rscratch2);\n@@ -3029,1 +3208,1 @@\n-  fast_storefield_helper(field, rax);\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3035,1 +3214,1 @@\n-  fast_storefield_helper(field, rax);\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3040,1 +3219,3 @@\n-void TemplateTable::fast_storefield_helper(Address field, Register rax) {\n+void TemplateTable::fast_storefield_helper(Address field, Register rax, Register flags) {\n+\n+  \/\/ DANGER: 'field' argument depends on rcx and rbx\n@@ -3044,0 +3225,15 @@\n+  case Bytecodes::_fast_vputfield:\n+    {\n+      \/\/ Field is either flat (nullable or not) or non-flat and null-free\n+      Label is_flat, done;\n+      __ test_field_is_flat(flags, rscratch1, is_flat);\n+      __ null_check(rax);  \/\/ FIXME JDK-8341120\n+      do_oop_store(_masm, field, rax);\n+      __ jmp(done);\n+      __ bind(is_flat);\n+      __ load_field_entry(r8, r9);\n+      __ movptr(rscratch2, rcx);  \/\/ re-shuffle registers because of VM call calling convention\n+      __ write_flat_field(r8, rscratch1, r9, rscratch2, rbx, rax);\n+      __ bind(done);\n+    }\n+    break;\n@@ -3045,1 +3241,3 @@\n-    do_oop_store(_masm, field, rax);\n+    {\n+      do_oop_store(_masm, field, rax);\n+    }\n@@ -3101,1 +3299,1 @@\n-  __ load_sized_value(rbx, Address(rcx, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n+  __ load_sized_value(rdx, Address(rcx, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n@@ -3106,1 +3304,1 @@\n-  Address field(rax, rbx, Address::times_1);\n+  Address field(rax, rdx, Address::times_1);\n@@ -3110,0 +3308,4 @@\n+  case Bytecodes::_fast_vgetfield:\n+    __ read_flat_field(rcx, rdx, rbx, rax);\n+    __ verify_oop(rax);\n+    break;\n@@ -3534,2 +3736,0 @@\n-  Label slow_case_no_pop;\n-  Label initialize_header;\n@@ -3545,1 +3745,1 @@\n-  __ jcc(Assembler::notEqual, slow_case_no_pop);\n+  __ jcc(Assembler::notEqual, slow_case);\n@@ -3549,1 +3749,0 @@\n-  __ push(rcx);  \/\/ save the contexts of klass for initializing the header\n@@ -3556,83 +3755,2 @@\n-  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n-  __ movl(rdx, Address(rcx, Klass::layout_helper_offset()));\n-  \/\/ test to see if it is malformed in some way\n-  __ testl(rdx, Klass::_lh_instance_slow_path_bit);\n-  __ jcc(Assembler::notZero, slow_case);\n-\n-  \/\/ Allocate the instance:\n-  \/\/  If TLAB is enabled:\n-  \/\/    Try to allocate in the TLAB.\n-  \/\/    If fails, go to the slow path.\n-  \/\/    Initialize the allocation.\n-  \/\/    Exit.\n-  \/\/\n-  \/\/  Go to slow path.\n-\n-  if (UseTLAB) {\n-    __ tlab_allocate(rax, rdx, 0, rcx, rbx, slow_case);\n-    if (ZeroTLAB) {\n-      \/\/ the fields have been already cleared\n-      __ jmp(initialize_header);\n-    }\n-\n-    \/\/ The object is initialized before the header.  If the object size is\n-    \/\/ zero, go directly to the header initialization.\n-    if (UseCompactObjectHeaders) {\n-      assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n-      __ decrement(rdx, oopDesc::base_offset_in_bytes());\n-    } else {\n-      __ decrement(rdx, sizeof(oopDesc));\n-    }\n-    __ jcc(Assembler::zero, initialize_header);\n-\n-    \/\/ Initialize topmost object field, divide rdx by 8, check if odd and\n-    \/\/ test if zero.\n-    __ xorl(rcx, rcx);    \/\/ use zero reg to clear memory (shorter code)\n-    __ shrl(rdx, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n-\n-    \/\/ rdx must have been multiple of 8\n-#ifdef ASSERT\n-    \/\/ make sure rdx was multiple of 8\n-    Label L;\n-    \/\/ Ignore partial flag stall after shrl() since it is debug VM\n-    __ jcc(Assembler::carryClear, L);\n-    __ stop(\"object size is not multiple of 2 - adjust this code\");\n-    __ bind(L);\n-    \/\/ rdx must be > 0, no extra check needed here\n-#endif\n-\n-    \/\/ initialize remaining object fields: rdx was a multiple of 8\n-    { Label loop;\n-    __ bind(loop);\n-    int header_size_bytes = oopDesc::header_size() * HeapWordSize;\n-    assert(is_aligned(header_size_bytes, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n-    __ movptr(Address(rax, rdx, Address::times_8, header_size_bytes - 1*oopSize), rcx);\n-    __ decrement(rdx);\n-    __ jcc(Assembler::notZero, loop);\n-    }\n-\n-    \/\/ initialize object header only.\n-    __ bind(initialize_header);\n-    if (UseCompactObjectHeaders) {\n-      __ pop(rcx);   \/\/ get saved klass back in the register.\n-      __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));\n-      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()), rbx);\n-    } else {\n-      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n-                (intptr_t)markWord::prototype().value()); \/\/ header\n-      __ pop(rcx);   \/\/ get saved klass back in the register.\n-      __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-      __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n-      __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n-    }\n-\n-    if (DTraceAllocProbes) {\n-      \/\/ Trigger dtrace event for fastpath\n-      __ push(atos);\n-      __ call_VM_leaf(\n-           CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), rax);\n-      __ pop(atos);\n-    }\n-\n-    __ jmp(done);\n-  }\n+  __ allocate_instance(rcx, rax, rdx, rbx, true, slow_case);\n+  __ jmp(done);\n@@ -3642,2 +3760,0 @@\n-  __ pop(rcx);   \/\/ restore stack pointer to what it was when we came in.\n-  __ bind(slow_case_no_pop);\n@@ -3685,4 +3801,4 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+      Address::times_1,\n+      Array<u1>::base_offset_in_bytes()));\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -3718,0 +3834,3 @@\n+  __ jmp(done);\n+\n+  __ bind(is_null);\n@@ -3721,4 +3840,1 @@\n-    __ jmp(done);\n-    __ bind(is_null);\n-  } else {\n-    __ bind(is_null);   \/\/ same as 'done'\n+\n@@ -3740,4 +3856,4 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+        Address::times_1,\n+        Array<u1>::base_offset_in_bytes()));\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -3787,1 +3903,0 @@\n-\n@@ -3847,0 +3962,4 @@\n+  Label is_inline_type;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ test_markword_is_inline_type(rbx, is_inline_type);\n+\n@@ -3939,0 +4058,5 @@\n+\n+  __ bind(is_inline_type);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                    InterpreterRuntime::throw_identity_exception), rax);\n+  __ should_not_reach_here();\n@@ -3947,0 +4071,11 @@\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  Label has_identity;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ andptr(rbx, is_inline_type_mask);\n+  __ cmpl(rbx, is_inline_type_mask);\n+  __ jcc(Assembler::notEqual, has_identity);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                     InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+  __ bind(has_identity);\n+\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":289,"deletions":154,"binary":false,"changes":443,"status":"modified"},{"patch":"@@ -96,0 +96,1 @@\n+  virtual void do_ProfileACmpTypes(ProfileACmpTypes*  x);\n","filename":"src\/hotspot\/share\/c1\/c1_Canonicalizer.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -38,0 +38,18 @@\n+class DelayedFieldAccess : public CompilationResourceObj {\n+private:\n+  Value            _obj;\n+  ciInstanceKlass* _holder;\n+  int              _offset;\n+  ValueStack*      _state_before;\n+\n+public:\n+  DelayedFieldAccess(Value obj, ciInstanceKlass* holder, int offset, ValueStack* state_before)\n+  : _obj(obj), _holder(holder) , _offset(offset), _state_before(state_before) { }\n+\n+  Value obj() const { return _obj; }\n+  ciInstanceKlass* holder() const { return _holder; }\n+  int offset() const { return _offset; }\n+  void inc_offset(int offset) { _offset += offset; }\n+  ValueStack* state_before() const { return _state_before; }\n+};\n+\n@@ -195,0 +213,4 @@\n+  \/\/ support for optimization of accesses to flat fields and flat arrays\n+  DelayedFieldAccess* _pending_field_access;\n+  DelayedLoadIndexed* _pending_load_indexed;\n+\n@@ -212,0 +234,6 @@\n+  bool              has_pending_field_access()   { return _pending_field_access != nullptr; }\n+  DelayedFieldAccess* pending_field_access()     { return _pending_field_access; }\n+  void              set_pending_field_access(DelayedFieldAccess* delayed) { _pending_field_access = delayed; }\n+  bool              has_pending_load_indexed()   { return _pending_load_indexed != nullptr; }\n+  DelayedLoadIndexed* pending_load_indexed()     { return _pending_load_indexed; }\n+  void              set_pending_load_indexed(DelayedLoadIndexed* delayed) { _pending_load_indexed = delayed; }\n@@ -270,0 +298,3 @@\n+  \/\/ inline types\n+  void copy_inline_content(ciInlineKlass* vk, Value src, int src_off, Value dest, int dest_off, ValueStack* state_before, ciField* encloding_field = nullptr);\n+\n@@ -398,0 +429,1 @@\n+  bool profile_array_accesses(){ return _compilation->profile_array_accesses();}\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.hpp","additions":32,"deletions":0,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -77,0 +77,1 @@\n+class     Deoptimize;\n@@ -100,0 +101,1 @@\n+class   ProfileACmpTypes;\n@@ -194,0 +196,1 @@\n+  virtual void do_ProfileACmpTypes(ProfileACmpTypes*  x) = 0;\n@@ -210,3 +213,4 @@\n-#define HASH2(x1, x2        )                    ((HASH1(x1        ) << 7) ^ HASH1(x2))\n-#define HASH3(x1, x2, x3    )                    ((HASH2(x1, x2    ) << 7) ^ HASH1(x3))\n-#define HASH4(x1, x2, x3, x4)                    ((HASH3(x1, x2, x3) << 7) ^ HASH1(x4))\n+#define HASH2(x1, x2        )                    ((HASH1(x1            ) << 7) ^ HASH1(x2))\n+#define HASH3(x1, x2, x3    )                    ((HASH2(x1, x2        ) << 7) ^ HASH1(x3))\n+#define HASH4(x1, x2, x3, x4)                    ((HASH3(x1, x2, x3    ) << 7) ^ HASH1(x4))\n+#define HASH5(x1, x2, x3, x4, x5)                ((HASH4(x1, x2, x3, x4) << 7) ^ HASH1(x5))\n@@ -271,0 +275,15 @@\n+#define HASHING4(class_name, enabled, f1, f2, f3, f4) \\\n+  virtual intx hash() const {                         \\\n+    return (enabled) ? HASH5(name(), f1, f2, f3, f4) : 0; \\\n+  }                                                   \\\n+  virtual bool is_equal(Value v) const {              \\\n+    if (!(enabled)  ) return false;                   \\\n+    class_name* _v = v->as_##class_name();            \\\n+    if (_v == nullptr  ) return false;                   \\\n+    if (f1 != _v->f1) return false;                   \\\n+    if (f2 != _v->f2) return false;                   \\\n+    if (f3 != _v->f3) return false;                   \\\n+    if (f4 != _v->f4) return false;                   \\\n+    return true;                                      \\\n+  }                                                   \\\n+\n@@ -293,0 +312,1 @@\n+  friend class GraphBuilder;\n@@ -345,0 +365,1 @@\n+    NeverNullFlag,\n@@ -435,0 +456,2 @@\n+  void set_null_free(bool f)                     { set_flag(NeverNullFlag, f); }\n+  bool is_null_free() const                      { return check_flag(NeverNullFlag); }\n@@ -445,0 +468,1 @@\n+  ciKlass* as_loaded_klass_or_null() const;\n@@ -489,0 +513,4 @@\n+  bool is_loaded_flat_array() const;\n+  bool maybe_flat_array();\n+  bool maybe_null_free_array();\n+\n@@ -816,1 +844,3 @@\n-  {}\n+  {\n+    set_null_free(field->is_null_free());\n+  }\n@@ -828,0 +858,1 @@\n+  ciField* _enclosing_field;   \/\/ enclosing field (the flat one) for nested fields\n@@ -832,7 +863,1 @@\n-             ValueStack* state_before, bool needs_patching)\n-  : AccessField(obj, offset, field, is_static, state_before, needs_patching)\n-  , _value(value)\n-  {\n-    ASSERT_VALUES\n-    pin();\n-  }\n+             ValueStack* state_before, bool needs_patching);\n@@ -842,0 +867,2 @@\n+  ciField* enclosing_field() const               { return _enclosing_field; }\n+  void set_enclosing_field(ciField* field)       { _enclosing_field = field; }\n@@ -899,0 +926,2 @@\n+  ciMethod* _profiled_method;\n+  int       _profiled_bci;\n@@ -908,0 +937,1 @@\n+  , _profiled_method(nullptr), _profiled_bci(0)\n@@ -923,0 +953,9 @@\n+  \/\/ Helpers for MethodData* profiling\n+  void set_should_profile(bool value)                { set_flag(ProfileMDOFlag, value); }\n+  void set_profiled_method(ciMethod* method)         { _profiled_method = method;   }\n+  void set_profiled_bci(int bci)                     { _profiled_bci = bci;         }\n+  bool      should_profile() const                   { return check_flag(ProfileMDOFlag); }\n+  ciMethod* profiled_method() const                  { return _profiled_method;     }\n+  int       profiled_bci() const                     { return _profiled_bci;        }\n+\n+\n@@ -927,0 +966,1 @@\n+class DelayedLoadIndexed;\n@@ -931,0 +971,2 @@\n+  NewInstance* _vt;\n+  DelayedLoadIndexed* _delayed;\n@@ -936,1 +978,1 @@\n-  , _explicit_null_check(nullptr) {}\n+  , _explicit_null_check(nullptr), _vt(nullptr), _delayed(nullptr) {}\n@@ -948,0 +990,6 @@\n+  NewInstance* vt() const { return _vt; }\n+  void set_vt(NewInstance* vt) { _vt = vt; }\n+\n+  DelayedLoadIndexed* delayed() const { return _delayed; }\n+  void set_delayed(DelayedLoadIndexed* delayed) { _delayed = delayed; }\n+\n@@ -949,1 +997,1 @@\n-  HASHING3(LoadIndexed, true, elt_type(), array()->subst(), index()->subst())\n+  HASHING4(LoadIndexed, delayed() == nullptr && !should_profile(), elt_type(), array()->subst(), index()->subst(), vt())\n@@ -952,0 +1000,24 @@\n+class DelayedLoadIndexed : public CompilationResourceObj {\n+private:\n+  LoadIndexed* _load_instr;\n+  ValueStack* _state_before;\n+  ciField* _field;\n+  size_t _offset;\n+ public:\n+  DelayedLoadIndexed(LoadIndexed* load, ValueStack* state_before)\n+  : _load_instr(load)\n+  , _state_before(state_before)\n+  , _field(nullptr)\n+  , _offset(0) { }\n+\n+  void update(ciField* field, int offset) {\n+    assert(offset >= 0, \"must be\");\n+    _field = field;\n+    _offset += offset;\n+  }\n+\n+  LoadIndexed* load_instr() const { return _load_instr; }\n+  ValueStack* state_before() const { return _state_before; }\n+  ciField* field() const { return _field; }\n+  size_t offset() const { return _offset; }\n+};\n@@ -957,2 +1029,0 @@\n-  ciMethod* _profiled_method;\n-  int       _profiled_bci;\n@@ -964,7 +1034,1 @@\n-               bool check_boolean, bool mismatched = false)\n-  : AccessIndexed(array, index, length, elt_type, state_before, mismatched)\n-  , _value(value), _profiled_method(nullptr), _profiled_bci(0), _check_boolean(check_boolean)\n-  {\n-    ASSERT_VALUES\n-    pin();\n-  }\n+               bool check_boolean, bool mismatched = false);\n@@ -975,7 +1039,3 @@\n-  \/\/ Helpers for MethodData* profiling\n-  void set_should_profile(bool value)                { set_flag(ProfileMDOFlag, value); }\n-  void set_profiled_method(ciMethod* method)         { _profiled_method = method;   }\n-  void set_profiled_bci(int bci)                     { _profiled_bci = bci;         }\n-  bool      should_profile() const                   { return check_flag(ProfileMDOFlag); }\n-  ciMethod* profiled_method() const                  { return _profiled_method;     }\n-  int       profiled_bci() const                     { return _profiled_bci;        }\n+\n+  \/\/ Flattened array support\n+  bool is_exact_flat_array_store() const;\n@@ -1092,0 +1152,1 @@\n+  bool _substitutability_check;\n@@ -1095,1 +1156,1 @@\n-  IfOp(Value x, Condition cond, Value y, Value tval, Value fval)\n+  IfOp(Value x, Condition cond, Value y, Value tval, Value fval, ValueStack* state_before, bool substitutability_check)\n@@ -1099,0 +1160,1 @@\n+  , _substitutability_check(substitutability_check)\n@@ -1102,0 +1164,1 @@\n+    set_state_before(state_before);\n@@ -1110,1 +1173,1 @@\n-\n+  bool substitutability_check() const             { return _substitutability_check; }\n@@ -1267,0 +1330,1 @@\n+  bool _needs_state_before;\n@@ -1270,1 +1334,1 @@\n-  NewInstance(ciInstanceKlass* klass, ValueStack* state_before, bool is_unresolved)\n+  NewInstance(ciInstanceKlass* klass, ValueStack* state_before, bool is_unresolved, bool needs_state_before)\n@@ -1272,1 +1336,1 @@\n-  , _klass(klass), _is_unresolved(is_unresolved)\n+  , _klass(klass), _is_unresolved(is_unresolved), _needs_state_before(needs_state_before)\n@@ -1278,0 +1342,1 @@\n+  bool needs_state_before() const                { return _needs_state_before; }\n@@ -1287,1 +1352,0 @@\n-\n@@ -1341,1 +1405,2 @@\n-  NewObjectArray(ciKlass* klass, Value length, ValueStack* state_before) : NewArray(length, state_before), _klass(klass) {}\n+  NewObjectArray(ciKlass* klass, Value length, ValueStack* state_before)\n+  : NewArray(length, state_before), _klass(klass) { }\n@@ -1376,0 +1441,2 @@\n+\n+  ciType* exact_type() const;\n@@ -1423,1 +1490,1 @@\n-  : TypeCheck(klass, obj, objectType, state_before) {}\n+  : TypeCheck(klass, obj, objectType, state_before) { }\n@@ -1481,0 +1548,1 @@\n+  bool _maybe_inlinetype;\n@@ -1483,1 +1551,1 @@\n-  MonitorEnter(Value obj, int monitor_no, ValueStack* state_before)\n+  MonitorEnter(Value obj, int monitor_no, ValueStack* state_before, bool maybe_inlinetype)\n@@ -1485,0 +1553,1 @@\n+  , _maybe_inlinetype(maybe_inlinetype)\n@@ -1489,0 +1558,3 @@\n+  \/\/ accessors\n+  bool maybe_inlinetype() const                   { return _maybe_inlinetype; }\n+\n@@ -1944,0 +2016,1 @@\n+  bool        _substitutability_check;\n@@ -1947,1 +2020,1 @@\n-  If(Value x, Condition cond, bool unordered_is_true, Value y, BlockBegin* tsux, BlockBegin* fsux, ValueStack* state_before, bool is_safepoint)\n+  If(Value x, Condition cond, bool unordered_is_true, Value y, BlockBegin* tsux, BlockBegin* fsux, ValueStack* state_before, bool is_safepoint, bool substitutability_check=false)\n@@ -1955,0 +2028,1 @@\n+  , _substitutability_check(substitutability_check)\n@@ -1989,0 +2063,1 @@\n+  bool substitutability_check() const              { return _substitutability_check; }\n@@ -2299,1 +2374,1 @@\n-    \/\/ The ProfileType has side-effects and must occur precisely where located\n+    \/\/ The ProfileReturnType has side-effects and must occur precisely where located\n@@ -2315,0 +2390,42 @@\n+LEAF(ProfileACmpTypes, Instruction)\n+ private:\n+  ciMethod*        _method;\n+  int              _bci;\n+  Value            _left;\n+  Value            _right;\n+  bool             _left_maybe_null;\n+  bool             _right_maybe_null;\n+\n+ public:\n+  ProfileACmpTypes(ciMethod* method, int bci, Value left, Value right)\n+    : Instruction(voidType)\n+    , _method(method)\n+    , _bci(bci)\n+    , _left(left)\n+    , _right(right)\n+  {\n+    \/\/ The ProfileACmp has side-effects and must occur precisely where located\n+    pin();\n+    _left_maybe_null = true;\n+    _right_maybe_null = true;\n+  }\n+\n+  ciMethod* method()             const { return _method; }\n+  int bci()                      const { return _bci; }\n+  Value left()                   const { return _left; }\n+  Value right()                  const { return _right; }\n+  bool left_maybe_null()         const { return _left_maybe_null; }\n+  bool right_maybe_null()        const { return _right_maybe_null; }\n+  void set_left_maybe_null(bool v)     { _left_maybe_null = v; }\n+  void set_right_maybe_null(bool v)    { _right_maybe_null = v; }\n+\n+  virtual void input_values_do(ValueVisitor* f)   {\n+    if (_left != nullptr) {\n+      f->visit(&_left);\n+    }\n+    if (_right != nullptr) {\n+      f->visit(&_right);\n+    }\n+  }\n+};\n+\n","filename":"src\/hotspot\/share\/c1\/c1_Instruction.hpp","additions":156,"deletions":39,"binary":false,"changes":195,"status":"modified"},{"patch":"@@ -128,0 +128,1 @@\n+  virtual void do_ProfileACmpTypes(ProfileACmpTypes*  x);\n","filename":"src\/hotspot\/share\/c1\/c1_InstructionPrinter.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -162,0 +162,1 @@\n+    void do_ProfileACmpTypes(ProfileACmpTypes*  x) { \/* nothing to do *\/ };\n","filename":"src\/hotspot\/share\/c1\/c1_RangeCheckElimination.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -297,1 +298,1 @@\n-  memset(mem, 0, objArrayOopDesc::object_size(element_count));\n+  memset(mem, 0, refArrayOopDesc::object_size(element_count));\n@@ -303,0 +304,1 @@\n+    assert(!Arguments::is_valhalla_enabled() || Universe::objectArrayKlass()->prototype_header() == markWord::prototype(), \"should be the same\");\n@@ -333,1 +335,1 @@\n-  assert(objArrayOopDesc::object_size(max_elem_count)*HeapWordSize == MIN_GC_REGION_ALIGNMENT,\n+  assert(refArrayOopDesc::object_size(max_elem_count)*HeapWordSize == MIN_GC_REGION_ALIGNMENT,\n@@ -448,1 +450,1 @@\n-  size_t byte_size = objArrayOopDesc::object_size(length) * HeapWordSize;\n+  size_t byte_size = refArrayOopDesc::object_size(length) * HeapWordSize;\n@@ -477,0 +479,1 @@\n+    assert(!Arguments::is_valhalla_enabled() || Universe::objectArrayKlass()->prototype_header() == markWord::prototype(), \"should be the same\");\n@@ -731,1 +734,2 @@\n-    fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk));\n+    markWord prototype_header = src_klass->prototype_header().set_narrow_klass(nk);\n+    fake_oop->set_mark(prototype_header);\n@@ -741,1 +745,1 @@\n-  if (!src_obj->fast_no_hash_check()) {\n+  if (!src_obj->fast_no_hash_check() && (!(Arguments::is_valhalla_enabled() && src_obj->mark().is_inline_type()))) {\n@@ -744,1 +748,3 @@\n-      fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n+      fake_oop->set_mark(fake_oop->mark().copy_set_hash(src_hash));\n+    } else if (Arguments::is_valhalla_enabled()) {\n+      fake_oop->set_mark(src_klass->prototype_header().copy_set_hash(src_hash));\n","filename":"src\/hotspot\/share\/cds\/aotMappedHeapWriter.cpp","additions":12,"deletions":6,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -204,0 +204,1 @@\n+    assert(o->is_objArray_klass() && !o->is_flatArray_klass() && !o->is_refArray_klass(), \"must be exact\");\n@@ -206,0 +207,8 @@\n+  ciFlatArrayKlass* get_flat_array_klass(Klass* o) {\n+    if (o == nullptr) return nullptr;\n+    return get_metadata(o)->as_flat_array_klass();\n+  }\n+  ciRefArrayKlass* get_ref_array_klass(Klass* o) {\n+    if (o == nullptr) return nullptr;\n+    return get_metadata(o)->as_ref_array_klass();\n+  }\n@@ -503,0 +512,8 @@\n+  ciWrapper* make_early_larval_wrapper(ciType* type) {\n+    return _factory->make_early_larval_wrapper(type);\n+  }\n+\n+  ciWrapper* make_null_free_wrapper(ciType* type) {\n+    return _factory->make_null_free_wrapper(type);\n+  }\n+\n","filename":"src\/hotspot\/share\/ci\/ciEnv.hpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+  virtual bool is_inlinetype() const        { return false; }\n@@ -60,0 +61,2 @@\n+  virtual bool is_flat_array_klass() const  { return false; }\n+  virtual bool is_ref_array_klass() const   { return false; }\n@@ -61,0 +64,2 @@\n+  virtual bool is_early_larval() const      { return false; }\n+  virtual bool maybe_flat_in_array() const  { return false; }\n@@ -99,0 +104,8 @@\n+  ciFlatArrayKlass*        as_flat_array_klass() {\n+    assert(is_flat_array_klass(), \"bad cast\");\n+    return (ciFlatArrayKlass*)this;\n+  }\n+  ciRefArrayKlass*         as_ref_array_klass() {\n+    assert(is_ref_array_klass(), \"bad cast\");\n+    return (ciRefArrayKlass*)this;\n+  }\n@@ -103,0 +116,4 @@\n+  ciInlineKlass*           as_inline_klass() {\n+    assert(is_inlinetype(), \"bad cast\");\n+    return (ciInlineKlass*)this;\n+  }\n","filename":"src\/hotspot\/share\/ci\/ciMetadata.hpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -337,2 +337,2 @@\n-    \/\/ pop_objArray and pop_typeArray narrow the tos to ciObjArrayKlass\n-    \/\/ or ciTypeArrayKlass (resp.).  In the rare case that an explicit\n+    \/\/ pop_objOrFlatArray and pop_typeArray narrow the tos to ciObjArrayKlass,\n+    \/\/ ciFlatArrayKlass or ciTypeArrayKlass (resp.). In the rare case that an explicit\n@@ -340,1 +340,1 @@\n-    ciObjArrayKlass* pop_objArray() {\n+    ciArrayKlass* pop_objOrFlatArray() {\n@@ -343,2 +343,2 @@\n-      assert(array->is_obj_array_klass(), \"must be object array type\");\n-      return array->as_obj_array_klass();\n+      assert(array->is_obj_array_klass(), \"must be an object array type\");\n+      return array->as_array_klass();\n@@ -358,1 +358,1 @@\n-    void do_aaload(ciBytecodeStream* str);\n+    void do_aload(ciBytecodeStream* str);\n@@ -855,0 +855,3 @@\n+  ciType* mark_as_early_larval(ciType* type);\n+  ciType* mark_as_null_free(ciType* type);\n+\n","filename":"src\/hotspot\/share\/ci\/ciTypeFlow.hpp","additions":9,"deletions":6,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -60,1 +60,1 @@\n-  static ClassLoaderData* find_or_create(Handle class_loader);\n+  static ClassLoaderData* find_or_create(Handle class_loader, bool& created);\n","filename":"src\/hotspot\/share\/classfile\/classLoaderDataGraph.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -58,0 +58,2 @@\n+#include \"oops\/fieldStreams.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -75,0 +77,1 @@\n+#include \"runtime\/os.hpp\"\n@@ -189,0 +192,15 @@\n+\/\/ These migrated value classes are loaded by the bootstrap class loader but are added to the initiating\n+\/\/ loaders automatically so that fields of these types can be found and potentially flattened during\n+\/\/ field layout.\n+static void add_migrated_value_classes(ClassLoaderData* cld) {\n+  JavaThread* current = JavaThread::current();\n+  auto add_klass = [&] (Symbol* classname) {\n+    InstanceKlass* ik = SystemDictionary::find_instance_klass(current, classname, Handle(current, nullptr));\n+    assert(ik != nullptr, \"Must exist\");\n+    SystemDictionary::add_to_initiating_loader(current, ik, cld);\n+  };\n+\n+  MonitorLocker mu1(SystemDictionary_lock);\n+  vmSymbols::migrated_class_names_do(add_klass);\n+}\n+\n@@ -194,2 +212,18 @@\n-    return (class_loader() == nullptr) ? ClassLoaderData::the_null_class_loader_data() :\n-                                      ClassLoaderDataGraph::find_or_create(class_loader);\n+    if (class_loader() == nullptr) {\n+      return ClassLoaderData::the_null_class_loader_data();\n+    } else {\n+      bool created = false;\n+      ClassLoaderData* cld = ClassLoaderDataGraph::find_or_create(class_loader, created);\n+      if (created && Arguments::enable_preview()) {\n+        if (CDSConfig::is_using_aot_linked_classes() && java_system_loader() == nullptr) {\n+          \/\/ We are inside AOTLinkedClassBulkLoader::preload_classes().\n+          \/\/\n+          \/\/ AOTLinkedClassBulkLoader will automatically initiate the loading of all archived\n+          \/\/ public classes from the boot loader into platform\/system loaders, so there's\n+          \/\/ no need to call add_migrated_value_classes().\n+        } else {\n+          add_migrated_value_classes(cld);\n+        }\n+      }\n+      return cld;\n+    }\n@@ -403,1 +437,2 @@\n-\/\/ during class definition to allow class circularity checking\n+\/\/ during class definition, or may be called for inline field layout processing\n+\/\/ to detect class circularity errors.\n@@ -416,0 +451,2 @@\n+\/\/ inline field layout callers:\n+\/\/    The field's class must be loaded to determine layout.\n@@ -419,1 +456,1 @@\n-\/\/ placeholder for the same thread, class, classloader is found.\n+\/\/ placeholder for the same thread, class, and classloader is found.\n@@ -454,1 +491,1 @@\n-    \/\/ Must check ClassCircularity before resolving next_name (superclass or interface).\n+    \/\/ Must check ClassCircularity before resolving next_name (superclass, interface, field types or speculatively preloaded argument types).\n@@ -478,1 +515,1 @@\n-  \/\/ Resolve the superclass or superinterface, check results on return\n+  \/\/ Resolve the superclass, superinterface, field type or speculatively preloaded argument types and check results on return.\n@@ -916,2 +953,1 @@\n-  assert(!ModuleEntryTable::javabase_moduleEntry()->is_patched(),\n-         \"Cannot use sharing if java.base is patched\");\n+  assert(!CDSConfig::module_patching_disables_cds(), \"Cannot use CDS\");\n@@ -993,1 +1029,1 @@\n-        assert(!mod_entry->is_patched(), \"cannot load archived classes for patched module\");\n+        assert(!CDSConfig::module_patching_disables_cds(), \"Cannot use CDS\");\n@@ -1071,0 +1107,69 @@\n+\/\/ Pre-load class referred to in non-static null-free instance field. These fields trigger MANDATORY loading.\n+\/\/ Some pre-loading does not fail fatally\n+bool SystemDictionary::preload_from_null_free_field(InstanceKlass* ik, Handle class_loader, Symbol* sig, int field_index, TRAPS) {\n+  TempNewSymbol name = Signature::strip_envelope(sig);\n+  log_info(class, preload)(\"Preloading of class %s during loading of shared class %s. \"\n+                           \"Cause: a null-free non-static field is declared with this type\",\n+                           name->as_C_string(), ik->name()->as_C_string());\n+  InstanceKlass* real_k = SystemDictionary::resolve_with_circularity_detection(ik->name(), name,\n+                                                                               class_loader, false, CHECK_false);\n+  if (HAS_PENDING_EXCEPTION) {\n+    log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                \"(cause: null-free non-static field) failed: %s\",\n+                                name->as_C_string(), ik->name()->as_C_string(),\n+                                PENDING_EXCEPTION->klass()->name()->as_C_string());\n+    return false; \/\/ Exception is still pending\n+  }\n+\n+  InstanceKlass* k = ik->get_inline_type_field_klass_or_null(field_index);\n+  if (real_k != k) {\n+    \/\/ oops, the app has substituted a different version of k! Does not fail fatally\n+    log_info(class, preload)(\"Preloading of class %s during loading of shared class %s \"\n+                                \"(cause: null-free non-static field) failed : \"\n+                                \"app substituted a different version of %s\",\n+                                name->as_C_string(), ik->name()->as_C_string(),\n+                                name->as_C_string());\n+    return false;\n+  }\n+  log_info(class, preload)(\"Preloading of class %s during loading of shared class %s \"\n+                           \"(cause: null-free non-static field) succeeded\",\n+                           name->as_C_string(), ik->name()->as_C_string());\n+\n+  assert(real_k != nullptr, \"Sanity check\");\n+  InstanceKlass::check_can_be_annotated_with_NullRestricted(real_k, ik->name(), CHECK_false);\n+\n+  return true;\n+}\n+\n+\/\/ Tries to pre-load classes referred to in non-static nullable instance fields if they are found in the\n+\/\/ loadable descriptors attribute. If loading fails, we can fail silently.\n+void SystemDictionary::try_preload_from_loadable_descriptors(InstanceKlass* ik, Handle class_loader, Symbol* sig, int field_index, TRAPS) {\n+  TempNewSymbol name = Signature::strip_envelope(sig);\n+  if (name != ik->name() && ik->is_class_in_loadable_descriptors_attribute(sig)) {\n+    log_info(class, preload)(\"Preloading of class %s during loading of shared class %s. \"\n+                             \"Cause: field type in LoadableDescriptors attribute\",\n+                             name->as_C_string(), ik->name()->as_C_string());\n+    InstanceKlass* real_k = SystemDictionary::resolve_with_circularity_detection(ik->name(), name,\n+                                                                                 class_loader, false, THREAD);\n+    if (HAS_PENDING_EXCEPTION) {\n+      CLEAR_PENDING_EXCEPTION;\n+    }\n+\n+    InstanceKlass* k = ik->get_inline_type_field_klass_or_null(field_index);\n+    if (real_k != k) {\n+      \/\/ oops, the app has substituted a different version of k!\n+      log_info(class, preload)(\"Preloading of class %s during loading of shared class %s \"\n+                                  \"(cause: field type in LoadableDescriptors attribute) failed : \"\n+                                  \"app substituted a different version of %s\",\n+                                  name->as_C_string(), ik->name()->as_C_string(),\n+                                  k->name()->as_C_string());\n+      return;\n+    } else if (real_k != nullptr) {\n+      log_info(class, preload)(\"Preloading of class %s during loading of shared class %s \"\n+                               \"(cause: field type in LoadableDescriptors attribute) succeeded\",\n+                                name->as_C_string(), ik->name()->as_C_string());\n+    }\n+  }\n+}\n+\n+\n@@ -1094,0 +1199,21 @@\n+  if (ik->has_inline_type_fields()) {\n+    for (AllFieldStream fs(ik); !fs.done(); fs.next()) {\n+      if (fs.access_flags().is_static()) continue;\n+\n+      Symbol* sig = fs.signature();\n+      int field_index = fs.index();\n+\n+      if (fs.is_null_free_inline_type()) {\n+        \/\/ A false return means that the class didn't load for other reasons than an exception.\n+        bool check = preload_from_null_free_field(ik, class_loader, sig, field_index, CHECK_NULL);\n+        if (!check) {\n+          ik->set_shared_loading_failed();\n+          return nullptr;\n+        }\n+      } else if (Signature::has_envelope(sig)) {\n+          \/\/ Pending exceptions are cleared so we can fail silently\n+          try_preload_from_loadable_descriptors(ik, class_loader, sig, field_index, CHECK_NULL);\n+      }\n+    }\n+  }\n+\n@@ -1129,0 +1255,1 @@\n+\n@@ -1710,1 +1837,0 @@\n-#if INCLUDE_CDS\n@@ -1713,1 +1839,3 @@\n-\/\/ This API should be used only by AOTLinkedClassBulkLoader\n+\/\/ This API is used by AOTLinkedClassBulkLoader and to register boxing\n+\/\/ classes from java.lang in all class loaders to enable more value\n+\/\/ classes optimizations\n@@ -1717,1 +1845,0 @@\n-  assert(CDSConfig::is_using_aot_linked_classes(), \"must be\");\n@@ -1723,2 +1850,3 @@\n-  assert(dictionary->find_class(current, name) == nullptr, \"sanity\");\n-  dictionary->add_klass(current, name, k);\n+  if (dictionary->find_class(current, name) == nullptr) {\n+    dictionary->add_klass(current, name, k);\n+  }\n@@ -1726,1 +1854,0 @@\n-#endif\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":142,"deletions":15,"binary":false,"changes":157,"status":"modified"},{"patch":"@@ -112,2 +112,2 @@\n-  void set_to_monomorphic();\n-  void set_to_megamorphic(CallInfo* call_info);\n+  void set_to_monomorphic(bool caller_is_c1);\n+  void set_to_megamorphic(CallInfo* call_info, bool caller_is_c1);\n@@ -134,1 +134,1 @@\n-  void update(CallInfo* call_info, Klass* receiver_klass);\n+  void update(CallInfo* call_info, Klass* receiver_klass, bool caller_is_c1);\n@@ -161,1 +161,1 @@\n-\/\/    compilled code <------------> interpreted code\n+\/\/    compiled code <------------> interpreted code\n@@ -216,1 +216,1 @@\n-  void set(const methodHandle& callee_method);\n+  void set(const methodHandle& callee_method, bool caller_is_c1);\n","filename":"src\/hotspot\/share\/code\/compiledIC.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -130,1 +130,1 @@\n-    if (obj->is_objArray()) {\n+    if (obj->is_refArray()) {\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCompactionManager.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"utilities\/exceptions.hpp\"\n@@ -315,0 +316,5 @@\n+\n+    static void value_copy_in_heap(void* src, void* dst, InlineKlass* md, LayoutKind lk) {\n+      Raw::value_copy(src, dst, md, lk);\n+    }\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSet.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -110,0 +110,5 @@\n+  private:\n+    \/\/ Failing checkcast or check null during copy, still needs barrier\n+    template <typename T>\n+    static inline void oop_arraycopy_partial_barrier(BarrierSetT *bs, T* dst_raw, T* p);\n+  public:\n@@ -124,0 +129,2 @@\n+\n+    static void value_copy_in_heap(void* src, void* dst, InlineKlass* md, LayoutKind lk);\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTableBarrierSet.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+  assert(!klass->is_objArray_klass() || klass->is_refArray_klass() || klass->is_flatArray_klass(), \"ObjArrayKlass must never be used to allocate array instances directly\");\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.inline.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrier.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -154,0 +154,2 @@\n+    static void value_copy_in_heap(void* src, void* dst, InlineKlass* md, LayoutKind lk);\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSet.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -371,1 +371,1 @@\n-                             Bytecodes::Code byte, TRAPS);\n+                             Bytecodes::Code byte, bool check_null_and_abstract, TRAPS);\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1630,1 +1630,1 @@\n-              Deoptimization::reassign_fields(vf->frame_pointer(), &reg_map, objects, realloc_failures, false);\n+              Deoptimization::reassign_fields(vf->frame_pointer(), &reg_map, objects, realloc_failures, false, CHECK_NULL);\n@@ -1881,1 +1881,1 @@\n-  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false);\n+  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false, THREAD);\n@@ -2209,1 +2209,1 @@\n-    if (m->is_object_initializer()) {\n+    if (m->is_object_constructor()) {\n@@ -2236,1 +2236,1 @@\n-    if (!m->is_object_initializer() && !m->is_static_initializer() && !m->is_overpass()) {\n+    if (!(m->is_object_constructor() || m->is_class_initializer()) && !m->is_overpass()) {\n@@ -2976,1 +2976,5 @@\n-  if (m->is_object_initializer()) {\n+  if (m->is_class_initializer()) {\n+      JVMCI_THROW_MSG_NULL(IllegalArgumentException,\n+          \"Cannot create java.lang.reflect.Method for class initializer\");\n+  }\n+  else if (m->is_object_constructor()) {\n@@ -2978,3 +2982,0 @@\n-  } else if (m->is_static_initializer()) {\n-    JVMCI_THROW_MSG_NULL(IllegalArgumentException,\n-        \"Cannot create java.lang.reflect.Method for class initializer\");\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -193,0 +193,5 @@\n+class PrintClassLayout : AllStatic {\n+ public:\n+  static void print_class_layout(outputStream* st, char* classname);\n+};\n+\n","filename":"src\/hotspot\/share\/memory\/heapInspection.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -60,0 +61,1 @@\n+\/\/ * value_copy: Copy the contents of a value type from one heap address to another\n@@ -124,0 +126,6 @@\n+  template <DecoratorSet expected_mo_decorators>\n+  static void verify_heap_value_decorators() {\n+    const DecoratorSet heap_value_decorators = IN_HEAP | IS_DEST_UNINITIALIZED;\n+    verify_decorators<expected_mo_decorators | heap_value_decorators>();\n+  }\n+\n@@ -214,0 +222,8 @@\n+  \/\/ inline type heap access (when flat)...\n+\n+  \/\/ Copy value type data from src to dst\n+  static inline void value_copy(void* src, void* dst, InlineKlass* md, LayoutKind lk) {\n+    verify_heap_value_decorators<IN_HEAP>();\n+    AccessInternal::value_copy<decorators>(src, dst, md, lk);\n+  }\n+\n@@ -331,0 +347,1 @@\n+    static_assert((decorators & ARRAYCOPY_NOTNULL) == 0);\n","filename":"src\/hotspot\/share\/oops\/access.hpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -203,0 +203,7 @@\n+  template <class GCBarrierType, DecoratorSet decorators>\n+  struct PostRuntimeDispatch<GCBarrierType, BARRIER_VALUE_COPY, decorators>: public AllStatic {\n+    static void access_barrier(void* src, void* dst, InlineKlass* md, LayoutKind lk) {\n+      GCBarrierType::value_copy_in_heap(src, dst, md, lk);\n+    }\n+  };\n+\n@@ -350,0 +357,7 @@\n+\n+  template <DecoratorSet decorators, typename T>\n+  void RuntimeDispatch<decorators, T, BARRIER_VALUE_COPY>::value_copy_init(void* src, void* dst, InlineKlass* md, LayoutKind lk) {\n+    func_t function = BarrierResolver<decorators, func_t, BARRIER_VALUE_COPY>::resolve_barrier();\n+    _value_copy_func = function;\n+    function(src, dst, md,lk);\n+  }\n","filename":"src\/hotspot\/share\/oops\/access.inline.hpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"oops\/layoutKind.hpp\"\n@@ -33,0 +34,1 @@\n+#include \"utilities\/vmEnums.hpp\"\n@@ -44,1 +46,1 @@\n-\/\/  unused:22 hash:31 -->| unused_gap:4  age:4  self-fwd:1  lock:2 (normal object)\n+\/\/  unused:22 hash:31 -->| valhalla:4  age:4  self-fwd:1  lock:2 (normal object)\n@@ -48,1 +50,1 @@\n-\/\/  klass:22  hash:31 -->| unused_gap:4  age:4  self-fwd:1  lock:2 (normal object)\n+\/\/  klass:22  hash:31 -->| valhalla:4  age:4  self-fwd:1  lock:2 (normal object)\n@@ -63,0 +65,18 @@\n+\/\/\n+\/\/  VALHALLA EXTENSIONS:\n+\/\/\n+\/\/  N.B.: 32 bit mode is not supported, this section assumes 64 bit systems.\n+\/\/\n+\/\/  Project Valhalla uses markWord bits to denote the following oops (listed least to most significant):\n+\/\/  * inline types: have alternative bytecode behavior, e.g. can not be locked\n+\/\/  * flat arrays: load\/decode of klass layout helper is expensive for aaload\n+\/\/  * \"null free\" arrays: load\/decode of klass layout helper again for aaload\n+\/\/  * inline type: \"larval state\": mutable state, but only during object init, observable\n+\/\/      by only by a single thread (generally do not mutate markWord)\n+\/\/\n+\/\/  Inline types cannot be locked, monitored or inflating.\n+\/\/\n+\/\/  Note the position of 'self-fwd' is not by accident. When forwarding an\n+\/\/  object to a new heap position, HeapWord alignment guarantees the lower\n+\/\/  bits, including 'self-fwd' are 0. \"is_self_forwarded()\" will be correctly\n+\/\/  set to false. Otherwise encode_pointer_as_mark() may have 'self-fwd' set.\n@@ -101,2 +121,1 @@\n-  \/\/ Constants\n-  static const int age_bits                       = 4;\n+  \/\/ Constants, in least significant bit order\n@@ -105,1 +124,8 @@\n-  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - self_fwd_bits;\n+  \/\/ instance state\n+  static const int age_bits                       = 4;\n+  \/\/ prototype header bits (fast path instead of klass layout_helper)\n+  static const int inline_type_bits               = 1;\n+  static const int null_free_array_bits           = LP64_ONLY(1) NOT_LP64(0);\n+  static const int flat_array_bits                = LP64_ONLY(1) NOT_LP64(0);\n+  static const int larval_bits                    = 1;\n+  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - inline_type_bits - larval_bits - flat_array_bits - null_free_array_bits - self_fwd_bits;\n@@ -107,1 +133,0 @@\n-  static const int unused_gap_bits                = LP64_ONLY(4) NOT_LP64(0); \/\/ Reserved for Valhalla.\n@@ -112,1 +137,5 @@\n-  static const int hash_shift                     = age_shift + age_bits + unused_gap_bits;\n+  static const int inline_type_shift              = age_shift + age_bits;\n+  static const int null_free_array_shift          = inline_type_shift + inline_type_bits;\n+  static const int flat_array_shift               = null_free_array_shift + null_free_array_bits;\n+  static const int larval_shift                   = flat_array_shift + flat_array_bits;\n+  static const int hash_shift                     = larval_shift + larval_bits;\n@@ -118,0 +147,8 @@\n+  static const uintptr_t inline_type_bit_in_place = right_n_bits(inline_type_bits) << inline_type_shift;\n+  static const uintptr_t inline_type_mask_in_place = inline_type_bit_in_place + lock_mask;\n+  static const uintptr_t null_free_array_mask     = right_n_bits(null_free_array_bits);\n+  static const uintptr_t null_free_array_mask_in_place = (null_free_array_mask << null_free_array_shift) | lock_mask_in_place;\n+  static const uintptr_t null_free_array_bit_in_place  = (right_n_bits(null_free_array_bits) << null_free_array_shift);\n+  static const uintptr_t flat_array_mask          = right_n_bits(flat_array_bits);\n+  static const uintptr_t flat_array_mask_in_place = (flat_array_mask << flat_array_shift) | null_free_array_mask_in_place | lock_mask_in_place;\n+  static const uintptr_t flat_array_bit_in_place  = right_n_bits(flat_array_bits) << flat_array_shift;\n@@ -120,0 +157,5 @@\n+\n+  static const uintptr_t larval_mask              = right_n_bits(larval_bits);\n+  static const uintptr_t larval_mask_in_place     = (larval_mask << larval_shift) | inline_type_mask_in_place;\n+  static const uintptr_t larval_bit_in_place      = right_n_bits(larval_bits) << larval_shift;\n+\n@@ -142,0 +184,7 @@\n+  static const uintptr_t inline_type_pattern      = inline_type_bit_in_place | unlocked_value;\n+  static const uintptr_t null_free_array_pattern  = null_free_array_bit_in_place | unlocked_value;\n+  static const uintptr_t null_free_flat_array_pattern = flat_array_bit_in_place | null_free_array_pattern;\n+  static const uintptr_t nullable_flat_array_pattern = flat_array_bit_in_place | unlocked_value;\n+\n+  static const uintptr_t larval_pattern           = larval_bit_in_place | inline_type_pattern;\n+\n@@ -151,0 +200,4 @@\n+  bool is_inline_type() const {\n+    return (mask_bits(value(), inline_type_mask_in_place) == inline_type_pattern);\n+  }\n+\n@@ -161,0 +214,7 @@\n+\n+  \/\/ is unlocked and not an inline type (which cannot be involved in locking, displacement or inflation)\n+  \/\/ i.e. test both lock bits and the inline type bit together\n+  bool is_neutral()  const {  \/\/ Not locked, or marked - a \"clean\" neutral state\n+    return (mask_bits(value(), inline_type_mask_in_place) == unlocked_value);\n+  }\n+\n@@ -165,3 +225,0 @@\n-  bool is_neutral()  const {  \/\/ Not locked, or marked - a \"clean\" neutral state\n-    return (mask_bits(value(), lock_mask_in_place) == unlocked_value);\n-  }\n@@ -171,1 +228,1 @@\n-    return (!is_unlocked() || !has_no_hash());\n+    return (!is_unlocked() || !has_no_hash() || is_larval_state());\n@@ -238,0 +295,28 @@\n+  \/\/ private buffered value operations\n+  markWord enter_larval_state() const {\n+    return markWord(value() | larval_bit_in_place);\n+  }\n+  markWord exit_larval_state() const {\n+    return markWord(value() & ~larval_bit_in_place);\n+  }\n+  bool is_larval_state() const {\n+    return (mask_bits(value(), larval_mask_in_place) == larval_pattern);\n+  }\n+\n+  bool is_flat_array() const {\n+#ifdef _LP64 \/\/ 64 bit encodings only\n+    return (mask_bits(value(), flat_array_mask_in_place) == null_free_flat_array_pattern)\n+           || (mask_bits(value(), flat_array_mask_in_place) == nullable_flat_array_pattern);\n+#else\n+    return false;\n+#endif\n+  }\n+\n+  bool is_null_free_array() const {\n+#ifdef _LP64 \/\/ 64 bit encodings only\n+    return (mask_bits(value(), null_free_array_mask_in_place) == null_free_array_pattern);\n+#else\n+    return false;\n+#endif\n+  }\n+\n@@ -255,0 +340,12 @@\n+  static markWord inline_type_prototype() {\n+    return markWord(inline_type_pattern);\n+  }\n+\n+#ifdef _LP64 \/\/ 64 bit encodings only\n+  static markWord flat_array_prototype(LayoutKind lk);\n+\n+  static markWord null_free_array_prototype() {\n+    return markWord(null_free_array_pattern);\n+  }\n+#endif\n+\n@@ -261,2 +358,3 @@\n-  \/\/ Recover address of oop from encoded form used in mark\n-  inline void* decode_pointer() const { return (void*)clear_lock_bits().value(); }\n+  inline void* decode_pointer() const {\n+    return (void*) (clear_lock_bits().value());\n+  }\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":111,"deletions":13,"binary":false,"changes":124,"status":"modified"},{"patch":"@@ -130,1 +130,4 @@\n-    speculative_trap_data_tag\n+    speculative_trap_data_tag,\n+    array_store_data_tag,\n+    array_load_data_tag,\n+    acmp_data_tag\n@@ -290,0 +293,1 @@\n+class         ArrayStoreData;\n@@ -294,0 +298,1 @@\n+class       ACmpData;\n@@ -299,0 +304,1 @@\n+class   ArrayLoadData;\n@@ -306,1 +312,1 @@\n-  friend class ReturnTypeEntry;\n+  friend class SingleTypeEntry;\n@@ -425,0 +431,3 @@\n+  virtual bool is_ArrayStoreData() const { return false; }\n+  virtual bool is_ArrayLoadData() const { return false; }\n+  virtual bool is_ACmpData()           const { return false; }\n@@ -483,0 +492,12 @@\n+  ArrayStoreData* as_ArrayStoreData() const {\n+    assert(is_ArrayStoreData(), \"wrong type\");\n+    return is_ArrayStoreData() ? (ArrayStoreData*)this : nullptr;\n+  }\n+  ArrayLoadData* as_ArrayLoadData() const {\n+    assert(is_ArrayLoadData(), \"wrong type\");\n+    return is_ArrayLoadData() ? (ArrayLoadData*)this : nullptr;\n+  }\n+  ACmpData* as_ACmpData() const {\n+    assert(is_ACmpData(), \"wrong type\");\n+    return is_ACmpData() ? (ACmpData*)this : nullptr;\n+  }\n@@ -528,0 +549,1 @@\n+    , last_bit_data_flag\n@@ -548,1 +570,1 @@\n-  bool null_seen()     { return flag_at(null_seen_flag); }\n+  bool null_seen() const  { return flag_at(null_seen_flag); }\n@@ -649,1 +671,2 @@\n-      layout->tag() == DataLayout::branch_data_tag, \"wrong type\");\n+      layout->tag() == DataLayout::branch_data_tag ||\n+      layout->tag() == DataLayout::acmp_data_tag, \"wrong type\");\n@@ -893,1 +916,1 @@\n-class ReturnTypeEntry : public TypeEntries {\n+class SingleTypeEntry : public TypeEntries {\n@@ -901,1 +924,1 @@\n-  ReturnTypeEntry(int base_off)\n+  SingleTypeEntry(int base_off)\n@@ -942,1 +965,1 @@\n-\/\/ (TypeStackSlotEntries), a return type (ReturnTypeEntry) and a\n+\/\/ (TypeStackSlotEntries), a return type (SingleTypeEntry) and a\n@@ -996,1 +1019,1 @@\n-    return ReturnTypeEntry::size() + in_ByteSize(header_cell_count() * DataLayout::cell_size);\n+    return SingleTypeEntry::size() + in_ByteSize(header_cell_count() * DataLayout::cell_size);\n@@ -1011,1 +1034,1 @@\n-  ReturnTypeEntry _ret;\n+  SingleTypeEntry _ret;\n@@ -1030,1 +1053,1 @@\n-    _ret(cell_count() - ReturnTypeEntry::static_cell_count())\n+    _ret(cell_count() - SingleTypeEntry::static_cell_count())\n@@ -1043,1 +1066,1 @@\n-  const ReturnTypeEntry* ret() const {\n+  const SingleTypeEntry* ret() const {\n@@ -1167,1 +1190,2 @@\n-           layout->tag() == DataLayout::virtual_call_type_data_tag, \"wrong type\");\n+           layout->tag() == DataLayout::virtual_call_type_data_tag ||\n+           layout->tag() == DataLayout::array_store_data_tag, \"wrong type\");\n@@ -1300,1 +1324,1 @@\n-  ReturnTypeEntry _ret;\n+  SingleTypeEntry _ret;\n@@ -1319,1 +1343,1 @@\n-    _ret(cell_count() - ReturnTypeEntry::static_cell_count())\n+    _ret(cell_count() - SingleTypeEntry::static_cell_count())\n@@ -1332,1 +1356,1 @@\n-  const ReturnTypeEntry* ret() const {\n+  const SingleTypeEntry* ret() const {\n@@ -1545,1 +1569,1 @@\n-    assert(layout->tag() == DataLayout::branch_data_tag, \"wrong type\");\n+    assert(layout->tag() == DataLayout::branch_data_tag || layout->tag() == DataLayout::acmp_data_tag, \"wrong type\");\n@@ -1906,0 +1930,223 @@\n+class ArrayStoreData : public ReceiverTypeData {\n+private:\n+  enum {\n+    flat_array_flag = BitData::last_bit_data_flag,\n+    null_free_array_flag = flat_array_flag + 1,\n+  };\n+\n+  SingleTypeEntry _array;\n+\n+public:\n+  ArrayStoreData(DataLayout* layout) :\n+    ReceiverTypeData(layout),\n+    _array(ReceiverTypeData::static_cell_count()) {\n+    assert(layout->tag() == DataLayout::array_store_data_tag, \"wrong type\");\n+    _array.set_profile_data(this);\n+  }\n+\n+  const SingleTypeEntry* array() const {\n+    return &_array;\n+  }\n+\n+  virtual bool is_ArrayStoreData() const { return true; }\n+\n+  static int static_cell_count() {\n+    return ReceiverTypeData::static_cell_count() + SingleTypeEntry::static_cell_count();\n+  }\n+\n+  virtual int cell_count() const {\n+    return static_cell_count();\n+  }\n+\n+  void set_flat_array() { set_flag_at(flat_array_flag); }\n+  bool flat_array() const { return flag_at(flat_array_flag); }\n+\n+  void set_null_free_array() { set_flag_at(null_free_array_flag); }\n+  bool null_free_array() const { return flag_at(null_free_array_flag); }\n+\n+  \/\/ Code generation support\n+  static int flat_array_byte_constant() {\n+    return flag_number_to_constant(flat_array_flag);\n+  }\n+\n+  static int null_free_array_byte_constant() {\n+    return flag_number_to_constant(null_free_array_flag);\n+  }\n+\n+  static ByteSize array_offset() {\n+    return cell_offset(ReceiverTypeData::static_cell_count());\n+  }\n+\n+  virtual void clean_weak_klass_links(bool always_clean) {\n+    ReceiverTypeData::clean_weak_klass_links(always_clean);\n+    _array.clean_weak_klass_links(always_clean);\n+  }\n+\n+  virtual void metaspace_pointers_do(MetaspaceClosure* it) {\n+    ReceiverTypeData::metaspace_pointers_do(it);\n+    _array.metaspace_pointers_do(it);\n+  }\n+\n+  static ByteSize array_store_data_size() {\n+    return cell_offset(static_cell_count());\n+  }\n+\n+  virtual void print_data_on(outputStream* st, const char* extra = nullptr) const;\n+};\n+\n+class ArrayLoadData : public BitData {\n+private:\n+  enum {\n+    flat_array_flag = BitData::last_bit_data_flag,\n+    null_free_array_flag = flat_array_flag + 1,\n+  };\n+\n+  SingleTypeEntry _array;\n+  SingleTypeEntry _element;\n+\n+public:\n+  ArrayLoadData(DataLayout* layout) :\n+    BitData(layout),\n+    _array(0),\n+    _element(SingleTypeEntry::static_cell_count()) {\n+    assert(layout->tag() == DataLayout::array_load_data_tag, \"wrong type\");\n+    _array.set_profile_data(this);\n+    _element.set_profile_data(this);\n+  }\n+\n+  const SingleTypeEntry* array() const {\n+    return &_array;\n+  }\n+\n+  const SingleTypeEntry* element() const {\n+    return &_element;\n+  }\n+\n+  virtual bool is_ArrayLoadData() const { return true; }\n+\n+  static int static_cell_count() {\n+    return SingleTypeEntry::static_cell_count() * 2;\n+  }\n+\n+  virtual int cell_count() const {\n+    return static_cell_count();\n+  }\n+\n+  void set_flat_array() { set_flag_at(flat_array_flag); }\n+  bool flat_array() const { return flag_at(flat_array_flag); }\n+\n+  void set_null_free_array() { set_flag_at(null_free_array_flag); }\n+  bool null_free_array() const { return flag_at(null_free_array_flag); }\n+\n+  \/\/ Code generation support\n+  static int flat_array_byte_constant() {\n+    return flag_number_to_constant(flat_array_flag);\n+  }\n+\n+  static int null_free_array_byte_constant() {\n+    return flag_number_to_constant(null_free_array_flag);\n+  }\n+\n+  static ByteSize array_offset() {\n+    return cell_offset(0);\n+  }\n+\n+  static ByteSize element_offset() {\n+    return cell_offset(SingleTypeEntry::static_cell_count());\n+  }\n+\n+  virtual void clean_weak_klass_links(bool always_clean) {\n+    _array.clean_weak_klass_links(always_clean);\n+    _element.clean_weak_klass_links(always_clean);\n+  }\n+\n+  virtual void metaspace_pointers_do(MetaspaceClosure* it) {\n+    _array.metaspace_pointers_do(it);\n+    _element.metaspace_pointers_do(it);\n+  }\n+\n+  static ByteSize array_load_data_size() {\n+    return cell_offset(static_cell_count());\n+  }\n+\n+  virtual void print_data_on(outputStream* st, const char* extra = nullptr) const;\n+};\n+\n+class ACmpData : public BranchData {\n+private:\n+  enum {\n+    left_inline_type_flag = DataLayout::first_flag,\n+    right_inline_type_flag\n+  };\n+\n+  SingleTypeEntry _left;\n+  SingleTypeEntry _right;\n+\n+public:\n+  ACmpData(DataLayout* layout) :\n+    BranchData(layout),\n+    _left(BranchData::static_cell_count()),\n+    _right(BranchData::static_cell_count() + SingleTypeEntry::static_cell_count()) {\n+    assert(layout->tag() == DataLayout::acmp_data_tag, \"wrong type\");\n+    _left.set_profile_data(this);\n+    _right.set_profile_data(this);\n+  }\n+\n+  const SingleTypeEntry* left() const {\n+    return &_left;\n+  }\n+\n+  const SingleTypeEntry* right() const {\n+    return &_right;\n+  }\n+\n+  virtual bool is_ACmpData() const { return true; }\n+\n+  static int static_cell_count() {\n+    return BranchData::static_cell_count() + SingleTypeEntry::static_cell_count() * 2;\n+  }\n+\n+  virtual int cell_count() const {\n+    return static_cell_count();\n+  }\n+\n+  void set_left_inline_type() { set_flag_at(left_inline_type_flag); }\n+  bool left_inline_type() const { return flag_at(left_inline_type_flag); }\n+\n+  void set_right_inline_type() { set_flag_at(right_inline_type_flag); }\n+  bool right_inline_type() const { return flag_at(right_inline_type_flag); }\n+\n+  \/\/ Code generation support\n+  static int left_inline_type_byte_constant() {\n+    return flag_number_to_constant(left_inline_type_flag);\n+  }\n+\n+  static int right_inline_type_byte_constant() {\n+    return flag_number_to_constant(right_inline_type_flag);\n+  }\n+\n+  static ByteSize left_offset() {\n+    return cell_offset(BranchData::static_cell_count());\n+  }\n+\n+  static ByteSize right_offset() {\n+    return cell_offset(BranchData::static_cell_count() + SingleTypeEntry::static_cell_count());\n+  }\n+\n+  virtual void clean_weak_klass_links(bool always_clean) {\n+    _left.clean_weak_klass_links(always_clean);\n+    _right.clean_weak_klass_links(always_clean);\n+  }\n+\n+  virtual void metaspace_pointers_do(MetaspaceClosure* it) {\n+    _left.metaspace_pointers_do(it);\n+    _right.metaspace_pointers_do(it);\n+  }\n+\n+  static ByteSize acmp_data_size() {\n+    return cell_offset(static_cell_count());\n+  }\n+\n+  virtual void print_data_on(outputStream* st, const char* extra = nullptr) const;\n+};\n+\n","filename":"src\/hotspot\/share\/oops\/methodData.hpp","additions":263,"deletions":16,"binary":false,"changes":279,"status":"modified"},{"patch":"@@ -869,0 +869,6 @@\n+  if (tp->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to extract the field being accessed from\n+    \/\/ the address computation\n+    return tp->is_aryptr()->add_field_offset_and_offset(txoffset);\n+  }\n@@ -889,0 +895,6 @@\n+  if (p1->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to extract the field being accessed from\n+    \/\/ the address computation\n+    return p1->is_aryptr()->add_field_offset_and_offset(p2offset);\n+  }\n","filename":"src\/hotspot\/share\/opto\/addnode.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -47,3 +47,2 @@\n-  virtual bool           do_late_inline_check(Compile* C, JVMState* jvms) { ShouldNotReachHere(); return false;  }\n-  virtual CallGenerator* inline_cg()    const                             { ShouldNotReachHere(); return nullptr;}\n-  virtual bool           is_pure_call() const                             { ShouldNotReachHere(); return false;  }\n+  virtual bool           do_late_inline_check(Compile* C, JVMState* jvms) { ShouldNotReachHere(); return false; }\n+  virtual bool           is_pure_call() const                             { ShouldNotReachHere(); return false; }\n@@ -91,0 +90,2 @@\n+  virtual CallGenerator* inline_cg()    const                             { ShouldNotReachHere(); return nullptr;  }\n+\n","filename":"src\/hotspot\/share\/opto\/callGenerator.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -523,0 +524,1 @@\n+\n@@ -969,1 +971,2 @@\n-             cmp1->is_SubTypeCheck() || cmp2->is_SubTypeCheck()) {\n+             cmp1->is_SubTypeCheck() || cmp2->is_SubTypeCheck() ||\n+             cmp1->is_FlatArrayCheck() || cmp2->is_FlatArrayCheck()) {\n@@ -1048,1 +1051,1 @@\n-\/\/ note that these functions assume that the _adr_type field is flattened\n+\/\/ note that these functions assume that the _adr_type field is flat\n@@ -1066,1 +1069,1 @@\n-  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at), \"flatten at\");\n+  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at) || (flatten_phi_adr_type(at) == TypeAryPtr::INLINES && Compile::current()->flat_accesses_share_alias()), \"flatten at\");\n@@ -1195,0 +1198,8 @@\n+  \/\/ Flat array element shouldn't get their own memory slice until flat_accesses_share_alias is cleared.\n+  \/\/ It could be the graph has no loads\/stores and flat_accesses_share_alias is never cleared. EA could still\n+  \/\/ creates per element Phis but that wouldn't be a problem as there are no memory accesses for that array.\n+  assert(_adr_type == nullptr || _adr_type->isa_aryptr() == nullptr ||\n+         _adr_type->is_aryptr()->is_known_instance() ||\n+         !_adr_type->is_aryptr()->is_flat() ||\n+         !Compile::current()->flat_accesses_share_alias() ||\n+         _adr_type == TypeAryPtr::INLINES, \"flat array element shouldn't get its own slice yet\");\n@@ -1478,0 +1489,1 @@\n+\n@@ -1532,0 +1544,4 @@\n+  uin = unique_constant_input_recursive(phase);\n+  if (uin != nullptr) {\n+    return uin;\n+  }\n@@ -1622,0 +1638,36 @@\n+\/\/ Find the unique input, try to look recursively through input Phis\n+Node* PhiNode::unique_constant_input_recursive(PhaseGVN* phase) {\n+  if (!phase->is_IterGVN()) {\n+    return nullptr;\n+  }\n+\n+  ResourceMark rm;\n+  Node* unique = nullptr;\n+  Unique_Node_List visited;\n+  visited.push(this);\n+\n+  for (uint visited_idx = 0; visited_idx < visited.size(); visited_idx++) {\n+    Node* current = visited.at(visited_idx);\n+    for (uint i = 1; i < current->req(); i++) {\n+      Node* phi_in = current->in(i);\n+      if (phi_in == nullptr) {\n+        continue;\n+      }\n+\n+      if (phi_in->is_Phi()) {\n+        visited.push(phi_in);\n+      } else {\n+        if (unique == nullptr) {\n+          if (!phi_in->is_Con()) {\n+            return nullptr;\n+          }\n+          unique = phi_in;\n+        } else if (unique != phi_in) {\n+          return nullptr;\n+        }\n+      }\n+    }\n+  }\n+  return unique;\n+}\n+\n@@ -2089,0 +2141,52 @@\n+\/\/ Push inline type input nodes (and null) down through the phi recursively (can handle data loops).\n+InlineTypeNode* PhiNode::push_inline_types_down(PhaseGVN* phase, bool can_reshape, ciInlineKlass* inline_klass) {\n+  assert(inline_klass != nullptr, \"must be\");\n+  InlineTypeNode* vt = InlineTypeNode::make_null(*phase, inline_klass, \/* transform = *\/ false)->clone_with_phis(phase, in(0), nullptr, !_type->maybe_null(), true);\n+  if (can_reshape) {\n+    \/\/ Replace phi right away to be able to use the inline\n+    \/\/ type node when reaching the phi again through data loops.\n+    PhaseIterGVN* igvn = phase->is_IterGVN();\n+    for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+      Node* u = fast_out(i);\n+      igvn->rehash_node_delayed(u);\n+      imax -= u->replace_edge(this, vt);\n+      --i;\n+    }\n+    igvn->rehash_node_delayed(this);\n+    assert(outcnt() == 0, \"should be dead now\");\n+  }\n+  ResourceMark rm;\n+  Node_List casts;\n+  for (uint i = 1; i < req(); ++i) {\n+    Node* n = in(i);\n+    while (n->is_ConstraintCast()) {\n+      casts.push(n);\n+      n = n->in(1);\n+    }\n+    if (phase->type(n)->is_zero_type()) {\n+      n = InlineTypeNode::make_null(*phase, inline_klass);\n+    } else if (n->is_Phi()) {\n+      assert(can_reshape, \"can only handle phis during IGVN\");\n+      n = phase->transform(n->as_Phi()->push_inline_types_down(phase, can_reshape, inline_klass));\n+    }\n+    while (casts.size() != 0) {\n+      \/\/ Push the cast(s) through the InlineTypeNode\n+      \/\/ TODO 8302217 Can we avoid cloning? See InlineTypeNode::clone_if_required\n+      Node* cast = casts.pop()->clone();\n+      cast->set_req_X(1, n->as_InlineType()->get_oop(), phase);\n+      n = n->clone();\n+      n->as_InlineType()->set_oop(*phase, phase->transform(cast));\n+      n = phase->transform(n);\n+      if (n->is_top()) {\n+        break;\n+      }\n+    }\n+    bool transform = !can_reshape && (i == (req()-1)); \/\/ Transform phis on last merge\n+    assert(n->is_top() || n->is_InlineType(), \"Only InlineType or top at this point.\");\n+    if (n->is_InlineType()) {\n+      vt->merge_with(phase, n->as_InlineType(), i, transform);\n+    } \/\/ else nothing to do: phis above vt created by clone_with_phis are initialized to top already.\n+  }\n+  return vt;\n+}\n+\n@@ -2525,0 +2629,2 @@\n+    \/\/ TODO revisit this with JDK-8247216\n+    bool mergemem_only = true;\n@@ -2540,0 +2646,2 @@\n+      } else {\n+        mergemem_only = false;\n@@ -2563,1 +2671,1 @@\n-    if (!split_always_terminates && adr_type() == TypePtr::BOTTOM &&\n+    if (!mergemem_only && !split_always_terminates && adr_type() == TypePtr::BOTTOM &&\n@@ -2598,1 +2706,1 @@\n-      } else if (split_always_terminates) {\n+      } else if (mergemem_only || split_always_terminates) {\n@@ -2634,0 +2742,5 @@\n+            if (igvn) {\n+              \/\/ TODO revisit this with JDK-8247216\n+              \/\/ Put 'n' on the worklist because it might be modified by MergeMemStream::iteration_setup\n+              igvn->_worklist.push(n);\n+            }\n@@ -2678,1 +2791,1 @@\n-        set_req_X(i, new_in, phase);\n+        set_req_X(i, new_in, phase->is_IterGVN());\n@@ -2746,0 +2859,5 @@\n+  Node* inline_type = try_push_inline_types_down(phase, can_reshape);\n+  if (inline_type != this) {\n+    return inline_type;\n+  }\n+\n@@ -2808,0 +2926,95 @@\n+\/\/ Check recursively if inputs are either an inline type, constant null\n+\/\/ or another Phi (including self references through data loops). If so,\n+\/\/ push the inline types down through the phis to enable folding of loads.\n+Node* PhiNode::try_push_inline_types_down(PhaseGVN* phase, const bool can_reshape) {\n+  if (!can_be_inline_type()) {\n+    return this;\n+  }\n+\n+  ciInlineKlass* inline_klass;\n+  if (can_push_inline_types_down(phase, can_reshape, inline_klass)) {\n+    assert(inline_klass != nullptr, \"must be\");\n+    return push_inline_types_down(phase, can_reshape, inline_klass);\n+  }\n+  return this;\n+}\n+\n+bool PhiNode::can_push_inline_types_down(PhaseGVN* phase, const bool can_reshape, ciInlineKlass*& inline_klass) {\n+  if (req() <= 2) {\n+    \/\/ Dead phi.\n+    return false;\n+  }\n+  inline_klass = nullptr;\n+\n+  \/\/ TODO 8302217 We need to prevent endless pushing through\n+  bool only_phi = (outcnt() != 0);\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    Node* n = fast_out(i);\n+    if (n->is_InlineType() && n->in(1) == this) {\n+      return false;\n+    }\n+    if (!n->is_Phi()) {\n+      only_phi = false;\n+    }\n+  }\n+  if (only_phi) {\n+    return false;\n+  }\n+\n+  ResourceMark rm;\n+  Unique_Node_List worklist;\n+  worklist.push(this);\n+  Node_List casts;\n+\n+  for (uint next = 0; next < worklist.size(); next++) {\n+    Node* phi = worklist.at(next);\n+    for (uint i = 1; i < phi->req(); i++) {\n+      Node* n = phi->in(i);\n+      if (n == nullptr) {\n+        return false;\n+      }\n+      while (n->is_ConstraintCast()) {\n+        if (n->in(0) != nullptr && n->in(0)->is_top()) {\n+          \/\/ Will die, don't optimize\n+          return false;\n+        }\n+        casts.push(n);\n+        n = n->in(1);\n+      }\n+      const Type* type = phase->type(n);\n+      if (n->is_InlineType() && (inline_klass == nullptr || inline_klass == type->inline_klass())) {\n+        inline_klass = type->inline_klass();\n+      } else if (n->is_Phi() && can_reshape && n->bottom_type()->isa_ptr()) {\n+        worklist.push(n);\n+      } else if (!type->is_zero_type()) {\n+        return false;\n+      }\n+    }\n+  }\n+  if (inline_klass == nullptr) {\n+    return false;\n+  }\n+\n+  \/\/ Check if cast nodes can be pushed through\n+  const Type* t = Type::get_const_type(inline_klass);\n+  while (casts.size() != 0 && t != nullptr) {\n+    Node* cast = casts.pop();\n+    if (t->filter(cast->bottom_type()) == Type::TOP) {\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+#ifdef ASSERT\n+bool PhiNode::can_push_inline_types_down(PhaseGVN* phase) {\n+  if (!can_be_inline_type()) {\n+    return false;\n+  }\n+\n+  ciInlineKlass* inline_klass;\n+  return can_push_inline_types_down(phase, true, inline_klass);\n+}\n+#endif \/\/ ASSERT\n+\n@@ -3197,0 +3410,6 @@\n+\n+  \/\/ CheckCastPPNode::Ideal() for inline types reuses the exception\n+  \/\/ paths of a call to perform an allocation: we can see a Phi here.\n+  if (in(1)->is_Phi()) {\n+    return this;\n+  }\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":225,"deletions":6,"binary":false,"changes":231,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -183,0 +184,3 @@\n+  bool can_push_inline_types_down(PhaseGVN* phase, bool can_reshape, ciInlineKlass*& inline_klass);\n+  InlineTypeNode* push_inline_types_down(PhaseGVN* phase, bool can_reshape, ciInlineKlass* inline_klass);\n+\n@@ -235,0 +239,1 @@\n+  Node* unique_constant_input_recursive(PhaseGVN* phase);\n@@ -261,0 +266,7 @@\n+  bool can_be_inline_type() const {\n+    return Arguments::is_valhalla_enabled() && _type->isa_instptr() && _type->is_instptr()->can_be_inline_type();\n+  }\n+\n+  Node* try_push_inline_types_down(PhaseGVN* phase, bool can_reshape);\n+  DEBUG_ONLY(bool can_push_inline_types_down(PhaseGVN* phase);)\n+\n@@ -473,0 +485,2 @@\n+  bool is_flat_array_check(PhaseTransform* phase, Node** array = nullptr);\n+\n@@ -761,0 +775,1 @@\n+    init_class_id(Class_Blackhole);\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -488,0 +488,2 @@\n+  \/\/ Expand flat accesses to accesses to each component if the object does not escape\n+  void optimize_flat_accesses(GrowableArray<SafePointNode*>& sfn_worklist);\n","filename":"src\/hotspot\/share\/opto\/escape.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -26,0 +26,3 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n+#include \"ci\/ciMethod.hpp\"\n@@ -34,0 +37,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -39,0 +43,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -42,0 +47,2 @@\n+#include \"opto\/multnode.hpp\"\n+#include \"opto\/narrowptrnode.hpp\"\n@@ -47,0 +54,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -49,0 +57,1 @@\n+#include \"runtime\/stubRoutines.hpp\"\n@@ -55,1 +64,1 @@\n-GraphKit::GraphKit(JVMState* jvms)\n+GraphKit::GraphKit(JVMState* jvms, PhaseGVN* gvn)\n@@ -58,1 +67,1 @@\n-    _gvn(*C->initial_gvn()),\n+    _gvn((gvn != nullptr) ? *gvn : *C->initial_gvn()),\n@@ -61,0 +70,1 @@\n+  assert(gvn == nullptr || !gvn->is_IterGVN() || gvn->is_IterGVN()->delay_transform(), \"delay transform should be enabled\");\n@@ -64,0 +74,7 @@\n+#ifdef ASSERT\n+  if (_gvn.is_IterGVN() != nullptr) {\n+    assert(_gvn.is_IterGVN()->delay_transform(), \"Transformation must be delayed if IterGVN is used\");\n+    \/\/ Save the initial size of _for_igvn worklist for verification (see ~GraphKit)\n+    _worklist_size = _gvn.C->igvn_worklist()->size();\n+  }\n+#endif\n@@ -348,1 +365,2 @@\n-  assert(ex_jvms->sp() == phi_map->_jvms->sp(), \"matching stack sizes\");\n+  \/\/ TODO 8325632 Re-enable\n+  \/\/ assert(ex_jvms->sp() == phi_map->_jvms->sp(), \"matching stack sizes\");\n@@ -876,1 +894,1 @@\n-           (is_anewarray && code == Bytecodes::_multianewarray);\n+           (is_anewarray && (code == Bytecodes::_multianewarray));\n@@ -944,1 +962,2 @@\n-    assert(out_jvms->sp() >= (uint)inputs, \"not enough operands for reexecution\");\n+    \/\/ TODO 8371125\n+    \/\/ assert(out_jvms->sp() >= (uint)inputs, \"not enough operands for reexecution\");\n@@ -967,0 +986,2 @@\n+\n+  JVMState* callee_jvms = nullptr;\n@@ -992,2 +1013,3 @@\n-      for (j = 0; j < l; j++)\n-        call->set_req(p++, in_map->in(k+j));\n+      for (j = 0; j < l; j++) {\n+        call->set_req(p++, in_map->in(k + j));\n+      }\n@@ -1003,2 +1025,3 @@\n-      for (j = 0; j < l; j++)\n-        call->set_req(p++, in_map->in(k+j));\n+      for (j = 0; j < l; j++) {\n+        call->set_req(p++, in_map->in(k + j));\n+      }\n@@ -1043,0 +1066,1 @@\n+    callee_jvms = out_jvms;\n@@ -1218,1 +1242,1 @@\n-  return _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+  return _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n@@ -1267,1 +1291,2 @@\n-                                  bool speculative) {\n+                                  bool speculative,\n+                                  bool null_marker_check) {\n@@ -1272,0 +1297,23 @@\n+  if (value->is_InlineType()) {\n+    \/\/ Null checking a scalarized but nullable inline type. Check the null marker\n+    \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+    InlineTypeNode* vtptr = value->as_InlineType();\n+    while (vtptr->get_oop()->is_InlineType()) {\n+      vtptr = vtptr->get_oop()->as_InlineType();\n+    }\n+    null_check_common(vtptr->get_null_marker(), T_INT, assert_null, null_control, speculative, true);\n+    if (stopped()) {\n+      return top();\n+    }\n+    if (assert_null) {\n+      \/\/ TODO 8284443 Scalarize here (this currently leads to compilation bailouts)\n+      \/\/ vtptr = InlineTypeNode::make_null(_gvn, vtptr->type()->inline_klass());\n+      \/\/ replace_in_map(value, vtptr);\n+      \/\/ return vtptr;\n+      replace_in_map(value, null());\n+      return null();\n+    }\n+    bool do_replace_in_map = (null_control == nullptr || (*null_control) == top());\n+    return cast_not_null(value, do_replace_in_map);\n+  }\n+\n@@ -1375,1 +1423,1 @@\n-  } else if (type == T_OBJECT) {\n+  } else if (type == T_OBJECT || null_marker_check) {\n@@ -1449,1 +1497,0 @@\n-\n@@ -1453,0 +1500,9 @@\n+  if (obj->is_InlineType()) {\n+    Node* vt = obj->isa_InlineType()->clone_if_required(&gvn(), map(), do_replace_in_map);\n+    vt->as_InlineType()->set_null_marker(_gvn);\n+    vt = _gvn.transform(vt);\n+    if (do_replace_in_map) {\n+      replace_in_map(obj, vt);\n+    }\n+    return vt;\n+  }\n@@ -1469,0 +1525,11 @@\n+Node* GraphKit::cast_to_non_larval(Node* obj) {\n+  const Type* obj_type = gvn().type(obj);\n+  if (obj->is_InlineType() || !obj_type->is_inlinetypeptr()) {\n+    return obj;\n+  }\n+\n+  Node* new_obj = InlineTypeNode::make_from_oop(this, obj, obj_type->inline_klass());\n+  replace_in_map(obj, new_obj);\n+  return new_obj;\n+}\n+\n@@ -1581,0 +1648,1 @@\n+\n@@ -1634,1 +1702,3 @@\n-                                DecoratorSet decorators) {\n+                                DecoratorSet decorators,\n+                                bool safe_for_replace,\n+                                const InlineTypeNode* vt) {\n@@ -1647,0 +1717,7 @@\n+  if (val->is_InlineType()) {\n+    \/\/ Store to non-flat field. Buffer the inline type and make sure\n+    \/\/ the store is re-executed if the allocation triggers deoptimization.\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    val = val->as_InlineType()->buffer(this, safe_for_replace);\n+  }\n@@ -1650,1 +1727,1 @@\n-  C2ParseAccess access(this, decorators | C2_WRITE_ACCESS, bt, obj, addr);\n+  C2ParseAccess access(this, decorators | C2_WRITE_ACCESS, bt, obj, addr, nullptr, vt);\n@@ -1663,1 +1740,2 @@\n-                               DecoratorSet decorators) {\n+                               DecoratorSet decorators,\n+                               Node* ctl) {\n@@ -1669,1 +1747,1 @@\n-  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr);\n+  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr, ctl);\n@@ -1774,2 +1852,15 @@\n-  uint shift  = exact_log2(type2aelembytes(elembt));\n-  uint header = arrayOopDesc::base_offset_in_bytes(elembt);\n+  const TypeAryPtr* arytype = _gvn.type(ary)->is_aryptr();\n+  uint shift;\n+  uint header;\n+  if (arytype->is_flat() && arytype->klass_is_exact()) {\n+    \/\/ We can only determine the flat array layout statically if the klass is exact. Otherwise, we could have different\n+    \/\/ value classes at runtime with a potentially different layout. The caller needs to fall back to call\n+    \/\/ load\/store_unknown_inline_Type() at runtime. We could return a sentinel node for the non-exact case but that\n+    \/\/ might mess with other GVN transformations in between. Thus, we just continue in the else branch normally, even\n+    \/\/ though we don't need the address node in this case and throw it away again.\n+    shift = arytype->flat_log_elem_size();\n+    header = arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT);\n+  } else {\n+    shift = exact_log2(type2aelembytes(elembt));\n+    header = arrayOopDesc::base_offset_in_bytes(elembt);\n+  }\n@@ -1791,0 +1882,33 @@\n+Node* GraphKit::cast_to_flat_array(Node* array, ciInlineKlass* elem_vk) {\n+  assert(elem_vk->maybe_flat_in_array(), \"no flat array for %s\", elem_vk->name()->as_utf8());\n+  if (!elem_vk->has_atomic_layout() && !elem_vk->has_nullable_atomic_layout()) {\n+    return cast_to_flat_array_exact(array, elem_vk, true, false);\n+  } else if (!elem_vk->has_nullable_atomic_layout() && !elem_vk->has_non_atomic_layout()) {\n+    return cast_to_flat_array_exact(array, elem_vk, true, true);\n+  } else if (!elem_vk->has_atomic_layout() && !elem_vk->has_non_atomic_layout()) {\n+    return cast_to_flat_array_exact(array, elem_vk, false, true);\n+  }\n+\n+  bool is_null_free = false;\n+  if (!elem_vk->has_nullable_atomic_layout()) {\n+    \/\/ Element does not have a nullable flat layout, cannot be nullable\n+    is_null_free = true;\n+  }\n+\n+  ciArrayKlass* array_klass = ciObjArrayKlass::make(elem_vk, false);\n+  const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();\n+  arytype = arytype->cast_to_flat(true)->cast_to_null_free(is_null_free);\n+  return _gvn.transform(new CheckCastPPNode(control(), array, arytype, ConstraintCastNode::DependencyType::NonFloatingNarrowing));\n+}\n+\n+Node* GraphKit::cast_to_flat_array_exact(Node* array, ciInlineKlass* elem_vk, bool is_null_free, bool is_atomic) {\n+  assert(is_null_free || is_atomic, \"nullable arrays must be atomic\");\n+  ciArrayKlass* array_klass = ciObjArrayKlass::make(elem_vk, true, is_null_free, is_atomic);\n+  const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();\n+  assert(arytype->klass_is_exact(), \"inconsistency\");\n+  assert(arytype->is_flat(), \"inconsistency\");\n+  assert(arytype->is_null_free() == is_null_free, \"inconsistency\");\n+  assert(arytype->is_not_null_free() == !is_null_free, \"inconsistency\");\n+  return _gvn.transform(new CheckCastPPNode(control(), array, arytype, ConstraintCastNode::DependencyType::NonFloatingNarrowing));\n+}\n+\n@@ -1806,6 +1930,42 @@\n-void GraphKit::set_arguments_for_java_call(CallJavaNode* call) {\n-  \/\/ Add the call arguments:\n-  uint nargs = call->method()->arg_size();\n-  for (uint i = 0; i < nargs; i++) {\n-    Node* arg = argument(i);\n-    call->init_req(i + TypeFunc::Parms, arg);\n+void GraphKit::set_arguments_for_java_call(CallJavaNode* call, bool is_late_inline) {\n+  PreserveReexecuteState preexecs(this);\n+  if (Arguments::is_valhalla_enabled()) {\n+    \/\/ Make sure the call is \"re-executed\", if buffering of inline type arguments triggers deoptimization.\n+    \/\/ At this point, the call hasn't been executed yet, so we will only ever execute the call once.\n+    jvms()->set_should_reexecute(true);\n+    int arg_size = method()->get_declared_signature_at_bci(bci())->arg_size_for_bc(java_bc());\n+    inc_sp(arg_size);\n+  }\n+  \/\/ Add the call arguments\n+  const TypeTuple* domain = call->tf()->domain_sig();\n+  uint nargs = domain->cnt();\n+  int arg_num = 0;\n+  for (uint i = TypeFunc::Parms, idx = TypeFunc::Parms; i < nargs; i++) {\n+    Node* arg = argument(i-TypeFunc::Parms);\n+    const Type* t = domain->field_at(i);\n+    \/\/ TODO 8284443 A static call to a mismatched method should still be scalarized\n+    if (t->is_inlinetypeptr() && !call->method()->get_Method()->mismatch() && call->method()->is_scalarized_arg(arg_num)) {\n+      \/\/ We don't pass inline type arguments by reference but instead pass each field of the inline type\n+      if (!arg->is_InlineType()) {\n+        assert(_gvn.type(arg)->is_zero_type() && !t->inline_klass()->is_null_free(), \"Unexpected argument type\");\n+        arg = InlineTypeNode::make_from_oop(this, arg, t->inline_klass());\n+      }\n+      InlineTypeNode* vt = arg->as_InlineType();\n+      vt->pass_fields(this, call, idx, true, !t->maybe_null());\n+      \/\/ If an inline type argument is passed as fields, attach the Method* to the call site\n+      \/\/ to be able to access the extended signature later via attached_method_before_pc().\n+      \/\/ For example, see CompiledMethod::preserve_callee_argument_oops().\n+      call->set_override_symbolic_info(true);\n+      \/\/ Register an calling convention dependency on the callee method to make sure that this method is deoptimized and\n+      \/\/ re-compiled with a non-scalarized calling convention if the callee method is later marked as mismatched.\n+      C->dependencies()->assert_mismatch_calling_convention(call->method());\n+      arg_num++;\n+      continue;\n+    } else if (arg->is_InlineType()) {\n+      \/\/ Pass inline type argument via oop to callee\n+      arg = arg->as_InlineType()->buffer(this, true);\n+    }\n+    if (t != Type::HALF) {\n+      arg_num++;\n+    }\n+    call->init_req(idx++, arg);\n@@ -1849,7 +2009,0 @@\n-  \/\/ Capture the return value, if any.\n-  Node* ret;\n-  if (call->method() == nullptr ||\n-      call->method()->return_type()->basic_type() == T_VOID)\n-        ret = top();\n-  else  ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n-\n@@ -1868,0 +2021,79 @@\n+\n+  \/\/ Capture the return value, if any.\n+  Node* ret;\n+  if (call->method() == nullptr || call->method()->return_type()->basic_type() == T_VOID) {\n+    ret = top();\n+  } else if (call->tf()->returns_inline_type_as_fields()) {\n+    \/\/ Return of multiple values (inline type fields): we create a\n+    \/\/ InlineType node, each field is a projection from the call.\n+    ciInlineKlass* vk = call->method()->return_type()->as_inline_klass();\n+    uint base_input = TypeFunc::Parms;\n+    ret = InlineTypeNode::make_from_multi(this, call, vk, base_input, false, false);\n+  } else {\n+    ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+    ciType* t = call->method()->return_type();\n+    if (!t->is_loaded() && InlineTypeReturnedAsFields) {\n+      \/\/ The return type is unloaded but the callee might later be C2 compiled and then return\n+      \/\/ in scalarized form when the return type is loaded. Handle this similar to what we do in\n+      \/\/ PhaseMacroExpand::expand_mh_intrinsic_return by calling into the runtime to buffer.\n+      \/\/ It's a bit unfortunate because we will deopt anyway but the interpreter needs an oop.\n+      IdealKit ideal(this);\n+      IdealVariable res(ideal);\n+      ideal.declarations_done();\n+      \/\/ Change return type of call to scalarized return\n+      const TypeFunc* tf = call->_tf;\n+      const TypeTuple* domain = OptoRuntime::store_inline_type_fields_Type()->domain_cc();\n+      const TypeFunc* new_tf = TypeFunc::make(tf->domain_sig(), tf->domain_cc(), tf->range_sig(), domain);\n+      call->_tf = new_tf;\n+      _gvn.set_type(call, call->Value(&_gvn));\n+      _gvn.set_type(ret, ret->Value(&_gvn));\n+      \/\/ Don't add store to buffer call if we are strength reducing\n+      if (!C->strength_reduction()) {\n+        ideal.if_then(ret, BoolTest::eq, ideal.makecon(TypePtr::NULL_PTR)); {\n+          \/\/ Return value is null\n+          ideal.set(res, makecon(TypePtr::NULL_PTR));\n+        } ideal.else_(); {\n+          \/\/ Return value is non-null\n+          sync_kit(ideal);\n+\n+          Node* store_to_buf_call = make_runtime_call(RC_NO_LEAF | RC_NO_IO,\n+                                                      OptoRuntime::store_inline_type_fields_Type(),\n+                                                      StubRoutines::store_inline_type_fields_to_buf(),\n+                                                      nullptr, TypePtr::BOTTOM, ret);\n+\n+          \/\/ We don't know how many values are returned. This assumes the\n+          \/\/ worst case, that all available registers are used.\n+          for (uint i = TypeFunc::Parms+1; i < domain->cnt(); i++) {\n+            if (domain->field_at(i) == Type::HALF) {\n+              store_to_buf_call->init_req(i, top());\n+              continue;\n+            }\n+            Node* proj =_gvn.transform(new ProjNode(call, i));\n+            store_to_buf_call->init_req(i, proj);\n+          }\n+          make_slow_call_ex(store_to_buf_call, env()->Throwable_klass(), false);\n+\n+          Node* buf = _gvn.transform(new ProjNode(store_to_buf_call, TypeFunc::Parms));\n+          const Type* buf_type = TypeOopPtr::make_from_klass(t->as_klass())->join_speculative(TypePtr::NOTNULL);\n+          buf = _gvn.transform(new CheckCastPPNode(control(), buf, buf_type));\n+\n+          ideal.set(res, buf);\n+          ideal.sync_kit(this);\n+        } ideal.end_if();\n+      } else {\n+        for (uint i = TypeFunc::Parms+1; i < domain->cnt(); i++) {\n+          Node* proj =_gvn.transform(new ProjNode(call, i));\n+        }\n+        ideal.set(res, ret);\n+      }\n+      sync_kit(ideal);\n+      ret = _gvn.transform(ideal.value(res));\n+    }\n+    if (t->is_klass()) {\n+      const Type* type = TypeOopPtr::make_from_klass(t->as_klass());\n+      if (type->is_inlinetypeptr()) {\n+        ret = InlineTypeNode::make_from_oop(this, ret, type->inline_klass());\n+      }\n+    }\n+  }\n+\n@@ -1969,2 +2201,1 @@\n-  CallProjections callprojs;\n-  call->extract_projections(&callprojs, true, do_asserts);\n+  CallProjections* callprojs = call->extract_projections(true, do_asserts);\n@@ -1979,2 +2210,2 @@\n-  if (callprojs.fallthrough_catchproj != nullptr) {\n-    C->gvn_replace_by(callprojs.fallthrough_catchproj, final_ctl);\n+  if (callprojs->fallthrough_catchproj != nullptr) {\n+    C->gvn_replace_by(callprojs->fallthrough_catchproj, final_ctl);\n@@ -1982,1 +2213,1 @@\n-  if (callprojs.fallthrough_memproj != nullptr) {\n+  if (callprojs->fallthrough_memproj != nullptr) {\n@@ -1987,1 +2218,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_memproj,   final_mem);\n+    C->gvn_replace_by(callprojs->fallthrough_memproj,   final_mem);\n@@ -1990,2 +2221,2 @@\n-  if (callprojs.fallthrough_ioproj != nullptr) {\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj,    final_io);\n+  if (callprojs->fallthrough_ioproj != nullptr) {\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj,    final_io);\n@@ -1995,2 +2226,16 @@\n-  if (callprojs.resproj != nullptr && result != nullptr) {\n-    C->gvn_replace_by(callprojs.resproj, result);\n+  if (callprojs->resproj[0] != nullptr && result != nullptr) {\n+    \/\/ If the inlined code is dead, the result projections for an inline type returned as\n+    \/\/ fields have not been replaced. They will go away once the call is replaced by TOP below.\n+    assert(callprojs->nb_resproj == 1 || (call->tf()->returns_inline_type_as_fields() && stopped()) ||\n+           (C->strength_reduction() && InlineTypeReturnedAsFields && !call->as_CallJava()->method()->return_type()->is_loaded()),\n+           \"unexpected number of results\");\n+    \/\/ If we are doing strength reduction and the return type is not loaded we\n+    \/\/ need to rewire all projections since store_inline_type_fields_to_buf is already present\n+    if (C->strength_reduction() && InlineTypeReturnedAsFields && !call->as_CallJava()->method()->return_type()->is_loaded()) {\n+      const TypeTuple* domain = OptoRuntime::store_inline_type_fields_Type()->domain_cc();\n+      for (uint i = TypeFunc::Parms; i < domain->cnt(); i++) {\n+        C->gvn_replace_by(callprojs->resproj[0], final_state->in(i));\n+      }\n+    } else {\n+      C->gvn_replace_by(callprojs->resproj[0], result);\n+    }\n@@ -2001,2 +2246,2 @@\n-    if (callprojs.catchall_catchproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, C->top());\n+    if (callprojs->catchall_catchproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, C->top());\n@@ -2004,2 +2249,2 @@\n-    if (callprojs.catchall_memproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_memproj,   C->top());\n+    if (callprojs->catchall_memproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_memproj,   C->top());\n@@ -2007,2 +2252,2 @@\n-    if (callprojs.catchall_ioproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    C->top());\n+    if (callprojs->catchall_ioproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    C->top());\n@@ -2011,2 +2256,2 @@\n-    if (callprojs.exobj != nullptr) {\n-      C->gvn_replace_by(callprojs.exobj, C->top());\n+    if (callprojs->exobj != nullptr) {\n+      C->gvn_replace_by(callprojs->exobj, C->top());\n@@ -2023,2 +2268,2 @@\n-    if (callprojs.catchall_catchproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, ekit.control());\n+    if (callprojs->catchall_catchproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, ekit.control());\n@@ -2027,1 +2272,1 @@\n-    if (callprojs.catchall_memproj != nullptr) {\n+    if (callprojs->catchall_memproj != nullptr) {\n@@ -2029,1 +2274,1 @@\n-      C->gvn_replace_by(callprojs.catchall_memproj,   ex_mem);\n+      C->gvn_replace_by(callprojs->catchall_memproj,   ex_mem);\n@@ -2032,2 +2277,2 @@\n-    if (callprojs.catchall_ioproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    ekit.i_o());\n+    if (callprojs->catchall_ioproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    ekit.i_o());\n@@ -2037,2 +2282,2 @@\n-    if (callprojs.exobj != nullptr) {\n-      C->gvn_replace_by(callprojs.exobj, ex_oop);\n+    if (callprojs->exobj != nullptr) {\n+      C->gvn_replace_by(callprojs->exobj, ex_oop);\n@@ -2052,1 +2297,1 @@\n-  if (callprojs.fallthrough_catchproj != nullptr && !final_ctl->is_top() && do_replaced_nodes) {\n+  if (callprojs->fallthrough_catchproj != nullptr && !final_ctl->is_top() && do_replaced_nodes) {\n@@ -2252,1 +2497,1 @@\n-    const TypePtr* ptr = (ptr_kind == ProfileMaybeNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n+    const TypePtr* ptr = (ptr_kind != ProfileNeverNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n@@ -2275,1 +2520,1 @@\n-    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::OffsetBot, TypeOopPtr::InstanceBot, speculative);\n+    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::Offset::bottom, TypeOopPtr::InstanceBot, speculative);\n@@ -2309,2 +2554,9 @@\n-      if (!data->as_BitData()->null_seen()) {\n-        ptr_kind = ProfileNeverNull;\n+      if (java_bc() == Bytecodes::_aastore) {\n+        ciKlass* array_type = nullptr;\n+        ciKlass* element_type = nullptr;\n+        ProfilePtrKind element_ptr = ProfileMaybeNull;\n+        bool flat_array = true;\n+        bool null_free_array = true;\n+        method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+        exact_kls = element_type;\n+        ptr_kind = element_ptr;\n@@ -2312,8 +2564,12 @@\n-        if (TypeProfileCasts) {\n-          assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n-          ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n-          uint i = 0;\n-          for (; i < call->row_limit(); i++) {\n-            ciKlass* receiver = call->receiver(i);\n-            if (receiver != nullptr) {\n-              break;\n+        if (!data->as_BitData()->null_seen()) {\n+          ptr_kind = ProfileNeverNull;\n+        } else {\n+          if (TypeProfileCasts) {\n+            assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n+            ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n+            uint i = 0;\n+            for (; i < call->row_limit(); i++) {\n+              ciKlass* receiver = call->receiver(i);\n+              if (receiver != nullptr) {\n+                break;\n+              }\n@@ -2321,0 +2577,1 @@\n+            ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2322,1 +2579,0 @@\n-          ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2342,1 +2598,1 @@\n-  int             nargs = tf->domain()->cnt() - TypeFunc::Parms;\n+  int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;\n@@ -2345,1 +2601,1 @@\n-    const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);\n+    const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);\n@@ -2505,1 +2761,1 @@\n-    uint num_bits = call_type->range()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte;\n+    uint num_bits = call_type->range_sig()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte;\n@@ -2540,1 +2796,1 @@\n-  assert(call->in(call->req()-1) != nullptr, \"must initialize all parms\");\n+  assert(call->in(call->req()-1) != nullptr || (call->req()-1) > (TypeFunc::Parms+7), \"must initialize all parms\");\n@@ -2588,0 +2844,1 @@\n+\n@@ -2684,0 +2941,8 @@\n+  const TypeKlassPtr* klass_ptr_type = gvn.type(superklass)->is_klassptr();\n+  \/\/ For a direct pointer comparison, we need the refined array klass pointer\n+  Node* vm_superklass = superklass;\n+  if (klass_ptr_type->isa_aryklassptr() && klass_ptr_type->klass_is_exact()) {\n+    assert(!klass_ptr_type->is_aryklassptr()->is_refined_type(), \"Unexpected refined array klass pointer\");\n+    vm_superklass = gvn.makecon(klass_ptr_type->is_aryklassptr()->cast_to_refined_array_klass_ptr());\n+  }\n+\n@@ -2721,1 +2986,1 @@\n-        IfNode* iff = gen_subtype_check_compare(*ctrl, subklass, superklass, BoolTest::eq, PROB_STATIC_FREQUENT, gvn, T_ADDRESS);\n+        IfNode* iff = gen_subtype_check_compare(*ctrl, subklass, vm_superklass, BoolTest::eq, PROB_STATIC_FREQUENT, gvn, T_ADDRESS);\n@@ -2799,0 +3064,4 @@\n+        if (klass_t->isa_aryklassptr()) {\n+          \/\/ For a direct pointer comparison, we need the refined array klass pointer\n+          klass_t = klass_t->is_aryklassptr()->cast_to_refined_array_klass_ptr();\n+        }\n@@ -2852,1 +3121,1 @@\n-  IfNode *iff3 = gen_subtype_check_compare(*ctrl, subklass, superklass, BoolTest::eq, PROB_LIKELY(0.36f), gvn, T_ADDRESS);\n+  IfNode *iff3 = gen_subtype_check_compare(*ctrl, subklass, vm_superklass, BoolTest::eq, PROB_LIKELY(0.36f), gvn, T_ADDRESS);\n@@ -2890,0 +3159,5 @@\n+  const Type* sub_t = _gvn.type(obj_or_subklass);\n+  if (sub_t->make_oopptr() != nullptr && sub_t->make_oopptr()->is_inlinetypeptr()) {\n+    sub_t = TypeKlassPtr::make(sub_t->inline_klass());\n+    obj_or_subklass = makecon(sub_t);\n+  }\n@@ -2895,1 +3169,1 @@\n-    if (!_gvn.type(obj_or_subklass)->isa_klassptr()) {\n+    if (!sub_t->isa_klassptr()) {\n@@ -2913,2 +3187,1 @@\n-                                    float prob,\n-                                    Node* *casted_receiver) {\n+                                    float prob, Node* *casted_receiver) {\n@@ -2916,1 +3189,12 @@\n-\n+  Node* fail = top();\n+  const Type* rec_t = _gvn.type(receiver);\n+  if (rec_t->is_inlinetypeptr()) {\n+    if (klass->equals(rec_t->inline_klass())) {\n+      (*casted_receiver) = receiver; \/\/ Always passes\n+    } else {\n+      (*casted_receiver) = top();    \/\/ Always fails\n+      fail = control();\n+      set_control(top());\n+    }\n+    return fail;\n+  }\n@@ -2918,0 +3202,4 @@\n+  if (tklass->isa_aryklassptr()) {\n+    \/\/ For a direct pointer comparison, we need the refined array klass pointer\n+    tklass = tklass->is_aryklassptr()->cast_to_refined_array_klass_ptr();\n+  }\n@@ -2919,6 +3207,1 @@\n-  Node* want_klass = makecon(tklass);\n-  Node* cmp = _gvn.transform(new CmpPNode(recv_klass, want_klass));\n-  Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n-  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n-  set_control( _gvn.transform(new IfTrueNode (iff)));\n-  Node* fail = _gvn.transform(new IfFalseNode(iff));\n+  fail = type_check(recv_klass, tklass, prob);\n@@ -2928,2 +3211,2 @@\n-    const TypeOopPtr* recvx_type = tklass->as_instance_type();\n-    assert(recvx_type->klass_is_exact(), \"\");\n+    const TypeOopPtr* recv_xtype = tklass->as_instance_type();\n+    assert(recv_xtype->klass_is_exact(), \"\");\n@@ -2931,1 +3214,1 @@\n-    if (!receiver_type->higher_equal(recvx_type)) { \/\/ ignore redundant casts\n+    if (!receiver_type->higher_equal(recv_xtype)) { \/\/ ignore redundant casts\n@@ -2934,2 +3217,7 @@\n-      Node* cast = new CheckCastPPNode(control(), receiver, recvx_type);\n-      (*casted_receiver) = _gvn.transform(cast);\n+      Node* cast = new CheckCastPPNode(control(), receiver, recv_xtype);\n+      Node* res = _gvn.transform(cast);\n+      if (recv_xtype->is_inlinetypeptr()) {\n+        assert(!gvn().type(res)->maybe_null(), \"receiver should never be null\");\n+        res = InlineTypeNode::make_from_oop(this, res, recv_xtype->inline_klass());\n+      }\n+      (*casted_receiver) = res;\n@@ -2944,0 +3232,11 @@\n+Node* GraphKit::type_check(Node* recv_klass, const TypeKlassPtr* tklass,\n+                           float prob) {\n+  Node* want_klass = makecon(tklass);\n+  Node* cmp = _gvn.transform(new CmpPNode(recv_klass, want_klass));\n+  Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n+  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n+  set_control(_gvn.transform(new IfTrueNode (iff)));\n+  Node* fail = _gvn.transform(new IfFalseNode(iff));\n+  return fail;\n+}\n+\n@@ -2956,3 +3255,6 @@\n-    if (!receiver_type->higher_equal(recv_type)) { \/\/ ignore redundant casts\n-      Node* cast = new CheckCastPPNode(control(), receiver, recv_type);\n-      (*casted_receiver) = _gvn.transform(cast);\n+    if (receiver_type != nullptr && !receiver_type->higher_equal(recv_type)) { \/\/ ignore redundant casts\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), receiver, recv_type));\n+      if (recv_type->is_inlinetypeptr()) {\n+        cast = InlineTypeNode::make_from_oop(this, cast, recv_type->inline_klass());\n+      }\n+      (*casted_receiver) = cast;\n@@ -3067,1 +3369,14 @@\n-  ciKlass* exact_kls = spec_klass == nullptr ? profile_has_unique_klass() : spec_klass;\n+  ciKlass* exact_kls = spec_klass;\n+  if (exact_kls == nullptr) {\n+    if (java_bc() == Bytecodes::_aastore) {\n+      ciKlass* array_type = nullptr;\n+      ciKlass* element_type = nullptr;\n+      ProfilePtrKind element_ptr = ProfileMaybeNull;\n+      bool flat_array = true;\n+      bool null_free_array = true;\n+      method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+      exact_kls = element_type;\n+    } else {\n+      exact_kls = profile_has_unique_klass();\n+    }\n+  }\n@@ -3197,1 +3512,1 @@\n-    if (subk->is_loaded()) {\n+    if (subk != nullptr && subk->is_loaded()) {\n@@ -3253,2 +3568,1 @@\n-Node* GraphKit::gen_checkcast(Node *obj, Node* superklass,\n-                              Node* *failure_control) {\n+Node* GraphKit::gen_checkcast(Node* obj, Node* superklass, Node* *failure_control, bool null_free, bool maybe_larval) {\n@@ -3257,0 +3571,16 @@\n+  const Type* obj_type = _gvn.type(obj);\n+  if (obj_type->is_inlinetypeptr() && !obj_type->maybe_null() && klass_ptr_type->klass_is_exact() && obj_type->inline_klass() == klass_ptr_type->exact_klass(true)) {\n+    \/\/ Special case: larval inline objects must not be scalarized. They are also generally not\n+    \/\/ allowed to participate in most operations except as the first operand of putfield, or as an\n+    \/\/ argument to a constructor invocation with it being a receiver, Unsafe::putXXX with it being\n+    \/\/ the first argument, or Unsafe::finishPrivateBuffer. This allows us to aggressively scalarize\n+    \/\/ value objects in all other places. This special case comes from the limitation of the Java\n+    \/\/ language, Unsafe::makePrivateBuffer returns an Object that is checkcast-ed to the concrete\n+    \/\/ value type. We must do this first because C->static_subtype_check may do nothing when\n+    \/\/ StressReflectiveCode is set.\n+    return obj;\n+  }\n+\n+  \/\/ Else it must be a non-larval object\n+  obj = cast_to_non_larval(obj);\n+\n@@ -3259,0 +3589,2 @@\n+  bool safe_for_replace = (failure_control == nullptr);\n+  assert(!null_free || toop->can_be_inline_type(), \"must be an inline type pointer\");\n@@ -3267,3 +3599,10 @@\n-    const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();\n-    if (objtp != nullptr) {\n-      switch (C->static_subtype_check(improved_klass_ptr_type, objtp->as_klass_type())) {\n+    const TypeKlassPtr* kptr = nullptr;\n+    if (obj_type->isa_oop_ptr()) {\n+      kptr = obj_type->is_oopptr()->as_klass_type();\n+    } else if (obj->is_InlineType()) {\n+      ciInlineKlass* vk = obj_type->inline_klass();\n+      kptr = TypeInstKlassPtr::make(TypePtr::NotNull, vk, Type::Offset(0));\n+    }\n+\n+    if (kptr != nullptr) {\n+      switch (C->static_subtype_check(improved_klass_ptr_type, kptr)) {\n@@ -3274,1 +3613,7 @@\n-        return record_profiled_receiver_for_speculation(obj);\n+        obj = record_profiled_receiver_for_speculation(obj);\n+        if (null_free) {\n+          assert(safe_for_replace, \"must be\");\n+          obj = null_check(obj);\n+        }\n+        assert(stopped() || !toop->is_inlinetypeptr() || obj->is_InlineType(), \"should have been scalarized\");\n+        return obj;\n@@ -3276,0 +3621,4 @@\n+        if (null_free) {\n+          assert(safe_for_replace, \"must be\");\n+          obj = null_check(obj);\n+        }\n@@ -3277,2 +3626,1 @@\n-        \/\/ A non-null value will always produce an exception.\n-        if (!objtp->maybe_null()) {\n+        if (obj_type->isa_oopptr() != nullptr && !obj_type->is_oopptr()->maybe_null()) {\n@@ -3295,1 +3643,0 @@\n-  bool safe_for_replace = false;\n@@ -3300,2 +3647,3 @@\n-    data = method()->method_data()->bci_to_data(bci());\n-    safe_for_replace = true;\n+    if (method()->method_data()->is_mature()) {\n+      data = method()->method_data()->bci_to_data(bci());\n+    }\n@@ -3308,0 +3656,3 @@\n+  _gvn.set_type(region, Type::CONTROL);\n+  _gvn.set_type(phi, toop);\n+\n@@ -3315,0 +3666,7 @@\n+  if (obj->is_InlineType()) {\n+    \/\/ Re-execute if buffering during triggers deoptimization\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    obj = obj->as_InlineType()->buffer(this, safe_for_replace);\n+  }\n+\n@@ -3317,1 +3675,7 @@\n-  Node* not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  Node* not_null_obj = nullptr;\n+  if (null_free) {\n+    assert(safe_for_replace, \"must be\");\n+    not_null_obj = null_check(obj);\n+  } else {\n+    not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  }\n@@ -3322,0 +3686,3 @@\n+    if (toop->is_inlinetypeptr()) {\n+      return InlineTypeNode::make_null(_gvn, toop->inline_klass());\n+    }\n@@ -3359,0 +3726,3 @@\n+      \/\/ Only improve the super class for constants which allows subsequent sub type checks to possibly be commoned up.\n+      \/\/ The other non-constant cases cannot be improved with a cast node here since they could be folded to top.\n+      \/\/ Additionally, the benefit would only be minor in non-constant cases.\n@@ -3362,1 +3732,0 @@\n-\n@@ -3400,1 +3769,165 @@\n-  return record_profiled_receiver_for_speculation(res);\n+  bool not_inline = !toop->can_be_inline_type();\n+  bool not_flat_in_array = !UseArrayFlattening || not_inline || (toop->is_inlinetypeptr() && !toop->inline_klass()->maybe_flat_in_array());\n+  if (Arguments::is_valhalla_enabled() && (not_inline || not_flat_in_array)) {\n+    \/\/ Check if obj has been loaded from an array\n+    obj = obj->isa_DecodeN() ? obj->in(1) : obj;\n+    Node* array = nullptr;\n+    if (obj->isa_Load()) {\n+      Node* address = obj->in(MemNode::Address);\n+      if (address->isa_AddP()) {\n+        array = address->as_AddP()->in(AddPNode::Base);\n+      }\n+    } else if (obj->is_Phi()) {\n+      Node* region = obj->in(0);\n+      \/\/ TODO make this more robust (see JDK-8231346)\n+      if (region->req() == 3 && region->in(2) != nullptr && region->in(2)->in(0) != nullptr) {\n+        IfNode* iff = region->in(2)->in(0)->isa_If();\n+        if (iff != nullptr) {\n+          iff->is_flat_array_check(&_gvn, &array);\n+        }\n+      }\n+    }\n+    if (array != nullptr) {\n+      const TypeAryPtr* ary_t = _gvn.type(array)->isa_aryptr();\n+      if (ary_t != nullptr) {\n+        if (!ary_t->is_not_null_free() && !ary_t->is_null_free() && not_inline) {\n+          \/\/ Casting array element to a non-inline-type, mark array as not null-free.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_null_free()));\n+          replace_in_map(array, cast);\n+          array = cast;\n+        }\n+        if (!ary_t->is_not_flat() && !ary_t->is_flat() && not_flat_in_array) {\n+          \/\/ Casting array element to a non-flat-in-array type, mark array as not flat.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_flat()));\n+          replace_in_map(array, cast);\n+          array = cast;\n+        }\n+      }\n+    }\n+  }\n+\n+  if (!stopped() && !res->is_InlineType()) {\n+    res = record_profiled_receiver_for_speculation(res);\n+    if (toop->is_inlinetypeptr() && !maybe_larval) {\n+      Node* vt = InlineTypeNode::make_from_oop(this, res, toop->inline_klass());\n+      res = vt;\n+      if (safe_for_replace) {\n+        replace_in_map(obj, vt);\n+        replace_in_map(not_null_obj, vt);\n+        replace_in_map(res, vt);\n+      }\n+    }\n+  }\n+  return res;\n+}\n+\n+Node* GraphKit::mark_word_test(Node* obj, uintptr_t mask_val, bool eq, bool check_lock) {\n+  \/\/ Load markword\n+  Node* mark_adr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+  Node* mark = make_load(nullptr, mark_adr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  if (check_lock && !UseCompactObjectHeaders) {\n+    \/\/ COH: Locking does not override the markword with a tagged pointer. We can directly read from the markword.\n+    \/\/ Check if obj is locked\n+    Node* locked_bit = MakeConX(markWord::unlocked_value);\n+    locked_bit = _gvn.transform(new AndXNode(locked_bit, mark));\n+    Node* cmp = _gvn.transform(new CmpXNode(locked_bit, MakeConX(0)));\n+    Node* is_unlocked = _gvn.transform(new BoolNode(cmp, BoolTest::ne));\n+    IfNode* iff = new IfNode(control(), is_unlocked, PROB_MAX, COUNT_UNKNOWN);\n+    _gvn.transform(iff);\n+    Node* locked_region = new RegionNode(3);\n+    Node* mark_phi = new PhiNode(locked_region, TypeX_X);\n+\n+    \/\/ Unlocked: Use bits from mark word\n+    locked_region->init_req(1, _gvn.transform(new IfTrueNode(iff)));\n+    mark_phi->init_req(1, mark);\n+\n+    \/\/ Locked: Load prototype header from klass\n+    set_control(_gvn.transform(new IfFalseNode(iff)));\n+    \/\/ Make loads control dependent to make sure they are only executed if array is locked\n+    Node* klass_adr = basic_plus_adr(obj, oopDesc::klass_offset_in_bytes());\n+    Node* klass = _gvn.transform(LoadKlassNode::make(_gvn, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+    Node* proto_adr = basic_plus_adr(klass, in_bytes(Klass::prototype_header_offset()));\n+    Node* proto = _gvn.transform(LoadNode::make(_gvn, control(), C->immutable_memory(), proto_adr, proto_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+\n+    locked_region->init_req(2, control());\n+    mark_phi->init_req(2, proto);\n+    set_control(_gvn.transform(locked_region));\n+    record_for_igvn(locked_region);\n+\n+    mark = mark_phi;\n+  }\n+\n+  \/\/ Now check if mark word bits are set\n+  Node* mask = MakeConX(mask_val);\n+  Node* masked = _gvn.transform(new AndXNode(_gvn.transform(mark), mask));\n+  record_for_igvn(masked); \/\/ Give it a chance to be optimized out by IGVN\n+  Node* cmp = _gvn.transform(new CmpXNode(masked, mask));\n+  return _gvn.transform(new BoolNode(cmp, eq ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::inline_type_test(Node* obj, bool is_inline) {\n+  return mark_word_test(obj, markWord::inline_type_pattern, is_inline, \/* check_lock = *\/ false);\n+}\n+\n+Node* GraphKit::flat_array_test(Node* array_or_klass, bool flat) {\n+  \/\/ We can't use immutable memory here because the mark word is mutable.\n+  \/\/ PhaseIdealLoop::move_flat_array_check_out_of_loop will make sure the\n+  \/\/ check is moved out of loops (mainly to enable loop unswitching).\n+  Node* cmp = _gvn.transform(new FlatArrayCheckNode(C, memory(Compile::AliasIdxRaw), array_or_klass));\n+  record_for_igvn(cmp); \/\/ Give it a chance to be optimized out by IGVN\n+  return _gvn.transform(new BoolNode(cmp, flat ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::null_free_array_test(Node* array, bool null_free) {\n+  return mark_word_test(array, markWord::null_free_array_bit_in_place, null_free);\n+}\n+\n+Node* GraphKit::null_free_atomic_array_test(Node* array, ciInlineKlass* vk) {\n+  assert(vk->has_atomic_layout() || vk->has_non_atomic_layout(), \"Can't be null-free and flat\");\n+\n+  \/\/ TODO 8350865 Add a stress flag to always access atomic if layout exists?\n+  if (!vk->has_non_atomic_layout()) {\n+    return intcon(1); \/\/ Always atomic\n+  } else if (!vk->has_atomic_layout()) {\n+    return intcon(0); \/\/ Never atomic\n+  }\n+\n+  Node* array_klass = load_object_klass(array);\n+  int layout_kind_offset = in_bytes(FlatArrayKlass::layout_kind_offset());\n+  Node* layout_kind_addr = basic_plus_adr(array_klass, array_klass, layout_kind_offset);\n+  Node* layout_kind = make_load(nullptr, layout_kind_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+  Node* cmp = _gvn.transform(new CmpINode(layout_kind, intcon((int)LayoutKind::NULL_FREE_ATOMIC_FLAT)));\n+  return _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n+}\n+\n+\/\/ Deoptimize if 'ary' is a null-free inline type array and 'val' is null\n+Node* GraphKit::inline_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace) {\n+  RegionNode* region = new RegionNode(3);\n+  Node* null_ctl = top();\n+  null_check_oop(val, &null_ctl);\n+  if (null_ctl != top()) {\n+    PreserveJVMState pjvms(this);\n+    set_control(null_ctl);\n+    {\n+      \/\/ Deoptimize if null-free array\n+      BuildCutout unless(this, null_free_array_test(ary, \/* null_free = *\/ false), PROB_MAX);\n+      inc_sp(nargs);\n+      uncommon_trap(Deoptimization::Reason_null_check,\n+                    Deoptimization::Action_none);\n+    }\n+    region->init_req(1, control());\n+  }\n+  region->init_req(2, control());\n+  set_control(_gvn.transform(region));\n+  record_for_igvn(region);\n+  if (_gvn.type(val) == TypePtr::NULL_PTR) {\n+    \/\/ Since we were just successfully storing null, the array can't be null free.\n+    const TypeAryPtr* ary_t = _gvn.type(ary)->is_aryptr();\n+    ary_t = ary_t->cast_to_not_null_free();\n+    Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));\n+    if (safe_for_replace) {\n+      replace_in_map(ary, cast);\n+    }\n+    ary = cast;\n+  }\n+  return ary;\n@@ -3534,0 +4067,1 @@\n+  assert(!obj->is_InlineType(), \"should not unlock on inline type\");\n@@ -3574,1 +4108,8 @@\n-    if (xklass || (klass_t->isa_aryklassptr() && klass_t->is_aryklassptr()->elem() != Type::BOTTOM)) {\n+    bool can_be_flat = false;\n+    const TypeAryPtr* ary_type = klass_t->as_instance_type()->isa_aryptr();\n+    if (UseArrayFlattening && !xklass && ary_type != nullptr && !ary_type->is_null_free()) {\n+      \/\/ Don't constant fold if the runtime type might be a flat array but the static type is not.\n+      const TypeOopPtr* elem = ary_type->elem()->make_oopptr();\n+      can_be_flat = ary_type->can_be_inline_array() && (!elem->is_inlinetypeptr() || elem->inline_klass()->maybe_flat_in_array());\n+    }\n+    if (!can_be_flat && (xklass || (klass_t->isa_aryklassptr() && klass_t->is_aryklassptr()->elem() != Type::BOTTOM))) {\n@@ -3576,2 +4117,4 @@\n-      if (klass_t->isa_aryklassptr()) {\n-        BasicType elem = klass_t->as_instance_type()->isa_aryptr()->elem()->array_element_basic_type();\n+      if (klass_t->is_flat()) {\n+        lhelper = ary_type->flat_layout_helper();\n+      } else if (klass_t->isa_aryklassptr()) {\n+        BasicType elem = ary_type->elem()->array_element_basic_type();\n@@ -3606,1 +4149,3 @@\n-  kit.set_memory(init_out_raw, alias_idx);\n+  if (init_out_raw != nullptr) {\n+    kit.set_memory(init_out_raw, alias_idx);\n+  }\n@@ -3645,0 +4190,1 @@\n+    _gvn.set_type(minit_in, Type::MEMORY);\n@@ -3655,0 +4201,5 @@\n+      \/\/ Initially all flat array accesses share a single slice\n+      \/\/ but that changes after parsing. Prepare the memory graph so\n+      \/\/ it can optimize flat array accesses properly once they\n+      \/\/ don't share a single slice.\n+      assert(C->flat_accesses_share_alias(), \"should be set at parse time\");\n@@ -3657,1 +4208,5 @@\n-      hook_memory_on_init(*this, elemidx, minit_in, _gvn.transform(new NarrowMemProjNode(init, C->get_adr_type(elemidx))));\n+      const TypePtr* alias_adr_type = C->get_adr_type(elemidx);\n+      if (alias_adr_type->is_flat()) {\n+        C->set_flat_accesses();\n+      }\n+      hook_memory_on_init(*this, elemidx, minit_in, _gvn.transform(new NarrowMemProjNode(init, alias_adr_type)));\n@@ -3709,1 +4264,2 @@\n-                             bool deoptimize_on_exception) {\n+                             bool deoptimize_on_exception,\n+                             InlineTypeNode* inline_type_node) {\n@@ -3716,1 +4272,1 @@\n-  int   layout_is_con = (layout_val == nullptr);\n+  bool  layout_is_con = (layout_val == nullptr);\n@@ -3767,1 +4323,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3774,1 +4330,1 @@\n-                                         initial_slow_test);\n+                                         initial_slow_test, inline_type_node);\n@@ -3780,1 +4336,1 @@\n-\/\/ helper for both newarray and anewarray\n+\/\/ helper for newarray and anewarray\n@@ -3790,1 +4346,2 @@\n-                          bool deoptimize_on_exception) {\n+                          bool deoptimize_on_exception,\n+                          Node* init_val) {\n@@ -3793,1 +4350,1 @@\n-  int   layout_is_con = (layout_val == nullptr);\n+  bool  layout_is_con = (layout_val == nullptr);\n@@ -3823,3 +4380,1 @@\n-    assert(fast_size_limit == 0 || count_leading_zeros(fast_size_limit) > static_cast<unsigned>(LogBytesPerLong - log2_esize),\n-           \"fast_size_limit (%d) overflow when shifted left by %d\", fast_size_limit, LogBytesPerLong - log2_esize);\n-    fast_size_limit <<= (LogBytesPerLong - log2_esize);\n+    fast_size_limit <<= MAX2(LogBytesPerLong - log2_esize, 0);\n@@ -3842,0 +4397,1 @@\n+    bool is_flat_array = Klass::layout_helper_is_flatArray(layout_con);\n@@ -3844,1 +4400,1 @@\n-    assert((hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n+    assert(is_flat_array || (hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n@@ -3933,1 +4489,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3942,1 +4498,21 @@\n-  const TypeOopPtr* ary_type = _gvn.type(klass_node)->is_klassptr()->as_instance_type();\n+  const TypeKlassPtr* ary_klass = _gvn.type(klass_node)->isa_klassptr();\n+  const TypeOopPtr* ary_type = ary_klass->as_instance_type();\n+\n+  Node* raw_init_value = nullptr;\n+  if (init_val != nullptr) {\n+    \/\/ TODO 8350865 Fast non-zero init not implemented yet for flat, null-free arrays\n+    if (ary_type->is_flat()) {\n+      initial_slow_test = intcon(1);\n+    }\n+\n+    if (UseCompressedOops) {\n+      \/\/ With compressed oops, the 64-bit init value is built from two 32-bit compressed oops\n+      init_val = _gvn.transform(new EncodePNode(init_val, init_val->bottom_type()->make_narrowoop()));\n+      Node* lower = _gvn.transform(new CastP2XNode(control(), init_val));\n+      Node* upper = _gvn.transform(new LShiftLNode(lower, intcon(32)));\n+      raw_init_value = _gvn.transform(new OrLNode(lower, upper));\n+    } else {\n+      raw_init_value = _gvn.transform(new CastP2XNode(control(), init_val));\n+    }\n+  }\n+\n@@ -3957,2 +4533,2 @@\n-                            length, valid_length_test);\n-\n+                            length, valid_length_test,\n+                            init_val, raw_init_value);\n@@ -4093,0 +4669,1 @@\n+  reset_memory();\n@@ -4113,1 +4690,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4116,2 +4693,2 @@\n-                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS),\n-                                                  ciTypeArrayKlass::make(T_BYTE), true, 0);\n+                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS, false, false, true, true, true),\n+                                                  ciTypeArrayKlass::make(T_BYTE), true, Type::Offset(0));\n@@ -4130,1 +4707,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4142,1 +4719,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4152,1 +4729,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4265,1 +4842,7 @@\n-    return makecon(con_type);\n+    Node* con = makecon(con_type);\n+    if (field->type()->is_inlinetype()) {\n+      con = InlineTypeNode::make_from_oop(this, con, field->type()->as_inline_klass());\n+    } else if (con_type->is_inlinetypeptr()) {\n+      con = InlineTypeNode::make_from_oop(this, con, con_type->inline_klass());\n+    }\n+    return con;\n@@ -4271,1 +4854,1 @@\n-  const TypeOopPtr* obj_type = obj->bottom_type()->isa_oopptr();\n+  const Type* obj_type = obj->bottom_type();\n@@ -4273,1 +4856,1 @@\n-  if (obj_type != nullptr && sig_type->is_loaded() && !obj_type->higher_equal(sig_type)) {\n+  if (obj_type->isa_oopptr() && sig_type->is_loaded() && !obj_type->higher_equal(sig_type)) {\n@@ -4276,1 +4859,4 @@\n-    return casted_obj;\n+    obj = casted_obj;\n+  }\n+  if (sig_type->is_inlinetypeptr()) {\n+    obj = InlineTypeNode::make_from_oop(this, obj, sig_type->inline_klass());\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":728,"deletions":142,"binary":false,"changes":870,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -69,0 +70,3 @@\n+#ifdef ASSERT\n+  uint              _worklist_size;\n+#endif\n@@ -81,1 +85,1 @@\n-  GraphKit(JVMState* jvms);     \/\/ the JVM state on which to operate\n+  GraphKit(JVMState* jvms, PhaseGVN* gvn = nullptr);     \/\/ the JVM state on which to operate\n@@ -87,0 +91,7 @@\n+#if 0\n+    \/\/ During incremental inlining, the Node_Array of the C->for_igvn() worklist and the IGVN\n+    \/\/ worklist are shared but the _in_worklist VectorSet is not. To avoid inconsistencies,\n+    \/\/ we should not add nodes to the _for_igvn worklist when using IGVN for the GraphKit.\n+    assert((_gvn.is_IterGVN() == nullptr) || (_gvn.C->for_igvn()->size() == _worklist_size),\n+           \"GraphKit should not modify _for_igvn worklist after parsing\");\n+#endif\n@@ -97,1 +108,1 @@\n-  void record_for_igvn(Node* n) const { C->record_for_igvn(n); }  \/\/ delegate to Compile\n+  void record_for_igvn(Node* n) const { _gvn.record_for_igvn(n); }\n@@ -375,1 +386,2 @@\n-                          bool speculative = false);\n+                          bool speculative = false,\n+                          bool null_marker_check = false);\n@@ -380,1 +392,0 @@\n-    assert(argument(0)->bottom_type()->isa_ptr(), \"must be\");\n@@ -452,0 +463,7 @@\n+  \/\/ If a larval object appears multiple times in the JVMS and we encounter a loop, they will\n+  \/\/ become multiple Phis and we cannot change all of them to non-larval when we invoke the\n+  \/\/ constructor on one. The other case is that we don't know whether a parameter of an OSR\n+  \/\/ compilation is larval or not. If such a maybe-larval object is passed into an operation that\n+  \/\/ does not permit larval objects, we can be sure that it is not larval and scalarize it if it\n+  \/\/ is a value object.\n+  Node* cast_to_non_larval(Node* obj);\n@@ -582,1 +600,3 @@\n-                        DecoratorSet decorators);\n+                        DecoratorSet decorators,\n+                        bool safe_for_replace = true,\n+                        const InlineTypeNode* vt = nullptr);\n@@ -589,1 +609,2 @@\n-                       DecoratorSet decorators);\n+                       DecoratorSet decorators,\n+                       Node* ctl = nullptr);\n@@ -642,0 +663,2 @@\n+  Node* cast_to_flat_array(Node* array, ciInlineKlass* elem_vk);\n+  Node* cast_to_flat_array_exact(Node* array, ciInlineKlass* elem_vk, bool is_null_free, bool is_atomic);\n@@ -681,1 +704,1 @@\n-  void  set_arguments_for_java_call(CallJavaNode* call);\n+  void  set_arguments_for_java_call(CallJavaNode* call, bool is_late_inline = false);\n@@ -812,2 +835,9 @@\n-  Node* gen_checkcast( Node *subobj, Node* superkls,\n-                       Node* *failure_control = nullptr );\n+  Node* gen_checkcast(Node *subobj, Node* superkls, Node* *failure_control = nullptr, bool null_free = false, bool maybe_larval = false);\n+\n+  \/\/ Inline types\n+  Node* mark_word_test(Node* obj, uintptr_t mask_val, bool eq, bool check_lock = true);\n+  Node* inline_type_test(Node* obj, bool is_inline = true);\n+  Node* flat_array_test(Node* array_or_klass, bool flat = true);\n+  Node* null_free_array_test(Node* array, bool null_free = true);\n+  Node* null_free_atomic_array_test(Node* array, ciInlineKlass* vk);\n+  Node* inline_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace = false);\n@@ -822,0 +852,1 @@\n+  Node* type_check(Node* recv_klass, const TypeKlassPtr* tklass, float prob);\n@@ -835,1 +866,2 @@\n-                     bool deoptimize_on_exception = false);\n+                     bool deoptimize_on_exception = false,\n+                     InlineTypeNode* inline_type_node = nullptr);\n@@ -838,1 +870,2 @@\n-                  bool deoptimize_on_exception = false);\n+                  bool deoptimize_on_exception = false,\n+                  Node* init_val = nullptr);\n@@ -872,0 +905,1 @@\n+  Node* load_mirror_from_klass(Node* klass);\n","filename":"src\/hotspot\/share\/opto\/graphKit.hpp","additions":45,"deletions":11,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -166,0 +166,1 @@\n+  void if_then(Node* bol, float prob = PROB_FAIR, float cnt = COUNT_UNKNOWN, bool push_new_state = true);\n","filename":"src\/hotspot\/share\/opto\/idealKit.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -66,0 +67,6 @@\n+  \/\/ Inline types should not be split through Phis because they cannot be merged\n+  \/\/ through Phi nodes but each value input needs to be merged individually.\n+  if (n->is_InlineType()) {\n+    return nullptr;\n+  }\n+\n@@ -795,0 +802,4 @@\n+      if (inp->isa_InlineType()) {\n+        \/\/ TODO 8302217 This prevents PhiNode::push_inline_types_through\n+        return nullptr;\n+      }\n@@ -1120,0 +1131,48 @@\n+\/\/ We can't use immutable memory for the flat array check because we are loading the mark word which is\n+\/\/ mutable. Although the bits we are interested in are immutable (we check for markWord::unlocked_value),\n+\/\/ we need to use raw memory to not break anti dependency analysis. Below code will attempt to still move\n+\/\/ flat array checks out of loops, mainly to enable loop unswitching.\n+void PhaseIdealLoop::move_flat_array_check_out_of_loop(Node* n) {\n+  \/\/ Skip checks for more than one array\n+  if (n->req() > 3) {\n+    return;\n+  }\n+  Node* mem = n->in(FlatArrayCheckNode::Memory);\n+  Node* array = n->in(FlatArrayCheckNode::ArrayOrKlass)->uncast();\n+  IdealLoopTree* check_loop = get_loop(get_ctrl(n));\n+  IdealLoopTree* ary_loop = get_loop(get_ctrl(array));\n+\n+  \/\/ Check if array is loop invariant\n+  if (!check_loop->is_member(ary_loop)) {\n+    \/\/ Walk up memory graph from the check until we leave the loop\n+    VectorSet wq;\n+    wq.set(mem->_idx);\n+    while (check_loop->is_member(get_loop(ctrl_or_self(mem)))) {\n+      if (mem->is_Phi()) {\n+        mem = mem->in(1);\n+      } else if (mem->is_MergeMem()) {\n+        mem = mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw);\n+      } else if (mem->is_Proj()) {\n+        mem = mem->in(0);\n+      } else if (mem->is_MemBar() || mem->is_SafePoint()) {\n+        mem = mem->in(TypeFunc::Memory);\n+      } else if (mem->is_Store() || mem->is_LoadStore() || mem->is_ClearArray()) {\n+        mem = mem->in(MemNode::Memory);\n+      } else {\n+#ifdef ASSERT\n+        mem->dump();\n+#endif\n+        ShouldNotReachHere();\n+      }\n+      if (wq.test_set(mem->_idx)) {\n+        return;\n+      }\n+    }\n+    \/\/ Replace memory input and re-compute ctrl to move the check out of the loop\n+    _igvn.replace_input_of(n, 1, mem);\n+    set_ctrl_and_loop(n, get_early_ctrl(n));\n+    Node* bol = n->unique_out();\n+    set_ctrl_and_loop(bol, get_early_ctrl(bol));\n+  }\n+}\n+\n@@ -1132,0 +1191,6 @@\n+\n+  if (n->isa_FlatArrayCheck()) {\n+    move_flat_array_check_out_of_loop(n);\n+    return n;\n+  }\n+\n@@ -1404,0 +1469,98 @@\n+bool PhaseIdealLoop::flat_array_element_type_check(Node *n) {\n+  \/\/ If the CmpP is a subtype check for a value that has just been\n+  \/\/ loaded from an array, the subtype check guarantees the value\n+  \/\/ can't be stored in a flat array and the load of the value\n+  \/\/ happens with a flat array check then: push the type check\n+  \/\/ through the phi of the flat array check. This needs special\n+  \/\/ logic because the subtype check's input is not a phi but a\n+  \/\/ LoadKlass that must first be cloned through the phi.\n+  if (n->Opcode() != Op_CmpP) {\n+    return false;\n+  }\n+\n+  Node* klassptr = n->in(1);\n+  Node* klasscon = n->in(2);\n+\n+  if (klassptr->is_DecodeNarrowPtr()) {\n+    klassptr = klassptr->in(1);\n+  }\n+\n+  if (klassptr->Opcode() != Op_LoadKlass && klassptr->Opcode() != Op_LoadNKlass) {\n+    return false;\n+  }\n+\n+  if (!klasscon->is_Con()) {\n+    return false;\n+  }\n+\n+  Node* addr = klassptr->in(MemNode::Address);\n+\n+  if (!addr->is_AddP()) {\n+    return false;\n+  }\n+\n+  intptr_t offset;\n+  Node* obj = AddPNode::Ideal_base_and_offset(addr, &_igvn, offset);\n+\n+  if (obj == nullptr) {\n+    return false;\n+  }\n+\n+  assert(obj != nullptr && addr->in(AddPNode::Base) == addr->in(AddPNode::Address), \"malformed AddP?\");\n+  if (obj->Opcode() == Op_CastPP) {\n+    obj = obj->in(1);\n+  }\n+\n+  if (!obj->is_Phi()) {\n+    return false;\n+  }\n+\n+  Node* region = obj->in(0);\n+\n+  Node* phi = PhiNode::make_blank(region, n->in(1));\n+  for (uint i = 1; i < region->req(); i++) {\n+    Node* in = obj->in(i);\n+    Node* ctrl = region->in(i);\n+    if (addr->in(AddPNode::Base) != obj) {\n+      Node* cast = addr->in(AddPNode::Base);\n+      assert(cast->Opcode() == Op_CastPP && cast->in(0) != nullptr, \"inconsistent subgraph\");\n+      Node* cast_clone = cast->clone();\n+      cast_clone->set_req(0, ctrl);\n+      cast_clone->set_req(1, in);\n+      register_new_node(cast_clone, ctrl);\n+      const Type* tcast = cast_clone->Value(&_igvn);\n+      _igvn.set_type(cast_clone, tcast);\n+      cast_clone->as_Type()->set_type(tcast);\n+      in = cast_clone;\n+    }\n+    Node* addr_clone = addr->clone();\n+    addr_clone->set_req(AddPNode::Base, in);\n+    addr_clone->set_req(AddPNode::Address, in);\n+    register_new_node(addr_clone, ctrl);\n+    _igvn.set_type(addr_clone, addr_clone->Value(&_igvn));\n+    Node* klassptr_clone = klassptr->clone();\n+    klassptr_clone->set_req(2, addr_clone);\n+    register_new_node(klassptr_clone, ctrl);\n+    _igvn.set_type(klassptr_clone, klassptr_clone->Value(&_igvn));\n+    if (klassptr != n->in(1)) {\n+      Node* decode = n->in(1);\n+      assert(decode->is_DecodeNarrowPtr(), \"inconsistent subgraph\");\n+      Node* decode_clone = decode->clone();\n+      decode_clone->set_req(1, klassptr_clone);\n+      register_new_node(decode_clone, ctrl);\n+      _igvn.set_type(decode_clone, decode_clone->Value(&_igvn));\n+      klassptr_clone = decode_clone;\n+    }\n+    phi->set_req(i, klassptr_clone);\n+  }\n+  register_new_node(phi, region);\n+  Node* orig = n->in(1);\n+  _igvn.replace_input_of(n, 1, phi);\n+  split_if_with_blocks_post(n);\n+  if (n->outcnt() != 0) {\n+    _igvn.replace_input_of(n, 1, orig);\n+    _igvn.remove_dead_node(phi);\n+  }\n+  return true;\n+}\n+\n@@ -1410,0 +1573,4 @@\n+  if (flat_array_element_type_check(n)) {\n+    return;\n+  }\n+\n@@ -1553,0 +1720,5 @@\n+\n+  \/\/ Remove multiple allocations of the same inline type\n+  if (n->is_InlineType()) {\n+    n->as_InlineType()->remove_redundant_allocations(this);\n+  }\n@@ -2062,1 +2234,9 @@\n-  Node *sample_cmp = sample_bool->in(1);\n+  Node* sample_cmp = sample_bool->in(1);\n+  const Type* t = Type::TOP;\n+  const TypePtr* at = nullptr;\n+  if (sample_cmp->is_FlatArrayCheck()) {\n+    \/\/ Left input of a FlatArrayCheckNode is memory, set the (adr) type of the phi accordingly\n+    assert(sample_cmp->in(1)->bottom_type() == Type::MEMORY, \"unexpected input type\");\n+    t = Type::MEMORY;\n+    at = TypeRawPtr::BOTTOM;\n+  }\n@@ -2065,1 +2245,1 @@\n-  PhiNode *phi1 = new PhiNode(phi->in(0), Type::TOP);\n+  PhiNode *phi1 = new PhiNode(phi->in(0), t, at);\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":182,"deletions":2,"binary":false,"changes":184,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-  virtual Node *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node *match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n","filename":"src\/hotspot\/share\/opto\/multnode.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -574,0 +575,6 @@\n+  if (n->is_InlineType()) {\n+    C->add_inline_type(n);\n+  }\n+  if (n->is_LoadFlat() || n->is_StoreFlat()) {\n+    C->add_flat_access(n);\n+  }\n@@ -634,0 +641,3 @@\n+  if (is_InlineType()) {\n+    compile->remove_inline_type(this);\n+  }\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -90,0 +90,1 @@\n+class FlatArrayCheckNode;\n@@ -122,0 +123,1 @@\n+class MachPrologNode;\n@@ -128,0 +130,1 @@\n+class MachVEPNode;\n@@ -186,0 +189,3 @@\n+class InlineTypeNode;\n+class LoadFlatNode;\n+class StoreFlatNode;\n@@ -692,0 +698,2 @@\n+        DEFINE_CLASS_ID(LoadFlat,  SafePoint, 1)\n+        DEFINE_CLASS_ID(StoreFlat, SafePoint, 2)\n@@ -708,0 +716,1 @@\n+        DEFINE_CLASS_ID(Blackhole,        MemBar, 2)\n@@ -729,0 +738,2 @@\n+      DEFINE_CLASS_ID(MachProlog,       Mach, 8)\n+      DEFINE_CLASS_ID(MachVEP,          Mach, 9)\n@@ -761,1 +772,2 @@\n-      DEFINE_CLASS_ID(Con, Type, 8)\n+      DEFINE_CLASS_ID(InlineType, Type, 8)\n+      DEFINE_CLASS_ID(Con, Type, 9)\n@@ -763,2 +775,2 @@\n-      DEFINE_CLASS_ID(SafePointScalarMerge, Type, 9)\n-      DEFINE_CLASS_ID(Convert, Type, 10)\n+      DEFINE_CLASS_ID(SafePointScalarMerge, Type, 10)\n+      DEFINE_CLASS_ID(Convert, Type, 11)\n@@ -803,3 +815,4 @@\n-        DEFINE_CLASS_ID(FastLock,   Cmp, 0)\n-        DEFINE_CLASS_ID(FastUnlock, Cmp, 1)\n-        DEFINE_CLASS_ID(SubTypeCheck,Cmp, 2)\n+        DEFINE_CLASS_ID(FastLock,       Cmp, 0)\n+        DEFINE_CLASS_ID(FastUnlock,     Cmp, 1)\n+        DEFINE_CLASS_ID(SubTypeCheck,   Cmp, 2)\n+        DEFINE_CLASS_ID(FlatArrayCheck, Cmp, 3)\n@@ -914,0 +927,1 @@\n+  DEFINE_CLASS_QUERY(Blackhole)\n@@ -947,0 +961,1 @@\n+  DEFINE_CLASS_QUERY(FlatArrayCheck)\n@@ -979,0 +994,1 @@\n+  DEFINE_CLASS_QUERY(MachProlog)\n@@ -985,0 +1001,1 @@\n+  DEFINE_CLASS_QUERY(MachVEP)\n@@ -1023,0 +1040,3 @@\n+  DEFINE_CLASS_QUERY(InlineType)\n+  DEFINE_CLASS_QUERY(LoadFlat)\n+  DEFINE_CLASS_QUERY(StoreFlat)\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":26,"deletions":6,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"oops\/layoutKind.hpp\"\n@@ -34,1 +35,42 @@\n-class JvmtiTagMapKeyClosure;\n+\n+\/\/ Describes an object which can be tagged during heap walk operation.\n+\/\/ - generic heap object: _obj: oop, offset == 0, _inline_klass == nullptr;\n+\/\/ - value heap object: _obj: oop, offset == 0, _inline_klass == _obj.klass();\n+\/\/ - flat value object: _obj: holder object, offset == offset in the holder, _inline_klass == klass of the flattened object;\n+class JvmtiHeapwalkObject {\n+  oop _obj;                   \/\/ for flattened value object this is holder object\n+  int _offset;                \/\/ == 0 for heap objects\n+  InlineKlass* _inline_klass; \/\/ for value object, nullptr otherwise\n+  LayoutKind _layout_kind;    \/\/ layout kind in holder object, used only for flat->heap conversion\n+\n+  static InlineKlass* inline_klass_or_null(oop obj) {\n+    Klass* k = obj->klass();\n+    return k->is_inline_klass() ? InlineKlass::cast(k) : nullptr;\n+  }\n+public:\n+  JvmtiHeapwalkObject(): _obj(nullptr), _offset(0), _inline_klass(nullptr), _layout_kind(LayoutKind::UNKNOWN) {}\n+  JvmtiHeapwalkObject(oop obj): _obj(obj), _offset(0), _inline_klass(inline_klass_or_null(obj)), _layout_kind(LayoutKind::REFERENCE) {}\n+  JvmtiHeapwalkObject(oop obj, int offset, InlineKlass* ik, LayoutKind lk): _obj(obj), _offset(offset), _inline_klass(ik), _layout_kind(lk) {}\n+\n+  inline bool is_empty() const { return _obj == nullptr; }\n+  inline bool is_value() const { return _inline_klass != nullptr; }\n+  inline bool is_flat() const { return _offset != 0; }\n+\n+  inline oop obj() const { return _obj; }\n+  inline int offset() const { return _offset; }\n+  inline InlineKlass* inline_klass() const { return _inline_klass; }\n+  inline LayoutKind layout_kind() const { return _layout_kind; }\n+\n+  inline Klass* klass() const { return is_value() ? _inline_klass : obj()->klass(); }\n+\n+  static bool equals(const JvmtiHeapwalkObject& obj1, const JvmtiHeapwalkObject& obj2);\n+\n+  bool operator==(const JvmtiHeapwalkObject& other) const {\n+    \/\/ need to compare inline_klass too to handle the case when flat object has flat field at offset 0\n+    return _obj == other._obj && _offset == other._offset && _inline_klass == other._inline_klass;\n+  }\n+  bool operator!=(const JvmtiHeapwalkObject& other) const {\n+    return !(*this == other);\n+  }\n+};\n+\n@@ -40,1 +82,1 @@\n-\/\/ This class is the Key type for inserting in ResizeableResourceHashTable\n+\/\/ This class is the Key type for inserting in ResizeableHashTable\n@@ -43,0 +85,8 @@\n+\/\/\n+\/\/ Value objects: keep just one tag for all equal value objects including heap allocated value objects.\n+\/\/ We have to keep a strong reference to each unique value object with a non-zero tag.\n+\/\/ During heap walking flattened value object tags stored in separate JvmtiFlatTagMapTable,\n+\/\/ converted to standard strong entries in JvmtiTagMapTable outside of sefepoint.\n+\/\/ All equal value objects should have the same tag.\n+\/\/ Keep value objects alive (1 copy for each \"value\") until their tags are removed.\n+\n@@ -44,2 +94,8 @@\n-  WeakHandle _wh;\n-  oop _obj; \/\/ temporarily hold obj while searching\n+  union {\n+    WeakHandle _wh;\n+    OopHandle _h; \/\/ for value objects (_is_weak == false)\n+  };\n+  bool _is_weak;\n+  \/\/ temporarily hold obj while searching\n+  const JvmtiHeapwalkObject* _obj;\n+\n@@ -47,1 +103,2 @@\n-  JvmtiTagMapKey(oop obj);\n+  JvmtiTagMapKey(const JvmtiHeapwalkObject* obj);\n+  \/\/ Copy ctor is called when we put entry to the hash table.\n@@ -49,0 +106,1 @@\n+\n@@ -51,0 +109,2 @@\n+  JvmtiHeapwalkObject heapwalk_object() const;\n+\n@@ -53,6 +113,1 @@\n-  void release_weak_handle();\n-\n-  static unsigned get_hash(const JvmtiTagMapKey& entry) {\n-    assert(entry._obj != nullptr, \"must lookup obj to hash\");\n-    return (unsigned)entry._obj->identity_hash();\n-  }\n+  void release_handle();\n@@ -60,5 +115,2 @@\n-  static bool equals(const JvmtiTagMapKey& lhs, const JvmtiTagMapKey& rhs) {\n-    oop lhs_obj = lhs._obj != nullptr ? lhs._obj : lhs.object_no_keepalive();\n-    oop rhs_obj = rhs._obj != nullptr ? rhs._obj : rhs.object_no_keepalive();\n-    return lhs_obj == rhs_obj;\n-  }\n+  static unsigned get_hash(const JvmtiTagMapKey& entry);\n+  static bool equals(const JvmtiTagMapKey& lhs, const JvmtiTagMapKey& rhs);\n@@ -73,0 +125,6 @@\n+\/\/ A supporting class for iterating over all entries in Hashmap\n+class JvmtiTagMapKeyClosure {\n+public:\n+  virtual bool do_entry(JvmtiTagMapKey& key, jlong& value) = 0;\n+};\n+\n@@ -77,0 +135,2 @@\n+  jlong* lookup(const JvmtiHeapwalkObject& obj) const;\n+\n@@ -81,2 +141,7 @@\n-  jlong find(oop obj);\n-  void add(oop obj, jlong tag);\n+  int number_of_entries() const { return _table.number_of_entries(); }\n+\n+  jlong find(const JvmtiHeapwalkObject& obj) const;\n+  \/\/ obj cannot be flat\n+  void add(const JvmtiHeapwalkObject& obj, jlong tag);\n+  \/\/ update the tag if the entry exists, returns false otherwise\n+  bool update(const JvmtiHeapwalkObject& obj, jlong tag);\n@@ -84,1 +149,1 @@\n-  void remove(oop obj);\n+  bool remove(const JvmtiHeapwalkObject& obj);\n@@ -96,4 +161,68 @@\n-\/\/ A supporting class for iterating over all entries in Hashmap\n-class JvmtiTagMapKeyClosure {\n- public:\n-  virtual bool do_entry(JvmtiTagMapKey& key, jlong& value) = 0;\n+\n+\/\/ This class is the Key type for hash table to keep flattened value objects during heap walk operations.\n+\/\/ The objects needs to be moved to JvmtiTagMapTable outside of safepoint.\n+class JvmtiFlatTagMapKey: public CHeapObj<mtServiceability> {\n+private:\n+  \/\/ holder object\n+  OopHandle _h;\n+  \/\/ temporarily holds holder object while searching\n+  oop _holder;\n+  int _offset;\n+  InlineKlass* _inline_klass;\n+  LayoutKind _layout_kind;\n+public:\n+  JvmtiFlatTagMapKey(const JvmtiHeapwalkObject& obj);\n+  \/\/ Copy ctor is called when we put entry to the hash table.\n+  JvmtiFlatTagMapKey(const JvmtiFlatTagMapKey& src);\n+\n+  JvmtiFlatTagMapKey& operator=(const JvmtiFlatTagMapKey&) = delete;\n+\n+  JvmtiHeapwalkObject heapwalk_object() const;\n+\n+  oop holder() const;\n+  oop holder_no_keepalive() const;\n+  int offset() const { return _offset; }\n+  InlineKlass* inline_klass() const { return _inline_klass; }\n+  LayoutKind layout_kind() const { return _layout_kind; }\n+\n+  void release_handle();\n+\n+  static unsigned get_hash(const JvmtiFlatTagMapKey& entry);\n+  static bool equals(const JvmtiFlatTagMapKey& lhs, const JvmtiFlatTagMapKey& rhs);\n+};\n+\n+typedef\n+ResizeableHashTable <JvmtiFlatTagMapKey, jlong,\n+                     AnyObj::C_HEAP, mtServiceability,\n+                     JvmtiFlatTagMapKey::get_hash,\n+                     JvmtiFlatTagMapKey::equals> FlatObjectHashtable;\n+\n+\/\/ A supporting class for iterating over all entries in JvmtiFlatTagMapTable.\n+class JvmtiFlatTagMapKeyClosure {\n+public:\n+  virtual bool do_entry(JvmtiFlatTagMapKey& key, jlong& value) = 0;\n+};\n+\n+class JvmtiFlatTagMapTable: public CHeapObj<mtServiceability> {\n+private:\n+  FlatObjectHashtable _table;\n+\n+public:\n+  JvmtiFlatTagMapTable();\n+  ~JvmtiFlatTagMapTable();\n+\n+  int number_of_entries() const { return _table.number_of_entries(); }\n+\n+  jlong find(const JvmtiHeapwalkObject& obj) const;\n+  \/\/ obj must be flat\n+  void add(const JvmtiHeapwalkObject& obj, jlong tag);\n+\n+  \/\/ returns tag for the entry, 0 is not found\n+  jlong remove(const JvmtiHeapwalkObject& obj);\n+\n+  \/\/ iterate over entries in the hashmap\n+  void entry_iterate(JvmtiFlatTagMapKeyClosure* closure);\n+\n+  bool is_empty() const { return _table.number_of_entries() == 0; }\n+\n+  void clear();\n","filename":"src\/hotspot\/share\/prims\/jvmtiTagMapTable.hpp","additions":152,"deletions":23,"binary":false,"changes":175,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -63,0 +64,1 @@\n+#include \"oops\/access.hpp\"\n@@ -65,0 +67,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -90,0 +93,1 @@\n+#include \"runtime\/keepStackGCProcessed.hpp\"\n@@ -2019,0 +2023,103 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return nullptr;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(ih->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return nullptr;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayHandle result_array =\n+      oopFactory::new_objArray_handle(vmClasses::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ih->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+WB_END\n+\n+\/\/ Collect Object oops but not value objects...loaded from heap\n+class CollectObjectOops : public BasicOopIterateClosure {\n+  public:\n+  GrowableArray<Handle>* _array;\n+\n+  CollectObjectOops() {\n+      _array = new GrowableArray<Handle>(128);\n+  }\n+\n+  void add_oop(oop o) {\n+    Handle oh = Handle(Thread::current(), o);\n+    if (oh != nullptr && oh->is_inline_type()) {\n+      oh->oop_iterate(this);\n+    } else {\n+      _array->append(oh);\n+    }\n+  }\n+\n+  template <class T> inline void add_oop(T* p) { add_oop(HeapAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    objArrayHandle result_array =\n+        oopFactory::new_objArray_handle(vmClasses::Object_klass(), _array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < _array->length(); i++) {\n+      result_array->obj_at_put(i, _array->at(i)());\n+    }\n+    return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+  }\n+};\n+\n+\/\/ Collect Object oops but not value objects...loaded from frames\n+class CollectFrameObjectOops : public BasicOopIterateClosure {\n+ public:\n+  CollectObjectOops _collect;\n+\n+  template <class T> inline void add_oop(T* p) { _collect.add_oop(RawAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    return _collect.create_jni_result(env, THREAD);\n+  }\n+};\n+\n+\/\/ Collect Object oops for the given oop, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(thread);\n+  Handle objh(thread, JNIHandles::resolve(thing));\n+  CollectObjectOops collectOops;\n+  objh->oop_iterate(&collectOops);\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\/\/ Collect Object oops for the given frame deep, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  KeepStackGCProcessedMark ksgcpm(THREAD);\n+  ResourceMark rm(THREAD);\n+  CollectFrameObjectOops collectOops;\n+  StackFrameStream sfs(thread, true \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, nullptr, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n@@ -3036,0 +3143,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":113,"deletions":0,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -472,1 +472,1 @@\n-  template<typename FKind> frame new_heap_frame(frame& f, frame& caller);\n+  template<typename FKind> frame new_heap_frame(frame& f, frame& caller, int size_adjust = 0);\n@@ -474,1 +474,1 @@\n-  inline void patch_pd(frame& callee, const frame& caller);\n+  inline void patch_pd(frame& callee, const frame& caller, bool is_bottom_frame);\n@@ -1184,1 +1184,1 @@\n-  patch_pd(hf, caller);\n+  patch_pd(hf, caller, is_bottom_frame);\n@@ -1281,2 +1281,24 @@\n-  const int argsize = ContinuationHelper::CompiledFrame::stack_argsize(f) + frame::metadata_words_at_top;\n-  const int fsize = pointer_delta_as_int(stack_frame_bottom + argsize, stack_frame_top);\n+  int argsize = ContinuationHelper::CompiledFrame::stack_argsize(f) + frame::metadata_words_at_top;\n+  int fsize = pointer_delta_as_int(stack_frame_bottom + argsize, stack_frame_top);\n+\n+  int real_frame_size = 0;\n+  bool augmented = f.was_augmented_on_entry(real_frame_size);\n+  if (augmented) {\n+    \/\/ The args reside inside the frame so clear argsize. If the caller is compiled,\n+    \/\/ this will cause the stack arguments passed by the caller to be freezed when\n+    \/\/ freezing the caller frame itself. If the caller is interpreted this will have\n+    \/\/ the effect of discarding the arg area created in the i2c stub.\n+    argsize = 0;\n+    fsize = real_frame_size - (callee_interpreted ? 0 : callee_argsize);\n+#ifdef ASSERT\n+    nmethod* nm = f.cb()->as_nmethod();\n+    Method* method = nm->method();\n+    address return_pc = ContinuationHelper::CompiledFrame::return_pc(f);\n+    CodeBlob* caller_cb = CodeCache::find_blob_fast(return_pc);\n+    assert(nm->is_compiled_by_c2() || (caller_cb->is_nmethod() && caller_cb->as_nmethod()->is_compiled_by_c2()), \"caller or callee should be c2 compiled\");\n+    assert((!caller_cb->is_nmethod() && nm->is_compiled_by_c2()) ||\n+           (nm->compiler_type() != caller_cb->as_nmethod()->compiler_type()) ||\n+           (nm->is_compiled_by_c2() && !method->is_static() && method->method_holder()->is_inline_klass()),\n+           \"frame should not be extended\");\n+#endif\n+  }\n@@ -1284,1 +1306,1 @@\n-  log_develop_trace(continuations)(\"recurse_freeze_compiled_frame %s _size: %d fsize: %d argsize: %d\",\n+  log_develop_trace(continuations)(\"recurse_freeze_compiled_frame %s _size: %d fsize: %d argsize: %d augmented: %d\",\n@@ -1287,1 +1309,1 @@\n-                             _freeze_size, fsize, argsize);\n+                             _freeze_size, fsize, argsize, augmented);\n@@ -1298,0 +1320,1 @@\n+  assert(!is_bottom_frame || !augmented, \"thaw extended frame without caller?\");\n@@ -1301,1 +1324,1 @@\n-  frame hf = new_heap_frame<ContinuationHelper::CompiledFrame>(f, caller);\n+  frame hf = new_heap_frame<ContinuationHelper::CompiledFrame>(f, caller, augmented ? real_frame_size - f.cb()->as_nmethod()->frame_size() : 0);\n@@ -2081,0 +2104,1 @@\n+  int remove_scalarized_frames(StackChunkFrameStream<ChunkFrames::CompiledOnly>& scfs, int &argsize);\n@@ -2106,1 +2130,1 @@\n-  inline void patch(frame& f, const frame& caller, bool bottom);\n+  inline void patch(frame& f, const frame& caller, bool bottom, bool augmented = false);\n@@ -2116,1 +2140,1 @@\n-  template<typename FKind> frame new_stack_frame(const frame& hf, frame& caller, bool bottom);\n+  template<typename FKind> frame new_stack_frame(const frame& hf, frame& caller, bool bottom, int size_adjust = 0);\n@@ -2193,0 +2217,14 @@\n+int ThawBase::remove_scalarized_frames(StackChunkFrameStream<ChunkFrames::CompiledOnly>& f, int &argsize) {\n+  intptr_t* top = f.sp();\n+\n+  while (f.cb()->as_nmethod()->needs_stack_repair()) {\n+    f.next(SmallRegisterMap::instance_no_args(), false \/* stop *\/);\n+  }\n+  assert(!f.is_done(), \"\");\n+  assert(f.is_compiled(), \"\");\n+\n+  intptr_t* bottom = f.sp() + f.cb()->frame_size();\n+  argsize = f.stack_argsize();\n+  return bottom - top;\n+}\n+\n@@ -2214,3 +2252,0 @@\n-    frame_size += f.cb()->frame_size();\n-    argsize = f.stack_argsize();\n-\n@@ -2223,0 +2258,9 @@\n+\n+    if (f.cb()->as_nmethod()->needs_stack_repair()) {\n+      frame_size += remove_scalarized_frames(f, argsize);\n+    } else {\n+      frame_size += f.cb()->frame_size();\n+      argsize = f.stack_argsize();\n+    }\n+  } else if (f.cb()->as_nmethod()->needs_stack_repair()) {\n+    frame_size = remove_scalarized_frames(f, argsize);\n@@ -2522,0 +2566,1 @@\n+  CodeBlob* cb = _stream.cb();\n@@ -2526,3 +2571,8 @@\n-  \/\/ we never leave a compiled caller of an interpreted frame as the top frame in the chunk\n-  \/\/ as it makes detecting that situation and adjusting unextended_sp tricky\n-  if (num_frames == 1 && !_stream.is_done() && FKind::interpreted && _stream.is_compiled()) {\n+  \/\/ We never leave a compiled caller of an interpreted frame as the top frame in the chunk\n+  \/\/ as it makes detecting that situation and adjusting unextended_sp tricky. We also always\n+  \/\/ thaw the caller of a frame that needs_stack_repair, as it would otherwise complicate things:\n+  \/\/ - Regardless of whether the frame was extended or not, we would need to copy the right arg\n+  \/\/   size if its greater than the one given by the normal method signature (non-scalarized).\n+  \/\/ - If the frame was indeed extended, leaving its caller as the top frame would complicate walking\n+  \/\/   the chunk (we need unextended_sp, but we only have sp).\n+  if (num_frames == 1 && !_stream.is_done() && ((FKind::interpreted && _stream.is_compiled()) || (FKind::compiled && cb->as_nmethod_or_null()->needs_stack_repair()))) {\n@@ -2588,1 +2638,1 @@\n-inline void ThawBase::patch(frame& f, const frame& caller, bool bottom) {\n+inline void ThawBase::patch(frame& f, const frame& caller, bool bottom, bool augmented) {\n@@ -2593,1 +2643,1 @@\n-  } else {\n+  } else if (caller.is_compiled_frame()){\n@@ -2596,1 +2646,1 @@\n-    ContinuationHelper::Frame::patch_pc(caller, caller.raw_pc());\n+    ContinuationHelper::Frame::patch_pc(caller, caller.raw_pc(), augmented \/*callee_augmented*\/);\n@@ -2829,0 +2879,10 @@\n+  int fsize = 0;\n+  int added_argsize = 0;\n+  bool augmented = hf.was_augmented_on_entry(fsize);\n+  if (!augmented) {\n+    added_argsize = (is_bottom_frame || caller.is_interpreted_frame()) ? hf.compiled_frame_stack_argsize() : 0;\n+    fsize += added_argsize;\n+  }\n+  assert(!is_bottom_frame || !augmented, \"\");\n+\n+\n@@ -2832,1 +2892,3 @@\n-  frame f = new_stack_frame<ContinuationHelper::CompiledFrame>(hf, caller, is_bottom_frame);\n+  frame f = new_stack_frame<ContinuationHelper::CompiledFrame>(hf, caller, is_bottom_frame, augmented ? fsize - hf.cb()->frame_size() : 0);\n+  assert(f.cb()->frame_size() == (int)(caller.sp() - f.sp()), \"\");\n+\n@@ -2835,5 +2897,0 @@\n-\n-  const int added_argsize = (is_bottom_frame || caller.is_interpreted_frame()) ? hf.compiled_frame_stack_argsize() : 0;\n-  int fsize = ContinuationHelper::CompiledFrame::size(hf) + added_argsize;\n-  assert(fsize <= (int)(caller.unextended_sp() - f.unextended_sp()), \"\");\n-\n@@ -2852,1 +2909,1 @@\n-  patch(f, caller, is_bottom_frame);\n+  patch(f, caller, is_bottom_frame, augmented);\n","filename":"src\/hotspot\/share\/runtime\/continuationFreezeThaw.cpp","additions":83,"deletions":26,"binary":false,"changes":109,"status":"modified"},{"patch":"@@ -817,1 +817,1 @@\n-  develop(bool, PrintFieldLayout, false,                                    \\\n+  product(bool, PrintFieldLayout, false, DIAGNOSTIC,                        \\\n@@ -820,0 +820,27 @@\n+  product(bool, PrintInlineLayout, false, DIAGNOSTIC,                       \\\n+          \"Print field layout for each inline type or class with inline fields\") \\\n+                                                                            \\\n+  product(bool, PrintFlatArrayLayout, false, DIAGNOSTIC,                    \\\n+          \"Print array layout for each inline type array\")                  \\\n+                                                                            \\\n+  product(bool, UseArrayFlattening, true,                                   \\\n+          \"Allow the VM to flatten arrays\")                                 \\\n+                                                                            \\\n+  product(bool, UseFieldFlattening, true,                                   \\\n+          \"Allow the VM to flatten value fields\")                           \\\n+                                                                            \\\n+  product(bool, UseNonAtomicValueFlattening, true,                          \\\n+          \"Allow the JVM to flatten some non-atomic null-free values\")      \\\n+                                                                            \\\n+  product(bool, UseNullableValueFlattening, true,                           \\\n+          \"Allow the JVM to flatten some nullable values\")                  \\\n+                                                                            \\\n+  product(bool, UseAtomicValueFlattening, true,                             \\\n+          \"Allow the JVM to flatten some atomic values\")                    \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxOops, 4,                                 \\\n+          \"Max nof embedded object references in an inline type to flatten, <0 no limit\")  \\\n+                                                                            \\\n+  develop(ccstrlist, PrintInlineKlassFields, \"\",                            \\\n+          \"Print fields collected by InlineKlass::collect_fields\")          \\\n+                                                                            \\\n@@ -1780,0 +1807,3 @@\n+  product(bool, IgnoreAssertUnsetFields, false, DIAGNOSTIC,                           \\\n+          \"Ignore assert_unset_fields\")                                     \\\n+                                                                            \\\n@@ -1951,0 +1981,17 @@\n+  product_pd(bool, InlineTypePassFieldsAsArgs,                              \\\n+          \"Pass each inline type field as an argument at calls\")            \\\n+                                                                            \\\n+  product_pd(bool, InlineTypeReturnedAsFields,                              \\\n+          \"Return fields instead of an inline type reference\")              \\\n+                                                                            \\\n+  develop(bool, StressCallingConvention, false,                             \\\n+          \"Stress the scalarized calling convention.\")                      \\\n+                                                                            \\\n+  develop(bool, PreloadClasses, true,                                       \\\n+          \"Preloading all classes from the LoadableDescriptors attribute\")  \\\n+                                                                            \\\n+  product(ccstrlist, ForceNonTearable, \"\", DIAGNOSTIC,                      \\\n+          \"List of inline classes which are forced to be atomic \"           \\\n+          \"(whitespace and commas separate names, \"                         \\\n+          \"and leading and trailing stars '*' are wildcards)\")              \\\n+                                                                            \\\n@@ -2009,0 +2056,3 @@\n+  product(bool, UseAltSubstitutabilityMethod, false,                        \\\n+          \"Use alternate version of the isSubstitutable method to \"         \\\n+          \"compare value class instances\")                                  \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":51,"deletions":1,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+class InlineKlass;\n@@ -131,0 +132,2 @@\n+DEF_HANDLE(flatArray        , is_flatArray_noinline        )\n+DEF_HANDLE(refArray         , is_refArray_noinline         )\n","filename":"src\/hotspot\/share\/runtime\/handles.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -64,0 +64,2 @@\n+DEF_HANDLE_CONSTR(flatArray, is_flatArray_noinline)\n+DEF_HANDLE_CONSTR(refArray , is_refArray_noinline )\n","filename":"src\/hotspot\/share\/runtime\/handles.inline.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -51,0 +51,3 @@\n+\n+  friend class JNI_FastGetField;\n+\n@@ -55,1 +58,2 @@\n-    address_bits           = BitsPerWord - checked_bits - instance_bits,\n+    flat_bits              = 1,\n+    address_bits           = BitsPerWord - checked_bits - instance_bits - flat_bits,\n@@ -63,1 +67,2 @@\n-    address_shift          = instance_shift + instance_bits,\n+    flat_shift             = instance_shift + instance_bits,\n+    address_shift          = flat_shift + flat_bits,\n@@ -70,0 +75,1 @@\n+    flat_mask_in_place     = right_n_bits(flat_bits) << flat_shift,\n@@ -114,2 +120,11 @@\n-  static jfieldID to_instance_jfieldID(Klass* k, int offset) {\n-    intptr_t as_uint = ((offset & large_offset_mask) << offset_shift) | instance_mask_in_place;\n+  static bool is_flat_jfieldID(jfieldID id) {\n+    uintptr_t as_uint = (uintptr_t) id;\n+    return ((as_uint & flat_mask_in_place) != 0);\n+  }\n+\n+  static jfieldID to_instance_jfieldID(Klass* k, int offset, bool is_flat) {\n+    intptr_t as_uint = ((offset & large_offset_mask) << offset_shift) |\n+                        instance_mask_in_place;\n+    if (is_flat) {\n+      as_uint |= flat_mask_in_place;\n+    }\n@@ -157,1 +172,1 @@\n-  static jfieldID to_jfieldID(InstanceKlass* k, int offset, bool is_static) {\n+  static jfieldID to_jfieldID(InstanceKlass* k, int offset, bool is_static, bool is_flat) {\n@@ -163,1 +178,1 @@\n-      return jfieldIDWorkaround::to_instance_jfieldID(k, offset);\n+      return jfieldIDWorkaround::to_instance_jfieldID(k, offset, is_flat);\n","filename":"src\/hotspot\/share\/runtime\/jfieldIDWorkaround.hpp","additions":21,"deletions":6,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -54,0 +55,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -231,1 +233,2 @@\n-    value->l = cast_from_oop<jobject>(objArrayOop(a)->obj_at(index));\n+    oop o = objArrayOop(a)->obj_at(index, CHECK_(T_ILLEGAL)); \/\/ reading from a flat array can throw an OOM\n+    value->l = cast_from_oop<jobject>(o);\n@@ -273,0 +276,1 @@\n+\n@@ -276,0 +280,4 @@\n+    if (a->is_null_free_array() && obj == nullptr) {\n+      THROW_MSG(vmSymbols::java_lang_NullPointerException(), \"null-restricted array\");\n+    }\n+\n@@ -759,3 +767,0 @@\n-  if (log_is_enabled(Debug, class, resolve)) {\n-    trace_class_resolution(nt);\n-  }\n@@ -767,3 +772,2 @@\n-  assert(!method()->is_object_initializer() &&\n-         (for_constant_pool_access || !method()->is_static_initializer()),\n-         \"Should not be the initializer\");\n+  assert(!method()->name()->starts_with('<') || for_constant_pool_access,\n+         \"should call new_constructor instead\");\n@@ -817,1 +821,2 @@\n-  assert(method()->is_object_initializer(), \"Should be the initializer\");\n+  assert(method()->is_object_constructor(),\n+         \"should call new_method instead\");\n@@ -866,0 +871,2 @@\n+\n+  int flags = 0;\n@@ -867,1 +874,4 @@\n-    java_lang_reflect_Field::set_trusted_final(rh());\n+    flags |= TRUSTED_FINAL;\n+  }\n+  if (fd->is_null_free_inline_type()) {\n+    flags |= NULL_RESTRICTED;\n@@ -869,0 +879,2 @@\n+  java_lang_reflect_Field::set_flags(rh(), flags);\n+\n@@ -979,1 +991,2 @@\n-    if (reflected_method->is_private() || reflected_method->name() == vmSymbols::object_initializer_name()) {\n+    if (reflected_method->is_private() ||\n+        reflected_method->name() == vmSymbols::object_initializer_name()) {\n@@ -1059,2 +1072,2 @@\n-    oop type_mirror = ptypes->obj_at(i);\n-    oop arg = args->obj_at(i);\n+    oop type_mirror = ptypes->obj_at(i, CHECK_NULL);\n+    oop arg = args->obj_at(i, CHECK_NULL);\n@@ -1161,1 +1174,0 @@\n-  assert(method->name() == vmSymbols::object_initializer_name(), \"invalid constructor\");\n","filename":"src\/hotspot\/share\/runtime\/reflection.cpp","additions":25,"deletions":13,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"memory\/oopFactory.hpp\"\n@@ -51,0 +52,3 @@\n+#include \"oops\/access.hpp\"\n+#include \"oops\/fieldStreams.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -54,0 +58,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -73,0 +78,1 @@\n+#include \"runtime\/signature.hpp\"\n@@ -1230,0 +1236,16 @@\n+  \/\/ Substitutability test implementation piggy backs on static call resolution\n+  Bytecodes::Code code = caller->java_code_at(bci);\n+  if (code == Bytecodes::_if_acmpeq || code == Bytecodes::_if_acmpne) {\n+    bc = Bytecodes::_invokestatic;\n+    methodHandle attached_method(THREAD, extract_attached_method(vfst));\n+    assert(attached_method.not_null(), \"must have attached method\");\n+    vmClasses::ValueObjectMethods_klass()->initialize(CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, false, CHECK_NH);\n+#ifdef ASSERT\n+    Symbol* subst_method_name = UseAltSubstitutabilityMethod ? vmSymbols::isSubstitutableAlt_name() : vmSymbols::isSubstitutable_name();\n+    Method* is_subst = vmClasses::ValueObjectMethods_klass()->find_method(subst_method_name, vmSymbols::object_object_boolean_signature());\n+    assert(callinfo.selected_method() == is_subst, \"must be isSubstitutable method\");\n+#endif\n+    return receiver;\n+  }\n+\n@@ -1265,0 +1287,6 @@\n+    } else {\n+      assert(attached_method->has_scalarized_args(), \"invalid use of attached method\");\n+      if (!attached_method->method_holder()->is_inline_klass()) {\n+        \/\/ Ignore the attached method in this case to not confuse below code\n+        attached_method = methodHandle(current, nullptr);\n+      }\n@@ -1273,0 +1301,1 @@\n+  bool check_null_and_abstract = true;\n@@ -1286,2 +1315,3 @@\n-    if (attached_method.is_null()) {\n-      Method* callee = bytecode.static_target(CHECK_NH);\n+    Method* callee = attached_method();\n+    if (callee == nullptr) {\n+      callee = bytecode.static_target(CHECK_NH);\n@@ -1292,7 +1322,17 @@\n-\n-    \/\/ Retrieve from a compiled argument list\n-    receiver = Handle(current, callerFrame.retrieve_receiver(&reg_map2));\n-    assert(oopDesc::is_oop_or_null(receiver()), \"\");\n-\n-    if (receiver.is_null()) {\n-      THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+    bool caller_is_c1 = callerFrame.is_compiled_frame() && callerFrame.cb()->as_nmethod()->is_compiled_by_c1();\n+    if (!caller_is_c1 && callee->is_scalarized_arg(0)) {\n+      \/\/ If the receiver is an inline type that is passed as fields, no oop is available\n+      \/\/ Resolve the call without receiver null checking.\n+      assert(!callee->mismatch(), \"calls with inline type receivers should never mismatch\");\n+      assert(attached_method.not_null() && !attached_method->is_abstract(), \"must have non-abstract attached method\");\n+      if (bc == Bytecodes::_invokeinterface) {\n+        bc = Bytecodes::_invokevirtual; \/\/ C2 optimistically replaces interface calls by virtual calls\n+      }\n+      check_null_and_abstract = false;\n+    } else {\n+      \/\/ Retrieve from a compiled argument list\n+      receiver = Handle(current, callerFrame.retrieve_receiver(&reg_map2));\n+      assert(oopDesc::is_oop_or_null(receiver()), \"\");\n+      if (receiver.is_null()) {\n+        THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+      }\n@@ -1305,1 +1345,1 @@\n-    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, check_null_and_abstract, CHECK_NH);\n@@ -1314,1 +1354,1 @@\n-  if (has_receiver) {\n+  if (has_receiver && check_null_and_abstract) {\n@@ -1342,1 +1382,1 @@\n-methodHandle SharedRuntime::find_callee_method(TRAPS) {\n+methodHandle SharedRuntime::find_callee_method(bool& caller_does_not_scalarize, TRAPS) {\n@@ -1368,0 +1408,4 @@\n+    \/\/ Calls via mismatching methods are always non-scalarized\n+    if (callinfo.resolved_method()->mismatch()) {\n+      caller_does_not_scalarize = true;\n+    }\n@@ -1375,1 +1419,1 @@\n-methodHandle SharedRuntime::resolve_helper(bool is_virtual, bool is_optimized, TRAPS) {\n+methodHandle SharedRuntime::resolve_helper(bool is_virtual, bool is_optimized, bool& caller_does_not_scalarize, TRAPS) {\n@@ -1398,0 +1442,4 @@\n+  \/\/ Calls via mismatching methods are always non-scalarized\n+  if (caller_nm->is_compiled_by_c1() || call_info.resolved_method()->mismatch()) {\n+    caller_does_not_scalarize = true;\n+  }\n@@ -1416,1 +1464,1 @@\n-    tty->print(\"resolving %s%s (%s) call to\",\n+    tty->print(\"resolving %s%s (%s) %s call to\",\n@@ -1418,1 +1466,1 @@\n-               Bytecodes::name(invoke_code));\n+               Bytecodes::name(invoke_code), (caller_does_not_scalarize) ? \"non-scalar\" : \"\");\n@@ -1457,1 +1505,1 @@\n-    inline_cache->update(&call_info, receiver->klass());\n+    inline_cache->update(&call_info, receiver->klass(), caller_does_not_scalarize);\n@@ -1461,1 +1509,1 @@\n-    callsite->set(callee_method);\n+    callsite->set(callee_method, caller_does_not_scalarize);\n@@ -1481,0 +1529,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1482,1 +1531,1 @@\n-    callee_method = SharedRuntime::handle_ic_miss_helper(CHECK_NULL);\n+    callee_method = SharedRuntime::handle_ic_miss_helper(caller_does_not_scalarize, CHECK_NULL);\n@@ -1487,1 +1536,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, false, caller_does_not_scalarize);\n@@ -1528,1 +1577,5 @@\n-      return callee->get_c2i_entry();\n+      if (caller_frame.is_interpreted_frame()) {\n+        return callee->get_c2i_inline_entry();\n+      } else {\n+        return callee->get_c2i_entry();\n+      }\n@@ -1534,0 +1587,3 @@\n+  bool is_static_call = false;\n+  bool is_optimized = false;\n+  bool caller_does_not_scalarize = false;\n@@ -1536,1 +1592,1 @@\n-    callee_method = SharedRuntime::reresolve_call_site(CHECK_NULL);\n+    callee_method = SharedRuntime::reresolve_call_site(is_optimized, caller_does_not_scalarize, CHECK_NULL);\n@@ -1540,1 +1596,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, callee_method->is_static(), is_optimized, caller_does_not_scalarize);\n@@ -1579,1 +1635,2 @@\n-address SharedRuntime::get_resolved_entry(JavaThread* current, methodHandle callee_method) {\n+address SharedRuntime::get_resolved_entry(JavaThread* current, methodHandle callee_method,\n+                                          bool is_static_call, bool is_optimized, bool caller_does_not_scalarize) {\n@@ -1585,2 +1642,11 @@\n-  assert(callee_method->verified_code_entry() != nullptr, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+\n+  if (caller_does_not_scalarize) {\n+    assert(callee_method->verified_inline_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_inline_code_entry();\n+  } else if (is_static_call || is_optimized) {\n+    assert(callee_method->verified_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_code_entry();\n+  } else {\n+    assert(callee_method->verified_inline_ro_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_inline_ro_code_entry();\n+  }\n@@ -1592,0 +1658,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1594,1 +1661,1 @@\n-    callee_method = SharedRuntime::resolve_helper(false, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(false, false, caller_does_not_scalarize, CHECK_NULL);\n@@ -1598,1 +1665,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, true, false, caller_does_not_scalarize);\n@@ -1604,0 +1671,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1605,1 +1673,1 @@\n-    callee_method = SharedRuntime::resolve_helper(true, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(true, false, caller_does_not_scalarize, CHECK_NULL);\n@@ -1609,1 +1677,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, false, caller_does_not_scalarize);\n@@ -1617,0 +1685,1 @@\n+  bool caller_does_not_scalarize = false;\n@@ -1618,1 +1687,1 @@\n-    callee_method = SharedRuntime::resolve_helper(true, true, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(true, true, caller_does_not_scalarize, CHECK_NULL);\n@@ -1622,1 +1691,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, true, caller_does_not_scalarize);\n@@ -1625,1 +1694,3 @@\n-methodHandle SharedRuntime::handle_ic_miss_helper(TRAPS) {\n+\n+\n+methodHandle SharedRuntime::handle_ic_miss_helper(bool& caller_does_not_scalarize, TRAPS) {\n@@ -1643,1 +1714,1 @@\n-    tty->print(\"IC miss (%s) call to\", Bytecodes::name(bc));\n+    tty->print(\"IC miss (%s) %s call to\", Bytecodes::name(bc), (caller_does_not_scalarize) ? \"non-scalar\" : \"\");\n@@ -1675,0 +1746,4 @@\n+  \/\/ Calls via mismatching methods are always non-scalarized\n+  if (caller_nm->is_compiled_by_c1() || call_info.resolved_method()->mismatch()) {\n+    caller_does_not_scalarize = true;\n+  }\n@@ -1678,1 +1753,1 @@\n-  inline_cache->update(&call_info, receiver()->klass());\n+  inline_cache->update(&call_info, receiver()->klass(), caller_does_not_scalarize);\n@@ -1689,1 +1764,1 @@\n-methodHandle SharedRuntime::reresolve_call_site(TRAPS) {\n+methodHandle SharedRuntime::reresolve_call_site(bool& is_optimized, bool& caller_does_not_scalarize, TRAPS) {\n@@ -1699,8 +1774,21 @@\n-\n-  \/\/ Do nothing if the frame isn't a live compiled frame.\n-  \/\/ nmethod could be deoptimized by the time we get here\n-  \/\/ so no update to the caller is needed.\n-\n-  if ((caller.is_compiled_frame() && !caller.is_deoptimized_frame()) ||\n-      (caller.is_native_frame() && caller.cb()->as_nmethod()->method()->is_continuation_enter_intrinsic())) {\n-\n+  if (caller.is_compiled_frame()) {\n+    caller_does_not_scalarize = caller.cb()->as_nmethod()->is_compiled_by_c1();\n+  }\n+  assert(!caller.is_interpreted_frame(), \"must be compiled\");\n+\n+  \/\/ If the frame isn't a live compiled frame (i.e. deoptimized by the time we get here), no IC clearing must be done\n+  \/\/ for the caller. However, when the caller is C2 compiled and the callee a C1 or C2 compiled method, then we still\n+  \/\/ need to figure out whether it was an optimized virtual call with an inline type receiver. Otherwise, we end up\n+  \/\/ using the wrong method entry point and accidentally skip the buffering of the receiver.\n+  methodHandle callee_method = find_callee_method(caller_does_not_scalarize, CHECK_(methodHandle()));\n+  const bool caller_is_compiled_and_not_deoptimized = caller.is_compiled_frame() && !caller.is_deoptimized_frame();\n+  const bool caller_is_continuation_enter_intrinsic =\n+    caller.is_native_frame() && caller.cb()->as_nmethod()->method()->is_continuation_enter_intrinsic();\n+  const bool do_IC_clearing = caller_is_compiled_and_not_deoptimized || caller_is_continuation_enter_intrinsic;\n+\n+  const bool callee_compiled_with_scalarized_receiver = callee_method->has_compiled_code() &&\n+                                                        !callee_method()->is_static() &&\n+                                                        callee_method()->is_scalarized_arg(0);\n+  const bool compute_is_optimized = !caller_does_not_scalarize && callee_compiled_with_scalarized_receiver;\n+\n+  if (do_IC_clearing || compute_is_optimized) {\n@@ -1739,0 +1827,1 @@\n+        is_optimized = false;\n@@ -1741,0 +1830,1 @@\n+            assert(callee_method->is_static(), \"must be\");\n@@ -1742,2 +1832,5 @@\n-            CompiledDirectCall* cdc = CompiledDirectCall::at(call_addr);\n-            cdc->set_to_clean();\n+            is_optimized = (iter.type() == relocInfo::opt_virtual_call_type);\n+            if (do_IC_clearing) {\n+              CompiledDirectCall* cdc = CompiledDirectCall::at(call_addr);\n+              cdc->set_to_clean();\n+            }\n@@ -1746,4 +1839,5 @@\n-\n-            \/\/ compiled, dispatched call (which used to call an interpreted method)\n-            CompiledIC* inline_cache = CompiledIC_at(caller_nm, call_addr);\n-            inline_cache->set_to_clean();\n+            if (do_IC_clearing) {\n+              \/\/ compiled, dispatched call (which used to call an interpreted method)\n+              CompiledIC* inline_cache = CompiledIC_at(caller_nm, call_addr);\n+              inline_cache->set_to_clean();\n+            }\n@@ -1760,3 +1854,0 @@\n-  methodHandle callee_method = find_callee_method(CHECK_(methodHandle()));\n-\n-\n@@ -1768,1 +1859,1 @@\n-    tty->print(\"handle_wrong_method reresolving call to\");\n+    tty->print(\"handle_wrong_method reresolving %s call to\", (caller_does_not_scalarize) ? \"non-scalar\" : \"\");\n@@ -1974,0 +2065,15 @@\n+char* SharedRuntime::generate_identity_exception_message(JavaThread* current, Klass* klass) {\n+  assert(klass->is_inline_klass(), \"Must be a concrete value class\");\n+  const char* desc = \"Cannot synchronize on an instance of value class \";\n+  const char* className = klass->external_name();\n+  size_t msglen = strlen(desc) + strlen(className) + 1;\n+  char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+  if (nullptr == message) {\n+    \/\/ Out of memory: can't create detailed error message\n+    message = const_cast<char*>(klass->external_name());\n+  } else {\n+    jio_snprintf(message, msglen, \"%s%s\", desc, className);\n+  }\n+  return message;\n+}\n+\n@@ -2207,5 +2313,30 @@\n- private:\n-  enum {\n-    _basic_type_bits = 4,\n-    _basic_type_mask = right_n_bits(_basic_type_bits),\n-    _basic_types_per_int = BitsPerInt \/ _basic_type_bits,\n+public:\n+  class Element {\n+  private:\n+    \/\/ The highest byte is the type of the argument. The remaining bytes contain the offset of the\n+    \/\/ field if it is flattened in the calling convention, -1 otherwise.\n+    juint _payload;\n+\n+    static constexpr int offset_bit_width = 24;\n+    static constexpr juint offset_bit_mask = (1 << offset_bit_width) - 1;\n+  public:\n+    Element(BasicType bt, int offset) : _payload((static_cast<juint>(bt) << offset_bit_width) | (juint(offset) & offset_bit_mask)) {\n+      assert(offset >= -1 && offset < jint(offset_bit_mask), \"invalid offset %d\", offset);\n+    }\n+\n+    BasicType bt() const {\n+      return static_cast<BasicType>(_payload >> offset_bit_width);\n+    }\n+\n+    int offset() const {\n+      juint res = _payload & offset_bit_mask;\n+      return res == offset_bit_mask ? -1 : res;\n+    }\n+\n+    juint hash() const {\n+      return _payload;\n+    }\n+\n+    bool operator!=(const Element& other) const {\n+      return _payload != other._payload;\n+    }\n@@ -2213,3 +2344,3 @@\n-  \/\/ TO DO:  Consider integrating this with a more global scheme for compressing signatures.\n-  \/\/ For now, 4 bits per components (plus T_VOID gaps after double\/long) is not excessive.\n-  int _length;\n+private:\n+  const bool _has_ro_adapter;\n+  const int _length;\n@@ -2219,2 +2350,8 @@\n-  int* data_pointer() {\n-    return (int*)((address)this + data_offset());\n+  Element* data_pointer() {\n+    return reinterpret_cast<Element*>(reinterpret_cast<address>(this) + data_offset());\n+  }\n+\n+  const Element& element_at(int index) {\n+    assert(index < length(), \"index %d out of bounds for length %d\", index, length());\n+    Element* data = data_pointer();\n+    return data[index];\n@@ -2224,6 +2361,5 @@\n-  AdapterFingerPrint(int total_args_passed, BasicType* sig_bt, int len) {\n-    int* data = data_pointer();\n-    \/\/ Pack the BasicTypes with 8 per int\n-    assert(len == length(total_args_passed), \"sanity\");\n-    _length = len;\n-    int sig_index = 0;\n+  AdapterFingerPrint(const GrowableArray<SigEntry>* sig, bool has_ro_adapter)\n+    : _has_ro_adapter(has_ro_adapter), _length(total_args_passed_in_sig(sig)) {\n+    Element* data = data_pointer();\n+    BasicType prev_bt = T_ILLEGAL;\n+    int vt_count = 0;\n@@ -2231,5 +2367,15 @@\n-      int value = 0;\n-      for (int byte = 0; sig_index < total_args_passed && byte < _basic_types_per_int; byte++) {\n-        int bt = adapter_encoding(sig_bt[sig_index++]);\n-        assert((bt & _basic_type_mask) == bt, \"must fit in 4 bits\");\n-        value = (value << _basic_type_bits) | bt;\n+      const SigEntry& sig_entry = sig->at(index);\n+      BasicType bt = sig_entry._bt;\n+      if (bt == T_METADATA) {\n+        \/\/ Found start of inline type in signature\n+        assert(InlineTypePassFieldsAsArgs, \"unexpected start of inline type\");\n+        vt_count++;\n+      } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+        \/\/ Found end of inline type in signature\n+        assert(InlineTypePassFieldsAsArgs, \"unexpected end of inline type\");\n+        vt_count--;\n+        assert(vt_count >= 0, \"invalid vt_count\");\n+      } else if (vt_count == 0) {\n+        \/\/ Widen fields that are not part of a scalarized inline type argument\n+        assert(sig_entry._offset == -1, \"invalid offset for argument that is not a flattened field %d\", sig_entry._offset);\n+        bt = adapter_encoding(bt);\n@@ -2237,1 +2383,3 @@\n-      data[index] = value;\n+\n+      ::new(&data[index]) Element(bt, sig_entry._offset);\n+      prev_bt = bt;\n@@ -2239,0 +2387,1 @@\n+    assert(vt_count == 0, \"invalid vt_count\");\n@@ -2246,2 +2395,2 @@\n-  static int length(int total_args) {\n-    return (total_args + (_basic_types_per_int-1)) \/ _basic_types_per_int;\n+  static int total_args_passed_in_sig(const GrowableArray<SigEntry>* sig) {\n+    return (sig != nullptr) ? sig->length() : 0;\n@@ -2251,1 +2400,1 @@\n-    return (int)heap_word_size(sizeof(AdapterFingerPrint) + (len * sizeof(int)));\n+    return (int)heap_word_size(sizeof(AdapterFingerPrint) + (len * sizeof(Element)));\n@@ -2257,1 +2406,1 @@\n-  static int adapter_encoding(BasicType in) {\n+  static BasicType adapter_encoding(BasicType in) {\n@@ -2263,1 +2412,1 @@\n-        \/\/ There are all promoted to T_INT in the calling convention\n+        \/\/ They are all promoted to T_INT in the calling convention\n@@ -2296,0 +2445,1 @@\n+public:\n@@ -2299,10 +2449,1 @@\n-      unsigned val = (unsigned)value(i);\n-      \/\/ args are packed so that first\/lower arguments are in the highest\n-      \/\/ bits of each int value, so iterate from highest to the lowest\n-      for (int j = 32 - _basic_type_bits; j >= 0; j -= _basic_type_bits) {\n-        unsigned v = (val >> j) & _basic_type_mask;\n-        if (v == 0) {\n-          continue;\n-        }\n-        function(v);\n-      }\n+      function(element_at(i));\n@@ -2312,3 +2453,2 @@\n- public:\n-  static AdapterFingerPrint* allocate(int total_args_passed, BasicType* sig_bt) {\n-    int len = length(total_args_passed);\n+  static AdapterFingerPrint* allocate(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false) {\n+    int len = total_args_passed_in_sig(sig);\n@@ -2316,1 +2456,1 @@\n-    AdapterFingerPrint* afp = new (size_in_bytes) AdapterFingerPrint(total_args_passed, sig_bt, len);\n+    AdapterFingerPrint* afp = new (size_in_bytes) AdapterFingerPrint(sig, has_ro_adapter);\n@@ -2325,3 +2465,2 @@\n-  int value(int index) {\n-    int* data = data_pointer();\n-    return data[index];\n+  bool has_ro_adapter() const {\n+    return _has_ro_adapter;\n@@ -2330,1 +2469,1 @@\n-  int length() {\n+  int length() const {\n@@ -2337,1 +2476,1 @@\n-      int v = value(i);\n+      const Element& v = element_at(i);\n@@ -2339,1 +2478,1 @@\n-      hash = ((hash << 8) ^ v ^ (hash >> 5)) + 3;\n+      hash = ((hash << 8) ^ v.hash() ^ (hash >> 5)) + 3;\n@@ -2346,1 +2485,6 @@\n-    st.print(\"0x\");\n+    st.print(\"{\");\n+    if (_has_ro_adapter) {\n+      st.print(\"has_ro_adapter\");\n+    } else {\n+      st.print(\"no_ro_adapter\");\n+    }\n@@ -2348,1 +2492,3 @@\n-      st.print(\"%x\", value(i));\n+      st.print(\", \");\n+      const Element& elem = element_at(i);\n+      st.print(\"{%s, %d}\", type2name(elem.bt()), elem.offset());\n@@ -2350,0 +2496,1 @@\n+    st.print(\"}\");\n@@ -2356,1 +2503,1 @@\n-    iterate_args([&] (int arg) {\n+    iterate_args([&] (const Element& arg) {\n@@ -2359,1 +2506,1 @@\n-        if (arg == T_VOID) {\n+        if (arg.bt() == T_VOID) {\n@@ -2365,7 +2512,4 @@\n-      switch (arg) {\n-        case T_INT:    st.print(\"I\");    break;\n-        case T_LONG:   long_prev = true; break;\n-        case T_FLOAT:  st.print(\"F\");    break;\n-        case T_DOUBLE: st.print(\"D\");    break;\n-        case T_VOID:   break;\n-        default: ShouldNotReachHere();\n+      if (arg.bt() == T_LONG) {\n+        long_prev = true;\n+      } else if (arg.bt() != T_VOID) {\n+        st.print(\"%c\", type2char(arg.bt()));\n@@ -2380,52 +2524,3 @@\n-  BasicType* as_basic_type(int& nargs) {\n-    nargs = 0;\n-    GrowableArray<BasicType> btarray;\n-    bool long_prev = false;\n-\n-    iterate_args([&] (int arg) {\n-      if (long_prev) {\n-        long_prev = false;\n-        if (arg == T_VOID) {\n-          btarray.append(T_LONG);\n-        } else {\n-          btarray.append(T_OBJECT); \/\/ it could be T_ARRAY; it shouldn't matter\n-        }\n-      }\n-      switch (arg) {\n-        case T_INT: \/\/ fallthrough\n-        case T_FLOAT: \/\/ fallthrough\n-        case T_DOUBLE:\n-        case T_VOID:\n-          btarray.append((BasicType)arg);\n-          break;\n-        case T_LONG:\n-          long_prev = true;\n-          break;\n-        default: ShouldNotReachHere();\n-      }\n-    });\n-\n-    if (long_prev) {\n-      btarray.append(T_OBJECT);\n-    }\n-\n-    nargs = btarray.length();\n-    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, nargs);\n-    int index = 0;\n-    GrowableArrayIterator<BasicType> iter = btarray.begin();\n-    while (iter != btarray.end()) {\n-      sig_bt[index++] = *iter;\n-      ++iter;\n-    }\n-    assert(index == btarray.length(), \"sanity check\");\n-#ifdef ASSERT\n-    {\n-      AdapterFingerPrint* compare_fp = AdapterFingerPrint::allocate(nargs, sig_bt);\n-      assert(this->equals(compare_fp), \"sanity check\");\n-      AdapterFingerPrint::deallocate(compare_fp);\n-    }\n-#endif\n-    return sig_bt;\n-  }\n-\n-    if (other->_length != _length) {\n+    if (other->_has_ro_adapter != _has_ro_adapter) {\n+      return false;\n+    } else if (other->_length != _length) {\n@@ -2436,1 +2531,1 @@\n-        if (value(i) != other->value(i)) {\n+        if (element_at(i) != other->element_at(i)) {\n@@ -2479,1 +2574,1 @@\n-AdapterHandlerEntry* AdapterHandlerLibrary::lookup(int total_args_passed, BasicType* sig_bt) {\n+AdapterHandlerEntry* AdapterHandlerLibrary::lookup(const GrowableArray<SigEntry>* sig, bool has_ro_adapter) {\n@@ -2482,1 +2577,1 @@\n-  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(total_args_passed, sig_bt);\n+  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(sig, has_ro_adapter);\n@@ -2539,1 +2634,1 @@\n-static const int AdapterHandlerLibrary_size = 16*K;\n+static const int AdapterHandlerLibrary_size = 48*K;\n@@ -2587,1 +2682,3 @@\n-    _no_arg_handler = create_adapter(0, nullptr);\n+    CompiledEntrySignature no_args;\n+    no_args.compute_calling_conventions();\n+    _no_arg_handler = create_adapter(no_args, true);\n@@ -2589,2 +2686,4 @@\n-    BasicType obj_args[] = { T_OBJECT };\n-    _obj_arg_handler = create_adapter(1, obj_args);\n+    CompiledEntrySignature obj_args;\n+    SigEntry::add_entry(obj_args.sig(), T_OBJECT);\n+    obj_args.compute_calling_conventions();\n+    _obj_arg_handler = create_adapter(obj_args, true);\n@@ -2592,2 +2691,4 @@\n-    BasicType int_args[] = { T_INT };\n-    _int_arg_handler = create_adapter(1, int_args);\n+    CompiledEntrySignature int_args;\n+    SigEntry::add_entry(int_args.sig(), T_INT);\n+    int_args.compute_calling_conventions();\n+    _int_arg_handler = create_adapter(int_args, true);\n@@ -2595,2 +2696,5 @@\n-    BasicType obj_int_args[] = { T_OBJECT, T_INT };\n-    _obj_int_arg_handler = create_adapter(2, obj_int_args);\n+    CompiledEntrySignature obj_int_args;\n+    SigEntry::add_entry(obj_int_args.sig(), T_OBJECT);\n+    SigEntry::add_entry(obj_int_args.sig(), T_INT);\n+    obj_int_args.compute_calling_conventions();\n+    _obj_int_arg_handler = create_adapter(obj_int_args, true);\n@@ -2598,2 +2702,5 @@\n-    BasicType obj_obj_args[] = { T_OBJECT, T_OBJECT };\n-    _obj_obj_arg_handler = create_adapter(2, obj_obj_args);\n+    CompiledEntrySignature obj_obj_args;\n+    SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+    SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+    obj_obj_args.compute_calling_conventions();\n+    _obj_obj_arg_handler = create_adapter(obj_obj_args, true);\n@@ -2633,0 +2740,3 @@\n+      if (InlineTypePassFieldsAsArgs && method->method_holder()->is_inline_klass()) {\n+        return nullptr;\n+      }\n@@ -2636,1 +2746,10 @@\n-      case JVM_SIGNATURE_CLASS:\n+      case JVM_SIGNATURE_CLASS: {\n+        if (InlineTypePassFieldsAsArgs) {\n+          SignatureStream ss(method->signature());\n+          InlineKlass* vk = ss.as_inline_klass(method->method_holder());\n+          if (vk != nullptr) {\n+            return nullptr;\n+          }\n+        }\n+        return _obj_arg_handler;\n+      }\n@@ -2647,1 +2766,1 @@\n-             !method->is_static()) {\n+             !method->is_static() && (!InlineTypePassFieldsAsArgs || !method->method_holder()->is_inline_klass())) {\n@@ -2649,1 +2768,10 @@\n-      case JVM_SIGNATURE_CLASS:\n+      case JVM_SIGNATURE_CLASS: {\n+        if (InlineTypePassFieldsAsArgs) {\n+          SignatureStream ss(method->signature());\n+          InlineKlass* vk = ss.as_inline_klass(method->method_holder());\n+          if (vk != nullptr) {\n+            return nullptr;\n+          }\n+        }\n+        return _obj_obj_arg_handler;\n+      }\n@@ -2663,5 +2791,9 @@\n-class AdapterSignatureIterator : public SignatureIterator {\n- private:\n-  BasicType stack_sig_bt[16];\n-  BasicType* sig_bt;\n-  int index;\n+CompiledEntrySignature::CompiledEntrySignature(Method* method) :\n+  _method(method), _num_inline_args(0), _has_inline_recv(false),\n+  _regs(nullptr), _regs_cc(nullptr), _regs_cc_ro(nullptr),\n+  _args_on_stack(0), _args_on_stack_cc(0), _args_on_stack_cc_ro(0),\n+  _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _supers(nullptr) {\n+  _sig = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+  _sig_cc = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+  _sig_cc_ro = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+}\n@@ -2669,11 +2801,24 @@\n- public:\n-  AdapterSignatureIterator(Symbol* signature,\n-                           fingerprint_t fingerprint,\n-                           bool is_static,\n-                           int total_args_passed) :\n-    SignatureIterator(signature, fingerprint),\n-    index(0)\n-  {\n-    sig_bt = (total_args_passed <= 16) ? stack_sig_bt : NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n-    if (!is_static) { \/\/ Pass in receiver first\n-      sig_bt[index++] = T_OBJECT;\n+\/\/ See if we can save space by sharing the same entry for VIEP and VIEP(RO),\n+\/\/ or the same entry for VEP and VIEP(RO).\n+CodeOffsets::Entries CompiledEntrySignature::c1_inline_ro_entry_type() const {\n+  if (!has_scalarized_args()) {\n+    \/\/ VEP\/VIEP\/VIEP(RO) all share the same entry. There's no packing.\n+    return CodeOffsets::Verified_Entry;\n+  }\n+  if (_method->is_static()) {\n+    \/\/ Static methods don't need VIEP(RO)\n+    return CodeOffsets::Verified_Entry;\n+  }\n+\n+  if (has_inline_recv()) {\n+    if (num_inline_args() == 1) {\n+      \/\/ Share same entry for VIEP and VIEP(RO).\n+      \/\/ This is quite common: we have an instance method in an InlineKlass that has\n+      \/\/ no inline type args other than <this>.\n+      return CodeOffsets::Verified_Inline_Entry;\n+    } else {\n+      assert(num_inline_args() > 1, \"must be\");\n+      \/\/ No sharing:\n+      \/\/   VIEP(RO) -- <this> is passed as object\n+      \/\/   VEP      -- <this> is passed as fields\n+      return CodeOffsets::Verified_Inline_Entry_RO;\n@@ -2681,1 +2826,0 @@\n-    do_parameters_on(this);\n@@ -2684,2 +2828,51 @@\n-  BasicType* basic_types() {\n-    return sig_bt;\n+  \/\/ Either a static method, or <this> is not an inline type\n+  if (args_on_stack_cc() != args_on_stack_cc_ro()) {\n+    \/\/ No sharing:\n+    \/\/ Some arguments are passed on the stack, and we have inserted reserved entries\n+    \/\/ into the VEP, but we never insert reserved entries into the VIEP(RO).\n+    return CodeOffsets::Verified_Inline_Entry_RO;\n+  } else {\n+    \/\/ Share same entry for VEP and VIEP(RO).\n+    return CodeOffsets::Verified_Entry;\n+  }\n+}\n+\n+\/\/ Returns all super methods (transitive) in classes and interfaces that are overridden by the current method.\n+GrowableArray<Method*>* CompiledEntrySignature::get_supers() {\n+  if (_supers != nullptr) {\n+    return _supers;\n+  }\n+  _supers = new GrowableArray<Method*>();\n+  \/\/ Skip private, static, and <init> methods\n+  if (_method->is_private() || _method->is_static() || _method->is_object_constructor()) {\n+    return _supers;\n+  }\n+  Symbol* name = _method->name();\n+  Symbol* signature = _method->signature();\n+  const Klass* holder = _method->method_holder()->super();\n+  Symbol* holder_name = holder->name();\n+  ThreadInVMfromUnknown tiv;\n+  JavaThread* current = JavaThread::current();\n+  HandleMark hm(current);\n+  Handle loader(current, _method->method_holder()->class_loader());\n+\n+  \/\/ Walk up the class hierarchy and search for super methods\n+  while (holder != nullptr) {\n+    Method* super_method = holder->lookup_method(name, signature);\n+    if (super_method == nullptr) {\n+      break;\n+    }\n+    if (!super_method->is_static() && !super_method->is_private() &&\n+        (!super_method->is_package_private() ||\n+         super_method->method_holder()->is_same_class_package(loader(), holder_name))) {\n+      _supers->push(super_method);\n+    }\n+    holder = super_method->method_holder()->super();\n+  }\n+  \/\/ Search interfaces for super methods\n+  Array<InstanceKlass*>* interfaces = _method->method_holder()->transitive_interfaces();\n+  for (int i = 0; i < interfaces->length(); ++i) {\n+    Method* m = interfaces->at(i)->lookup_method(name, signature);\n+    if (m != nullptr && !m->is_static() && m->is_public()) {\n+      _supers->push(m);\n+    }\n@@ -2687,0 +2880,2 @@\n+  return _supers;\n+}\n@@ -2688,0 +2883,38 @@\n+\/\/ Iterate over arguments and compute scalarized and non-scalarized signatures\n+void CompiledEntrySignature::compute_calling_conventions(bool init) {\n+  bool has_scalarized = false;\n+  if (_method != nullptr) {\n+    InstanceKlass* holder = _method->method_holder();\n+    int arg_num = 0;\n+    if (!_method->is_static()) {\n+      \/\/ We shouldn't scalarize 'this' in a value class constructor\n+      if (holder->is_inline_klass() && InlineKlass::cast(holder)->can_be_passed_as_fields() && !_method->is_object_constructor() &&\n+          (init || _method->is_scalarized_arg(arg_num))) {\n+        _sig_cc->appendAll(InlineKlass::cast(holder)->extended_sig());\n+        has_scalarized = true;\n+        _has_inline_recv = true;\n+        _num_inline_args++;\n+      } else {\n+        SigEntry::add_entry(_sig_cc, T_OBJECT, holder->name());\n+      }\n+      SigEntry::add_entry(_sig, T_OBJECT, holder->name());\n+      SigEntry::add_entry(_sig_cc_ro, T_OBJECT, holder->name());\n+      arg_num++;\n+    }\n+    for (SignatureStream ss(_method->signature()); !ss.at_return_type(); ss.next()) {\n+      BasicType bt = ss.type();\n+      if (bt == T_OBJECT) {\n+        InlineKlass* vk = ss.as_inline_klass(holder);\n+        if (vk != nullptr && vk->can_be_passed_as_fields() && (init || _method->is_scalarized_arg(arg_num))) {\n+          \/\/ Check for a calling convention mismatch with super method(s)\n+          bool scalar_super = false;\n+          bool non_scalar_super = false;\n+          GrowableArray<Method*>* supers = get_supers();\n+          for (int i = 0; i < supers->length(); ++i) {\n+            Method* super_method = supers->at(i);\n+            if (super_method->is_scalarized_arg(arg_num)) {\n+              scalar_super = true;\n+            } else {\n+              non_scalar_super = true;\n+            }\n+          }\n@@ -2689,3 +2922,8 @@\n-  int slots() {\n-    return index;\n-  }\n+          \/\/ Randomly enable below code paths for stress testing\n+          bool stress = init && StressCallingConvention;\n+          if (stress && (os::random() & 1) == 1) {\n+            non_scalar_super = true;\n+            if ((os::random() & 1) == 1) {\n+              scalar_super = true;\n+            }\n+          }\n@@ -2693,0 +2931,50 @@\n+          if (non_scalar_super) {\n+            \/\/ Found a super method with a non-scalarized argument. Fall back to the non-scalarized calling convention.\n+            if (scalar_super) {\n+              \/\/ Found non-scalar *and* scalar super methods. We can't handle both.\n+              \/\/ Mark the scalar method as mismatch and re-compile call sites to use non-scalarized calling convention.\n+              for (int i = 0; i < supers->length(); ++i) {\n+                Method* super_method = supers->at(i);\n+                if (super_method->is_scalarized_arg(arg_num) DEBUG_ONLY(|| (stress && (os::random() & 1) == 1))) {\n+                  super_method->set_mismatch();\n+                  MutexLocker ml(Compile_lock, Mutex::_safepoint_check_flag);\n+                  JavaThread* thread = JavaThread::current();\n+                  HandleMark hm(thread);\n+                  methodHandle mh(thread, super_method);\n+                  DeoptimizationScope deopt_scope;\n+                  CodeCache::mark_for_deoptimization(&deopt_scope, mh());\n+                  deopt_scope.deoptimize_marked();\n+                }\n+              }\n+            }\n+            \/\/ Fall back to non-scalarized calling convention\n+            SigEntry::add_entry(_sig_cc, T_OBJECT, ss.as_symbol());\n+            SigEntry::add_entry(_sig_cc_ro, T_OBJECT, ss.as_symbol());\n+          } else {\n+            _num_inline_args++;\n+            has_scalarized = true;\n+            int last = _sig_cc->length();\n+            int last_ro = _sig_cc_ro->length();\n+            _sig_cc->appendAll(vk->extended_sig());\n+            _sig_cc_ro->appendAll(vk->extended_sig());\n+            if (bt == T_OBJECT) {\n+              \/\/ Nullable inline type argument, insert InlineTypeNode::NullMarker field right after T_METADATA delimiter\n+              _sig_cc->insert_before(last+1, SigEntry(T_BOOLEAN, -1, nullptr, true));\n+              _sig_cc_ro->insert_before(last_ro+1, SigEntry(T_BOOLEAN, -1, nullptr, true));\n+            }\n+          }\n+        } else {\n+          SigEntry::add_entry(_sig_cc, T_OBJECT, ss.as_symbol());\n+          SigEntry::add_entry(_sig_cc_ro, T_OBJECT, ss.as_symbol());\n+        }\n+        bt = T_OBJECT;\n+      } else {\n+        SigEntry::add_entry(_sig_cc, ss.type(), ss.as_symbol());\n+        SigEntry::add_entry(_sig_cc_ro, ss.type(), ss.as_symbol());\n+      }\n+      SigEntry::add_entry(_sig, bt, ss.as_symbol());\n+      if (bt != T_VOID) {\n+        arg_num++;\n+      }\n+    }\n+  }\n@@ -2694,1 +2982,11 @@\n- private:\n+  \/\/ Compute the non-scalarized calling convention\n+  _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig->length());\n+  _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);\n+\n+  \/\/ Compute the scalarized calling conventions if there are scalarized inline types in the signature\n+  if (has_scalarized && !_method->is_native()) {\n+    _regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc->length());\n+    _args_on_stack_cc = SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);\n+\n+    _regs_cc_ro = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc_ro->length());\n+    _args_on_stack_cc_ro = SharedRuntime::java_calling_convention(_sig_cc_ro, _regs_cc_ro);\n@@ -2696,5 +2994,8 @@\n-  friend class SignatureIterator;  \/\/ so do_parameters_on can call do_type\n-  void do_type(BasicType type) {\n-    sig_bt[index++] = type;\n-    if (type == T_LONG || type == T_DOUBLE) {\n-      sig_bt[index++] = T_VOID; \/\/ Longs & doubles take 2 Java slots\n+    _c1_needs_stack_repair = (_args_on_stack_cc < _args_on_stack) || (_args_on_stack_cc_ro < _args_on_stack);\n+    _c2_needs_stack_repair = (_args_on_stack_cc > _args_on_stack) || (_args_on_stack_cc > _args_on_stack_cc_ro);\n+\n+    \/\/ Upper bound on stack arguments to avoid hitting the argument limit and\n+    \/\/ bailing out of compilation (\"unsupported incoming calling sequence\").\n+    \/\/ TODO we need a reasonable limit (flag?) here\n+    if (MAX2(_args_on_stack_cc, _args_on_stack_cc_ro) <= 60) {\n+      return; \/\/ Success\n@@ -2703,1 +3004,130 @@\n-};\n+  \/\/ No scalarized args\n+  _sig_cc = _sig;\n+  _regs_cc = _regs;\n+  _args_on_stack_cc = _args_on_stack;\n+\n+  _sig_cc_ro = _sig;\n+  _regs_cc_ro = _regs;\n+  _args_on_stack_cc_ro = _args_on_stack;\n+}\n+\n+void CompiledEntrySignature::initialize_from_fingerprint(AdapterFingerPrint* fingerprint) {\n+  _has_inline_recv = fingerprint->has_ro_adapter();\n+\n+  int value_object_count = 0;\n+  BasicType prev_bt = T_ILLEGAL;\n+  bool has_scalarized_arguments = false;\n+  bool long_prev = false;\n+  int long_prev_offset = -1;\n+\n+  fingerprint->iterate_args([&] (const AdapterFingerPrint::Element& arg) {\n+    BasicType bt = arg.bt();\n+    int offset = arg.offset();\n+\n+    if (long_prev) {\n+      long_prev = false;\n+      BasicType bt_to_add;\n+      if (bt == T_VOID) {\n+        bt_to_add = T_LONG;\n+      } else {\n+        bt_to_add = T_OBJECT;\n+      }\n+      if (value_object_count == 0) {\n+        SigEntry::add_entry(_sig, bt_to_add);\n+      }\n+      SigEntry::add_entry(_sig_cc, bt_to_add, nullptr, long_prev_offset);\n+      SigEntry::add_entry(_sig_cc_ro, bt_to_add, nullptr, long_prev_offset);\n+    }\n+\n+    switch (bt) {\n+      case T_VOID:\n+        if (prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+          assert(InlineTypePassFieldsAsArgs, \"unexpected end of inline type\");\n+          value_object_count--;\n+          SigEntry::add_entry(_sig_cc, T_VOID, nullptr, offset);\n+          SigEntry::add_entry(_sig_cc_ro, T_VOID, nullptr, offset);\n+          assert(value_object_count >= 0, \"invalid value object count\");\n+        } else {\n+          \/\/ Nothing to add for _sig: We already added an addition T_VOID in add_entry() when adding T_LONG or T_DOUBLE.\n+        }\n+        break;\n+      case T_INT:\n+      case T_FLOAT:\n+      case T_DOUBLE:\n+        if (value_object_count == 0) {\n+          SigEntry::add_entry(_sig, bt);\n+        }\n+        SigEntry::add_entry(_sig_cc, bt, nullptr, offset);\n+        SigEntry::add_entry(_sig_cc_ro, bt, nullptr, offset);\n+        break;\n+      case T_LONG:\n+        long_prev = true;\n+        long_prev_offset = offset;\n+        break;\n+      case T_BOOLEAN:\n+      case T_CHAR:\n+      case T_BYTE:\n+      case T_SHORT:\n+      case T_OBJECT:\n+      case T_ARRAY:\n+        assert(value_object_count > 0, \"must be value object field\");\n+        SigEntry::add_entry(_sig_cc, bt, nullptr, offset);\n+        SigEntry::add_entry(_sig_cc_ro, bt, nullptr, offset);\n+        break;\n+      case T_METADATA:\n+        assert(InlineTypePassFieldsAsArgs, \"unexpected start of inline type\");\n+        if (value_object_count == 0) {\n+          SigEntry::add_entry(_sig, T_OBJECT);\n+        }\n+        SigEntry::add_entry(_sig_cc, T_METADATA, nullptr, offset);\n+        SigEntry::add_entry(_sig_cc_ro, T_METADATA, nullptr, offset);\n+        value_object_count++;\n+        has_scalarized_arguments = true;\n+        break;\n+      default: {\n+        fatal(\"Unexpected BasicType: %s\", basictype_to_str(bt));\n+      }\n+    }\n+    prev_bt = bt;\n+  });\n+\n+  if (long_prev) {\n+    \/\/ If previous bt was T_LONG and we reached the end of the signature, we know that it must be a T_OBJECT.\n+    SigEntry::add_entry(_sig, T_OBJECT);\n+    SigEntry::add_entry(_sig_cc, T_OBJECT);\n+    SigEntry::add_entry(_sig_cc_ro, T_OBJECT);\n+  }\n+  assert(value_object_count == 0, \"invalid value object count\");\n+\n+  _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig->length());\n+  _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);\n+\n+  \/\/ Compute the scalarized calling conventions if there are scalarized inline types in the signature\n+  if (has_scalarized_arguments) {\n+    _regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc->length());\n+    _args_on_stack_cc = SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);\n+\n+    _regs_cc_ro = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc_ro->length());\n+    _args_on_stack_cc_ro = SharedRuntime::java_calling_convention(_sig_cc_ro, _regs_cc_ro);\n+\n+    _c1_needs_stack_repair = (_args_on_stack_cc < _args_on_stack) || (_args_on_stack_cc_ro < _args_on_stack);\n+    _c2_needs_stack_repair = (_args_on_stack_cc > _args_on_stack) || (_args_on_stack_cc > _args_on_stack_cc_ro);\n+  } else {\n+    \/\/ No scalarized args\n+    _sig_cc = _sig;\n+    _regs_cc = _regs;\n+    _args_on_stack_cc = _args_on_stack;\n+\n+    _sig_cc_ro = _sig;\n+    _regs_cc_ro = _regs;\n+    _args_on_stack_cc_ro = _args_on_stack;\n+  }\n+\n+#ifdef ASSERT\n+  {\n+    AdapterFingerPrint* compare_fp = AdapterFingerPrint::allocate(_sig_cc, _has_inline_recv);\n+    assert(fingerprint->equals(compare_fp), \"%s - %s\", fingerprint->as_string(), compare_fp->as_string());\n+    AdapterFingerPrint::deallocate(compare_fp);\n+  }\n+#endif\n+}\n@@ -2711,1 +3141,1 @@\n-void AdapterHandlerLibrary::verify_adapter_sharing(int total_args_passed, BasicType* sig_bt, AdapterHandlerEntry* cached_entry) {\n+void AdapterHandlerLibrary::verify_adapter_sharing(CompiledEntrySignature& ces, AdapterHandlerEntry* cached_entry) {\n@@ -2714,1 +3144,1 @@\n-  AdapterHandlerEntry* comparison_entry = create_adapter(total_args_passed, sig_bt, true);\n+  AdapterHandlerEntry* comparison_entry = create_adapter(ces, false, true);\n@@ -2739,2 +3169,13 @@\n-  \/\/ Fill in the signature array, for the calling-convention call.\n-  int total_args_passed = method->size_of_parameters(); \/\/ All args on stack\n+  CompiledEntrySignature ces(method());\n+  ces.compute_calling_conventions();\n+  if (ces.has_scalarized_args()) {\n+    if (!method->has_scalarized_args()) {\n+      method->set_has_scalarized_args();\n+    }\n+    if (ces.c1_needs_stack_repair()) {\n+      method->set_c1_needs_stack_repair();\n+    }\n+    if (ces.c2_needs_stack_repair() && !method->c2_needs_stack_repair()) {\n+      method->set_c2_needs_stack_repair();\n+    }\n+  }\n@@ -2742,4 +3183,0 @@\n-  AdapterSignatureIterator si(method->signature(), method->constMethod()->fingerprint(),\n-                              method->is_static(), total_args_passed);\n-  assert(si.slots() == total_args_passed, \"\");\n-  BasicType* sig_bt = si.basic_types();\n@@ -2750,1 +3187,1 @@\n-    entry = lookup(total_args_passed, sig_bt);\n+    entry = lookup(ces.sig_cc(), ces.has_inline_recv());\n@@ -2758,1 +3195,1 @@\n-        verify_adapter_sharing(total_args_passed, sig_bt, entry);\n+        verify_adapter_sharing(ces, entry);\n@@ -2762,1 +3199,1 @@\n-      entry = create_adapter(total_args_passed, sig_bt);\n+      entry = create_adapter(ces, \/* allocate_code_blob *\/ true);\n@@ -2817,0 +3254,2 @@\n+  entry_offset[AdapterBlob::C2I_Inline] = entry_address[AdapterBlob::C2I_Inline] - entry_address[AdapterBlob::I2C];\n+  entry_offset[AdapterBlob::C2I_Inline_RO] = entry_address[AdapterBlob::C2I_Inline_RO] - entry_address[AdapterBlob::I2C];\n@@ -2818,0 +3257,1 @@\n+  entry_offset[AdapterBlob::C2I_Unverified_Inline] = entry_address[AdapterBlob::C2I_Unverified_Inline] - entry_address[AdapterBlob::I2C];\n@@ -2826,2 +3266,2 @@\n-                                                  int total_args_passed,\n-                                                  BasicType* sig_bt,\n+                                                  CompiledEntrySignature& ces,\n+                                                  bool allocate_code_blob,\n@@ -2834,0 +3274,1 @@\n+  AdapterBlob* adapter_blob = nullptr;\n@@ -2840,2 +3281,1 @@\n-  VMRegPair stack_regs[16];\n-  VMRegPair* regs = (total_args_passed <= 16) ? stack_regs : NEW_RESOURCE_ARRAY(VMRegPair, total_args_passed);\n+  address entry_address[AdapterBlob::ENTRY_COUNT];\n@@ -2844,7 +3284,17 @@\n-  int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed);\n-  address entry_address[AdapterBlob::ENTRY_COUNT];\n-                                         total_args_passed,\n-                                         comp_args_on_stack,\n-                                         sig_bt,\n-                                         regs,\n-                                         entry_address);\n+                                         ces.args_on_stack(),\n+                                         ces.sig(),\n+                                         ces.regs(),\n+                                         ces.sig_cc(),\n+                                         ces.regs_cc(),\n+                                         ces.sig_cc_ro(),\n+                                         ces.regs_cc_ro(),\n+                                         entry_address,\n+                                         adapter_blob,\n+                                         allocate_code_blob);\n+\n+  if (ces.has_scalarized_args()) {\n+    \/\/ Save a C heap allocated version of the scalarized signature and store it in the adapter\n+    GrowableArray<SigEntry>* heap_sig = new (mtInternal) GrowableArray<SigEntry>(ces.sig_cc()->length(), mtInternal);\n+    heap_sig->appendAll(ces.sig_cc());\n+    handler->set_sig_cc(heap_sig);\n+  }\n@@ -2864,1 +3314,0 @@\n-  AdapterBlob* adapter_blob = AdapterBlob::create(&buffer, entry_offset);\n@@ -2891,2 +3340,2 @@\n-AdapterHandlerEntry* AdapterHandlerLibrary::create_adapter(int total_args_passed,\n-                                                           BasicType* sig_bt,\n+AdapterHandlerEntry* AdapterHandlerLibrary::create_adapter(CompiledEntrySignature& ces,\n+                                                           bool allocate_code_blob,\n@@ -2894,1 +3343,6 @@\n-  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(total_args_passed, sig_bt);\n+  AdapterFingerPrint* fp = AdapterFingerPrint::allocate(ces.sig_cc(), ces.has_inline_recv());\n+#ifdef ASSERT\n+  \/\/ Verify that we can successfully restore the compiled entry signature object.\n+  CompiledEntrySignature ces_verify;\n+  ces_verify.initialize_from_fingerprint(fp);\n+#endif\n@@ -2896,1 +3350,1 @@\n-  if (!generate_adapter_code(handler, total_args_passed, sig_bt, is_transient)) {\n+  if (!generate_adapter_code(handler, ces, allocate_code_blob, is_transient)) {\n@@ -2915,0 +3369,1 @@\n+   _sig_cc = nullptr;\n@@ -2995,0 +3450,14 @@\n+\n+    if (get_sig_cc() == nullptr) {\n+      \/\/ Calling conventions have to be regenerated at runtime and are accessed through method adapters,\n+      \/\/ which are archived in the AOT code cache. If the adapters are not regenerated, the\n+      \/\/ calling conventions should be regenerated here.\n+      CompiledEntrySignature ces;\n+      ces.initialize_from_fingerprint(_fingerprint);\n+      if (ces.has_scalarized_args()) {\n+        \/\/ Save a C heap allocated version of the scalarized signature and store it in the adapter\n+        GrowableArray<SigEntry>* heap_sig = new (mtInternal) GrowableArray<SigEntry>(ces.sig_cc()->length(), mtInternal);\n+        heap_sig->appendAll(ces.sig_cc());\n+        set_sig_cc(heap_sig);\n+      }\n+    }\n@@ -2998,0 +3467,1 @@\n+\n@@ -2999,3 +3469,3 @@\n-    int nargs;\n-    BasicType* bt = _fingerprint->as_basic_type(nargs);\n-    if (!AdapterHandlerLibrary::generate_adapter_code(this, nargs, bt, \/* is_transient *\/ false)) {\n+    CompiledEntrySignature ces;\n+    ces.initialize_from_fingerprint(_fingerprint);\n+    if (!AdapterHandlerLibrary::generate_adapter_code(this, ces, true, false)) {\n@@ -3039,13 +3509,26 @@\n-  _no_arg_handler = lookup(0, nullptr);\n-\n-  BasicType obj_args[] = { T_OBJECT };\n-  _obj_arg_handler = lookup(1, obj_args);\n-\n-  BasicType int_args[] = { T_INT };\n-  _int_arg_handler = lookup(1, int_args);\n-\n-  BasicType obj_int_args[] = { T_OBJECT, T_INT };\n-  _obj_int_arg_handler = lookup(2, obj_int_args);\n-\n-  BasicType obj_obj_args[] = { T_OBJECT, T_OBJECT };\n-  _obj_obj_arg_handler = lookup(2, obj_obj_args);\n+  ResourceMark rm;\n+  CompiledEntrySignature no_args;\n+  no_args.compute_calling_conventions();\n+  _no_arg_handler = lookup(no_args.sig_cc(), no_args.has_inline_recv());\n+\n+  CompiledEntrySignature obj_args;\n+  SigEntry::add_entry(obj_args.sig(), T_OBJECT);\n+  obj_args.compute_calling_conventions();\n+  _obj_arg_handler = lookup(obj_args.sig_cc(), obj_args.has_inline_recv());\n+\n+  CompiledEntrySignature int_args;\n+  SigEntry::add_entry(int_args.sig(), T_INT);\n+  int_args.compute_calling_conventions();\n+  _int_arg_handler = lookup(int_args.sig_cc(), int_args.has_inline_recv());\n+\n+  CompiledEntrySignature obj_int_args;\n+  SigEntry::add_entry(obj_int_args.sig(), T_OBJECT);\n+  SigEntry::add_entry(obj_int_args.sig(), T_INT);\n+  obj_int_args.compute_calling_conventions();\n+  _obj_int_arg_handler = lookup(obj_int_args.sig_cc(), obj_int_args.has_inline_recv());\n+\n+  CompiledEntrySignature obj_obj_args;\n+  SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+  SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+  obj_obj_args.compute_calling_conventions();\n+  _obj_obj_arg_handler = lookup(obj_obj_args.sig_cc(), obj_obj_args.has_inline_recv());\n@@ -3080,0 +3563,3 @@\n+  if (_sig_cc != nullptr) {\n+    delete _sig_cc;\n+  }\n@@ -3167,0 +3653,1 @@\n+      BasicType stack_sig_bt[16];\n@@ -3168,0 +3655,1 @@\n+      BasicType* sig_bt = (total_args_passed <= 16) ? stack_sig_bt : NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n@@ -3170,5 +3658,13 @@\n-      AdapterSignatureIterator si(method->signature(), method->constMethod()->fingerprint(),\n-                              method->is_static(), total_args_passed);\n-      BasicType* sig_bt = si.basic_types();\n-      assert(si.slots() == total_args_passed, \"\");\n-      BasicType ret_type = si.return_type();\n+      int i = 0;\n+      if (!method->is_static()) {  \/\/ Pass in receiver first\n+        sig_bt[i++] = T_OBJECT;\n+      }\n+      SignatureStream ss(method->signature());\n+      for (; !ss.at_return_type(); ss.next()) {\n+        sig_bt[i++] = ss.type();  \/\/ Collect remaining bits of signature\n+        if (ss.type() == T_LONG || ss.type() == T_DOUBLE) {\n+          sig_bt[i++] = T_VOID;   \/\/ Longs & doubles take 2 Java slots\n+        }\n+      }\n+      assert(i == total_args_passed, \"\");\n+      BasicType ret_type = ss.type();\n@@ -3422,1 +3918,4 @@\n-    st->print(\" c2iUV: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+    st->print(\" c2iVE: \" INTPTR_FORMAT, p2i(get_c2i_inline_entry()));\n+    st->print(\" c2iVROE: \" INTPTR_FORMAT, p2i(get_c2i_inline_ro_entry()));\n+    st->print(\" c2iUE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+    st->print(\" c2iUVE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_inline_entry()));\n@@ -3509,0 +4008,195 @@\n+\n+\/\/ We are at a compiled code to interpreter call. We need backing\n+\/\/ buffers for all inline type arguments. Allocate an object array to\n+\/\/ hold them (convenient because once we're done with it we don't have\n+\/\/ to worry about freeing it).\n+oop SharedRuntime::allocate_inline_types_impl(JavaThread* current, methodHandle callee, bool allocate_receiver, TRAPS) {\n+  assert(InlineTypePassFieldsAsArgs, \"no reason to call this\");\n+  ResourceMark rm;\n+\n+  int nb_slots = 0;\n+  InstanceKlass* holder = callee->method_holder();\n+  allocate_receiver &= !callee->is_static() && holder->is_inline_klass() && callee->is_scalarized_arg(0);\n+  if (allocate_receiver) {\n+    nb_slots++;\n+  }\n+  int arg_num = callee->is_static() ? 0 : 1;\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    BasicType bt = ss.type();\n+    if (bt == T_OBJECT && callee->is_scalarized_arg(arg_num)) {\n+      nb_slots++;\n+    }\n+    if (bt != T_VOID) {\n+      arg_num++;\n+    }\n+  }\n+  objArrayOop array_oop = oopFactory::new_objectArray(nb_slots, CHECK_NULL);\n+  objArrayHandle array(THREAD, array_oop);\n+  arg_num = callee->is_static() ? 0 : 1;\n+  int i = 0;\n+  if (allocate_receiver) {\n+    InlineKlass* vk = InlineKlass::cast(holder);\n+    oop res = vk->allocate_instance(CHECK_NULL);\n+    array->obj_at_put(i++, res);\n+  }\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    BasicType bt = ss.type();\n+    if (bt == T_OBJECT && callee->is_scalarized_arg(arg_num)) {\n+      InlineKlass* vk = ss.as_inline_klass(holder);\n+      assert(vk != nullptr, \"Unexpected klass\");\n+      oop res = vk->allocate_instance(CHECK_NULL);\n+      array->obj_at_put(i++, res);\n+    }\n+    if (bt != T_VOID) {\n+      arg_num++;\n+    }\n+  }\n+  return array();\n+}\n+\n+JRT_ENTRY(void, SharedRuntime::allocate_inline_types(JavaThread* current, Method* callee_method, bool allocate_receiver))\n+  methodHandle callee(current, callee_method);\n+  oop array = SharedRuntime::allocate_inline_types_impl(current, callee, allocate_receiver, CHECK);\n+  current->set_vm_result_oop(array);\n+  current->set_vm_result_metadata(callee()); \/\/ TODO: required to keep callee live?\n+JRT_END\n+\n+\/\/ We're returning from an interpreted method: load each field into a\n+\/\/ register following the calling convention\n+JRT_LEAF(void, SharedRuntime::load_inline_type_fields_in_regs(JavaThread* current, oopDesc* res))\n+{\n+  assert(res->klass()->is_inline_klass(), \"only inline types here\");\n+  ResourceMark rm;\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  frame stubFrame = current->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+  assert(callerFrame.is_interpreted_frame(), \"should be coming from interpreter\");\n+\n+  InlineKlass* vk = InlineKlass::cast(res->klass());\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  if (regs == nullptr) {\n+    \/\/ The fields of the inline klass don't fit in registers, bail out\n+    return;\n+  }\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    address loc = reg_map.location(pair.first(), nullptr);\n+    switch(bt) {\n+    case T_BOOLEAN:\n+      *(jboolean*)loc = res->bool_field(off);\n+      break;\n+    case T_CHAR:\n+      *(jchar*)loc = res->char_field(off);\n+      break;\n+    case T_BYTE:\n+      *(jbyte*)loc = res->byte_field(off);\n+      break;\n+    case T_SHORT:\n+      *(jshort*)loc = res->short_field(off);\n+      break;\n+    case T_INT: {\n+      *(jint*)loc = res->int_field(off);\n+      break;\n+    }\n+    case T_LONG:\n+#ifdef _LP64\n+      *(intptr_t*)loc = res->long_field(off);\n+#else\n+      Unimplemented();\n+#endif\n+      break;\n+    case T_OBJECT:\n+    case T_ARRAY: {\n+      *(oop*)loc = res->obj_field(off);\n+      break;\n+    }\n+    case T_FLOAT:\n+      *(jfloat*)loc = res->float_field(off);\n+      break;\n+    case T_DOUBLE:\n+      *(jdouble*)loc = res->double_field(off);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+#ifdef ASSERT\n+  VMRegPair pair = regs->at(0);\n+  address loc = reg_map.location(pair.first(), nullptr);\n+  assert(*(oopDesc**)loc == res, \"overwritten object\");\n+#endif\n+\n+  current->set_vm_result_oop(res);\n+}\n+JRT_END\n+\n+\/\/ We've returned to an interpreted method, the interpreter needs a\n+\/\/ reference to an inline type instance. Allocate it and initialize it\n+\/\/ from field's values in registers.\n+JRT_BLOCK_ENTRY(void, SharedRuntime::store_inline_type_fields_to_buf(JavaThread* current, intptr_t res))\n+{\n+  ResourceMark rm;\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  frame stubFrame = current->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+\n+#ifdef ASSERT\n+  InlineKlass* verif_vk = InlineKlass::returned_inline_klass(reg_map);\n+#endif\n+\n+  if (!is_set_nth_bit(res, 0)) {\n+    \/\/ We're not returning with inline type fields in registers (the\n+    \/\/ calling convention didn't allow it for this inline klass)\n+    assert(!Metaspace::contains((void*)res), \"should be oop or pointer in buffer area\");\n+    current->set_vm_result_oop((oopDesc*)res);\n+    assert(verif_vk == nullptr, \"broken calling convention\");\n+    return;\n+  }\n+\n+  clear_nth_bit(res, 0);\n+  InlineKlass* vk = (InlineKlass*)res;\n+  assert(verif_vk == vk, \"broken calling convention\");\n+  assert(Metaspace::contains((void*)res), \"should be klass\");\n+\n+  \/\/ Allocate handles for every oop field so they are safe in case of\n+  \/\/ a safepoint when allocating\n+  GrowableArray<Handle> handles;\n+  vk->save_oop_fields(reg_map, handles);\n+\n+  \/\/ It's unsafe to safepoint until we are here\n+  JRT_BLOCK;\n+  {\n+    JavaThread* THREAD = current;\n+    oop vt = vk->realloc_result(reg_map, handles, CHECK);\n+    current->set_vm_result_oop(vt);\n+  }\n+  JRT_BLOCK_END;\n+}\n+JRT_END\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":951,"deletions":257,"binary":false,"changes":1208,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -39,0 +40,1 @@\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n@@ -50,1 +52,1 @@\n-\/\/ FieldType  = \"B\" | \"C\" | \"D\" | \"F\" | \"I\" | \"J\" | \"S\" | \"Z\" | \"L\" ClassName \";\" | \"[\" FieldType.\n+\/\/ FieldType  = \"B\" | \"C\" | \"D\" | \"F\" | \"I\" | \"J\" | \"S\" | \"Z\" | \"L\" ClassName \";\" | \"Q\" ValueClassName \";\" | \"[\" FieldType.\n@@ -502,0 +504,14 @@\n+InlineKlass* SignatureStream::as_inline_klass(InstanceKlass* holder) {\n+  ThreadInVMfromUnknown tiv;\n+  JavaThread* THREAD = JavaThread::current();\n+  HandleMark hm(THREAD);\n+  Handle class_loader(THREAD, holder->class_loader());\n+  Klass* k = as_klass(class_loader, SignatureStream::CachedOrNull, THREAD);\n+  assert(!HAS_PENDING_EXCEPTION, \"Should never throw\");\n+  if (k != nullptr && k->is_inline_klass()) {\n+    return InlineKlass::cast(k);\n+  } else {\n+    return nullptr;\n+  }\n+}\n+\n@@ -576,1 +592,0 @@\n-\n@@ -595,1 +610,1 @@\n-bool SignatureVerifier::is_valid_method_signature(Symbol* sig) {\n+bool SignatureVerifier::is_valid_method_signature(const Symbol* sig) {\n@@ -618,1 +633,1 @@\n-bool SignatureVerifier::is_valid_type_signature(Symbol* sig) {\n+bool SignatureVerifier::is_valid_type_signature(const Symbol* sig) {\n@@ -665,0 +680,56 @@\n+\n+\/\/ Adds an argument to the signature\n+void SigEntry::add_entry(GrowableArray<SigEntry>* sig, BasicType bt, Symbol* name, int offset) {\n+  sig->append(SigEntry(bt, offset, name, false));\n+  if (bt == T_LONG || bt == T_DOUBLE) {\n+    sig->append(SigEntry(T_VOID, offset, name, false)); \/\/ Longs and doubles take two stack slots\n+  }\n+}\n+void SigEntry::add_null_marker(GrowableArray<SigEntry>* sig, Symbol* name, int offset) {\n+  sig->append(SigEntry(T_BOOLEAN, offset, name, true));\n+}\n+\n+\/\/ Returns true if the argument at index 'i' is not an inline type delimiter\n+bool SigEntry::skip_value_delimiters(const GrowableArray<SigEntry>* sig, int i) {\n+  return (sig->at(i)._bt != T_METADATA &&\n+          (sig->at(i)._bt != T_VOID || sig->at(i-1)._bt == T_LONG || sig->at(i-1)._bt == T_DOUBLE));\n+}\n+\n+\/\/ Fill basic type array from signature array\n+int SigEntry::fill_sig_bt(const GrowableArray<SigEntry>* sig, BasicType* sig_bt) {\n+  int count = 0;\n+  for (int i = 0; i < sig->length(); i++) {\n+    if (skip_value_delimiters(sig, i)) {\n+      sig_bt[count++] = sig->at(i)._bt;\n+    }\n+  }\n+  return count;\n+}\n+\n+\/\/ Create a temporary symbol from the signature array\n+TempNewSymbol SigEntry::create_symbol(const GrowableArray<SigEntry>* sig) {\n+  ResourceMark rm;\n+  int length = sig->length();\n+  char* sig_str = NEW_RESOURCE_ARRAY(char, 2*length + 3);\n+  int idx = 0;\n+  sig_str[idx++] = '(';\n+  for (int i = 0; i < length; i++) {\n+    BasicType bt = sig->at(i)._bt;\n+    if (bt == T_METADATA || bt == T_VOID) {\n+      \/\/ Ignore\n+    } else {\n+      if (bt == T_ARRAY) {\n+        bt = T_OBJECT; \/\/ We don't know the element type, treat as Object\n+      }\n+      sig_str[idx++] = type2char(bt);\n+      if (bt == T_OBJECT) {\n+        sig_str[idx++] = ';';\n+      }\n+    }\n+  }\n+  sig_str[idx++] = ')';\n+  \/\/ Add a dummy return type. It won't be used but SignatureStream needs it.\n+  sig_str[idx++] = 'V';\n+  sig_str[idx++] = '\\0';\n+  return SymbolTable::new_symbol(sig_str);\n+}\n","filename":"src\/hotspot\/share\/runtime\/signature.cpp","additions":75,"deletions":4,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+  bool _callee_augmented;\n@@ -72,1 +73,1 @@\n-  inline intptr_t* unextended_sp() const { return frame_kind == ChunkFrames::Mixed ? _unextended_sp : _sp; }\n+  inline intptr_t* unextended_sp() const { return _unextended_sp; }\n","filename":"src\/hotspot\/share\/runtime\/stackChunkFrameStream.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+  _callee_augmented = false;\n@@ -64,0 +65,2 @@\n+  } else {\n+    _unextended_sp = _sp;\n@@ -65,1 +68,0 @@\n-  DEBUG_ONLY(else _unextended_sp = nullptr;)\n@@ -89,0 +91,2 @@\n+  } else {\n+    _unextended_sp = _sp;\n@@ -90,1 +94,0 @@\n-  DEBUG_ONLY(else _unextended_sp = nullptr;)\n@@ -93,0 +96,1 @@\n+  _callee_augmented = false;\n@@ -220,0 +224,1 @@\n+  _callee_augmented = false;\n@@ -228,1 +233,8 @@\n-      _unextended_sp = is_interpreted() ? unextended_sp_for_interpreter_frame() : _sp;\n+      if (is_interpreted()) {\n+        _unextended_sp = unextended_sp_for_interpreter_frame();\n+      } else if (cb()->is_nmethod() && cb()->as_nmethod()->needs_stack_repair()) {\n+        _unextended_sp = frame::repair_sender_sp(cb()->as_nmethod(), _unextended_sp, (intptr_t**)(_sp - frame::sender_sp_offset));\n+        _callee_augmented = _unextended_sp != _sp;\n+      } else {\n+        _unextended_sp = _sp;\n+      }\n@@ -232,1 +244,7 @@\n-    _sp += cb()->frame_size();\n+    _sp = _unextended_sp + cb()->frame_size();\n+    if (cb()->is_nmethod() && cb()->as_nmethod()->needs_stack_repair()) {\n+      _unextended_sp = frame::repair_sender_sp(cb()->as_nmethod(), _unextended_sp, (intptr_t**)(_sp - frame::sender_sp_offset));\n+      _callee_augmented = _unextended_sp != _sp;\n+    } else {\n+      _unextended_sp = _sp;\n+    }\n","filename":"src\/hotspot\/share\/runtime\/stackChunkFrameStream.inline.hpp","additions":22,"deletions":4,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -277,0 +277,10 @@\n+class VM_PrintClassLayout: public VM_Operation {\n+ private:\n+  outputStream* _out;\n+  char* _class_name;\n+ public:\n+  VM_PrintClassLayout(outputStream* st, char* class_name): _out(st), _class_name(class_name) {}\n+  VMOp_Type type() const { return VMOp_PrintClassHierarchy; }\n+  void doit();\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -120,0 +120,1 @@\n+  DCmdFactory::register_DCmdFactory(new DCmdFactoryImpl<PrintClassLayoutDCmd>(full_export));\n@@ -948,1 +949,25 @@\n-#endif\n+\n+PrintClassLayoutDCmd::PrintClassLayoutDCmd(outputStream* output, bool heap) :\n+                                       DCmdWithParser(output, heap),\n+  _classname(\"classname\", \"Name of class whose layout should be printed. \",\n+             \"STRING\", true) {\n+  _dcmdparser.add_dcmd_argument(&_classname);\n+}\n+\n+void PrintClassLayoutDCmd::execute(DCmdSource source, TRAPS) {\n+  VM_PrintClassLayout printClassLayoutOp(output(), _classname.value());\n+  VMThread::execute(&printClassLayoutOp);\n+}\n+\n+int PrintClassLayoutDCmd::num_arguments() {\n+  ResourceMark rm;\n+  PrintClassLayoutDCmd* dcmd = new PrintClassLayoutDCmd(nullptr, false);\n+  if (dcmd != nullptr) {\n+    DCmdMark mark(dcmd);\n+    return dcmd->_dcmdparser.num_arguments();\n+  } else {\n+    return 0;\n+  }\n+}\n+\n+#endif \/\/ INCLUDE_SERVICES\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.cpp","additions":26,"deletions":1,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -476,0 +476,1 @@\n+             ArrayVarHandle,\n@@ -503,1 +504,0 @@\n-             VarHandleReferences.Array,\n@@ -508,1 +508,5 @@\n-             VarHandleShorts.FieldStaticReadOnly {\n+             VarHandleShorts.FieldStaticReadOnly,\n+             VarHandleFlatValues.FieldInstanceReadOnly,\n+             VarHandleNonAtomicReferences.FieldInstanceReadOnly,\n+             VarHandleNonAtomicReferences.FieldStaticReadOnly,\n+             VarHandleNonAtomicFlatValues.FieldInstanceReadOnly {\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/VarHandle.java","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+import java.util.Objects;\n@@ -35,0 +36,8 @@\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          The referent must have {@linkplain Objects#hasIdentity(Object) object identity}.\n+ *          When preview features are enabled, attempts to create a reference\n+ *          to a {@linkplain Class#isValue value object} result in an {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n+ *\n@@ -103,0 +112,2 @@\n+     * @throws IdentityException if the referent is not an\n+     *         {@link java.util.Objects#hasIdentity(Object) identity object}\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/PhantomReference.java","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+import java.util.Objects;\n+\n@@ -34,0 +36,8 @@\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          The referent must have {@linkplain Objects#hasIdentity(Object) object identity}.\n+ *          When preview features are enabled, attempts to create a reference\n+ *          to a {@linkplain Class#isValue value object} result in an {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n+ *\n@@ -84,0 +94,2 @@\n+     * @throws IdentityException if the referent is not an\n+     *         {@link java.util.Objects#hasIdentity(Object) identity object}\n@@ -97,1 +109,2 @@\n-     *\n+     * @throws IdentityException if the referent is not an\n+     *         {@link java.util.Objects#hasIdentity(Object) identity object}\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/SoftReference.java","additions":14,"deletions":1,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+import java.util.Objects;\n+\n@@ -34,0 +36,8 @@\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          The referent must have {@linkplain Objects#hasIdentity(Object) object identity}.\n+ *          When preview features are enabled, attempts to create a reference\n+ *          to a {@linkplain Class#isValue value object} result in an {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n+ *\n@@ -56,0 +66,2 @@\n+     * @throws IdentityException if the referent is not an\n+     *         {@link java.util.Objects#hasIdentity(Object) identity object}\n@@ -68,0 +80,2 @@\n+     * @throws IdentityException if the referent is not an\n+     *         {@link java.util.Objects#hasIdentity(Object) identity object}\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/WeakReference.java","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+import java.util.NoSuchElementException;\n@@ -41,0 +42,1 @@\n+import java.util.Random;\n@@ -42,0 +44,2 @@\n+import java.util.stream.Stream;\n+\n@@ -320,0 +324,50 @@\n+    \/**\n+     * Returns the \"raw\" API for accessing underlying jimage resource entries.\n+     *\n+     * <p>This is only meaningful for use by code dealing directly with jimage\n+     * files, and cannot be used to reliably lookup resources used at runtime.\n+     *\n+     * <p>This API remains valid until the image reader from which it was\n+     * obtained is closed.\n+     *\/\n+    \/\/ Package visible for use by ImageReader.\n+    ResourceEntries getResourceEntries() {\n+        return new ResourceEntries() {\n+            @Override\n+            public Stream<String> getEntryNames(String module) {\n+                if (module.isEmpty() || module.equals(\"modules\") || module.equals(\"packages\")) {\n+                    throw new IllegalArgumentException(\"Invalid module name: \" + module);\n+                }\n+                return IntStream.range(0, offsets.capacity())\n+                        .map(offsets::get)\n+                        .filter(offset -> offset != 0)\n+                        \/\/ Reusing a location instance or getting the module\n+                        \/\/ offset directly would save a lot of allocations here.\n+                        .mapToObj(offset -> ImageLocation.readFrom(BasicImageReader.this, offset))\n+                        \/\/ Reverse lookup of module offset would be faster here.\n+                        .filter(loc -> module.equals(loc.getModule()))\n+                        .map(ImageLocation::getFullName);\n+            }\n+\n+            private ImageLocation getResourceLocation(String name) {\n+                if (!name.startsWith(\"\/modules\/\") && !name.startsWith(\"\/packages\/\")) {\n+                    ImageLocation location = BasicImageReader.this.findLocation(name);\n+                    if (location != null) {\n+                        return location;\n+                    }\n+                }\n+                throw new NoSuchElementException(\"No such resource entry: \" + name);\n+            }\n+\n+            @Override\n+            public long getSize(String name) {\n+                return getResourceLocation(name).getUncompressedSize();\n+            }\n+\n+            @Override\n+            public byte[] getBytes(String name) {\n+                return BasicImageReader.this.getResource(getResourceLocation(name));\n+            }\n+        };\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/jimage\/BasicImageReader.java","additions":54,"deletions":0,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -27,0 +27,2 @@\n+import jdk.internal.jimage.ImageLocation.LocationType;\n+\n@@ -37,0 +39,1 @@\n+import java.util.Comparator;\n@@ -43,0 +46,1 @@\n+import java.util.TreeMap;\n@@ -47,0 +51,8 @@\n+import static jdk.internal.jimage.ImageLocation.LocationType.MODULES_DIR;\n+import static jdk.internal.jimage.ImageLocation.LocationType.MODULES_ROOT;\n+import static jdk.internal.jimage.ImageLocation.LocationType.PACKAGES_DIR;\n+import static jdk.internal.jimage.ImageLocation.LocationType.RESOURCE;\n+import static jdk.internal.jimage.ImageLocation.MODULES_PREFIX;\n+import static jdk.internal.jimage.ImageLocation.PACKAGES_PREFIX;\n+import static jdk.internal.jimage.ImageLocation.PREVIEW_INFIX;\n+\n@@ -89,2 +101,4 @@\n-     * Opens an image reader for a jimage file at the specified path, using the\n-     * given byte order.\n+     * Opens an image reader for a jimage file at the specified path.\n+     *\n+     * @param imagePath file system path of the jimage file.\n+     * @param mode whether to return preview resources.\n@@ -92,5 +106,2 @@\n-    public static ImageReader open(Path imagePath, ByteOrder byteOrder) throws IOException {\n-        Objects.requireNonNull(imagePath);\n-        Objects.requireNonNull(byteOrder);\n-\n-        return SharedImageReader.open(imagePath, byteOrder);\n+    public static ImageReader open(Path imagePath, PreviewMode mode) throws IOException {\n+        return open(imagePath, ByteOrder.nativeOrder(), mode);\n@@ -100,2 +111,5 @@\n-     * Opens an image reader for a jimage file at the specified path, using the\n-     * platform native byte order.\n+     * Opens an image reader for a jimage file at the specified path.\n+     *\n+     * @param imagePath file system path of the jimage file.\n+     * @param byteOrder the byte-order to be used when reading the jimage file.\n+     * @param mode controls whether preview resources are visible.\n@@ -103,2 +117,5 @@\n-    public static ImageReader open(Path imagePath) throws IOException {\n-        return open(imagePath, ByteOrder.nativeOrder());\n+    public static ImageReader open(Path imagePath, ByteOrder byteOrder, PreviewMode mode)\n+            throws IOException {\n+        Objects.requireNonNull(imagePath);\n+        Objects.requireNonNull(byteOrder);\n+        return SharedImageReader.open(imagePath, byteOrder, mode.isPreviewModeEnabled());\n@@ -203,0 +220,5 @@\n+    \/\/ Package protected for use only by SystemImageReader.\n+    ResourceEntries getResourceEntries() {\n+        return reader.getResourceEntries();\n+    }\n+\n@@ -204,3 +226,0 @@\n-        private static final Map<Path, SharedImageReader> OPEN_FILES = new HashMap<>();\n-        private static final String MODULES_ROOT = \"\/modules\";\n-        private static final String PACKAGES_ROOT = \"\/packages\";\n@@ -212,0 +231,27 @@\n+        static final class ReaderKey {\n+            private final Path imagePath;\n+            private final boolean previewMode;\n+\n+            public ReaderKey(Path imagePath, boolean previewMode) {\n+                this.imagePath = imagePath;\n+                this.previewMode = previewMode;\n+            }\n+\n+            @Override\n+            public boolean equals(Object obj) {\n+                \/\/ No pattern variables here (Java 8 compatible source).\n+                if (obj instanceof ReaderKey) {\n+                    ReaderKey other = (ReaderKey) obj;\n+                    return this.imagePath.equals(other.imagePath) && this.previewMode == other.previewMode;\n+                }\n+                return false;\n+            }\n+\n+            @Override\n+            public int hashCode() {\n+                return imagePath.hashCode() ^ Boolean.hashCode(previewMode);\n+            }\n+        }\n+\n+        private static final Map<ReaderKey, SharedImageReader> OPEN_FILES = new HashMap<>();\n+\n@@ -222,4 +268,8 @@\n-        \/\/ Used to classify ImageLocation instances without string comparison.\n-        private final int modulesStringOffset;\n-        private final int packagesStringOffset;\n-        private SharedImageReader(Path imagePath, ByteOrder byteOrder) throws IOException {\n+        \/\/ Preview mode support.\n+        private final boolean previewMode;\n+        \/\/ A relativized mapping from non-preview name to directories containing\n+        \/\/ preview-only nodes. This is used to add preview-only content to\n+        \/\/ directories as they are completed.\n+        private final HashMap<String, Directory> previewDirectoriesToMerge;\n+\n+        private SharedImageReader(Path imagePath, ByteOrder byteOrder, boolean previewMode) throws IOException {\n@@ -230,4 +280,1 @@\n-            \/\/ Pick stable jimage names from which to extract string offsets (we cannot\n-            \/\/ use \"\/modules\" or \"\/packages\", since those have a module offset of zero).\n-            this.modulesStringOffset = getModuleOffset(\"\/modules\/java.base\");\n-            this.packagesStringOffset = getModuleOffset(\"\/packages\/java.lang\");\n+            this.previewMode = previewMode;\n@@ -237,4 +284,2 @@\n-            Directory packages = newDirectory(PACKAGES_ROOT);\n-            nodes.put(packages.getName(), packages);\n-            Directory modules = newDirectory(MODULES_ROOT);\n-            nodes.put(modules.getName(), modules);\n+            Directory packages = ensureCached(newDirectory(PACKAGES_PREFIX));\n+            Directory modules = ensureCached(newDirectory(MODULES_PREFIX));\n@@ -244,1 +289,8 @@\n-            nodes.put(root.getName(), root);\n+            ensureCached(root);\n+\n+            \/\/ By scanning the \/packages directory information early we can determine\n+            \/\/ which module\/package pairs have preview resources, and build the (small)\n+            \/\/ set of preview nodes early. This also ensures that preview-only entries\n+            \/\/ in the \/packages directory are not present in non-preview mode.\n+            this.previewDirectoriesToMerge = previewMode ? new HashMap<>() : null;\n+            packages.setChildren(processPackagesDirectory(previewMode));\n@@ -248,3 +300,8 @@\n-         * Returns the offset of the string denoting the leading \"module\" segment in\n-         * the given path (e.g. {@code <module>\/<path>}). We can't just pass in the\n-         * {@code \/<module>} string here because that has a module offset of zero.\n+         * Process {@code \"\/packages\/xxx\"} entries to build the child nodes for the\n+         * root {@code \"\/packages\"} node. Preview-only entries will be skipped if\n+         * {@code previewMode == false}.\n+         *\n+         * <p>If {@code previewMode == true}, this method also populates the {@link\n+         * #previewDirectoriesToMerge} map with any preview-only nodes, to be merged\n+         * into directories as they are completed. It also caches preview resources\n+         * and preview-only directories for direct lookup.\n@@ -252,6 +309,78 @@\n-        private int getModuleOffset(String path) {\n-            ImageLocation location = findLocation(path);\n-            assert location != null : \"Cannot find expected jimage location: \" + path;\n-            int offset = location.getModuleOffset();\n-            assert offset != 0 : \"Invalid module offset for jimage location: \" + path;\n-            return offset;\n+        private ArrayList<Node> processPackagesDirectory(boolean previewMode) {\n+            ImageLocation pkgRoot = findLocation(PACKAGES_PREFIX);\n+            assert pkgRoot != null : \"Invalid jimage file\";\n+            IntBuffer offsets = getOffsetBuffer(pkgRoot);\n+            ArrayList<Node> pkgDirs = new ArrayList<>(offsets.capacity());\n+            \/\/ Package path to module map, sorted in reverse order so that\n+            \/\/ longer child paths get processed first.\n+            Map<String, List<String>> previewPackagesToModules =\n+                    new TreeMap<>(Comparator.reverseOrder());\n+            for (int i = 0; i < offsets.capacity(); i++) {\n+                ImageLocation pkgDir = getLocation(offsets.get(i));\n+                int flags = pkgDir.getFlags();\n+                \/\/ A package subdirectory is \"preview only\" if all the modules\n+                \/\/ it references have that package marked as preview only.\n+                \/\/ Skipping these entries avoids empty package subdirectories.\n+                if (previewMode || !ImageLocation.isPreviewOnly(flags)) {\n+                    pkgDirs.add(ensureCached(newDirectory(pkgDir.getFullName())));\n+                }\n+                if (previewMode && ImageLocation.hasPreviewVersion(flags)) {\n+                    \/\/ Only do this in preview mode for the small set of packages with\n+                    \/\/ preview versions (the number of preview entries should be small).\n+                    List<String> moduleNames = new ArrayList<>();\n+                    ModuleReference.readNameOffsets(getOffsetBuffer(pkgDir), \/*normal*\/ false, \/*preview*\/ true)\n+                            .forEachRemaining(n -> moduleNames.add(getString(n)));\n+                    previewPackagesToModules.put(pkgDir.getBase().replace('.', '\/'), moduleNames);\n+                }\n+            }\n+            \/\/ Reverse sorted map means child directories are processed first.\n+            previewPackagesToModules.forEach((pkgPath, modules) ->\n+                    modules.forEach(modName -> processPreviewDir(MODULES_PREFIX + \"\/\" + modName, pkgPath)));\n+            \/\/ We might have skipped some preview-only package entries.\n+            pkgDirs.trimToSize();\n+            return pkgDirs;\n+        }\n+\n+        void processPreviewDir(String namePrefix, String pkgPath) {\n+            String previewDirName = namePrefix + PREVIEW_INFIX + \"\/\" + pkgPath;\n+            ImageLocation previewLoc = findLocation(previewDirName);\n+            assert previewLoc != null : \"Missing preview directory location: \" + previewDirName;\n+            String nonPreviewDirName = namePrefix + \"\/\" + pkgPath;\n+            List<Node> previewOnlyChildren = createChildNodes(previewLoc, 0, childLoc -> {\n+                String baseName = getBaseName(childLoc);\n+                String nonPreviewChildName = nonPreviewDirName + \"\/\" + baseName;\n+                boolean isPreviewOnly = ImageLocation.isPreviewOnly(childLoc.getFlags());\n+                LocationType type = childLoc.getType();\n+                if (type == RESOURCE) {\n+                    \/\/ Preview resources are cached to override non-preview versions.\n+                    Node childNode = ensureCached(newResource(nonPreviewChildName, childLoc));\n+                    return isPreviewOnly ? childNode : null;\n+                } else {\n+                    \/\/ Child directories are not cached here (they are either cached\n+                    \/\/ already or have been added to previewDirectoriesToMerge).\n+                    assert type == MODULES_DIR : \"Invalid location type: \" + childLoc;\n+                    Node childNode = nodes.get(nonPreviewChildName);\n+                    assert isPreviewOnly == (childNode != null) :\n+                            \"Inconsistent child node: \" + nonPreviewChildName;\n+                    return childNode;\n+                }\n+            });\n+            Directory previewDir = newDirectory(nonPreviewDirName);\n+            previewDir.setChildren(previewOnlyChildren);\n+            if (ImageLocation.isPreviewOnly(previewLoc.getFlags())) {\n+                \/\/ If we are preview-only, our children are also preview-only, so\n+                \/\/ this directory is a complete hierarchy and should be cached.\n+                assert !previewOnlyChildren.isEmpty() : \"Invalid empty preview-only directory: \" + nonPreviewDirName;\n+                ensureCached(previewDir);\n+            } else if (!previewOnlyChildren.isEmpty()) {\n+                \/\/ A partial directory containing extra preview-only nodes\n+                \/\/ to be merged when the non-preview directory is completed.\n+                previewDirectoriesToMerge.put(nonPreviewDirName, previewDir);\n+            }\n+        }\n+\n+        \/\/ Adds a node to the cache, ensuring that no matching entry already existed.\n+        private <T extends Node> T ensureCached(T node) {\n+            Node existingNode = nodes.put(node.getName(), node);\n+            assert existingNode == null : \"Unexpected node already cached for: \" + node;\n+            return node;\n@@ -260,1 +389,1 @@\n-        private static ImageReader open(Path imagePath, ByteOrder byteOrder) throws IOException {\n+        private static ImageReader open(Path imagePath, ByteOrder byteOrder, boolean previewMode) throws IOException {\n@@ -265,1 +394,2 @@\n-                SharedImageReader reader = OPEN_FILES.get(imagePath);\n+                ReaderKey key = new ReaderKey(imagePath, previewMode);\n+                SharedImageReader reader = OPEN_FILES.get(key);\n@@ -269,2 +399,2 @@\n-                    reader =  new SharedImageReader(imagePath, byteOrder);\n-                    OPEN_FILES.put(imagePath, reader);\n+                    reader = new SharedImageReader(imagePath, byteOrder, previewMode);\n+                    OPEN_FILES.put(key, reader);\n@@ -294,1 +424,1 @@\n-                    if (!OPEN_FILES.remove(this.getImagePath(), this)) {\n+                    if (!OPEN_FILES.remove(new ReaderKey(getImagePath(), previewMode), this)) {\n@@ -312,0 +442,2 @@\n+            \/\/ Root directories \"\/\", \"\/modules\" and \"\/packages\", as well\n+            \/\/ as all \"\/packages\/xxx\" subdirectories are already cached.\n@@ -314,12 +446,4 @@\n-                \/\/ We cannot get the root paths (\"\/modules\" or \"\/packages\") here\n-                \/\/ because those nodes are already in the nodes cache.\n-                if (name.startsWith(MODULES_ROOT + \"\/\")) {\n-                    \/\/ This may perform two lookups, one for a directory (in\n-                    \/\/ \"\/modules\/...\") and one for a non-prefixed resource\n-                    \/\/ (with \"\/modules\" removed).\n-                    node = buildModulesNode(name);\n-                } else if (name.startsWith(PACKAGES_ROOT + \"\/\")) {\n-                    node = buildPackagesNode(name);\n-                }\n-                if (node != null) {\n-                    nodes.put(node.getName(), node);\n+                if (name.startsWith(MODULES_PREFIX + \"\/\")) {\n+                    node = buildAndCacheModulesNode(name);\n+                } else if (name.startsWith(PACKAGES_PREFIX + \"\/\")) {\n+                    node = buildAndCacheLinkNode(name);\n@@ -349,1 +473,1 @@\n-            String nodeName = MODULES_ROOT + \"\/\" + moduleName + \"\/\" + resourcePath;\n+            String nodeName = MODULES_PREFIX + \"\/\" + moduleName + \"\/\" + resourcePath;\n@@ -355,1 +479,1 @@\n-                    if (loc != null && isResource(loc)) {\n+                    if (loc != null && loc.getType() == RESOURCE) {\n@@ -371,3 +495,4 @@\n-         * search). As such, it skips checking the nodes cache and only checks\n-         * for an entry in the jimage file, as this is faster if the resource\n-         * is not present. This also means it doesn't need synchronization.\n+         * search). As such, it skips checking the nodes cache if possible, and\n+         * only checks for an entry in the jimage file, as this is faster if the\n+         * resource is not present. This also means it doesn't need\n+         * synchronization most of the time.\n@@ -379,2 +504,12 @@\n-            \/\/ If the given module name is 'modules', then 'isResource()'\n-            \/\/ returns false to prevent false positives.\n+            \/\/ In preview mode, preview-only resources are eagerly added to the\n+            \/\/ cache, so we must check that first.\n+            if (previewMode) {\n+                String nodeName = MODULES_PREFIX + \"\/\" + moduleName + \"\/\" + resourcePath;\n+                \/\/ Synchronize as tightly as possible to reduce locking contention.\n+                synchronized (this) {\n+                    Node node = nodes.get(nodeName);\n+                    if (node != null) {\n+                        return node.isResource();\n+                    }\n+                }\n+            }\n@@ -382,1 +517,1 @@\n-            return loc != null && isResource(loc);\n+            return loc != null && loc.getType() == RESOURCE;\n@@ -391,2 +526,5 @@\n-        private Node buildModulesNode(String name) {\n-            assert name.startsWith(MODULES_ROOT + \"\/\") : \"Invalid module node name: \" + name;\n+        private Node buildAndCacheModulesNode(String name) {\n+            assert name.startsWith(MODULES_PREFIX + \"\/\") : \"Invalid module node name: \" + name;\n+            if (isPreviewName(name)) {\n+                return null;\n+            }\n@@ -398,2 +536,2 @@\n-                assert isModulesSubdirectory(loc) : \"Invalid modules directory: \" + name;\n-                return completeModuleDirectory(newDirectory(name), loc);\n+                assert loc.getType() == MODULES_DIR : \"Invalid modules directory: \" + name;\n+                return ensureCached(completeModuleDirectory(newDirectory(name), loc));\n@@ -404,2 +542,4 @@\n-            loc = findLocation(name.substring(MODULES_ROOT.length()));\n-            return loc != null && isResource(loc) ? newResource(name, loc) : null;\n+            loc = findLocation(name.substring(MODULES_PREFIX.length()));\n+            return loc != null && loc.getType() == RESOURCE\n+                    ? ensureCached(newResource(name, loc))\n+                    : null;\n@@ -409,1 +549,31 @@\n-         * Builds a node in the \"\/packages\/...\" namespace.\n+         * Returns whether a directory name in the \"\/modules\/\" directory could be referencing\n+         * the \"META-INF\" directory\".\n+         *\/\n+        private boolean isMetaInf(Directory dir) {\n+            String name = dir.getName();\n+            int pathStart = name.indexOf('\/', MODULES_PREFIX.length() + 1);\n+            return name.length() == pathStart + \"\/META-INF\".length()\n+                    && name.endsWith(\"\/META-INF\");\n+        }\n+\n+        \/**\n+         * Returns whether a node name in the \"\/modules\/\" directory could be referencing\n+         * a preview resource or directory under \"META-INF\/preview\".\n+         *\/\n+        private boolean isPreviewName(String name) {\n+            int pathStart = name.indexOf('\/', MODULES_PREFIX.length() + 1);\n+            int previewEnd = pathStart + PREVIEW_INFIX.length();\n+            return pathStart > 0\n+                    && name.regionMatches(pathStart, PREVIEW_INFIX, 0, PREVIEW_INFIX.length())\n+                    && (name.length() == previewEnd || name.charAt(previewEnd) == '\/');\n+        }\n+\n+        private String getBaseName(ImageLocation loc) {\n+            \/\/ Matches logic in ImageLocation#getFullName() regarding extensions.\n+            String trailingParts = loc.getBase()\n+                    + ((loc.getExtensionOffset() != 0) ? \".\" + loc.getExtension() : \"\");\n+            return trailingParts.substring(trailingParts.lastIndexOf('\/') + 1);\n+        }\n+\n+        \/**\n+         * Builds a link node of the form \"\/packages\/xxx\/yyy\".\n@@ -411,2 +581,2 @@\n-         * <p>Called by {@link #findNode(String)} if a {@code \/packages\/...} node\n-         * is not present in the cache.\n+         * <p>Called by {@link #findNode(String)} if a {@code \/packages\/...}\n+         * node is not present in the cache (the name is not trusted).\n@@ -414,7 +584,5 @@\n-        private Node buildPackagesNode(String name) {\n-            \/\/ There are only locations for the root \"\/packages\" or \"\/packages\/xxx\"\n-            \/\/ directories, but not the symbolic links below them (the links can be\n-            \/\/ entirely derived from the name information in the parent directory).\n-            \/\/ However, unlike resources this means that we do not have a constant\n-            \/\/ time lookup for link nodes when creating them.\n-            int packageStart = PACKAGES_ROOT.length() + 1;\n+        private Node buildAndCacheLinkNode(String name) {\n+            \/\/ There are only locations for \"\/packages\" or \"\/packages\/xxx\"\n+            \/\/ directories, but not the symbolic links below them (links are\n+            \/\/ derived from the name information in the parent directory).\n+            int packageStart = PACKAGES_PREFIX.length() + 1;\n@@ -422,8 +590,3 @@\n-            if (packageEnd == -1) {\n-                ImageLocation loc = findLocation(name);\n-                return loc != null ? completePackageDirectory(newDirectory(name), loc) : null;\n-            } else {\n-                \/\/ We cannot assume that the parent directory exists for a link node, since\n-                \/\/ the given name is untrusted and could reference a non-existent link.\n-                \/\/ However, if the parent directory is present, we can conclude that the\n-                \/\/ given name was not a valid link (or else it would already be cached).\n+            \/\/ We already built the 2-level \"\/packages\/xxx\" directories,\n+            \/\/ so if this is a 2-level name, it cannot reference a node.\n+            if (packageEnd >= 0) {\n@@ -431,8 +594,6 @@\n-                if (!nodes.containsKey(dirName)) {\n-                    ImageLocation loc = findLocation(dirName);\n-                    \/\/ If the parent location doesn't exist, the link node cannot exist.\n-                    if (loc != null) {\n-                        nodes.put(dirName, completePackageDirectory(newDirectory(dirName), loc));\n-                        \/\/ When the parent is created its child nodes are created and cached,\n-                        \/\/ but this can still return null if given name wasn't a valid link.\n-                        return nodes.get(name);\n+                \/\/ If no parent exists here, the name cannot be valid.\n+                Directory parent = (Directory) nodes.get(dirName);\n+                if (parent != null) {\n+                    if (!parent.isCompleted()) {\n+                        \/\/ This caches all child links of the parent directory.\n+                        completePackageSubdirectory(parent, findLocation(dirName));\n@@ -440,0 +601,1 @@\n+                    return nodes.get(name);\n@@ -451,1 +613,1 @@\n-            assert name.startsWith(MODULES_ROOT) || name.startsWith(PACKAGES_ROOT);\n+            assert name.startsWith(MODULES_PREFIX) || name.startsWith(PACKAGES_PREFIX);\n@@ -454,4 +616,2 @@\n-            \/\/ We cannot use 'isXxxSubdirectory()' methods here since we could\n-            \/\/ be given a top-level directory (for which that test doesn't work).\n-            \/\/ The string MUST start \"\/modules\" or \"\/packages\" here.\n-            if (name.charAt(1) == 'm') {\n+            LocationType type = loc.getType();\n+            if (type == MODULES_DIR || type == MODULES_ROOT) {\n@@ -460,1 +620,2 @@\n-                completePackageDirectory(dir, loc);\n+                assert type == PACKAGES_DIR : \"Invalid location type: \" + loc;\n+                completePackageSubdirectory(dir, loc);\n@@ -465,6 +626,1 @@\n-        \/**\n-         * Completes a modules directory by setting the list of child nodes.\n-         *\n-         * <p>The given directory can be the top level {@code \/modules} directory,\n-         * so it is NOT safe to use {@code isModulesSubdirectory(loc)} here.\n-         *\/\n+        \/** Completes a modules directory by setting the list of child nodes. *\/\n@@ -473,3 +629,11 @@\n-            List<Node> children = createChildNodes(loc, childLoc -> {\n-                if (isModulesSubdirectory(childLoc)) {\n-                    return nodes.computeIfAbsent(childLoc.getFullName(), this::newDirectory);\n+            List<Node> previewOnlyNodes = getPreviewNodesToMerge(dir);\n+            \/\/ We hide preview names from direct lookup, but must also prevent\n+            \/\/ the preview directory from appearing in any META-INF directories.\n+            boolean parentIsMetaInfDir = isMetaInf(dir);\n+            List<Node> children = createChildNodes(loc, previewOnlyNodes.size(), childLoc -> {\n+                LocationType type = childLoc.getType();\n+                if (type == MODULES_DIR) {\n+                    String name = childLoc.getFullName();\n+                    return parentIsMetaInfDir && name.endsWith(\"\/preview\")\n+                            ? null\n+                            : nodes.computeIfAbsent(name, this::newDirectory);\n@@ -477,0 +641,1 @@\n+                    assert type == RESOURCE : \"Invalid location type: \" + loc;\n@@ -482,0 +647,1 @@\n+            children.addAll(previewOnlyNodes);\n@@ -486,0 +652,21 @@\n+        \/** Completes a package directory by setting the list of child nodes. *\/\n+        private void completePackageSubdirectory(Directory dir, ImageLocation loc) {\n+            assert dir.getName().equals(loc.getFullName()) : \"Mismatched location for directory: \" + dir;\n+            assert !dir.isCompleted() : \"Directory already completed: \" + dir;\n+            assert loc.getType() == PACKAGES_DIR : \"Invalid location type: \" + loc.getType();\n+\n+            \/\/ In non-preview mode we might skip a very small number of preview-only\n+            \/\/ entries, but it's not worth \"right-sizing\" the array for that.\n+            IntBuffer offsets = getOffsetBuffer(loc);\n+            List<Node> children = new ArrayList<>(offsets.capacity() \/ 2);\n+            ModuleReference.readNameOffsets(offsets, \/*normal*\/ true, previewMode)\n+                    .forEachRemaining(n -> {\n+                        String modName = getString(n);\n+                        Node link = newLinkNode(dir.getName() + \"\/\" + modName, MODULES_PREFIX + \"\/\" + modName);\n+                        children.add(ensureCached(link));\n+                    });\n+            \/\/ If the parent directory exists, there must be at least one child node.\n+            assert !children.isEmpty() : \"Invalid empty package directory: \" + dir;\n+            dir.setChildren(children);\n+        }\n+\n@@ -487,1 +674,1 @@\n-         * Completes a package directory by setting the list of child nodes.\n+         * Returns the list of child preview nodes to be merged into the given directory.\n@@ -489,2 +676,4 @@\n-         * <p>The given directory can be the top level {@code \/packages} directory,\n-         * so it is NOT safe to use {@code isPackagesSubdirectory(loc)} here.\n+         * <p>Because this is only called once per-directory (since the result is cached\n+         * indefinitely) we can remove any entries we find from the cache. If ever the\n+         * node cache allowed entries to expire, this would have to be changed so that\n+         * directories could be completed more than once.\n@@ -492,22 +681,5 @@\n-        private Directory completePackageDirectory(Directory dir, ImageLocation loc) {\n-            assert dir.getName().equals(loc.getFullName()) : \"Mismatched location for directory: \" + dir;\n-            \/\/ The only directories in the \"\/packages\" namespace are \"\/packages\" or\n-            \/\/ \"\/packages\/<package>\". However, unlike \"\/modules\" directories, the\n-            \/\/ location offsets mean different things.\n-            List<Node> children;\n-            if (dir.getName().equals(PACKAGES_ROOT)) {\n-                \/\/ Top-level directory just contains a list of subdirectories.\n-                children = createChildNodes(loc, c -> nodes.computeIfAbsent(c.getFullName(), this::newDirectory));\n-            } else {\n-                \/\/ A package directory's content is array of offset PAIRS in the\n-                \/\/ Strings table, but we only need the 2nd value of each pair.\n-                IntBuffer intBuffer = getOffsetBuffer(loc);\n-                int offsetCount = intBuffer.capacity();\n-                assert (offsetCount & 0x1) == 0 : \"Offset count must be even: \" + offsetCount;\n-                children = new ArrayList<>(offsetCount \/ 2);\n-                \/\/ Iterate the 2nd offset in each pair (odd indices).\n-                for (int i = 1; i < offsetCount; i += 2) {\n-                    String moduleName = getString(intBuffer.get(i));\n-                    children.add(nodes.computeIfAbsent(\n-                            dir.getName() + \"\/\" + moduleName,\n-                            n -> newLinkNode(n, MODULES_ROOT + \"\/\" + moduleName)));\n+        List<Node> getPreviewNodesToMerge(Directory dir) {\n+            if (previewDirectoriesToMerge != null) {\n+                Directory mergeDir = previewDirectoriesToMerge.remove(dir.getName());\n+                if (mergeDir != null) {\n+                    return mergeDir.children;\n@@ -516,3 +688,1 @@\n-            \/\/ This only happens once and \"completes\" the directory.\n-            dir.setChildren(children);\n-            return dir;\n+            return Collections.emptyList();\n@@ -522,1 +692,7 @@\n-         * Creates the list of child nodes for a {@code Directory} based on a given\n+         * Creates the list of child nodes for a modules {@code Directory} from\n+         * its parent location.\n+         *\n+         * <p>The {@code getChildFn} may return existing cached nodes rather\n+         * than creating them, and if newly created nodes are to be cached,\n+         * it is the job of {@code getChildFn}, or the caller of this method,\n+         * to do that.\n@@ -524,2 +700,7 @@\n-         * <p>Note: This cannot be used for package subdirectories as they have\n-         * child offsets stored differently to other directories.\n+         * @param loc a location relating to a \"\/modules\" directory.\n+         * @param extraNodesCount a known number of preview-only child nodes\n+         *     which will be merged onto the end of the returned list later.\n+         * @param getChildFn a function to return a node for each child location\n+         *     (or null to skip putting anything in the list).\n+         * @return the list of the non-null child nodes, returned by\n+         *     {@code getChildFn}, in the order of the locations entries.\n@@ -527,1 +708,3 @@\n-        private List<Node> createChildNodes(ImageLocation loc, Function<ImageLocation, Node> newChildFn) {\n+        private List<Node> createChildNodes(ImageLocation loc, int extraNodesCount, Function<ImageLocation, Node> getChildFn) {\n+            LocationType type = loc.getType();\n+            assert type == MODULES_DIR || type == MODULES_ROOT : \"Invalid location type: \" + loc;\n@@ -530,1 +713,1 @@\n-            List<Node> children = new ArrayList<>(childCount);\n+            List<Node> children = new ArrayList<>(childCount + extraNodesCount);\n@@ -532,1 +715,4 @@\n-                children.add(newChildFn.apply(getLocation(offsets.get(i))));\n+                Node childNode = getChildFn.apply(getLocation(offsets.get(i)));\n+                if (childNode != null) {\n+                    children.add(childNode);\n+                }\n@@ -539,1 +725,1 @@\n-            assert !isResource(dir) : \"Not a directory: \" + dir.getFullName();\n+            assert dir.getType() != RESOURCE : \"Not a directory: \" + dir.getFullName();\n@@ -546,26 +732,0 @@\n-        \/**\n-         * Efficiently determines if an image location is a resource.\n-         *\n-         * <p>A resource must have a valid module associated with it, so its\n-         * module offset must be non-zero, and not equal to the offsets for\n-         * \"\/modules\/...\" or \"\/packages\/...\" entries.\n-         *\/\n-        private boolean isResource(ImageLocation loc) {\n-            int moduleOffset = loc.getModuleOffset();\n-            return moduleOffset != 0\n-                    && moduleOffset != modulesStringOffset\n-                    && moduleOffset != packagesStringOffset;\n-        }\n-\n-        \/**\n-         * Determines if an image location is a directory in the {@code \/modules}\n-         * namespace (if so, the location name is the node name).\n-         *\n-         * <p>In jimage, every {@code ImageLocation} under {@code \/modules\/} is a\n-         * directory and has the same value for {@code getModule()}, and {@code\n-         * getModuleOffset()}.\n-         *\/\n-        private boolean isModulesSubdirectory(ImageLocation loc) {\n-            return loc.getModuleOffset() == modulesStringOffset;\n-        }\n-\n@@ -587,1 +747,0 @@\n-            assert name.equals(loc.getFullName(true)) : \"Mismatched location for resource: \" + name;\n@@ -819,1 +978,1 @@\n-        private void setChildren(List<Node> children) {\n+        private void setChildren(List<? extends Node> children) {\n@@ -824,0 +983,1 @@\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/jimage\/ImageReader.java","additions":331,"deletions":171,"binary":false,"changes":502,"status":"modified"},{"patch":"@@ -57,1 +57,1 @@\n-import jdk.internal.jimage.ImageReaderFactory;\n+import jdk.internal.jimage.SystemImageReader;\n@@ -395,1 +395,1 @@\n-        static final ImageReader READER = ImageReaderFactory.getImageReader();\n+        static final ImageReader READER = SystemImageReader.get();\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/module\/SystemModuleFinders.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2165,2 +2165,1 @@\n-                        && ((opcode == JVM_OPC_astore) || (opcode == JVM_OPC_aload)\n-                            || (opcode == JVM_OPC_ifnull) || (opcode == JVM_OPC_ifnonnull)))\n+                        && ((opcode == JVM_OPC_astore) || (opcode == JVM_OPC_aload)))\n","filename":"src\/java.base\/share\/native\/libverify\/check_code.c","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -119,1 +119,1 @@\n-    metadataTypeArray = new Type[11];\n+    metadataTypeArray = new Type[14];\n@@ -133,0 +133,3 @@\n+    metadataTypeArray[11] = db.lookupType(\"FlatArrayKlass\");\n+    metadataTypeArray[12] = db.lookupType(\"InlineKlass\");\n+    metadataTypeArray[11] = db.lookupType(\"RefArrayKlass\");\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/memory\/FileMapInfo.java","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -43,1 +43,2 @@\n-static void assert_test_pattern(Handle object, const char* pattern) {\n+template<typename Printable>\n+static void assert_test_pattern(Printable object, const char* pattern) {\n@@ -49,0 +50,5 @@\n+template<typename Printable>\n+static void assert_mark_word_print_pattern(Printable object, const char* pattern) {\n+  assert_test_pattern(object, pattern);\n+}\n+\n@@ -88,1 +94,1 @@\n-    assert_test_pattern(h_obj, \"locked\");\n+    assert_mark_word_print_pattern(h_obj, \"locked\");\n@@ -90,1 +96,1 @@\n-  assert_test_pattern(h_obj, \"is_unlocked no_hash\");\n+  assert_mark_word_print_pattern(h_obj, \"is_unlocked no_hash\");\n@@ -94,1 +100,1 @@\n-  assert_test_pattern(h_obj, \"is_unlocked hash=0x\");\n+  assert_mark_word_print_pattern(h_obj, \"is_unlocked hash=0x\");\n@@ -110,0 +116,137 @@\n+\n+static void assert_unlocked_state(markWord mark) {\n+  EXPECT_FALSE(mark.has_displaced_mark_helper());\n+  EXPECT_FALSE(mark.is_fast_locked());\n+  EXPECT_FALSE(mark.has_monitor());\n+  EXPECT_FALSE(mark.is_locked());\n+  EXPECT_TRUE(mark.is_unlocked());\n+}\n+\n+static void assert_copy_set_hash(markWord mark) {\n+  const intptr_t hash = 4711;\n+  EXPECT_TRUE(mark.has_no_hash());\n+  markWord copy = mark.copy_set_hash(hash);\n+  EXPECT_EQ(hash, copy.hash());\n+  EXPECT_FALSE(copy.has_no_hash());\n+}\n+\n+static void assert_type(markWord mark) {\n+  EXPECT_FALSE(mark.is_flat_array());\n+  EXPECT_FALSE(mark.is_inline_type());\n+  EXPECT_FALSE(mark.is_larval_state());\n+  EXPECT_FALSE(mark.is_null_free_array());\n+}\n+\n+TEST_VM(markWord, prototype) {\n+  markWord mark = markWord::prototype();\n+  assert_unlocked_state(mark);\n+  EXPECT_TRUE(mark.is_neutral());\n+\n+  assert_type(mark);\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+\n+  assert_copy_set_hash(mark);\n+  assert_type(mark);\n+}\n+\n+static void assert_inline_type(markWord mark) {\n+  EXPECT_FALSE(mark.is_flat_array());\n+  EXPECT_TRUE(mark.is_inline_type());\n+  EXPECT_FALSE(mark.is_null_free_array());\n+}\n+\n+TEST_VM(markWord, inline_type_prototype) {\n+  markWord mark = markWord::inline_type_prototype();\n+  assert_unlocked_state(mark);\n+  EXPECT_FALSE(mark.is_neutral());\n+  assert_test_pattern(&mark, \" inline_type\");\n+\n+  assert_inline_type(mark);\n+  EXPECT_FALSE(mark.is_larval_state());\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+\n+  markWord larval = mark.enter_larval_state();\n+  EXPECT_TRUE(larval.is_larval_state());\n+  assert_inline_type(larval);\n+  assert_test_pattern(&larval, \" inline_type=larval\");\n+\n+  mark = larval.exit_larval_state();\n+  EXPECT_FALSE(mark.is_larval_state());\n+  assert_inline_type(mark);\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+}\n+\n+#if _LP64\n+\n+static void assert_flat_array_type(markWord mark) {\n+  EXPECT_TRUE(mark.is_flat_array());\n+  EXPECT_FALSE(mark.is_inline_type());\n+  EXPECT_FALSE(mark.is_larval_state());\n+}\n+\n+TEST_VM(markWord, null_free_flat_array_prototype) {\n+  markWord mark = markWord::flat_array_prototype(LayoutKind::NULL_FREE_NON_ATOMIC_FLAT);\n+  assert_unlocked_state(mark);\n+  EXPECT_TRUE(mark.is_neutral());\n+\n+  assert_flat_array_type(mark);\n+  EXPECT_TRUE(mark.is_null_free_array());\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+\n+  assert_copy_set_hash(mark);\n+  assert_flat_array_type(mark);\n+  EXPECT_TRUE(mark.is_null_free_array());\n+\n+  assert_test_pattern(&mark, \" flat_null_free_array\");\n+}\n+\n+TEST_VM(markWord, nullable_flat_array_prototype) {\n+  markWord mark = markWord::flat_array_prototype(LayoutKind::NULLABLE_ATOMIC_FLAT);\n+  assert_unlocked_state(mark);\n+  EXPECT_TRUE(mark.is_neutral());\n+\n+  assert_flat_array_type(mark);\n+  EXPECT_FALSE(mark.is_null_free_array());\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+\n+  assert_copy_set_hash(mark);\n+  assert_flat_array_type(mark);\n+  EXPECT_FALSE(mark.is_null_free_array());\n+\n+  assert_test_pattern(&mark, \" flat_array\");\n+}\n+\n+static void assert_null_free_array_type(markWord mark) {\n+  EXPECT_FALSE(mark.is_flat_array());\n+  EXPECT_FALSE(mark.is_inline_type());\n+  EXPECT_FALSE(mark.is_larval_state());\n+  EXPECT_TRUE(mark.is_null_free_array());\n+}\n+\n+TEST_VM(markWord, null_free_array_prototype) {\n+  markWord mark = markWord::null_free_array_prototype();\n+  assert_unlocked_state(mark);\n+  EXPECT_TRUE(mark.is_neutral());\n+\n+  assert_null_free_array_type(mark);\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+\n+  assert_copy_set_hash(mark);\n+  assert_null_free_array_type(mark);\n+\n+  assert_test_pattern(&mark, \" null_free_array\");\n+}\n+#endif \/\/ _LP64\n+\n","filename":"test\/hotspot\/gtest\/oops\/test_markWord.cpp","additions":147,"deletions":4,"binary":false,"changes":151,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-  runtime\n+  runtime \\\n@@ -65,0 +65,8 @@\n+hotspot_valhalla = \\\n+  runtime\/valhalla \\\n+  compiler\/valhalla \\\n+  serviceability\/jvmti\/valhalla\n+\n+hotspot_valhalla_runtime = \\\n+  runtime\/valhalla\n+\n@@ -207,0 +215,1 @@\n+  compiler\/valhalla\/ \\\n@@ -248,0 +257,7 @@\n+\n+tier1_compiler_no_valhalla = \\\n+  :tier1_compiler_1 \\\n+  :tier1_compiler_2 \\\n+  :tier1_compiler_3 \\\n+  -compiler\/valhalla\n+\n@@ -401,0 +417,4 @@\n+tier1_runtime_no_valhalla = \\\n+  :tier1_runtime \\\n+  -runtime\/valhalla\n+\n@@ -560,0 +580,1 @@\n+ -runtime\/cds\/appcds\/RewriteBytecodesInlineTest.java \\\n@@ -628,0 +649,4 @@\n+tier1_serviceability_no_valhalla = \\\n+  :tier1_serviceability \\\n+  -serviceability\/jvmti\/valhalla\n+\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":26,"deletions":1,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n- * @test\n+ * @test id=Z\n@@ -42,1 +42,1 @@\n- * @test\n+ * @test id=ZGen\n@@ -190,1 +190,1 @@\n-    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n@@ -199,1 +199,1 @@\n-    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n@@ -206,1 +206,1 @@\n-    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.REMAINING, \"2\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, Common.REMAINING, \"2\" }, phase = CompilePhase.FINAL_CODE)\n@@ -213,1 +213,1 @@\n-    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n@@ -221,1 +221,1 @@\n-    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n@@ -229,1 +229,1 @@\n-    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.REMAINING, \"2\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, Common.REMAINING, \"2\" }, phase = CompilePhase.FINAL_CODE)\n@@ -390,1 +390,1 @@\n-    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n@@ -397,1 +397,1 @@\n-    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n@@ -405,1 +405,1 @@\n-    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n@@ -413,2 +413,2 @@\n-    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n-    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    \/\/@IR(counts = { IRNode.Z_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n@@ -421,2 +421,2 @@\n-    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n-    @IR(counts = { IRNode.Z_GET_AND_SET_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, Common.REMAINING, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_COMPARE_AND_SWAP_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n","filename":"test\/hotspot\/jtreg\/compiler\/gcbarriers\/TestZGCBarrierElision.java","additions":15,"deletions":15,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -128,2 +128,1 @@\n-        vmOpts.add(\"-Xlog:all=warning:stdout:level,tags\");\n-        vmOpts.add(\"-Xlog:aot=off\");\n+        vmOpts.add(\"-Xlog:all=warning,aot=off:stdout:level,tags\");\n","filename":"test\/hotspot\/jtreg\/compiler\/intrinsics\/bmi\/BMITestRunner.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n- * @test id\n+ * @test\n@@ -60,0 +60,1 @@\n+                \"--enable-preview\",\n","filename":"test\/hotspot\/jtreg\/runtime\/ErrorHandling\/UncaughtNativeExceptionTest.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -91,1 +91,1 @@\n-            out.shouldMatch(\"public class \" + APP_DOT_CLASSNAME);\n+            out.shouldMatch(\"public identity class \" + APP_DOT_CLASSNAME);\n","filename":"test\/hotspot\/jtreg\/serviceability\/sa\/ClhsdbDumpclass.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+valhalla\/valuetypes\/SubstitutabilityTest.java                  8374025 generic-all\n","filename":"test\/jdk\/ProblemList-AotJdk.txt","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n-# Copyright (c) 2009, 2025, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2009, 2026, Oracle and\/or its affiliates. All rights reserved.\n@@ -507,0 +507,5 @@\n+java\/lang\/ModuleLayer\/LayerControllerTest.java                  8337048 generic-all\n+java\/lang\/ModuleLayer\/BasicLayerTest.java                       8337048 generic-all\n+\n+java\/lang\/Thread\/virtual\/stress\/Skynet.java#default             8342977 generic-all\n+\n@@ -524,0 +529,1 @@\n+com\/sun\/management\/HotSpotDiagnosticMXBean\/DumpThreadsWithEliminatedLock.java 8360599 generic-all\n@@ -545,0 +551,2 @@\n+java\/net\/CookieHandler\/B6644726.java                            8365811 generic-all\n+\n@@ -684,0 +692,4 @@\n+com\/sun\/jdi\/cds\/CDSBreakpointTest.java                          8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSDeleteAllBkptsTest.java                      8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSFieldWatchpoints.java                        8304168 generic-all\n+\n@@ -692,0 +704,1 @@\n+java\/util\/Collection\/MOAT.java 8375086 generic-all\n@@ -778,0 +791,1 @@\n+\n@@ -783,0 +797,19 @@\n+\n+############################################################################\n+\n+# valhalla\n+java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessValue.java 8367346 generic-all\n+\n+jdk\/classfile\/AccessFlagsTest.java 8366270 generic-all\n+jdk\/jfr\/event\/runtime\/TestClassLoaderStatsEvent.java 8366820 generic-all\n+\n+jdk\/internal\/vm\/Continuation\/Fuzz.java#default 8370177 generic-aarch64,generic-x64\n+jdk\/internal\/vm\/Continuation\/Fuzz.java#preserve-fp 8370177 generic-aarch64,generic-x64\n+\n+sun\/tools\/jhsdb\/BasicLauncherTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTestWithActiveProcess.java 8366806 generic-all\n+sun\/tools\/jhsdb\/JShellHeapDumpTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/JStackStressTest.java 8375470 macosx-aarch64\n+\n+tools\/sincechecker\/modules\/java.base\/JavaBaseCheckSince.java 8375574 generic-all\n","filename":"test\/jdk\/ProblemList.txt","additions":34,"deletions":1,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n- * @build jdk.test.lib.util.ModuleInfoWriter\n","filename":"test\/jdk\/java\/lang\/module\/ClassFileVersionsTest.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n- * @build jdk.test.lib.util.ModuleInfoWriter\n","filename":"test\/jdk\/java\/lang\/module\/ModuleDescriptorTest.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -388,0 +388,1 @@\n+        vmOptFinalFlag(map, \"TieredCompilation\");\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+ * @ignore Verifier error\n","filename":"test\/langtools\/tools\/javac\/patterns\/PrettyTest.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+    @SuppressWarnings(\"initialization\")\n","filename":"test\/lib\/jdk\/test\/lib\/NetworkConfiguration.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+    @SuppressWarnings(\"initialization\")\n","filename":"test\/lib\/jdk\/test\/lib\/containers\/docker\/DockerRunOptions.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -162,0 +162,14 @@\n+  private native Object[] getObjectsViaKlassOopMaps0(Object thing);\n+  public Object[] getObjectsViaKlassOopMaps(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaKlassOopMaps0(thing);\n+  }\n+\n+  private native Object[] getObjectsViaOopIterator0(Object thing);\n+  public Object[] getObjectsViaOopIterator(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaOopIterator0(thing);\n+  }\n+\n+  public native Object[] getObjectsViaFrameOopIterator(int depth);\n+\n@@ -740,4 +754,7 @@\n-  private final List<Function<String,Object>> flagsGetters = Arrays.asList(\n-    this::getBooleanVMFlag, this::getIntVMFlag, this::getUintVMFlag,\n-    this::getIntxVMFlag, this::getUintxVMFlag, this::getUint64VMFlag,\n-    this::getSizeTVMFlag, this::getStringVMFlag, this::getDoubleVMFlag);\n+  private final List<Function<String,Object>> flagsGetters;\n+  {\n+      flagsGetters = Arrays.asList(\n+          this::getBooleanVMFlag, this::getIntVMFlag, this::getUintVMFlag,\n+          this::getIntxVMFlag, this::getUintxVMFlag, this::getUint64VMFlag,\n+          this::getSizeTVMFlag, this::getStringVMFlag, this::getDoubleVMFlag);\n+  }\n@@ -782,0 +799,1 @@\n+  @SuppressWarnings(\"initialization\")\n","filename":"test\/lib\/jdk\/test\/whitebox\/WhiteBox.java","additions":22,"deletions":4,"binary":false,"changes":26,"status":"modified"}]}