{"files":[{"patch":"@@ -711,0 +711,2 @@\n+# Default disabled within Valhalla until support added (JDK-8348568)\n+#\n@@ -713,1 +715,1 @@\n-  UTIL_ARG_ENABLE(NAME: cds-archive-coh, DEFAULT: auto, RESULT: BUILD_CDS_ARCHIVE_COH,\n+  UTIL_ARG_ENABLE(NAME: cds-archive-coh, DEFAULT: false, RESULT: BUILD_CDS_ARCHIVE_COH,\n","filename":"make\/autoconf\/jdk-options.m4","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -55,1 +55,0 @@\n-    JAVAC_FLAGS := --enable-preview, \\\n@@ -63,1 +62,1 @@\n-    EXCLUDES := jdk\/test\/lib\/containers jdk\/test\/lib\/security, \\\n+    EXCLUDES := jdk\/test\/lib\/containers jdk\/test\/lib\/security org, \\\n@@ -67,0 +66,1 @@\n+    DISABLED_WARNINGS := try deprecation rawtypes unchecked serial cast removal preview restricted varargs dangling-doc-comments, \\\n@@ -72,0 +72,6 @@\n+        --add-exports jdk.compiler\/com.sun.tools.javac.api=ALL-UNNAMED \\\n+        --add-exports jdk.compiler\/com.sun.tools.javac.code=ALL-UNNAMED \\\n+        --add-exports jdk.compiler\/com.sun.tools.javac.comp=ALL-UNNAMED \\\n+        --add-exports jdk.compiler\/com.sun.tools.javac.main=ALL-UNNAMED \\\n+        --add-exports jdk.compiler\/com.sun.tools.javac.tree=ALL-UNNAMED \\\n+        --add-exports jdk.compiler\/com.sun.tools.javac.util=ALL-UNNAMED \\\n","filename":"make\/test\/BuildTestLib.gmk","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -122,0 +122,66 @@\n+\/\/ Implementation of LoadFlattenedArrayStub\n+\n+LoadFlattenedArrayStub::LoadFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr result, CodeEmitInfo* info) {\n+  _array = array;\n+  _index = index;\n+  _result = result;\n+  _scratch_reg = FrameMap::r0_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void LoadFlattenedArrayStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_array->as_register(), 1);\n+  ce->store_parameter(_index->as_register(), 0);\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(C1StubId::load_flat_array_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  if (_result->as_register() != r0) {\n+    __ mov(_result->as_register(), r0);\n+  }\n+  __ b(_continuation);\n+}\n+\n+\n+\/\/ Implementation of StoreFlattenedArrayStub\n+\n+StoreFlattenedArrayStub::StoreFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr value, CodeEmitInfo* info) {\n+  _array = array;\n+  _index = index;\n+  _value = value;\n+  _scratch_reg = FrameMap::r0_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+\n+void StoreFlattenedArrayStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_array->as_register(), 2);\n+  ce->store_parameter(_index->as_register(), 1);\n+  ce->store_parameter(_value->as_register(), 0);\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(C1StubId::store_flat_array_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  __ b(_continuation);\n+}\n+\n+\/\/ Implementation of SubstitutabilityCheckStub\n+SubstitutabilityCheckStub::SubstitutabilityCheckStub(LIR_Opr left, LIR_Opr right, CodeEmitInfo* info) {\n+  _left = left;\n+  _right = right;\n+  _scratch_reg = FrameMap::r0_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void SubstitutabilityCheckStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_left->as_register(), 1);\n+  ce->store_parameter(_right->as_register(), 0);\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(C1StubId::substitutability_check_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  __ b(_continuation);\n+}\n@@ -138,2 +204,0 @@\n-\n-\n@@ -179,1 +243,2 @@\n-NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info) {\n+NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result,\n+                                       CodeEmitInfo* info, bool is_null_free) {\n@@ -184,0 +249,1 @@\n+  _is_null_free = is_null_free;\n@@ -192,1 +258,7 @@\n-  __ far_call(RuntimeAddress(Runtime1::entry_for(C1StubId::new_object_array_id)));\n+\n+  if (_is_null_free) {\n+    __ far_call(RuntimeAddress(Runtime1::entry_for(C1StubId::new_null_free_array_id)));\n+  } else {\n+    __ far_call(RuntimeAddress(Runtime1::entry_for(C1StubId::new_object_array_id)));\n+  }\n+\n@@ -202,0 +274,10 @@\n+  if (_throw_ie_stub != nullptr) {\n+    \/\/ When we come here, _obj_reg has already been checked to be non-null.\n+    __ ldr(rscratch1, Address(_obj_reg->as_register(), oopDesc::mark_offset_in_bytes()));\n+    __ mov(rscratch2, markWord::inline_type_pattern);\n+    __ andr(rscratch1, rscratch1, rscratch2);\n+\n+    __ cmp(rscratch1, rscratch2);\n+    __ br(Assembler::EQ, *_throw_ie_stub->entry());\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":86,"deletions":4,"binary":false,"changes":90,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -41,0 +42,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -433,1 +435,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -477,0 +479,28 @@\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    ciType* return_type = compilation()->method()->return_type();\n+    if (return_type->is_inlinetype()) {\n+      ciInlineKlass* vk = return_type->as_inline_klass();\n+      if (vk->can_be_returned_as_fields()) {\n+        address unpack_handler = vk->unpack_handler();\n+        assert(unpack_handler != nullptr, \"must be\");\n+        __ far_call(RuntimeAddress(unpack_handler));\n+      }\n+    } else if (return_type->is_instance_klass() && (!return_type->is_loaded() || StressCallingConvention)) {\n+      Label skip;\n+      __ test_oop_is_not_inline_type(r0, rscratch2, skip);\n+\n+      \/\/ Load fields from a buffered value with an inline class specific handler\n+      __ load_klass(rscratch1 \/*dst*\/, r0 \/*src*\/);\n+      __ ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));\n+      \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+      __ cbz(rscratch1, skip);\n+      __ blr(rscratch1);\n+\n+      __ bind(skip);\n+    }\n+    \/\/ At this point, r0 points to the value object (for interpreter or C1 caller).\n+    \/\/ The fields of the object are copied into registers (for C2 caller).\n+  }\n+\n@@ -478,1 +508,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -490,0 +520,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -536,3 +570,1 @@\n-        if (patch_code == lir_patch_none) {\n-          jobject2reg(c->as_jobject(), dest->as_register());\n-        } else {\n+        if (patch_code != lir_patch_none) {\n@@ -540,0 +572,2 @@\n+        } else {\n+          jobject2reg(c->as_jobject(), dest->as_register());\n@@ -649,0 +683,2 @@\n+    \/\/ Non-null case is not handled on aarch64 but handled on x86\n+    \/\/ FIXME: do we need to add it here?\n@@ -996,0 +1032,14 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, nullptr);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n@@ -1187,1 +1237,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->is_null_free() ||\n@@ -1293,22 +1343,16 @@\n-  if (should_profile) {\n-    Register mdo  = klass_RInfo;\n-    __ mov_metadata(mdo, md->constant_encoding());\n-    Label not_null;\n-    __ cbnz(obj, not_null);\n-    \/\/ Object is null; update MDO and exit\n-    Address data_addr\n-      = __ form_address(rscratch2, mdo,\n-                        md->byte_offset_of_slot(data, DataLayout::flags_offset()),\n-                        0);\n-    __ ldrb(rscratch1, data_addr);\n-    __ orr(rscratch1, rscratch1, BitData::null_seen_byte_constant());\n-    __ strb(rscratch1, data_addr);\n-    __ b(*obj_is_null);\n-    __ bind(not_null);\n-\n-    Label update_done;\n-    Register recv = k_RInfo;\n-    __ load_klass(recv, obj);\n-    type_profile_helper(mdo, md, data, recv, &update_done);\n-    Address counter_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n-    __ addptr(counter_addr, DataLayout::counter_increment);\n+  if (op->need_null_check()) {\n+    if (should_profile) {\n+      Register mdo  = klass_RInfo;\n+      __ mov_metadata(mdo, md->constant_encoding());\n+      Label not_null;\n+      __ cbnz(obj, not_null);\n+      \/\/ Object is null; update MDO and exit\n+      Address data_addr\n+        = __ form_address(rscratch2, mdo,\n+                          md->byte_offset_of_slot(data, DataLayout::flags_offset()),\n+                          0);\n+      __ ldrb(rscratch1, data_addr);\n+      __ orr(rscratch1, rscratch1, BitData::null_seen_byte_constant());\n+      __ strb(rscratch1, data_addr);\n+      __ b(*obj_is_null);\n+      __ bind(not_null);\n@@ -1316,3 +1360,11 @@\n-    __ bind(update_done);\n-  } else {\n-    __ cbz(obj, *obj_is_null);\n+      Label update_done;\n+      Register recv = k_RInfo;\n+      __ load_klass(recv, obj);\n+      type_profile_helper(mdo, md, data, recv, &update_done);\n+      Address counter_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n+      __ addptr(counter_addr, DataLayout::counter_increment);\n+\n+      __ bind(update_done);\n+    } else {\n+      __ cbz(obj, *obj_is_null);\n+    }\n@@ -1477,0 +1529,106 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing from\/to an array that *may* be a flat array (the\n+  \/\/ declared type is Object[], abstract[], interface[] or VT.ref[]).\n+  \/\/ If this array is a flat array, take the slow path.\n+  __ test_flat_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+  if (!op->value()->is_illegal()) {\n+    \/\/ The array is not a flat array, but it might be null-free. If we are storing\n+    \/\/ a null into a null-free array, take the slow path (which will throw NPE).\n+    Label skip;\n+    __ cbnz(op->value()->as_register(), skip);\n+    __ test_null_free_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+    __ bind(skip);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ We are storing into an array that *may* be null-free (the declared type is\n+  \/\/ Object[], abstract[], interface[] or VT.ref[]).\n+  Label test_mark_word;\n+  Register tmp = op->tmp()->as_register();\n+  __ ldr(tmp, Address(op->array()->as_register(), oopDesc::mark_offset_in_bytes()));\n+  __ tst(tmp, markWord::unlocked_value);\n+  __ br(Assembler::NE, test_mark_word);\n+  __ load_prototype_header(tmp, op->array()->as_register());\n+  __ bind(test_mark_word);\n+  __ tst(tmp, markWord::null_free_array_bit_in_place);\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmp(left, right);\n+  __ br(Assembler::EQ, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  {\n+    __ cbz(left, L_oops_not_equal);\n+    __ cbz(right, L_oops_not_equal);\n+  }\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Inline type check -- if either of the operands is not a inline type,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are inline type\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = op->tmp1()->as_register();\n+    __ mov(tmp1, markWord::inline_type_pattern);\n+    __ ldr(rscratch1, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, rscratch1);\n+    __ ldr(rscratch1, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, rscratch1);\n+    __ cmp(tmp1, (u1)markWord::inline_type_pattern);\n+    __ br(Assembler::NE, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ b(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedClassPointers) {\n+      __ ldrw(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldrw(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpw(left_klass_op, right_klass_op);\n+    } else {\n+      __ ldr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmp(left_klass_op, right_klass_op);\n+    }\n+\n+    __ br(Assembler::EQ, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  \/\/ We've returned from the stub. R0 contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+  __ cbz(r0, L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+}\n+\n+\n@@ -1993,1 +2151,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2004,1 +2162,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2167,0 +2325,10 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {\n+  if (null_check) {\n+    __ cbz(obj, *slow_path->entry());\n+  }\n+  if (is_dest) {\n+    __ test_null_free_array_oop(obj, tmp, *slow_path->entry());\n+  } else {\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  }\n+}\n@@ -2185,0 +2353,6 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ b(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n@@ -2238,0 +2412,9 @@\n+  \/\/ Handle inline type arrays\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false, (flags & LIR_OpArrayCopy::src_null_check));\n+  }\n+\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true, (flags & LIR_OpArrayCopy::dst_null_check));\n+  }\n+\n@@ -2790,0 +2973,20 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Register obj = op->obj()->as_register();\n+  Register tmp = op->tmp()->as_pointer_register();\n+  bool not_null = op->not_null();\n+  int flag = op->flag();\n+\n+  Label not_inline_type;\n+  if (!not_null) {\n+    __ cbz(obj, not_inline_type);\n+  }\n+\n+  __ test_oop_is_not_inline_type(obj, tmp, not_inline_type);\n+\n+  Address mdo_addr = as_Address(op->mdp()->as_address_ptr(), rscratch2);\n+  __ ldrb(rscratch1, mdo_addr);\n+  __ orr(rscratch1, rscratch1, flag);\n+  __ strb(rscratch1, mdo_addr);\n+\n+  __ bind(not_inline_type);\n+}\n@@ -2930,0 +3133,4 @@\n+void LIR_Assembler::check_orig_pc() {\n+  __ ldr(rscratch2, frame_map()->address_for_orig_pc_addr());\n+  __ cmp(rscratch2, (u1)NULL_WORD);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":240,"deletions":33,"binary":false,"changes":273,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -105,0 +106,6 @@\n+void LIRGenerator::init_temps_for_substitutability_check(LIR_Opr& tmp1, LIR_Opr& tmp2) {\n+  tmp1 = new_register(T_INT);\n+  tmp2 = LIR_OprFact::illegalOpr;\n+}\n+\n+\n@@ -326,0 +333,6 @@\n+\n+  CodeStub* throw_ie_stub =\n+      x->maybe_inlinetype() ?\n+      new SimpleExceptionStub(C1StubId::throw_identity_exception_id, obj.result(), state_for(x)) :\n+      nullptr;\n+\n@@ -330,1 +343,1 @@\n-                        x->monitor_no(), info_for_exception, info);\n+                x->monitor_no(), info_for_exception, info, throw_ie_stub);\n@@ -1130,1 +1143,1 @@\n-  CodeEmitInfo* info = state_for(x, x->state());\n+  CodeEmitInfo* info = state_for(x, x->needs_state_before() ? x->state_before() : x->state());\n@@ -1133,5 +1146,6 @@\n-                       FrameMap::r10_oop_opr,\n-                       FrameMap::r11_oop_opr,\n-                       FrameMap::r4_oop_opr,\n-                       LIR_OprFact::illegalOpr,\n-                       FrameMap::r3_metadata_opr, info);\n+               \/* allow_inline *\/ false,\n+               FrameMap::r10_oop_opr,\n+               FrameMap::r11_oop_opr,\n+               FrameMap::r4_oop_opr,\n+               LIR_OprFact::illegalOpr,\n+               FrameMap::r3_metadata_opr, info);\n@@ -1193,2 +1207,2 @@\n-  CodeStub* slow_path = new NewObjectArrayStub(klass_reg, len, reg, info);\n-  ciKlass* obj = (ciKlass*) ciObjArrayKlass::make(x->klass());\n+  ciKlass* obj = (ciKlass*) x->exact_type();\n+  CodeStub* slow_path = new NewObjectArrayStub(klass_reg, len, reg, info, x->is_null_free());\n@@ -1198,0 +1212,1 @@\n+\n@@ -1199,1 +1214,1 @@\n-  __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, T_OBJECT, klass_reg, slow_path);\n+  __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, T_OBJECT, klass_reg, slow_path, true, x->is_null_free());\n@@ -1275,0 +1290,3 @@\n+  if (x->is_null_free()) {\n+    __ null_check(obj.result(), new CodeEmitInfo(info_for_exception));\n+  }\n@@ -1293,0 +1311,2 @@\n+\n+\n@@ -1296,1 +1316,2 @@\n-               x->profiled_method(), x->profiled_bci());\n+               x->profiled_method(), x->profiled_bci(), x->is_null_free());\n+\n@@ -1374,1 +1395,6 @@\n-  __ cmp(lir_cond(cond), left, right);\n+  if (x->substitutability_check()) {\n+    substitutability_check(x, *xin, *yin);\n+  } else {\n+    __ cmp(lir_cond(cond), left, right);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRGenerator_aarch64.cpp","additions":38,"deletions":12,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+  assert(index()->is_illegal() || disp() == 0, \"cannot set both index and displacement\");\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIR_aarch64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,0 +30,2 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n@@ -90,0 +92,6 @@\n+\n+    if (EnableValhalla) {\n+      \/\/ Mask always_locked bit such that we go to the slow path if object is an inline type\n+      andr(hdr, hdr, ~markWord::inline_type_bit_in_place);\n+    }\n+\n@@ -178,1 +186,1 @@\n-  if (UseCompactObjectHeaders) {\n+  if (UseCompactObjectHeaders || EnableValhalla) {\n@@ -184,0 +192,3 @@\n+  }\n+\n+  if (!UseCompactObjectHeaders) {\n@@ -320,2 +331,13 @@\n-void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes) {\n-  assert(bang_size_in_bytes >= framesize, \"stack bang size incorrect\");\n+void C1_MacroAssembler::build_frame_helper(int frame_size_in_bytes, int sp_offset_for_orig_pc, int sp_inc, bool reset_orig_pc, bool needs_stack_repair) {\n+  MacroAssembler::build_frame(frame_size_in_bytes);\n+\n+  if (needs_stack_repair) {\n+    save_stack_increment(sp_inc, frame_size_in_bytes);\n+  }\n+  if (reset_orig_pc) {\n+    \/\/ Zero orig_pc to detect deoptimization during buffering in the entry points\n+    str(zr, Address(sp, sp_offset_for_orig_pc));\n+  }\n+}\n+\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, bool needs_stack_repair, bool has_scalarized_args, Label* verified_inline_entry_label) {\n@@ -324,0 +346,1 @@\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n@@ -325,1 +348,2 @@\n-  MacroAssembler::build_frame(framesize);\n+\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, has_scalarized_args, needs_stack_repair);\n@@ -330,3 +354,4 @@\n-}\n-void C1_MacroAssembler::remove_frame(int framesize) {\n-  MacroAssembler::remove_frame(framesize);\n+  if (verified_inline_entry_label != nullptr) {\n+    \/\/ Jump here from the scalarized entry points that already created the frame.\n+    bind(*verified_inline_entry_label);\n+  }\n@@ -336,1 +361,0 @@\n-\n@@ -343,0 +367,62 @@\n+  if (C1Breakpoint) brk(1);\n+}\n+\n+int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label, bool is_inline_ro_entry) {\n+  assert(InlineTypePassFieldsAsArgs, \"sanity\");\n+  \/\/ Make sure there is enough stack space for this method's activation.\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+  generate_stack_overflow_check(bang_size_in_bytes);\n+\n+  GrowableArray<SigEntry>* sig    = ces->sig();\n+  GrowableArray<SigEntry>* sig_cc = is_inline_ro_entry ? ces->sig_cc_ro() : ces->sig_cc();\n+  VMRegPair* regs      = ces->regs();\n+  VMRegPair* regs_cc   = is_inline_ro_entry ? ces->regs_cc_ro() : ces->regs_cc();\n+  int args_on_stack    = ces->args_on_stack();\n+  int args_on_stack_cc = is_inline_ro_entry ? ces->args_on_stack_cc_ro() : ces->args_on_stack_cc();\n+\n+  assert(sig->length() <= sig_cc->length(), \"Zero-sized inline class not allowed!\");\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc->length());\n+  int args_passed = sig->length();\n+  int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);\n+\n+  \/\/ Create a temp frame so we can call into the runtime. It must be properly set up to accommodate GC.\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, true, ces->c1_needs_stack_repair());\n+\n+  \/\/ The runtime call might safepoint, make sure nmethod entry barrier is executed\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ C1 code is not hot enough to micro optimize the nmethod entry barrier with an out-of-line stub\n+  bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/, nullptr \/* guard *\/);\n+\n+  \/\/ FIXME -- call runtime only if we cannot in-line allocate all the incoming inline type args.\n+  mov(r19, (intptr_t) ces->method());\n+  if (is_inline_ro_entry) {\n+    far_call(RuntimeAddress(Runtime1::entry_for(C1StubId::buffer_inline_args_no_receiver_id)));\n+  } else {\n+    far_call(RuntimeAddress(Runtime1::entry_for(C1StubId::buffer_inline_args_id)));\n+  }\n+  int rt_call_offset = offset();\n+\n+  \/\/ The runtime call returns the new array in r20 instead of the usual r0\n+  \/\/ because r0 is also j_rarg7 which may be holding a live argument here.\n+  Register val_array = r20;\n+\n+  \/\/ Remove the temp frame\n+  MacroAssembler::remove_frame(frame_size_in_bytes);\n+\n+  \/\/ Check if we need to extend the stack for packing\n+  int sp_inc = 0;\n+  if (args_on_stack > args_on_stack_cc) {\n+    sp_inc = extend_stack_for_inline_args(args_on_stack);\n+  }\n+\n+  shuffle_inline_args(true, is_inline_ro_entry, sig_cc,\n+                      args_passed_cc, args_on_stack_cc, regs_cc, \/\/ from\n+                      args_passed, args_on_stack, regs,          \/\/ to\n+                      sp_inc, val_array);\n+\n+  \/\/ Create the real frame. Below jump will then skip over the stack banging and frame\n+  \/\/ setup code in the verified_inline_entry (which has a different real_frame_size).\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, sp_inc, false, ces->c1_needs_stack_repair());\n+\n+  b(verified_inline_entry_label);\n+  return rt_call_offset;\n@@ -345,0 +431,1 @@\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":95,"deletions":8,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -685,1 +685,2 @@\n-        int call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_instance), klass);\n+        int call_offset;\n+        call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_instance), klass);\n@@ -718,0 +719,1 @@\n+    case C1StubId::new_null_free_array_id:\n@@ -725,1 +727,1 @@\n-        } else {\n+        } else if (id == C1StubId::new_object_array_id) {\n@@ -727,0 +729,2 @@\n+        } else {\n+          __ set_info(\"new_null_free_array\", dont_gc_arguments);\n@@ -736,7 +740,22 @@\n-          int tag = ((id == C1StubId::new_type_array_id)\n-                     ? Klass::_lh_array_tag_type_value\n-                     : Klass::_lh_array_tag_obj_value);\n-          __ mov(rscratch1, tag);\n-          __ cmpw(t0, rscratch1);\n-          __ br(Assembler::EQ, ok);\n-          __ stop(\"assert(is an array klass)\");\n+          switch (id) {\n+          case C1StubId::new_type_array_id:\n+            __ cmpw(t0, Klass::_lh_array_tag_type_value);\n+            __ br(Assembler::EQ, ok);\n+            __ stop(\"assert(is a type array klass)\");\n+            break;\n+          case C1StubId::new_object_array_id:\n+            __ cmpw(t0, Klass::_lh_array_tag_obj_value); \/\/ new \"[Ljava\/lang\/Object;\"\n+            __ br(Assembler::EQ, ok);\n+            __ cmpw(t0, Klass::_lh_array_tag_vt_value);  \/\/ new \"[LVT;\"\n+            __ br(Assembler::EQ, ok);\n+            __ stop(\"assert(is an object or inline type array klass)\");\n+            break;\n+          case C1StubId::new_null_free_array_id:\n+            __ cmpw(t0, Klass::_lh_array_tag_vt_value);  \/\/ the array can be a flat array.\n+            __ br(Assembler::EQ, ok);\n+            __ cmpw(t0, Klass::_lh_array_tag_obj_value); \/\/ the array cannot be a flat array (due to InlineArrayElementMaxFlatSize, etc)\n+            __ br(Assembler::EQ, ok);\n+            __ stop(\"assert(is an object or inline type array klass)\");\n+            break;\n+          default:  ShouldNotReachHere();\n+          }\n@@ -753,1 +772,1 @@\n-        } else {\n+        } else if (id == C1StubId::new_object_array_id) {\n@@ -755,0 +774,3 @@\n+        } else {\n+          assert(id == C1StubId::new_null_free_array_id, \"must be\");\n+          call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_null_free_array), klass, length);\n@@ -789,0 +811,86 @@\n+    case C1StubId::buffer_inline_args_id:\n+    case C1StubId::buffer_inline_args_no_receiver_id:\n+      {\n+        const char* name = (id == C1StubId::buffer_inline_args_id) ?\n+          \"buffer_inline_args\" : \"buffer_inline_args_no_receiver\";\n+        StubFrame f(sasm, name, dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm);\n+        Register method = r19;   \/\/ Incoming\n+        address entry = (id == C1StubId::buffer_inline_args_id) ?\n+          CAST_FROM_FN_PTR(address, buffer_inline_args) :\n+          CAST_FROM_FN_PTR(address, buffer_inline_args_no_receiver);\n+        \/\/ This is called from a C1 method's scalarized entry point\n+        \/\/ where r0-r7 may be holding live argument values so we can't\n+        \/\/ return the result in r0 as the other stubs do. LR is used as\n+        \/\/ a temporay below to avoid the result being clobbered by\n+        \/\/ restore_live_registers.\n+        int call_offset = __ call_RT(lr, noreg, entry, method);\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers(sasm);\n+        __ mov(r20, lr);\n+        __ verify_oop(r20);  \/\/ r20: an array of buffered value objects\n+     }\n+     break;\n+\n+    case C1StubId::load_flat_array_id:\n+      {\n+        StubFrame f(sasm, \"load_flat_array\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(1, r0); \/\/ r0,: array\n+        f.load_argument(0, r1); \/\/ r1,: index\n+        int call_offset = __ call_RT(r0, noreg, CAST_FROM_FN_PTR(address, load_flat_array), r0, r1);\n+\n+        \/\/ Ensure the stores that initialize the buffer are visible\n+        \/\/ before any subsequent store that publishes this reference.\n+        __ membar(Assembler::StoreStore);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_r0(sasm);\n+\n+        \/\/ r0: loaded element at array[index]\n+        __ verify_oop(r0);\n+      }\n+      break;\n+\n+    case C1StubId::store_flat_array_id:\n+      {\n+        StubFrame f(sasm, \"store_flat_array\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 4);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(2, r0); \/\/ r0: array\n+        f.load_argument(1, r1); \/\/ r1: index\n+        f.load_argument(0, r2); \/\/ r2: value\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, store_flat_array), r0, r1, r2);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_r0(sasm);\n+      }\n+      break;\n+\n+    case C1StubId::substitutability_check_id:\n+      {\n+        StubFrame f(sasm, \"substitutability_check\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(1, r1); \/\/ r1,: left\n+        f.load_argument(0, r2); \/\/ r2,: right\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, substitutability_check), r1, r2);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_r0(sasm);\n+\n+        \/\/ r0,: are the two operands substitutable\n+      }\n+      break;\n+\n@@ -828,1 +936,1 @@\n-      { StubFrame f(sasm, \"throw_incompatible_class_cast_exception\", dont_gc_arguments, does_not_return);\n+      { StubFrame f(sasm, \"throw_incompatible_class_change_error\", dont_gc_arguments, does_not_return);\n@@ -833,0 +941,12 @@\n+    case C1StubId::throw_illegal_monitor_state_exception_id:\n+      { StubFrame f(sasm, \"throw_illegal_monitor_state_exception\", dont_gc_arguments);\n+        oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_illegal_monitor_state_exception), false);\n+      }\n+      break;\n+\n+    case C1StubId::throw_identity_exception_id:\n+      { StubFrame f(sasm, \"throw_identity_exception\", dont_gc_arguments);\n+        oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_identity_exception), true);\n+      }\n+      break;\n+\n@@ -1042,0 +1162,2 @@\n+      \/\/ FIXME: For unhandled trap_id this code fails with assert during vm intialization\n+      \/\/ rather than insert a call to unimplemented_entry\n@@ -1049,0 +1171,2 @@\n+\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Runtime1_aarch64.cpp","additions":135,"deletions":11,"binary":false,"changes":146,"status":"modified"},{"patch":"@@ -49,0 +49,23 @@\n+void C2_MacroAssembler::entry_barrier() {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  if (BarrierSet::barrier_set()->barrier_set_nmethod() != nullptr) {\n+    \/\/ Dummy labels for just measuring the code size\n+    Label dummy_slow_path;\n+    Label dummy_continuation;\n+    Label dummy_guard;\n+    Label* slow_path = &dummy_slow_path;\n+    Label* continuation = &dummy_continuation;\n+    Label* guard = &dummy_guard;\n+    if (!Compile::current()->output()->in_scratch_emit_size()) {\n+      \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+      C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n+      Compile::current()->output()->add_stub(stub);\n+      slow_path = &stub->entry();\n+      continuation = &stub->continuation();\n+      guard = &stub->guard();\n+    }\n+    \/\/ In the C2 code, we move the non-hot part of nmethod entry barriers out-of-line to a stub.\n+    bs->nmethod_entry_barrier(this, slow_path, continuation, guard);\n+  }\n+}\n+\n@@ -178,0 +201,5 @@\n+    if (EnableValhalla) {\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      andr(tmp, tmp, ~((int) markWord::inline_type_bit_in_place));\n+    }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":28,"deletions":0,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -156,2 +156,2 @@\n-      sender_unextended_sp = sender_sp;\n-      saved_fp = (intptr_t*) *(sender_sp - frame::sender_sp_offset);\n+      intptr_t **saved_fp_addr = (intptr_t**) (sender_sp - frame::sender_sp_offset);\n+      saved_fp = *saved_fp_addr;\n@@ -162,1 +162,4 @@\n-    }\n+      \/\/ Repair the sender sp if this is a method with scalarized inline type args\n+      sender_sp = repair_sender_sp(sender_sp, saved_fp_addr);\n+      sender_unextended_sp = sender_sp;\n+    }\n@@ -825,0 +828,16 @@\n+\/\/ Check for a method with scalarized inline type arguments that needs\n+\/\/ a stack repair and return the repaired sender stack pointer.\n+intptr_t* frame::repair_sender_sp(intptr_t* sender_sp, intptr_t** saved_fp_addr) const {\n+  nmethod* nm = _cb->as_nmethod_or_null();\n+  if (nm != nullptr && nm->needs_stack_repair()) {\n+    \/\/ The stack increment resides just below the saved FP on the stack and\n+    \/\/ records the total frame size excluding the two words for saving FP and LR.\n+    intptr_t* sp_inc_addr = (intptr_t*) (saved_fp_addr - 1);\n+    assert(*sp_inc_addr % StackAlignmentInBytes == 0, \"sp_inc not aligned\");\n+    int real_frame_size = (*sp_inc_addr \/ wordSize) + 2;\n+    assert(real_frame_size >= _cb->frame_size() && real_frame_size <= 1000000, \"invalid frame size\");\n+    sender_sp = unextended_sp() + real_frame_size;\n+  }\n+  return sender_sp;\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.cpp","additions":22,"deletions":3,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -175,1 +175,21 @@\n-  __ push_call_clobbered_registers();\n+  \/\/ save the live input values\n+  RegSet saved = RegSet::of(pre_val);\n+  FloatRegSet fsaved;\n+\n+  \/\/ Barriers might be emitted when converting between (scalarized) calling\n+  \/\/ conventions for inline types. Save all argument registers before calling\n+  \/\/ into the runtime.\n+  if (EnableValhalla && InlineTypePassFieldsAsArgs) {\n+    if (tosca_live) saved += RegSet::of(r0);\n+    if (obj != noreg) saved += RegSet::of(obj);\n+    saved += RegSet::of(j_rarg0, j_rarg1, j_rarg2, j_rarg3);\n+    saved += RegSet::of(j_rarg4, j_rarg5, j_rarg6, j_rarg7);\n+\n+    fsaved += FloatRegSet::of(j_farg0, j_farg1, j_farg2, j_farg3);\n+    fsaved += FloatRegSet::of(j_farg4, j_farg5, j_farg6, j_farg7);\n+\n+    __ push(saved, sp);\n+    __ push_fp(fsaved, sp);\n+  } else {\n+    __ push_call_clobbered_registers();\n+  }\n@@ -196,1 +216,6 @@\n-  __ pop_call_clobbered_registers();\n+  if (EnableValhalla && InlineTypePassFieldsAsArgs) {\n+  __ pop_fp(fsaved, sp);\n+  __ pop(saved, sp);\n+  } else {\n+    __ pop_call_clobbered_registers();\n+  }\n@@ -267,0 +292,1 @@\n+\n@@ -269,0 +295,13 @@\n+  FloatRegSet fsaved;\n+\n+  \/\/ Barriers might be emitted when converting between (scalarized) calling\n+  \/\/ conventions for inline types. Save all argument registers before calling\n+  \/\/ into the runtime.\n+  if (EnableValhalla && InlineTypePassFieldsAsArgs) {\n+    saved += RegSet::of(j_rarg0, j_rarg1, j_rarg2, j_rarg3);\n+    saved += RegSet::of(j_rarg4, j_rarg5, j_rarg6, j_rarg7);\n+\n+    fsaved += FloatRegSet::of(j_farg0, j_farg1, j_farg2, j_farg3);\n+    fsaved += FloatRegSet::of(j_farg4, j_farg5, j_farg6, j_farg7);\n+  }\n+\n@@ -270,0 +309,1 @@\n+  __ push_fp(fsaved, sp);\n@@ -271,0 +311,1 @@\n+  __ pop_fp(fsaved, sp);\n@@ -394,0 +435,10 @@\n+\n+  bool in_heap = (decorators & IN_HEAP) != 0;\n+  bool as_normal = (decorators & AS_NORMAL) != 0;\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n+\n+  bool needs_pre_barrier = as_normal && !dest_uninitialized;\n+  bool needs_post_barrier = (val != noreg && in_heap);\n+\n+  assert_different_registers(val, tmp1, tmp2, tmp3);\n+\n@@ -403,8 +454,10 @@\n-  g1_write_barrier_pre(masm,\n-                       tmp3 \/* obj *\/,\n-                       tmp2 \/* pre_val *\/,\n-                       rthread \/* thread *\/,\n-                       tmp1  \/* tmp1 *\/,\n-                       rscratch2  \/* tmp2 *\/,\n-                       val != noreg \/* tosca_live *\/,\n-                       false \/* expand_call *\/);\n+  if (needs_pre_barrier) {\n+    g1_write_barrier_pre(masm,\n+                         tmp3 \/* obj *\/,\n+                         tmp2 \/* pre_val *\/,\n+                         rthread \/* thread *\/,\n+                         tmp1  \/* tmp1 *\/,\n+                         rscratch2  \/* tmp2 *\/,\n+                         val != noreg \/* tosca_live *\/,\n+                         false \/* expand_call *\/);\n+  }\n@@ -417,3 +470,5 @@\n-    if (UseCompressedOops) {\n-      new_val = rscratch2;\n-      __ mov(new_val, val);\n+    if (needs_post_barrier) {\n+      if (UseCompressedOops) {\n+        new_val = rscratch2;\n+        __ mov(new_val, val);\n+      }\n@@ -421,0 +476,1 @@\n+\n@@ -422,6 +478,8 @@\n-    g1_write_barrier_post(masm,\n-                          tmp3 \/* store_adr *\/,\n-                          new_val \/* new_val *\/,\n-                          rthread \/* thread *\/,\n-                          tmp1 \/* tmp1 *\/,\n-                          tmp2 \/* tmp2 *\/);\n+    if (needs_post_barrier) {\n+      g1_write_barrier_post(masm,\n+                            tmp3 \/* store_adr *\/,\n+                            new_val \/* new_val *\/,\n+                            rthread \/* thread *\/,\n+                            tmp1 \/* tmp1 *\/,\n+                            tmp2 \/* tmp2 *\/);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.cpp","additions":77,"deletions":19,"binary":false,"changes":96,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/barrierSetRuntime.hpp\"\n@@ -52,0 +53,1 @@\n+\n@@ -89,0 +91,2 @@\n+  bool is_not_null = (decorators & IS_NOT_NULL) != 0;\n+\n@@ -92,5 +96,6 @@\n-    val = val == noreg ? zr : val;\n-      if (UseCompressedOops) {\n-        assert(!dst.uses(val), \"not enough registers\");\n-        if (val != zr) {\n-          __ encode_heap_oop(val);\n+      if (val == noreg) {\n+        assert(!is_not_null, \"inconsistent access\");\n+        if (UseCompressedOops) {\n+          __ strw(zr, dst);\n+        } else {\n+          __ str(zr, dst);\n@@ -99,2 +104,11 @@\n-        __ strw(val, dst);\n-        __ str(val, dst);\n+        if (UseCompressedOops) {\n+          assert(!dst.uses(val), \"not enough registers\");\n+          if (is_not_null) {\n+            __ encode_heap_oop_not_null(val);\n+          } else {\n+            __ encode_heap_oop(val);\n+          }\n+          __ strw(val, dst);\n+        } else {\n+          __ str(val, dst);\n+        }\n@@ -105,0 +119,1 @@\n+      assert(val != noreg, \"not supported\");\n@@ -125,0 +140,13 @@\n+void BarrierSetAssembler::flat_field_copy(MacroAssembler* masm, DecoratorSet decorators,\n+                                     Register src, Register dst, Register inline_layout_info) {\n+  \/\/ flat_field_copy implementation is fairly complex, and there are not any\n+  \/\/ \"short-cuts\" to be made from asm. What there is, appears to have the same\n+  \/\/ cost in C++, so just \"call_VM_leaf\" for now rather than maintain hundreds\n+  \/\/ of hand-rolled instructions...\n+  if (decorators & IS_DEST_UNINITIALIZED) {\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, BarrierSetRuntime::value_copy_is_dest_uninitialized), src, dst, inline_layout_info);\n+  } else {\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, BarrierSetRuntime::value_copy), src, dst, inline_layout_info);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.cpp","additions":35,"deletions":7,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+public:\n@@ -84,2 +85,1 @@\n-public:\n-  NativeNMethodBarrier(nmethod* nm): _nm(nm) {\n+  NativeNMethodBarrier(nmethod* nm, address alt_entry_instruction_address = 0): _nm(nm) {\n@@ -88,0 +88,1 @@\n+      assert(alt_entry_instruction_address == 0, \"invariant\");\n@@ -98,1 +99,2 @@\n-        _instruction_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset(nm);\n+        _instruction_address = (alt_entry_instruction_address != 0) ? alt_entry_instruction_address :\n+          nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset(nm);\n@@ -184,0 +186,25 @@\n+static void set_value(nmethod* nm, jint val) {\n+  NativeNMethodBarrier cmp1 = NativeNMethodBarrier(nm);\n+  cmp1.set_value(val);\n+\n+  if (!nm->is_osr_method() && nm->method()->has_scalarized_args()) {\n+    \/\/ nmethods with scalarized arguments have multiple entry points that each have an own nmethod entry barrier\n+    assert(nm->verified_entry_point() != nm->verified_inline_entry_point(), \"scalarized entry point not found\");\n+    address method_body = nm->is_compiled_by_c1() ? nm->verified_inline_entry_point() : nm->verified_entry_point();\n+    address entry_point2 = nm->is_compiled_by_c1() ? nm->verified_entry_point() : nm->verified_inline_entry_point();\n+\n+    int barrier_offset = cmp1.instruction_address() - method_body;\n+    NativeNMethodBarrier cmp2 = NativeNMethodBarrier(nm, entry_point2 + barrier_offset);\n+    assert(cmp1.instruction_address() != cmp2.instruction_address(), \"sanity\");\n+    debug_only(cmp2.verify());\n+    cmp2.set_value(val);\n+\n+    if (method_body != nm->verified_inline_ro_entry_point() && entry_point2 != nm->verified_inline_ro_entry_point()) {\n+      NativeNMethodBarrier cmp3 = NativeNMethodBarrier(nm, nm->verified_inline_ro_entry_point() + barrier_offset);\n+      assert(cmp1.instruction_address() != cmp3.instruction_address() && cmp2.instruction_address() != cmp3.instruction_address(), \"sanity\");\n+      debug_only(cmp3.verify());\n+      cmp3.set_value(val);\n+    }\n+  }\n+}\n+\n@@ -200,2 +227,1 @@\n-  NativeNMethodBarrier barrier(nm);\n-  barrier.set_value(value);\n+  set_value(nm, value);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetNMethod_aarch64.cpp","additions":31,"deletions":5,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -91,1 +91,9 @@\n-      store_check(masm, dst.base(), dst);\n+      if (tmp3 != noreg) {\n+        \/\/ Called by MacroAssembler::pack_inline_helper. We cannot corrupt the dst.base() register\n+        __ mov(tmp3, dst.base());\n+        store_check(masm, tmp3, dst);\n+      } else {\n+        \/\/ It's OK to corrupt the dst.base() register.\n+        store_check(masm, dst.base(), dst);\n+      }\n+\n@@ -97,0 +105,1 @@\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/cardTableBarrierSetAssembler_aarch64.cpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/constMethodFlags.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -211,0 +213,59 @@\n+void InterpreterMacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                                  Register t1, Register t2,\n+                                                  bool clear_fields, Label& alloc_failed) {\n+  MacroAssembler::allocate_instance(klass, new_obj, t1, t2, clear_fields, alloc_failed);\n+  if (DTraceMethodProbes) {\n+      \/\/ Trigger dtrace event for fastpath\n+    push(atos);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), new_obj);\n+    pop(atos);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::read_flat_field(Register entry,\n+                                                Register field_index, Register field_offset,\n+                                                Register temp, Register obj) {\n+  Label alloc_failed, empty_value, done;\n+  const Register src = field_offset;\n+  const Register alloc_temp = r10;\n+  const Register dst_temp   = field_index;\n+  const Register layout_info = temp;\n+  assert_different_registers(obj, entry, field_index, field_offset, temp, alloc_temp);\n+\n+  \/\/ Grab the inline field klass\n+  ldr(rscratch1, Address(entry, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+  inline_layout_info(rscratch1, field_index, layout_info);\n+\n+  const Register field_klass = dst_temp;\n+  ldr(field_klass, Address(layout_info, in_bytes(InlineLayoutInfo::klass_offset())));\n+\n+  \/\/ check for empty value klass\n+  test_klass_is_empty_inline_type(field_klass, rscratch1, empty_value);\n+\n+  \/\/ allocate buffer\n+  push(obj); \/\/ save holder\n+  allocate_instance(field_klass, obj, alloc_temp, rscratch2, false, alloc_failed);\n+\n+  \/\/ Have an oop instance buffer, copy into it\n+  payload_address(obj, dst_temp, field_klass);  \/\/ danger, uses rscratch1\n+  pop(alloc_temp);             \/\/ restore holder\n+  lea(src, Address(alloc_temp, field_offset));\n+  \/\/ call_VM_leaf, clobbers a few regs, save restore new obj\n+  push(obj);\n+  flat_field_copy(IS_DEST_UNINITIALIZED, src, dst_temp, layout_info);\n+  pop(obj);\n+  b(done);\n+\n+  bind(empty_value);\n+  get_empty_inline_type_oop(field_klass, alloc_temp, obj);\n+  b(done);\n+\n+  bind(alloc_failed);\n+  pop(obj);\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flat_field),\n+          obj, entry);\n+\n+  bind(done);\n+  membar(Assembler::StoreStore);\n+}\n+\n@@ -245,1 +306,2 @@\n-                                                  Label& ok_is_subtype) {\n+                                                  Label& ok_is_subtype,\n+                                                  bool profile) {\n@@ -251,1 +313,3 @@\n-  profile_typecheck(r2, Rsub_klass, r5); \/\/ blows r2, reloads r5\n+  if (profile) {\n+    profile_typecheck(r2, Rsub_klass, r5); \/\/ blows r2, reloads r5\n+  }\n@@ -625,0 +689,1 @@\n+\n@@ -650,0 +715,31 @@\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    Label skip;\n+    test_oop_is_not_inline_type(r0, rscratch2, skip);\n+\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+    load_klass(rscratch1 \/*dst*\/, r0 \/*src*\/);\n+    ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));\n+    \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+    cbz(rscratch1, skip);\n+\n+    blr(rscratch1);\n+#ifdef ASSERT\n+    \/\/ TODO 8284443 Enable\n+    if (StressCallingConvention && false) {\n+      Label skip_stress;\n+      ldr(rscratch1, Address(rfp, frame::interpreter_frame_method_offset * wordSize));\n+      ldrw(rscratch1, Address(rscratch1, Method::flags_offset()));\n+      tstw(rscratch1, MethodFlags::has_scalarized_return_flag());\n+      br(Assembler::EQ, skip_stress);\n+      load_klass(r0, r0);\n+      orr(r0, r0, 1);\n+      bind(skip_stress);\n+    }\n+#endif\n+    bind(skip);\n+    \/\/ Check above kills sender esp in rscratch2. Reload it.\n+    ldr(rscratch2, Address(rfp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+  }\n+\n@@ -710,0 +806,4 @@\n+      if (EnableValhalla) {\n+        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+        andr(swap_reg, swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+      }\n@@ -1063,1 +1163,1 @@\n-void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp) {\n+void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp, bool acmp) {\n@@ -1075,1 +1175,1 @@\n-    update_mdp_by_constant(mdp, in_bytes(BranchData::branch_data_size()));\n+    update_mdp_by_constant(mdp, acmp ? in_bytes(ACmpData::acmp_data_size()) : in_bytes(BranchData::branch_data_size()));\n@@ -1398,0 +1498,114 @@\n+template <class ArrayData> void InterpreterMacroAssembler::profile_array_type(Register mdp,\n+                                                                              Register array,\n+                                                                              Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, array);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayData::array_offset())));\n+\n+    Label not_flat;\n+    test_non_flat_array_oop(array, tmp, not_flat);\n+\n+    set_mdp_flag_at(mdp, ArrayData::flat_array_byte_constant());\n+\n+    bind(not_flat);\n+\n+    Label not_null_free;\n+    test_non_null_free_array_oop(array, tmp, not_null_free);\n+\n+    set_mdp_flag_at(mdp, ArrayData::null_free_array_byte_constant());\n+\n+    bind(not_null_free);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+template void InterpreterMacroAssembler::profile_array_type<ArrayLoadData>(Register mdp,\n+                                                                           Register array,\n+                                                                           Register tmp);\n+template void InterpreterMacroAssembler::profile_array_type<ArrayStoreData>(Register mdp,\n+                                                                            Register array,\n+                                                                            Register tmp);\n+\n+void InterpreterMacroAssembler::profile_multiple_element_types(Register mdp, Register element, Register tmp, const Register tmp2) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    Label done, update;\n+    cbnz(element, update);\n+    set_mdp_flag_at(mdp, BitData::null_seen_byte_constant());\n+    b(done);\n+\n+    bind(update);\n+    load_klass(tmp, element);\n+\n+    \/\/ Record the object type.\n+    record_klass_in_profile(tmp, mdp, tmp2);\n+\n+    bind(done);\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayStoreData::array_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+\n+void InterpreterMacroAssembler::profile_element_type(Register mdp,\n+                                                     Register element,\n+                                                     Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, element);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadData::element_offset())));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadData::array_load_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_acmp(Register mdp,\n+                                             Register left,\n+                                             Register right,\n+                                             Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, left);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::left_offset())));\n+\n+    Label left_not_inline_type;\n+    test_oop_is_not_inline_type(left, tmp, left_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::left_inline_type_byte_constant());\n+    bind(left_not_inline_type);\n+\n+    mov(tmp, right);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::right_offset())));\n+\n+    Label right_not_inline_type;\n+    test_oop_is_not_inline_type(right, tmp, right_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::right_inline_type_byte_constant());\n+    bind(right_not_inline_type);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n@@ -1699,1 +1913,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -1745,1 +1959,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":220,"deletions":6,"binary":false,"changes":226,"status":"modified"},{"patch":"@@ -176,0 +176,4 @@\n+void InterpreterRuntime::SignatureHandlerGenerator::pass_valuetype() {\n+   pass_object();\n+}\n+\n@@ -260,0 +264,5 @@\n+  virtual void pass_valuetype() {\n+    \/\/ values are handled with oops, like objects\n+    pass_object();\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/interpreterRT_aarch64.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/jfieldIDWorkaround.hpp\"\n@@ -155,1 +156,1 @@\n-  __ lsr(roffset, c_rarg2, 2);                \/\/ offset\n+  __ lsr(roffset, c_rarg2, jfieldIDWorkaround::offset_shift);       \/\/ offset\n","filename":"src\/hotspot\/cpu\/aarch64\/jniFastGetField_aarch64.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -50,0 +51,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -56,0 +58,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -59,0 +62,1 @@\n+#include \"vmreg_aarch64.inline.hpp\"\n@@ -1188,0 +1192,36 @@\n+void MacroAssembler::get_default_value_oop(Register inline_klass, Register temp_reg, Register obj) {\n+  assert_different_registers(inline_klass, temp_reg, obj, rscratch2);\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_default_value_oop from non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  Register offset = temp_reg;\n+  \/\/ Getting the offset of the pre-allocated default value\n+  ldr(offset, Address(inline_klass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())));\n+  load_sized_value(offset, Address(offset, in_bytes(InlineKlass::default_value_offset_offset())), sizeof(int), true \/*is_signed*\/);\n+\n+  \/\/ Getting the mirror\n+  ldr(obj, Address(inline_klass, in_bytes(Klass::java_mirror_offset())));\n+  resolve_oop_handle(obj, inline_klass, rscratch2);\n+\n+  \/\/ Getting the pre-allocated default value from the mirror\n+  Address field(obj, offset);\n+  load_heap_oop(obj, field, inline_klass, rscratch2);\n+}\n+\n+void MacroAssembler::get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_empty_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_empty_value from non-empty inline klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  get_default_value_oop(inline_klass, temp_reg, obj);\n+}\n+\n@@ -2063,1 +2103,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -2096,1 +2140,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -2194,0 +2242,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2239,0 +2291,106 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  assert_different_registers(markword, rscratch2);\n+  mov(rscratch2, markWord::inline_type_mask_in_place);\n+  andr(markword, markword, rscratch2);\n+  mov(rscratch2, markWord::inline_type_pattern);\n+  cmp(markword, rscratch2);\n+  br(Assembler::EQ, is_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type) {\n+  ldrh(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  andr(temp_reg, temp_reg, JVM_ACC_IDENTITY);\n+  cbz(temp_reg, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type) {\n+  assert_different_registers(tmp, rscratch1);\n+  cbz(object, not_inline_type);\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  ldr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  mov(rscratch1, is_inline_type_mask);\n+  andr(tmp, tmp, rscratch1);\n+  cmp(tmp, rscratch1);\n+  br(Assembler::NE, not_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(klass, temp_reg, done_check);\n+    stop(\"test_klass_is_empty_inline_type with non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  ldrw(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));\n+  andr(temp_reg, temp_reg, InstanceKlassFlags::is_empty_inline_type_value());\n+  cbnz(temp_reg, is_empty_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::is_null_free_inline_type_shift, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbz(flags, ResolvedFieldEntry::is_null_free_inline_type_shift, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_flat(Register flags, Register temp_reg, Label& is_flat) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::is_flat_shift, is_flat);\n+}\n+\n+void MacroAssembler::test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::has_null_marker_shift, has_null_marker);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  ldr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  tst(temp_reg, markWord::unlocked_value);\n+  br(Assembler::NE, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  load_prototype_header(temp_reg, oop);\n+\n+  bind(test_mark_word);\n+  andr(temp_reg, temp_reg, test_bit);\n+  if (jmp_set) {\n+    cbnz(temp_reg, jmp_label);\n+  } else {\n+    cbz(temp_reg, jmp_label);\n+  }\n+}\n+\n+void MacroAssembler::test_flat_array_oop(Register oop, Register temp_reg, Label& is_flat_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flat_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flat_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+}\n+\n+void MacroAssembler::test_flat_array_layout(Register lh, Label& is_flat_array) {\n+  tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  br(Assembler::NE, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_layout(Register lh, Label& is_non_flat_array) {\n+  tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  br(Assembler::EQ, is_non_flat_array);\n+}\n+\n@@ -5026,0 +5184,10 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(dst, src);\n+  } else if (UseCompressedClassPointers) {\n+    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -5136,0 +5304,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src) {\n+  load_klass(dst, src);\n+  ldr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -5476,0 +5649,40 @@\n+void MacroAssembler::flat_field_copy(DecoratorSet decorators, Register src, Register dst,\n+                                     Register inline_layout_info) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->flat_field_copy(this, decorators, src, dst, inline_layout_info);\n+}\n+\n+void MacroAssembler::payload_offset(Register inline_klass, Register offset) {\n+  ldr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  ldrw(offset, Address(offset, InlineKlass::payload_offset_offset()));\n+}\n+\n+void MacroAssembler::payload_address(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->payload_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  payload_offset(inline_klass, offset);\n+  if (data == oop) {\n+    add(data, data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rscratch1, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  ldrw(rscratch1, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  lsr(rscratch1, rscratch1, Klass::_lh_log2_element_size_shift);\n+  andr(rscratch1, rscratch1, Klass::_lh_log2_element_size_mask);\n+  lslv(index, index, rscratch1);\n+\n+  add(data, array, index);\n+  add(data, data, arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT));\n+}\n+\n@@ -5552,0 +5765,104 @@\n+\/\/ Object \/ value buffer allocation...\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == r0, \"needs to be r0\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  ldrw(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it is malformed in some way\n+  tst(layout_size, Klass::_lh_instance_slow_path_bit);\n+  br(Assembler::NE, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  if (UseTLAB) {\n+    push(klass);\n+    tlab_allocate(new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      b(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      b(initialize_object);\n+    }\n+\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      int header_size = oopDesc::header_size() * HeapWordSize;\n+      assert(is_aligned(header_size, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n+      subs(layout_size, layout_size, header_size);\n+      br(Assembler::EQ, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      tst(layout_size, 7);\n+      br(Assembler::EQ, L);\n+      stop(\"object size is not multiple of 8 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      lsr(layout_size, layout_size, LogBytesPerLong);\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        Register base = t2;\n+\n+        bind(loop);\n+        add(rscratch1, new_obj, layout_size, Assembler::LSL, LogBytesPerLong);\n+        str(zr, Address(rscratch1, header_size - 1*oopSize));\n+        subs(layout_size, layout_size, 1);\n+        br(Assembler::NE, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    if (UseCompactObjectHeaders || EnableValhalla) {\n+      ldr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+      str(mark_word, Address(new_obj, oopDesc::mark_offset_in_bytes()));\n+    } else {\n+      mov(mark_word, (intptr_t)markWord::prototype().value());\n+      str(mark_word, Address(new_obj, oopDesc::mark_offset_in_bytes()));\n+    }\n+    if (!UseCompactObjectHeaders) {\n+      store_klass_gap(new_obj, zr);  \/\/ zero klass gap for compressed oops\n+      mov(t2, klass);                \/\/ preserve klass\n+      store_klass(new_obj, t2);      \/\/ src klass reg is potentially compressed\n+    }\n+    \/\/ TODO: Valhalla removed SharedRuntime::dtrace_object_alloc from here ?\n+\n+    b(done);\n+  }\n+\n+  if (UseTLAB) {\n+    bind(slow_case);\n+    pop(klass);\n+  }\n+  bind(slow_case_no_pop);\n+  b(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -5623,0 +5940,20 @@\n+void MacroAssembler::get_inline_type_field_klass(Register holder_klass, Register index, Register inline_klass) {\n+  inline_layout_info(holder_klass, index, inline_klass);\n+  ldr(inline_klass, Address(inline_klass, InlineLayoutInfo::klass_offset()));\n+}\n+\n+void MacroAssembler::inline_layout_info(Register holder_klass, Register index, Register layout_info) {\n+  assert_different_registers(holder_klass, index, layout_info);\n+  InlineLayoutInfo array[2];\n+  int size = (char*)&array[1] - (char*)&array[0]; \/\/ computing size of array elements\n+  if (is_power_of_2(size)) {\n+    lsl(index, index, log2i_exact(size)); \/\/ Scale index by power of 2\n+  } else {\n+    mov(layout_info, size);\n+    mul(index, index, layout_info); \/\/ Scale the index to be the entry index * array_element_size\n+  }\n+  ldr(layout_info, Address(holder_klass, InstanceKlass::inline_layout_info_array_offset()));\n+  add(layout_info, layout_info, Array<InlineLayoutInfo>::base_offset_in_bytes());\n+  lea(layout_info, Address(layout_info, index));\n+}\n+\n@@ -5748,0 +6085,51 @@\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  if (needs_stack_repair) {\n+    \/\/ Remove the extension of the caller's frame used for inline type unpacking\n+    \/\/\n+    \/\/ Right now the stack looks like this:\n+    \/\/\n+    \/\/ | Arguments from caller     |\n+    \/\/ |---------------------------|  <-- caller's SP\n+    \/\/ | Saved LR #1               |\n+    \/\/ | Saved FP #1               |\n+    \/\/ |---------------------------|\n+    \/\/ | Extension space for       |\n+    \/\/ |   inline arg (un)packing  |\n+    \/\/ |---------------------------|  <-- start of this method's frame\n+    \/\/ | Saved LR #2               |\n+    \/\/ | Saved FP #2               |\n+    \/\/ |---------------------------|  <-- FP\n+    \/\/ | sp_inc                    |\n+    \/\/ | method locals             |\n+    \/\/ |---------------------------|  <-- SP\n+    \/\/\n+    \/\/ There are two copies of FP and LR on the stack. They will be identical\n+    \/\/ unless the caller has been deoptimized, in which case LR #1 will be patched\n+    \/\/ to point at the deopt blob, and LR #2 will still point into the old method.\n+    \/\/\n+    \/\/ The sp_inc stack slot holds the total size of the frame including the\n+    \/\/ extension space minus two words for the saved FP and LR.\n+\n+    int sp_inc_offset = initial_framesize - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+\n+    ldr(rscratch1, Address(sp, sp_inc_offset));\n+    add(sp, sp, rscratch1);\n+    ldp(rfp, lr, Address(post(sp, 2 * wordSize)));\n+  } else {\n+    remove_frame(initial_framesize);\n+  }\n+}\n+\n+void MacroAssembler::save_stack_increment(int sp_inc, int frame_size) {\n+  int real_frame_size = frame_size + sp_inc;\n+  assert(sp_inc == 0 || sp_inc > 2*wordSize, \"invalid sp_inc value\");\n+  assert(real_frame_size >= 2*wordSize, \"frame size must include FP\/LR space\");\n+  assert((real_frame_size & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+\n+  int sp_inc_offset = frame_size - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+\n+  \/\/ Subtract two words for the saved FP and LR as these will be popped\n+  \/\/ separately. See remove_frame above.\n+  mov(rscratch1, real_frame_size - 2*wordSize);\n+  str(rscratch1, Address(sp, sp_inc_offset));\n+}\n@@ -6661,0 +7049,443 @@\n+#ifdef COMPILER2\n+\/\/ C2 compiled method's prolog code\n+\/\/ Moved here from aarch64.ad to support Valhalla code belows\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+\n+    mov_metadata(rscratch2, C->method()->holder()->constant_encoding());\n+    clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n+    far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+    bind(L_skip_barrier);\n+  }\n+\n+  if (C->max_vector_size() > 0) {\n+    reinitialize_ptrue();\n+  }\n+\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  if (C->output()->need_stack_bang(bangsize))\n+    generate_stack_overflow_check(bangsize);\n+\n+  \/\/ n.b. frame size includes space for return pc and rfp\n+  const long framesize = C->output()->frame_size_in_bytes();\n+  build_frame(framesize);\n+\n+  if (C->needs_stack_repair()) {\n+    save_stack_increment(sp_inc, framesize);\n+  }\n+\n+  if (VerifyStackAtCalls) {\n+    Unimplemented();\n+  }\n+}\n+#endif \/\/ COMPILER2\n+\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  tbz(r0, 0, skip);\n+  int call_offset = -1;\n+\n+  \/\/ Be careful not to clobber r1-7 which hold returned fields\n+  \/\/ Also do not use callee-saved registers as these may be live in the interpreter\n+  Register tmp1 = r13, tmp2 = r14, klass = r15, r0_preserved = r12;\n+\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != nullptr, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(r0_preserved, r0); \/\/ save r0 for slow_case since *_allocate may corrupt it when allocation failed\n+\n+  if (vk != nullptr) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(klass, (intptr_t)vk->get_InlineKlass());\n+    jint lh = vk->layout_helper();\n+    assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB && !Klass::layout_helper_needs_slow_path(lh)) {\n+      tlab_allocate(r0, noreg, lh, tmp1, tmp2, slow_case);\n+    } else {\n+      b(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)\n+    andr(klass, r0, -2);\n+    if (UseTLAB) {\n+      ldrw(tmp2, Address(klass, Klass::layout_helper_offset()));\n+      tst(tmp2, Klass::_lh_instance_slow_path_bit);\n+      br(Assembler::NE, slow_case);\n+      tlab_allocate(r0, tmp2, 0, tmp1, tmp2, slow_case);\n+    } else {\n+      b(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = r0;\n+    mov(rscratch1, (intptr_t)markWord::inline_type_prototype().value());\n+    str(rscratch1, Address(buffer_obj, oopDesc::mark_offset_in_bytes()));\n+    store_klass_gap(buffer_obj, zr);\n+    if (vk == nullptr) {\n+      \/\/ store_klass corrupts klass, so save it for later use (interpreter case only).\n+      mov(tmp1, klass);\n+    }\n+    store_klass(buffer_obj, klass);\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != nullptr) {\n+      far_call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      \/\/ tmp1 holds klass preserved above\n+      ldr(tmp1, Address(tmp1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      ldr(tmp1, Address(tmp1, InlineKlass::pack_handler_offset()));\n+      blr(tmp1);\n+    }\n+\n+    membar(Assembler::StoreStore);\n+    b(skip);\n+  } else {\n+    \/\/ Must have already branched to slow_case above.\n+    DEBUG_ONLY(should_not_reach_here());\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(r0, r0_preserved);\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    far_call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+  membar(Assembler::StoreStore);\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_Register() && to->is_Register()) {\n+          mov(to->as_Register(), from->as_Register());\n+        } else if (from->is_FloatRegister() && to->is_FloatRegister()) {\n+          fmovd(to->as_FloatRegister(), from->as_FloatRegister());\n+        } else {\n+          ShouldNotReachHere();\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        Address to_addr = Address(sp, st_off);\n+        if (from->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             strd(from->as_FloatRegister(), to_addr);\n+          } else {\n+             assert(bt == T_FLOAT, \"must be float\");\n+             strs(from->as_FloatRegister(), to_addr);\n+          }\n+        } else {\n+          str(from->as_Register(), to_addr);\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size);\n+      if (to->is_reg()) {\n+        if (to->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+            ldrd(to->as_FloatRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            ldrs(to->as_FloatRegister(), from_addr);\n+          }\n+        } else {\n+          ldr(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(rscratch1, from_addr);\n+        str(rscratch1, Address(sp, st_off));\n+      }\n+    }\n+  }\n+\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  int sp_inc = args_on_stack * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  assert(sp_inc > 0, \"sanity\");\n+\n+  \/\/ Save a copy of the FP and LR here for deoptimization patching and frame walking\n+  stp(rfp, lr, Address(pre(sp, -2 * wordSize)));\n+\n+  \/\/ Adjust the stack pointer. This will be repaired on return by MacroAssembler::remove_frame\n+  if (sp_inc < (1 << 9)) {\n+    sub(sp, sp, sp_inc);   \/\/ Fits in an immediate\n+  } else {\n+    mov(rscratch1, sp_inc);\n+    sub(sp, sp, rscratch1);\n+  }\n+\n+  return sp_inc + 2 * wordSize;  \/\/ Account for the FP\/LR space\n+}\n+\n+\/\/ Read all fields from an inline type oop and store the values in registers\/stack slots\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  Register tmp1 = r10;\n+  Register tmp2 = r11;\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(tmp1, Address(sp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        cbz(fromReg, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        mov(tmp2, 1);\n+        str(tmp2, Address(sp, st_off));\n+      } else {\n+        mov(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_FloatRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr, rscratch1, rscratch2);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        str(dst, Address(sp, st_off));\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      ldrd(toReg->as_FloatRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      ldrs(toReg->as_FloatRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      b(L_notNull);\n+      bind(L_null);\n+      \/\/ Set IsInit field to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+            str(zr, Address(sp, st_off));\n+          } else {\n+            mov(toReg->as_Register(), zr);\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+\/\/ Pack fields back into an inline type oop\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_METADATA, \"should be at delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ The GC barrier expanded by store_heap_oop below may call into the\n+  \/\/ runtime so use callee-saved registers for any values that need to be\n+  \/\/ preserved. The GC barrier assembler should take care of saving the\n+  \/\/ Java argument registers.\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r21;\n+  Register from_reg_tmp = r22;\n+  Register tmp1 = r14;\n+  Register tmp2 = r13;\n+  Register tmp3 = r12;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index), tmp1, tmp2);\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldrb(tmp2, Address(sp, ld_off));\n+        cbnz(tmp2, L_notNull);\n+      } else {\n+        cbnz(fromReg->as_Register(), L_notNull);\n+      }\n+      mov(val_obj, 0);\n+      b(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+\n+    if (!fromReg->is_FloatRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        load_sized_value(src, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      strd(fromReg->as_FloatRegister(), dst);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      strs(fromReg->as_FloatRegister(), dst);\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return (reg->is_FloatRegister()) ? v8->as_VMReg() : r14->as_VMReg();\n+}\n+\n@@ -7059,0 +7890,4 @@\n+  if (EnableValhalla) {\n+    \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+    andr(mark, mark, ~((int) markWord::inline_type_bit_in_place));\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":837,"deletions":2,"binary":false,"changes":839,"status":"modified"},{"patch":"@@ -118,1 +118,5 @@\n-  const ByteSize entry_offset = for_compiler_entry ? Method::from_compiled_offset() :\n+  \/\/ The following jump might pass an inline type argument that was erased to Object as oop to a\n+  \/\/ callee that expects inline type arguments to be passed as fields. We need to call the compiled\n+  \/\/ value entry (_code->inline_entry_point() or _adapter->c2i_inline_entry()) which will take care\n+  \/\/ of translating between the calling conventions.\n+  const ByteSize entry_offset = for_compiler_entry ? Method::from_compiled_inline_offset() :\n","filename":"src\/hotspot\/cpu\/aarch64\/methodHandles_aarch64.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -361,0 +362,79 @@\n+\n+const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt, VMRegPair *regs, int total_args_passed) {\n+\n+  \/\/ Create the mapping between argument positions and registers.\n+\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+    r0 \/* j_rarg7 *\/, j_rarg6, j_rarg5, j_rarg4, j_rarg3, j_rarg2, j_rarg1, j_rarg0\n+  };\n+\n+  static const FloatRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3, j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+      \/\/ Should T_METADATA be added to java_calling_convention as well ?\n+    case T_METADATA:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < SharedRuntime::java_return_convention_max_float) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < SharedRuntime::java_return_convention_max_float) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -395,0 +475,108 @@\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+     for (int i = 0; i < sig_extended->length(); i++) {\n+       BasicType bt = sig_extended->at(i)._bt;\n+       if (bt == T_METADATA) {\n+         \/\/ In sig_extended, an inline type argument starts with:\n+         \/\/ T_METADATA, followed by the types of the fields of the\n+         \/\/ inline type and T_VOID to mark the end of the value\n+         \/\/ type. Inline types are flattened so, for instance, in the\n+         \/\/ case of an inline type with an int field and an inline type\n+         \/\/ field that itself has 2 fields, an int and a long:\n+         \/\/ T_METADATA T_INT T_METADATA T_INT T_LONG T_VOID (second\n+         \/\/ slot for the T_LONG) T_VOID (inner inline type) T_VOID\n+         \/\/ (outer inline type)\n+         total_args_passed++;\n+         int vt = 1;\n+         do {\n+           i++;\n+           BasicType bt = sig_extended->at(i)._bt;\n+           BasicType prev_bt = sig_extended->at(i-1)._bt;\n+           if (bt == T_METADATA) {\n+             vt++;\n+           } else if (bt == T_VOID &&\n+                      prev_bt != T_LONG &&\n+                      prev_bt != T_DOUBLE) {\n+             vt--;\n+           }\n+         } while (vt != 0);\n+       } else {\n+         total_args_passed++;\n+       }\n+     }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n+\n+  return total_args_passed;\n+}\n+\n+\n+static void gen_c2i_adapter_helper(MacroAssembler* masm,\n+                                   BasicType bt,\n+                                   BasicType prev_bt,\n+                                   size_t size_in_bytes,\n+                                   const VMRegPair& reg_pair,\n+                                   const Address& to,\n+                                   Register tmp1,\n+                                   Register tmp2,\n+                                   Register tmp3,\n+                                   int extraspace,\n+                                   bool is_oop) {\n+  if (bt == T_VOID) {\n+    assert(prev_bt == T_LONG || prev_bt == T_DOUBLE, \"missing half\");\n+    return;\n+  }\n+\n+  \/\/ Say 4 args:\n+  \/\/ i   st_off\n+  \/\/ 0   32 T_LONG\n+  \/\/ 1   24 T_VOID\n+  \/\/ 2   16 T_OBJECT\n+  \/\/ 3    8 T_BOOL\n+  \/\/ -    0 return address\n+  \/\/\n+  \/\/ However to make thing extra confusing. Because we can fit a Java long\/double in\n+  \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n+  \/\/ leaves one slot empty and only stores to a single slot. In this case the\n+  \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+\n+  bool wide = (size_in_bytes == wordSize);\n+  VMReg r_1 = reg_pair.first();\n+  VMReg r_2 = reg_pair.second();\n+  assert(r_2->is_valid() == wide, \"invalid size\");\n+  if (!r_1->is_valid()) {\n+    assert(!r_2->is_valid(), \"\");\n+    return;\n+  }\n+\n+  if (!r_1->is_FloatRegister()) {\n+    Register val = r25;\n+    if (r_1->is_stack()) {\n+      \/\/ memory to memory use r25 (scratch registers is used by store_heap_oop)\n+      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+      __ load_sized_value(val, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+    } else {\n+      val = r_1->as_Register();\n+    }\n+    assert_different_registers(to.base(), val, tmp1, tmp2, tmp3);\n+    if (is_oop) {\n+      __ store_heap_oop(to, val, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+    } else {\n+      __ store_sized_value(to, val, size_in_bytes);\n+    }\n+  } else {\n+    if (wide) {\n+      __ strd(r_1->as_FloatRegister(), to);\n+    } else {\n+      \/\/ only a float use just part of the slot\n+      __ strs(r_1->as_FloatRegister(), to);\n+    }\n+  }\n+}\n+\n@@ -396,3 +584,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>* sig_extended,\n@@ -400,1 +586,28 @@\n-                            Label& skip_fixup) {\n+                            bool requires_clinit_barrier,\n+                            address& c2i_no_clinit_check_entry,\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n+  if (requires_clinit_barrier && VM_Version::supports_fast_class_init_checks()) {\n+    Label L_skip_barrier;\n+\n+    { \/\/ Bypass the barrier for non-static methods\n+      __ ldrh(rscratch1, Address(rmethod, Method::access_flags_offset()));\n+      __ andsw(zr, rscratch1, JVM_ACC_STATIC);\n+      __ br(Assembler::EQ, L_skip_barrier); \/\/ non-static\n+    }\n+\n+    __ load_method_holder(rscratch2, rmethod);\n+    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n+    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+\n+    __ bind(L_skip_barrier);\n+    c2i_no_clinit_check_entry = __ pc();\n+  }\n+\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->c2i_entry_barrier(masm);\n+\n@@ -410,1 +623,22 @@\n-  int words_pushed = 0;\n+  \/\/ Name some registers to be used in the following code. We can use\n+  \/\/ anything except r0-r7 which are arguments in the Java calling\n+  \/\/ convention, rmethod (r12), and r13 which holds the outgoing sender\n+  \/\/ SP for the interpreter.\n+  Register buf_array = r10;   \/\/ Array of buffered inline types\n+  Register buf_oop = r11;     \/\/ Buffered inline type oop\n+  Register tmp1 = r15;\n+  Register tmp2 = r16;\n+  Register tmp3 = r17;\n+\n+  if (InlineTypePassFieldsAsArgs) {\n+    \/\/ Is there an inline type argument?\n+    bool has_inline_argument = false;\n+    for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+      has_inline_argument = (sig_extended->at(i)._bt == T_METADATA);\n+    }\n+    if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types\n+      \/\/ Allocate the buffers here with a runtime call.\n+      RegisterSaver reg_save(false \/* save_vectors *\/);\n+      OopMap* map = reg_save.save_live_registers(masm, 0, &frame_size_in_words);\n@@ -412,2 +646,2 @@\n-  \/\/ Since all args are passed on the stack, total_args_passed *\n-  \/\/ Interpreter::stackElementSize is the space we need.\n+      frame_complete = __ offset();\n+      address the_pc = __ pc();\n@@ -415,1 +649,2 @@\n-  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+      Label retaddr;\n+      __ set_last_Java_frame(sp, noreg, retaddr, rscratch1);\n@@ -417,1 +652,3 @@\n-  __ mov(r19_sender_sp, sp);\n+      __ mov(c_rarg0, rthread);\n+      __ mov(c_rarg1, rmethod);\n+      __ mov(c_rarg2, (int64_t)alloc_inline_receiver);\n@@ -419,2 +656,3 @@\n-  \/\/ stack is aligned, keep it that way\n-  extraspace = align_up(extraspace, 2*wordSize);\n+      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+      __ blr(rscratch1);\n+      __ bind(retaddr);\n@@ -422,2 +660,2 @@\n-  if (extraspace)\n-    __ sub(sp, sp, extraspace);\n+      oop_maps->add_gc_map(__ pc() - start, map);\n+      __ reset_last_Java_frame(false);\n@@ -425,6 +663,1 @@\n-  \/\/ Now write the args into the outgoing interpreter space\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n+      reg_save.restore_live_registers(masm);\n@@ -432,16 +665,3 @@\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n-\n-    \/\/ Say 4 args:\n-    \/\/ i   st_off\n-    \/\/ 0   32 T_LONG\n-    \/\/ 1   24 T_VOID\n-    \/\/ 2   16 T_OBJECT\n-    \/\/ 3    8 T_BOOL\n-    \/\/ -    0 return address\n-    \/\/\n-    \/\/ However to make thing extra confusing. Because we can fit a Java long\/double in\n-    \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n-    \/\/ leaves one slot empty and only stores to a single slot. In this case the\n-    \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+      Label no_exception;\n+      __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));\n+      __ cbz(rscratch1, no_exception);\n@@ -449,5 +669,9 @@\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n-    if (!r_1->is_valid()) {\n-      assert(!r_2->is_valid(), \"\");\n-      continue;\n+      __ str(zr, Address(rthread, JavaThread::vm_result_offset()));\n+      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));\n+      __ b(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result(buf_array, rthread);\n+      __ get_vm_result_2(rmethod, rthread); \/\/ TODO: required to keep the callee Method live?\n@@ -455,9 +679,1 @@\n-    if (r_1->is_stack()) {\n-      \/\/ memory to memory use rscratch1\n-      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size\n-                    + extraspace\n-                    + words_pushed * wordSize);\n-      if (!r_2->is_valid()) {\n-        \/\/ sign extend??\n-        __ ldrw(rscratch1, Address(sp, ld_off));\n-        __ str(rscratch1, Address(sp, st_off));\n+  }\n@@ -465,1 +681,11 @@\n-      } else {\n+  \/\/ Since all args are passed on the stack, total_args_passed *\n+  \/\/ Interpreter::stackElementSize is the space we need.\n+\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n+  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+\n+  \/\/ stack is aligned, keep it that way\n+  extraspace = align_up(extraspace, StackAlignmentInBytes);\n+\n+  \/\/ set senderSP value\n+  __ mov(r19_sender_sp, sp);\n@@ -467,1 +693,1 @@\n-        __ ldr(rscratch1, Address(sp, ld_off));\n+  __ sub(sp, sp, extraspace);\n@@ -469,6 +695,26 @@\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ str(rscratch1, Address(sp, next_off));\n+  \/\/ Now write the args into the outgoing interpreter space\n+\n+  \/\/ next_arg_comp is the next argument from the compiler point of\n+  \/\/ view (inline type fields are passed in registers\/on the stack). In\n+  \/\/ sig_extended, an inline type argument starts with: T_METADATA,\n+  \/\/ followed by the types of the fields of the inline type and T_VOID\n+  \/\/ to mark the end of the inline type. ignored counts the number of\n+  \/\/ T_METADATA\/T_VOID. next_vt_arg is the next inline type argument:\n+  \/\/ used to get the buffer for that argument from the pool of buffers\n+  \/\/ we allocated above and want to pass to the\n+  \/\/ interpreter. next_arg_int is the next argument from the\n+  \/\/ interpreter point of view (inline types are passed by reference).\n+  for (int next_arg_comp = 0, ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+       next_arg_comp < sig_extended->length(); next_arg_comp++) {\n+    assert(ignored <= next_arg_comp, \"shouldn't skip over more slots than there are arguments\");\n+    assert(next_arg_int <= total_args_passed, \"more arguments for the interpreter than expected?\");\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    int st_off = (total_args_passed - next_arg_int - 1) * Interpreter::stackElementSize;\n+    if (!InlineTypePassFieldsAsArgs || bt != T_METADATA) {\n+      int next_off = st_off - Interpreter::stackElementSize;\n+      const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+      const VMRegPair reg_pair = regs[next_arg_comp-ignored];\n+      size_t size_in_bytes = reg_pair.second()->is_valid() ? 8 : 4;\n+      gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                             size_in_bytes, reg_pair, Address(sp, offset), tmp1, tmp2, tmp3, extraspace, false);\n+      next_arg_int++;\n@@ -476,7 +722,4 @@\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaaaull);\n-          __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ str(rscratch1, Address(sp, st_off));\n-        }\n+      if (bt == T_LONG || bt == T_DOUBLE) {\n+        \/\/ Overwrite the unused slot with known junk\n+        __ mov(rscratch1, CONST64(0xdeadffffdeadaaaa));\n+        __ str(rscratch1, Address(sp, st_off));\n@@ -484,16 +727,24 @@\n-    } else if (r_1->is_Register()) {\n-      Register r = r_1->as_Register();\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ str(r, Address(sp, st_off));\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ jlong\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaabull);\n-          __ str(rscratch1, Address(sp, st_off));\n-          __ str(r, Address(sp, next_off));\n+    } else {\n+      ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_OBJECT);\n+      __ load_heap_oop(buf_oop, Address(buf_array, index), tmp1, tmp2);\n+      next_vt_arg++; next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of inline\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      Label L_null;\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp - 1)._bt;\n+        if (bt == T_METADATA) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n@@ -502,1 +753,22 @@\n-          __ str(r, Address(sp, st_off));\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          if (off == -1) {\n+            \/\/ Nullable inline type argument, emit null check\n+            VMReg reg = regs[next_arg_comp-ignored].first();\n+            Label L_notNull;\n+            if (reg->is_stack()) {\n+              int ld_off = reg->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+              __ ldrb(tmp1, Address(sp, ld_off));\n+              __ cbnz(tmp1, L_notNull);\n+            } else {\n+              __ cbnz(reg->as_Register(), L_notNull);\n+            }\n+            __ str(zr, Address(sp, st_off));\n+            __ b(L_null);\n+            __ bind(L_notNull);\n+            continue;\n+          }\n+          assert(off > 0, \"offset in object should be positive\");\n+          size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+          bool is_oop = is_reference_type(bt);\n+          gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                                 size_in_bytes, regs[next_arg_comp-ignored], Address(buf_oop, off), tmp1, tmp2, tmp3, extraspace, is_oop);\n@@ -504,14 +776,4 @@\n-      }\n-    } else {\n-      assert(r_1->is_FloatRegister(), \"\");\n-      if (!r_2->is_valid()) {\n-        \/\/ only a float use just part of the slot\n-        __ strs(r_1->as_FloatRegister(), Address(sp, st_off));\n-      } else {\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaacull);\n-        __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        __ strd(r_1->as_FloatRegister(), Address(sp, next_off));\n-      }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ str(buf_oop, Address(sp, st_off));\n+      __ bind(L_null);\n@@ -527,0 +789,1 @@\n+void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm, int comp_args_on_stack, const GrowableArray<SigEntry>* sig, const VMRegPair *regs) {\n@@ -528,5 +791,0 @@\n-void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,\n-                                    int total_args_passed,\n-                                    int comp_args_on_stack,\n-                                    const BasicType *sig_bt,\n-                                    const VMRegPair *regs) {\n@@ -595,1 +853,1 @@\n-  int comp_words_on_stack = align_up(comp_args_on_stack*VMRegImpl::stack_slot_size, wordSize)>>LogBytesPerWord;\n+  int comp_words_on_stack = 0;\n@@ -597,2 +855,3 @@\n-    __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n-    __ andr(sp, rscratch1, -16);\n+     comp_words_on_stack = align_up(comp_args_on_stack * VMRegImpl::stack_slot_size, wordSize) >> LogBytesPerWord;\n+     __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n+     __ andr(sp, rscratch1, -16);\n@@ -603,1 +862,1 @@\n-  __ ldr(rscratch1, Address(rmethod, in_bytes(Method::from_compiled_offset())));\n+  __ ldr(rscratch1, Address(rmethod, in_bytes(Method::from_compiled_inline_offset())));\n@@ -617,0 +876,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -619,2 +880,3 @@\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+    BasicType bt = sig->at(i)._bt;\n+    if (bt == T_VOID) {\n+      assert(i > 0 && (sig->at(i - 1)._bt == T_LONG || sig->at(i - 1)._bt == T_DOUBLE), \"missing half\");\n@@ -625,0 +887,1 @@\n+    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(), \"scrambled load targets?\");\n@@ -626,3 +889,1 @@\n-    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(),\n-            \"scrambled load targets?\");\n-    int ld_off = (total_args_passed - i - 1)*Interpreter::stackElementSize;\n+    int ld_off = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n@@ -643,1 +904,1 @@\n-      int st_off = regs[i].first()->reg2stack()*VMRegImpl::stack_slot_size;\n+      int st_off = regs[i].first()->reg2stack() * VMRegImpl::stack_slot_size;\n@@ -660,2 +921,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n@@ -664,11 +924,28 @@\n-        __ str(rscratch2, Address(sp, st_off));\n-      }\n-    } else if (r_1->is_Register()) {  \/\/ Register argument\n-      Register r = r_1->as_Register();\n-      if (r_2->is_valid()) {\n-        \/\/\n-        \/\/ We are using two VMRegs. This can be either T_OBJECT,\n-        \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n-        \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n-        \/\/ So we must adjust where to pick up the data to match the\n-        \/\/ interpreter.\n+         __ str(rscratch2, Address(sp, st_off));\n+       }\n+     } else if (r_1->is_Register()) {  \/\/ Register argument\n+       Register r = r_1->as_Register();\n+       if (r_2->is_valid()) {\n+         \/\/\n+         \/\/ We are using two VMRegs. This can be either T_OBJECT,\n+         \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n+         \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n+         \/\/ So we must adjust where to pick up the data to match the\n+         \/\/ interpreter.\n+\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n+\n+         \/\/ this can be a misaligned move\n+         __ ldr(r, Address(esp, offset));\n+       } else {\n+         \/\/ sign extend and use a full word?\n+         __ ldrw(r, Address(esp, ld_off));\n+       }\n+     } else {\n+       if (!r_2->is_valid()) {\n+         __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n+       } else {\n+         __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n+       }\n+     }\n+   }\n@@ -676,17 +953,0 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n-\n-        \/\/ this can be a misaligned move\n-        __ ldr(r, Address(esp, offset));\n-      } else {\n-        \/\/ sign extend and use a full word?\n-        __ ldrw(r, Address(esp, ld_off));\n-      }\n-    } else {\n-      if (!r_2->is_valid()) {\n-        __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n-      } else {\n-        __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n-      }\n-    }\n-  }\n@@ -709,1 +969,0 @@\n-\n@@ -713,8 +972,4 @@\n-\/\/ ---------------------------------------------------------------\n-AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n-                                                            int total_args_passed,\n-                                                            int comp_args_on_stack,\n-                                                            const BasicType *sig_bt,\n-                                                            const VMRegPair *regs,\n-                                                            AdapterFingerPrint* fingerprint) {\n-  address i2c_entry = __ pc();\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n+  Register data = rscratch2;\n+  __ ic_check(1 \/* end_alignment *\/);\n+  __ ldr(rmethod, Address(data, CompiledICData::speculated_method_offset()));\n@@ -722,1 +977,7 @@\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  \/\/ Method might have been compiled since the call site was patched to\n+  \/\/ interpreted; if that is the case treat it as a miss so we can get\n+  \/\/ the call site corrected.\n+  __ ldr(rscratch1, Address(rmethod, in_bytes(Method::code_offset())));\n+  __ cbz(rscratch1, skip_fixup);\n+  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+}\n@@ -724,2 +985,12 @@\n-  address c2i_unverified_entry = __ pc();\n-  Label skip_fixup;\n+\/\/ ---------------------------------------------------------------\n+AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler* masm,\n+                                                            int comp_args_on_stack,\n+                                                            const GrowableArray<SigEntry>* sig,\n+                                                            const VMRegPair* regs,\n+                                                            const GrowableArray<SigEntry>* sig_cc,\n+                                                            const VMRegPair* regs_cc,\n+                                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                            const VMRegPair* regs_cc_ro,\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter,\n+                                                            bool allocate_code_blob) {\n@@ -727,3 +998,2 @@\n-  Register data = rscratch2;\n-  Register receiver = j_rarg0;\n-  Register tmp = r10;  \/\/ A call-clobbered register not used for arg passing\n+  address i2c_entry = __ pc();\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n@@ -740,7 +1010,3 @@\n-  {\n-    __ block_comment(\"c2i_unverified_entry {\");\n-    \/\/ Method might have been compiled since the call site was patched to\n-    \/\/ interpreted; if that is the case treat it as a miss so we can get\n-    \/\/ the call site corrected.\n-    __ ic_check(1 \/* end_alignment *\/);\n-    __ ldr(rmethod, Address(data, CompiledICData::speculated_method_offset()));\n+  address c2i_unverified_entry        = __ pc();\n+  address c2i_unverified_inline_entry = __ pc();\n+  Label skip_fixup;\n@@ -748,5 +1014,1 @@\n-    __ ldr(rscratch1, Address(rmethod, in_bytes(Method::code_offset())));\n-    __ cbz(rscratch1, skip_fixup);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-    __ block_comment(\"} c2i_unverified_entry\");\n-  }\n+  gen_inline_cache_check(masm, skip_fixup);\n@@ -754,1 +1016,3 @@\n-  address c2i_entry = __ pc();\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n@@ -756,1 +1020,1 @@\n-  \/\/ Class initialization barrier for static methods\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n@@ -758,2 +1022,7 @@\n-  if (VM_Version::supports_fast_class_init_checks()) {\n-    Label L_skip_barrier;\n+  address c2i_inline_ro_entry = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    \/\/ No class init barrier needed because method is guaranteed to be non-static\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, \/* requires_clinit_barrier = *\/ false, c2i_no_clinit_check_entry,\n+                    skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n+    skip_fixup.reset();\n+  }\n@@ -761,5 +1030,5 @@\n-    { \/\/ Bypass the barrier for non-static methods\n-      __ ldrh(rscratch1, Address(rmethod, Method::access_flags_offset()));\n-      __ andsw(zr, rscratch1, JVM_ACC_STATIC);\n-      __ br(Assembler::EQ, L_skip_barrier); \/\/ non-static\n-    }\n+  \/\/ Scalarized c2i adapter\n+  address c2i_entry        = __ pc();\n+  address c2i_inline_entry = __ pc();\n+  gen_c2i_adapter(masm, sig_cc, regs_cc, \/* requires_clinit_barrier = *\/ true, c2i_no_clinit_check_entry,\n+                  skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ true);\n@@ -767,3 +1036,5 @@\n-    __ load_method_holder(rscratch2, rmethod);\n-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+  \/\/ Non-scalarized c2i adapter\n+  if (regs != regs_cc) {\n+    c2i_unverified_inline_entry = __ pc();\n+    Label inline_entry_skip_fixup;\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n@@ -771,2 +1042,3 @@\n-    __ bind(L_skip_barrier);\n-    c2i_no_clinit_check_entry = __ pc();\n+    c2i_inline_entry = __ pc();\n+    gen_c2i_adapter(masm, sig, regs, \/* requires_clinit_barrier = *\/ true, c2i_no_clinit_check_entry,\n+                    inline_entry_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n@@ -775,3 +1047,6 @@\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->c2i_entry_barrier(masm);\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n+  \/\/ The c2i adapter might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+  if (allocate_code_blob) {\n+    bool caller_must_gc_arguments = (regs != regs_cc);\n+    new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n+  }\n@@ -780,1 +1055,1 @@\n-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -1817,0 +2092,4 @@\n+      if (EnableValhalla) {\n+        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+        __ andr(swap_reg, swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+      }\n@@ -2819,0 +3098,116 @@\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler _masm(&buffer);\n+  MacroAssembler* masm = &_masm;\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_jobject_off = __ offset();\n+  \/\/ Resolve pre-allocated buffer from JNI handle.\n+  \/\/ We cannot do this in generate_call_stub() because it requires GC code to be initialized.\n+  Register Rresult = r14;  \/\/ See StubGenerator::generate_call_stub().\n+  __ ldr(r0, Address(Rresult));\n+  __ resolve_jobject(r0 \/* value *\/,\n+                     rthread \/* thread *\/,\n+                     r12 \/* tmp *\/);\n+  __ str(r0, Address(Rresult));\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ strs(r_1->as_FloatRegister(), to);\n+    } else if (bt == T_DOUBLE) {\n+      __ strd(r_1->as_FloatRegister(), to);\n+    } else {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(to.base(), val, r15, r16, r17);\n+      if (is_reference_type(bt)) {\n+        __ store_heap_oop(to, val, r15, r16, r17, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        __ store_sized_value(to, r_1->as_Register(), type2aelembytes(bt));\n+      }\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+  if (vk->has_nullable_atomic_layout()) {\n+    \/\/ Zero the null marker (setting it to 1 would be better but would require an additional register)\n+    __ strb(zr, Address(r0, vk->null_marker_offset()));\n+  }\n+  __ ret(lr);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  Label skip;\n+  __ cbz(r0, skip);\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ ldrs(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ ldrd(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      assert_different_registers(r0, r_1->as_Register());\n+      __ load_heap_oop(r_1->as_Register(), from, rscratch1, rscratch2);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(r0, r_1->as_Register());\n+\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ bind(skip);\n+\n+  __ ret(lr);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":580,"deletions":185,"binary":false,"changes":765,"status":"modified"},{"patch":"@@ -330,4 +330,11 @@\n-    __ ldr(j_rarg2, result);\n-    Label is_long, is_float, is_double, exit;\n-    __ ldr(j_rarg1, result_type);\n-    __ cmp(j_rarg1, (u1)T_OBJECT);\n+    \/\/ All of j_rargN may be used to return inline type fields so be careful\n+    \/\/ not to clobber those.\n+    \/\/ SharedRuntime::generate_buffered_inline_type_adapter() knows the register\n+    \/\/ assignment of Rresult below.\n+    Register Rresult = r14, Rresult_type = r15;\n+    __ ldr(Rresult, result);\n+    Label is_long, is_float, is_double, check_prim, exit;\n+    __ ldr(Rresult_type, result_type);\n+    __ cmp(Rresult_type, (u1)T_OBJECT);\n+    __ br(Assembler::EQ, check_prim);\n+    __ cmp(Rresult_type, (u1)T_LONG);\n@@ -335,3 +342,1 @@\n-    __ cmp(j_rarg1, (u1)T_LONG);\n-    __ br(Assembler::EQ, is_long);\n-    __ cmp(j_rarg1, (u1)T_FLOAT);\n+    __ cmp(Rresult_type, (u1)T_FLOAT);\n@@ -339,1 +344,1 @@\n-    __ cmp(j_rarg1, (u1)T_DOUBLE);\n+    __ cmp(Rresult_type, (u1)T_DOUBLE);\n@@ -343,1 +348,1 @@\n-    __ strw(r0, Address(j_rarg2));\n+    __ strw(r0, Address(Rresult));\n@@ -395,0 +400,11 @@\n+    __ BIND(check_prim);\n+    if (InlineTypeReturnedAsFields) {\n+      \/\/ Check for scalarized return value\n+      __ tbz(r0, 0, is_long);\n+      \/\/ Load pack handler address\n+      __ andr(rscratch1, r0, -2);\n+      __ ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ ldr(rscratch1, Address(rscratch1, InlineKlass::pack_handler_jobject_offset()));\n+      __ blr(rscratch1);\n+      __ b(exit);\n+    }\n@@ -397,1 +413,1 @@\n-    __ str(r0, Address(j_rarg2, 0));\n+    __ str(r0, Address(Rresult, 0));\n@@ -401,1 +417,1 @@\n-    __ strs(j_farg0, Address(j_rarg2, 0));\n+    __ strs(j_farg0, Address(Rresult, 0));\n@@ -405,1 +421,1 @@\n-    __ strd(j_farg0, Address(j_rarg2, 0));\n+    __ strd(j_farg0, Address(Rresult, 0));\n@@ -2239,0 +2255,6 @@\n+    \/\/ Check for flat inline type array -> return -1\n+    __ test_flat_array_oop(src, rscratch2, L_failed);\n+\n+    \/\/ Check for null-free (non-flat) inline type array -> handle as object array\n+    __ test_null_free_array_oop(src, rscratch2, L_objArray);\n+\n@@ -8600,0 +8622,128 @@\n+  \/\/ Call here from the interpreter or compiled code to either load\n+  \/\/ multiple returned values from the inline type instance being\n+  \/\/ returned to registers or to store returned values to a newly\n+  \/\/ allocated inline type instance.\n+  address generate_return_value_stub(address destination, const char* name, bool has_res) {\n+    \/\/ We need to save all registers the calling convention may use so\n+    \/\/ the runtime calls read or update those registers. This needs to\n+    \/\/ be in sync with SharedRuntime::java_return_convention().\n+    \/\/ n.b. aarch64 asserts that frame::arg_reg_save_area_bytes == 0\n+    enum layout {\n+      j_rarg7_off = 0, j_rarg7_2,    \/\/ j_rarg7 is r0\n+      j_rarg6_off, j_rarg6_2,\n+      j_rarg5_off, j_rarg5_2,\n+      j_rarg4_off, j_rarg4_2,\n+      j_rarg3_off, j_rarg3_2,\n+      j_rarg2_off, j_rarg2_2,\n+      j_rarg1_off, j_rarg1_2,\n+      j_rarg0_off, j_rarg0_2,\n+\n+      j_farg7_off, j_farg7_2,\n+      j_farg6_off, j_farg6_2,\n+      j_farg5_off, j_farg5_2,\n+      j_farg4_off, j_farg4_2,\n+      j_farg3_off, j_farg3_2,\n+      j_farg2_off, j_farg2_2,\n+      j_farg1_off, j_farg1_2,\n+      j_farg0_off, j_farg0_2,\n+\n+      rfp_off, rfp_off2,\n+      return_off, return_off2,\n+\n+      framesize \/\/ inclusive of return address\n+    };\n+\n+    CodeBuffer code(name, 512, 64);\n+    MacroAssembler* masm = new MacroAssembler(&code);\n+\n+    int frame_size_in_bytes = align_up(framesize*BytesPerInt, 16);\n+    assert(frame_size_in_bytes == framesize*BytesPerInt, \"misaligned\");\n+    int frame_size_in_slots = frame_size_in_bytes \/ BytesPerInt;\n+    int frame_size_in_words = frame_size_in_bytes \/ wordSize;\n+\n+    OopMapSet* oop_maps = new OopMapSet();\n+    OopMap* map = new OopMap(frame_size_in_slots, 0);\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg7_off), j_rarg7->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg6_off), j_rarg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n+\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n+\n+    address start = __ pc();\n+\n+    __ enter(); \/\/ Save FP and LR before call\n+\n+    __ stpd(j_farg1, j_farg0, Address(__ pre(sp, -2 * wordSize)));\n+    __ stpd(j_farg3, j_farg2, Address(__ pre(sp, -2 * wordSize)));\n+    __ stpd(j_farg5, j_farg4, Address(__ pre(sp, -2 * wordSize)));\n+    __ stpd(j_farg7, j_farg6, Address(__ pre(sp, -2 * wordSize)));\n+\n+    __ stp(j_rarg1, j_rarg0, Address(__ pre(sp, -2 * wordSize)));\n+    __ stp(j_rarg3, j_rarg2, Address(__ pre(sp, -2 * wordSize)));\n+    __ stp(j_rarg5, j_rarg4, Address(__ pre(sp, -2 * wordSize)));\n+    __ stp(j_rarg7, j_rarg6, Address(__ pre(sp, -2 * wordSize)));\n+\n+    int frame_complete = __ offset();\n+\n+    \/\/ Set up last_Java_sp and last_Java_fp\n+    address the_pc = __ pc();\n+    __ set_last_Java_frame(sp, noreg, the_pc, rscratch1);\n+\n+    \/\/ Call runtime\n+    __ mov(c_rarg1, r0);\n+    __ mov(c_rarg0, rthread);\n+\n+    __ mov(rscratch1, destination);\n+    __ blr(rscratch1);\n+\n+    oop_maps->add_gc_map(the_pc - start, map);\n+\n+    __ reset_last_Java_frame(false);\n+\n+    __ ldp(j_rarg7, j_rarg6, Address(__ post(sp, 2 * wordSize)));\n+    __ ldp(j_rarg5, j_rarg4, Address(__ post(sp, 2 * wordSize)));\n+    __ ldp(j_rarg3, j_rarg2, Address(__ post(sp, 2 * wordSize)));\n+    __ ldp(j_rarg1, j_rarg0, Address(__ post(sp, 2 * wordSize)));\n+\n+    __ ldpd(j_farg7, j_farg6, Address(__ post(sp, 2 * wordSize)));\n+    __ ldpd(j_farg5, j_farg4, Address(__ post(sp, 2 * wordSize)));\n+    __ ldpd(j_farg3, j_farg2, Address(__ post(sp, 2 * wordSize)));\n+    __ ldpd(j_farg1, j_farg0, Address(__ post(sp, 2 * wordSize)));\n+\n+    __ leave();\n+\n+    \/\/ check for pending exceptions\n+    Label pending;\n+    __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));\n+    __ cbnz(rscratch1, pending);\n+\n+    if (has_res) {\n+      __ get_vm_result(r0, rthread);\n+    }\n+\n+    __ ret(lr);\n+\n+    __ bind(pending);\n+    __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+    \/\/ -------------\n+    \/\/ make sure all code is generated\n+    masm->flush();\n+\n+    RuntimeStub* stub = RuntimeStub::new_runtime_stub(name, &code, frame_complete, frame_size_in_words, oop_maps, false);\n+    return stub->entry_point();\n+  }\n+\n@@ -8646,0 +8796,8 @@\n+\n+    if (InlineTypeReturnedAsFields) {\n+      StubRoutines::_load_inline_type_fields_in_regs =\n+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs), \"load_inline_type_fields_in_regs\", false);\n+      StubRoutines::_store_inline_type_fields_to_buf =\n+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf), \"store_inline_type_fields_to_buf\", true);\n+    }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":170,"deletions":12,"binary":false,"changes":182,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -470,0 +471,5 @@\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    __ store_inline_type_fields_to_buf(nullptr, true);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -175,0 +175,1 @@\n+  case Bytecodes::_fast_vputfield:\n@@ -756,4 +757,4 @@\n-    \/\/ ??? convention: move array into r3 for exception message\n-  __ mov(r3, array);\n-  __ mov(rscratch1, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);\n-  __ br(rscratch1);\n+  \/\/ ??? convention: move array into r3 for exception message\n+   __ mov(r3, array);\n+   __ mov(rscratch1, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);\n+   __ br(rscratch1);\n@@ -819,5 +820,20 @@\n-  __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n-  do_oop_load(_masm,\n-              Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)),\n-              r0,\n-              IS_ARRAY);\n+  __ profile_array_type<ArrayLoadData>(r2, r0, r4);\n+  if (UseArrayFlattening) {\n+    Label is_flat_array, done;\n+\n+    __ test_flat_array_oop(r0, r8 \/*temp*\/, is_flat_array);\n+    __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+    do_oop_load(_masm, Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)), r0, IS_ARRAY);\n+\n+    __ b(done);\n+    __ bind(is_flat_array);\n+    __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::flat_array_load), r0, r1);\n+    \/\/ Ensure the stores to copy the inline field contents are visible\n+    \/\/ before any subsequent store that publishes this reference.\n+    __ membar(Assembler::StoreStore);\n+    __ bind(done);\n+  } else {\n+    __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);\n+    do_oop_load(_masm, Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)), r0, IS_ARRAY);\n+  }\n+  __ profile_element_type(r2, r0, r4);\n@@ -1110,1 +1126,1 @@\n-  Label is_null, ok_is_subtype, done;\n+  Label is_null, is_flat_array, ok_is_subtype, done;\n@@ -1117,2 +1133,4 @@\n-  Address element_address(r3, r4, Address::uxtw(LogBytesPerHeapOop));\n-\n+\n+  __ profile_array_type<ArrayStoreData>(r4, r3, r5);\n+  __ profile_multiple_element_types(r4, r0, r5, r6);\n+\n@@ -1121,0 +1139,2 @@\n+  Address element_address(r3, r4, Address::uxtw(LogBytesPerHeapOop));\n+  \/\/ Be careful not to clobber r4 below\n@@ -1125,0 +1145,8 @@\n+  \/\/ Move array class to r5\n+  __ load_klass(r5, r3);\n+\n+  if (UseArrayFlattening) {\n+    __ ldrw(r6, Address(r5, Klass::layout_helper_offset()));\n+    __ test_flat_array_layout(r6, is_flat_array);\n+  }\n+\n@@ -1127,4 +1155,3 @@\n-  \/\/ Move superklass into r0\n-  __ load_klass(r0, r3);\n-  __ ldr(r0, Address(r0,\n-                     ObjArrayKlass::element_klass_offset()));\n+\n+  \/\/ Move array element superklass into r0\n+  __ ldr(r0, Address(r5, ObjArrayKlass::element_klass_offset()));\n@@ -1135,1 +1162,3 @@\n-  __ gen_subtype_check(r1, ok_is_subtype);\n+\n+  \/\/ is \"r1 <: r0\" ? (value subclass <: array element superclass)\n+  __ gen_subtype_check(r1, ok_is_subtype, false);\n@@ -1152,1 +1181,16 @@\n-  __ profile_null_seen(r2);\n+  if (EnableValhalla) {\n+    Label is_null_into_value_array_npe, store_null;\n+\n+    if (UseArrayFlattening) {\n+      __ test_flat_array_oop(r3, r8, is_flat_array);\n+    }\n+\n+    \/\/ No way to store null in a null-free array\n+    __ test_null_free_array_oop(r3, r8, is_null_into_value_array_npe);\n+    __ b(store_null);\n+\n+    __ bind(is_null_into_value_array_npe);\n+    __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));\n+\n+    __ bind(store_null);\n+  }\n@@ -1156,0 +1200,11 @@\n+  __ b(done);\n+\n+  if (UseArrayFlattening) {\n+     Label is_type_ok;\n+    __ bind(is_flat_array); \/\/ Store non-null value to flat\n+\n+    __ ldr(r0, at_tos());    \/\/ value\n+    __ ldr(r3, at_tos_p1()); \/\/ index\n+    __ ldr(r2, at_tos_p2()); \/\/ array\n+    __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::flat_array_store), r0, r2, r3);\n+  }\n@@ -1960,2 +2015,1 @@\n-void TemplateTable::if_acmp(Condition cc)\n-{\n+void TemplateTable::if_acmp(Condition cc) {\n@@ -1964,1 +2018,1 @@\n-  Label not_taken;\n+  Label taken, not_taken;\n@@ -1966,0 +2020,38 @@\n+\n+  __ profile_acmp(r2, r1, r0, r4);\n+\n+  Register is_inline_type_mask = rscratch1;\n+  __ mov(is_inline_type_mask, markWord::inline_type_pattern);\n+\n+  if (EnableValhalla) {\n+    __ cmp(r1, r0);\n+    __ br(Assembler::EQ, (cc == equal) ? taken : not_taken);\n+\n+    \/\/ might be substitutable, test if either r0 or r1 is null\n+    __ andr(r2, r0, r1);\n+    __ cbz(r2, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ and both are values ?\n+    __ ldr(r2, Address(r1, oopDesc::mark_offset_in_bytes()));\n+    __ andr(r2, r2, is_inline_type_mask);\n+    __ ldr(r4, Address(r0, oopDesc::mark_offset_in_bytes()));\n+    __ andr(r4, r4, is_inline_type_mask);\n+    __ andr(r2, r2, r4);\n+    __ cmp(r2,  is_inline_type_mask);\n+    __ br(Assembler::NE, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ same value klass ?\n+    __ load_metadata(r2, r1);\n+    __ load_metadata(r4, r0);\n+    __ cmp(r2, r4);\n+    __ br(Assembler::NE, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ Know both are the same type, let's test for substitutability...\n+    if (cc == equal) {\n+      invoke_is_substitutable(r0, r1, taken, not_taken);\n+    } else {\n+      invoke_is_substitutable(r0, r1, not_taken, taken);\n+    }\n+    __ stop(\"Not reachable\");\n+  }\n+\n@@ -1968,0 +2060,1 @@\n+  __ bind(taken);\n@@ -1970,1 +2063,10 @@\n-  __ profile_not_taken_branch(r0);\n+  __ profile_not_taken_branch(r0, true);\n+}\n+\n+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,\n+                                            Label& is_subst, Label& not_subst) {\n+\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);\n+  \/\/ Restored... r0 answer, jmp to outcome...\n+  __ cbz(r0, not_subst);\n+  __ b(is_subst);\n@@ -1973,0 +2075,1 @@\n+\n@@ -2582,1 +2685,1 @@\n-  const Register cache     = r4;\n+  const Register cache     = r2;\n@@ -2584,0 +2687,3 @@\n+  const Register klass     = r5;\n+  const Register inline_klass = r7;\n+  const Register field_index = r23;\n@@ -2592,0 +2698,5 @@\n+\n+  \/\/ Valhalla extras\n+  __ load_unsigned_short(field_index, Address(cache, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+  __ ldr(klass, Address(cache, ResolvedFieldEntry::field_holder_offset()));\n+\n@@ -2650,4 +2761,73 @@\n-  do_oop_load(_masm, field, r0, IN_HEAP);\n-  __ push(atos);\n-  if (rc == may_rewrite) {\n-    patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+  if (!EnableValhalla) {\n+    do_oop_load(_masm, field, r0, IN_HEAP);\n+    __ push(atos);\n+    if (rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+    }\n+    __ b(Done);\n+  } else { \/\/ Valhalla\n+    if (is_static) {\n+      __ load_heap_oop(r0, field, rscratch1, rscratch2);\n+      Label is_null_free_inline_type, uninitialized;\n+      \/\/ Issue below if the static field has not been initialized yet\n+      __ test_field_is_null_free_inline_type(flags, noreg \/*temp*\/, is_null_free_inline_type);\n+        \/\/ field is not a null free inline type\n+        __ push(atos);\n+        __ b(Done);\n+      \/\/ field is a null free inline type, must not return null even if uninitialized\n+      __ bind(is_null_free_inline_type);\n+        __ cbz(r0, uninitialized);\n+          __ push(atos);\n+          __ b(Done);\n+        __ bind(uninitialized);\n+          Label slow_case, finish;\n+          __ ldrb(rscratch1, Address(klass, InstanceKlass::init_state_offset()));\n+          __ cmp(rscratch1, (u1)InstanceKlass::fully_initialized);\n+          __ br(Assembler::NE, slow_case);\n+          __ get_default_value_oop(klass, off \/* temp *\/, r0);\n+        __ b(finish);\n+        __ bind(slow_case);\n+          __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_static_inline_type_field), obj, cache);\n+          __ bind(finish);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+          __ b(Done);\n+    } else {\n+      Label is_flat, nonnull, is_inline_type, has_null_marker, rewrite_inline;\n+      __ test_field_is_null_free_inline_type(flags, noreg \/*temp*\/, is_inline_type);\n+      __ test_field_has_null_marker(flags, noreg \/*temp*\/, has_null_marker);\n+        \/\/ Non-inline field case\n+        __ load_heap_oop(r0, field, rscratch1, rscratch2);\n+        __ push(atos);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);\n+        }\n+        __ b(Done);\n+      __ bind(is_inline_type);\n+        __ test_field_is_flat(flags, noreg \/* temp *\/, is_flat);\n+         \/\/ field is not flat\n+          __ load_heap_oop(r0, field, rscratch1, rscratch2);\n+          __ cbnz(r0, nonnull);\n+            __ get_inline_type_field_klass(klass, field_index, inline_klass);\n+            __ get_default_value_oop(inline_klass, klass \/* temp *\/, r0);\n+          __ bind(nonnull);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+          __ b(rewrite_inline);\n+        __ bind(is_flat);\n+        \/\/ field is flat\n+          __ mov(r0, obj);\n+          __ read_flat_field(cache, field_index, off, inline_klass \/* temp *\/, r0);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+          __ b(rewrite_inline);\n+        __ bind(has_null_marker);\n+          call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_nullable_flat_field), obj, cache);\n+          __ verify_oop(r0);\n+          __ push(atos);\n+      __ bind(rewrite_inline);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_vgetfield, bc, r1);\n+      }\n+      __ b(Done);\n+    }\n@@ -2655,1 +2835,0 @@\n-  __ b(Done);\n@@ -2816,1 +2995,1 @@\n-  const Register flags     = r0;\n+  const Register flags     = r6;\n@@ -2818,0 +2997,1 @@\n+  const Register inline_klass = r5;\n@@ -2824,2 +3004,0 @@\n-  __ mov(r5, flags);\n-\n@@ -2828,1 +3006,1 @@\n-    __ tbz(r5, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n+    __ tbz(flags, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n@@ -2877,8 +3055,63 @@\n-    __ pop(atos);\n-    if (!is_static) pop_and_check_object(obj);\n-    \/\/ Store into the field\n-    do_oop_store(_masm, field, r0, IN_HEAP);\n-    if (rc == may_rewrite) {\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);\n-    }\n-    __ b(Done);\n+     if (!EnableValhalla) {\n+      __ pop(atos);\n+      if (!is_static) pop_and_check_object(obj);\n+      \/\/ Store into the field\n+      do_oop_store(_masm, field, r0, IN_HEAP);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);\n+      }\n+      __ b(Done);\n+     } else { \/\/ Valhalla\n+      __ pop(atos);\n+      if (is_static) {\n+        Label is_inline_type;\n+         __ test_field_is_not_null_free_inline_type(flags, noreg \/* temp *\/, is_inline_type);\n+         __ null_check(r0);\n+         __ bind(is_inline_type);\n+         do_oop_store(_masm, field, r0, IN_HEAP);\n+         __ b(Done);\n+      } else {\n+        Label is_inline_type, is_flat, has_null_marker, rewrite_not_inline, rewrite_inline;\n+        __ test_field_is_null_free_inline_type(flags, noreg \/*temp*\/, is_inline_type);\n+        __ test_field_has_null_marker(flags, noreg \/*temp*\/, has_null_marker);\n+        \/\/ Not an inline type\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, r0, IN_HEAP);\n+        __ bind(rewrite_not_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_aputfield, bc, r19, true, byte_no);\n+        }\n+        __ b(Done);\n+        \/\/ Implementation of the inline type semantic\n+        __ bind(is_inline_type);\n+        __ null_check(r0);\n+        __ test_field_is_flat(flags, noreg \/*temp*\/, is_flat);\n+        \/\/ field is not flat\n+        pop_and_check_object(obj);\n+        \/\/ Store into the field\n+        do_oop_store(_masm, field, r0, IN_HEAP);\n+        __ b(rewrite_inline);\n+        __ bind(is_flat);\n+        __ load_field_entry(cache, index); \/\/ reload field entry (cache) because it was erased by tos_state\n+        __ load_unsigned_short(index, Address(cache, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+        __ ldr(r2, Address(cache, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+        __ inline_layout_info(r2, index, r6);\n+        pop_and_check_object(obj);\n+        __ load_klass(inline_klass, r0);\n+        __ payload_address(r0, r0, inline_klass);\n+        __ add(obj, obj, off);\n+        \/\/ because we use InlineLayoutInfo, we need special value access code specialized for fields (arrays will need a different API)\n+        __ flat_field_copy(IN_HEAP, r0, obj, r6);\n+        __ b(rewrite_inline);\n+        __ bind(has_null_marker);\n+        assert_different_registers(r0, cache, r19);\n+        pop_and_check_object(r19);\n+        __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_nullable_flat_field), r19, r0, cache);\n+        __ bind(rewrite_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_vputfield, bc, r19, true, byte_no);\n+        }\n+        __ b(Done);\n+      }\n+     }  \/\/ Valhalla\n@@ -2989,1 +3222,1 @@\n-    __ tbz(r5, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n+    __ tbz(flags, ResolvedFieldEntry::is_volatile_shift, notVolatile);\n@@ -3023,0 +3256,1 @@\n+    case Bytecodes::_fast_vputfield: \/\/fall through\n@@ -3049,0 +3283,1 @@\n+    case Bytecodes::_fast_vputfield: \/\/fall through\n@@ -3095,0 +3330,27 @@\n+  case Bytecodes::_fast_vputfield:\n+   {\n+      Label is_flat, has_null_marker, done;\n+      __ test_field_has_null_marker(r3, noreg \/* temp *\/, has_null_marker);\n+      __ null_check(r0);\n+      __ test_field_is_flat(r3, noreg \/* temp *\/, is_flat);\n+      \/\/ field is not flat\n+      do_oop_store(_masm, field, r0, IN_HEAP);\n+      __ b(done);\n+      __ bind(is_flat);\n+      \/\/ field is flat\n+      __ load_field_entry(r4, r3);\n+      __ load_unsigned_short(r3, Address(r4, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+      __ ldr(r4, Address(r4, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+      __ inline_layout_info(r4, r3, r5);\n+      __ load_klass(r4, r0);\n+      __ payload_address(r0, r0, r4);\n+      __ lea(rscratch1, field);\n+      __ flat_field_copy(IN_HEAP, r0, rscratch1, r5);\n+      __ b(done);\n+      __ bind(has_null_marker);\n+      __ load_field_entry(r4, r1);\n+      __ mov(r1, r2);\n+      __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_nullable_flat_field), r1, r0, r4);\n+      __ bind(done);\n+    }\n+    break;\n@@ -3187,0 +3449,28 @@\n+  case Bytecodes::_fast_vgetfield:\n+    {\n+      Register index = r4, klass = r5, inline_klass = r6, tmp = r7;\n+      Label is_flat, has_null_marker, nonnull, Done;\n+      __ test_field_has_null_marker(r3, noreg \/*temp*\/, has_null_marker);\n+      __ test_field_is_flat(r3, noreg \/* temp *\/, is_flat);\n+        \/\/ field is not flat\n+        __ load_heap_oop(r0, field, rscratch1, rscratch2);\n+        __ cbnz(r0, nonnull);\n+          __ load_unsigned_short(index, Address(r2, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+          __ ldr(klass, Address(r2, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+          __ get_inline_type_field_klass(klass, index, inline_klass);\n+          __ get_default_value_oop(inline_klass, tmp \/* temp *\/, r0);\n+        __ bind(nonnull);\n+        __ verify_oop(r0);\n+        __ b(Done);\n+      __ bind(is_flat);\n+      \/\/ field is flat\n+        __ load_unsigned_short(index, Address(r2, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+        __ read_flat_field(r2, index, r1, tmp \/* temp *\/, r0);\n+        __ verify_oop(r0);\n+        __ b(Done);\n+      __ bind(has_null_marker);\n+        call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_nullable_flat_field), r0, r2);\n+        __ verify_oop(r0);\n+      __ bind(Done);\n+    }\n+    break;\n@@ -3605,53 +3895,1 @@\n-  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n-  __ ldrw(r3,\n-          Address(r4,\n-                  Klass::layout_helper_offset()));\n-  \/\/ test to see if it is malformed in some way\n-  __ tbnz(r3, exact_log2(Klass::_lh_instance_slow_path_bit), slow_case);\n-\n-  \/\/ Allocate the instance:\n-  \/\/  If TLAB is enabled:\n-  \/\/    Try to allocate in the TLAB.\n-  \/\/    If fails, go to the slow path.\n-  \/\/    Initialize the allocation.\n-  \/\/    Exit.\n-  \/\/\n-  \/\/  Go to slow path.\n-\n-  if (UseTLAB) {\n-    __ tlab_allocate(r0, r3, 0, noreg, r1, slow_case);\n-\n-    if (ZeroTLAB) {\n-      \/\/ the fields have been already cleared\n-      __ b(initialize_header);\n-    }\n-\n-    \/\/ The object is initialized before the header.  If the object size is\n-    \/\/ zero, go directly to the header initialization.\n-    int header_size = oopDesc::header_size() * HeapWordSize;\n-    assert(is_aligned(header_size, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n-    __ sub(r3, r3, header_size);\n-    __ cbz(r3, initialize_header);\n-\n-    \/\/ Initialize object fields\n-    {\n-      __ add(r2, r0, header_size);\n-      Label loop;\n-      __ bind(loop);\n-      __ str(zr, Address(__ post(r2, BytesPerLong)));\n-      __ sub(r3, r3, BytesPerLong);\n-      __ cbnz(r3, loop);\n-    }\n-\n-    \/\/ initialize object header only.\n-    __ bind(initialize_header);\n-    if (UseCompactObjectHeaders) {\n-      __ ldr(rscratch1, Address(r4, Klass::prototype_header_offset()));\n-      __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n-    } else {\n-      __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n-      __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n-      __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n-      __ store_klass(r0, r4);      \/\/ store klass last\n-    }\n-\n+  __ allocate_instance(r4, r0, r3, r1, true, slow_case);\n@@ -3666,2 +3904,1 @@\n-    __ b(done);\n-  }\n+  __ b(done);\n@@ -3752,0 +3989,3 @@\n+  __ b(done);\n+  __ bind(is_null);\n+\n@@ -3754,4 +3994,1 @@\n-    __ b(done);\n-    __ bind(is_null);\n-  } else {\n-    __ bind(is_null);   \/\/ same as 'done'\n+\n@@ -3878,0 +4115,4 @@\n+  Label is_inline_type;\n+  __ ldr(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+  __ test_markword_is_inline_type(rscratch1, is_inline_type);\n+\n@@ -3979,0 +4220,5 @@\n+\n+  __ bind(is_inline_type);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                    InterpreterRuntime::throw_identity_exception), r0);\n+  __ should_not_reach_here();\n@@ -3989,0 +4235,12 @@\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  Label has_identity;\n+  __ ldr(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+  __ mov(rscratch2, is_inline_type_mask);\n+  __ andr(rscratch1, rscratch1, rscratch2);\n+  __ cmp(rscratch1, rscratch2);\n+  __ br(Assembler::NE, has_identity);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                     InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+  __ bind(has_identity);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":358,"deletions":100,"binary":false,"changes":458,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-VtableStub* VtableStubs::create_vtable_stub(int vtable_index) {\n+VtableStub* VtableStubs::create_vtable_stub(int vtable_index, bool caller_is_c1) {\n@@ -52,1 +52,1 @@\n-  VtableStub* s = new(stub_code_length) VtableStub(true, vtable_index);\n+  VtableStub* s = new(stub_code_length) VtableStub(true, vtable_index, caller_is_c1);\n@@ -65,0 +65,4 @@\n+\/\/ No variance was detected in vtable stub sizes. Setting index_dependent_slop == 0 will unveil any deviation from this observation.\n+  const int index_dependent_slop     = 0;\n+  ByteSize  entry_offset = caller_is_c1 ? Method::from_compiled_inline_offset() :  Method::from_compiled_inline_ro_offset();\n+\n@@ -118,1 +122,1 @@\n-    __ ldr(rscratch1, Address(rmethod, Method::from_compiled_offset()));\n+    __ ldr(rscratch1, Address(rmethod, entry_offset));\n@@ -129,1 +133,1 @@\n-  __ ldr(rscratch1, Address(rmethod, Method::from_compiled_offset()));\n+  __ ldr(rscratch1, Address(rmethod, entry_offset));\n@@ -133,1 +137,2 @@\n-  bookkeeping(masm, tty, s, npe_addr, ame_addr, true, vtable_index, slop_bytes, 0);\n+  slop_bytes += index_dependent_slop; \/\/ add'l slop for size variance due to large itable offsets\n+  bookkeeping(masm, tty, s, npe_addr, ame_addr, true, vtable_index, slop_bytes, index_dependent_slop);\n@@ -139,1 +144,1 @@\n-VtableStub* VtableStubs::create_itable_stub(int itable_index) {\n+VtableStub* VtableStubs::create_itable_stub(int itable_index, bool caller_is_c1) {\n@@ -142,1 +147,1 @@\n-  VtableStub* s = new(stub_code_length) VtableStub(false, itable_index);\n+  VtableStub* s = new(stub_code_length) VtableStub(false, itable_index, caller_is_c1);\n@@ -155,0 +160,4 @@\n+  const int index_dependent_slop = (itable_index == 0) ? 4 :     \/\/ code size change with transition from 8-bit to 32-bit constant (@index == 16).\n+                                   (itable_index < 16) ? 3 : 0;  \/\/ index == 0 generates even shorter code.\n+  ByteSize  entry_offset = caller_is_c1 ? Method::from_compiled_inline_offset() :  Method::from_compiled_inline_ro_offset();\n+\n@@ -209,1 +218,1 @@\n-    __ ldr(rscratch1, Address(rmethod, Method::from_compiled_offset()));\n+    __ ldr(rscratch1, Address(rmethod, entry_offset));\n@@ -219,1 +228,1 @@\n-  __ ldr(rscratch1, Address(rmethod, Method::from_compiled_offset()));\n+  __ ldr(rscratch1, Address(rmethod, entry_offset));\n@@ -232,1 +241,2 @@\n-  bookkeeping(masm, tty, s, npe_addr, ame_addr, false, itable_index, slop_bytes, 0);\n+  slop_bytes += index_dependent_slop; \/\/ add'l slop for size variance due to large itable offsets\n+  bookkeeping(masm, tty, s, npe_addr, ame_addr, false, itable_index, slop_bytes, index_dependent_slop);\n","filename":"src\/hotspot\/cpu\/aarch64\/vtableStubs_aarch64.cpp","additions":20,"deletions":10,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -2573,0 +2573,4 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3197,0 +3197,3 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1835,1 +1835,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n@@ -1876,1 +1876,1 @@\n-    profile_obj_type(ret, R28_mdx, -in_bytes(ReturnTypeEntry::size()), tmp1, tmp2);\n+    profile_obj_type(ret, R28_mdx, -in_bytes(SingleTypeEntry::size()), tmp1, tmp2);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3072,0 +3072,4 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1765,1 +1765,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -1814,1 +1814,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"oops\/objArrayKlass.hpp\"\n@@ -172,0 +173,73 @@\n+\/\/ Implementation of LoadFlattenedArrayStub\n+\n+LoadFlattenedArrayStub::LoadFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr result, CodeEmitInfo* info) {\n+  _array = array;\n+  _index = index;\n+  _result = result;\n+  \/\/ Tell the register allocator that the runtime call will scratch rax.\n+  _scratch_reg = FrameMap::rax_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void LoadFlattenedArrayStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_array->as_register(), 1);\n+  ce->store_parameter(_index->as_register(), 0);\n+  __ call(RuntimeAddress(Runtime1::entry_for(C1StubId::load_flat_array_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  if (_result->as_register() != rax) {\n+    __ movptr(_result->as_register(), rax);\n+  }\n+  __ jmp(_continuation);\n+}\n+\n+\n+\/\/ Implementation of StoreFlattenedArrayStub\n+\n+StoreFlattenedArrayStub::StoreFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr value, CodeEmitInfo* info) {\n+  _array = array;\n+  _index = index;\n+  _value = value;\n+  \/\/ Tell the register allocator that the runtime call will scratch rax.\n+  _scratch_reg = FrameMap::rax_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+\n+void StoreFlattenedArrayStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_array->as_register(), 2);\n+  ce->store_parameter(_index->as_register(), 1);\n+  ce->store_parameter(_value->as_register(), 0);\n+  __ call(RuntimeAddress(Runtime1::entry_for(C1StubId::store_flat_array_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  __ jmp(_continuation);\n+}\n+\n+\n+\/\/ Implementation of SubstitutabilityCheckStub\n+\n+SubstitutabilityCheckStub::SubstitutabilityCheckStub(LIR_Opr left, LIR_Opr right, CodeEmitInfo* info) {\n+  _left = left;\n+  _right = right;\n+  \/\/ Tell the register allocator that the runtime call will scratch rax.\n+  _scratch_reg = FrameMap::rax_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void SubstitutabilityCheckStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_left->as_register(), 1);\n+  ce->store_parameter(_right->as_register(), 0);\n+  __ call(RuntimeAddress(Runtime1::entry_for(C1StubId::substitutability_check_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  __ jmp(_continuation);\n+}\n+\n+\n@@ -224,1 +298,2 @@\n-NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info) {\n+NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result,\n+                                       CodeEmitInfo* info, bool is_null_free) {\n@@ -229,0 +304,1 @@\n+  _is_null_free = is_null_free;\n@@ -237,1 +313,5 @@\n-  __ call(RuntimeAddress(Runtime1::entry_for(C1StubId::new_object_array_id)));\n+  if (_is_null_free) {\n+    __ call(RuntimeAddress(Runtime1::entry_for(C1StubId::new_null_free_array_id)));\n+  } else {\n+    __ call(RuntimeAddress(Runtime1::entry_for(C1StubId::new_object_array_id)));\n+  }\n@@ -247,0 +327,9 @@\n+  if (_throw_ie_stub != nullptr) {\n+    \/\/ When we come here, _obj_reg has already been checked to be non-null.\n+    const int is_value_mask = markWord::inline_type_pattern;\n+    Register mark = _scratch_reg->as_register();\n+    __ movptr(mark, Address(_obj_reg->as_register(), oopDesc::mark_offset_in_bytes()));\n+    __ andptr(mark, is_value_mask);\n+    __ cmpl(mark, is_value_mask);\n+    __ jcc(Assembler::equal, *_throw_ie_stub->entry());\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c1_CodeStubs_x86.cpp","additions":91,"deletions":2,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -39,0 +40,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -464,1 +466,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -501,0 +503,31 @@\n+  if (InlineTypeReturnedAsFields) {\n+  #ifndef _LP64\n+     Unimplemented();\n+  #endif\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    ciType* return_type = compilation()->method()->return_type();\n+    if (return_type->is_inlinetype()) {\n+      ciInlineKlass* vk = return_type->as_inline_klass();\n+      if (vk->can_be_returned_as_fields()) {\n+        address unpack_handler = vk->unpack_handler();\n+        assert(unpack_handler != nullptr, \"must be\");\n+        __ call(RuntimeAddress(unpack_handler));\n+      }\n+    } else if (return_type->is_instance_klass() && (!return_type->is_loaded() || StressCallingConvention)) {\n+      Label skip;\n+      __ test_oop_is_not_inline_type(rax, rscratch1, skip);\n+\n+      \/\/ Load fields from a buffered value with an inline class specific handler\n+      __ load_klass(rdi, rax, rscratch1);\n+      __ movptr(rdi, Address(rdi, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ movptr(rdi, Address(rdi, InlineKlass::unpack_handler_offset()));\n+      \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+      __ testptr(rdi, rdi);\n+      __ jcc(Assembler::zero, skip);\n+      __ call(rdi);\n+\n+      __ bind(skip);\n+    }\n+    \/\/ At this point, rax points to the value object (for interpreter or C1 caller).\n+    \/\/ The fields of the object are copied into registers (for C2 caller).\n+  }\n@@ -503,1 +536,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -525,0 +558,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -1597,1 +1634,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->is_null_free() ||\n@@ -1696,24 +1733,26 @@\n-  __ testptr(obj, obj);\n-  if (op->should_profile()) {\n-    Label not_null;\n-    Register mdo  = klass_RInfo;\n-    __ mov_metadata(mdo, md->constant_encoding());\n-    __ jccb(Assembler::notEqual, not_null);\n-    \/\/ Object is null; update MDO and exit\n-    Address data_addr(mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n-    int header_bits = BitData::null_seen_byte_constant();\n-    __ orb(data_addr, header_bits);\n-    __ jmp(*obj_is_null);\n-    __ bind(not_null);\n-\n-    Label update_done;\n-    Register recv = k_RInfo;\n-    __ load_klass(recv, obj, tmp_load_klass);\n-    type_profile_helper(mdo, md, data, recv, &update_done);\n-\n-    Address nonprofiled_receiver_count_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n-    __ addptr(nonprofiled_receiver_count_addr, DataLayout::counter_increment);\n-\n-    __ bind(update_done);\n-  } else {\n-    __ jcc(Assembler::equal, *obj_is_null);\n+  if (op->need_null_check()) {\n+    __ testptr(obj, obj);\n+    if (op->should_profile()) {\n+      Label not_null;\n+      Register mdo  = klass_RInfo;\n+      __ mov_metadata(mdo, md->constant_encoding());\n+      __ jccb(Assembler::notEqual, not_null);\n+      \/\/ Object is null; update MDO and exit\n+      Address data_addr(mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n+      int header_bits = BitData::null_seen_byte_constant();\n+      __ orb(data_addr, header_bits);\n+      __ jmp(*obj_is_null);\n+      __ bind(not_null);\n+\n+      Label update_done;\n+      Register recv = k_RInfo;\n+      __ load_klass(recv, obj, tmp_load_klass);\n+      type_profile_helper(mdo, md, data, recv, &update_done);\n+\n+      Address nonprofiled_receiver_count_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n+      __ addptr(nonprofiled_receiver_count_addr, DataLayout::counter_increment);\n+\n+      __ bind(update_done);\n+    } else {\n+      __ jcc(Assembler::equal, *obj_is_null);\n+    }\n@@ -1911,0 +1950,102 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing from\/to an array that *may* be a flat array (the\n+  \/\/ declared type is Object[], abstract[], interface[] or VT.ref[]).\n+  \/\/ If this array is a flat array, take the slow path.\n+  __ test_flat_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+  if (!op->value()->is_illegal()) {\n+    \/\/ The array is not a flat array, but it might be null-free. If we are storing\n+    \/\/ a null into a null-free array, take the slow path (which will throw NPE).\n+    Label skip;\n+    __ cmpptr(op->value()->as_register(), NULL_WORD);\n+    __ jcc(Assembler::notEqual, skip);\n+    __ test_null_free_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+    __ bind(skip);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ We are storing into an array that *may* be null-free (the declared type is\n+  \/\/ Object[], abstract[], interface[] or VT.ref[]).\n+  Label test_mark_word;\n+  Register tmp = op->tmp()->as_register();\n+  __ movptr(tmp, Address(op->array()->as_register(), oopDesc::mark_offset_in_bytes()));\n+  __ testl(tmp, markWord::unlocked_value);\n+  __ jccb(Assembler::notZero, test_mark_word);\n+  __ load_prototype_header(tmp, op->array()->as_register(), rscratch1);\n+  __ bind(test_mark_word);\n+  __ testl(tmp, markWord::null_free_array_bit_in_place);\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmpptr(left, right);\n+  __ jcc(Assembler::equal, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  __ testptr(left, right);\n+  __ jcc(Assembler::zero, L_oops_not_equal);\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Inline type check -- if either of the operands is not a inline type,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are inline type\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = op->tmp1()->as_register();\n+    __ movptr(tmp1, (intptr_t)markWord::inline_type_pattern);\n+    __ andptr(tmp1, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(tmp1, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ cmpptr(tmp1, (intptr_t)markWord::inline_type_pattern);\n+    __ jcc(Assembler::notEqual, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ jmp(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedClassPointers) {\n+      __ movl(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ movl(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpl(left_klass_op, right_klass_op);\n+    } else {\n+      __ movptr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ movptr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpptr(left_klass_op, right_klass_op);\n+    }\n+\n+    __ jcc(Assembler::equal, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ jmp(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ jmp(L_end);\n+\n+  \/\/ We've returned from the stub. RAX contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+  __ cmpl(rax, 0);\n+  __ jcc(Assembler::equal, L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+}\n@@ -1971,0 +2112,15 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, nullptr);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n@@ -2840,1 +2996,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2847,1 +3003,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -3028,0 +3184,13 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {\n+  if (null_check) {\n+    __ testptr(obj, obj);\n+    __ jcc(Assembler::zero, *slow_path->entry());\n+  }\n+  if (is_dest) {\n+    __ test_null_free_array_oop(obj, tmp, *slow_path->entry());\n+  } else {\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  }\n+}\n+\n+\n@@ -3047,0 +3216,6 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ jmp(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n@@ -3140,0 +3315,8 @@\n+  \/\/ Handle inline type arrays\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false, (flags & LIR_OpArrayCopy::src_null_check));\n+  }\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true, (flags & LIR_OpArrayCopy::dst_null_check));\n+  }\n+\n@@ -3752,0 +3935,20 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Register obj = op->obj()->as_register();\n+  Register tmp = op->tmp()->as_pointer_register();\n+  Address mdo_addr = as_Address(op->mdp()->as_address_ptr());\n+  bool not_null = op->not_null();\n+  int flag = op->flag();\n+\n+  Label not_inline_type;\n+  if (!not_null) {\n+    __ testptr(obj, obj);\n+    __ jccb(Assembler::zero, not_inline_type);\n+  }\n+\n+  __ test_oop_is_not_inline_type(obj, tmp, not_inline_type);\n+\n+  __ orb(mdo_addr, flag);\n+\n+  __ bind(not_inline_type);\n+}\n+\n@@ -3995,0 +4198,3 @@\n+void LIR_Assembler::check_orig_pc() {\n+  __ cmpptr(frame_map()->address_for_orig_pc_addr(), NULL_WORD);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":235,"deletions":29,"binary":false,"changes":264,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -118,0 +119,13 @@\n+void LIRGenerator::init_temps_for_substitutability_check(LIR_Opr& tmp1, LIR_Opr& tmp2) {\n+  \/\/ We just need one 32-bit temp register for x86\/x64, to check whether both\n+  \/\/ oops have markWord::always_locked_pattern. See LIR_Assembler::emit_opSubstitutabilityCheck().\n+  \/\/ @temp = %r10d\n+  \/\/ mov $0x405, %r10d\n+  \/\/ and (%left), %r10d   \/* if need to check left *\/\n+  \/\/ and (%right), %r10d  \/* if need to check right *\/\n+  \/\/ cmp $0x405, $r10d\n+  \/\/ jne L_oops_not_equal\n+  tmp1 = new_register(T_INT);\n+  tmp2 = LIR_OprFact::illegalOpr;\n+}\n+\n@@ -313,0 +327,6 @@\n+  \/\/ Need a scratch register for inline types on x86\n+  LIR_Opr scratch = LIR_OprFact::illegalOpr;\n+  if ((LockingMode == LM_LIGHTWEIGHT) ||\n+      (EnableValhalla && x->maybe_inlinetype())) {\n+    scratch = new_register(T_ADDRESS);\n+  }\n@@ -318,0 +338,6 @@\n+\n+  CodeStub* throw_ie_stub = x->maybe_inlinetype() ?\n+      new SimpleExceptionStub(C1StubId::throw_identity_exception_id,\n+                              obj.result(), state_for(x))\n+    : nullptr;\n+\n@@ -321,3 +347,2 @@\n-  LIR_Opr tmp = LockingMode == LM_LIGHTWEIGHT ? new_register(T_ADDRESS) : LIR_OprFact::illegalOpr;\n-  monitor_enter(obj.result(), lock, syncTempOpr(), tmp,\n-                        x->monitor_no(), info_for_exception, info);\n+  monitor_enter(obj.result(), lock, syncTempOpr(), scratch,\n+                x->monitor_no(), info_for_exception, info, throw_ie_stub);\n@@ -1301,1 +1326,1 @@\n-  CodeEmitInfo* info = state_for(x, x->state());\n+  CodeEmitInfo* info = state_for(x, x->needs_state_before() ? x->state_before() : x->state());\n@@ -1304,5 +1329,6 @@\n-                       FrameMap::rcx_oop_opr,\n-                       FrameMap::rdi_oop_opr,\n-                       FrameMap::rsi_oop_opr,\n-                       LIR_OprFact::illegalOpr,\n-                       FrameMap::rdx_metadata_opr, info);\n+               !x->is_unresolved() && x->klass()->is_inlinetype(),\n+               FrameMap::rcx_oop_opr,\n+               FrameMap::rdi_oop_opr,\n+               FrameMap::rsi_oop_opr,\n+               LIR_OprFact::illegalOpr,\n+               FrameMap::rdx_metadata_opr, info);\n@@ -1313,1 +1339,0 @@\n-\n@@ -1366,2 +1391,2 @@\n-  CodeStub* slow_path = new NewObjectArrayStub(klass_reg, len, reg, info);\n-  ciKlass* obj = (ciKlass*) ciObjArrayKlass::make(x->klass());\n+  ciKlass* obj = (ciKlass*) x->exact_type();\n+  CodeStub* slow_path = new NewObjectArrayStub(klass_reg, len, reg, info, x->is_null_free());\n@@ -1372,1 +1397,1 @@\n-  __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, T_OBJECT, klass_reg, slow_path);\n+  __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, T_OBJECT, klass_reg, slow_path, true, x->is_null_free());\n@@ -1451,0 +1476,4 @@\n+  if (x->is_null_free()) {\n+    __ null_check(obj.result(), new CodeEmitInfo(info_for_exception));\n+  }\n+\n@@ -1469,1 +1498,1 @@\n-               x->profiled_method(), x->profiled_bci());\n+               x->profiled_method(), x->profiled_bci(), x->is_null_free());\n@@ -1520,1 +1549,1 @@\n-  } else if (tag == longTag || tag == floatTag || tag == doubleTag) {\n+  } else if (tag == longTag || tag == floatTag || tag == doubleTag || x->substitutability_check()) {\n@@ -1540,1 +1569,5 @@\n-  __ cmp(lir_cond(cond), left, right);\n+  if (x->substitutability_check()) {\n+    substitutability_check(x, *xin, *yin);\n+  } else {\n+    __ cmp(lir_cond(cond), left, right);\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":49,"deletions":16,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"runtime\/frame.inline.hpp\"\n@@ -80,0 +81,4 @@\n+    if (EnableValhalla) {\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      andptr(hdr, ~((int) markWord::inline_type_bit_in_place));\n+    }\n@@ -172,2 +177,1 @@\n-#ifdef _LP64\n-  if (UseCompactObjectHeaders) {\n+  if (UseCompactObjectHeaders || EnableValhalla) {\n@@ -176,1 +180,1 @@\n-  } else if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+  } else {\n@@ -178,0 +182,3 @@\n+  }\n+#ifdef _LP64\n+  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n@@ -181,1 +188,1 @@\n-  } else\n+  } else if (!UseCompactObjectHeaders)\n@@ -184,1 +191,0 @@\n-    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n@@ -322,9 +328,1 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n-  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n-  \/\/ Make sure there is enough stack space for this method's activation.\n-  \/\/ Note that we do this before doing an enter(). This matches the\n-  \/\/ ordering of C2's stack overflow check \/ rsp decrement and allows\n-  \/\/ the SharedRuntime stack overflow handling to be consistent\n-  \/\/ between the two compilers.\n-  generate_stack_overflow_check(bang_size_in_bytes);\n-\n+void C1_MacroAssembler::build_frame_helper(int frame_size_in_bytes, int sp_offset_for_orig_pc, int sp_inc, bool reset_orig_pc, bool needs_stack_repair) {\n@@ -337,3 +335,3 @@\n-    \/\/ c2 leaves fpu stack dirty. Clean it on entry\n-    empty_FPU_stack();\n-  }\n+      \/\/ c2 leaves fpu stack dirty. Clean it on entry\n+      empty_FPU_stack();\n+    }\n@@ -341,1 +339,24 @@\n-  decrement(rsp, frame_size_in_bytes); \/\/ does not emit code for frame_size == 0\n+  decrement(rsp, frame_size_in_bytes);\n+\n+  if (needs_stack_repair) {\n+    \/\/ Save stack increment (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    int real_frame_size = sp_inc + frame_size_in_bytes + wordSize;\n+    movptr(Address(rsp, frame_size_in_bytes - wordSize), real_frame_size);\n+  }\n+  if (reset_orig_pc) {\n+    \/\/ Zero orig_pc to detect deoptimization during buffering in the entry points\n+    movptr(Address(rsp, sp_offset_for_orig_pc), 0);\n+  }\n+}\n+\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, bool needs_stack_repair, bool has_scalarized_args, Label* verified_inline_entry_label) {\n+  \/\/ Make sure there is enough stack space for this method's activation.\n+  \/\/ Note that we do this before doing an enter(). This matches the\n+  \/\/ ordering of C2's stack overflow check \/ rsp decrement and allows\n+  \/\/ the SharedRuntime stack overflow handling to be consistent\n+  \/\/ between the two compilers.\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+  generate_stack_overflow_check(bang_size_in_bytes);\n+\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, has_scalarized_args, needs_stack_repair);\n@@ -346,5 +367,4 @@\n-}\n-\n-void C1_MacroAssembler::remove_frame(int frame_size_in_bytes) {\n-  increment(rsp, frame_size_in_bytes);  \/\/ Does not emit code for frame_size == 0\n-  pop(rbp);\n+  if (verified_inline_entry_label != nullptr) {\n+    \/\/ Jump here from the scalarized entry points that already created the frame.\n+    bind(*verified_inline_entry_label);\n+  }\n@@ -354,1 +374,0 @@\n-\n@@ -371,0 +390,58 @@\n+int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label, bool is_inline_ro_entry) {\n+  assert(InlineTypePassFieldsAsArgs, \"sanity\");\n+  \/\/ Make sure there is enough stack space for this method's activation.\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+  generate_stack_overflow_check(bang_size_in_bytes);\n+\n+  GrowableArray<SigEntry>* sig    = ces->sig();\n+  GrowableArray<SigEntry>* sig_cc = is_inline_ro_entry ? ces->sig_cc_ro() : ces->sig_cc();\n+  VMRegPair* regs      = ces->regs();\n+  VMRegPair* regs_cc   = is_inline_ro_entry ? ces->regs_cc_ro() : ces->regs_cc();\n+  int args_on_stack    = ces->args_on_stack();\n+  int args_on_stack_cc = is_inline_ro_entry ? ces->args_on_stack_cc_ro() : ces->args_on_stack_cc();\n+\n+  assert(sig->length() <= sig_cc->length(), \"Zero-sized inline class not allowed!\");\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc->length());\n+  int args_passed = sig->length();\n+  int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);\n+\n+  \/\/ Create a temp frame so we can call into the runtime. It must be properly set up to accommodate GC.\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, true, ces->c1_needs_stack_repair());\n+\n+  \/\/ The runtime call might safepoint, make sure nmethod entry barrier is executed\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ C1 code is not hot enough to micro optimize the nmethod entry barrier with an out-of-line stub\n+  bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n+\n+  \/\/ FIXME -- call runtime only if we cannot in-line allocate all the incoming inline type args.\n+  movptr(rbx, (intptr_t)(ces->method()));\n+  if (is_inline_ro_entry) {\n+    call(RuntimeAddress(Runtime1::entry_for(C1StubId::buffer_inline_args_no_receiver_id)));\n+  } else {\n+    call(RuntimeAddress(Runtime1::entry_for(C1StubId::buffer_inline_args_id)));\n+  }\n+  int rt_call_offset = offset();\n+\n+  \/\/ Remove the temp frame\n+  addptr(rsp, frame_size_in_bytes);\n+  pop(rbp);\n+\n+  \/\/ Check if we need to extend the stack for packing\n+  int sp_inc = 0;\n+  if (args_on_stack > args_on_stack_cc) {\n+    sp_inc = extend_stack_for_inline_args(args_on_stack);\n+  }\n+\n+  shuffle_inline_args(true, is_inline_ro_entry, sig_cc,\n+                      args_passed_cc, args_on_stack_cc, regs_cc, \/\/ from\n+                      args_passed, args_on_stack, regs,          \/\/ to\n+                      sp_inc, rax);\n+\n+  \/\/ Create the real frame. Below jump will then skip over the stack banging and frame\n+  \/\/ setup code in the verified_inline_entry (which has a different real_frame_size).\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, sp_inc, false, ces->c1_needs_stack_repair());\n+\n+  jmp(verified_inline_entry_label);\n+  return rt_call_offset;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":101,"deletions":24,"binary":false,"changes":125,"status":"modified"},{"patch":"@@ -1062,1 +1062,2 @@\n-        int call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_instance), klass);\n+        int call_offset;\n+        call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_instance), klass);\n@@ -1095,0 +1096,1 @@\n+    case C1StubId::new_null_free_array_id:\n@@ -1102,1 +1104,1 @@\n-        } else {\n+        } else if (id == C1StubId::new_object_array_id) {\n@@ -1104,0 +1106,2 @@\n+        } else {\n+          __ set_info(\"new_null_free_array\", dont_gc_arguments);\n@@ -1113,6 +1117,22 @@\n-          int tag = ((id == C1StubId::new_type_array_id)\n-                     ? Klass::_lh_array_tag_type_value\n-                     : Klass::_lh_array_tag_obj_value);\n-          __ cmpl(t0, tag);\n-          __ jcc(Assembler::equal, ok);\n-          __ stop(\"assert(is an array klass)\");\n+          switch (id) {\n+          case C1StubId::new_type_array_id:\n+            __ cmpl(t0, Klass::_lh_array_tag_type_value);\n+            __ jcc(Assembler::equal, ok);\n+            __ stop(\"assert(is a type array klass)\");\n+            break;\n+          case C1StubId::new_object_array_id:\n+            __ cmpl(t0, Klass::_lh_array_tag_obj_value); \/\/ new \"[Ljava\/lang\/Object;\"\n+            __ jcc(Assembler::equal, ok);\n+            __ cmpl(t0, Klass::_lh_array_tag_vt_value);  \/\/ new \"[LVT;\"\n+            __ jcc(Assembler::equal, ok);\n+            __ stop(\"assert(is an object or inline type array klass)\");\n+            break;\n+          case C1StubId::new_null_free_array_id:\n+            __ cmpl(t0, Klass::_lh_array_tag_vt_value);  \/\/ the array can be a flat array.\n+            __ jcc(Assembler::equal, ok);\n+            __ cmpl(t0, Klass::_lh_array_tag_obj_value); \/\/ the array cannot be a flat array (due to InlineArrayElementMaxFlatSize, etc)\n+            __ jcc(Assembler::equal, ok);\n+            __ stop(\"assert(is an object or inline type array klass)\");\n+            break;\n+          default:  ShouldNotReachHere();\n+          }\n@@ -1129,1 +1149,1 @@\n-        } else {\n+        } else if (id == C1StubId::new_object_array_id) {\n@@ -1131,0 +1151,3 @@\n+        } else {\n+          assert(id == C1StubId::new_null_free_array_id, \"must be\");\n+          call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_null_free_array), klass, length);\n@@ -1162,0 +1185,77 @@\n+    case C1StubId::load_flat_array_id:\n+      {\n+        StubFrame f(sasm, \"load_flat_array\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 3);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(1, rax); \/\/ rax,: array\n+        f.load_argument(0, rbx); \/\/ rbx,: index\n+        int call_offset = __ call_RT(rax, noreg, CAST_FROM_FN_PTR(address, load_flat_array), rax, rbx);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_rax(sasm);\n+\n+        \/\/ rax,: loaded element at array[index]\n+        __ verify_oop(rax);\n+      }\n+      break;\n+\n+    case C1StubId::store_flat_array_id:\n+      {\n+        StubFrame f(sasm, \"store_flat_array\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 4);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(2, rax); \/\/ rax,: array\n+        f.load_argument(1, rbx); \/\/ rbx,: index\n+        f.load_argument(0, rcx); \/\/ rcx,: value\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, store_flat_array), rax, rbx, rcx);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_rax(sasm);\n+      }\n+      break;\n+\n+    case C1StubId::substitutability_check_id:\n+      {\n+        StubFrame f(sasm, \"substitutability_check\", dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 3);\n+\n+        \/\/ Called with store_parameter and not C abi\n+\n+        f.load_argument(1, rax); \/\/ rax,: left\n+        f.load_argument(0, rbx); \/\/ rbx,: right\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, substitutability_check), rax, rbx);\n+\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_rax(sasm);\n+\n+        \/\/ rax,: are the two operands substitutable\n+      }\n+      break;\n+\n+\n+    case C1StubId::buffer_inline_args_id:\n+    case C1StubId::buffer_inline_args_no_receiver_id:\n+      {\n+        const char* name = (id == C1StubId::buffer_inline_args_id) ?\n+          \"buffer_inline_args\" : \"buffer_inline_args_no_receiver\";\n+        StubFrame f(sasm, name, dont_gc_arguments);\n+        OopMap* map = save_live_registers(sasm, 2);\n+        Register method = rbx;\n+        address entry = (id == C1StubId::buffer_inline_args_id) ?\n+          CAST_FROM_FN_PTR(address, buffer_inline_args) :\n+          CAST_FROM_FN_PTR(address, buffer_inline_args_no_receiver);\n+        int call_offset = __ call_RT(rax, noreg, entry, method);\n+        oop_maps = new OopMapSet();\n+        oop_maps->add_gc_map(call_offset, map);\n+        restore_live_registers_except_rax(sasm);\n+        __ verify_oop(rax);  \/\/ rax: an array of buffered value objects\n+      }\n+      break;\n+\n@@ -1262,1 +1362,1 @@\n-      { StubFrame f(sasm, \"throw_incompatible_class_cast_exception\", dont_gc_arguments);\n+      { StubFrame f(sasm, \"throw_incompatible_class_change_error\", dont_gc_arguments);\n@@ -1267,0 +1367,12 @@\n+    case C1StubId::throw_illegal_monitor_state_exception_id:\n+      { StubFrame f(sasm, \"throw_illegal_monitor_state_exception\", dont_gc_arguments);\n+        oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_illegal_monitor_state_exception), false);\n+      }\n+      break;\n+\n+    case C1StubId::throw_identity_exception_id:\n+      { StubFrame f(sasm, \"throw_identity_exception\", dont_gc_arguments);\n+        oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_identity_exception), true);\n+      }\n+      break;\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_Runtime1_x86.cpp","additions":122,"deletions":10,"binary":false,"changes":132,"status":"modified"},{"patch":"@@ -52,1 +52,20 @@\n-void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void C2_MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+    Register klass = rscratch1;\n+\n+    mov_metadata(klass, C->method()->holder()->constant_encoding());\n+    clinit_barrier(klass, r15_thread, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+\n+    bind(L_skip_barrier);\n+  }\n+\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  bool fp_mode_24b = false;\n+  int stack_bang_size = C->output()->need_stack_bang(bangsize) ? bangsize : 0;\n@@ -105,0 +124,6 @@\n+  if (C->needs_stack_repair()) {\n+    \/\/ Save stack increment just below the saved rbp (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    movptr(Address(rsp, framesize - wordSize), sp_inc + framesize + wordSize);\n+  }\n+\n@@ -133,0 +158,1 @@\n+}\n@@ -134,17 +160,15 @@\n-  if (!is_stub) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n- #ifdef _LP64\n-    if (BarrierSet::barrier_set()->barrier_set_nmethod() != nullptr) {\n-      \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n-      Label dummy_slow_path;\n-      Label dummy_continuation;\n-      Label* slow_path = &dummy_slow_path;\n-      Label* continuation = &dummy_continuation;\n-      if (!Compile::current()->output()->in_scratch_emit_size()) {\n-        \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n-        C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n-        Compile::current()->output()->add_stub(stub);\n-        slow_path = &stub->entry();\n-        continuation = &stub->continuation();\n-      }\n-      bs->nmethod_entry_barrier(this, slow_path, continuation);\n+void C2_MacroAssembler::entry_barrier() {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+#ifdef _LP64\n+  if (BarrierSet::barrier_set()->barrier_set_nmethod() != nullptr) {\n+    \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n+    Label dummy_slow_path;\n+    Label dummy_continuation;\n+    Label* slow_path = &dummy_slow_path;\n+    Label* continuation = &dummy_continuation;\n+    if (!Compile::current()->output()->in_scratch_emit_size()) {\n+      \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+      C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n+      Compile::current()->output()->add_stub(stub);\n+      slow_path = &stub->entry();\n+      continuation = &stub->continuation();\n@@ -152,0 +176,2 @@\n+    bs->nmethod_entry_barrier(this, slow_path, continuation);\n+  }\n@@ -153,2 +179,2 @@\n-    \/\/ Don't bother with out-of-line nmethod entry barrier stub for x86_32.\n-    bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n+  \/\/ Don't bother with out-of-line nmethod entry barrier stub for x86_32.\n+  bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n@@ -156,1 +182,0 @@\n-  }\n@@ -294,0 +319,4 @@\n+    if (EnableValhalla) {\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      andptr(tmpReg, ~((int) markWord::inline_type_bit_in_place));\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":50,"deletions":21,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -148,1 +148,0 @@\n-      sender_unextended_sp = sender_sp;\n@@ -152,2 +151,2 @@\n-      saved_fp = (intptr_t*) *(sender_sp - frame::sender_sp_offset);\n-    }\n+      intptr_t** saved_fp_addr = (intptr_t**) (sender_sp - frame::sender_sp_offset);\n+      saved_fp = *saved_fp_addr;\n@@ -155,0 +154,4 @@\n+      \/\/ Repair the sender sp if this is a method with scalarized inline type args\n+      sender_sp = repair_sender_sp(sender_sp, saved_fp_addr);\n+      sender_unextended_sp = sender_sp;\n+    }\n@@ -699,0 +702,15 @@\n+\/\/ Check for a method with scalarized inline type arguments that needs\n+\/\/ a stack repair and return the repaired sender stack pointer.\n+intptr_t* frame::repair_sender_sp(intptr_t* sender_sp, intptr_t** saved_fp_addr) const {\n+  nmethod* nm = _cb->as_nmethod_or_null();\n+  if (nm != nullptr && nm->needs_stack_repair()) {\n+    \/\/ The stack increment resides just below the saved rbp on the stack\n+    \/\/ and does not account for the return address.\n+    intptr_t* real_frame_size_addr = (intptr_t*) (saved_fp_addr - 1);\n+    int real_frame_size = ((*real_frame_size_addr) + wordSize) \/ wordSize;\n+    assert(real_frame_size >= _cb->frame_size() && real_frame_size <= 1000000, \"invalid frame size\");\n+    sender_sp = unextended_sp() + real_frame_size;\n+  }\n+  return sender_sp;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.cpp","additions":21,"deletions":3,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -247,2 +247,18 @@\n-  \/\/ Determine and save the live input values\n-  __ push_call_clobbered_registers();\n+  if (EnableValhalla && InlineTypePassFieldsAsArgs) {\n+    \/\/ Barriers might be emitted when converting between (scalarized) calling conventions for inline\n+    \/\/ types. Save all argument registers before calling into the runtime.\n+    \/\/ TODO: use push_set() (see JDK-8283327 push\/pop_call_clobbered_registers & aarch64 )\n+    __ pusha();\n+    __ subptr(rsp, 64);\n+    __ movdbl(Address(rsp, 0),  j_farg0);\n+    __ movdbl(Address(rsp, 8),  j_farg1);\n+    __ movdbl(Address(rsp, 16), j_farg2);\n+    __ movdbl(Address(rsp, 24), j_farg3);\n+    __ movdbl(Address(rsp, 32), j_farg4);\n+    __ movdbl(Address(rsp, 40), j_farg5);\n+    __ movdbl(Address(rsp, 48), j_farg6);\n+    __ movdbl(Address(rsp, 56), j_farg7);\n+  } else {\n+    \/\/ Determine and save the live input values\n+    __ push_call_clobbered_registers();\n+  }\n@@ -280,1 +296,15 @@\n-  __ pop_call_clobbered_registers();\n+  if (EnableValhalla && InlineTypePassFieldsAsArgs) {\n+    \/\/ Restore registers\n+    __ movdbl(j_farg0, Address(rsp, 0));\n+    __ movdbl(j_farg1, Address(rsp, 8));\n+    __ movdbl(j_farg2, Address(rsp, 16));\n+    __ movdbl(j_farg3, Address(rsp, 24));\n+    __ movdbl(j_farg4, Address(rsp, 32));\n+    __ movdbl(j_farg5, Address(rsp, 40));\n+    __ movdbl(j_farg6, Address(rsp, 48));\n+    __ movdbl(j_farg7, Address(rsp, 56));\n+    __ addptr(rsp, 64);\n+    __ popa();\n+  } else {\n+    __ pop_call_clobbered_registers();\n+  }\n@@ -352,3 +382,14 @@\n-  \/\/ save the live input values\n-  RegSet saved = RegSet::of(store_addr NOT_LP64(COMMA thread));\n-  __ push_set(saved);\n+  \/\/ Barriers might be emitted when converting between (scalarized) calling conventions for inline\n+  \/\/ types. Save all argument registers before calling into the runtime.\n+  \/\/ TODO: use push_set() (see JDK-8283327 push\/pop_call_clobbered_registers & aarch64)\n+  __ pusha();\n+  __ subptr(rsp, 64);\n+  __ movdbl(Address(rsp, 0),  j_farg0);\n+  __ movdbl(Address(rsp, 8),  j_farg1);\n+  __ movdbl(Address(rsp, 16), j_farg2);\n+  __ movdbl(Address(rsp, 24), j_farg3);\n+  __ movdbl(Address(rsp, 32), j_farg4);\n+  __ movdbl(Address(rsp, 40), j_farg5);\n+  __ movdbl(Address(rsp, 48), j_farg6);\n+  __ movdbl(Address(rsp, 56), j_farg7);\n+\n@@ -356,1 +397,12 @@\n-  __ pop_set(saved);\n+\n+  \/\/ Restore registers\n+  __ movdbl(j_farg0, Address(rsp, 0));\n+  __ movdbl(j_farg1, Address(rsp, 8));\n+  __ movdbl(j_farg2, Address(rsp, 16));\n+  __ movdbl(j_farg3, Address(rsp, 24));\n+  __ movdbl(j_farg4, Address(rsp, 32));\n+  __ movdbl(j_farg5, Address(rsp, 40));\n+  __ movdbl(j_farg6, Address(rsp, 48));\n+  __ movdbl(j_farg7, Address(rsp, 56));\n+  __ addptr(rsp, 64);\n+  __ popa();\n@@ -466,0 +518,1 @@\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n@@ -467,1 +520,1 @@\n-  bool needs_pre_barrier = as_normal;\n+  bool needs_pre_barrier = as_normal && !dest_uninitialized;\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.cpp","additions":61,"deletions":8,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"asm\/macroAssembler.inline.hpp\"\n@@ -29,0 +30,1 @@\n+#include \"gc\/shared\/barrierSetRuntime.hpp\"\n@@ -200,0 +202,13 @@\n+void BarrierSetAssembler::flat_field_copy(MacroAssembler* masm, DecoratorSet decorators,\n+                                     Register src, Register dst, Register inline_layout_info) {\n+  \/\/ flat_field_copy implementation is fairly complex, and there are not any\n+  \/\/ \"short-cuts\" to be made from asm. What there is, appears to have the same\n+  \/\/ cost in C++, so just \"call_VM_leaf\" for now rather than maintain hundreds\n+  \/\/ of hand-rolled instructions...\n+  if (decorators & IS_DEST_UNINITIALIZED) {\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, BarrierSetRuntime::value_copy_is_dest_uninitialized), src, dst, inline_layout_info);\n+  } else {\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, BarrierSetRuntime::value_copy), src, dst, inline_layout_info);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetAssembler_x86.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -199,0 +199,25 @@\n+static void set_immediate(nmethod* nm, jint val) {\n+  NativeNMethodCmpBarrier* cmp1 = native_nmethod_barrier(nm);\n+  cmp1->set_immediate(val);\n+\n+  if (!nm->is_osr_method() && nm->method()->has_scalarized_args()) {\n+    \/\/ nmethods with scalarized arguments have multiple entry points that each have an own nmethod entry barrier\n+    assert(nm->verified_entry_point() != nm->verified_inline_entry_point(), \"scalarized entry point not found\");\n+    address method_body = nm->is_compiled_by_c1() ? nm->verified_inline_entry_point() : nm->verified_entry_point();\n+    address entry_point2 = nm->is_compiled_by_c1() ? nm->verified_entry_point() : nm->verified_inline_entry_point();\n+\n+    int barrier_offset = reinterpret_cast<address>(cmp1) - method_body;\n+    NativeNMethodCmpBarrier* cmp2 = reinterpret_cast<NativeNMethodCmpBarrier*>(entry_point2 + barrier_offset);\n+    assert(cmp1 != cmp2, \"sanity\");\n+    debug_only(cmp2->verify());\n+    cmp2->set_immediate(val);\n+\n+    if (method_body != nm->verified_inline_ro_entry_point() && entry_point2 != nm->verified_inline_ro_entry_point()) {\n+      NativeNMethodCmpBarrier* cmp3 = reinterpret_cast<NativeNMethodCmpBarrier*>(nm->verified_inline_ro_entry_point() + barrier_offset);\n+      assert(cmp1 != cmp3 && cmp2 != cmp3, \"sanity\");\n+      debug_only(cmp3->verify());\n+      cmp3->set_immediate(val);\n+    }\n+  }\n+}\n+\n@@ -204,2 +229,1 @@\n-  NativeNMethodCmpBarrier* cmp = native_nmethod_barrier(nm);\n-  cmp->set_immediate(value);\n+  set_immediate(nm, value);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetNMethod_x86.cpp","additions":26,"deletions":2,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -143,1 +143,8 @@\n-      store_check(masm, dst.base(), dst);\n+      if (tmp3 != noreg) {\n+        \/\/ Called by MacroAssembler::pack_inline_helper. We cannot corrupt the dst.base() register\n+        __ movptr(tmp3, dst.base());\n+        store_check(masm, tmp3, dst);\n+      } else {\n+        \/\/ It's OK to corrupt the dst.base() register.\n+        store_check(masm, dst.base(), dst);\n+      }\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/cardTableBarrierSetAssembler_x86.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"oops\/constMethodFlags.hpp\"\n@@ -34,0 +35,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -175,1 +177,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -220,1 +222,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n@@ -570,1 +572,2 @@\n-                                                  Label& ok_is_subtype) {\n+                                                  Label& ok_is_subtype,\n+                                                  bool profile) {\n@@ -578,1 +581,3 @@\n-  profile_typecheck(rcx, Rsub_klass, rdi); \/\/ blows rcx, reloads rdi\n+  if (profile) {\n+    profile_typecheck(rcx, Rsub_klass, rdi); \/\/ blows rcx, reloads rdi\n+  }\n@@ -1030,1 +1035,1 @@\n- \/\/ get method access flags\n+  \/\/ get method access flags\n@@ -1156,4 +1161,2 @@\n-  \/\/ remove activation\n-  \/\/ get sender sp\n-  movptr(rbx,\n-         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    movptr(rbx,\n+               Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n@@ -1183,0 +1186,40 @@\n+\n+  \/\/ remove activation\n+  \/\/ get sender sp\n+  movptr(rbx,\n+         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    Label skip;\n+    test_oop_is_not_inline_type(rax, rscratch1, skip);\n+\n+#ifndef _LP64\n+    super_call_VM_leaf(StubRoutines::load_inline_type_fields_in_regs());\n+#else\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+    load_klass(rdi, rax, rscratch1);\n+    movptr(rdi, Address(rdi, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    movptr(rdi, Address(rdi, InlineKlass::unpack_handler_offset()));\n+    \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+    testptr(rdi, rdi);\n+    jcc(Assembler::zero, skip);\n+    call(rdi);\n+#endif\n+#ifdef ASSERT\n+    \/\/ TODO 8284443 Enable\n+    if (StressCallingConvention && false) {\n+      Label skip_stress;\n+      movptr(rscratch1, Address(rbp, frame::interpreter_frame_method_offset * wordSize));\n+      movl(rscratch1, Address(rscratch1, Method::flags_offset()));\n+      testl(rcx, MethodFlags::has_scalarized_return_flag());\n+      jcc(Assembler::zero, skip_stress);\n+      load_klass(rax, rax, rscratch1);\n+      orptr(rax, 1);\n+      bind(skip_stress);\n+    }\n+#endif\n+    \/\/ call above kills the value in rbx. Reload it.\n+    movptr(rbx, Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    bind(skip);\n+  }\n@@ -1203,0 +1246,61 @@\n+void InterpreterMacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                                  Register t1, Register t2,\n+                                                  bool clear_fields, Label& alloc_failed) {\n+  MacroAssembler::allocate_instance(klass, new_obj, t1, t2, clear_fields, alloc_failed);\n+  if (DTraceMethodProbes) {\n+    \/\/ Trigger dtrace event for fastpath\n+    push(atos);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(oopDesc*)>(SharedRuntime::dtrace_object_alloc)), new_obj);\n+    pop(atos);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::read_flat_field(Register entry, Register tmp1, Register tmp2, Register obj) {\n+  Label alloc_failed, empty_value, done;\n+  const Register alloc_temp = LP64_ONLY(rscratch1) NOT_LP64(rsi);\n+  const Register dst_temp   = LP64_ONLY(rscratch2) NOT_LP64(rdi);\n+  assert_different_registers(obj, entry, tmp1, tmp2, dst_temp, r8, r9);\n+\n+  \/\/ FIXME: code below could be re-written to better use InlineLayoutInfo data structure\n+  \/\/ see aarch64 version\n+\n+  \/\/ Grap the inline field klass\n+  const Register field_klass = tmp1;\n+  load_unsigned_short(tmp2, Address(entry, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+  movptr(tmp1, Address(entry, ResolvedFieldEntry::field_holder_offset()));\n+  get_inline_type_field_klass(tmp1, tmp2, field_klass);\n+\n+    \/\/check for empty value klass\n+  test_klass_is_empty_inline_type(field_klass, dst_temp, empty_value);\n+\n+  \/\/ allocate buffer\n+  push(obj);  \/\/ push object being read from     \/\/ FIXME spilling on stack could probably be avoided by using tmp2\n+  allocate_instance(field_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+\n+  \/\/ Have an oop instance buffer, copy into it\n+  load_unsigned_short(r9, Address(entry, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+  movptr(r8, Address(entry, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+  inline_layout_info(r8, r9, r8); \/\/ holder, index, info => InlineLayoutInfo into r8\n+\n+  payload_addr(obj, dst_temp, field_klass);\n+  pop(alloc_temp);             \/\/ restore object being read from\n+  load_sized_value(tmp2, Address(entry, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n+  lea(tmp2, Address(alloc_temp, tmp2));\n+  \/\/ call_VM_leaf, clobbers a few regs, save restore new obj\n+  push(obj);\n+  \/\/ access_value_copy(IS_DEST_UNINITIALIZED, tmp2, dst_temp, field_klass);\n+  flat_field_copy(IS_DEST_UNINITIALIZED, tmp2, dst_temp, r8);\n+  pop(obj);\n+  jmp(done);\n+\n+  bind(empty_value);\n+  get_empty_inline_type_oop(field_klass, dst_temp, obj);\n+  jmp(done);\n+\n+  bind(alloc_failed);\n+  pop(obj);\n+  call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flat_field),\n+          obj, entry);\n+  get_vm_result(obj, r15_thread);\n+  bind(done);\n+}\n@@ -1255,0 +1359,4 @@\n+      if (EnableValhalla) {\n+        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+        andptr(swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+      }\n@@ -1612,1 +1720,1 @@\n-void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp) {\n+void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp, bool acmp) {\n@@ -1624,1 +1732,1 @@\n-    update_mdp_by_constant(mdp, in_bytes(BranchData::branch_data_size()));\n+    update_mdp_by_constant(mdp, acmp ? in_bytes(ACmpData::acmp_data_size()): in_bytes(BranchData::branch_data_size()));\n@@ -1687,1 +1795,1 @@\n-    record_klass_in_profile(receiver, mdp, reg2, true);\n+    record_klass_in_profile(receiver, mdp, reg2);\n@@ -1707,4 +1815,3 @@\n-void InterpreterMacroAssembler::record_klass_in_profile_helper(\n-                                        Register receiver, Register mdp,\n-                                        Register reg2, int start_row,\n-                                        Label& done, bool is_virtual_call) {\n+void InterpreterMacroAssembler::record_klass_in_profile_helper(Register receiver, Register mdp,\n+                                                               Register reg2, int start_row,\n+                                                               Label& done) {\n@@ -1814,3 +1921,1 @@\n-void InterpreterMacroAssembler::record_klass_in_profile(Register receiver,\n-                                                        Register mdp, Register reg2,\n-                                                        bool is_virtual_call) {\n+void InterpreterMacroAssembler::record_klass_in_profile(Register receiver, Register mdp, Register reg2) {\n@@ -1820,1 +1925,1 @@\n-  record_klass_in_profile_helper(receiver, mdp, reg2, 0, done, is_virtual_call);\n+  record_klass_in_profile_helper(receiver, mdp, reg2, 0, done);\n@@ -1897,1 +2002,1 @@\n-      record_klass_in_profile(klass, mdp, reg2, false);\n+      record_klass_in_profile(klass, mdp, reg2);\n@@ -1959,0 +2064,114 @@\n+template <class ArrayData> void InterpreterMacroAssembler::profile_array_type(Register mdp,\n+                                                                              Register array,\n+                                                                              Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, array);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayData::array_offset())));\n+\n+    Label not_flat;\n+    test_non_flat_array_oop(array, tmp, not_flat);\n+\n+    set_mdp_flag_at(mdp, ArrayData::flat_array_byte_constant());\n+\n+    bind(not_flat);\n+\n+    Label not_null_free;\n+    test_non_null_free_array_oop(array, tmp, not_null_free);\n+\n+    set_mdp_flag_at(mdp, ArrayData::null_free_array_byte_constant());\n+\n+    bind(not_null_free);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+template void InterpreterMacroAssembler::profile_array_type<ArrayLoadData>(Register mdp,\n+                                                                           Register array,\n+                                                                           Register tmp);\n+template void InterpreterMacroAssembler::profile_array_type<ArrayStoreData>(Register mdp,\n+                                                                            Register array,\n+                                                                            Register tmp);\n+\n+\n+void InterpreterMacroAssembler::profile_multiple_element_types(Register mdp, Register element, Register tmp, const Register tmp2) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    Label done, update;\n+    testptr(element, element);\n+    jccb(Assembler::notZero, update);\n+    set_mdp_flag_at(mdp, BitData::null_seen_byte_constant());\n+    jmp(done);\n+\n+    bind(update);\n+    load_klass(tmp, element, rscratch1);\n+\n+    \/\/ Record the object type.\n+    record_klass_in_profile(tmp, mdp, tmp2);\n+\n+    bind(done);\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayStoreData::array_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_element_type(Register mdp,\n+                                                     Register element,\n+                                                     Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, element);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadData::element_offset())));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadData::array_load_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_acmp(Register mdp,\n+                                             Register left,\n+                                             Register right,\n+                                             Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, left);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::left_offset())));\n+\n+    Label left_not_inline_type;\n+    test_oop_is_not_inline_type(left, tmp, left_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::left_inline_type_byte_constant());\n+    bind(left_not_inline_type);\n+\n+    mov(tmp, right);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::right_offset())));\n+\n+    Label right_not_inline_type;\n+    test_oop_is_not_inline_type(right, tmp, right_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::right_inline_type_byte_constant());\n+    bind(right_not_inline_type);\n+\n+    bind(profile_continue);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":240,"deletions":21,"binary":false,"changes":261,"status":"modified"},{"patch":"@@ -63,0 +63,4 @@\n+void InterpreterRuntime::SignatureHandlerGenerator::pass_valuetype() {\n+  box (offset(), jni_offset() + 1);\n+}\n+\n@@ -130,0 +134,7 @@\n+  virtual void pass_valuetype() {\n+    \/\/ pass address of from\n+    intptr_t from_addr = (intptr_t)(_from + Interpreter::local_offset_in_bytes(0));\n+    *_to++ = (*(intptr_t*)from_addr == 0) ? NULL_WORD : from_addr;\n+    _from -= Interpreter::stackElementSize;\n+   }\n+\n","filename":"src\/hotspot\/cpu\/x86\/interpreterRT_x86_32.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"runtime\/jfieldIDWorkaround.hpp\"\n@@ -83,1 +84,1 @@\n-  __ shrptr(roffset, 2);                         \/\/ offset\n+  __ shrptr(roffset, jfieldIDWorkaround::offset_shift);                         \/\/ offset\n@@ -185,1 +186,1 @@\n-  __ shrptr(roffset, 2);                         \/\/ offset\n+  __ shrptr(roffset, jfieldIDWorkaround::offset_shift);                         \/\/ offset\n","filename":"src\/hotspot\/cpu\/x86\/jniFastGetField_x86_64.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -45,0 +46,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -55,0 +57,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -58,0 +61,4 @@\n+#include \"vmreg_x86.inline.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/output.hpp\"\n+#endif\n@@ -1731,0 +1738,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2915,0 +2926,125 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  andptr(markword, markWord::inline_type_mask_in_place);\n+  cmpptr(markword, markWord::inline_type_pattern);\n+  jcc(Assembler::equal, is_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type) {\n+  load_unsigned_short(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  testl(temp_reg, JVM_ACC_IDENTITY);\n+  jcc(Assembler::zero, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type) {\n+  testptr(object, object);\n+  jcc(Assembler::zero, not_inline_type);\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  movptr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  andptr(tmp, is_inline_type_mask);\n+  cmpptr(tmp, is_inline_type_mask);\n+  jcc(Assembler::notEqual, not_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(klass, temp_reg, done_check);\n+    stop(\"test_klass_is_empty_inline_type with non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  movl(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));\n+  testl(temp_reg, InstanceKlassFlags::is_empty_inline_type_value());\n+  jcc(Assembler::notZero, is_empty_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::notEqual, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::equal, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_flat(Register flags, Register temp_reg, Label& is_flat) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_flat_shift);\n+  jcc(Assembler::notEqual, is_flat);\n+}\n+\n+void MacroAssembler::test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::has_null_marker_shift);\n+  jcc(Assembler::notEqual, has_null_marker);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  movptr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  testl(temp_reg, markWord::unlocked_value);\n+  jccb(Assembler::notZero, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  push(rscratch1);\n+  load_prototype_header(temp_reg, oop, rscratch1);\n+  pop(rscratch1);\n+\n+  bind(test_mark_word);\n+  testl(temp_reg, test_bit);\n+  jcc((jmp_set) ? Assembler::notZero : Assembler::zero, jmp_label);\n+}\n+\n+void MacroAssembler::test_flat_array_oop(Register oop, Register temp_reg,\n+                                         Label& is_flat_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flat_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_flat_array_layout(temp_reg, is_flat_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_flat_array_oop(Register oop, Register temp_reg,\n+                                             Label& is_non_flat_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flat_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_flat_array_layout(temp_reg, is_non_flat_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+\n+void MacroAssembler::test_flat_array_layout(Register lh, Label& is_flat_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::notZero, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_layout(Register lh, Label& is_non_flat_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::zero, is_non_flat_array);\n+}\n+\n@@ -4102,0 +4238,128 @@\n+\/\/ Object \/ value buffer allocation...\n+\/\/\n+\/\/ Kills klass and rsi on LP64\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == rax, \"needs to be rax\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  movl(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it is malformed in some way\n+  testl(layout_size, Klass::_lh_instance_slow_path_bit);\n+  jcc(Assembler::notZero, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path.\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  push(klass);\n+  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(klass);\n+#ifndef _LP64\n+  if (UseTLAB) {\n+    get_thread(thread);\n+  }\n+#endif \/\/ _LP64\n+\n+  if (UseTLAB) {\n+    tlab_allocate(thread, new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      jmp(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      jmp(initialize_object);\n+    }\n+  } else {\n+    jmp(slow_case);\n+  }\n+\n+  \/\/ If UseTLAB is true, the object is created above and there is an initialize need.\n+  \/\/ Otherwise, skip and go to the slow path.\n+  if (UseTLAB) {\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      if (UseCompactObjectHeaders) {\n+        assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n+        decrement(layout_size, oopDesc::base_offset_in_bytes());\n+      } else {\n+        decrement(layout_size, sizeof(oopDesc));\n+      }\n+      jcc(Assembler::zero, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+      Register zero = klass;\n+      xorl(zero, zero);    \/\/ use zero reg to clear memory (shorter code)\n+      shrl(layout_size, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      \/\/ Ignore partial flag stall after shrl() since it is debug VM\n+      jcc(Assembler::carryClear, L);\n+      stop(\"object size is not multiple of 2 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        bind(loop);\n+        int header_size_bytes = oopDesc::header_size() * HeapWordSize;\n+        assert(is_aligned(header_size_bytes, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n+        movptr(Address(new_obj, layout_size, Address::times_8, header_size_bytes - 1*oopSize), zero);\n+        NOT_LP64(movptr(Address(new_obj, layout_size, Address::times_8, header_size_bytes - 2*oopSize), zero));\n+        decrement(layout_size);\n+        jcc(Assembler::notZero, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    if (UseCompactObjectHeaders || EnableValhalla) {\n+      pop(klass);\n+      Register mark_word = t2;\n+      movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+      movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+    } else {\n+     movptr(Address(new_obj, oopDesc::mark_offset_in_bytes()),\n+            (intptr_t)markWord::prototype().value()); \/\/ header\n+     pop(klass);   \/\/ get saved klass back in the register.\n+    }\n+    if (!UseCompactObjectHeaders) {\n+#ifdef _LP64\n+      xorl(rsi, rsi);                 \/\/ use zero reg to clear memory (shorter code)\n+      store_klass_gap(new_obj, rsi);  \/\/ zero klass gap for compressed oops\n+#endif\n+      movptr(t2, klass);         \/\/ preserve klass\n+      store_klass(new_obj, t2, rscratch1);  \/\/ src klass reg is potentially compressed\n+    }\n+    jmp(done);\n+  }\n+\n+  bind(slow_case);\n+  pop(klass);\n+  bind(slow_case_no_pop);\n+  jmp(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -4360,0 +4624,63 @@\n+void MacroAssembler::get_inline_type_field_klass(Register holder_klass, Register index, Register inline_klass) {\n+  inline_layout_info(holder_klass, index, inline_klass);\n+  movptr(inline_klass, Address(inline_klass, InlineLayoutInfo::klass_offset()));\n+}\n+\n+void MacroAssembler::inline_layout_info(Register holder_klass, Register index, Register layout_info) {\n+  movptr(layout_info, Address(holder_klass, InstanceKlass::inline_layout_info_array_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cmpptr(layout_info, 0);\n+    jcc(Assembler::notEqual, done);\n+    stop(\"inline_layout_info_array is null\");\n+    bind(done);\n+  }\n+#endif\n+\n+  InlineLayoutInfo array[2];\n+  int size = (char*)&array[1] - (char*)&array[0]; \/\/ computing size of array elements\n+  if (is_power_of_2(size)) {\n+    shll(index, log2i_exact(size)); \/\/ Scale index by power of 2\n+  } else {\n+    imull(index, index, size); \/\/ Scale the index to be the entry index * array_element_size\n+  }\n+  lea(layout_info, Address(layout_info, index, Address::times_1, Array<InlineLayoutInfo>::base_offset_in_bytes()));\n+}\n+\n+void MacroAssembler::get_default_value_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_default_value_oop from non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  Register offset = temp_reg;\n+  \/\/ Getting the offset of the pre-allocated default value\n+  movptr(offset, Address(inline_klass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())));\n+  movl(offset, Address(offset, in_bytes(InlineKlass::default_value_offset_offset())));\n+\n+  \/\/ Getting the mirror\n+  movptr(obj, Address(inline_klass, in_bytes(Klass::java_mirror_offset())));\n+  resolve_oop_handle(obj, inline_klass);\n+\n+  \/\/ Getting the pre-allocated default value from the mirror\n+  Address field(obj, offset, Address::times_1);\n+  load_heap_oop(obj, field);\n+}\n+\n+void MacroAssembler::get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_empty_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_empty_value from non-empty inline klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  get_default_value_oop(inline_klass, temp_reg, obj);\n+}\n+\n+\n@@ -5429,1 +5756,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -5490,1 +5821,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -5977,0 +6312,13 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(dst, src);\n+  } else if (UseCompressedClassPointers) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else\n+#endif\n+  {\n+    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -6002,0 +6350,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src, Register tmp) {\n+  load_klass(dst, src, tmp);\n+  movptr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -6074,0 +6427,40 @@\n+void MacroAssembler::flat_field_copy(DecoratorSet decorators, Register src, Register dst,\n+                                     Register inline_layout_info) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->flat_field_copy(this, decorators, src, dst, inline_layout_info);\n+}\n+\n+void MacroAssembler::payload_offset(Register inline_klass, Register offset) {\n+  movptr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  movl(offset, Address(offset, InlineKlass::payload_offset_offset()));\n+}\n+\n+void MacroAssembler::payload_addr(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->payload_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  payload_offset(inline_klass, offset);\n+  if (data == oop) {\n+    addptr(data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert(index != rcx, \"index needs to shift by rcx\");\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rcx, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  movl(rcx, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  shrl(rcx, Klass::_lh_log2_element_size_shift);\n+  andl(rcx, Klass::_lh_log2_element_size_mask);\n+  shlptr(index); \/\/ index << rcx\n+\n+  lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT)));\n+}\n+\n@@ -6422,1 +6815,1 @@\n-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, KRegister mask) {\n+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, KRegister mask) {\n@@ -6428,1 +6821,1 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_512bit);\n+    evpbroadcastq(xtmp, val, AVX_512bit);\n@@ -6430,1 +6823,3 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_256bit);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n+    vinserti128_high(xtmp, xtmp);\n@@ -6432,1 +6827,2 @@\n-    pxor(xtmp, xtmp);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n@@ -6455,1 +6851,1 @@\n-    fill64_masked(3, base, 0, xtmp, mask, cnt, rtmp, true);\n+    fill64_masked(3, base, 0, xtmp, mask, cnt, val, true);\n@@ -6474,1 +6870,1 @@\n-    fill32_masked(3, base, 0, xtmp, mask, cnt, rtmp);\n+    fill32_masked(3, base, 0, xtmp, mask, cnt, val);\n@@ -6487,0 +6883,400 @@\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  testptr(rax, 1);\n+  jcc(Assembler::zero, skip);\n+  int call_offset = -1;\n+\n+#ifdef _LP64\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != nullptr, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(rscratch1, rax); \/\/ save rax for slow_case since *_allocate may corrupt it when allocation failed\n+  if (vk != nullptr) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(rbx, (intptr_t)vk->get_InlineKlass());\n+    jint lh = vk->layout_helper();\n+    assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB && !Klass::layout_helper_needs_slow_path(lh)) {\n+      tlab_allocate(r15_thread, rax, noreg, lh, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. RAX contains ((the InlineKlass* of the return type) | 0x01)\n+    mov(rbx, rax);\n+    andptr(rbx, -2);\n+    if (UseTLAB) {\n+      movl(r14, Address(rbx, Klass::layout_helper_offset()));\n+      testl(r14, Klass::_lh_instance_slow_path_bit);\n+      jcc(Assembler::notZero, slow_case);\n+      tlab_allocate(r15_thread, rax, r14, 0, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = rax;\n+    movptr(Address(buffer_obj, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::inline_type_prototype().value());\n+    xorl(r13, r13);\n+    store_klass_gap(buffer_obj, r13);\n+    if (vk == nullptr) {\n+      \/\/ store_klass corrupts rbx(klass), so save it in r13 for later use (interpreter case only).\n+      mov(r13, rbx);\n+    }\n+    store_klass(buffer_obj, rbx, rscratch1);\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != nullptr) {\n+      call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      movptr(rbx, Address(r13, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      movptr(rbx, Address(rbx, InlineKlass::pack_handler_offset()));\n+      call(rbx);\n+    }\n+    jmp(skip);\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(rax, rscratch1);\n+#endif\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to->as_Register(), from->as_Register());\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        Address to_addr = Address(rsp, st_off);\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to_addr, from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to_addr, from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to_addr, from->as_Register());\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(rsp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      if (to->is_reg()) {\n+        if (to->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from_addr);\n+          }\n+        } else {\n+          movq(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(r13, from_addr);\n+        movq(Address(rsp, st_off), r13);\n+      }\n+    }\n+  }\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  \/\/ Two additional slots to account for return address\n+  int sp_inc = (args_on_stack + 2) * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  \/\/ Save the return address, adjust the stack (make sure it is properly\n+  \/\/ 16-byte aligned) and copy the return address to the new top of the stack.\n+  \/\/ The stack will be repaired on return (see MacroAssembler::remove_frame).\n+  assert(sp_inc > 0, \"sanity\");\n+  pop(r13);\n+  subptr(rsp, sp_inc);\n+  push(r13);\n+  return sp_inc;\n+}\n+\n+\/\/ Read all fields from an inline type buffer and store the field values in registers\/stack slots.\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(tmp1, Address(rsp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        testptr(fromReg, fromReg);\n+        jcc(Assembler::zero, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), 1);\n+      } else {\n+        movq(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_XMMRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), dst);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(toReg->as_XMMRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(toReg->as_XMMRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      jmp(L_notNull);\n+      bind(L_null);\n+      \/\/ Set IsInit field to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+            movq(Address(rsp, st_off), 0);\n+          } else {\n+            xorq(toReg->as_Register(), toReg->as_Register());\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_METADATA, \"should be at delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r14;\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register tmp3 = rbx;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        testb(Address(rsp, ld_off), 1);\n+      } else {\n+        testb(fromReg->as_Register(), 1);\n+      }\n+      jcc(Assembler::notZero, L_notNull);\n+      movptr(val_obj, 0);\n+      jmp(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    Address dst(val_obj, off);\n+    if (!fromReg->is_XMMRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(src, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(dst, fromReg->as_XMMRegister());\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(dst, fromReg->as_XMMRegister());\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();\n+}\n+\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  if (needs_stack_repair) {\n+    movq(rbp, Address(rsp, initial_framesize));\n+    \/\/ The stack increment resides just below the saved rbp\n+    addq(rsp, Address(rsp, initial_framesize - wordSize));\n+  } else {\n+    if (initial_framesize > 0) {\n+      addq(rsp, initial_framesize);\n+    }\n+    pop(rbp);\n+  }\n+}\n+\n@@ -6576,2 +7372,2 @@\n-void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp,\n-                               bool is_large, KRegister mask) {\n+void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp,\n+                               bool is_large, bool word_copy_only, KRegister mask) {\n@@ -6582,1 +7378,1 @@\n-  assert(tmp==rax,   \"tmp register must be eax for rep stos\");\n+  assert(val==rax,   \"val register must be eax for rep stos\");\n@@ -6588,3 +7384,0 @@\n-  if (!is_large || !UseXMMForObjInit) {\n-    xorptr(tmp, tmp);\n-  }\n@@ -6604,1 +7397,1 @@\n-    movptr(Address(base, cnt, Address::times_ptr), tmp);\n+    movptr(Address(base, cnt, Address::times_ptr), val);\n@@ -6613,1 +7406,1 @@\n-  if (UseFastStosb) {\n+  if (UseFastStosb && !word_copy_only) {\n@@ -6617,1 +7410,1 @@\n-    xmm_clear_mem(base, cnt, tmp, xtmp, mask);\n+    xmm_clear_mem(base, cnt, val, xtmp, mask);\n@@ -10738,0 +11531,4 @@\n+  if (EnableValhalla) {\n+    \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+    andptr(reg_rax, ~((int) markWord::inline_type_bit_in_place));\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":814,"deletions":17,"binary":false,"changes":831,"status":"modified"},{"patch":"@@ -156,1 +156,5 @@\n-  const ByteSize entry_offset = for_compiler_entry ? Method::from_compiled_offset() :\n+  \/\/ The following jump might pass an inline type argument that was erased to Object as oop to a\n+  \/\/ callee that expects inline type arguments to be passed as fields. We need to call the compiled\n+  \/\/ value entry (_code->inline_entry_point() or _adapter->c2i_inline_entry()) which will take care\n+  \/\/ of translating between the calling conventions.\n+  const ByteSize entry_offset = for_compiler_entry ? Method::from_compiled_inline_offset() :\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -539,0 +539,9 @@\n+const uint SharedRuntime::java_return_convention_max_int = 1;\n+const uint SharedRuntime::java_return_convention_max_float = 1;\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt,\n+                                          VMRegPair *regs,\n+                                          int total_args_passed) {\n+  Unimplemented();\n+  return 0;\n+}\n+\n@@ -600,3 +609,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>& sig_extended,\n@@ -604,1 +611,5 @@\n-                            Label& skip_fixup) {\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet*& oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words) {\n@@ -626,1 +637,1 @@\n-  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+  int extraspace = sig_extended.length() * Interpreter::stackElementSize;\n@@ -637,3 +648,3 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+  for (int i = 0; i < sig_extended.length(); i++) {\n+    if (sig_extended.at(i)._bt == T_VOID) {\n+      assert(i > 0 && (sig_extended.at(i-1)._bt == T_LONG || sig_extended.at(i-1)._bt == T_DOUBLE), \"missing half\");\n@@ -644,1 +655,1 @@\n-    int st_off = ((total_args_passed - 1) - i) * Interpreter::stackElementSize;\n+    int st_off = ((sig_extended.length() - 1) - i) * Interpreter::stackElementSize;\n@@ -690,1 +701,1 @@\n-        assert(sig_bt[i] == T_DOUBLE || sig_bt[i] == T_LONG, \"wrong type\");\n+        assert(sig_extended.at(i)._bt == T_DOUBLE || sig_extended.at(i)._bt == T_LONG, \"wrong type\");\n@@ -723,2 +734,1 @@\n-                                    int total_args_passed,\n-                                    const BasicType *sig_bt,\n+                                    const GrowableArray<SigEntry>& sig_extended,\n@@ -727,0 +737,1 @@\n+\n@@ -820,2 +831,2 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n+  for (int i = 0; i < sig_extended.length(); i++) {\n+    if (sig_extended.at(i)._bt == T_VOID) {\n@@ -824,1 +835,1 @@\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+      assert(i > 0 && (sig_extended.at(i-1)._bt == T_LONG || sig_extended.at(i-1)._bt == T_DOUBLE), \"missing half\");\n@@ -833,1 +844,1 @@\n-    int ld_off = (total_args_passed - i) * Interpreter::stackElementSize;\n+    int ld_off = (sig_extended.length() - i) * Interpreter::stackElementSize;\n@@ -931,2 +942,1 @@\n-                                                            int total_args_passed,\n-                                                            const BasicType *sig_bt,\n+                                                            const GrowableArray<SigEntry>& sig_extended,\n@@ -935,1 +945,2 @@\n-                                                            AdapterFingerPrint* fingerprint) {\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter) {\n@@ -938,1 +949,1 @@\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig_extended, regs);\n@@ -971,1 +982,4 @@\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n+  OopMapSet* oop_maps = nullptr;\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n+  gen_c2i_adapter(masm, sig_extended, regs, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words);\n@@ -973,0 +987,1 @@\n+  new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words, oop_maps);\n@@ -2647,0 +2662,5 @@\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  Unimplemented();\n+  return nullptr;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":41,"deletions":21,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -637,0 +638,81 @@\n+\/\/ Same as java_calling_convention() but for multiple return\n+\/\/ values. There's no way to store them on the stack so if we don't\n+\/\/ have enough registers, multiple values can't be returned.\n+const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j+1;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt,\n+                                          VMRegPair *regs,\n+                                          int total_args_passed) {\n+  \/\/ Create the mapping between argument positions and\n+  \/\/ registers.\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+    rax, j_rarg5, j_rarg4, j_rarg3, j_rarg2, j_rarg1, j_rarg0\n+  };\n+  static const XMMRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3,\n+    j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < Argument::n_int_register_parameters_j+1) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert(sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+    case T_METADATA:\n+      if (int_args < Argument::n_int_register_parameters_j+1) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert(sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < Argument::n_float_register_parameters_j) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -679,0 +761,105 @@\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+    for (int i = 0; i < sig_extended->length(); i++) {\n+      BasicType bt = sig_extended->at(i)._bt;\n+      if (bt == T_METADATA) {\n+        \/\/ In sig_extended, an inline type argument starts with:\n+        \/\/ T_METADATA, followed by the types of the fields of the\n+        \/\/ inline type and T_VOID to mark the end of the value\n+        \/\/ type. Inline types are flattened so, for instance, in the\n+        \/\/ case of an inline type with an int field and an inline type\n+        \/\/ field that itself has 2 fields, an int and a long:\n+        \/\/ T_METADATA T_INT T_METADATA T_INT T_LONG T_VOID (second\n+        \/\/ slot for the T_LONG) T_VOID (inner inline type) T_VOID\n+        \/\/ (outer inline type)\n+        total_args_passed++;\n+        int vt = 1;\n+        do {\n+          i++;\n+          BasicType bt = sig_extended->at(i)._bt;\n+          BasicType prev_bt = sig_extended->at(i-1)._bt;\n+          if (bt == T_METADATA) {\n+            vt++;\n+          } else if (bt == T_VOID &&\n+                     prev_bt != T_LONG &&\n+                     prev_bt != T_DOUBLE) {\n+            vt--;\n+          }\n+        } while (vt != 0);\n+      } else {\n+        total_args_passed++;\n+      }\n+    }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n+  return total_args_passed;\n+}\n+\n+\n+static void gen_c2i_adapter_helper(MacroAssembler* masm,\n+                                   BasicType bt,\n+                                   BasicType prev_bt,\n+                                   size_t size_in_bytes,\n+                                   const VMRegPair& reg_pair,\n+                                   const Address& to,\n+                                   int extraspace,\n+                                   bool is_oop) {\n+  if (bt == T_VOID) {\n+    assert(prev_bt == T_LONG || prev_bt == T_DOUBLE, \"missing half\");\n+    return;\n+  }\n+\n+  \/\/ Say 4 args:\n+  \/\/ i   st_off\n+  \/\/ 0   32 T_LONG\n+  \/\/ 1   24 T_VOID\n+  \/\/ 2   16 T_OBJECT\n+  \/\/ 3    8 T_BOOL\n+  \/\/ -    0 return address\n+  \/\/\n+  \/\/ However to make thing extra confusing. Because we can fit a long\/double in\n+  \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n+  \/\/ leaves one slot empty and only stores to a single slot. In this case the\n+  \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+\n+  bool wide = (size_in_bytes == wordSize);\n+  VMReg r_1 = reg_pair.first();\n+  VMReg r_2 = reg_pair.second();\n+  assert(r_2->is_valid() == wide, \"invalid size\");\n+  if (!r_1->is_valid()) {\n+    assert(!r_2->is_valid(), \"must be invalid\");\n+    return;\n+  }\n+\n+  if (!r_1->is_XMMRegister()) {\n+    Register val = rax;\n+    if (r_1->is_stack()) {\n+      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+      __ load_sized_value(val, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+    } else {\n+      val = r_1->as_Register();\n+    }\n+    assert_different_registers(to.base(), val, rscratch1);\n+    if (is_oop) {\n+      __ push(r13);\n+      __ push(rbx);\n+      __ store_heap_oop(to, val, rscratch1, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      __ pop(rbx);\n+      __ pop(r13);\n+    } else {\n+      __ store_sized_value(to, val, size_in_bytes);\n+    }\n+  } else {\n+    if (wide) {\n+      __ movdbl(to, r_1->as_XMMRegister());\n+    } else {\n+      __ movflt(to, r_1->as_XMMRegister());\n+    }\n+  }\n+}\n@@ -681,3 +868,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>* sig_extended,\n@@ -685,1 +870,32 @@\n-                            Label& skip_fixup) {\n+                            bool requires_clinit_barrier,\n+                            address& c2i_no_clinit_check_entry,\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n+  if (requires_clinit_barrier && VM_Version::supports_fast_class_init_checks()) {\n+    Label L_skip_barrier;\n+    Register method = rbx;\n+\n+    { \/\/ Bypass the barrier for non-static methods\n+      Register flags = rscratch1;\n+      __ load_unsigned_short(flags, Address(method, Method::access_flags_offset()));\n+      __ testl(flags, JVM_ACC_STATIC);\n+      __ jcc(Assembler::zero, L_skip_barrier); \/\/ non-static\n+    }\n+\n+    Register klass = rscratch1;\n+    __ load_method_holder(klass, method);\n+    __ clinit_barrier(klass, r15_thread, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+\n+    __ bind(L_skip_barrier);\n+    c2i_no_clinit_check_entry = __ pc();\n+  }\n+\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->c2i_entry_barrier(masm);\n+\n@@ -695,0 +911,42 @@\n+  if (InlineTypePassFieldsAsArgs) {\n+    \/\/ Is there an inline type argument?\n+    bool has_inline_argument = false;\n+    for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+      has_inline_argument = (sig_extended->at(i)._bt == T_METADATA);\n+    }\n+    if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types.\n+      \/\/ Allocate the buffers here with a runtime call.\n+      OopMap* map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_vectors*\/ false);\n+\n+      frame_complete = __ offset();\n+\n+      __ set_last_Java_frame(noreg, noreg, nullptr, rscratch1);\n+\n+      __ mov(c_rarg0, r15_thread);\n+      __ mov(c_rarg1, rbx);\n+      __ mov64(c_rarg2, (int64_t)alloc_inline_receiver);\n+      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+\n+      oop_maps->add_gc_map((int)(__ pc() - start), map);\n+      __ reset_last_Java_frame(false);\n+\n+      RegisterSaver::restore_live_registers(masm);\n+\n+      Label no_exception;\n+      __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n+      __ jcc(Assembler::equal, no_exception);\n+\n+      __ movptr(Address(r15_thread, JavaThread::vm_result_offset()), NULL_WORD);\n+      __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+      __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result(rscratch2, r15_thread); \/\/ Use rscratch2 (r11) as temporary because rscratch1 (r10) is trashed by movptr()\n+      __ get_vm_result_2(rbx, r15_thread); \/\/ TODO: required to keep the callee Method live?\n+    }\n+  }\n+\n@@ -697,1 +955,1 @@\n-\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n@@ -732,46 +990,24 @@\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n-\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n-\n-    \/\/ Say 4 args:\n-    \/\/ i   st_off\n-    \/\/ 0   32 T_LONG\n-    \/\/ 1   24 T_VOID\n-    \/\/ 2   16 T_OBJECT\n-    \/\/ 3    8 T_BOOL\n-    \/\/ -    0 return address\n-    \/\/\n-    \/\/ However to make thing extra confusing. Because we can fit a long\/double in\n-    \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n-    \/\/ leaves one slot empty and only stores to a single slot. In this case the\n-    \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n-\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n-    if (!r_1->is_valid()) {\n-      assert(!r_2->is_valid(), \"\");\n-      continue;\n-    }\n-    if (r_1->is_stack()) {\n-      \/\/ memory to memory use rax\n-      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n-      if (!r_2->is_valid()) {\n-        \/\/ sign extend??\n-        __ movl(rax, Address(rsp, ld_off));\n-        __ movptr(Address(rsp, st_off), rax);\n-      } else {\n-\n-        __ movq(rax, Address(rsp, ld_off));\n-\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ movq(Address(rsp, next_off), rax);\n+  \/\/ next_arg_comp is the next argument from the compiler point of\n+  \/\/ view (inline type fields are passed in registers\/on the stack). In\n+  \/\/ sig_extended, an inline type argument starts with: T_METADATA,\n+  \/\/ followed by the types of the fields of the inline type and T_VOID\n+  \/\/ to mark the end of the inline type. ignored counts the number of\n+  \/\/ T_METADATA\/T_VOID. next_vt_arg is the next inline type argument:\n+  \/\/ used to get the buffer for that argument from the pool of buffers\n+  \/\/ we allocated above and want to pass to the\n+  \/\/ interpreter. next_arg_int is the next argument from the\n+  \/\/ interpreter point of view (inline types are passed by reference).\n+  for (int next_arg_comp = 0, ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+       next_arg_comp < sig_extended->length(); next_arg_comp++) {\n+    assert(ignored <= next_arg_comp, \"shouldn't skip over more slots than there are arguments\");\n+    assert(next_arg_int <= total_args_passed, \"more arguments for the interpreter than expected?\");\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    int st_off = (total_args_passed - next_arg_int) * Interpreter::stackElementSize;\n+    if (!InlineTypePassFieldsAsArgs || bt != T_METADATA) {\n+      int next_off = st_off - Interpreter::stackElementSize;\n+      const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+      const VMRegPair reg_pair = regs[next_arg_comp-ignored];\n+      size_t size_in_bytes = reg_pair.second()->is_valid() ? 8 : 4;\n+      gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                             size_in_bytes, reg_pair, Address(rsp, offset), extraspace, false);\n+      next_arg_int++;\n@@ -780,7 +1016,4 @@\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov64(rax, CONST64(0xdeadffffdeadaaaa));\n-          __ movptr(Address(rsp, st_off), rax);\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ movq(Address(rsp, st_off), rax);\n-        }\n+      if (bt == T_LONG || bt == T_DOUBLE) {\n+        \/\/ Overwrite the unused slot with known junk\n+        __ mov64(rax, CONST64(0xdeadffffdeadaaaa));\n+        __ movptr(Address(rsp, st_off), rax);\n@@ -788,16 +1021,26 @@\n-    } else if (r_1->is_Register()) {\n-      Register r = r_1->as_Register();\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ movl(Address(rsp, st_off), r);\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ long\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov64(rax, CONST64(0xdeadffffdeadaaab));\n-          __ movptr(Address(rsp, st_off), rax);\n-          __ movq(Address(rsp, next_off), r);\n+    } else {\n+      ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_OBJECT);\n+      __ load_heap_oop(r14, Address(rscratch2, index));\n+      next_vt_arg++; next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of inline\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      Label L_null;\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp-1)._bt;\n+        if (bt == T_METADATA) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID &&\n+                   prev_bt != T_LONG &&\n+                   prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n@@ -806,1 +1049,22 @@\n-          __ movptr(Address(rsp, st_off), r);\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          if (off == -1) {\n+            \/\/ Nullable inline type argument, emit null check\n+            VMReg reg = regs[next_arg_comp-ignored].first();\n+            Label L_notNull;\n+            if (reg->is_stack()) {\n+              int ld_off = reg->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+              __ testb(Address(rsp, ld_off), 1);\n+            } else {\n+              __ testb(reg->as_Register(), 1);\n+            }\n+            __ jcc(Assembler::notZero, L_notNull);\n+            __ movptr(Address(rsp, st_off), 0);\n+            __ jmp(L_null);\n+            __ bind(L_notNull);\n+            continue;\n+          }\n+          assert(off > 0, \"offset in object should be positive\");\n+          size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+          bool is_oop = is_reference_type(bt);\n+          gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                                 size_in_bytes, regs[next_arg_comp-ignored], Address(r14, off), extraspace, is_oop);\n@@ -808,14 +1072,4 @@\n-      }\n-    } else {\n-      assert(r_1->is_XMMRegister(), \"\");\n-      if (!r_2->is_valid()) {\n-        \/\/ only a float use just part of the slot\n-        __ movflt(Address(rsp, st_off), r_1->as_XMMRegister());\n-      } else {\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov64(rax, CONST64(0xdeadffffdeadaaac));\n-        __ movptr(Address(rsp, st_off), rax);\n-#endif \/* ASSERT *\/\n-        __ movdbl(Address(rsp, next_off), r_1->as_XMMRegister());\n-      }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ movptr(Address(rsp, st_off), r14);\n+      __ bind(L_null);\n@@ -844,2 +1098,1 @@\n-                                    int total_args_passed,\n-                                    const BasicType *sig_bt,\n+                                    const GrowableArray<SigEntry>* sig,\n@@ -937,1 +1190,1 @@\n-  __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_offset())));\n+  __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_inline_offset())));\n@@ -951,0 +1204,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -954,1 +1209,2 @@\n-    if (sig_bt[i] == T_VOID) {\n+    BasicType bt = sig->at(i)._bt;\n+    if (bt == T_VOID) {\n@@ -957,1 +1213,2 @@\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+      BasicType prev_bt = (i > 0) ? sig->at(i-1)._bt : T_ILLEGAL;\n+      assert(i > 0 && (prev_bt == T_LONG || prev_bt == T_DOUBLE), \"missing half\");\n@@ -999,1 +1256,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (bt==T_LONG||bt==T_DOUBLE)?\n@@ -1014,1 +1271,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n+        const int offset = (bt==T_LONG||bt==T_DOUBLE)?\n@@ -1047,1 +1304,1 @@\n-  \/\/ only needed because eof c2 resolve stubs return Method* as a result in\n+  \/\/ only needed because of c2 resolve stubs return Method* as a result in\n@@ -1053,0 +1310,13 @@\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n+  Register data = rax;\n+  __ ic_check(1 \/* end_alignment *\/);\n+  __ movptr(rbx, Address(data, CompiledICData::speculated_method_offset()));\n+\n+  \/\/ Method might have been compiled since the call site was patched to\n+  \/\/ interpreted if that is the case treat it as a miss so we can get\n+  \/\/ the call site corrected.\n+  __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), NULL_WORD);\n+  __ jcc(Assembler::equal, skip_fixup);\n+  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+}\n+\n@@ -1054,2 +1324,1 @@\n-AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n-                                                            int total_args_passed,\n+AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler* masm,\n@@ -1057,3 +1326,9 @@\n-                                                            const BasicType *sig_bt,\n-                                                            const VMRegPair *regs,\n-                                                            AdapterFingerPrint* fingerprint) {\n+                                                            const GrowableArray<SigEntry>* sig,\n+                                                            const VMRegPair* regs,\n+                                                            const GrowableArray<SigEntry>* sig_cc,\n+                                                            const VMRegPair* regs_cc,\n+                                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                                            const VMRegPair* regs_cc_ro,\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter,\n+                                                            bool allocate_code_blob) {\n@@ -1061,2 +1336,1 @@\n-\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n@@ -1073,1 +1347,2 @@\n-  address c2i_unverified_entry = __ pc();\n+  address c2i_unverified_entry        = __ pc();\n+  address c2i_unverified_inline_entry = __ pc();\n@@ -1076,14 +1351,1 @@\n-  Register data = rax;\n-  Register receiver = j_rarg0;\n-  Register temp = rbx;\n-\n-  {\n-    __ ic_check(1 \/* end_alignment *\/);\n-    __ movptr(rbx, Address(data, CompiledICData::speculated_method_offset()));\n-    \/\/ Method might have been compiled since the call site was patched to\n-    \/\/ interpreted if that is the case treat it as a miss so we can get\n-    \/\/ the call site corrected.\n-    __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), NULL_WORD);\n-    __ jcc(Assembler::equal, skip_fixup);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-  }\n+  gen_inline_cache_check(masm, skip_fixup);\n@@ -1091,1 +1353,3 @@\n-  address c2i_entry = __ pc();\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n@@ -1093,1 +1357,1 @@\n-  \/\/ Class initialization barrier for static methods\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n@@ -1095,19 +1359,6 @@\n-  if (VM_Version::supports_fast_class_init_checks()) {\n-    Label L_skip_barrier;\n-    Register method = rbx;\n-\n-    { \/\/ Bypass the barrier for non-static methods\n-      Register flags = rscratch1;\n-      __ load_unsigned_short(flags, Address(method, Method::access_flags_offset()));\n-      __ testl(flags, JVM_ACC_STATIC);\n-      __ jcc(Assembler::zero, L_skip_barrier); \/\/ non-static\n-    }\n-\n-    Register klass = rscratch1;\n-    __ load_method_holder(klass, method);\n-    __ clinit_barrier(klass, r15_thread, &L_skip_barrier \/*L_fast_path*\/);\n-\n-    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n-\n-    __ bind(L_skip_barrier);\n-    c2i_no_clinit_check_entry = __ pc();\n+  address c2i_inline_ro_entry = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    \/\/ No class init barrier needed because method is guaranteed to be non-static\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, \/* requires_clinit_barrier = *\/ false, c2i_no_clinit_check_entry,\n+                    skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n+    skip_fixup.reset();\n@@ -1116,2 +1367,16 @@\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->c2i_entry_barrier(masm);\n+  \/\/ Scalarized c2i adapter\n+  address c2i_entry        = __ pc();\n+  address c2i_inline_entry = __ pc();\n+  gen_c2i_adapter(masm, sig_cc, regs_cc, \/* requires_clinit_barrier = *\/ true, c2i_no_clinit_check_entry,\n+                  skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ true);\n+\n+  \/\/ Non-scalarized c2i adapter\n+  if (regs != regs_cc) {\n+    c2i_unverified_inline_entry = __ pc();\n+    Label inline_entry_skip_fixup;\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n+\n+    c2i_inline_entry = __ pc();\n+    gen_c2i_adapter(masm, sig, regs, \/* requires_clinit_barrier = *\/ true, c2i_no_clinit_check_entry,\n+                    inline_entry_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n+  }\n@@ -1119,1 +1384,6 @@\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n+  \/\/ The c2i adapters might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+  if (allocate_code_blob) {\n+    bool caller_must_gc_arguments = (regs != regs_cc);\n+    new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n+  }\n@@ -1121,1 +1391,1 @@\n-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);\n+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -2253,0 +2523,4 @@\n+      if (EnableValhalla) {\n+        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+        __ andptr(swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+      }\n@@ -3635,0 +3909,114 @@\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler* masm = new MacroAssembler(&buffer);\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_jobject_off = __ offset();\n+  \/\/ Resolve pre-allocated buffer from JNI handle.\n+  \/\/ We cannot do this in generate_call_stub() because it requires GC code to be initialized.\n+  __ movptr(rax, Address(r13, 0));\n+  __ resolve_jobject(rax \/* value *\/,\n+                     r15_thread \/* thread *\/,\n+                     r12 \/* tmp *\/);\n+  __ movptr(Address(r13, 0), rax);\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(rax, off);\n+    if (bt == T_FLOAT) {\n+      __ movflt(to, r_1->as_XMMRegister());\n+    } else if (bt == T_DOUBLE) {\n+      __ movdbl(to, r_1->as_XMMRegister());\n+    } else {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(to.base(), val, r14, r13, rbx, rscratch1);\n+      if (is_reference_type(bt)) {\n+        __ store_heap_oop(to, val, r14, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        __ store_sized_value(to, r_1->as_Register(), type2aelembytes(bt));\n+      }\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+  if (vk->has_nullable_atomic_layout()) {\n+    \/\/ Set the null marker\n+    __ movb(Address(rax, vk->null_marker_offset()), 1);\n+  }\n+  __ ret(0);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  Label skip;\n+  __ testptr(rax, rax);\n+  __ jcc(Assembler::zero, skip);\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(rax, off);\n+    if (bt == T_FLOAT) {\n+      __ movflt(r_1->as_XMMRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ movdbl(r_1->as_XMMRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      assert_different_registers(rax, r_1->as_Register());\n+      __ load_heap_oop(r_1->as_Register(), from);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(rax, r_1->as_Register());\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ bind(skip);\n+  __ ret(0);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+}\n+\n@@ -3727,1 +4115,0 @@\n-\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":532,"deletions":145,"binary":false,"changes":677,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"asm\/assembler.hpp\"\n@@ -41,0 +42,2 @@\n+#include \"utilities\/macros.hpp\"\n+#include \"vmreg_x86.inline.hpp\"\n@@ -306,4 +309,6 @@\n-  __ movptr(c_rarg0, result);\n-  Label is_long, is_float, is_double, exit;\n-  __ movl(c_rarg1, result_type);\n-  __ cmpl(c_rarg1, T_OBJECT);\n+  __ movptr(r13, result);\n+  Label is_long, is_float, is_double, check_prim, exit;\n+  __ movl(rbx, result_type);\n+  __ cmpl(rbx, T_OBJECT);\n+  __ jcc(Assembler::equal, check_prim);\n+  __ cmpl(rbx, T_LONG);\n@@ -311,3 +316,1 @@\n-  __ cmpl(c_rarg1, T_LONG);\n-  __ jcc(Assembler::equal, is_long);\n-  __ cmpl(c_rarg1, T_FLOAT);\n+  __ cmpl(rbx, T_FLOAT);\n@@ -315,1 +318,1 @@\n-  __ cmpl(c_rarg1, T_DOUBLE);\n+  __ cmpl(rbx, T_DOUBLE);\n@@ -321,1 +324,1 @@\n-    __ cmpl(c_rarg1, T_INT);\n+    __ cmpl(rbx, T_INT);\n@@ -329,1 +332,1 @@\n-  __ movl(Address(c_rarg0, 0), rax);\n+  __ movl(Address(r13, 0), rax);\n@@ -387,0 +390,13 @@\n+  __ BIND(check_prim);\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ Check for scalarized return value\n+    __ testptr(rax, 1);\n+    __ jcc(Assembler::zero, is_long);\n+    \/\/ Load pack handler address\n+    __ andptr(rax, -2);\n+    __ movptr(rax, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    __ movptr(rbx, Address(rax, InlineKlass::pack_handler_jobject_offset()));\n+    \/\/ Call pack handler to initialize the buffer\n+    __ call(rbx);\n+    __ jmp(exit);\n+  }\n@@ -388,1 +404,1 @@\n-  __ movq(Address(c_rarg0, 0), rax);\n+  __ movq(Address(r13, 0), rax);\n@@ -392,1 +408,1 @@\n-  __ movflt(Address(c_rarg0, 0), xmm0);\n+  __ movflt(Address(r13, 0), xmm0);\n@@ -396,1 +412,1 @@\n-  __ movdbl(Address(c_rarg0, 0), xmm0);\n+  __ movdbl(Address(r13, 0), xmm0);\n@@ -3929,0 +3945,10 @@\n+  \/\/ Generate these first because they are called from other stubs\n+  if (InlineTypeReturnedAsFields) {\n+    StubRoutines::_load_inline_type_fields_in_regs =\n+      generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs),\n+                                 \"load_inline_type_fields_in_regs\", false);\n+    StubRoutines::_store_inline_type_fields_to_buf =\n+      generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf),\n+                                 \"store_inline_type_fields_to_buf\", true);\n+  }\n+\n@@ -3981,0 +4007,144 @@\n+\/\/ Call here from the interpreter or compiled code to either load\n+\/\/ multiple returned values from the inline type instance being\n+\/\/ returned to registers or to store returned values to a newly\n+\/\/ allocated inline type instance.\n+\/\/ Register is a class, but it would be assigned numerical value.\n+\/\/ \"0\" is assigned for xmm0. Thus we need to ignore -Wnonnull.\n+PRAGMA_DIAG_PUSH\n+PRAGMA_NONNULL_IGNORED\n+address StubGenerator::generate_return_value_stub(address destination, const char* name, bool has_res) {\n+  \/\/ We need to save all registers the calling convention may use so\n+  \/\/ the runtime calls read or update those registers. This needs to\n+  \/\/ be in sync with SharedRuntime::java_return_convention().\n+  enum layout {\n+    pad_off = frame::arg_reg_save_area_bytes\/BytesPerInt, pad_off_2,\n+    rax_off, rax_off_2,\n+    j_rarg5_off, j_rarg5_2,\n+    j_rarg4_off, j_rarg4_2,\n+    j_rarg3_off, j_rarg3_2,\n+    j_rarg2_off, j_rarg2_2,\n+    j_rarg1_off, j_rarg1_2,\n+    j_rarg0_off, j_rarg0_2,\n+    j_farg0_off, j_farg0_2,\n+    j_farg1_off, j_farg1_2,\n+    j_farg2_off, j_farg2_2,\n+    j_farg3_off, j_farg3_2,\n+    j_farg4_off, j_farg4_2,\n+    j_farg5_off, j_farg5_2,\n+    j_farg6_off, j_farg6_2,\n+    j_farg7_off, j_farg7_2,\n+    rbp_off, rbp_off_2,\n+    return_off, return_off_2,\n+\n+    framesize\n+  };\n+\n+  CodeBuffer buffer(name, 1000, 512);\n+  MacroAssembler* _masm = new MacroAssembler(&buffer);\n+\n+  int frame_size_in_bytes = align_up(framesize*BytesPerInt, 16);\n+  assert(frame_size_in_bytes == framesize*BytesPerInt, \"misaligned\");\n+  int frame_size_in_slots = frame_size_in_bytes \/ BytesPerInt;\n+  int frame_size_in_words = frame_size_in_bytes \/ wordSize;\n+\n+  OopMapSet *oop_maps = new OopMapSet();\n+  OopMap* map = new OopMap(frame_size_in_slots, 0);\n+\n+  map->set_callee_saved(VMRegImpl::stack2reg(rax_off), rax->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n+\n+  int start = __ offset();\n+\n+  __ subptr(rsp, frame_size_in_bytes - 8 \/* return address*\/);\n+\n+  __ movptr(Address(rsp, rbp_off * BytesPerInt), rbp);\n+  __ movdbl(Address(rsp, j_farg7_off * BytesPerInt), j_farg7);\n+  __ movdbl(Address(rsp, j_farg6_off * BytesPerInt), j_farg6);\n+  __ movdbl(Address(rsp, j_farg5_off * BytesPerInt), j_farg5);\n+  __ movdbl(Address(rsp, j_farg4_off * BytesPerInt), j_farg4);\n+  __ movdbl(Address(rsp, j_farg3_off * BytesPerInt), j_farg3);\n+  __ movdbl(Address(rsp, j_farg2_off * BytesPerInt), j_farg2);\n+  __ movdbl(Address(rsp, j_farg1_off * BytesPerInt), j_farg1);\n+  __ movdbl(Address(rsp, j_farg0_off * BytesPerInt), j_farg0);\n+\n+  __ movptr(Address(rsp, j_rarg0_off * BytesPerInt), j_rarg0);\n+  __ movptr(Address(rsp, j_rarg1_off * BytesPerInt), j_rarg1);\n+  __ movptr(Address(rsp, j_rarg2_off * BytesPerInt), j_rarg2);\n+  __ movptr(Address(rsp, j_rarg3_off * BytesPerInt), j_rarg3);\n+  __ movptr(Address(rsp, j_rarg4_off * BytesPerInt), j_rarg4);\n+  __ movptr(Address(rsp, j_rarg5_off * BytesPerInt), j_rarg5);\n+  __ movptr(Address(rsp, rax_off * BytesPerInt), rax);\n+\n+  int frame_complete = __ offset();\n+\n+  __ set_last_Java_frame(noreg, noreg, nullptr, rscratch1);\n+\n+  __ mov(c_rarg0, r15_thread);\n+  __ mov(c_rarg1, rax);\n+\n+  __ call(RuntimeAddress(destination));\n+\n+  \/\/ Set an oopmap for the call site.\n+\n+  oop_maps->add_gc_map( __ offset() - start, map);\n+\n+  \/\/ clear last_Java_sp\n+  __ reset_last_Java_frame(false);\n+\n+  __ movptr(rbp, Address(rsp, rbp_off * BytesPerInt));\n+  __ movdbl(j_farg7, Address(rsp, j_farg7_off * BytesPerInt));\n+  __ movdbl(j_farg6, Address(rsp, j_farg6_off * BytesPerInt));\n+  __ movdbl(j_farg5, Address(rsp, j_farg5_off * BytesPerInt));\n+  __ movdbl(j_farg4, Address(rsp, j_farg4_off * BytesPerInt));\n+  __ movdbl(j_farg3, Address(rsp, j_farg3_off * BytesPerInt));\n+  __ movdbl(j_farg2, Address(rsp, j_farg2_off * BytesPerInt));\n+  __ movdbl(j_farg1, Address(rsp, j_farg1_off * BytesPerInt));\n+  __ movdbl(j_farg0, Address(rsp, j_farg0_off * BytesPerInt));\n+\n+  __ movptr(j_rarg0, Address(rsp, j_rarg0_off * BytesPerInt));\n+  __ movptr(j_rarg1, Address(rsp, j_rarg1_off * BytesPerInt));\n+  __ movptr(j_rarg2, Address(rsp, j_rarg2_off * BytesPerInt));\n+  __ movptr(j_rarg3, Address(rsp, j_rarg3_off * BytesPerInt));\n+  __ movptr(j_rarg4, Address(rsp, j_rarg4_off * BytesPerInt));\n+  __ movptr(j_rarg5, Address(rsp, j_rarg5_off * BytesPerInt));\n+  __ movptr(rax, Address(rsp, rax_off * BytesPerInt));\n+\n+  __ addptr(rsp, frame_size_in_bytes-8);\n+\n+  \/\/ check for pending exceptions\n+  Label pending;\n+  __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+  __ jcc(Assembler::notEqual, pending);\n+\n+  if (has_res) {\n+    __ get_vm_result(rax, r15_thread);\n+  }\n+\n+  __ ret(0);\n+\n+  __ bind(pending);\n+\n+  __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+  __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+  \/\/ -------------\n+  \/\/ make sure all code is generated\n+  _masm->flush();\n+\n+  RuntimeStub* stub = RuntimeStub::new_runtime_stub(name, &buffer, frame_complete, frame_size_in_words, oop_maps, false);\n+  return stub->entry_point();\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":183,"deletions":13,"binary":false,"changes":196,"status":"modified"},{"patch":"@@ -2860,0 +2860,6 @@\n+  \/\/ Check for flat inline type array -> return -1\n+  __ test_flat_array_oop(src, rax, L_failed);\n+\n+  \/\/ Check for null-free (non-flat) inline type array -> handle as object array\n+  __ test_null_free_array_oop(src, rax, L_objArray);\n+\n@@ -2863,0 +2869,4 @@\n+  \/\/ Check for flat inline type array -> return -1\n+  __ testl(rax_lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  __ jcc(Assembler::notZero, L_failed);\n+\n@@ -2872,2 +2882,4 @@\n-    __ cmpl(rax_lh, (Klass::_lh_array_tag_type_value << Klass::_lh_array_tag_shift));\n-    __ jcc(Assembler::greaterEqual, L);\n+    __ movl(rklass_tmp, rax_lh);\n+    __ sarl(rklass_tmp, Klass::_lh_array_tag_shift);\n+    __ cmpl(rklass_tmp, Klass::_lh_array_tag_type_value);\n+    __ jcc(Assembler::equal, L);\n@@ -2981,0 +2993,1 @@\n+    \/\/ This check also fails for flat arrays which are not supported.\n@@ -2984,0 +2997,11 @@\n+#ifdef ASSERT\n+    {\n+      BLOCK_COMMENT(\"assert not null-free array {\");\n+      Label L;\n+      __ test_non_null_free_array_oop(dst, rklass_tmp, L);\n+      __ stop(\"unexpected null-free array\");\n+      __ bind(L);\n+      BLOCK_COMMENT(\"} assert not null-free array\");\n+    }\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_arraycopy.cpp","additions":26,"deletions":2,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -67,1 +68,1 @@\n-int TemplateInterpreter::InterpreterCodeSize = JVMCI_ONLY(268) NOT_JVMCI(256) * 1024;\n+int TemplateInterpreter::InterpreterCodeSize = JVMCI_ONLY(280) NOT_JVMCI(268) * 1024;\n@@ -212,2 +213,2 @@\n-  __ movptr(rcx, Address(rbp, frame::interpreter_frame_last_sp_offset * wordSize));\n-  __ lea(rsp, Address(rbp, rcx, Address::times_ptr));\n+  __ movptr(rscratch1, Address(rbp, frame::interpreter_frame_last_sp_offset * wordSize));\n+  __ lea(rsp, Address(rbp, rscratch1, Address::times_ptr));\n@@ -217,0 +218,4 @@\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    __ store_inline_type_fields_to_buf(NULL);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86.cpp","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -185,0 +186,1 @@\n+  case Bytecodes::_fast_vputfield:\n@@ -832,9 +834,28 @@\n-  \/\/ rax: index\n-  \/\/ rdx: array\n-  index_check(rdx, rax); \/\/ kills rbx\n-  do_oop_load(_masm,\n-              Address(rdx, rax,\n-                      UseCompressedOops ? Address::times_4 : Address::times_ptr,\n-                      arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n-              rax,\n-              IS_ARRAY);\n+  Register array = rdx;\n+  Register index = rax;\n+\n+  index_check(array, index); \/\/ kills rbx\n+  __ profile_array_type<ArrayLoadData>(rbx, array, rcx);\n+  if (UseArrayFlattening) {\n+    Label is_flat_array, done;\n+    __ test_flat_array_oop(array, rbx, is_flat_array);\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+    __ jmp(done);\n+    __ bind(is_flat_array);\n+    __ movptr(rcx, array);\n+    call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::flat_array_load), rcx, index);\n+    __ bind(done);\n+  } else {\n+    do_oop_load(_masm,\n+                Address(array, index,\n+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,\n+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),\n+                rax,\n+                IS_ARRAY);\n+  }\n+  __ profile_element_type(rbx, rax, rcx);\n@@ -1126,1 +1147,1 @@\n-  Label is_null, ok_is_subtype, done;\n+  Label is_null, is_flat_array, ok_is_subtype, done;\n@@ -1138,0 +1159,4 @@\n+\n+  __ profile_array_type<ArrayStoreData>(rdi, rdx, rbx);\n+  __ profile_multiple_element_types(rdi, rax, rbx, rcx);\n+\n@@ -1141,0 +1166,7 @@\n+  \/\/ Move array class to rdi\n+  __ load_klass(rdi, rdx, rscratch1);\n+  if (UseArrayFlattening) {\n+    __ movl(rbx, Address(rdi, Klass::layout_helper_offset()));\n+    __ test_flat_array_layout(rbx, is_flat_array);\n+  }\n+\n@@ -1143,3 +1175,2 @@\n-  \/\/ Move superklass into rax\n-  __ load_klass(rax, rdx, rscratch1);\n-  __ movptr(rax, Address(rax,\n+  \/\/ Move array element superklass into rax\n+  __ movptr(rax, Address(rdi,\n@@ -1150,1 +1181,2 @@\n-  __ gen_subtype_check(rbx, ok_is_subtype);\n+  \/\/ is \"rbx <: rax\" ? (value subclass <: array element superclass)\n+  __ gen_subtype_check(rbx, ok_is_subtype, false);\n@@ -1168,1 +1200,13 @@\n-  __ profile_null_seen(rbx);\n+  if (EnableValhalla) {\n+    Label is_null_into_value_array_npe, store_null;\n+\n+      \/\/ Move array class to rdi\n+    __ load_klass(rdi, rdx, rscratch1);\n+    if (UseArrayFlattening) {\n+      __ movl(rbx, Address(rdi, Klass::layout_helper_offset()));\n+      __ test_flat_array_layout(rbx, is_flat_array);\n+    }\n+\n+    \/\/ No way to store null in null-free array\n+    __ test_null_free_array_oop(rdx, rbx, is_null_into_value_array_npe);\n+    __ jmp(store_null);\n@@ -1170,0 +1214,5 @@\n+    __ bind(is_null_into_value_array_npe);\n+    __ jump(RuntimeAddress(Interpreter::_throw_NullPointerException_entry));\n+\n+    __ bind(store_null);\n+  }\n@@ -1172,0 +1221,5 @@\n+  __ jmp(done);\n+\n+  if (UseArrayFlattening) {\n+    Label is_type_ok;\n+    __ bind(is_flat_array); \/\/ Store non-null value to flat\n@@ -1173,0 +1227,6 @@\n+    __ movptr(rax, at_tos());\n+    __ movl(rcx, at_tos_p1()); \/\/ index\n+    __ movptr(rdx, at_tos_p2()); \/\/ array\n+\n+    call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::flat_array_store), rax, rdx, rcx);\n+  }\n@@ -2341,1 +2401,1 @@\n-  Label not_taken;\n+  Label taken, not_taken;\n@@ -2343,0 +2403,36 @@\n+\n+  __ profile_acmp(rbx, rdx, rax, rcx);\n+\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  if (EnableValhalla) {\n+    __ cmpoop(rdx, rax);\n+    __ jcc(Assembler::equal, (cc == equal) ? taken : not_taken);\n+\n+    \/\/ might be substitutable, test if either rax or rdx is null\n+    __ testptr(rax, rax);\n+    __ jcc(Assembler::zero, (cc == equal) ? not_taken : taken);\n+    __ testptr(rdx, rdx);\n+    __ jcc(Assembler::zero, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ and both are values ?\n+    __ movptr(rbx, Address(rdx, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(rbx, is_inline_type_mask);\n+    __ cmpptr(rbx, is_inline_type_mask);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ same value klass ?\n+    __ load_metadata(rbx, rdx);\n+    __ load_metadata(rcx, rax);\n+    __ cmpptr(rbx, rcx);\n+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);\n+\n+    \/\/ Know both are the same type, let's test for substitutability...\n+    if (cc == equal) {\n+      invoke_is_substitutable(rax, rdx, taken, not_taken);\n+    } else {\n+      invoke_is_substitutable(rax, rdx, not_taken, taken);\n+    }\n+    __ stop(\"Not reachable\");\n+  }\n+\n@@ -2345,0 +2441,1 @@\n+  __ bind(taken);\n@@ -2347,1 +2444,10 @@\n-  __ profile_not_taken_branch(rax);\n+  __ profile_not_taken_branch(rax, true);\n+}\n+\n+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,\n+                                            Label& is_subst, Label& not_subst) {\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);\n+  \/\/ Restored...rax answer, jmp to outcome...\n+  __ testl(rax, rax);\n+  __ jcc(Assembler::zero, not_subst);\n+  __ jmp(is_subst);\n@@ -2616,1 +2722,2 @@\n-  __ remove_activation(state, rbcp);\n+\n+  __ remove_activation(state, rbcp, true, true, true);\n@@ -2982,1 +3089,1 @@\n-  const Register obj   = LP64_ONLY(c_rarg3) NOT_LP64(rcx);\n+  const Register obj   = LP64_ONLY(r9) NOT_LP64(rcx);\n@@ -2994,2 +3101,0 @@\n-  if (!is_static) pop_and_check_object(obj);\n-\n@@ -2998,1 +3103,1 @@\n-  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj;\n+  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj, notInlineType;\n@@ -3006,0 +3111,1 @@\n+  if (!is_static) pop_and_check_object(obj);\n@@ -3017,1 +3123,1 @@\n-\n+   if (!is_static) pop_and_check_object(obj);\n@@ -3032,4 +3138,86 @@\n-  do_oop_load(_masm, field, rax);\n-  __ push(atos);\n-  if (!is_static && rc == may_rewrite) {\n-    patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+  if (!EnableValhalla) {\n+    if (!is_static) pop_and_check_object(obj);\n+    do_oop_load(_masm, field, rax);\n+    __ push(atos);\n+    if (!is_static && rc == may_rewrite) {\n+      patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+    }\n+    __ jmp(Done);\n+  } else {\n+    if (is_static) {\n+      __ load_heap_oop(rax, field);\n+      Label is_null_free_inline_type, uninitialized;\n+      \/\/ Issue below if the static field has not been initialized yet\n+      __ test_field_is_null_free_inline_type(flags, rscratch1, is_null_free_inline_type);\n+        \/\/ field is not a null free inline type\n+        __ push(atos);\n+        __ jmp(Done);\n+      \/\/ field is a null free inline type, must not return null even if uninitialized\n+      __ bind(is_null_free_inline_type);\n+          __ testptr(rax, rax);\n+        __ jcc(Assembler::zero, uninitialized);\n+          __ push(atos);\n+          __ jmp(Done);\n+        __ bind(uninitialized);\n+#ifdef _LP64\n+          Label slow_case, finish;\n+          __ movptr(rbx, Address(obj, java_lang_Class::klass_offset()));\n+          __ cmpb(Address(rbx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+          __ jcc(Assembler::notEqual, slow_case);\n+        __ get_default_value_oop(rbx, rscratch1, rax);\n+        __ jmp(finish);\n+        __ bind(slow_case);\n+#endif \/\/ LP64\n+          __ call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_static_inline_type_field),\n+                obj, cache);\n+#ifdef _LP64\n+          __ bind(finish);\n+  #endif \/\/ _LP64\n+        __ verify_oop(rax);\n+        __ push(atos);\n+        __ jmp(Done);\n+    } else {\n+      Label is_flat, nonnull, is_inline_type, rewrite_inline, has_null_marker;\n+      __ test_field_is_null_free_inline_type(flags, rscratch1, is_inline_type);\n+      __ test_field_has_null_marker(flags, rscratch1, has_null_marker);\n+      \/\/ field is not a null free inline type\n+      pop_and_check_object(obj);\n+      __ load_heap_oop(rax, field);\n+      __ push(atos);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);\n+      }\n+      __ jmp(Done);\n+      __ bind(is_inline_type);\n+      __ test_field_is_flat(flags, rscratch1, is_flat);\n+          \/\/ field is not flat\n+          pop_and_check_object(obj);\n+          __ load_heap_oop(rax, field);\n+          __ testptr(rax, rax);\n+          __ jcc(Assembler::notZero, nonnull);\n+            __ load_unsigned_short(flags, Address(cache, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+            __ movptr(rcx, Address(cache, ResolvedFieldEntry::field_holder_offset()));\n+            __ get_inline_type_field_klass(rcx, flags, rbx);\n+            __ get_default_value_oop(rbx, rcx, rax);\n+          __ bind(nonnull);\n+          __ verify_oop(rax);\n+          __ push(atos);\n+          __ jmp(rewrite_inline);\n+        __ bind(is_flat);\n+          pop_and_check_object(rax);\n+          __ read_flat_field(rcx, rdx, rbx, rax);\n+          __ verify_oop(rax);\n+          __ push(atos);\n+          __ jmp(rewrite_inline);\n+      __ bind(has_null_marker);\n+        pop_and_check_object(rax);\n+        __ load_field_entry(rcx, rbx);\n+        call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_nullable_flat_field), rax, rcx);\n+        __ get_vm_result(rax, r15_thread);\n+        __ push(atos);\n+      __ bind(rewrite_inline);\n+      if (rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_vgetfield, bc, rbx);\n+      }\n+        __ jmp(Done);\n+    }\n@@ -3037,1 +3225,0 @@\n-  __ jmp(Done);\n@@ -3040,0 +3227,3 @@\n+\n+  if (!is_static) pop_and_check_object(obj);\n+\n@@ -3139,1 +3329,0 @@\n-\n@@ -3221,1 +3410,1 @@\n-  const Register flags = rax;\n+  const Register flags = r9;\n@@ -3234,2 +3423,3 @@\n-  __ andl(flags, (1 << ResolvedFieldEntry::is_volatile_shift));\n-  __ testl(flags, flags);\n+  __ movl(rscratch1, flags);\n+  __ andl(rscratch1, (1 << ResolvedFieldEntry::is_volatile_shift));\n+  __ testl(rscratch1, rscratch1);\n@@ -3238,1 +3428,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state, flags);\n@@ -3244,1 +3434,1 @@\n-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state);\n+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, tos_state, flags);\n@@ -3250,1 +3440,1 @@\n-                                              Register obj, Register off, Register tos_state) {\n+                                              Register obj, Register off, Register tos_state, Register flags) {\n@@ -3257,1 +3447,1 @@\n-        notLong, notFloat, notObj;\n+        notLong, notFloat, notObj, notInlineType;\n@@ -3298,6 +3488,63 @@\n-    __ pop(atos);\n-    if (!is_static) pop_and_check_object(obj);\n-    \/\/ Store into the field\n-    do_oop_store(_masm, field, rax);\n-    if (!is_static && rc == may_rewrite) {\n-      patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+    if (!EnableValhalla) {\n+      __ pop(atos);\n+      if (!is_static) pop_and_check_object(obj);\n+      \/\/ Store into the field\n+      do_oop_store(_masm, field, rax);\n+      if (!is_static && rc == may_rewrite) {\n+        patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+      }\n+      __ jmp(Done);\n+    } else {\n+      __ pop(atos);\n+      if (is_static) {\n+        Label is_inline_type;\n+        __ test_field_is_not_null_free_inline_type(flags, rscratch1, is_inline_type);\n+        __ null_check(rax);\n+        __ bind(is_inline_type);\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(Done);\n+      } else {\n+        Label is_null_free_inline_type, is_flat, has_null_marker,\n+              write_null, rewrite_not_inline, rewrite_inline;\n+        __ test_field_is_null_free_inline_type(flags, rscratch1, is_null_free_inline_type);\n+        __ test_field_has_null_marker(flags, rscratch1, has_null_marker);\n+          \/\/ Not an inline type\n+          pop_and_check_object(obj);\n+          \/\/ Store into the field\n+          do_oop_store(_masm, field, rax);\n+          __ bind(rewrite_not_inline);\n+          if (rc == may_rewrite) {\n+            patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);\n+          }\n+          __ jmp(Done);\n+        \/\/ Implementation of the inline type semantic\n+        __ bind(is_null_free_inline_type);\n+          __ null_check(rax);\n+          __ test_field_is_flat(flags, rscratch1, is_flat);\n+            \/\/ field is not flat\n+            pop_and_check_object(obj);\n+            \/\/ Store into the field\n+            do_oop_store(_masm, field, rax);\n+          __ jmp(rewrite_inline);\n+          __ bind(is_flat);\n+            \/\/ field is flat\n+            __ load_unsigned_short(rdx, Address(rcx, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+            __ movptr(r9, Address(rcx, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+            pop_and_check_object(obj);  \/\/ obj = rcx\n+            __ load_klass(r8, rax, rscratch1);\n+            __ payload_addr(rax, rax, r8);\n+            __ addptr(obj, off);\n+            __ inline_layout_info(r9, rdx, rbx);\n+            \/\/ because we use InlineLayoutInfo, we need special value access code specialized for fields (arrays will need a different API)\n+            __ flat_field_copy(IN_HEAP, rax, obj, rbx);\n+            __ jmp(rewrite_inline);\n+        __ bind(has_null_marker); \/\/ has null marker means the field is flat with a null marker\n+          pop_and_check_object(rbx);\n+          __ load_field_entry(rcx, rdx);\n+          call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_nullable_flat_field), rbx, rax, rcx);\n+        __ bind(rewrite_inline);\n+        if (rc == may_rewrite) {\n+          patch_bytecode(Bytecodes::_fast_vputfield, bc, rbx, true, byte_no);\n+        }\n+        __ jmp(Done);\n+      }\n@@ -3305,1 +3552,0 @@\n-    __ jmp(Done);\n@@ -3444,0 +3690,1 @@\n+    case Bytecodes::_fast_vputfield: \/\/fall through\n@@ -3469,0 +3716,1 @@\n+    case Bytecodes::_fast_vputfield: \/\/ fall through\n@@ -3487,2 +3735,0 @@\n-  Register cache = rcx;\n-\n@@ -3495,3 +3741,1 @@\n-  load_resolved_field_entry(noreg, cache, rax, rbx, rdx);\n-  \/\/ RBX: field offset, RAX: TOS, RDX: flags\n-  __ andl(rdx, (1 << ResolvedFieldEntry::is_volatile_shift));\n+  load_resolved_field_entry(noreg, rcx, rax, rbx, rdx);\n@@ -3499,0 +3743,1 @@\n+  \/\/ RBX: field offset, RCX: RAX: TOS, RDX: flags\n@@ -3507,1 +3752,3 @@\n-  __ testl(rdx, rdx);\n+  __ movl(rscratch2, rdx);  \/\/ saving flags for is_flat test\n+  __ andl(rscratch2, (1 << ResolvedFieldEntry::is_volatile_shift));\n+  __ testl(rscratch2, rscratch2);\n@@ -3510,1 +3757,1 @@\n-  fast_storefield_helper(field, rax);\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3516,1 +3763,1 @@\n-  fast_storefield_helper(field, rax);\n+  fast_storefield_helper(field, rax, rdx);\n@@ -3521,1 +3768,3 @@\n-void TemplateTable::fast_storefield_helper(Address field, Register rax) {\n+void TemplateTable::fast_storefield_helper(Address field, Register rax, Register flags) {\n+\n+  \/\/ DANGER: 'field' argument depends on rcx and rbx\n@@ -3525,0 +3774,27 @@\n+  case Bytecodes::_fast_vputfield:\n+    {\n+      Label is_flat, has_null_marker, write_null, done;\n+      __ test_field_has_null_marker(flags, rscratch1, has_null_marker);\n+      \/\/ Null free field cases: flat or not flat\n+      __ null_check(rax);\n+      __ test_field_is_flat(flags, rscratch1, is_flat);\n+        \/\/ field is not flat\n+        do_oop_store(_masm, field, rax);\n+        __ jmp(done);\n+      __ bind(is_flat);\n+        __ load_field_entry(r8, r9);\n+        __ load_unsigned_short(r9, Address(r8, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+        __ movptr(r8, Address(r8, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+        __ inline_layout_info(r8, r9, r8);\n+        __ load_klass(rdx, rax, rscratch1);\n+        __ payload_addr(rax, rax, rdx);\n+        __ lea(rcx, field);\n+        __ flat_field_copy(IN_HEAP, rax, rcx, r8);\n+        __ jmp(done);\n+      __ bind(has_null_marker); \/\/ has null marker means the field is flat with a null marker\n+        __ movptr(rbx, rcx);\n+        __ load_field_entry(rcx, rdx);\n+        call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_nullable_flat_field), rbx, rax, rcx);\n+      __ bind(done);\n+    }\n+    break;\n@@ -3526,1 +3802,3 @@\n-    do_oop_store(_masm, field, rax);\n+    {\n+      do_oop_store(_masm, field, rax);\n+    }\n@@ -3588,1 +3866,1 @@\n-  __ load_sized_value(rbx, Address(rcx, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n+  __ load_sized_value(rdx, Address(rcx, in_bytes(ResolvedFieldEntry::field_offset_offset())), sizeof(int), true \/*is_signed*\/);\n@@ -3593,1 +3871,1 @@\n-  Address field(rax, rbx, Address::times_1);\n+  Address field(rax, rdx, Address::times_1);\n@@ -3597,0 +3875,29 @@\n+  case Bytecodes::_fast_vgetfield:\n+    {\n+      Label is_flat, nonnull, Done, has_null_marker;\n+      __ load_unsigned_byte(rscratch1, Address(rcx, in_bytes(ResolvedFieldEntry::flags_offset())));\n+      __ test_field_has_null_marker(rscratch1, rscratch2, has_null_marker);\n+      __ test_field_is_flat(rscratch1, rscratch2, is_flat);\n+        \/\/ field is not flat\n+        __ load_heap_oop(rax, field);\n+        __ testptr(rax, rax);\n+        __ jcc(Assembler::notZero, nonnull);\n+          __ load_unsigned_short(rdx, Address(rcx, in_bytes(ResolvedFieldEntry::field_index_offset())));\n+          __ movptr(rcx, Address(rcx, ResolvedFieldEntry::field_holder_offset()));\n+          __ get_inline_type_field_klass(rcx, rdx, rbx);\n+          __ get_default_value_oop(rbx, rcx, rax);\n+        __ bind(nonnull);\n+        __ verify_oop(rax);\n+        __ jmp(Done);\n+      __ bind(is_flat);\n+      \/\/ field is flat\n+        __ read_flat_field(rcx, rdx, rbx, rax);\n+        __ jmp(Done);\n+      __ bind(has_null_marker);\n+        \/\/ rax = instance, rcx = resolved entry\n+        call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_nullable_flat_field), rax, rcx);\n+        __ get_vm_result(rax, r15_thread);\n+      __ bind(Done);\n+      __ verify_oop(rax);\n+    }\n+    break;\n@@ -4032,2 +4339,0 @@\n-  Label slow_case_no_pop;\n-  Label initialize_header;\n@@ -4043,1 +4348,1 @@\n-  __ jcc(Assembler::notEqual, slow_case_no_pop);\n+  __ jcc(Assembler::notEqual, slow_case);\n@@ -4047,1 +4352,0 @@\n-  __ push(rcx);  \/\/ save the contexts of klass for initializing the header\n@@ -4059,79 +4363,1 @@\n-  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n-  __ movl(rdx, Address(rcx, Klass::layout_helper_offset()));\n-  \/\/ test to see if it is malformed in some way\n-  __ testl(rdx, Klass::_lh_instance_slow_path_bit);\n-  __ jcc(Assembler::notZero, slow_case);\n-\n-  \/\/ Allocate the instance:\n-  \/\/  If TLAB is enabled:\n-  \/\/    Try to allocate in the TLAB.\n-  \/\/    If fails, go to the slow path.\n-  \/\/    Initialize the allocation.\n-  \/\/    Exit.\n-  \/\/\n-  \/\/  Go to slow path.\n-\n-  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n-\n-  if (UseTLAB) {\n-    NOT_LP64(__ get_thread(thread);)\n-    __ tlab_allocate(thread, rax, rdx, 0, rcx, rbx, slow_case);\n-    if (ZeroTLAB) {\n-      \/\/ the fields have been already cleared\n-      __ jmp(initialize_header);\n-    }\n-\n-    \/\/ The object is initialized before the header.  If the object size is\n-    \/\/ zero, go directly to the header initialization.\n-    if (UseCompactObjectHeaders) {\n-      assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n-      __ decrement(rdx, oopDesc::base_offset_in_bytes());\n-    } else {\n-      __ decrement(rdx, sizeof(oopDesc));\n-    }\n-    __ jcc(Assembler::zero, initialize_header);\n-\n-    \/\/ Initialize topmost object field, divide rdx by 8, check if odd and\n-    \/\/ test if zero.\n-    __ xorl(rcx, rcx);    \/\/ use zero reg to clear memory (shorter code)\n-    __ shrl(rdx, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n-\n-    \/\/ rdx must have been multiple of 8\n-#ifdef ASSERT\n-    \/\/ make sure rdx was multiple of 8\n-    Label L;\n-    \/\/ Ignore partial flag stall after shrl() since it is debug VM\n-    __ jcc(Assembler::carryClear, L);\n-    __ stop(\"object size is not multiple of 2 - adjust this code\");\n-    __ bind(L);\n-    \/\/ rdx must be > 0, no extra check needed here\n-#endif\n-\n-    \/\/ initialize remaining object fields: rdx was a multiple of 8\n-    { Label loop;\n-    __ bind(loop);\n-    int header_size_bytes = oopDesc::header_size() * HeapWordSize;\n-    assert(is_aligned(header_size_bytes, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n-    __ movptr(Address(rax, rdx, Address::times_8, header_size_bytes - 1*oopSize), rcx);\n-    NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, header_size_bytes - 2*oopSize), rcx));\n-    __ decrement(rdx);\n-    __ jcc(Assembler::notZero, loop);\n-    }\n-\n-    \/\/ initialize object header only.\n-    __ bind(initialize_header);\n-    if (UseCompactObjectHeaders) {\n-      __ pop(rcx);   \/\/ get saved klass back in the register.\n-      __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));\n-      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()), rbx);\n-    } else {\n-      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n-                (intptr_t)markWord::prototype().value()); \/\/ header\n-      __ pop(rcx);   \/\/ get saved klass back in the register.\n-#ifdef _LP64\n-      __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-      __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n-#endif\n-      __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n-    }\n-\n+  __ allocate_instance(rcx, rax, rdx, rbx, true, slow_case);\n@@ -4145,3 +4371,1 @@\n-\n-    __ jmp(done);\n-  }\n+  __ jmp(done);\n@@ -4151,2 +4375,0 @@\n-  __ pop(rcx);   \/\/ restore stack pointer to what it was when we came in.\n-  __ bind(slow_case_no_pop);\n@@ -4201,4 +4423,4 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+      Address::times_1,\n+      Array<u1>::base_offset_in_bytes()));\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -4242,0 +4464,3 @@\n+  __ jmp(done);\n+\n+  __ bind(is_null);\n@@ -4245,4 +4470,1 @@\n-    __ jmp(done);\n-    __ bind(is_null);\n-  } else {\n-    __ bind(is_null);   \/\/ same as 'done'\n+\n@@ -4264,4 +4486,4 @@\n-  __ cmpb(Address(rdx, rbx,\n-                  Address::times_1,\n-                  Array<u1>::base_offset_in_bytes()),\n-          JVM_CONSTANT_Class);\n+  __ movzbl(rdx, Address(rdx, rbx,\n+        Address::times_1,\n+        Array<u1>::base_offset_in_bytes()));\n+  __ cmpl(rdx, JVM_CONSTANT_Class);\n@@ -4319,1 +4541,0 @@\n-\n@@ -4381,0 +4602,4 @@\n+  Label is_inline_type;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ test_markword_is_inline_type(rbx, is_inline_type);\n+\n@@ -4473,0 +4698,5 @@\n+\n+  __ bind(is_inline_type);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                    InterpreterRuntime::throw_identity_exception), rax);\n+  __ should_not_reach_here();\n@@ -4481,0 +4711,11 @@\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  Label has_identity;\n+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));\n+  __ andptr(rbx, is_inline_type_mask);\n+  __ cmpl(rbx, is_inline_type_mask);\n+  __ jcc(Assembler::notEqual, has_identity);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                     InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+  __ bind(has_identity);\n+\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":397,"deletions":156,"binary":false,"changes":553,"status":"modified"},{"patch":"@@ -1856,1 +1856,1 @@\n-  if (!UseFastStosb && UseSSE >= 2 && UseUnalignedLoadStores) {\n+  if (UseSSE >= 2 && UseUnalignedLoadStores) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-VtableStub* VtableStubs::create_vtable_stub(int vtable_index) {\n+VtableStub* VtableStubs::create_vtable_stub(int vtable_index, bool caller_is_c1) {\n@@ -50,2 +50,2 @@\n-  VtableStub* s = new(stub_code_length) VtableStub(true, vtable_index);\n-  \/\/ Can be null if there is no free space in the code cache.\n+  VtableStub* s = new(stub_code_length) VtableStub(true, vtable_index, caller_is_c1);\n+  \/\/ Can be nullptr if there is no free space in the code cache.\n@@ -64,0 +64,1 @@\n+  ByteSize  entry_offset = caller_is_c1 ? Method::from_compiled_inline_offset() :  Method::from_compiled_inline_ro_offset();\n@@ -120,1 +121,1 @@\n-    __ cmpptr(Address(method, Method::from_compiled_offset()), NULL_WORD);\n+    __ cmpptr(Address(method, entry_offset), NULL_WORD);\n@@ -131,1 +132,1 @@\n-  __ jmp( Address(rbx, Method::from_compiled_offset()));\n+  __ jmp( Address(rbx, entry_offset));\n@@ -141,1 +142,1 @@\n-VtableStub* VtableStubs::create_itable_stub(int itable_index) {\n+VtableStub* VtableStubs::create_itable_stub(int itable_index, bool caller_is_c1) {\n@@ -144,2 +145,3 @@\n-  VtableStub* s = new(stub_code_length) VtableStub(false, itable_index);\n-  \/\/ Can be null if there is no free space in the code cache.\n+  ByteSize  entry_offset = caller_is_c1 ? Method::from_compiled_inline_offset() :  Method::from_compiled_inline_ro_offset();\n+  VtableStub* s = new(stub_code_length) VtableStub(false, itable_index, caller_is_c1);\n+  \/\/ Can be nullptr if there is no free space in the code cache.\n@@ -212,1 +214,1 @@\n-  const ptrdiff_t estimate = 136;\n+  const ptrdiff_t estimate = 144;\n@@ -231,1 +233,1 @@\n-    __ cmpptr(Address(method, Method::from_compiled_offset()), NULL_WORD);\n+    __ cmpptr(Address(method, entry_offset), NULL_WORD);\n@@ -239,1 +241,1 @@\n-  __ jmp(Address(method, Method::from_compiled_offset()));\n+  __ jmp(Address(method, entry_offset));\n","filename":"src\/hotspot\/cpu\/x86\/vtableStubs_x86_64.cpp","additions":13,"deletions":11,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -613,4 +613,1 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, C->in_24_bit_fp_mode(), C->stub_function() != nullptr);\n+  __ verified_entry(C);\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -53,7 +53,26 @@\n-AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(\n-                        MacroAssembler *masm,\n-                        int total_args_passed,\n-                        int comp_args_on_stack,\n-                        const BasicType *sig_bt,\n-                        const VMRegPair *regs,\n-                        AdapterFingerPrint *fingerprint) {\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt,\n+                                           VMRegPair *regs,\n+                                           int total_args_passed) {\n+  Unimplemented();\n+  return 0;\n+}\n+\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  Unimplemented();\n+  return NULL;\n+}\n+\n+AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler* masm,\n+                                                            int comp_args_on_stack,\n+                                                            const GrowableArray <SigEntry>* sig,\n+                                                            const VMRegPair* regs,\n+                                                            const GrowableArray <SigEntry>* sig_cc,\n+                                                            const VMRegPair* regs_cc,\n+                                                            const GrowableArray <SigEntry>* sig_cc_ro,\n+                                                            const VMRegPair* regs_cc_ro,\n+                                                            AdapterFingerPrint* fingerprint,\n+                                                            AdapterBlob*& new_adapter,\n+                                                            bool allocate_code_blob) {\n+  if (allocate_code_blob) {\n+    new_adapter = AdapterBlob::create(masm->code(), 0, 0, NULL);\n+  }\n@@ -64,0 +83,4 @@\n+    CAST_FROM_FN_PTR(address,zero_null_code_stub),\n+    CAST_FROM_FN_PTR(address,zero_null_code_stub),\n+    CAST_FROM_FN_PTR(address,zero_null_code_stub),\n+    CAST_FROM_FN_PTR(address,zero_null_code_stub),\n","filename":"src\/hotspot\/cpu\/zero\/sharedRuntime_zero.cpp","additions":30,"deletions":7,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-VtableStub* VtableStubs::create_vtable_stub(int vtable_index) {\n+VtableStub* VtableStubs::create_vtable_stub(int vtable_index, bool caller_is_c1) {\n@@ -34,1 +34,1 @@\n-VtableStub* VtableStubs::create_itable_stub(int vtable_index) {\n+VtableStub* VtableStubs::create_itable_stub(int vtable_index, bool caller_is_c1) {\n","filename":"src\/hotspot\/cpu\/zero\/vtableStubs_zero.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -220,0 +220,1 @@\n+  AD.addInclude(AD._CPP_file, \"gc\/shared\/barrierSetAssembler.hpp\");\n","filename":"src\/hotspot\/share\/adlc\/main.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -55,0 +55,3 @@\n+                 Inline_Entry,\n+                 Verified_Inline_Entry,\n+                 Verified_Inline_Entry_RO,\n@@ -70,0 +73,1 @@\n+  void check(int e) const { assert(0 <= e && e < max_Entries, \"must be\"); }\n@@ -75,0 +79,3 @@\n+    _values[Inline_Entry  ] = 0;\n+    _values[Verified_Inline_Entry] = -1;\n+    _values[Verified_Inline_Entry_RO] = -1;\n@@ -83,2 +90,2 @@\n-  int value(Entries e) { return _values[e]; }\n-  void set_value(Entries e, int val) { _values[e] = val; }\n+  int value(Entries e) const { check(e); return _values[e]; }\n+  void set_value(Entries e, int val) { check(e); _values[e] = val; }\n","filename":"src\/hotspot\/share\/asm\/codeBuffer.hpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -0,0 +1,250 @@\n+\/*\n+ * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"jvm.h\"\n+#include \"asm\/assembler.hpp\"\n+#include \"asm\/assembler.inline.hpp\"\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n+#include \"runtime\/signature_cc.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/compile.hpp\"\n+#include \"opto\/node.hpp\"\n+#endif\n+\n+void MacroAssembler::skip_unpacked_fields(const GrowableArray<SigEntry>* sig, int& sig_index, VMRegPair* regs_from, int regs_from_count, int& from_index) {\n+  ScalarizedInlineArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);\n+  VMReg reg;\n+  BasicType bt;\n+  while (stream.next(reg, bt)) {}\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+}\n+\n+bool MacroAssembler::is_reg_in_unpacked_fields(const GrowableArray<SigEntry>* sig, int sig_index, VMReg to, VMRegPair* regs_from, int regs_from_count, int from_index) {\n+  ScalarizedInlineArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);\n+  VMReg reg;\n+  BasicType bt;\n+  while (stream.next(reg, bt)) {\n+    if (reg == to) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+void MacroAssembler::mark_reg_writable(const VMRegPair* regs, int num_regs, int reg_index, MacroAssembler::RegState* reg_state) {\n+  assert(0 <= reg_index && reg_index < num_regs, \"sanity\");\n+  VMReg from_reg = regs[reg_index].first();\n+  if (from_reg->is_valid()) {\n+    assert(from_reg->is_stack(), \"reserved entries must be stack\");\n+    reg_state[from_reg->value()] = MacroAssembler::reg_writable;\n+  }\n+}\n+\n+MacroAssembler::RegState* MacroAssembler::init_reg_state(VMRegPair* regs, int num_regs, int sp_inc, int max_stack) {\n+  int max_reg = VMRegImpl::stack2reg(max_stack)->value();\n+  MacroAssembler::RegState* reg_state = NEW_RESOURCE_ARRAY(MacroAssembler::RegState, max_reg);\n+\n+  \/\/ Make all writable\n+  for (int i = 0; i < max_reg; ++i) {\n+    reg_state[i] = MacroAssembler::reg_writable;\n+  }\n+  \/\/ Set all source registers\/stack slots to readonly to prevent accidental overwriting\n+  for (int i = 0; i < num_regs; ++i) {\n+    VMReg reg = regs[i].first();\n+    if (!reg->is_valid()) continue;\n+    if (reg->is_stack()) {\n+      \/\/ Update source stack location by adding stack increment\n+      reg = VMRegImpl::stack2reg(reg->reg2stack() + sp_inc\/VMRegImpl::stack_slot_size);\n+      regs[i] = reg;\n+    }\n+    assert(reg->value() >= 0 && reg->value() < max_reg, \"reg value out of bounds\");\n+    reg_state[reg->value()] = MacroAssembler::reg_readonly;\n+  }\n+  return reg_state;\n+}\n+\n+#ifdef COMPILER2\n+int MacroAssembler::unpack_inline_args(Compile* C, bool receiver_only) {\n+  assert(C->has_scalarized_args(), \"inline type argument scalarization is disabled\");\n+  Method* method = C->method()->get_Method();\n+  const GrowableArray<SigEntry>* sig = method->adapter()->get_sig_cc();\n+  assert(sig != nullptr, \"must have scalarized signature\");\n+\n+  \/\/ Get unscalarized calling convention\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, 256);\n+  int args_passed = 0;\n+  if (!method->is_static()) {\n+    sig_bt[args_passed++] = T_OBJECT;\n+  }\n+  if (!receiver_only) {\n+    for (SignatureStream ss(method->signature()); !ss.at_return_type(); ss.next()) {\n+      BasicType bt = ss.type();\n+      sig_bt[args_passed++] = bt;\n+      if (type2size[bt] == 2) {\n+        sig_bt[args_passed++] = T_VOID;\n+      }\n+    }\n+  } else {\n+    \/\/ Only unpack the receiver, all other arguments are already scalarized\n+    InstanceKlass* holder = method->method_holder();\n+    int rec_len = (holder->is_inline_klass() && method->is_scalarized_arg(0)) ? InlineKlass::cast(holder)->extended_sig()->length() : 1;\n+    \/\/ Copy scalarized signature but skip receiver and inline type delimiters\n+    for (int i = 0; i < sig->length(); i++) {\n+      if (SigEntry::skip_value_delimiters(sig, i) && rec_len <= 0) {\n+        sig_bt[args_passed++] = sig->at(i)._bt;\n+      }\n+      rec_len--;\n+    }\n+  }\n+  VMRegPair* regs = NEW_RESOURCE_ARRAY(VMRegPair, args_passed);\n+  int args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, args_passed);\n+\n+  \/\/ Get scalarized calling convention\n+  int args_passed_cc = SigEntry::fill_sig_bt(sig, sig_bt);\n+  VMRegPair* regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, sig->length());\n+  int args_on_stack_cc = SharedRuntime::java_calling_convention(sig_bt, regs_cc, args_passed_cc);\n+\n+  \/\/ Check if we need to extend the stack for unpacking\n+  int sp_inc = 0;\n+  if (args_on_stack_cc > args_on_stack) {\n+    sp_inc = extend_stack_for_inline_args(args_on_stack_cc);\n+  }\n+  shuffle_inline_args(false, receiver_only, sig,\n+                      args_passed, args_on_stack, regs,           \/\/ from\n+                      args_passed_cc, args_on_stack_cc, regs_cc,  \/\/ to\n+                      sp_inc, noreg);\n+  return sp_inc;\n+}\n+#endif \/\/ COMPILER2\n+\n+void MacroAssembler::shuffle_inline_args(bool is_packing, bool receiver_only,\n+                                         const GrowableArray<SigEntry>* sig,\n+                                         int args_passed, int args_on_stack, VMRegPair* regs,\n+                                         int args_passed_to, int args_on_stack_to, VMRegPair* regs_to,\n+                                         int sp_inc, Register val_array) {\n+  int max_stack = MAX2(args_on_stack + sp_inc\/VMRegImpl::stack_slot_size, args_on_stack_to);\n+  RegState* reg_state = init_reg_state(regs, args_passed, sp_inc, max_stack);\n+\n+  \/\/ Emit code for packing\/unpacking inline type arguments\n+  \/\/ We try multiple times and eventually start spilling to resolve (circular) dependencies\n+  bool done = (args_passed_to == 0);\n+  for (int i = 0; i < 2*args_passed_to && !done; ++i) {\n+    done = true;\n+    bool spill = (i > args_passed_to); \/\/ Start spilling?\n+    \/\/ Iterate over all arguments (when unpacking, do in reverse)\n+    int step = is_packing ? 1 : -1;\n+    int from_index    = is_packing ? 0 : args_passed      - 1;\n+    int to_index      = is_packing ? 0 : args_passed_to   - 1;\n+    int sig_index     = is_packing ? 0 : sig->length()    - 1;\n+    int sig_index_end = is_packing ? sig->length() : -1;\n+    int vtarg_index = 0;\n+    for (; sig_index != sig_index_end; sig_index += step) {\n+      assert(0 <= sig_index && sig_index < sig->length(), \"index out of bounds\");\n+      if (spill) {\n+        \/\/ This call returns true IFF we should keep trying to spill in this round.\n+        spill = shuffle_inline_args_spill(is_packing, sig, sig_index, regs, from_index, args_passed,\n+                                          reg_state);\n+      }\n+      BasicType bt = sig->at(sig_index)._bt;\n+      if (SigEntry::skip_value_delimiters(sig, sig_index)) {\n+        VMReg from_reg = regs[from_index].first();\n+        if (from_reg->is_valid()) {\n+          done &= move_helper(from_reg, regs_to[to_index].first(), bt, reg_state);\n+        } else {\n+          \/\/ halves of T_LONG or T_DOUBLE\n+          assert(bt == T_VOID, \"unexpected basic type\");\n+        }\n+        to_index += step;\n+        from_index += step;\n+      } else if (is_packing) {\n+        assert(val_array != noreg, \"must be\");\n+        VMReg reg_to = regs_to[to_index].first();\n+        done &= pack_inline_helper(sig, sig_index, vtarg_index,\n+                                   regs, args_passed, from_index, reg_to,\n+                                   reg_state, val_array);\n+        vtarg_index++;\n+        to_index++;\n+      } else if (!receiver_only || (from_index == 0 && bt == T_VOID)) {\n+        VMReg from_reg = regs[from_index].first();\n+        done &= unpack_inline_helper(sig, sig_index,\n+                                     from_reg, from_index, regs_to, args_passed_to, to_index,\n+                                     reg_state);\n+        if (from_index == -1 && sig_index != 0) {\n+          \/\/ This can happen when we are confusing an empty inline type argument which is\n+          \/\/ not counted in the scalarized signature for the receiver. Just ignore it.\n+          assert(receiver_only, \"sanity\");\n+          from_index = 0;\n+        }\n+      }\n+    }\n+  }\n+  guarantee(done, \"Could not resolve circular dependency when shuffling inline type arguments\");\n+}\n+\n+bool MacroAssembler::shuffle_inline_args_spill(bool is_packing, const GrowableArray<SigEntry>* sig, int sig_index,\n+                                               VMRegPair* regs_from, int from_index, int regs_from_count, RegState* reg_state) {\n+  VMReg reg;\n+  if (!is_packing || SigEntry::skip_value_delimiters(sig, sig_index)) {\n+    reg = regs_from[from_index].first();\n+    if (!reg->is_valid() || reg_state[reg->value()] != reg_readonly) {\n+      \/\/ Spilling this won't break cycles\n+      return true;\n+    }\n+  } else {\n+    ScalarizedInlineArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);\n+    VMReg from_reg;\n+    BasicType bt;\n+    bool found = false;\n+    while (stream.next(from_reg, bt)) {\n+      reg = from_reg;\n+      assert(from_reg->is_valid(), \"must be\");\n+      if (reg_state[from_reg->value()] == reg_readonly) {\n+        found = true;\n+        break;\n+      }\n+    }\n+    if (!found) {\n+      \/\/ Spilling fields in this inline type arg won't break cycles\n+      return true;\n+    }\n+  }\n+\n+  \/\/ Spill argument to be able to write the source and resolve circular dependencies\n+  VMReg spill_reg = spill_reg_for(reg);\n+  if (reg_state[spill_reg->value()] == reg_readonly) {\n+    \/\/ We have already spilled (in previous round). The spilled register should be consumed by this round.\n+  } else {\n+    bool res = move_helper(reg, spill_reg, T_DOUBLE, reg_state);\n+    assert(res, \"Spilling should not fail\");\n+    \/\/ Set spill_reg as new source and update state\n+    reg = spill_reg;\n+    regs_from[from_index].set1(reg);\n+    reg_state[reg->value()] = reg_readonly;\n+  }\n+\n+  return false; \/\/ Do not spill again in this round\n+}\n","filename":"src\/hotspot\/share\/asm\/macroAssembler_common.cpp","additions":250,"deletions":0,"binary":false,"changes":250,"status":"added"},{"patch":"@@ -283,1 +283,1 @@\n-  if (!x->mismatched() && array != nullptr && index != nullptr) {\n+  if (!x->should_profile() && !x->mismatched() && array != nullptr && index != nullptr) {\n@@ -678,1 +678,2 @@\n-      if (!is_interface && klass->is_subtype_of(x->klass())) {\n+      if (!is_interface && klass->is_subtype_of(x->klass()) && (!x->is_null_free() || obj->is_null_free())) {\n+        assert(!x->klass()->is_inlinetype() || x->klass() == klass, \"Inline klasses can't have subtypes\");\n@@ -683,2 +684,2 @@\n-    \/\/ checkcast of null returns null\n-    if (obj->as_Constant() && obj->type()->as_ObjectType()->constant_value()->is_null_object()) {\n+    \/\/ checkcast of null returns null for non null-free klasses\n+    if (!x->is_null_free() && obj->is_null_obj()) {\n@@ -698,1 +699,1 @@\n-    if (obj->as_Constant() && obj->type()->as_ObjectType()->constant_value()->is_null_object()) {\n+    if (obj->as_Constant() && obj->is_null_obj()) {\n@@ -876,2 +877,3 @@\n-void Canonicalizer::do_ProfileInvoke  (ProfileInvoke*   x) {}\n-void Canonicalizer::do_RuntimeCall    (RuntimeCall*     x) {}\n+void Canonicalizer::do_ProfileInvoke    (ProfileInvoke* x) {}\n+void Canonicalizer::do_ProfileACmpTypes (ProfileACmpTypes* x) {}\n+void Canonicalizer::do_RuntimeCall      (RuntimeCall* x) {}\n","filename":"src\/hotspot\/share\/c1\/c1_Canonicalizer.cpp","additions":9,"deletions":7,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -587,0 +587,1 @@\n+, _compiled_entry_signature(method->get_Method())\n@@ -604,1 +605,0 @@\n-\n@@ -607,0 +607,6 @@\n+  {\n+    ResetNoHandleMark rnhm; \/\/ Huh? Required when doing class lookup of the Q-types\n+    \/\/ TODO 8284443 Should only be computed once\n+    _compiled_entry_signature.compute_calling_conventions(false);\n+  }\n+\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -189,1 +189,1 @@\n-bool FrameMap::finalize_frame(int nof_slots) {\n+bool FrameMap::finalize_frame(int nof_slots, bool needs_stack_repair) {\n@@ -196,1 +196,2 @@\n-                         (int)sizeof(intptr_t) +                        \/\/ offset of deopt orig pc\n+                         (int)sizeof(intptr_t) +                             \/\/ offset of deopt orig pc\n+                         (needs_stack_repair ? (int)sizeof(intptr_t) : 0) +  \/\/ stack increment value\n","filename":"src\/hotspot\/share\/c1\/c1_FrameMap.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -32,0 +32,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -1061,1 +1063,9 @@\n-  ValueStack* state_before = copy_state_indexed_access();\n+  ValueStack* state_before = nullptr;\n+  int array_idx = state()->stack_size() - 2;\n+  if (type == T_OBJECT && state()->stack_at(array_idx)->maybe_flat_array()) {\n+    \/\/ Save the entire state and re-execute on deopt when accessing flat arrays\n+    state_before = copy_state_before();\n+    state_before->set_should_reexecute(true);\n+  } else {\n+    state_before = copy_state_indexed_access();\n+  }\n@@ -1073,1 +1083,61 @@\n-  push(as_ValueType(type), append(new LoadIndexed(array, index, length, type, state_before)));\n+\n+  bool need_membar = false;\n+  LoadIndexed* load_indexed = nullptr;\n+  Instruction* result = nullptr;\n+  if (array->is_loaded_flat_array()) {\n+    ciType* array_type = array->declared_type();\n+    ciInlineKlass* elem_klass = array_type->as_flat_array_klass()->element_klass()->as_inline_klass();\n+\n+    bool can_delay_access = false;\n+    ciBytecodeStream s(method());\n+    s.force_bci(bci());\n+    s.next();\n+    if (s.cur_bc() == Bytecodes::_getfield) {\n+      bool will_link;\n+      ciField* next_field = s.get_field(will_link);\n+      bool next_needs_patching = !next_field->holder()->is_initialized() ||\n+                                 !next_field->will_link(method(), Bytecodes::_getfield) ||\n+                                 PatchALot;\n+      can_delay_access = C1UseDelayedFlattenedFieldReads && !next_needs_patching;\n+    }\n+    if (can_delay_access) {\n+      \/\/ potentially optimizable array access, storing information for delayed decision\n+      LoadIndexed* li = new LoadIndexed(array, index, length, type, state_before);\n+      DelayedLoadIndexed* dli = new DelayedLoadIndexed(li, state_before);\n+      li->set_delayed(dli);\n+      set_pending_load_indexed(dli);\n+      return; \/\/ Nothing else to do for now\n+    } else {\n+      if (elem_klass->is_empty()) {\n+        \/\/ No need to create a new instance, the default instance will be used instead\n+        load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+        apush(append(load_indexed));\n+      } else {\n+        NewInstance* new_instance = new NewInstance(elem_klass, state_before, false, true);\n+        _memory->new_instance(new_instance);\n+        apush(append_split(new_instance));\n+        load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+        load_indexed->set_vt(new_instance);\n+        \/\/ The LoadIndexed node will initialise this instance by copying from\n+        \/\/ the flat field.  Ensure these stores are visible before any\n+        \/\/ subsequent store that publishes this reference.\n+        need_membar = true;\n+      }\n+    }\n+  } else {\n+    load_indexed = new LoadIndexed(array, index, length, type, state_before);\n+    if (profile_array_accesses() && is_reference_type(type)) {\n+      compilation()->set_would_profile(true);\n+      load_indexed->set_should_profile(true);\n+      load_indexed->set_profiled_method(method());\n+      load_indexed->set_profiled_bci(bci());\n+    }\n+  }\n+  result = append(load_indexed);\n+  if (need_membar) {\n+    append(new MemBar(lir_membar_storestore));\n+  }\n+  assert(!load_indexed->should_profile() || load_indexed == result, \"should not be optimized out\");\n+  if (!array->is_loaded_flat_array()) {\n+    push(as_ValueType(type), result);\n+  }\n@@ -1079,1 +1149,9 @@\n-  ValueStack* state_before = copy_state_indexed_access();\n+  ValueStack* state_before = nullptr;\n+  int array_idx = state()->stack_size() - 3;\n+  if (type == T_OBJECT && state()->stack_at(array_idx)->maybe_flat_array()) {\n+    \/\/ Save the entire state and re-execute on deopt when accessing flat arrays\n+    state_before = copy_state_before();\n+    state_before->set_should_reexecute(true);\n+  } else {\n+    state_before = copy_state_indexed_access();\n+  }\n@@ -1104,5 +1182,2 @@\n-  StoreIndexed* result = new StoreIndexed(array, index, length, type, value, state_before, check_boolean);\n-  append(result);\n-  _memory->store_value(value);\n-  if (type == T_OBJECT && is_profiling()) {\n-    \/\/ Note that we'd collect profile data in this method if we wanted it.\n+  StoreIndexed* store_indexed = new StoreIndexed(array, index, length, type, value, state_before, check_boolean);\n+  if (profile_array_accesses() && is_reference_type(type) && !array->is_loaded_flat_array()) {\n@@ -1111,6 +1186,3 @@\n-\n-    if (profile_checkcasts()) {\n-      result->set_profiled_method(method());\n-      result->set_profiled_bci(bci());\n-      result->set_should_profile(true);\n-    }\n+    store_indexed->set_should_profile(true);\n+    store_indexed->set_profiled_method(method());\n+    store_indexed->set_profiled_bci(bci());\n@@ -1118,0 +1190,3 @@\n+  Instruction* result = append(store_indexed);\n+  assert(!store_indexed->should_profile() || store_indexed == result, \"should not be optimized out\");\n+  _memory->store_value(value);\n@@ -1120,1 +1195,0 @@\n-\n@@ -1124,1 +1198,1 @@\n-      { state()->raw_pop();\n+      { Value w = state()->raw_pop();\n@@ -1128,2 +1202,2 @@\n-      { state()->raw_pop();\n-        state()->raw_pop();\n+      { Value w1 = state()->raw_pop();\n+        Value w2 = state()->raw_pop();\n@@ -1308,0 +1382,27 @@\n+\n+  bool subst_check = false;\n+  if (EnableValhalla && (stream()->cur_bc() == Bytecodes::_if_acmpeq || stream()->cur_bc() == Bytecodes::_if_acmpne)) {\n+    ValueType* left_vt = x->type();\n+    ValueType* right_vt = y->type();\n+    if (left_vt->is_object()) {\n+      assert(right_vt->is_object(), \"must be\");\n+      ciKlass* left_klass = x->as_loaded_klass_or_null();\n+      ciKlass* right_klass = y->as_loaded_klass_or_null();\n+\n+      if (left_klass == nullptr || right_klass == nullptr) {\n+        \/\/ The klass is still unloaded, or came from a Phi node. Go slow case;\n+        subst_check = true;\n+      } else if (left_klass->can_be_inline_klass() || right_klass->can_be_inline_klass()) {\n+        \/\/ Either operand may be a value object, but we're not sure. Go slow case;\n+        subst_check = true;\n+      } else {\n+        \/\/ No need to do substitutability check\n+      }\n+    }\n+  }\n+  if ((stream()->cur_bc() == Bytecodes::_if_acmpeq || stream()->cur_bc() == Bytecodes::_if_acmpne) &&\n+      is_profiling() && profile_branches()) {\n+    compilation()->set_would_profile(true);\n+    append(new ProfileACmpTypes(method(), bci(), x, y));\n+  }\n+\n@@ -1310,1 +1411,1 @@\n-  Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()->is_optimistic()) ? state_before : nullptr, is_bb));\n+  Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()->is_optimistic() || subst_check) ? state_before : nullptr, is_bb, subst_check));\n@@ -1565,1 +1666,1 @@\n-  if (method()->name() == ciSymbols::object_initializer_name() &&\n+  if (method()->is_object_constructor() &&\n@@ -1716,0 +1817,25 @@\n+void GraphBuilder::copy_inline_content(ciInlineKlass* vk, Value src, int src_off, Value dest, int dest_off, ValueStack* state_before, ciField* enclosing_field) {\n+  for (int i = 0; i < vk->nof_declared_nonstatic_fields(); i++) {\n+    ciField* field = vk->declared_nonstatic_field_at(i);\n+    int offset = field->offset_in_bytes() - vk->payload_offset();\n+    if (field->is_flat()) {\n+      bool needs_atomic_access = !field->is_null_free() || field->is_volatile();\n+      assert(!needs_atomic_access, \"Atomic access in non-atomic container\");\n+      copy_inline_content(field->type()->as_inline_klass(), src, src_off + offset, dest, dest_off + offset, state_before, enclosing_field);\n+      if (!field->is_null_free()) {\n+        \/\/ Nullable, copy the null marker using Unsafe because null markers are no real fields\n+        int null_marker_offset = field->null_marker_offset() - vk->payload_offset();\n+        Value offset = append(new Constant(new LongConstant(src_off + null_marker_offset)));\n+        Value nm = append(new UnsafeGet(T_BOOLEAN, src, offset, false));\n+        offset = append(new Constant(new LongConstant(dest_off + null_marker_offset)));\n+        append(new UnsafePut(T_BOOLEAN, dest, offset, nm, false));\n+      }\n+    } else {\n+      Value value = append(new LoadField(src, src_off + offset, field, false, state_before, false));\n+      StoreField* store = new StoreField(dest, dest_off + offset, field, value, false, state_before, false);\n+      store->set_enclosing_field(enclosing_field);\n+      append(store);\n+    }\n+  }\n+}\n+\n@@ -1722,0 +1848,1 @@\n+\n@@ -1725,1 +1852,1 @@\n-                              PatchALot;\n+                              (!field->is_flat() && PatchALot);\n@@ -1757,1 +1884,1 @@\n-  const int offset = !needs_patching ? field->offset_in_bytes() : -1;\n+  int offset = !needs_patching ? field->offset_in_bytes() : -1;\n@@ -1767,0 +1894,4 @@\n+      } else if (field->is_null_free() && field->type()->as_instance_klass()->is_initialized() &&\n+                 field->type()->as_inline_klass()->is_empty()) {\n+        \/\/ Loading from a field of an empty, null-free inline type. Just return the default instance.\n+        constant = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n@@ -1774,2 +1905,3 @@\n-        push(type, append(new LoadField(append(obj), offset, field, true,\n-                                        state_before, needs_patching)));\n+        LoadField* load_field = new LoadField(append(obj), offset, field, true,\n+                                        state_before, needs_patching);\n+        push(type, append(load_field));\n@@ -1784,1 +1916,1 @@\n-      if (field->type()->basic_type() == T_BOOLEAN) {\n+      if (field_type == T_BOOLEAN) {\n@@ -1788,0 +1920,7 @@\n+      if (field->is_null_free()) {\n+        null_check(val);\n+      }\n+      if (field->is_null_free() && field->type()->is_loaded() && field->type()->as_inline_klass()->is_empty()) {\n+        \/\/ Storing to a field of an empty, null-free inline type. Ignore.\n+        break;\n+      }\n@@ -1794,14 +1933,31 @@\n-      obj = apop();\n-      ObjectType* obj_type = obj->type()->as_ObjectType();\n-      if (field->is_constant() && obj_type->is_constant() && !PatchALot) {\n-        ciObject* const_oop = obj_type->constant_value();\n-        if (!const_oop->is_null_object() && const_oop->is_loaded()) {\n-          ciConstant field_value = field->constant_value_of(const_oop);\n-          if (field_value.is_valid()) {\n-            constant = make_constant(field_value, field);\n-            \/\/ For CallSite objects add a dependency for invalidation of the optimization.\n-            if (field->is_call_site_target()) {\n-              ciCallSite* call_site = const_oop->as_call_site();\n-              if (!call_site->is_fully_initialized_constant_call_site()) {\n-                ciMethodHandle* target = field_value.as_object()->as_method_handle();\n-                dependency_recorder()->assert_call_site_target_value(call_site, target);\n+      if (state_before == nullptr && field->is_flat()) {\n+        \/\/ Save the entire state and re-execute on deopt when accessing flat fields\n+        assert(Interpreter::bytecode_should_reexecute(code), \"should reexecute\");\n+        state_before = copy_state_before();\n+      }\n+      if (!has_pending_field_access() && !has_pending_load_indexed()) {\n+        obj = apop();\n+        ObjectType* obj_type = obj->type()->as_ObjectType();\n+        if (field->is_null_free() && field->type()->as_instance_klass()->is_initialized()\n+            && field->type()->as_inline_klass()->is_empty()) {\n+          \/\/ Loading from a field of an empty, null-free inline type. Just return the default instance.\n+          null_check(obj);\n+          constant = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n+        } else if (field->is_constant() && !field->is_flat() && obj_type->is_constant() && !PatchALot) {\n+          ciObject* const_oop = obj_type->constant_value();\n+          if (!const_oop->is_null_object() && const_oop->is_loaded()) {\n+            ciConstant field_value = field->constant_value_of(const_oop);\n+            if (field_value.is_valid()) {\n+              if (field->is_null_free() && field_value.is_null_or_zero()) {\n+                \/\/ Non-flat inline type field. Replace null by the default value.\n+                constant = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n+              } else {\n+                constant = make_constant(field_value, field);\n+              }\n+              \/\/ For CallSite objects add a dependency for invalidation of the optimization.\n+              if (field->is_call_site_target()) {\n+                ciCallSite* call_site = const_oop->as_call_site();\n+                if (!call_site->is_fully_initialized_constant_call_site()) {\n+                  ciMethodHandle* target = field_value.as_object()->as_method_handle();\n+                  dependency_recorder()->assert_call_site_target_value(call_site, target);\n+                }\n@@ -1819,19 +1975,15 @@\n-        LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);\n-        Value replacement = !needs_patching ? _memory->load(load) : load;\n-        if (replacement != load) {\n-          assert(replacement->is_linked() || !replacement->can_be_linked(), \"should already by linked\");\n-          \/\/ Writing an (integer) value to a boolean, byte, char or short field includes an implicit narrowing\n-          \/\/ conversion. Emit an explicit conversion here to get the correct field value after the write.\n-          BasicType bt = field->type()->basic_type();\n-          switch (bt) {\n-          case T_BOOLEAN:\n-          case T_BYTE:\n-            replacement = append(new Convert(Bytecodes::_i2b, replacement, as_ValueType(bt)));\n-            break;\n-          case T_CHAR:\n-            replacement = append(new Convert(Bytecodes::_i2c, replacement, as_ValueType(bt)));\n-            break;\n-          case T_SHORT:\n-            replacement = append(new Convert(Bytecodes::_i2s, replacement, as_ValueType(bt)));\n-            break;\n-          default:\n+        if (!field->is_flat()) {\n+          if (has_pending_field_access()) {\n+            assert(!needs_patching, \"Can't patch delayed field access\");\n+            obj = pending_field_access()->obj();\n+            offset += pending_field_access()->offset() - field->holder()->as_inline_klass()->payload_offset();\n+            field = pending_field_access()->holder()->get_field_by_offset(offset, false);\n+            assert(field != nullptr, \"field not found\");\n+            set_pending_field_access(nullptr);\n+          } else if (has_pending_load_indexed()) {\n+            assert(!needs_patching, \"Can't patch delayed field access\");\n+            pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->payload_offset());\n+            LoadIndexed* li = pending_load_indexed()->load_instr();\n+            li->set_type(type);\n+            push(type, append(li));\n+            set_pending_load_indexed(nullptr);\n@@ -1840,1 +1992,24 @@\n-          push(type, replacement);\n+          LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);\n+          Value replacement = !needs_patching ? _memory->load(load) : load;\n+          if (replacement != load) {\n+            assert(replacement->is_linked() || !replacement->can_be_linked(), \"should already by linked\");\n+            \/\/ Writing an (integer) value to a boolean, byte, char or short field includes an implicit narrowing\n+            \/\/ conversion. Emit an explicit conversion here to get the correct field value after the write.\n+            switch (field_type) {\n+            case T_BOOLEAN:\n+            case T_BYTE:\n+              replacement = append(new Convert(Bytecodes::_i2b, replacement, type));\n+              break;\n+            case T_CHAR:\n+              replacement = append(new Convert(Bytecodes::_i2c, replacement, type));\n+              break;\n+            case T_SHORT:\n+              replacement = append(new Convert(Bytecodes::_i2s, replacement, type));\n+              break;\n+            default:\n+              break;\n+            }\n+            push(type, replacement);\n+          } else {\n+            push(type, append(load));\n+          }\n@@ -1842,1 +2017,81 @@\n-          push(type, append(load));\n+          \/\/ Flat field\n+          assert(!needs_patching, \"Can't patch flat inline type field access\");\n+          ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+          bool is_naturally_atomic = inline_klass->nof_declared_nonstatic_fields() <= 1;\n+          bool needs_atomic_access = !field->is_null_free() || (field->is_volatile() && !is_naturally_atomic);\n+          if (needs_atomic_access) {\n+            assert(!has_pending_field_access(), \"Pending field accesses are not supported\");\n+            LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);\n+            push(type, append(load));\n+          } else {\n+            assert(field->is_null_free(), \"must be null-free\");\n+            \/\/ Look at the next bytecode to check if we can delay the field access\n+            bool can_delay_access = false;\n+            ciBytecodeStream s(method());\n+            s.force_bci(bci());\n+            s.next();\n+            if (s.cur_bc() == Bytecodes::_getfield && !needs_patching) {\n+              ciField* next_field = s.get_field(will_link);\n+              bool next_needs_patching = !next_field->holder()->is_loaded() ||\n+                                         !next_field->will_link(method(), Bytecodes::_getfield) ||\n+                                         PatchALot;\n+              \/\/ We can't update the offset for atomic accesses\n+              bool next_needs_atomic_access = !next_field->is_null_free() || next_field->is_volatile();\n+              can_delay_access = C1UseDelayedFlattenedFieldReads && !next_needs_patching && !next_needs_atomic_access;\n+            }\n+            if (can_delay_access) {\n+              if (has_pending_load_indexed()) {\n+                pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->payload_offset());\n+              } else if (has_pending_field_access()) {\n+                pending_field_access()->inc_offset(offset - field->holder()->as_inline_klass()->payload_offset());\n+              } else {\n+                null_check(obj);\n+                DelayedFieldAccess* dfa = new DelayedFieldAccess(obj, field->holder(), field->offset_in_bytes(), state_before);\n+                set_pending_field_access(dfa);\n+              }\n+            } else {\n+              scope()->set_wrote_final();\n+              scope()->set_wrote_fields();\n+              bool need_membar = false;\n+              if (field->is_null_free() && inline_klass->is_initialized() && inline_klass->is_empty()) {\n+                apush(append(new Constant(new InstanceConstant(inline_klass->default_instance()))));\n+                if (has_pending_field_access()) {\n+                  set_pending_field_access(nullptr);\n+                } else if (has_pending_load_indexed()) {\n+                  set_pending_load_indexed(nullptr);\n+                }\n+              } else if (has_pending_load_indexed()) {\n+                assert(!needs_patching, \"Can't patch delayed field access\");\n+                pending_load_indexed()->update(field, offset - field->holder()->as_inline_klass()->payload_offset());\n+                NewInstance* vt = new NewInstance(inline_klass, pending_load_indexed()->state_before(), false, true);\n+                _memory->new_instance(vt);\n+                pending_load_indexed()->load_instr()->set_vt(vt);\n+                apush(append_split(vt));\n+                append(pending_load_indexed()->load_instr());\n+                set_pending_load_indexed(nullptr);\n+                need_membar = true;\n+              } else {\n+                if (has_pending_field_access()) {\n+                  state_before = pending_field_access()->state_before();\n+                }\n+                NewInstance* new_instance = new NewInstance(inline_klass, state_before, false, true);\n+                _memory->new_instance(new_instance);\n+                apush(append_split(new_instance));\n+                if (has_pending_field_access()) {\n+                  copy_inline_content(inline_klass, pending_field_access()->obj(),\n+                                      pending_field_access()->offset() + field->offset_in_bytes() - field->holder()->as_inline_klass()->payload_offset(),\n+                                      new_instance, inline_klass->payload_offset(), state_before);\n+                  set_pending_field_access(nullptr);\n+                } else {\n+                  copy_inline_content(inline_klass, obj, field->offset_in_bytes(), new_instance, inline_klass->payload_offset(), state_before);\n+                }\n+                need_membar = true;\n+              }\n+              if (need_membar) {\n+                \/\/ If we allocated a new instance ensure the stores to copy the\n+                \/\/ field contents are visible before any subsequent store that\n+                \/\/ publishes this reference.\n+                append(new MemBar(lir_membar_storestore));\n+              }\n+            }\n+          }\n@@ -1853,1 +2108,1 @@\n-      if (field->type()->basic_type() == T_BOOLEAN) {\n+      if (field_type == T_BOOLEAN) {\n@@ -1857,4 +2112,28 @@\n-      StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);\n-      if (!needs_patching) store = _memory->store(store);\n-      if (store != nullptr) {\n-        append(store);\n+      if (field->is_null_free() && field->type()->is_loaded() && field->type()->as_inline_klass()->is_empty()) {\n+        \/\/ Storing to a field of an empty, null-free inline type. Ignore.\n+        null_check(obj);\n+        null_check(val);\n+      } else if (!field->is_flat()) {\n+        if (field->is_null_free()) {\n+          null_check(val);\n+        }\n+        StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);\n+        if (!needs_patching) store = _memory->store(store);\n+        if (store != nullptr) {\n+          append(store);\n+        }\n+      } else {\n+        \/\/ Flat field\n+        assert(!needs_patching, \"Can't patch flat inline type field access\");\n+        ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+        bool is_naturally_atomic = inline_klass->nof_declared_nonstatic_fields() <= 1;\n+        bool needs_atomic_access = !field->is_null_free() || (field->is_volatile() && !is_naturally_atomic);\n+        if (needs_atomic_access) {\n+          if (field->is_null_free()) {\n+            null_check(val);\n+          }\n+          append(new StoreField(obj, offset, field, val, false, state_before, needs_patching));\n+        } else {\n+          assert(field->is_null_free(), \"must be null-free\");\n+          copy_inline_content(inline_klass, val, inline_klass->payload_offset(), obj, offset, state_before, field);\n+        }\n@@ -1870,1 +2149,0 @@\n-\n@@ -1987,1 +2265,1 @@\n-    } else if (bc_raw == Bytecodes::_invokespecial && !target->is_object_initializer() && calling_klass->is_interface()) {\n+    } else if (bc_raw == Bytecodes::_invokespecial && !target->is_object_constructor() && calling_klass->is_interface()) {\n@@ -2243,3 +2521,9 @@\n-  NewInstance* new_instance = new NewInstance(klass->as_instance_klass(), state_before, stream()->is_unresolved_klass());\n-  _memory->new_instance(new_instance);\n-  apush(append_split(new_instance));\n+  if (!stream()->is_unresolved_klass() && klass->is_inlinetype() &&\n+      klass->as_inline_klass()->is_initialized() && klass->as_inline_klass()->is_empty()) {\n+    ciInlineKlass* vk = klass->as_inline_klass();\n+    apush(append(new Constant(new InstanceConstant(vk->default_instance()))));\n+  } else {\n+    NewInstance* new_instance = new NewInstance(klass->as_instance_klass(), state_before, stream()->is_unresolved_klass(), false);\n+    _memory->new_instance(new_instance);\n+    apush(append_split(new_instance));\n+  }\n@@ -2248,1 +2532,0 @@\n-\n@@ -2321,0 +2604,19 @@\n+  bool maybe_inlinetype = false;\n+  if (bci == InvocationEntryBci) {\n+    \/\/ Called by GraphBuilder::inline_sync_entry.\n+#ifdef ASSERT\n+    ciType* obj_type = x->declared_type();\n+    assert(obj_type == nullptr || !obj_type->is_inlinetype(), \"inline types cannot have synchronized methods\");\n+#endif\n+  } else {\n+    \/\/ We are compiling a monitorenter bytecode\n+    if (EnableValhalla) {\n+      ciType* obj_type = x->declared_type();\n+      if (obj_type == nullptr || obj_type->as_klass()->can_be_inline_klass()) {\n+        \/\/ If we're (possibly) locking on an inline type, check for markWord::always_locked_pattern\n+        \/\/ and throw IMSE. (obj_type is null for Phi nodes, so let's just be conservative).\n+        maybe_inlinetype = true;\n+      }\n+    }\n+  }\n+\n@@ -2323,1 +2625,1 @@\n-  append_with_bci(new MonitorEnter(x, state()->lock(x), state_before), bci);\n+  append_with_bci(new MonitorEnter(x, state()->lock(x), state_before, maybe_inlinetype), bci);\n@@ -2471,0 +2773,1 @@\n+    if (value->is_null_free()) return;\n@@ -2496,1 +2799,3 @@\n-    assert(cur_bci == SynchronizationEntryBCI || cur_bci == cur_scope_data->stream()->cur_bci(), \"invalid bci\");\n+    assert(cur_bci == SynchronizationEntryBCI || cur_bci == cur_scope_data->stream()->cur_bci()\n+           || has_pending_field_access() || has_pending_load_indexed(), \"invalid bci\");\n+\n@@ -3278,1 +3583,2 @@\n-    state->store_local(idx, new Local(method()->holder(), objectType, idx, true));\n+    state->store_local(idx, new Local(method()->holder(), objectType, idx,\n+             \/*receiver*\/ true, \/*null_free*\/ method()->holder()->is_flat_array_klass()));\n@@ -3290,1 +3596,1 @@\n-    state->store_local(idx, new Local(type, vt, idx, false));\n+    state->store_local(idx, new Local(type, vt, idx, false, false));\n@@ -3310,0 +3616,2 @@\n+  , _pending_field_access(nullptr)\n+  , _pending_load_indexed(nullptr)\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":383,"deletions":75,"binary":false,"changes":458,"status":"modified"},{"patch":"@@ -174,0 +174,3 @@\n+  if (_should_reexecute) {\n+    return true;\n+  }\n@@ -183,1 +186,0 @@\n-\n@@ -217,1 +219,1 @@\n-void CodeEmitInfo::record_debug_info(DebugInformationRecorder* recorder, int pc_offset) {\n+void CodeEmitInfo::record_debug_info(DebugInformationRecorder* recorder, int pc_offset, bool maybe_return_as_fields) {\n@@ -221,1 +223,1 @@\n-  _scope_debug_info->record_debug_info(recorder, pc_offset, reexecute, _is_method_handle_invoke);\n+  _scope_debug_info->record_debug_info(recorder, pc_offset, reexecute, _is_method_handle_invoke, maybe_return_as_fields);\n","filename":"src\/hotspot\/share\/c1\/c1_IR.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -108,1 +110,1 @@\n-  ciType* t =  declared_type();\n+  ciType* t = declared_type();\n@@ -115,0 +117,60 @@\n+ciKlass* Instruction::as_loaded_klass_or_null() const {\n+  ciType* type = declared_type();\n+  if (type != nullptr && type->is_klass()) {\n+    ciKlass* klass = type->as_klass();\n+    if (klass->is_loaded()) {\n+      return klass;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+bool Instruction::is_loaded_flat_array() const {\n+  if (UseArrayFlattening) {\n+    ciType* type = declared_type();\n+    return type != nullptr && type->is_flat_array_klass();\n+  }\n+  return false;\n+}\n+\n+bool Instruction::maybe_flat_array() {\n+  if (UseArrayFlattening) {\n+    ciType* type = declared_type();\n+    if (type != nullptr) {\n+      if (type->is_obj_array_klass()) {\n+        \/\/ Due to array covariance, the runtime type might be a flat array.\n+        ciKlass* element_klass = type->as_obj_array_klass()->element_klass();\n+        if (element_klass->can_be_inline_klass() && (!element_klass->is_inlinetype() || element_klass->as_inline_klass()->flat_in_array())) {\n+          return true;\n+        }\n+      } else if (type->is_flat_array_klass()) {\n+        return true;\n+      } else if (type->is_klass() && type->as_klass()->is_java_lang_Object()) {\n+        \/\/ This can happen as a parameter to System.arraycopy()\n+        return true;\n+      }\n+    } else {\n+      \/\/ Type info gets lost during Phi merging (Phi, IfOp, etc), but we might be storing into a\n+      \/\/ flat array, so we should do a runtime check.\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool Instruction::maybe_null_free_array() {\n+  ciType* type = declared_type();\n+  if (type != nullptr) {\n+    if (type->is_obj_array_klass()) {\n+      \/\/ Due to array covariance, the runtime type might be a null-free array.\n+      if (type->as_obj_array_klass()->can_be_inline_array_klass()) {\n+        return true;\n+      }\n+    }\n+  } else {\n+    \/\/ Type info gets lost during Phi merging (Phi, IfOp, etc), but we might be storing into a\n+    \/\/ null-free array, so we should do a runtime check.\n+    return true;\n+  }\n+  return false;\n+}\n@@ -175,1 +237,1 @@\n-  if (array_type != nullptr) {\n+  if (delayed() == nullptr && array_type != nullptr) {\n@@ -189,1 +251,3 @@\n-\n+  if (delayed() != nullptr) {\n+    return delayed()->field()->type();\n+  }\n@@ -200,0 +264,14 @@\n+bool StoreIndexed::is_exact_flat_array_store() const {\n+  if (array()->is_loaded_flat_array() && value()->as_Constant() == nullptr && value()->declared_type() != nullptr) {\n+    ciKlass* element_klass = array()->declared_type()->as_flat_array_klass()->element_klass();\n+    ciKlass* actual_klass = value()->declared_type()->as_klass();\n+\n+    \/\/ The following check can fail with inlining:\n+    \/\/     void test45_inline(Object[] oa, Object o, int index) { oa[index] = o; }\n+    \/\/     void test45(MyValue1[] va, int index, MyValue2 v) { test45_inline(va, v, index); }\n+    if (element_klass == actual_klass) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n@@ -211,1 +289,5 @@\n-  return ciObjArrayKlass::make(klass());\n+  return ciArrayKlass::make(klass());\n+}\n+\n+ciType* NewMultiArray::exact_type() const {\n+  return _klass;\n@@ -321,0 +403,26 @@\n+StoreField::StoreField(Value obj, int offset, ciField* field, Value value, bool is_static,\n+                       ValueStack* state_before, bool needs_patching)\n+  : AccessField(obj, offset, field, is_static, state_before, needs_patching)\n+  , _value(value)\n+  , _enclosing_field(nullptr)\n+{\n+#ifdef ASSERT\n+  AssertValues assert_value;\n+  values_do(&assert_value);\n+#endif\n+  pin();\n+}\n+\n+StoreIndexed::StoreIndexed(Value array, Value index, Value length, BasicType elt_type, Value value,\n+                           ValueStack* state_before, bool check_boolean, bool mismatched)\n+  : AccessIndexed(array, index, length, elt_type, state_before, mismatched)\n+  , _value(value), _check_boolean(check_boolean)\n+{\n+#ifdef ASSERT\n+  AssertValues assert_value;\n+  values_do(&assert_value);\n+#endif\n+  pin();\n+}\n+\n+\n@@ -347,1 +455,2 @@\n-    ValueType* t = argument_at(i)->type();\n+    Value v = argument_at(i);\n+    ValueType* t = v->type();\n@@ -992,0 +1101,1 @@\n+\n","filename":"src\/hotspot\/share\/c1\/c1_Instruction.cpp","additions":115,"deletions":5,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -383,1 +384,6 @@\n-  output()->print(\" (%c)\", type2char(x->elt_type()));\n+  if (x->delayed() != nullptr) {\n+    output()->print(\" +%d\", x->delayed()->offset());\n+    output()->print(\" (%c)\", type2char(x->delayed()->field()->type()->basic_type()));\n+  } else {\n+    output()->print(\" (%c)\", type2char(x->elt_type()));\n+  }\n@@ -497,1 +503,0 @@\n-\n@@ -517,1 +522,0 @@\n-\n@@ -854,0 +858,1 @@\n+\n@@ -861,0 +866,7 @@\n+void InstructionPrinter::do_ProfileACmpTypes(ProfileACmpTypes* x) {\n+  output()->print(\"profile acmp types \");\n+  print_value(x->left());\n+  output()->print(\", \");\n+  print_value(x->right());\n+}\n+\n","filename":"src\/hotspot\/share\/c1\/c1_InstructionPrinter.cpp","additions":15,"deletions":3,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -289,1 +290,1 @@\n-                                 CodeStub* stub)\n+                                 CodeStub* stub, bool need_null_check)\n@@ -305,0 +306,1 @@\n+  , _need_null_check(need_null_check)\n@@ -332,0 +334,1 @@\n+  , _need_null_check(true)\n@@ -341,0 +344,31 @@\n+LIR_OpFlattenedArrayCheck::LIR_OpFlattenedArrayCheck(LIR_Opr array, LIR_Opr value, LIR_Opr tmp, CodeStub* stub)\n+  : LIR_Op(lir_flat_array_check, LIR_OprFact::illegalOpr, nullptr)\n+  , _array(array)\n+  , _value(value)\n+  , _tmp(tmp)\n+  , _stub(stub) {}\n+\n+\n+LIR_OpNullFreeArrayCheck::LIR_OpNullFreeArrayCheck(LIR_Opr array, LIR_Opr tmp)\n+  : LIR_Op(lir_null_free_array_check, LIR_OprFact::illegalOpr, nullptr)\n+  , _array(array)\n+  , _tmp(tmp) {}\n+\n+\n+LIR_OpSubstitutabilityCheck::LIR_OpSubstitutabilityCheck(LIR_Opr result, LIR_Opr left, LIR_Opr right, LIR_Opr equal_result, LIR_Opr not_equal_result,\n+                                                         LIR_Opr tmp1, LIR_Opr tmp2,\n+                                                         ciKlass* left_klass, ciKlass* right_klass, LIR_Opr left_klass_op, LIR_Opr right_klass_op,\n+                                                         CodeEmitInfo* info, CodeStub* stub)\n+  : LIR_Op(lir_substitutability_check, result, info)\n+  , _left(left)\n+  , _right(right)\n+  , _equal_result(equal_result)\n+  , _not_equal_result(not_equal_result)\n+  , _tmp1(tmp1)\n+  , _tmp2(tmp2)\n+  , _left_klass(left_klass)\n+  , _right_klass(right_klass)\n+  , _left_klass_op(left_klass_op)\n+  , _right_klass_op(right_klass_op)\n+  , _stub(stub) {}\n+\n@@ -415,0 +449,1 @@\n+    case lir_check_orig_pc:            \/\/ result and info always invalid\n@@ -814,0 +849,1 @@\n+      do_stub(opLock->_throw_ie_stub);\n@@ -850,0 +886,47 @@\n+\/\/ LIR_OpFlattenedArrayCheck\n+    case lir_flat_array_check: {\n+      assert(op->as_OpFlattenedArrayCheck() != nullptr, \"must be\");\n+      LIR_OpFlattenedArrayCheck* opFlattenedArrayCheck = (LIR_OpFlattenedArrayCheck*)op;\n+\n+      if (opFlattenedArrayCheck->_array->is_valid()) do_input(opFlattenedArrayCheck->_array);\n+      if (opFlattenedArrayCheck->_value->is_valid()) do_input(opFlattenedArrayCheck->_value);\n+      if (opFlattenedArrayCheck->_tmp->is_valid())   do_temp(opFlattenedArrayCheck->_tmp);\n+\n+      do_stub(opFlattenedArrayCheck->_stub);\n+\n+      break;\n+    }\n+\n+\/\/ LIR_OpNullFreeArrayCheck\n+    case lir_null_free_array_check: {\n+      assert(op->as_OpNullFreeArrayCheck() != nullptr, \"must be\");\n+      LIR_OpNullFreeArrayCheck* opNullFreeArrayCheck = (LIR_OpNullFreeArrayCheck*)op;\n+\n+      if (opNullFreeArrayCheck->_array->is_valid()) do_input(opNullFreeArrayCheck->_array);\n+      if (opNullFreeArrayCheck->_tmp->is_valid())   do_temp(opNullFreeArrayCheck->_tmp);\n+      break;\n+    }\n+\n+\/\/ LIR_OpSubstitutabilityCheck\n+    case lir_substitutability_check: {\n+      assert(op->as_OpSubstitutabilityCheck() != nullptr, \"must be\");\n+      LIR_OpSubstitutabilityCheck* opSubstitutabilityCheck = (LIR_OpSubstitutabilityCheck*)op;\n+                                                                do_input(opSubstitutabilityCheck->_left);\n+                                                                do_temp (opSubstitutabilityCheck->_left);\n+                                                                do_input(opSubstitutabilityCheck->_right);\n+                                                                do_temp (opSubstitutabilityCheck->_right);\n+                                                                do_input(opSubstitutabilityCheck->_equal_result);\n+                                                                do_temp (opSubstitutabilityCheck->_equal_result);\n+                                                                do_input(opSubstitutabilityCheck->_not_equal_result);\n+                                                                do_temp (opSubstitutabilityCheck->_not_equal_result);\n+      if (opSubstitutabilityCheck->_tmp1->is_valid())           do_temp(opSubstitutabilityCheck->_tmp1);\n+      if (opSubstitutabilityCheck->_tmp2->is_valid())           do_temp(opSubstitutabilityCheck->_tmp2);\n+      if (opSubstitutabilityCheck->_left_klass_op->is_valid())  do_temp(opSubstitutabilityCheck->_left_klass_op);\n+      if (opSubstitutabilityCheck->_right_klass_op->is_valid()) do_temp(opSubstitutabilityCheck->_right_klass_op);\n+      if (opSubstitutabilityCheck->_result->is_valid())         do_output(opSubstitutabilityCheck->_result);\n+\n+      do_info(opSubstitutabilityCheck->_info);\n+      do_stub(opSubstitutabilityCheck->_stub);\n+      break;\n+    }\n+\n@@ -927,1 +1010,12 @@\n-  default:\n+\n+    \/\/ LIR_OpProfileInlineType:\n+    case lir_profile_inline_type: {\n+      assert(op->as_OpProfileInlineType() != nullptr, \"must be\");\n+      LIR_OpProfileInlineType* opProfileInlineType = (LIR_OpProfileInlineType*)op;\n+\n+      do_input(opProfileInlineType->_mdp); do_temp(opProfileInlineType->_mdp);\n+      do_input(opProfileInlineType->_obj);\n+      do_temp(opProfileInlineType->_tmp);\n+      break;\n+    }\n+default:\n@@ -1000,0 +1094,28 @@\n+bool LIR_OpJavaCall::maybe_return_as_fields(ciInlineKlass** vk_ret) const {\n+  ciType* return_type = method()->return_type();\n+  if (InlineTypeReturnedAsFields) {\n+    if (return_type->is_inlinetype()) {\n+      ciInlineKlass* vk = return_type->as_inline_klass();\n+      if (vk->can_be_returned_as_fields()) {\n+        if (vk_ret != nullptr) {\n+          *vk_ret = vk;\n+        }\n+        return true;\n+      }\n+    } else if (return_type->is_instance_klass() &&\n+               (method()->is_method_handle_intrinsic() || !return_type->is_loaded() ||\n+                StressCallingConvention)) {\n+      \/\/ An inline type might be returned from the call but we don't know its type.\n+      \/\/ This can happen with method handle intrinsics or when the return type is\n+      \/\/ not loaded (method holder is not loaded or preload attribute is missing).\n+      \/\/ If an inline type is returned, we either get an oop to a buffer and nothing\n+      \/\/ needs to be done or one of the values being returned is the klass of the\n+      \/\/ inline type (RAX on x64, with LSB set to 1) and we need to allocate an inline\n+      \/\/ type instance of that type and initialize it with the fields values being\n+      \/\/ returned in other registers.\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -1063,0 +1185,18 @@\n+void LIR_OpFlattenedArrayCheck::emit_code(LIR_Assembler* masm) {\n+  masm->emit_opFlattenedArrayCheck(this);\n+  if (stub() != nullptr) {\n+    masm->append_code_stub(stub());\n+  }\n+}\n+\n+void LIR_OpNullFreeArrayCheck::emit_code(LIR_Assembler* masm) {\n+  masm->emit_opNullFreeArrayCheck(this);\n+}\n+\n+void LIR_OpSubstitutabilityCheck::emit_code(LIR_Assembler* masm) {\n+  masm->emit_opSubstitutabilityCheck(this);\n+  if (stub() != nullptr) {\n+    masm->append_code_stub(stub());\n+  }\n+}\n+\n@@ -1080,0 +1220,3 @@\n+  if (throw_ie_stub()) {\n+    masm->append_code_stub(throw_ie_stub());\n+  }\n@@ -1104,0 +1247,4 @@\n+void LIR_OpProfileInlineType::emit_code(LIR_Assembler* masm) {\n+  masm->emit_profile_inline_type(this);\n+}\n+\n@@ -1379,1 +1526,1 @@\n-void LIR_List::allocate_array(LIR_Opr dst, LIR_Opr len, LIR_Opr t1,LIR_Opr t2, LIR_Opr t3,LIR_Opr t4, BasicType type, LIR_Opr klass, CodeStub* stub, bool zero_array) {\n+void LIR_List::allocate_array(LIR_Opr dst, LIR_Opr len, LIR_Opr t1,LIR_Opr t2, LIR_Opr t3,LIR_Opr t4, BasicType type, LIR_Opr klass, CodeStub* stub, bool zero_array, bool is_null_free) {\n@@ -1390,1 +1537,2 @@\n-                           zero_array));\n+                           zero_array,\n+                           is_null_free));\n@@ -1428,1 +1576,1 @@\n-void LIR_List::lock_object(LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info) {\n+void LIR_List::lock_object(LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info, CodeStub* throw_ie_stub) {\n@@ -1436,1 +1584,2 @@\n-                    info));\n+                    info,\n+                    throw_ie_stub));\n@@ -1461,1 +1610,4 @@\n-                          ciMethod* profiled_method, int profiled_bci) {\n+                          ciMethod* profiled_method, int profiled_bci, bool is_null_free) {\n+  \/\/ If klass is non-nullable,  LIRGenerator::do_CheckCast has already performed null-check\n+  \/\/ on the object.\n+  bool need_null_check = !is_null_free;\n@@ -1463,1 +1615,2 @@\n-                                           tmp1, tmp2, tmp3, fast_check, info_for_exception, info_for_patch, stub);\n+                                           tmp1, tmp2, tmp3, fast_check, info_for_exception, info_for_patch, stub,\n+                                           need_null_check);\n@@ -1485,0 +1638,1 @@\n+  \/\/ FIXME -- if the types of the array and\/or the object are known statically, we can avoid loading the klass\n@@ -1506,0 +1660,21 @@\n+void LIR_List::check_flat_array(LIR_Opr array, LIR_Opr value, LIR_Opr tmp, CodeStub* stub) {\n+  LIR_OpFlattenedArrayCheck* c = new LIR_OpFlattenedArrayCheck(array, value, tmp, stub);\n+  append(c);\n+}\n+\n+void LIR_List::check_null_free_array(LIR_Opr array, LIR_Opr tmp) {\n+  LIR_OpNullFreeArrayCheck* c = new LIR_OpNullFreeArrayCheck(array, tmp);\n+  append(c);\n+}\n+\n+void LIR_List::substitutability_check(LIR_Opr result, LIR_Opr left, LIR_Opr right, LIR_Opr equal_result, LIR_Opr not_equal_result,\n+                                      LIR_Opr tmp1, LIR_Opr tmp2,\n+                                      ciKlass* left_klass, ciKlass* right_klass, LIR_Opr left_klass_op, LIR_Opr right_klass_op,\n+                                      CodeEmitInfo* info, CodeStub* stub) {\n+  LIR_OpSubstitutabilityCheck* c = new LIR_OpSubstitutabilityCheck(result, left, right, equal_result, not_equal_result,\n+                                                                   tmp1, tmp2,\n+                                                                   left_klass, right_klass, left_klass_op, right_klass_op,\n+                                                                   info, stub);\n+  append(c);\n+}\n+\n@@ -1722,0 +1897,1 @@\n+     case lir_check_orig_pc:         s = \"check_orig_pc\"; break;\n@@ -1790,0 +1966,6 @@\n+     \/\/ LIR_OpFlattenedArrayCheck\n+     case lir_flat_array_check:      s = \"flat_array_check\"; break;\n+     \/\/ LIR_OpNullFreeArrayCheck\n+     case lir_null_free_array_check: s = \"null_free_array_check\"; break;\n+     \/\/ LIR_OpSubstitutabilityCheck\n+     case lir_substitutability_check: s = \"substitutability_check\"; break;\n@@ -1798,0 +1980,2 @@\n+     \/\/ LIR_OpProfileInlineType\n+     case lir_profile_inline_type:   s = \"profile_inline_type\"; break;\n@@ -2029,0 +2213,38 @@\n+void LIR_OpFlattenedArrayCheck::print_instr(outputStream* out) const {\n+  array()->print(out);                   out->print(\" \");\n+  value()->print(out);                   out->print(\" \");\n+  tmp()->print(out);                     out->print(\" \");\n+  if (stub() != nullptr) {\n+    out->print(\"[label:\" INTPTR_FORMAT \"]\", p2i(stub()->entry()));\n+  }\n+}\n+\n+void LIR_OpNullFreeArrayCheck::print_instr(outputStream* out) const {\n+  array()->print(out);                   out->print(\" \");\n+  tmp()->print(out);                     out->print(\" \");\n+}\n+\n+void LIR_OpSubstitutabilityCheck::print_instr(outputStream* out) const {\n+  result_opr()->print(out);              out->print(\" \");\n+  left()->print(out);                    out->print(\" \");\n+  right()->print(out);                   out->print(\" \");\n+  equal_result()->print(out);            out->print(\" \");\n+  not_equal_result()->print(out);        out->print(\" \");\n+  tmp1()->print(out);                    out->print(\" \");\n+  tmp2()->print(out);                    out->print(\" \");\n+  if (left_klass() == nullptr) {\n+    out->print(\"unknown \");\n+  } else {\n+    left_klass()->print(out);            out->print(\" \");\n+  }\n+  if (right_klass() == nullptr) {\n+    out->print(\"unknown \");\n+  } else {\n+    right_klass()->print(out);           out->print(\" \");\n+  }\n+  left_klass_op()->print(out);           out->print(\" \");\n+  right_klass_op()->print(out);          out->print(\" \");\n+  if (stub() != nullptr) {\n+    out->print(\"[label:\" INTPTR_FORMAT \"]\", p2i(stub()->entry()));\n+  }\n+}\n@@ -2104,0 +2326,8 @@\n+\/\/ LIR_OpProfileInlineType\n+void LIR_OpProfileInlineType::print_instr(outputStream* out) const {\n+  out->print(\" flag = %x \", flag());\n+  mdp()->print(out);          out->print(\" \");\n+  obj()->print(out);          out->print(\" \");\n+  tmp()->print(out);          out->print(\" \");\n+}\n+\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":238,"deletions":8,"binary":false,"changes":246,"status":"modified"},{"patch":"@@ -900,0 +900,3 @@\n+class    LIR_OpFlattenedArrayCheck;\n+class    LIR_OpNullFreeArrayCheck;\n+class    LIR_OpSubstitutabilityCheck;\n@@ -904,0 +907,1 @@\n+class    LIR_OpProfileInlineType;\n@@ -928,0 +932,1 @@\n+      , lir_check_orig_pc\n@@ -1007,0 +1012,9 @@\n+  , begin_opFlattenedArrayCheck\n+    , lir_flat_array_check\n+  , end_opFlattenedArrayCheck\n+  , begin_opNullFreeArrayCheck\n+    , lir_null_free_array_check\n+  , end_opNullFreeArrayCheck\n+  , begin_opSubstitutabilityCheck\n+    , lir_substitutability_check\n+  , end_opSubstitutabilityCheck\n@@ -1015,0 +1029,1 @@\n+    , lir_profile_inline_type\n@@ -1163,0 +1178,3 @@\n+  virtual LIR_OpFlattenedArrayCheck* as_OpFlattenedArrayCheck() { return nullptr; }\n+  virtual LIR_OpNullFreeArrayCheck* as_OpNullFreeArrayCheck() { return nullptr; }\n+  virtual LIR_OpSubstitutabilityCheck* as_OpSubstitutabilityCheck() { return nullptr; }\n@@ -1167,0 +1185,1 @@\n+  virtual LIR_OpProfileInlineType* as_OpProfileInlineType() { return nullptr; }\n@@ -1239,0 +1258,2 @@\n+\n+  bool maybe_return_as_fields(ciInlineKlass** vk = nullptr) const;\n@@ -1290,1 +1311,4 @@\n-    all_flags              = (1 << 12) - 1\n+    always_slow_path       = 1 << 12,\n+    src_inlinetype_check   = 1 << 13,\n+    dst_inlinetype_check   = 1 << 14,\n+    all_flags              = (1 << 15) - 1\n@@ -1565,0 +1589,1 @@\n+  bool          _need_null_check;\n@@ -1569,1 +1594,1 @@\n-                  CodeEmitInfo* info_for_exception, CodeEmitInfo* info_for_patch, CodeStub* stub);\n+                  CodeEmitInfo* info_for_exception, CodeEmitInfo* info_for_patch, CodeStub* stub, bool need_null_check = true);\n@@ -1591,1 +1616,1 @@\n-\n+  bool      need_null_check() const              { return _need_null_check;   }\n@@ -1598,0 +1623,76 @@\n+\/\/ LIR_OpFlattenedArrayCheck\n+class LIR_OpFlattenedArrayCheck: public LIR_Op {\n+ friend class LIR_OpVisitState;\n+\n+ private:\n+  LIR_Opr       _array;\n+  LIR_Opr       _value;\n+  LIR_Opr       _tmp;\n+  CodeStub*     _stub;\n+public:\n+  LIR_OpFlattenedArrayCheck(LIR_Opr array, LIR_Opr value, LIR_Opr tmp, CodeStub* stub);\n+  LIR_Opr array() const                          { return _array;         }\n+  LIR_Opr value() const                          { return _value;         }\n+  LIR_Opr tmp() const                            { return _tmp;           }\n+  CodeStub* stub() const                         { return _stub;          }\n+\n+  virtual void emit_code(LIR_Assembler* masm);\n+  virtual LIR_OpFlattenedArrayCheck* as_OpFlattenedArrayCheck() { return this; }\n+  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n+\n+\/\/ LIR_OpNullFreeArrayCheck\n+class LIR_OpNullFreeArrayCheck: public LIR_Op {\n+ friend class LIR_OpVisitState;\n+\n+ private:\n+  LIR_Opr       _array;\n+  LIR_Opr       _tmp;\n+public:\n+  LIR_OpNullFreeArrayCheck(LIR_Opr array, LIR_Opr tmp);\n+  LIR_Opr array() const                          { return _array;         }\n+  LIR_Opr tmp() const                            { return _tmp;           }\n+\n+  virtual void emit_code(LIR_Assembler* masm);\n+  virtual LIR_OpNullFreeArrayCheck* as_OpNullFreeArrayCheck() { return this; }\n+  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n+\n+class LIR_OpSubstitutabilityCheck: public LIR_Op {\n+ friend class LIR_OpVisitState;\n+\n+ private:\n+  LIR_Opr       _left;\n+  LIR_Opr       _right;\n+  LIR_Opr       _equal_result;\n+  LIR_Opr       _not_equal_result;\n+  LIR_Opr       _tmp1;\n+  LIR_Opr       _tmp2;\n+  ciKlass*      _left_klass;\n+  ciKlass*      _right_klass;\n+  LIR_Opr       _left_klass_op;\n+  LIR_Opr       _right_klass_op;\n+  CodeStub*     _stub;\n+public:\n+  LIR_OpSubstitutabilityCheck(LIR_Opr result, LIR_Opr left, LIR_Opr right, LIR_Opr equal_result, LIR_Opr not_equal_result,\n+                              LIR_Opr tmp1, LIR_Opr tmp2,\n+                              ciKlass* left_klass, ciKlass* right_klass, LIR_Opr left_klass_op, LIR_Opr right_klass_op,\n+                              CodeEmitInfo* info, CodeStub* stub);\n+\n+  LIR_Opr left() const             { return _left; }\n+  LIR_Opr right() const            { return _right; }\n+  LIR_Opr equal_result() const     { return _equal_result; }\n+  LIR_Opr not_equal_result() const { return _not_equal_result; }\n+  LIR_Opr tmp1() const             { return _tmp1; }\n+  LIR_Opr tmp2() const             { return _tmp2; }\n+  ciKlass* left_klass() const      { return _left_klass; }\n+  ciKlass* right_klass() const     { return _right_klass; }\n+  LIR_Opr left_klass_op() const    { return _left_klass_op; }\n+  LIR_Opr right_klass_op() const   { return _right_klass_op; }\n+  CodeStub* stub() const           { return _stub; }\n+\n+  virtual void emit_code(LIR_Assembler* masm);\n+  virtual LIR_OpSubstitutabilityCheck* as_OpSubstitutabilityCheck() { return this; }\n+  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n+\n@@ -1756,0 +1857,1 @@\n+  bool      _is_null_free;\n@@ -1758,1 +1860,1 @@\n-  LIR_OpAllocArray(LIR_Opr klass, LIR_Opr len, LIR_Opr result, LIR_Opr t1, LIR_Opr t2, LIR_Opr t3, LIR_Opr t4, BasicType type, CodeStub* stub, bool zero_array)\n+  LIR_OpAllocArray(LIR_Opr klass, LIR_Opr len, LIR_Opr result, LIR_Opr t1, LIR_Opr t2, LIR_Opr t3, LIR_Opr t4, BasicType type, CodeStub* stub, bool zero_array, bool is_null_free)\n@@ -1768,1 +1870,2 @@\n-    , _zero_array(zero_array) {}\n+    , _zero_array(zero_array)\n+    , _is_null_free(is_null_free) {}\n@@ -1779,1 +1882,2 @@\n-  bool zero_array()   const                      { return _zero_array;  }\n+  bool      zero_array()   const                 { return _zero_array;  }\n+  bool      is_null_free() const                 { return _is_null_free;}\n@@ -1886,0 +1990,1 @@\n+  CodeStub* _throw_ie_stub;\n@@ -1887,1 +1992,1 @@\n-  LIR_OpLock(LIR_Code code, LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info)\n+  LIR_OpLock(LIR_Code code, LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info, CodeStub* throw_ie_stub=nullptr)\n@@ -1893,1 +1998,2 @@\n-    , _stub(stub)                      {}\n+    , _stub(stub)\n+    , _throw_ie_stub(throw_ie_stub)                    {}\n@@ -1900,0 +2006,1 @@\n+  CodeStub* throw_ie_stub() const                { return _throw_ie_stub; }\n@@ -2084,0 +2191,32 @@\n+\/\/ LIR_OpProfileInlineType\n+class LIR_OpProfileInlineType : public LIR_Op {\n+ friend class LIR_OpVisitState;\n+\n+ private:\n+  LIR_Opr      _mdp;\n+  LIR_Opr      _obj;\n+  int          _flag;\n+  LIR_Opr      _tmp;\n+  bool         _not_null;      \/\/ true if we know statically that _obj cannot be null\n+\n+ public:\n+  \/\/ Destroys recv\n+  LIR_OpProfileInlineType(LIR_Opr mdp, LIR_Opr obj, int flag, LIR_Opr tmp, bool not_null)\n+    : LIR_Op(lir_profile_inline_type, LIR_OprFact::illegalOpr, nullptr)  \/\/ no result, no info\n+    , _mdp(mdp)\n+    , _obj(obj)\n+    , _flag(flag)\n+    , _tmp(tmp)\n+    , _not_null(not_null) { }\n+\n+  LIR_Opr      mdp()              const             { return _mdp;              }\n+  LIR_Opr      obj()              const             { return _obj;              }\n+  int          flag()             const             { return _flag;             }\n+  LIR_Opr      tmp()              const             { return _tmp;              }\n+  bool         not_null()         const             { return _not_null;         }\n+\n+  virtual void emit_code(LIR_Assembler* masm);\n+  virtual LIR_OpProfileInlineType* as_OpProfileInlineType() { return this; }\n+  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n+\n@@ -2309,1 +2448,1 @@\n-  void allocate_array(LIR_Opr dst, LIR_Opr len, LIR_Opr t1,LIR_Opr t2, LIR_Opr t3,LIR_Opr t4, BasicType type, LIR_Opr klass, CodeStub* stub, bool zero_array = true);\n+  void allocate_array(LIR_Opr dst, LIR_Opr len, LIR_Opr t1,LIR_Opr t2, LIR_Opr t3,LIR_Opr t4, BasicType type, LIR_Opr klass, CodeStub* stub, bool zero_array = true, bool is_null_free = false);\n@@ -2356,1 +2495,1 @@\n-  void lock_object(LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info);\n+  void lock_object(LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info, CodeStub* throw_ie_stub=nullptr);\n@@ -2366,0 +2505,6 @@\n+  void check_flat_array(LIR_Opr array, LIR_Opr value, LIR_Opr tmp, CodeStub* stub);\n+  void check_null_free_array(LIR_Opr array, LIR_Opr tmp);\n+  void substitutability_check(LIR_Opr result, LIR_Opr left, LIR_Opr right, LIR_Opr equal_result, LIR_Opr not_equal_result,\n+                              LIR_Opr tmp1, LIR_Opr tmp2,\n+                              ciKlass* left_klass, ciKlass* right_klass, LIR_Opr left_klass_op, LIR_Opr right_klass_op,\n+                              CodeEmitInfo* info, CodeStub* stub);\n@@ -2370,1 +2515,1 @@\n-                  ciMethod* profiled_method, int profiled_bci);\n+                  ciMethod* profiled_method, int profiled_bci, bool is_null_free);\n@@ -2378,0 +2523,3 @@\n+  void profile_inline_type(LIR_Address* mdp, LIR_Opr obj, int flag, LIR_Opr tmp, bool not_null) {\n+    append(new LIR_OpProfileInlineType(LIR_OprFact::address(mdp), obj, flag, tmp, not_null));\n+  }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":159,"deletions":11,"binary":false,"changes":170,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -120,0 +122,1 @@\n+  _verified_inline_entry.reset();\n@@ -331,1 +334,0 @@\n-\n@@ -341,2 +343,1 @@\n-\n-void LIR_Assembler::add_call_info(int pc_offset, CodeEmitInfo* cinfo) {\n+void LIR_Assembler::add_call_info(int pc_offset, CodeEmitInfo* cinfo, bool maybe_return_as_fields) {\n@@ -344,1 +345,1 @@\n-  cinfo->record_debug_info(compilation()->debug_info_recorder(), pc_offset);\n+  cinfo->record_debug_info(compilation()->debug_info_recorder(), pc_offset, maybe_return_as_fields);\n@@ -489,0 +490,6 @@\n+  ciInlineKlass* vk = nullptr;\n+  if (op->maybe_return_as_fields(&vk)) {\n+    int offset = store_inline_type_fields_to_buf(vk);\n+    add_call_info(offset, op->info(), true);\n+  }\n+\n@@ -606,0 +613,135 @@\n+void LIR_Assembler::add_scalarized_entry_info(int pc_offset) {\n+  flush_debug_info(pc_offset);\n+  DebugInformationRecorder* debug_info = compilation()->debug_info_recorder();\n+  \/\/ The VEP and VIEP(RO) of a C1-compiled method call buffer_inline_args_xxx()\n+  \/\/ before doing any argument shuffling. This call may cause GC. When GC happens,\n+  \/\/ all the parameters are still as passed by the caller, so we just use\n+  \/\/ map->set_include_argument_oops() inside frame::sender_for_compiled_frame(RegisterMap* map).\n+  \/\/ There's no need to build a GC map here.\n+  OopMap* oop_map = new OopMap(0, 0);\n+  debug_info->add_safepoint(pc_offset, oop_map);\n+  DebugToken* locvals = debug_info->create_scope_values(nullptr); \/\/ FIXME is this needed (for Java debugging to work properly??)\n+  DebugToken* expvals = debug_info->create_scope_values(nullptr); \/\/ FIXME is this needed (for Java debugging to work properly??)\n+  DebugToken* monvals = debug_info->create_monitor_values(nullptr); \/\/ FIXME: need testing with synchronized method\n+  bool reexecute = false;\n+  bool return_oop = false; \/\/ This flag will be ignored since it used only for C2 with escape analysis.\n+  bool rethrow_exception = false;\n+  bool is_method_handle_invoke = false;\n+  debug_info->describe_scope(pc_offset, methodHandle(), method(), 0, reexecute, rethrow_exception, is_method_handle_invoke, return_oop, false, locvals, expvals, monvals);\n+  debug_info->end_safepoint(pc_offset);\n+}\n+\n+\/\/ The entries points of C1-compiled methods can have the following types:\n+\/\/ (1) Methods with no inline type args\n+\/\/ (2) Methods with inline type receiver but no inline type args\n+\/\/     VIEP_RO is the same as VIEP\n+\/\/ (3) Methods with non-inline type receiver and some inline type args\n+\/\/     VIEP_RO is the same as VEP\n+\/\/ (4) Methods with inline type receiver and other inline type args\n+\/\/     Separate VEP, VIEP and VIEP_RO\n+\/\/\n+\/\/ (1)               (2)                 (3)                    (4)\n+\/\/ UEP\/UIEP:         VEP:                UEP:                   UEP:\n+\/\/   check_icache      pack receiver       check_icache           check_icache\n+\/\/ VEP\/VIEP\/VIEP_RO    jump to VIEP      VEP\/VIEP_RO:           VIEP_RO:\n+\/\/   body            UEP\/UIEP:             pack inline args       pack inline args (except receiver)\n+\/\/                     check_icache        jump to VIEP           jump to VIEP\n+\/\/                   VIEP\/VIEP_RO        UIEP:                  VEP:\n+\/\/                     body                check_icache           pack all inline args\n+\/\/                                       VIEP:                    jump to VIEP\n+\/\/                                         body                 UIEP:\n+\/\/                                                                check_icache\n+\/\/                                                              VIEP:\n+\/\/                                                                body\n+void LIR_Assembler::emit_std_entries() {\n+  offsets()->set_value(CodeOffsets::OSR_Entry, _masm->offset());\n+\n+  _masm->align(CodeEntryAlignment);\n+  const CompiledEntrySignature* ces = compilation()->compiled_entry_signature();\n+  if (ces->has_scalarized_args()) {\n+    assert(InlineTypePassFieldsAsArgs && method()->get_Method()->has_scalarized_args(), \"must be\");\n+    CodeOffsets::Entries ro_entry_type = ces->c1_inline_ro_entry_type();\n+\n+    \/\/ UEP: check icache and fall-through\n+    if (ro_entry_type != CodeOffsets::Verified_Inline_Entry) {\n+      offsets()->set_value(CodeOffsets::Entry, _masm->offset());\n+      if (needs_icache(method())) {\n+        check_icache();\n+      }\n+    }\n+\n+    \/\/ VIEP_RO: pack all value parameters, except the receiver\n+    if (ro_entry_type == CodeOffsets::Verified_Inline_Entry_RO) {\n+      emit_std_entry(CodeOffsets::Verified_Inline_Entry_RO, ces);\n+    }\n+\n+    \/\/ VEP: pack all value parameters\n+    _masm->align(CodeEntryAlignment);\n+    emit_std_entry(CodeOffsets::Verified_Entry, ces);\n+\n+    \/\/ UIEP: check icache and fall-through\n+    _masm->align(CodeEntryAlignment);\n+    offsets()->set_value(CodeOffsets::Inline_Entry, _masm->offset());\n+    if (ro_entry_type == CodeOffsets::Verified_Inline_Entry) {\n+      \/\/ Special case if we have VIEP == VIEP(RO):\n+      \/\/ this means UIEP (called by C1) == UEP (called by C2).\n+      offsets()->set_value(CodeOffsets::Entry, _masm->offset());\n+    }\n+    if (needs_icache(method())) {\n+      check_icache();\n+    }\n+\n+    \/\/ VIEP: all value parameters are passed as refs - no packing.\n+    emit_std_entry(CodeOffsets::Verified_Inline_Entry, nullptr);\n+\n+    if (ro_entry_type != CodeOffsets::Verified_Inline_Entry_RO) {\n+      \/\/ The VIEP(RO) is the same as VEP or VIEP\n+      assert(ro_entry_type == CodeOffsets::Verified_Entry ||\n+             ro_entry_type == CodeOffsets::Verified_Inline_Entry, \"must be\");\n+      offsets()->set_value(CodeOffsets::Verified_Inline_Entry_RO,\n+                           offsets()->value(ro_entry_type));\n+    }\n+  } else {\n+    \/\/ All 3 entries are the same (no inline type packing)\n+    offsets()->set_value(CodeOffsets::Entry, _masm->offset());\n+    offsets()->set_value(CodeOffsets::Inline_Entry, _masm->offset());\n+    if (needs_icache(method())) {\n+      check_icache();\n+    }\n+    emit_std_entry(CodeOffsets::Verified_Inline_Entry, nullptr);\n+    offsets()->set_value(CodeOffsets::Verified_Entry, offsets()->value(CodeOffsets::Verified_Inline_Entry));\n+    offsets()->set_value(CodeOffsets::Verified_Inline_Entry_RO, offsets()->value(CodeOffsets::Verified_Inline_Entry));\n+  }\n+}\n+\n+void LIR_Assembler::emit_std_entry(CodeOffsets::Entries entry, const CompiledEntrySignature* ces) {\n+  offsets()->set_value(entry, _masm->offset());\n+  _masm->verified_entry(compilation()->directive()->BreakAtExecuteOption);\n+  switch (entry) {\n+  case CodeOffsets::Verified_Entry: {\n+    if (needs_clinit_barrier_on_entry(method())) {\n+      clinit_barrier(method());\n+    }\n+    int rt_call_offset = _masm->verified_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()->sp_offset_for_orig_pc()), _verified_inline_entry);\n+    add_scalarized_entry_info(rt_call_offset);\n+    break;\n+  }\n+  case CodeOffsets::Verified_Inline_Entry_RO: {\n+    assert(!needs_clinit_barrier_on_entry(method()), \"can't be static\");\n+    int rt_call_offset = _masm->verified_inline_ro_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()->sp_offset_for_orig_pc()), _verified_inline_entry);\n+    add_scalarized_entry_info(rt_call_offset);\n+    break;\n+  }\n+  case CodeOffsets::Verified_Inline_Entry: {\n+    if (needs_clinit_barrier_on_entry(method())) {\n+      clinit_barrier(method());\n+    }\n+    build_frame();\n+    offsets()->set_value(CodeOffsets::Frame_Complete, _masm->offset());\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n+    break;\n+  }\n+}\n@@ -618,15 +760,2 @@\n-    case lir_std_entry: {\n-      \/\/ init offsets\n-      offsets()->set_value(CodeOffsets::OSR_Entry, _masm->offset());\n-      if (needs_icache(compilation()->method())) {\n-        int offset = check_icache();\n-        offsets()->set_value(CodeOffsets::Entry, offset);\n-      }\n-      _masm->align(CodeEntryAlignment);\n-      offsets()->set_value(CodeOffsets::Verified_Entry, _masm->offset());\n-      _masm->verified_entry(compilation()->directive()->BreakAtExecuteOption);\n-      if (needs_clinit_barrier_on_entry(compilation()->method())) {\n-        clinit_barrier(compilation()->method());\n-      }\n-      build_frame();\n-      offsets()->set_value(CodeOffsets::Frame_Complete, _masm->offset());\n+    case lir_std_entry:\n+      emit_std_entries();\n@@ -634,1 +763,0 @@\n-    }\n@@ -687,0 +815,4 @@\n+    case lir_check_orig_pc:\n+      check_orig_pc();\n+      break;\n+\n@@ -774,1 +906,2 @@\n-  _masm->build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  _masm->build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()->sp_offset_for_orig_pc()),\n+                     needs_stack_repair(), method()->has_scalarized_args(), &_verified_inline_entry);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":154,"deletions":21,"binary":false,"changes":175,"status":"modified"},{"patch":"@@ -33,0 +33,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -218,0 +220,2 @@\n+  assert(!_gen->in_conditional_code(), \"LIRItem cannot be loaded in conditional code\");\n+\n@@ -625,1 +629,2 @@\n-void LIRGenerator::monitor_enter(LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no, CodeEmitInfo* info_for_exception, CodeEmitInfo* info) {\n+void LIRGenerator::monitor_enter(LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no,\n+                                 CodeEmitInfo* info_for_exception, CodeEmitInfo* info, CodeStub* throw_ie_stub) {\n@@ -628,1 +633,1 @@\n-  CodeStub* slow_path = new MonitorEnterStub(object, lock, info);\n+  CodeStub* slow_path = new MonitorEnterStub(object, lock, info, throw_ie_stub, scratch);\n@@ -631,1 +636,1 @@\n-  __ lock_object(hdr, object, lock, scratch, slow_path, info_for_exception);\n+  __ lock_object(hdr, object, lock, scratch, slow_path, info_for_exception, throw_ie_stub);\n@@ -655,4 +660,9 @@\n-void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {\n-  klass2reg_with_patching(klass_reg, klass, info, is_unresolved);\n-  \/\/ If klass is not loaded we do not know if the klass has finalizers:\n-  if (UseFastNewInstance && klass->is_loaded()\n+void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, bool allow_inline, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {\n+  if (allow_inline) {\n+    assert(!is_unresolved && klass->is_loaded(), \"inline type klass should be resolved\");\n+    __ metadata2reg(klass->constant_encoding(), klass_reg);\n+  } else {\n+    klass2reg_with_patching(klass_reg, klass, info, is_unresolved);\n+  }\n+  \/\/ If klass is not loaded we do not know if the klass has finalizers or is an unexpected inline klass\n+  if (UseFastNewInstance && klass->is_loaded() && (allow_inline || !klass->is_inlinetype())\n@@ -673,1 +683,1 @@\n-    __ branch(lir_cond_always, slow_path);\n+    __ jump(slow_path);\n@@ -773,0 +783,10 @@\n+  if (!src->is_loaded_flat_array() && !dst->is_loaded_flat_array()) {\n+    flags &= ~LIR_OpArrayCopy::always_slow_path;\n+  }\n+  if (!src->maybe_flat_array()) {\n+    flags &= ~LIR_OpArrayCopy::src_inlinetype_check;\n+  }\n+  if (!dst->maybe_flat_array() && !dst->maybe_null_free_array()) {\n+    flags &= ~LIR_OpArrayCopy::dst_inlinetype_check;\n+  }\n+\n@@ -1326,0 +1346,9 @@\n+  \/\/ Valhalla update: the code is now a bit convuloted because arrays and primitive\n+  \/\/ classes don't have the same modifiers set anymore, but we cannot introduce\n+  \/\/ branches in LIR generation (JDK-8211231). So, the first part of the code remains\n+  \/\/ identical, using the byteArrayKlass object to avoid a NPE when accessing the\n+  \/\/ modifiers. But then the code also prepares the correct modifiers set for\n+  \/\/ primitive classes, and there's a second conditional move to put the right\n+  \/\/ value into result.\n+\n+\n@@ -1327,1 +1356,2 @@\n-  assert(univ_klass->modifier_flags() == (JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC), \"Sanity\");\n+  assert(univ_klass->modifier_flags() == (JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC\n+                                          | (Arguments::enable_preview() ? JVM_ACC_IDENTITY : 0)), \"Sanity\");\n@@ -1337,0 +1367,2 @@\n+  LIR_Opr klass_modifiers = new_register(T_INT);\n+  __ move(new LIR_Address(klass, in_bytes(Klass::modifier_flags_offset()), T_CHAR), klass_modifiers);\n@@ -1338,2 +1370,4 @@\n-  \/\/ Get the answer.\n-  __ move(new LIR_Address(klass, in_bytes(Klass::modifier_flags_offset()), T_CHAR), result);\n+  LIR_Opr prim_modifiers = load_immediate(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC, T_INT);\n+\n+  __ cmp(lir_cond_equal, recv_klass, LIR_OprFact::metadataConst(0));\n+  __ cmove(lir_cond_equal, prim_modifiers, klass_modifiers, result, T_CHAR);\n@@ -1572,2 +1606,4 @@\n-  _constants.append(c);\n-  _reg_for_constants.append(result);\n+  if (!in_conditional_code()) {\n+    _constants.append(c);\n+    _reg_for_constants.append(result);\n+  }\n@@ -1577,0 +1613,6 @@\n+void LIRGenerator::set_in_conditional_code(bool v) {\n+  assert(v != _in_conditional_code, \"must change state\");\n+  _in_conditional_code = v;\n+}\n+\n+\n@@ -1594,0 +1636,8 @@\n+\/\/ Returns a int\/long value with the null marker bit set\n+static LIR_Opr null_marker_mask(BasicType bt, ciField* field) {\n+  assert(field->null_marker_offset() != -1, \"field does not have null marker\");\n+  int nm_offset = field->null_marker_offset() - field->offset_in_bytes();\n+  jlong null_marker = 1ULL << (nm_offset << LogBitsPerByte);\n+  return (bt == T_LONG) ? LIR_OprFact::longConst(null_marker) : LIR_OprFact::intConst(null_marker);\n+}\n+\n@@ -1623,0 +1673,1 @@\n+  ciField* field = x->field();\n@@ -1624,1 +1675,1 @@\n-  bool is_volatile = x->field()->is_volatile();\n+  bool is_volatile = field->is_volatile();\n@@ -1645,10 +1696,2 @@\n-  if (is_volatile || needs_patching) {\n-    \/\/ load item if field is volatile (fewer special cases for volatiles)\n-    \/\/ load item if field not initialized\n-    \/\/ load item if field not constant\n-    \/\/ because of code patching we cannot inline constants\n-    if (field_type == T_BYTE || field_type == T_BOOLEAN) {\n-      value.load_byte_item();\n-    } else  {\n-      value.load_item();\n-    }\n+  if (field->is_flat()) {\n+    value.load_item();\n@@ -1656,1 +1699,13 @@\n-    value.load_for_store(field_type);\n+    if (is_volatile || needs_patching) {\n+      \/\/ load item if field is volatile (fewer special cases for volatiles)\n+      \/\/ load item if field not initialized\n+      \/\/ load item if field not constant\n+      \/\/ because of code patching we cannot inline constants\n+      if (field_type == T_BYTE || field_type == T_BOOLEAN) {\n+        value.load_byte_item();\n+      } else  {\n+        value.load_item();\n+      }\n+    } else {\n+      value.load_for_store(field_type);\n+    }\n@@ -1685,0 +1740,43 @@\n+  if (field->is_flat()) {\n+    ciInlineKlass* vk = field->type()->as_inline_klass();\n+\n+#ifdef ASSERT\n+    bool is_naturally_atomic = vk->nof_declared_nonstatic_fields() <= 1;\n+    bool needs_atomic_access = !field->is_null_free() || (field->is_volatile() && !is_naturally_atomic);\n+    assert(needs_atomic_access, \"No atomic access required\");\n+    \/\/ ZGC does not support compressed oops, so only one oop can be in the payload which is written by a \"normal\" oop store.\n+    assert(!vk->contains_oops() || !UseZGC, \"ZGC does not support embedded oops in flat fields\");\n+#endif\n+\n+    \/\/ Zero the payload\n+    BasicType bt = vk->payload_size_to_basic_type();\n+    LIR_Opr payload = new_register((bt == T_LONG) ? bt : T_INT);\n+    LIR_Opr zero = (bt == T_LONG) ? LIR_OprFact::longConst(0) : LIR_OprFact::intConst(0);\n+    __ move(zero, payload);\n+\n+    bool is_constant_null = value.is_constant() && value.value()->is_null_obj();\n+    if (!is_constant_null) {\n+      LabelObj* L_isNull = new LabelObj();\n+      bool needs_null_check = !value.is_constant() || value.value()->is_null_obj();\n+      if (needs_null_check) {\n+        __ cmp(lir_cond_equal, value.result(), LIR_OprFact::oopConst(nullptr));\n+        __ branch(lir_cond_equal, L_isNull->label());\n+      }\n+      \/\/ Load payload (if not empty) and set null marker (if not null-free)\n+      if (!vk->is_empty()) {\n+        access_load_at(decorators, bt, value, LIR_OprFact::intConst(vk->payload_offset()), payload);\n+      }\n+      if (!field->is_null_free()) {\n+        __ logical_or(payload, null_marker_mask(bt, field), payload);\n+      }\n+      if (needs_null_check) {\n+        __ branch_destination(L_isNull->label());\n+      }\n+    }\n+    access_store_at(decorators, bt, object, LIR_OprFact::intConst(x->offset()), payload,\n+                    \/\/ Make sure to emit an implicit null check and pass the information\n+                    \/\/ that this is a flat store that might require gc barriers for oop fields.\n+                    info != nullptr ? new CodeEmitInfo(info) : nullptr, info, vk);\n+    return;\n+  }\n+\n@@ -1689,0 +1787,173 @@\n+\/\/ FIXME -- I can't find any other way to pass an address to access_load_at().\n+class TempResolvedAddress: public Instruction {\n+ public:\n+  TempResolvedAddress(ValueType* type, LIR_Opr addr) : Instruction(type) {\n+    set_operand(addr);\n+  }\n+  virtual void input_values_do(ValueVisitor*) {}\n+  virtual void visit(InstructionVisitor* v)   {}\n+  virtual const char* name() const  { return \"TempResolvedAddress\"; }\n+};\n+\n+LIR_Opr LIRGenerator::get_and_load_element_address(LIRItem& array, LIRItem& index) {\n+  ciType* array_type = array.value()->declared_type();\n+  ciFlatArrayKlass* flat_array_klass = array_type->as_flat_array_klass();\n+  assert(flat_array_klass->is_loaded(), \"must be\");\n+\n+  int array_header_size = flat_array_klass->array_header_in_bytes();\n+  int shift = flat_array_klass->log2_element_size();\n+\n+#ifndef _LP64\n+  LIR_Opr index_op = new_register(T_INT);\n+  \/\/ FIXME -- on 32-bit, the shift below can overflow, so we need to check that\n+  \/\/ the top (shift+1) bits of index_op must be zero, or\n+  \/\/ else throw ArrayIndexOutOfBoundsException\n+  if (index.result()->is_constant()) {\n+    jint const_index = index.result()->as_jint();\n+    __ move(LIR_OprFact::intConst(const_index << shift), index_op);\n+  } else {\n+    __ shift_left(index_op, shift, index.result());\n+  }\n+#else\n+  LIR_Opr index_op = new_register(T_LONG);\n+  if (index.result()->is_constant()) {\n+    jint const_index = index.result()->as_jint();\n+    __ move(LIR_OprFact::longConst(const_index << shift), index_op);\n+  } else {\n+    __ convert(Bytecodes::_i2l, index.result(), index_op);\n+    \/\/ Need to shift manually, as LIR_Address can scale only up to 3.\n+    __ shift_left(index_op, shift, index_op);\n+  }\n+#endif\n+\n+  LIR_Opr elm_op = new_pointer_register();\n+  LIR_Address* elm_address = generate_address(array.result(), index_op, 0, array_header_size, T_ADDRESS);\n+  __ leal(LIR_OprFact::address(elm_address), elm_op);\n+  return elm_op;\n+}\n+\n+void LIRGenerator::access_sub_element(LIRItem& array, LIRItem& index, LIR_Opr& result, ciField* field, int sub_offset) {\n+  assert(field != nullptr, \"Need a subelement type specified\");\n+\n+  \/\/ Find the starting address of the source (inside the array)\n+  LIR_Opr elm_op = get_and_load_element_address(array, index);\n+\n+  BasicType subelt_type = field->type()->basic_type();\n+  TempResolvedAddress* elm_resolved_addr = new TempResolvedAddress(as_ValueType(subelt_type), elm_op);\n+  LIRItem elm_item(elm_resolved_addr, this);\n+\n+  DecoratorSet decorators = IN_HEAP;\n+  access_load_at(decorators, subelt_type,\n+                     elm_item, LIR_OprFact::intConst(sub_offset), result,\n+                     nullptr, nullptr);\n+\n+  if (field->is_null_free()) {\n+    assert(field->type()->is_loaded(), \"Must be\");\n+    assert(field->type()->is_inlinetype(), \"Must be if loaded\");\n+    assert(field->type()->as_inline_klass()->is_initialized(), \"Must be\");\n+    LabelObj* L_end = new LabelObj();\n+    __ cmp(lir_cond_notEqual, result, LIR_OprFact::oopConst(nullptr));\n+    __ branch(lir_cond_notEqual, L_end->label());\n+    set_in_conditional_code(true);\n+    Constant* default_value = new Constant(new InstanceConstant(field->type()->as_inline_klass()->default_instance()));\n+    if (default_value->is_pinned()) {\n+      __ move(LIR_OprFact::value_type(default_value->type()), result);\n+    } else {\n+      __ move(load_constant(default_value), result);\n+    }\n+    __ branch_destination(L_end->label());\n+    set_in_conditional_code(false);\n+  }\n+}\n+\n+void LIRGenerator::access_flat_array(bool is_load, LIRItem& array, LIRItem& index, LIRItem& obj_item,\n+                                          ciField* field, int sub_offset) {\n+  assert(sub_offset == 0 || field != nullptr, \"Sanity check\");\n+\n+  \/\/ Find the starting address of the source (inside the array)\n+  LIR_Opr elm_op = get_and_load_element_address(array, index);\n+\n+  ciInlineKlass* elem_klass = nullptr;\n+  if (field != nullptr) {\n+    elem_klass = field->type()->as_inline_klass();\n+  } else {\n+    elem_klass = array.value()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+  }\n+  for (int i = 0; i < elem_klass->nof_nonstatic_fields(); i++) {\n+    ciField* inner_field = elem_klass->nonstatic_field_at(i);\n+    assert(!inner_field->is_flat(), \"flat fields must have been expanded\");\n+    int obj_offset = inner_field->offset_in_bytes();\n+    int elm_offset = obj_offset - elem_klass->payload_offset() + sub_offset; \/\/ object header is not stored in array.\n+    BasicType field_type = inner_field->type()->basic_type();\n+\n+    \/\/ Types which are smaller than int are still passed in an int register.\n+    BasicType reg_type = field_type;\n+    switch (reg_type) {\n+    case T_BYTE:\n+    case T_BOOLEAN:\n+    case T_SHORT:\n+    case T_CHAR:\n+      reg_type = T_INT;\n+      break;\n+    default:\n+      break;\n+    }\n+\n+    LIR_Opr temp = new_register(reg_type);\n+    TempResolvedAddress* elm_resolved_addr = new TempResolvedAddress(as_ValueType(field_type), elm_op);\n+    LIRItem elm_item(elm_resolved_addr, this);\n+\n+    DecoratorSet decorators = IN_HEAP;\n+    if (is_load) {\n+      access_load_at(decorators, field_type,\n+                     elm_item, LIR_OprFact::intConst(elm_offset), temp,\n+                     nullptr, nullptr);\n+      access_store_at(decorators, field_type,\n+                      obj_item, LIR_OprFact::intConst(obj_offset), temp,\n+                      nullptr, nullptr);\n+    } else {\n+      access_load_at(decorators, field_type,\n+                     obj_item, LIR_OprFact::intConst(obj_offset), temp,\n+                     nullptr, nullptr);\n+      access_store_at(decorators, field_type,\n+                      elm_item, LIR_OprFact::intConst(elm_offset), temp,\n+                      nullptr, nullptr);\n+    }\n+  }\n+}\n+\n+void LIRGenerator::check_flat_array(LIR_Opr array, LIR_Opr value, CodeStub* slow_path) {\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_flat_array(array, value, tmp, slow_path);\n+}\n+\n+void LIRGenerator::check_null_free_array(LIRItem& array, LIRItem& value, CodeEmitInfo* info) {\n+  LabelObj* L_end = new LabelObj();\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_null_free_array(array.result(), tmp);\n+  __ branch(lir_cond_equal, L_end->label());\n+  __ null_check(value.result(), info);\n+  __ branch_destination(L_end->label());\n+}\n+\n+bool LIRGenerator::needs_flat_array_store_check(StoreIndexed* x) {\n+  if (x->elt_type() == T_OBJECT && x->array()->maybe_flat_array()) {\n+    ciType* type = x->value()->declared_type();\n+    if (type != nullptr && type->is_klass()) {\n+      ciKlass* klass = type->as_klass();\n+      if (!klass->can_be_inline_klass() || (klass->is_inlinetype() && !klass->as_inline_klass()->flat_in_array())) {\n+        \/\/ This is known to be a non-flat object. If the array is a flat array,\n+        \/\/ it will be caught by the code generated by array_store_check().\n+        return false;\n+      }\n+    }\n+    \/\/ We're not 100% sure, so let's do the flat_array_store_check.\n+    return true;\n+  }\n+  return false;\n+}\n+\n+bool LIRGenerator::needs_null_free_array_store_check(StoreIndexed* x) {\n+  return x->elt_type() == T_OBJECT && x->array()->maybe_null_free_array();\n+}\n+\n@@ -1691,0 +1962,2 @@\n+  assert(x->elt_type() != T_ARRAY, \"never used\");\n+  bool is_loaded_flat_array = x->array()->is_loaded_flat_array();\n@@ -1694,3 +1967,3 @@\n-  bool needs_store_check = obj_store && (x->value()->as_Constant() == nullptr ||\n-                                         !get_jobject_constant(x->value())->is_null_object() ||\n-                                         x->should_profile());\n+  bool needs_store_check = obj_store && !(is_loaded_flat_array && x->is_exact_flat_array_store()) &&\n+                                        (x->value()->as_Constant() == nullptr ||\n+                                         !get_jobject_constant(x->value())->is_null_object());\n@@ -1709,2 +1982,3 @@\n-\n-  if (needs_store_check || x->check_boolean()) {\n+\n+  if (needs_store_check || x->check_boolean()\n+      || is_loaded_flat_array || needs_flat_array_store_check(x) || needs_null_free_array_store_check(x)) {\n@@ -1739,0 +2013,20 @@\n+  if (x->should_profile()) {\n+    if (x->array()->is_loaded_flat_array()) {\n+      \/\/ No need to profile a store to a flat array of known type. This can happen if\n+      \/\/ the type only became known after optimizations (for example, after the PhiSimplifier).\n+      x->set_should_profile(false);\n+    } else {\n+      int bci = x->profiled_bci();\n+      ciMethodData* md = x->profiled_method()->method_data();\n+      assert(md != nullptr, \"Sanity\");\n+      ciProfileData* data = md->bci_to_data(bci);\n+      assert(data != nullptr && data->is_ArrayStoreData(), \"incorrect profiling entry\");\n+      ciArrayStoreData* store_data = (ciArrayStoreData*)data;\n+      profile_array_type(x, md, store_data);\n+      assert(store_data->is_ArrayStoreData(), \"incorrect profiling entry\");\n+      if (x->array()->maybe_null_free_array()) {\n+        profile_null_free_array(array, md, store_data);\n+      }\n+    }\n+  }\n+\n@@ -1744,4 +2038,21 @@\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-  if (x->check_boolean()) {\n-    decorators |= C1_MASK_BOOLEAN;\n-  }\n+  if (is_loaded_flat_array) {\n+    if (!x->value()->is_null_free()) {\n+      __ null_check(value.result(), new CodeEmitInfo(range_check_info));\n+    }\n+    \/\/ If array element is an empty inline type, no need to copy anything\n+    if (!x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass()->is_empty()) {\n+      access_flat_array(false, array, index, value);\n+    }\n+  } else {\n+    StoreFlattenedArrayStub* slow_path = nullptr;\n+\n+    if (needs_flat_array_store_check(x)) {\n+      \/\/ Check if we indeed have a flat array\n+      index.load_item();\n+      slow_path = new StoreFlattenedArrayStub(array.result(), index.result(), value.result(), state_for(x, x->state_before()));\n+      check_flat_array(array.result(), value.result(), slow_path);\n+      set_in_conditional_code(true);\n+    } else if (needs_null_free_array_store_check(x)) {\n+      CodeEmitInfo* info = new CodeEmitInfo(range_check_info);\n+      check_null_free_array(array, value, info);\n+    }\n@@ -1749,2 +2060,12 @@\n-  access_store_at(decorators, x->elt_type(), array, index.result(), value.result(),\n-                  nullptr, null_check_info);\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    if (x->check_boolean()) {\n+      decorators |= C1_MASK_BOOLEAN;\n+    }\n+\n+    access_store_at(decorators, x->elt_type(), array, index.result(), value.result(),\n+                    nullptr, null_check_info);\n+    if (slow_path != nullptr) {\n+      __ branch_destination(slow_path->continuation());\n+      set_in_conditional_code(false);\n+    }\n+  }\n@@ -1779,1 +2100,2 @@\n-                                   CodeEmitInfo* patch_info, CodeEmitInfo* store_emit_info) {\n+                                   CodeEmitInfo* patch_info, CodeEmitInfo* store_emit_info,\n+                                   ciInlineKlass* vk) {\n@@ -1781,1 +2103,1 @@\n-  LIRAccess access(this, decorators, base, offset, type, patch_info, store_emit_info);\n+  LIRAccess access(this, decorators, base, offset, type, patch_info, store_emit_info, vk);\n@@ -1832,0 +2154,1 @@\n+  ciField* field = x->field();\n@@ -1833,1 +2156,1 @@\n-  bool is_volatile = x->field()->is_volatile();\n+  bool is_volatile = field->is_volatile();\n@@ -1884,0 +2207,37 @@\n+  if (field->is_flat()) {\n+    ciInlineKlass* vk = field->type()->as_inline_klass();\n+#ifdef ASSERT\n+    bool is_naturally_atomic = vk->nof_declared_nonstatic_fields() <= 1;\n+    bool needs_atomic_access = !field->is_null_free() || (field->is_volatile() && !is_naturally_atomic);\n+    assert(needs_atomic_access, \"No atomic access required\");\n+    assert(x->state_before() != nullptr, \"Needs state before\");\n+#endif\n+\n+    \/\/ Allocate buffer (we can't easily do this conditionally on the null check below\n+    \/\/ because branches added in the LIR are opaque to the register allocator).\n+    NewInstance* buffer = new NewInstance(vk, x->state_before(), false, true);\n+    do_NewInstance(buffer);\n+    LIRItem dest(buffer, this);\n+\n+    \/\/ Copy the payload to the buffer\n+    BasicType bt = vk->payload_size_to_basic_type();\n+    LIR_Opr payload = new_register((bt == T_LONG) ? bt : T_INT);\n+    access_load_at(decorators, bt, object, LIR_OprFact::intConst(field->offset_in_bytes()), payload,\n+                   \/\/ Make sure to emit an implicit null check\n+                   info ? new CodeEmitInfo(info) : nullptr, info);\n+    access_store_at(decorators, bt, dest, LIR_OprFact::intConst(vk->payload_offset()), payload);\n+\n+    if (field->is_null_free()) {\n+      set_result(x, buffer->operand());\n+    } else {\n+      \/\/ Check the null marker and set result to null if it's not set\n+      __ logical_and(payload, null_marker_mask(bt, field), payload);\n+      __ cmp(lir_cond_equal, payload, (bt == T_LONG) ? LIR_OprFact::longConst(0) : LIR_OprFact::intConst(0));\n+      __ cmove(lir_cond_equal, LIR_OprFact::oopConst(nullptr), buffer->operand(), rlock_result(x), T_OBJECT);\n+    }\n+\n+    \/\/ Ensure the copy is visible before any subsequent store that publishes the buffer.\n+    __ membar_storestore();\n+    return;\n+  }\n+\n@@ -1888,0 +2248,33 @@\n+\n+  if (field->is_null_free()) {\n+    \/\/ Load from non-flat inline type field requires\n+    \/\/ a null check to replace null with the default value.\n+    ciInstanceKlass* holder = field->holder();\n+    if (field->is_static() && holder->is_loaded()) {\n+      ciObject* val = holder->java_mirror()->field_value(field).as_object();\n+      if (!val->is_null_object()) {\n+        \/\/ Static field is initialized, we don't need to perform a null check.\n+        return;\n+      }\n+    }\n+    ciInlineKlass* inline_klass = field->type()->as_inline_klass();\n+    if (inline_klass->is_initialized()) {\n+      LabelObj* L_end = new LabelObj();\n+      __ cmp(lir_cond_notEqual, result, LIR_OprFact::oopConst(nullptr));\n+      __ branch(lir_cond_notEqual, L_end->label());\n+      set_in_conditional_code(true);\n+      Constant* default_value = new Constant(new InstanceConstant(inline_klass->default_instance()));\n+      if (default_value->is_pinned()) {\n+        __ move(LIR_OprFact::value_type(default_value->type()), result);\n+      } else {\n+        __ move(load_constant(default_value), result);\n+      }\n+      __ branch_destination(L_end->label());\n+      set_in_conditional_code(false);\n+    } else {\n+      info = state_for(x, x->state_before());\n+      __ cmp(lir_cond_equal, result, LIR_OprFact::oopConst(nullptr));\n+      __ branch(lir_cond_equal, new DeoptimizeStub(info, Deoptimization::Reason_uninitialized,\n+                                                         Deoptimization::Action_make_not_entrant));\n+    }\n+  }\n@@ -2032,1 +2425,74 @@\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+  ciMethodData* md = nullptr;\n+  ciArrayLoadData* load_data = nullptr;\n+  if (x->should_profile()) {\n+    if (x->array()->is_loaded_flat_array()) {\n+      \/\/ No need to profile a load from a flat array of known type. This can happen if\n+      \/\/ the type only became known after optimizations (for example, after the PhiSimplifier).\n+      x->set_should_profile(false);\n+    } else {\n+      int bci = x->profiled_bci();\n+      md = x->profiled_method()->method_data();\n+      assert(md != nullptr, \"Sanity\");\n+      ciProfileData* data = md->bci_to_data(bci);\n+      assert(data != nullptr && data->is_ArrayLoadData(), \"incorrect profiling entry\");\n+      load_data = (ciArrayLoadData*)data;\n+      profile_array_type(x, md, load_data);\n+    }\n+  }\n+\n+  Value element;\n+  if (x->vt() != nullptr) {\n+    assert(x->array()->is_loaded_flat_array(), \"must be\");\n+    \/\/ Find the destination address (of the NewInlineTypeInstance).\n+    LIRItem obj_item(x->vt(), this);\n+\n+    access_flat_array(true, array, index, obj_item,\n+                      x->delayed() == nullptr ? 0 : x->delayed()->field(),\n+                      x->delayed() == nullptr ? 0 : x->delayed()->offset());\n+    set_no_result(x);\n+  } else if (x->delayed() != nullptr) {\n+    assert(x->array()->is_loaded_flat_array(), \"must be\");\n+    LIR_Opr result = rlock_result(x, x->delayed()->field()->type()->basic_type());\n+    access_sub_element(array, index, result, x->delayed()->field(), x->delayed()->offset());\n+  } else if (x->array() != nullptr && x->array()->is_loaded_flat_array() &&\n+             x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass()->is_initialized() &&\n+             x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass()->is_empty()) {\n+    \/\/ Load the default instance instead of reading the element\n+    ciInlineKlass* elem_klass = x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+    LIR_Opr result = rlock_result(x, x->elt_type());\n+    assert(elem_klass->is_initialized(), \"Must be\");\n+    Constant* default_value = new Constant(new InstanceConstant(elem_klass->default_instance()));\n+    if (default_value->is_pinned()) {\n+      __ move(LIR_OprFact::value_type(default_value->type()), result);\n+    } else {\n+      __ move(load_constant(default_value), result);\n+    }\n+  } else {\n+    LIR_Opr result = rlock_result(x, x->elt_type());\n+    LoadFlattenedArrayStub* slow_path = nullptr;\n+\n+    if (x->should_profile() && x->array()->maybe_null_free_array()) {\n+      profile_null_free_array(array, md, load_data);\n+    }\n+\n+    if (x->elt_type() == T_OBJECT && x->array()->maybe_flat_array()) {\n+      assert(x->delayed() == nullptr, \"Delayed LoadIndexed only apply to loaded_flat_arrays\");\n+      index.load_item();\n+      \/\/ if we are loading from a flat array, load it using a runtime call\n+      slow_path = new LoadFlattenedArrayStub(array.result(), index.result(), result, state_for(x, x->state_before()));\n+      check_flat_array(array.result(), LIR_OprFact::illegalOpr, slow_path);\n+      set_in_conditional_code(true);\n+    }\n+\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    access_load_at(decorators, x->elt_type(),\n+                   array, index.result(), result,\n+                   nullptr, null_check_info);\n+\n+    if (slow_path != nullptr) {\n+      __ branch_destination(slow_path->continuation());\n+      set_in_conditional_code(false);\n+    }\n+\n+    element = x;\n+  }\n@@ -2034,4 +2500,3 @@\n-  LIR_Opr result = rlock_result(x, x->elt_type());\n-  access_load_at(decorators, x->elt_type(),\n-                 array, index.result(), result,\n-                 nullptr, null_check_info);\n+  if (x->should_profile()) {\n+    profile_element_type(element, md, load_data);\n+  }\n@@ -2539,1 +3004,1 @@\n-  if (do_update) {\n+  if (do_update && signature_at_call_k != nullptr) {\n@@ -2624,0 +3089,40 @@\n+void LIRGenerator::profile_flags(ciMethodData* md, ciProfileData* data, int flag, LIR_Condition condition) {\n+  assert(md != nullptr && data != nullptr, \"should have been initialized\");\n+  LIR_Opr mdp = new_register(T_METADATA);\n+  __ metadata2reg(md->constant_encoding(), mdp);\n+  LIR_Address* addr = new LIR_Address(mdp, md->byte_offset_of_slot(data, DataLayout::flags_offset()), T_BYTE);\n+  LIR_Opr flags = new_register(T_INT);\n+  __ move(addr, flags);\n+  if (condition != lir_cond_always) {\n+    LIR_Opr update = new_register(T_INT);\n+    __ cmove(condition, LIR_OprFact::intConst(0), LIR_OprFact::intConst(flag), update, T_INT);\n+  } else {\n+    __ logical_or(flags, LIR_OprFact::intConst(flag), flags);\n+  }\n+  __ store(flags, addr);\n+}\n+\n+template <class ArrayData> void LIRGenerator::profile_null_free_array(LIRItem array, ciMethodData* md, ArrayData* load_store) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  LabelObj* L_end = new LabelObj();\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_null_free_array(array.result(), tmp);\n+\n+  profile_flags(md, load_store, ArrayStoreData::null_free_array_byte_constant(), lir_cond_equal);\n+}\n+\n+template <class ArrayData> void LIRGenerator::profile_array_type(AccessIndexed* x, ciMethodData*& md, ArrayData*& load_store) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(load_store, ArrayData::array_offset()), 0,\n+               load_store->array()->type(), x->array(), mdp, true, nullptr, nullptr);\n+}\n+\n+void LIRGenerator::profile_element_type(Value element, ciMethodData* md, ciArrayLoadData* load_data) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  assert(md != nullptr && load_data != nullptr, \"should have been initialized\");\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(load_data, ArrayLoadData::element_offset()), 0,\n+               load_data->element()->type(), element, mdp, false, nullptr, nullptr);\n+}\n+\n@@ -2706,0 +3211,8 @@\n+  if (method()->has_scalarized_args()) {\n+    \/\/ Check if deoptimization was triggered (i.e. orig_pc was set) while buffering scalarized inline type arguments\n+    \/\/ in the entry point (see comments in frame::deoptimize). If so, deoptimize only now that we have the right state.\n+    CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, 0), nullptr, false);\n+    CodeStub* deopt_stub = new DeoptimizeStub(info, Deoptimization::Reason_none, Deoptimization::Action_none);\n+    __ append(new LIR_Op0(lir_check_orig_pc));\n+    __ branch(lir_cond_notEqual, deopt_stub);\n+  }\n@@ -2721,0 +3234,13 @@\n+void LIRGenerator::invoke_load_one_argument(LIRItem* param, LIR_Opr loc) {\n+  if (loc->is_register()) {\n+    param->load_item_force(loc);\n+  } else {\n+    LIR_Address* addr = loc->as_address_ptr();\n+    param->load_for_store(addr->type());\n+    if (addr->type() == T_OBJECT) {\n+      __ move_wide(param->result(), addr);\n+    } else {\n+      __ move(param->result(), addr);\n+    }\n+  }\n+}\n@@ -2728,10 +3254,1 @@\n-    if (loc->is_register()) {\n-      param->load_item_force(loc);\n-    } else {\n-      LIR_Address* addr = loc->as_address_ptr();\n-      param->load_for_store(addr->type());\n-      if (addr->type() == T_OBJECT) {\n-        __ move_wide(param->result(), addr);\n-      } else\n-        __ move(param->result(), addr);\n-    }\n+    invoke_load_one_argument(param, loc);\n@@ -2903,1 +3420,1 @@\n-  if (can_inline_as_constant(right.value())) {\n+  if (can_inline_as_constant(right.value()) && !x->substitutability_check()) {\n@@ -2906,0 +3423,1 @@\n+    \/\/ substitutability_check() needs to use right as a base register.\n@@ -2913,3 +3431,60 @@\n-  LIR_Opr reg = rlock_result(x);\n-  __ cmp(lir_cond(x->cond()), left.result(), right.result());\n-  __ cmove(lir_cond(x->cond()), t_val.result(), f_val.result(), reg, as_BasicType(x->x()->type()));\n+  if (x->substitutability_check()) {\n+    substitutability_check(x, left, right, t_val, f_val);\n+  } else {\n+    LIR_Opr reg = rlock_result(x);\n+    __ cmp(lir_cond(x->cond()), left.result(), right.result());\n+    __ cmove(lir_cond(x->cond()), t_val.result(), f_val.result(), reg, as_BasicType(x->x()->type()));\n+  }\n+}\n+\n+void LIRGenerator::substitutability_check(IfOp* x, LIRItem& left, LIRItem& right, LIRItem& t_val, LIRItem& f_val) {\n+  assert(x->cond() == If::eql || x->cond() == If::neq, \"must be\");\n+  bool is_acmpeq = (x->cond() == If::eql);\n+  LIR_Opr equal_result     = is_acmpeq ? t_val.result() : f_val.result();\n+  LIR_Opr not_equal_result = is_acmpeq ? f_val.result() : t_val.result();\n+  LIR_Opr result = rlock_result(x);\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+\n+  substitutability_check_common(x->x(), x->y(), left, right, equal_result, not_equal_result, result, info);\n+}\n+\n+void LIRGenerator::substitutability_check(If* x, LIRItem& left, LIRItem& right) {\n+  LIR_Opr equal_result     = LIR_OprFact::intConst(1);\n+  LIR_Opr not_equal_result = LIR_OprFact::intConst(0);\n+  LIR_Opr result = new_register(T_INT);\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+\n+  substitutability_check_common(x->x(), x->y(), left, right, equal_result, not_equal_result, result, info);\n+\n+  assert(x->cond() == If::eql || x->cond() == If::neq, \"must be\");\n+  __ cmp(lir_cond(x->cond()), result, equal_result);\n+}\n+\n+void LIRGenerator::substitutability_check_common(Value left_val, Value right_val, LIRItem& left, LIRItem& right,\n+                                                 LIR_Opr equal_result, LIR_Opr not_equal_result, LIR_Opr result,\n+                                                 CodeEmitInfo* info) {\n+  LIR_Opr tmp1 = LIR_OprFact::illegalOpr;\n+  LIR_Opr tmp2 = LIR_OprFact::illegalOpr;\n+  LIR_Opr left_klass_op = LIR_OprFact::illegalOpr;\n+  LIR_Opr right_klass_op = LIR_OprFact::illegalOpr;\n+\n+  ciKlass* left_klass  = left_val ->as_loaded_klass_or_null();\n+  ciKlass* right_klass = right_val->as_loaded_klass_or_null();\n+\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    init_temps_for_substitutability_check(tmp1, tmp2);\n+  }\n+\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+  } else {\n+    BasicType t_klass = UseCompressedOops ? T_INT : T_METADATA;\n+    left_klass_op = new_register(t_klass);\n+    right_klass_op = new_register(t_klass);\n+  }\n+\n+  CodeStub* slow_path = new SubstitutabilityCheckStub(left.result(), right.result(), info);\n+  __ substitutability_check(result, left.result(), right.result(), equal_result, not_equal_result,\n+                            tmp1, tmp2,\n+                            left_klass, right_klass, left_klass_op, right_klass_op, info, slow_path);\n@@ -3191,1 +3766,1 @@\n-    ciReturnTypeEntry* ret = data->is_CallTypeData() ? ((ciCallTypeData*)data)->ret() : ((ciVirtualCallTypeData*)data)->ret();\n+    ciSingleTypeEntry* ret = data->is_CallTypeData() ? ((ciCallTypeData*)data)->ret() : ((ciVirtualCallTypeData*)data)->ret();\n@@ -3212,0 +3787,47 @@\n+bool LIRGenerator::profile_inline_klass(ciMethodData* md, ciProfileData* data, Value value, int flag) {\n+  ciKlass* klass = value->as_loaded_klass_or_null();\n+  if (klass != nullptr) {\n+    if (klass->is_inlinetype()) {\n+      profile_flags(md, data, flag, lir_cond_always);\n+    } else if (klass->can_be_inline_klass()) {\n+      return false;\n+    }\n+  } else {\n+    return false;\n+  }\n+  return true;\n+}\n+\n+\n+void LIRGenerator::do_ProfileACmpTypes(ProfileACmpTypes* x) {\n+  ciMethod* method = x->method();\n+  assert(method != nullptr, \"method should be set if branch is profiled\");\n+  ciMethodData* md = method->method_data_or_null();\n+  assert(md != nullptr, \"Sanity\");\n+  ciProfileData* data = md->bci_to_data(x->bci());\n+  assert(data != nullptr, \"must have profiling data\");\n+  assert(data->is_ACmpData(), \"need BranchData for two-way branches\");\n+  ciACmpData* acmp = (ciACmpData*)data;\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(acmp, ACmpData::left_offset()), 0,\n+               acmp->left()->type(), x->left(), mdp, !x->left_maybe_null(), nullptr, nullptr);\n+  int flags_offset = md->byte_offset_of_slot(data, DataLayout::flags_offset());\n+  if (!profile_inline_klass(md, acmp, x->left(), ACmpData::left_inline_type_byte_constant())) {\n+    LIR_Opr mdp = new_register(T_METADATA);\n+    __ metadata2reg(md->constant_encoding(), mdp);\n+    LIRItem value(x->left(), this);\n+    value.load_item();\n+    __ profile_inline_type(new LIR_Address(mdp, flags_offset, T_INT), value.result(), ACmpData::left_inline_type_byte_constant(), new_register(T_INT), !x->left_maybe_null());\n+  }\n+  profile_type(md, md->byte_offset_of_slot(acmp, ACmpData::left_offset()),\n+               in_bytes(ACmpData::right_offset()) - in_bytes(ACmpData::left_offset()),\n+               acmp->right()->type(), x->right(), mdp, !x->right_maybe_null(), nullptr, nullptr);\n+  if (!profile_inline_klass(md, acmp, x->right(), ACmpData::right_inline_type_byte_constant())) {\n+    LIR_Opr mdp = new_register(T_METADATA);\n+    __ metadata2reg(md->constant_encoding(), mdp);\n+    LIRItem value(x->right(), this);\n+    value.load_item();\n+    __ profile_inline_type(new LIR_Address(mdp, flags_offset, T_INT), value.result(), ACmpData::right_inline_type_byte_constant(), new_register(T_INT), !x->left_maybe_null());\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":682,"deletions":60,"binary":false,"changes":742,"status":"modified"},{"patch":"@@ -65,1 +65,1 @@\n-static int type2spill_size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 0, 2,  1, 2, 1, -1};\n+static int type2spill_size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 0, 2,  1, 2, 1, -1};\n@@ -67,1 +67,1 @@\n-static int type2spill_size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, -1, 1, 1, -1};\n+static int type2spill_size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 0, 1, -1, 1, 1, -1};\n@@ -262,1 +262,1 @@\n-  if (!frame_map()->finalize_frame(max_spills())) {\n+  if (!frame_map()->finalize_frame(max_spills(), compilation()->needs_stack_repair())) {\n@@ -2953,1 +2953,1 @@\n-  return new IRScopeDebugInfo(cur_scope, cur_state->bci(), locals, expressions, monitors, caller_debug_info);\n+  return new IRScopeDebugInfo(cur_scope, cur_state->bci(), locals, expressions, monitors, caller_debug_info, cur_state->should_reexecute());\n","filename":"src\/hotspot\/share\/c1\/c1_LinearScan.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -89,1 +89,2 @@\n-  Value make_ifop(Value x, Instruction::Condition cond, Value y, Value tval, Value fval);\n+  Value make_ifop(Value x, Instruction::Condition cond, Value y, Value tval, Value fval,\n+                  ValueStack* state_before, bool substitutability_check);\n@@ -218,1 +219,2 @@\n-  Value result = make_ifop(if_->x(), if_->cond(), if_->y(), t_value, f_value);\n+  Value result = make_ifop(if_->x(), if_->cond(), if_->y(), t_value, f_value,\n+                           if_->state_before(), if_->substitutability_check());\n@@ -273,1 +275,2 @@\n-Value CE_Eliminator::make_ifop(Value x, Instruction::Condition cond, Value y, Value tval, Value fval) {\n+Value CE_Eliminator::make_ifop(Value x, Instruction::Condition cond, Value y, Value tval, Value fval,\n+                               ValueStack* state_before, bool substitutability_check) {\n@@ -275,1 +278,1 @@\n-    return new IfOp(x, cond, y, tval, fval);\n+    return new IfOp(x, cond, y, tval, fval, state_before, substitutability_check);\n@@ -310,1 +313,1 @@\n-            return new IfOp(x_ifop->x(), x_ifop_cond, x_ifop->y(), new_tval, new_fval);\n+            return new IfOp(x_ifop->x(), x_ifop_cond, x_ifop->y(), new_tval, new_fval, state_before, substitutability_check);\n@@ -326,1 +329,1 @@\n-  return new IfOp(x, cond, y, tval, fval);\n+  return new IfOp(x, cond, y, tval, fval, state_before, substitutability_check);\n@@ -466,1 +469,1 @@\n-      if (con && ifop) {\n+      if (con && ifop && !ifop->substitutability_check()) {\n@@ -491,1 +494,1 @@\n-                                 tblock, fblock, if_->state_before(), if_->is_safepoint());\n+                                 tblock, fblock, if_->state_before(), if_->is_safepoint(), ifop->substitutability_check());\n@@ -586,0 +589,1 @@\n+  void do_ProfileACmpTypes(ProfileACmpTypes*  x);\n@@ -714,0 +718,1 @@\n+  void handle_ProfileACmpTypes(ProfileACmpTypes* x);\n@@ -773,0 +778,1 @@\n+void NullCheckVisitor::do_ProfileACmpTypes(ProfileACmpTypes* x) { nce()->handle_ProfileACmpTypes(x); }\n@@ -1202,0 +1208,5 @@\n+void NullCheckEliminator::handle_ProfileACmpTypes(ProfileACmpTypes* x) {\n+  x->set_left_maybe_null(!set_contains(x->left()));\n+  x->set_right_maybe_null(!set_contains(x->right()));\n+}\n+\n","filename":"src\/hotspot\/share\/c1\/c1_Optimizer.cpp","additions":19,"deletions":8,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -52,0 +52,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n@@ -122,0 +124,1 @@\n+uint Runtime1::_new_null_free_array_slowcase_cnt = 0;\n@@ -124,0 +127,5 @@\n+uint Runtime1::_load_flat_array_slowcase_cnt = 0;\n+uint Runtime1::_store_flat_array_slowcase_cnt = 0;\n+uint Runtime1::_substitutability_check_slowcase_cnt = 0;\n+uint Runtime1::_buffer_inline_args_slowcase_cnt = 0;\n+uint Runtime1::_buffer_inline_args_no_receiver_slowcase_cnt = 0;\n@@ -133,0 +141,2 @@\n+uint Runtime1::_throw_illegal_monitor_state_exception_count = 0;\n+uint Runtime1::_throw_identity_exception_count = 0;\n@@ -352,2 +362,1 @@\n-\n-JRT_ENTRY(void, Runtime1::new_instance(JavaThread* current, Klass* klass))\n+static void allocate_instance(JavaThread* current, Klass* klass, TRAPS) {\n@@ -356,1 +365,1 @@\n-    _new_instance_slowcase_cnt++;\n+    Runtime1::_new_instance_slowcase_cnt++;\n@@ -365,2 +374,8 @@\n-  \/\/ allocate instance and return via TLS\n-  oop obj = h->allocate_instance(CHECK);\n+  oop obj = nullptr;\n+  if (h->is_inline_klass() && InlineKlass::cast(h)->is_empty_inline_type()) {\n+    obj = InlineKlass::cast(h)->default_value();\n+    assert(obj != nullptr, \"default value must exist\");\n+  } else {\n+    \/\/ allocate instance and return via TLS\n+    obj = h->allocate_instance(CHECK);\n+  }\n@@ -370,0 +385,3 @@\n+JRT_ENTRY(void, Runtime1::new_instance(JavaThread* current, Klass* klass))\n+  allocate_instance(current, klass, CHECK);\n+JRT_END\n@@ -404,1 +422,1 @@\n-  Klass* elem_klass = ObjArrayKlass::cast(array_klass)->element_klass();\n+  Klass* elem_klass = ArrayKlass::cast(array_klass)->element_klass();\n@@ -415,0 +433,29 @@\n+JRT_ENTRY(void, Runtime1::new_null_free_array(JavaThread* current, Klass* array_klass, jint length))\n+  NOT_PRODUCT(_new_null_free_array_slowcase_cnt++;)\n+\n+  \/\/ Note: no handle for klass needed since they are not used\n+  \/\/       anymore after new_objArray() and no GC can happen before.\n+  \/\/       (This may have to change if this code changes!)\n+  assert(array_klass->is_klass(), \"not a class\");\n+  Handle holder(THREAD, array_klass->klass_holder()); \/\/ keep the klass alive\n+  Klass* elem_klass = ArrayKlass::cast(array_klass)->element_klass();\n+  assert(elem_klass->is_inline_klass(), \"must be\");\n+  InlineKlass* vk = InlineKlass::cast(elem_klass);\n+  \/\/ Logically creates elements, ensure klass init\n+  elem_klass->initialize(CHECK);\n+  arrayOop obj= nullptr;\n+  \/\/  Limitation here, only non-atomic layouts are supported\n+  if (UseArrayFlattening && vk->has_non_atomic_layout()) {\n+    obj = oopFactory::new_flatArray(elem_klass, length, LayoutKind::NON_ATOMIC_FLAT, CHECK);\n+  } else {\n+    obj = oopFactory::new_null_free_objArray(elem_klass, length, CHECK);\n+  }\n+  current->set_vm_result(obj);\n+  \/\/ This is pretty rare but this runtime patch is stressful to deoptimization\n+  \/\/ if we deoptimize here so force a deopt to stress the path.\n+  if (DeoptimizeALot) {\n+    deopt_caller(current);\n+  }\n+JRT_END\n+\n+\n@@ -429,0 +476,92 @@\n+static void profile_flat_array(JavaThread* current, bool load) {\n+  ResourceMark rm(current);\n+  vframeStream vfst(current, true);\n+  assert(!vfst.at_end(), \"Java frame must exist\");\n+  \/\/ Check if array access profiling is enabled\n+  if (vfst.nm()->comp_level() != CompLevel_full_profile || !C1UpdateMethodData) {\n+    return;\n+  }\n+  int bci = vfst.bci();\n+  Method* method = vfst.method();\n+  MethodData* md = method->method_data();\n+  if (md != nullptr) {\n+    \/\/ Lock to access ProfileData, and ensure lock is not broken by a safepoint\n+    MutexLocker ml(md->extra_data_lock(), Mutex::_no_safepoint_check_flag);\n+\n+    ProfileData* data = md->bci_to_data(bci);\n+    assert(data != nullptr, \"incorrect profiling entry\");\n+    if (data->is_ArrayLoadData()) {\n+      assert(load, \"should be an array load\");\n+      ArrayLoadData* load_data = (ArrayLoadData*) data;\n+      load_data->set_flat_array();\n+    } else {\n+      assert(data->is_ArrayStoreData(), \"\");\n+      assert(!load, \"should be an array store\");\n+      ArrayStoreData* store_data = (ArrayStoreData*) data;\n+      store_data->set_flat_array();\n+    }\n+  }\n+}\n+\n+JRT_ENTRY(void, Runtime1::load_flat_array(JavaThread* current, flatArrayOopDesc* array, int index))\n+  assert(array->klass()->is_flatArray_klass(), \"should not be called\");\n+  profile_flat_array(current, true);\n+\n+  NOT_PRODUCT(_load_flat_array_slowcase_cnt++;)\n+  assert(array->length() > 0 && index < array->length(), \"already checked\");\n+  flatArrayHandle vah(current, array);\n+  oop obj = array->read_value_from_flat_array(index, CHECK);\n+  current->set_vm_result(obj);\n+JRT_END\n+\n+\n+JRT_ENTRY(void, Runtime1::store_flat_array(JavaThread* current, flatArrayOopDesc* array, int index, oopDesc* value))\n+  if (array->klass()->is_flatArray_klass()) {\n+    profile_flat_array(current, false);\n+  }\n+\n+  NOT_PRODUCT(_store_flat_array_slowcase_cnt++;)\n+  if (value == nullptr) {\n+    assert(array->klass()->is_flatArray_klass() || array->klass()->is_null_free_array_klass(), \"should not be called\");\n+    SharedRuntime::throw_and_post_jvmti_exception(current, vmSymbols::java_lang_NullPointerException());\n+  } else {\n+    assert(array->klass()->is_flatArray_klass(), \"should not be called\");\n+    array->write_value_to_flat_array(value, index, CHECK);\n+  }\n+JRT_END\n+\n+\n+JRT_ENTRY(int, Runtime1::substitutability_check(JavaThread* current, oopDesc* left, oopDesc* right))\n+  NOT_PRODUCT(_substitutability_check_slowcase_cnt++;)\n+  JavaCallArguments args;\n+  args.push_oop(Handle(THREAD, left));\n+  args.push_oop(Handle(THREAD, right));\n+  JavaValue result(T_BOOLEAN);\n+  JavaCalls::call_static(&result,\n+                         vmClasses::ValueObjectMethods_klass(),\n+                         vmSymbols::isSubstitutable_name(),\n+                         vmSymbols::object_object_boolean_signature(),\n+                         &args, CHECK_0);\n+  return result.get_jboolean() ? 1 : 0;\n+JRT_END\n+\n+\n+extern \"C\" void ps();\n+\n+void Runtime1::buffer_inline_args_impl(JavaThread* current, Method* m, bool allocate_receiver) {\n+  JavaThread* THREAD = current;\n+  methodHandle method(current, m); \/\/ We are inside the verified_entry or verified_inline_ro_entry of this method.\n+  oop obj = SharedRuntime::allocate_inline_types_impl(current, method, allocate_receiver, CHECK);\n+  current->set_vm_result(obj);\n+}\n+\n+JRT_ENTRY(void, Runtime1::buffer_inline_args(JavaThread* current, Method* method))\n+  NOT_PRODUCT(_buffer_inline_args_slowcase_cnt++;)\n+  buffer_inline_args_impl(current, method, true);\n+JRT_END\n+\n+JRT_ENTRY(void, Runtime1::buffer_inline_args_no_receiver(JavaThread* current, Method* method))\n+  NOT_PRODUCT(_buffer_inline_args_no_receiver_slowcase_cnt++;)\n+  buffer_inline_args_impl(current, method, false);\n+JRT_END\n+\n@@ -752,0 +891,13 @@\n+JRT_ENTRY(void, Runtime1::throw_illegal_monitor_state_exception(JavaThread* current))\n+  NOT_PRODUCT(_throw_illegal_monitor_state_exception_count++;)\n+  ResourceMark rm(current);\n+  SharedRuntime::throw_and_post_jvmti_exception(current, vmSymbols::java_lang_IllegalMonitorStateException());\n+JRT_END\n+\n+JRT_ENTRY(void, Runtime1::throw_identity_exception(JavaThread* current, oopDesc* object))\n+  NOT_PRODUCT(_throw_identity_exception_count++;)\n+  ResourceMark rm(current);\n+  char* message = SharedRuntime::generate_identity_exception_message(current, object->klass());\n+  SharedRuntime::throw_and_post_jvmti_exception(current, vmSymbols::java_lang_IdentityException(), message);\n+JRT_END\n+\n@@ -957,0 +1109,2 @@\n+  bool deoptimize_for_null_free = false;\n+  bool deoptimize_for_flat = false;\n@@ -1000,0 +1154,10 @@\n+    \/\/ The field we are patching is null-free. Deoptimize and regenerate\n+    \/\/ the compiled code if we patch a putfield\/putstatic because it\n+    \/\/ does not contain the required null check.\n+    deoptimize_for_null_free = result.is_null_free_inline_type() && (field_access.is_putfield() || field_access.is_putstatic());\n+\n+    \/\/ The field we are patching is flat. Deoptimize and regenerate\n+    \/\/ the compiled code which can't handle the layout of the flat\n+    \/\/ field because it was unknown at compile time.\n+    deoptimize_for_flat = result.is_flat();\n+\n@@ -1072,1 +1236,1 @@\n-  if (deoptimize_for_volatile || deoptimize_for_atomic) {\n+  if (deoptimize_for_volatile || deoptimize_for_atomic || deoptimize_for_null_free || deoptimize_for_flat) {\n@@ -1083,0 +1247,6 @@\n+      if (deoptimize_for_null_free) {\n+        tty->print_cr(\"Deoptimizing for patching null-free field reference\");\n+      }\n+      if (deoptimize_for_flat) {\n+        tty->print_cr(\"Deoptimizing for patching flat field reference\");\n+      }\n@@ -1533,0 +1703,1 @@\n+  tty->print_cr(\" _new_null_free_array_slowcase_cnt: %u\", _new_null_free_array_slowcase_cnt);\n@@ -1535,0 +1706,6 @@\n+  tty->print_cr(\" _load_flat_array_slowcase_cnt:   %u\", _load_flat_array_slowcase_cnt);\n+  tty->print_cr(\" _store_flat_array_slowcase_cnt:  %u\", _store_flat_array_slowcase_cnt);\n+  tty->print_cr(\" _substitutability_check_slowcase_cnt: %u\", _substitutability_check_slowcase_cnt);\n+  tty->print_cr(\" _buffer_inline_args_slowcase_cnt:%u\", _buffer_inline_args_slowcase_cnt);\n+  tty->print_cr(\" _buffer_inline_args_no_receiver_slowcase_cnt:%u\", _buffer_inline_args_no_receiver_slowcase_cnt);\n+\n@@ -1545,0 +1722,2 @@\n+  tty->print_cr(\" _throw_illegal_monitor_state_exception_count:  %u:\", _throw_illegal_monitor_state_exception_count);\n+  tty->print_cr(\" _throw_identity_exception_count:               %u:\", _throw_identity_exception_count);\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":186,"deletions":7,"binary":false,"changes":193,"status":"modified"},{"patch":"@@ -616,0 +616,2 @@\n+    assert(instr->as_LoadIndexed() == nullptr || !instr->as_LoadIndexed()->should_profile(), \"should not be optimized out\");\n+    assert(instr->as_StoreIndexed() == nullptr, \"should not be optimized out\");\n","filename":"src\/hotspot\/share\/c1\/c1_ValueMap.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+, _should_reexecute(false)\n@@ -45,1 +46,1 @@\n-ValueStack::ValueStack(ValueStack* copy_from, Kind kind, int bci)\n+ValueStack::ValueStack(ValueStack* copy_from, Kind kind, int bci, bool reexecute)\n@@ -50,0 +51,1 @@\n+  , _should_reexecute(reexecute)\n@@ -213,1 +215,0 @@\n-\n","filename":"src\/hotspot\/share\/c1\/c1_ValueStack.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -590,1 +590,1 @@\n-  if (!src_obj->fast_no_hash_check()) {\n+  if (!src_obj->fast_no_hash_check() && (!(EnableValhalla && src_obj->mark().is_inline_type()))) {\n@@ -594,0 +594,2 @@\n+    } else if (EnableValhalla) {\n+      fake_oop->set_mark(src_klass->prototype_header().copy_set_hash(src_hash));\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -50,0 +51,8 @@\n+bool CDSConfig::_module_patching_disables_cds = false;\n+bool CDSConfig::_java_base_module_patching_disables_cds = false;\n+\n+bool CDSConfig::is_valhalla_preview() {\n+  return Arguments::enable_preview() && EnableValhalla;\n+}\n+\n+\n@@ -103,0 +112,3 @@\n+    if (is_valhalla_preview()) {\n+      tmp.print_raw(\"_valhalla\");\n+    }\n@@ -285,2 +297,1 @@\n-    \"jdk.module.upgrade.path\",\n-    \"jdk.module.patch.0\"\n+    \"jdk.module.upgrade.path\"\n@@ -290,2 +301,1 @@\n-    \"--upgrade-module-path\",\n-    \"--patch-module\"\n+    \"--upgrade-module-path\"\n@@ -314,0 +324,6 @@\n+\n+  if (module_patching_disables_cds()) {\n+    vm_exit_during_initialization(\n+            \"Cannot use the following option when dumping the shared archive\", \"--patch-module\");\n+  }\n+\n@@ -338,0 +354,10 @@\n+\n+  if (module_patching_disables_cds()) {\n+    if (RequireSharedSpaces) {\n+      warning(\"CDS is disabled when the %s option is specified.\", \"--patch-module\");\n+    } else {\n+      log_info(cds)(\"CDS is disabled when the %s option is specified.\", \"--patch-module\");\n+    }\n+    return true;\n+  }\n+\n@@ -419,1 +445,1 @@\n-bool CDSConfig::check_vm_args_consistency(bool patch_mod_javabase, bool mode_flag_cmd_line) {\n+bool CDSConfig::check_vm_args_consistency(bool mode_flag_cmd_line) {\n@@ -477,1 +503,1 @@\n-  if (is_using_archive() && patch_mod_javabase) {\n+  if (is_using_archive() && java_base_module_patching_disables_cds() && module_patching_disables_cds()) {\n@@ -532,0 +558,4 @@\n+  if (is_valhalla_preview()) {\n+    \/\/ Not working yet -- e.g., HeapShared::oop_hash() needs to be implemented for value oops\n+    return false;\n+  }\n","filename":"src\/hotspot\/share\/cds\/cdsConfig.cpp","additions":36,"deletions":6,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -518,1 +518,5 @@\n-  if (k->local_interfaces()->length() != _interfaces->length()) {\n+  const int actual_num_interfaces = k->local_interfaces()->length();\n+  const int specified_num_interfaces = _interfaces->length(); \/\/ specified in classlist\n+  int expected_num_interfaces = actual_num_interfaces;\n+\n+  if (specified_num_interfaces != expected_num_interfaces) {\n@@ -522,1 +526,1 @@\n-          _interfaces->length(), k->local_interfaces()->length());\n+          specified_num_interfaces, expected_num_interfaces);\n","filename":"src\/hotspot\/share\/cds\/classListParser.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -31,0 +31,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -32,0 +34,1 @@\n+#include \"oops\/instanceKlass.inline.hpp\"\n@@ -55,0 +58,1 @@\n+\/\/ NOTE: this table must be in-sync with sun.jvm.hotspot.memory.FileMapInfo::populateMetadataTypeArray().\n@@ -64,1 +68,3 @@\n-  f(TypeArrayKlass)\n+  f(TypeArrayKlass) \\\n+  f(FlatArrayKlass) \\\n+  f(InlineKlass)\n","filename":"src\/hotspot\/share\/cds\/cppVtables.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -87,0 +87,65 @@\n+inline void CDSMustMatchFlags::do_print(outputStream* st, bool v) {\n+  st->print(\"%s\", v ? \"true\" : \"false\");\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, intx v) {\n+  st->print(\"%zd\", v);\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, uintx v) {\n+  st->print(\"%zu\", v);\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, double v) {\n+  st->print(\"%f\", v);\n+}\n+\n+void CDSMustMatchFlags::init() {\n+  assert(CDSConfig::is_dumping_archive(), \"sanity\");\n+  _max_name_width = 0;\n+\n+#define INIT_CDS_MUST_MATCH_FLAG(n) \\\n+  _v_##n = n; \\\n+  _max_name_width = MAX2(_max_name_width,strlen(#n));\n+  CDS_MUST_MATCH_FLAGS_DO(INIT_CDS_MUST_MATCH_FLAG);\n+#undef INIT_CDS_MUST_MATCH_FLAG\n+}\n+\n+bool CDSMustMatchFlags::runtime_check() const {\n+#define CHECK_CDS_MUST_MATCH_FLAG(n) \\\n+  if (_v_##n != n) { \\\n+    ResourceMark rm; \\\n+    stringStream ss; \\\n+    ss.print(\"VM option %s is different between dumptime (\", #n);  \\\n+    do_print(&ss, _v_ ## n); \\\n+    ss.print(\") and runtime (\"); \\\n+    do_print(&ss, n); \\\n+    ss.print(\")\"); \\\n+    log_info(cds)(\"%s\", ss.as_string()); \\\n+    return false; \\\n+  }\n+  CDS_MUST_MATCH_FLAGS_DO(CHECK_CDS_MUST_MATCH_FLAG);\n+#undef CHECK_CDS_MUST_MATCH_FLAG\n+\n+  return true;\n+}\n+\n+void CDSMustMatchFlags::print_info() const {\n+  LogTarget(Info, cds) lt;\n+  if (lt.is_enabled()) {\n+    LogStream ls(lt);\n+    ls.print_cr(\"Recorded VM flags during dumptime:\");\n+    print(&ls);\n+  }\n+}\n+\n+void CDSMustMatchFlags::print(outputStream* st) const {\n+#define PRINT_CDS_MUST_MATCH_FLAG(n) \\\n+  st->print(\"- %-s \", #n);                   \\\n+  st->sp(int(_max_name_width - strlen(#n))); \\\n+  do_print(st, _v_##n);                      \\\n+  st->cr();\n+  CDS_MUST_MATCH_FLAGS_DO(PRINT_CDS_MUST_MATCH_FLAG);\n+#undef PRINT_CDS_MUST_MATCH_FLAG\n+}\n+\n@@ -230,0 +295,1 @@\n+  _has_valhalla_patched_classes = CDSConfig::is_valhalla_preview();\n@@ -251,0 +317,1 @@\n+  _must_match.init();\n@@ -320,0 +387,2 @@\n+  st->print_cr(\"- has_valhalla_patched_classes    %d\", _has_valhalla_patched_classes);\n+  _must_match.print(st);\n@@ -1417,0 +1486,4 @@\n+  if (!header()->check_must_match_flags()) {\n+    return false;\n+  }\n+\n@@ -2637,0 +2710,18 @@\n+  if (is_static()) {\n+    const char* err = nullptr;\n+    if (CDSConfig::is_valhalla_preview()) {\n+      if (!_has_valhalla_patched_classes) {\n+        err = \"not created\";\n+      }\n+    } else {\n+      if (_has_valhalla_patched_classes) {\n+        err = \"created\";\n+      }\n+    }\n+    if (err != nullptr) {\n+      log_warning(cds)(\"This archive was %s with --enable-preview -XX:+EnableValhalla. It is \"\n+                         \"incompatible with the current JVM setting\", err);\n+      return false;\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":91,"deletions":0,"binary":false,"changes":91,"status":"modified"},{"patch":"@@ -1990,0 +1990,7 @@\n+\n+    if (CDSConfig::is_valhalla_preview() && strcmp(klass_name, \"jdk\/internal\/module\/ArchivedModuleGraph\") == 0) {\n+      \/\/ FIXME -- ArchivedModuleGraph doesn't work when java.base is patched with valhalla classes.\n+      i++;\n+      continue;\n+    }\n+\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -72,0 +72,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -116,1 +118,1 @@\n-\/\/ [0] All classes are loaded in MetaspaceShared::preload_classes(). All metadata are\n+\/\/ [0] All classes are loaded in MetaspaceShared::loadable_descriptors(). All metadata are\n@@ -841,1 +843,1 @@\n-void MetaspaceShared::preload_classes(TRAPS) {\n+void MetaspaceShared::loadable_descriptors(TRAPS) {\n@@ -886,1 +888,1 @@\n-  preload_classes(CHECK);\n+  loadable_descriptors(CHECK);\n@@ -1012,0 +1014,4 @@\n+  if (CDSConfig::is_valhalla_preview()) {\n+    log_info(cds)(\"Archived java heap is not yet supported with Valhalla preview\");\n+    return;\n+  }\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -557,0 +557,3 @@\n+        \/\/ If the array is a flat array, a larger part of it is modified than\n+        \/\/ the size of a reference. However, if OFFSET_ANY is given as\n+        \/\/ parameter to set_modified(), size is not taken into account.\n","filename":"src\/hotspot\/share\/ci\/bcEscapeAnalyzer.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -25,0 +25,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -29,0 +31,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -62,1 +65,1 @@\n-    return as_obj_array_klass()->element_klass()->as_klass();\n+    return element_klass()->as_klass();\n@@ -74,1 +77,1 @@\n-  } else {\n+  } else if (is_obj_array_klass()) {\n@@ -80,0 +83,2 @@\n+  } else {\n+    return as_flat_array_klass()->base_element_klass();\n@@ -99,1 +104,1 @@\n-ciArrayKlass* ciArrayKlass::make(ciType* element_type) {\n+ciArrayKlass* ciArrayKlass::make(ciType* element_type, bool null_free) {\n@@ -102,2 +107,36 @@\n-  } else {\n-    return ciObjArrayKlass::make(element_type->as_klass());\n+\n+  ciKlass* klass = element_type->as_klass();\n+  assert(!null_free || !klass->is_loaded() || klass->is_inlinetype() || klass->is_abstract() ||\n+         klass->is_java_lang_Object(), \"only value classes are null free\");\n+  if (null_free && klass->is_loaded() && klass->is_inlinetype()) {\n+    GUARDED_VM_ENTRY(\n+      EXCEPTION_CONTEXT;\n+      Klass* ak = nullptr;\n+      InlineKlass* vk = InlineKlass::cast(klass->get_Klass());\n+      if (vk->flat_array()) {\n+        \/\/ Current limitation: returns only non-atomic flat arrays, atomic layout not supported here\n+        ak = vk->flat_array_klass(LayoutKind::NON_ATOMIC_FLAT, THREAD);\n+      } else {\n+        ak = vk->null_free_reference_array(THREAD);\n+      }\n+      if (HAS_PENDING_EXCEPTION) {\n+        CLEAR_PENDING_EXCEPTION;\n+      } else if (ak->is_flatArray_klass()) {\n+        return CURRENT_THREAD_ENV->get_flat_array_klass(ak);\n+      } else if (ak->is_objArray_klass()) {\n+        return CURRENT_THREAD_ENV->get_obj_array_klass(ak);\n+      }\n+    )\n+  }\n+  return ciObjArrayKlass::make(klass);\n+}\n+\n+int ciArrayKlass::array_header_in_bytes() {\n+  return get_ArrayKlass()->array_header_in_bytes();\n+}\n+\n+ciInstance* ciArrayKlass::component_mirror_instance() const {\n+  GUARDED_VM_ENTRY(\n+    oop component_mirror = ArrayKlass::cast(get_Klass())->component_mirror();\n+    return CURRENT_ENV->get_instance(component_mirror);\n+  )\n","filename":"src\/hotspot\/share\/ci\/ciArrayKlass.cpp","additions":44,"deletions":5,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -479,1 +480,2 @@\n-      (sym->char_at(1) == JVM_SIGNATURE_ARRAY || sym->char_at(1) == JVM_SIGNATURE_CLASS)) {\n+      (sym->char_at(1) == JVM_SIGNATURE_ARRAY ||\n+       sym->char_at(1) == JVM_SIGNATURE_CLASS )) {\n@@ -492,1 +494,1 @@\n-      return ciObjArrayKlass::make_impl(elem_klass);\n+      return ciArrayKlass::make(elem_klass);\n@@ -518,0 +520,4 @@\n+  int i = 0;\n+  while (sym->char_at(i) == JVM_SIGNATURE_ARRAY) {\n+    i++;\n+  }\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -73,1 +73,1 @@\n-    _known_to_link_with_put(nullptr), _known_to_link_with_get(nullptr) {\n+  _original_holder(nullptr), _is_flat(false), _known_to_link_with_put(nullptr), _known_to_link_with_get(nullptr) {\n@@ -107,0 +107,3 @@\n+  _is_null_free = false;\n+  _null_marker_offset = -1;\n+\n@@ -218,0 +221,26 @@\n+\/\/ Special copy constructor used to flatten inline type fields by\n+\/\/ copying the fields of the inline type to a new holder klass.\n+ciField::ciField(ciField* field, ciInstanceKlass* holder, int offset, bool is_final) {\n+  assert(field->holder()->is_inlinetype() || field->holder()->is_abstract(), \"should only be used for inline type field flattening\");\n+  \/\/ Set the is_final flag\n+  jint final = is_final ? JVM_ACC_FINAL : ~JVM_ACC_FINAL;\n+  AccessFlags flags(field->flags().as_int() & final);\n+  _flags = ciFlags(flags);\n+  _holder = holder;\n+  _offset = offset;\n+  \/\/ Copy remaining fields\n+  _name = field->_name;\n+  _signature = field->_signature;\n+  _type = field->_type;\n+  \/\/ Trust final flat fields\n+  _is_constant = is_final;\n+  _known_to_link_with_put = field->_known_to_link_with_put;\n+  _known_to_link_with_get = field->_known_to_link_with_get;\n+  _constant_value = field->_constant_value;\n+  assert(!field->is_flat(), \"field must not be flat\");\n+  _is_flat = false;\n+  _is_null_free = field->_is_null_free;\n+  _null_marker_offset = field->_null_marker_offset;\n+  _original_holder = (field->_original_holder != nullptr) ? field->_original_holder : field->_holder;\n+}\n+\n@@ -235,0 +264,3 @@\n+  \/\/ Trust final fields in inline type buffers\n+  if (holder->is_inlinetype())\n+    return true;\n@@ -259,1 +291,1 @@\n-  Klass* field_holder = fd->field_holder();\n+  InstanceKlass* field_holder = fd->field_holder();\n@@ -262,0 +294,9 @@\n+  _is_flat = fd->is_flat();\n+  _is_null_free = fd->is_null_free_inline_type();\n+  if (fd->has_null_marker()) {\n+    InlineLayoutInfo* li = field_holder->inline_layout_info_adr(fd->index());\n+    _null_marker_offset = li->null_marker_offset();\n+  } else {\n+    _null_marker_offset = -1;\n+  }\n+  _original_holder = nullptr;\n@@ -344,1 +385,3 @@\n-  ciKlass* type = CURRENT_ENV->get_klass_by_name_impl(_holder, constantPoolHandle(), _signature, false);\n+  \/\/ Use original holder for fields that came in through flattening\n+  ciKlass* accessing_klass = (_original_holder != nullptr) ? _original_holder : _holder;\n+  ciKlass* type = CURRENT_ENV->get_klass_by_name_impl(accessing_klass, constantPoolHandle(), _signature, false);\n@@ -457,0 +500,3 @@\n+  tty->print(\" is_flat=%s\", bool_to_str(_is_flat));\n+  tty->print(\" is_null_free=%s\", bool_to_str(_is_null_free));\n+  tty->print(\" null_marker_offset=%s\", bool_to_str(_null_marker_offset));\n","filename":"src\/hotspot\/share\/ci\/ciField.cpp","additions":49,"deletions":3,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -43,3 +43,0 @@\n-  if (is_super()) {\n-    st->print(\",super\");\n-  }\n","filename":"src\/hotspot\/share\/ci\/ciFlags.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,78 @@\n+\/*\n+ * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n+#include \"ci\/ciInstanceKlass.hpp\"\n+#include \"ci\/ciObjArrayKlass.hpp\"\n+#include \"ci\/ciSymbol.hpp\"\n+#include \"ci\/ciUtilities.hpp\"\n+#include \"ci\/ciUtilities.inline.hpp\"\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n+\n+\/\/ ciFlatArrayKlass\n+\/\/\n+\/\/ This class represents a Klass* in the HotSpot virtual machine\n+\/\/ whose Klass part is a FlatArrayKlass.\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciFlatArrayKlass::ciFlatArrayKlass\n+\/\/\n+\/\/ Constructor for loaded inline type array klasses.\n+ciFlatArrayKlass::ciFlatArrayKlass(Klass* h_k) : ciArrayKlass(h_k) {\n+  assert(get_Klass()->is_flatArray_klass(), \"wrong type\");\n+  InlineKlass* element_Klass = get_FlatArrayKlass()->element_klass();\n+  _base_element_klass = CURRENT_ENV->get_klass(element_Klass);\n+  assert(_base_element_klass->is_inlinetype(), \"bad base klass\");\n+  if (dimension() == 1) {\n+    _element_klass = _base_element_klass;\n+  } else {\n+    _element_klass = nullptr;\n+  }\n+  if (!ciObjectFactory::is_initialized()) {\n+    assert(_element_klass->is_java_lang_Object(), \"only arrays of object are shared\");\n+  }\n+}\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciFlatArrayKlass::element_klass\n+\/\/\n+\/\/ What is the one-level element type of this array?\n+ciKlass* ciFlatArrayKlass::element_klass() {\n+  if (_element_klass == nullptr) {\n+    assert(dimension() > 1, \"_element_klass should not be nullptr\");\n+    assert(is_loaded(), \"FlatArrayKlass must be loaded\");\n+    \/\/ Produce the element klass.\n+    VM_ENTRY_MARK;\n+    Klass* element_Klass = get_FlatArrayKlass()->element_klass();\n+    _element_klass = CURRENT_THREAD_ENV->get_klass(element_Klass);\n+  }\n+  return _element_klass;\n+}\n+\n+ciKlass* ciFlatArrayKlass::exact_klass() {\n+  assert(element_klass()->is_loaded() && element_klass()->as_inline_klass()->exact_klass() != nullptr, \"must have exact klass\");\n+  return this;\n+}\n","filename":"src\/hotspot\/share\/ci\/ciFlatArrayKlass.cpp","additions":78,"deletions":0,"binary":false,"changes":78,"status":"added"},{"patch":"@@ -0,0 +1,161 @@\n+\/*\n+ * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"ci\/ciField.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n+#include \"ci\/ciUtilities.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n+\n+int ciInlineKlass::compute_nonstatic_fields() {\n+  int result = ciInstanceKlass::compute_nonstatic_fields();\n+\n+  \/\/ Abstract value classes can also have declared fields.\n+  ciInstanceKlass* super_klass = super();\n+  GrowableArray<ciField*>* super_klass_fields = nullptr;\n+  if (super_klass != nullptr && super_klass->has_nonstatic_fields()) {\n+    int super_flen = super_klass->nof_nonstatic_fields();\n+    super_klass_fields = super_klass->_nonstatic_fields;\n+    assert(super_flen == 0 || super_klass_fields != nullptr, \"first get nof_fields\");\n+  }\n+\n+  \/\/ Compute declared non-static fields (without flattening of inline type fields)\n+  GrowableArray<ciField*>* fields = nullptr;\n+  GUARDED_VM_ENTRY(fields = compute_nonstatic_fields_impl(super_klass_fields, false \/* no flattening *\/);)\n+  Arena* arena = CURRENT_ENV->arena();\n+  _declared_nonstatic_fields = (fields != nullptr) ? fields : new (arena) GrowableArray<ciField*>(arena, 0, 0, 0);\n+  return result;\n+}\n+\n+\/\/ Offset of the first field in the inline type\n+int ciInlineKlass::payload_offset() const {\n+  GUARDED_VM_ENTRY(return to_InlineKlass()->payload_offset();)\n+}\n+\n+\/\/ Returns the index of the field with the given offset. If the field at 'offset'\n+\/\/ belongs to a flat field, return the index of the field in the inline type of the flat field.\n+int ciInlineKlass::field_index_by_offset(int offset) {\n+  assert(contains_field_offset(offset), \"invalid field offset\");\n+  int best_offset = 0;\n+  int best_index = -1;\n+  \/\/ Search the field with the given offset\n+  for (int i = 0; i < nof_declared_nonstatic_fields(); ++i) {\n+    int field_offset = _declared_nonstatic_fields->at(i)->offset_in_bytes();\n+    if (field_offset == offset) {\n+      \/\/ Exact match\n+      return i;\n+    } else if (field_offset < offset && field_offset > best_offset) {\n+      \/\/ No exact match. Save the index of the field with the closest offset that\n+      \/\/ is smaller than the given field offset. This index corresponds to the\n+      \/\/ flat field that holds the field we are looking for.\n+      best_offset = field_offset;\n+      best_index = i;\n+    }\n+  }\n+  assert(best_index >= 0, \"field not found\");\n+  assert(best_offset == offset || _declared_nonstatic_fields->at(best_index)->type()->is_inlinetype(), \"offset should match for non-inline types\");\n+  return best_index;\n+}\n+\n+\/\/ Are arrays containing this inline type flat arrays?\n+bool ciInlineKlass::flat_in_array() const {\n+  GUARDED_VM_ENTRY(return to_InlineKlass()->flat_array();)\n+}\n+\n+\/\/ Can this inline type be passed as multiple values?\n+bool ciInlineKlass::can_be_passed_as_fields() const {\n+  GUARDED_VM_ENTRY(return to_InlineKlass()->can_be_passed_as_fields();)\n+}\n+\n+\/\/ Can this inline type be returned as multiple values?\n+bool ciInlineKlass::can_be_returned_as_fields() const {\n+  GUARDED_VM_ENTRY(return to_InlineKlass()->can_be_returned_as_fields();)\n+}\n+\n+bool ciInlineKlass::is_empty() {\n+  \/\/ Do not use InlineKlass::is_empty_inline_type here because it does\n+  \/\/ consider the container empty even if fields of empty inline types\n+  \/\/ are not flat\n+  return nof_declared_nonstatic_fields() == 0;\n+}\n+\n+\/\/ When passing an inline type's fields as arguments, count the number\n+\/\/ of argument slots that are needed\n+int ciInlineKlass::inline_arg_slots() {\n+  VM_ENTRY_MARK;\n+  const Array<SigEntry>* sig_vk = get_InlineKlass()->extended_sig();\n+  int slots = 0;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA || bt == T_VOID) {\n+      continue;\n+    }\n+    slots += type2size[bt];\n+  }\n+  return slots;\n+}\n+\n+ciInstance* ciInlineKlass::default_instance() const {\n+  VM_ENTRY_MARK;\n+  oop default_value = to_InlineKlass()->default_value();\n+  return CURRENT_ENV->get_instance(default_value);\n+}\n+\n+bool ciInlineKlass::contains_oops() const {\n+  GUARDED_VM_ENTRY(return get_InlineKlass()->contains_oops();)\n+}\n+\n+int ciInlineKlass::oop_count() const {\n+  GUARDED_VM_ENTRY(return get_InlineKlass()->nonstatic_oop_count();)\n+}\n+\n+address ciInlineKlass::pack_handler() const {\n+  GUARDED_VM_ENTRY(return get_InlineKlass()->pack_handler();)\n+}\n+\n+address ciInlineKlass::unpack_handler() const {\n+  GUARDED_VM_ENTRY(return get_InlineKlass()->unpack_handler();)\n+}\n+\n+InlineKlass* ciInlineKlass::get_InlineKlass() const {\n+  GUARDED_VM_ENTRY(return to_InlineKlass();)\n+}\n+\n+\/\/ Convert payload size in bytes to corresponding BasicType\n+BasicType ciInlineKlass::payload_size_to_basic_type() const {\n+  VM_ENTRY_MARK\n+  int size = get_InlineKlass()->payload_size_in_bytes();\n+  BasicType bt;\n+  if (size == sizeof(jlong)) {\n+    bt = T_LONG;\n+  } else if (size == sizeof(jint)) {\n+    bt = T_INT;\n+  } else if (size == sizeof(jshort)) {\n+    bt = T_SHORT;\n+  } else if (size == sizeof(jbyte)) {\n+    bt = T_BYTE;\n+  } else {\n+    assert(false, \"Unsupported size: %d\", size);\n+  }\n+  return bt;\n+}\n","filename":"src\/hotspot\/share\/ci\/ciInlineKlass.cpp","additions":161,"deletions":0,"binary":false,"changes":161,"status":"added"},{"patch":"@@ -42,1 +42,1 @@\n-ciType* ciInstance::java_mirror_type() {\n+ciType* ciInstance::java_mirror_type(bool* is_null_free_array) {\n@@ -55,0 +55,3 @@\n+    if (is_null_free_array != nullptr && (k->is_flatArray_klass() || (k->is_objArray_klass() && k->is_null_free_array_klass()))) {\n+      *is_null_free_array = true;\n+    }\n@@ -110,1 +113,3 @@\n-  assert(field->is_static() || klass()->is_subclass_of(field->holder()), \"invalid access - must be subclass\");\n+  assert(field->is_static() || field->holder()->is_inlinetype() || klass()->is_subclass_of(field->holder()),\n+         \"invalid access - must be subclass\");\n+\n","filename":"src\/hotspot\/share\/ci\/ciInstance.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -38,0 +40,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -121,2 +124,3 @@\n-                                 jobject loader, jobject protection_domain)\n-  : ciKlass(name, T_OBJECT)\n+                                 jobject loader, jobject protection_domain,\n+                                 BasicType bt)\n+  : ciKlass(name, bt)\n@@ -127,1 +131,1 @@\n-  _nonstatic_fields = nullptr;\n+  _nonstatic_fields = nullptr;         \/\/ initialized lazily by compute_nonstatic_fields\n@@ -339,1 +343,1 @@\n-    _flags.print_klass_flags();\n+    _flags.print_klass_flags(st);\n@@ -343,1 +347,1 @@\n-      _super->print_name();\n+      _super->print_name_on(st);\n@@ -437,0 +441,23 @@\n+ciField* ciInstanceKlass::get_non_flat_field_by_offset(int field_offset) {\n+  if (super() != nullptr && super()->has_nonstatic_fields()) {\n+    ciField* f = super()->get_non_flat_field_by_offset(field_offset);\n+    if (f != nullptr) {\n+      return f;\n+    }\n+  }\n+\n+  VM_ENTRY_MARK;\n+  InstanceKlass* k = get_instanceKlass();\n+  Arena* arena = CURRENT_ENV->arena();\n+  for (JavaFieldStream fs(k); !fs.done(); fs.next()) {\n+    if (fs.access_flags().is_static())  continue;\n+    fieldDescriptor& fd = fs.field_descriptor();\n+    if (fd.offset() == field_offset) {\n+      ciField* f = new (arena) ciField(&fd);\n+      return f;\n+    }\n+  }\n+\n+  return nullptr;\n+}\n+\n@@ -495,6 +522,1 @@\n-  int flen = fields->length();\n-\n-  \/\/ Now sort them by offset, ascending.\n-  \/\/ (In principle, they could mix with superclass fields.)\n-  fields->sort(sort_field_by_offset);\n-  return flen;\n+  return fields->length();\n@@ -504,3 +526,1 @@\n-GrowableArray<ciField*>*\n-ciInstanceKlass::compute_nonstatic_fields_impl(GrowableArray<ciField*>*\n-                                               super_fields) {\n+GrowableArray<ciField*>* ciInstanceKlass::compute_nonstatic_fields_impl(GrowableArray<ciField*>* super_fields, bool is_flat) {\n@@ -518,1 +538,1 @@\n-  if (flen == 0) {\n+  if (flen == 0 && !is_inlinetype()) {\n@@ -532,2 +552,22 @@\n-    ciField* field = new (arena) ciField(&fd);\n-    fields->append(field);\n+    if (fd.is_flat() && is_flat) {\n+      \/\/ Inline type fields are embedded\n+      int field_offset = fd.offset();\n+      \/\/ Get InlineKlass and adjust number of fields\n+      Klass* k = get_instanceKlass()->get_inline_type_field_klass(fd.index());\n+      ciInlineKlass* vk = CURRENT_ENV->get_klass(k)->as_inline_klass();\n+      flen += vk->nof_nonstatic_fields() - 1;\n+      \/\/ Iterate over fields of the flat inline type and copy them to 'this'\n+      for (int i = 0; i < vk->nof_nonstatic_fields(); ++i) {\n+        ciField* flat_field = vk->nonstatic_field_at(i);\n+        \/\/ Adjust offset to account for missing oop header\n+        int offset = field_offset + (flat_field->offset_in_bytes() - vk->payload_offset());\n+        \/\/ A flat field can be treated as final if the non-flat\n+        \/\/ field is declared final or the holder klass is an inline type itself.\n+        bool is_final = fd.is_final() || is_inlinetype();\n+        ciField* field = new (arena) ciField(flat_field, this, offset, is_final);\n+        fields->append(field);\n+      }\n+    } else {\n+      ciField* field = new (arena) ciField(&fd);\n+      fields->append(field);\n+    }\n@@ -536,0 +576,3 @@\n+  \/\/ Now sort them by offset, ascending.\n+  \/\/ (In principle, they could mix with superclass fields.)\n+  fields->sort(sort_field_by_offset);\n@@ -642,0 +685,16 @@\n+bool ciInstanceKlass::can_be_inline_klass(bool is_exact) {\n+  if (!EnableValhalla) {\n+    return false;\n+  }\n+  if (!is_loaded() || is_inlinetype()) {\n+    \/\/ Not loaded or known to be an inline klass\n+    return true;\n+  }\n+  if (!is_exact) {\n+    \/\/ Not exact, check if this is a valid super for an inline klass\n+    VM_ENTRY_MARK;\n+    return !get_instanceKlass()->access_flags().is_identity_class() || is_java_lang_Object() ;\n+  }\n+  return false;\n+}\n+\n@@ -650,1 +709,2 @@\n-class StaticFinalFieldPrinter : public FieldClosure {\n+class StaticFieldPrinter : public FieldClosure {\n+protected:\n@@ -652,0 +712,8 @@\n+public:\n+  StaticFieldPrinter(outputStream* out) :\n+    _out(out) {\n+  }\n+  void do_field_helper(fieldDescriptor* fd, oop obj, bool is_flat);\n+};\n+\n+class StaticFinalFieldPrinter : public StaticFieldPrinter {\n@@ -655,2 +723,1 @@\n-    _out(out),\n-    _holder(holder) {\n+    StaticFieldPrinter(out), _holder(holder) {\n@@ -661,46 +728,59 @@\n-      oop mirror = fd->field_holder()->java_mirror();\n-      _out->print(\"staticfield %s %s %s \", _holder, fd->name()->as_quoted_ascii(), fd->signature()->as_quoted_ascii());\n-      BasicType field_type = fd->field_type();\n-      switch (field_type) {\n-        case T_BYTE:    _out->print_cr(\"%d\", mirror->byte_field(fd->offset()));   break;\n-        case T_BOOLEAN: _out->print_cr(\"%d\", mirror->bool_field(fd->offset()));   break;\n-        case T_SHORT:   _out->print_cr(\"%d\", mirror->short_field(fd->offset()));  break;\n-        case T_CHAR:    _out->print_cr(\"%d\", mirror->char_field(fd->offset()));   break;\n-        case T_INT:     _out->print_cr(\"%d\", mirror->int_field(fd->offset()));    break;\n-        case T_LONG:    _out->print_cr(INT64_FORMAT, (int64_t)(mirror->long_field(fd->offset())));   break;\n-        case T_FLOAT: {\n-          float f = mirror->float_field(fd->offset());\n-          _out->print_cr(\"%d\", *(int*)&f);\n-          break;\n-        }\n-        case T_DOUBLE: {\n-          double d = mirror->double_field(fd->offset());\n-          _out->print_cr(INT64_FORMAT, *(int64_t*)&d);\n-          break;\n-        }\n-        case T_ARRAY:  \/\/ fall-through\n-        case T_OBJECT: {\n-          oop value =  mirror->obj_field_acquire(fd->offset());\n-          if (value == nullptr) {\n-            if (field_type == T_ARRAY) {\n-              _out->print(\"%d\", -1);\n-            }\n-            _out->cr();\n-          } else if (value->is_instance()) {\n-            assert(field_type == T_OBJECT, \"\");\n-            if (value->is_a(vmClasses::String_klass())) {\n-              const char* ascii_value = java_lang_String::as_quoted_ascii(value);\n-              _out->print_cr(\"\\\"%s\\\"\", (ascii_value != nullptr) ? ascii_value : \"\");\n-            } else {\n-              const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n-              _out->print_cr(\"%s\", klass_name);\n-            }\n-          } else if (value->is_array()) {\n-            typeArrayOop ta = (typeArrayOop)value;\n-            _out->print(\"%d\", ta->length());\n-            if (value->is_objArray()) {\n-              objArrayOop oa = (objArrayOop)value;\n-              const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n-              _out->print(\" %s\", klass_name);\n-            }\n-            _out->cr();\n+      InstanceKlass* holder = fd->field_holder();\n+      oop mirror = holder->java_mirror();\n+      _out->print(\"staticfield %s %s \", _holder, fd->name()->as_quoted_ascii());\n+      BasicType bt = fd->field_type();\n+      if (bt != T_OBJECT && bt != T_ARRAY) {\n+        _out->print(\"%s \", fd->signature()->as_quoted_ascii());\n+      }\n+      do_field_helper(fd, mirror, false);\n+      _out->cr();\n+    }\n+  }\n+};\n+\n+class InlineTypeFieldPrinter : public StaticFieldPrinter {\n+  oop _obj;\n+public:\n+  InlineTypeFieldPrinter(outputStream* out, oop obj) :\n+    StaticFieldPrinter(out), _obj(obj) {\n+  }\n+  void do_field(fieldDescriptor* fd) {\n+    do_field_helper(fd, _obj, true);\n+    _out->print(\" \");\n+  }\n+};\n+\n+void StaticFieldPrinter::do_field_helper(fieldDescriptor* fd, oop mirror, bool is_flat) {\n+  BasicType field_type = fd->field_type();\n+  switch (field_type) {\n+    case T_BYTE:    _out->print(\"%d\", mirror->byte_field(fd->offset()));   break;\n+    case T_BOOLEAN: _out->print(\"%d\", mirror->bool_field(fd->offset()));   break;\n+    case T_SHORT:   _out->print(\"%d\", mirror->short_field(fd->offset()));  break;\n+    case T_CHAR:    _out->print(\"%d\", mirror->char_field(fd->offset()));   break;\n+    case T_INT:     _out->print(\"%d\", mirror->int_field(fd->offset()));    break;\n+    case T_LONG:    _out->print(INT64_FORMAT, (int64_t)(mirror->long_field(fd->offset())));   break;\n+    case T_FLOAT: {\n+      float f = mirror->float_field(fd->offset());\n+      _out->print(\"%d\", *(int*)&f);\n+      break;\n+    }\n+    case T_DOUBLE: {\n+      double d = mirror->double_field(fd->offset());\n+      _out->print(INT64_FORMAT, *(int64_t*)&d);\n+      break;\n+    }\n+    case T_ARRAY:  \/\/ fall-through\n+    case T_OBJECT:\n+      if (!fd->is_null_free_inline_type()) {\n+        _out->print(\"%s \", fd->signature()->as_quoted_ascii());\n+        oop value =  mirror->obj_field_acquire(fd->offset());\n+        if (value == nullptr) {\n+          if (field_type == T_ARRAY) {\n+            _out->print(\"%d\", -1);\n+          }\n+          _out->cr();\n+        } else if (value->is_instance()) {\n+          assert(field_type == T_OBJECT, \"\");\n+          if (value->is_a(vmClasses::String_klass())) {\n+            const char* ascii_value = java_lang_String::as_quoted_ascii(value);\n+            _out->print(\"\\\"%s\\\"\", (ascii_value != nullptr) ? ascii_value : \"\");\n@@ -708,1 +788,2 @@\n-            ShouldNotReachHere();\n+            const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n+            _out->print(\"%s\", klass_name);\n@@ -710,3 +791,9 @@\n-          break;\n-        }\n-        default:\n+        } else if (value->is_array()) {\n+          typeArrayOop ta = (typeArrayOop)value;\n+          _out->print(\"%d\", ta->length());\n+          if (value->is_objArray() || value->is_flatArray()) {\n+            objArrayOop oa = (objArrayOop)value;\n+            const char* klass_name  = value->klass()->name()->as_quoted_ascii();\n+            _out->print(\" %s\", klass_name);\n+          }\n+        } else {\n@@ -715,1 +802,26 @@\n-    }\n+        break;\n+      } else {\n+        \/\/ handling of null free inline type\n+        ResetNoHandleMark rnhm;\n+        Thread* THREAD = Thread::current();\n+        SignatureStream ss(fd->signature(), false);\n+        Symbol* name = ss.as_symbol();\n+        assert(!HAS_PENDING_EXCEPTION, \"can resolve klass?\");\n+        InstanceKlass* holder = fd->field_holder();\n+        InstanceKlass* k = SystemDictionary::find_instance_klass(THREAD, name,\n+                                                                 Handle(THREAD, holder->class_loader()));\n+        assert(k != nullptr && !HAS_PENDING_EXCEPTION, \"can resolve klass?\");\n+        InlineKlass* vk = InlineKlass::cast(k);\n+        oop obj;\n+        if (is_flat) {\n+          int field_offset = fd->offset() - vk->payload_offset();\n+          obj = cast_to_oop(cast_from_oop<address>(mirror) + field_offset);\n+        } else {\n+          obj = mirror->obj_field_acquire(fd->offset());\n+        }\n+        InlineTypeFieldPrinter print_field(_out, obj);\n+        vk->do_nonstatic_fields(&print_field);\n+        break;\n+      }\n+    default:\n+      ShouldNotReachHere();\n@@ -717,1 +829,1 @@\n-};\n+}\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.cpp","additions":184,"deletions":72,"binary":false,"changes":256,"status":"modified"},{"patch":"@@ -228,0 +228,9 @@\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciKlass::prototype_header\n+markWord ciKlass::prototype_header() const {\n+  assert(is_loaded(), \"not loaded\");\n+  GUARDED_VM_ENTRY(\n+    return get_Klass()->prototype_header();\n+  )\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciKlass.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -663,0 +664,54 @@\n+bool ciMethod::array_access_profiled_type(int bci, ciKlass*& array_type, ciKlass*& element_type, ProfilePtrKind& element_ptr, bool &flat_array, bool &null_free_array) {\n+  if (method_data() != nullptr && method_data()->is_mature()) {\n+    ciProfileData* data = method_data()->bci_to_data(bci);\n+    if (data != nullptr) {\n+      if (data->is_ArrayLoadData()) {\n+        ciArrayLoadData* array_access = (ciArrayLoadData*) data->as_ArrayLoadData();\n+        array_type = array_access->array()->valid_type();\n+        element_type = array_access->element()->valid_type();\n+        element_ptr = array_access->element()->ptr_kind();\n+        flat_array = array_access->flat_array();\n+        null_free_array = array_access->null_free_array();\n+        return true;\n+      } else if (data->is_ArrayStoreData()) {\n+        ciArrayStoreData* array_access = (ciArrayStoreData*) data->as_ArrayStoreData();\n+        array_type = array_access->array()->valid_type();\n+        flat_array = array_access->flat_array();\n+        null_free_array = array_access->null_free_array();\n+        ciCallProfile call_profile = call_profile_at_bci(bci);\n+        if (call_profile.morphism() == 1) {\n+          element_type = call_profile.receiver(0);\n+        } else {\n+          element_type = nullptr;\n+        }\n+        if (!array_access->null_seen()) {\n+          element_ptr = ProfileNeverNull;\n+        } else if (call_profile.count() == 0) {\n+          element_ptr = ProfileAlwaysNull;\n+        } else {\n+          element_ptr = ProfileMaybeNull;\n+        }\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+bool ciMethod::acmp_profiled_type(int bci, ciKlass*& left_type, ciKlass*& right_type, ProfilePtrKind& left_ptr, ProfilePtrKind& right_ptr, bool &left_inline_type, bool &right_inline_type) {\n+  if (method_data() != nullptr && method_data()->is_mature()) {\n+    ciProfileData* data = method_data()->bci_to_data(bci);\n+    if (data != nullptr && data->is_ACmpData()) {\n+      ciACmpData* acmp = (ciACmpData*)data->as_ACmpData();\n+      left_type = acmp->left()->valid_type();\n+      right_type = acmp->right()->valid_type();\n+      left_ptr = acmp->left()->ptr_kind();\n+      right_ptr = acmp->right()->ptr_kind();\n+      left_inline_type = acmp->left_inline_type();\n+      right_inline_type = acmp->right_inline_type();\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -969,1 +1024,1 @@\n-\/\/ ciMethod::is_object_initializer\n+\/\/ ciMethod::is_object_constructor\n@@ -971,2 +1026,5 @@\n-bool ciMethod::is_object_initializer() const {\n-   return name() == ciSymbols::object_initializer_name();\n+bool ciMethod::is_object_constructor() const {\n+   return (name() == ciSymbols::object_initializer_name()\n+           && signature()->return_type()->is_void());\n+   \/\/ Note:  We can't test is_static, because that would\n+   \/\/ require the method to be loaded.  Sometimes it isn't.\n@@ -1494,0 +1552,19 @@\n+\n+bool ciMethod::is_scalarized_arg(int idx) const {\n+  VM_ENTRY_MARK;\n+  return get_Method()->is_scalarized_arg(idx);\n+}\n+\n+bool ciMethod::has_scalarized_args() const {\n+  VM_ENTRY_MARK;\n+  return get_Method()->has_scalarized_args();\n+}\n+\n+const GrowableArray<SigEntry>* ciMethod::get_sig_cc() const {\n+  VM_ENTRY_MARK;\n+  if (get_Method()->adapter() == nullptr) {\n+    return nullptr;\n+  }\n+  return get_Method()->adapter()->get_sig_cc();\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciMethod.cpp","additions":80,"deletions":3,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -333,1 +333,1 @@\n-void ciReturnTypeEntry::translate_type_data_from(const ReturnTypeEntry* ret) {\n+void ciSingleTypeEntry::translate_type_data_from(const SingleTypeEntry* ret) {\n@@ -338,1 +338,1 @@\n-    set_type(ReturnTypeEntry::with_status((Klass*)nullptr, k));\n+    set_type(SingleTypeEntry::with_status((Klass*)nullptr, k));\n@@ -389,0 +389,6 @@\n+  case DataLayout::array_store_data_tag:\n+    return new ciArrayStoreData(data_layout);\n+  case DataLayout::array_load_data_tag:\n+    return new ciArrayLoadData(data_layout);\n+  case DataLayout::acmp_data_tag:\n+    return new ciACmpData(data_layout);\n@@ -806,0 +812,20 @@\n+      } else if (pdata->is_CallTypeData()) {\n+        ciCallTypeData* call_type_data = (ciCallTypeData*)pdata;\n+        dump_replay_data_call_type_helper<ciCallTypeData>(out, round, count, call_type_data);\n+      } else if (pdata->is_ArrayStoreData()) {\n+        ciArrayStoreData* array_store_data = (ciArrayStoreData*)pdata;\n+        dump_replay_data_type_helper(out, round, count, array_store_data, ciArrayStoreData::array_offset(),\n+                                     array_store_data->array()->valid_type());\n+        dump_replay_data_receiver_type_helper<ciArrayStoreData>(out, round, count, array_store_data);\n+      } else if (pdata->is_ArrayLoadData()) {\n+        ciArrayLoadData* array_load_data = (ciArrayLoadData*)pdata;\n+        dump_replay_data_type_helper(out, round, count, array_load_data, ciArrayLoadData::array_offset(),\n+                                     array_load_data->array()->valid_type());\n+        dump_replay_data_type_helper(out, round, count, array_load_data, ciArrayLoadData::element_offset(),\n+                                     array_load_data->element()->valid_type());\n+      } else if (pdata->is_ACmpData()) {\n+        ciACmpData* acmp_data = (ciACmpData*)pdata;\n+        dump_replay_data_type_helper(out, round, count, acmp_data, ciACmpData::left_offset(),\n+                                     acmp_data->left()->valid_type());\n+        dump_replay_data_type_helper(out, round, count, acmp_data, ciACmpData::right_offset(),\n+                                     acmp_data->right()->valid_type());\n@@ -809,3 +835,0 @@\n-      } else if (pdata->is_CallTypeData()) {\n-          ciCallTypeData* call_type_data = (ciCallTypeData*)pdata;\n-          dump_replay_data_call_type_helper<ciCallTypeData>(out, round, count, call_type_data);\n@@ -894,1 +917,1 @@\n-void ciReturnTypeEntry::print_data_on(outputStream* st) const {\n+void ciSingleTypeEntry::print_data_on(outputStream* st) const {\n@@ -967,0 +990,31 @@\n+\n+void ciArrayStoreData::print_data_on(outputStream* st, const char* extra) const {\n+  print_shared(st, \"ciArrayStoreData\", extra);\n+  tab(st, true);\n+  st->print(\"array\");\n+  array()->print_data_on(st);\n+  tab(st, true);\n+  st->print(\"element\");\n+  rtd_super()->print_receiver_data_on(st);\n+}\n+\n+void ciArrayLoadData::print_data_on(outputStream* st, const char* extra) const {\n+  print_shared(st, \"ciArrayLoadData\", extra);\n+  tab(st, true);\n+  st->print(\"array\");\n+  array()->print_data_on(st);\n+  tab(st, true);\n+  st->print(\"element\");\n+  element()->print_data_on(st);\n+}\n+\n+void ciACmpData::print_data_on(outputStream* st, const char* extra) const {\n+  BranchData::print_data_on(st, extra);\n+  st->cr();\n+  tab(st, true);\n+  st->print(\"left\");\n+  left()->print_data_on(st);\n+  tab(st, true);\n+  st->print(\"right\");\n+  right()->print_data_on(st);\n+}\n","filename":"src\/hotspot\/share\/ci\/ciMethodData.cpp","additions":60,"deletions":6,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -46,0 +47,5 @@\n+\n+bool ciObjArray::is_null_free() {\n+  VM_ENTRY_MARK;\n+  return get_objArrayOop()->is_null_free_array();\n+}\n","filename":"src\/hotspot\/share\/ci\/ciObjArray.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -66,8 +67,9 @@\n-    _base_element_klass = base_element_klass;\n-    assert(_base_element_klass->is_instance_klass() ||\n-           _base_element_klass->is_type_array_klass(), \"bad base klass\");\n-    if (dimension == 1) {\n-      _element_klass = base_element_klass;\n-    } else {\n-      _element_klass = nullptr;\n-    }\n+  _base_element_klass = base_element_klass;\n+  assert(_base_element_klass->is_instance_klass() ||\n+         _base_element_klass->is_type_array_klass() ||\n+         _base_element_klass->is_flat_array_klass(), \"bad base klass\");\n+  if (dimension == 1) {\n+    _element_klass = base_element_klass;\n+  } else {\n+    _element_klass = nullptr;\n+  }\n@@ -118,1 +120,0 @@\n-\n@@ -137,1 +138,0 @@\n-\n@@ -150,2 +150,1 @@\n-  \/\/ The array klass was unable to be made or the element klass was\n-  \/\/ not loaded.\n+  \/\/ The array klass was unable to be made or the element klass was not loaded.\n@@ -178,0 +177,3 @@\n+  if (!is_loaded()) {\n+    return nullptr;\n+  }\n@@ -181,0 +183,5 @@\n+    \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+    \/\/ is null-free due to null-free [LMyValue <: null-able [LMyValue.\n+    if (ik->is_inlinetype() && !is_elem_null_free()) {\n+      return nullptr;\n+    }\n@@ -189,0 +196,4 @@\n+\n+bool ciObjArrayKlass::is_elem_null_free() const {\n+  GUARDED_VM_ENTRY(return get_Klass()->is_null_free_array_klass();)\n+}\n","filename":"src\/hotspot\/share\/ci\/ciObjArrayKlass.cpp","additions":23,"deletions":12,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -25,0 +25,2 @@\n+#include \"ci\/ciFlatArray.hpp\"\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -44,0 +46,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -367,0 +370,3 @@\n+  } else if (o->is_flatArray()) {\n+    flatArrayHandle h_ta(THREAD, (flatArrayOop)o);\n+    return new (arena()) ciFlatArray(h_ta);\n@@ -386,1 +392,3 @@\n-    if (k->is_instance_klass()) {\n+    if (k->is_inline_klass()) {\n+      return new (arena()) ciInlineKlass(k);\n+    } else if (k->is_instance_klass()) {\n@@ -389,0 +397,2 @@\n+    } else if (k->is_flatArray_klass()) {\n+      return new (arena()) ciFlatArrayKlass(k);\n@@ -629,0 +639,6 @@\n+ciWrapper* ciObjectFactory::make_null_free_wrapper(ciType* type) {\n+  ciWrapper* wrapper = new (arena()) ciWrapper(type);\n+  init_ident_of(wrapper);\n+  return wrapper;\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciObjectFactory.cpp","additions":17,"deletions":1,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -964,0 +965,1 @@\n+\n@@ -1010,27 +1012,79 @@\n-  \/\/ staticfield <klass> <name> <signature> <value>\n-  \/\/\n-  \/\/ Initialize a class and fill in the value for a static field.\n-  \/\/ This is useful when the compile was dependent on the value of\n-  \/\/ static fields but it's impossible to properly rerun the static\n-  \/\/ initializer.\n-  void process_staticfield(TRAPS) {\n-    InstanceKlass* k = (InstanceKlass *)parse_klass(CHECK);\n-\n-    if (k == nullptr || ReplaySuppressInitializers == 0 ||\n-        (ReplaySuppressInitializers == 2 && k->class_loader() == nullptr)) {\n-      skip_remaining();\n-      return;\n-    }\n-\n-    assert(k->is_initialized(), \"must be\");\n-\n-    const char* field_name = parse_escaped_string();\n-    const char* field_signature = parse_string();\n-    fieldDescriptor fd;\n-    Symbol* name = SymbolTable::new_symbol(field_name);\n-    Symbol* sig = SymbolTable::new_symbol(field_signature);\n-    if (!k->find_local_field(name, sig, &fd) ||\n-        !fd.is_static() ||\n-        fd.has_initial_value()) {\n-      report_error(field_name);\n-      return;\n+  class InlineTypeFieldInitializer : public FieldClosure {\n+    oop _vt;\n+    CompileReplay* _replay;\n+  public:\n+    InlineTypeFieldInitializer(oop vt, CompileReplay* replay)\n+  : _vt(vt), _replay(replay) {}\n+\n+    void do_field(fieldDescriptor* fd) {\n+      BasicType bt = fd->field_type();\n+      const char* string_value = fd->is_null_free_inline_type() ? nullptr : _replay->parse_escaped_string();\n+      switch (bt) {\n+      case T_BYTE: {\n+        int value = atoi(string_value);\n+        _vt->byte_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_BOOLEAN: {\n+        int value = atoi(string_value);\n+        _vt->bool_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_SHORT: {\n+        int value = atoi(string_value);\n+        _vt->short_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_CHAR: {\n+        int value = atoi(string_value);\n+        _vt->char_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_INT: {\n+        int value = atoi(string_value);\n+        _vt->int_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_LONG: {\n+        jlong value;\n+        if (sscanf(string_value, JLONG_FORMAT, &value) != 1) {\n+          fprintf(stderr, \"Error parsing long: %s\\n\", string_value);\n+          break;\n+        }\n+        _vt->long_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_FLOAT: {\n+        float value = atof(string_value);\n+        _vt->float_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_DOUBLE: {\n+        double value = atof(string_value);\n+        _vt->double_field_put(fd->offset(), value);\n+        break;\n+      }\n+      case T_ARRAY:\n+      case T_OBJECT:\n+        if (!fd->is_null_free_inline_type()) {\n+          JavaThread* THREAD = JavaThread::current();\n+          bool res = _replay->process_staticfield_reference(string_value, _vt, fd, THREAD);\n+          assert(res, \"should succeed for arrays & objects\");\n+          break;\n+        } else {\n+          InlineKlass* vk = InlineKlass::cast(fd->field_holder()->get_inline_type_field_klass(fd->index()));\n+          if (fd->is_flat()) {\n+            int field_offset = fd->offset() - vk->payload_offset();\n+            oop obj = cast_to_oop(cast_from_oop<address>(_vt) + field_offset);\n+            InlineTypeFieldInitializer init_fields(obj, _replay);\n+            vk->do_nonstatic_fields(&init_fields);\n+          } else {\n+            oop value = vk->allocate_instance(JavaThread::current());\n+            _vt->obj_field_put(fd->offset(), value);\n+          }\n+          break;\n+        }\n+      default: {\n+        fatal(\"Unhandled type: %s\", type2name(bt));\n+      }\n+      }\n@@ -1038,0 +1092,1 @@\n+  };\n@@ -1039,1 +1094,1 @@\n-    oop java_mirror = k->java_mirror();\n+  bool process_staticfield_reference(const char* field_signature, oop java_mirror, fieldDescriptor* fd, TRAPS) {\n@@ -1047,4 +1102,2 @@\n-          ArrayKlass* kelem = (ArrayKlass *)parse_klass(CHECK);\n-          if (kelem == nullptr) {\n-            return;\n-          }\n+          Klass* k = resolve_klass(field_signature, CHECK_(true));\n+          ArrayKlass* kelem = (ArrayKlass *)k;\n@@ -1060,1 +1113,1 @@\n-          value = kelem->multi_allocate(rank, dims, CHECK);\n+          value = kelem->multi_allocate(rank, dims, CHECK_(true));\n@@ -1063,1 +1116,1 @@\n-            value = oopFactory::new_byteArray(length, CHECK);\n+            value = oopFactory::new_byteArray(length, CHECK_(true));\n@@ -1065,1 +1118,1 @@\n-            value = oopFactory::new_boolArray(length, CHECK);\n+            value = oopFactory::new_boolArray(length, CHECK_(true));\n@@ -1067,1 +1120,1 @@\n-            value = oopFactory::new_charArray(length, CHECK);\n+            value = oopFactory::new_charArray(length, CHECK_(true));\n@@ -1069,1 +1122,1 @@\n-            value = oopFactory::new_shortArray(length, CHECK);\n+            value = oopFactory::new_shortArray(length, CHECK_(true));\n@@ -1071,1 +1124,1 @@\n-            value = oopFactory::new_floatArray(length, CHECK);\n+            value = oopFactory::new_floatArray(length, CHECK_(true));\n@@ -1073,1 +1126,1 @@\n-            value = oopFactory::new_doubleArray(length, CHECK);\n+            value = oopFactory::new_doubleArray(length, CHECK_(true));\n@@ -1075,1 +1128,1 @@\n-            value = oopFactory::new_intArray(length, CHECK);\n+            value = oopFactory::new_intArray(length, CHECK_(true));\n@@ -1077,1 +1130,1 @@\n-            value = oopFactory::new_longArray(length, CHECK);\n+            value = oopFactory::new_longArray(length, CHECK_(true));\n@@ -1080,1 +1133,1 @@\n-            Klass* actual_array_klass = parse_klass(CHECK);\n+            Klass* actual_array_klass = parse_klass(CHECK_(true));\n@@ -1082,1 +1135,1 @@\n-            value = oopFactory::new_objArray(kelem, length, CHECK);\n+            value = oopFactory::new_objArray(kelem, length, CHECK_(true));\n@@ -1087,0 +1140,14 @@\n+        java_mirror->obj_field_put(fd->offset(), value);\n+        return true;\n+      }\n+    } else if (strcmp(field_signature, \"Ljava\/lang\/String;\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      Handle value = java_lang_String::create_from_str(string_value, CHECK_(true));\n+      java_mirror->obj_field_put(fd->offset(), value());\n+      return true;\n+    } else if (field_signature[0] == JVM_SIGNATURE_CLASS) {\n+      const char* instance = parse_escaped_string();\n+      oop value = nullptr;\n+      if (instance != nullptr) {\n+        Klass* k = resolve_klass(instance, CHECK_(true));\n+        value = InstanceKlass::cast(k)->allocate_instance(CHECK_(true));\n@@ -1088,0 +1155,76 @@\n+      java_mirror->obj_field_put(fd->offset(), value);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  \/\/ Initialize a class and fill in the value for a static field.\n+  \/\/ This is useful when the compile was dependent on the value of\n+  \/\/ static fields but it's impossible to properly rerun the static\n+  \/\/ initializer.\n+  void process_staticfield(TRAPS) {\n+    InstanceKlass* k = (InstanceKlass *)parse_klass(CHECK);\n+\n+    if (k == nullptr || ReplaySuppressInitializers == 0 ||\n+        (ReplaySuppressInitializers == 2 && k->class_loader() == nullptr)) {\n+        skip_remaining();\n+      return;\n+    }\n+\n+    assert(k->is_initialized(), \"must be\");\n+\n+    const char* field_name = parse_escaped_string();\n+    const char* field_signature = parse_string();\n+    fieldDescriptor fd;\n+    Symbol* name = SymbolTable::new_symbol(field_name);\n+    Symbol* sig = SymbolTable::new_symbol(field_signature);\n+    if (!k->find_local_field(name, sig, &fd) ||\n+        !fd.is_static() ||\n+        fd.has_initial_value()) {\n+      report_error(field_name);\n+      return;\n+    }\n+\n+    oop java_mirror = k->java_mirror();\n+    if (strcmp(field_signature, \"I\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->int_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"B\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->byte_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"C\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->char_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"S\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->short_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"Z\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      int value = atoi(string_value);\n+      java_mirror->bool_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"J\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      jlong value;\n+      if (sscanf(string_value, JLONG_FORMAT, &value) != 1) {\n+        fprintf(stderr, \"Error parsing long: %s\\n\", string_value);\n+        return;\n+      }\n+      java_mirror->long_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"F\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      float value = atof(string_value);\n+      java_mirror->float_field_put(fd.offset(), value);\n+    } else if (strcmp(field_signature, \"D\") == 0) {\n+      const char* string_value = parse_escaped_string();\n+      double value = atof(string_value);\n+      java_mirror->double_field_put(fd.offset(), value);\n+    } else if (fd.is_null_free_inline_type()) {\n+      Klass* kelem = resolve_klass(field_signature, CHECK);\n+      InlineKlass* vk = InlineKlass::cast(kelem);\n+      oop value = vk->allocate_instance(CHECK);\n+      InlineTypeFieldInitializer init_fields(value, this);\n+      vk->do_nonstatic_fields(&init_fields);\n@@ -1090,40 +1233,2 @@\n-      const char* string_value = parse_escaped_string();\n-      if (strcmp(field_signature, \"I\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->int_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"B\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->byte_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"C\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->char_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"S\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->short_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"Z\") == 0) {\n-        int value = atoi(string_value);\n-        java_mirror->bool_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"J\") == 0) {\n-        jlong value;\n-        if (sscanf(string_value, JLONG_FORMAT, &value) != 1) {\n-          fprintf(stderr, \"Error parsing long: %s\\n\", string_value);\n-          return;\n-        }\n-        java_mirror->long_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"F\") == 0) {\n-        float value = atof(string_value);\n-        java_mirror->float_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"D\") == 0) {\n-        double value = atof(string_value);\n-        java_mirror->double_field_put(fd.offset(), value);\n-      } else if (strcmp(field_signature, \"Ljava\/lang\/String;\") == 0) {\n-        Handle value = java_lang_String::create_from_str(string_value, CHECK);\n-        java_mirror->obj_field_put(fd.offset(), value());\n-      } else if (field_signature[0] == JVM_SIGNATURE_CLASS) {\n-        oop value = nullptr;\n-        if (string_value != nullptr) {\n-          Klass* k = resolve_klass(string_value, CHECK);\n-          value = InstanceKlass::cast(k)->allocate_instance(CHECK);\n-        }\n-        java_mirror->obj_field_put(fd.offset(), value);\n-      } else {\n+      bool res = process_staticfield_reference(field_signature, java_mirror, &fd, CHECK);\n+      if (!res)  {\n","filename":"src\/hotspot\/share\/ci\/ciReplay.cpp","additions":188,"deletions":83,"binary":false,"changes":271,"status":"modified"},{"patch":"@@ -82,0 +82,14 @@\n+bool ciSymbol::starts_with(char prefix_char) const {\n+  GUARDED_VM_ENTRY(return get_symbol()->starts_with(prefix_char);)\n+}\n+\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciSymbol::ends_with\n+\/\/\n+\/\/ Tests if the symbol ends with the given suffix.\n+bool ciSymbol::ends_with(const char* suffix, int len) const {\n+  GUARDED_VM_ENTRY(return get_symbol()->ends_with(suffix, len);)\n+}\n+bool ciSymbol::ends_with(char suffix_char) const {\n+  GUARDED_VM_ENTRY(return get_symbol()->ends_with(suffix_char);)\n+}\n","filename":"src\/hotspot\/share\/ci\/ciSymbol.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-\/\/ This class represents a Java reference or primitive type.\n+\/\/ This class represents a Java reference, inline type or primitive type.\n","filename":"src\/hotspot\/share\/ci\/ciType.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -275,1 +276,11 @@\n-  } else if (t1->is_primitive_type() || t2->is_primitive_type()) {\n+  }\n+  \/\/ Unwrap after saving nullness information and handling top meets\n+  bool null_free1 = t1->is_null_free();\n+  bool null_free2 = t2->is_null_free();\n+  if (t1->unwrap() == t2->unwrap() && null_free1 == null_free2) {\n+    return t1;\n+  }\n+  t1 = t1->unwrap();\n+  t2 = t2->unwrap();\n+\n+  if (t1->is_primitive_type() || t2->is_primitive_type()) {\n@@ -277,1 +288,1 @@\n-    \/\/ is T.  null_type meet null_type is null_type.\n+    \/\/ is T. null_type meet null_type is null_type.\n@@ -291,35 +302,36 @@\n-  } else {\n-    \/\/ Both types are non-top non-primitive types.  That is,\n-    \/\/ both types are either instanceKlasses or arrayKlasses.\n-    ciKlass* object_klass = analyzer->env()->Object_klass();\n-    ciKlass* k1 = t1->as_klass();\n-    ciKlass* k2 = t2->as_klass();\n-    if (k1->equals(object_klass) || k2->equals(object_klass)) {\n-      return object_klass;\n-    } else if (!k1->is_loaded() || !k2->is_loaded()) {\n-      \/\/ Unloaded classes fall to java.lang.Object at a merge.\n-      return object_klass;\n-    } else if (k1->is_interface() != k2->is_interface()) {\n-      \/\/ When an interface meets a non-interface, we get Object;\n-      \/\/ This is what the verifier does.\n-      return object_klass;\n-    } else if (k1->is_array_klass() || k2->is_array_klass()) {\n-      \/\/ When an array meets a non-array, we get Object.\n-      \/\/ When objArray meets typeArray, we also get Object.\n-      \/\/ And when typeArray meets different typeArray, we again get Object.\n-      \/\/ But when objArray meets objArray, we look carefully at element types.\n-      if (k1->is_obj_array_klass() && k2->is_obj_array_klass()) {\n-        \/\/ Meet the element types, then construct the corresponding array type.\n-        ciKlass* elem1 = k1->as_obj_array_klass()->element_klass();\n-        ciKlass* elem2 = k2->as_obj_array_klass()->element_klass();\n-        ciKlass* elem  = type_meet_internal(elem1, elem2, analyzer)->as_klass();\n-        \/\/ Do an easy shortcut if one type is a super of the other.\n-        if (elem == elem1) {\n-          assert(k1 == ciObjArrayKlass::make(elem), \"shortcut is OK\");\n-          return k1;\n-        } else if (elem == elem2) {\n-          assert(k2 == ciObjArrayKlass::make(elem), \"shortcut is OK\");\n-          return k2;\n-        } else {\n-          return ciObjArrayKlass::make(elem);\n-        }\n+  }\n+\n+  \/\/ Both types are non-top non-primitive types.  That is,\n+  \/\/ both types are either instanceKlasses or arrayKlasses.\n+  ciKlass* object_klass = analyzer->env()->Object_klass();\n+  ciKlass* k1 = t1->as_klass();\n+  ciKlass* k2 = t2->as_klass();\n+  if (k1->equals(object_klass) || k2->equals(object_klass)) {\n+    return object_klass;\n+  } else if (!k1->is_loaded() || !k2->is_loaded()) {\n+    \/\/ Unloaded classes fall to java.lang.Object at a merge.\n+    return object_klass;\n+  } else if (k1->is_interface() != k2->is_interface()) {\n+    \/\/ When an interface meets a non-interface, we get Object;\n+    \/\/ This is what the verifier does.\n+    return object_klass;\n+  } else if (k1->is_array_klass() || k2->is_array_klass()) {\n+    \/\/ When an array meets a non-array, we get Object.\n+    \/\/ When (obj\/flat)Array meets typeArray, we also get Object.\n+    \/\/ And when typeArray meets different typeArray, we again get Object.\n+    \/\/ But when (obj\/flat)Array meets (obj\/flat)Array, we look carefully at element types.\n+    if ((k1->is_obj_array_klass() || k1->is_flat_array_klass()) &&\n+        (k2->is_obj_array_klass() || k2->is_flat_array_klass())) {\n+      ciType* elem1 = k1->as_array_klass()->element_klass();\n+      ciType* elem2 = k2->as_array_klass()->element_klass();\n+      ciType* elem = elem1;\n+      if (elem1 != elem2) {\n+        elem = type_meet_internal(elem1, elem2, analyzer)->as_klass();\n+      }\n+      \/\/ Do an easy shortcut if one type is a super of the other.\n+      if (elem == elem1 && !elem->is_inlinetype()) {\n+        assert(k1 == ciArrayKlass::make(elem), \"shortcut is OK\");\n+        return k1;\n+      } else if (elem == elem2 && !elem->is_inlinetype()) {\n+        assert(k2 == ciArrayKlass::make(elem), \"shortcut is OK\");\n+        return k2;\n@@ -327,1 +339,1 @@\n-        return object_klass;\n+        return ciArrayKlass::make(elem);\n@@ -330,4 +342,9 @@\n-      \/\/ Must be two plain old instance klasses.\n-      assert(k1->is_instance_klass(), \"previous cases handle non-instances\");\n-      assert(k2->is_instance_klass(), \"previous cases handle non-instances\");\n-      return k1->least_common_ancestor(k2);\n+      return object_klass;\n+    }\n+  } else {\n+    \/\/ Must be two plain old instance klasses.\n+    assert(k1->is_instance_klass(), \"previous cases handle non-instances\");\n+    assert(k2->is_instance_klass(), \"previous cases handle non-instances\");\n+    ciType* result = k1->least_common_ancestor(k2);\n+    if (null_free1 && null_free2 && result->is_inlinetype()) {\n+      result = analyzer->mark_as_null_free(result);\n@@ -335,0 +352,1 @@\n+    return result;\n@@ -396,1 +414,6 @@\n-    state->push(method()->holder());\n+    ciType* holder = method()->holder();\n+    if (holder->is_inlinetype()) {\n+      \/\/ The receiver is null-free\n+      holder = mark_as_null_free(holder);\n+    }\n+    state->push(holder);\n@@ -546,2 +569,2 @@\n-\/\/ ciTypeFlow::StateVector::do_aaload\n-void ciTypeFlow::StateVector::do_aaload(ciBytecodeStream* str) {\n+\/\/ ciTypeFlow::StateVector::do_aload\n+void ciTypeFlow::StateVector::do_aload(ciBytecodeStream* str) {\n@@ -549,1 +572,1 @@\n-  ciObjArrayKlass* array_klass = pop_objArray();\n+  ciArrayKlass* array_klass = pop_objOrFlatArray();\n@@ -551,1 +574,1 @@\n-    \/\/ Did aaload on a null reference; push a null and ignore the exception.\n+    \/\/ Did aload on a null reference; push a null and ignore the exception.\n@@ -587,1 +610,1 @@\n-    \/\/ VM's interpreter will not load 'klass' if object is null.\n+    \/\/ VM's interpreter will not load 'klass' if object is nullptr.\n@@ -594,1 +617,7 @@\n-    pop_object();\n+    ciType* type = pop_value();\n+    type = type->unwrap();\n+    if (type->is_loaded() && klass->is_loaded() &&\n+        type != klass && type->is_subtype_of(klass)) {\n+      \/\/ Useless cast, propagate more precise type of object\n+      klass = type->as_klass();\n+    }\n@@ -616,1 +645,10 @@\n-    if (!field_type->is_loaded()) {\n+    if (field->is_static() && field->is_null_free() &&\n+        !field_type->as_instance_klass()->is_initialized()) {\n+      \/\/ Deoptimize if we load from a static field with an uninitialized inline type\n+      \/\/ because we need to throw an exception if initialization of the type failed.\n+      trap(str, field_type->as_klass(),\n+           Deoptimization::make_trap_request\n+           (Deoptimization::Reason_unloaded,\n+            Deoptimization::Action_reinterpret));\n+      return;\n+    } else if (!field_type->is_loaded()) {\n@@ -637,0 +675,3 @@\n+      if (field->is_null_free()) {\n+        field_type = outer()->mark_as_null_free(field_type);\n+      }\n@@ -702,1 +743,11 @@\n-        do_null_assert(return_type->as_klass());\n+        if (InlineTypeReturnedAsFields) {\n+          \/\/ Return might be in scalarized form but we can't handle it because we\n+          \/\/ don't know the type. This can happen due to a missing preload attribute.\n+          \/\/ TODO 8284443 Use PhaseMacroExpand::expand_mh_intrinsic_return for this\n+          trap(str, nullptr,\n+               Deoptimization::make_trap_request\n+               (Deoptimization::Reason_uninitialized,\n+                Deoptimization::Action_reinterpret));\n+        } else {\n+          do_null_assert(return_type->as_klass());\n+        }\n@@ -740,1 +791,5 @@\n-        push_object(obj->klass());\n+        ciType* type = obj->klass();\n+        if (type->is_inlinetype()) {\n+          type = outer()->mark_as_null_free(type);\n+        }\n+        push(type);\n@@ -887,1 +942,1 @@\n-  case Bytecodes::_aaload: do_aaload(str);                       break;\n+  case Bytecodes::_aaload: do_aload(str);                           break;\n@@ -893,1 +948,1 @@\n-      pop_objArray();\n+      pop_objOrFlatArray();\n@@ -915,1 +970,1 @@\n-        push_object(ciObjArrayKlass::make(element_klass));\n+        push_object(ciArrayKlass::make(element_klass));\n@@ -1474,0 +1529,1 @@\n+\n@@ -1494,1 +1550,1 @@\n-  ciType* type = type_at(c);\n+  ciType* type = type_at(c)->unwrap();\n@@ -1757,3 +1813,6 @@\n-      case Bytecodes::_athrow:     case Bytecodes::_ireturn:\n-      case Bytecodes::_lreturn:    case Bytecodes::_freturn:\n-      case Bytecodes::_dreturn:    case Bytecodes::_areturn:\n+      case Bytecodes::_athrow:\n+      case Bytecodes::_ireturn:\n+      case Bytecodes::_lreturn:\n+      case Bytecodes::_freturn:\n+      case Bytecodes::_dreturn:\n+      case Bytecodes::_areturn:\n@@ -3152,0 +3211,5 @@\n+ciType* ciTypeFlow::mark_as_null_free(ciType* type) {\n+  \/\/ Wrap the type to carry the information that it is null-free\n+  return env()->make_null_free_wrapper(type);\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciTypeFlow.cpp","additions":123,"deletions":59,"binary":false,"changes":182,"status":"modified"},{"patch":"@@ -24,0 +24,2 @@\n+\n+#include \"oops\/inlineKlass.hpp\"\n@@ -53,0 +55,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -87,0 +90,1 @@\n+#include \"utilities\/stringUtils.hpp\"\n@@ -153,0 +157,2 @@\n+#define CONSTANT_CLASS_DESCRIPTORS        69\n+\n@@ -195,1 +201,1 @@\n-      case JVM_CONSTANT_Class : {\n+      case JVM_CONSTANT_Class: {\n@@ -499,0 +505,3 @@\n+\n+        Symbol* const name = cp->symbol_at(class_index);\n+        const unsigned int name_len = name->utf8_length();\n@@ -708,1 +717,1 @@\n-            } else if (!Signature::is_void_method(signature)) { \/\/ must have void signature.\n+            } else if (!Signature::is_void_method(signature)) {  \/\/ must have void signature.\n@@ -728,2 +737,3 @@\n-            if (ref_kind == JVM_REF_newInvokeSpecial) {\n-              if (name != vmSymbols::object_initializer_name()) {\n+\n+            if (name != vmSymbols::object_initializer_name()) { \/\/ !<init>\n+              if (ref_kind == JVM_REF_newInvokeSpecial) {\n@@ -735,2 +745,10 @@\n-            } else {\n-              if (name == vmSymbols::object_initializer_name()) {\n+            } else { \/\/ <init>\n+              \/\/ The allowed invocation mode of <init> depends on its signature.\n+              \/\/ This test corresponds to verify_invoke_instructions in the verifier.\n+              const int signature_ref_index =\n+                cp->signature_ref_index_at(name_and_type_ref_index);\n+              const Symbol* const signature = cp->symbol_at(signature_ref_index);\n+              if (signature->is_void_method_signature()\n+                  && ref_kind == JVM_REF_newInvokeSpecial) {\n+                \/\/ OK, could be a constructor call\n+              } else {\n@@ -788,4 +806,13 @@\n-\/\/ Side-effects: populates the _local_interfaces field\n-void ClassFileParser::parse_interfaces(const ClassFileStream* const stream,\n-                                       const int itfs_len,\n-                                       ConstantPool* const cp,\n+static void check_identity_and_value_modifiers(ClassFileParser* current, const InstanceKlass* super_type, TRAPS) {\n+  assert(super_type != nullptr,\"Method doesn't support null super type\");\n+  if (super_type->access_flags().is_identity_class() && !current->access_flags().is_identity_class()\n+      && super_type->name() != vmSymbols::java_lang_Object()) {\n+      THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                err_msg(\"Value type %s has an identity type as supertype\",\n+                current->class_name()->as_klass_external_name()));\n+  }\n+}\n+\n+void ClassFileParser::parse_interfaces(const ClassFileStream* stream,\n+                                       int itfs_len,\n+                                       ConstantPool* cp,\n@@ -793,0 +820,6 @@\n+                                       \/\/ FIXME: lots of these functions\n+                                       \/\/ declare their parameters as const,\n+                                       \/\/ which adds only noise to the code.\n+                                       \/\/ Remove the spurious const modifiers.\n+                                       \/\/ Many are of the form \"const int x\"\n+                                       \/\/ or \"T* const x\".\n@@ -800,0 +833,1 @@\n+\n@@ -802,3 +836,2 @@\n-    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, nullptr, CHECK);\n-\n-    int index;\n+    _local_interface_indexes = new GrowableArray<u2>(itfs_len);\n+    int index = 0;\n@@ -807,1 +840,0 @@\n-      Klass* interf;\n@@ -812,29 +844,1 @@\n-      if (cp->tag_at(interface_index).is_klass()) {\n-        interf = cp->resolved_klass_at(interface_index);\n-      } else {\n-        Symbol* const unresolved_klass  = cp->klass_name_at(interface_index);\n-\n-        \/\/ Don't need to check legal name because it's checked when parsing constant pool.\n-        \/\/ But need to make sure it's not an array type.\n-        guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,\n-                           \"Bad interface name in class file %s\", CHECK);\n-\n-        \/\/ Call resolve on the interface class name with class circularity checking\n-        interf = SystemDictionary::resolve_super_or_fail(_class_name,\n-                                                         unresolved_klass,\n-                                                         Handle(THREAD, _loader_data->class_loader()),\n-                                                         false, CHECK);\n-      }\n-\n-      if (!interf->is_interface()) {\n-        THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n-                  err_msg(\"class %s can not implement %s, because it is not an interface (%s)\",\n-                          _class_name->as_klass_external_name(),\n-                          interf->external_name(),\n-                          interf->class_in_module_of_loader()));\n-      }\n-\n-      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n-        *has_nonstatic_concrete_methods = true;\n-      }\n-      _local_interfaces->at_put(index, InstanceKlass::cast(interf));\n+      _local_interface_indexes->at_put_grow(index, interface_index);\n@@ -852,2 +856,1 @@\n-      const InstanceKlass* const k = _local_interfaces->at(index);\n-      Symbol* interface_name = k->name();\n+      Symbol* interface_name = cp->klass_name_at(_local_interface_indexes->at(index));\n@@ -940,0 +943,3 @@\n+    _jdk_internal_ImplicitlyConstructible,\n+    _jdk_internal_LooselyConsistentValue,\n+    _jdk_internal_NullRestricted,\n@@ -1362,1 +1368,1 @@\n-                                   bool is_interface,\n+                                   AccessFlags class_access_flags,\n@@ -1375,0 +1381,1 @@\n+  bool is_inline_type = !class_access_flags.is_identity_class() && !class_access_flags.is_abstract();\n@@ -1382,1 +1389,5 @@\n-  const int total_fields = length + num_injected;\n+\n+  \/\/ two more slots are required for inline classes:\n+  \/\/ one for the static field with a reference to the pre-allocated default value\n+  \/\/ one for the field the JVM injects when detecting an empty inline class\n+  const int total_fields = length + num_injected + (is_inline_type ? 2 : 0);\n@@ -1388,0 +1399,1 @@\n+  int instance_fields_count = 0;\n@@ -1393,0 +1405,7 @@\n+    jint recognized_modifiers = JVM_RECOGNIZED_FIELD_MODIFIERS;\n+    if (!supports_inline_types()) {\n+      recognized_modifiers &= ~JVM_ACC_STRICT;\n+    }\n+\n+    const jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+    verify_legal_field_modifiers(flags, class_access_flags, CHECK);\n@@ -1394,2 +1413,0 @@\n-    const jint flags = cfs->get_u2_fast() & JVM_RECOGNIZED_FIELD_MODIFIERS;\n-    verify_legal_field_modifiers(flags, is_interface, CHECK);\n@@ -1412,0 +1429,1 @@\n+    if (!access_flags.is_static()) instance_fields_count++;\n@@ -1419,0 +1437,2 @@\n+    bool is_null_restricted = false;\n+\n@@ -1438,0 +1458,10 @@\n+        if (parsed_annotations.has_annotation(AnnotationCollector::_jdk_internal_NullRestricted)) {\n+          if (!Signature::has_envelope(sig)) {\n+            Exceptions::fthrow(\n+              THREAD_AND_LOCATION,\n+              vmSymbols::java_lang_ClassFormatError(),\n+              \"Illegal use of @jdk.internal.vm.annotation.NullRestricted annotation on field %s with signature %s (primitive types can never be null)\",\n+              name->as_C_string(), sig->as_C_string());\n+          }\n+          is_null_restricted = true;\n+        }\n@@ -1460,0 +1490,4 @@\n+    if (is_null_restricted) {\n+      fieldFlags.update_null_free_inline_type(true);\n+    }\n+\n@@ -1480,1 +1514,0 @@\n-  int index = length;\n@@ -1508,3 +1541,2 @@\n-      fi.set_index(index);\n-      _temp_field_info->append(fi);\n-      index++;\n+      int idx = _temp_field_info->append(fi);\n+      _temp_field_info->adr_at(idx)->set_index(idx);\n@@ -1514,1 +1546,26 @@\n-  assert(_temp_field_info->length() == index, \"Must be\");\n+  if (is_inline_type) {\n+    \/\/ Inject static \".default\" field\n+    FieldInfo::FieldFlags fflags(0);\n+    fflags.update_injected(true);\n+    AccessFlags aflags(JVM_ACC_STATIC);\n+    FieldInfo fi(aflags,\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(default_value_name)),\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(object_signature)),\n+                 0,\n+                 fflags);\n+    int idx = _temp_field_info->append(fi);\n+    _temp_field_info->adr_at(idx)->set_index(idx);\n+    _static_oop_count++;\n+    \/\/ Inject static \".null_reset\" field. This is the same as the .default field but will never have its null-channel set to non-zero.\n+    FieldInfo::FieldFlags fflags2(0);\n+    fflags2.update_injected(true);\n+    AccessFlags aflags2(JVM_ACC_STATIC);\n+    FieldInfo fi2(aflags2,\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(null_reset_value_name)),\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(object_signature)),\n+                 0,\n+                 fflags2);\n+    int idx2 = _temp_field_info->append(fi2);\n+    _temp_field_info->adr_at(idx2)->set_index(idx2);\n+    _static_oop_count++;\n+  }\n@@ -1894,0 +1951,12 @@\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_ImplicitlyConstructible_signature): {\n+      if (_location != _in_class)   break; \/\/ only allow for classes\n+      return _jdk_internal_ImplicitlyConstructible;\n+    }\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_LooselyConsistentValue_signature): {\n+      if (_location != _in_class)   break; \/\/ only allow for classes\n+      return _jdk_internal_LooselyConsistentValue;\n+    }\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_NullRestricted_signature): {\n+      if (_location != _in_field)   break; \/\/ only allow for fields\n+      return _jdk_internal_NullRestricted;\n+    }\n@@ -2117,0 +2186,2 @@\n+                                      bool is_value_class,\n+                                      bool is_abstract_class,\n@@ -2158,1 +2229,1 @@\n-    verify_legal_method_modifiers(flags, is_interface, name, CHECK_NULL);\n+    verify_legal_method_modifiers(flags, access_flags() , name, CHECK_NULL);\n@@ -2166,0 +2237,9 @@\n+  if (EnableValhalla) {\n+    if (((flags & JVM_ACC_SYNCHRONIZED) == JVM_ACC_SYNCHRONIZED)\n+        && ((flags & JVM_ACC_STATIC) == 0 )\n+        && !_access_flags.is_identity_class()) {\n+      classfile_parse_error(\"Invalid synchronized method in non-identity class %s\", THREAD);\n+        return nullptr;\n+    }\n+  }\n+\n@@ -2693,0 +2773,2 @@\n+                                    bool is_value_class,\n+                                    bool is_abstract_type,\n@@ -2717,0 +2799,2 @@\n+                                    is_value_class,\n+                                    is_abstract_type,\n@@ -2982,2 +3066,2 @@\n-    \/\/ Access flags\n-    u2 flags;\n+\n+    u2 recognized_modifiers = RECOGNIZED_INNER_CLASS_MODIFIERS;\n@@ -2986,3 +3070,1 @@\n-      flags = cfs->get_u2_fast() & (RECOGNIZED_INNER_CLASS_MODIFIERS | JVM_ACC_MODULE);\n-    } else {\n-      flags = cfs->get_u2_fast() & RECOGNIZED_INNER_CLASS_MODIFIERS;\n+      recognized_modifiers |= JVM_ACC_MODULE;\n@@ -2990,0 +3072,4 @@\n+\n+    \/\/ Access flags\n+    u2 flags = cfs->get_u2_fast() & recognized_modifiers;\n+\n@@ -2994,1 +3080,11 @@\n-    verify_legal_class_modifiers(flags, CHECK_0);\n+\n+    if (!supports_inline_types()) {\n+      const bool is_module = (flags & JVM_ACC_MODULE) != 0;\n+      const bool is_interface = (flags & JVM_ACC_INTERFACE) != 0;\n+      if (!is_module && !is_interface) {\n+        flags |= JVM_ACC_IDENTITY;\n+      }\n+    }\n+\n+    const char* name = inner_name_index == 0 ? \"unnamed\" : cp->symbol_at(inner_name_index)->as_utf8();\n+    verify_legal_class_modifiers(flags, name, false, CHECK_0);\n@@ -3098,0 +3194,43 @@\n+u2 ClassFileParser::parse_classfile_loadable_descriptors_attribute(const ClassFileStream* const cfs,\n+                                                                   const u1* const loadable_descriptors_attribute_start,\n+                                                                   TRAPS) {\n+  const u1* const current_mark = cfs->current();\n+  u2 length = 0;\n+  if (loadable_descriptors_attribute_start != nullptr) {\n+    cfs->set_current(loadable_descriptors_attribute_start);\n+    cfs->guarantee_more(2, CHECK_0);  \/\/ length\n+    length = cfs->get_u2_fast();\n+  }\n+  const int size = length;\n+  Array<u2>* const loadable_descriptors = MetadataFactory::new_array<u2>(_loader_data, size, CHECK_0);\n+  _loadable_descriptors = loadable_descriptors;\n+  if (length > 0) {\n+    int index = 0;\n+    cfs->guarantee_more(2 * length, CHECK_0);\n+    for (int n = 0; n < length; n++) {\n+      const u2 descriptor_index = cfs->get_u2_fast();\n+      guarantee_property(\n+        valid_symbol_at(descriptor_index),\n+        \"LoadableDescriptors descriptor_index %u has bad constant type in class file %s\",\n+        descriptor_index, CHECK_0);\n+      Symbol* descriptor = _cp->symbol_at(descriptor_index);\n+      bool valid = legal_field_signature(descriptor, CHECK_0);\n+      if(!valid) {\n+        ResourceMark rm(THREAD);\n+        Exceptions::fthrow(THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_ClassFormatError(),\n+          \"Descriptor from LoadableDescriptors attribute at index \\\"%d\\\" in class %s has illegal signature \\\"%s\\\"\",\n+          descriptor_index, _class_name->as_C_string(), descriptor->as_C_string());\n+        return 0;\n+      }\n+      loadable_descriptors->at_put(index++, descriptor_index);\n+    }\n+    assert(index == size, \"wrong size\");\n+  }\n+\n+  \/\/ Restore buffer's current position.\n+  cfs->set_current(current_mark);\n+\n+  return length;\n+}\n+\n@@ -3363,0 +3502,2 @@\n+  \/\/ Set _loadable_descriptors attribute to default sentinel\n+  _loadable_descriptors = Universe::the_empty_short_array();\n@@ -3369,0 +3510,1 @@\n+  bool parsed_loadable_descriptors_attribute = false;\n@@ -3390,0 +3532,2 @@\n+  const u1* loadable_descriptors_attribute_start = nullptr;\n+  u4  loadable_descriptors_attribute_length = 0;\n@@ -3605,0 +3749,9 @@\n+            if (EnableValhalla && tag == vmSymbols::tag_loadable_descriptors()) {\n+              if (parsed_loadable_descriptors_attribute) {\n+                classfile_parse_error(\"Multiple LoadableDescriptors attributes in class file %s\", CHECK);\n+                return;\n+              }\n+              parsed_loadable_descriptors_attribute = true;\n+              loadable_descriptors_attribute_start = cfs->current();\n+              loadable_descriptors_attribute_length = attribute_length;\n+            }\n@@ -3681,0 +3834,12 @@\n+  if (parsed_loadable_descriptors_attribute) {\n+    const u2 num_classes = parse_classfile_loadable_descriptors_attribute(\n+                            cfs,\n+                            loadable_descriptors_attribute_start,\n+                            CHECK);\n+    if (_need_verify) {\n+      guarantee_property(\n+        loadable_descriptors_attribute_length == sizeof(num_classes) + sizeof(u2) * num_classes,\n+        \"Wrong LoadableDescriptors attribute length in class file %s\", CHECK);\n+    }\n+  }\n+\n@@ -3746,0 +3911,1 @@\n+  this_klass->set_loadable_descriptors(_loadable_descriptors);\n@@ -3749,0 +3915,1 @@\n+  this_klass->set_inline_layout_info_array(_inline_layout_info_array);\n@@ -3791,3 +3958,2 @@\n-                       \"Invalid superclass index %u in class file %s\",\n-                       super_class_index,\n-                       CHECK_NULL);\n+                   \"Invalid superclass index 0 in class file %s\",\n+                   CHECK_NULL);\n@@ -3801,1 +3967,0 @@\n-    bool is_array = false;\n@@ -3804,4 +3969,0 @@\n-      if (need_verify)\n-        is_array = super_klass->is_array_klass();\n-    } else if (need_verify) {\n-      is_array = (cp->klass_name_at(super_class_index)->char_at(0) == JVM_SIGNATURE_ARRAY);\n@@ -3810,0 +3971,1 @@\n+      bool is_array = (cp->klass_name_at(super_class_index)->char_at(0) == JVM_SIGNATURE_ARRAY);\n@@ -3983,0 +4145,6 @@\n+bool ClassFileParser::supports_inline_types() const {\n+  \/\/ Inline types are only supported by class file version 69.65535 and later\n+  return _major_version > JAVA_25_VERSION ||\n+         (_major_version == JAVA_25_VERSION && _minor_version == JAVA_PREVIEW_MINOR_VERSION);\n+}\n+\n@@ -4026,3 +4194,4 @@\n-  } else if (max_transitive_size == local_size) {\n-    \/\/ only local interfaces added, share local interface array\n-    return local_ifs;\n+    \/\/ The three lines below are commented to work around bug JDK-8245487\n+\/\/  } else if (max_transitive_size == local_size) {\n+\/\/    \/\/ only local interfaces added, share local interface array\n+\/\/    return local_ifs;\n@@ -4049,0 +4218,1 @@\n+\n@@ -4081,0 +4251,10 @@\n+    \/\/ The JVMS says that super classes for value types must not have the ACC_IDENTITY\n+    \/\/ flag set. But, java.lang.Object must still be allowed to be a direct super class\n+    \/\/ for a value classes.  So, it is treated as a special case for now.\n+    if (!this_klass->access_flags().is_identity_class() &&\n+        super_ik->name() != vmSymbols::java_lang_Object() &&\n+        super_ik->is_identity_class()) {\n+      classfile_icce_error(\"value class %s cannot inherit from class %s\", super_ik, THREAD);\n+      return;\n+    }\n+\n@@ -4253,1 +4433,1 @@\n-void ClassFileParser::verify_legal_class_modifiers(jint flags, TRAPS) const {\n+void ClassFileParser::verify_legal_class_modifiers(jint flags, const char* name, bool is_Object, TRAPS) const {\n@@ -4255,0 +4435,1 @@\n+  const bool is_inner_class = name != nullptr;\n@@ -4272,1 +4453,1 @@\n-  const bool is_super      = (flags & JVM_ACC_SUPER)      != 0;\n+  const bool is_identity   = (flags & JVM_ACC_IDENTITY)   != 0;\n@@ -4276,0 +4457,2 @@\n+  const bool valid_value_class = is_identity || is_interface ||\n+                                 (supports_inline_types() && (!is_identity && (is_abstract || is_final)));\n@@ -4279,2 +4462,3 @@\n-      (is_interface && major_gte_1_5 && (is_super || is_enum)) ||\n-      (!is_interface && major_gte_1_5 && is_annotation)) {\n+      (is_interface && major_gte_1_5 && (is_identity || is_enum)) ||   \/\/  ACC_SUPER (now ACC_IDENTITY) was illegal for interfaces\n+      (!is_interface && major_gte_1_5 && is_annotation) ||\n+      (!valid_value_class)) {\n@@ -4282,8 +4466,22 @@\n-    \/\/ Names are all known to be < 64k so we know this formatted message is not excessively large.\n-    Exceptions::fthrow(\n-      THREAD_AND_LOCATION,\n-      vmSymbols::java_lang_ClassFormatError(),\n-      \"Illegal class modifiers in class %s: 0x%X\",\n-      _class_name->as_C_string(), flags\n-    );\n-    return;\n+    const char* class_note = \"\";\n+    if (!valid_value_class) {\n+      class_note = \" (a value class must be final or else abstract)\";\n+    }\n+    if (name == nullptr) { \/\/ Not an inner class\n+      Exceptions::fthrow(\n+        THREAD_AND_LOCATION,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"Illegal class modifiers in class %s%s: 0x%X\",\n+        _class_name->as_C_string(), class_note, flags\n+      );\n+      return;\n+    } else {\n+      \/\/ Names are all known to be < 64k so we know this formatted message is not excessively large.\n+      Exceptions::fthrow(\n+        THREAD_AND_LOCATION,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"Illegal class modifiers in declaration of inner class %s%s of class %s: 0x%X\",\n+        name, class_note, _class_name->as_C_string(), flags\n+      );\n+      return;\n+    }\n@@ -4357,2 +4555,2 @@\n-void ClassFileParser::verify_legal_field_modifiers(jint flags,\n-                                                   bool is_interface,\n+void ClassFileParser:: verify_legal_field_modifiers(jint flags,\n+                                                   AccessFlags class_access_flags,\n@@ -4370,0 +4568,1 @@\n+  const bool is_strict    = (flags & JVM_ACC_STRICT)    != 0;\n@@ -4372,0 +4571,3 @@\n+  const bool is_interface = class_access_flags.is_interface();\n+  const bool is_identity_class = class_access_flags.is_identity_class();\n+\n@@ -4373,0 +4575,1 @@\n+  const char* error_msg = \"\";\n@@ -4374,4 +4577,8 @@\n-  if (is_interface) {\n-    if (!is_public || !is_static || !is_final || is_private ||\n-        is_protected || is_volatile || is_transient ||\n-        (major_gte_1_5 && is_enum)) {\n+  \/\/ There is some overlap in the checks that apply, for example interface fields\n+  \/\/ must be static, static fields can't be strict, and therefore interfaces can't\n+  \/\/ have strict fields. So we don't have to check every possible invalid combination\n+  \/\/ individually as long as all are covered. Once we have found an illegal combination\n+  \/\/ we can stop checking.\n+\n+  if (supports_inline_types()) {\n+    if (is_strict && is_static) {\n@@ -4379,0 +4586,1 @@\n+      error_msg = \"field cannot be strict and static\";\n@@ -4380,2 +4588,1 @@\n-  } else { \/\/ not interface\n-    if (has_illegal_visibility(flags) || (is_final && is_volatile)) {\n+    else if (is_strict && !is_final) {\n@@ -4383,0 +4590,25 @@\n+      error_msg = \"strict field must be final\";\n+    }\n+  }\n+\n+  if (!is_illegal) {\n+    if (is_interface) {\n+      if (!is_public || !is_static || !is_final || is_private ||\n+          is_protected || is_volatile || is_transient ||\n+          (major_gte_1_5 && is_enum)) {\n+        is_illegal = true;\n+        error_msg = \"interface fields must be public, static and final, and may be synthetic\";\n+      }\n+    } else { \/\/ not interface\n+      if (has_illegal_visibility(flags)) {\n+        is_illegal = true;\n+        error_msg = \"invalid visibility flags for class field\";\n+      } else if (is_final && is_volatile) {\n+        is_illegal = true;\n+        error_msg = \"fields cannot be final and volatile\";\n+      } else if (supports_inline_types()) {\n+        if (!is_identity_class && !is_static && !is_strict) {\n+          is_illegal = true;\n+          error_msg = \"value class fields must be either strict or static\";\n+        }\n+      }\n@@ -4392,2 +4624,2 @@\n-      \"Illegal field modifiers in class %s: 0x%X\",\n-      _class_name->as_C_string(), flags);\n+      \"Illegal field modifiers (%s) in class %s: 0x%X\",\n+      error_msg, _class_name->as_C_string(), flags);\n@@ -4399,1 +4631,1 @@\n-                                                    bool is_interface,\n+                                                    AccessFlags class_access_flags,\n@@ -4418,0 +4650,4 @@\n+  \/\/ LW401 CR required: removal of value factories support\n+  const bool is_interface    = class_access_flags.is_interface();\n+  const bool is_identity_class = class_access_flags.is_identity_class();\n+  const bool is_abstract_class = class_access_flags.is_abstract();\n@@ -4421,0 +4657,1 @@\n+  const char* class_note = \"\";\n@@ -4460,4 +4697,9 @@\n-        if (is_abstract) {\n-          if ((is_final || is_native || is_private || is_static ||\n-              (major_gte_1_5 && (is_synchronized || (!major_gte_17 && is_strict))))) {\n-            is_illegal = true;\n+        if (!is_identity_class && is_synchronized && !is_static) {\n+          is_illegal = true;\n+          class_note = \" (not an identity class)\";\n+        } else {\n+          if (is_abstract) {\n+            if ((is_final || is_native || is_private || is_static ||\n+                (major_gte_1_5 && (is_synchronized || (!major_gte_17 && is_strict))))) {\n+              is_illegal = true;\n+            }\n@@ -4476,2 +4718,3 @@\n-      \"Method %s in class %s has illegal modifiers: 0x%X\",\n-      name->as_C_string(), _class_name->as_C_string(), flags);\n+      \"Method %s in class %s%s has illegal modifiers: 0x%X\",\n+      name->as_C_string(), _class_name->as_C_string(),\n+      class_note, flags);\n@@ -4535,0 +4778,9 @@\n+bool ClassFileParser::is_class_in_loadable_descriptors_attribute(Symbol *klass) {\n+  if (_loadable_descriptors == nullptr) return false;\n+  for (int i = 0; i < _loadable_descriptors->length(); i++) {\n+        Symbol* class_name = _cp->symbol_at(_loadable_descriptors->at(i));\n+        if (class_name == klass) return true;\n+  }\n+  return false;\n+}\n+\n@@ -4636,1 +4888,2 @@\n-    case JVM_SIGNATURE_CLASS: {\n+    case JVM_SIGNATURE_CLASS:\n+    {\n@@ -4647,1 +4900,1 @@\n-        \/\/ Skip leading 'L' and ignore first appearance of ';'\n+        \/\/ Skip leading 'L' or 'Q' and ignore first appearance of ';'\n@@ -4703,0 +4956,4 @@\n+    } else if ((_major_version >= CONSTANT_CLASS_DESCRIPTORS || _class_name->starts_with(\"jdk\/internal\/reflect\/\"))\n+                   && bytes[length - 1] == ';' ) {\n+      \/\/ Support for L...; descriptors\n+      legal = verify_unqualified_name(bytes + 1, length - 2, LegalClass);\n@@ -4770,1 +5027,2 @@\n-      if (name == vmSymbols::object_initializer_name() || name == vmSymbols::class_initializer_name()) {\n+      if (name == vmSymbols::object_initializer_name() ||\n+          name == vmSymbols::class_initializer_name()) {\n@@ -4797,0 +5055,10 @@\n+bool ClassFileParser::legal_field_signature(const Symbol* signature, TRAPS) const {\n+  const char* const bytes = (const char*)signature->bytes();\n+  const unsigned int length = signature->utf8_length();\n+  const char* const p = skip_over_field_signature(bytes, false, length, CHECK_false);\n+\n+  if (p == nullptr || (p - bytes) != (int)length) {\n+    return false;\n+  }\n+  return true;\n+}\n@@ -4832,3 +5100,3 @@\n-      name->char_at(0) == JVM_SIGNATURE_SPECIAL &&\n-      sig_length > 0 &&\n-      signature->char_at(sig_length - 1) != JVM_SIGNATURE_VOID) {\n+    name->char_at(0) == JVM_SIGNATURE_SPECIAL &&\n+    sig_length > 0 &&\n+    signature->char_at(sig_length - 1) != JVM_SIGNATURE_VOID) {\n@@ -4885,2 +5153,2 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->_static_field_size;\n+  assert(_layout_info != nullptr, \"invariant\");\n+  return _layout_info->_static_field_size;\n@@ -4890,2 +5158,2 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->oop_map_blocks->_nonstatic_oop_map_count;\n+  assert(_layout_info != nullptr, \"invariant\");\n+  return _layout_info->oop_map_blocks->_nonstatic_oop_map_count;\n@@ -4895,2 +5163,2 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->_instance_size;\n+  assert(_layout_info != nullptr, \"invariant\");\n+  return _layout_info->_instance_size;\n@@ -5011,1 +5279,0 @@\n-\n@@ -5033,3 +5300,3 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  assert(ik->static_field_size() == _field_info->_static_field_size, \"sanity\");\n-  assert(ik->nonstatic_oop_map_count() == _field_info->oop_map_blocks->_nonstatic_oop_map_count,\n+  assert(_layout_info != nullptr, \"invariant\");\n+  assert(ik->static_field_size() == _layout_info->_static_field_size, \"sanity\");\n+  assert(ik->nonstatic_oop_map_count() == _layout_info->oop_map_blocks->_nonstatic_oop_map_count,\n@@ -5039,1 +5306,1 @@\n-  assert(ik->size_helper() == _field_info->_instance_size, \"sanity\");\n+  assert(ik->size_helper() == _layout_info->_instance_size, \"sanity\");\n@@ -5045,2 +5312,14 @@\n-  ik->set_nonstatic_field_size(_field_info->_nonstatic_field_size);\n-  ik->set_has_nonstatic_fields(_field_info->_has_nonstatic_fields);\n+  ik->set_nonstatic_field_size(_layout_info->_nonstatic_field_size);\n+  ik->set_has_nonstatic_fields(_layout_info->_has_nonstatic_fields);\n+\n+  if (_layout_info->_is_naturally_atomic) {\n+    ik->set_is_naturally_atomic();\n+  }\n+\n+  if (_layout_info->_must_be_atomic) {\n+    ik->set_must_be_atomic();\n+  }\n+  if (_is_implicitly_constructible) {\n+    ik->set_is_implicitly_constructible();\n+  }\n+\n@@ -5052,0 +5331,3 @@\n+  if (ik->is_inline_klass()) {\n+    InlineKlass::cast(ik)->init_fixed_block();\n+  }\n@@ -5065,0 +5347,1 @@\n+  assert(nullptr == _loadable_descriptors, \"invariant\");\n@@ -5068,0 +5351,1 @@\n+  assert(nullptr == _inline_layout_info_array, \"invariant\");\n@@ -5145,1 +5429,1 @@\n-  OopMapBlocksBuilder* oop_map_blocks = _field_info->oop_map_blocks;\n+  OopMapBlocksBuilder* oop_map_blocks = _layout_info->oop_map_blocks;\n@@ -5206,0 +5490,16 @@\n+  if (is_inline_type()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    vk->set_payload_alignment(_layout_info->_payload_alignment);\n+    vk->set_payload_offset(_layout_info->_payload_offset);\n+    vk->set_payload_size_in_bytes(_layout_info->_payload_size_in_bytes);\n+    vk->set_non_atomic_size_in_bytes(_layout_info->_non_atomic_size_in_bytes);\n+    vk->set_non_atomic_alignment(_layout_info->_non_atomic_alignment);\n+    vk->set_atomic_size_in_bytes(_layout_info->_atomic_layout_size_in_bytes);\n+    vk->set_nullable_size_in_bytes(_layout_info->_nullable_layout_size_in_bytes);\n+    vk->set_null_marker_offset(_layout_info->_null_marker_offset);\n+    vk->set_default_value_offset(_layout_info->_default_value_offset);\n+    vk->set_null_reset_value_offset(_layout_info->_null_reset_value_offset);\n+    if (_layout_info->_is_empty_inline_klass) vk->set_is_empty_inline_type();\n+    vk->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -5287,0 +5587,1 @@\n+  _loadable_descriptors(nullptr),\n@@ -5289,0 +5590,1 @@\n+  _local_interface_indexes(nullptr),\n@@ -5298,1 +5600,2 @@\n-  _field_info(nullptr),\n+  _layout_info(nullptr),\n+  _inline_layout_info_array(nullptr),\n@@ -5326,0 +5629,6 @@\n+  _has_inline_type_fields(false),\n+  _is_naturally_atomic(false),\n+  _must_be_atomic(true),\n+  _is_implicitly_constructible(false),\n+  _has_loosely_consistent_annotation(false),\n+  _has_implicitly_constructible_annotation(false),\n@@ -5361,0 +5670,1 @@\n+  _loadable_descriptors = nullptr;\n@@ -5365,0 +5675,1 @@\n+  _inline_layout_info_array = nullptr;\n@@ -5383,0 +5694,4 @@\n+  if (_inline_layout_info_array != nullptr) {\n+    MetadataFactory::free_array<InlineLayoutInfo>(_loader_data, _inline_layout_info_array);\n+  }\n+\n@@ -5405,0 +5720,4 @@\n+  if (_loadable_descriptors != nullptr && _loadable_descriptors != Universe::the_empty_short_array()) {\n+    MetadataFactory::free_array<u2>(_loader_data, _loadable_descriptors);\n+  }\n+\n@@ -5489,2 +5808,1 @@\n-  \/\/ Access flags\n-  u2 flags;\n+  u2 recognized_modifiers = JVM_RECOGNIZED_CLASS_MODIFIERS;\n@@ -5493,3 +5811,1 @@\n-    flags = stream->get_u2_fast() & (JVM_RECOGNIZED_CLASS_MODIFIERS | JVM_ACC_MODULE);\n-  } else {\n-    flags = stream->get_u2_fast() & JVM_RECOGNIZED_CLASS_MODIFIERS;\n+    recognized_modifiers |= JVM_ACC_MODULE;\n@@ -5498,0 +5814,3 @@\n+  \/\/ Access flags\n+  u2 flags = stream->get_u2_fast() & recognized_modifiers;\n+\n@@ -5503,8 +5822,7 @@\n-  verify_legal_class_modifiers(flags, CHECK);\n-\n-  short bad_constant = class_bad_constant_seen();\n-  if (bad_constant != 0) {\n-    \/\/ Do not throw CFE until after the access_flags are checked because if\n-    \/\/ ACC_MODULE is set in the access flags, then NCDFE must be thrown, not CFE.\n-    classfile_parse_error(\"Unknown constant tag %u in class file %s\", bad_constant, THREAD);\n-    return;\n+  \/\/ Fixing ACC_SUPER\/ACC_IDENTITY for old class files\n+  if (!supports_inline_types()) {\n+    const bool is_module = (flags & JVM_ACC_MODULE) != 0;\n+    const bool is_interface = (flags & JVM_ACC_INTERFACE) != 0;\n+    if (!is_module && !is_interface) {\n+      flags |= JVM_ACC_IDENTITY;\n+    }\n@@ -5513,1 +5831,0 @@\n-  _access_flags.set_flags(flags);\n@@ -5526,0 +5843,14 @@\n+  bool is_java_lang_Object = class_name_in_cp == vmSymbols::java_lang_Object();\n+\n+  verify_legal_class_modifiers(flags, nullptr, is_java_lang_Object, CHECK);\n+\n+  _access_flags.set_flags(flags);\n+\n+  short bad_constant = class_bad_constant_seen();\n+  if (bad_constant != 0) {\n+    \/\/ Do not throw CFE until after the access_flags are checked because if\n+    \/\/ ACC_MODULE is set in the access flags, then NCDFE must be thrown, not CFE.\n+    classfile_parse_error(\"Unknown constant tag %u in class file %s\", bad_constant, THREAD);\n+    return;\n+  }\n+\n@@ -5606,2 +5937,0 @@\n-  assert(_local_interfaces != nullptr, \"invariant\");\n-\n@@ -5610,1 +5939,1 @@\n-               _access_flags.is_interface(),\n+               _access_flags,\n@@ -5620,1 +5949,3 @@\n-                _access_flags.is_interface(),\n+                is_interface(),\n+                !is_identity_class(),\n+                is_abstract_class(),\n@@ -5700,2 +6031,2 @@\n-                       \"java.lang.Object cannot implement an interface in class file %s\",\n-                       CHECK);\n+        \"java.lang.Object cannot implement an interface in class file %s\",\n+        CHECK);\n@@ -5706,1 +6037,1 @@\n-    if (_access_flags.is_interface()) {\n+    if (is_interface()) {\n@@ -5718,1 +6049,1 @@\n-                       SystemDictionary::resolve_super_or_fail(_class_name,\n+                       SystemDictionary::resolve_with_circularity_detection_or_fail(_class_name,\n@@ -5727,0 +6058,14 @@\n+    if (_super_klass->is_interface()) {\n+      classfile_icce_error(\"class %s has interface %s as super class\", _super_klass, THREAD);\n+      return;\n+    }\n+\n+    if (_super_klass->is_final()) {\n+      classfile_icce_error(\"class %s cannot inherit from final class %s\", _super_klass, THREAD);\n+      return;\n+    }\n+\n+    if (EnableValhalla) {\n+      check_identity_and_value_modifiers(this, _super_klass, CHECK);\n+    }\n+\n@@ -5730,0 +6075,1 @@\n+  }\n@@ -5731,3 +6077,34 @@\n-    if (_super_klass->is_interface()) {\n-      classfile_icce_error(\"class %s has interface %s as super class\", _super_klass, THREAD);\n-      return;\n+  if (_parsed_annotations->has_annotation(AnnotationCollector::_jdk_internal_LooselyConsistentValue) && _access_flags.is_identity_class()) {\n+    THROW_MSG(vmSymbols::java_lang_ClassFormatError(),\n+          err_msg(\"class %s cannot have annotation jdk.internal.vm.annotation.LooselyConsistentValue, because it is not a value class\",\n+                  _class_name->as_klass_external_name()));\n+  }\n+  if (_parsed_annotations->has_annotation(AnnotationCollector::_jdk_internal_ImplicitlyConstructible) && _access_flags.is_identity_class()) {\n+    THROW_MSG(vmSymbols::java_lang_ClassFormatError(),\n+          err_msg(\"class %s cannot have annotation jdk.internal.vm.annotation.ImplicitlyConstructible, because it is not a value class\",\n+                  _class_name->as_klass_external_name()));\n+  }\n+\n+  \/\/ Determining is the class allows tearing or not (default is not)\n+  if (EnableValhalla && !_access_flags.is_identity_class()) {\n+    if (_parsed_annotations->has_annotation(ClassAnnotationCollector::_jdk_internal_LooselyConsistentValue)\n+        && (_super_klass == vmClasses::Object_klass() || !_super_klass->must_be_atomic())) {\n+      \/\/ Conditions above are not sufficient to determine atomicity requirements,\n+      \/\/ the presence of fields with atomic requirements could force the current class to have atomicy requirements too\n+      \/\/ Marking as not needing atomicity for now, can be updated when computing the fields layout\n+      \/\/ The InstanceKlass must be filled with the value from the FieldLayoutInfo returned by\n+      \/\/ the FieldLayoutBuilder, not with this _must_be_atomic field.\n+      _must_be_atomic = false;\n+    }\n+    if (_parsed_annotations->has_annotation(ClassAnnotationCollector::_jdk_internal_ImplicitlyConstructible)\n+        && (_super_klass == vmClasses::Object_klass() || _super_klass == vmClasses::Record_klass()\n+        || _super_klass->is_implicitly_constructible())) {\n+      _is_implicitly_constructible = true;\n+    }\n+    \/\/ Apply VM options override\n+    if (*ForceNonTearable != '\\0') {\n+      \/\/ Allow a command line switch to force the same atomicity property:\n+      const char* class_name_str = _class_name->as_C_string();\n+      if (StringUtils::class_list_match(ForceNonTearable, class_name_str)) {\n+        _must_be_atomic = true;\n+      }\n@@ -5737,0 +6114,46 @@\n+  int itfs_len = _local_interface_indexes == nullptr ? 0 : _local_interface_indexes->length();\n+  _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, nullptr, CHECK);\n+  if (_local_interface_indexes != nullptr) {\n+    for (int i = 0; i < _local_interface_indexes->length(); i++) {\n+      u2 interface_index = _local_interface_indexes->at(i);\n+      Klass* interf;\n+      if (cp->tag_at(interface_index).is_klass()) {\n+        interf = cp->resolved_klass_at(interface_index);\n+      } else {\n+        Symbol* const unresolved_klass  = cp->klass_name_at(interface_index);\n+\n+        \/\/ Don't need to check legal name because it's checked when parsing constant pool.\n+        \/\/ But need to make sure it's not an array type.\n+        guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,\n+                            \"Bad interface name in class file %s\", CHECK);\n+\n+        \/\/ Call resolve on the interface class name with class circularity checking\n+        interf = SystemDictionary::resolve_with_circularity_detection_or_fail(\n+                                                  _class_name,\n+                                                  unresolved_klass,\n+                                                  Handle(THREAD, _loader_data->class_loader()),\n+                                                  false,\n+                                                  CHECK);\n+      }\n+\n+      if (!interf->is_interface()) {\n+        THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                  err_msg(\"class %s can not implement %s, because it is not an interface (%s)\",\n+                          _class_name->as_klass_external_name(),\n+                          interf->external_name(),\n+                          interf->class_in_module_of_loader()));\n+      }\n+\n+      if (EnableValhalla) {\n+        \/\/ Check modifiers and set carries_identity_modifier\/carries_value_modifier flags\n+        check_identity_and_value_modifiers(this, InstanceKlass::cast(interf), CHECK);\n+      }\n+\n+      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n+        _has_nonstatic_concrete_methods = true;\n+      }\n+      _local_interfaces->at_put(i, InstanceKlass::cast(interf));\n+    }\n+  }\n+  assert(_local_interfaces != nullptr, \"invariant\");\n+\n@@ -5764,1 +6187,1 @@\n-  _itable_size = _access_flags.is_interface() ? 0 :\n+  _itable_size = is_interface() ? 0 :\n@@ -5769,3 +6192,80 @@\n-  _field_info = new FieldLayoutInfo();\n-  FieldLayoutBuilder lb(class_name(), super_klass(), _cp, \/*_fields*\/ _temp_field_info,\n-                        _parsed_annotations->is_contended(), _field_info);\n+  if (EnableValhalla) {\n+    _inline_layout_info_array = MetadataFactory::new_array<InlineLayoutInfo>(_loader_data,\n+                                                   java_fields_count(),\n+                                                   CHECK);\n+    for (GrowableArrayIterator<FieldInfo> it = _temp_field_info->begin(); it != _temp_field_info->end(); ++it) {\n+      FieldInfo fieldinfo = *it;\n+      if (fieldinfo.access_flags().is_static()) continue;  \/\/ Only non-static fields are processed at load time\n+      Symbol* sig = fieldinfo.signature(cp);\n+      if (fieldinfo.field_flags().is_null_free_inline_type()) {\n+        \/\/ Pre-load classes of null-free fields that are candidate for flattening\n+        TempNewSymbol s = Signature::strip_envelope(sig);\n+        if (s == _class_name) {\n+          THROW_MSG(vmSymbols::java_lang_ClassCircularityError(), err_msg(\"Class %s cannot have a null-free non-static field of its own type\", _class_name->as_C_string()));\n+        }\n+        log_info(class, preload)(\"Preloading class %s during loading of class %s. Cause: a null-free non-static field is declared with this type\", s->as_C_string(), _class_name->as_C_string());\n+        Klass* klass = SystemDictionary::resolve_with_circularity_detection_or_fail(_class_name, s, Handle(THREAD, _loader_data->class_loader()), false, THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+          log_warning(class, preload)(\"Preloading of class %s during loading of class %s (cause: null-free non-static field) failed: %s\",\n+                                      s->as_C_string(), _class_name->as_C_string(), PENDING_EXCEPTION->klass()->name()->as_C_string());\n+          return; \/\/ Exception is still pending\n+        }\n+        assert(klass != nullptr, \"Sanity check\");\n+        if (klass->access_flags().is_identity_class()) {\n+          assert(klass->is_instance_klass(), \"Sanity check\");\n+          ResourceMark rm(THREAD);\n+          THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                    err_msg(\"Class %s expects class %s to be a value class, but it is an identity class\",\n+                    _class_name->as_C_string(),\n+                    InstanceKlass::cast(klass)->external_name()));\n+        }\n+        if (klass->is_abstract()) {\n+          assert(klass->is_instance_klass(), \"Sanity check\");\n+          ResourceMark rm(THREAD);\n+          THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                    err_msg(\"Class %s expects class %s to be concrete value type, but it is an abstract class\",\n+                    _class_name->as_C_string(),\n+                    InstanceKlass::cast(klass)->external_name()));\n+        }\n+        InlineKlass* vk = InlineKlass::cast(klass);\n+        if (!vk->is_implicitly_constructible()) {\n+          THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                    err_msg(\"class %s is not implicitly constructible and it is used in a null restricted non-static field (not supported)\",\n+                    klass->name()->as_C_string()));\n+        }\n+        _inline_layout_info_array->adr_at(fieldinfo.index())->set_klass(vk);\n+        log_info(class, preload)(\"Preloading of class %s during loading of class %s (cause: null-free non-static field) succeeded\", s->as_C_string(), _class_name->as_C_string());\n+      } else if (Signature::has_envelope(sig)) {\n+        \/\/ Preloading classes for nullable fields that are listed in the LoadableDescriptors attribute\n+        \/\/ Those classes would be required later for the flattening of nullable inline type fields\n+        TempNewSymbol name = Signature::strip_envelope(sig);\n+        if (name != _class_name && is_class_in_loadable_descriptors_attribute(sig)) {\n+          log_info(class, preload)(\"Preloading class %s during loading of class %s. Cause: field type in LoadableDescriptors attribute\", name->as_C_string(), _class_name->as_C_string());\n+          oop loader = loader_data()->class_loader();\n+          Klass* klass = SystemDictionary::resolve_with_circularity_detection_or_fail(_class_name, name, Handle(THREAD, loader), false, THREAD);\n+          if (klass != nullptr) {\n+            if (klass->is_inline_klass()) {\n+              _inline_layout_info_array->adr_at(fieldinfo.index())->set_klass(InlineKlass::cast(klass));\n+              log_info(class, preload)(\"Preloading of class %s during loading of class %s (cause: field type in LoadableDescriptors attribute) succeeded\", name->as_C_string(), _class_name->as_C_string());\n+            } else {\n+              \/\/ Non value class are allowed by the current spec, but it could be an indication of an issue so let's log a warning\n+              log_warning(class, preload)(\"Preloading class %s during loading of class %s (cause: field type in LoadableDescriptors attribute) but loaded class is not a value class\", name->as_C_string(), _class_name->as_C_string());\n+            }\n+            } else {\n+            log_warning(class, preload)(\"Preloading of class %s during loading of class %s (cause: field type in LoadableDescriptors attribute) failed : %s\",\n+                                          name->as_C_string(), _class_name->as_C_string(), PENDING_EXCEPTION->klass()->name()->as_C_string());\n+          }\n+          \/\/ Loads triggered by the LoadableDescriptors attribute are speculative, failures must not impact loading of current class\n+          if (HAS_PENDING_EXCEPTION) {\n+            CLEAR_PENDING_EXCEPTION;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  _layout_info = new FieldLayoutInfo();\n+  FieldLayoutBuilder lb(class_name(), loader_data(), super_klass(), _cp, \/*_fields*\/ _temp_field_info,\n+      _parsed_annotations->is_contended(), is_inline_type(),\n+      access_flags().is_abstract() && !access_flags().is_identity_class() && !access_flags().is_interface(),\n+      _must_be_atomic, _layout_info, _inline_layout_info_array);\n@@ -5773,0 +6273,1 @@\n+  _has_inline_type_fields = _layout_info->_has_inline_fields;\n@@ -5778,0 +6279,1 @@\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":653,"deletions":151,"binary":false,"changes":804,"status":"modified"},{"patch":"@@ -1159,0 +1159,1 @@\n+  bool is_patched = false;\n@@ -1178,2 +1179,16 @@\n-    assert(!CDSConfig::is_dumping_archive(), \"CDS doesn't support --patch-module during dumping\");\n-    stream = search_module_entries(THREAD, _patch_mod_entries, pkg_entry, file_name);\n+    \/\/ At CDS dump time, the --patch-module entries are ignored. That means a\n+    \/\/ class is still loaded from the runtime image even if it might\n+    \/\/ appear in the _patch_mod_entries. The runtime shared class visibility\n+    \/\/ check will determine if a shared class is visible based on the runtime\n+    \/\/ environment, including the runtime --patch-module setting.\n+    if (!CDSConfig::is_valhalla_preview()) {\n+      \/\/ Dynamic dumping requires UseSharedSpaces to be enabled. Since --patch-module\n+      \/\/ is not supported with UseSharedSpaces, we can never come here during dynamic dumping.\n+      assert(!CDSConfig::is_dumping_archive(), \"CDS doesn't support --patch-module during dumping\");\n+    }\n+    if (CDSConfig::is_valhalla_preview() || !CDSConfig::is_dumping_static_archive()) {\n+      stream = search_module_entries(THREAD, _patch_mod_entries, pkg_entry, file_name);\n+      if (stream != nullptr) {\n+        is_patched = true;\n+      }\n+    }\n@@ -1228,0 +1243,4 @@\n+  if (is_patched) {\n+    result->set_shared_classpath_index(0);\n+    result->set_shared_class_loader_type(ClassLoader::BOOT_LOADER);\n+  }\n@@ -1304,0 +1323,4 @@\n+  if (ik->shared_classpath_index() == 0 && ik->is_shared_boot_class()) {\n+    return;\n+  }\n+\n@@ -1391,1 +1414,3 @@\n-    assert(stream->from_boot_loader_modules_image(), \"stream must be loaded by boot loader from modules image\");\n+    if (!CDSConfig::is_valhalla_preview()) {\n+      assert(stream->from_boot_loader_modules_image(), \"stream must be loaded by boot loader from modules image\");\n+    }\n","filename":"src\/hotspot\/share\/classfile\/classLoader.cpp","additions":28,"deletions":3,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -68,0 +68,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -446,0 +447,10 @@\n+void ClassLoaderData::inline_classes_do(void f(InlineKlass*)) {\n+  \/\/ Lock-free access requires load_acquire\n+  for (Klass* k = Atomic::load_acquire(&_klasses); k != nullptr; k = k->next_link()) {\n+    if (k->is_inline_klass()) {\n+      f(InlineKlass::cast(k));\n+    }\n+    assert(k != k->next_link(), \"no loops!\");\n+  }\n+}\n+\n@@ -597,0 +608,2 @@\n+  inline_classes_do(InlineKlass::cleanup);\n+\n@@ -887,1 +900,5 @@\n-        MetadataFactory::free_metadata(this, (InstanceKlass*)m);\n+        if (!((Klass*)m)->is_inline_klass()) {\n+          MetadataFactory::free_metadata(this, (InstanceKlass*)m);\n+        } else {\n+          MetadataFactory::free_metadata(this, (InlineKlass*)m);\n+        }\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.cpp","additions":18,"deletions":1,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -27,0 +27,2 @@\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n@@ -34,0 +36,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -35,0 +38,1 @@\n+#include \"utilities\/powerOfTwo.hpp\"\n@@ -36,0 +40,51 @@\n+static LayoutKind field_layout_selection(FieldInfo field_info, Array<InlineLayoutInfo>* inline_layout_info_array,\n+                                         bool use_atomic_flat) {\n+\n+  if (field_info.field_flags().is_injected()) {\n+    \/\/ don't flatten injected fields\n+    return LayoutKind::REFERENCE;\n+  }\n+\n+  if (inline_layout_info_array == nullptr || inline_layout_info_array->adr_at(field_info.index())->klass() == nullptr) {\n+    \/\/ field's type is not a known value class, using a reference\n+    return LayoutKind::REFERENCE;\n+  }\n+\n+  InlineLayoutInfo* inline_field_info = inline_layout_info_array->adr_at(field_info.index());\n+  InlineKlass* vk = inline_field_info->klass();\n+\n+  if (field_info.field_flags().is_null_free_inline_type()) {\n+    assert(vk->is_implicitly_constructible(), \"null-free fields must be implicitly constructible\");\n+    if (vk->must_be_atomic() || field_info.access_flags().is_volatile() || AlwaysAtomicAccesses) {\n+      if (vk->is_naturally_atomic() && vk->has_non_atomic_layout()) return LayoutKind::NON_ATOMIC_FLAT;\n+      return (vk->has_atomic_layout() && use_atomic_flat) ? LayoutKind::ATOMIC_FLAT : LayoutKind::REFERENCE;\n+    } else {\n+      return vk->has_non_atomic_layout() ? LayoutKind::NON_ATOMIC_FLAT : LayoutKind::REFERENCE;\n+    }\n+  } else {\n+    if (UseNullableValueFlattening && vk->has_nullable_atomic_layout()) {\n+      return use_atomic_flat ? LayoutKind::NULLABLE_ATOMIC_FLAT : LayoutKind::REFERENCE;\n+    } else {\n+      return LayoutKind::REFERENCE;\n+    }\n+  }\n+}\n+\n+static void get_size_and_alignment(InlineKlass* vk, LayoutKind kind, int* size, int* alignment) {\n+  switch(kind) {\n+    case LayoutKind::NON_ATOMIC_FLAT:\n+      *size = vk->non_atomic_size_in_bytes();\n+      *alignment = vk->non_atomic_alignment();\n+      break;\n+    case LayoutKind::ATOMIC_FLAT:\n+      *size = vk->atomic_size_in_bytes();\n+      *alignment = *size;\n+      break;\n+    case LayoutKind::NULLABLE_ATOMIC_FLAT:\n+      *size = vk->nullable_atomic_size_in_bytes();\n+      *alignment = *size;\n+    break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n@@ -40,1 +95,2 @@\n-  _kind(kind),\n+  _inline_klass(nullptr),\n+  _block_kind(kind),\n@@ -44,3 +100,2 @@\n-  _field_index(-1),\n-  _is_reference(false) {\n-  assert(kind == EMPTY || kind == RESERVED || kind == PADDING || kind == INHERITED,\n+  _field_index(-1) {\n+  assert(kind == EMPTY || kind == RESERVED || kind == PADDING || kind == INHERITED || kind == NULL_MARKER,\n@@ -52,1 +107,1 @@\n-LayoutRawBlock::LayoutRawBlock(int index, Kind kind, int size, int alignment, bool is_reference) :\n+LayoutRawBlock::LayoutRawBlock(int index, Kind kind, int size, int alignment) :\n@@ -55,1 +110,2 @@\n- _kind(kind),\n+ _inline_klass(nullptr),\n+ _block_kind(kind),\n@@ -59,3 +115,2 @@\n- _field_index(index),\n- _is_reference(is_reference) {\n-  assert(kind == REGULAR || kind == FLATTENED || kind == INHERITED,\n+ _field_index(index) {\n+  assert(kind == REGULAR || kind == FLAT || kind == INHERITED,\n@@ -77,1 +132,2 @@\n-  _primitive_fields(nullptr),\n+  _small_primitive_fields(nullptr),\n+  _big_primitive_fields(nullptr),\n@@ -84,3 +140,5 @@\n-  LayoutRawBlock* block = new LayoutRawBlock(idx, LayoutRawBlock::REGULAR, size, size \/* alignment == size for primitive types *\/, false);\n-  if (_primitive_fields == nullptr) {\n-    _primitive_fields = new GrowableArray<LayoutRawBlock*>(INITIAL_LIST_SIZE);\n+  LayoutRawBlock* block = new LayoutRawBlock(idx, LayoutRawBlock::REGULAR, size, size \/* alignment == size for primitive types *\/);\n+  if (size >= oopSize) {\n+    add_to_big_primitive_list(block);\n+  } else {\n+    add_to_small_primitive_list(block);\n@@ -88,1 +146,0 @@\n-  _primitive_fields->append(block);\n@@ -93,1 +150,1 @@\n-  LayoutRawBlock* block = new LayoutRawBlock(idx, LayoutRawBlock::REGULAR, size, size \/* alignment == size for oops *\/, true);\n+  LayoutRawBlock* block = new LayoutRawBlock(idx, LayoutRawBlock::REGULAR, size, size \/* alignment == size for oops *\/);\n@@ -101,0 +158,11 @@\n+void FieldGroup::add_flat_field(int idx, InlineKlass* vk, LayoutKind lk, int size, int alignment) {\n+  LayoutRawBlock* block = new LayoutRawBlock(idx, LayoutRawBlock::FLAT, size, alignment);\n+  block->set_inline_klass(vk);\n+  block->set_layout_kind(lk);\n+  if (block->size() >= oopSize) {\n+    add_to_big_primitive_list(block);\n+  } else {\n+    add_to_small_primitive_list(block);\n+  }\n+}\n+\n@@ -102,2 +170,5 @@\n-  if (_primitive_fields != nullptr) {\n-    _primitive_fields->sort(LayoutRawBlock::compare_size_inverted);\n+  if (_small_primitive_fields != nullptr) {\n+    _small_primitive_fields->sort(LayoutRawBlock::compare_size_inverted);\n+  }\n+  if (_big_primitive_fields != nullptr) {\n+    _big_primitive_fields->sort(LayoutRawBlock::compare_size_inverted);\n@@ -107,1 +178,15 @@\n-FieldLayout::FieldLayout(GrowableArray<FieldInfo>* field_info, ConstantPool* cp) :\n+void FieldGroup::add_to_small_primitive_list(LayoutRawBlock* block) {\n+  if (_small_primitive_fields == nullptr) {\n+    _small_primitive_fields = new GrowableArray<LayoutRawBlock*>(INITIAL_LIST_SIZE);\n+  }\n+  _small_primitive_fields->append(block);\n+}\n+\n+void FieldGroup::add_to_big_primitive_list(LayoutRawBlock* block) {\n+  if (_big_primitive_fields == nullptr) {\n+    _big_primitive_fields = new GrowableArray<LayoutRawBlock*>(INITIAL_LIST_SIZE);\n+  }\n+  _big_primitive_fields->append(block);\n+}\n+\n+FieldLayout::FieldLayout(GrowableArray<FieldInfo>* field_info, Array<InlineLayoutInfo>* inline_layout_info_array, ConstantPool* cp) :\n@@ -109,0 +194,1 @@\n+  _inline_layout_info_array(inline_layout_info_array),\n@@ -112,1 +198,8 @@\n-  _last(_blocks) {}\n+  _last(_blocks),\n+  _super_first_field_offset(-1),\n+  _super_alignment(-1),\n+  _super_min_align_required(-1),\n+  _default_value_offset(-1),\n+  _null_reset_value_offset(-1),\n+  _super_has_fields(false),\n+  _has_inherited_fields(false) {}\n@@ -137,1 +230,1 @@\n-    bool has_fields = reconstruct_layout(super_klass);\n+    _super_has_fields = reconstruct_layout(super_klass);\n@@ -139,1 +232,1 @@\n-    if (!super_klass->has_contended_annotations() || !has_fields) {\n+    if ((!super_klass->has_contended_annotations()) || !_super_has_fields) {\n@@ -148,3 +241,6 @@\n-  LayoutRawBlock* block = _start;\n-  while (block->kind() != LayoutRawBlock::INHERITED && block->kind() != LayoutRawBlock::REGULAR\n-      && block->kind() != LayoutRawBlock::FLATTENED && block->kind() != LayoutRawBlock::PADDING) {\n+  LayoutRawBlock* block = _blocks;\n+  while (block != nullptr\n+         && block->block_kind() != LayoutRawBlock::INHERITED\n+         && block->block_kind() != LayoutRawBlock::REGULAR\n+         && block->block_kind() != LayoutRawBlock::FLAT\n+         && block->block_kind() != LayoutRawBlock::NULL_MARKER) {\n@@ -156,3 +252,2 @@\n-\n-\/\/ Insert a set of fields into a layout using a best-fit strategy.\n-\/\/ For each field, search for the smallest empty slot able to fit the field\n+\/\/ Insert a set of fields into a layout.\n+\/\/ For each field, search for an empty slot able to fit the field\n@@ -172,1 +267,0 @@\n-\n@@ -190,0 +284,1 @@\n+\n@@ -191,1 +286,1 @@\n-        if (cursor->kind() == LayoutRawBlock::EMPTY && cursor->fit(b->size(), b->alignment())) {\n+        if (cursor->block_kind() == LayoutRawBlock::EMPTY && cursor->fit(b->size(), b->alignment())) {\n@@ -203,1 +298,1 @@\n-      assert(candidate->kind() == LayoutRawBlock::EMPTY, \"Candidate must be an empty block\");\n+      assert(candidate->block_kind() == LayoutRawBlock::EMPTY, \"Candidate must be an empty block\");\n@@ -206,1 +301,0 @@\n-\n@@ -222,2 +316,2 @@\n-      assert(slot->kind() == LayoutRawBlock::EMPTY, \"Matching slot must be an empty slot\");\n-      assert(slot->size() >= block->offset() + block->size() ,\"Matching slot must be big enough\");\n+      assert(slot->block_kind() == LayoutRawBlock::EMPTY, \"Matching slot must be an empty slot\");\n+      assert(slot->size() >= block->offset() - slot->offset() + block->size() ,\"Matching slot must be big enough\");\n@@ -233,1 +327,3 @@\n-      _field_info->adr_at(block->field_index())->set_offset(block->offset());\n+      if (block->block_kind() == LayoutRawBlock::REGULAR || block->block_kind() == LayoutRawBlock::FLAT) {\n+        _field_info->adr_at(block->field_index())->set_offset(block->offset());\n+      }\n@@ -262,1 +358,1 @@\n-    while (candidate->kind() != LayoutRawBlock::EMPTY || !candidate->fit(size, first->alignment())) {\n+    while (candidate->block_kind() != LayoutRawBlock::EMPTY || !candidate->fit(size, first->alignment())) {\n@@ -270,1 +366,1 @@\n-    assert(candidate->kind() == LayoutRawBlock::EMPTY, \"Candidate must be an empty block\");\n+    assert(candidate->block_kind() == LayoutRawBlock::EMPTY, \"Candidate must be an empty block\");\n@@ -282,1 +378,1 @@\n-  assert(slot->kind() == LayoutRawBlock::EMPTY, \"Blocks can only be inserted in empty blocks\");\n+  assert(slot->block_kind() == LayoutRawBlock::EMPTY, \"Blocks can only be inserted in empty blocks\");\n@@ -288,0 +384,1 @@\n+  assert(block->size() >= block->size(), \"Enough space must remain after adjustment\");\n@@ -292,1 +389,16 @@\n-  _field_info->adr_at(block->field_index())->set_offset(block->offset());\n+  \/\/ NULL_MARKER blocks are not real fields, so they don't have an entry in the FieldInfo array\n+  if (block->block_kind() != LayoutRawBlock::NULL_MARKER) {\n+    _field_info->adr_at(block->field_index())->set_offset(block->offset());\n+    if (_field_info->adr_at(block->field_index())->name(_cp) == vmSymbols::default_value_name()) {\n+      _default_value_offset = block->offset();\n+    }\n+    if (_field_info->adr_at(block->field_index())->name(_cp) == vmSymbols::null_reset_value_name()) {\n+      _null_reset_value_offset = block->offset();\n+    }\n+  }\n+  if (block->block_kind() == LayoutRawBlock::FLAT && block->layout_kind() == LayoutKind::NULLABLE_ATOMIC_FLAT) {\n+    int nm_offset = block->inline_klass()->null_marker_offset() - block->inline_klass()->payload_offset() + block->offset();\n+    _field_info->adr_at(block->field_index())->set_null_marker_offset(nm_offset);\n+    _inline_layout_info_array->adr_at(block->field_index())->set_null_marker_offset(nm_offset);\n+  }\n+\n@@ -298,0 +410,3 @@\n+  if (ik->is_abstract() && !ik->is_identity_class()) {\n+    _super_alignment = type2aelembytes(BasicType::T_LONG);\n+  }\n@@ -305,3 +420,19 @@\n-      int size = type2aelembytes(type);\n-      \/\/ INHERITED blocks are marked as non-reference because oop_maps are handled by their holder class\n-      LayoutRawBlock* block = new LayoutRawBlock(fs.index(), LayoutRawBlock::INHERITED, size, size, false);\n+      _has_inherited_fields = true;\n+      if (_super_first_field_offset == -1 || fs.offset() < _super_first_field_offset) _super_first_field_offset = fs.offset();\n+      LayoutRawBlock* block;\n+      if (fs.is_flat()) {\n+        InlineLayoutInfo layout_info = ik->inline_layout_info(fs.index());\n+        InlineKlass* vk = layout_info.klass();\n+        block = new LayoutRawBlock(fs.index(), LayoutRawBlock::INHERITED,\n+                                   vk->layout_size_in_bytes(layout_info.kind()),\n+                                   vk->layout_alignment(layout_info.kind()));\n+        assert(_super_alignment == -1 || _super_alignment >=  vk->payload_alignment(), \"Invalid value alignment\");\n+        _super_min_align_required = _super_min_align_required > vk->payload_alignment() ? _super_min_align_required : vk->payload_alignment();\n+      } else {\n+        int size = type2aelembytes(type);\n+        \/\/ INHERITED blocks are marked as non-reference because oop_maps are handled by their holder class\n+        block = new LayoutRawBlock(fs.index(), LayoutRawBlock::INHERITED, size, size);\n+        \/\/ For primitive types, the alignment is equal to the size\n+        assert(_super_alignment == -1 || _super_alignment >=  size, \"Invalid value alignment\");\n+        _super_min_align_required = _super_min_align_required > size ? _super_min_align_required : size;\n+      }\n@@ -313,1 +444,0 @@\n-\n@@ -318,1 +448,0 @@\n-\n@@ -343,0 +472,1 @@\n+      \/\/ FIXME it would be better if initial empty block where tagged as PADDING for value classes\n@@ -353,2 +483,1 @@\n-  assert(b->kind() != LayoutRawBlock::EMPTY, \"Sanity check\");\n-\n+  assert(b->block_kind() != LayoutRawBlock::EMPTY, \"Sanity check\");\n@@ -375,1 +504,1 @@\n-  assert(slot->kind() == LayoutRawBlock::EMPTY, \"Blocks can only be inserted in empty blocks\");\n+  assert(slot->block_kind() == LayoutRawBlock::EMPTY, \"Blocks can only be inserted in empty blocks\");\n@@ -391,0 +520,3 @@\n+  if (_start == slot) {\n+    _start = block;\n+  }\n@@ -412,1 +544,77 @@\n-void FieldLayout::print(outputStream* output, bool is_static, const InstanceKlass* super) {\n+void FieldLayout::shift_fields(int shift) {\n+  LayoutRawBlock* b = first_field_block();\n+  LayoutRawBlock* previous = b->prev_block();\n+  if (previous->block_kind() == LayoutRawBlock::EMPTY) {\n+    previous->set_size(previous->size() + shift);\n+  } else {\n+    LayoutRawBlock* nb = new LayoutRawBlock(LayoutRawBlock::PADDING, shift);\n+    nb->set_offset(b->offset());\n+    previous->set_next_block(nb);\n+    nb->set_prev_block(previous);\n+    b->set_prev_block(nb);\n+    nb->set_next_block(b);\n+  }\n+  while (b != nullptr) {\n+    b->set_offset(b->offset() + shift);\n+    if (b->block_kind() == LayoutRawBlock::REGULAR || b->block_kind() == LayoutRawBlock::FLAT) {\n+      _field_info->adr_at(b->field_index())->set_offset(b->offset());\n+      if (b->layout_kind() == LayoutKind::NULLABLE_ATOMIC_FLAT) {\n+        int new_nm_offset = _field_info->adr_at(b->field_index())->null_marker_offset() + shift;\n+        _field_info->adr_at(b->field_index())->set_null_marker_offset(new_nm_offset);\n+        _inline_layout_info_array->adr_at(b->field_index())->set_null_marker_offset(new_nm_offset);\n+\n+      }\n+    }\n+    assert(b->block_kind() == LayoutRawBlock::EMPTY || b->offset() % b->alignment() == 0, \"Must still be correctly aligned\");\n+    b = b->next_block();\n+  }\n+}\n+\n+LayoutRawBlock* FieldLayout::find_null_marker() {\n+  LayoutRawBlock* b = _blocks;\n+  while (b != nullptr) {\n+    if (b->block_kind() == LayoutRawBlock::NULL_MARKER) {\n+      return b;\n+    }\n+    b = b->next_block();\n+  }\n+  ShouldNotReachHere();\n+}\n+\n+void FieldLayout::remove_null_marker() {\n+  LayoutRawBlock* b = first_field_block();\n+  while (b != nullptr) {\n+    if (b->block_kind() == LayoutRawBlock::NULL_MARKER) {\n+      if (b->next_block()->block_kind() == LayoutRawBlock::EMPTY) {\n+        LayoutRawBlock* n = b->next_block();\n+        remove(b);\n+        n->set_offset(b->offset());\n+        n->set_size(n->size() + b->size());\n+      } else {\n+        b->set_block_kind(LayoutRawBlock::EMPTY);\n+      }\n+      return;\n+    }\n+    b = b->next_block();\n+  }\n+  ShouldNotReachHere(); \/\/ if we reach this point, the null marker was not found!\n+}\n+\n+static const char* layout_kind_to_string(LayoutKind lk) {\n+  switch(lk) {\n+    case LayoutKind::REFERENCE:\n+      return \"REFERENCE\";\n+    case LayoutKind::NON_ATOMIC_FLAT:\n+      return \"NON_ATOMIC_FLAT\";\n+    case LayoutKind::ATOMIC_FLAT:\n+      return \"ATOMIC_FLAT\";\n+    case LayoutKind::NULLABLE_ATOMIC_FLAT:\n+      return \"NULLABLE_ATOMIC_FLAT\";\n+    case LayoutKind::UNKNOWN:\n+      return \"UNKNOWN\";\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void FieldLayout::print(outputStream* output, bool is_static, const InstanceKlass* super, Array<InlineLayoutInfo>* inline_fields) {\n@@ -416,1 +624,1 @@\n-    switch(b->kind()) {\n+    switch(b->block_kind()) {\n@@ -419,1 +627,1 @@\n-        output->print_cr(\" @%d \\\"%s\\\" %s %d\/%d %s\",\n+        output->print_cr(\" @%d %s %d\/%d \\\"%s\\\" %s\",\n@@ -421,2 +629,1 @@\n-                         fi->name(_cp)->as_C_string(),\n-                         fi->signature(_cp)->as_C_string(),\n+                         \"REGULAR\",\n@@ -425,1 +632,2 @@\n-                         \"REGULAR\");\n+                         fi->name(_cp)->as_C_string(),\n+                         fi->signature(_cp)->as_C_string());\n@@ -428,1 +636,1 @@\n-      case LayoutRawBlock::FLATTENED: {\n+      case LayoutRawBlock::FLAT: {\n@@ -430,1 +638,3 @@\n-        output->print_cr(\" @%d \\\"%s\\\" %s %d\/%d %s\",\n+        InlineKlass* ik = inline_fields->adr_at(fi->index())->klass();\n+        assert(ik != nullptr, \"\");\n+        output->print_cr(\" @%d %s %d\/%d \\\"%s\\\" %s %s@%p %s\",\n@@ -432,2 +642,1 @@\n-                         fi->name(_cp)->as_C_string(),\n-                         fi->signature(_cp)->as_C_string(),\n+                         \"FLAT\",\n@@ -436,1 +645,4 @@\n-                         \"FLATTENED\");\n+                         fi->name(_cp)->as_C_string(),\n+                         fi->signature(_cp)->as_C_string(),\n+                         ik->name()->as_C_string(),\n+                         ik->class_loader_data(), layout_kind_to_string(b->layout_kind()));\n@@ -440,1 +652,1 @@\n-        output->print_cr(\" @%d %d\/- %s\",\n+        output->print_cr(\" @%d %s %d\/-\",\n@@ -442,2 +654,2 @@\n-                         b->size(),\n-                         \"RESERVED\");\n+                         \"RESERVED\",\n+                         b->size());\n@@ -453,2 +665,2 @@\n-            if (fs.offset() == b->offset()) {\n-              output->print_cr(\" @%d \\\"%s\\\" %s %d\/%d %s\",\n+            if (fs.offset() == b->offset() && fs.access_flags().is_static() == is_static) {\n+              output->print_cr(\" @%d %s %d\/%d \\\"%s\\\" %s\",\n@@ -456,2 +668,1 @@\n-                  fs.name()->as_C_string(),\n-                  fs.signature()->as_C_string(),\n+                  \"INHERITED\",\n@@ -459,2 +670,3 @@\n-                  b->size(), \/\/ so far, alignment constraint == size, will change with Valhalla\n-                  \"INHERITED\");\n+                  b->size(), \/\/ so far, alignment constraint == size, will change with Valhalla => FIXME\n+                  fs.name()->as_C_string(),\n+                  fs.signature()->as_C_string());\n@@ -464,3 +676,1 @@\n-          }\n-          ik = ik->java_super();\n-        break;\n+        ik = ik->java_super();\n@@ -469,12 +679,24 @@\n-      case LayoutRawBlock::EMPTY:\n-        output->print_cr(\" @%d %d\/1 %s\",\n-                         b->offset(),\n-                         b->size(),\n-                        \"EMPTY\");\n-        break;\n-      case LayoutRawBlock::PADDING:\n-        output->print_cr(\" @%d %d\/1 %s\",\n-                         b->offset(),\n-                         b->size(),\n-                        \"PADDING\");\n-        break;\n+      break;\n+    }\n+    case LayoutRawBlock::EMPTY:\n+      output->print_cr(\" @%d %s %d\/1\",\n+                       b->offset(),\n+                      \"EMPTY\",\n+                       b->size());\n+      break;\n+    case LayoutRawBlock::PADDING:\n+      output->print_cr(\" @%d %s %d\/1\",\n+                      b->offset(),\n+                      \"PADDING\",\n+                      b->size());\n+      break;\n+    case LayoutRawBlock::NULL_MARKER:\n+    {\n+      output->print_cr(\" @%d %s %d\/1 \",\n+                      b->offset(),\n+                      \"NULL_MARKER\",\n+                      b->size());\n+      break;\n+    }\n+    default:\n+      fatal(\"Unknown block type\");\n@@ -486,2 +708,3 @@\n-FieldLayoutBuilder::FieldLayoutBuilder(const Symbol* classname, const InstanceKlass* super_klass, ConstantPool* constant_pool,\n-      GrowableArray<FieldInfo>* field_info, bool is_contended, FieldLayoutInfo* info) :\n+FieldLayoutBuilder::FieldLayoutBuilder(const Symbol* classname, ClassLoaderData* loader_data, const InstanceKlass* super_klass, ConstantPool* constant_pool,\n+                                       GrowableArray<FieldInfo>* field_info, bool is_contended, bool is_inline_type,bool is_abstract_value,\n+                                       bool must_be_atomic, FieldLayoutInfo* info, Array<InlineLayoutInfo>* inline_layout_info_array) :\n@@ -489,0 +712,1 @@\n+  _loader_data(loader_data),\n@@ -493,0 +717,1 @@\n+  _inline_layout_info_array(inline_layout_info_array),\n@@ -499,1 +724,13 @@\n-  _alignment(-1),\n+  _payload_alignment(-1),\n+  _payload_offset(-1),\n+  _null_marker_offset(-1),\n+  _payload_size_in_bytes(-1),\n+  _non_atomic_layout_size_in_bytes(-1),\n+  _non_atomic_layout_alignment(-1),\n+  _atomic_layout_size_in_bytes(-1),\n+  _nullable_layout_size_in_bytes(-1),\n+  _fields_size_sum(0),\n+  _declared_non_static_fields_count(0),\n+  _has_non_naturally_atomic_fields(false),\n+  _is_naturally_atomic(false),\n+  _must_be_atomic(must_be_atomic),\n@@ -501,2 +738,6 @@\n-  _is_contended(is_contended) {}\n-\n+  _has_inline_type_fields(false),\n+  _is_contended(is_contended),\n+  _is_inline_type(is_inline_type),\n+  _is_abstract_value(is_abstract_value),\n+  _has_flattening_information(is_inline_type),\n+  _is_empty_inline_class(false) {}\n@@ -517,1 +758,1 @@\n-  _layout = new FieldLayout(_field_info, _constant_pool);\n+  _layout = new FieldLayout(_field_info, _inline_layout_info_array, _constant_pool);\n@@ -520,0 +761,1 @@\n+  _nonstatic_oopmap_count = super_klass == nullptr ? 0 : super_klass->nonstatic_oop_map_count();\n@@ -523,1 +765,1 @@\n-  _static_layout = new FieldLayout(_field_info, _constant_pool);\n+  _static_layout = new FieldLayout(_field_info, _inline_layout_info_array, _constant_pool);\n@@ -529,1 +771,1 @@\n-\/\/ Field sorting for regular classes:\n+\/\/ Field sorting for regular (non-inline) classes:\n@@ -534,0 +776,1 @@\n+\/\/   - field flattening decisions are taken in this method\n@@ -537,1 +780,0 @@\n-    FieldInfo ctrl = _field_info->at(0);\n@@ -559,12 +801,22 @@\n-      case T_BYTE:\n-      case T_CHAR:\n-      case T_DOUBLE:\n-      case T_FLOAT:\n-      case T_INT:\n-      case T_LONG:\n-      case T_SHORT:\n-      case T_BOOLEAN:\n-        group->add_primitive_field(idx, type);\n-        break;\n-      case T_OBJECT:\n-      case T_ARRAY:\n+    case T_BYTE:\n+    case T_CHAR:\n+    case T_DOUBLE:\n+    case T_FLOAT:\n+    case T_INT:\n+    case T_LONG:\n+    case T_SHORT:\n+    case T_BOOLEAN:\n+      group->add_primitive_field(idx, type);\n+      break;\n+    case T_OBJECT:\n+    case T_ARRAY:\n+    {\n+      LayoutKind lk = field_layout_selection(fieldinfo, _inline_layout_info_array, true);\n+      if (fieldinfo.field_flags().is_null_free_inline_type() || lk != LayoutKind::REFERENCE\n+          || (!fieldinfo.field_flags().is_injected()\n+              && _inline_layout_info_array != nullptr && _inline_layout_info_array->adr_at(fieldinfo.index())->klass() != nullptr\n+              && !_inline_layout_info_array->adr_at(fieldinfo.index())->klass()->is_identity_class())) {\n+        _has_inline_type_fields = true;\n+        _has_flattening_information = true;\n+      }\n+      if (lk == LayoutKind::REFERENCE) {\n@@ -573,3 +825,16 @@\n-        break;\n-      default:\n-        fatal(\"Something wrong?\");\n+      } else {\n+        _has_flattening_information = true;\n+        InlineKlass* vk = _inline_layout_info_array->adr_at(fieldinfo.index())->klass();\n+        int size, alignment;\n+        get_size_and_alignment(vk, lk, &size, &alignment);\n+        group->add_flat_field(idx, vk, lk, size, alignment);\n+        _inline_layout_info_array->adr_at(fieldinfo.index())->set_kind(lk);\n+        _nonstatic_oopmap_count += vk->nonstatic_oop_map_count();\n+        _field_info->adr_at(idx)->field_flags_addr()->update_flat(true);\n+        _field_info->adr_at(idx)->set_layout_kind(lk);\n+        \/\/ no need to update _must_be_atomic if vk->must_be_atomic() is true because current class is not an inline class\n+      }\n+      break;\n+    }\n+    default:\n+      fatal(\"Something wrong?\");\n@@ -587,0 +852,84 @@\n+\/* Field sorting for inline classes:\n+ *   - because inline classes are immutable, the @Contended annotation is ignored\n+ *     when computing their layout (with only read operation, there's no false\n+ *     sharing issue)\n+ *   - this method also records the alignment of the field with the most\n+ *     constraining alignment, this value is then used as the alignment\n+ *     constraint when flattening this inline type into another container\n+ *   - field flattening decisions are taken in this method (those decisions are\n+ *     currently only based in the size of the fields to be flattened, the size\n+ *     of the resulting instance is not considered)\n+ *\/\n+void FieldLayoutBuilder::inline_class_field_sorting() {\n+  assert(_is_inline_type || _is_abstract_value, \"Should only be used for inline classes\");\n+  int alignment = -1;\n+  int idx = 0;\n+  for (GrowableArrayIterator<FieldInfo> it = _field_info->begin(); it != _field_info->end(); ++it, ++idx) {\n+    FieldGroup* group = nullptr;\n+    FieldInfo fieldinfo = *it;\n+    int field_alignment = 1;\n+    if (fieldinfo.access_flags().is_static()) {\n+      group = _static_fields;\n+    } else {\n+      _has_nonstatic_fields = true;\n+      _declared_non_static_fields_count++;\n+      group = _root_group;\n+    }\n+    assert(group != nullptr, \"invariant\");\n+    BasicType type = Signature::basic_type(fieldinfo.signature(_constant_pool));\n+    switch(type) {\n+    case T_BYTE:\n+    case T_CHAR:\n+    case T_DOUBLE:\n+    case T_FLOAT:\n+    case T_INT:\n+    case T_LONG:\n+    case T_SHORT:\n+    case T_BOOLEAN:\n+      if (group != _static_fields) {\n+        field_alignment = type2aelembytes(type); \/\/ alignment == size for primitive types\n+      }\n+      group->add_primitive_field(fieldinfo.index(), type);\n+      break;\n+    case T_OBJECT:\n+    case T_ARRAY:\n+    {\n+      bool use_atomic_flat = _must_be_atomic; \/\/ flatten atomic fields only if the container is itself atomic\n+      LayoutKind lk = field_layout_selection(fieldinfo, _inline_layout_info_array, use_atomic_flat);\n+      if (fieldinfo.field_flags().is_null_free_inline_type() || lk != LayoutKind::REFERENCE\n+          || (!fieldinfo.field_flags().is_injected()\n+              && _inline_layout_info_array != nullptr && _inline_layout_info_array->adr_at(fieldinfo.index())->klass() != nullptr\n+              && !_inline_layout_info_array->adr_at(fieldinfo.index())->klass()->is_identity_class())) {\n+        _has_inline_type_fields = true;\n+        _has_flattening_information = true;\n+      }\n+      if (lk == LayoutKind::REFERENCE) {\n+        if (group != _static_fields) {\n+          _nonstatic_oopmap_count++;\n+          field_alignment = type2aelembytes(type); \/\/ alignment == size for oops\n+        }\n+        group->add_oop_field(idx);\n+      } else {\n+        _has_flattening_information = true;\n+        InlineKlass* vk = _inline_layout_info_array->adr_at(fieldinfo.index())->klass();\n+        if (!vk->is_naturally_atomic()) _has_non_naturally_atomic_fields = true;\n+        int size, alignment;\n+        get_size_and_alignment(vk, lk, &size, &alignment);\n+        group->add_flat_field(idx, vk, lk, size, alignment);\n+        _inline_layout_info_array->adr_at(fieldinfo.index())->set_kind(lk);\n+        _nonstatic_oopmap_count += vk->nonstatic_oop_map_count();\n+        field_alignment = alignment;\n+        _field_info->adr_at(idx)->field_flags_addr()->update_flat(true);\n+        _field_info->adr_at(idx)->set_layout_kind(lk);\n+      }\n+      break;\n+    }\n+    default:\n+      fatal(\"Unexpected BasicType\");\n+    }\n+    if (!fieldinfo.access_flags().is_static() && field_alignment > alignment) alignment = field_alignment;\n+  }\n+  _payload_alignment = alignment;\n+  assert(_has_nonstatic_fields || _is_abstract_value, \"Concrete value types do not support zero instance size yet\");\n+}\n+\n@@ -594,5 +943,7 @@\n-\/\/ Computation of regular classes layout is an evolution of the previous default layout\n-\/\/ (FieldAllocationStyle 1):\n-\/\/   - primitive fields are allocated first (from the biggest to the smallest)\n-\/\/   - then oop fields are allocated, either in existing gaps or at the end of\n-\/\/     the layout\n+\/* Computation of regular classes layout is an evolution of the previous default layout\n+ * (FieldAllocationStyle 1):\n+ *   - primitive fields (both primitive types and flat inline types) are allocated\n+ *     first, from the biggest to the smallest\n+ *   - then oop fields are allocated (to increase chances to have contiguous oops and\n+ *     a simpler oopmap).\n+ *\/\n@@ -603,1 +954,0 @@\n-\n@@ -611,1 +961,2 @@\n-  _layout->add(_root_group->primitive_fields());\n+  _layout->add(_root_group->big_primitive_fields());\n+  _layout->add(_root_group->small_primitive_fields());\n@@ -619,1 +970,2 @@\n-      _layout->add(cg->primitive_fields(), start);\n+      _layout->add(cg->big_primitive_fields());\n+      _layout->add(cg->small_primitive_fields(), start);\n@@ -629,2 +981,4 @@\n-  _static_layout->add_contiguously(this->_static_fields->oop_fields());\n-  _static_layout->add(this->_static_fields->primitive_fields());\n+  \/\/ Warning: IntanceMirrorKlass expects static oops to be allocated first\n+  _static_layout->add_contiguously(_static_fields->oop_fields());\n+  _static_layout->add(_static_fields->big_primitive_fields());\n+  _static_layout->add(_static_fields->small_primitive_fields());\n@@ -635,4 +989,13 @@\n-void FieldLayoutBuilder::epilogue() {\n-  \/\/ Computing oopmaps\n-  int super_oop_map_count = (_super_klass == nullptr) ? 0 :_super_klass->nonstatic_oop_map_count();\n-  int max_oop_map_count = super_oop_map_count + _nonstatic_oopmap_count;\n+\/* Computation of inline classes has a slightly different strategy than for\n+ * regular classes. Regular classes have their oop fields allocated at the end\n+ * of the layout to increase GC performances. Unfortunately, this strategy\n+ * increases the number of empty slots inside an instance. Because the purpose\n+ * of inline classes is to be embedded into other containers, it is critical\n+ * to keep their size as small as possible. For this reason, the allocation\n+ * strategy is:\n+ *   - big primitive fields (primitive types and flat inline type smaller\n+ *     than an oop) are allocated first (from the biggest to the smallest)\n+ *   - then oop fields\n+ *   - then small primitive fields (from the biggest to the smallest)\n+ *\/\n+void FieldLayoutBuilder::compute_inline_class_layout() {\n@@ -640,5 +1003,76 @@\n-  OopMapBlocksBuilder* nonstatic_oop_maps =\n-      new OopMapBlocksBuilder(max_oop_map_count);\n-  if (super_oop_map_count > 0) {\n-    nonstatic_oop_maps->initialize_inherited_blocks(_super_klass->start_of_nonstatic_oop_maps(),\n-    _super_klass->nonstatic_oop_map_count());\n+  \/\/ Test if the concrete inline class is an empty class (no instance fields)\n+  \/\/ and insert a dummy field if needed\n+  if (!_is_abstract_value) {\n+    bool declares_non_static_fields = false;\n+    for (GrowableArrayIterator<FieldInfo> it = _field_info->begin(); it != _field_info->end(); ++it) {\n+      FieldInfo fieldinfo = *it;\n+      if (!fieldinfo.access_flags().is_static()) {\n+        declares_non_static_fields = true;\n+        break;\n+      }\n+    }\n+    if (!declares_non_static_fields) {\n+      bool has_inherited_fields = false;\n+      const InstanceKlass* super = _super_klass;\n+      while(super != nullptr) {\n+        if (super->has_nonstatic_fields()) {\n+          has_inherited_fields = true;\n+          break;\n+        }\n+        super = super->super() == nullptr ? nullptr : InstanceKlass::cast(super->super());\n+      }\n+\n+      if (!has_inherited_fields) {\n+        \/\/ Inject \".empty\" dummy field\n+        _is_empty_inline_class = true;\n+        FieldInfo::FieldFlags fflags(0);\n+        fflags.update_injected(true);\n+        AccessFlags aflags;\n+        FieldInfo fi(aflags,\n+                    (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(empty_marker_name)),\n+                    (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(byte_signature)),\n+                    0,\n+                    fflags);\n+        int idx = _field_info->append(fi);\n+        _field_info->adr_at(idx)->set_index(idx);\n+      }\n+    }\n+  }\n+\n+  prologue();\n+  inline_class_field_sorting();\n+\n+  assert(_layout->start()->block_kind() == LayoutRawBlock::RESERVED, \"Unexpected\");\n+\n+  if (_layout->super_has_fields() && !_is_abstract_value) {  \/\/ non-static field layout\n+    if (!_has_nonstatic_fields) {\n+      assert(_is_abstract_value, \"Concrete value types have at least one field\");\n+      \/\/ Nothing to do\n+    } else {\n+      \/\/ decide which alignment to use, then set first allowed field offset\n+\n+      assert(_layout->super_alignment() >= _payload_alignment, \"Incompatible alignment\");\n+      assert(_layout->super_alignment() % _payload_alignment == 0, \"Incompatible alignment\");\n+\n+      if (_payload_alignment < _layout->super_alignment()) {\n+        int new_alignment = _payload_alignment > _layout->super_min_align_required() ? _payload_alignment : _layout->super_min_align_required();\n+        assert(new_alignment % _payload_alignment == 0, \"Must be\");\n+        assert(new_alignment % _layout->super_min_align_required() == 0, \"Must be\");\n+        _payload_alignment = new_alignment;\n+      }\n+      _layout->set_start(_layout->first_field_block());\n+    }\n+  } else {\n+    if (_is_abstract_value && _has_nonstatic_fields) {\n+      _payload_alignment = type2aelembytes(BasicType::T_LONG);\n+    }\n+    assert(_layout->start()->next_block()->block_kind() == LayoutRawBlock::EMPTY || !UseCompressedClassPointers, \"Unexpected\");\n+    LayoutRawBlock* first_empty = _layout->start()->next_block();\n+    if (first_empty->offset() % _payload_alignment != 0) {\n+      LayoutRawBlock* padding = new LayoutRawBlock(LayoutRawBlock::PADDING, _payload_alignment - (first_empty->offset() % _payload_alignment));\n+      _layout->insert(first_empty, padding);\n+      if (first_empty->size() == 0) {\n+        _layout->remove(first_empty);\n+      }\n+      _layout->set_start(padding);\n+    }\n@@ -647,3 +1081,168 @@\n-  if (_root_group->oop_fields() != nullptr) {\n-    for (int i = 0; i < _root_group->oop_fields()->length(); i++) {\n-      LayoutRawBlock* b = _root_group->oop_fields()->at(i);\n+  _layout->add(_root_group->big_primitive_fields());\n+  _layout->add(_root_group->oop_fields());\n+  _layout->add(_root_group->small_primitive_fields());\n+\n+  LayoutRawBlock* first_field = _layout->first_field_block();\n+  if (first_field != nullptr) {\n+    _payload_offset = _layout->first_field_block()->offset();\n+    _payload_size_in_bytes = _layout->last_block()->offset() - _layout->first_field_block()->offset();\n+  } else {\n+    assert(_is_abstract_value, \"Concrete inline types must have at least one field\");\n+    _payload_offset = _layout->blocks()->size();\n+    _payload_size_in_bytes = 0;\n+  }\n+\n+  \/\/ Determining if the value class is naturally atomic:\n+  if ((!_layout->super_has_fields() && _declared_non_static_fields_count <= 1 && !_has_non_naturally_atomic_fields)\n+      || (_layout->super_has_fields() && _super_klass->is_naturally_atomic() && _declared_non_static_fields_count == 0)) {\n+        _is_naturally_atomic = true;\n+  }\n+\n+  \/\/ At this point, the characteristics of the raw layout (used in standalone instances) are known.\n+  \/\/ From this, additional layouts will be computed: atomic and nullable layouts\n+  \/\/ Once those additional layouts are computed, the raw layout might need some adjustments\n+\n+  if (!_is_abstract_value) { \/\/ Flat layouts are only for concrete value classes\n+    \/\/ Validation of the non atomic layout\n+    if (UseNonAtomicValueFlattening && !AlwaysAtomicAccesses && (!_must_be_atomic || _is_naturally_atomic)) {\n+      _non_atomic_layout_size_in_bytes = _payload_size_in_bytes;\n+      _non_atomic_layout_alignment = _payload_alignment;\n+    }\n+\n+    \/\/ Next step is to compute the characteristics for a layout enabling atomic updates\n+    if (UseAtomicValueFlattening) {\n+      int atomic_size = _payload_size_in_bytes == 0 ? 0 : round_up_power_of_2(_payload_size_in_bytes);\n+      if (atomic_size <= (int)MAX_ATOMIC_OP_SIZE && UseFieldFlattening) {\n+        _atomic_layout_size_in_bytes = atomic_size;\n+      }\n+    }\n+\n+    \/\/ Next step is the nullable layout: the layout must include a null marker and must also be atomic\n+    if (UseNullableValueFlattening) {\n+      \/\/ Looking if there's an empty slot inside the layout that could be used to store a null marker\n+      \/\/ FIXME: could it be possible to re-use the .empty field as a null marker for empty values?\n+      LayoutRawBlock* b = _layout->first_field_block();\n+      assert(b != nullptr, \"A concrete value class must have at least one (possible dummy) field\");\n+      int null_marker_offset = -1;\n+      if (_is_empty_inline_class) {\n+        \/\/ Reusing the dummy field as a field marker\n+        assert(_field_info->adr_at(b->field_index())->name(_constant_pool) == vmSymbols::empty_marker_name(), \"b must be the dummy field\");\n+        null_marker_offset = b->offset();\n+      } else {\n+        while (b != _layout->last_block()) {\n+          if (b->block_kind() == LayoutRawBlock::EMPTY) {\n+            break;\n+          }\n+          b = b->next_block();\n+        }\n+        if (b != _layout->last_block()) {\n+          \/\/ found an empty slot, register its offset from the beginning of the payload\n+          null_marker_offset = b->offset();\n+          LayoutRawBlock* marker = new LayoutRawBlock(LayoutRawBlock::NULL_MARKER, 1);\n+          _layout->add_field_at_offset(marker, b->offset());\n+        }\n+        if (null_marker_offset == -1) { \/\/ no empty slot available to store the null marker, need to inject one\n+          int last_offset = _layout->last_block()->offset();\n+          LayoutRawBlock* marker = new LayoutRawBlock(LayoutRawBlock::NULL_MARKER, 1);\n+          _layout->insert_field_block(_layout->last_block(), marker);\n+          assert(marker->offset() == last_offset, \"Null marker should have been inserted at the end\");\n+          null_marker_offset = marker->offset();\n+        }\n+      }\n+\n+      \/\/ Now that the null marker is there, the size of the nullable layout must computed (remember, must be atomic too)\n+      int new_raw_size = _layout->last_block()->offset() - _layout->first_field_block()->offset();\n+      int nullable_size = round_up_power_of_2(new_raw_size);\n+      if (nullable_size <= (int)MAX_ATOMIC_OP_SIZE && UseFieldFlattening) {\n+        _nullable_layout_size_in_bytes = nullable_size;\n+        _null_marker_offset = null_marker_offset;\n+      } else {\n+        \/\/ If the nullable layout is rejected, the NULL_MARKER block should be removed\n+        \/\/ from the layout, otherwise it will appear anyway if the layout is printer\n+        if (!_is_empty_inline_class) {  \/\/ empty values don't have a dedicated NULL_MARKER block\n+          _layout->remove_null_marker();\n+        }\n+        _null_marker_offset = -1;\n+      }\n+    }\n+    \/\/ If the inline class has an atomic or nullable (which is also atomic) layout,\n+    \/\/ we want the raw layout to have the same alignment as those atomic layouts so access codes\n+    \/\/ could remain  simple (single instruction without intermediate copy). This might required\n+    \/\/ to shift all fields in the raw layout, but this operation is possible only if the class\n+    \/\/ doesn't have inherited fields (offsets of inherited fields cannot be changed). If a\n+    \/\/ field shift is needed but not possible, all atomic layouts are disabled and only reference\n+    \/\/ and loosely consistent are supported.\n+    int required_alignment = _payload_alignment;\n+    if (has_atomic_layout() && _payload_alignment < atomic_layout_size_in_bytes()) {\n+      required_alignment = atomic_layout_size_in_bytes();\n+    }\n+    if (has_nullable_atomic_layout() && _payload_alignment < nullable_layout_size_in_bytes()) {\n+      required_alignment = nullable_layout_size_in_bytes();\n+    }\n+    int shift = first_field->offset() % required_alignment;\n+    if (shift != 0) {\n+      if (required_alignment > _payload_alignment && !_layout->has_inherited_fields()) {\n+        assert(_layout->first_field_block() != nullptr, \"A concrete value class must have at least one (possible dummy) field\");\n+        _layout->shift_fields(shift);\n+        _payload_offset = _layout->first_field_block()->offset();\n+        if (has_nullable_atomic_layout()) {\n+          assert(!_is_empty_inline_class, \"Should not get here with empty values\");\n+          _null_marker_offset = _layout->find_null_marker()->offset();\n+        }\n+        _payload_alignment = required_alignment;\n+      } else {\n+        _atomic_layout_size_in_bytes = -1;\n+        if (has_nullable_atomic_layout() && !_is_empty_inline_class) {  \/\/ empty values don't have a dedicated NULL_MARKER block\n+          _layout->remove_null_marker();\n+        }\n+        _nullable_layout_size_in_bytes = -1;\n+        _null_marker_offset = -1;\n+      }\n+    } else {\n+      _payload_alignment = required_alignment;\n+    }\n+\n+    \/\/ If the inline class has a nullable layout, the layout used in heap allocated standalone\n+    \/\/ instances must also be the nullable layout, in order to be able to set the null marker to\n+    \/\/ non-null before copying the payload to other containers.\n+    if (has_nullable_atomic_layout() && payload_layout_size_in_bytes() < nullable_layout_size_in_bytes()) {\n+      _payload_size_in_bytes = nullable_layout_size_in_bytes();\n+    }\n+  }\n+  \/\/ Warning:: InstanceMirrorKlass expects static oops to be allocated first\n+  _static_layout->add_contiguously(_static_fields->oop_fields());\n+  _static_layout->add(_static_fields->big_primitive_fields());\n+  _static_layout->add(_static_fields->small_primitive_fields());\n+\n+  epilogue();\n+}\n+\n+void FieldLayoutBuilder::add_flat_field_oopmap(OopMapBlocksBuilder* nonstatic_oop_maps,\n+                InlineKlass* vklass, int offset) {\n+  int diff = offset - vklass->payload_offset();\n+  const OopMapBlock* map = vklass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* last_map = map + vklass->nonstatic_oop_map_count();\n+  while (map < last_map) {\n+    nonstatic_oop_maps->add(map->offset() + diff, map->count());\n+    map++;\n+  }\n+}\n+\n+void FieldLayoutBuilder::register_embedded_oops_from_list(OopMapBlocksBuilder* nonstatic_oop_maps, GrowableArray<LayoutRawBlock*>* list) {\n+  if (list == nullptr) return;\n+  for (int i = 0; i < list->length(); i++) {\n+    LayoutRawBlock* f = list->at(i);\n+    if (f->block_kind() == LayoutRawBlock::FLAT) {\n+      InlineKlass* vk = f->inline_klass();\n+      assert(vk != nullptr, \"Should have been initialized\");\n+      if (vk->contains_oops()) {\n+        add_flat_field_oopmap(nonstatic_oop_maps, vk, f->offset());\n+      }\n+    }\n+  }\n+}\n+\n+void FieldLayoutBuilder::register_embedded_oops(OopMapBlocksBuilder* nonstatic_oop_maps, FieldGroup* group) {\n+  if (group->oop_fields() != nullptr) {\n+    for (int i = 0; i < group->oop_fields()->length(); i++) {\n+      LayoutRawBlock* b = group->oop_fields()->at(i);\n@@ -653,0 +1252,3 @@\n+  register_embedded_oops_from_list(nonstatic_oop_maps, group->big_primitive_fields());\n+  register_embedded_oops_from_list(nonstatic_oop_maps, group->small_primitive_fields());\n+}\n@@ -654,0 +1256,10 @@\n+void FieldLayoutBuilder::epilogue() {\n+  \/\/ Computing oopmaps\n+  OopMapBlocksBuilder* nonstatic_oop_maps =\n+      new OopMapBlocksBuilder(_nonstatic_oopmap_count);\n+  int super_oop_map_count = (_super_klass == nullptr) ? 0 :_super_klass->nonstatic_oop_map_count();\n+  if (super_oop_map_count > 0) {\n+    nonstatic_oop_maps->initialize_inherited_blocks(_super_klass->start_of_nonstatic_oop_maps(),\n+    _super_klass->nonstatic_oop_map_count());\n+  }\n+  register_embedded_oops(nonstatic_oop_maps, _root_group);\n@@ -659,1 +1271,1 @@\n-        nonstatic_oop_maps->add(cg->oop_fields()->at(0)->offset(), cg->oop_count());\n+        register_embedded_oops(nonstatic_oop_maps, cg);\n@@ -663,1 +1275,0 @@\n-\n@@ -679,0 +1290,48 @@\n+  _info->_has_inline_fields = _has_inline_type_fields;\n+  _info->_is_naturally_atomic = _is_naturally_atomic;\n+  if (_is_inline_type) {\n+    _info->_must_be_atomic = _must_be_atomic;\n+    _info->_payload_alignment = _payload_alignment;\n+    _info->_payload_offset = _payload_offset;\n+    _info->_payload_size_in_bytes = _payload_size_in_bytes;\n+    _info->_non_atomic_size_in_bytes = _non_atomic_layout_size_in_bytes;\n+    _info->_non_atomic_alignment = _non_atomic_layout_alignment;\n+    _info->_atomic_layout_size_in_bytes = _atomic_layout_size_in_bytes;\n+    _info->_nullable_layout_size_in_bytes = _nullable_layout_size_in_bytes;\n+    _info->_null_marker_offset = _null_marker_offset;\n+    _info->_default_value_offset = _static_layout->default_value_offset();\n+    _info->_null_reset_value_offset = _static_layout->null_reset_value_offset();\n+    _info->_is_empty_inline_klass = _is_empty_inline_class;\n+  }\n+\n+  \/\/ This may be too restrictive, since if all the fields fit in 64\n+  \/\/ bits we could make the decision to align instances of this class\n+  \/\/ to 64-bit boundaries, and load and store them as single words.\n+  \/\/ And on machines which supported larger atomics we could similarly\n+  \/\/ allow larger values to be atomic, if properly aligned.\n+\n+#ifdef ASSERT\n+  \/\/ Tests verifying integrity of field layouts are using the output of -XX:+PrintFieldLayout\n+  \/\/ which prints the details of LayoutRawBlocks used to compute the layout.\n+  \/\/ The code below checks that offsets in the _field_info meta-data match offsets\n+  \/\/ in the LayoutRawBlocks\n+  LayoutRawBlock* b = _layout->blocks();\n+  while(b != _layout->last_block()) {\n+    if (b->block_kind() == LayoutRawBlock::REGULAR || b->block_kind() == LayoutRawBlock::FLAT) {\n+      if (_field_info->adr_at(b->field_index())->offset() != (u4)b->offset()) {\n+        tty->print_cr(\"Offset from field info = %d, offset from block = %d\", (int)_field_info->adr_at(b->field_index())->offset(), b->offset());\n+      }\n+      assert(_field_info->adr_at(b->field_index())->offset() == (u4)b->offset(),\" Must match\");\n+    }\n+    b = b->next_block();\n+  }\n+  b = _static_layout->blocks();\n+  while(b != _static_layout->last_block()) {\n+    if (b->block_kind() == LayoutRawBlock::REGULAR || b->block_kind() == LayoutRawBlock::FLAT) {\n+      assert(_field_info->adr_at(b->field_index())->offset() == (u4)b->offset(),\" Must match\");\n+    }\n+    b = b->next_block();\n+  }\n+#endif \/\/ ASSERT\n+\n+  static bool first_layout_print = true;\n@@ -680,1 +1339,2 @@\n-  if (PrintFieldLayout) {\n+\n+  if (PrintFieldLayout || (PrintInlineLayout && _has_flattening_information)) {\n@@ -682,7 +1342,42 @@\n-    tty->print_cr(\"Layout of class %s\", _classname->as_C_string());\n-    tty->print_cr(\"Instance fields:\");\n-    _layout->print(tty, false, _super_klass);\n-    tty->print_cr(\"Static fields:\");\n-    _static_layout->print(tty, true, nullptr);\n-    tty->print_cr(\"Instance size = %d bytes\", _info->_instance_size * wordSize);\n-    tty->print_cr(\"---\");\n+    stringStream st;\n+    if (first_layout_print) {\n+      st.print_cr(\"Field layout log format: @offset size\/alignment [name] [signature] [comment]\");\n+      st.print_cr(\"Heap oop size = %d\", heapOopSize);\n+      first_layout_print = false;\n+    }\n+    if (_super_klass != nullptr) {\n+      st.print_cr(\"Layout of class %s@%p extends %s@%p\", _classname->as_C_string(),\n+                    _loader_data, _super_klass->name()->as_C_string(), _super_klass->class_loader_data());\n+    } else {\n+      st.print_cr(\"Layout of class %s@%p\", _classname->as_C_string(), _loader_data);\n+    }\n+    st.print_cr(\"Instance fields:\");\n+    _layout->print(&st, false, _super_klass, _inline_layout_info_array);\n+    st.print_cr(\"Static fields:\");\n+    _static_layout->print(&st, true, nullptr, _inline_layout_info_array);\n+    st.print_cr(\"Instance size = %d bytes\", _info->_instance_size * wordSize);\n+    if (_is_inline_type) {\n+      st.print_cr(\"First field offset = %d\", _payload_offset);\n+      st.print_cr(\"Payload layout: %d\/%d\", _payload_size_in_bytes, _payload_alignment);\n+      if (has_non_atomic_flat_layout()) {\n+        st.print_cr(\"Non atomic flat layout: %d\/%d\", _non_atomic_layout_size_in_bytes, _non_atomic_layout_alignment);\n+      } else {\n+        st.print_cr(\"Non atomic flat layout: -\/-\");\n+      }\n+      if (has_atomic_layout()) {\n+        st.print_cr(\"Atomic flat layout: %d\/%d\", _atomic_layout_size_in_bytes, _atomic_layout_size_in_bytes);\n+      } else {\n+        st.print_cr(\"Atomic flat layout: -\/-\");\n+      }\n+      if (has_nullable_atomic_layout()) {\n+        st.print_cr(\"Nullable flat layout: %d\/%d\", _nullable_layout_size_in_bytes, _nullable_layout_size_in_bytes);\n+      } else {\n+        st.print_cr(\"Nullable flat layout: -\/-\");\n+      }\n+      if (_null_marker_offset != -1) {\n+        st.print_cr(\"Null marker offset = %d\", _null_marker_offset);\n+      }\n+    }\n+    st.print_cr(\"---\");\n+    \/\/ Print output all together.\n+    tty->print_raw(st.as_string());\n@@ -693,1 +1388,5 @@\n-  compute_regular_layout();\n+  if (_is_inline_type || _is_abstract_value) {\n+    compute_inline_class_layout();\n+  } else {\n+    compute_regular_layout();\n+  }\n","filename":"src\/hotspot\/share\/classfile\/fieldLayoutBuilder.cpp","additions":837,"deletions":138,"binary":false,"changes":975,"status":"modified"},{"patch":"@@ -55,0 +55,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -56,1 +58,1 @@\n-#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.inline.hpp\"\n@@ -1070,1 +1072,9 @@\n-    if (k->is_typeArray_klass()) {\n+    if (k->is_flatArray_klass()) {\n+      Klass* element_klass = (Klass*) FlatArrayKlass::cast(k)->element_klass();\n+      assert(element_klass->is_inline_klass(), \"Must be inline type component\");\n+      if (is_scratch) {\n+        comp_mirror = Handle(THREAD, HeapShared::scratch_java_mirror(element_klass));\n+      } else {\n+        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+      }\n+    } else if (k->is_typeArray_klass()) {\n@@ -1081,0 +1091,1 @@\n+      oop comp_oop = element_klass->java_mirror();\n@@ -1084,1 +1095,1 @@\n-        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+        comp_mirror = Handle(THREAD, comp_oop);\n@@ -1140,0 +1151,1 @@\n+\n@@ -1162,3 +1174,3 @@\n-  if (k->class_loader() != nullptr &&\n-      k->class_loader() != SystemDictionary::java_platform_loader() &&\n-      k->class_loader() != SystemDictionary::java_system_loader()) {\n+  if ((k->class_loader() != nullptr &&\n+       k->class_loader() != SystemDictionary::java_platform_loader() &&\n+       k->class_loader() != SystemDictionary::java_system_loader())) {\n@@ -1382,1 +1394,3 @@\n-  if (is_instance)  st->print(\"L\");\n+  if (is_instance)  {\n+    st->print(\"L\");\n+  }\n@@ -1441,0 +1455,4 @@\n+  if (klass->is_flatArray_klass() || (klass->is_objArray_klass() && ObjArrayKlass::cast(klass)->is_null_free_array_klass())) {\n+    \/\/ TODO 8336006 Ignore flat \/ null-free arrays\n+    return;\n+  }\n@@ -2813,1 +2831,1 @@\n-      if (method->name() == vmSymbols::object_initializer_name() &&\n+      if (method->is_object_constructor() &&\n@@ -3172,2 +3190,2 @@\n-  if (m->is_object_initializer()) {\n-    flags |= java_lang_invoke_MemberName::MN_IS_CONSTRUCTOR;\n+  if (m->is_object_constructor()) {\n+    flags |= java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR;\n@@ -3564,1 +3582,1 @@\n-int java_lang_reflect_Field::_trusted_final_offset;\n+int java_lang_reflect_Field::_flags_offset;\n@@ -3574,1 +3592,1 @@\n-  macro(_trusted_final_offset,    k, vmSymbols::trusted_final_name(),    bool_signature,       false); \\\n+  macro(_flags_offset,     k, vmSymbols::flags_name(),     int_signature,    false); \\\n@@ -3639,2 +3657,2 @@\n-void java_lang_reflect_Field::set_trusted_final(oop field) {\n-  field->bool_field_put(_trusted_final_offset, true);\n+void java_lang_reflect_Field::set_flags(oop field, int value) {\n+  field->int_field_put(_flags_offset, value);\n@@ -3947,1 +3965,1 @@\n-  macro(_value_offset,      integerKlass, \"value\", int_signature, false); \\\n+  macro(_value_offset,      byteKlass, \"value\", byte_signature, false); \\\n@@ -3951,1 +3969,1 @@\n-  InstanceKlass* integerKlass = vmClasses::Integer_klass();\n+  InstanceKlass* byteKlass = vmClasses::Byte_klass();\n@@ -4472,1 +4490,0 @@\n-\n@@ -4482,1 +4499,1 @@\n-  return (flags(mname) & (MN_IS_METHOD | MN_IS_CONSTRUCTOR)) > 0;\n+  return (flags(mname) & (MN_IS_METHOD | MN_IS_OBJECT_CONSTRUCTOR)) > 0;\n@@ -5510,1 +5527,5 @@\n-  CHECK_OFFSET(\"java\/lang\/Float\",     java_lang_boxing_object, value, \"F\");\n+  if (Arguments::enable_preview() && UseNullableValueFlattening && UseFieldFlattening) {\n+    CHECK_LONG_OFFSET(\"java\/lang\/Float\",     java_lang_boxing_object, value, \"F\");\n+  } else {\n+    CHECK_OFFSET(\"java\/lang\/Float\",     java_lang_boxing_object, value, \"F\");\n+  }\n@@ -5514,1 +5535,5 @@\n-  CHECK_OFFSET(\"java\/lang\/Integer\",   java_lang_boxing_object, value, \"I\");\n+  if (Arguments::enable_preview() && UseNullableValueFlattening && UseFieldFlattening) {\n+    CHECK_LONG_OFFSET(\"java\/lang\/Integer\",   java_lang_boxing_object, value, \"I\");\n+  } else {\n+    CHECK_OFFSET(\"java\/lang\/Integer\",   java_lang_boxing_object, value, \"I\");\n+  }\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":45,"deletions":20,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -64,0 +64,2 @@\n+\/\/ For INLINE_FIELD, set when loading inline type fields for\n+\/\/ class circularity checking.\n","filename":"src\/hotspot\/share\/classfile\/placeholders.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -83,1 +83,1 @@\n-    if (m->name() == vmSymbols::object_initializer_name() &&\n+    if (m->is_object_constructor() &&\n","filename":"src\/hotspot\/share\/classfile\/stackMapFrame.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -191,1 +191,2 @@\n-    return VerificationType::reference_type(_cp->klass_name_at(class_index));\n+    Symbol* klass_name = _cp->klass_name_at(class_index);\n+    return VerificationType::reference_type(klass_name);\n","filename":"src\/hotspot\/share\/classfile\/stackMapTable.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -57,0 +57,1 @@\n+#include \"oops\/fieldStreams.inline.hpp\"\n@@ -68,0 +69,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -76,0 +78,1 @@\n+#include \"runtime\/os.hpp\"\n@@ -351,1 +354,1 @@\n-      \/\/ Ignore wrapping L and ;.\n+      \/\/ Ignore wrapping L and ; (and Q and ; for value types).\n@@ -453,0 +456,1 @@\n+    \/\/ Otherwise, a LinkageError will be thrown later.\n@@ -928,2 +932,1 @@\n-  assert(!ModuleEntryTable::javabase_moduleEntry()->is_patched(),\n-         \"Cannot use sharing if java.base is patched\");\n+  assert(!CDSConfig::module_patching_disables_cds(), \"Cannot use CDS\");\n@@ -1005,1 +1008,1 @@\n-        assert(!mod_entry->is_patched(), \"cannot load archived classes for patched module\");\n+        assert(!CDSConfig::module_patching_disables_cds(), \"Cannot use CDS\");\n@@ -1137,0 +1140,32 @@\n+  if (ik->has_inline_type_fields()) {\n+    for (AllFieldStream fs(ik); !fs.done(); fs.next()) {\n+      if (fs.access_flags().is_static()) continue;\n+      Symbol* sig = fs.signature();\n+      if (fs.is_null_free_inline_type()) {\n+        \/\/ Pre-load inline class\n+        TempNewSymbol name = Signature::strip_envelope(sig);\n+        Klass* real_k = SystemDictionary::resolve_with_circularity_detection_or_fail(ik->name(), name,\n+          class_loader, false, CHECK_NULL);\n+        Klass* k = ik->get_inline_type_field_klass_or_null(fs.index());\n+        if (real_k != k) {\n+          \/\/ oops, the app has substituted a different version of k!\n+          return nullptr;\n+        }\n+      } else if (Signature::has_envelope(sig)) {\n+        TempNewSymbol name = Signature::strip_envelope(sig);\n+        if (name != ik->name() && ik->is_class_in_loadable_descriptors_attribute(name)) {\n+          Klass* real_k = SystemDictionary::resolve_with_circularity_detection_or_fail(ik->name(), name,\n+            class_loader, false, THREAD);\n+          if (HAS_PENDING_EXCEPTION) {\n+            CLEAR_PENDING_EXCEPTION;\n+          }\n+          Klass* k = ik->get_inline_type_field_klass_or_null(fs.index());\n+          if (real_k != k) {\n+            \/\/ oops, the app has substituted a different version of k!\n+            return nullptr;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1172,0 +1207,1 @@\n+\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":40,"deletions":4,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -65,0 +65,7 @@\n+  \/\/ Need to do this check when called from CDS.\n+  \/\/ if (this_class->access_flags().is_primitive_class()) {\n+  \/\/   Klass* from_class = SystemDictionary::resolve_or_fail(\n+  \/\/     from_name, Handle(THREAD, klass->class_loader()),\n+  \/\/     Handle(THREAD, klass->protection_domain()), true, CHECK_false);\n+  \/\/   return from_class == this_class;\n+  \/\/ }\n@@ -69,1 +76,1 @@\n-    \/\/ to interfaces java.lang.Cloneable and java.io.Serializable.\n+    \/\/ to interfaces java.lang.Cloneable and java.io.Serializable\n@@ -124,0 +131,1 @@\n+\n","filename":"src\/hotspot\/share\/classfile\/verificationType.cpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -67,0 +67,1 @@\n+#define INLINE_TYPE_MAJOR_VERSION                       56\n@@ -496,0 +497,7 @@\n+    case WRONG_INLINE_TYPE:\n+      ss->print(\"Type \");\n+      _type.details(ss);\n+      ss->print(\" and type \");\n+      _expected.details(ss);\n+      ss->print(\" must be identical inline types.\");\n+      break;\n@@ -616,0 +624,5 @@\n+static bool supports_strict_fields(InstanceKlass* klass) {\n+  int ver = klass->major_version();\n+  return ver > Verifier::VALUE_TYPES_MAJOR_VERSION ||\n+         (ver == Verifier::VALUE_TYPES_MAJOR_VERSION && klass->minor_version() == Verifier::JAVA_PREVIEW_MINOR_VERSION);\n+}\n@@ -1683,1 +1696,1 @@\n-          if (_method->name() == vmSymbols::object_initializer_name() &&\n+          if (_method->is_object_constructor() &&\n@@ -1706,4 +1719,0 @@\n-          verify_invoke_instructions(\n-            &bcs, code_length, &current_frame, (bci >= ex_min && bci < ex_max),\n-            &this_uninit, return_type, cp, &stackmap_table, CHECK_VERIFY(this));\n-          no_control_flow = false; break;\n@@ -1714,1 +1723,1 @@\n-            &this_uninit, return_type, cp, &stackmap_table, CHECK_VERIFY(this));\n+            &this_uninit, cp, &stackmap_table, CHECK_VERIFY(this));\n@@ -1772,2 +1781,2 @@\n-        case Bytecodes::_monitorexit :\n-          current_frame.pop_stack(\n+        case Bytecodes::_monitorexit : {\n+          VerificationType ref = current_frame.pop_stack(\n@@ -1776,0 +1785,1 @@\n+        }\n@@ -2040,0 +2050,1 @@\n+\n@@ -2152,1 +2163,1 @@\n-            | (1 << JVM_CONSTANT_String)  | (1 << JVM_CONSTANT_Class)\n+            | (1 << JVM_CONSTANT_String) | (1 << JVM_CONSTANT_Class)\n@@ -2328,1 +2339,1 @@\n-    (!allow_arrays || !ref_class_type.is_array())) {\n+      (!allow_arrays || !ref_class_type.is_array())) {\n@@ -2335,0 +2346,1 @@\n+\n@@ -2376,1 +2388,1 @@\n-      \/\/ The JVMS 2nd edition allows field initialization before the superclass\n+      \/\/ Field initialization is allowed before the superclass\n@@ -2379,4 +2391,14 @@\n-      if (stack_object_type == VerificationType::uninitialized_this_type() &&\n-          target_class_type.equals(current_type()) &&\n-          _klass->find_local_field(field_name, field_sig, &fd)) {\n-        stack_object_type = current_type();\n+      bool is_local_field = _klass->find_local_field(field_name, field_sig, &fd) &&\n+                            target_class_type.equals(current_type());\n+      if (stack_object_type == VerificationType::uninitialized_this_type()) {\n+        if (is_local_field) {\n+          \/\/ Set the type to the current type so the is_assignable check passes.\n+          stack_object_type = current_type();\n+        }\n+      } else if (supports_strict_fields(_klass)) {\n+        \/\/ `strict` fields are not writable, but only local fields produce verification errors\n+        if (is_local_field && fd.access_flags().is_strict()) {\n+          verify_error(ErrorContext::bad_code(bci),\n+                       \"Illegal use of putfield on a strict field\");\n+          return;\n+        }\n@@ -2787,1 +2809,1 @@\n-    bool in_try_block, bool *this_uninit, VerificationType return_type,\n+    bool in_try_block, bool *this_uninit,\n@@ -2819,1 +2841,1 @@\n-  \/\/ Get referenced class type\n+  \/\/ Get referenced class\n@@ -2885,1 +2907,2 @@\n-    \/\/ Make sure <init> can only be invoked by invokespecial\n+    \/\/ Make sure:\n+    \/\/   <init> can only be invoked by invokespecial.\n@@ -2887,1 +2910,1 @@\n-        method_name != vmSymbols::object_initializer_name()) {\n+          method_name != vmSymbols::object_initializer_name()) {\n@@ -2895,1 +2918,1 @@\n-                  current_class()->super()->name()))) {\n+                  current_class()->super()->name()))) { \/\/ super() can never be an inline_type.\n@@ -2980,3 +3003,1 @@\n-      \/\/ <init> method must have a void return type\n-      \/* Unreachable?  Class file parser verifies that methods with '<' have\n-       * void return *\/\n+      \/\/ an <init> method must have a void return type\n","filename":"src\/hotspot\/share\/classfile\/verifier.cpp","additions":44,"deletions":23,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -268,0 +268,1 @@\n+  case vmIntrinsics::_newNullRestrictedArray:\n@@ -332,0 +333,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:\n+  case vmIntrinsics::_finishPrivateBuffer:\n@@ -341,0 +344,1 @@\n+  case vmIntrinsics::_getValue:\n@@ -350,0 +354,1 @@\n+  case vmIntrinsics::_putValue:\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -170,0 +170,1 @@\n+  template(tag_loadable_descriptors,                  \"LoadableDescriptors\")                      \\\n@@ -206,0 +207,1 @@\n+  template(java_lang_IdentityException,               \"java\/lang\/IdentityException\")              \\\n@@ -253,0 +255,3 @@\n+  template(jdk_internal_vm_annotation_ImplicitlyConstructible_signature,     \"Ljdk\/internal\/vm\/annotation\/ImplicitlyConstructible;\") \\\n+  template(jdk_internal_vm_annotation_LooselyConsistentValue_signature,      \"Ljdk\/internal\/vm\/annotation\/LooselyConsistentValue;\") \\\n+  template(jdk_internal_vm_annotation_NullRestricted_signature,              \"Ljdk\/internal\/vm\/annotation\/NullRestricted;\") \\\n@@ -282,1 +287,0 @@\n-  template(trusted_final_name,                        \"trustedFinal\")                             \\\n@@ -505,0 +509,3 @@\n+  template(default_value_name,                        \".default\")                                 \\\n+  template(null_reset_value_name,                     \".null_reset\")                              \\\n+  template(empty_marker_name,                         \".empty\")                                   \\\n@@ -579,0 +586,1 @@\n+  template(class_class_signature,                     \"(Ljava\/lang\/Class;)Ljava\/lang\/Class;\")     \\\n@@ -588,0 +596,1 @@\n+  template(object_object_boolean_signature,           \"(Ljava\/lang\/Object;Ljava\/lang\/Object;)Z\")                  \\\n@@ -715,0 +724,2 @@\n+  template(primaryType_name,                           \"primaryType\")                                             \\\n+  template(secondaryType_name,                         \"secondaryType\")                                           \\\n@@ -744,0 +755,6 @@\n+  template(java_lang_runtime_ValueObjectMethods,            \"java\/lang\/runtime\/ValueObjectMethods\")               \\\n+  template(isSubstitutable_name,                            \"isSubstitutable\")                                    \\\n+  template(valueObjectHashCode_name,                        \"valueObjectHashCode\")                                \\\n+  template(jdk_internal_value_PrimitiveClass,               \"jdk\/internal\/value\/PrimitiveClass\")                  \\\n+  template(jdk_internal_value_ValueClass,                   \"jdk\/internal\/value\/ValueClass\")                      \\\n+                                                                                                                  \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":18,"deletions":1,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -249,2 +249,2 @@\n-BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size)\n-  : RuntimeBlob(name, kind, cb, size, sizeof(BufferBlob), CodeOffsets::frame_never_safe, 0, nullptr)\n+BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int header_size)\n+  : RuntimeBlob(name, kind, cb, size, header_size, CodeOffsets::frame_never_safe, 0, nullptr)\n@@ -262,1 +262,1 @@\n-    blob = new (size) BufferBlob(name, CodeBlobKind::Buffer, cb, size);\n+    blob = new (size) BufferBlob(name, CodeBlobKind::Buffer, cb, size, sizeof(BufferBlob));\n@@ -278,0 +278,4 @@\n+BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments)\n+  : RuntimeBlob(name, kind, cb, size, sizeof(BufferBlob), frame_complete, frame_size, oop_maps, caller_must_gc_arguments)\n+{}\n+\n@@ -282,2 +286,2 @@\n-AdapterBlob::AdapterBlob(int size, CodeBuffer* cb) :\n-  BufferBlob(\"I2C\/C2I adapters\", CodeBlobKind::Adapter, cb, size) {\n+AdapterBlob::AdapterBlob(int size, CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) :\n+  BufferBlob(\"I2C\/C2I adapters\", CodeBlobKind::Adapter, cb, size, frame_complete, frame_size, oop_maps, caller_must_gc_arguments) {\n@@ -287,1 +291,1 @@\n-AdapterBlob* AdapterBlob::create(CodeBuffer* cb) {\n+AdapterBlob* AdapterBlob::create(CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) {\n@@ -296,1 +300,1 @@\n-    blob = new (size) AdapterBlob(size, cb);\n+    blob = new (size) AdapterBlob(size, cb, frame_complete, frame_size, oop_maps, caller_must_gc_arguments);\n@@ -377,0 +381,25 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Implementation of BufferedInlineTypeBlob\n+BufferedInlineTypeBlob::BufferedInlineTypeBlob(int size, CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off) :\n+  BufferBlob(\"buffered inline type\", CodeBlobKind::BufferedInlineType, cb, size, sizeof(BufferedInlineTypeBlob)),\n+  _pack_fields_off(pack_fields_off),\n+  _pack_fields_jobject_off(pack_fields_jobject_off),\n+  _unpack_fields_off(unpack_fields_off) {\n+  CodeCache::commit(this);\n+}\n+\n+BufferedInlineTypeBlob* BufferedInlineTypeBlob::create(CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off) {\n+  ThreadInVMfromUnknown __tiv;  \/\/ get to VM state in case we block on CodeCache_lock\n+\n+  BufferedInlineTypeBlob* blob = nullptr;\n+  unsigned int size = CodeBlob::allocation_size(cb, sizeof(BufferedInlineTypeBlob));\n+  {\n+    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+    blob = new (size) BufferedInlineTypeBlob(size, cb, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+  }\n+  \/\/ Track memory usage statistic after releasing CodeCache_lock\n+  MemoryService::track_code_cache_memory_usage();\n+\n+  return blob;\n+}\n+\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":36,"deletions":7,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+\/\/    BufferedInlineTypeBlob   : used for pack\/unpack handlers\n@@ -84,0 +85,1 @@\n+  BufferedInlineType,\n@@ -163,0 +165,1 @@\n+  bool is_buffered_inline_type_blob() const   { return _kind == CodeBlobKind::BufferedInlineType; }\n@@ -304,0 +307,1 @@\n+  friend class BufferedInlineTypeBlob;\n@@ -310,1 +314,2 @@\n-  BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size);\n+  BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int header_size);\n+  BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments = false);\n@@ -334,1 +339,1 @@\n-  AdapterBlob(int size, CodeBuffer* cb);\n+  AdapterBlob(int size, CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments = false);\n@@ -338,1 +343,7 @@\n-  static AdapterBlob* create(CodeBuffer* cb);\n+  static AdapterBlob* create(CodeBuffer* cb,\n+                             int frame_complete,\n+                             int frame_size,\n+                             OopMapSet* oop_maps,\n+                             bool caller_must_gc_arguments = false);\n+\n+  bool caller_must_gc_arguments(JavaThread* thread) const { return true; }\n@@ -365,0 +376,19 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ BufferedInlineTypeBlob : used for pack\/unpack handlers\n+\n+class BufferedInlineTypeBlob: public BufferBlob {\n+private:\n+  const int _pack_fields_off;\n+  const int _pack_fields_jobject_off;\n+  const int _unpack_fields_off;\n+\n+  BufferedInlineTypeBlob(int size, CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off);\n+\n+public:\n+  \/\/ Creation\n+  static BufferedInlineTypeBlob* create(CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off);\n+\n+  address pack_fields() const { return code_begin() + _pack_fields_off; }\n+  address pack_fields_jobject() const { return code_begin() + _pack_fields_jobject_off; }\n+  address unpack_fields() const { return code_begin() + _unpack_fields_off; }\n+};\n","filename":"src\/hotspot\/share\/code\/codeBlob.hpp","additions":33,"deletions":3,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -204,1 +204,1 @@\n-void CompiledIC::set_to_monomorphic() {\n+void CompiledIC::set_to_monomorphic(bool caller_is_c1) {\n@@ -212,1 +212,1 @@\n-    entry = code->entry_point();\n+    entry = caller_is_c1 ? code->inline_entry_point() : code->entry_point();\n@@ -214,1 +214,1 @@\n-    entry = method->get_c2i_unverified_entry();\n+    entry = caller_is_c1 ? method->get_c2i_unverified_inline_entry() : method->get_c2i_unverified_entry();\n@@ -225,1 +225,1 @@\n-void CompiledIC::set_to_megamorphic(CallInfo* call_info) {\n+void CompiledIC::set_to_megamorphic(CallInfo* call_info, bool caller_is_c1) {\n@@ -236,1 +236,1 @@\n-    entry = VtableStubs::find_itable_stub(itable_index);\n+    entry = VtableStubs::find_itable_stub(itable_index, caller_is_c1);\n@@ -252,1 +252,1 @@\n-    entry = VtableStubs::find_vtable_stub(vtable_index);\n+    entry = VtableStubs::find_vtable_stub(vtable_index, caller_is_c1);\n@@ -266,1 +266,1 @@\n-void CompiledIC::update(CallInfo* call_info, Klass* receiver_klass) {\n+void CompiledIC::update(CallInfo* call_info, Klass* receiver_klass, bool caller_is_c1) {\n@@ -278,1 +278,1 @@\n-    set_to_monomorphic();\n+    set_to_monomorphic(caller_is_c1);\n@@ -282,1 +282,1 @@\n-    set_to_megamorphic(call_info);\n+    set_to_megamorphic(call_info, caller_is_c1);\n@@ -347,1 +347,1 @@\n-void CompiledDirectCall::set(const methodHandle& callee_method) {\n+void CompiledDirectCall::set(const methodHandle& callee_method, bool caller_is_c1) {\n@@ -358,1 +358,1 @@\n-    _call->set_destination_mt_safe(code->verified_entry_point());\n+    _call->set_destination_mt_safe(caller_is_c1 ? code->verified_inline_entry_point() : code->verified_entry_point());\n@@ -364,1 +364,1 @@\n-    set_to_interpreted(callee_method, callee_method->get_c2i_entry());\n+    set_to_interpreted(callee_method, caller_is_c1 ? callee_method->get_c2i_inline_entry() : callee_method->get_c2i_entry());\n","filename":"src\/hotspot\/share\/code\/compiledIC.cpp","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -164,0 +164,1 @@\n+  _is_init = read_from(stream);\n@@ -182,0 +183,5 @@\n+    if (_is_init == nullptr) {\n+      \/\/ MarkerValue is used for null-free objects\n+      _is_init = new MarkerValue();\n+    }\n+    _is_init->write_on(stream);\n","filename":"src\/hotspot\/share\/code\/debugInfo.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -288,0 +288,1 @@\n+                                              bool        return_scalarized,\n@@ -306,0 +307,1 @@\n+  last_pd->set_return_scalarized(return_scalarized);\n","filename":"src\/hotspot\/share\/code\/debugInfoRec.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1699,1 +1699,2 @@\n-      || m->number_of_breakpoints() > 0) {\n+      || m->number_of_breakpoints() > 0\n+      || m->mismatch()) {\n","filename":"src\/hotspot\/share\/code\/dependencies.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -727,0 +727,11 @@\n+\n+      \/\/ If inline types are passed as fields, use the extended signature\n+      \/\/ which contains the types of all (oop) fields of the inline type.\n+      if (is_compiled_by_c2() && callee->has_scalarized_args()) {\n+        const GrowableArray<SigEntry>* sig = callee->adapter()->get_sig_cc();\n+        assert(sig != nullptr, \"sig should never be null\");\n+        TempNewSymbol tmp_sig = SigEntry::create_symbol(sig);\n+        has_receiver = false; \/\/ The extended signature contains the receiver type\n+        fr.oops_compiled_arguments_do(tmp_sig, has_receiver, has_appendix, reg_map, f);\n+        return;\n+      }\n@@ -1253,0 +1264,4 @@\n+  _inline_entry_point             = entry_point();\n+  _verified_inline_entry_point    = verified_entry_point();\n+  _verified_inline_ro_entry_point = verified_entry_point();\n+\n@@ -1291,1 +1306,1 @@\n-\n+    assert(!method->has_scalarized_args(), \"scalarized native wrappers not supported yet\");\n@@ -1499,0 +1514,3 @@\n+    _inline_entry_point             = code_begin() + offsets->value(CodeOffsets::Inline_Entry);\n+    _verified_inline_entry_point    = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry);\n+    _verified_inline_ro_entry_point = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry_RO);\n@@ -3674,0 +3692,1 @@\n+  if (pos == inline_entry_point())                                      label = \"[Inline Entry Point]\";\n@@ -3675,0 +3694,2 @@\n+  if (pos == verified_inline_entry_point())                             label = \"[Verified Inline Entry Point]\";\n+  if (pos == verified_inline_ro_entry_point())                          label = \"[Verified Inline Entry Point (RO)]\";\n@@ -3684,0 +3705,10 @@\n+static int maybe_print_entry_label(outputStream* stream, address pos, address entry, const char* label) {\n+  if (pos == entry) {\n+    stream->bol();\n+    stream->print_cr(\"%s\", label);\n+    return 1;\n+  } else {\n+    return 0;\n+  }\n+}\n+\n@@ -3686,33 +3717,12 @@\n-    const char* label = nmethod_section_label(block_begin);\n-    if (label != nullptr) {\n-      stream->bol();\n-      stream->print_cr(\"%s\", label);\n-    }\n-  }\n-\n-  if (block_begin == entry_point()) {\n-    Method* m = method();\n-    if (m != nullptr) {\n-      stream->print(\"  # \");\n-      m->print_value_on(stream);\n-      stream->cr();\n-    }\n-    if (m != nullptr && !is_osr_method()) {\n-      ResourceMark rm;\n-      int sizeargs = m->size_of_parameters();\n-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n-      VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n-      {\n-        int sig_index = 0;\n-        if (!m->is_static())\n-          sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n-        for (SignatureStream ss(m->signature()); !ss.at_return_type(); ss.next()) {\n-          BasicType t = ss.type();\n-          sig_bt[sig_index++] = t;\n-          if (type2size[t] == 2) {\n-            sig_bt[sig_index++] = T_VOID;\n-          } else {\n-            assert(type2size[t] == 1, \"size is 1 or 2\");\n-          }\n-        }\n-        assert(sig_index == sizeargs, \"\");\n+    int n = 0;\n+    \/\/ Multiple entry points may be at the same position. Print them all.\n+    n += maybe_print_entry_label(stream, block_begin, entry_point(),                    \"[Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, inline_entry_point(),             \"[Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_entry_point(),           \"[Verified Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_entry_point(),    \"[Verified Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_ro_entry_point(), \"[Verified Inline Entry Point (RO)]\");\n+    if (n == 0) {\n+      const char* label = nmethod_section_label(block_begin);\n+      if (label != nullptr) {\n+        stream->bol();\n+        stream->print_cr(\"%s\", label);\n@@ -3720,54 +3730,63 @@\n-      const char* spname = \"sp\"; \/\/ make arch-specific?\n-      SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n-      int stack_slot_offset = this->frame_size() * wordSize;\n-      int tab1 = 14, tab2 = 24;\n-      int sig_index = 0;\n-      int arg_index = (m->is_static() ? 0 : -1);\n-      bool did_old_sp = false;\n-      for (SignatureStream ss(m->signature()); !ss.at_return_type(); ) {\n-        bool at_this = (arg_index == -1);\n-        bool at_old_sp = false;\n-        BasicType t = (at_this ? T_OBJECT : ss.type());\n-        assert(t == sig_bt[sig_index], \"sigs in sync\");\n-        if (at_this)\n-          stream->print(\"  # this: \");\n-        else\n-          stream->print(\"  # parm%d: \", arg_index);\n-        stream->move_to(tab1);\n-        VMReg fst = regs[sig_index].first();\n-        VMReg snd = regs[sig_index].second();\n-        if (fst->is_reg()) {\n-          stream->print(\"%s\", fst->name());\n-          if (snd->is_valid())  {\n-            stream->print(\":%s\", snd->name());\n-          }\n-        } else if (fst->is_stack()) {\n-          int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n-          if (offset == stack_slot_offset)  at_old_sp = true;\n-          stream->print(\"[%s+0x%x]\", spname, offset);\n-        } else {\n-          stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n-        }\n-        stream->print(\" \");\n-        stream->move_to(tab2);\n-        stream->print(\"= \");\n-        if (at_this) {\n-          m->method_holder()->print_value_on(stream);\n-        } else {\n-          bool did_name = false;\n-          if (!at_this && ss.is_reference()) {\n-            Symbol* name = ss.as_symbol();\n-            name->print_value_on(stream);\n-            did_name = true;\n-          }\n-          if (!did_name)\n-            stream->print(\"%s\", type2name(t));\n-        }\n-        if (at_old_sp) {\n-          stream->print(\"  (%s of caller)\", spname);\n-          did_old_sp = true;\n-        }\n-        stream->cr();\n-        sig_index += type2size[t];\n-        arg_index += 1;\n-        if (!at_this)  ss.next();\n+    }\n+  }\n+\n+  Method* m = method();\n+  if (m == nullptr || is_osr_method()) {\n+    return;\n+  }\n+\n+  \/\/ Print the name of the method (only once)\n+  address low = MIN4(entry_point(), verified_entry_point(), verified_inline_entry_point(), verified_inline_ro_entry_point());\n+  low = MIN2(low, inline_entry_point());\n+  assert(low != 0, \"sanity\");\n+  if (block_begin == low) {\n+    stream->print(\"  # \");\n+    m->print_value_on(stream);\n+    stream->cr();\n+  }\n+\n+  \/\/ Print the arguments for the 3 types of verified entry points\n+  CompiledEntrySignature ces(m);\n+  ces.compute_calling_conventions(false);\n+  const GrowableArray<SigEntry>* sig_cc;\n+  const VMRegPair* regs;\n+  if (block_begin == verified_entry_point()) {\n+    sig_cc = ces.sig_cc();\n+    regs = ces.regs_cc();\n+  } else if (block_begin == verified_inline_entry_point()) {\n+    sig_cc = ces.sig();\n+    regs = ces.regs();\n+  } else if (block_begin == verified_inline_ro_entry_point()) {\n+    sig_cc = ces.sig_cc_ro();\n+    regs = ces.regs_cc_ro();\n+  } else {\n+    return;\n+  }\n+\n+  bool has_this = !m->is_static();\n+  if (ces.has_inline_recv() && block_begin == verified_entry_point()) {\n+    \/\/ <this> argument is scalarized for verified_entry_point()\n+    has_this = false;\n+  }\n+  const char* spname = \"sp\"; \/\/ make arch-specific?\n+  int stack_slot_offset = this->frame_size() * wordSize;\n+  int tab1 = 14, tab2 = 24;\n+  int sig_index = 0;\n+  int arg_index = has_this ? -1 : 0;\n+  bool did_old_sp = false;\n+  for (ExtendedSignature sig = ExtendedSignature(sig_cc, SigEntryFilter()); !sig.at_end(); ++sig) {\n+    bool at_this = (arg_index == -1);\n+    bool at_old_sp = false;\n+    BasicType t = (*sig)._bt;\n+    if (at_this) {\n+      stream->print(\"  # this: \");\n+    } else {\n+      stream->print(\"  # parm%d: \", arg_index);\n+    }\n+    stream->move_to(tab1);\n+    VMReg fst = regs[sig_index].first();\n+    VMReg snd = regs[sig_index].second();\n+    if (fst->is_reg()) {\n+      stream->print(\"%s\", fst->name());\n+      if (snd->is_valid())  {\n+        stream->print(\":%s\", snd->name());\n@@ -3775,6 +3794,24 @@\n-      if (!did_old_sp) {\n-        stream->print(\"  # \");\n-        stream->move_to(tab1);\n-        stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n-        stream->print(\"  (%s of caller)\", spname);\n-        stream->cr();\n+    } else if (fst->is_stack()) {\n+      int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n+      if (offset == stack_slot_offset)  at_old_sp = true;\n+      stream->print(\"[%s+0x%x]\", spname, offset);\n+    } else {\n+      stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n+    }\n+    stream->print(\" \");\n+    stream->move_to(tab2);\n+    stream->print(\"= \");\n+    if (at_this) {\n+      m->method_holder()->print_value_on(stream);\n+    } else {\n+      bool did_name = false;\n+      if (is_reference_type(t)) {\n+        Symbol* name = (*sig)._symbol;\n+        name->print_value_on(stream);\n+        did_name = true;\n+      }\n+      if (!did_name)\n+        stream->print(\"%s\", type2name(t));\n+      \/\/ If the entry has a non-default sort_offset, it must be a null marker\n+      if ((*sig)._sort_offset != (*sig)._offset) {\n+        stream->print(\" (null marker)\");\n@@ -3783,0 +3820,14 @@\n+    if (at_old_sp) {\n+      stream->print(\"  (%s of caller)\", spname);\n+      did_old_sp = true;\n+    }\n+    stream->cr();\n+    sig_index += type2size[t];\n+    arg_index += 1;\n+  }\n+  if (!did_old_sp) {\n+    stream->print(\"  # \");\n+    stream->move_to(tab1);\n+    stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n+    stream->print(\"  (%s of caller)\", spname);\n+    stream->cr();\n@@ -3906,1 +3957,1 @@\n-      st->print(\" {reexecute=%d rethrow=%d return_oop=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop());\n+      st->print(\" {reexecute=%d rethrow=%d return_oop=%d return_scalarized=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop(), sd->return_scalarized());\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":146,"deletions":95,"binary":false,"changes":241,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+  _return_scalarized = pd->return_scalarized();\n@@ -55,0 +56,1 @@\n+  _return_scalarized = false;\n","filename":"src\/hotspot\/share\/code\/scopeDesc.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -207,1 +207,1 @@\n-address VtableStubs::find_stub(bool is_vtable_stub, int vtable_index) {\n+address VtableStubs::find_stub(bool is_vtable_stub, int vtable_index, bool caller_is_c1) {\n@@ -213,1 +213,1 @@\n-    s = lookup(is_vtable_stub, vtable_index);\n+    s = lookup(is_vtable_stub, vtable_index, caller_is_c1);\n@@ -216,1 +216,1 @@\n-        s = create_vtable_stub(vtable_index);\n+        s = create_vtable_stub(vtable_index, caller_is_c1);\n@@ -218,1 +218,1 @@\n-        s = create_itable_stub(vtable_index);\n+        s = create_itable_stub(vtable_index, caller_is_c1);\n@@ -226,1 +226,1 @@\n-      enter(is_vtable_stub, vtable_index, s);\n+      enter(is_vtable_stub, vtable_index, caller_is_c1, s);\n@@ -228,1 +228,2 @@\n-        tty->print_cr(\"Decoding VtableStub %s[%d]@\" PTR_FORMAT \" [\" PTR_FORMAT \", \" PTR_FORMAT \"] (%zu bytes)\",\n+        tty->print_cr(\"Decoding VtableStub (%s) %s[%d]@\" PTR_FORMAT \" [\" PTR_FORMAT \", \" PTR_FORMAT \"] (%zu bytes)\",\n+                      caller_is_c1 ? \"c1\" : \"full opt\",\n@@ -238,1 +239,1 @@\n-        JvmtiExport::post_dynamic_code_generated_while_holding_locks(is_vtable_stub? \"vtable stub\": \"itable stub\",\n+        JvmtiExport::post_dynamic_code_generated_while_holding_locks(is_vtable_stub? \"vtable stub\": \"itable stub\",  \/\/ FIXME: need to pass caller_is_c1??\n@@ -247,1 +248,1 @@\n-inline uint VtableStubs::hash(bool is_vtable_stub, int vtable_index){\n+inline uint VtableStubs::hash(bool is_vtable_stub, int vtable_index, bool caller_is_c1) {\n@@ -250,0 +251,3 @@\n+  if (caller_is_c1) {\n+    hash = 7 - hash;\n+  }\n@@ -254,1 +258,1 @@\n-inline uint VtableStubs::unsafe_hash(address entry_point) {\n+inline uint VtableStubs::unsafe_hash(address entry_point, bool caller_is_c1) {\n@@ -264,1 +268,1 @@\n-  return hash(is_vtable_stub, vtable_index);\n+  return hash(is_vtable_stub, vtable_index, caller_is_c1);\n@@ -267,2 +271,1 @@\n-\n-VtableStub* VtableStubs::lookup(bool is_vtable_stub, int vtable_index) {\n+VtableStub* VtableStubs::lookup(bool is_vtable_stub, int vtable_index, bool caller_is_c1) {\n@@ -270,1 +273,1 @@\n-  unsigned hash = VtableStubs::hash(is_vtable_stub, vtable_index);\n+  unsigned hash = VtableStubs::hash(is_vtable_stub, vtable_index, caller_is_c1);\n@@ -272,1 +275,1 @@\n-  while( s && !s->matches(is_vtable_stub, vtable_index)) s = s->next();\n+  while( s && !s->matches(is_vtable_stub, vtable_index, caller_is_c1)) s = s->next();\n@@ -277,1 +280,1 @@\n-void VtableStubs::enter(bool is_vtable_stub, int vtable_index, VtableStub* s) {\n+void VtableStubs::enter(bool is_vtable_stub, int vtable_index, bool caller_is_c1, VtableStub* s) {\n@@ -279,2 +282,2 @@\n-  assert(s->matches(is_vtable_stub, vtable_index), \"bad vtable stub\");\n-  unsigned int h = VtableStubs::hash(is_vtable_stub, vtable_index);\n+  assert(s->matches(is_vtable_stub, vtable_index, caller_is_c1), \"bad vtable stub\");\n+  unsigned int h = VtableStubs::hash(is_vtable_stub, vtable_index, caller_is_c1);\n@@ -293,1 +296,2 @@\n-  uint hash = VtableStubs::unsafe_hash(pc);\n+  VtableStub* stub = (VtableStub*)(pc - VtableStub::entry_offset());\n+  uint hash = VtableStubs::unsafe_hash(pc, stub->caller_is_c1());\n","filename":"src\/hotspot\/share\/code\/vtableStubs.cpp","additions":22,"deletions":18,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -1267,1 +1267,1 @@\n-          if (vfst.method()->is_static_initializer() ||\n+        if (vfst.method()->is_class_initializer() ||\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -59,1 +59,1 @@\n-static GrowableArray<const char*>* phase_names = nullptr;\n+static GrowableArray<const char*>* _phase_names = nullptr;\n@@ -65,3 +65,3 @@\n-    assert(phase_names != nullptr, \"invariant\");\n-    assert(phase_names->is_nonempty(), \"invariant\");\n-    const u4 nof_entries = phase_names->length();\n+    assert(_phase_names != nullptr, \"invariant\");\n+    assert(_phase_names->is_nonempty(), \"invariant\");\n+    const u4 nof_entries = _phase_names->length();\n@@ -71,1 +71,1 @@\n-      writer.write(phase_names->at(i));\n+      writer.write(_phase_names->at(i));\n@@ -77,2 +77,2 @@\n-  for (int i = 0; i < phase_names->length(); i++) {\n-    const char* name = phase_names->at(i);\n+  for (int i = 0; i < _phase_names->length(); i++) {\n+    const char* name = _phase_names->at(i);\n@@ -91,2 +91,2 @@\n-    if (phase_names == nullptr) {\n-      phase_names = new (mtInternal) GrowableArray<const char*>(100, mtCompiler);\n+    if (_phase_names == nullptr) {\n+      _phase_names = new (mtInternal) GrowableArray<const char*>(100, mtCompiler);\n@@ -103,2 +103,2 @@\n-    index = phase_names->length();\n-    phase_names->append(use_strdup ? os::strdup(phase_name) : phase_name);\n+    index = _phase_names->length();\n+    _phase_names->append(use_strdup ? os::strdup(phase_name) : phase_name);\n","filename":"src\/hotspot\/share\/compiler\/compilerEvent.cpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -626,4 +626,12 @@\n-    case Bytecodes::_return:\n-      if (instruction->method()->intrinsic_id() == vmIntrinsics::_Object_init) {\n-        \/\/ return from Object.init implicitly registers a finalizer\n-        \/\/ for the receiver if needed, so keep it alive.\n+    case Bytecodes::_return: {\n+      ciMethod* method = instruction->method();\n+      ciInstanceKlass* holder = method->holder();\n+      const bool abstract_klass = holder->is_abstract();\n+      const bool concrete_value_klass = !abstract_klass && holder->is_inlinetype();\n+      if (method->intrinsic_id() == vmIntrinsics::_Object_init ||\n+          (method->is_object_constructor() && (concrete_value_klass || abstract_klass))) {\n+        \/\/ Returning from Object.<init> implicitly registers a finalizer for the receiver if needed, to keep it alive.\n+        \/\/ Value class constructors update the scalarized receiver. We need to keep it live so that we can find it after\n+        \/\/ (chained) constructor calls and propagate updates to the caller. If the holder of the constructor is abstract,\n+        \/\/ we do not know if the constructor was called on a value class or not. We therefore keep the receiver of all\n+        \/\/ abstract constructors live.\n@@ -633,2 +641,1 @@\n-\n-\n+    }\n","filename":"src\/hotspot\/share\/compiler\/methodLiveness.cpp","additions":13,"deletions":6,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -307,1 +307,1 @@\n-        error_msg = \"Chars '<' and '>' only allowed in <init> and <clinit>\";\n+        error_msg = \"Chars '<' and '>' only allowed in <init>, <clinit>\";\n","filename":"src\/hotspot\/share\/compiler\/methodMatcher.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/compiler\/oopMap.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -215,1 +215,1 @@\n-void G1BarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+void G1BarrierSetC2::eliminate_gc_barrier(PhaseIterGVN* igvn, Node* node) const {\n@@ -218,1 +218,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -512,1 +512,2 @@\n-    if (klass->is_array_klass()) {\n+    \/\/ CMH: Valhalla flat arrays can split this work up, but for now, doesn't\n+    if (klass->is_array_klass() && !klass->is_flatArray_klass()) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"oops\/flatArrayKlass.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCompactionManager.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -78,0 +78,1 @@\n+#include \"oops\/flatArrayKlass.inline.hpp\"\n@@ -2456,0 +2457,3 @@\n+    \/\/ Read the klass before the copying, since it might destroy the klass (i.e. overlapping copy)\n+    \/\/ and if partial copy, the destination klass may not be copied yet\n+    Klass* klass = cast_to_oop(source())->klass();\n@@ -2457,1 +2461,1 @@\n-    cast_to_oop(copy_destination())->init_mark();\n+    cast_to_oop(copy_destination())->set_mark(Klass::default_prototype_header(klass));\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+#include \"oops\/flatArrayKlass.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"classfile\/vmSymbols.hpp\"\n@@ -29,0 +30,2 @@\n+#include \"memory\/resourceArea.hpp\"\n+#include \"oops\/objArrayKlass.inline.hpp\"\n@@ -54,0 +57,27 @@\n+void BarrierSet::throw_array_null_pointer_store_exception(arrayOop src, arrayOop dst, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  Klass* bound = ObjArrayKlass::cast(dst->klass())->element_klass();\n+  stringStream ss;\n+  ss.print(\"arraycopy: can not copy null values into %s[]\",\n+           bound->external_name());\n+  THROW_MSG(vmSymbols::java_lang_NullPointerException(), ss.as_string());\n+}\n+\n+void BarrierSet::throw_array_store_exception(arrayOop src, arrayOop dst, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  Klass* bound = ObjArrayKlass::cast(dst->klass())->element_klass();\n+  Klass* stype = ObjArrayKlass::cast(src->klass())->element_klass();\n+  stringStream ss;\n+  if (!bound->is_subtype_of(stype)) {\n+    ss.print(\"arraycopy: type mismatch: can not copy %s[] into %s[]\",\n+             stype->external_name(), bound->external_name());\n+  } else {\n+    \/\/ oop_arraycopy should return the index in the source array that\n+    \/\/ contains the problematic oop.\n+    ss.print(\"arraycopy: element type mismatch: can not cast one of the elements\"\n+             \" of %s[] to the type of the destination array, %s\",\n+             stype->external_name(), bound->external_name());\n+  }\n+  THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSet.cpp","additions":30,"deletions":0,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -0,0 +1,36 @@\n+\/*\n+ * Copyright (c) 2019, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"gc\/shared\/barrierSetRuntime.hpp\"\n+#include \"oops\/access.inline.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+JRT_LEAF(void, BarrierSetRuntime::value_copy(void* src, void* dst, InlineLayoutInfo* li))\n+  HeapAccess<>::value_copy(src, dst, li->klass(), li->kind());\n+JRT_END\n+\n+JRT_LEAF(void, BarrierSetRuntime::value_copy_is_dest_uninitialized(void* src, void* dst, InlineLayoutInfo* li))\n+  HeapAccess<IS_DEST_UNINITIALIZED>::value_copy(src, dst, li->klass(), li->kind());\n+JRT_END\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSetRuntime.cpp","additions":36,"deletions":0,"binary":false,"changes":36,"status":"added"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -39,0 +40,15 @@\n+  \/\/ Is this a flat, atomic access that might require gc barriers on oop fields?\n+  ciInlineKlass* vk = access.vk();\n+  if (vk != nullptr && vk->has_object_fields()) {\n+    \/\/ Add pre-barriers for oop fields\n+    for (int i = 0; i < vk->nof_nonstatic_fields(); i++) {\n+      ciField* field = vk->nonstatic_field_at(i);\n+      if (!field->type()->is_primitive_type()) {\n+        int off = access.offset().opr().as_jint() + field->offset_in_bytes() - vk->payload_offset();\n+        LIRAccess inner_access(access.gen(), decorators, access.base(), LIR_OprFact::intConst(off), field->type()->basic_type(), access.patch_emit_info(), access.access_emit_info());\n+        pre_barrier(inner_access, resolve_address(inner_access, false),\n+                    LIR_OprFact::illegalOpr \/* pre_val *\/, inner_access.patch_emit_info());\n+      }\n+    }\n+  }\n+\n@@ -51,0 +67,25 @@\n+\n+  if (vk != nullptr && vk->has_object_fields()) {\n+    \/\/ Add post-barriers for oop fields\n+    for (int i = 0; i < vk->nof_nonstatic_fields(); i++) {\n+      ciField* field = vk->nonstatic_field_at(i);\n+      if (!field->type()->is_primitive_type()) {\n+        int inner_off = field->offset_in_bytes() - vk->payload_offset();\n+        int off = access.offset().opr().as_jint() + inner_off;\n+        LIRAccess inner_access(access.gen(), decorators, access.base(), LIR_OprFact::intConst(off), field->type()->basic_type(), access.patch_emit_info(), access.access_emit_info());\n+\n+        \/\/ Shift long value to extract the narrow oop field value and zero-extend\n+        LIR_Opr field_val = access.gen()->new_register(T_LONG);\n+        access.gen()->lir()->unsigned_shift_right(value,\n+                                                  LIR_OprFact::intConst(inner_off << LogBitsPerByte),\n+                                                  field_val, LIR_Opr::illegalOpr());\n+        LIR_Opr mask = access.gen()->load_immediate((julong) max_juint, T_LONG);\n+        access.gen()->lir()->logical_and(field_val, mask, field_val);\n+        LIR_Opr oop_val = access.gen()->new_register(T_OBJECT);\n+        access.gen()->lir()->move(field_val, oop_val);\n+\n+        assert(!is_array && !on_anonymous, \"not suppported\");\n+        post_barrier(inner_access, access.base().opr(), oop_val);\n+      }\n+    }\n+  }\n","filename":"src\/hotspot\/share\/gc\/shared\/c1\/modRefBarrierSetC1.cpp","additions":41,"deletions":0,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -51,0 +51,4 @@\n+Node* C2ParseAccess::control() const {\n+  return _ctl == nullptr ? _kit->control() : _ctl;\n+}\n+\n@@ -209,1 +213,1 @@\n-    Node* control = control_dependent ? kit->control() : nullptr;\n+    Node* control = control_dependent ? parse_access.control() : nullptr;\n@@ -870,1 +874,1 @@\n-  phase->igvn().replace_node(ac, call);\n+  phase->replace_node(ac, call);\n@@ -894,1 +898,1 @@\n-  phase->igvn().replace_node(ac, call);\n+  phase->replace_node(ac, call);\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -127,1 +127,1 @@\n-void CardTableBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+void CardTableBarrierSetC2::eliminate_gc_barrier(PhaseIterGVN* igvn, Node* node) const {\n@@ -129,10 +129,16 @@\n-  Node *shift = node->unique_out();\n-  Node *addp = shift->unique_out();\n-  for (DUIterator_Last jmin, j = addp->last_outs(jmin); j >= jmin; --j) {\n-    Node *mem = addp->last_out(j);\n-    if (UseCondCardMark && mem->is_Load()) {\n-      assert(mem->Opcode() == Op_LoadB, \"unexpected code shape\");\n-      \/\/ The load is checking if the card has been written so\n-      \/\/ replace it with zero to fold the test.\n-      macro->replace_node(mem, macro->intcon(0));\n-      continue;\n+  for (DUIterator_Last imin, i = node->last_outs(imin); i >= imin; --i) {\n+    Node* shift = node->last_out(i);\n+    for (DUIterator_Last jmin, j = shift->last_outs(jmin); j >= jmin; --j) {\n+      Node* addp = shift->last_out(j);\n+      for (DUIterator_Last kmin, k = addp->last_outs(kmin); k >= kmin; --k) {\n+        Node* mem = addp->last_out(k);\n+        if (UseCondCardMark && mem->is_Load()) {\n+          assert(mem->Opcode() == Op_LoadB, \"unexpected code shape\");\n+          \/\/ The load is checking if the card has been written so\n+          \/\/ replace it with zero to fold the test.\n+          igvn->replace_node(mem, igvn->intcon(0));\n+          continue;\n+        }\n+        assert(mem->is_Store(), \"store required\");\n+        igvn->replace_node(mem, mem->in(MemNode::Memory));\n+      }\n@@ -140,2 +146,0 @@\n-    assert(mem->is_Store(), \"store required\");\n-    macro->replace_node(mem, mem->in(MemNode::Memory));\n@@ -146,1 +150,1 @@\n-  bool is_oop = is_reference_type(type);\n+  bool is_oop = type == T_OBJECT || type == T_ARRAY;\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/cardTableBarrierSetC2.cpp","additions":18,"deletions":14,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n-  virtual void eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const;\n+  virtual void eliminate_gc_barrier(PhaseIterGVN* igvn, Node* node) const;\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/cardTableBarrierSetC2.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -41,1 +41,6 @@\n-  if (!access.is_oop() || tightly_coupled_alloc || (!in_heap && !anonymous)) {\n+  const InlineTypeNode* vt = nullptr;\n+  if (access.is_parse_access() && static_cast<C2ParseAccess&>(access).vt() != nullptr) {\n+    vt = static_cast<C2ParseAccess&>(access).vt();\n+  }\n+\n+  if (vt == nullptr && (!access.is_oop() || tightly_coupled_alloc || (!in_heap && !anonymous))) {\n@@ -47,1 +52,20 @@\n-  post_barrier(parse_access.kit(), access.base(), adr, val.node(), use_precise);\n+\n+  \/\/ TODO 8341767\n+  \/\/ - We actually only need the post barrier once for non-arrays (same for C1, right)?\n+  \/\/ - Value is only needed to determine if we are storing null. Maybe we can go with a simple boolean?\n+  GraphKit* kit = parse_access.kit();\n+  if (vt != nullptr) {\n+    for (uint i = 0; i < vt->field_count(); ++i) {\n+      ciType* type = vt->field_type(i);\n+      if (!type->is_primitive_type()) {\n+        assert(!is_array, \"array access not supported\");\n+        ciInlineKlass* vk = vt->bottom_type()->inline_klass();\n+        int field_offset = vt->field_offset(i) - vk->payload_offset();\n+        Node* value = vt->field_value(i);\n+        Node* field_adr = kit->basic_plus_adr(access.base(), adr, field_offset);\n+        post_barrier(kit, access.base(), field_adr, value, use_precise);\n+      }\n+    }\n+  } else {\n+    post_barrier(kit, access.base(), adr, val.node(), use_precise);\n+  }\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/modRefBarrierSetC2.cpp","additions":26,"deletions":2,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -375,1 +375,1 @@\n-    oopDesc::release_set_mark(mem, _klass->prototype_header());\n+    oopDesc::release_set_mark(mem, Klass::default_prototype_header(_klass));\n@@ -377,1 +377,5 @@\n-    oopDesc::set_mark(mem, markWord::prototype());\n+    if (EnableValhalla) {\n+      oopDesc::set_mark(mem, Klass::default_prototype_header(_klass));\n+    } else {\n+      oopDesc::set_mark(mem, markWord::prototype());\n+    }\n@@ -388,0 +392,6 @@\n+oop ObjBufferAllocator::initialize(HeapWord* mem) const {\n+  oopDesc::set_klass_gap(mem, 0);\n+  return finish(mem);\n+}\n+\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":12,"deletions":2,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -904,1 +904,1 @@\n-    Node* offset = phase->igvn().MakeConX(in_bytes(ShenandoahThreadLocalData::gc_state_offset()));\n+    Node* offset = phase->MakeConX(in_bytes(ShenandoahThreadLocalData::gc_state_offset()));\n@@ -952,1 +952,1 @@\n-    phase->igvn().replace_node(ac, call);\n+    phase->replace_node(ac, call);\n@@ -972,1 +972,1 @@\n-void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseIterGVN* igvn, Node* node) const {\n@@ -974,1 +974,1 @@\n-    shenandoah_eliminate_wb_pre(node, &macro->igvn());\n+    shenandoah_eliminate_wb_pre(node, igvn);\n@@ -985,1 +985,1 @@\n-        macro->replace_node(mem, macro->intcon(0));\n+        igvn->replace_node(mem, igvn->intcon(0));\n@@ -989,1 +989,1 @@\n-      macro->replace_node(mem, mem->in(MemNode::Memory));\n+      igvn->replace_node(mem, mem->in(MemNode::Memory));\n@@ -1121,1 +1121,1 @@\n-    uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_Type()->domain()->cnt();\n+    uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_Type()->domain_sig()->cnt();\n@@ -1207,1 +1207,1 @@\n-        uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_Type()->domain()->cnt();\n+        uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_Type()->domain_sig()->cnt();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -456,1 +456,1 @@\n-        const TypeTuple* args = n->as_Call()->_tf->domain();\n+        const TypeTuple* args = n->as_Call()->_tf->domain_sig();\n@@ -577,1 +577,1 @@\n-      uint stop = n->is_Call() ? n->as_Call()->tf()->domain()->cnt() : n->req();\n+      uint stop = n->is_Call() ? n->as_Call()->tf()->domain_sig()->cnt() : n->req();\n@@ -797,6 +797,5 @@\n-        CallProjections projs;\n-        c->as_Call()->extract_projections(&projs, true, false);\n-        if (projs.fallthrough_memproj != nullptr) {\n-          if (projs.fallthrough_memproj->adr_type() == TypePtr::BOTTOM) {\n-            if (projs.catchall_memproj == nullptr) {\n-              mem = projs.fallthrough_memproj;\n+        CallProjections* projs = c->as_Call()->extract_projections(true, false);\n+        if (projs->fallthrough_memproj != nullptr) {\n+          if (projs->fallthrough_memproj->adr_type() == TypePtr::BOTTOM) {\n+            if (projs->catchall_memproj == nullptr) {\n+              mem = projs->fallthrough_memproj;\n@@ -804,2 +803,2 @@\n-              if (phase->is_dominator(projs.fallthrough_catchproj, ctrl)) {\n-                mem = projs.fallthrough_memproj;\n+              if (phase->is_dominator(projs->fallthrough_catchproj, ctrl)) {\n+                mem = projs->fallthrough_memproj;\n@@ -807,2 +806,2 @@\n-                assert(phase->is_dominator(projs.catchall_catchproj, ctrl), \"one proj must dominate barrier\");\n-                mem = projs.catchall_memproj;\n+                assert(phase->is_dominator(projs->catchall_catchproj, ctrl), \"one proj must dominate barrier\");\n+                mem = projs->catchall_memproj;\n@@ -1069,1 +1068,1 @@\n-static Node* create_phis_on_call_return(Node* ctrl, Node* c, Node* n, Node* n_clone, const CallProjections& projs, PhaseIdealLoop* phase) {\n+static Node* create_phis_on_call_return(Node* ctrl, Node* c, Node* n, Node* n_clone, const CallProjections* projs, PhaseIdealLoop* phase) {\n@@ -1081,1 +1080,1 @@\n-    if (phase->is_dominator(projs.fallthrough_catchproj, in)) {\n+    if (phase->is_dominator(projs->fallthrough_catchproj, in)) {\n@@ -1083,1 +1082,1 @@\n-    } else if (phase->is_dominator(projs.catchall_catchproj, in)) {\n+    } else if (phase->is_dominator(projs->catchall_catchproj, in)) {\n@@ -1186,2 +1185,1 @@\n-      CallProjections projs;\n-      call->extract_projections(&projs, false, false);\n+      CallProjections* projs = call->extract_projections(false, false);\n@@ -1190,1 +1188,1 @@\n-      if (projs.fallthrough_catchproj == nullptr) {\n+      if (projs->fallthrough_catchproj == nullptr) {\n@@ -1192,1 +1190,1 @@\n-        assert(projs.catchall_catchproj == nullptr, \"runtime call should not have catch all projection\");\n+        assert(projs->catchall_catchproj == nullptr, \"runtime call should not have catch all projection\");\n@@ -1201,2 +1199,2 @@\n-      phase->register_new_node(lrb_clone, projs.catchall_catchproj);\n-      phase->set_ctrl(lrb, projs.fallthrough_catchproj);\n+      phase->register_new_node(lrb_clone, projs->catchall_catchproj);\n+      phase->set_ctrl(lrb, projs->fallthrough_catchproj);\n@@ -1224,1 +1222,1 @@\n-          if (phase->is_dominator(call, c) && phase->is_dominator(c, projs.fallthrough_proj)) {\n+          if (phase->is_dominator(call, c) && phase->is_dominator(c, projs->fallthrough_proj)) {\n@@ -1232,1 +1230,1 @@\n-            phase->register_new_node(u_clone, projs.catchall_catchproj);\n+            phase->register_new_node(u_clone, projs->catchall_catchproj);\n@@ -1234,1 +1232,1 @@\n-            phase->set_ctrl(u, projs.fallthrough_catchproj);\n+            phase->set_ctrl(u, projs->fallthrough_catchproj);\n@@ -1240,1 +1238,1 @@\n-                  if (phase->is_dominator(projs.catchall_catchproj, u->in(0)->in(k))) {\n+                  if (phase->is_dominator(projs->catchall_catchproj, u->in(0)->in(k))) {\n@@ -1243,1 +1241,1 @@\n-                  } else if (!phase->is_dominator(projs.fallthrough_catchproj, u->in(0)->in(k))) {\n+                  } else if (!phase->is_dominator(projs->fallthrough_catchproj, u->in(0)->in(k))) {\n@@ -1250,1 +1248,1 @@\n-              if (phase->is_dominator(projs.catchall_catchproj, c)) {\n+              if (phase->is_dominator(projs->catchall_catchproj, c)) {\n@@ -1255,1 +1253,1 @@\n-              } else if (!phase->is_dominator(projs.fallthrough_catchproj, c)) {\n+              } else if (!phase->is_dominator(projs->fallthrough_catchproj, c)) {\n@@ -2076,5 +2074,4 @@\n-    CallProjections projs;\n-    call->extract_projections(&projs, true, false);\n-    if (projs.catchall_memproj != nullptr) {\n-      if (projs.fallthrough_memproj == n) {\n-        c = projs.fallthrough_catchproj;\n+    CallProjections* projs = call->extract_projections(true, false);\n+    if (projs->catchall_memproj != nullptr) {\n+      if (projs->fallthrough_memproj == n) {\n+        c = projs->fallthrough_catchproj;\n@@ -2082,2 +2079,2 @@\n-        assert(projs.catchall_memproj == n, \"\");\n-        c = projs.catchall_catchproj;\n+        assert(projs->catchall_memproj == n, \"\");\n+        c = projs->catchall_catchproj;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.cpp","additions":32,"deletions":35,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -362,1 +362,1 @@\n-bool ShenandoahBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+void ShenandoahBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n@@ -369,2 +369,4 @@\n-  bs->arraycopy_barrier(src, dst, length);\n-  bool result = Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);\n+  bs->arraycopy_barrier(arrayOopDesc::obj_offset_to_raw(src_obj, src_offset_in_bytes, src_raw),\n+                        arrayOopDesc::obj_offset_to_raw(dst_obj, dst_offset_in_bytes, dst_raw),\n+                        length);\n+  Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);\n@@ -374,1 +376,0 @@\n-  return result;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -64,1 +64,1 @@\n-    \/\/ will re-resovle the call and update the compiled IC.\n+    \/\/ will re-resolve the call and update the compiled IC.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSetNMethod.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -423,1 +423,1 @@\n-    if (is_reference_type(bt)) {\n+    if (is_reference_type(bt) && !ary_ptr->is_flat()) {\n@@ -449,1 +449,1 @@\n-        length = phase->transform_later(new SubLNode(length, phase->longcon(1))); \/\/ Size is in longs\n+        length = phase->transform_later(new SubXNode(length, phase->longcon(1))); \/\/ Size is in longs\n@@ -813,1 +813,2 @@\n-void ZBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+\n+void ZBarrierSetC2::eliminate_gc_barrier(PhaseIterGVN* igvn, Node* node) const {\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -52,1 +53,1 @@\n-  if (_word_size <= segment_max) {\n+  if (_word_size <= segment_max || ArrayKlass::cast(_klass)->is_flatArray_klass()) {\n@@ -68,1 +69,5 @@\n-    arrayOopDesc::set_mark(mem, markWord::prototype().set_marked());\n+    if (EnableValhalla) {\n+      arrayOopDesc::set_mark(mem, Klass::default_prototype_header(_klass).set_marked());\n+    } else {\n+      arrayOopDesc::set_mark(mem, markWord::prototype().set_marked());\n+    }\n@@ -158,1 +163,1 @@\n-  if (UseCompactObjectHeaders) {\n+  if (UseCompactObjectHeaders || EnableValhalla) {\n","filename":"src\/hotspot\/share\/gc\/z\/zObjArrayAllocator.cpp","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1113,0 +1113,1 @@\n+    case Bytecodes::_checkcast:\n@@ -1173,1 +1174,7 @@\n-    os->print(\"\\\" is null\");\n+    address code_base = _method->constMethod()->code_base();\n+    Bytecodes::Code code = Bytecodes::java_code_at(_method, code_base + bci);\n+    if (code == Bytecodes::_aastore) {\n+      os->print(\"\\\" is null or is a null-free array and there's an attempt to store null in it\");\n+    } else {\n+      os->print(\"\\\" is null\");\n+    }\n@@ -1435,0 +1442,5 @@\n+    case Bytecodes::_checkcast: {\n+        int cp_index = Bytes::get_Java_u2(code_base + pos);\n+        ConstantPool* cp = _method->constants();\n+        os->print(\"Cannot cast to null-free type \\\"%s\\\"\", cp->klass_at_noresolve(cp_index)->as_C_string());\n+      } break;\n","filename":"src\/hotspot\/share\/interpreter\/bytecodeUtils.cpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+  def(_fast_vgetfield            , \"fast_vgetfield\"            , \"bJJ\"  , nullptr    , T_OBJECT ,  0, true , _getfield          ) \\\n@@ -42,0 +43,1 @@\n+  def(_fast_vputfield            , \"fast_vputfield\"            , \"bJJ\"  , nullptr    , T_OBJECT ,  0, true , _putfield          ) \\\n","filename":"src\/hotspot\/share\/interpreter\/bytecodes.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -47,0 +48,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -79,0 +83,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -226,0 +231,113 @@\n+JRT_ENTRY(void, InterpreterRuntime::uninitialized_static_inline_type_field(JavaThread* current, oopDesc* mirror, ResolvedFieldEntry* entry))\n+  \/\/ The interpreter tries to access an inline static field that has not been initialized.\n+  \/\/ This situation can happen in different scenarios:\n+  \/\/   1 - if the load or initialization of the field failed during step 8 of\n+  \/\/       the initialization of the holder of the field, in this case the access to the field\n+  \/\/       must fail\n+  \/\/   2 - it can also happen when the initialization of the holder class triggered the initialization of\n+  \/\/       another class which accesses this field in its static initializer, in this case the\n+  \/\/       access must succeed to allow circularity\n+  \/\/ The code below tries to load and initialize the field's class again before returning the default value.\n+  \/\/ If the field was not initialized because of an error, an exception should be thrown.\n+  \/\/ If the class is being initialized, the default value is returned.\n+  assert(entry->is_valid(), \"Invalid ResolvedFieldEntry\");\n+  instanceHandle mirror_h(THREAD, (instanceOop)mirror);\n+  InstanceKlass* klass = entry->field_holder();\n+  u2 index = entry->field_index();\n+  assert(klass == java_lang_Class::as_Klass(mirror), \"Not the field holder klass\");\n+  assert(klass->field_is_null_free_inline_type(index), \"Sanity check\");\n+  if (klass->is_being_initialized() && klass->is_reentrant_initialization(THREAD)) {\n+    int offset = klass->field_offset(index);\n+    Klass* field_k = klass->get_inline_type_field_klass_or_null(index);\n+    if (field_k == nullptr) {\n+      field_k = SystemDictionary::resolve_or_fail(klass->field_signature(index)->fundamental_name(THREAD),\n+          Handle(THREAD, klass->class_loader()),\n+          true, CHECK);\n+      assert(field_k != nullptr, \"Should have been loaded or an exception thrown above\");\n+      if (!field_k->is_inline_klass()) {\n+        THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                  err_msg(\"class %s expects class %s to be a concrete value class but it is not\",\n+                  klass->name()->as_C_string(), field_k->external_name()));\n+      }\n+      InlineLayoutInfo* li = klass->inline_layout_info_adr(index);\n+      li->set_klass(InlineKlass::cast(field_k));\n+      li->set_kind(LayoutKind::REFERENCE);\n+    }\n+    field_k->initialize(CHECK);\n+    oop defaultvalue = InlineKlass::cast(field_k)->default_value();\n+    \/\/ It is safe to initialize the static field because 1) the current thread is the initializing thread\n+    \/\/ and is the only one that can access it, and 2) the field is actually not initialized (i.e. null)\n+    \/\/ otherwise the JVM should not be executing this code.\n+    mirror_h()->obj_field_put(offset, defaultvalue);\n+    current->set_vm_result(defaultvalue);\n+  } else {\n+    assert(klass->is_in_error_state(), \"If not initializing, initialization must have failed to get there\");\n+    ResourceMark rm(THREAD);\n+    const char* desc = \"Could not initialize class \";\n+    const char* className = klass->external_name();\n+    size_t msglen = strlen(desc) + strlen(className) + 1;\n+    char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+    if (nullptr == message) {\n+      \/\/ Out of memory: can't create detailed error message\n+      THROW_MSG(vmSymbols::java_lang_NoClassDefFoundError(), className);\n+    } else {\n+      jio_snprintf(message, msglen, \"%s%s\", desc, className);\n+      THROW_MSG(vmSymbols::java_lang_NoClassDefFoundError(), message);\n+    }\n+  }\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::read_flat_field(JavaThread* current, oopDesc* obj, ResolvedFieldEntry* entry))\n+  assert(oopDesc::is_oop(obj), \"Sanity check\");\n+  Handle obj_h(THREAD, obj);\n+\n+  InstanceKlass* holder = InstanceKlass::cast(entry->field_holder());\n+  assert(entry->field_holder()->field_is_flat(entry->field_index()), \"Sanity check\");\n+\n+  InlineLayoutInfo* layout_info = holder->inline_layout_info_adr(entry->field_index());\n+  InlineKlass* field_vklass = layout_info->klass();\n+\n+#ifdef ASSERT\n+  fieldDescriptor fd;\n+  bool found = holder->find_field_from_offset(entry->field_offset(), false, &fd);\n+  assert(found, \"Field not found\");\n+  assert(fd.is_flat(), \"Field must be flat\");\n+#endif \/\/ ASSERT\n+\n+  oop res = field_vklass->read_payload_from_addr(obj_h(), entry->field_offset(), layout_info->kind(), CHECK);\n+  current->set_vm_result(res);\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::read_nullable_flat_field(JavaThread* current, oopDesc* obj, ResolvedFieldEntry* entry))\n+  assert(oopDesc::is_oop(obj), \"Sanity check\");\n+  assert(entry->has_null_marker(), \"Otherwise should not get there\");\n+  Handle obj_h(THREAD, obj);\n+\n+  InstanceKlass* holder = entry->field_holder();\n+  int field_index = entry->field_index();\n+  InlineLayoutInfo* li= holder->inline_layout_info_adr(field_index);\n+\n+#ifdef ASSERT\n+  fieldDescriptor fd;\n+  bool found = holder->find_field_from_offset(entry->field_offset(), false, &fd);\n+  assert(found, \"Field not found\");\n+  assert(fd.is_flat(), \"Field must be flat\");\n+#endif \/\/ ASSERT\n+\n+  InlineKlass* field_vklass = InlineKlass::cast(li->klass());\n+  oop res = field_vklass->read_payload_from_addr(obj_h(), entry->field_offset(), li->kind(), CHECK);\n+  current->set_vm_result(res);\n+\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::write_nullable_flat_field(JavaThread* current, oopDesc* obj, oopDesc* value, ResolvedFieldEntry* entry))\n+  assert(oopDesc::is_oop(obj), \"Sanity check\");\n+  Handle obj_h(THREAD, obj);\n+  assert(value == nullptr || oopDesc::is_oop(value), \"Sanity check\");\n+  Handle val_h(THREAD, value);\n+\n+  InstanceKlass* holder = entry->field_holder();\n+  InlineLayoutInfo* li = holder->inline_layout_info_adr(entry->field_index());\n+  InlineKlass* vk = li->klass();\n+  vk->write_value_to_addr(val_h(), ((char*)(oopDesc*)obj_h()) + entry->field_offset(), li->kind(), true, CHECK);\n+JRT_END\n@@ -235,1 +353,1 @@\n-  objArrayOop obj = oopFactory::new_objArray(klass, size, CHECK);\n+  arrayOop obj = oopFactory::new_objArray(klass, size, CHECK);\n@@ -239,0 +357,12 @@\n+JRT_ENTRY(void, InterpreterRuntime::flat_array_load(JavaThread* current, arrayOopDesc* array, int index))\n+  assert(array->is_flatArray(), \"Must be\");\n+  flatArrayOop farray = (flatArrayOop)array;\n+  oop res = farray->read_value_from_flat_array(index, CHECK);\n+  current->set_vm_result(res);\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::flat_array_store(JavaThread* current, oopDesc* val, arrayOopDesc* array, int index))\n+  assert(array->is_flatArray(), \"Must be\");\n+  flatArrayOop farray = (flatArrayOop)array;\n+  farray->write_value_to_flat_array(val, index, CHECK);\n+JRT_END\n@@ -244,2 +374,2 @@\n-  int          i = last_frame.get_index_u2(Bytecodes::_multianewarray);\n-  Klass* klass   = constants->klass_at(i, CHECK);\n+  int i = last_frame.get_index_u2(Bytecodes::_multianewarray);\n+  Klass* klass = constants->klass_at(i, CHECK);\n@@ -274,0 +404,24 @@\n+JRT_ENTRY(jboolean, InterpreterRuntime::is_substitutable(JavaThread* current, oopDesc* aobj, oopDesc* bobj))\n+  assert(oopDesc::is_oop(aobj) && oopDesc::is_oop(bobj), \"must be valid oops\");\n+\n+  Handle ha(THREAD, aobj);\n+  Handle hb(THREAD, bobj);\n+  JavaValue result(T_BOOLEAN);\n+  JavaCallArguments args;\n+  args.push_oop(ha);\n+  args.push_oop(hb);\n+  methodHandle method(current, Universe::is_substitutable_method());\n+  method->method_holder()->initialize(CHECK_false); \/\/ Ensure class ValueObjectMethods is initialized\n+  JavaCalls::call(&result, method, &args, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+    \/\/ Something really bad happened because isSubstitutable() should not throw exceptions\n+    \/\/ If it is an error, just let it propagate\n+    \/\/ If it is an exception, wrap it into an InternalError\n+    if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+      Handle e(THREAD, PENDING_EXCEPTION);\n+      CLEAR_PENDING_EXCEPTION;\n+      THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in substitutability test\", e, false);\n+    }\n+  }\n+  return result.get_jboolean();\n+JRT_END\n@@ -617,0 +771,4 @@\n+JRT_ENTRY(void, InterpreterRuntime::throw_InstantiationError(JavaThread* current))\n+  THROW(vmSymbols::java_lang_InstantiationError());\n+JRT_END\n+\n@@ -701,1 +859,5 @@\n-    get_code = ((is_static) ? Bytecodes::_getstatic : Bytecodes::_getfield);\n+    if (is_static) {\n+      get_code = Bytecodes::_getstatic;\n+    } else {\n+      get_code = Bytecodes::_getfield;\n+    }\n@@ -703,1 +865,1 @@\n-      put_code = ((is_static) ? Bytecodes::_putstatic : Bytecodes::_putfield);\n+        put_code = ((is_static) ? Bytecodes::_putstatic : Bytecodes::_putfield);\n@@ -708,1 +870,4 @@\n-  entry->set_flags(info.access_flags().is_final(), info.access_flags().is_volatile());\n+  entry->set_flags(info.access_flags().is_final(), info.access_flags().is_volatile(),\n+                   info.is_flat(), info.is_null_free_inline_type(),\n+                   info.has_null_marker());\n+\n@@ -755,1 +920,0 @@\n-\n@@ -760,1 +924,0 @@\n-\n@@ -775,0 +938,15 @@\n+JRT_ENTRY(void, InterpreterRuntime::throw_identity_exception(JavaThread* current, oopDesc* obj))\n+  Klass* klass = cast_to_oop(obj)->klass();\n+  ResourceMark rm(THREAD);\n+  const char* desc = \"Cannot synchronize on an instance of value class \";\n+  const char* className = klass->external_name();\n+  size_t msglen = strlen(desc) + strlen(className) + 1;\n+  char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+  if (nullptr == message) {\n+    \/\/ Out of memory: can't create detailed error message\n+    THROW_MSG(vmSymbols::java_lang_IdentityException(), className);\n+  } else {\n+    jio_snprintf(message, msglen, \"%s%s\", desc, className);\n+    THROW_MSG(vmSymbols::java_lang_IdentityException(), message);\n+  }\n+JRT_END\n@@ -1184,0 +1362,1 @@\n+  assert(entry->is_valid(), \"Invalid ResolvedFieldEntry\");\n@@ -1191,0 +1370,1 @@\n+  bool is_flat = entry->is_flat();\n@@ -1199,1 +1379,1 @@\n-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(field_holder, entry->field_offset(), is_static);\n+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(field_holder, entry->field_offset(), is_static, is_flat);\n@@ -1207,0 +1387,1 @@\n+  assert(entry->is_valid(), \"Invalid ResolvedFieldEntry\");\n@@ -1228,0 +1409,1 @@\n+\n@@ -1229,0 +1411,1 @@\n+  bool is_flat = entry->is_flat();\n@@ -1231,1 +1414,1 @@\n-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, entry->field_offset(), is_static);\n+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, entry->field_offset(), is_static, is_flat);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":193,"deletions":10,"binary":false,"changes":203,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -61,1 +61,1 @@\n-  static void    anewarray     (JavaThread* current, ConstantPool* pool, int index, jint size);\n+  static void    anewarray     (JavaThread* threcurrentad, ConstantPool* pool, int index, jint size);\n@@ -64,0 +64,10 @@\n+  static void    uninitialized_static_inline_type_field(JavaThread* current, oopDesc* mirror, ResolvedFieldEntry* entry);\n+  static void    write_heap_copy (JavaThread* current, oopDesc* value, int offset, oopDesc* rcv);\n+  static void    read_flat_field(JavaThread* current, oopDesc* object, ResolvedFieldEntry* entry);\n+  static void    read_nullable_flat_field(JavaThread* current, oopDesc* object, ResolvedFieldEntry* entry);\n+  static void    write_nullable_flat_field(JavaThread* current, oopDesc* object, oopDesc* value, ResolvedFieldEntry* entry);\n+\n+  static void flat_array_load(JavaThread* current, arrayOopDesc* array, int index);\n+  static void flat_array_store(JavaThread* current, oopDesc* val, arrayOopDesc* array, int index);\n+\n+  static jboolean is_substitutable(JavaThread* current, oopDesc* aobj, oopDesc* bobj);\n@@ -75,0 +85,1 @@\n+  static void    throw_InstantiationError(JavaThread* current);\n@@ -124,0 +135,1 @@\n+  static void    throw_identity_exception(JavaThread* current, oopDesc* obj);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.hpp","additions":14,"deletions":2,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -1004,1 +1004,2 @@\n-  bool is_put    = (byte == Bytecodes::_putfield  || byte == Bytecodes::_putstatic || byte == Bytecodes::_nofast_putfield);\n+  bool is_put    = (byte == Bytecodes::_putfield  || byte == Bytecodes::_putstatic ||\n+                    byte == Bytecodes::_nofast_putfield);\n@@ -1047,2 +1048,2 @@\n-                 is_static ? \"static\" : \"non-static\", resolved_klass->external_name(), fd.name()->as_C_string(),\n-                current_klass->external_name());\n+                  is_static ? \"static\" : \"non-static\", resolved_klass->external_name(), fd.name()->as_C_string(),\n+                  current_klass->external_name());\n@@ -1057,1 +1058,1 @@\n-                                                   !m->is_static_initializer());\n+                                                   !m->is_class_initializer());\n@@ -1060,1 +1061,1 @@\n-                                                     !m->is_object_initializer());\n+                                                     !m->is_object_constructor());\n@@ -1185,0 +1186,2 @@\n+  \/\/ Since this method is never inherited from a super, any appearance here under\n+  \/\/ the wrong class would be an error.\n@@ -1251,1 +1254,1 @@\n-      \/\/ check if the method is not <init>\n+      \/\/ check if the method is not <init>, which is never inherited\n@@ -1715,2 +1718,2 @@\n-                             const methodHandle& attached_method,\n-                             Bytecodes::Code byte, TRAPS) {\n+                                  const methodHandle& attached_method,\n+                                  Bytecodes::Code byte, bool check_null_and_abstract, TRAPS) {\n@@ -1721,0 +1724,1 @@\n+  Klass* recv_klass = recv.is_null() ? defc : recv->klass();\n@@ -1723,2 +1727,2 @@\n-      resolve_virtual_call(result, recv, recv->klass(), link_info,\n-                           \/*check_null_and_abstract=*\/true, CHECK);\n+      resolve_virtual_call(result, recv, recv_klass, link_info,\n+                           check_null_and_abstract, CHECK);\n@@ -1727,2 +1731,2 @@\n-      resolve_interface_call(result, recv, recv->klass(), link_info,\n-                             \/*check_null_and_abstract=*\/true, CHECK);\n+      resolve_interface_call(result, recv, recv_klass, link_info,\n+                             check_null_and_abstract, CHECK);\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.cpp","additions":16,"deletions":12,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -284,1 +284,1 @@\n-    bool v2 = vars[i].is_reference()  ? true : false;\n+    bool v2 = vars[i].is_reference();\n@@ -293,1 +293,1 @@\n-    bool v2 = stack[j].is_reference() ? true : false;\n+    bool v2 = stack[j].is_reference();\n@@ -377,1 +377,1 @@\n-    if ( cell->is_reference()) {\n+    if (cell->is_reference()) {\n","filename":"src\/hotspot\/share\/interpreter\/oopMapCache.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -457,1 +457,1 @@\n-                  if (!method->is_static_initializer()) {\n+                  if (!method->is_class_initializer()) {\n@@ -461,1 +461,1 @@\n-                  if (!method->is_object_initializer()) {\n+                  if (!method->is_object_constructor()) {\n","filename":"src\/hotspot\/share\/interpreter\/rewriter.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -161,1 +161,2 @@\n-  assert(_desc->tos_in()  == tos_in , \"inconsistent tos_in  information\");\n+  assert(_desc->tos_in()  == tos_in,\n+         \"inconsistent tos_in  information\");\n@@ -232,0 +233,1 @@\n+\n@@ -284,1 +286,1 @@\n-  def(Bytecodes::_aaload              , ____|____|____|____, itos, atos, aaload              ,  _           );\n+  def(Bytecodes::_aaload              , ____|____|clvm|____, itos, atos, aaload              ,  _           );\n@@ -436,0 +438,1 @@\n+  def(Bytecodes::_breakpoint          , ubcp|disp|clvm|____, vtos, vtos, _breakpoint         ,  _           );\n@@ -454,0 +457,1 @@\n+  def(Bytecodes::_fast_vgetfield      , ubcp|____|clvm|____, atos, atos, fast_accessfield    ,  atos        );\n@@ -463,0 +467,1 @@\n+  def(Bytecodes::_fast_vputfield      , ubcp|____|clvm|____, atos, vtos, fast_storefield ,   atos        );\n@@ -499,0 +504,1 @@\n+\n","filename":"src\/hotspot\/share\/interpreter\/templateTable.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1166,0 +1166,1 @@\n+      const bool return_scalarized     = false;\n@@ -1169,1 +1170,1 @@\n-                                      has_ea_local_in_scope, arg_escape,\n+                                      return_scalarized, has_ea_local_in_scope, arg_escape,\n@@ -1313,0 +1314,2 @@\n+      _offsets.set_value(CodeOffsets::Verified_Inline_Entry, pc_offset);\n+      _offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, pc_offset);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -89,1 +89,4 @@\n-    if (!mh->is_native() && !mh->is_static() && !mh->is_object_initializer() && !mh->is_static_initializer()) {\n+    if (!mh->is_native() &&\n+        !mh->is_static() &&\n+        !mh->is_object_constructor() &&\n+        !mh->is_class_initializer()) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompiler.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1603,1 +1603,1 @@\n-              Deoptimization::reassign_fields(vf->frame_pointer(), &reg_map, objects, realloc_failures, false);\n+              Deoptimization::reassign_fields(vf->frame_pointer(), &reg_map, objects, realloc_failures, false, CHECK_NULL);\n@@ -1854,1 +1854,1 @@\n-  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false);\n+  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false, THREAD);\n@@ -2182,1 +2182,1 @@\n-    if (m->is_object_initializer()) {\n+    if (m->is_object_constructor()) {\n@@ -2209,1 +2209,1 @@\n-    if (!m->is_object_initializer() && !m->is_static_initializer() && !m->is_overpass()) {\n+    if (!(m->is_object_constructor() || m->is_class_initializer()) && !m->is_overpass()) {\n@@ -2921,1 +2921,5 @@\n-  if (m->is_object_initializer()) {\n+  if (m->is_class_initializer()) {\n+      JVMCI_THROW_MSG_NULL(IllegalArgumentException,\n+          \"Cannot create java.lang.reflect.Method for class initializer\");\n+  }\n+  else if (m->is_object_constructor()) {\n@@ -2923,3 +2927,0 @@\n-  } else if (m->is_static_initializer()) {\n-    JVMCI_THROW_MSG_NULL(IllegalArgumentException,\n-        \"Cannot create java.lang.reflect.Method for class initializer\");\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -213,1 +213,1 @@\n-  nonstatic_field(InstanceKlass,               _misc_flags._flags,                            u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _misc_flags._flags,                            u4)                                    \\\n@@ -684,0 +684,3 @@\n+  declare_constant(DataLayout::array_store_data_tag)                      \\\n+  declare_constant(DataLayout::array_load_data_tag)                       \\\n+  declare_constant(DataLayout::acmp_data_tag)                             \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -36,0 +36,2 @@\n+#include \"oops\/fieldInfo.hpp\"\n+#include \"oops\/fieldStreams.inline.hpp\"\n@@ -37,0 +39,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -39,0 +42,1 @@\n+#include \"runtime\/fieldDescriptor.inline.hpp\"\n@@ -511,0 +515,124 @@\n+\n+class FindClassByNameClosure : public KlassInfoClosure {\n+ private:\n+  GrowableArray<Klass*>* _klasses;\n+  Symbol* _classname;\n+ public:\n+  FindClassByNameClosure(GrowableArray<Klass*>* klasses, Symbol* classname) :\n+    _klasses(klasses), _classname(classname) { }\n+\n+  void do_cinfo(KlassInfoEntry* cie) {\n+    if (cie->klass()->name() == _classname) {\n+      _klasses->append(cie->klass());\n+    }\n+  }\n+};\n+\n+class FieldDesc {\n+private:\n+  Symbol* _name;\n+  Symbol* _signature;\n+  int _offset;\n+  int _index;\n+  InstanceKlass* _holder;\n+  AccessFlags _access_flags;\n+  FieldInfo::FieldFlags _field_flags;\n+ public:\n+  FieldDesc() : _name(nullptr), _signature(nullptr), _offset(-1), _index(-1), _holder(nullptr),\n+                _access_flags(AccessFlags()), _field_flags(FieldInfo::FieldFlags((u4)0)) { }\n+\n+  FieldDesc(fieldDescriptor& fd) : _name(fd.name()), _signature(fd.signature()), _offset(fd.offset()),\n+                                   _index(fd.index()), _holder(fd.field_holder()),\n+                                   _access_flags(fd.access_flags()), _field_flags(fd.field_flags()) { }\n+\n+  const Symbol* name() { return _name;}\n+  const Symbol* signature() { return _signature; }\n+  int offset() const { return _offset; }\n+  int index() const { return _index; }\n+  const InstanceKlass* holder() { return _holder; }\n+  const AccessFlags& access_flags() { return _access_flags; }\n+  bool is_null_free_inline_type() const { return _field_flags.is_null_free_inline_type(); }\n+};\n+\n+static int compare_offset(FieldDesc* f1, FieldDesc* f2) {\n+   return f1->offset() > f2->offset() ? 1 : -1;\n+}\n+\n+static void print_field(outputStream* st, int level, int offset, FieldDesc& fd, bool is_inline_type, bool is_flat ) {\n+  const char* flat_field_msg = \"\";\n+  if (is_flat) {\n+    flat_field_msg = is_flat ? \"flat\" : \"not flat\";\n+  }\n+  st->print_cr(\"  @ %d %*s \\\"%s\\\" %s %s %s\",\n+      offset, level * 3, \"\",\n+      fd.name()->as_C_string(),\n+      fd.signature()->as_C_string(),\n+      is_inline_type ? \" \/\/ inline type \" : \"\",\n+      flat_field_msg);\n+}\n+\n+static void print_flat_field(outputStream* st, int level, int offset, InstanceKlass* klass) {\n+  assert(klass->is_inline_klass(), \"Only inline types can be flat\");\n+  InlineKlass* vklass = InlineKlass::cast(klass);\n+  GrowableArray<FieldDesc>* fields = new (mtServiceability) GrowableArray<FieldDesc>(100, mtServiceability);\n+  for (AllFieldStream fd(klass); !fd.done(); fd.next()) {\n+    if (!fd.access_flags().is_static()) {\n+      fields->append(FieldDesc(fd.field_descriptor()));\n+    }\n+  }\n+  fields->sort(compare_offset);\n+  for(int i = 0; i < fields->length(); i++) {\n+    FieldDesc fd = fields->at(i);\n+    int offset2 = offset + fd.offset() - vklass->payload_offset();\n+    print_field(st, level, offset2, fd,\n+        fd.is_null_free_inline_type(), fd.holder()->field_is_flat(fd.index()));\n+    if (fd.holder()->field_is_flat(fd.index())) {\n+      print_flat_field(st, level + 1, offset2 ,\n+          InstanceKlass::cast(fd.holder()->get_inline_type_field_klass(fd.index())));\n+    }\n+  }\n+}\n+\n+void PrintClassLayout::print_class_layout(outputStream* st, char* class_name) {\n+  KlassInfoTable cit(true);\n+  if (cit.allocation_failed()) {\n+    st->print_cr(\"ERROR: Ran out of C-heap; hierarchy not generated\");\n+    return;\n+  }\n+\n+  Thread* THREAD = Thread::current();\n+\n+  Symbol* classname = SymbolTable::probe(class_name, (int)strlen(class_name));\n+\n+  GrowableArray<Klass*>* klasses = new (mtServiceability) GrowableArray<Klass*>(100, mtServiceability);\n+\n+  FindClassByNameClosure fbnc(klasses, classname);\n+  cit.iterate(&fbnc);\n+\n+  for(int i = 0; i < klasses->length(); i++) {\n+    Klass* klass = klasses->at(i);\n+    if (!klass->is_instance_klass()) continue;  \/\/ Skip\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    int tab = 1;\n+    st->print_cr(\"Class %s [@%s]:\", klass->name()->as_C_string(),\n+        klass->class_loader_data()->loader_name());\n+    ResourceMark rm;\n+    GrowableArray<FieldDesc>* fields = new (mtServiceability) GrowableArray<FieldDesc>(100, mtServiceability);\n+    for (AllFieldStream fd(ik); !fd.done(); fd.next()) {\n+      if (!fd.access_flags().is_static()) {\n+        fields->append(FieldDesc(fd.field_descriptor()));\n+      }\n+    }\n+    fields->sort(compare_offset);\n+    for(int i = 0; i < fields->length(); i++) {\n+      FieldDesc fd = fields->at(i);\n+      print_field(st, 0, fd.offset(), fd, fd.is_null_free_inline_type(), fd.holder()->field_is_flat(fd.index()));\n+      if (fd.holder()->field_is_flat(fd.index())) {\n+        print_flat_field(st, 1, fd.offset(),\n+            InstanceKlass::cast(fd.holder()->get_inline_type_field_klass(fd.index())));\n+      }\n+    }\n+  }\n+  st->cr();\n+}\n+\n","filename":"src\/hotspot\/share\/memory\/heapInspection.cpp","additions":128,"deletions":0,"binary":false,"changes":128,"status":"modified"},{"patch":"@@ -32,0 +32,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/flatArrayOop.hpp\"\n@@ -35,0 +38,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -118,0 +122,45 @@\n+objArrayOop oopFactory::new_null_free_objArray(Klass* k, int length, TRAPS) {\n+  InlineKlass* klass = InlineKlass::cast(k);\n+  ObjArrayKlass* array_klass = klass->null_free_reference_array(CHECK_NULL);\n+\n+  assert(array_klass->is_objArray_klass(), \"Must be\");\n+  assert(array_klass->is_null_free_array_klass(), \"Must be\");\n+\n+  objArrayOop oop = array_klass->allocate(length, CHECK_NULL);\n+\n+  assert(oop == nullptr || oop->is_objArray(), \"Sanity\");\n+  assert(oop == nullptr || oop->klass()->is_null_free_array_klass(), \"Sanity\");\n+\n+  return oop;\n+}\n+\n+flatArrayOop oopFactory::new_flatArray(Klass* k, int length, LayoutKind lk, TRAPS) {\n+  InlineKlass* klass = InlineKlass::cast(k);\n+  Klass* array_klass = klass->flat_array_klass(lk, CHECK_NULL);\n+\n+  assert(array_klass->is_flatArray_klass(), \"Must be\");\n+\n+  flatArrayOop oop = FlatArrayKlass::cast(array_klass)->allocate(length, lk, CHECK_NULL);\n+  assert(oop == nullptr || oop->is_flatArray(), \"sanity\");\n+  assert(oop == nullptr || oop->klass()->is_flatArray_klass(), \"sanity\");\n+\n+  return oop;\n+}\n+\n+objArrayHandle oopFactory::copy_flatArray_to_objArray(flatArrayHandle array, TRAPS) {\n+  int len = array->length();\n+  FlatArrayKlass* vak = FlatArrayKlass::cast(array->klass());\n+  objArrayOop oarray = new_objectArray(array->length(), CHECK_(objArrayHandle()));\n+  objArrayHandle oarrayh(THREAD, oarray);\n+  vak->copy_array(array(), 0, oarrayh(), 0, len, CHECK_(objArrayHandle()));\n+  return oarrayh;\n+}\n+\n+objArrayHandle  oopFactory::ensure_objArray(oop array, TRAPS) {\n+  if (array != nullptr && array->is_flatArray()) {\n+    return copy_flatArray_to_objArray(flatArrayHandle(THREAD, flatArrayOop(array)), THREAD);\n+  } else {\n+    return objArrayHandle(THREAD, objArrayOop(array));\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/memory\/oopFactory.cpp","additions":49,"deletions":0,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -113,0 +113,2 @@\n+static LatestMethodCache _is_substitutable_cache;           \/\/ ValueObjectMethods.isSubstitutable()\n+static LatestMethodCache _value_object_hash_code_cache;     \/\/ ValueObjectMethods.valueObjectHashCode()\n@@ -455,0 +457,1 @@\n+\n@@ -888,1 +891,0 @@\n-\n@@ -1047,0 +1049,2 @@\n+Method* Universe::is_substitutable_method()       { return _is_substitutable_cache.get_method(); }\n+Method* Universe::value_object_hash_code_method() { return _value_object_hash_code_cache.get_method(); }\n@@ -1076,0 +1080,11 @@\n+\n+  \/\/ Set up substitutability testing\n+  ResourceMark rm(current);\n+  _is_substitutable_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::isSubstitutable_name()->as_C_string(),\n+                          vmSymbols::object_object_boolean_signature(), true);\n+  _value_object_hash_code_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::valueObjectHashCode_name()->as_C_string(),\n+                          vmSymbols::object_int_signature(), true);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -166,0 +166,4 @@\n+  void value_copy_internal(void* src, void* dst, size_t length) {\n+    Copy::copy_value_content(src, dst, length);\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/accessBackend.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -40,0 +41,1 @@\n+#include \"oops\/objArrayKlass.hpp\"\n@@ -111,0 +113,19 @@\n+Symbol* ArrayKlass::create_element_klass_array_name(Klass* element_klass, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  Symbol* name = nullptr;\n+  char *name_str = element_klass->name()->as_C_string();\n+  int len = element_klass->name()->utf8_length();\n+  char *new_str = NEW_RESOURCE_ARRAY(char, len + 4);\n+  int idx = 0;\n+  new_str[idx++] = JVM_SIGNATURE_ARRAY;\n+  if (element_klass->is_instance_klass()) { \/\/ it could be an array or simple type\n+    new_str[idx++] = JVM_SIGNATURE_CLASS;\n+  }\n+  memcpy(&new_str[idx], name_str, len * sizeof(char));\n+  idx += len;\n+  if (element_klass->is_instance_klass()) {\n+    new_str[idx++] = JVM_SIGNATURE_ENDCLASS;\n+  }\n+  new_str[idx++] = '\\0';\n+  return SymbolTable::new_symbol(new_str);\n+}\n@@ -142,1 +163,1 @@\n-          ObjArrayKlass::allocate_objArray_klass(class_loader_data(), dim + 1, this, CHECK_NULL);\n+      ObjArrayKlass::allocate_objArray_klass(class_loader_data(), dim + 1, this, false, CHECK_NULL);\n@@ -200,0 +221,4 @@\n+oop ArrayKlass::component_mirror() const {\n+  return java_lang_Class::component_mirror(java_mirror());\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/arrayKlass.cpp","additions":26,"deletions":1,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -265,1 +266,1 @@\n-      \/\/ All of these should have been reverted back to ClassIndex before calling\n+      \/\/ All of these should have been reverted back to Unresolved before calling\n@@ -637,0 +638,6 @@\n+void check_is_inline_type(Klass* k, TRAPS) {\n+  if (!k->is_inline_klass()) {\n+    THROW(vmSymbols::java_lang_IncompatibleClassChangeError());\n+  }\n+}\n+\n@@ -675,0 +682,1 @@\n+  bool inline_type_signature = false;\n@@ -683,0 +691,3 @@\n+  if (inline_type_signature) {\n+    name->decrement_refcount();\n+  }\n@@ -691,0 +702,16 @@\n+  if (!HAS_PENDING_EXCEPTION && inline_type_signature) {\n+    check_is_inline_type(k, THREAD);\n+  }\n+\n+  if (!HAS_PENDING_EXCEPTION) {\n+    Klass* bottom_klass = nullptr;\n+    if (k->is_objArray_klass()) {\n+      bottom_klass = ObjArrayKlass::cast(k)->bottom_klass();\n+      assert(bottom_klass != nullptr, \"Should be set\");\n+      assert(bottom_klass->is_instance_klass() || bottom_klass->is_typeArray_klass(), \"Sanity check\");\n+    } else if (k->is_flatArray_klass()) {\n+      bottom_klass = FlatArrayKlass::cast(k)->element_klass();\n+      assert(bottom_klass != nullptr, \"Should be set\");\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":28,"deletions":1,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -213,0 +213,1 @@\n+      invoke_code = Bytecodes::_invokevirtual;\n@@ -228,1 +229,1 @@\n-    method_entry->set_bytecode2(Bytecodes::_invokevirtual);\n+    method_entry->set_bytecode2(invoke_code);\n","filename":"src\/hotspot\/share\/oops\/cpCache.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -110,0 +110,6 @@\n+    if (fi_ref->field_flags().is_flat()) {\n+      assert(fi_ref->layout_kind() == fi.layout_kind(), \"Must be\");\n+    }\n+    if (fi_ref->field_flags().has_null_marker()) {\n+      assert(fi_ref->null_marker_offset() == fi.null_marker_offset(), \"Must be\");\n+    }\n","filename":"src\/hotspot\/share\/oops\/fieldInfo.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"oops\/instanceKlass.hpp\"\n@@ -88,0 +89,7 @@\n+    if (fi.field_flags().is_flat()) {\n+      assert(fi.layout_kind() != LayoutKind::UNKNOWN, \"Must be set\");\n+      _consumer->accept_uint(fi.layout_kind());\n+    }\n+    if (fi.field_flags().has_null_marker()) {\n+      _consumer->accept_uint(fi.null_marker_offset());\n+    }\n@@ -92,0 +100,1 @@\n+    assert(fi.null_marker_offset() == 0, \"\");\n@@ -122,0 +131,8 @@\n+  if (fi._field_flags.is_flat()) {\n+    fi._layout_kind = static_cast<LayoutKind>(next_uint());\n+  }\n+  if (fi._field_flags.has_null_marker()) {\n+    fi._null_marker_offset = next_uint();\n+  } else {\n+    fi._null_marker_offset = 0;\n+  }\n@@ -132,1 +149,3 @@\n-                                ff.is_contended());\n+                                ff.is_contended() +\n+                                ff.is_flat() +\n+                                ff.has_null_marker());\n","filename":"src\/hotspot\/share\/oops\/fieldInfo.inline.hpp","additions":20,"deletions":1,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -0,0 +1,476 @@\n+\/*\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"classfile\/moduleEntry.hpp\"\n+#include \"classfile\/packageEntry.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"gc\/shared\/collectedHeap.inline.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/metadataFactory.hpp\"\n+#include \"memory\/metaspaceClosure.hpp\"\n+#include \"memory\/oopFactory.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/arrayKlass.inline.hpp\"\n+#include \"oops\/arrayOop.hpp\"\n+#include \"oops\/flatArrayOop.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n+#include \"oops\/instanceKlass.hpp\"\n+#include \"oops\/klass.inline.hpp\"\n+#include \"oops\/objArrayKlass.hpp\"\n+#include \"oops\/objArrayOop.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"oops\/verifyOopClosure.hpp\"\n+#include \"runtime\/handles.inline.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"utilities\/copy.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+#include \"oops\/flatArrayKlass.hpp\"\n+\n+\/\/ Allocation...\n+\n+FlatArrayKlass::FlatArrayKlass(Klass* element_klass, Symbol* name, LayoutKind lk) : ArrayKlass(name, Kind) {\n+  assert(element_klass->is_inline_klass(), \"Expected Inline\");\n+  assert(lk == NON_ATOMIC_FLAT || lk == ATOMIC_FLAT || lk == NULLABLE_ATOMIC_FLAT, \"Must be a flat layout\");\n+\n+  set_element_klass(InlineKlass::cast(element_klass));\n+  set_class_loader_data(element_klass->class_loader_data());\n+  set_layout_kind(lk);\n+\n+  set_layout_helper(array_layout_helper(InlineKlass::cast(element_klass), lk));\n+  assert(is_array_klass(), \"sanity\");\n+  assert(is_flatArray_klass(), \"sanity\");\n+  assert(is_null_free_array_klass(), \"sanity\");\n+\n+#ifdef _LP64\n+  set_prototype_header(markWord::flat_array_prototype(lk));\n+  assert(prototype_header().is_flat_array(), \"sanity\");\n+#else\n+  fatal(\"Not supported yet\");\n+  set_prototype_header(markWord::inline_type_prototype());\n+#endif\n+\n+#ifdef ASSERT\n+  switch(lk) {\n+    case NON_ATOMIC_FLAT:\n+      assert(layout_helper_is_null_free(layout_helper()), \"Must be\");\n+      assert(layout_helper_is_array(layout_helper()), \"Must be\");\n+      assert(layout_helper_is_flatArray(layout_helper()), \"Must be\");\n+      assert(layout_helper_element_type(layout_helper()) == T_FLAT_ELEMENT, \"Must be\");\n+      assert(prototype_header().is_null_free_array(), \"Must be\");\n+      assert(prototype_header().is_flat_array(), \"Must be\");\n+    break;\n+    default:\n+    break;\n+  }\n+#endif \/\/ ASSERT\n+\n+#ifndef PRODUCT\n+  if (PrintFlatArrayLayout) {\n+    print();\n+  }\n+#endif\n+}\n+\n+FlatArrayKlass* FlatArrayKlass::allocate_klass(Klass* eklass, LayoutKind lk, TRAPS) {\n+  guarantee((!Universe::is_bootstrapping() || vmClasses::Object_klass_loaded()), \"Really ?!\");\n+  assert(UseArrayFlattening, \"Flatten array required\");\n+  assert(MultiArray_lock->holds_lock(THREAD), \"must hold lock after bootstrapping\");\n+\n+  InlineKlass* element_klass = InlineKlass::cast(eklass);\n+  assert(element_klass->must_be_atomic() || (!AlwaysAtomicAccesses), \"Atomic by-default\");\n+\n+  \/\/ Eagerly allocate the direct array supertype.\n+  Klass* super_klass = nullptr;\n+  Klass* element_super = element_klass->super();\n+  if (element_super != nullptr) {\n+    \/\/ The element type has a direct super.  E.g., String[] has direct super of Object[].\n+    super_klass = element_klass->array_klass(CHECK_NULL);\n+    \/\/ Also, see if the element has secondary supertypes.\n+    \/\/ We need an array type for each.\n+    const Array<Klass*>* element_supers = element_klass->secondary_supers();\n+    for( int i = element_supers->length()-1; i >= 0; i-- ) {\n+      Klass* elem_super = element_supers->at(i);\n+      elem_super->array_klass(CHECK_NULL);\n+    }\n+   \/\/ Fall through because inheritance is acyclic and we hold the global recursive lock to allocate all the arrays.\n+  }\n+\n+  Symbol* name = ArrayKlass::create_element_klass_array_name(element_klass, CHECK_NULL);\n+  ClassLoaderData* loader_data = element_klass->class_loader_data();\n+  int size = ArrayKlass::static_size(FlatArrayKlass::header_size());\n+  FlatArrayKlass* vak = new (loader_data, size, THREAD) FlatArrayKlass(element_klass, name, lk);\n+\n+  ModuleEntry* module = vak->module();\n+  assert(module != nullptr, \"No module entry for array\");\n+  complete_create_array_klass(vak, super_klass, module, CHECK_NULL);\n+\n+  loader_data->add_class(vak);\n+\n+  return vak;\n+}\n+\n+void FlatArrayKlass::initialize(TRAPS) {\n+  element_klass()->initialize(THREAD);\n+}\n+\n+void FlatArrayKlass::metaspace_pointers_do(MetaspaceClosure* it) {\n+  ArrayKlass::metaspace_pointers_do(it);\n+  it->push(&_element_klass);\n+}\n+\n+\/\/ Oops allocation...\n+flatArrayOop FlatArrayKlass::allocate(int length, LayoutKind lk, TRAPS) {\n+  check_array_allocation_length(length, max_elements(), CHECK_NULL);\n+  int size = flatArrayOopDesc::object_size(layout_helper(), length);\n+  flatArrayOop array = (flatArrayOop) Universe::heap()->array_allocate(this, size, length, true, CHECK_NULL);\n+  return array;\n+}\n+\n+oop FlatArrayKlass::multi_allocate(int rank, jint* last_size, TRAPS) {\n+  \/\/ FlatArrays only have one dimension\n+  ShouldNotReachHere();\n+}\n+\n+jint FlatArrayKlass::array_layout_helper(InlineKlass* vk, LayoutKind lk) {\n+  BasicType etype = T_FLAT_ELEMENT;\n+  int esize = log2i_exact(round_up_power_of_2(vk->layout_size_in_bytes(lk)));\n+  int hsize = arrayOopDesc::base_offset_in_bytes(etype);\n+\n+  int lh = Klass::array_layout_helper(_lh_array_tag_vt_value, true, hsize, etype, esize);\n+\n+  assert(lh < (int)_lh_neutral_value, \"must look like an array layout\");\n+  assert(layout_helper_is_array(lh), \"correct kind\");\n+  assert(layout_helper_is_flatArray(lh), \"correct kind\");\n+  assert(!layout_helper_is_typeArray(lh), \"correct kind\");\n+  assert(!layout_helper_is_objArray(lh), \"correct kind\");\n+  assert(layout_helper_is_null_free(lh), \"correct kind\");\n+  assert(layout_helper_header_size(lh) == hsize, \"correct decode\");\n+  assert(layout_helper_element_type(lh) == etype, \"correct decode\");\n+  assert(layout_helper_log2_element_size(lh) == esize, \"correct decode\");\n+  assert((1 << esize) < BytesPerLong || is_aligned(hsize, HeapWordsPerLong), \"unaligned base\");\n+\n+  return lh;\n+}\n+\n+size_t FlatArrayKlass::oop_size(oop obj) const {\n+  assert(obj->klass()->is_flatArray_klass(),\"must be an flat array\");\n+  flatArrayOop array = flatArrayOop(obj);\n+  return array->object_size();\n+}\n+\n+\/\/ For now return the maximum number of array elements that will not exceed:\n+\/\/ nof bytes = \"max_jint * HeapWord\" since the \"oopDesc::oop_iterate_size\"\n+\/\/ returns \"int\" HeapWords, need fix for JDK-4718400 and JDK-8233189\n+jint FlatArrayKlass::max_elements() const {\n+  \/\/ Check the max number of heap words limit first (because of int32_t in oopDesc_oop_size() etc)\n+  size_t max_size = max_jint;\n+  max_size -= (arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT) >> LogHeapWordSize);\n+  max_size = align_down(max_size, MinObjAlignment);\n+  max_size <<= LogHeapWordSize;                                  \/\/ convert to max payload size in bytes\n+  max_size >>= layout_helper_log2_element_size(_layout_helper);  \/\/ divide by element size (in bytes) = max elements\n+  \/\/ Within int32_t heap words, still can't exceed Java array element limit\n+  if (max_size > max_jint) {\n+    max_size = max_jint;\n+  }\n+  assert((max_size >> LogHeapWordSize) <= max_jint, \"Overflow\");\n+  return (jint) max_size;\n+}\n+\n+oop FlatArrayKlass::protection_domain() const {\n+  return element_klass()->protection_domain();\n+}\n+\n+\/\/ Temp hack having this here: need to move towards Access API\n+static bool needs_backwards_copy(arrayOop s, int src_pos,\n+                                 arrayOop d, int dst_pos, int length) {\n+  return (s == d) && (dst_pos > src_pos) && (dst_pos - src_pos) < length;\n+}\n+\n+void FlatArrayKlass::copy_array(arrayOop s, int src_pos,\n+                                arrayOop d, int dst_pos, int length, TRAPS) {\n+\n+  assert(s->is_objArray() || s->is_flatArray(), \"must be obj or flat array\");\n+\n+  \/\/ Check destination\n+  if ((!d->is_flatArray()) && (!d->is_objArray())) {\n+    THROW(vmSymbols::java_lang_ArrayStoreException());\n+  }\n+\n+  \/\/ Check if all offsets and lengths are non negative\n+  if (src_pos < 0 || dst_pos < 0 || length < 0) {\n+    THROW(vmSymbols::java_lang_ArrayIndexOutOfBoundsException());\n+  }\n+  \/\/ Check if the ranges are valid\n+  if  ( (((unsigned int) length + (unsigned int) src_pos) > (unsigned int) s->length())\n+      || (((unsigned int) length + (unsigned int) dst_pos) > (unsigned int) d->length()) ) {\n+    THROW(vmSymbols::java_lang_ArrayIndexOutOfBoundsException());\n+  }\n+  \/\/ Check zero copy\n+  if (length == 0)\n+    return;\n+\n+  ArrayKlass* sk = ArrayKlass::cast(s->klass());\n+  ArrayKlass* dk = ArrayKlass::cast(d->klass());\n+  Klass* d_elem_klass = dk->element_klass();\n+  Klass* s_elem_klass = sk->element_klass();\n+  \/**** CMH: compare and contrast impl, re-factor once we find edge cases... ****\/\n+\n+  if (sk->is_flatArray_klass()) {\n+    assert(sk == this, \"Unexpected call to copy_array\");\n+    FlatArrayKlass* fsk = FlatArrayKlass::cast(sk);\n+    \/\/ Check subtype, all src homogeneous, so just once\n+    if (!s_elem_klass->is_subtype_of(d_elem_klass)) {\n+      THROW(vmSymbols::java_lang_ArrayStoreException());\n+    }\n+\n+    flatArrayOop sa = flatArrayOop(s);\n+    InlineKlass* s_elem_vklass = element_klass();\n+\n+    \/\/ flatArray-to-flatArray\n+    if (dk->is_flatArray_klass()) {\n+      \/\/ element types MUST be exact, subtype check would be dangerous\n+      if (d_elem_klass != this->element_klass()) {\n+        THROW(vmSymbols::java_lang_ArrayStoreException());\n+      }\n+\n+      FlatArrayKlass* fdk = FlatArrayKlass::cast(dk);\n+      InlineKlass* vk = InlineKlass::cast(s_elem_klass);\n+      flatArrayOop da = flatArrayOop(d);\n+      int src_incr = fsk->element_byte_size();\n+      int dst_incr = fdk->element_byte_size();\n+\n+      if (fsk->layout_kind() == fdk->layout_kind()) {\n+        assert(src_incr == dst_incr, \"Must be\");\n+        if (needs_backwards_copy(sa, src_pos, da, dst_pos, length)) {\n+          address dst = (address) da->value_at_addr(dst_pos + length - 1, fdk->layout_helper());\n+          address src = (address) sa->value_at_addr(src_pos + length - 1, fsk->layout_helper());\n+          for (int i = 0; i < length; i++) {\n+            \/\/ because source and destination have the same layout, bypassing the InlineKlass copy methods\n+            \/\/ and call AccessAPI directly\n+            HeapAccess<>::value_copy(src, dst, vk, fsk->layout_kind());\n+            dst -= dst_incr;\n+            src -= src_incr;\n+          }\n+        } else {\n+          \/\/ source and destination share same layout, direct copy from array to array is possible\n+          address dst = (address) da->value_at_addr(dst_pos, fdk->layout_helper());\n+          address src = (address) sa->value_at_addr(src_pos, fsk->layout_helper());\n+          for (int i = 0; i < length; i++) {\n+            \/\/ because source and destination have the same layout, bypassing the InlineKlass copy methods\n+            \/\/ and call AccessAPI directly\n+            HeapAccess<>::value_copy(src, dst, vk, fsk->layout_kind());\n+            dst += dst_incr;\n+            src += src_incr;\n+          }\n+        }\n+      } else {\n+        flatArrayHandle hd(THREAD, da);\n+        flatArrayHandle hs(THREAD, sa);\n+        \/\/ source and destination layouts mismatch, simpler solution is to copy through an intermediate buffer (heap instance)\n+        bool need_null_check = fsk->layout_kind() == NULLABLE_ATOMIC_FLAT && fdk->layout_kind() != NULLABLE_ATOMIC_FLAT;\n+        oop buffer = vk->allocate_instance(CHECK);\n+        address dst = (address) hd->value_at_addr(dst_pos, fdk->layout_helper());\n+        address src = (address) hs->value_at_addr(src_pos, fsk->layout_helper());\n+        for (int i = 0; i < length; i++) {\n+          if (need_null_check) {\n+            if (vk->is_payload_marked_as_null(src)) {\n+              THROW(vmSymbols::java_lang_NullPointerException());\n+            }\n+          }\n+          vk->copy_payload_to_addr(src, vk->payload_addr(buffer), fsk->layout_kind(), true);\n+          if (vk->has_nullable_atomic_layout()) {\n+            \/\/ Setting null marker to not zero for non-nullable source layouts\n+            vk->mark_payload_as_non_null(vk->payload_addr(buffer));\n+          }\n+          vk->copy_payload_to_addr(vk->payload_addr(buffer), dst, fdk->layout_kind(), true);\n+          dst += dst_incr;\n+          src += src_incr;\n+        }\n+      }\n+    } else { \/\/ flatArray-to-objArray\n+      assert(dk->is_objArray_klass(), \"Expected objArray here\");\n+      \/\/ Need to allocate each new src elem payload -> dst oop\n+      objArrayHandle dh(THREAD, (objArrayOop)d);\n+      flatArrayHandle sh(THREAD, sa);\n+      InlineKlass* vk = InlineKlass::cast(s_elem_klass);\n+      for (int i = 0; i < length; i++) {\n+        oop o = sh->read_value_from_flat_array(src_pos + i, CHECK);\n+        dh->obj_at_put(dst_pos + i, o);\n+      }\n+    }\n+  } else {\n+    assert(s->is_objArray(), \"Expected objArray\");\n+    objArrayOop sa = objArrayOop(s);\n+    assert(d->is_flatArray(), \"Expected flatArray\");  \/\/ objArray-to-flatArray\n+    InlineKlass* d_elem_vklass = InlineKlass::cast(d_elem_klass);\n+    flatArrayOop da = flatArrayOop(d);\n+    FlatArrayKlass* fdk = FlatArrayKlass::cast(da->klass());\n+    InlineKlass* vk = InlineKlass::cast(d_elem_klass);\n+\n+    for (int i = 0; i < length; i++) {\n+      da->write_value_to_flat_array(sa->obj_at(src_pos + i), dst_pos + i, CHECK);\n+    }\n+  }\n+}\n+\n+ModuleEntry* FlatArrayKlass::module() const {\n+  assert(element_klass() != nullptr, \"FlatArrayKlass returned unexpected nullptr bottom_klass\");\n+  \/\/ The array is defined in the module of its bottom class\n+  return element_klass()->module();\n+}\n+\n+PackageEntry* FlatArrayKlass::package() const {\n+  assert(element_klass() != nullptr, \"FlatArrayKlass returned unexpected nullptr bottom_klass\");\n+  return element_klass()->package();\n+}\n+\n+bool FlatArrayKlass::can_be_primary_super_slow() const {\n+    return true;\n+}\n+\n+GrowableArray<Klass*>* FlatArrayKlass::compute_secondary_supers(int num_extra_slots,\n+                                                                Array<InstanceKlass*>* transitive_interfaces) {\n+  assert(transitive_interfaces == nullptr, \"sanity\");\n+  \/\/ interfaces = { cloneable_klass, serializable_klass, elemSuper[], ... };\n+  Array<Klass*>* elem_supers = element_klass()->secondary_supers();\n+  int num_elem_supers = elem_supers == nullptr ? 0 : elem_supers->length();\n+  int num_secondaries = num_extra_slots + 2 + num_elem_supers;\n+  GrowableArray<Klass*>* secondaries = new GrowableArray<Klass*>(num_elem_supers+2);\n+\n+  secondaries->push(vmClasses::Cloneable_klass());\n+  secondaries->push(vmClasses::Serializable_klass());\n+  for (int i = 0; i < num_elem_supers; i++) {\n+    Klass* elem_super = (Klass*) elem_supers->at(i);\n+    Klass* array_super = elem_super->array_klass_or_null();\n+    assert(array_super != nullptr, \"must already have been created\");\n+    secondaries->push(array_super);\n+  }\n+  return secondaries;\n+}\n+\n+u2 FlatArrayKlass::compute_modifier_flags() const {\n+  \/\/ The modifier for an flatArray is the same as its element\n+  \/\/ With the addition of ACC_IDENTITY\n+  u2 element_flags = element_klass()->compute_modifier_flags();\n+\n+  u2 identity_flag = (Arguments::enable_preview()) ? JVM_ACC_IDENTITY : 0;\n+\n+  return (element_flags & (JVM_ACC_PUBLIC | JVM_ACC_PRIVATE | JVM_ACC_PROTECTED))\n+                        | (identity_flag | JVM_ACC_ABSTRACT | JVM_ACC_FINAL);\n+}\n+\n+void FlatArrayKlass::print_on(outputStream* st) const {\n+#ifndef PRODUCT\n+  assert(!is_objArray_klass(), \"Unimplemented\");\n+\n+  st->print(\"Flat Type Array: \");\n+  Klass::print_on(st);\n+\n+  st->print(\" - element klass: \");\n+  element_klass()->print_value_on(st);\n+  st->cr();\n+\n+  int elem_size = element_byte_size();\n+  st->print(\" - element size %i \", elem_size);\n+  st->print(\"aligned layout size %i\", 1 << layout_helper_log2_element_size(layout_helper()));\n+  st->cr();\n+#endif \/\/PRODUCT\n+}\n+\n+void FlatArrayKlass::print_value_on(outputStream* st) const {\n+  assert(is_klass(), \"must be klass\");\n+\n+  element_klass()->print_value_on(st);\n+  st->print(\"[]\");\n+}\n+\n+\n+#ifndef PRODUCT\n+void FlatArrayKlass::oop_print_on(oop obj, outputStream* st) {\n+  ArrayKlass::oop_print_on(obj, st);\n+  flatArrayOop va = flatArrayOop(obj);\n+  InlineKlass* vk = element_klass();\n+  int print_len = MIN2(va->length(), MaxElementPrintSize);\n+  for(int index = 0; index < print_len; index++) {\n+    int off = (address) va->value_at_addr(index, layout_helper()) - cast_from_oop<address>(obj);\n+    st->print_cr(\" - Index %3d offset %3d: \", index, off);\n+    oop obj = cast_to_oop((address)va->value_at_addr(index, layout_helper()) - vk->payload_offset());\n+    FieldPrinter print_field(st, obj);\n+    vk->do_nonstatic_fields(&print_field);\n+    st->cr();\n+  }\n+  int remaining = va->length() - print_len;\n+  if (remaining > 0) {\n+    st->print_cr(\" - <%d more elements, increase MaxElementPrintSize to print>\", remaining);\n+  }\n+}\n+#endif \/\/PRODUCT\n+\n+void FlatArrayKlass::oop_print_value_on(oop obj, outputStream* st) {\n+  assert(obj->is_flatArray(), \"must be flatArray\");\n+  st->print(\"a \");\n+  element_klass()->print_value_on(st);\n+  int len = flatArrayOop(obj)->length();\n+  st->print(\"[%d] \", len);\n+  obj->print_address_on(st);\n+  if (PrintMiscellaneous && (WizardMode || Verbose)) {\n+    int lh = layout_helper();\n+    st->print(\"{\");\n+    for (int i = 0; i < len; i++) {\n+      if (i > 4) {\n+        st->print(\"...\"); break;\n+      }\n+      st->print(\" \" INTPTR_FORMAT, (intptr_t)(void*)flatArrayOop(obj)->value_at_addr(i , lh));\n+    }\n+    st->print(\" }\");\n+  }\n+}\n+\n+\/\/ Verification\n+class VerifyElementClosure: public BasicOopIterateClosure {\n+ public:\n+  virtual void do_oop(oop* p)       { VerifyOopClosure::verify_oop.do_oop(p); }\n+  virtual void do_oop(narrowOop* p) { VerifyOopClosure::verify_oop.do_oop(p); }\n+};\n+\n+void FlatArrayKlass::oop_verify_on(oop obj, outputStream* st) {\n+  ArrayKlass::oop_verify_on(obj, st);\n+  guarantee(obj->is_flatArray(), \"must be flatArray\");\n+\n+  if (contains_oops()) {\n+    flatArrayOop va = flatArrayOop(obj);\n+    VerifyElementClosure ec;\n+    va->oop_iterate(&ec);\n+  }\n+}\n+\n+void FlatArrayKlass::verify_on(outputStream* st) {\n+  ArrayKlass::verify_on(st);\n+  guarantee(element_klass()->is_inline_klass(), \"should be inline type klass\");\n+}\n","filename":"src\/hotspot\/share\/oops\/flatArrayKlass.cpp","additions":476,"deletions":0,"binary":false,"changes":476,"status":"added"},{"patch":"@@ -0,0 +1,28 @@\n+\/*\n+ * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\n+#include \"flatArrayOop.hpp\"\n+#include \"flatArrayKlass.inline.hpp\"\n+\n","filename":"src\/hotspot\/share\/oops\/flatArrayOop.cpp","additions":28,"deletions":0,"binary":false,"changes":28,"status":"added"},{"patch":"@@ -141,1 +141,1 @@\n-    if (!is_static)\n+    if (!is_static) {\n@@ -143,0 +143,1 @@\n+    }\n@@ -1604,0 +1605,1 @@\n+    case Bytecodes::_invokeinterface:\n@@ -1605,4 +1607,3 @@\n-    case Bytecodes::_invokespecial:     do_method(false, false, itr->get_index_u2(), itr->bci(), itr->code()); break;\n-    case Bytecodes::_invokestatic:      do_method(true,  false, itr->get_index_u2(), itr->bci(), itr->code()); break;\n-    case Bytecodes::_invokedynamic:     do_method(true,  false, itr->get_index_u4(), itr->bci(), itr->code()); break;\n-    case Bytecodes::_invokeinterface:   do_method(false, true,  itr->get_index_u2(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_invokespecial:     do_method(false, itr->get_index_u2(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_invokestatic:      do_method(true , itr->get_index_u2(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_invokedynamic:     do_method(true , itr->get_index_u4(), itr->bci(), itr->code()); break;\n@@ -1628,0 +1629,1 @@\n+\n@@ -1953,1 +1955,3 @@\n-  if (!is_static) in[i++] = CellTypeState::ref;\n+  if (!is_static) {\n+    in[i++] = CellTypeState::ref;\n+  }\n@@ -1959,1 +1963,1 @@\n-void GenerateOopMap::do_method(int is_static, int is_interface, int idx, int bci, Bytecodes::Code bc) {\n+void GenerateOopMap::do_method(int is_static, int idx, int bci, Bytecodes::Code bc) {\n","filename":"src\/hotspot\/share\/oops\/generateOopMap.cpp","additions":11,"deletions":7,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -0,0 +1,782 @@\n+\/*\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"cds\/archiveUtils.hpp\"\n+#include \"cds\/cdsConfig.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"code\/codeCache.hpp\"\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/collectedHeap.inline.hpp\"\n+#include \"gc\/shared\/gcLocker.inline.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/metaspaceClosure.hpp\"\n+#include \"memory\/metadataFactory.hpp\"\n+#include \"oops\/access.hpp\"\n+#include \"oops\/compressedOops.inline.hpp\"\n+#include \"oops\/fieldStreams.inline.hpp\"\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n+#include \"oops\/instanceKlass.inline.hpp\"\n+#include \"oops\/method.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"oops\/objArrayKlass.hpp\"\n+#include \"runtime\/fieldDescriptor.inline.hpp\"\n+#include \"runtime\/handles.inline.hpp\"\n+#include \"runtime\/safepointVerifiers.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/signature.hpp\"\n+#include \"runtime\/thread.inline.hpp\"\n+#include \"utilities\/copy.hpp\"\n+\n+  \/\/ Constructor\n+InlineKlass::InlineKlass(const ClassFileParser& parser)\n+    : InstanceKlass(parser, InlineKlass::Kind) {\n+  set_prototype_header(markWord::inline_type_prototype());\n+  assert(is_inline_klass(), \"sanity\");\n+  assert(prototype_header().is_inline_type(), \"sanity\");\n+}\n+\n+InlineKlass::InlineKlass() {\n+  assert(CDSConfig::is_dumping_archive() || UseSharedSpaces, \"only for CDS\");\n+}\n+\n+void InlineKlass::init_fixed_block() {\n+  _adr_inlineklass_fixed_block = inlineklass_static_block();\n+  \/\/ Addresses used for inline type calling convention\n+  *((Array<SigEntry>**)adr_extended_sig()) = nullptr;\n+  *((Array<VMRegPair>**)adr_return_regs()) = nullptr;\n+  *((address*)adr_pack_handler()) = nullptr;\n+  *((address*)adr_pack_handler_jobject()) = nullptr;\n+  *((address*)adr_unpack_handler()) = nullptr;\n+  assert(pack_handler() == nullptr, \"pack handler not null\");\n+  *((address*)adr_non_atomic_flat_array_klass()) = nullptr;\n+  *((address*)adr_atomic_flat_array_klass()) = nullptr;\n+  *((address*)adr_nullable_atomic_flat_array_klass()) = nullptr;\n+  *((address*)adr_null_free_reference_array_klass()) = nullptr;\n+  set_default_value_offset(0);\n+  set_null_reset_value_offset(0);\n+  set_payload_offset(-1);\n+  set_payload_size_in_bytes(-1);\n+  set_payload_alignment(-1);\n+  set_non_atomic_size_in_bytes(-1);\n+  set_non_atomic_alignment(-1);\n+  set_atomic_size_in_bytes(-1);\n+  set_nullable_size_in_bytes(-1);\n+  set_null_marker_offset(-1);\n+}\n+\n+void InlineKlass::set_default_value(oop val) {\n+  assert(val != nullptr, \"Sanity check\");\n+  assert(oopDesc::is_oop(val), \"Sanity check\");\n+  assert(val->is_inline_type(), \"Sanity check\");\n+  assert(val->klass() == this, \"sanity check\");\n+  java_mirror()->obj_field_put(default_value_offset(), val);\n+}\n+\n+void InlineKlass::set_null_reset_value(oop val) {\n+  assert(val != nullptr, \"Sanity check\");\n+  assert(oopDesc::is_oop(val), \"Sanity check\");\n+  assert(val->is_inline_type(), \"Sanity check\");\n+  assert(val->klass() == this, \"sanity check\");\n+  java_mirror()->obj_field_put(null_reset_value_offset(), val);\n+}\n+\n+instanceOop InlineKlass::allocate_instance(TRAPS) {\n+  int size = size_helper();  \/\/ Query before forming handle.\n+\n+  instanceOop oop = (instanceOop)Universe::heap()->obj_allocate(this, size, CHECK_NULL);\n+  assert(oop->mark().is_inline_type(), \"Expected inline type\");\n+  return oop;\n+}\n+\n+instanceOop InlineKlass::allocate_instance_buffer(TRAPS) {\n+  int size = size_helper();  \/\/ Query before forming handle.\n+\n+  instanceOop oop = (instanceOop)Universe::heap()->obj_buffer_allocate(this, size, CHECK_NULL);\n+  assert(oop->mark().is_inline_type(), \"Expected inline type\");\n+  return oop;\n+}\n+\n+int InlineKlass::nonstatic_oop_count() {\n+  int oops = 0;\n+  int map_count = nonstatic_oop_map_count();\n+  OopMapBlock* block = start_of_nonstatic_oop_maps();\n+  OopMapBlock* end = block + map_count;\n+  while (block != end) {\n+    oops += block->count();\n+    block++;\n+  }\n+  return oops;\n+}\n+\n+int InlineKlass::layout_size_in_bytes(LayoutKind kind) const {\n+  switch(kind) {\n+    case LayoutKind::NON_ATOMIC_FLAT:\n+      assert(has_non_atomic_layout(), \"Layout not available\");\n+      return non_atomic_size_in_bytes();\n+      break;\n+    case LayoutKind::ATOMIC_FLAT:\n+      assert(has_atomic_layout(), \"Layout not available\");\n+      return atomic_size_in_bytes();\n+      break;\n+    case LayoutKind::NULLABLE_ATOMIC_FLAT:\n+      assert(has_nullable_atomic_layout(), \"Layout not available\");\n+      return nullable_atomic_size_in_bytes();\n+      break;\n+    case BUFFERED:\n+      return payload_size_in_bytes();\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+int InlineKlass::layout_alignment(LayoutKind kind) const {\n+  switch(kind) {\n+    case LayoutKind::NON_ATOMIC_FLAT:\n+      assert(has_non_atomic_layout(), \"Layout not available\");\n+      return non_atomic_alignment();\n+      break;\n+    case LayoutKind::ATOMIC_FLAT:\n+      assert(has_atomic_layout(), \"Layout not available\");\n+      return atomic_size_in_bytes();\n+      break;\n+    case LayoutKind::NULLABLE_ATOMIC_FLAT:\n+      assert(has_nullable_atomic_layout(), \"Layout not available\");\n+      return nullable_atomic_size_in_bytes();\n+      break;\n+    case LayoutKind::BUFFERED:\n+      return payload_alignment();\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+bool InlineKlass::is_layout_supported(LayoutKind lk) {\n+  switch(lk) {\n+    case LayoutKind::NON_ATOMIC_FLAT:\n+      return has_non_atomic_layout();\n+      break;\n+    case LayoutKind::ATOMIC_FLAT:\n+      return has_atomic_layout();\n+      break;\n+    case LayoutKind::NULLABLE_ATOMIC_FLAT:\n+      return has_nullable_atomic_layout();\n+      break;\n+    case LayoutKind::BUFFERED:\n+      return true;\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void InlineKlass::copy_payload_to_addr(void* src, void* dst, LayoutKind lk, bool dest_is_initialized) {\n+  assert(is_layout_supported(lk), \"Unsupported layout\");\n+  assert(lk != LayoutKind::REFERENCE && lk != LayoutKind::UNKNOWN, \"Sanity check\");\n+  switch(lk) {\n+    case NULLABLE_ATOMIC_FLAT: {\n+    if (is_payload_marked_as_null((address)src)) {\n+        if (!contains_oops()) {\n+          mark_payload_as_null((address)dst);\n+          return;\n+        }\n+        \/\/ copy null_reset value to dest\n+        if (dest_is_initialized) {\n+          HeapAccess<>::value_copy(payload_addr(null_reset_value()), dst, this, lk);\n+        } else {\n+          HeapAccess<IS_DEST_UNINITIALIZED>::value_copy(payload_addr(null_reset_value()), dst, this, lk);\n+        }\n+      } else {\n+        \/\/ Copy has to be performed, even if this is an empty value, because of the null marker\n+        mark_payload_as_non_null((address)src);\n+        if (dest_is_initialized) {\n+          HeapAccess<>::value_copy(src, dst, this, lk);\n+        } else {\n+          HeapAccess<IS_DEST_UNINITIALIZED>::value_copy(src, dst, this, lk);\n+        }\n+      }\n+    }\n+    break;\n+    case BUFFERED:\n+    case ATOMIC_FLAT:\n+    case NON_ATOMIC_FLAT: {\n+      if (is_empty_inline_type()) return; \/\/ nothing to do\n+      if (dest_is_initialized) {\n+        HeapAccess<>::value_copy(src, dst, this, lk);\n+      } else {\n+        HeapAccess<IS_DEST_UNINITIALIZED>::value_copy(src, dst, this, lk);\n+      }\n+    }\n+    break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+oop InlineKlass::read_payload_from_addr(oop src, int offset, LayoutKind lk, TRAPS) {\n+  assert(src != nullptr, \"Must be\");\n+  assert(is_layout_supported(lk), \"Unsupported layout\");\n+  switch(lk) {\n+    case NULLABLE_ATOMIC_FLAT: {\n+      if (is_payload_marked_as_null((address)((char*)(oopDesc*)src + offset))) {\n+        return nullptr;\n+      }\n+    } \/\/ Fallthrough\n+    case BUFFERED:\n+    case ATOMIC_FLAT:\n+    case NON_ATOMIC_FLAT: {\n+      if (is_empty_inline_type()) {\n+        return default_value();\n+      }\n+      Handle obj_h(THREAD, src);\n+      oop res = allocate_instance_buffer(CHECK_NULL);\n+      copy_payload_to_addr((void*)((char*)(oopDesc*)obj_h() + offset), payload_addr(res), lk, false);\n+      if (lk == NULLABLE_ATOMIC_FLAT) {\n+        if(is_payload_marked_as_null(payload_addr(res))) {\n+          return nullptr;\n+        }\n+      }\n+      return res;\n+    }\n+    break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void InlineKlass::write_value_to_addr(oop src, void* dst, LayoutKind lk, bool dest_is_initialized, TRAPS) {\n+  void* src_addr = nullptr;\n+  if (src == nullptr) {\n+    if (lk != NULLABLE_ATOMIC_FLAT) {\n+      THROW_MSG(vmSymbols::java_lang_NullPointerException(), \"Value is null\");\n+    }\n+    \/\/ Writing null to a nullable flat field\/element is usually done by writing\n+    \/\/ the whole pre-allocated null_reset_value at the payload address to ensure\n+    \/\/ that the null marker and all potential oops are reset to \"zeros\".\n+    \/\/ However, the null_reset_value is allocated during class initialization.\n+    \/\/ If the current value of the field is null, it is possible that the class\n+    \/\/ of the field has not been initialized yet and thus the null_reset_value\n+    \/\/ might not be available yet.\n+    \/\/ Writing null over an already null value should not trigger class initialization.\n+    \/\/ The solution is to detect null being written over null cases and return immediately\n+    \/\/ (writing null over null is a no-op from a field modification point of view)\n+    if (is_payload_marked_as_null((address)dst)) return;\n+    src_addr = payload_addr(null_reset_value());\n+  } else {\n+    src_addr = payload_addr(src);\n+    if (lk == NULLABLE_ATOMIC_FLAT) {\n+      mark_payload_as_non_null((address)src_addr);\n+    }\n+  }\n+  copy_payload_to_addr(src_addr, dst, lk, dest_is_initialized);\n+}\n+\n+\/\/ Arrays of...\n+\n+bool InlineKlass::flat_array() {\n+  if (!UseArrayFlattening) {\n+    return false;\n+  }\n+  \/\/ Too many embedded oops\n+  if ((FlatArrayElementMaxOops >= 0) && (nonstatic_oop_count() > FlatArrayElementMaxOops)) {\n+    return false;\n+  }\n+  \/\/ Declared atomic but not naturally atomic.\n+  if (must_be_atomic() && !is_naturally_atomic()) {\n+    return false;\n+  }\n+  \/\/ VM enforcing AlwaysAtomicAccess only...\n+  if (AlwaysAtomicAccesses && (!is_naturally_atomic())) {\n+    return false;\n+  }\n+  return true;\n+}\n+\n+ObjArrayKlass* InlineKlass::null_free_reference_array(TRAPS) {\n+  if (Atomic::load_acquire(adr_null_free_reference_array_klass()) == nullptr) {\n+    \/\/ Atomic creation of array_klasses\n+    RecursiveLocker rl(MultiArray_lock, THREAD);\n+\n+    \/\/ Check if update has already taken place\n+    if (null_free_reference_array_klass() == nullptr) {\n+      ObjArrayKlass* k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this, true, CHECK_NULL);\n+\n+      \/\/ use 'release' to pair with lock-free load\n+      Atomic::release_store(adr_null_free_reference_array_klass(), k);\n+    }\n+  }\n+  return null_free_reference_array_klass();\n+}\n+\n+\n+\/\/ There's no reason for this method to have a TRAP argument\n+FlatArrayKlass* InlineKlass::flat_array_klass(LayoutKind lk, TRAPS) {\n+  FlatArrayKlass* volatile* adr_flat_array_klass = nullptr;\n+  switch(lk) {\n+    case NON_ATOMIC_FLAT:\n+      assert(has_non_atomic_layout(), \"Must be\");\n+      adr_flat_array_klass = adr_non_atomic_flat_array_klass();\n+      break;\n+    case ATOMIC_FLAT:\n+    assert(has_atomic_layout(), \"Must be\");\n+      adr_flat_array_klass = adr_atomic_flat_array_klass();\n+      break;\n+    case NULLABLE_ATOMIC_FLAT:\n+      assert(has_nullable_atomic_layout(), \"Must be\");\n+      adr_flat_array_klass = adr_nullable_atomic_flat_array_klass();\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+\n+  if (Atomic::load_acquire(adr_flat_array_klass) == nullptr) {\n+    \/\/ Atomic creation of array_klasses\n+    RecursiveLocker rl(MultiArray_lock, THREAD);\n+\n+    if (*adr_flat_array_klass == nullptr) {\n+      FlatArrayKlass* k = FlatArrayKlass::allocate_klass(this, lk, CHECK_NULL);\n+      Atomic::release_store(adr_flat_array_klass, k);\n+    }\n+  }\n+  return *adr_flat_array_klass;\n+}\n+\n+FlatArrayKlass* InlineKlass::flat_array_klass_or_null(LayoutKind lk) {\n+    FlatArrayKlass* volatile* adr_flat_array_klass = nullptr;\n+  switch(lk) {\n+    case NON_ATOMIC_FLAT:\n+      assert(has_non_atomic_layout(), \"Must be\");\n+      adr_flat_array_klass = adr_non_atomic_flat_array_klass();\n+      break;\n+    case ATOMIC_FLAT:\n+    assert(has_atomic_layout(), \"Must be\");\n+      adr_flat_array_klass = adr_atomic_flat_array_klass();\n+      break;\n+    case NULLABLE_ATOMIC_FLAT:\n+      assert(has_nullable_atomic_layout(), \"Must be\");\n+      adr_flat_array_klass = adr_nullable_atomic_flat_array_klass();\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+\n+  \/\/ Need load-acquire for lock-free read\n+  FlatArrayKlass* k = Atomic::load_acquire(adr_flat_array_klass);\n+  return k;\n+}\n+\n+\/\/ Inline type arguments are not passed by reference, instead each\n+\/\/ field of the inline type is passed as an argument. This helper\n+\/\/ function collects the flat field (recursively)\n+\/\/ in a list. Included with the field's type is\n+\/\/ the offset of each field in the inline type: i2c and c2i adapters\n+\/\/ need that to load or store fields. Finally, the list of fields is\n+\/\/ sorted in order of increasing offsets: the adapters and the\n+\/\/ compiled code need to agree upon the order of fields.\n+\/\/\n+\/\/ The list of basic types that is returned starts with a T_METADATA\n+\/\/ and ends with an extra T_VOID. T_METADATA\/T_VOID pairs are used as\n+\/\/ delimiters. Every entry between the two is a field of the inline\n+\/\/ type. If there's an embedded inline type in the list, it also starts\n+\/\/ with a T_METADATA and ends with a T_VOID. This is so we can\n+\/\/ generate a unique fingerprint for the method's adapters and we can\n+\/\/ generate the list of basic types from the interpreter point of view\n+\/\/ (inline types passed as reference: iterate on the list until a\n+\/\/ T_METADATA, drop everything until and including the closing\n+\/\/ T_VOID) or the compiler point of view (each field of the inline\n+\/\/ types is an argument: drop all T_METADATA\/T_VOID from the list).\n+\/\/\n+\/\/ Value classes could also have fields in abstract super value classes.\n+\/\/ Use a HierarchicalFieldStream to get them as well.\n+int InlineKlass::collect_fields(GrowableArray<SigEntry>* sig, float& max_offset, int base_off, int null_marker_offset) {\n+  int count = 0;\n+  SigEntry::add_entry(sig, T_METADATA, name(), base_off);\n+  max_offset = base_off;\n+  for (HierarchicalFieldStream<JavaFieldStream> fs(this); !fs.done(); fs.next()) {\n+    if (fs.access_flags().is_static()) continue;\n+    int offset = base_off + fs.offset() - (base_off > 0 ? payload_offset() : 0);\n+    \/\/ TODO 8284443 Use different heuristic to decide what should be scalarized in the calling convention\n+    if (fs.is_flat()) {\n+      \/\/ Resolve klass of flat field and recursively collect fields\n+      int field_null_marker_offset = -1;\n+      if (!fs.is_null_free_inline_type()) {\n+        field_null_marker_offset = base_off + fs.null_marker_offset() - (base_off > 0 ? payload_offset() : 0);\n+      }\n+      Klass* vk = get_inline_type_field_klass(fs.index());\n+      count += InlineKlass::cast(vk)->collect_fields(sig, max_offset, offset, field_null_marker_offset);\n+    } else {\n+      BasicType bt = Signature::basic_type(fs.signature());\n+      SigEntry::add_entry(sig, bt, fs.signature(), offset);\n+      count += type2size[bt];\n+    }\n+    if (fs.field_descriptor().field_holder() != this) {\n+      \/\/ Inherited field, add an empty wrapper to this to distinguish it from a \"local\" field\n+      \/\/ with a different offset and avoid false adapter sharing. TODO 8348547 Is this sufficient?\n+      SigEntry::add_entry(sig, T_METADATA, name(), base_off);\n+      SigEntry::add_entry(sig, T_VOID, name(), offset);\n+    }\n+    max_offset = MAX2(max_offset, (float)offset);\n+  }\n+  int offset = base_off + size_helper()*HeapWordSize - (base_off > 0 ? payload_offset() : 0);\n+  \/\/ Null markers are no real fields, add them manually at the end (C2 relies on this) of the flat fields\n+  if (null_marker_offset != -1) {\n+    max_offset += 0.1f; \/\/ We add the markers \"in-between\" because they are no real fields\n+    SigEntry::add_entry(sig, T_BOOLEAN, name(), null_marker_offset, max_offset);\n+    count++;\n+  }\n+  SigEntry::add_entry(sig, T_VOID, name(), offset);\n+  if (base_off == 0) {\n+    sig->sort(SigEntry::compare);\n+  }\n+  assert(sig->at(0)._bt == T_METADATA && sig->at(sig->length()-1)._bt == T_VOID, \"broken structure\");\n+  return count;\n+}\n+\n+void InlineKlass::initialize_calling_convention(TRAPS) {\n+  \/\/ Because the pack and unpack handler addresses need to be loadable from generated code,\n+  \/\/ they are stored at a fixed offset in the klass metadata. Since inline type klasses do\n+  \/\/ not have a vtable, the vtable offset is used to store these addresses.\n+  if (InlineTypeReturnedAsFields || InlineTypePassFieldsAsArgs) {\n+    ResourceMark rm;\n+    GrowableArray<SigEntry> sig_vk;\n+    float max_offset = 0;\n+    int nb_fields = collect_fields(&sig_vk, max_offset);\n+    Array<SigEntry>* extended_sig = MetadataFactory::new_array<SigEntry>(class_loader_data(), sig_vk.length(), CHECK);\n+    *((Array<SigEntry>**)adr_extended_sig()) = extended_sig;\n+    for (int i = 0; i < sig_vk.length(); i++) {\n+      extended_sig->at_put(i, sig_vk.at(i));\n+    }\n+    if (can_be_returned_as_fields(\/* init= *\/ true)) {\n+      nb_fields++;\n+      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, nb_fields);\n+      sig_bt[0] = T_METADATA;\n+      SigEntry::fill_sig_bt(&sig_vk, sig_bt+1);\n+      VMRegPair* regs = NEW_RESOURCE_ARRAY(VMRegPair, nb_fields);\n+      int total = SharedRuntime::java_return_convention(sig_bt, regs, nb_fields);\n+\n+      if (total > 0) {\n+        Array<VMRegPair>* return_regs = MetadataFactory::new_array<VMRegPair>(class_loader_data(), nb_fields, CHECK);\n+        *((Array<VMRegPair>**)adr_return_regs()) = return_regs;\n+        for (int i = 0; i < nb_fields; i++) {\n+          return_regs->at_put(i, regs[i]);\n+        }\n+\n+        BufferedInlineTypeBlob* buffered_blob = SharedRuntime::generate_buffered_inline_type_adapter(this);\n+        *((address*)adr_pack_handler()) = buffered_blob->pack_fields();\n+        *((address*)adr_pack_handler_jobject()) = buffered_blob->pack_fields_jobject();\n+        *((address*)adr_unpack_handler()) = buffered_blob->unpack_fields();\n+        assert(CodeCache::find_blob(pack_handler()) == buffered_blob, \"lost track of blob\");\n+        assert(can_be_returned_as_fields(), \"sanity\");\n+      }\n+    }\n+    if (!can_be_returned_as_fields() && !can_be_passed_as_fields()) {\n+      MetadataFactory::free_array<SigEntry>(class_loader_data(), extended_sig);\n+      assert(return_regs() == nullptr, \"sanity\");\n+    }\n+  }\n+}\n+\n+void InlineKlass::deallocate_contents(ClassLoaderData* loader_data) {\n+  if (extended_sig() != nullptr) {\n+    MetadataFactory::free_array<SigEntry>(loader_data, extended_sig());\n+    *((Array<SigEntry>**)adr_extended_sig()) = nullptr;\n+  }\n+  if (return_regs() != nullptr) {\n+    MetadataFactory::free_array<VMRegPair>(loader_data, return_regs());\n+    *((Array<VMRegPair>**)adr_return_regs()) = nullptr;\n+  }\n+  cleanup_blobs();\n+  InstanceKlass::deallocate_contents(loader_data);\n+}\n+\n+void InlineKlass::cleanup(InlineKlass* ik) {\n+  ik->cleanup_blobs();\n+}\n+\n+void InlineKlass::cleanup_blobs() {\n+  if (pack_handler() != nullptr) {\n+    CodeBlob* buffered_blob = CodeCache::find_blob(pack_handler());\n+    assert(buffered_blob->is_buffered_inline_type_blob(), \"bad blob type\");\n+    BufferBlob::free((BufferBlob*)buffered_blob);\n+    *((address*)adr_pack_handler()) = nullptr;\n+    *((address*)adr_pack_handler_jobject()) = nullptr;\n+    *((address*)adr_unpack_handler()) = nullptr;\n+  }\n+}\n+\n+\/\/ Can this inline type be passed as multiple values?\n+bool InlineKlass::can_be_passed_as_fields() const {\n+  return InlineTypePassFieldsAsArgs;\n+}\n+\n+\/\/ Can this inline type be returned as multiple values?\n+bool InlineKlass::can_be_returned_as_fields(bool init) const {\n+  return InlineTypeReturnedAsFields && (init || return_regs() != nullptr);\n+}\n+\n+\/\/ Create handles for all oop fields returned in registers that are going to be live across a safepoint\n+void InlineKlass::save_oop_fields(const RegisterMap& reg_map, GrowableArray<Handle>& handles) const {\n+  Thread* thread = Thread::current();\n+  const Array<SigEntry>* sig_vk = extended_sig();\n+  const Array<VMRegPair>* regs = return_regs();\n+  int j = 1;\n+\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_OBJECT || bt == T_ARRAY) {\n+      VMRegPair pair = regs->at(j);\n+      address loc = reg_map.location(pair.first(), nullptr);\n+      oop v = *(oop*)loc;\n+      assert(v == nullptr || oopDesc::is_oop(v), \"not an oop?\");\n+      assert(Universe::heap()->is_in_or_null(v), \"must be heap pointer\");\n+      handles.push(Handle(thread, v));\n+    }\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID &&\n+        sig_vk->at(i-1)._bt != T_LONG &&\n+        sig_vk->at(i-1)._bt != T_DOUBLE) {\n+      continue;\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+}\n+\n+\/\/ Update oop fields in registers from handles after a safepoint\n+void InlineKlass::restore_oop_results(RegisterMap& reg_map, GrowableArray<Handle>& handles) const {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  const Array<SigEntry>* sig_vk = extended_sig();\n+  const Array<VMRegPair>* regs = return_regs();\n+  assert(regs != nullptr, \"inconsistent\");\n+\n+  int j = 1;\n+  for (int i = 0, k = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_OBJECT || bt == T_ARRAY) {\n+      VMRegPair pair = regs->at(j);\n+      address loc = reg_map.location(pair.first(), nullptr);\n+      *(oop*)loc = handles.at(k++)();\n+    }\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID &&\n+        sig_vk->at(i-1)._bt != T_LONG &&\n+        sig_vk->at(i-1)._bt != T_DOUBLE) {\n+      continue;\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+}\n+\n+\/\/ Fields are in registers. Create an instance of the inline type and\n+\/\/ initialize it with the values of the fields.\n+oop InlineKlass::realloc_result(const RegisterMap& reg_map, const GrowableArray<Handle>& handles, TRAPS) {\n+  oop new_vt = allocate_instance(CHECK_NULL);\n+  const Array<SigEntry>* sig_vk = extended_sig();\n+  const Array<VMRegPair>* regs = return_regs();\n+\n+  int j = 1;\n+  int k = 0;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    address loc = reg_map.location(pair.first(), nullptr);\n+    switch(bt) {\n+    case T_BOOLEAN: {\n+      new_vt->bool_field_put(off, *(jboolean*)loc);\n+      break;\n+    }\n+    case T_CHAR: {\n+      new_vt->char_field_put(off, *(jchar*)loc);\n+      break;\n+    }\n+    case T_BYTE: {\n+      new_vt->byte_field_put(off, *(jbyte*)loc);\n+      break;\n+    }\n+    case T_SHORT: {\n+      new_vt->short_field_put(off, *(jshort*)loc);\n+      break;\n+    }\n+    case T_INT: {\n+      new_vt->int_field_put(off, *(jint*)loc);\n+      break;\n+    }\n+    case T_LONG: {\n+#ifdef _LP64\n+      new_vt->double_field_put(off,  *(jdouble*)loc);\n+#else\n+      Unimplemented();\n+#endif\n+      break;\n+    }\n+    case T_OBJECT:\n+    case T_ARRAY: {\n+      Handle handle = handles.at(k++);\n+      new_vt->obj_field_put(off, handle());\n+      break;\n+    }\n+    case T_FLOAT: {\n+      new_vt->float_field_put(off,  *(jfloat*)loc);\n+      break;\n+    }\n+    case T_DOUBLE: {\n+      new_vt->double_field_put(off, *(jdouble*)loc);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+    }\n+    *(intptr_t*)loc = 0xDEAD;\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+  assert(k == handles.length(), \"missed an oop?\");\n+  return new_vt;\n+}\n+\n+\/\/ Check the return register for an InlineKlass oop\n+InlineKlass* InlineKlass::returned_inline_klass(const RegisterMap& map) {\n+  BasicType bt = T_METADATA;\n+  VMRegPair pair;\n+  int nb = SharedRuntime::java_return_convention(&bt, &pair, 1);\n+  assert(nb == 1, \"broken\");\n+\n+  address loc = map.location(pair.first(), nullptr);\n+  intptr_t ptr = *(intptr_t*)loc;\n+  if (is_set_nth_bit(ptr, 0)) {\n+    \/\/ Return value is tagged, must be an InlineKlass pointer\n+    clear_nth_bit(ptr, 0);\n+    assert(Metaspace::contains((void*)ptr), \"should be klass\");\n+    InlineKlass* vk = (InlineKlass*)ptr;\n+    assert(vk->can_be_returned_as_fields(), \"must be able to return as fields\");\n+    return vk;\n+  }\n+  \/\/ Return value is not tagged, must be a valid oop\n+  assert(oopDesc::is_oop_or_null(cast_to_oop(ptr), true),\n+         \"Bad oop return: \" PTR_FORMAT, ptr);\n+  return nullptr;\n+}\n+\n+\/\/ CDS support\n+\n+void InlineKlass::metaspace_pointers_do(MetaspaceClosure* it) {\n+  InstanceKlass::metaspace_pointers_do(it);\n+\n+  InlineKlass* this_ptr = this;\n+  it->push((Klass**)adr_non_atomic_flat_array_klass());\n+  it->push((Klass**)adr_atomic_flat_array_klass());\n+  it->push((Klass**)adr_nullable_atomic_flat_array_klass());\n+  it->push((Klass**)adr_null_free_reference_array_klass());\n+}\n+\n+void InlineKlass::remove_unshareable_info() {\n+  InstanceKlass::remove_unshareable_info();\n+\n+  \/\/ update it to point to the \"buffered\" copy of this class.\n+  _adr_inlineklass_fixed_block = inlineklass_static_block();\n+  ArchivePtrMarker::mark_pointer((address*)&_adr_inlineklass_fixed_block);\n+\n+  *((Array<SigEntry>**)adr_extended_sig()) = nullptr;\n+  *((Array<VMRegPair>**)adr_return_regs()) = nullptr;\n+  *((address*)adr_pack_handler()) = nullptr;\n+  *((address*)adr_pack_handler_jobject()) = nullptr;\n+  *((address*)adr_unpack_handler()) = nullptr;\n+  assert(pack_handler() == nullptr, \"pack handler not null\");\n+  if (non_atomic_flat_array_klass() != nullptr) {\n+    non_atomic_flat_array_klass()->remove_unshareable_info();\n+  }\n+  if (atomic_flat_array_klass() != nullptr) {\n+    atomic_flat_array_klass()->remove_unshareable_info();\n+  }\n+  if (nullable_atomic_flat_array_klass() != nullptr) {\n+    nullable_atomic_flat_array_klass()->remove_unshareable_info();\n+  }\n+  if (null_free_reference_array_klass() != nullptr) {\n+    null_free_reference_array_klass()->remove_unshareable_info();\n+  }\n+}\n+\n+void InlineKlass::remove_java_mirror() {\n+  InstanceKlass::remove_java_mirror();\n+  if (non_atomic_flat_array_klass() != nullptr) {\n+    non_atomic_flat_array_klass()->remove_java_mirror();\n+  }\n+  if (atomic_flat_array_klass() != nullptr) {\n+    atomic_flat_array_klass()->remove_java_mirror();\n+  }\n+  if (nullable_atomic_flat_array_klass() != nullptr) {\n+    nullable_atomic_flat_array_klass()->remove_java_mirror();\n+  }\n+  if (null_free_reference_array_klass() != nullptr) {\n+    null_free_reference_array_klass()->remove_java_mirror();\n+  }\n+}\n+\n+void InlineKlass::restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS) {\n+  InstanceKlass::restore_unshareable_info(loader_data, protection_domain, pkg_entry, CHECK);\n+  if (non_atomic_flat_array_klass() != nullptr) {\n+    non_atomic_flat_array_klass()->restore_unshareable_info(ClassLoaderData::the_null_class_loader_data(), Handle(), CHECK);\n+  }\n+  if (atomic_flat_array_klass() != nullptr) {\n+    atomic_flat_array_klass()->restore_unshareable_info(ClassLoaderData::the_null_class_loader_data(), Handle(), CHECK);\n+  }\n+  if (nullable_atomic_flat_array_klass() != nullptr) {\n+    nullable_atomic_flat_array_klass()->restore_unshareable_info(ClassLoaderData::the_null_class_loader_data(), Handle(), CHECK);\n+  }\n+  if (null_free_reference_array_klass() != nullptr) {\n+    null_free_reference_array_klass()->restore_unshareable_info(ClassLoaderData::the_null_class_loader_data(), Handle(), CHECK);\n+  }\n+}\n+\n+\/\/ oop verify\n+\n+void InlineKlass::verify_on(outputStream* st) {\n+  InstanceKlass::verify_on(st);\n+  guarantee(prototype_header().is_inline_type(), \"Prototype header is not inline type\");\n+}\n+\n+void InlineKlass::oop_verify_on(oop obj, outputStream* st) {\n+  InstanceKlass::oop_verify_on(obj, st);\n+  guarantee(obj->mark().is_inline_type(), \"Header is not inline type\");\n+}\n","filename":"src\/hotspot\/share\/oops\/inlineKlass.cpp","additions":782,"deletions":0,"binary":false,"changes":782,"status":"added"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"oops\/instanceKlass.inline.hpp\"\n","filename":"src\/hotspot\/share\/oops\/instanceClassLoaderKlass.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -76,0 +76,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -153,0 +154,5 @@\n+void InlineLayoutInfo::metaspace_pointers_do(MetaspaceClosure* it) {\n+  log_trace(cds)(\"Iter(InlineFieldInfo): %p\", this);\n+  it->push(&_klass);\n+}\n+\n@@ -174,0 +180,13 @@\n+bool InstanceKlass::field_is_null_free_inline_type(int index) const {\n+  return field(index).field_flags().is_null_free_inline_type();\n+}\n+\n+bool InstanceKlass::is_class_in_loadable_descriptors_attribute(Symbol* name) const {\n+  if (_loadable_descriptors == nullptr) return false;\n+  for (int i = 0; i < _loadable_descriptors->length(); i++) {\n+        Symbol* class_name = _constants->symbol_at(_loadable_descriptors->at(i));\n+        if (class_name == name) return true;\n+  }\n+  return false;\n+}\n+\n@@ -468,1 +487,2 @@\n-                                       parser.is_interface());\n+                                       parser.is_interface(),\n+                                       parser.is_inline_type());\n@@ -491,0 +511,3 @@\n+  } else if (parser.is_inline_type()) {\n+    \/\/ inline type\n+    ik = new (loader_data, size, use_class_space, THREAD) InlineKlass(parser);\n@@ -507,0 +530,6 @@\n+#ifdef ASSERT\n+  ik->bounds_check((address) ik->start_of_vtable(), false, size);\n+  ik->bounds_check((address) ik->start_of_itable(), false, size);\n+  ik->bounds_check((address) ik->end_of_itable(), true, size);\n+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);\n+#endif \/\/ASSERT\n@@ -510,0 +539,23 @@\n+#ifndef PRODUCT\n+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {\n+  const char* bad = nullptr;\n+  address end = nullptr;\n+  if (addr < (address)this) {\n+    bad = \"before\";\n+  } else if (addr == (address)this) {\n+    if (edge_ok)  return true;\n+    bad = \"just before\";\n+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {\n+    if (edge_ok)  return true;\n+    bad = \"just after\";\n+  } else if (addr > end) {\n+    bad = \"after\";\n+  } else {\n+    return true;\n+  }\n+  tty->print_cr(\"%s object bounds: \" INTPTR_FORMAT \" [\" INTPTR_FORMAT \"..\" INTPTR_FORMAT \"]\",\n+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);\n+  Verbose = WizardMode = true; this->print(); \/\/@@\n+  return false;\n+}\n+#endif \/\/PRODUCT\n@@ -549,1 +601,4 @@\n-  _init_thread(nullptr)\n+  _init_thread(nullptr),\n+  _inline_layout_info_array(nullptr),\n+  _loadable_descriptors(nullptr),\n+  _adr_inlineklass_fixed_block(nullptr)\n@@ -556,0 +611,3 @@\n+  if (parser.has_inline_fields()) {\n+    set_has_inline_type_fields();\n+  }\n@@ -695,0 +753,5 @@\n+  if (inline_layout_info_array() != nullptr) {\n+    MetadataFactory::free_array<InlineLayoutInfo>(loader_data, inline_layout_info_array());\n+  }\n+  set_inline_layout_info_array(nullptr);\n+\n@@ -729,0 +792,7 @@\n+  if (loadable_descriptors() != nullptr &&\n+      loadable_descriptors() != Universe::the_empty_short_array() &&\n+      !loadable_descriptors()->is_shared()) {\n+    MetadataFactory::free_array<jushort>(loader_data, loadable_descriptors());\n+  }\n+  set_loadable_descriptors(nullptr);\n+\n@@ -963,0 +1033,105 @@\n+\n+  \/\/ If a class declares a method that uses an inline class as an argument\n+  \/\/ type or return inline type, this inline class must be loaded during the\n+  \/\/ linking of this class because size and properties of the inline class\n+  \/\/ must be known in order to be able to perform inline type optimizations.\n+  \/\/ The implementation below is an approximation of this rule, the code\n+  \/\/ iterates over all methods of the current class (including overridden\n+  \/\/ methods), not only the methods declared by this class. This\n+  \/\/ approximation makes the code simpler, and doesn't change the semantic\n+  \/\/ because classes declaring methods overridden by the current class are\n+  \/\/ linked (and have performed their own pre-loading) before the linking\n+  \/\/ of the current class.\n+\n+\n+  \/\/ Note:\n+  \/\/ Inline class types are loaded during\n+  \/\/ the loading phase (see ClassFileParser::post_process_parsed_stream()).\n+  \/\/ Inline class types used as element types for array creation\n+  \/\/ are not pre-loaded. Their loading is triggered by either anewarray\n+  \/\/ or multianewarray bytecodes.\n+\n+  \/\/ Could it be possible to do the following processing only if the\n+  \/\/ class uses inline types?\n+  if (EnableValhalla) {\n+    ResourceMark rm(THREAD);\n+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {\n+      if (fs.is_null_free_inline_type() && fs.access_flags().is_static()) {\n+        Symbol* sig = fs.signature();\n+        TempNewSymbol s = Signature::strip_envelope(sig);\n+        if (s != name()) {\n+          log_info(class, preload)(\"Preloading class %s during linking of class %s. Cause: a null-free static field is declared with this type\", s->as_C_string(), name()->as_C_string());\n+          Klass* klass = SystemDictionary::resolve_or_fail(s,\n+                                                          Handle(THREAD, class_loader()), true,\n+                                                          CHECK_false);\n+          if (HAS_PENDING_EXCEPTION) {\n+            log_warning(class, preload)(\"Preloading of class %s during linking of class %s (cause: null-free static field) failed: %s\",\n+                                      s->as_C_string(), name()->as_C_string(), PENDING_EXCEPTION->klass()->name()->as_C_string());\n+            return false; \/\/ Exception is still pending\n+          }\n+          log_info(class, preload)(\"Preloading of class %s during linking of class %s (cause: null-free static field) succeeded\",\n+                                   s->as_C_string(), name()->as_C_string());\n+          assert(klass != nullptr, \"Sanity check\");\n+          if (klass->is_abstract()) {\n+            THROW_MSG_(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                      err_msg(\"Class %s expects class %s to be concrete value class, but it is an abstract class\",\n+                      name()->as_C_string(),\n+                      InstanceKlass::cast(klass)->external_name()), false);\n+          }\n+          if (!klass->is_inline_klass()) {\n+            THROW_MSG_(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                       err_msg(\"class %s expects class %s to be a value class but it is an identity class\",\n+                       name()->as_C_string(), klass->external_name()), false);\n+          }\n+          InlineKlass* vk = InlineKlass::cast(klass);\n+          if (!vk->is_implicitly_constructible()) {\n+             THROW_MSG_(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                        err_msg(\"class %s is not implicitly constructible and it is used in a null restricted static field (not supported)\",\n+                        klass->external_name()), false);\n+          }\n+          \/\/ the inline_type_field_klasses_array might have been loaded with CDS, so update only if not already set and check consistency\n+          InlineLayoutInfo* li = inline_layout_info_adr(fs.index());\n+          if (li->klass() == nullptr) {\n+            li->set_klass(InlineKlass::cast(vk));\n+            li->set_kind(LayoutKind::REFERENCE);\n+          }\n+          assert(get_inline_type_field_klass(fs.index()) == vk, \"Must match\");\n+        } else {\n+          InlineLayoutInfo* li = inline_layout_info_adr(fs.index());\n+          if (li->klass() == nullptr) {\n+            li->set_klass(InlineKlass::cast(this));\n+            li->set_kind(LayoutKind::REFERENCE);\n+          }\n+          assert(get_inline_type_field_klass(fs.index()) == this, \"Must match\");\n+        }\n+      }\n+    }\n+\n+    \/\/ Aggressively preloading all classes from the LoadableDescriptors attribute\n+    if (loadable_descriptors() != nullptr) {\n+      HandleMark hm(THREAD);\n+      for (int i = 0; i < loadable_descriptors()->length(); i++) {\n+        Symbol* sig = constants()->symbol_at(loadable_descriptors()->at(i));\n+        if (!Signature::has_envelope(sig)) continue;\n+        TempNewSymbol class_name = Signature::strip_envelope(sig);\n+        if (class_name == name()) continue;\n+        log_info(class, preload)(\"Preloading class %s during linking of class %s because of the class is listed in the LoadableDescriptors attribute\", sig->as_C_string(), name()->as_C_string());\n+        oop loader = class_loader();\n+        Klass* klass = SystemDictionary::resolve_or_null(class_name,\n+                                                         Handle(THREAD, loader), THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+          CLEAR_PENDING_EXCEPTION;\n+        }\n+        if (klass != nullptr) {\n+          log_info(class, preload)(\"Preloading of class %s during linking of class %s (cause: LoadableDescriptors attribute) succeeded\", class_name->as_C_string(), name()->as_C_string());\n+          if (!klass->is_inline_klass()) {\n+            \/\/ Non value class are allowed by the current spec, but it could be an indication of an issue so let's log a warning\n+              log_warning(class, preload)(\"Preloading class %s during linking of class %s (cause: LoadableDescriptors attribute) but loaded class is not a value class\", class_name->as_C_string(), name()->as_C_string());\n+          }\n+        } else {\n+          log_warning(class, preload)(\"Preloading of class %s during linking of class %s (cause: LoadableDescriptors attribute) failed\", class_name->as_C_string(), name()->as_C_string());\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1267,0 +1442,35 @@\n+  \/\/ Pre-allocating an instance of the default value\n+  if (is_inline_klass()) {\n+      InlineKlass* vk = InlineKlass::cast(this);\n+      oop val = vk->allocate_instance(THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          {\n+              EXCEPTION_MARK;\n+              add_initialization_error(THREAD, e);\n+              \/\/ Locks object, set state, and notify all waiting threads\n+              set_initialization_state_and_notify(initialization_error, THREAD);\n+              CLEAR_PENDING_EXCEPTION;\n+          }\n+          THROW_OOP(e());\n+      }\n+      vk->set_default_value(val);\n+      if (vk->has_nullable_atomic_layout()) {\n+        val = vk->allocate_instance(THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+            Handle e(THREAD, PENDING_EXCEPTION);\n+            CLEAR_PENDING_EXCEPTION;\n+            {\n+                EXCEPTION_MARK;\n+                add_initialization_error(THREAD, e);\n+                \/\/ Locks object, set state, and notify all waiting threads\n+                set_initialization_state_and_notify(initialization_error, THREAD);\n+                CLEAR_PENDING_EXCEPTION;\n+            }\n+            THROW_OOP(e());\n+        }\n+        vk->set_null_reset_value(val);\n+      }\n+  }\n+\n@@ -1299,1 +1509,33 @@\n-\n+  \/\/ Initialize classes of inline fields\n+  if (EnableValhalla) {\n+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {\n+      if (fs.is_null_free_inline_type()) {\n+\n+        \/\/ inline type field klass array entries must have alreadyt been filed at load time or link time\n+        Klass* klass = get_inline_type_field_klass(fs.index());\n+\n+        InstanceKlass::cast(klass)->initialize(THREAD);\n+        if (fs.access_flags().is_static()) {\n+          if (java_mirror()->obj_field(fs.offset()) == nullptr) {\n+            java_mirror()->obj_field_put(fs.offset(), InlineKlass::cast(klass)->default_value());\n+          }\n+        }\n+\n+        if (HAS_PENDING_EXCEPTION) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          {\n+            EXCEPTION_MARK;\n+            add_initialization_error(THREAD, e);\n+            \/\/ Locks object, set state, and notify all waiting threads\n+            set_initialization_state_and_notify(initialization_error, THREAD);\n+            CLEAR_PENDING_EXCEPTION;\n+          }\n+          THROW_OOP(e());\n+        }\n+      }\n+    }\n+  }\n+\n+\n+  \/\/ Step 9\n@@ -1322,1 +1564,1 @@\n-  \/\/ Step 9\n+  \/\/ Step 10\n@@ -1328,1 +1570,1 @@\n-    \/\/ Step 10 and 11\n+    \/\/ Step 11 and 12\n@@ -1624,1 +1866,1 @@\n-      ObjArrayKlass* k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this, CHECK_NULL);\n+      ObjArrayKlass* k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this, false, CHECK_NULL);\n@@ -1631,1 +1873,1 @@\n-  ObjArrayKlass* ak = array_klasses();\n+  ArrayKlass* ak = array_klasses();\n@@ -1638,2 +1880,2 @@\n-  ObjArrayKlass* oak = array_klasses_acquire();\n-  if (oak == nullptr) {\n+  ArrayKlass* ak = array_klasses_acquire();\n+  if (ak == nullptr) {\n@@ -1642,1 +1884,1 @@\n-    return oak->array_klass_or_null(n);\n+    return ak->array_klass_or_null(n);\n@@ -1659,1 +1901,1 @@\n-  if (clinit != nullptr && clinit->has_valid_initializer_flags()) {\n+  if (clinit != nullptr && clinit->is_class_initializer()) {\n@@ -1768,4 +2010,0 @@\n-bool InstanceKlass::contains_field_offset(int offset) {\n-  fieldDescriptor fd;\n-  return find_field_from_offset(offset, false, &fd);\n-}\n@@ -1853,0 +2091,9 @@\n+bool InstanceKlass::contains_field_offset(int offset) {\n+  if (this->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(this);\n+    return offset >= vk->payload_offset() && offset < (vk->payload_offset() + vk->payload_size_in_bytes());\n+  } else {\n+    fieldDescriptor fd;\n+    return find_field_from_offset(offset, false, &fd);\n+  }\n+}\n@@ -2244,0 +2491,3 @@\n+    if (name == vmSymbols::object_initializer_name()) {\n+      break;  \/\/ <init> is never inherited\n+    }\n@@ -2641,0 +2891,1 @@\n+  it->push(&_loadable_descriptors);\n@@ -2642,0 +2893,1 @@\n+  it->push(&_inline_layout_info_array, MetaspaceClosure::_writable);\n@@ -2687,1 +2939,1 @@\n-  \/\/ These are not allocated from metaspace. They are safe to set to null.\n+  \/\/ These are not allocated from metaspace. They are safe to set to nullptr.\n@@ -2775,0 +3027,4 @@\n+  if (is_inline_klass()) {\n+    InlineKlass::cast(this)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -2808,1 +3064,1 @@\n-    assert(this == array_klasses()->bottom_klass(), \"sanity\");\n+    assert(this == ObjArrayKlass::cast(array_klasses())->bottom_klass(), \"sanity\");\n@@ -3005,0 +3261,2 @@\n+  return signature_name_of_carrier(JVM_SIGNATURE_CLASS);\n+}\n@@ -3006,0 +3264,1 @@\n+const char* InstanceKlass::signature_name_of_carrier(char c) const {\n@@ -3012,1 +3271,1 @@\n-  \/\/ Add L as type indicator\n+  \/\/ Add L or Q as type indicator\n@@ -3014,1 +3273,1 @@\n-  dest[dest_index++] = JVM_SIGNATURE_CLASS;\n+  dest[dest_index++] = c;\n@@ -3361,2 +3620,1 @@\n-  \/\/ Remember to strip ACC_SUPER bit\n-  return (access & (~JVM_ACC_SUPER));\n+  return access;\n@@ -3616,1 +3874,4 @@\n-static void print_vtable(intptr_t* start, int len, outputStream* st) {\n+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {\n+  ResourceMark rm;\n+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);\n+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;\n@@ -3620,0 +3881,5 @@\n+    if (forward_refs[i] != 0) {\n+      int from = forward_refs[i];\n+      int off = (int) start[from];\n+      st->print(\" (offset %d <= [%d])\", off, from);\n+    }\n@@ -3623,0 +3889,6 @@\n+    } else if (self != nullptr && e > 0 && e < 0x10000) {\n+      address location = self + e;\n+      int index = (int)((intptr_t*)location - start);\n+      st->print(\" (offset %d => [%d])\", (int)e, index);\n+      if (index >= 0 && index < len)\n+        forward_refs[index] = i;\n@@ -3629,1 +3901,22 @@\n-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);\n+  return print_vtable(nullptr, reinterpret_cast<intptr_t*>(start), len, st);\n+}\n+\n+template<typename T>\n+ static void print_array_on(outputStream* st, Array<T>* array) {\n+   if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+   array->print_value_on(st); st->cr();\n+   if (Verbose || WizardMode) {\n+     for (int i = 0; i < array->length(); i++) {\n+       st->print(\"%d : \", i); array->at(i)->print_value_on(st); st->cr();\n+     }\n+   }\n+ }\n+\n+static void print_array_on(outputStream* st, Array<int>* array) {\n+  if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+  array->print_value_on(st); st->cr();\n+  if (Verbose || WizardMode) {\n+    for (int i = 0; i < array->length(); i++) {\n+      st->print(\"%d : %d\", i, array->at(i)); st->cr();\n+    }\n+  }\n@@ -3670,8 +3963,2 @@\n-  st->print(BULLET\"methods:           \"); methods()->print_value_on(st);               st->cr();\n-  if (Verbose || WizardMode) {\n-    Array<Method*>* method_array = methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n-  st->print(BULLET\"method ordering:   \"); method_ordering()->print_value_on(st);      st->cr();\n+  st->print(BULLET\"methods:           \"); print_array_on(st, methods());\n+  st->print(BULLET\"method ordering:   \"); print_array_on(st, method_ordering());\n@@ -3679,7 +3966,1 @@\n-    st->print(BULLET\"default_methods:   \"); default_methods()->print_value_on(st);    st->cr();\n-    if (Verbose) {\n-      Array<Method*>* method_array = default_methods();\n-      for (int i = 0; i < method_array->length(); i++) {\n-        st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-      }\n-    }\n+    st->print(BULLET\"default_methods:   \"); print_array_on(st, default_methods());\n@@ -3745,0 +4026,1 @@\n+  st->print(BULLET\"loadable descriptors:     \"); loadable_descriptors()->print_value_on(st); st->cr();\n@@ -3755,1 +4037,1 @@\n-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);\n+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(nullptr, start_of_itable(), itable_length(), st);\n@@ -3782,0 +4064,1 @@\n+  for (int i = 0; i < _indent; i++) _st->print(\"  \");\n@@ -3784,1 +4067,1 @@\n-     fd->print_on(_st);\n+     fd->print_on(_st, _base_offset);\n@@ -3787,2 +4070,2 @@\n-     fd->print_on_for(_st, _obj);\n-     _st->cr();\n+     fd->print_on_for(_st, _obj, _indent, _base_offset);\n+     if (!fd->field_flags().is_flat()) _st->cr();\n@@ -3793,1 +4076,1 @@\n-void InstanceKlass::oop_print_on(oop obj, outputStream* st) {\n+void InstanceKlass::oop_print_on(oop obj, outputStream* st, int indent, int base_offset) {\n@@ -3809,1 +4092,1 @@\n-  FieldPrinter print_field(st, obj);\n+  FieldPrinter print_field(st, obj, indent, base_offset);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":327,"deletions":44,"binary":false,"changes":371,"status":"modified"},{"patch":"@@ -318,1 +318,1 @@\n-  int lh = array_layout_helper(tag, hsize, etype, exact_log2(esize));\n+  int lh = array_layout_helper(tag, false, hsize, etype, exact_log2(esize));\n@@ -1013,4 +1013,2 @@\n-     if (UseCompactObjectHeaders) {\n-       st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n-       st->cr();\n-     }\n+     st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n+     st->cr();\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1232,4 +1232,4 @@\n-  if (m->is_static())             return false; \/\/ e.g., Stream.empty\n-  if (m->is_object_initializer()) return false; \/\/ <init>\n-  if (m->is_static_initializer()) return false; \/\/ <clinit>\n-  if (m->is_private())            return false; \/\/ uses direct call\n+  if (m->is_static())             return false;   \/\/ e.g., Stream.empty\n+  if (m->is_private())            return false;   \/\/ uses direct call\n+  if (m->is_object_constructor()) return false;   \/\/ <init>(...)V\n+  if (m->is_class_initializer())  return false;   \/\/ <clinit>()V\n@@ -1444,0 +1444,12 @@\n+int count_interface_methods_needing_itable_index(Array<Method*>* methods) {\n+  int method_count = 0;\n+  if (methods->length() > 0) {\n+    for (int i = methods->length(); --i >= 0; ) {\n+      if (interface_method_needs_itable_index(methods->at(i))) {\n+        method_count++;\n+      }\n+    }\n+  }\n+  return method_count;\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/klassVtable.cpp","additions":16,"deletions":4,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -102,0 +102,14 @@\n+\n+markWord markWord::flat_array_prototype(LayoutKind lk) {\n+  switch(lk) {\n+    case ATOMIC_FLAT:\n+    case NON_ATOMIC_FLAT:\n+      return markWord(null_free_flat_array_pattern);\n+      break;\n+    case NULLABLE_ATOMIC_FLAT:\n+      return markWord(nullable_flat_array_pattern);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n","filename":"src\/hotspot\/share\/oops\/markWord.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -122,1 +123,0 @@\n-\n@@ -160,0 +160,5 @@\n+address Method::get_c2i_inline_entry() {\n+  assert(adapter() != nullptr, \"must have\");\n+  return adapter()->get_c2i_inline_entry();\n+}\n+\n@@ -165,0 +170,5 @@\n+address Method::get_c2i_unverified_inline_entry() {\n+  assert(adapter() != nullptr, \"must have\");\n+  return adapter()->get_c2i_unverified_inline_entry();\n+}\n+\n@@ -390,1 +400,1 @@\n-  if (!method_holder()->is_rewritten()) {\n+  if (!method_holder()->is_rewritten() || CDSConfig::is_valhalla_preview()) {\n@@ -667,0 +677,16 @@\n+\/\/ InlineKlass the method is declared to return. This must not\n+\/\/ safepoint as it is called with references live on the stack at\n+\/\/ locations the GC is unaware of.\n+InlineKlass* Method::returns_inline_type(Thread* thread) const {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  if (is_native()) {\n+    return nullptr;\n+  }\n+  NoSafepointVerifier nsv;\n+  SignatureStream ss(signature());\n+  while (!ss.at_return_type()) {\n+    ss.next();\n+  }\n+  return ss.as_inline_klass(method_holder());\n+}\n+\n@@ -815,0 +841,5 @@\n+  if (has_scalarized_return()) {\n+    \/\/ Don't treat this as (trivial) getter method because the\n+    \/\/ inline type should be returned in a scalarized form.\n+    return false;\n+  }\n@@ -836,0 +867,5 @@\n+  if (has_scalarized_args()) {\n+    \/\/ Don't treat this as (trivial) setter method because the\n+    \/\/ inline type argument should be passed in a scalarized form.\n+    return false;\n+  }\n@@ -846,1 +882,2 @@\n-          Bytecodes::is_return(java_code_at(last_index)));\n+          Bytecodes::is_return(java_code_at(last_index)) &&\n+          !has_scalarized_args());\n@@ -849,6 +886,1 @@\n-bool Method::has_valid_initializer_flags() const {\n-  return (is_static() ||\n-          method_holder()->major_version() < 51);\n-}\n-\n-bool Method::is_static_initializer() const {\n+bool Method::is_class_initializer() const {\n@@ -858,2 +890,3 @@\n-  return name() == vmSymbols::class_initializer_name() &&\n-         has_valid_initializer_flags();\n+  return (name() == vmSymbols::class_initializer_name() &&\n+          (is_static() ||\n+           method_holder()->major_version() < 51));\n@@ -862,2 +895,3 @@\n-bool Method::is_object_initializer() const {\n-   return name() == vmSymbols::object_initializer_name();\n+\/\/ A method named <init>, is a classic object constructor.\n+bool Method::is_object_constructor() const {\n+  return name() == vmSymbols::object_initializer_name();\n@@ -926,1 +960,1 @@\n-  if( constants()->tag_at(klass_index).is_unresolved_klass() ) {\n+  if( constants()->tag_at(klass_index).is_unresolved_klass()) {\n@@ -941,1 +975,3 @@\n-    if (constants()->tag_at(klass_index).is_unresolved_klass()) return false;\n+    if (constants()->tag_at(klass_index).is_unresolved_klass()) {\n+      return false;\n+    }\n@@ -1110,0 +1146,2 @@\n+    _from_compiled_inline_entry = nullptr;\n+    _from_compiled_inline_ro_entry = nullptr;\n@@ -1112,0 +1150,2 @@\n+    _from_compiled_inline_entry = adapter()->get_c2i_inline_entry();\n+    _from_compiled_inline_ro_entry = adapter()->get_c2i_inline_ro_entry();\n@@ -1143,0 +1183,2 @@\n+  _from_compiled_inline_entry = nullptr;\n+  _from_compiled_inline_ro_entry = nullptr;\n@@ -1168,0 +1210,2 @@\n+  set_has_scalarized_args(false);\n+  set_has_scalarized_return(false);\n@@ -1200,0 +1244,3 @@\n+  if (InlineTypeReturnedAsFields && returns_inline_type(THREAD) && !has_scalarized_return()) {\n+    set_has_scalarized_return();\n+  }\n@@ -1248,0 +1295,2 @@\n+  mh->_from_compiled_inline_entry = adapter->get_c2i_inline_entry();\n+  mh->_from_compiled_inline_ro_entry = adapter->get_c2i_inline_ro_entry();\n@@ -1264,0 +1313,12 @@\n+address Method::verified_inline_code_entry() {\n+  debug_only(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_entry != nullptr, \"must be set\");\n+  return _from_compiled_inline_entry;\n+}\n+\n+address Method::verified_inline_ro_code_entry() {\n+  debug_only(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_ro_entry != nullptr, \"must be set\");\n+  return _from_compiled_inline_ro_entry;\n+}\n+\n@@ -1295,0 +1356,2 @@\n+  mh->_from_compiled_inline_entry = code->verified_inline_entry_point();\n+  mh->_from_compiled_inline_ro_entry = code->verified_inline_ro_entry_point();\n@@ -2283,0 +2346,25 @@\n+bool Method::is_scalarized_arg(int idx) const {\n+  if (!has_scalarized_args()) {\n+    return false;\n+  }\n+  \/\/ Search through signature and check if argument is wrapped in T_METADATA\/T_VOID\n+  int depth = 0;\n+  const GrowableArray<SigEntry>* sig = adapter()->get_sig_cc();\n+  for (int i = 0; i < sig->length(); i++) {\n+    BasicType bt = sig->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      depth++;\n+    }\n+    if (idx == 0) {\n+      break; \/\/ Argument found\n+    }\n+    if (bt == T_VOID && (sig->at(i-1)._bt != T_LONG && sig->at(i-1)._bt != T_DOUBLE)) {\n+      depth--;\n+    }\n+    if (depth == 0 && bt != T_LONG && bt != T_DOUBLE) {\n+      idx--; \/\/ Advance to next argument\n+    }\n+  }\n+  return depth != 0;\n+}\n+\n@@ -2315,0 +2403,4 @@\n+#ifdef ASSERT\n+  if (valid_itable_index())\n+    st->print_cr(\" - itable index:      %d\",   itable_index());\n+#endif\n@@ -2322,1 +2414,3 @@\n-  st->print_cr(\" - compiled entry     \" PTR_FORMAT, p2i(from_compiled_entry()));\n+  st->print_cr(\" - compiled entry           \" PTR_FORMAT, p2i(from_compiled_entry()));\n+  st->print_cr(\" - compiled inline entry    \" PTR_FORMAT, p2i(from_compiled_inline_entry()));\n+  st->print_cr(\" - compiled inline ro entry \" PTR_FORMAT, p2i(from_compiled_inline_ro_entry()));\n@@ -2392,0 +2486,1 @@\n+  if (WizardMode) access_flags().print_on(st);\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":111,"deletions":16,"binary":false,"changes":127,"status":"modified"},{"patch":"@@ -148,1 +148,1 @@\n-    st->print(\"flags(%d) \", flags);\n+    st->print(\"flags(%d) %p\/%d\", flags, data(), in_bytes(DataLayout::flags_offset()));\n@@ -218,1 +218,1 @@\n-  assert(TypeStackSlotEntries::per_arg_count() > ReturnTypeEntry::static_cell_count(), \"code to test for arguments\/results broken\");\n+  assert(TypeStackSlotEntries::per_arg_count() > SingleTypeEntry::static_cell_count(), \"code to test for arguments\/results broken\");\n@@ -228,1 +228,1 @@\n-    ret_cell = ReturnTypeEntry::static_cell_count();\n+    ret_cell = SingleTypeEntry::static_cell_count();\n@@ -331,1 +331,1 @@\n-void ReturnTypeEntry::clean_weak_klass_links(bool always_clean) {\n+void SingleTypeEntry::clean_weak_klass_links(bool always_clean) {\n@@ -369,1 +369,1 @@\n-void ReturnTypeEntry::print_data_on(outputStream* st) const {\n+void SingleTypeEntry::print_data_on(outputStream* st) const {\n@@ -530,0 +530,4 @@\n+  if (data()->flags()) {\n+    st->cr();\n+    tab(st);\n+  }\n@@ -655,0 +659,36 @@\n+void ArrayStoreData::print_data_on(outputStream* st, const char* extra) const {\n+  print_shared(st, \"ArrayStore\", extra);\n+  st->cr();\n+  tab(st, true);\n+  st->print(\"array\");\n+  _array.print_data_on(st);\n+  tab(st, true);\n+  st->print(\"element\");\n+  if (null_seen()) {\n+    st->print(\" (null seen)\");\n+  }\n+  tab(st);\n+  print_receiver_data_on(st);\n+}\n+\n+void ArrayLoadData::print_data_on(outputStream* st, const char* extra) const {\n+  print_shared(st, \"ArrayLoad\", extra);\n+  st->cr();\n+  tab(st, true);\n+  st->print(\"array\");\n+  _array.print_data_on(st);\n+  tab(st, true);\n+  st->print(\"element\");\n+  _element.print_data_on(st);\n+}\n+\n+void ACmpData::print_data_on(outputStream* st, const char* extra) const {\n+  BranchData::print_data_on(st, extra);\n+  tab(st, true);\n+  st->print(\"left\");\n+  _left.print_data_on(st);\n+  tab(st, true);\n+  st->print(\"right\");\n+  _right.print_data_on(st);\n+}\n+\n@@ -673,1 +713,0 @@\n-  case Bytecodes::_aastore:\n@@ -679,0 +718,4 @@\n+  case Bytecodes::_aaload:\n+    return ArrayLoadData::static_cell_count();\n+  case Bytecodes::_aastore:\n+    return ArrayStoreData::static_cell_count();\n@@ -718,2 +761,0 @@\n-  case Bytecodes::_if_acmpeq:\n-  case Bytecodes::_if_acmpne:\n@@ -723,0 +764,3 @@\n+  case Bytecodes::_if_acmpne:\n+  case Bytecodes::_if_acmpeq:\n+    return ACmpData::static_cell_count();\n@@ -781,0 +825,1 @@\n+  case Bytecodes::_aaload:\n@@ -997,1 +1042,0 @@\n-  case Bytecodes::_aastore:\n@@ -1006,0 +1050,8 @@\n+  case Bytecodes::_aaload:\n+    cell_count = ArrayLoadData::static_cell_count();\n+    tag = DataLayout::array_load_data_tag;\n+    break;\n+  case Bytecodes::_aastore:\n+    cell_count = ArrayStoreData::static_cell_count();\n+    tag = DataLayout::array_store_data_tag;\n+    break;\n@@ -1077,2 +1129,0 @@\n-  case Bytecodes::_if_acmpeq:\n-  case Bytecodes::_if_acmpne:\n@@ -1084,0 +1134,5 @@\n+  case Bytecodes::_if_acmpeq:\n+  case Bytecodes::_if_acmpne:\n+    cell_count = ACmpData::static_cell_count();\n+    tag = DataLayout::acmp_data_tag;\n+    break;\n@@ -1151,0 +1206,6 @@\n+  case DataLayout::array_store_data_tag:\n+    return ((new ArrayStoreData(this))->cell_count());\n+  case DataLayout::array_load_data_tag:\n+    return ((new ArrayLoadData(this))->cell_count());\n+  case DataLayout::acmp_data_tag:\n+    return ((new ACmpData(this))->cell_count());\n@@ -1185,0 +1246,6 @@\n+  case DataLayout::array_store_data_tag:\n+    return new ArrayStoreData(this);\n+  case DataLayout::array_load_data_tag:\n+    return new ArrayLoadData(this);\n+  case DataLayout::acmp_data_tag:\n+    return new ACmpData(this);\n","filename":"src\/hotspot\/share\/oops\/methodData.cpp","additions":78,"deletions":11,"binary":false,"changes":89,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"memory\/oopFactory.hpp\"\n@@ -47,1 +48,3 @@\n-ObjArrayKlass* ObjArrayKlass::allocate(ClassLoaderData* loader_data, int n, Klass* k, Symbol* name, TRAPS) {\n+ObjArrayKlass* ObjArrayKlass::allocate(ClassLoaderData* loader_data, int n,\n+                                       Klass* k, Symbol* name, bool null_free,\n+                                       TRAPS) {\n@@ -53,1 +56,1 @@\n-  return new (loader_data, size, THREAD) ObjArrayKlass(n, k, name);\n+  return new (loader_data, size, THREAD) ObjArrayKlass(n, k, name, null_free);\n@@ -57,1 +60,3 @@\n-                                                      int n, Klass* element_klass, TRAPS) {\n+                                                      int n, Klass* element_klass,\n+                                                      bool null_free, TRAPS) {\n+  assert(!null_free || (n == 1 && element_klass->is_inline_klass()), \"null-free unsupported\");\n@@ -68,1 +73,5 @@\n-      super_klass = element_super->array_klass(CHECK_NULL);\n+      if (null_free) {\n+        super_klass = element_klass->array_klass(CHECK_NULL);\n+      } else {\n+        super_klass = element_super->array_klass(CHECK_NULL);\n+      }\n@@ -82,19 +91,1 @@\n-  Symbol* name = nullptr;\n-  {\n-    ResourceMark rm(THREAD);\n-    char *name_str = element_klass->name()->as_C_string();\n-    int len = element_klass->name()->utf8_length();\n-    char *new_str = NEW_RESOURCE_ARRAY(char, len + 4);\n-    int idx = 0;\n-    new_str[idx++] = JVM_SIGNATURE_ARRAY;\n-    if (element_klass->is_instance_klass()) { \/\/ it could be an array or simple type\n-      new_str[idx++] = JVM_SIGNATURE_CLASS;\n-    }\n-    memcpy(&new_str[idx], name_str, len * sizeof(char));\n-    idx += len;\n-    if (element_klass->is_instance_klass()) {\n-      new_str[idx++] = JVM_SIGNATURE_ENDCLASS;\n-    }\n-    new_str[idx++] = '\\0';\n-    name = SymbolTable::new_symbol(new_str);\n-  }\n+  Symbol* name = ArrayKlass::create_element_klass_array_name(element_klass, CHECK_NULL);\n@@ -103,1 +94,1 @@\n-  ObjArrayKlass* oak = ObjArrayKlass::allocate(loader_data, n, element_klass, name, CHECK_NULL);\n+  ObjArrayKlass* oak = ObjArrayKlass::allocate(loader_data, n, element_klass, name, null_free, CHECK_NULL);\n@@ -121,1 +112,1 @@\n-ObjArrayKlass::ObjArrayKlass(int n, Klass* element_klass, Symbol* name) : ArrayKlass(name, Kind) {\n+ObjArrayKlass::ObjArrayKlass(int n, Klass* element_klass, Symbol* name, bool null_free) : ArrayKlass(name, Kind) {\n@@ -128,0 +119,2 @@\n+  } else if (element_klass->is_flatArray_klass()) {\n+    bk = FlatArrayKlass::cast(element_klass)->element_klass();\n@@ -139,1 +132,12 @@\n-  set_layout_helper(array_layout_helper(T_OBJECT));\n+  int lh = array_layout_helper(T_OBJECT);\n+  if (null_free) {\n+    assert(n == 1, \"Bytecode does not support null-free multi-dim\");\n+    lh = layout_helper_set_null_free(lh);\n+#ifdef _LP64\n+    set_prototype_header(markWord::null_free_array_prototype());\n+    assert(prototype_header().is_null_free_array(), \"sanity\");\n+#else\n+    set_prototype_header(markWord::inline_type_prototype());\n+#endif\n+  }\n+  set_layout_helper(lh);\n@@ -158,2 +162,16 @@\n-  return (objArrayOop)Universe::heap()->array_allocate(this, size, length,\n-                                                       \/* do_zero *\/ true, THREAD);\n+  bool populate_null_free = is_null_free_array_klass();\n+  objArrayOop array =  (objArrayOop)Universe::heap()->array_allocate(this, size, length,\n+                                                       \/* do_zero *\/ true, CHECK_NULL);\n+  objArrayHandle array_h(THREAD, array);\n+  if (populate_null_free) {\n+    assert(dimension() == 1, \"Can only populate the final dimension\");\n+    assert(element_klass()->is_inline_klass(), \"Unexpected\");\n+    assert(!element_klass()->is_array_klass(), \"ArrayKlass unexpected here\");\n+    element_klass()->initialize(CHECK_NULL);\n+    \/\/ Populate default values...\n+    instanceOop value = (instanceOop) InlineKlass::cast(element_klass())->default_value();\n+    for (int i = 0; i < length; i++) {\n+      array_h->obj_at_put(i, value);\n+    }\n+  }\n+  return array_h();\n@@ -171,1 +189,1 @@\n-        oop sub_array = ld_klass->multi_allocate(rank - 1, &sizes[1], CHECK_NULL);\n+        oop sub_array = ld_klass->multi_allocate(rank-1, &sizes[1], CHECK_NULL);\n@@ -200,0 +218,3 @@\n+    \/\/ Perform null check if dst is null-free but src has no such guarantee\n+    bool null_check = ((!s->klass()->is_null_free_array_klass()) &&\n+        d->klass()->is_null_free_array_klass());\n@@ -201,2 +222,5 @@\n-      \/\/ elements are guaranteed to be subtypes, so no check necessary\n-      ArrayAccess<ARRAYCOPY_DISJOINT>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n+      if (null_check) {\n+        ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_NOTNULL>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n+      } else {\n+        ArrayAccess<ARRAYCOPY_DISJOINT>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n+      }\n@@ -204,16 +228,4 @@\n-      \/\/ slow case: need individual subtype checks\n-      \/\/ note: don't use obj_at_put below because it includes a redundant store check\n-      if (!ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_CHECKCAST>::oop_arraycopy(s, src_offset, d, dst_offset, length)) {\n-        ResourceMark rm(THREAD);\n-        stringStream ss;\n-        if (!bound->is_subtype_of(stype)) {\n-          ss.print(\"arraycopy: type mismatch: can not copy %s[] into %s[]\",\n-                   stype->external_name(), bound->external_name());\n-        } else {\n-          \/\/ oop_arraycopy should return the index in the source array that\n-          \/\/ contains the problematic oop.\n-          ss.print(\"arraycopy: element type mismatch: can not cast one of the elements\"\n-                   \" of %s[] to the type of the destination array, %s\",\n-                   stype->external_name(), bound->external_name());\n-        }\n-        THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+      if (null_check) {\n+        ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_CHECKCAST | ARRAYCOPY_NOTNULL>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n+      } else {\n+        ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_CHECKCAST>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n@@ -229,0 +241,7 @@\n+  if (UseArrayFlattening) {\n+    if (d->is_flatArray()) {\n+      FlatArrayKlass::cast(d->klass())->copy_array(s, src_pos, d, dst_pos, length, THREAD);\n+      return;\n+    }\n+  }\n+\n@@ -350,0 +369,2 @@\n+  int identity_flag = (Arguments::enable_preview()) ? JVM_ACC_IDENTITY : 0;\n+\n@@ -351,1 +372,1 @@\n-                        | (JVM_ACC_ABSTRACT | JVM_ACC_FINAL);\n+                        | (identity_flag | JVM_ACC_ABSTRACT | JVM_ACC_FINAL);\n@@ -370,1 +391,1 @@\n-  st->print(\" - instance klass: \");\n+  st->print(\" - element klass: \");\n@@ -432,1 +453,2 @@\n-  guarantee(bk->is_instance_klass() || bk->is_typeArray_klass(),  \"invalid bottom klass\");\n+  guarantee(bk->is_instance_klass() || bk->is_typeArray_klass() || bk->is_flatArray_klass(),\n+            \"invalid bottom klass\");\n@@ -438,0 +460,1 @@\n+  guarantee(obj->is_null_free_array() || (!is_null_free_array_klass()), \"null-free klass but not object\");\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":73,"deletions":50,"binary":false,"changes":123,"status":"modified"},{"patch":"@@ -147,6 +147,8 @@\n-bool oopDesc::is_instance_noinline()    const { return is_instance();    }\n-bool oopDesc::is_instanceRef_noinline() const { return is_instanceRef(); }\n-bool oopDesc::is_stackChunk_noinline()  const { return is_stackChunk();  }\n-bool oopDesc::is_array_noinline()       const { return is_array();       }\n-bool oopDesc::is_objArray_noinline()    const { return is_objArray();    }\n-bool oopDesc::is_typeArray_noinline()   const { return is_typeArray();   }\n+bool oopDesc::is_instance_noinline()        const { return is_instance();         }\n+bool oopDesc::is_instanceRef_noinline()     const { return is_instanceRef();      }\n+bool oopDesc::is_stackChunk_noinline()      const { return is_stackChunk();       }\n+bool oopDesc::is_array_noinline()           const { return is_array();            }\n+bool oopDesc::is_objArray_noinline()        const { return is_objArray();         }\n+bool oopDesc::is_typeArray_noinline()       const { return is_typeArray();        }\n+bool oopDesc::is_flatArray_noinline()       const { return is_flatArray();        }\n+bool oopDesc::is_null_free_array_noinline() const { return is_null_free_array();  }\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -87,1 +87,1 @@\n-  if (UseCompactObjectHeaders) {\n+  if (UseCompactObjectHeaders || EnableValhalla) {\n@@ -235,0 +235,15 @@\n+bool oopDesc::is_inline_type() const { return mark().is_inline_type(); }\n+#ifdef _LP64\n+bool oopDesc::is_flatArray() const {\n+  markWord mrk = mark();\n+  return (mrk.is_unlocked()) ? mrk.is_flat_array() : klass()->is_flatArray_klass();\n+}\n+bool oopDesc::is_null_free_array() const {\n+  markWord mrk = mark();\n+  return (mrk.is_unlocked()) ? mrk.is_null_free_array() : klass()->is_null_free_array_klass();\n+}\n+#else\n+bool oopDesc::is_flatArray()       const { return klass()->is_flatArray_klass(); }\n+bool oopDesc::is_null_free_array() const { return klass()->is_null_free_array_klass(); }\n+#endif\n+\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -25,1 +25,2 @@\n-#include \"cds\/archiveBuilder.hpp\"\n+#include \"interpreter\/bytecodes.hpp\"\n+#include \"oops\/instanceOop.hpp\"\n@@ -27,0 +28,2 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"cds\/archiveBuilder.hpp\"\n@@ -42,0 +45,2 @@\n+  st->print_cr(\" - Is Flat: %d\", is_flat());\n+  st->print_cr(\" - Is Null Free Inline Type: %d\", is_null_free_inline_type());\n@@ -46,0 +51,9 @@\n+bool ResolvedFieldEntry::is_valid() const {\n+  return field_holder()->is_instance_klass() &&\n+    field_offset() >= instanceOopDesc::base_offset_in_bytes() && field_offset() < 0x7fffffff &&\n+    as_BasicType((TosState)tos_state()) != T_ILLEGAL &&\n+    _flags < (1 << (max_flag_shift + 1)) &&\n+    (get_code() == 0 || get_code() == Bytecodes::_getstatic || get_code() == Bytecodes::_getfield) &&\n+    (put_code() == 0 || put_code() == Bytecodes::_putstatic || put_code() == Bytecodes::_putfield);\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/resolvedFieldEntry.cpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -88,0 +88,40 @@\n+Symbol* Symbol::fundamental_name(TRAPS) {\n+  if (char_at(0) == JVM_SIGNATURE_CLASS && ends_with(JVM_SIGNATURE_ENDCLASS)) {\n+    return SymbolTable::new_symbol(this, 1, utf8_length() - 1);\n+  } else {\n+    \/\/ reference count is incremented to be consistent with the behavior with\n+    \/\/ the SymbolTable::new_symbol() call above\n+    this->increment_refcount();\n+    return this;\n+  }\n+}\n+\n+bool Symbol::is_same_fundamental_type(Symbol* s) const {\n+  if (this == s) return true;\n+  if (utf8_length() < 3) return false;\n+  int offset1, offset2, len;\n+  if (ends_with(JVM_SIGNATURE_ENDCLASS)) {\n+    if (char_at(0) != JVM_SIGNATURE_CLASS) return false;\n+    offset1 = 1;\n+    len = utf8_length() - 2;\n+  } else {\n+    offset1 = 0;\n+    len = utf8_length();\n+  }\n+  if (ends_with(JVM_SIGNATURE_ENDCLASS)) {\n+    if (s->char_at(0) != JVM_SIGNATURE_CLASS) return false;\n+    offset2 = 1;\n+  } else {\n+    offset2 = 0;\n+  }\n+  if ((offset2 + len) > s->utf8_length()) return false;\n+  if ((utf8_length() - offset1 * 2) != (s->utf8_length() - offset2 * 2))\n+    return false;\n+  int l = len;\n+  while (l-- > 0) {\n+    if (char_at(offset1 + l) != s->char_at(offset2 + l))\n+      return false;\n+  }\n+  return true;\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/symbol.cpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -78,1 +78,4 @@\n-  return JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC;\n+  u2 identity_flag = (Arguments::enable_preview()) ? JVM_ACC_IDENTITY : 0;\n+\n+  return JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC\n+                    | identity_flag;\n@@ -104,1 +107,0 @@\n-  \/\/ For typeArrays this is only called for the last dimension\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -678,0 +678,6 @@\n+  if (tp->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to extract the field being accessed from\n+    \/\/ the address computation\n+    return tp->is_aryptr()->add_field_offset_and_offset(txoffset);\n+  }\n@@ -698,0 +704,6 @@\n+  if (p1->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to extract the field being accessed from\n+    \/\/ the address computation\n+    return p1->is_aryptr()->add_field_offset_and_offset(p2offset);\n+  }\n","filename":"src\/hotspot\/share\/opto\/addnode.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -118,3 +120,7 @@\n-  Node* src = in(ArrayCopyNode::Src);\n-  const Type* src_type = phase->type(src);\n-\n+    Node* src = in(ArrayCopyNode::Src);\n+    const Type* src_type = phase->type(src);\n+\n+    if (src_type == Type::TOP) {\n+      return -1;\n+    }\n+\n@@ -144,0 +150,1 @@\n+             (UseArrayFlattening && ary_src->elem()->make_oopptr() != nullptr && ary_src->elem()->make_oopptr()->can_be_inline_type()) ||\n@@ -197,0 +204,1 @@\n+  phase->record_for_igvn(mem);\n@@ -287,1 +295,1 @@\n-    if (src_elem != dest_elem || dest_elem == T_VOID) {\n+    if (src_elem != dest_elem || ary_src->is_flat() != ary_dest->is_flat() || dest_elem == T_VOID) {\n@@ -293,3 +301,4 @@\n-    if (bs->array_copy_requires_gc_barriers(is_alloc_tightly_coupled(), dest_elem, false, false, BarrierSetC2::Optimization)) {\n-      \/\/ It's an object array copy but we can't emit the card marking\n-      \/\/ that is needed\n+    if ((!ary_dest->is_flat() && bs->array_copy_requires_gc_barriers(is_alloc_tightly_coupled(), dest_elem, false, false, BarrierSetC2::Optimization)) ||\n+        (ary_dest->is_flat() && ary_src->elem()->inline_klass()->contains_oops() &&\n+         bs->array_copy_requires_gc_barriers(is_alloc_tightly_coupled(), T_OBJECT, false, false, BarrierSetC2::Optimization))) {\n+      \/\/ It's an object array copy but we can't emit the card marking that is needed\n@@ -302,0 +311,3 @@\n+    if (ary_dest->is_flat()) {\n+      shift = ary_src->flat_log_elem_size();\n+    }\n@@ -341,0 +353,5 @@\n+    if (ary_src->elem()->make_oopptr() != nullptr &&\n+        ary_src->elem()->make_oopptr()->can_be_inline_type()) {\n+      return false;\n+    }\n+\n@@ -347,1 +364,4 @@\n-    if (bs->array_copy_requires_gc_barriers(true, elem, true, is_clone_inst(), BarrierSetC2::Optimization)) {\n+    if ((!ary_src->is_flat() && bs->array_copy_requires_gc_barriers(true, elem, true, is_clone_inst(), BarrierSetC2::Optimization)) ||\n+        (ary_src->is_flat() && ary_src->elem()->inline_klass()->contains_oops() &&\n+         bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, is_clone_inst(), BarrierSetC2::Optimization))) {\n+      \/\/ It's an object array copy but we can't emit the card marking that is needed\n@@ -371,1 +391,1 @@\n-const TypePtr* ArrayCopyNode::get_address_type(PhaseGVN* phase, const TypePtr* atp, Node* n) {\n+const TypeAryPtr* ArrayCopyNode::get_address_type(PhaseGVN* phase, const TypePtr* atp, Node* n) {\n@@ -376,1 +396,1 @@\n-  return atp->add_offset(Type::OffsetBot);\n+  return atp->add_offset(Type::OffsetBot)->is_aryptr();\n@@ -379,2 +399,2 @@\n-void ArrayCopyNode::array_copy_test_overlap(PhaseGVN *phase, bool can_reshape, bool disjoint_bases, int count, Node*& forward_ctl, Node*& backward_ctl) {\n-  Node* ctl = in(TypeFunc::Control);\n+void ArrayCopyNode::array_copy_test_overlap(GraphKit& kit, bool disjoint_bases, int count, Node*& backward_ctl) {\n+  Node* ctl = kit.control();\n@@ -382,0 +402,1 @@\n+    PhaseGVN& gvn = kit.gvn();\n@@ -385,2 +406,2 @@\n-    Node* cmp = phase->transform(new CmpINode(src_offset, dest_offset));\n-    Node *bol = phase->transform(new BoolNode(cmp, BoolTest::lt));\n+    Node* cmp = gvn.transform(new CmpINode(src_offset, dest_offset));\n+    Node *bol = gvn.transform(new BoolNode(cmp, BoolTest::lt));\n@@ -389,1 +410,6 @@\n-    phase->transform(iff);\n+    gvn.transform(iff);\n+\n+    kit.set_control(gvn.transform(new IfFalseNode(iff)));\n+    backward_ctl = gvn.transform(new IfTrueNode(iff));\n+  }\n+}\n@@ -391,2 +417,29 @@\n-    forward_ctl = phase->transform(new IfFalseNode(iff));\n-    backward_ctl = phase->transform(new IfTrueNode(iff));\n+void ArrayCopyNode::copy(GraphKit& kit,\n+                         const TypeAryPtr* atp_src,\n+                         const TypeAryPtr* atp_dest,\n+                         int i,\n+                         Node* base_src,\n+                         Node* base_dest,\n+                         Node* adr_src,\n+                         Node* adr_dest,\n+                         BasicType copy_type,\n+                         const Type* value_type) {\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  Node* ctl = kit.control();\n+  if (atp_dest->is_flat()) {\n+    ciInlineKlass* vk = atp_src->elem()->inline_klass();\n+    for (int j = 0; j < vk->nof_nonstatic_fields(); j++) {\n+      ciField* field = vk->nonstatic_field_at(j);\n+      int off_in_vt = field->offset_in_bytes() - vk->payload_offset();\n+      Node* off  = kit.MakeConX(off_in_vt + i * atp_src->flat_elem_size());\n+      ciType* ft = field->type();\n+      BasicType bt = type2field[ft->basic_type()];\n+      assert(!field->is_flat(), \"flat field encountered\");\n+      const Type* rt = Type::get_const_type(ft);\n+      const TypePtr* adr_type = atp_src->with_field_offset(off_in_vt)->add_offset(Type::OffsetBot);\n+      assert(!bs->array_copy_requires_gc_barriers(is_alloc_tightly_coupled(), bt, false, false, BarrierSetC2::Optimization), \"GC barriers required\");\n+      Node* next_src = kit.gvn().transform(new AddPNode(base_src, adr_src, off));\n+      Node* next_dest = kit.gvn().transform(new AddPNode(base_dest, adr_dest, off));\n+      Node* v = load(bs, &kit.gvn(), ctl, kit.merged_memory(), next_src, adr_type, rt, bt);\n+      store(bs, &kit.gvn(), ctl, kit.merged_memory(), next_dest, adr_type, v, rt, bt);\n+    }\n@@ -394,1 +447,5 @@\n-    forward_ctl = ctl;\n+    Node* off = kit.MakeConX(type2aelembytes(copy_type) * i);\n+    Node* next_src = kit.gvn().transform(new AddPNode(base_src, adr_src, off));\n+    Node* next_dest = kit.gvn().transform(new AddPNode(base_dest, adr_dest, off));\n+    Node* v = load(bs, &kit.gvn(), ctl, kit.merged_memory(), next_src, atp_src, value_type, copy_type);\n+    store(bs, &kit.gvn(), ctl, kit.merged_memory(), next_dest, atp_dest, v, value_type, copy_type);\n@@ -396,0 +453,1 @@\n+  kit.set_control(ctl);\n@@ -398,16 +456,13 @@\n-Node* ArrayCopyNode::array_copy_forward(PhaseGVN *phase,\n-                                        bool can_reshape,\n-                                        Node*& forward_ctl,\n-                                        Node* mem,\n-                                        const TypePtr* atp_src,\n-                                        const TypePtr* atp_dest,\n-                                        Node* adr_src,\n-                                        Node* base_src,\n-                                        Node* adr_dest,\n-                                        Node* base_dest,\n-                                        BasicType copy_type,\n-                                        const Type* value_type,\n-                                        int count) {\n-  if (!forward_ctl->is_top()) {\n-    \/\/ copy forward\n-    MergeMemNode* mm = MergeMemNode::make(mem);\n+void ArrayCopyNode::array_copy_forward(GraphKit& kit,\n+                                       bool can_reshape,\n+                                       const TypeAryPtr* atp_src,\n+                                       const TypeAryPtr* atp_dest,\n+                                       Node* adr_src,\n+                                       Node* base_src,\n+                                       Node* adr_dest,\n+                                       Node* base_dest,\n+                                       BasicType copy_type,\n+                                       const Type* value_type,\n+                                       int count) {\n+  if (!kit.stopped()) {\n+    \/\/ copy forward\n@@ -416,9 +471,2 @@\n-      BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n-      Node* v = load(bs, phase, forward_ctl, mm, adr_src, atp_src, value_type, copy_type);\n-      store(bs, phase, forward_ctl, mm, adr_dest, atp_dest, v, value_type, copy_type);\n-      for (int i = 1; i < count; i++) {\n-        Node* off  = phase->MakeConX(type2aelembytes(copy_type) * i);\n-        Node* next_src = phase->transform(new AddPNode(base_src,adr_src,off));\n-        Node* next_dest = phase->transform(new AddPNode(base_dest,adr_dest,off));\n-        v = load(bs, phase, forward_ctl, mm, next_src, atp_src, value_type, copy_type);\n-        store(bs, phase, forward_ctl, mm, next_dest, atp_dest, v, value_type, copy_type);\n+      for (int i = 0; i < count; i++) {\n+        copy(kit, atp_src, atp_dest, i, base_src, base_dest, adr_src, adr_dest, copy_type, value_type);\n@@ -427,3 +475,4 @@\n-      PhaseIterGVN* igvn = phase->is_IterGVN();\n-      igvn->_worklist.push(adr_src);\n-      igvn->_worklist.push(adr_dest);\n+      PhaseGVN& gvn = kit.gvn();\n+      assert(gvn.is_IterGVN(), \"\");\n+      gvn.record_for_igvn(adr_src);\n+      gvn.record_for_igvn(adr_dest);\n@@ -431,2 +480,0 @@\n-    return mm;\n-  return phase->C->top();\n@@ -436,14 +483,12 @@\n-Node* ArrayCopyNode::array_copy_backward(PhaseGVN *phase,\n-                                         bool can_reshape,\n-                                         Node*& backward_ctl,\n-                                         Node* mem,\n-                                         const TypePtr* atp_src,\n-                                         const TypePtr* atp_dest,\n-                                         Node* adr_src,\n-                                         Node* base_src,\n-                                         Node* adr_dest,\n-                                         Node* base_dest,\n-                                         BasicType copy_type,\n-                                         const Type* value_type,\n-                                         int count) {\n-  if (!backward_ctl->is_top()) {\n+void ArrayCopyNode::array_copy_backward(GraphKit& kit,\n+                                        bool can_reshape,\n+                                        const TypeAryPtr* atp_src,\n+                                        const TypeAryPtr* atp_dest,\n+                                        Node* adr_src,\n+                                        Node* base_src,\n+                                        Node* adr_dest,\n+                                        Node* base_dest,\n+                                        BasicType copy_type,\n+                                        const Type* value_type,\n+                                        int count) {\n+  if (!kit.stopped()) {\n@@ -451,4 +496,1 @@\n-    MergeMemNode* mm = MergeMemNode::make(mem);\n-\n-    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n-    assert(copy_type != T_OBJECT || !bs->array_copy_requires_gc_barriers(false, T_OBJECT, false, false, BarrierSetC2::Optimization), \"only tightly coupled allocations for object arrays\");\n+    PhaseGVN& gvn = kit.gvn();\n@@ -457,6 +499,2 @@\n-      for (int i = count-1; i >= 1; i--) {\n-        Node* off  = phase->MakeConX(type2aelembytes(copy_type) * i);\n-        Node* next_src = phase->transform(new AddPNode(base_src,adr_src,off));\n-        Node* next_dest = phase->transform(new AddPNode(base_dest,adr_dest,off));\n-        Node* v = load(bs, phase, backward_ctl, mm, next_src, atp_src, value_type, copy_type);\n-        store(bs, phase, backward_ctl, mm, next_dest, atp_dest, v, value_type, copy_type);\n+      for (int i = count-1; i >= 0; i--) {\n+        copy(kit, atp_src, atp_dest, i, base_src, base_dest, adr_src, adr_dest, copy_type, value_type);\n@@ -464,6 +502,5 @@\n-      Node* v = load(bs, phase, backward_ctl, mm, adr_src, atp_src, value_type, copy_type);\n-      store(bs, phase, backward_ctl, mm, adr_dest, atp_dest, v, value_type, copy_type);\n-    } else if (can_reshape) {\n-      PhaseIterGVN* igvn = phase->is_IterGVN();\n-      igvn->_worklist.push(adr_src);\n-      igvn->_worklist.push(adr_dest);\n+    } else if(can_reshape) {\n+      PhaseGVN& gvn = kit.gvn();\n+      assert(gvn.is_IterGVN(), \"\");\n+      gvn.record_for_igvn(adr_src);\n+      gvn.record_for_igvn(adr_dest);\n@@ -471,2 +508,0 @@\n-    return phase->transform(mm);\n-  return phase->C->top();\n@@ -498,2 +533,1 @@\n-      CallProjections callprojs;\n-      extract_projections(&callprojs, true, false);\n+      CallProjections* callprojs = extract_projections(true, false);\n@@ -501,2 +535,2 @@\n-      if (callprojs.fallthrough_ioproj != nullptr) {\n-        igvn->replace_node(callprojs.fallthrough_ioproj, in(TypeFunc::I_O));\n+      if (callprojs->fallthrough_ioproj != nullptr) {\n+        igvn->replace_node(callprojs->fallthrough_ioproj, in(TypeFunc::I_O));\n@@ -504,2 +538,2 @@\n-      if (callprojs.fallthrough_memproj != nullptr) {\n-        igvn->replace_node(callprojs.fallthrough_memproj, mem);\n+      if (callprojs->fallthrough_memproj != nullptr) {\n+        igvn->replace_node(callprojs->fallthrough_memproj, mem);\n@@ -507,2 +541,2 @@\n-      if (callprojs.fallthrough_catchproj != nullptr) {\n-        igvn->replace_node(callprojs.fallthrough_catchproj, ctl);\n+      if (callprojs->fallthrough_catchproj != nullptr) {\n+        igvn->replace_node(callprojs->fallthrough_catchproj, ctl);\n@@ -533,1 +567,5 @@\n-  if (remove_dead_region(phase, can_reshape))  return this;\n+  \/\/ Perform any generic optimizations first\n+  Node* result = SafePointNode::Ideal(phase, can_reshape);\n+  if (result != nullptr) {\n+    return result;\n+  }\n@@ -575,0 +613,11 @@\n+  Node* src = in(ArrayCopyNode::Src);\n+  Node* dest = in(ArrayCopyNode::Dest);\n+  const Type* src_type = phase->type(src);\n+  const Type* dest_type = phase->type(dest);\n+\n+  if (src_type->isa_aryptr() && dest_type->isa_instptr()) {\n+    \/\/ clone used for load of unknown inline type can't be optimized at\n+    \/\/ this point\n+    return nullptr;\n+  }\n+\n@@ -596,5 +645,21 @@\n-  Node* src = in(ArrayCopyNode::Src);\n-  Node* dest = in(ArrayCopyNode::Dest);\n-  const TypePtr* atp_src = get_address_type(phase, _src_type, src);\n-  const TypePtr* atp_dest = get_address_type(phase, _dest_type, dest);\n-  Node* in_mem = in(TypeFunc::Memory);\n+  JVMState* new_jvms = nullptr;\n+  SafePointNode* new_map = nullptr;\n+  if (!is_clonebasic()) {\n+    new_jvms = jvms()->clone_shallow(phase->C);\n+    new_map = new SafePointNode(req(), new_jvms);\n+    for (uint i = TypeFunc::FramePtr; i < req(); i++) {\n+      new_map->init_req(i, in(i));\n+    }\n+    new_jvms->set_map(new_map);\n+  } else {\n+    new_jvms = new (phase->C) JVMState(0);\n+    new_map = new SafePointNode(TypeFunc::Parms, new_jvms);\n+    new_jvms->set_map(new_map);\n+  }\n+  new_map->set_control(in(TypeFunc::Control));\n+  new_map->set_memory(MergeMemNode::make(in(TypeFunc::Memory)));\n+  new_map->set_i_o(in(TypeFunc::I_O));\n+  phase->record_for_igvn(new_map);\n+\n+  const TypeAryPtr* atp_src = get_address_type(phase, _src_type, src);\n+  const TypeAryPtr* atp_dest = get_address_type(phase, _dest_type, dest);\n@@ -607,0 +672,4 @@\n+  GraphKit kit(new_jvms, phase);\n+\n+  SafePointNode* backward_map = nullptr;\n+  SafePointNode* forward_map = nullptr;\n@@ -608,36 +677,36 @@\n-  Node* forward_ctl = phase->C->top();\n-  array_copy_test_overlap(phase, can_reshape, disjoint_bases, count, forward_ctl, backward_ctl);\n-\n-  Node* forward_mem = array_copy_forward(phase, can_reshape, forward_ctl,\n-                                         in_mem,\n-                                         atp_src, atp_dest,\n-                                         adr_src, base_src, adr_dest, base_dest,\n-                                         copy_type, value_type, count);\n-\n-  Node* backward_mem = array_copy_backward(phase, can_reshape, backward_ctl,\n-                                           in_mem,\n-                                           atp_src, atp_dest,\n-                                           adr_src, base_src, adr_dest, base_dest,\n-                                           copy_type, value_type, count);\n-\n-  Node* ctl = nullptr;\n-  if (!forward_ctl->is_top() && !backward_ctl->is_top()) {\n-    ctl = new RegionNode(3);\n-    ctl->init_req(1, forward_ctl);\n-    ctl->init_req(2, backward_ctl);\n-    ctl = phase->transform(ctl);\n-    MergeMemNode* forward_mm = forward_mem->as_MergeMem();\n-    MergeMemNode* backward_mm = backward_mem->as_MergeMem();\n-    for (MergeMemStream mms(forward_mm, backward_mm); mms.next_non_empty2(); ) {\n-      if (mms.memory() != mms.memory2()) {\n-        Node* phi = new PhiNode(ctl, Type::MEMORY, phase->C->get_adr_type(mms.alias_idx()));\n-        phi->init_req(1, mms.memory());\n-        phi->init_req(2, mms.memory2());\n-        phi = phase->transform(phi);\n-        mms.set_memory(phi);\n-      }\n-    }\n-    mem = forward_mem;\n-  } else if (!forward_ctl->is_top()) {\n-    ctl = forward_ctl;\n-    mem = forward_mem;\n+\n+  array_copy_test_overlap(kit, disjoint_bases, count, backward_ctl);\n+\n+  {\n+    PreserveJVMState pjvms(&kit);\n+\n+    array_copy_forward(kit, can_reshape,\n+                       atp_src, atp_dest,\n+                       adr_src, base_src, adr_dest, base_dest,\n+                       copy_type, value_type, count);\n+\n+    forward_map = kit.stop();\n+  }\n+\n+  kit.set_control(backward_ctl);\n+  array_copy_backward(kit, can_reshape,\n+                      atp_src, atp_dest,\n+                      adr_src, base_src, adr_dest, base_dest,\n+                      copy_type, value_type, count);\n+\n+  backward_map = kit.stop();\n+\n+  if (!forward_map->control()->is_top() && !backward_map->control()->is_top()) {\n+    assert(forward_map->i_o() == backward_map->i_o(), \"need a phi on IO?\");\n+    Node* ctl = new RegionNode(3);\n+    Node* mem = new PhiNode(ctl, Type::MEMORY, TypePtr::BOTTOM);\n+    kit.set_map(forward_map);\n+    ctl->init_req(1, kit.control());\n+    mem->init_req(1, kit.reset_memory());\n+    kit.set_map(backward_map);\n+    ctl->init_req(2, kit.control());\n+    mem->init_req(2, kit.reset_memory());\n+    kit.set_control(phase->transform(ctl));\n+    kit.set_all_memory(phase->transform(mem));\n+  } else if (!forward_map->control()->is_top()) {\n+    kit.set_map(forward_map);\n@@ -645,3 +714,2 @@\n-    assert(!backward_ctl->is_top(), \"no copy?\");\n-    ctl = backward_ctl;\n-    mem = backward_mem;\n+    assert(!backward_map->control()->is_top(), \"no copy?\");\n+    kit.set_map(backward_map);\n@@ -655,2 +723,5 @@\n-  if (!finish_transform(phase, can_reshape, ctl, mem)) {\n-    if (can_reshape) {\n+  mem = kit.map()->memory();\n+  if (!finish_transform(phase, can_reshape, kit.control(), mem)) {\n+    if (!can_reshape) {\n+      phase->record_for_igvn(this);\n+    } else {\n@@ -755,1 +826,1 @@\n-  uint elemsize = type2aelembytes(ary_elem);\n+  uint elemsize = ary_t->is_flat() ? ary_t->flat_elem_size() : type2aelembytes(ary_elem);\n","filename":"src\/hotspot\/share\/opto\/arraycopynode.cpp","additions":207,"deletions":136,"binary":false,"changes":343,"status":"modified"},{"patch":"@@ -103,1 +103,1 @@\n-  static const TypePtr* get_address_type(PhaseGVN* phase, const TypePtr* atp, Node* n);\n+  static const TypeAryPtr* get_address_type(PhaseGVN* phase, const TypePtr* atp, Node* n);\n@@ -109,1 +109,1 @@\n-  void array_copy_test_overlap(PhaseGVN *phase, bool can_reshape,\n+  void array_copy_test_overlap(GraphKit& kit,\n@@ -111,4 +111,7 @@\n-                               Node*& forward_ctl, Node*& backward_ctl);\n-  Node* array_copy_forward(PhaseGVN *phase, bool can_reshape, Node*& ctl,\n-                           Node* mem,\n-                           const TypePtr* atp_src, const TypePtr* atp_dest,\n+                               Node*& backward_ctl);\n+  void array_copy_forward(GraphKit& kit, bool can_reshape,\n+                          const TypeAryPtr* atp_src, const TypeAryPtr* atp_dest,\n+                          Node* adr_src, Node* base_src, Node* adr_dest, Node* base_dest,\n+                          BasicType copy_type, const Type* value_type, int count);\n+  void array_copy_backward(GraphKit& kit, bool can_reshape,\n+                           const TypeAryPtr* atp_src, const TypeAryPtr* atp_dest,\n@@ -117,5 +120,0 @@\n-  Node* array_copy_backward(PhaseGVN *phase, bool can_reshape, Node*& ctl,\n-                            Node* mem,\n-                            const TypePtr* atp_src, const TypePtr* atp_dest,\n-                            Node* adr_src, Node* base_src, Node* adr_dest, Node* base_dest,\n-                            BasicType copy_type, const Type* value_type, int count);\n@@ -124,0 +122,4 @@\n+  void copy(GraphKit& kit, const TypeAryPtr* atp_src, const TypeAryPtr* atp_dest, int i,\n+            Node* base_src, Node* base_dest, Node* adr_src, Node* adr_dest,\n+            BasicType copy_type, const Type* value_type);\n+\n","filename":"src\/hotspot\/share\/opto\/arraycopynode.hpp","additions":13,"deletions":11,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -267,1 +267,1 @@\n-      if( t->is_ptr()->_offset == 0 ) { \/\/ Not derived?\n+      if (t->is_ptr()->offset() == 0) { \/\/ Not derived?\n@@ -272,1 +272,1 @@\n-          uint cnt = mcall->tf()->domain()->cnt();\n+          uint cnt = mcall->tf()->domain_cc()->cnt();\n@@ -342,1 +342,1 @@\n-        uint cnt = mcall->tf()->domain()->cnt();\n+        uint cnt = mcall->tf()->domain_cc()->cnt();\n","filename":"src\/hotspot\/share\/opto\/buildOopMap.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -88,1 +88,1 @@\n-  if (callee_method->is_object_initializer()) {\n+  if (callee_method->is_object_constructor()) {\n@@ -91,1 +91,1 @@\n-  if (caller_method->is_object_initializer() &&\n+  if ((caller_method->is_object_constructor() || caller_method->is_class_initializer()) &&\n","filename":"src\/hotspot\/share\/opto\/bytecodeInfo.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -126,1 +126,2 @@\n-  bool eliminate_boxing = EliminateAutoBox;\n+  \/\/ TODO 8328675 Re-enable\n+  bool eliminate_boxing = false; \/\/ EliminateAutoBox;\n@@ -645,0 +646,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:\n+  case vmIntrinsics::_finishPrivateBuffer:\n@@ -654,0 +657,1 @@\n+  case vmIntrinsics::_getValue:\n@@ -663,0 +667,1 @@\n+  case vmIntrinsics::_putValue:\n@@ -745,0 +750,1 @@\n+  case vmIntrinsics::_isFlatArray:\n@@ -746,0 +752,1 @@\n+  case vmIntrinsics::_newNullRestrictedArray:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -122,1 +123,1 @@\n-  \/\/ paths to facilitate late inlinig.\n+  \/\/ paths to facilitate late inlining.\n@@ -131,0 +132,1 @@\n+      _call_node(nullptr),\n@@ -133,0 +135,8 @@\n+    if (InlineTypeReturnedAsFields && method->is_method_handle_intrinsic()) {\n+      \/\/ If that call has not been optimized by the time optimizations are over,\n+      \/\/ we'll need to add a call to create an inline type instance from the klass\n+      \/\/ returned by the call (see PhaseMacroExpand::expand_mh_intrinsic_return).\n+      \/\/ Separating memory and I\/O projections for exceptions is required to\n+      \/\/ perform that graph transformation.\n+      _separate_io_proj = true;\n+    }\n@@ -147,0 +157,1 @@\n+  PhaseGVN& gvn = kit.gvn();\n@@ -179,1 +190,4 @@\n-  kit.set_arguments_for_java_call(call);\n+  kit.set_arguments_for_java_call(call, is_late_inline());\n+  if (kit.stopped()) {\n+    return kit.transfer_exceptions_into_jvms();\n+  }\n@@ -220,1 +234,0 @@\n-\n@@ -280,0 +293,3 @@\n+  if (kit.stopped()) {\n+    return kit.transfer_exceptions_into_jvms();\n+  }\n@@ -373,0 +389,4 @@\n+  virtual CallGenerator* inline_cg() {\n+    return _inline_cg;\n+  }\n+\n@@ -434,0 +454,8 @@\n+    \/\/ AlwaysIncrementalInline causes for_method_handle_inline() to\n+    \/\/ return a LateInlineCallGenerator. Extract the\n+    \/\/ InlineCallGenerator from it.\n+    if (AlwaysIncrementalInline && cg->is_late_inline() && !cg->is_virtual_late_inline()) {\n+      cg = cg->inline_cg();\n+      assert(cg != nullptr, \"inline call generator expected\");\n+    }\n+\n@@ -604,3 +632,3 @@\n-  const TypeTuple *r = call->tf()->domain();\n-  for (int i1 = 0; i1 < method()->arg_size(); i1++) {\n-    if (call->in(TypeFunc::Parms + i1)->is_top() && r->field_at(TypeFunc::Parms + i1) != Type::HALF) {\n+  const TypeTuple* r = call->tf()->domain_cc();\n+  for (uint i1 = TypeFunc::Parms; i1 < r->cnt(); i1++) {\n+    if (call->in(i1)->is_top() && r->field_at(i1) != Type::HALF) {\n@@ -624,1 +652,0 @@\n-  CallProjections callprojs;\n@@ -628,9 +655,8 @@\n-  call->extract_projections(&callprojs, true, do_asserts);\n-  if ((callprojs.fallthrough_catchproj == call->in(0)) ||\n-      (callprojs.catchall_catchproj    == call->in(0)) ||\n-      (callprojs.fallthrough_memproj   == call->in(TypeFunc::Memory)) ||\n-      (callprojs.catchall_memproj      == call->in(TypeFunc::Memory)) ||\n-      (callprojs.fallthrough_ioproj    == call->in(TypeFunc::I_O)) ||\n-      (callprojs.catchall_ioproj       == call->in(TypeFunc::I_O)) ||\n-      (callprojs.resproj != nullptr && call->find_edge(callprojs.resproj) != -1) ||\n-      (callprojs.exobj   != nullptr && call->find_edge(callprojs.exobj) != -1)) {\n+  CallProjections* callprojs = call->extract_projections(true, do_asserts);\n+  if ((callprojs->fallthrough_catchproj == call->in(0)) ||\n+      (callprojs->catchall_catchproj    == call->in(0)) ||\n+      (callprojs->fallthrough_memproj   == call->in(TypeFunc::Memory)) ||\n+      (callprojs->catchall_memproj      == call->in(TypeFunc::Memory)) ||\n+      (callprojs->fallthrough_ioproj    == call->in(TypeFunc::I_O)) ||\n+      (callprojs->catchall_ioproj       == call->in(TypeFunc::I_O)) ||\n+      (callprojs->exobj != nullptr && call->find_edge(callprojs->exobj) != -1)) {\n@@ -646,3 +672,12 @@\n-  \/\/ The call is marked as pure (no important side effects), but result isn't used.\n-  \/\/ It's safe to remove the call.\n-  bool result_not_used = (callprojs.resproj == nullptr || callprojs.resproj->outcnt() == 0);\n+\n+  bool result_not_used = true;\n+  for (uint i = 0; i < callprojs->nb_resproj; i++) {\n+    if (callprojs->resproj[i] != nullptr) {\n+      if (callprojs->resproj[i]->outcnt() != 0) {\n+        result_not_used = false;\n+      }\n+      if (call->find_edge(callprojs->resproj[i]) != -1) {\n+        return;\n+      }\n+    }\n+  }\n@@ -651,0 +686,2 @@\n+    \/\/ The call is marked as pure (no important side effects), but result isn't used.\n+    \/\/ It's safe to remove the call.\n@@ -663,0 +700,1 @@\n+    PhaseGVN& gvn = *C->initial_gvn();\n@@ -666,1 +704,1 @@\n-      C->initial_gvn()->set_type_bottom(mem);\n+      gvn.set_type_bottom(mem);\n@@ -670,4 +708,2 @@\n-    uint nargs = method()->arg_size();\n-    Node* top = C->top();\n-    for (uint i1 = 0; i1 < nargs; i1++) {\n-      map->set_req(TypeFunc::Parms + i1, top);\n+    for (uint i1 = TypeFunc::Parms; i1 < r->cnt(); i1++) {\n+      map->set_req(i1, C->top());\n@@ -681,0 +717,6 @@\n+    const TypeTuple* domain_sig = call->_tf->domain_sig();\n+    uint nargs = method()->arg_size();\n+    assert(domain_sig->cnt() - TypeFunc::Parms == nargs, \"inconsistent signature\");\n+\n+    uint j = TypeFunc::Parms;\n+    int arg_num = 0;\n@@ -682,1 +724,14 @@\n-      map->set_argument(jvms, i1, call->in(TypeFunc::Parms + i1));\n+      const Type* t = domain_sig->field_at(TypeFunc::Parms + i1);\n+      if (t->is_inlinetypeptr() && !method()->get_Method()->mismatch() && method()->is_scalarized_arg(arg_num)) {\n+        \/\/ Inline type arguments are not passed by reference: we get an argument per\n+        \/\/ field of the inline type. Build InlineTypeNodes from the inline type arguments.\n+        GraphKit arg_kit(jvms, &gvn);\n+        Node* vt = InlineTypeNode::make_from_multi(&arg_kit, call, t->inline_klass(), j, \/* in= *\/ true, \/* null_free= *\/ !t->maybe_null());\n+        map->set_control(arg_kit.control());\n+        map->set_argument(jvms, i1, vt);\n+      } else {\n+        map->set_argument(jvms, i1, call->in(j++));\n+      }\n+      if (t != Type::HALF) {\n+        arg_num++;\n+      }\n@@ -701,0 +756,20 @@\n+    \/\/ Check if we are late inlining a method handle call that returns an inline type as fields.\n+    Node* buffer_oop = nullptr;\n+    ciMethod* inline_method = inline_cg()->method();\n+    ciType* return_type = inline_method->return_type();\n+    if (!call->tf()->returns_inline_type_as_fields() && is_mh_late_inline() &&\n+        return_type->is_inlinetype() && return_type->as_inline_klass()->can_be_returned_as_fields()) {\n+      \/\/ Allocate a buffer for the inline type returned as fields because the caller expects an oop return.\n+      \/\/ Do this before the method handle call in case the buffer allocation triggers deoptimization and\n+      \/\/ we need to \"re-execute\" the call in the interpreter (to make sure the call is only executed once).\n+      GraphKit arg_kit(jvms, &gvn);\n+      {\n+        PreserveReexecuteState preexecs(&arg_kit);\n+        arg_kit.jvms()->set_should_reexecute(true);\n+        arg_kit.inc_sp(nargs);\n+        Node* klass_node = arg_kit.makecon(TypeKlassPtr::make(return_type->as_inline_klass()));\n+        buffer_oop = arg_kit.new_instance(klass_node, nullptr, nullptr, \/* deoptimize_on_exception *\/ true);\n+      }\n+      jvms = arg_kit.transfer_exceptions_into_jvms();\n+    }\n+\n@@ -729,2 +804,2 @@\n-      C->set_has_loops(C->has_loops() || inline_cg()->method()->has_loops());\n-      C->env()->notice_inlined_method(inline_cg()->method());\n+      C->set_has_loops(C->has_loops() || inline_method->has_loops());\n+      C->env()->notice_inlined_method(inline_method);\n@@ -734,0 +809,52 @@\n+\n+    \/\/ Handle inline type returns\n+    InlineTypeNode* vt = result->isa_InlineType();\n+    if (vt != nullptr) {\n+      if (call->tf()->returns_inline_type_as_fields()) {\n+        vt->replace_call_results(&kit, call, C);\n+      } else if (vt->is_InlineType()) {\n+        \/\/ Result might still be allocated (for example, if it has been stored to a non-flat field)\n+        if (!vt->is_allocated(&kit.gvn())) {\n+          assert(buffer_oop != nullptr, \"should have allocated a buffer\");\n+          RegionNode* region = new RegionNode(3);\n+\n+          \/\/ Check if result is null\n+          Node* null_ctl = kit.top();\n+          kit.null_check_common(vt->get_is_init(), T_INT, false, &null_ctl);\n+          region->init_req(1, null_ctl);\n+          PhiNode* oop = PhiNode::make(region, kit.gvn().zerocon(T_OBJECT), TypeInstPtr::make(TypePtr::BotPTR, vt->type()->inline_klass()));\n+          Node* init_mem = kit.reset_memory();\n+          PhiNode* mem = PhiNode::make(region, init_mem, Type::MEMORY, TypePtr::BOTTOM);\n+\n+          \/\/ Not null, initialize the buffer\n+          kit.set_all_memory(init_mem);\n+          vt->store(&kit, buffer_oop, buffer_oop, vt->type()->inline_klass());\n+          \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+          \/\/ store that would make this buffer accessible by other threads.\n+          AllocateNode* alloc = AllocateNode::Ideal_allocation(buffer_oop);\n+          assert(alloc != nullptr, \"must have an allocation node\");\n+          kit.insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out_or_null(AllocateNode::RawAddress));\n+          region->init_req(2, kit.control());\n+          oop->init_req(2, buffer_oop);\n+          mem->init_req(2, kit.merged_memory());\n+\n+          \/\/ Update oop input to buffer\n+          kit.gvn().hash_delete(vt);\n+          vt->set_oop(kit.gvn(), kit.gvn().transform(oop));\n+          vt->set_is_buffered(kit.gvn());\n+          vt = kit.gvn().transform(vt)->as_InlineType();\n+\n+          kit.set_control(kit.gvn().transform(region));\n+          kit.set_all_memory(kit.gvn().transform(mem));\n+          kit.record_for_igvn(region);\n+          kit.record_for_igvn(oop);\n+          kit.record_for_igvn(mem);\n+        }\n+        result = vt;\n+      }\n+      DEBUG_ONLY(buffer_oop = nullptr);\n+    } else {\n+      assert(result->is_top() || !call->tf()->returns_inline_type_as_fields(), \"Unexpected return value\");\n+    }\n+    assert(buffer_oop == nullptr, \"unused buffer allocation\");\n+\n@@ -956,0 +1083,23 @@\n+  \/\/ Allocate inline types if they are merged with objects (similar to Parse::merge_common())\n+  uint tos = kit.jvms()->stkoff() + kit.sp();\n+  uint limit = slow_map->req();\n+  for (uint i = TypeFunc::Parms; i < limit; i++) {\n+    Node* m = kit.map()->in(i);\n+    Node* n = slow_map->in(i);\n+    const Type* t = gvn.type(m)->meet_speculative(gvn.type(n));\n+    \/\/ TODO 8284443 still needed?\n+    if (m->is_InlineType() && !t->is_inlinetypeptr()) {\n+      \/\/ Allocate inline type in fast path\n+      m = m->as_InlineType()->buffer(&kit);\n+      kit.map()->set_req(i, m);\n+    }\n+    if (n->is_InlineType() && !t->is_inlinetypeptr()) {\n+      \/\/ Allocate inline type in slow path\n+      PreserveJVMState pjvms(&kit);\n+      kit.set_map(slow_map);\n+      n = n->as_InlineType()->buffer(&kit);\n+      kit.map()->set_req(i, n);\n+      slow_map = kit.stop();\n+    }\n+  }\n+\n@@ -979,2 +1129,0 @@\n-  uint tos = kit.jvms()->stkoff() + kit.sp();\n-  uint limit = slow_map->req();\n@@ -1017,2 +1165,2 @@\n-  if (IncrementalInlineMH && call_site_count > 0 &&\n-      (should_delay || input_not_const || !C->inlining_incrementally() || C->over_inlining_cutoff())) {\n+  if (IncrementalInlineMH && (AlwaysIncrementalInline ||\n+                            (call_site_count > 0 && (should_delay || input_not_const || !C->inlining_incrementally() || C->over_inlining_cutoff())))) {\n@@ -1026,0 +1174,1 @@\n+\n@@ -1077,0 +1226,1 @@\n+      int nargs = callee->arg_size();\n@@ -1078,1 +1228,1 @@\n-      Node* member_name = kit.argument(callee->arg_size() - 1);\n+      Node* member_name = kit.argument(nargs - 1);\n@@ -1151,1 +1301,2 @@\n-                                              speculative_receiver_type);\n+                                              speculative_receiver_type,\n+                                              true);\n@@ -1226,1 +1377,1 @@\n-    Node* receiver = kit.null_check_receiver_before_call(method());\n+    kit.null_check_receiver_before_call(method());\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":185,"deletions":34,"binary":false,"changes":219,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -36,0 +37,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -45,0 +47,1 @@\n+#include \"runtime\/stubRoutines.hpp\"\n@@ -80,1 +83,1 @@\n-Node *StartNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *StartNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -104,11 +107,0 @@\n-\/\/------------------------------StartOSRNode----------------------------------\n-\/\/ The method start node for an on stack replacement adapter\n-\n-\/\/------------------------------osr_domain-----------------------------\n-const TypeTuple *StartOSRNode::osr_domain() {\n-  const Type **fields = TypeTuple::fields(2);\n-  fields[TypeFunc::Parms+0] = TypeRawPtr::BOTTOM;  \/\/ address of osr buffer\n-\n-  return TypeTuple::make(TypeFunc::Parms+1, fields);\n-}\n-\n@@ -500,0 +492,8 @@\n+      } else if (cik->is_flat_array_klass()) {\n+        ciKlass* cie = cik->as_flat_array_klass()->base_element_klass();\n+        cie->print_name_on(st);\n+        st->print(\"[%d]\", spobj->n_fields());\n+        int ndim = cik->as_array_klass()->dimension() - 1;\n+        while (ndim-- > 0) {\n+          st->print(\"[]\");\n+        }\n@@ -505,0 +505,7 @@\n+        if (iklass != nullptr && iklass->is_inlinetype()) {\n+          Node* init_node = mcall->in(first_ind++);\n+          if (!init_node->is_top()) {\n+            st->print(\" [is_init\");\n+            format_helper(regalloc, st, init_node, \":\", -1, nullptr);\n+          }\n+        }\n@@ -509,2 +516,7 @@\n-          cifield = iklass->nonstatic_field_at(0);\n-          cifield->print_name_on(st);\n+          if (0 < (uint)iklass->nof_nonstatic_fields()) {\n+            cifield = iklass->nonstatic_field_at(0);\n+            cifield->print_name_on(st);\n+          } else {\n+            \/\/ Must be a null marker\n+            st->print(\"null marker\");\n+          }\n@@ -519,2 +531,7 @@\n-            cifield = iklass->nonstatic_field_at(j);\n-            cifield->print_name_on(st);\n+            if (j < (uint)iklass->nof_nonstatic_fields()) {\n+              cifield = iklass->nonstatic_field_at(j);\n+              cifield->print_name_on(st);\n+            } else {\n+              \/\/ Must be a null marker\n+              st->print(\"null marker\");\n+            }\n@@ -718,1 +735,1 @@\n-const Type *CallNode::bottom_type() const { return tf()->range(); }\n+const Type *CallNode::bottom_type() const { return tf()->range_cc(); }\n@@ -723,1 +740,1 @@\n-  return tf()->range();\n+  return tf()->range_cc();\n@@ -728,0 +745,7 @@\n+  if (_entry_point == StubRoutines::store_inline_type_fields_to_buf()) {\n+    \/\/ The call to that stub is a special case: its inputs are\n+    \/\/ multiple values returned from a call and so it should follow\n+    \/\/ the return convention.\n+    SharedRuntime::java_return_convention(sig_bt, parm_regs, argcnt);\n+    return;\n+  }\n@@ -736,27 +760,26 @@\n-Node *CallNode::match( const ProjNode *proj, const Matcher *match ) {\n-  switch (proj->_con) {\n-  case TypeFunc::Control:\n-  case TypeFunc::I_O:\n-  case TypeFunc::Memory:\n-    return new MachProjNode(this,proj->_con,RegMask::Empty,MachProjNode::unmatched_proj);\n-\n-  case TypeFunc::Parms+1:       \/\/ For LONG & DOUBLE returns\n-    assert(tf()->range()->field_at(TypeFunc::Parms+1) == Type::HALF, \"\");\n-    \/\/ 2nd half of doubles and longs\n-    return new MachProjNode(this,proj->_con, RegMask::Empty, (uint)OptoReg::Bad);\n-\n-  case TypeFunc::Parms: {       \/\/ Normal returns\n-    uint ideal_reg = tf()->range()->field_at(TypeFunc::Parms)->ideal_reg();\n-    OptoRegPair regs = Opcode() == Op_CallLeafVector\n-      ? match->vector_return_value(ideal_reg)      \/\/ Calls into assembly vector routine\n-      : is_CallRuntime()\n-        ? match->c_return_value(ideal_reg)  \/\/ Calls into C runtime\n-        : match->  return_value(ideal_reg); \/\/ Calls into compiled Java code\n-    RegMask rm = RegMask(regs.first());\n-\n-    if (Opcode() == Op_CallLeafVector) {\n-      \/\/ If the return is in vector, compute appropriate regmask taking into account the whole range\n-      if(ideal_reg >= Op_VecA && ideal_reg <= Op_VecZ) {\n-        if(OptoReg::is_valid(regs.second())) {\n-          for (OptoReg::Name r = regs.first(); r <= regs.second(); r = OptoReg::add(r, 1)) {\n-            rm.Insert(r);\n+Node *CallNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n+  uint con = proj->_con;\n+  const TypeTuple* range_cc = tf()->range_cc();\n+  if (con >= TypeFunc::Parms) {\n+    if (tf()->returns_inline_type_as_fields()) {\n+      \/\/ The call returns multiple values (inline type fields): we\n+      \/\/ create one projection per returned value.\n+      assert(con <= TypeFunc::Parms+1 || InlineTypeReturnedAsFields, \"only for multi value return\");\n+      uint ideal_reg = range_cc->field_at(con)->ideal_reg();\n+      return new MachProjNode(this, con, mask[con-TypeFunc::Parms], ideal_reg);\n+    } else {\n+      if (con == TypeFunc::Parms) {\n+        uint ideal_reg = range_cc->field_at(TypeFunc::Parms)->ideal_reg();\n+        OptoRegPair regs = Opcode() == Op_CallLeafVector\n+          ? match->vector_return_value(ideal_reg)      \/\/ Calls into assembly vector routine\n+          : match->c_return_value(ideal_reg);\n+        RegMask rm = RegMask(regs.first());\n+\n+        if (Opcode() == Op_CallLeafVector) {\n+          \/\/ If the return is in vector, compute appropriate regmask taking into account the whole range\n+          if(ideal_reg >= Op_VecA && ideal_reg <= Op_VecZ) {\n+            if(OptoReg::is_valid(regs.second())) {\n+              for (OptoReg::Name r = regs.first(); r <= regs.second(); r = OptoReg::add(r, 1)) {\n+                rm.Insert(r);\n+              }\n+            }\n@@ -765,0 +788,9 @@\n+\n+        if (OptoReg::is_valid(regs.second())) {\n+          rm.Insert(regs.second());\n+        }\n+        return new MachProjNode(this,con,rm,ideal_reg);\n+      } else {\n+        assert(con == TypeFunc::Parms+1, \"only one return value\");\n+        assert(range_cc->field_at(TypeFunc::Parms+1) == Type::HALF, \"\");\n+        return new MachProjNode(this,con, RegMask::Empty, (uint)OptoReg::Bad);\n@@ -767,4 +799,0 @@\n-\n-    if( OptoReg::is_valid(regs.second()) )\n-      rm.Insert( regs.second() );\n-    return new MachProjNode(this,proj->_con,rm,ideal_reg);\n@@ -773,0 +801,6 @@\n+  switch (con) {\n+  case TypeFunc::Control:\n+  case TypeFunc::I_O:\n+  case TypeFunc::Memory:\n+    return new MachProjNode(this,proj->_con,RegMask::Empty,MachProjNode::unmatched_proj);\n+\n@@ -793,1 +827,1 @@\n-    const TypeTuple* args = _tf->domain();\n+    const TypeTuple* args = _tf->domain_sig();\n@@ -842,1 +876,1 @@\n-      const TypeTuple* d = tf()->domain();\n+      const TypeTuple* d = tf()->domain_cc();\n@@ -857,2 +891,2 @@\n-bool CallNode::has_non_debug_use(Node *n) {\n-  const TypeTuple * d = tf()->domain();\n+bool CallNode::has_non_debug_use(Node* n) {\n+  const TypeTuple* d = tf()->domain_cc();\n@@ -860,2 +894,1 @@\n-    Node *arg = in(i);\n-    if (arg == n) {\n+    if (in(i) == n) {\n@@ -868,0 +901,11 @@\n+bool CallNode::has_debug_use(Node* n) {\n+  if (jvms() != nullptr) {\n+    for (uint i = jvms()->debug_start(); i < jvms()->debug_end(); i++) {\n+      if (in(i) == n) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -899,10 +943,15 @@\n-void CallNode::extract_projections(CallProjections* projs, bool separate_io_proj, bool do_asserts) {\n-  projs->fallthrough_proj      = nullptr;\n-  projs->fallthrough_catchproj = nullptr;\n-  projs->fallthrough_ioproj    = nullptr;\n-  projs->catchall_ioproj       = nullptr;\n-  projs->catchall_catchproj    = nullptr;\n-  projs->fallthrough_memproj   = nullptr;\n-  projs->catchall_memproj      = nullptr;\n-  projs->resproj               = nullptr;\n-  projs->exobj                 = nullptr;\n+CallProjections* CallNode::extract_projections(bool separate_io_proj, bool do_asserts) {\n+  uint max_res = TypeFunc::Parms-1;\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    ProjNode *pn = fast_out(i)->as_Proj();\n+    max_res = MAX2(max_res, pn->_con);\n+  }\n+\n+  assert(max_res < _tf->range_cc()->cnt(), \"result out of bounds\");\n+\n+  uint projs_size = sizeof(CallProjections);\n+  if (max_res > TypeFunc::Parms) {\n+    projs_size += (max_res-TypeFunc::Parms)*sizeof(Node*);\n+  }\n+  char* projs_storage = resource_allocate_bytes(projs_size);\n+  CallProjections* projs = new(projs_storage)CallProjections(max_res - TypeFunc::Parms + 1);\n@@ -954,1 +1003,1 @@\n-      projs->resproj = pn;\n+      projs->resproj[0] = pn;\n@@ -957,1 +1006,3 @@\n-      assert(false, \"unexpected projection from allocation node.\");\n+      assert(pn->_con <= max_res, \"unexpected projection from allocation node.\");\n+      projs->resproj[pn->_con-TypeFunc::Parms] = pn;\n+      break;\n@@ -964,1 +1015,1 @@\n-  assert(projs->fallthrough_proj      != nullptr, \"must be found\");\n+  assert(!do_asserts || projs->fallthrough_proj      != nullptr, \"must be found\");\n@@ -974,0 +1025,1 @@\n+  return projs;\n@@ -1005,2 +1057,2 @@\n-  uint old_dbg_start = sfpt->is_Call() ? sfpt->as_Call()->tf()->domain()->cnt() : (uint)TypeFunc::Parms+1;\n-  uint new_dbg_start = tf()->domain()->cnt();\n+  uint old_dbg_start = sfpt->is_Call() ? sfpt->as_Call()->tf()->domain_sig()->cnt() : (uint)TypeFunc::Parms+1;\n+  uint new_dbg_start = tf()->domain_sig()->cnt();\n@@ -1047,0 +1099,4 @@\n+  Bytecodes::Code bc = jvms()->method()->java_code_at_bci(jvms()->bci());\n+  if (EnableValhalla && (bc == Bytecodes::_if_acmpeq || bc == Bytecodes::_if_acmpne)) {\n+    return true;\n+  }\n@@ -1080,0 +1136,10 @@\n+  if (can_reshape && uncommon_trap_request() != 0) {\n+    PhaseIterGVN* igvn = phase->is_IterGVN();\n+    if (remove_unknown_flat_array_load(igvn, in(0), in(TypeFunc::Memory), in(TypeFunc::Parms))) {\n+      if (!in(0)->is_Region()) {\n+        igvn->replace_input_of(this, 0, phase->C->top());\n+      }\n+      return this;\n+    }\n+  }\n+\n@@ -1132,0 +1198,122 @@\n+\/\/ Split if can cause the flat array branch of an array load with unknown type (see\n+\/\/ Parse::array_load) to end in an uncommon trap. In that case, the call to\n+\/\/ 'load_unknown_inline' is useless. Replace it with an uncommon trap with the same JVMState.\n+bool CallStaticJavaNode::remove_unknown_flat_array_load(PhaseIterGVN* igvn, Node* ctl, Node* mem, Node* unc_arg) {\n+  if (ctl == nullptr || ctl->is_top() || mem == nullptr || mem->is_top() || !mem->is_MergeMem()) {\n+    return false;\n+  }\n+  if (ctl->is_Region()) {\n+    bool res = false;\n+    for (uint i = 1; i < ctl->req(); i++) {\n+      MergeMemNode* mm = mem->clone()->as_MergeMem();\n+      for (MergeMemStream mms(mm); mms.next_non_empty(); ) {\n+        Node* m = mms.memory();\n+        if (m->is_Phi() && m->in(0) == ctl) {\n+          mms.set_memory(m->in(i));\n+        }\n+      }\n+      if (remove_unknown_flat_array_load(igvn, ctl->in(i), mm, unc_arg)) {\n+        res = true;\n+        if (!ctl->in(i)->is_Region()) {\n+          igvn->replace_input_of(ctl, i, igvn->C->top());\n+        }\n+      }\n+      igvn->remove_dead_node(mm);\n+    }\n+    return res;\n+  }\n+  \/\/ Verify the control flow is ok\n+  Node* call = ctl;\n+  MemBarNode* membar = nullptr;\n+  for (;;) {\n+    if (call == nullptr || call->is_top()) {\n+      return false;\n+    }\n+    if (call->is_Proj() || call->is_Catch() || call->is_MemBar()) {\n+      call = call->in(0);\n+    } else if (call->Opcode() == Op_CallStaticJava && !call->in(0)->is_top() &&\n+               call->as_Call()->entry_point() == OptoRuntime::load_unknown_inline_Java()) {\n+      assert(call->in(0)->is_Proj() && call->in(0)->in(0)->is_MemBar(), \"missing membar\");\n+      membar = call->in(0)->in(0)->as_MemBar();\n+      break;\n+    } else {\n+      return false;\n+    }\n+  }\n+\n+  JVMState* jvms = call->jvms();\n+  if (igvn->C->too_many_traps(jvms->method(), jvms->bci(), Deoptimization::trap_request_reason(uncommon_trap_request()))) {\n+    return false;\n+  }\n+\n+  Node* call_mem = call->in(TypeFunc::Memory);\n+  if (call_mem == nullptr || call_mem->is_top()) {\n+    return false;\n+  }\n+  if (!call_mem->is_MergeMem()) {\n+    call_mem = MergeMemNode::make(call_mem);\n+    igvn->register_new_node_with_optimizer(call_mem);\n+  }\n+\n+  \/\/ Verify that there's no unexpected side effect\n+  for (MergeMemStream mms2(mem->as_MergeMem(), call_mem->as_MergeMem()); mms2.next_non_empty2(); ) {\n+    Node* m1 = mms2.is_empty() ? mms2.base_memory() : mms2.memory();\n+    Node* m2 = mms2.memory2();\n+\n+    for (uint i = 0; i < 100; i++) {\n+      if (m1 == m2) {\n+        break;\n+      } else if (m1->is_Proj()) {\n+        m1 = m1->in(0);\n+      } else if (m1->is_MemBar()) {\n+        m1 = m1->in(TypeFunc::Memory);\n+      } else if (m1->Opcode() == Op_CallStaticJava &&\n+                 m1->as_Call()->entry_point() == OptoRuntime::load_unknown_inline_Java()) {\n+        if (m1 != call) {\n+          return false;\n+        }\n+        break;\n+      } else if (m1->is_MergeMem()) {\n+        MergeMemNode* mm = m1->as_MergeMem();\n+        int idx = mms2.alias_idx();\n+        if (idx == Compile::AliasIdxBot) {\n+          m1 = mm->base_memory();\n+        } else {\n+          m1 = mm->memory_at(idx);\n+        }\n+      } else {\n+        return false;\n+      }\n+    }\n+  }\n+  if (call_mem->outcnt() == 0) {\n+    igvn->remove_dead_node(call_mem);\n+  }\n+\n+  \/\/ Remove membar preceding the call\n+  membar->remove(igvn);\n+\n+  address call_addr = OptoRuntime::uncommon_trap_blob()->entry_point();\n+  CallNode* unc = new CallStaticJavaNode(OptoRuntime::uncommon_trap_Type(), call_addr, \"uncommon_trap\", nullptr);\n+  unc->init_req(TypeFunc::Control, call->in(0));\n+  unc->init_req(TypeFunc::I_O, call->in(TypeFunc::I_O));\n+  unc->init_req(TypeFunc::Memory, call->in(TypeFunc::Memory));\n+  unc->init_req(TypeFunc::FramePtr,  call->in(TypeFunc::FramePtr));\n+  unc->init_req(TypeFunc::ReturnAdr, call->in(TypeFunc::ReturnAdr));\n+  unc->init_req(TypeFunc::Parms+0, unc_arg);\n+  unc->set_cnt(PROB_UNLIKELY_MAG(4));\n+  unc->copy_call_debug_info(igvn, call->as_CallStaticJava());\n+\n+  \/\/ Replace the call with an uncommon trap\n+  igvn->replace_input_of(call, 0, igvn->C->top());\n+\n+  igvn->register_new_node_with_optimizer(unc);\n+\n+  Node* ctrl = igvn->transform(new ProjNode(unc, TypeFunc::Control));\n+  Node* halt = igvn->transform(new HaltNode(ctrl, call->in(TypeFunc::FramePtr), \"uncommon trap returned which should never happen\"));\n+  igvn->add_input_to(igvn->C->root(), halt);\n+\n+  return true;\n+}\n+\n+\n@@ -1236,0 +1424,7 @@\n+  if (_entry_point == nullptr) {\n+    \/\/ The call to that stub is a special case: its inputs are\n+    \/\/ multiple values returned from a call and so it should follow\n+    \/\/ the return convention.\n+    SharedRuntime::java_return_convention(sig_bt, parm_regs, argcnt);\n+    return;\n+  }\n@@ -1241,1 +1436,1 @@\n-  assert(tf()->range()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte == _num_bits,\n+  assert(tf()->range_sig()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte == _num_bits,\n@@ -1243,1 +1438,1 @@\n-  const TypeTuple* d = tf()->domain();\n+  const TypeTuple* d = tf()->domain_sig();\n@@ -1267,0 +1462,6 @@\n+uint CallLeafNoFPNode::match_edge(uint idx) const {\n+  \/\/ Null entry point is a special case for which the target is in a\n+  \/\/ register. Need to match that edge.\n+  return entry_point() == nullptr && idx == TypeFunc::Parms;\n+}\n+\n@@ -1317,1 +1518,14 @@\n-  return remove_dead_region(phase, can_reshape) ? this : nullptr;\n+  if (remove_dead_region(phase, can_reshape)) {\n+    return this;\n+  }\n+  \/\/ Scalarize inline types in safepoint debug info.\n+  \/\/ Delay this until all inlining is over to avoid getting inconsistent debug info.\n+  if (phase->C->scalarize_in_safepoints() && can_reshape && jvms() != nullptr) {\n+    for (uint i = jvms()->debug_start(); i < jvms()->debug_end(); i++) {\n+      Node* n = in(i)->uncast();\n+      if (n->is_InlineType()) {\n+        n->as_InlineType()->make_scalar_in_safepoints(phase->is_IterGVN());\n+      }\n+    }\n+  }\n+  return nullptr;\n@@ -1475,1 +1689,1 @@\n-  if (!alloc->is_Allocate() && !(alloc->Opcode() == Op_VectorBox)) {\n+  if (alloc != nullptr && !alloc->is_Allocate() && !(alloc->Opcode() == Op_VectorBox)) {\n@@ -1580,1 +1794,3 @@\n-                           Node *size, Node *klass_node, Node *initial_test)\n+                           Node *size, Node *klass_node,\n+                           Node* initial_test,\n+                           InlineTypeNode* inline_type_node)\n@@ -1588,0 +1804,1 @@\n+  _larval = false;\n@@ -1600,0 +1817,3 @@\n+  init_req( InlineType     , inline_type_node);\n+  \/\/ DefaultValue defaults to nullptr\n+  \/\/ RawDefaultValue defaults to nullptr\n@@ -1605,1 +1825,2 @@\n-  assert(initializer != nullptr && initializer->is_object_initializer(),\n+  assert(initializer != nullptr &&\n+         (initializer->is_object_constructor() || initializer->is_class_initializer()),\n@@ -1617,1 +1838,2 @@\n-Node *AllocateNode::make_ideal_mark(PhaseGVN *phase, Node* obj, Node* control, Node* mem) {\n+\n+Node* AllocateNode::make_ideal_mark(PhaseGVN* phase, Node* control, Node* mem) {\n@@ -1619,1 +1841,1 @@\n-  if (UseCompactObjectHeaders) {\n+  if (UseCompactObjectHeaders || EnableValhalla) {\n@@ -1623,0 +1845,6 @@\n+    if (EnableValhalla) {\n+        mark_node = phase->transform(mark_node);\n+        \/\/ Avoid returning a constant (old node) here because this method is used by LoadNode::Ideal\n+        mark_node = new OrXNode(mark_node, phase->MakeConX(_larval ? markWord::larval_bit_in_place : 0));\n+    }\n+    return mark_node;\n@@ -1624,2 +1852,1 @@\n-    \/\/ For now only enable fast locking for non-array types\n-    mark_node = phase->MakeConX(markWord::prototype().value());\n+    return phase->MakeConX(markWord::prototype().value());\n@@ -1627,1 +1854,0 @@\n-  return mark_node;\n@@ -2023,1 +2249,2 @@\n-  if (can_reshape && EliminateLocks && !is_non_esc_obj()) {\n+  const Type* obj_type = phase->type(obj_node());\n+  if (can_reshape && EliminateLocks && !is_non_esc_obj() && !obj_type->is_inlinetypeptr()) {\n@@ -2224,1 +2451,2 @@\n-  if (can_reshape && EliminateLocks && !is_non_esc_obj()) {\n+  const Type* obj_type = phase->type(obj_node());\n+  if (can_reshape && EliminateLocks && !is_non_esc_obj() && !obj_type->is_inlinetypeptr()) {\n@@ -2304,1 +2532,2 @@\n-    dest_t = dest_t->add_offset(Type::OffsetBot)->is_oopptr();\n+    dest_t = dest_t->is_aryptr()->with_field_offset(Type::OffsetBot)->add_offset(Type::OffsetBot)->is_oopptr();\n+    t_oop = t_oop->is_aryptr()->with_field_offset(Type::OffsetBot);\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":313,"deletions":84,"binary":false,"changes":397,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-  virtual Node *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node *match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n@@ -93,1 +93,0 @@\n-  static  const TypeTuple *osr_domain();\n@@ -656,1 +655,1 @@\n-class CallProjections : public StackObj {\n+class CallProjections {\n@@ -665,1 +664,19 @@\n-  Node* resproj;\n+  uint nb_resproj;\n+  Node* resproj[1]; \/\/ at least one projection\n+\n+  CallProjections(uint nbres) {\n+    fallthrough_proj      = nullptr;\n+    fallthrough_catchproj = nullptr;\n+    fallthrough_memproj   = nullptr;\n+    fallthrough_ioproj    = nullptr;\n+    catchall_catchproj    = nullptr;\n+    catchall_memproj      = nullptr;\n+    catchall_ioproj       = nullptr;\n+    exobj                 = nullptr;\n+    nb_resproj            = nbres;\n+    resproj[0]            = nullptr;\n+    for (uint i = 1; i < nb_resproj; i++) {\n+      resproj[i]          = nullptr;\n+    }\n+  }\n+\n@@ -688,1 +705,1 @@\n-    : SafePointNode(tf->domain()->cnt(), jvms, adr_type),\n+    : SafePointNode(tf->domain_cc()->cnt(), jvms, adr_type),\n@@ -715,1 +732,1 @@\n-  virtual Node*       match(const ProjNode* proj, const Matcher* m);\n+  virtual Node*       match(const ProjNode* proj, const Matcher* m, const RegMask* mask);\n@@ -729,0 +746,1 @@\n+  bool                has_debug_use(Node* n);\n@@ -735,2 +753,3 @@\n-    const TypeTuple* r = tf()->range();\n-    return (r->cnt() > TypeFunc::Parms &&\n+    const TypeTuple* r = tf()->range_sig();\n+    return (!tf()->returns_inline_type_as_fields() &&\n+            r->cnt() > TypeFunc::Parms &&\n@@ -743,1 +762,1 @@\n-  void extract_projections(CallProjections* projs, bool separate_io_proj, bool do_asserts = true);\n+  CallProjections* extract_projections(bool separate_io_proj, bool do_asserts = true);\n@@ -813,0 +832,3 @@\n+\n+  bool remove_unknown_flat_array_load(PhaseIterGVN* igvn, Node* ctl, Node* mem, Node* unc_arg);\n+\n@@ -821,0 +843,11 @@\n+    const TypeTuple *r = tf->range_sig();\n+    if (InlineTypeReturnedAsFields &&\n+        method != nullptr &&\n+        method->is_method_handle_intrinsic() &&\n+        r->cnt() > TypeFunc::Parms &&\n+        r->field_at(TypeFunc::Parms)->isa_oopptr() &&\n+        r->field_at(TypeFunc::Parms)->is_oopptr()->can_be_inline_type()) {\n+      \/\/ Make sure this call is processed by PhaseMacroExpand::expand_mh_intrinsic_return\n+      init_flags(Flag_is_macro);\n+      C->add_macro_node(this);\n+    }\n@@ -931,0 +964,1 @@\n+  virtual uint match_edge(uint idx) const;\n@@ -973,0 +1007,3 @@\n+    InlineType,                       \/\/ InlineTypeNode if this is an inline type allocation\n+    DefaultValue,                     \/\/ default value in case of non-flat inline type array\n+    RawDefaultValue,                  \/\/ same as above but as raw machine word\n@@ -983,0 +1020,3 @@\n+    fields[InlineType] = Type::BOTTOM;\n+    fields[DefaultValue] = TypeInstPtr::NOTNULL;\n+    fields[RawDefaultValue] = TypeX_X;\n@@ -1000,0 +1040,1 @@\n+  bool _larval;\n@@ -1003,1 +1044,2 @@\n-               Node *size, Node *klass_node, Node *initial_test);\n+               Node *size, Node *klass_node, Node *initial_test,\n+               InlineTypeNode* inline_type_node = nullptr);\n@@ -1068,1 +1110,1 @@\n-  Node* make_ideal_mark(PhaseGVN *phase, Node* obj, Node* control, Node* mem);\n+  Node* make_ideal_mark(PhaseGVN* phase, Node* control, Node* mem);\n@@ -1078,1 +1120,2 @@\n-                    Node* initial_test, Node* count_val, Node* valid_length_test)\n+                    Node* initial_test, Node* count_val, Node* valid_length_test,\n+                    Node* default_value, Node* raw_default_value)\n@@ -1085,0 +1128,2 @@\n+    init_req(AllocateNode::DefaultValue,  default_value);\n+    init_req(AllocateNode::RawDefaultValue, raw_default_value);\n@@ -1086,0 +1131,1 @@\n+  virtual uint size_of() const { return sizeof(*this); }\n","filename":"src\/hotspot\/share\/opto\/callnode.hpp","additions":58,"deletions":12,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+#include \"opto\/graphKit.hpp\"\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -31,0 +33,1 @@\n+#include \"opto\/rootnode.hpp\"\n@@ -100,1 +103,18 @@\n-  return (in(0) && remove_dead_region(phase, can_reshape)) ? this : nullptr;\n+  if (in(0) && remove_dead_region(phase, can_reshape)) {\n+    return this;\n+  }\n+\n+  \/\/ Push cast through InlineTypeNode\n+  InlineTypeNode* vt = in(1)->isa_InlineType();\n+  if (vt != nullptr && phase->type(vt)->filter_speculative(_type) != Type::TOP) {\n+    Node* cast = clone();\n+    cast->set_req(1, vt->get_oop());\n+    vt = vt->clone()->as_InlineType();\n+    if (!_type->maybe_null()) {\n+      vt->as_InlineType()->set_is_init(*phase);\n+    }\n+    vt->set_oop(*phase, phase->transform(cast));\n+    return vt;\n+  }\n+\n+  return nullptr;\n@@ -352,0 +372,10 @@\n+\/\/=============================================================================\n+\/\/------------------------------Identity---------------------------------------\n+\/\/ If input is already higher or equal to cast type, then this is an identity.\n+Node* CheckCastPPNode::Identity(PhaseGVN* phase) {\n+  if (in(1)->is_InlineType() && _type->isa_instptr() && phase->type(in(1))->inline_klass()->is_subtype_of(_type->is_instptr()->instance_klass())) {\n+    return in(1);\n+  }\n+  return ConstraintCastNode::Identity(phase);\n+}\n+\n@@ -368,0 +398,10 @@\n+    \/\/ TODO 8302672\n+    if (!StressReflectiveCode && my_type->isa_aryptr() && in_type->isa_aryptr()) {\n+      \/\/ Propagate array properties (not flat\/null-free)\n+      \/\/ Don't do this when StressReflectiveCode is enabled because it might lead to\n+      \/\/ a dying data path while the corresponding flat\/null-free check is not folded.\n+      my_type = my_type->is_aryptr()->update_properties(in_type->is_aryptr());\n+      if (my_type == nullptr) {\n+        return Type::TOP; \/\/ Inconsistent properties\n+      }\n+    }\n@@ -372,1 +412,1 @@\n-      result =  my_type->cast_to_ptr_type(my_type->join_ptr(in_ptr));\n+      result = my_type->cast_to_ptr_type(my_type->join_ptr(in_ptr));\n@@ -459,0 +499,16 @@\n+\n+  if (t->is_zero_type() || !t->maybe_null()) {\n+    for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+      Node* u = fast_out(i);\n+      if (u->Opcode() == Op_OrL) {\n+        for (DUIterator_Fast jmax, j = u->fast_outs(jmax); j < jmax; j++) {\n+          Node* cmp = u->fast_out(j);\n+          if (cmp->Opcode() == Op_CmpL) {\n+            \/\/ Give CmpL a chance to get optimized\n+            phase->record_for_igvn(cmp);\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/castnode.cpp","additions":58,"deletions":2,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -520,0 +521,1 @@\n+\n@@ -966,1 +968,2 @@\n-             cmp1->is_SubTypeCheck() || cmp2->is_SubTypeCheck()) {\n+             cmp1->is_SubTypeCheck() || cmp2->is_SubTypeCheck() ||\n+             cmp1->is_FlatArrayCheck() || cmp2->is_FlatArrayCheck()) {\n@@ -1045,1 +1048,1 @@\n-\/\/ note that these functions assume that the _adr_type field is flattened\n+\/\/ note that these functions assume that the _adr_type field is flat\n@@ -1063,1 +1066,1 @@\n-  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at), \"flatten at\");\n+  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at) || (flatten_phi_adr_type(at) == TypeAryPtr::INLINES && Compile::current()->flat_accesses_share_alias()), \"flatten at\");\n@@ -1191,0 +1194,8 @@\n+  \/\/ Flat array element shouldn't get their own memory slice until flat_accesses_share_alias is cleared.\n+  \/\/ It could be the graph has no loads\/stores and flat_accesses_share_alias is never cleared. EA could still\n+  \/\/ creates per element Phis but that wouldn't be a problem as there are no memory accesses for that array.\n+  assert(_adr_type == nullptr || _adr_type->isa_aryptr() == nullptr ||\n+         _adr_type->is_aryptr()->is_known_instance() ||\n+         !_adr_type->is_aryptr()->is_flat() ||\n+         !Compile::current()->flat_accesses_share_alias() ||\n+         _adr_type == TypeAryPtr::INLINES, \"flat array element shouldn't get its own slice yet\");\n@@ -1409,0 +1420,1 @@\n+\n@@ -2029,0 +2041,46 @@\n+\/\/ Push inline type input nodes (and null) down through the phi recursively (can handle data loops).\n+InlineTypeNode* PhiNode::push_inline_types_down(PhaseGVN* phase, bool can_reshape, ciInlineKlass* inline_klass) {\n+  assert(inline_klass != nullptr, \"must be\");\n+  InlineTypeNode* vt = InlineTypeNode::make_null(*phase, inline_klass, \/* transform = *\/ false)->clone_with_phis(phase, in(0), nullptr, !_type->maybe_null());\n+  if (can_reshape) {\n+    \/\/ Replace phi right away to be able to use the inline\n+    \/\/ type node when reaching the phi again through data loops.\n+    PhaseIterGVN* igvn = phase->is_IterGVN();\n+    for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+      Node* u = fast_out(i);\n+      igvn->rehash_node_delayed(u);\n+      imax -= u->replace_edge(this, vt);\n+      --i;\n+    }\n+    igvn->rehash_node_delayed(this);\n+    assert(outcnt() == 0, \"should be dead now\");\n+  }\n+  ResourceMark rm;\n+  Node_List casts;\n+  for (uint i = 1; i < req(); ++i) {\n+    Node* n = in(i);\n+    while (n->is_ConstraintCast()) {\n+      casts.push(n);\n+      n = n->in(1);\n+    }\n+    if (phase->type(n)->is_zero_type()) {\n+      n = InlineTypeNode::make_null(*phase, inline_klass);\n+    } else if (n->is_Phi()) {\n+      assert(can_reshape, \"can only handle phis during IGVN\");\n+      n = phase->transform(n->as_Phi()->push_inline_types_down(phase, can_reshape, inline_klass));\n+    }\n+    while (casts.size() != 0) {\n+      \/\/ Push the cast(s) through the InlineTypeNode\n+      \/\/ TODO 8302217 Can we avoid cloning? See InlineTypeNode::clone_if_required\n+      Node* cast = casts.pop()->clone();\n+      cast->set_req_X(1, n->as_InlineType()->get_oop(), phase);\n+      n = n->clone();\n+      n->as_InlineType()->set_oop(*phase, phase->transform(cast));\n+      n = phase->transform(n);\n+    }\n+    bool transform = !can_reshape && (i == (req()-1)); \/\/ Transform phis on last merge\n+    vt->merge_with(phase, n->as_InlineType(), i, transform);\n+  }\n+  return vt;\n+}\n+\n@@ -2361,0 +2419,2 @@\n+    \/\/ TODO revisit this with JDK-8247216\n+    bool mergemem_only = true;\n@@ -2373,0 +2433,2 @@\n+      } else {\n+        mergemem_only = false;\n@@ -2377,1 +2439,1 @@\n-    if (!saw_self && adr_type() == TypePtr::BOTTOM)  merge_width = 0;\n+    if (!mergemem_only && !saw_self && adr_type() == TypePtr::BOTTOM)  merge_width = 0;\n@@ -2448,0 +2510,5 @@\n+            if (igvn) {\n+              \/\/ TODO revisit this with JDK-8247216\n+              \/\/ Put 'n' on the worklist because it might be modified by MergeMemStream::iteration_setup\n+              igvn->_worklist.push(n);\n+            }\n@@ -2566,0 +2633,5 @@\n+  Node* inline_type = try_push_inline_types_down(phase, can_reshape);\n+  if (inline_type != this) {\n+    return inline_type;\n+  }\n+\n@@ -2609,0 +2681,95 @@\n+\/\/ Check recursively if inputs are either an inline type, constant null\n+\/\/ or another Phi (including self references through data loops). If so,\n+\/\/ push the inline types down through the phis to enable folding of loads.\n+Node* PhiNode::try_push_inline_types_down(PhaseGVN* phase, const bool can_reshape) {\n+  if (!can_be_inline_type()) {\n+    return this;\n+  }\n+\n+  ciInlineKlass* inline_klass;\n+  if (can_push_inline_types_down(phase, can_reshape, inline_klass)) {\n+    assert(inline_klass != nullptr, \"must be\");\n+    return push_inline_types_down(phase, can_reshape, inline_klass);\n+  }\n+  return this;\n+}\n+\n+bool PhiNode::can_push_inline_types_down(PhaseGVN* phase, const bool can_reshape, ciInlineKlass*& inline_klass) {\n+  if (req() <= 2) {\n+    \/\/ Dead phi.\n+    return false;\n+  }\n+  inline_klass = nullptr;\n+\n+  \/\/ TODO 8302217 We need to prevent endless pushing through\n+  bool only_phi = (outcnt() != 0);\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    Node* n = fast_out(i);\n+    if (n->is_InlineType() && n->in(1) == this) {\n+      return false;\n+    }\n+    if (!n->is_Phi()) {\n+      only_phi = false;\n+    }\n+  }\n+  if (only_phi) {\n+    return false;\n+  }\n+\n+  ResourceMark rm;\n+  Unique_Node_List worklist;\n+  worklist.push(this);\n+  Node_List casts;\n+\n+  for (uint next = 0; next < worklist.size(); next++) {\n+    Node* phi = worklist.at(next);\n+    for (uint i = 1; i < phi->req(); i++) {\n+      Node* n = phi->in(i);\n+      if (n == nullptr) {\n+        return false;\n+      }\n+      while (n->is_ConstraintCast()) {\n+        if (n->in(0) != nullptr && n->in(0)->is_top()) {\n+          \/\/ Will die, don't optimize\n+          return false;\n+        }\n+        casts.push(n);\n+        n = n->in(1);\n+      }\n+      const Type* type = phase->type(n);\n+      if (n->is_InlineType() && (inline_klass == nullptr || inline_klass == type->inline_klass())) {\n+        inline_klass = type->inline_klass();\n+      } else if (n->is_Phi() && can_reshape && n->bottom_type()->isa_ptr()) {\n+        worklist.push(n);\n+      } else if (!type->is_zero_type()) {\n+        return false;\n+      }\n+    }\n+  }\n+  if (inline_klass == nullptr) {\n+    return false;\n+  }\n+\n+  \/\/ Check if cast nodes can be pushed through\n+  const Type* t = Type::get_const_type(inline_klass);\n+  while (casts.size() != 0 && t != nullptr) {\n+    Node* cast = casts.pop();\n+    if (t->filter(cast->bottom_type()) == Type::TOP) {\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+#ifdef ASSERT\n+bool PhiNode::can_push_inline_types_down(PhaseGVN* phase) {\n+  if (!can_be_inline_type()) {\n+    return false;\n+  }\n+\n+  ciInlineKlass* inline_klass;\n+  return can_push_inline_types_down(phase, true, inline_klass);\n+}\n+#endif \/\/ ASSERT\n+\n@@ -2991,0 +3158,6 @@\n+\n+  \/\/ CheckCastPPNode::Ideal() for inline types reuses the exception\n+  \/\/ paths of a call to perform an allocation: we can see a Phi here.\n+  if (in(1)->is_Phi()) {\n+    return this;\n+  }\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":177,"deletions":4,"binary":false,"changes":181,"status":"modified"},{"patch":"@@ -1845,1 +1845,1 @@\n-          derived->bottom_type()->make_ptr()->is_ptr()->_offset == 0, \"sanity\");\n+         derived->bottom_type()->make_ptr()->is_ptr()->offset() == 0, \"sanity\");\n@@ -1848,1 +1848,1 @@\n-  if( tj == nullptr || tj->_offset == 0 ) {\n+  if (tj == nullptr || tj->offset() == 0) {\n@@ -2014,1 +2014,1 @@\n-                  derived->bottom_type()->make_ptr()->is_ptr()->_offset == 0, \"sanity\");\n+                 derived->bottom_type()->make_ptr()->is_ptr()->offset() == 0, \"sanity\");\n@@ -2016,1 +2016,1 @@\n-          if( tj && tj->_offset != 0 && tj->isa_oop_ptr() ) {\n+          if (tj && tj->offset() != 0 && tj->isa_oop_ptr()) {\n@@ -2306,1 +2306,1 @@\n-  const TypeTuple *domain = C->tf()->domain();\n+  const TypeTuple *domain = C->tf()->domain_cc();\n@@ -2515,1 +2515,1 @@\n-                  if (is_derived && check->bottom_type()->is_ptr()->_offset != 0) {\n+                  if (is_derived && check->bottom_type()->is_ptr()->offset() != 0) {\n@@ -2519,1 +2519,1 @@\n-                    assert(check->bottom_type()->is_ptr()->_offset == 0, \"Bad base pointer\");\n+                    assert(check->bottom_type()->is_ptr()->offset() == 0, \"Bad base pointer\");\n@@ -2528,1 +2528,1 @@\n-                } else if (check->bottom_type()->is_ptr()->_offset == 0) {\n+                } else if (check->bottom_type()->is_ptr()->offset() == 0) {\n","filename":"src\/hotspot\/share\/opto\/chaitin.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n","filename":"src\/hotspot\/share\/opto\/classes.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -66,0 +67,1 @@\n+#include \"opto\/movenode.hpp\"\n@@ -404,0 +406,3 @@\n+  if (dead->is_InlineType()) {\n+    remove_inline_type(dead);\n+  }\n@@ -445,0 +450,3 @@\n+    if (n->outcnt() == 0) {\n+      worklist.push(n);\n+    }\n@@ -452,0 +460,6 @@\n+  remove_useless_nodes(_inline_type_nodes,  useful); \/\/ remove useless inline type nodes\n+#ifdef ASSERT\n+  if (_modified_nodes != nullptr) {\n+    _modified_nodes->remove_useless_nodes(useful.member_set());\n+  }\n+#endif\n@@ -631,0 +645,1 @@\n+                  _has_circular_inline_type(false),\n@@ -650,0 +665,1 @@\n+                  _inline_type_nodes (comp_arena(), 8, 0, nullptr),\n@@ -756,4 +772,2 @@\n-      const TypeTuple *domain = StartOSRNode::osr_domain();\n-      const TypeTuple *range = TypeTuple::make_range(method()->signature());\n-      init_tf(TypeFunc::make(domain, range));\n-      StartNode* s = new StartOSRNode(root(), domain);\n+      init_tf(TypeFunc::make(method(), \/* is_osr_compilation = *\/ true));\n+      StartNode* s = new StartOSRNode(root(), tf()->domain_sig());\n@@ -766,1 +780,1 @@\n-      StartNode* s = new StartNode(root(), tf()->domain());\n+      StartNode* s = new StartNode(root(), tf()->domain_cc());\n@@ -877,0 +891,10 @@\n+  if (needs_stack_repair()) {\n+    \/\/ One extra slot for the special stack increment value\n+    next_slot += 2;\n+  }\n+  \/\/ TODO 8284443 Only reserve extra slot if needed\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ One extra slot to hold the IsInit information for a nullable\n+    \/\/ inline type return if we run out of registers.\n+    next_slot += 2;\n+  }\n@@ -911,0 +935,1 @@\n+    _has_circular_inline_type(false),\n@@ -1057,0 +1082,4 @@\n+  _has_flat_accesses = false;\n+  _flat_accesses_share_alias = true;\n+  _scalarize_in_safepoints = false;\n+\n@@ -1328,1 +1357,2 @@\n-    assert(InlineUnsafeOps || StressReflectiveCode, \"indeterminate pointers come only from unsafe ops\");\n+    bool default_value_load = EnableValhalla && tj->is_instptr()->instance_klass() == ciEnv::current()->Class_klass();\n+    assert(InlineUnsafeOps || StressReflectiveCode || default_value_load, \"indeterminate pointers come only from unsafe ops\");\n@@ -1341,0 +1371,9 @@\n+  if (ta && ta->is_not_flat()) {\n+    \/\/ Erase not flat property for alias analysis.\n+    tj = ta = ta->cast_to_not_flat(false);\n+  }\n+  if (ta && ta->is_not_null_free()) {\n+    \/\/ Erase not null free property for alias analysis.\n+    tj = ta = ta->cast_to_not_null_free(false);\n+  }\n+\n@@ -1354,0 +1393,2 @@\n+    \/\/ For flat inline type array, each field has its own slice so\n+    \/\/ we must include the field offset.\n@@ -1394,1 +1435,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n@@ -1398,1 +1439,6 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n+    }\n+    \/\/ Initially all flattened array accesses share a single slice\n+    if (ta->is_flat() && ta->elem() != TypeInstPtr::BOTTOM && _flat_accesses_share_alias) {\n+      const TypeAry* tary = TypeAry::make(TypeInstPtr::BOTTOM, ta->size(), \/* stable= *\/ false, \/* flat= *\/ true);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));\n@@ -1405,1 +1451,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,Type::Offset(offset), ta->field_offset());\n@@ -1455,1 +1501,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, Type::Offset(offset));\n@@ -1476,1 +1522,1 @@\n-        assert(tj == TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, offset, instance_id), \"exact type should be canonical type\");\n+        assert(tj == TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, Type::Offset(offset), instance_id), \"exact type should be canonical type\");\n@@ -1479,1 +1525,1 @@\n-        tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, offset, instance_id);\n+        tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, Type::Offset(offset), instance_id);\n@@ -1494,1 +1540,1 @@\n-                                       offset);\n+                                       Type::Offset(offset));\n@@ -1500,1 +1546,1 @@\n-        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), offset);\n+        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), Type::Offset(offset));\n@@ -1502,1 +1548,1 @@\n-        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, offset);\n+        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, Type::Offset(offset), tk->is_not_flat(), tk->is_not_null_free(), tk->is_null_free());\n@@ -1505,1 +1551,0 @@\n-\n@@ -1635,1 +1680,1 @@\n-Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {\n+Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {\n@@ -1640,3 +1685,6 @@\n-  AliasCacheEntry* ace = probe_alias_cache(adr_type);\n-  if (ace->_adr_type == adr_type) {\n-    return alias_type(ace->_index);\n+  AliasCacheEntry* ace = nullptr;\n+  if (!uncached) {\n+    ace = probe_alias_cache(adr_type);\n+    if (ace->_adr_type == adr_type) {\n+      return alias_type(ace->_index);\n+    }\n@@ -1692,0 +1740,1 @@\n+    ciField* field = nullptr;\n@@ -1698,0 +1747,1 @@\n+      const Type* elemtype = flat->is_aryptr()->elem();\n@@ -1699,1 +1749,8 @@\n-        alias_type(idx)->set_element(flat->is_aryptr()->elem());\n+        alias_type(idx)->set_element(elemtype);\n+      }\n+      int field_offset = flat->is_aryptr()->field_offset().get();\n+      if (flat->is_flat() &&\n+          field_offset != Type::OffsetBot) {\n+        ciInlineKlass* vk = elemtype->inline_klass();\n+        field_offset += vk->payload_offset();\n+        field = vk->get_field_by_offset(field_offset, false);\n@@ -1717,0 +1774,2 @@\n+      if (flat->offset() == in_bytes(Klass::layout_helper_offset()))\n+        alias_type(idx)->set_rewritable(false);\n@@ -1727,1 +1786,0 @@\n-      ciField* field;\n@@ -1734,0 +1792,4 @@\n+      } else if (tinst->is_inlinetypeptr()) {\n+        \/\/ Inline type field\n+        ciInlineKlass* vk = tinst->inline_klass();\n+        field = vk->get_field_by_offset(tinst->offset(), false);\n@@ -1738,7 +1800,14 @@\n-      assert(field == nullptr ||\n-             original_field == nullptr ||\n-             (field->holder() == original_field->holder() &&\n-              field->offset_in_bytes() == original_field->offset_in_bytes() &&\n-              field->is_static() == original_field->is_static()), \"wrong field?\");\n-      \/\/ Set field() and is_rewritable() attributes.\n-      if (field != nullptr)  alias_type(idx)->set_field(field);\n+    }\n+    assert(field == nullptr ||\n+           original_field == nullptr ||\n+           (field->holder() == original_field->holder() &&\n+            field->offset_in_bytes() == original_field->offset_in_bytes() &&\n+            field->is_static() == original_field->is_static()), \"wrong field?\");\n+    \/\/ Set field() and is_rewritable() attributes.\n+    if (field != nullptr) {\n+      alias_type(idx)->set_field(field);\n+      if (flat->isa_aryptr()) {\n+        \/\/ Fields of flat arrays are rewritable although they are declared final\n+        assert(flat->is_flat(), \"must be a flat array\");\n+        alias_type(idx)->set_rewritable(true);\n+      }\n@@ -1749,3 +1818,4 @@\n-  ace->_adr_type = adr_type;\n-  ace->_index    = idx;\n-  assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n+  if (!uncached) {\n+    ace->_adr_type = adr_type;\n+    ace->_index    = idx;\n+    assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n@@ -1753,6 +1823,7 @@\n-  \/\/ Might as well try to fill the cache for the flattened version, too.\n-  AliasCacheEntry* face = probe_alias_cache(flat);\n-  if (face->_adr_type == nullptr) {\n-    face->_adr_type = flat;\n-    face->_index    = idx;\n-    assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    \/\/ Might as well try to fill the cache for the flattened version, too.\n+    AliasCacheEntry* face = probe_alias_cache(flat);\n+    if (face->_adr_type == nullptr) {\n+      face->_adr_type = flat;\n+      face->_index    = idx;\n+      assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    }\n@@ -1878,0 +1949,398 @@\n+void Compile::add_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  _inline_type_nodes.push(n);\n+}\n+\n+void Compile::remove_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  if (_inline_type_nodes.contains(n)) {\n+    _inline_type_nodes.remove(n);\n+  }\n+}\n+\n+\/\/ Does the return value keep otherwise useless inline type allocations alive?\n+static bool return_val_keeps_allocations_alive(Node* ret_val) {\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(ret_val);\n+  bool some_allocations = false;\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->outcnt() > 1) {\n+      \/\/ Some other use for the allocation\n+      return false;\n+    } else if (n->is_InlineType()) {\n+      wq.push(n->in(1));\n+    } else if (n->is_Phi()) {\n+      for (uint j = 1; j < n->req(); j++) {\n+        wq.push(n->in(j));\n+      }\n+    } else if (n->is_CheckCastPP() &&\n+               n->in(1)->is_Proj() &&\n+               n->in(1)->in(0)->is_Allocate()) {\n+      some_allocations = true;\n+    } else if (n->is_CheckCastPP()) {\n+      wq.push(n->in(1));\n+    }\n+  }\n+  return some_allocations;\n+}\n+\n+void Compile::process_inline_types(PhaseIterGVN &igvn, bool remove) {\n+  \/\/ Make sure that the return value does not keep an otherwise unused allocation alive\n+  if (tf()->returns_inline_type_as_fields()) {\n+    Node* ret = nullptr;\n+    for (uint i = 1; i < root()->req(); i++) {\n+      Node* in = root()->in(i);\n+      if (in->Opcode() == Op_Return) {\n+        assert(ret == nullptr, \"only one return\");\n+        ret = in;\n+      }\n+    }\n+    if (ret != nullptr) {\n+      Node* ret_val = ret->in(TypeFunc::Parms);\n+      if (igvn.type(ret_val)->isa_oopptr() &&\n+          return_val_keeps_allocations_alive(ret_val)) {\n+        igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)->inline_klass(), igvn));\n+        assert(ret_val->outcnt() == 0, \"should be dead now\");\n+        igvn.remove_dead_node(ret_val);\n+      }\n+    }\n+  }\n+  if (_inline_type_nodes.length() == 0) {\n+    return;\n+  }\n+  \/\/ Scalarize inline types in safepoint debug info.\n+  \/\/ Delay this until all inlining is over to avoid getting inconsistent debug info.\n+  set_scalarize_in_safepoints(true);\n+  for (int i = _inline_type_nodes.length()-1; i >= 0; i--) {\n+    InlineTypeNode* vt = _inline_type_nodes.at(i)->as_InlineType();\n+    vt->make_scalar_in_safepoints(&igvn);\n+    igvn.record_for_igvn(vt);\n+  }\n+  if (remove) {\n+    \/\/ Remove inline type nodes by replacing them with their oop input\n+    while (_inline_type_nodes.length() > 0) {\n+      InlineTypeNode* vt = _inline_type_nodes.pop()->as_InlineType();\n+      if (vt->outcnt() == 0) {\n+        igvn.remove_dead_node(vt);\n+        continue;\n+      }\n+      for (DUIterator i = vt->outs(); vt->has_out(i); i++) {\n+        DEBUG_ONLY(bool must_be_buffered = false);\n+        Node* u = vt->out(i);\n+        \/\/ Check if any users are blackholes. If so, rewrite them to use either the\n+        \/\/ allocated buffer, or individual components, instead of the inline type node\n+        \/\/ that goes away.\n+        if (u->is_Blackhole()) {\n+          BlackholeNode* bh = u->as_Blackhole();\n+\n+          \/\/ Unlink the old input\n+          int idx = bh->find_edge(vt);\n+          assert(idx != -1, \"The edge should be there\");\n+          bh->del_req(idx);\n+          --i;\n+\n+          if (vt->is_allocated(&igvn)) {\n+            \/\/ Already has the allocated instance, blackhole that\n+            bh->add_req(vt->get_oop());\n+          } else {\n+            \/\/ Not allocated yet, blackhole the components\n+            for (uint c = 0; c < vt->field_count(); c++) {\n+              bh->add_req(vt->field_value(c));\n+            }\n+          }\n+\n+          \/\/ Node modified, record for IGVN\n+          igvn.record_for_igvn(bh);\n+        }\n+#ifdef ASSERT\n+        \/\/ Verify that inline type is buffered when replacing by oop\n+        else if (u->is_InlineType()) {\n+          \/\/ InlineType uses don't need buffering because they are about to be replaced as well\n+        } else if (u->is_Phi()) {\n+          \/\/ TODO 8302217 Remove this once InlineTypeNodes are reliably pushed through\n+        } else {\n+          must_be_buffered = true;\n+        }\n+        if (must_be_buffered && !vt->is_allocated(&igvn)) {\n+          vt->dump(0);\n+          u->dump(0);\n+          assert(false, \"Should have been buffered\");\n+        }\n+#endif\n+      }\n+      igvn.replace_node(vt, vt->get_oop());\n+    }\n+  }\n+  igvn.optimize();\n+}\n+\n+void Compile::adjust_flat_array_access_aliases(PhaseIterGVN& igvn) {\n+  if (!_has_flat_accesses) {\n+    return;\n+  }\n+  \/\/ Initially, all flat array accesses share the same slice to\n+  \/\/ keep dependencies with Object[] array accesses (that could be\n+  \/\/ to a flat array) correct. We're done with parsing so we\n+  \/\/ now know all flat array accesses in this compile\n+  \/\/ unit. Let's move flat array accesses to their own slice,\n+  \/\/ one per element field. This should help memory access\n+  \/\/ optimizations.\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(root());\n+\n+  Node_List mergememnodes;\n+  Node_List memnodes;\n+\n+  \/\/ Alias index currently shared by all flat memory accesses\n+  int index = get_alias_index(TypeAryPtr::INLINES);\n+\n+  \/\/ Find MergeMem nodes and flat array accesses\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->is_Mem()) {\n+      const TypePtr* adr_type = nullptr;\n+      adr_type = get_adr_type(get_alias_index(n->adr_type()));\n+      if (adr_type == TypeAryPtr::INLINES) {\n+        memnodes.push(n);\n+      }\n+    } else if (n->is_MergeMem()) {\n+      MergeMemNode* mm = n->as_MergeMem();\n+      if (mm->memory_at(index) != mm->base_memory()) {\n+        mergememnodes.push(n);\n+      }\n+    }\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* m = n->in(j);\n+      if (m != nullptr) {\n+        wq.push(m);\n+      }\n+    }\n+  }\n+\n+  if (memnodes.size() > 0) {\n+    _flat_accesses_share_alias = false;\n+\n+    \/\/ We are going to change the slice for the flat array\n+    \/\/ accesses so we need to clear the cache entries that refer to\n+    \/\/ them.\n+    for (uint i = 0; i < AliasCacheSize; i++) {\n+      AliasCacheEntry* ace = &_alias_cache[i];\n+      if (ace->_adr_type != nullptr &&\n+          ace->_adr_type->is_flat()) {\n+        ace->_adr_type = nullptr;\n+        ace->_index = (i != 0) ? 0 : AliasIdxTop; \/\/ Make sure the nullptr adr_type resolves to AliasIdxTop\n+      }\n+    }\n+\n+    \/\/ Find what aliases we are going to add\n+    int start_alias = num_alias_types()-1;\n+    int stop_alias = 0;\n+\n+    for (uint i = 0; i < memnodes.size(); i++) {\n+      Node* m = memnodes.at(i);\n+      const TypePtr* adr_type = nullptr;\n+      adr_type = m->adr_type();\n+#ifdef ASSERT\n+      m->as_Mem()->set_adr_type(adr_type);\n+#endif\n+      int idx = get_alias_index(adr_type);\n+      start_alias = MIN2(start_alias, idx);\n+      stop_alias = MAX2(stop_alias, idx);\n+    }\n+\n+    assert(stop_alias >= start_alias, \"should have expanded aliases\");\n+\n+    Node_Stack stack(0);\n+#ifdef ASSERT\n+    VectorSet seen(Thread::current()->resource_area());\n+#endif\n+    \/\/ Now let's fix the memory graph so each flat array access\n+    \/\/ is moved to the right slice. Start from the MergeMem nodes.\n+    uint last = unique();\n+    for (uint i = 0; i < mergememnodes.size(); i++) {\n+      MergeMemNode* current = mergememnodes.at(i)->as_MergeMem();\n+      Node* n = current->memory_at(index);\n+      MergeMemNode* mm = nullptr;\n+      do {\n+        \/\/ Follow memory edges through memory accesses, phis and\n+        \/\/ narrow membars and push nodes on the stack. Once we hit\n+        \/\/ bottom memory, we pop element off the stack one at a\n+        \/\/ time, in reverse order, and move them to the right slice\n+        \/\/ by changing their memory edges.\n+        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() || n->adr_type() == TypeAryPtr::INLINES) {\n+          assert(!seen.test_set(n->_idx), \"\");\n+          \/\/ Uses (a load for instance) will need to be moved to the\n+          \/\/ right slice as well and will get a new memory state\n+          \/\/ that we don't know yet. The use could also be the\n+          \/\/ backedge of a loop. We put a place holder node between\n+          \/\/ the memory node and its uses. We replace that place\n+          \/\/ holder with the correct memory state once we know it,\n+          \/\/ i.e. when nodes are popped off the stack. Using the\n+          \/\/ place holder make the logic work in the presence of\n+          \/\/ loops.\n+          if (n->outcnt() > 1) {\n+            Node* place_holder = nullptr;\n+            assert(!n->has_out_with(Op_Node), \"\");\n+            for (DUIterator k = n->outs(); n->has_out(k); k++) {\n+              Node* u = n->out(k);\n+              if (u != current && u->_idx < last) {\n+                bool success = false;\n+                for (uint l = 0; l < u->req(); l++) {\n+                  if (!stack.is_empty() && u == stack.node() && l == stack.index()) {\n+                    continue;\n+                  }\n+                  Node* in = u->in(l);\n+                  if (in == n) {\n+                    if (place_holder == nullptr) {\n+                      place_holder = new Node(1);\n+                      place_holder->init_req(0, n);\n+                    }\n+                    igvn.replace_input_of(u, l, place_holder);\n+                    success = true;\n+                  }\n+                }\n+                if (success) {\n+                  --k;\n+                }\n+              }\n+            }\n+          }\n+          if (n->is_Phi()) {\n+            stack.push(n, 1);\n+            n = n->in(1);\n+          } else if (n->is_Mem()) {\n+            stack.push(n, n->req());\n+            n = n->in(MemNode::Memory);\n+          } else {\n+            assert(n->is_Proj() && n->in(0)->Opcode() == Op_MemBarCPUOrder, \"\");\n+            stack.push(n, n->req());\n+            n = n->in(0)->in(TypeFunc::Memory);\n+          }\n+        } else {\n+          assert(n->adr_type() == TypePtr::BOTTOM || (n->Opcode() == Op_Node && n->_idx >= last) || (n->is_Proj() && n->in(0)->is_Initialize()), \"\");\n+          \/\/ Build a new MergeMem node to carry the new memory state\n+          \/\/ as we build it. IGVN should fold extraneous MergeMem\n+          \/\/ nodes.\n+          mm = MergeMemNode::make(n);\n+          igvn.register_new_node_with_optimizer(mm);\n+          while (stack.size() > 0) {\n+            Node* m = stack.node();\n+            uint idx = stack.index();\n+            if (m->is_Mem()) {\n+              \/\/ Move memory node to its new slice\n+              const TypePtr* adr_type = m->adr_type();\n+              int alias = get_alias_index(adr_type);\n+              Node* prev = mm->memory_at(alias);\n+              igvn.replace_input_of(m, MemNode::Memory, prev);\n+              mm->set_memory_at(alias, m);\n+            } else if (m->is_Phi()) {\n+              \/\/ We need as many new phis as there are new aliases\n+              igvn.replace_input_of(m, idx, mm);\n+              if (idx == m->req()-1) {\n+                Node* r = m->in(0);\n+                for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                  const TypePtr* adr_type = get_adr_type(j);\n+                  if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                    continue;\n+                  }\n+                  Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));\n+                  igvn.register_new_node_with_optimizer(phi);\n+                  for (uint k = 1; k < m->req(); k++) {\n+                    phi->init_req(k, m->in(k)->as_MergeMem()->memory_at(j));\n+                  }\n+                  mm->set_memory_at(j, phi);\n+                }\n+                Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+                igvn.register_new_node_with_optimizer(base_phi);\n+                for (uint k = 1; k < m->req(); k++) {\n+                  base_phi->init_req(k, m->in(k)->as_MergeMem()->base_memory());\n+                }\n+                mm->set_base_memory(base_phi);\n+              }\n+            } else {\n+              \/\/ This is a MemBarCPUOrder node from\n+              \/\/ Parse::array_load()\/Parse::array_store(), in the\n+              \/\/ branch that handles flat arrays hidden under\n+              \/\/ an Object[] array. We also need one new membar per\n+              \/\/ new alias to keep the unknown access that the\n+              \/\/ membars protect properly ordered with accesses to\n+              \/\/ known flat array.\n+              assert(m->is_Proj(), \"projection expected\");\n+              Node* ctrl = m->in(0)->in(TypeFunc::Control);\n+              igvn.replace_input_of(m->in(0), TypeFunc::Control, top());\n+              for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                const TypePtr* adr_type = get_adr_type(j);\n+                if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                  continue;\n+                }\n+                MemBarNode* mb = new MemBarCPUOrderNode(this, j, nullptr);\n+                igvn.register_new_node_with_optimizer(mb);\n+                Node* mem = mm->memory_at(j);\n+                mb->init_req(TypeFunc::Control, ctrl);\n+                mb->init_req(TypeFunc::Memory, mem);\n+                ctrl = new ProjNode(mb, TypeFunc::Control);\n+                igvn.register_new_node_with_optimizer(ctrl);\n+                mem = new ProjNode(mb, TypeFunc::Memory);\n+                igvn.register_new_node_with_optimizer(mem);\n+                mm->set_memory_at(j, mem);\n+              }\n+              igvn.replace_node(m->in(0)->as_Multi()->proj_out(TypeFunc::Control), ctrl);\n+            }\n+            if (idx < m->req()-1) {\n+              idx += 1;\n+              stack.set_index(idx);\n+              n = m->in(idx);\n+              break;\n+            }\n+            \/\/ Take care of place holder nodes\n+            if (m->has_out_with(Op_Node)) {\n+              Node* place_holder = m->find_out_with(Op_Node);\n+              if (place_holder != nullptr) {\n+                Node* mm_clone = mm->clone();\n+                igvn.register_new_node_with_optimizer(mm_clone);\n+                Node* hook = new Node(1);\n+                hook->init_req(0, mm);\n+                igvn.replace_node(place_holder, mm_clone);\n+                hook->destruct(&igvn);\n+              }\n+              assert(!m->has_out_with(Op_Node), \"place holder should be gone now\");\n+            }\n+            stack.pop();\n+          }\n+        }\n+      } while(stack.size() > 0);\n+      \/\/ Fix the memory state at the MergeMem we started from\n+      igvn.rehash_node_delayed(current);\n+      for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+        const TypePtr* adr_type = get_adr_type(j);\n+        if (!adr_type->isa_aryptr() || !adr_type->is_flat()) {\n+          continue;\n+        }\n+        current->set_memory_at(j, mm);\n+      }\n+      current->set_memory_at(index, current->base_memory());\n+    }\n+    igvn.optimize();\n+  }\n+  print_method(PHASE_SPLIT_INLINES_ARRAY, 2);\n+#ifdef ASSERT\n+  if (!_flat_accesses_share_alias) {\n+    wq.clear();\n+    wq.push(root());\n+    for (uint i = 0; i < wq.size(); i++) {\n+      Node* n = wq.at(i);\n+      assert(n->adr_type() != TypeAryPtr::INLINES, \"should have been removed from the graph\");\n+      for (uint j = 0; j < n->req(); j++) {\n+        Node* m = n->in(j);\n+        if (m != nullptr) {\n+          wq.push(m);\n+        }\n+      }\n+    }\n+  }\n+#endif\n+}\n+\n@@ -1953,1 +2422,1 @@\n-        if (!live_locals.at(i) && !local->is_top() && local != lhs && local!= rhs) {\n+        if (!live_locals.at(i) && !local->is_top() && local != lhs && local != rhs) {\n@@ -1968,1 +2437,1 @@\n-    \/\/ keep the mondified trap for late query\n+    \/\/ keep the modified trap for late query\n@@ -2163,1 +2632,4 @@\n-  assert(_modified_nodes == nullptr, \"not allowed\");\n+#ifdef ASSERT\n+  Unique_Node_List* modified_nodes = _modified_nodes;\n+  _modified_nodes = nullptr;\n+#endif\n@@ -2176,0 +2648,1 @@\n+  DEBUG_ONLY( _modified_nodes = modified_nodes; )\n@@ -2322,0 +2795,5 @@\n+  \/\/ Process inline type nodes now that all inlining is over\n+  process_inline_types(igvn);\n+\n+  adjust_flat_array_access_aliases(igvn);\n+\n@@ -2445,0 +2923,8 @@\n+  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n+\n+  if (_late_inlines.length() > 0) {\n+    \/\/ More opportunities to optimize virtual and MH calls.\n+    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n+    process_late_inline_calls_no_inline(igvn);\n+  }\n+\n@@ -2456,0 +2942,4 @@\n+  \/\/ Process inline type nodes again and remove them. From here\n+  \/\/ on we don't need to keep track of field values anymore.\n+  process_inline_types(igvn, \/* remove= *\/ true);\n+\n@@ -2472,0 +2962,1 @@\n+  DEBUG_ONLY( _late_inlines.clear(); )\n@@ -2474,9 +2965,0 @@\n-\n-  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n-\n-  if (_late_inlines.length() > 0) {\n-    \/\/ More opportunities to optimize virtual and MH calls.\n-    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n-    process_late_inline_calls_no_inline(igvn);\n-    if (failing())  return;\n-  }\n@@ -3250,0 +3732,1 @@\n+  case Op_StoreLSpecial:\n@@ -3793,0 +4276,5 @@\n+  case Op_InlineType: {\n+    n->dump(-1);\n+    assert(false, \"inline type node was not removed\");\n+    break;\n+  }\n@@ -4179,2 +4667,2 @@\n-      if (accessing_method->is_static_initializer() ||\n-          accessing_method->is_object_initializer() ||\n+      if (accessing_method->is_class_initializer() ||\n+          accessing_method->is_object_constructor() ||\n@@ -4188,1 +4676,1 @@\n-      if (accessing_method->is_static_initializer()) {\n+      if (accessing_method->is_class_initializer()) {\n@@ -4244,0 +4732,1 @@\n+               (n->is_Allocate() && i >= AllocateNode::InlineType) ||\n@@ -4246,1 +4735,1 @@\n-              \"only region, phi, arraycopy, unlock or membar nodes have null data edges\");\n+              \"only region, phi, arraycopy, allocate, unlock or membar nodes have null data edges\");\n@@ -4385,0 +4874,7 @@\n+\n+    \/\/ Do not fold the subtype check to an array klass pointer comparison for null-able inline type arrays\n+    \/\/ because null-free [LMyValue <: null-able [LMyValue but the klasses are different. Perform a full test.\n+    if (!superk->is_aryklassptr()->is_null_free() && superk->is_aryklassptr()->elem()->isa_instklassptr() &&\n+        superk->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->is_inlinetype()) {\n+      return SSC_full_test;\n+    }\n@@ -4984,0 +5480,21 @@\n+Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {\n+  const TypeInstPtr* ta = phase->type(a)->isa_instptr();\n+  const TypeInstPtr* tb = phase->type(b)->isa_instptr();\n+  if (!EnableValhalla || ta == nullptr || tb == nullptr ||\n+      ta->is_zero_type() || tb->is_zero_type() ||\n+      !ta->can_be_inline_type() || !tb->can_be_inline_type()) {\n+    \/\/ Use old acmp if one operand is null or not an inline type\n+    return new CmpPNode(a, b);\n+  } else if (ta->is_inlinetypeptr() || tb->is_inlinetypeptr()) {\n+    \/\/ We know that one operand is an inline type. Therefore,\n+    \/\/ new acmp will only return true if both operands are nullptr.\n+    \/\/ Check if both operands are null by or'ing the oops.\n+    a = phase->transform(new CastP2XNode(nullptr, a));\n+    b = phase->transform(new CastP2XNode(nullptr, b));\n+    a = phase->transform(new OrXNode(a, b));\n+    return new CmpXNode(a, phase->MakeConX(0));\n+  }\n+  \/\/ Use new acmp\n+  return nullptr;\n+}\n+\n@@ -5329,0 +5846,2 @@\n+  } else if (bt == T_FLOAT) {\n+    result = new MoveI2FNode(value);\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":573,"deletions":54,"binary":false,"changes":627,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -66,0 +67,1 @@\n+\/\/------------------------------Ideal------------------------------------------\n@@ -67,0 +69,6 @@\n+  if (in(1)->is_InlineType()) {\n+    \/\/ Null checking a scalarized but nullable inline type. Check the IsInit\n+    \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+    set_req_X(1, in(1)->as_InlineType()->get_is_init(), phase);\n+    return this;\n+  }\n","filename":"src\/hotspot\/share\/opto\/convertnode.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1498,5 +1498,4 @@\n-  CallProjections projs;\n-  extract_projections(&projs, false, false);\n-  C->gvn_replace_by(projs.fallthrough_proj, in(TypeFunc::Control));\n-  if (projs.fallthrough_catchproj != nullptr) {\n-    C->gvn_replace_by(projs.fallthrough_catchproj, in(TypeFunc::Control));\n+  CallProjections* projs = extract_projections(false, false);\n+  C->gvn_replace_by(projs->fallthrough_proj, in(TypeFunc::Control));\n+  if (projs->fallthrough_catchproj != nullptr) {\n+    C->gvn_replace_by(projs->fallthrough_catchproj, in(TypeFunc::Control));\n@@ -1504,2 +1503,2 @@\n-  if (projs.fallthrough_memproj != nullptr) {\n-    C->gvn_replace_by(projs.fallthrough_memproj, in(TypeFunc::Memory));\n+  if (projs->fallthrough_memproj != nullptr) {\n+    C->gvn_replace_by(projs->fallthrough_memproj, in(TypeFunc::Memory));\n@@ -1507,2 +1506,2 @@\n-  if (projs.catchall_memproj != nullptr) {\n-    C->gvn_replace_by(projs.catchall_memproj, C->top());\n+  if (projs->catchall_memproj != nullptr) {\n+    C->gvn_replace_by(projs->catchall_memproj, C->top());\n@@ -1510,2 +1509,2 @@\n-  if (projs.fallthrough_ioproj != nullptr) {\n-    C->gvn_replace_by(projs.fallthrough_ioproj, in(TypeFunc::I_O));\n+  if (projs->fallthrough_ioproj != nullptr) {\n+    C->gvn_replace_by(projs->fallthrough_ioproj, in(TypeFunc::I_O));\n@@ -1513,4 +1512,4 @@\n-  assert(projs.catchall_ioproj == nullptr, \"no exceptions from floating mod\");\n-  assert(projs.catchall_catchproj == nullptr, \"no exceptions from floating mod\");\n-  if (projs.resproj != nullptr) {\n-    C->gvn_replace_by(projs.resproj, con_node);\n+  assert(projs->catchall_ioproj == nullptr, \"no exceptions from floating mod\");\n+  assert(projs->catchall_catchproj == nullptr, \"no exceptions from floating mod\");\n+  if (projs->resproj[0] != nullptr) {\n+    C->gvn_replace_by(projs->resproj[0], con_node);\n@@ -1576,1 +1575,1 @@\n-Node *DivModINode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *DivModINode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -1591,1 +1590,1 @@\n-Node *DivModLNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *DivModLNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -1629,1 +1628,1 @@\n-Node* UDivModINode::match( const ProjNode *proj, const Matcher *match ) {\n+Node* UDivModINode::match(const ProjNode* proj, const Matcher* match, const RegMask* mask) {\n@@ -1644,1 +1643,1 @@\n-Node* UDivModLNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node* UDivModLNode::match( const ProjNode* proj, const Matcher* match, const RegMask* mask) {\n","filename":"src\/hotspot\/share\/opto\/divnode.cpp","additions":18,"deletions":19,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -236,1 +236,1 @@\n-  virtual Node *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node *match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n@@ -249,1 +249,1 @@\n-  virtual Node *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node *match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n@@ -263,1 +263,1 @@\n-  virtual Node *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node* match(const ProjNode* proj, const Matcher* m, const RegMask* mask);\n@@ -276,1 +276,1 @@\n-  virtual Node *match( const ProjNode *proj, const Matcher *m );\n+  virtual Node* match(const ProjNode* proj, const Matcher* m, const RegMask* mask);\n","filename":"src\/hotspot\/share\/opto\/divnode.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -599,1 +600,1 @@\n-  if (iter().cur_bc_raw() == Bytecodes::_invokespecial && !orig_callee->is_object_initializer()) {\n+  if (iter().cur_bc_raw() == Bytecodes::_invokespecial && !orig_callee->is_object_constructor()) {\n@@ -729,1 +730,1 @@\n-          \/\/ It's OK for a method  to return a value that is discarded.\n+          \/\/ It's OK for a method to return a value that is discarded.\n@@ -787,0 +788,53 @@\n+    if (rtype->is_inlinetype() && !peek()->is_InlineType()) {\n+      Node* retnode = pop();\n+      retnode = InlineTypeNode::make_from_oop(this, retnode, rtype->as_inline_klass(), !gvn().type(retnode)->maybe_null());\n+      push_node(T_OBJECT, retnode);\n+    }\n+\n+    \/\/ Note that:\n+    \/\/ - The caller map is the state just before the call of the currently parsed method with all arguments\n+    \/\/   on the stack. Therefore, we have caller_map->arg(0) == this.\n+    \/\/ - local(0) contains the updated receiver after calling an inline type constructor.\n+    \/\/ - Abstract value classes are not ciInlineKlass instances and thus abstract_value_klass->is_inlinetype() is false.\n+    \/\/   We use the bottom type of the receiver node to determine if we have a value class or not.\n+    const bool is_current_method_inline_type_constructor =\n+        \/\/ Is current method a constructor (i.e <init>)?\n+        _method->is_object_constructor() &&\n+        \/\/ Is the holder of the current constructor method an inline type?\n+        _caller->map()->argument(_caller, 0)->bottom_type()->is_inlinetypeptr();\n+    assert(!is_current_method_inline_type_constructor || !cg->method()->is_object_constructor() || receiver != nullptr,\n+           \"must have valid receiver after calling another constructor\");\n+    if (is_current_method_inline_type_constructor &&\n+        \/\/ Is the just called method an inline type constructor?\n+        cg->method()->is_object_constructor() && receiver->bottom_type()->is_inlinetypeptr() &&\n+         \/\/ AND:\n+         \/\/ 1) ... invoked on the same receiver? Then it's another constructor on the same object doing the initialization.\n+        (receiver == _caller->map()->argument(_caller, 0) ||\n+         \/\/ 2) ... abstract? Then it's the call to the super constructor which eventually calls Object.<init> to\n+         \/\/                    finish the initialization of this larval.\n+         cg->method()->holder()->is_abstract() ||\n+         \/\/ 3) ... Object.<init>? Then we know it's the final call to finish the larval initialization. Other\n+         \/\/        Object.<init> calls would have a non-inline-type receiver which we already excluded in the check above.\n+         cg->method()->holder()->is_java_lang_Object())\n+        ) {\n+      assert(local(0)->is_InlineType() && receiver->bottom_type()->is_inlinetypeptr() && receiver->is_InlineType() &&\n+             _caller->map()->argument(_caller, 0)->bottom_type()->inline_klass() == receiver->bottom_type()->inline_klass(),\n+             \"Unexpected receiver\");\n+      InlineTypeNode* updated_receiver = local(0)->as_InlineType();\n+      InlineTypeNode* cloned_updated_receiver = updated_receiver->clone_if_required(&_gvn, _map);\n+      cloned_updated_receiver->set_is_larval(false);\n+      cloned_updated_receiver = _gvn.transform(cloned_updated_receiver)->as_InlineType();\n+      \/\/ Receiver updated by the just called constructor. We need to update the map to make the effect visible. After\n+      \/\/ the super() call, only the updated receiver in local(0) will be used from now on. Therefore, we do not need\n+      \/\/ to update the original receiver 'receiver' but only the 'updated_receiver'.\n+      replace_in_map(updated_receiver, cloned_updated_receiver);\n+\n+      if (_caller->has_method()) {\n+        \/\/ If the current method is inlined, we also need to update the exit map to propagate the updated receiver\n+        \/\/ to the caller map.\n+        Node* receiver_in_caller = _caller->map()->argument(_caller, 0);\n+        assert(receiver_in_caller->bottom_type()->inline_klass() == receiver->bottom_type()->inline_klass(),\n+               \"Receiver type mismatch\");\n+        _exits.map()->replace_edge(receiver_in_caller, cloned_updated_receiver, &_gvn);\n+      }\n+    }\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":56,"deletions":2,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/metaspace.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -166,0 +168,10 @@\n+    if ((n->Opcode() == Op_LoadX || n->Opcode() == Op_StoreX) &&\n+        !n->in(MemNode::Address)->is_AddP() &&\n+        _igvn->type(n->in(MemNode::Address))->isa_oopptr()) {\n+      \/\/ Load\/Store at mark work address is at offset 0 so has no AddP which confuses EA\n+      Node* addp = new AddPNode(n->in(MemNode::Address), n->in(MemNode::Address), _igvn->MakeConX(0));\n+      _igvn->register_new_node_with_optimizer(addp);\n+      _igvn->replace_input_of(n, MemNode::Address, addp);\n+      ideal_nodes.push(addp);\n+      _nodes.at_put_grow(addp->_idx, nullptr, nullptr);\n+    }\n@@ -1248,1 +1260,9 @@\n-      SafePointScalarObjectNode* sobj = mexp.create_scalarized_object_description(alloc, sfpt);\n+      Unique_Node_List value_worklist;\n+#ifdef ASSERT\n+      const Type* res_type = alloc->result_cast()->bottom_type();\n+      if (res_type->is_inlinetypeptr() && !Compile::current()->has_circular_inline_type()) {\n+        PhiNode* phi = ophi->as_Phi();\n+        assert(!ophi->as_Phi()->can_push_inline_types_down(_igvn), \"missed earlier scalarization opportunity\");\n+      }\n+#endif\n+      SafePointScalarObjectNode* sobj = mexp.create_scalarized_object_description(alloc, sfpt, &value_worklist);\n@@ -1250,0 +1270,1 @@\n+        _compile->record_failure(C2Compiler::retry_no_reduce_allocation_merges());\n@@ -1260,0 +1281,9 @@\n+\n+      \/\/ Scalarize inline types that were added to the safepoint.\n+      \/\/ Don't allow linking a constant oop (if available) for flat array elements\n+      \/\/ because Deoptimization::reassign_flat_array_elements needs field values.\n+      const bool allow_oop = !merge_t->is_flat();\n+      for (uint j = 0; j < value_worklist.size(); ++j) {\n+        InlineTypeNode* vt = value_worklist.at(j)->as_InlineType();\n+        vt->make_scalar_in_safepoints(_igvn, allow_oop);\n+      }\n@@ -1458,1 +1488,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_sig();\n@@ -1532,0 +1562,11 @@\n+      } else if (n->as_Call()->tf()->returns_inline_type_as_fields()) {\n+        bool returns_oop = false;\n+        for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax && !returns_oop; i++) {\n+          ProjNode* pn = n->fast_out(i)->as_Proj();\n+          if (pn->_con >= TypeFunc::Parms && pn->bottom_type()->isa_ptr()) {\n+            returns_oop = true;\n+          }\n+        }\n+        if (returns_oop) {\n+          add_call_node(n->as_Call());\n+        }\n@@ -1559,1 +1600,2 @@\n-    case Op_CastX2P: {\n+    case Op_CastX2P:\n+    case Op_CastI2N: {\n@@ -1563,0 +1605,1 @@\n+    case Op_InlineType:\n@@ -1634,2 +1677,4 @@\n-      if (n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-          n->in(0)->as_Call()->returns_pointer()) {\n+      if (n->as_Proj()->_con >= TypeFunc::Parms && n->in(0)->is_Call() &&\n+          (n->in(0)->as_Call()->returns_pointer() || n->bottom_type()->isa_ptr())) {\n+        assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+               n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n@@ -1737,0 +1782,1 @@\n+    case Op_InlineType:\n@@ -1791,2 +1837,2 @@\n-      assert(n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-             n->in(0)->as_Call()->returns_pointer(), \"Unexpected node type\");\n+      assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->as_Call()->returns_pointer()) ||\n+             n->in(0)->as_Call()->tf()->returns_inline_type_as_fields(), \"what kind of oop return is it?\");\n@@ -1968,1 +2014,1 @@\n-  assert(call->returns_pointer(), \"only for call which returns pointer\");\n+  assert(call->returns_pointer() || call->tf()->returns_inline_type_as_fields(), \"only for call which returns pointer\");\n@@ -2044,1 +2090,2 @@\n-      assert(strncmp(name, \"C2 Runtime multianewarray\", 25) == 0, \"TODO: add failed case check\");\n+      assert(strncmp(name, \"C2 Runtime multianewarray\", 25) == 0 ||\n+             strncmp(name, \"C2 Runtime load_unknown_inline\", 30) == 0, \"TODO: add failed case check\");\n@@ -2075,1 +2122,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -2123,1 +2170,1 @@\n-      const TypeTuple * d = call->tf()->domain();\n+      const TypeTuple * d = call->tf()->domain_sig();\n@@ -2154,1 +2201,4 @@\n-                               (aat->isa_aryptr() && (aat->isa_aryptr()->elem() == Type::BOTTOM || aat->isa_aryptr()->elem()->make_oopptr() != nullptr)));\n+                               (aat->isa_aryptr() && (aat->isa_aryptr()->elem() == Type::BOTTOM || aat->isa_aryptr()->elem()->make_oopptr() != nullptr)) ||\n+                               (aat->isa_aryptr() && aat->isa_aryptr()->elem() != nullptr &&\n+                                                               aat->isa_aryptr()->is_flat() &&\n+                                                               aat->isa_aryptr()->elem()->inline_klass()->contains_oops()));\n@@ -2205,0 +2255,3 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"vectorizedMismatch\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"load_unknown_inline\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"store_unknown_inline\") == 0 ||\n@@ -2271,1 +2324,1 @@\n-        const TypeTuple* d = call->tf()->domain();\n+        const TypeTuple* d = call->tf()->domain_cc();\n@@ -2315,1 +2368,1 @@\n-      const TypeTuple* d = call->tf()->domain();\n+      const TypeTuple* d = call->tf()->domain_cc();\n@@ -2728,0 +2781,1 @@\n+  PointsToNode* init_val = phantom_obj;\n@@ -2733,1 +2787,8 @@\n-    return 0;\n+    if (alloc->as_Allocate()->in(AllocateNode::DefaultValue) != nullptr) {\n+      \/\/ Non-flat inline type arrays are initialized with\n+      \/\/ the default value instead of null. Handle them here.\n+      init_val = ptnode_adr(alloc->as_Allocate()->in(AllocateNode::DefaultValue)->_idx);\n+      assert(init_val != nullptr, \"default value should be registered\");\n+    } else {\n+      return 0;\n+    }\n@@ -2735,1 +2796,2 @@\n-  assert(pta->arraycopy_dst() || alloc->as_CallStaticJava(), \"sanity\");\n+  \/\/ Non-escaped allocation returned from Java or runtime call has unknown values in fields.\n+  assert(pta->arraycopy_dst() || alloc->is_CallStaticJava() || init_val != phantom_obj, \"sanity\");\n@@ -2737,1 +2799,1 @@\n-  if (!pta->arraycopy_dst() && alloc->as_CallStaticJava()->method() == nullptr) {\n+  if (alloc->is_CallStaticJava() && alloc->as_CallStaticJava()->method() == nullptr) {\n@@ -2739,1 +2801,2 @@\n-    assert(strncmp(name, \"C2 Runtime multianewarray\", 25) == 0, \"sanity\");\n+    assert(strncmp(name, \"C2 Runtime multianewarray\", 25) == 0 ||\n+           strncmp(name, \"C2 Runtime load_unknown_inline\", 30) == 0, \"sanity\");\n@@ -2747,1 +2810,1 @@\n-      if (add_edge(field, phantom_obj)) {\n+      if (add_edge(field, init_val)) {\n@@ -2762,1 +2825,1 @@\n-  if (!alloc->is_Allocate()) {\n+  if (!alloc->is_Allocate() || alloc->as_Allocate()->in(AllocateNode::DefaultValue) != nullptr) {\n@@ -2848,1 +2911,1 @@\n-                tty->print_cr(\"----------missed referernce to object-----------\");\n+                tty->print_cr(\"----------missed reference to object------------\");\n@@ -2850,1 +2913,1 @@\n-                tty->print_cr(\"----------object referernced by init store -----\");\n+                tty->print_cr(\"----------object referenced by init store-------\");\n@@ -3206,1 +3269,2 @@\n-          if (can_eliminate_lock(alock)) {\n+          const Type* obj_type = igvn->type(alock->obj_node());\n+          if (can_eliminate_lock(alock) && !obj_type->is_inlinetypeptr()) {\n@@ -3248,5 +3312,10 @@\n-      MemBarNode* mb = MemBarNode::make(C, Op_MemBarCPUOrder, Compile::AliasIdxBot);\n-      mb->init_req(TypeFunc::Memory,  storestore->in(TypeFunc::Memory));\n-      mb->init_req(TypeFunc::Control, storestore->in(TypeFunc::Control));\n-      igvn->register_new_node_with_optimizer(mb);\n-      igvn->replace_node(storestore, mb);\n+      if (alloc->in(AllocateNode::InlineType) != nullptr) {\n+        \/\/ Non-escaping inline type buffer allocations don't require a membar\n+        storestore->as_MemBar()->remove(_igvn);\n+      } else {\n+        MemBarNode* mb = MemBarNode::make(C, Op_MemBarCPUOrder, Compile::AliasIdxBot);\n+        mb->init_req(TypeFunc::Memory,  storestore->in(TypeFunc::Memory));\n+        mb->init_req(TypeFunc::Control, storestore->in(TypeFunc::Control));\n+        igvn->register_new_node_with_optimizer(mb);\n+        igvn->replace_node(storestore, mb);\n+      }\n@@ -3414,0 +3483,1 @@\n+  int field_offset = adr_type->isa_aryptr() ? adr_type->isa_aryptr()->field_offset().get() : Type::OffsetBot;\n@@ -3415,1 +3485,1 @@\n-  if (offset == Type::OffsetBot) {\n+  if (offset == Type::OffsetBot && field_offset == Type::OffsetBot) {\n@@ -3427,1 +3497,1 @@\n-      ciField* field = _compile->alias_type(adr_type->isa_instptr())->field();\n+      ciField* field = _compile->alias_type(adr_type->is_ptr())->field();\n@@ -3446,2 +3516,8 @@\n-        const Type* elemtype = adr_type->isa_aryptr()->elem();\n-        bt = elemtype->array_element_basic_type();\n+        const Type* elemtype = adr_type->is_aryptr()->elem();\n+        if (adr_type->is_aryptr()->is_flat() && field_offset != Type::OffsetBot) {\n+          ciInlineKlass* vk = elemtype->inline_klass();\n+          field_offset += vk->payload_offset();\n+          bt = vk->get_field_by_offset(field_offset, false)->layout_type();\n+        } else {\n+          bt = elemtype->array_element_basic_type();\n+        }\n@@ -3644,3 +3720,1 @@\n-  const TypePtr *t_ptr = adr_type->isa_ptr();\n-  assert(t_ptr != nullptr, \"must be a pointer type\");\n-  return t_ptr->offset();\n+  return adr_type->is_ptr()->flat_offset();\n@@ -3800,1 +3874,8 @@\n-    t = base_t->add_offset(offs)->is_oopptr();\n+    if (base_t->isa_aryptr() != nullptr) {\n+      \/\/ In the case of a flat inline type array, each field has its\n+      \/\/ own slice so we need to extract the field being accessed from\n+      \/\/ the address computation\n+      t = base_t->isa_aryptr()->add_field_offset_and_offset(offs)->is_oopptr();\n+    } else {\n+      t = base_t->add_offset(offs)->is_oopptr();\n+    }\n@@ -3802,1 +3883,1 @@\n-  int inst_id =  base_t->instance_id();\n+  int inst_id = base_t->instance_id();\n@@ -3816,1 +3897,1 @@\n-  \/\/ It could happened when CHA type is different from MDO type on a dead path\n+  \/\/ It could happen when CHA type is different from MDO type on a dead path\n@@ -3826,1 +3907,12 @@\n-  const TypeOopPtr *tinst = base_t->add_offset(t->offset())->is_oopptr();\n+  const TypePtr* tinst = base_t->add_offset(t->offset());\n+  if (tinst->isa_aryptr() && t->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to keep track of the field being accessed.\n+    tinst = tinst->is_aryptr()->with_field_offset(t->is_aryptr()->field_offset().get());\n+    \/\/ Keep array properties (not flat\/null-free)\n+    tinst = tinst->is_aryptr()->update_properties(t->is_aryptr());\n+    if (tinst == nullptr) {\n+      return false; \/\/ Skip dead path with inconsistent properties\n+    }\n+  }\n+\n@@ -4532,0 +4624,7 @@\n+          if (tn_t->isa_aryptr()) {\n+            \/\/ Keep array properties (not flat\/null-free)\n+            tinst = tinst->is_aryptr()->update_properties(tn_t->is_aryptr());\n+            if (tinst == nullptr) {\n+              continue; \/\/ Skip dead path with inconsistent properties\n+            }\n+          }\n@@ -4557,1 +4656,1 @@\n-      if(use->is_Mem() && use->in(MemNode::Address) == n) {\n+      if (use->is_Mem() && use->in(MemNode::Address) == n) {\n@@ -4593,0 +4692,3 @@\n+      } else if (use->Opcode() == Op_Return) {\n+        \/\/ Allocation is referenced by field of returned inline type\n+        assert(_compile->tf()->returns_inline_type_as_fields(), \"EA: unexpected reference by ReturnNode\");\n@@ -4606,1 +4708,1 @@\n-              op == Op_SubTypeCheck ||\n+              op == Op_SubTypeCheck || op == Op_InlineType || op == Op_FlatArrayCheck ||\n@@ -4706,0 +4808,3 @@\n+    } else if (n->is_CallLeaf() && n->as_CallLeaf()->_name != nullptr &&\n+               strcmp(n->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+      n = n->as_CallLeaf()->proj_out(TypeFunc::Memory);\n@@ -4745,1 +4850,1 @@\n-      } else if(use->is_Mem()) {\n+      } else if (use->is_Mem()) {\n@@ -4754,0 +4859,4 @@\n+      } else if (use->is_CallLeaf() && use->as_CallLeaf()->_name != nullptr &&\n+                 strcmp(use->as_CallLeaf()->_name, \"store_unknown_inline\") == 0) {\n+        \/\/ store_unknown_inline overwrites destination array\n+        memnode_worklist.append_if_missing(use);\n@@ -4763,1 +4872,1 @@\n-              op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar)) {\n+              op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar || op == Op_FlatArrayCheck)) {\n@@ -4864,1 +4973,1 @@\n-  \/\/ chains as is done in split_memory_phi() since they  will\n+  \/\/ chains as is done in split_memory_phi() since they will\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":153,"deletions":44,"binary":false,"changes":197,"status":"modified"},{"patch":"@@ -236,1 +236,1 @@\n-      for (int i = node->req()-1; i >= 0; --i) {\n+      for (int i = node->len()-1; i >= 0; --i) {\n@@ -1450,0 +1450,3 @@\n+      case Op_CastI2N:\n+        early->add_inst(self);\n+        continue;\n","filename":"src\/hotspot\/share\/opto\/gcm.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -50,2 +50,2 @@\n-  const TypeTuple *jdomain = C->tf()->domain();\n-  const TypeTuple *jrange  = C->tf()->range();\n+  const TypeTuple *jdomain = C->tf()->domain_sig();\n+  const TypeTuple *jrange  = C->tf()->range_sig();\n@@ -272,1 +272,1 @@\n-    if (C->tf()->range()->cnt() > TypeFunc::Parms)\n+    if (C->tf()->range_sig()->cnt() > TypeFunc::Parms)\n","filename":"src\/hotspot\/share\/opto\/generateOptoStub.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -25,0 +25,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -39,0 +41,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -42,0 +45,1 @@\n+#include \"opto\/narrowptrnode.hpp\"\n@@ -55,1 +59,1 @@\n-GraphKit::GraphKit(JVMState* jvms)\n+GraphKit::GraphKit(JVMState* jvms, PhaseGVN* gvn)\n@@ -58,1 +62,1 @@\n-    _gvn(*C->initial_gvn()),\n+    _gvn((gvn != nullptr) ? *gvn : *C->initial_gvn()),\n@@ -61,0 +65,1 @@\n+  assert(gvn == nullptr || !gvn->is_IterGVN() || gvn->is_IterGVN()->delay_transform(), \"delay transform should be enabled\");\n@@ -64,0 +69,7 @@\n+#ifdef ASSERT\n+  if (_gvn.is_IterGVN() != nullptr) {\n+    assert(_gvn.is_IterGVN()->delay_transform(), \"Transformation must be delayed if IterGVN is used\");\n+    \/\/ Save the initial size of _for_igvn worklist for verification (see ~GraphKit)\n+    _worklist_size = _gvn.C->igvn_worklist()->size();\n+  }\n+#endif\n@@ -861,1 +873,1 @@\n-           (is_anewarray && code == Bytecodes::_multianewarray);\n+           (is_anewarray && (code == Bytecodes::_multianewarray));\n@@ -952,0 +964,2 @@\n+\n+  JVMState* callee_jvms = nullptr;\n@@ -977,2 +991,10 @@\n-      for (j = 0; j < l; j++)\n-        call->set_req(p++, in_map->in(k+j));\n+      for (j = 0; j < l; j++) {\n+        Node* val = in_map->in(k + j);\n+        \/\/ Check if there's a larval that has been written in the callee state (constructor) and update it in the caller state\n+        if (callee_jvms != nullptr && val->is_InlineType() && val->as_InlineType()->is_larval() &&\n+            callee_jvms->method()->is_object_constructor() && val == in_map->argument(in_jvms, 0) &&\n+            val->bottom_type()->is_inlinetypeptr()) {\n+          val = callee_jvms->map()->local(callee_jvms, 0); \/\/ Receiver\n+        }\n+        call->set_req(p++, val);\n+      }\n@@ -988,2 +1010,10 @@\n-      for (j = 0; j < l; j++)\n-        call->set_req(p++, in_map->in(k+j));\n+      for (j = 0; j < l; j++) {\n+        Node* val = in_map->in(k + j);\n+        \/\/ Check if there's a larval that has been written in the callee state (constructor) and update it in the caller state\n+        if (callee_jvms != nullptr && val->is_InlineType() && val->as_InlineType()->is_larval() &&\n+            callee_jvms->method()->is_object_constructor() && val == in_map->argument(in_jvms, 0) &&\n+            val->bottom_type()->is_inlinetypeptr()) {\n+          val = callee_jvms->map()->local(callee_jvms, 0); \/\/ Receiver\n+        }\n+        call->set_req(p++, val);\n+      }\n@@ -1028,0 +1058,1 @@\n+    callee_jvms = out_jvms;\n@@ -1203,1 +1234,1 @@\n-  return _gvn.transform(LoadKlassNode::make(_gvn, nullptr, immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+  return _gvn.transform(LoadKlassNode::make(_gvn, nullptr, immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n@@ -1252,1 +1283,2 @@\n-                                  bool speculative) {\n+                                  bool speculative,\n+                                  bool is_init_check) {\n@@ -1257,0 +1289,23 @@\n+  if (value->is_InlineType()) {\n+    \/\/ Null checking a scalarized but nullable inline type. Check the IsInit\n+    \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+    InlineTypeNode* vtptr = value->as_InlineType();\n+    while (vtptr->get_oop()->is_InlineType()) {\n+      vtptr = vtptr->get_oop()->as_InlineType();\n+    }\n+    null_check_common(vtptr->get_is_init(), T_INT, assert_null, null_control, speculative, true);\n+    if (stopped()) {\n+      return top();\n+    }\n+    if (assert_null) {\n+      \/\/ TODO 8284443 Scalarize here (this currently leads to compilation bailouts)\n+      \/\/ vtptr = InlineTypeNode::make_null(_gvn, vtptr->type()->inline_klass());\n+      \/\/ replace_in_map(value, vtptr);\n+      \/\/ return vtptr;\n+      replace_in_map(value, null());\n+      return null();\n+    }\n+    bool do_replace_in_map = (null_control == nullptr || (*null_control) == top());\n+    return cast_not_null(value, do_replace_in_map);\n+  }\n+\n@@ -1360,1 +1415,1 @@\n-  } else if (type == T_OBJECT) {\n+  } else if (type == T_OBJECT || is_init_check) {\n@@ -1434,1 +1489,0 @@\n-\n@@ -1438,0 +1492,9 @@\n+  if (obj->is_InlineType()) {\n+    Node* vt = obj->isa_InlineType()->clone_if_required(&gvn(), map(), do_replace_in_map);\n+    vt->as_InlineType()->set_is_init(_gvn);\n+    vt = _gvn.transform(vt);\n+    if (do_replace_in_map) {\n+      replace_in_map(obj, vt);\n+    }\n+    return vt;\n+  }\n@@ -1566,0 +1629,1 @@\n+\n@@ -1619,1 +1683,3 @@\n-                                DecoratorSet decorators) {\n+                                DecoratorSet decorators,\n+                                bool safe_for_replace,\n+                                const InlineTypeNode* vt) {\n@@ -1632,0 +1698,7 @@\n+  if (val->is_InlineType()) {\n+    \/\/ Store to non-flat field. Buffer the inline type and make sure\n+    \/\/ the store is re-executed if the allocation triggers deoptimization.\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    val = val->as_InlineType()->buffer(this, safe_for_replace);\n+  }\n@@ -1635,1 +1708,1 @@\n-  C2ParseAccess access(this, decorators | C2_WRITE_ACCESS, bt, obj, addr);\n+  C2ParseAccess access(this, decorators | C2_WRITE_ACCESS, bt, obj, addr, nullptr, vt);\n@@ -1648,1 +1721,2 @@\n-                               DecoratorSet decorators) {\n+                               DecoratorSet decorators,\n+                               Node* ctl) {\n@@ -1654,1 +1728,1 @@\n-  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr);\n+  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr, ctl);\n@@ -1759,1 +1833,13 @@\n-  uint shift  = exact_log2(type2aelembytes(elembt));\n+  const TypeAryPtr* arytype = _gvn.type(ary)->is_aryptr();\n+  assert(!arytype->is_flat() || elembt == T_OBJECT, \"element type of flat arrays are T_OBJECT\");\n+  uint shift;\n+  if (arytype->is_flat() && arytype->klass_is_exact()) {\n+    \/\/ We can only determine the flat array layout statically if the klass is exact. Otherwise, we could have different\n+    \/\/ value classes at runtime with a potentially different layout. The caller needs to fall back to call\n+    \/\/ load\/store_unknown_inline_Type() at runtime. We could return a sentinel node for the non-exact case but that\n+    \/\/ might mess with other GVN transformations in between. Thus, we just continue in the else branch normally, even\n+    \/\/ though we don't need the address node in this case and throw it away again.\n+    shift = arytype->flat_log_elem_size();\n+  } else {\n+    shift = exact_log2(type2aelembytes(elembt));\n+  }\n@@ -1791,6 +1877,63 @@\n-void GraphKit::set_arguments_for_java_call(CallJavaNode* call) {\n-  \/\/ Add the call arguments:\n-  uint nargs = call->method()->arg_size();\n-  for (uint i = 0; i < nargs; i++) {\n-    Node* arg = argument(i);\n-    call->init_req(i + TypeFunc::Parms, arg);\n+void GraphKit::set_arguments_for_java_call(CallJavaNode* call, bool is_late_inline) {\n+  PreserveReexecuteState preexecs(this);\n+  if (EnableValhalla) {\n+    \/\/ Make sure the call is \"re-executed\", if buffering of inline type arguments triggers deoptimization.\n+    \/\/ At this point, the call hasn't been executed yet, so we will only ever execute the call once.\n+    jvms()->set_should_reexecute(true);\n+    int arg_size = method()->get_declared_signature_at_bci(bci())->arg_size_for_bc(java_bc());\n+    inc_sp(arg_size);\n+  }\n+  \/\/ Add the call arguments\n+  const TypeTuple* domain = call->tf()->domain_sig();\n+  uint nargs = domain->cnt();\n+  int arg_num = 0;\n+  for (uint i = TypeFunc::Parms, idx = TypeFunc::Parms; i < nargs; i++) {\n+    Node* arg = argument(i-TypeFunc::Parms);\n+    const Type* t = domain->field_at(i);\n+    \/\/ TODO 8284443 A static call to a mismatched method should still be scalarized\n+    if (t->is_inlinetypeptr() && !call->method()->get_Method()->mismatch() && call->method()->is_scalarized_arg(arg_num)) {\n+      \/\/ We don't pass inline type arguments by reference but instead pass each field of the inline type\n+      if (!arg->is_InlineType()) {\n+        assert(_gvn.type(arg)->is_zero_type() && !t->inline_klass()->is_null_free(), \"Unexpected argument type\");\n+        arg = InlineTypeNode::make_from_oop(this, arg, t->inline_klass(), t->inline_klass()->is_null_free());\n+      }\n+      InlineTypeNode* vt = arg->as_InlineType();\n+      vt->pass_fields(this, call, idx, true, !t->maybe_null());\n+      \/\/ If an inline type argument is passed as fields, attach the Method* to the call site\n+      \/\/ to be able to access the extended signature later via attached_method_before_pc().\n+      \/\/ For example, see CompiledMethod::preserve_callee_argument_oops().\n+      call->set_override_symbolic_info(true);\n+      \/\/ Register an evol dependency on the callee method to make sure that this method is deoptimized and\n+      \/\/ re-compiled with a non-scalarized calling convention if the callee method is later marked as mismatched.\n+      C->dependencies()->assert_evol_method(call->method());\n+      arg_num++;\n+      continue;\n+    } else if (arg->is_InlineType()) {\n+      \/\/ Pass inline type argument via oop to callee\n+      InlineTypeNode* inline_type = arg->as_InlineType();\n+      const ciMethod* method = call->method();\n+      ciInstanceKlass* holder = method->holder();\n+      const bool is_receiver = (i == TypeFunc::Parms);\n+      const bool is_abstract_or_object_klass_constructor = method->is_object_constructor() &&\n+                                                           (holder->is_abstract() || holder->is_java_lang_Object());\n+      const bool is_larval_receiver_on_super_constructor = is_receiver && is_abstract_or_object_klass_constructor;\n+      bool must_init_buffer = true;\n+      \/\/ We always need to buffer inline types when they are escaping. However, we can skip the actual initialization\n+      \/\/ of the buffer if the inline type is a larval because we are going to update the buffer anyway which requires\n+      \/\/ us to create a new one. But there is one special case where we are still required to initialize the buffer:\n+      \/\/ When we have a larval receiver invoked on an abstract (value class) constructor or the Object constructor (that\n+      \/\/ is not going to be inlined). After this call, the larval is completely initialized and thus not a larval anymore.\n+      \/\/ We therefore need to force an initialization of the buffer to not lose all the field writes so far in case the\n+      \/\/ buffer needs to be used (e.g. to read from when deoptimizing at runtime) or further updated in abstract super\n+      \/\/ value class constructors which could have more fields to be initialized. Note that we do not need to\n+      \/\/ initialize the buffer when invoking another constructor in the same class on a larval receiver because we\n+      \/\/ have not initialized any fields, yet (this is done completely by the other constructor call).\n+      if (inline_type->is_larval() && !is_larval_receiver_on_super_constructor) {\n+        must_init_buffer = false;\n+      }\n+      arg = inline_type->buffer(this, true, must_init_buffer);\n+    }\n+    if (t != Type::HALF) {\n+      arg_num++;\n+    }\n+    call->init_req(idx++, arg);\n@@ -1834,7 +1977,0 @@\n-  \/\/ Capture the return value, if any.\n-  Node* ret;\n-  if (call->method() == nullptr ||\n-      call->method()->return_type()->basic_type() == T_VOID)\n-        ret = top();\n-  else  ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n-\n@@ -1853,0 +1989,36 @@\n+\n+  \/\/ Capture the return value, if any.\n+  Node* ret;\n+  if (call->method() == nullptr || call->method()->return_type()->basic_type() == T_VOID) {\n+    ret = top();\n+  } else if (call->tf()->returns_inline_type_as_fields()) {\n+    \/\/ Return of multiple values (inline type fields): we create a\n+    \/\/ InlineType node, each field is a projection from the call.\n+    ciInlineKlass* vk = call->method()->return_type()->as_inline_klass();\n+    uint base_input = TypeFunc::Parms;\n+    ret = InlineTypeNode::make_from_multi(this, call, vk, base_input, false, false);\n+  } else {\n+    ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+    ciType* t = call->method()->return_type();\n+    if (t->is_klass()) {\n+      const Type* type = TypeOopPtr::make_from_klass(t->as_klass());\n+      if (type->is_inlinetypeptr()) {\n+        ret = InlineTypeNode::make_from_oop(this, ret, type->inline_klass(), type->inline_klass()->is_null_free());\n+      }\n+    }\n+  }\n+\n+  \/\/ We just called the constructor on a value type receiver. Reload it from the buffer\n+  ciMethod* method = call->method();\n+  if (method->is_object_constructor() && !method->holder()->is_java_lang_Object()) {\n+    InlineTypeNode* inline_type_receiver = call->in(TypeFunc::Parms)->isa_InlineType();\n+    if (inline_type_receiver != nullptr) {\n+      assert(inline_type_receiver->is_larval(), \"must be larval\");\n+      assert(inline_type_receiver->is_allocated(&gvn()), \"larval must be buffered\");\n+      InlineTypeNode* reloaded = InlineTypeNode::make_from_oop(this, inline_type_receiver->get_oop(),\n+                                                               inline_type_receiver->bottom_type()->inline_klass(), true);\n+      assert(!reloaded->is_larval(), \"should not be larval anymore\");\n+      replace_in_map(inline_type_receiver, reloaded);\n+    }\n+  }\n+\n@@ -1943,2 +2115,1 @@\n-  CallProjections callprojs;\n-  call->extract_projections(&callprojs, true, do_asserts);\n+  CallProjections* callprojs = call->extract_projections(true, do_asserts);\n@@ -1953,2 +2124,2 @@\n-  if (callprojs.fallthrough_catchproj != nullptr) {\n-    C->gvn_replace_by(callprojs.fallthrough_catchproj, final_ctl);\n+  if (callprojs->fallthrough_catchproj != nullptr) {\n+    C->gvn_replace_by(callprojs->fallthrough_catchproj, final_ctl);\n@@ -1956,1 +2127,1 @@\n-  if (callprojs.fallthrough_memproj != nullptr) {\n+  if (callprojs->fallthrough_memproj != nullptr) {\n@@ -1961,1 +2132,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_memproj,   final_mem);\n+    C->gvn_replace_by(callprojs->fallthrough_memproj,   final_mem);\n@@ -1964,2 +2135,2 @@\n-  if (callprojs.fallthrough_ioproj != nullptr) {\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj,    final_io);\n+  if (callprojs->fallthrough_ioproj != nullptr) {\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj,    final_io);\n@@ -1969,2 +2140,6 @@\n-  if (callprojs.resproj != nullptr && result != nullptr) {\n-    C->gvn_replace_by(callprojs.resproj, result);\n+  if (callprojs->resproj[0] != nullptr && result != nullptr) {\n+    \/\/ If the inlined code is dead, the result projections for an inline type returned as\n+    \/\/ fields have not been replaced. They will go away once the call is replaced by TOP below.\n+    assert(callprojs->nb_resproj == 1 || (call->tf()->returns_inline_type_as_fields() && stopped()),\n+           \"unexpected number of results\");\n+    C->gvn_replace_by(callprojs->resproj[0], result);\n@@ -1975,2 +2150,2 @@\n-    if (callprojs.catchall_catchproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, C->top());\n+    if (callprojs->catchall_catchproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, C->top());\n@@ -1978,2 +2153,2 @@\n-    if (callprojs.catchall_memproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_memproj,   C->top());\n+    if (callprojs->catchall_memproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_memproj,   C->top());\n@@ -1981,2 +2156,2 @@\n-    if (callprojs.catchall_ioproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    C->top());\n+    if (callprojs->catchall_ioproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    C->top());\n@@ -1985,2 +2160,2 @@\n-    if (callprojs.exobj != nullptr) {\n-      C->gvn_replace_by(callprojs.exobj, C->top());\n+    if (callprojs->exobj != nullptr) {\n+      C->gvn_replace_by(callprojs->exobj, C->top());\n@@ -1997,2 +2172,2 @@\n-    if (callprojs.catchall_catchproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_catchproj, ekit.control());\n+    if (callprojs->catchall_catchproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_catchproj, ekit.control());\n@@ -2001,1 +2176,1 @@\n-    if (callprojs.catchall_memproj != nullptr) {\n+    if (callprojs->catchall_memproj != nullptr) {\n@@ -2003,1 +2178,1 @@\n-      C->gvn_replace_by(callprojs.catchall_memproj,   ex_mem);\n+      C->gvn_replace_by(callprojs->catchall_memproj,   ex_mem);\n@@ -2006,2 +2181,2 @@\n-    if (callprojs.catchall_ioproj != nullptr) {\n-      C->gvn_replace_by(callprojs.catchall_ioproj,    ekit.i_o());\n+    if (callprojs->catchall_ioproj != nullptr) {\n+      C->gvn_replace_by(callprojs->catchall_ioproj,    ekit.i_o());\n@@ -2011,2 +2186,2 @@\n-    if (callprojs.exobj != nullptr) {\n-      C->gvn_replace_by(callprojs.exobj, ex_oop);\n+    if (callprojs->exobj != nullptr) {\n+      C->gvn_replace_by(callprojs->exobj, ex_oop);\n@@ -2026,1 +2201,1 @@\n-  if (callprojs.fallthrough_catchproj != nullptr && !final_ctl->is_top() && do_replaced_nodes) {\n+  if (callprojs->fallthrough_catchproj != nullptr && !final_ctl->is_top() && do_replaced_nodes) {\n@@ -2226,1 +2401,1 @@\n-    const TypePtr* ptr = (ptr_kind == ProfileMaybeNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n+    const TypePtr* ptr = (ptr_kind != ProfileNeverNull && current_type->speculative_maybe_null()) ? TypePtr::BOTTOM : TypePtr::NOTNULL;\n@@ -2249,1 +2424,1 @@\n-    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::OffsetBot, TypeOopPtr::InstanceBot, speculative);\n+    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::Offset::bottom, TypeOopPtr::InstanceBot, speculative);\n@@ -2283,2 +2458,9 @@\n-      if (!data->as_BitData()->null_seen()) {\n-        ptr_kind = ProfileNeverNull;\n+      if (java_bc() == Bytecodes::_aastore) {\n+        ciKlass* array_type = nullptr;\n+        ciKlass* element_type = nullptr;\n+        ProfilePtrKind element_ptr = ProfileMaybeNull;\n+        bool flat_array = true;\n+        bool null_free_array = true;\n+        method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+        exact_kls = element_type;\n+        ptr_kind = element_ptr;\n@@ -2286,7 +2468,11 @@\n-        assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n-        ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n-        uint i = 0;\n-        for (; i < call->row_limit(); i++) {\n-          ciKlass* receiver = call->receiver(i);\n-          if (receiver != nullptr) {\n-            break;\n+        if (!data->as_BitData()->null_seen()) {\n+          ptr_kind = ProfileNeverNull;\n+        } else {\n+          assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n+          ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n+          uint i = 0;\n+          for (; i < call->row_limit(); i++) {\n+            ciKlass* receiver = call->receiver(i);\n+            if (receiver != nullptr) {\n+              break;\n+            }\n@@ -2294,0 +2480,1 @@\n+          ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2295,1 +2482,0 @@\n-        ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2314,1 +2500,1 @@\n-  int             nargs = tf->domain()->cnt() - TypeFunc::Parms;\n+  int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;\n@@ -2317,1 +2503,1 @@\n-    const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);\n+    const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);\n@@ -2371,1 +2557,1 @@\n-    int             nargs = tf->domain()->cnt() - TypeFunc::Parms;\n+    int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;\n@@ -2373,1 +2559,1 @@\n-      const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);\n+      const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);\n@@ -2522,1 +2708,1 @@\n-    uint num_bits = call_type->range()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte;\n+    uint num_bits = call_type->range_sig()->field_at(TypeFunc::Parms)->is_vect()->length_in_bytes() * BitsPerByte;\n@@ -2602,0 +2788,1 @@\n+\n@@ -2904,0 +3091,5 @@\n+  const Type* sub_t = _gvn.type(obj_or_subklass);\n+  if (sub_t->make_oopptr() != nullptr && sub_t->make_oopptr()->is_inlinetypeptr()) {\n+    sub_t = TypeKlassPtr::make(sub_t->inline_klass());\n+    obj_or_subklass = makecon(sub_t);\n+  }\n@@ -2909,1 +3101,1 @@\n-    if (!_gvn.type(obj_or_subklass)->isa_klassptr()) {\n+    if (!sub_t->isa_klassptr()) {\n@@ -2927,2 +3119,1 @@\n-                                    float prob,\n-                                    Node* *casted_receiver) {\n+                                    float prob, Node* *casted_receiver) {\n@@ -2930,1 +3121,12 @@\n-\n+  Node* fail = top();\n+  const Type* rec_t = _gvn.type(receiver);\n+  if (rec_t->is_inlinetypeptr()) {\n+    if (klass->equals(rec_t->inline_klass())) {\n+      (*casted_receiver) = receiver; \/\/ Always passes\n+    } else {\n+      (*casted_receiver) = top();    \/\/ Always fails\n+      fail = control();\n+      set_control(top());\n+    }\n+    return fail;\n+  }\n@@ -2933,6 +3135,1 @@\n-  Node* want_klass = makecon(tklass);\n-  Node* cmp = _gvn.transform(new CmpPNode(recv_klass, want_klass));\n-  Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n-  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n-  set_control( _gvn.transform(new IfTrueNode (iff)));\n-  Node* fail = _gvn.transform(new IfFalseNode(iff));\n+  fail = type_check(recv_klass, tklass, prob);\n@@ -2942,2 +3139,2 @@\n-    const TypeOopPtr* recvx_type = tklass->as_instance_type();\n-    assert(recvx_type->klass_is_exact(), \"\");\n+    const TypeOopPtr* recv_xtype = tklass->as_instance_type();\n+    assert(recv_xtype->klass_is_exact(), \"\");\n@@ -2945,1 +3142,1 @@\n-    if (!receiver_type->higher_equal(recvx_type)) { \/\/ ignore redundant casts\n+    if (!receiver_type->higher_equal(recv_xtype)) { \/\/ ignore redundant casts\n@@ -2948,2 +3145,7 @@\n-      Node* cast = new CheckCastPPNode(control(), receiver, recvx_type);\n-      (*casted_receiver) = _gvn.transform(cast);\n+      Node* cast = new CheckCastPPNode(control(), receiver, recv_xtype);\n+      Node* res = _gvn.transform(cast);\n+      if (recv_xtype->is_inlinetypeptr()) {\n+        assert(!gvn().type(res)->maybe_null(), \"receiver should never be null\");\n+        res = InlineTypeNode::make_from_oop(this, res, recv_xtype->inline_klass());\n+      }\n+      (*casted_receiver) = res;\n@@ -2958,0 +3160,11 @@\n+Node* GraphKit::type_check(Node* recv_klass, const TypeKlassPtr* tklass,\n+                           float prob) {\n+  Node* want_klass = makecon(tklass);\n+  Node* cmp = _gvn.transform(new CmpPNode(recv_klass, want_klass));\n+  Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n+  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);\n+  set_control(_gvn.transform(new IfTrueNode (iff)));\n+  Node* fail = _gvn.transform(new IfFalseNode(iff));\n+  return fail;\n+}\n+\n@@ -2970,3 +3183,6 @@\n-    if (!receiver_type->higher_equal(recv_type)) { \/\/ ignore redundant casts\n-      Node* cast = new CheckCastPPNode(control(), receiver, recv_type);\n-      (*casted_receiver) = _gvn.transform(cast);\n+    if (receiver_type != nullptr && !receiver_type->higher_equal(recv_type)) { \/\/ ignore redundant casts\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), receiver, recv_type));\n+      if (recv_type->is_inlinetypeptr()) {\n+        cast = InlineTypeNode::make_from_oop(this, cast, recv_type->inline_klass());\n+      }\n+      (*casted_receiver) = cast;\n@@ -3081,1 +3297,14 @@\n-  ciKlass* exact_kls = spec_klass == nullptr ? profile_has_unique_klass() : spec_klass;\n+  ciKlass* exact_kls = spec_klass;\n+  if (exact_kls == nullptr) {\n+    if (java_bc() == Bytecodes::_aastore) {\n+      ciKlass* array_type = nullptr;\n+      ciKlass* element_type = nullptr;\n+      ProfilePtrKind element_ptr = ProfileMaybeNull;\n+      bool flat_array = true;\n+      bool null_free_array = true;\n+      method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+      exact_kls = element_type;\n+    } else {\n+      exact_kls = profile_has_unique_klass();\n+    }\n+  }\n@@ -3211,1 +3440,1 @@\n-    if (subk->is_loaded()) {\n+    if (subk != nullptr && subk->is_loaded()) {\n@@ -3267,2 +3496,1 @@\n-Node* GraphKit::gen_checkcast(Node *obj, Node* superklass,\n-                              Node* *failure_control) {\n+Node* GraphKit::gen_checkcast(Node *obj, Node* superklass, Node* *failure_control, bool null_free) {\n@@ -3273,0 +3501,2 @@\n+  bool safe_for_replace = (failure_control == nullptr);\n+  assert(!null_free || toop->can_be_inline_type(), \"must be an inline type pointer\");\n@@ -3281,3 +3511,10 @@\n-    const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();\n-    if (objtp != nullptr) {\n-      switch (C->static_subtype_check(improved_klass_ptr_type, objtp->as_klass_type())) {\n+    const TypeKlassPtr* kptr = nullptr;\n+    const Type* t = _gvn.type(obj);\n+    if (t->isa_oop_ptr()) {\n+      kptr = t->is_oopptr()->as_klass_type();\n+    } else if (obj->is_InlineType()) {\n+      ciInlineKlass* vk = t->inline_klass();\n+      kptr = TypeInstKlassPtr::make(TypePtr::NotNull, vk, Type::Offset(0));\n+    }\n+    if (kptr != nullptr) {\n+      switch (C->static_subtype_check(improved_klass_ptr_type, kptr)) {\n@@ -3288,1 +3525,7 @@\n-        return record_profiled_receiver_for_speculation(obj);\n+        obj = record_profiled_receiver_for_speculation(obj);\n+        if (null_free) {\n+          assert(safe_for_replace, \"must be\");\n+          obj = null_check(obj);\n+        }\n+        assert(stopped() || !toop->is_inlinetypeptr() || obj->is_InlineType(), \"should have been scalarized\");\n+        return obj;\n@@ -3290,0 +3533,4 @@\n+        if (null_free) {\n+          assert(safe_for_replace, \"must be\");\n+          obj = null_check(obj);\n+        }\n@@ -3291,2 +3538,1 @@\n-        \/\/ A non-null value will always produce an exception.\n-        if (!objtp->maybe_null()) {\n+        if (t->isa_oopptr() != nullptr && !t->is_oopptr()->maybe_null()) {\n@@ -3309,1 +3555,0 @@\n-  bool safe_for_replace = false;\n@@ -3314,2 +3559,3 @@\n-    data = method()->method_data()->bci_to_data(bci());\n-    safe_for_replace = true;\n+    if (method()->method_data()->is_mature()) {\n+      data = method()->method_data()->bci_to_data(bci());\n+    }\n@@ -3322,0 +3568,3 @@\n+  _gvn.set_type(region, Type::CONTROL);\n+  _gvn.set_type(phi, toop);\n+\n@@ -3329,0 +3578,7 @@\n+  if (obj->is_InlineType()) {\n+    \/\/ Re-execute if buffering during triggers deoptimization\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    obj = obj->as_InlineType()->buffer(this, safe_for_replace);\n+  }\n+\n@@ -3331,1 +3587,7 @@\n-  Node* not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  Node* not_null_obj = nullptr;\n+  if (null_free) {\n+    assert(safe_for_replace, \"must be\");\n+    not_null_obj = null_check(obj);\n+  } else {\n+    not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);\n+  }\n@@ -3336,0 +3598,3 @@\n+    if (toop->is_inlinetypeptr()) {\n+      return InlineTypeNode::make_null(_gvn, toop->inline_klass());\n+    }\n@@ -3373,0 +3638,3 @@\n+      \/\/ Only improve the super class for constants which allows subsequent sub type checks to possibly be commoned up.\n+      \/\/ The other non-constant cases cannot be improved with a cast node here since they could be folded to top.\n+      \/\/ Additionally, the benefit would only be minor in non-constant cases.\n@@ -3376,1 +3644,0 @@\n-\n@@ -3384,0 +3651,6 @@\n+        Node* obj_klass = nullptr;\n+        if (not_null_obj->is_InlineType()) {\n+          obj_klass = makecon(TypeKlassPtr::make(_gvn.type(not_null_obj)->inline_klass()));\n+        } else {\n+          obj_klass = load_object_klass(not_null_obj);\n+        }\n@@ -3414,1 +3687,143 @@\n-  return record_profiled_receiver_for_speculation(res);\n+  bool not_inline = !toop->can_be_inline_type();\n+  bool not_flat_in_array = !UseArrayFlattening || not_inline || (toop->is_inlinetypeptr() && !toop->inline_klass()->flat_in_array());\n+  if (EnableValhalla && not_flat_in_array) {\n+    \/\/ Check if obj has been loaded from an array\n+    obj = obj->isa_DecodeN() ? obj->in(1) : obj;\n+    Node* array = nullptr;\n+    if (obj->isa_Load()) {\n+      Node* address = obj->in(MemNode::Address);\n+      if (address->isa_AddP()) {\n+        array = address->as_AddP()->in(AddPNode::Base);\n+      }\n+    } else if (obj->is_Phi()) {\n+      Node* region = obj->in(0);\n+      \/\/ TODO make this more robust (see JDK-8231346)\n+      if (region->req() == 3 && region->in(2) != nullptr && region->in(2)->in(0) != nullptr) {\n+        IfNode* iff = region->in(2)->in(0)->isa_If();\n+        if (iff != nullptr) {\n+          iff->is_flat_array_check(&_gvn, &array);\n+        }\n+      }\n+    }\n+    if (array != nullptr) {\n+      const TypeAryPtr* ary_t = _gvn.type(array)->isa_aryptr();\n+      if (ary_t != nullptr && !ary_t->is_flat()) {\n+        if (!ary_t->is_not_null_free() && not_inline) {\n+          \/\/ Casting array element to a non-inline-type, mark array as not null-free.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_null_free()));\n+          replace_in_map(array, cast);\n+        } else if (!ary_t->is_not_flat()) {\n+          \/\/ Casting array element to a non-flat type, mark array as not flat.\n+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_flat()));\n+          replace_in_map(array, cast);\n+        }\n+      }\n+    }\n+  }\n+\n+  if (!stopped() && !res->is_InlineType()) {\n+    res = record_profiled_receiver_for_speculation(res);\n+    if (toop->is_inlinetypeptr()) {\n+      Node* vt = InlineTypeNode::make_from_oop(this, res, toop->inline_klass(), !gvn().type(res)->maybe_null());\n+      res = vt;\n+      if (safe_for_replace) {\n+        replace_in_map(obj, vt);\n+        replace_in_map(not_null_obj, vt);\n+        replace_in_map(res, vt);\n+      }\n+    }\n+  }\n+  return res;\n+}\n+\n+Node* GraphKit::mark_word_test(Node* obj, uintptr_t mask_val, bool eq, bool check_lock) {\n+  \/\/ Load markword\n+  Node* mark_adr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+  Node* mark = make_load(nullptr, mark_adr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  if (check_lock) {\n+    \/\/ Check if obj is locked\n+    Node* locked_bit = MakeConX(markWord::unlocked_value);\n+    locked_bit = _gvn.transform(new AndXNode(locked_bit, mark));\n+    Node* cmp = _gvn.transform(new CmpXNode(locked_bit, MakeConX(0)));\n+    Node* is_unlocked = _gvn.transform(new BoolNode(cmp, BoolTest::ne));\n+    IfNode* iff = new IfNode(control(), is_unlocked, PROB_MAX, COUNT_UNKNOWN);\n+    _gvn.transform(iff);\n+    Node* locked_region = new RegionNode(3);\n+    Node* mark_phi = new PhiNode(locked_region, TypeX_X);\n+\n+    \/\/ Unlocked: Use bits from mark word\n+    locked_region->init_req(1, _gvn.transform(new IfTrueNode(iff)));\n+    mark_phi->init_req(1, mark);\n+\n+    \/\/ Locked: Load prototype header from klass\n+    set_control(_gvn.transform(new IfFalseNode(iff)));\n+    \/\/ Make loads control dependent to make sure they are only executed if array is locked\n+    Node* klass_adr = basic_plus_adr(obj, oopDesc::klass_offset_in_bytes());\n+    Node* klass = _gvn.transform(LoadKlassNode::make(_gvn, control(), C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+    Node* proto_adr = basic_plus_adr(klass, in_bytes(Klass::prototype_header_offset()));\n+    Node* proto = _gvn.transform(LoadNode::make(_gvn, control(), C->immutable_memory(), proto_adr, proto_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+\n+    locked_region->init_req(2, control());\n+    mark_phi->init_req(2, proto);\n+    set_control(_gvn.transform(locked_region));\n+    record_for_igvn(locked_region);\n+\n+    mark = mark_phi;\n+  }\n+\n+  \/\/ Now check if mark word bits are set\n+  Node* mask = MakeConX(mask_val);\n+  Node* masked = _gvn.transform(new AndXNode(_gvn.transform(mark), mask));\n+  record_for_igvn(masked); \/\/ Give it a chance to be optimized out by IGVN\n+  Node* cmp = _gvn.transform(new CmpXNode(masked, mask));\n+  return _gvn.transform(new BoolNode(cmp, eq ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::inline_type_test(Node* obj, bool is_inline) {\n+  return mark_word_test(obj, markWord::inline_type_pattern, is_inline, \/* check_lock = *\/ false);\n+}\n+\n+Node* GraphKit::flat_array_test(Node* array_or_klass, bool flat) {\n+  \/\/ We can't use immutable memory here because the mark word is mutable.\n+  \/\/ PhaseIdealLoop::move_flat_array_check_out_of_loop will make sure the\n+  \/\/ check is moved out of loops (mainly to enable loop unswitching).\n+  Node* cmp = _gvn.transform(new FlatArrayCheckNode(C, memory(Compile::AliasIdxRaw), array_or_klass));\n+  record_for_igvn(cmp); \/\/ Give it a chance to be optimized out by IGVN\n+  return _gvn.transform(new BoolNode(cmp, flat ? BoolTest::eq : BoolTest::ne));\n+}\n+\n+Node* GraphKit::null_free_array_test(Node* array, bool null_free) {\n+  return mark_word_test(array, markWord::null_free_array_bit_in_place, null_free);\n+}\n+\n+\/\/ Deoptimize if 'ary' is a null-free inline type array and 'val' is null\n+Node* GraphKit::inline_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace) {\n+  RegionNode* region = new RegionNode(3);\n+  Node* null_ctl = top();\n+  null_check_oop(val, &null_ctl);\n+  if (null_ctl != top()) {\n+    PreserveJVMState pjvms(this);\n+    set_control(null_ctl);\n+    {\n+      \/\/ Deoptimize if null-free array\n+      BuildCutout unless(this, null_free_array_test(ary, \/* null_free = *\/ false), PROB_MAX);\n+      inc_sp(nargs);\n+      uncommon_trap(Deoptimization::Reason_null_check,\n+                    Deoptimization::Action_none);\n+    }\n+    region->init_req(1, control());\n+  }\n+  region->init_req(2, control());\n+  set_control(_gvn.transform(region));\n+  record_for_igvn(region);\n+  if (_gvn.type(val) == TypePtr::NULL_PTR) {\n+    \/\/ Since we were just successfully storing null, the array can't be null free.\n+    const TypeAryPtr* ary_t = _gvn.type(ary)->is_aryptr();\n+    ary_t = ary_t->cast_to_not_null_free();\n+    Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));\n+    if (safe_for_replace) {\n+      replace_in_map(ary, cast);\n+    }\n+    ary = cast;\n+  }\n+  return ary;\n@@ -3482,0 +3897,1 @@\n+\n@@ -3550,0 +3966,1 @@\n+  assert(!obj->is_InlineType(), \"should not unlock on inline type\");\n@@ -3590,1 +4007,8 @@\n-    if (xklass || (klass_t->isa_aryklassptr() && klass_t->is_aryklassptr()->elem() != Type::BOTTOM)) {\n+    bool can_be_flat = false;\n+    const TypeAryPtr* ary_type = klass_t->as_instance_type()->isa_aryptr();\n+    if (UseArrayFlattening && !xklass && ary_type != nullptr && !ary_type->is_null_free()) {\n+      \/\/ Don't constant fold if the runtime type might be a flat array but the static type is not.\n+      const TypeOopPtr* elem = ary_type->elem()->make_oopptr();\n+      can_be_flat = ary_type->can_be_inline_array() && (!elem->is_inlinetypeptr() || elem->inline_klass()->flat_in_array());\n+    }\n+    if (!can_be_flat && (xklass || (klass_t->isa_aryklassptr() && klass_t->is_aryklassptr()->elem() != Type::BOTTOM))) {\n@@ -3592,2 +4016,4 @@\n-      if (klass_t->isa_aryklassptr()) {\n-        BasicType elem = klass_t->as_instance_type()->isa_aryptr()->elem()->array_element_basic_type();\n+      if (klass_t->is_flat()) {\n+        lhelper = ary_type->flat_layout_helper();\n+      } else if (klass_t->isa_aryklassptr()) {\n+        BasicType elem = ary_type->elem()->array_element_basic_type();\n@@ -3622,1 +4048,3 @@\n-  kit.set_memory(init_out_raw, alias_idx);\n+  if (init_out_raw != nullptr) {\n+    kit.set_memory(init_out_raw, alias_idx);\n+  }\n@@ -3661,0 +4089,1 @@\n+    _gvn.set_type(minit_in, Type::MEMORY);\n@@ -3668,3 +4097,28 @@\n-      const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);\n-      int            elemidx  = C->get_alias_index(telemref);\n-      hook_memory_on_init(*this, elemidx, minit_in, minit_out);\n+      const TypeAryPtr* arytype = oop_type->is_aryptr();\n+      if (arytype->is_flat()) {\n+        \/\/ Initially all flat array accesses share a single slice\n+        \/\/ but that changes after parsing. Prepare the memory graph so\n+        \/\/ it can optimize flat array accesses properly once they\n+        \/\/ don't share a single slice.\n+        assert(C->flat_accesses_share_alias(), \"should be set at parse time\");\n+        C->set_flat_accesses_share_alias(false);\n+        ciInlineKlass* vk = arytype->elem()->inline_klass();\n+        for (int i = 0, len = vk->nof_nonstatic_fields(); i < len; i++) {\n+          ciField* field = vk->nonstatic_field_at(i);\n+          if (field->offset_in_bytes() >= TrackedInitializationLimit * HeapWordSize)\n+            continue;  \/\/ do not bother to track really large numbers of fields\n+          int off_in_vt = field->offset_in_bytes() - vk->payload_offset();\n+          const TypePtr* adr_type = arytype->with_field_offset(off_in_vt)->add_offset(Type::OffsetBot);\n+          int fieldidx = C->get_alias_index(adr_type, true);\n+          \/\/ Pass nullptr for init_out. Having per flat array element field memory edges as uses of the Initialize node\n+          \/\/ can result in per flat array field Phis to be created which confuses the logic of\n+          \/\/ Compile::adjust_flat_array_access_aliases().\n+          hook_memory_on_init(*this, fieldidx, minit_in, nullptr);\n+        }\n+        C->set_flat_accesses_share_alias(true);\n+        hook_memory_on_init(*this, C->get_alias_index(TypeAryPtr::INLINES), minit_in, minit_out);\n+      } else {\n+        const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);\n+        int            elemidx  = C->get_alias_index(telemref);\n+        hook_memory_on_init(*this, elemidx, minit_in, minit_out);\n+      }\n@@ -3672,0 +4126,1 @@\n+      set_memory(minit_out, C->get_alias_index(oop_type)); \/\/ mark word\n@@ -3722,1 +4177,2 @@\n-                             bool deoptimize_on_exception) {\n+                             bool deoptimize_on_exception,\n+                             InlineTypeNode* inline_type_node) {\n@@ -3729,1 +4185,1 @@\n-  int   layout_is_con = (layout_val == nullptr);\n+  bool  layout_is_con = (layout_val == nullptr);\n@@ -3780,1 +4236,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3787,1 +4243,1 @@\n-                                         initial_slow_test);\n+                                         initial_slow_test, inline_type_node);\n@@ -3793,1 +4249,1 @@\n-\/\/ helper for both newarray and anewarray\n+\/\/ helper for newarray and anewarray\n@@ -3806,1 +4262,1 @@\n-  int   layout_is_con = (layout_val == nullptr);\n+  bool  layout_is_con = (layout_val == nullptr);\n@@ -3836,1 +4292,1 @@\n-    fast_size_limit <<= (LogBytesPerLong - log2_esize);\n+    fast_size_limit <<= MAX2(LogBytesPerLong - log2_esize, 0);\n@@ -3853,0 +4309,1 @@\n+    bool is_flat_array = Klass::layout_helper_is_flatArray(layout_con);\n@@ -3855,1 +4312,1 @@\n-    assert((hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n+    assert(is_flat_array || (hsize & right_n_bits(eshift)) == 0, \"hsize is pre-rounded\");\n@@ -3942,1 +4399,1 @@\n-  \/\/ since GC and deoptimization can happened.\n+  \/\/ since GC and deoptimization can happen.\n@@ -3951,1 +4408,29 @@\n-  const TypeOopPtr* ary_type = _gvn.type(klass_node)->is_klassptr()->as_instance_type();\n+  const TypeKlassPtr* ary_klass = _gvn.type(klass_node)->isa_klassptr();\n+  const TypeOopPtr* ary_type = ary_klass->as_instance_type();\n+  const TypeAryPtr* ary_ptr = ary_type->isa_aryptr();\n+\n+  \/\/ Inline type array variants:\n+  \/\/ - null-ok:         ciObjArrayKlass  with is_elem_null_free() = false\n+  \/\/ - null-free:       ciObjArrayKlass  with is_elem_null_free() = true\n+  \/\/ - null-free, flat: ciFlatArrayKlass with is_elem_null_free() = true\n+  \/\/ Check if array is a null-free, non-flat inline type array\n+  \/\/ that needs to be initialized with the default inline type.\n+  Node* default_value = nullptr;\n+  Node* raw_default_value = nullptr;\n+  if (ary_ptr != nullptr && ary_ptr->klass_is_exact()) {\n+    \/\/ Array type is known\n+    if (ary_ptr->is_null_free() && !ary_ptr->is_flat()) {\n+      ciInlineKlass* vk = ary_ptr->elem()->inline_klass();\n+      default_value = InlineTypeNode::default_oop(gvn(), vk);\n+      if (UseCompressedOops) {\n+        \/\/ With compressed oops, the 64-bit init value is built from two 32-bit compressed oops\n+        default_value = _gvn.transform(new EncodePNode(default_value, default_value->bottom_type()->make_narrowoop()));\n+        Node* lower = _gvn.transform(new CastP2XNode(control(), default_value));\n+        Node* upper = _gvn.transform(new LShiftLNode(lower, intcon(32)));\n+        raw_default_value = _gvn.transform(new OrLNode(lower, upper));\n+      } else {\n+        raw_default_value = _gvn.transform(new CastP2XNode(control(), default_value));\n+      }\n+    }\n+  }\n+\n@@ -3966,2 +4451,2 @@\n-                            length, valid_length_test);\n-\n+                            length, valid_length_test,\n+                            default_value, raw_default_value);\n@@ -4114,1 +4599,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4117,2 +4602,2 @@\n-                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS),\n-                                                  ciTypeArrayKlass::make(T_BYTE), true, 0);\n+                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS, false, false, true, true),\n+                                                  ciTypeArrayKlass::make(T_BYTE), true, Type::Offset(0));\n@@ -4131,1 +4616,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4143,1 +4628,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4153,1 +4638,1 @@\n-                                                     false, nullptr, 0);\n+                                                     false, nullptr, Type::Offset(0));\n@@ -4266,1 +4751,7 @@\n-    return makecon(con_type);\n+    Node* con = makecon(con_type);\n+    if (field->type()->is_inlinetype()) {\n+      con = InlineTypeNode::make_from_oop(this, con, field->type()->as_inline_klass(), field->is_null_free());\n+    } else if (con_type->is_inlinetypeptr()) {\n+      con = InlineTypeNode::make_from_oop(this, con, con_type->inline_klass(), field->is_null_free());\n+    }\n+    return con;\n@@ -4271,0 +4762,9 @@\n+\/\/---------------------------load_mirror_from_klass----------------------------\n+\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n+Node* GraphKit::load_mirror_from_klass(Node* klass) {\n+  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n+  Node* load = make_load(nullptr, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n+  \/\/ mirror = ((OopHandle)mirror)->resolve();\n+  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n+}\n+\n@@ -4272,1 +4772,1 @@\n-  const TypeOopPtr* obj_type = obj->bottom_type()->isa_oopptr();\n+  const Type* obj_type = obj->bottom_type();\n@@ -4274,1 +4774,1 @@\n-  if (obj_type != nullptr && sig_type->is_loaded() && !obj_type->higher_equal(sig_type)) {\n+  if (obj_type->isa_oopptr() && sig_type->is_loaded() && !obj_type->higher_equal(sig_type)) {\n@@ -4277,1 +4777,4 @@\n-    return casted_obj;\n+    obj = casted_obj;\n+  }\n+  if (sig_type->is_inlinetypeptr()) {\n+    obj = InlineTypeNode::make_from_oop(this, obj, sig_type->inline_klass(), !gvn().type(obj)->maybe_null());\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":639,"deletions":136,"binary":false,"changes":775,"status":"modified"},{"patch":"@@ -50,1 +50,0 @@\n-  assert(!_gvn.is_IterGVN(), \"IdealKit can't be used during Optimize phase\");\n@@ -82,1 +81,0 @@\n-\n@@ -86,0 +84,4 @@\n+  if_then(bol, prob, cnt, push_new_state);\n+}\n+\n+void IdealKit::if_then(Node* bol, float prob, float cnt, bool push_new_state) {\n@@ -298,1 +300,1 @@\n-    C->record_for_igvn(n);\n+    gvn().record_for_igvn(n);\n@@ -307,1 +309,1 @@\n-  C->record_for_igvn(n);\n+  gvn().record_for_igvn(n);\n@@ -507,2 +509,2 @@\n-  if (slow_call_type->range()->cnt() > TypeFunc::Parms) {\n-    assert(slow_call_type->range()->cnt() == TypeFunc::Parms+1, \"only one return value\");\n+  if (slow_call_type->range_sig()->cnt() > TypeFunc::Parms) {\n+    assert(slow_call_type->range_sig()->cnt() == TypeFunc::Parms+1, \"only one return value\");\n","filename":"src\/hotspot\/share\/opto\/idealKit.cpp","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -1275,0 +1275,17 @@\n+\/\/ Returns true if this IfNode belongs to a flat array check\n+\/\/ and returns the corresponding array in the 'array' parameter.\n+bool IfNode::is_flat_array_check(PhaseTransform* phase, Node** array) {\n+  Node* bol = in(1);\n+  if (!bol->is_Bool()) {\n+    return false;\n+  }\n+  Node* cmp = bol->in(1);\n+  if (cmp->isa_FlatArrayCheck()) {\n+    if (array != nullptr) {\n+      *array = cmp->in(FlatArrayCheckNode::ArrayOrKlass);\n+    }\n+    return true;\n+  }\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/ifnode.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -0,0 +1,1651 @@\n+\/*\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"ci\/ciInlineKlass.hpp\"\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"opto\/addnode.hpp\"\n+#include \"opto\/castnode.hpp\"\n+#include \"opto\/convertnode.hpp\"\n+#include \"opto\/graphKit.hpp\"\n+#include \"opto\/inlinetypenode.hpp\"\n+#include \"opto\/movenode.hpp\"\n+#include \"opto\/narrowptrnode.hpp\"\n+#include \"opto\/rootnode.hpp\"\n+#include \"opto\/phaseX.hpp\"\n+\n+\/\/ Clones the inline type to handle control flow merges involving multiple inline types.\n+\/\/ The inputs are replaced by PhiNodes to represent the merged values for the given region.\n+InlineTypeNode* InlineTypeNode::clone_with_phis(PhaseGVN* gvn, Node* region, SafePointNode* map, bool is_init) {\n+  InlineTypeNode* vt = clone_if_required(gvn, map);\n+  const Type* t = Type::get_const_type(inline_klass());\n+  gvn->set_type(vt, t);\n+  vt->as_InlineType()->set_type(t);\n+\n+  \/\/ Create a PhiNode for merging the oop values\n+  PhiNode* oop = PhiNode::make(region, vt->get_oop(), t);\n+  gvn->set_type(oop, t);\n+  gvn->record_for_igvn(oop);\n+  vt->set_oop(*gvn, oop);\n+\n+  \/\/ Create a PhiNode for merging the is_buffered values\n+  t = Type::get_const_basic_type(T_BOOLEAN);\n+  Node* is_buffered_node = PhiNode::make(region, vt->get_is_buffered(), t);\n+  gvn->set_type(is_buffered_node, t);\n+  gvn->record_for_igvn(is_buffered_node);\n+  vt->set_req(IsBuffered, is_buffered_node);\n+\n+  \/\/ Create a PhiNode for merging the is_init values\n+  Node* is_init_node;\n+  if (is_init) {\n+    is_init_node = gvn->intcon(1);\n+  } else {\n+    t = Type::get_const_basic_type(T_BOOLEAN);\n+    is_init_node = PhiNode::make(region, vt->get_is_init(), t);\n+    gvn->set_type(is_init_node, t);\n+    gvn->record_for_igvn(is_init_node);\n+  }\n+  vt->set_req(IsInit, is_init_node);\n+\n+  \/\/ Create a PhiNode each for merging the field values\n+  for (uint i = 0; i < vt->field_count(); ++i) {\n+    ciType* type = vt->field_type(i);\n+    Node*  value = vt->field_value(i);\n+    \/\/ We limit scalarization for inline types with circular fields and can therefore observe nodes\n+    \/\/ of the same type but with different scalarization depth during GVN. To avoid inconsistencies\n+    \/\/ during merging, make sure that we only create Phis for fields that are guaranteed to be scalarized.\n+    bool no_circularity = !gvn->C->has_circular_inline_type() || field_is_flat(i);\n+    if (type->is_inlinetype() && no_circularity) {\n+      \/\/ Handle inline type fields recursively\n+      value = value->as_InlineType()->clone_with_phis(gvn, region, map);\n+    } else {\n+      t = Type::get_const_type(type);\n+      value = PhiNode::make(region, value, t);\n+      gvn->set_type(value, t);\n+      gvn->record_for_igvn(value);\n+    }\n+    vt->set_field_value(i, value);\n+  }\n+  gvn->record_for_igvn(vt);\n+  return vt;\n+}\n+\n+\/\/ Checks if the inputs of the InlineTypeNode were replaced by PhiNodes\n+\/\/ for the given region (see InlineTypeNode::clone_with_phis).\n+bool InlineTypeNode::has_phi_inputs(Node* region) {\n+  \/\/ Check oop input\n+  bool result = get_oop()->is_Phi() && get_oop()->as_Phi()->region() == region;\n+#ifdef ASSERT\n+  if (result) {\n+    \/\/ Check all field value inputs for consistency\n+    for (uint i = Values; i < field_count(); ++i) {\n+      Node* n = in(i);\n+      if (n->is_InlineType()) {\n+        assert(n->as_InlineType()->has_phi_inputs(region), \"inconsistent phi inputs\");\n+      } else {\n+        assert(n->is_Phi() && n->as_Phi()->region() == region, \"inconsistent phi inputs\");\n+      }\n+    }\n+  }\n+#endif\n+  return result;\n+}\n+\n+\/\/ Merges 'this' with 'other' by updating the input PhiNodes added by 'clone_with_phis'\n+InlineTypeNode* InlineTypeNode::merge_with(PhaseGVN* gvn, const InlineTypeNode* other, int pnum, bool transform) {\n+  assert(inline_klass() == other->inline_klass(), \"Merging incompatible types\");\n+\n+  \/\/ Merge oop inputs\n+  PhiNode* phi = get_oop()->as_Phi();\n+  phi->set_req(pnum, other->get_oop());\n+  if (transform) {\n+    set_oop(*gvn, gvn->transform(phi));\n+  }\n+\n+  \/\/ Merge is_buffered inputs\n+  phi = get_is_buffered()->as_Phi();\n+  phi->set_req(pnum, other->get_is_buffered());\n+  if (transform) {\n+    set_req(IsBuffered, gvn->transform(phi));\n+  }\n+\n+  \/\/ Merge is_init inputs\n+  Node* is_init = get_is_init();\n+  if (is_init->is_Phi()) {\n+    phi = is_init->as_Phi();\n+    phi->set_req(pnum, other->get_is_init());\n+    if (transform) {\n+      set_req(IsInit, gvn->transform(phi));\n+    }\n+  } else {\n+    assert(is_init->find_int_con(0) == 1, \"only with a non null inline type\");\n+  }\n+\n+  \/\/ Merge field values\n+  for (uint i = 0; i < field_count(); ++i) {\n+    Node* val1 =        field_value(i);\n+    Node* val2 = other->field_value(i);\n+    if (val1->is_InlineType()) {\n+      if (val2->is_Phi()) {\n+        val2 = gvn->transform(val2);\n+      }\n+      val1->as_InlineType()->merge_with(gvn, val2->as_InlineType(), pnum, transform);\n+    } else {\n+      assert(val1->is_Phi(), \"must be a phi node\");\n+      val1->set_req(pnum, val2);\n+    }\n+    if (transform) {\n+      set_field_value(i, gvn->transform(val1));\n+    }\n+  }\n+  return this;\n+}\n+\n+\/\/ Adds a new merge path to an inline type node with phi inputs\n+void InlineTypeNode::add_new_path(Node* region) {\n+  assert(has_phi_inputs(region), \"must have phi inputs\");\n+\n+  PhiNode* phi = get_oop()->as_Phi();\n+  phi->add_req(nullptr);\n+  assert(phi->req() == region->req(), \"must be same size as region\");\n+\n+  phi = get_is_buffered()->as_Phi();\n+  phi->add_req(nullptr);\n+  assert(phi->req() == region->req(), \"must be same size as region\");\n+\n+  phi = get_is_init()->as_Phi();\n+  phi->add_req(nullptr);\n+  assert(phi->req() == region->req(), \"must be same size as region\");\n+\n+  for (uint i = 0; i < field_count(); ++i) {\n+    Node* val = field_value(i);\n+    if (val->is_InlineType()) {\n+      val->as_InlineType()->add_new_path(region);\n+    } else {\n+      val->as_Phi()->add_req(nullptr);\n+      assert(val->req() == region->req(), \"must be same size as region\");\n+    }\n+  }\n+}\n+\n+Node* InlineTypeNode::field_value(uint index) const {\n+  assert(index < field_count(), \"index out of bounds\");\n+  return in(Values + index);\n+}\n+\n+\/\/ Get the value of the null marker at the given offset.\n+Node* InlineTypeNode::null_marker_by_offset(int offset, int holder_offset) const {\n+  \/\/ Search through the null markers of all flat fields\n+  for (uint i = 0; i < field_count(); ++i) {\n+    if (field_is_flat(i)) {\n+      InlineTypeNode* value = field_value(i)->as_InlineType();\n+      if (!field_is_null_free(i)) {\n+        int nm_offset = holder_offset + field_null_marker_offset(i);\n+        if (nm_offset == offset) {\n+          return value->get_is_init();\n+        }\n+      }\n+      int flat_holder_offset = holder_offset + field_offset(i) - value->inline_klass()->payload_offset();\n+      Node* nm_value = value->null_marker_by_offset(offset, flat_holder_offset);\n+      if (nm_value != nullptr) {\n+        return nm_value;\n+      }\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+\/\/ Get the value of the field at the given offset.\n+\/\/ If 'recursive' is true, flat inline type fields will be resolved recursively.\n+Node* InlineTypeNode::field_value_by_offset(int offset, bool recursive, bool search_null_marker) const {\n+  \/\/ First check if we are loading a null marker which is not a real field\n+  if (recursive && search_null_marker) {\n+    Node* value = null_marker_by_offset(offset);\n+    if (value != nullptr){\n+      return value;\n+    }\n+  }\n+\n+  \/\/ If the field at 'offset' belongs to a flat inline type field, 'index' refers to the\n+  \/\/ corresponding InlineTypeNode input and 'sub_offset' is the offset in the flattened inline type.\n+  int index = inline_klass()->field_index_by_offset(offset);\n+  int sub_offset = offset - field_offset(index);\n+  Node* value = field_value(index);\n+  assert(value != nullptr, \"field value not found\");\n+  if (recursive && value->is_InlineType()) {\n+    if (field_is_flat(index)) {\n+      \/\/ Flat inline type field\n+      InlineTypeNode* vt = value->as_InlineType();\n+      sub_offset += vt->inline_klass()->payload_offset(); \/\/ Add header size\n+      return vt->field_value_by_offset(sub_offset, recursive, false);\n+    } else {\n+      assert(sub_offset == 0, \"should not have a sub offset\");\n+      return value;\n+    }\n+  }\n+  assert(!(recursive && value->is_InlineType()), \"should not be an inline type\");\n+  assert(sub_offset == 0, \"offset mismatch\");\n+  return value;\n+}\n+\n+void InlineTypeNode::set_field_value(uint index, Node* value) {\n+  assert(index < field_count(), \"index out of bounds\");\n+  set_req(Values + index, value);\n+}\n+\n+void InlineTypeNode::set_field_value_by_offset(int offset, Node* value) {\n+  set_field_value(field_index(offset), value);\n+}\n+\n+int InlineTypeNode::field_offset(uint index) const {\n+  assert(index < field_count(), \"index out of bounds\");\n+  return inline_klass()->declared_nonstatic_field_at(index)->offset_in_bytes();\n+}\n+\n+uint InlineTypeNode::field_index(int offset) const {\n+  uint i = 0;\n+  for (; i < field_count() && field_offset(i) != offset; i++) { }\n+  assert(i < field_count(), \"field not found\");\n+  return i;\n+}\n+\n+ciType* InlineTypeNode::field_type(uint index) const {\n+  assert(index < field_count(), \"index out of bounds\");\n+  return inline_klass()->declared_nonstatic_field_at(index)->type();\n+}\n+\n+bool InlineTypeNode::field_is_flat(uint index) const {\n+  assert(index < field_count(), \"index out of bounds\");\n+  ciField* field = inline_klass()->declared_nonstatic_field_at(index);\n+  assert(!field->is_flat() || field->type()->is_inlinetype(), \"must be an inline type\");\n+  return field->is_flat();\n+}\n+\n+bool InlineTypeNode::field_is_null_free(uint index) const {\n+  assert(index < field_count(), \"index out of bounds\");\n+  ciField* field = inline_klass()->declared_nonstatic_field_at(index);\n+  assert(!field->is_flat() || field->type()->is_inlinetype(), \"must be an inline type\");\n+  return field->is_null_free();\n+}\n+\n+bool InlineTypeNode::field_is_volatile(uint index) const {\n+  assert(index < field_count(), \"index out of bounds\");\n+  ciField* field = inline_klass()->declared_nonstatic_field_at(index);\n+  assert(!field->is_flat() || field->type()->is_inlinetype(), \"must be an inline type\");\n+  return field->is_volatile();\n+}\n+\n+int InlineTypeNode::field_null_marker_offset(uint index) const {\n+  assert(index < field_count(), \"index out of bounds\");\n+  ciField* field = inline_klass()->declared_nonstatic_field_at(index);\n+  assert(field->is_flat(), \"must be an inline type\");\n+  return field->null_marker_offset();\n+}\n+\n+uint InlineTypeNode::add_fields_to_safepoint(Unique_Node_List& worklist, Node_List& null_markers, SafePointNode* sfpt) {\n+  uint cnt = 0;\n+  for (uint i = 0; i < field_count(); ++i) {\n+    Node* value = field_value(i);\n+    if (field_is_flat(i)) {\n+      InlineTypeNode* vt = value->as_InlineType();\n+      cnt += vt->add_fields_to_safepoint(worklist, null_markers, sfpt);\n+      if (!field_is_null_free(i)) {\n+        null_markers.push(vt->get_is_init());\n+        cnt++;\n+      }\n+      continue;\n+    }\n+    if (value->is_InlineType()) {\n+      \/\/ Add inline type to the worklist to process later\n+      worklist.push(value);\n+    }\n+    sfpt->add_req(value);\n+    cnt++;\n+  }\n+  return cnt;\n+}\n+\n+void InlineTypeNode::make_scalar_in_safepoint(PhaseIterGVN* igvn, Unique_Node_List& worklist, SafePointNode* sfpt) {\n+  \/\/ We should not scalarize larvals in debug info of their constructor calls because their fields could still be\n+  \/\/ updated. If we scalarize and update the fields in the constructor, the updates won't be visible in the caller after\n+  \/\/ deoptimization because the scalarized field values are local to the caller. We need to use a buffer to make the\n+  \/\/ updates visible to the outside.\n+  if (is_larval() && sfpt->is_CallJava() && sfpt->as_CallJava()->method() != nullptr &&\n+      sfpt->as_CallJava()->method()->is_object_constructor() && bottom_type()->is_inlinetypeptr() &&\n+      sfpt->in(TypeFunc::Parms) == this) {\n+    \/\/ Receiver is always buffered because it's passed as oop, see special case in CompiledEntrySignature::compute_calling_conventions().\n+    assert(is_allocated(igvn), \"receiver must be allocated\");\n+    return;\n+  }\n+\n+  JVMState* jvms = sfpt->jvms();\n+  assert(jvms != nullptr, \"missing JVMS\");\n+  uint first_ind = (sfpt->req() - jvms->scloff());\n+\n+  \/\/ Iterate over the inline type fields in order of increasing offset and add the\n+  \/\/ field values to the safepoint. Nullable inline types have an IsInit field that\n+  \/\/ needs to be checked before using the field values.\n+  const TypeInt* tinit = igvn->type(get_is_init())->isa_int();\n+  if (tinit != nullptr && !tinit->is_con(1)) {\n+    sfpt->add_req(get_is_init());\n+  } else {\n+    sfpt->add_req(igvn->C->top());\n+  }\n+  Node_List null_markers;\n+  uint nfields = add_fields_to_safepoint(worklist, null_markers, sfpt);\n+  \/\/ Add null markers after the field values\n+  for (uint i = 0; i < null_markers.size(); ++i) {\n+    sfpt->add_req(null_markers.at(i));\n+  }\n+  jvms->set_endoff(sfpt->req());\n+  \/\/ Replace safepoint edge by SafePointScalarObjectNode\n+  SafePointScalarObjectNode* sobj = new SafePointScalarObjectNode(type()->isa_instptr(),\n+                                                                  nullptr,\n+                                                                  first_ind,\n+                                                                  sfpt->jvms()->depth(),\n+                                                                  nfields);\n+  sobj->init_req(0, igvn->C->root());\n+  sobj = igvn->transform(sobj)->as_SafePointScalarObject();\n+  igvn->rehash_node_delayed(sfpt);\n+  for (uint i = jvms->debug_start(); i < jvms->debug_end(); i++) {\n+    Node* debug = sfpt->in(i);\n+    if (debug != nullptr && debug->uncast() == this) {\n+      sfpt->set_req(i, sobj);\n+    }\n+  }\n+}\n+\n+void InlineTypeNode::make_scalar_in_safepoints(PhaseIterGVN* igvn, bool allow_oop) {\n+  \/\/ If the inline type has a constant or loaded oop, use the oop instead of scalarization\n+  \/\/ in the safepoint to avoid keeping field loads live just for the debug info.\n+  Node* oop = get_oop();\n+  bool use_oop = false;\n+  if (allow_oop && is_allocated(igvn) && oop->is_Phi()) {\n+    Unique_Node_List worklist;\n+    VectorSet visited;\n+    visited.set(oop->_idx);\n+    worklist.push(oop);\n+    use_oop = true;\n+    while (worklist.size() > 0 && use_oop) {\n+      Node* n = worklist.pop();\n+      for (uint i = 1; i < n->req(); i++) {\n+        Node* in = n->in(i);\n+        if (in->is_Phi() && !visited.test_set(in->_idx)) {\n+          worklist.push(in);\n+        } else if (!(in->is_Con() || in->is_Parm())) {\n+          use_oop = false;\n+          break;\n+        }\n+      }\n+    }\n+  } else {\n+    use_oop = allow_oop && is_allocated(igvn) &&\n+              (oop->is_Con() || oop->is_Parm() || oop->is_Load() || (oop->isa_DecodeN() && oop->in(1)->is_Load()));\n+  }\n+\n+  ResourceMark rm;\n+  Unique_Node_List safepoints;\n+  Unique_Node_List vt_worklist;\n+  Unique_Node_List worklist;\n+  worklist.push(this);\n+  while (worklist.size() > 0) {\n+    Node* n = worklist.pop();\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* use = n->fast_out(i);\n+      if (use->is_SafePoint() && !use->is_CallLeaf() && (!use->is_Call() || use->as_Call()->has_debug_use(n))) {\n+        safepoints.push(use);\n+      } else if (use->is_ConstraintCast()) {\n+        worklist.push(use);\n+      }\n+    }\n+  }\n+\n+  \/\/ Process all safepoint uses and scalarize inline type\n+  while (safepoints.size() > 0) {\n+    SafePointNode* sfpt = safepoints.pop()->as_SafePoint();\n+    if (use_oop) {\n+      for (uint i = sfpt->jvms()->debug_start(); i < sfpt->jvms()->debug_end(); i++) {\n+        Node* debug = sfpt->in(i);\n+        if (debug != nullptr && debug->uncast() == this) {\n+          sfpt->set_req(i, get_oop());\n+        }\n+      }\n+      igvn->rehash_node_delayed(sfpt);\n+    } else {\n+      make_scalar_in_safepoint(igvn, vt_worklist, sfpt);\n+    }\n+  }\n+  \/\/ Now scalarize non-flat fields\n+  for (uint i = 0; i < vt_worklist.size(); ++i) {\n+    InlineTypeNode* vt = vt_worklist.at(i)->isa_InlineType();\n+    vt->make_scalar_in_safepoints(igvn);\n+  }\n+  if (outcnt() == 0) {\n+    igvn->record_for_igvn(this);\n+  }\n+}\n+\n+const TypePtr* InlineTypeNode::field_adr_type(Node* base, int offset, ciInstanceKlass* holder, DecoratorSet decorators, PhaseGVN& gvn) const {\n+  const TypeAryPtr* ary_type = gvn.type(base)->isa_aryptr();\n+  const TypePtr* adr_type = nullptr;\n+  bool is_array = ary_type != nullptr;\n+  if ((decorators & C2_MISMATCHED) != 0) {\n+    adr_type = TypeRawPtr::BOTTOM;\n+  } else if (is_array) {\n+    \/\/ In the case of a flat inline type array, each field has its own slice\n+    adr_type = ary_type->with_field_offset(offset)->add_offset(Type::OffsetBot);\n+  } else {\n+    ciField* field = holder->get_field_by_offset(offset, false);\n+    assert(field != nullptr, \"field not found\");\n+    adr_type = gvn.C->alias_type(field)->adr_type();\n+  }\n+  return adr_type;\n+}\n+\n+\/\/ We limit scalarization for inline types with circular fields and can therefore observe nodes\n+\/\/ of the same type but with different scalarization depth during GVN. This method adjusts the\n+\/\/ scalarization depth to avoid inconsistencies during merging.\n+InlineTypeNode* InlineTypeNode::adjust_scalarization_depth(GraphKit* kit) {\n+  if (!kit->C->has_circular_inline_type()) {\n+    return this;\n+  }\n+  GrowableArray<ciType*> visited;\n+  visited.push(inline_klass());\n+  return adjust_scalarization_depth_impl(kit, visited);\n+}\n+\n+InlineTypeNode* InlineTypeNode::adjust_scalarization_depth_impl(GraphKit* kit, GrowableArray<ciType*>& visited) {\n+  InlineTypeNode* val = this;\n+  for (uint i = 0; i < field_count(); ++i) {\n+    Node* value = field_value(i);\n+    Node* new_value = value;\n+    ciType* ft = field_type(i);\n+    if (value->is_InlineType()) {\n+      if (!field_is_flat(i) && visited.contains(ft)) {\n+        new_value = value->as_InlineType()->buffer(kit)->get_oop();\n+      } else {\n+        int old_len = visited.length();\n+        visited.push(ft);\n+        new_value = value->as_InlineType()->adjust_scalarization_depth_impl(kit, visited);\n+        visited.trunc_to(old_len);\n+      }\n+    } else if (ft->is_inlinetype() && !visited.contains(ft)) {\n+      int old_len = visited.length();\n+      visited.push(ft);\n+      new_value = make_from_oop_impl(kit, value, ft->as_inline_klass(), field_is_null_free(i), visited);\n+      visited.trunc_to(old_len);\n+    }\n+    if (value != new_value) {\n+      if (val == this) {\n+        val = clone_if_required(&kit->gvn(), kit->map());\n+      }\n+      val->set_field_value(i, new_value);\n+    }\n+  }\n+  return (val == this) ? this : kit->gvn().transform(val)->as_InlineType();\n+}\n+\n+void InlineTypeNode::load(GraphKit* kit, Node* base, Node* ptr, ciInstanceKlass* holder, GrowableArray<ciType*>& visited, int holder_offset, DecoratorSet decorators) {\n+  \/\/ Initialize the inline type by loading its field values from\n+  \/\/ memory and adding the values as input edges to the node.\n+  for (uint i = 0; i < field_count(); ++i) {\n+    int offset = holder_offset + field_offset(i);\n+    Node* value = nullptr;\n+    ciType* ft = field_type(i);\n+    bool null_free = field_is_null_free(i);\n+    if (null_free && ft->as_inline_klass()->is_empty()) {\n+      \/\/ Loading from a field of an empty inline type. Just return the default instance.\n+      value = make_default_impl(kit->gvn(), ft->as_inline_klass(), visited);\n+    } else if (field_is_flat(i)) {\n+      \/\/ Recursively load the flat inline type field\n+      bool needs_atomic_access = !null_free || field_is_volatile(i);\n+      assert(!needs_atomic_access, \"Atomic access in non-atomic container\");\n+      int nm_offset = field_is_null_free(i) ? -1 : (holder_offset + field_null_marker_offset(i));\n+      value = make_from_flat_impl(kit, ft->as_inline_klass(), base, ptr, holder, offset, false, nm_offset, decorators, visited);\n+    } else {\n+      const TypeOopPtr* oop_ptr = kit->gvn().type(base)->isa_oopptr();\n+      bool is_array = (oop_ptr->isa_aryptr() != nullptr);\n+      bool mismatched = (decorators & C2_MISMATCHED) != 0;\n+      if (base->is_Con() && !is_array && !mismatched) {\n+        \/\/ If the oop to the inline type is constant (static final field), we can\n+        \/\/ also treat the fields as constants because the inline type is immutable.\n+        ciObject* constant_oop = oop_ptr->const_oop();\n+        ciField* field = holder->get_field_by_offset(offset, false);\n+        assert(field != nullptr, \"field not found\");\n+        ciConstant constant = constant_oop->as_instance()->field_value(field);\n+        const Type* con_type = Type::make_from_constant(constant, \/*require_const=*\/ true);\n+        assert(con_type != nullptr, \"type not found\");\n+        value = kit->gvn().transform(kit->makecon(con_type));\n+        \/\/ Check type of constant which might be more precise than the static field type\n+        if (con_type->is_inlinetypeptr() && !con_type->is_zero_type()) {\n+          ft = con_type->inline_klass();\n+          null_free = true;\n+        }\n+      } else {\n+        \/\/ Load field value from memory\n+        const TypePtr* adr_type = field_adr_type(base, offset, holder, decorators, kit->gvn());\n+        Node* adr = kit->basic_plus_adr(base, ptr, offset);\n+        BasicType bt = type2field[ft->basic_type()];\n+        assert(is_java_primitive(bt) || adr->bottom_type()->is_ptr_to_narrowoop() == UseCompressedOops, \"inconsistent\");\n+        const Type* val_type = Type::get_const_type(ft);\n+        value = kit->access_load_at(base, adr, adr_type, val_type, bt, is_array ? (decorators | IS_ARRAY) : decorators);\n+      }\n+      \/\/ Loading a non-flattened inline type from memory\n+      if (visited.contains(ft)) {\n+        kit->C->set_has_circular_inline_type(true);\n+      } else if (ft->is_inlinetype()) {\n+        int old_len = visited.length();\n+        visited.push(ft);\n+        value = make_from_oop_impl(kit, value, ft->as_inline_klass(), null_free, visited);\n+        visited.trunc_to(old_len);\n+      }\n+    }\n+    set_field_value(i, value);\n+  }\n+}\n+\n+\/\/ Get a field value from the payload by shifting it according to the offset\n+static Node* get_payload_value(PhaseGVN* gvn, Node* payload, BasicType bt, BasicType val_bt, int offset) {\n+  \/\/ Shift to the right position in the long value\n+  assert((offset + type2aelembytes(val_bt)) <= type2aelembytes(bt), \"Value does not fit into payload\");\n+  Node* value = nullptr;\n+  Node* shift_val = gvn->intcon(offset << LogBitsPerByte);\n+  if (bt == T_LONG) {\n+    value = gvn->transform(new URShiftLNode(payload, shift_val));\n+    value = gvn->transform(new ConvL2INode(value));\n+  } else {\n+    value = gvn->transform(new URShiftINode(payload, shift_val));\n+  }\n+\n+  if (val_bt == T_INT || val_bt == T_OBJECT) {\n+    return value;\n+  } else {\n+    \/\/ Make sure to zero unused bits in the 32-bit value\n+    return Compile::narrow_value(val_bt, value, nullptr, gvn, true);\n+  }\n+}\n+\n+\/\/ Convert a payload value to field values\n+void InlineTypeNode::convert_from_payload(GraphKit* kit, BasicType bt, Node* payload, int holder_offset, bool null_free, int null_marker_offset) {\n+  PhaseGVN* gvn = &kit->gvn();\n+  Node* value = nullptr;\n+  if (!null_free) {\n+    \/\/ Get the null marker\n+    value = get_payload_value(gvn, payload, bt, T_BOOLEAN, null_marker_offset);\n+    set_req(IsInit, value);\n+  }\n+  \/\/ Iterate over the fields and get their values from the payload\n+  for (uint i = 0; i < field_count(); ++i) {\n+    ciType* ft = field_type(i);\n+    int offset = holder_offset + field_offset(i) - inline_klass()->payload_offset();\n+    if (field_is_flat(i)) {\n+      null_marker_offset = holder_offset + field_null_marker_offset(i) - inline_klass()->payload_offset();\n+      InlineTypeNode* vt = make_uninitialized(*gvn, ft->as_inline_klass(), field_is_null_free(i));\n+      vt->convert_from_payload(kit, bt, payload, offset, field_is_null_free(i), null_marker_offset);\n+      value = gvn->transform(vt);\n+    } else {\n+      value = get_payload_value(gvn, payload, bt, ft->basic_type(), offset);\n+      if (!ft->is_primitive_type()) {\n+        \/\/ Narrow oop field\n+        assert(UseCompressedOops && bt == T_LONG, \"Naturally atomic\");\n+        const Type* val_type = Type::get_const_type(ft)->make_narrowoop();\n+        value = gvn->transform(new CastI2NNode(kit->control(), value));\n+        value = gvn->transform(new DecodeNNode(value, val_type));\n+        \/\/ TODO 8341767 Should we add the membar to the CastI2N and give it a type?\n+        value = gvn->transform(new CastPPNode(kit->control(), value, Type::get_const_type(ft), ConstraintCastNode::UnconditionalDependency));\n+        \/\/ Prevent the CastI2N from floating below a safepoint\n+        kit->insert_mem_bar(Op_MemBarVolatile, value);\n+\n+        if (ft->is_inlinetype()) {\n+          GrowableArray<ciType*> visited;\n+          value = make_from_oop_impl(kit, value, ft->as_inline_klass(), field_is_null_free(i), visited);\n+        }\n+      }\n+    }\n+    set_field_value(i, value);\n+  }\n+}\n+\n+\/\/ Set a field value in the payload by shifting it according to the offset\n+static Node* set_payload_value(PhaseGVN* gvn, Node* payload, BasicType bt, Node* value, BasicType val_bt, int offset) {\n+  assert((offset + type2aelembytes(val_bt)) <= type2aelembytes(bt), \"Value does not fit into payload\");\n+\n+  \/\/ Make sure to zero unused bits in the 32-bit value\n+  if (val_bt == T_BYTE || val_bt == T_BOOLEAN) {\n+    value = gvn->transform(new AndINode(value, gvn->intcon(0xFF)));\n+  } else if (val_bt == T_CHAR || val_bt == T_SHORT) {\n+    value = gvn->transform(new AndINode(value, gvn->intcon(0xFFFF)));\n+  } else if (val_bt == T_FLOAT) {\n+    value = gvn->transform(new MoveF2INode(value));\n+  } else {\n+    assert(val_bt == T_INT, \"Unsupported type: %s\", type2name(val_bt));\n+  }\n+\n+  Node* shift_val = gvn->intcon(offset << LogBitsPerByte);\n+  if (bt == T_LONG) {\n+    \/\/ Convert to long and remove the sign bit (the backend will fold this and emit a zero extend i2l)\n+    value = gvn->transform(new ConvI2LNode(value));\n+    value = gvn->transform(new AndLNode(value, gvn->longcon(0xFFFFFFFF)));\n+\n+    Node* shift_value = gvn->transform(new LShiftLNode(value, shift_val));\n+    payload = new OrLNode(shift_value, payload);\n+  } else {\n+    Node* shift_value = gvn->transform(new LShiftINode(value, shift_val));\n+    payload = new OrINode(shift_value, payload);\n+  }\n+  return gvn->transform(payload);\n+}\n+\n+\/\/ Convert the field values to a payload value of type 'bt'\n+Node* InlineTypeNode::convert_to_payload(GraphKit* kit, BasicType bt, Node* payload, int holder_offset, bool null_free, int null_marker_offset, int& oop_off_1, int& oop_off_2) const {\n+  PhaseGVN* gvn = &kit->gvn();\n+  Node* value = nullptr;\n+  if (!null_free) {\n+    \/\/ Set the null marker\n+    value = get_is_init();\n+    payload = set_payload_value(gvn, payload, bt, value, T_BYTE, null_marker_offset);\n+  }\n+  \/\/ Iterate over the fields and add their values to the payload\n+  for (uint i = 0; i < field_count(); ++i) {\n+    value = field_value(i);\n+    int inner_offset = field_offset(i) - inline_klass()->payload_offset();\n+    int offset = holder_offset + inner_offset;\n+    if (field_is_flat(i)) {\n+      null_marker_offset = holder_offset + field_null_marker_offset(i) - inline_klass()->payload_offset();\n+      payload = value->as_InlineType()->convert_to_payload(kit, bt, payload, offset, field_is_null_free(i), null_marker_offset, oop_off_1, oop_off_2);\n+    } else {\n+      ciType* ft = field_type(i);\n+      BasicType field_bt = ft->basic_type();\n+      if (!ft->is_primitive_type()) {\n+        \/\/ Narrow oop field\n+        assert(UseCompressedOops && bt == T_LONG, \"Naturally atomic\");\n+        if (oop_off_1 == -1) {\n+          oop_off_1 = inner_offset;\n+        } else {\n+          assert(oop_off_2 == -1, \"already set\");\n+          oop_off_2 = inner_offset;\n+        }\n+        const Type* val_type = Type::get_const_type(ft)->make_narrowoop();\n+        value = gvn->transform(new EncodePNode(value, val_type));\n+        value = gvn->transform(new CastP2XNode(kit->control(), value));\n+        value = gvn->transform(new ConvL2INode(value));\n+        field_bt = T_INT;\n+      }\n+      payload = set_payload_value(gvn, payload, bt, value, field_bt, offset);\n+    }\n+  }\n+  return payload;\n+}\n+\n+void InlineTypeNode::store_flat(GraphKit* kit, Node* base, Node* ptr, ciInstanceKlass* holder, int holder_offset, bool atomic, int null_marker_offset, DecoratorSet decorators) const {\n+  if (kit->gvn().type(base)->isa_aryptr()) {\n+    kit->C->set_flat_accesses();\n+  }\n+  bool null_free = (null_marker_offset == -1);\n+\n+  if (atomic) {\n+#ifdef ASSERT\n+    bool is_naturally_atomic = inline_klass()->is_empty() || (null_free && inline_klass()->nof_declared_nonstatic_fields() == 1);\n+    assert(!is_naturally_atomic, \"No atomic access required\");\n+#endif\n+    \/\/ Convert to a payload value <= 64-bit and write atomically.\n+    \/\/ The payload might contain at most two oop fields that must be narrow because otherwise they would be 64-bit\n+    \/\/ in size and would then be written by a \"normal\" oop store. If the payload contains oops, its size is always\n+    \/\/ 64-bit because the next smaller size would be 32-bit which could only hold one narrow oop that would then be\n+    \/\/ written by a normal narrow oop store. These properties are asserted in 'convert_to_payload'.\n+    BasicType bt = inline_klass()->payload_size_to_basic_type();\n+    Node* payload = (bt == T_LONG) ? kit->longcon(0) : kit->intcon(0);\n+    int oop_off_1 = -1;\n+    int oop_off_2 = -1;\n+    payload = convert_to_payload(kit, bt, payload, 0, null_free, null_marker_offset - holder_offset, oop_off_1, oop_off_2);\n+    Node* adr = kit->basic_plus_adr(base, ptr, holder_offset);\n+    if (!UseG1GC || oop_off_1 == -1) {\n+      \/\/ No oop fields or no late barrier expansion. Emit an atomic store of the payload and add GC barriers if needed.\n+      assert(oop_off_2 == -1 || !UseG1GC, \"sanity\");\n+      \/\/ ZGC does not support compressed oops, so only one oop can be in the payload which is written by a \"normal\" oop store.\n+      assert((oop_off_1 == -1 && oop_off_2 == -1) || !UseZGC, \"ZGC does not support embedded oops in flat fields\");\n+      const Type* val_type = Type::get_const_basic_type(bt);\n+      bool is_array = (kit->gvn().type(base)->isa_aryptr() != nullptr);\n+      decorators |= C2_MISMATCHED;\n+      kit->access_store_at(base, adr, TypeRawPtr::BOTTOM, payload, val_type, bt, is_array ? (decorators | IS_ARRAY) : decorators, true, this);\n+    } else {\n+      \/\/ Contains oops and requires late barrier expansion. Emit a special store node that allows to emit GC barriers in the backend.\n+      assert(UseG1GC, \"Unexpected GC\");\n+      assert(bt == T_LONG, \"Unexpected payload type\");\n+      const TypePtr* adr_type = TypeRawPtr::BOTTOM;\n+      Node* mem = kit->memory(adr_type);\n+      \/\/ If one oop, set the offset (if no offset is set, two oops are assumed by the backend)\n+      Node* oop_offset = (oop_off_2 == -1) ? kit->intcon(oop_off_1) : nullptr;\n+      Node* st = kit->gvn().transform(new StoreLSpecialNode(kit->control(), mem, adr, adr_type, payload, oop_offset, MemNode::unordered));\n+      kit->set_memory(st, adr_type);\n+    }\n+    \/\/ Prevent loads from floating above this mismatched store\n+    kit->insert_mem_bar(Op_MemBarCPUOrder);\n+    return;\n+  }\n+\n+  if (!null_free) {\n+    \/\/ Nullable, store the null marker\n+    Node* adr = kit->basic_plus_adr(base, ptr, null_marker_offset);\n+    kit->store_to_memory(kit->control(), adr, get_is_init(), T_BOOLEAN, MemNode::unordered);\n+  }\n+\n+  \/\/ The inline type is embedded into the object without an oop header. Subtract the\n+  \/\/ offset of the first field to account for the missing header when storing the values.\n+  if (holder == nullptr) {\n+    holder = inline_klass();\n+  }\n+  holder_offset -= inline_klass()->payload_offset();\n+  store(kit, base, ptr, holder, holder_offset, -1, decorators);\n+}\n+\n+void InlineTypeNode::store(GraphKit* kit, Node* base, Node* ptr, ciInstanceKlass* holder, int holder_offset, int offsetOnly, DecoratorSet decorators) const {\n+  \/\/ Write field values to memory\n+  for (uint i = 0; i < field_count(); ++i) {\n+    if (offsetOnly != -1 && offsetOnly != field_offset(i)) continue;\n+    int offset = holder_offset + field_offset(i);\n+    Node* value = field_value(i);\n+    ciType* ft = field_type(i);\n+    if (field_is_flat(i)) {\n+      \/\/ Recursively store the flat inline type field\n+      bool needs_atomic_access = !field_is_null_free(i) || field_is_volatile(i);\n+      assert(!needs_atomic_access, \"Atomic access in non-atomic container\");\n+      int nm_offset = field_is_null_free(i) ? -1 : (holder_offset + field_null_marker_offset(i));\n+      value->as_InlineType()->store_flat(kit, base, ptr, holder, offset, false, nm_offset, decorators);\n+    } else {\n+      \/\/ Store field value to memory\n+      const TypePtr* adr_type = field_adr_type(base, offset, holder, decorators, kit->gvn());\n+      Node* adr = kit->basic_plus_adr(base, ptr, offset);\n+      BasicType bt = type2field[ft->basic_type()];\n+      assert(is_java_primitive(bt) || adr->bottom_type()->is_ptr_to_narrowoop() == UseCompressedOops, \"inconsistent\");\n+      const Type* val_type = Type::get_const_type(ft);\n+      bool is_array = (kit->gvn().type(base)->isa_aryptr() != nullptr);\n+      kit->access_store_at(base, adr, adr_type, value, val_type, bt, is_array ? (decorators | IS_ARRAY) : decorators);\n+    }\n+  }\n+}\n+\n+InlineTypeNode* InlineTypeNode::buffer(GraphKit* kit, bool safe_for_replace, bool must_init) {\n+  if (kit->gvn().find_int_con(get_is_buffered(), 0) == 1) {\n+    \/\/ Already buffered\n+    return this;\n+  }\n+\n+  \/\/ Check if inline type is already buffered\n+  Node* not_buffered_ctl = kit->top();\n+  Node* not_null_oop = kit->null_check_oop(get_oop(), &not_buffered_ctl, \/* never_see_null = *\/ false, safe_for_replace);\n+  if (not_buffered_ctl->is_top()) {\n+    \/\/ Already buffered\n+    InlineTypeNode* vt = clone_if_required(&kit->gvn(), kit->map(), safe_for_replace);\n+    vt->set_is_buffered(kit->gvn());\n+    vt = kit->gvn().transform(vt)->as_InlineType();\n+    if (safe_for_replace) {\n+      kit->replace_in_map(this, vt);\n+    }\n+    return vt;\n+  }\n+  Node* buffered_ctl = kit->control();\n+  kit->set_control(not_buffered_ctl);\n+\n+  \/\/ Inline type is not buffered, check if it is null.\n+  Node* null_ctl = kit->top();\n+  kit->null_check_common(get_is_init(), T_INT, false, &null_ctl);\n+  bool null_free = null_ctl->is_top();\n+\n+  RegionNode* region = new RegionNode(4);\n+  PhiNode* oop = PhiNode::make(region, not_null_oop, type()->join_speculative(null_free ? TypePtr::NOTNULL : TypePtr::BOTTOM));\n+\n+  \/\/ InlineType is already buffered\n+  region->init_req(1, buffered_ctl);\n+  oop->init_req(1, not_null_oop);\n+\n+  \/\/ InlineType is null\n+  region->init_req(2, null_ctl);\n+  oop->init_req(2, kit->gvn().zerocon(T_OBJECT));\n+\n+  PhiNode* io  = PhiNode::make(region, kit->i_o(), Type::ABIO);\n+  PhiNode* mem = PhiNode::make(region, kit->merged_memory(), Type::MEMORY, TypePtr::BOTTOM);\n+\n+  if (!kit->stopped()) {\n+    assert(!is_allocated(&kit->gvn()), \"already buffered\");\n+    PreserveJVMState pjvms(kit);\n+    ciInlineKlass* vk = inline_klass();\n+    if (vk->is_initialized() && (vk->is_empty() || (is_default(&kit->gvn()) && !is_larval(&kit->gvn()) && !is_larval()))) {\n+      \/\/ Don't buffer an empty or default inline type, use the default oop instead\n+      oop->init_req(3, default_oop(kit->gvn(), vk));\n+    } else {\n+      \/\/ Allocate and initialize buffer, re-execute on deoptimization.\n+      kit->jvms()->set_bci(kit->bci());\n+      kit->jvms()->set_should_reexecute(true);\n+      kit->kill_dead_locals();\n+      Node* klass_node = kit->makecon(TypeKlassPtr::make(vk));\n+      Node* alloc_oop  = kit->new_instance(klass_node, nullptr, nullptr, \/* deoptimize_on_exception *\/ true, this);\n+\n+      if (must_init) {\n+        \/\/ Either not a larval or a larval receiver on which we are about to invoke an abstract value class constructor\n+        \/\/ or the Object constructor which is not inlined. It is therefore escaping, and we must initialize the buffer\n+        \/\/ because we have not done this, yet, for larvals (see else case).\n+        store(kit, alloc_oop, alloc_oop, vk);\n+\n+        \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+        \/\/ store that would make this buffer accessible by other threads.\n+        AllocateNode* alloc = AllocateNode::Ideal_allocation(alloc_oop);\n+        assert(alloc != nullptr, \"must have an allocation node\");\n+        kit->insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out_or_null(AllocateNode::RawAddress));\n+      } else {\n+        \/\/ We do not need to initialize the buffer because a larval could still be updated which will create a new buffer.\n+        \/\/ Once the larval escapes, we will initialize the buffer (must_init set).\n+        assert(is_larval(), \"only larvals can possibly skip the initialization of their buffer\");\n+      }\n+      oop->init_req(3, alloc_oop);\n+    }\n+    region->init_req(3, kit->control());\n+    io    ->init_req(3, kit->i_o());\n+    mem   ->init_req(3, kit->merged_memory());\n+  }\n+\n+  \/\/ Update GraphKit\n+  kit->set_control(kit->gvn().transform(region));\n+  kit->set_i_o(kit->gvn().transform(io));\n+  kit->set_all_memory(kit->gvn().transform(mem));\n+  kit->record_for_igvn(region);\n+  kit->record_for_igvn(oop);\n+  kit->record_for_igvn(io);\n+  kit->record_for_igvn(mem);\n+\n+  \/\/ Use cloned InlineTypeNode to propagate oop from now on\n+  Node* res_oop = kit->gvn().transform(oop);\n+  InlineTypeNode* vt = clone_if_required(&kit->gvn(), kit->map(), safe_for_replace);\n+  vt->set_oop(kit->gvn(), res_oop);\n+  vt->set_is_buffered(kit->gvn());\n+  vt = kit->gvn().transform(vt)->as_InlineType();\n+  if (safe_for_replace) {\n+    kit->replace_in_map(this, vt);\n+  }\n+  \/\/ InlineTypeNode::remove_redundant_allocations piggybacks on split if.\n+  \/\/ Make sure it gets a chance to remove this allocation.\n+  kit->C->set_has_split_ifs(true);\n+  return vt;\n+}\n+\n+bool InlineTypeNode::is_allocated(PhaseGVN* phase) const {\n+  if (phase->find_int_con(get_is_buffered(), 0) == 1) {\n+    return true;\n+  }\n+  Node* oop = get_oop();\n+  const Type* oop_type = (phase != nullptr) ? phase->type(oop) : oop->bottom_type();\n+  return !oop_type->maybe_null();\n+}\n+\n+static void replace_proj(Compile* C, CallNode* call, uint& proj_idx, Node* value, BasicType bt) {\n+  ProjNode* pn = call->proj_out_or_null(proj_idx);\n+  if (pn != nullptr) {\n+    C->gvn_replace_by(pn, value);\n+    C->initial_gvn()->hash_delete(pn);\n+    pn->set_req(0, C->top());\n+  }\n+  proj_idx += type2size[bt];\n+}\n+\n+\/\/ When a call returns multiple values, it has several result\n+\/\/ projections, one per field. Replacing the result of the call by an\n+\/\/ inline type node (after late inlining) requires that for each result\n+\/\/ projection, we find the corresponding inline type field.\n+void InlineTypeNode::replace_call_results(GraphKit* kit, CallNode* call, Compile* C) {\n+  uint proj_idx = TypeFunc::Parms;\n+  \/\/ Replace oop projection\n+  replace_proj(C, call, proj_idx, get_oop(), T_OBJECT);\n+  \/\/ Replace field projections\n+  replace_field_projs(C, call, proj_idx);\n+  \/\/ Replace is_init projection\n+  replace_proj(C, call, proj_idx, get_is_init(), T_BOOLEAN);\n+  assert(proj_idx == call->tf()->range_cc()->cnt(), \"missed a projection\");\n+}\n+\n+void InlineTypeNode::replace_field_projs(Compile* C, CallNode* call, uint& proj_idx) {\n+  for (uint i = 0; i < field_count(); ++i) {\n+    Node* value = field_value(i);\n+    if (field_is_flat(i)) {\n+      InlineTypeNode* vt = value->as_InlineType();\n+      \/\/ Replace field projections for flat field\n+      vt->replace_field_projs(C, call, proj_idx);\n+      if (!field_is_null_free(i)) {\n+        \/\/ Replace is_init projection for nullable field\n+        replace_proj(C, call, proj_idx, vt->get_is_init(), T_BOOLEAN);\n+      }\n+      continue;\n+    }\n+    \/\/ Replace projection for field value\n+    replace_proj(C, call, proj_idx, value, field_type(i)->basic_type());\n+  }\n+}\n+\n+Node* InlineTypeNode::allocate_fields(GraphKit* kit) {\n+  InlineTypeNode* vt = clone_if_required(&kit->gvn(), kit->map());\n+  for (uint i = 0; i < field_count(); i++) {\n+     Node* value = field_value(i);\n+     if (field_is_flat(i)) {\n+       \/\/ Flat inline type field\n+       vt->set_field_value(i, value->as_InlineType()->allocate_fields(kit));\n+     } else if (value->is_InlineType()) {\n+       \/\/ Non-flat inline type field\n+       vt->set_field_value(i, value->as_InlineType()->buffer(kit));\n+     }\n+  }\n+  vt = kit->gvn().transform(vt)->as_InlineType();\n+  kit->replace_in_map(this, vt);\n+  return vt;\n+}\n+\n+\/\/ Replace a buffer allocation by a dominating allocation\n+static void replace_allocation(PhaseIterGVN* igvn, Node* res, Node* dom) {\n+  \/\/ Remove initializing stores and GC barriers\n+  for (DUIterator_Fast imax, i = res->fast_outs(imax); i < imax; i++) {\n+    Node* use = res->fast_out(i);\n+    if (use->is_AddP()) {\n+      for (DUIterator_Fast jmax, j = use->fast_outs(jmax); j < jmax; j++) {\n+        Node* store = use->fast_out(j)->isa_Store();\n+        if (store != nullptr) {\n+          igvn->rehash_node_delayed(store);\n+          igvn->replace_in_uses(store, store->in(MemNode::Memory));\n+        }\n+      }\n+    } else if (use->Opcode() == Op_CastP2X) {\n+      if (UseG1GC && use->find_out_with(Op_XorX)->in(1) != use) {\n+        \/\/ The G1 pre-barrier uses a CastP2X both for the pointer of the object\n+        \/\/ we store into, as well as the value we are storing. Skip if this is a\n+        \/\/ barrier for storing 'res' into another object.\n+        continue;\n+      }\n+      BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+      bs->eliminate_gc_barrier(igvn, use);\n+      --i; --imax;\n+    }\n+  }\n+  igvn->replace_node(res, dom);\n+}\n+\n+Node* InlineTypeNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  Node* oop = get_oop();\n+  const TypeInt* tinit = phase->type(get_is_init())->isa_int();\n+  if ((tinit != nullptr && tinit->is_con(1)) &&\n+      ((is_default(phase) && !is_larval(phase) && !is_larval()) || inline_klass()->is_empty()) &&\n+      inline_klass()->is_initialized() &&\n+      (!oop->is_Con() || phase->type(oop)->is_zero_type())) {\n+    \/\/ Use the pre-allocated oop for null-free default or empty inline types\n+    set_oop(*phase, default_oop(*phase, inline_klass()));\n+    assert(is_allocated(phase), \"should now be allocated\");\n+    return this;\n+  }\n+  if (oop->isa_InlineType() && !phase->type(oop)->maybe_null()) {\n+    InlineTypeNode* vtptr = oop->as_InlineType();\n+    set_oop(*phase, vtptr->get_oop());\n+    set_is_buffered(*phase);\n+    set_is_init(*phase);\n+    for (uint i = Values; i < vtptr->req(); ++i) {\n+      set_req(i, vtptr->in(i));\n+    }\n+    return this;\n+  }\n+\n+  \/\/ Use base oop if fields are loaded from memory\n+  Node* base = is_loaded(phase);\n+  if (base != nullptr && get_oop() != base && !phase->type(base)->maybe_null()) {\n+    set_oop(*phase, base);\n+    assert(is_allocated(phase), \"should now be allocated\");\n+    return this;\n+  }\n+\n+  if (can_reshape) {\n+    PhaseIterGVN* igvn = phase->is_IterGVN();\n+    if (is_allocated(phase)) {\n+      \/\/ Search for and remove re-allocations of this inline type. Ignore scalar replaceable ones,\n+      \/\/ they will be removed anyway and changing the memory chain will confuse other optimizations.\n+      \/\/ This can happen with late inlining when we first allocate an inline type argument\n+      \/\/ but later decide to inline the call after the callee code also triggered allocation.\n+      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+        AllocateNode* alloc = fast_out(i)->isa_Allocate();\n+        if (alloc != nullptr && alloc->in(AllocateNode::InlineType) == this && !alloc->_is_scalar_replaceable) {\n+          \/\/ Found a re-allocation\n+          Node* res = alloc->result_cast();\n+          if (res != nullptr && res->is_CheckCastPP()) {\n+            \/\/ Replace allocation by oop and unlink AllocateNode\n+            replace_allocation(igvn, res, oop);\n+            igvn->replace_input_of(alloc, AllocateNode::InlineType, igvn->C->top());\n+            --i; --imax;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  return nullptr;\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_uninitialized(PhaseGVN& gvn, ciInlineKlass* vk, bool null_free) {\n+  \/\/ Create a new InlineTypeNode with uninitialized values and nullptr oop\n+  bool use_default_oop = vk->is_empty() && vk->is_initialized() && null_free;\n+  Node* oop = use_default_oop ? default_oop(gvn, vk) : gvn.zerocon(T_OBJECT);\n+  InlineTypeNode* vt = new InlineTypeNode(vk, oop, null_free);\n+  vt->set_is_buffered(gvn, use_default_oop);\n+  vt->set_is_init(gvn);\n+  return vt;\n+}\n+\n+Node* InlineTypeNode::default_oop(PhaseGVN& gvn, ciInlineKlass* vk) {\n+  \/\/ Returns the constant oop of the default inline type allocation\n+  return gvn.makecon(TypeInstPtr::make(vk->default_instance()));\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_default(PhaseGVN& gvn, ciInlineKlass* vk, bool is_larval) {\n+  GrowableArray<ciType*> visited;\n+  visited.push(vk);\n+  return make_default_impl(gvn, vk, visited, is_larval);\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_default_impl(PhaseGVN& gvn, ciInlineKlass* vk, GrowableArray<ciType*>& visited, bool is_larval) {\n+  \/\/ Create a new InlineTypeNode with default values\n+  Node* oop = vk->is_initialized() && !is_larval ? default_oop(gvn, vk) : gvn.zerocon(T_OBJECT);\n+  InlineTypeNode* vt = new InlineTypeNode(vk, oop, \/* null_free= *\/ true);\n+  vt->set_is_buffered(gvn, vk->is_initialized() && !is_larval);\n+  vt->set_is_init(gvn);\n+  vt->set_is_larval(is_larval);\n+  for (uint i = 0; i < vt->field_count(); ++i) {\n+    ciType* ft = vt->field_type(i);\n+    Node* value = gvn.zerocon(ft->basic_type());\n+    if (!vt->field_is_flat(i) && visited.contains(ft)) {\n+      gvn.C->set_has_circular_inline_type(true);\n+    } else if (ft->is_inlinetype()) {\n+      int old_len = visited.length();\n+      visited.push(ft);\n+      ciInlineKlass* vk = ft->as_inline_klass();\n+      if (vt->field_is_null_free(i)) {\n+        value = make_default_impl(gvn, vk, visited);\n+      } else {\n+        value = make_null_impl(gvn, vk, visited);\n+      }\n+      visited.trunc_to(old_len);\n+    }\n+    vt->set_field_value(i, value);\n+  }\n+  vt = gvn.transform(vt)->as_InlineType();\n+  assert(vt->is_default(&gvn), \"must be the default inline type\");\n+  return vt;\n+}\n+\n+bool InlineTypeNode::is_default(PhaseGVN* gvn) const {\n+  const TypeInt* tinit = gvn->type(get_is_init())->isa_int();\n+  if (tinit == nullptr || !tinit->is_con(1)) {\n+    return false; \/\/ May be null\n+  }\n+  for (uint i = 0; i < field_count(); ++i) {\n+    Node* value = field_value(i);\n+    if (field_is_null_free(i)) {\n+      \/\/ Null-free value class field must have the default value\n+      if (!value->is_InlineType() || !value->as_InlineType()->is_default(gvn)) {\n+        return false;\n+      }\n+      continue;\n+    } else if (value->is_InlineType()) {\n+      \/\/ Nullable value class field must be null\n+      tinit = gvn->type(value->as_InlineType()->get_is_init())->isa_int();\n+      if (tinit != nullptr && tinit->is_con(0)) {\n+        continue;\n+      }\n+      return false;\n+    }\n+    if (!gvn->type(value)->is_zero_type()) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_from_oop(GraphKit* kit, Node* oop, ciInlineKlass* vk, bool null_free, bool is_larval) {\n+  GrowableArray<ciType*> visited;\n+  visited.push(vk);\n+  return make_from_oop_impl(kit, oop, vk, null_free, visited, is_larval);\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_from_oop_impl(GraphKit* kit, Node* oop, ciInlineKlass* vk, bool null_free, GrowableArray<ciType*>& visited, bool is_larval) {\n+  PhaseGVN& gvn = kit->gvn();\n+\n+  if (!is_larval && vk->is_empty() && null_free) {\n+    InlineTypeNode* def = make_default_impl(gvn, vk, visited);\n+    kit->record_for_igvn(def);\n+    return def;\n+  }\n+  \/\/ Create and initialize an InlineTypeNode by loading all field\n+  \/\/ values from a heap-allocated version and also save the oop.\n+  InlineTypeNode* vt = nullptr;\n+\n+  if (oop->isa_InlineType()) {\n+    \/\/ TODO 8335256 Re-enable assert and fix OSR code\n+    \/\/ Issue triggers with TestValueConstruction.java and -XX:Tier0BackedgeNotifyFreqLog=0 -XX:Tier2BackedgeNotifyFreqLog=0 -XX:Tier3BackedgeNotifyFreqLog=0 -XX:Tier2BackEdgeThreshold=1 -XX:Tier3BackEdgeThreshold=1 -XX:Tier4BackEdgeThreshold=1 -Xbatch -XX:-TieredCompilation\n+    \/\/ assert(!is_larval || oop->as_InlineType()->is_larval(), \"must be larval\");\n+    if (is_larval && !oop->as_InlineType()->is_larval()) {\n+      vt = oop->clone()->as_InlineType();\n+      vt->set_is_larval(true);\n+      return gvn.transform(vt)->as_InlineType();\n+    }\n+    return oop->as_InlineType();\n+  } else if (gvn.type(oop)->maybe_null()) {\n+    \/\/ Add a null check because the oop may be null\n+    Node* null_ctl = kit->top();\n+    Node* not_null_oop = kit->null_check_oop(oop, &null_ctl);\n+    if (kit->stopped()) {\n+      \/\/ Constant null\n+      kit->set_control(null_ctl);\n+      if (null_free) {\n+        vt = make_default_impl(gvn, vk, visited);\n+      } else {\n+        vt = make_null_impl(gvn, vk, visited);\n+      }\n+      kit->record_for_igvn(vt);\n+      return vt;\n+    }\n+    vt = new InlineTypeNode(vk, not_null_oop, null_free);\n+    vt->set_is_buffered(gvn);\n+    vt->set_is_init(gvn);\n+    vt->set_is_larval(is_larval);\n+    vt->load(kit, not_null_oop, not_null_oop, vk, visited);\n+\n+    if (null_ctl != kit->top()) {\n+      InlineTypeNode* null_vt = nullptr;\n+      if (null_free) {\n+        null_vt = make_default_impl(gvn, vk, visited);\n+      } else {\n+        null_vt = make_null_impl(gvn, vk, visited);\n+      }\n+      Node* region = new RegionNode(3);\n+      region->init_req(1, kit->control());\n+      region->init_req(2, null_ctl);\n+      vt = vt->clone_with_phis(&gvn, region, kit->map());\n+      vt->merge_with(&gvn, null_vt, 2, true);\n+      if (!null_free) {\n+        vt->set_oop(gvn, oop);\n+      }\n+      kit->set_control(gvn.transform(region));\n+    }\n+  } else {\n+    \/\/ Oop can never be null\n+    vt = new InlineTypeNode(vk, oop, \/* null_free= *\/ true);\n+    Node* init_ctl = kit->control();\n+    vt->set_is_buffered(gvn);\n+    vt->set_is_init(gvn);\n+    vt->set_is_larval(is_larval);\n+    vt->load(kit, oop, oop, vk, visited);\n+\/\/ TODO 8284443\n+\/\/    assert(!null_free || vt->as_InlineType()->is_default(&gvn) || init_ctl != kit->control() || !gvn.type(oop)->is_inlinetypeptr() || oop->is_Con() || oop->Opcode() == Op_InlineType ||\n+\/\/           AllocateNode::Ideal_allocation(oop, &gvn) != nullptr || vt->as_InlineType()->is_loaded(&gvn) == oop, \"inline type should be loaded\");\n+  }\n+  assert(vt->is_allocated(&gvn) || (null_free && !vk->is_initialized()), \"inline type should be allocated\");\n+  kit->record_for_igvn(vt);\n+  return gvn.transform(vt)->as_InlineType();\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_from_flat(GraphKit* kit, ciInlineKlass* vk, Node* obj, Node* ptr, ciInstanceKlass* holder, int holder_offset,\n+                                               bool atomic, int null_marker_offset, DecoratorSet decorators) {\n+  GrowableArray<ciType*> visited;\n+  visited.push(vk);\n+  return make_from_flat_impl(kit, vk, obj, ptr, holder, holder_offset, atomic, null_marker_offset, decorators, visited);\n+}\n+\n+\/\/ GraphKit wrapper for the 'make_from_flat' method\n+InlineTypeNode* InlineTypeNode::make_from_flat_impl(GraphKit* kit, ciInlineKlass* vk, Node* obj, Node* ptr, ciInstanceKlass* holder, int holder_offset,\n+                                                    bool atomic, int null_marker_offset, DecoratorSet decorators, GrowableArray<ciType*>& visited) {\n+  if (kit->gvn().type(obj)->isa_aryptr()) {\n+    kit->C->set_flat_accesses();\n+  }\n+  \/\/ Create and initialize an InlineTypeNode by loading all field values from\n+  \/\/ a flat inline type field at 'holder_offset' or from an inline type array.\n+  bool null_free = (null_marker_offset == -1);\n+  InlineTypeNode* vt = make_uninitialized(kit->gvn(), vk, null_free);\n+\n+  if (atomic) {\n+    \/\/ Read atomically and convert from payload\n+#ifdef ASSERT\n+    bool is_naturally_atomic = vk->is_empty() || (null_free && vk->nof_declared_nonstatic_fields() == 1);\n+    assert(!is_naturally_atomic, \"No atomic access required\");\n+#endif\n+    BasicType bt = vk->payload_size_to_basic_type();\n+    decorators |= C2_MISMATCHED | C2_CONTROL_DEPENDENT_LOAD;\n+    Node* adr = kit->basic_plus_adr(obj, ptr, holder_offset);\n+    const Type* val_type = Type::get_const_basic_type(bt);\n+    bool is_array = (kit->gvn().type(obj)->isa_aryptr() != nullptr);\n+    Node* payload = kit->access_load_at(obj, adr, TypeRawPtr::BOTTOM, val_type, bt, is_array ? (decorators | IS_ARRAY) : decorators, kit->control());\n+    vt->convert_from_payload(kit, bt, payload, 0, null_free, null_marker_offset - holder_offset);\n+    return kit->gvn().transform(vt)->as_InlineType();\n+  }\n+\n+  if (!null_free) {\n+    \/\/ Nullable, read the null marker\n+    Node* adr = kit->basic_plus_adr(obj, ptr, null_marker_offset);\n+    Node* is_init = kit->make_load(kit->control(), adr, TypeInt::BOOL, T_BOOLEAN, MemNode::unordered);\n+    vt->set_req(IsInit, is_init);\n+  }\n+\n+  \/\/ The inline type is flattened into the object without an oop header. Subtract the\n+  \/\/ offset of the first field to account for the missing header when loading the values.\n+  holder_offset -= vk->payload_offset();\n+  vt->load(kit, obj, ptr, holder, visited, holder_offset, decorators);\n+  assert(vt->is_loaded(&kit->gvn()) != obj, \"holder oop should not be used as flattened inline type oop\");\n+  return kit->gvn().transform(vt)->as_InlineType();\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_from_multi(GraphKit* kit, MultiNode* multi, ciInlineKlass* vk, uint& base_input, bool in, bool null_free) {\n+  InlineTypeNode* vt = make_uninitialized(kit->gvn(), vk, null_free);\n+  if (!in) {\n+    \/\/ Keep track of the oop. The returned inline type might already be buffered.\n+    Node* oop = kit->gvn().transform(new ProjNode(multi, base_input++));\n+    vt->set_oop(kit->gvn(), oop);\n+  }\n+  GrowableArray<ciType*> visited;\n+  visited.push(vk);\n+  vt->initialize_fields(kit, multi, base_input, in, null_free, nullptr, visited);\n+  return kit->gvn().transform(vt)->as_InlineType();\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_larval(GraphKit* kit, bool allocate) const {\n+  ciInlineKlass* vk = inline_klass();\n+  InlineTypeNode* res = make_uninitialized(kit->gvn(), vk);\n+  for (uint i = 1; i < req(); ++i) {\n+    res->set_req(i, in(i));\n+  }\n+\n+  if (allocate) {\n+    \/\/ Re-execute if buffering triggers deoptimization\n+    PreserveReexecuteState preexecs(kit);\n+    kit->jvms()->set_should_reexecute(true);\n+    Node* klass_node = kit->makecon(TypeKlassPtr::make(vk));\n+    Node* alloc_oop  = kit->new_instance(klass_node, nullptr, nullptr, true);\n+    AllocateNode* alloc = AllocateNode::Ideal_allocation(alloc_oop);\n+    alloc->_larval = true;\n+\n+    store(kit, alloc_oop, alloc_oop, vk);\n+    res->set_oop(kit->gvn(), alloc_oop);\n+  }\n+  \/\/ TODO 8239003\n+  \/\/res->set_type(TypeInlineType::make(vk, true));\n+  res = kit->gvn().transform(res)->as_InlineType();\n+  assert(!allocate || res->is_allocated(&kit->gvn()), \"must be allocated\");\n+  return res;\n+}\n+\n+InlineTypeNode* InlineTypeNode::finish_larval(GraphKit* kit) const {\n+  Node* obj = get_oop();\n+  Node* mark_addr = kit->basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+  Node* mark = kit->make_load(nullptr, mark_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  mark = kit->gvn().transform(new AndXNode(mark, kit->MakeConX(~markWord::larval_bit_in_place)));\n+  kit->store_to_memory(kit->control(), mark_addr, mark, TypeX_X->basic_type(), MemNode::unordered);\n+\n+  \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+  \/\/ store that would make this buffer accessible by other threads.\n+  AllocateNode* alloc = AllocateNode::Ideal_allocation(obj);\n+  assert(alloc != nullptr, \"must have an allocation node\");\n+  kit->insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out_or_null(AllocateNode::RawAddress));\n+\n+  ciInlineKlass* vk = inline_klass();\n+  InlineTypeNode* res = make_uninitialized(kit->gvn(), vk);\n+  for (uint i = 1; i < req(); ++i) {\n+    res->set_req(i, in(i));\n+  }\n+  \/\/ TODO 8239003\n+  \/\/res->set_type(TypeInlineType::make(vk, false));\n+  res = kit->gvn().transform(res)->as_InlineType();\n+  return res;\n+}\n+\n+bool InlineTypeNode::is_larval(PhaseGVN* gvn) const {\n+  if (!is_allocated(gvn)) {\n+    return false;\n+  }\n+\n+  Node* oop = get_oop();\n+  AllocateNode* alloc = AllocateNode::Ideal_allocation(oop);\n+  return alloc != nullptr && alloc->_larval;\n+}\n+\n+Node* InlineTypeNode::is_loaded(PhaseGVN* phase, ciInlineKlass* vk, Node* base, int holder_offset) {\n+  if (is_larval() || is_larval(phase)) {\n+    return nullptr;\n+  }\n+  if (vk == nullptr) {\n+    vk = inline_klass();\n+  }\n+  if (field_count() == 0 && vk->is_initialized()) {\n+    const TypeInt* tinit = phase->type(get_is_init())->isa_int();\n+    if (tinit != nullptr && tinit->is_con(1)) {\n+      assert(is_allocated(phase), \"must be allocated\");\n+      return get_oop();\n+    } else {\n+      \/\/ TODO 8284443\n+      return nullptr;\n+    }\n+  }\n+  for (uint i = 0; i < field_count(); ++i) {\n+    int offset = holder_offset + field_offset(i);\n+    Node* value = field_value(i);\n+    if (value->is_InlineType()) {\n+      InlineTypeNode* vt = value->as_InlineType();\n+      if (vt->type()->inline_klass()->is_empty()) {\n+        continue;\n+      } else if (field_is_flat(i) && vt->is_InlineType()) {\n+        \/\/ Check inline type field load recursively\n+        base = vt->as_InlineType()->is_loaded(phase, vk, base, offset - vt->type()->inline_klass()->payload_offset());\n+        if (base == nullptr) {\n+          return nullptr;\n+        }\n+        continue;\n+      } else {\n+        value = vt->get_oop();\n+        if (value->Opcode() == Op_CastPP) {\n+          \/\/ Skip CastPP\n+          value = value->in(1);\n+        }\n+      }\n+    }\n+    if (value->isa_DecodeN()) {\n+      \/\/ Skip DecodeN\n+      value = value->in(1);\n+    }\n+    if (value->isa_Load()) {\n+      \/\/ Check if base and offset of field load matches inline type layout\n+      intptr_t loffset = 0;\n+      Node* lbase = AddPNode::Ideal_base_and_offset(value->in(MemNode::Address), phase, loffset);\n+      if (lbase == nullptr || (lbase != base && base != nullptr) || loffset != offset) {\n+        return nullptr;\n+      } else if (base == nullptr) {\n+        \/\/ Set base and check if pointer type matches\n+        base = lbase;\n+        const TypeInstPtr* vtptr = phase->type(base)->isa_instptr();\n+        if (vtptr == nullptr || !vtptr->instance_klass()->equals(vk)) {\n+          return nullptr;\n+        }\n+      }\n+    } else {\n+      return nullptr;\n+    }\n+  }\n+  return base;\n+}\n+\n+Node* InlineTypeNode::tagged_klass(ciInlineKlass* vk, PhaseGVN& gvn) {\n+  const TypeKlassPtr* tk = TypeKlassPtr::make(vk);\n+  intptr_t bits = tk->get_con();\n+  set_nth_bit(bits, 0);\n+  return gvn.longcon((jlong)bits);\n+}\n+\n+void InlineTypeNode::pass_fields(GraphKit* kit, Node* n, uint& base_input, bool in, bool null_free) {\n+  if (!null_free && in) {\n+    n->init_req(base_input++, get_is_init());\n+  }\n+  for (uint i = 0; i < field_count(); i++) {\n+    Node* arg = field_value(i);\n+    if (field_is_flat(i)) {\n+      \/\/ Flat inline type field\n+      arg->as_InlineType()->pass_fields(kit, n, base_input, in);\n+      if (!field_is_null_free(i)) {\n+        assert(field_null_marker_offset(i) != -1, \"inconsistency\");\n+        n->init_req(base_input++, arg->as_InlineType()->get_is_init());\n+      }\n+    } else {\n+      if (arg->is_InlineType()) {\n+        \/\/ Non-flat inline type field\n+        InlineTypeNode* vt = arg->as_InlineType();\n+        assert(n->Opcode() != Op_Return || vt->is_allocated(&kit->gvn()), \"inline type field should be allocated on return\");\n+        arg = vt->buffer(kit);\n+      }\n+      \/\/ Initialize call\/return arguments\n+      n->init_req(base_input++, arg);\n+      if (field_type(i)->size() == 2) {\n+        n->init_req(base_input++, kit->top());\n+      }\n+    }\n+  }\n+  \/\/ The last argument is used to pass IsInit information to compiled code and not required here.\n+  if (!null_free && !in) {\n+    n->init_req(base_input++, kit->top());\n+  }\n+}\n+\n+void InlineTypeNode::initialize_fields(GraphKit* kit, MultiNode* multi, uint& base_input, bool in, bool null_free, Node* null_check_region, GrowableArray<ciType*>& visited) {\n+  PhaseGVN& gvn = kit->gvn();\n+  Node* is_init = nullptr;\n+  if (!null_free) {\n+    \/\/ Nullable inline type\n+    if (in) {\n+      \/\/ Set IsInit field\n+      if (multi->is_Start()) {\n+        is_init = gvn.transform(new ParmNode(multi->as_Start(), base_input));\n+      } else {\n+        is_init = multi->as_Call()->in(base_input);\n+      }\n+      set_req(IsInit, is_init);\n+      base_input++;\n+    }\n+    \/\/ Add a null check to make subsequent loads dependent on\n+    assert(null_check_region == nullptr, \"already set\");\n+    if (is_init == nullptr) {\n+      \/\/ Will only be initialized below, use dummy node for now\n+      is_init = new Node(1);\n+      is_init->init_req(0, kit->control()); \/\/ Add an input to prevent dummy from being dead\n+      gvn.set_type_bottom(is_init);\n+    }\n+    Node* null_ctrl = kit->top();\n+    kit->null_check_common(is_init, T_INT, false, &null_ctrl);\n+    Node* non_null_ctrl = kit->control();\n+    null_check_region = new RegionNode(3);\n+    null_check_region->init_req(1, non_null_ctrl);\n+    null_check_region->init_req(2, null_ctrl);\n+    null_check_region = gvn.transform(null_check_region);\n+    kit->set_control(null_check_region);\n+  }\n+\n+  for (uint i = 0; i < field_count(); ++i) {\n+    ciType* type = field_type(i);\n+    Node* parm = nullptr;\n+    if (field_is_flat(i)) {\n+      \/\/ Flat inline type field\n+      InlineTypeNode* vt = make_uninitialized(gvn, type->as_inline_klass(), field_is_null_free(i));\n+      vt->initialize_fields(kit, multi, base_input, in, true, null_check_region, visited);\n+      if (!field_is_null_free(i)) {\n+        assert(field_null_marker_offset(i) != -1, \"inconsistency\");\n+        Node* is_init = nullptr;\n+        if (multi->is_Start()) {\n+          is_init = gvn.transform(new ParmNode(multi->as_Start(), base_input));\n+        } else if (in) {\n+          is_init = multi->as_Call()->in(base_input);\n+        } else {\n+          is_init = gvn.transform(new ProjNode(multi->as_Call(), base_input));\n+        }\n+        vt->set_req(IsInit, is_init);\n+        base_input++;\n+      }\n+      parm = gvn.transform(vt);\n+    } else {\n+      if (multi->is_Start()) {\n+        assert(in, \"return from start?\");\n+        parm = gvn.transform(new ParmNode(multi->as_Start(), base_input));\n+      } else if (in) {\n+        parm = multi->as_Call()->in(base_input);\n+      } else {\n+        parm = gvn.transform(new ProjNode(multi->as_Call(), base_input));\n+      }\n+      \/\/ Non-flat inline type field\n+      if (type->is_inlinetype()) {\n+        if (null_check_region != nullptr) {\n+          \/\/ We limit scalarization for inline types with circular fields and can therefore observe nodes\n+          \/\/ of the same type but with different scalarization depth during GVN. To avoid inconsistencies\n+          \/\/ during merging, make sure that we only create Phis for fields that are guaranteed to be scalarized.\n+          if (parm->is_InlineType() && kit->C->has_circular_inline_type()) {\n+            parm = parm->as_InlineType()->get_oop();\n+          }\n+          \/\/ Holder is nullable, set field to nullptr if holder is nullptr to avoid loading from uninitialized memory\n+          parm = PhiNode::make(null_check_region, parm, TypeInstPtr::make(TypePtr::BotPTR, type->as_inline_klass()));\n+          parm->set_req(2, kit->zerocon(T_OBJECT));\n+          parm = gvn.transform(parm);\n+        }\n+        if (visited.contains(type)) {\n+          kit->C->set_has_circular_inline_type(true);\n+        } else if (!parm->is_InlineType()) {\n+          int old_len = visited.length();\n+          visited.push(type);\n+          parm = make_from_oop_impl(kit, parm, type->as_inline_klass(), field_is_null_free(i), visited);\n+          visited.trunc_to(old_len);\n+        }\n+      }\n+      base_input += type->size();\n+    }\n+    assert(parm != nullptr, \"should never be null\");\n+    assert(field_value(i) == nullptr, \"already set\");\n+    set_field_value(i, parm);\n+    gvn.record_for_igvn(parm);\n+  }\n+  \/\/ The last argument is used to pass IsInit information to compiled code\n+  if (!null_free && !in) {\n+    Node* cmp = is_init->raw_out(0);\n+    is_init = gvn.transform(new ProjNode(multi->as_Call(), base_input));\n+    set_req(IsInit, is_init);\n+    gvn.hash_delete(cmp);\n+    cmp->set_req(1, is_init);\n+    gvn.hash_find_insert(cmp);\n+    gvn.record_for_igvn(cmp);\n+    base_input++;\n+  }\n+}\n+\n+\/\/ Search for multiple allocations of this inline type and try to replace them by dominating allocations.\n+\/\/ Equivalent InlineTypeNodes are merged by GVN, so we just need to search for AllocateNode users to find redundant allocations.\n+void InlineTypeNode::remove_redundant_allocations(PhaseIdealLoop* phase) {\n+  \/\/ TODO 8332886 Really needed? GVN is disabled anyway.\n+  if (is_larval()) {\n+    return;\n+  }\n+  PhaseIterGVN* igvn = &phase->igvn();\n+  \/\/ Search for allocations of this inline type. Ignore scalar replaceable ones, they\n+  \/\/ will be removed anyway and changing the memory chain will confuse other optimizations.\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    AllocateNode* alloc = fast_out(i)->isa_Allocate();\n+    if (alloc != nullptr && alloc->in(AllocateNode::InlineType) == this && !alloc->_is_scalar_replaceable) {\n+      Node* res = alloc->result_cast();\n+      if (res == nullptr || !res->is_CheckCastPP()) {\n+        break; \/\/ No unique CheckCastPP\n+      }\n+      assert((!is_default(igvn) || !inline_klass()->is_initialized()) && !is_allocated(igvn), \"re-allocation should be removed by Ideal transformation\");\n+      \/\/ Search for a dominating allocation of the same inline type\n+      Node* res_dom = res;\n+      for (DUIterator_Fast jmax, j = fast_outs(jmax); j < jmax; j++) {\n+        AllocateNode* alloc_other = fast_out(j)->isa_Allocate();\n+        if (alloc_other != nullptr && alloc_other->in(AllocateNode::InlineType) == this && !alloc_other->_is_scalar_replaceable) {\n+          Node* res_other = alloc_other->result_cast();\n+          if (res_other != nullptr && res_other->is_CheckCastPP() && res_other != res_dom &&\n+              phase->is_dominator(res_other->in(0), res_dom->in(0))) {\n+            res_dom = res_other;\n+          }\n+        }\n+      }\n+      if (res_dom != res) {\n+        \/\/ Replace allocation by dominating one.\n+        replace_allocation(igvn, res, res_dom);\n+        \/\/ The result of the dominated allocation is now unused and will be removed\n+        \/\/ later in PhaseMacroExpand::eliminate_allocate_node to not confuse loop opts.\n+        igvn->_worklist.push(alloc);\n+      }\n+    }\n+  }\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_null(PhaseGVN& gvn, ciInlineKlass* vk, bool transform) {\n+  GrowableArray<ciType*> visited;\n+  visited.push(vk);\n+  return make_null_impl(gvn, vk, visited, transform);\n+}\n+\n+InlineTypeNode* InlineTypeNode::make_null_impl(PhaseGVN& gvn, ciInlineKlass* vk, GrowableArray<ciType*>& visited, bool transform) {\n+  InlineTypeNode* vt = new InlineTypeNode(vk, gvn.zerocon(T_OBJECT), \/* null_free= *\/ false);\n+  vt->set_is_buffered(gvn);\n+  vt->set_is_init(gvn, false);\n+  for (uint i = 0; i < vt->field_count(); i++) {\n+    ciType* ft = vt->field_type(i);\n+    Node* value = gvn.zerocon(ft->basic_type());\n+    if (!vt->field_is_flat(i) && visited.contains(ft)) {\n+      gvn.C->set_has_circular_inline_type(true);\n+    } else if (ft->is_inlinetype()) {\n+      int old_len = visited.length();\n+      visited.push(ft);\n+      value = make_null_impl(gvn, ft->as_inline_klass(), visited);\n+      visited.trunc_to(old_len);\n+    }\n+    vt->set_field_value(i, value);\n+  }\n+  return transform ? gvn.transform(vt)->as_InlineType() : vt;\n+}\n+\n+InlineTypeNode* InlineTypeNode::clone_if_required(PhaseGVN* gvn, SafePointNode* map, bool safe_for_replace) {\n+  if (!safe_for_replace || (map == nullptr && outcnt() != 0)) {\n+    return clone()->as_InlineType();\n+  }\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    if (fast_out(i) != map) {\n+      return clone()->as_InlineType();\n+    }\n+  }\n+  gvn->hash_delete(this);\n+  return this;\n+}\n+\n+const Type* InlineTypeNode::Value(PhaseGVN* phase) const {\n+  Node* oop = get_oop();\n+  const Type* toop = phase->type(oop);\n+#ifdef ASSERT\n+  if (oop->is_Con() && toop->is_zero_type() && _type->isa_oopptr()->is_known_instance()) {\n+    \/\/ We are not allocated (anymore) and should therefore not have an instance id\n+    dump(1);\n+    assert(false, \"Unbuffered inline type should not have known instance id\");\n+  }\n+#endif\n+  const Type* t = toop->filter_speculative(_type);\n+  if (t->singleton()) {\n+    \/\/ Don't replace InlineType by a constant\n+    t = _type;\n+  }\n+  const Type* tinit = phase->type(in(IsInit));\n+  if (tinit == Type::TOP) {\n+    return Type::TOP;\n+  }\n+  if (tinit->isa_int() && tinit->is_int()->is_con(1)) {\n+    t = t->join_speculative(TypePtr::NOTNULL);\n+  }\n+  return t;\n+}\n+\n+#ifndef PRODUCT\n+void InlineTypeNode::dump_spec(outputStream* st) const {\n+  if (_is_larval) {\n+    st->print(\" #larval\");\n+  }\n+}\n+#endif \/\/ NOT PRODUCT\n","filename":"src\/hotspot\/share\/opto\/inlinetypenode.cpp","additions":1651,"deletions":0,"binary":false,"changes":1651,"status":"added"},{"patch":"@@ -197,0 +197,1 @@\n+    case Op_StoreLSpecial:\n@@ -286,1 +287,1 @@\n-        if (offset == Type::OffsetBot || tptr->_offset == Type::OffsetBot)\n+        if (offset == Type::OffsetBot || tptr->offset() == Type::OffsetBot)\n@@ -288,1 +289,1 @@\n-        offset += tptr->_offset; \/\/ correct if base is offsetted\n+        offset += tptr->offset(); \/\/ correct if base is offsetted\n@@ -325,1 +326,5 @@\n-      Block *inb = get_block_for_node(mach->in(j));\n+      Block* inb = get_block_for_node(mach->in(j));\n+      if (mach->in(j)->is_Con() && mach->in(j)->req() == 1 && inb == get_block_for_node(mach)) {\n+        \/\/ Ignore constant loads scheduled in the same block (we can simply hoist them as well)\n+        continue;\n+      }\n@@ -418,0 +423,21 @@\n+\n+  \/\/ Hoist constant load inputs as well.\n+  for (uint i = 1; i < best->req(); ++i) {\n+    Node* n = best->in(i);\n+    if (n->is_Con() && get_block_for_node(n) == get_block_for_node(best)) {\n+      get_block_for_node(n)->find_remove(n);\n+      block->add_inst(n);\n+      map_node_to_block(n, block);\n+      \/\/ Constant loads may kill flags (for example, when XORing a register).\n+      \/\/ Check for flag-killing projections that also need to be hoisted.\n+      for (DUIterator_Fast jmax, j = n->fast_outs(jmax); j < jmax; j++) {\n+        Node* proj = n->fast_out(j);\n+        if (proj->is_MachProj()) {\n+          get_block_for_node(proj)->find_remove(proj);\n+          block->add_inst(proj);\n+          map_node_to_block(proj, block);\n+        }\n+      }\n+    }\n+  }\n+\n@@ -731,0 +757,1 @@\n+        case Op_StoreLSpecial:\n@@ -891,1 +918,1 @@\n-  uint r_cnt = mcall->tf()->range()->cnt();\n+  uint r_cnt = mcall->tf()->range_cc()->cnt();\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":31,"deletions":4,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -328,0 +329,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:        return inline_unsafe_make_private_buffer();\n+  case vmIntrinsics::_finishPrivateBuffer:      return inline_unsafe_finish_private_buffer();\n@@ -337,0 +340,1 @@\n+  case vmIntrinsics::_getValue:                 return inline_unsafe_access(!is_store, T_OBJECT,   Relaxed, false, true);\n@@ -347,0 +351,1 @@\n+  case vmIntrinsics::_putValue:                 return inline_unsafe_access( is_store, T_OBJECT,   Relaxed, false, true);\n@@ -513,0 +518,1 @@\n+  case vmIntrinsics::_isFlatArray:              return inline_unsafe_isFlatArray();\n@@ -525,0 +531,1 @@\n+  case vmIntrinsics::_newNullRestrictedArray:   return inline_newNullRestrictedArray();\n@@ -2249,0 +2256,1 @@\n+  bool null_free = false;\n@@ -2254,0 +2262,1 @@\n+      null_free = alias_type->field()->is_null_free();\n@@ -2262,0 +2271,1 @@\n+      null_free = adr_type->is_aryptr()->is_null_free();\n@@ -2274,0 +2284,3 @@\n+    if (null_free) {\n+      result = result->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n@@ -2304,1 +2317,1 @@\n-bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned) {\n+bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned, const bool is_flat) {\n@@ -2329,1 +2342,1 @@\n-      assert(sig->count() == 2, \"oop getter has 2 arguments\");\n+      assert(sig->count() == 2 || (is_flat && sig->count() == 3), \"oop getter has 2 or 3 arguments\");\n@@ -2335,1 +2348,1 @@\n-      assert(sig->count() == 3, \"oop putter has 3 arguments\");\n+      assert(sig->count() == 3 || (is_flat && sig->count() == 4), \"oop putter has 3 arguments\");\n@@ -2361,0 +2374,55 @@\n+\n+  ciInlineKlass* inline_klass = nullptr;\n+  if (is_flat) {\n+    const TypeInstPtr* cls = _gvn.type(argument(4))->isa_instptr();\n+    if (cls == nullptr || cls->const_oop() == nullptr) {\n+      return false;\n+    }\n+    ciType* mirror_type = cls->const_oop()->as_instance()->java_mirror_type();\n+    if (!mirror_type->is_inlinetype()) {\n+      return false;\n+    }\n+    inline_klass = mirror_type->as_inline_klass();\n+  }\n+\n+  if (base->is_InlineType()) {\n+    InlineTypeNode* vt = base->as_InlineType();\n+    if (is_store) {\n+      if (!vt->is_allocated(&_gvn)) {\n+        return false;\n+      }\n+      base = vt->get_oop();\n+    } else {\n+      if (offset->is_Con()) {\n+        long off = find_long_con(offset, 0);\n+        ciInlineKlass* vk = vt->type()->inline_klass();\n+        if ((long)(int)off != off || !vk->contains_field_offset(off)) {\n+          return false;\n+        }\n+\n+        ciField* field = vk->get_non_flat_field_by_offset(off);\n+        if (field != nullptr) {\n+          BasicType bt = type2field[field->type()->basic_type()];\n+          if (bt == T_ARRAY || bt == T_NARROWOOP) {\n+            bt = T_OBJECT;\n+          }\n+          if (bt == type && (!field->is_flat() || field->type() == inline_klass)) {\n+            Node* value = vt->field_value_by_offset(off, false);\n+            if (value->is_InlineType()) {\n+              value = value->as_InlineType()->adjust_scalarization_depth(this);\n+            }\n+            set_result(value);\n+            return true;\n+          }\n+        }\n+      }\n+      {\n+        \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        vt = vt->buffer(this);\n+      }\n+      base = vt->get_oop();\n+    }\n+  }\n+\n@@ -2372,1 +2440,1 @@\n-    if (type != T_OBJECT) {\n+    if (type != T_OBJECT && (inline_klass == nullptr || !inline_klass->has_object_fields())) {\n@@ -2390,1 +2458,1 @@\n-  Node* val = is_store ? argument(4) : nullptr;\n+  Node* val = is_store ? argument(4 + (is_flat ? 1 : 0)) : nullptr;\n@@ -2411,1 +2479,22 @@\n-  BasicType bt = alias_type->basic_type();\n+  BasicType bt = T_ILLEGAL;\n+  ciField* field = nullptr;\n+  if (adr_type->isa_instptr()) {\n+    const TypeInstPtr* instptr = adr_type->is_instptr();\n+    ciInstanceKlass* k = instptr->instance_klass();\n+    int off = instptr->offset();\n+    if (instptr->const_oop() != nullptr &&\n+        k == ciEnv::current()->Class_klass() &&\n+        instptr->offset() >= (k->size_helper() * wordSize)) {\n+      k = instptr->const_oop()->as_instance()->java_lang_Class_klass()->as_instance_klass();\n+      field = k->get_field_by_offset(off, true);\n+    } else {\n+      field = k->get_non_flat_field_by_offset(off);\n+    }\n+    if (field != nullptr) {\n+      bt = type2field[field->type()->basic_type()];\n+    }\n+    assert(bt == alias_type->basic_type() || is_flat, \"should match\");\n+  } else {\n+    bt = alias_type->basic_type();\n+  }\n+\n@@ -2434,0 +2523,23 @@\n+  if (is_flat) {\n+    if (adr_type->isa_instptr()) {\n+      if (field == nullptr || field->type() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else if (adr_type->isa_aryptr()) {\n+      const Type* elem = adr_type->is_aryptr()->elem();\n+      if (!adr_type->is_flat() || elem->inline_klass() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else {\n+      mismatched = true;\n+    }\n+    if (is_store) {\n+      const Type* val_t = _gvn.type(val);\n+      if (!val_t->is_inlinetypeptr() || val_t->inline_klass() != inline_klass) {\n+        set_map(old_map);\n+        set_sp(old_sp);\n+        return false;\n+      }\n+    }\n+  }\n+\n@@ -2435,1 +2547,1 @@\n-  assert(!mismatched || alias_type->adr_type()->is_oopptr(), \"off-heap access can't be mismatched\");\n+  assert(!mismatched || is_flat || alias_type->adr_type()->is_oopptr(), \"off-heap access can't be mismatched\");\n@@ -2447,4 +2559,6 @@\n-  if (!is_store && type == T_OBJECT) {\n-    const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n-    if (tjp != nullptr) {\n-      value_type = tjp;\n+  if (!is_store) {\n+    if (type == T_OBJECT && !is_flat) {\n+      const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n+      if (tjp != nullptr) {\n+        value_type = tjp;\n+      }\n@@ -2466,2 +2580,2 @@\n-    ciField* field = alias_type->field();\n-    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !mismatched) {\n+\n+    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !field->is_flat() && !mismatched) {\n@@ -2473,1 +2587,16 @@\n-      p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+      if (is_flat) {\n+        if (adr_type->isa_instptr() && !mismatched) {\n+          ciInstanceKlass* holder = adr_type->is_instptr()->instance_klass();\n+          int offset = adr_type->is_instptr()->offset();\n+          p = InlineTypeNode::make_from_flat(this, inline_klass, base, base, holder, offset, false, -1, decorators);\n+        } else {\n+          p = InlineTypeNode::make_from_flat(this, inline_klass, base, adr, nullptr, 0, false, -1, decorators);\n+        }\n+      } else {\n+        p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+        const TypeOopPtr* ptr = value_type->make_oopptr();\n+        if (ptr != nullptr && ptr->is_inlinetypeptr()) {\n+          \/\/ Load a non-flattened inline type from memory\n+          p = InlineTypeNode::make_from_oop(this, p, ptr->inline_klass(), !ptr->maybe_null());\n+        }\n+      }\n@@ -2511,1 +2640,17 @@\n-    access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    if (is_flat) {\n+      if (adr_type->isa_instptr() && !mismatched) {\n+        ciInstanceKlass* holder = adr_type->is_instptr()->instance_klass();\n+        int offset = adr_type->is_instptr()->offset();\n+        val->as_InlineType()->store_flat(this, base, base, holder, offset, false, -1, decorators);\n+      } else {\n+        val->as_InlineType()->store_flat(this, base, adr, nullptr, 0, false, -1, decorators);\n+      }\n+    } else {\n+      access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    }\n+  }\n+\n+  if (argument(1)->is_InlineType() && is_store) {\n+    InlineTypeNode* value = InlineTypeNode::make_from_oop(this, base, _gvn.type(argument(1))->inline_klass());\n+    value = value->make_larval(this, false);\n+    replace_in_map(argument(1), value);\n@@ -2517,0 +2662,40 @@\n+bool LibraryCallKit::inline_unsafe_make_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* value = argument(1);\n+  if (!value->is_InlineType()) {\n+    return false;\n+  }\n+\n+  receiver = null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  set_result(value->as_InlineType()->make_larval(this, true));\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_finish_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* buffer = argument(1);\n+  if (!buffer->is_InlineType()) {\n+    return false;\n+  }\n+  InlineTypeNode* vt = buffer->as_InlineType();\n+  if (!vt->is_allocated(&_gvn)) {\n+    return false;\n+  }\n+  \/\/ TODO 8239003 Why is this needed?\n+  if (AllocateNode::Ideal_allocation(vt->get_oop()) == nullptr) {\n+    return false;\n+  }\n+\n+  receiver = null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  set_result(vt->finish_larval(this));\n+  return true;\n+}\n+\n@@ -2722,0 +2907,13 @@\n+    if (oldval != nullptr && oldval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      oldval = oldval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+    if (newval != nullptr && newval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      newval = newval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+\n@@ -2908,2 +3106,7 @@\n-\n-  Node* obj = new_instance(kls, test);\n+  Node* obj = nullptr;\n+  const TypeInstKlassPtr* tkls = _gvn.type(kls)->isa_instklassptr();\n+  if (tkls != nullptr && tkls->instance_klass()->is_inlinetype()) {\n+    obj = InlineTypeNode::make_default(_gvn, tkls->instance_klass()->as_inline_klass())->buffer(this);\n+  } else {\n+    obj = new_instance(kls, test);\n+  }\n@@ -3691,1 +3894,1 @@\n-  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3696,1 +3899,1 @@\n-  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, 0);\n+  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, TypeAryPtr::Offset(0));\n@@ -3819,9 +4022,0 @@\n-\/\/---------------------------load_mirror_from_klass----------------------------\n-\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n-Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {\n-  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n-  Node* load = make_load(nullptr, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n-  \/\/ mirror = ((OopHandle)mirror)->resolve();\n-  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n-}\n-\n@@ -3871,0 +4065,1 @@\n+\n@@ -4071,0 +4266,1 @@\n+\n@@ -4086,1 +4282,2 @@\n-  ciType* tm = mirror_con->java_mirror_type();\n+  bool is_null_free_array = false;\n+  ciType* tm = mirror_con->java_mirror_type(&is_null_free_array);\n@@ -4093,1 +4290,5 @@\n-      int static_res = C->static_subtype_check(TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces), tp->as_klass_type());\n+      const TypeKlassPtr* tklass = TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces);\n+      if (is_null_free_array) {\n+        tklass = tklass->is_aryklassptr()->cast_to_null_free();\n+      }\n+      int static_res = C->static_subtype_check(tklass, tp->as_klass_type());\n@@ -4122,2 +4323,2 @@\n-  \/\/ Not-subtype or the mirror's klass ptr is null (in case it is a primitive).\n-  enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };\n+  \/\/ Not-subtype or the mirror's klass ptr is nullptr (in case it is a primitive).\n+  enum { _bad_type_path = 1, _prim_path = 2, _npe_path = 3, PATH_LIMIT };\n@@ -4133,0 +4334,2 @@\n+  Node* io = i_o();\n+  Node* mem = merged_memory();\n@@ -4134,0 +4337,1 @@\n+\n@@ -4140,1 +4344,2 @@\n-      region->in(_bad_type_path) != top()) {\n+      region->in(_bad_type_path) != top() ||\n+      region->in(_npe_path) != top()) {\n@@ -4144,0 +4349,3 @@\n+    \/\/ Set IO and memory because gen_checkcast may override them when buffering inline types\n+    set_i_o(io);\n+    set_all_memory(mem);\n@@ -4177,0 +4385,1 @@\n+  RegionNode* prim_region = new RegionNode(2);\n@@ -4179,0 +4388,1 @@\n+  record_for_igvn(prim_region);\n@@ -4203,2 +4413,5 @@\n-    int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);\n-    region->init_req(prim_path, null_ctl);\n+    if (which_arg == 0) {\n+      prim_region->init_req(1, null_ctl);\n+    } else {\n+      region->init_req(_prim_1_path, null_ctl);\n+    }\n@@ -4214,1 +4427,0 @@\n-    \/\/ now we have a successful reference subtype check\n@@ -4221,1 +4433,2 @@\n-  set_control(region->in(_prim_0_path)); \/\/ go back to first null check\n+  \/\/ This path is also used if superc is a value mirror.\n+  set_control(_gvn.transform(prim_region));\n@@ -4226,1 +4439,1 @@\n-    generate_guard(bol_eq, region, PROB_FAIR);\n+    generate_fair_guard(bol_eq, region);\n@@ -4257,2 +4470,1 @@\n-Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,\n-                                                  bool obj_array, bool not_array, Node** obj) {\n+Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind, Node** obj) {\n@@ -4264,9 +4476,0 @@\n-  \/\/ If obj_array\/non_array==false\/false:\n-  \/\/ Branch around if the given klass is in fact an array (either obj or prim).\n-  \/\/ If obj_array\/non_array==false\/true:\n-  \/\/ Branch around if the given klass is not an array klass of any kind.\n-  \/\/ If obj_array\/non_array==true\/true:\n-  \/\/ Branch around if the kls is not an oop array (kls is int[], String, etc.)\n-  \/\/ If obj_array\/non_array==true\/false:\n-  \/\/ Branch around if the kls is an oop array (Object[] or subtype)\n-  \/\/\n@@ -4277,4 +4480,11 @@\n-    bool query = (obj_array\n-                  ? Klass::layout_helper_is_objArray(layout_con)\n-                  : Klass::layout_helper_is_array(layout_con));\n-    if (query == not_array) {\n+    bool query = 0;\n+    switch(kind) {\n+      case ObjectArray:    query = Klass::layout_helper_is_objArray(layout_con); break;\n+      case NonObjectArray: query = !Klass::layout_helper_is_objArray(layout_con); break;\n+      case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;\n+      case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;\n+      case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    if (!query) {\n@@ -4290,0 +4500,21 @@\n+  unsigned int value = 0;\n+  BoolTest::mask btest = BoolTest::illegal;\n+  switch(kind) {\n+    case ObjectArray:\n+    case NonObjectArray: {\n+      value = Klass::_lh_array_tag_obj_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = (kind == ObjectArray) ? BoolTest::eq : BoolTest::ne;\n+      break;\n+    }\n+    case TypeArray: {\n+      value = Klass::_lh_array_tag_type_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = BoolTest::eq;\n+      break;\n+    }\n+    case AnyArray:    value = Klass::_lh_neutral_value; btest = BoolTest::lt; break;\n+    case NonArray:    value = Klass::_lh_neutral_value; btest = BoolTest::gt; break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -4291,4 +4522,1 @@\n-  jint  nval = (obj_array\n-                ? (jint)(Klass::_lh_array_tag_type_value\n-                   <<    Klass::_lh_array_tag_shift)\n-                : Klass::_lh_neutral_value);\n+  jint nval = (jint)value;\n@@ -4296,3 +4524,0 @@\n-  BoolTest::mask btest = BoolTest::lt;  \/\/ correct for testing is_[obj]array\n-  \/\/ invert the test if we are looking for a non-array\n-  if (not_array)  btest = BoolTest(btest).negate();\n@@ -4301,1 +4526,1 @@\n-  Node* is_array_ctrl = not_array ? control() : ctrl;\n+  Node* is_array_ctrl = kind == NonArray ? control() : ctrl;\n@@ -4310,0 +4535,26 @@\n+\/\/-----------------------inline_newNullRestrictedArray--------------------------\n+\/\/ public static native Object[] newNullRestrictedArray(Class<?> componentType, int length);\n+bool LibraryCallKit::inline_newNullRestrictedArray() {\n+  Node* componentType = argument(0);\n+  Node* length = argument(1);\n+\n+  const TypeInstPtr* tp = _gvn.type(componentType)->isa_instptr();\n+  if (tp != nullptr) {\n+    ciInstanceKlass* ik = tp->instance_klass();\n+    if (ik == C->env()->Class_klass()) {\n+      ciType* t = tp->java_mirror_type();\n+      if (t != nullptr && t->is_inlinetype()) {\n+        ciArrayKlass* array_klass = ciArrayKlass::make(t, true);\n+        if (array_klass->is_loaded() && array_klass->element_klass()->as_inline_klass()->is_initialized()) {\n+          const TypeAryKlassPtr* array_klass_type = TypeKlassPtr::make(array_klass, Type::trust_interfaces)->is_aryklassptr();\n+          array_klass_type = array_klass_type->cast_to_null_free();\n+          Node* obj = new_array(makecon(array_klass_type), length, 0, nullptr, false);  \/\/ no arguments to push\n+          set_result(obj);\n+          assert(gvn().type(obj)->is_aryptr()->is_null_free(), \"must be null-free\");\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n@@ -4312,1 +4563,1 @@\n-\/\/ private static native Object java.lang.reflect.newArray(Class<?> componentType, int length);\n+\/\/ private static native Object java.lang.reflect.Array.newArray(Class<?> componentType, int length);\n@@ -4458,1 +4709,13 @@\n-    Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);\n+    \/\/ Inline type array may have object field that would require a\n+    \/\/ write barrier. Conservatively, go to slow path.\n+    \/\/ TODO 8251971: Optimize for the case when flat src\/dst are later found\n+    \/\/ to not contain oops (i.e., move this check to the macro expansion phase).\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    const TypeAryPtr* orig_t = _gvn.type(original)->isa_aryptr();\n+    const TypeKlassPtr* tklass = _gvn.type(klass_node)->is_klassptr();\n+    bool exclude_flat = UseArrayFlattening && bs->array_copy_requires_gc_barriers(true, T_OBJECT, false, false, BarrierSetC2::Parsing) &&\n+                        \/\/ Can src array be flat and contain oops?\n+                        (orig_t == nullptr || (!orig_t->is_not_flat() && (!orig_t->is_flat() || orig_t->elem()->inline_klass()->contains_oops()))) &&\n+                        \/\/ Can dest array be flat and contain oops?\n+                        tklass->can_be_inline_array() && (!tklass->is_flat() || tklass->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->as_inline_klass()->contains_oops());\n+    Node* not_objArray = exclude_flat ? generate_non_objArray_guard(klass_node, bailout) : generate_typeArray_guard(klass_node, bailout);\n@@ -4462,1 +4725,1 @@\n-      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0\/*offset*\/);\n+      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0));\n@@ -4482,0 +4745,38 @@\n+    \/\/ Handle inline type arrays\n+    bool can_validate = !too_many_traps(Deoptimization::Reason_class_check);\n+    if (!stopped()) {\n+      \/\/ TODO JDK-8329224\n+      if (!orig_t->is_null_free()) {\n+        \/\/ Not statically known to be null free, add a check\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+      orig_t = _gvn.type(original)->isa_aryptr();\n+      if (orig_t != nullptr && orig_t->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (exclude_flat) {\n+          \/\/ Dest can't be flat, bail out\n+          bailout->add_req(control());\n+          set_control(top());\n+        } else {\n+          generate_fair_guard(flat_array_test(klass_node, \/* flat = *\/ false), bailout);\n+        }\n+      } else if (UseArrayFlattening && (orig_t == nullptr || !orig_t->is_not_flat()) &&\n+                 \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check if validated).\n+                 ((!tklass->is_flat() && tklass->can_be_inline_array()) || !can_validate)) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        generate_fair_guard(flat_array_test(load_object_klass(original)), bailout);\n+        if (orig_t != nullptr) {\n+          orig_t = orig_t->cast_to_not_flat();\n+          original = _gvn.transform(new CheckCastPPNode(control(), original, orig_t));\n+        }\n+      }\n+      if (!can_validate) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat\/null-free.\n+        generate_fair_guard(flat_array_test(klass_node), bailout);\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+    }\n+\n@@ -4527,1 +4828,1 @@\n-      if (!too_many_traps(Deoptimization::Reason_class_check)) {\n+      if (can_validate) {\n@@ -4613,1 +4914,1 @@\n-    const TypeTuple* range = tf->range();\n+    const TypeTuple* range = tf->range_cc();\n@@ -4617,1 +4918,1 @@\n-    tf = TypeFunc::make(tf->domain(), new_range);\n+    tf = TypeFunc::make(tf->domain_cc(), new_range);\n@@ -4674,1 +4975,8 @@\n-  Node* obj = nullptr;\n+  Node* obj = argument(0);\n+\n+  \/\/ Don't intrinsify hashcode on inline types for now.\n+  \/\/ The \"is locked\" runtime check below also serves as inline type check and goes to the slow path.\n+  if (gvn().type(obj)->is_inlinetypeptr()) {\n+    return false;\n+  }\n+\n@@ -4684,1 +4992,0 @@\n-    obj = argument(0);\n@@ -4725,1 +5032,2 @@\n-    Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n+  \/\/ This also serves as guard against inline types\n+    Node *lock_mask      = _gvn.MakeConX(markWord::inline_type_mask_in_place);\n@@ -4800,1 +5108,10 @@\n-  Node* obj = null_check_receiver();\n+  Node* obj = argument(0);\n+  if (obj->is_InlineType()) {\n+    const Type* t = _gvn.type(obj);\n+    if (t->maybe_null()) {\n+      null_check(obj);\n+    }\n+    set_result(makecon(TypeInstPtr::make(t->inline_klass()->java_mirror())));\n+    return true;\n+  }\n+  obj = null_check_receiver();\n@@ -5152,0 +5469,14 @@\n+\/\/----------------------inline_unsafe_isFlatArray------------------------\n+\/\/ public native boolean Unsafe.isFlatArray(Class<?> arrayClass);\n+\/\/ This intrinsic exploits assumptions made by the native implementation\n+\/\/ (arrayClass is neither null nor primitive) to avoid unnecessary null checks.\n+bool LibraryCallKit::inline_unsafe_isFlatArray() {\n+  Node* cls = argument(1);\n+  Node* p = basic_plus_adr(cls, java_lang_Class::klass_offset());\n+  Node* kls = _gvn.transform(LoadKlassNode::make(_gvn, nullptr, immutable_memory(), p,\n+                                                 TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT));\n+  Node* result = flat_array_test(kls);\n+  set_result(result);\n+  return true;\n+}\n+\n@@ -5222,1 +5553,2 @@\n-    Node* obj = null_check_receiver();\n+    Node* obj = argument(0);\n+    obj = null_check_receiver();\n@@ -5226,0 +5558,6 @@\n+    if (obj_type->is_inlinetypeptr()) {\n+      \/\/ If the object to clone is an inline type, we can simply return it (i.e. a nop) since inline types have\n+      \/\/ no identity.\n+      set_result(obj);\n+      return true;\n+    }\n@@ -5232,1 +5570,2 @@\n-        obj_type->speculative_type()->is_instance_klass()) {\n+        obj_type->speculative_type()->is_instance_klass() &&\n+        !obj_type->speculative_type()->is_inlinetype()) {\n@@ -5262,0 +5601,5 @@\n+    \/\/ We only go to the fast case code if we pass a number of guards.\n+    \/\/ The paths which do not pass are accumulated in the slow_region.\n+    RegionNode* slow_region = new RegionNode(1);\n+    record_for_igvn(slow_region);\n+\n@@ -5268,3 +5612,0 @@\n-      Node* obj_length = load_array_length(array_obj);\n-      Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n-      Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n@@ -5273,20 +5614,7 @@\n-      if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n-        \/\/ If it is an oop array, it requires very special treatment,\n-        \/\/ because gc barriers are required when accessing the array.\n-        Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)nullptr);\n-        if (is_obja != nullptr) {\n-          PreserveJVMState pjvms2(this);\n-          set_control(is_obja);\n-          \/\/ Generate a direct call to the right arraycopy function(s).\n-          \/\/ Clones are always tightly coupled.\n-          ArrayCopyNode* ac = ArrayCopyNode::make(this, true, array_obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n-          ac->set_clone_oop_array();\n-          Node* n = _gvn.transform(ac);\n-          assert(n == ac, \"cannot disappear\");\n-          ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n-\n-          result_reg->init_req(_objArray_path, control());\n-          result_val->init_req(_objArray_path, alloc_obj);\n-          result_i_o ->set_req(_objArray_path, i_o());\n-          result_mem ->set_req(_objArray_path, reset_memory());\n-        }\n+      const TypeAryPtr* ary_ptr = obj_type->isa_aryptr();\n+      if (UseArrayFlattening && bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Expansion) &&\n+          obj_type->can_be_inline_array() &&\n+          (ary_ptr == nullptr || (!ary_ptr->is_not_flat() && (!ary_ptr->is_flat() || ary_ptr->elem()->inline_klass()->contains_oops())))) {\n+        \/\/ Flat inline type array may have object field that would require a\n+        \/\/ write barrier. Conservatively, go to slow path.\n+        generate_fair_guard(flat_array_test(obj_klass), slow_region);\n@@ -5294,7 +5622,0 @@\n-      \/\/ Otherwise, there are no barriers to worry about.\n-      \/\/ (We can dispense with card marks if we know the allocation\n-      \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n-      \/\/  causes the non-eden paths to take compensating steps to\n-      \/\/  simulate a fresh allocation, so that no further\n-      \/\/  card marks are required in compiled code to initialize\n-      \/\/  the object.)\n@@ -5303,7 +5624,43 @@\n-        copy_to_clone(array_obj, alloc_obj, array_size, true);\n-\n-        \/\/ Present the results of the copy.\n-        result_reg->init_req(_array_path, control());\n-        result_val->init_req(_array_path, alloc_obj);\n-        result_i_o ->set_req(_array_path, i_o());\n-        result_mem ->set_req(_array_path, reset_memory());\n+        Node* obj_length = load_array_length(array_obj);\n+        Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n+        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n+\n+        BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+        if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n+          \/\/ If it is an oop array, it requires very special treatment,\n+          \/\/ because gc barriers are required when accessing the array.\n+          Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)nullptr);\n+          if (is_obja != nullptr) {\n+            PreserveJVMState pjvms2(this);\n+            set_control(is_obja);\n+            \/\/ Generate a direct call to the right arraycopy function(s).\n+            \/\/ Clones are always tightly coupled.\n+            ArrayCopyNode* ac = ArrayCopyNode::make(this, true, array_obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n+            ac->set_clone_oop_array();\n+            Node* n = _gvn.transform(ac);\n+            assert(n == ac, \"cannot disappear\");\n+            ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n+\n+            result_reg->init_req(_objArray_path, control());\n+            result_val->init_req(_objArray_path, alloc_obj);\n+            result_i_o ->set_req(_objArray_path, i_o());\n+            result_mem ->set_req(_objArray_path, reset_memory());\n+          }\n+        }\n+        \/\/ Otherwise, there are no barriers to worry about.\n+        \/\/ (We can dispense with card marks if we know the allocation\n+        \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n+        \/\/  causes the non-eden paths to take compensating steps to\n+        \/\/  simulate a fresh allocation, so that no further\n+        \/\/  card marks are required in compiled code to initialize\n+        \/\/  the object.)\n+\n+        if (!stopped()) {\n+          copy_to_clone(obj, alloc_obj, array_size, true);\n+\n+          \/\/ Present the results of the copy.\n+          result_reg->init_req(_array_path, control());\n+          result_val->init_req(_array_path, alloc_obj);\n+          result_i_o ->set_req(_array_path, i_o());\n+          result_mem ->set_req(_array_path, reset_memory());\n+        }\n@@ -5313,4 +5670,0 @@\n-    \/\/ We only go to the instance fast case code if we pass a number of guards.\n-    \/\/ The paths which do not pass are accumulated in the slow_region.\n-    RegionNode* slow_region = new RegionNode(1);\n-    record_for_igvn(slow_region);\n@@ -5448,0 +5801,12 @@\n+  int adjustment = 1;\n+  const TypeAryKlassPtr* ary_klass_ptr = alloc->in(AllocateNode::KlassNode)->bottom_type()->is_aryklassptr();\n+  if (ary_klass_ptr->is_null_free()) {\n+    \/\/ A null-free, tightly coupled array allocation can only come from LibraryCallKit::inline_newNullRestrictedArray\n+    \/\/ which requires both the component type and the array length on stack for re-execution. Re-create and push\n+    \/\/ the component type.\n+    ciArrayKlass* klass = ary_klass_ptr->exact_klass()->as_array_klass();\n+    ciInstance* instance = klass->component_mirror_instance();\n+    const TypeInstPtr* t_instance = TypeInstPtr::make(instance);\n+    sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), makecon(t_instance));\n+    adjustment++;\n+  }\n@@ -5449,5 +5814,5 @@\n-  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), alloc->in(AllocateNode::ALength));\n-  old_jvms->set_sp(old_jvms->sp()+1);\n-  old_jvms->set_monoff(old_jvms->monoff()+1);\n-  old_jvms->set_scloff(old_jvms->scloff()+1);\n-  old_jvms->set_endoff(old_jvms->endoff()+1);\n+  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp() + adjustment - 1, alloc->in(AllocateNode::ALength));\n+  old_jvms->set_sp(old_jvms->sp() + adjustment);\n+  old_jvms->set_monoff(old_jvms->monoff() + adjustment);\n+  old_jvms->set_scloff(old_jvms->scloff() + adjustment);\n+  old_jvms->set_endoff(old_jvms->endoff() + adjustment);\n@@ -5486,2 +5851,1 @@\n-    CallProjections callprojs;\n-    alloc->extract_projections(&callprojs, true);\n+    CallProjections* callprojs = alloc->extract_projections(true);\n@@ -5490,1 +5854,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -5532,1 +5896,1 @@\n-    set_i_o(callprojs.fallthrough_ioproj);\n+    set_i_o(callprojs->fallthrough_ioproj);\n@@ -5870,1 +6234,1 @@\n-    if (src_elem == dest_elem && src_elem == T_OBJECT) {\n+    if (src_elem == dest_elem && top_src->is_flat() == top_dest->is_flat() && src_elem == T_OBJECT) {\n@@ -5897,0 +6261,2 @@\n+          src_type = _gvn.type(src);\n+          top_src = src_type->isa_aryptr();\n@@ -5900,0 +6266,2 @@\n+          dest_type = _gvn.type(dest);\n+          top_dest = dest_type->isa_aryptr();\n@@ -5915,2 +6283,1 @@\n-      can_emit_guards &&\n-      !src->is_top() && !dest->is_top()) {\n+      can_emit_guards && !src->is_top() && !dest->is_top()) {\n@@ -5959,0 +6326,2 @@\n+      slow_region->add_req(not_subtype_ctrl);\n+    }\n@@ -5960,6 +6329,28 @@\n-      if (not_subtype_ctrl != top()) {\n-        PreserveJVMState pjvms(this);\n-        set_control(not_subtype_ctrl);\n-        uncommon_trap(Deoptimization::Reason_intrinsic,\n-                      Deoptimization::Action_make_not_entrant);\n-        assert(stopped(), \"Should be stopped\");\n+    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n+    const Type* toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n+    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n+    src_type = _gvn.type(src);\n+    top_src  = src_type->isa_aryptr();\n+\n+    \/\/ Handle flat inline type arrays (null-free arrays are handled by the subtype check above)\n+    if (!stopped() && UseArrayFlattening) {\n+      \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check). Handle flat src here.\n+      assert(top_dest == nullptr || !top_dest->is_flat() || top_src->is_flat(), \"src array must be flat\");\n+      if (top_src != nullptr && top_src->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (top_dest != nullptr && !top_dest->is_flat()) {\n+          generate_fair_guard(flat_array_test(dest_klass, \/* flat = *\/ false), slow_region);\n+          \/\/ Since dest is flat and src <: dest, dest must have the same type as src.\n+          top_dest = top_src->cast_to_exactness(false);\n+          assert(top_dest->is_flat(), \"dest must be flat\");\n+          dest = _gvn.transform(new CheckCastPPNode(control(), dest, top_dest));\n+        }\n+      } else if (top_src == nullptr || !top_src->is_not_flat()) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        assert(top_dest == nullptr || !top_dest->is_flat(), \"dest array must not be flat\");\n+        generate_fair_guard(flat_array_test(src), slow_region);\n+        if (top_src != nullptr) {\n+          top_src = top_src->cast_to_not_flat();\n+          src = _gvn.transform(new CheckCastPPNode(control(), src, top_src));\n+        }\n@@ -5968,0 +6359,1 @@\n+\n@@ -5975,4 +6367,0 @@\n-\n-    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n-    const Type *toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n-    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":524,"deletions":136,"binary":false,"changes":660,"status":"modified"},{"patch":"@@ -187,0 +187,12 @@\n+const Type* FastLockNode::Value(PhaseGVN* phase) const {\n+  const Type* in1_t = phase->type(in(1));\n+  if (in1_t == Type::TOP) {\n+    return Type::TOP;\n+  }\n+  if (in1_t->is_inlinetypeptr()) {\n+    \/\/ Locking on inline types always fails\n+    return TypeInt::CC_GT;\n+  }\n+  return TypeInt::CC;\n+}\n+\n@@ -206,0 +218,6 @@\n+  {\n+    \/\/ Synchronizing on an inline type is not allowed\n+    BuildCutout unless(this, inline_type_test(obj, \/* is_inline = *\/ false), PROB_MAX);\n+    uncommon_trap_exact(Deoptimization::Reason_class_check, Deoptimization::Action_none);\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/locknode.cpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -111,1 +111,6 @@\n-  if (phase->find_unswitch_candidate(this) == nullptr) {\n+\n+  if (head->is_flat_arrays()) {\n+    return false;\n+  }\n+\n+  if (no_unswitch_candidate()) {\n@@ -119,0 +124,7 @@\n+\/\/ Check the absence of any If node that can be used for Loop Unswitching. In that case, no Loop Unswitching can be done.\n+bool IdealLoopTree::no_unswitch_candidate() const {\n+  ResourceMark rm;\n+  Node_List dont_care;\n+  return _phase->find_unswitch_candidates(this, dont_care) == nullptr;\n+}\n+\n@@ -120,2 +132,39 @@\n-\/\/ one in the loop body. Return the \"unswitch candidate\" If to apply Loop Unswitching on.\n-IfNode* PhaseIdealLoop::find_unswitch_candidate(const IdealLoopTree* loop) const {\n+\/\/ one in the loop body as \"unswitch candidate\" to apply Loop Unswitching on.\n+\/\/ Depending on whether we find such a candidate and if we do, whether it's a flat array check, we do the following:\n+\/\/ (1) Candidate is not a flat array check:\n+\/\/     Return the unique unswitch candidate.\n+\/\/ (2) Candidate is a flat array check:\n+\/\/     Collect all remaining non-loop-exiting flat array checks in the loop body in the provided 'flat_array_checks'\n+\/\/     list in order to create an unswitched loop version without any flat array checks and a version with checks\n+\/\/     (i.e. same as original loop). Return the initially found candidate which could be unique if no further flat array\n+\/\/     checks are found.\n+\/\/ (3) No candidate is initially found:\n+\/\/     As in (2), we collect all non-loop-exiting flat array checks in the loop body in the provided 'flat_array_checks'\n+\/\/     list. Pick the first collected flat array check as unswitch candidate, which could be unique, and return it (a).\n+\/\/     If there are no flat array checks, we cannot apply Loop Unswitching (b).\n+\/\/\n+\/\/ Note that for both (2) and (3a), if there are multiple flat array checks, then the candidate's FlatArrayCheckNode is\n+\/\/ later updated in Loop Unswitching to perform a flat array check on all collected flat array checks.\n+IfNode* PhaseIdealLoop::find_unswitch_candidates(const IdealLoopTree* loop, Node_List& flat_array_checks) const {\n+  IfNode* unswitch_candidate = find_unswitch_candidate_from_idoms(loop);\n+  if (unswitch_candidate != nullptr && !unswitch_candidate->is_flat_array_check(&_igvn)) {\n+    \/\/ Case (1)\n+    return unswitch_candidate;\n+  }\n+\n+  collect_flat_array_checks(loop, flat_array_checks);\n+  if (unswitch_candidate != nullptr) {\n+    \/\/ Case (2)\n+    assert(unswitch_candidate->is_flat_array_check(&_igvn), \"is a flat array check\");\n+    return unswitch_candidate;\n+  } else if (flat_array_checks.size() > 0) {\n+    \/\/ Case (3a): Pick first one found as candidate (there could be multiple).\n+    return flat_array_checks[0]->as_If();\n+  }\n+\n+  \/\/ Case (3b): No suitable unswitch candidate found.\n+  return nullptr;\n+}\n+\n+\/\/ Find an unswitch candidate by following the idom chain from the loop back edge.\n+IfNode* PhaseIdealLoop::find_unswitch_candidate_from_idoms(const IdealLoopTree* loop) const {\n@@ -148,0 +197,144 @@\n+\/\/ Collect all flat array checks in the provided 'flat_array_checks' list.\n+void PhaseIdealLoop::collect_flat_array_checks(const IdealLoopTree* loop, Node_List& flat_array_checks) const {\n+  assert(flat_array_checks.size() == 0, \"should be empty initially\");\n+  for (uint i = 0; i < loop->_body.size(); i++) {\n+    Node* next = loop->_body.at(i);\n+    if (next->is_If() && next->as_If()->is_flat_array_check(&_igvn) && loop->is_invariant(next->in(1)) &&\n+        !loop->is_loop_exit(next)) {\n+      flat_array_checks.push(next);\n+    }\n+  }\n+}\n+\n+\/\/ This class represents an \"unswitch candidate\" which is an If that can be used to perform Loop Unswitching on. If the\n+\/\/ candidate is a flat array check candidate, then we also collect all remaining non-loop-exiting flat array checks.\n+\/\/ These are candidates as well. We want to get rid of all these flat array checks in the true-path-loop for the\n+\/\/ following reason:\n+\/\/\n+\/\/ FlatArrayCheckNodes are used with array accesses to switch between a flat and a non-flat array access. We want\n+\/\/ the performance impact on non-flat array accesses to be as small as possible. We therefore create the following\n+\/\/ loops in Loop Unswitching:\n+\/\/ - True-path-loop:  We remove all non-loop-exiting flat array checks to get a loop with only non-flat array accesses\n+\/\/                    (i.e. a fast path loop).\n+\/\/ - False-path-loop: We keep all flat array checks in this loop (i.e. a slow path loop).\n+class UnswitchCandidate : public StackObj {\n+  PhaseIdealLoop* const _phase;\n+  const Node_List& _old_new;\n+  Node* const _original_loop_entry;\n+  \/\/ If _candidate is a flat array check, this list contains all non-loop-exiting flat array checks in the loop body.\n+  Node_List _flat_array_check_candidates;\n+  IfNode* const _candidate;\n+\n+ public:\n+  UnswitchCandidate(IdealLoopTree* loop, const Node_List& old_new)\n+      : _phase(loop->_phase),\n+        _old_new(old_new),\n+        _original_loop_entry(loop->_head->as_Loop()->skip_strip_mined()->in(LoopNode::EntryControl)),\n+        _flat_array_check_candidates(),\n+        _candidate(find_unswitch_candidate(loop)) {}\n+  NONCOPYABLE(UnswitchCandidate);\n+\n+  IfNode* find_unswitch_candidate(IdealLoopTree* loop) {\n+    IfNode* unswitch_candidate = _phase->find_unswitch_candidates(loop, _flat_array_check_candidates);\n+    assert(unswitch_candidate != nullptr, \"guaranteed to exist by policy_unswitching\");\n+    assert(_phase->is_member(loop, unswitch_candidate), \"must be inside original loop\");\n+    return unswitch_candidate;\n+  }\n+\n+  IfNode* candidate() const {\n+    return _candidate;\n+  }\n+\n+  \/\/ Is the candidate a flat array check and are there other flat array checks as well?\n+  bool has_multiple_flat_array_check_candidates() const {\n+    return _flat_array_check_candidates.size() > 1;\n+  }\n+\n+  \/\/ Remove all candidates from the true-path-loop which are now dominated by the loop selector\n+  \/\/ (i.e. 'true_path_loop_proj'). The removed candidates are folded in the next IGVN round.\n+  void update_in_true_path_loop(IfTrueNode* true_path_loop_proj) const {\n+    remove_from_loop(true_path_loop_proj, _candidate);\n+    if (has_multiple_flat_array_check_candidates()) {\n+      remove_flat_array_checks(true_path_loop_proj);\n+    }\n+  }\n+\n+  \/\/ Remove a unique candidate from the false-path-loop which is now dominated by the loop selector\n+  \/\/ (i.e. 'false_path_loop_proj'). The removed candidate is folded in the next IGVN round. If there are multiple\n+  \/\/ candidates (i.e. flat array checks), then we leave them in the false-path-loop and only mark the loop such that it\n+  \/\/ is not unswitched anymore in later loop opts rounds.\n+  void update_in_false_path_loop(IfFalseNode* false_path_loop_proj, LoopNode* false_path_loop) const {\n+    if (has_multiple_flat_array_check_candidates()) {\n+      \/\/ Leave the flat array checks in the false-path-loop and prevent it from being unswitched again based on these\n+      \/\/ checks.\n+      false_path_loop->mark_flat_arrays();\n+    } else {\n+      remove_from_loop(false_path_loop_proj, _old_new[_candidate->_idx]->as_If());\n+    }\n+  }\n+\n+ private:\n+  void remove_from_loop(IfProjNode* dominating_proj, IfNode* candidate) const {\n+    _phase->igvn().rehash_node_delayed(candidate);\n+    _phase->dominated_by(dominating_proj, candidate);\n+  }\n+\n+  void remove_flat_array_checks(IfProjNode* dominating_proj) const {\n+    for (uint i = 0; i < _flat_array_check_candidates.size(); i++) {\n+      IfNode* flat_array_check = _flat_array_check_candidates.at(i)->as_If();\n+      _phase->igvn().rehash_node_delayed(flat_array_check);\n+      _phase->dominated_by(dominating_proj, flat_array_check);\n+    }\n+  }\n+\n+ public:\n+  \/\/ Merge all flat array checks into a single new BoolNode and return it.\n+  BoolNode* merge_flat_array_checks() const {\n+    assert(has_multiple_flat_array_check_candidates(), \"must have multiple flat array checks to merge\");\n+    assert(_candidate->in(1)->as_Bool()->_test._test == BoolTest::ne, \"IfTrue proj must point to flat array\");\n+    BoolNode* merged_flat_array_check_bool = create_bool_node();\n+    create_flat_array_check_node(merged_flat_array_check_bool);\n+    return merged_flat_array_check_bool;\n+  }\n+\n+ private:\n+  BoolNode* create_bool_node() const {\n+    BoolNode* merged_flat_array_check_bool = _candidate->in(1)->clone()->as_Bool();\n+    _phase->register_new_node(merged_flat_array_check_bool, _original_loop_entry);\n+    return merged_flat_array_check_bool;\n+  }\n+\n+  void create_flat_array_check_node(BoolNode* merged_flat_array_check_bool) const {\n+    FlatArrayCheckNode* cloned_flat_array_check = merged_flat_array_check_bool->in(1)->clone()->as_FlatArrayCheck();\n+    _phase->register_new_node(cloned_flat_array_check, _original_loop_entry);\n+    merged_flat_array_check_bool->set_req(1, cloned_flat_array_check);\n+    set_flat_array_check_inputs(cloned_flat_array_check);\n+  }\n+\n+  \/\/ Combine all checks into a single one that fails if one array is flat.\n+  void set_flat_array_check_inputs(FlatArrayCheckNode* cloned_flat_array_check) const {\n+    assert(cloned_flat_array_check->req() == 3, \"unexpected number of inputs for FlatArrayCheck\");\n+    cloned_flat_array_check->add_req_batch(_phase->C->top(), _flat_array_check_candidates.size() - 1);\n+    for (uint i = 0; i < _flat_array_check_candidates.size(); i++) {\n+      Node* array = _flat_array_check_candidates.at(i)->in(1)->in(1)->in(FlatArrayCheckNode::ArrayOrKlass);\n+      cloned_flat_array_check->set_req(FlatArrayCheckNode::ArrayOrKlass + i, array);\n+    }\n+  }\n+\n+ public:\n+#ifndef PRODUCT\n+  void trace_flat_array_checks() const {\n+    if (has_multiple_flat_array_check_candidates()) {\n+      tty->print_cr(\"- Unswitched and Merged Flat Array Checks:\");\n+      for (uint i = 0; i < _flat_array_check_candidates.size(); i++) {\n+        Node* unswitch_iff = _flat_array_check_candidates.at(i);\n+        Node* cloned_unswitch_iff = _old_new[unswitch_iff->_idx];\n+        assert(cloned_unswitch_iff != nullptr, \"must exist\");\n+        tty->print_cr(\"  - %d %s  ->  %d %s\", unswitch_iff->_idx, unswitch_iff->Name(),\n+                      cloned_unswitch_iff->_idx, cloned_unswitch_iff->Name());\n+      }\n+    }\n+  }\n+#endif \/\/ NOT PRODUCT\n+};\n+\n@@ -155,1 +348,1 @@\n-  IfNode* const _unswitch_candidate;\n+  const UnswitchCandidate& _unswitch_candidate;\n@@ -160,1 +353,3 @@\n-  enum PathToLoop { TRUE_PATH, FALSE_PATH };\n+  enum PathToLoop {\n+    TRUE_PATH, FALSE_PATH\n+  };\n@@ -163,1 +358,1 @@\n-  UnswitchedLoopSelector(IdealLoopTree* loop)\n+  UnswitchedLoopSelector(IdealLoopTree* loop, const UnswitchCandidate& unswitch_candidate)\n@@ -167,1 +362,1 @@\n-        _unswitch_candidate(find_unswitch_candidate(loop)),\n+        _unswitch_candidate(unswitch_candidate),\n@@ -175,7 +370,0 @@\n-  IfNode* find_unswitch_candidate(IdealLoopTree* loop) {\n-    IfNode* unswitch_candidate = _phase->find_unswitch_candidate(loop);\n-    assert(unswitch_candidate != nullptr, \"guaranteed to exist by policy_unswitching\");\n-    assert(_phase->is_member(loop, unswitch_candidate), \"must be inside original loop\");\n-    return unswitch_candidate;\n-  }\n-\n@@ -185,3 +373,8 @@\n-    BoolNode* unswitch_candidate_bool = _unswitch_candidate->in(1)->as_Bool();\n-    IfNode* selector_if = IfNode::make_with_same_profile(_unswitch_candidate, _original_loop_entry,\n-                                                         unswitch_candidate_bool);\n+    IfNode* unswitch_candidate_if = _unswitch_candidate.candidate();\n+    BoolNode* selector_bool;\n+    if (_unswitch_candidate.has_multiple_flat_array_check_candidates()) {\n+      selector_bool = _unswitch_candidate.merge_flat_array_checks();\n+    } else {\n+      selector_bool = unswitch_candidate_if->in(1)->as_Bool();\n+    }\n+    IfNode* selector_if = IfNode::make_with_same_profile(unswitch_candidate_if, _original_loop_entry, selector_bool);\n@@ -205,4 +398,0 @@\n-  IfNode* unswitch_candidate() const {\n-    return _unswitch_candidate;\n-  }\n-\n@@ -249,1 +438,0 @@\n-    remove_unswitch_candidate_from_loops(unswitched_loop_selector);\n@@ -301,13 +489,0 @@\n-  \/\/ Remove the unswitch candidate If nodes in both unswitched loop versions which are now dominated by the loop selector\n-  \/\/ If node. Keep the true-path-path in the true-path-loop and the false-path-path in the false-path-loop by setting\n-  \/\/ the bool input accordingly. The unswitch candidate If nodes are folded in the next IGVN round.\n-  void remove_unswitch_candidate_from_loops(const UnswitchedLoopSelector& unswitched_loop_selector) {\n-    IfNode* unswitching_candidate = unswitched_loop_selector.unswitch_candidate();\n-    _phase->igvn().rehash_node_delayed(unswitching_candidate);\n-    _phase->dominated_by(unswitched_loop_selector.true_path_loop_proj(), unswitching_candidate);\n-\n-    IfNode* unswitching_candidate_clone = _old_new[unswitching_candidate->_idx]->as_If();\n-    _phase->igvn().rehash_node_delayed(unswitching_candidate_clone);\n-    _phase->dominated_by(unswitched_loop_selector.false_path_loop_proj(), unswitching_candidate_clone);\n-  }\n-\n@@ -331,1 +506,2 @@\n-  const UnswitchedLoopSelector unswitched_loop_selector(loop);\n+  const UnswitchCandidate unswitch_candidate(loop, old_new);\n+  const UnswitchedLoopSelector unswitched_loop_selector(loop, unswitch_candidate);\n@@ -335,1 +511,4 @@\n-  hoist_invariant_check_casts(loop, old_new, unswitched_loop_selector);\n+  unswitch_candidate.update_in_true_path_loop(unswitched_loop_selector.true_path_loop_proj());\n+  unswitch_candidate.update_in_false_path_loop(unswitched_loop_selector.false_path_loop_proj(),\n+                                               old_new[original_head->_idx]->as_Loop());\n+  hoist_invariant_check_casts(loop, old_new, unswitch_candidate, unswitched_loop_selector.selector());\n@@ -341,1 +520,1 @@\n-  NOT_PRODUCT(trace_loop_unswitching_result(unswitched_loop_selector, original_head, new_head);)\n+  NOT_PRODUCT(trace_loop_unswitching_result(unswitched_loop_selector, unswitch_candidate, original_head, new_head);)\n@@ -377,0 +556,1 @@\n+                                                   const UnswitchCandidate& unswitch_candidate,\n@@ -379,1 +559,1 @@\n-    IfNode* unswitch_candidate = unswitched_loop_selector.unswitch_candidate();\n+    IfNode* unswitch_candidate_if = unswitch_candidate.candidate();\n@@ -382,1 +562,1 @@\n-    tty->print_cr(\"- Unswitch-Candidate-If: %d %s\", unswitch_candidate->_idx, unswitch_candidate->Name());\n+    tty->print_cr(\"- Unswitch-Candidate-If: %d %s\", unswitch_candidate_if->_idx, unswitch_candidate_if->Name());\n@@ -386,0 +566,1 @@\n+    unswitch_candidate.trace_flat_array_checks();\n@@ -401,3 +582,2 @@\n-                                                 const UnswitchedLoopSelector& unswitched_loop_selector) {\n-  IfNode* unswitch_candidate = unswitched_loop_selector.unswitch_candidate();\n-  IfNode* loop_selector = unswitched_loop_selector.selector();\n+                                                 const UnswitchCandidate& unswitch_candidate,\n+                                                 const IfNode* loop_selector) {\n@@ -406,2 +586,3 @@\n-  for (DUIterator_Fast imax, i = unswitch_candidate->fast_outs(imax); i < imax; i++) {\n-    IfProjNode* proj = unswitch_candidate->fast_out(i)->as_IfProj();\n+  const IfNode* unswitch_candidate_if = unswitch_candidate.candidate();\n+  for (DUIterator_Fast imax, i = unswitch_candidate_if->fast_outs(imax); i < imax; i++) {\n+    IfProjNode* proj = unswitch_candidate_if->fast_out(i)->as_IfProj();\n@@ -422,3 +603,6 @@\n-      \/\/ Same for the clone\n-      Node* use_clone = old_new[cast->_idx];\n-      _igvn.replace_input_of(use_clone, 1, cast_clone);\n+      \/\/ Same for the false-path-loop if there are not multiple flat array checks (in that case we leave the\n+      \/\/ false-path-loop unchanged).\n+      if (!unswitch_candidate.has_multiple_flat_array_check_candidates()) {\n+        Node* use_clone = old_new[cast->_idx];\n+        _igvn.replace_input_of(use_clone, 1, cast_clone);\n+      }\n@@ -432,1 +616,1 @@\n-  for(int i = loop->_body.size() - 1; i >= 0 ; i--) {\n+  for (int i = loop->_body.size() - 1; i >= 0; i--) {\n@@ -444,1 +628,0 @@\n-\n","filename":"src\/hotspot\/share\/opto\/loopUnswitch.cpp","additions":233,"deletions":50,"binary":false,"changes":283,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+class UnswitchCandidate;\n@@ -82,1 +83,2 @@\n-         LoopNestLongOuterLoop = 1<<16 };\n+         LoopNestLongOuterLoop = 1<<16,\n+         FlatArrays            = 1<<17};\n@@ -105,0 +107,1 @@\n+  bool is_flat_arrays() const { return _loop_flags & FlatArrays; }\n@@ -118,0 +121,1 @@\n+  void mark_flat_arrays() { _loop_flags |= FlatArrays; }\n@@ -693,0 +697,1 @@\n+  bool no_unswitch_candidate() const;\n@@ -1444,1 +1449,2 @@\n-  IfNode* find_unswitch_candidate(const IdealLoopTree* loop) const;\n+  IfNode* find_unswitch_candidates(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+  IfNode* find_unswitch_candidate_from_idoms(const IdealLoopTree* loop) const;\n@@ -1451,1 +1457,1 @@\n-                                   const UnswitchedLoopSelector& unswitched_loop_selector);\n+                                   const UnswitchCandidate& unswitch_candidate, const IfNode* loop_selector);\n@@ -1459,0 +1465,1 @@\n+                                            const UnswitchCandidate& unswitch_candidate,\n@@ -1593,0 +1600,1 @@\n+  void move_flat_array_check_out_of_loop(Node* n);\n@@ -1594,0 +1602,1 @@\n+  bool flat_array_element_type_check(Node *n);\n@@ -1783,0 +1792,2 @@\n+  void collect_flat_array_checks(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -66,0 +67,6 @@\n+  \/\/ Inline types should not be split through Phis because they cannot be merged\n+  \/\/ through Phi nodes but each value input needs to be merged individually.\n+  if (n->is_InlineType()) {\n+    return nullptr;\n+  }\n+\n@@ -763,0 +770,4 @@\n+      if (inp->isa_InlineType()) {\n+        \/\/ TODO 8302217 This prevents PhiNode::push_inline_types_through\n+        return nullptr;\n+      }\n@@ -1090,0 +1101,48 @@\n+\/\/ We can't use immutable memory for the flat array check because we are loading the mark word which is\n+\/\/ mutable. Although the bits we are interested in are immutable (we check for markWord::unlocked_value),\n+\/\/ we need to use raw memory to not break anti dependency analysis. Below code will attempt to still move\n+\/\/ flat array checks out of loops, mainly to enable loop unswitching.\n+void PhaseIdealLoop::move_flat_array_check_out_of_loop(Node* n) {\n+  \/\/ Skip checks for more than one array\n+  if (n->req() > 3) {\n+    return;\n+  }\n+  Node* mem = n->in(FlatArrayCheckNode::Memory);\n+  Node* array = n->in(FlatArrayCheckNode::ArrayOrKlass)->uncast();\n+  IdealLoopTree* check_loop = get_loop(get_ctrl(n));\n+  IdealLoopTree* ary_loop = get_loop(get_ctrl(array));\n+\n+  \/\/ Check if array is loop invariant\n+  if (!check_loop->is_member(ary_loop)) {\n+    \/\/ Walk up memory graph from the check until we leave the loop\n+    VectorSet wq;\n+    wq.set(mem->_idx);\n+    while (check_loop->is_member(get_loop(ctrl_or_self(mem)))) {\n+      if (mem->is_Phi()) {\n+        mem = mem->in(1);\n+      } else if (mem->is_MergeMem()) {\n+        mem = mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw);\n+      } else if (mem->is_Proj()) {\n+        mem = mem->in(0);\n+      } else if (mem->is_MemBar() || mem->is_SafePoint()) {\n+        mem = mem->in(TypeFunc::Memory);\n+      } else if (mem->is_Store() || mem->is_LoadStore() || mem->is_ClearArray()) {\n+        mem = mem->in(MemNode::Memory);\n+      } else {\n+#ifdef ASSERT\n+        mem->dump();\n+#endif\n+        ShouldNotReachHere();\n+      }\n+      if (wq.test_set(mem->_idx)) {\n+        return;\n+      }\n+    }\n+    \/\/ Replace memory input and re-compute ctrl to move the check out of the loop\n+    _igvn.replace_input_of(n, 1, mem);\n+    set_ctrl_and_loop(n, get_early_ctrl(n));\n+    Node* bol = n->unique_out();\n+    set_ctrl_and_loop(bol, get_early_ctrl(bol));\n+  }\n+}\n+\n@@ -1121,0 +1180,6 @@\n+\n+  if (n->isa_FlatArrayCheck()) {\n+    move_flat_array_check_out_of_loop(n);\n+    return n;\n+  }\n+\n@@ -1400,0 +1465,98 @@\n+bool PhaseIdealLoop::flat_array_element_type_check(Node *n) {\n+  \/\/ If the CmpP is a subtype check for a value that has just been\n+  \/\/ loaded from an array, the subtype check guarantees the value\n+  \/\/ can't be stored in a flat array and the load of the value\n+  \/\/ happens with a flat array check then: push the type check\n+  \/\/ through the phi of the flat array check. This needs special\n+  \/\/ logic because the subtype check's input is not a phi but a\n+  \/\/ LoadKlass that must first be cloned through the phi.\n+  if (n->Opcode() != Op_CmpP) {\n+    return false;\n+  }\n+\n+  Node* klassptr = n->in(1);\n+  Node* klasscon = n->in(2);\n+\n+  if (klassptr->is_DecodeNarrowPtr()) {\n+    klassptr = klassptr->in(1);\n+  }\n+\n+  if (klassptr->Opcode() != Op_LoadKlass && klassptr->Opcode() != Op_LoadNKlass) {\n+    return false;\n+  }\n+\n+  if (!klasscon->is_Con()) {\n+    return false;\n+  }\n+\n+  Node* addr = klassptr->in(MemNode::Address);\n+\n+  if (!addr->is_AddP()) {\n+    return false;\n+  }\n+\n+  intptr_t offset;\n+  Node* obj = AddPNode::Ideal_base_and_offset(addr, &_igvn, offset);\n+\n+  if (obj == nullptr) {\n+    return false;\n+  }\n+\n+  assert(obj != nullptr && addr->in(AddPNode::Base) == addr->in(AddPNode::Address), \"malformed AddP?\");\n+  if (obj->Opcode() == Op_CastPP) {\n+    obj = obj->in(1);\n+  }\n+\n+  if (!obj->is_Phi()) {\n+    return false;\n+  }\n+\n+  Node* region = obj->in(0);\n+\n+  Node* phi = PhiNode::make_blank(region, n->in(1));\n+  for (uint i = 1; i < region->req(); i++) {\n+    Node* in = obj->in(i);\n+    Node* ctrl = region->in(i);\n+    if (addr->in(AddPNode::Base) != obj) {\n+      Node* cast = addr->in(AddPNode::Base);\n+      assert(cast->Opcode() == Op_CastPP && cast->in(0) != nullptr, \"inconsistent subgraph\");\n+      Node* cast_clone = cast->clone();\n+      cast_clone->set_req(0, ctrl);\n+      cast_clone->set_req(1, in);\n+      register_new_node(cast_clone, ctrl);\n+      const Type* tcast = cast_clone->Value(&_igvn);\n+      _igvn.set_type(cast_clone, tcast);\n+      cast_clone->as_Type()->set_type(tcast);\n+      in = cast_clone;\n+    }\n+    Node* addr_clone = addr->clone();\n+    addr_clone->set_req(AddPNode::Base, in);\n+    addr_clone->set_req(AddPNode::Address, in);\n+    register_new_node(addr_clone, ctrl);\n+    _igvn.set_type(addr_clone, addr_clone->Value(&_igvn));\n+    Node* klassptr_clone = klassptr->clone();\n+    klassptr_clone->set_req(2, addr_clone);\n+    register_new_node(klassptr_clone, ctrl);\n+    _igvn.set_type(klassptr_clone, klassptr_clone->Value(&_igvn));\n+    if (klassptr != n->in(1)) {\n+      Node* decode = n->in(1);\n+      assert(decode->is_DecodeNarrowPtr(), \"inconsistent subgraph\");\n+      Node* decode_clone = decode->clone();\n+      decode_clone->set_req(1, klassptr_clone);\n+      register_new_node(decode_clone, ctrl);\n+      _igvn.set_type(decode_clone, decode_clone->Value(&_igvn));\n+      klassptr_clone = decode_clone;\n+    }\n+    phi->set_req(i, klassptr_clone);\n+  }\n+  register_new_node(phi, region);\n+  Node* orig = n->in(1);\n+  _igvn.replace_input_of(n, 1, phi);\n+  split_if_with_blocks_post(n);\n+  if (n->outcnt() != 0) {\n+    _igvn.replace_input_of(n, 1, orig);\n+    _igvn.remove_dead_node(phi);\n+  }\n+  return true;\n+}\n+\n@@ -1406,0 +1569,4 @@\n+  if (flat_array_element_type_check(n)) {\n+    return;\n+  }\n+\n@@ -1556,0 +1723,5 @@\n+\n+  \/\/ Remove multiple allocations of the same inline type\n+  if (n->is_InlineType()) {\n+    n->as_InlineType()->remove_redundant_allocations(this);\n+  }\n@@ -2047,1 +2219,9 @@\n-  Node *sample_cmp = sample_bool->in(1);\n+  Node* sample_cmp = sample_bool->in(1);\n+  const Type* t = Type::TOP;\n+  const TypePtr* at = nullptr;\n+  if (sample_cmp->is_FlatArrayCheck()) {\n+    \/\/ Left input of a FlatArrayCheckNode is memory, set the (adr) type of the phi accordingly\n+    assert(sample_cmp->in(1)->bottom_type() == Type::MEMORY, \"unexpected input type\");\n+    t = Type::MEMORY;\n+    at = TypeRawPtr::BOTTOM;\n+  }\n@@ -2050,1 +2230,1 @@\n-  PhiNode *phi1 = new PhiNode(phi->in(0), Type::TOP);\n+  PhiNode *phi1 = new PhiNode(phi->in(0), t, at);\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":182,"deletions":2,"binary":false,"changes":184,"status":"modified"},{"patch":"@@ -418,0 +418,16 @@\n+  if (tp->isa_aryptr()) {\n+    \/\/ In the case of a flat inline type array, each field has its\n+    \/\/ own slice so we need to extract the field being accessed from\n+    \/\/ the address computation\n+    if (offset == Type::OffsetBot) {\n+      Node* base;\n+      Node* index;\n+      const MachOper* oper = memory_inputs(base, index);\n+      if (oper != (MachOper*)-1) {\n+        offset = oper->constant_disp();\n+        return tp->is_aryptr()->add_field_offset_and_offset(offset)->add_offset(Type::OffsetBot);\n+      }\n+    }\n+    return tp->is_aryptr()->add_field_offset_and_offset(offset);\n+  }\n+\n@@ -482,0 +498,5 @@\n+  \/\/ Never rematerialize CastI2N because it might \"hide\" narrow oops from a safepoint\n+  if (ideal_Opcode() == Op_CastI2N) {\n+    return false;\n+  }\n+\n@@ -711,2 +732,2 @@\n-const Type *MachCallNode::bottom_type() const { return tf()->range(); }\n-const Type* MachCallNode::Value(PhaseGVN* phase) const { return tf()->range(); }\n+const Type *MachCallNode::bottom_type() const { return tf()->range_cc(); }\n+const Type* MachCallNode::Value(PhaseGVN* phase) const { return tf()->range_cc(); }\n@@ -723,2 +744,1 @@\n-#ifndef _LP64\n-  if (tf()->range()->cnt() == TypeFunc::Parms) {\n+  if (tf()->range_sig()->cnt() == TypeFunc::Parms) {\n@@ -740,1 +760,0 @@\n-#endif\n@@ -746,1 +765,1 @@\n-  const TypeTuple *r = tf()->range();\n+  const TypeTuple *r = tf()->range_sig();\n@@ -751,0 +770,4 @@\n+bool MachCallNode::returns_scalarized() const {\n+  return tf()->returns_inline_type_as_fields();\n+}\n+\n@@ -755,1 +778,6 @@\n-  if (idx < tf()->domain()->cnt()) {\n+  if (entry_point() == nullptr && idx == TypeFunc::Parms) {\n+    \/\/ Null entry point is a special cast where the target of the call\n+    \/\/ is in a register.\n+    return MachNode::in_RegMask(idx);\n+  }\n+  if (idx < tf()->domain_sig()->cnt()) {\n@@ -788,1 +816,1 @@\n-  if (idx < tf()->domain()->cnt()) {\n+  if (idx < tf()->domain_cc()->cnt()) {\n","filename":"src\/hotspot\/share\/opto\/machnode.cpp","additions":36,"deletions":8,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -55,0 +57,1 @@\n+#include \"runtime\/stubRoutines.hpp\"\n@@ -84,12 +87,0 @@\n-void PhaseMacroExpand::migrate_outs(Node *old, Node *target) {\n-  assert(old != nullptr, \"sanity\");\n-  for (DUIterator_Fast imax, i = old->fast_outs(imax); i < imax; i++) {\n-    Node* use = old->fast_out(i);\n-    _igvn.rehash_node_delayed(use);\n-    imax -= replace_input(use, old, target);\n-    \/\/ back up iterator\n-    --i;\n-  }\n-  assert(old->outcnt() == 0, \"all uses must be deleted\");\n-}\n-\n@@ -158,1 +149,1 @@\n-  bs->eliminate_gc_barrier(this, p2x);\n+  bs->eliminate_gc_barrier(&_igvn, p2x);\n@@ -213,1 +204,1 @@\n-        int adr_offset = atype->offset();\n+        int adr_offset = atype->flat_offset();\n@@ -258,1 +249,1 @@\n-   } else if (mem->Opcode() == Op_StrInflatedCopy) {\n+    } else if (mem->Opcode() == Op_StrInflatedCopy) {\n@@ -303,1 +294,5 @@\n-      const TypePtr* adr_type = nullptr;\n+      Node* base = ac->in(ArrayCopyNode::Src);\n+      const TypeAryPtr* adr_type = _igvn.type(base)->is_aryptr();\n+      if (adr_type->is_flat()) {\n+        shift = adr_type->flat_log_elem_size();\n+      }\n@@ -306,2 +301,2 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(off);\n+        adr_type = _igvn.type(adr)->is_aryptr();\n+        assert(adr_type == _igvn.type(base)->is_aryptr()->add_field_offset_and_offset(off), \"incorrect address type\");\n@@ -311,1 +306,1 @@\n-          return value_from_mem(ac->in(TypeFunc::Memory), ctl, ft, ftype, adr_type->isa_oopptr(), alloc);\n+          return value_from_mem(ac->in(TypeFunc::Memory), ctl, ft, ftype, adr_type, alloc);\n@@ -314,0 +309,5 @@\n+        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n+          \/\/ Non constant offset in the array: we can't statically\n+          \/\/ determine the value\n+          return nullptr;\n+        }\n@@ -321,7 +321,5 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(Type::OffsetBot);\n-        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n-          \/\/ Non constant offset in the array: we can't statically\n-          \/\/ determine the value\n-          return nullptr;\n-        }\n+        \/\/ In the case of a flat inline type array, each field has its\n+        \/\/ own slice so we need to extract the field being accessed from\n+        \/\/ the address computation\n+        adr_type = adr_type->add_field_offset_and_offset(offset)->add_offset(Type::OffsetBot)->is_aryptr();\n+        adr = _igvn.transform(new CastPPNode(ctl, adr, adr_type));\n@@ -338,0 +336,1 @@\n+      assert(res->isa_DecodeN(), \"should be narrow oop\");\n@@ -353,1 +352,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flat_offset();\n@@ -389,1 +388,1 @@\n-    } else  {\n+    } else {\n@@ -393,1 +392,7 @@\n-        values.at_put(j, _igvn.zerocon(ft));\n+        Node* default_value = alloc->in(AllocateNode::DefaultValue);\n+        if (default_value != nullptr) {\n+          values.at_put(j, default_value);\n+        } else {\n+          assert(alloc->in(AllocateNode::RawDefaultValue) == nullptr, \"default value may not be null\");\n+          values.at_put(j, _igvn.zerocon(ft));\n+        }\n@@ -413,1 +418,7 @@\n-        values.at_put(j, _igvn.zerocon(ft));\n+        Node* default_value = alloc->in(AllocateNode::DefaultValue);\n+        if (default_value != nullptr) {\n+          values.at_put(j, default_value);\n+        } else {\n+          assert(alloc->in(AllocateNode::RawDefaultValue) == nullptr, \"default value may not be null\");\n+          values.at_put(j, _igvn.zerocon(ft));\n+        }\n@@ -461,1 +472,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flat_offset();\n@@ -463,1 +474,0 @@\n-  Node *alloc_ctrl = alloc->in(TypeFunc::Control);\n@@ -480,1 +490,1 @@\n-        done = true; \/\/ Something go wrong.\n+        done = true; \/\/ Something went wrong.\n@@ -490,1 +500,1 @@\n-             atype->is_known_instance_field() && atype->offset() == offset &&\n+             atype->is_known_instance_field() && atype->flat_offset() == offset &&\n@@ -523,0 +533,5 @@\n+      Node* default_value = alloc->in(AllocateNode::DefaultValue);\n+      if (default_value != nullptr) {\n+        return default_value;\n+      }\n+      assert(alloc->in(AllocateNode::RawDefaultValue) == nullptr, \"default value may not be null\");\n@@ -554,1 +569,1 @@\n-  \/\/ Something go wrong.\n+  \/\/ Something went wrong.\n@@ -558,0 +573,47 @@\n+\/\/ Search the last value stored into the inline type's fields (for flat arrays).\n+Node* PhaseMacroExpand::inline_type_from_mem(Node* mem, Node* ctl, ciInlineKlass* vk, const TypeAryPtr* adr_type, int offset, AllocateNode* alloc) {\n+  \/\/ Subtract the offset of the first field to account for the missing oop header\n+  offset -= vk->payload_offset();\n+  \/\/ Create a new InlineTypeNode and retrieve the field values from memory\n+  InlineTypeNode* vt = InlineTypeNode::make_uninitialized(_igvn, vk);\n+  transform_later(vt);\n+  for (int i = 0; i < vk->nof_declared_nonstatic_fields(); ++i) {\n+    ciType* field_type = vt->field_type(i);\n+    int field_offset = offset + vt->field_offset(i);\n+    Node* value = nullptr;\n+    if (vt->field_is_flat(i)) {\n+      \/\/ TODO 8341767 Fix this\n+      \/\/ assert(vt->field_is_null_free(i), \"Unexpected nullable flat field\");\n+      if (!vt->field_is_null_free(i)) {\n+        return nullptr;\n+      }\n+      value = inline_type_from_mem(mem, ctl, field_type->as_inline_klass(), adr_type, field_offset, alloc);\n+    } else {\n+      const Type* ft = Type::get_const_type(field_type);\n+      BasicType bt = type2field[field_type->basic_type()];\n+      if (UseCompressedOops && !is_java_primitive(bt)) {\n+        ft = ft->make_narrowoop();\n+        bt = T_NARROWOOP;\n+      }\n+      \/\/ Each inline type field has its own memory slice\n+      adr_type = adr_type->with_field_offset(field_offset);\n+      value = value_from_mem(mem, ctl, bt, ft, adr_type, alloc);\n+      if (value != nullptr && ft->isa_narrowoop()) {\n+        assert(UseCompressedOops, \"unexpected narrow oop\");\n+        if (value->is_EncodeP()) {\n+          value = value->in(1);\n+        } else {\n+          value = transform_later(new DecodeNNode(value, value->get_ptr_type()));\n+        }\n+      }\n+    }\n+    if (value != nullptr) {\n+      vt->set_field_value(i, value);\n+    } else {\n+      \/\/ We might have reached the TrackedInitializationLimit\n+      return nullptr;\n+    }\n+  }\n+  return vt;\n+}\n+\n@@ -567,0 +629,1 @@\n+  Unique_Node_List worklist;\n@@ -575,0 +638,1 @@\n+    worklist.push(res);\n@@ -591,1 +655,1 @@\n-  if (can_eliminate && res != nullptr) {\n+  while (can_eliminate && worklist.size() > 0) {\n@@ -593,2 +657,2 @@\n-    for (DUIterator_Fast jmax, j = res->fast_outs(jmax);\n-                               j < jmax && can_eliminate; j++) {\n+    res = worklist.pop();\n+    for (DUIterator_Fast jmax, j = res->fast_outs(jmax); j < jmax && can_eliminate; j++) {\n@@ -640,0 +704,1 @@\n+          assert(!res->is_Phi() || !res->as_Phi()->can_be_inline_type(), \"Inline type allocations should not have safepoint uses\");\n@@ -642,0 +707,23 @@\n+      } else if (use->is_InlineType() && use->as_InlineType()->get_oop() == res) {\n+        \/\/ Look at uses\n+        for (DUIterator_Fast kmax, k = use->fast_outs(kmax); k < kmax; k++) {\n+          Node* u = use->fast_out(k);\n+          if (u->is_InlineType()) {\n+            \/\/ Use in flat field can be eliminated\n+            InlineTypeNode* vt = u->as_InlineType();\n+            for (uint i = 0; i < vt->field_count(); ++i) {\n+              if (vt->field_value(i) == use && !vt->field_is_flat(i)) {\n+                can_eliminate = false; \/\/ Use in non-flat field\n+                break;\n+              }\n+            }\n+          } else {\n+            \/\/ Add other uses to the worklist to process individually\n+            worklist.push(use);\n+          }\n+        }\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ Store to mark word of inline type larval buffer\n+        assert(res_type->is_inlinetypeptr(), \"Unexpected store to mark word\");\n+      } else if (res_type->is_inlinetypeptr() && (use->Opcode() == Op_MemBarRelease || use->Opcode() == Op_MemBarStoreStore)) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n@@ -664,0 +752,3 @@\n+      } else {\n+        assert(use->Opcode() == Op_CastP2X, \"should be\");\n+        assert(!use->has_out_with(Op_OrL), \"should have been removed because oop is never null\");\n@@ -676,1 +767,1 @@\n-    } else if (alloc->_is_scalar_replaceable) {\n+    } else {\n@@ -746,1 +837,2 @@\n-SafePointScalarObjectNode* PhaseMacroExpand::create_scalarized_object_description(AllocateNode *alloc, SafePointNode* sfpt) {\n+SafePointScalarObjectNode* PhaseMacroExpand::create_scalarized_object_description(AllocateNode *alloc, SafePointNode* sfpt,\n+                                                                                  Unique_Node_List* value_worklist) {\n@@ -778,0 +870,14 @@\n+      if (res_type->is_flat()) {\n+        \/\/ Flat inline type array\n+        element_size = res_type->is_aryptr()->flat_elem_size();\n+      }\n+    }\n+\n+    if (res->bottom_type()->is_inlinetypeptr()) {\n+      \/\/ Nullable inline types have an IsInit field which is added to the safepoint when scalarizing them (see\n+      \/\/ InlineTypeNode::make_scalar_in_safepoint()). When having circular inline types, we stop scalarizing at depth 1\n+      \/\/ to avoid an endless recursion. Therefore, we do not have a SafePointScalarObjectNode node here, yet.\n+      \/\/ We are about to create a SafePointScalarObjectNode as if this is a normal object. Add an additional int input\n+      \/\/ with value 1 which sets IsInit to true to indicate that the object is always non-null. This input is checked\n+      \/\/ later in PhaseOutput::filLocArray() for inline types.\n+      sfpt->add_req(_igvn.intcon(1));\n@@ -794,0 +900,1 @@\n+      assert(!field->is_flat(), \"flat inline type fields should not have safepoint uses\");\n@@ -819,3 +926,9 @@\n-    const TypeOopPtr *field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n-\n-    Node *field_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, field_type, field_addr_type, alloc);\n+    Node* field_val = nullptr;\n+    const TypeOopPtr* field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n+    if (res_type->is_flat()) {\n+      ciInlineKlass* inline_klass = res_type->is_aryptr()->elem()->inline_klass();\n+      assert(inline_klass->flat_in_array(), \"must be flat in array\");\n+      field_val = inline_type_from_mem(sfpt->memory(), sfpt->control(), inline_klass, field_addr_type->isa_aryptr(), 0, alloc);\n+    } else {\n+      field_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, field_type, field_addr_type, alloc);\n+    }\n@@ -858,1 +971,1 @@\n-      } else {\n+      } else if (!field_val->is_InlineType()) {\n@@ -862,0 +975,13 @@\n+\n+    \/\/ Keep track of inline types to scalarize them later\n+    if (field_val->is_InlineType()) {\n+      value_worklist->push(field_val);\n+    } else if (field_val->is_Phi()) {\n+      PhiNode* phi = field_val->as_Phi();\n+      \/\/ Eagerly replace inline type phis now since we could be removing an inline type allocation where we must\n+      \/\/ scalarize all its fields in safepoints.\n+      field_val = phi->try_push_inline_types_down(&_igvn, true);\n+      if (field_val->is_InlineType()) {\n+        value_worklist->push(field_val);\n+      }\n+    }\n@@ -875,0 +1001,4 @@\n+  const TypeOopPtr* res_type = nullptr;\n+  if (res != nullptr) { \/\/ Could be null when there are no users\n+    res_type = _igvn.type(res)->isa_oopptr();\n+  }\n@@ -877,0 +1007,3 @@\n+  assert(safepoints.length() == 0 || !res_type->is_inlinetypeptr() || C->has_circular_inline_type(),\n+         \"Inline type allocations should have been scalarized earlier\");\n+  Unique_Node_List value_worklist;\n@@ -879,1 +1012,1 @@\n-    SafePointScalarObjectNode* sobj = create_scalarized_object_description(alloc, sfpt);\n+    SafePointScalarObjectNode* sobj = create_scalarized_object_description(alloc, sfpt, &value_worklist);\n@@ -895,1 +1028,8 @@\n-\n+  \/\/ Scalarize inline types that were added to the safepoint.\n+  \/\/ Don't allow linking a constant oop (if available) for flat array elements\n+  \/\/ because Deoptimization::reassign_flat_array_elements needs field values.\n+  bool allow_oop = (res_type != nullptr) && !res_type->is_flat();\n+  for (uint i = 0; i < value_worklist.size(); ++i) {\n+    InlineTypeNode* vt = value_worklist.at(i)->as_InlineType();\n+    vt->make_scalar_in_safepoints(&_igvn, allow_oop);\n+  }\n@@ -911,1 +1051,2 @@\n-void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc) {\n+void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc, bool inline_alloc) {\n+  Unique_Node_List worklist;\n@@ -914,0 +1055,4 @@\n+    worklist.push(res);\n+  }\n+  while (worklist.size() > 0) {\n+    res = worklist.pop();\n@@ -923,10 +1068,7 @@\n-#ifdef ASSERT\n-            \/\/ Verify that there is no dependent MemBarVolatile nodes,\n-            \/\/ they should be removed during IGVN, see MemBarNode::Ideal().\n-            for (DUIterator_Fast pmax, p = n->fast_outs(pmax);\n-                                       p < pmax; p++) {\n-              Node* mb = n->fast_out(p);\n-              assert(mb->is_Initialize() || !mb->is_MemBar() ||\n-                     mb->req() <= MemBarNode::Precedent ||\n-                     mb->in(MemBarNode::Precedent) != n,\n-                     \"MemBarVolatile should be eliminated for non-escaping object\");\n+            for (DUIterator_Fast pmax, p = n->fast_outs(pmax); p < pmax; p++) {\n+              MemBarNode* mb = n->fast_out(p)->isa_MemBar();\n+              if (mb != nullptr && mb->req() <= MemBarNode::Precedent && mb->in(MemBarNode::Precedent) == n) {\n+                \/\/ MemBarVolatiles should have been removed by MemBarNode::Ideal() for non-inline allocations\n+                assert(inline_alloc, \"MemBarVolatile should be eliminated for non-escaping object\");\n+                mb->remove(&_igvn);\n+              }\n@@ -934,1 +1076,0 @@\n-#endif\n@@ -958,2 +1099,1 @@\n-          CallProjections callprojs;\n-          ac->extract_projections(&callprojs, true);\n+          CallProjections* callprojs = ac->extract_projections(true);\n@@ -961,3 +1101,3 @@\n-          _igvn.replace_node(callprojs.fallthrough_ioproj, ac->in(TypeFunc::I_O));\n-          _igvn.replace_node(callprojs.fallthrough_memproj, ac->in(TypeFunc::Memory));\n-          _igvn.replace_node(callprojs.fallthrough_catchproj, ac->in(TypeFunc::Control));\n+          _igvn.replace_node(callprojs->fallthrough_ioproj, ac->in(TypeFunc::I_O));\n+          _igvn.replace_node(callprojs->fallthrough_memproj, ac->in(TypeFunc::Memory));\n+          _igvn.replace_node(callprojs->fallthrough_catchproj, ac->in(TypeFunc::Control));\n@@ -980,0 +1120,23 @@\n+      } else if (use->is_InlineType()) {\n+        assert(use->as_InlineType()->get_oop() == res, \"unexpected inline type ptr use\");\n+        \/\/ Cut off oop input and remove known instance id from type\n+        _igvn.rehash_node_delayed(use);\n+        use->as_InlineType()->set_oop(_igvn, _igvn.zerocon(T_OBJECT));\n+        const TypeOopPtr* toop = _igvn.type(use)->is_oopptr()->cast_to_instance_id(TypeOopPtr::InstanceBot);\n+        _igvn.set_type(use, toop);\n+        use->as_InlineType()->set_type(toop);\n+        \/\/ Process users\n+        for (DUIterator_Fast kmax, k = use->fast_outs(kmax); k < kmax; k++) {\n+          Node* u = use->fast_out(k);\n+          if (!u->is_InlineType()) {\n+            worklist.push(u);\n+          }\n+        }\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ Store to mark word of inline type larval buffer\n+        assert(inline_alloc, \"Unexpected store to mark word\");\n+        _igvn.replace_node(use, use->in(MemNode::Memory));\n+      } else if (use->Opcode() == Op_MemBarRelease || use->Opcode() == Op_MemBarStoreStore) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n+        assert(inline_alloc, \"Unexpected MemBarRelease\");\n+        use->as_MemBar()->remove(&_igvn);\n@@ -992,1 +1155,1 @@\n-  if (_callprojs.resproj != nullptr && _callprojs.resproj->outcnt() != 0) {\n+  if (_callprojs->resproj[0] != nullptr && _callprojs->resproj[0]->outcnt() != 0) {\n@@ -996,2 +1159,2 @@\n-    for (DUIterator_Fast jmax, j = _callprojs.resproj->fast_outs(jmax);  j < jmax; j++) {\n-      Node* use = _callprojs.resproj->fast_out(j);\n+    for (DUIterator_Fast jmax, j = _callprojs->resproj[0]->fast_outs(jmax);  j < jmax; j++) {\n+      Node* use = _callprojs->resproj[0]->fast_out(j);\n@@ -1004,3 +1167,3 @@\n-    for (DUIterator_Last jmin, j = _callprojs.resproj->last_outs(jmin); j >= jmin; ) {\n-      Node* use = _callprojs.resproj->last_out(j);\n-      uint oc1 = _callprojs.resproj->outcnt();\n+    for (DUIterator_Last jmin, j = _callprojs->resproj[0]->last_outs(jmin); j >= jmin; ) {\n+      Node* use = _callprojs->resproj[0]->last_out(j);\n+      uint oc1 = _callprojs->resproj[0]->outcnt();\n@@ -1017,1 +1180,1 @@\n-          assert(tmp == nullptr || tmp == _callprojs.fallthrough_catchproj, \"allocation control projection\");\n+          assert(tmp == nullptr || tmp == _callprojs->fallthrough_catchproj, \"allocation control projection\");\n@@ -1025,1 +1188,1 @@\n-            assert(mem->in(TypeFunc::Memory) == _callprojs.fallthrough_memproj, \"allocation memory projection\");\n+            assert(mem->in(TypeFunc::Memory) == _callprojs->fallthrough_memproj, \"allocation memory projection\");\n@@ -1027,1 +1190,1 @@\n-            assert(mem == _callprojs.fallthrough_memproj, \"allocation memory projection\");\n+            assert(mem == _callprojs->fallthrough_memproj, \"allocation memory projection\");\n@@ -1032,0 +1195,4 @@\n+      } else if (use->Opcode() == Op_MemBarStoreStore) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n+        assert(inline_alloc, \"Unexpected MemBarStoreStore\");\n+        use->as_MemBar()->remove(&_igvn);\n@@ -1035,1 +1202,1 @@\n-      j -= (oc1 - _callprojs.resproj->outcnt());\n+      j -= (oc1 - _callprojs->resproj[0]->outcnt());\n@@ -1038,2 +1205,2 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, alloc->in(TypeFunc::Control));\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, alloc->in(TypeFunc::Control));\n@@ -1041,2 +1208,2 @@\n-  if (_callprojs.fallthrough_memproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_memproj, alloc->in(TypeFunc::Memory));\n+  if (_callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_memproj, alloc->in(TypeFunc::Memory));\n@@ -1044,2 +1211,2 @@\n-  if (_callprojs.catchall_memproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_memproj, C->top());\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_memproj, C->top());\n@@ -1047,2 +1214,2 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -1050,2 +1217,2 @@\n-  if (_callprojs.catchall_ioproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_ioproj, C->top());\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_ioproj, C->top());\n@@ -1053,2 +1220,2 @@\n-  if (_callprojs.catchall_catchproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_catchproj, C->top());\n+  if (_callprojs->catchall_catchproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_catchproj, C->top());\n@@ -1064,1 +1231,1 @@\n-  if (!EliminateAllocations || !alloc->_is_non_escaping) {\n+  if (!EliminateAllocations) {\n@@ -1069,1 +1236,8 @@\n-  Node* res = alloc->result_cast();\n+\n+  \/\/ Attempt to eliminate inline type buffer allocations\n+  \/\/ regardless of usage and escape\/replaceable status.\n+  bool inline_alloc = tklass->isa_instklassptr() &&\n+                      tklass->is_instklassptr()->instance_klass()->is_inlinetype();\n+  if (!alloc->_is_non_escaping && !inline_alloc) {\n+    return false;\n+  }\n@@ -1072,1 +1246,2 @@\n-  bool boxing_alloc = C->eliminate_boxing() &&\n+  Node* res = alloc->result_cast();\n+  bool boxing_alloc = (res == nullptr) && C->eliminate_boxing() &&\n@@ -1075,1 +1250,1 @@\n-  if (!alloc->_is_scalar_replaceable && (!boxing_alloc || (res != nullptr))) {\n+  if (!alloc->_is_scalar_replaceable && !boxing_alloc && !inline_alloc) {\n@@ -1079,1 +1254,1 @@\n-  alloc->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = alloc->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1087,1 +1262,1 @@\n-    assert(res == nullptr, \"sanity\");\n+    assert(res == nullptr || inline_alloc, \"sanity\");\n@@ -1092,0 +1267,2 @@\n+      assert(!inline_alloc || C->has_circular_inline_type(),\n+             \"Inline type allocations should have been scalarized earlier\");\n@@ -1112,1 +1289,1 @@\n-  process_users_of_allocation(alloc);\n+  process_users_of_allocation(alloc, inline_alloc);\n@@ -1134,1 +1311,1 @@\n-  boxing->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = boxing->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1136,1 +1313,1 @@\n-  const TypeTuple* r = boxing->tf()->range();\n+  const TypeTuple* r = boxing->tf()->range_sig();\n@@ -1322,1 +1499,1 @@\n-    Node *toobig_true = new IfTrueNode( toobig_iff );\n+    Node* toobig_true = new IfTrueNode(toobig_iff);\n@@ -1325,1 +1502,1 @@\n-    toobig_false = new IfFalseNode( toobig_iff );\n+    toobig_false = new IfFalseNode(toobig_iff);\n@@ -1364,0 +1541,1 @@\n+\n@@ -1421,0 +1599,3 @@\n+  } else {\n+    \/\/ Let the runtime know if this is a larval allocation\n+    call->init_req(TypeFunc::Parms+1, _igvn.intcon(alloc->_larval));\n@@ -1452,1 +1633,1 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1458,2 +1639,2 @@\n-  if (expand_fast_path && _callprojs.fallthrough_memproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_memproj, result_phi_rawmem);\n+  if (expand_fast_path && _callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_memproj, result_phi_rawmem);\n@@ -1463,4 +1644,4 @@\n-  if (_callprojs.catchall_memproj != nullptr ) {\n-    if (_callprojs.fallthrough_memproj == nullptr) {\n-      _callprojs.fallthrough_memproj = new ProjNode(call, TypeFunc::Memory);\n-      transform_later(_callprojs.fallthrough_memproj);\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    if (_callprojs->fallthrough_memproj == nullptr) {\n+      _callprojs->fallthrough_memproj = new ProjNode(call, TypeFunc::Memory);\n+      transform_later(_callprojs->fallthrough_memproj);\n@@ -1468,2 +1649,2 @@\n-    migrate_outs(_callprojs.catchall_memproj, _callprojs.fallthrough_memproj);\n-    _igvn.remove_dead_node(_callprojs.catchall_memproj);\n+    _igvn.replace_in_uses(_callprojs->catchall_memproj, _callprojs->fallthrough_memproj);\n+    _igvn.remove_dead_node(_callprojs->catchall_memproj);\n@@ -1477,2 +1658,2 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_ioproj, result_phi_i_o);\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_ioproj, result_phi_i_o);\n@@ -1482,4 +1663,4 @@\n-  if (_callprojs.catchall_ioproj != nullptr ) {\n-    if (_callprojs.fallthrough_ioproj == nullptr) {\n-      _callprojs.fallthrough_ioproj = new ProjNode(call, TypeFunc::I_O);\n-      transform_later(_callprojs.fallthrough_ioproj);\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    if (_callprojs->fallthrough_ioproj == nullptr) {\n+      _callprojs->fallthrough_ioproj = new ProjNode(call, TypeFunc::I_O);\n+      transform_later(_callprojs->fallthrough_ioproj);\n@@ -1487,2 +1668,2 @@\n-    migrate_outs(_callprojs.catchall_ioproj, _callprojs.fallthrough_ioproj);\n-    _igvn.remove_dead_node(_callprojs.catchall_ioproj);\n+    _igvn.replace_in_uses(_callprojs->catchall_ioproj, _callprojs->fallthrough_ioproj);\n+    _igvn.remove_dead_node(_callprojs->catchall_ioproj);\n@@ -1507,2 +1688,2 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    ctrl = _callprojs.fallthrough_catchproj->clone();\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    ctrl = _callprojs->fallthrough_catchproj->clone();\n@@ -1510,1 +1691,1 @@\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, result_region);\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, result_region);\n@@ -1515,1 +1696,1 @@\n-  if (_callprojs.resproj == nullptr) {\n+  if (_callprojs->resproj[0] == nullptr) {\n@@ -1519,1 +1700,1 @@\n-    slow_result = _callprojs.resproj->clone();\n+    slow_result = _callprojs->resproj[0]->clone();\n@@ -1521,1 +1702,1 @@\n-    _igvn.replace_node(_callprojs.resproj, result_phi_rawoop);\n+    _igvn.replace_node(_callprojs->resproj[0], result_phi_rawoop);\n@@ -1531,1 +1712,1 @@\n-  result_phi_rawmem->init_req(slow_result_path, _callprojs.fallthrough_memproj);\n+  result_phi_rawmem->init_req(slow_result_path, _callprojs->fallthrough_memproj);\n@@ -1543,4 +1724,4 @@\n-  alloc->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  if (_callprojs.resproj != nullptr) {\n-    for (DUIterator_Fast imax, i = _callprojs.resproj->fast_outs(imax); i < imax; i++) {\n-      Node* use = _callprojs.resproj->fast_out(i);\n+  _callprojs = alloc->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  if (_callprojs->resproj[0] != nullptr) {\n+    for (DUIterator_Fast imax, i = _callprojs->resproj[0]->fast_outs(imax); i < imax; i++) {\n+      Node* use = _callprojs->resproj[0]->fast_out(i);\n@@ -1551,2 +1732,2 @@\n-    assert(_callprojs.resproj->outcnt() == 0, \"all uses must be deleted\");\n-    _igvn.remove_dead_node(_callprojs.resproj);\n+    assert(_callprojs->resproj[0]->outcnt() == 0, \"all uses must be deleted\");\n+    _igvn.remove_dead_node(_callprojs->resproj[0]);\n@@ -1554,3 +1735,3 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_catchproj, ctrl);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_catchproj);\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_catchproj, ctrl);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_catchproj);\n@@ -1558,3 +1739,3 @@\n-  if (_callprojs.catchall_catchproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_catchproj);\n-    _callprojs.catchall_catchproj->set_req(0, top());\n+  if (_callprojs->catchall_catchproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_catchproj);\n+    _callprojs->catchall_catchproj->set_req(0, top());\n@@ -1562,2 +1743,2 @@\n-  if (_callprojs.fallthrough_proj != nullptr) {\n-    Node* catchnode = _callprojs.fallthrough_proj->unique_ctrl_out();\n+  if (_callprojs->fallthrough_proj != nullptr) {\n+    Node* catchnode = _callprojs->fallthrough_proj->unique_ctrl_out();\n@@ -1565,1 +1746,1 @@\n-    _igvn.remove_dead_node(_callprojs.fallthrough_proj);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_proj);\n@@ -1567,3 +1748,3 @@\n-  if (_callprojs.fallthrough_memproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_memproj, mem);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_memproj);\n+  if (_callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_memproj, mem);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_memproj);\n@@ -1571,3 +1752,3 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_ioproj, i_o);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_ioproj);\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_ioproj, i_o);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_ioproj);\n@@ -1575,3 +1756,3 @@\n-  if (_callprojs.catchall_memproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_memproj);\n-    _callprojs.catchall_memproj->set_req(0, top());\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_memproj);\n+    _callprojs->catchall_memproj->set_req(0, top());\n@@ -1579,3 +1760,3 @@\n-  if (_callprojs.catchall_ioproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_ioproj);\n-    _callprojs.catchall_ioproj->set_req(0, top());\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_ioproj);\n+    _callprojs->catchall_ioproj->set_req(0, top());\n@@ -1699,5 +1880,4 @@\n-Node*\n-PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n-                                    Node* control, Node* rawmem, Node* object,\n-                                    Node* klass_node, Node* length,\n-                                    Node* size_in_bytes) {\n+Node* PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n+                                          Node* control, Node* rawmem, Node* object,\n+                                          Node* klass_node, Node* length,\n+                                          Node* size_in_bytes) {\n@@ -1706,1 +1886,1 @@\n-  Node* mark_node = alloc->make_ideal_mark(&_igvn, object, control, rawmem);\n+  Node* mark_node = alloc->make_ideal_mark(&_igvn, control, rawmem);\n@@ -1744,0 +1924,2 @@\n+                                            alloc->in(AllocateNode::DefaultValue),\n+                                            alloc->in(AllocateNode::RawDefaultValue),\n@@ -2151,1 +2333,1 @@\n-  alock->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = alock->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -2155,2 +2337,2 @@\n-         _callprojs.fallthrough_proj != nullptr &&\n-         _callprojs.fallthrough_memproj != nullptr,\n+         _callprojs->fallthrough_proj != nullptr &&\n+         _callprojs->fallthrough_memproj != nullptr,\n@@ -2159,2 +2341,2 @@\n-  Node* fallthroughproj = _callprojs.fallthrough_proj;\n-  Node* memproj_fallthrough = _callprojs.fallthrough_memproj;\n+  Node* fallthroughproj = _callprojs->fallthrough_proj;\n+  Node* memproj_fallthrough = _callprojs->fallthrough_memproj;\n@@ -2231,1 +2413,1 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -2237,2 +2419,2 @@\n-  assert(_callprojs.fallthrough_ioproj == nullptr && _callprojs.catchall_ioproj == nullptr &&\n-         _callprojs.catchall_memproj == nullptr && _callprojs.catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n+  assert(_callprojs->fallthrough_ioproj == nullptr && _callprojs->catchall_ioproj == nullptr &&\n+         _callprojs->catchall_memproj == nullptr && _callprojs->catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n@@ -2243,1 +2425,1 @@\n-  Node *slow_ctrl = _callprojs.fallthrough_proj->clone();\n+  Node *slow_ctrl = _callprojs->fallthrough_proj->clone();\n@@ -2245,2 +2427,2 @@\n-  _igvn.hash_delete(_callprojs.fallthrough_proj);\n-  _callprojs.fallthrough_proj->disconnect_inputs(C);\n+  _igvn.hash_delete(_callprojs->fallthrough_proj);\n+  _callprojs->fallthrough_proj->disconnect_inputs(C);\n@@ -2250,1 +2432,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_proj, region);\n+  _igvn.replace_node(_callprojs->fallthrough_proj, region);\n@@ -2258,1 +2440,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, mem_phi);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, mem_phi);\n@@ -2291,3 +2473,3 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  assert(_callprojs.fallthrough_ioproj == nullptr && _callprojs.catchall_ioproj == nullptr &&\n-         _callprojs.catchall_memproj == nullptr && _callprojs.catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  assert(_callprojs->fallthrough_ioproj == nullptr && _callprojs->catchall_ioproj == nullptr &&\n+         _callprojs->catchall_memproj == nullptr && _callprojs->catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n@@ -2299,1 +2481,1 @@\n-  Node *slow_ctrl = _callprojs.fallthrough_proj->clone();\n+  Node *slow_ctrl = _callprojs->fallthrough_proj->clone();\n@@ -2301,2 +2483,2 @@\n-  _igvn.hash_delete(_callprojs.fallthrough_proj);\n-  _callprojs.fallthrough_proj->disconnect_inputs(C);\n+  _igvn.hash_delete(_callprojs->fallthrough_proj);\n+  _callprojs->fallthrough_proj->disconnect_inputs(C);\n@@ -2306,1 +2488,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_proj, region);\n+  _igvn.replace_node(_callprojs->fallthrough_proj, region);\n@@ -2313,1 +2495,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, mem_phi);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, mem_phi);\n@@ -2316,0 +2498,211 @@\n+\/\/ An inline type might be returned from the call but we don't know its\n+\/\/ type. Either we get a buffered inline type (and nothing needs to be done)\n+\/\/ or one of the values being returned is the klass of the inline type\n+\/\/ and we need to allocate an inline type instance of that type and\n+\/\/ initialize it with other values being returned. In that case, we\n+\/\/ first try a fast path allocation and initialize the value with the\n+\/\/ inline klass's pack handler or we fall back to a runtime call.\n+void PhaseMacroExpand::expand_mh_intrinsic_return(CallStaticJavaNode* call) {\n+  assert(call->method()->is_method_handle_intrinsic(), \"must be a method handle intrinsic call\");\n+  Node* ret = call->proj_out_or_null(TypeFunc::Parms);\n+  if (ret == nullptr) {\n+    return;\n+  }\n+  const TypeFunc* tf = call->_tf;\n+  const TypeTuple* domain = OptoRuntime::store_inline_type_fields_Type()->domain_cc();\n+  const TypeFunc* new_tf = TypeFunc::make(tf->domain_sig(), tf->domain_cc(), tf->range_sig(), domain);\n+  call->_tf = new_tf;\n+  \/\/ Make sure the change of type is applied before projections are processed by igvn\n+  _igvn.set_type(call, call->Value(&_igvn));\n+  _igvn.set_type(ret, ret->Value(&_igvn));\n+\n+  \/\/ Before any new projection is added:\n+  CallProjections* projs = call->extract_projections(true, true);\n+\n+  \/\/ Create temporary hook nodes that will be replaced below.\n+  \/\/ Add an input to prevent hook nodes from being dead.\n+  Node* ctl = new Node(call);\n+  Node* mem = new Node(ctl);\n+  Node* io = new Node(ctl);\n+  Node* ex_ctl = new Node(ctl);\n+  Node* ex_mem = new Node(ctl);\n+  Node* ex_io = new Node(ctl);\n+  Node* res = new Node(ctl);\n+\n+  \/\/ Allocate a new buffered inline type only if a new one is not returned\n+  Node* cast = transform_later(new CastP2XNode(ctl, res));\n+  Node* mask = MakeConX(0x1);\n+  Node* masked = transform_later(new AndXNode(cast, mask));\n+  Node* cmp = transform_later(new CmpXNode(masked, mask));\n+  Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+  IfNode* allocation_iff = new IfNode(ctl, bol, PROB_MAX, COUNT_UNKNOWN);\n+  transform_later(allocation_iff);\n+  Node* allocation_ctl = transform_later(new IfTrueNode(allocation_iff));\n+  Node* no_allocation_ctl = transform_later(new IfFalseNode(allocation_iff));\n+  Node* no_allocation_res = transform_later(new CheckCastPPNode(no_allocation_ctl, res, TypeInstPtr::BOTTOM));\n+\n+  \/\/ Try to allocate a new buffered inline instance either from TLAB or eden space\n+  Node* needgc_ctrl = nullptr; \/\/ needgc means slowcase, i.e. allocation failed\n+  CallLeafNoFPNode* handler_call;\n+  const bool alloc_in_place = UseTLAB;\n+  if (alloc_in_place) {\n+    Node* fast_oop_ctrl = nullptr;\n+    Node* fast_oop_rawmem = nullptr;\n+    Node* mask2 = MakeConX(-2);\n+    Node* masked2 = transform_later(new AndXNode(cast, mask2));\n+    Node* rawklassptr = transform_later(new CastX2PNode(masked2));\n+    Node* klass_node = transform_later(new CheckCastPPNode(allocation_ctl, rawklassptr, TypeInstKlassPtr::OBJECT_OR_NULL));\n+    Node* layout_val = make_load(nullptr, mem, klass_node, in_bytes(Klass::layout_helper_offset()), TypeInt::INT, T_INT);\n+    Node* size_in_bytes = ConvI2X(layout_val);\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    Node* fast_oop = bs->obj_allocate(this, mem, allocation_ctl, size_in_bytes, io, needgc_ctrl,\n+                                      fast_oop_ctrl, fast_oop_rawmem,\n+                                      AllocateInstancePrefetchLines);\n+    \/\/ Allocation succeed, initialize buffered inline instance header firstly,\n+    \/\/ and then initialize its fields with an inline class specific handler\n+    Node* mark_node = makecon(TypeRawPtr::make((address)markWord::inline_type_prototype().value()));\n+    fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::mark_offset_in_bytes(), mark_node, T_ADDRESS);\n+    fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+    if (UseCompressedClassPointers) {\n+      fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::klass_gap_offset_in_bytes(), intcon(0), T_INT);\n+    }\n+    Node* fixed_block  = make_load(fast_oop_ctrl, fast_oop_rawmem, klass_node, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+    Node* pack_handler = make_load(fast_oop_ctrl, fast_oop_rawmem, fixed_block, in_bytes(InlineKlass::pack_handler_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+    handler_call = new CallLeafNoFPNode(OptoRuntime::pack_inline_type_Type(),\n+                                        nullptr,\n+                                        \"pack handler\",\n+                                        TypeRawPtr::BOTTOM);\n+    handler_call->init_req(TypeFunc::Control, fast_oop_ctrl);\n+    handler_call->init_req(TypeFunc::Memory, fast_oop_rawmem);\n+    handler_call->init_req(TypeFunc::I_O, top());\n+    handler_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+    handler_call->init_req(TypeFunc::ReturnAdr, top());\n+    handler_call->init_req(TypeFunc::Parms, pack_handler);\n+    handler_call->init_req(TypeFunc::Parms+1, fast_oop);\n+  } else {\n+    needgc_ctrl = allocation_ctl;\n+  }\n+\n+  \/\/ Allocation failed, fall back to a runtime call\n+  CallStaticJavaNode* slow_call = new CallStaticJavaNode(OptoRuntime::store_inline_type_fields_Type(),\n+                                                         StubRoutines::store_inline_type_fields_to_buf(),\n+                                                         \"store_inline_type_fields\",\n+                                                         TypePtr::BOTTOM);\n+  slow_call->init_req(TypeFunc::Control, needgc_ctrl);\n+  slow_call->init_req(TypeFunc::Memory, mem);\n+  slow_call->init_req(TypeFunc::I_O, io);\n+  slow_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+  slow_call->init_req(TypeFunc::ReturnAdr, call->in(TypeFunc::ReturnAdr));\n+  slow_call->init_req(TypeFunc::Parms, res);\n+\n+  Node* slow_ctl = transform_later(new ProjNode(slow_call, TypeFunc::Control));\n+  Node* slow_mem = transform_later(new ProjNode(slow_call, TypeFunc::Memory));\n+  Node* slow_io = transform_later(new ProjNode(slow_call, TypeFunc::I_O));\n+  Node* slow_res = transform_later(new ProjNode(slow_call, TypeFunc::Parms));\n+  Node* slow_catc = transform_later(new CatchNode(slow_ctl, slow_io, 2));\n+  Node* slow_norm = transform_later(new CatchProjNode(slow_catc, CatchProjNode::fall_through_index, CatchProjNode::no_handler_bci));\n+  Node* slow_excp = transform_later(new CatchProjNode(slow_catc, CatchProjNode::catch_all_index,    CatchProjNode::no_handler_bci));\n+\n+  Node* ex_r = new RegionNode(3);\n+  Node* ex_mem_phi = new PhiNode(ex_r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* ex_io_phi = new PhiNode(ex_r, Type::ABIO);\n+  ex_r->init_req(1, slow_excp);\n+  ex_mem_phi->init_req(1, slow_mem);\n+  ex_io_phi->init_req(1, slow_io);\n+  ex_r->init_req(2, ex_ctl);\n+  ex_mem_phi->init_req(2, ex_mem);\n+  ex_io_phi->init_req(2, ex_io);\n+  transform_later(ex_r);\n+  transform_later(ex_mem_phi);\n+  transform_later(ex_io_phi);\n+\n+  \/\/ We don't know how many values are returned. This assumes the\n+  \/\/ worst case, that all available registers are used.\n+  for (uint i = TypeFunc::Parms+1; i < domain->cnt(); i++) {\n+    if (domain->field_at(i) == Type::HALF) {\n+      slow_call->init_req(i, top());\n+      if (alloc_in_place) {\n+        handler_call->init_req(i+1, top());\n+      }\n+      continue;\n+    }\n+    Node* proj = transform_later(new ProjNode(call, i));\n+    slow_call->init_req(i, proj);\n+    if (alloc_in_place) {\n+      handler_call->init_req(i+1, proj);\n+    }\n+  }\n+  \/\/ We can safepoint at that new call\n+  slow_call->copy_call_debug_info(&_igvn, call);\n+  transform_later(slow_call);\n+  if (alloc_in_place) {\n+    transform_later(handler_call);\n+  }\n+\n+  Node* fast_ctl = nullptr;\n+  Node* fast_res = nullptr;\n+  MergeMemNode* fast_mem = nullptr;\n+  if (alloc_in_place) {\n+    fast_ctl = transform_later(new ProjNode(handler_call, TypeFunc::Control));\n+    Node* rawmem = transform_later(new ProjNode(handler_call, TypeFunc::Memory));\n+    fast_res = transform_later(new ProjNode(handler_call, TypeFunc::Parms));\n+    fast_mem = MergeMemNode::make(mem);\n+    fast_mem->set_memory_at(Compile::AliasIdxRaw, rawmem);\n+    transform_later(fast_mem);\n+  }\n+\n+  Node* r = new RegionNode(alloc_in_place ? 4 : 3);\n+  Node* mem_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* io_phi = new PhiNode(r, Type::ABIO);\n+  Node* res_phi = new PhiNode(r, TypeInstPtr::BOTTOM);\n+  r->init_req(1, no_allocation_ctl);\n+  mem_phi->init_req(1, mem);\n+  io_phi->init_req(1, io);\n+  res_phi->init_req(1, no_allocation_res);\n+  r->init_req(2, slow_norm);\n+  mem_phi->init_req(2, slow_mem);\n+  io_phi->init_req(2, slow_io);\n+  res_phi->init_req(2, slow_res);\n+  if (alloc_in_place) {\n+    r->init_req(3, fast_ctl);\n+    mem_phi->init_req(3, fast_mem);\n+    io_phi->init_req(3, io);\n+    res_phi->init_req(3, fast_res);\n+  }\n+  transform_later(r);\n+  transform_later(mem_phi);\n+  transform_later(io_phi);\n+  transform_later(res_phi);\n+\n+  \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+  \/\/ store that would make this buffer accessible by other threads.\n+  MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxBot);\n+  transform_later(mb);\n+  mb->init_req(TypeFunc::Memory, mem_phi);\n+  mb->init_req(TypeFunc::Control, r);\n+  r = new ProjNode(mb, TypeFunc::Control);\n+  transform_later(r);\n+  mem_phi = new ProjNode(mb, TypeFunc::Memory);\n+  transform_later(mem_phi);\n+\n+  assert(projs->nb_resproj == 1, \"unexpected number of results\");\n+  _igvn.replace_in_uses(projs->fallthrough_catchproj, r);\n+  _igvn.replace_in_uses(projs->fallthrough_memproj, mem_phi);\n+  _igvn.replace_in_uses(projs->fallthrough_ioproj, io_phi);\n+  _igvn.replace_in_uses(projs->resproj[0], res_phi);\n+  _igvn.replace_in_uses(projs->catchall_catchproj, ex_r);\n+  _igvn.replace_in_uses(projs->catchall_memproj, ex_mem_phi);\n+  _igvn.replace_in_uses(projs->catchall_ioproj, ex_io_phi);\n+  \/\/ The CatchNode should not use the ex_io_phi. Re-connect it to the catchall_ioproj.\n+  Node* cn = projs->fallthrough_catchproj->in(0);\n+  _igvn.replace_input_of(cn, 1, projs->catchall_ioproj);\n+\n+  _igvn.replace_node(ctl, projs->fallthrough_catchproj);\n+  _igvn.replace_node(mem, projs->fallthrough_memproj);\n+  _igvn.replace_node(io, projs->fallthrough_ioproj);\n+  _igvn.replace_node(res, projs->resproj[0]);\n+  _igvn.replace_node(ex_ctl, projs->catchall_catchproj);\n+  _igvn.replace_node(ex_mem, projs->catchall_memproj);\n+  _igvn.replace_node(ex_io, projs->catchall_ioproj);\n+ }\n+\n@@ -2341,1 +2734,1 @@\n-      subklass = _igvn.transform(LoadKlassNode::make(_igvn, nullptr, C->immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+      subklass = _igvn.transform(LoadKlassNode::make(_igvn, nullptr, C->immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n@@ -2353,0 +2746,113 @@\n+\/\/ FlatArrayCheckNode (array1 array2 ...) is expanded into:\n+\/\/\n+\/\/ long mark = array1.mark | array2.mark | ...;\n+\/\/ long locked_bit = markWord::unlocked_value & array1.mark & array2.mark & ...;\n+\/\/ if (locked_bit == 0) {\n+\/\/   \/\/ One array is locked, load prototype header from the klass\n+\/\/   mark = array1.klass.proto | array2.klass.proto | ...\n+\/\/ }\n+\/\/ if ((mark & markWord::flat_array_bit_in_place) == 0) {\n+\/\/    ...\n+\/\/ }\n+void PhaseMacroExpand::expand_flatarraycheck_node(FlatArrayCheckNode* check) {\n+  bool array_inputs = _igvn.type(check->in(FlatArrayCheckNode::ArrayOrKlass))->isa_oopptr() != nullptr;\n+  if (array_inputs) {\n+    Node* mark = MakeConX(0);\n+    Node* locked_bit = MakeConX(markWord::unlocked_value);\n+    Node* mem = check->in(FlatArrayCheckNode::Memory);\n+    for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+      Node* ary = check->in(i);\n+      const TypeOopPtr* t = _igvn.type(ary)->isa_oopptr();\n+      assert(t != nullptr, \"Mixing array and klass inputs\");\n+      assert(!t->is_flat() && !t->is_not_flat(), \"Should have been optimized out\");\n+      Node* mark_adr = basic_plus_adr(ary, oopDesc::mark_offset_in_bytes());\n+      Node* mark_load = _igvn.transform(LoadNode::make(_igvn, nullptr, mem, mark_adr, mark_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+      mark = _igvn.transform(new OrXNode(mark, mark_load));\n+      locked_bit = _igvn.transform(new AndXNode(locked_bit, mark_load));\n+    }\n+    assert(!mark->is_Con(), \"Should have been optimized out\");\n+    Node* cmp = _igvn.transform(new CmpXNode(locked_bit, MakeConX(0)));\n+    Node* is_unlocked = _igvn.transform(new BoolNode(cmp, BoolTest::ne));\n+\n+    \/\/ BoolNode might be shared, replace each if user\n+    Node* old_bol = check->unique_out();\n+    assert(old_bol->is_Bool() && old_bol->as_Bool()->_test._test == BoolTest::ne, \"unexpected condition\");\n+    for (DUIterator_Last imin, i = old_bol->last_outs(imin); i >= imin; --i) {\n+      IfNode* old_iff = old_bol->last_out(i)->as_If();\n+      Node* ctrl = old_iff->in(0);\n+      RegionNode* region = new RegionNode(3);\n+      Node* mark_phi = new PhiNode(region, TypeX_X);\n+\n+      \/\/ Check if array is unlocked\n+      IfNode* iff = _igvn.transform(new IfNode(ctrl, is_unlocked, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+\n+      \/\/ Unlocked: Use bits from mark word\n+      region->init_req(1, _igvn.transform(new IfTrueNode(iff)));\n+      mark_phi->init_req(1, mark);\n+\n+      \/\/ Locked: Load prototype header from klass\n+      ctrl = _igvn.transform(new IfFalseNode(iff));\n+      Node* proto = MakeConX(0);\n+      for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+        Node* ary = check->in(i);\n+        \/\/ Make loads control dependent to make sure they are only executed if array is locked\n+        Node* klass_adr = basic_plus_adr(ary, oopDesc::klass_offset_in_bytes());\n+        Node* klass = _igvn.transform(LoadKlassNode::make(_igvn, ctrl, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+        Node* proto_adr = basic_plus_adr(klass, in_bytes(Klass::prototype_header_offset()));\n+        Node* proto_load = _igvn.transform(LoadNode::make(_igvn, ctrl, C->immutable_memory(), proto_adr, proto_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+        proto = _igvn.transform(new OrXNode(proto, proto_load));\n+      }\n+      region->init_req(2, ctrl);\n+      mark_phi->init_req(2, proto);\n+\n+      \/\/ Check if flat array bits are set\n+      Node* mask = MakeConX(markWord::flat_array_bit_in_place);\n+      Node* masked = _igvn.transform(new AndXNode(_igvn.transform(mark_phi), mask));\n+      cmp = _igvn.transform(new CmpXNode(masked, MakeConX(0)));\n+      Node* is_not_flat = _igvn.transform(new BoolNode(cmp, BoolTest::eq));\n+\n+      ctrl = _igvn.transform(region);\n+      iff = _igvn.transform(new IfNode(ctrl, is_not_flat, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+      _igvn.replace_node(old_iff, iff);\n+    }\n+    _igvn.replace_node(check, C->top());\n+  } else {\n+    \/\/ Fall back to layout helper check\n+    Node* lhs = intcon(0);\n+    for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+      Node* array_or_klass = check->in(i);\n+      Node* klass = nullptr;\n+      const TypePtr* t = _igvn.type(array_or_klass)->is_ptr();\n+      assert(!t->is_flat() && !t->is_not_flat(), \"Should have been optimized out\");\n+      if (t->isa_oopptr() != nullptr) {\n+        Node* klass_adr = basic_plus_adr(array_or_klass, oopDesc::klass_offset_in_bytes());\n+        klass = transform_later(LoadKlassNode::make(_igvn, nullptr, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+      } else {\n+        assert(t->isa_klassptr(), \"Unexpected input type\");\n+        klass = array_or_klass;\n+      }\n+      Node* lh_addr = basic_plus_adr(klass, in_bytes(Klass::layout_helper_offset()));\n+      Node* lh_val = _igvn.transform(LoadNode::make(_igvn, nullptr, C->immutable_memory(), lh_addr, lh_addr->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));\n+      lhs = _igvn.transform(new OrINode(lhs, lh_val));\n+    }\n+    Node* masked = transform_later(new AndINode(lhs, intcon(Klass::_lh_array_tag_flat_value_bit_inplace)));\n+    Node* cmp = transform_later(new CmpINode(masked, intcon(0)));\n+    Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+    Node* m2b = transform_later(new Conv2BNode(masked));\n+    \/\/ The matcher expects the input to If nodes to be produced by a Bool(CmpI..)\n+    \/\/ pattern, but the input to other potential users (e.g. Phi) to be some\n+    \/\/ other pattern (e.g. a Conv2B node, possibly idealized as a CMoveI).\n+    Node* old_bol = check->unique_out();\n+    for (DUIterator_Last imin, i = old_bol->last_outs(imin); i >= imin; --i) {\n+      Node* user = old_bol->last_out(i);\n+      for (uint j = 0; j < user->req(); j++) {\n+        Node* n = user->in(j);\n+        if (n == old_bol) {\n+          _igvn.replace_input_of(user, j, user->is_If() ? bol : m2b);\n+        }\n+      }\n+    }\n+    _igvn.replace_node(check, C->top());\n+  }\n+}\n+\n@@ -2418,2 +2924,5 @@\n-      case Node::Class_CallStaticJava:\n-        success = eliminate_boxing_node(n->as_CallStaticJava());\n+      case Node::Class_CallStaticJava: {\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          success = eliminate_boxing_node(n->as_CallStaticJava());\n+        }\n@@ -2421,0 +2930,1 @@\n+      }\n@@ -2434,0 +2944,2 @@\n+      case Node::Class_FlatArrayCheck:\n+        break;\n@@ -2483,4 +2995,7 @@\n-        \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n-        C->remove_macro_node(n);\n-        _igvn._worklist.push(n);\n-        success = true;\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n+          C->remove_macro_node(n);\n+          _igvn._worklist.push(n);\n+          success = true;\n+        }\n@@ -2588,0 +3103,7 @@\n+    case Node::Class_CallStaticJava:\n+      expand_mh_intrinsic_return(n->as_CallStaticJava());\n+      C->remove_macro_node(n);\n+      break;\n+    case Node::Class_FlatArrayCheck:\n+      expand_flatarraycheck_node(n->as_FlatArrayCheck());\n+      break;\n@@ -2603,1 +3125,1 @@\n-        for (unsigned int i = 0; i < mod_macro->tf()->domain()->cnt() - TypeFunc::Parms; i++) {\n+        for (unsigned int i = 0; i < mod_macro->tf()->domain_cc()->cnt() - TypeFunc::Parms; i++) {\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":704,"deletions":182,"binary":false,"changes":886,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -141,1 +142,1 @@\n-inline Node* PhaseMacroExpand::generate_slow_guard(Node** ctrl, Node* test, RegionNode* region) {\n+Node* PhaseMacroExpand::generate_slow_guard(Node** ctrl, Node* test, RegionNode* region) {\n@@ -145,0 +146,4 @@\n+inline Node* PhaseMacroExpand::generate_fair_guard(Node** ctrl, Node* test, RegionNode* region) {\n+  return generate_guard(ctrl, test, region, PROB_FAIR);\n+}\n+\n@@ -285,0 +290,43 @@\n+Node* PhaseMacroExpand::mark_word_test(Node** ctrl, Node* obj, MergeMemNode* mem, uintptr_t mask_val, RegionNode* region) {\n+  \/\/ Load markword and check if obj is locked\n+  Node* mark = make_load(nullptr, mem->memory_at(Compile::AliasIdxRaw), obj, oopDesc::mark_offset_in_bytes(), TypeX_X, TypeX_X->basic_type());\n+  Node* locked_bit = MakeConX(markWord::unlocked_value);\n+  locked_bit = transform_later(new AndXNode(locked_bit, mark));\n+  Node* cmp = transform_later(new CmpXNode(locked_bit, MakeConX(0)));\n+  Node* is_unlocked = transform_later(new BoolNode(cmp, BoolTest::ne));\n+  IfNode* iff = transform_later(new IfNode(*ctrl, is_unlocked, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+  Node* locked_region = transform_later(new RegionNode(3));\n+  Node* mark_phi = transform_later(new PhiNode(locked_region, TypeX_X));\n+\n+  \/\/ Unlocked: Use bits from mark word\n+  locked_region->init_req(1, transform_later(new IfTrueNode(iff)));\n+  mark_phi->init_req(1, mark);\n+\n+  \/\/ Locked: Load prototype header from klass\n+  *ctrl = transform_later(new IfFalseNode(iff));\n+  \/\/ Make loads control dependent to make sure they are only executed if array is locked\n+  Node* klass_adr = basic_plus_adr(obj, oopDesc::klass_offset_in_bytes());\n+  Node* klass = transform_later(LoadKlassNode::make(_igvn, *ctrl, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+  Node* proto_adr = basic_plus_adr(klass, in_bytes(Klass::prototype_header_offset()));\n+  Node* proto = transform_later(LoadNode::make(_igvn, *ctrl, C->immutable_memory(), proto_adr, proto_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+\n+  locked_region->init_req(2, *ctrl);\n+  mark_phi->init_req(2, proto);\n+  *ctrl = locked_region;\n+\n+  \/\/ Now check if mark word bits are set\n+  Node* mask = MakeConX(mask_val);\n+  Node* masked = transform_later(new AndXNode(mark_phi, mask));\n+  cmp = transform_later(new CmpXNode(masked, mask));\n+  Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+  return generate_fair_guard(ctrl, bol, region);\n+}\n+\n+Node* PhaseMacroExpand::generate_flat_array_guard(Node** ctrl, Node* array, MergeMemNode* mem, RegionNode* region) {\n+  return mark_word_test(ctrl, array, mem, markWord::flat_array_bit_in_place, region);\n+}\n+\n+Node* PhaseMacroExpand::generate_null_free_array_guard(Node** ctrl, Node* array, MergeMemNode* mem, RegionNode* region) {\n+  return mark_word_test(ctrl, array, mem, markWord::null_free_array_bit_in_place, region);\n+}\n+\n@@ -379,0 +427,1 @@\n+                                           Node* dest_length,\n@@ -390,0 +439,2 @@\n+  Node* default_value = nullptr;\n+  Node* raw_default_value = nullptr;\n@@ -420,0 +471,2 @@\n+      default_value = alloc->in(AllocateNode::DefaultValue);\n+      raw_default_value = alloc->in(AllocateNode::RawDefaultValue);\n@@ -489,1 +542,0 @@\n-      Node* dest_length = alloc->in(AllocateNode::ALength);\n@@ -496,1 +548,3 @@\n-                             adr_type, dest, basic_elem_type,\n+                             adr_type, dest,\n+                             default_value, raw_default_value,\n+                             basic_elem_type,\n@@ -527,1 +581,0 @@\n-    Node* dest_length = alloc->in(AllocateNode::ALength);\n@@ -533,1 +586,3 @@\n-                           adr_type, dest, basic_elem_type,\n+                           adr_type, dest,\n+                           default_value, raw_default_value,\n+                           basic_elem_type,\n@@ -582,1 +637,3 @@\n-                             adr_type, dest, basic_elem_type,\n+                             adr_type, dest,\n+                             default_value, raw_default_value,\n+                             basic_elem_type,\n@@ -592,1 +649,3 @@\n-                             adr_type, dest, basic_elem_type,\n+                             adr_type, dest,\n+                             default_value, raw_default_value,\n+                             basic_elem_type,\n@@ -770,1 +829,3 @@\n-                           adr_type, dest, basic_elem_type,\n+                           adr_type, dest,\n+                           default_value, raw_default_value,\n+                           basic_elem_type,\n@@ -832,3 +893,3 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, out_mem);\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_ioproj, *io);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, out_mem);\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_ioproj, *io);\n@@ -836,1 +897,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_catchproj, *ctrl);\n+  _igvn.replace_node(_callprojs->fallthrough_catchproj, *ctrl);\n@@ -876,0 +937,2 @@\n+                                            Node* val,\n+                                            Node* raw_val,\n@@ -914,1 +977,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, val, raw_val,\n@@ -919,1 +982,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, val, raw_val,\n@@ -932,1 +995,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, val, raw_val,\n@@ -961,1 +1024,7 @@\n-        mem = StoreNode::make(_igvn, ctrl, mem, p1, adr_type, intcon(0), T_INT, MemNode::unordered);\n+        if (val == nullptr) {\n+          assert(raw_val == nullptr, \"val may not be null\");\n+          mem = StoreNode::make(_igvn, ctrl, mem, p1, adr_type, intcon(0), T_INT, MemNode::unordered);\n+        } else {\n+          assert(_igvn.type(val)->isa_narrowoop(), \"should be narrow oop\");\n+          mem = new StoreNNode(ctrl, mem, p1, adr_type, val, MemNode::unordered);\n+        }\n@@ -966,1 +1035,1 @@\n-    mem = ClearArrayNode::clear_memory(ctrl, mem, dest,\n+    mem = ClearArrayNode::clear_memory(ctrl, mem, dest, raw_val,\n@@ -1082,2 +1151,2 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  *ctrl = _callprojs.fallthrough_catchproj->clone();\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  *ctrl = _callprojs->fallthrough_catchproj->clone();\n@@ -1086,1 +1155,1 @@\n-  Node* m = _callprojs.fallthrough_memproj->clone();\n+  Node* m = _callprojs->fallthrough_memproj->clone();\n@@ -1100,3 +1169,3 @@\n-  \/\/ could be null. Skip clone and update null fallthrough_ioproj.\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    *io = _callprojs.fallthrough_ioproj->clone();\n+  \/\/ could be nullptr. Skip clone and update nullptr fallthrough_ioproj.\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    *io = _callprojs->fallthrough_ioproj->clone();\n@@ -1234,0 +1303,36 @@\n+const TypePtr* PhaseMacroExpand::adjust_for_flat_array(const TypeAryPtr* top_dest, Node*& src_offset,\n+                                                       Node*& dest_offset, Node*& length, BasicType& dest_elem,\n+                                                       Node*& dest_length) {\n+#ifdef ASSERT\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  bool needs_barriers = top_dest->elem()->inline_klass()->contains_oops() &&\n+    bs->array_copy_requires_gc_barriers(dest_length != nullptr, T_OBJECT, false, false, BarrierSetC2::Optimization);\n+  assert(!needs_barriers || StressReflectiveCode, \"Flat arracopy would require GC barriers\");\n+#endif\n+  int elem_size = top_dest->flat_elem_size();\n+  if (elem_size >= 8) {\n+    if (elem_size > 8) {\n+      \/\/ treat as array of long but scale length, src offset and dest offset\n+      assert((elem_size % 8) == 0, \"not a power of 2?\");\n+      int factor = elem_size \/ 8;\n+      length = transform_later(new MulINode(length, intcon(factor)));\n+      src_offset = transform_later(new MulINode(src_offset, intcon(factor)));\n+      dest_offset = transform_later(new MulINode(dest_offset, intcon(factor)));\n+      if (dest_length != nullptr) {\n+        dest_length = transform_later(new MulINode(dest_length, intcon(factor)));\n+      }\n+      elem_size = 8;\n+    }\n+    dest_elem = T_LONG;\n+  } else if (elem_size == 4) {\n+    dest_elem = T_INT;\n+  } else if (elem_size == 2) {\n+    dest_elem = T_CHAR;\n+  } else if (elem_size == 1) {\n+    dest_elem = T_BYTE;\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+  return TypeRawPtr::BOTTOM;\n+}\n+\n@@ -1251,3 +1356,16 @@\n-    Node* mem = ac->in(TypeFunc::Memory);\n-    merge_mem = MergeMemNode::make(mem);\n-    transform_later(merge_mem);\n+    const Type* src_type = _igvn.type(src);\n+    const Type* dest_type = _igvn.type(dest);\n+    const TypeAryPtr* top_src = src_type->isa_aryptr();\n+    \/\/ Note: The destination could have type Object (i.e. non-array) when directly invoking the protected method\n+    \/\/       Object::clone() with reflection on a declared Object that is an array at runtime. top_dest is then null.\n+    const TypeAryPtr* top_dest = dest_type->isa_aryptr();\n+    BasicType dest_elem = T_OBJECT;\n+    if (top_dest != nullptr && top_dest->elem() != Type::BOTTOM) {\n+      dest_elem = top_dest->elem()->array_element_basic_type();\n+    }\n+    if (is_reference_type(dest_elem, true)) dest_elem = T_OBJECT;\n+\n+    if (top_src != nullptr && top_src->is_flat()) {\n+      \/\/ If src is flat, dest is guaranteed to be flat as well\n+      top_dest = top_src;\n+    }\n@@ -1256,0 +1374,1 @@\n+    Node* dest_length = nullptr;\n@@ -1259,0 +1378,1 @@\n+      dest_length = alloc->in(AllocateNode::ALength);\n@@ -1261,3 +1381,16 @@\n-    const TypePtr* adr_type = _igvn.type(dest)->is_oopptr()->add_offset(Type::OffsetBot);\n-    if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n-      adr_type = ac->_dest_type->add_offset(Type::OffsetBot)->is_ptr();\n+    Node* mem = ac->in(TypeFunc::Memory);\n+    const TypePtr* adr_type = nullptr;\n+    if (top_dest != nullptr && top_dest->is_flat()) {\n+      assert(dest_length != nullptr || StressReflectiveCode, \"must be tightly coupled\");\n+      \/\/ Copy to a flat array modifies multiple memory slices. Conservatively insert a barrier\n+      \/\/ on all slices to prevent writes into the source from floating below the arraycopy.\n+      insert_mem_bar(&ctrl, &mem, Op_MemBarCPUOrder);\n+      adr_type = adjust_for_flat_array(top_dest, src_offset, dest_offset, length, dest_elem, dest_length);\n+    } else {\n+      adr_type = dest_type->is_oopptr()->add_offset(Type::OffsetBot);\n+      if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n+        adr_type = ac->_dest_type->add_offset(Type::OffsetBot)->is_ptr();\n+      }\n+      if (ac->_src_type != ac->_dest_type) {\n+        adr_type = TypeRawPtr::BOTTOM;\n+      }\n@@ -1265,0 +1398,3 @@\n+    merge_mem = MergeMemNode::make(mem);\n+    transform_later(merge_mem);\n+\n@@ -1266,1 +1402,1 @@\n-                       adr_type, T_OBJECT,\n+                       adr_type, dest_elem,\n@@ -1268,0 +1404,1 @@\n+                       dest_length,\n@@ -1303,3 +1440,1 @@\n-  if (ac->is_arraycopy_validated() &&\n-      dest_elem != T_CONFLICT &&\n-      src_elem == T_CONFLICT) {\n+  if (ac->is_arraycopy_validated() && dest_elem != T_CONFLICT && src_elem == T_CONFLICT) {\n@@ -1321,6 +1456,7 @@\n-    Node* mem = generate_arraycopy(ac, nullptr, &ctrl, merge_mem, &io,\n-                                   TypeRawPtr::BOTTOM, T_CONFLICT,\n-                                   src, src_offset, dest, dest_offset, length,\n-                                   \/\/ If a  negative length guard was generated for the ArrayCopyNode,\n-                                   \/\/ the length of the array can never be negative.\n-                                   false, ac->has_negative_length_guard());\n+    generate_arraycopy(ac, nullptr, &ctrl, merge_mem, &io,\n+                       TypeRawPtr::BOTTOM, T_CONFLICT,\n+                       src, src_offset, dest, dest_offset, length,\n+                       nullptr,\n+                       \/\/ If a  negative length guard was generated for the ArrayCopyNode,\n+                       \/\/ the length of the array can never be negative.\n+                       false, ac->has_negative_length_guard());\n@@ -1334,1 +1470,8 @@\n-  if (src_elem != dest_elem || dest_elem == T_VOID) {\n+  \/\/\n+  \/\/ We have no stub to copy flat inline type arrays with oop\n+  \/\/ fields if we need to emit write barriers.\n+  \/\/\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  if (src_elem != dest_elem || top_src->is_flat() != top_dest->is_flat() || dest_elem == T_VOID ||\n+      (top_src->is_flat() && top_dest->elem()->inline_klass()->contains_oops() &&\n+       bs->array_copy_requires_gc_barriers(alloc != nullptr, T_OBJECT, false, false, BarrierSetC2::Optimization))) {\n@@ -1342,3 +1485,3 @@\n-    _igvn.replace_node(_callprojs.fallthrough_memproj, merge_mem);\n-    if (_callprojs.fallthrough_ioproj != nullptr) {\n-      _igvn.replace_node(_callprojs.fallthrough_ioproj, io);\n+    _igvn.replace_node(_callprojs->fallthrough_memproj, merge_mem);\n+    if (_callprojs->fallthrough_ioproj != nullptr) {\n+      _igvn.replace_node(_callprojs->fallthrough_ioproj, io);\n@@ -1346,1 +1489,1 @@\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, ctrl);\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, ctrl);\n@@ -1363,4 +1506,5 @@\n-  {\n-    Node* mem = ac->in(TypeFunc::Memory);\n-    merge_mem = MergeMemNode::make(mem);\n-    transform_later(merge_mem);\n+  Node* mem = ac->in(TypeFunc::Memory);\n+  if (top_dest->is_flat()) {\n+    \/\/ Copy to a flat array modifies multiple memory slices. Conservatively insert a barrier\n+    \/\/ on all slices to prevent writes into the source from floating below the arraycopy.\n+    insert_mem_bar(&ctrl, &mem, Op_MemBarCPUOrder);\n@@ -1368,0 +1512,2 @@\n+  merge_mem = MergeMemNode::make(mem);\n+  transform_later(merge_mem);\n@@ -1408,0 +1554,15 @@\n+\n+    \/\/ Handle inline type arrays\n+    if (!top_src->is_flat()) {\n+      if (UseArrayFlattening && !top_src->is_not_flat()) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        generate_flat_array_guard(&ctrl, src, merge_mem, slow_region);\n+      }\n+      if (EnableValhalla) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        generate_null_free_array_guard(&ctrl, dest, merge_mem, slow_region);\n+      }\n+    } else {\n+      assert(top_dest->is_flat(), \"dest array must be flat\");\n+    }\n@@ -1409,0 +1570,1 @@\n+\n@@ -1411,1 +1573,5 @@\n-  if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n+  Node* dest_length = (alloc != nullptr) ? alloc->in(AllocateNode::ALength) : nullptr;\n+\n+  if (top_dest->is_flat()) {\n+    adr_type = adjust_for_flat_array(top_dest, src_offset, dest_offset, length, dest_elem, dest_length);\n+  } else if (ac->_dest_type != TypeOopPtr::BOTTOM) {\n@@ -1420,0 +1586,1 @@\n+                     dest_length,\n@@ -1422,1 +1589,2 @@\n-                     false, ac->has_negative_length_guard(), slow_region);\n+                     false, ac->has_negative_length_guard(),\n+                     slow_region);\n","filename":"src\/hotspot\/share\/opto\/macroArrayCopy.cpp","additions":218,"deletions":50,"binary":false,"changes":268,"status":"modified"},{"patch":"@@ -189,0 +189,44 @@\n+\/\/ Array of RegMask, one per returned values (inline type instances can\n+\/\/ be returned as multiple return values, one per field)\n+RegMask* Matcher::return_values_mask(const TypeFunc* tf) {\n+  const TypeTuple* range = tf->range_cc();\n+  uint cnt = range->cnt() - TypeFunc::Parms;\n+  if (cnt == 0) {\n+    return nullptr;\n+  }\n+  RegMask* mask = NEW_RESOURCE_ARRAY(RegMask, cnt);\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, cnt);\n+  VMRegPair* vm_parm_regs = NEW_RESOURCE_ARRAY(VMRegPair, cnt);\n+  for (uint i = 0; i < cnt; i++) {\n+    sig_bt[i] = range->field_at(i+TypeFunc::Parms)->basic_type();\n+  }\n+\n+  int regs = SharedRuntime::java_return_convention(sig_bt, vm_parm_regs, cnt);\n+  if (regs <= 0) {\n+    \/\/ We ran out of registers to store the IsInit information for a nullable inline type return.\n+    \/\/ Since it is only set in the 'call_epilog', we can simply put it on the stack.\n+    assert(tf->returns_inline_type_as_fields(), \"should have been tested during graph construction\");\n+    \/\/ TODO 8284443 Can we teach the register allocator to reserve a stack slot instead?\n+    \/\/ mask[--cnt] = STACK_ONLY_mask does not work (test with -XX:+StressGCM)\n+    int slot = C->fixed_slots() - 2;\n+    if (C->needs_stack_repair()) {\n+      slot -= 2; \/\/ Account for stack increment value\n+    }\n+    mask[--cnt].Clear();\n+    mask[cnt].Insert(OptoReg::stack2reg(slot));\n+  }\n+  for (uint i = 0; i < cnt; i++) {\n+    mask[i].Clear();\n+\n+    OptoReg::Name reg1 = OptoReg::as_OptoReg(vm_parm_regs[i].first());\n+    if (OptoReg::is_valid(reg1)) {\n+      mask[i].Insert(reg1);\n+    }\n+    OptoReg::Name reg2 = OptoReg::as_OptoReg(vm_parm_regs[i].second());\n+    if (OptoReg::is_valid(reg2)) {\n+      mask[i].Insert(reg2);\n+    }\n+  }\n+\n+  return mask;\n+}\n@@ -207,15 +251,3 @@\n-  \/\/ Map a Java-signature return type into return register-value\n-  \/\/ machine registers for 0, 1 and 2 returned values.\n-  const TypeTuple *range = C->tf()->range();\n-  if( range->cnt() > TypeFunc::Parms ) { \/\/ If not a void function\n-    \/\/ Get ideal-register return type\n-    uint ireg = range->field_at(TypeFunc::Parms)->ideal_reg();\n-    \/\/ Get machine return register\n-    uint sop = C->start()->Opcode();\n-    OptoRegPair regs = return_value(ireg);\n-\n-    \/\/ And mask for same\n-    _return_value_mask = RegMask(regs.first());\n-    if( OptoReg::is_valid(regs.second()) )\n-      _return_value_mask.Insert(regs.second());\n-  }\n+  \/\/ Map Java-signature return types into return register-value\n+  \/\/ machine registers.\n+  _return_values_mask = return_values_mask(C->tf());\n@@ -229,1 +261,1 @@\n-  const TypeTuple *domain = C->tf()->domain();\n+  const TypeTuple *domain = C->tf()->domain_cc();\n@@ -552,0 +584,1 @@\n+\n@@ -810,1 +843,1 @@\n-  uint ret_edge_cnt = TypeFunc::Parms + ((C->tf()->range()->cnt() == TypeFunc::Parms) ? 0 : 1);\n+  uint ret_edge_cnt = C->tf()->range_cc()->cnt();\n@@ -812,4 +845,3 @@\n-  \/\/ Returns have 0 or 1 returned values depending on call signature.\n-  \/\/ Return register is specified by return_value in the AD file.\n-  if (ret_edge_cnt > TypeFunc::Parms)\n-    ret_rms[TypeFunc::Parms+0] = _return_value_mask;\n+  for (i = TypeFunc::Parms; i < ret_edge_cnt; i++) {\n+    ret_rms[i] = _return_values_mask[i-TypeFunc::Parms];\n+  }\n@@ -887,1 +919,1 @@\n-  int proj_cnt = C->tf()->domain()->cnt();\n+  int proj_cnt = C->tf()->domain_cc()->cnt();\n@@ -1168,1 +1200,5 @@\n-              m = n->in(0)->as_Multi()->match( n->as_Proj(), this );\n+              RegMask* mask = nullptr;\n+              if (n->in(0)->is_Call() && n->in(0)->as_Call()->tf()->returns_inline_type_as_fields()) {\n+                mask = return_values_mask(n->in(0)->as_Call()->tf());\n+              }\n+              m = n->in(0)->as_Multi()->match(n->as_Proj(), this, mask);\n@@ -1308,1 +1344,1 @@\n-    domain = call->tf()->domain();\n+    domain = call->tf()->domain_cc();\n@@ -1387,1 +1423,4 @@\n-  int argcnt = cnt - TypeFunc::Parms;\n+  \/\/ Null entry point is a special cast where the target of the call\n+  \/\/ is in a register.\n+  int adj = (call != nullptr && call->entry_point() == nullptr) ? 1 : 0;\n+  int argcnt = cnt - TypeFunc::Parms - adj;\n@@ -1393,1 +1432,1 @@\n-      sig_bt[i] = domain->field_at(i+TypeFunc::Parms)->basic_type();\n+      sig_bt[i] = domain->field_at(i+TypeFunc::Parms+adj)->basic_type();\n@@ -1434,1 +1473,1 @@\n-      RegMask *rm = &mcall->_in_rms[i+TypeFunc::Parms];\n+      RegMask *rm = &mcall->_in_rms[i+TypeFunc::Parms+adj];\n@@ -1455,1 +1494,1 @@\n-      if (OptoReg::is_valid(reg1))\n+      if (OptoReg::is_valid(reg1)) {\n@@ -1457,0 +1496,1 @@\n+      }\n@@ -1462,1 +1502,1 @@\n-      if (OptoReg::is_valid(reg2))\n+      if (OptoReg::is_valid(reg2)) {\n@@ -1464,0 +1504,1 @@\n+      }\n@@ -1479,1 +1520,1 @@\n-    uint r_cnt = mcall->tf()->range()->cnt();\n+    uint r_cnt = mcall->tf()->range_sig()->cnt();\n@@ -1501,1 +1542,1 @@\n-         (mcall->jvms()->debug_start() + mcall->_jvmadj == mcall->tf()->domain()->cnt()), \"\");\n+         (mcall->jvms()->debug_start() + mcall->_jvmadj == mcall->tf()->domain_cc()->cnt()), \"\");\n@@ -2190,1 +2231,1 @@\n-      for (int i = n->req() - 1; i >= 0; --i) { \/\/ For my children\n+      for (int i = n->len() - 1; i >= 0; --i) { \/\/ For my children\n@@ -2497,0 +2538,7 @@\n+    case Op_ClearArray: {\n+      Node* pair = new BinaryNode(n->in(2), n->in(3));\n+      n->set_req(2, pair);\n+      n->set_req(3, n->in(4));\n+      n->del_req(4);\n+      break;\n+    }\n@@ -2561,0 +2609,8 @@\n+    case Op_StoreLSpecial: {\n+      if (n->req() > (MemNode::ValueIn + 1) && n->in(MemNode::ValueIn + 1) != nullptr) {\n+        Node* pair = new BinaryNode(n->in(MemNode::ValueIn), n->in(MemNode::ValueIn + 1));\n+        n->set_req(MemNode::ValueIn, pair);\n+        n->del_req(MemNode::ValueIn + 1);\n+      }\n+      break;\n+    }\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":88,"deletions":32,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -27,0 +28,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -41,0 +43,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -236,0 +239,2 @@\n+                     ->cast_to_not_flat(t_oop->is_aryptr()->is_not_flat())\n+                     ->cast_to_not_null_free(t_oop->is_aryptr()->is_not_null_free())\n@@ -262,1 +267,1 @@\n-               tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n+        tp->isa_aryptr() &&        tp->offset() == Type::OffsetBot &&\n@@ -1015,1 +1020,1 @@\n-    return (eliminate_boxing && non_volatile) || is_stable_ary;\n+    return (eliminate_boxing && non_volatile) || is_stable_ary || tp->is_inlinetypeptr();\n@@ -1072,1 +1077,1 @@\n-      uint shift  = exact_log2(type2aelembytes(ary_elem));\n+      uint shift  = ary_t->is_flat() ? ary_t->flat_log_elem_size() : exact_log2(type2aelembytes(ary_elem));\n@@ -1191,1 +1196,1 @@\n-        const TypeVect* out_vt = as_LoadVector()->vect_type();\n+        const TypeVect* out_vt = is_Load() ? as_LoadVector()->vect_type() : as_StoreVector()->vect_type();\n@@ -1209,0 +1214,5 @@\n+      Node* default_value = ld_alloc->in(AllocateNode::DefaultValue);\n+      if (default_value != nullptr) {\n+        return default_value;\n+      }\n+      assert(ld_alloc->in(AllocateNode::RawDefaultValue) == nullptr, \"default value may not be null\");\n@@ -1276,0 +1286,17 @@\n+  \/\/ Loading from an InlineType? The InlineType has the values of\n+  \/\/ all fields as input. Look for the field with matching offset.\n+  Node* addr = in(Address);\n+  intptr_t offset;\n+  Node* base = AddPNode::Ideal_base_and_offset(addr, phase, offset);\n+  if (!is_mismatched_access() && base != nullptr && base->is_InlineType() && offset > oopDesc::klass_offset_in_bytes()) {\n+    Node* value = base->as_InlineType()->field_value_by_offset((int)offset, true);\n+    if (value != nullptr) {\n+      if (Opcode() == Op_LoadN) {\n+        \/\/ Encode oop value if we are loading a narrow oop\n+        assert(!phase->type(value)->isa_narrowoop(), \"should already be decoded\");\n+        value = phase->transform(new EncodePNode(value, bottom_type()));\n+      }\n+      return value;\n+    }\n+  }\n+\n@@ -1869,0 +1896,1 @@\n+        && !(phase->type(address)->is_inlinetypeptr() && is_mismatched_access())\n@@ -2073,0 +2101,1 @@\n+        && !ary->is_flat()\n@@ -2108,0 +2137,2 @@\n+            \/\/ Default value load\n+            tp->is_instptr()->instance_klass() == ciEnv::current()->Class_klass() ||\n@@ -2113,1 +2144,3 @@\n-    \/\/ Optimize loads from constant fields.\n+    BasicType bt = memory_type();\n+\n+    \/\/ Optimize loads from constant fields.\n@@ -2117,1 +2150,1 @@\n-      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), memory_type());\n+      const Type* con_type = Type::make_constant_from_field(const_oop->as_instance(), off, is_unsigned(), bt);\n@@ -2164,1 +2197,1 @@\n-      if (UseCompactObjectHeaders) {\n+      if (UseCompactObjectHeaders) { \/\/ TODO: Should EnableValhalla also take this path ?\n@@ -2245,1 +2278,0 @@\n-\n@@ -2249,1 +2281,10 @@\n-      return TypeX::make(markWord::prototype().value());\n+      if (EnableValhalla) {\n+        \/\/ The mark word may contain property bits (inline, flat, null-free)\n+        Node* klass_node = alloc->in(AllocateNode::KlassNode);\n+        const TypeKlassPtr* tkls = phase->type(klass_node)->isa_klassptr();\n+        if (tkls != nullptr && tkls->is_loaded() && tkls->klass_is_exact()) {\n+          return TypeX::make(tkls->exact_klass()->prototype_header());\n+        }\n+      } else {\n+        return TypeX::make(markWord::prototype().value());\n+      }\n@@ -2401,1 +2442,2 @@\n-Node* LoadKlassNode::make(PhaseGVN& gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at, const TypeKlassPtr* tk) {\n+Node* LoadKlassNode::make(PhaseGVN& gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at,\n+                          const TypeKlassPtr* tk) {\n@@ -2448,1 +2490,2 @@\n-      ciType* t = tinst->java_mirror_type();\n+      bool is_null_free_array = false;\n+      ciType* t = tinst->java_mirror_type(&is_null_free_array);\n@@ -2458,1 +2501,5 @@\n-          return TypeKlassPtr::make(ciArrayKlass::make(t), Type::trust_interfaces);\n+          const TypeKlassPtr* tklass = TypeKlassPtr::make(ciArrayKlass::make(t), Type::trust_interfaces);\n+          if (is_null_free_array) {\n+            tklass = tklass->is_aryklassptr()->cast_to_null_free();\n+          }\n+          return tklass;\n@@ -2465,1 +2512,5 @@\n-        return TypeKlassPtr::make(t->as_klass(), Type::trust_interfaces);\n+        const TypeKlassPtr* tklass = TypeKlassPtr::make(t->as_klass(), Type::trust_interfaces);\n+        if (is_null_free_array) {\n+          tklass = tklass->is_aryklassptr()->cast_to_null_free();\n+        }\n+        return tklass;\n@@ -2477,1 +2528,1 @@\n-  const TypeAryPtr *tary = tp->isa_aryptr();\n+  const TypeAryPtr* tary = tp->isa_aryptr();\n@@ -3314,2 +3365,2 @@\n-  \/\/ unsafe if I have intervening uses.\n-  {\n+  \/\/ unsafe if I have intervening uses...\n+  if (phase->C->get_adr_type(phase->C->get_alias_index(adr_type())) != TypeAryPtr::INLINES) {\n@@ -3335,0 +3386,2 @@\n+             (Opcode() == Op_StoreL && st->Opcode() == Op_StoreN) ||\n+             (st->adr_type()->isa_aryptr() && st->adr_type()->is_aryptr()->is_flat()) || \/\/ TODO 8343835\n@@ -3461,2 +3514,1 @@\n-  if (result == this &&\n-      ReduceFieldZeroing && phase->type(val)->is_zero_type()) {\n+  if (result == this && ReduceFieldZeroing) {\n@@ -3464,1 +3516,2 @@\n-    if (mem->is_Proj() && mem->in(0)->is_Allocate()) {\n+    if (mem->is_Proj() && mem->in(0)->is_Allocate() &&\n+        (phase->type(val)->is_zero_type() || mem->in(0)->in(AllocateNode::DefaultValue) == val)) {\n@@ -3468,1 +3521,1 @@\n-    if (result == this) {\n+    if (result == this && phase->type(val)->is_zero_type()) {\n@@ -3784,1 +3837,1 @@\n-    return new ClearArrayNode(in(0), in(1), in(2), in(3), true);\n+    return new ClearArrayNode(in(0), in(1), in(2), in(3), in(4), true);\n@@ -3802,1 +3855,1 @@\n-  Node *zero = phase->makecon(TypeLong::ZERO);\n+  Node *val = in(4);\n@@ -3804,1 +3857,1 @@\n-  mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+  mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -3809,1 +3862,1 @@\n-    mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);\n+    mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);\n@@ -3843,0 +3896,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -3853,1 +3908,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != nullptr) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == nullptr, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -3860,1 +3921,1 @@\n-  return clear_memory(ctl, mem, dest, phase->MakeConX(offset), end_offset, phase);\n+  return clear_memory(ctl, mem, dest, raw_val, phase->MakeConX(offset), end_offset, phase);\n@@ -3864,0 +3925,1 @@\n+                                   Node* raw_val,\n@@ -3886,1 +3948,4 @@\n-  mem = new ClearArrayNode(ctl, mem, zsize, adr, false);\n+  if (raw_val == nullptr) {\n+    raw_val = phase->MakeConX(0);\n+  }\n+  mem = new ClearArrayNode(ctl, mem, zsize, adr, raw_val, false);\n@@ -3891,0 +3956,2 @@\n+                                   Node* val,\n+                                   Node* raw_val,\n@@ -3905,1 +3972,1 @@\n-    mem = clear_memory(ctl, mem, dest,\n+    mem = clear_memory(ctl, mem, dest, val, raw_val,\n@@ -3912,1 +3979,7 @@\n-    mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    if (val != nullptr) {\n+      assert(phase->type(val)->isa_narrowoop(), \"should be narrow oop\");\n+      mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);\n+    } else {\n+      assert(raw_val == nullptr, \"val may not be null\");\n+      mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase->zerocon(T_INT), T_INT, MemNode::unordered);\n+    }\n@@ -4058,1 +4131,1 @@\n-Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {\n+Node *MemBarNode::match(const ProjNode *proj, const Matcher *m, const RegMask* mask) {\n@@ -4345,1 +4418,3 @@\n-  if (init == nullptr || init->is_complete())  return false;\n+  if (init == nullptr || init->is_complete()) {\n+    return false;\n+  }\n@@ -4529,0 +4604,6 @@\n+                if (base->is_Phi()) {\n+                  \/\/ In rare case, base may be a PhiNode and it may read\n+                  \/\/ the same memory slice between InitializeNode and store.\n+                  failed = true;\n+                  break;\n+                }\n@@ -5115,0 +5196,2 @@\n+                                              allocation()->in(AllocateNode::DefaultValue),\n+                                              allocation()->in(AllocateNode::RawDefaultValue),\n@@ -5174,0 +5257,2 @@\n+                                            allocation()->in(AllocateNode::DefaultValue),\n+                                            allocation()->in(AllocateNode::RawDefaultValue),\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":116,"deletions":31,"binary":false,"changes":147,"status":"modified"},{"patch":"@@ -199,0 +199,12 @@\n+  \/\/ Code pattern on return from a call that returns an __Value.  Can\n+  \/\/ be optimized away if the return value turns out to be an oop.\n+  if (op == Op_AndX &&\n+      in(1) != nullptr &&\n+      in(1)->Opcode() == Op_CastP2X &&\n+      in(1)->in(1) != nullptr &&\n+      phase->type(in(1)->in(1))->isa_oopptr() &&\n+      t2->isa_intptr_t()->_lo >= 0 &&\n+      t2->isa_intptr_t()->_hi <= MinObjAlignmentInBytesMask) {\n+    return add_id();\n+  }\n+\n@@ -901,0 +913,41 @@\n+  \/\/ Search for GraphKit::mark_word_test patterns and fold the test if the result is statically known\n+  Node* load1 = in(1);\n+  Node* load2 = nullptr;\n+  if (load1->is_Phi() && phase->type(load1)->isa_long()) {\n+    load1 = in(1)->in(1);\n+    load2 = in(1)->in(2);\n+  }\n+  if (load1 != nullptr && load1->is_Load() && phase->type(load1)->isa_long() &&\n+      (load2 == nullptr || (load2->is_Load() && phase->type(load2)->isa_long()))) {\n+    const TypePtr* adr_t1 = phase->type(load1->in(MemNode::Address))->isa_ptr();\n+    const TypePtr* adr_t2 = (load2 != nullptr) ? phase->type(load2->in(MemNode::Address))->isa_ptr() : nullptr;\n+    if (adr_t1 != nullptr && adr_t1->offset() == oopDesc::mark_offset_in_bytes() &&\n+        (load2 == nullptr || (adr_t2 != nullptr && adr_t2->offset() == in_bytes(Klass::prototype_header_offset())))) {\n+      if (mask == markWord::inline_type_pattern) {\n+        if (adr_t1->is_inlinetypeptr()) {\n+          set_req_X(1, in(2), phase);\n+          return this;\n+        } else if (!adr_t1->can_be_inline_type()) {\n+          set_req_X(1, phase->longcon(0), phase);\n+          return this;\n+        }\n+      } else if (mask == markWord::null_free_array_bit_in_place) {\n+        if (adr_t1->is_null_free()) {\n+          set_req_X(1, in(2), phase);\n+          return this;\n+        } else if (adr_t1->is_not_null_free()) {\n+          set_req_X(1, phase->longcon(0), phase);\n+          return this;\n+        }\n+      } else if (mask == markWord::flat_array_bit_in_place) {\n+        if (adr_t1->is_flat()) {\n+          set_req_X(1, in(2), phase);\n+          return this;\n+        } else if (adr_t1->is_not_flat()) {\n+          set_req_X(1, phase->longcon(0), phase);\n+          return this;\n+        }\n+      }\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/mulnode.cpp","additions":53,"deletions":0,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-Node *MultiNode::match( const ProjNode *proj, const Matcher *m ) { return proj->clone(); }\n+Node *MultiNode::match(const ProjNode *proj, const Matcher *m, const RegMask* mask) { return proj->clone(); }\n","filename":"src\/hotspot\/share\/opto\/multnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-  assert(t->isa_narrowoop(), \"only  narrowoop here\");\n+  assert(t->isa_narrowoop(), \"only narrowoop here\");\n","filename":"src\/hotspot\/share\/opto\/narrowptrnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -562,0 +563,3 @@\n+  if (n->is_InlineType()) {\n+    C->add_inline_type(n);\n+  }\n@@ -622,0 +626,3 @@\n+  if (is_InlineType()) {\n+    compile->remove_inline_type(this);\n+  }\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -243,1 +244,9 @@\n-    _orig_pc_slot = C->fixed_slots() - (sizeof(address) \/ VMRegImpl::stack_slot_size);\n+    int fixed_slots = C->fixed_slots();\n+    if (C->needs_stack_repair()) {\n+      fixed_slots -= 2;\n+    }\n+    \/\/ TODO 8284443 Only reserve extra slot if needed\n+    if (InlineTypeReturnedAsFields) {\n+      fixed_slots -= 2;\n+    }\n+    _orig_pc_slot = fixed_slots - (sizeof(address) \/ VMRegImpl::stack_slot_size);\n@@ -284,1 +293,2 @@\n-  MachPrologNode *prolog = new MachPrologNode();\n+  Label verified_entry;\n+  MachPrologNode* prolog = new MachPrologNode(&verified_entry);\n@@ -290,3 +300,2 @@\n-\n-  if( C->is_osr_compilation() ) {\n-    if( PoisonOSREntry ) {\n+  if (C->is_osr_compilation()) {\n+    if (PoisonOSREntry) {\n@@ -297,3 +306,14 @@\n-    if( C->method() && !C->method()->flags().is_static() ) {\n-      \/\/ Insert unvalidated entry point\n-      C->cfg()->insert( broot, 0, new MachUEPNode() );\n+    if (C->method()) {\n+      if (C->method()->has_scalarized_args()) {\n+        \/\/ Add entry point to unpack all inline type arguments\n+        C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ true, \/* receiver_only *\/ false));\n+        if (!C->method()->is_static()) {\n+          \/\/ Add verified\/unverified entry points to only unpack inline type receiver at interface calls\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ false, \/* receiver_only *\/ false));\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ true,  \/* receiver_only *\/ true));\n+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, \/* verified *\/ false, \/* receiver_only *\/ true));\n+        }\n+      } else if (!C->method()->is_static()) {\n+        \/\/ Insert unvalidated entry point\n+        C->cfg()->insert(broot, 0, new MachUEPNode());\n+      }\n@@ -301,1 +321,0 @@\n-\n@@ -341,0 +360,25 @@\n+  if (!C->is_osr_compilation() && C->has_scalarized_args()) {\n+    \/\/ Compute the offsets of the entry points required by the inline type calling convention\n+    if (!C->method()->is_static()) {\n+      \/\/ We have entries at the beginning of the method, implemented by the first 4 nodes.\n+      \/\/ Entry                     (unverified) @ offset 0\n+      \/\/ Verified_Inline_Entry_RO\n+      \/\/ Inline_Entry              (unverified)\n+      \/\/ Verified_Inline_Entry\n+      uint offset = 0;\n+      _code_offsets.set_value(CodeOffsets::Entry, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(0))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(1))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Inline_Entry, offset);\n+\n+      offset += ((MachVEPNode*)broot->get_node(2))->size(C->regalloc());\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, offset);\n+    } else {\n+      _code_offsets.set_value(CodeOffsets::Entry, -1); \/\/ will be patched later\n+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, 0);\n+    }\n+  }\n+\n@@ -502,1 +546,3 @@\n-          mcall->method_set((intptr_t)mcall->entry_point());\n+          if (mcall->entry_point() != nullptr) {\n+            mcall->method_set((intptr_t)mcall->entry_point());\n+          }\n@@ -757,0 +803,17 @@\n+      uint first_ind = spobj->first_index(sfpt->jvms());\n+      \/\/ Nullable, scalarized inline types have an is_init input\n+      \/\/ that needs to be checked before using the field values.\n+      ScopeValue* is_init = nullptr;\n+      if (cik->is_inlinetype()) {\n+        Node* init_node = sfpt->in(first_ind++);\n+        assert(init_node != nullptr, \"is_init node not found\");\n+        if (!init_node->is_top()) {\n+          const TypeInt* init_type = init_node->bottom_type()->is_int();\n+          if (init_node->is_Con()) {\n+            is_init = new ConstantIntValue(init_type->get_con());\n+          } else {\n+            OptoReg::Name init_reg = C->regalloc()->get_reg_first(init_node);\n+            is_init = new_loc_value(C->regalloc(), init_reg, Location::normal);\n+          }\n+        }\n+      }\n@@ -758,1 +821,1 @@\n-                           new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()));\n+                           new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()), true, is_init);\n@@ -761,1 +824,0 @@\n-      uint first_ind = spobj->first_index(sfpt->jvms());\n@@ -1008,0 +1070,1 @@\n+  bool return_scalarized = false;\n@@ -1028,1 +1091,1 @@\n-    if (mcall->returns_pointer()) {\n+    if (mcall->returns_pointer() || mcall->returns_scalarized()) {\n@@ -1031,0 +1094,3 @@\n+    if (mcall->returns_scalarized()) {\n+      return_scalarized = true;\n+    }\n@@ -1209,0 +1275,1 @@\n+      return_scalarized,\n@@ -1584,2 +1651,4 @@\n-          \/\/ This destination address is NOT PC-relative\n-          mcall->method_set((intptr_t)mcall->entry_point());\n+          if (mcall->entry_point() != nullptr) {\n+            \/\/ This destination address is NOT PC-relative\n+            mcall->method_set((intptr_t)mcall->entry_point());\n+          }\n@@ -1726,1 +1795,0 @@\n-\n@@ -3162,0 +3230,13 @@\n+\n+      \/\/ Do not allow a CheckCastPP node whose input is a raw pointer to\n+      \/\/ float past a safepoint.  This can occur when a buffered inline\n+      \/\/ type is allocated in a loop and the CheckCastPP from that\n+      \/\/ allocation is reused outside the loop.  If the use inside the\n+      \/\/ loop is scalarized the CheckCastPP will no longer be connected\n+      \/\/ to the loop safepoint.  See JDK-8264340.\n+      if (m->is_Mach() && m->as_Mach()->ideal_Opcode() == Op_CheckCastPP) {\n+        Node *def = m->in(1);\n+        if (def != nullptr && def->bottom_type()->base() == Type::RawPtr) {\n+          last_safept_node->add_prec(m);\n+        }\n+      }\n@@ -3320,0 +3401,19 @@\n+    if (C->has_scalarized_args()) {\n+      \/\/ Inline type entry points (MachVEPNodes) require lots of space for GC barriers and oop verification\n+      \/\/ when loading object fields from the buffered argument. Increase scratch buffer size accordingly.\n+      ciMethod* method = C->method();\n+      int barrier_size = UseZGC ? 200 : (7 DEBUG_ONLY(+ 37));\n+      int arg_num = 0;\n+      if (!method->is_static()) {\n+        if (method->is_scalarized_arg(arg_num)) {\n+          size += method->holder()->as_inline_klass()->oop_count() * barrier_size;\n+        }\n+        arg_num++;\n+      }\n+      for (ciSignatureStream str(method->signature()); !str.at_return_type(); str.next()) {\n+        if (method->is_scalarized_arg(arg_num)) {\n+          size += str.type()->as_inline_klass()->oop_count() * barrier_size;\n+        }\n+        arg_num++;\n+      }\n+    }\n@@ -3390,1 +3490,2 @@\n-  if (is_branch) \/\/ Restore label.\n+  \/\/ Restore label.\n+  if (is_branch) {\n@@ -3392,0 +3493,1 @@\n+  }\n@@ -3432,6 +3534,9 @@\n-      if (!target->is_static()) {\n-        \/\/ The UEP of an nmethod ensures that the VEP is padded. However, the padding of the UEP is placed\n-        \/\/ before the inline cache check, so we don't have to execute any nop instructions when dispatching\n-        \/\/ through the UEP, yet we can ensure that the VEP is aligned appropriately.\n-        _code_offsets.set_value(CodeOffsets::Entry, _first_block_size - MacroAssembler::ic_check_size());\n-      }\n+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, _first_block_size);\n+      }\n+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry_RO) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, _first_block_size);\n+      }\n+      if (_code_offsets.value(CodeOffsets::Entry) == -1) {\n+        _code_offsets.set_value(CodeOffsets::Entry, _first_block_size);\n+      }\n@@ -3443,14 +3548,14 @@\n-                                     entry_bci,\n-                                     &_code_offsets,\n-                                     _orig_pc_slot_offset_in_bytes,\n-                                     code_buffer(),\n-                                     frame_size_in_words(),\n-                                     oop_map_set(),\n-                                     &_handler_table,\n-                                     inc_table(),\n-                                     compiler,\n-                                     has_unsafe_access,\n-                                     SharedRuntime::is_wide_vector(C->max_vector_size()),\n-                                     C->has_monitors(),\n-                                     C->has_scoped_access(),\n-                                     0);\n+                              entry_bci,\n+                              &_code_offsets,\n+                              _orig_pc_slot_offset_in_bytes,\n+                              code_buffer(),\n+                              frame_size_in_words(),\n+                              _oop_map_set,\n+                              &_handler_table,\n+                              inc_table(),\n+                              compiler,\n+                              has_unsafe_access,\n+                              SharedRuntime::is_wide_vector(C->max_vector_size()),\n+                              C->has_monitors(),\n+                              C->has_scoped_access(),\n+                              0);\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":142,"deletions":37,"binary":false,"changes":179,"status":"modified"},{"patch":"@@ -448,1 +448,1 @@\n-  Node *fetch_interpreter_state(int index, BasicType bt, Node *local_addrs, Node *local_addrs_base);\n+  Node* fetch_interpreter_state(int index, const Type* type, Node* local_addrs, Node* local_addrs_base);\n@@ -494,1 +494,1 @@\n-  void array_store_check();\n+  Node* array_store_check(Node*& adr, const Type*& elemtype);\n@@ -497,0 +497,1 @@\n+  Node* load_from_unknown_flat_array(Node* array, Node* array_index, const TypeOopPtr* element_ptr);\n@@ -499,0 +500,1 @@\n+  void store_to_unknown_flat_array(Node* array, Node* idx, Node* non_null_stored_value);\n@@ -501,0 +503,8 @@\n+  bool needs_range_check(const TypeInt* size_type, const Node* index) const;\n+  Node* create_speculative_inline_type_array_checks(Node* array, const TypeAryPtr* array_type, const Type*& element_type);\n+  Node* cast_to_speculative_array_type(Node* array, const TypeAryPtr*& array_type, const Type*& element_type);\n+  Node* cast_to_profiled_array_type(Node* const array);\n+  Node* speculate_non_null_free_array(Node* array, const TypeAryPtr*& array_type);\n+  Node* speculate_non_flat_array(Node* array, const TypeAryPtr* array_type);\n+  void create_range_check(Node* idx, Node* ary, const TypeInt* sizetype);\n+  Node* record_profile_for_speculation_at_array_load(Node* ld);\n@@ -546,1 +556,1 @@\n-  void do_get_xxx(Node* obj, ciField* field, bool is_field);\n+  void do_get_xxx(Node* obj, ciField* field);\n@@ -548,0 +558,3 @@\n+  void set_inline_type_field(Node* obj, ciField* field, Node* val);\n+\n+  ciType* improve_abstract_inline_type_klass(ciType* field_klass);\n@@ -552,1 +565,1 @@\n-  void do_anewarray();\n+  void do_newarray();\n@@ -566,1 +579,6 @@\n-  void    do_if(BoolTest::mask btest, Node* c);\n+  void    do_if(BoolTest::mask btest, Node* c, bool can_trap = true, bool new_path = false, Node** ctrl_taken = nullptr);\n+  void    do_acmp(BoolTest::mask btest, Node* left, Node* right);\n+  void    acmp_always_null_input(Node* input, const TypeOopPtr* tinput, BoolTest::mask btest, Node* eq_region);\n+  void    acmp_known_non_inline_type_input(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, ciKlass* input_type, BoolTest::mask btest, Node* eq_region);\n+  Node*   acmp_null_check(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, Node*& null_ctl);\n+  void    acmp_unknown_non_inline_type_input(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, BoolTest::mask btest, Node* eq_region);\n@@ -568,1 +586,1 @@\n-  void    adjust_map_after_if(BoolTest::mask btest, Node* c, float prob, Block* path);\n+  void    adjust_map_after_if(BoolTest::mask btest, Node* c, float prob, Block* path, bool can_trap = true);\n","filename":"src\/hotspot\/share\/opto\/parse.hpp","additions":24,"deletions":6,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"opto\/convertnode.hpp\"\n@@ -33,0 +34,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -104,4 +106,10 @@\n-Node *Parse::fetch_interpreter_state(int index,\n-                                     BasicType bt,\n-                                     Node *local_addrs,\n-                                     Node *local_addrs_base) {\n+Node* Parse::fetch_interpreter_state(int index,\n+                                     const Type* type,\n+                                     Node* local_addrs,\n+                                     Node* local_addrs_base) {\n+  BasicType bt = type->basic_type();\n+  if (type == TypePtr::NULL_PTR) {\n+    \/\/ Ptr types are mixed together with T_ADDRESS but nullptr is\n+    \/\/ really for T_OBJECT types so correct it.\n+    bt = T_OBJECT;\n+  }\n@@ -149,1 +157,0 @@\n-\n@@ -173,0 +180,6 @@\n+    if (tp->is_inlinetypeptr() && !tp->maybe_null()) {\n+      \/\/ Check inline types for null here to prevent checkcast from adding an\n+      \/\/ exception state before the bytecode entry (use 'bad_type_ctrl' instead).\n+      l = null_check_oop(l, &bad_type_ctrl);\n+      bad_type_exit->control()->add_req(bad_type_ctrl);\n+    }\n@@ -189,1 +202,0 @@\n-\n@@ -244,1 +256,1 @@\n-    Node *lock_object = fetch_interpreter_state(index*2, T_OBJECT, monitors_addr, osr_buf);\n+    Node* lock_object = fetch_interpreter_state(index*2, Type::get_const_basic_type(T_OBJECT), monitors_addr, osr_buf);\n@@ -246,2 +258,1 @@\n-    Node *displaced_hdr = fetch_interpreter_state((index*2) + 1, T_ADDRESS, monitors_addr, osr_buf);\n-\n+    Node* displaced_hdr = fetch_interpreter_state((index*2) + 1, Type::get_const_basic_type(T_ADDRESS), monitors_addr, osr_buf);\n@@ -315,7 +326,1 @@\n-    BasicType bt = type->basic_type();\n-    if (type == TypePtr::NULL_PTR) {\n-      \/\/ Ptr types are mixed together with T_ADDRESS but null is\n-      \/\/ really for T_OBJECT types so correct it.\n-      bt = T_OBJECT;\n-    }\n-    Node *value = fetch_interpreter_state(index, bt, locals_addr, osr_buf);\n+    Node* value = fetch_interpreter_state(index, type, locals_addr, osr_buf);\n@@ -521,1 +526,3 @@\n-    assert(false, \"type flow analysis failed during parsing\");\n+    \/\/ TODO Adding a trap due to an unloaded return type in ciTypeFlow::StateVector::do_invoke\n+    \/\/ can lead to this. Re-enable once 8284443 is fixed.\n+    \/\/assert(false, \"type flow analysis failed during parsing\");\n@@ -612,0 +619,22 @@\n+  \/\/ Handle inline type arguments\n+  int arg_size = method()->arg_size();\n+  for (int i = 0; i < arg_size; i++) {\n+    Node* parm = local(i);\n+    const Type* t = _gvn.type(parm);\n+    if (t->is_inlinetypeptr()) {\n+      \/\/ Create InlineTypeNode from the oop and replace the parameter\n+      bool is_larval = (i == 0) && method()->is_object_constructor() && !method()->holder()->is_java_lang_Object();\n+      Node* vt = InlineTypeNode::make_from_oop(this, parm, t->inline_klass(), !t->maybe_null(), is_larval);\n+      replace_in_map(parm, vt);\n+    } else if (UseTypeSpeculation && (i == (arg_size - 1)) && !is_osr_parse() && method()->has_vararg() &&\n+               t->isa_aryptr() != nullptr && !t->is_aryptr()->is_null_free() && !t->is_aryptr()->is_not_null_free()) {\n+      \/\/ Speculate on varargs Object array being not null-free (and therefore also not flat)\n+      const TypePtr* spec_type = t->speculative();\n+      spec_type = (spec_type != nullptr && spec_type->isa_aryptr() != nullptr) ? spec_type : t->is_aryptr();\n+      spec_type = spec_type->remove_speculative()->is_aryptr()->cast_to_not_null_free();\n+      spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::Offset::bottom, TypeOopPtr::InstanceBot, spec_type);\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), parm, t->join_speculative(spec_type)));\n+      replace_in_map(parm, cast);\n+    }\n+  }\n+\n@@ -797,2 +826,2 @@\n-  if (tf()->range()->cnt() > TypeFunc::Parms) {\n-    const Type* ret_type = tf()->range()->field_at(TypeFunc::Parms);\n+  if (tf()->range_sig()->cnt() > TypeFunc::Parms) {\n+    const Type* ret_type = tf()->range_sig()->field_at(TypeFunc::Parms);\n@@ -820,1 +849,1 @@\n-    assert((int)(tf()->range()->cnt() - TypeFunc::Parms) == ret_size, \"good tf range\");\n+    assert((int)(tf()->range_sig()->cnt() - TypeFunc::Parms) == ret_size, \"good tf range\");\n@@ -827,1 +856,0 @@\n-\n@@ -832,2 +860,2 @@\n-  int        arg_size = tf->domain()->cnt();\n-  int        max_size = MAX2(arg_size, (int)tf->range()->cnt());\n+  int        arg_size = tf->domain_sig()->cnt();\n+  int        max_size = MAX2(arg_size, (int)tf->range_cc()->cnt());\n@@ -836,0 +864,1 @@\n+  jvms->set_map(map);\n@@ -847,3 +876,20 @@\n-  uint i;\n-  for (i = 0; i < (uint)arg_size; i++) {\n-    Node* parm = initial_gvn()->transform(new ParmNode(start, i));\n+  PhaseGVN& gvn = *initial_gvn();\n+  uint i = 0;\n+  int arg_num = 0;\n+  for (uint j = 0; i < (uint)arg_size; i++) {\n+    const Type* t = tf->domain_sig()->field_at(i);\n+    Node* parm = nullptr;\n+    if (t->is_inlinetypeptr() && method()->is_scalarized_arg(arg_num)) {\n+      \/\/ Inline type arguments are not passed by reference: we get an argument per\n+      \/\/ field of the inline type. Build InlineTypeNodes from the inline type arguments.\n+      GraphKit kit(jvms, &gvn);\n+      kit.set_control(map->control());\n+      Node* old_mem = map->memory();\n+      \/\/ Use immutable memory for inline type loads and restore it below\n+      kit.set_all_memory(C->immutable_memory());\n+      parm = InlineTypeNode::make_from_multi(&kit, start, t->inline_klass(), j, \/* in= *\/ true, \/* null_free= *\/ !t->maybe_null());\n+      map->set_control(kit.control());\n+      map->set_memory(old_mem);\n+    } else {\n+      parm = gvn.transform(new ParmNode(start, j++));\n+    }\n@@ -853,0 +899,3 @@\n+    if (i >= TypeFunc::Parms && t != Type::HALF) {\n+      arg_num++;\n+    }\n@@ -859,1 +908,0 @@\n-  jvms->set_map(map);\n@@ -886,1 +934,1 @@\n-  int ret_size = tf()->range()->cnt() - TypeFunc::Parms;\n+  int ret_size = tf()->range_sig()->cnt() - TypeFunc::Parms;\n@@ -890,2 +938,24 @@\n-    ret->add_req(kit.argument(0));\n-    \/\/ Note:  The second dummy edge is not needed by a ReturnNode.\n+    Node* res = kit.argument(0);\n+    if (tf()->returns_inline_type_as_fields()) {\n+      \/\/ Multiple return values (inline type fields): add as many edges\n+      \/\/ to the Return node as returned values.\n+      InlineTypeNode* vt = res->as_InlineType();\n+      ret->add_req_batch(nullptr, tf()->range_cc()->cnt() - TypeFunc::Parms);\n+      if (vt->is_allocated(&kit.gvn()) && !StressCallingConvention) {\n+        ret->init_req(TypeFunc::Parms, vt);\n+      } else {\n+        \/\/ Return the tagged klass pointer to signal scalarization to the caller\n+        Node* tagged_klass = vt->tagged_klass(kit.gvn());\n+        \/\/ Return null if the inline type is null (IsInit field is not set)\n+        Node* conv   = kit.gvn().transform(new ConvI2LNode(vt->get_is_init()));\n+        Node* shl    = kit.gvn().transform(new LShiftLNode(conv, kit.intcon(63)));\n+        Node* shr    = kit.gvn().transform(new RShiftLNode(shl, kit.intcon(63)));\n+        tagged_klass = kit.gvn().transform(new AndLNode(tagged_klass, shr));\n+        ret->init_req(TypeFunc::Parms, tagged_klass);\n+      }\n+      uint idx = TypeFunc::Parms + 1;\n+      vt->pass_fields(&kit, ret, idx, false, false);\n+    } else {\n+      ret->add_req(res);\n+      \/\/ Note:  The second dummy edge is not needed by a ReturnNode.\n+    }\n@@ -1015,1 +1085,1 @@\n-  if (method()->is_object_initializer() &&\n+  if ((method()->is_object_constructor() || method()->is_class_initializer()) &&\n@@ -1043,2 +1113,2 @@\n-  if (tf()->range()->cnt() > TypeFunc::Parms) {\n-    const Type* ret_type = tf()->range()->field_at(TypeFunc::Parms);\n+  if (tf()->range_sig()->cnt() > TypeFunc::Parms) {\n+    const Type* ret_type = tf()->range_sig()->field_at(TypeFunc::Parms);\n@@ -1138,1 +1208,2 @@\n-    kit.null_check_receiver_before_call(method());\n+    Node* receiver = kit.argument(0);\n+    Node* null_free = kit.null_check_receiver_before_call(method());\n@@ -1140,0 +1211,5 @@\n+    if (receiver->is_InlineType() && receiver->as_InlineType()->is_larval()) {\n+      \/\/ Replace the larval inline type receiver in the exit map as well to make sure that\n+      \/\/ we can find and update it in Parse::do_call when we are done with the initialization.\n+      _exits.map()->replace_edge(receiver, null_free);\n+    }\n@@ -1176,1 +1252,1 @@\n-  uint arg_size = tf()->domain()->cnt();\n+  uint arg_size = tf()->domain_sig()->cnt();\n@@ -1250,0 +1326,1 @@\n+      assert(!_gvn.type(lock_obj)->make_oopptr()->can_be_inline_type(), \"can't be an inline type\");\n@@ -1685,0 +1762,36 @@\n+  \/\/ Check for merge conflicts involving inline types\n+  JVMState* old_jvms = map()->jvms();\n+  int old_bci = bci();\n+  JVMState* tmp_jvms = old_jvms->clone_shallow(C);\n+  tmp_jvms->set_should_reexecute(true);\n+  tmp_jvms->bind_map(map());\n+  \/\/ Execution needs to restart a the next bytecode (entry of next\n+  \/\/ block)\n+  if (target->is_merged() ||\n+      pnum > PhiNode::Input ||\n+      target->is_handler() ||\n+      target->is_loop_head()) {\n+    set_parse_bci(target->start());\n+    for (uint j = TypeFunc::Parms; j < map()->req(); j++) {\n+      Node* n = map()->in(j);                 \/\/ Incoming change to target state.\n+      const Type* t = nullptr;\n+      if (tmp_jvms->is_loc(j)) {\n+        t = target->local_type_at(j - tmp_jvms->locoff());\n+      } else if (tmp_jvms->is_stk(j) && j < (uint)sp() + tmp_jvms->stkoff()) {\n+        t = target->stack_type_at(j - tmp_jvms->stkoff());\n+      }\n+      if (t != nullptr && t != Type::BOTTOM) {\n+        if (n->is_InlineType() && !t->is_inlinetypeptr()) {\n+          \/\/ Allocate inline type in src block to be able to merge it with oop in target block\n+          map()->set_req(j, n->as_InlineType()->buffer(this));\n+        } else if (!n->is_InlineType() && t->is_inlinetypeptr()) {\n+          \/\/ Scalarize null in src block to be able to merge it with inline type in target block\n+          assert(gvn().type(n)->is_zero_type(), \"Should have been scalarized\");\n+          map()->set_req(j, InlineTypeNode::make_null(gvn(), t->inline_klass()));\n+        }\n+      }\n+    }\n+  }\n+  old_jvms->bind_map(map());\n+  set_parse_bci(old_bci);\n+\n@@ -1739,0 +1852,1 @@\n+\n@@ -1774,0 +1888,1 @@\n+    bool last_merge = (pnum == PhiNode::Input);\n@@ -1778,1 +1893,1 @@\n-      if (m->is_Phi() && m->as_Phi()->region() == r)\n+      if (m->is_Phi() && m->as_Phi()->region() == r) {\n@@ -1780,1 +1895,3 @@\n-      else\n+      } else if (m->is_InlineType() && m->as_InlineType()->has_phi_inputs(r)) {\n+        phi = m->as_InlineType()->get_oop()->as_Phi();\n+      } else {\n@@ -1782,0 +1899,1 @@\n+      }\n@@ -1828,1 +1946,24 @@\n-      if (phi != nullptr) {\n+      \/\/ Merging two inline types?\n+      if (phi != nullptr && phi->bottom_type()->is_inlinetypeptr()) {\n+        \/\/ Reload current state because it may have been updated by ensure_phi\n+        m = map()->in(j);\n+        InlineTypeNode* vtm = m->as_InlineType(); \/\/ Current inline type\n+        InlineTypeNode* vtn = n->as_InlineType(); \/\/ Incoming inline type\n+        assert(vtm->get_oop() == phi, \"Inline type should have Phi input\");\n+        if (TraceOptoParse) {\n+#ifdef ASSERT\n+          tty->print_cr(\"\\nMerging inline types\");\n+          tty->print_cr(\"Current:\");\n+          vtm->dump(2);\n+          tty->print_cr(\"Incoming:\");\n+          vtn->dump(2);\n+          tty->cr();\n+#endif\n+        }\n+        \/\/ Do the merge\n+        vtm->merge_with(&_gvn, vtn, pnum, last_merge);\n+        if (last_merge) {\n+          map()->set_req(j, _gvn.transform(vtm));\n+          record_for_igvn(vtm);\n+        }\n+      } else if (phi != nullptr) {\n@@ -1832,1 +1973,1 @@\n-        if (pnum == PhiNode::Input) {\n+        if (last_merge) {\n@@ -1848,2 +1989,1 @@\n-    if (pnum == PhiNode::Input &&\n-        !r->in(0)) {         \/\/ The occasional useless Region\n+    if (last_merge && !r->in(0)) {         \/\/ The occasional useless Region\n@@ -2001,0 +2141,2 @@\n+      } else if (n->is_InlineType() && n->as_InlineType()->has_phi_inputs(r)) {\n+        n->as_InlineType()->add_new_path(r);\n@@ -2023,0 +2165,4 @@\n+  InlineTypeNode* vt = o->isa_InlineType();\n+  if (vt != nullptr && vt->has_phi_inputs(region)) {\n+    return vt->get_oop()->as_Phi();\n+  }\n@@ -2042,2 +2188,2 @@\n-  \/\/ is mixing ints and oops or some such.  Forcing it to top\n-  \/\/ makes it go dead.\n+  \/\/ is already dead or is mixing ints and oops or some such.\n+  \/\/ Forcing it to top makes it go dead.\n@@ -2056,5 +2202,14 @@\n-  PhiNode* phi = PhiNode::make(region, o, t);\n-  gvn().set_type(phi, t);\n-  if (C->do_escape_analysis()) record_for_igvn(phi);\n-  map->set_req(idx, phi);\n-  return phi;\n+  if (vt != nullptr && t->is_inlinetypeptr()) {\n+    \/\/ Inline types are merged by merging their field values.\n+    \/\/ Create a cloned InlineTypeNode with phi inputs that\n+    \/\/ represents the merged inline type and update the map.\n+    vt = vt->clone_with_phis(&_gvn, region);\n+    map->set_req(idx, vt);\n+    return vt->get_oop()->as_Phi();\n+  } else {\n+    PhiNode* phi = PhiNode::make(region, o, t);\n+    gvn().set_type(phi, t);\n+    if (C->do_escape_analysis()) record_for_igvn(phi);\n+    map->set_req(idx, phi);\n+    return phi;\n+  }\n@@ -2190,0 +2345,35 @@\n+  \/\/ frame pointer is always same, already captured\n+  if (value != nullptr) {\n+    Node* phi = _exits.argument(0);\n+    const Type* return_type = phi->bottom_type();\n+    const TypeInstPtr* tr = return_type->isa_instptr();\n+    assert(!value->is_InlineType() || !value->as_InlineType()->is_larval(), \"returning a larval\");\n+    if ((tf()->returns_inline_type_as_fields() || (_caller->has_method() && !Compile::current()->inlining_incrementally())) &&\n+        return_type->is_inlinetypeptr()) {\n+      \/\/ Inline type is returned as fields, make sure it is scalarized\n+      if (!value->is_InlineType()) {\n+        value = InlineTypeNode::make_from_oop(this, value, return_type->inline_klass(), false);\n+      }\n+      if (!_caller->has_method() || Compile::current()->inlining_incrementally()) {\n+        \/\/ Returning from root or an incrementally inlined method. Make sure all non-flat\n+        \/\/ fields are buffered and re-execute if allocation triggers deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        assert(tf()->returns_inline_type_as_fields(), \"must be returned as fields\");\n+        jvms()->set_should_reexecute(true);\n+        inc_sp(1);\n+        value = value->as_InlineType()->allocate_fields(this);\n+      }\n+    } else if (value->is_InlineType()) {\n+      \/\/ Inline type is returned as oop, make sure it is buffered and re-execute\n+      \/\/ if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      inc_sp(1);\n+      value = value->as_InlineType()->buffer(this);\n+    }\n+    \/\/ ...else\n+    \/\/ If returning oops to an interface-return, there is a silent free\n+    \/\/ cast from oop to interface allowed by the Verifier. Make it explicit here.\n+    phi->add_req(value);\n+  }\n+\n@@ -2198,0 +2388,1 @@\n+\n@@ -2215,9 +2406,0 @@\n-  \/\/ frame pointer is always same, already captured\n-  if (value != nullptr) {\n-    \/\/ If returning oops to an interface-return, there is a silent free\n-    \/\/ cast from oop to interface allowed by the Verifier.  Make it explicit\n-    \/\/ here.\n-    Node* phi = _exits.argument(0);\n-    phi->add_req(value);\n-  }\n-\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":239,"deletions":57,"binary":false,"changes":296,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciSymbols.hpp\"\n@@ -38,0 +39,2 @@\n+#include \"opto\/idealKit.hpp\"\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -52,0 +55,17 @@\n+Node* Parse::record_profile_for_speculation_at_array_load(Node* ld) {\n+  \/\/ Feed unused profile data to type speculation\n+  if (UseTypeSpeculation && UseArrayLoadStoreProfile) {\n+    ciKlass* array_type = nullptr;\n+    ciKlass* element_type = nullptr;\n+    ProfilePtrKind element_ptr = ProfileMaybeNull;\n+    bool flat_array = true;\n+    bool null_free_array = true;\n+    method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+    if (element_type != nullptr || element_ptr != ProfileMaybeNull) {\n+      ld = record_profile_for_speculation(ld, element_type, element_ptr);\n+    }\n+  }\n+  return ld;\n+}\n+\n+\n@@ -55,1 +75,0 @@\n-  bool big_val = bt == T_DOUBLE || bt == T_LONG;\n@@ -59,2 +78,84 @@\n-  pop();                      \/\/ index (already used)\n-  Node* array = pop();        \/\/ the array itself\n+  Node* array_index = pop();\n+  Node* array = pop();\n+\n+  \/\/ Handle inline type arrays\n+  const TypeOopPtr* element_ptr = elemtype->make_oopptr();\n+  const TypeAryPtr* array_type = _gvn.type(array)->is_aryptr();\n+  if (array_type->is_flat()) {\n+    \/\/ Load from flat inline type array\n+    Node* inline_type;\n+    if (element_ptr->klass_is_exact()) {\n+      inline_type = InlineTypeNode::make_from_flat(this, elemtype->inline_klass(), array, adr);\n+    } else {\n+      \/\/ Element type of flat array is not exact. Therefore, we cannot determine the flat array layout statically.\n+      \/\/ Emit a runtime call to load the element from the flat array.\n+      inline_type = load_from_unknown_flat_array(array, array_index, element_ptr);\n+      inline_type = record_profile_for_speculation_at_array_load(inline_type);\n+    }\n+    push(inline_type);\n+    return;\n+  }\n+\n+  if (!array_type->is_not_flat()) {\n+    \/\/ Cannot statically determine if array is a flat array, emit runtime check\n+    assert(UseArrayFlattening && is_reference_type(bt) && element_ptr->can_be_inline_type() && !array_type->is_not_null_free() &&\n+           (!element_ptr->is_inlinetypeptr() || element_ptr->inline_klass()->flat_in_array()), \"array can't be flat\");\n+    IdealKit ideal(this);\n+    IdealVariable res(ideal);\n+    ideal.declarations_done();\n+    ideal.if_then(flat_array_test(array, \/* flat = *\/ false)); {\n+      \/\/ Non-flat array\n+      assert(ideal.ctrl()->in(0)->as_If()->is_flat_array_check(&_gvn), \"Should be found\");\n+      sync_kit(ideal);\n+      const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);\n+      DecoratorSet decorator_set = IN_HEAP | IS_ARRAY | C2_CONTROL_DEPENDENT_LOAD;\n+      if (needs_range_check(array_type->size(), array_index)) {\n+        \/\/ We've emitted a RangeCheck but now insert an additional check between the range check and the actual load.\n+        \/\/ We cannot pin the load to two separate nodes. Instead, we pin it conservatively here such that it cannot\n+        \/\/ possibly float above the range check at any point.\n+        decorator_set |= C2_UNKNOWN_CONTROL_LOAD;\n+      }\n+      Node* ld = access_load_at(array, adr, adr_type, element_ptr, bt, decorator_set);\n+      if (element_ptr->is_inlinetypeptr()) {\n+        assert(element_ptr->maybe_null(), \"null free array should be handled above\");\n+        ld = InlineTypeNode::make_from_oop(this, ld, element_ptr->inline_klass(), false);\n+      }\n+      ideal.sync_kit(this);\n+      ideal.set(res, ld);\n+    } ideal.else_(); {\n+      \/\/ Flat array\n+      sync_kit(ideal);\n+      if (element_ptr->is_inlinetypeptr()) {\n+        \/\/ Element type is known, cast and load from flat array layout.\n+        ciInlineKlass* vk = element_ptr->inline_klass();\n+        assert(vk->flat_in_array() && element_ptr->maybe_null(), \"never\/always flat - should be optimized\");\n+        ciArrayKlass* array_klass = ciArrayKlass::make(vk, \/* null_free *\/ true);\n+        const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();\n+        Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, arytype));\n+        Node* casted_adr = array_element_address(cast, array_index, T_OBJECT, array_type->size(), control());\n+        \/\/ Re-execute flat array load if buffering triggers deoptimization\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        inc_sp(2);\n+        Node* vt = InlineTypeNode::make_from_flat(this, vk, cast, casted_adr)->buffer(this, false);\n+        ideal.set(res, vt);\n+        ideal.sync_kit(this);\n+      } else {\n+        \/\/ Element type is unknown, and thus we cannot statically determine the exact flat array layout. Emit a\n+        \/\/ runtime call to correctly load the inline type element from the flat array.\n+        Node* inline_type = load_from_unknown_flat_array(array, array_index, element_ptr);\n+        ideal.sync_kit(this);\n+        ideal.set(res, inline_type);\n+      }\n+    } ideal.end_if();\n+    sync_kit(ideal);\n+    Node* ld = _gvn.transform(ideal.value(res));\n+    ld = record_profile_for_speculation_at_array_load(ld);\n+    push_node(bt, ld);\n+    return;\n+  }\n+\n+  if (array_type->is_null_free()) {\n+    \/\/ Load from non-flat inline type array (elements can never be null)\n+    bt = T_OBJECT;\n+  }\n@@ -66,1 +167,0 @@\n-\n@@ -69,4 +169,5 @@\n-  if (big_val) {\n-    push_pair(ld);\n-  } else {\n-    push(ld);\n+  ld = record_profile_for_speculation_at_array_load(ld);\n+  \/\/ Loading an inline type from a non-flat array\n+  if (element_ptr != nullptr && element_ptr->is_inlinetypeptr()) {\n+    assert(!array_type->is_null_free() || !element_ptr->maybe_null(), \"inline type array elements should never be null\");\n+    ld = InlineTypeNode::make_from_oop(this, ld, element_ptr->inline_klass(), !element_ptr->maybe_null());\n@@ -74,0 +175,1 @@\n+  push_node(bt, ld);\n@@ -76,0 +178,28 @@\n+Node* Parse::load_from_unknown_flat_array(Node* array, Node* array_index, const TypeOopPtr* element_ptr) {\n+  \/\/ Below membars keep this access to an unknown flat array correctly\n+  \/\/ ordered with other unknown and known flat array accesses.\n+  insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+\n+  Node* call = nullptr;\n+  {\n+    \/\/ Re-execute flat array load if runtime call triggers deoptimization\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_bci(_bci);\n+    jvms()->set_should_reexecute(true);\n+    inc_sp(2);\n+    kill_dead_locals();\n+    call = make_runtime_call(RC_NO_LEAF | RC_NO_IO,\n+                             OptoRuntime::load_unknown_inline_Type(),\n+                             OptoRuntime::load_unknown_inline_Java(),\n+                             nullptr, TypeRawPtr::BOTTOM,\n+                             array, array_index);\n+  }\n+  make_slow_call_ex(call, env()->Throwable_klass(), false);\n+  Node* buffer = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+\n+  insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+\n+  \/\/ Keep track of the information that the inline type is in flat arrays\n+  const Type* unknown_value = element_ptr->is_instptr()->cast_to_flat_in_array();\n+  return _gvn.transform(new CheckCastPPNode(control(), buffer, unknown_value));\n+}\n@@ -80,2 +210,1 @@\n-  bool big_val = bt == T_DOUBLE || bt == T_LONG;\n-  Node* adr = array_addressing(bt, big_val ? 2 : 1, elemtype);\n+  Node* adr = array_addressing(bt, type2size[bt], elemtype);\n@@ -83,0 +212,1 @@\n+  Node* stored_value_casted = nullptr;\n@@ -84,1 +214,1 @@\n-    array_store_check();\n+    stored_value_casted = array_store_check(adr, elemtype);\n@@ -89,8 +219,6 @@\n-  Node* val;                  \/\/ Oop to store\n-  if (big_val) {\n-    val = pop_pair();\n-  } else {\n-    val = pop();\n-  }\n-  pop();                      \/\/ index (already used)\n-  Node* array = pop();        \/\/ the array itself\n+  Node* const stored_value = pop_node(bt); \/\/ Value to store\n+  Node* const array_index = pop();         \/\/ Index in the array\n+  Node* array = pop();                     \/\/ The array itself\n+\n+  const TypeAryPtr* array_type = _gvn.type(array)->is_aryptr();\n+  const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);\n@@ -100,0 +228,115 @@\n+  } else if (bt == T_OBJECT) {\n+    elemtype = elemtype->make_oopptr();\n+    const Type* stored_value_casted_type = _gvn.type(stored_value_casted);\n+    \/\/ Based on the value to be stored, try to determine if the array is not null-free and\/or not flat.\n+    \/\/ This is only legal for non-null stores because the array_store_check always passes for null, even\n+    \/\/ if the array is null-free. Null stores are handled in GraphKit::inline_array_null_guard().\n+    bool not_null_free = !stored_value_casted_type->maybe_null() &&\n+                         !stored_value_casted_type->is_oopptr()->can_be_inline_type();\n+    bool not_flat = not_null_free || (stored_value_casted_type->is_inlinetypeptr() &&\n+                                      !stored_value_casted_type->inline_klass()->flat_in_array());\n+    if (!array_type->is_not_null_free() && not_null_free) {\n+      \/\/ Storing a non-inline type, mark array as not null-free (-> not flat).\n+      array_type = array_type->cast_to_not_null_free();\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, array_type));\n+      replace_in_map(array, cast);\n+      array = cast;\n+    } else if (!array_type->is_not_flat() && not_flat) {\n+      \/\/ Storing to a non-flat array, mark array as not flat.\n+      array_type = array_type->cast_to_not_flat();\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, array_type));\n+      replace_in_map(array, cast);\n+      array = cast;\n+    }\n+\n+    if (array_type->is_flat()) {\n+      \/\/ Store to flat inline type array\n+      assert(!stored_value_casted_type->maybe_null(), \"should be guaranteed by array store check\");\n+      if (array_type->klass_is_exact()) {\n+        \/\/ Store to exact flat inline type array where we know the flat array layout statically.\n+        \/\/ Re-execute flat array store if buffering triggers deoptimization\n+        PreserveReexecuteState preexecs(this);\n+        inc_sp(3);\n+        jvms()->set_should_reexecute(true);\n+        stored_value_casted->as_InlineType()->store_flat(this, array, adr, nullptr, 0, false, -1, MO_UNORDERED | IN_HEAP | IS_ARRAY);\n+      } else {\n+        \/\/ Element type of flat array is not exact. Therefore, we cannot determine the flat array layout statically.\n+        \/\/ Emit a runtime call to store the element to the flat array.\n+        store_to_unknown_flat_array(array, array_index, stored_value_casted);\n+      }\n+      return;\n+    }\n+    if (array_type->is_null_free()) {\n+      \/\/ Store to non-flat null-free inline type array (elements can never be null)\n+      assert(!stored_value_casted_type->maybe_null(), \"should be guaranteed by array store check\");\n+      if (elemtype->inline_klass()->is_empty()) {\n+        \/\/ Ignore empty inline stores, array is already initialized.\n+        return;\n+      }\n+    } else if (!array_type->is_not_flat() && (stored_value_casted_type != TypePtr::NULL_PTR || StressReflectiveCode)) {\n+      \/\/ Array might be a flat array, emit runtime checks (for nullptr, a simple inline_array_null_guard is sufficient).\n+      assert(UseArrayFlattening && !not_flat && elemtype->is_oopptr()->can_be_inline_type() &&\n+             !array_type->klass_is_exact() && !array_type->is_not_null_free(), \"array can't be a flat array\");\n+      IdealKit ideal(this);\n+      ideal.if_then(flat_array_test(array, \/* flat = *\/ false)); {\n+        \/\/ Non-flat array\n+        assert(ideal.ctrl()->in(0)->as_If()->is_flat_array_check(&_gvn), \"Should be found\");\n+        sync_kit(ideal);\n+        Node* cast_array = inline_array_null_guard(array, stored_value_casted, 3);\n+        inc_sp(3);\n+        access_store_at(cast_array, adr, adr_type, stored_value_casted, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY, false);\n+        dec_sp(3);\n+        ideal.sync_kit(this);\n+      } ideal.else_(); {\n+        sync_kit(ideal);\n+        \/\/ flat array\n+        Node* null_ctl = top();\n+        Node* null_checked_stored_value_casted = null_check_oop(stored_value_casted, &null_ctl);\n+        if (null_ctl != top()) {\n+          PreserveJVMState pjvms(this);\n+          inc_sp(3);\n+          set_control(null_ctl);\n+          uncommon_trap(Deoptimization::Reason_null_check, Deoptimization::Action_none);\n+          dec_sp(3);\n+        }\n+        \/\/ Try to determine the inline klass\n+        ciInlineKlass* inline_Klass = nullptr;\n+        if (stored_value_casted_type->is_inlinetypeptr()) {\n+          inline_Klass = stored_value_casted_type->inline_klass();\n+        } else if (elemtype->is_inlinetypeptr()) {\n+          inline_Klass = elemtype->inline_klass();\n+        }\n+        if (!stopped()) {\n+          if (inline_Klass != nullptr) {\n+            \/\/ Element type is known, cast and store to flat array layout.\n+            assert(inline_Klass->flat_in_array() && elemtype->maybe_null(), \"never\/always flat - should be optimized\");\n+            ciArrayKlass* array_klass = ciArrayKlass::make(inline_Klass, \/* null_free *\/ true);\n+            const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();\n+            Node* casted_array = _gvn.transform(new CheckCastPPNode(control(), array, arytype));\n+            Node* casted_adr = array_element_address(casted_array, array_index, T_OBJECT, arytype->size(), control());\n+            if (!null_checked_stored_value_casted->is_InlineType()) {\n+              assert(!gvn().type(null_checked_stored_value_casted)->maybe_null(),\n+                     \"inline type array elements should never be null\");\n+              null_checked_stored_value_casted = InlineTypeNode::make_from_oop(this, null_checked_stored_value_casted,\n+                                                                               inline_Klass);\n+            }\n+            \/\/ Re-execute flat array store if buffering triggers deoptimization\n+            PreserveReexecuteState preexecs(this);\n+            inc_sp(3);\n+            jvms()->set_should_reexecute(true);\n+            null_checked_stored_value_casted->as_InlineType()->store_flat(this, casted_array, casted_adr, nullptr, 0, false, -1, MO_UNORDERED | IN_HEAP | IS_ARRAY);\n+          } else {\n+            \/\/ Element type is unknown, emit a runtime call since the flat array layout is not statically known.\n+            store_to_unknown_flat_array(array, array_index, null_checked_stored_value_casted);\n+          }\n+        }\n+        ideal.sync_kit(this);\n+      }\n+      ideal.end_if();\n+      sync_kit(ideal);\n+      return;\n+    } else if (!array_type->is_not_null_free()) {\n+      \/\/ Array is not flat but may be null free\n+      assert(elemtype->is_oopptr()->can_be_inline_type(), \"array can't be null-free\");\n+      array = inline_array_null_guard(array, stored_value_casted, 3, true);\n+    }\n@@ -101,3 +344,3 @@\n-  const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);\n-\n-  access_store_at(array, adr, adr_type, val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY);\n+  inc_sp(3);\n+  access_store_at(array, adr, adr_type, stored_value, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY);\n+  dec_sp(3);\n@@ -106,0 +349,25 @@\n+\/\/ Emit a runtime call to store to a flat array whose element type is either unknown (i.e. we do not know the flat\n+\/\/ array layout) or not exact (could have different flat array layouts at runtime).\n+void Parse::store_to_unknown_flat_array(Node* array, Node* const idx, Node* non_null_stored_value) {\n+  \/\/ Below membars keep this access to an unknown flat array correctly\n+  \/\/ ordered with other unknown and known flat array accesses.\n+  insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+\n+  Node* call = nullptr;\n+  {\n+    \/\/ Re-execute flat array store if runtime call triggers deoptimization\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_bci(_bci);\n+    jvms()->set_should_reexecute(true);\n+    inc_sp(3);\n+    kill_dead_locals();\n+    call = make_runtime_call(RC_NO_LEAF | RC_NO_IO,\n+                      OptoRuntime::store_unknown_inline_Type(),\n+                      OptoRuntime::store_unknown_inline_Java(),\n+                      nullptr, TypeRawPtr::BOTTOM,\n+                      non_null_stored_value, array, idx);\n+  }\n+  make_slow_call_ex(call, env()->Throwable_klass(), false);\n+\n+  insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+}\n@@ -134,11 +402,0 @@\n-  \/\/ Check for big class initializers with all constant offsets\n-  \/\/ feeding into a known-size array.\n-  const TypeInt* idxtype = _gvn.type(idx)->is_int();\n-  \/\/ See if the highest idx value is less than the lowest array bound,\n-  \/\/ and if the idx value cannot be negative:\n-  bool need_range_check = true;\n-  if (idxtype->_hi < sizetype->_lo && idxtype->_lo >= 0) {\n-    need_range_check = false;\n-    if (C->log() != nullptr)   C->log()->elem(\"observe that='!need_range_check'\");\n-  }\n-\n@@ -156,12 +413,1 @@\n-  \/\/ Do the range check\n-  if (need_range_check) {\n-    Node* tst;\n-    if (sizetype->_hi <= 0) {\n-      \/\/ The greatest array bound is negative, so we can conclude that we're\n-      \/\/ compiling unreachable code, but the unsigned compare trick used below\n-      \/\/ only works with non-negative lengths.  Instead, hack \"tst\" to be zero so\n-      \/\/ the uncommon_trap path will always be taken.\n-      tst = _gvn.intcon(0);\n-    } else {\n-      \/\/ Range is constant in array-oop, so we can use the original state of mem\n-      Node* len = load_array_length(ary);\n+  ary = create_speculative_inline_type_array_checks(ary, arytype, elemtype);\n@@ -169,31 +415,4 @@\n-      \/\/ Test length vs index (standard trick using unsigned compare)\n-      Node* chk = _gvn.transform( new CmpUNode(idx, len) );\n-      BoolTest::mask btest = BoolTest::lt;\n-      tst = _gvn.transform( new BoolNode(chk, btest) );\n-    }\n-    RangeCheckNode* rc = new RangeCheckNode(control(), tst, PROB_MAX, COUNT_UNKNOWN);\n-    _gvn.set_type(rc, rc->Value(&_gvn));\n-    if (!tst->is_Con()) {\n-      record_for_igvn(rc);\n-    }\n-    set_control(_gvn.transform(new IfTrueNode(rc)));\n-    \/\/ Branch to failure if out of bounds\n-    {\n-      PreserveJVMState pjvms(this);\n-      set_control(_gvn.transform(new IfFalseNode(rc)));\n-      if (C->allow_range_check_smearing()) {\n-        \/\/ Do not use builtin_throw, since range checks are sometimes\n-        \/\/ made more stringent by an optimistic transformation.\n-        \/\/ This creates \"tentative\" range checks at this point,\n-        \/\/ which are not guaranteed to throw exceptions.\n-        \/\/ See IfNode::Ideal, is_range_check, adjust_check.\n-        uncommon_trap(Deoptimization::Reason_range_check,\n-                      Deoptimization::Action_make_not_entrant,\n-                      nullptr, \"range_check\");\n-      } else {\n-        \/\/ If we have already recompiled with the range-check-widening\n-        \/\/ heroic optimization turned off, then we must really be throwing\n-        \/\/ range check exceptions.\n-        builtin_throw(Deoptimization::Reason_range_check);\n-      }\n-    }\n+  if (needs_range_check(sizetype, idx)) {\n+    create_range_check(idx, ary, sizetype);\n+  } else if (C->log() != nullptr) {\n+    C->log()->elem(\"observe that='!need_range_check'\");\n@@ -201,0 +420,1 @@\n+\n@@ -212,0 +432,200 @@\n+\/\/ Check if we need a range check for an array access. This is the case if the index is either negative or if it could\n+\/\/ be greater or equal the smallest possible array size (i.e. out-of-bounds).\n+bool Parse::needs_range_check(const TypeInt* size_type, const Node* index) const {\n+  const TypeInt* index_type = _gvn.type(index)->is_int();\n+  return index_type->_hi >= size_type->_lo || index_type->_lo < 0;\n+}\n+\n+void Parse::create_range_check(Node* idx, Node* ary, const TypeInt* sizetype) {\n+  Node* tst;\n+  if (sizetype->_hi <= 0) {\n+    \/\/ The greatest array bound is negative, so we can conclude that we're\n+    \/\/ compiling unreachable code, but the unsigned compare trick used below\n+    \/\/ only works with non-negative lengths.  Instead, hack \"tst\" to be zero so\n+    \/\/ the uncommon_trap path will always be taken.\n+    tst = _gvn.intcon(0);\n+  } else {\n+    \/\/ Range is constant in array-oop, so we can use the original state of mem\n+    Node* len = load_array_length(ary);\n+\n+    \/\/ Test length vs index (standard trick using unsigned compare)\n+    Node* chk = _gvn.transform(new CmpUNode(idx, len) );\n+    BoolTest::mask btest = BoolTest::lt;\n+    tst = _gvn.transform(new BoolNode(chk, btest) );\n+  }\n+  RangeCheckNode* rc = new RangeCheckNode(control(), tst, PROB_MAX, COUNT_UNKNOWN);\n+  _gvn.set_type(rc, rc->Value(&_gvn));\n+  if (!tst->is_Con()) {\n+    record_for_igvn(rc);\n+  }\n+  set_control(_gvn.transform(new IfTrueNode(rc)));\n+  \/\/ Branch to failure if out of bounds\n+  {\n+    PreserveJVMState pjvms(this);\n+    set_control(_gvn.transform(new IfFalseNode(rc)));\n+    if (C->allow_range_check_smearing()) {\n+      \/\/ Do not use builtin_throw, since range checks are sometimes\n+      \/\/ made more stringent by an optimistic transformation.\n+      \/\/ This creates \"tentative\" range checks at this point,\n+      \/\/ which are not guaranteed to throw exceptions.\n+      \/\/ See IfNode::Ideal, is_range_check, adjust_check.\n+      uncommon_trap(Deoptimization::Reason_range_check,\n+                    Deoptimization::Action_make_not_entrant,\n+                    nullptr, \"range_check\");\n+    } else {\n+      \/\/ If we have already recompiled with the range-check-widening\n+      \/\/ heroic optimization turned off, then we must really be throwing\n+      \/\/ range check exceptions.\n+      builtin_throw(Deoptimization::Reason_range_check);\n+    }\n+  }\n+}\n+\n+\/\/ For inline type arrays, we can use the profiling information for array accesses to speculate on the type, flatness,\n+\/\/ and null-freeness. We can either prepare the speculative type for later uses or emit explicit speculative checks with\n+\/\/ traps now. In the latter case, the speculative type guarantees can avoid additional runtime checks later (e.g.\n+\/\/ non-null-free implies non-flat which allows us to remove flatness checks). This makes the graph simpler.\n+Node* Parse::create_speculative_inline_type_array_checks(Node* array, const TypeAryPtr* array_type,\n+                                                         const Type*& element_type) {\n+  if (!array_type->is_flat() && !array_type->is_not_flat()) {\n+    \/\/ For arrays that might be flat, speculate that the array has the exact type reported in the profile data such that\n+    \/\/ we can rely on a fixed memory layout (i.e. either a flat layout or not).\n+    array = cast_to_speculative_array_type(array, array_type, element_type);\n+  } else if (UseTypeSpeculation && UseArrayLoadStoreProfile) {\n+    \/\/ Array is known to be either flat or not flat. If possible, update the speculative type by using the profile data\n+    \/\/ at this bci.\n+    array = cast_to_profiled_array_type(array);\n+  }\n+\n+  \/\/ Even though the type does not tell us whether we have an inline type array or not, we can still check the profile data\n+  \/\/ whether we have a non-null-free or non-flat array. Since non-null-free implies non-flat, we check this first.\n+  \/\/ Speculating on a non-null-free array doesn't help aaload but could be profitable for a subsequent aastore.\n+  if (!array_type->is_null_free() && !array_type->is_not_null_free()) {\n+    array = speculate_non_null_free_array(array, array_type);\n+  }\n+\n+  if (!array_type->is_flat() && !array_type->is_not_flat()) {\n+    array = speculate_non_flat_array(array, array_type);\n+  }\n+  return array;\n+}\n+\n+\/\/ Speculate that the array has the exact type reported in the profile data. We emit a trap when this turns out to be\n+\/\/ wrong. On the fast path, we add a CheckCastPP to use the exact type.\n+Node* Parse::cast_to_speculative_array_type(Node* const array, const TypeAryPtr*& array_type, const Type*& element_type) {\n+  Deoptimization::DeoptReason reason = Deoptimization::Reason_speculate_class_check;\n+  ciKlass* speculative_array_type = array_type->speculative_type();\n+  if (too_many_traps_or_recompiles(reason) || speculative_array_type == nullptr) {\n+    \/\/ No speculative type, check profile data at this bci\n+    speculative_array_type = nullptr;\n+    reason = Deoptimization::Reason_class_check;\n+    if (UseArrayLoadStoreProfile && !too_many_traps_or_recompiles(reason)) {\n+      ciKlass* profiled_element_type = nullptr;\n+      ProfilePtrKind element_ptr = ProfileMaybeNull;\n+      bool flat_array = true;\n+      bool null_free_array = true;\n+      method()->array_access_profiled_type(bci(), speculative_array_type, profiled_element_type, element_ptr, flat_array,\n+                                           null_free_array);\n+    }\n+  }\n+  if (speculative_array_type != nullptr) {\n+    \/\/ Speculate that this array has the exact type reported by profile data\n+    Node* casted_array = nullptr;\n+    DEBUG_ONLY(Node* old_control = control();)\n+    Node* slow_ctl = type_check_receiver(array, speculative_array_type, 1.0, &casted_array);\n+    if (stopped()) {\n+      \/\/ The check always fails and therefore profile information is incorrect. Don't use it.\n+      assert(old_control == slow_ctl, \"type check should have been removed\");\n+      set_control(slow_ctl);\n+    } else if (!slow_ctl->is_top()) {\n+      { PreserveJVMState pjvms(this);\n+        set_control(slow_ctl);\n+        uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+      }\n+      replace_in_map(array, casted_array);\n+      array_type = _gvn.type(casted_array)->is_aryptr();\n+      element_type = array_type->elem();\n+      return casted_array;\n+    }\n+  }\n+  return array;\n+}\n+\n+\/\/ Create a CheckCastPP when the speculative type can improve the current type.\n+Node* Parse::cast_to_profiled_array_type(Node* const array) {\n+  ciKlass* array_type = nullptr;\n+  ciKlass* element_type = nullptr;\n+  ProfilePtrKind element_ptr = ProfileMaybeNull;\n+  bool flat_array = true;\n+  bool null_free_array = true;\n+  method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+  if (array_type != nullptr) {\n+    return record_profile_for_speculation(array, array_type, ProfileMaybeNull);\n+  }\n+  return array;\n+}\n+\n+\/\/ Speculate that the array is non-null-free. This will imply non-flatness. We emit a trap when this turns out to be\n+\/\/ wrong. On the fast path, we add a CheckCastPP to use the non-null-free type.\n+Node* Parse::speculate_non_null_free_array(Node* const array, const TypeAryPtr*& array_type) {\n+  bool null_free_array = true;\n+  Deoptimization::DeoptReason reason = Deoptimization::Reason_none;\n+  if (array_type->speculative() != nullptr &&\n+      array_type->speculative()->is_aryptr()->is_not_null_free() &&\n+      !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+    null_free_array = false;\n+    reason = Deoptimization::Reason_speculate_class_check;\n+  } else if (UseArrayLoadStoreProfile && !too_many_traps_or_recompiles(Deoptimization::Reason_class_check)) {\n+    ciKlass* profiled_array_type = nullptr;\n+    ciKlass* profiled_element_type = nullptr;\n+    ProfilePtrKind element_ptr = ProfileMaybeNull;\n+    bool flat_array = true;\n+    method()->array_access_profiled_type(bci(), profiled_array_type, profiled_element_type, element_ptr, flat_array,\n+                                         null_free_array);\n+    reason = Deoptimization::Reason_class_check;\n+  }\n+  if (!null_free_array) {\n+    { \/\/ Deoptimize if null-free array\n+      BuildCutout unless(this, null_free_array_test(array, \/* null_free = *\/ false), PROB_MAX);\n+      uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+    }\n+    assert(!stopped(), \"null-free array should have been caught earlier\");\n+    Node* casted_array = _gvn.transform(new CheckCastPPNode(control(), array, array_type->cast_to_not_null_free()));\n+    replace_in_map(array, casted_array);\n+    array_type = _gvn.type(casted_array)->is_aryptr();\n+    return casted_array;\n+  }\n+  return array;\n+}\n+\n+\/\/ Speculate that the array is non-flat. We emit a trap when this turns out to be wrong. On the fast path, we add a\n+\/\/ CheckCastPP to use the non-flat type.\n+Node* Parse::speculate_non_flat_array(Node* const array, const TypeAryPtr* const array_type) {\n+  bool flat_array = true;\n+  Deoptimization::DeoptReason reason = Deoptimization::Reason_none;\n+  if (array_type->speculative() != nullptr &&\n+      array_type->speculative()->is_aryptr()->is_not_flat() &&\n+      !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+    flat_array = false;\n+    reason = Deoptimization::Reason_speculate_class_check;\n+  } else if (UseArrayLoadStoreProfile && !too_many_traps_or_recompiles(reason)) {\n+    ciKlass* profiled_array_type = nullptr;\n+    ciKlass* profiled_element_type = nullptr;\n+    ProfilePtrKind element_ptr = ProfileMaybeNull;\n+    bool null_free_array = true;\n+    method()->array_access_profiled_type(bci(), profiled_array_type, profiled_element_type, element_ptr, flat_array,\n+                                         null_free_array);\n+    reason = Deoptimization::Reason_class_check;\n+  }\n+  if (!flat_array) {\n+    { \/\/ Deoptimize if flat array\n+      BuildCutout unless(this, flat_array_test(array, \/* flat = *\/ false), PROB_MAX);\n+      uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+    }\n+    assert(!stopped(), \"flat array should have been caught earlier\");\n+    Node* casted_array = _gvn.transform(new CheckCastPPNode(control(), array, array_type->cast_to_not_flat()));\n+    replace_in_map(array, casted_array);\n+    return casted_array;\n+  }\n+  return array;\n+}\n@@ -1448,1 +1868,1 @@\n-void Parse::do_if(BoolTest::mask btest, Node* c) {\n+void Parse::do_if(BoolTest::mask btest, Node* c, bool can_trap, bool new_path, Node** ctrl_taken) {\n@@ -1539,2 +1959,2 @@\n-      if (C->eliminate_boxing()) {\n-        \/\/ Mark the successor block as parsed\n+      if (C->eliminate_boxing() && !new_path) {\n+        \/\/ Mark the successor block as parsed (if we haven't created a new path)\n@@ -1544,1 +1964,1 @@\n-      adjust_map_after_if(taken_btest, c, prob, branch_block);\n+      adjust_map_after_if(taken_btest, c, prob, branch_block, can_trap);\n@@ -1546,1 +1966,9 @@\n-        merge(target_bci);\n+        if (new_path) {\n+          \/\/ Merge by using a new path\n+          merge_new_path(target_bci);\n+        } else if (ctrl_taken != nullptr) {\n+          \/\/ Don't merge but save taken branch to be wired by caller\n+          *ctrl_taken = control();\n+        } else {\n+          merge(target_bci);\n+        }\n@@ -1555,1 +1983,1 @@\n-  if (stopped()) {\n+  if (stopped() && ctrl_taken == nullptr) {\n@@ -1557,1 +1985,1 @@\n-      \/\/ Mark the successor block as parsed\n+      \/\/ Mark the successor block as parsed (if caller does not re-wire control flow)\n@@ -1561,1 +1989,1 @@\n-    adjust_map_after_if(untaken_btest, c, untaken_prob, next_block);\n+    adjust_map_after_if(untaken_btest, c, untaken_prob, next_block, can_trap);\n@@ -1569,0 +1997,404 @@\n+\n+static ProfilePtrKind speculative_ptr_kind(const TypeOopPtr* t) {\n+  if (t->speculative() == nullptr) {\n+    return ProfileUnknownNull;\n+  }\n+  if (t->speculative_always_null()) {\n+    return ProfileAlwaysNull;\n+  }\n+  if (t->speculative_maybe_null()) {\n+    return ProfileMaybeNull;\n+  }\n+  return ProfileNeverNull;\n+}\n+\n+void Parse::acmp_always_null_input(Node* input, const TypeOopPtr* tinput, BoolTest::mask btest, Node* eq_region) {\n+  inc_sp(2);\n+  Node* cast = null_check_common(input, T_OBJECT, true, nullptr,\n+                                 !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check) &&\n+                                 speculative_ptr_kind(tinput) == ProfileAlwaysNull);\n+  dec_sp(2);\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      replace_in_map(input, cast);\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+  } else {\n+    replace_in_map(input, cast);\n+  }\n+}\n+\n+Node* Parse::acmp_null_check(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, Node*& null_ctl) {\n+  inc_sp(2);\n+  null_ctl = top();\n+  Node* cast = null_check_oop(input, &null_ctl,\n+                              input_ptr == ProfileNeverNull || (input_ptr == ProfileUnknownNull && !too_many_traps_or_recompiles(Deoptimization::Reason_null_check)),\n+                              false,\n+                              speculative_ptr_kind(tinput) == ProfileNeverNull &&\n+                              !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check));\n+  dec_sp(2);\n+  assert(!stopped(), \"null input should have been caught earlier\");\n+  return cast;\n+}\n+\n+void Parse::acmp_known_non_inline_type_input(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, ciKlass* input_type, BoolTest::mask btest, Node* eq_region) {\n+  Node* ne_region = new RegionNode(1);\n+  Node* null_ctl;\n+  Node* cast = acmp_null_check(input, tinput, input_ptr, null_ctl);\n+  ne_region->add_req(null_ctl);\n+\n+  Node* slow_ctl = type_check_receiver(cast, input_type, 1.0, &cast);\n+  {\n+    PreserveJVMState pjvms(this);\n+    inc_sp(2);\n+    set_control(slow_ctl);\n+    Deoptimization::DeoptReason reason;\n+    if (tinput->speculative_type() != nullptr && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+      reason = Deoptimization::Reason_speculate_class_check;\n+    } else {\n+      reason = Deoptimization::Reason_class_check;\n+    }\n+    uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+  }\n+  ne_region->add_req(control());\n+\n+  record_for_igvn(ne_region);\n+  set_control(_gvn.transform(ne_region));\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      if (null_ctl == top()) {\n+        replace_in_map(input, cast);\n+      }\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+  } else {\n+    if (null_ctl == top()) {\n+      replace_in_map(input, cast);\n+    }\n+    set_control(_gvn.transform(ne_region));\n+  }\n+}\n+\n+void Parse::acmp_unknown_non_inline_type_input(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, BoolTest::mask btest, Node* eq_region) {\n+  Node* ne_region = new RegionNode(1);\n+  Node* null_ctl;\n+  Node* cast = acmp_null_check(input, tinput, input_ptr, null_ctl);\n+  ne_region->add_req(null_ctl);\n+\n+  {\n+    BuildCutout unless(this, inline_type_test(cast, \/* is_inline = *\/ false), PROB_MAX);\n+    inc_sp(2);\n+    uncommon_trap_exact(Deoptimization::Reason_class_check, Deoptimization::Action_maybe_recompile);\n+  }\n+\n+  ne_region->add_req(control());\n+\n+  record_for_igvn(ne_region);\n+  set_control(_gvn.transform(ne_region));\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      if (null_ctl == top()) {\n+        replace_in_map(input, cast);\n+      }\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+  } else {\n+    if (null_ctl == top()) {\n+      replace_in_map(input, cast);\n+    }\n+    set_control(_gvn.transform(ne_region));\n+  }\n+}\n+\n+void Parse::do_acmp(BoolTest::mask btest, Node* left, Node* right) {\n+  ciKlass* left_type = nullptr;\n+  ciKlass* right_type = nullptr;\n+  ProfilePtrKind left_ptr = ProfileUnknownNull;\n+  ProfilePtrKind right_ptr = ProfileUnknownNull;\n+  bool left_inline_type = true;\n+  bool right_inline_type = true;\n+\n+  \/\/ Leverage profiling at acmp\n+  if (UseACmpProfile) {\n+    method()->acmp_profiled_type(bci(), left_type, right_type, left_ptr, right_ptr, left_inline_type, right_inline_type);\n+    if (too_many_traps_or_recompiles(Deoptimization::Reason_class_check)) {\n+      left_type = nullptr;\n+      right_type = nullptr;\n+      left_inline_type = true;\n+      right_inline_type = true;\n+    }\n+    if (too_many_traps_or_recompiles(Deoptimization::Reason_null_check)) {\n+      left_ptr = ProfileUnknownNull;\n+      right_ptr = ProfileUnknownNull;\n+    }\n+  }\n+\n+  if (UseTypeSpeculation) {\n+    record_profile_for_speculation(left, left_type, left_ptr);\n+    record_profile_for_speculation(right, right_type, right_ptr);\n+  }\n+\n+  if (!EnableValhalla) {\n+    Node* cmp = CmpP(left, right);\n+    cmp = optimize_cmp_with_klass(cmp);\n+    do_if(btest, cmp);\n+    return;\n+  }\n+\n+  \/\/ Check for equality before potentially allocating\n+  if (left == right) {\n+    do_if(btest, makecon(TypeInt::CC_EQ));\n+    return;\n+  }\n+\n+  \/\/ Allocate inline type operands and re-execute on deoptimization\n+  if (left->is_InlineType()) {\n+    if (_gvn.type(right)->is_zero_type() ||\n+        (right->is_InlineType() && _gvn.type(right->as_InlineType()->get_is_init())->is_zero_type())) {\n+      \/\/ Null checking a scalarized but nullable inline type. Check the IsInit\n+      \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+      Node* cmp = CmpI(left->as_InlineType()->get_is_init(), intcon(0));\n+      do_if(btest, cmp);\n+      return;\n+    } else {\n+      PreserveReexecuteState preexecs(this);\n+      inc_sp(2);\n+      jvms()->set_should_reexecute(true);\n+      left = left->as_InlineType()->buffer(this)->get_oop();\n+    }\n+  }\n+  if (right->is_InlineType()) {\n+    PreserveReexecuteState preexecs(this);\n+    inc_sp(2);\n+    jvms()->set_should_reexecute(true);\n+    right = right->as_InlineType()->buffer(this)->get_oop();\n+  }\n+\n+  \/\/ First, do a normal pointer comparison\n+  const TypeOopPtr* tleft = _gvn.type(left)->isa_oopptr();\n+  const TypeOopPtr* tright = _gvn.type(right)->isa_oopptr();\n+  Node* cmp = CmpP(left, right);\n+  cmp = optimize_cmp_with_klass(cmp);\n+  if (tleft == nullptr || !tleft->can_be_inline_type() ||\n+      tright == nullptr || !tright->can_be_inline_type()) {\n+    \/\/ This is sufficient, if one of the operands can't be an inline type\n+    do_if(btest, cmp);\n+    return;\n+  }\n+\n+  \/\/ Don't add traps to unstable if branches because additional checks are required to\n+  \/\/ decide if the operands are equal\/substitutable and we therefore shouldn't prune\n+  \/\/ branches for one if based on the profiling of the acmp branches.\n+  \/\/ Also, OptimizeUnstableIf would set an incorrect re-rexecution state because it\n+  \/\/ assumes that there is a 1-1 mapping between the if and the acmp branches and that\n+  \/\/ hitting a trap means that we will take the corresponding acmp branch on re-execution.\n+  const bool can_trap = true;\n+\n+  Node* eq_region = nullptr;\n+  if (btest == BoolTest::eq) {\n+    do_if(btest, cmp, !can_trap, true);\n+    if (stopped()) {\n+      \/\/ Pointers are equal, operands must be equal\n+      return;\n+    }\n+  } else {\n+    assert(btest == BoolTest::ne, \"only eq or ne\");\n+    Node* is_not_equal = nullptr;\n+    eq_region = new RegionNode(3);\n+    {\n+      PreserveJVMState pjvms(this);\n+      \/\/ Pointers are not equal, but more checks are needed to determine if the operands are (not) substitutable\n+      do_if(btest, cmp, !can_trap, false, &is_not_equal);\n+      if (!stopped()) {\n+        eq_region->init_req(1, control());\n+      }\n+    }\n+    if (is_not_equal == nullptr || is_not_equal->is_top()) {\n+      record_for_igvn(eq_region);\n+      set_control(_gvn.transform(eq_region));\n+      return;\n+    }\n+    set_control(is_not_equal);\n+  }\n+\n+  \/\/ Prefer speculative types if available\n+  if (!too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+    if (tleft->speculative_type() != nullptr) {\n+      left_type = tleft->speculative_type();\n+    }\n+    if (tright->speculative_type() != nullptr) {\n+      right_type = tright->speculative_type();\n+    }\n+  }\n+\n+  if (speculative_ptr_kind(tleft) != ProfileMaybeNull && speculative_ptr_kind(tleft) != ProfileUnknownNull) {\n+    ProfilePtrKind speculative_left_ptr = speculative_ptr_kind(tleft);\n+    if (speculative_left_ptr == ProfileAlwaysNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_assert)) {\n+      left_ptr = speculative_left_ptr;\n+    } else if (speculative_left_ptr == ProfileNeverNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check)) {\n+      left_ptr = speculative_left_ptr;\n+    }\n+  }\n+  if (speculative_ptr_kind(tright) != ProfileMaybeNull && speculative_ptr_kind(tright) != ProfileUnknownNull) {\n+    ProfilePtrKind speculative_right_ptr = speculative_ptr_kind(tright);\n+    if (speculative_right_ptr == ProfileAlwaysNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_assert)) {\n+      right_ptr = speculative_right_ptr;\n+    } else if (speculative_right_ptr == ProfileNeverNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check)) {\n+      right_ptr = speculative_right_ptr;\n+    }\n+  }\n+\n+  if (left_ptr == ProfileAlwaysNull) {\n+    \/\/ Comparison with null. Assert the input is indeed null and we're done.\n+    acmp_always_null_input(left, tleft, btest, eq_region);\n+    return;\n+  }\n+  if (right_ptr == ProfileAlwaysNull) {\n+    \/\/ Comparison with null. Assert the input is indeed null and we're done.\n+    acmp_always_null_input(right, tright, btest, eq_region);\n+    return;\n+  }\n+  if (left_type != nullptr && !left_type->is_inlinetype()) {\n+    \/\/ Comparison with an object of known type\n+    acmp_known_non_inline_type_input(left, tleft, left_ptr, left_type, btest, eq_region);\n+    return;\n+  }\n+  if (right_type != nullptr && !right_type->is_inlinetype()) {\n+    \/\/ Comparison with an object of known type\n+    acmp_known_non_inline_type_input(right, tright, right_ptr, right_type, btest, eq_region);\n+    return;\n+  }\n+  if (!left_inline_type) {\n+    \/\/ Comparison with an object known not to be an inline type\n+    acmp_unknown_non_inline_type_input(left, tleft, left_ptr, btest, eq_region);\n+    return;\n+  }\n+  if (!right_inline_type) {\n+    \/\/ Comparison with an object known not to be an inline type\n+    acmp_unknown_non_inline_type_input(right, tright, right_ptr, btest, eq_region);\n+    return;\n+  }\n+\n+  \/\/ Pointers are not equal, check if first operand is non-null\n+  Node* ne_region = new RegionNode(6);\n+  Node* null_ctl;\n+  Node* not_null_right = acmp_null_check(right, tright, right_ptr, null_ctl);\n+  ne_region->init_req(1, null_ctl);\n+\n+  \/\/ First operand is non-null, check if it is an inline type\n+  Node* is_value = inline_type_test(not_null_right);\n+  IfNode* is_value_iff = create_and_map_if(control(), is_value, PROB_FAIR, COUNT_UNKNOWN);\n+  Node* not_value = _gvn.transform(new IfFalseNode(is_value_iff));\n+  ne_region->init_req(2, not_value);\n+  set_control(_gvn.transform(new IfTrueNode(is_value_iff)));\n+\n+  \/\/ The first operand is an inline type, check if the second operand is non-null\n+  Node* not_null_left = acmp_null_check(left, tleft, left_ptr, null_ctl);\n+  ne_region->init_req(3, null_ctl);\n+\n+  \/\/ Check if both operands are of the same class.\n+  Node* kls_left = load_object_klass(not_null_left);\n+  Node* kls_right = load_object_klass(not_null_right);\n+  Node* kls_cmp = CmpP(kls_left, kls_right);\n+  Node* kls_bol = _gvn.transform(new BoolNode(kls_cmp, BoolTest::ne));\n+  IfNode* kls_iff = create_and_map_if(control(), kls_bol, PROB_FAIR, COUNT_UNKNOWN);\n+  Node* kls_ne = _gvn.transform(new IfTrueNode(kls_iff));\n+  set_control(_gvn.transform(new IfFalseNode(kls_iff)));\n+  ne_region->init_req(4, kls_ne);\n+\n+  if (stopped()) {\n+    record_for_igvn(ne_region);\n+    set_control(_gvn.transform(ne_region));\n+    if (btest == BoolTest::ne) {\n+      {\n+        PreserveJVMState pjvms(this);\n+        int target_bci = iter().get_dest();\n+        merge(target_bci);\n+      }\n+      record_for_igvn(eq_region);\n+      set_control(_gvn.transform(eq_region));\n+    }\n+    return;\n+  }\n+\n+  \/\/ Both operands are values types of the same class, we need to perform a\n+  \/\/ substitutability test. Delegate to ValueObjectMethods::isSubstitutable().\n+  Node* ne_io_phi = PhiNode::make(ne_region, i_o());\n+  Node* mem = reset_memory();\n+  Node* ne_mem_phi = PhiNode::make(ne_region, mem);\n+\n+  Node* eq_io_phi = nullptr;\n+  Node* eq_mem_phi = nullptr;\n+  if (eq_region != nullptr) {\n+    eq_io_phi = PhiNode::make(eq_region, i_o());\n+    eq_mem_phi = PhiNode::make(eq_region, mem);\n+  }\n+\n+  set_all_memory(mem);\n+\n+  kill_dead_locals();\n+  ciMethod* subst_method = ciEnv::current()->ValueObjectMethods_klass()->find_method(ciSymbols::isSubstitutable_name(), ciSymbols::object_object_boolean_signature());\n+  CallStaticJavaNode *call = new CallStaticJavaNode(C, TypeFunc::make(subst_method), SharedRuntime::get_resolve_static_call_stub(), subst_method);\n+  call->set_override_symbolic_info(true);\n+  call->init_req(TypeFunc::Parms, not_null_left);\n+  call->init_req(TypeFunc::Parms+1, not_null_right);\n+  inc_sp(2);\n+  set_edges_for_java_call(call, false, false);\n+  Node* ret = set_results_for_java_call(call, false, true);\n+  dec_sp(2);\n+\n+  \/\/ Test the return value of ValueObjectMethods::isSubstitutable()\n+  \/\/ This is the last check, do_if can emit traps now.\n+  Node* subst_cmp = _gvn.transform(new CmpINode(ret, intcon(1)));\n+  Node* ctl = C->top();\n+  if (btest == BoolTest::eq) {\n+    PreserveJVMState pjvms(this);\n+    do_if(btest, subst_cmp, can_trap);\n+    if (!stopped()) {\n+      ctl = control();\n+    }\n+  } else {\n+    assert(btest == BoolTest::ne, \"only eq or ne\");\n+    PreserveJVMState pjvms(this);\n+    do_if(btest, subst_cmp, can_trap, false, &ctl);\n+    if (!stopped()) {\n+      eq_region->init_req(2, control());\n+      eq_io_phi->init_req(2, i_o());\n+      eq_mem_phi->init_req(2, reset_memory());\n+    }\n+  }\n+  ne_region->init_req(5, ctl);\n+  ne_io_phi->init_req(5, i_o());\n+  ne_mem_phi->init_req(5, reset_memory());\n+\n+  record_for_igvn(ne_region);\n+  set_control(_gvn.transform(ne_region));\n+  set_i_o(_gvn.transform(ne_io_phi));\n+  set_all_memory(_gvn.transform(ne_mem_phi));\n+\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+    set_i_o(_gvn.transform(eq_io_phi));\n+    set_all_memory(_gvn.transform(eq_mem_phi));\n+  }\n+}\n+\n@@ -1641,1 +2473,1 @@\n-void Parse::adjust_map_after_if(BoolTest::mask btest, Node* c, float prob, Block* path) {\n+void Parse::adjust_map_after_if(BoolTest::mask btest, Node* c, float prob, Block* path, bool can_trap) {\n@@ -1653,1 +2485,1 @@\n-  if (path_is_suitable_for_uncommon_trap(prob)) {\n+  if (can_trap && path_is_suitable_for_uncommon_trap(prob)) {\n@@ -1750,0 +2582,3 @@\n+            if (tboth->is_inlinetypeptr()) {\n+              ccast = InlineTypeNode::make_from_oop(this, ccast, tboth->exact_klass(true)->as_inline_klass());\n+            }\n@@ -1854,0 +2689,4 @@\n+        if (obj->is_InlineType()) {\n+          assert(obj->as_InlineType()->is_allocated(&_gvn), \"must be allocated\");\n+          obj = obj->as_InlineType()->get_oop();\n+        }\n@@ -2669,14 +3508,20 @@\n-    if (!_gvn.type(b)->speculative_maybe_null() &&\n-        !too_many_traps(Deoptimization::Reason_speculate_null_check)) {\n-      inc_sp(1);\n-      Node* null_ctl = top();\n-      b = null_check_oop(b, &null_ctl, true, true, true);\n-      assert(null_ctl->is_top(), \"no null control here\");\n-      dec_sp(1);\n-    } else if (_gvn.type(b)->speculative_always_null() &&\n-               !too_many_traps(Deoptimization::Reason_speculate_null_assert)) {\n-      inc_sp(1);\n-      b = null_assert(b);\n-      dec_sp(1);\n-    }\n-    c = _gvn.transform( new CmpPNode(b, a) );\n+    if (b->is_InlineType()) {\n+      \/\/ Null checking a scalarized but nullable inline type. Check the IsInit\n+      \/\/ input instead of the oop input to avoid keeping buffer allocations alive\n+      c = _gvn.transform(new CmpINode(b->as_InlineType()->get_is_init(), zerocon(T_INT)));\n+    } else {\n+      if (!_gvn.type(b)->speculative_maybe_null() &&\n+          !too_many_traps(Deoptimization::Reason_speculate_null_check)) {\n+        inc_sp(1);\n+        Node* null_ctl = top();\n+        b = null_check_oop(b, &null_ctl, true, true, true);\n+        assert(null_ctl->is_top(), \"no null control here\");\n+        dec_sp(1);\n+      } else if (_gvn.type(b)->speculative_always_null() &&\n+                 !too_many_traps(Deoptimization::Reason_speculate_null_assert)) {\n+        inc_sp(1);\n+        b = null_assert(b);\n+        dec_sp(1);\n+      }\n+      c = _gvn.transform( new CmpPNode(b, a) );\n+    }\n@@ -2693,3 +3538,1 @@\n-    c = _gvn.transform( new CmpPNode(b, a) );\n-    c = optimize_cmp_with_klass(c);\n-    do_if(btest, c);\n+    do_acmp(btest, b, a);\n@@ -2750,1 +3593,1 @@\n-    do_anewarray();\n+    do_newarray();\n","filename":"src\/hotspot\/share\/opto\/parse2.cpp","additions":947,"deletions":104,"binary":false,"changes":1051,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -42,0 +44,1 @@\n+\n@@ -49,0 +52,12 @@\n+  if (is_get && is_field && field_holder->is_inlinetype() && peek()->is_InlineType()) {\n+    InlineTypeNode* vt = peek()->as_InlineType();\n+    null_check(vt);\n+    Node* value = vt->field_value_by_offset(field->offset_in_bytes());\n+    if (value->is_InlineType()) {\n+      value = value->as_InlineType()->adjust_scalarization_depth(this);\n+    }\n+    pop();\n+    push_node(field->layout_type(), value);\n+    return;\n+  }\n+\n@@ -59,1 +74,1 @@\n-      !(method()->holder() == field_holder && method()->is_object_initializer())) {\n+      !(method()->holder() == field_holder && method()->is_object_constructor())) {\n@@ -90,1 +105,1 @@\n-      do_get_xxx(obj, field, is_field);\n+      do_get_xxx(obj, field);\n@@ -93,0 +108,3 @@\n+      if (stopped()) {\n+        return;\n+      }\n@@ -99,1 +117,1 @@\n-      do_get_xxx(obj, field, is_field);\n+      do_get_xxx(obj, field);\n@@ -106,2 +124,1 @@\n-\n-void Parse::do_get_xxx(Node* obj, ciField* field, bool is_field) {\n+void Parse::do_get_xxx(Node* obj, ciField* field) {\n@@ -109,2 +126,1 @@\n-\n-  if (field->is_constant() &&\n+  if (field->is_constant() && !field->is_flat() &&\n@@ -127,3 +143,1 @@\n-  bool is_vol = field->is_volatile();\n-\n-  \/\/ Compute address and memory type.\n+  field_klass = improve_abstract_inline_type_klass(field_klass);\n@@ -131,8 +145,0 @@\n-  const TypePtr* adr_type = C->alias_type(field)->adr_type();\n-  Node *adr = basic_plus_adr(obj, obj, offset);\n-  assert(C->get_alias_index(adr_type) == C->get_alias_index(_gvn.type(adr)->isa_ptr()),\n-    \"slice of address and input slice don't match\");\n-\n-  \/\/ Build the resultant type of the load\n-  const Type *type;\n-\n@@ -141,16 +147,28 @@\n-  DecoratorSet decorators = IN_HEAP;\n-  decorators |= is_vol ? MO_SEQ_CST : MO_UNORDERED;\n-\n-  bool is_obj = is_reference_type(bt);\n-\n-  if (is_obj) {\n-    if (!field->type()->is_loaded()) {\n-      type = TypeInstPtr::BOTTOM;\n-      must_assert_null = true;\n-    } else if (field->is_static_constant()) {\n-      \/\/ This can happen if the constant oop is non-perm.\n-      ciObject* con = field->constant_value().as_object();\n-      \/\/ Do not \"join\" in the previous type; it doesn't add value,\n-      \/\/ and may yield a vacuous result if the field is of interface type.\n-      if (con->is_null_object()) {\n-        type = TypePtr::NULL_PTR;\n+  Node* ld = nullptr;\n+  if (field->is_null_free() && field_klass->as_inline_klass()->is_empty()) {\n+    \/\/ Loading from a field of an empty inline type. Just return the default instance.\n+    ld = InlineTypeNode::make_default(_gvn, field_klass->as_inline_klass());\n+  } else if (field->is_flat()) {\n+    \/\/ Loading from a flat inline type field.\n+    ciInlineKlass* vk = field->type()->as_inline_klass();\n+    bool is_naturally_atomic = vk->is_empty() || (field->is_null_free() && vk->nof_declared_nonstatic_fields() == 1);\n+    bool needs_atomic_access = (!field->is_null_free() || field->is_volatile()) && !is_naturally_atomic;\n+    ld = InlineTypeNode::make_from_flat(this, field_klass->as_inline_klass(), obj, obj, field->holder(), offset, needs_atomic_access, field->null_marker_offset());\n+  } else {\n+    \/\/ Build the resultant type of the load\n+    const Type* type;\n+    if (is_reference_type(bt)) {\n+      if (!field_klass->is_loaded()) {\n+        type = TypeInstPtr::BOTTOM;\n+        must_assert_null = true;\n+      } else if (field->is_static_constant()) {\n+        \/\/ This can happen if the constant oop is non-perm.\n+        ciObject* con = field->constant_value().as_object();\n+        \/\/ Do not \"join\" in the previous type; it doesn't add value,\n+        \/\/ and may yield a vacuous result if the field is of interface type.\n+        if (con->is_null_object()) {\n+          type = TypePtr::NULL_PTR;\n+        } else {\n+          type = TypeOopPtr::make_from_constant(con)->isa_oopptr();\n+        }\n+        assert(type != nullptr, \"field singleton type must be consistent\");\n@@ -158,1 +176,9 @@\n-        type = TypeOopPtr::make_from_constant(con)->isa_oopptr();\n+        type = TypeOopPtr::make_from_klass(field_klass->as_klass());\n+        if (field->is_null_free() && field->is_static()) {\n+          \/\/ Check if static inline type field is already initialized\n+          ciInstance* mirror = field->holder()->java_mirror();\n+          ciObject* val = mirror->field_value(field).as_object();\n+          if (!val->is_null_object()) {\n+            type = type->join_speculative(TypePtr::NOTNULL);\n+          }\n+        }\n@@ -160,2 +186,10 @@\n-      assert(type != nullptr, \"field singleton type must be consistent\");\n-      type = TypeOopPtr::make_from_klass(field_klass->as_klass());\n+      type = Type::get_const_basic_type(bt);\n+    }\n+    Node* adr = basic_plus_adr(obj, obj, offset);\n+    const TypePtr* adr_type = C->alias_type(field)->adr_type();\n+    DecoratorSet decorators = IN_HEAP;\n+    decorators |= field->is_volatile() ? MO_SEQ_CST : MO_UNORDERED;\n+    ld = access_load_at(obj, adr, adr_type, type, bt, decorators);\n+    if (field_klass->is_inlinetype()) {\n+      \/\/ Load a non-flattened inline type from memory\n+      ld = InlineTypeNode::make_from_oop(this, ld, field_klass->as_inline_klass(), field->is_null_free());\n@@ -164,2 +198,0 @@\n-  } else {\n-    type = Type::get_const_basic_type(bt);\n@@ -168,2 +200,0 @@\n-  Node* ld = access_load_at(obj, adr, adr_type, type, bt, decorators);\n-\n@@ -192,1 +222,1 @@\n-                     C->log()->identify(field->type()));\n+                     C->log()->identify(field_klass));\n@@ -201,0 +231,17 @@\n+\/\/ If the field klass is an abstract value klass (for which we do not know the layout, yet), it could have a unique\n+\/\/ concrete sub klass for which we have a fixed layout. This allows us to use InlineTypeNodes instead.\n+ciType* Parse::improve_abstract_inline_type_klass(ciType* field_klass) {\n+  Dependencies* dependencies = C->dependencies();\n+  if (UseUniqueSubclasses && dependencies != nullptr && field_klass->is_instance_klass()) {\n+    ciInstanceKlass* instance_klass = field_klass->as_instance_klass();\n+    if (instance_klass->is_loaded() && instance_klass->is_abstract_value_klass()) {\n+      ciInstanceKlass* sub_klass = instance_klass->unique_concrete_subklass();\n+      if (sub_klass != nullptr && sub_klass != field_klass) {\n+        field_klass = sub_klass;\n+        dependencies->assert_abstract_with_unique_concrete_subtype(instance_klass, sub_klass);\n+      }\n+    }\n+  }\n+  return field_klass;\n+}\n+\n@@ -203,7 +250,0 @@\n-\n-  \/\/ Compute address and memory type.\n-  const TypePtr* adr_type = C->alias_type(field)->adr_type();\n-  Node* adr = basic_plus_adr(obj, obj, offset);\n-  assert(C->get_alias_index(adr_type) == C->get_alias_index(_gvn.type(adr)->isa_ptr()),\n-    \"slice of address and input slice don't match\");\n-  \/\/ Value to be stored\n@@ -214,9 +254,27 @@\n-  DecoratorSet decorators = IN_HEAP;\n-  decorators |= is_vol ? MO_SEQ_CST : MO_UNORDERED;\n-\n-  bool is_obj = is_reference_type(bt);\n-\n-  \/\/ Store the value.\n-  const Type* field_type;\n-  if (!field->type()->is_loaded()) {\n-    field_type = TypeInstPtr::BOTTOM;\n+  if (field->is_null_free()) {\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n+    inc_sp(1);\n+    val = null_check(val);\n+    if (stopped()) {\n+      return;\n+    }\n+  }\n+  if (obj->is_InlineType()) {\n+    set_inline_type_field(obj, field, val);\n+    return;\n+  }\n+  if (field->is_null_free() && field->type()->as_inline_klass()->is_empty()) {\n+    \/\/ Storing to a field of an empty inline type. Ignore.\n+    return;\n+  } else if (field->is_flat()) {\n+    \/\/ Storing to a flat inline type field.\n+    ciInlineKlass* vk = field->type()->as_inline_klass();\n+    if (!val->is_InlineType()) {\n+      val = InlineTypeNode::make_from_oop(this, val, vk, field->is_null_free());\n+    }\n+    inc_sp(1);\n+    bool is_naturally_atomic = vk->is_empty() || (field->is_null_free() && vk->nof_declared_nonstatic_fields() == 1);\n+    bool needs_atomic_access = (!field->is_null_free() || field->is_volatile()) && !is_naturally_atomic;\n+    val->as_InlineType()->store_flat(this, obj, obj, field->holder(), offset, needs_atomic_access, field->null_marker_offset(), IN_HEAP | MO_UNORDERED);\n+    dec_sp(1);\n@@ -224,2 +282,4 @@\n-    if (is_obj) {\n-      field_type = TypeOopPtr::make_from_klass(field->type()->as_klass());\n+    \/\/ Store the value.\n+    const Type* field_type;\n+    if (!field->type()->is_loaded()) {\n+      field_type = TypeInstPtr::BOTTOM;\n@@ -227,1 +287,5 @@\n-      field_type = Type::BOTTOM;\n+      if (is_reference_type(bt)) {\n+        field_type = TypeOopPtr::make_from_klass(field->type()->as_klass());\n+      } else {\n+        field_type = Type::BOTTOM;\n+      }\n@@ -229,0 +293,7 @@\n+    Node* adr = basic_plus_adr(obj, obj, offset);\n+    const TypePtr* adr_type = C->alias_type(field)->adr_type();\n+    DecoratorSet decorators = IN_HEAP;\n+    decorators |= is_vol ? MO_SEQ_CST : MO_UNORDERED;\n+    inc_sp(1);\n+    access_store_at(obj, adr, adr_type, val, field_type, bt, decorators);\n+    dec_sp(1);\n@@ -230,1 +301,0 @@\n-  access_store_at(obj, adr, adr_type, val, field_type, bt, decorators);\n@@ -264,0 +334,40 @@\n+void Parse::set_inline_type_field(Node* obj, ciField* field, Node* val) {\n+  assert(_method->is_object_constructor(), \"inline type is initialized outside of constructor\");\n+  assert(obj->as_InlineType()->is_larval(), \"must be larval\");\n+  assert(!_gvn.type(obj)->maybe_null(), \"should never be null\");\n+\n+  \/\/ Re-execute if buffering in below code triggers deoptimization.\n+  PreserveReexecuteState preexecs(this);\n+  jvms()->set_should_reexecute(true);\n+  inc_sp(1);\n+\n+  if (!val->is_InlineType() && field->type()->is_inlinetype()) {\n+    \/\/ Scalarize inline type field value\n+    val = InlineTypeNode::make_from_oop(this, val, field->type()->as_inline_klass(), field->is_null_free());\n+  } else if (val->is_InlineType() && !field->is_flat()) {\n+    \/\/ Field value needs to be allocated because it can be merged with a non-inline type.\n+    val = val->as_InlineType()->buffer(this);\n+  }\n+\n+  \/\/ Clone the inline type node and set the new field value\n+  InlineTypeNode* new_vt = obj->as_InlineType()->clone_if_required(&_gvn, _map);\n+  new_vt->set_field_value_by_offset(field->offset_in_bytes(), val);\n+  new_vt = new_vt->adjust_scalarization_depth(this);\n+\n+  \/\/ If the inline type is buffered and the caller might use the buffer, update it.\n+  if (new_vt->is_allocated(&gvn()) && (!_caller->has_method() || C->inlining_incrementally() || _caller->method()->is_object_constructor())) {\n+    new_vt->store(this, new_vt->get_oop(), new_vt->get_oop(), new_vt->bottom_type()->inline_klass(), 0, field->offset_in_bytes());\n+\n+    \/\/ Preserve allocation ptr to create precedent edge to it in membar\n+    \/\/ generated on exit from constructor.\n+    AllocateNode* alloc = AllocateNode::Ideal_allocation(new_vt->get_oop());\n+    if (alloc != nullptr) {\n+      set_alloc_with_final_or_stable(new_vt->get_oop());\n+    }\n+    set_wrote_final(true);\n+  }\n+\n+  replace_in_map(obj, _gvn.transform(new_vt));\n+  return;\n+}\n+\n@@ -265,1 +375,2 @@\n-void Parse::do_anewarray() {\n+\n+void Parse::do_newarray() {\n@@ -272,1 +383,3 @@\n-  assert(will_link, \"anewarray: typeflow responsibility\");\n+  assert(will_link, \"newarray: typeflow responsibility\");\n+\n+  ciArrayKlass* array_klass = ciArrayKlass::make(klass);\n@@ -274,1 +387,0 @@\n-  ciObjArrayKlass* array_klass = ciObjArrayKlass::make(klass);\n@@ -282,0 +394,7 @@\n+  } else if (array_klass->element_klass() != nullptr &&\n+             array_klass->element_klass()->is_inlinetype() &&\n+             !array_klass->element_klass()->as_inline_klass()->is_initialized()) {\n+    uncommon_trap(Deoptimization::Reason_uninitialized,\n+                  Deoptimization::Action_reinterpret,\n+                  nullptr);\n+    return;\n@@ -342,1 +461,12 @@\n-  for (j = ndimensions-1; j >= 0 ; j--) length[j] = pop();\n+  ciKlass* elem_klass = array_klass;\n+  for (j = ndimensions-1; j >= 0; j--) {\n+    length[j] = pop();\n+    elem_klass = elem_klass->as_array_klass()->element_klass();\n+  }\n+  if (elem_klass != nullptr && elem_klass->is_inlinetype() && !elem_klass->as_inline_klass()->is_initialized()) {\n+    inc_sp(ndimensions);\n+    uncommon_trap(Deoptimization::Reason_uninitialized,\n+                  Deoptimization::Action_reinterpret,\n+                  nullptr);\n+    return;\n+  }\n","filename":"src\/hotspot\/share\/opto\/parse3.cpp","additions":196,"deletions":66,"binary":false,"changes":262,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -27,0 +28,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -29,0 +31,2 @@\n+#include \"opto\/castnode.hpp\"\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -67,1 +71,0 @@\n-\n@@ -140,2 +143,1 @@\n-void Parse::array_store_check() {\n-\n+Node* Parse::array_store_check(Node*& adr, const Type*& elemtype) {\n@@ -152,1 +154,4 @@\n-    return;\n+    if (_gvn.type(ary)->is_aryptr()->is_null_free()) {\n+      null_check(obj);\n+    }\n+    return obj;\n@@ -156,4 +161,1 @@\n-  int klass_offset = oopDesc::klass_offset_in_bytes();\n-  Node* p = basic_plus_adr( ary, ary, klass_offset );\n-  \/\/ p's type is array-of-OOPS plus klass_offset\n-  Node* array_klass = _gvn.transform(LoadKlassNode::make(_gvn, nullptr, immutable_memory(), p, TypeInstPtr::KLASS));\n+  Node* array_klass = load_object_klass(ary);\n@@ -161,1 +163,1 @@\n-  const TypeKlassPtr *tak = _gvn.type(array_klass)->is_klassptr();\n+  const TypeKlassPtr* tak = _gvn.type(array_klass)->is_klassptr();\n@@ -168,6 +170,26 @@\n-  if (MonomorphicArrayCheck\n-      && !too_many_traps(Deoptimization::Reason_array_check)\n-      && !tak->klass_is_exact()\n-      && tak != TypeInstKlassPtr::OBJECT) {\n-      \/\/ Regarding the fourth condition in the if-statement from above:\n-      \/\/\n+  if (MonomorphicArrayCheck && !tak->klass_is_exact()) {\n+    \/\/ Make a constant out of the inexact array klass\n+    const TypeKlassPtr* extak = nullptr;\n+    const TypeOopPtr* ary_t = _gvn.type(ary)->is_oopptr();\n+    ciKlass* ary_spec = ary_t->speculative_type();\n+    Deoptimization::DeoptReason reason = Deoptimization::Reason_none;\n+    \/\/ Try to cast the array to an exact type from profile data. First\n+    \/\/ check the speculative type.\n+    if (ary_spec != nullptr && !too_many_traps(Deoptimization::Reason_speculate_class_check)) {\n+      extak = TypeKlassPtr::make(ary_spec);\n+      reason = Deoptimization::Reason_speculate_class_check;\n+    } else if (UseArrayLoadStoreProfile) {\n+      \/\/ No speculative type: check profile data at this bci.\n+      reason = Deoptimization::Reason_class_check;\n+      if (!too_many_traps(reason)) {\n+        ciKlass* array_type = nullptr;\n+        ciKlass* element_type = nullptr;\n+        ProfilePtrKind element_ptr = ProfileMaybeNull;\n+        bool flat_array = true;\n+        bool null_free_array = true;\n+        method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+        if (array_type != nullptr) {\n+          extak = TypeKlassPtr::make(array_type);\n+        }\n+      }\n+    } else if (!too_many_traps(Deoptimization::Reason_array_check) && tak != TypeInstKlassPtr::OBJECT) {\n@@ -192,8 +214,4 @@\n-\n-    always_see_exact_class = true;\n-    \/\/ (If no MDO at all, hope for the best, until a trap actually occurs.)\n-\n-    \/\/ Make a constant out of the inexact array klass\n-    const TypeKlassPtr *extak = tak->cast_to_exactness(true);\n-\n-    if (extak->exact_klass(true) != nullptr) {\n+      extak = tak->cast_to_exactness(true);\n+      reason = Deoptimization::Reason_array_check;\n+    }\n+    if (extak != nullptr && extak->exact_klass(true) != nullptr) {\n@@ -201,13 +219,22 @@\n-      Node* cmp = _gvn.transform(new CmpPNode( array_klass, con ));\n-      Node* bol = _gvn.transform(new BoolNode( cmp, BoolTest::eq ));\n-      Node* ctrl= control();\n-      { BuildCutout unless(this, bol, PROB_MAX);\n-        uncommon_trap(Deoptimization::Reason_array_check,\n-                      Deoptimization::Action_maybe_recompile,\n-                      extak->exact_klass());\n-      }\n-      if (stopped()) {          \/\/ MUST uncommon-trap?\n-        set_control(ctrl);      \/\/ Then Don't Do It, just fall into the normal checking\n-      } else {                  \/\/ Cast array klass to exactness:\n-        \/\/ Use the exact constant value we know it is.\n-        replace_in_map(array_klass,con);\n+      Node* cmp = _gvn.transform(new CmpPNode(array_klass, con));\n+      Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));\n+      \/\/ Only do it if the check does not always pass\/fail\n+      if (!bol->is_Con()) {\n+        always_see_exact_class = true;\n+        { BuildCutout unless(this, bol, PROB_MAX);\n+          uncommon_trap(reason,\n+                        Deoptimization::Action_maybe_recompile,\n+                        extak->exact_klass());\n+        }\n+        \/\/ Cast array klass to exactness\n+        replace_in_map(array_klass, con);\n+        array_klass = con;\n+        Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, extak->as_instance_type()));\n+        replace_in_map(ary, cast);\n+        ary = cast;\n+\n+        \/\/ Recompute element type and address\n+        const TypeAryPtr* arytype = _gvn.type(ary)->is_aryptr();\n+        elemtype = arytype->elem();\n+        adr = array_element_address(ary, idx, T_OBJECT, arytype->size(), control());\n+\n@@ -219,1 +246,0 @@\n-        array_klass = con;      \/\/ Use cast value moving forward\n@@ -227,1 +253,2 @@\n-  int element_klass_offset = in_bytes(ObjArrayKlass::element_klass_offset());\n+  int element_klass_offset = in_bytes(ArrayKlass::element_klass_offset());\n+\n@@ -235,0 +262,32 @@\n+  \/\/ If we statically know that this is an inline type array, use precise element klass for checkcast\n+  const TypeAryPtr* arytype = _gvn.type(ary)->is_aryptr();\n+  const TypePtr* elem_ptr = elemtype->make_ptr();\n+  bool null_free;\n+  if (elem_ptr->is_inlinetypeptr()) {\n+    \/\/ We statically know that this is an inline type array, use precise klass ptr\n+    null_free = arytype->is_flat() || !elem_ptr->maybe_null();\n+    a_e_klass = makecon(TypeKlassPtr::make(elemtype->inline_klass()));\n+  } else {\n+    \/\/ TODO: Should move to TypeAry::is_null_free() with JDK-8345681\n+    TypePtr::PTR ptr = elem_ptr->ptr();\n+    null_free = ((ptr == TypePtr::NotNull) || (ptr == TypePtr::AnyNull));\n+#ifdef ASSERT\n+    \/\/ If the element type is exact, the array can be null-free (i.e. the element type is NotNull) if:\n+    \/\/   - The elements are inline types\n+    \/\/   - The array is from an autobox cache.\n+    \/\/ If the element type is inexact, it could represent multiple null-free arrays. Since autobox cache arrays\n+    \/\/ are local to very few cache classes and are only used in the valueOf() methods, they are always exact and are not\n+    \/\/ merged or hidden behind super types. Therefore, an inexact null-free array always represents some kind of\n+    \/\/ inline type array - either of an abstract value class or Object.\n+    if (null_free) {\n+      ciKlass* klass = elem_ptr->is_instptr()->instance_klass();\n+      if (klass->exact_klass()) {\n+        assert(elem_ptr->is_inlinetypeptr() || arytype->is_autobox_cache(), \"elements must be inline type or autobox cache\");\n+      } else {\n+        assert(!arytype->is_autobox_cache() && elem_ptr->can_be_inline_type() &&\n+               (klass->is_java_lang_Object() || klass->is_abstract()), \"cannot have inexact non-inline type elements\");\n+      }\n+    }\n+#endif \/\/ ASSERT\n+  }\n+\n@@ -236,2 +295,1 @@\n-  \/\/ Result is ignored, we just need the CFG effects.\n-  gen_checkcast(obj, a_e_klass);\n+  return gen_checkcast(obj, a_e_klass, nullptr, null_free);\n@@ -264,0 +322,5 @@\n+  if (klass->is_inlinetype()) {\n+    push(InlineTypeNode::make_default(_gvn, klass->as_inline_klass(), \/* is_larval *\/ true));\n+    return;\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/parseHelper.cpp","additions":103,"deletions":40,"binary":false,"changes":143,"status":"modified"},{"patch":"@@ -1138,1 +1138,1 @@\n-  n->dump_bfs(1, nullptr, \"\");\n+  n->dump_bfs(3, nullptr, \"\");\n@@ -1163,6 +1163,0 @@\n-  if (_delay_transform) {\n-    \/\/ Register the node but don't optimize for now\n-    register_new_node_with_optimizer(n);\n-    return n;\n-  }\n-\n@@ -1175,0 +1169,6 @@\n+  if (_delay_transform) {\n+    \/\/ Add the node to the worklist but don't optimize for now\n+    _worklist.push(n);\n+    return n;\n+  }\n+\n@@ -1445,0 +1445,13 @@\n+void PhaseIterGVN::replace_in_uses(Node* n, Node* m) {\n+  assert(n != nullptr, \"sanity\");\n+  for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+    Node* u = n->fast_out(i);\n+    if (u != n) {\n+      rehash_node_delayed(u);\n+      int nb = u->replace_edge(n, m);\n+      --i, imax -= nb;\n+    }\n+  }\n+  assert(n->outcnt() == 0, \"all uses must be deleted\");\n+}\n+\n@@ -1500,0 +1513,10 @@\n+  \/\/ AndLNode::Ideal folds GraphKit::mark_word_test patterns. Give it a chance to run.\n+  if (n->is_Load() && use->is_Phi()) {\n+    for (DUIterator_Fast imax, i = use->fast_outs(imax); i < imax; i++) {\n+      Node* u = use->fast_out(i);\n+      if (u->Opcode() == Op_AndL) {\n+        worklist.push(u);\n+      }\n+    }\n+  }\n+\n@@ -1597,0 +1620,9 @@\n+  \/\/ Inline type nodes can have other inline types as users. If an input gets\n+  \/\/ updated, make sure that inline type users get a chance for optimization.\n+  if (use->is_InlineType()) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->is_InlineType())\n+        worklist.push(u);\n+    }\n+  }\n@@ -1680,0 +1712,8 @@\n+  if (use_op == Op_CastP2X) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->Opcode() == Op_AndX) {\n+        worklist.push(u);\n+      }\n+    }\n+  }\n@@ -1698,0 +1738,10 @@\n+  \/\/ Give CallStaticJavaNode::remove_useless_allocation a chance to run\n+  if (use->is_Region()) {\n+    Node* c = use;\n+    do {\n+      c = c->unique_ctrl_out_or_null();\n+    } while (c != nullptr && c->is_Region());\n+    if (c != nullptr && c->is_CallStaticJava() && c->as_CallStaticJava()->uncommon_trap_request() != 0) {\n+      worklist.push(c);\n+    }\n+  }\n@@ -1785,1 +1835,1 @@\n-    n->dump(1);\n+    n->dump(3);\n@@ -1902,0 +1952,1 @@\n+  push_cast(worklist, use);\n@@ -1964,0 +2015,12 @@\n+void PhaseCCP::push_cast(Unique_Node_List& worklist, const Node* use) {\n+  uint use_op = use->Opcode();\n+  if (use_op == Op_CastP2X) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->Opcode() == Op_AndX) {\n+        worklist.push(u);\n+      }\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":71,"deletions":8,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -140,1 +140,0 @@\n-\n","filename":"src\/hotspot\/share\/opto\/replacednodes.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -46,0 +46,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n@@ -312,1 +314,1 @@\n-JRT_BLOCK_ENTRY(void, OptoRuntime::new_instance_C(Klass* klass, JavaThread* current))\n+JRT_BLOCK_ENTRY(void, OptoRuntime::new_instance_C(Klass* klass, bool is_larval, JavaThread* current))\n@@ -332,1 +334,5 @@\n-    oop result = InstanceKlass::cast(klass)->allocate_instance(THREAD);\n+    instanceOop result = InstanceKlass::cast(klass)->allocate_instance(THREAD);\n+    if (is_larval) {\n+      \/\/ Check if this is a larval buffer allocation\n+      result->set_mark(result->mark().enter_larval_state());\n+    }\n@@ -360,1 +366,6 @@\n-  if (array_type->is_typeArray_klass()) {\n+  if (array_type->is_flatArray_klass()) {\n+    Handle holder(current, array_type->klass_holder()); \/\/ keep the array klass alive\n+    FlatArrayKlass* fak = FlatArrayKlass::cast(array_type);\n+    Klass* elem_type = fak->element_klass();\n+    result = oopFactory::new_flatArray(elem_type, len, fak->layout_kind(), THREAD);\n+  } else if (array_type->is_typeArray_klass()) {\n@@ -366,5 +377,1 @@\n-    \/\/ Although the oopFactory likes to work with the elem_type,\n-    \/\/ the compiler prefers the array_type, since it must already have\n-    \/\/ that latter value in hand for the fast path.\n-    Klass* elem_type = ObjArrayKlass::cast(array_type)->element_klass();\n-    result = oopFactory::new_objArray(elem_type, len, THREAD);\n+    result = ObjArrayKlass::cast(array_type)->allocate(len, THREAD);\n@@ -568,1 +575,1 @@\n-  const Type **fields = TypeTuple::fields(1);\n+  const Type **fields = TypeTuple::fields(2);\n@@ -570,1 +577,2 @@\n-  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+1, fields);\n+  fields[TypeFunc::Parms+1] = TypeInt::BOOL;        \/\/ is_larval\n+  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+2, fields);\n@@ -688,1 +696,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -1806,1 +1814,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -1838,1 +1846,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -1854,1 +1862,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -2032,0 +2040,106 @@\n+\n+const TypeFunc *OptoRuntime::store_inline_type_fields_Type() {\n+  \/\/ create input type (domain)\n+  uint total = SharedRuntime::java_return_convention_max_int + SharedRuntime::java_return_convention_max_float*2;\n+  const Type **fields = TypeTuple::fields(total);\n+  \/\/ We don't know the number of returned values and their\n+  \/\/ types. Assume all registers available to the return convention\n+  \/\/ are used.\n+  fields[TypeFunc::Parms] = TypePtr::BOTTOM;\n+  uint i = 1;\n+  for (; i < SharedRuntime::java_return_convention_max_int; i++) {\n+    fields[TypeFunc::Parms+i] = TypeInt::INT;\n+  }\n+  for (; i < total; i+=2) {\n+    fields[TypeFunc::Parms+i] = Type::DOUBLE;\n+    fields[TypeFunc::Parms+i+1] = Type::HALF;\n+  }\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + total, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL;\n+\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1,fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+const TypeFunc *OptoRuntime::pack_inline_type_Type() {\n+  \/\/ create input type (domain)\n+  uint total = 1 + SharedRuntime::java_return_convention_max_int + SharedRuntime::java_return_convention_max_float*2;\n+  const Type **fields = TypeTuple::fields(total);\n+  \/\/ We don't know the number of returned values and their\n+  \/\/ types. Assume all registers available to the return convention\n+  \/\/ are used.\n+  fields[TypeFunc::Parms] = TypeRawPtr::BOTTOM;\n+  fields[TypeFunc::Parms+1] = TypeRawPtr::BOTTOM;\n+  uint i = 2;\n+  for (; i < SharedRuntime::java_return_convention_max_int+1; i++) {\n+    fields[TypeFunc::Parms+i] = TypeInt::INT;\n+  }\n+  for (; i < total; i+=2) {\n+    fields[TypeFunc::Parms+i] = Type::DOUBLE;\n+    fields[TypeFunc::Parms+i+1] = Type::HALF;\n+  }\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + total, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL;\n+\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1,fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+JRT_BLOCK_ENTRY(void, OptoRuntime::load_unknown_inline_C(flatArrayOopDesc* array, int index, JavaThread* current))\n+  JRT_BLOCK;\n+  oop buffer = array->read_value_from_flat_array(index, THREAD);\n+  deoptimize_caller_frame(current, HAS_PENDING_EXCEPTION);\n+  current->set_vm_result(buffer);\n+  JRT_BLOCK_END;\n+JRT_END\n+\n+const TypeFunc* OptoRuntime::load_unknown_inline_Type() {\n+  \/\/ create input type (domain)\n+  const Type** fields = TypeTuple::fields(2);\n+  fields[TypeFunc::Parms] = TypeOopPtr::NOTNULL;\n+  fields[TypeFunc::Parms+1] = TypeInt::POS;\n+\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms+2, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms] = TypeInstPtr::NOTNULL;\n+\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms+1, fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+JRT_BLOCK_ENTRY(void, OptoRuntime::store_unknown_inline_C(instanceOopDesc* buffer, flatArrayOopDesc* array, int index, JavaThread* current))\n+  JRT_BLOCK;\n+  assert(buffer != nullptr, \"can't store null into flat array\");\n+  array->write_value_to_flat_array(buffer, index, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+      fatal(\"This entry must be changed to be a non-leaf entry because writing to a flat array can now throw an exception\");\n+  }\n+  JRT_BLOCK_END;\n+JRT_END\n+\n+const TypeFunc* OptoRuntime::store_unknown_inline_Type() {\n+  \/\/ create input type (domain)\n+  const Type** fields = TypeTuple::fields(3);\n+  fields[TypeFunc::Parms] = TypeInstPtr::NOTNULL;\n+  fields[TypeFunc::Parms+1] = TypeOopPtr::NOTNULL;\n+  fields[TypeFunc::Parms+2] = TypeInt::POS;\n+\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms+3, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(0);\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms, fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":128,"deletions":14,"binary":false,"changes":142,"status":"modified"},{"patch":"@@ -213,1 +213,1 @@\n-  static void new_instance_C(Klass* instance_klass, JavaThread* current);\n+  static void new_instance_C(Klass* instance_klass, bool is_larval, JavaThread* current);\n@@ -260,0 +260,2 @@\n+  static void load_unknown_inline_C(flatArrayOopDesc* array, int index, JavaThread* current);\n+  static void store_unknown_inline_C(instanceOopDesc* buffer, flatArrayOopDesc* array, int index, JavaThread* current);\n@@ -292,0 +294,2 @@\n+  static address load_unknown_inline_Java()              { return _load_unknown_inline_Java; }\n+  static address store_unknown_inline_Java()             { return _store_unknown_inline_Java; }\n@@ -655,0 +659,6 @@\n+  static const TypeFunc* load_unknown_inline_Type();\n+  static const TypeFunc* store_unknown_inline_Type();\n+\n+  static const TypeFunc* store_inline_type_fields_Type();\n+  static const TypeFunc* pack_inline_type_Type();\n+\n","filename":"src\/hotspot\/share\/opto\/runtime.hpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -634,2 +635,1 @@\n-        }\n-        else if (m != iff && split_up(m, region, iff)) {\n+        } else if (m != iff && split_up(m, region, iff)) {\n","filename":"src\/hotspot\/share\/opto\/split_if.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -317,4 +317,3 @@\n-  CallProjections projs;\n-  call->extract_projections(&projs, false);\n-  if (projs.fallthrough_catchproj != nullptr) {\n-    C->gvn_replace_by(projs.fallthrough_catchproj, call->in(TypeFunc::Control));\n+  CallProjections* projs = call->extract_projections(false);\n+  if (projs->fallthrough_catchproj != nullptr) {\n+    C->gvn_replace_by(projs->fallthrough_catchproj, call->in(TypeFunc::Control));\n@@ -322,2 +321,2 @@\n-  if (projs.fallthrough_memproj != nullptr) {\n-    C->gvn_replace_by(projs.fallthrough_memproj, call->in(TypeFunc::Memory));\n+  if (projs->fallthrough_memproj != nullptr) {\n+    C->gvn_replace_by(projs->fallthrough_memproj, call->in(TypeFunc::Memory));\n@@ -325,2 +324,2 @@\n-  if (projs.catchall_memproj != nullptr) {\n-    C->gvn_replace_by(projs.catchall_memproj, C->top());\n+  if (projs->catchall_memproj != nullptr) {\n+    C->gvn_replace_by(projs->catchall_memproj, C->top());\n@@ -328,2 +327,2 @@\n-  if (projs.fallthrough_ioproj != nullptr) {\n-    C->gvn_replace_by(projs.fallthrough_ioproj, call->in(TypeFunc::I_O));\n+  if (projs->fallthrough_ioproj != nullptr) {\n+    C->gvn_replace_by(projs->fallthrough_ioproj, call->in(TypeFunc::I_O));\n@@ -331,2 +330,2 @@\n-  if (projs.catchall_ioproj != nullptr) {\n-    C->gvn_replace_by(projs.catchall_ioproj, C->top());\n+  if (projs->catchall_ioproj != nullptr) {\n+    C->gvn_replace_by(projs->catchall_ioproj, C->top());\n@@ -334,1 +333,1 @@\n-  if (projs.catchall_catchproj != nullptr) {\n+  if (projs->catchall_catchproj != nullptr) {\n@@ -337,1 +336,1 @@\n-    for (SimpleDUIterator i(projs.catchall_catchproj); i.has_next(); i.next()) {\n+    for (SimpleDUIterator i(projs->catchall_catchproj); i.has_next(); i.next()) {\n@@ -344,1 +343,1 @@\n-    C->gvn_replace_by(projs.catchall_catchproj, C->top());\n+    C->gvn_replace_by(projs->catchall_catchproj, C->top());\n@@ -346,2 +345,3 @@\n-  if (projs.resproj != nullptr) {\n-    C->gvn_replace_by(projs.resproj, C->top());\n+  if (projs->resproj[0] != nullptr) {\n+    assert(projs->nb_resproj == 1, \"unexpected number of results\");\n+    C->gvn_replace_by(projs->resproj[0], C->top());\n","filename":"src\/hotspot\/share\/opto\/stringopts.cpp","additions":17,"deletions":17,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -894,1 +895,8 @@\n-Node *CmpLNode::Ideal( PhaseGVN *phase, bool can_reshape ) {\n+\/\/------------------------------Ideal------------------------------------------\n+Node* CmpLNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  Node* a = nullptr;\n+  Node* b = nullptr;\n+  if (is_double_null_check(phase, a, b) && (phase->type(a)->is_zero_type() || phase->type(b)->is_zero_type())) {\n+    \/\/ Degraded to a simple null check, use old acmp\n+    return new CmpPNode(a, b);\n+  }\n@@ -905,0 +913,25 @@\n+\/\/ Match double null check emitted by Compile::optimize_acmp()\n+bool CmpLNode::is_double_null_check(PhaseGVN* phase, Node*& a, Node*& b) const {\n+  if (in(1)->Opcode() == Op_OrL &&\n+      in(1)->in(1)->Opcode() == Op_CastP2X &&\n+      in(1)->in(2)->Opcode() == Op_CastP2X &&\n+      in(2)->bottom_type()->is_zero_type()) {\n+    assert(EnableValhalla, \"unexpected double null check\");\n+    a = in(1)->in(1)->in(1);\n+    b = in(1)->in(2)->in(1);\n+    return true;\n+  }\n+  return false;\n+}\n+\n+\/\/------------------------------Value------------------------------------------\n+const Type* CmpLNode::Value(PhaseGVN* phase) const {\n+  Node* a = nullptr;\n+  Node* b = nullptr;\n+  if (is_double_null_check(phase, a, b) && (!phase->type(a)->maybe_null() || !phase->type(b)->maybe_null())) {\n+    \/\/ One operand is never nullptr, emit constant false\n+    return TypeInt::CC_GT;\n+  }\n+  return SubNode::Value(phase);\n+}\n+\n@@ -1030,1 +1063,16 @@\n-\n+    if (!unrelated_classes) {\n+      \/\/ Handle inline type arrays\n+      if ((r0->flat_in_array() && r1->not_flat_in_array()) ||\n+          (r1->flat_in_array() && r0->not_flat_in_array())) {\n+        \/\/ One type is in flat arrays but the other type is not. Must be unrelated.\n+        unrelated_classes = true;\n+      } else if ((r0->is_not_flat() && r1->is_flat()) ||\n+                 (r1->is_not_flat() && r0->is_flat())) {\n+        \/\/ One type is a non-flat array and the other type is a flat array. Must be unrelated.\n+        unrelated_classes = true;\n+      } else if ((r0->is_not_null_free() && r1->is_null_free()) ||\n+                 (r1->is_not_null_free() && r0->is_null_free())) {\n+        \/\/ One type is a nullable array and the other type is a null-free array. Must be unrelated.\n+        unrelated_classes = true;\n+      }\n+    }\n@@ -1115,1 +1163,8 @@\n-Node *CmpPNode::Ideal( PhaseGVN *phase, bool can_reshape ) {\n+Node* CmpPNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n+  \/\/ TODO 8284443 in(1) could be cast?\n+  if (in(1)->is_InlineType() && phase->type(in(2))->is_zero_type()) {\n+    \/\/ Null checking a scalarized but nullable inline type. Check the IsInit\n+    \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+    return new CmpINode(in(1)->as_InlineType()->get_is_init(), phase->intcon(0));\n+  }\n+\n@@ -1187,0 +1242,7 @@\n+  \/\/ Do not fold the subtype check to an array klass pointer comparison for null-able inline type arrays\n+  \/\/ because null-free [LMyValue <: null-able [LMyValue but the klasses are different. Perform a full test.\n+  if (superklass->is_obj_array_klass() && !superklass->as_array_klass()->is_elem_null_free() &&\n+      superklass->as_array_klass()->element_klass()->is_inlinetype()) {\n+    return nullptr;\n+  }\n+\n@@ -1330,0 +1392,37 @@\n+\/\/=============================================================================\n+\/\/------------------------------Value------------------------------------------\n+const Type* FlatArrayCheckNode::Value(PhaseGVN* phase) const {\n+  bool all_not_flat = true;\n+  for (uint i = ArrayOrKlass; i < req(); ++i) {\n+    const Type* t = phase->type(in(i));\n+    if (t == Type::TOP) {\n+      return Type::TOP;\n+    }\n+    if (t->is_ptr()->is_flat()) {\n+      \/\/ One of the input arrays is flat, check always passes\n+      return TypeInt::CC_EQ;\n+    } else if (!t->is_ptr()->is_not_flat()) {\n+      \/\/ One of the input arrays might be flat\n+      all_not_flat = false;\n+    }\n+  }\n+  if (all_not_flat) {\n+    \/\/ None of the input arrays can be flat, check always fails\n+    return TypeInt::CC_GT;\n+  }\n+  return TypeInt::CC;\n+}\n+\n+\/\/------------------------------Ideal------------------------------------------\n+Node* FlatArrayCheckNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  bool changed = false;\n+  \/\/ Remove inputs that are known to be non-flat\n+  for (uint i = ArrayOrKlass; i < req(); ++i) {\n+    const Type* t = phase->type(in(i));\n+    if (t->isa_ptr() && t->is_ptr()->is_not_flat()) {\n+      del_req(i--);\n+      changed = true;\n+    }\n+  }\n+  return changed ? this : nullptr;\n+}\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":102,"deletions":3,"binary":false,"changes":105,"status":"modified"},{"patch":"@@ -208,1 +208,2 @@\n-  virtual Node *Ideal(PhaseGVN *phase, bool can_reshape);\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+  virtual const Type* Value(PhaseGVN* phase) const;\n@@ -210,0 +211,1 @@\n+  bool is_double_null_check(PhaseGVN* phase, Node*& a, Node*& b) const;\n@@ -300,0 +302,20 @@\n+\/\/--------------------------FlatArrayCheckNode---------------------------------\n+\/\/ Returns true if one of the input array objects or array klass ptrs (there\n+\/\/ can be multiple) is flat.\n+class FlatArrayCheckNode : public CmpNode {\n+public:\n+  enum {\n+    Control,\n+    Memory,\n+    ArrayOrKlass\n+  };\n+  FlatArrayCheckNode(Compile* C, Node* mem, Node* array_or_klass) : CmpNode(mem, array_or_klass) {\n+    init_class_id(Class_FlatArrayCheck);\n+    init_flags(Flag_is_macro);\n+    C->add_macro_node(this);\n+  }\n+  virtual int Opcode() const;\n+  virtual const Type* sub(const Type*, const Type*) const { ShouldNotReachHere(); return nullptr; }\n+  const Type* Value(PhaseGVN* phase) const;\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+};\n","filename":"src\/hotspot\/share\/opto\/subnode.hpp","additions":23,"deletions":1,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -49,0 +49,21 @@\n+  \/\/ FIXME: shouldn't this be encoded in helper methods of the type system (maybe_java_subtype_of() etc.?)\n+  \/\/ Similar to logic in CmpPNode::sub()\n+  bool unrelated_classes = false;\n+  \/\/ Handle inline type arrays\n+  if (subk->flat_in_array() && superk->not_flat_in_array()) {\n+    \/\/ The subtype is in flat arrays and the supertype is not in flat arrays. Must be unrelated.\n+    unrelated_classes = true;\n+  } else if (subk->is_not_flat() && superk->is_flat()) {\n+    \/\/ The subtype is a non-flat array and the supertype is a flat array. Must be unrelated.\n+    unrelated_classes = true;\n+  } else if (subk->is_not_null_free() && superk->is_null_free()) {\n+    \/\/ The subtype is a nullable array and the supertype is null-free array. Must be unrelated.\n+    unrelated_classes = true;\n+  }\n+  if (unrelated_classes) {\n+    TypePtr::PTR jp = sub_t->is_ptr()->join_ptr(super_t->is_ptr()->_ptr);\n+    if (jp != TypePtr::Null && jp != TypePtr::BotPTR) {\n+      return TypeInt::CC_GT;\n+    }\n+  }\n+\n@@ -246,1 +267,1 @@\n-#endif\n+#endif\n","filename":"src\/hotspot\/share\/opto\/subtypenode.cpp","additions":22,"deletions":1,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -25,0 +25,3 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciField.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -54,0 +57,45 @@\n+const Type::Offset Type::Offset::top(Type::OffsetTop);\n+const Type::Offset Type::Offset::bottom(Type::OffsetBot);\n+\n+const Type::Offset Type::Offset::meet(const Type::Offset other) const {\n+  \/\/ Either is 'TOP' offset?  Return the other offset!\n+  if (_offset == OffsetTop) return other;\n+  if (other._offset == OffsetTop) return *this;\n+  \/\/ If either is different, return 'BOTTOM' offset\n+  if (_offset != other._offset) return bottom;\n+  return Offset(_offset);\n+}\n+\n+const Type::Offset Type::Offset::dual() const {\n+  if (_offset == OffsetTop) return bottom;\/\/ Map 'TOP' into 'BOTTOM'\n+  if (_offset == OffsetBot) return top;\/\/ Map 'BOTTOM' into 'TOP'\n+  return Offset(_offset);               \/\/ Map everything else into self\n+}\n+\n+const Type::Offset Type::Offset::add(intptr_t offset) const {\n+  \/\/ Adding to 'TOP' offset?  Return 'TOP'!\n+  if (_offset == OffsetTop || offset == OffsetTop) return top;\n+  \/\/ Adding to 'BOTTOM' offset?  Return 'BOTTOM'!\n+  if (_offset == OffsetBot || offset == OffsetBot) return bottom;\n+  \/\/ Addition overflows or \"accidentally\" equals to OffsetTop? Return 'BOTTOM'!\n+  offset += (intptr_t)_offset;\n+  if (offset != (int)offset || offset == OffsetTop) return bottom;\n+\n+  \/\/ assert( _offset >= 0 && _offset+offset >= 0, \"\" );\n+  \/\/ It is possible to construct a negative offset during PhaseCCP\n+\n+  return Offset((int)offset);        \/\/ Sum valid offsets\n+}\n+\n+void Type::Offset::dump2(outputStream *st) const {\n+  if (_offset == 0) {\n+    return;\n+  } else if (_offset == OffsetTop) {\n+    st->print(\"+top\");\n+  }\n+  else if (_offset == OffsetBot) {\n+    st->print(\"+bot\");\n+  } else if (_offset) {\n+    st->print(\"+%d\", _offset);\n+  }\n+}\n@@ -228,0 +276,3 @@\n+  case T_OBJECT:\n+    return Type::get_const_type(type->unwrap())->join_speculative(type->is_null_free() ? TypePtr::NOTNULL : TypePtr::BOTTOM);\n+\n@@ -540,3 +591,3 @@\n-  TypePtr::NULL_PTR= TypePtr::make(AnyPtr, TypePtr::Null, 0);\n-  TypePtr::NOTNULL = TypePtr::make(AnyPtr, TypePtr::NotNull, OffsetBot);\n-  TypePtr::BOTTOM  = TypePtr::make(AnyPtr, TypePtr::BotPTR, OffsetBot);\n+  TypePtr::NULL_PTR= TypePtr::make(AnyPtr, TypePtr::Null, Offset(0));\n+  TypePtr::NOTNULL = TypePtr::make(AnyPtr, TypePtr::NotNull, Offset::bottom);\n+  TypePtr::BOTTOM  = TypePtr::make(AnyPtr, TypePtr::BotPTR, Offset::bottom);\n@@ -559,1 +610,1 @@\n-                                           false, nullptr, oopDesc::mark_offset_in_bytes());\n+                                           false, nullptr, Offset(oopDesc::mark_offset_in_bytes()));\n@@ -561,2 +612,2 @@\n-                                           false, nullptr, oopDesc::klass_offset_in_bytes());\n-  TypeOopPtr::BOTTOM  = TypeOopPtr::make(TypePtr::BotPTR, OffsetBot, TypeOopPtr::InstanceBot);\n+                                           false, nullptr, Offset(oopDesc::klass_offset_in_bytes()));\n+  TypeOopPtr::BOTTOM  = TypeOopPtr::make(TypePtr::BotPTR, Offset::bottom, TypeOopPtr::InstanceBot);\n@@ -564,1 +615,1 @@\n-  TypeMetadataPtr::BOTTOM = TypeMetadataPtr::make(TypePtr::BotPTR, nullptr, OffsetBot);\n+  TypeMetadataPtr::BOTTOM = TypeMetadataPtr::make(TypePtr::BotPTR, nullptr, Offset::bottom);\n@@ -587,2 +638,2 @@\n-  TypeAryPtr::BOTTOM = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::BOTTOM, TypeInt::POS), nullptr, false, Type::OffsetBot);\n-  TypeAryPtr::RANGE   = TypeAryPtr::make( TypePtr::BotPTR, TypeAry::make(Type::BOTTOM,TypeInt::POS), nullptr \/* current->env()->Object_klass() *\/, false, arrayOopDesc::length_offset_in_bytes());\n+  TypeAryPtr::BOTTOM = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::BOTTOM, TypeInt::POS), nullptr, false, Offset::bottom);\n+  TypeAryPtr::RANGE   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::BOTTOM,TypeInt::POS), nullptr \/* current->env()->Object_klass() *\/, false, Offset(arrayOopDesc::length_offset_in_bytes()));\n@@ -590,1 +641,1 @@\n-  TypeAryPtr::NARROWOOPS = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeNarrowOop::BOTTOM, TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Type::OffsetBot);\n+  TypeAryPtr::NARROWOOPS = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeNarrowOop::BOTTOM, TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Offset::bottom);\n@@ -600,1 +651,1 @@\n-    TypeAryPtr::OOPS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Type::OffsetBot);\n+    TypeAryPtr::OOPS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Offset::bottom);\n@@ -602,7 +653,8 @@\n-  TypeAryPtr::BYTES   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::BYTE      ,TypeInt::POS), ciTypeArrayKlass::make(T_BYTE),   true,  Type::OffsetBot);\n-  TypeAryPtr::SHORTS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::SHORT     ,TypeInt::POS), ciTypeArrayKlass::make(T_SHORT),  true,  Type::OffsetBot);\n-  TypeAryPtr::CHARS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::CHAR      ,TypeInt::POS), ciTypeArrayKlass::make(T_CHAR),   true,  Type::OffsetBot);\n-  TypeAryPtr::INTS    = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::INT       ,TypeInt::POS), ciTypeArrayKlass::make(T_INT),    true,  Type::OffsetBot);\n-  TypeAryPtr::LONGS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeLong::LONG     ,TypeInt::POS), ciTypeArrayKlass::make(T_LONG),   true,  Type::OffsetBot);\n-  TypeAryPtr::FLOATS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::FLOAT        ,TypeInt::POS), ciTypeArrayKlass::make(T_FLOAT),  true,  Type::OffsetBot);\n-  TypeAryPtr::DOUBLES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::DOUBLE       ,TypeInt::POS), ciTypeArrayKlass::make(T_DOUBLE), true,  Type::OffsetBot);\n+  TypeAryPtr::BYTES   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::BYTE      ,TypeInt::POS), ciTypeArrayKlass::make(T_BYTE),   true,  Offset::bottom);\n+  TypeAryPtr::SHORTS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::SHORT     ,TypeInt::POS), ciTypeArrayKlass::make(T_SHORT),  true,  Offset::bottom);\n+  TypeAryPtr::CHARS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::CHAR      ,TypeInt::POS), ciTypeArrayKlass::make(T_CHAR),   true,  Offset::bottom);\n+  TypeAryPtr::INTS    = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInt::INT       ,TypeInt::POS), ciTypeArrayKlass::make(T_INT),    true,  Offset::bottom);\n+  TypeAryPtr::LONGS   = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeLong::LONG     ,TypeInt::POS), ciTypeArrayKlass::make(T_LONG),   true,  Offset::bottom);\n+  TypeAryPtr::FLOATS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::FLOAT        ,TypeInt::POS), ciTypeArrayKlass::make(T_FLOAT),  true,  Offset::bottom);\n+  TypeAryPtr::DOUBLES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(Type::DOUBLE       ,TypeInt::POS), ciTypeArrayKlass::make(T_DOUBLE), true,  Offset::bottom);\n+  TypeAryPtr::INLINES = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ true), nullptr, false, Offset::bottom);\n@@ -613,0 +665,1 @@\n+  TypeAryPtr::_array_body_type[T_FLAT_ELEMENT] = TypeAryPtr::OOPS;\n@@ -623,2 +676,2 @@\n-  TypeInstKlassPtr::OBJECT = TypeInstKlassPtr::make(TypePtr::NotNull, current->env()->Object_klass(), 0);\n-  TypeInstKlassPtr::OBJECT_OR_NULL = TypeInstKlassPtr::make(TypePtr::BotPTR, current->env()->Object_klass(), 0);\n+  TypeInstKlassPtr::OBJECT = TypeInstKlassPtr::make(TypePtr::NotNull, current->env()->Object_klass(), Offset(0));\n+  TypeInstKlassPtr::OBJECT_OR_NULL = TypeInstKlassPtr::make(TypePtr::BotPTR, current->env()->Object_klass(), Offset(0));\n@@ -663,0 +716,1 @@\n+  _const_basic_type[T_FLAT_ELEMENT] = TypeInstPtr::BOTTOM;\n@@ -679,0 +733,1 @@\n+  _zero_type[T_FLAT_ELEMENT] = TypePtr::NULL_PTR;\n@@ -953,0 +1008,3 @@\n+\n+  \/\/ Verify that:\n+  \/\/      this meet t == t meet this\n@@ -969,0 +1027,6 @@\n+  \/\/ Verify that:\n+  \/\/      !(t meet this)  meet !t ==\n+  \/\/      (!t join !this) meet !t == !t\n+  \/\/ and\n+  \/\/      !(t meet this)  meet !this ==\n+  \/\/      (!t join !this) meet !this == !this\n@@ -2139,0 +2203,21 @@\n+static void collect_inline_fields(ciInlineKlass* vk, const Type** field_array, uint& pos) {\n+  for (int i = 0; i < vk->nof_declared_nonstatic_fields(); i++) {\n+    ciField* field = vk->declared_nonstatic_field_at(i);\n+    if (field->is_flat()) {\n+      collect_inline_fields(field->type()->as_inline_klass(), field_array, pos);\n+      if (!field->is_null_free()) {\n+        \/\/ Use T_INT instead of T_BOOLEAN here because the upper bits can contain garbage if the holder\n+        \/\/ is null and C2 will only zero them for T_INT assuming that T_BOOLEAN is already canonicalized.\n+        field_array[pos++] = Type::get_const_basic_type(T_INT);\n+      }\n+    } else {\n+      BasicType bt = field->type()->basic_type();\n+      const Type* ft = Type::get_const_type(field->type());\n+      field_array[pos++] = ft;\n+      if (type2size[bt] == 2) {\n+        field_array[pos++] = Type::HALF;\n+      }\n+    }\n+  }\n+}\n+\n@@ -2141,1 +2226,1 @@\n-const TypeTuple *TypeTuple::make_range(ciSignature* sig, InterfaceHandling interface_handling) {\n+const TypeTuple *TypeTuple::make_range(ciSignature* sig, InterfaceHandling interface_handling, bool ret_vt_fields) {\n@@ -2144,0 +2229,5 @@\n+  if (ret_vt_fields) {\n+    arg_cnt = return_type->as_inline_klass()->inline_arg_slots() + 1;\n+    \/\/ InlineTypeNode::IsInit field used for null checking\n+    arg_cnt++;\n+  }\n@@ -2155,0 +2245,12 @@\n+    if (return_type->is_inlinetype() && ret_vt_fields) {\n+      uint pos = TypeFunc::Parms;\n+      field_array[pos++] = get_const_type(return_type); \/\/ Oop might be null when returning as fields\n+      collect_inline_fields(return_type->as_inline_klass(), field_array, pos);\n+      \/\/ InlineTypeNode::IsInit field used for null checking\n+      field_array[pos++] = get_const_basic_type(T_BOOLEAN);\n+      assert(pos == (TypeFunc::Parms + arg_cnt), \"out of bounds\");\n+      break;\n+    } else {\n+      field_array[TypeFunc::Parms] = get_const_type(return_type, interface_handling)->join_speculative(TypePtr::BOTTOM);\n+    }\n+    break;\n@@ -2173,2 +2275,10 @@\n-const TypeTuple *TypeTuple::make_domain(ciInstanceKlass* recv, ciSignature* sig, InterfaceHandling interface_handling) {\n-  uint arg_cnt = sig->size();\n+const TypeTuple *TypeTuple::make_domain(ciMethod* method, InterfaceHandling interface_handling, bool vt_fields_as_args) {\n+  ciSignature* sig = method->signature();\n+  uint arg_cnt = sig->size() + (method->is_static() ? 0 : 1);\n+  if (vt_fields_as_args) {\n+    arg_cnt = 0;\n+    assert(method->get_sig_cc() != nullptr, \"Should have scalarized signature\");\n+    for (ExtendedSignature sig_cc = ExtendedSignature(method->get_sig_cc(), SigEntryFilter()); !sig_cc.at_end(); ++sig_cc) {\n+      arg_cnt += type2size[(*sig_cc)._bt];\n+    }\n+  }\n@@ -2177,8 +2287,8 @@\n-  const Type **field_array;\n-  if (recv != nullptr) {\n-    arg_cnt++;\n-    field_array = fields(arg_cnt);\n-    \/\/ Use get_const_type here because it respects UseUniqueSubclasses:\n-    field_array[pos++] = get_const_type(recv, interface_handling)->join_speculative(TypePtr::NOTNULL);\n-  } else {\n-    field_array = fields(arg_cnt);\n+  const Type** field_array = fields(arg_cnt);\n+  if (!method->is_static()) {\n+    ciInstanceKlass* recv = method->holder();\n+    if (vt_fields_as_args && recv->is_inlinetype() && recv->as_inline_klass()->can_be_passed_as_fields() && method->is_scalarized_arg(0)) {\n+      collect_inline_fields(recv->as_inline_klass(), field_array, pos);\n+    } else {\n+      field_array[pos++] = get_const_type(recv, interface_handling)->join_speculative(TypePtr::NOTNULL);\n+    }\n@@ -2190,0 +2300,1 @@\n+    BasicType bt = type->basic_type();\n@@ -2191,1 +2302,1 @@\n-    switch (type->basic_type()) {\n+    switch (bt) {\n@@ -2201,0 +2312,8 @@\n+      if (type->is_inlinetype() && vt_fields_as_args && method->is_scalarized_arg(i + (method->is_static() ? 0 : 1))) {\n+        \/\/ InlineTypeNode::IsInit field used for null checking\n+        field_array[pos++] = get_const_basic_type(T_BOOLEAN);\n+        collect_inline_fields(type->as_inline_klass(), field_array, pos);\n+      } else {\n+        field_array[pos++] = get_const_type(type, interface_handling);\n+      }\n+      break;\n@@ -2217,0 +2336,1 @@\n+  assert(pos == TypeFunc::Parms + arg_cnt, \"wrong number of arguments\");\n@@ -2351,1 +2471,2 @@\n-const TypeAry* TypeAry::make(const Type* elem, const TypeInt* size, bool stable) {\n+const TypeAry* TypeAry::make(const Type* elem, const TypeInt* size, bool stable,\n+                             bool flat, bool not_flat, bool not_null_free) {\n@@ -2356,1 +2477,1 @@\n-  return (TypeAry*)(new TypeAry(elem,size,stable))->hashcons();\n+  return (TypeAry*)(new TypeAry(elem, size, stable, flat, not_flat, not_null_free))->hashcons();\n@@ -2378,1 +2499,4 @@\n-                         _stable && a->_stable);\n+                         _stable && a->_stable,\n+                         _flat && a->_flat,\n+                         _not_flat && a->_not_flat,\n+                         _not_null_free && a->_not_null_free);\n@@ -2391,1 +2515,1 @@\n-  return new TypeAry(_elem->dual(), size_dual, !_stable);\n+  return new TypeAry(_elem->dual(), size_dual, !_stable, !_flat, !_not_flat, !_not_null_free);\n@@ -2400,1 +2524,5 @@\n-    _size == a->_size;\n+    _size == a->_size &&\n+    _flat == a->_flat &&\n+    _not_flat == a->_not_flat &&\n+    _not_null_free == a->_not_null_free;\n+\n@@ -2406,1 +2534,2 @@\n-  return (uint)(uintptr_t)_elem + (uint)(uintptr_t)_size + (uint)(_stable ? 43 : 0);\n+  return (uint)(uintptr_t)_elem + (uint)(uintptr_t)_size + (uint)(_stable ? 43 : 0) +\n+      (uint)(_flat ? 44 : 0) + (uint)(_not_flat ? 45 : 0) + (uint)(_not_null_free ? 46 : 0);\n@@ -2413,1 +2542,1 @@\n-  return make(_elem->remove_speculative(), _size, _stable);\n+  return make(_elem->remove_speculative(), _size, _stable, _flat, _not_flat, _not_null_free);\n@@ -2420,1 +2549,1 @@\n-  return make(_elem->cleanup_speculative(), _size, _stable);\n+  return make(_elem->cleanup_speculative(), _size, _stable, _flat, _not_flat, _not_null_free);\n@@ -2439,0 +2568,5 @@\n+  if (_flat) st->print(\"flat:\");\n+  if (Verbose) {\n+    if (_not_flat) st->print(\"not flat:\");\n+    if (_not_null_free) st->print(\"not null free:\");\n+  }\n@@ -2478,2 +2612,10 @@\n-  if (tinst)\n-    return tinst->instance_klass()->is_final();\n+  if (tinst) {\n+    if (tinst->instance_klass()->is_final()) {\n+      \/\/ Even though MyValue is final, [LMyValue is not exact because null-free [LMyValue is a subtype.\n+      if (tinst->is_inlinetypeptr() && (tinst->ptr() == TypePtr::BotPTR || tinst->ptr() == TypePtr::TopPTR)) {\n+        return false;\n+      }\n+      return true;\n+    }\n+    return false;\n+  }\n@@ -2654,1 +2796,1 @@\n-const TypePtr *TypePtr::make(TYPES t, enum PTR ptr, int offset, const TypePtr* speculative, int inline_depth) {\n+const TypePtr* TypePtr::make(TYPES t, enum PTR ptr, Offset offset, const TypePtr* speculative, int inline_depth) {\n@@ -2668,1 +2810,1 @@\n-  return _offset;\n+  return offset();\n@@ -2739,7 +2881,2 @@\n-int TypePtr::meet_offset( int offset ) const {\n-  \/\/ Either is 'TOP' offset?  Return the other offset!\n-  if( _offset == OffsetTop ) return offset;\n-  if( offset == OffsetTop ) return _offset;\n-  \/\/ If either is different, return 'BOTTOM' offset\n-  if( _offset != offset ) return OffsetBot;\n-  return _offset;\n+Type::Offset TypePtr::meet_offset(int offset) const {\n+  return _offset.meet(Offset(offset));\n@@ -2749,4 +2886,2 @@\n-int TypePtr::dual_offset( ) const {\n-  if( _offset == OffsetTop ) return OffsetBot;\/\/ Map 'TOP' into 'BOTTOM'\n-  if( _offset == OffsetBot ) return OffsetTop;\/\/ Map 'BOTTOM' into 'TOP'\n-  return _offset;               \/\/ Map everything else into self\n+Type::Offset TypePtr::dual_offset() const {\n+  return _offset.dual();\n@@ -2765,13 +2900,2 @@\n-int TypePtr::xadd_offset( intptr_t offset ) const {\n-  \/\/ Adding to 'TOP' offset?  Return 'TOP'!\n-  if( _offset == OffsetTop || offset == OffsetTop ) return OffsetTop;\n-  \/\/ Adding to 'BOTTOM' offset?  Return 'BOTTOM'!\n-  if( _offset == OffsetBot || offset == OffsetBot ) return OffsetBot;\n-  \/\/ Addition overflows or \"accidentally\" equals to OffsetTop? Return 'BOTTOM'!\n-  offset += (intptr_t)_offset;\n-  if (offset != (int)offset || offset == OffsetTop) return OffsetBot;\n-\n-  \/\/ assert( _offset >= 0 && _offset+offset >= 0, \"\" );\n-  \/\/ It is possible to construct a negative offset during PhaseCCP\n-\n-  return (int)offset;        \/\/ Sum valid offsets\n+Type::Offset TypePtr::xadd_offset(intptr_t offset) const {\n+  return _offset.add(offset);\n@@ -2786,1 +2910,1 @@\n-  return make(AnyPtr, _ptr, offset, _speculative, _inline_depth);\n+  return make(AnyPtr, _ptr, Offset(offset), _speculative, _inline_depth);\n@@ -2793,1 +2917,1 @@\n-  return _ptr == a->ptr() && _offset == a->offset() && eq_speculative(a) && _inline_depth == a->_inline_depth;\n+  return _ptr == a->ptr() && _offset == a->_offset && eq_speculative(a) && _inline_depth == a->_inline_depth;\n@@ -2799,1 +2923,1 @@\n-  return (uint)_ptr + (uint)_offset + (uint)hash_speculative() + (uint)_inline_depth;\n+  return (uint)_ptr + (uint)offset() + (uint)hash_speculative() + (uint)_inline_depth;\n@@ -3065,3 +3189,1 @@\n-  if( _offset == OffsetTop ) st->print(\"+top\");\n-  else if( _offset == OffsetBot ) st->print(\"+bot\");\n-  else if( _offset ) st->print(\"+%d\", _offset);\n+  _offset.dump2(st);\n@@ -3102,1 +3224,1 @@\n-  return (_offset != OffsetBot) && !below_centerline(_ptr);\n+  return (_offset != Offset::bottom) && !below_centerline(_ptr);\n@@ -3106,1 +3228,1 @@\n-  return (_offset == OffsetTop) || above_centerline(_ptr);\n+  return (_offset == Offset::top) || above_centerline(_ptr);\n@@ -3507,1 +3629,1 @@\n-TypeOopPtr::TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int offset,\n+TypeOopPtr::TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset, Offset field_offset,\n@@ -3523,2 +3645,2 @@\n-      (offset > 0) && xk && (k != nullptr) && k->is_instance_klass()) {\n-    _is_ptr_to_boxed_value = k->as_instance_klass()->is_boxed_value_offset(offset);\n+      (offset.get() > 0) && xk && (k != nullptr) && k->is_instance_klass()) {\n+    _is_ptr_to_boxed_value = k->as_instance_klass()->is_boxed_value_offset(offset.get());\n@@ -3527,2 +3649,2 @@\n-  if (_offset > 0 || _offset == Type::OffsetTop || _offset == Type::OffsetBot) {\n-    if (_offset == oopDesc::klass_offset_in_bytes()) {\n+  if (this->offset() > 0 || this->offset() == Type::OffsetTop || this->offset() == Type::OffsetBot) {\n+    if (this->offset() == oopDesc::klass_offset_in_bytes()) {\n@@ -3534,3 +3656,12 @@\n-    } else if (this->isa_aryptr()) {\n-      _is_ptr_to_narrowoop = (UseCompressedOops && klass()->is_obj_array_klass() &&\n-                             _offset != arrayOopDesc::length_offset_in_bytes());\n+    } else if (UseCompressedOops && this->isa_aryptr() && this->offset() != arrayOopDesc::length_offset_in_bytes()) {\n+      if (klass()->is_obj_array_klass()) {\n+        _is_ptr_to_narrowoop = true;\n+      } else if (klass()->is_flat_array_klass() && field_offset != Offset::top && field_offset != Offset::bottom) {\n+        \/\/ Check if the field of the inline type array element contains oops\n+        ciInlineKlass* vk = klass()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+        int foffset = field_offset.get() + vk->payload_offset();\n+        ciField* field = vk->get_field_by_offset(foffset, false);\n+        assert(field != nullptr, \"missing field\");\n+        BasicType bt = field->layout_type();\n+        _is_ptr_to_narrowoop = UseCompressedOops && ::is_reference_type(bt);\n+      }\n@@ -3538,1 +3669,0 @@\n-      ciInstanceKlass* ik = klass()->as_instance_klass();\n@@ -3541,1 +3671,1 @@\n-      } else if (_offset == OffsetBot || _offset == OffsetTop) {\n+      } else if (_offset == Offset::bottom || _offset == Offset::top) {\n@@ -3546,3 +3676,2 @@\n-\n-            (_offset == java_lang_Class::klass_offset() ||\n-             _offset == java_lang_Class::array_klass_offset())) {\n+            (this->offset() == java_lang_Class::klass_offset() ||\n+             this->offset() == java_lang_Class::array_klass_offset())) {\n@@ -3554,1 +3683,1 @@\n-                   _offset >= InstanceMirrorKlass::offset_of_static_fields()) {\n+                   this->offset() >= InstanceMirrorKlass::offset_of_static_fields()) {\n@@ -3559,1 +3688,1 @@\n-            field = k->get_field_by_offset(_offset, true);\n+            field = k->get_field_by_offset(this->offset(), true);\n@@ -3570,1 +3699,2 @@\n-          ciField* field = ik->get_field_by_offset(_offset, false);\n+          ciInstanceKlass* ik = klass()->as_instance_klass();\n+          ciField* field = ik->get_field_by_offset(this->offset(), false);\n@@ -3590,2 +3720,2 @@\n-const TypeOopPtr *TypeOopPtr::make(PTR ptr, int offset, int instance_id,\n-                                     const TypePtr* speculative, int inline_depth) {\n+const TypeOopPtr *TypeOopPtr::make(PTR ptr, Offset offset, int instance_id,\n+                                   const TypePtr* speculative, int inline_depth) {\n@@ -3597,1 +3727,1 @@\n-  return (TypeOopPtr*)(new TypeOopPtr(OopPtr, ptr, k, interfaces, xk, o, offset, instance_id, speculative, inline_depth))->hashcons();\n+  return (TypeOopPtr*)(new TypeOopPtr(OopPtr, ptr, k, interfaces, xk, o, offset, Offset::bottom, instance_id, speculative, inline_depth))->hashcons();\n@@ -3622,1 +3752,0 @@\n-\n@@ -3668,1 +3797,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -3710,1 +3839,1 @@\n-  return new TypeOopPtr(_base, dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n+  return new TypeOopPtr(_base, dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), Offset::bottom, dual_instance_id(), dual_speculative(), dual_inline_depth());\n@@ -3715,2 +3844,2 @@\n-const TypeOopPtr* TypeOopPtr::make_from_klass_common(ciKlass* klass, bool klass_change, bool try_for_exact, InterfaceHandling interface_handling) {\n-  if (klass->is_instance_klass()) {\n+const TypeOopPtr* TypeOopPtr::make_from_klass_common(ciKlass *klass, bool klass_change, bool try_for_exact, InterfaceHandling interface_handling) {\n+  if (klass->is_instance_klass() || klass->is_inlinetype()) {\n@@ -3743,1 +3872,1 @@\n-    return TypeInstPtr::make(TypePtr::BotPTR, klass, interfaces, klass_is_exact, nullptr, 0);\n+    return TypeInstPtr::make(TypePtr::BotPTR, klass, interfaces, klass_is_exact, nullptr, Offset(0));\n@@ -3745,5 +3874,13 @@\n-    \/\/ Element is an object array. Recursively call ourself.\n-    ciKlass* eklass = klass->as_obj_array_klass()->element_klass();\n-    const TypeOopPtr *etype = TypeOopPtr::make_from_klass_common(eklass, false, try_for_exact, interface_handling);\n-    bool xk = etype->klass_is_exact();\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+    \/\/ Element is an object or inline type array. Recursively call ourself.\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_common(klass->as_array_klass()->element_klass(), \/* klass_change= *\/ false, try_for_exact, interface_handling);\n+    \/\/ Determine null-free\/flat properties\n+    const TypeOopPtr* exact_etype = etype;\n+    if (etype->can_be_inline_type()) {\n+      \/\/ Use exact type if element can be an inline type\n+      exact_etype = TypeOopPtr::make_from_klass_common(klass->as_array_klass()->element_klass(), \/* klass_change= *\/ true, \/* try_for_exact= *\/ true, interface_handling);\n+    }\n+    bool not_null_free = !exact_etype->can_be_inline_type();\n+    bool not_flat = !UseArrayFlattening || not_null_free || (exact_etype->is_inlinetypeptr() && !exact_etype->inline_klass()->flat_in_array());\n+    \/\/ Even though MyValue is final, [LMyValue is not exact because null-free [LMyValue is a subtype.\n+    bool xk = etype->klass_is_exact() && !etype->is_inlinetypeptr();\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, not_flat, not_null_free);\n@@ -3752,2 +3889,2 @@\n-    \/\/ slam nulls down in the subarrays.\n-    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, nullptr, xk, 0);\n+    \/\/ slam nullptrs down in the subarrays.\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, nullptr, xk, Offset(0));\n@@ -3758,1 +3895,2 @@\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS,\n+                                        \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3761,1 +3899,7 @@\n-    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, 0);\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, Offset(0));\n+    return arr;\n+  } else if (klass->is_flat_array_klass()) {\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_raw(klass->as_array_klass()->element_klass(), trust_interfaces);\n+    etype = etype->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ true);\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, klass, true, Offset(0));\n@@ -3777,2 +3921,2 @@\n-  if (klass->is_instance_klass()) {\n-    \/\/ Element is an instance\n+  if (klass->is_instance_klass() || klass->is_inlinetype()) {\n+    \/\/ Element is an instance or inline type\n@@ -3782,1 +3926,1 @@\n-      return TypeInstPtr::make(TypePtr::NotNull, klass, true, nullptr, 0);\n+      return TypeInstPtr::make(TypePtr::NotNull, klass, true, nullptr, Offset(0));\n@@ -3786,3 +3930,8 @@\n-    const TypeOopPtr *etype =\n-      TypeOopPtr::make_from_klass_raw(klass->as_obj_array_klass()->element_klass(), trust_interfaces);\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()));\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_raw(klass->as_array_klass()->element_klass(), trust_interfaces);\n+    bool is_flat = o->as_obj_array()->is_flat();\n+    bool is_null_free = o->as_obj_array()->is_null_free();\n+    if (is_null_free) {\n+      etype = etype->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()),\n+                                        \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ !is_flat, \/* not_null_free= *\/ !is_null_free);\n@@ -3793,1 +3942,1 @@\n-      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n@@ -3795,1 +3944,1 @@\n-      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n@@ -3799,3 +3948,3 @@\n-    const Type* etype =\n-      (Type*)get_const_basic_type(klass->as_type_array_klass()->element_type());\n-    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()));\n+    const Type* etype = (Type*)get_const_basic_type(klass->as_type_array_klass()->element_type());\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()),\n+                                        \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3805,1 +3954,1 @@\n-      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n@@ -3807,1 +3956,13 @@\n-      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, 0);\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n+    }\n+  } else if (klass->is_flat_array_klass()) {\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass_raw(klass->as_array_klass()->element_klass(), trust_interfaces);\n+    etype = etype->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::make(o->as_array()->length()), \/* stable= *\/ false, \/* flat= *\/ true);\n+    \/\/ We used to pass NotNull in here, asserting that the sub-arrays\n+    \/\/ are all not-null.  This is not true in generally, as code can\n+    \/\/ slam nullptrs down in the subarrays.\n+    if (make_constant) {\n+      return TypeAryPtr::make(TypePtr::Constant, o, arr0, klass, true, Offset(0));\n+    } else {\n+      return TypeAryPtr::make(TypePtr::NotNull, arr0, klass, true, Offset(0));\n@@ -3818,1 +3979,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert(offset() >= 0, \"\");\n@@ -3820,1 +3981,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -3881,6 +4042,1 @@\n-  switch( _offset ) {\n-  case OffsetTop: st->print(\"+top\"); break;\n-  case OffsetBot: st->print(\"+any\"); break;\n-  case         0: break;\n-  default:        st->print(\"+%d\",_offset); break;\n-  }\n+  _offset.dump2(st);\n@@ -3903,1 +4059,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -3912,1 +4068,1 @@\n-  return make(_ptr, offset, _instance_id, with_offset_speculative(offset), _inline_depth);\n+  return make(_ptr, Offset(offset), _instance_id, with_offset_speculative(offset), _inline_depth);\n@@ -4025,3 +4181,4 @@\n-TypeInstPtr::TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int off,\n-                         int instance_id, const TypePtr* speculative, int inline_depth)\n-  : TypeOopPtr(InstPtr, ptr, k, interfaces, xk, o, off, instance_id, speculative, inline_depth) {\n+TypeInstPtr::TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset off,\n+                         bool flat_in_array, int instance_id, const TypePtr* speculative, int inline_depth)\n+  : TypeOopPtr(InstPtr, ptr, k, interfaces, xk, o, off, Offset::bottom, instance_id, speculative, inline_depth),\n+    _flat_in_array(flat_in_array) {\n@@ -4032,0 +4189,2 @@\n+  assert(!klass()->flat_in_array() || flat_in_array, \"Should be flat in array\");\n+  assert(!flat_in_array || can_be_inline_type(), \"Only inline types can be flat in array\");\n@@ -4040,1 +4199,2 @@\n-                                     int offset,\n+                                     Offset offset,\n+                                     bool flat_in_array,\n@@ -4062,0 +4222,3 @@\n+  \/\/ Check if this type is known to be flat in arrays\n+  flat_in_array = flat_in_array || k->flat_in_array();\n+\n@@ -4064,1 +4227,1 @@\n-    (TypeInstPtr*)(new TypeInstPtr(ptr, k, interfaces, xk, o ,offset, instance_id, speculative, inline_depth))->hashcons();\n+    (TypeInstPtr*)(new TypeInstPtr(ptr, k, interfaces, xk, o, offset, flat_in_array, instance_id, speculative, inline_depth))->hashcons();\n@@ -4130,1 +4293,1 @@\n-  return make(ptr, klass(), _interfaces, klass_is_exact(), ptr == Constant ? const_oop() : nullptr, _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, klass(), _interfaces, klass_is_exact(), ptr == Constant ? const_oop() : nullptr, _offset, _flat_in_array, _instance_id, _speculative, _inline_depth);\n@@ -4141,1 +4304,1 @@\n-  return make(ptr(), klass(), _interfaces, klass_is_exact, const_oop(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), klass(), _interfaces, klass_is_exact, const_oop(), _offset, _flat_in_array, _instance_id, _speculative, _inline_depth);\n@@ -4147,1 +4310,1 @@\n-  return make(_ptr, klass(),  _interfaces, _klass_is_exact, const_oop(), _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, klass(), _interfaces, _klass_is_exact, const_oop(), _offset, _flat_in_array, instance_id, _speculative, _inline_depth);\n@@ -4154,1 +4317,1 @@\n-  int off = meet_offset(tinst->offset());\n+  Offset off = meet_offset(tinst->offset());\n@@ -4179,1 +4342,1 @@\n-    else if (loaded->ptr() == TypePtr::AnyNull)  { return make(ptr, unloaded->klass(), interfaces, false, nullptr, off, instance_id, speculative, depth); }\n+    else if (loaded->ptr() == TypePtr::AnyNull)  { return make(ptr, unloaded->klass(), interfaces, false, nullptr, off, false, instance_id, speculative, depth); }\n@@ -4240,1 +4403,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4249,1 +4412,1 @@\n-                  (ptr == Constant ? const_oop() : nullptr), offset, instance_id, speculative, depth);\n+                  (ptr == Constant ? const_oop() : nullptr), offset, flat_in_array(), instance_id, speculative, depth);\n@@ -4265,1 +4428,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4277,1 +4440,1 @@\n-                  (ptr == Constant ? const_oop() : nullptr), offset, instance_id, speculative, depth);\n+                  (ptr == Constant ? const_oop() : nullptr), offset, flat_in_array(), instance_id, speculative, depth);\n@@ -4305,1 +4468,1 @@\n-    int off = meet_offset(tinst->offset());\n+    Offset off = meet_offset(tinst->offset());\n@@ -4317,0 +4480,1 @@\n+    bool res_flat_in_array = false;\n@@ -4318,1 +4482,1 @@\n-    MeetResult kind = meet_instptr(ptr, interfaces, this, tinst, res_klass, res_xk);\n+    MeetResult kind = meet_instptr(ptr, interfaces, this, tinst, res_klass, res_xk, res_flat_in_array);\n@@ -4359,1 +4523,1 @@\n-      res = make(ptr, res_klass, interfaces, res_xk, o, off, instance_id, speculative, depth);\n+      res = make(ptr, res_klass, interfaces, res_xk, o, off, res_flat_in_array, instance_id, speculative, depth);\n@@ -4371,1 +4535,1 @@\n-                                                            ciKlass*& res_klass, bool& res_xk) {\n+                                                            ciKlass*& res_klass, bool& res_xk, bool& res_flat_in_array) {\n@@ -4374,0 +4538,5 @@\n+  const bool this_flat_in_array = this_type->flat_in_array();\n+  const bool other_flat_in_array = other_type->flat_in_array();\n+  const bool this_not_flat_in_array = this_type->not_flat_in_array();\n+  const bool other_not_flat_in_array = other_type->not_flat_in_array();\n+\n@@ -4384,1 +4553,1 @@\n-  if (ptr != Constant && this_klass->equals(other_klass) && this_xk == other_xk) {\n+  if (ptr != Constant && this_klass->equals(other_klass) && this_xk == other_xk && this_flat_in_array == other_flat_in_array) {\n@@ -4387,0 +4556,1 @@\n+    res_flat_in_array = this_flat_in_array;\n@@ -4420,0 +4590,9 @@\n+  \/\/ Flat in array matrix, yes = y, no = n, maybe = m, top\/empty = T:\n+  \/\/        yes maybe no   -> Super Klass\n+  \/\/   yes   y    y    y\n+  \/\/ maybe   y    m    m\n+  \/\/    no   T    n    n\n+  \/\/    |\n+  \/\/    v\n+  \/\/ Sub Klass\n+\n@@ -4422,0 +4601,1 @@\n+  bool flat_in_array = false;\n@@ -4425,1 +4605,2 @@\n-  } else if (!other_xk && this_type->is_meet_subtype_of(other_type)) {\n+    flat_in_array = below_centerline(ptr) ? (this_flat_in_array && other_flat_in_array) : (this_flat_in_array || other_flat_in_array);\n+  } else if (!other_xk && is_meet_subtype_of(this_type, other_type)) {\n@@ -4428,1 +4609,3 @@\n-  } else if(!this_xk && other_type->is_meet_subtype_of(this_type)) {\n+    bool other_flat_this_maybe_flat = other_flat_in_array && (!this_flat_in_array && !this_not_flat_in_array);\n+    flat_in_array = this_flat_in_array || other_flat_this_maybe_flat;\n+  } else if (!this_xk && is_meet_subtype_of(other_type, this_type)) {\n@@ -4431,0 +4614,2 @@\n+    bool this_flat_other_maybe_flat = this_flat_in_array && (!other_flat_in_array && !other_not_flat_in_array);\n+    flat_in_array = other_flat_in_array || this_flat_other_maybe_flat;\n@@ -4434,1 +4619,2 @@\n-    if (above_centerline(ptr)) { \/\/ both are up?\n+    if (above_centerline(ptr)) {\n+      \/\/ Both types are empty.\n@@ -4438,1 +4624,2 @@\n-      this_type = other_type; \/\/ tinst is down; keep down man\n+      \/\/ this_type is empty while other_type is not. Take other_type.\n+      this_type = other_type;\n@@ -4440,0 +4627,1 @@\n+      flat_in_array = other_flat_in_array;\n@@ -4441,0 +4629,1 @@\n+      \/\/ other_type is empty while this_type is not. Take this_type.\n@@ -4442,1 +4631,1 @@\n-      other_xk = this_xk;\n+      flat_in_array = this_flat_in_array;\n@@ -4444,0 +4633,1 @@\n+      \/\/ this_type and other_type are both non-empty.\n@@ -4455,0 +4645,1 @@\n+    res_flat_in_array = subtype ? flat_in_array : this_flat_in_array;\n@@ -4471,0 +4662,1 @@\n+  res_flat_in_array = this_flat_in_array && other_flat_in_array;\n@@ -4475,0 +4667,4 @@\n+template<class T> bool TypePtr::is_meet_subtype_of(const T* sub_type, const T* super_type) {\n+  return sub_type->is_meet_subtype_of(super_type) && !(super_type->flat_in_array() && sub_type->not_flat_in_array());\n+}\n+\n@@ -4476,1 +4672,1 @@\n-ciType* TypeInstPtr::java_mirror_type() const {\n+ciType* TypeInstPtr::java_mirror_type(bool* is_null_free_array) const {\n@@ -4482,2 +4678,1 @@\n-\n-  return const_oop()->as_instance()->java_mirror_type();\n+  return const_oop()->as_instance()->java_mirror_type(is_null_free_array);\n@@ -4491,1 +4686,1 @@\n-  return new TypeInstPtr(dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n+  return new TypeInstPtr(dual_ptr(), klass(), _interfaces, klass_is_exact(), const_oop(), dual_offset(), flat_in_array(), dual_instance_id(), dual_speculative(), dual_inline_depth());\n@@ -4500,0 +4695,1 @@\n+    flat_in_array() == p->flat_in_array() &&\n@@ -4507,1 +4703,1 @@\n-  return klass()->hash() + TypeOopPtr::hash() + _interfaces->hash();\n+  return klass()->hash() + TypeOopPtr::hash() + _interfaces->hash() + (uint)flat_in_array();\n@@ -4561,5 +4757,1 @@\n-  if( _offset ) {               \/\/ Dump offset, if any\n-    if( _offset == OffsetBot )      st->print(\"+any\");\n-    else if( _offset == OffsetTop ) st->print(\"+unknown\");\n-    else st->print(\"+%d\", _offset);\n-  }\n+  _offset.dump2(st);\n@@ -4568,0 +4760,5 @@\n+\n+  if (flat_in_array() && !klass()->is_inlinetype()) {\n+    st->print(\" (flat in array)\");\n+  }\n+\n@@ -4580,1 +4777,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), xadd_offset(offset),\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), xadd_offset(offset), flat_in_array(),\n@@ -4585,1 +4782,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), offset,\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), Offset(offset), flat_in_array(),\n@@ -4594,1 +4791,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset,\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, flat_in_array(),\n@@ -4599,1 +4796,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, _instance_id, speculative, _inline_depth);\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, flat_in_array(), _instance_id, speculative, _inline_depth);\n@@ -4606,1 +4803,1 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, _instance_id, _speculative, depth);\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, flat_in_array(), _instance_id, _speculative, depth);\n@@ -4611,1 +4808,5 @@\n-  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, flat_in_array(), instance_id, _speculative, _inline_depth);\n+}\n+\n+const TypeInstPtr *TypeInstPtr::cast_to_flat_in_array() const {\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, true, _instance_id, _speculative, _inline_depth);\n@@ -4625,1 +4826,1 @@\n-  return TypeInstKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, klass(), _interfaces, 0);\n+  return TypeInstKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, klass(), _interfaces, Offset(0), flat_in_array());\n@@ -4670,1 +4871,0 @@\n-\n@@ -4693,10 +4893,11 @@\n-const TypeAryPtr* TypeAryPtr::RANGE;\n-const TypeAryPtr* TypeAryPtr::OOPS;\n-const TypeAryPtr* TypeAryPtr::NARROWOOPS;\n-const TypeAryPtr* TypeAryPtr::BYTES;\n-const TypeAryPtr* TypeAryPtr::SHORTS;\n-const TypeAryPtr* TypeAryPtr::CHARS;\n-const TypeAryPtr* TypeAryPtr::INTS;\n-const TypeAryPtr* TypeAryPtr::LONGS;\n-const TypeAryPtr* TypeAryPtr::FLOATS;\n-const TypeAryPtr* TypeAryPtr::DOUBLES;\n+const TypeAryPtr *TypeAryPtr::RANGE;\n+const TypeAryPtr *TypeAryPtr::OOPS;\n+const TypeAryPtr *TypeAryPtr::NARROWOOPS;\n+const TypeAryPtr *TypeAryPtr::BYTES;\n+const TypeAryPtr *TypeAryPtr::SHORTS;\n+const TypeAryPtr *TypeAryPtr::CHARS;\n+const TypeAryPtr *TypeAryPtr::INTS;\n+const TypeAryPtr *TypeAryPtr::LONGS;\n+const TypeAryPtr *TypeAryPtr::FLOATS;\n+const TypeAryPtr *TypeAryPtr::DOUBLES;\n+const TypeAryPtr *TypeAryPtr::INLINES;\n@@ -4705,1 +4906,1 @@\n-const TypeAryPtr *TypeAryPtr::make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+const TypeAryPtr* TypeAryPtr::make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, Offset offset, Offset field_offset,\n@@ -4715,1 +4916,4 @@\n-  return (TypeAryPtr*)(new TypeAryPtr(ptr, nullptr, ary, k, xk, offset, instance_id, false, speculative, inline_depth))->hashcons();\n+  if (k != nullptr && k->is_flat_array_klass() && !ary->_flat) {\n+    k = nullptr;\n+  }\n+  return (TypeAryPtr*)(new TypeAryPtr(ptr, nullptr, ary, k, xk, offset, field_offset, instance_id, false, speculative, inline_depth))->hashcons();\n@@ -4719,1 +4923,1 @@\n-const TypeAryPtr *TypeAryPtr::make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+const TypeAryPtr* TypeAryPtr::make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, Offset offset, Offset field_offset,\n@@ -4731,1 +4935,4 @@\n-  return (TypeAryPtr*)(new TypeAryPtr(ptr, o, ary, k, xk, offset, instance_id, is_autobox_cache, speculative, inline_depth))->hashcons();\n+  if (k != nullptr && k->is_flat_array_klass() && !ary->_flat) {\n+    k = nullptr;\n+  }\n+  return (TypeAryPtr*)(new TypeAryPtr(ptr, o, ary, k, xk, offset, field_offset, instance_id, is_autobox_cache, speculative, inline_depth))->hashcons();\n@@ -4737,1 +4944,1 @@\n-  return make(ptr, ptr == Constant ? const_oop() : nullptr, _ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, ptr == Constant ? const_oop() : nullptr, _ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4745,1 +4952,1 @@\n-  return make(ptr(), const_oop(), _ary, klass(), klass_is_exact, _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), const_oop(), _ary, klass(), klass_is_exact, _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4751,1 +4958,1 @@\n-  return make(_ptr, const_oop(), _ary, klass(), _klass_is_exact, _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, const_oop(), _ary, klass(), _klass_is_exact, _offset, _field_offset, instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4807,2 +5014,62 @@\n-  const TypeAry* new_ary = TypeAry::make(elem(), new_size, is_stable());\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  const TypeAry* new_ary = TypeAry::make(elem(), new_size, is_stable(), is_flat(), is_not_flat(), is_not_null_free());\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+\/\/-------------------------------cast_to_not_flat------------------------------\n+const TypeAryPtr* TypeAryPtr::cast_to_not_flat(bool not_flat) const {\n+  if (not_flat == is_not_flat()) {\n+    return this;\n+  }\n+  assert(!not_flat || !is_flat(), \"inconsistency\");\n+  const TypeAry* new_ary = TypeAry::make(elem(), size(), is_stable(), is_flat(), not_flat, is_not_null_free());\n+  const TypeAryPtr* res = make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+  \/\/ We keep the speculative part if it contains information about flat-\/nullability.\n+  \/\/ Make sure it's removed if it's not better than the non-speculative type anymore.\n+  if (res->speculative() == res->remove_speculative()) {\n+    return res->remove_speculative();\n+  }\n+  return res;\n+}\n+\n+\/\/-------------------------------cast_to_not_null_free-------------------------\n+const TypeAryPtr* TypeAryPtr::cast_to_not_null_free(bool not_null_free) const {\n+  if (not_null_free == is_not_null_free()) {\n+    return this;\n+  }\n+  assert(!not_null_free || !is_flat(), \"inconsistency\");\n+  const TypeAry* new_ary = TypeAry::make(elem(), size(), is_stable(), is_flat(), \/* not_flat= *\/ not_null_free ? true : is_not_flat(), not_null_free);\n+  const TypeAryPtr* res = make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset,\n+                               _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+  \/\/ We keep the speculative part if it contains information about flat-\/nullability.\n+  \/\/ Make sure it's removed if it's not better than the non-speculative type anymore.\n+  if (res->speculative() == res->remove_speculative()) {\n+    return res->remove_speculative();\n+  }\n+  return res;\n+}\n+\n+\/\/---------------------------------update_properties---------------------------\n+const TypeAryPtr* TypeAryPtr::update_properties(const TypeAryPtr* from) const {\n+  if ((from->is_flat()          && is_not_flat()) ||\n+      (from->is_not_flat()      && is_flat()) ||\n+      (from->is_null_free()     && is_not_null_free()) ||\n+      (from->is_not_null_free() && is_null_free())) {\n+    return nullptr; \/\/ Inconsistent properties\n+  } else if (from->is_not_null_free()) {\n+    return cast_to_not_null_free(); \/\/ Implies not flat\n+  } else if (from->is_not_flat()) {\n+    return cast_to_not_flat();\n+  }\n+  return this;\n+}\n+\n+jint TypeAryPtr::flat_layout_helper() const {\n+  return klass()->as_flat_array_klass()->layout_helper();\n+}\n+\n+int TypeAryPtr::flat_elem_size() const {\n+  return klass()->as_flat_array_klass()->element_byte_size();\n+}\n+\n+int TypeAryPtr::flat_log_elem_size() const {\n+  return klass()->as_flat_array_klass()->log2_element_size();\n@@ -4824,1 +5091,1 @@\n-  const TypeAry* new_ary = TypeAry::make(elem, size(), stable);\n+  const TypeAry* new_ary = TypeAry::make(elem, size(), stable, is_flat(), is_not_flat(), is_not_null_free());\n@@ -4826,1 +5093,1 @@\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4846,2 +5113,2 @@\n-  const TypeAry* new_ary = TypeAry::make(etype, size(), is_stable());\n-  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth, \/*is_autobox_cache=*\/true);\n+  const TypeAry* new_ary = TypeAry::make(etype, size(), is_stable(), is_flat(), is_not_flat(), is_not_null_free());\n+  return make(ptr(), const_oop(), new_ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, \/*is_autobox_cache=*\/true);\n@@ -4856,1 +5123,2 @@\n-    TypeOopPtr::eq(p);  \/\/ Check sub-parts\n+    TypeOopPtr::eq(p) &&\/\/ Check sub-parts\n+    _field_offset == p->_field_offset;\n@@ -4862,1 +5130,1 @@\n-  return (uint)(uintptr_t)_ary + TypeOopPtr::hash();\n+  return (uint)(uintptr_t)_ary + TypeOopPtr::hash() + _field_offset.get();\n@@ -4906,1 +5174,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4915,1 +5183,1 @@\n-                  _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                  _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4929,1 +5197,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -4945,1 +5213,1 @@\n-                  _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                  _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -4959,1 +5227,2 @@\n-    int off = meet_offset(tap->offset());\n+    Offset off = meet_offset(tap->offset());\n+    Offset field_off = meet_field_offset(tap->field_offset());\n@@ -4968,0 +5237,3 @@\n+    bool res_flat = false;\n+    bool res_not_flat = false;\n+    bool res_not_null_free = false;\n@@ -4969,1 +5241,1 @@\n-    if (meet_aryptr(ptr, elem, this, tap, res_klass, res_xk) == NOT_SUBTYPE) {\n+    if (meet_aryptr(ptr, elem, this, tap, res_klass, res_xk, res_flat, res_not_flat, res_not_null_free) == NOT_SUBTYPE) {\n@@ -4971,0 +5243,14 @@\n+    } else if (this->is_flat() != tap->is_flat()) {\n+      \/\/ Meeting flat inline type array with non-flat array. Adjust (field) offset accordingly.\n+      if (tary->_flat) {\n+        \/\/ Result is in a flat representation\n+        off = Offset(is_flat() ? offset() : tap->offset());\n+        field_off = is_flat() ? field_offset() : tap->field_offset();\n+      } else if (below_centerline(ptr)) {\n+        \/\/ Result is in a non-flat representation\n+        off = Offset(flat_offset()).meet(Offset(tap->flat_offset()));\n+        field_off = (field_off == Offset::top) ? Offset::top : Offset::bottom;\n+      } else if (flat_offset() == tap->flat_offset()) {\n+        off = Offset(!is_flat() ? offset() : tap->offset());\n+        field_off = !is_flat() ? field_offset() : tap->field_offset();\n+      }\n@@ -4988,1 +5274,1 @@\n-    return make(ptr, o, TypeAry::make(elem, tary->_size, tary->_stable), res_klass, res_xk, off, instance_id, speculative, depth);\n+    return make(ptr, o, TypeAry::make(elem, tary->_size, tary->_stable, res_flat, res_not_flat, res_not_null_free), res_klass, res_xk, off, field_off, instance_id, speculative, depth);\n@@ -4994,1 +5280,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5009,2 +5295,2 @@\n-      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact()) {\n-        return TypeAryPtr::make(ptr, _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact() && !tp->flat_in_array()) {\n+        return TypeAryPtr::make(ptr, _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -5016,1 +5302,1 @@\n-        return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr,offset, instance_id, speculative, depth);\n+        return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr, offset, false, instance_id, speculative, depth);\n@@ -5028,1 +5314,1 @@\n-        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact()) {\n+        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact() && !tp->flat_in_array()) {\n@@ -5031,1 +5317,1 @@\n-                      _ary, _klass, _klass_is_exact, offset, instance_id, speculative, depth);\n+                      _ary, _klass, _klass_is_exact, offset, _field_offset, instance_id, speculative, depth);\n@@ -5043,1 +5329,1 @@\n-      return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr, offset, instance_id, speculative, depth);\n+      return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr, offset, false, instance_id, speculative, depth);\n@@ -5052,2 +5338,2 @@\n-template<class T> TypePtr::MeetResult TypePtr::meet_aryptr(PTR& ptr, const Type*& elem, const T* this_ary,\n-                                                           const T* other_ary, ciKlass*& res_klass, bool& res_xk) {\n+template<class T> TypePtr::MeetResult TypePtr::meet_aryptr(PTR& ptr, const Type*& elem, const T* this_ary, const T* other_ary,\n+                                                           ciKlass*& res_klass, bool& res_xk, bool &res_flat, bool& res_not_flat, bool& res_not_null_free) {\n@@ -5063,0 +5349,6 @@\n+  bool this_flat = this_ary->is_flat();\n+  bool this_not_flat = this_ary->is_not_flat();\n+  bool other_flat = other_ary->is_flat();\n+  bool other_not_flat = other_ary->is_not_flat();\n+  bool this_not_null_free = this_ary->is_not_null_free();\n+  bool other_not_null_free = other_ary->is_not_null_free();\n@@ -5065,0 +5357,4 @@\n+  res_flat = this_flat && other_flat;\n+  res_not_flat = this_not_flat && other_not_flat;\n+  res_not_null_free = this_not_null_free && other_not_null_free;\n+\n@@ -5068,3 +5364,3 @@\n-    if (this_top_or_bottom)\n-      res_klass = other_klass;\n-    else if (other_top_or_bottom || other_klass == this_klass) {\n+      if (this_top_or_bottom) {\n+        res_klass = other_klass;\n+      } else if (other_top_or_bottom || other_klass == this_klass) {\n@@ -5112,0 +5408,3 @@\n+        if (this_ary->is_flat()) {\n+          elem = this_ary->elem();\n+        }\n@@ -5115,1 +5414,1 @@\n-      return result;\n+      break;\n@@ -5119,1 +5418,1 @@\n-      } else if(above_centerline(this_ptr)) {\n+      } else if (above_centerline(this_ptr)) {\n@@ -5124,0 +5423,5 @@\n+        \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+        \/\/ is null-free due to null-free [LMyValue <: null-able [LMyValue.\n+        if (res_xk && !res_not_null_free) {\n+          res_xk = false;\n+        }\n@@ -5125,1 +5429,1 @@\n-      return result;\n+      break;\n@@ -5132,0 +5436,3 @@\n+        if (other_ary->is_flat()) {\n+          elem = other_ary->elem();\n+        }\n@@ -5135,0 +5442,5 @@\n+        \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+        \/\/ is null-free due to null-free [LMyValue <: null-able [LMyValue.\n+        if (res_xk && !res_not_null_free) {\n+          res_xk = false;\n+        }\n@@ -5136,1 +5448,1 @@\n-      return result;\n+      break;\n@@ -5149,1 +5461,10 @@\n-  return new TypeAryPtr(dual_ptr(), _const_oop, _ary->dual()->is_ary(),_klass, _klass_is_exact, dual_offset(), dual_instance_id(), is_autobox_cache(), dual_speculative(), dual_inline_depth());\n+  return new TypeAryPtr(dual_ptr(), _const_oop, _ary->dual()->is_ary(), _klass, _klass_is_exact, dual_offset(), dual_field_offset(), dual_instance_id(), is_autobox_cache(), dual_speculative(), dual_inline_depth());\n+}\n+\n+Type::Offset TypeAryPtr::meet_field_offset(const Type::Offset offset) const {\n+  return _field_offset.meet(offset);\n+}\n+\n+\/\/------------------------------dual_offset------------------------------------\n+Type::Offset TypeAryPtr::dual_field_offset() const {\n+  return _field_offset.dual();\n@@ -5177,1 +5498,10 @@\n-  if( _offset != 0 ) {\n+  if (is_flat()) {\n+    st->print(\":flat\");\n+    st->print(\"(\");\n+    _field_offset.dump2(st);\n+    st->print(\")\");\n+  }\n+  if (is_null_free()) {\n+    st->print(\":null_free\");\n+  }\n+  if (offset() != 0) {\n@@ -5180,3 +5510,3 @@\n-    if( _offset == OffsetTop )       st->print(\"+undefined\");\n-    else if( _offset == OffsetBot )  st->print(\"+any\");\n-    else if( _offset < header_size ) st->print(\"+%d\", _offset);\n+    if( _offset == Offset::top )       st->print(\"+undefined\");\n+    else if( _offset == Offset::bottom )  st->print(\"+any\");\n+    else if( offset() < header_size ) st->print(\"+%d\", offset());\n@@ -5188,1 +5518,1 @@\n-        st->print(\"[%d]\", (_offset - header_size)\/elem_size);\n+        st->print(\"[%d]\", (offset() - header_size)\/elem_size);\n@@ -5205,0 +5535,4 @@\n+  \/\/ FIXME: Does this belong here? Or in the meet code itself?\n+  if (is_flat() && is_not_flat()) {\n+    return true;\n+  }\n@@ -5210,1 +5544,1 @@\n-  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, xadd_offset(offset), _instance_id, add_offset_speculative(offset), _inline_depth);\n+  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, xadd_offset(offset), _field_offset, _instance_id, add_offset_speculative(offset), _inline_depth, _is_autobox_cache);\n@@ -5214,1 +5548,1 @@\n-  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, offset, _instance_id, with_offset_speculative(offset), _inline_depth);\n+  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, Offset(offset), _field_offset, _instance_id, with_offset_speculative(offset), _inline_depth, _is_autobox_cache);\n@@ -5218,1 +5552,1 @@\n-  return make(_ptr, _const_oop, ary, _klass, _klass_is_exact, _offset, _instance_id, _speculative, _inline_depth);\n+  return make(_ptr, _const_oop, ary, _klass, _klass_is_exact, _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -5226,1 +5560,14 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _instance_id, nullptr, _inline_depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, _instance_id, nullptr, _inline_depth, _is_autobox_cache);\n+}\n+\n+const Type* TypeAryPtr::cleanup_speculative() const {\n+  if (speculative() == nullptr) {\n+    return this;\n+  }\n+  \/\/ Keep speculative part if it contains information about flat-\/nullability\n+  const TypeAryPtr* spec_aryptr = speculative()->isa_aryptr();\n+  if (spec_aryptr != nullptr && !above_centerline(spec_aryptr->ptr()) &&\n+      (spec_aryptr->is_not_flat() || spec_aryptr->is_not_null_free())) {\n+    return this;\n+  }\n+  return TypeOopPtr::cleanup_speculative();\n@@ -5233,1 +5580,44 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _instance_id, _speculative, depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, _instance_id, _speculative, depth, _is_autobox_cache);\n+}\n+\n+const TypeAryPtr* TypeAryPtr::with_field_offset(int offset) const {\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, Offset(offset), _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+const TypePtr* TypeAryPtr::add_field_offset_and_offset(intptr_t offset) const {\n+  int adj = 0;\n+  if (is_flat() && offset != Type::OffsetBot && offset != Type::OffsetTop) {\n+    if (_offset.get() != OffsetBot && _offset.get() != OffsetTop) {\n+      adj = _offset.get();\n+      offset += _offset.get();\n+    }\n+    uint header = arrayOopDesc::base_offset_in_bytes(T_OBJECT);\n+    if (_field_offset.get() != OffsetBot && _field_offset.get() != OffsetTop) {\n+      offset += _field_offset.get();\n+      if (_offset.get() == OffsetBot || _offset.get() == OffsetTop) {\n+        offset += header;\n+      }\n+    }\n+    if (elem()->make_oopptr()->is_inlinetypeptr() && (offset >= (intptr_t)header || offset < 0)) {\n+      \/\/ Try to get the field of the inline type array element we are pointing to\n+      ciInlineKlass* vk = elem()->inline_klass();\n+      int shift = flat_log_elem_size();\n+      int mask = (1 << shift) - 1;\n+      intptr_t field_offset = ((offset - header) & mask);\n+      ciField* field = vk->get_field_by_offset(field_offset + vk->payload_offset(), false);\n+      if (field != nullptr) {\n+        return with_field_offset(field_offset)->add_offset(offset - field_offset - adj);\n+      }\n+    }\n+  }\n+  return add_offset(offset - adj);\n+}\n+\n+\/\/ Return offset incremented by field_offset for flat inline type arrays\n+int TypeAryPtr::flat_offset() const {\n+  int offset = _offset.get();\n+  if (offset != Type::OffsetBot && offset != Type::OffsetTop &&\n+      _field_offset != Offset::bottom && _field_offset != Offset::top) {\n+    offset += _field_offset.get();\n+  }\n+  return offset;\n@@ -5238,1 +5628,1 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, instance_id, _speculative, _inline_depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _field_offset, instance_id, _speculative, _inline_depth);\n@@ -5243,0 +5633,1 @@\n+\n@@ -5333,1 +5724,0 @@\n-\n@@ -5417,1 +5807,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -5437,1 +5827,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert(offset() >= 0, \"\");\n@@ -5439,1 +5829,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -5490,1 +5880,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5518,1 +5908,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5551,1 +5941,1 @@\n-  switch( _offset ) {\n+  switch (offset()) {\n@@ -5555,1 +5945,1 @@\n-  default:        st->print(\"+%d\",_offset); break;\n+  default:        st->print(\"+%d\",offset()); break;\n@@ -5565,1 +5955,1 @@\n-TypeMetadataPtr::TypeMetadataPtr(PTR ptr, ciMetadata* metadata, int offset):\n+TypeMetadataPtr::TypeMetadataPtr(PTR ptr, ciMetadata* metadata, Offset offset):\n@@ -5570,1 +5960,1 @@\n-  return make(Constant, m, 0);\n+  return make(Constant, m, Offset(0));\n@@ -5573,1 +5963,1 @@\n-  return make(Constant, m, 0);\n+  return make(Constant, m, Offset(0));\n@@ -5578,1 +5968,1 @@\n-const TypeMetadataPtr *TypeMetadataPtr::make(PTR ptr, ciMetadata* m, int offset) {\n+const TypeMetadataPtr* TypeMetadataPtr::make(PTR ptr, ciMetadata* m, Offset offset) {\n@@ -5589,1 +5979,4 @@\n-    if (elem->is_klassptr()->klass_is_exact()) {\n+    if (elem->is_klassptr()->klass_is_exact() &&\n+        \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+        \/\/ is null-free due to null-free [LMyValue <: null-able [LMyValue.\n+        (is_null_free() || !_ary->_elem->make_oopptr()->is_inlinetypeptr())) {\n@@ -5593,1 +5986,1 @@\n-  return TypeAryKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, elem, klass(), 0);\n+  return TypeAryKlassPtr::make(xk ? TypePtr::Constant : TypePtr::NotNull, elem, klass(), Offset(0), is_not_flat(), is_not_null_free(), is_null_free());\n@@ -5596,1 +5989,1 @@\n-const TypeKlassPtr* TypeKlassPtr::make(ciKlass *klass, InterfaceHandling interface_handling) {\n+const TypeKlassPtr* TypeKlassPtr::make(ciKlass* klass, InterfaceHandling interface_handling) {\n@@ -5603,1 +5996,1 @@\n-const TypeKlassPtr* TypeKlassPtr::make(PTR ptr, ciKlass* klass, int offset, InterfaceHandling interface_handling) {\n+const TypeKlassPtr* TypeKlassPtr::make(PTR ptr, ciKlass* klass, Offset offset, InterfaceHandling interface_handling) {\n@@ -5611,3 +6004,1 @@\n-\n-\/\/------------------------------TypeKlassPtr-----------------------------------\n-TypeKlassPtr::TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, int offset)\n+TypeKlassPtr::TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, Offset offset)\n@@ -5616,1 +6007,1 @@\n-         klass->is_type_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"no interface here\");\n+         klass->is_type_array_klass() || klass->is_flat_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"no interface here\");\n@@ -5655,1 +6046,1 @@\n-  return (_offset == 0) && !below_centerline(_ptr);\n+  return (offset() == 0) && !below_centerline(_ptr);\n@@ -5687,1 +6078,1 @@\n-  assert( _offset >= 0, \"\" );\n+  assert( offset() >= 0, \"\" );\n@@ -5689,1 +6080,1 @@\n-  if (_offset != 0) {\n+  if (offset() != 0) {\n@@ -5733,5 +6124,2 @@\n-\n-  if (_offset) {               \/\/ Dump offset, if any\n-    if (_offset == OffsetBot)      { st->print(\"+any\"); }\n-    else if (_offset == OffsetTop) { st->print(\"+unknown\"); }\n-    else                            { st->print(\"+%d\", _offset); }\n+  if (Verbose) {\n+    if (isa_instklassptr() && is_instklassptr()->flat_in_array()) st->print(\":flat in array\");\n@@ -5739,1 +6127,1 @@\n-\n+  _offset.dump2(st);\n@@ -5755,0 +6143,1 @@\n+    flat_in_array() == p->flat_in_array() &&\n@@ -5759,1 +6148,1 @@\n-  return klass()->hash() + TypeKlassPtr::hash();\n+  return klass()->hash() + TypeKlassPtr::hash() + (uint)flat_in_array();\n@@ -5762,1 +6151,3 @@\n-const TypeInstKlassPtr *TypeInstKlassPtr::make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, int offset) {\n+const TypeInstKlassPtr *TypeInstKlassPtr::make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, Offset offset, bool flat_in_array) {\n+  flat_in_array = flat_in_array || k->flat_in_array();\n+\n@@ -5764,1 +6155,1 @@\n-    (TypeInstKlassPtr*)(new TypeInstKlassPtr(ptr, k, interfaces, offset))->hashcons();\n+    (TypeInstKlassPtr*)(new TypeInstKlassPtr(ptr, k, interfaces, offset, flat_in_array))->hashcons();\n@@ -5771,2 +6162,2 @@\n-const TypePtr* TypeInstKlassPtr::add_offset( intptr_t offset ) const {\n-  return make( _ptr, klass(), _interfaces, xadd_offset(offset) );\n+const TypePtr *TypeInstKlassPtr::add_offset( intptr_t offset ) const {\n+  return make(_ptr, klass(), _interfaces, xadd_offset(offset), flat_in_array());\n@@ -5776,1 +6167,1 @@\n-  return make(_ptr, klass(), _interfaces, offset);\n+  return make(_ptr, klass(), _interfaces, Offset(offset), flat_in_array());\n@@ -5783,1 +6174,1 @@\n-  return make(ptr, _klass, _interfaces, _offset);\n+  return make(ptr, _klass, _interfaces, _offset, flat_in_array());\n@@ -5799,1 +6190,1 @@\n-  return make(klass_is_exact ? Constant : NotNull, k, _interfaces, _offset);\n+  return make(klass_is_exact ? Constant : NotNull, k, _interfaces, _offset, flat_in_array());\n@@ -5831,1 +6222,1 @@\n-  return TypeInstPtr::make(TypePtr::BotPTR, k, interfaces, xk, nullptr, 0);\n+  return TypeInstPtr::make(TypePtr::BotPTR, k, interfaces, xk, nullptr, Offset(0), flat_in_array() && !klass()->is_inlinetype());\n@@ -5864,1 +6255,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5872,1 +6263,1 @@\n-      return make( ptr, klass(), _interfaces, offset );\n+      return make(ptr, klass(), _interfaces, offset, flat_in_array());\n@@ -5885,1 +6276,1 @@\n-    return TypePtr::BOTTOM;\n+      return TypePtr::BOTTOM;\n@@ -5905,1 +6296,1 @@\n-    int  off     = meet_offset(tkls->offset());\n+    Offset  off     = meet_offset(tkls->offset());\n@@ -5911,1 +6302,2 @@\n-    switch(meet_instptr(ptr, interfaces, this, tkls, res_klass, res_xk)) {\n+    bool res_flat_in_array = false;\n+    switch(meet_instptr(ptr, interfaces, this, tkls, res_klass, res_xk, res_flat_in_array)) {\n@@ -5919,1 +6311,1 @@\n-        const Type* res = make(ptr, res_klass, interfaces, off);\n+        const Type* res = make(ptr, res_klass, interfaces, off, res_flat_in_array);\n@@ -5928,1 +6320,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -5941,1 +6333,1 @@\n-        return TypeAryKlassPtr::make(ptr, tp->elem(), tp->klass(), offset);\n+        return TypeAryKlassPtr::make(ptr, tp->elem(), tp->klass(), offset, tp->is_not_flat(), tp->is_not_null_free(), tp->is_null_free());\n@@ -5946,1 +6338,1 @@\n-        return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+        return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, false);\n@@ -5960,2 +6352,1 @@\n-          return TypeAryKlassPtr::make(ptr,\n-                                       tp->elem(), tp->klass(), offset);\n+          return TypeAryKlassPtr::make(ptr, tp->elem(), tp->klass(), offset, tp->is_not_flat(), tp->is_not_null_free(), tp->is_null_free());\n@@ -5969,1 +6360,1 @@\n-      return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+      return make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, false);\n@@ -5981,1 +6372,1 @@\n-  return new TypeInstKlassPtr(dual_ptr(), klass(), _interfaces, dual_offset());\n+  return new TypeInstKlassPtr(dual_ptr(), klass(), _interfaces, dual_offset(), flat_in_array());\n@@ -6082,0 +6473,7 @@\n+bool TypeInstKlassPtr::can_be_inline_array() const {\n+  return _klass->equals(ciEnv::current()->Object_klass()) && TypeAryKlassPtr::_array_interfaces->contains(_interfaces);\n+}\n+\n+bool TypeAryKlassPtr::can_be_inline_array() const {\n+  return _elem->isa_instklassptr() && _elem->is_instklassptr()->_klass->can_be_inline_klass();\n+}\n@@ -6083,2 +6481,2 @@\n-const TypeAryKlassPtr *TypeAryKlassPtr::make(PTR ptr, const Type* elem, ciKlass* k, int offset) {\n-  return (TypeAryKlassPtr*)(new TypeAryKlassPtr(ptr, elem, k, offset))->hashcons();\n+bool TypeInstPtr::can_be_inline_array() const {\n+  return _klass->equals(ciEnv::current()->Object_klass()) && TypeAryPtr::_array_interfaces->contains(_interfaces);\n@@ -6087,1 +6485,9 @@\n-const TypeAryKlassPtr *TypeAryKlassPtr::make(PTR ptr, ciKlass* k, int offset, InterfaceHandling interface_handling) {\n+bool TypeAryPtr::can_be_inline_array() const {\n+  return elem()->make_ptr() && elem()->make_ptr()->isa_instptr() && elem()->make_ptr()->is_instptr()->_klass->can_be_inline_klass();\n+}\n+\n+const TypeAryKlassPtr *TypeAryKlassPtr::make(PTR ptr, const Type* elem, ciKlass* k, Offset offset, bool not_flat, bool not_null_free, bool null_free) {\n+  return (TypeAryKlassPtr*)(new TypeAryKlassPtr(ptr, elem, k, offset, not_flat, not_null_free, null_free))->hashcons();\n+}\n+\n+const TypeAryKlassPtr* TypeAryKlassPtr::make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling, bool not_flat, bool not_null_free, bool null_free) {\n@@ -6091,2 +6497,2 @@\n-    const TypeKlassPtr *etype = TypeKlassPtr::make(eklass, interface_handling)->cast_to_exactness(false);\n-    return TypeAryKlassPtr::make(ptr, etype, nullptr, offset);\n+    const TypeKlassPtr* etype = TypeKlassPtr::make(eklass, interface_handling)->cast_to_exactness(false);\n+    return TypeAryKlassPtr::make(ptr, etype, nullptr, offset, not_flat, not_null_free, null_free);\n@@ -6096,1 +6502,5 @@\n-    return TypeAryKlassPtr::make(ptr, etype, k, offset);\n+    return TypeAryKlassPtr::make(ptr, etype, k, offset, not_flat, not_null_free, null_free);\n+  } else if (k->is_flat_array_klass()) {\n+    ciKlass* eklass = k->as_flat_array_klass()->element_klass();\n+    const TypeKlassPtr* etype = TypeKlassPtr::make(eklass);\n+    return TypeAryKlassPtr::make(ptr, etype, k, offset, not_flat, not_null_free, null_free);\n@@ -6103,0 +6513,11 @@\n+const TypeAryKlassPtr* TypeAryKlassPtr::make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling) {\n+  bool null_free = k->as_array_klass()->is_elem_null_free();\n+  bool not_null_free = (ptr == Constant) ? !null_free : !k->is_flat_array_klass() && (k->is_type_array_klass() || !k->as_array_klass()->element_klass()->can_be_inline_klass(false));\n+\n+  bool not_flat = !UseArrayFlattening || not_null_free || (k->as_array_klass()->element_klass() != nullptr &&\n+                                                     k->as_array_klass()->element_klass()->is_inlinetype() &&\n+                                                     !k->as_array_klass()->element_klass()->flat_in_array());\n+\n+  return TypeAryKlassPtr::make(ptr, k, offset, interface_handling, not_flat, not_null_free, null_free);\n+}\n+\n@@ -6104,1 +6525,1 @@\n-  return TypeAryKlassPtr::make(Constant, klass, 0, interface_handling);\n+  return TypeAryKlassPtr::make(Constant, klass, Offset(0), interface_handling);\n@@ -6113,0 +6534,3 @@\n+    _not_flat == p->_not_flat &&\n+    _not_null_free == p->_not_null_free &&\n+    _null_free == p->_null_free &&\n@@ -6119,1 +6543,2 @@\n-  return (uint)(uintptr_t)_elem + TypeKlassPtr::hash();\n+  return (uint)(uintptr_t)_elem + TypeKlassPtr::hash() + (uint)(_not_flat ? 43 : 0) +\n+      (uint)(_not_null_free ? 44 : 0) + (uint)(_null_free ? 45 : 0);\n@@ -6135,2 +6560,7 @@\n-  if ((tinst = el->isa_instptr()) != nullptr) {\n-    \/\/ Leave k_ary at null.\n+  if (is_flat() && el->is_inlinetypeptr()) {\n+    \/\/ Klass is required by TypeAryPtr::flat_layout_helper() and others\n+    if (el->inline_klass() != nullptr) {\n+      k_ary = ciArrayKlass::make(el->inline_klass(), \/* null_free *\/ true);\n+    }\n+  } else if ((tinst = el->isa_instptr()) != nullptr) {\n+    \/\/ Leave k_ary at nullptr.\n@@ -6138,1 +6568,1 @@\n-    \/\/ Leave k_ary at null.\n+    \/\/ Leave k_ary at nullptr.\n@@ -6186,1 +6616,1 @@\n-    k = ciObjArrayKlass::make(k);\n+    k = ciArrayKlass::make(k, is_null_free());\n@@ -6206,1 +6636,1 @@\n-  return make(_ptr, elem(), klass(), xadd_offset(offset));\n+  return make(_ptr, elem(), klass(), xadd_offset(offset), is_not_flat(), is_not_null_free(), _null_free);\n@@ -6210,1 +6640,1 @@\n-  return make(_ptr, elem(), klass(), offset);\n+  return make(_ptr, elem(), klass(), Offset(offset), is_not_flat(), is_not_null_free(), _null_free);\n@@ -6217,1 +6647,1 @@\n-  return make(ptr, elem(), _klass, _offset);\n+  return make(ptr, elem(), _klass, _offset, is_not_flat(), is_not_null_free(), _null_free);\n@@ -6225,0 +6655,5 @@\n+  \/\/ Even though MyValue is final, [LMyValue is only exact if the array\n+  \/\/ is null-free due to null-free [LMyValue <: null-able [LMyValue.\n+  if (tk->isa_instklassptr() && tk->klass()->is_inlinetype() && !is_null_free()) {\n+    return false;\n+  }\n@@ -6231,1 +6666,4 @@\n-  if (must_be_exact()) return this;  \/\/ cannot clear xk\n+  if (must_be_exact() && !klass_is_exact) return this;  \/\/ cannot clear xk\n+  if (klass_is_exact == this->klass_is_exact()) {\n+    return this;\n+  }\n@@ -6237,1 +6675,18 @@\n-  return make(klass_is_exact ? Constant : NotNull, elem, k, _offset);\n+  bool not_flat = is_not_flat();\n+  bool not_null_free = is_not_null_free();\n+  if (_elem->isa_klassptr()) {\n+    if (klass_is_exact || _elem->isa_aryklassptr()) {\n+      assert((!is_null_free() && !is_flat()) ||\n+             _elem->is_klassptr()->klass()->is_abstract() || _elem->is_klassptr()->klass()->is_java_lang_Object(),\n+             \"null-free (or flat) concrete inline type arrays should always be exact\");\n+      \/\/ An array can't be null-free (or flat) if the klass is exact\n+      not_null_free = true;\n+      not_flat = true;\n+    } else {\n+      \/\/ Klass is not exact (anymore), re-compute null-free\/flat properties\n+      const TypeOopPtr* exact_etype = TypeOopPtr::make_from_klass_unique(_elem->is_instklassptr()->instance_klass());\n+      not_null_free = !exact_etype->can_be_inline_type();\n+      not_flat = !UseArrayFlattening || not_null_free || (exact_etype->is_inlinetypeptr() && !exact_etype->inline_klass()->flat_in_array());\n+    }\n+  }\n+  return make(klass_is_exact ? Constant : NotNull, elem, k, _offset, not_flat, not_null_free, _null_free);\n@@ -6240,0 +6695,3 @@\n+const TypeAryKlassPtr* TypeAryKlassPtr::cast_to_null_free() const {\n+  return make(_ptr, elem(), klass(), _offset, is_not_flat(), false, true);\n+}\n@@ -6254,1 +6712,5 @@\n-  return TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(el, TypeInt::POS), k, xk, 0);\n+  bool null_free = _null_free;\n+  if (null_free && el->isa_ptr()) {\n+    el = el->is_ptr()->join_speculative(TypePtr::NOTNULL);\n+  }\n+  return TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(el, TypeInt::POS, false, is_flat(), is_not_flat(), is_not_null_free()), k, xk, Offset(0));\n@@ -6288,1 +6750,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -6296,1 +6758,1 @@\n-      return make( ptr, _elem, klass(), offset );\n+      return make(ptr, _elem, klass(), offset, is_not_flat(), is_not_null_free(), is_null_free());\n@@ -6329,1 +6791,1 @@\n-    int off = meet_offset(tap->offset());\n+    Offset off = meet_offset(tap->offset());\n@@ -6331,1 +6793,0 @@\n-\n@@ -6335,1 +6796,5 @@\n-    meet_aryptr(ptr, elem, this, tap, res_klass, res_xk);\n+    bool res_flat = false;\n+    bool res_not_flat = false;\n+    bool res_not_null_free = false;\n+    MeetResult res = meet_aryptr(ptr, elem, this, tap,\n+                                 res_klass, res_xk, res_flat, res_not_flat, res_not_null_free);\n@@ -6337,1 +6802,13 @@\n-    return make(ptr, elem, res_klass, off);\n+    bool null_free = meet_null_free(tap->_null_free);\n+    if (res == NOT_SUBTYPE) {\n+      null_free = false;\n+    } else if (res == SUBTYPE) {\n+      if (above_centerline(tap->ptr()) && !above_centerline(this->ptr())) {\n+        null_free = _null_free;\n+      } else if (above_centerline(this->ptr()) && !above_centerline(tap->ptr())) {\n+        null_free = tap->_null_free;\n+      } else if (above_centerline(this->ptr()) && above_centerline(tap->ptr())) {\n+        null_free = _null_free || tap->_null_free;\n+      }\n+    }\n+    return make(ptr, elem, res_klass, off, res_not_flat, res_not_null_free, null_free);\n@@ -6341,1 +6818,1 @@\n-    int offset = meet_offset(tp->offset());\n+    Offset offset = meet_offset(tp->offset());\n@@ -6355,1 +6832,1 @@\n-        return TypeAryKlassPtr::make(ptr, _elem, _klass, offset);\n+        return TypeAryKlassPtr::make(ptr, _elem, _klass, offset, is_not_flat(), is_not_null_free(), is_null_free());\n@@ -6360,1 +6837,1 @@\n-        return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+        return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, false);\n@@ -6375,1 +6852,1 @@\n-          return make(ptr, _elem, _klass, offset);\n+          return make(ptr, _elem, _klass, offset, is_not_flat(), is_not_null_free(), is_null_free());\n@@ -6383,1 +6860,1 @@\n-      return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset);\n+      return TypeInstKlassPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, offset, false);\n@@ -6421,0 +6898,3 @@\n+    if (other->is_null_free() && !this_one->is_null_free()) {\n+      return false; \/\/ A nullable array can't be a subtype of a null-free array\n+    }\n@@ -6513,1 +6993,1 @@\n-  return new TypeAryKlassPtr(dual_ptr(), elem()->dual(), klass(), dual_offset());\n+  return new TypeAryKlassPtr(dual_ptr(), elem()->dual(), klass(), dual_offset(), !is_not_flat(), !is_not_null_free(), dual_null_free());\n@@ -6523,1 +7003,1 @@\n-    k = ciObjArrayKlass::make(k);\n+    k = ciArrayKlass::make(k, _null_free);\n@@ -6570,5 +7050,5 @@\n-\n-  if( _offset ) {               \/\/ Dump offset, if any\n-    if( _offset == OffsetBot )      { st->print(\"+any\"); }\n-    else if( _offset == OffsetTop ) { st->print(\"+unknown\"); }\n-    else                            { st->print(\"+%d\", _offset); }\n+  if (is_flat()) st->print(\":flat\");\n+  if (_null_free) st->print(\":null free\");\n+  if (Verbose) {\n+    if (_not_flat) st->print(\":not flat\");\n+    if (_not_null_free) st->print(\":not null free\");\n@@ -6577,0 +7057,2 @@\n+  _offset.dump2(st);\n+\n@@ -6595,2 +7077,14 @@\n-const TypeFunc *TypeFunc::make( const TypeTuple *domain, const TypeTuple *range ) {\n-  return (TypeFunc*)(new TypeFunc(domain,range))->hashcons();\n+const TypeFunc *TypeFunc::make(const TypeTuple *domain_sig, const TypeTuple* domain_cc,\n+                               const TypeTuple *range_sig, const TypeTuple *range_cc) {\n+  return (TypeFunc*)(new TypeFunc(domain_sig, domain_cc, range_sig, range_cc))->hashcons();\n+}\n+\n+const TypeFunc *TypeFunc::make(const TypeTuple *domain, const TypeTuple *range) {\n+  return make(domain, domain, range, range);\n+}\n+\n+\/\/------------------------------osr_domain-----------------------------\n+const TypeTuple* osr_domain() {\n+  const Type **fields = TypeTuple::fields(2);\n+  fields[TypeFunc::Parms+0] = TypeRawPtr::BOTTOM;  \/\/ address of osr buffer\n+  return TypeTuple::make(TypeFunc::Parms+1, fields);\n@@ -6600,1 +7094,1 @@\n-const TypeFunc *TypeFunc::make(ciMethod* method) {\n+const TypeFunc* TypeFunc::make(ciMethod* method, bool is_osr_compilation) {\n@@ -6602,7 +7096,24 @@\n-  const TypeFunc* tf = C->last_tf(method); \/\/ check cache\n-  if (tf != nullptr)  return tf;  \/\/ The hit rate here is almost 50%.\n-  const TypeTuple *domain;\n-  if (method->is_static()) {\n-    domain = TypeTuple::make_domain(nullptr, method->signature(), ignore_interfaces);\n-  } else {\n-    domain = TypeTuple::make_domain(method->holder(), method->signature(), ignore_interfaces);\n+  const TypeFunc* tf = nullptr;\n+  if (!is_osr_compilation) {\n+    tf = C->last_tf(method); \/\/ check cache\n+    if (tf != nullptr)  return tf;  \/\/ The hit rate here is almost 50%.\n+  }\n+  \/\/ Inline types are not passed\/returned by reference, instead each field of\n+  \/\/ the inline type is passed\/returned as an argument. We maintain two views of\n+  \/\/ the argument\/return list here: one based on the signature (with an inline\n+  \/\/ type argument\/return as a single slot), one based on the actual calling\n+  \/\/ convention (with an inline type argument\/return as a list of its fields).\n+  bool has_scalar_args = method->has_scalarized_args() && !is_osr_compilation;\n+  \/\/ Fall back to the non-scalarized calling convention when compiling a call via a mismatching method\n+  if (method != C->method() && method->get_Method()->mismatch()) {\n+    has_scalar_args = false;\n+  }\n+  const TypeTuple* domain_sig = is_osr_compilation ? osr_domain() : TypeTuple::make_domain(method, ignore_interfaces, false);\n+  const TypeTuple* domain_cc = has_scalar_args ? TypeTuple::make_domain(method, ignore_interfaces, true) : domain_sig;\n+  ciSignature* sig = method->signature();\n+  bool has_scalar_ret = !method->is_native() && sig->return_type()->is_inlinetype() && sig->return_type()->as_inline_klass()->can_be_returned_as_fields();\n+  const TypeTuple* range_sig = TypeTuple::make_range(sig, ignore_interfaces, false);\n+  const TypeTuple* range_cc = has_scalar_ret ? TypeTuple::make_range(sig, ignore_interfaces, true) : range_sig;\n+  tf = TypeFunc::make(domain_sig, domain_cc, range_sig, range_cc);\n+  if (!is_osr_compilation) {\n+    C->set_last_tf(method, tf);  \/\/ fill cache\n@@ -6610,3 +7121,0 @@\n-  const TypeTuple *range  = TypeTuple::make_range(method->signature(), ignore_interfaces);\n-  tf = TypeFunc::make(domain, range);\n-  C->set_last_tf(method, tf);  \/\/ fill cache\n@@ -6647,2 +7155,4 @@\n-  return _domain == a->_domain &&\n-    _range == a->_range;\n+  return _domain_sig == a->_domain_sig &&\n+    _domain_cc == a->_domain_cc &&\n+    _range_sig == a->_range_sig &&\n+    _range_cc == a->_range_cc;\n@@ -6654,1 +7164,1 @@\n-  return (uint)(uintptr_t)_domain + (uint)(uintptr_t)_range;\n+  return (uint)(intptr_t)_domain_sig + (uint)(intptr_t)_domain_cc + (uint)(intptr_t)_range_sig + (uint)(intptr_t)_range_cc;\n@@ -6661,1 +7171,1 @@\n-  if( _range->cnt() <= Parms )\n+  if( _range_sig->cnt() <= Parms )\n@@ -6665,2 +7175,2 @@\n-    for (i = Parms; i < _range->cnt()-1; i++) {\n-      _range->field_at(i)->dump2(d,depth,st);\n+    for (i = Parms; i < _range_sig->cnt()-1; i++) {\n+      _range_sig->field_at(i)->dump2(d,depth,st);\n@@ -6669,1 +7179,1 @@\n-    _range->field_at(i)->dump2(d,depth,st);\n+    _range_sig->field_at(i)->dump2(d,depth,st);\n@@ -6678,3 +7188,3 @@\n-  if (Parms < _domain->cnt())\n-    _domain->field_at(Parms)->dump2(d,depth-1,st);\n-  for (uint i = Parms+1; i < _domain->cnt(); i++) {\n+  if (Parms < _domain_sig->cnt())\n+    _domain_sig->field_at(Parms)->dump2(d,depth-1,st);\n+  for (uint i = Parms+1; i < _domain_sig->cnt(); i++) {\n@@ -6682,1 +7192,1 @@\n-    _domain->field_at(i)->dump2(d,depth-1,st);\n+    _domain_sig->field_at(i)->dump2(d,depth-1,st);\n@@ -6702,1 +7212,1 @@\n-  if (range()->cnt() == TypeFunc::Parms) {\n+  if (range_sig()->cnt() == TypeFunc::Parms) {\n@@ -6705,1 +7215,1 @@\n-  return range()->field_at(TypeFunc::Parms)->basic_type();\n+  return range_sig()->field_at(TypeFunc::Parms)->basic_type();\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":858,"deletions":348,"binary":false,"changes":1206,"status":"modified"},{"patch":"@@ -203,1 +203,1 @@\n-      for (uint i = TypeFunc::Parms; i < call->tf()->domain()->cnt(); i++) {\n+      for (uint i = TypeFunc::Parms; i < call->tf()->domain_sig()->cnt(); i++) {\n","filename":"src\/hotspot\/share\/opto\/vector.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -74,0 +74,1 @@\n+#ifdef JDK_8254007_IS_FIXED\n@@ -76,0 +77,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/precompiled\/precompiled.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -57,0 +57,2 @@\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -424,0 +426,1 @@\n+  bool is_flat = InstanceKlass::cast(k1)->field_is_flat(slot);\n@@ -425,1 +428,1 @@\n-  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset);\n+  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset, is_flat);\n@@ -442,1 +445,1 @@\n-  if (m->is_object_initializer()) {\n+  if (m->is_object_constructor()) {\n@@ -801,1 +804,1 @@\n-    case T_OBJECT:      push_object(va_arg(_ap, jobject)); break;\n+    case T_OBJECT: push_object(va_arg(_ap, jobject)); break;\n@@ -841,1 +844,2 @@\n-    case T_OBJECT:      push_object((_ap++)->l); break;\n+    case T_OBJECT:\n+    case T_FLAT_ELEMENT: push_object((_ap++)->l); break;\n@@ -968,1 +972,7 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr || k->is_inline_klass()) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+  instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n@@ -982,1 +992,8 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+\n+  instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n@@ -987,0 +1004,1 @@\n+\n@@ -988,1 +1006,1 @@\n-JNI_END\n+  JNI_END\n@@ -1000,1 +1018,8 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+\n+  instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n@@ -1005,0 +1030,1 @@\n+\n@@ -1018,1 +1044,8 @@\n-  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);\n+  oop clazzoop = JNIHandles::resolve_non_null(clazz);\n+  Klass* k = java_lang_Class::as_Klass(clazzoop);\n+  if (k == nullptr) {\n+    ResourceMark rm(THREAD);\n+    THROW_(vmSymbols::java_lang_InstantiationException(), nullptr);\n+  }\n+\n+  instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);\n@@ -1026,0 +1059,1 @@\n+\n@@ -1776,1 +1810,1 @@\n-  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset());\n+  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset(), fd.is_flat());\n@@ -1786,0 +1820,1 @@\n+  oop res = nullptr;\n@@ -1791,2 +1826,15 @@\n-  oop loaded_obj = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);\n-  jobject ret = JNIHandles::make_local(THREAD, loaded_obj);\n+  if (!jfieldIDWorkaround::is_flat_jfieldID(fieldID)) {\n+    res = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);\n+  } else {\n+    assert(k->is_instance_klass(), \"Only instance can have flat fields\");\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    fieldDescriptor fd;\n+    bool found = ik->find_field_from_offset(offset, false, &fd);  \/\/ performance bottleneck\n+    assert(found, \"Field not found\");\n+    InstanceKlass* holder = fd.field_holder();\n+    assert(holder->field_is_flat(fd.index()), \"Must be\");\n+    InlineLayoutInfo* li = holder->inline_layout_info_adr(fd.index());\n+    InlineKlass* field_vklass = li->klass();\n+    res = field_vklass->read_payload_from_addr(o, ik->field_offset(fd.index()), li->kind(), CHECK_NULL);\n+  }\n+  jobject ret = JNIHandles::make_local(THREAD, res);\n@@ -1797,2 +1845,0 @@\n-\n-\n@@ -1884,1 +1930,13 @@\n-  HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, JNIHandles::resolve(value));\n+  if (!jfieldIDWorkaround::is_flat_jfieldID(fieldID)) {\n+    HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, JNIHandles::resolve(value));\n+  } else {\n+    assert(k->is_instance_klass(), \"Only instances can have flat fields\");\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    fieldDescriptor fd;\n+    ik->find_field_from_offset(offset, false, &fd);\n+    InstanceKlass* holder = fd.field_holder();\n+    InlineLayoutInfo* li = holder->inline_layout_info_adr(fd.index());\n+    InlineKlass* vklass = li->klass();\n+    oop v = JNIHandles::resolve_non_null(value);\n+    vklass->write_value_to_addr(v, ((char*)(oopDesc*)obj) + offset, li->kind(), true, CHECK);\n+  }\n@@ -2309,4 +2367,12 @@\n-  objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n-  if (a->is_within_bounds(index)) {\n-    ret = JNIHandles::make_local(THREAD, a->obj_at(index));\n-    return ret;\n+  oop res = nullptr;\n+  arrayOop arr((arrayOop)JNIHandles::resolve_non_null(array));\n+  if (arr->is_within_bounds(index)) {\n+    if (arr->is_flatArray()) {\n+      flatArrayOop a = flatArrayOop(JNIHandles::resolve_non_null(array));\n+      res = a->read_value_from_flat_array(index, CHECK_NULL);\n+      assert(res != nullptr, \"Must be set in one of two paths above\");\n+    } else {\n+      assert(arr->is_objArray(), \"If not a valueArray. must be an objArray\");\n+      objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n+      res = a->obj_at(index);\n+    }\n@@ -2316,1 +2382,1 @@\n-    ss.print(\"Index %d out of bounds for length %d\", index, a->length());\n+    ss.print(\"Index %d out of bounds for length %d\", index,arr->length());\n@@ -2319,0 +2385,2 @@\n+  ret = JNIHandles::make_local(THREAD, res);\n+  return ret;\n@@ -2328,24 +2396,51 @@\n-  objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n-  oop v = JNIHandles::resolve(value);\n-  if (a->is_within_bounds(index)) {\n-    if (v == nullptr || v->is_a(ObjArrayKlass::cast(a->klass())->element_klass())) {\n-      a->obj_at_put(index, v);\n-    } else {\n-      ResourceMark rm(THREAD);\n-      stringStream ss;\n-      Klass *bottom_kl = ObjArrayKlass::cast(a->klass())->bottom_klass();\n-      ss.print(\"type mismatch: can not store %s to %s[%d]\",\n-               v->klass()->external_name(),\n-               bottom_kl->is_typeArray_klass() ? type2name_tab[ArrayKlass::cast(bottom_kl)->element_type()] : bottom_kl->external_name(),\n-               index);\n-      for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n-        ss.print(\"[]\");\n-      }\n-      THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n-    }\n-  } else {\n-    ResourceMark rm(THREAD);\n-    stringStream ss;\n-    ss.print(\"Index %d out of bounds for length %d\", index, a->length());\n-    THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n-  }\n+   bool oob = false;\n+   int length = -1;\n+   oop res = nullptr;\n+   arrayOop arr((arrayOop)JNIHandles::resolve_non_null(array));\n+   if (arr->is_within_bounds(index)) {\n+     if (arr->is_flatArray()) {\n+       flatArrayOop a = flatArrayOop(JNIHandles::resolve_non_null(array));\n+       oop v = JNIHandles::resolve(value);\n+       FlatArrayKlass* vaklass = FlatArrayKlass::cast(a->klass());\n+       InlineKlass* element_vklass = vaklass->element_klass();\n+       if (v != nullptr && v->is_a(element_vklass)) {\n+         a->write_value_to_flat_array(v, index, CHECK);\n+       } else {\n+         ResourceMark rm(THREAD);\n+         stringStream ss;\n+         Klass *kl = FlatArrayKlass::cast(a->klass());\n+         ss.print(\"type mismatch: can not store %s to %s[%d]\",\n+             v->klass()->external_name(),\n+             kl->external_name(),\n+             index);\n+         for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n+           ss.print(\"[]\");\n+         }\n+         THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+       }\n+     } else {\n+       assert(arr->is_objArray(), \"If not a valueArray. must be an objArray\");\n+       objArrayOop a = objArrayOop(JNIHandles::resolve_non_null(array));\n+       oop v = JNIHandles::resolve(value);\n+       if (v == nullptr || v->is_a(ObjArrayKlass::cast(a->klass())->element_klass())) {\n+         a->obj_at_put(index, v);\n+       } else {\n+         ResourceMark rm(THREAD);\n+         stringStream ss;\n+         Klass *bottom_kl = ObjArrayKlass::cast(a->klass())->bottom_klass();\n+         ss.print(\"type mismatch: can not store %s to %s[%d]\",\n+             v->klass()->external_name(),\n+             bottom_kl->is_typeArray_klass() ? type2name_tab[ArrayKlass::cast(bottom_kl)->element_type()] : bottom_kl->external_name(),\n+                 index);\n+         for (int dims = ArrayKlass::cast(a->klass())->dimension(); dims > 1; --dims) {\n+           ss.print(\"[]\");\n+         }\n+         THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+       }\n+     }\n+   } else {\n+     ResourceMark rm(THREAD);\n+     stringStream ss;\n+     ss.print(\"Index %d out of bounds for length %d\", index, arr->length());\n+     THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n+   }\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":139,"deletions":44,"binary":false,"changes":183,"status":"modified"},{"patch":"@@ -258,1 +258,1 @@\n-      !(fd.field_type() == T_ARRAY && ftype == T_OBJECT)) {\n+      !(fd.field_type() == T_ARRAY && ftype == T_OBJECT))  {\n@@ -346,1 +346,1 @@\n-check_is_obj_array(JavaThread* thr, jarray jArray) {\n+check_is_obj_or_inline_array(JavaThread* thr, jarray jArray) {\n@@ -348,1 +348,1 @@\n-  if (!aOop->is_objArray()) {\n+  if (!aOop->is_objArray() && !aOop->is_flatArray()) {\n@@ -1629,1 +1629,1 @@\n-      check_is_obj_array(thr, array);\n+      check_is_obj_or_inline_array(thr, array);\n@@ -1643,1 +1643,1 @@\n-      check_is_obj_array(thr, array);\n+      check_is_obj_or_inline_array(thr, array);\n","filename":"src\/hotspot\/share\/prims\/jniCheck.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -414,0 +415,75 @@\n+static void validate_array_arguments(Klass* elmClass, jint len, TRAPS) {\n+  if (len < 0) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Array length is negative\");\n+  }\n+  elmClass->initialize(CHECK);\n+  if (elmClass->is_identity_class()) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Element class is not a value class\");\n+  }\n+  if (elmClass->is_abstract()) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Element class is abstract\");\n+  }\n+}\n+\n+JVM_ENTRY(jarray, JVM_NewNullRestrictedArray(JNIEnv *env, jclass elmClass, jint len))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  klass->initialize(CHECK_NULL);\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  if (!vk->is_implicitly_constructible()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Element class is not implicitly constructible\");\n+  }\n+  oop array = nullptr;\n+  if (vk->flat_array()) {\n+    array = oopFactory::new_flatArray(vk, len, LayoutKind::NON_ATOMIC_FLAT, CHECK_NULL);\n+  } else {\n+    array = oopFactory::new_null_free_objArray(vk, len, CHECK_NULL);\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jarray, JVM_NewNullRestrictedAtomicArray(JNIEnv *env, jclass elmClass, jint len))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  klass->initialize(CHECK_NULL);\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  if (!vk->is_implicitly_constructible()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Element class is not implicitly constructible\");\n+  }\n+  oop array = nullptr;\n+  if (UseArrayFlattening && vk->has_atomic_layout()) {\n+    array = oopFactory::new_flatArray(vk, len, LayoutKind::ATOMIC_FLAT, CHECK_NULL);\n+  } else if (UseArrayFlattening && vk->is_naturally_atomic()) {\n+    array = oopFactory::new_flatArray(vk, len, LayoutKind::NON_ATOMIC_FLAT, CHECK_NULL);\n+  } else {\n+    array = oopFactory::new_null_free_objArray(vk, len, CHECK_NULL);\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jarray, JVM_NewNullableAtomicArray(JNIEnv *env, jclass elmClass, jint len))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  klass->initialize(CHECK_NULL);\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  oop array = nullptr;\n+  if (UseArrayFlattening && vk->has_nullable_atomic_layout()) {\n+    array = oopFactory::new_flatArray(vk, len, LayoutKind::NULLABLE_ATOMIC_FLAT, CHECK_NULL);\n+  } else {\n+    array = oopFactory::new_objArray(vk, len, CHECK_NULL);\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsFlatArray(JNIEnv *env, jobject obj))\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  return oop->is_flatArray();\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsNullRestrictedArray(JNIEnv *env, jobject obj))\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  return oop->is_null_free_array();\n+JVM_END\n@@ -622,2 +698,22 @@\n-  return handle == nullptr ? 0 :\n-         checked_cast<jint>(ObjectSynchronizer::FastHashCode (THREAD, JNIHandles::resolve_non_null(handle)));\n+  if (handle == nullptr) {\n+    return 0;\n+  }\n+  oop obj = JNIHandles::resolve_non_null(handle);\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+      JavaValue result(T_INT);\n+      JavaCallArguments args;\n+      Handle ho(THREAD, obj);\n+      args.push_oop(ho);\n+      methodHandle method(THREAD, Universe::value_object_hash_code_method());\n+      JavaCalls::call(&result, method, &args, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in hashCode\", e, false);\n+        }\n+      }\n+      return result.get_jint();\n+  } else {\n+    return checked_cast<jint>(ObjectSynchronizer::FastHashCode(THREAD, obj));\n+  }\n@@ -681,0 +777,6 @@\n+  if (klass->is_inline_klass()) {\n+    \/\/ Value instances have no identity, so return the current instance instead of allocating a new one\n+    \/\/ Value classes cannot have finalizers, so the method can return immediately\n+    return JNIHandles::make_local(THREAD, obj());\n+  }\n+\n@@ -1175,1 +1277,2 @@\n-    size = InstanceKlass::cast(klass)->local_interfaces()->length();\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    size = ik->local_interfaces()->length();\n@@ -1177,1 +1280,1 @@\n-    assert(klass->is_objArray_klass() || klass->is_typeArray_klass(), \"Illegal mirror klass\");\n+    assert(klass->is_objArray_klass() || klass->is_typeArray_klass() || klass->is_flatArray_klass(), \"Illegal mirror klass\");\n@@ -1188,1 +1291,2 @@\n-      Klass* k = InstanceKlass::cast(klass)->local_interfaces()->at(index);\n+      InstanceKlass* ik = InstanceKlass::cast(klass);\n+      Klass* k = ik->local_interfaces()->at(index);\n@@ -1223,0 +1327,19 @@\n+JVM_ENTRY(jboolean, JVM_IsIdentityClass(JNIEnv *env, jclass cls))\n+  oop mirror = JNIHandles::resolve_non_null(cls);\n+  if (java_lang_Class::is_primitive(mirror)) {\n+    return JNI_FALSE;\n+  }\n+  Klass* k = java_lang_Class::as_Klass(mirror);\n+  if (EnableValhalla) {\n+    return k->is_array_klass() || k->is_identity_class();\n+  } else {\n+    return k->is_interface() ? JNI_FALSE : JNI_TRUE;\n+  }\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsImplicitlyConstructibleClass(JNIEnv *env, jclass cls))\n+  oop mirror = JNIHandles::resolve_non_null(cls);\n+  InstanceKlass* ik = InstanceKlass::cast(java_lang_Class::as_Klass(mirror));\n+  return ik->is_implicitly_constructible();\n+JVM_END\n+\n@@ -1751,1 +1874,1 @@\n-    if (want_constructor && !method->is_object_initializer()) {\n+    if (want_constructor && !method->is_object_constructor()) {\n@@ -1755,1 +1878,1 @@\n-        (method->is_object_initializer() || method->is_static_initializer() ||\n+        (method->is_object_constructor() || method->is_class_initializer() ||\n@@ -1783,0 +1906,1 @@\n+        assert(method->is_object_constructor(), \"must be\");\n@@ -2065,1 +2189,1 @@\n-  if (m->is_object_initializer()) {\n+  if (m->is_object_constructor()) {\n@@ -2068,1 +2192,0 @@\n-    \/\/ new_method accepts <clinit> as Method here\n@@ -2340,0 +2463,37 @@\n+\/\/ Arrays support \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+JVM_ENTRY(jboolean, JVM_ArrayIsAccessAtomic(JNIEnv *env, jclass unused, jobject array))\n+  oop o = JNIHandles::resolve(array);\n+  Klass* k = o->klass();\n+  if ((o == nullptr) || (!k->is_array_klass())) {\n+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());\n+  }\n+  return ArrayKlass::cast(k)->element_access_must_be_atomic();\n+JVM_END\n+\n+JVM_ENTRY(jobject, JVM_ArrayEnsureAccessAtomic(JNIEnv *env, jclass unused, jobject array))\n+  oop o = JNIHandles::resolve(array);\n+  Klass* k = o->klass();\n+  if ((o == nullptr) || (!k->is_array_klass())) {\n+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());\n+  }\n+  if (k->is_flatArray_klass()) {\n+    FlatArrayKlass* vk = FlatArrayKlass::cast(k);\n+    if (!vk->element_access_must_be_atomic()) {\n+      \/**\n+       * Need to decide how to implement:\n+       *\n+       * 1) Change to objArrayOop layout, therefore oop->klass() differs so\n+       * then \"<atomic>[Qfoo;\" klass needs to subclass \"[Qfoo;\" to pass through\n+       * \"checkcast\" & \"instanceof\"\n+       *\n+       * 2) Use extra header in the flatArrayOop to flag atomicity required and\n+       * possibly per instance lock structure. Said info, could be placed in\n+       * \"trailer\" rather than disturb the current arrayOop\n+       *\/\n+      Unimplemented();\n+    }\n+  }\n+  return array;\n+JVM_END\n+\n@@ -2502,1 +2662,1 @@\n-  return method->name() == vmSymbols::object_initializer_name();\n+  return method->is_object_constructor();\n@@ -3293,0 +3453,4 @@\n+JVM_LEAF(jboolean, JVM_IsValhallaEnabled(void))\n+  return EnableValhalla ? JNI_TRUE : JNI_FALSE;\n+JVM_END\n+\n@@ -3372,1 +3536,1 @@\n-    objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n+    objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);\n@@ -3392,0 +3556,1 @@\n+  objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);\n@@ -3393,1 +3558,0 @@\n-  objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":176,"deletions":12,"binary":false,"changes":188,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"classfile\/vmClasses.hpp\"\n@@ -474,0 +475,20 @@\n+\/\/ LoadableDescriptors {\n+\/\/   u2 attribute_name_index;\n+\/\/   u4 attribute_length;\n+\/\/   u2 number_of_descriptors;\n+\/\/   u2 descriptors[number_of_descriptors];\n+\/\/ }\n+void JvmtiClassFileReconstituter::write_loadable_descriptors_attribute() {\n+  Array<u2>* loadable_descriptors = ik()->loadable_descriptors();\n+  int number_of_descriptors = loadable_descriptors->length();\n+  int length = sizeof(u2) * (1 + number_of_descriptors); \/\/ '1 +' is for number_of_descriptors field\n+\n+  write_attribute_name_index(\"LoadableDescriptors\");\n+  write_u4(length);\n+  write_u2(checked_cast<u2>(number_of_descriptors));\n+  for (int i = 0; i < number_of_descriptors; i++) {\n+    u2 utf8_index = loadable_descriptors->at(i);\n+    write_u2(utf8_index);\n+  }\n+}\n+\n@@ -813,0 +834,3 @@\n+  if (ik()->loadable_descriptors() != Universe::the_empty_short_array()) {\n+    ++attr_count;\n+  }\n@@ -843,0 +867,3 @@\n+  if (ik()->loadable_descriptors() != Universe::the_empty_short_array()) {\n+    write_loadable_descriptors_attribute();\n+  }\n@@ -1032,1 +1059,1 @@\n-      case Bytecodes::_putfield        :  {\n+      case Bytecodes::_putfield        : {\n","filename":"src\/hotspot\/share\/prims\/jvmtiClassFileReconstituter.cpp","additions":28,"deletions":1,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -2728,3 +2728,3 @@\n-    if (k->is_super()) {\n-      result |= JVM_ACC_SUPER;\n-    }\n+    \/\/ if (k->is_super()) {\n+    \/\/   result |= JVM_ACC_SUPER;\n+    \/\/ }\n@@ -2861,1 +2861,2 @@\n-                                                     flds.access_flags().is_static());\n+                                                     flds.access_flags().is_static(),\n+                                                     flds.field_descriptor().is_flat());\n@@ -2899,2 +2900,3 @@\n-    Array<InstanceKlass*>* interface_list = InstanceKlass::cast(k)->local_interfaces();\n-    const int result_length = (interface_list == nullptr ? 0 : interface_list->length());\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    Array<InstanceKlass*>* interface_list = ik->local_interfaces();\n+    int result_length = (interface_list == nullptr ? 0 : interface_list->length());\n@@ -3096,3 +3098,6 @@\n-  {\n-    jint result = (jint) mirror->identity_hash();\n-    *hash_code_ptr = result;\n+  if (mirror->is_inline_type()) {\n+    \/\/ For inline types, use the klass as a hash code.\n+    \/\/ TBD to improve this (see also JvmtiTagMapKey::get_hash for similar case).\n+    *hash_code_ptr = (jint)((int64_t)mirror->klass() >> 3);\n+  } else {\n+    *hash_code_ptr = (jint)mirror->identity_hash();\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnv.cpp","additions":14,"deletions":9,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -80,0 +81,1 @@\n+      \/\/ CMH flat arrays (InlineKlass)\n","filename":"src\/hotspot\/share\/prims\/jvmtiGetLoadedClasses.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -614,2 +614,1 @@\n-    \/\/ At this stage JVM_CONSTANT_UnresolvedClassInError should not be\n-    \/\/ here\n+    \/\/ At this stage JVM_CONSTANT_UnresolvedClassInError should not be here\n@@ -1939,0 +1938,6 @@\n+  \/\/ rewrite constant pool references in the LoadableDescriptors attribute:\n+  if (!rewrite_cp_refs_in_loadable_descriptors_attribute(scratch_class)) {\n+    \/\/ propagate failure back to caller\n+    return false;\n+  }\n+\n@@ -2087,0 +2092,13 @@\n+\/\/ Rewrite constant pool references in the LoadableDescriptors attribute.\n+bool VM_RedefineClasses::rewrite_cp_refs_in_loadable_descriptors_attribute(\n+       InstanceKlass* scratch_class) {\n+\n+  Array<u2>* loadable_descriptors = scratch_class->loadable_descriptors();\n+  assert(loadable_descriptors != nullptr, \"unexpected null loadable_descriptors\");\n+  for (int i = 0; i < loadable_descriptors->length(); i++) {\n+    u2 cp_index = loadable_descriptors->at(i);\n+    loadable_descriptors->at_put(i, find_new_index(cp_index));\n+  }\n+  return true;\n+}\n+\n","filename":"src\/hotspot\/share\/prims\/jvmtiRedefineClasses.cpp","additions":20,"deletions":2,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -66,0 +66,17 @@\n+unsigned JvmtiTagMapKey::get_hash(const JvmtiTagMapKey& entry) {\n+  oop obj = entry._obj;\n+  assert(obj != nullptr, \"must lookup obj to hash\");\n+  if (obj->is_inline_type()) {\n+    \/\/ For inline types, use the klass as a hash code and let the equals match the obj.\n+    \/\/ It might have a long bucket but TBD to improve this if a customer situation arises.\n+    return (unsigned)((int64_t)obj->klass() >> 3);\n+  } else {\n+    return (unsigned)obj->identity_hash();\n+  }\n+}\n+\n+\/\/ Inline types don't use hash for this table.\n+static inline bool fast_no_hash_check(oop obj) {\n+  return (obj->fast_no_hash_check() && !obj->is_inline_type());\n+}\n+\n@@ -96,2 +113,2 @@\n-  if (obj->fast_no_hash_check()) {\n-    \/\/ Objects in the table all have a hashcode.\n+  if (fast_no_hash_check(obj)) {\n+    \/\/ Objects in the table all have a hashcode, unless inlined types.\n@@ -109,1 +126,1 @@\n-  if (obj->fast_no_hash_check()) {\n+  if (fast_no_hash_check(obj)) {\n","filename":"src\/hotspot\/share\/prims\/jvmtiTagMapTable.cpp","additions":20,"deletions":3,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -132,13 +132,16 @@\n-  IS_METHOD            = java_lang_invoke_MemberName::MN_IS_METHOD,\n-  IS_CONSTRUCTOR       = java_lang_invoke_MemberName::MN_IS_CONSTRUCTOR,\n-  IS_FIELD             = java_lang_invoke_MemberName::MN_IS_FIELD,\n-  IS_TYPE              = java_lang_invoke_MemberName::MN_IS_TYPE,\n-  CALLER_SENSITIVE     = java_lang_invoke_MemberName::MN_CALLER_SENSITIVE,\n-  TRUSTED_FINAL        = java_lang_invoke_MemberName::MN_TRUSTED_FINAL,\n-  HIDDEN_MEMBER        = java_lang_invoke_MemberName::MN_HIDDEN_MEMBER,\n-  REFERENCE_KIND_SHIFT = java_lang_invoke_MemberName::MN_REFERENCE_KIND_SHIFT,\n-  REFERENCE_KIND_MASK  = java_lang_invoke_MemberName::MN_REFERENCE_KIND_MASK,\n-  LM_UNCONDITIONAL     = java_lang_invoke_MemberName::MN_UNCONDITIONAL_MODE,\n-  LM_MODULE            = java_lang_invoke_MemberName::MN_MODULE_MODE,\n-  LM_TRUSTED           = java_lang_invoke_MemberName::MN_TRUSTED_MODE,\n-  ALL_KINDS      = IS_METHOD | IS_CONSTRUCTOR | IS_FIELD | IS_TYPE\n+  IS_METHOD             = java_lang_invoke_MemberName::MN_IS_METHOD,\n+  IS_OBJECT_CONSTRUCTOR = java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR,\n+  IS_FIELD              = java_lang_invoke_MemberName::MN_IS_FIELD,\n+  IS_TYPE               = java_lang_invoke_MemberName::MN_IS_TYPE,\n+  CALLER_SENSITIVE      = java_lang_invoke_MemberName::MN_CALLER_SENSITIVE,\n+  TRUSTED_FINAL         = java_lang_invoke_MemberName::MN_TRUSTED_FINAL,\n+  HIDDEN_MEMBER         = java_lang_invoke_MemberName::MN_HIDDEN_MEMBER,\n+  NULL_RESTRICTED       = java_lang_invoke_MemberName::MN_NULL_RESTRICTED_FIELD,\n+  REFERENCE_KIND_SHIFT  = java_lang_invoke_MemberName::MN_REFERENCE_KIND_SHIFT,\n+  REFERENCE_KIND_MASK   = java_lang_invoke_MemberName::MN_REFERENCE_KIND_MASK,\n+  LAYOUT_SHIFT          = java_lang_invoke_MemberName::MN_LAYOUT_SHIFT,\n+  LAYOUT_MASK           = java_lang_invoke_MemberName::MN_LAYOUT_MASK,\n+  LM_UNCONDITIONAL      = java_lang_invoke_MemberName::MN_UNCONDITIONAL_MODE,\n+  LM_MODULE             = java_lang_invoke_MemberName::MN_MODULE_MODE,\n+  LM_TRUSTED            = java_lang_invoke_MemberName::MN_TRUSTED_MODE,\n+  ALL_KINDS      = IS_METHOD | IS_OBJECT_CONSTRUCTOR | IS_FIELD | IS_TYPE\n@@ -155,1 +158,1 @@\n-    flags |= IS_CONSTRUCTOR;\n+    flags |= IS_OBJECT_CONSTRUCTOR;\n@@ -174,1 +177,1 @@\n-    case IS_CONSTRUCTOR:\n+    case IS_OBJECT_CONSTRUCTOR:\n@@ -315,1 +318,1 @@\n-      assert(!m->is_static_initializer(), \"Cannot be static initializer\");\n+      assert(!m->is_class_initializer(), \"Cannot be static initializer\");\n@@ -317,2 +320,2 @@\n-    } else if (m->is_object_initializer()) {\n-      flags |= IS_CONSTRUCTOR | (JVM_REF_invokeSpecial << REFERENCE_KIND_SHIFT);\n+    } else if (m->is_object_constructor()) {\n+      flags |= IS_OBJECT_CONSTRUCTOR | (JVM_REF_invokeSpecial << REFERENCE_KIND_SHIFT);\n@@ -357,0 +360,6 @@\n+  if (fd.is_flat()) {\n+    int layout_kind = fd.layout_kind();\n+    assert((layout_kind & LAYOUT_MASK) == layout_kind, \"Layout information loss\");\n+    flags |= layout_kind << LAYOUT_SHIFT;\n+  }\n+  if (fd.is_null_free_inline_type()) flags |= NULL_RESTRICTED;\n@@ -807,1 +816,1 @@\n-  case IS_CONSTRUCTOR:\n+  case IS_OBJECT_CONSTRUCTOR:\n@@ -813,1 +822,1 @@\n-        if (name == vmSymbols::object_initializer_name()) {\n+        if (name == vmSymbols::object_initializer_name() && type->is_void_method_signature()) {\n@@ -876,1 +885,1 @@\n-  case IS_CONSTRUCTOR:\n+  case IS_OBJECT_CONSTRUCTOR:\n@@ -998,1 +1007,1 @@\n-    template(java_lang_invoke_MemberName,MN_IS_CONSTRUCTOR) \\\n+    template(java_lang_invoke_MemberName,MN_IS_OBJECT_CONSTRUCTOR) \\\n@@ -1006,0 +1015,2 @@\n+    template(java_lang_invoke_MemberName,MN_LAYOUT_SHIFT) \\\n+    template(java_lang_invoke_MemberName,MN_LAYOUT_MASK) \\\n@@ -1141,1 +1152,1 @@\n-               (flags & ALL_KINDS) == IS_CONSTRUCTOR) {\n+               (flags & ALL_KINDS) == IS_OBJECT_CONSTRUCTOR) {\n","filename":"src\/hotspot\/share\/prims\/methodHandles.cpp","additions":34,"deletions":23,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -36,0 +36,2 @@\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logStream.hpp\"\n@@ -38,0 +40,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -45,0 +50,1 @@\n+#include \"runtime\/fieldDescriptor.inline.hpp\"\n@@ -171,1 +177,0 @@\n-\n@@ -250,0 +255,1 @@\n+    assert(_obj == nullptr || !_obj->is_inline_type() || _obj->mark().is_larval_state(), \"must be an object instance or a larval inline type\");\n@@ -253,1 +259,0 @@\n-\n@@ -266,0 +271,62 @@\n+#ifdef ASSERT\n+\/*\n+ * Get the field descriptor of the field of the given object at the given offset.\n+ *\/\n+static bool get_field_descriptor(oop p, jlong offset, fieldDescriptor* fd) {\n+  bool found = false;\n+  Klass* k = p->klass();\n+  if (k->is_instance_klass()) {\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    found = ik->find_field_from_offset((int)offset, false, fd);\n+    if (!found && ik->is_mirror_instance_klass()) {\n+      Klass* k2 = java_lang_Class::as_Klass(p);\n+      if (k2->is_instance_klass()) {\n+        ik = InstanceKlass::cast(k2);\n+        found = ik->find_field_from_offset((int)offset, true, fd);\n+      }\n+    }\n+  }\n+  return found;\n+}\n+#endif \/\/ ASSERT\n+\n+static void assert_and_log_unsafe_value_access(oop p, jlong offset, InlineKlass* vk) {\n+  Klass* k = p->klass();\n+#ifdef ASSERT\n+  if (k->is_instance_klass()) {\n+    assert_field_offset_sane(p, offset);\n+    fieldDescriptor fd;\n+    bool found = get_field_descriptor(p, offset, &fd);\n+    if (found) {\n+      assert(found, \"value field not found\");\n+      assert(fd.is_flat(), \"field not flat\");\n+    } else {\n+      if (log_is_enabled(Trace, valuetypes)) {\n+        log_trace(valuetypes)(\"not a field in %s at offset \" UINT64_FORMAT_X,\n+                              p->klass()->external_name(), (uint64_t)offset);\n+      }\n+    }\n+  } else if (k->is_flatArray_klass()) {\n+    FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+    int index = (offset - vak->array_header_in_bytes()) \/ vak->element_byte_size();\n+    address dest = (address)((flatArrayOop)p)->value_at_addr(index, vak->layout_helper());\n+    assert(dest == (cast_from_oop<address>(p) + offset), \"invalid offset\");\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+#endif \/\/ ASSERT\n+  if (log_is_enabled(Trace, valuetypes)) {\n+    if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+      int index = (offset - vak->array_header_in_bytes()) \/ vak->element_byte_size();\n+      address dest = (address)((flatArrayOop)p)->value_at_addr(index, vak->layout_helper());\n+      log_trace(valuetypes)(\"%s array type %s index %d element size %d offset \" UINT64_FORMAT_X \" at \" INTPTR_FORMAT,\n+                            p->klass()->external_name(), vak->external_name(),\n+                            index, vak->element_byte_size(), (uint64_t)offset, p2i(dest));\n+    } else {\n+      log_trace(valuetypes)(\"%s field type %s at offset \" UINT64_FORMAT_X,\n+                            p->klass()->external_name(), vk->external_name(), (uint64_t)offset);\n+    }\n+  }\n+}\n+\n@@ -280,0 +347,1 @@\n+  assert(!p->is_inline_type() || p->mark().is_larval_state(), \"must be an object instance or a larval inline type\");\n@@ -283,0 +351,156 @@\n+UNSAFE_ENTRY(jlong, Unsafe_ValueHeaderSize(JNIEnv *env, jobject unsafe, jclass c)) {\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(c));\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  return vk->payload_offset();\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jboolean, Unsafe_IsFlatField(JNIEnv *env, jobject unsafe, jobject o)) {\n+  oop f = JNIHandles::resolve_non_null(o);\n+  Klass* k = java_lang_Class::as_Klass(java_lang_reflect_Field::clazz(f));\n+  int slot = java_lang_reflect_Field::slot(f);\n+  return InstanceKlass::cast(k)->field_is_flat(slot);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jboolean, Unsafe_HasNullMarker(JNIEnv *env, jobject unsafe, jobject o)) {\n+  oop f = JNIHandles::resolve_non_null(o);\n+  Klass* k = java_lang_Class::as_Klass(java_lang_reflect_Field::clazz(f));\n+  int slot = java_lang_reflect_Field::slot(f);\n+  return InstanceKlass::cast(k)->field_has_null_marker(slot);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jint, Unsafe_NullMarkerOffset(JNIEnv *env, jobject unsafe, jobject o)) {\n+  oop f = JNIHandles::resolve_non_null(o);\n+  Klass* k = java_lang_Class::as_Klass(java_lang_reflect_Field::clazz(f));\n+  int slot = java_lang_reflect_Field::slot(f);\n+  return InstanceKlass::cast(k)->null_marker_offset(slot);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jint, Unsafe_ArrayLayout(JNIEnv *env, jobject unsafe, jclass c)) {\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(c));\n+  if (!k->is_flatArray_klass()) {\n+    return LayoutKind::REFERENCE;\n+  } else {\n+    return (jint)FlatArrayKlass::cast(k)->layout_kind();\n+  }\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jint, Unsafe_FieldLayout(JNIEnv *env, jobject unsafe, jobject field)) {\n+  assert(field != nullptr, \"field must not be null\");\n+\n+  oop reflected   = JNIHandles::resolve_non_null(field);\n+  oop mirror      = java_lang_reflect_Field::clazz(reflected);\n+  Klass* k        = java_lang_Class::as_Klass(mirror);\n+  int slot        = java_lang_reflect_Field::slot(reflected);\n+  int modifiers   = java_lang_reflect_Field::modifiers(reflected);\n+\n+  if ((modifiers & JVM_ACC_STATIC) != 0) {\n+    return (jint)LayoutKind::REFERENCE; \/\/ static fields are never flat\n+  } else {\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    if (ik->field_is_flat(slot)) {\n+      return (jint)ik->inline_layout_info(slot).kind();\n+    } else {\n+      return (jint)LayoutKind::REFERENCE;\n+    }\n+  }\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jboolean, Unsafe_IsFlatArray(JNIEnv *env, jobject unsafe, jclass c)) {\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(c));\n+  return k->is_flatArray_klass();\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jobject, Unsafe_GetValue(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jclass vc)) {\n+  oop base = JNIHandles::resolve(obj);\n+  if (base == nullptr) {\n+    THROW_NULL(vmSymbols::java_lang_NullPointerException());\n+  }\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  assert_and_log_unsafe_value_access(base, offset, vk);\n+  LayoutKind lk = LayoutKind::UNKNOWN;\n+  if (base->is_array()) {\n+    FlatArrayKlass* fak = FlatArrayKlass::cast(base->klass());\n+    lk = fak->layout_kind();\n+  } else {\n+    fieldDescriptor fd;\n+    InstanceKlass::cast(base->klass())->find_field_from_offset(offset, false, &fd);\n+    lk = fd.field_holder()->inline_layout_info(fd.index()).kind();\n+  }\n+  Handle base_h(THREAD, base);\n+  oop v = vk->read_payload_from_addr(base_h(), offset, lk, CHECK_NULL);\n+  return JNIHandles::make_local(THREAD, v);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jobject, Unsafe_GetFlatValue(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint layoutKind, jclass vc)) {\n+  assert(layoutKind != LayoutKind::REFERENCE, \"This method handles only flat layouts\");\n+  oop base = JNIHandles::resolve(obj);\n+  if (base == nullptr) {\n+    THROW_NULL(vmSymbols::java_lang_NullPointerException());\n+  }\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  assert_and_log_unsafe_value_access(base, offset, vk);\n+  LayoutKind lk = (LayoutKind)layoutKind;\n+  Handle base_h(THREAD, base);\n+  oop v = vk->read_payload_from_addr(base_h(), offset, lk, CHECK_NULL);\n+  return JNIHandles::make_local(THREAD, v);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(void, Unsafe_PutValue(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jclass vc, jobject value)) {\n+  oop base = JNIHandles::resolve(obj);\n+  if (base == nullptr) {\n+    THROW(vmSymbols::java_lang_NullPointerException());\n+  }\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  assert(!base->is_inline_type() || base->mark().is_larval_state(), \"must be an object instance or a larval inline type\");\n+  assert_and_log_unsafe_value_access(base, offset, vk);\n+  LayoutKind lk = LayoutKind::UNKNOWN;\n+  if (base->is_array()) {\n+    FlatArrayKlass* fak = FlatArrayKlass::cast(base->klass());\n+    lk = fak->layout_kind();\n+  } else {\n+    fieldDescriptor fd;\n+    InstanceKlass::cast(base->klass())->find_field_from_offset(offset, false, &fd);\n+    lk = fd.field_holder()->inline_layout_info(fd.index()).kind();\n+  }\n+  oop v = JNIHandles::resolve(value);\n+  vk->write_value_to_addr(v, ((char*)(oopDesc*)base) + offset, lk, true, CHECK);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(void, Unsafe_PutFlatValue(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint layoutKind, jclass vc, jobject value)) {\n+  assert(layoutKind != LayoutKind::REFERENCE, \"This method handles only flat layouts\");\n+  oop base = JNIHandles::resolve(obj);\n+  if (base == nullptr) {\n+    THROW(vmSymbols::java_lang_NullPointerException());\n+  }\n+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));\n+  InlineKlass* vk = InlineKlass::cast(k);\n+  assert(!base->is_inline_type() || base->mark().is_larval_state(), \"must be an object instance or a larval inline type\");\n+  assert_and_log_unsafe_value_access(base, offset, vk);\n+  LayoutKind lk = (LayoutKind)layoutKind;\n+  oop v = JNIHandles::resolve(value);\n+  vk->write_value_to_addr(v, ((char*)(oopDesc*)base) + offset, lk, true, CHECK);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jobject, Unsafe_MakePrivateBuffer(JNIEnv *env, jobject unsafe, jobject value)) {\n+  oop v = JNIHandles::resolve_non_null(value);\n+  assert(v->is_inline_type(), \"must be an inline type instance\");\n+  Handle vh(THREAD, v);\n+  InlineKlass* vk = InlineKlass::cast(v->klass());\n+  instanceOop new_value = vk->allocate_instance_buffer(CHECK_NULL);\n+  vk->copy_payload_to_addr(vk->payload_addr(vh()), vk->payload_addr(new_value), LayoutKind::BUFFERED, false);\n+  markWord mark = new_value->mark();\n+  new_value->set_mark(mark.enter_larval_state());\n+  return JNIHandles::make_local(THREAD, new_value);\n+} UNSAFE_END\n+\n+UNSAFE_ENTRY(jobject, Unsafe_FinishPrivateBuffer(JNIEnv *env, jobject unsafe, jobject value)) {\n+  oop v = JNIHandles::resolve(value);\n+  assert(v->mark().is_larval_state(), \"must be a larval value\");\n+  markWord mark = v->mark();\n+  v->set_mark(mark.exit_larval_state());\n+  return JNIHandles::make_local(THREAD, v);\n+} UNSAFE_END\n+\n@@ -603,0 +827,5 @@\n+  } else if (k->is_flatArray_klass()) {\n+    FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+    InlineKlass* vklass = vak->element_klass();\n+    base = vak->array_header_in_bytes();\n+    scale = vak->element_byte_size();\n@@ -638,0 +867,6 @@\n+UNSAFE_ENTRY(jlong, Unsafe_GetObjectSize0(JNIEnv* env, jobject o, jobject obj))\n+  oop p = JNIHandles::resolve(obj);\n+  return p->size() * HeapWordSize;\n+UNSAFE_END\n+\n+\n@@ -857,4 +1092,4 @@\n-    {CC \"get\" #Type,      CC \"(\" OBJ \"J)\" #Desc,       FN_PTR(Unsafe_Get##Type)}, \\\n-    {CC \"put\" #Type,      CC \"(\" OBJ \"J\" #Desc \")V\",   FN_PTR(Unsafe_Put##Type)}, \\\n-    {CC \"get\" #Type \"Volatile\",      CC \"(\" OBJ \"J)\" #Desc,       FN_PTR(Unsafe_Get##Type##Volatile)}, \\\n-    {CC \"put\" #Type \"Volatile\",      CC \"(\" OBJ \"J\" #Desc \")V\",   FN_PTR(Unsafe_Put##Type##Volatile)}\n+    {CC \"get\"  #Type,      CC \"(\" OBJ \"J)\" #Desc,                 FN_PTR(Unsafe_Get##Type)}, \\\n+    {CC \"put\"  #Type,      CC \"(\" OBJ \"J\" #Desc \")V\",             FN_PTR(Unsafe_Put##Type)}, \\\n+    {CC \"get\"  #Type \"Volatile\",      CC \"(\" OBJ \"J)\" #Desc,      FN_PTR(Unsafe_Get##Type##Volatile)}, \\\n+    {CC \"put\"  #Type \"Volatile\",      CC \"(\" OBJ \"J\" #Desc \")V\",  FN_PTR(Unsafe_Put##Type##Volatile)}\n@@ -869,0 +1104,14 @@\n+    {CC \"isFlatArray\",          CC \"(\" CLS \")Z\",          FN_PTR(Unsafe_IsFlatArray)},\n+    {CC \"isFlatField0\",         CC \"(\" OBJ \")Z\",          FN_PTR(Unsafe_IsFlatField)},\n+    {CC \"hasNullMarker0\",       CC \"(\" OBJ \")Z\",          FN_PTR(Unsafe_HasNullMarker)},\n+    {CC \"nullMarkerOffset0\",    CC \"(\" OBJ \")I\",          FN_PTR(Unsafe_NullMarkerOffset)},\n+    {CC \"arrayLayout0\",         CC \"(\" OBJ \")I\",          FN_PTR(Unsafe_ArrayLayout)},\n+    {CC \"fieldLayout0\",         CC \"(\" OBJ \")I\",          FN_PTR(Unsafe_FieldLayout)},\n+    {CC \"getValue\",             CC \"(\" OBJ \"J\" CLS \")\" OBJ, FN_PTR(Unsafe_GetValue)},\n+    {CC \"getFlatValue\",         CC \"(\" OBJ \"JI\" CLS \")\" OBJ, FN_PTR(Unsafe_GetFlatValue)},\n+    {CC \"putValue\",             CC \"(\" OBJ \"J\" CLS OBJ \")V\", FN_PTR(Unsafe_PutValue)},\n+    {CC \"putFlatValue\",         CC \"(\" OBJ \"JI\" CLS OBJ \")V\", FN_PTR(Unsafe_PutFlatValue)},\n+    {CC \"makePrivateBuffer\",     CC \"(\" OBJ \")\" OBJ,      FN_PTR(Unsafe_MakePrivateBuffer)},\n+    {CC \"finishPrivateBuffer\",   CC \"(\" OBJ \")\" OBJ,      FN_PTR(Unsafe_FinishPrivateBuffer)},\n+    {CC \"valueHeaderSize\",       CC \"(\" CLS \")J\",         FN_PTR(Unsafe_ValueHeaderSize)},\n+\n@@ -891,0 +1140,1 @@\n+    {CC \"getObjectSize0\",     CC \"(Ljava\/lang\/Object;)J\", FN_PTR(Unsafe_GetObjectSize0)},\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":256,"deletions":6,"binary":false,"changes":262,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -61,0 +62,1 @@\n+#include \"oops\/access.hpp\"\n@@ -63,0 +65,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -70,0 +73,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -88,0 +92,1 @@\n+#include \"runtime\/keepStackGCProcessed.hpp\"\n@@ -1927,0 +1932,103 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return nullptr;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(ih->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return nullptr;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayHandle result_array =\n+      oopFactory::new_objArray_handle(vmClasses::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ih->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+WB_END\n+\n+\/\/ Collect Object oops but not value objects...loaded from heap\n+class CollectObjectOops : public BasicOopIterateClosure {\n+  public:\n+  GrowableArray<Handle>* _array;\n+\n+  CollectObjectOops() {\n+      _array = new GrowableArray<Handle>(128);\n+  }\n+\n+  void add_oop(oop o) {\n+    Handle oh = Handle(Thread::current(), o);\n+    if (oh != nullptr && oh->is_inline_type()) {\n+      oh->oop_iterate(this);\n+    } else {\n+      _array->append(oh);\n+    }\n+  }\n+\n+  template <class T> inline void add_oop(T* p) { add_oop(HeapAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    objArrayHandle result_array =\n+        oopFactory::new_objArray_handle(vmClasses::Object_klass(), _array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < _array->length(); i++) {\n+      result_array->obj_at_put(i, _array->at(i)());\n+    }\n+    return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+  }\n+};\n+\n+\/\/ Collect Object oops but not value objects...loaded from frames\n+class CollectFrameObjectOops : public BasicOopIterateClosure {\n+ public:\n+  CollectObjectOops _collect;\n+\n+  template <class T> inline void add_oop(T* p) { _collect.add_oop(RawAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    return _collect.create_jni_result(env, THREAD);\n+  }\n+};\n+\n+\/\/ Collect Object oops for the given oop, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(thread);\n+  Handle objh(thread, JNIHandles::resolve(thing));\n+  CollectObjectOops collectOops;\n+  objh->oop_iterate(&collectOops);\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\/\/ Collect Object oops for the given frame deep, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  KeepStackGCProcessedMark ksgcpm(THREAD);\n+  ResourceMark rm(THREAD);\n+  CollectFrameObjectOops collectOops;\n+  StackFrameStream sfs(thread, true \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, nullptr, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n@@ -2896,0 +3004,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":114,"deletions":0,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -78,0 +78,1 @@\n+#include <string.h>\n@@ -1767,1 +1768,0 @@\n-static unsigned int patch_mod_count = 0;\n@@ -1774,1 +1774,1 @@\n-  if (!CDSConfig::check_vm_args_consistency(patch_mod_javabase, mode_flag_cmd_line)) {\n+  if (!CDSConfig::check_vm_args_consistency(mode_flag_cmd_line)) {\n@@ -2073,1 +2073,1 @@\n-      add_patch_mod_prefix(module_name, module_equal + 1);\n+      add_patch_mod_prefix(module_name, module_equal + 1, false \/* no append *\/, false \/* no cds *\/);\n@@ -2075,3 +2075,0 @@\n-      if (!create_numbered_module_property(\"jdk.module.patch\", patch_mod_tail, patch_mod_count++)) {\n-        return JNI_ENOMEM;\n-      }\n@@ -2085,0 +2082,64 @@\n+\/\/ VALUECLASS_STR must match string used in the build\n+#define VALUECLASS_STR \"valueclasses\"\n+#define VALUECLASS_JAR \"-\" VALUECLASS_STR \".jar\"\n+\n+\/\/ Finalize --patch-module args and --enable-preview related to value class module patches.\n+\/\/ Create all numbered properties passing module patches.\n+int Arguments::finalize_patch_module() {\n+  \/\/ If --enable-preview and EnableValhalla is true, each module may have value classes that\n+  \/\/ are to be patched into the module.\n+  \/\/ For each <module>-valueclasses.jar in <JAVA_HOME>\/lib\/valueclasses\/\n+  \/\/ appends the equivalent of --patch-module <module>=<JAVA_HOME>\/lib\/valueclasses\/<module>-valueclasses.jar\n+  if (enable_preview() && EnableValhalla) {\n+    char * valueclasses_dir = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+    const char * fileSep = os::file_separator();\n+\n+    jio_snprintf(valueclasses_dir, JVM_MAXPATHLEN, \"%s%slib%s\" VALUECLASS_STR \"%s\",\n+                 Arguments::get_java_home(), fileSep, fileSep, fileSep);\n+    DIR* dir = os::opendir(valueclasses_dir);\n+    if (dir != nullptr) {\n+      char * module_name = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+      char * path = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+\n+      for (dirent * entry = os::readdir(dir); entry != nullptr; entry = os::readdir(dir)) {\n+        \/\/ Test if file ends-with \"-valueclasses.jar\"\n+        int len = (int)strlen(entry->d_name) - (sizeof(VALUECLASS_JAR) - 1);\n+        if (len <= 0 || strcmp(&entry->d_name[len], VALUECLASS_JAR) != 0) {\n+          continue;         \/\/ too short or not the expected suffix\n+        }\n+\n+        strcpy(module_name, entry->d_name);\n+        module_name[len] = '\\0';     \/\/ truncate to just module-name\n+\n+        jio_snprintf(path, JVM_MAXPATHLEN, \"%s%s\", valueclasses_dir, &entry->d_name);\n+        add_patch_mod_prefix(module_name, path, true \/* append *\/, true \/* cds OK*\/);\n+        log_info(class)(\"--enable-preview appending value classes for module %s: %s\", module_name, entry->d_name);\n+      }\n+      FreeHeap(module_name);\n+      FreeHeap(path);\n+      os::closedir(dir);\n+    }\n+    FreeHeap(valueclasses_dir);\n+  }\n+\n+  \/\/ Create numbered properties for each module that has been patched either\n+  \/\/ by --patch-module or --enable-preview\n+  \/\/ Format is \"jdk.module.patch.<n>=<module_name>=<path>\"\n+  if (_patch_mod_prefix != nullptr) {\n+    char * prop_value = AllocateHeap(JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, mtArguments);\n+    unsigned int patch_mod_count = 0;\n+\n+    for (GrowableArrayIterator<ModulePatchPath *> it = _patch_mod_prefix->begin();\n+            it != _patch_mod_prefix->end(); ++it) {\n+      jio_snprintf(prop_value, JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, \"%s=%s\",\n+                   (*it)->module_name(), (*it)->path_string());\n+      if (!create_numbered_module_property(\"jdk.module.patch\", prop_value, patch_mod_count++)) {\n+        FreeHeap(prop_value);\n+        return JNI_ENOMEM;\n+      }\n+    }\n+    FreeHeap(prop_value);\n+  }\n+  return JNI_OK;\n+}\n+\n@@ -2336,0 +2397,4 @@\n+      \/\/ --enable-preview enables Valhalla, EnableValhalla VM option will eventually be removed before integration\n+      if (FLAG_SET_CMDLINE(EnableValhalla, true) != JVMFlag::SUCCESS) {\n+        return JNI_EINVAL;\n+      }\n@@ -2815,10 +2880,5 @@\n-void Arguments::add_patch_mod_prefix(const char* module_name, const char* path) {\n-  \/\/ For java.base check for duplicate --patch-module options being specified on the command line.\n-  \/\/ This check is only required for java.base, all other duplicate module specifications\n-  \/\/ will be checked during module system initialization.  The module system initialization\n-  \/\/ will throw an ExceptionInInitializerError if this situation occurs.\n-  if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n-    if (patch_mod_javabase) {\n-      vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n-    } else {\n-      patch_mod_javabase = true;\n+void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool allow_append, bool allow_cds) {\n+  if (!allow_cds) {\n+    CDSConfig::set_module_patching_disables_cds();\n+    if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+      CDSConfig::set_java_base_module_patching_disables_cds();\n@@ -2833,1 +2893,18 @@\n-  _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  \/\/ Scan patches for matching module\n+  int i = _patch_mod_prefix->find_if([&](ModulePatchPath* patch) {\n+    return (strcmp(module_name, patch->module_name()) == 0);\n+  });\n+  if (i == -1) {\n+    _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  } else {\n+    if (allow_append) {\n+      \/\/ append path to existing module entry\n+      _patch_mod_prefix->at(i)->append_path(path);\n+    } else {\n+      if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+        vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n+      } else {\n+        vm_exit_during_initialization(\"Cannot specify a module more than once to --patch-module\", module_name);\n+      }\n+    }\n+  }\n@@ -2946,1 +3023,2 @@\n-  if (!check_vm_args_consistency()) {\n+  \/\/ finalize --module-patch and related --enable-preview\n+  if (finalize_patch_module() != JNI_OK) {\n@@ -2950,0 +3028,3 @@\n+  if (!check_vm_args_consistency()) {\n+    return JNI_ERR;\n+  }\n@@ -3751,0 +3832,7 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !CDSConfig::is_dumping_archive() && !UseSharedSpaces)) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported.\n+    \/\/ Also these aren't useful in -Xint. However, don't disable them when dumping or using\n+    \/\/ the CDS archive, as the values must match between dumptime and runtime.\n+    InlineTypePassFieldsAsArgs = false;\n+    InlineTypeReturnedAsFields = false;\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":106,"deletions":18,"binary":false,"changes":124,"status":"modified"},{"patch":"@@ -94,0 +94,1 @@\n+  inline void append_path(const char* path) { _path->append_value(path); }\n@@ -476,1 +477,2 @@\n-  static void add_patch_mod_prefix(const char *module_name, const char *path);\n+  static void add_patch_mod_prefix(const char *module_name, const char *path, bool allow_append, bool allow_cds);\n+  static int finalize_patch_module();\n","filename":"src\/hotspot\/share\/runtime\/arguments.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -52,0 +52,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.hpp\"\n@@ -57,0 +59,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -352,2 +355,13 @@\n-  bool save_oop_result = chunk->at(0)->scope()->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n-  Handle return_value;\n+  ScopeDesc* scope = chunk->at(0)->scope();\n+  bool save_oop_result = scope->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n+  \/\/ In case of the return of multiple values, we must take care\n+  \/\/ of all oop return values.\n+  GrowableArray<Handle> return_oops;\n+  InlineKlass* vk = nullptr;\n+  if (save_oop_result && scope->return_scalarized()) {\n+    vk = InlineKlass::returned_inline_klass(map);\n+    if (vk != nullptr) {\n+      vk->save_oop_fields(map, return_oops);\n+      save_oop_result = false;\n+    }\n+  }\n@@ -359,1 +373,1 @@\n-    return_value = Handle(thread, result);\n+    return_oops.push(Handle(thread, result));\n@@ -366,1 +380,1 @@\n-  if (objects != nullptr) {\n+  if (objects != nullptr || vk != nullptr) {\n@@ -371,1 +385,8 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+      if (vk != nullptr) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, CHECK_AND_CLEAR_(true));\n+      }\n+      if (objects != nullptr) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+        bool skip_internal = (compiled_method != nullptr) && !compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal, CHECK_AND_CLEAR_(true));\n+      }\n@@ -376,1 +397,8 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+      if (vk != nullptr) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, THREAD);\n+      }\n+      if (objects != nullptr) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+        bool skip_internal = (compiled_method != nullptr) && !compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal, THREAD);\n+      }\n@@ -379,3 +407,1 @@\n-    bool skip_internal = (compiled_method != nullptr) && !compiled_method->is_compiled_by_jvmci();\n-    Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal);\n-    if (TraceDeoptimization) {\n+    if (TraceDeoptimization && objects != nullptr) {\n@@ -385,1 +411,1 @@\n-  if (save_oop_result) {\n+  if (save_oop_result || vk != nullptr) {\n@@ -387,1 +413,2 @@\n-    deoptee.set_saved_oop_result(&map, return_value());\n+    assert(return_oops.length() == 1, \"no inline type\");\n+    deoptee.set_saved_oop_result(&map, return_oops.pop()());\n@@ -720,1 +747,1 @@\n-  \/\/ If the sender is deoptimized the we must retrieve the address of the handler\n+  \/\/ If the sender is deoptimized we must retrieve the address of the handler\n@@ -1220,2 +1247,11 @@\n-\n-    oop obj = nullptr;\n+    \/\/ Check if the object may be null and has an additional is_init input that needs\n+    \/\/ to be checked before using the field values. Skip re-allocation if it is null.\n+    if (sv->maybe_null()) {\n+      assert(k->is_inline_klass(), \"must be an inline klass\");\n+      jint is_init = StackValue::create_stack_value(fr, reg_map, sv->is_init())->get_jint();\n+      if (is_init == 0) {\n+        continue;\n+      }\n+    }\n+\n+    oop obj = nullptr;\n@@ -1251,0 +1287,4 @@\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* ak = FlatArrayKlass::cast(k);\n+      \/\/ Inline type array must be zeroed because not all memory is reassigned\n+      obj = ak->allocate(sv->field_size(), ak->layout_kind(), THREAD);\n@@ -1282,0 +1322,15 @@\n+\/\/ We're deoptimizing at the return of a call, inline type fields are\n+\/\/ in registers. When we go back to the interpreter, it will expect a\n+\/\/ reference to an inline type instance. Allocate and initialize it from\n+\/\/ the register values here.\n+bool Deoptimization::realloc_inline_type_result(InlineKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS) {\n+  oop new_vt = vk->realloc_result(map, return_oops, THREAD);\n+  if (new_vt == nullptr) {\n+    CLEAR_PENDING_EXCEPTION;\n+    THROW_OOP_(Universe::out_of_memory_error_realloc_objects(), true);\n+  }\n+  return_oops.clear();\n+  return_oops.push(Handle(THREAD, new_vt));\n+  return false;\n+}\n+\n@@ -1447,0 +1502,3 @@\n+  InstanceKlass* _klass;\n+  bool _is_flat;\n+  bool _is_null_free;\n@@ -1448,4 +1506,1 @@\n-  ReassignedField() {\n-    _offset = 0;\n-    _type = T_ILLEGAL;\n-  }\n+  ReassignedField() : _offset(0), _type(T_ILLEGAL), _klass(nullptr), _is_flat(false), _is_null_free(false) { }\n@@ -1460,1 +1515,1 @@\n-static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal) {\n+static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal, int base_offset, GrowableArray<int>* null_marker_offsets, TRAPS) {\n@@ -1469,0 +1524,6 @@\n+        if (fs.is_flat()) {\n+          field._is_flat = true;\n+          field._is_null_free = fs.is_null_free_inline_type();\n+          \/\/ Resolve klass of flat inline type field\n+          field._klass = InlineKlass::cast(klass->get_inline_type_field_klass(fs.index()));\n+        }\n@@ -1475,0 +1536,6 @@\n+  \/\/ Keep track of null marker offset for flat fields\n+  bool set_null_markers = false;\n+  if (null_marker_offsets == nullptr) {\n+    set_null_markers = true;\n+    null_marker_offsets = new GrowableArray<int>();\n+  }\n@@ -1476,0 +1543,15 @@\n+    BasicType type = fields->at(i)._type;\n+    int offset = base_offset + fields->at(i)._offset;\n+    \/\/ Check for flat inline type field before accessing the ScopeValue because it might not have any fields\n+    if (fields->at(i)._is_flat) {\n+      \/\/ Recursively re-assign flat inline type fields\n+      InstanceKlass* vk = fields->at(i)._klass;\n+      assert(vk != nullptr, \"must be resolved\");\n+      offset -= InlineKlass::cast(vk)->payload_offset(); \/\/ Adjust offset to omit oop header\n+      svIndex = reassign_fields_by_klass(vk, fr, reg_map, sv, svIndex, obj, skip_internal, offset, null_marker_offsets, CHECK_0);\n+      if (!fields->at(i)._is_null_free) {\n+        int nm_offset = offset + InlineKlass::cast(vk)->null_marker_offset();\n+        null_marker_offsets->append(nm_offset);\n+      }\n+      continue; \/\/ Continue because we don't need to increment svIndex\n+    }\n@@ -1478,3 +1560,2 @@\n-    int offset = fields->at(i)._offset;\n-    BasicType type = fields->at(i)._type;\n-      case T_OBJECT: case T_ARRAY:\n+      case T_OBJECT:\n+      case T_ARRAY:\n@@ -1552,0 +1633,8 @@\n+  if (set_null_markers) {\n+    \/\/ The null marker values come after all the field values in the debug info\n+    for (int i = 0; i < null_marker_offsets->length(); ++i) {\n+      int offset = null_marker_offsets->at(i);\n+      jbyte is_init = (jbyte)StackValue::create_stack_value(fr, reg_map, sv->field_at(svIndex++))->get_jint();\n+      obj->byte_field_put(offset, is_init);\n+    }\n+  }\n@@ -1555,0 +1644,14 @@\n+\/\/ restore fields of an eliminated inline type array\n+void Deoptimization::reassign_flat_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, flatArrayOop obj, FlatArrayKlass* vak, bool skip_internal, TRAPS) {\n+  InlineKlass* vk = vak->element_klass();\n+  assert(vk->flat_array(), \"should only be used for flat inline type arrays\");\n+  \/\/ Adjust offset to omit oop header\n+  int base_offset = arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT) - InlineKlass::cast(vk)->payload_offset();\n+  \/\/ Initialize all elements of the flat inline type array\n+  for (int i = 0; i < sv->field_size(); i++) {\n+    ScopeValue* val = sv->field_at(i);\n+    int offset = base_offset + (i << Klass::layout_helper_log2_element_size(vak->layout_helper()));\n+    reassign_fields_by_klass(vk, fr, reg_map, val->as_ObjectValue(), 0, (oop)obj, skip_internal, offset, nullptr, CHECK);\n+  }\n+}\n+\n@@ -1556,1 +1659,1 @@\n-void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal) {\n+void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal, TRAPS) {\n@@ -1562,1 +1665,1 @@\n-    assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n+    assert(obj.not_null() || realloc_failures || sv->maybe_null(), \"reallocation was missed\");\n@@ -1600,1 +1703,4 @@\n-      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal);\n+      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal, 0, nullptr, CHECK);\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+      reassign_flat_array_elements(fr, reg_map, sv, (flatArrayOop) obj(), vak, skip_internal, CHECK);\n@@ -1790,1 +1896,1 @@\n-  \/\/ Deoptimize only if the frame comes from compile code.\n+  \/\/ Deoptimize only if the frame comes from compiled code.\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":132,"deletions":26,"binary":false,"changes":158,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -46,1 +47,2 @@\n-  return is_final() && (is_static() || ik->is_hidden() || ik->is_record());\n+  return is_final() && (is_static() || ik->is_hidden() || ik->is_record() || ik->is_inline_klass()\n+                        || (ik->is_abstract() && !ik->is_identity_class() && !ik->is_interface()));\n@@ -102,1 +104,1 @@\n-void fieldDescriptor::print_on(outputStream* st) const {\n+void fieldDescriptor::print_on(outputStream* st, int base_offset) const {\n@@ -108,1 +110,1 @@\n-  st->print(\" @%d \", offset());\n+  st->print(\" @%d \", offset() + base_offset);\n@@ -126,4 +128,1 @@\n-void fieldDescriptor::print_on_for(outputStream* st, oop obj) {\n-  print_on(st);\n-  st->print(\" \");\n-\n+void fieldDescriptor::print_on_for(outputStream* st, oop obj, int indent, int base_offset) {\n@@ -131,0 +130,3 @@\n+  print_on(st, base_offset);\n+  st->print(\" \");\n+  jint as_int = 0;\n@@ -160,6 +162,27 @@\n-      if (obj->obj_field(offset()) != nullptr) {\n-        obj->obj_field(offset())->print_value_on(st);\n-      } else {\n-        st->print(\"null\");\n-      }\n-      break;\n+      if (is_flat()) { \/\/ only some inline types can be flat\n+        InlineKlass* vk = InlineKlass::cast(field_holder()->get_inline_type_field_klass(index()));\n+        st->print(\"Flat inline type field '%s':\", vk->name()->as_C_string());\n+        if (!is_null_free_inline_type()) {\n+          assert(has_null_marker(), \"should have null marker\");\n+          InlineLayoutInfo* li = field_holder()->inline_layout_info_adr(index());\n+          int nm_offset = li->null_marker_offset();\n+          if (obj->byte_field_acquire(nm_offset) == 0) {\n+            st->print(\" null\");\n+            return;\n+          }\n+        }\n+        st->cr();\n+        \/\/ Print fields of flat field (recursively)\n+        int field_offset = offset() - vk->payload_offset();\n+        obj = cast_to_oop(cast_from_oop<address>(obj) + field_offset);\n+        FieldPrinter print_field(st, obj, indent + 1, base_offset + field_offset );\n+        vk->do_nonstatic_fields(&print_field);\n+        if (this->field_flags().has_null_marker()) {\n+          for (int i = 0; i < indent + 1; i++) st->print(\"  \");\n+          st->print_cr(\" - [null_marker] @%d %s\",\n+                    vk->null_marker_offset() - base_offset + field_offset,\n+                    obj->bool_field(vk->null_marker_offset()) ? \"Field marked as non-null\" : \"Field marked as null\");\n+        }\n+        return; \/\/ Do not print underlying representation\n+      }\n+      \/\/ Not flat inline type field, fall through\n","filename":"src\/hotspot\/share\/runtime\/fieldDescriptor.cpp","additions":36,"deletions":13,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -62,0 +63,3 @@\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#endif\n@@ -361,0 +365,19 @@\n+\n+#ifdef COMPILER1\n+  if (nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n+      pc() < nm->verified_inline_entry_point()) {\n+    \/\/ The VEP and VIEP(RO) of C1-compiled methods call into the runtime to buffer scalarized value\n+    \/\/ type args. We can't deoptimize at that point because the buffers have not yet been initialized.\n+    \/\/ Also, if the method is synchronized, we first need to acquire the lock.\n+    \/\/ Don't patch the return pc to delay deoptimization until we enter the method body (the check\n+    \/\/ added in LIRGenerator::do_Base will detect the pending deoptimization by checking the original_pc).\n+#if defined ASSERT && !defined AARCH64   \/\/ Stub call site does not look like NativeCall on AArch64\n+    NativeCall* call = nativeCall_before(this->pc());\n+    address dest = call->destination();\n+    assert(dest == Runtime1::entry_for(C1StubId::buffer_inline_args_no_receiver_id) ||\n+           dest == Runtime1::entry_for(C1StubId::buffer_inline_args_id), \"unexpected safepoint in entry point\");\n+#endif\n+    return;\n+  }\n+#endif\n+\n@@ -759,1 +782,1 @@\n-                          OopClosure* f) {\n+                          OopClosure* f, BufferedValueClosure* bvt_f) {\n@@ -771,1 +794,3 @@\n-      _f->do_oop(addr);\n+      if (_f != nullptr) {\n+        _f->do_oop(addr);\n+      }\n@@ -783,1 +808,3 @@\n-        _f->do_oop(addr);\n+        if (_f != nullptr) {\n+          _f->do_oop(addr);\n+        }\n@@ -958,1 +985,1 @@\n-  InterpreterFrameClosure blk(this, max_locals, m->max_stack(), f);\n+  InterpreterFrameClosure blk(this, max_locals, m->max_stack(), f, nullptr);\n@@ -970,0 +997,17 @@\n+void frame::buffered_values_interpreted_do(BufferedValueClosure* f) {\n+  assert(is_interpreted_frame(), \"Not an interpreted frame\");\n+  Thread *thread = Thread::current();\n+  methodHandle m (thread, interpreter_frame_method());\n+  jint      bci = interpreter_frame_bci();\n+\n+  assert(m->is_method(), \"checking frame value\");\n+  assert(!m->is_native() && bci >= 0 && bci < m->code_size(),\n+         \"invalid bci value\");\n+\n+  InterpreterFrameClosure blk(this, m->max_locals(), m->max_stack(), nullptr, f);\n+\n+  \/\/ process locals & expression stack\n+  InterpreterOopMap mask;\n+  m->mask_for(bci, &mask);\n+  mask.iterate_oop(&blk);\n+}\n@@ -1022,0 +1066,1 @@\n+    assert(_offset < _arg_size, \"out of bounds\");\n@@ -1048,5 +1093,1 @@\n-    _arg_size  = ArgumentSizeComputer(signature).size() + (has_receiver ? 1 : 0) + (has_appendix ? 1 : 0);\n-\n-    int arg_size;\n-    _regs = SharedRuntime::find_callee_arguments(signature, has_receiver, has_appendix, &arg_size);\n-    assert(arg_size == _arg_size, \"wrong arg size\");\n+    _regs = SharedRuntime::find_callee_arguments(signature, has_receiver, has_appendix, &_arg_size);\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":50,"deletions":9,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+#include \"oops\/inlineKlass.hpp\"\n+#include \"runtime\/atomic.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/handles.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -128,0 +128,1 @@\n+  VMRegImpl::set_regName();       \/\/ need this before generate_stubs (for printing oop maps).\n@@ -155,1 +156,0 @@\n-  VMRegImpl::set_regName();  \/\/ need this before generate_stubs (for printing oop maps).\n","filename":"src\/hotspot\/share\/runtime\/init.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -143,4 +144,4 @@\n-    case T_BOOLEAN: \/\/ fall through\n-    case T_CHAR   : \/\/ fall through\n-    case T_SHORT  : \/\/ fall through\n-    case T_INT    : \/\/ fall through\n+    case T_BOOLEAN  : \/\/ fall through\n+    case T_CHAR     : \/\/ fall through\n+    case T_SHORT    : \/\/ fall through\n+    case T_INT      : \/\/ fall through\n@@ -148,2 +149,3 @@\n-    case T_OBJECT : \/\/ fall through\n-    case T_ARRAY  : \/\/ fall through\n+    case T_OBJECT   : \/\/ fall through\n+    case T_ARRAY    : \/\/ fall through\n+    case T_FLAT_ELEMENT: \/\/ fall through\n@@ -151,5 +153,5 @@\n-    case T_BYTE   : \/\/ fall through\n-    case T_VOID   : return T_INT;\n-    case T_LONG   : return T_LONG;\n-    case T_FLOAT  : return T_FLOAT;\n-    case T_DOUBLE : return T_DOUBLE;\n+    case T_BYTE     : \/\/ fall through\n+    case T_VOID     : return T_INT;\n+    case T_LONG     : return T_LONG;\n+    case T_FLOAT    : return T_FLOAT;\n+    case T_DOUBLE   : return T_DOUBLE;\n@@ -157,2 +159,2 @@\n-    case T_ARRAY  : \/\/ fall through\n-    case T_OBJECT:  return T_OBJECT;\n+    case T_ARRAY    : \/\/ fall through\n+    case T_OBJECT   : return T_OBJECT;\n@@ -379,0 +381,12 @@\n+  jobject value_buffer = nullptr;\n+  if (InlineTypeReturnedAsFields && (result->get_type() == T_OBJECT)) {\n+    \/\/ Pre allocate a buffered inline type in case the result is returned\n+    \/\/ flattened by compiled code\n+    InlineKlass* vk = method->returns_inline_type(thread);\n+    if (vk != nullptr && vk->can_be_returned_as_fields()) {\n+      oop instance = vk->allocate_instance(CHECK);\n+      value_buffer = JNIHandles::make_local(thread, instance);\n+      result->set_jobject(value_buffer);\n+    }\n+  }\n+\n@@ -443,0 +457,1 @@\n+    JNIHandles::destroy_local(value_buffer);\n@@ -591,1 +606,1 @@\n-  if (is_reference_type(return_type)) return_type = T_OBJECT;\n+  if (return_type == T_ARRAY) return_type = T_OBJECT;\n","filename":"src\/hotspot\/share\/runtime\/javaCalls.cpp","additions":29,"deletions":14,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"classfile\/vmSymbols.hpp\"\n@@ -34,0 +35,1 @@\n+#include \"runtime\/javaCalls.hpp\"\n@@ -285,0 +287,38 @@\n+bool JNIHandles::is_same_object(jobject handle1, jobject handle2) {\n+  oop obj1 = resolve_no_keepalive(handle1);\n+  oop obj2 = resolve_no_keepalive(handle2);\n+\n+  bool ret = obj1 == obj2;\n+\n+  if (EnableValhalla) {\n+    if (!ret && obj1 != nullptr && obj2 != nullptr && obj1->klass() == obj2->klass() && obj1->klass()->is_inline_klass()) {\n+      \/\/ The two references are different, they are not null and they are both inline types,\n+      \/\/ a full substitutability test is required, calling ValueObjectMethods.isSubstitutable()\n+      \/\/ (similarly to InterpreterRuntime::is_substitutable)\n+      JavaThread* THREAD = JavaThread::current();\n+      Handle ha(THREAD, obj1);\n+      Handle hb(THREAD, obj2);\n+      JavaValue result(T_BOOLEAN);\n+      JavaCallArguments args;\n+      args.push_oop(ha);\n+      args.push_oop(hb);\n+      methodHandle method(THREAD, Universe::is_substitutable_method());\n+      JavaCalls::call(&result, method, &args, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        \/\/ Something really bad happened because isSubstitutable() should not throw exceptions\n+        \/\/ If it is an error, just let it propagate\n+        \/\/ If it is an exception, wrap it into an InternalError\n+        if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in substitutability test\", e, false);\n+        }\n+      }\n+      ret = result.get_jboolean();\n+    }\n+  }\n+\n+  return ret;\n+}\n+\n+\n","filename":"src\/hotspot\/share\/runtime\/jniHandles.cpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -54,0 +55,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -273,0 +275,1 @@\n+\n@@ -276,0 +279,4 @@\n+      if (a->is_null_free_array() && obj == nullptr) {\n+         THROW_MSG(vmSymbols::java_lang_NullPointerException(), \"null-restricted array\");\n+      }\n+\n@@ -750,3 +757,0 @@\n-  if (log_is_enabled(Debug, class, resolve)) {\n-    trace_class_resolution(nt);\n-  }\n@@ -758,3 +762,2 @@\n-  assert(!method()->is_object_initializer() &&\n-         (for_constant_pool_access || !method()->is_static_initializer()),\n-         \"Should not be the initializer\");\n+  assert(!method()->name()->starts_with('<') || for_constant_pool_access,\n+         \"should call new_constructor instead\");\n@@ -808,1 +811,2 @@\n-  assert(method()->is_object_initializer(), \"Should be the initializer\");\n+  assert(method()->is_object_constructor(),\n+         \"should call new_method instead\");\n@@ -857,0 +861,2 @@\n+\n+  int flags = 0;\n@@ -858,1 +864,4 @@\n-    java_lang_reflect_Field::set_trusted_final(rh());\n+    flags |= TRUSTED_FINAL;\n+  }\n+  if (fd->is_null_free_inline_type()) {\n+    flags |= NULL_RESTRICTED;\n@@ -860,0 +869,2 @@\n+  java_lang_reflect_Field::set_flags(rh(), flags);\n+\n@@ -977,1 +988,2 @@\n-    if (reflected_method->is_private() || reflected_method->name() == vmSymbols::object_initializer_name()) {\n+    if (reflected_method->is_private() ||\n+        reflected_method->name() == vmSymbols::object_initializer_name()) {\n@@ -1159,1 +1171,0 @@\n-  assert(method->name() == vmSymbols::object_initializer_name(), \"invalid constructor\");\n","filename":"src\/hotspot\/share\/runtime\/reflection.cpp","additions":21,"deletions":10,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -788,0 +789,1 @@\n+    ResourceMark rm;\n@@ -789,1 +791,2 @@\n-    bool return_oop = nm->method()->is_returning_oop();\n+    Method* method = nm->method();\n+    bool return_oop = method->is_returning_oop();\n@@ -791,1 +794,16 @@\n-    Handle return_value;\n+    GrowableArray<Handle> return_values;\n+    InlineKlass* vk = nullptr;\n+    if (return_oop && InlineTypeReturnedAsFields &&\n+        (method->result_type() == T_OBJECT)) {\n+      \/\/ Check if an inline type is returned as fields\n+      vk = InlineKlass::returned_inline_klass(map);\n+      if (vk != nullptr) {\n+        \/\/ We're at a safepoint at the return of a method that returns\n+        \/\/ multiple values. We must make sure we preserve the oop values\n+        \/\/ across the safepoint.\n+        assert(vk == method->returns_inline_type(thread()), \"bad inline klass\");\n+        vk->save_oop_fields(map, return_values);\n+        return_oop = false;\n+      }\n+    }\n+\n@@ -798,1 +816,1 @@\n-      return_value = Handle(self, result);\n+      return_values.push(Handle(self, result));\n@@ -812,1 +830,4 @@\n-      caller_fr.set_saved_oop_result(&map, return_value());\n+      assert(return_values.length() == 1, \"only one return value\");\n+      caller_fr.set_saved_oop_result(&map, return_values.pop()());\n+    } else if (vk != nullptr) {\n+      vk->restore_oop_results(map, return_values);\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":25,"deletions":4,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+#include \"memory\/oopFactory.hpp\"\n@@ -48,0 +49,2 @@\n+#include \"oops\/access.hpp\"\n+#include \"oops\/fieldStreams.inline.hpp\"\n@@ -52,0 +55,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -53,0 +57,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -1189,0 +1194,15 @@\n+  \/\/ Substitutability test implementation piggy backs on static call resolution\n+  Bytecodes::Code code = caller->java_code_at(bci);\n+  if (code == Bytecodes::_if_acmpeq || code == Bytecodes::_if_acmpne) {\n+    bc = Bytecodes::_invokestatic;\n+    methodHandle attached_method(THREAD, extract_attached_method(vfst));\n+    assert(attached_method.not_null(), \"must have attached method\");\n+    vmClasses::ValueObjectMethods_klass()->initialize(CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, false, CHECK_NH);\n+#ifdef ASSERT\n+    Method* is_subst = vmClasses::ValueObjectMethods_klass()->find_method(vmSymbols::isSubstitutable_name(), vmSymbols::object_object_boolean_signature());\n+    assert(callinfo.selected_method() == is_subst, \"must be isSubstitutable method\");\n+#endif\n+    return receiver;\n+  }\n+\n@@ -1224,0 +1244,6 @@\n+    } else {\n+      assert(attached_method->has_scalarized_args(), \"invalid use of attached method\");\n+      if (!attached_method->method_holder()->is_inline_klass()) {\n+        \/\/ Ignore the attached method in this case to not confuse below code\n+        attached_method = methodHandle(current, nullptr);\n+      }\n@@ -1232,0 +1258,1 @@\n+  bool check_null_and_abstract = true;\n@@ -1245,2 +1272,3 @@\n-    if (attached_method.is_null()) {\n-      Method* callee = bytecode.static_target(CHECK_NH);\n+    Method* callee = attached_method();\n+    if (callee == nullptr) {\n+      callee = bytecode.static_target(CHECK_NH);\n@@ -1251,7 +1279,17 @@\n-\n-    \/\/ Retrieve from a compiled argument list\n-    receiver = Handle(current, callerFrame.retrieve_receiver(&reg_map2));\n-    assert(oopDesc::is_oop_or_null(receiver()), \"\");\n-\n-    if (receiver.is_null()) {\n-      THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+    bool caller_is_c1 = callerFrame.is_compiled_frame() && callerFrame.cb()->as_nmethod()->is_compiled_by_c1();\n+    if (!caller_is_c1 && callee->is_scalarized_arg(0)) {\n+      \/\/ If the receiver is an inline type that is passed as fields, no oop is available\n+      \/\/ Resolve the call without receiver null checking.\n+      assert(!callee->mismatch(), \"calls with inline type receivers should never mismatch\");\n+      assert(attached_method.not_null() && !attached_method->is_abstract(), \"must have non-abstract attached method\");\n+      if (bc == Bytecodes::_invokeinterface) {\n+        bc = Bytecodes::_invokevirtual; \/\/ C2 optimistically replaces interface calls by virtual calls\n+      }\n+      check_null_and_abstract = false;\n+    } else {\n+      \/\/ Retrieve from a compiled argument list\n+      receiver = Handle(current, callerFrame.retrieve_receiver(&reg_map2));\n+      assert(oopDesc::is_oop_or_null(receiver()), \"\");\n+      if (receiver.is_null()) {\n+        THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);\n+      }\n@@ -1264,1 +1302,1 @@\n-    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, CHECK_NH);\n+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, check_null_and_abstract, CHECK_NH);\n@@ -1273,1 +1311,1 @@\n-  if (has_receiver) {\n+  if (has_receiver && check_null_and_abstract) {\n@@ -1301,1 +1339,1 @@\n-methodHandle SharedRuntime::find_callee_method(TRAPS) {\n+methodHandle SharedRuntime::find_callee_method(bool is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1327,0 +1365,4 @@\n+    \/\/ Calls via mismatching methods are always non-scalarized\n+    if (callinfo.resolved_method()->mismatch() && !is_optimized) {\n+      caller_is_c1 = true;\n+    }\n@@ -1334,1 +1376,1 @@\n-methodHandle SharedRuntime::resolve_helper(bool is_virtual, bool is_optimized, TRAPS) {\n+methodHandle SharedRuntime::resolve_helper(bool is_virtual, bool is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1357,0 +1399,4 @@\n+  \/\/ Calls via mismatching methods are always non-scalarized\n+  if (caller_nm->is_compiled_by_c1() || (call_info.resolved_method()->mismatch() && !is_optimized)) {\n+    caller_is_c1 = true;\n+  }\n@@ -1375,1 +1421,1 @@\n-    tty->print(\"resolving %s%s (%s) call to\",\n+    tty->print(\"resolving %s%s (%s) call%s to\",\n@@ -1377,1 +1423,1 @@\n-               Bytecodes::name(invoke_code));\n+               Bytecodes::name(invoke_code), (caller_is_c1) ? \" from C1\" : \"\");\n@@ -1416,1 +1462,1 @@\n-    inline_cache->update(&call_info, receiver->klass());\n+    inline_cache->update(&call_info, receiver->klass(), caller_is_c1);\n@@ -1420,1 +1466,1 @@\n-    callsite->set(callee_method);\n+    callsite->set(callee_method, caller_is_c1);\n@@ -1440,0 +1486,2 @@\n+  bool is_optimized = false;\n+  bool caller_is_c1 = false;\n@@ -1441,1 +1489,1 @@\n-    callee_method = SharedRuntime::handle_ic_miss_helper(CHECK_NULL);\n+    callee_method = SharedRuntime::handle_ic_miss_helper(is_optimized, caller_is_c1, CHECK_NULL);\n@@ -1446,1 +1494,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, is_optimized, caller_is_c1);\n@@ -1487,1 +1535,5 @@\n-      return callee->get_c2i_entry();\n+      if (caller_frame.is_interpreted_frame()) {\n+        return callee->get_c2i_inline_entry();\n+      } else {\n+        return callee->get_c2i_entry();\n+      }\n@@ -1493,0 +1545,3 @@\n+  bool is_static_call = false;\n+  bool is_optimized = false;\n+  bool caller_is_c1 = false;\n@@ -1495,1 +1550,1 @@\n-    callee_method = SharedRuntime::reresolve_call_site(CHECK_NULL);\n+    callee_method = SharedRuntime::reresolve_call_site(is_static_call, is_optimized, caller_is_c1, CHECK_NULL);\n@@ -1499,1 +1554,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, is_static_call, is_optimized, caller_is_c1);\n@@ -1538,1 +1593,2 @@\n-address SharedRuntime::get_resolved_entry(JavaThread* current, methodHandle callee_method) {\n+address SharedRuntime::get_resolved_entry(JavaThread* current, methodHandle callee_method,\n+                                          bool is_static_call, bool is_optimized, bool caller_is_c1) {\n@@ -1544,2 +1600,11 @@\n-  assert(callee_method->verified_code_entry() != nullptr, \" Jump to zero!\");\n-  return callee_method->verified_code_entry();\n+\n+  if (caller_is_c1) {\n+    assert(callee_method->verified_inline_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_inline_code_entry();\n+  } else if (is_static_call || is_optimized) {\n+    assert(callee_method->verified_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_code_entry();\n+  } else {\n+    assert(callee_method->verified_inline_ro_code_entry() != nullptr, \"Jump to zero!\");\n+    return callee_method->verified_inline_ro_code_entry();\n+  }\n@@ -1551,0 +1616,1 @@\n+  bool caller_is_c1 = false;\n@@ -1553,1 +1619,1 @@\n-    callee_method = SharedRuntime::resolve_helper(false, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(false, false, caller_is_c1, CHECK_NULL);\n@@ -1557,1 +1623,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, true, false, caller_is_c1);\n@@ -1563,0 +1629,1 @@\n+  bool caller_is_c1 = false;\n@@ -1564,1 +1631,1 @@\n-    callee_method = SharedRuntime::resolve_helper(true, false, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(true, false, caller_is_c1, CHECK_NULL);\n@@ -1568,1 +1635,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, false, caller_is_c1);\n@@ -1576,0 +1643,1 @@\n+  bool caller_is_c1 = false;\n@@ -1577,1 +1645,1 @@\n-    callee_method = SharedRuntime::resolve_helper(true, true, CHECK_NULL);\n+    callee_method = SharedRuntime::resolve_helper(true, true, caller_is_c1, CHECK_NULL);\n@@ -1581,1 +1649,1 @@\n-  return get_resolved_entry(current, callee_method);\n+  return get_resolved_entry(current, callee_method, false, true, caller_is_c1);\n@@ -1584,1 +1652,3 @@\n-methodHandle SharedRuntime::handle_ic_miss_helper(TRAPS) {\n+\n+\n+methodHandle SharedRuntime::handle_ic_miss_helper(bool& is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1602,1 +1672,1 @@\n-    tty->print(\"IC miss (%s) call to\", Bytecodes::name(bc));\n+    tty->print(\"IC miss (%s) call%s to\", Bytecodes::name(bc), (caller_is_c1) ? \" from C1\" : \"\");\n@@ -1634,0 +1704,4 @@\n+  \/\/ Calls via mismatching methods are always non-scalarized\n+  if (caller_nm->is_compiled_by_c1() || call_info.resolved_method()->mismatch()) {\n+    caller_is_c1 = true;\n+  }\n@@ -1637,1 +1711,1 @@\n-  inline_cache->update(&call_info, receiver()->klass());\n+  inline_cache->update(&call_info, receiver()->klass(), caller_is_c1);\n@@ -1648,1 +1722,1 @@\n-methodHandle SharedRuntime::reresolve_call_site(TRAPS) {\n+methodHandle SharedRuntime::reresolve_call_site(bool& is_static_call, bool& is_optimized, bool& caller_is_c1, TRAPS) {\n@@ -1658,0 +1732,3 @@\n+  if (caller.is_compiled_frame()) {\n+    caller_is_c1 = caller.cb()->as_nmethod()->is_compiled_by_c1();\n+  }\n@@ -1698,0 +1775,2 @@\n+        is_static_call = false;\n+        is_optimized = false;\n@@ -1700,0 +1779,1 @@\n+            is_static_call = true;\n@@ -1701,0 +1781,1 @@\n+            is_optimized = (iter.type() == relocInfo::opt_virtual_call_type);\n@@ -1705,1 +1786,0 @@\n-\n@@ -1719,2 +1799,1 @@\n-  methodHandle callee_method = find_callee_method(CHECK_(methodHandle()));\n-\n+  methodHandle callee_method = find_callee_method(is_optimized, caller_is_c1, CHECK_(methodHandle()));\n@@ -1727,1 +1806,1 @@\n-    tty->print(\"handle_wrong_method reresolving call to\");\n+    tty->print(\"handle_wrong_method reresolving call%s to\", (caller_is_c1) ? \" from C1\" : \"\");\n@@ -1933,0 +2012,15 @@\n+char* SharedRuntime::generate_identity_exception_message(JavaThread* current, Klass* klass) {\n+  assert(klass->is_inline_klass(), \"Must be a concrete value class\");\n+  const char* desc = \"Cannot synchronize on an instance of value class \";\n+  const char* className = klass->external_name();\n+  size_t msglen = strlen(desc) + strlen(className) + 1;\n+  char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+  if (nullptr == message) {\n+    \/\/ Out of memory: can't create detailed error message\n+    message = const_cast<char*>(klass->external_name());\n+  } else {\n+    jio_snprintf(message, msglen, \"%s%s\", desc, className);\n+  }\n+  return message;\n+}\n+\n@@ -2183,1 +2277,1 @@\n-    _basic_type_bits = 4,\n+    _basic_type_bits = 5,\n@@ -2201,1 +2295,1 @@\n-  static int adapter_encoding(BasicType in) {\n+  static BasicType adapter_encoding(BasicType in) {\n@@ -2207,1 +2301,1 @@\n-        \/\/ There are all promoted to T_INT in the calling convention\n+        \/\/ They are all promoted to T_INT in the calling convention\n@@ -2234,1 +2328,1 @@\n-  AdapterFingerPrint(int total_args_passed, BasicType* sig_bt) {\n+  AdapterFingerPrint(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false) {\n@@ -2237,0 +2331,1 @@\n+    int total_args_passed = (sig != nullptr) ? sig->length() : 0;\n@@ -2254,0 +2349,2 @@\n+    BasicType prev_bt = T_ILLEGAL;\n+    int vt_count = 0;\n@@ -2256,4 +2353,27 @@\n-      for (int byte = 0; sig_index < total_args_passed && byte < _basic_types_per_int; byte++) {\n-        int bt = adapter_encoding(sig_bt[sig_index++]);\n-        assert((bt & _basic_type_mask) == bt, \"must fit in 4 bits\");\n-        value = (value << _basic_type_bits) | bt;\n+      for (int byte = 0; byte < _basic_types_per_int; byte++) {\n+        BasicType bt = T_ILLEGAL;\n+        if (sig_index < total_args_passed) {\n+          bt = sig->at(sig_index++)._bt;\n+          if (bt == T_METADATA) {\n+            \/\/ Found start of inline type in signature\n+            assert(InlineTypePassFieldsAsArgs, \"unexpected start of inline type\");\n+            if (sig_index == 1 && has_ro_adapter) {\n+              \/\/ With a ro_adapter, replace receiver inline type delimiter by T_VOID to prevent matching\n+              \/\/ with other adapters that have the same inline type as first argument and no receiver.\n+              bt = T_VOID;\n+            }\n+            vt_count++;\n+          } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+            \/\/ Found end of inline type in signature\n+            assert(InlineTypePassFieldsAsArgs, \"unexpected end of inline type\");\n+            vt_count--;\n+            assert(vt_count >= 0, \"invalid vt_count\");\n+          } else if (vt_count == 0) {\n+            \/\/ Widen fields that are not part of a scalarized inline type argument\n+            bt = adapter_encoding(bt);\n+          }\n+          prev_bt = bt;\n+        }\n+        int bt_val = (bt == T_ILLEGAL) ? 0 : bt;\n+        assert((bt_val & _basic_type_mask) == bt_val, \"must fit in 4 bits\");\n+        value = (value << _basic_type_bits) | bt_val;\n@@ -2263,0 +2383,1 @@\n+    assert(vt_count == 0, \"invalid vt_count\");\n@@ -2327,8 +2448,4 @@\n-        }\n-        switch (v) {\n-          case T_INT:    st.print(\"I\");    break;\n-          case T_LONG:   long_prev = true; break;\n-          case T_FLOAT:  st.print(\"F\");    break;\n-          case T_DOUBLE: st.print(\"D\");    break;\n-          case T_VOID:   break;\n-          default: ShouldNotReachHere();\n+        } else if (v == T_LONG) {\n+          long_prev = true;\n+        } else if (v != T_VOID){\n+          st.print(\"%c\", type2char((BasicType)v));\n@@ -2382,1 +2499,1 @@\n-static AdapterHandlerEntry* lookup(int total_args_passed, BasicType* sig_bt) {\n+static AdapterHandlerEntry* lookup(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false) {\n@@ -2385,1 +2502,1 @@\n-  AdapterFingerPrint fp(total_args_passed, sig_bt);\n+  AdapterFingerPrint fp(sig, has_ro_adapter);\n@@ -2419,1 +2536,1 @@\n-const int AdapterHandlerLibrary_size = 16*K;\n+const int AdapterHandlerLibrary_size = 48*K;\n@@ -2462,1 +2579,1 @@\n-    _abstract_method_handler = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(0, nullptr),\n+    _abstract_method_handler = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(nullptr),\n@@ -2464,0 +2581,1 @@\n+                                                                wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,\n@@ -2465,4 +2583,8 @@\n-\n-    _no_arg_handler = create_adapter(no_arg_blob, 0, nullptr, true);\n-    BasicType obj_args[] = { T_OBJECT };\n-    _obj_arg_handler = create_adapter(obj_arg_blob, 1, obj_args, true);\n+    CompiledEntrySignature no_args;\n+    no_args.compute_calling_conventions();\n+    _no_arg_handler = create_adapter(no_arg_blob, no_args, true);\n+\n+    CompiledEntrySignature obj_args;\n+    SigEntry::add_entry(obj_args.sig(), T_OBJECT);\n+    obj_args.compute_calling_conventions();\n+    _obj_arg_handler = create_adapter(obj_arg_blob, obj_args, true);\n@@ -2472,2 +2594,4 @@\n-    BasicType int_args[] = { T_INT };\n-    _int_arg_handler = create_adapter(int_arg_blob, 1, int_args, true);\n+    CompiledEntrySignature int_args;\n+    SigEntry::add_entry(int_args.sig(), T_INT);\n+    int_args.compute_calling_conventions();\n+    _int_arg_handler = create_adapter(int_arg_blob, int_args, true);\n@@ -2475,2 +2599,5 @@\n-    BasicType obj_int_args[] = { T_OBJECT, T_INT };\n-    _obj_int_arg_handler = create_adapter(obj_int_arg_blob, 2, obj_int_args, true);\n+    CompiledEntrySignature obj_int_args;\n+    SigEntry::add_entry(obj_int_args.sig(), T_OBJECT);\n+    SigEntry::add_entry(obj_int_args.sig(), T_INT);\n+    obj_int_args.compute_calling_conventions();\n+    _obj_int_arg_handler = create_adapter(obj_int_arg_blob, obj_int_args, true);\n@@ -2478,2 +2605,5 @@\n-    BasicType obj_obj_args[] = { T_OBJECT, T_OBJECT };\n-    _obj_obj_arg_handler = create_adapter(obj_obj_arg_blob, 2, obj_obj_args, true);\n+    CompiledEntrySignature obj_obj_args;\n+    SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+    SigEntry::add_entry(obj_obj_args.sig(), T_OBJECT);\n+    obj_obj_args.compute_calling_conventions();\n+    _obj_obj_arg_handler = create_adapter(obj_obj_arg_blob, obj_obj_args, true);\n@@ -2487,0 +2617,1 @@\n+  return;\n@@ -2499,0 +2630,2 @@\n+                                                      address c2i_inline_entry,\n+                                                      address c2i_inline_ro_entry,\n@@ -2500,0 +2633,1 @@\n+                                                      address c2i_unverified_inline_entry,\n@@ -2501,3 +2635,2 @@\n-  \/\/ Insert an entry into the table\n-  return new AdapterHandlerEntry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry,\n-                                 c2i_no_clinit_check_entry);\n+  return new AdapterHandlerEntry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry,\n+                              c2i_unverified_inline_entry, c2i_no_clinit_check_entry);\n@@ -2508,1 +2641,1 @@\n-    return _abstract_method_handler;\n+    return nullptr;\n@@ -2515,0 +2648,3 @@\n+      if (InlineTypePassFieldsAsArgs && method->method_holder()->is_inline_klass()) {\n+        return nullptr;\n+      }\n@@ -2518,1 +2654,10 @@\n-      case JVM_SIGNATURE_CLASS:\n+      case JVM_SIGNATURE_CLASS: {\n+        if (InlineTypePassFieldsAsArgs) {\n+          SignatureStream ss(method->signature());\n+          InlineKlass* vk = ss.as_inline_klass(method->method_holder());\n+          if (vk != nullptr) {\n+            return nullptr;\n+          }\n+        }\n+        return _obj_arg_handler;\n+      }\n@@ -2529,1 +2674,1 @@\n-             !method->is_static()) {\n+             !method->is_static() && (!InlineTypePassFieldsAsArgs || !method->method_holder()->is_inline_klass())) {\n@@ -2531,1 +2676,10 @@\n-      case JVM_SIGNATURE_CLASS:\n+      case JVM_SIGNATURE_CLASS: {\n+        if (InlineTypePassFieldsAsArgs) {\n+          SignatureStream ss(method->signature());\n+          InlineKlass* vk = ss.as_inline_klass(method->method_holder());\n+          if (vk != nullptr) {\n+            return nullptr;\n+          }\n+        }\n+        return _obj_obj_arg_handler;\n+      }\n@@ -2545,5 +2699,9 @@\n-class AdapterSignatureIterator : public SignatureIterator {\n- private:\n-  BasicType stack_sig_bt[16];\n-  BasicType* sig_bt;\n-  int index;\n+CompiledEntrySignature::CompiledEntrySignature(Method* method) :\n+  _method(method), _num_inline_args(0), _has_inline_recv(false),\n+  _regs(nullptr), _regs_cc(nullptr), _regs_cc_ro(nullptr),\n+  _args_on_stack(0), _args_on_stack_cc(0), _args_on_stack_cc_ro(0),\n+  _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _supers(nullptr) {\n+  _sig = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+  _sig_cc = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+  _sig_cc_ro = new GrowableArray<SigEntry>((method != nullptr) ? method->size_of_parameters() : 1);\n+}\n@@ -2551,11 +2709,24 @@\n- public:\n-  AdapterSignatureIterator(Symbol* signature,\n-                           fingerprint_t fingerprint,\n-                           bool is_static,\n-                           int total_args_passed) :\n-    SignatureIterator(signature, fingerprint),\n-    index(0)\n-  {\n-    sig_bt = (total_args_passed <= 16) ? stack_sig_bt : NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n-    if (!is_static) { \/\/ Pass in receiver first\n-      sig_bt[index++] = T_OBJECT;\n+\/\/ See if we can save space by sharing the same entry for VIEP and VIEP(RO),\n+\/\/ or the same entry for VEP and VIEP(RO).\n+CodeOffsets::Entries CompiledEntrySignature::c1_inline_ro_entry_type() const {\n+  if (!has_scalarized_args()) {\n+    \/\/ VEP\/VIEP\/VIEP(RO) all share the same entry. There's no packing.\n+    return CodeOffsets::Verified_Entry;\n+  }\n+  if (_method->is_static()) {\n+    \/\/ Static methods don't need VIEP(RO)\n+    return CodeOffsets::Verified_Entry;\n+  }\n+\n+  if (has_inline_recv()) {\n+    if (num_inline_args() == 1) {\n+      \/\/ Share same entry for VIEP and VIEP(RO).\n+      \/\/ This is quite common: we have an instance method in an InlineKlass that has\n+      \/\/ no inline type args other than <this>.\n+      return CodeOffsets::Verified_Inline_Entry;\n+    } else {\n+      assert(num_inline_args() > 1, \"must be\");\n+      \/\/ No sharing:\n+      \/\/   VIEP(RO) -- <this> is passed as object\n+      \/\/   VEP      -- <this> is passed as fields\n+      return CodeOffsets::Verified_Inline_Entry_RO;\n@@ -2563,1 +2734,0 @@\n-    do_parameters_on(this);\n@@ -2566,2 +2736,9 @@\n-  BasicType* basic_types() {\n-    return sig_bt;\n+  \/\/ Either a static method, or <this> is not an inline type\n+  if (args_on_stack_cc() != args_on_stack_cc_ro()) {\n+    \/\/ No sharing:\n+    \/\/ Some arguments are passed on the stack, and we have inserted reserved entries\n+    \/\/ into the VEP, but we never insert reserved entries into the VIEP(RO).\n+    return CodeOffsets::Verified_Inline_Entry_RO;\n+  } else {\n+    \/\/ Share same entry for VEP and VIEP(RO).\n+    return CodeOffsets::Verified_Entry;\n@@ -2569,0 +2746,1 @@\n+}\n@@ -2570,3 +2748,39 @@\n-#ifdef ASSERT\n-  int slots() {\n-    return index;\n+\/\/ Returns all super methods (transitive) in classes and interfaces that are overridden by the current method.\n+GrowableArray<Method*>* CompiledEntrySignature::get_supers() {\n+  if (_supers != nullptr) {\n+    return _supers;\n+  }\n+  _supers = new GrowableArray<Method*>();\n+  \/\/ Skip private, static, and <init> methods\n+  if (_method->is_private() || _method->is_static() || _method->is_object_constructor()) {\n+    return _supers;\n+  }\n+  Symbol* name = _method->name();\n+  Symbol* signature = _method->signature();\n+  const Klass* holder = _method->method_holder()->super();\n+  Symbol* holder_name = holder->name();\n+  ThreadInVMfromUnknown tiv;\n+  JavaThread* current = JavaThread::current();\n+  HandleMark hm(current);\n+  Handle loader(current, _method->method_holder()->class_loader());\n+\n+  \/\/ Walk up the class hierarchy and search for super methods\n+  while (holder != nullptr) {\n+    Method* super_method = holder->lookup_method(name, signature);\n+    if (super_method == nullptr) {\n+      break;\n+    }\n+    if (!super_method->is_static() && !super_method->is_private() &&\n+        (!super_method->is_package_private() ||\n+         super_method->method_holder()->is_same_class_package(loader(), holder_name))) {\n+      _supers->push(super_method);\n+    }\n+    holder = super_method->method_holder()->super();\n+  }\n+  \/\/ Search interfaces for super methods\n+  Array<InstanceKlass*>* interfaces = _method->method_holder()->transitive_interfaces();\n+  for (int i = 0; i < interfaces->length(); ++i) {\n+    Method* m = interfaces->at(i)->lookup_method(name, signature);\n+    if (m != nullptr && !m->is_static() && m->is_public()) {\n+      _supers->push(m);\n+    }\n@@ -2574,0 +2788,50 @@\n+  return _supers;\n+}\n+\n+\/\/ Iterate over arguments and compute scalarized and non-scalarized signatures\n+void CompiledEntrySignature::compute_calling_conventions(bool init) {\n+  bool has_scalarized = false;\n+  if (_method != nullptr) {\n+    InstanceKlass* holder = _method->method_holder();\n+    int arg_num = 0;\n+    if (!_method->is_static()) {\n+      \/\/ We shouldn't scalarize 'this' in a value class constructor\n+      if (holder->is_inline_klass() && InlineKlass::cast(holder)->can_be_passed_as_fields() && !_method->is_object_constructor() &&\n+          (init || _method->is_scalarized_arg(arg_num))) {\n+        _sig_cc->appendAll(InlineKlass::cast(holder)->extended_sig());\n+        has_scalarized = true;\n+        _has_inline_recv = true;\n+        _num_inline_args++;\n+      } else {\n+        SigEntry::add_entry(_sig_cc, T_OBJECT, holder->name());\n+      }\n+      SigEntry::add_entry(_sig, T_OBJECT, holder->name());\n+      SigEntry::add_entry(_sig_cc_ro, T_OBJECT, holder->name());\n+      arg_num++;\n+    }\n+    for (SignatureStream ss(_method->signature()); !ss.at_return_type(); ss.next()) {\n+      BasicType bt = ss.type();\n+      if (bt == T_OBJECT) {\n+        InlineKlass* vk = ss.as_inline_klass(holder);\n+        if (vk != nullptr && vk->can_be_passed_as_fields() && (init || _method->is_scalarized_arg(arg_num))) {\n+          \/\/ Check for a calling convention mismatch with super method(s)\n+          bool scalar_super = false;\n+          bool non_scalar_super = false;\n+          GrowableArray<Method*>* supers = get_supers();\n+          for (int i = 0; i < supers->length(); ++i) {\n+            Method* super_method = supers->at(i);\n+            if (super_method->is_scalarized_arg(arg_num)) {\n+              scalar_super = true;\n+            } else {\n+              non_scalar_super = true;\n+            }\n+          }\n+#ifdef ASSERT\n+          \/\/ Randomly enable below code paths for stress testing\n+          bool stress = init && StressCallingConvention;\n+          if (stress && (os::random() & 1) == 1) {\n+            non_scalar_super = true;\n+            if ((os::random() & 1) == 1) {\n+              scalar_super = true;\n+            }\n+          }\n@@ -2575,0 +2839,51 @@\n+          if (non_scalar_super) {\n+            \/\/ Found a super method with a non-scalarized argument. Fall back to the non-scalarized calling convention.\n+            if (scalar_super) {\n+              \/\/ Found non-scalar *and* scalar super methods. We can't handle both.\n+              \/\/ Mark the scalar method as mismatch and re-compile call sites to use non-scalarized calling convention.\n+              for (int i = 0; i < supers->length(); ++i) {\n+                Method* super_method = supers->at(i);\n+                if (super_method->is_scalarized_arg(arg_num) debug_only(|| (stress && (os::random() & 1) == 1))) {\n+                  super_method->set_mismatch();\n+                  MutexLocker ml(Compile_lock, Mutex::_safepoint_check_flag);\n+                  JavaThread* thread = JavaThread::current();\n+                  HandleMark hm(thread);\n+                  methodHandle mh(thread, super_method);\n+                  DeoptimizationScope deopt_scope;\n+                  CodeCache::mark_for_deoptimization(&deopt_scope, mh());\n+                  deopt_scope.deoptimize_marked();\n+                }\n+              }\n+            }\n+            \/\/ Fall back to non-scalarized calling convention\n+            SigEntry::add_entry(_sig_cc, T_OBJECT, ss.as_symbol());\n+            SigEntry::add_entry(_sig_cc_ro, T_OBJECT, ss.as_symbol());\n+          } else {\n+            _num_inline_args++;\n+            has_scalarized = true;\n+            int last = _sig_cc->length();\n+            int last_ro = _sig_cc_ro->length();\n+            _sig_cc->appendAll(vk->extended_sig());\n+            _sig_cc_ro->appendAll(vk->extended_sig());\n+            if (bt == T_OBJECT) {\n+              \/\/ Nullable inline type argument, insert InlineTypeNode::IsInit field right after T_METADATA delimiter\n+              \/\/ Set the sort_offset so that the field is detected as null marker by nmethod::print_nmethod_labels.\n+              _sig_cc->insert_before(last+1, SigEntry(T_BOOLEAN, -1, 0));\n+              _sig_cc_ro->insert_before(last_ro+1, SigEntry(T_BOOLEAN, -1, 0));\n+            }\n+          }\n+        } else {\n+          SigEntry::add_entry(_sig_cc, T_OBJECT, ss.as_symbol());\n+          SigEntry::add_entry(_sig_cc_ro, T_OBJECT, ss.as_symbol());\n+        }\n+        bt = T_OBJECT;\n+      } else {\n+        SigEntry::add_entry(_sig_cc, ss.type(), ss.as_symbol());\n+        SigEntry::add_entry(_sig_cc_ro, ss.type(), ss.as_symbol());\n+      }\n+      SigEntry::add_entry(_sig, bt, ss.as_symbol());\n+      if (bt != T_VOID) {\n+        arg_num++;\n+      }\n+    }\n+  }\n@@ -2576,1 +2891,11 @@\n- private:\n+  \/\/ Compute the non-scalarized calling convention\n+  _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig->length());\n+  _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);\n+\n+  \/\/ Compute the scalarized calling conventions if there are scalarized inline types in the signature\n+  if (has_scalarized && !_method->is_native()) {\n+    _regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc->length());\n+    _args_on_stack_cc = SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);\n+\n+    _regs_cc_ro = NEW_RESOURCE_ARRAY(VMRegPair, _sig_cc_ro->length());\n+    _args_on_stack_cc_ro = SharedRuntime::java_calling_convention(_sig_cc_ro, _regs_cc_ro);\n@@ -2578,5 +2903,8 @@\n-  friend class SignatureIterator;  \/\/ so do_parameters_on can call do_type\n-  void do_type(BasicType type) {\n-    sig_bt[index++] = type;\n-    if (type == T_LONG || type == T_DOUBLE) {\n-      sig_bt[index++] = T_VOID; \/\/ Longs & doubles take 2 Java slots\n+    _c1_needs_stack_repair = (_args_on_stack_cc < _args_on_stack) || (_args_on_stack_cc_ro < _args_on_stack);\n+    _c2_needs_stack_repair = (_args_on_stack_cc > _args_on_stack) || (_args_on_stack_cc > _args_on_stack_cc_ro);\n+\n+    \/\/ Upper bound on stack arguments to avoid hitting the argument limit and\n+    \/\/ bailing out of compilation (\"unsupported incoming calling sequence\").\n+    \/\/ TODO we need a reasonable limit (flag?) here\n+    if (MAX2(_args_on_stack_cc, _args_on_stack_cc_ro) <= 60) {\n+      return; \/\/ Success\n@@ -2585,1 +2913,10 @@\n-};\n+\n+  \/\/ No scalarized args\n+  _sig_cc = _sig;\n+  _regs_cc = _regs;\n+  _args_on_stack_cc = _args_on_stack;\n+\n+  _sig_cc_ro = _sig;\n+  _regs_cc_ro = _regs;\n+  _args_on_stack_cc_ro = _args_on_stack;\n+}\n@@ -2602,2 +2939,15 @@\n-  \/\/ Fill in the signature array, for the calling-convention call.\n-  int total_args_passed = method->size_of_parameters(); \/\/ All args on stack\n+  CompiledEntrySignature ces(method());\n+  ces.compute_calling_conventions();\n+  if (ces.has_scalarized_args()) {\n+    if (!method->has_scalarized_args()) {\n+      method->set_has_scalarized_args();\n+    }\n+    if (ces.c1_needs_stack_repair()) {\n+      method->set_c1_needs_stack_repair();\n+    }\n+    if (ces.c2_needs_stack_repair() && !method->c2_needs_stack_repair()) {\n+      method->set_c2_needs_stack_repair();\n+    }\n+  } else if (method->is_abstract()) {\n+    return _abstract_method_handler;\n+  }\n@@ -2605,4 +2955,0 @@\n-  AdapterSignatureIterator si(method->signature(), method->constMethod()->fingerprint(),\n-                              method->is_static(), total_args_passed);\n-  assert(si.slots() == total_args_passed, \"\");\n-  BasicType* sig_bt = si.basic_types();\n@@ -2612,0 +2958,13 @@\n+    if (ces.has_scalarized_args() && method->is_abstract()) {\n+      \/\/ Save a C heap allocated version of the signature for abstract methods with scalarized inline type arguments\n+      address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();\n+      entry = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(nullptr),\n+                                               SharedRuntime::throw_AbstractMethodError_entry(),\n+                                               wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,\n+                                               wrong_method_abstract, wrong_method_abstract);\n+      GrowableArray<SigEntry>* heap_sig = new (mtInternal) GrowableArray<SigEntry>(ces.sig_cc_ro()->length(), mtInternal);\n+      heap_sig->appendAll(ces.sig_cc_ro());\n+      entry->set_sig_cc(heap_sig);\n+      return entry;\n+    }\n+\n@@ -2613,1 +2972,1 @@\n-    entry = lookup(total_args_passed, sig_bt);\n+    entry = lookup(ces.sig_cc(), ces.has_inline_recv());\n@@ -2619,1 +2978,1 @@\n-        AdapterHandlerEntry* comparison_entry = create_adapter(comparison_blob, total_args_passed, sig_bt, false);\n+        AdapterHandlerEntry* comparison_entry = create_adapter(comparison_blob, ces, false);\n@@ -2629,1 +2988,1 @@\n-    entry = create_adapter(new_adapter, total_args_passed, sig_bt, \/* allocate_code_blob *\/ true);\n+    entry = create_adapter(new_adapter, ces, \/* allocate_code_blob *\/ true);\n@@ -2640,2 +2999,1 @@\n-                                                           int total_args_passed,\n-                                                           BasicType* sig_bt,\n+                                                           CompiledEntrySignature& ces,\n@@ -2653,5 +3011,0 @@\n-  VMRegPair stack_regs[16];\n-  VMRegPair* regs = (total_args_passed <= 16) ? stack_regs : NEW_RESOURCE_ARRAY(VMRegPair, total_args_passed);\n-\n-  \/\/ Get a description of the compiled java calling convention and the largest used (VMReg) stack slot usage\n-  int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed);\n@@ -2665,1 +3018,1 @@\n-  AdapterFingerPrint* fingerprint = new AdapterFingerPrint(total_args_passed, sig_bt);\n+  AdapterFingerPrint* fingerprint = new AdapterFingerPrint(ces.sig_cc(), ces.has_inline_recv());\n@@ -2668,5 +3021,17 @@\n-                                                total_args_passed,\n-                                                comp_args_on_stack,\n-                                                sig_bt,\n-                                                regs,\n-                                                fingerprint);\n+                                                ces.args_on_stack(),\n+                                                ces.sig(),\n+                                                ces.regs(),\n+                                                ces.sig_cc(),\n+                                                ces.regs_cc(),\n+                                                ces.sig_cc_ro(),\n+                                                ces.regs_cc_ro(),\n+                                                fingerprint,\n+                                                new_adapter,\n+                                                allocate_code_blob);\n+\n+  if (ces.has_scalarized_args()) {\n+    \/\/ Save a C heap allocated version of the scalarized signature and store it in the adapter\n+    GrowableArray<SigEntry>* heap_sig = new (mtInternal) GrowableArray<SigEntry>(ces.sig_cc()->length(), mtInternal);\n+    heap_sig->appendAll(ces.sig_cc());\n+    entry->set_sig_cc(heap_sig);\n+  }\n@@ -2683,1 +3048,0 @@\n-  new_adapter = AdapterBlob::create(&buffer);\n@@ -2725,0 +3089,2 @@\n+  assert(base <= _c2i_inline_entry || _c2i_inline_entry == nullptr, \"\");\n+  assert(base <= _c2i_inline_ro_entry || _c2i_inline_ro_entry == nullptr, \"\");\n@@ -2726,0 +3092,1 @@\n+  assert(base <= _c2i_unverified_inline_entry || _c2i_unverified_inline_entry == nullptr, \"\");\n@@ -2738,0 +3105,4 @@\n+  if (_c2i_inline_entry != nullptr)\n+    _c2i_inline_entry += delta;\n+  if (_c2i_inline_ro_entry != nullptr)\n+    _c2i_inline_ro_entry += delta;\n@@ -2740,0 +3111,2 @@\n+  if (_c2i_unverified_inline_entry != nullptr)\n+    _c2i_unverified_inline_entry += delta;\n@@ -2748,0 +3121,3 @@\n+  if (_sig_cc != nullptr) {\n+    delete _sig_cc;\n+  }\n@@ -2834,0 +3210,1 @@\n+      BasicType stack_sig_bt[16];\n@@ -2835,0 +3212,1 @@\n+      BasicType* sig_bt = (total_args_passed <= 16) ? stack_sig_bt : NEW_RESOURCE_ARRAY(BasicType, total_args_passed);\n@@ -2837,5 +3215,13 @@\n-      AdapterSignatureIterator si(method->signature(), method->constMethod()->fingerprint(),\n-                              method->is_static(), total_args_passed);\n-      BasicType* sig_bt = si.basic_types();\n-      assert(si.slots() == total_args_passed, \"\");\n-      BasicType ret_type = si.return_type();\n+      int i = 0;\n+      if (!method->is_static()) {  \/\/ Pass in receiver first\n+        sig_bt[i++] = T_OBJECT;\n+      }\n+      SignatureStream ss(method->signature());\n+      for (; !ss.at_return_type(); ss.next()) {\n+        sig_bt[i++] = ss.type();  \/\/ Collect remaining bits of signature\n+        if (ss.type() == T_LONG || ss.type() == T_DOUBLE) {\n+          sig_bt[i++] = T_VOID;   \/\/ Longs & doubles take 2 Java slots\n+        }\n+      }\n+      assert(i == total_args_passed, \"\");\n+      BasicType ret_type = ss.type();\n@@ -3088,0 +3474,6 @@\n+  if (get_c2i_entry() != nullptr) {\n+    st->print(\" c2iVE: \" INTPTR_FORMAT, p2i(get_c2i_inline_entry()));\n+  }\n+  if (get_c2i_entry() != nullptr) {\n+    st->print(\" c2iVROE: \" INTPTR_FORMAT, p2i(get_c2i_inline_ro_entry()));\n+  }\n@@ -3089,1 +3481,4 @@\n-    st->print(\" c2iUV: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+    st->print(\" c2iUE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));\n+  }\n+  if (get_c2i_unverified_entry() != nullptr) {\n+    st->print(\" c2iUVE: \" INTPTR_FORMAT, p2i(get_c2i_unverified_inline_entry()));\n@@ -3178,0 +3573,195 @@\n+\n+\/\/ We are at a compiled code to interpreter call. We need backing\n+\/\/ buffers for all inline type arguments. Allocate an object array to\n+\/\/ hold them (convenient because once we're done with it we don't have\n+\/\/ to worry about freeing it).\n+oop SharedRuntime::allocate_inline_types_impl(JavaThread* current, methodHandle callee, bool allocate_receiver, TRAPS) {\n+  assert(InlineTypePassFieldsAsArgs, \"no reason to call this\");\n+  ResourceMark rm;\n+\n+  int nb_slots = 0;\n+  InstanceKlass* holder = callee->method_holder();\n+  allocate_receiver &= !callee->is_static() && holder->is_inline_klass() && callee->is_scalarized_arg(0);\n+  if (allocate_receiver) {\n+    nb_slots++;\n+  }\n+  int arg_num = callee->is_static() ? 0 : 1;\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    BasicType bt = ss.type();\n+    if (bt == T_OBJECT && callee->is_scalarized_arg(arg_num)) {\n+      nb_slots++;\n+    }\n+    if (bt != T_VOID) {\n+      arg_num++;\n+    }\n+  }\n+  objArrayOop array_oop = oopFactory::new_objectArray(nb_slots, CHECK_NULL);\n+  objArrayHandle array(THREAD, array_oop);\n+  arg_num = callee->is_static() ? 0 : 1;\n+  int i = 0;\n+  if (allocate_receiver) {\n+    InlineKlass* vk = InlineKlass::cast(holder);\n+    oop res = vk->allocate_instance(CHECK_NULL);\n+    array->obj_at_put(i++, res);\n+  }\n+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {\n+    BasicType bt = ss.type();\n+    if (bt == T_OBJECT && callee->is_scalarized_arg(arg_num)) {\n+      InlineKlass* vk = ss.as_inline_klass(holder);\n+      assert(vk != nullptr, \"Unexpected klass\");\n+      oop res = vk->allocate_instance(CHECK_NULL);\n+      array->obj_at_put(i++, res);\n+    }\n+    if (bt != T_VOID) {\n+      arg_num++;\n+    }\n+  }\n+  return array();\n+}\n+\n+JRT_ENTRY(void, SharedRuntime::allocate_inline_types(JavaThread* current, Method* callee_method, bool allocate_receiver))\n+  methodHandle callee(current, callee_method);\n+  oop array = SharedRuntime::allocate_inline_types_impl(current, callee, allocate_receiver, CHECK);\n+  current->set_vm_result(array);\n+  current->set_vm_result_2(callee()); \/\/ TODO: required to keep callee live?\n+JRT_END\n+\n+\/\/ We're returning from an interpreted method: load each field into a\n+\/\/ register following the calling convention\n+JRT_LEAF(void, SharedRuntime::load_inline_type_fields_in_regs(JavaThread* current, oopDesc* res))\n+{\n+  assert(res->klass()->is_inline_klass(), \"only inline types here\");\n+  ResourceMark rm;\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  frame stubFrame = current->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+  assert(callerFrame.is_interpreted_frame(), \"should be coming from interpreter\");\n+\n+  InlineKlass* vk = InlineKlass::cast(res->klass());\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  if (regs == nullptr) {\n+    \/\/ The fields of the inline klass don't fit in registers, bail out\n+    return;\n+  }\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    address loc = reg_map.location(pair.first(), nullptr);\n+    switch(bt) {\n+    case T_BOOLEAN:\n+      *(jboolean*)loc = res->bool_field(off);\n+      break;\n+    case T_CHAR:\n+      *(jchar*)loc = res->char_field(off);\n+      break;\n+    case T_BYTE:\n+      *(jbyte*)loc = res->byte_field(off);\n+      break;\n+    case T_SHORT:\n+      *(jshort*)loc = res->short_field(off);\n+      break;\n+    case T_INT: {\n+      *(jint*)loc = res->int_field(off);\n+      break;\n+    }\n+    case T_LONG:\n+#ifdef _LP64\n+      *(intptr_t*)loc = res->long_field(off);\n+#else\n+      Unimplemented();\n+#endif\n+      break;\n+    case T_OBJECT:\n+    case T_ARRAY: {\n+      *(oop*)loc = res->obj_field(off);\n+      break;\n+    }\n+    case T_FLOAT:\n+      *(jfloat*)loc = res->float_field(off);\n+      break;\n+    case T_DOUBLE:\n+      *(jdouble*)loc = res->double_field(off);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+#ifdef ASSERT\n+  VMRegPair pair = regs->at(0);\n+  address loc = reg_map.location(pair.first(), nullptr);\n+  assert(*(oopDesc**)loc == res, \"overwritten object\");\n+#endif\n+\n+  current->set_vm_result(res);\n+}\n+JRT_END\n+\n+\/\/ We've returned to an interpreted method, the interpreter needs a\n+\/\/ reference to an inline type instance. Allocate it and initialize it\n+\/\/ from field's values in registers.\n+JRT_BLOCK_ENTRY(void, SharedRuntime::store_inline_type_fields_to_buf(JavaThread* current, intptr_t res))\n+{\n+  ResourceMark rm;\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  frame stubFrame = current->last_frame();\n+  frame callerFrame = stubFrame.sender(&reg_map);\n+\n+#ifdef ASSERT\n+  InlineKlass* verif_vk = InlineKlass::returned_inline_klass(reg_map);\n+#endif\n+\n+  if (!is_set_nth_bit(res, 0)) {\n+    \/\/ We're not returning with inline type fields in registers (the\n+    \/\/ calling convention didn't allow it for this inline klass)\n+    assert(!Metaspace::contains((void*)res), \"should be oop or pointer in buffer area\");\n+    current->set_vm_result((oopDesc*)res);\n+    assert(verif_vk == nullptr, \"broken calling convention\");\n+    return;\n+  }\n+\n+  clear_nth_bit(res, 0);\n+  InlineKlass* vk = (InlineKlass*)res;\n+  assert(verif_vk == vk, \"broken calling convention\");\n+  assert(Metaspace::contains((void*)res), \"should be klass\");\n+\n+  \/\/ Allocate handles for every oop field so they are safe in case of\n+  \/\/ a safepoint when allocating\n+  GrowableArray<Handle> handles;\n+  vk->save_oop_fields(reg_map, handles);\n+\n+  \/\/ It's unsafe to safepoint until we are here\n+  JRT_BLOCK;\n+  {\n+    JavaThread* THREAD = current;\n+    oop vt = vk->realloc_result(reg_map, handles, CHECK);\n+    current->set_vm_result(vt);\n+  }\n+  JRT_BLOCK_END;\n+}\n+JRT_END\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":724,"deletions":134,"binary":false,"changes":858,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -39,0 +40,1 @@\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n@@ -51,1 +53,1 @@\n-\/\/ FieldType  = \"B\" | \"C\" | \"D\" | \"F\" | \"I\" | \"J\" | \"S\" | \"Z\" | \"L\" ClassName \";\" | \"[\" FieldType.\n+\/\/ FieldType  = \"B\" | \"C\" | \"D\" | \"F\" | \"I\" | \"J\" | \"S\" | \"Z\" | \"L\" ClassName \";\" | \"Q\" ValueClassName \";\" | \"[\" FieldType.\n@@ -503,0 +505,14 @@\n+InlineKlass* SignatureStream::as_inline_klass(InstanceKlass* holder) {\n+  ThreadInVMfromUnknown tiv;\n+  JavaThread* THREAD = JavaThread::current();\n+  HandleMark hm(THREAD);\n+  Handle class_loader(THREAD, holder->class_loader());\n+  Klass* k = as_klass(class_loader, SignatureStream::CachedOrNull, THREAD);\n+  assert(!HAS_PENDING_EXCEPTION, \"Should never throw\");\n+  if (k != nullptr && k->is_inline_klass()) {\n+    return InlineKlass::cast(k);\n+  } else {\n+    return nullptr;\n+  }\n+}\n+\n@@ -577,1 +593,0 @@\n-\n@@ -596,1 +611,1 @@\n-bool SignatureVerifier::is_valid_method_signature(Symbol* sig) {\n+bool SignatureVerifier::is_valid_method_signature(const Symbol* sig) {\n@@ -619,1 +634,1 @@\n-bool SignatureVerifier::is_valid_type_signature(Symbol* sig) {\n+bool SignatureVerifier::is_valid_type_signature(const Symbol* sig) {\n@@ -666,0 +681,56 @@\n+\n+\/\/ Adds an argument to the signature\n+void SigEntry::add_entry(GrowableArray<SigEntry>* sig, BasicType bt, Symbol* symbol, int offset, float sort_offset) {\n+  if (sort_offset == -1) {\n+    sort_offset = offset;\n+  }\n+  sig->append(SigEntry(bt, offset, sort_offset, symbol));\n+  if (bt == T_LONG || bt == T_DOUBLE) {\n+    sig->append(SigEntry(T_VOID, offset, sort_offset, symbol)); \/\/ Longs and doubles take two stack slots\n+  }\n+}\n+\n+\/\/ Returns true if the argument at index 'i' is not an inline type delimiter\n+bool SigEntry::skip_value_delimiters(const GrowableArray<SigEntry>* sig, int i) {\n+  return (sig->at(i)._bt != T_METADATA &&\n+          (sig->at(i)._bt != T_VOID || sig->at(i-1)._bt == T_LONG || sig->at(i-1)._bt == T_DOUBLE));\n+}\n+\n+\/\/ Fill basic type array from signature array\n+int SigEntry::fill_sig_bt(const GrowableArray<SigEntry>* sig, BasicType* sig_bt) {\n+  int count = 0;\n+  for (int i = 0; i < sig->length(); i++) {\n+    if (skip_value_delimiters(sig, i)) {\n+      sig_bt[count++] = sig->at(i)._bt;\n+    }\n+  }\n+  return count;\n+}\n+\n+\/\/ Create a temporary symbol from the signature array\n+TempNewSymbol SigEntry::create_symbol(const GrowableArray<SigEntry>* sig) {\n+  ResourceMark rm;\n+  int length = sig->length();\n+  char* sig_str = NEW_RESOURCE_ARRAY(char, 2*length + 3);\n+  int idx = 0;\n+  sig_str[idx++] = '(';\n+  for (int i = 0; i < length; i++) {\n+    BasicType bt = sig->at(i)._bt;\n+    if (bt == T_METADATA || bt == T_VOID) {\n+      \/\/ Ignore\n+    } else {\n+      if (bt == T_ARRAY) {\n+        bt = T_OBJECT; \/\/ We don't know the element type, treat as Object\n+      }\n+      sig_str[idx++] = type2char(bt);\n+      if (bt == T_OBJECT) {\n+        sig_str[idx++] = ';';\n+      }\n+    }\n+  }\n+  sig_str[idx++] = ')';\n+  \/\/ Add a dummy return type. It won't be used but SignatureStream needs it.\n+  sig_str[idx++] = 'V';\n+  sig_str[idx++] = '\\0';\n+  return SymbolTable::new_symbol(sig_str);\n+}\n","filename":"src\/hotspot\/share\/runtime\/signature.cpp","additions":75,"deletions":4,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -43,7 +43,0 @@\n-template StackValue* StackValue::create_stack_value(const frame* fr, const RegisterMap* reg_map, ScopeValue* sv);\n-template StackValue* StackValue::create_stack_value(const frame* fr, const SmallRegisterMap* reg_map, ScopeValue* sv);\n-\n-template<typename RegisterMapT>\n-StackValue* StackValue::create_stack_value(const frame* fr, const RegisterMapT* reg_map, ScopeValue* sv) {\n-  return create_stack_value(sv, stack_value_address(fr, reg_map, sv), reg_map);\n-}\n@@ -150,0 +143,4 @@\n+\n+template StackValue* StackValue::create_stack_value(const frame* fr, const RegisterMap* reg_map, ScopeValue* sv);\n+template StackValue* StackValue::create_stack_value(const frame* fr, const SmallRegisterMap* reg_map, ScopeValue* sv);\n+\n@@ -151,1 +148,2 @@\n-StackValue* StackValue::create_stack_value(ScopeValue* sv, address value_addr, const RegisterMapT* reg_map) {\n+StackValue* StackValue::create_stack_value(const frame* fr, const RegisterMapT* reg_map, ScopeValue* sv) {\n+  address value_addr = stack_value_address(fr, reg_map, sv);\n@@ -252,1 +250,7 @@\n-    return new StackValue(hdl, hdl.is_null() && ov->is_scalar_replaced() ? 1 : 0);\n+    bool scalar_replaced = hdl.is_null() && ov->is_scalar_replaced();\n+    if (ov->maybe_null()) {\n+      \/\/ Don't treat inline type as scalar replaced if it is null\n+      jint is_init = StackValue::create_stack_value(fr, reg_map, ov->is_init())->get_jint();\n+      scalar_replaced &= (is_init != 0);\n+    }\n+    return new StackValue(hdl, scalar_replaced ? 1 : 0);\n","filename":"src\/hotspot\/share\/runtime\/stackValue.cpp","additions":13,"deletions":9,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -175,0 +175,3 @@\n+address StubRoutines::_load_inline_type_fields_in_regs = nullptr;\n+address StubRoutines::_store_inline_type_fields_to_buf = nullptr;\n+\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -314,0 +314,16 @@\n+\/\/ These checks are required for wait, notify and exit to avoid inflating the monitor to\n+\/\/ find out this inline type object cannot be locked.\n+#define CHECK_THROW_NOSYNC_IMSE(obj)  \\\n+  if (EnableValhalla && (obj)->mark().is_inline_type()) {  \\\n+    JavaThread* THREAD = current;           \\\n+    ResourceMark rm(THREAD);                \\\n+    THROW_MSG(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \\\n+  }\n+\n+#define CHECK_THROW_NOSYNC_IMSE_0(obj)  \\\n+  if (EnableValhalla && (obj)->mark().is_inline_type()) {  \\\n+    JavaThread* THREAD = current;             \\\n+    ResourceMark rm(THREAD);                  \\\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \\\n+  }\n+\n@@ -340,0 +356,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -404,0 +421,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -523,0 +541,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"JITed code should never have locked an instance of a value class\");\n@@ -545,0 +564,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"This method should never be called on an instance of an inline class\");\n@@ -564,0 +584,1 @@\n+  guarantee(!EnableValhalla || !obj->klass()->is_inline_klass(), \"Attempt to inflate inline type\");\n@@ -611,0 +632,3 @@\n+    if (EnableValhalla && mark.is_inline_type()) {\n+      return;\n+    }\n@@ -667,0 +691,1 @@\n+  JavaThread* THREAD = current;\n@@ -675,0 +700,10 @@\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+    ResourceMark rm(THREAD);\n+    const char* desc = \"Cannot synchronize on an instance of value class \";\n+    const char* className = obj->klass()->external_name();\n+    size_t msglen = strlen(desc) + strlen(className) + 1;\n+    char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+    assert(message != nullptr, \"NEW_RESOURCE_ARRAY should have called vm_exit_out_of_memory and not return nullptr\");\n+    THROW_MSG(vmSymbols::java_lang_IdentityException(), className);\n+  }\n+\n@@ -701,0 +736,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -745,0 +781,1 @@\n+  CHECK_THROW_NOSYNC_IMSE_0(obj);\n@@ -787,0 +824,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -815,0 +853,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -996,0 +1035,4 @@\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+    \/\/ VM should be calling bootstrap method\n+    ShouldNotReachHere();\n+  }\n@@ -1122,0 +1165,3 @@\n+  if (EnableValhalla && h_obj->mark().is_inline_type()) {\n+    return false;\n+  }\n@@ -1453,0 +1499,3 @@\n+  if (EnableValhalla) {\n+    guarantee(!object->klass()->is_inline_klass(), \"Attempt to inflate inline type\");\n+  }\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":49,"deletions":0,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -411,0 +411,1 @@\n+  initialize_class(vmSymbols::java_lang_IdentityException(), CHECK);\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -634,0 +634,4 @@\n+\n+void VM_PrintClassLayout::doit() {\n+  PrintClassLayout::print_class_layout(_out, _class_name);\n+}\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -70,0 +70,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -232,1 +234,1 @@\n-  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ObjArrayKlass*)                        \\\n+  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ArrayKlass*)                        \\\n@@ -1172,0 +1174,1 @@\n+           declare_type(FlatArrayKlass, ArrayKlass)                       \\\n@@ -1175,0 +1178,1 @@\n+        declare_type(InlineKlass, InstanceKlass)                          \\\n@@ -1554,0 +1558,1 @@\n+  declare_c2_type(MachVEPNode, MachIdealNode)                             \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -120,0 +120,1 @@\n+  DCmdFactory::register_DCmdFactory(new DCmdFactoryImpl<PrintClassLayoutDCmd>(full_export, true, false));\n@@ -948,1 +949,25 @@\n-#endif\n+\n+PrintClassLayoutDCmd::PrintClassLayoutDCmd(outputStream* output, bool heap) :\n+                                       DCmdWithParser(output, heap),\n+  _classname(\"classname\", \"Name of class whose layout should be printed. \",\n+             \"STRING\", true) {\n+  _dcmdparser.add_dcmd_argument(&_classname);\n+}\n+\n+void PrintClassLayoutDCmd::execute(DCmdSource source, TRAPS) {\n+  VM_PrintClassLayout printClassLayoutOp(output(), _classname.value());\n+  VMThread::execute(&printClassLayoutOp);\n+}\n+\n+int PrintClassLayoutDCmd::num_arguments() {\n+  ResourceMark rm;\n+  PrintClassLayoutDCmd* dcmd = new PrintClassLayoutDCmd(nullptr, false);\n+  if (dcmd != nullptr) {\n+    DCmdMark mark(dcmd);\n+    return dcmd->_dcmdparser.num_arguments();\n+  } else {\n+    return 0;\n+  }\n+}\n+\n+#endif \/\/ INCLUDE_SERVICES\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.cpp","additions":26,"deletions":1,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -44,0 +44,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n@@ -47,0 +49,1 @@\n+#include \"runtime\/fieldDescriptor.inline.hpp\"\n@@ -318,0 +321,22 @@\n+ * HPROF_FLAT_ARRAYS        list of flat arrays\n+ *\n+ *               [flat array sub-records]*\n+ *\n+ *               HPROF_FLAT_ARRAY      flat array\n+ *\n+ *                          id         array object ID (dumped as HPROF_GC_PRIM_ARRAY_DUMP)\n+ *                          id         element class ID (dumped by HPROF_GC_CLASS_DUMP)\n+ *\n+ * HPROF_INLINED_FIELDS     decribes inlined fields\n+ *\n+ *               [class with inlined fields sub-records]*\n+ *\n+ *               HPROF_CLASS_WITH_INLINED_FIELDS\n+ *\n+ *                          id         class ID (dumped as HPROF_GC_CLASS_DUMP)\n+ *\n+ *                          u2         number of instance inlined fields (not including super)\n+ *                          [u2,       inlined field index,\n+ *                           u2,       synthetic field count,\n+ *                           id,       original field name,\n+ *                           id]*      inlined field class ID (dumped by HPROF_GC_CLASS_DUMP)\n@@ -355,0 +380,7 @@\n+  \/\/ inlined object support\n+  HPROF_FLAT_ARRAYS             = 0x12,\n+  HPROF_INLINED_FIELDS          = 0x13,\n+  \/\/ inlined object subrecords\n+  HPROF_FLAT_ARRAY                  = 0x01,\n+  HPROF_CLASS_WITH_INLINED_FIELDS   = 0x01,\n+\n@@ -389,0 +421,65 @@\n+\n+class AbstractDumpWriter;\n+\n+class InlinedObjects {\n+\n+  struct ClassInlinedFields {\n+    const Klass *klass;\n+    uintx base_index;   \/\/ base index of the inlined field names (1st field has index base_index+1).\n+    ClassInlinedFields(const Klass *klass = nullptr, uintx base_index = 0) : klass(klass), base_index(base_index) {}\n+\n+    \/\/ For GrowableArray::find_sorted().\n+    static int compare(const ClassInlinedFields& a, const ClassInlinedFields& b) {\n+      return a.klass - b.klass;\n+    }\n+    \/\/ For GrowableArray::sort().\n+    static int compare(ClassInlinedFields* a, ClassInlinedFields* b) {\n+      return compare(*a, *b);\n+    }\n+  };\n+\n+  uintx _min_string_id;\n+  uintx _max_string_id;\n+\n+  GrowableArray<ClassInlinedFields> *_inlined_field_map;\n+\n+  \/\/ counters for classes with inlined fields and for the fields\n+  int _classes_count;\n+  int _inlined_fields_count;\n+\n+  static InlinedObjects *_instance;\n+\n+  static void inlined_field_names_callback(InlinedObjects* _this, const Klass *klass, uintx base_index, int count);\n+\n+  GrowableArray<oop> *_flat_arrays;\n+\n+public:\n+  InlinedObjects()\n+    : _min_string_id(0), _max_string_id(0),\n+    _inlined_field_map(nullptr),\n+    _classes_count(0), _inlined_fields_count(0),\n+    _flat_arrays(nullptr) {\n+  }\n+\n+  static InlinedObjects* get_instance() {\n+    return _instance;\n+  }\n+\n+  void init();\n+  void release();\n+\n+  void dump_inlined_field_names(AbstractDumpWriter *writer);\n+\n+  uintx get_base_index_for(Klass* k);\n+  uintx get_next_string_id(uintx id);\n+\n+  void dump_classed_with_inlined_fields(AbstractDumpWriter* writer);\n+\n+  void add_flat_array(oop array);\n+  void dump_flat_arrays(AbstractDumpWriter* writer);\n+\n+};\n+\n+InlinedObjects *InlinedObjects::_instance = nullptr;\n+\n+\n@@ -745,1 +842,1 @@\n-  \/\/ returns the size of the instance of the given class\n+  \/\/ calculates the total size of the all fields of the given class.\n@@ -758,2 +855,8 @@\n-  \/\/ dump the raw values of the instance fields of the given object\n-  static void dump_instance_fields(AbstractDumpWriter* writer, oop o, DumperClassCacheTableEntry* class_cache_entry);\n+  \/\/ dump the raw values of the instance fields of the given identity or inlined object;\n+  \/\/ for identity objects offset is 0 and 'klass' is o->klass(),\n+  \/\/ for inlined objects offset is the offset in the holder object, 'klass' is inlined object class\n+  static void dump_instance_fields(AbstractDumpWriter* writer, oop o, int offset, DumperClassCacheTable* class_cache, DumperClassCacheTableEntry* class_cache_entry);\n+  \/\/ dump the raw values of the instance fields of the given inlined object;\n+  \/\/ dump_instance_fields wrapper for inlined objects\n+  static void dump_inlined_object_fields(AbstractDumpWriter* writer, oop o, int offset, DumperClassCacheTable* class_cache, DumperClassCacheTableEntry* class_cache_entry);\n+\n@@ -763,1 +866,1 @@\n-  static void dump_instance_field_descriptors(AbstractDumpWriter* writer, Klass* k);\n+  static void dump_instance_field_descriptors(AbstractDumpWriter* writer, InstanceKlass* k, uintx *inlined_fields_index = nullptr);\n@@ -773,0 +876,2 @@\n+  \/\/ creates HPROF_GC_PRIM_ARRAY_DUMP record for the given flat array\n+  static void dump_flat_array(AbstractDumpWriter* writer, flatArrayOop array, DumperClassCacheTable* class_cache);\n@@ -780,0 +885,3 @@\n+  \/\/ extended version to dump flat arrays as primitive arrays;\n+  \/\/ type_size specifies size of the inlined objects.\n+  static int calculate_array_max_length(AbstractDumpWriter* writer, arrayOop array, int type_size, short header_size);\n@@ -795,0 +903,10 @@\n+  \/\/ helper methods for inlined fields.\n+  static bool is_inlined_field(const fieldDescriptor& fld) {\n+    return fld.is_flat();\n+  }\n+  static InlineKlass* get_inlined_field_klass(const fieldDescriptor& fld) {\n+    assert(is_inlined_field(fld), \"must be inlined field\");\n+    InstanceKlass* holder_klass = fld.field_holder();\n+    return InlineKlass::cast(holder_klass->get_inline_type_field_klass(fld.index()));\n+  }\n+\n@@ -819,0 +937,1 @@\n+  GrowableArray<InlineKlass*> _inline_klasses;\n@@ -827,0 +946,3 @@\n+  void push_sig_start_inlined() { _sigs_start.push('Q'); }\n+  bool is_inlined(int field_idx){ return _sigs_start.at(field_idx) == 'Q'; }\n+  InlineKlass* inline_klass(int field_idx) { assert(is_inlined(field_idx), \"Not inlined\"); return _inline_klasses.at(field_idx); }\n@@ -875,2 +997,11 @@\n-          Symbol* sig = fld.signature();\n-          entry->_sigs_start.push(sig->char_at(0));\n+          InlineKlass* inlineKlass = nullptr;\n+          if (DumperSupport::is_inlined_field(fld.field_descriptor())) {\n+            inlineKlass = DumperSupport::get_inlined_field_klass(fld.field_descriptor());\n+            entry->push_sig_start_inlined();\n+            entry->_instance_size += DumperSupport::instance_size(inlineKlass);\n+          } else {\n+            Symbol* sig = fld.signature();\n+            entry->_sigs_start.push(sig->char_at(0));\n+            entry->_instance_size += DumperSupport::sig2size(sig);\n+          }\n+          entry->_inline_klasses.push(inlineKlass);\n@@ -879,1 +1010,0 @@\n-          entry->_instance_size += DumperSupport::sig2size(sig);\n@@ -989,0 +1119,1 @@\n+\n@@ -1047,1 +1178,1 @@\n-\/\/ returns the size of the instance of the given class\n+\/\/ calculates the total size of the all fields of the given class.\n@@ -1055,1 +1186,5 @@\n-        size += sig2size(fld.signature());\n+        if (is_inlined_field(fld.field_descriptor())) {\n+          size += instance_size(get_inlined_field_klass(fld.field_descriptor()));\n+        } else {\n+          size += sig2size(fld.signature());\n+        }\n@@ -1068,0 +1203,2 @@\n+      assert(!is_inlined_field(fldc.field_descriptor()), \"static fields cannot be inlined\");\n+\n@@ -1110,0 +1247,2 @@\n+      assert(!is_inlined_field(fld.field_descriptor()), \"static fields cannot be inlined\");\n+\n@@ -1146,2 +1285,4 @@\n-\/\/ dump the raw values of the instance fields of the given object\n-void DumperSupport::dump_instance_fields(AbstractDumpWriter* writer, oop o, DumperClassCacheTableEntry* class_cache_entry) {\n+\/\/ dump the raw values of the instance fields of the given identity or inlined object;\n+\/\/ for identity objects offset is 0 and 'klass' is o->klass(),\n+\/\/ for inlined objects offset is the offset in the holder object, 'klass' is inlined object class.\n+void DumperSupport::dump_instance_fields(AbstractDumpWriter* writer, oop o, int offset, DumperClassCacheTable* class_cache, DumperClassCacheTableEntry* class_cache_entry) {\n@@ -1150,1 +1291,8 @@\n-    dump_field_value(writer, class_cache_entry->sig_start(idx), o, class_cache_entry->offset(idx));\n+    if (class_cache_entry->is_inlined(idx)) {\n+      InlineKlass* field_klass = class_cache_entry->inline_klass(idx);\n+      int fields_offset = offset + (class_cache_entry->offset(idx) - field_klass->payload_offset());\n+      DumperClassCacheTableEntry* inline_class_cache_entry = class_cache->lookup_or_create(field_klass);\n+      dump_inlined_object_fields(writer, o, fields_offset, class_cache, inline_class_cache_entry);\n+    } else {\n+      dump_field_value(writer, class_cache_entry->sig_start(idx), o, class_cache_entry->offset(idx));\n+    }\n@@ -1154,1 +1302,6 @@\n-\/\/ dumps the definition of the instance fields for a given class\n+void DumperSupport::dump_inlined_object_fields(AbstractDumpWriter* writer, oop o, int offset, DumperClassCacheTable* class_cache, DumperClassCacheTableEntry* class_cache_entry) {\n+  \/\/ the object is inlined, so all its fields are stored without headers.\n+  dump_instance_fields(writer, o, offset, class_cache, class_cache_entry);\n+}\n+\n+\/\/ gets the count of the instance fields for a given class\n@@ -1159,1 +1312,8 @@\n-    if (!fldc.access_flags().is_static()) field_count++;\n+    if (!fldc.access_flags().is_static()) {\n+      if (is_inlined_field(fldc.field_descriptor())) {\n+        \/\/ add \"synthetic\" fields for inlined fields.\n+        field_count += get_instance_fields_count(get_inlined_field_klass(fldc.field_descriptor()));\n+      } else {\n+        field_count++;\n+      }\n+    }\n@@ -1166,2 +1326,8 @@\n-void DumperSupport::dump_instance_field_descriptors(AbstractDumpWriter* writer, Klass* k) {\n-  InstanceKlass* ik = InstanceKlass::cast(k);\n+\/\/ inlined_fields_id is not-nullptr for inlined fields (to get synthetic field name IDs\n+\/\/ by using InlinedObjects::get_next_string_id()).\n+void DumperSupport::dump_instance_field_descriptors(AbstractDumpWriter* writer, InstanceKlass* ik, uintx* inlined_fields_id) {\n+  \/\/ inlined_fields_id != nullptr means ik is a class of inlined field.\n+  \/\/ Inlined field id pointer for this class; lazyly initialized\n+  \/\/ if the class has inlined field(s) and the caller didn't provide inlined_fields_id.\n+  uintx *this_klass_inlined_fields_id = inlined_fields_id;\n+  uintx inlined_id = 0;\n@@ -1172,1 +1338,23 @@\n-      Symbol* sig = fld.signature();\n+      if (is_inlined_field(fld.field_descriptor())) {\n+        \/\/ dump \"synthetic\" fields for inlined fields.\n+        if (this_klass_inlined_fields_id == nullptr) {\n+          inlined_id = InlinedObjects::get_instance()->get_base_index_for(ik);\n+          this_klass_inlined_fields_id = &inlined_id;\n+        }\n+        dump_instance_field_descriptors(writer, get_inlined_field_klass(fld.field_descriptor()), this_klass_inlined_fields_id);\n+      } else {\n+        Symbol* sig = fld.signature();\n+        Symbol* name = nullptr;\n+        \/\/ Use inlined_fields_id provided by caller.\n+        if (inlined_fields_id != nullptr) {\n+          uintx name_id = InlinedObjects::get_instance()->get_next_string_id(*inlined_fields_id);\n+\n+          \/\/ name_id == 0 is returned on error. use original field signature.\n+          if (name_id != 0) {\n+            *inlined_fields_id = name_id;\n+            name = reinterpret_cast<Symbol*>(name_id);\n+          }\n+        }\n+        if (name == nullptr) {\n+          name = fld.name();\n+        }\n@@ -1174,2 +1362,3 @@\n-      writer->write_symbolID(fld.name());   \/\/ name\n-      writer->write_u1(sig2tag(sig));       \/\/ type\n+        writer->write_symbolID(name);         \/\/ name\n+        writer->write_u1(sig2tag(sig));       \/\/ type\n+      }\n@@ -1200,1 +1389,1 @@\n-  dump_instance_fields(writer, o, cache_entry);\n+  dump_instance_fields(writer, o, 0, class_cache, cache_entry);\n@@ -1245,1 +1434,1 @@\n-  writer->write_u4(DumperSupport::instance_size(ik));\n+  writer->write_u4(HeapWordSize * ik->size_helper());\n@@ -1299,4 +1488,1 @@\n-int DumperSupport::calculate_array_max_length(AbstractDumpWriter* writer, arrayOop array, short header_size) {\n-  BasicType type = ArrayKlass::cast(array->klass())->element_type();\n-  assert(type >= T_BOOLEAN && type <= T_OBJECT, \"invalid array element type\");\n-\n+int DumperSupport::calculate_array_max_length(AbstractDumpWriter* writer, arrayOop array, int type_size, short header_size) {\n@@ -1305,7 +1491,0 @@\n-  int type_size;\n-  if (type == T_OBJECT) {\n-    type_size = sizeof(address);\n-  } else {\n-    type_size = type2aelembytes(type);\n-  }\n-\n@@ -1319,0 +1498,1 @@\n+    BasicType type = ArrayKlass::cast(array->klass())->element_type();\n@@ -1325,0 +1505,16 @@\n+int DumperSupport::calculate_array_max_length(AbstractDumpWriter* writer, arrayOop array, short header_size) {\n+  BasicType type = ArrayKlass::cast(array->klass())->element_type();\n+  assert((type >= T_BOOLEAN && type <= T_OBJECT) || type == T_FLAT_ELEMENT, \"invalid array element type\");\n+  int type_size;\n+  if (type == T_OBJECT) {\n+    type_size = sizeof(address);\n+  } else if (type == T_FLAT_ELEMENT) {\n+      \/\/ TODO: FIXME\n+      fatal(\"Not supported yet\"); \/\/ FIXME: JDK-8325678\n+  } else {\n+    type_size = type2aelembytes(type);\n+  }\n+\n+  return calculate_array_max_length(writer, array, type_size, header_size);\n+}\n+\n@@ -1350,0 +1546,41 @@\n+\/\/ creates HPROF_GC_PRIM_ARRAY_DUMP record for the given flat array\n+void DumperSupport::dump_flat_array(AbstractDumpWriter* writer, flatArrayOop array, DumperClassCacheTable* class_cache) {\n+  FlatArrayKlass* array_klass = FlatArrayKlass::cast(array->klass());\n+  InlineKlass* element_klass = array_klass->element_klass();\n+  int element_size = instance_size(element_klass);\n+  \/*                          id         array object ID\n+   *                          u4         stack trace serial number\n+   *                          u4         number of elements\n+   *                          u1         element type\n+   *\/\n+  short header_size = 1 + sizeof(address) + 2 * 4 + 1;\n+\n+  \/\/ TODO: use T_SHORT\/T_INT\/T_LONG if needed to avoid truncation\n+  BasicType type = T_BYTE;\n+  int type_size = type2aelembytes(type);\n+  int length = calculate_array_max_length(writer, array, element_size, header_size);\n+  u4 length_in_bytes = (u4)(length * element_size);\n+  u4 size = header_size + length_in_bytes;\n+\n+  writer->start_sub_record(HPROF_GC_PRIM_ARRAY_DUMP, size);\n+  writer->write_objectID(array);\n+  writer->write_u4(STACK_TRACE_ID);\n+  \/\/ TODO: round up array length for T_SHORT\/T_INT\/T_LONG\n+  writer->write_u4(length * element_size);\n+  writer->write_u1(type2tag(type));\n+\n+  for (int index = 0; index < length; index++) {\n+    \/\/ need offset in the holder to read inlined object. calculate it from flatArrayOop::value_at_addr()\n+    int offset = (int)((address)array->value_at_addr(index, array_klass->layout_helper())\n+                  - cast_from_oop<address>(array));\n+    DumperClassCacheTableEntry* class_cache_entry = class_cache->lookup_or_create(element_klass);\n+    dump_inlined_object_fields(writer, array, offset, class_cache, class_cache_entry);\n+  }\n+\n+  \/\/ TODO: write padding bytes for T_SHORT\/T_INT\/T_LONG\n+\n+  InlinedObjects::get_instance()->add_flat_array(array);\n+\n+  writer->end_sub_record();\n+}\n+\n@@ -1471,0 +1708,264 @@\n+class InlinedFieldNameDumper : public LockedClassesDo {\n+public:\n+  typedef void (*Callback)(InlinedObjects *owner, const Klass *klass, uintx base_index, int count);\n+\n+private:\n+  AbstractDumpWriter* _writer;\n+  InlinedObjects *_owner;\n+  Callback       _callback;\n+  uintx _index;\n+\n+  void dump_inlined_field_names(GrowableArray<Symbol*>* super_names, Symbol* field_name, InlineKlass* klass) {\n+    super_names->push(field_name);\n+    for (HierarchicalFieldStream<JavaFieldStream> fld(klass); !fld.done(); fld.next()) {\n+      if (!fld.access_flags().is_static()) {\n+        if (DumperSupport::is_inlined_field(fld.field_descriptor())) {\n+          dump_inlined_field_names(super_names, fld.name(), DumperSupport::get_inlined_field_klass(fld.field_descriptor()));\n+        } else {\n+          \/\/ get next string ID.\n+          uintx next_index = _owner->get_next_string_id(_index);\n+          if (next_index == 0) {\n+            \/\/ something went wrong (overflow?)\n+            \/\/ stop generation; the rest of inlined objects will have original field names.\n+            return;\n+          }\n+          _index = next_index;\n+\n+          \/\/ Calculate length.\n+          int len = fld.name()->utf8_length();\n+          for (GrowableArrayIterator<Symbol*> it = super_names->begin(); it != super_names->end(); ++it) {\n+            len += (*it)->utf8_length() + 1;    \/\/ +1 for \".\".\n+          }\n+\n+          DumperSupport::write_header(_writer, HPROF_UTF8, oopSize + len);\n+          _writer->write_symbolID(reinterpret_cast<Symbol*>(_index));\n+          \/\/ Write the string value.\n+          \/\/ 1) super_names.\n+          for (GrowableArrayIterator<Symbol*> it = super_names->begin(); it != super_names->end(); ++it) {\n+            _writer->write_raw((*it)->bytes(), (*it)->utf8_length());\n+            _writer->write_u1('.');\n+          }\n+          \/\/ 2) field name.\n+          _writer->write_raw(fld.name()->bytes(), fld.name()->utf8_length());\n+        }\n+      }\n+    }\n+    super_names->pop();\n+  }\n+\n+  void dump_inlined_field_names(Symbol* field_name, InlineKlass* field_klass) {\n+    GrowableArray<Symbol*> super_names(4, mtServiceability);\n+    dump_inlined_field_names(&super_names, field_name, field_klass);\n+  }\n+\n+public:\n+  InlinedFieldNameDumper(AbstractDumpWriter* writer, InlinedObjects* owner, Callback callback)\n+    : _writer(writer), _owner(owner), _callback(callback), _index(0)  {\n+  }\n+\n+  void do_klass(Klass* k) {\n+    if (!k->is_instance_klass()) {\n+      return;\n+    }\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    \/\/ if (ik->has_inline_type_fields()) {\n+    \/\/   return;\n+    \/\/ }\n+\n+    uintx base_index = _index;\n+    int count = 0;\n+\n+    for (HierarchicalFieldStream<JavaFieldStream> fld(ik); !fld.done(); fld.next()) {\n+      if (!fld.access_flags().is_static()) {\n+        if (DumperSupport::is_inlined_field(fld.field_descriptor())) {\n+          dump_inlined_field_names(fld.name(), DumperSupport::get_inlined_field_klass(fld.field_descriptor()));\n+          count++;\n+        }\n+      }\n+    }\n+\n+    if (count != 0) {\n+      _callback(_owner, k, base_index, count);\n+    }\n+  }\n+};\n+\n+class InlinedFieldsDumper : public LockedClassesDo {\n+private:\n+  AbstractDumpWriter* _writer;\n+\n+public:\n+  InlinedFieldsDumper(AbstractDumpWriter* writer) : _writer(writer) {}\n+\n+  void do_klass(Klass* k) {\n+    if (!k->is_instance_klass()) {\n+      return;\n+    }\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    \/\/ if (ik->has_inline_type_fields()) {\n+    \/\/   return;\n+    \/\/ }\n+\n+    \/\/ We can be at a point where java mirror does not exist yet.\n+    \/\/ So we need to check that the class is at least loaded, to avoid crash from a null mirror.\n+    if (!ik->is_loaded()) {\n+      return;\n+    }\n+\n+    u2 inlined_count = 0;\n+    for (HierarchicalFieldStream<JavaFieldStream> fld(ik); !fld.done(); fld.next()) {\n+      if (!fld.access_flags().is_static()) {\n+        if (DumperSupport::is_inlined_field(fld.field_descriptor())) {\n+          inlined_count++;\n+        }\n+      }\n+    }\n+    if (inlined_count != 0) {\n+      _writer->write_u1(HPROF_CLASS_WITH_INLINED_FIELDS);\n+\n+      \/\/ class ID\n+      _writer->write_classID(ik);\n+      \/\/ number of inlined fields\n+      _writer->write_u2(inlined_count);\n+      u2 index = 0;\n+      for (HierarchicalFieldStream<JavaFieldStream> fld(ik); !fld.done(); fld.next()) {\n+        if (!fld.access_flags().is_static()) {\n+          if (DumperSupport::is_inlined_field(fld.field_descriptor())) {\n+            \/\/ inlined field index\n+            _writer->write_u2(index);\n+            \/\/ synthetic field count\n+            u2 field_count = DumperSupport::get_instance_fields_count(DumperSupport::get_inlined_field_klass(fld.field_descriptor()));\n+            _writer->write_u2(field_count);\n+            \/\/ original field name\n+            _writer->write_symbolID(fld.name());\n+            \/\/ inlined field class ID\n+            _writer->write_classID(DumperSupport::get_inlined_field_klass(fld.field_descriptor()));\n+\n+            index += field_count;\n+          } else {\n+            index++;\n+          }\n+        }\n+      }\n+    }\n+  }\n+};\n+\n+\n+void InlinedObjects::init() {\n+  _instance = this;\n+\n+  struct Closure : public SymbolClosure {\n+    uintx _min_id = max_uintx;\n+    uintx _max_id = 0;\n+    Closure() : _min_id(max_uintx), _max_id(0) {}\n+\n+    void do_symbol(Symbol** p) {\n+      uintx val = reinterpret_cast<uintx>(*p);\n+      if (val < _min_id) {\n+        _min_id = val;\n+      }\n+      if (val > _max_id) {\n+        _max_id = val;\n+      }\n+    }\n+  } closure;\n+\n+  SymbolTable::symbols_do(&closure);\n+\n+  _min_string_id = closure._min_id;\n+  _max_string_id = closure._max_id;\n+}\n+\n+void InlinedObjects::release() {\n+  _instance = nullptr;\n+\n+  if (_inlined_field_map != nullptr) {\n+    delete _inlined_field_map;\n+    _inlined_field_map = nullptr;\n+  }\n+  if (_flat_arrays != nullptr) {\n+    delete _flat_arrays;\n+    _flat_arrays = nullptr;\n+  }\n+}\n+\n+void InlinedObjects::inlined_field_names_callback(InlinedObjects* _this, const Klass* klass, uintx base_index, int count) {\n+  if (_this->_inlined_field_map == nullptr) {\n+    _this->_inlined_field_map = new (mtServiceability) GrowableArray<ClassInlinedFields>(100, mtServiceability);\n+  }\n+  _this->_inlined_field_map->append(ClassInlinedFields(klass, base_index));\n+\n+  \/\/ counters for dumping classes with inlined fields\n+  _this->_classes_count++;\n+  _this->_inlined_fields_count += count;\n+}\n+\n+void InlinedObjects::dump_inlined_field_names(AbstractDumpWriter* writer) {\n+  InlinedFieldNameDumper nameDumper(writer, this, inlined_field_names_callback);\n+  ClassLoaderDataGraph::classes_do(&nameDumper);\n+\n+  if (_inlined_field_map != nullptr) {\n+    \/\/ prepare the map for  get_base_index_for().\n+    _inlined_field_map->sort(ClassInlinedFields::compare);\n+  }\n+}\n+\n+uintx InlinedObjects::get_base_index_for(Klass* k) {\n+  if (_inlined_field_map != nullptr) {\n+    bool found = false;\n+    int idx = _inlined_field_map->find_sorted<ClassInlinedFields, ClassInlinedFields::compare>(ClassInlinedFields(k, 0), found);\n+    if (found) {\n+        return _inlined_field_map->at(idx).base_index;\n+    }\n+  }\n+\n+  \/\/ return max_uintx, so get_next_string_id returns 0.\n+  return max_uintx;\n+}\n+\n+uintx InlinedObjects::get_next_string_id(uintx id) {\n+  if (++id == _min_string_id) {\n+    return _max_string_id + 1;\n+  }\n+  return id;\n+}\n+\n+void InlinedObjects::dump_classed_with_inlined_fields(AbstractDumpWriter* writer) {\n+  if (_classes_count != 0) {\n+    \/\/ Record for each class contains tag(u1), class ID and count(u2)\n+    \/\/ for each inlined field index(u2), synthetic fields count(u2), original field name and class ID\n+    int size = _classes_count * (1 + sizeof(address) + 2)\n+             + _inlined_fields_count * (2 + 2 + sizeof(address) + sizeof(address));\n+    DumperSupport::write_header(writer, HPROF_INLINED_FIELDS, (u4)size);\n+\n+    InlinedFieldsDumper dumper(writer);\n+    ClassLoaderDataGraph::classes_do(&dumper);\n+  }\n+}\n+\n+void InlinedObjects::add_flat_array(oop array) {\n+  if (_flat_arrays == nullptr) {\n+    _flat_arrays = new (mtServiceability) GrowableArray<oop>(100, mtServiceability);\n+  }\n+  _flat_arrays->append(array);\n+}\n+\n+void InlinedObjects::dump_flat_arrays(AbstractDumpWriter* writer) {\n+  if (_flat_arrays != nullptr) {\n+    \/\/ For each flat array the record contains tag (u1), object ID and class ID.\n+    int size = _flat_arrays->length() * (1 + sizeof(address) + sizeof(address));\n+\n+    DumperSupport::write_header(writer, HPROF_FLAT_ARRAYS, (u4)size);\n+    for (GrowableArrayIterator<oop> it = _flat_arrays->begin(); it != _flat_arrays->end(); ++it) {\n+      flatArrayOop array = flatArrayOop(*it);\n+      FlatArrayKlass* array_klass = FlatArrayKlass::cast(array->klass());\n+      InlineKlass* element_klass = array_klass->element_klass();\n+      writer->write_u1(HPROF_FLAT_ARRAY);\n+      writer->write_objectID(array);\n+      writer->write_classID(element_klass);\n+    }\n+  }\n+}\n+\n+\n@@ -2003,0 +2504,2 @@\n+  } else if (o->is_flatArray()) {\n+    DumperSupport::dump_flat_array(writer(), flatArrayOop(o), &_class_cache);\n@@ -2082,0 +2585,1 @@\n+  InlinedObjects*  _inlined_objects;\n@@ -2092,1 +2596,1 @@\n-  DumpMerger(const char* path, DumpWriter* writer, int dump_seq) :\n+  DumpMerger(const char* path, DumpWriter* writer, InlinedObjects* inlined_objects, int dump_seq) :\n@@ -2094,0 +2598,1 @@\n+    _inlined_objects(inlined_objects),\n@@ -2125,0 +2630,1 @@\n+    _inlined_objects->dump_flat_arrays(_writer);\n@@ -2126,0 +2632,1 @@\n+    _inlined_objects->release();\n@@ -2245,0 +2752,4 @@\n+\n+  \/\/ Inlined object support.\n+  InlinedObjects          _inlined_objects;\n+\n@@ -2325,0 +2836,2 @@\n+  InlinedObjects* inlined_objects() { return &_inlined_objects; }\n+\n@@ -2461,0 +2974,7 @@\n+    \/\/ HPROF_UTF8 records for inlined field names.\n+    inlined_objects()->init();\n+    inlined_objects()->dump_inlined_field_names(writer());\n+\n+    \/\/ HPROF_INLINED_FIELDS\n+    inlined_objects()->dump_classed_with_inlined_fields(writer());\n+\n@@ -2660,1 +3180,1 @@\n-  DumpMerger merger(path, &writer, dumper.dump_seq());\n+  DumpMerger merger(path, &writer, dumper.inlined_objects(), dumper.dump_seq());\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":554,"deletions":34,"binary":false,"changes":588,"status":"modified"},{"patch":"@@ -44,0 +44,2 @@\n+  if (is_identity_class()) st->print(\"identity \"  );\n+  if (!is_identity_class()) st->print(\"value \"    );\n","filename":"src\/hotspot\/share\/utilities\/accessFlags.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-  switch (_tag) {\n+  switch (value()) {\n@@ -71,1 +71,1 @@\n-  switch (_tag) {\n+  switch (value()) {\n@@ -81,1 +81,1 @@\n-    return _tag;\n+    return value();\n@@ -87,1 +87,1 @@\n-  switch (_tag) {\n+  switch (value()) {\n@@ -103,1 +103,1 @@\n-  switch (_tag) {\n+  switch (value()) {\n","filename":"src\/hotspot\/share\/utilities\/constantTag.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -56,0 +56,40 @@\n+#define COPY_ALIGNED_SEGMENT(t) \\\n+  if (bits % sizeof(t) == 0) { \\\n+    size_t segment = remain \/ sizeof(t); \\\n+    if (segment > 0) { \\\n+      Copy::conjoint_##t##s_atomic((const t*) cursor_from, (t*) cursor_to, segment); \\\n+      remain -= segment * sizeof(t); \\\n+      cursor_from = (void*)(((char*)cursor_from) + segment * sizeof(t)); \\\n+      cursor_to = (void*)(((char*)cursor_to) + segment * sizeof(t)); \\\n+    } \\\n+  } \\\n+\n+void Copy::copy_value_content(const void* from, void* to, size_t size) {\n+  \/\/ Simple cases first\n+  uintptr_t bits = (uintptr_t) from | (uintptr_t) to | (uintptr_t) size;\n+  if (bits % sizeof(jlong) == 0) {\n+    Copy::conjoint_jlongs_atomic((const jlong*) from, (jlong*) to, size \/ sizeof(jlong));\n+    return;\n+  } else if (bits % sizeof(jint) == 0) {\n+    Copy::conjoint_jints_atomic((const jint*) from, (jint*) to, size \/ sizeof(jint));\n+    return;\n+  } else if (bits % sizeof(jshort) == 0) {\n+    Copy::conjoint_jshorts_atomic((const jshort*) from, (jshort*) to, size \/ sizeof(jshort));\n+    return;\n+  }\n+\n+  \/\/ Complex cases\n+  bits = (uintptr_t) from | (uintptr_t) to;\n+  const void* cursor_from = from;\n+  void* cursor_to = to;\n+  size_t remain = size;\n+  COPY_ALIGNED_SEGMENT(jlong)\n+  COPY_ALIGNED_SEGMENT(jint)\n+  COPY_ALIGNED_SEGMENT(jshort)\n+  if (remain > 0) {\n+    Copy::conjoint_jbytes((const void*) cursor_from, (void*) cursor_to, remain);\n+  }\n+}\n+\n+#undef COPY_ALIGNED_SEGMENT\n+\n","filename":"src\/hotspot\/share\/utilities\/copy.cpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -118,1 +118,1 @@\n-  assert(num_type_chars == 11, \"must have tested the right number of mappings\");\n+  assert(num_type_chars == 12, \"must have tested the right number of mappings\");\n@@ -200,0 +200,1 @@\n+  _type2aelembytes[T_FLAT_ELEMENT]  = heapOopSize;\n@@ -211,2 +212,2 @@\n-  JVM_SIGNATURE_VOID,    0,\n-  0, 0, 0, 0\n+  JVM_SIGNATURE_FLAT_ELEMENT, JVM_SIGNATURE_VOID,\n+  0, 0, 0, 0, 0\n@@ -228,0 +229,1 @@\n+  \"inline_type\",\n@@ -258,1 +260,1 @@\n-int type2size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, -1};\n+int type2size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, -1};\n@@ -275,6 +277,7 @@\n-  T_VOID,                  \/\/ T_VOID     = 14,\n-  T_ADDRESS,               \/\/ T_ADDRESS  = 15,\n-  T_NARROWOOP,             \/\/ T_NARROWOOP= 16,\n-  T_METADATA,              \/\/ T_METADATA = 17,\n-  T_NARROWKLASS,           \/\/ T_NARROWKLASS = 18,\n-  T_CONFLICT               \/\/ T_CONFLICT = 19,\n+  T_OBJECT,                \/\/ T_PRIMITIVE_OBJECT = 14,\n+  T_VOID,                  \/\/ T_VOID     = 15,\n+  T_ADDRESS,               \/\/ T_ADDRESS  = 16,\n+  T_NARROWOOP,             \/\/ T_NARROWOOP= 17,\n+  T_METADATA,              \/\/ T_METADATA = 18,\n+  T_NARROWKLASS,           \/\/ T_NARROWKLASS = 19,\n+  T_CONFLICT               \/\/ T_CONFLICT = 20\n@@ -299,6 +302,7 @@\n-  T_VOID,    \/\/ T_VOID     = 14,\n-  T_ADDRESS, \/\/ T_ADDRESS  = 15,\n-  T_NARROWOOP, \/\/ T_NARROWOOP  = 16,\n-  T_METADATA,  \/\/ T_METADATA   = 17,\n-  T_NARROWKLASS, \/\/ T_NARROWKLASS  = 18,\n-  T_CONFLICT \/\/ T_CONFLICT = 19,\n+  T_OBJECT,  \/\/ T_PRIMITIVE_OBJECT = 14,\n+  T_VOID,    \/\/ T_VOID     = 15,\n+  T_ADDRESS, \/\/ T_ADDRESS  = 16,\n+  T_NARROWOOP, \/\/ T_NARROWOOP  = 17,\n+  T_METADATA,  \/\/ T_METADATA   = 18,\n+  T_NARROWKLASS, \/\/ T_NARROWKLASS  = 19,\n+  T_CONFLICT \/\/ T_CONFLICT = 20\n@@ -323,6 +327,7 @@\n-  0,                         \/\/ T_VOID     = 14,\n-  T_OBJECT_aelem_bytes,      \/\/ T_ADDRESS  = 15,\n-  T_NARROWOOP_aelem_bytes,   \/\/ T_NARROWOOP= 16,\n-  T_OBJECT_aelem_bytes,      \/\/ T_METADATA = 17,\n-  T_NARROWKLASS_aelem_bytes, \/\/ T_NARROWKLASS= 18,\n-  0                          \/\/ T_CONFLICT = 19,\n+  T_FLAT_ELEMENT_aelem_bytes, \/\/ T_PRIMITIVE_OBJECT = 14,\n+  0,                         \/\/ T_VOID     = 15,\n+  T_OBJECT_aelem_bytes,      \/\/ T_ADDRESS  = 16,\n+  T_NARROWOOP_aelem_bytes,   \/\/ T_NARROWOOP= 17,\n+  T_OBJECT_aelem_bytes,      \/\/ T_METADATA = 18,\n+  T_NARROWKLASS_aelem_bytes, \/\/ T_NARROWKLASS= 19,\n+  0                          \/\/ T_CONFLICT = 20\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.cpp","additions":27,"deletions":22,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -622,0 +622,9 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Prototyping\n+\/\/ \"Code Missing Here\" macro, un-define when integrating back from prototyping stage and break\n+\/\/ compilation on purpose (i.e. \"forget me not\")\n+#define PROTOTYPE\n+#ifdef PROTOTYPE\n+#define CMH(m)\n+#endif\n+\n@@ -707,6 +716,7 @@\n-  T_VOID        = 14,\n-  T_ADDRESS     = 15,\n-  T_NARROWOOP   = 16,\n-  T_METADATA    = 17,\n-  T_NARROWKLASS = 18,\n-  T_CONFLICT    = 19, \/\/ for stack value type with conflicting contents\n+  T_FLAT_ELEMENT = 14, \/\/ Not a true BasicType, only use in headers of flat arrays\n+  T_VOID        = 15,\n+  T_ADDRESS     = 16,\n+  T_NARROWOOP   = 17,\n+  T_METADATA    = 18,\n+  T_NARROWKLASS = 19,\n+  T_CONFLICT    = 20, \/\/ for stack value type with conflicting contents\n@@ -727,0 +737,1 @@\n+    F(JVM_SIGNATURE_FLAT_ELEMENT, T_FLAT_ELEMENT, N) \\\n@@ -756,1 +767,1 @@\n-  return (t == T_OBJECT || t == T_ARRAY || (include_narrow_oop && t == T_NARROWOOP));\n+  return (t == T_OBJECT || t == T_ARRAY || t == T_FLAT_ELEMENT || (include_narrow_oop && t == T_NARROWOOP));\n@@ -814,1 +825,2 @@\n-  T_VOID_size        = 0\n+  T_VOID_size        = 0,\n+  T_FLAT_ELEMENT_size = 1\n@@ -844,0 +856,1 @@\n+  T_FLAT_ELEMENT_aelem_bytes = 8,\n@@ -847,0 +860,1 @@\n+  T_FLAT_ELEMENT_aelem_bytes = 4,\n@@ -939,1 +953,1 @@\n-  vtos = 9,             \/\/ tos not cached\n+  vtos = 9,             \/\/ tos not cached,\n@@ -956,1 +970,1 @@\n-    case T_ARRAY  : \/\/ fall through\n+    case T_ARRAY  :   \/\/ fall through\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":24,"deletions":10,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"utilities\/ostream.hpp\"\n@@ -73,0 +74,215 @@\n+class StringMatcher {\n+ public:\n+  typedef int getc_function_t(const char* &source, const char* limit);\n+\n+ private:\n+  \/\/ These do not get properly inlined.\n+  \/\/ For full performance, this should be a template class\n+  \/\/ parameterized by two function arguments.\n+  getc_function_t* _pattern_getc;\n+  getc_function_t* _string_getc;\n+\n+ public:\n+  StringMatcher(getc_function_t pattern_getc,\n+                getc_function_t string_getc)\n+    : _pattern_getc(pattern_getc),\n+      _string_getc(string_getc)\n+  { }\n+\n+  enum {  \/\/ special results from _pattern_getc\n+    string_match_comma  = -0x100 + ',',\n+    string_match_star   = -0x100 + '*',\n+    string_match_eos    = -0x100 + '\\0'\n+  };\n+\n+ private:\n+  const char*\n+  skip_anchor_word(const char* match,\n+                   const char* match_end,\n+                   int anchor_length,\n+                   const char* pattern,\n+                   const char* pattern_end) {\n+    assert(pattern < pattern_end && anchor_length > 0, \"\");\n+    const char* begp = pattern;\n+    int ch1 = _pattern_getc(begp, pattern_end);\n+    \/\/ note that begp is now advanced over ch1\n+    assert(ch1 > 0, \"regular char only\");\n+    const char* matchp = match;\n+    const char* limitp = match_end - anchor_length;\n+    while (matchp <= limitp) {\n+      int mch = _string_getc(matchp, match_end);\n+      if (mch == ch1) {\n+        const char* patp = begp;\n+        const char* anchorp = matchp;\n+        while (patp < pattern_end) {\n+          char ch = _pattern_getc(patp, pattern_end);\n+          char mch = _string_getc(anchorp, match_end);\n+          if (mch != ch) {\n+            anchorp = nullptr;\n+            break;\n+          }\n+        }\n+        if (anchorp != nullptr) {\n+          return anchorp;  \/\/ Found a full copy of the anchor.\n+        }\n+        \/\/ That did not work, so restart the search for ch1.\n+      }\n+    }\n+    return nullptr;\n+  }\n+\n+ public:\n+  bool string_match(const char* pattern,\n+                    const char* string) {\n+    return string_match(pattern, pattern + strlen(pattern),\n+                        string, string + strlen(string));\n+  }\n+  bool string_match(const char* pattern, const char* pattern_end,\n+                    const char* string, const char* string_end) {\n+    const char* patp = pattern;\n+    switch (_pattern_getc(patp, pattern_end)) {\n+    case string_match_eos:\n+      return false;  \/\/ Empty pattern is always false.\n+    case string_match_star:\n+      if (patp == pattern_end) {\n+        return true;   \/\/ Lone star pattern is always true.\n+      }\n+      break;\n+    }\n+    patp = pattern;  \/\/ Reset after lookahead.\n+    const char* matchp = string;  \/\/ nullptr if failing\n+    for (;;) {\n+      int ch = _pattern_getc(patp, pattern_end);\n+      switch (ch) {\n+      case string_match_eos:\n+      case string_match_comma:\n+        \/\/ End of a list item; see if it's a match.\n+        if (matchp == string_end) {\n+          return true;\n+        }\n+        if (ch == string_match_comma) {\n+          \/\/ Get ready to match the next item.\n+          matchp = string;\n+          continue;\n+        }\n+        return false;  \/\/ End of all items.\n+\n+      case string_match_star:\n+        if (matchp != nullptr) {\n+          \/\/ Wildcard:  Parse out following anchor word and look for it.\n+          const char* begp = patp;\n+          const char* endp = patp;\n+          int anchor_len = 0;\n+          for (;;) {\n+            \/\/ get as many following regular characters as possible\n+            endp = patp;\n+            ch = _pattern_getc(patp, pattern_end);\n+            if (ch <= 0) {\n+              break;\n+            }\n+            anchor_len += 1;\n+          }\n+          \/\/ Anchor word [begp..endp) does not contain ch, so back up.\n+          \/\/ Now do an eager match to the anchor word, and commit to it.\n+          patp = endp;\n+          if (ch == string_match_eos ||\n+              ch == string_match_comma) {\n+            \/\/ Anchor word is at end of pattern, so treat it as a fixed pattern.\n+            const char* limitp = string_end - anchor_len;\n+            matchp = limitp;\n+            patp = begp;\n+            \/\/ Resume normal scanning at the only possible match position.\n+            continue;\n+          }\n+          \/\/ Find a floating occurrence of the anchor and continue matching.\n+          \/\/ Note:  This is greedy; there is no backtrack here.  Good enough.\n+          matchp = skip_anchor_word(matchp, string_end, anchor_len, begp, endp);\n+        }\n+        continue;\n+      }\n+      \/\/ Normal character.\n+      if (matchp != nullptr) {\n+        int mch = _string_getc(matchp, string_end);\n+        if (mch != ch) {\n+          matchp = nullptr;\n+        }\n+      }\n+    }\n+  }\n+};\n+\n+\/\/ Match a wildcarded class list to a proposed class name (in internal form).\n+\/\/ Commas or newlines separate multiple possible matches; stars are shell-style wildcards.\n+class ClassListMatcher : public StringMatcher {\n+ public:\n+  ClassListMatcher()\n+    : StringMatcher(pattern_list_getc, class_name_getc)\n+  { }\n+\n+ private:\n+  static int pattern_list_getc(const char* &pattern_ptr,\n+                               const char* pattern_end) {\n+    if (pattern_ptr == pattern_end) {\n+      return string_match_eos;\n+    }\n+    int ch = (unsigned char) *pattern_ptr++;\n+    switch (ch) {\n+    case ' ': case '\\t': case '\\n': case '\\r':\n+    case ',':\n+      \/\/ End of list item.\n+      for (;;) {\n+        switch (*pattern_ptr) {\n+        case ' ': case '\\t': case '\\n': case '\\r':\n+        case ',':\n+          pattern_ptr += 1;  \/\/ Collapse multiple commas or spaces.\n+          continue;\n+        }\n+        break;\n+      }\n+      return string_match_comma;\n+\n+    case '*':\n+      \/\/ Wildcard, matching any number of chars.\n+      while (*pattern_ptr == '*') {\n+        pattern_ptr += 1;  \/\/ Collapse multiple stars.\n+      }\n+      return string_match_star;\n+\n+    case '.':\n+      ch = '\/';   \/\/ Look for internal form of package separator\n+      break;\n+\n+    case '\\\\':\n+      \/\/ Superquote in pattern escapes * , whitespace, and itself.\n+      if (pattern_ptr < pattern_end) {\n+        ch = (unsigned char) *pattern_ptr++;\n+      }\n+      break;\n+    }\n+\n+    assert(ch > 0, \"regular char only\");\n+    return ch;\n+  }\n+\n+  static int class_name_getc(const char* &name_ptr,\n+                             const char* name_end) {\n+    if (name_ptr == name_end) {\n+      return string_match_eos;\n+    }\n+    int ch = (unsigned char) *name_ptr++;\n+    if (ch == '.') {\n+      ch = '\/';   \/\/ Normalize to internal form of package separator\n+    }\n+    return ch;  \/\/ plain character\n+  }\n+};\n+\n+bool StringUtils::class_list_match(const char* class_pattern_list,\n+                                   const char* class_name) {\n+  if (class_pattern_list == nullptr || class_name == nullptr || class_name[0] == '\\0')\n+    return false;\n+  ClassListMatcher clm;\n+  return clm.string_match(class_pattern_list, class_name);\n+}\n+\n+\n","filename":"src\/hotspot\/share\/utilities\/stringUtils.cpp","additions":216,"deletions":0,"binary":false,"changes":216,"status":"modified"},{"patch":"@@ -278,0 +278,5 @@\n+        \/**\n+         * Warn about issues related to migration of JDK classes.\n+         *\/\n+        MIGRATION(\"migration\"),\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Lint.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -234,0 +234,1 @@\n+            case VALUE_CLASSES -> true;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Preview.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -65,0 +65,1 @@\n+import com.sun.tools.javac.resources.CompilerProperties;\n@@ -69,0 +70,1 @@\n+import com.sun.tools.javac.tree.JCTree;\n@@ -79,0 +81,1 @@\n+import static com.sun.tools.javac.code.Scope.LookupKind.NON_RECURSIVE;\n@@ -114,0 +117,4 @@\n+    \/** Switch: allow value classes.\n+     *\/\n+    boolean allowValueClasses;\n+\n@@ -296,0 +303,2 @@\n+        allowValueClasses = (!preview.isPreview(Feature.VALUE_CLASSES) || preview.isEnabled()) &&\n+                Feature.VALUE_CLASSES.allowedInSource(source);\n@@ -590,3 +599,5 @@\n-                    return (outer == Type.noType) ?\n-                            t.erasure(types) :\n-                        new ClassType(outer, List.nil(), t);\n+                    if (outer == Type.noType) {\n+                        ClassType et = (ClassType) t.erasure(types);\n+                        return new ClassType(et.getEnclosingType(), List.nil(), et.tsym, et.getMetadata());\n+                    }\n+                    return new ClassType(outer, List.nil(), t, List.nil());\n@@ -614,1 +625,1 @@\n-                outer = new ClassType(outer, actuals, t) {\n+                outer = new ClassType(outer, actuals, t, List.nil()) {\n@@ -698,1 +709,1 @@\n-                    outer = new ClassType(outer, List.nil(), t);\n+                    outer = new ClassType(outer, List.nil(), t, List.nil());\n@@ -1561,0 +1572,7 @@\n+            } else if (proxy.type.tsym.flatName() == syms.migratedValueClassInternalType.tsym.flatName()) {\n+                Assert.check(sym.kind == TYP);\n+                sym.flags_field |= MIGRATED_VALUE_CLASS;\n+                if (needsValueFlag(sym, sym.flags_field)) {\n+                    sym.flags_field |= VALUE_CLASS;\n+                    sym.flags_field &= ~IDENTITY_TYPE;\n+                }\n@@ -1577,0 +1595,6 @@\n+                }  else if (proxy.type.tsym == syms.migratedValueClassType.tsym && sym.kind == TYP) {\n+                    sym.flags_field |= MIGRATED_VALUE_CLASS;\n+                    if (needsValueFlag(sym, sym.flags_field)) {\n+                        sym.flags_field |= VALUE_CLASS;\n+                        sym.flags_field &= ~IDENTITY_TYPE;\n+                    }\n@@ -3065,1 +3089,1 @@\n-        long flags = adjustClassFlags(f);\n+        long flags = adjustClassFlags(c, f);\n@@ -3157,1 +3181,1 @@\n-            long flags = adjustClassFlags(nextChar());\n+            long flags = adjustClassFlags(c, nextChar());\n@@ -3299,0 +3323,5 @@\n+        boolean previewClassFile = minorVersion == ClassFile.PREVIEW_MINOR_VERSION;\n+        if (allowValueClasses && previewClassFile && (flags & ACC_STRICT) != 0) {\n+            flags &= ~ACC_STRICT;\n+            flags |= STRICT;\n+        }\n@@ -3314,1 +3343,1 @@\n-    long adjustClassFlags(long flags) {\n+    long adjustClassFlags(ClassSymbol c, long flags) {\n@@ -3319,1 +3348,23 @@\n-        return flags & ~ACC_SUPER; \/\/ SUPER and SYNCHRONIZED bits overloaded\n+        if (((flags & ACC_IDENTITY) != 0 && !isMigratedValueClass(flags)) || (majorVersion < V67.major && (flags & INTERFACE) == 0)) {\n+            flags |= IDENTITY_TYPE;\n+        } else if (needsValueFlag(c, flags)) {\n+            flags |= VALUE_CLASS;\n+            flags &= ~IDENTITY_TYPE;\n+        }\n+        flags &= ~ACC_IDENTITY; \/\/ ACC_IDENTITY and SYNCHRONIZED bits overloaded\n+        return flags;\n+    }\n+\n+    private boolean needsValueFlag(Symbol c, long flags) {\n+        boolean previewClassFile = minorVersion == ClassFile.PREVIEW_MINOR_VERSION;\n+        if (allowValueClasses) {\n+            if (previewClassFile && majorVersion >= V67.major && (flags & INTERFACE) == 0 ||\n+                    majorVersion >= V67.major && isMigratedValueClass(flags)) {\n+                return true;\n+            }\n+        }\n+        return false;\n+    }\n+\n+    private boolean isMigratedValueClass(long flags) {\n+        return allowValueClasses && ((flags & MIGRATED_VALUE_CLASS) != 0);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/ClassReader.java","additions":60,"deletions":9,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -1275,1 +1275,1 @@\n-                            cs.type = new ClassType(cs.type.getEnclosingType(), null, cs);\n+                            cs.type = new ClassType(cs.type.getEnclosingType(), null, cs, List.nil());\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/processing\/JavacProcessingEnvironment.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -783,1 +783,1 @@\n-    improperly formed type, some parameters are missing\n+    improperly formed type, some parameters are missing or misplaced\n@@ -2124,0 +2124,8 @@\n+# lint: serial\n+compiler.warn.serializable.value.class.without.write.replace.1=\\\n+    serializable value class does not declare, or inherits, a writeReplace method\n+\n+# lint: serial\n+compiler.warn.serializable.value.class.without.write.replace.2=\\\n+    serializable class does not declare, or inherits, a writeReplace method\n+\n@@ -2844,0 +2852,3 @@\n+compiler.misc.type.req.identity=\\\n+    a type with identity\n+\n@@ -3893,0 +3904,3 @@\n+compiler.misc.bad.access.flags=\\\n+    bad access flags combination: {0}\n+\n@@ -4230,0 +4244,22 @@\n+compiler.misc.feature.value.classes=\\\n+    value classes\n+\n+# 0: type, 1: type\n+compiler.err.value.type.has.identity.super.type=\\\n+    The identity type {1} cannot be a supertype of the value type {0}\n+\n+# 0: symbol, 1: type\n+compiler.err.concrete.supertype.for.value.class=\\\n+    The concrete class {1} is not allowed to be a super class of the value class {0} either directly or indirectly\n+\n+# 0: symbol, 1: symbol, 2: type\n+compiler.err.super.class.method.cannot.be.synchronized=\\\n+    The method {0} in the super class {2} of the value class {1} is synchronized. This is disallowed\n+\n+# 0: symbol or name\n+compiler.err.cant.ref.after.ctor.called=\\\n+    cannot assign to {0} after supertype constructor has been called\n+\n+compiler.err.non.abstract.value.class.cant.be.sealed.or.non.sealed=\\\n+    ''sealed'' or ''non-sealed'' modifiers are only applicable to abstract value classes\n+\n@@ -4256,0 +4292,3 @@\n+\n+compiler.warn.value.finalize=\\\n+    value classes should not have finalize methods, they are not invoked\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/compiler.properties","additions":40,"deletions":1,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-  VtableStubs::find_vtable_stub(0); \/\/ min vtable index\n+  VtableStubs::find_vtable_stub(0, false); \/\/ min vtable index\n@@ -37,2 +37,2 @@\n-    VtableStubs::find_vtable_stub((1 << i) - 1);\n-    VtableStubs::find_vtable_stub((1 << i));\n+    VtableStubs::find_vtable_stub((1 << i) - 1, false);\n+    VtableStubs::find_vtable_stub((1 << i), false);\n@@ -40,1 +40,1 @@\n-  VtableStubs::find_vtable_stub((1 << 15) - 1); \/\/ max vtable index\n+  VtableStubs::find_vtable_stub((1 << 15) - 1, false); \/\/ max vtable index\n@@ -47,1 +47,1 @@\n-  VtableStubs::find_itable_stub(0); \/\/ min itable index\n+  VtableStubs::find_itable_stub(0, false); \/\/ min itable index\n@@ -49,2 +49,2 @@\n-    VtableStubs::find_itable_stub((1 << i) - 1);\n-    VtableStubs::find_itable_stub((1 << i));\n+    VtableStubs::find_itable_stub((1 << i) - 1, false);\n+    VtableStubs::find_itable_stub((1 << i), false);\n@@ -52,1 +52,1 @@\n-  VtableStubs::find_itable_stub((1 << 15) - 1); \/\/ max itable index\n+  VtableStubs::find_itable_stub((1 << 15) - 1, false); \/\/ max itable index\n","filename":"test\/hotspot\/gtest\/code\/test_vtableStub.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -49,0 +49,10 @@\n+static void assert_mark_word_print_pattern(Handle object, const char* pattern) {\n+  if (LockingMode == LM_MONITOR) {\n+    \/\/ With heavy monitors, we do not use the mark word. Printing the oop only shows \"monitor\" regardless of the\n+    \/\/ locking state.\n+    assert_test_pattern(object, \"monitor\");\n+  } else {\n+    assert_test_pattern(object, pattern);\n+  }\n+}\n+\n@@ -88,1 +98,1 @@\n-    assert_test_pattern(h_obj, \"locked\");\n+    assert_mark_word_print_pattern(h_obj, \"locked\");\n@@ -90,1 +100,1 @@\n-  assert_test_pattern(h_obj, \"is_unlocked no_hash\");\n+  assert_mark_word_print_pattern(h_obj, \"is_unlocked no_hash\");\n@@ -94,1 +104,1 @@\n-  assert_test_pattern(h_obj, \"is_unlocked hash=0x\");\n+  assert_mark_word_print_pattern(h_obj, \"is_unlocked hash=0x\");\n@@ -110,0 +120,139 @@\n+\n+static void assert_unlocked_state(markWord mark) {\n+  EXPECT_FALSE(mark.has_displaced_mark_helper());\n+  if (LockingMode == LM_LEGACY) {\n+    EXPECT_FALSE(mark.has_locker());\n+  } else if (LockingMode == LM_LIGHTWEIGHT) {\n+    EXPECT_FALSE(mark.is_fast_locked());\n+  }\n+  EXPECT_FALSE(mark.has_monitor());\n+  EXPECT_FALSE(mark.is_being_inflated());\n+  EXPECT_FALSE(mark.is_locked());\n+  EXPECT_TRUE(mark.is_unlocked());\n+}\n+\n+static void assert_copy_set_hash(markWord mark) {\n+  const intptr_t hash = 4711;\n+  EXPECT_TRUE(mark.has_no_hash());\n+  markWord copy = mark.copy_set_hash(hash);\n+  EXPECT_EQ(hash, copy.hash());\n+  EXPECT_FALSE(copy.has_no_hash());\n+}\n+\n+static void assert_type(markWord mark) {\n+  EXPECT_FALSE(mark.is_flat_array());\n+  EXPECT_FALSE(mark.is_inline_type());\n+  EXPECT_FALSE(mark.is_larval_state());\n+  EXPECT_FALSE(mark.is_null_free_array());\n+}\n+\n+TEST_VM(markWord, prototype) {\n+  markWord mark = markWord::prototype();\n+  assert_unlocked_state(mark);\n+  EXPECT_TRUE(mark.is_neutral());\n+\n+  assert_type(mark);\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+  EXPECT_TRUE(mark.decode_pointer() == NULL);\n+\n+  assert_copy_set_hash(mark);\n+  assert_type(mark);\n+}\n+\n+static void assert_inline_type(markWord mark) {\n+  EXPECT_FALSE(mark.is_flat_array());\n+  EXPECT_TRUE(mark.is_inline_type());\n+  EXPECT_FALSE(mark.is_null_free_array());\n+}\n+\n+TEST_VM(markWord, inline_type_prototype) {\n+  markWord mark = markWord::inline_type_prototype();\n+  assert_unlocked_state(mark);\n+  EXPECT_FALSE(mark.is_neutral());\n+\n+  assert_inline_type(mark);\n+  EXPECT_FALSE(mark.is_larval_state());\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+  EXPECT_TRUE(mark.decode_pointer() == NULL);\n+\n+  markWord larval = mark.enter_larval_state();\n+  EXPECT_TRUE(larval.is_larval_state());\n+  assert_inline_type(larval);\n+  mark = larval.exit_larval_state();\n+  EXPECT_FALSE(mark.is_larval_state());\n+  assert_inline_type(mark);\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+  EXPECT_TRUE(mark.decode_pointer() == NULL);\n+}\n+\n+#if _LP64\n+\n+static void assert_flat_array_type(markWord mark) {\n+  EXPECT_TRUE(mark.is_flat_array());\n+  EXPECT_FALSE(mark.is_inline_type());\n+  EXPECT_FALSE(mark.is_larval_state());\n+}\n+\n+TEST_VM(markWord, null_free_flat_array_prototype) {\n+  markWord mark = markWord::flat_array_prototype(LayoutKind::NON_ATOMIC_FLAT);\n+  assert_unlocked_state(mark);\n+  EXPECT_TRUE(mark.is_neutral());\n+\n+  assert_flat_array_type(mark);\n+  EXPECT_TRUE(mark.is_null_free_array());\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+  EXPECT_TRUE(mark.decode_pointer() == NULL);\n+\n+  assert_copy_set_hash(mark);\n+  assert_flat_array_type(mark);\n+  EXPECT_TRUE(mark.is_null_free_array());\n+}\n+\n+TEST_VM(markWord, nullable_flat_array_prototype) {\n+  markWord mark = markWord::flat_array_prototype(LayoutKind::NULLABLE_ATOMIC_FLAT);\n+  assert_unlocked_state(mark);\n+  EXPECT_TRUE(mark.is_neutral());\n+\n+  assert_flat_array_type(mark);\n+  EXPECT_FALSE(mark.is_null_free_array());\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+  EXPECT_TRUE(mark.decode_pointer() == NULL);\n+\n+  assert_copy_set_hash(mark);\n+  assert_flat_array_type(mark);\n+  EXPECT_FALSE(mark.is_null_free_array());\n+}\n+\n+static void assert_null_free_array_type(markWord mark) {\n+  EXPECT_FALSE(mark.is_flat_array());\n+  EXPECT_FALSE(mark.is_inline_type());\n+  EXPECT_FALSE(mark.is_larval_state());\n+  EXPECT_TRUE(mark.is_null_free_array());\n+}\n+\n+TEST_VM(markWord, null_free_array_prototype) {\n+  markWord mark = markWord::null_free_array_prototype();\n+  assert_unlocked_state(mark);\n+  EXPECT_TRUE(mark.is_neutral());\n+\n+  assert_null_free_array_type(mark);\n+\n+  EXPECT_TRUE(mark.has_no_hash());\n+  EXPECT_FALSE(mark.is_marked());\n+  EXPECT_TRUE(mark.decode_pointer() == NULL);\n+\n+  assert_copy_set_hash(mark);\n+  assert_null_free_array_type(mark);\n+}\n+#endif \/\/ _LP64\n+\n","filename":"test\/hotspot\/gtest\/oops\/test_markWord.cpp","additions":152,"deletions":3,"binary":false,"changes":155,"status":"modified"},{"patch":"@@ -80,0 +80,3 @@\n+compiler\/c2\/irTests\/scalarReplacement\/ScalarReplacementWithGCBarrierTests.java 8342488 generic-all\n+compiler\/c2\/TestMergeStores.java#id1 8348959 generic-all\n+\n@@ -107,0 +110,1 @@\n+runtime\/cds\/appcds\/redefineClass\/RedefineRunningMethods_Shared.java  8304168 generic-all\n@@ -128,0 +132,40 @@\n+\n+# Valhalla\n+runtime\/AccModule\/ConstModule.java 8294051 generic-all\n+runtime\/valhalla\/inlinetypes\/CircularityTest.java 8349037 generic-all\n+runtime\/valhalla\/inlinetypes\/PreloadCircularityTest.java 8349631 linux-aarch64-debug\n+runtime\/valhalla\/inlinetypes\/ValuePreloadTest.java 8349630 linux-aarch64-debug\n+compiler\/gcbarriers\/TestG1BarrierGeneration.java 8343420 generic-all\n+\n+# Valhalla + COH\n+compiler\/c2\/autovectorization\/TestIndexOverflowIR.java                          8348568 generic-all\n+compiler\/c2\/irTests\/TestVectorConditionalMove.java                              8348568 generic-all\n+compiler\/c2\/irTests\/TestVectorizationMismatchedAccess.java                      8348568 generic-all\n+compiler\/c2\/irTests\/TestVectorizationNotRun.java                                8348568 generic-all\n+compiler\/c2\/TestCastX2NotProcessedIGVN.java                                     8348568 generic-all\n+compiler\/loopopts\/superword\/TestAlignVector.java                                8348568 generic-all\n+compiler\/loopopts\/superword\/TestAlignVector.java#NoAlignVector-COH              8348568 generic-all\n+compiler\/loopopts\/superword\/TestAlignVector.java#VerifyAlignVector-COH          8348568 generic-all\n+compiler\/loopopts\/superword\/TestIndependentPacksWithCyclicDependency.java       8348568 generic-all\n+compiler\/loopopts\/superword\/TestMulAddS2I.java                                  8348568 generic-all\n+compiler\/loopopts\/superword\/TestScheduleReordersScalarMemops.java               8348568 generic-all\n+compiler\/loopopts\/superword\/TestSplitPacks.java                                 8348568 generic-all\n+compiler\/loopopts\/superword\/TestUnorderedReductionPartialVectorization.java     8348568 generic-all\n+compiler\/vectorization\/TestFloatConversionsVector.java                          8348568 generic-all\n+compiler\/vectorization\/runner\/ArrayTypeConvertTest.java                         8348568 generic-all\n+compiler\/vectorization\/runner\/LoopCombinedOpTest.java                           8348568 generic-all\n+compiler\/vectorization\/runner\/VectorizationTestRunner.java                      8348568 generic-all\n+\n+gc\/stress\/gcbasher\/TestGCBasherWithParallel.java                                8348568 generic-all\n+\n+gtest\/CompressedKlassGtest.java#use-zero-based-encoding-coh                     8348568 generic-all\n+gtest\/CompressedKlassGtest.java#use-zero-based-encoding-coh-large-class-space   8348568 generic-all\n+gtest\/MetaspaceGtests.java#UseCompactObjectHeaders                              8348568 generic-all\n+\n+runtime\/CompressedOops\/CompressedClassPointersEncodingScheme.java               8348568 generic-all\n+runtime\/FieldLayout\/BaseOffsets.java#no-coops-with-coh                          8348568 generic-all\n+runtime\/FieldLayout\/BaseOffsets.java#with-coop--with-coh                        8348568 generic-all\n+runtime\/cds\/TestDefaultArchiveLoading.java#coops_coh                            8348568 generic-all\n+runtime\/cds\/TestDefaultArchiveLoading.java#nocoops_coh                          8348568 generic-all\n+runtime\/cds\/appcds\/TestZGCWithCDS.java                                          8348568 generic-all\n+\n@@ -151,0 +195,31 @@\n+# Valhalla TODO:\n+serviceability\/jvmti\/valhalla\/HeapDump\/HeapDump.java 8317416 generic-all\n+\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbDumpclass.java 8190936 generic-all\n+\n+\n@@ -189,0 +264,2 @@\n+vmTestbase\/vm\/mlvm\/hiddenloader\/stress\/byteMutation\/Test.java 8317172 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":77,"deletions":0,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import compiler.valhalla.inlinetypes.InlineTypeIRNode;\n@@ -155,0 +156,6 @@\n+    \/\/ Valhalla: Make sure that all Valhalla specific IR nodes are also properly initialized. Doing it here also\n+    \/\/           ensures that the Flag VM is able to pick up the correct compile phases.\n+    static {\n+        InlineTypeIRNode.forceStaticInitialization();\n+    }\n+\n@@ -330,1 +337,1 @@\n-        String optoRegex = \"(.*precise .*\\\\R((.*(?i:mov|mv|xorl|nop|spill).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: C2 Runtime new_instance\" + END;\n+        String optoRegex = \"(.*precise .*\\\\R((.*(?i:mov|mv|xorl|nop|spill|pushq|popq).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: C2 Runtime new_instance\" + END;\n@@ -336,1 +343,1 @@\n-        String regex = \"(.*precise .*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: C2 Runtime new_instance\" + END;\n+        String regex = \"(.*precise .*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill|pushq|popq).*|\\\\s*)\\\\R)*.*(?i:call,static).*wrapper for: C2 Runtime new_instance\" + END;\n@@ -342,1 +349,1 @@\n-        String optoRegex = \"(.*precise \\\\[.*\\\\R((.*(?i:mov|mv|xor|nop|spill).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: C2 Runtime new_array\" + END;\n+        String optoRegex = \"(.*precise \\\\[.*\\\\R((.*(?i:mov|mv|xor|nop|spill|pushq|popq).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: C2 Runtime new_array\" + END;\n@@ -348,1 +355,1 @@\n-        String regex = \"(.*precise \\\\[.*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: C2 Runtime new_array\" + END;\n+        String regex = \"(.*precise \\\\[.*\" + IS_REPLACED + \":.*\\\\R((.*(?i:mov|mv|xorl|nop|spill|pushq|popq).*|\\\\s*|.*(LGHI|LI).*)\\\\R)*.*(?i:call,static).*wrapper for: C2 Runtime new_array\" + END;\n@@ -746,0 +753,5 @@\n+    public static final String INLINE_TYPE = PREFIX + \"INLINE_TYPE\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(INLINE_TYPE, \"InlineType\");\n+    }\n+\n@@ -1787,1 +1799,1 @@\n-        beforeMatchingNameRegex(SUBTYPE_CHECK, \"SubTypeCheck\");\n+        macroNodes(SUBTYPE_CHECK, \"SubTypeCheck\");\n@@ -2472,1 +2484,1 @@\n-    private static void beforeMatching(String irNodePlaceholder, String regex) {\n+    public static void beforeMatching(String irNodePlaceholder, String regex) {\n@@ -2518,1 +2530,1 @@\n-    private static void optoOnly(String irNodePlaceholder, String regex) {\n+    public static void optoOnly(String irNodePlaceholder, String regex) {\n@@ -2569,0 +2581,13 @@\n+    \/**\n+     * Apply a regex that matches a macro node IR node name {@code macroNodeName} exactly on all machine independent\n+     * ideal graph phases up to and including {@link CompilePhase#BEFORE_MACRO_EXPANSION}. By default, we match on\n+     * {@link CompilePhase#BEFORE_MACRO_EXPANSION} when no {@link CompilePhase} is chosen.\n+     *\/\n+    private static void macroNodes(String irNodePlaceholder, String macroNodeName) {\n+        String macroNodeRegex = START + macroNodeName + \"\\\\b\" + MID + END;\n+        IR_NODE_MAPPINGS.put(irNodePlaceholder, new SinglePhaseRangeEntry(CompilePhase.BEFORE_MACRO_EXPANSION,\n+                                                                          macroNodeRegex,\n+                                                                          CompilePhase.BEFORE_STRINGOPTS,\n+                                                                          CompilePhase.BEFORE_MACRO_EXPANSION));\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":32,"deletions":7,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -68,0 +68,4 @@\n+    private Map<Number, Number> flatArrays = new HashMap<>();\n+\n+    private Map<Number, ClassInlinedFields[]> inlinedFields = new HashMap<>();\n+\n@@ -208,0 +212,35 @@\n+    public void addFlatArray(long objID, long elementClassID) {\n+        flatArrays.put(makeId(objID), makeId(elementClassID));\n+    }\n+\n+    \/**\n+     * @return null if the array is not flat array\n+     *\/\n+    Number findFlatArrayElementType(long arrayObjectID) {\n+        return flatArrays.get(makeId(arrayObjectID));\n+    }\n+\n+    public static class ClassInlinedFields {\n+        final int fieldIndex;\n+        final int synthFieldCount;\n+        final String fieldName;\n+        final long fieldClassID;\n+\n+        public ClassInlinedFields(int fieldIndex, int synthFieldCount, String fieldName, long fieldClassID) {\n+            this.fieldIndex = fieldIndex;\n+            this.synthFieldCount = synthFieldCount;\n+            this.fieldName = fieldName;\n+            this.fieldClassID =  fieldClassID;\n+        }\n+    }\n+\n+    public void addClassInlinedFields(long classID, ClassInlinedFields[] fields) {\n+        inlinedFields.put(makeId(classID), fields);\n+    }\n+\n+    \/**\n+     * @return null if the class has no inlined fields\n+     *\/\n+    ClassInlinedFields[] findClassInlinedFields(long classID) {\n+        return inlinedFields.get(makeId(classID));\n+    }\n","filename":"test\/lib\/jdk\/test\/lib\/hprof\/model\/Snapshot.java","additions":39,"deletions":0,"binary":false,"changes":39,"status":"modified"}]}