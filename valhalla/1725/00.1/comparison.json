{"files":[{"patch":"@@ -40,1 +40,1 @@\n-define_pd_global(bool, TieredCompilation,              false);\n+define_pd_global(bool, TieredCompilation,              true);\n","filename":"src\/hotspot\/cpu\/x86\/c1_globals_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"asm\/assembler.hpp\"\n@@ -41,0 +42,2 @@\n+#include \"utilities\/macros.hpp\"\n+#include \"vmreg_x86.inline.hpp\"\n@@ -304,4 +307,6 @@\n-  __ movptr(c_rarg0, result);\n-  Label is_long, is_float, is_double, exit;\n-  __ movl(c_rarg1, result_type);\n-  __ cmpl(c_rarg1, T_OBJECT);\n+  __ movptr(r13, result);\n+  Label is_long, is_float, is_double, check_prim, exit;\n+  __ movl(rbx, result_type);\n+  __ cmpl(rbx, T_OBJECT);\n+  __ jcc(Assembler::equal, check_prim);\n+  __ cmpl(rbx, T_LONG);\n@@ -309,3 +314,1 @@\n-  __ cmpl(c_rarg1, T_LONG);\n-  __ jcc(Assembler::equal, is_long);\n-  __ cmpl(c_rarg1, T_FLOAT);\n+  __ cmpl(rbx, T_FLOAT);\n@@ -313,1 +316,1 @@\n-  __ cmpl(c_rarg1, T_DOUBLE);\n+  __ cmpl(rbx, T_DOUBLE);\n@@ -319,1 +322,1 @@\n-    __ cmpl(c_rarg1, T_INT);\n+    __ cmpl(rbx, T_INT);\n@@ -327,1 +330,1 @@\n-  __ movl(Address(c_rarg0, 0), rax);\n+  __ movl(Address(r13, 0), rax);\n@@ -385,0 +388,13 @@\n+  __ BIND(check_prim);\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ Check for scalarized return value\n+    __ testptr(rax, 1);\n+    __ jcc(Assembler::zero, is_long);\n+    \/\/ Load pack handler address\n+    __ andptr(rax, -2);\n+    __ movptr(rax, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    __ movptr(rbx, Address(rax, InlineKlass::pack_handler_jobject_offset()));\n+    \/\/ Call pack handler to initialize the buffer\n+    __ call(rbx);\n+    __ jmp(exit);\n+  }\n@@ -386,1 +402,1 @@\n-  __ movq(Address(c_rarg0, 0), rax);\n+  __ movq(Address(r13, 0), rax);\n@@ -390,1 +406,1 @@\n-  __ movflt(Address(c_rarg0, 0), xmm0);\n+  __ movflt(Address(r13, 0), xmm0);\n@@ -394,1 +410,1 @@\n-  __ movdbl(Address(c_rarg0, 0), xmm0);\n+  __ movdbl(Address(r13, 0), xmm0);\n@@ -3752,0 +3768,61 @@\n+static void save_return_registers(MacroAssembler* masm) {\n+  masm->push_ppx(rax);\n+  if (InlineTypeReturnedAsFields) {\n+    masm->push(rdi);\n+    masm->push(rsi);\n+    masm->push(rdx);\n+    masm->push(rcx);\n+    masm->push(r8);\n+    masm->push(r9);\n+  }\n+  masm->push_d(xmm0);\n+  if (InlineTypeReturnedAsFields) {\n+    masm->push_d(xmm1);\n+    masm->push_d(xmm2);\n+    masm->push_d(xmm3);\n+    masm->push_d(xmm4);\n+    masm->push_d(xmm5);\n+    masm->push_d(xmm6);\n+    masm->push_d(xmm7);\n+  }\n+#ifdef ASSERT\n+  masm->movq(rax, 0xBADC0FFE);\n+  masm->movq(rdi, rax);\n+  masm->movq(rsi, rax);\n+  masm->movq(rdx, rax);\n+  masm->movq(rcx, rax);\n+  masm->movq(r8, rax);\n+  masm->movq(r9, rax);\n+  masm->movq(xmm0, rax);\n+  masm->movq(xmm1, rax);\n+  masm->movq(xmm2, rax);\n+  masm->movq(xmm3, rax);\n+  masm->movq(xmm4, rax);\n+  masm->movq(xmm5, rax);\n+  masm->movq(xmm6, rax);\n+  masm->movq(xmm7, rax);\n+#endif\n+}\n+\n+static void restore_return_registers(MacroAssembler* masm) {\n+  if (InlineTypeReturnedAsFields) {\n+    masm->pop_d(xmm7);\n+    masm->pop_d(xmm6);\n+    masm->pop_d(xmm5);\n+    masm->pop_d(xmm4);\n+    masm->pop_d(xmm3);\n+    masm->pop_d(xmm2);\n+    masm->pop_d(xmm1);\n+  }\n+  masm->pop_d(xmm0);\n+  if (InlineTypeReturnedAsFields) {\n+    masm->pop(r9);\n+    masm->pop(r8);\n+    masm->pop(rcx);\n+    masm->pop(rdx);\n+    masm->pop(rsi);\n+    masm->pop(rdi);\n+  }\n+  masm->pop_ppx(rax);\n+}\n+\n@@ -3803,2 +3880,1 @@\n-    __ push_ppx(rax);\n-    __ push_d(xmm0);\n+    save_return_registers(_masm);\n@@ -3815,2 +3891,1 @@\n-    __ pop_d(xmm0);\n-    __ pop_ppx(rax);\n+    restore_return_registers(_masm);\n@@ -3842,2 +3917,1 @@\n-    __ push_ppx(rax);\n-    __ push_d(xmm0);\n+    save_return_registers(_masm);\n@@ -3855,2 +3929,1 @@\n-    __ pop_d(xmm0);\n-    __ pop_ppx(rax);\n+    restore_return_registers(_masm);\n@@ -4063,0 +4136,10 @@\n+  \/\/ Generate these first because they are called from other stubs\n+  if (InlineTypeReturnedAsFields) {\n+    StubRoutines::_load_inline_type_fields_in_regs =\n+      generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs),\n+                                 \"load_inline_type_fields_in_regs\", false);\n+    StubRoutines::_store_inline_type_fields_to_buf =\n+      generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf),\n+                                 \"store_inline_type_fields_to_buf\", true);\n+  }\n+\n@@ -4106,0 +4189,144 @@\n+\/\/ Call here from the interpreter or compiled code to either load\n+\/\/ multiple returned values from the inline type instance being\n+\/\/ returned to registers or to store returned values to a newly\n+\/\/ allocated inline type instance.\n+\/\/ Register is a class, but it would be assigned numerical value.\n+\/\/ \"0\" is assigned for xmm0. Thus we need to ignore -Wnonnull.\n+PRAGMA_DIAG_PUSH\n+PRAGMA_NONNULL_IGNORED\n+address StubGenerator::generate_return_value_stub(address destination, const char* name, bool has_res) {\n+  \/\/ We need to save all registers the calling convention may use so\n+  \/\/ the runtime calls read or update those registers. This needs to\n+  \/\/ be in sync with SharedRuntime::java_return_convention().\n+  enum layout {\n+    pad_off = frame::arg_reg_save_area_bytes\/BytesPerInt, pad_off_2,\n+    rax_off, rax_off_2,\n+    j_rarg5_off, j_rarg5_2,\n+    j_rarg4_off, j_rarg4_2,\n+    j_rarg3_off, j_rarg3_2,\n+    j_rarg2_off, j_rarg2_2,\n+    j_rarg1_off, j_rarg1_2,\n+    j_rarg0_off, j_rarg0_2,\n+    j_farg0_off, j_farg0_2,\n+    j_farg1_off, j_farg1_2,\n+    j_farg2_off, j_farg2_2,\n+    j_farg3_off, j_farg3_2,\n+    j_farg4_off, j_farg4_2,\n+    j_farg5_off, j_farg5_2,\n+    j_farg6_off, j_farg6_2,\n+    j_farg7_off, j_farg7_2,\n+    rbp_off, rbp_off_2,\n+    return_off, return_off_2,\n+\n+    framesize\n+  };\n+\n+  CodeBuffer buffer(name, 1000, 512);\n+  MacroAssembler* _masm = new MacroAssembler(&buffer);\n+\n+  int frame_size_in_bytes = align_up(framesize*BytesPerInt, 16);\n+  assert(frame_size_in_bytes == framesize*BytesPerInt, \"misaligned\");\n+  int frame_size_in_slots = frame_size_in_bytes \/ BytesPerInt;\n+  int frame_size_in_words = frame_size_in_bytes \/ wordSize;\n+\n+  OopMapSet *oop_maps = new OopMapSet();\n+  OopMap* map = new OopMap(frame_size_in_slots, 0);\n+\n+  map->set_callee_saved(VMRegImpl::stack2reg(rax_off), rax->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n+\n+  int start = __ offset();\n+\n+  __ subptr(rsp, frame_size_in_bytes - 8 \/* return address*\/);\n+\n+  __ movptr(Address(rsp, rbp_off * BytesPerInt), rbp);\n+  __ movdbl(Address(rsp, j_farg7_off * BytesPerInt), j_farg7);\n+  __ movdbl(Address(rsp, j_farg6_off * BytesPerInt), j_farg6);\n+  __ movdbl(Address(rsp, j_farg5_off * BytesPerInt), j_farg5);\n+  __ movdbl(Address(rsp, j_farg4_off * BytesPerInt), j_farg4);\n+  __ movdbl(Address(rsp, j_farg3_off * BytesPerInt), j_farg3);\n+  __ movdbl(Address(rsp, j_farg2_off * BytesPerInt), j_farg2);\n+  __ movdbl(Address(rsp, j_farg1_off * BytesPerInt), j_farg1);\n+  __ movdbl(Address(rsp, j_farg0_off * BytesPerInt), j_farg0);\n+\n+  __ movptr(Address(rsp, j_rarg0_off * BytesPerInt), j_rarg0);\n+  __ movptr(Address(rsp, j_rarg1_off * BytesPerInt), j_rarg1);\n+  __ movptr(Address(rsp, j_rarg2_off * BytesPerInt), j_rarg2);\n+  __ movptr(Address(rsp, j_rarg3_off * BytesPerInt), j_rarg3);\n+  __ movptr(Address(rsp, j_rarg4_off * BytesPerInt), j_rarg4);\n+  __ movptr(Address(rsp, j_rarg5_off * BytesPerInt), j_rarg5);\n+  __ movptr(Address(rsp, rax_off * BytesPerInt), rax);\n+\n+  int frame_complete = __ offset();\n+\n+  __ set_last_Java_frame(noreg, noreg, nullptr, rscratch1);\n+\n+  __ mov(c_rarg0, r15_thread);\n+  __ mov(c_rarg1, rax);\n+\n+  __ call(RuntimeAddress(destination));\n+\n+  \/\/ Set an oopmap for the call site.\n+\n+  oop_maps->add_gc_map( __ offset() - start, map);\n+\n+  \/\/ clear last_Java_sp\n+  __ reset_last_Java_frame(false);\n+\n+  __ movptr(rbp, Address(rsp, rbp_off * BytesPerInt));\n+  __ movdbl(j_farg7, Address(rsp, j_farg7_off * BytesPerInt));\n+  __ movdbl(j_farg6, Address(rsp, j_farg6_off * BytesPerInt));\n+  __ movdbl(j_farg5, Address(rsp, j_farg5_off * BytesPerInt));\n+  __ movdbl(j_farg4, Address(rsp, j_farg4_off * BytesPerInt));\n+  __ movdbl(j_farg3, Address(rsp, j_farg3_off * BytesPerInt));\n+  __ movdbl(j_farg2, Address(rsp, j_farg2_off * BytesPerInt));\n+  __ movdbl(j_farg1, Address(rsp, j_farg1_off * BytesPerInt));\n+  __ movdbl(j_farg0, Address(rsp, j_farg0_off * BytesPerInt));\n+\n+  __ movptr(j_rarg0, Address(rsp, j_rarg0_off * BytesPerInt));\n+  __ movptr(j_rarg1, Address(rsp, j_rarg1_off * BytesPerInt));\n+  __ movptr(j_rarg2, Address(rsp, j_rarg2_off * BytesPerInt));\n+  __ movptr(j_rarg3, Address(rsp, j_rarg3_off * BytesPerInt));\n+  __ movptr(j_rarg4, Address(rsp, j_rarg4_off * BytesPerInt));\n+  __ movptr(j_rarg5, Address(rsp, j_rarg5_off * BytesPerInt));\n+  __ movptr(rax, Address(rsp, rax_off * BytesPerInt));\n+\n+  __ addptr(rsp, frame_size_in_bytes-8);\n+\n+  \/\/ check for pending exceptions\n+  Label pending;\n+  __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+  __ jcc(Assembler::notEqual, pending);\n+\n+  if (has_res) {\n+    __ get_vm_result_oop(rax);\n+  }\n+\n+  __ ret(0);\n+\n+  __ bind(pending);\n+\n+  __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+  __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+  \/\/ -------------\n+  \/\/ make sure all code is generated\n+  _masm->flush();\n+\n+  RuntimeStub* stub = RuntimeStub::new_runtime_stub(name, &buffer, frame_complete, frame_size_in_words, oop_maps, false);\n+  return stub->entry_point();\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":248,"deletions":21,"binary":false,"changes":269,"status":"modified"},{"patch":"@@ -625,0 +625,3 @@\n+  \/\/ interpreter or compiled code marshalling registers to\/from inline type instance\n+  address generate_return_value_stub(address destination, const char* name, bool has_res);\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -261,0 +261,77 @@\n+class LoadFlattenedArrayStub: public CodeStub {\n+ private:\n+  LIR_Opr          _array;\n+  LIR_Opr          _index;\n+  LIR_Opr          _result;\n+  LIR_Opr          _scratch_reg;\n+  CodeEmitInfo*    _info;\n+\n+ public:\n+  LoadFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr result, CodeEmitInfo* info);\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual CodeEmitInfo* info() const             { return _info; }\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_slow_case(_info);\n+    visitor->do_input(_array);\n+    visitor->do_input(_index);\n+    visitor->do_output(_result);\n+    if (_scratch_reg != LIR_OprFact::illegalOpr) {\n+      visitor->do_temp(_scratch_reg);\n+    }\n+  }\n+\n+#ifndef PRODUCT\n+  virtual void print_name(outputStream* out) const { out->print(\"LoadFlattenedArrayStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n+\n+class StoreFlattenedArrayStub: public CodeStub {\n+ private:\n+  LIR_Opr          _array;\n+  LIR_Opr          _index;\n+  LIR_Opr          _value;\n+  LIR_Opr          _scratch_reg;\n+  CodeEmitInfo*    _info;\n+\n+ public:\n+  StoreFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr value, CodeEmitInfo* info);\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual CodeEmitInfo* info() const             { return _info; }\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_slow_case(_info);\n+    visitor->do_input(_array);\n+    visitor->do_input(_index);\n+    visitor->do_input(_value);\n+    if (_scratch_reg != LIR_OprFact::illegalOpr) {\n+      visitor->do_temp(_scratch_reg);\n+    }\n+  }\n+#ifndef PRODUCT\n+  virtual void print_name(outputStream* out) const { out->print(\"StoreFlattenedArrayStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n+class SubstitutabilityCheckStub: public CodeStub {\n+ private:\n+  LIR_Opr          _left;\n+  LIR_Opr          _right;\n+  LIR_Opr          _scratch_reg;\n+  CodeEmitInfo*    _info;\n+ public:\n+  SubstitutabilityCheckStub(LIR_Opr left, LIR_Opr right, CodeEmitInfo* info);\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual CodeEmitInfo* info() const             { return _info; }\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_slow_case(_info);\n+    visitor->do_input(_left);\n+    visitor->do_input(_right);\n+    if (_scratch_reg != LIR_OprFact::illegalOpr) {\n+      visitor->do_temp(_scratch_reg);\n+    }\n+  }\n+#ifndef PRODUCT\n+  virtual void print_name(outputStream* out) const { out->print(\"SubstitutabilityCheckStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n@@ -313,1 +390,1 @@\n-\n+  bool           _is_null_free;\n@@ -315,1 +392,1 @@\n-  NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info);\n+  NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info, bool is_null_free);\n@@ -350,0 +427,2 @@\n+  CodeStub* _throw_ie_stub;\n+  LIR_Opr _scratch_reg;\n@@ -352,1 +431,2 @@\n-  MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info)\n+  MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info,\n+                   CodeStub* throw_ie_stub = nullptr, LIR_Opr scratch_reg = LIR_OprFact::illegalOpr)\n@@ -355,0 +435,5 @@\n+    _scratch_reg = scratch_reg;\n+    _throw_ie_stub = throw_ie_stub;\n+    if (_throw_ie_stub != nullptr) {\n+      assert(_scratch_reg != LIR_OprFact::illegalOpr, \"must be\");\n+    }\n@@ -364,0 +449,3 @@\n+    if (_scratch_reg != LIR_OprFact::illegalOpr) {\n+      visitor->do_temp(_scratch_reg);\n+    }\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":91,"deletions":3,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -120,0 +122,1 @@\n+  _verified_inline_entry.reset();\n@@ -330,1 +333,0 @@\n-\n@@ -340,2 +342,1 @@\n-\n-void LIR_Assembler::add_call_info(int pc_offset, CodeEmitInfo* cinfo) {\n+void LIR_Assembler::add_call_info(int pc_offset, CodeEmitInfo* cinfo, bool maybe_return_as_fields) {\n@@ -343,1 +344,1 @@\n-  cinfo->record_debug_info(compilation()->debug_info_recorder(), pc_offset);\n+  cinfo->record_debug_info(compilation()->debug_info_recorder(), pc_offset, maybe_return_as_fields);\n@@ -481,0 +482,6 @@\n+\n+  ciInlineKlass* vk = nullptr;\n+  if (op->maybe_return_as_fields(&vk)) {\n+    int offset = store_inline_type_fields_to_buf(vk);\n+    add_call_info(offset, op->info(), true);\n+  }\n@@ -570,0 +577,135 @@\n+void LIR_Assembler::add_scalarized_entry_info(int pc_offset) {\n+  flush_debug_info(pc_offset);\n+  DebugInformationRecorder* debug_info = compilation()->debug_info_recorder();\n+  \/\/ The VEP and VIEP(RO) of a C1-compiled method call buffer_inline_args_xxx()\n+  \/\/ before doing any argument shuffling. This call may cause GC. When GC happens,\n+  \/\/ all the parameters are still as passed by the caller, so we just use\n+  \/\/ map->set_include_argument_oops() inside frame::sender_for_compiled_frame(RegisterMap* map).\n+  \/\/ There's no need to build a GC map here.\n+  OopMap* oop_map = new OopMap(0, 0);\n+  debug_info->add_safepoint(pc_offset, oop_map);\n+  DebugToken* locvals = debug_info->create_scope_values(nullptr); \/\/ FIXME is this needed (for Java debugging to work properly??)\n+  DebugToken* expvals = debug_info->create_scope_values(nullptr); \/\/ FIXME is this needed (for Java debugging to work properly??)\n+  DebugToken* monvals = debug_info->create_monitor_values(nullptr); \/\/ FIXME: need testing with synchronized method\n+  bool reexecute = false;\n+  bool return_oop = false; \/\/ This flag will be ignored since it used only for C2 with escape analysis.\n+  bool rethrow_exception = false;\n+  bool is_method_handle_invoke = false;\n+  debug_info->describe_scope(pc_offset, methodHandle(), method(), 0, reexecute, rethrow_exception, is_method_handle_invoke, return_oop, false, locvals, expvals, monvals);\n+  debug_info->end_safepoint(pc_offset);\n+}\n+\n+\/\/ The entries points of C1-compiled methods can have the following types:\n+\/\/ (1) Methods with no inline type args\n+\/\/ (2) Methods with inline type receiver but no inline type args\n+\/\/     VIEP_RO is the same as VIEP\n+\/\/ (3) Methods with non-inline type receiver and some inline type args\n+\/\/     VIEP_RO is the same as VEP\n+\/\/ (4) Methods with inline type receiver and other inline type args\n+\/\/     Separate VEP, VIEP and VIEP_RO\n+\/\/\n+\/\/ (1)               (2)                 (3)                    (4)\n+\/\/ UEP\/UIEP:         VEP:                UEP:                   UEP:\n+\/\/   check_icache      pack receiver       check_icache           check_icache\n+\/\/ VEP\/VIEP\/VIEP_RO    jump to VIEP      VEP\/VIEP_RO:           VIEP_RO:\n+\/\/   body            UEP\/UIEP:             pack inline args       pack inline args (except receiver)\n+\/\/                     check_icache        jump to VIEP           jump to VIEP\n+\/\/                   VIEP\/VIEP_RO        UIEP:                  VEP:\n+\/\/                     body                check_icache           pack all inline args\n+\/\/                                       VIEP:                    jump to VIEP\n+\/\/                                         body                 UIEP:\n+\/\/                                                                check_icache\n+\/\/                                                              VIEP:\n+\/\/                                                                body\n+void LIR_Assembler::emit_std_entries() {\n+  offsets()->set_value(CodeOffsets::OSR_Entry, _masm->offset());\n+\n+  _masm->align(CodeEntryAlignment);\n+  const CompiledEntrySignature* ces = compilation()->compiled_entry_signature();\n+  if (ces->has_scalarized_args()) {\n+    assert(InlineTypePassFieldsAsArgs && method()->get_Method()->has_scalarized_args(), \"must be\");\n+    CodeOffsets::Entries ro_entry_type = ces->c1_inline_ro_entry_type();\n+\n+    \/\/ UEP: check icache and fall-through\n+    if (ro_entry_type != CodeOffsets::Verified_Inline_Entry) {\n+      offsets()->set_value(CodeOffsets::Entry, _masm->offset());\n+      if (needs_icache(method())) {\n+        check_icache();\n+      }\n+    }\n+\n+    \/\/ VIEP_RO: pack all value parameters, except the receiver\n+    if (ro_entry_type == CodeOffsets::Verified_Inline_Entry_RO) {\n+      emit_std_entry(CodeOffsets::Verified_Inline_Entry_RO, ces);\n+    }\n+\n+    \/\/ VEP: pack all value parameters\n+    _masm->align(CodeEntryAlignment);\n+    emit_std_entry(CodeOffsets::Verified_Entry, ces);\n+\n+    \/\/ UIEP: check icache and fall-through\n+    _masm->align(CodeEntryAlignment);\n+    offsets()->set_value(CodeOffsets::Inline_Entry, _masm->offset());\n+    if (ro_entry_type == CodeOffsets::Verified_Inline_Entry) {\n+      \/\/ Special case if we have VIEP == VIEP(RO):\n+      \/\/ this means UIEP (called by C1) == UEP (called by C2).\n+      offsets()->set_value(CodeOffsets::Entry, _masm->offset());\n+    }\n+    if (needs_icache(method())) {\n+      check_icache();\n+    }\n+\n+    \/\/ VIEP: all value parameters are passed as refs - no packing.\n+    emit_std_entry(CodeOffsets::Verified_Inline_Entry, nullptr);\n+\n+    if (ro_entry_type != CodeOffsets::Verified_Inline_Entry_RO) {\n+      \/\/ The VIEP(RO) is the same as VEP or VIEP\n+      assert(ro_entry_type == CodeOffsets::Verified_Entry ||\n+             ro_entry_type == CodeOffsets::Verified_Inline_Entry, \"must be\");\n+      offsets()->set_value(CodeOffsets::Verified_Inline_Entry_RO,\n+                           offsets()->value(ro_entry_type));\n+    }\n+  } else {\n+    \/\/ All 3 entries are the same (no inline type packing)\n+    offsets()->set_value(CodeOffsets::Entry, _masm->offset());\n+    offsets()->set_value(CodeOffsets::Inline_Entry, _masm->offset());\n+    if (needs_icache(method())) {\n+      check_icache();\n+    }\n+    emit_std_entry(CodeOffsets::Verified_Inline_Entry, nullptr);\n+    offsets()->set_value(CodeOffsets::Verified_Entry, offsets()->value(CodeOffsets::Verified_Inline_Entry));\n+    offsets()->set_value(CodeOffsets::Verified_Inline_Entry_RO, offsets()->value(CodeOffsets::Verified_Inline_Entry));\n+  }\n+}\n+\n+void LIR_Assembler::emit_std_entry(CodeOffsets::Entries entry, const CompiledEntrySignature* ces) {\n+  offsets()->set_value(entry, _masm->offset());\n+  _masm->verified_entry(compilation()->directive()->BreakAtExecuteOption);\n+  switch (entry) {\n+  case CodeOffsets::Verified_Entry: {\n+    if (needs_clinit_barrier_on_entry(method())) {\n+      clinit_barrier(method());\n+    }\n+    int rt_call_offset = _masm->verified_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()->sp_offset_for_orig_pc()), _verified_inline_entry);\n+    add_scalarized_entry_info(rt_call_offset);\n+    break;\n+  }\n+  case CodeOffsets::Verified_Inline_Entry_RO: {\n+    assert(!needs_clinit_barrier_on_entry(method()), \"can't be static\");\n+    int rt_call_offset = _masm->verified_inline_ro_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()->sp_offset_for_orig_pc()), _verified_inline_entry);\n+    add_scalarized_entry_info(rt_call_offset);\n+    break;\n+  }\n+  case CodeOffsets::Verified_Inline_Entry: {\n+    if (needs_clinit_barrier_on_entry(method())) {\n+      clinit_barrier(method());\n+    }\n+    build_frame();\n+    offsets()->set_value(CodeOffsets::Frame_Complete, _masm->offset());\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n+    break;\n+  }\n+}\n@@ -582,15 +724,2 @@\n-    case lir_std_entry: {\n-      \/\/ init offsets\n-      offsets()->set_value(CodeOffsets::OSR_Entry, _masm->offset());\n-      if (needs_icache(compilation()->method())) {\n-        int offset = check_icache();\n-        offsets()->set_value(CodeOffsets::Entry, offset);\n-      }\n-      _masm->align(CodeEntryAlignment);\n-      offsets()->set_value(CodeOffsets::Verified_Entry, _masm->offset());\n-      _masm->verified_entry(compilation()->directive()->BreakAtExecuteOption);\n-      if (needs_clinit_barrier_on_entry(compilation()->method())) {\n-        clinit_barrier(compilation()->method());\n-      }\n-      build_frame();\n-      offsets()->set_value(CodeOffsets::Frame_Complete, _masm->offset());\n+    case lir_std_entry:\n+      emit_std_entries();\n@@ -598,1 +727,0 @@\n-    }\n@@ -645,0 +773,4 @@\n+    case lir_check_orig_pc:\n+      check_orig_pc();\n+      break;\n+\n@@ -730,1 +862,2 @@\n-  _masm->build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  _masm->build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()->sp_offset_for_orig_pc()),\n+                     needs_stack_repair(), method()->has_scalarized_args(), &_verified_inline_entry);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":154,"deletions":21,"binary":false,"changes":175,"status":"modified"},{"patch":"@@ -90,0 +90,65 @@\n+inline void CDSMustMatchFlags::do_print(outputStream* st, bool v) {\n+  st->print(\"%s\", v ? \"true\" : \"false\");\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, intx v) {\n+  st->print(\"%zd\", v);\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, uintx v) {\n+  st->print(\"%zu\", v);\n+}\n+\n+inline void CDSMustMatchFlags::do_print(outputStream* st, double v) {\n+  st->print(\"%f\", v);\n+}\n+\n+void CDSMustMatchFlags::init() {\n+  assert(CDSConfig::is_dumping_archive(), \"sanity\");\n+  _max_name_width = 0;\n+\n+#define INIT_CDS_MUST_MATCH_FLAG(n) \\\n+  _v_##n = n; \\\n+  _max_name_width = MAX2(_max_name_width,strlen(#n));\n+  CDS_MUST_MATCH_FLAGS_DO(INIT_CDS_MUST_MATCH_FLAG);\n+#undef INIT_CDS_MUST_MATCH_FLAG\n+}\n+\n+bool CDSMustMatchFlags::runtime_check() const {\n+#define CHECK_CDS_MUST_MATCH_FLAG(n) \\\n+  if (_v_##n != n) { \\\n+    ResourceMark rm; \\\n+    stringStream ss; \\\n+    ss.print(\"VM option %s is different between dumptime (\", #n);  \\\n+    do_print(&ss, _v_ ## n); \\\n+    ss.print(\") and runtime (\"); \\\n+    do_print(&ss, n); \\\n+    ss.print(\")\"); \\\n+    log_info(cds)(\"%s\", ss.as_string()); \\\n+    return false; \\\n+  }\n+  CDS_MUST_MATCH_FLAGS_DO(CHECK_CDS_MUST_MATCH_FLAG);\n+#undef CHECK_CDS_MUST_MATCH_FLAG\n+\n+  return true;\n+}\n+\n+void CDSMustMatchFlags::print_info() const {\n+  LogTarget(Info, cds) lt;\n+  if (lt.is_enabled()) {\n+    LogStream ls(lt);\n+    ls.print_cr(\"Recorded VM flags during dumptime:\");\n+    print(&ls);\n+  }\n+}\n+\n+void CDSMustMatchFlags::print(outputStream* st) const {\n+#define PRINT_CDS_MUST_MATCH_FLAG(n) \\\n+  st->print(\"- %-s \", #n);                   \\\n+  st->sp(int(_max_name_width - strlen(#n))); \\\n+  do_print(st, _v_##n);                      \\\n+  st->cr();\n+  CDS_MUST_MATCH_FLAGS_DO(PRINT_CDS_MUST_MATCH_FLAG);\n+#undef PRINT_CDS_MUST_MATCH_FLAG\n+}\n+\n@@ -248,0 +313,1 @@\n+  _has_valhalla_patched_classes = CDSConfig::is_valhalla_preview();\n@@ -261,0 +327,1 @@\n+  _must_match.init();\n@@ -319,0 +386,2 @@\n+  st->print_cr(\"- has_valhalla_patched_classes    %d\", _has_valhalla_patched_classes);\n+  _must_match.print(st);\n@@ -702,0 +771,4 @@\n+  if (!header()->check_must_match_flags()) {\n+    return false;\n+  }\n+\n@@ -2059,0 +2132,18 @@\n+  if (is_static()) {\n+    const char* err = nullptr;\n+    if (CDSConfig::is_valhalla_preview()) {\n+      if (!_has_valhalla_patched_classes) {\n+        err = \"not created\";\n+      }\n+    } else {\n+      if (_has_valhalla_patched_classes) {\n+        err = \"created\";\n+      }\n+    }\n+    if (err != nullptr) {\n+      log_warning(cds)(\"This archive was %s with --enable-preview -XX:+EnableValhalla. It is \"\n+                         \"incompatible with the current JVM setting\", err);\n+      return false;\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":91,"deletions":0,"binary":false,"changes":91,"status":"modified"},{"patch":"@@ -328,0 +328,14 @@\n+  do_intrinsic(_newNullRestrictedAtomicArray, jdk_internal_value_ValueClass, newNullRestrictedAtomicArray_name, newArray_signature3, F_SN) \\\n+   do_name(     newNullRestrictedAtomicArray_name,                \"newNullRestrictedAtomicArray\")                       \\\n+  do_intrinsic(_newNullRestrictedNonAtomicArray, jdk_internal_value_ValueClass, newNullRestrictedNonAtomicArray_name, newArray_signature3, F_SN) \\\n+   do_name(     newNullRestrictedNonAtomicArray_name,             \"newNullRestrictedNonAtomicArray\")                    \\\n+  do_intrinsic(_newNullableAtomicArray, jdk_internal_value_ValueClass, newNullableAtomicArray_name, newArray_signature2, F_SN) \\\n+   do_name(     newNullableAtomicArray_name,                      \"newNullableAtomicArray\")                             \\\n+   do_signature(newArray_signature2,                              \"(Ljava\/lang\/Class;I)[Ljava\/lang\/Object;\")            \\\n+   do_signature(newArray_signature3,                              \"(Ljava\/lang\/Class;ILjava\/lang\/Object;)[Ljava\/lang\/Object;\") \\\n+  do_intrinsic(_isFlatArray, jdk_internal_value_ValueClass, isFlatArray_name, object_boolean_signature, F_SN)           \\\n+   do_name(     isFlatArray_name,                                 \"isFlatArray\")                                        \\\n+  do_intrinsic(_isNullRestrictedArray, jdk_internal_value_ValueClass, isNullRestrictedArray_name, object_boolean_signature, F_SN) \\\n+   do_name(     isNullRestrictedArray_name,                       \"isNullRestrictedArray\")                              \\\n+  do_intrinsic(_isAtomicArray, jdk_internal_value_ValueClass, isAtomicArray_name, object_boolean_signature, F_SN)       \\\n+   do_name(     isAtomicArray_name,                               \"isAtomicArray\")                                      \\\n@@ -731,0 +745,4 @@\n+  do_signature(getValue_signature,        \"(Ljava\/lang\/Object;JLjava\/lang\/Class;)Ljava\/lang\/Object;\")                   \\\n+  do_signature(putValue_signature,        \"(Ljava\/lang\/Object;JLjava\/lang\/Class;Ljava\/lang\/Object;)V\")                  \\\n+  do_signature(getFlatValue_signature,    \"(Ljava\/lang\/Object;JILjava\/lang\/Class;)Ljava\/lang\/Object;\")                  \\\n+  do_signature(putFlatValue_signature,    \"(Ljava\/lang\/Object;JILjava\/lang\/Class;Ljava\/lang\/Object;)V\")                 \\\n@@ -741,0 +759,4 @@\n+  do_name(getValue_name,\"getValue\")             do_name(putValue_name,\"putValue\")                                       \\\n+  do_name(getFlatValue_name,\"getFlatValue\")     do_name(putFlatValue_name,\"putFlatValue\")                               \\\n+  do_name(makePrivateBuffer_name,\"makePrivateBuffer\")                                                                   \\\n+  do_name(finishPrivateBuffer_name,\"finishPrivateBuffer\")                                                               \\\n@@ -751,0 +773,2 @@\n+  do_intrinsic(_getValue,           jdk_internal_misc_Unsafe,     getValue_name, getValue_signature,             F_RN)  \\\n+  do_intrinsic(_getFlatValue,       jdk_internal_misc_Unsafe,     getFlatValue_name, getFlatValue_signature,     F_RN)  \\\n@@ -760,0 +784,5 @@\n+  do_intrinsic(_putValue,           jdk_internal_misc_Unsafe,     putValue_name, putValue_signature,             F_RN)  \\\n+  do_intrinsic(_putFlatValue,       jdk_internal_misc_Unsafe,     putFlatValue_name, putFlatValue_signature,     F_RN)  \\\n+                                                                                                                        \\\n+  do_intrinsic(_makePrivateBuffer,  jdk_internal_misc_Unsafe,     makePrivateBuffer_name, object_object_signature, F_RN)   \\\n+  do_intrinsic(_finishPrivateBuffer,  jdk_internal_misc_Unsafe,   finishPrivateBuffer_name, object_object_signature, F_RN) \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":29,"deletions":0,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -66,0 +66,1 @@\n+static_assert(!std::is_polymorphic<BufferedInlineTypeBlob>::value,   \"no virtual methods are allowed in code blobs\");\n@@ -94,0 +95,1 @@\n+      &BufferedInlineTypeBlob::_vpntr,\n@@ -429,1 +431,1 @@\n-    blob = new (size) BufferBlob(name, CodeBlobKind::Buffer, cb, size);\n+    blob = new (size) BufferBlob(name, CodeBlobKind::Buffer, cb, size, sizeof(BufferBlob));\n@@ -445,0 +447,4 @@\n+BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, uint16_t header_size, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments)\n+  : RuntimeBlob(name, kind, cb, size, header_size, frame_complete, frame_size, oop_maps, caller_must_gc_arguments)\n+{}\n+\n@@ -449,3 +455,2 @@\n-AdapterBlob::AdapterBlob(int size, CodeBuffer* cb, int entry_offset[AdapterBlob::ENTRY_COUNT]) :\n-  BufferBlob(\"I2C\/C2I adapters\", CodeBlobKind::Adapter, cb, size, sizeof(AdapterBlob)) {\n-  assert(entry_offset[I2C] == 0, \"sanity check\");\n+AdapterBlob::AdapterBlob(int size, CodeBuffer* cb, int entry_offset[AdapterBlob::ENTRY_COUNT], int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) :\n+  BufferBlob(\"I2C\/C2I adapters\", CodeBlobKind::Adapter, cb, size, sizeof(AdapterBlob), frame_complete, frame_size, oop_maps, caller_must_gc_arguments) {\n@@ -453,0 +458,1 @@\n+  assert(entry_offset[I2C] == 0, \"sanity check\");\n@@ -462,0 +468,2 @@\n+  _c2i_inline_offset = entry_offset[C2I_Inline];\n+  _c2i_inline_ro_offset = entry_offset[C2I_Inline_RO];\n@@ -463,0 +471,1 @@\n+  _c2i_unverified_inline_offset = entry_offset[C2I_Unverified_Inline];\n@@ -467,1 +476,1 @@\n-AdapterBlob* AdapterBlob::create(CodeBuffer* cb, int entry_offset[AdapterBlob::ENTRY_COUNT]) {\n+AdapterBlob* AdapterBlob::create(CodeBuffer* cb, int entry_offset[AdapterBlob::ENTRY_COUNT], int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) {\n@@ -476,1 +485,1 @@\n-    blob = new (size) AdapterBlob(size, cb, entry_offset);\n+    blob = new (size) AdapterBlob(size, cb, entry_offset, frame_complete, frame_size, oop_maps, caller_must_gc_arguments);\n@@ -557,0 +566,25 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Implementation of BufferedInlineTypeBlob\n+BufferedInlineTypeBlob::BufferedInlineTypeBlob(int size, CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off) :\n+  BufferBlob(\"buffered inline type\", CodeBlobKind::BufferedInlineType, cb, size, sizeof(BufferedInlineTypeBlob)),\n+  _pack_fields_off(pack_fields_off),\n+  _pack_fields_jobject_off(pack_fields_jobject_off),\n+  _unpack_fields_off(unpack_fields_off) {\n+  CodeCache::commit(this);\n+}\n+\n+BufferedInlineTypeBlob* BufferedInlineTypeBlob::create(CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off) {\n+  ThreadInVMfromUnknown __tiv;  \/\/ get to VM state in case we block on CodeCache_lock\n+\n+  BufferedInlineTypeBlob* blob = nullptr;\n+  unsigned int size = CodeBlob::allocation_size(cb, sizeof(BufferedInlineTypeBlob));\n+  {\n+    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+    blob = new (size) BufferedInlineTypeBlob(size, cb, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+  }\n+  \/\/ Track memory usage statistic after releasing CodeCache_lock\n+  MemoryService::track_code_cache_memory_usage();\n+\n+  return blob;\n+}\n+\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":40,"deletions":6,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -716,0 +716,11 @@\n+\n+      \/\/ If inline types are passed as fields, use the extended signature\n+      \/\/ which contains the types of all (oop) fields of the inline type.\n+      if (is_compiled_by_c2() && callee->has_scalarized_args()) {\n+        const GrowableArray<SigEntry>* sig = callee->adapter()->get_sig_cc();\n+        assert(sig != nullptr, \"sig should never be null\");\n+        TempNewSymbol tmp_sig = SigEntry::create_symbol(sig);\n+        has_receiver = false; \/\/ The extended signature contains the receiver type\n+        fr.oops_compiled_arguments_do(tmp_sig, has_receiver, has_appendix, reg_map, f);\n+        return;\n+      }\n@@ -1246,0 +1257,4 @@\n+  _inline_entry_point             = entry_point();\n+  _verified_inline_entry_point    = verified_entry_point();\n+  _verified_inline_ro_entry_point = verified_entry_point();\n+\n@@ -1285,1 +1300,1 @@\n-\n+    assert(!method->has_scalarized_args(), \"scalarized native wrappers not supported yet\");\n@@ -1736,0 +1751,4 @@\n+    _inline_entry_point             = code_begin() + offsets->value(CodeOffsets::Inline_Entry);\n+    _verified_inline_entry_point    = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry);\n+    _verified_inline_ro_entry_point = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry_RO);\n+\n@@ -3186,4 +3205,4 @@\n-    if (deps.type() != Dependencies::evol_method)\n-      continue;\n-    Method* method = deps.method_argument(0);\n-    if (method == dependee) return true;\n+    if (Dependencies::has_method_dep(deps.type())) {\n+      Method* method = deps.method_argument(0);\n+      if (method == dependee) return true;\n+    }\n@@ -4027,0 +4046,1 @@\n+  if (pos == inline_entry_point())                                      label = \"[Inline Entry Point]\";\n@@ -4028,0 +4048,2 @@\n+  if (pos == verified_inline_entry_point())                             label = \"[Verified Inline Entry Point]\";\n+  if (pos == verified_inline_ro_entry_point())                          label = \"[Verified Inline Entry Point (RO)]\";\n@@ -4036,0 +4058,10 @@\n+static int maybe_print_entry_label(outputStream* stream, address pos, address entry, const char* label) {\n+  if (pos == entry) {\n+    stream->bol();\n+    stream->print_cr(\"%s\", label);\n+    return 1;\n+  } else {\n+    return 0;\n+  }\n+}\n+\n@@ -4038,4 +4070,13 @@\n-    const char* label = nmethod_section_label(block_begin);\n-    if (label != nullptr) {\n-      stream->bol();\n-      stream->print_cr(\"%s\", label);\n+    int n = 0;\n+    \/\/ Multiple entry points may be at the same position. Print them all.\n+    n += maybe_print_entry_label(stream, block_begin, entry_point(),                    \"[Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, inline_entry_point(),             \"[Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_entry_point(),           \"[Verified Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_entry_point(),    \"[Verified Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_ro_entry_point(), \"[Verified Inline Entry Point (RO)]\");\n+    if (n == 0) {\n+      const char* label = nmethod_section_label(block_begin);\n+      if (label != nullptr) {\n+        stream->bol();\n+        stream->print_cr(\"%s\", label);\n+      }\n@@ -4045,6 +4086,62 @@\n-  if (block_begin == entry_point()) {\n-    Method* m = method();\n-    if (m != nullptr) {\n-      stream->print(\"  # \");\n-      m->print_value_on(stream);\n-      stream->cr();\n+  Method* m = method();\n+  if (m == nullptr || is_osr_method()) {\n+    return;\n+  }\n+\n+  \/\/ Print the name of the method (only once)\n+  address low = MIN3(entry_point(),\n+                     verified_entry_point(),\n+                     inline_entry_point());\n+  \/\/ The verified inline entry point and verified inline RO entry point are not always\n+  \/\/ used. When they are unused. CodeOffsets::Verified_Inline_Entry(_RO) is -1. Hence,\n+  \/\/ the calculated entry point is smaller than the block they are offsetting into.\n+  if (verified_inline_entry_point() >= block_begin) {\n+    low = MIN2(low, verified_inline_entry_point());\n+  }\n+  if (verified_inline_ro_entry_point() >= block_begin) {\n+    low = MIN2(low, verified_inline_ro_entry_point());\n+  }\n+  assert(low != 0, \"sanity\");\n+  if (block_begin == low) {\n+    stream->print(\"  # \");\n+    m->print_value_on(stream);\n+    stream->cr();\n+  }\n+\n+  \/\/ Print the arguments for the 3 types of verified entry points\n+  CompiledEntrySignature ces(m);\n+  ces.compute_calling_conventions(false);\n+  const GrowableArray<SigEntry>* sig_cc;\n+  const VMRegPair* regs;\n+  if (block_begin == verified_entry_point()) {\n+    sig_cc = ces.sig_cc();\n+    regs = ces.regs_cc();\n+  } else if (block_begin == verified_inline_entry_point()) {\n+    sig_cc = ces.sig();\n+    regs = ces.regs();\n+  } else if (block_begin == verified_inline_ro_entry_point()) {\n+    sig_cc = ces.sig_cc_ro();\n+    regs = ces.regs_cc_ro();\n+  } else {\n+    return;\n+  }\n+\n+  bool has_this = !m->is_static();\n+  if (ces.has_inline_recv() && block_begin == verified_entry_point()) {\n+    \/\/ <this> argument is scalarized for verified_entry_point()\n+    has_this = false;\n+  }\n+  const char* spname = \"sp\"; \/\/ make arch-specific?\n+  int stack_slot_offset = this->frame_size() * wordSize;\n+  int tab1 = 14, tab2 = 24;\n+  int sig_index = 0;\n+  int arg_index = has_this ? -1 : 0;\n+  bool did_old_sp = false;\n+  for (ExtendedSignature sig = ExtendedSignature(sig_cc, SigEntryFilter()); !sig.at_end(); ++sig) {\n+    bool at_this = (arg_index == -1);\n+    bool at_old_sp = false;\n+    BasicType t = (*sig)._bt;\n+    if (at_this) {\n+      stream->print(\"  # this: \");\n+    } else {\n+      stream->print(\"  # parm%d: \", arg_index);\n@@ -4052,19 +4149,7 @@\n-    if (m != nullptr && !is_osr_method()) {\n-      ResourceMark rm;\n-      int sizeargs = m->size_of_parameters();\n-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n-      VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n-      {\n-        int sig_index = 0;\n-        if (!m->is_static())\n-          sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n-        for (SignatureStream ss(m->signature()); !ss.at_return_type(); ss.next()) {\n-          BasicType t = ss.type();\n-          sig_bt[sig_index++] = t;\n-          if (type2size[t] == 2) {\n-            sig_bt[sig_index++] = T_VOID;\n-          } else {\n-            assert(type2size[t] == 1, \"size is 1 or 2\");\n-          }\n-        }\n-        assert(sig_index == sizeargs, \"\");\n+    stream->move_to(tab1);\n+    VMReg fst = regs[sig_index].first();\n+    VMReg snd = regs[sig_index].second();\n+    if (fst->is_reg()) {\n+      stream->print(\"%s\", fst->name());\n+      if (snd->is_valid())  {\n+        stream->print(\":%s\", snd->name());\n@@ -4072,54 +4157,18 @@\n-      const char* spname = \"sp\"; \/\/ make arch-specific?\n-      SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n-      int stack_slot_offset = this->frame_size() * wordSize;\n-      int tab1 = 14, tab2 = 24;\n-      int sig_index = 0;\n-      int arg_index = (m->is_static() ? 0 : -1);\n-      bool did_old_sp = false;\n-      for (SignatureStream ss(m->signature()); !ss.at_return_type(); ) {\n-        bool at_this = (arg_index == -1);\n-        bool at_old_sp = false;\n-        BasicType t = (at_this ? T_OBJECT : ss.type());\n-        assert(t == sig_bt[sig_index], \"sigs in sync\");\n-        if (at_this)\n-          stream->print(\"  # this: \");\n-        else\n-          stream->print(\"  # parm%d: \", arg_index);\n-        stream->move_to(tab1);\n-        VMReg fst = regs[sig_index].first();\n-        VMReg snd = regs[sig_index].second();\n-        if (fst->is_reg()) {\n-          stream->print(\"%s\", fst->name());\n-          if (snd->is_valid())  {\n-            stream->print(\":%s\", snd->name());\n-          }\n-        } else if (fst->is_stack()) {\n-          int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n-          if (offset == stack_slot_offset)  at_old_sp = true;\n-          stream->print(\"[%s+0x%x]\", spname, offset);\n-        } else {\n-          stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n-        }\n-        stream->print(\" \");\n-        stream->move_to(tab2);\n-        stream->print(\"= \");\n-        if (at_this) {\n-          m->method_holder()->print_value_on(stream);\n-        } else {\n-          bool did_name = false;\n-          if (!at_this && ss.is_reference()) {\n-            Symbol* name = ss.as_symbol();\n-            name->print_value_on(stream);\n-            did_name = true;\n-          }\n-          if (!did_name)\n-            stream->print(\"%s\", type2name(t));\n-        }\n-        if (at_old_sp) {\n-          stream->print(\"  (%s of caller)\", spname);\n-          did_old_sp = true;\n-        }\n-        stream->cr();\n-        sig_index += type2size[t];\n-        arg_index += 1;\n-        if (!at_this)  ss.next();\n+    } else if (fst->is_stack()) {\n+      int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n+      if (offset == stack_slot_offset)  at_old_sp = true;\n+      stream->print(\"[%s+0x%x]\", spname, offset);\n+    } else {\n+      stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n+    }\n+    stream->print(\" \");\n+    stream->move_to(tab2);\n+    stream->print(\"= \");\n+    if (at_this) {\n+      m->method_holder()->print_value_on(stream);\n+    } else {\n+      bool did_name = false;\n+      if (is_reference_type(t)) {\n+        Symbol* name = (*sig)._name;\n+        name->print_value_on(stream);\n+        did_name = true;\n@@ -4127,6 +4176,4 @@\n-      if (!did_old_sp) {\n-        stream->print(\"  # \");\n-        stream->move_to(tab1);\n-        stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n-        stream->print(\"  (%s of caller)\", spname);\n-        stream->cr();\n+      if (!did_name)\n+        stream->print(\"%s\", type2name(t));\n+      if ((*sig)._null_marker) {\n+        stream->print(\" (null marker)\");\n@@ -4135,0 +4182,14 @@\n+    if (at_old_sp) {\n+      stream->print(\"  (%s of caller)\", spname);\n+      did_old_sp = true;\n+    }\n+    stream->cr();\n+    sig_index += type2size[t];\n+    arg_index += 1;\n+  }\n+  if (!did_old_sp) {\n+    stream->print(\"  # \");\n+    stream->move_to(tab1);\n+    stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n+    stream->print(\"  (%s of caller)\", spname);\n+    stream->cr();\n@@ -4258,1 +4319,1 @@\n-      st->print(\" {reexecute=%d rethrow=%d return_oop=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop());\n+      st->print(\" {reexecute=%d rethrow=%d return_oop=%d return_scalarized=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop(), sd->return_scalarized());\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":156,"deletions":95,"binary":false,"changes":251,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"compiler\/compilerDefinitions.hpp\"\n@@ -218,0 +219,4 @@\n+  \/\/ TODO: can these be uint16_t, seem rely on -1 CodeOffset, can change later...\n+  address _inline_entry_point;              \/\/ inline type entry point (unpack all inline type args) with class check\n+  address _verified_inline_entry_point;     \/\/ inline type entry point (unpack all inline type args) without class check\n+  address _verified_inline_ro_entry_point;  \/\/ inline type entry point (unpack receiver only) without class check\n@@ -696,0 +701,3 @@\n+  address inline_entry_point() const              { return _inline_entry_point; }             \/\/ inline type entry point (unpack all inline type args)\n+  address verified_inline_entry_point() const     { return _verified_inline_entry_point; }    \/\/ inline type entry point (unpack all inline type args) without class check\n+  address verified_inline_ro_entry_point() const  { return _verified_inline_ro_entry_point; } \/\/ inline type entry point (only unpack receiver) without class check\n@@ -762,0 +770,10 @@\n+  bool  needs_stack_repair() const {\n+    if (is_compiled_by_c1()) {\n+      return method()->c1_needs_stack_repair();\n+    } else if (is_compiled_by_c2()) {\n+      return method()->c2_needs_stack_repair();\n+    } else {\n+      return false;\n+    }\n+  }\n+\n@@ -1058,3 +1076,4 @@\n-  \/\/ Fast breakpoint support. Tells if this compiled method is\n-  \/\/ dependent on the given method. Returns true if this nmethod\n-  \/\/ corresponds to the given method as well.\n+  \/\/ Tells if this compiled method is dependent on the given method.\n+  \/\/ Returns true if this nmethod corresponds to the given method as well.\n+  \/\/ It is used for fast breakpoint support and updating the calling convention\n+  \/\/ in case of mismatch.\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":22,"deletions":3,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -130,1 +130,1 @@\n-    if (obj->is_objArray()) {\n+    if (obj->is_refArray()) {\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCompactionManager.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -450,1 +450,1 @@\n-        const TypeTuple* args = n->as_Call()->_tf->domain();\n+        const TypeTuple* args = n->as_Call()->_tf->domain_sig();\n@@ -571,1 +571,1 @@\n-      uint stop = n->is_Call() ? n->as_Call()->tf()->domain()->cnt() : n->req();\n+      uint stop = n->is_Call() ? n->as_Call()->tf()->domain_sig()->cnt() : n->req();\n@@ -806,6 +806,5 @@\n-        CallProjections projs;\n-        c->as_Call()->extract_projections(&projs, true, false);\n-        if (projs.fallthrough_memproj != nullptr) {\n-          if (projs.fallthrough_memproj->adr_type() == TypePtr::BOTTOM) {\n-            if (projs.catchall_memproj == nullptr) {\n-              mem = projs.fallthrough_memproj;\n+        CallProjections* projs = c->as_Call()->extract_projections(true, false);\n+        if (projs->fallthrough_memproj != nullptr) {\n+          if (projs->fallthrough_memproj->adr_type() == TypePtr::BOTTOM) {\n+            if (projs->catchall_memproj == nullptr) {\n+              mem = projs->fallthrough_memproj;\n@@ -813,2 +812,2 @@\n-              if (phase->is_dominator(projs.fallthrough_catchproj, ctrl)) {\n-                mem = projs.fallthrough_memproj;\n+              if (phase->is_dominator(projs->fallthrough_catchproj, ctrl)) {\n+                mem = projs->fallthrough_memproj;\n@@ -816,2 +815,2 @@\n-                assert(phase->is_dominator(projs.catchall_catchproj, ctrl), \"one proj must dominate barrier\");\n-                mem = projs.catchall_memproj;\n+                assert(phase->is_dominator(projs->catchall_catchproj, ctrl), \"one proj must dominate barrier\");\n+                mem = projs->catchall_memproj;\n@@ -1078,1 +1077,1 @@\n-static Node* create_phis_on_call_return(Node* ctrl, Node* c, Node* n, Node* n_clone, const CallProjections& projs, PhaseIdealLoop* phase) {\n+static Node* create_phis_on_call_return(Node* ctrl, Node* c, Node* n, Node* n_clone, const CallProjections* projs, PhaseIdealLoop* phase) {\n@@ -1090,1 +1089,1 @@\n-    if (phase->is_dominator(projs.fallthrough_catchproj, in)) {\n+    if (phase->is_dominator(projs->fallthrough_catchproj, in)) {\n@@ -1092,1 +1091,1 @@\n-    } else if (phase->is_dominator(projs.catchall_catchproj, in)) {\n+    } else if (phase->is_dominator(projs->catchall_catchproj, in)) {\n@@ -1195,2 +1194,1 @@\n-      CallProjections projs;\n-      call->extract_projections(&projs, false, false);\n+      CallProjections* projs = call->extract_projections(false, false);\n@@ -1199,1 +1197,1 @@\n-      if (projs.fallthrough_catchproj == nullptr) {\n+      if (projs->fallthrough_catchproj == nullptr) {\n@@ -1201,1 +1199,1 @@\n-        assert(projs.catchall_catchproj == nullptr, \"runtime call should not have catch all projection\");\n+        assert(projs->catchall_catchproj == nullptr, \"runtime call should not have catch all projection\");\n@@ -1210,2 +1208,2 @@\n-      phase->register_new_node(lrb_clone, projs.catchall_catchproj);\n-      phase->set_ctrl(lrb, projs.fallthrough_catchproj);\n+      phase->register_new_node(lrb_clone, projs->catchall_catchproj);\n+      phase->set_ctrl(lrb, projs->fallthrough_catchproj);\n@@ -1233,1 +1231,1 @@\n-          if (phase->is_dominator(call, c) && phase->is_dominator(c, projs.fallthrough_proj)) {\n+          if (phase->is_dominator(call, c) && phase->is_dominator(c, projs->fallthrough_proj)) {\n@@ -1241,1 +1239,1 @@\n-            phase->register_new_node(u_clone, projs.catchall_catchproj);\n+            phase->register_new_node(u_clone, projs->catchall_catchproj);\n@@ -1243,1 +1241,1 @@\n-            phase->set_ctrl(u, projs.fallthrough_catchproj);\n+            phase->set_ctrl(u, projs->fallthrough_catchproj);\n@@ -1249,1 +1247,1 @@\n-                  if (phase->is_dominator(projs.catchall_catchproj, u->in(0)->in(k))) {\n+                  if (phase->is_dominator(projs->catchall_catchproj, u->in(0)->in(k))) {\n@@ -1252,1 +1250,1 @@\n-                  } else if (!phase->is_dominator(projs.fallthrough_catchproj, u->in(0)->in(k))) {\n+                  } else if (!phase->is_dominator(projs->fallthrough_catchproj, u->in(0)->in(k))) {\n@@ -1259,1 +1257,1 @@\n-              if (phase->is_dominator(projs.catchall_catchproj, c)) {\n+              if (phase->is_dominator(projs->catchall_catchproj, c)) {\n@@ -1264,1 +1262,1 @@\n-              } else if (!phase->is_dominator(projs.fallthrough_catchproj, c)) {\n+              } else if (!phase->is_dominator(projs->fallthrough_catchproj, c)) {\n@@ -1855,5 +1853,4 @@\n-    CallProjections projs;\n-    call->extract_projections(&projs, true, false);\n-    if (projs.catchall_memproj != nullptr) {\n-      if (projs.fallthrough_memproj == n) {\n-        c = projs.fallthrough_catchproj;\n+    CallProjections* projs = call->extract_projections(true, false);\n+    if (projs->catchall_memproj != nullptr) {\n+      if (projs->fallthrough_memproj == n) {\n+        c = projs->fallthrough_catchproj;\n@@ -1861,2 +1858,2 @@\n-        assert(projs.catchall_memproj == n, \"\");\n-        c = projs.catchall_catchproj;\n+        assert(projs->catchall_memproj == n, \"\");\n+        c = projs->catchall_catchproj;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.cpp","additions":32,"deletions":35,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -68,0 +68,1 @@\n+    object_init,                                                \/\/ special barrier on entry\n","filename":"src\/hotspot\/share\/interpreter\/abstractInterpreter.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -47,0 +48,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -78,0 +82,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -228,0 +233,32 @@\n+JRT_ENTRY(void, InterpreterRuntime::read_flat_field(JavaThread* current, oopDesc* obj, ResolvedFieldEntry* entry))\n+  assert(oopDesc::is_oop(obj), \"Sanity check\");\n+  Handle obj_h(THREAD, obj);\n+\n+  InstanceKlass* holder = InstanceKlass::cast(entry->field_holder());\n+  assert(entry->field_holder()->field_is_flat(entry->field_index()), \"Sanity check\");\n+\n+  InlineLayoutInfo* layout_info = holder->inline_layout_info_adr(entry->field_index());\n+  InlineKlass* field_vklass = layout_info->klass();\n+\n+#ifdef ASSERT\n+  fieldDescriptor fd;\n+  bool found = holder->find_field_from_offset(entry->field_offset(), false, &fd);\n+  assert(found, \"Field not found\");\n+  assert(fd.is_flat(), \"Field must be flat\");\n+#endif \/\/ ASSERT\n+\n+  oop res = field_vklass->read_payload_from_addr(obj_h(), entry->field_offset(), layout_info->kind(), CHECK);\n+  current->set_vm_result_oop(res);\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::write_flat_field(JavaThread* current, oopDesc* obj, oopDesc* value, ResolvedFieldEntry* entry))\n+  assert(oopDesc::is_oop(obj), \"Sanity check\");\n+  Handle obj_h(THREAD, obj);\n+  assert(value == nullptr || oopDesc::is_oop(value), \"Sanity check\");\n+  Handle val_h(THREAD, value);\n+\n+  InstanceKlass* holder = entry->field_holder();\n+  InlineLayoutInfo* li = holder->inline_layout_info_adr(entry->field_index());\n+  InlineKlass* vk = li->klass();\n+  vk->write_value_to_addr(val_h(), ((char*)(oopDesc*)obj_h()) + entry->field_offset(), li->kind(), true, CHECK);\n+JRT_END\n@@ -237,1 +274,1 @@\n-  objArrayOop obj = oopFactory::new_objArray(klass, size, CHECK);\n+  arrayOop obj = oopFactory::new_objArray(klass, size, CHECK);\n@@ -241,0 +278,12 @@\n+JRT_ENTRY(void, InterpreterRuntime::flat_array_load(JavaThread* current, arrayOopDesc* array, int index))\n+  assert(array->is_flatArray(), \"Must be\");\n+  flatArrayOop farray = (flatArrayOop)array;\n+  oop res = farray->obj_at(index, CHECK);\n+  current->set_vm_result_oop(res);\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::flat_array_store(JavaThread* current, oopDesc* val, arrayOopDesc* array, int index))\n+  assert(array->is_flatArray(), \"Must be\");\n+  flatArrayOop farray = (flatArrayOop)array;\n+  farray->obj_at_put(index, val, CHECK);\n+JRT_END\n@@ -246,2 +295,2 @@\n-  int          i = last_frame.get_index_u2(Bytecodes::_multianewarray);\n-  Klass* klass   = constants->klass_at(i, CHECK);\n+  int i = last_frame.get_index_u2(Bytecodes::_multianewarray);\n+  Klass* klass = constants->klass_at(i, CHECK);\n@@ -276,0 +325,24 @@\n+JRT_ENTRY(jboolean, InterpreterRuntime::is_substitutable(JavaThread* current, oopDesc* aobj, oopDesc* bobj))\n+  assert(oopDesc::is_oop(aobj) && oopDesc::is_oop(bobj), \"must be valid oops\");\n+\n+  Handle ha(THREAD, aobj);\n+  Handle hb(THREAD, bobj);\n+  JavaValue result(T_BOOLEAN);\n+  JavaCallArguments args;\n+  args.push_oop(ha);\n+  args.push_oop(hb);\n+  methodHandle method(current, UseAltSubstitutabilityMethod ?  Universe::is_substitutableAlt_method() : Universe::is_substitutable_method());\n+  method->method_holder()->initialize(CHECK_false); \/\/ Ensure class ValueObjectMethods is initialized\n+  JavaCalls::call(&result, method, &args, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+    \/\/ Something really bad happened because isSubstitutable() should not throw exceptions\n+    \/\/ If it is an error, just let it propagate\n+    \/\/ If it is an exception, wrap it into an InternalError\n+    if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+      Handle e(THREAD, PENDING_EXCEPTION);\n+      CLEAR_PENDING_EXCEPTION;\n+      THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in substitutability test\", e, false);\n+    }\n+  }\n+  return result.get_jboolean();\n+JRT_END\n@@ -623,0 +696,4 @@\n+JRT_ENTRY(void, InterpreterRuntime::throw_InstantiationError(JavaThread* current))\n+  THROW(vmSymbols::java_lang_InstantiationError());\n+JRT_END\n+\n@@ -702,0 +779,1 @@\n+  bool strict_static_final = info.is_strict() && info.is_static() && info.is_final();\n@@ -706,1 +784,1 @@\n-  if (!uninitialized_static || VM_Version::supports_fast_class_init_checks()) {\n+  if (!uninitialized_static) {\n@@ -711,0 +789,13 @@\n+    assert(!info.is_strict_static_unset(), \"after initialization, no unset flags\");\n+  } else if (is_static && (info.is_strict_static_unset() || strict_static_final)) {\n+    \/\/ During <clinit>, closely track the state of strict statics.\n+    \/\/ 1. if we are reading an uninitialized strict static, throw\n+    \/\/ 2. if we are writing one, clear the \"unset\" flag\n+    \/\/\n+    \/\/ Note: If we were handling an attempted write of a null to a\n+    \/\/ null-restricted strict static, we would NOT clear the \"unset\"\n+    \/\/ flag.\n+    assert(klass->is_being_initialized(), \"else should have thrown\");\n+    assert(klass->is_reentrant_initialization(THREAD),\n+      \"<clinit> must be running in current thread\");\n+    klass->notify_strict_static_access(info.index(), is_put, CHECK);\n@@ -714,1 +805,4 @@\n-  entry->set_flags(info.access_flags().is_final(), info.access_flags().is_volatile());\n+  entry->set_flags(info.access_flags().is_final(), info.access_flags().is_volatile(),\n+                   info.is_flat(), info.is_null_free_inline_type(),\n+                   info.has_null_marker());\n+\n@@ -761,1 +855,0 @@\n-\n@@ -766,1 +859,0 @@\n-\n@@ -781,0 +873,15 @@\n+JRT_ENTRY(void, InterpreterRuntime::throw_identity_exception(JavaThread* current, oopDesc* obj))\n+  Klass* klass = cast_to_oop(obj)->klass();\n+  ResourceMark rm(THREAD);\n+  const char* desc = \"Cannot synchronize on an instance of value class \";\n+  const char* className = klass->external_name();\n+  size_t msglen = strlen(desc) + strlen(className) + 1;\n+  char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+  if (nullptr == message) {\n+    \/\/ Out of memory: can't create detailed error message\n+    THROW_MSG(vmSymbols::java_lang_IdentityException(), className);\n+  } else {\n+    jio_snprintf(message, msglen, \"%s%s\", desc, className);\n+    THROW_MSG(vmSymbols::java_lang_IdentityException(), message);\n+  }\n+JRT_END\n@@ -1192,0 +1299,1 @@\n+  assert(entry->is_valid(), \"Invalid ResolvedFieldEntry\");\n@@ -1199,0 +1307,1 @@\n+  bool is_flat = entry->is_flat();\n@@ -1207,1 +1316,1 @@\n-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(field_holder, entry->field_offset(), is_static);\n+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(field_holder, entry->field_offset(), is_static, is_flat);\n@@ -1215,0 +1324,1 @@\n+  assert(entry->is_valid(), \"Invalid ResolvedFieldEntry\");\n@@ -1236,0 +1346,1 @@\n+\n@@ -1237,0 +1348,1 @@\n+  bool is_flat = entry->is_flat();\n@@ -1239,1 +1351,1 @@\n-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, entry->field_offset(), is_static);\n+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, entry->field_offset(), is_static, is_flat);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":121,"deletions":9,"binary":false,"changes":130,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -61,1 +61,1 @@\n-  static void    anewarray     (JavaThread* current, ConstantPool* pool, int index, jint size);\n+  static void    anewarray     (JavaThread* threcurrentad, ConstantPool* pool, int index, jint size);\n@@ -64,0 +64,8 @@\n+  static void    write_heap_copy (JavaThread* current, oopDesc* value, int offset, oopDesc* rcv);\n+  static void    read_flat_field(JavaThread* current, oopDesc* object, ResolvedFieldEntry* entry);\n+  static void    write_flat_field(JavaThread* current, oopDesc* object, oopDesc* value, ResolvedFieldEntry* entry);\n+\n+  static void flat_array_load(JavaThread* current, arrayOopDesc* array, int index);\n+  static void flat_array_store(JavaThread* current, oopDesc* val, arrayOopDesc* array, int index);\n+\n+  static jboolean is_substitutable(JavaThread* current, oopDesc* aobj, oopDesc* bobj);\n@@ -75,0 +83,1 @@\n+  static void    throw_InstantiationError(JavaThread* current);\n@@ -124,0 +133,1 @@\n+  static void    throw_identity_exception(JavaThread* current, oopDesc* obj);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.hpp","additions":12,"deletions":2,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -1004,1 +1004,2 @@\n-  bool is_put    = (byte == Bytecodes::_putfield  || byte == Bytecodes::_putstatic || byte == Bytecodes::_nofast_putfield);\n+  bool is_put    = (byte == Bytecodes::_putfield  || byte == Bytecodes::_putstatic ||\n+                    byte == Bytecodes::_nofast_putfield);\n@@ -1047,2 +1048,2 @@\n-                 is_static ? \"static\" : \"non-static\", resolved_klass->external_name(), fd.name()->as_C_string(),\n-                current_klass->external_name());\n+                  is_static ? \"static\" : \"non-static\", resolved_klass->external_name(), fd.name()->as_C_string(),\n+                  current_klass->external_name());\n@@ -1057,1 +1058,1 @@\n-                                                   !m->is_static_initializer());\n+                                                   !m->is_class_initializer());\n@@ -1060,1 +1061,1 @@\n-                                                     !m->is_object_initializer());\n+                                                     !m->is_object_constructor());\n@@ -1189,0 +1190,2 @@\n+  \/\/ Since this method is never inherited from a super, any appearance here under\n+  \/\/ the wrong class would be an error.\n@@ -1255,1 +1258,1 @@\n-      \/\/ check if the method is not <init>\n+      \/\/ check if the method is not <init>, which is never inherited\n@@ -1719,2 +1722,2 @@\n-                             const methodHandle& attached_method,\n-                             Bytecodes::Code byte, TRAPS) {\n+                                  const methodHandle& attached_method,\n+                                  Bytecodes::Code byte, bool check_null_and_abstract, TRAPS) {\n@@ -1725,0 +1728,1 @@\n+  Klass* recv_klass = recv.is_null() ? defc : recv->klass();\n@@ -1727,2 +1731,2 @@\n-      resolve_virtual_call(result, recv, recv->klass(), link_info,\n-                           \/*check_null_and_abstract=*\/true, CHECK);\n+      resolve_virtual_call(result, recv, recv_klass, link_info,\n+                           check_null_and_abstract, CHECK);\n@@ -1731,2 +1735,2 @@\n-      resolve_interface_call(result, recv, recv->klass(), link_info,\n-                             \/*check_null_and_abstract=*\/true, CHECK);\n+      resolve_interface_call(result, recv, recv_klass, link_info,\n+                             check_null_and_abstract, CHECK);\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.cpp","additions":16,"deletions":12,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -361,1 +361,1 @@\n-                             Bytecodes::Code byte, TRAPS);\n+                             Bytecodes::Code byte, bool check_null_and_abstract, TRAPS);\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -165,0 +165,1 @@\n+  LOG_TAG(preload)   \/* Trace successfull class preloading *\/ \\\n@@ -219,0 +220,1 @@\n+  LOG_TAG(valuetypes) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -214,0 +214,1 @@\n+      invoke_code = Bytecodes::_invokevirtual;\n@@ -229,1 +230,1 @@\n-    method_entry->set_bytecode2(Bytecodes::_invokevirtual);\n+    method_entry->set_bytecode2(invoke_code);\n","filename":"src\/hotspot\/share\/oops\/cpCache.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -118,3 +120,7 @@\n-  Node* src = in(ArrayCopyNode::Src);\n-  const Type* src_type = phase->type(src);\n-\n+    Node* src = in(ArrayCopyNode::Src);\n+    const Type* src_type = phase->type(src);\n+\n+    if (src_type == Type::TOP) {\n+      return -1;\n+    }\n+\n@@ -144,0 +150,1 @@\n+             (UseArrayFlattening && ary_src->elem()->make_oopptr() != nullptr && ary_src->elem()->make_oopptr()->can_be_inline_type()) ||\n@@ -197,0 +204,1 @@\n+  phase->record_for_igvn(mem);\n@@ -296,1 +304,2 @@\n-    if (src_elem != dest_elem || dest_elem == T_VOID) {\n+    \/\/ TODO 8350865 What about atomicity?\n+    if (src_elem != dest_elem || ary_src->is_null_free() != ary_dest->is_null_free() || ary_src->is_flat() != ary_dest->is_flat() || dest_elem == T_VOID) {\n@@ -302,3 +311,4 @@\n-    if (bs->array_copy_requires_gc_barriers(is_alloc_tightly_coupled(), dest_elem, false, false, BarrierSetC2::Optimization)) {\n-      \/\/ It's an object array copy but we can't emit the card marking\n-      \/\/ that is needed\n+    if ((!ary_dest->is_flat() && bs->array_copy_requires_gc_barriers(is_alloc_tightly_coupled(), dest_elem, false, false, BarrierSetC2::Optimization)) ||\n+        (ary_dest->is_flat() && ary_src->elem()->inline_klass()->contains_oops() &&\n+         bs->array_copy_requires_gc_barriers(is_alloc_tightly_coupled(), T_OBJECT, false, false, BarrierSetC2::Optimization))) {\n+      \/\/ It's an object array copy but we can't emit the card marking that is needed\n@@ -311,1 +321,8 @@\n-    uint header = arrayOopDesc::base_offset_in_bytes(dest_elem);\n+    if (ary_dest->is_flat()) {\n+      assert(ary_src->is_flat(), \"src and dest must be flat\");\n+      shift = ary_src->flat_log_elem_size();\n+      src_elem = T_FLAT_ELEMENT;\n+      dest_elem = T_FLAT_ELEMENT;\n+    }\n+\n+    const uint header = arrayOopDesc::base_offset_in_bytes(dest_elem);\n@@ -350,0 +367,5 @@\n+    if (ary_src->elem()->make_oopptr() != nullptr &&\n+        ary_src->elem()->make_oopptr()->can_be_inline_type()) {\n+      return false;\n+    }\n+\n@@ -356,1 +378,4 @@\n-    if (bs->array_copy_requires_gc_barriers(true, elem, true, is_clone_inst(), BarrierSetC2::Optimization)) {\n+    if ((!ary_src->is_flat() && bs->array_copy_requires_gc_barriers(true, elem, true, is_clone_inst(), BarrierSetC2::Optimization)) ||\n+        (ary_src->is_flat() && ary_src->elem()->inline_klass()->contains_oops() &&\n+         bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, is_clone_inst(), BarrierSetC2::Optimization))) {\n+      \/\/ It's an object array copy but we can't emit the card marking that is needed\n@@ -380,1 +405,1 @@\n-const TypePtr* ArrayCopyNode::get_address_type(PhaseGVN* phase, const TypePtr* atp, Node* n) {\n+const TypeAryPtr* ArrayCopyNode::get_address_type(PhaseGVN* phase, const TypePtr* atp, Node* n) {\n@@ -385,1 +410,1 @@\n-  return atp->add_offset(Type::OffsetBot);\n+  return atp->add_offset(Type::OffsetBot)->is_aryptr();\n@@ -388,2 +413,2 @@\n-void ArrayCopyNode::array_copy_test_overlap(PhaseGVN *phase, bool can_reshape, bool disjoint_bases, int count, Node*& forward_ctl, Node*& backward_ctl) {\n-  Node* ctl = in(TypeFunc::Control);\n+void ArrayCopyNode::array_copy_test_overlap(GraphKit& kit, bool disjoint_bases, int count, Node*& backward_ctl) {\n+  Node* ctl = kit.control();\n@@ -391,0 +416,1 @@\n+    PhaseGVN& gvn = kit.gvn();\n@@ -394,2 +420,2 @@\n-    Node* cmp = phase->transform(new CmpINode(src_offset, dest_offset));\n-    Node *bol = phase->transform(new BoolNode(cmp, BoolTest::lt));\n+    Node* cmp = gvn.transform(new CmpINode(src_offset, dest_offset));\n+    Node *bol = gvn.transform(new BoolNode(cmp, BoolTest::lt));\n@@ -398,1 +424,1 @@\n-    phase->transform(iff);\n+    gvn.transform(iff);\n@@ -400,2 +426,34 @@\n-    forward_ctl = phase->transform(new IfFalseNode(iff));\n-    backward_ctl = phase->transform(new IfTrueNode(iff));\n+    kit.set_control(gvn.transform(new IfFalseNode(iff)));\n+    backward_ctl = gvn.transform(new IfTrueNode(iff));\n+  }\n+}\n+\n+void ArrayCopyNode::copy(GraphKit& kit,\n+                         const TypeAryPtr* atp_src,\n+                         const TypeAryPtr* atp_dest,\n+                         int i,\n+                         Node* base_src,\n+                         Node* base_dest,\n+                         Node* adr_src,\n+                         Node* adr_dest,\n+                         BasicType copy_type,\n+                         const Type* value_type) {\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  Node* ctl = kit.control();\n+  if (atp_dest->is_flat()) {\n+    ciInlineKlass* vk = atp_src->elem()->inline_klass();\n+    for (int j = 0; j < vk->nof_nonstatic_fields(); j++) {\n+      ciField* field = vk->nonstatic_field_at(j);\n+      int off_in_vt = field->offset_in_bytes() - vk->payload_offset();\n+      Node* off  = kit.MakeConX(off_in_vt + i * atp_src->flat_elem_size());\n+      ciType* ft = field->type();\n+      BasicType bt = type2field[ft->basic_type()];\n+      assert(!field->is_flat(), \"flat field encountered\");\n+      const Type* rt = Type::get_const_type(ft);\n+      const TypePtr* adr_type = atp_src->with_field_offset(off_in_vt)->add_offset(Type::OffsetBot);\n+      assert(!bs->array_copy_requires_gc_barriers(is_alloc_tightly_coupled(), bt, false, false, BarrierSetC2::Optimization), \"GC barriers required\");\n+      Node* next_src = kit.gvn().transform(new AddPNode(base_src, adr_src, off));\n+      Node* next_dest = kit.gvn().transform(new AddPNode(base_dest, adr_dest, off));\n+      Node* v = load(bs, &kit.gvn(), ctl, kit.merged_memory(), next_src, adr_type, rt, bt);\n+      store(bs, &kit.gvn(), ctl, kit.merged_memory(), next_dest, adr_type, v, rt, bt);\n+    }\n@@ -403,1 +461,5 @@\n-    forward_ctl = ctl;\n+    Node* off = kit.MakeConX(type2aelembytes(copy_type) * i);\n+    Node* next_src = kit.gvn().transform(new AddPNode(base_src, adr_src, off));\n+    Node* next_dest = kit.gvn().transform(new AddPNode(base_dest, adr_dest, off));\n+    Node* v = load(bs, &kit.gvn(), ctl, kit.merged_memory(), next_src, atp_src, value_type, copy_type);\n+    store(bs, &kit.gvn(), ctl, kit.merged_memory(), next_dest, atp_dest, v, value_type, copy_type);\n@@ -405,0 +467,1 @@\n+  kit.set_control(ctl);\n@@ -407,16 +470,13 @@\n-Node* ArrayCopyNode::array_copy_forward(PhaseGVN *phase,\n-                                        bool can_reshape,\n-                                        Node*& forward_ctl,\n-                                        Node* mem,\n-                                        const TypePtr* atp_src,\n-                                        const TypePtr* atp_dest,\n-                                        Node* adr_src,\n-                                        Node* base_src,\n-                                        Node* adr_dest,\n-                                        Node* base_dest,\n-                                        BasicType copy_type,\n-                                        const Type* value_type,\n-                                        int count) {\n-  if (!forward_ctl->is_top()) {\n-    \/\/ copy forward\n-    MergeMemNode* mm = MergeMemNode::make(mem);\n+void ArrayCopyNode::array_copy_forward(GraphKit& kit,\n+                                       bool can_reshape,\n+                                       const TypeAryPtr* atp_src,\n+                                       const TypeAryPtr* atp_dest,\n+                                       Node* adr_src,\n+                                       Node* base_src,\n+                                       Node* adr_dest,\n+                                       Node* base_dest,\n+                                       BasicType copy_type,\n+                                       const Type* value_type,\n+                                       int count) {\n+  if (!kit.stopped()) {\n+    \/\/ copy forward\n@@ -425,9 +485,2 @@\n-      BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n-      Node* v = load(bs, phase, forward_ctl, mm, adr_src, atp_src, value_type, copy_type);\n-      store(bs, phase, forward_ctl, mm, adr_dest, atp_dest, v, value_type, copy_type);\n-      for (int i = 1; i < count; i++) {\n-        Node* off  = phase->MakeConX(type2aelembytes(copy_type) * i);\n-        Node* next_src = phase->transform(new AddPNode(base_src,adr_src,off));\n-        Node* next_dest = phase->transform(new AddPNode(base_dest,adr_dest,off));\n-        v = load(bs, phase, forward_ctl, mm, next_src, atp_src, value_type, copy_type);\n-        store(bs, phase, forward_ctl, mm, next_dest, atp_dest, v, value_type, copy_type);\n+      for (int i = 0; i < count; i++) {\n+        copy(kit, atp_src, atp_dest, i, base_src, base_dest, adr_src, adr_dest, copy_type, value_type);\n@@ -436,3 +489,4 @@\n-      PhaseIterGVN* igvn = phase->is_IterGVN();\n-      igvn->_worklist.push(adr_src);\n-      igvn->_worklist.push(adr_dest);\n+      PhaseGVN& gvn = kit.gvn();\n+      assert(gvn.is_IterGVN(), \"\");\n+      gvn.record_for_igvn(adr_src);\n+      gvn.record_for_igvn(adr_dest);\n@@ -440,2 +494,0 @@\n-    return mm;\n-  return phase->C->top();\n@@ -445,14 +497,12 @@\n-Node* ArrayCopyNode::array_copy_backward(PhaseGVN *phase,\n-                                         bool can_reshape,\n-                                         Node*& backward_ctl,\n-                                         Node* mem,\n-                                         const TypePtr* atp_src,\n-                                         const TypePtr* atp_dest,\n-                                         Node* adr_src,\n-                                         Node* base_src,\n-                                         Node* adr_dest,\n-                                         Node* base_dest,\n-                                         BasicType copy_type,\n-                                         const Type* value_type,\n-                                         int count) {\n-  if (!backward_ctl->is_top()) {\n+void ArrayCopyNode::array_copy_backward(GraphKit& kit,\n+                                        bool can_reshape,\n+                                        const TypeAryPtr* atp_src,\n+                                        const TypeAryPtr* atp_dest,\n+                                        Node* adr_src,\n+                                        Node* base_src,\n+                                        Node* adr_dest,\n+                                        Node* base_dest,\n+                                        BasicType copy_type,\n+                                        const Type* value_type,\n+                                        int count) {\n+  if (!kit.stopped()) {\n@@ -460,4 +510,1 @@\n-    MergeMemNode* mm = MergeMemNode::make(mem);\n-\n-    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n-    assert(copy_type != T_OBJECT || !bs->array_copy_requires_gc_barriers(false, T_OBJECT, false, false, BarrierSetC2::Optimization), \"only tightly coupled allocations for object arrays\");\n+    PhaseGVN& gvn = kit.gvn();\n@@ -466,6 +513,2 @@\n-      for (int i = count-1; i >= 1; i--) {\n-        Node* off  = phase->MakeConX(type2aelembytes(copy_type) * i);\n-        Node* next_src = phase->transform(new AddPNode(base_src,adr_src,off));\n-        Node* next_dest = phase->transform(new AddPNode(base_dest,adr_dest,off));\n-        Node* v = load(bs, phase, backward_ctl, mm, next_src, atp_src, value_type, copy_type);\n-        store(bs, phase, backward_ctl, mm, next_dest, atp_dest, v, value_type, copy_type);\n+      for (int i = count-1; i >= 0; i--) {\n+        copy(kit, atp_src, atp_dest, i, base_src, base_dest, adr_src, adr_dest, copy_type, value_type);\n@@ -473,6 +516,5 @@\n-      Node* v = load(bs, phase, backward_ctl, mm, adr_src, atp_src, value_type, copy_type);\n-      store(bs, phase, backward_ctl, mm, adr_dest, atp_dest, v, value_type, copy_type);\n-    } else if (can_reshape) {\n-      PhaseIterGVN* igvn = phase->is_IterGVN();\n-      igvn->_worklist.push(adr_src);\n-      igvn->_worklist.push(adr_dest);\n+    } else if(can_reshape) {\n+      PhaseGVN& gvn = kit.gvn();\n+      assert(gvn.is_IterGVN(), \"\");\n+      gvn.record_for_igvn(adr_src);\n+      gvn.record_for_igvn(adr_dest);\n@@ -480,2 +522,0 @@\n-    return phase->transform(mm);\n-  return phase->C->top();\n@@ -507,2 +547,1 @@\n-      CallProjections callprojs;\n-      extract_projections(&callprojs, true, false);\n+      CallProjections* callprojs = extract_projections(true, false);\n@@ -510,2 +549,2 @@\n-      if (callprojs.fallthrough_ioproj != nullptr) {\n-        igvn->replace_node(callprojs.fallthrough_ioproj, in(TypeFunc::I_O));\n+      if (callprojs->fallthrough_ioproj != nullptr) {\n+        igvn->replace_node(callprojs->fallthrough_ioproj, in(TypeFunc::I_O));\n@@ -513,2 +552,2 @@\n-      if (callprojs.fallthrough_memproj != nullptr) {\n-        igvn->replace_node(callprojs.fallthrough_memproj, mem);\n+      if (callprojs->fallthrough_memproj != nullptr) {\n+        igvn->replace_node(callprojs->fallthrough_memproj, mem);\n@@ -516,2 +555,2 @@\n-      if (callprojs.fallthrough_catchproj != nullptr) {\n-        igvn->replace_node(callprojs.fallthrough_catchproj, ctl);\n+      if (callprojs->fallthrough_catchproj != nullptr) {\n+        igvn->replace_node(callprojs->fallthrough_catchproj, ctl);\n@@ -542,1 +581,5 @@\n-  if (remove_dead_region(phase, can_reshape))  return this;\n+  \/\/ Perform any generic optimizations first\n+  Node* result = SafePointNode::Ideal(phase, can_reshape);\n+  if (result != nullptr) {\n+    return result;\n+  }\n@@ -584,0 +627,11 @@\n+  Node* src = in(ArrayCopyNode::Src);\n+  Node* dest = in(ArrayCopyNode::Dest);\n+  const Type* src_type = phase->type(src);\n+  const Type* dest_type = phase->type(dest);\n+\n+  if (src_type->isa_aryptr() && dest_type->isa_instptr()) {\n+    \/\/ clone used for load of unknown inline type can't be optimized at\n+    \/\/ this point\n+    return nullptr;\n+  }\n+\n@@ -605,5 +659,21 @@\n-  Node* src = in(ArrayCopyNode::Src);\n-  Node* dest = in(ArrayCopyNode::Dest);\n-  const TypePtr* atp_src = get_address_type(phase, _src_type, src);\n-  const TypePtr* atp_dest = get_address_type(phase, _dest_type, dest);\n-  Node* in_mem = in(TypeFunc::Memory);\n+  JVMState* new_jvms = nullptr;\n+  SafePointNode* new_map = nullptr;\n+  if (!is_clonebasic()) {\n+    new_jvms = jvms()->clone_shallow(phase->C);\n+    new_map = new SafePointNode(req(), new_jvms);\n+    for (uint i = TypeFunc::FramePtr; i < req(); i++) {\n+      new_map->init_req(i, in(i));\n+    }\n+    new_jvms->set_map(new_map);\n+  } else {\n+    new_jvms = new (phase->C) JVMState(0);\n+    new_map = new SafePointNode(TypeFunc::Parms, new_jvms);\n+    new_jvms->set_map(new_map);\n+  }\n+  new_map->set_control(in(TypeFunc::Control));\n+  new_map->set_memory(MergeMemNode::make(in(TypeFunc::Memory)));\n+  new_map->set_i_o(in(TypeFunc::I_O));\n+  phase->record_for_igvn(new_map);\n+\n+  const TypeAryPtr* atp_src = get_address_type(phase, _src_type, src);\n+  const TypeAryPtr* atp_dest = get_address_type(phase, _dest_type, dest);\n@@ -616,0 +686,4 @@\n+  GraphKit kit(new_jvms, phase);\n+\n+  SafePointNode* backward_map = nullptr;\n+  SafePointNode* forward_map = nullptr;\n@@ -617,36 +691,36 @@\n-  Node* forward_ctl = phase->C->top();\n-  array_copy_test_overlap(phase, can_reshape, disjoint_bases, count, forward_ctl, backward_ctl);\n-\n-  Node* forward_mem = array_copy_forward(phase, can_reshape, forward_ctl,\n-                                         in_mem,\n-                                         atp_src, atp_dest,\n-                                         adr_src, base_src, adr_dest, base_dest,\n-                                         copy_type, value_type, count);\n-\n-  Node* backward_mem = array_copy_backward(phase, can_reshape, backward_ctl,\n-                                           in_mem,\n-                                           atp_src, atp_dest,\n-                                           adr_src, base_src, adr_dest, base_dest,\n-                                           copy_type, value_type, count);\n-\n-  Node* ctl = nullptr;\n-  if (!forward_ctl->is_top() && !backward_ctl->is_top()) {\n-    ctl = new RegionNode(3);\n-    ctl->init_req(1, forward_ctl);\n-    ctl->init_req(2, backward_ctl);\n-    ctl = phase->transform(ctl);\n-    MergeMemNode* forward_mm = forward_mem->as_MergeMem();\n-    MergeMemNode* backward_mm = backward_mem->as_MergeMem();\n-    for (MergeMemStream mms(forward_mm, backward_mm); mms.next_non_empty2(); ) {\n-      if (mms.memory() != mms.memory2()) {\n-        Node* phi = new PhiNode(ctl, Type::MEMORY, phase->C->get_adr_type(mms.alias_idx()));\n-        phi->init_req(1, mms.memory());\n-        phi->init_req(2, mms.memory2());\n-        phi = phase->transform(phi);\n-        mms.set_memory(phi);\n-      }\n-    }\n-    mem = forward_mem;\n-  } else if (!forward_ctl->is_top()) {\n-    ctl = forward_ctl;\n-    mem = forward_mem;\n+\n+  array_copy_test_overlap(kit, disjoint_bases, count, backward_ctl);\n+\n+  {\n+    PreserveJVMState pjvms(&kit);\n+\n+    array_copy_forward(kit, can_reshape,\n+                       atp_src, atp_dest,\n+                       adr_src, base_src, adr_dest, base_dest,\n+                       copy_type, value_type, count);\n+\n+    forward_map = kit.stop();\n+  }\n+\n+  kit.set_control(backward_ctl);\n+  array_copy_backward(kit, can_reshape,\n+                      atp_src, atp_dest,\n+                      adr_src, base_src, adr_dest, base_dest,\n+                      copy_type, value_type, count);\n+\n+  backward_map = kit.stop();\n+\n+  if (!forward_map->control()->is_top() && !backward_map->control()->is_top()) {\n+    assert(forward_map->i_o() == backward_map->i_o(), \"need a phi on IO?\");\n+    Node* ctl = new RegionNode(3);\n+    Node* mem = new PhiNode(ctl, Type::MEMORY, TypePtr::BOTTOM);\n+    kit.set_map(forward_map);\n+    ctl->init_req(1, kit.control());\n+    mem->init_req(1, kit.reset_memory());\n+    kit.set_map(backward_map);\n+    ctl->init_req(2, kit.control());\n+    mem->init_req(2, kit.reset_memory());\n+    kit.set_control(phase->transform(ctl));\n+    kit.set_all_memory(phase->transform(mem));\n+  } else if (!forward_map->control()->is_top()) {\n+    kit.set_map(forward_map);\n@@ -654,3 +728,2 @@\n-    assert(!backward_ctl->is_top(), \"no copy?\");\n-    ctl = backward_ctl;\n-    mem = backward_mem;\n+    assert(!backward_map->control()->is_top(), \"no copy?\");\n+    kit.set_map(backward_map);\n@@ -664,2 +737,5 @@\n-  if (!finish_transform(phase, can_reshape, ctl, mem)) {\n-    if (can_reshape) {\n+  mem = kit.map()->memory();\n+  if (!finish_transform(phase, can_reshape, kit.control(), mem)) {\n+    if (!can_reshape) {\n+      phase->record_for_igvn(this);\n+    } else {\n@@ -762,2 +838,9 @@\n-  uint header = arrayOopDesc::base_offset_in_bytes(ary_elem);\n-  uint elemsize = type2aelembytes(ary_elem);\n+  uint header;\n+  uint elem_size;\n+  if (ary_t->is_flat()) {\n+    header = arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT);\n+    elem_size = ary_t->flat_elem_size();\n+  } else {\n+    header = arrayOopDesc::base_offset_in_bytes(ary_elem);\n+    elem_size = type2aelembytes(ary_elem);\n+  }\n@@ -765,4 +848,4 @@\n-  jlong dest_pos_plus_len_lo = (((jlong)dest_pos_t->_lo) + len_t->_lo) * elemsize + header;\n-  jlong dest_pos_plus_len_hi = (((jlong)dest_pos_t->_hi) + len_t->_hi) * elemsize + header;\n-  jlong dest_pos_lo = ((jlong)dest_pos_t->_lo) * elemsize + header;\n-  jlong dest_pos_hi = ((jlong)dest_pos_t->_hi) * elemsize + header;\n+  jlong dest_pos_plus_len_lo = (((jlong)dest_pos_t->_lo) + len_t->_lo) * elem_size + header;\n+  jlong dest_pos_plus_len_hi = (((jlong)dest_pos_t->_hi) + len_t->_hi) * elem_size + header;\n+  jlong dest_pos_lo = ((jlong)dest_pos_t->_lo) * elem_size + header;\n+  jlong dest_pos_hi = ((jlong)dest_pos_t->_hi) * elem_size + header;\n","filename":"src\/hotspot\/share\/opto\/arraycopynode.cpp","additions":225,"deletions":142,"binary":false,"changes":367,"status":"modified"},{"patch":"@@ -1879,1 +1879,1 @@\n-          derived->bottom_type()->make_ptr()->is_ptr()->_offset == 0, \"sanity\");\n+         derived->bottom_type()->make_ptr()->is_ptr()->offset() == 0, \"sanity\");\n@@ -1882,1 +1882,1 @@\n-  if( tj == nullptr || tj->_offset == 0 ) {\n+  if (tj == nullptr || tj->offset() == 0) {\n@@ -2048,1 +2048,1 @@\n-                  derived->bottom_type()->make_ptr()->is_ptr()->_offset == 0, \"sanity\");\n+                 derived->bottom_type()->make_ptr()->is_ptr()->offset() == 0, \"sanity\");\n@@ -2050,1 +2050,1 @@\n-          if( tj && tj->_offset != 0 && tj->isa_oop_ptr() ) {\n+          if (tj && tj->offset() != 0 && tj->isa_oop_ptr()) {\n@@ -2340,1 +2340,1 @@\n-  const TypeTuple *domain = C->tf()->domain();\n+  const TypeTuple *domain = C->tf()->domain_cc();\n@@ -2549,1 +2549,1 @@\n-                  if (is_derived && check->bottom_type()->is_ptr()->_offset != 0) {\n+                  if (is_derived && check->bottom_type()->is_ptr()->offset() != 0) {\n@@ -2553,1 +2553,1 @@\n-                    assert(check->bottom_type()->is_ptr()->_offset == 0, \"Bad base pointer\");\n+                    assert(check->bottom_type()->is_ptr()->offset() == 0, \"Bad base pointer\");\n@@ -2562,1 +2562,1 @@\n-                } else if (check->bottom_type()->is_ptr()->_offset == 0) {\n+                } else if (check->bottom_type()->is_ptr()->offset() == 0) {\n","filename":"src\/hotspot\/share\/opto\/chaitin.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -1600,1 +1600,1 @@\n-      tf()->range(),\n+      tf()->range_cc(),\n@@ -1663,1 +1663,1 @@\n-Node *DivModINode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *DivModINode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -1678,1 +1678,1 @@\n-Node *DivModLNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *DivModLNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -1716,1 +1716,1 @@\n-Node* UDivModINode::match( const ProjNode *proj, const Matcher *match ) {\n+Node* UDivModINode::match(const ProjNode* proj, const Matcher* match, const RegMask* mask) {\n@@ -1731,1 +1731,1 @@\n-Node* UDivModLNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node* UDivModLNode::match( const ProjNode* proj, const Matcher* match, const RegMask* mask) {\n","filename":"src\/hotspot\/share\/opto\/divnode.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"classfile\/vmIntrinsics.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"jvm_io.h\"\n@@ -40,0 +42,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -89,0 +92,58 @@\n+static bool arg_can_be_larval(ciMethod* callee, int arg_idx) {\n+  if (callee->is_object_constructor() && arg_idx == 0) {\n+    return true;\n+  }\n+\n+  if (arg_idx != 1 || callee->intrinsic_id() == vmIntrinsicID::_none) {\n+    return false;\n+  }\n+\n+  switch (callee->intrinsic_id()) {\n+    case vmIntrinsicID::_finishPrivateBuffer:\n+    case vmIntrinsicID::_putBoolean:\n+    case vmIntrinsicID::_putBooleanOpaque:\n+    case vmIntrinsicID::_putBooleanRelease:\n+    case vmIntrinsicID::_putBooleanVolatile:\n+    case vmIntrinsicID::_putByte:\n+    case vmIntrinsicID::_putByteOpaque:\n+    case vmIntrinsicID::_putByteRelease:\n+    case vmIntrinsicID::_putByteVolatile:\n+    case vmIntrinsicID::_putChar:\n+    case vmIntrinsicID::_putCharOpaque:\n+    case vmIntrinsicID::_putCharRelease:\n+    case vmIntrinsicID::_putCharUnaligned:\n+    case vmIntrinsicID::_putCharVolatile:\n+    case vmIntrinsicID::_putShort:\n+    case vmIntrinsicID::_putShortOpaque:\n+    case vmIntrinsicID::_putShortRelease:\n+    case vmIntrinsicID::_putShortUnaligned:\n+    case vmIntrinsicID::_putShortVolatile:\n+    case vmIntrinsicID::_putInt:\n+    case vmIntrinsicID::_putIntOpaque:\n+    case vmIntrinsicID::_putIntRelease:\n+    case vmIntrinsicID::_putIntUnaligned:\n+    case vmIntrinsicID::_putIntVolatile:\n+    case vmIntrinsicID::_putLong:\n+    case vmIntrinsicID::_putLongOpaque:\n+    case vmIntrinsicID::_putLongRelease:\n+    case vmIntrinsicID::_putLongUnaligned:\n+    case vmIntrinsicID::_putLongVolatile:\n+    case vmIntrinsicID::_putFloat:\n+    case vmIntrinsicID::_putFloatOpaque:\n+    case vmIntrinsicID::_putFloatRelease:\n+    case vmIntrinsicID::_putFloatVolatile:\n+    case vmIntrinsicID::_putDouble:\n+    case vmIntrinsicID::_putDoubleOpaque:\n+    case vmIntrinsicID::_putDoubleRelease:\n+    case vmIntrinsicID::_putDoubleVolatile:\n+    case vmIntrinsicID::_putReference:\n+    case vmIntrinsicID::_putReferenceOpaque:\n+    case vmIntrinsicID::_putReferenceRelease:\n+    case vmIntrinsicID::_putReferenceVolatile:\n+    case vmIntrinsicID::_putValue:\n+      return true;\n+    default:\n+      return false;\n+  }\n+}\n+\n@@ -150,1 +211,15 @@\n-  if (allow_inline && allow_intrinsics) {\n+  if (callee->intrinsic_id() == vmIntrinsics::_makePrivateBuffer || callee->intrinsic_id() == vmIntrinsics::_finishPrivateBuffer) {\n+    \/\/ These methods must be inlined so that we don't have larval value objects crossing method\n+    \/\/ boundaries\n+    assert(!call_does_dispatch, \"callee should not be virtual %s\", callee->name()->as_utf8());\n+    CallGenerator* cg = find_intrinsic(callee, call_does_dispatch);\n+\n+    if (cg == nullptr) {\n+      \/\/ This is probably because the intrinsics is disabled from the command line\n+      char reason[256];\n+      jio_snprintf(reason, sizeof(reason), \"cannot find an intrinsics for %s\", callee->name()->as_utf8());\n+      C->record_method_not_compilable(reason);\n+      return nullptr;\n+    }\n+    return cg;\n+  } else if (allow_inline && allow_intrinsics) {\n@@ -610,1 +685,1 @@\n-  if (iter().cur_bc_raw() == Bytecodes::_invokespecial && !orig_callee->is_object_initializer()) {\n+  if (iter().cur_bc_raw() == Bytecodes::_invokespecial && !orig_callee->is_object_constructor()) {\n@@ -638,0 +713,9 @@\n+  \/\/ Scalarize value objects passed into this invocation if we know that they are not larval\n+  for (int arg_idx = 0; arg_idx < nargs; arg_idx++) {\n+    if (arg_can_be_larval(callee, arg_idx)) {\n+      continue;\n+    }\n+\n+    cast_to_non_larval(peek(nargs - 1 - arg_idx));\n+  }\n+\n@@ -652,0 +736,4 @@\n+  if (failing()) {\n+    return;\n+  }\n+  assert(cg != nullptr, \"must find a CallGenerator for callee %s\", callee->name()->as_utf8());\n@@ -738,1 +826,1 @@\n-          \/\/ It's OK for a method  to return a value that is discarded.\n+          \/\/ It's OK for a method to return a value that is discarded.\n@@ -796,0 +884,21 @@\n+\n+    if (!rtype->is_void() && cg->method()->intrinsic_id() != vmIntrinsicID::_makePrivateBuffer) {\n+      Node* retnode = peek();\n+      const Type* rettype = gvn().type(retnode);\n+      if (rettype->is_inlinetypeptr() && !retnode->is_InlineType()) {\n+        retnode = InlineTypeNode::make_from_oop(this, retnode, rettype->inline_klass());\n+        dec_sp(1);\n+        push(retnode);\n+      }\n+    }\n+\n+    if (cg->method()->is_object_constructor() && receiver != nullptr && gvn().type(receiver)->is_inlinetypeptr()) {\n+      InlineTypeNode* non_larval = InlineTypeNode::make_from_oop(this, receiver, gvn().type(receiver)->inline_klass());\n+      \/\/ Relinquish the oop input, we will delay the allocation to the point it is needed, see the\n+      \/\/ comments in InlineTypeNode::Ideal for more details\n+      non_larval = non_larval->clone_if_required(&gvn(), nullptr);\n+      non_larval->set_oop(gvn(), null());\n+      non_larval->set_is_buffered(gvn(), false);\n+      non_larval = gvn().transform(non_larval)->as_InlineType();\n+      map()->replace_edge(receiver, non_larval);\n+    }\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":112,"deletions":3,"binary":false,"changes":115,"status":"modified"},{"patch":"@@ -26,0 +26,3 @@\n+#include \"ci\/ciArrayKlass.hpp\"\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInstanceKlass.hpp\"\n@@ -32,0 +35,1 @@\n+#include \"gc\/shared\/c2\/barrierSetC2.hpp\"\n@@ -34,0 +38,1 @@\n+#include \"oops\/accessDecorators.hpp\"\n@@ -35,0 +40,1 @@\n+#include \"oops\/layoutKind.hpp\"\n@@ -43,0 +49,1 @@\n+#include \"opto\/graphKit.hpp\"\n@@ -44,0 +51,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -49,0 +57,1 @@\n+#include \"opto\/opcodes.hpp\"\n@@ -53,0 +62,1 @@\n+#include \"opto\/type.hpp\"\n@@ -61,0 +71,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -321,0 +332,2 @@\n+  case vmIntrinsics::_makePrivateBuffer:        return inline_unsafe_make_private_buffer();\n+  case vmIntrinsics::_finishPrivateBuffer:      return inline_unsafe_finish_private_buffer();\n@@ -330,0 +343,1 @@\n+  case vmIntrinsics::_getValue:                 return inline_unsafe_access(!is_store, T_OBJECT,   Relaxed, false, true);\n@@ -340,0 +354,1 @@\n+  case vmIntrinsics::_putValue:                 return inline_unsafe_access( is_store, T_OBJECT,   Relaxed, false, true);\n@@ -411,0 +426,3 @@\n+  case vmIntrinsics::_getFlatValue:             return inline_unsafe_flat_access(!is_store, Relaxed);\n+  case vmIntrinsics::_putFlatValue:             return inline_unsafe_flat_access( is_store, Relaxed);\n+\n@@ -518,0 +536,6 @@\n+  case vmIntrinsics::_newNullRestrictedNonAtomicArray: return inline_newArray(\/* null_free *\/ true, \/* atomic *\/ false);\n+  case vmIntrinsics::_newNullRestrictedAtomicArray: return inline_newArray(\/* null_free *\/ true, \/* atomic *\/ true);\n+  case vmIntrinsics::_newNullableAtomicArray:     return inline_newArray(\/* null_free *\/ false, \/* atomic *\/ true);\n+  case vmIntrinsics::_isFlatArray:              return inline_getArrayProperties(IsFlat);\n+  case vmIntrinsics::_isNullRestrictedArray:    return inline_getArrayProperties(IsNullRestricted);\n+  case vmIntrinsics::_isAtomicArray:            return inline_getArrayProperties(IsAtomic);\n@@ -2335,0 +2359,1 @@\n+  bool null_free = false;\n@@ -2340,0 +2365,1 @@\n+      null_free = alias_type->field()->is_null_free();\n@@ -2346,1 +2372,1 @@\n-    if (adr_type->offset() >= objArrayOopDesc::base_offset_in_bytes()) {\n+    if (adr_type->offset() >= refArrayOopDesc::base_offset_in_bytes()) {\n@@ -2348,0 +2374,1 @@\n+      null_free = adr_type->is_aryptr()->is_null_free();\n@@ -2360,0 +2387,3 @@\n+    if (null_free) {\n+      result = result->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+    }\n@@ -2431,1 +2461,1 @@\n-bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned) {\n+bool LibraryCallKit::inline_unsafe_access(bool is_store, const BasicType type, const AccessKind kind, const bool unaligned, const bool is_flat) {\n@@ -2456,1 +2486,1 @@\n-      assert(sig->count() == 2, \"oop getter has 2 arguments\");\n+      assert(sig->count() == 2 || (is_flat && sig->count() == 3), \"oop getter has 2 or 3 arguments\");\n@@ -2462,1 +2492,1 @@\n-      assert(sig->count() == 3, \"oop putter has 3 arguments\");\n+      assert(sig->count() == 3 || (is_flat && sig->count() == 4), \"oop putter has 3 arguments\");\n@@ -2488,0 +2518,49 @@\n+\n+  ciInlineKlass* inline_klass = nullptr;\n+  if (is_flat) {\n+    const TypeInstPtr* cls = _gvn.type(argument(4))->isa_instptr();\n+    if (cls == nullptr || cls->const_oop() == nullptr) {\n+      return false;\n+    }\n+    ciType* mirror_type = cls->const_oop()->as_instance()->java_mirror_type();\n+    if (!mirror_type->is_inlinetype()) {\n+      return false;\n+    }\n+    inline_klass = mirror_type->as_inline_klass();\n+  }\n+\n+  if (base->is_InlineType()) {\n+    assert(!is_store, \"InlineTypeNodes are non-larval value objects\");\n+    InlineTypeNode* vt = base->as_InlineType();\n+    if (offset->is_Con()) {\n+      long off = find_long_con(offset, 0);\n+      ciInlineKlass* vk = vt->type()->inline_klass();\n+      if ((long)(int)off != off || !vk->contains_field_offset(off)) {\n+        return false;\n+      }\n+\n+      ciField* field = vk->get_non_flat_field_by_offset(off);\n+      if (field != nullptr) {\n+        BasicType bt = type2field[field->type()->basic_type()];\n+        if (bt == T_ARRAY || bt == T_NARROWOOP) {\n+          bt = T_OBJECT;\n+        }\n+        if (bt == type && (!field->is_flat() || field->type() == inline_klass)) {\n+          Node* value = vt->field_value_by_offset(off, false);\n+          if (value->is_InlineType()) {\n+            value = value->as_InlineType()->adjust_scalarization_depth(this);\n+          }\n+          set_result(value);\n+          return true;\n+        }\n+      }\n+    }\n+    {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      vt = vt->buffer(this);\n+    }\n+    base = vt->get_oop();\n+  }\n+\n@@ -2498,1 +2577,1 @@\n-    if (type != T_OBJECT) {\n+    if (type != T_OBJECT && (inline_klass == nullptr || !inline_klass->has_object_fields())) {\n@@ -2514,1 +2593,1 @@\n-  Node* val = is_store ? argument(4) : nullptr;\n+  Node* val = is_store ? argument(4 + (is_flat ? 1 : 0)) : nullptr;\n@@ -2531,1 +2610,29 @@\n-  BasicType bt = alias_type->basic_type();\n+  BasicType bt = T_ILLEGAL;\n+  ciField* field = nullptr;\n+  if (adr_type->isa_instptr()) {\n+    const TypeInstPtr* instptr = adr_type->is_instptr();\n+    ciInstanceKlass* k = instptr->instance_klass();\n+    int off = instptr->offset();\n+    if (instptr->const_oop() != nullptr &&\n+        k == ciEnv::current()->Class_klass() &&\n+        instptr->offset() >= (k->size_helper() * wordSize)) {\n+      k = instptr->const_oop()->as_instance()->java_lang_Class_klass()->as_instance_klass();\n+      field = k->get_field_by_offset(off, true);\n+    } else {\n+      field = k->get_non_flat_field_by_offset(off);\n+    }\n+    if (field != nullptr) {\n+      bt = type2field[field->type()->basic_type()];\n+    }\n+    if (bt != alias_type->basic_type()) {\n+      \/\/ Type mismatch. Is it an access to a nested flat field?\n+      field = k->get_field_by_offset(off, false);\n+      if (field != nullptr) {\n+        bt = type2field[field->type()->basic_type()];\n+      }\n+    }\n+    assert(bt == alias_type->basic_type() || is_flat, \"should match\");\n+  } else {\n+    bt = alias_type->basic_type();\n+  }\n+\n@@ -2552,0 +2659,21 @@\n+  if (is_flat) {\n+    if (adr_type->isa_instptr()) {\n+      if (field == nullptr || field->type() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else if (adr_type->isa_aryptr()) {\n+      const Type* elem = adr_type->is_aryptr()->elem();\n+      if (!adr_type->is_flat() || elem->inline_klass() != inline_klass) {\n+        mismatched = true;\n+      }\n+    } else {\n+      mismatched = true;\n+    }\n+    if (is_store) {\n+      const Type* val_t = _gvn.type(val);\n+      if (!val_t->is_inlinetypeptr() || val_t->inline_klass() != inline_klass) {\n+        return false;\n+      }\n+    }\n+  }\n+\n@@ -2565,4 +2693,6 @@\n-  if (!is_store && type == T_OBJECT) {\n-    const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n-    if (tjp != nullptr) {\n-      value_type = tjp;\n+  if (!is_store) {\n+    if (type == T_OBJECT && !is_flat) {\n+      const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);\n+      if (tjp != nullptr) {\n+        value_type = tjp;\n+      }\n@@ -2584,2 +2714,2 @@\n-    ciField* field = alias_type->field();\n-    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !mismatched) {\n+\n+    if (heap_base_oop != top() && field != nullptr && field->is_constant() && !field->is_flat() && !mismatched) {\n@@ -2591,1 +2721,10 @@\n-      p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+      if (is_flat) {\n+        p = InlineTypeNode::make_from_flat(this, inline_klass, base, adr, adr_type, false, false, true);\n+      } else {\n+        p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);\n+        const TypeOopPtr* ptr = value_type->make_oopptr();\n+        if (ptr != nullptr && ptr->is_inlinetypeptr()) {\n+          \/\/ Load a non-flattened inline type from memory\n+          p = InlineTypeNode::make_from_oop(this, p, ptr->inline_klass());\n+        }\n+      }\n@@ -2629,1 +2768,137 @@\n-    access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    if (is_flat) {\n+      val->as_InlineType()->store_flat(this, base, adr, false, false, true, decorators);\n+    } else {\n+      access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_flat_access(bool is_store, AccessKind kind) {\n+#ifdef ASSERT\n+  {\n+    ResourceMark rm;\n+    \/\/ Check the signatures.\n+    ciSignature* sig = callee()->signature();\n+    assert(sig->type_at(0)->basic_type() == T_OBJECT, \"base should be object, but is %s\", type2name(sig->type_at(0)->basic_type()));\n+    assert(sig->type_at(1)->basic_type() == T_LONG, \"offset should be long, but is %s\", type2name(sig->type_at(1)->basic_type()));\n+    assert(sig->type_at(2)->basic_type() == T_INT, \"layout kind should be int, but is %s\", type2name(sig->type_at(3)->basic_type()));\n+    assert(sig->type_at(3)->basic_type() == T_OBJECT, \"value klass should be object, but is %s\", type2name(sig->type_at(4)->basic_type()));\n+    if (is_store) {\n+      assert(sig->return_type()->basic_type() == T_VOID, \"putter must not return a value, but returns %s\", type2name(sig->return_type()->basic_type()));\n+      assert(sig->count() == 5, \"flat putter should have 5 arguments, but has %d\", sig->count());\n+      assert(sig->type_at(4)->basic_type() == T_OBJECT, \"put value should be object, but is %s\", type2name(sig->type_at(5)->basic_type()));\n+    } else {\n+      assert(sig->return_type()->basic_type() == T_OBJECT, \"getter must return an object, but returns %s\", type2name(sig->return_type()->basic_type()));\n+      assert(sig->count() == 4, \"flat getter should have 4 arguments, but has %d\", sig->count());\n+    }\n+ }\n+#endif \/\/ ASSERT\n+\n+  assert(kind == Relaxed, \"Only plain accesses for now\");\n+  if (callee()->is_static()) {\n+    \/\/ caller must have the capability!\n+    return false;\n+  }\n+  C->set_has_unsafe_access(true);\n+\n+  const TypeInstPtr* value_klass_node = _gvn.type(argument(5))->isa_instptr();\n+  if (value_klass_node == nullptr || value_klass_node->const_oop() == nullptr) {\n+    \/\/ parameter valueType is not a constant\n+    return false;\n+  }\n+  ciType* mirror_type = value_klass_node->const_oop()->as_instance()->java_mirror_type();\n+  if (!mirror_type->is_inlinetype()) {\n+    \/\/ Dead code\n+    return false;\n+  }\n+  ciInlineKlass* value_klass = mirror_type->as_inline_klass();\n+\n+  const TypeInt* layout_type = _gvn.type(argument(4))->isa_int();\n+  if (layout_type == nullptr || !layout_type->is_con()) {\n+    \/\/ parameter layoutKind is not a constant\n+    return false;\n+  }\n+  assert(layout_type->get_con() >= static_cast<int>(LayoutKind::REFERENCE) &&\n+         layout_type->get_con() <= static_cast<int>(LayoutKind::UNKNOWN),\n+         \"invalid layoutKind %d\", layout_type->get_con());\n+  LayoutKind layout = static_cast<LayoutKind>(layout_type->get_con());\n+  assert(layout == LayoutKind::REFERENCE || layout == LayoutKind::NON_ATOMIC_FLAT ||\n+         layout == LayoutKind::ATOMIC_FLAT || layout == LayoutKind::NULLABLE_ATOMIC_FLAT,\n+         \"unexpected layoutKind %d\", layout_type->get_con());\n+\n+  null_check(argument(0));\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  Node* base = must_be_not_null(argument(1), true);\n+  Node* offset = argument(2);\n+  const Type* base_type = _gvn.type(base);\n+\n+  Node* ptr;\n+  bool immutable_memory = false;\n+  DecoratorSet decorators = C2_UNSAFE_ACCESS | IN_HEAP | MO_UNORDERED;\n+  if (base_type->isa_instptr()) {\n+    const TypeLong* offset_type = _gvn.type(offset)->isa_long();\n+    if (offset_type == nullptr || !offset_type->is_con()) {\n+      \/\/ Offset into a non-array should be a constant\n+      decorators |= C2_MISMATCHED;\n+    } else {\n+      int offset_con = checked_cast<int>(offset_type->get_con());\n+      ciInstanceKlass* base_klass = base_type->is_instptr()->instance_klass();\n+      ciField* field = base_klass->get_non_flat_field_by_offset(offset_con);\n+      if (field == nullptr) {\n+        assert(!base_klass->is_final(), \"non-existence field at offset %d of class %s\", offset_con, base_klass->name()->as_utf8());\n+        decorators |= C2_MISMATCHED;\n+      } else {\n+        assert(field->type() == value_klass, \"field at offset %d of %s is of type %s, but valueType is %s\",\n+               offset_con, base_klass->name()->as_utf8(), field->type()->name(), value_klass->name()->as_utf8());\n+        immutable_memory = field->is_strict() && field->is_final();\n+\n+        if (base->is_InlineType()) {\n+          assert(!is_store, \"Cannot store into a non-larval value object\");\n+          set_result(base->as_InlineType()->field_value_by_offset(offset_con, false));\n+          return true;\n+        }\n+      }\n+    }\n+\n+    if (base->is_InlineType()) {\n+      assert(!is_store, \"Cannot store into a non-larval value object\");\n+      base = base->as_InlineType()->buffer(this, true);\n+    }\n+    ptr = basic_plus_adr(base, ConvL2X(offset));\n+  } else if (base_type->isa_aryptr()) {\n+    decorators |= IS_ARRAY;\n+    if (layout == LayoutKind::REFERENCE) {\n+      if (!base_type->is_aryptr()->is_not_flat()) {\n+        const TypeAryPtr* array_type = base_type->is_aryptr()->cast_to_not_flat();\n+        Node* new_base = _gvn.transform(new CastPPNode(control(), base, array_type, ConstraintCastNode::StrongDependency));\n+        replace_in_map(base, new_base);\n+        base = new_base;\n+      }\n+      ptr = basic_plus_adr(base, ConvL2X(offset));\n+    } else {\n+      if (UseArrayFlattening) {\n+        \/\/ Flat array must have an exact type\n+        bool is_null_free = layout != LayoutKind::NULLABLE_ATOMIC_FLAT;\n+        bool is_atomic = layout != LayoutKind::NON_ATOMIC_FLAT;\n+        Node* new_base = cast_to_flat_array(base, value_klass, is_null_free, !is_null_free, is_atomic);\n+        replace_in_map(base, new_base);\n+        base = new_base;\n+        ptr = basic_plus_adr(base, ConvL2X(offset));\n+        const TypeAryPtr* ptr_type = _gvn.type(ptr)->is_aryptr();\n+        if (ptr_type->field_offset().get() != 0) {\n+          ptr = _gvn.transform(new CastPPNode(control(), ptr, ptr_type->with_field_offset(0), ConstraintCastNode::StrongDependency));\n+        }\n+      } else {\n+        uncommon_trap(Deoptimization::Reason_intrinsic,\n+                      Deoptimization::Action_none);\n+        return true;\n+      }\n+    }\n+  } else {\n+    decorators |= C2_MISMATCHED;\n+    ptr = basic_plus_adr(base, ConvL2X(offset));\n@@ -2632,0 +2907,103 @@\n+  if (is_store) {\n+    Node* value = argument(6);\n+    const Type* value_type = _gvn.type(value);\n+    if (!value_type->is_inlinetypeptr()) {\n+      value_type = Type::get_const_type(value_klass)->filter_speculative(value_type);\n+      Node* new_value = _gvn.transform(new CastPPNode(control(), value, value_type, ConstraintCastNode::StrongDependency));\n+      new_value = InlineTypeNode::make_from_oop(this, new_value, value_klass);\n+      replace_in_map(value, new_value);\n+      value = new_value;\n+    }\n+\n+    assert(value_type->inline_klass() == value_klass, \"value is of type %s while valueType is %s\", value_type->inline_klass()->name()->as_utf8(), value_klass->name()->as_utf8());\n+    if (layout == LayoutKind::REFERENCE) {\n+      const TypePtr* ptr_type = (decorators & C2_MISMATCHED) != 0 ? TypeRawPtr::BOTTOM : _gvn.type(ptr)->is_ptr();\n+      access_store_at(base, ptr, ptr_type, value, value_type, T_OBJECT, decorators);\n+    } else {\n+      bool atomic = layout != LayoutKind::NON_ATOMIC_FLAT;\n+      bool null_free = layout != LayoutKind::NULLABLE_ATOMIC_FLAT;\n+      value->as_InlineType()->store_flat(this, base, ptr, atomic, immutable_memory, null_free, decorators);\n+    }\n+\n+    return true;\n+  } else {\n+    decorators |= (C2_CONTROL_DEPENDENT_LOAD | C2_UNKNOWN_CONTROL_LOAD);\n+    InlineTypeNode* result;\n+    if (layout == LayoutKind::REFERENCE) {\n+      const TypePtr* ptr_type = (decorators & C2_MISMATCHED) != 0 ? TypeRawPtr::BOTTOM : _gvn.type(ptr)->is_ptr();\n+      Node* oop = access_load_at(base, ptr, ptr_type, Type::get_const_type(value_klass), T_OBJECT, decorators);\n+      result = InlineTypeNode::make_from_oop(this, oop, value_klass);\n+    } else {\n+      bool atomic = layout != LayoutKind::NON_ATOMIC_FLAT;\n+      bool null_free = layout != LayoutKind::NULLABLE_ATOMIC_FLAT;\n+      result = InlineTypeNode::make_from_flat(this, value_klass, base, ptr, atomic, immutable_memory, null_free, decorators);\n+    }\n+\n+    set_result(result);\n+    return true;\n+  }\n+}\n+\n+bool LibraryCallKit::inline_unsafe_make_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* value = argument(1);\n+\n+  const Type* type = gvn().type(value);\n+  if (!type->is_inlinetypeptr()) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::makePrivateBuffer is not of a constant value type\");\n+    return false;\n+  }\n+\n+  null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  value = null_check(value);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  ciInlineKlass* vk = type->inline_klass();\n+  Node* klass = makecon(TypeKlassPtr::make(vk));\n+  Node* obj = new_instance(klass);\n+  AllocateNode::Ideal_allocation(obj)->_larval = true;\n+\n+  assert(value->is_InlineType(), \"must be an InlineTypeNode\");\n+  Node* payload_ptr = basic_plus_adr(obj, vk->payload_offset());\n+  value->as_InlineType()->store_flat(this, obj, payload_ptr, false, true, true, IN_HEAP | MO_UNORDERED);\n+\n+  set_result(obj);\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_finish_private_buffer() {\n+  Node* receiver = argument(0);\n+  Node* buffer = argument(1);\n+\n+  const Type* type = gvn().type(buffer);\n+  if (!type->is_inlinetypeptr()) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::finishPrivateBuffer is not of a constant value type\");\n+    return false;\n+  }\n+\n+  AllocateNode* alloc = AllocateNode::Ideal_allocation(buffer);\n+  if (alloc == nullptr) {\n+    C->record_method_not_compilable(\"value passed to Unsafe::finishPrivateBuffer must be allocated by Unsafe::makePrivateBuffer\");\n+    return false;\n+  }\n+\n+  null_check(receiver);\n+  if (stopped()) {\n+    return true;\n+  }\n+\n+  \/\/ Unset the larval bit in the object header\n+  Node* old_header = make_load(control(), buffer, TypeX_X, TypeX_X->basic_type(), MemNode::unordered, LoadNode::Pinned);\n+  Node* new_header = gvn().transform(new AndXNode(old_header, MakeConX(~markWord::larval_bit_in_place)));\n+  access_store_at(buffer, buffer, type->is_ptr(), new_header, TypeX_X, TypeX_X->basic_type(), MO_UNORDERED | IN_HEAP);\n+\n+  \/\/ We must ensure that the buffer is properly published\n+  insert_mem_bar(Op_MemBarStoreStore, alloc->proj_out(AllocateNode::RawAddress));\n+  assert(!type->maybe_null(), \"result of an allocation should not be null\");\n+  set_result(InlineTypeNode::make_from_oop(this, buffer, type->inline_klass()));\n@@ -2837,0 +3215,13 @@\n+    if (oldval != nullptr && oldval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      oldval = oldval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+    if (newval != nullptr && newval->is_InlineType()) {\n+      \/\/ Re-execute the unsafe access if allocation triggers deoptimization.\n+      PreserveReexecuteState preexecs(this);\n+      jvms()->set_should_reexecute(true);\n+      newval = newval->as_InlineType()->buffer(this)->get_oop();\n+    }\n+\n@@ -3023,2 +3414,7 @@\n-\n-  Node* obj = new_instance(kls, test);\n+  Node* obj = nullptr;\n+  const TypeInstKlassPtr* tkls = _gvn.type(kls)->isa_instklassptr();\n+  if (tkls != nullptr && tkls->instance_klass()->is_inlinetype()) {\n+    obj = InlineTypeNode::make_all_zero(_gvn, tkls->instance_klass()->as_inline_klass())->buffer(this);\n+  } else {\n+    obj = new_instance(kls, test);\n+  }\n@@ -3805,1 +4201,1 @@\n-  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS, \/* stable= *\/ false, \/* flat= *\/ false, \/* not_flat= *\/ true, \/* not_null_free= *\/ true);\n@@ -3810,1 +4206,1 @@\n-  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, 0);\n+  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, TypeAryPtr::Offset(0));\n@@ -3934,9 +4330,0 @@\n-\/\/---------------------------load_mirror_from_klass----------------------------\n-\/\/ Given a klass oop, load its java mirror (a java.lang.Class oop).\n-Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {\n-  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));\n-  Node* load = make_load(nullptr, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n-  \/\/ mirror = ((OopHandle)mirror)->resolve();\n-  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);\n-}\n-\n@@ -3986,0 +4373,1 @@\n+\n@@ -4107,10 +4495,12 @@\n-    p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));\n-    kls = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT_OR_NULL));\n-    null_ctl = top();\n-    kls = null_check_oop(kls, &null_ctl);\n-    if (null_ctl != top()) {\n-      \/\/ If the guard is taken, Object.superClass is null (both klass and mirror).\n-      region->add_req(null_ctl);\n-      phi   ->add_req(null());\n-    }\n-      query_value = load_mirror_from_klass(kls);\n+      p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));\n+      kls = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT_OR_NULL));\n+      null_ctl = top();\n+      kls = null_check_oop(kls, &null_ctl);\n+      if (null_ctl != top()) {\n+        \/\/ If the guard is taken, Object.superClass is null (both klass and mirror).\n+        region->add_req(null_ctl);\n+        phi   ->add_req(null());\n+      }\n+      if (!stopped()) {\n+        query_value = load_mirror_from_klass(kls);\n+      }\n@@ -4135,0 +4525,1 @@\n+\n@@ -4157,1 +4548,2 @@\n-      int static_res = C->static_subtype_check(TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces), tp->as_klass_type());\n+      const TypeKlassPtr* tklass = TypeKlassPtr::make(tm->as_klass(), Type::trust_interfaces);\n+      int static_res = C->static_subtype_check(tklass, tp->as_klass_type());\n@@ -4186,2 +4578,2 @@\n-  \/\/ Not-subtype or the mirror's klass ptr is null (in case it is a primitive).\n-  enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };\n+  \/\/ Not-subtype or the mirror's klass ptr is nullptr (in case it is a primitive).\n+  enum { _bad_type_path = 1, _prim_path = 2, _npe_path = 3, PATH_LIMIT };\n@@ -4197,0 +4589,2 @@\n+  Node* io = i_o();\n+  Node* mem = merged_memory();\n@@ -4198,0 +4592,1 @@\n+\n@@ -4204,1 +4599,2 @@\n-      region->in(_bad_type_path) != top()) {\n+      region->in(_bad_type_path) != top() ||\n+      region->in(_npe_path) != top()) {\n@@ -4208,0 +4604,3 @@\n+    \/\/ Set IO and memory because gen_checkcast may override them when buffering inline types\n+    set_i_o(io);\n+    set_all_memory(mem);\n@@ -4241,0 +4640,1 @@\n+  RegionNode* prim_region = new RegionNode(2);\n@@ -4243,0 +4643,1 @@\n+  record_for_igvn(prim_region);\n@@ -4267,2 +4668,5 @@\n-    int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);\n-    region->init_req(prim_path, null_ctl);\n+    if (which_arg == 0) {\n+      prim_region->init_req(1, null_ctl);\n+    } else {\n+      region->init_req(_prim_1_path, null_ctl);\n+    }\n@@ -4278,1 +4682,0 @@\n-    \/\/ now we have a successful reference subtype check\n@@ -4285,1 +4688,2 @@\n-  set_control(region->in(_prim_0_path)); \/\/ go back to first null check\n+  \/\/ This path is also used if superc is a value mirror.\n+  set_control(_gvn.transform(prim_region));\n@@ -4290,1 +4694,1 @@\n-    generate_guard(bol_eq, region, PROB_FAIR);\n+    generate_fair_guard(bol_eq, region);\n@@ -4321,2 +4725,1 @@\n-Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,\n-                                                  bool obj_array, bool not_array, Node** obj) {\n+Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind, Node** obj) {\n@@ -4328,9 +4731,0 @@\n-  \/\/ If obj_array\/non_array==false\/false:\n-  \/\/ Branch around if the given klass is in fact an array (either obj or prim).\n-  \/\/ If obj_array\/non_array==false\/true:\n-  \/\/ Branch around if the given klass is not an array klass of any kind.\n-  \/\/ If obj_array\/non_array==true\/true:\n-  \/\/ Branch around if the kls is not an oop array (kls is int[], String, etc.)\n-  \/\/ If obj_array\/non_array==true\/false:\n-  \/\/ Branch around if the kls is an oop array (Object[] or subtype)\n-  \/\/\n@@ -4341,4 +4735,11 @@\n-    bool query = (obj_array\n-                  ? Klass::layout_helper_is_objArray(layout_con)\n-                  : Klass::layout_helper_is_array(layout_con));\n-    if (query == not_array) {\n+    bool query = 0;\n+    switch(kind) {\n+      case RefArray:       query = Klass::layout_helper_is_refArray(layout_con); break;\n+      case NonRefArray:    query = !Klass::layout_helper_is_refArray(layout_con); break;\n+      case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;\n+      case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;\n+      case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    if (!query) {\n@@ -4354,0 +4755,21 @@\n+  unsigned int value = 0;\n+  BoolTest::mask btest = BoolTest::illegal;\n+  switch(kind) {\n+    case RefArray:\n+    case NonRefArray: {\n+      value = Klass::_lh_array_tag_ref_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = (kind == RefArray) ? BoolTest::eq : BoolTest::ne;\n+      break;\n+    }\n+    case TypeArray: {\n+      value = Klass::_lh_array_tag_type_value;\n+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));\n+      btest = BoolTest::eq;\n+      break;\n+    }\n+    case AnyArray:    value = Klass::_lh_neutral_value; btest = BoolTest::lt; break;\n+    case NonArray:    value = Klass::_lh_neutral_value; btest = BoolTest::gt; break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -4355,4 +4777,1 @@\n-  jint  nval = (obj_array\n-                ? (jint)(Klass::_lh_array_tag_type_value\n-                   <<    Klass::_lh_array_tag_shift)\n-                : Klass::_lh_neutral_value);\n+  jint nval = (jint)value;\n@@ -4360,3 +4779,0 @@\n-  BoolTest::mask btest = BoolTest::lt;  \/\/ correct for testing is_[obj]array\n-  \/\/ invert the test if we are looking for a non-array\n-  if (not_array)  btest = BoolTest(btest).negate();\n@@ -4365,1 +4781,1 @@\n-  Node* is_array_ctrl = not_array ? control() : ctrl;\n+  Node* is_array_ctrl = kind == NonArray ? control() : ctrl;\n@@ -4374,0 +4790,131 @@\n+\/\/ public static native Object[] ValueClass::newNullRestrictedAtomicArray(Class<?> componentType, int length, Object initVal);\n+\/\/ public static native Object[] ValueClass::newNullRestrictedNonAtomicArray(Class<?> componentType, int length, Object initVal);\n+\/\/ public static native Object[] ValueClass::newNullableAtomicArray(Class<?> componentType, int length);\n+bool LibraryCallKit::inline_newArray(bool null_free, bool atomic) {\n+  assert(null_free || atomic, \"nullable implies atomic\");\n+  Node* componentType = argument(0);\n+  Node* length = argument(1);\n+  Node* init_val = null_free ? argument(2) : nullptr;\n+\n+  const TypeInstPtr* tp = _gvn.type(componentType)->isa_instptr();\n+  if (tp != nullptr) {\n+    ciInstanceKlass* ik = tp->instance_klass();\n+    if (ik == C->env()->Class_klass()) {\n+      ciType* t = tp->java_mirror_type();\n+      if (t != nullptr && t->is_inlinetype()) {\n+\n+        ciArrayKlass* array_klass = ciArrayKlass::make(t, null_free, atomic, true);\n+        assert(array_klass->is_elem_null_free() == null_free, \"inconsistency\");\n+        assert(array_klass->is_elem_atomic() == atomic, \"inconsistency\");\n+\n+        \/\/ TOOD 8350865 ZGC needs card marks on initializing oop stores\n+        if (UseZGC && null_free && !array_klass->is_flat_array_klass()) {\n+          return false;\n+        }\n+\n+        if (array_klass->is_loaded() && array_klass->element_klass()->as_inline_klass()->is_initialized()) {\n+          const TypeAryKlassPtr* array_klass_type = TypeAryKlassPtr::make(array_klass, Type::trust_interfaces, true);\n+          if (null_free) {\n+            if (init_val->is_InlineType()) {\n+              if (array_klass_type->is_flat() && init_val->as_InlineType()->is_all_zero(&gvn(), \/* flat *\/ true)) {\n+                \/\/ Zeroing is enough because the init value is the all-zero value\n+                init_val = nullptr;\n+              } else {\n+                init_val = init_val->as_InlineType()->buffer(this);\n+              }\n+            }\n+            \/\/ TODO 8350865 Should we add a check of the init_val type (maybe in debug only + halt)?\n+          }\n+          Node* obj = new_array(makecon(array_klass_type), length, 0, nullptr, false, init_val);\n+          const TypeAryPtr* arytype = gvn().type(obj)->is_aryptr();\n+          assert(arytype->is_null_free() == null_free, \"inconsistency\");\n+          assert(arytype->is_not_null_free() == !null_free, \"inconsistency\");\n+          assert(arytype->is_atomic() == atomic, \"inconsistency\");\n+          set_result(obj);\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n+\/\/ public static native boolean ValueClass::isFlatArray(Object array);\n+\/\/ public static native boolean ValueClass::isNullRestrictedArray(Object array);\n+\/\/ public static native boolean ValueClass::isAtomicArray(Object array);\n+bool LibraryCallKit::inline_getArrayProperties(ArrayPropertiesCheck check) {\n+  Node* array = argument(0);\n+\n+  Node* bol;\n+  switch(check) {\n+    case IsFlat:\n+      \/\/ TODO 8350865 Use the object version here instead of loading the klass\n+      \/\/ The problem is that PhaseMacroExpand::expand_flatarraycheck_node can only handle some IR shapes and will fail, for example, if the bol is directly wired to a ReturnNode\n+      bol = flat_array_test(load_object_klass(array));\n+      break;\n+    case IsNullRestricted:\n+      bol = null_free_array_test(array);\n+      break;\n+    case IsAtomic:\n+      \/\/ TODO 8350865 Implement this. It's a bit more complicated, see conditions in JVM_IsAtomicArray\n+      \/\/ Enable TestIntrinsics::test87\/88 once this is implemented\n+      \/\/ bol = null_free_atomic_array_test\n+      return false;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+\n+  Node* res = gvn().transform(new CMoveINode(bol, intcon(0), intcon(1), TypeInt::BOOL));\n+  set_result(res);\n+  return true;\n+}\n+\n+\/\/ Load the default refined array klass from an ObjArrayKlass. This relies on the first entry in the\n+\/\/ '_next_refined_array_klass' linked list being the default (see ObjArrayKlass::klass_with_properties).\n+Node* LibraryCallKit::load_default_refined_array_klass(Node* klass_node, bool type_array_guard) {\n+  RegionNode* region = new RegionNode(2);\n+  Node* phi = new PhiNode(region, TypeInstKlassPtr::OBJECT_OR_NULL);\n+\n+  if (type_array_guard) {\n+    generate_typeArray_guard(klass_node, region);\n+    if (region->req() == 3) {\n+      phi->add_req(klass_node);\n+    }\n+  }\n+  Node* adr_refined_klass = basic_plus_adr(klass_node, in_bytes(ObjArrayKlass::next_refined_array_klass_offset()));\n+  Node* refined_klass = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), adr_refined_klass, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT_OR_NULL));\n+\n+  \/\/ Can be null if not initialized yet, just deopt\n+  Node* null_ctl = top();\n+  refined_klass = null_check_oop(refined_klass, &null_ctl, \/* never_see_null= *\/ true);\n+\n+  region->init_req(1, control());\n+  phi->init_req(1, refined_klass);\n+\n+  set_control(_gvn.transform(region));\n+  return _gvn.transform(phi);\n+}\n+\n+\/\/ Load the non-refined array klass from an ObjArrayKlass.\n+Node* LibraryCallKit::load_non_refined_array_klass(Node* klass_node) {\n+  const TypeAryKlassPtr* ary_klass_ptr = _gvn.type(klass_node)->isa_aryklassptr();\n+  if (ary_klass_ptr != nullptr && ary_klass_ptr->klass_is_exact()) {\n+    return _gvn.makecon(ary_klass_ptr->cast_to_refined_array_klass_ptr(false));\n+  }\n+\n+  RegionNode* region = new RegionNode(2);\n+  Node* phi = new PhiNode(region, TypeInstKlassPtr::OBJECT);\n+\n+  generate_typeArray_guard(klass_node, region);\n+  if (region->req() == 3) {\n+    phi->add_req(klass_node);\n+  }\n+  Node* super_adr = basic_plus_adr(klass_node, in_bytes(Klass::super_offset()));\n+  Node* super_klass = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), super_adr, TypeRawPtr::BOTTOM, TypeInstKlassPtr::OBJECT));\n+\n+  region->init_req(1, control());\n+  phi->init_req(1, super_klass);\n+\n+  set_control(_gvn.transform(region));\n+  return _gvn.transform(phi);\n+}\n@@ -4376,1 +4923,1 @@\n-\/\/ private static native Object java.lang.reflect.newArray(Class<?> componentType, int length);\n+\/\/ private static native Object java.lang.reflect.Array.newArray(Class<?> componentType, int length);\n@@ -4434,0 +4981,3 @@\n+\n+    klass_node = load_default_refined_array_klass(klass_node);\n+\n@@ -4522,1 +5072,16 @@\n-    Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);\n+    \/\/ Inline type array may have object field that would require a\n+    \/\/ write barrier. Conservatively, go to slow path.\n+    \/\/ TODO 8251971: Optimize for the case when flat src\/dst are later found\n+    \/\/ to not contain oops (i.e., move this check to the macro expansion phase).\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    const TypeAryPtr* orig_t = _gvn.type(original)->isa_aryptr();\n+    const TypeKlassPtr* tklass = _gvn.type(klass_node)->is_klassptr();\n+    bool exclude_flat = UseArrayFlattening && bs->array_copy_requires_gc_barriers(true, T_OBJECT, false, false, BarrierSetC2::Parsing) &&\n+                        \/\/ Can src array be flat and contain oops?\n+                        (orig_t == nullptr || (!orig_t->is_not_flat() && (!orig_t->is_flat() || orig_t->elem()->inline_klass()->contains_oops()))) &&\n+                        \/\/ Can dest array be flat and contain oops?\n+                        tklass->can_be_inline_array() && (!tklass->is_flat() || tklass->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->as_inline_klass()->contains_oops());\n+    Node* not_objArray = exclude_flat ? generate_non_refArray_guard(klass_node, bailout) : generate_typeArray_guard(klass_node, bailout);\n+\n+    Node* refined_klass_node = load_default_refined_array_klass(klass_node, \/* type_array_guard= *\/ false);\n+\n@@ -4526,3 +5091,3 @@\n-      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0\/*offset*\/);\n-      Node* cast = new CastPPNode(control(), klass_node, akls);\n-      klass_node = _gvn.transform(cast);\n+      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0));\n+      Node* cast = new CastPPNode(control(), refined_klass_node, akls);\n+      refined_klass_node = _gvn.transform(cast);\n@@ -4546,0 +5111,39 @@\n+    \/\/ Handle inline type arrays\n+    bool can_validate = !too_many_traps(Deoptimization::Reason_class_check);\n+    if (!stopped()) {\n+      \/\/ TODO JDK-8329224\n+      if (!orig_t->is_null_free()) {\n+        \/\/ Not statically known to be null free, add a check\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+      orig_t = _gvn.type(original)->isa_aryptr();\n+      if (orig_t != nullptr && orig_t->is_flat()) {\n+        \/\/ Src is flat, check that dest is flat as well\n+        if (exclude_flat) {\n+          \/\/ Dest can't be flat, bail out\n+          bailout->add_req(control());\n+          set_control(top());\n+        } else {\n+          generate_fair_guard(flat_array_test(refined_klass_node, \/* flat = *\/ false), bailout);\n+        }\n+        \/\/ TODO 8350865 This is not correct anymore. Write tests and fix logic similar to arraycopy.\n+      } else if (UseArrayFlattening && (orig_t == nullptr || !orig_t->is_not_flat()) &&\n+                 \/\/ If dest is flat, src must be flat as well (guaranteed by src <: dest check if validated).\n+                 ((!tklass->is_flat() && tklass->can_be_inline_array()) || !can_validate)) {\n+        \/\/ Src might be flat and dest might not be flat. Go to the slow path if src is flat.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat.\n+        generate_fair_guard(flat_array_test(load_object_klass(original)), bailout);\n+        if (orig_t != nullptr) {\n+          orig_t = orig_t->cast_to_not_flat();\n+          original = _gvn.transform(new CheckCastPPNode(control(), original, orig_t));\n+        }\n+      }\n+      if (!can_validate) {\n+        \/\/ No validation. The subtype check emitted at macro expansion time will not go to the slow\n+        \/\/ path but call checkcast_arraycopy which can not handle flat\/null-free inline type arrays.\n+        \/\/ TODO 8251971: Optimize for the case when src\/dest are later found to be both flat\/null-free.\n+        generate_fair_guard(flat_array_test(refined_klass_node), bailout);\n+        generate_fair_guard(null_free_array_test(original), bailout);\n+      }\n+    }\n+\n@@ -4591,1 +5195,1 @@\n-      if (!too_many_traps(Deoptimization::Reason_class_check)) {\n+      if (can_validate) {\n@@ -4605,1 +5209,1 @@\n-        newcopy = new_array(klass_node, length, 0);  \/\/ no arguments to push\n+        newcopy = new_array(refined_klass_node, length, 0);  \/\/ no arguments to push\n@@ -4677,1 +5281,1 @@\n-    const TypeTuple* range = tf->range();\n+    const TypeTuple* range = tf->range_cc();\n@@ -4681,1 +5285,1 @@\n-    tf = TypeFunc::make(tf->domain(), new_range);\n+    tf = TypeFunc::make(tf->domain_cc(), new_range);\n@@ -4738,1 +5342,8 @@\n-  Node* obj = nullptr;\n+  Node* obj = argument(0);\n+\n+  \/\/ Don't intrinsify hashcode on inline types for now.\n+  \/\/ The \"is locked\" runtime check also subsumes the inline type check (as inline types cannot be locked) and goes to the slow path.\n+  if (gvn().type(obj)->is_inlinetypeptr()) {\n+    return false;\n+  }\n+\n@@ -4748,1 +5359,0 @@\n-    obj = argument(0);\n@@ -4789,0 +5399,2 @@\n+    \/\/ We cannot use the inline type mask as this may check bits that are overriden\n+    \/\/ by an object monitor's pointer when inflating locking.\n@@ -4856,1 +5468,10 @@\n-  Node* obj = null_check_receiver();\n+  Node* obj = argument(0);\n+  if (obj->is_InlineType()) {\n+    const Type* t = _gvn.type(obj);\n+    if (t->maybe_null()) {\n+      null_check(obj);\n+    }\n+    set_result(makecon(TypeInstPtr::make(t->inline_klass()->java_mirror())));\n+    return true;\n+  }\n+  obj = null_check_receiver();\n@@ -5278,1 +5899,2 @@\n-    Node* obj = null_check_receiver();\n+    Node* obj = argument(0);\n+    obj = null_check_receiver();\n@@ -5282,0 +5904,6 @@\n+    if (obj_type->is_inlinetypeptr()) {\n+      \/\/ If the object to clone is an inline type, we can simply return it (i.e. a nop) since inline types have\n+      \/\/ no identity.\n+      set_result(obj);\n+      return true;\n+    }\n@@ -5288,1 +5916,2 @@\n-        obj_type->speculative_type()->is_instance_klass()) {\n+        obj_type->speculative_type()->is_instance_klass() &&\n+        !obj_type->speculative_type()->is_inlinetype()) {\n@@ -5318,0 +5947,5 @@\n+    \/\/ We only go to the fast case code if we pass a number of guards.\n+    \/\/ The paths which do not pass are accumulated in the slow_region.\n+    RegionNode* slow_region = new RegionNode(1);\n+    record_for_igvn(slow_region);\n+\n@@ -5324,3 +5958,0 @@\n-      Node* obj_length = load_array_length(array_obj);\n-      Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n-      Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n@@ -5329,20 +5960,7 @@\n-      if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n-        \/\/ If it is an oop array, it requires very special treatment,\n-        \/\/ because gc barriers are required when accessing the array.\n-        Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)nullptr);\n-        if (is_obja != nullptr) {\n-          PreserveJVMState pjvms2(this);\n-          set_control(is_obja);\n-          \/\/ Generate a direct call to the right arraycopy function(s).\n-          \/\/ Clones are always tightly coupled.\n-          ArrayCopyNode* ac = ArrayCopyNode::make(this, true, array_obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n-          ac->set_clone_oop_array();\n-          Node* n = _gvn.transform(ac);\n-          assert(n == ac, \"cannot disappear\");\n-          ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n-\n-          result_reg->init_req(_objArray_path, control());\n-          result_val->init_req(_objArray_path, alloc_obj);\n-          result_i_o ->set_req(_objArray_path, i_o());\n-          result_mem ->set_req(_objArray_path, reset_memory());\n-        }\n+      const TypeAryPtr* ary_ptr = obj_type->isa_aryptr();\n+      if (UseArrayFlattening && bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Expansion) &&\n+          obj_type->can_be_inline_array() &&\n+          (ary_ptr == nullptr || (!ary_ptr->is_not_flat() && (!ary_ptr->is_flat() || ary_ptr->elem()->inline_klass()->contains_oops())))) {\n+        \/\/ Flat inline type array may have object field that would require a\n+        \/\/ write barrier. Conservatively, go to slow path.\n+        generate_fair_guard(flat_array_test(obj_klass), slow_region);\n@@ -5350,7 +5968,0 @@\n-      \/\/ Otherwise, there are no barriers to worry about.\n-      \/\/ (We can dispense with card marks if we know the allocation\n-      \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n-      \/\/  causes the non-eden paths to take compensating steps to\n-      \/\/  simulate a fresh allocation, so that no further\n-      \/\/  card marks are required in compiled code to initialize\n-      \/\/  the object.)\n@@ -5359,7 +5970,43 @@\n-        copy_to_clone(array_obj, alloc_obj, array_size, true);\n-\n-        \/\/ Present the results of the copy.\n-        result_reg->init_req(_array_path, control());\n-        result_val->init_req(_array_path, alloc_obj);\n-        result_i_o ->set_req(_array_path, i_o());\n-        result_mem ->set_req(_array_path, reset_memory());\n+        Node* obj_length = load_array_length(array_obj);\n+        Node* array_size = nullptr; \/\/ Size of the array without object alignment padding.\n+        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &array_size, \/*deoptimize_on_exception=*\/true);\n+\n+        BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+        if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, false, BarrierSetC2::Parsing)) {\n+          \/\/ If it is an oop array, it requires very special treatment,\n+          \/\/ because gc barriers are required when accessing the array.\n+          Node* is_obja = generate_refArray_guard(obj_klass, (RegionNode*)nullptr);\n+          if (is_obja != nullptr) {\n+            PreserveJVMState pjvms2(this);\n+            set_control(is_obja);\n+            \/\/ Generate a direct call to the right arraycopy function(s).\n+            \/\/ Clones are always tightly coupled.\n+            ArrayCopyNode* ac = ArrayCopyNode::make(this, true, array_obj, intcon(0), alloc_obj, intcon(0), obj_length, true, false);\n+            ac->set_clone_oop_array();\n+            Node* n = _gvn.transform(ac);\n+            assert(n == ac, \"cannot disappear\");\n+            ac->connect_outputs(this, \/*deoptimize_on_exception=*\/true);\n+\n+            result_reg->init_req(_objArray_path, control());\n+            result_val->init_req(_objArray_path, alloc_obj);\n+            result_i_o ->set_req(_objArray_path, i_o());\n+            result_mem ->set_req(_objArray_path, reset_memory());\n+          }\n+        }\n+        \/\/ Otherwise, there are no barriers to worry about.\n+        \/\/ (We can dispense with card marks if we know the allocation\n+        \/\/  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks\n+        \/\/  causes the non-eden paths to take compensating steps to\n+        \/\/  simulate a fresh allocation, so that no further\n+        \/\/  card marks are required in compiled code to initialize\n+        \/\/  the object.)\n+\n+        if (!stopped()) {\n+          copy_to_clone(obj, alloc_obj, array_size, true);\n+\n+          \/\/ Present the results of the copy.\n+          result_reg->init_req(_array_path, control());\n+          result_val->init_req(_array_path, alloc_obj);\n+          result_i_o ->set_req(_array_path, i_o());\n+          result_mem ->set_req(_array_path, reset_memory());\n+        }\n@@ -5369,4 +6016,0 @@\n-    \/\/ We only go to the instance fast case code if we pass a number of guards.\n-    \/\/ The paths which do not pass are accumulated in the slow_region.\n-    RegionNode* slow_region = new RegionNode(1);\n-    record_for_igvn(slow_region);\n@@ -5504,0 +6147,12 @@\n+  int adjustment = 1;\n+  const TypeAryKlassPtr* ary_klass_ptr = alloc->in(AllocateNode::KlassNode)->bottom_type()->is_aryklassptr();\n+  if (ary_klass_ptr->is_null_free()) {\n+    \/\/ A null-free, tightly coupled array allocation can only come from LibraryCallKit::inline_newArray which\n+    \/\/ also requires the componentType and initVal on stack for re-execution.\n+    \/\/ Re-create and push the componentType.\n+    ciArrayKlass* klass = ary_klass_ptr->exact_klass()->as_array_klass();\n+    ciInstance* instance = klass->component_mirror_instance();\n+    const TypeInstPtr* t_instance = TypeInstPtr::make(instance);\n+    sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), makecon(t_instance));\n+    adjustment++;\n+  }\n@@ -5505,5 +6160,16 @@\n-  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp(), alloc->in(AllocateNode::ALength));\n-  old_jvms->set_sp(old_jvms->sp()+1);\n-  old_jvms->set_monoff(old_jvms->monoff()+1);\n-  old_jvms->set_scloff(old_jvms->scloff()+1);\n-  old_jvms->set_endoff(old_jvms->endoff()+1);\n+  sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp() + adjustment - 1, alloc->in(AllocateNode::ALength));\n+  if (ary_klass_ptr->is_null_free()) {\n+    \/\/ Re-create and push the initVal.\n+    Node* init_val = alloc->in(AllocateNode::InitValue);\n+    if (init_val == nullptr) {\n+      init_val = InlineTypeNode::make_all_zero(_gvn, ary_klass_ptr->elem()->is_instklassptr()->instance_klass()->as_inline_klass());\n+    } else if (UseCompressedOops) {\n+      init_val = _gvn.transform(new DecodeNNode(init_val, init_val->bottom_type()->make_ptr()));\n+    }\n+    sfpt->ins_req(old_jvms->stkoff() + old_jvms->sp() + adjustment, init_val);\n+    adjustment++;\n+  }\n+  old_jvms->set_sp(old_jvms->sp() + adjustment);\n+  old_jvms->set_monoff(old_jvms->monoff() + adjustment);\n+  old_jvms->set_scloff(old_jvms->scloff() + adjustment);\n+  old_jvms->set_endoff(old_jvms->endoff() + adjustment);\n@@ -5542,2 +6208,1 @@\n-    CallProjections callprojs;\n-    alloc->extract_projections(&callprojs, true);\n+    CallProjections* callprojs = alloc->extract_projections(true);\n@@ -5546,1 +6211,1 @@\n-    C->gvn_replace_by(callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+    C->gvn_replace_by(callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -5588,1 +6253,1 @@\n-    set_i_o(callprojs.fallthrough_ioproj);\n+    set_i_o(callprojs->fallthrough_ioproj);\n@@ -5926,1 +6591,1 @@\n-    if (src_elem == dest_elem && src_elem == T_OBJECT) {\n+    if (src_elem == dest_elem && top_src->is_flat() == top_dest->is_flat() && src_elem == T_OBJECT) {\n@@ -5953,0 +6618,2 @@\n+          src_type = _gvn.type(src);\n+          top_src = src_type->isa_aryptr();\n@@ -5956,0 +6623,2 @@\n+          dest_type = _gvn.type(dest);\n+          top_dest = dest_type->isa_aryptr();\n@@ -5971,2 +6640,1 @@\n-      can_emit_guards &&\n-      !src->is_top() && !dest->is_top()) {\n+      can_emit_guards && !src->is_top() && !dest->is_top()) {\n@@ -6013,0 +6681,1 @@\n+    Node* refined_dest_klass = dest_klass;\n@@ -6014,0 +6683,1 @@\n+      dest_klass = load_non_refined_array_klass(refined_dest_klass);\n@@ -6015,8 +6685,1 @@\n-\n-      if (not_subtype_ctrl != top()) {\n-        PreserveJVMState pjvms(this);\n-        set_control(not_subtype_ctrl);\n-        uncommon_trap(Deoptimization::Reason_intrinsic,\n-                      Deoptimization::Action_make_not_entrant);\n-        assert(stopped(), \"Should be stopped\");\n-      }\n+      slow_region->add_req(not_subtype_ctrl);\n@@ -6024,0 +6687,21 @@\n+\n+    \/\/ TODO 8350865 Improve this. What about atomicity? Make sure this is always folded for type arrays.\n+    \/\/ If destination is null-restricted, source must be null-restricted as well: src_null_restricted || !dst_null_restricted\n+    Node* src_klass = load_object_klass(src);\n+    Node* adr_prop_src = basic_plus_adr(src_klass, in_bytes(ArrayKlass::properties_offset()));\n+    Node* prop_src = _gvn.transform(LoadNode::make(_gvn, control(), immutable_memory(), adr_prop_src, TypeRawPtr::BOTTOM, TypeInt::INT, T_INT, MemNode::unordered));\n+    Node* adr_prop_dest = basic_plus_adr(refined_dest_klass, in_bytes(ArrayKlass::properties_offset()));\n+    Node* prop_dest = _gvn.transform(LoadNode::make(_gvn, control(), immutable_memory(), adr_prop_dest, TypeRawPtr::BOTTOM, TypeInt::INT, T_INT, MemNode::unordered));\n+\n+    prop_dest = _gvn.transform(new XorINode(prop_dest, intcon(ArrayKlass::ArrayProperties::NULL_RESTRICTED)));\n+    prop_src = _gvn.transform(new OrINode(prop_dest, prop_src));\n+    prop_src = _gvn.transform(new AndINode(prop_src, intcon(ArrayKlass::ArrayProperties::NULL_RESTRICTED)));\n+\n+    Node* chk = _gvn.transform(new CmpINode(prop_src, intcon(ArrayKlass::ArrayProperties::NULL_RESTRICTED)));\n+    Node* tst = _gvn.transform(new BoolNode(chk, BoolTest::ne));\n+    generate_fair_guard(tst, slow_region);\n+\n+    \/\/ TODO 8350865 This is too strong\n+    generate_fair_guard(flat_array_test(src), slow_region);\n+    generate_fair_guard(flat_array_test(dest), slow_region);\n+\n@@ -6032,2 +6716,2 @@\n-    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();\n-    const Type *toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n+    const TypeKlassPtr* dest_klass_t = _gvn.type(refined_dest_klass)->is_klassptr();\n+    const Type* toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n@@ -6042,0 +6726,3 @@\n+  Node* dest_klass = load_object_klass(dest);\n+  dest_klass = load_non_refined_array_klass(dest_klass);\n+\n@@ -6046,1 +6733,1 @@\n-                                          load_object_klass(src), load_object_klass(dest),\n+                                          load_object_klass(src), dest_klass,\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":835,"deletions":148,"binary":false,"changes":983,"status":"modified"},{"patch":"@@ -128,1 +128,6 @@\n-  if (phase->find_unswitch_candidate(this) == nullptr) {\n+\n+  if (head->is_flat_arrays()) {\n+    return false;\n+  }\n+\n+  if (no_unswitch_candidate()) {\n@@ -136,0 +141,7 @@\n+\/\/ Check the absence of any If node that can be used for Loop Unswitching. In that case, no Loop Unswitching can be done.\n+bool IdealLoopTree::no_unswitch_candidate() const {\n+  ResourceMark rm;\n+  Node_List dont_care;\n+  return _phase->find_unswitch_candidates(this, dont_care) == nullptr;\n+}\n+\n@@ -137,2 +149,39 @@\n-\/\/ one in the loop body. Return the \"unswitch candidate\" If to apply Loop Unswitching on.\n-IfNode* PhaseIdealLoop::find_unswitch_candidate(const IdealLoopTree* loop) const {\n+\/\/ one in the loop body as \"unswitch candidate\" to apply Loop Unswitching on.\n+\/\/ Depending on whether we find such a candidate and if we do, whether it's a flat array check, we do the following:\n+\/\/ (1) Candidate is not a flat array check:\n+\/\/     Return the unique unswitch candidate.\n+\/\/ (2) Candidate is a flat array check:\n+\/\/     Collect all remaining non-loop-exiting flat array checks in the loop body in the provided 'flat_array_checks'\n+\/\/     list in order to create an unswitched loop version without any flat array checks and a version with checks\n+\/\/     (i.e. same as original loop). Return the initially found candidate which could be unique if no further flat array\n+\/\/     checks are found.\n+\/\/ (3) No candidate is initially found:\n+\/\/     As in (2), we collect all non-loop-exiting flat array checks in the loop body in the provided 'flat_array_checks'\n+\/\/     list. Pick the first collected flat array check as unswitch candidate, which could be unique, and return it (a).\n+\/\/     If there are no flat array checks, we cannot apply Loop Unswitching (b).\n+\/\/\n+\/\/ Note that for both (2) and (3a), if there are multiple flat array checks, then the candidate's FlatArrayCheckNode is\n+\/\/ later updated in Loop Unswitching to perform a flat array check on all collected flat array checks.\n+IfNode* PhaseIdealLoop::find_unswitch_candidates(const IdealLoopTree* loop, Node_List& flat_array_checks) const {\n+  IfNode* unswitch_candidate = find_unswitch_candidate_from_idoms(loop);\n+  if (unswitch_candidate != nullptr && !unswitch_candidate->is_flat_array_check(&_igvn)) {\n+    \/\/ Case (1)\n+    return unswitch_candidate;\n+  }\n+\n+  collect_flat_array_checks(loop, flat_array_checks);\n+  if (unswitch_candidate != nullptr) {\n+    \/\/ Case (2)\n+    assert(unswitch_candidate->is_flat_array_check(&_igvn), \"is a flat array check\");\n+    return unswitch_candidate;\n+  } else if (flat_array_checks.size() > 0) {\n+    \/\/ Case (3a): Pick first one found as candidate (there could be multiple).\n+    return flat_array_checks[0]->as_If();\n+  }\n+\n+  \/\/ Case (3b): No suitable unswitch candidate found.\n+  return nullptr;\n+}\n+\n+\/\/ Find an unswitch candidate by following the idom chain from the loop back edge.\n+IfNode* PhaseIdealLoop::find_unswitch_candidate_from_idoms(const IdealLoopTree* loop) const {\n@@ -165,0 +214,144 @@\n+\/\/ Collect all flat array checks in the provided 'flat_array_checks' list.\n+void PhaseIdealLoop::collect_flat_array_checks(const IdealLoopTree* loop, Node_List& flat_array_checks) const {\n+  assert(flat_array_checks.size() == 0, \"should be empty initially\");\n+  for (uint i = 0; i < loop->_body.size(); i++) {\n+    Node* next = loop->_body.at(i);\n+    if (next->is_If() && next->as_If()->is_flat_array_check(&_igvn) && loop->is_invariant(next->in(1)) &&\n+        !loop->is_loop_exit(next)) {\n+      flat_array_checks.push(next);\n+    }\n+  }\n+}\n+\n+\/\/ This class represents an \"unswitch candidate\" which is an If that can be used to perform Loop Unswitching on. If the\n+\/\/ candidate is a flat array check candidate, then we also collect all remaining non-loop-exiting flat array checks.\n+\/\/ These are candidates as well. We want to get rid of all these flat array checks in the true-path-loop for the\n+\/\/ following reason:\n+\/\/\n+\/\/ FlatArrayCheckNodes are used with array accesses to switch between a flat and a non-flat array access. We want\n+\/\/ the performance impact on non-flat array accesses to be as small as possible. We therefore create the following\n+\/\/ loops in Loop Unswitching:\n+\/\/ - True-path-loop:  We remove all non-loop-exiting flat array checks to get a loop with only non-flat array accesses\n+\/\/                    (i.e. a fast path loop).\n+\/\/ - False-path-loop: We keep all flat array checks in this loop (i.e. a slow path loop).\n+class UnswitchCandidate : public StackObj {\n+  PhaseIdealLoop* const _phase;\n+  const Node_List& _old_new;\n+  Node* const _original_loop_entry;\n+  \/\/ If _candidate is a flat array check, this list contains all non-loop-exiting flat array checks in the loop body.\n+  Node_List _flat_array_check_candidates;\n+  IfNode* const _candidate;\n+\n+ public:\n+  UnswitchCandidate(IdealLoopTree* loop, const Node_List& old_new)\n+      : _phase(loop->_phase),\n+        _old_new(old_new),\n+        _original_loop_entry(loop->_head->as_Loop()->skip_strip_mined()->in(LoopNode::EntryControl)),\n+        _flat_array_check_candidates(),\n+        _candidate(find_unswitch_candidate(loop)) {}\n+  NONCOPYABLE(UnswitchCandidate);\n+\n+  IfNode* find_unswitch_candidate(IdealLoopTree* loop) {\n+    IfNode* unswitch_candidate = _phase->find_unswitch_candidates(loop, _flat_array_check_candidates);\n+    assert(unswitch_candidate != nullptr, \"guaranteed to exist by policy_unswitching\");\n+    assert(_phase->is_member(loop, unswitch_candidate), \"must be inside original loop\");\n+    return unswitch_candidate;\n+  }\n+\n+  IfNode* candidate() const {\n+    return _candidate;\n+  }\n+\n+  \/\/ Is the candidate a flat array check and are there other flat array checks as well?\n+  bool has_multiple_flat_array_check_candidates() const {\n+    return _flat_array_check_candidates.size() > 1;\n+  }\n+\n+  \/\/ Remove all candidates from the true-path-loop which are now dominated by the loop selector\n+  \/\/ (i.e. 'true_path_loop_proj'). The removed candidates are folded in the next IGVN round.\n+  void update_in_true_path_loop(IfTrueNode* true_path_loop_proj) const {\n+    remove_from_loop(true_path_loop_proj, _candidate);\n+    if (has_multiple_flat_array_check_candidates()) {\n+      remove_flat_array_checks(true_path_loop_proj);\n+    }\n+  }\n+\n+  \/\/ Remove a unique candidate from the false-path-loop which is now dominated by the loop selector\n+  \/\/ (i.e. 'false_path_loop_proj'). The removed candidate is folded in the next IGVN round. If there are multiple\n+  \/\/ candidates (i.e. flat array checks), then we leave them in the false-path-loop and only mark the loop such that it\n+  \/\/ is not unswitched anymore in later loop opts rounds.\n+  void update_in_false_path_loop(IfFalseNode* false_path_loop_proj, LoopNode* false_path_loop) const {\n+    if (has_multiple_flat_array_check_candidates()) {\n+      \/\/ Leave the flat array checks in the false-path-loop and prevent it from being unswitched again based on these\n+      \/\/ checks.\n+      false_path_loop->mark_flat_arrays();\n+    } else {\n+      remove_from_loop(false_path_loop_proj, _old_new[_candidate->_idx]->as_If());\n+    }\n+  }\n+\n+ private:\n+  void remove_from_loop(IfProjNode* dominating_proj, IfNode* candidate) const {\n+    _phase->igvn().rehash_node_delayed(candidate);\n+    _phase->dominated_by(dominating_proj, candidate);\n+  }\n+\n+  void remove_flat_array_checks(IfProjNode* dominating_proj) const {\n+    for (uint i = 0; i < _flat_array_check_candidates.size(); i++) {\n+      IfNode* flat_array_check = _flat_array_check_candidates.at(i)->as_If();\n+      _phase->igvn().rehash_node_delayed(flat_array_check);\n+      _phase->dominated_by(dominating_proj, flat_array_check);\n+    }\n+  }\n+\n+ public:\n+  \/\/ Merge all flat array checks into a single new BoolNode and return it.\n+  BoolNode* merge_flat_array_checks() const {\n+    assert(has_multiple_flat_array_check_candidates(), \"must have multiple flat array checks to merge\");\n+    assert(_candidate->in(1)->as_Bool()->_test._test == BoolTest::ne, \"IfTrue proj must point to flat array\");\n+    BoolNode* merged_flat_array_check_bool = create_bool_node();\n+    create_flat_array_check_node(merged_flat_array_check_bool);\n+    return merged_flat_array_check_bool;\n+  }\n+\n+ private:\n+  BoolNode* create_bool_node() const {\n+    BoolNode* merged_flat_array_check_bool = _candidate->in(1)->clone()->as_Bool();\n+    _phase->register_new_node(merged_flat_array_check_bool, _original_loop_entry);\n+    return merged_flat_array_check_bool;\n+  }\n+\n+  void create_flat_array_check_node(BoolNode* merged_flat_array_check_bool) const {\n+    FlatArrayCheckNode* cloned_flat_array_check = merged_flat_array_check_bool->in(1)->clone()->as_FlatArrayCheck();\n+    _phase->register_new_node(cloned_flat_array_check, _original_loop_entry);\n+    merged_flat_array_check_bool->set_req(1, cloned_flat_array_check);\n+    set_flat_array_check_inputs(cloned_flat_array_check);\n+  }\n+\n+  \/\/ Combine all checks into a single one that fails if one array is flat.\n+  void set_flat_array_check_inputs(FlatArrayCheckNode* cloned_flat_array_check) const {\n+    assert(cloned_flat_array_check->req() == 3, \"unexpected number of inputs for FlatArrayCheck\");\n+    cloned_flat_array_check->add_req_batch(_phase->C->top(), _flat_array_check_candidates.size() - 1);\n+    for (uint i = 0; i < _flat_array_check_candidates.size(); i++) {\n+      Node* array = _flat_array_check_candidates.at(i)->in(1)->in(1)->in(FlatArrayCheckNode::ArrayOrKlass);\n+      cloned_flat_array_check->set_req(FlatArrayCheckNode::ArrayOrKlass + i, array);\n+    }\n+  }\n+\n+ public:\n+#ifndef PRODUCT\n+  void trace_flat_array_checks() const {\n+    if (has_multiple_flat_array_check_candidates()) {\n+      tty->print_cr(\"- Unswitched and Merged Flat Array Checks:\");\n+      for (uint i = 0; i < _flat_array_check_candidates.size(); i++) {\n+        Node* unswitch_iff = _flat_array_check_candidates.at(i);\n+        Node* cloned_unswitch_iff = _old_new[unswitch_iff->_idx];\n+        assert(cloned_unswitch_iff != nullptr, \"must exist\");\n+        tty->print_cr(\"  - %d %s  ->  %d %s\", unswitch_iff->_idx, unswitch_iff->Name(),\n+                      cloned_unswitch_iff->_idx, cloned_unswitch_iff->Name());\n+      }\n+    }\n+  }\n+#endif \/\/ NOT PRODUCT\n+};\n+\n@@ -179,1 +372,3 @@\n-  enum PathToLoop { TRUE_PATH, FALSE_PATH };\n+  enum PathToLoop {\n+    TRUE_PATH, FALSE_PATH\n+  };\n@@ -195,1 +390,1 @@\n-  LoopSelector(IdealLoopTree* loop, IfNode* unswitch_candidate)\n+  LoopSelector(IdealLoopTree* loop, const UnswitchCandidate& unswitch_candidate)\n@@ -206,0 +401,1 @@\n+ private:\n@@ -213,1 +409,2 @@\n-  IfNode* create_unswitching_if(IfNode* unswitch_candidate) {\n+  IfNode* create_unswitching_if(const UnswitchCandidate& unswitch_candidate) {\n+    const uint dom_depth = _phase->dom_depth(_original_loop_entry);\n@@ -215,4 +412,9 @@\n-    BoolNode* unswitch_candidate_bool = unswitch_candidate->in(1)->as_Bool();\n-    IfNode* selector_if = IfNode::make_with_same_profile(unswitch_candidate, _original_loop_entry,\n-                                                         unswitch_candidate_bool);\n-    _phase->register_node(selector_if, _outer_loop, _original_loop_entry, _dom_depth);\n+    IfNode* unswitch_candidate_if = unswitch_candidate.candidate();\n+    BoolNode* selector_bool;\n+    if (unswitch_candidate.has_multiple_flat_array_check_candidates()) {\n+      selector_bool = unswitch_candidate.merge_flat_array_checks();\n+    } else {\n+      selector_bool = unswitch_candidate_if->in(1)->as_Bool();\n+    }\n+    IfNode* selector_if = IfNode::make_with_same_profile(unswitch_candidate_if, _original_loop_entry, selector_bool);\n+    _phase->register_node(selector_if, _outer_loop, _original_loop_entry, dom_depth);\n@@ -222,1 +424,0 @@\n- private:\n@@ -252,1 +453,1 @@\n-  IfNode* const _unswitch_candidate;\n+  const UnswitchCandidate& _unswitch_candidate;\n@@ -256,2 +457,2 @@\n-  UnswitchedLoopSelector(IdealLoopTree* loop)\n-      : _unswitch_candidate(find_unswitch_candidate(loop)),\n+  UnswitchedLoopSelector(IdealLoopTree* loop, const UnswitchCandidate& unswitch_candidate)\n+      : _unswitch_candidate(unswitch_candidate),\n@@ -261,11 +462,2 @@\n- private:\n-  static IfNode* find_unswitch_candidate(IdealLoopTree* loop) {\n-    IfNode* unswitch_candidate = loop->_phase->find_unswitch_candidate(loop);\n-    assert(unswitch_candidate != nullptr, \"guaranteed to exist by policy_unswitching\");\n-    assert(loop->_phase->is_member(loop, unswitch_candidate), \"must be inside original loop\");\n-    return unswitch_candidate;\n-  }\n-\n- public:\n-  IfNode* unswitch_candidate() const {\n-    return _unswitch_candidate;\n+  IfNode* selector_if() const {\n+    return _loop_selector.selector();\n@@ -301,1 +493,0 @@\n-    remove_unswitch_candidate_from_loops(unswitched_loop_selector);\n@@ -366,14 +557,0 @@\n-\n-  \/\/ Remove the unswitch candidate If nodes in both unswitched loop versions which are now dominated by the loop selector\n-  \/\/ If node. Keep the true-path-path in the true-path-loop and the false-path-path in the false-path-loop by setting\n-  \/\/ the bool input accordingly. The unswitch candidate If nodes are folded in the next IGVN round.\n-  void remove_unswitch_candidate_from_loops(const UnswitchedLoopSelector& unswitched_loop_selector) {\n-    const LoopSelector& loop_selector = unswitched_loop_selector.loop_selector();;\n-    IfNode* unswitch_candidate        = unswitched_loop_selector.unswitch_candidate();\n-    _phase->igvn().rehash_node_delayed(unswitch_candidate);\n-    _phase->dominated_by(loop_selector.true_path_loop_proj(), unswitch_candidate);\n-\n-    IfNode* unswitch_candidate_clone = _old_new[unswitch_candidate->_idx]->as_If();\n-    _phase->igvn().rehash_node_delayed(unswitch_candidate_clone);\n-    _phase->dominated_by(loop_selector.false_path_loop_proj(), unswitch_candidate_clone);\n-  }\n@@ -397,1 +574,2 @@\n-  const UnswitchedLoopSelector unswitched_loop_selector(loop);\n+  const UnswitchCandidate unswitch_candidate(loop, old_new);\n+  const UnswitchedLoopSelector unswitched_loop_selector(loop, unswitch_candidate);\n@@ -401,1 +579,4 @@\n-  hoist_invariant_check_casts(loop, old_new, unswitched_loop_selector);\n+  unswitch_candidate.update_in_true_path_loop(unswitched_loop_selector.loop_selector().true_path_loop_proj());\n+  unswitch_candidate.update_in_false_path_loop(unswitched_loop_selector.loop_selector().false_path_loop_proj(),\n+                                               old_new[original_head->_idx]->as_Loop());\n+  hoist_invariant_check_casts(loop, old_new, unswitch_candidate, unswitched_loop_selector.selector_if());\n@@ -407,1 +588,1 @@\n-  NOT_PRODUCT(trace_loop_unswitching_result(unswitched_loop_selector, original_head, new_head);)\n+  NOT_PRODUCT(trace_loop_unswitching_result(unswitched_loop_selector, unswitch_candidate, original_head, new_head);)\n@@ -614,0 +795,1 @@\n+                                                   const UnswitchCandidate& unswitch_candidate,\n@@ -616,2 +798,2 @@\n-    IfNode* unswitch_candidate = unswitched_loop_selector.unswitch_candidate();\n-    IfNode* loop_selector = unswitched_loop_selector.loop_selector().selector();\n+    IfNode* unswitch_candidate_if = unswitch_candidate.candidate();\n+    IfNode* loop_selector = unswitched_loop_selector.selector_if();\n@@ -619,1 +801,1 @@\n-    tty->print_cr(\"- Unswitch-Candidate-If: %d %s\", unswitch_candidate->_idx, unswitch_candidate->Name());\n+    tty->print_cr(\"- Unswitch-Candidate-If: %d %s\", unswitch_candidate_if->_idx, unswitch_candidate_if->Name());\n@@ -623,0 +805,1 @@\n+    unswitch_candidate.trace_flat_array_checks();\n@@ -649,3 +832,2 @@\n-                                                 const UnswitchedLoopSelector& unswitched_loop_selector) {\n-  IfNode* unswitch_candidate = unswitched_loop_selector.unswitch_candidate();\n-  IfNode* loop_selector = unswitched_loop_selector.loop_selector().selector();\n+                                                 const UnswitchCandidate& unswitch_candidate,\n+                                                 const IfNode* loop_selector) {\n@@ -654,2 +836,3 @@\n-  for (DUIterator_Fast imax, i = unswitch_candidate->fast_outs(imax); i < imax; i++) {\n-    IfProjNode* proj = unswitch_candidate->fast_out(i)->as_IfProj();\n+  const IfNode* unswitch_candidate_if = unswitch_candidate.candidate();\n+  for (DUIterator_Fast imax, i = unswitch_candidate_if->fast_outs(imax); i < imax; i++) {\n+    IfProjNode* proj = unswitch_candidate_if->fast_out(i)->as_IfProj();\n@@ -670,3 +853,6 @@\n-      \/\/ Same for the clone\n-      Node* use_clone = old_new[cast->_idx];\n-      _igvn.replace_input_of(use_clone, 1, cast_clone);\n+      \/\/ Same for the false-path-loop if there are not multiple flat array checks (in that case we leave the\n+      \/\/ false-path-loop unchanged).\n+      if (!unswitch_candidate.has_multiple_flat_array_check_candidates()) {\n+        Node* use_clone = old_new[cast->_idx];\n+        _igvn.replace_input_of(use_clone, 1, cast_clone);\n+      }\n@@ -680,1 +866,1 @@\n-  for(int i = loop->_body.size() - 1; i >= 0 ; i--) {\n+  for (int i = loop->_body.size() - 1; i >= 0; i--) {\n@@ -692,1 +878,0 @@\n-\n","filename":"src\/hotspot\/share\/opto\/loopUnswitch.cpp","additions":241,"deletions":56,"binary":false,"changes":297,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+class UnswitchCandidate;\n@@ -88,1 +89,1 @@\n-       };\n+         FlatArrays            = 1<<18};\n@@ -111,0 +112,1 @@\n+  bool is_flat_arrays() const { return _loop_flags & FlatArrays; }\n@@ -124,0 +126,1 @@\n+  void mark_flat_arrays() { _loop_flags |= FlatArrays; }\n@@ -756,0 +759,1 @@\n+  bool no_unswitch_candidate() const;\n@@ -1560,1 +1564,2 @@\n-  IfNode* find_unswitch_candidate(const IdealLoopTree* loop) const;\n+  IfNode* find_unswitch_candidates(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+  IfNode* find_unswitch_candidate_from_idoms(const IdealLoopTree* loop) const;\n@@ -1567,1 +1572,1 @@\n-                                   const UnswitchedLoopSelector& unswitched_loop_selector);\n+                                   const UnswitchCandidate& unswitch_candidate, const IfNode* loop_selector);\n@@ -1575,0 +1580,1 @@\n+                                            const UnswitchCandidate& unswitch_candidate,\n@@ -1769,0 +1775,1 @@\n+  void move_flat_array_check_out_of_loop(Node* n);\n@@ -1770,0 +1777,1 @@\n+  bool flat_array_element_type_check(Node *n);\n@@ -1961,0 +1969,2 @@\n+  void collect_flat_array_checks(const IdealLoopTree* loop, Node_List& flat_array_checks) const;\n+\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":13,"deletions":3,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -66,0 +67,6 @@\n+  \/\/ Inline types should not be split through Phis because they cannot be merged\n+  \/\/ through Phi nodes but each value input needs to be merged individually.\n+  if (n->is_InlineType()) {\n+    return nullptr;\n+  }\n+\n@@ -770,0 +777,4 @@\n+      if (inp->isa_InlineType()) {\n+        \/\/ TODO 8302217 This prevents PhiNode::push_inline_types_through\n+        return nullptr;\n+      }\n@@ -1099,0 +1110,48 @@\n+\/\/ We can't use immutable memory for the flat array check because we are loading the mark word which is\n+\/\/ mutable. Although the bits we are interested in are immutable (we check for markWord::unlocked_value),\n+\/\/ we need to use raw memory to not break anti dependency analysis. Below code will attempt to still move\n+\/\/ flat array checks out of loops, mainly to enable loop unswitching.\n+void PhaseIdealLoop::move_flat_array_check_out_of_loop(Node* n) {\n+  \/\/ Skip checks for more than one array\n+  if (n->req() > 3) {\n+    return;\n+  }\n+  Node* mem = n->in(FlatArrayCheckNode::Memory);\n+  Node* array = n->in(FlatArrayCheckNode::ArrayOrKlass)->uncast();\n+  IdealLoopTree* check_loop = get_loop(get_ctrl(n));\n+  IdealLoopTree* ary_loop = get_loop(get_ctrl(array));\n+\n+  \/\/ Check if array is loop invariant\n+  if (!check_loop->is_member(ary_loop)) {\n+    \/\/ Walk up memory graph from the check until we leave the loop\n+    VectorSet wq;\n+    wq.set(mem->_idx);\n+    while (check_loop->is_member(get_loop(ctrl_or_self(mem)))) {\n+      if (mem->is_Phi()) {\n+        mem = mem->in(1);\n+      } else if (mem->is_MergeMem()) {\n+        mem = mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw);\n+      } else if (mem->is_Proj()) {\n+        mem = mem->in(0);\n+      } else if (mem->is_MemBar() || mem->is_SafePoint()) {\n+        mem = mem->in(TypeFunc::Memory);\n+      } else if (mem->is_Store() || mem->is_LoadStore() || mem->is_ClearArray()) {\n+        mem = mem->in(MemNode::Memory);\n+      } else {\n+#ifdef ASSERT\n+        mem->dump();\n+#endif\n+        ShouldNotReachHere();\n+      }\n+      if (wq.test_set(mem->_idx)) {\n+        return;\n+      }\n+    }\n+    \/\/ Replace memory input and re-compute ctrl to move the check out of the loop\n+    _igvn.replace_input_of(n, 1, mem);\n+    set_ctrl_and_loop(n, get_early_ctrl(n));\n+    Node* bol = n->unique_out();\n+    set_ctrl_and_loop(bol, get_early_ctrl(bol));\n+  }\n+}\n+\n@@ -1111,0 +1170,6 @@\n+\n+  if (n->isa_FlatArrayCheck()) {\n+    move_flat_array_check_out_of_loop(n);\n+    return n;\n+  }\n+\n@@ -1386,0 +1451,98 @@\n+bool PhaseIdealLoop::flat_array_element_type_check(Node *n) {\n+  \/\/ If the CmpP is a subtype check for a value that has just been\n+  \/\/ loaded from an array, the subtype check guarantees the value\n+  \/\/ can't be stored in a flat array and the load of the value\n+  \/\/ happens with a flat array check then: push the type check\n+  \/\/ through the phi of the flat array check. This needs special\n+  \/\/ logic because the subtype check's input is not a phi but a\n+  \/\/ LoadKlass that must first be cloned through the phi.\n+  if (n->Opcode() != Op_CmpP) {\n+    return false;\n+  }\n+\n+  Node* klassptr = n->in(1);\n+  Node* klasscon = n->in(2);\n+\n+  if (klassptr->is_DecodeNarrowPtr()) {\n+    klassptr = klassptr->in(1);\n+  }\n+\n+  if (klassptr->Opcode() != Op_LoadKlass && klassptr->Opcode() != Op_LoadNKlass) {\n+    return false;\n+  }\n+\n+  if (!klasscon->is_Con()) {\n+    return false;\n+  }\n+\n+  Node* addr = klassptr->in(MemNode::Address);\n+\n+  if (!addr->is_AddP()) {\n+    return false;\n+  }\n+\n+  intptr_t offset;\n+  Node* obj = AddPNode::Ideal_base_and_offset(addr, &_igvn, offset);\n+\n+  if (obj == nullptr) {\n+    return false;\n+  }\n+\n+  assert(obj != nullptr && addr->in(AddPNode::Base) == addr->in(AddPNode::Address), \"malformed AddP?\");\n+  if (obj->Opcode() == Op_CastPP) {\n+    obj = obj->in(1);\n+  }\n+\n+  if (!obj->is_Phi()) {\n+    return false;\n+  }\n+\n+  Node* region = obj->in(0);\n+\n+  Node* phi = PhiNode::make_blank(region, n->in(1));\n+  for (uint i = 1; i < region->req(); i++) {\n+    Node* in = obj->in(i);\n+    Node* ctrl = region->in(i);\n+    if (addr->in(AddPNode::Base) != obj) {\n+      Node* cast = addr->in(AddPNode::Base);\n+      assert(cast->Opcode() == Op_CastPP && cast->in(0) != nullptr, \"inconsistent subgraph\");\n+      Node* cast_clone = cast->clone();\n+      cast_clone->set_req(0, ctrl);\n+      cast_clone->set_req(1, in);\n+      register_new_node(cast_clone, ctrl);\n+      const Type* tcast = cast_clone->Value(&_igvn);\n+      _igvn.set_type(cast_clone, tcast);\n+      cast_clone->as_Type()->set_type(tcast);\n+      in = cast_clone;\n+    }\n+    Node* addr_clone = addr->clone();\n+    addr_clone->set_req(AddPNode::Base, in);\n+    addr_clone->set_req(AddPNode::Address, in);\n+    register_new_node(addr_clone, ctrl);\n+    _igvn.set_type(addr_clone, addr_clone->Value(&_igvn));\n+    Node* klassptr_clone = klassptr->clone();\n+    klassptr_clone->set_req(2, addr_clone);\n+    register_new_node(klassptr_clone, ctrl);\n+    _igvn.set_type(klassptr_clone, klassptr_clone->Value(&_igvn));\n+    if (klassptr != n->in(1)) {\n+      Node* decode = n->in(1);\n+      assert(decode->is_DecodeNarrowPtr(), \"inconsistent subgraph\");\n+      Node* decode_clone = decode->clone();\n+      decode_clone->set_req(1, klassptr_clone);\n+      register_new_node(decode_clone, ctrl);\n+      _igvn.set_type(decode_clone, decode_clone->Value(&_igvn));\n+      klassptr_clone = decode_clone;\n+    }\n+    phi->set_req(i, klassptr_clone);\n+  }\n+  register_new_node(phi, region);\n+  Node* orig = n->in(1);\n+  _igvn.replace_input_of(n, 1, phi);\n+  split_if_with_blocks_post(n);\n+  if (n->outcnt() != 0) {\n+    _igvn.replace_input_of(n, 1, orig);\n+    _igvn.remove_dead_node(phi);\n+  }\n+  return true;\n+}\n+\n@@ -1392,0 +1555,4 @@\n+  if (flat_array_element_type_check(n)) {\n+    return;\n+  }\n+\n@@ -1542,0 +1709,5 @@\n+\n+  \/\/ Remove multiple allocations of the same inline type\n+  if (n->is_InlineType()) {\n+    n->as_InlineType()->remove_redundant_allocations(this);\n+  }\n@@ -2050,1 +2222,9 @@\n-  Node *sample_cmp = sample_bool->in(1);\n+  Node* sample_cmp = sample_bool->in(1);\n+  const Type* t = Type::TOP;\n+  const TypePtr* at = nullptr;\n+  if (sample_cmp->is_FlatArrayCheck()) {\n+    \/\/ Left input of a FlatArrayCheckNode is memory, set the (adr) type of the phi accordingly\n+    assert(sample_cmp->in(1)->bottom_type() == Type::MEMORY, \"unexpected input type\");\n+    t = Type::MEMORY;\n+    at = TypeRawPtr::BOTTOM;\n+  }\n@@ -2053,1 +2233,1 @@\n-  PhiNode *phi1 = new PhiNode(phi->in(0), Type::TOP);\n+  PhiNode *phi1 = new PhiNode(phi->in(0), t, at);\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":182,"deletions":2,"binary":false,"changes":184,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+class MachVEPNode;\n@@ -512,0 +513,30 @@\n+\/\/------------------------------MachVEPNode-----------------------------------\n+\/\/ Machine Inline Type Entry Point Node\n+class MachVEPNode : public MachIdealNode {\n+public:\n+  Label* _verified_entry;\n+\n+  MachVEPNode(Label* verified_entry, bool verified, bool receiver_only) :\n+    _verified_entry(verified_entry),\n+    _verified(verified),\n+    _receiver_only(receiver_only) {\n+    init_class_id(Class_MachVEP);\n+  }\n+  virtual bool cmp(const Node &n) const {\n+    return (_verified_entry == ((MachVEPNode&)n)._verified_entry) &&\n+           (_verified == ((MachVEPNode&)n)._verified) &&\n+           (_receiver_only == ((MachVEPNode&)n)._receiver_only) &&\n+           MachIdealNode::cmp(n);\n+  }\n+  virtual uint size_of() const { return sizeof(*this); }\n+  virtual void emit(C2_MacroAssembler *masm, PhaseRegAlloc* ra_) const;\n+\n+#ifndef PRODUCT\n+  virtual const char* Name() const { return \"InlineType Entry-Point\"; }\n+  virtual void format(PhaseRegAlloc*, outputStream* st) const;\n+#endif\n+private:\n+  bool   _verified;\n+  bool   _receiver_only;\n+};\n+\n@@ -518,1 +549,0 @@\n-  virtual uint size(PhaseRegAlloc *ra_) const;\n@@ -530,1 +560,9 @@\n-  MachPrologNode( ) {}\n+  Label* _verified_entry;\n+\n+  MachPrologNode(Label* verified_entry) : _verified_entry(verified_entry) {\n+    init_class_id(Class_MachProlog);\n+  }\n+  virtual bool cmp(const Node &n) const {\n+    return (_verified_entry == ((MachPrologNode&)n)._verified_entry) && MachIdealNode::cmp(n);\n+  }\n+  virtual uint size_of() const { return sizeof(*this); }\n@@ -532,1 +570,0 @@\n-  virtual uint size(PhaseRegAlloc *ra_) const;\n@@ -549,1 +586,0 @@\n-  virtual uint size(PhaseRegAlloc *ra_) const;\n@@ -941,1 +977,1 @@\n-  NOT_LP64(bool return_value_is_used() const;)\n+  bool return_value_is_used() const;\n@@ -945,0 +981,1 @@\n+  bool returns_scalarized() const;\n","filename":"src\/hotspot\/share\/opto\/machnode.hpp","additions":42,"deletions":5,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -201,0 +201,12 @@\n+  \/\/ Code pattern on return from a call that returns an __Value.  Can\n+  \/\/ be optimized away if the return value turns out to be an oop.\n+  if (op == Op_AndX &&\n+      in(1) != nullptr &&\n+      in(1)->Opcode() == Op_CastP2X &&\n+      in(1)->in(1) != nullptr &&\n+      phase->type(in(1)->in(1))->isa_oopptr() &&\n+      t2->isa_intptr_t()->_lo >= 0 &&\n+      t2->isa_intptr_t()->_hi <= MinObjAlignmentInBytesMask) {\n+    return add_id();\n+  }\n+\n@@ -932,0 +944,41 @@\n+  \/\/ Search for GraphKit::mark_word_test patterns and fold the test if the result is statically known\n+  Node* load1 = in(1);\n+  Node* load2 = nullptr;\n+  if (load1->is_Phi() && phase->type(load1)->isa_long()) {\n+    load1 = in(1)->in(1);\n+    load2 = in(1)->in(2);\n+  }\n+  if (load1 != nullptr && load1->is_Load() && phase->type(load1)->isa_long() &&\n+      (load2 == nullptr || (load2->is_Load() && phase->type(load2)->isa_long()))) {\n+    const TypePtr* adr_t1 = phase->type(load1->in(MemNode::Address))->isa_ptr();\n+    const TypePtr* adr_t2 = (load2 != nullptr) ? phase->type(load2->in(MemNode::Address))->isa_ptr() : nullptr;\n+    if (adr_t1 != nullptr && adr_t1->offset() == oopDesc::mark_offset_in_bytes() &&\n+        (load2 == nullptr || (adr_t2 != nullptr && adr_t2->offset() == in_bytes(Klass::prototype_header_offset())))) {\n+      if (mask == markWord::inline_type_pattern) {\n+        if (adr_t1->is_inlinetypeptr()) {\n+          set_req_X(1, in(2), phase);\n+          return this;\n+        } else if (!adr_t1->can_be_inline_type()) {\n+          set_req_X(1, phase->longcon(0), phase);\n+          return this;\n+        }\n+      } else if (mask == markWord::null_free_array_bit_in_place) {\n+        if (adr_t1->is_null_free()) {\n+          set_req_X(1, in(2), phase);\n+          return this;\n+        } else if (adr_t1->is_not_null_free()) {\n+          set_req_X(1, phase->longcon(0), phase);\n+          return this;\n+        }\n+      } else if (mask == markWord::flat_array_bit_in_place) {\n+        if (adr_t1->is_flat()) {\n+          set_req_X(1, in(2), phase);\n+          return this;\n+        } else if (adr_t1->is_not_flat()) {\n+          set_req_X(1, phase->longcon(0), phase);\n+          return this;\n+        }\n+      }\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/mulnode.cpp","additions":53,"deletions":0,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -26,0 +27,1 @@\n+#include \"ci\/ciSymbols.hpp\"\n@@ -38,0 +40,2 @@\n+#include \"opto\/idealKit.hpp\"\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -52,0 +56,17 @@\n+Node* Parse::record_profile_for_speculation_at_array_load(Node* ld) {\n+  \/\/ Feed unused profile data to type speculation\n+  if (UseTypeSpeculation && UseArrayLoadStoreProfile) {\n+    ciKlass* array_type = nullptr;\n+    ciKlass* element_type = nullptr;\n+    ProfilePtrKind element_ptr = ProfileMaybeNull;\n+    bool flat_array = true;\n+    bool null_free_array = true;\n+    method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+    if (element_type != nullptr || element_ptr != ProfileMaybeNull) {\n+      ld = record_profile_for_speculation(ld, element_type, element_ptr);\n+    }\n+  }\n+  return ld;\n+}\n+\n+\n@@ -55,1 +76,0 @@\n-  bool big_val = bt == T_DOUBLE || bt == T_LONG;\n@@ -59,2 +79,62 @@\n-  pop();                      \/\/ index (already used)\n-  Node* array = pop();        \/\/ the array itself\n+  Node* array_index = pop();\n+  Node* array = pop();\n+\n+  \/\/ Handle inline type arrays\n+  const TypeOopPtr* element_ptr = elemtype->make_oopptr();\n+  const TypeAryPtr* array_type = _gvn.type(array)->is_aryptr();\n+\n+  if (!array_type->is_not_flat()) {\n+    \/\/ Cannot statically determine if array is a flat array, emit runtime check\n+    assert(UseArrayFlattening && is_reference_type(bt) && element_ptr->can_be_inline_type() &&\n+           (!element_ptr->is_inlinetypeptr() || element_ptr->inline_klass()->maybe_flat_in_array()), \"array can't be flat\");\n+    IdealKit ideal(this);\n+    IdealVariable res(ideal);\n+    ideal.declarations_done();\n+    ideal.if_then(flat_array_test(array, \/* flat = *\/ false)); {\n+      \/\/ Non-flat array\n+      sync_kit(ideal);\n+      if (!array_type->is_flat()) {\n+        assert(array_type->is_flat() || control()->in(0)->as_If()->is_flat_array_check(&_gvn), \"Should be found\");\n+        const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);\n+        DecoratorSet decorator_set = IN_HEAP | IS_ARRAY | C2_CONTROL_DEPENDENT_LOAD;\n+        if (needs_range_check(array_type->size(), array_index)) {\n+          \/\/ We've emitted a RangeCheck but now insert an additional check between the range check and the actual load.\n+          \/\/ We cannot pin the load to two separate nodes. Instead, we pin it conservatively here such that it cannot\n+          \/\/ possibly float above the range check at any point.\n+          decorator_set |= C2_UNKNOWN_CONTROL_LOAD;\n+        }\n+        Node* ld = access_load_at(array, adr, adr_type, element_ptr, bt, decorator_set);\n+        if (element_ptr->is_inlinetypeptr()) {\n+          ld = InlineTypeNode::make_from_oop(this, ld, element_ptr->inline_klass());\n+        }\n+        ideal.set(res, ld);\n+      }\n+      ideal.sync_kit(this);\n+    } ideal.else_(); {\n+      \/\/ Flat array\n+      sync_kit(ideal);\n+      if (!array_type->is_not_flat()) {\n+        if (element_ptr->is_inlinetypeptr()) {\n+          ciInlineKlass* vk = element_ptr->inline_klass();\n+          Node* flat_array = cast_to_flat_array(array, vk, false, false, false);\n+          Node* vt = InlineTypeNode::make_from_flat_array(this, vk, flat_array, array_index);\n+          ideal.set(res, vt);\n+        } else {\n+          \/\/ Element type is unknown, and thus we cannot statically determine the exact flat array layout. Emit a\n+          \/\/ runtime call to correctly load the inline type element from the flat array.\n+          Node* inline_type = load_from_unknown_flat_array(array, array_index, element_ptr);\n+          bool is_null_free = array_type->is_null_free() || !UseNullableValueFlattening;\n+          if (is_null_free) {\n+            inline_type = cast_not_null(inline_type);\n+          }\n+          ideal.set(res, inline_type);\n+        }\n+      }\n+      ideal.sync_kit(this);\n+    } ideal.end_if();\n+    sync_kit(ideal);\n+    Node* ld = _gvn.transform(ideal.value(res));\n+    ld = record_profile_for_speculation_at_array_load(ld);\n+    push_node(bt, ld);\n+    return;\n+  }\n@@ -66,1 +146,0 @@\n-\n@@ -69,4 +148,5 @@\n-  if (big_val) {\n-    push_pair(ld);\n-  } else {\n-    push(ld);\n+  ld = record_profile_for_speculation_at_array_load(ld);\n+  \/\/ Loading an inline type from a non-flat array\n+  if (element_ptr != nullptr && element_ptr->is_inlinetypeptr()) {\n+    assert(!array_type->is_null_free() || !element_ptr->maybe_null(), \"inline type array elements should never be null\");\n+    ld = InlineTypeNode::make_from_oop(this, ld, element_ptr->inline_klass());\n@@ -74,0 +154,1 @@\n+  push_node(bt, ld);\n@@ -76,0 +157,28 @@\n+Node* Parse::load_from_unknown_flat_array(Node* array, Node* array_index, const TypeOopPtr* element_ptr) {\n+  \/\/ Below membars keep this access to an unknown flat array correctly\n+  \/\/ ordered with other unknown and known flat array accesses.\n+  insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+\n+  Node* call = nullptr;\n+  {\n+    \/\/ Re-execute flat array load if runtime call triggers deoptimization\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_bci(_bci);\n+    jvms()->set_should_reexecute(true);\n+    inc_sp(2);\n+    kill_dead_locals();\n+    call = make_runtime_call(RC_NO_LEAF | RC_NO_IO,\n+                             OptoRuntime::load_unknown_inline_Type(),\n+                             OptoRuntime::load_unknown_inline_Java(),\n+                             nullptr, TypeRawPtr::BOTTOM,\n+                             array, array_index);\n+  }\n+  make_slow_call_ex(call, env()->Throwable_klass(), false);\n+  Node* buffer = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+\n+  insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+\n+  \/\/ Keep track of the information that the inline type is in flat arrays\n+  const Type* unknown_value = element_ptr->is_instptr()->cast_to_flat_in_array();\n+  return _gvn.transform(new CheckCastPPNode(control(), buffer, unknown_value));\n+}\n@@ -80,2 +189,1 @@\n-  bool big_val = bt == T_DOUBLE || bt == T_LONG;\n-  Node* adr = array_addressing(bt, big_val ? 2 : 1, elemtype);\n+  Node* adr = array_addressing(bt, type2size[bt], elemtype);\n@@ -83,0 +191,1 @@\n+  Node* stored_value_casted = nullptr;\n@@ -84,1 +193,1 @@\n-    array_store_check();\n+    stored_value_casted = array_store_check(adr, elemtype);\n@@ -89,8 +198,6 @@\n-  Node* val;                  \/\/ Oop to store\n-  if (big_val) {\n-    val = pop_pair();\n-  } else {\n-    val = pop();\n-  }\n-  pop();                      \/\/ index (already used)\n-  Node* array = pop();        \/\/ the array itself\n+  Node* const stored_value = pop_node(bt); \/\/ Value to store\n+  Node* const array_index = pop();         \/\/ Index in the array\n+  Node* array = pop();                     \/\/ The array itself\n+\n+  const TypeAryPtr* array_type = _gvn.type(array)->is_aryptr();\n+  const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);\n@@ -100,2 +207,74 @@\n-  }\n-  const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);\n+  } else if (bt == T_OBJECT) {\n+    elemtype = elemtype->make_oopptr();\n+    const Type* stored_value_casted_type = _gvn.type(stored_value_casted);\n+    \/\/ Based on the value to be stored, try to determine if the array is not null-free and\/or not flat.\n+    \/\/ This is only legal for non-null stores because the array_store_check always passes for null, even\n+    \/\/ if the array is null-free. Null stores are handled in GraphKit::inline_array_null_guard().\n+    bool not_inline = !stored_value_casted_type->maybe_null() && !stored_value_casted_type->is_oopptr()->can_be_inline_type();\n+    bool not_null_free = not_inline;\n+    bool not_flat = not_inline || ( stored_value_casted_type->is_inlinetypeptr() &&\n+                                   !stored_value_casted_type->inline_klass()->maybe_flat_in_array());\n+    if (!array_type->is_not_null_free() && not_null_free) {\n+      \/\/ Storing a non-inline type, mark array as not null-free.\n+      array_type = array_type->cast_to_not_null_free();\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, array_type));\n+      replace_in_map(array, cast);\n+      array = cast;\n+    }\n+    if (!array_type->is_not_flat() && not_flat) {\n+      \/\/ Storing to a non-flat array, mark array as not flat.\n+      array_type = array_type->cast_to_not_flat();\n+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, array_type));\n+      replace_in_map(array, cast);\n+      array = cast;\n+    }\n+\n+    if (!array_type->is_flat() && array_type->is_null_free()) {\n+      \/\/ Store to non-flat null-free inline type array (elements can never be null)\n+      assert(!stored_value_casted_type->maybe_null(), \"should be guaranteed by array store check\");\n+      if (elemtype->is_inlinetypeptr() && elemtype->inline_klass()->is_empty()) {\n+        \/\/ Ignore empty inline stores, array is already initialized.\n+        return;\n+      }\n+    } else if (!array_type->is_not_flat()) {\n+      \/\/ Array might be a flat array, emit runtime checks (for nullptr, a simple inline_array_null_guard is sufficient).\n+      assert(UseArrayFlattening && !not_flat && elemtype->is_oopptr()->can_be_inline_type() &&\n+             (!array_type->klass_is_exact() || array_type->is_flat()), \"array can't be a flat array\");\n+      \/\/ TODO 8350865 Depending on the available layouts, we can avoid this check in below flat\/not-flat branches. Also the safe_for_replace arg is now always true.\n+      array = inline_array_null_guard(array, stored_value_casted, 3, true);\n+      IdealKit ideal(this);\n+      ideal.if_then(flat_array_test(array, \/* flat = *\/ false)); {\n+        \/\/ Non-flat array\n+        if (!array_type->is_flat()) {\n+          sync_kit(ideal);\n+          assert(array_type->is_flat() || ideal.ctrl()->in(0)->as_If()->is_flat_array_check(&_gvn), \"Should be found\");\n+          inc_sp(3);\n+          access_store_at(array, adr, adr_type, stored_value_casted, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY, false);\n+          dec_sp(3);\n+          ideal.sync_kit(this);\n+        }\n+      } ideal.else_(); {\n+        \/\/ Flat array\n+        sync_kit(ideal);\n+        if (!array_type->is_not_flat()) {\n+          \/\/ Try to determine the inline klass type of the stored value\n+          ciInlineKlass* vk = nullptr;\n+          if (stored_value_casted_type->is_inlinetypeptr()) {\n+            vk = stored_value_casted_type->inline_klass();\n+          } else if (elemtype->is_inlinetypeptr()) {\n+            vk = elemtype->inline_klass();\n+          }\n+\n+          if (vk != nullptr) {\n+            \/\/ Element type is known, cast and store to flat array layout.\n+            Node* flat_array = cast_to_flat_array(array, vk, false, false, false);\n+\n+            \/\/ Re-execute flat array store if buffering triggers deoptimization\n+            PreserveReexecuteState preexecs(this);\n+            jvms()->set_should_reexecute(true);\n+            inc_sp(3);\n+\n+            if (!stored_value_casted->is_InlineType()) {\n+              assert(_gvn.type(stored_value_casted) == TypePtr::NULL_PTR, \"Unexpected value\");\n+              stored_value_casted = InlineTypeNode::make_null(_gvn, vk);\n+            }\n@@ -103,1 +282,20 @@\n-  access_store_at(array, adr, adr_type, val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY);\n+            stored_value_casted->as_InlineType()->store_flat_array(this, flat_array, array_index);\n+          } else {\n+            \/\/ Element type is unknown, emit a runtime call since the flat array layout is not statically known.\n+            store_to_unknown_flat_array(array, array_index, stored_value_casted);\n+          }\n+        }\n+        ideal.sync_kit(this);\n+      }\n+      ideal.end_if();\n+      sync_kit(ideal);\n+      return;\n+    } else if (!array_type->is_not_null_free()) {\n+      \/\/ Array is not flat but may be null free\n+      assert(elemtype->is_oopptr()->can_be_inline_type(), \"array can't be null-free\");\n+      array = inline_array_null_guard(array, stored_value_casted, 3, true);\n+    }\n+  }\n+  inc_sp(3);\n+  access_store_at(array, adr, adr_type, stored_value, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY);\n+  dec_sp(3);\n@@ -106,0 +304,25 @@\n+\/\/ Emit a runtime call to store to a flat array whose element type is either unknown (i.e. we do not know the flat\n+\/\/ array layout) or not exact (could have different flat array layouts at runtime).\n+void Parse::store_to_unknown_flat_array(Node* array, Node* const idx, Node* non_null_stored_value) {\n+  \/\/ Below membars keep this access to an unknown flat array correctly\n+  \/\/ ordered with other unknown and known flat array accesses.\n+  insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+\n+  Node* call = nullptr;\n+  {\n+    \/\/ Re-execute flat array store if runtime call triggers deoptimization\n+    PreserveReexecuteState preexecs(this);\n+    jvms()->set_bci(_bci);\n+    jvms()->set_should_reexecute(true);\n+    inc_sp(3);\n+    kill_dead_locals();\n+    call = make_runtime_call(RC_NO_LEAF | RC_NO_IO,\n+                      OptoRuntime::store_unknown_inline_Type(),\n+                      OptoRuntime::store_unknown_inline_Java(),\n+                      nullptr, TypeRawPtr::BOTTOM,\n+                      non_null_stored_value, array, idx);\n+  }\n+  make_slow_call_ex(call, env()->Throwable_klass(), false);\n+\n+  insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));\n+}\n@@ -134,11 +357,0 @@\n-  \/\/ Check for big class initializers with all constant offsets\n-  \/\/ feeding into a known-size array.\n-  const TypeInt* idxtype = _gvn.type(idx)->is_int();\n-  \/\/ See if the highest idx value is less than the lowest array bound,\n-  \/\/ and if the idx value cannot be negative:\n-  bool need_range_check = true;\n-  if (idxtype->_hi < sizetype->_lo && idxtype->_lo >= 0) {\n-    need_range_check = false;\n-    if (C->log() != nullptr)   C->log()->elem(\"observe that='!need_range_check'\");\n-  }\n-\n@@ -156,12 +368,1 @@\n-  \/\/ Do the range check\n-  if (need_range_check) {\n-    Node* tst;\n-    if (sizetype->_hi <= 0) {\n-      \/\/ The greatest array bound is negative, so we can conclude that we're\n-      \/\/ compiling unreachable code, but the unsigned compare trick used below\n-      \/\/ only works with non-negative lengths.  Instead, hack \"tst\" to be zero so\n-      \/\/ the uncommon_trap path will always be taken.\n-      tst = _gvn.intcon(0);\n-    } else {\n-      \/\/ Range is constant in array-oop, so we can use the original state of mem\n-      Node* len = load_array_length(ary);\n+  ary = create_speculative_inline_type_array_checks(ary, arytype, elemtype);\n@@ -169,31 +370,4 @@\n-      \/\/ Test length vs index (standard trick using unsigned compare)\n-      Node* chk = _gvn.transform( new CmpUNode(idx, len) );\n-      BoolTest::mask btest = BoolTest::lt;\n-      tst = _gvn.transform( new BoolNode(chk, btest) );\n-    }\n-    RangeCheckNode* rc = new RangeCheckNode(control(), tst, PROB_MAX, COUNT_UNKNOWN);\n-    _gvn.set_type(rc, rc->Value(&_gvn));\n-    if (!tst->is_Con()) {\n-      record_for_igvn(rc);\n-    }\n-    set_control(_gvn.transform(new IfTrueNode(rc)));\n-    \/\/ Branch to failure if out of bounds\n-    {\n-      PreserveJVMState pjvms(this);\n-      set_control(_gvn.transform(new IfFalseNode(rc)));\n-      if (C->allow_range_check_smearing()) {\n-        \/\/ Do not use builtin_throw, since range checks are sometimes\n-        \/\/ made more stringent by an optimistic transformation.\n-        \/\/ This creates \"tentative\" range checks at this point,\n-        \/\/ which are not guaranteed to throw exceptions.\n-        \/\/ See IfNode::Ideal, is_range_check, adjust_check.\n-        uncommon_trap(Deoptimization::Reason_range_check,\n-                      Deoptimization::Action_make_not_entrant,\n-                      nullptr, \"range_check\");\n-      } else {\n-        \/\/ If we have already recompiled with the range-check-widening\n-        \/\/ heroic optimization turned off, then we must really be throwing\n-        \/\/ range check exceptions.\n-        builtin_throw(Deoptimization::Reason_range_check);\n-      }\n-    }\n+  if (needs_range_check(sizetype, idx)) {\n+    create_range_check(idx, ary, sizetype);\n+  } else if (C->log() != nullptr) {\n+    C->log()->elem(\"observe that='!need_range_check'\");\n@@ -201,0 +375,1 @@\n+\n@@ -212,0 +387,199 @@\n+\/\/ Check if we need a range check for an array access. This is the case if the index is either negative or if it could\n+\/\/ be greater or equal the smallest possible array size (i.e. out-of-bounds).\n+bool Parse::needs_range_check(const TypeInt* size_type, const Node* index) const {\n+  const TypeInt* index_type = _gvn.type(index)->is_int();\n+  return index_type->_hi >= size_type->_lo || index_type->_lo < 0;\n+}\n+\n+void Parse::create_range_check(Node* idx, Node* ary, const TypeInt* sizetype) {\n+  Node* tst;\n+  if (sizetype->_hi <= 0) {\n+    \/\/ The greatest array bound is negative, so we can conclude that we're\n+    \/\/ compiling unreachable code, but the unsigned compare trick used below\n+    \/\/ only works with non-negative lengths.  Instead, hack \"tst\" to be zero so\n+    \/\/ the uncommon_trap path will always be taken.\n+    tst = _gvn.intcon(0);\n+  } else {\n+    \/\/ Range is constant in array-oop, so we can use the original state of mem\n+    Node* len = load_array_length(ary);\n+\n+    \/\/ Test length vs index (standard trick using unsigned compare)\n+    Node* chk = _gvn.transform(new CmpUNode(idx, len) );\n+    BoolTest::mask btest = BoolTest::lt;\n+    tst = _gvn.transform(new BoolNode(chk, btest) );\n+  }\n+  RangeCheckNode* rc = new RangeCheckNode(control(), tst, PROB_MAX, COUNT_UNKNOWN);\n+  _gvn.set_type(rc, rc->Value(&_gvn));\n+  if (!tst->is_Con()) {\n+    record_for_igvn(rc);\n+  }\n+  set_control(_gvn.transform(new IfTrueNode(rc)));\n+  \/\/ Branch to failure if out of bounds\n+  {\n+    PreserveJVMState pjvms(this);\n+    set_control(_gvn.transform(new IfFalseNode(rc)));\n+    if (C->allow_range_check_smearing()) {\n+      \/\/ Do not use builtin_throw, since range checks are sometimes\n+      \/\/ made more stringent by an optimistic transformation.\n+      \/\/ This creates \"tentative\" range checks at this point,\n+      \/\/ which are not guaranteed to throw exceptions.\n+      \/\/ See IfNode::Ideal, is_range_check, adjust_check.\n+      uncommon_trap(Deoptimization::Reason_range_check,\n+                    Deoptimization::Action_make_not_entrant,\n+                    nullptr, \"range_check\");\n+    } else {\n+      \/\/ If we have already recompiled with the range-check-widening\n+      \/\/ heroic optimization turned off, then we must really be throwing\n+      \/\/ range check exceptions.\n+      builtin_throw(Deoptimization::Reason_range_check);\n+    }\n+  }\n+}\n+\n+\/\/ For inline type arrays, we can use the profiling information for array accesses to speculate on the type, flatness,\n+\/\/ and null-freeness. We can either prepare the speculative type for later uses or emit explicit speculative checks with\n+\/\/ traps now. In the latter case, the speculative type guarantees can avoid additional runtime checks later (e.g.\n+\/\/ non-null-free implies non-flat which allows us to remove flatness checks). This makes the graph simpler.\n+Node* Parse::create_speculative_inline_type_array_checks(Node* array, const TypeAryPtr* array_type,\n+                                                         const Type*& element_type) {\n+  if (!array_type->is_flat() && !array_type->is_not_flat()) {\n+    \/\/ For arrays that might be flat, speculate that the array has the exact type reported in the profile data such that\n+    \/\/ we can rely on a fixed memory layout (i.e. either a flat layout or not).\n+    array = cast_to_speculative_array_type(array, array_type, element_type);\n+  } else if (UseTypeSpeculation && UseArrayLoadStoreProfile) {\n+    \/\/ Array is known to be either flat or not flat. If possible, update the speculative type by using the profile data\n+    \/\/ at this bci.\n+    array = cast_to_profiled_array_type(array);\n+  }\n+\n+  \/\/ Even though the type does not tell us whether we have an inline type array or not, we can still check the profile data\n+  \/\/ whether we have a non-null-free or non-flat array. Speculating on a non-null-free array doesn't help aaload but could\n+  \/\/ be profitable for a subsequent aastore.\n+  if (!array_type->is_null_free() && !array_type->is_not_null_free()) {\n+    array = speculate_non_null_free_array(array, array_type);\n+  }\n+  if (!array_type->is_flat() && !array_type->is_not_flat()) {\n+    array = speculate_non_flat_array(array, array_type);\n+  }\n+  return array;\n+}\n+\n+\/\/ Speculate that the array has the exact type reported in the profile data. We emit a trap when this turns out to be\n+\/\/ wrong. On the fast path, we add a CheckCastPP to use the exact type.\n+Node* Parse::cast_to_speculative_array_type(Node* const array, const TypeAryPtr*& array_type, const Type*& element_type) {\n+  Deoptimization::DeoptReason reason = Deoptimization::Reason_speculate_class_check;\n+  ciKlass* speculative_array_type = array_type->speculative_type();\n+  if (too_many_traps_or_recompiles(reason) || speculative_array_type == nullptr) {\n+    \/\/ No speculative type, check profile data at this bci\n+    speculative_array_type = nullptr;\n+    reason = Deoptimization::Reason_class_check;\n+    if (UseArrayLoadStoreProfile && !too_many_traps_or_recompiles(reason)) {\n+      ciKlass* profiled_element_type = nullptr;\n+      ProfilePtrKind element_ptr = ProfileMaybeNull;\n+      bool flat_array = true;\n+      bool null_free_array = true;\n+      method()->array_access_profiled_type(bci(), speculative_array_type, profiled_element_type, element_ptr, flat_array,\n+                                           null_free_array);\n+    }\n+  }\n+  if (speculative_array_type != nullptr) {\n+    \/\/ Speculate that this array has the exact type reported by profile data\n+    Node* casted_array = nullptr;\n+    DEBUG_ONLY(Node* old_control = control();)\n+    Node* slow_ctl = type_check_receiver(array, speculative_array_type, 1.0, &casted_array);\n+    if (stopped()) {\n+      \/\/ The check always fails and therefore profile information is incorrect. Don't use it.\n+      assert(old_control == slow_ctl, \"type check should have been removed\");\n+      set_control(slow_ctl);\n+    } else if (!slow_ctl->is_top()) {\n+      { PreserveJVMState pjvms(this);\n+        set_control(slow_ctl);\n+        uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+      }\n+      replace_in_map(array, casted_array);\n+      array_type = _gvn.type(casted_array)->is_aryptr();\n+      element_type = array_type->elem();\n+      return casted_array;\n+    }\n+  }\n+  return array;\n+}\n+\n+\/\/ Create a CheckCastPP when the speculative type can improve the current type.\n+Node* Parse::cast_to_profiled_array_type(Node* const array) {\n+  ciKlass* array_type = nullptr;\n+  ciKlass* element_type = nullptr;\n+  ProfilePtrKind element_ptr = ProfileMaybeNull;\n+  bool flat_array = true;\n+  bool null_free_array = true;\n+  method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);\n+  if (array_type != nullptr) {\n+    return record_profile_for_speculation(array, array_type, ProfileMaybeNull);\n+  }\n+  return array;\n+}\n+\n+\/\/ Speculate that the array is non-null-free. We emit a trap when this turns out to be\n+\/\/ wrong. On the fast path, we add a CheckCastPP to use the non-null-free type.\n+Node* Parse::speculate_non_null_free_array(Node* const array, const TypeAryPtr*& array_type) {\n+  bool null_free_array = true;\n+  Deoptimization::DeoptReason reason = Deoptimization::Reason_none;\n+  if (array_type->speculative() != nullptr &&\n+      array_type->speculative()->is_aryptr()->is_not_null_free() &&\n+      !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+    null_free_array = false;\n+    reason = Deoptimization::Reason_speculate_class_check;\n+  } else if (UseArrayLoadStoreProfile && !too_many_traps_or_recompiles(Deoptimization::Reason_class_check)) {\n+    ciKlass* profiled_array_type = nullptr;\n+    ciKlass* profiled_element_type = nullptr;\n+    ProfilePtrKind element_ptr = ProfileMaybeNull;\n+    bool flat_array = true;\n+    method()->array_access_profiled_type(bci(), profiled_array_type, profiled_element_type, element_ptr, flat_array,\n+                                         null_free_array);\n+    reason = Deoptimization::Reason_class_check;\n+  }\n+  if (!null_free_array) {\n+    { \/\/ Deoptimize if null-free array\n+      BuildCutout unless(this, null_free_array_test(array, \/* null_free = *\/ false), PROB_MAX);\n+      uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+    }\n+    assert(!stopped(), \"null-free array should have been caught earlier\");\n+    Node* casted_array = _gvn.transform(new CheckCastPPNode(control(), array, array_type->cast_to_not_null_free()));\n+    replace_in_map(array, casted_array);\n+    array_type = _gvn.type(casted_array)->is_aryptr();\n+    return casted_array;\n+  }\n+  return array;\n+}\n+\n+\/\/ Speculate that the array is non-flat. We emit a trap when this turns out to be wrong.\n+\/\/ On the fast path, we add a CheckCastPP to use the non-flat type.\n+Node* Parse::speculate_non_flat_array(Node* const array, const TypeAryPtr* const array_type) {\n+  bool flat_array = true;\n+  Deoptimization::DeoptReason reason = Deoptimization::Reason_none;\n+  if (array_type->speculative() != nullptr &&\n+      array_type->speculative()->is_aryptr()->is_not_flat() &&\n+      !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+    flat_array = false;\n+    reason = Deoptimization::Reason_speculate_class_check;\n+  } else if (UseArrayLoadStoreProfile && !too_many_traps_or_recompiles(reason)) {\n+    ciKlass* profiled_array_type = nullptr;\n+    ciKlass* profiled_element_type = nullptr;\n+    ProfilePtrKind element_ptr = ProfileMaybeNull;\n+    bool null_free_array = true;\n+    method()->array_access_profiled_type(bci(), profiled_array_type, profiled_element_type, element_ptr, flat_array,\n+                                         null_free_array);\n+    reason = Deoptimization::Reason_class_check;\n+  }\n+  if (!flat_array) {\n+    { \/\/ Deoptimize if flat array\n+      BuildCutout unless(this, flat_array_test(array, \/* flat = *\/ false), PROB_MAX);\n+      uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+    }\n+    assert(!stopped(), \"flat array should have been caught earlier\");\n+    Node* casted_array = _gvn.transform(new CheckCastPPNode(control(), array, array_type->cast_to_not_flat()));\n+    replace_in_map(array, casted_array);\n+    return casted_array;\n+  }\n+  return array;\n+}\n@@ -1448,1 +1822,1 @@\n-void Parse::do_if(BoolTest::mask btest, Node* c) {\n+void Parse::do_if(BoolTest::mask btest, Node* c, bool can_trap, bool new_path, Node** ctrl_taken) {\n@@ -1539,2 +1913,2 @@\n-      if (C->eliminate_boxing()) {\n-        \/\/ Mark the successor block as parsed\n+      if (C->eliminate_boxing() && !new_path) {\n+        \/\/ Mark the successor block as parsed (if we haven't created a new path)\n@@ -1544,1 +1918,1 @@\n-      adjust_map_after_if(taken_btest, c, prob, branch_block);\n+      adjust_map_after_if(taken_btest, c, prob, branch_block, can_trap);\n@@ -1546,1 +1920,9 @@\n-        merge(target_bci);\n+        if (new_path) {\n+          \/\/ Merge by using a new path\n+          merge_new_path(target_bci);\n+        } else if (ctrl_taken != nullptr) {\n+          \/\/ Don't merge but save taken branch to be wired by caller\n+          *ctrl_taken = control();\n+        } else {\n+          merge(target_bci);\n+        }\n@@ -1555,1 +1937,1 @@\n-  if (stopped()) {\n+  if (stopped() && ctrl_taken == nullptr) {\n@@ -1557,1 +1939,1 @@\n-      \/\/ Mark the successor block as parsed\n+      \/\/ Mark the successor block as parsed (if caller does not re-wire control flow)\n@@ -1561,1 +1943,1 @@\n-    adjust_map_after_if(untaken_btest, c, untaken_prob, next_block);\n+    adjust_map_after_if(untaken_btest, c, untaken_prob, next_block, can_trap);\n@@ -1569,0 +1951,405 @@\n+\n+static ProfilePtrKind speculative_ptr_kind(const TypeOopPtr* t) {\n+  if (t->speculative() == nullptr) {\n+    return ProfileUnknownNull;\n+  }\n+  if (t->speculative_always_null()) {\n+    return ProfileAlwaysNull;\n+  }\n+  if (t->speculative_maybe_null()) {\n+    return ProfileMaybeNull;\n+  }\n+  return ProfileNeverNull;\n+}\n+\n+void Parse::acmp_always_null_input(Node* input, const TypeOopPtr* tinput, BoolTest::mask btest, Node* eq_region) {\n+  inc_sp(2);\n+  Node* cast = null_check_common(input, T_OBJECT, true, nullptr,\n+                                 !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check) &&\n+                                 speculative_ptr_kind(tinput) == ProfileAlwaysNull);\n+  dec_sp(2);\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      replace_in_map(input, cast);\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+  } else {\n+    replace_in_map(input, cast);\n+  }\n+}\n+\n+Node* Parse::acmp_null_check(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, Node*& null_ctl) {\n+  inc_sp(2);\n+  null_ctl = top();\n+  Node* cast = null_check_oop(input, &null_ctl,\n+                              input_ptr == ProfileNeverNull || (input_ptr == ProfileUnknownNull && !too_many_traps_or_recompiles(Deoptimization::Reason_null_check)),\n+                              false,\n+                              speculative_ptr_kind(tinput) == ProfileNeverNull &&\n+                              !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check));\n+  dec_sp(2);\n+  assert(!stopped(), \"null input should have been caught earlier\");\n+  return cast;\n+}\n+\n+void Parse::acmp_known_non_inline_type_input(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, ciKlass* input_type, BoolTest::mask btest, Node* eq_region) {\n+  Node* ne_region = new RegionNode(1);\n+  Node* null_ctl;\n+  Node* cast = acmp_null_check(input, tinput, input_ptr, null_ctl);\n+  ne_region->add_req(null_ctl);\n+\n+  Node* slow_ctl = type_check_receiver(cast, input_type, 1.0, &cast);\n+  {\n+    PreserveJVMState pjvms(this);\n+    inc_sp(2);\n+    set_control(slow_ctl);\n+    Deoptimization::DeoptReason reason;\n+    if (tinput->speculative_type() != nullptr && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+      reason = Deoptimization::Reason_speculate_class_check;\n+    } else {\n+      reason = Deoptimization::Reason_class_check;\n+    }\n+    uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);\n+  }\n+  ne_region->add_req(control());\n+\n+  record_for_igvn(ne_region);\n+  set_control(_gvn.transform(ne_region));\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      if (null_ctl == top()) {\n+        replace_in_map(input, cast);\n+      }\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+  } else {\n+    if (null_ctl == top()) {\n+      replace_in_map(input, cast);\n+    }\n+    set_control(_gvn.transform(ne_region));\n+  }\n+}\n+\n+void Parse::acmp_unknown_non_inline_type_input(Node* input, const TypeOopPtr* tinput, ProfilePtrKind input_ptr, BoolTest::mask btest, Node* eq_region) {\n+  Node* ne_region = new RegionNode(1);\n+  Node* null_ctl;\n+  Node* cast = acmp_null_check(input, tinput, input_ptr, null_ctl);\n+  ne_region->add_req(null_ctl);\n+\n+  {\n+    BuildCutout unless(this, inline_type_test(cast, \/* is_inline = *\/ false), PROB_MAX);\n+    inc_sp(2);\n+    uncommon_trap_exact(Deoptimization::Reason_class_check, Deoptimization::Action_maybe_recompile);\n+  }\n+\n+  ne_region->add_req(control());\n+\n+  record_for_igvn(ne_region);\n+  set_control(_gvn.transform(ne_region));\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      if (null_ctl == top()) {\n+        replace_in_map(input, cast);\n+      }\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+  } else {\n+    if (null_ctl == top()) {\n+      replace_in_map(input, cast);\n+    }\n+    set_control(_gvn.transform(ne_region));\n+  }\n+}\n+\n+void Parse::do_acmp(BoolTest::mask btest, Node* left, Node* right) {\n+  ciKlass* left_type = nullptr;\n+  ciKlass* right_type = nullptr;\n+  ProfilePtrKind left_ptr = ProfileUnknownNull;\n+  ProfilePtrKind right_ptr = ProfileUnknownNull;\n+  bool left_inline_type = true;\n+  bool right_inline_type = true;\n+\n+  \/\/ Leverage profiling at acmp\n+  if (UseACmpProfile) {\n+    method()->acmp_profiled_type(bci(), left_type, right_type, left_ptr, right_ptr, left_inline_type, right_inline_type);\n+    if (too_many_traps_or_recompiles(Deoptimization::Reason_class_check)) {\n+      left_type = nullptr;\n+      right_type = nullptr;\n+      left_inline_type = true;\n+      right_inline_type = true;\n+    }\n+    if (too_many_traps_or_recompiles(Deoptimization::Reason_null_check)) {\n+      left_ptr = ProfileUnknownNull;\n+      right_ptr = ProfileUnknownNull;\n+    }\n+  }\n+\n+  if (UseTypeSpeculation) {\n+    record_profile_for_speculation(left, left_type, left_ptr);\n+    record_profile_for_speculation(right, right_type, right_ptr);\n+  }\n+\n+  if (!EnableValhalla) {\n+    Node* cmp = CmpP(left, right);\n+    cmp = optimize_cmp_with_klass(cmp);\n+    do_if(btest, cmp);\n+    return;\n+  }\n+\n+  \/\/ Check for equality before potentially allocating\n+  if (left == right) {\n+    do_if(btest, makecon(TypeInt::CC_EQ));\n+    return;\n+  }\n+\n+  \/\/ Allocate inline type operands and re-execute on deoptimization\n+  if (left->is_InlineType()) {\n+    if (_gvn.type(right)->is_zero_type() ||\n+        (right->is_InlineType() && _gvn.type(right->as_InlineType()->get_null_marker())->is_zero_type())) {\n+      \/\/ Null checking a scalarized but nullable inline type. Check the null marker\n+      \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+      Node* cmp = CmpI(left->as_InlineType()->get_null_marker(), intcon(0));\n+      do_if(btest, cmp);\n+      return;\n+    } else {\n+      PreserveReexecuteState preexecs(this);\n+      inc_sp(2);\n+      jvms()->set_should_reexecute(true);\n+      left = left->as_InlineType()->buffer(this)->get_oop();\n+    }\n+  }\n+  if (right->is_InlineType()) {\n+    PreserveReexecuteState preexecs(this);\n+    inc_sp(2);\n+    jvms()->set_should_reexecute(true);\n+    right = right->as_InlineType()->buffer(this)->get_oop();\n+  }\n+\n+  \/\/ First, do a normal pointer comparison\n+  const TypeOopPtr* tleft = _gvn.type(left)->isa_oopptr();\n+  const TypeOopPtr* tright = _gvn.type(right)->isa_oopptr();\n+  Node* cmp = CmpP(left, right);\n+  cmp = optimize_cmp_with_klass(cmp);\n+  if (tleft == nullptr || !tleft->can_be_inline_type() ||\n+      tright == nullptr || !tright->can_be_inline_type()) {\n+    \/\/ This is sufficient, if one of the operands can't be an inline type\n+    do_if(btest, cmp);\n+    return;\n+  }\n+\n+  \/\/ Don't add traps to unstable if branches because additional checks are required to\n+  \/\/ decide if the operands are equal\/substitutable and we therefore shouldn't prune\n+  \/\/ branches for one if based on the profiling of the acmp branches.\n+  \/\/ Also, OptimizeUnstableIf would set an incorrect re-rexecution state because it\n+  \/\/ assumes that there is a 1-1 mapping between the if and the acmp branches and that\n+  \/\/ hitting a trap means that we will take the corresponding acmp branch on re-execution.\n+  const bool can_trap = true;\n+\n+  Node* eq_region = nullptr;\n+  if (btest == BoolTest::eq) {\n+    do_if(btest, cmp, !can_trap, true);\n+    if (stopped()) {\n+      \/\/ Pointers are equal, operands must be equal\n+      return;\n+    }\n+  } else {\n+    assert(btest == BoolTest::ne, \"only eq or ne\");\n+    Node* is_not_equal = nullptr;\n+    eq_region = new RegionNode(3);\n+    {\n+      PreserveJVMState pjvms(this);\n+      \/\/ Pointers are not equal, but more checks are needed to determine if the operands are (not) substitutable\n+      do_if(btest, cmp, !can_trap, false, &is_not_equal);\n+      if (!stopped()) {\n+        eq_region->init_req(1, control());\n+      }\n+    }\n+    if (is_not_equal == nullptr || is_not_equal->is_top()) {\n+      record_for_igvn(eq_region);\n+      set_control(_gvn.transform(eq_region));\n+      return;\n+    }\n+    set_control(is_not_equal);\n+  }\n+\n+  \/\/ Prefer speculative types if available\n+  if (!too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {\n+    if (tleft->speculative_type() != nullptr) {\n+      left_type = tleft->speculative_type();\n+    }\n+    if (tright->speculative_type() != nullptr) {\n+      right_type = tright->speculative_type();\n+    }\n+  }\n+\n+  if (speculative_ptr_kind(tleft) != ProfileMaybeNull && speculative_ptr_kind(tleft) != ProfileUnknownNull) {\n+    ProfilePtrKind speculative_left_ptr = speculative_ptr_kind(tleft);\n+    if (speculative_left_ptr == ProfileAlwaysNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_assert)) {\n+      left_ptr = speculative_left_ptr;\n+    } else if (speculative_left_ptr == ProfileNeverNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check)) {\n+      left_ptr = speculative_left_ptr;\n+    }\n+  }\n+  if (speculative_ptr_kind(tright) != ProfileMaybeNull && speculative_ptr_kind(tright) != ProfileUnknownNull) {\n+    ProfilePtrKind speculative_right_ptr = speculative_ptr_kind(tright);\n+    if (speculative_right_ptr == ProfileAlwaysNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_assert)) {\n+      right_ptr = speculative_right_ptr;\n+    } else if (speculative_right_ptr == ProfileNeverNull && !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_null_check)) {\n+      right_ptr = speculative_right_ptr;\n+    }\n+  }\n+\n+  if (left_ptr == ProfileAlwaysNull) {\n+    \/\/ Comparison with null. Assert the input is indeed null and we're done.\n+    acmp_always_null_input(left, tleft, btest, eq_region);\n+    return;\n+  }\n+  if (right_ptr == ProfileAlwaysNull) {\n+    \/\/ Comparison with null. Assert the input is indeed null and we're done.\n+    acmp_always_null_input(right, tright, btest, eq_region);\n+    return;\n+  }\n+  if (left_type != nullptr && !left_type->is_inlinetype()) {\n+    \/\/ Comparison with an object of known type\n+    acmp_known_non_inline_type_input(left, tleft, left_ptr, left_type, btest, eq_region);\n+    return;\n+  }\n+  if (right_type != nullptr && !right_type->is_inlinetype()) {\n+    \/\/ Comparison with an object of known type\n+    acmp_known_non_inline_type_input(right, tright, right_ptr, right_type, btest, eq_region);\n+    return;\n+  }\n+  if (!left_inline_type) {\n+    \/\/ Comparison with an object known not to be an inline type\n+    acmp_unknown_non_inline_type_input(left, tleft, left_ptr, btest, eq_region);\n+    return;\n+  }\n+  if (!right_inline_type) {\n+    \/\/ Comparison with an object known not to be an inline type\n+    acmp_unknown_non_inline_type_input(right, tright, right_ptr, btest, eq_region);\n+    return;\n+  }\n+\n+  \/\/ Pointers are not equal, check if first operand is non-null\n+  Node* ne_region = new RegionNode(6);\n+  Node* null_ctl;\n+  Node* not_null_right = acmp_null_check(right, tright, right_ptr, null_ctl);\n+  ne_region->init_req(1, null_ctl);\n+\n+  \/\/ First operand is non-null, check if it is an inline type\n+  Node* is_value = inline_type_test(not_null_right);\n+  IfNode* is_value_iff = create_and_map_if(control(), is_value, PROB_FAIR, COUNT_UNKNOWN);\n+  Node* not_value = _gvn.transform(new IfFalseNode(is_value_iff));\n+  ne_region->init_req(2, not_value);\n+  set_control(_gvn.transform(new IfTrueNode(is_value_iff)));\n+\n+  \/\/ The first operand is an inline type, check if the second operand is non-null\n+  Node* not_null_left = acmp_null_check(left, tleft, left_ptr, null_ctl);\n+  ne_region->init_req(3, null_ctl);\n+\n+  \/\/ Check if both operands are of the same class.\n+  Node* kls_left = load_object_klass(not_null_left);\n+  Node* kls_right = load_object_klass(not_null_right);\n+  Node* kls_cmp = CmpP(kls_left, kls_right);\n+  Node* kls_bol = _gvn.transform(new BoolNode(kls_cmp, BoolTest::ne));\n+  IfNode* kls_iff = create_and_map_if(control(), kls_bol, PROB_FAIR, COUNT_UNKNOWN);\n+  Node* kls_ne = _gvn.transform(new IfTrueNode(kls_iff));\n+  set_control(_gvn.transform(new IfFalseNode(kls_iff)));\n+  ne_region->init_req(4, kls_ne);\n+\n+  if (stopped()) {\n+    record_for_igvn(ne_region);\n+    set_control(_gvn.transform(ne_region));\n+    if (btest == BoolTest::ne) {\n+      {\n+        PreserveJVMState pjvms(this);\n+        int target_bci = iter().get_dest();\n+        merge(target_bci);\n+      }\n+      record_for_igvn(eq_region);\n+      set_control(_gvn.transform(eq_region));\n+    }\n+    return;\n+  }\n+\n+  \/\/ Both operands are values types of the same class, we need to perform a\n+  \/\/ substitutability test. Delegate to ValueObjectMethods::isSubstitutable().\n+  Node* ne_io_phi = PhiNode::make(ne_region, i_o());\n+  Node* mem = reset_memory();\n+  Node* ne_mem_phi = PhiNode::make(ne_region, mem);\n+\n+  Node* eq_io_phi = nullptr;\n+  Node* eq_mem_phi = nullptr;\n+  if (eq_region != nullptr) {\n+    eq_io_phi = PhiNode::make(eq_region, i_o());\n+    eq_mem_phi = PhiNode::make(eq_region, mem);\n+  }\n+\n+  set_all_memory(mem);\n+\n+  kill_dead_locals();\n+  ciSymbol* subst_method_name = UseAltSubstitutabilityMethod ? ciSymbols::isSubstitutableAlt_name() : ciSymbols::isSubstitutable_name();\n+  ciMethod* subst_method = ciEnv::current()->ValueObjectMethods_klass()->find_method(subst_method_name, ciSymbols::object_object_boolean_signature());\n+  CallStaticJavaNode *call = new CallStaticJavaNode(C, TypeFunc::make(subst_method), SharedRuntime::get_resolve_static_call_stub(), subst_method);\n+  call->set_override_symbolic_info(true);\n+  call->init_req(TypeFunc::Parms, not_null_left);\n+  call->init_req(TypeFunc::Parms+1, not_null_right);\n+  inc_sp(2);\n+  set_edges_for_java_call(call, false, false);\n+  Node* ret = set_results_for_java_call(call, false, true);\n+  dec_sp(2);\n+\n+  \/\/ Test the return value of ValueObjectMethods::isSubstitutable()\n+  \/\/ This is the last check, do_if can emit traps now.\n+  Node* subst_cmp = _gvn.transform(new CmpINode(ret, intcon(1)));\n+  Node* ctl = C->top();\n+  if (btest == BoolTest::eq) {\n+    PreserveJVMState pjvms(this);\n+    do_if(btest, subst_cmp, can_trap);\n+    if (!stopped()) {\n+      ctl = control();\n+    }\n+  } else {\n+    assert(btest == BoolTest::ne, \"only eq or ne\");\n+    PreserveJVMState pjvms(this);\n+    do_if(btest, subst_cmp, can_trap, false, &ctl);\n+    if (!stopped()) {\n+      eq_region->init_req(2, control());\n+      eq_io_phi->init_req(2, i_o());\n+      eq_mem_phi->init_req(2, reset_memory());\n+    }\n+  }\n+  ne_region->init_req(5, ctl);\n+  ne_io_phi->init_req(5, i_o());\n+  ne_mem_phi->init_req(5, reset_memory());\n+\n+  record_for_igvn(ne_region);\n+  set_control(_gvn.transform(ne_region));\n+  set_i_o(_gvn.transform(ne_io_phi));\n+  set_all_memory(_gvn.transform(ne_mem_phi));\n+\n+  if (btest == BoolTest::ne) {\n+    {\n+      PreserveJVMState pjvms(this);\n+      int target_bci = iter().get_dest();\n+      merge(target_bci);\n+    }\n+\n+    record_for_igvn(eq_region);\n+    set_control(_gvn.transform(eq_region));\n+    set_i_o(_gvn.transform(eq_io_phi));\n+    set_all_memory(_gvn.transform(eq_mem_phi));\n+  }\n+}\n+\n@@ -1641,1 +2428,1 @@\n-void Parse::adjust_map_after_if(BoolTest::mask btest, Node* c, float prob, Block* path) {\n+void Parse::adjust_map_after_if(BoolTest::mask btest, Node* c, float prob, Block* path, bool can_trap) {\n@@ -1653,1 +2440,1 @@\n-  if (path_is_suitable_for_uncommon_trap(prob)) {\n+  if (can_trap && path_is_suitable_for_uncommon_trap(prob)) {\n@@ -1750,0 +2537,3 @@\n+            if (tboth->is_inlinetypeptr()) {\n+              ccast = InlineTypeNode::make_from_oop(this, ccast, tboth->exact_klass(true)->as_inline_klass());\n+            }\n@@ -1854,0 +2644,4 @@\n+        if (obj->is_InlineType()) {\n+          assert(obj->as_InlineType()->is_allocated(&_gvn), \"must be allocated\");\n+          obj = obj->as_InlineType()->get_oop();\n+        }\n@@ -2593,1 +3387,1 @@\n-    return_current(pop());\n+    return_current(cast_to_non_larval(pop()));\n@@ -2596,2 +3390,0 @@\n-    return_current(pop_pair());\n-    break;\n@@ -2650,15 +3442,21 @@\n-    b = pop();\n-    if (!_gvn.type(b)->speculative_maybe_null() &&\n-        !too_many_traps(Deoptimization::Reason_speculate_null_check)) {\n-      inc_sp(1);\n-      Node* null_ctl = top();\n-      b = null_check_oop(b, &null_ctl, true, true, true);\n-      assert(null_ctl->is_top(), \"no null control here\");\n-      dec_sp(1);\n-    } else if (_gvn.type(b)->speculative_always_null() &&\n-               !too_many_traps(Deoptimization::Reason_speculate_null_assert)) {\n-      inc_sp(1);\n-      b = null_assert(b);\n-      dec_sp(1);\n-    }\n-    c = _gvn.transform( new CmpPNode(b, a) );\n+    b = cast_to_non_larval(pop());\n+    if (b->is_InlineType()) {\n+      \/\/ Null checking a scalarized but nullable inline type. Check the null marker\n+      \/\/ input instead of the oop input to avoid keeping buffer allocations alive\n+      c = _gvn.transform(new CmpINode(b->as_InlineType()->get_null_marker(), zerocon(T_INT)));\n+    } else {\n+      if (!_gvn.type(b)->speculative_maybe_null() &&\n+          !too_many_traps(Deoptimization::Reason_speculate_null_check)) {\n+        inc_sp(1);\n+        Node* null_ctl = top();\n+        b = null_check_oop(b, &null_ctl, true, true, true);\n+        assert(null_ctl->is_top(), \"no null control here\");\n+        dec_sp(1);\n+      } else if (_gvn.type(b)->speculative_always_null() &&\n+                 !too_many_traps(Deoptimization::Reason_speculate_null_assert)) {\n+        inc_sp(1);\n+        b = null_assert(b);\n+        dec_sp(1);\n+      }\n+      c = _gvn.transform( new CmpPNode(b, a) );\n+    }\n@@ -2673,5 +3471,3 @@\n-    a = pop();\n-    b = pop();\n-    c = _gvn.transform( new CmpPNode(b, a) );\n-    c = optimize_cmp_with_klass(c);\n-    do_if(btest, c);\n+    a = cast_to_non_larval(pop());\n+    b = cast_to_non_larval(pop());\n+    do_acmp(btest, b, a);\n@@ -2732,1 +3528,1 @@\n-    do_anewarray();\n+    do_newarray();\n","filename":"src\/hotspot\/share\/opto\/parse2.cpp","additions":906,"deletions":110,"binary":false,"changes":1016,"status":"modified"},{"patch":"@@ -47,0 +47,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n@@ -191,0 +193,1 @@\n+const TypeFunc* OptoRuntime::_new_array_nozero_Type               = nullptr;\n@@ -321,1 +324,1 @@\n-JRT_BLOCK_ENTRY(void, OptoRuntime::new_instance_C(Klass* klass, JavaThread* current))\n+JRT_BLOCK_ENTRY(void, OptoRuntime::new_instance_C(Klass* klass, bool is_larval, JavaThread* current))\n@@ -341,1 +344,5 @@\n-    oop result = InstanceKlass::cast(klass)->allocate_instance(THREAD);\n+    instanceOop result = InstanceKlass::cast(klass)->allocate_instance(THREAD);\n+    if (is_larval) {\n+      \/\/ Check if this is a larval buffer allocation\n+      result->set_mark(result->mark().enter_larval_state());\n+    }\n@@ -359,1 +366,1 @@\n-JRT_BLOCK_ENTRY(void, OptoRuntime::new_array_C(Klass* array_type, int len, JavaThread* current))\n+JRT_BLOCK_ENTRY(void, OptoRuntime::new_array_C(Klass* array_type, int len, oopDesc* init_val, JavaThread* current))\n@@ -368,0 +375,1 @@\n+  Handle h_init_val(current, init_val); \/\/ keep the init_val object alive\n@@ -369,1 +377,26 @@\n-  if (array_type->is_typeArray_klass()) {\n+  if (array_type->is_flatArray_klass()) {\n+    Handle holder(current, array_type->klass_holder()); \/\/ keep the array klass alive\n+    FlatArrayKlass* fak = FlatArrayKlass::cast(array_type);\n+    InlineKlass* vk = fak->element_klass();\n+    ArrayKlass::ArrayProperties props = ArrayKlass::ArrayProperties::DEFAULT;\n+    switch(fak->layout_kind()) {\n+      case LayoutKind::ATOMIC_FLAT:\n+        props = ArrayKlass::ArrayProperties::NULL_RESTRICTED;\n+      break;\n+      case LayoutKind::NON_ATOMIC_FLAT:\n+        props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::NULL_RESTRICTED | ArrayKlass::ArrayProperties::NON_ATOMIC);\n+      break;\n+      case LayoutKind::NULLABLE_ATOMIC_FLAT:\n+      props = ArrayKlass::ArrayProperties::NON_ATOMIC;\n+      break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    result = oopFactory::new_flatArray(vk, len, props, fak->layout_kind(), THREAD);\n+    if (array_type->is_null_free_array_klass() && !h_init_val.is_null()) {\n+      \/\/ Null-free arrays need to be initialized\n+      for (int i = 0; i < len; i++) {\n+        vk->write_value_to_addr(h_init_val(), ((flatArrayOop)result)->value_at_addr(i, fak->layout_helper()), fak->layout_kind(), true, CHECK);\n+      }\n+    }\n+  } else if (array_type->is_typeArray_klass()) {\n@@ -375,5 +408,7 @@\n-    \/\/ Although the oopFactory likes to work with the elem_type,\n-    \/\/ the compiler prefers the array_type, since it must already have\n-    \/\/ that latter value in hand for the fast path.\n-    Klass* elem_type = ObjArrayKlass::cast(array_type)->element_klass();\n-    result = oopFactory::new_objArray(elem_type, len, THREAD);\n+    result = oopFactory::new_refArray(array_type, len, THREAD);\n+    if (array_type->is_null_free_array_klass() && !h_init_val.is_null()) {\n+      \/\/ Null-free arrays need to be initialized\n+      for (int i = 0; i < len; i++) {\n+        ((objArrayOop)result)->obj_at_put(i, h_init_val());\n+      }\n+    }\n@@ -577,1 +612,1 @@\n-  const Type **fields = TypeTuple::fields(1);\n+  const Type **fields = TypeTuple::fields(2);\n@@ -579,1 +614,2 @@\n-  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+1, fields);\n+  fields[TypeFunc::Parms+1] = TypeInt::BOOL;        \/\/ is_larval\n+  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+2, fields);\n@@ -622,0 +658,17 @@\n+  \/\/ create input type (domain)\n+  const Type **fields = TypeTuple::fields(3);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL;   \/\/ element klass\n+  fields[TypeFunc::Parms+1] = TypeInt::INT;       \/\/ array size\n+  fields[TypeFunc::Parms+2] = TypeInstPtr::NOTNULL;       \/\/ init value\n+  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+3, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = TypeRawPtr::NOTNULL; \/\/ Returned oop\n+\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1, fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+static const TypeFunc* make_new_array_nozero_Type() {\n@@ -697,1 +750,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -2131,1 +2184,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -2163,1 +2216,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -2179,1 +2232,1 @@\n-  return TypeFunc::make(domain,range);\n+  return TypeFunc::make(domain, range);\n@@ -2270,0 +2323,1 @@\n+  _new_array_nozero_Type              = make_new_array_nozero_Type();\n@@ -2370,0 +2424,105 @@\n+\n+const TypeFunc *OptoRuntime::store_inline_type_fields_Type() {\n+  \/\/ create input type (domain)\n+  uint total = SharedRuntime::java_return_convention_max_int + SharedRuntime::java_return_convention_max_float*2;\n+  const Type **fields = TypeTuple::fields(total);\n+  \/\/ We don't know the number of returned values and their\n+  \/\/ types. Assume all registers available to the return convention\n+  \/\/ are used.\n+  fields[TypeFunc::Parms] = TypePtr::BOTTOM;\n+  uint i = 1;\n+  for (; i < SharedRuntime::java_return_convention_max_int; i++) {\n+    fields[TypeFunc::Parms+i] = TypeInt::INT;\n+  }\n+  for (; i < total; i+=2) {\n+    fields[TypeFunc::Parms+i] = Type::DOUBLE;\n+    fields[TypeFunc::Parms+i+1] = Type::HALF;\n+  }\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + total, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::BOTTOM;\n+\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1,fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+const TypeFunc *OptoRuntime::pack_inline_type_Type() {\n+  \/\/ create input type (domain)\n+  uint total = 1 + SharedRuntime::java_return_convention_max_int + SharedRuntime::java_return_convention_max_float*2;\n+  const Type **fields = TypeTuple::fields(total);\n+  \/\/ We don't know the number of returned values and their\n+  \/\/ types. Assume all registers available to the return convention\n+  \/\/ are used.\n+  fields[TypeFunc::Parms] = TypeRawPtr::BOTTOM;\n+  fields[TypeFunc::Parms+1] = TypeRawPtr::BOTTOM;\n+  uint i = 2;\n+  for (; i < SharedRuntime::java_return_convention_max_int+1; i++) {\n+    fields[TypeFunc::Parms+i] = TypeInt::INT;\n+  }\n+  for (; i < total; i+=2) {\n+    fields[TypeFunc::Parms+i] = Type::DOUBLE;\n+    fields[TypeFunc::Parms+i+1] = Type::HALF;\n+  }\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + total, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL;\n+\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1,fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+JRT_BLOCK_ENTRY(void, OptoRuntime::load_unknown_inline_C(flatArrayOopDesc* array, int index, JavaThread* current))\n+  JRT_BLOCK;\n+  oop buffer = array->obj_at(index, THREAD);\n+  deoptimize_caller_frame(current, HAS_PENDING_EXCEPTION);\n+  current->set_vm_result_oop(buffer);\n+  JRT_BLOCK_END;\n+JRT_END\n+\n+const TypeFunc* OptoRuntime::load_unknown_inline_Type() {\n+  \/\/ create input type (domain)\n+  const Type** fields = TypeTuple::fields(2);\n+  fields[TypeFunc::Parms] = TypeOopPtr::NOTNULL;\n+  fields[TypeFunc::Parms+1] = TypeInt::POS;\n+\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms+2, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms] = TypeInstPtr::BOTTOM;\n+\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms+1, fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+JRT_BLOCK_ENTRY(void, OptoRuntime::store_unknown_inline_C(instanceOopDesc* buffer, flatArrayOopDesc* array, int index, JavaThread* current))\n+  JRT_BLOCK;\n+  array->obj_at_put(index, buffer, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+      fatal(\"This entry must be changed to be a non-leaf entry because writing to a flat array can now throw an exception\");\n+  }\n+  JRT_BLOCK_END;\n+JRT_END\n+\n+const TypeFunc* OptoRuntime::store_unknown_inline_Type() {\n+  \/\/ create input type (domain)\n+  const Type** fields = TypeTuple::fields(3);\n+  fields[TypeFunc::Parms] = TypeInstPtr::NOTNULL;\n+  fields[TypeFunc::Parms+1] = TypeOopPtr::NOTNULL;\n+  fields[TypeFunc::Parms+2] = TypeInt::POS;\n+\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms+3, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(0);\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms, fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":174,"deletions":15,"binary":false,"changes":189,"status":"modified"},{"patch":"@@ -131,0 +131,1 @@\n+  static const TypeFunc* _new_array_nozero_Type;\n@@ -217,1 +218,1 @@\n-  static void new_instance_C(Klass* instance_klass, JavaThread* current);\n+  static void new_instance_C(Klass* instance_klass, bool is_larval, JavaThread* current);\n@@ -220,1 +221,1 @@\n-  static void new_array_C(Klass* array_klass, int len, JavaThread* current);\n+  static void new_array_C(Klass* array_klass, int len, oopDesc* init_val, JavaThread* current);\n@@ -264,0 +265,2 @@\n+  static void load_unknown_inline_C(flatArrayOopDesc* array, int index, JavaThread* current);\n+  static void store_unknown_inline_C(instanceOopDesc* buffer, flatArrayOopDesc* array, int index, JavaThread* current);\n@@ -296,0 +299,2 @@\n+  static address load_unknown_inline_Java()              { return _load_unknown_inline_Java; }\n+  static address store_unknown_inline_Java()             { return _store_unknown_inline_Java; }\n@@ -328,1 +333,2 @@\n-    return new_array_Type();\n+    assert(_new_array_nozero_Type != nullptr, \"should be initialized\");\n+    return _new_array_nozero_Type;\n@@ -728,0 +734,6 @@\n+  static const TypeFunc* load_unknown_inline_Type();\n+  static const TypeFunc* store_unknown_inline_Type();\n+\n+  static const TypeFunc* store_inline_type_fields_Type();\n+  static const TypeFunc* pack_inline_type_Type();\n+\n","filename":"src\/hotspot\/share\/opto\/runtime.hpp","additions":15,"deletions":3,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -615,2 +616,1 @@\n-        }\n-        else if (m != iff && split_up(m, region, iff)) {\n+        } else if (m != iff && split_up(m, region, iff)) {\n","filename":"src\/hotspot\/share\/opto\/split_if.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -220,1 +220,2 @@\n-  virtual Node *Ideal(PhaseGVN *phase, bool can_reshape);\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+  virtual const Type* Value(PhaseGVN* phase) const;\n@@ -222,0 +223,1 @@\n+  bool is_double_null_check(PhaseGVN* phase, Node*& a, Node*& b) const;\n@@ -312,0 +314,20 @@\n+\/\/--------------------------FlatArrayCheckNode---------------------------------\n+\/\/ Returns true if one of the input array objects or array klass ptrs (there\n+\/\/ can be multiple) is flat.\n+class FlatArrayCheckNode : public CmpNode {\n+public:\n+  enum {\n+    Control,\n+    Memory,\n+    ArrayOrKlass\n+  };\n+  FlatArrayCheckNode(Compile* C, Node* mem, Node* array_or_klass) : CmpNode(mem, array_or_klass) {\n+    init_class_id(Class_FlatArrayCheck);\n+    init_flags(Flag_is_macro);\n+    C->add_macro_node(this);\n+  }\n+  virtual int Opcode() const;\n+  virtual const Type* sub(const Type*, const Type*) const { ShouldNotReachHere(); return nullptr; }\n+  const Type* Value(PhaseGVN* phase) const;\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n+};\n","filename":"src\/hotspot\/share\/opto\/subnode.hpp","additions":23,"deletions":1,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -68,0 +69,1 @@\n+#include \"oops\/refArrayOop.inline.hpp\"\n@@ -416,0 +418,148 @@\n+static void validate_array_arguments(Klass* elmClass, jint len, TRAPS) {\n+  if (len < 0) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Array length is negative\");\n+  }\n+  elmClass->initialize(CHECK);\n+  if (elmClass->is_array_klass() || elmClass->is_identity_class()) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Element class is not a value class\");\n+  }\n+  if (elmClass->is_abstract()) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Element class is abstract\");\n+  }\n+}\n+\n+JVM_ENTRY(jarray, JVM_CopyOfSpecialArray(JNIEnv *env, jarray orig, jint from, jint to))\n+  oop o = JNIHandles::resolve_non_null(orig);\n+  assert(o->is_array(), \"Must be\");\n+  oop array = nullptr;\n+  arrayOop org = (arrayOop)o;\n+  arrayHandle oh(THREAD, org);\n+  ObjArrayKlass* ak = ObjArrayKlass::cast(org->klass());\n+  InlineKlass* vk = InlineKlass::cast(ak->element_klass());\n+  int len = to - from;  \/\/ length of the new array\n+  if (ak->is_null_free_array_klass()) {\n+    if ((len != 0) && (from >= org->length() || to > org->length())) {\n+      THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Copying of null-free array with uninitialized elements\");\n+    }\n+  }\n+  if (org->is_flatArray()) {\n+    FlatArrayKlass* fak = FlatArrayKlass::cast(org->klass());\n+    LayoutKind lk = fak->layout_kind();\n+    ArrayKlass::ArrayProperties props = ArrayKlass::ArrayProperties::DEFAULT;\n+    switch(lk) {\n+      case LayoutKind::ATOMIC_FLAT:\n+        props = ArrayKlass::ArrayProperties::NULL_RESTRICTED;\n+      break;\n+      case LayoutKind::NON_ATOMIC_FLAT:\n+        props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::NULL_RESTRICTED | ArrayKlass::ArrayProperties::NON_ATOMIC);\n+      break;\n+      case LayoutKind::NULLABLE_ATOMIC_FLAT:\n+      props = ArrayKlass::ArrayProperties::NON_ATOMIC;\n+      break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    array = oopFactory::new_flatArray(vk, len, props, lk, CHECK_NULL);\n+    arrayHandle ah(THREAD, (arrayOop)array);\n+    int end = to < oh()->length() ? to : oh()->length();\n+    for (int i = from; i < end; i++) {\n+      void* src = ((flatArrayOop)oh())->value_at_addr(i, fak->layout_helper());\n+      void* dst = ((flatArrayOop)ah())->value_at_addr(i - from, fak->layout_helper());\n+      vk->copy_payload_to_addr(src, dst, lk, false);\n+    }\n+    array = ah();\n+  } else {\n+    ArrayKlass::ArrayProperties props = org->is_null_free_array() ? ArrayKlass::ArrayProperties::NULL_RESTRICTED : ArrayKlass::ArrayProperties::DEFAULT;\n+    array = oopFactory::new_objArray(vk, len, props,  CHECK_NULL);\n+    int end = to < oh()->length() ? to : oh()->length();\n+    for (int i = from; i < end; i++) {\n+      if (i < ((objArrayOop)oh())->length()) {\n+        ((objArrayOop)array)->obj_at_put(i - from, ((objArrayOop)oh())->obj_at(i));\n+      } else {\n+        assert(!ak->is_null_free_array_klass(), \"Must be a nullable array\");\n+        ((objArrayOop)array)->obj_at_put(i - from, nullptr);\n+      }\n+    }\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jarray, JVM_NewNullRestrictedNonAtomicArray(JNIEnv *env, jclass elmClass, jint len, jobject initVal))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  oop init = JNIHandles::resolve(initVal);\n+  if (init == nullptr) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Initial value cannot be null\");\n+  }\n+  Handle init_h(THREAD, init);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  if (klass != init_h()->klass()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Type mismatch between array and initial value\");\n+  }\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  ArrayKlass::ArrayProperties props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::NON_ATOMIC | ArrayKlass::ArrayProperties::NULL_RESTRICTED);\n+  objArrayOop array = oopFactory::new_objArray(klass, len, props, CHECK_NULL);\n+  for (int i = 0; i < len; i++) {\n+    array->obj_at_put(i, init_h() \/*, CHECK_NULL*\/ );\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jarray, JVM_NewNullRestrictedAtomicArray(JNIEnv *env, jclass elmClass, jint len, jobject initVal))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  oop init = JNIHandles::resolve(initVal);\n+  if (init == nullptr) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Initial value cannot be null\");\n+  }\n+  Handle init_h(THREAD, init);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  if (klass != init_h()->klass()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Type mismatch between array and initial value\");\n+  }\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  ArrayKlass::ArrayProperties props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::NULL_RESTRICTED);\n+  objArrayOop array = oopFactory::new_objArray(klass, len, props, CHECK_NULL);\n+  for (int i = 0; i < len; i++) {\n+    array->obj_at_put(i, init_h() \/*, CHECK_NULL*\/ );\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jarray, JVM_NewNullableAtomicArray(JNIEnv *env, jclass elmClass, jint len))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  klass->initialize(CHECK_NULL);\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  ArrayKlass::ArrayProperties props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::DEFAULT);\n+  objArrayOop array = oopFactory::new_objArray(klass, len, props, CHECK_NULL);\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsFlatArray(JNIEnv *env, jobject obj))\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  return oop->is_flatArray();\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsNullRestrictedArray(JNIEnv *env, jobject obj))\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  return oop->is_null_free_array();\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsAtomicArray(JNIEnv *env, jobject obj))\n+  \/\/ There are multiple cases where an array can\/must support atomic access:\n+  \/\/   - the array is a reference array\n+  \/\/   - the array uses an atomic flat layout: NULLABLE_ATOMIC_FLAT or ATOMIC_FLAT\n+  \/\/   - the array is flat and its component type is naturally atomic\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  if (oop->is_refArray()) return true;\n+  if (oop->is_flatArray()) {\n+    FlatArrayKlass* fak = FlatArrayKlass::cast(oop->klass());\n+    if (fak->layout_kind() == LayoutKind::ATOMIC_FLAT || fak->layout_kind() == LayoutKind::NULLABLE_ATOMIC_FLAT) {\n+      return true;\n+    }\n+    if (fak->element_klass()->is_naturally_atomic()) return true;\n+  }\n+  return false;\n+JVM_END\n@@ -624,2 +774,22 @@\n-  return handle == nullptr ? 0 :\n-         checked_cast<jint>(ObjectSynchronizer::FastHashCode (THREAD, JNIHandles::resolve_non_null(handle)));\n+  if (handle == nullptr) {\n+    return 0;\n+  }\n+  oop obj = JNIHandles::resolve_non_null(handle);\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+      JavaValue result(T_INT);\n+      JavaCallArguments args;\n+      Handle ho(THREAD, obj);\n+      args.push_oop(ho);\n+      methodHandle method(THREAD, Universe::value_object_hash_code_method());\n+      JavaCalls::call(&result, method, &args, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in hashCode\", e, false);\n+        }\n+      }\n+      return result.get_jint();\n+  } else {\n+    return checked_cast<jint>(ObjectSynchronizer::FastHashCode(THREAD, obj));\n+  }\n@@ -673,0 +843,6 @@\n+  if (klass->is_inline_klass()) {\n+    \/\/ Value instances have no identity, so return the current instance instead of allocating a new one\n+    \/\/ Value classes cannot have finalizers, so the method can return immediately\n+    return JNIHandles::make_local(THREAD, obj());\n+  }\n+\n@@ -1165,1 +1341,2 @@\n-    size = InstanceKlass::cast(klass)->local_interfaces()->length();\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    size = ik->local_interfaces()->length();\n@@ -1199,1 +1376,0 @@\n-\n@@ -1680,1 +1856,1 @@\n-    if (want_constructor && !method->is_object_initializer()) {\n+    if (want_constructor && !method->is_object_constructor()) {\n@@ -1684,1 +1860,1 @@\n-        (method->is_object_initializer() || method->is_static_initializer() ||\n+        (method->is_object_constructor() || method->is_class_initializer() ||\n@@ -1712,0 +1888,1 @@\n+        assert(method->is_object_constructor(), \"must be\");\n@@ -1972,1 +2149,1 @@\n-  if (m->is_object_initializer()) {\n+  if (m->is_object_constructor()) {\n@@ -1975,1 +2152,0 @@\n-    \/\/ new_method accepts <clinit> as Method here\n@@ -2422,1 +2598,1 @@\n-  return method->name() == vmSymbols::object_initializer_name();\n+  return method->is_object_constructor();\n@@ -3216,0 +3392,4 @@\n+JVM_LEAF(jboolean, JVM_IsValhallaEnabled(void))\n+  return EnableValhalla ? JNI_TRUE : JNI_FALSE;\n+JVM_END\n+\n@@ -3295,1 +3475,3 @@\n-    objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n+    objArrayHandle args(THREAD, (objArrayOop)JNIHandles::resolve(args0));\n+    assert(args() == nullptr || !args->is_flatArray(), \"args are never flat or are they???\");\n+\n@@ -3315,0 +3497,2 @@\n+  objArrayHandle args(THREAD, (objArrayOop)JNIHandles::resolve(args0));\n+  assert(args() == nullptr || !args->is_flatArray(), \"args are never flat or are they???\");\n@@ -3316,1 +3500,0 @@\n-  objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n@@ -3553,0 +3736,1 @@\n+  refArrayHandle rah(THREAD, (refArrayOop)ah()); \/\/ j.l.Thread is an identity class, arrays are always reference arrays\n@@ -3558,1 +3742,1 @@\n-    oop thread_obj = ah->obj_at(i);\n+    oop thread_obj = rah->obj_at(i);\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":196,"deletions":12,"binary":false,"changes":208,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -63,0 +64,1 @@\n+#include \"oops\/access.hpp\"\n@@ -65,0 +67,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -90,0 +93,1 @@\n+#include \"runtime\/keepStackGCProcessed.hpp\"\n@@ -2008,0 +2012,103 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return nullptr;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(ih->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return nullptr;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayHandle result_array =\n+      oopFactory::new_objArray_handle(vmClasses::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ih->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+WB_END\n+\n+\/\/ Collect Object oops but not value objects...loaded from heap\n+class CollectObjectOops : public BasicOopIterateClosure {\n+  public:\n+  GrowableArray<Handle>* _array;\n+\n+  CollectObjectOops() {\n+      _array = new GrowableArray<Handle>(128);\n+  }\n+\n+  void add_oop(oop o) {\n+    Handle oh = Handle(Thread::current(), o);\n+    if (oh != nullptr && oh->is_inline_type()) {\n+      oh->oop_iterate(this);\n+    } else {\n+      _array->append(oh);\n+    }\n+  }\n+\n+  template <class T> inline void add_oop(T* p) { add_oop(HeapAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    objArrayHandle result_array =\n+        oopFactory::new_objArray_handle(vmClasses::Object_klass(), _array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < _array->length(); i++) {\n+      result_array->obj_at_put(i, _array->at(i)());\n+    }\n+    return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+  }\n+};\n+\n+\/\/ Collect Object oops but not value objects...loaded from frames\n+class CollectFrameObjectOops : public BasicOopIterateClosure {\n+ public:\n+  CollectObjectOops _collect;\n+\n+  template <class T> inline void add_oop(T* p) { _collect.add_oop(RawAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    return _collect.create_jni_result(env, THREAD);\n+  }\n+};\n+\n+\/\/ Collect Object oops for the given oop, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(thread);\n+  Handle objh(thread, JNIHandles::resolve(thing));\n+  CollectObjectOops collectOops;\n+  objh->oop_iterate(&collectOops);\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\/\/ Collect Object oops for the given frame deep, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  KeepStackGCProcessedMark ksgcpm(THREAD);\n+  ResourceMark rm(THREAD);\n+  CollectFrameObjectOops collectOops;\n+  StackFrameStream sfs(thread, true \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, nullptr, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n@@ -3004,0 +3111,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":113,"deletions":0,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -79,0 +79,1 @@\n+#include <string.h>\n@@ -366,0 +367,12 @@\n+bool Arguments::patching_migrated_classes(const char* property, const char* value) {\n+  if (strncmp(property, MODULE_PROPERTY_PREFIX, MODULE_PROPERTY_PREFIX_LEN) == 0) {\n+    const char* property_suffix = property + MODULE_PROPERTY_PREFIX_LEN;\n+    if (matches_property_suffix(property_suffix, PATCH, PATCH_LEN)) {\n+      if (strcmp(value, \"java.base-valueclasses.jar\")) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -1808,1 +1821,0 @@\n-static unsigned int patch_mod_count = 0;\n@@ -1815,1 +1827,1 @@\n-  if (!CDSConfig::check_vm_args_consistency(patch_mod_javabase, mode_flag_cmd_line)) {\n+  if (!CDSConfig::check_vm_args_consistency(mode_flag_cmd_line)) {\n@@ -1982,0 +1994,4 @@\n+  if (UseAltSubstitutabilityMethod) {\n+    no_shared_spaces(\"Alternate substitutability method doesn't work with CDS yet\");\n+  }\n+\n@@ -2064,1 +2080,1 @@\n-      add_patch_mod_prefix(module_name, module_equal + 1);\n+      add_patch_mod_prefix(module_name, module_equal + 1, false \/* no append *\/, false \/* no cds *\/);\n@@ -2066,3 +2082,0 @@\n-      if (!create_numbered_module_property(\"jdk.module.patch\", patch_mod_tail, patch_mod_count++)) {\n-        return JNI_ENOMEM;\n-      }\n@@ -2076,0 +2089,82 @@\n+\/\/ Temporary system property to disable preview patching and enable the new preview mode\n+\/\/ feature for testing\/development. Once the preview mode feature is finished, the value\n+\/\/ will be always 'true' and this code, and all related dead-code can be removed.\n+#define DISABLE_PREVIEW_PATCHING_DEFAULT false\n+\n+bool Arguments::disable_preview_patching() {\n+  const char* prop = get_property(\"DISABLE_PREVIEW_PATCHING\");\n+  return (prop != nullptr)\n+      ? strncmp(prop, \"true\", strlen(\"true\")) == 0\n+      : DISABLE_PREVIEW_PATCHING_DEFAULT;\n+}\n+\n+\/\/ VALUECLASS_STR must match string used in the build\n+#define VALUECLASS_STR \"valueclasses\"\n+#define VALUECLASS_JAR \"-\" VALUECLASS_STR \".jar\"\n+\n+\/\/ Finalize --patch-module args and --enable-preview related to value class module patches.\n+\/\/ Create all numbered properties passing module patches.\n+int Arguments::finalize_patch_module() {\n+  \/\/ If --enable-preview and EnableValhalla is true, modules may have preview mode resources.\n+  bool enable_valhalla_preview = enable_preview() && EnableValhalla;\n+  \/\/ Whether to use module patching, or the new preview mode feature for preview resources.\n+  bool disable_patching = disable_preview_patching();\n+\n+  \/\/ This must be called, even with 'false', to enable resource lookup from JImage.\n+  ClassLoader::init_jimage(disable_patching && enable_valhalla_preview);\n+\n+  \/\/ For each <module>-valueclasses.jar in <JAVA_HOME>\/lib\/valueclasses\/\n+  \/\/ appends the equivalent of --patch-module <module>=<JAVA_HOME>\/lib\/valueclasses\/<module>-valueclasses.jar\n+  if (!disable_patching && enable_valhalla_preview) {\n+    char * valueclasses_dir = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+    const char * fileSep = os::file_separator();\n+\n+    jio_snprintf(valueclasses_dir, JVM_MAXPATHLEN, \"%s%slib%s\" VALUECLASS_STR \"%s\",\n+                 Arguments::get_java_home(), fileSep, fileSep, fileSep);\n+    DIR* dir = os::opendir(valueclasses_dir);\n+    if (dir != nullptr) {\n+      char * module_name = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+      char * path = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+\n+      for (dirent * entry = os::readdir(dir); entry != nullptr; entry = os::readdir(dir)) {\n+        \/\/ Test if file ends-with \"-valueclasses.jar\"\n+        int len = (int)strlen(entry->d_name) - (sizeof(VALUECLASS_JAR) - 1);\n+        if (len <= 0 || strcmp(&entry->d_name[len], VALUECLASS_JAR) != 0) {\n+          continue;         \/\/ too short or not the expected suffix\n+        }\n+\n+        strcpy(module_name, entry->d_name);\n+        module_name[len] = '\\0';     \/\/ truncate to just module-name\n+\n+        jio_snprintf(path, JVM_MAXPATHLEN, \"%s%s\", valueclasses_dir, &entry->d_name);\n+        add_patch_mod_prefix(module_name, path, true \/* append *\/, true \/* cds OK*\/);\n+        log_info(class)(\"--enable-preview appending value classes for module %s: %s\", module_name, entry->d_name);\n+      }\n+      FreeHeap(module_name);\n+      FreeHeap(path);\n+      os::closedir(dir);\n+    }\n+    FreeHeap(valueclasses_dir);\n+  }\n+\n+  \/\/ Create numbered properties for each module that has been patched either\n+  \/\/ by --patch-module (or --enable-preview if disable_patching is false).\n+  \/\/ Format is \"jdk.module.patch.<n>=<module_name>=<path>\"\n+  if (_patch_mod_prefix != nullptr) {\n+    char * prop_value = AllocateHeap(JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, mtArguments);\n+    unsigned int patch_mod_count = 0;\n+\n+    for (GrowableArrayIterator<ModulePatchPath *> it = _patch_mod_prefix->begin();\n+            it != _patch_mod_prefix->end(); ++it) {\n+      jio_snprintf(prop_value, JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, \"%s=%s\",\n+                   (*it)->module_name(), (*it)->path_string());\n+      if (!create_numbered_module_property(\"jdk.module.patch\", prop_value, patch_mod_count++)) {\n+        FreeHeap(prop_value);\n+        return JNI_ENOMEM;\n+      }\n+    }\n+    FreeHeap(prop_value);\n+  }\n+  return JNI_OK;\n+}\n+\n@@ -2344,0 +2439,4 @@\n+      \/\/ --enable-preview enables Valhalla, EnableValhalla VM option will eventually be removed before integration\n+      if (FLAG_SET_CMDLINE(EnableValhalla, true) != JVMFlag::SUCCESS) {\n+        return JNI_EINVAL;\n+      }\n@@ -2847,10 +2946,5 @@\n-void Arguments::add_patch_mod_prefix(const char* module_name, const char* path) {\n-  \/\/ For java.base check for duplicate --patch-module options being specified on the command line.\n-  \/\/ This check is only required for java.base, all other duplicate module specifications\n-  \/\/ will be checked during module system initialization.  The module system initialization\n-  \/\/ will throw an ExceptionInInitializerError if this situation occurs.\n-  if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n-    if (patch_mod_javabase) {\n-      vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n-    } else {\n-      patch_mod_javabase = true;\n+void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool allow_append, bool allow_cds) {\n+  if (!allow_cds) {\n+    CDSConfig::set_module_patching_disables_cds();\n+    if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+      CDSConfig::set_java_base_module_patching_disables_cds();\n@@ -2865,1 +2959,18 @@\n-  _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  \/\/ Scan patches for matching module\n+  int i = _patch_mod_prefix->find_if([&](ModulePatchPath* patch) {\n+    return (strcmp(module_name, patch->module_name()) == 0);\n+  });\n+  if (i == -1) {\n+    _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  } else {\n+    if (allow_append) {\n+      \/\/ append path to existing module entry\n+      _patch_mod_prefix->at(i)->append_path(path);\n+    } else {\n+      if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+        vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n+      } else {\n+        vm_exit_during_initialization(\"Cannot specify a module more than once to --patch-module\", module_name);\n+      }\n+    }\n+  }\n@@ -2978,1 +3089,2 @@\n-  if (!check_vm_args_consistency()) {\n+  \/\/ finalize --module-patch and related --enable-preview\n+  if (finalize_patch_module() != JNI_OK) {\n@@ -2982,0 +3094,3 @@\n+  if (!check_vm_args_consistency()) {\n+    return JNI_ERR;\n+  }\n@@ -3867,0 +3982,12 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !CDSConfig::is_dumping_archive() && !UseSharedSpaces)) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported.\n+    \/\/ Also these aren't useful in -Xint. However, don't disable them when dumping or using\n+    \/\/ the CDS archive, as the values must match between dumptime and runtime.\n+    FLAG_SET_DEFAULT(InlineTypePassFieldsAsArgs, false);\n+    FLAG_SET_DEFAULT(InlineTypeReturnedAsFields, false);\n+  }\n+  if (!UseNonAtomicValueFlattening && !UseNullableValueFlattening && !UseAtomicValueFlattening) {\n+    \/\/ Flattening is disabled\n+    FLAG_SET_DEFAULT(UseArrayFlattening, false);\n+    FLAG_SET_DEFAULT(UseFieldFlattening, false);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":145,"deletions":18,"binary":false,"changes":163,"status":"modified"},{"patch":"@@ -816,1 +816,1 @@\n-  develop(bool, PrintFieldLayout, false,                                    \\\n+  product(bool, PrintFieldLayout, false, DIAGNOSTIC,                        \\\n@@ -819,0 +819,27 @@\n+  product(bool, PrintInlineLayout, false, DIAGNOSTIC,                       \\\n+          \"Print field layout for each inline type or class with inline fields\") \\\n+                                                                            \\\n+  product(bool, PrintFlatArrayLayout, false, DIAGNOSTIC,                    \\\n+          \"Print array layout for each inline type array\")                  \\\n+                                                                            \\\n+  product(bool, UseArrayFlattening, true,                                   \\\n+          \"Allow the VM to flatten arrays\")                                 \\\n+                                                                            \\\n+  product(bool, UseFieldFlattening, true,                                   \\\n+          \"Allow the VM to flatten value fields\")                           \\\n+                                                                            \\\n+  product(bool, UseNonAtomicValueFlattening, true,                          \\\n+          \"Allow the JVM to flatten some non-atomic null-free values\")      \\\n+                                                                            \\\n+  product(bool, UseNullableValueFlattening, true,                           \\\n+          \"Allow the JVM to flatten some nullable values\")                  \\\n+                                                                            \\\n+  product(bool, UseAtomicValueFlattening, true,                             \\\n+          \"Allow the JVM to flatten some atomic values\")                    \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxOops, 4,                                 \\\n+          \"Max nof embedded object references in an inline type to flatten, <0 no limit\")  \\\n+                                                                            \\\n+  develop(ccstrlist, PrintInlineKlassFields, \"\",                            \\\n+          \"Print fields collected by InlineKlass::collect_fields\")          \\\n+                                                                            \\\n@@ -1777,0 +1804,3 @@\n+  product(bool, IgnoreAssertUnsetFields, false, DIAGNOSTIC,                           \\\n+          \"Ignore assert_unset_fields\")                                     \\\n+                                                                            \\\n@@ -1948,0 +1978,20 @@\n+  product(bool, EnableValhalla, true,                                       \\\n+          \"Enable experimental Valhalla features\")                          \\\n+                                                                            \\\n+  product_pd(bool, InlineTypePassFieldsAsArgs,                              \\\n+          \"Pass each inline type field as an argument at calls\")            \\\n+                                                                            \\\n+  product_pd(bool, InlineTypeReturnedAsFields,                              \\\n+          \"Return fields instead of an inline type reference\")              \\\n+                                                                            \\\n+  develop(bool, StressCallingConvention, false,                             \\\n+          \"Stress the scalarized calling convention.\")                      \\\n+                                                                            \\\n+  develop(bool, PreloadClasses, true,                                       \\\n+          \"Preloading all classes from the LoadableDescriptors attribute\")  \\\n+                                                                            \\\n+  product(ccstrlist, ForceNonTearable, \"\", DIAGNOSTIC,                      \\\n+          \"List of inline classes which are forced to be atomic \"           \\\n+          \"(whitespace and commas separate names, \"                         \\\n+          \"and leading and trailing stars '*' are wildcards)\")              \\\n+                                                                            \\\n@@ -2006,0 +2056,3 @@\n+  product(bool, UseAltSubstitutabilityMethod, false,                        \\\n+          \"Use alternate version of the isSubstitutable method to \"         \\\n+          \"compare value class instances\")                                  \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":54,"deletions":1,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -54,0 +55,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -231,1 +233,2 @@\n-    value->l = cast_from_oop<jobject>(objArrayOop(a)->obj_at(index));\n+    oop o = objArrayOop(a)->obj_at(index, CHECK_(T_ILLEGAL)); \/\/ reading from a flat array can throw an OOM\n+    value->l = cast_from_oop<jobject>(o);\n@@ -273,0 +276,1 @@\n+\n@@ -276,0 +280,4 @@\n+      if (a->is_null_free_array() && obj == nullptr) {\n+         THROW_MSG(vmSymbols::java_lang_NullPointerException(), \"null-restricted array\");\n+      }\n+\n@@ -760,3 +768,0 @@\n-  if (log_is_enabled(Debug, class, resolve)) {\n-    trace_class_resolution(nt);\n-  }\n@@ -768,3 +773,2 @@\n-  assert(!method()->is_object_initializer() &&\n-         (for_constant_pool_access || !method()->is_static_initializer()),\n-         \"Should not be the initializer\");\n+  assert(!method()->name()->starts_with('<') || for_constant_pool_access,\n+         \"should call new_constructor instead\");\n@@ -818,1 +822,2 @@\n-  assert(method()->is_object_initializer(), \"Should be the initializer\");\n+  assert(method()->is_object_constructor(),\n+         \"should call new_method instead\");\n@@ -867,0 +872,2 @@\n+\n+  int flags = 0;\n@@ -868,1 +875,4 @@\n-    java_lang_reflect_Field::set_trusted_final(rh());\n+    flags |= TRUSTED_FINAL;\n+  }\n+  if (fd->is_null_free_inline_type()) {\n+    flags |= NULL_RESTRICTED;\n@@ -870,0 +880,2 @@\n+  java_lang_reflect_Field::set_flags(rh(), flags);\n+\n@@ -980,1 +992,2 @@\n-    if (reflected_method->is_private() || reflected_method->name() == vmSymbols::object_initializer_name()) {\n+    if (reflected_method->is_private() ||\n+        reflected_method->name() == vmSymbols::object_initializer_name()) {\n@@ -1060,2 +1073,2 @@\n-    oop type_mirror = ptypes->obj_at(i);\n-    oop arg = args->obj_at(i);\n+    oop type_mirror = ptypes->obj_at(i, CHECK_NULL);\n+    oop arg = args->obj_at(i, CHECK_NULL);\n@@ -1162,1 +1175,0 @@\n-  assert(method->name() == vmSymbols::object_initializer_name(), \"invalid constructor\");\n","filename":"src\/hotspot\/share\/runtime\/reflection.cpp","additions":25,"deletions":13,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -416,0 +416,1 @@\n+  initialize_class(vmSymbols::java_lang_IdentityException(), CHECK);\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+import jdk.internal.value.DeserializeConstructor;\n@@ -53,4 +54,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Double} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -359,0 +368,1 @@\n+@jdk.internal.MigratedValueClass\n@@ -972,0 +982,1 @@\n+    @DeserializeConstructor\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Double.java","additions":15,"deletions":4,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+import jdk.internal.misc.PreviewFeatures;\n@@ -31,0 +32,1 @@\n+import jdk.internal.value.DeserializeConstructor;\n@@ -59,4 +61,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Integer} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -73,0 +83,1 @@\n+@jdk.internal.MigratedValueClass\n@@ -953,8 +964,20 @@\n-     * {@code int} value.  If a new {@code Integer} instance is not\n-     * required, this method should generally be used in preference to\n-     * the constructor {@link #Integer(int)}, as this method is likely\n-     * to yield significantly better space and time performance by\n-     * caching frequently requested values.\n-     *\n-     * This method will always cache values in the range -128 to 127,\n-     * inclusive, and may cache other values outside of this range.\n+     * {@code int} value.\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          <p>\n+     *              - When preview features are NOT enabled, {@code Integer} is an identity class.\n+     *              If a new {@code Integer} instance is not\n+     *              required, this method should generally be used in preference to\n+     *              the constructor {@link #Integer(int)}, as this method is likely\n+     *              to yield significantly better space and time performance by\n+     *              caching frequently requested values.\n+     *              This method will always cache values in the range -128 to 127,\n+     *              inclusive, and may cache other values outside of this range.\n+     *          <\/p>\n+     *          <p>\n+     *              - When preview features are enabled, {@code Integer} is a {@linkplain Class#isValue value class}.\n+     *              The {@code valueOf} behavior is the same as invoking the constructor,\n+     *              whether cached or not.\n+     *          <\/p>\n+     *      <\/div>\n+     * <\/div>\n@@ -967,0 +990,1 @@\n+    @DeserializeConstructor\n@@ -968,2 +992,4 @@\n-        if (i >= IntegerCache.low && i <= IntegerCache.high)\n-            return IntegerCache.cache[i + (-IntegerCache.low)];\n+        if (!PreviewFeatures.isEnabled()) {\n+            if (i >= IntegerCache.low && i <= IntegerCache.high)\n+                return IntegerCache.cache[i + (-IntegerCache.low)];\n+        }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Integer.java","additions":40,"deletions":14,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -37,0 +37,2 @@\n+import jdk.internal.misc.PreviewFeatures;\n+import jdk.internal.value.DeserializeConstructor;\n@@ -59,4 +61,13 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Long} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n+ *\n@@ -73,0 +84,1 @@\n+@jdk.internal.MigratedValueClass\n@@ -944,8 +956,19 @@\n-     * If a new {@code Long} instance is not required, this method\n-     * should generally be used in preference to the constructor\n-     * {@link #Long(long)}, as this method is likely to yield\n-     * significantly better space and time performance by caching\n-     * frequently requested values.\n-     *\n-     * This method will always cache values in the range -128 to 127,\n-     * inclusive, and may cache other values outside of this range.\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          <p>\n+     *              - When preview features are NOT enabled, {@code Long} is an identity class.\n+     *              If a new {@code Long} instance is not required, this method\n+     *              should generally be used in preference to the constructor\n+     *              {@link #Long(long)}, as this method is likely to yield\n+     *              significantly better space and time performance by caching\n+     *              frequently requested values.\n+     *              This method will always cache values in the range -128 to 127,\n+     *              inclusive, and may cache other values outside of this range.\n+     *          <\/p>\n+     *          <p>\n+     *              - When preview features are enabled, {@code Long} is a {@linkplain Class#isValue value class}.\n+     *              The {@code valueOf} behavior is the same as invoking the constructor,\n+     *              whether cached or not.\n+     *          <\/p>\n+     *      <\/div>\n+     * <\/div>\n@@ -958,0 +981,1 @@\n+    @DeserializeConstructor\n@@ -959,3 +983,5 @@\n-        final int offset = 128;\n-        if (l >= -128 && l <= 127) { \/\/ will cache\n-            return LongCache.cache[(int)l + offset];\n+        if (!PreviewFeatures.isEnabled()) {\n+            if (l >= -128 && l <= 127) { \/\/ will cache\n+                final int offset = 128;\n+                return LongCache.cache[(int) l + offset];\n+            }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Long.java","additions":41,"deletions":15,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -36,0 +36,11 @@\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, subclasses of {@code java.lang.Object} can be either\n+ *          an {@linkplain Class#isIdentity identity class} or a {@linkplain Class#isValue value class}.\n+ *          See {@jls The Java Language Specification 8.1.1.5 Value Classes}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n+ *\n@@ -300,0 +311,6 @@\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          If this object is a {@linkplain Class#isValue() value object},\n+     *          it does does not have a monitor, an {@code IllegalMonitorStateException} is thrown.\n+     *      <\/div>\n+     * <\/div>\n@@ -302,1 +319,2 @@\n-     *               the owner of this object's monitor.\n+     *               the owner of this object's monitor or\n+     *               if this object is a {@linkplain Class#isValue() value object}.\n@@ -326,0 +344,7 @@\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          If this object is a {@linkplain Class#isValue() value object},\n+     *          it does does not have a monitor, an {@code IllegalMonitorStateException} is thrown.\n+     *      <\/div>\n+     * <\/div>\n+     *\n@@ -327,1 +352,2 @@\n-     *               the owner of this object's monitor.\n+     *               the owner of this object's monitor or\n+     *               if this object is a {@linkplain Class#isValue() value object}.\n@@ -342,0 +368,7 @@\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          If this object is a {@linkplain Class#isValue() value object},\n+     *          it does does not have a monitor, an {@code IllegalMonitorStateException} is thrown.\n+     *      <\/div>\n+     * <\/div>\n+     *\n@@ -343,1 +376,2 @@\n-     *         the owner of the object's monitor\n+     *         the owner of the object's monitor or\n+     *         if this object is a {@linkplain Class#isValue() value object}.\n@@ -365,0 +399,7 @@\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          If this object is a {@linkplain Class#isValue() value object},\n+     *          it does does not have a monitor, an {@code IllegalMonitorStateException} is thrown.\n+     *      <\/div>\n+     * <\/div>\n+     *\n@@ -368,1 +409,2 @@\n-     *         the owner of the object's monitor\n+     *         the owner of the object's monitor or\n+     *         if this object is a {@linkplain Class#isValue() value object}.\n@@ -478,0 +520,6 @@\n+     * <div class=\"preview-block\">\n+     *      <div class=\"preview-comment\">\n+     *          If this object is a {@linkplain Class#isValue() value object},\n+     *          it does does not have a monitor, an {@code IllegalMonitorStateException} is thrown.\n+     *      <\/div>\n+     * <\/div>\n@@ -483,1 +531,2 @@\n-     *         the owner of the object's monitor\n+     *         the owner of the object's monitor or\n+     *         if this object is a {@linkplain Class#isValue() value object}.\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Object.java","additions":54,"deletions":5,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -1558,0 +1558,5 @@\n+            public boolean isNullRestrictedField(MethodHandle mh) {\n+                var memberName = mh.internalMemberName();\n+                return memberName.isNullRestricted();\n+            }\n+\n@@ -1648,0 +1653,4 @@\n+            @Override\n+            public MethodHandle assertAsType(MethodHandle original, MethodType assertedType) {\n+                return original.viewAsType(assertedType, false);\n+            }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandleImpl.java","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -34,0 +34,2 @@\n+import java.util.Objects;\n+\n@@ -39,0 +41,8 @@\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          The referent must have {@linkplain Objects#hasIdentity(Object) object identity}.\n+ *          When preview features are enabled, attempts to create a reference\n+ *          to a {@linkplain Class#isValue value object} result in an {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -532,0 +542,3 @@\n+        if (referent != null) {\n+            Objects.requireIdentity(referent);\n+        }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/Reference.java","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -198,5 +198,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n- * The {@code equals} method should be used for comparisons.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Instant} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -210,0 +217,1 @@\n+@jdk.internal.MigratedValueClass\n","filename":"src\/java.base\/share\/classes\/java\/time\/Instant.java","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -100,0 +100,1 @@\n+    private final Preview preview;\n@@ -119,0 +120,1 @@\n+        preview = Preview.instance(context);\n@@ -378,0 +380,16 @@\n+            if (!c.type.isErroneous()\n+                    && toAnnotate.kind == TYP\n+                    && types.isSameType(c.type, syms.migratedValueClassType)) {\n+                toAnnotate.flags_field |= Flags.MIGRATED_VALUE_CLASS;\n+            }\n+\n+            if (!c.type.isErroneous()\n+                    && toAnnotate.kind == VAR\n+                    && toAnnotate.owner.kind == TYP\n+                    && types.isSameType(c.type, syms.strictType)) {\n+                preview.checkSourceLevel(pos.get(c), Feature.VALUE_CLASSES);\n+                toAnnotate.flags_field |= Flags.STRICT;\n+                \/\/ temporary hack to indicate that a class has at least one strict field\n+                toAnnotate.owner.flags_field |= Flags.HAS_STRICT;\n+            }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Annotate.java","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -127,0 +127,1 @@\n+    final LocalProxyVarsGen localProxyVarsGen;\n@@ -166,0 +167,1 @@\n+        localProxyVarsGen = LocalProxyVarsGen.instance(context);\n@@ -187,0 +189,2 @@\n+        allowValueClasses = (!preview.isPreview(Feature.VALUE_CLASSES) || preview.isEnabled()) &&\n+                Feature.VALUE_CLASSES.allowedInSource(source);\n@@ -205,0 +209,4 @@\n+    \/** Are value classes allowed\n+     *\/\n+    private final boolean allowValueClasses;\n+\n@@ -298,3 +306,1 @@\n-            return;\n-        }\n-        if ((v.flags() & FINAL) != 0 &&\n+        } else if ((v.flags() & FINAL) != 0 &&\n@@ -311,17 +317,0 @@\n-            return;\n-        }\n-\n-        \/\/ Check instance field assignments that appear in constructor prologues\n-        if (rs.isEarlyReference(env, base, v)) {\n-\n-            \/\/ Field may not be inherited from a superclass\n-            if (v.owner != env.enclClass.sym) {\n-                log.error(pos, Errors.CantRefBeforeCtorCalled(v));\n-                return;\n-            }\n-\n-            \/\/ Field may not have an initializer\n-            if ((v.flags() & HASINIT) != 0) {\n-                log.error(pos, Errors.CantAssignInitializedBeforeCtorCalled(v));\n-                return;\n-            }\n@@ -1122,1 +1111,1 @@\n-                            if (TreeInfo.hasAnyConstructorCall(tree)) {\n+                            if (!allowValueClasses && TreeInfo.hasAnyConstructorCall(tree)) {\n@@ -1200,0 +1189,1 @@\n+                boolean addedSuperInIdentityClass = false;\n@@ -1204,1 +1194,6 @@\n-                        tree.body.stats = tree.body.stats.prepend(supCall);\n+                        if (allowValueClasses && (owner.isValueClass() || owner.hasStrict() || ((owner.flags_field & RECORD) != 0))) {\n+                            tree.body.stats = tree.body.stats.append(supCall);\n+                        } else {\n+                            tree.body.stats = tree.body.stats.prepend(supCall);\n+                            addedSuperInIdentityClass = true;\n+                        }\n@@ -1242,0 +1237,28 @@\n+                if (localEnv.info.ctorPrologue) {\n+                    boolean thisInvocation = false;\n+                    ListBuffer<JCTree> prologueCode = new ListBuffer<>();\n+                    for (JCTree stat : tree.body.stats) {\n+                        prologueCode.add(stat);\n+                        \/* gather all the stats in the body until a `super` or `this` constructor invocation is found,\n+                         * including the constructor invocation, that way we don't need to worry in the visitor below if\n+                         * if we are dealing or not with prologue code\n+                         *\/\n+                        if (stat instanceof JCExpressionStatement expStmt &&\n+                                expStmt.expr instanceof JCMethodInvocation mi &&\n+                                TreeInfo.isConstructorCall(mi)) {\n+                            thisInvocation = TreeInfo.name(mi.meth) == names._this;\n+                            if (!addedSuperInIdentityClass || !allowValueClasses) {\n+                                break;\n+                            }\n+                        }\n+                    }\n+                    if (!prologueCode.isEmpty()) {\n+                        CtorPrologueVisitor ctorPrologueVisitor = new CtorPrologueVisitor(localEnv,\n+                                addedSuperInIdentityClass && allowValueClasses ?\n+                                        PrologueVisitorMode.WARNINGS_ONLY :\n+                                        thisInvocation ?\n+                                                PrologueVisitorMode.THIS_CONSTRUCTOR :\n+                                                PrologueVisitorMode.SUPER_CONSTRUCTOR);\n+                        ctorPrologueVisitor.scan(prologueCode.toList());\n+                    }\n+                }\n@@ -1253,0 +1276,321 @@\n+    enum PrologueVisitorMode {\n+        WARNINGS_ONLY,\n+        SUPER_CONSTRUCTOR,\n+        THIS_CONSTRUCTOR\n+    }\n+\n+    class CtorPrologueVisitor extends TreeScanner {\n+        Env<AttrContext> localEnv;\n+        PrologueVisitorMode mode;\n+\n+        CtorPrologueVisitor(Env<AttrContext> localEnv, PrologueVisitorMode mode) {\n+            this.localEnv = localEnv;\n+            currentClassSym = localEnv.enclClass.sym;\n+            this.mode = mode;\n+        }\n+\n+        boolean insideLambdaOrClassDef = false;\n+\n+        @Override\n+        public void visitLambda(JCLambda lambda) {\n+            boolean previousInsideLambdaOrClassDef = insideLambdaOrClassDef;\n+            try {\n+                insideLambdaOrClassDef = true;\n+                super.visitLambda(lambda);\n+            } finally {\n+                insideLambdaOrClassDef = previousInsideLambdaOrClassDef;\n+            }\n+        }\n+\n+        ClassSymbol currentClassSym;\n+\n+        @Override\n+        public void visitClassDef(JCClassDecl classDecl) {\n+            boolean previousInsideLambdaOrClassDef = insideLambdaOrClassDef;\n+            ClassSymbol previousClassSym = currentClassSym;\n+            try {\n+                insideLambdaOrClassDef = true;\n+                currentClassSym = classDecl.sym;\n+                super.visitClassDef(classDecl);\n+            } finally {\n+                insideLambdaOrClassDef = previousInsideLambdaOrClassDef;\n+                currentClassSym = previousClassSym;\n+            }\n+        }\n+\n+        private void reportPrologueError(JCTree tree, Symbol sym) {\n+            reportPrologueError(tree, sym, false);\n+        }\n+\n+        private void reportPrologueError(JCTree tree, Symbol sym, boolean hasInit) {\n+            preview.checkSourceLevel(tree, Feature.FLEXIBLE_CONSTRUCTORS);\n+            if (mode != PrologueVisitorMode.WARNINGS_ONLY) {\n+                if (hasInit) {\n+                    log.error(tree, Errors.CantAssignInitializedBeforeCtorCalled(sym));\n+                } else {\n+                    log.error(tree, Errors.CantRefBeforeCtorCalled(sym));\n+                }\n+            } else if (allowValueClasses) {\n+                \/\/ issue lint warning\n+                log.warning(tree, LintWarnings.WouldNotBeAllowedInPrologue(sym));\n+            }\n+        }\n+\n+        @Override\n+        public void visitApply(JCMethodInvocation tree) {\n+            super.visitApply(tree);\n+            Name name = TreeInfo.name(tree.meth);\n+            boolean isConstructorCall = name == names._this || name == names._super;\n+            Symbol msym = TreeInfo.symbolFor(tree.meth);\n+            \/\/ is this an instance method call or an illegal constructor invocation like: `this.super()`?\n+            if (msym != null && \/\/ for erroneous invocations msym can be null, ignore those\n+                (!isConstructorCall ||\n+                isConstructorCall && tree.meth.hasTag(SELECT))) {\n+                if (isEarlyReference(localEnv, tree.meth, msym))\n+                    reportPrologueError(tree.meth, msym);\n+            }\n+        }\n+\n+        @Override\n+        public void visitIdent(JCIdent tree) {\n+            analyzeSymbol(tree);\n+        }\n+\n+        boolean isIndexed = false;\n+\n+        @Override\n+        public void visitIndexed(JCArrayAccess tree) {\n+            boolean previousIsIndexed = isIndexed;\n+            try {\n+                isIndexed = true;\n+                scan(tree.indexed);\n+            } finally {\n+                isIndexed = previousIsIndexed;\n+            }\n+            scan(tree.index);\n+            if (mode == PrologueVisitorMode.SUPER_CONSTRUCTOR && isInstanceField(tree.indexed)) {\n+                localProxyVarsGen.addFieldReadInPrologue(localEnv.enclMethod, TreeInfo.symbolFor(tree.indexed));\n+            }\n+        }\n+\n+        @Override\n+        public void visitSelect(JCFieldAccess tree) {\n+            SelectScanner ss = new SelectScanner();\n+            ss.scan(tree);\n+            if (ss.scanLater == null) {\n+                analyzeSymbol(tree);\n+            } else {\n+                boolean prevLhs = isInLHS;\n+                try {\n+                    isInLHS = false;\n+                    scan(ss.scanLater);\n+                } finally {\n+                    isInLHS = prevLhs;\n+                }\n+            }\n+            if (mode == PrologueVisitorMode.SUPER_CONSTRUCTOR) {\n+                for (JCTree subtree : ss.selectorTrees) {\n+                    if (isInstanceField(subtree)) {\n+                        \/\/ we need to add a proxy for this one\n+                        localProxyVarsGen.addFieldReadInPrologue(localEnv.enclMethod, TreeInfo.symbolFor(subtree));\n+                    }\n+                }\n+            }\n+        }\n+\n+        boolean isInstanceField(JCTree tree) {\n+            Symbol sym = TreeInfo.symbolFor(tree);\n+            return (sym != null &&\n+                    !sym.isStatic() &&\n+                    sym.kind == VAR &&\n+                    sym.owner.kind == TYP &&\n+                    sym.name != names._this &&\n+                    sym.name != names._super &&\n+                    isEarlyReference(localEnv, tree, sym));\n+        }\n+\n+        @Override\n+        public void visitNewClass(JCNewClass tree) {\n+            super.visitNewClass(tree);\n+            checkNewClassAndMethRefs(tree, tree.type);\n+        }\n+\n+        @Override\n+        public void visitReference(JCMemberReference tree) {\n+            super.visitReference(tree);\n+            if (tree.getMode() == JCMemberReference.ReferenceMode.NEW) {\n+                checkNewClassAndMethRefs(tree, tree.expr.type);\n+            }\n+        }\n+\n+        void checkNewClassAndMethRefs(JCTree tree, Type t) {\n+            if (t.tsym.isEnclosedBy(localEnv.enclClass.sym) &&\n+                    !t.tsym.isStatic() &&\n+                    !t.tsym.isDirectlyOrIndirectlyLocal()) {\n+                reportPrologueError(tree, t.getEnclosingType().tsym);\n+            }\n+        }\n+\n+        \/* if a symbol is in the LHS of an assignment expression we won't consider it as a candidate\n+         * for a proxy local variable later on\n+         *\/\n+        boolean isInLHS = false;\n+\n+        @Override\n+        public void visitAssign(JCAssign tree) {\n+            boolean previousIsInLHS = isInLHS;\n+            try {\n+                isInLHS = true;\n+                scan(tree.lhs);\n+            } finally {\n+                isInLHS = previousIsInLHS;\n+            }\n+            scan(tree.rhs);\n+        }\n+\n+        @Override\n+        public void visitMethodDef(JCMethodDecl tree) {\n+            \/\/ ignore any declarative part, mainly to avoid scanning receiver parameters\n+            scan(tree.body);\n+        }\n+\n+        void analyzeSymbol(JCTree tree) {\n+            Symbol sym = TreeInfo.symbolFor(tree);\n+            \/\/ make sure that there is a symbol and it is not static\n+            if (sym == null || sym.isStatic()) {\n+                return;\n+            }\n+            if (isInLHS && !insideLambdaOrClassDef) {\n+                \/\/ Check instance field assignments that appear in constructor prologues\n+                if (isEarlyReference(localEnv, tree, sym)) {\n+                    \/\/ Field may not be inherited from a superclass\n+                    if (sym.owner != localEnv.enclClass.sym) {\n+                        reportPrologueError(tree, sym);\n+                        return;\n+                    }\n+                    \/\/ Field may not have an initializer\n+                    if ((sym.flags() & HASINIT) != 0) {\n+                        if (!localEnv.enclClass.sym.isValueClass() || !sym.type.hasTag(ARRAY) || !isIndexed) {\n+                            reportPrologueError(tree, sym, true);\n+                        }\n+                        return;\n+                    }\n+                    \/\/ cant reference an instance field before a this constructor\n+                    if (allowValueClasses && mode == PrologueVisitorMode.THIS_CONSTRUCTOR) {\n+                        reportPrologueError(tree, sym);\n+                        return;\n+                    }\n+                }\n+                return;\n+            }\n+            tree = TreeInfo.skipParens(tree);\n+            if (sym.kind == VAR && sym.owner.kind == TYP) {\n+                if (sym.name == names._this || sym.name == names._super) {\n+                    \/\/ are we seeing something like `this` or `CurrentClass.this` or `SuperClass.super::foo`?\n+                    if (TreeInfo.isExplicitThisReference(\n+                            types,\n+                            (ClassType)localEnv.enclClass.sym.type,\n+                            tree)) {\n+                        reportPrologueError(tree, sym);\n+                    }\n+                } else if (sym.kind == VAR && sym.owner.kind == TYP) { \/\/ now fields only\n+                    if (sym.owner != localEnv.enclClass.sym) {\n+                        if (localEnv.enclClass.sym.isSubClass(sym.owner, types) &&\n+                                sym.isInheritedIn(localEnv.enclClass.sym, types)) {\n+                            \/* if we are dealing with a field that doesn't belong to the current class, but the\n+                             * field is inherited, this is an error. Unless, the super class is also an outer\n+                             * class and the field's qualifier refers to the outer class\n+                             *\/\n+                            if (tree.hasTag(IDENT) ||\n+                                TreeInfo.isExplicitThisReference(\n+                                        types,\n+                                        (ClassType)localEnv.enclClass.sym.type,\n+                                        ((JCFieldAccess)tree).selected)) {\n+                                reportPrologueError(tree, sym);\n+                            }\n+                        }\n+                    } else if (isEarlyReference(localEnv, tree, sym)) {\n+                        \/* now this is a `proper` instance field of the current class\n+                         * references to fields of identity classes which happen to have initializers are\n+                         * not allowed in the prologue\n+                         *\/\n+                        if (insideLambdaOrClassDef ||\n+                            (!localEnv.enclClass.sym.isValueClass() && (sym.flags_field & HASINIT) != 0))\n+                            reportPrologueError(tree, sym);\n+                        \/\/ we will need to generate a proxy for this field later on\n+                        if (!isInLHS) {\n+                            if (!allowValueClasses) {\n+                                reportPrologueError(tree, sym);\n+                            } else {\n+                                if (mode == PrologueVisitorMode.THIS_CONSTRUCTOR) {\n+                                    reportPrologueError(tree, sym);\n+                                } else if (mode == PrologueVisitorMode.SUPER_CONSTRUCTOR) {\n+                                    localProxyVarsGen.addFieldReadInPrologue(localEnv.enclMethod, sym);\n+                                }\n+                                \/* we do nothing in warnings only mode, as in that mode we are simulating what\n+                                 * the compiler would do in case the constructor code would be in the prologue\n+                                 * phase\n+                                 *\/\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        \/**\n+         * Determine if the symbol appearance constitutes an early reference to the current class.\n+         *\n+         * <p>\n+         * This means the symbol is an instance field, or method, of the current class and it appears\n+         * in an early initialization context of it (i.e., one of its constructor prologues).\n+         *\n+         * @param env    The current environment\n+         * @param tree   the AST referencing the variable\n+         * @param sym    The symbol\n+         *\/\n+        private boolean isEarlyReference(Env<AttrContext> env, JCTree tree, Symbol sym) {\n+            if ((sym.flags() & STATIC) == 0 &&\n+                    (sym.kind == VAR || sym.kind == MTH) &&\n+                    sym.isMemberOf(env.enclClass.sym, types)) {\n+                \/\/ Allow \"Foo.this.x\" when \"Foo\" is (also) an outer class, as this refers to the outer instance\n+                if (tree instanceof JCFieldAccess fa) {\n+                    return TreeInfo.isExplicitThisReference(types, (ClassType)env.enclClass.type, fa.selected);\n+                } else if (currentClassSym != env.enclClass.sym) {\n+                    \/* so we are inside a class, CI, in the prologue of an outer class, CO, and the symbol being\n+                     * analyzed has no qualifier. So if the symbol is a member of CI the reference is allowed,\n+                     * otherwise it is not.\n+                     * It could be that the reference to CI's member happens inside CI's own prologue, but that\n+                     * will be checked separately, when CI's prologue is analyzed.\n+                     *\/\n+                    return !sym.isMemberOf(currentClassSym, types);\n+                }\n+                return true;\n+            }\n+            return false;\n+        }\n+\n+        \/* scanner for a select expression, anything that is not a select or identifier\n+         * will be stored for further analysis\n+         *\/\n+        class SelectScanner extends DeferredAttr.FilterScanner {\n+            JCTree scanLater;\n+            java.util.List<JCTree> selectorTrees = new ArrayList<>();\n+\n+            SelectScanner() {\n+                super(Set.of(IDENT, SELECT, PARENS));\n+            }\n+\n+            @Override\n+            public void visitSelect(JCFieldAccess tree) {\n+                super.visitSelect(tree);\n+                selectorTrees.add(tree.selected);\n+            }\n+\n+            @Override\n+            void skip(JCTree tree) {\n+                scanLater = tree;\n+            }\n+        }\n+    }\n+\n@@ -1313,4 +1657,19 @@\n-                    attribExpr(tree.init, initEnv, v.type);\n-                    if (tree.isImplicitlyTyped()) {\n-                        \/\/fixup local variable type\n-                        v.type = chk.checkLocalVarType(tree, tree.init.type, tree.name);\n+                    boolean previousCtorPrologue = initEnv.info.ctorPrologue;\n+                    try {\n+                        if (v.owner.kind == TYP && !v.isStatic() && v.isStrict()) {\n+                            \/\/ strict instance initializer in a value class\n+                            initEnv.info.ctorPrologue = true;\n+                        }\n+                        attribExpr(tree.init, initEnv, v.type);\n+                        if (tree.isImplicitlyTyped()) {\n+                            \/\/fixup local variable type\n+                            v.type = chk.checkLocalVarType(tree, tree.init.type, tree.name);\n+                        }\n+                        if (allowValueClasses && v.owner.kind == TYP && !v.isStatic()) {\n+                            \/\/ strict field initializers are inlined in constructor's prologues\n+                            CtorPrologueVisitor ctorPrologueVisitor = new CtorPrologueVisitor(initEnv,\n+                                    !v.isStrict() ? PrologueVisitorMode.WARNINGS_ONLY : PrologueVisitorMode.SUPER_CONSTRUCTOR);\n+                            ctorPrologueVisitor.scan(tree.init);\n+                        }\n+                    } finally {\n+                        initEnv.info.ctorPrologue = previousCtorPrologue;\n@@ -1437,1 +1796,5 @@\n-            if ((tree.flags & STATIC) != 0) localEnv.info.staticLevel++;\n+            if ((tree.flags & STATIC) != 0) {\n+                localEnv.info.staticLevel++;\n+            } else {\n+                localEnv.info.instanceInitializerBlock = true;\n+            }\n@@ -1950,2 +2313,2 @@\n-        chk.checkRefType(tree.pos(), attribExpr(tree.lock, env));\n-        if (tree.lock.type != null && tree.lock.type.isValueBased()) {\n+        boolean identityType = chk.checkIdentityType(tree.pos(), attribExpr(tree.lock, env));\n+        if (identityType && tree.lock.type != null && tree.lock.type.isValueBased()) {\n@@ -4396,0 +4759,1 @@\n+        Assert.check(site == tree.selected.type);\n@@ -5507,1 +5871,1 @@\n-                } else {\n+                } else if ((c.flags_field & Flags.COMPOUND) == 0) {\n@@ -5544,0 +5908,5 @@\n+                if (c.isValueClass()) {\n+                    Assert.check(env.tree.hasTag(CLASSDEF));\n+                    chk.checkConstraintsOfValueClass((JCClassDecl) env.tree, c);\n+                }\n+\n@@ -5686,1 +6055,1 @@\n-            chk.checkSerialStructure(tree, c);\n+            chk.checkSerialStructure(env, tree, c);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":400,"deletions":31,"binary":false,"changes":431,"status":"modified"},{"patch":"@@ -170,0 +170,2 @@\n+        allowValueClasses = (!preview.isPreview(Feature.VALUE_CLASSES) || preview.isEnabled()) &&\n+                Feature.VALUE_CLASSES.allowedInSource(source);\n@@ -193,0 +195,4 @@\n+    \/** Are value classes allowed\n+     *\/\n+    private final boolean allowValueClasses;\n+\n@@ -670,0 +676,25 @@\n+    void checkConstraintsOfValueClass(JCClassDecl tree, ClassSymbol c) {\n+        DiagnosticPosition pos = tree.pos();\n+        for (Type st : types.closure(c.type)) {\n+            if (st == null || st.tsym == null || st.tsym.kind == ERR)\n+                continue;\n+            if  (st.tsym == syms.objectType.tsym || st.tsym == syms.recordType.tsym || st.isInterface())\n+                continue;\n+            if (!st.tsym.isAbstract()) {\n+                if (c != st.tsym) {\n+                    log.error(pos, Errors.ConcreteSupertypeForValueClass(c, st));\n+                }\n+                continue;\n+            }\n+            \/\/ dealing with an abstract value or value super class below.\n+            for (Symbol s : st.tsym.members().getSymbols(NON_RECURSIVE)) {\n+                if (s.kind == MTH) {\n+                    if ((s.flags() & (SYNCHRONIZED | STATIC)) == SYNCHRONIZED) {\n+                        log.error(pos, Errors.SuperClassMethodCannotBeSynchronized(s, c, st));\n+                    }\n+                    break;\n+                }\n+            }\n+        }\n+    }\n+\n@@ -727,0 +758,26 @@\n+    \/** Check that type is an identity type, i.e. not a value type.\n+     *  When not discernible statically, give it the benefit of doubt\n+     *  and defer to runtime.\n+     *\n+     *  @param pos           Position to be used for error reporting.\n+     *  @param t             The type to be checked.\n+     *\/\n+    boolean checkIdentityType(DiagnosticPosition pos, Type t) {\n+        if (t.hasTag(TYPEVAR)) {\n+            t = types.skipTypeVars(t, false);\n+        }\n+        if (t.isIntersection()) {\n+            IntersectionClassType ict = (IntersectionClassType)t;\n+            boolean result = true;\n+            for (Type component : ict.getExplicitComponents()) {\n+                result &= checkIdentityType(pos, component);\n+            }\n+            return result;\n+        }\n+        if (t.isPrimitive() || (t.isValueClass() && !t.tsym.isAbstract())) {\n+            typeTagError(pos, diags.fragment(Fragments.TypeReqIdentity), t);\n+            return false;\n+        }\n+        return true;\n+    }\n+\n@@ -1118,2 +1175,11 @@\n-            else\n-                mask = VarFlags;\n+            else {\n+                boolean isInstanceField = (flags & STATIC) == 0;\n+                boolean isInstanceFieldOfValueClass = isInstanceField && sym.owner.type.isValueClass();\n+                boolean isRecordField = isInstanceField && (sym.owner.flags_field & RECORD) != 0;\n+                if (allowValueClasses && (isInstanceFieldOfValueClass || isRecordField)) {\n+                    implicit |= FINAL | STRICT;\n+                    mask = ValueFieldFlags;\n+                } else {\n+                    mask = VarFlags;\n+                }\n+            }\n@@ -1145,1 +1211,2 @@\n-                mask = RecordMethodFlags;\n+                mask = ((sym.owner.flags_field & VALUE_CLASS) != 0 && (flags & Flags.STATIC) == 0) ?\n+                        RecordMethodFlags & ~SYNCHRONIZED : RecordMethodFlags;\n@@ -1147,1 +1214,3 @@\n-                mask = MethodFlags;\n+                \/\/ value objects do not have an associated monitor\/lock\n+                mask = ((sym.owner.flags_field & VALUE_CLASS) != 0 && (flags & Flags.STATIC) == 0) ?\n+                        MethodFlags & ~SYNCHRONIZED : MethodFlags;\n@@ -1164,1 +1233,1 @@\n-                mask = staticOrImplicitlyStatic && allowRecords && (flags & ANNOTATION) == 0 ? StaticLocalFlags : LocalClassFlags;\n+                mask = staticOrImplicitlyStatic && allowRecords && (flags & ANNOTATION) == 0 ? ExtendedStaticLocalClassFlags : ExtendedLocalClassFlags;\n@@ -1180,0 +1249,4 @@\n+            if ((flags & (VALUE_CLASS | SEALED | ABSTRACT)) == (VALUE_CLASS | SEALED) ||\n+                (flags & (VALUE_CLASS | NON_SEALED | ABSTRACT)) == (VALUE_CLASS | NON_SEALED)) {\n+                log.error(pos, Errors.NonAbstractValueClassCantBeSealedOrNonSealed);\n+            }\n@@ -1183,0 +1256,4 @@\n+            if ((flags & (INTERFACE | VALUE_CLASS)) == 0) {\n+                implicit |= IDENTITY_TYPE;\n+            }\n+\n@@ -1184,2 +1261,2 @@\n-                \/\/ enums can't be declared abstract, final, sealed or non-sealed\n-                mask &= ~(ABSTRACT | FINAL | SEALED | NON_SEALED);\n+                \/\/ enums can't be declared abstract, final, sealed or non-sealed or value\n+                mask &= ~(ABSTRACT | FINAL | SEALED | NON_SEALED | VALUE_CLASS);\n@@ -1198,0 +1275,5 @@\n+\n+            \/\/ concrete value classes are implicitly final\n+            if ((flags & (ABSTRACT | INTERFACE | VALUE_CLASS)) == VALUE_CLASS) {\n+                implicit |= FINAL;\n+            }\n@@ -1212,2 +1294,1 @@\n-        }\n-        else if ((sym.kind == TYP ||\n+        } else if ((sym.kind == TYP ||\n@@ -1236,1 +1317,2 @@\n-                 checkDisjoint(pos, flags,\n+                 \/\/ we are using `implicit` here as instance fields of value classes are implicitly final\n+                 checkDisjoint(pos, flags | implicit,\n@@ -1252,1 +1334,7 @@\n-                                ANNOTATION)) {\n+                                ANNOTATION)\n+                && checkDisjoint(pos, flags,\n+                                VALUE_CLASS,\n+                                ANNOTATION)\n+                && checkDisjoint(pos, flags,\n+                                VALUE_CLASS,\n+                                INTERFACE) ) {\n@@ -2049,0 +2137,5 @@\n+        if (allowValueClasses && origin.isValueClass() && names.finalize.equals(m.name)) {\n+            if (m.overrides(syms.objectFinalize, origin, types, false)) {\n+                log.warning(tree.pos(), Warnings.ValueFinalize);\n+            }\n+        }\n@@ -2484,0 +2577,12 @@\n+\n+        Type identitySuper = null;\n+        for (Type t : types.closure(c)) {\n+            if (t != c) {\n+                if (t.isIdentityClass() && (t.tsym.flags() & VALUE_BASED) == 0)\n+                    identitySuper = t;\n+                if (c.isValueClass() && identitySuper != null && identitySuper.tsym != syms.objectType.tsym) { \/\/ Object is special\n+                    log.error(pos, Errors.ValueTypeHasIdentitySuperType(c, identitySuper));\n+                    break;\n+                }\n+            }\n+        }\n@@ -4817,2 +4922,2 @@\n-    public void checkSerialStructure(JCClassDecl tree, ClassSymbol c) {\n-        (new SerialTypeVisitor()).visit(c, tree);\n+    public void checkSerialStructure(Env<AttrContext> env, JCClassDecl tree, ClassSymbol c) {\n+        (new SerialTypeVisitor(env)).visit(c, tree);\n@@ -4849,1 +4954,2 @@\n-        SerialTypeVisitor() {\n+        Env<AttrContext> env;\n+        SerialTypeVisitor(Env<AttrContext> env) {\n@@ -4851,0 +4957,1 @@\n+            this.env = env;\n@@ -4910,0 +5017,1 @@\n+            final boolean[] hasWriteReplace = {false};\n@@ -4984,1 +5092,1 @@\n-                            case \"writeReplace\"     -> checkWriteReplace(tree,e, method);\n+                            case \"writeReplace\"     -> {hasWriteReplace[0] = true; hasAppropriateWriteReplace(tree, method, true);}\n@@ -4995,1 +5103,20 @@\n-\n+            if (!hasWriteReplace[0] &&\n+                    (c.isValueClass() || hasAbstractValueSuperClass(c, Set.of(syms.numberType.tsym))) &&\n+                    !c.isAbstract() && !c.isRecord() &&\n+                    types.unboxedType(c.type) == Type.noType) {\n+                \/\/ we need to check if the class is inheriting an appropriate writeReplace method\n+                MethodSymbol ms = null;\n+                Log.DiagnosticHandler discardHandler = log.new DiscardDiagnosticHandler();\n+                try {\n+                    ms = rs.resolveInternalMethod(env.tree, env, c.type, names.writeReplace, List.nil(), List.nil());\n+                } catch (FatalError fe) {\n+                    \/\/ ignore no method was found\n+                } finally {\n+                    log.popDiagnosticHandler(discardHandler);\n+                }\n+                if (ms == null || !hasAppropriateWriteReplace(p, ms, false)) {\n+                    log.warning(p.pos(),\n+                            c.isValueClass() ? LintWarnings.SerializableValueClassWithoutWriteReplace1 :\n+                                    LintWarnings.SerializableValueClassWithoutWriteReplace2);\n+                }\n+            }\n@@ -5003,0 +5130,16 @@\n+        private boolean hasAbstractValueSuperClass(Symbol c, Set<Symbol> excluding) {\n+            while (c.getKind() == ElementKind.CLASS) {\n+                Type sup = ((ClassSymbol)c).getSuperclass();\n+                if (!sup.hasTag(CLASS) || sup.isErroneous() ||\n+                        sup.tsym == syms.objectType.tsym) {\n+                    return false;\n+                }\n+                \/\/ if it is a value super class it has to be abstract\n+                if (sup.isValueClass() && !excluding.contains(sup.tsym)) {\n+                    return true;\n+                }\n+                c = sup.tsym;\n+            }\n+            return false;\n+        }\n+\n@@ -5126,1 +5269,1 @@\n-            checkReturnType(tree, e, method, syms.voidType);\n+            isExpectedReturnType(tree, method, syms.voidType, true);\n@@ -5128,1 +5271,1 @@\n-            checkExceptions(tree, e, method, syms.ioExceptionType);\n+            hasExpectedExceptions(tree, method, true, syms.ioExceptionType);\n@@ -5132,1 +5275,1 @@\n-        private void checkWriteReplace(JCClassDecl tree, Element e, MethodSymbol method) {\n+        private boolean hasAppropriateWriteReplace(JCClassDecl tree, MethodSymbol method, boolean warn) {\n@@ -5138,4 +5281,4 @@\n-            checkConcreteInstanceMethod(tree, e, method);\n-            checkReturnType(tree, e, method, syms.objectType);\n-            checkNoArgs(tree, e, method);\n-            checkExceptions(tree, e, method, syms.objectStreamExceptionType);\n+            return isConcreteInstanceMethod(tree, method, warn) &&\n+                    isExpectedReturnType(tree, method, syms.objectType, warn) &&\n+                    hasNoArgs(tree, method, warn) &&\n+                    hasExpectedExceptions(tree, method, warn, syms.objectStreamExceptionType);\n@@ -5152,1 +5295,1 @@\n-            checkReturnType(tree, e, method, syms.voidType);\n+            isExpectedReturnType(tree, method, syms.voidType, true);\n@@ -5154,1 +5297,1 @@\n-            checkExceptions(tree, e, method, syms.ioExceptionType, syms.classNotFoundExceptionType);\n+            hasExpectedExceptions(tree, method, true, syms.ioExceptionType, syms.classNotFoundExceptionType);\n@@ -5161,3 +5304,3 @@\n-            checkReturnType(tree, e, method, syms.voidType);\n-            checkNoArgs(tree, e, method);\n-            checkExceptions(tree, e, method, syms.objectStreamExceptionType);\n+            isExpectedReturnType(tree, method, syms.voidType, true);\n+            hasNoArgs(tree, method, true);\n+            hasExpectedExceptions(tree, method, true, syms.objectStreamExceptionType);\n@@ -5173,4 +5316,4 @@\n-            checkConcreteInstanceMethod(tree, e, method);\n-            checkReturnType(tree,e, method, syms.objectType);\n-            checkNoArgs(tree, e, method);\n-            checkExceptions(tree, e, method, syms.objectStreamExceptionType);\n+            isConcreteInstanceMethod(tree, method, true);\n+            isExpectedReturnType(tree, method, syms.objectType, true);\n+            hasNoArgs(tree, method, true);\n+            hasExpectedExceptions(tree, method, true, syms.objectStreamExceptionType);\n@@ -5426,1 +5569,1 @@\n-                        case \"writeReplace\" -> checkWriteReplace(tree, e, method);\n+                        case \"writeReplace\" -> hasAppropriateWriteReplace(tree, method, true);\n@@ -5444,3 +5587,3 @@\n-        void checkConcreteInstanceMethod(JCClassDecl tree,\n-                                         Element enclosing,\n-                                         MethodSymbol method) {\n+        boolean isConcreteInstanceMethod(JCClassDecl tree,\n+                                         MethodSymbol method,\n+                                         boolean warn) {\n@@ -5448,0 +5591,1 @@\n+                if (warn) {\n@@ -5451,0 +5595,2 @@\n+                }\n+                return false;\n@@ -5452,0 +5598,1 @@\n+            return true;\n@@ -5454,4 +5601,4 @@\n-        private void checkReturnType(JCClassDecl tree,\n-                                     Element enclosing,\n-                                     MethodSymbol method,\n-                                     Type expectedReturnType) {\n+        private boolean isExpectedReturnType(JCClassDecl tree,\n+                                          MethodSymbol method,\n+                                          Type expectedReturnType,\n+                                          boolean warn) {\n@@ -5465,2 +5612,3 @@\n-                log.warning(\n-                        TreeInfo.diagnosticPositionFor(method, tree),\n+                if (warn) {\n+                    log.warning(\n+                            TreeInfo.diagnosticPositionFor(method, tree),\n@@ -5469,0 +5617,2 @@\n+                }\n+                return false;\n@@ -5470,0 +5620,1 @@\n+            return true;\n@@ -5507,1 +5658,1 @@\n-        private void checkNoArgs(JCClassDecl tree, Element enclosing, MethodSymbol method) {\n+        boolean hasNoArgs(JCClassDecl tree, MethodSymbol method, boolean warn) {\n@@ -5510,2 +5661,3 @@\n-                log.warning(\n-                        TreeInfo.diagnosticPositionFor(parameters.get(0), tree),\n+                if (warn) {\n+                    log.warning(\n+                            TreeInfo.diagnosticPositionFor(parameters.get(0), tree),\n@@ -5513,0 +5665,2 @@\n+                }\n+                return false;\n@@ -5514,0 +5668,1 @@\n+            return true;\n@@ -5526,4 +5681,4 @@\n-        private void checkExceptions(JCClassDecl tree,\n-                                     Element enclosing,\n-                                     MethodSymbol method,\n-                                     Type... declaredExceptions) {\n+        private boolean hasExpectedExceptions(JCClassDecl tree,\n+                                              MethodSymbol method,\n+                                              boolean warn,\n+                                              Type... declaredExceptions) {\n@@ -5548,2 +5703,3 @@\n-                        log.warning(\n-                                TreeInfo.diagnosticPositionFor(method, tree),\n+                        if (warn) {\n+                            log.warning(\n+                                    TreeInfo.diagnosticPositionFor(method, tree),\n@@ -5552,0 +5708,2 @@\n+                        }\n+                        return false;\n@@ -5555,1 +5713,1 @@\n-            return;\n+            return true;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Check.java","additions":210,"deletions":52,"binary":false,"changes":262,"status":"modified"},{"patch":"@@ -786,1 +786,1 @@\n-    improperly formed type, some parameters are missing\n+    improperly formed type, some parameters are missing or misplaced\n@@ -2139,0 +2139,8 @@\n+# lint: serial\n+compiler.warn.serializable.value.class.without.write.replace.1=\\\n+    serializable value class does not declare, or inherits, a writeReplace method\n+\n+# lint: serial\n+compiler.warn.serializable.value.class.without.write.replace.2=\\\n+    serializable class does not declare, or inherits, a writeReplace method\n+\n@@ -2870,0 +2878,3 @@\n+compiler.misc.type.req.identity=\\\n+    a type with identity\n+\n@@ -3930,0 +3941,3 @@\n+compiler.misc.bad.access.flags=\\\n+    bad access flags combination: {0}\n+\n@@ -4271,0 +4285,22 @@\n+compiler.misc.feature.value.classes=\\\n+    value classes\n+\n+# 0: type, 1: type\n+compiler.err.value.type.has.identity.super.type=\\\n+    The identity type {1} cannot be a supertype of the value type {0}\n+\n+# 0: symbol, 1: type\n+compiler.err.concrete.supertype.for.value.class=\\\n+    The concrete class {1} is not allowed to be a super class of the value class {0} either directly or indirectly\n+\n+# 0: symbol, 1: symbol, 2: type\n+compiler.err.super.class.method.cannot.be.synchronized=\\\n+    The method {0} in the super class {2} of the value class {1} is synchronized. This is disallowed\n+\n+compiler.err.non.abstract.value.class.cant.be.sealed.or.non.sealed=\\\n+    ''sealed'' or ''non-sealed'' modifiers are only applicable to abstract value classes\n+\n+# 0: symbol\n+compiler.err.strict.field.not.have.been.initialized.before.super=\\\n+    strict field {0} is not initialized before the supertype constructor has been called\n+\n@@ -4298,0 +4334,5 @@\n+# 0: symbol or name\n+# lint: initialization\n+compiler.warn.would.not.be.allowed.in.prologue=\\\n+    reference to {0} would not be allowed in the prologue phase\n+\n@@ -4302,0 +4343,3 @@\n+\n+compiler.warn.value.finalize=\\\n+    value classes should not have finalize methods, they are not invoked\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/compiler.properties","additions":45,"deletions":1,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -82,0 +82,3 @@\n+compiler\/jvmci\/jdk.vm.ci.hotspot.test\/src\/jdk\/vm\/ci\/hotspot\/test\/MemoryAccessProviderTest.java 8350208 generic-all\n+compiler\/jvmci\/jdk.vm.ci.hotspot.test\/src\/jdk\/vm\/ci\/hotspot\/test\/TestHotSpotResolvedJavaField.java 8350208 generic-all\n+\n@@ -84,0 +87,24 @@\n+# Valhalla\n+compiler\/regalloc\/TestVerifyRegisterAllocator.java 8365895 windows-x64\n+compiler\/types\/TestArrayManyDimensions.java 8365895 windows-x64\n+compiler\/types\/correctness\/OffTest.java 8365895 windows-x64\n+compiler\/whitebox\/DeoptimizeRelocatedNMethod.java 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#C1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#C2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#G1C1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#G1C2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ParallelC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ParallelC2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#SerialC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#SerialC2 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ZGCC1 8370571 generic-all\n+compiler\/whitebox\/RelocateNMethodMultiplePaths.java#ZGCC2 8370571 generic-all\n+compiler\/whitebox\/StressNMethodRelocation.java 8370571 generic-all\n+compiler\/valhalla\/inlinetypes\/TestNullableArrays.java#id0 8367553 generic-aarch64\n+compiler\/valhalla\/inlinetypes\/TestNullableArrays.java#id1 8367553 generic-aarch64\n+compiler\/valhalla\/inlinetypes\/TestNullableArrays.java#id2 8367553 generic-aarch64\n+compiler\/valhalla\/inlinetypes\/TestNullableArrays.java#id3 8367553 generic-aarch64\n+compiler\/valhalla\/inlinetypes\/TestNullableArrays.java#id4 8367553 generic-aarch64\n+compiler\/valhalla\/inlinetypes\/TestNullableArrays.java#id5 8367553 generic-aarch64\n+compiler\/valhalla\/inlinetypes\/TestNullableArrays.java#id6 8367553 generic-aarch64\n+\n@@ -102,0 +129,1 @@\n+runtime\/cds\/appcds\/redefineClass\/RedefineRunningMethods_Shared.java  8304168 generic-all\n@@ -122,0 +150,15 @@\n+\n+# Valhalla\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictInstanceFieldsTest.java CODETOOLS-7904031 generic-all\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictStaticFieldsTest.java CODETOOLS-7904031 generic-all\n+\n+runtime\/cds\/TestDefaultArchiveLoading.java#coops_nocoh            8366774           generic-all\n+runtime\/cds\/TestDefaultArchiveLoading.java#nocoops_nocoh          8366774           generic-all\n+\n+# Valhalla + AOT\n+runtime\/cds\/appcds\/aotCache\/HelloAOTCache.java                                  8369043 generic-aarch64\n+runtime\/cds\/appcds\/aotCode\/AOTCodeFlags.java                                    8369043 generic-aarch64\n+runtime\/cds\/appcds\/methodHandles\/MethodHandlesGeneralTest.java#aot              8367408 generic-all\n+runtime\/cds\/appcds\/resolvedConstants\/ResolvedConstants.java#aot                 8371456 generic-all\n+runtime\/cds\/appcds\/resolvedConstants\/ResolvedConstants.java#static              8371456 generic-all\n+\n@@ -147,0 +190,60 @@\n+\n+# Valhalla TODO:\n+serviceability\/jvmti\/valhalla\/HeapDump\/HeapDump.java 8317416 generic-all\n+\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbDumpclass.java 8190936 generic-all\n+\n+# Array Changes TODO\n+serviceability\/sa\/CDSJMapClstats.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbClasses.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DisableRegistryTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbDumpheap.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbTestConnectArgument.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DebugdConnectTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJhisto.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id1 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackWithConcurrentLock.java 8365722 generic-all\n+serviceability\/sa\/TestJhsdbJstackWithVirtualThread.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackXcompStress.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#process 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#core 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id0 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id1 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#serial 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#parallel 8365722 generic-all\n+serviceability\/sa\/DeadlockDetectionTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id0 8365722 generic-all\n+serviceability\/sa\/TestInstanceKlassSize.java 8365722 generic-all\n+serviceability\/sa\/TestSysProps.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbAttachToDebugServer.java 8365722 generic-all\n+resourcehogs\/serviceability\/sa\/TestHeapDumpForLargeArray.java 8365722 generic-all\n+serviceability\/HeapDump\/DuplicateArrayClassesTest.java 8365722 generic-all\n+\n+resourcehogs\/serviceability\/sa\/ClhsdbRegionDetailsScanOopsForG1.java 8190936 generic-all\n+vmTestbase\/nsk\/jvmti\/scenarios\/events\/EM04\/em04t001\/TestDescription.java 8367590 generic-all\n+\n@@ -185,0 +288,2 @@\n+vmTestbase\/vm\/mlvm\/hiddenloader\/stress\/byteMutation\/Test.java 8317172 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":105,"deletions":0,"binary":false,"changes":105,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-  runtime\n+  runtime \\\n@@ -65,0 +65,8 @@\n+hotspot_valhalla = \\\n+  runtime\/valhalla \\\n+  compiler\/valhalla \\\n+  serviceability\/jvmti\/valhalla\n+\n+hotspot_valhalla_runtime = \\\n+  runtime\/valhalla\n+\n@@ -214,0 +222,1 @@\n+  compiler\/valhalla\/ \\\n@@ -255,0 +264,7 @@\n+\n+tier1_compiler_no_valhalla = \\\n+  :tier1_compiler_1 \\\n+  :tier1_compiler_2 \\\n+  :tier1_compiler_3 \\\n+  -compiler\/valhalla\n+\n@@ -407,0 +423,4 @@\n+tier1_runtime_no_valhalla = \\\n+  :tier1_runtime \\\n+  -runtime\/valhalla\n+\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":21,"deletions":1,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -213,2 +213,3 @@\n-            case coh_no_cds -> run_test(true, false);\n-            case coh_cds -> run_test(true, true);\n+            \/\/ TODO 8348568 Re-enable\n+            \/\/ case coh_no_cds -> run_test(true, false);\n+            \/\/ case coh_cds -> run_test(true, true);\n","filename":"test\/hotspot\/jtreg\/runtime\/ErrorHandling\/AccessZeroNKlassHitsProtectionZone.java","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -532,0 +532,5 @@\n+java\/lang\/ModuleLayer\/LayerControllerTest.java                  8337048 generic-all\n+java\/lang\/ModuleLayer\/BasicLayerTest.java                       8337048 generic-all\n+\n+java\/lang\/Thread\/virtual\/stress\/Skynet.java#default             8342977 generic-all\n+\n@@ -549,0 +554,1 @@\n+com\/sun\/management\/HotSpotDiagnosticMXBean\/DumpThreadsWithEliminatedLock.java 8360599 generic-all\n@@ -570,0 +576,2 @@\n+java\/net\/CookieHandler\/B6644726.java                            8365811 generic-all\n+\n@@ -722,0 +730,4 @@\n+com\/sun\/jdi\/cds\/CDSBreakpointTest.java                          8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSDeleteAllBkptsTest.java                      8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSFieldWatchpoints.java                        8304168 generic-all\n+\n@@ -730,0 +742,1 @@\n+java\/util\/logging\/LoggingDeadlock2.java       8368801 generic-all\n@@ -816,0 +829,1 @@\n+\n@@ -821,0 +835,13 @@\n+\n+############################################################################\n+\n+# valhalla\n+java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessValue.java 8367346 generic-all\n+\n+jdk\/classfile\/AccessFlagsTest.java 8366270 generic-all\n+jdk\/jfr\/event\/runtime\/TestClassLoaderStatsEvent.java 8366820 generic-all\n+\n+sun\/tools\/jhsdb\/BasicLauncherTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTestWithActiveProcess.java 8366806 generic-all\n+sun\/tools\/jhsdb\/JShellHeapDumpTest.java 8366806 generic-all\n","filename":"test\/jdk\/ProblemList.txt","additions":27,"deletions":0,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -156,0 +156,14 @@\n+  private native Object[] getObjectsViaKlassOopMaps0(Object thing);\n+  public Object[] getObjectsViaKlassOopMaps(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaKlassOopMaps0(thing);\n+  }\n+\n+  private native Object[] getObjectsViaOopIterator0(Object thing);\n+  public Object[] getObjectsViaOopIterator(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaOopIterator0(thing);\n+  }\n+\n+  public native Object[] getObjectsViaFrameOopIterator(int depth);\n+\n@@ -734,4 +748,7 @@\n-  private final List<Function<String,Object>> flagsGetters = Arrays.asList(\n-    this::getBooleanVMFlag, this::getIntVMFlag, this::getUintVMFlag,\n-    this::getIntxVMFlag, this::getUintxVMFlag, this::getUint64VMFlag,\n-    this::getSizeTVMFlag, this::getStringVMFlag, this::getDoubleVMFlag);\n+  private final List<Function<String,Object>> flagsGetters;\n+  {\n+      flagsGetters = Arrays.asList(\n+          this::getBooleanVMFlag, this::getIntVMFlag, this::getUintVMFlag,\n+          this::getIntxVMFlag, this::getUintxVMFlag, this::getUint64VMFlag,\n+          this::getSizeTVMFlag, this::getStringVMFlag, this::getDoubleVMFlag);\n+  }\n@@ -776,0 +793,1 @@\n+  @SuppressWarnings(\"initialization\")\n","filename":"test\/lib\/jdk\/test\/whitebox\/WhiteBox.java","additions":22,"deletions":4,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+import jdk.internal.jimage.PreviewMode;\n@@ -42,1 +43,0 @@\n-import java.nio.ByteOrder;\n@@ -74,1 +74,0 @@\n-        protected ByteOrder byteOrder;\n@@ -79,1 +78,0 @@\n-            byteOrder = ByteOrder.nativeOrder();\n@@ -96,1 +94,1 @@\n-            reader = ImageReader.open(copiedImageFile, byteOrder);\n+            reader = ImageReader.open(copiedImageFile, PreviewMode.DISABLED);\n@@ -125,1 +123,1 @@\n-            reader = ImageReader.open(copiedImageFile, byteOrder);\n+            reader = ImageReader.open(copiedImageFile, PreviewMode.DISABLED);\n@@ -152,1 +150,1 @@\n-        try (var reader = ImageReader.open(state.copiedImageFile, state.byteOrder)) {\n+        try (var reader = ImageReader.open(state.copiedImageFile, PreviewMode.DISABLED)) {\n@@ -176,1 +174,1 @@\n-        try (var reader = ImageReader.open(state.copiedImageFile, state.byteOrder)) {\n+        try (var reader = ImageReader.open(state.copiedImageFile, PreviewMode.DISABLED)) {\n@@ -213,1 +211,1 @@\n-        try (var reader = ImageReader.open(state.copiedImageFile, state.byteOrder)) {\n+        try (var reader = ImageReader.open(state.copiedImageFile, PreviewMode.DISABLED)) {\n@@ -510,1 +508,1 @@\n-            \"\/modules\/java.base\/java\/nio\/ByteOrder.class\",\n+            \"\/modules\/java.base\/java\/nio\/PreviewMode.DISABLED.class\",\n","filename":"test\/micro\/org\/openjdk\/bench\/jdk\/internal\/jrtfs\/ImageReaderBenchmark.java","additions":7,"deletions":9,"binary":false,"changes":16,"status":"modified"}]}