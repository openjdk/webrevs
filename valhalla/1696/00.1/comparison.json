{"files":[{"patch":"@@ -757,0 +757,2 @@\n+# Default disabled within Valhalla until support added (JDK-8348568)\n+#\n@@ -759,1 +761,1 @@\n-  UTIL_ARG_ENABLE(NAME: cds-archive-coh, DEFAULT: auto, RESULT: BUILD_CDS_ARCHIVE_COH,\n+  UTIL_ARG_ENABLE(NAME: cds-archive-coh, DEFAULT: false, RESULT: BUILD_CDS_ARCHIVE_COH,\n","filename":"make\/autoconf\/jdk-options.m4","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -122,0 +122,66 @@\n+\/\/ Implementation of LoadFlattenedArrayStub\n+\n+LoadFlattenedArrayStub::LoadFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr result, CodeEmitInfo* info) {\n+  _array = array;\n+  _index = index;\n+  _result = result;\n+  _scratch_reg = FrameMap::r0_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void LoadFlattenedArrayStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_array->as_register(), 1);\n+  ce->store_parameter(_index->as_register(), 0);\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(StubId::c1_load_flat_array_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  if (_result->as_register() != r0) {\n+    __ mov(_result->as_register(), r0);\n+  }\n+  __ b(_continuation);\n+}\n+\n+\n+\/\/ Implementation of StoreFlattenedArrayStub\n+\n+StoreFlattenedArrayStub::StoreFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr value, CodeEmitInfo* info) {\n+  _array = array;\n+  _index = index;\n+  _value = value;\n+  _scratch_reg = FrameMap::r0_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+\n+void StoreFlattenedArrayStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_array->as_register(), 2);\n+  ce->store_parameter(_index->as_register(), 1);\n+  ce->store_parameter(_value->as_register(), 0);\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(StubId::c1_store_flat_array_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  __ b(_continuation);\n+}\n+\n+\/\/ Implementation of SubstitutabilityCheckStub\n+SubstitutabilityCheckStub::SubstitutabilityCheckStub(LIR_Opr left, LIR_Opr right, CodeEmitInfo* info) {\n+  _left = left;\n+  _right = right;\n+  _scratch_reg = FrameMap::r0_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void SubstitutabilityCheckStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_left->as_register(), 1);\n+  ce->store_parameter(_right->as_register(), 0);\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(StubId::c1_substitutability_check_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  __ b(_continuation);\n+}\n@@ -138,2 +204,0 @@\n-\n-\n@@ -179,1 +243,2 @@\n-NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info) {\n+NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result,\n+                                       CodeEmitInfo* info, bool is_null_free) {\n@@ -184,0 +249,1 @@\n+  _is_null_free = is_null_free;\n@@ -192,1 +258,7 @@\n-  __ far_call(RuntimeAddress(Runtime1::entry_for(StubId::c1_new_object_array_id)));\n+\n+  if (_is_null_free) {\n+    __ far_call(RuntimeAddress(Runtime1::entry_for(StubId::c1_new_null_free_array_id)));\n+  } else {\n+    __ far_call(RuntimeAddress(Runtime1::entry_for(StubId::c1_new_object_array_id)));\n+  }\n+\n@@ -202,0 +274,10 @@\n+  if (_throw_ie_stub != nullptr) {\n+    \/\/ When we come here, _obj_reg has already been checked to be non-null.\n+    __ ldr(rscratch1, Address(_obj_reg->as_register(), oopDesc::mark_offset_in_bytes()));\n+    __ mov(rscratch2, markWord::inline_type_pattern);\n+    __ andr(rscratch1, rscratch1, rscratch2);\n+\n+    __ cmp(rscratch1, rscratch2);\n+    __ br(Assembler::EQ, *_throw_ie_stub->entry());\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":86,"deletions":4,"binary":false,"changes":90,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -36,0 +37,1 @@\n+#include \"ci\/ciObjArrayKlass.hpp\"\n@@ -41,0 +43,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -429,1 +432,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -473,0 +476,42 @@\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    ciType* return_type = compilation()->method()->return_type();\n+    if (return_type->is_inlinetype()) {\n+      ciInlineKlass* vk = return_type->as_inline_klass();\n+      if (vk->can_be_returned_as_fields()) {\n+        address unpack_handler = vk->unpack_handler();\n+        assert(unpack_handler != nullptr, \"must be\");\n+        __ far_call(RuntimeAddress(unpack_handler));\n+      }\n+    } else if (return_type->is_instance_klass() && (!return_type->is_loaded() || StressCallingConvention)) {\n+      Label skip;\n+      Label not_null;\n+      __ cbnz(r0, not_null);\n+      \/\/ Returned value is null, zero all return registers because they may belong to oop fields\n+      __ mov(j_rarg1, zr);\n+      __ mov(j_rarg2, zr);\n+      __ mov(j_rarg3, zr);\n+      __ mov(j_rarg4, zr);\n+      __ mov(j_rarg5, zr);\n+      __ mov(j_rarg6, zr);\n+      __ mov(j_rarg7, zr);\n+      __ b(skip);\n+      __ bind(not_null);\n+\n+      \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+      __ test_oop_is_not_inline_type(r0, rscratch2, skip, \/* can_be_null= *\/ false);\n+\n+      \/\/ Load fields from a buffered value with an inline class specific handler\n+      __ load_klass(rscratch1 \/*dst*\/, r0 \/*src*\/);\n+      __ ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));\n+      \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+      __ cbz(rscratch1, skip);\n+      __ blr(rscratch1);\n+\n+      __ bind(skip);\n+    }\n+    \/\/ At this point, r0 points to the value object (for interpreter or C1 caller).\n+    \/\/ The fields of the object are copied into registers (for C2 caller).\n+  }\n+\n@@ -474,1 +519,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -486,0 +531,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -532,3 +581,1 @@\n-        if (patch_code == lir_patch_none) {\n-          jobject2reg(c->as_jobject(), dest->as_register());\n-        } else {\n+        if (patch_code != lir_patch_none) {\n@@ -536,0 +583,2 @@\n+        } else {\n+          jobject2reg(c->as_jobject(), dest->as_register());\n@@ -645,0 +694,2 @@\n+    \/\/ Non-null case is not handled on aarch64 but handled on x86\n+    \/\/ FIXME: do we need to add it here?\n@@ -992,0 +1043,14 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, nullptr);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n@@ -1183,1 +1248,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->always_slow_path() ||\n@@ -1295,22 +1360,16 @@\n-  if (should_profile) {\n-    Register mdo  = klass_RInfo;\n-    __ mov_metadata(mdo, md->constant_encoding());\n-    Label not_null;\n-    __ cbnz(obj, not_null);\n-    \/\/ Object is null; update MDO and exit\n-    Address data_addr\n-      = __ form_address(rscratch2, mdo,\n-                        md->byte_offset_of_slot(data, DataLayout::flags_offset()),\n-                        0);\n-    __ ldrb(rscratch1, data_addr);\n-    __ orr(rscratch1, rscratch1, BitData::null_seen_byte_constant());\n-    __ strb(rscratch1, data_addr);\n-    __ b(*obj_is_null);\n-    __ bind(not_null);\n-\n-    Label update_done;\n-    Register recv = k_RInfo;\n-    __ load_klass(recv, obj);\n-    type_profile_helper(mdo, md, data, recv, &update_done);\n-    Address counter_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n-    __ addptr(counter_addr, DataLayout::counter_increment);\n+  if (op->need_null_check()) {\n+    if (should_profile) {\n+      Register mdo  = klass_RInfo;\n+      __ mov_metadata(mdo, md->constant_encoding());\n+      Label not_null;\n+      __ cbnz(obj, not_null);\n+      \/\/ Object is null; update MDO and exit\n+      Address data_addr\n+        = __ form_address(rscratch2, mdo,\n+                          md->byte_offset_of_slot(data, DataLayout::flags_offset()),\n+                          0);\n+      __ ldrb(rscratch1, data_addr);\n+      __ orr(rscratch1, rscratch1, BitData::null_seen_byte_constant());\n+      __ strb(rscratch1, data_addr);\n+      __ b(*obj_is_null);\n+      __ bind(not_null);\n@@ -1318,3 +1377,11 @@\n-    __ bind(update_done);\n-  } else {\n-    __ cbz(obj, *obj_is_null);\n+      Label update_done;\n+      Register recv = k_RInfo;\n+      __ load_klass(recv, obj);\n+      type_profile_helper(mdo, md, data, recv, &update_done);\n+      Address counter_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n+      __ addptr(counter_addr, DataLayout::counter_increment);\n+\n+      __ bind(update_done);\n+    } else {\n+      __ cbz(obj, *obj_is_null);\n+    }\n@@ -1331,0 +1398,1 @@\n+    assert(!k->is_loaded() || !k->is_obj_array_klass(), \"Use refined array for a direct pointer comparison\");\n@@ -1353,1 +1421,8 @@\n-        __ cmp(klass_RInfo, k_RInfo);\n+        if (k->is_loaded() && k->is_obj_array_klass()) {\n+          \/\/ For a direct pointer comparison, we need the refined array klass pointer\n+          ciKlass* k_refined = ciObjArrayKlass::make(k->as_obj_array_klass()->element_klass());\n+          __ mov_metadata(rscratch1, k_refined->constant_encoding());\n+          __ cmp(klass_RInfo, rscratch1);\n+        } else {\n+          __ cmp(klass_RInfo, k_RInfo);\n+        }\n@@ -1478,0 +1553,106 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing from\/to an array that *may* be a flat array (the\n+  \/\/ declared type is Object[], abstract[], interface[] or VT.ref[]).\n+  \/\/ If this array is a flat array, take the slow path.\n+  __ test_flat_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+  if (!op->value()->is_illegal()) {\n+    \/\/ The array is not a flat array, but it might be null-free. If we are storing\n+    \/\/ a null into a null-free array, take the slow path (which will throw NPE).\n+    Label skip;\n+    __ cbnz(op->value()->as_register(), skip);\n+    __ test_null_free_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+    __ bind(skip);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ We are storing into an array that *may* be null-free (the declared type is\n+  \/\/ Object[], abstract[], interface[] or VT.ref[]).\n+  Label test_mark_word;\n+  Register tmp = op->tmp()->as_register();\n+  __ ldr(tmp, Address(op->array()->as_register(), oopDesc::mark_offset_in_bytes()));\n+  __ tst(tmp, markWord::unlocked_value);\n+  __ br(Assembler::NE, test_mark_word);\n+  __ load_prototype_header(tmp, op->array()->as_register());\n+  __ bind(test_mark_word);\n+  __ tst(tmp, markWord::null_free_array_bit_in_place);\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmp(left, right);\n+  __ br(Assembler::EQ, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  {\n+    __ cbz(left, L_oops_not_equal);\n+    __ cbz(right, L_oops_not_equal);\n+  }\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Inline type check -- if either of the operands is not a inline type,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are inline type\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = op->tmp1()->as_register();\n+    __ mov(tmp1, markWord::inline_type_pattern);\n+    __ ldr(rscratch1, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, rscratch1);\n+    __ ldr(rscratch1, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, rscratch1);\n+    __ cmp(tmp1, (u1)markWord::inline_type_pattern);\n+    __ br(Assembler::NE, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ b(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedClassPointers) {\n+      __ ldrw(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldrw(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpw(left_klass_op, right_klass_op);\n+    } else {\n+      __ ldr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ ldr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmp(left_klass_op, right_klass_op);\n+    }\n+\n+    __ br(Assembler::EQ, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ b(L_end);\n+\n+  \/\/ We've returned from the stub. R0 contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+  __ cbz(r0, L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+}\n+\n+\n@@ -1991,1 +2172,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2002,1 +2183,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2165,0 +2346,12 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {\n+  if (null_check) {\n+    __ cbz(obj, *slow_path->entry());\n+  }\n+  if (is_dest) {\n+    __ test_null_free_array_oop(obj, tmp, *slow_path->entry());\n+    \/\/ TODO 8350865 Flat no longer implies null-free, so we need to check for flat dest. Can we do better here?\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  } else {\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  }\n+}\n@@ -2183,0 +2376,6 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ b(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n@@ -2236,0 +2435,8 @@\n+  \/\/ Handle inline type arrays\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false, (flags & LIR_OpArrayCopy::src_null_check));\n+  }\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true, (flags & LIR_OpArrayCopy::dst_null_check));\n+  }\n+\n@@ -2777,0 +2984,20 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Register obj = op->obj()->as_register();\n+  Register tmp = op->tmp()->as_pointer_register();\n+  bool not_null = op->not_null();\n+  int flag = op->flag();\n+\n+  Label not_inline_type;\n+  if (!not_null) {\n+    __ cbz(obj, not_inline_type);\n+  }\n+\n+  __ test_oop_is_not_inline_type(obj, tmp, not_inline_type);\n+\n+  Address mdo_addr = as_Address(op->mdp()->as_address_ptr(), rscratch2);\n+  __ ldrb(rscratch1, mdo_addr);\n+  __ orr(rscratch1, rscratch1, flag);\n+  __ strb(rscratch1, mdo_addr);\n+\n+  __ bind(not_inline_type);\n+}\n@@ -2916,0 +3143,4 @@\n+void LIR_Assembler::check_orig_pc() {\n+  __ ldr(rscratch2, frame_map()->address_for_orig_pc_addr());\n+  __ cmp(rscratch2, (u1)NULL_WORD);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":265,"deletions":34,"binary":false,"changes":299,"status":"modified"},{"patch":"@@ -30,0 +30,2 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n@@ -102,1 +104,4 @@\n-  if (UseCompactObjectHeaders) {\n+  if (UseCompactObjectHeaders || EnableValhalla) {\n+    \/\/ COH: Markword contains class pointer which is only known at runtime.\n+    \/\/ Valhalla: Could have value class which has a different prototype header to a normal object.\n+    \/\/ In both cases, we need to fetch dynamically.\n@@ -106,0 +111,1 @@\n+    \/\/ Otherwise: Can use the statically computed prototype header which is the same for every object.\n@@ -108,0 +114,5 @@\n+  }\n+\n+  if (!UseCompactObjectHeaders) {\n+    \/\/ COH: Markword already contains class pointer. Nothing else to do.\n+    \/\/ Otherwise: Fetch klass pointer following the markword\n@@ -244,2 +255,13 @@\n-void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes) {\n-  assert(bang_size_in_bytes >= framesize, \"stack bang size incorrect\");\n+void C1_MacroAssembler::build_frame_helper(int frame_size_in_bytes, int sp_offset_for_orig_pc, int sp_inc, bool reset_orig_pc, bool needs_stack_repair) {\n+  MacroAssembler::build_frame(frame_size_in_bytes);\n+\n+  if (needs_stack_repair) {\n+    save_stack_increment(sp_inc, frame_size_in_bytes);\n+  }\n+  if (reset_orig_pc) {\n+    \/\/ Zero orig_pc to detect deoptimization during buffering in the entry points\n+    str(zr, Address(sp, sp_offset_for_orig_pc));\n+  }\n+}\n+\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, bool needs_stack_repair, bool has_scalarized_args, Label* verified_inline_entry_label) {\n@@ -248,0 +270,1 @@\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n@@ -249,1 +272,2 @@\n-  MacroAssembler::build_frame(framesize);\n+\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, has_scalarized_args, needs_stack_repair);\n@@ -254,3 +278,4 @@\n-}\n-void C1_MacroAssembler::remove_frame(int framesize) {\n-  MacroAssembler::remove_frame(framesize);\n+  if (verified_inline_entry_label != nullptr) {\n+    \/\/ Jump here from the scalarized entry points that already created the frame.\n+    bind(*verified_inline_entry_label);\n+  }\n@@ -260,1 +285,0 @@\n-\n@@ -267,0 +291,1 @@\n+  if (C1Breakpoint) brk(1);\n@@ -269,0 +294,62 @@\n+int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label, bool is_inline_ro_entry) {\n+  assert(InlineTypePassFieldsAsArgs, \"sanity\");\n+  \/\/ Make sure there is enough stack space for this method's activation.\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+  generate_stack_overflow_check(bang_size_in_bytes);\n+\n+  GrowableArray<SigEntry>* sig    = ces->sig();\n+  GrowableArray<SigEntry>* sig_cc = is_inline_ro_entry ? ces->sig_cc_ro() : ces->sig_cc();\n+  VMRegPair* regs      = ces->regs();\n+  VMRegPair* regs_cc   = is_inline_ro_entry ? ces->regs_cc_ro() : ces->regs_cc();\n+  int args_on_stack    = ces->args_on_stack();\n+  int args_on_stack_cc = is_inline_ro_entry ? ces->args_on_stack_cc_ro() : ces->args_on_stack_cc();\n+\n+  assert(sig->length() <= sig_cc->length(), \"Zero-sized inline class not allowed!\");\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc->length());\n+  int args_passed = sig->length();\n+  int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);\n+\n+  \/\/ Create a temp frame so we can call into the runtime. It must be properly set up to accommodate GC.\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, true, ces->c1_needs_stack_repair());\n+\n+  \/\/ The runtime call might safepoint, make sure nmethod entry barrier is executed\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ C1 code is not hot enough to micro optimize the nmethod entry barrier with an out-of-line stub\n+  bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/, nullptr \/* guard *\/);\n+\n+  \/\/ FIXME -- call runtime only if we cannot in-line allocate all the incoming inline type args.\n+  mov(r19, (intptr_t) ces->method());\n+  if (is_inline_ro_entry) {\n+    far_call(RuntimeAddress(Runtime1::entry_for(StubId::c1_buffer_inline_args_no_receiver_id)));\n+  } else {\n+    far_call(RuntimeAddress(Runtime1::entry_for(StubId::c1_buffer_inline_args_id)));\n+  }\n+  int rt_call_offset = offset();\n+\n+  \/\/ The runtime call returns the new array in r20 instead of the usual r0\n+  \/\/ because r0 is also j_rarg7 which may be holding a live argument here.\n+  Register val_array = r20;\n+\n+  \/\/ Remove the temp frame\n+  MacroAssembler::remove_frame(frame_size_in_bytes);\n+\n+  \/\/ Check if we need to extend the stack for packing\n+  int sp_inc = 0;\n+  if (args_on_stack > args_on_stack_cc) {\n+    sp_inc = extend_stack_for_inline_args(args_on_stack);\n+  }\n+\n+  shuffle_inline_args(true, is_inline_ro_entry, sig_cc,\n+                      args_passed_cc, args_on_stack_cc, regs_cc, \/\/ from\n+                      args_passed, args_on_stack, regs,          \/\/ to\n+                      sp_inc, val_array);\n+\n+  \/\/ Create the real frame. Below jump will then skip over the stack banging and frame\n+  \/\/ setup code in the verified_inline_entry (which has a different real_frame_size).\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, sp_inc, false, ces->c1_needs_stack_repair());\n+\n+  b(verified_inline_entry_label);\n+  return rt_call_offset;\n+}\n+\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":95,"deletions":8,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/barrierSetRuntime.hpp\"\n@@ -52,0 +53,1 @@\n+\n@@ -89,0 +91,2 @@\n+  bool is_not_null = (decorators & IS_NOT_NULL) != 0;\n+\n@@ -92,5 +96,6 @@\n-    val = val == noreg ? zr : val;\n-      if (UseCompressedOops) {\n-        assert(!dst.uses(val), \"not enough registers\");\n-        if (val != zr) {\n-          __ encode_heap_oop(val);\n+      if (val == noreg) {\n+        assert(!is_not_null, \"inconsistent access\");\n+        if (UseCompressedOops) {\n+          __ strw(zr, dst);\n+        } else {\n+          __ str(zr, dst);\n@@ -99,2 +104,11 @@\n-        __ strw(val, dst);\n-        __ str(val, dst);\n+        if (UseCompressedOops) {\n+          assert(!dst.uses(val), \"not enough registers\");\n+          if (is_not_null) {\n+            __ encode_heap_oop_not_null(val);\n+          } else {\n+            __ encode_heap_oop(val);\n+          }\n+          __ strw(val, dst);\n+        } else {\n+          __ str(val, dst);\n+        }\n@@ -105,0 +119,1 @@\n+      assert(val != noreg, \"not supported\");\n@@ -125,0 +140,13 @@\n+void BarrierSetAssembler::flat_field_copy(MacroAssembler* masm, DecoratorSet decorators,\n+                                     Register src, Register dst, Register inline_layout_info) {\n+  \/\/ flat_field_copy implementation is fairly complex, and there are not any\n+  \/\/ \"short-cuts\" to be made from asm. What there is, appears to have the same\n+  \/\/ cost in C++, so just \"call_VM_leaf\" for now rather than maintain hundreds\n+  \/\/ of hand-rolled instructions...\n+  if (decorators & IS_DEST_UNINITIALIZED) {\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, BarrierSetRuntime::value_copy_is_dest_uninitialized), src, dst, inline_layout_info);\n+  } else {\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, BarrierSetRuntime::value_copy), src, dst, inline_layout_info);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.cpp","additions":35,"deletions":7,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"classfile\/symbolTable.hpp\"\n@@ -362,0 +363,79 @@\n+\n+const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j;\n+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;\n+\n+int SharedRuntime::java_return_convention(const BasicType *sig_bt, VMRegPair *regs, int total_args_passed) {\n+\n+  \/\/ Create the mapping between argument positions and registers.\n+\n+  static const Register INT_ArgReg[java_return_convention_max_int] = {\n+    r0 \/* j_rarg7 *\/, j_rarg6, j_rarg5, j_rarg4, j_rarg3, j_rarg2, j_rarg1, j_rarg0\n+  };\n+\n+  static const FloatRegister FP_ArgReg[java_return_convention_max_float] = {\n+    j_farg0, j_farg1, j_farg2, j_farg3, j_farg4, j_farg5, j_farg6, j_farg7\n+  };\n+\n+  uint int_args = 0;\n+  uint fp_args = 0;\n+\n+  for (int i = 0; i < total_args_passed; i++) {\n+    switch (sig_bt[i]) {\n+    case T_BOOLEAN:\n+    case T_CHAR:\n+    case T_BYTE:\n+    case T_SHORT:\n+    case T_INT:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_VOID:\n+      \/\/ halves of T_LONG or T_DOUBLE\n+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+      regs[i].set_bad();\n+      break;\n+    case T_LONG:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      \/\/ fall through\n+    case T_OBJECT:\n+    case T_ARRAY:\n+    case T_ADDRESS:\n+      \/\/ Should T_METADATA be added to java_calling_convention as well ?\n+    case T_METADATA:\n+      if (int_args < SharedRuntime::java_return_convention_max_int) {\n+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());\n+        int_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_FLOAT:\n+      if (fp_args < SharedRuntime::java_return_convention_max_float) {\n+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    case T_DOUBLE:\n+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+      if (fp_args < SharedRuntime::java_return_convention_max_float) {\n+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());\n+        fp_args ++;\n+      } else {\n+        return -1;\n+      }\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n+  }\n+\n+  return int_args + fp_args;\n+}\n+\n@@ -396,0 +476,107 @@\n+\/\/ For each inline type argument, sig includes the list of fields of\n+\/\/ the inline type. This utility function computes the number of\n+\/\/ arguments for the call if inline types are passed by reference (the\n+\/\/ calling convention the interpreter expects).\n+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {\n+  int total_args_passed = 0;\n+  if (InlineTypePassFieldsAsArgs) {\n+    for (int i = 0; i < sig_extended->length(); i++) {\n+      BasicType bt = sig_extended->at(i)._bt;\n+      if (bt == T_METADATA) {\n+        \/\/ In sig_extended, an inline type argument starts with:\n+        \/\/ T_METADATA, followed by the types of the fields of the\n+        \/\/ inline type and T_VOID to mark the end of the value\n+        \/\/ type. Inline types are flattened so, for instance, in the\n+        \/\/ case of an inline type with an int field and an inline type\n+        \/\/ field that itself has 2 fields, an int and a long:\n+        \/\/ T_METADATA T_INT T_METADATA T_INT T_LONG T_VOID (second\n+        \/\/ slot for the T_LONG) T_VOID (inner inline type) T_VOID\n+        \/\/ (outer inline type)\n+        total_args_passed++;\n+        int vt = 1;\n+        do {\n+          i++;\n+          BasicType bt = sig_extended->at(i)._bt;\n+          BasicType prev_bt = sig_extended->at(i-1)._bt;\n+          if (bt == T_METADATA) {\n+            vt++;\n+          } else if (bt == T_VOID &&\n+                     prev_bt != T_LONG &&\n+                     prev_bt != T_DOUBLE) {\n+            vt--;\n+          }\n+        } while (vt != 0);\n+      } else {\n+        total_args_passed++;\n+      }\n+    }\n+  } else {\n+    total_args_passed = sig_extended->length();\n+  }\n+  return total_args_passed;\n+}\n+\n+\n+static void gen_c2i_adapter_helper(MacroAssembler* masm,\n+                                   BasicType bt,\n+                                   BasicType prev_bt,\n+                                   size_t size_in_bytes,\n+                                   const VMRegPair& reg_pair,\n+                                   const Address& to,\n+                                   Register tmp1,\n+                                   Register tmp2,\n+                                   Register tmp3,\n+                                   int extraspace,\n+                                   bool is_oop) {\n+  if (bt == T_VOID) {\n+    assert(prev_bt == T_LONG || prev_bt == T_DOUBLE, \"missing half\");\n+    return;\n+  }\n+\n+  \/\/ Say 4 args:\n+  \/\/ i   st_off\n+  \/\/ 0   32 T_LONG\n+  \/\/ 1   24 T_VOID\n+  \/\/ 2   16 T_OBJECT\n+  \/\/ 3    8 T_BOOL\n+  \/\/ -    0 return address\n+  \/\/\n+  \/\/ However to make thing extra confusing. Because we can fit a Java long\/double in\n+  \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n+  \/\/ leaves one slot empty and only stores to a single slot. In this case the\n+  \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+\n+  bool wide = (size_in_bytes == wordSize);\n+  VMReg r_1 = reg_pair.first();\n+  VMReg r_2 = reg_pair.second();\n+  assert(r_2->is_valid() == wide, \"invalid size\");\n+  if (!r_1->is_valid()) {\n+    assert(!r_2->is_valid(), \"\");\n+    return;\n+  }\n+\n+  if (!r_1->is_FloatRegister()) {\n+    Register val = r25;\n+    if (r_1->is_stack()) {\n+      \/\/ memory to memory use r25 (scratch registers is used by store_heap_oop)\n+      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+      __ load_sized_value(val, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+    } else {\n+      val = r_1->as_Register();\n+    }\n+    assert_different_registers(to.base(), val, tmp1, tmp2, tmp3);\n+    if (is_oop) {\n+      __ store_heap_oop(to, val, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+    } else {\n+      __ store_sized_value(to, val, size_in_bytes);\n+    }\n+  } else {\n+    if (wide) {\n+      __ strd(r_1->as_FloatRegister(), to);\n+    } else {\n+      \/\/ only a float use just part of the slot\n+      __ strs(r_1->as_FloatRegister(), to);\n+    }\n+  }\n+}\n+\n@@ -397,3 +584,1 @@\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n+                            const GrowableArray<SigEntry>* sig_extended,\n@@ -401,1 +586,28 @@\n-                            Label& skip_fixup) {\n+                            bool requires_clinit_barrier,\n+                            address& c2i_no_clinit_check_entry,\n+                            Label& skip_fixup,\n+                            address start,\n+                            OopMapSet* oop_maps,\n+                            int& frame_complete,\n+                            int& frame_size_in_words,\n+                            bool alloc_inline_receiver) {\n+  if (requires_clinit_barrier && VM_Version::supports_fast_class_init_checks()) {\n+    Label L_skip_barrier;\n+\n+    { \/\/ Bypass the barrier for non-static methods\n+      __ ldrh(rscratch1, Address(rmethod, Method::access_flags_offset()));\n+      __ andsw(zr, rscratch1, JVM_ACC_STATIC);\n+      __ br(Assembler::EQ, L_skip_barrier); \/\/ non-static\n+    }\n+\n+    __ load_method_holder(rscratch2, rmethod);\n+    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n+    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+\n+    __ bind(L_skip_barrier);\n+    c2i_no_clinit_check_entry = __ pc();\n+  }\n+\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->c2i_entry_barrier(masm);\n+\n@@ -411,1 +623,25 @@\n-  int words_pushed = 0;\n+  \/\/ TODO 8366717 Is the comment about r13 correct? Isn't that r19_sender_sp?\n+  \/\/ Name some registers to be used in the following code. We can use\n+  \/\/ anything except r0-r7 which are arguments in the Java calling\n+  \/\/ convention, rmethod (r12), and r13 which holds the outgoing sender\n+  \/\/ SP for the interpreter.\n+  \/\/ TODO 8366717 We need to make sure that buf_array, buf_oop (and potentially other long-life regs) are kept live in slowpath runtime calls in GC barriers\n+  Register buf_array = r10;   \/\/ Array of buffered inline types\n+  Register buf_oop = r11;     \/\/ Buffered inline type oop\n+  Register tmp1 = r15;\n+  Register tmp2 = r16;\n+  Register tmp3 = r17;\n+\n+  if (InlineTypePassFieldsAsArgs) {\n+    \/\/ Is there an inline type argument?\n+    bool has_inline_argument = false;\n+    for (int i = 0; i < sig_extended->length() && !has_inline_argument; i++) {\n+      has_inline_argument = (sig_extended->at(i)._bt == T_METADATA);\n+    }\n+    if (has_inline_argument) {\n+      \/\/ There is at least an inline type argument: we're coming from\n+      \/\/ compiled code so we have no buffers to back the inline types\n+      \/\/ Allocate the buffers here with a runtime call.\n+      \/\/ TODO 8366717 Do we need to save vectors here? They could be used as arg registers, right? Same on x64.\n+      RegisterSaver reg_save(true \/* save_vectors *\/);\n+      OopMap* map = reg_save.save_live_registers(masm, 0, &frame_size_in_words);\n@@ -413,2 +649,2 @@\n-  \/\/ Since all args are passed on the stack, total_args_passed *\n-  \/\/ Interpreter::stackElementSize is the space we need.\n+      frame_complete = __ offset();\n+      address the_pc = __ pc();\n@@ -416,1 +652,2 @@\n-  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+      Label retaddr;\n+      __ set_last_Java_frame(sp, noreg, retaddr, rscratch1);\n@@ -418,1 +655,3 @@\n-  __ mov(r19_sender_sp, sp);\n+      __ mov(c_rarg0, rthread);\n+      __ mov(c_rarg1, rmethod);\n+      __ mov(c_rarg2, (int64_t)alloc_inline_receiver);\n@@ -420,2 +659,3 @@\n-  \/\/ stack is aligned, keep it that way\n-  extraspace = align_up(extraspace, 2*wordSize);\n+      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));\n+      __ blr(rscratch1);\n+      __ bind(retaddr);\n@@ -423,2 +663,2 @@\n-  if (extraspace)\n-    __ sub(sp, sp, extraspace);\n+      oop_maps->add_gc_map(__ pc() - start, map);\n+      __ reset_last_Java_frame(false);\n@@ -426,6 +666,1 @@\n-  \/\/ Now write the args into the outgoing interpreter space\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n+      reg_save.restore_live_registers(masm);\n@@ -433,16 +668,3 @@\n-    \/\/ offset to start parameters\n-    int st_off   = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n-\n-    \/\/ Say 4 args:\n-    \/\/ i   st_off\n-    \/\/ 0   32 T_LONG\n-    \/\/ 1   24 T_VOID\n-    \/\/ 2   16 T_OBJECT\n-    \/\/ 3    8 T_BOOL\n-    \/\/ -    0 return address\n-    \/\/\n-    \/\/ However to make thing extra confusing. Because we can fit a Java long\/double in\n-    \/\/ a single slot on a 64 bt vm and it would be silly to break them up, the interpreter\n-    \/\/ leaves one slot empty and only stores to a single slot. In this case the\n-    \/\/ slot that is occupied is the T_VOID slot. See I said it was confusing.\n+      Label no_exception;\n+      __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));\n+      __ cbz(rscratch1, no_exception);\n@@ -450,5 +672,9 @@\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n-    if (!r_1->is_valid()) {\n-      assert(!r_2->is_valid(), \"\");\n-      continue;\n+      __ str(zr, Address(rthread, JavaThread::vm_result_oop_offset()));\n+      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));\n+      __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+      __ bind(no_exception);\n+\n+      \/\/ We get an array of objects from the runtime call\n+      __ get_vm_result_oop(buf_array, rthread);\n+      __ get_vm_result_metadata(rmethod, rthread); \/\/ TODO: required to keep the callee Method live?\n@@ -456,9 +682,1 @@\n-    if (r_1->is_stack()) {\n-      \/\/ memory to memory use rscratch1\n-      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size\n-                    + extraspace\n-                    + words_pushed * wordSize);\n-      if (!r_2->is_valid()) {\n-        \/\/ sign extend??\n-        __ ldrw(rscratch1, Address(sp, ld_off));\n-        __ str(rscratch1, Address(sp, st_off));\n+  }\n@@ -466,1 +684,11 @@\n-      } else {\n+  \/\/ Since all args are passed on the stack, total_args_passed *\n+  \/\/ Interpreter::stackElementSize is the space we need.\n+\n+  int total_args_passed = compute_total_args_passed_int(sig_extended);\n+  int extraspace = total_args_passed * Interpreter::stackElementSize;\n+\n+  \/\/ stack is aligned, keep it that way\n+  extraspace = align_up(extraspace, StackAlignmentInBytes);\n+\n+  \/\/ set senderSP value\n+  __ mov(r19_sender_sp, sp);\n@@ -468,1 +696,1 @@\n-        __ ldr(rscratch1, Address(sp, ld_off));\n+  __ sub(sp, sp, extraspace);\n@@ -470,6 +698,26 @@\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ ld_off == LSW, ld_off+wordSize == MSW\n-          \/\/ st_off == MSW, next_off == LSW\n-          __ str(rscratch1, Address(sp, next_off));\n+  \/\/ Now write the args into the outgoing interpreter space\n+\n+  \/\/ next_arg_comp is the next argument from the compiler point of\n+  \/\/ view (inline type fields are passed in registers\/on the stack). In\n+  \/\/ sig_extended, an inline type argument starts with: T_METADATA,\n+  \/\/ followed by the types of the fields of the inline type and T_VOID\n+  \/\/ to mark the end of the inline type. ignored counts the number of\n+  \/\/ T_METADATA\/T_VOID. next_vt_arg is the next inline type argument:\n+  \/\/ used to get the buffer for that argument from the pool of buffers\n+  \/\/ we allocated above and want to pass to the\n+  \/\/ interpreter. next_arg_int is the next argument from the\n+  \/\/ interpreter point of view (inline types are passed by reference).\n+  for (int next_arg_comp = 0, ignored = 0, next_vt_arg = 0, next_arg_int = 0;\n+       next_arg_comp < sig_extended->length(); next_arg_comp++) {\n+    assert(ignored <= next_arg_comp, \"shouldn't skip over more slots than there are arguments\");\n+    assert(next_arg_int <= total_args_passed, \"more arguments for the interpreter than expected?\");\n+    BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+    int st_off = (total_args_passed - next_arg_int - 1) * Interpreter::stackElementSize;\n+    if (!InlineTypePassFieldsAsArgs || bt != T_METADATA) {\n+      int next_off = st_off - Interpreter::stackElementSize;\n+      const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;\n+      const VMRegPair reg_pair = regs[next_arg_comp-ignored];\n+      size_t size_in_bytes = reg_pair.second()->is_valid() ? 8 : 4;\n+      gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                             size_in_bytes, reg_pair, Address(sp, offset), tmp1, tmp2, tmp3, extraspace, false);\n+      next_arg_int++;\n@@ -477,7 +725,4 @@\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaaaull);\n-          __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        } else {\n-          __ str(rscratch1, Address(sp, st_off));\n-        }\n+      if (bt == T_LONG || bt == T_DOUBLE) {\n+        \/\/ Overwrite the unused slot with known junk\n+        __ mov(rscratch1, CONST64(0xdeadffffdeadaaaa));\n+        __ str(rscratch1, Address(sp, st_off));\n@@ -485,16 +730,24 @@\n-    } else if (r_1->is_Register()) {\n-      Register r = r_1->as_Register();\n-      if (!r_2->is_valid()) {\n-        \/\/ must be only an int (or less ) so move only 32bits to slot\n-        \/\/ why not sign extend??\n-        __ str(r, Address(sp, st_off));\n-      } else {\n-        \/\/ Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG\n-        \/\/ T_DOUBLE and T_LONG use two slots in the interpreter\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n-          \/\/ jlong\/double in gpr\n-#ifdef ASSERT\n-          \/\/ Overwrite the unused slot with known junk\n-          __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaabull);\n-          __ str(rscratch1, Address(sp, st_off));\n-          __ str(r, Address(sp, next_off));\n+    } else {\n+      ignored++;\n+      \/\/ get the buffer from the just allocated pool of buffers\n+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_OBJECT);\n+      __ load_heap_oop(buf_oop, Address(buf_array, index), tmp1, tmp2);\n+      next_vt_arg++; next_arg_int++;\n+      int vt = 1;\n+      \/\/ write fields we get from compiled code in registers\/stack\n+      \/\/ slots to the buffer: we know we are done with that inline type\n+      \/\/ argument when we hit the T_VOID that acts as an end of inline\n+      \/\/ type delimiter for this inline type. Inline types are flattened\n+      \/\/ so we might encounter embedded inline types. Each entry in\n+      \/\/ sig_extended contains a field offset in the buffer.\n+      Label L_null;\n+      do {\n+        next_arg_comp++;\n+        BasicType bt = sig_extended->at(next_arg_comp)._bt;\n+        BasicType prev_bt = sig_extended->at(next_arg_comp - 1)._bt;\n+        if (bt == T_METADATA) {\n+          vt++;\n+          ignored++;\n+        } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {\n+          vt--;\n+          ignored++;\n@@ -503,1 +756,22 @@\n-          __ str(r, Address(sp, st_off));\n+          int off = sig_extended->at(next_arg_comp)._offset;\n+          if (off == -1) {\n+            \/\/ Nullable inline type argument, emit null check\n+            VMReg reg = regs[next_arg_comp-ignored].first();\n+            Label L_notNull;\n+            if (reg->is_stack()) {\n+              int ld_off = reg->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n+              __ ldrb(tmp1, Address(sp, ld_off));\n+              __ cbnz(tmp1, L_notNull);\n+            } else {\n+              __ cbnz(reg->as_Register(), L_notNull);\n+            }\n+            __ str(zr, Address(sp, st_off));\n+            __ b(L_null);\n+            __ bind(L_notNull);\n+            continue;\n+          }\n+          assert(off > 0, \"offset in object should be positive\");\n+          size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+          bool is_oop = is_reference_type(bt);\n+          gen_c2i_adapter_helper(masm, bt, next_arg_comp > 0 ? sig_extended->at(next_arg_comp-1)._bt : T_ILLEGAL,\n+                                 size_in_bytes, regs[next_arg_comp-ignored], Address(buf_oop, off), tmp1, tmp2, tmp3, extraspace, is_oop);\n@@ -505,14 +779,4 @@\n-      }\n-    } else {\n-      assert(r_1->is_FloatRegister(), \"\");\n-      if (!r_2->is_valid()) {\n-        \/\/ only a float use just part of the slot\n-        __ strs(r_1->as_FloatRegister(), Address(sp, st_off));\n-      } else {\n-#ifdef ASSERT\n-        \/\/ Overwrite the unused slot with known junk\n-        __ mov(rscratch1, (uint64_t)0xdeadffffdeadaaacull);\n-        __ str(rscratch1, Address(sp, st_off));\n-#endif \/* ASSERT *\/\n-        __ strd(r_1->as_FloatRegister(), Address(sp, next_off));\n-      }\n+      } while (vt != 0);\n+      \/\/ pass the buffer to the interpreter\n+      __ str(buf_oop, Address(sp, st_off));\n+      __ bind(L_null);\n@@ -528,0 +792,1 @@\n+void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm, int comp_args_on_stack, const GrowableArray<SigEntry>* sig, const VMRegPair *regs) {\n@@ -529,5 +794,0 @@\n-void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,\n-                                    int total_args_passed,\n-                                    int comp_args_on_stack,\n-                                    const BasicType *sig_bt,\n-                                    const VMRegPair *regs) {\n@@ -562,1 +822,1 @@\n-  int comp_words_on_stack = align_up(comp_args_on_stack*VMRegImpl::stack_slot_size, wordSize)>>LogBytesPerWord;\n+  int comp_words_on_stack = 0;\n@@ -564,2 +824,3 @@\n-    __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n-    __ andr(sp, rscratch1, -16);\n+     comp_words_on_stack = align_up(comp_args_on_stack * VMRegImpl::stack_slot_size, wordSize) >> LogBytesPerWord;\n+     __ sub(rscratch1, sp, comp_words_on_stack * wordSize);\n+     __ andr(sp, rscratch1, -16);\n@@ -570,1 +831,1 @@\n-  __ ldr(rscratch1, Address(rmethod, in_bytes(Method::from_compiled_offset())));\n+  __ ldr(rscratch1, Address(rmethod, in_bytes(Method::from_compiled_inline_offset())));\n@@ -584,0 +845,2 @@\n+  int total_args_passed = sig->length();\n+\n@@ -586,2 +849,3 @@\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n+    BasicType bt = sig->at(i)._bt;\n+    if (bt == T_VOID) {\n+      assert(i > 0 && (sig->at(i - 1)._bt == T_LONG || sig->at(i - 1)._bt == T_DOUBLE), \"missing half\");\n@@ -592,0 +856,1 @@\n+    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(), \"scrambled load targets?\");\n@@ -593,3 +858,1 @@\n-    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(),\n-            \"scrambled load targets?\");\n-    int ld_off = (total_args_passed - i - 1)*Interpreter::stackElementSize;\n+    int ld_off = (total_args_passed - i - 1) * Interpreter::stackElementSize;\n@@ -610,1 +873,1 @@\n-      int st_off = regs[i].first()->reg2stack()*VMRegImpl::stack_slot_size;\n+      int st_off = regs[i].first()->reg2stack() * VMRegImpl::stack_slot_size;\n@@ -627,2 +890,1 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n@@ -631,11 +893,28 @@\n-        __ str(rscratch2, Address(sp, st_off));\n-      }\n-    } else if (r_1->is_Register()) {  \/\/ Register argument\n-      Register r = r_1->as_Register();\n-      if (r_2->is_valid()) {\n-        \/\/\n-        \/\/ We are using two VMRegs. This can be either T_OBJECT,\n-        \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n-        \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n-        \/\/ So we must adjust where to pick up the data to match the\n-        \/\/ interpreter.\n+         __ str(rscratch2, Address(sp, st_off));\n+       }\n+     } else if (r_1->is_Register()) {  \/\/ Register argument\n+       Register r = r_1->as_Register();\n+       if (r_2->is_valid()) {\n+         \/\/\n+         \/\/ We are using two VMRegs. This can be either T_OBJECT,\n+         \/\/ T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates\n+         \/\/ two slots but only uses one for thr T_LONG or T_DOUBLE case\n+         \/\/ So we must adjust where to pick up the data to match the\n+         \/\/ interpreter.\n+\n+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;\n+\n+         \/\/ this can be a misaligned move\n+         __ ldr(r, Address(esp, offset));\n+       } else {\n+         \/\/ sign extend and use a full word?\n+         __ ldrw(r, Address(esp, ld_off));\n+       }\n+     } else {\n+       if (!r_2->is_valid()) {\n+         __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n+       } else {\n+         __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n+       }\n+     }\n+   }\n@@ -643,17 +922,0 @@\n-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?\n-                           next_off : ld_off;\n-\n-        \/\/ this can be a misaligned move\n-        __ ldr(r, Address(esp, offset));\n-      } else {\n-        \/\/ sign extend and use a full word?\n-        __ ldrw(r, Address(esp, ld_off));\n-      }\n-    } else {\n-      if (!r_2->is_valid()) {\n-        __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));\n-      } else {\n-        __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));\n-      }\n-    }\n-  }\n@@ -676,1 +938,0 @@\n-\n@@ -680,8 +941,4 @@\n-\/\/ ---------------------------------------------------------------\n-void SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n-                                            int total_args_passed,\n-                                            int comp_args_on_stack,\n-                                            const BasicType *sig_bt,\n-                                            const VMRegPair *regs,\n-                                            address entry_address[AdapterBlob::ENTRY_COUNT]) {\n-  entry_address[AdapterBlob::I2C] = __ pc();\n+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {\n+  Register data = rscratch2;\n+  __ ic_check(1 \/* end_alignment *\/);\n+  __ ldr(rmethod, Address(data, CompiledICData::speculated_method_offset()));\n@@ -689,1 +946,7 @@\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n+  \/\/ Method might have been compiled since the call site was patched to\n+  \/\/ interpreted; if that is the case treat it as a miss so we can get\n+  \/\/ the call site corrected.\n+  __ ldr(rscratch1, Address(rmethod, in_bytes(Method::code_offset())));\n+  __ cbz(rscratch1, skip_fixup);\n+  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+}\n@@ -691,2 +954,12 @@\n-  entry_address[AdapterBlob::C2I_Unverified] = __ pc();\n-  Label skip_fixup;\n+\/\/ ---------------------------------------------------------------\n+void SharedRuntime::generate_i2c2i_adapters(MacroAssembler* masm,\n+                                            int comp_args_on_stack,\n+                                            const GrowableArray<SigEntry>* sig,\n+                                            const VMRegPair* regs,\n+                                            const GrowableArray<SigEntry>* sig_cc,\n+                                            const VMRegPair* regs_cc,\n+                                            const GrowableArray<SigEntry>* sig_cc_ro,\n+                                            const VMRegPair* regs_cc_ro,\n+                                            address entry_address[AdapterBlob::ENTRY_COUNT],\n+                                            AdapterBlob*& new_adapter,\n+                                            bool allocate_code_blob) {\n@@ -694,3 +967,2 @@\n-  Register data = rscratch2;\n-  Register receiver = j_rarg0;\n-  Register tmp = r10;  \/\/ A call-clobbered register not used for arg passing\n+  entry_address[AdapterBlob::I2C] = __ pc();\n+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);\n@@ -707,7 +979,3 @@\n-  {\n-    __ block_comment(\"c2i_unverified_entry {\");\n-    \/\/ Method might have been compiled since the call site was patched to\n-    \/\/ interpreted; if that is the case treat it as a miss so we can get\n-    \/\/ the call site corrected.\n-    __ ic_check(1 \/* end_alignment *\/);\n-    __ ldr(rmethod, Address(data, CompiledICData::speculated_method_offset()));\n+  entry_address[AdapterBlob::C2I_Unverified] = __ pc();\n+  entry_address[AdapterBlob::C2I_Unverified_Inline] = __ pc();\n+  Label skip_fixup;\n@@ -715,5 +983,1 @@\n-    __ ldr(rscratch1, Address(rmethod, in_bytes(Method::code_offset())));\n-    __ cbz(rscratch1, skip_fixup);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-    __ block_comment(\"} c2i_unverified_entry\");\n-  }\n+  gen_inline_cache_check(masm, skip_fixup);\n@@ -721,1 +985,3 @@\n-  entry_address[AdapterBlob::C2I] = __ pc();\n+  OopMapSet* oop_maps = new OopMapSet();\n+  int frame_complete = CodeOffsets::frame_never_safe;\n+  int frame_size_in_words = 0;\n@@ -723,1 +989,1 @@\n-  \/\/ Class initialization barrier for static methods\n+  \/\/ Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)\n@@ -725,15 +991,33 @@\n-  if (VM_Version::supports_fast_class_init_checks()) {\n-    Label L_skip_barrier;\n-\n-    { \/\/ Bypass the barrier for non-static methods\n-      __ ldrh(rscratch1, Address(rmethod, Method::access_flags_offset()));\n-      __ andsw(zr, rscratch1, JVM_ACC_STATIC);\n-      __ br(Assembler::EQ, L_skip_barrier); \/\/ non-static\n-    }\n-\n-    __ load_method_holder(rscratch2, rmethod);\n-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n-\n-    __ bind(L_skip_barrier);\n-    entry_address[AdapterBlob::C2I_No_Clinit_Check] = __ pc();\n+  entry_address[AdapterBlob::C2I_Inline_RO] = __ pc();\n+  if (regs_cc != regs_cc_ro) {\n+    \/\/ No class init barrier needed because method is guaranteed to be non-static\n+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, \/* requires_clinit_barrier = *\/ false, entry_address[AdapterBlob::C2I_No_Clinit_Check],\n+                    skip_fixup, entry_address[AdapterBlob::I2C], oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n+    skip_fixup.reset();\n+  }\n+\n+  \/\/ Scalarized c2i adapter\n+  entry_address[AdapterBlob::C2I]        = __ pc();\n+  entry_address[AdapterBlob::C2I_Inline] = __ pc();\n+  gen_c2i_adapter(masm, sig_cc, regs_cc, \/* requires_clinit_barrier = *\/ true, entry_address[AdapterBlob::C2I_No_Clinit_Check],\n+                  skip_fixup, entry_address[AdapterBlob::I2C], oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ true);\n+\n+  \/\/ Non-scalarized c2i adapter\n+  if (regs != regs_cc) {\n+    entry_address[AdapterBlob::C2I_Unverified_Inline] = __ pc();\n+    Label inline_entry_skip_fixup;\n+    gen_inline_cache_check(masm, inline_entry_skip_fixup);\n+\n+    entry_address[AdapterBlob::C2I_Inline] = __ pc();\n+    gen_c2i_adapter(masm, sig, regs, \/* requires_clinit_barrier = *\/ true, entry_address[AdapterBlob::C2I_No_Clinit_Check],\n+                    inline_entry_skip_fixup, entry_address[AdapterBlob::I2C], oop_maps, frame_complete, frame_size_in_words, \/* alloc_inline_receiver = *\/ false);\n+  }\n+\n+  \/\/ The c2i adapters might safepoint and trigger a GC. The caller must make sure that\n+  \/\/ the GC knows about the location of oop argument locations passed to the c2i adapter.\n+  if (allocate_code_blob) {\n+    bool caller_must_gc_arguments = (regs != regs_cc);\n+    int entry_offset[AdapterHandlerEntry::ENTRIES_COUNT];\n+    assert(AdapterHandlerEntry::ENTRIES_COUNT == 7, \"sanity\");\n+    AdapterHandlerLibrary::address_to_offset(entry_address, entry_offset);\n+    new_adapter = AdapterBlob::create(masm->code(), entry_offset, frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);\n@@ -741,6 +1025,0 @@\n-\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->c2i_entry_barrier(masm);\n-\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n-  return;\n@@ -2754,0 +3032,143 @@\n+BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {\n+  BufferBlob* buf = BufferBlob::create(\"inline types pack\/unpack\", 16 * K);\n+  if (buf == nullptr) {\n+    return nullptr;\n+  }\n+  CodeBuffer buffer(buf);\n+  short buffer_locs[20];\n+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,\n+                                         sizeof(buffer_locs)\/sizeof(relocInfo));\n+\n+  MacroAssembler _masm(&buffer);\n+  MacroAssembler* masm = &_masm;\n+\n+  const Array<SigEntry>* sig_vk = vk->extended_sig();\n+  const Array<VMRegPair>* regs = vk->return_regs();\n+\n+  int pack_fields_jobject_off = __ offset();\n+  \/\/ Resolve pre-allocated buffer from JNI handle.\n+  \/\/ We cannot do this in generate_call_stub() because it requires GC code to be initialized.\n+  Register Rresult = r14;  \/\/ See StubGenerator::generate_call_stub().\n+  __ ldr(r0, Address(Rresult));\n+  __ resolve_jobject(r0 \/* value *\/,\n+                     rthread \/* thread *\/,\n+                     r12 \/* tmp *\/);\n+  __ str(r0, Address(Rresult));\n+\n+  int pack_fields_off = __ offset();\n+\n+  int j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address to(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ strs(r_1->as_FloatRegister(), to);\n+    } else if (bt == T_DOUBLE) {\n+      __ strd(r_1->as_FloatRegister(), to);\n+    } else {\n+      Register val = r_1->as_Register();\n+      assert_different_registers(to.base(), val, r15, r16, r17);\n+      if (is_reference_type(bt)) {\n+        __ store_heap_oop(to, val, r15, r16, r17, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        __ store_sized_value(to, r_1->as_Register(), type2aelembytes(bt));\n+      }\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+  if (vk->has_nullable_atomic_layout()) {\n+    \/\/ Zero the null marker (setting it to 1 would be better but would require an additional register)\n+    __ strb(zr, Address(r0, vk->null_marker_offset()));\n+  }\n+  __ ret(lr);\n+\n+  int unpack_fields_off = __ offset();\n+\n+  Label skip;\n+  Label not_null;\n+  __ cbnz(r0, not_null);\n+\n+  \/\/ Return value is null. Zero oop registers to make the GC happy.\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    if (bt == T_OBJECT || bt == T_ARRAY) {\n+      VMRegPair pair = regs->at(j);\n+      VMReg r_1 = pair.first();\n+      __ mov(r_1->as_Register(), zr);\n+    }\n+    j++;\n+  }\n+  __ b(skip);\n+  __ bind(not_null);\n+\n+  j = 1;\n+  for (int i = 0; i < sig_vk->length(); i++) {\n+    BasicType bt = sig_vk->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      continue;\n+    }\n+    if (bt == T_VOID) {\n+      if (sig_vk->at(i-1)._bt == T_LONG ||\n+          sig_vk->at(i-1)._bt == T_DOUBLE) {\n+        j++;\n+      }\n+      continue;\n+    }\n+    int off = sig_vk->at(i)._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    VMRegPair pair = regs->at(j);\n+    VMReg r_1 = pair.first();\n+    VMReg r_2 = pair.second();\n+    Address from(r0, off);\n+    if (bt == T_FLOAT) {\n+      __ ldrs(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_DOUBLE) {\n+      __ ldrd(r_1->as_FloatRegister(), from);\n+    } else if (bt == T_OBJECT || bt == T_ARRAY) {\n+      assert_different_registers(r0, r_1->as_Register());\n+      __ load_heap_oop(r_1->as_Register(), from, rscratch1, rscratch2);\n+    } else {\n+      assert(is_java_primitive(bt), \"unexpected basic type\");\n+      assert_different_registers(r0, r_1->as_Register());\n+      size_t size_in_bytes = type2aelembytes(bt);\n+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);\n+    }\n+    j++;\n+  }\n+  assert(j == regs->length(), \"missed a field?\");\n+\n+  __ bind(skip);\n+\n+  __ ret(lr);\n+\n+  __ flush();\n+\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":611,"deletions":190,"binary":false,"changes":801,"status":"modified"},{"patch":"@@ -2554,0 +2554,4 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3126,0 +3126,3 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -3056,0 +3056,4 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"oops\/objArrayKlass.hpp\"\n@@ -118,0 +119,73 @@\n+\/\/ Implementation of LoadFlattenedArrayStub\n+\n+LoadFlattenedArrayStub::LoadFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr result, CodeEmitInfo* info) {\n+  _array = array;\n+  _index = index;\n+  _result = result;\n+  \/\/ Tell the register allocator that the runtime call will scratch rax.\n+  _scratch_reg = FrameMap::rax_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void LoadFlattenedArrayStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_array->as_register(), 1);\n+  ce->store_parameter(_index->as_register(), 0);\n+  __ call(RuntimeAddress(Runtime1::entry_for(StubId::c1_load_flat_array_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  if (_result->as_register() != rax) {\n+    __ movptr(_result->as_register(), rax);\n+  }\n+  __ jmp(_continuation);\n+}\n+\n+\n+\/\/ Implementation of StoreFlattenedArrayStub\n+\n+StoreFlattenedArrayStub::StoreFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr value, CodeEmitInfo* info) {\n+  _array = array;\n+  _index = index;\n+  _value = value;\n+  \/\/ Tell the register allocator that the runtime call will scratch rax.\n+  _scratch_reg = FrameMap::rax_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+\n+void StoreFlattenedArrayStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_array->as_register(), 2);\n+  ce->store_parameter(_index->as_register(), 1);\n+  ce->store_parameter(_value->as_register(), 0);\n+  __ call(RuntimeAddress(Runtime1::entry_for(StubId::c1_store_flat_array_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  __ jmp(_continuation);\n+}\n+\n+\n+\/\/ Implementation of SubstitutabilityCheckStub\n+\n+SubstitutabilityCheckStub::SubstitutabilityCheckStub(LIR_Opr left, LIR_Opr right, CodeEmitInfo* info) {\n+  _left = left;\n+  _right = right;\n+  \/\/ Tell the register allocator that the runtime call will scratch rax.\n+  _scratch_reg = FrameMap::rax_oop_opr;\n+  _info = new CodeEmitInfo(info);\n+}\n+\n+void SubstitutabilityCheckStub::emit_code(LIR_Assembler* ce) {\n+  assert(__ rsp_offset() == 0, \"frame size should be fixed\");\n+  __ bind(_entry);\n+  ce->store_parameter(_left->as_register(), 1);\n+  ce->store_parameter(_right->as_register(), 0);\n+  __ call(RuntimeAddress(Runtime1::entry_for(StubId::c1_substitutability_check_id)));\n+  ce->add_call_info_here(_info);\n+  ce->verify_oop_map(_info);\n+  __ jmp(_continuation);\n+}\n+\n+\n@@ -170,1 +244,2 @@\n-NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info) {\n+NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result,\n+                                       CodeEmitInfo* info, bool is_null_free) {\n@@ -175,0 +250,1 @@\n+  _is_null_free = is_null_free;\n@@ -183,1 +259,5 @@\n-  __ call(RuntimeAddress(Runtime1::entry_for(StubId::c1_new_object_array_id)));\n+  if (_is_null_free) {\n+    __ call(RuntimeAddress(Runtime1::entry_for(StubId::c1_new_null_free_array_id)));\n+  } else {\n+    __ call(RuntimeAddress(Runtime1::entry_for(StubId::c1_new_object_array_id)));\n+  }\n@@ -193,0 +273,9 @@\n+  if (_throw_ie_stub != nullptr) {\n+    \/\/ When we come here, _obj_reg has already been checked to be non-null.\n+    const int is_value_mask = markWord::inline_type_pattern;\n+    Register mark = _scratch_reg->as_register();\n+    __ movptr(mark, Address(_obj_reg->as_register(), oopDesc::mark_offset_in_bytes()));\n+    __ andptr(mark, is_value_mask);\n+    __ cmpl(mark, is_value_mask);\n+    __ jcc(Assembler::equal, *_throw_ie_stub->entry());\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c1_CodeStubs_x86.cpp","additions":91,"deletions":2,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"ci\/ciObjArrayKlass.hpp\"\n@@ -39,0 +41,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -431,1 +434,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -468,0 +471,44 @@\n+  if (InlineTypeReturnedAsFields) {\n+  #ifndef _LP64\n+     Unimplemented();\n+  #endif\n+    \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+    ciType* return_type = compilation()->method()->return_type();\n+    if (return_type->is_inlinetype()) {\n+      ciInlineKlass* vk = return_type->as_inline_klass();\n+      if (vk->can_be_returned_as_fields()) {\n+        address unpack_handler = vk->unpack_handler();\n+        assert(unpack_handler != nullptr, \"must be\");\n+        __ call(RuntimeAddress(unpack_handler));\n+      }\n+    } else if (return_type->is_instance_klass() && (!return_type->is_loaded() || StressCallingConvention)) {\n+      Label skip;\n+      Label not_null;\n+      __ testptr(rax, rax);\n+      __ jcc(Assembler::notZero, not_null);\n+      \/\/ Returned value is null, zero all return registers because they may belong to oop fields\n+      __ xorq(j_rarg1, j_rarg1);\n+      __ xorq(j_rarg2, j_rarg2);\n+      __ xorq(j_rarg3, j_rarg3);\n+      __ xorq(j_rarg4, j_rarg4);\n+      __ xorq(j_rarg5, j_rarg5);\n+      __ jmp(skip);\n+      __ bind(not_null);\n+\n+      \/\/ Check if we are returning an non-null inline type and load its fields into registers\n+      __ test_oop_is_not_inline_type(rax, rscratch1, skip, \/* can_be_null= *\/ false);\n+\n+      \/\/ Load fields from a buffered value with an inline class specific handler\n+      __ load_klass(rdi, rax, rscratch1);\n+      __ movptr(rdi, Address(rdi, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      __ movptr(rdi, Address(rdi, InlineKlass::unpack_handler_offset()));\n+      \/\/ Unpack handler can be null if inline type is not scalarizable in returns\n+      __ testptr(rdi, rdi);\n+      __ jcc(Assembler::zero, skip);\n+      __ call(rdi);\n+\n+      __ bind(skip);\n+    }\n+    \/\/ At this point, rax points to the value object (for interpreter or C1 caller).\n+    \/\/ The fields of the object are copied into registers (for C2 caller).\n+  }\n@@ -470,1 +517,1 @@\n-  __ remove_frame(initial_frame_size_in_bytes());\n+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n@@ -486,0 +533,4 @@\n+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {\n+  return (__ store_inline_type_fields_to_buf(vk, false));\n+}\n+\n@@ -1224,1 +1275,1 @@\n-  if (UseSlowPath ||\n+  if (UseSlowPath || op->always_slow_path() ||\n@@ -1323,24 +1374,26 @@\n-  __ testptr(obj, obj);\n-  if (op->should_profile()) {\n-    Label not_null;\n-    Register mdo  = klass_RInfo;\n-    __ mov_metadata(mdo, md->constant_encoding());\n-    __ jccb(Assembler::notEqual, not_null);\n-    \/\/ Object is null; update MDO and exit\n-    Address data_addr(mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n-    int header_bits = BitData::null_seen_byte_constant();\n-    __ orb(data_addr, header_bits);\n-    __ jmp(*obj_is_null);\n-    __ bind(not_null);\n-\n-    Label update_done;\n-    Register recv = k_RInfo;\n-    __ load_klass(recv, obj, tmp_load_klass);\n-    type_profile_helper(mdo, md, data, recv, &update_done);\n-\n-    Address nonprofiled_receiver_count_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n-    __ addptr(nonprofiled_receiver_count_addr, DataLayout::counter_increment);\n-\n-    __ bind(update_done);\n-  } else {\n-    __ jcc(Assembler::equal, *obj_is_null);\n+  if (op->need_null_check()) {\n+    __ testptr(obj, obj);\n+    if (op->should_profile()) {\n+      Label not_null;\n+      Register mdo  = klass_RInfo;\n+      __ mov_metadata(mdo, md->constant_encoding());\n+      __ jccb(Assembler::notEqual, not_null);\n+      \/\/ Object is null; update MDO and exit\n+      Address data_addr(mdo, md->byte_offset_of_slot(data, DataLayout::flags_offset()));\n+      int header_bits = BitData::null_seen_byte_constant();\n+      __ orb(data_addr, header_bits);\n+      __ jmp(*obj_is_null);\n+      __ bind(not_null);\n+\n+      Label update_done;\n+      Register recv = k_RInfo;\n+      __ load_klass(recv, obj, tmp_load_klass);\n+      type_profile_helper(mdo, md, data, recv, &update_done);\n+\n+      Address nonprofiled_receiver_count_addr(mdo, md->byte_offset_of_slot(data, CounterData::count_offset()));\n+      __ addptr(nonprofiled_receiver_count_addr, DataLayout::counter_increment);\n+\n+      __ bind(update_done);\n+    } else {\n+      __ jcc(Assembler::equal, *obj_is_null);\n+    }\n@@ -1357,0 +1410,1 @@\n+    assert(!k->is_loaded() || !k->is_obj_array_klass(), \"Use refined array for a direct pointer comparison\");\n@@ -1381,1 +1435,8 @@\n-        __ cmpptr(klass_RInfo, k_RInfo);\n+        if (k->is_loaded() && k->is_obj_array_klass()) {\n+          \/\/ For a direct pointer comparison, we need the refined array klass pointer\n+          ciKlass* k_refined = ciObjArrayKlass::make(k->as_obj_array_klass()->element_klass());\n+          __ mov_metadata(tmp_load_klass, k_refined->constant_encoding());\n+          __ cmpptr(klass_RInfo, tmp_load_klass);\n+        } else {\n+          __ cmpptr(klass_RInfo, k_RInfo);\n+        }\n@@ -1516,0 +1577,103 @@\n+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {\n+  \/\/ We are loading\/storing from\/to an array that *may* be a flat array (the\n+  \/\/ declared type is Object[], abstract[], interface[] or VT.ref[]).\n+  \/\/ If this array is a flat array, take the slow path.\n+  __ test_flat_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+  if (!op->value()->is_illegal()) {\n+    \/\/ TODO 8350865 This is also used for profiling code, right? And in that case we don't care about null but just want to know if the array is flat or not.\n+    \/\/ The array is not a flat array, but it might be null-free. If we are storing\n+    \/\/ a null into a null-free array, take the slow path (which will throw NPE).\n+    Label skip;\n+    __ cmpptr(op->value()->as_register(), NULL_WORD);\n+    __ jcc(Assembler::notEqual, skip);\n+    __ test_null_free_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+    __ bind(skip);\n+  }\n+}\n+\n+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {\n+  \/\/ We are storing into an array that *may* be null-free (the declared type is\n+  \/\/ Object[], abstract[], interface[] or VT.ref[]).\n+  Label test_mark_word;\n+  Register tmp = op->tmp()->as_register();\n+  __ movptr(tmp, Address(op->array()->as_register(), oopDesc::mark_offset_in_bytes()));\n+  __ testl(tmp, markWord::unlocked_value);\n+  __ jccb(Assembler::notZero, test_mark_word);\n+  __ load_prototype_header(tmp, op->array()->as_register(), rscratch1);\n+  __ bind(test_mark_word);\n+  __ testl(tmp, markWord::null_free_array_bit_in_place);\n+}\n+\n+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {\n+  Label L_oops_equal;\n+  Label L_oops_not_equal;\n+  Label L_end;\n+\n+  Register left  = op->left()->as_register();\n+  Register right = op->right()->as_register();\n+\n+  __ cmpptr(left, right);\n+  __ jcc(Assembler::equal, L_oops_equal);\n+\n+  \/\/ (1) Null check -- if one of the operands is null, the other must not be null (because\n+  \/\/     the two references are not equal), so they are not substitutable,\n+  \/\/     FIXME: do null check only if the operand is nullable\n+  __ testptr(left, right);\n+  __ jcc(Assembler::zero, L_oops_not_equal);\n+\n+  ciKlass* left_klass = op->left_klass();\n+  ciKlass* right_klass = op->right_klass();\n+\n+  \/\/ (2) Inline type check -- if either of the operands is not a inline type,\n+  \/\/     they are not substitutable. We do this only if we are not sure that the\n+  \/\/     operands are inline type\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    Register tmp1  = op->tmp1()->as_register();\n+    __ movptr(tmp1, (intptr_t)markWord::inline_type_pattern);\n+    __ andptr(tmp1, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andptr(tmp1, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ cmpptr(tmp1, (intptr_t)markWord::inline_type_pattern);\n+    __ jcc(Assembler::notEqual, L_oops_not_equal);\n+  }\n+\n+  \/\/ (3) Same klass check: if the operands are of different klasses, they are not substitutable.\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+    __ jmp(*op->stub()->entry());\n+  } else {\n+    Register left_klass_op = op->left_klass_op()->as_register();\n+    Register right_klass_op = op->right_klass_op()->as_register();\n+\n+    if (UseCompressedClassPointers) {\n+      __ movl(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ movl(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpl(left_klass_op, right_klass_op);\n+    } else {\n+      __ movptr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));\n+      __ movptr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));\n+      __ cmpptr(left_klass_op, right_klass_op);\n+    }\n+\n+    __ jcc(Assembler::equal, *op->stub()->entry()); \/\/ same klass -> do slow check\n+    \/\/ fall through to L_oops_not_equal\n+  }\n+\n+  __ bind(L_oops_not_equal);\n+  move(op->not_equal_result(), op->result_opr());\n+  __ jmp(L_end);\n+\n+  __ bind(L_oops_equal);\n+  move(op->equal_result(), op->result_opr());\n+  __ jmp(L_end);\n+\n+  \/\/ We've returned from the stub. RAX contains 0x0 IFF the two\n+  \/\/ operands are not substitutable. (Don't compare against 0x1 in case the\n+  \/\/ C compiler is naughty)\n+  __ bind(*op->stub()->continuation());\n+  __ cmpl(rax, 0);\n+  __ jcc(Assembler::equal, L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n+  move(op->equal_result(), op->result_opr()); \/\/ (call_stub() != 0x0) -> equal\n+  \/\/ fall-through\n+  __ bind(L_end);\n+}\n@@ -1561,0 +1725,15 @@\n+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {\n+  assert(dst->is_cpu_register(), \"must be\");\n+  assert(dst->type() == src->type(), \"must be\");\n+\n+  if (src->is_cpu_register()) {\n+    reg2reg(src, dst);\n+  } else if (src->is_stack()) {\n+    stack2reg(src, dst, dst->type());\n+  } else if (src->is_constant()) {\n+    const2reg(src, dst, lir_patch_none, nullptr);\n+  } else {\n+    ShouldNotReachHere();\n+  }\n+}\n+\n@@ -2171,1 +2350,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2178,1 +2357,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2345,0 +2524,15 @@\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {\n+  if (null_check) {\n+    __ testptr(obj, obj);\n+    __ jcc(Assembler::zero, *slow_path->entry());\n+  }\n+  if (is_dest) {\n+    __ test_null_free_array_oop(obj, tmp, *slow_path->entry());\n+    \/\/ TODO 8350865 Flat no longer implies null-free, so we need to check for flat dest. Can we do better here?\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  } else {\n+    __ test_flat_array_oop(obj, tmp, *slow_path->entry());\n+  }\n+}\n+\n+\n@@ -2364,0 +2558,6 @@\n+  if (flags & LIR_OpArrayCopy::always_slow_path) {\n+    __ jmp(*stub->entry());\n+    __ bind(*stub->continuation());\n+    return;\n+  }\n+\n@@ -2440,0 +2640,8 @@\n+  \/\/ Handle inline type arrays\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false, (flags & LIR_OpArrayCopy::src_null_check));\n+  }\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true, (flags & LIR_OpArrayCopy::dst_null_check));\n+  }\n+\n@@ -2666,0 +2874,1 @@\n+\n@@ -3002,0 +3211,21 @@\n+void LIR_Assembler::emit_profile_inline_type(LIR_OpProfileInlineType* op) {\n+  Register obj = op->obj()->as_register();\n+  Register tmp = op->tmp()->as_pointer_register();\n+  Address mdo_addr = as_Address(op->mdp()->as_address_ptr());\n+  bool not_null = op->not_null();\n+  int flag = op->flag();\n+\n+  Label not_inline_type;\n+  if (!not_null) {\n+    __ testptr(obj, obj);\n+    __ jccb(Assembler::zero, not_inline_type);\n+  }\n+\n+  __ test_oop_is_not_inline_type(obj, tmp, not_inline_type);\n+\n+  __ orb(mdo_addr, flag);\n+\n+  __ bind(not_inline_type);\n+}\n+\n+\n@@ -3187,0 +3417,3 @@\n+void LIR_Assembler::check_orig_pc() {\n+  __ cmpptr(frame_map()->address_for_orig_pc_addr(), NULL_WORD);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":263,"deletions":30,"binary":false,"changes":293,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"runtime\/frame.inline.hpp\"\n@@ -85,1 +86,4 @@\n-  if (UseCompactObjectHeaders) {\n+  if (UseCompactObjectHeaders || EnableValhalla) {\n+    \/\/ COH: Markword contains class pointer which is only known at runtime.\n+    \/\/ Valhalla: Could have value class which has a different prototype header to a normal object.\n+    \/\/ In both cases, we need to fetch dynamically.\n@@ -88,5 +92,1 @@\n-  } else if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n-    movptr(t1, klass);\n-    encode_klass_not_null(t1, rscratch1);\n-    movl(Address(obj, oopDesc::klass_offset_in_bytes()), t1);\n+    \/\/ Otherwise: Can use the statically computed prototype header which is the same for every object.\n@@ -95,1 +95,11 @@\n-    movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n+  }\n+  if (!UseCompactObjectHeaders) {\n+    \/\/ COH: Markword already contains class pointer. Nothing else to do.\n+    \/\/ Otherwise: Fetch klass pointer following the markword\n+    if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+      movptr(t1, klass);\n+      encode_klass_not_null(t1, rscratch1);\n+      movl(Address(obj, oopDesc::klass_offset_in_bytes()), t1);\n+    } else {\n+      movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n+    }\n@@ -225,2 +235,20 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n-  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+void C1_MacroAssembler::build_frame_helper(int frame_size_in_bytes, int sp_offset_for_orig_pc, int sp_inc, bool reset_orig_pc, bool needs_stack_repair) {\n+  push(rbp);\n+  if (PreserveFramePointer) {\n+    mov(rbp, rsp);\n+  }\n+  decrement(rsp, frame_size_in_bytes);\n+\n+  if (needs_stack_repair) {\n+    \/\/ Save stack increment (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    int real_frame_size = sp_inc + frame_size_in_bytes + wordSize;\n+    movptr(Address(rsp, frame_size_in_bytes - wordSize), real_frame_size);\n+  }\n+  if (reset_orig_pc) {\n+    \/\/ Zero orig_pc to detect deoptimization during buffering in the entry points\n+    movptr(Address(rsp, sp_offset_for_orig_pc), 0);\n+  }\n+}\n+\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, bool needs_stack_repair, bool has_scalarized_args, Label* verified_inline_entry_label) {\n@@ -232,0 +260,1 @@\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n@@ -234,5 +263,1 @@\n-  push(rbp);\n-  if (PreserveFramePointer) {\n-    mov(rbp, rsp);\n-  }\n-  decrement(rsp, frame_size_in_bytes); \/\/ does not emit code for frame_size == 0\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, has_scalarized_args, needs_stack_repair);\n@@ -243,5 +268,4 @@\n-}\n-\n-void C1_MacroAssembler::remove_frame(int frame_size_in_bytes) {\n-  increment(rsp, frame_size_in_bytes);  \/\/ Does not emit code for frame_size == 0\n-  pop(rbp);\n+  if (verified_inline_entry_label != nullptr) {\n+    \/\/ Jump here from the scalarized entry points that already created the frame.\n+    bind(*verified_inline_entry_label);\n+  }\n@@ -251,1 +275,0 @@\n-\n@@ -257,0 +280,58 @@\n+int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label, bool is_inline_ro_entry) {\n+  assert(InlineTypePassFieldsAsArgs, \"sanity\");\n+  \/\/ Make sure there is enough stack space for this method's activation.\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+  generate_stack_overflow_check(bang_size_in_bytes);\n+\n+  GrowableArray<SigEntry>* sig    = ces->sig();\n+  GrowableArray<SigEntry>* sig_cc = is_inline_ro_entry ? ces->sig_cc_ro() : ces->sig_cc();\n+  VMRegPair* regs      = ces->regs();\n+  VMRegPair* regs_cc   = is_inline_ro_entry ? ces->regs_cc_ro() : ces->regs_cc();\n+  int args_on_stack    = ces->args_on_stack();\n+  int args_on_stack_cc = is_inline_ro_entry ? ces->args_on_stack_cc_ro() : ces->args_on_stack_cc();\n+\n+  assert(sig->length() <= sig_cc->length(), \"Zero-sized inline class not allowed!\");\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc->length());\n+  int args_passed = sig->length();\n+  int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);\n+\n+  \/\/ Create a temp frame so we can call into the runtime. It must be properly set up to accommodate GC.\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, 0, true, ces->c1_needs_stack_repair());\n+\n+  \/\/ The runtime call might safepoint, make sure nmethod entry barrier is executed\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ C1 code is not hot enough to micro optimize the nmethod entry barrier with an out-of-line stub\n+  bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n+\n+  \/\/ FIXME -- call runtime only if we cannot in-line allocate all the incoming inline type args.\n+  movptr(rbx, (intptr_t)(ces->method()));\n+  if (is_inline_ro_entry) {\n+    call(RuntimeAddress(Runtime1::entry_for(StubId::c1_buffer_inline_args_no_receiver_id)));\n+  } else {\n+    call(RuntimeAddress(Runtime1::entry_for(StubId::c1_buffer_inline_args_id)));\n+  }\n+  int rt_call_offset = offset();\n+\n+  \/\/ Remove the temp frame\n+  addptr(rsp, frame_size_in_bytes);\n+  pop(rbp);\n+\n+  \/\/ Check if we need to extend the stack for packing\n+  int sp_inc = 0;\n+  if (args_on_stack > args_on_stack_cc) {\n+    sp_inc = extend_stack_for_inline_args(args_on_stack);\n+  }\n+\n+  shuffle_inline_args(true, is_inline_ro_entry, sig_cc,\n+                      args_passed_cc, args_on_stack_cc, regs_cc, \/\/ from\n+                      args_passed, args_on_stack, regs,          \/\/ to\n+                      sp_inc, rax);\n+\n+  \/\/ Create the real frame. Below jump will then skip over the stack banging and frame\n+  \/\/ setup code in the verified_inline_entry (which has a different real_frame_size).\n+  build_frame_helper(frame_size_in_bytes, sp_offset_for_orig_pc, sp_inc, false, ces->c1_needs_stack_repair());\n+\n+  jmp(verified_inline_entry_label);\n+  return rt_call_offset;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":101,"deletions":20,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -52,1 +52,21 @@\n-void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void C2_MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+    Register klass = rscratch1;\n+\n+    mov_metadata(klass, C->method()->holder()->constant_encoding());\n+    clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+\n+    bind(L_skip_barrier);\n+  }\n+\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  bool fp_mode_24b = false;\n+  int stack_bang_size = C->output()->need_stack_bang(bangsize) ? bangsize : 0;\n+\n@@ -97,0 +117,6 @@\n+  if (C->needs_stack_repair()) {\n+    \/\/ Save stack increment just below the saved rbp (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    movptr(Address(rsp, framesize - wordSize), sp_inc + framesize + wordSize);\n+  }\n+\n@@ -115,0 +141,1 @@\n+}\n@@ -116,15 +143,13 @@\n-  if (!is_stub) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n-    Label dummy_slow_path;\n-    Label dummy_continuation;\n-    Label* slow_path = &dummy_slow_path;\n-    Label* continuation = &dummy_continuation;\n-    if (!Compile::current()->output()->in_scratch_emit_size()) {\n-      \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n-      C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n-      Compile::current()->output()->add_stub(stub);\n-      slow_path = &stub->entry();\n-      continuation = &stub->continuation();\n-    }\n-    bs->nmethod_entry_barrier(this, slow_path, continuation);\n+void C2_MacroAssembler::entry_barrier() {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n+  Label dummy_slow_path;\n+  Label dummy_continuation;\n+  Label* slow_path = &dummy_slow_path;\n+  Label* continuation = &dummy_continuation;\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+    C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n+    Compile::current()->output()->add_stub(stub);\n+    slow_path = &stub->entry();\n+    continuation = &stub->continuation();\n@@ -132,0 +157,1 @@\n+  bs->nmethod_entry_barrier(this, slow_path, continuation);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":42,"deletions":16,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);\n+  void verified_entry(Compile* C, int sp_inc = 0);\n@@ -34,0 +34,1 @@\n+  void entry_barrier();\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2724,0 +2724,33 @@\n+    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic() && _method->return_type()->is_loaded()) {\n+      \/\/ The last return value is not set by the callee but used to pass the null marker to compiled code.\n+      \/\/ Search for the corresponding projection, get the register and emit code that initialized it.\n+      uint con = (tf()->range_cc()->cnt() - 1);\n+      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+        ProjNode* proj = fast_out(i)->as_Proj();\n+        if (proj->_con == con) {\n+          \/\/ Set null marker if rax is non-null (a non-null value is returned buffered or scalarized)\n+          OptoReg::Name optoReg = ra_->get_reg_first(proj);\n+          VMReg reg = OptoReg::as_VMReg(optoReg, ra_->_framesize, OptoReg::reg2stack(ra_->_matcher._new_SP));\n+          Register toReg = reg->is_reg() ? reg->as_Register() : rscratch1;\n+          __ testq(rax, rax);\n+          __ setb(Assembler::notZero, toReg);\n+          __ movzbl(toReg, toReg);\n+          if (reg->is_stack()) {\n+            int st_off = reg->reg2stack() * VMRegImpl::stack_slot_size;\n+            __ movq(Address(rsp, st_off), toReg);\n+          }\n+          break;\n+        }\n+      }\n+      if (return_value_is_used()) {\n+        \/\/ An inline type is returned as fields in multiple registers.\n+        \/\/ Rax either contains an oop if the inline type is buffered or a pointer\n+        \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero rax\n+        \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n+        \/\/ rax &= (rax & 1) - 1\n+        __ movptr(rscratch1, rax);\n+        __ andptr(rscratch1, 0x1);\n+        __ subptr(rscratch1, 0x1);\n+        __ andptr(rax, rscratch1);\n+      }\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":33,"deletions":0,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -618,0 +618,4 @@\n+  if (_entry_point == nullptr) {\n+    \/\/ CallLeafNoFPInDirect\n+    return 3; \/\/ callq (register)\n+  }\n@@ -624,0 +628,1 @@\n+\n@@ -849,14 +854,1 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n-\n-    Label L_skip_barrier;\n-    Register klass = rscratch1;\n-\n-    __ mov_metadata(klass, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n-\n-    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+  __ verified_entry(C);\n@@ -864,1 +856,2 @@\n-    __ bind(L_skip_barrier);\n+  if (ra_->C->stub_function() == nullptr) {\n+    __ entry_barrier();\n@@ -867,1 +860,3 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != nullptr);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n+  }\n@@ -879,6 +874,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -931,13 +920,3 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove word for return adr already pushed\n-  \/\/ and RBP\n-  framesize -= 2*wordSize;\n-\n-  \/\/ Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here\n-\n-  if (framesize) {\n-    __ addq(rsp, framesize);\n-  }\n-\n-  __ popq(rbp);\n+  \/\/ Subtract two words to account for return address and rbp\n+  int initial_framesize = C->output()->frame_size_in_bytes() - 2*wordSize;\n+  __ remove_frame(initial_framesize, C->needs_stack_repair());\n@@ -962,6 +941,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -1569,0 +1542,43 @@\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"MachVEPNode\");\n+}\n+#endif\n+\n+void MachVEPNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  CodeBuffer* cbuf = masm->code();\n+  uint insts_size = cbuf->insts_size();\n+  if (!_verified) {\n+    __ ic_check(1);\n+  } else {\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == nullptr) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int initial_framesize = ra_->C->output()->frame_size_in_bytes() - 2*wordSize;\n+      __ remove_frame(initial_framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ jmp(dummy_verified_entry);\n+    } else {\n+      __ jmp(*_verified_entry);\n+    }\n+  }\n+  \/* WARNING these NOPs are critical so that verified entry point is properly\n+     4 bytes aligned for patching by NativeJump::patch_verified_entry() *\/\n+  int nops_cnt = 4 - ((cbuf->insts_size() - insts_size) & 0x3);\n+  nops_cnt &= 0x3; \/\/ Do not add nops if code is aligned.\n+  if (nops_cnt > 0) {\n+    __ nop(nops_cnt);\n+  }\n+}\n+\n@@ -1589,7 +1605,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n-\n@@ -3070,0 +3079,16 @@\n+\/\/ Indirect Narrow Oop Operand\n+operand indCompressedOop(rRegN reg) %{\n+  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(DecodeN reg);\n+\n+  op_cost(10);\n+  format %{\"[R12 + $reg << 3] (compressed oop addressing)\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0xc); \/\/ R12\n+    index($reg);\n+    scale(0x3);\n+    disp(0x0);\n+  %}\n+%}\n+\n@@ -3416,1 +3441,1 @@\n-               indCompressedOopOffset,\n+               indCompressedOop, indCompressedOopOffset,\n@@ -5998,0 +6023,26 @@\n+instruct castI2N(rRegN dst, rRegI src)\n+%{\n+  match(Set dst (CastI2N src));\n+\n+  format %{ \"movq    $dst, $src\\t# int -> narrow ptr\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movl($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+instruct castN2X(rRegL dst, rRegN src)\n+%{\n+  match(Set dst (CastP2X src));\n+\n+  format %{ \"movq    $dst, $src\\t# ptr -> long\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n@@ -12221,0 +12272,1 @@\n+\n@@ -12223,1 +12275,1 @@\n-instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -12226,3 +12278,120 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX <= 2));\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                            Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Small non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                       Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  ins_cost(125);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -12276,2 +12445,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false, $ktmp$$KRegister);\n@@ -12282,3 +12451,2 @@\n-\/\/ Small non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                       Universe dummy, rFlagsReg cr)\n+instruct rep_stos_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                 Universe dummy, rFlagsReg cr)\n@@ -12286,2 +12454,2 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX > 2));\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -12289,1 +12457,1 @@\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -12337,2 +12505,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true, $ktmp$$KRegister);\n@@ -12344,1 +12512,1 @@\n-instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -12347,3 +12515,99 @@\n-  predicate((UseAVX <=2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_large_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                                  Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Large non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                             Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -12388,2 +12652,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false, $ktmp$$KRegister);\n@@ -12394,3 +12658,2 @@\n-\/\/ Large non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                             Universe dummy, rFlagsReg cr)\n+instruct rep_stos_large_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                       Universe dummy, rFlagsReg cr)\n@@ -12398,3 +12661,3 @@\n-  predicate((UseAVX > 2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -12439,2 +12702,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true, $ktmp$$KRegister);\n@@ -12446,1 +12709,1 @@\n-instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rRegI zero, kReg ktmp, Universe dummy, rFlagsReg cr)\n+instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rax_RegL val, kReg ktmp, Universe dummy, rFlagsReg cr)\n@@ -12448,2 +12711,3 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (MaxVectorSize >= 32) && VM_Version::supports_avx512vl());\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() &&\n+            ((MaxVectorSize >= 32) && VM_Version::supports_avx512vl()));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -12451,1 +12715,1 @@\n-  effect(TEMP tmp, TEMP zero, TEMP ktmp, KILL cr);\n+  effect(TEMP tmp, USE_KILL val, TEMP ktmp, KILL cr);\n@@ -12454,1 +12718,1 @@\n-   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$constant, $val$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n@@ -14300,0 +14564,15 @@\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPInDirect(rRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == nullptr);\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(300);\n+  format %{ \"call_leaf_nofp,runtime indirect \" %}\n+  ins_encode %{\n+     __ call($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -14302,0 +14581,1 @@\n+  predicate(n->as_Call()->entry_point() != nullptr);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":362,"deletions":82,"binary":false,"changes":444,"status":"modified"},{"patch":"@@ -261,0 +261,77 @@\n+class LoadFlattenedArrayStub: public CodeStub {\n+ private:\n+  LIR_Opr          _array;\n+  LIR_Opr          _index;\n+  LIR_Opr          _result;\n+  LIR_Opr          _scratch_reg;\n+  CodeEmitInfo*    _info;\n+\n+ public:\n+  LoadFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr result, CodeEmitInfo* info);\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual CodeEmitInfo* info() const             { return _info; }\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_slow_case(_info);\n+    visitor->do_input(_array);\n+    visitor->do_input(_index);\n+    visitor->do_output(_result);\n+    if (_scratch_reg != LIR_OprFact::illegalOpr) {\n+      visitor->do_temp(_scratch_reg);\n+    }\n+  }\n+\n+#ifndef PRODUCT\n+  virtual void print_name(outputStream* out) const { out->print(\"LoadFlattenedArrayStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n+\n+class StoreFlattenedArrayStub: public CodeStub {\n+ private:\n+  LIR_Opr          _array;\n+  LIR_Opr          _index;\n+  LIR_Opr          _value;\n+  LIR_Opr          _scratch_reg;\n+  CodeEmitInfo*    _info;\n+\n+ public:\n+  StoreFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr value, CodeEmitInfo* info);\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual CodeEmitInfo* info() const             { return _info; }\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_slow_case(_info);\n+    visitor->do_input(_array);\n+    visitor->do_input(_index);\n+    visitor->do_input(_value);\n+    if (_scratch_reg != LIR_OprFact::illegalOpr) {\n+      visitor->do_temp(_scratch_reg);\n+    }\n+  }\n+#ifndef PRODUCT\n+  virtual void print_name(outputStream* out) const { out->print(\"StoreFlattenedArrayStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n+class SubstitutabilityCheckStub: public CodeStub {\n+ private:\n+  LIR_Opr          _left;\n+  LIR_Opr          _right;\n+  LIR_Opr          _scratch_reg;\n+  CodeEmitInfo*    _info;\n+ public:\n+  SubstitutabilityCheckStub(LIR_Opr left, LIR_Opr right, CodeEmitInfo* info);\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual CodeEmitInfo* info() const             { return _info; }\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_slow_case(_info);\n+    visitor->do_input(_left);\n+    visitor->do_input(_right);\n+    if (_scratch_reg != LIR_OprFact::illegalOpr) {\n+      visitor->do_temp(_scratch_reg);\n+    }\n+  }\n+#ifndef PRODUCT\n+  virtual void print_name(outputStream* out) const { out->print(\"SubstitutabilityCheckStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n@@ -313,1 +390,1 @@\n-\n+  bool           _is_null_free;\n@@ -315,1 +392,1 @@\n-  NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info);\n+  NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info, bool is_null_free);\n@@ -350,0 +427,2 @@\n+  CodeStub* _throw_ie_stub;\n+  LIR_Opr _scratch_reg;\n@@ -352,1 +431,2 @@\n-  MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info)\n+  MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info,\n+                   CodeStub* throw_ie_stub = nullptr, LIR_Opr scratch_reg = LIR_OprFact::illegalOpr)\n@@ -355,0 +435,5 @@\n+    _scratch_reg = scratch_reg;\n+    _throw_ie_stub = throw_ie_stub;\n+    if (_throw_ie_stub != nullptr) {\n+      assert(_scratch_reg != LIR_OprFact::illegalOpr, \"must be\");\n+    }\n@@ -364,0 +449,3 @@\n+    if (_scratch_reg != LIR_OprFact::illegalOpr) {\n+      visitor->do_temp(_scratch_reg);\n+    }\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":91,"deletions":3,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -33,0 +33,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -35,0 +37,1 @@\n+#include \"ci\/ciObjArrayKlass.hpp\"\n@@ -218,0 +221,2 @@\n+  assert(!_gen->in_conditional_code(), \"LIRItem cannot be loaded in conditional code\");\n+\n@@ -625,1 +630,2 @@\n-void LIRGenerator::monitor_enter(LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no, CodeEmitInfo* info_for_exception, CodeEmitInfo* info) {\n+void LIRGenerator::monitor_enter(LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no,\n+                                 CodeEmitInfo* info_for_exception, CodeEmitInfo* info, CodeStub* throw_ie_stub) {\n@@ -627,1 +633,1 @@\n-  CodeStub* slow_path = new MonitorEnterStub(object, lock, info);\n+  CodeStub* slow_path = new MonitorEnterStub(object, lock, info, throw_ie_stub, scratch);\n@@ -630,1 +636,1 @@\n-  __ lock_object(hdr, object, lock, scratch, slow_path, info_for_exception);\n+  __ lock_object(hdr, object, lock, scratch, slow_path, info_for_exception, throw_ie_stub);\n@@ -653,4 +659,9 @@\n-void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {\n-  klass2reg_with_patching(klass_reg, klass, info, is_unresolved);\n-  \/\/ If klass is not loaded we do not know if the klass has finalizers:\n-  if (UseFastNewInstance && klass->is_loaded()\n+void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, bool allow_inline, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {\n+  if (allow_inline) {\n+    assert(!is_unresolved && klass->is_loaded(), \"inline type klass should be resolved\");\n+    __ metadata2reg(klass->constant_encoding(), klass_reg);\n+  } else {\n+    klass2reg_with_patching(klass_reg, klass, info, is_unresolved);\n+  }\n+  \/\/ If klass is not loaded we do not know if the klass has finalizers or is an unexpected inline klass\n+  if (UseFastNewInstance && klass->is_loaded() && (allow_inline || !klass->is_inlinetype())\n@@ -671,1 +682,1 @@\n-    __ branch(lir_cond_always, slow_path);\n+    __ jump(slow_path);\n@@ -763,0 +774,5 @@\n+    if (expected_type != nullptr && expected_type->is_obj_array_klass()) {\n+      \/\/ For a direct pointer comparison, we need the refined array klass pointer\n+      expected_type = ciObjArrayKlass::make(expected_type->as_array_klass()->element_klass());\n+    }\n+\n@@ -771,0 +787,10 @@\n+  if (!src->is_loaded_flat_array() && !dst->is_loaded_flat_array()) {\n+    flags &= ~LIR_OpArrayCopy::always_slow_path;\n+  }\n+  if (!src->maybe_flat_array()) {\n+    flags &= ~LIR_OpArrayCopy::src_inlinetype_check;\n+  }\n+  if (!dst->maybe_flat_array() && !dst->maybe_null_free_array()) {\n+    flags &= ~LIR_OpArrayCopy::dst_inlinetype_check;\n+  }\n+\n@@ -1463,1 +1489,1 @@\n-  for (int i = 0; i < _constants.length(); i++) {\n+  for (int i = 0; i < _constants.length() && !in_conditional_code(); i++) {\n@@ -1488,2 +1514,4 @@\n-  _constants.append(c);\n-  _reg_for_constants.append(result);\n+  if (!in_conditional_code()) {\n+    _constants.append(c);\n+    _reg_for_constants.append(result);\n+  }\n@@ -1493,0 +1521,6 @@\n+void LIRGenerator::set_in_conditional_code(bool v) {\n+  assert(v != _in_conditional_code, \"must change state\");\n+  _in_conditional_code = v;\n+}\n+\n+\n@@ -1510,0 +1544,8 @@\n+\/\/ Returns a int\/long value with the null marker bit set\n+static LIR_Opr null_marker_mask(BasicType bt, ciField* field) {\n+  assert(field->null_marker_offset() != -1, \"field does not have null marker\");\n+  int nm_offset = field->null_marker_offset() - field->offset_in_bytes();\n+  jlong null_marker = 1ULL << (nm_offset << LogBitsPerByte);\n+  return (bt == T_LONG) ? LIR_OprFact::longConst(null_marker) : LIR_OprFact::intConst(null_marker);\n+}\n+\n@@ -1539,0 +1581,1 @@\n+  ciField* field = x->field();\n@@ -1540,1 +1583,1 @@\n-  bool is_volatile = x->field()->is_volatile();\n+  bool is_volatile = field->is_volatile();\n@@ -1561,10 +1604,2 @@\n-  if (is_volatile || needs_patching) {\n-    \/\/ load item if field is volatile (fewer special cases for volatiles)\n-    \/\/ load item if field not initialized\n-    \/\/ load item if field not constant\n-    \/\/ because of code patching we cannot inline constants\n-    if (field_type == T_BYTE || field_type == T_BOOLEAN) {\n-      value.load_byte_item();\n-    } else  {\n-      value.load_item();\n-    }\n+  if (field->is_flat()) {\n+    value.load_item();\n@@ -1572,1 +1607,13 @@\n-    value.load_for_store(field_type);\n+    if (is_volatile || needs_patching) {\n+      \/\/ load item if field is volatile (fewer special cases for volatiles)\n+      \/\/ load item if field not initialized\n+      \/\/ load item if field not constant\n+      \/\/ because of code patching we cannot inline constants\n+      if (field_type == T_BYTE || field_type == T_BOOLEAN) {\n+        value.load_byte_item();\n+      } else  {\n+        value.load_item();\n+      }\n+    } else {\n+      value.load_for_store(field_type);\n+    }\n@@ -1601,0 +1648,43 @@\n+  if (field->is_flat()) {\n+    ciInlineKlass* vk = field->type()->as_inline_klass();\n+\n+#ifdef ASSERT\n+    bool is_naturally_atomic = vk->nof_declared_nonstatic_fields() <= 1;\n+    bool needs_atomic_access = !field->is_null_free() || (field->is_volatile() && !is_naturally_atomic);\n+    assert(needs_atomic_access, \"No atomic access required\");\n+    \/\/ ZGC does not support compressed oops, so only one oop can be in the payload which is written by a \"normal\" oop store.\n+    assert(!vk->contains_oops() || !UseZGC, \"ZGC does not support embedded oops in flat fields\");\n+#endif\n+\n+    \/\/ Zero the payload\n+    BasicType bt = vk->atomic_size_to_basic_type(field->is_null_free());\n+    LIR_Opr payload = new_register((bt == T_LONG) ? bt : T_INT);\n+    LIR_Opr zero = (bt == T_LONG) ? LIR_OprFact::longConst(0) : LIR_OprFact::intConst(0);\n+    __ move(zero, payload);\n+\n+    bool is_constant_null = value.is_constant() && value.value()->is_null_obj();\n+    if (!is_constant_null) {\n+      LabelObj* L_isNull = new LabelObj();\n+      bool needs_null_check = !value.is_constant() || value.value()->is_null_obj();\n+      if (needs_null_check) {\n+        __ cmp(lir_cond_equal, value.result(), LIR_OprFact::oopConst(nullptr));\n+        __ branch(lir_cond_equal, L_isNull->label());\n+      }\n+      \/\/ Load payload (if not empty) and set null marker (if not null-free)\n+      if (!vk->is_empty()) {\n+        access_load_at(decorators, bt, value, LIR_OprFact::intConst(vk->payload_offset()), payload);\n+      }\n+      if (!field->is_null_free()) {\n+        __ logical_or(payload, null_marker_mask(bt, field), payload);\n+      }\n+      if (needs_null_check) {\n+        __ branch_destination(L_isNull->label());\n+      }\n+    }\n+    access_store_at(decorators, bt, object, LIR_OprFact::intConst(x->offset()), payload,\n+                    \/\/ Make sure to emit an implicit null check and pass the information\n+                    \/\/ that this is a flat store that might require gc barriers for oop fields.\n+                    info != nullptr ? new CodeEmitInfo(info) : nullptr, info, vk);\n+    return;\n+  }\n+\n@@ -1605,0 +1695,155 @@\n+\/\/ FIXME -- I can't find any other way to pass an address to access_load_at().\n+class TempResolvedAddress: public Instruction {\n+ public:\n+  TempResolvedAddress(ValueType* type, LIR_Opr addr) : Instruction(type) {\n+    set_operand(addr);\n+  }\n+  virtual void input_values_do(ValueVisitor*) {}\n+  virtual void visit(InstructionVisitor* v)   {}\n+  virtual const char* name() const  { return \"TempResolvedAddress\"; }\n+};\n+\n+LIR_Opr LIRGenerator::get_and_load_element_address(LIRItem& array, LIRItem& index) {\n+  ciType* array_type = array.value()->declared_type();\n+  ciFlatArrayKlass* flat_array_klass = array_type->as_flat_array_klass();\n+  assert(flat_array_klass->is_loaded(), \"must be\");\n+\n+  int array_header_size = flat_array_klass->array_header_in_bytes();\n+  int shift = flat_array_klass->log2_element_size();\n+\n+#ifndef _LP64\n+  LIR_Opr index_op = new_register(T_INT);\n+  \/\/ FIXME -- on 32-bit, the shift below can overflow, so we need to check that\n+  \/\/ the top (shift+1) bits of index_op must be zero, or\n+  \/\/ else throw ArrayIndexOutOfBoundsException\n+  if (index.result()->is_constant()) {\n+    jint const_index = index.result()->as_jint();\n+    __ move(LIR_OprFact::intConst(const_index << shift), index_op);\n+  } else {\n+    __ shift_left(index_op, shift, index.result());\n+  }\n+#else\n+  LIR_Opr index_op = new_register(T_LONG);\n+  if (index.result()->is_constant()) {\n+    jint const_index = index.result()->as_jint();\n+    __ move(LIR_OprFact::longConst(const_index << shift), index_op);\n+  } else {\n+    __ convert(Bytecodes::_i2l, index.result(), index_op);\n+    \/\/ Need to shift manually, as LIR_Address can scale only up to 3.\n+    __ shift_left(index_op, shift, index_op);\n+  }\n+#endif\n+\n+  LIR_Opr elm_op = new_pointer_register();\n+  LIR_Address* elm_address = generate_address(array.result(), index_op, 0, array_header_size, T_ADDRESS);\n+  __ leal(LIR_OprFact::address(elm_address), elm_op);\n+  return elm_op;\n+}\n+\n+void LIRGenerator::access_sub_element(LIRItem& array, LIRItem& index, LIR_Opr& result, ciField* field, int sub_offset) {\n+  assert(field != nullptr, \"Need a subelement type specified\");\n+\n+  \/\/ Find the starting address of the source (inside the array)\n+  LIR_Opr elm_op = get_and_load_element_address(array, index);\n+\n+  BasicType subelt_type = field->type()->basic_type();\n+  TempResolvedAddress* elm_resolved_addr = new TempResolvedAddress(as_ValueType(subelt_type), elm_op);\n+  LIRItem elm_item(elm_resolved_addr, this);\n+\n+  DecoratorSet decorators = IN_HEAP;\n+  access_load_at(decorators, subelt_type,\n+                     elm_item, LIR_OprFact::intConst(sub_offset), result,\n+                     nullptr, nullptr);\n+}\n+\n+void LIRGenerator::access_flat_array(bool is_load, LIRItem& array, LIRItem& index, LIRItem& obj_item,\n+                                          ciField* field, int sub_offset) {\n+  assert(sub_offset == 0 || field != nullptr, \"Sanity check\");\n+\n+  \/\/ Find the starting address of the source (inside the array)\n+  LIR_Opr elm_op = get_and_load_element_address(array, index);\n+\n+  ciInlineKlass* elem_klass = nullptr;\n+  if (field != nullptr) {\n+    elem_klass = field->type()->as_inline_klass();\n+  } else {\n+    elem_klass = array.value()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass();\n+  }\n+  for (int i = 0; i < elem_klass->nof_nonstatic_fields(); i++) {\n+    ciField* inner_field = elem_klass->nonstatic_field_at(i);\n+    assert(!inner_field->is_flat(), \"flat fields must have been expanded\");\n+    int obj_offset = inner_field->offset_in_bytes();\n+    int elm_offset = obj_offset - elem_klass->payload_offset() + sub_offset; \/\/ object header is not stored in array.\n+    BasicType field_type = inner_field->type()->basic_type();\n+\n+    \/\/ Types which are smaller than int are still passed in an int register.\n+    BasicType reg_type = field_type;\n+    switch (reg_type) {\n+    case T_BYTE:\n+    case T_BOOLEAN:\n+    case T_SHORT:\n+    case T_CHAR:\n+      reg_type = T_INT;\n+      break;\n+    default:\n+      break;\n+    }\n+\n+    LIR_Opr temp = new_register(reg_type);\n+    TempResolvedAddress* elm_resolved_addr = new TempResolvedAddress(as_ValueType(field_type), elm_op);\n+    LIRItem elm_item(elm_resolved_addr, this);\n+\n+    DecoratorSet decorators = IN_HEAP;\n+    if (is_load) {\n+      access_load_at(decorators, field_type,\n+                     elm_item, LIR_OprFact::intConst(elm_offset), temp,\n+                     nullptr, nullptr);\n+      access_store_at(decorators, field_type,\n+                      obj_item, LIR_OprFact::intConst(obj_offset), temp,\n+                      nullptr, nullptr);\n+    } else {\n+      access_load_at(decorators, field_type,\n+                     obj_item, LIR_OprFact::intConst(obj_offset), temp,\n+                     nullptr, nullptr);\n+      access_store_at(decorators, field_type,\n+                      elm_item, LIR_OprFact::intConst(elm_offset), temp,\n+                      nullptr, nullptr);\n+    }\n+  }\n+}\n+\n+void LIRGenerator::check_flat_array(LIR_Opr array, LIR_Opr value, CodeStub* slow_path) {\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_flat_array(array, value, tmp, slow_path);\n+}\n+\n+void LIRGenerator::check_null_free_array(LIRItem& array, LIRItem& value, CodeEmitInfo* info) {\n+  LabelObj* L_end = new LabelObj();\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_null_free_array(array.result(), tmp);\n+  __ branch(lir_cond_equal, L_end->label());\n+  __ null_check(value.result(), info);\n+  __ branch_destination(L_end->label());\n+}\n+\n+bool LIRGenerator::needs_flat_array_store_check(StoreIndexed* x) {\n+  if (x->elt_type() == T_OBJECT && x->array()->maybe_flat_array()) {\n+    ciType* type = x->value()->declared_type();\n+    if (type != nullptr && type->is_klass()) {\n+      ciKlass* klass = type->as_klass();\n+      if (!klass->can_be_inline_klass() || (klass->is_inlinetype() && !klass->as_inline_klass()->maybe_flat_in_array())) {\n+        \/\/ This is known to be a non-flat object. If the array is a flat array,\n+        \/\/ it will be caught by the code generated by array_store_check().\n+        return false;\n+      }\n+    }\n+    \/\/ We're not 100% sure, so let's do the flat_array_store_check.\n+    return true;\n+  }\n+  return false;\n+}\n+\n+bool LIRGenerator::needs_null_free_array_store_check(StoreIndexed* x) {\n+  return x->elt_type() == T_OBJECT && x->array()->maybe_null_free_array();\n+}\n+\n@@ -1607,0 +1852,2 @@\n+  assert(x->elt_type() != T_ARRAY, \"never used\");\n+  bool is_loaded_flat_array = x->array()->is_loaded_flat_array();\n@@ -1610,3 +1857,3 @@\n-  bool needs_store_check = obj_store && (x->value()->as_Constant() == nullptr ||\n-                                         !get_jobject_constant(x->value())->is_null_object() ||\n-                                         x->should_profile());\n+  bool needs_store_check = obj_store && !(is_loaded_flat_array && x->is_exact_flat_array_store()) &&\n+                                        (x->value()->as_Constant() == nullptr ||\n+                                         !get_jobject_constant(x->value())->is_null_object());\n@@ -1625,2 +1872,3 @@\n-\n-  if (needs_store_check || x->check_boolean()) {\n+\n+  if (needs_store_check || x->check_boolean()\n+      || is_loaded_flat_array || needs_flat_array_store_check(x) || needs_null_free_array_store_check(x)) {\n@@ -1660,3 +1908,18 @@\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-  if (x->check_boolean()) {\n-    decorators |= C1_MASK_BOOLEAN;\n+  if (x->should_profile()) {\n+    if (is_loaded_flat_array) {\n+      \/\/ No need to profile a store to a flat array of known type. This can happen if\n+      \/\/ the type only became known after optimizations (for example, after the PhiSimplifier).\n+      x->set_should_profile(false);\n+    } else {\n+      int bci = x->profiled_bci();\n+      ciMethodData* md = x->profiled_method()->method_data();\n+      assert(md != nullptr, \"Sanity\");\n+      ciProfileData* data = md->bci_to_data(bci);\n+      assert(data != nullptr && data->is_ArrayStoreData(), \"incorrect profiling entry\");\n+      ciArrayStoreData* store_data = (ciArrayStoreData*)data;\n+      profile_array_type(x, md, store_data);\n+      assert(store_data->is_ArrayStoreData(), \"incorrect profiling entry\");\n+      if (x->array()->maybe_null_free_array()) {\n+        profile_null_free_array(array, md, data);\n+      }\n+    }\n@@ -1665,2 +1928,34 @@\n-  access_store_at(decorators, x->elt_type(), array, index.result(), value.result(),\n-                  nullptr, null_check_info);\n+  if (is_loaded_flat_array) {\n+    \/\/ TODO 8350865 This is currently dead code\n+    if (!x->value()->is_null_free()) {\n+      __ null_check(value.result(), new CodeEmitInfo(range_check_info));\n+    }\n+    \/\/ If array element is an empty inline type, no need to copy anything\n+    if (!x->array()->declared_type()->as_flat_array_klass()->element_klass()->as_inline_klass()->is_empty()) {\n+      access_flat_array(false, array, index, value);\n+    }\n+  } else {\n+    StoreFlattenedArrayStub* slow_path = nullptr;\n+\n+    if (needs_flat_array_store_check(x)) {\n+      \/\/ Check if we indeed have a flat array\n+      index.load_item();\n+      slow_path = new StoreFlattenedArrayStub(array.result(), index.result(), value.result(), state_for(x, x->state_before()));\n+      check_flat_array(array.result(), value.result(), slow_path);\n+      set_in_conditional_code(true);\n+    } else if (needs_null_free_array_store_check(x)) {\n+      CodeEmitInfo* info = new CodeEmitInfo(range_check_info);\n+      check_null_free_array(array, value, info);\n+    }\n+\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    if (x->check_boolean()) {\n+      decorators |= C1_MASK_BOOLEAN;\n+    }\n+\n+    access_store_at(decorators, x->elt_type(), array, index.result(), value.result(), nullptr, null_check_info);\n+    if (slow_path != nullptr) {\n+      __ branch_destination(slow_path->continuation());\n+      set_in_conditional_code(false);\n+    }\n+  }\n@@ -1695,1 +1990,2 @@\n-                                   CodeEmitInfo* patch_info, CodeEmitInfo* store_emit_info) {\n+                                   CodeEmitInfo* patch_info, CodeEmitInfo* store_emit_info,\n+                                   ciInlineKlass* vk) {\n@@ -1697,1 +1993,1 @@\n-  LIRAccess access(this, decorators, base, offset, type, patch_info, store_emit_info);\n+  LIRAccess access(this, decorators, base, offset, type, patch_info, store_emit_info, vk);\n@@ -1748,0 +2044,1 @@\n+  ciField* field = x->field();\n@@ -1749,1 +2046,1 @@\n-  bool is_volatile = x->field()->is_volatile();\n+  bool is_volatile = field->is_volatile();\n@@ -1800,0 +2097,37 @@\n+  if (field->is_flat()) {\n+    ciInlineKlass* vk = field->type()->as_inline_klass();\n+#ifdef ASSERT\n+    bool is_naturally_atomic = vk->nof_declared_nonstatic_fields() <= 1;\n+    bool needs_atomic_access = !field->is_null_free() || (field->is_volatile() && !is_naturally_atomic);\n+    assert(needs_atomic_access, \"No atomic access required\");\n+    assert(x->state_before() != nullptr, \"Needs state before\");\n+#endif\n+\n+    \/\/ Allocate buffer (we can't easily do this conditionally on the null check below\n+    \/\/ because branches added in the LIR are opaque to the register allocator).\n+    NewInstance* buffer = new NewInstance(vk, x->state_before(), false, true);\n+    do_NewInstance(buffer);\n+    LIRItem dest(buffer, this);\n+\n+    \/\/ Copy the payload to the buffer\n+    BasicType bt = vk->atomic_size_to_basic_type(field->is_null_free());\n+    LIR_Opr payload = new_register((bt == T_LONG) ? bt : T_INT);\n+    access_load_at(decorators, bt, object, LIR_OprFact::intConst(field->offset_in_bytes()), payload,\n+                   \/\/ Make sure to emit an implicit null check\n+                   info ? new CodeEmitInfo(info) : nullptr, info);\n+    access_store_at(decorators, bt, dest, LIR_OprFact::intConst(vk->payload_offset()), payload);\n+\n+    if (field->is_null_free()) {\n+      set_result(x, buffer->operand());\n+    } else {\n+      \/\/ Check the null marker and set result to null if it's not set\n+      __ logical_and(payload, null_marker_mask(bt, field), payload);\n+      __ cmp(lir_cond_equal, payload, (bt == T_LONG) ? LIR_OprFact::longConst(0) : LIR_OprFact::intConst(0));\n+      __ cmove(lir_cond_equal, LIR_OprFact::oopConst(nullptr), buffer->operand(), rlock_result(x), T_OBJECT);\n+    }\n+\n+    \/\/ Ensure the copy is visible before any subsequent store that publishes the buffer.\n+    __ membar_storestore();\n+    return;\n+  }\n+\n@@ -1948,1 +2282,61 @@\n-  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+  ciMethodData* md = nullptr;\n+  ciProfileData* data = nullptr;\n+  if (x->should_profile()) {\n+    if (x->array()->is_loaded_flat_array()) {\n+      \/\/ No need to profile a load from a flat array of known type. This can happen if\n+      \/\/ the type only became known after optimizations (for example, after the PhiSimplifier).\n+      x->set_should_profile(false);\n+    } else {\n+      int bci = x->profiled_bci();\n+      md = x->profiled_method()->method_data();\n+      assert(md != nullptr, \"Sanity\");\n+      data = md->bci_to_data(bci);\n+      assert(data != nullptr && data->is_ArrayLoadData(), \"incorrect profiling entry\");\n+      ciArrayLoadData* load_data = (ciArrayLoadData*)data;\n+      profile_array_type(x, md, load_data);\n+    }\n+  }\n+\n+  Value element;\n+  if (x->vt() != nullptr) {\n+    assert(x->array()->is_loaded_flat_array(), \"must be\");\n+    \/\/ Find the destination address (of the NewInlineTypeInstance).\n+    LIRItem obj_item(x->vt(), this);\n+\n+    access_flat_array(true, array, index, obj_item,\n+                      x->delayed() == nullptr ? 0 : x->delayed()->field(),\n+                      x->delayed() == nullptr ? 0 : x->delayed()->offset());\n+    set_no_result(x);\n+  } else if (x->delayed() != nullptr) {\n+    assert(x->array()->is_loaded_flat_array(), \"must be\");\n+    LIR_Opr result = rlock_result(x, x->delayed()->field()->type()->basic_type());\n+    access_sub_element(array, index, result, x->delayed()->field(), x->delayed()->offset());\n+  } else {\n+    LIR_Opr result = rlock_result(x, x->elt_type());\n+    LoadFlattenedArrayStub* slow_path = nullptr;\n+\n+    if (x->should_profile() && x->array()->maybe_null_free_array()) {\n+      profile_null_free_array(array, md, data);\n+    }\n+\n+    if (x->elt_type() == T_OBJECT && x->array()->maybe_flat_array()) {\n+      assert(x->delayed() == nullptr, \"Delayed LoadIndexed only apply to loaded_flat_arrays\");\n+      index.load_item();\n+      \/\/ if we are loading from a flat array, load it using a runtime call\n+      slow_path = new LoadFlattenedArrayStub(array.result(), index.result(), result, state_for(x, x->state_before()));\n+      check_flat_array(array.result(), LIR_OprFact::illegalOpr, slow_path);\n+      set_in_conditional_code(true);\n+    }\n+\n+    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+    access_load_at(decorators, x->elt_type(),\n+                   array, index.result(), result,\n+                   nullptr, null_check_info);\n+\n+    if (slow_path != nullptr) {\n+      __ branch_destination(slow_path->continuation());\n+      set_in_conditional_code(false);\n+    }\n+\n+    element = x;\n+  }\n@@ -1950,4 +2344,3 @@\n-  LIR_Opr result = rlock_result(x, x->elt_type());\n-  access_load_at(decorators, x->elt_type(),\n-                 array, index.result(), result,\n-                 nullptr, null_check_info);\n+  if (x->should_profile()) {\n+    profile_element_type(element, md, (ciArrayLoadData*)data);\n+  }\n@@ -2436,1 +2829,1 @@\n-  if (do_update) {\n+  if (do_update && signature_at_call_k != nullptr) {\n@@ -2468,0 +2861,12 @@\n+  if (exact_klass != nullptr && exact_klass->is_obj_array_klass()) {\n+    if (exact_klass->can_be_inline_array_klass()) {\n+      \/\/ Inline type arrays can have additional properties, we need to load the klass\n+      \/\/ TODO 8350865 Can we do better here and track the properties?\n+      exact_klass = nullptr;\n+      do_update = true;\n+    } else {\n+      \/\/ For a direct pointer comparison, we need the refined array klass pointer\n+      exact_klass = ciObjArrayKlass::make(exact_klass->as_array_klass()->element_klass());\n+      do_update = ciTypeEntries::valid_ciklass(profiled_k) != exact_klass;\n+    }\n+  }\n@@ -2521,0 +2926,42 @@\n+void LIRGenerator::profile_flags(ciMethodData* md, ciProfileData* data, int flag, LIR_Condition condition) {\n+  assert(md != nullptr && data != nullptr, \"should have been initialized\");\n+  LIR_Opr mdp = new_register(T_METADATA);\n+  __ metadata2reg(md->constant_encoding(), mdp);\n+  LIR_Address* addr = new LIR_Address(mdp, md->byte_offset_of_slot(data, DataLayout::flags_offset()), T_BYTE);\n+  LIR_Opr flags = new_register(T_INT);\n+  __ move(addr, flags);\n+  LIR_Opr update;\n+  if (condition != lir_cond_always) {\n+    update = new_register(T_INT);\n+    __ cmove(condition, LIR_OprFact::intConst(0), LIR_OprFact::intConst(flag), update, T_INT);\n+  } else {\n+    update = LIR_OprFact::intConst(flag);\n+  }\n+  __ logical_or(flags, update, flags);\n+  __ store(flags, addr);\n+}\n+\n+void LIRGenerator::profile_null_free_array(LIRItem array, ciMethodData* md, ciProfileData* data) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  LabelObj* L_end = new LabelObj();\n+  LIR_Opr tmp = new_register(T_METADATA);\n+  __ check_null_free_array(array.result(), tmp);\n+\n+  profile_flags(md, data, ArrayStoreData::null_free_array_byte_constant(), lir_cond_equal);\n+}\n+\n+template <class ArrayData> void LIRGenerator::profile_array_type(AccessIndexed* x, ciMethodData*& md, ArrayData*& load_store) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(load_store, ArrayData::array_offset()), 0,\n+               load_store->array()->type(), x->array(), mdp, true, nullptr, nullptr);\n+}\n+\n+void LIRGenerator::profile_element_type(Value element, ciMethodData* md, ciArrayLoadData* load_data) {\n+  assert(compilation()->profile_array_accesses(), \"array access profiling is disabled\");\n+  assert(md != nullptr && load_data != nullptr, \"should have been initialized\");\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(load_data, ArrayLoadData::element_offset()), 0,\n+               load_data->element()->type(), element, mdp, false, nullptr, nullptr);\n+}\n+\n@@ -2562,0 +3009,6 @@\n+  \/\/ Check if we need a membar at the beginning of the java.lang.Object\n+  \/\/ constructor to satisfy the memory model for strict fields.\n+  if (EnableValhalla && method()->intrinsic_id() == vmIntrinsics::_Object_init) {\n+    __ membar_storestore();\n+  }\n+\n@@ -2603,0 +3056,8 @@\n+  if (method()->has_scalarized_args()) {\n+    \/\/ Check if deoptimization was triggered (i.e. orig_pc was set) while buffering scalarized inline type arguments\n+    \/\/ in the entry point (see comments in frame::deoptimize). If so, deoptimize only now that we have the right state.\n+    CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, 0), nullptr, false);\n+    CodeStub* deopt_stub = new DeoptimizeStub(info, Deoptimization::Reason_none, Deoptimization::Action_none);\n+    __ append(new LIR_Op0(lir_check_orig_pc));\n+    __ branch(lir_cond_notEqual, deopt_stub);\n+  }\n@@ -2618,0 +3079,13 @@\n+void LIRGenerator::invoke_load_one_argument(LIRItem* param, LIR_Opr loc) {\n+  if (loc->is_register()) {\n+    param->load_item_force(loc);\n+  } else {\n+    LIR_Address* addr = loc->as_address_ptr();\n+    param->load_for_store(addr->type());\n+    if (addr->type() == T_OBJECT) {\n+      __ move_wide(param->result(), addr);\n+    } else {\n+      __ move(param->result(), addr);\n+    }\n+  }\n+}\n@@ -2625,10 +3099,1 @@\n-    if (loc->is_register()) {\n-      param->load_item_force(loc);\n-    } else {\n-      LIR_Address* addr = loc->as_address_ptr();\n-      param->load_for_store(addr->type());\n-      if (addr->type() == T_OBJECT) {\n-        __ move_wide(param->result(), addr);\n-      } else\n-        __ move(param->result(), addr);\n-    }\n+    invoke_load_one_argument(param, loc);\n@@ -2800,1 +3265,1 @@\n-  if (can_inline_as_constant(right.value())) {\n+  if (can_inline_as_constant(right.value()) && !x->substitutability_check()) {\n@@ -2803,0 +3268,1 @@\n+    \/\/ substitutability_check() needs to use right as a base register.\n@@ -2810,3 +3276,60 @@\n-  LIR_Opr reg = rlock_result(x);\n-  __ cmp(lir_cond(x->cond()), left.result(), right.result());\n-  __ cmove(lir_cond(x->cond()), t_val.result(), f_val.result(), reg, as_BasicType(x->x()->type()));\n+  if (x->substitutability_check()) {\n+    substitutability_check(x, left, right, t_val, f_val);\n+  } else {\n+    LIR_Opr reg = rlock_result(x);\n+    __ cmp(lir_cond(x->cond()), left.result(), right.result());\n+    __ cmove(lir_cond(x->cond()), t_val.result(), f_val.result(), reg, as_BasicType(x->x()->type()));\n+  }\n+}\n+\n+void LIRGenerator::substitutability_check(IfOp* x, LIRItem& left, LIRItem& right, LIRItem& t_val, LIRItem& f_val) {\n+  assert(x->cond() == If::eql || x->cond() == If::neq, \"must be\");\n+  bool is_acmpeq = (x->cond() == If::eql);\n+  LIR_Opr equal_result     = is_acmpeq ? t_val.result() : f_val.result();\n+  LIR_Opr not_equal_result = is_acmpeq ? f_val.result() : t_val.result();\n+  LIR_Opr result = rlock_result(x);\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+\n+  substitutability_check_common(x->x(), x->y(), left, right, equal_result, not_equal_result, result, info);\n+}\n+\n+void LIRGenerator::substitutability_check(If* x, LIRItem& left, LIRItem& right) {\n+  LIR_Opr equal_result     = LIR_OprFact::intConst(1);\n+  LIR_Opr not_equal_result = LIR_OprFact::intConst(0);\n+  LIR_Opr result = new_register(T_INT);\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n+\n+  substitutability_check_common(x->x(), x->y(), left, right, equal_result, not_equal_result, result, info);\n+\n+  assert(x->cond() == If::eql || x->cond() == If::neq, \"must be\");\n+  __ cmp(lir_cond(x->cond()), result, equal_result);\n+}\n+\n+void LIRGenerator::substitutability_check_common(Value left_val, Value right_val, LIRItem& left, LIRItem& right,\n+                                                 LIR_Opr equal_result, LIR_Opr not_equal_result, LIR_Opr result,\n+                                                 CodeEmitInfo* info) {\n+  LIR_Opr tmp1 = LIR_OprFact::illegalOpr;\n+  LIR_Opr tmp2 = LIR_OprFact::illegalOpr;\n+  LIR_Opr left_klass_op = LIR_OprFact::illegalOpr;\n+  LIR_Opr right_klass_op = LIR_OprFact::illegalOpr;\n+\n+  ciKlass* left_klass  = left_val ->as_loaded_klass_or_null();\n+  ciKlass* right_klass = right_val->as_loaded_klass_or_null();\n+\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {\n+    init_temps_for_substitutability_check(tmp1, tmp2);\n+  }\n+\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n+    \/\/ No need to load klass -- the operands are statically known to be the same inline klass.\n+  } else {\n+    BasicType t_klass = UseCompressedOops ? T_INT : T_METADATA;\n+    left_klass_op = new_register(t_klass);\n+    right_klass_op = new_register(t_klass);\n+  }\n+\n+  CodeStub* slow_path = new SubstitutabilityCheckStub(left.result(), right.result(), info);\n+  __ substitutability_check(result, left.result(), right.result(), equal_result, not_equal_result,\n+                            tmp1, tmp2,\n+                            left_klass, right_klass, left_klass_op, right_klass_op, info, slow_path);\n@@ -3088,1 +3611,1 @@\n-    ciReturnTypeEntry* ret = data->is_CallTypeData() ? ((ciCallTypeData*)data)->ret() : ((ciVirtualCallTypeData*)data)->ret();\n+    ciSingleTypeEntry* ret = data->is_CallTypeData() ? ((ciCallTypeData*)data)->ret() : ((ciVirtualCallTypeData*)data)->ret();\n@@ -3109,0 +3632,47 @@\n+bool LIRGenerator::profile_inline_klass(ciMethodData* md, ciProfileData* data, Value value, int flag) {\n+  ciKlass* klass = value->as_loaded_klass_or_null();\n+  if (klass != nullptr) {\n+    if (klass->is_inlinetype()) {\n+      profile_flags(md, data, flag, lir_cond_always);\n+    } else if (klass->can_be_inline_klass()) {\n+      return false;\n+    }\n+  } else {\n+    return false;\n+  }\n+  return true;\n+}\n+\n+\n+void LIRGenerator::do_ProfileACmpTypes(ProfileACmpTypes* x) {\n+  ciMethod* method = x->method();\n+  assert(method != nullptr, \"method should be set if branch is profiled\");\n+  ciMethodData* md = method->method_data_or_null();\n+  assert(md != nullptr, \"Sanity\");\n+  ciProfileData* data = md->bci_to_data(x->bci());\n+  assert(data != nullptr, \"must have profiling data\");\n+  assert(data->is_ACmpData(), \"need BranchData for two-way branches\");\n+  ciACmpData* acmp = (ciACmpData*)data;\n+  LIR_Opr mdp = LIR_OprFact::illegalOpr;\n+  profile_type(md, md->byte_offset_of_slot(acmp, ACmpData::left_offset()), 0,\n+               acmp->left()->type(), x->left(), mdp, !x->left_maybe_null(), nullptr, nullptr);\n+  int flags_offset = md->byte_offset_of_slot(data, DataLayout::flags_offset());\n+  if (!profile_inline_klass(md, acmp, x->left(), ACmpData::left_inline_type_byte_constant())) {\n+    LIR_Opr mdp = new_register(T_METADATA);\n+    __ metadata2reg(md->constant_encoding(), mdp);\n+    LIRItem value(x->left(), this);\n+    value.load_item();\n+    __ profile_inline_type(new LIR_Address(mdp, flags_offset, T_INT), value.result(), ACmpData::left_inline_type_byte_constant(), new_register(T_INT), !x->left_maybe_null());\n+  }\n+  profile_type(md, md->byte_offset_of_slot(acmp, ACmpData::left_offset()),\n+               in_bytes(ACmpData::right_offset()) - in_bytes(ACmpData::left_offset()),\n+               acmp->right()->type(), x->right(), mdp, !x->right_maybe_null(), nullptr, nullptr);\n+  if (!profile_inline_klass(md, acmp, x->right(), ACmpData::right_inline_type_byte_constant())) {\n+    LIR_Opr mdp = new_register(T_METADATA);\n+    __ metadata2reg(md->constant_encoding(), mdp);\n+    LIRItem value(x->right(), this);\n+    value.load_item();\n+    __ profile_inline_type(new LIR_Address(mdp, flags_offset, T_INT), value.result(), ACmpData::right_inline_type_byte_constant(), new_register(T_INT), !x->left_maybe_null());\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":627,"deletions":57,"binary":false,"changes":684,"status":"modified"},{"patch":"@@ -24,0 +24,1 @@\n+\n@@ -53,0 +54,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -87,0 +89,1 @@\n+#include \"utilities\/stringUtils.hpp\"\n@@ -159,0 +162,2 @@\n+#define CONSTANT_CLASS_DESCRIPTORS        70\n+\n@@ -197,1 +202,1 @@\n-      case JVM_CONSTANT_Class : {\n+      case JVM_CONSTANT_Class: {\n@@ -501,0 +506,3 @@\n+\n+        Symbol* const name = cp->symbol_at(class_index);\n+        const unsigned int name_len = name->utf8_length();\n@@ -710,1 +718,1 @@\n-            } else if (!Signature::is_void_method(signature)) { \/\/ must have void signature.\n+            } else if (!Signature::is_void_method(signature)) {  \/\/ must have void signature.\n@@ -730,2 +738,3 @@\n-            if (ref_kind == JVM_REF_newInvokeSpecial) {\n-              if (name != vmSymbols::object_initializer_name()) {\n+\n+            if (name != vmSymbols::object_initializer_name()) { \/\/ !<init>\n+              if (ref_kind == JVM_REF_newInvokeSpecial) {\n@@ -737,2 +746,10 @@\n-            } else {\n-              if (name == vmSymbols::object_initializer_name()) {\n+            } else { \/\/ <init>\n+              \/\/ The allowed invocation mode of <init> depends on its signature.\n+              \/\/ This test corresponds to verify_invoke_instructions in the verifier.\n+              const int signature_ref_index =\n+                cp->signature_ref_index_at(name_and_type_ref_index);\n+              const Symbol* const signature = cp->symbol_at(signature_ref_index);\n+              if (signature->is_void_method_signature()\n+                  && ref_kind == JVM_REF_newInvokeSpecial) {\n+                \/\/ OK, could be a constructor call\n+              } else {\n@@ -790,4 +807,13 @@\n-\/\/ Side-effects: populates the _local_interfaces field\n-void ClassFileParser::parse_interfaces(const ClassFileStream* const stream,\n-                                       const int itfs_len,\n-                                       ConstantPool* const cp,\n+static void check_identity_and_value_modifiers(ClassFileParser* current, const InstanceKlass* super_type, TRAPS) {\n+  assert(super_type != nullptr,\"Method doesn't support null super type\");\n+  if (super_type->access_flags().is_identity_class() && !current->access_flags().is_identity_class()\n+      && super_type->name() != vmSymbols::java_lang_Object()) {\n+      THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                err_msg(\"Value type %s has an identity type as supertype\",\n+                current->class_name()->as_klass_external_name()));\n+  }\n+}\n+\n+void ClassFileParser::parse_interfaces(const ClassFileStream* stream,\n+                                       int itfs_len,\n+                                       ConstantPool* cp,\n@@ -795,0 +821,6 @@\n+                                       \/\/ FIXME: lots of these functions\n+                                       \/\/ declare their parameters as const,\n+                                       \/\/ which adds only noise to the code.\n+                                       \/\/ Remove the spurious const modifiers.\n+                                       \/\/ Many are of the form \"const int x\"\n+                                       \/\/ or \"T* const x\".\n@@ -802,0 +834,1 @@\n+\n@@ -804,3 +837,2 @@\n-    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, nullptr, CHECK);\n-\n-    int index;\n+    _local_interface_indexes = new GrowableArray<u2>(itfs_len);\n+    int index = 0;\n@@ -809,1 +841,0 @@\n-      Klass* interf;\n@@ -814,29 +845,1 @@\n-      if (cp->tag_at(interface_index).is_klass()) {\n-        interf = cp->resolved_klass_at(interface_index);\n-      } else {\n-        Symbol* const unresolved_klass  = cp->klass_name_at(interface_index);\n-\n-        \/\/ Don't need to check legal name because it's checked when parsing constant pool.\n-        \/\/ But need to make sure it's not an array type.\n-        guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,\n-                           \"Bad interface name in class file %s\", CHECK);\n-\n-        \/\/ Call resolve on the interface class name with class circularity checking\n-        interf = SystemDictionary::resolve_super_or_fail(_class_name,\n-                                                         unresolved_klass,\n-                                                         Handle(THREAD, _loader_data->class_loader()),\n-                                                         false, CHECK);\n-      }\n-\n-      if (!interf->is_interface()) {\n-        THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n-                  err_msg(\"class %s can not implement %s, because it is not an interface (%s)\",\n-                          _class_name->as_klass_external_name(),\n-                          interf->external_name(),\n-                          interf->class_in_module_of_loader()));\n-      }\n-\n-      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n-        *has_nonstatic_concrete_methods = true;\n-      }\n-      _local_interfaces->at_put(index, InstanceKlass::cast(interf));\n+      _local_interface_indexes->at_put_grow(index, interface_index);\n@@ -854,2 +857,1 @@\n-      const InstanceKlass* const k = _local_interfaces->at(index);\n-      Symbol* interface_name = k->name();\n+      Symbol* interface_name = cp->klass_name_at(_local_interface_indexes->at(index));\n@@ -942,0 +944,2 @@\n+    _jdk_internal_LooselyConsistentValue,\n+    _jdk_internal_NullRestricted,\n@@ -1368,1 +1372,1 @@\n-                                   bool is_interface,\n+                                   AccessFlags class_access_flags,\n@@ -1381,0 +1385,1 @@\n+  bool is_inline_type = !class_access_flags.is_identity_class() && !class_access_flags.is_abstract();\n@@ -1388,1 +1393,5 @@\n-  const int total_fields = length + num_injected;\n+\n+  \/\/ two more slots are required for inline classes:\n+  \/\/ one for the static field with a reference to the pre-allocated default value\n+  \/\/ one for the field the JVM injects when detecting an empty inline class\n+  const int total_fields = length + num_injected + (is_inline_type ? 2 : 0);\n@@ -1394,0 +1403,1 @@\n+  int instance_fields_count = 0;\n@@ -1399,0 +1409,7 @@\n+    jint recognized_modifiers = JVM_RECOGNIZED_FIELD_MODIFIERS;\n+    if (!supports_inline_types()) {\n+      recognized_modifiers &= ~JVM_ACC_STRICT;\n+    }\n+\n+    const jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+    verify_legal_field_modifiers(flags, class_access_flags, CHECK);\n@@ -1400,2 +1417,0 @@\n-    const jint flags = cfs->get_u2_fast() & JVM_RECOGNIZED_FIELD_MODIFIERS;\n-    verify_legal_field_modifiers(flags, is_interface, CHECK);\n@@ -1418,0 +1433,1 @@\n+    if (!access_flags.is_static()) instance_fields_count++;\n@@ -1425,0 +1441,2 @@\n+    bool is_null_restricted = false;\n+\n@@ -1444,0 +1462,18 @@\n+        if (parsed_annotations.has_annotation(AnnotationCollector::_jdk_internal_NullRestricted)) {\n+          if (!Signature::has_envelope(sig)) {\n+            Exceptions::fthrow(\n+              THREAD_AND_LOCATION,\n+              vmSymbols::java_lang_ClassFormatError(),\n+              \"Illegal use of @jdk.internal.vm.annotation.NullRestricted annotation on field %s.%s with signature %s (primitive types can never be null)\",\n+              class_name()->as_C_string(), name->as_C_string(), sig->as_C_string());\n+          }\n+          const bool is_strict = (flags & JVM_ACC_STRICT) != 0;\n+          if (!is_strict) {\n+            Exceptions::fthrow(\n+              THREAD_AND_LOCATION,\n+              vmSymbols::java_lang_ClassFormatError(),\n+              \"Illegal use of @jdk.internal.vm.annotation.NullRestricted annotation on field %s.%s which doesn't have the @jdk.internal.vm.annotation.Strict annotation\",\n+              class_name()->as_C_string(), name->as_C_string());\n+          }\n+          is_null_restricted = true;\n+        }\n@@ -1466,0 +1502,4 @@\n+    if (is_null_restricted) {\n+      fieldFlags.update_null_free_inline_type(true);\n+    }\n+\n@@ -1482,0 +1522,3 @@\n+    if (access_flags.is_strict() && access_flags.is_static()) {\n+      _has_strict_static_fields = true;\n+    }\n@@ -1486,1 +1529,0 @@\n-  int index = length;\n@@ -1514,3 +1556,2 @@\n-      fi.set_index(index);\n-      _temp_field_info->append(fi);\n-      index++;\n+      int idx = _temp_field_info->append(fi);\n+      _temp_field_info->adr_at(idx)->set_index(idx);\n@@ -1520,1 +1561,17 @@\n-  assert(_temp_field_info->length() == index, \"Must be\");\n+  if (is_inline_type) {\n+    \/\/ Inject static \".null_reset\" field. This is an all-zero value with its null-channel set to zero.\n+    \/\/ IT should never be seen by user code, it is used when writing \"null\" to a nullable flat field\n+    \/\/ The all-zero value ensure that any embedded oop will be set to null, to avoid keeping dead objects\n+    \/\/ alive.\n+    FieldInfo::FieldFlags fflags2(0);\n+    fflags2.update_injected(true);\n+    AccessFlags aflags2(JVM_ACC_STATIC);\n+    FieldInfo fi2(aflags2,\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(null_reset_value_name)),\n+                 (u2)vmSymbols::as_int(VM_SYMBOL_ENUM_NAME(object_signature)),\n+                 0,\n+                 fflags2);\n+    int idx2 = _temp_field_info->append(fi2);\n+    _temp_field_info->adr_at(idx2)->set_index(idx2);\n+    _static_oop_count++;\n+  }\n@@ -1900,0 +1957,8 @@\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_LooselyConsistentValue_signature): {\n+      if (_location != _in_class)   break; \/\/ only allow for classes\n+      return _jdk_internal_LooselyConsistentValue;\n+    }\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_NullRestricted_signature): {\n+      if (_location != _in_field)   break; \/\/ only allow for fields\n+      return _jdk_internal_NullRestricted;\n+    }\n@@ -2136,0 +2201,2 @@\n+                                      bool is_value_class,\n+                                      bool is_abstract_class,\n@@ -2177,1 +2244,1 @@\n-    verify_legal_method_modifiers(flags, is_interface, name, CHECK_NULL);\n+    verify_legal_method_modifiers(flags, access_flags() , name, CHECK_NULL);\n@@ -2185,0 +2252,9 @@\n+  if (EnableValhalla) {\n+    if (((flags & JVM_ACC_SYNCHRONIZED) == JVM_ACC_SYNCHRONIZED)\n+        && ((flags & JVM_ACC_STATIC) == 0 )\n+        && !_access_flags.is_identity_class()) {\n+      classfile_parse_error(\"Invalid synchronized method in non-identity class %s\", THREAD);\n+        return nullptr;\n+    }\n+  }\n+\n@@ -2719,0 +2795,2 @@\n+                                    bool is_value_class,\n+                                    bool is_abstract_type,\n@@ -2743,0 +2821,2 @@\n+                                    is_value_class,\n+                                    is_abstract_type,\n@@ -3008,0 +3088,1 @@\n+\n@@ -3016,0 +3097,1 @@\n+\n@@ -3021,0 +3103,8 @@\n+    if (!supports_inline_types()) {\n+      const bool is_module = (flags & JVM_ACC_MODULE) != 0;\n+      const bool is_interface = (flags & JVM_ACC_INTERFACE) != 0;\n+      if (!is_module && !is_interface) {\n+        flags |= JVM_ACC_IDENTITY;\n+      }\n+    }\n+\n@@ -3126,0 +3216,43 @@\n+u2 ClassFileParser::parse_classfile_loadable_descriptors_attribute(const ClassFileStream* const cfs,\n+                                                                   const u1* const loadable_descriptors_attribute_start,\n+                                                                   TRAPS) {\n+  const u1* const current_mark = cfs->current();\n+  u2 length = 0;\n+  if (loadable_descriptors_attribute_start != nullptr) {\n+    cfs->set_current(loadable_descriptors_attribute_start);\n+    cfs->guarantee_more(2, CHECK_0);  \/\/ length\n+    length = cfs->get_u2_fast();\n+  }\n+  const int size = length;\n+  Array<u2>* const loadable_descriptors = MetadataFactory::new_array<u2>(_loader_data, size, CHECK_0);\n+  _loadable_descriptors = loadable_descriptors;\n+  if (length > 0) {\n+    int index = 0;\n+    cfs->guarantee_more(2 * length, CHECK_0);\n+    for (int n = 0; n < length; n++) {\n+      const u2 descriptor_index = cfs->get_u2_fast();\n+      guarantee_property(\n+        valid_symbol_at(descriptor_index),\n+        \"LoadableDescriptors descriptor_index %u has bad constant type in class file %s\",\n+        descriptor_index, CHECK_0);\n+      Symbol* descriptor = _cp->symbol_at(descriptor_index);\n+      bool valid = legal_field_signature(descriptor, CHECK_0);\n+      if(!valid) {\n+        ResourceMark rm(THREAD);\n+        Exceptions::fthrow(THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_ClassFormatError(),\n+          \"Descriptor from LoadableDescriptors attribute at index \\\"%d\\\" in class %s has illegal signature \\\"%s\\\"\",\n+          descriptor_index, _class_name->as_C_string(), descriptor->as_C_string());\n+        return 0;\n+      }\n+      loadable_descriptors->at_put(index++, descriptor_index);\n+    }\n+    assert(index == size, \"wrong size\");\n+  }\n+\n+  \/\/ Restore buffer's current position.\n+  cfs->set_current(current_mark);\n+\n+  return length;\n+}\n+\n@@ -3391,0 +3524,2 @@\n+  \/\/ Set _loadable_descriptors attribute to default sentinel\n+  _loadable_descriptors = Universe::the_empty_short_array();\n@@ -3397,0 +3532,1 @@\n+  bool parsed_loadable_descriptors_attribute = false;\n@@ -3418,0 +3554,2 @@\n+  const u1* loadable_descriptors_attribute_start = nullptr;\n+  u4  loadable_descriptors_attribute_length = 0;\n@@ -3633,0 +3771,9 @@\n+            if (EnableValhalla && tag == vmSymbols::tag_loadable_descriptors()) {\n+              if (parsed_loadable_descriptors_attribute) {\n+                classfile_parse_error(\"Multiple LoadableDescriptors attributes in class file %s\", CHECK);\n+                return;\n+              }\n+              parsed_loadable_descriptors_attribute = true;\n+              loadable_descriptors_attribute_start = cfs->current();\n+              loadable_descriptors_attribute_length = attribute_length;\n+            }\n@@ -3709,0 +3856,12 @@\n+  if (parsed_loadable_descriptors_attribute) {\n+    const u2 num_classes = parse_classfile_loadable_descriptors_attribute(\n+                            cfs,\n+                            loadable_descriptors_attribute_start,\n+                            CHECK);\n+    if (_need_verify) {\n+      guarantee_property(\n+        loadable_descriptors_attribute_length == sizeof(num_classes) + sizeof(u2) * num_classes,\n+        \"Wrong LoadableDescriptors attribute length in class file %s\", CHECK);\n+    }\n+  }\n+\n@@ -3775,0 +3934,1 @@\n+  this_klass->set_loadable_descriptors(_loadable_descriptors);\n@@ -3778,0 +3938,1 @@\n+  this_klass->set_inline_layout_info_array(_inline_layout_info_array);\n@@ -3816,2 +3977,1 @@\n-                       \"Invalid superclass index %u in class file %s\",\n-                       super_class_index,\n+                       \"Invalid superclass index 0 in class file %s\",\n@@ -4009,0 +4169,6 @@\n+bool ClassFileParser::supports_inline_types() const {\n+  \/\/ Inline types are only supported by class file version 70.65535 and later\n+  return _major_version > JAVA_26_VERSION ||\n+         (_major_version == JAVA_26_VERSION && _minor_version == JAVA_PREVIEW_MINOR_VERSION);\n+}\n+\n@@ -4052,3 +4218,4 @@\n-  } else if (max_transitive_size == local_size) {\n-    \/\/ only local interfaces added, share local interface array\n-    return local_ifs;\n+    \/\/ The three lines below are commented to work around bug JDK-8245487\n+\/\/  } else if (max_transitive_size == local_size) {\n+\/\/    \/\/ only local interfaces added, share local interface array\n+\/\/    return local_ifs;\n@@ -4075,0 +4242,1 @@\n+\n@@ -4105,0 +4273,10 @@\n+    \/\/ The JVMS says that super classes for value types must not have the ACC_IDENTITY\n+    \/\/ flag set. But, java.lang.Object must still be allowed to be a direct super class\n+    \/\/ for a value classes.  So, it is treated as a special case for now.\n+    if (!this_klass->access_flags().is_identity_class() &&\n+        super->name() != vmSymbols::java_lang_Object() &&\n+        super->is_identity_class()) {\n+      classfile_icce_error(\"value class %s cannot inherit from class %s\", super, THREAD);\n+      return;\n+    }\n+\n@@ -4297,1 +4475,1 @@\n-  const bool is_super      = (flags & JVM_ACC_SUPER)      != 0;\n+  const bool is_identity   = (flags & JVM_ACC_IDENTITY)   != 0;\n@@ -4301,0 +4479,2 @@\n+  const bool valid_value_class = is_identity || is_interface ||\n+                                 (supports_inline_types() && (!is_identity && (is_abstract || is_final)));\n@@ -4304,2 +4484,3 @@\n-      (is_interface && major_gte_1_5 && (is_super || is_enum)) ||\n-      (!is_interface && major_gte_1_5 && is_annotation)) {\n+      (is_interface && major_gte_1_5 && (is_identity || is_enum)) ||   \/\/  ACC_SUPER (now ACC_IDENTITY) was illegal for interfaces\n+      (!is_interface && major_gte_1_5 && is_annotation) ||\n+      (!valid_value_class)) {\n@@ -4307,0 +4488,4 @@\n+    const char* class_note = \"\";\n+    if (!valid_value_class) {\n+      class_note = \" (a value class must be final or else abstract)\";\n+    }\n@@ -4400,2 +4585,2 @@\n-void ClassFileParser::verify_legal_field_modifiers(jint flags,\n-                                                   bool is_interface,\n+void ClassFileParser:: verify_legal_field_modifiers(jint flags,\n+                                                   AccessFlags class_access_flags,\n@@ -4413,0 +4598,1 @@\n+  const bool is_strict    = (flags & JVM_ACC_STRICT)    != 0;\n@@ -4415,1 +4601,2 @@\n-  bool is_illegal = false;\n+  const bool is_interface = class_access_flags.is_interface();\n+  const bool is_identity_class = class_access_flags.is_identity_class();\n@@ -4417,9 +4604,30 @@\n-  if (is_interface) {\n-    if (!is_public || !is_static || !is_final || is_private ||\n-        is_protected || is_volatile || is_transient ||\n-        (major_gte_1_5 && is_enum)) {\n-      is_illegal = true;\n-    }\n-  } else { \/\/ not interface\n-    if (has_illegal_visibility(flags) || (is_final && is_volatile)) {\n-      is_illegal = true;\n+  bool is_illegal = false;\n+  const char* error_msg = \"\";\n+\n+  \/\/ There is some overlap in the checks that apply, for example interface fields\n+  \/\/ must be static, static fields can't be strict, and therefore interfaces can't\n+  \/\/ have strict fields. So we don't have to check every possible invalid combination\n+  \/\/ individually as long as all are covered. Once we have found an illegal combination\n+  \/\/ we can stop checking.\n+\n+  if (!is_illegal) {\n+    if (is_interface) {\n+      if (!is_public || !is_static || !is_final || is_private ||\n+          is_protected || is_volatile || is_transient ||\n+          (major_gte_1_5 && is_enum)) {\n+        is_illegal = true;\n+        error_msg = \"interface fields must be public, static and final, and may be synthetic\";\n+      }\n+    } else { \/\/ not interface\n+      if (has_illegal_visibility(flags)) {\n+        is_illegal = true;\n+        error_msg = \"invalid visibility flags for class field\";\n+      } else if (is_final && is_volatile) {\n+        is_illegal = true;\n+        error_msg = \"fields cannot be final and volatile\";\n+      } else if (supports_inline_types()) {\n+        if (!is_identity_class && !is_static && (!is_strict || !is_final)) {\n+          is_illegal = true;\n+          error_msg = \"value class fields must be either non-static final and strict, or static\";\n+        }\n+      }\n@@ -4435,2 +4643,2 @@\n-      \"Illegal field modifiers in class %s: 0x%X\",\n-      _class_name->as_C_string(), flags);\n+      \"Illegal field modifiers (%s) in class %s: 0x%X\",\n+      error_msg, _class_name->as_C_string(), flags);\n@@ -4442,1 +4650,1 @@\n-                                                    bool is_interface,\n+                                                    AccessFlags class_access_flags,\n@@ -4461,0 +4669,4 @@\n+  \/\/ LW401 CR required: removal of value factories support\n+  const bool is_interface    = class_access_flags.is_interface();\n+  const bool is_identity_class = class_access_flags.is_identity_class();\n+  const bool is_abstract_class = class_access_flags.is_abstract();\n@@ -4464,0 +4676,1 @@\n+  const char* class_note = \"\";\n@@ -4503,4 +4716,9 @@\n-        if (is_abstract) {\n-          if ((is_final || is_native || is_private || is_static ||\n-              (major_gte_1_5 && (is_synchronized || (!major_gte_17 && is_strict))))) {\n-            is_illegal = true;\n+        if (!is_identity_class && is_synchronized && !is_static) {\n+          is_illegal = true;\n+          class_note = \" (not an identity class)\";\n+        } else {\n+          if (is_abstract) {\n+            if ((is_final || is_native || is_private || is_static ||\n+                (major_gte_1_5 && (is_synchronized || (!major_gte_17 && is_strict))))) {\n+              is_illegal = true;\n+            }\n@@ -4519,2 +4737,3 @@\n-      \"Method %s in class %s has illegal modifiers: 0x%X\",\n-      name->as_C_string(), _class_name->as_C_string(), flags);\n+      \"Method %s in class %s%s has illegal modifiers: 0x%X\",\n+      name->as_C_string(), _class_name->as_C_string(),\n+      class_note, flags);\n@@ -4578,0 +4797,9 @@\n+bool ClassFileParser::is_class_in_loadable_descriptors_attribute(Symbol *klass) {\n+  if (_loadable_descriptors == nullptr) return false;\n+  for (int i = 0; i < _loadable_descriptors->length(); i++) {\n+        Symbol* class_name = _cp->symbol_at(_loadable_descriptors->at(i));\n+        if (class_name == klass) return true;\n+  }\n+  return false;\n+}\n+\n@@ -4679,1 +4907,2 @@\n-    case JVM_SIGNATURE_CLASS: {\n+    case JVM_SIGNATURE_CLASS:\n+    {\n@@ -4690,1 +4919,1 @@\n-        \/\/ Skip leading 'L' and ignore first appearance of ';'\n+        \/\/ Skip leading 'L' or 'Q' and ignore first appearance of ';'\n@@ -4746,0 +4975,4 @@\n+    } else if ((_major_version >= CONSTANT_CLASS_DESCRIPTORS || _class_name->starts_with(\"jdk\/internal\/reflect\/\"))\n+                   && bytes[length - 1] == ';' ) {\n+      \/\/ Support for L...; descriptors\n+      legal = verify_unqualified_name(bytes + 1, length - 2, LegalClass);\n@@ -4813,1 +5046,2 @@\n-      if (name == vmSymbols::object_initializer_name() || name == vmSymbols::class_initializer_name()) {\n+      if (name == vmSymbols::object_initializer_name() ||\n+          name == vmSymbols::class_initializer_name()) {\n@@ -4840,0 +5074,10 @@\n+bool ClassFileParser::legal_field_signature(const Symbol* signature, TRAPS) const {\n+  const char* const bytes = (const char*)signature->bytes();\n+  const unsigned int length = signature->utf8_length();\n+  const char* const p = skip_over_field_signature(bytes, false, length, CHECK_false);\n+\n+  if (p == nullptr || (p - bytes) != (int)length) {\n+    return false;\n+  }\n+  return true;\n+}\n@@ -4875,3 +5119,3 @@\n-      name->char_at(0) == JVM_SIGNATURE_SPECIAL &&\n-      sig_length > 0 &&\n-      signature->char_at(sig_length - 1) != JVM_SIGNATURE_VOID) {\n+    name->char_at(0) == JVM_SIGNATURE_SPECIAL &&\n+    sig_length > 0 &&\n+    signature->char_at(sig_length - 1) != JVM_SIGNATURE_VOID) {\n@@ -4928,2 +5172,2 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->_static_field_size;\n+  assert(_layout_info != nullptr, \"invariant\");\n+  return _layout_info->_static_field_size;\n@@ -4933,2 +5177,2 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->oop_map_blocks->_nonstatic_oop_map_count;\n+  assert(_layout_info != nullptr, \"invariant\");\n+  return _layout_info->oop_map_blocks->_nonstatic_oop_map_count;\n@@ -4938,2 +5182,2 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->_instance_size;\n+  assert(_layout_info != nullptr, \"invariant\");\n+  return _layout_info->_instance_size;\n@@ -5054,1 +5298,0 @@\n-\n@@ -5077,3 +5320,3 @@\n-  assert(_field_info != nullptr, \"invariant\");\n-  assert(ik->static_field_size() == _field_info->_static_field_size, \"sanity\");\n-  assert(ik->nonstatic_oop_map_count() == _field_info->oop_map_blocks->_nonstatic_oop_map_count,\n+  assert(_layout_info != nullptr, \"invariant\");\n+  assert(ik->static_field_size() == _layout_info->_static_field_size, \"sanity\");\n+  assert(ik->nonstatic_oop_map_count() == _layout_info->oop_map_blocks->_nonstatic_oop_map_count,\n@@ -5083,1 +5326,1 @@\n-  assert(ik->size_helper() == _field_info->_instance_size, \"sanity\");\n+  assert(ik->size_helper() == _layout_info->_instance_size, \"sanity\");\n@@ -5089,2 +5332,12 @@\n-  ik->set_nonstatic_field_size(_field_info->_nonstatic_field_size);\n-  ik->set_has_nonstatic_fields(_field_info->_has_nonstatic_fields);\n+  ik->set_nonstatic_field_size(_layout_info->_nonstatic_field_size);\n+  ik->set_has_nonstatic_fields(_layout_info->_has_nonstatic_fields);\n+  ik->set_has_strict_static_fields(_has_strict_static_fields);\n+\n+  if (_layout_info->_is_naturally_atomic) {\n+    ik->set_is_naturally_atomic();\n+  }\n+\n+  if (_layout_info->_must_be_atomic) {\n+    ik->set_must_be_atomic();\n+  }\n+\n@@ -5096,0 +5349,3 @@\n+  if (ik->is_inline_klass()) {\n+    InlineKlass::cast(ik)->init_fixed_block();\n+  }\n@@ -5110,0 +5366,1 @@\n+  assert(nullptr == _loadable_descriptors, \"invariant\");\n@@ -5113,0 +5370,1 @@\n+  assert(nullptr == _inline_layout_info_array, \"invariant\");\n@@ -5189,1 +5447,1 @@\n-  OopMapBlocksBuilder* oop_map_blocks = _field_info->oop_map_blocks;\n+  OopMapBlocksBuilder* oop_map_blocks = _layout_info->oop_map_blocks;\n@@ -5250,0 +5508,15 @@\n+  if (is_inline_type()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    vk->set_payload_alignment(_layout_info->_payload_alignment);\n+    vk->set_payload_offset(_layout_info->_payload_offset);\n+    vk->set_payload_size_in_bytes(_layout_info->_payload_size_in_bytes);\n+    vk->set_non_atomic_size_in_bytes(_layout_info->_non_atomic_size_in_bytes);\n+    vk->set_non_atomic_alignment(_layout_info->_non_atomic_alignment);\n+    vk->set_atomic_size_in_bytes(_layout_info->_atomic_layout_size_in_bytes);\n+    vk->set_nullable_size_in_bytes(_layout_info->_nullable_layout_size_in_bytes);\n+    vk->set_null_marker_offset(_layout_info->_null_marker_offset);\n+    vk->set_null_reset_value_offset(_layout_info->_null_reset_value_offset);\n+    if (_layout_info->_is_empty_inline_klass) vk->set_is_empty_inline_type();\n+    vk->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -5332,0 +5605,1 @@\n+  _loadable_descriptors(nullptr),\n@@ -5334,0 +5608,1 @@\n+  _local_interface_indexes(nullptr),\n@@ -5343,1 +5618,2 @@\n-  _field_info(nullptr),\n+  _layout_info(nullptr),\n+  _inline_layout_info_array(nullptr),\n@@ -5372,0 +5648,5 @@\n+  _has_strict_static_fields(false),\n+  _has_inline_type_fields(false),\n+  _is_naturally_atomic(false),\n+  _must_be_atomic(true),\n+  _has_loosely_consistent_annotation(false),\n@@ -5409,0 +5690,1 @@\n+  _loadable_descriptors = nullptr;\n@@ -5413,0 +5695,1 @@\n+  _inline_layout_info_array = nullptr;\n@@ -5432,0 +5715,4 @@\n+  if (_inline_layout_info_array != nullptr) {\n+    MetadataFactory::free_array<InlineLayoutInfo>(_loader_data, _inline_layout_info_array);\n+  }\n+\n@@ -5454,0 +5741,4 @@\n+  if (_loadable_descriptors != nullptr && _loadable_descriptors != Universe::the_empty_short_array()) {\n+    MetadataFactory::free_array<u2>(_loader_data, _loadable_descriptors);\n+  }\n+\n@@ -5552,0 +5843,9 @@\n+  \/\/ Fixing ACC_SUPER\/ACC_IDENTITY for old class files\n+  if (!supports_inline_types()) {\n+    const bool is_module = (flags & JVM_ACC_MODULE) != 0;\n+    const bool is_interface = (flags & JVM_ACC_INTERFACE) != 0;\n+    if (!is_module && !is_interface) {\n+      flags |= JVM_ACC_IDENTITY;\n+    }\n+  }\n+\n@@ -5655,2 +5955,0 @@\n-  assert(_local_interfaces != nullptr, \"invariant\");\n-\n@@ -5659,1 +5957,1 @@\n-               _access_flags.is_interface(),\n+               _access_flags,\n@@ -5669,1 +5967,3 @@\n-                _access_flags.is_interface(),\n+                is_interface(),\n+                !is_identity_class(),\n+                is_abstract_class(),\n@@ -5757,1 +6057,1 @@\n-    if (_access_flags.is_interface()) {\n+    if (is_interface()) {\n@@ -5779,0 +6079,14 @@\n+    if (_super_klass->is_interface()) {\n+      classfile_icce_error(\"class %s has interface %s as super class\", _super_klass, THREAD);\n+      return;\n+    }\n+\n+    if (_super_klass->is_final()) {\n+      classfile_icce_error(\"class %s cannot inherit from final class %s\", _super_klass, THREAD);\n+      return;\n+    }\n+\n+    if (EnableValhalla) {\n+      check_identity_and_value_modifiers(this, _super_klass, CHECK);\n+    }\n+\n@@ -5782,0 +6096,1 @@\n+  }\n@@ -5783,3 +6098,69 @@\n-    if (_super_klass->is_interface()) {\n-      classfile_icce_error(\"class %s has interface %s as super class\", _super_klass, THREAD);\n-      return;\n+  if (_parsed_annotations->has_annotation(AnnotationCollector::_jdk_internal_LooselyConsistentValue) && _access_flags.is_identity_class()) {\n+    THROW_MSG(vmSymbols::java_lang_ClassFormatError(),\n+          err_msg(\"class %s cannot have annotation jdk.internal.vm.annotation.LooselyConsistentValue, because it is not a value class\",\n+                  _class_name->as_klass_external_name()));\n+  }\n+\n+  \/\/ Determining is the class allows tearing or not (default is not)\n+  if (EnableValhalla && !_access_flags.is_identity_class()) {\n+    if (_parsed_annotations->has_annotation(ClassAnnotationCollector::_jdk_internal_LooselyConsistentValue)\n+        && (_super_klass == vmClasses::Object_klass() || !_super_klass->must_be_atomic())) {\n+      \/\/ Conditions above are not sufficient to determine atomicity requirements,\n+      \/\/ the presence of fields with atomic requirements could force the current class to have atomicy requirements too\n+      \/\/ Marking as not needing atomicity for now, can be updated when computing the fields layout\n+      \/\/ The InstanceKlass must be filled with the value from the FieldLayoutInfo returned by\n+      \/\/ the FieldLayoutBuilder, not with this _must_be_atomic field.\n+      _must_be_atomic = false;\n+    }\n+    \/\/ Apply VM options override\n+    if (*ForceNonTearable != '\\0') {\n+      \/\/ Allow a command line switch to force the same atomicity property:\n+      const char* class_name_str = _class_name->as_C_string();\n+      if (StringUtils::class_list_match(ForceNonTearable, class_name_str)) {\n+        _must_be_atomic = true;\n+      }\n+    }\n+  }\n+\n+  int itfs_len = _local_interface_indexes == nullptr ? 0 : _local_interface_indexes->length();\n+  _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, nullptr, CHECK);\n+  if (_local_interface_indexes != nullptr) {\n+    for (int i = 0; i < _local_interface_indexes->length(); i++) {\n+      u2 interface_index = _local_interface_indexes->at(i);\n+      Klass* interf;\n+      if (cp->tag_at(interface_index).is_klass()) {\n+        interf = cp->resolved_klass_at(interface_index);\n+      } else {\n+        Symbol* const unresolved_klass  = cp->klass_name_at(interface_index);\n+\n+        \/\/ Don't need to check legal name because it's checked when parsing constant pool.\n+        \/\/ But need to make sure it's not an array type.\n+        guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,\n+                            \"Bad interface name in class file %s\", CHECK);\n+\n+        \/\/ Call resolve on the interface class name with class circularity checking\n+        interf = SystemDictionary::resolve_super_or_fail(\n+                                                  _class_name,\n+                                                  unresolved_klass,\n+                                                  Handle(THREAD, _loader_data->class_loader()),\n+                                                  false,\n+                                                  CHECK);\n+      }\n+\n+      if (!interf->is_interface()) {\n+        THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                  err_msg(\"class %s can not implement %s, because it is not an interface (%s)\",\n+                          _class_name->as_klass_external_name(),\n+                          interf->external_name(),\n+                          interf->class_in_module_of_loader()));\n+      }\n+\n+      if (EnableValhalla) {\n+        \/\/ Check modifiers and set carries_identity_modifier\/carries_value_modifier flags\n+        check_identity_and_value_modifiers(this, InstanceKlass::cast(interf), CHECK);\n+      }\n+\n+      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n+        _has_nonstatic_concrete_methods = true;\n+      }\n+      _local_interfaces->at_put(i, InstanceKlass::cast(interf));\n@@ -5788,0 +6169,1 @@\n+  assert(_local_interfaces != nullptr, \"invariant\");\n@@ -5816,1 +6198,1 @@\n-  _itable_size = _access_flags.is_interface() ? 0 :\n+  _itable_size = is_interface() ? 0 :\n@@ -5821,3 +6203,92 @@\n-  _field_info = new FieldLayoutInfo();\n-  FieldLayoutBuilder lb(class_name(), super_klass(), _cp, \/*_fields*\/ _temp_field_info,\n-                        _parsed_annotations->is_contended(), _field_info);\n+  if (EnableValhalla) {\n+    _inline_layout_info_array = MetadataFactory::new_array<InlineLayoutInfo>(_loader_data,\n+                                                   java_fields_count(),\n+                                                   CHECK);\n+    for (GrowableArrayIterator<FieldInfo> it = _temp_field_info->begin(); it != _temp_field_info->end(); ++it) {\n+      FieldInfo fieldinfo = *it;\n+      if (fieldinfo.access_flags().is_static()) continue;  \/\/ Only non-static fields are processed at load time\n+      Symbol* sig = fieldinfo.signature(cp);\n+      if (fieldinfo.field_flags().is_null_free_inline_type()) {\n+        \/\/ Pre-load classes of null-free fields that are candidate for flattening\n+        TempNewSymbol s = Signature::strip_envelope(sig);\n+        if (s == _class_name) {\n+          THROW_MSG(vmSymbols::java_lang_ClassCircularityError(),\n+                    err_msg(\"Class %s cannot have a null-free non-static field of its own type\", _class_name->as_C_string()));\n+        }\n+        log_info(class, preload)(\"Preloading of class %s during loading of class %s. \"\n+                                  \"Cause: a null-free non-static field is declared with this type\",\n+                                  s->as_C_string(), _class_name->as_C_string());\n+        InstanceKlass* klass = SystemDictionary::resolve_with_circularity_detection(_class_name, s,\n+                                                                                    Handle(THREAD,\n+                                                                                    _loader_data->class_loader()),\n+                                                                                    false, THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+          log_warning(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                      \"(cause: null-free non-static field) failed: %s\",\n+                                      s->as_C_string(), _class_name->as_C_string(),\n+                                      PENDING_EXCEPTION->klass()->name()->as_C_string());\n+          return; \/\/ Exception is still pending\n+        }\n+        assert(klass != nullptr, \"Sanity check\");\n+        InstanceKlass::check_can_be_annotated_with_NullRestricted(klass, _class_name, CHECK);\n+        InlineKlass* vk = InlineKlass::cast(klass);\n+        _inline_layout_info_array->adr_at(fieldinfo.index())->set_klass(vk);\n+        log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                 \"(cause: null-free non-static field) succeeded\",\n+                                 s->as_C_string(), _class_name->as_C_string());\n+      } else if (Signature::has_envelope(sig) && PreloadClasses) {\n+        \/\/ Preloading classes for nullable fields that are listed in the LoadableDescriptors attribute\n+        \/\/ Those classes would be required later for the flattening of nullable inline type fields\n+        TempNewSymbol name = Signature::strip_envelope(sig);\n+        if (name != _class_name && is_class_in_loadable_descriptors_attribute(sig)) {\n+          log_info(class, preload)(\"Preloading of class %s during loading of class %s. \"\n+                                   \"Cause: field type in LoadableDescriptors attribute\",\n+                                   name->as_C_string(), _class_name->as_C_string());\n+          oop loader = loader_data()->class_loader();\n+          Klass* klass = SystemDictionary::resolve_super_or_fail(_class_name, name,\n+                                                                 Handle(THREAD, loader),\n+                                                                 false, THREAD);\n+          if (klass != nullptr) {\n+            if (klass->is_inline_klass()) {\n+              _inline_layout_info_array->adr_at(fieldinfo.index())->set_klass(InlineKlass::cast(klass));\n+              log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                       \"(cause: field type in LoadableDescriptors attribute) succeeded\",\n+                                       name->as_C_string(), _class_name->as_C_string());\n+            } else {\n+              \/\/ Non value class are allowed by the current spec, but it could be an indication of an issue so let's log a warning\n+              log_warning(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                          \"(cause: field type in LoadableDescriptors attribute) but loaded class is not a value class\",\n+                                          name->as_C_string(), _class_name->as_C_string());\n+            }\n+          } else {\n+            log_warning(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                        \"(cause: field type in LoadableDescriptors attribute) failed : %s\",\n+                                        name->as_C_string(), _class_name->as_C_string(),\n+                                        PENDING_EXCEPTION->klass()->name()->as_C_string());\n+          }\n+          \/\/ Loads triggered by the LoadableDescriptors attribute are speculative, failures must not impact loading of current class\n+          if (HAS_PENDING_EXCEPTION) {\n+            CLEAR_PENDING_EXCEPTION;\n+          }\n+        } else {\n+          \/\/ Just poking the system dictionary to see if the class has already be loaded. Looking for migrated classes\n+          \/\/ used when --enable-preview when jdk isn't compiled with --enable-preview so doesn't include LoadableDescriptors.\n+          \/\/ This is temporary.\n+          oop loader = loader_data()->class_loader();\n+          InstanceKlass* klass = SystemDictionary::find_instance_klass(THREAD, name, Handle(THREAD, loader));\n+          if (klass != nullptr && klass->is_inline_klass()) {\n+            _inline_layout_info_array->adr_at(fieldinfo.index())->set_klass(InlineKlass::cast(klass));\n+            log_info(class, preload)(\"Preloading of class %s during loading of class %s \"\n+                                     \"(cause: field type not in LoadableDescriptors attribute) succeeded\",\n+                                     name->as_C_string(), _class_name->as_C_string());\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  _layout_info = new FieldLayoutInfo();\n+  FieldLayoutBuilder lb(class_name(), loader_data(), super_klass(), _cp, \/*_fields*\/ _temp_field_info,\n+      _parsed_annotations->is_contended(), is_inline_type(),\n+      access_flags().is_abstract() && !access_flags().is_identity_class() && !access_flags().is_interface(),\n+      _must_be_atomic, _layout_info, _inline_layout_info_array);\n@@ -5825,0 +6296,1 @@\n+  _has_inline_type_fields = _layout_info->_has_inline_fields;\n@@ -5834,0 +6306,21 @@\n+\n+  \/\/ Strict static fields track initialization status from the beginning of time.\n+  \/\/ After this class runs <clinit>, they will be verified as being \"not unset\".\n+  \/\/ See Step 8 of InstanceKlass::initialize_impl.\n+  if (_has_strict_static_fields) {\n+    bool found_one = false;\n+    for (int i = 0; i < _temp_field_info->length(); i++) {\n+      FieldInfo& fi = *_temp_field_info->adr_at(i);\n+      if (fi.access_flags().is_strict() && fi.access_flags().is_static()) {\n+        found_one = true;\n+        if (fi.initializer_index() != 0) {\n+          \/\/ skip strict static fields with ConstantValue attributes\n+        } else {\n+          _fields_status->adr_at(fi.index())->update_strict_static_unset(true);\n+          _fields_status->adr_at(fi.index())->update_strict_static_unread(true);\n+        }\n+      }\n+    }\n+    assert(found_one == _has_strict_static_fields,\n+           \"correct prediction = %d\", (int)_has_strict_static_fields);\n+  }\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":610,"deletions":117,"binary":false,"changes":727,"status":"modified"},{"patch":"@@ -213,0 +213,1 @@\n+  void inline_classes_do(void f(InlineKlass*));\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -56,0 +56,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -57,1 +59,1 @@\n-#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.inline.hpp\"\n@@ -1107,1 +1109,9 @@\n-    if (k->is_typeArray_klass()) {\n+    if (k->is_flatArray_klass()) {\n+      Klass* element_klass = (Klass*) FlatArrayKlass::cast(k)->element_klass();\n+      assert(element_klass->is_inline_klass(), \"Must be inline type component\");\n+      if (is_scratch) {\n+        comp_mirror = Handle(THREAD, HeapShared::scratch_java_mirror(element_klass));\n+      } else {\n+        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+      }\n+    } else if (k->is_typeArray_klass()) {\n@@ -1116,0 +1126,1 @@\n+      assert(!k->is_refArray_klass() || !k->is_flatArray_klass(), \"Must not have mirror\");\n@@ -1118,0 +1129,1 @@\n+      oop comp_oop = element_klass->java_mirror();\n@@ -1121,1 +1133,1 @@\n-        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+        comp_mirror = Handle(THREAD, comp_oop);\n@@ -1150,1 +1162,0 @@\n-\n@@ -1154,0 +1165,9 @@\n+\n+    if (k->is_refined_objArray_klass()) {\n+      Klass* super_klass = k->super();\n+      assert(super_klass != nullptr, \"Must be\");\n+      Handle mirror(THREAD, super_klass->java_mirror());\n+      k->set_java_mirror(mirror);\n+      return;\n+    }\n+\n@@ -1176,0 +1196,1 @@\n+\n@@ -1199,3 +1220,3 @@\n-  if (k->class_loader() != nullptr &&\n-      k->class_loader() != SystemDictionary::java_platform_loader() &&\n-      k->class_loader() != SystemDictionary::java_system_loader()) {\n+  if ((k->class_loader() != nullptr &&\n+       k->class_loader() != SystemDictionary::java_platform_loader() &&\n+       k->class_loader() != SystemDictionary::java_system_loader())) {\n@@ -1240,1 +1261,0 @@\n-  assert(as_Klass(m) == k, \"must be\");\n@@ -1244,0 +1264,1 @@\n+    assert(as_Klass(m) == k, \"must be\");\n@@ -1253,0 +1274,4 @@\n+  } else {\n+    ObjArrayKlass* objarray_k = (ObjArrayKlass*)as_Klass(m);\n+    \/\/ Mirror should be restored for an ObjArrayKlass or one of its refined array klasses\n+    assert(objarray_k == k || objarray_k->next_refined_array_klass() == k, \"must be\");\n@@ -1431,1 +1456,3 @@\n-  if (is_instance)  st->print(\"L\");\n+  if (is_instance)  {\n+    st->print(\"L\");\n+  }\n@@ -1490,0 +1517,1 @@\n+  assert(!klass->is_refined_objArray_klass(), \"should not be ref or flat array klass\");\n@@ -2868,1 +2896,1 @@\n-      if (method->name() == vmSymbols::object_initializer_name() &&\n+      if (method->is_object_constructor() &&\n@@ -3227,2 +3255,2 @@\n-  if (m->is_object_initializer()) {\n-    flags |= java_lang_invoke_MemberName::MN_IS_CONSTRUCTOR;\n+  if (m->is_object_constructor()) {\n+    flags |= java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR;\n@@ -3619,1 +3647,1 @@\n-int java_lang_reflect_Field::_trusted_final_offset;\n+int java_lang_reflect_Field::_flags_offset;\n@@ -3629,1 +3657,1 @@\n-  macro(_trusted_final_offset,    k, vmSymbols::trusted_final_name(),    bool_signature,       false); \\\n+  macro(_flags_offset,     k, vmSymbols::flags_name(),     int_signature,    false); \\\n@@ -3694,2 +3722,2 @@\n-void java_lang_reflect_Field::set_trusted_final(oop field) {\n-  field->bool_field_put(_trusted_final_offset, true);\n+void java_lang_reflect_Field::set_flags(oop field, int value) {\n+  field->int_field_put(_flags_offset, value);\n@@ -3995,2 +4023,1 @@\n-int java_lang_boxing_object::_value_offset;\n-int java_lang_boxing_object::_long_value_offset;\n+int* java_lang_boxing_object::_offsets;\n@@ -3998,3 +4025,9 @@\n-#define BOXING_FIELDS_DO(macro) \\\n-  macro(_value_offset,      integerKlass, \"value\", int_signature, false); \\\n-  macro(_long_value_offset, longKlass, \"value\", long_signature, false);\n+#define BOXING_FIELDS_DO(macro)                                                                                                    \\\n+  macro(java_lang_boxing_object::_offsets[T_BOOLEAN - T_BOOLEAN], vmClasses::Boolean_klass(),   \"value\", bool_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_CHAR - T_BOOLEAN],    vmClasses::Character_klass(), \"value\", char_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_FLOAT - T_BOOLEAN],   vmClasses::Float_klass(),     \"value\", float_signature,  false); \\\n+  macro(java_lang_boxing_object::_offsets[T_DOUBLE - T_BOOLEAN],  vmClasses::Double_klass(),    \"value\", double_signature, false); \\\n+  macro(java_lang_boxing_object::_offsets[T_BYTE - T_BOOLEAN],    vmClasses::Byte_klass(),      \"value\", byte_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_SHORT - T_BOOLEAN],   vmClasses::Short_klass(),     \"value\", short_signature,  false); \\\n+  macro(java_lang_boxing_object::_offsets[T_INT - T_BOOLEAN],     vmClasses::Integer_klass(),   \"value\", int_signature,    false); \\\n+  macro(java_lang_boxing_object::_offsets[T_LONG - T_BOOLEAN],    vmClasses::Long_klass(),      \"value\", long_signature,   false);\n@@ -4003,2 +4036,2 @@\n-  InstanceKlass* integerKlass = vmClasses::Integer_klass();\n-  InstanceKlass* longKlass = vmClasses::Long_klass();\n+  assert(T_LONG - T_BOOLEAN == 7, \"Sanity check\");\n+  java_lang_boxing_object::_offsets = NEW_C_HEAP_ARRAY(int, 8, mtInternal);\n@@ -4010,0 +4043,4 @@\n+  if (f->reading()) {\n+    assert(T_LONG - T_BOOLEAN == 7, \"Sanity check\");\n+    java_lang_boxing_object::_offsets = NEW_C_HEAP_ARRAY(int, 8, mtInternal);\n+  }\n@@ -4030,1 +4067,1 @@\n-      box->bool_field_put(_value_offset, value->z);\n+      box->bool_field_put(value_offset(type), value->z);\n@@ -4033,1 +4070,1 @@\n-      box->char_field_put(_value_offset, value->c);\n+      box->char_field_put(value_offset(type), value->c);\n@@ -4036,1 +4073,1 @@\n-      box->float_field_put(_value_offset, value->f);\n+      box->float_field_put(value_offset(type), value->f);\n@@ -4039,1 +4076,1 @@\n-      box->double_field_put(_long_value_offset, value->d);\n+      box->double_field_put(value_offset(type), value->d);\n@@ -4042,1 +4079,1 @@\n-      box->byte_field_put(_value_offset, value->b);\n+      box->byte_field_put(value_offset(type), value->b);\n@@ -4045,1 +4082,1 @@\n-      box->short_field_put(_value_offset, value->s);\n+      box->short_field_put(value_offset(type), value->s);\n@@ -4048,1 +4085,1 @@\n-      box->int_field_put(_value_offset, value->i);\n+      box->int_field_put(value_offset(type), value->i);\n@@ -4051,1 +4088,1 @@\n-      box->long_field_put(_long_value_offset, value->j);\n+      box->long_field_put(value_offset(type), value->j);\n@@ -4073,1 +4110,1 @@\n-    value->z = box->bool_field(_value_offset);\n+    value->z = box->bool_field(value_offset(type));\n@@ -4076,1 +4113,1 @@\n-    value->c = box->char_field(_value_offset);\n+    value->c = box->char_field(value_offset(type));\n@@ -4079,1 +4116,1 @@\n-    value->f = box->float_field(_value_offset);\n+    value->f = box->float_field(value_offset(type));\n@@ -4082,1 +4119,1 @@\n-    value->d = box->double_field(_long_value_offset);\n+    value->d = box->double_field(value_offset(type));\n@@ -4085,1 +4122,1 @@\n-    value->b = box->byte_field(_value_offset);\n+    value->b = box->byte_field(value_offset(type));\n@@ -4088,1 +4125,1 @@\n-    value->s = box->short_field(_value_offset);\n+    value->s = box->short_field(value_offset(type));\n@@ -4091,1 +4128,1 @@\n-    value->i = box->int_field(_value_offset);\n+    value->i = box->int_field(value_offset(type));\n@@ -4094,1 +4131,1 @@\n-    value->j = box->long_field(_long_value_offset);\n+    value->j = box->long_field(value_offset(type));\n@@ -4107,1 +4144,1 @@\n-    box->bool_field_put(_value_offset, value->z);\n+    box->bool_field_put(value_offset(type), value->z);\n@@ -4110,1 +4147,1 @@\n-    box->char_field_put(_value_offset, value->c);\n+    box->char_field_put(value_offset(type), value->c);\n@@ -4113,1 +4150,1 @@\n-    box->float_field_put(_value_offset, value->f);\n+    box->float_field_put(value_offset(type), value->f);\n@@ -4116,1 +4153,1 @@\n-    box->double_field_put(_long_value_offset, value->d);\n+    box->double_field_put(value_offset(type), value->d);\n@@ -4119,1 +4156,1 @@\n-    box->byte_field_put(_value_offset, value->b);\n+    box->byte_field_put(value_offset(type), value->b);\n@@ -4122,1 +4159,1 @@\n-    box->short_field_put(_value_offset, value->s);\n+    box->short_field_put(value_offset(type), value->s);\n@@ -4125,1 +4162,1 @@\n-    box->int_field_put(_value_offset, value->i);\n+    box->int_field_put(value_offset(type), value->i);\n@@ -4128,1 +4165,1 @@\n-    box->long_field_put(_long_value_offset, value->j);\n+    box->long_field_put(value_offset(type), value->j);\n@@ -4524,1 +4561,0 @@\n-\n@@ -4534,1 +4570,1 @@\n-  return (flags(mname) & (MN_IS_METHOD | MN_IS_CONSTRUCTOR)) > 0;\n+  return (flags(mname) & (MN_IS_METHOD | MN_IS_OBJECT_CONSTRUCTOR)) > 0;\n@@ -5527,16 +5563,11 @@\n-#define CHECK_OFFSET(klass_name, cpp_klass_name, field_name, field_sig) \\\n-  valid &= check_offset(klass_name, cpp_klass_name :: _##field_name ## _offset, #field_name, field_sig)\n-\n-#define CHECK_LONG_OFFSET(klass_name, cpp_klass_name, field_name, field_sig) \\\n-  valid &= check_offset(klass_name, cpp_klass_name :: _##long_ ## field_name ## _offset, #field_name, field_sig)\n-\n-  \/\/ Boxed primitive objects (java_lang_boxing_object)\n-\n-  CHECK_OFFSET(\"java\/lang\/Boolean\",   java_lang_boxing_object, value, \"Z\");\n-  CHECK_OFFSET(\"java\/lang\/Character\", java_lang_boxing_object, value, \"C\");\n-  CHECK_OFFSET(\"java\/lang\/Float\",     java_lang_boxing_object, value, \"F\");\n-  CHECK_LONG_OFFSET(\"java\/lang\/Double\", java_lang_boxing_object, value, \"D\");\n-  CHECK_OFFSET(\"java\/lang\/Byte\",      java_lang_boxing_object, value, \"B\");\n-  CHECK_OFFSET(\"java\/lang\/Short\",     java_lang_boxing_object, value, \"S\");\n-  CHECK_OFFSET(\"java\/lang\/Integer\",   java_lang_boxing_object, value, \"I\");\n-  CHECK_LONG_OFFSET(\"java\/lang\/Long\", java_lang_boxing_object, value, \"J\");\n+#define CHECK_OFFSET(klass_name, type, field_sig) \\\n+  valid &= check_offset(klass_name, java_lang_boxing_object::value_offset(type), \"value\", field_sig)\n+\n+  CHECK_OFFSET(\"java\/lang\/Boolean\",   T_BOOLEAN, \"Z\");\n+  CHECK_OFFSET(\"java\/lang\/Character\", T_CHAR,    \"C\");\n+  CHECK_OFFSET(\"java\/lang\/Float\",     T_FLOAT,   \"F\");\n+  CHECK_OFFSET(\"java\/lang\/Double\",    T_DOUBLE,  \"D\");\n+  CHECK_OFFSET(\"java\/lang\/Byte\",      T_BYTE,    \"B\");\n+  CHECK_OFFSET(\"java\/lang\/Short\",     T_SHORT,   \"S\");\n+  CHECK_OFFSET(\"java\/lang\/Integer\",   T_INT,     \"I\");\n+  CHECK_OFFSET(\"java\/lang\/Long\",      T_LONG,    \"J\");\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":96,"deletions":65,"binary":false,"changes":161,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+#include \"oops\/fieldStreams.inline.hpp\"\n@@ -51,1 +52,1 @@\n-#include \"runtime\/fieldDescriptor.hpp\"\n+#include \"runtime\/fieldDescriptor.inline.hpp\"\n@@ -69,0 +70,1 @@\n+#define INLINE_TYPE_MAJOR_VERSION                       56\n@@ -480,0 +482,3 @@\n+    case BAD_STRICT_FIELDS:\n+      ss->print(\"Invalid use of strict instance fields\");\n+      break;\n@@ -486,0 +491,3 @@\n+    case STRICT_FIELDS_MISMATCH:\n+      ss->print(\"Current frame's strict instance fields not compatible with stackmap.\");\n+      break;\n@@ -498,0 +506,7 @@\n+    case WRONG_INLINE_TYPE:\n+      ss->print(\"Type \");\n+      _type.details(ss);\n+      ss->print(\" and type \");\n+      _expected.details(ss);\n+      ss->print(\" must be identical inline types.\");\n+      break;\n@@ -620,0 +635,5 @@\n+static bool supports_strict_fields(InstanceKlass* klass) {\n+  int ver = klass->major_version();\n+  return ver > Verifier::VALUE_TYPES_MAJOR_VERSION ||\n+         (ver == Verifier::VALUE_TYPES_MAJOR_VERSION && klass->minor_version() == Verifier::JAVA_PREVIEW_MINOR_VERSION);\n+}\n@@ -715,0 +735,15 @@\n+  \/\/ Collect the initial strict instance fields\n+  StackMapFrame::AssertUnsetFieldTable* strict_fields = new StackMapFrame::AssertUnsetFieldTable();\n+  if (m->is_object_constructor()) {\n+    for (AllFieldStream fs(m->method_holder()); !fs.done(); fs.next()) {\n+      if (fs.access_flags().is_strict() && !fs.access_flags().is_static()) {\n+        NameAndSig new_field(fs.name(), fs.signature());\n+        if (IgnoreAssertUnsetFields) {\n+          strict_fields->put(new_field, true);\n+        } else {\n+          strict_fields->put(new_field, false);\n+        }\n+      }\n+    }\n+  }\n+\n@@ -716,1 +751,1 @@\n-  StackMapFrame current_frame(max_locals, max_stack, this);\n+  StackMapFrame current_frame(max_locals, max_stack, strict_fields, this);\n@@ -743,1 +778,1 @@\n-  StackMapReader reader(this, &stream, code_data, code_length, &current_frame, max_locals, max_stack, THREAD);\n+  StackMapReader reader(this, &stream, code_data, code_length, &current_frame, max_locals, max_stack, strict_fields, THREAD);\n@@ -1616,1 +1651,1 @@\n-            VerificationType::reference_check(), CHECK_VERIFY(this));\n+            object_type(), CHECK_VERIFY(this));\n@@ -1621,1 +1656,1 @@\n-            VerificationType::reference_check(), CHECK_VERIFY(this));\n+            object_type(), CHECK_VERIFY(this));\n@@ -1684,1 +1719,1 @@\n-          if (_method->name() == vmSymbols::object_initializer_name() &&\n+          if (_method->is_object_constructor() &&\n@@ -1707,4 +1742,0 @@\n-          verify_invoke_instructions(\n-            &bcs, code_length, &current_frame, (bci >= ex_min && bci < ex_max),\n-            &this_uninit, return_type, cp, &stackmap_table, CHECK_VERIFY(this));\n-          no_control_flow = false; break;\n@@ -1715,1 +1746,1 @@\n-            &this_uninit, return_type, cp, &stackmap_table, CHECK_VERIFY(this));\n+            &this_uninit, cp, &stackmap_table, CHECK_VERIFY(this));\n@@ -1773,2 +1804,2 @@\n-        case Bytecodes::_monitorexit :\n-          current_frame.pop_stack(\n+        case Bytecodes::_monitorexit : {\n+          VerificationType ref = current_frame.pop_stack(\n@@ -1777,0 +1808,1 @@\n+        }\n@@ -2154,1 +2186,1 @@\n-            | (1 << JVM_CONSTANT_String)  | (1 << JVM_CONSTANT_Class)\n+            | (1 << JVM_CONSTANT_String) | (1 << JVM_CONSTANT_Class)\n@@ -2330,1 +2362,1 @@\n-    (!allow_arrays || !ref_class_type.is_array())) {\n+      (!allow_arrays || !ref_class_type.is_array())) {\n@@ -2337,0 +2369,1 @@\n+\n@@ -2378,1 +2411,1 @@\n-      \/\/ The JVMS 2nd edition allows field initialization before the superclass\n+      \/\/ Field initialization is allowed before the superclass\n@@ -2381,4 +2414,24 @@\n-      if (stack_object_type == VerificationType::uninitialized_this_type() &&\n-          target_class_type.equals(current_type()) &&\n-          _klass->find_local_field(field_name, field_sig, &fd)) {\n-        stack_object_type = current_type();\n+      bool is_local_field = _klass->find_local_field(field_name, field_sig, &fd) &&\n+                            target_class_type.equals(current_type());\n+      if (stack_object_type == VerificationType::uninitialized_this_type()) {\n+        if (is_local_field) {\n+          \/\/ Set the type to the current type so the is_assignable check passes.\n+          stack_object_type = current_type();\n+\n+          if (fd.access_flags().is_strict()) {\n+            ResourceMark rm(THREAD);\n+            if (!current_frame->satisfy_unset_field(fd.name(), fd.signature())) {\n+              log_info(verification)(\"Attempting to initialize field not found in initial strict instance fields: %s%s\",\n+                                     fd.name()->as_C_string(), fd.signature()->as_C_string());\n+              verify_error(ErrorContext::bad_strict_fields(bci, current_frame),\n+                           \"Initializing unknown strict field: %s:%s\", fd.name()->as_C_string(), fd.signature()->as_C_string());\n+            }\n+          }\n+        }\n+      } else if (supports_strict_fields(_klass)) {\n+        \/\/ `strict` fields are not writable, but only local fields produce verification errors\n+        if (is_local_field && fd.access_flags().is_strict() && fd.access_flags().is_final()) {\n+          verify_error(ErrorContext::bad_code(bci),\n+                       \"Illegal use of putfield on a strict field\");\n+          return;\n+        }\n@@ -2666,0 +2719,7 @@\n+    } else if (ref_class_type.name() == superk->name()) {\n+      \/\/ Strict final fields must be satisfied by this point\n+      if (!current_frame->verify_unset_fields_satisfied()) {\n+        log_info(verification)(\"Strict instance fields not initialized\");\n+        StackMapFrame::print_strict_fields(current_frame->assert_unset_fields());\n+        current_frame->unsatisfied_strict_fields_error(current_class(), bci);\n+      }\n@@ -2789,1 +2849,1 @@\n-    bool in_try_block, bool *this_uninit, VerificationType return_type,\n+    bool in_try_block, bool *this_uninit,\n@@ -2821,1 +2881,1 @@\n-  \/\/ Get referenced class type\n+  \/\/ Get referenced class\n@@ -2887,1 +2947,2 @@\n-    \/\/ Make sure <init> can only be invoked by invokespecial\n+    \/\/ Make sure:\n+    \/\/   <init> can only be invoked by invokespecial.\n@@ -2889,1 +2950,1 @@\n-        method_name != vmSymbols::object_initializer_name()) {\n+          method_name != vmSymbols::object_initializer_name()) {\n@@ -2899,1 +2960,1 @@\n-           && !ref_class_type.equals(VerificationType::reference_type(current_class()->super()->name()))) {\n+           && !ref_class_type.equals(VerificationType::reference_type(current_class()->super()->name()))) { \/\/ super() can never be an inline_type.\n@@ -2999,3 +3060,1 @@\n-      \/\/ <init> method must have a void return type\n-      \/* Unreachable?  Class file parser verifies that methods with '<' have\n-       * void return *\/\n+      \/\/ an <init> method must have a void return type\n","filename":"src\/hotspot\/share\/classfile\/verifier.cpp","additions":87,"deletions":28,"binary":false,"changes":115,"status":"modified"},{"patch":"@@ -723,0 +723,11 @@\n+\n+      \/\/ If inline types are passed as fields, use the extended signature\n+      \/\/ which contains the types of all (oop) fields of the inline type.\n+      if (is_compiled_by_c2() && callee->has_scalarized_args()) {\n+        const GrowableArray<SigEntry>* sig = callee->adapter()->get_sig_cc();\n+        assert(sig != nullptr, \"sig should never be null\");\n+        TempNewSymbol tmp_sig = SigEntry::create_symbol(sig);\n+        has_receiver = false; \/\/ The extended signature contains the receiver type\n+        fr.oops_compiled_arguments_do(tmp_sig, has_receiver, has_appendix, reg_map, f);\n+        return;\n+      }\n@@ -1253,0 +1264,4 @@\n+  _inline_entry_point             = entry_point();\n+  _verified_inline_entry_point    = verified_entry_point();\n+  _verified_inline_ro_entry_point = verified_entry_point();\n+\n@@ -1292,1 +1307,1 @@\n-\n+    assert(!method->has_scalarized_args(), \"scalarized native wrappers not supported yet\");\n@@ -1496,0 +1511,4 @@\n+    _inline_entry_point             = code_begin() + offsets->value(CodeOffsets::Inline_Entry);\n+    _verified_inline_entry_point    = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry);\n+    _verified_inline_ro_entry_point = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry_RO);\n+\n@@ -2872,4 +2891,4 @@\n-    if (deps.type() != Dependencies::evol_method)\n-      continue;\n-    Method* method = deps.method_argument(0);\n-    if (method == dependee) return true;\n+    if (Dependencies::has_method_dep(deps.type())) {\n+      Method* method = deps.method_argument(0);\n+      if (method == dependee) return true;\n+    }\n@@ -3713,0 +3732,1 @@\n+  if (pos == inline_entry_point())                                      label = \"[Inline Entry Point]\";\n@@ -3714,0 +3734,2 @@\n+  if (pos == verified_inline_entry_point())                             label = \"[Verified Inline Entry Point]\";\n+  if (pos == verified_inline_ro_entry_point())                          label = \"[Verified Inline Entry Point (RO)]\";\n@@ -3723,0 +3745,10 @@\n+static int maybe_print_entry_label(outputStream* stream, address pos, address entry, const char* label) {\n+  if (pos == entry) {\n+    stream->bol();\n+    stream->print_cr(\"%s\", label);\n+    return 1;\n+  } else {\n+    return 0;\n+  }\n+}\n+\n@@ -3725,33 +3757,12 @@\n-    const char* label = nmethod_section_label(block_begin);\n-    if (label != nullptr) {\n-      stream->bol();\n-      stream->print_cr(\"%s\", label);\n-    }\n-  }\n-\n-  if (block_begin == entry_point()) {\n-    Method* m = method();\n-    if (m != nullptr) {\n-      stream->print(\"  # \");\n-      m->print_value_on(stream);\n-      stream->cr();\n-    }\n-    if (m != nullptr && !is_osr_method()) {\n-      ResourceMark rm;\n-      int sizeargs = m->size_of_parameters();\n-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n-      VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n-      {\n-        int sig_index = 0;\n-        if (!m->is_static())\n-          sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n-        for (SignatureStream ss(m->signature()); !ss.at_return_type(); ss.next()) {\n-          BasicType t = ss.type();\n-          sig_bt[sig_index++] = t;\n-          if (type2size[t] == 2) {\n-            sig_bt[sig_index++] = T_VOID;\n-          } else {\n-            assert(type2size[t] == 1, \"size is 1 or 2\");\n-          }\n-        }\n-        assert(sig_index == sizeargs, \"\");\n+    int n = 0;\n+    \/\/ Multiple entry points may be at the same position. Print them all.\n+    n += maybe_print_entry_label(stream, block_begin, entry_point(),                    \"[Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, inline_entry_point(),             \"[Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_entry_point(),           \"[Verified Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_entry_point(),    \"[Verified Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_ro_entry_point(), \"[Verified Inline Entry Point (RO)]\");\n+    if (n == 0) {\n+      const char* label = nmethod_section_label(block_begin);\n+      if (label != nullptr) {\n+        stream->bol();\n+        stream->print_cr(\"%s\", label);\n@@ -3759,54 +3770,73 @@\n-      const char* spname = \"sp\"; \/\/ make arch-specific?\n-      SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n-      int stack_slot_offset = this->frame_size() * wordSize;\n-      int tab1 = 14, tab2 = 24;\n-      int sig_index = 0;\n-      int arg_index = (m->is_static() ? 0 : -1);\n-      bool did_old_sp = false;\n-      for (SignatureStream ss(m->signature()); !ss.at_return_type(); ) {\n-        bool at_this = (arg_index == -1);\n-        bool at_old_sp = false;\n-        BasicType t = (at_this ? T_OBJECT : ss.type());\n-        assert(t == sig_bt[sig_index], \"sigs in sync\");\n-        if (at_this)\n-          stream->print(\"  # this: \");\n-        else\n-          stream->print(\"  # parm%d: \", arg_index);\n-        stream->move_to(tab1);\n-        VMReg fst = regs[sig_index].first();\n-        VMReg snd = regs[sig_index].second();\n-        if (fst->is_reg()) {\n-          stream->print(\"%s\", fst->name());\n-          if (snd->is_valid())  {\n-            stream->print(\":%s\", snd->name());\n-          }\n-        } else if (fst->is_stack()) {\n-          int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n-          if (offset == stack_slot_offset)  at_old_sp = true;\n-          stream->print(\"[%s+0x%x]\", spname, offset);\n-        } else {\n-          stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n-        }\n-        stream->print(\" \");\n-        stream->move_to(tab2);\n-        stream->print(\"= \");\n-        if (at_this) {\n-          m->method_holder()->print_value_on(stream);\n-        } else {\n-          bool did_name = false;\n-          if (!at_this && ss.is_reference()) {\n-            Symbol* name = ss.as_symbol();\n-            name->print_value_on(stream);\n-            did_name = true;\n-          }\n-          if (!did_name)\n-            stream->print(\"%s\", type2name(t));\n-        }\n-        if (at_old_sp) {\n-          stream->print(\"  (%s of caller)\", spname);\n-          did_old_sp = true;\n-        }\n-        stream->cr();\n-        sig_index += type2size[t];\n-        arg_index += 1;\n-        if (!at_this)  ss.next();\n+    }\n+  }\n+\n+  Method* m = method();\n+  if (m == nullptr || is_osr_method()) {\n+    return;\n+  }\n+\n+  \/\/ Print the name of the method (only once)\n+  address low = MIN3(entry_point(),\n+                     verified_entry_point(),\n+                     inline_entry_point());\n+  \/\/ The verified inline entry point and verified inline RO entry point are not always\n+  \/\/ used. When they are unused. CodeOffsets::Verified_Inline_Entry(_RO) is -1. Hence,\n+  \/\/ the calculated entry point is smaller than the block they are offsetting into.\n+  if (verified_inline_entry_point() >= block_begin) {\n+    low = MIN2(low, verified_inline_entry_point());\n+  }\n+  if (verified_inline_ro_entry_point() >= block_begin) {\n+    low = MIN2(low, verified_inline_ro_entry_point());\n+  }\n+  assert(low != 0, \"sanity\");\n+  if (block_begin == low) {\n+    stream->print(\"  # \");\n+    m->print_value_on(stream);\n+    stream->cr();\n+  }\n+\n+  \/\/ Print the arguments for the 3 types of verified entry points\n+  CompiledEntrySignature ces(m);\n+  ces.compute_calling_conventions(false);\n+  const GrowableArray<SigEntry>* sig_cc;\n+  const VMRegPair* regs;\n+  if (block_begin == verified_entry_point()) {\n+    sig_cc = ces.sig_cc();\n+    regs = ces.regs_cc();\n+  } else if (block_begin == verified_inline_entry_point()) {\n+    sig_cc = ces.sig();\n+    regs = ces.regs();\n+  } else if (block_begin == verified_inline_ro_entry_point()) {\n+    sig_cc = ces.sig_cc_ro();\n+    regs = ces.regs_cc_ro();\n+  } else {\n+    return;\n+  }\n+\n+  bool has_this = !m->is_static();\n+  if (ces.has_inline_recv() && block_begin == verified_entry_point()) {\n+    \/\/ <this> argument is scalarized for verified_entry_point()\n+    has_this = false;\n+  }\n+  const char* spname = \"sp\"; \/\/ make arch-specific?\n+  int stack_slot_offset = this->frame_size() * wordSize;\n+  int tab1 = 14, tab2 = 24;\n+  int sig_index = 0;\n+  int arg_index = has_this ? -1 : 0;\n+  bool did_old_sp = false;\n+  for (ExtendedSignature sig = ExtendedSignature(sig_cc, SigEntryFilter()); !sig.at_end(); ++sig) {\n+    bool at_this = (arg_index == -1);\n+    bool at_old_sp = false;\n+    BasicType t = (*sig)._bt;\n+    if (at_this) {\n+      stream->print(\"  # this: \");\n+    } else {\n+      stream->print(\"  # parm%d: \", arg_index);\n+    }\n+    stream->move_to(tab1);\n+    VMReg fst = regs[sig_index].first();\n+    VMReg snd = regs[sig_index].second();\n+    if (fst->is_reg()) {\n+      stream->print(\"%s\", fst->name());\n+      if (snd->is_valid())  {\n+        stream->print(\":%s\", snd->name());\n@@ -3814,6 +3844,23 @@\n-      if (!did_old_sp) {\n-        stream->print(\"  # \");\n-        stream->move_to(tab1);\n-        stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n-        stream->print(\"  (%s of caller)\", spname);\n-        stream->cr();\n+    } else if (fst->is_stack()) {\n+      int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n+      if (offset == stack_slot_offset)  at_old_sp = true;\n+      stream->print(\"[%s+0x%x]\", spname, offset);\n+    } else {\n+      stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n+    }\n+    stream->print(\" \");\n+    stream->move_to(tab2);\n+    stream->print(\"= \");\n+    if (at_this) {\n+      m->method_holder()->print_value_on(stream);\n+    } else {\n+      bool did_name = false;\n+      if (is_reference_type(t)) {\n+        Symbol* name = (*sig)._name;\n+        name->print_value_on(stream);\n+        did_name = true;\n+      }\n+      if (!did_name)\n+        stream->print(\"%s\", type2name(t));\n+      if ((*sig)._null_marker) {\n+        stream->print(\" (null marker)\");\n@@ -3822,0 +3869,14 @@\n+    if (at_old_sp) {\n+      stream->print(\"  (%s of caller)\", spname);\n+      did_old_sp = true;\n+    }\n+    stream->cr();\n+    sig_index += type2size[t];\n+    arg_index += 1;\n+  }\n+  if (!did_old_sp) {\n+    stream->print(\"  # \");\n+    stream->move_to(tab1);\n+    stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n+    stream->print(\"  (%s of caller)\", spname);\n+    stream->cr();\n@@ -3945,1 +4006,1 @@\n-      st->print(\" {reexecute=%d rethrow=%d return_oop=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop());\n+      st->print(\" {reexecute=%d rethrow=%d return_oop=%d return_scalarized=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop(), sd->return_scalarized());\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":160,"deletions":99,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"compiler\/compilerDefinitions.hpp\"\n@@ -216,0 +217,4 @@\n+  \/\/ TODO: can these be uint16_t, seem rely on -1 CodeOffset, can change later...\n+  address _inline_entry_point;              \/\/ inline type entry point (unpack all inline type args) with class check\n+  address _verified_inline_entry_point;     \/\/ inline type entry point (unpack all inline type args) without class check\n+  address _verified_inline_ro_entry_point;  \/\/ inline type entry point (unpack receiver only) without class check\n@@ -686,0 +691,3 @@\n+  address inline_entry_point() const              { return _inline_entry_point; }             \/\/ inline type entry point (unpack all inline type args)\n+  address verified_inline_entry_point() const     { return _verified_inline_entry_point; }    \/\/ inline type entry point (unpack all inline type args) without class check\n+  address verified_inline_ro_entry_point() const  { return _verified_inline_ro_entry_point; } \/\/ inline type entry point (only unpack receiver) without class check\n@@ -755,0 +763,10 @@\n+  bool  needs_stack_repair() const {\n+    if (is_compiled_by_c1()) {\n+      return method()->c1_needs_stack_repair();\n+    } else if (is_compiled_by_c2()) {\n+      return method()->c2_needs_stack_repair();\n+    } else {\n+      return false;\n+    }\n+  }\n+\n@@ -1044,3 +1062,4 @@\n-  \/\/ Fast breakpoint support. Tells if this compiled method is\n-  \/\/ dependent on the given method. Returns true if this nmethod\n-  \/\/ corresponds to the given method as well.\n+  \/\/ Tells if this compiled method is dependent on the given method.\n+  \/\/ Returns true if this nmethod corresponds to the given method as well.\n+  \/\/ It is used for fast breakpoint support and updating the calling convention\n+  \/\/ in case of mismatch.\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":22,"deletions":3,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -1289,1 +1289,1 @@\n-          if (vfst.method()->is_static_initializer() ||\n+        if (vfst.method()->is_class_initializer() ||\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -78,0 +78,1 @@\n+#include \"oops\/flatArrayKlass.inline.hpp\"\n@@ -2339,1 +2340,1 @@\n-    cast_to_oop(copy_destination())->init_mark();\n+    cast_to_oop(copy_destination())->reinit_mark();\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -384,0 +384,1 @@\n+  assert(obj->is_refArray(), \"Must be\");\n@@ -399,1 +400,1 @@\n-  if (obj->is_objArray()) {\n+  if (obj->is_refArray()) {\n@@ -416,1 +417,1 @@\n-  array->oop_iterate_range(&mark_and_push_closure, beg_index, end_index);\n+  refArrayOop(array)->oop_iterate_range(&mark_and_push_closure, beg_index, end_index);\n","filename":"src\/hotspot\/share\/gc\/serial\/serialFullGC.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -224,1 +224,1 @@\n-  nonstatic_field(InstanceKlass,               _misc_flags._flags,                            u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _misc_flags._flags,                            u4)                                    \\\n@@ -738,0 +738,3 @@\n+  declare_constant(DataLayout::array_store_data_tag)                      \\\n+  declare_constant(DataLayout::array_load_data_tag)                       \\\n+  declare_constant(DataLayout::acmp_data_tag)                             \\\n@@ -818,1 +821,1 @@\n-  declare_constant(Klass::_lh_array_tag_obj_value)                        \\\n+  declare_constant(Klass::_lh_array_tag_ref_value)                        \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -70,0 +70,1 @@\n+#include \"oops\/refArrayKlass.hpp\"\n@@ -117,0 +118,2 @@\n+static LatestMethodCache _is_substitutable_cache;           \/\/ ValueObjectMethods.isSubstitutable()\n+static LatestMethodCache _value_object_hash_code_cache;     \/\/ ValueObjectMethods.valueObjectHashCode()\n@@ -462,0 +465,1 @@\n+\n@@ -509,2 +513,6 @@\n-    Klass* oak = vmClasses::Object_klass()->array_klass(CHECK);\n-    _objectArrayKlass = ObjArrayKlass::cast(oak);\n+    ArrayKlass* oak = vmClasses::Object_klass()->array_klass(CHECK);\n+    oak->append_to_sibling_list();\n+\n+    \/\/ Create a RefArrayKlass (which is the default) and initialize.\n+    ObjArrayKlass* rak = ObjArrayKlass::cast(oak)->klass_with_properties(ArrayKlass::ArrayProperties::DEFAULT, THREAD);\n+    _objectArrayKlass = rak;\n@@ -512,7 +520,0 @@\n-  \/\/ OLD\n-  \/\/ Add the class to the class hierarchy manually to make sure that\n-  \/\/ its vtable is initialized after core bootstrapping is completed.\n-  \/\/ ---\n-  \/\/ New\n-  \/\/ Have already been initialized.\n-  _objectArrayKlass->append_to_sibling_list();\n@@ -655,0 +656,3 @@\n+\n+  \/\/ This isn't added to the subclass list, so need to reinitialize vtables directly.\n+  Universe::objectArrayKlass()->vtable().initialize_vtable();\n@@ -900,1 +904,0 @@\n-\n@@ -1064,0 +1067,2 @@\n+Method* Universe::is_substitutable_method()       { return _is_substitutable_cache.get_method(); }\n+Method* Universe::value_object_hash_code_method() { return _value_object_hash_code_cache.get_method(); }\n@@ -1093,0 +1098,11 @@\n+\n+  \/\/ Set up substitutability testing\n+  ResourceMark rm(current);\n+  _is_substitutable_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::isSubstitutable_name()->as_C_string(),\n+                          vmSymbols::object_object_boolean_signature(), true);\n+  _value_object_hash_code_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::valueObjectHashCode_name()->as_C_string(),\n+                          vmSymbols::object_int_signature(), true);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":26,"deletions":10,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -61,0 +62,1 @@\n+#include \"oops\/refArrayOop.hpp\"\n@@ -194,1 +196,1 @@\n-  return resolved_references()->replace_if_null(index, new_result);\n+  return refArrayOopDesc::cast(resolved_references())->replace_if_null(index, new_result);\n@@ -265,1 +267,1 @@\n-      \/\/ All of these should have been reverted back to ClassIndex before calling\n+      \/\/ All of these should have been reverted back to Unresolved before calling\n@@ -481,0 +483,1 @@\n+    assert(src_k->is_instance_klass() || src_k->is_typeArray_klass(), \"Sanity check\");\n@@ -622,0 +625,6 @@\n+void check_is_inline_type(Klass* k, TRAPS) {\n+  if (!k->is_inline_klass()) {\n+    THROW(vmSymbols::java_lang_IncompatibleClassChangeError());\n+  }\n+}\n+\n@@ -659,0 +668,1 @@\n+  bool inline_type_signature = false;\n@@ -667,0 +677,3 @@\n+  if (inline_type_signature) {\n+    name->decrement_refcount();\n+  }\n@@ -675,0 +688,16 @@\n+  if (!HAS_PENDING_EXCEPTION && inline_type_signature) {\n+    check_is_inline_type(k, THREAD);\n+  }\n+\n+  if (!HAS_PENDING_EXCEPTION) {\n+    Klass* bottom_klass = nullptr;\n+    if (k->is_objArray_klass()) {\n+      bottom_klass = ObjArrayKlass::cast(k)->bottom_klass();\n+      assert(bottom_klass != nullptr, \"Should be set\");\n+      assert(bottom_klass->is_instance_klass() || bottom_klass->is_typeArray_klass(), \"Sanity check\");\n+    } else if (k->is_flatArray_klass()) {\n+      bottom_klass = FlatArrayKlass::cast(k)->element_klass();\n+      assert(bottom_klass != nullptr, \"Should be set\");\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":31,"deletions":2,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -0,0 +1,469 @@\n+\/*\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"classfile\/moduleEntry.hpp\"\n+#include \"classfile\/packageEntry.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"gc\/shared\/collectedHeap.inline.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/metadataFactory.hpp\"\n+#include \"memory\/metaspaceClosure.hpp\"\n+#include \"memory\/oopFactory.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/arrayKlass.inline.hpp\"\n+#include \"oops\/arrayOop.hpp\"\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n+#include \"oops\/instanceKlass.hpp\"\n+#include \"oops\/klass.inline.hpp\"\n+#include \"oops\/objArrayKlass.hpp\"\n+#include \"oops\/objArrayOop.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"oops\/verifyOopClosure.hpp\"\n+#include \"runtime\/arguments.hpp\"\n+#include \"runtime\/handles.inline.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"utilities\/copy.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+\/\/ Allocation...\n+\n+FlatArrayKlass::FlatArrayKlass(Klass* element_klass, Symbol* name, ArrayProperties props, LayoutKind lk) :\n+                ObjArrayKlass(1, element_klass, name, Kind, props, markWord::flat_array_prototype(lk)) {\n+  assert(element_klass->is_inline_klass(), \"Expected Inline\");\n+  assert(lk == LayoutKind::NON_ATOMIC_FLAT || lk == LayoutKind::ATOMIC_FLAT || lk == LayoutKind::NULLABLE_ATOMIC_FLAT, \"Must be a flat layout\");\n+\n+  set_element_klass(InlineKlass::cast(element_klass));\n+  set_class_loader_data(element_klass->class_loader_data());\n+  set_layout_kind(lk);\n+\n+  set_layout_helper(array_layout_helper(InlineKlass::cast(element_klass), lk));\n+  assert(is_array_klass(), \"sanity\");\n+  assert(is_flatArray_klass(), \"sanity\");\n+\n+#ifdef ASSERT\n+  assert(layout_helper_is_array(layout_helper()), \"Must be\");\n+  assert(layout_helper_is_flatArray(layout_helper()), \"Must be\");\n+  assert(layout_helper_element_type(layout_helper()) == T_FLAT_ELEMENT, \"Must be\");\n+  assert(prototype_header().is_flat_array(), \"Must be\");\n+  switch(lk) {\n+    case LayoutKind::NON_ATOMIC_FLAT:\n+    case LayoutKind::ATOMIC_FLAT:\n+      assert(layout_helper_is_null_free(layout_helper()), \"Must be\");\n+      assert(prototype_header().is_null_free_array(), \"Must be\");\n+    break;\n+    case LayoutKind::NULLABLE_ATOMIC_FLAT:\n+      assert(!layout_helper_is_null_free(layout_helper()), \"Must be\");\n+      assert(!prototype_header().is_null_free_array(), \"Must be\");\n+    break;\n+    default:\n+      ShouldNotReachHere();\n+    break;\n+  }\n+#endif \/\/ ASSERT\n+\n+#ifndef PRODUCT\n+  if (PrintFlatArrayLayout) {\n+    print();\n+  }\n+#endif\n+}\n+\n+FlatArrayKlass* FlatArrayKlass::allocate_klass(Klass* eklass, ArrayProperties props, LayoutKind lk, TRAPS) {\n+  guarantee((!Universe::is_bootstrapping() || vmClasses::Object_klass_is_loaded()), \"Really ?!\");\n+  assert(UseArrayFlattening, \"Flatten array required\");\n+  assert(MultiArray_lock->holds_lock(THREAD), \"must hold lock after bootstrapping\");\n+\n+  InlineKlass* element_klass = InlineKlass::cast(eklass);\n+  assert(element_klass->must_be_atomic() || (!AlwaysAtomicAccesses), \"Atomic by-default\");\n+\n+  \/\/ Eagerly allocate the direct array supertype.\n+  Klass* super_klass = nullptr;\n+  Klass* element_super = element_klass->super();\n+  if (element_super != nullptr) {\n+    \/\/ The element type has a direct super.  E.g., String[] has direct super of Object[].\n+    super_klass = element_klass->array_klass(CHECK_NULL);\n+  }\n+\n+  Symbol* name = ArrayKlass::create_element_klass_array_name(element_klass, CHECK_NULL);\n+  ClassLoaderData* loader_data = element_klass->class_loader_data();\n+  int size = ArrayKlass::static_size(FlatArrayKlass::header_size());\n+  FlatArrayKlass* vak = new (loader_data, size, THREAD) FlatArrayKlass(element_klass, name, props, lk);\n+\n+  ModuleEntry* module = vak->module();\n+  assert(module != nullptr, \"No module entry for array\");\n+  complete_create_array_klass(vak, super_klass, module, CHECK_NULL);\n+\n+  loader_data->add_class(vak);\n+\n+  return vak;\n+}\n+\n+void FlatArrayKlass::initialize(TRAPS) {\n+  element_klass()->initialize(THREAD);\n+}\n+\n+void FlatArrayKlass::metaspace_pointers_do(MetaspaceClosure* it) {\n+  ObjArrayKlass::metaspace_pointers_do(it);\n+}\n+\n+\/\/ Oops allocation...\n+objArrayOop FlatArrayKlass::allocate_instance(int length, ArrayProperties props, TRAPS) {\n+  assert(UseArrayFlattening, \"Must be enabled\");\n+  check_array_allocation_length(length, max_elements(), CHECK_NULL);\n+  int size = flatArrayOopDesc::object_size(layout_helper(), length);\n+  flatArrayOop array = (flatArrayOop) Universe::heap()->array_allocate(this, size, length, true, CHECK_NULL);\n+  return array;\n+}\n+\n+oop FlatArrayKlass::multi_allocate(int rank, jint* last_size, TRAPS) {\n+  \/\/ FlatArrays only have one dimension\n+  ShouldNotReachHere();\n+}\n+\n+jint FlatArrayKlass::array_layout_helper(InlineKlass* vk, LayoutKind lk) {\n+  BasicType etype = T_FLAT_ELEMENT;\n+  int esize = log2i_exact(round_up_power_of_2(vk->layout_size_in_bytes(lk)));\n+  int hsize = arrayOopDesc::base_offset_in_bytes(etype);\n+  bool null_free = lk != LayoutKind::NULLABLE_ATOMIC_FLAT;\n+  int lh = Klass::array_layout_helper(_lh_array_tag_flat_value, null_free, hsize, etype, esize);\n+\n+  assert(lh < (int)_lh_neutral_value, \"must look like an array layout\");\n+  assert(layout_helper_is_array(lh), \"correct kind\");\n+  assert(layout_helper_is_flatArray(lh), \"correct kind\");\n+  assert(!layout_helper_is_typeArray(lh), \"correct kind\");\n+  assert(layout_helper_is_null_free(lh) == null_free, \"correct kind\");\n+  assert(layout_helper_header_size(lh) == hsize, \"correct decode\");\n+  assert(layout_helper_element_type(lh) == etype, \"correct decode\");\n+  assert(layout_helper_log2_element_size(lh) == esize, \"correct decode\");\n+  assert((1 << esize) < BytesPerLong || is_aligned(hsize, HeapWordsPerLong), \"unaligned base\");\n+\n+  return lh;\n+}\n+\n+size_t FlatArrayKlass::oop_size(oop obj) const {\n+  \/\/ In this assert, we cannot safely access the Klass* with compact headers,\n+  \/\/ because size_given_klass() calls oop_size() on objects that might be\n+  \/\/ concurrently forwarded, which would overwrite the Klass*.\n+  \/\/ Also, why we need to pass this layout_helper() to flatArrayOop::object_size.\n+  assert(UseCompactObjectHeaders || obj->is_flatArray(),\"must be an flat array\");\n+  flatArrayOop array = flatArrayOop(obj);\n+  return array->object_size(layout_helper());\n+}\n+\n+\/\/ For now return the maximum number of array elements that will not exceed:\n+\/\/ nof bytes = \"max_jint * HeapWord\" since the \"oopDesc::oop_iterate_size\"\n+\/\/ returns \"int\" HeapWords, need fix for JDK-4718400 and JDK-8233189\n+jint FlatArrayKlass::max_elements() const {\n+  \/\/ Check the max number of heap words limit first (because of int32_t in oopDesc_oop_size() etc)\n+  size_t max_size = max_jint;\n+  max_size -= (arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT) >> LogHeapWordSize);\n+  max_size = align_down(max_size, MinObjAlignment);\n+  max_size <<= LogHeapWordSize;                                  \/\/ convert to max payload size in bytes\n+  max_size >>= layout_helper_log2_element_size(_layout_helper);  \/\/ divide by element size (in bytes) = max elements\n+  \/\/ Within int32_t heap words, still can't exceed Java array element limit\n+  if (max_size > max_jint) {\n+    max_size = max_jint;\n+  }\n+  assert((max_size >> LogHeapWordSize) <= max_jint, \"Overflow\");\n+  return (jint) max_size;\n+}\n+\n+oop FlatArrayKlass::protection_domain() const {\n+  return element_klass()->protection_domain();\n+}\n+\n+\/\/ Temp hack having this here: need to move towards Access API\n+static bool needs_backwards_copy(arrayOop s, int src_pos,\n+                                 arrayOop d, int dst_pos, int length) {\n+  return (s == d) && (dst_pos > src_pos) && (dst_pos - src_pos) < length;\n+}\n+\n+void FlatArrayKlass::copy_array(arrayOop s, int src_pos,\n+                                arrayOop d, int dst_pos, int length, TRAPS) {\n+\n+  assert(s->is_objArray() || s->is_flatArray(), \"must be obj or flat array\");\n+\n+  \/\/ Check destination\n+  if ((!d->is_flatArray()) && (!d->is_objArray())) {\n+    THROW(vmSymbols::java_lang_ArrayStoreException());\n+  }\n+\n+  \/\/ Check if all offsets and lengths are non negative\n+  if (src_pos < 0 || dst_pos < 0 || length < 0) {\n+    THROW(vmSymbols::java_lang_ArrayIndexOutOfBoundsException());\n+  }\n+  \/\/ Check if the ranges are valid\n+  if  ( (((unsigned int) length + (unsigned int) src_pos) > (unsigned int) s->length())\n+      || (((unsigned int) length + (unsigned int) dst_pos) > (unsigned int) d->length()) ) {\n+    THROW(vmSymbols::java_lang_ArrayIndexOutOfBoundsException());\n+  }\n+  \/\/ Check zero copy\n+  if (length == 0)\n+    return;\n+\n+  ObjArrayKlass* sk = ObjArrayKlass::cast(s->klass());\n+  ObjArrayKlass* dk = ObjArrayKlass::cast(d->klass());\n+  Klass* d_elem_klass = dk->element_klass();\n+  Klass* s_elem_klass = sk->element_klass();\n+  \/**** CMH: compare and contrast impl, re-factor once we find edge cases... ****\/\n+\n+  if (sk->is_flatArray_klass()) {\n+    assert(sk == this, \"Unexpected call to copy_array\");\n+    FlatArrayKlass* fsk = FlatArrayKlass::cast(sk);\n+    \/\/ Check subtype, all src homogeneous, so just once\n+    if (!s_elem_klass->is_subtype_of(d_elem_klass)) {\n+      THROW(vmSymbols::java_lang_ArrayStoreException());\n+    }\n+\n+    flatArrayOop sa = flatArrayOop(s);\n+    InlineKlass* s_elem_vklass = element_klass();\n+\n+    \/\/ flatArray-to-flatArray\n+    if (dk->is_flatArray_klass()) {\n+      \/\/ element types MUST be exact, subtype check would be dangerous\n+      if (d_elem_klass != this->element_klass()) {\n+        THROW(vmSymbols::java_lang_ArrayStoreException());\n+      }\n+\n+      FlatArrayKlass* fdk = FlatArrayKlass::cast(dk);\n+      InlineKlass* vk = InlineKlass::cast(s_elem_klass);\n+      flatArrayOop da = flatArrayOop(d);\n+      int src_incr = fsk->element_byte_size();\n+      int dst_incr = fdk->element_byte_size();\n+\n+      if (fsk->layout_kind() == fdk->layout_kind()) {\n+        assert(src_incr == dst_incr, \"Must be\");\n+        if (needs_backwards_copy(sa, src_pos, da, dst_pos, length)) {\n+          address dst = (address) da->value_at_addr(dst_pos + length - 1, fdk->layout_helper());\n+          address src = (address) sa->value_at_addr(src_pos + length - 1, fsk->layout_helper());\n+          for (int i = 0; i < length; i++) {\n+            \/\/ because source and destination have the same layout, bypassing the InlineKlass copy methods\n+            \/\/ and call AccessAPI directly\n+            HeapAccess<>::value_copy(src, dst, vk, fsk->layout_kind());\n+            dst -= dst_incr;\n+            src -= src_incr;\n+          }\n+        } else {\n+          \/\/ source and destination share same layout, direct copy from array to array is possible\n+          address dst = (address) da->value_at_addr(dst_pos, fdk->layout_helper());\n+          address src = (address) sa->value_at_addr(src_pos, fsk->layout_helper());\n+          for (int i = 0; i < length; i++) {\n+            \/\/ because source and destination have the same layout, bypassing the InlineKlass copy methods\n+            \/\/ and call AccessAPI directly\n+            HeapAccess<>::value_copy(src, dst, vk, fsk->layout_kind());\n+            dst += dst_incr;\n+            src += src_incr;\n+          }\n+        }\n+      } else {\n+        flatArrayHandle hd(THREAD, da);\n+        flatArrayHandle hs(THREAD, sa);\n+        \/\/ source and destination layouts mismatch, simpler solution is to copy through an intermediate buffer (heap instance)\n+        bool need_null_check = fsk->layout_kind() == LayoutKind::NULLABLE_ATOMIC_FLAT && fdk->layout_kind() != LayoutKind::NULLABLE_ATOMIC_FLAT;\n+        oop buffer = vk->allocate_instance(CHECK);\n+        address dst = (address) hd->value_at_addr(dst_pos, fdk->layout_helper());\n+        address src = (address) hs->value_at_addr(src_pos, fsk->layout_helper());\n+        for (int i = 0; i < length; i++) {\n+          if (need_null_check) {\n+            if (vk->is_payload_marked_as_null(src)) {\n+              THROW(vmSymbols::java_lang_NullPointerException());\n+            }\n+          }\n+          vk->copy_payload_to_addr(src, vk->payload_addr(buffer), fsk->layout_kind(), true);\n+          if (vk->has_nullable_atomic_layout()) {\n+            \/\/ Setting null marker to not zero for non-nullable source layouts\n+            vk->mark_payload_as_non_null(vk->payload_addr(buffer));\n+          }\n+          vk->copy_payload_to_addr(vk->payload_addr(buffer), dst, fdk->layout_kind(), true);\n+          dst += dst_incr;\n+          src += src_incr;\n+        }\n+      }\n+    } else { \/\/ flatArray-to-objArray\n+      assert(dk->is_refArray_klass(), \"Expected objArray here\");\n+      \/\/ Need to allocate each new src elem payload -> dst oop\n+      objArrayHandle dh(THREAD, (objArrayOop)d);\n+      flatArrayHandle sh(THREAD, sa);\n+      InlineKlass* vk = InlineKlass::cast(s_elem_klass);\n+      for (int i = 0; i < length; i++) {\n+        oop o = sh->obj_at(src_pos + i, CHECK);\n+        dh->obj_at_put(dst_pos + i, o);\n+      }\n+    }\n+  } else {\n+    assert(s->is_objArray(), \"Expected objArray\");\n+    objArrayOop sa = objArrayOop(s);\n+    assert(d->is_flatArray(), \"Expected flatArray\");  \/\/ objArray-to-flatArray\n+    InlineKlass* d_elem_vklass = InlineKlass::cast(d_elem_klass);\n+    flatArrayOop da = flatArrayOop(d);\n+    FlatArrayKlass* fdk = FlatArrayKlass::cast(da->klass());\n+    InlineKlass* vk = InlineKlass::cast(d_elem_klass);\n+\n+    for (int i = 0; i < length; i++) {\n+      da->obj_at_put( dst_pos + i, sa->obj_at(src_pos + i), CHECK);\n+    }\n+  }\n+}\n+\n+ModuleEntry* FlatArrayKlass::module() const {\n+  assert(element_klass() != nullptr, \"FlatArrayKlass returned unexpected nullptr bottom_klass\");\n+  \/\/ The array is defined in the module of its bottom class\n+  return element_klass()->module();\n+}\n+\n+PackageEntry* FlatArrayKlass::package() const {\n+  assert(element_klass() != nullptr, \"FlatArrayKlass returned unexpected nullptr bottom_klass\");\n+  return element_klass()->package();\n+}\n+\n+bool FlatArrayKlass::can_be_primary_super_slow() const {\n+    return true;\n+}\n+\n+GrowableArray<Klass*>* FlatArrayKlass::compute_secondary_supers(int num_extra_slots,\n+                                                                Array<InstanceKlass*>* transitive_interfaces) {\n+  assert(transitive_interfaces == nullptr, \"sanity\");\n+  \/\/ interfaces = { cloneable_klass, serializable_klass, elemSuper[], ... };\n+  Array<Klass*>* elem_supers = element_klass()->secondary_supers();\n+  int num_elem_supers = elem_supers == nullptr ? 0 : elem_supers->length();\n+  int num_secondaries = num_extra_slots + 2 + num_elem_supers;\n+  GrowableArray<Klass*>* secondaries = new GrowableArray<Klass*>(num_elem_supers+2);\n+\n+  secondaries->push(vmClasses::Cloneable_klass());\n+  secondaries->push(vmClasses::Serializable_klass());\n+  for (int i = 0; i < num_elem_supers; i++) {\n+    Klass* elem_super = (Klass*) elem_supers->at(i);\n+    Klass* array_super = elem_super->array_klass_or_null();\n+    assert(array_super != nullptr, \"must already have been created\");\n+    secondaries->push(array_super);\n+  }\n+  return secondaries;\n+}\n+\n+u2 FlatArrayKlass::compute_modifier_flags() const {\n+  \/\/ The modifier for an flatArray is the same as its element\n+  \/\/ With the addition of ACC_IDENTITY\n+  u2 element_flags = element_klass()->compute_modifier_flags();\n+\n+  u2 identity_flag = (Arguments::enable_preview()) ? JVM_ACC_IDENTITY : 0;\n+\n+  return (element_flags & (JVM_ACC_PUBLIC | JVM_ACC_PRIVATE | JVM_ACC_PROTECTED))\n+                        | (identity_flag | JVM_ACC_ABSTRACT | JVM_ACC_FINAL);\n+}\n+\n+void FlatArrayKlass::print_on(outputStream* st) const {\n+#ifndef PRODUCT\n+  assert(!is_refArray_klass(), \"Unimplemented\");\n+\n+  st->print(\"Flat Type Array: \");\n+  Klass::print_on(st);\n+\n+  st->print(\" - element klass: \");\n+  element_klass()->print_value_on(st);\n+  st->cr();\n+\n+  int elem_size = element_byte_size();\n+  st->print(\" - element size %i \", elem_size);\n+  st->print(\"aligned layout size %i\", 1 << layout_helper_log2_element_size(layout_helper()));\n+  st->cr();\n+#endif \/\/PRODUCT\n+}\n+\n+void FlatArrayKlass::print_value_on(outputStream* st) const {\n+  assert(is_klass(), \"must be klass\");\n+\n+  element_klass()->print_value_on(st);\n+  st->print(\"[]\");\n+}\n+\n+\n+#ifndef PRODUCT\n+void FlatArrayKlass::oop_print_on(oop obj, outputStream* st) {\n+  ArrayKlass::oop_print_on(obj, st);\n+  flatArrayOop va = flatArrayOop(obj);\n+  InlineKlass* vk = element_klass();\n+  int print_len = MIN2(va->length(), MaxElementPrintSize);\n+  for(int index = 0; index < print_len; index++) {\n+    int off = (address) va->value_at_addr(index, layout_helper()) - cast_from_oop<address>(obj);\n+    st->print_cr(\" - Index %3d offset %3d: \", index, off);\n+    oop obj = cast_to_oop((address)va->value_at_addr(index, layout_helper()) - vk->payload_offset());\n+    FieldPrinter print_field(st, obj);\n+    vk->do_nonstatic_fields(&print_field);\n+    st->cr();\n+  }\n+  int remaining = va->length() - print_len;\n+  if (remaining > 0) {\n+    st->print_cr(\" - <%d more elements, increase MaxElementPrintSize to print>\", remaining);\n+  }\n+}\n+#endif \/\/PRODUCT\n+\n+void FlatArrayKlass::oop_print_value_on(oop obj, outputStream* st) {\n+  assert(obj->is_flatArray(), \"must be flatArray\");\n+  st->print(\"a \");\n+  element_klass()->print_value_on(st);\n+  int len = flatArrayOop(obj)->length();\n+  st->print(\"[%d] \", len);\n+  obj->print_address_on(st);\n+  if (PrintMiscellaneous && (WizardMode || Verbose)) {\n+    int lh = layout_helper();\n+    st->print(\"{\");\n+    for (int i = 0; i < len; i++) {\n+      if (i > 4) {\n+        st->print(\"...\"); break;\n+      }\n+      st->print(\" \" INTPTR_FORMAT, (intptr_t)(void*)flatArrayOop(obj)->value_at_addr(i , lh));\n+    }\n+    st->print(\" }\");\n+  }\n+}\n+\n+\/\/ Verification\n+class VerifyElementClosure: public BasicOopIterateClosure {\n+ public:\n+  virtual void do_oop(oop* p)       { VerifyOopClosure::verify_oop.do_oop(p); }\n+  virtual void do_oop(narrowOop* p) { VerifyOopClosure::verify_oop.do_oop(p); }\n+};\n+\n+void FlatArrayKlass::oop_verify_on(oop obj, outputStream* st) {\n+  ArrayKlass::oop_verify_on(obj, st);\n+  guarantee(obj->is_flatArray(), \"must be flatArray\");\n+\n+  if (contains_oops()) {\n+    flatArrayOop va = flatArrayOop(obj);\n+    VerifyElementClosure ec;\n+    va->oop_iterate(&ec);\n+  }\n+}\n+\n+void FlatArrayKlass::verify_on(outputStream* st) {\n+  ArrayKlass::verify_on(st);\n+  guarantee(element_klass()->is_inline_klass(), \"should be inline type klass\");\n+}\n","filename":"src\/hotspot\/share\/oops\/flatArrayKlass.cpp","additions":469,"deletions":0,"binary":false,"changes":469,"status":"added"},{"patch":"@@ -66,0 +66,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -72,0 +73,1 @@\n+#include \"oops\/markWord.hpp\"\n@@ -75,0 +77,1 @@\n+#include \"oops\/refArrayKlass.hpp\"\n@@ -152,0 +155,5 @@\n+void InlineLayoutInfo::metaspace_pointers_do(MetaspaceClosure* it) {\n+  log_trace(cds)(\"Iter(InlineFieldInfo): %p\", this);\n+  it->push(&_klass);\n+}\n+\n@@ -173,0 +181,13 @@\n+bool InstanceKlass::field_is_null_free_inline_type(int index) const {\n+  return field(index).field_flags().is_null_free_inline_type();\n+}\n+\n+bool InstanceKlass::is_class_in_loadable_descriptors_attribute(Symbol* name) const {\n+  if (_loadable_descriptors == nullptr) return false;\n+  for (int i = 0; i < _loadable_descriptors->length(); i++) {\n+        Symbol* class_name = _constants->symbol_at(_loadable_descriptors->at(i));\n+        if (class_name == name) return true;\n+  }\n+  return false;\n+}\n+\n@@ -462,1 +483,2 @@\n-                                       parser.is_interface());\n+                                       parser.is_interface(),\n+                                       parser.is_inline_type());\n@@ -484,0 +506,3 @@\n+  } else if (parser.is_inline_type()) {\n+    \/\/ inline type\n+    ik = new (loader_data, size, THREAD) InlineKlass(parser);\n@@ -500,0 +525,6 @@\n+#ifdef ASSERT\n+  ik->bounds_check((address) ik->start_of_vtable(), false, size);\n+  ik->bounds_check((address) ik->start_of_itable(), false, size);\n+  ik->bounds_check((address) ik->end_of_itable(), true, size);\n+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);\n+#endif \/\/ASSERT\n@@ -503,0 +534,23 @@\n+#ifndef PRODUCT\n+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {\n+  const char* bad = nullptr;\n+  address end = nullptr;\n+  if (addr < (address)this) {\n+    bad = \"before\";\n+  } else if (addr == (address)this) {\n+    if (edge_ok)  return true;\n+    bad = \"just before\";\n+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {\n+    if (edge_ok)  return true;\n+    bad = \"just after\";\n+  } else if (addr > end) {\n+    bad = \"after\";\n+  } else {\n+    return true;\n+  }\n+  tty->print_cr(\"%s object bounds: \" INTPTR_FORMAT \" [\" INTPTR_FORMAT \"..\" INTPTR_FORMAT \"]\",\n+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);\n+  Verbose = WizardMode = true; this->print(); \/\/@@\n+  return false;\n+}\n+#endif \/\/PRODUCT\n@@ -530,2 +584,2 @@\n-InstanceKlass::InstanceKlass(const ClassFileParser& parser, KlassKind kind, ReferenceType reference_type) :\n-  Klass(kind),\n+InstanceKlass::InstanceKlass(const ClassFileParser& parser, KlassKind kind, markWord prototype_header, ReferenceType reference_type) :\n+  Klass(kind, prototype_header),\n@@ -542,1 +596,4 @@\n-  _init_thread(nullptr)\n+  _init_thread(nullptr),\n+  _inline_layout_info_array(nullptr),\n+  _loadable_descriptors(nullptr),\n+  _adr_inlineklass_fixed_block(nullptr)\n@@ -549,0 +606,3 @@\n+  if (parser.has_inline_fields()) {\n+    set_has_inline_type_fields();\n+  }\n@@ -693,0 +753,5 @@\n+  if (inline_layout_info_array() != nullptr) {\n+    MetadataFactory::free_array<InlineLayoutInfo>(loader_data, inline_layout_info_array());\n+  }\n+  set_inline_layout_info_array(nullptr);\n+\n@@ -727,0 +792,7 @@\n+  if (loadable_descriptors() != nullptr &&\n+      loadable_descriptors() != Universe::the_empty_short_array() &&\n+      !loadable_descriptors()->in_aot_cache()) {\n+    MetadataFactory::free_array<jushort>(loader_data, loadable_descriptors());\n+  }\n+  set_loadable_descriptors(nullptr);\n+\n@@ -895,0 +967,38 @@\n+static void load_classes_from_loadable_descriptors_attribute(InstanceKlass *ik, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  if (ik->loadable_descriptors() != nullptr && PreloadClasses) {\n+    HandleMark hm(THREAD);\n+    for (int i = 0; i < ik->loadable_descriptors()->length(); i++) {\n+      Symbol* sig = ik->constants()->symbol_at(ik->loadable_descriptors()->at(i));\n+      if (!Signature::has_envelope(sig)) continue;\n+      TempNewSymbol class_name = Signature::strip_envelope(sig);\n+      if (class_name == ik->name()) continue;\n+      log_info(class, preload)(\"Preloading of class %s during linking of class %s \"\n+                               \"because of the class is listed in the LoadableDescriptors attribute\",\n+                               sig->as_C_string(), ik->name()->as_C_string());\n+      oop loader = ik->class_loader();\n+      Klass* klass = SystemDictionary::resolve_or_null(class_name,\n+                                                        Handle(THREAD, loader), THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        CLEAR_PENDING_EXCEPTION;\n+      }\n+      if (klass != nullptr) {\n+        log_info(class, preload)(\"Preloading of class %s during linking of class %s \"\n+                                 \"(cause: LoadableDescriptors attribute) succeeded\",\n+                                 class_name->as_C_string(), ik->name()->as_C_string());\n+        if (!klass->is_inline_klass()) {\n+          \/\/ Non value class are allowed by the current spec, but it could be an indication\n+          \/\/ of an issue so let's log a warning\n+          log_warning(class, preload)(\"Preloading of class %s during linking of class %s \"\n+                                      \"(cause: LoadableDescriptors attribute) but loaded class is not a value class\",\n+                                      class_name->as_C_string(), ik->name()->as_C_string());\n+        }\n+      } else {\n+        log_warning(class, preload)(\"Preloading of class %s during linking of class %s \"\n+                                    \"(cause: LoadableDescriptors attribute) failed\",\n+                                    class_name->as_C_string(), ik->name()->as_C_string());\n+      }\n+    }\n+  }\n+}\n+\n@@ -965,0 +1075,7 @@\n+  if (EnableValhalla) {\n+    \/\/ Aggressively preloading all classes from the LoadableDescriptors attribute\n+    \/\/ so inline classes can be scalarized in the calling conventions computed below\n+    load_classes_from_loadable_descriptors_attribute(this, THREAD);\n+    assert(!HAS_PENDING_EXCEPTION, \"Shouldn't have pending exceptions from call above\");\n+  }\n+\n@@ -1269,0 +1386,21 @@\n+  \/\/ Pre-allocating an all-zero value to be used to reset nullable flat storages\n+  if (is_inline_klass()) {\n+      InlineKlass* vk = InlineKlass::cast(this);\n+      if (vk->has_nullable_atomic_layout()) {\n+        oop val = vk->allocate_instance(THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+            Handle e(THREAD, PENDING_EXCEPTION);\n+            CLEAR_PENDING_EXCEPTION;\n+            {\n+                EXCEPTION_MARK;\n+                add_initialization_error(THREAD, e);\n+                \/\/ Locks object, set state, and notify all waiting threads\n+                set_initialization_state_and_notify(initialization_error, THREAD);\n+                CLEAR_PENDING_EXCEPTION;\n+            }\n+            THROW_OOP(e());\n+        }\n+        vk->set_null_reset_value(val);\n+      }\n+  }\n+\n@@ -1301,1 +1439,0 @@\n-\n@@ -1322,0 +1459,30 @@\n+\n+    if (has_strict_static_fields() && !HAS_PENDING_EXCEPTION) {\n+      \/\/ Step 9 also verifies that strict static fields have been initialized.\n+      \/\/ Status bits were set in ClassFileParser::post_process_parsed_stream.\n+      \/\/ After <clinit>, bits must all be clear, or else we must throw an error.\n+      \/\/ This is an extremely fast check, so we won't bother with a timer.\n+      assert(fields_status() != nullptr, \"\");\n+      Symbol* bad_strict_static = nullptr;\n+      for (int index = 0; index < fields_status()->length(); index++) {\n+        \/\/ Very fast loop over single byte array looking for a set bit.\n+        if (fields_status()->adr_at(index)->is_strict_static_unset()) {\n+          \/\/ This strict static field has not been set by the class initializer.\n+          \/\/ Note that in the common no-error case, we read no field metadata.\n+          \/\/ We only unpack it when we need to report an error.\n+          FieldInfo fi = field(index);\n+          bad_strict_static = fi.name(constants());\n+          if (debug_logging_enabled) {\n+            ResourceMark rm(jt);\n+            const char* msg = format_strict_static_message(bad_strict_static);\n+            log_debug(class, init)(\"%s\", msg);\n+          } else {\n+            \/\/ If we are not logging, do not bother to look for a second offense.\n+            break;\n+          }\n+        }\n+      }\n+      if (bad_strict_static != nullptr) {\n+        throw_strict_static_exception(bad_strict_static, \"is unset after initialization of\", THREAD);\n+      }\n+    }\n@@ -1375,0 +1542,68 @@\n+void InstanceKlass::notify_strict_static_access(int field_index, bool is_writing, TRAPS) {\n+  guarantee(field_index >= 0 && field_index < fields_status()->length(), \"valid field index\");\n+  DEBUG_ONLY(FieldInfo debugfi = field(field_index));\n+  assert(debugfi.access_flags().is_strict(), \"\");\n+  assert(debugfi.access_flags().is_static(), \"\");\n+  FieldStatus& fs = *fields_status()->adr_at(field_index);\n+  LogTarget(Trace, class, init) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm(THREAD);\n+    LogStream ls(lt);\n+    FieldInfo fi = field(field_index);\n+    ls.print(\"notify %s %s %s%s \",\n+             external_name(), is_writing? \"Write\" : \"Read\",\n+             fs.is_strict_static_unset() ? \"Unset\" : \"(set)\",\n+             fs.is_strict_static_unread() ? \"+Unread\" : \"\");\n+    fi.print(&ls, constants());\n+  }\n+  if (fs.is_strict_static_unset()) {\n+    assert(fs.is_strict_static_unread(), \"ClassFileParser resp.\");\n+    \/\/ If it is not set, there are only two reasonable things we can do here:\n+    \/\/ - mark it set if this is putstatic\n+    \/\/ - throw an error (Read-Before-Write) if this is getstatic\n+\n+    \/\/ The unset state is (or should be) transient, and observable only in one\n+    \/\/ thread during the execution of <clinit>.  Something is wrong here as this\n+    \/\/ should not be possible\n+    guarantee(is_reentrant_initialization(THREAD), \"unscoped access to strict static\");\n+    if (is_writing) {\n+      \/\/ clear the \"unset\" bit, since the field is actually going to be written\n+      fs.update_strict_static_unset(false);\n+    } else {\n+      \/\/ throw an IllegalStateException, since we are reading before writing\n+      \/\/ see also InstanceKlass::initialize_impl, Step 8 (at end)\n+      Symbol* bad_strict_static = field(field_index).name(constants());\n+      throw_strict_static_exception(bad_strict_static, \"is unset before first read in\", CHECK);\n+    }\n+  } else {\n+    \/\/ Ensure no write after read for final strict statics\n+    FieldInfo fi = field(field_index);\n+    bool is_final = fi.access_flags().is_final();\n+    if (is_final) {\n+      \/\/ no final write after read, so observing a constant freezes it, as if <clinit> ended early\n+      \/\/ (maybe we could trust the constant a little earlier, before <clinit> ends)\n+      if (is_writing && !fs.is_strict_static_unread()) {\n+        Symbol* bad_strict_static = fi.name(constants());\n+        throw_strict_static_exception(bad_strict_static, \"is set after read (as final) in\", CHECK);\n+      } else if (!is_writing && fs.is_strict_static_unread()) {\n+        fs.update_strict_static_unread(false);\n+      }\n+    }\n+  }\n+}\n+\n+void InstanceKlass::throw_strict_static_exception(Symbol* field_name, const char* when, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  const char* msg = format_strict_static_message(field_name, when);\n+  THROW_MSG(vmSymbols::java_lang_IllegalStateException(), msg);\n+}\n+\n+const char* InstanceKlass::format_strict_static_message(Symbol* field_name, const char* when) {\n+  stringStream ss;\n+  ss.print(\"Strict static \\\"%s\\\" %s %s\",\n+           field_name->as_C_string(),\n+           when == nullptr ? \"is unset in\" : when,\n+           external_name());\n+  return ss.as_string();\n+}\n+\n@@ -1625,1 +1860,1 @@\n-  ObjArrayKlass* ak = array_klasses();\n+  ArrayKlass* ak = array_klasses();\n@@ -1632,2 +1867,2 @@\n-  ObjArrayKlass* oak = array_klasses_acquire();\n-  if (oak == nullptr) {\n+  ArrayKlass* ak = array_klasses_acquire();\n+  if (ak == nullptr) {\n@@ -1636,1 +1871,1 @@\n-    return oak->array_klass_or_null(n);\n+    return ak->array_klass_or_null(n);\n@@ -1653,1 +1888,1 @@\n-  if (clinit != nullptr && clinit->has_valid_initializer_flags()) {\n+  if (clinit != nullptr && clinit->is_class_initializer()) {\n@@ -1762,4 +1997,0 @@\n-bool InstanceKlass::contains_field_offset(int offset) {\n-  fieldDescriptor fd;\n-  return find_field_from_offset(offset, false, &fd);\n-}\n@@ -1846,0 +2077,9 @@\n+bool InstanceKlass::contains_field_offset(int offset) {\n+  if (this->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(this);\n+    return offset >= vk->payload_offset() && offset < (vk->payload_offset() + vk->payload_size_in_bytes());\n+  } else {\n+    fieldDescriptor fd;\n+    return find_field_from_offset(offset, false, &fd);\n+  }\n+}\n@@ -2229,0 +2469,3 @@\n+    if (name == vmSymbols::object_initializer_name()) {\n+      break;  \/\/ <init> is never inherited\n+    }\n@@ -2641,0 +2884,1 @@\n+  it->push(&_loadable_descriptors);\n@@ -2642,0 +2886,1 @@\n+  it->push(&_inline_layout_info_array, MetaspaceClosure::_writable);\n@@ -2689,1 +2934,1 @@\n-  \/\/ These are not allocated from metaspace. They are safe to set to null.\n+  \/\/ These are not allocated from metaspace. They are safe to set to nullptr.\n@@ -2780,0 +3025,4 @@\n+  if (is_inline_klass()) {\n+    InlineKlass::cast(this)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -2813,1 +3062,1 @@\n-    assert(this == array_klasses()->bottom_klass(), \"sanity\");\n+    assert(this == ObjArrayKlass::cast(array_klasses())->bottom_klass(), \"sanity\");\n@@ -2816,0 +3065,6 @@\n+    if (class_loader_data() == nullptr) {\n+      ResourceMark rm(THREAD);\n+      log_debug(cds)(\"  loader_data %s \", loader_data == nullptr ? \"nullptr\" : \"non null\");\n+      log_debug(cds)(\"  this %s array_klasses %s \", this->name()->as_C_string(), array_klasses()->name()->as_C_string());\n+    }\n+    assert(!array_klasses()->is_refined_objArray_klass(), \"must be non-refined objarrayklass\");\n@@ -2971,0 +3226,4 @@\n+bool InstanceKlass::supports_inline_types() const {\n+  return major_version() >= Verifier::VALUE_TYPES_MAJOR_VERSION && minor_version() == Verifier::JAVA_PREVIEW_MINOR_VERSION;\n+}\n+\n@@ -3003,0 +3262,2 @@\n+  return signature_name_of_carrier(JVM_SIGNATURE_CLASS);\n+}\n@@ -3004,0 +3265,1 @@\n+const char* InstanceKlass::signature_name_of_carrier(char c) const {\n@@ -3010,1 +3272,1 @@\n-  \/\/ Add L as type indicator\n+  \/\/ Add L or Q as type indicator\n@@ -3012,1 +3274,1 @@\n-  dest[dest_index++] = JVM_SIGNATURE_CLASS;\n+  dest[dest_index++] = c;\n@@ -3293,0 +3555,19 @@\n+void InstanceKlass::check_can_be_annotated_with_NullRestricted(InstanceKlass* type, Symbol* container_klass_name, TRAPS) {\n+  assert(type->is_instance_klass(), \"Sanity check\");\n+  if (type->is_identity_class()) {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+              err_msg(\"Class %s expects class %s to be a value class, but it is an identity class\",\n+              container_klass_name->as_C_string(),\n+              type->external_name()));\n+  }\n+\n+  if (type->is_abstract()) {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+              err_msg(\"Class %s expects class %s to be concrete value type, but it is an abstract class\",\n+              container_klass_name->as_C_string(),\n+              type->external_name()));\n+  }\n+}\n+\n@@ -3359,2 +3640,1 @@\n-  \/\/ Remember to strip ACC_SUPER bit\n-  return (access & (~JVM_ACC_SUPER));\n+  return access;\n@@ -3614,1 +3894,4 @@\n-static void print_vtable(intptr_t* start, int len, outputStream* st) {\n+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {\n+  ResourceMark rm;\n+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);\n+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;\n@@ -3618,0 +3901,5 @@\n+    if (forward_refs[i] != 0) {\n+      int from = forward_refs[i];\n+      int off = (int) start[from];\n+      st->print(\" (offset %d <= [%d])\", off, from);\n+    }\n@@ -3621,0 +3909,6 @@\n+    } else if (self != nullptr && e > 0 && e < 0x10000) {\n+      address location = self + e;\n+      int index = (int)((intptr_t*)location - start);\n+      st->print(\" (offset %d => [%d])\", (int)e, index);\n+      if (index >= 0 && index < len)\n+        forward_refs[index] = i;\n@@ -3627,1 +3921,22 @@\n-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);\n+  return print_vtable(nullptr, reinterpret_cast<intptr_t*>(start), len, st);\n+}\n+\n+template<typename T>\n+ static void print_array_on(outputStream* st, Array<T>* array) {\n+   if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+   array->print_value_on(st); st->cr();\n+   if (Verbose || WizardMode) {\n+     for (int i = 0; i < array->length(); i++) {\n+       st->print(\"%d : \", i); array->at(i)->print_value_on(st); st->cr();\n+     }\n+   }\n+ }\n+\n+static void print_array_on(outputStream* st, Array<int>* array) {\n+  if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+  array->print_value_on(st); st->cr();\n+  if (Verbose || WizardMode) {\n+    for (int i = 0; i < array->length(); i++) {\n+      st->print(\"%d : %d\", i, array->at(i)); st->cr();\n+    }\n+  }\n@@ -3668,8 +3983,2 @@\n-  st->print(BULLET\"methods:           \"); methods()->print_value_on(st);               st->cr();\n-  if (Verbose || WizardMode) {\n-    Array<Method*>* method_array = methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n-  st->print(BULLET\"method ordering:   \"); method_ordering()->print_value_on(st);      st->cr();\n+  st->print(BULLET\"methods:           \"); print_array_on(st, methods());\n+  st->print(BULLET\"method ordering:   \"); print_array_on(st, method_ordering());\n@@ -3677,7 +3986,1 @@\n-    st->print(BULLET\"default_methods:   \"); default_methods()->print_value_on(st);    st->cr();\n-    if (Verbose) {\n-      Array<Method*>* method_array = default_methods();\n-      for (int i = 0; i < method_array->length(); i++) {\n-        st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-      }\n-    }\n+    st->print(BULLET\"default_methods:   \"); print_array_on(st, default_methods());\n@@ -3743,0 +4046,1 @@\n+  st->print(BULLET\"loadable descriptors:     \"); loadable_descriptors()->print_value_on(st); st->cr();\n@@ -3753,1 +4057,1 @@\n-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);\n+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(nullptr, start_of_itable(), itable_length(), st);\n@@ -3785,0 +4089,1 @@\n+  for (int i = 0; i < _indent; i++) _st->print(\"  \");\n@@ -3787,1 +4092,1 @@\n-     fd->print_on(_st);\n+     fd->print_on(_st, _base_offset);\n@@ -3790,2 +4095,2 @@\n-     fd->print_on_for(_st, _obj);\n-     _st->cr();\n+     fd->print_on_for(_st, _obj, _indent, _base_offset);\n+     if (!fd->field_flags().is_flat()) _st->cr();\n@@ -3796,1 +4101,1 @@\n-void InstanceKlass::oop_print_on(oop obj, outputStream* st) {\n+void InstanceKlass::oop_print_on(oop obj, outputStream* st, int indent, int base_offset) {\n@@ -3812,1 +4117,1 @@\n-  FieldPrinter print_field(st, obj);\n+  FieldPrinter print_field(st, obj, indent, base_offset);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":348,"deletions":43,"binary":false,"changes":391,"status":"modified"},{"patch":"@@ -277,14 +277,0 @@\n-static markWord make_prototype(const Klass* kls) {\n-  markWord prototype = markWord::prototype();\n-#ifdef _LP64\n-  if (UseCompactObjectHeaders) {\n-    \/\/ With compact object headers, the narrow Klass ID is part of the mark word.\n-    \/\/ We therefore seed the mark word with the narrow Klass ID.\n-    precond(CompressedKlassPointers::is_encodable(kls));\n-    const narrowKlass nk = CompressedKlassPointers::encode(const_cast<Klass*>(kls));\n-    prototype = prototype.set_narrow_klass(nk);\n-  }\n-#endif\n-  return prototype;\n-}\n-\n@@ -303,2 +289,1 @@\n-Klass::Klass(KlassKind kind) : _kind(kind),\n-                               _prototype_header(make_prototype(this)),\n+Klass::Klass(KlassKind kind, markWord prototype_header) : _kind(kind),\n@@ -306,0 +291,1 @@\n+  set_prototype_header(make_prototype_header(this, prototype_header));\n@@ -318,2 +304,2 @@\n-  int  tag   =  isobj ? _lh_array_tag_obj_value : _lh_array_tag_type_value;\n-  int lh = array_layout_helper(tag, hsize, etype, exact_log2(esize));\n+  int  tag   =  isobj ? _lh_array_tag_ref_value : _lh_array_tag_type_value;\n+  int lh = array_layout_helper(tag, false, hsize, etype, exact_log2(esize));\n@@ -323,1 +309,1 @@\n-  assert(layout_helper_is_objArray(lh) == isobj, \"correct kind\");\n+  assert(layout_helper_is_refArray(lh) == isobj, \"correct kind\");\n@@ -1035,4 +1021,2 @@\n-     if (UseCompactObjectHeaders) {\n-       st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n-       st->cr();\n-     }\n+     st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n+     st->cr();\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":7,"deletions":23,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -69,12 +69,16 @@\n-  enum KlassKind : u2 {\n-    InstanceKlassKind,\n-    InstanceRefKlassKind,\n-    InstanceMirrorKlassKind,\n-    InstanceClassLoaderKlassKind,\n-    InstanceStackChunkKlassKind,\n-    TypeArrayKlassKind,\n-    ObjArrayKlassKind,\n-    UnknownKlassKind\n-  };\n-\n-  static const uint KLASS_KIND_COUNT = ObjArrayKlassKind + 1;\n+   enum KlassKind : u2\n+   {\n+     InstanceKlassKind,\n+     InlineKlassKind,\n+     InstanceRefKlassKind,\n+     InstanceMirrorKlassKind,\n+     InstanceClassLoaderKlassKind,\n+     InstanceStackChunkKlassKind,\n+     TypeArrayKlassKind,\n+     ObjArrayKlassKind,\n+     RefArrayKlassKind,\n+     FlatArrayKlassKind,\n+     UnknownKlassKind\n+   };\n+\n+   static const uint KLASS_KIND_COUNT = FlatArrayKlassKind + 1;\n@@ -102,1 +106,1 @@\n-  \/\/    tag is 0x80 if the elements are oops, 0xC0 if non-oops\n+  \/\/    tag is 0x80 if the elements are oops, 0xC0 if non-oops, 0xA0 if value types\n@@ -207,1 +211,1 @@\n-  Klass(KlassKind kind);\n+  Klass(KlassKind kind, markWord prototype_header = markWord::prototype());\n@@ -475,1 +479,1 @@\n-  static const int _lh_array_tag_bits          = 2;\n+  static const int _lh_array_tag_bits          = 4;\n@@ -477,2 +481,9 @@\n-  static const int _lh_array_tag_obj_value     = ~0x01;   \/\/ 0x80000000 >> 30\n-  static const unsigned int _lh_array_tag_type_value = 0Xffffffff; \/\/ ~0x00,  \/\/ 0xC0000000 >> 30\n+  static const unsigned int _lh_array_tag_type_value = 0Xfffffffc;\n+  static const unsigned int _lh_array_tag_flat_value = 0Xfffffffa;\n+  static const unsigned int _lh_array_tag_ref_value  = 0Xfffffff8;\n+\n+  \/\/ null-free array flag bit under the array tag bits, shift one more to get array tag value\n+  static const int _lh_null_free_shift = _lh_array_tag_shift - 1;\n+  static const int _lh_null_free_mask  = 1;\n+\n+  static const jint _lh_array_tag_flat_value_bit_inplace = (jint) (1 << (_lh_array_tag_shift + 1));\n@@ -496,2 +507,1 @@\n-    \/\/ _lh_array_tag_type_value == (lh >> _lh_array_tag_shift);\n-    return (juint)lh >= (juint)(_lh_array_tag_type_value << _lh_array_tag_shift);\n+    return (juint) _lh_array_tag_type_value == (juint)(lh >> _lh_array_tag_shift);\n@@ -499,3 +509,14 @@\n-  static bool layout_helper_is_objArray(jint lh) {\n-    \/\/ _lh_array_tag_obj_value == (lh >> _lh_array_tag_shift);\n-    return (jint)lh < (jint)(_lh_array_tag_type_value << _lh_array_tag_shift);\n+  static bool layout_helper_is_refArray(jint lh) {\n+    return (juint)_lh_array_tag_ref_value == (juint)(lh >> _lh_array_tag_shift);\n+  }\n+  static bool layout_helper_is_flatArray(jint lh) {\n+    return (juint)_lh_array_tag_flat_value == (juint)(lh >> _lh_array_tag_shift);\n+  }\n+  static bool layout_helper_is_null_free(jint lh) {\n+    assert(layout_helper_is_flatArray(lh) || layout_helper_is_refArray(lh), \"must be array of inline types\");\n+    return ((lh >> _lh_null_free_shift) & _lh_null_free_mask);\n+  }\n+  static jint layout_helper_set_null_free(jint lh) {\n+    lh |= (_lh_null_free_mask << _lh_null_free_shift);\n+    assert(layout_helper_is_null_free(lh), \"Bad encoding\");\n+    return lh;\n@@ -512,1 +533,1 @@\n-    assert(btvalue >= T_BOOLEAN && btvalue <= T_OBJECT, \"sanity\");\n+    assert((btvalue >= T_BOOLEAN && btvalue <= T_OBJECT) || btvalue == T_FLAT_ELEMENT, \"sanity\");\n@@ -533,1 +554,1 @@\n-    assert(l2esz <= LogBytesPerLong,\n+    assert(layout_helper_element_type(lh) == T_FLAT_ELEMENT || l2esz <= LogBytesPerLong,\n@@ -537,1 +558,1 @@\n-  static jint array_layout_helper(jint tag, int hsize, BasicType etype, int log2_esize) {\n+  static jint array_layout_helper(jint tag, bool null_free, int hsize, BasicType etype, int log2_esize) {\n@@ -539,0 +560,1 @@\n+      |    ((null_free ? 1 : 0) <<  _lh_null_free_shift)\n@@ -681,0 +703,1 @@\n+  virtual bool is_refArray_klass_slow()     const { return false; }\n@@ -682,0 +705,1 @@\n+  virtual bool is_flatArray_klass_slow()    const { return false; }\n@@ -683,0 +707,2 @@\n+  \/\/ current implementation uses this method even in non debug builds\n+  virtual bool is_inline_klass_slow()       const { return false; }\n@@ -698,2 +724,1 @@\n-  \/\/ Other is anything that is not one of the more specialized kinds of InstanceKlass.\n-  bool is_other_instance_klass()        const { return _kind == InstanceKlassKind; }\n+  bool is_inline_klass()                const { return assert_same_query(_kind == InlineKlassKind, is_inline_klass_slow()); }\n@@ -705,1 +730,3 @@\n-  bool is_objArray_klass()              const { return assert_same_query( _kind == ObjArrayKlassKind,  is_objArray_klass_slow()); }\n+  bool is_flatArray_klass()             const { return assert_same_query( _kind == FlatArrayKlassKind, is_flatArray_klass_slow()); }\n+  bool is_objArray_klass()              const { return assert_same_query( _kind == ObjArrayKlassKind || _kind == RefArrayKlassKind || _kind == FlatArrayKlassKind,  is_objArray_klass_slow()); }\n+  bool is_refArray_klass()              const { return assert_same_query( _kind == RefArrayKlassKind, is_refArray_klass_slow()); }\n@@ -707,0 +734,1 @@\n+  bool is_refined_objArray_klass()      const { return is_refArray_klass() || is_flatArray_klass(); }\n@@ -709,0 +737,2 @@\n+  inline bool is_null_free_array_klass() const { return !is_typeArray_klass() && layout_helper_is_null_free(layout_helper()); }\n+\n@@ -717,1 +747,1 @@\n-  bool is_super() const                 { return _access_flags.is_super(); }\n+  bool is_identity_class() const        { assert(is_instance_klass(), \"only for instanceKlass\"); return _access_flags.is_identity_class(); }\n@@ -734,0 +764,1 @@\n+  static inline markWord make_prototype_header(const Klass* kls, markWord prototype = markWord::prototype());\n@@ -737,0 +768,1 @@\n+  inline void set_prototype_header_klass(narrowKlass klass);\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":61,"deletions":29,"binary":false,"changes":90,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"cds\/cdsConfig.hpp\"\n@@ -34,0 +35,1 @@\n+#include \"memory\/oopFactory.hpp\"\n@@ -37,0 +39,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -39,0 +42,1 @@\n+#include \"oops\/markWord.hpp\"\n@@ -42,0 +46,1 @@\n+#include \"oops\/refArrayKlass.hpp\"\n@@ -43,0 +48,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -47,1 +53,3 @@\n-ObjArrayKlass* ObjArrayKlass::allocate_klass(ClassLoaderData* loader_data, int n, Klass* k, Symbol* name, TRAPS) {\n+ObjArrayKlass* ObjArrayKlass::allocate_klass(ClassLoaderData* loader_data, int n,\n+                                       Klass* k, Symbol* name, ArrayKlass::ArrayProperties props,\n+                                       TRAPS) {\n@@ -53,1 +61,1 @@\n-  return new (loader_data, size, THREAD) ObjArrayKlass(n, k, name);\n+  return new (loader_data, size, THREAD) ObjArrayKlass(n, k, name, Kind, props, ArrayKlass::is_null_restricted(props) ? markWord::null_free_array_prototype() : markWord::prototype());\n@@ -77,1 +85,1 @@\n-                                                      int n, Klass* element_klass, TRAPS) {\n+                                                      int n, Klass* element_klass,  TRAPS) {\n@@ -105,1 +113,1 @@\n-  ObjArrayKlass* oak = ObjArrayKlass::allocate_klass(loader_data, n, element_klass, name, CHECK_NULL);\n+  ObjArrayKlass* oak = ObjArrayKlass::allocate_klass(loader_data, n, element_klass, name, ArrayProperties::INVALID, CHECK_NULL);\n@@ -123,1 +131,2 @@\n-ObjArrayKlass::ObjArrayKlass(int n, Klass* element_klass, Symbol* name) : ArrayKlass(name, Kind) {\n+ObjArrayKlass::ObjArrayKlass(int n, Klass* element_klass, Symbol* name, KlassKind kind, ArrayKlass::ArrayProperties props, markWord mk) :\n+ArrayKlass(name, kind, props, mk) {\n@@ -126,0 +135,2 @@\n+  set_next_refined_klass_klass(nullptr);\n+  set_properties(props);\n@@ -131,0 +142,1 @@\n+    assert(!element_klass->is_refArray_klass(), \"Sanity\");\n@@ -141,1 +153,9 @@\n-  set_layout_helper(array_layout_helper(T_OBJECT));\n+  int lh = array_layout_helper(T_OBJECT);\n+  if (ArrayKlass::is_null_restricted(props)) {\n+    assert(n == 1, \"Bytecode does not support null-free multi-dim\");\n+    lh = layout_helper_set_null_free(lh);\n+#ifdef _LP64\n+    assert(prototype_header().is_null_free_array(), \"sanity\");\n+#endif\n+  }\n+  set_layout_helper(lh);\n@@ -151,1 +171,57 @@\n-  return objArrayOop(obj)->object_size();\n+  \/\/ return objArrayOop(obj)->object_size();\n+  return obj->is_flatArray() ? flatArrayOop(obj)->object_size(layout_helper()) : refArrayOop(obj)->object_size();\n+}\n+\n+ArrayDescription ObjArrayKlass::array_layout_selection(Klass* element, ArrayProperties properties) {\n+  \/\/ TODO FIXME: the layout selection should take the array size in consideration\n+  \/\/ to avoid creation of arrays too big to be handled by the VM. See JDK-8233189\n+  if (!UseArrayFlattening || element->is_array_klass() || element->is_identity_class() || element->is_abstract()) {\n+    return ArrayDescription(RefArrayKlassKind, properties, LayoutKind::REFERENCE);\n+  }\n+  assert(element->is_final(), \"Flat layouts below require monomorphic elements\");\n+  InlineKlass* vk = InlineKlass::cast(element);\n+  if (is_null_restricted(properties)) {\n+    if (is_non_atomic(properties)) {\n+      \/\/ Null-restricted + non-atomic\n+      if (vk->maybe_flat_in_array() && vk->has_non_atomic_layout()) {\n+        return ArrayDescription(FlatArrayKlassKind, properties, LayoutKind::NON_ATOMIC_FLAT);\n+      } else {\n+        return ArrayDescription(RefArrayKlassKind, properties, LayoutKind::REFERENCE);\n+      }\n+    } else {\n+      \/\/ Null-restricted + atomic\n+      if (vk->maybe_flat_in_array() && vk->is_naturally_atomic() && vk->has_non_atomic_layout()) {\n+        return ArrayDescription(FlatArrayKlassKind, properties, LayoutKind::NON_ATOMIC_FLAT);\n+      } else if (vk->maybe_flat_in_array() && vk->has_atomic_layout()) {\n+        return ArrayDescription(FlatArrayKlassKind, properties, LayoutKind::ATOMIC_FLAT);\n+      } else {\n+        return ArrayDescription(RefArrayKlassKind, properties, LayoutKind::REFERENCE);\n+      }\n+    }\n+  } else {\n+    \/\/ nullable implies atomic, so the non-atomic property is ignored\n+    if (vk->maybe_flat_in_array() && vk->has_nullable_atomic_layout()) {\n+      return ArrayDescription(FlatArrayKlassKind, properties, LayoutKind::NULLABLE_ATOMIC_FLAT);\n+    } else {\n+      return ArrayDescription(RefArrayKlassKind, properties, LayoutKind::REFERENCE);\n+    }\n+  }\n+}\n+\n+ObjArrayKlass* ObjArrayKlass::allocate_klass_with_properties(ArrayKlass::ArrayProperties props, TRAPS) {\n+  ObjArrayKlass* ak = nullptr;\n+  ArrayDescription ad = ObjArrayKlass::array_layout_selection(element_klass(), props);\n+  switch (ad._kind) {\n+    case Klass::RefArrayKlassKind: {\n+      ak = RefArrayKlass::allocate_refArray_klass(class_loader_data(), dimension(), element_klass(), props, CHECK_NULL);\n+      break;\n+    }\n+    case Klass::FlatArrayKlassKind: {\n+      assert(dimension() == 1, \"Flat arrays can only be dimension 1 arrays\");\n+      ak = FlatArrayKlass::allocate_klass(element_klass(), props, ad._layout_kind, CHECK_NULL);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n+  return ak;\n@@ -154,1 +230,1 @@\n-objArrayOop ObjArrayKlass::allocate_instance(int length, TRAPS) {\n+objArrayOop ObjArrayKlass::allocate_instance(int length, ArrayProperties props, TRAPS) {\n@@ -156,3 +232,19 @@\n-  size_t size = objArrayOopDesc::object_size(length);\n-  return (objArrayOop)Universe::heap()->array_allocate(this, size, length,\n-                                                       \/* do_zero *\/ true, THREAD);\n+  ObjArrayKlass* ak = klass_with_properties(props, THREAD);\n+  size_t size = 0;\n+  switch(ak->kind()) {\n+    case Klass::RefArrayKlassKind:\n+      size = refArrayOopDesc::object_size(length);\n+      break;\n+    case Klass::FlatArrayKlassKind:\n+      size = flatArrayOopDesc::object_size(ak->layout_helper(), length);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+  assert(size != 0, \"Sanity check\");\n+  objArrayOop array = (objArrayOop)Universe::heap()->array_allocate(\n+    ak, size, length,\n+    \/* do_zero *\/ true, CHECK_NULL);\n+  assert(array->is_refArray() || array->is_flatArray(), \"Must be\");\n+  objArrayHandle array_h(THREAD, array);\n+  return array_h();\n@@ -165,1 +257,3 @@\n-  objArrayOop array = allocate_instance(length, CHECK_NULL);\n+  ObjArrayKlass* oak = klass_with_properties(ArrayProperties::DEFAULT, CHECK_NULL);\n+  assert(oak->is_refArray_klass() || oak->is_flatArray_klass(), \"Must be\");\n+  objArrayOop array = oak->allocate_instance(length, ArrayProperties::DEFAULT, CHECK_NULL);\n@@ -170,1 +264,1 @@\n-        oop sub_array = ld_klass->multi_allocate(rank - 1, &sizes[1], CHECK_NULL);\n+        oop sub_array = ld_klass->multi_allocate(rank-1, &sizes[1], CHECK_NULL);\n@@ -188,36 +282,0 @@\n-\/\/ Either oop or narrowOop depending on UseCompressedOops.\n-void ObjArrayKlass::do_copy(arrayOop s, size_t src_offset,\n-                            arrayOop d, size_t dst_offset, int length, TRAPS) {\n-  if (s == d) {\n-    \/\/ since source and destination are equal we do not need conversion checks.\n-    assert(length > 0, \"sanity check\");\n-    ArrayAccess<>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n-  } else {\n-    \/\/ We have to make sure all elements conform to the destination array\n-    Klass* bound = ObjArrayKlass::cast(d->klass())->element_klass();\n-    Klass* stype = ObjArrayKlass::cast(s->klass())->element_klass();\n-    if (stype == bound || stype->is_subtype_of(bound)) {\n-      \/\/ elements are guaranteed to be subtypes, so no check necessary\n-      ArrayAccess<ARRAYCOPY_DISJOINT>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n-    } else {\n-      \/\/ slow case: need individual subtype checks\n-      \/\/ note: don't use obj_at_put below because it includes a redundant store check\n-      if (!ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_CHECKCAST>::oop_arraycopy(s, src_offset, d, dst_offset, length)) {\n-        ResourceMark rm(THREAD);\n-        stringStream ss;\n-        if (!bound->is_subtype_of(stype)) {\n-          ss.print(\"arraycopy: type mismatch: can not copy %s[] into %s[]\",\n-                   stype->external_name(), bound->external_name());\n-        } else {\n-          \/\/ oop_arraycopy should return the index in the source array that\n-          \/\/ contains the problematic oop.\n-          ss.print(\"arraycopy: element type mismatch: can not cast one of the elements\"\n-                   \" of %s[] to the type of the destination array, %s\",\n-                   stype->external_name(), bound->external_name());\n-        }\n-        THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n-      }\n-    }\n-  }\n-}\n-\n@@ -228,25 +286,4 @@\n-  if (!d->is_objArray()) {\n-    ResourceMark rm(THREAD);\n-    stringStream ss;\n-    if (d->is_typeArray()) {\n-      ss.print(\"arraycopy: type mismatch: can not copy object array[] into %s[]\",\n-               type2name_tab[ArrayKlass::cast(d->klass())->element_type()]);\n-    } else {\n-      ss.print(\"arraycopy: destination type %s is not an array\", d->klass()->external_name());\n-    }\n-    THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n-  }\n-\n-  \/\/ Check is all offsets and lengths are non negative\n-  if (src_pos < 0 || dst_pos < 0 || length < 0) {\n-    \/\/ Pass specific exception reason.\n-    ResourceMark rm(THREAD);\n-    stringStream ss;\n-    if (src_pos < 0) {\n-      ss.print(\"arraycopy: source index %d out of bounds for object array[%d]\",\n-               src_pos, s->length());\n-    } else if (dst_pos < 0) {\n-      ss.print(\"arraycopy: destination index %d out of bounds for object array[%d]\",\n-               dst_pos, d->length());\n-    } else {\n-      ss.print(\"arraycopy: length %d is negative\", length);\n+  if (UseArrayFlattening) {\n+    if (d->is_flatArray()) {\n+      FlatArrayKlass::cast(d->klass())->copy_array(s, src_pos, d, dst_pos, length, THREAD);\n+      return;\n@@ -254,14 +291,3 @@\n-    THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n-  }\n-  \/\/ Check if the ranges are valid\n-  if ((((unsigned int) length + (unsigned int) src_pos) > (unsigned int) s->length()) ||\n-      (((unsigned int) length + (unsigned int) dst_pos) > (unsigned int) d->length())) {\n-    \/\/ Pass specific exception reason.\n-    ResourceMark rm(THREAD);\n-    stringStream ss;\n-    if (((unsigned int) length + (unsigned int) src_pos) > (unsigned int) s->length()) {\n-      ss.print(\"arraycopy: last source index %u out of bounds for object array[%d]\",\n-               (unsigned int) length + (unsigned int) src_pos, s->length());\n-    } else {\n-      ss.print(\"arraycopy: last destination index %u out of bounds for object array[%d]\",\n-               (unsigned int) length + (unsigned int) dst_pos, d->length());\n+    if (s->is_flatArray()) {\n+      FlatArrayKlass::cast(s->klass())->copy_array(s, src_pos, d, dst_pos, length, THREAD);\n+      return;\n@@ -269,1 +295,0 @@\n-    THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), ss.as_string());\n@@ -272,24 +297,2 @@\n-  \/\/ Special case. Boundary cases must be checked first\n-  \/\/ This allows the following call: copy_array(s, s.length(), d.length(), 0).\n-  \/\/ This is correct, since the position is supposed to be an 'in between point', i.e., s.length(),\n-  \/\/ points to the right of the last element.\n-  if (length==0) {\n-    return;\n-  }\n-  if (UseCompressedOops) {\n-    size_t src_offset = (size_t) objArrayOopDesc::obj_at_offset<narrowOop>(src_pos);\n-    size_t dst_offset = (size_t) objArrayOopDesc::obj_at_offset<narrowOop>(dst_pos);\n-    assert(arrayOopDesc::obj_offset_to_raw<narrowOop>(s, src_offset, nullptr) ==\n-           objArrayOop(s)->obj_at_addr<narrowOop>(src_pos), \"sanity\");\n-    assert(arrayOopDesc::obj_offset_to_raw<narrowOop>(d, dst_offset, nullptr) ==\n-           objArrayOop(d)->obj_at_addr<narrowOop>(dst_pos), \"sanity\");\n-    do_copy(s, src_offset, d, dst_offset, length, CHECK);\n-  } else {\n-    size_t src_offset = (size_t) objArrayOopDesc::obj_at_offset<oop>(src_pos);\n-    size_t dst_offset = (size_t) objArrayOopDesc::obj_at_offset<oop>(dst_pos);\n-    assert(arrayOopDesc::obj_offset_to_raw<oop>(s, src_offset, nullptr) ==\n-           objArrayOop(s)->obj_at_addr<oop>(src_pos), \"sanity\");\n-    assert(arrayOopDesc::obj_offset_to_raw<oop>(d, dst_offset, nullptr) ==\n-           objArrayOop(d)->obj_at_addr<oop>(dst_pos), \"sanity\");\n-    do_copy(s, src_offset, d, dst_offset, length, CHECK);\n-  }\n+  assert(s->is_refArray() && d->is_refArray(), \"Must be\");\n+  RefArrayKlass::cast(s->klass())->copy_array(s, src_pos, d, dst_pos, length, THREAD);\n@@ -340,0 +343,3 @@\n+  if (_next_refined_array_klass != nullptr && !CDSConfig::is_dumping_dynamic_archive()) {\n+    it->push(&_next_refined_array_klass);\n+  }\n@@ -342,0 +348,25 @@\n+#if INCLUDE_CDS\n+void ObjArrayKlass::restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, TRAPS) {\n+  ArrayKlass::restore_unshareable_info(loader_data, protection_domain, CHECK);\n+  if (_next_refined_array_klass != nullptr) {\n+    _next_refined_array_klass->restore_unshareable_info(loader_data, protection_domain, CHECK);\n+  }\n+}\n+\n+void ObjArrayKlass::remove_unshareable_info() {\n+  ArrayKlass::remove_unshareable_info();\n+  if (_next_refined_array_klass != nullptr && !CDSConfig::is_dumping_dynamic_archive()) {\n+    _next_refined_array_klass->remove_unshareable_info();\n+  } else {\n+    _next_refined_array_klass = nullptr;\n+  }\n+}\n+\n+void ObjArrayKlass::remove_java_mirror() {\n+  ArrayKlass::remove_java_mirror();\n+  if (_next_refined_array_klass != nullptr && !CDSConfig::is_dumping_dynamic_archive()) {\n+    _next_refined_array_klass->remove_java_mirror();\n+  }\n+}\n+#endif \/\/ INCLUDE_CDS\n+\n@@ -349,0 +380,2 @@\n+  int identity_flag = (Arguments::enable_preview()) ? JVM_ACC_IDENTITY : 0;\n+\n@@ -350,1 +383,1 @@\n-                        | (JVM_ACC_ABSTRACT | JVM_ACC_FINAL);\n+                        | (identity_flag | JVM_ACC_ABSTRACT | JVM_ACC_FINAL);\n@@ -364,0 +397,33 @@\n+ObjArrayKlass* ObjArrayKlass::klass_with_properties(ArrayKlass::ArrayProperties props, TRAPS) {\n+  assert(props != ArrayProperties::INVALID, \"Sanity check\");\n+\n+  if (properties() == props) {\n+    assert(is_refArray_klass() || is_flatArray_klass(), \"Must be a concrete array klass\");\n+    return this;\n+  }\n+\n+  ObjArrayKlass* ak = next_refined_array_klass_acquire();\n+  if (ak == nullptr) {\n+    \/\/ Ensure atomic creation of refined array klasses\n+    RecursiveLocker rl(MultiArray_lock, THREAD);\n+\n+    if (next_refined_array_klass() == nullptr) {\n+      ObjArrayKlass* first = this;\n+      if (!is_refArray_klass() && !is_flatArray_klass() && props != ArrayKlass::ArrayProperties::DEFAULT) {\n+        \/\/ Make sure that the first entry in the linked list is always the default refined klass because\n+        \/\/ C2 relies on this for a fast lookup (see LibraryCallKit::load_default_refined_array_klass).\n+        first = allocate_klass_with_properties(ArrayKlass::ArrayProperties::DEFAULT, THREAD);\n+        release_set_next_refined_klass(first);\n+      }\n+      ak = allocate_klass_with_properties(props, THREAD);\n+      first->release_set_next_refined_klass(ak);\n+    }\n+  }\n+\n+  ak = next_refined_array_klass();\n+  assert(ak != nullptr, \"should be set\");\n+  THREAD->check_possible_safepoint();\n+  return ak->klass_with_properties(props, THREAD); \/\/ why not CHECK_NULL ?\n+}\n+\n+\n@@ -369,1 +435,1 @@\n-  st->print(\" - instance klass: \");\n+  st->print(\" - element klass: \");\n@@ -385,17 +451,1 @@\n-  ArrayKlass::oop_print_on(obj, st);\n-  assert(obj->is_objArray(), \"must be objArray\");\n-  objArrayOop oa = objArrayOop(obj);\n-  int print_len = MIN2(oa->length(), MaxElementPrintSize);\n-  for(int index = 0; index < print_len; index++) {\n-    st->print(\" - %3d : \", index);\n-    if (oa->obj_at(index) != nullptr) {\n-      oa->obj_at(index)->print_value_on(st);\n-      st->cr();\n-    } else {\n-      st->print_cr(\"null\");\n-    }\n-  }\n-  int remaining = oa->length() - print_len;\n-  if (remaining > 0) {\n-    st->print_cr(\" - <%d more elements, increase MaxElementPrintSize to print>\", remaining);\n-  }\n+  ShouldNotReachHere();\n@@ -407,10 +457,1 @@\n-  assert(obj->is_objArray(), \"must be objArray\");\n-  st->print(\"a \");\n-  element_klass()->print_value_on(st);\n-  int len = objArrayOop(obj)->length();\n-  st->print(\"[%d] \", len);\n-  if (obj != nullptr) {\n-    obj->print_address_on(st);\n-  } else {\n-    st->print_cr(\"null\");\n-  }\n+  ShouldNotReachHere();\n@@ -431,1 +472,2 @@\n-  guarantee(bk->is_instance_klass() || bk->is_typeArray_klass(),  \"invalid bottom klass\");\n+  guarantee(bk->is_instance_klass() || bk->is_typeArray_klass() || bk->is_flatArray_klass(),\n+            \"invalid bottom klass\");\n@@ -437,0 +479,1 @@\n+  guarantee(obj->is_null_free_array() || (!is_null_free_array_klass()), \"null-free klass but not object\");\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":186,"deletions":143,"binary":false,"changes":329,"status":"modified"},{"patch":"@@ -0,0 +1,376 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"classfile\/moduleEntry.hpp\"\n+#include \"classfile\/packageEntry.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/vmClasses.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"gc\/shared\/collectedHeap.inline.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/metadataFactory.hpp\"\n+#include \"memory\/metaspaceClosure.hpp\"\n+#include \"memory\/oopFactory.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/arrayKlass.hpp\"\n+#include \"oops\/instanceKlass.hpp\"\n+#include \"oops\/klass.inline.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"oops\/refArrayKlass.inline.hpp\"\n+#include \"oops\/refArrayOop.inline.hpp\"\n+#include \"oops\/symbol.hpp\"\n+#include \"runtime\/handles.inline.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+RefArrayKlass *RefArrayKlass::allocate_klass(ClassLoaderData* loader_data, int n,\n+                                       Klass* k, Symbol *name, ArrayKlass::ArrayProperties props,\n+                                       TRAPS) {\n+  assert(RefArrayKlass::header_size() <= InstanceKlass::header_size(),\n+         \"array klasses must be same size as InstanceKlass\");\n+\n+  int size = ArrayKlass::static_size(RefArrayKlass::header_size());\n+\n+  return new (loader_data, size, THREAD) RefArrayKlass(n, k, name, props);\n+}\n+\n+RefArrayKlass* RefArrayKlass::allocate_refArray_klass(ClassLoaderData* loader_data, int n,\n+                                       Klass* element_klass, ArrayKlass::ArrayProperties props,\n+                                       TRAPS) {\n+  assert(!ArrayKlass::is_null_restricted(props) || (n == 1 && element_klass->is_inline_klass()),\n+         \"null-free unsupported\");\n+\n+  \/\/ Eagerly allocate the direct array supertype.\n+  Klass* super_klass = nullptr;\n+  if (!Universe::is_bootstrapping() || vmClasses::Object_klass_is_loaded()) {\n+    assert(MultiArray_lock->holds_lock(THREAD),\n+           \"must hold lock after bootstrapping\");\n+    Klass* element_super = element_klass->super();\n+    super_klass = element_klass->array_klass(CHECK_NULL);\n+  }\n+\n+  \/\/ Create type name for klass.\n+  Symbol* name = ArrayKlass::create_element_klass_array_name(element_klass, CHECK_NULL);\n+\n+  \/\/ Initialize instance variables\n+  RefArrayKlass* oak = RefArrayKlass::allocate_klass(loader_data, n, element_klass,\n+                                               name, props, CHECK_NULL);\n+\n+  ModuleEntry* module = oak->module();\n+  assert(module != nullptr, \"No module entry for array\");\n+\n+  \/\/ Call complete_create_array_klass after all instance variables has been\n+  \/\/ initialized.\n+  ArrayKlass::complete_create_array_klass(oak, super_klass, module, CHECK_NULL);\n+\n+  \/\/ Add all classes to our internal class loader list here,\n+  \/\/ including classes in the bootstrap (null) class loader.\n+  \/\/ Do this step after creating the mirror so that if the\n+  \/\/ mirror creation fails, loaded_classes_do() doesn't find\n+  \/\/ an array class without a mirror.\n+  loader_data->add_class(oak);\n+\n+  return oak;\n+}\n+\n+RefArrayKlass::RefArrayKlass(int n, Klass* element_klass, Symbol* name,\n+                             ArrayKlass::ArrayProperties props)\n+    : ObjArrayKlass(n, element_klass, name, Kind, props,\n+                    ArrayKlass::is_null_restricted(props) ? markWord::null_free_array_prototype() : markWord::prototype()) {\n+  set_dimension(n);\n+  set_element_klass(element_klass);\n+\n+  Klass* bk;\n+  if (element_klass->is_objArray_klass()) {\n+    bk = ObjArrayKlass::cast(element_klass)->bottom_klass();\n+  } else {\n+    bk = element_klass;\n+  }\n+  assert(bk != nullptr && (bk->is_instance_klass() || bk->is_typeArray_klass()),\n+         \"invalid bottom klass\");\n+  set_bottom_klass(bk);\n+  set_class_loader_data(bk->class_loader_data());\n+\n+  if (element_klass->is_array_klass()) {\n+    set_lower_dimension(ArrayKlass::cast(element_klass));\n+  }\n+\n+  int lh = array_layout_helper(T_OBJECT);\n+  if (ArrayKlass::is_null_restricted(props)) {\n+    assert(n == 1, \"Bytecode does not support null-free multi-dim\");\n+    lh = layout_helper_set_null_free(lh);\n+#ifdef _LP64\n+    assert(prototype_header().is_null_free_array(), \"sanity\");\n+#endif\n+  }\n+  set_layout_helper(lh);\n+  assert(is_array_klass(), \"sanity\");\n+  assert(is_refArray_klass(), \"sanity\");\n+}\n+\n+size_t RefArrayKlass::oop_size(oop obj) const {\n+  \/\/ In this assert, we cannot safely access the Klass* with compact headers,\n+  \/\/ because size_given_klass() calls oop_size() on objects that might be\n+  \/\/ concurrently forwarded, which would overwrite the Klass*.\n+  assert(UseCompactObjectHeaders || obj->is_refArray(), \"must be a reference array\");\n+  return refArrayOop(obj)->object_size();\n+}\n+\n+objArrayOop RefArrayKlass::allocate_instance(int length, ArrayProperties props, TRAPS) {\n+  check_array_allocation_length(\n+      length, arrayOopDesc::max_array_length(T_OBJECT), CHECK_NULL);\n+  size_t size = refArrayOopDesc::object_size(length);\n+  objArrayOop array = (objArrayOop)Universe::heap()->array_allocate(\n+      this, size, length,\n+      \/* do_zero *\/ true, CHECK_NULL);\n+  assert(array->is_refArray(), \"Must be\");\n+  objArrayHandle array_h(THREAD, array);\n+  return array_h();\n+}\n+\n+\n+\/\/ Either oop or narrowOop depending on UseCompressedOops.\n+void RefArrayKlass::do_copy(arrayOop s, size_t src_offset, arrayOop d,\n+                            size_t dst_offset, int length, TRAPS) {\n+  if (s == d) {\n+    \/\/ since source and destination are equal we do not need conversion checks.\n+    assert(length > 0, \"sanity check\");\n+    ArrayAccess<>::oop_arraycopy(s, src_offset, d, dst_offset, length);\n+  } else {\n+    \/\/ We have to make sure all elements conform to the destination array\n+    Klass *bound = RefArrayKlass::cast(d->klass())->element_klass();\n+    Klass *stype = RefArrayKlass::cast(s->klass())->element_klass();\n+    \/\/ Perform null check if dst is null-free but src has no such guarantee\n+    bool null_check = ((!s->klass()->is_null_free_array_klass()) &&\n+                       d->klass()->is_null_free_array_klass());\n+    if (stype == bound || stype->is_subtype_of(bound)) {\n+      if (null_check) {\n+        ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_NOTNULL>::oop_arraycopy(\n+            s, src_offset, d, dst_offset, length);\n+      } else {\n+        ArrayAccess<ARRAYCOPY_DISJOINT>::oop_arraycopy(s, src_offset, d,\n+                                                       dst_offset, length);\n+      }\n+    } else {\n+      if (null_check) {\n+        ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_CHECKCAST |\n+                    ARRAYCOPY_NOTNULL>::oop_arraycopy(s, src_offset, d,\n+                                                      dst_offset, length);\n+      } else {\n+        ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_CHECKCAST>::oop_arraycopy(\n+            s, src_offset, d, dst_offset, length);\n+      }\n+    }\n+  }\n+}\n+\n+void RefArrayKlass::copy_array(arrayOop s, int src_pos, arrayOop d, int dst_pos,\n+                               int length, TRAPS) {\n+  assert(s->is_refArray(), \"must be a reference array\");\n+\n+  if (UseArrayFlattening) {\n+    if (d->is_flatArray()) {\n+      FlatArrayKlass::cast(d->klass())->copy_array(s, src_pos, d, dst_pos, length, THREAD);\n+      return;\n+    }\n+    if (s->is_flatArray()) {\n+      FlatArrayKlass::cast(s->klass())->copy_array(s, src_pos, d, dst_pos, length, THREAD);\n+      return;\n+    }\n+  }\n+\n+  if (!d->is_refArray()) {\n+    ResourceMark rm(THREAD);\n+    stringStream ss;\n+    if (d->is_typeArray()) {\n+      ss.print(\n+          \"arraycopy: type mismatch: can not copy object array[] into %s[]\",\n+          type2name_tab[ArrayKlass::cast(d->klass())->element_type()]);\n+    } else {\n+      ss.print(\"arraycopy: destination type %s is not an array\",\n+               d->klass()->external_name());\n+    }\n+    THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());\n+  }\n+\n+  \/\/ Check is all offsets and lengths are non negative\n+  if (src_pos < 0 || dst_pos < 0 || length < 0) {\n+    \/\/ Pass specific exception reason.\n+    ResourceMark rm(THREAD);\n+    stringStream ss;\n+    if (src_pos < 0) {\n+      ss.print(\"arraycopy: source index %d out of bounds for object array[%d]\",\n+               src_pos, s->length());\n+    } else if (dst_pos < 0) {\n+      ss.print(\n+          \"arraycopy: destination index %d out of bounds for object array[%d]\",\n+          dst_pos, d->length());\n+    } else {\n+      ss.print(\"arraycopy: length %d is negative\", length);\n+    }\n+    THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(),\n+              ss.as_string());\n+  }\n+  \/\/ Check if the ranges are valid\n+  if ((((unsigned int)length + (unsigned int)src_pos) >\n+       (unsigned int)s->length()) ||\n+      (((unsigned int)length + (unsigned int)dst_pos) >\n+       (unsigned int)d->length())) {\n+    \/\/ Pass specific exception reason.\n+    ResourceMark rm(THREAD);\n+    stringStream ss;\n+    if (((unsigned int)length + (unsigned int)src_pos) >\n+        (unsigned int)s->length()) {\n+      ss.print(\n+          \"arraycopy: last source index %u out of bounds for object array[%d]\",\n+          (unsigned int)length + (unsigned int)src_pos, s->length());\n+    } else {\n+      ss.print(\"arraycopy: last destination index %u out of bounds for object \"\n+               \"array[%d]\",\n+               (unsigned int)length + (unsigned int)dst_pos, d->length());\n+    }\n+    THROW_MSG(vmSymbols::java_lang_ArrayIndexOutOfBoundsException(),\n+              ss.as_string());\n+  }\n+\n+  \/\/ Special case. Boundary cases must be checked first\n+  \/\/ This allows the following call: copy_array(s, s.length(), d.length(), 0).\n+  \/\/ This is correct, since the position is supposed to be an 'in between\n+  \/\/ point', i.e., s.length(), points to the right of the last element.\n+  if (length == 0) {\n+    return;\n+  }\n+  if (UseCompressedOops) {\n+    size_t src_offset =\n+        (size_t)refArrayOopDesc::obj_at_offset<narrowOop>(src_pos);\n+    size_t dst_offset =\n+        (size_t)refArrayOopDesc::obj_at_offset<narrowOop>(dst_pos);\n+    assert(arrayOopDesc::obj_offset_to_raw<narrowOop>(s, src_offset, nullptr) ==\n+               refArrayOop(s)->obj_at_addr<narrowOop>(src_pos),\n+           \"sanity\");\n+    assert(arrayOopDesc::obj_offset_to_raw<narrowOop>(d, dst_offset, nullptr) ==\n+               refArrayOop(d)->obj_at_addr<narrowOop>(dst_pos),\n+           \"sanity\");\n+    do_copy(s, src_offset, d, dst_offset, length, CHECK);\n+  } else {\n+    size_t src_offset = (size_t)refArrayOopDesc::obj_at_offset<oop>(src_pos);\n+    size_t dst_offset = (size_t)refArrayOopDesc::obj_at_offset<oop>(dst_pos);\n+    assert(arrayOopDesc::obj_offset_to_raw<oop>(s, src_offset, nullptr) ==\n+               refArrayOop(s)->obj_at_addr<oop>(src_pos),\n+           \"sanity\");\n+    assert(arrayOopDesc::obj_offset_to_raw<oop>(d, dst_offset, nullptr) ==\n+               refArrayOop(d)->obj_at_addr<oop>(dst_pos),\n+           \"sanity\");\n+    do_copy(s, src_offset, d, dst_offset, length, CHECK);\n+  }\n+}\n+\n+void RefArrayKlass::initialize(TRAPS) {\n+  bottom_klass()->initialize(THREAD); \/\/ dispatches to either InstanceKlass or TypeArrayKlass\n+}\n+\n+void RefArrayKlass::metaspace_pointers_do(MetaspaceClosure *it) {\n+  ObjArrayKlass::metaspace_pointers_do(it);\n+}\n+\n+\/\/ Printing\n+\n+void RefArrayKlass::print_on(outputStream* st) const {\n+#ifndef PRODUCT\n+  Klass::print_on(st);\n+  st->print(\" - element klass: \");\n+  element_klass()->print_value_on(st);\n+  st->cr();\n+#endif \/\/ PRODUCT\n+}\n+\n+void RefArrayKlass::print_value_on(outputStream* st) const {\n+  assert(is_klass(), \"must be klass\");\n+\n+  element_klass()->print_value_on(st);\n+  st->print(\"[]\");\n+}\n+\n+#ifndef PRODUCT\n+\n+void RefArrayKlass::oop_print_on(oop obj, outputStream* st) {\n+  ArrayKlass::oop_print_on(obj, st);\n+  assert(obj->is_refArray(), \"must be refArray\");\n+  refArrayOop oa = refArrayOop(obj);\n+  int print_len = MIN2(oa->length(), MaxElementPrintSize);\n+  for (int index = 0; index < print_len; index++) {\n+    st->print(\" - %3d : \", index);\n+    if (oa->obj_at(index) != nullptr) {\n+      oa->obj_at(index)->print_value_on(st);\n+      st->cr();\n+    } else {\n+      st->print_cr(\"null\");\n+    }\n+  }\n+  int remaining = oa->length() - print_len;\n+  if (remaining > 0) {\n+    st->print_cr(\" - <%d more elements, increase MaxElementPrintSize to print>\",\n+                 remaining);\n+  }\n+}\n+\n+#endif \/\/ PRODUCT\n+\n+void RefArrayKlass::oop_print_value_on(oop obj, outputStream* st) {\n+  assert(obj->is_refArray(), \"must be refArray\");\n+  st->print(\"a \");\n+  element_klass()->print_value_on(st);\n+  int len = refArrayOop(obj)->length();\n+  st->print(\"[%d] \", len);\n+  if (obj != nullptr) {\n+    obj->print_address_on(st);\n+  } else {\n+    st->print_cr(\"null\");\n+  }\n+}\n+\n+\/\/ Verification\n+\n+void RefArrayKlass::verify_on(outputStream* st) {\n+  ArrayKlass::verify_on(st);\n+  guarantee(element_klass()->is_klass(), \"should be klass\");\n+  guarantee(bottom_klass()->is_klass(), \"should be klass\");\n+  Klass *bk = bottom_klass();\n+  guarantee(bk->is_instance_klass() || bk->is_typeArray_klass() ||\n+                bk->is_flatArray_klass(),\n+            \"invalid bottom klass\");\n+}\n+\n+void RefArrayKlass::oop_verify_on(oop obj, outputStream* st) {\n+  ArrayKlass::oop_verify_on(obj, st);\n+  guarantee(obj->is_refArray(), \"must be refArray\");\n+  guarantee(obj->is_null_free_array() || (!is_null_free_array_klass()),\n+            \"null-free klass but not object\");\n+  refArrayOop oa = refArrayOop(obj);\n+  for (int index = 0; index < oa->length(); index++) {\n+    guarantee(oopDesc::is_oop_or_null(oa->obj_at(index)), \"should be oop\");\n+  }\n+}\n","filename":"src\/hotspot\/share\/oops\/refArrayKlass.cpp","additions":376,"deletions":0,"binary":false,"changes":376,"status":"added"},{"patch":"@@ -1604,1 +1604,1 @@\n-      tf()->range(),\n+      tf()->range_cc(),\n@@ -1667,1 +1667,1 @@\n-Node *DivModINode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *DivModINode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -1682,1 +1682,1 @@\n-Node *DivModLNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *DivModLNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -1720,1 +1720,1 @@\n-Node* UDivModINode::match( const ProjNode *proj, const Matcher *match ) {\n+Node* UDivModINode::match(const ProjNode* proj, const Matcher* match, const RegMask* mask) {\n@@ -1735,1 +1735,1 @@\n-Node* UDivModLNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node* UDivModLNode::match( const ProjNode* proj, const Matcher* match, const RegMask* mask) {\n","filename":"src\/hotspot\/share\/opto\/divnode.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -618,2 +618,1 @@\n-    \/\/ At this stage JVM_CONSTANT_UnresolvedClassInError should not be\n-    \/\/ here\n+    \/\/ At this stage JVM_CONSTANT_UnresolvedClassInError should not be here\n@@ -1938,0 +1937,6 @@\n+  \/\/ rewrite constant pool references in the LoadableDescriptors attribute:\n+  if (!rewrite_cp_refs_in_loadable_descriptors_attribute(scratch_class)) {\n+    \/\/ propagate failure back to caller\n+    return false;\n+  }\n+\n@@ -2086,0 +2091,13 @@\n+\/\/ Rewrite constant pool references in the LoadableDescriptors attribute.\n+bool VM_RedefineClasses::rewrite_cp_refs_in_loadable_descriptors_attribute(\n+       InstanceKlass* scratch_class) {\n+\n+  Array<u2>* loadable_descriptors = scratch_class->loadable_descriptors();\n+  assert(loadable_descriptors != nullptr, \"unexpected null loadable_descriptors\");\n+  for (int i = 0; i < loadable_descriptors->length(); i++) {\n+    u2 cp_index = loadable_descriptors->at(i);\n+    loadable_descriptors->at_put(i, find_new_index(cp_index));\n+  }\n+  return true;\n+}\n+\n@@ -3273,0 +3291,8 @@\n+   if (frame_type == 246) {  \/\/ EARLY_LARVAL\n+     \/\/ rewrite_cp_refs in  unset fields and fall through.\n+     rewrite_cp_refs_in_early_larval_stackmaps(stackmap_p, stackmap_end, calc_number_of_entries, frame_type);\n+     \/\/ The larval frames point to the next frame, so advance to the next frame and fall through.\n+     frame_type = *stackmap_p;\n+     stackmap_p++;\n+   }\n+\n@@ -3482,0 +3508,23 @@\n+void VM_RedefineClasses::rewrite_cp_refs_in_early_larval_stackmaps(\n+       address& stackmap_p_ref, address stackmap_end, u2 frame_i,\n+       u1 frame_type) {\n+\n+    u2 num_early_larval_stackmaps = Bytes::get_Java_u2(stackmap_p_ref);\n+    stackmap_p_ref += 2;\n+\n+    for (u2 i = 0; i < num_early_larval_stackmaps; i++) {\n+\n+      u2 name_and_ref_index = Bytes::get_Java_u2(stackmap_p_ref);\n+      u2 new_cp_index = find_new_index(name_and_ref_index);\n+      if (new_cp_index != 0) {\n+        log_debug(redefine, class, stackmap)(\"mapped old name_and_ref_index=%d\", name_and_ref_index);\n+        Bytes::put_Java_u2(stackmap_p_ref, new_cp_index);\n+        name_and_ref_index = new_cp_index;\n+      }\n+      log_debug(redefine, class, stackmap)\n+        (\"frame_i=%u, frame_type=%u, name_and_ref_index=%d\", frame_i, frame_type, name_and_ref_index);\n+\n+      stackmap_p_ref += 2;\n+    }\n+} \/\/ rewrite_cp_refs_in_early_larval_stackmaps\n+\n","filename":"src\/hotspot\/share\/prims\/jvmtiRedefineClasses.cpp","additions":51,"deletions":2,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -62,0 +63,1 @@\n+#include \"oops\/access.hpp\"\n@@ -64,0 +66,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -89,0 +92,1 @@\n+#include \"runtime\/keepStackGCProcessed.hpp\"\n@@ -1949,0 +1953,103 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return nullptr;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(ih->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return nullptr;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayHandle result_array =\n+      oopFactory::new_objArray_handle(vmClasses::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ih->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+WB_END\n+\n+\/\/ Collect Object oops but not value objects...loaded from heap\n+class CollectObjectOops : public BasicOopIterateClosure {\n+  public:\n+  GrowableArray<Handle>* _array;\n+\n+  CollectObjectOops() {\n+      _array = new GrowableArray<Handle>(128);\n+  }\n+\n+  void add_oop(oop o) {\n+    Handle oh = Handle(Thread::current(), o);\n+    if (oh != nullptr && oh->is_inline_type()) {\n+      oh->oop_iterate(this);\n+    } else {\n+      _array->append(oh);\n+    }\n+  }\n+\n+  template <class T> inline void add_oop(T* p) { add_oop(HeapAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    objArrayHandle result_array =\n+        oopFactory::new_objArray_handle(vmClasses::Object_klass(), _array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < _array->length(); i++) {\n+      result_array->obj_at_put(i, _array->at(i)());\n+    }\n+    return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+  }\n+};\n+\n+\/\/ Collect Object oops but not value objects...loaded from frames\n+class CollectFrameObjectOops : public BasicOopIterateClosure {\n+ public:\n+  CollectObjectOops _collect;\n+\n+  template <class T> inline void add_oop(T* p) { _collect.add_oop(RawAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    return _collect.create_jni_result(env, THREAD);\n+  }\n+};\n+\n+\/\/ Collect Object oops for the given oop, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(thread);\n+  Handle objh(thread, JNIHandles::resolve(thing));\n+  CollectObjectOops collectOops;\n+  objh->oop_iterate(&collectOops);\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\/\/ Collect Object oops for the given frame deep, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  KeepStackGCProcessedMark ksgcpm(THREAD);\n+  ResourceMark rm(THREAD);\n+  CollectFrameObjectOops collectOops;\n+  StackFrameStream sfs(thread, true \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, nullptr, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n@@ -2950,0 +3057,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":113,"deletions":0,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -78,0 +78,1 @@\n+#include <string.h>\n@@ -365,0 +366,12 @@\n+bool Arguments::patching_migrated_classes(const char* property, const char* value) {\n+  if (strncmp(property, MODULE_PROPERTY_PREFIX, MODULE_PROPERTY_PREFIX_LEN) == 0) {\n+    const char* property_suffix = property + MODULE_PROPERTY_PREFIX_LEN;\n+    if (matches_property_suffix(property_suffix, PATCH, PATCH_LEN)) {\n+      if (strcmp(value, \"java.base-valueclasses.jar\")) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -1806,1 +1819,0 @@\n-static unsigned int patch_mod_count = 0;\n@@ -1813,1 +1825,1 @@\n-  if (!CDSConfig::check_vm_args_consistency(patch_mod_javabase, mode_flag_cmd_line)) {\n+  if (!CDSConfig::check_vm_args_consistency(mode_flag_cmd_line)) {\n@@ -2062,1 +2074,1 @@\n-      add_patch_mod_prefix(module_name, module_equal + 1);\n+      add_patch_mod_prefix(module_name, module_equal + 1, false \/* no append *\/, false \/* no cds *\/);\n@@ -2064,3 +2076,0 @@\n-      if (!create_numbered_module_property(\"jdk.module.patch\", patch_mod_tail, patch_mod_count++)) {\n-        return JNI_ENOMEM;\n-      }\n@@ -2074,0 +2083,64 @@\n+\/\/ VALUECLASS_STR must match string used in the build\n+#define VALUECLASS_STR \"valueclasses\"\n+#define VALUECLASS_JAR \"-\" VALUECLASS_STR \".jar\"\n+\n+\/\/ Finalize --patch-module args and --enable-preview related to value class module patches.\n+\/\/ Create all numbered properties passing module patches.\n+int Arguments::finalize_patch_module() {\n+  \/\/ If --enable-preview and EnableValhalla is true, each module may have value classes that\n+  \/\/ are to be patched into the module.\n+  \/\/ For each <module>-valueclasses.jar in <JAVA_HOME>\/lib\/valueclasses\/\n+  \/\/ appends the equivalent of --patch-module <module>=<JAVA_HOME>\/lib\/valueclasses\/<module>-valueclasses.jar\n+  if (enable_preview() && EnableValhalla) {\n+    char * valueclasses_dir = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+    const char * fileSep = os::file_separator();\n+\n+    jio_snprintf(valueclasses_dir, JVM_MAXPATHLEN, \"%s%slib%s\" VALUECLASS_STR \"%s\",\n+                 Arguments::get_java_home(), fileSep, fileSep, fileSep);\n+    DIR* dir = os::opendir(valueclasses_dir);\n+    if (dir != nullptr) {\n+      char * module_name = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+      char * path = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+\n+      for (dirent * entry = os::readdir(dir); entry != nullptr; entry = os::readdir(dir)) {\n+        \/\/ Test if file ends-with \"-valueclasses.jar\"\n+        int len = (int)strlen(entry->d_name) - (sizeof(VALUECLASS_JAR) - 1);\n+        if (len <= 0 || strcmp(&entry->d_name[len], VALUECLASS_JAR) != 0) {\n+          continue;         \/\/ too short or not the expected suffix\n+        }\n+\n+        strcpy(module_name, entry->d_name);\n+        module_name[len] = '\\0';     \/\/ truncate to just module-name\n+\n+        jio_snprintf(path, JVM_MAXPATHLEN, \"%s%s\", valueclasses_dir, &entry->d_name);\n+        add_patch_mod_prefix(module_name, path, true \/* append *\/, true \/* cds OK*\/);\n+        log_info(class)(\"--enable-preview appending value classes for module %s: %s\", module_name, entry->d_name);\n+      }\n+      FreeHeap(module_name);\n+      FreeHeap(path);\n+      os::closedir(dir);\n+    }\n+    FreeHeap(valueclasses_dir);\n+  }\n+\n+  \/\/ Create numbered properties for each module that has been patched either\n+  \/\/ by --patch-module or --enable-preview\n+  \/\/ Format is \"jdk.module.patch.<n>=<module_name>=<path>\"\n+  if (_patch_mod_prefix != nullptr) {\n+    char * prop_value = AllocateHeap(JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, mtArguments);\n+    unsigned int patch_mod_count = 0;\n+\n+    for (GrowableArrayIterator<ModulePatchPath *> it = _patch_mod_prefix->begin();\n+            it != _patch_mod_prefix->end(); ++it) {\n+      jio_snprintf(prop_value, JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, \"%s=%s\",\n+                   (*it)->module_name(), (*it)->path_string());\n+      if (!create_numbered_module_property(\"jdk.module.patch\", prop_value, patch_mod_count++)) {\n+        FreeHeap(prop_value);\n+        return JNI_ENOMEM;\n+      }\n+    }\n+    FreeHeap(prop_value);\n+  }\n+  return JNI_OK;\n+}\n+\n@@ -2342,0 +2415,4 @@\n+      \/\/ --enable-preview enables Valhalla, EnableValhalla VM option will eventually be removed before integration\n+      if (FLAG_SET_CMDLINE(EnableValhalla, true) != JVMFlag::SUCCESS) {\n+        return JNI_EINVAL;\n+      }\n@@ -2845,10 +2922,5 @@\n-void Arguments::add_patch_mod_prefix(const char* module_name, const char* path) {\n-  \/\/ For java.base check for duplicate --patch-module options being specified on the command line.\n-  \/\/ This check is only required for java.base, all other duplicate module specifications\n-  \/\/ will be checked during module system initialization.  The module system initialization\n-  \/\/ will throw an ExceptionInInitializerError if this situation occurs.\n-  if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n-    if (patch_mod_javabase) {\n-      vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n-    } else {\n-      patch_mod_javabase = true;\n+void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool allow_append, bool allow_cds) {\n+  if (!allow_cds) {\n+    CDSConfig::set_module_patching_disables_cds();\n+    if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+      CDSConfig::set_java_base_module_patching_disables_cds();\n@@ -2863,1 +2935,18 @@\n-  _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  \/\/ Scan patches for matching module\n+  int i = _patch_mod_prefix->find_if([&](ModulePatchPath* patch) {\n+    return (strcmp(module_name, patch->module_name()) == 0);\n+  });\n+  if (i == -1) {\n+    _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  } else {\n+    if (allow_append) {\n+      \/\/ append path to existing module entry\n+      _patch_mod_prefix->at(i)->append_path(path);\n+    } else {\n+      if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+        vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n+      } else {\n+        vm_exit_during_initialization(\"Cannot specify a module more than once to --patch-module\", module_name);\n+      }\n+    }\n+  }\n@@ -2976,1 +3065,2 @@\n-  if (!check_vm_args_consistency()) {\n+  \/\/ finalize --module-patch and related --enable-preview\n+  if (finalize_patch_module() != JNI_OK) {\n@@ -2980,0 +3070,3 @@\n+  if (!check_vm_args_consistency()) {\n+    return JNI_ERR;\n+  }\n@@ -3865,0 +3958,12 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !CDSConfig::is_dumping_archive() && !UseSharedSpaces)) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported.\n+    \/\/ Also these aren't useful in -Xint. However, don't disable them when dumping or using\n+    \/\/ the CDS archive, as the values must match between dumptime and runtime.\n+    FLAG_SET_DEFAULT(InlineTypePassFieldsAsArgs, false);\n+    FLAG_SET_DEFAULT(InlineTypeReturnedAsFields, false);\n+  }\n+  if (!UseNonAtomicValueFlattening && !UseNullableValueFlattening && !UseAtomicValueFlattening) {\n+    \/\/ Flattening is disabled\n+    FLAG_SET_DEFAULT(UseArrayFlattening, false);\n+    FLAG_SET_DEFAULT(UseFieldFlattening, false);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":123,"deletions":18,"binary":false,"changes":141,"status":"modified"},{"patch":"@@ -460,1 +460,1 @@\n-  template<typename FKind> frame new_heap_frame(frame& f, frame& caller);\n+  template<typename FKind> frame new_heap_frame(frame& f, frame& caller, int size_adjust = 0);\n@@ -462,1 +462,1 @@\n-  inline void patch_pd(frame& callee, const frame& caller);\n+  inline void patch_pd(frame& callee, const frame& caller, bool is_bottom_frame);\n@@ -1158,1 +1158,1 @@\n-  patch_pd(hf, caller);\n+  patch_pd(hf, caller, is_bottom_frame);\n@@ -1255,2 +1255,24 @@\n-  const int argsize = ContinuationHelper::CompiledFrame::stack_argsize(f) + frame::metadata_words_at_top;\n-  const int fsize = pointer_delta_as_int(stack_frame_bottom + argsize, stack_frame_top);\n+  int argsize = ContinuationHelper::CompiledFrame::stack_argsize(f) + frame::metadata_words_at_top;\n+  int fsize = pointer_delta_as_int(stack_frame_bottom + argsize, stack_frame_top);\n+\n+  int real_frame_size = 0;\n+  bool augmented = f.was_augmented_on_entry(real_frame_size);\n+  if (augmented) {\n+    \/\/ The args reside inside the frame so clear argsize. If the caller is compiled,\n+    \/\/ this will cause the stack arguments passed by the caller to be freezed when\n+    \/\/ freezing the caller frame itself. If the caller is interpreted this will have\n+    \/\/ the effect of discarding the arg area created in the i2c stub.\n+    argsize = 0;\n+    fsize = real_frame_size - (callee_interpreted ? 0 : callee_argsize);\n+#ifdef ASSERT\n+    nmethod* nm = f.cb()->as_nmethod();\n+    Method* method = nm->method();\n+    address return_pc = ContinuationHelper::CompiledFrame::return_pc(f);\n+    CodeBlob* caller_cb = CodeCache::find_blob_fast(return_pc);\n+    assert(nm->is_compiled_by_c2() || (caller_cb->is_nmethod() && caller_cb->as_nmethod()->is_compiled_by_c2()), \"caller or callee should be c2 compiled\");\n+    assert((!caller_cb->is_nmethod() && nm->is_compiled_by_c2()) ||\n+           (nm->compiler_type() != caller_cb->as_nmethod()->compiler_type()) ||\n+           (nm->is_compiled_by_c2() && !method->is_static() && method->method_holder()->is_inline_klass()),\n+           \"frame should not be extended\");\n+#endif\n+  }\n@@ -1258,1 +1280,1 @@\n-  log_develop_trace(continuations)(\"recurse_freeze_compiled_frame %s _size: %d fsize: %d argsize: %d\",\n+  log_develop_trace(continuations)(\"recurse_freeze_compiled_frame %s _size: %d fsize: %d argsize: %d augmented: %d\",\n@@ -1261,1 +1283,1 @@\n-                             _freeze_size, fsize, argsize);\n+                             _freeze_size, fsize, argsize, augmented);\n@@ -1272,0 +1294,1 @@\n+  assert(!is_bottom_frame || !augmented, \"thaw extended frame without caller?\");\n@@ -1275,1 +1298,1 @@\n-  frame hf = new_heap_frame<ContinuationHelper::CompiledFrame>(f, caller);\n+  frame hf = new_heap_frame<ContinuationHelper::CompiledFrame>(f, caller, augmented ? real_frame_size - f.cb()->as_nmethod()->frame_size() : 0);\n@@ -1936,0 +1959,1 @@\n+  int remove_scalarized_frames(StackChunkFrameStream<ChunkFrames::CompiledOnly>& scfs, int &argsize);\n@@ -1959,1 +1983,1 @@\n-  inline void patch(frame& f, const frame& caller, bool bottom);\n+  inline void patch(frame& f, const frame& caller, bool bottom, bool augmented = false);\n@@ -1969,1 +1993,1 @@\n-  template<typename FKind> frame new_stack_frame(const frame& hf, frame& caller, bool bottom);\n+  template<typename FKind> frame new_stack_frame(const frame& hf, frame& caller, bool bottom, int size_adjust = 0);\n@@ -2046,0 +2070,14 @@\n+int ThawBase::remove_scalarized_frames(StackChunkFrameStream<ChunkFrames::CompiledOnly>& f, int &argsize) {\n+  intptr_t* top = f.sp();\n+\n+  while (f.cb()->as_nmethod()->needs_stack_repair()) {\n+    f.next(SmallRegisterMap::instance(), false \/* stop *\/);\n+  }\n+  assert(!f.is_done(), \"\");\n+  assert(f.is_compiled(), \"\");\n+\n+  intptr_t* bottom = f.sp() + f.cb()->frame_size();\n+  argsize = f.stack_argsize();\n+  return bottom - top;\n+}\n+\n@@ -2067,3 +2105,0 @@\n-    frame_size += f.cb()->frame_size();\n-    argsize = f.stack_argsize();\n-\n@@ -2076,0 +2111,9 @@\n+\n+    if (f.cb()->as_nmethod()->needs_stack_repair()) {\n+      frame_size += remove_scalarized_frames(f, argsize);\n+    } else {\n+      frame_size += f.cb()->frame_size();\n+      argsize = f.stack_argsize();\n+    }\n+  } else if (f.cb()->as_nmethod()->needs_stack_repair()) {\n+    frame_size = remove_scalarized_frames(f, argsize);\n@@ -2355,0 +2399,1 @@\n+  CodeBlob* cb = _stream.cb();\n@@ -2359,3 +2404,8 @@\n-  \/\/ we never leave a compiled caller of an interpreted frame as the top frame in the chunk\n-  \/\/ as it makes detecting that situation and adjusting unextended_sp tricky\n-  if (num_frames == 1 && !_stream.is_done() && FKind::interpreted && _stream.is_compiled()) {\n+  \/\/ We never leave a compiled caller of an interpreted frame as the top frame in the chunk\n+  \/\/ as it makes detecting that situation and adjusting unextended_sp tricky. We also always\n+  \/\/ thaw the caller of a frame that needs_stack_repair, as it would otherwise complicate things:\n+  \/\/ - Regardless of whether the frame was extended or not, we would need to copy the right arg\n+  \/\/   size if its greater than the one given by the normal method signature (non-scalarized).\n+  \/\/ - If the frame was indeed extended, leaving its caller as the top frame would complicate walking\n+  \/\/   the chunk (we need unextended_sp, but we only have sp).\n+  if (num_frames == 1 && !_stream.is_done() && ((FKind::interpreted && _stream.is_compiled()) || (FKind::compiled && cb->as_nmethod_or_null()->needs_stack_repair()))) {\n@@ -2421,1 +2471,1 @@\n-inline void ThawBase::patch(frame& f, const frame& caller, bool bottom) {\n+inline void ThawBase::patch(frame& f, const frame& caller, bool bottom, bool augmented) {\n@@ -2426,1 +2476,1 @@\n-  } else {\n+  } else if (caller.is_compiled_frame()){\n@@ -2429,1 +2479,1 @@\n-    ContinuationHelper::Frame::patch_pc(caller, caller.raw_pc());\n+    ContinuationHelper::Frame::patch_pc(caller, caller.raw_pc(), augmented \/*callee_augmented*\/);\n@@ -2592,0 +2642,10 @@\n+  int fsize = 0;\n+  int added_argsize = 0;\n+  bool augmented = hf.was_augmented_on_entry(fsize);\n+  if (!augmented) {\n+    added_argsize = (is_bottom_frame || caller.is_interpreted_frame()) ? hf.compiled_frame_stack_argsize() : 0;\n+    fsize += added_argsize;\n+  }\n+  assert(!is_bottom_frame || !augmented, \"\");\n+\n+\n@@ -2595,1 +2655,3 @@\n-  frame f = new_stack_frame<ContinuationHelper::CompiledFrame>(hf, caller, is_bottom_frame);\n+  frame f = new_stack_frame<ContinuationHelper::CompiledFrame>(hf, caller, is_bottom_frame, augmented ? fsize - hf.cb()->frame_size() : 0);\n+  assert(f.cb()->frame_size() == (int)(caller.sp() - f.sp()), \"\");\n+\n@@ -2598,5 +2660,0 @@\n-\n-  const int added_argsize = (is_bottom_frame || caller.is_interpreted_frame()) ? hf.compiled_frame_stack_argsize() : 0;\n-  int fsize = ContinuationHelper::CompiledFrame::size(hf) + added_argsize;\n-  assert(fsize <= (int)(caller.unextended_sp() - f.unextended_sp()), \"\");\n-\n@@ -2615,1 +2672,1 @@\n-  patch(f, caller, is_bottom_frame);\n+  patch(f, caller, is_bottom_frame, augmented);\n","filename":"src\/hotspot\/share\/runtime\/continuationFreezeThaw.cpp","additions":83,"deletions":26,"binary":false,"changes":109,"status":"modified"},{"patch":"@@ -53,0 +53,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -301,0 +304,18 @@\n+\n+static Klass* get_refined_array_klass(Klass* k, frame* fr, RegisterMap* map, ObjectValue* sv, TRAPS) {\n+  \/\/ If it's an array, get the properties\n+  if (k->is_array_klass() && !k->is_typeArray_klass()) {\n+    assert(!k->is_refArray_klass() && !k->is_flatArray_klass(), \"Unexpected refined klass\");\n+    nmethod* nm = fr->cb()->as_nmethod_or_null();\n+    if (nm->is_compiled_by_c2()) {\n+      assert(sv->has_properties(), \"Property information is missing\");\n+      ArrayKlass::ArrayProperties props = static_cast<ArrayKlass::ArrayProperties>(StackValue::create_stack_value(fr, map, sv->properties())->get_jint());\n+      k = ObjArrayKlass::cast(k)->klass_with_properties(props, THREAD);\n+    } else {\n+      \/\/ TODO Graal needs to be fixed. Just go with the default properties for now\n+      k = ObjArrayKlass::cast(k)->klass_with_properties(ArrayKlass::ArrayProperties::DEFAULT, THREAD);\n+    }\n+  }\n+  return k;\n+}\n+\n@@ -302,2 +323,2 @@\n-static void print_objects(JavaThread* deoptee_thread,\n-                          GrowableArray<ScopeValue*>* objects, bool realloc_failures) {\n+static void print_objects(JavaThread* deoptee_thread, frame* deoptee, RegisterMap* map,\n+                          GrowableArray<ScopeValue*>* objects, bool realloc_failures, TRAPS) {\n@@ -319,0 +340,1 @@\n+    k = get_refined_array_klass(k, deoptee, map, sv, THREAD);\n@@ -352,2 +374,13 @@\n-  bool save_oop_result = chunk->at(0)->scope()->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n-  Handle return_value;\n+  ScopeDesc* scope = chunk->at(0)->scope();\n+  bool save_oop_result = scope->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n+  \/\/ In case of the return of multiple values, we must take care\n+  \/\/ of all oop return values.\n+  GrowableArray<Handle> return_oops;\n+  InlineKlass* vk = nullptr;\n+  if (save_oop_result && scope->return_scalarized()) {\n+    vk = InlineKlass::returned_inline_klass(map);\n+    if (vk != nullptr) {\n+      vk->save_oop_fields(map, return_oops);\n+      save_oop_result = false;\n+    }\n+  }\n@@ -359,1 +392,1 @@\n-    return_value = Handle(thread, result);\n+    return_oops.push(Handle(thread, result));\n@@ -366,1 +399,1 @@\n-  if (objects != nullptr) {\n+  if (objects != nullptr || vk != nullptr) {\n@@ -371,1 +404,9 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+      if (vk != nullptr) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, CHECK_AND_CLEAR_(true));\n+      }\n+      if (objects != nullptr) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+        guarantee(compiled_method != nullptr, \"deopt must be associated with an nmethod\");\n+        bool is_jvmci = compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, is_jvmci, CHECK_AND_CLEAR_(true));\n+      }\n@@ -376,1 +417,9 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+      if (vk != nullptr) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, THREAD);\n+      }\n+      if (objects != nullptr) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+        guarantee(compiled_method != nullptr, \"deopt must be associated with an nmethod\");\n+        bool is_jvmci = compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, is_jvmci, THREAD);\n+      }\n@@ -379,5 +428,2 @@\n-    guarantee(compiled_method != nullptr, \"deopt must be associated with an nmethod\");\n-    bool is_jvmci = compiled_method->is_compiled_by_jvmci();\n-    Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, is_jvmci);\n-    if (TraceDeoptimization) {\n-      print_objects(deoptee_thread, objects, realloc_failures);\n+    if (TraceDeoptimization && objects != nullptr) {\n+      print_objects(deoptee_thread, &deoptee, &map, objects, realloc_failures, thread);\n@@ -386,1 +432,1 @@\n-  if (save_oop_result) {\n+  if (save_oop_result || vk != nullptr) {\n@@ -388,1 +434,2 @@\n-    deoptee.set_saved_oop_result(&map, return_value());\n+    assert(return_oops.length() == 1, \"no inline type\");\n+    deoptee.set_saved_oop_result(&map, return_oops.pop()());\n@@ -724,1 +771,1 @@\n-  \/\/ If the sender is deoptimized the we must retrieve the address of the handler\n+  \/\/ If the sender is deoptimized we must retrieve the address of the handler\n@@ -1241,2 +1288,10 @@\n-\n-    oop obj = nullptr;\n+    k = get_refined_array_klass(k, fr, reg_map, sv, THREAD);\n+\n+    \/\/ Check if the object may be null and has an additional null_marker input that needs\n+    \/\/ to be checked before using the field values. Skip re-allocation if it is null.\n+    if (k->is_inline_klass() && sv->has_properties()) {\n+      jint null_marker = StackValue::create_stack_value(fr, reg_map, sv->properties())->get_jint();\n+      if (null_marker == 0) {\n+        continue;\n+      }\n+    }\n@@ -1245,0 +1300,1 @@\n+    oop obj = nullptr;\n@@ -1272,0 +1328,4 @@\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* ak = FlatArrayKlass::cast(k);\n+      \/\/ Inline type array must be zeroed because not all memory is reassigned\n+      obj = ak->allocate_instance(sv->field_size(), ak->properties(), THREAD);\n@@ -1278,2 +1338,2 @@\n-    } else if (k->is_objArray_klass()) {\n-      ObjArrayKlass* ak = ObjArrayKlass::cast(k);\n+    } else if (k->is_refArray_klass()) {\n+      RefArrayKlass* ak = RefArrayKlass::cast(k);\n@@ -1281,1 +1341,1 @@\n-      obj = ak->allocate_instance(sv->field_size(), THREAD);\n+      obj = ak->allocate_instance(sv->field_size(), ak->properties(), THREAD);\n@@ -1303,0 +1363,15 @@\n+\/\/ We're deoptimizing at the return of a call, inline type fields are\n+\/\/ in registers. When we go back to the interpreter, it will expect a\n+\/\/ reference to an inline type instance. Allocate and initialize it from\n+\/\/ the register values here.\n+bool Deoptimization::realloc_inline_type_result(InlineKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS) {\n+  oop new_vt = vk->realloc_result(map, return_oops, THREAD);\n+  if (new_vt == nullptr) {\n+    CLEAR_PENDING_EXCEPTION;\n+    THROW_OOP_(Universe::out_of_memory_error_realloc_objects(), true);\n+  }\n+  return_oops.clear();\n+  return_oops.push(Handle(THREAD, new_vt));\n+  return false;\n+}\n+\n@@ -1468,0 +1543,3 @@\n+  InstanceKlass* _klass;\n+  bool _is_flat;\n+  bool _is_null_free;\n@@ -1469,4 +1547,1 @@\n-  ReassignedField() {\n-    _offset = 0;\n-    _type = T_ILLEGAL;\n-  }\n+  ReassignedField() : _offset(0), _type(T_ILLEGAL), _klass(nullptr), _is_flat(false), _is_null_free(false) { }\n@@ -1486,0 +1561,6 @@\n+      if (fs.is_flat()) {\n+        field._is_flat = true;\n+        field._is_null_free = fs.is_null_free_inline_type();\n+        \/\/ Resolve klass of flat inline type field\n+        field._klass = InlineKlass::cast(klass->get_inline_type_field_klass(fs.index()));\n+      }\n@@ -1492,2 +1573,3 @@\n-\/\/ Restore fields of an eliminated instance object employing the same field order used by the compiler.\n-static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool is_jvmci) {\n+\/\/ Restore fields of an eliminated instance object employing the same field order used by the\n+\/\/ compiler when it scalarizes an object at safepoints.\n+static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool is_jvmci, int base_offset, TRAPS) {\n@@ -1496,0 +1578,19 @@\n+    BasicType type = fields->at(i)._type;\n+    int offset = base_offset + fields->at(i)._offset;\n+    \/\/ Check for flat inline type field before accessing the ScopeValue because it might not have any fields\n+    if (fields->at(i)._is_flat) {\n+      \/\/ Recursively re-assign flat inline type fields\n+      InstanceKlass* vk = fields->at(i)._klass;\n+      assert(vk != nullptr, \"must be resolved\");\n+      offset -= InlineKlass::cast(vk)->payload_offset(); \/\/ Adjust offset to omit oop header\n+      svIndex = reassign_fields_by_klass(vk, fr, reg_map, sv, svIndex, obj, is_jvmci, offset, CHECK_0);\n+      if (!fields->at(i)._is_null_free) {\n+        ScopeValue* scope_field = sv->field_at(svIndex);\n+        StackValue* value = StackValue::create_stack_value(fr, reg_map, scope_field);\n+        int nm_offset = offset + InlineKlass::cast(vk)->null_marker_offset();\n+        obj->bool_field_put(nm_offset, value->get_jint() & 1);\n+        svIndex++;\n+      }\n+      continue; \/\/ Continue because we don't need to increment svIndex\n+    }\n+\n@@ -1498,3 +1599,2 @@\n-    int offset = fields->at(i)._offset;\n-    BasicType type = fields->at(i)._type;\n-      case T_OBJECT: case T_ARRAY:\n+      case T_OBJECT:\n+      case T_ARRAY:\n@@ -1572,0 +1672,1 @@\n+\n@@ -1575,0 +1676,23 @@\n+\/\/ restore fields of an eliminated inline type array\n+void Deoptimization::reassign_flat_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, flatArrayOop obj, FlatArrayKlass* vak, bool is_jvmci, TRAPS) {\n+  InlineKlass* vk = vak->element_klass();\n+  assert(vk->maybe_flat_in_array(), \"should only be used for flat inline type arrays\");\n+  \/\/ Adjust offset to omit oop header\n+  int base_offset = arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT) - vk->payload_offset();\n+  \/\/ Initialize all elements of the flat inline type array\n+  for (int i = 0; i < sv->field_size(); i++) {\n+    ObjectValue* val = sv->field_at(i)->as_ObjectValue();\n+    int offset = base_offset + (i << Klass::layout_helper_log2_element_size(vak->layout_helper()));\n+    reassign_fields_by_klass(vk, fr, reg_map, val, 0, (oop)obj, is_jvmci, offset, CHECK);\n+    if (!obj->is_null_free_array()) {\n+      jboolean null_marker_value;\n+      if (val->has_properties()) {\n+        null_marker_value = StackValue::create_stack_value(fr, reg_map, val->properties())->get_jint() & 1;\n+      } else {\n+        null_marker_value = 1;\n+      }\n+      obj->bool_field_put(offset + vk->null_marker_offset(), null_marker_value);\n+    }\n+  }\n+}\n+\n@@ -1576,1 +1700,1 @@\n-void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool is_jvmci) {\n+void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool is_jvmci, TRAPS) {\n@@ -1581,0 +1705,2 @@\n+    k = get_refined_array_klass(k, fr, reg_map, sv, THREAD);\n+\n@@ -1582,1 +1708,1 @@\n-    assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n+    assert(obj.not_null() || realloc_failures || sv->has_properties(), \"reallocation was missed\");\n@@ -1620,1 +1746,4 @@\n-      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), is_jvmci);\n+      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), is_jvmci, 0, CHECK);\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+      reassign_flat_array_elements(fr, reg_map, sv, (flatArrayOop) obj(), vak, is_jvmci, CHECK);\n@@ -1624,1 +1753,1 @@\n-    } else if (k->is_objArray_klass()) {\n+    } else if (k->is_refArray_klass()) {\n@@ -1802,1 +1931,1 @@\n-  \/\/ Deoptimize only if the frame comes from compile code.\n+  \/\/ Deoptimize only if the frame comes from compiled code.\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":164,"deletions":35,"binary":false,"changes":199,"status":"modified"},{"patch":"@@ -410,0 +410,1 @@\n+  initialize_class(vmSymbols::java_lang_IdentityException(), CHECK);\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -61,0 +61,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -945,1 +947,3 @@\n-           declare_type(ObjArrayKlass, ArrayKlass)                        \\\n+           declare_type(ObjArrayKlass, ArrayKlass)                        \\\n+             declare_type(FlatArrayKlass, ArrayKlass)                     \\\n+             declare_type(RefArrayKlass, ArrayKlass)                      \\\n@@ -948,0 +952,1 @@\n+        declare_type(InlineKlass, InstanceKlass)                          \\\n@@ -1419,1 +1424,1 @@\n-  declare_constant(Klass::_lh_array_tag_obj_value)                        \\\n+  declare_constant(Klass::_lh_array_tag_ref_value)                        \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -41,0 +41,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n@@ -48,0 +50,1 @@\n+#include \"runtime\/fieldDescriptor.inline.hpp\"\n@@ -319,0 +322,22 @@\n+ * HPROF_FLAT_ARRAYS        list of flat arrays\n+ *\n+ *               [flat array sub-records]*\n+ *\n+ *               HPROF_FLAT_ARRAY      flat array\n+ *\n+ *                          id         array object ID (dumped as HPROF_GC_PRIM_ARRAY_DUMP)\n+ *                          id         element class ID (dumped by HPROF_GC_CLASS_DUMP)\n+ *\n+ * HPROF_INLINED_FIELDS     decribes inlined fields\n+ *\n+ *               [class with inlined fields sub-records]*\n+ *\n+ *               HPROF_CLASS_WITH_INLINED_FIELDS\n+ *\n+ *                          id         class ID (dumped as HPROF_GC_CLASS_DUMP)\n+ *\n+ *                          u2         number of instance inlined fields (not including super)\n+ *                          [u2,       inlined field index,\n+ *                           u2,       synthetic field count,\n+ *                           id,       original field name,\n+ *                           id]*      inlined field class ID (dumped by HPROF_GC_CLASS_DUMP)\n@@ -356,0 +381,7 @@\n+  \/\/ inlined object support\n+  HPROF_FLAT_ARRAYS             = 0x12,\n+  HPROF_INLINED_FIELDS          = 0x13,\n+  \/\/ inlined object subrecords\n+  HPROF_FLAT_ARRAY                  = 0x01,\n+  HPROF_CLASS_WITH_INLINED_FIELDS   = 0x01,\n+\n@@ -390,0 +422,65 @@\n+\n+class AbstractDumpWriter;\n+\n+class InlinedObjects {\n+\n+  struct ClassInlinedFields {\n+    const Klass *klass;\n+    uintx base_index;   \/\/ base index of the inlined field names (1st field has index base_index+1).\n+    ClassInlinedFields(const Klass *klass = nullptr, uintx base_index = 0) : klass(klass), base_index(base_index) {}\n+\n+    \/\/ For GrowableArray::find_sorted().\n+    static int compare(const ClassInlinedFields& a, const ClassInlinedFields& b) {\n+      return a.klass - b.klass;\n+    }\n+    \/\/ For GrowableArray::sort().\n+    static int compare(ClassInlinedFields* a, ClassInlinedFields* b) {\n+      return compare(*a, *b);\n+    }\n+  };\n+\n+  uintx _min_string_id;\n+  uintx _max_string_id;\n+\n+  GrowableArray<ClassInlinedFields> *_inlined_field_map;\n+\n+  \/\/ counters for classes with inlined fields and for the fields\n+  int _classes_count;\n+  int _inlined_fields_count;\n+\n+  static InlinedObjects *_instance;\n+\n+  static void inlined_field_names_callback(InlinedObjects* _this, const Klass *klass, uintx base_index, int count);\n+\n+  GrowableArray<oop> *_flat_arrays;\n+\n+public:\n+  InlinedObjects()\n+    : _min_string_id(0), _max_string_id(0),\n+    _inlined_field_map(nullptr),\n+    _classes_count(0), _inlined_fields_count(0),\n+    _flat_arrays(nullptr) {\n+  }\n+\n+  static InlinedObjects* get_instance() {\n+    return _instance;\n+  }\n+\n+  void init();\n+  void release();\n+\n+  void dump_inlined_field_names(AbstractDumpWriter *writer);\n+\n+  uintx get_base_index_for(Klass* k);\n+  uintx get_next_string_id(uintx id);\n+\n+  void dump_classed_with_inlined_fields(AbstractDumpWriter* writer);\n+\n+  void add_flat_array(oop array);\n+  void dump_flat_arrays(AbstractDumpWriter* writer);\n+\n+};\n+\n+InlinedObjects *InlinedObjects::_instance = nullptr;\n+\n+\n@@ -746,1 +843,1 @@\n-  \/\/ returns the size of the instance of the given class\n+  \/\/ calculates the total size of the all fields of the given class.\n@@ -759,2 +856,8 @@\n-  \/\/ dump the raw values of the instance fields of the given object\n-  static void dump_instance_fields(AbstractDumpWriter* writer, oop o, DumperClassCacheTableEntry* class_cache_entry);\n+  \/\/ dump the raw values of the instance fields of the given identity or inlined object;\n+  \/\/ for identity objects offset is 0 and 'klass' is o->klass(),\n+  \/\/ for inlined objects offset is the offset in the holder object, 'klass' is inlined object class\n+  static void dump_instance_fields(AbstractDumpWriter* writer, oop o, int offset, DumperClassCacheTable* class_cache, DumperClassCacheTableEntry* class_cache_entry);\n+  \/\/ dump the raw values of the instance fields of the given inlined object;\n+  \/\/ dump_instance_fields wrapper for inlined objects\n+  static void dump_inlined_object_fields(AbstractDumpWriter* writer, oop o, int offset, DumperClassCacheTable* class_cache, DumperClassCacheTableEntry* class_cache_entry);\n+\n@@ -764,1 +867,1 @@\n-  static void dump_instance_field_descriptors(AbstractDumpWriter* writer, Klass* k);\n+  static void dump_instance_field_descriptors(AbstractDumpWriter* writer, InstanceKlass* k, uintx *inlined_fields_index = nullptr);\n@@ -774,0 +877,2 @@\n+  \/\/ creates HPROF_GC_PRIM_ARRAY_DUMP record for the given flat array\n+  static void dump_flat_array(AbstractDumpWriter* writer, flatArrayOop array, DumperClassCacheTable* class_cache);\n@@ -781,0 +886,3 @@\n+  \/\/ extended version to dump flat arrays as primitive arrays;\n+  \/\/ type_size specifies size of the inlined objects.\n+  static int calculate_array_max_length(AbstractDumpWriter* writer, arrayOop array, int type_size, short header_size);\n@@ -796,0 +904,10 @@\n+  \/\/ helper methods for inlined fields.\n+  static bool is_inlined_field(const fieldDescriptor& fld) {\n+    return fld.is_flat();\n+  }\n+  static InlineKlass* get_inlined_field_klass(const fieldDescriptor& fld) {\n+    assert(is_inlined_field(fld), \"must be inlined field\");\n+    InstanceKlass* holder_klass = fld.field_holder();\n+    return InlineKlass::cast(holder_klass->get_inline_type_field_klass(fld.index()));\n+  }\n+\n@@ -820,0 +938,1 @@\n+  GrowableArray<InlineKlass*> _inline_klasses;\n@@ -828,0 +947,3 @@\n+  void push_sig_start_inlined() { _sigs_start.push('Q'); }\n+  bool is_inlined(int field_idx){ return _sigs_start.at(field_idx) == 'Q'; }\n+  InlineKlass* inline_klass(int field_idx) { assert(is_inlined(field_idx), \"Not inlined\"); return _inline_klasses.at(field_idx); }\n@@ -876,2 +998,11 @@\n-          Symbol* sig = fld.signature();\n-          entry->_sigs_start.push(sig->char_at(0));\n+          InlineKlass* inlineKlass = nullptr;\n+          if (DumperSupport::is_inlined_field(fld.field_descriptor())) {\n+            inlineKlass = DumperSupport::get_inlined_field_klass(fld.field_descriptor());\n+            entry->push_sig_start_inlined();\n+            entry->_instance_size += DumperSupport::instance_size(inlineKlass);\n+          } else {\n+            Symbol* sig = fld.signature();\n+            entry->_sigs_start.push(sig->char_at(0));\n+            entry->_instance_size += DumperSupport::sig2size(sig);\n+          }\n+          entry->_inline_klasses.push(inlineKlass);\n@@ -880,1 +1011,0 @@\n-          entry->_instance_size += DumperSupport::sig2size(sig);\n@@ -990,0 +1120,1 @@\n+\n@@ -1048,1 +1179,1 @@\n-\/\/ returns the size of the instance of the given class\n+\/\/ calculates the total size of the all fields of the given class.\n@@ -1056,1 +1187,5 @@\n-        size += sig2size(fld.signature());\n+        if (is_inlined_field(fld.field_descriptor())) {\n+          size += instance_size(get_inlined_field_klass(fld.field_descriptor()));\n+        } else {\n+          size += sig2size(fld.signature());\n+        }\n@@ -1069,0 +1204,2 @@\n+      assert(!is_inlined_field(fldc.field_descriptor()), \"static fields cannot be inlined\");\n+\n@@ -1111,0 +1248,2 @@\n+      assert(!is_inlined_field(fld.field_descriptor()), \"static fields cannot be inlined\");\n+\n@@ -1147,2 +1286,4 @@\n-\/\/ dump the raw values of the instance fields of the given object\n-void DumperSupport::dump_instance_fields(AbstractDumpWriter* writer, oop o, DumperClassCacheTableEntry* class_cache_entry) {\n+\/\/ dump the raw values of the instance fields of the given identity or inlined object;\n+\/\/ for identity objects offset is 0 and 'klass' is o->klass(),\n+\/\/ for inlined objects offset is the offset in the holder object, 'klass' is inlined object class.\n+void DumperSupport::dump_instance_fields(AbstractDumpWriter* writer, oop o, int offset, DumperClassCacheTable* class_cache, DumperClassCacheTableEntry* class_cache_entry) {\n@@ -1151,1 +1292,8 @@\n-    dump_field_value(writer, class_cache_entry->sig_start(idx), o, class_cache_entry->offset(idx));\n+    if (class_cache_entry->is_inlined(idx)) {\n+      InlineKlass* field_klass = class_cache_entry->inline_klass(idx);\n+      int fields_offset = offset + (class_cache_entry->offset(idx) - field_klass->payload_offset());\n+      DumperClassCacheTableEntry* inline_class_cache_entry = class_cache->lookup_or_create(field_klass);\n+      dump_inlined_object_fields(writer, o, fields_offset, class_cache, inline_class_cache_entry);\n+    } else {\n+      dump_field_value(writer, class_cache_entry->sig_start(idx), o, class_cache_entry->offset(idx));\n+    }\n@@ -1155,1 +1303,6 @@\n-\/\/ dumps the definition of the instance fields for a given class\n+void DumperSupport::dump_inlined_object_fields(AbstractDumpWriter* writer, oop o, int offset, DumperClassCacheTable* class_cache, DumperClassCacheTableEntry* class_cache_entry) {\n+  \/\/ the object is inlined, so all its fields are stored without headers.\n+  dump_instance_fields(writer, o, offset, class_cache, class_cache_entry);\n+}\n+\n+\/\/ gets the count of the instance fields for a given class\n@@ -1160,1 +1313,8 @@\n-    if (!fldc.access_flags().is_static()) field_count++;\n+    if (!fldc.access_flags().is_static()) {\n+      if (is_inlined_field(fldc.field_descriptor())) {\n+        \/\/ add \"synthetic\" fields for inlined fields.\n+        field_count += get_instance_fields_count(get_inlined_field_klass(fldc.field_descriptor()));\n+      } else {\n+        field_count++;\n+      }\n+    }\n@@ -1167,2 +1327,8 @@\n-void DumperSupport::dump_instance_field_descriptors(AbstractDumpWriter* writer, Klass* k) {\n-  InstanceKlass* ik = InstanceKlass::cast(k);\n+\/\/ inlined_fields_id is not-nullptr for inlined fields (to get synthetic field name IDs\n+\/\/ by using InlinedObjects::get_next_string_id()).\n+void DumperSupport::dump_instance_field_descriptors(AbstractDumpWriter* writer, InstanceKlass* ik, uintx* inlined_fields_id) {\n+  \/\/ inlined_fields_id != nullptr means ik is a class of inlined field.\n+  \/\/ Inlined field id pointer for this class; lazyly initialized\n+  \/\/ if the class has inlined field(s) and the caller didn't provide inlined_fields_id.\n+  uintx *this_klass_inlined_fields_id = inlined_fields_id;\n+  uintx inlined_id = 0;\n@@ -1173,1 +1339,23 @@\n-      Symbol* sig = fld.signature();\n+      if (is_inlined_field(fld.field_descriptor())) {\n+        \/\/ dump \"synthetic\" fields for inlined fields.\n+        if (this_klass_inlined_fields_id == nullptr) {\n+          inlined_id = InlinedObjects::get_instance()->get_base_index_for(ik);\n+          this_klass_inlined_fields_id = &inlined_id;\n+        }\n+        dump_instance_field_descriptors(writer, get_inlined_field_klass(fld.field_descriptor()), this_klass_inlined_fields_id);\n+      } else {\n+        Symbol* sig = fld.signature();\n+        Symbol* name = nullptr;\n+        \/\/ Use inlined_fields_id provided by caller.\n+        if (inlined_fields_id != nullptr) {\n+          uintx name_id = InlinedObjects::get_instance()->get_next_string_id(*inlined_fields_id);\n+\n+          \/\/ name_id == 0 is returned on error. use original field signature.\n+          if (name_id != 0) {\n+            *inlined_fields_id = name_id;\n+            name = reinterpret_cast<Symbol*>(name_id);\n+          }\n+        }\n+        if (name == nullptr) {\n+          name = fld.name();\n+        }\n@@ -1175,2 +1363,3 @@\n-      writer->write_symbolID(fld.name());   \/\/ name\n-      writer->write_u1(sig2tag(sig));       \/\/ type\n+        writer->write_symbolID(name);         \/\/ name\n+        writer->write_u1(sig2tag(sig));       \/\/ type\n+      }\n@@ -1201,1 +1390,1 @@\n-  dump_instance_fields(writer, o, cache_entry);\n+  dump_instance_fields(writer, o, 0, class_cache, cache_entry);\n@@ -1244,1 +1433,1 @@\n-  writer->write_u4(DumperSupport::instance_size(ik));\n+  writer->write_u4(HeapWordSize * ik->size_helper());\n@@ -1298,4 +1487,1 @@\n-int DumperSupport::calculate_array_max_length(AbstractDumpWriter* writer, arrayOop array, short header_size) {\n-  BasicType type = ArrayKlass::cast(array->klass())->element_type();\n-  assert(type >= T_BOOLEAN && type <= T_OBJECT, \"invalid array element type\");\n-\n+int DumperSupport::calculate_array_max_length(AbstractDumpWriter* writer, arrayOop array, int type_size, short header_size) {\n@@ -1304,7 +1490,0 @@\n-  int type_size;\n-  if (type == T_OBJECT) {\n-    type_size = sizeof(address);\n-  } else {\n-    type_size = type2aelembytes(type);\n-  }\n-\n@@ -1318,0 +1497,1 @@\n+    BasicType type = ArrayKlass::cast(array->klass())->element_type();\n@@ -1324,0 +1504,16 @@\n+int DumperSupport::calculate_array_max_length(AbstractDumpWriter* writer, arrayOop array, short header_size) {\n+  BasicType type = ArrayKlass::cast(array->klass())->element_type();\n+  assert((type >= T_BOOLEAN && type <= T_OBJECT) || type == T_FLAT_ELEMENT, \"invalid array element type\");\n+  int type_size;\n+  if (type == T_OBJECT) {\n+    type_size = sizeof(address);\n+  } else if (type == T_FLAT_ELEMENT) {\n+      \/\/ TODO: FIXME\n+      fatal(\"Not supported yet\"); \/\/ FIXME: JDK-8325678\n+  } else {\n+    type_size = type2aelembytes(type);\n+  }\n+\n+  return calculate_array_max_length(writer, array, type_size, header_size);\n+}\n+\n@@ -1349,0 +1545,41 @@\n+\/\/ creates HPROF_GC_PRIM_ARRAY_DUMP record for the given flat array\n+void DumperSupport::dump_flat_array(AbstractDumpWriter* writer, flatArrayOop array, DumperClassCacheTable* class_cache) {\n+  FlatArrayKlass* array_klass = FlatArrayKlass::cast(array->klass());\n+  InlineKlass* element_klass = array_klass->element_klass();\n+  int element_size = instance_size(element_klass);\n+  \/*                          id         array object ID\n+   *                          u4         stack trace serial number\n+   *                          u4         number of elements\n+   *                          u1         element type\n+   *\/\n+  short header_size = 1 + sizeof(address) + 2 * 4 + 1;\n+\n+  \/\/ TODO: use T_SHORT\/T_INT\/T_LONG if needed to avoid truncation\n+  BasicType type = T_BYTE;\n+  int type_size = type2aelembytes(type);\n+  int length = calculate_array_max_length(writer, array, element_size, header_size);\n+  u4 length_in_bytes = (u4)(length * element_size);\n+  u4 size = header_size + length_in_bytes;\n+\n+  writer->start_sub_record(HPROF_GC_PRIM_ARRAY_DUMP, size);\n+  writer->write_objectID(array);\n+  writer->write_u4(STACK_TRACE_ID);\n+  \/\/ TODO: round up array length for T_SHORT\/T_INT\/T_LONG\n+  writer->write_u4(length * element_size);\n+  writer->write_u1(type2tag(type));\n+\n+  for (int index = 0; index < length; index++) {\n+    \/\/ need offset in the holder to read inlined object. calculate it from flatArrayOop::value_at_addr()\n+    int offset = (int)((address)array->value_at_addr(index, array_klass->layout_helper())\n+                  - cast_from_oop<address>(array));\n+    DumperClassCacheTableEntry* class_cache_entry = class_cache->lookup_or_create(element_klass);\n+    dump_inlined_object_fields(writer, array, offset, class_cache, class_cache_entry);\n+  }\n+\n+  \/\/ TODO: write padding bytes for T_SHORT\/T_INT\/T_LONG\n+\n+  InlinedObjects::get_instance()->add_flat_array(array);\n+\n+  writer->end_sub_record();\n+}\n+\n@@ -1470,0 +1707,264 @@\n+class InlinedFieldNameDumper : public LockedClassesDo {\n+public:\n+  typedef void (*Callback)(InlinedObjects *owner, const Klass *klass, uintx base_index, int count);\n+\n+private:\n+  AbstractDumpWriter* _writer;\n+  InlinedObjects *_owner;\n+  Callback       _callback;\n+  uintx _index;\n+\n+  void dump_inlined_field_names(GrowableArray<Symbol*>* super_names, Symbol* field_name, InlineKlass* klass) {\n+    super_names->push(field_name);\n+    for (HierarchicalFieldStream<JavaFieldStream> fld(klass); !fld.done(); fld.next()) {\n+      if (!fld.access_flags().is_static()) {\n+        if (DumperSupport::is_inlined_field(fld.field_descriptor())) {\n+          dump_inlined_field_names(super_names, fld.name(), DumperSupport::get_inlined_field_klass(fld.field_descriptor()));\n+        } else {\n+          \/\/ get next string ID.\n+          uintx next_index = _owner->get_next_string_id(_index);\n+          if (next_index == 0) {\n+            \/\/ something went wrong (overflow?)\n+            \/\/ stop generation; the rest of inlined objects will have original field names.\n+            return;\n+          }\n+          _index = next_index;\n+\n+          \/\/ Calculate length.\n+          int len = fld.name()->utf8_length();\n+          for (GrowableArrayIterator<Symbol*> it = super_names->begin(); it != super_names->end(); ++it) {\n+            len += (*it)->utf8_length() + 1;    \/\/ +1 for \".\".\n+          }\n+\n+          DumperSupport::write_header(_writer, HPROF_UTF8, oopSize + len);\n+          _writer->write_symbolID(reinterpret_cast<Symbol*>(_index));\n+          \/\/ Write the string value.\n+          \/\/ 1) super_names.\n+          for (GrowableArrayIterator<Symbol*> it = super_names->begin(); it != super_names->end(); ++it) {\n+            _writer->write_raw((*it)->bytes(), (*it)->utf8_length());\n+            _writer->write_u1('.');\n+          }\n+          \/\/ 2) field name.\n+          _writer->write_raw(fld.name()->bytes(), fld.name()->utf8_length());\n+        }\n+      }\n+    }\n+    super_names->pop();\n+  }\n+\n+  void dump_inlined_field_names(Symbol* field_name, InlineKlass* field_klass) {\n+    GrowableArray<Symbol*> super_names(4, mtServiceability);\n+    dump_inlined_field_names(&super_names, field_name, field_klass);\n+  }\n+\n+public:\n+  InlinedFieldNameDumper(AbstractDumpWriter* writer, InlinedObjects* owner, Callback callback)\n+    : _writer(writer), _owner(owner), _callback(callback), _index(0)  {\n+  }\n+\n+  void do_klass(Klass* k) {\n+    if (!k->is_instance_klass()) {\n+      return;\n+    }\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    \/\/ if (ik->has_inline_type_fields()) {\n+    \/\/   return;\n+    \/\/ }\n+\n+    uintx base_index = _index;\n+    int count = 0;\n+\n+    for (HierarchicalFieldStream<JavaFieldStream> fld(ik); !fld.done(); fld.next()) {\n+      if (!fld.access_flags().is_static()) {\n+        if (DumperSupport::is_inlined_field(fld.field_descriptor())) {\n+          dump_inlined_field_names(fld.name(), DumperSupport::get_inlined_field_klass(fld.field_descriptor()));\n+          count++;\n+        }\n+      }\n+    }\n+\n+    if (count != 0) {\n+      _callback(_owner, k, base_index, count);\n+    }\n+  }\n+};\n+\n+class InlinedFieldsDumper : public LockedClassesDo {\n+private:\n+  AbstractDumpWriter* _writer;\n+\n+public:\n+  InlinedFieldsDumper(AbstractDumpWriter* writer) : _writer(writer) {}\n+\n+  void do_klass(Klass* k) {\n+    if (!k->is_instance_klass()) {\n+      return;\n+    }\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    \/\/ if (ik->has_inline_type_fields()) {\n+    \/\/   return;\n+    \/\/ }\n+\n+    \/\/ We can be at a point where java mirror does not exist yet.\n+    \/\/ So we need to check that the class is at least loaded, to avoid crash from a null mirror.\n+    if (!ik->is_loaded()) {\n+      return;\n+    }\n+\n+    u2 inlined_count = 0;\n+    for (HierarchicalFieldStream<JavaFieldStream> fld(ik); !fld.done(); fld.next()) {\n+      if (!fld.access_flags().is_static()) {\n+        if (DumperSupport::is_inlined_field(fld.field_descriptor())) {\n+          inlined_count++;\n+        }\n+      }\n+    }\n+    if (inlined_count != 0) {\n+      _writer->write_u1(HPROF_CLASS_WITH_INLINED_FIELDS);\n+\n+      \/\/ class ID\n+      _writer->write_classID(ik);\n+      \/\/ number of inlined fields\n+      _writer->write_u2(inlined_count);\n+      u2 index = 0;\n+      for (HierarchicalFieldStream<JavaFieldStream> fld(ik); !fld.done(); fld.next()) {\n+        if (!fld.access_flags().is_static()) {\n+          if (DumperSupport::is_inlined_field(fld.field_descriptor())) {\n+            \/\/ inlined field index\n+            _writer->write_u2(index);\n+            \/\/ synthetic field count\n+            u2 field_count = DumperSupport::get_instance_fields_count(DumperSupport::get_inlined_field_klass(fld.field_descriptor()));\n+            _writer->write_u2(field_count);\n+            \/\/ original field name\n+            _writer->write_symbolID(fld.name());\n+            \/\/ inlined field class ID\n+            _writer->write_classID(DumperSupport::get_inlined_field_klass(fld.field_descriptor()));\n+\n+            index += field_count;\n+          } else {\n+            index++;\n+          }\n+        }\n+      }\n+    }\n+  }\n+};\n+\n+\n+void InlinedObjects::init() {\n+  _instance = this;\n+\n+  struct Closure : public SymbolClosure {\n+    uintx _min_id = max_uintx;\n+    uintx _max_id = 0;\n+    Closure() : _min_id(max_uintx), _max_id(0) {}\n+\n+    void do_symbol(Symbol** p) {\n+      uintx val = reinterpret_cast<uintx>(*p);\n+      if (val < _min_id) {\n+        _min_id = val;\n+      }\n+      if (val > _max_id) {\n+        _max_id = val;\n+      }\n+    }\n+  } closure;\n+\n+  SymbolTable::symbols_do(&closure);\n+\n+  _min_string_id = closure._min_id;\n+  _max_string_id = closure._max_id;\n+}\n+\n+void InlinedObjects::release() {\n+  _instance = nullptr;\n+\n+  if (_inlined_field_map != nullptr) {\n+    delete _inlined_field_map;\n+    _inlined_field_map = nullptr;\n+  }\n+  if (_flat_arrays != nullptr) {\n+    delete _flat_arrays;\n+    _flat_arrays = nullptr;\n+  }\n+}\n+\n+void InlinedObjects::inlined_field_names_callback(InlinedObjects* _this, const Klass* klass, uintx base_index, int count) {\n+  if (_this->_inlined_field_map == nullptr) {\n+    _this->_inlined_field_map = new (mtServiceability) GrowableArray<ClassInlinedFields>(100, mtServiceability);\n+  }\n+  _this->_inlined_field_map->append(ClassInlinedFields(klass, base_index));\n+\n+  \/\/ counters for dumping classes with inlined fields\n+  _this->_classes_count++;\n+  _this->_inlined_fields_count += count;\n+}\n+\n+void InlinedObjects::dump_inlined_field_names(AbstractDumpWriter* writer) {\n+  InlinedFieldNameDumper nameDumper(writer, this, inlined_field_names_callback);\n+  ClassLoaderDataGraph::classes_do(&nameDumper);\n+\n+  if (_inlined_field_map != nullptr) {\n+    \/\/ prepare the map for  get_base_index_for().\n+    _inlined_field_map->sort(ClassInlinedFields::compare);\n+  }\n+}\n+\n+uintx InlinedObjects::get_base_index_for(Klass* k) {\n+  if (_inlined_field_map != nullptr) {\n+    bool found = false;\n+    int idx = _inlined_field_map->find_sorted<ClassInlinedFields, ClassInlinedFields::compare>(ClassInlinedFields(k, 0), found);\n+    if (found) {\n+        return _inlined_field_map->at(idx).base_index;\n+    }\n+  }\n+\n+  \/\/ return max_uintx, so get_next_string_id returns 0.\n+  return max_uintx;\n+}\n+\n+uintx InlinedObjects::get_next_string_id(uintx id) {\n+  if (++id == _min_string_id) {\n+    return _max_string_id + 1;\n+  }\n+  return id;\n+}\n+\n+void InlinedObjects::dump_classed_with_inlined_fields(AbstractDumpWriter* writer) {\n+  if (_classes_count != 0) {\n+    \/\/ Record for each class contains tag(u1), class ID and count(u2)\n+    \/\/ for each inlined field index(u2), synthetic fields count(u2), original field name and class ID\n+    int size = _classes_count * (1 + sizeof(address) + 2)\n+             + _inlined_fields_count * (2 + 2 + sizeof(address) + sizeof(address));\n+    DumperSupport::write_header(writer, HPROF_INLINED_FIELDS, (u4)size);\n+\n+    InlinedFieldsDumper dumper(writer);\n+    ClassLoaderDataGraph::classes_do(&dumper);\n+  }\n+}\n+\n+void InlinedObjects::add_flat_array(oop array) {\n+  if (_flat_arrays == nullptr) {\n+    _flat_arrays = new (mtServiceability) GrowableArray<oop>(100, mtServiceability);\n+  }\n+  _flat_arrays->append(array);\n+}\n+\n+void InlinedObjects::dump_flat_arrays(AbstractDumpWriter* writer) {\n+  if (_flat_arrays != nullptr) {\n+    \/\/ For each flat array the record contains tag (u1), object ID and class ID.\n+    int size = _flat_arrays->length() * (1 + sizeof(address) + sizeof(address));\n+\n+    DumperSupport::write_header(writer, HPROF_FLAT_ARRAYS, (u4)size);\n+    for (GrowableArrayIterator<oop> it = _flat_arrays->begin(); it != _flat_arrays->end(); ++it) {\n+      flatArrayOop array = flatArrayOop(*it);\n+      FlatArrayKlass* array_klass = FlatArrayKlass::cast(array->klass());\n+      InlineKlass* element_klass = array_klass->element_klass();\n+      writer->write_u1(HPROF_FLAT_ARRAY);\n+      writer->write_objectID(array);\n+      writer->write_classID(element_klass);\n+    }\n+  }\n+}\n+\n+\n@@ -2002,0 +2503,2 @@\n+  } else if (o->is_flatArray()) {\n+    DumperSupport::dump_flat_array(writer(), flatArrayOop(o), &_class_cache);\n@@ -2081,0 +2584,1 @@\n+  InlinedObjects*  _inlined_objects;\n@@ -2091,1 +2595,1 @@\n-  DumpMerger(const char* path, DumpWriter* writer, int dump_seq) :\n+  DumpMerger(const char* path, DumpWriter* writer, InlinedObjects* inlined_objects, int dump_seq) :\n@@ -2093,0 +2597,1 @@\n+    _inlined_objects(inlined_objects),\n@@ -2124,0 +2629,1 @@\n+    _inlined_objects->dump_flat_arrays(_writer);\n@@ -2125,0 +2631,1 @@\n+    _inlined_objects->release();\n@@ -2244,0 +2751,4 @@\n+\n+  \/\/ Inlined object support.\n+  InlinedObjects          _inlined_objects;\n+\n@@ -2324,0 +2835,2 @@\n+  InlinedObjects* inlined_objects() { return &_inlined_objects; }\n+\n@@ -2460,0 +2973,7 @@\n+    \/\/ HPROF_UTF8 records for inlined field names.\n+    inlined_objects()->init();\n+    inlined_objects()->dump_inlined_field_names(writer());\n+\n+    \/\/ HPROF_INLINED_FIELDS\n+    inlined_objects()->dump_classed_with_inlined_fields(writer());\n+\n@@ -2662,1 +3182,1 @@\n-  DumpMerger merger(path, &writer, dumper.dump_seq());\n+  DumpMerger merger(path, &writer, dumper.inlined_objects(), dumper.dump_seq());\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":554,"deletions":34,"binary":false,"changes":588,"status":"modified"},{"patch":"@@ -612,0 +612,9 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Prototyping\n+\/\/ \"Code Missing Here\" macro, un-define when integrating back from prototyping stage and break\n+\/\/ compilation on purpose (i.e. \"forget me not\")\n+#define PROTOTYPE\n+#ifdef PROTOTYPE\n+#define CMH(m)\n+#endif\n+\n@@ -690,5 +699,6 @@\n-  T_ADDRESS     = 15,\n-  T_NARROWOOP   = 16,\n-  T_METADATA    = 17,\n-  T_NARROWKLASS = 18,\n-  T_CONFLICT    = 19, \/\/ for stack value type with conflicting contents\n+  T_FLAT_ELEMENT = 15, \/\/ Not a true BasicType, only used in layout helpers of flat arrays\n+  T_ADDRESS     = 16,\n+  T_NARROWOOP   = 17,\n+  T_METADATA    = 18,\n+  T_NARROWKLASS = 19,\n+  T_CONFLICT    = 20, \/\/ for stack value type with conflicting contents\n@@ -738,0 +748,1 @@\n+  assert(t != T_FLAT_ELEMENT, \"\");  \/\/ Strong assert to detect misuses of T_FLAT_ELEMENT\n@@ -812,1 +823,2 @@\n-  T_VOID_size        = 0\n+  T_VOID_size        = 0,\n+  T_FLAT_ELEMENT_size = 0\n@@ -848,1 +860,2 @@\n-  T_VOID_aelem_bytes        = 0\n+  T_VOID_aelem_bytes        = 0,\n+  T_FLAT_ELEMENT_aelem_bytes = 0\n@@ -938,1 +951,1 @@\n-  vtos = 9,             \/\/ tos not cached\n+  vtos = 9,             \/\/ tos not cached,\n@@ -955,1 +968,1 @@\n-    case T_ARRAY  : \/\/ fall through\n+    case T_ARRAY  :   \/\/ fall through\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":22,"deletions":9,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"utilities\/ostream.hpp\"\n@@ -74,0 +75,215 @@\n+class StringMatcher {\n+ public:\n+  typedef int getc_function_t(const char* &source, const char* limit);\n+\n+ private:\n+  \/\/ These do not get properly inlined.\n+  \/\/ For full performance, this should be a template class\n+  \/\/ parameterized by two function arguments.\n+  getc_function_t* _pattern_getc;\n+  getc_function_t* _string_getc;\n+\n+ public:\n+  StringMatcher(getc_function_t pattern_getc,\n+                getc_function_t string_getc)\n+    : _pattern_getc(pattern_getc),\n+      _string_getc(string_getc)\n+  { }\n+\n+  enum {  \/\/ special results from _pattern_getc\n+    string_match_comma  = -0x100 + ',',\n+    string_match_star   = -0x100 + '*',\n+    string_match_eos    = -0x100 + '\\0'\n+  };\n+\n+ private:\n+  const char*\n+  skip_anchor_word(const char* match,\n+                   const char* match_end,\n+                   int anchor_length,\n+                   const char* pattern,\n+                   const char* pattern_end) {\n+    assert(pattern < pattern_end && anchor_length > 0, \"\");\n+    const char* begp = pattern;\n+    int ch1 = _pattern_getc(begp, pattern_end);\n+    \/\/ note that begp is now advanced over ch1\n+    assert(ch1 > 0, \"regular char only\");\n+    const char* matchp = match;\n+    const char* limitp = match_end - anchor_length;\n+    while (matchp <= limitp) {\n+      int mch = _string_getc(matchp, match_end);\n+      if (mch == ch1) {\n+        const char* patp = begp;\n+        const char* anchorp = matchp;\n+        while (patp < pattern_end) {\n+          char ch = _pattern_getc(patp, pattern_end);\n+          char mch = _string_getc(anchorp, match_end);\n+          if (mch != ch) {\n+            anchorp = nullptr;\n+            break;\n+          }\n+        }\n+        if (anchorp != nullptr) {\n+          return anchorp;  \/\/ Found a full copy of the anchor.\n+        }\n+        \/\/ That did not work, so restart the search for ch1.\n+      }\n+    }\n+    return nullptr;\n+  }\n+\n+ public:\n+  bool string_match(const char* pattern,\n+                    const char* string) {\n+    return string_match(pattern, pattern + strlen(pattern),\n+                        string, string + strlen(string));\n+  }\n+  bool string_match(const char* pattern, const char* pattern_end,\n+                    const char* string, const char* string_end) {\n+    const char* patp = pattern;\n+    switch (_pattern_getc(patp, pattern_end)) {\n+    case string_match_eos:\n+      return false;  \/\/ Empty pattern is always false.\n+    case string_match_star:\n+      if (patp == pattern_end) {\n+        return true;   \/\/ Lone star pattern is always true.\n+      }\n+      break;\n+    }\n+    patp = pattern;  \/\/ Reset after lookahead.\n+    const char* matchp = string;  \/\/ nullptr if failing\n+    for (;;) {\n+      int ch = _pattern_getc(patp, pattern_end);\n+      switch (ch) {\n+      case string_match_eos:\n+      case string_match_comma:\n+        \/\/ End of a list item; see if it's a match.\n+        if (matchp == string_end) {\n+          return true;\n+        }\n+        if (ch == string_match_comma) {\n+          \/\/ Get ready to match the next item.\n+          matchp = string;\n+          continue;\n+        }\n+        return false;  \/\/ End of all items.\n+\n+      case string_match_star:\n+        if (matchp != nullptr) {\n+          \/\/ Wildcard:  Parse out following anchor word and look for it.\n+          const char* begp = patp;\n+          const char* endp = patp;\n+          int anchor_len = 0;\n+          for (;;) {\n+            \/\/ get as many following regular characters as possible\n+            endp = patp;\n+            ch = _pattern_getc(patp, pattern_end);\n+            if (ch <= 0) {\n+              break;\n+            }\n+            anchor_len += 1;\n+          }\n+          \/\/ Anchor word [begp..endp) does not contain ch, so back up.\n+          \/\/ Now do an eager match to the anchor word, and commit to it.\n+          patp = endp;\n+          if (ch == string_match_eos ||\n+              ch == string_match_comma) {\n+            \/\/ Anchor word is at end of pattern, so treat it as a fixed pattern.\n+            const char* limitp = string_end - anchor_len;\n+            matchp = limitp;\n+            patp = begp;\n+            \/\/ Resume normal scanning at the only possible match position.\n+            continue;\n+          }\n+          \/\/ Find a floating occurrence of the anchor and continue matching.\n+          \/\/ Note:  This is greedy; there is no backtrack here.  Good enough.\n+          matchp = skip_anchor_word(matchp, string_end, anchor_len, begp, endp);\n+        }\n+        continue;\n+      }\n+      \/\/ Normal character.\n+      if (matchp != nullptr) {\n+        int mch = _string_getc(matchp, string_end);\n+        if (mch != ch) {\n+          matchp = nullptr;\n+        }\n+      }\n+    }\n+  }\n+};\n+\n+\/\/ Match a wildcarded class list to a proposed class name (in internal form).\n+\/\/ Commas or newlines separate multiple possible matches; stars are shell-style wildcards.\n+class ClassListMatcher : public StringMatcher {\n+ public:\n+  ClassListMatcher()\n+    : StringMatcher(pattern_list_getc, class_name_getc)\n+  { }\n+\n+ private:\n+  static int pattern_list_getc(const char* &pattern_ptr,\n+                               const char* pattern_end) {\n+    if (pattern_ptr == pattern_end) {\n+      return string_match_eos;\n+    }\n+    int ch = (unsigned char) *pattern_ptr++;\n+    switch (ch) {\n+    case ' ': case '\\t': case '\\n': case '\\r':\n+    case ',':\n+      \/\/ End of list item.\n+      for (;;) {\n+        switch (*pattern_ptr) {\n+        case ' ': case '\\t': case '\\n': case '\\r':\n+        case ',':\n+          pattern_ptr += 1;  \/\/ Collapse multiple commas or spaces.\n+          continue;\n+        }\n+        break;\n+      }\n+      return string_match_comma;\n+\n+    case '*':\n+      \/\/ Wildcard, matching any number of chars.\n+      while (*pattern_ptr == '*') {\n+        pattern_ptr += 1;  \/\/ Collapse multiple stars.\n+      }\n+      return string_match_star;\n+\n+    case '.':\n+      ch = '\/';   \/\/ Look for internal form of package separator\n+      break;\n+\n+    case '\\\\':\n+      \/\/ Superquote in pattern escapes * , whitespace, and itself.\n+      if (pattern_ptr < pattern_end) {\n+        ch = (unsigned char) *pattern_ptr++;\n+      }\n+      break;\n+    }\n+\n+    assert(ch > 0, \"regular char only\");\n+    return ch;\n+  }\n+\n+  static int class_name_getc(const char* &name_ptr,\n+                             const char* name_end) {\n+    if (name_ptr == name_end) {\n+      return string_match_eos;\n+    }\n+    int ch = (unsigned char) *name_ptr++;\n+    if (ch == '.') {\n+      ch = '\/';   \/\/ Normalize to internal form of package separator\n+    }\n+    return ch;  \/\/ plain character\n+  }\n+};\n+\n+bool StringUtils::class_list_match(const char* class_pattern_list,\n+                                   const char* class_name) {\n+  if (class_pattern_list == nullptr || class_name == nullptr || class_name[0] == '\\0')\n+    return false;\n+  ClassListMatcher clm;\n+  return clm.string_match(class_pattern_list, class_name);\n+}\n+\n+\n","filename":"src\/hotspot\/share\/utilities\/stringUtils.cpp","additions":216,"deletions":0,"binary":false,"changes":216,"status":"modified"},{"patch":"@@ -49,0 +49,4 @@\n+  \/\/ Match a wildcarded class list to a proposed class name (in internal form).\n+  \/\/ Commas separate multiple possible matches; stars are shell-style wildcards.\n+  static bool class_list_match(const char* class_list, const char* class_name);\n+\n","filename":"src\/hotspot\/share\/utilities\/stringUtils.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+import java.lang.reflect.ClassFileFormatVersion;\n@@ -62,0 +63,1 @@\n+import java.util.HashSet;\n@@ -72,0 +74,1 @@\n+import jdk.internal.javac.PreviewFeature;\n@@ -74,0 +77,1 @@\n+import jdk.internal.misc.PreviewFeatures;\n@@ -222,3 +226,3 @@\n-    private static final int ANNOTATION= 0x00002000;\n-    private static final int ENUM      = 0x00004000;\n-    private static final int SYNTHETIC = 0x00001000;\n+    private static final int ANNOTATION = 0x00002000;\n+    private static final int ENUM       = 0x00004000;\n+    private static final int SYNTHETIC  = 0x00001000;\n@@ -323,0 +327,2 @@\n+                \/\/ Modifier.toString() below mis-interprets SYNCHRONIZED, STRICT, and VOLATILE bits\n+                modifiers &= ~(Modifier.SYNCHRONIZED | Modifier.STRICT | Modifier.VOLATILE);\n@@ -341,4 +347,9 @@\n-                    else if (isRecord())\n-                        sb.append(\"record\");\n-                    else\n-                        sb.append(\"class\");\n+                    else {\n+                        if (isValue()) {\n+                            sb.append(\"value \");\n+                        }\n+                        if (isRecord())\n+                            sb.append(\"record\");\n+                        else\n+                            sb.append(\"class\");\n+                    }\n@@ -615,0 +626,54 @@\n+    \/**\n+     * {@return {@code true} if this {@code Class} object represents an identity class,\n+     * otherwise {@code false}}\n+     *\n+     * <ul>\n+     *      <li>\n+     *          If this {@code Class} object represents an array type this method returns {@code true}.\n+     *      <li>\n+     *          If this {@code Class} object represents an interface, a primitive type,\n+     *          or {@code void} this method returns {@code false}.\n+     *      <li>\n+     *          For all other {@code Class} objects, this method returns {@code true} if either\n+     *          preview features are disabled or {@linkplain Modifier#IDENTITY} is set in the\n+     *          {@linkplain #getModifiers() class modifiers}.\n+     * <\/ul>\n+     * @see AccessFlag#IDENTITY\n+     * @since Valhalla\n+     *\/\n+    @PreviewFeature(feature = PreviewFeature.Feature.VALUE_OBJECTS, reflective=true)\n+    public boolean isIdentity() {\n+        if (isPrimitive()) {\n+            return false;\n+        } else if (PreviewFeatures.isEnabled()) {\n+           return isArray() || Modifier.isIdentity(modifiers);\n+        } else {\n+            return !isInterface();\n+        }\n+    }\n+\n+    \/**\n+     * {@return {@code true} if this {@code Class} object represents a value class,\n+     * otherwise {@code false}}\n+     * <ul>\n+     *      <li>\n+     *          If this {@code Class} object represents an array type this method returns {@code false}.\n+     *      <li>\n+     *          If this {@code Class} object represents an interface, a primitive type,\n+     *          or {@code void} this method returns {@code true} only if preview features are enabled.\n+     *      <li>\n+     *          For all other {@code Class} objects, this method returns {@code true} only if\n+     *          preview features are enabled and {@linkplain Modifier#IDENTITY} is not set in the\n+     *          {@linkplain #getModifiers() class modifiers}.\n+     * <\/ul>\n+     * @see AccessFlag#IDENTITY\n+     * @since Valhalla\n+     *\/\n+    @PreviewFeature(feature = PreviewFeature.Feature.VALUE_OBJECTS, reflective=true)\n+    public boolean isValue() {\n+        if (!PreviewFeatures.isEnabled()) {\n+            return false;\n+        }\n+        return !isIdentity();\n+    }\n+\n@@ -1342,0 +1407,1 @@\n+     * <li> its {@code identity} modifier is always true\n@@ -1366,1 +1432,1 @@\n-    \/**\n+   \/**\n@@ -1369,0 +1435,1 @@\n+     * The {@code AccessFlags} may depend on the class file format version of the class.\n@@ -1377,0 +1444,1 @@\n+    * <li> its {@code identity} modifier is always true\n@@ -1394,0 +1462,1 @@\n+        \/\/ Arrays need to use PRIVATE\/PROTECTED from its component modifiers.\n@@ -1398,2 +1467,10 @@\n-        return getReflectionFactory().parseAccessFlags((location == AccessFlag.Location.CLASS) ?\n-                        getClassFileAccessFlags() : getModifiers(), location, this);\n+        int accessFlags = location == AccessFlag.Location.CLASS ? getClassFileAccessFlags() : getModifiers();\n+        var reflectionFactory = getReflectionFactory();\n+        var ans = reflectionFactory.parseAccessFlags(accessFlags, location, this);\n+        if (PreviewFeatures.isEnabled() && reflectionFactory.classFileFormatVersion(this) != ClassFileFormatVersion.CURRENT_PREVIEW_FEATURES\n+                && isIdentity()) {\n+            var set = new HashSet<>(ans);\n+            set.add(AccessFlag.IDENTITY);\n+            return Set.copyOf(set);\n+        }\n+        return ans;\n@@ -1402,1 +1479,1 @@\n-    \/**\n+   \/**\n@@ -1410,0 +1487,1 @@\n+\n@@ -3783,1 +3861,1 @@\n-         return TypeAnnotationParser.buildAnnotatedInterfaces(getRawTypeAnnotations(), getConstantPool(), this);\n+        return TypeAnnotationParser.buildAnnotatedInterfaces(getRawTypeAnnotations(), getConstantPool(), this);\n@@ -4123,0 +4201,1 @@\n+    \/* package-private *\/\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Class.java","additions":91,"deletions":12,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -122,5 +122,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n- * The {@code equals} method should be used for comparisons.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Duration} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -134,0 +141,1 @@\n+@jdk.internal.MigratedValueClass\n","filename":"src\/java.base\/share\/classes\/java\/time\/Duration.java","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -198,5 +198,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n- * The {@code equals} method should be used for comparisons.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Instant} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -210,0 +217,1 @@\n+@jdk.internal.MigratedValueClass\n","filename":"src\/java.base\/share\/classes\/java\/time\/Instant.java","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -122,5 +122,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n- * The {@code equals} method should be used for comparisons.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code Period} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -134,0 +141,1 @@\n+@jdk.internal.MigratedValueClass\n","filename":"src\/java.base\/share\/classes\/java\/time\/Period.java","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -149,5 +149,12 @@\n- * class; programmers should treat instances that are\n- * {@linkplain #equals(Object) equal} as interchangeable and should not\n- * use instances for synchronization, or unpredictable behavior may\n- * occur. For example, in a future release, synchronization may fail.\n- * The {@code equals} method should be used for comparisons.\n+ * class; programmers should treat instances that are {@linkplain #equals(Object) equal}\n+ * as interchangeable and should not use instances for synchronization, mutexes, or\n+ * with {@linkplain java.lang.ref.Reference object references}.\n+ *\n+ * <div class=\"preview-block\">\n+ *      <div class=\"preview-comment\">\n+ *          When preview features are enabled, {@code ZonedDateTime} is a {@linkplain Class#isValue value class}.\n+ *          Use of value class instances for synchronization, mutexes, or with\n+ *          {@linkplain java.lang.ref.Reference object references} result in\n+ *          {@link IdentityException}.\n+ *      <\/div>\n+ * <\/div>\n@@ -167,0 +174,1 @@\n+@jdk.internal.MigratedValueClass\n","filename":"src\/java.base\/share\/classes\/java\/time\/ZonedDateTime.java","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -281,0 +281,6 @@\n+        \/**\n+         * Warn about code in identity classes that wouldn't be allowed in early\n+         * construction due to a this dependency.\n+         *\/\n+        INITIALIZATION(\"initialization\"),\n+\n@@ -296,0 +302,5 @@\n+        \/**\n+         * Warn about issues related to migration of JDK classes.\n+         *\/\n+        MIGRATION(\"migration\"),\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Lint.java","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -224,0 +224,4 @@\n+javac.opt.Xlint.desc.initialization=\\\n+    Warn about code in identity classes that wouldn''t be allowed in early\\n\\\n+\\                         construction due to a this dependency.\n+\n@@ -230,0 +234,3 @@\n+javac.opt.Xlint.desc.migration=\\\n+    Warn about issues related to migration of JDK classes.\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/javac.properties","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -168,0 +168,2 @@\n+ * <tr><th scope=\"row\">{@code initialization}       <td>code in identity classes that wouldn't be allowed in early\n+ *                                                      construction due to a {@code this} dependency.\n@@ -261,0 +263,1 @@\n+        jdk.jdeps,\n@@ -270,0 +273,1 @@\n+        jdk.jdeps,\n","filename":"src\/jdk.compiler\/share\/classes\/module-info.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -604,0 +604,3 @@\n+    -   `initialization`: Warns about code in identity classes that wouldn't be\n+        allowed in early construction due to a `this` dependency.\n+\n","filename":"src\/jdk.compiler\/share\/man\/javac.md","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import compiler.valhalla.inlinetypes.InlineTypeIRNode;\n@@ -90,1 +91,1 @@\n-    private static final String PREFIX = \"_#\";\n+    public static final String PREFIX = \"_#\";\n@@ -153,0 +154,6 @@\n+    \/\/ Valhalla: Make sure that all Valhalla specific IR nodes are also properly initialized. Doing it here also\n+    \/\/           ensures that the Flag VM is able to pick up the correct compile phases.\n+    static {\n+        InlineTypeIRNode.forceStaticInitialization();\n+    }\n+\n@@ -384,2 +391,6 @@\n-        String regex = START + \"Allocate\\\\b\" + MID + \"allocationKlass:.*\\\\b\" + IS_REPLACED + \"\\\\s.*\" + END;\n-        macroNodes(ALLOC_OF, regex);\n+        allocateOfNodes(ALLOC_OF, IS_REPLACED);\n+    }\n+\n+    public static void allocateOfNodes(String irNodePlaceholder, String allocatee) {\n+        String regex = START + \"Allocate\\\\b\" + MID + \"allocationKlass:.*\\\\b\" + allocatee + \"\\\\s.*\" + END;\n+        macroNodes(irNodePlaceholder, regex);\n@@ -396,0 +407,4 @@\n+        allocateArrayOfNodes(ALLOC_ARRAY_OF, IS_REPLACED);\n+    }\n+\n+    public static void allocateArrayOfNodes(String irNodePlaceholder, String allocatee) {\n@@ -414,1 +429,1 @@\n-        String name_part = \"\\\\[+.(\" + partial_name_prefix + \")?\" + IS_REPLACED + \";\";\n+        String name_part = \"\\\\[+.(\" + partial_name_prefix + \")?\" + allocatee + \";\";\n@@ -416,1 +431,1 @@\n-        macroNodes(ALLOC_ARRAY_OF, regex);\n+        macroNodes(irNodePlaceholder, regex);\n@@ -481,1 +496,1 @@\n-        callOfNodes(CALL_OF, \"Call.*\");\n+        callOfNodes(CALL_OF, \"Call.*\", IS_REPLACED + \" \" );\n@@ -486,1 +501,6 @@\n-        callOfNodes(CALL_OF_METHOD, \"Call.*Java\");\n+        callOfNodes(CALL_OF_METHOD, \"Call.*Java\", IS_REPLACED + \" \");\n+    }\n+\n+    public static final String STATIC_CALL = PREFIX + \"STATIC_CALL\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(STATIC_CALL, \"CallStaticJava\");\n@@ -491,1 +511,19 @@\n-        callOfNodes(STATIC_CALL_OF_METHOD, \"CallStaticJava\");\n+        staticCallOfMethodNodes(STATIC_CALL_OF_METHOD, IS_REPLACED + \" \");\n+    }\n+\n+    public static void staticCallOfMethodNodes(String irNodePlaceholder, String calleeRegex) {\n+        callOfNodes(irNodePlaceholder, \"CallStaticJava\", calleeRegex);\n+    }\n+\n+    public static final String CALL_LEAF_NO_FP = PREFIX + \"CALL_LEAF_NO_FP\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(CALL_LEAF_NO_FP, \"CallLeafNoFP\");\n+    }\n+\n+    public static final String CALL_LEAF_NO_FP_OF_METHOD = COMPOSITE_PREFIX + \"CALL_LEAF_NO_FP_OF_METHOD\" + POSTFIX;\n+    static {\n+        callLeafNoFpOfMethodNodes(CALL_LEAF_NO_FP_OF_METHOD, IS_REPLACED);\n+    }\n+\n+    public static void callLeafNoFpOfMethodNodes(String irNodePlaceholder, String calleeRegex) {\n+        callOfNodes(irNodePlaceholder, \"CallLeafNoFP\", calleeRegex);\n@@ -588,0 +626,5 @@\n+    public static final String CMP_N = PREFIX + \"CMP_N\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(CMP_N, \"CmpN\");\n+    }\n+\n@@ -742,1 +785,1 @@\n-        callOfNodes(DYNAMIC_CALL_OF_METHOD, \"CallDynamicJava\");\n+        callOfNodes(DYNAMIC_CALL_OF_METHOD, \"CallDynamicJava\", IS_REPLACED);\n@@ -878,0 +921,5 @@\n+    public static final String INLINE_TYPE = PREFIX + \"INLINE_TYPE\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(INLINE_TYPE, \"InlineType\");\n+    }\n+\n@@ -924,1 +972,5 @@\n-        loadOfNodes(LOAD_OF_CLASS, \"Load(B|UB|S|US|I|L|F|D|P|N)\");\n+        anyLoadOfNodes(LOAD_OF_CLASS, IS_REPLACED);\n+    }\n+\n+    public static void anyLoadOfNodes(String irNodePlaceholder, String fieldHolder) {\n+        loadOfNodes(irNodePlaceholder, \"Load(B|UB|S|US|I|L|F|D|P|N)\", fieldHolder);\n@@ -934,1 +986,1 @@\n-        loadOfNodes(LOAD_B_OF_CLASS, \"LoadB\");\n+        loadOfNodes(LOAD_B_OF_CLASS, \"LoadB\", IS_REPLACED);\n@@ -944,1 +996,1 @@\n-        loadOfNodes(LOAD_D_OF_CLASS, \"LoadD\");\n+        loadOfNodes(LOAD_D_OF_CLASS, \"LoadD\", IS_REPLACED);\n@@ -954,1 +1006,1 @@\n-        loadOfNodes(LOAD_F_OF_CLASS, \"LoadF\");\n+        loadOfNodes(LOAD_F_OF_CLASS, \"LoadF\", IS_REPLACED);\n@@ -964,1 +1016,1 @@\n-        loadOfNodes(LOAD_I_OF_CLASS, \"LoadI\");\n+        loadOfNodes(LOAD_I_OF_CLASS, \"LoadI\", IS_REPLACED);\n@@ -989,1 +1041,1 @@\n-        loadOfNodes(LOAD_L_OF_CLASS, \"LoadL\");\n+        loadOfNodes(LOAD_L_OF_CLASS, \"LoadL\", IS_REPLACED);\n@@ -999,1 +1051,1 @@\n-        loadOfNodes(LOAD_N_OF_CLASS, \"LoadN\");\n+        loadOfNodes(LOAD_N_OF_CLASS, \"LoadN\", IS_REPLACED);\n@@ -1015,1 +1067,1 @@\n-        loadOfNodes(LOAD_P_OF_CLASS, \"LoadP\");\n+        loadOfNodes(LOAD_P_OF_CLASS, \"LoadP\", IS_REPLACED);\n@@ -1025,1 +1077,1 @@\n-        loadOfNodes(LOAD_S_OF_CLASS, \"LoadS\");\n+        loadOfNodes(LOAD_S_OF_CLASS, \"LoadS\", IS_REPLACED);\n@@ -1035,1 +1087,1 @@\n-        loadOfNodes(LOAD_UB_OF_CLASS, \"LoadUB\");\n+        loadOfNodes(LOAD_UB_OF_CLASS, \"LoadUB\", IS_REPLACED);\n@@ -1045,1 +1097,1 @@\n-        loadOfNodes(LOAD_US_OF_CLASS, \"LoadUS\");\n+        loadOfNodes(LOAD_US_OF_CLASS, \"LoadUS\", IS_REPLACED);\n@@ -1949,1 +2001,1 @@\n-        storeOfNodes(STORE_B_OF_CLASS, \"StoreB\");\n+        storeOfNodes(STORE_B_OF_CLASS, \"StoreB\", IS_REPLACED);\n@@ -1959,1 +2011,1 @@\n-        storeOfNodes(STORE_C_OF_CLASS, \"StoreC\");\n+        storeOfNodes(STORE_C_OF_CLASS, \"StoreC\", IS_REPLACED);\n@@ -1969,1 +2021,1 @@\n-        storeOfNodes(STORE_D_OF_CLASS, \"StoreD\");\n+        storeOfNodes(STORE_D_OF_CLASS, \"StoreD\", IS_REPLACED);\n@@ -1979,1 +2031,1 @@\n-        storeOfNodes(STORE_F_OF_CLASS, \"StoreF\");\n+        storeOfNodes(STORE_F_OF_CLASS, \"StoreF\", IS_REPLACED);\n@@ -1989,1 +2041,1 @@\n-        storeOfNodes(STORE_I_OF_CLASS, \"StoreI\");\n+        storeOfNodes(STORE_I_OF_CLASS, \"StoreI\", IS_REPLACED);\n@@ -1999,1 +2051,1 @@\n-        storeOfNodes(STORE_L_OF_CLASS, \"StoreL\");\n+        storeOfNodes(STORE_L_OF_CLASS, \"StoreL\", IS_REPLACED);\n@@ -2009,1 +2061,1 @@\n-        storeOfNodes(STORE_N_OF_CLASS, \"StoreN\");\n+        storeOfNodes(STORE_N_OF_CLASS, \"StoreN\", IS_REPLACED);\n@@ -2014,1 +2066,5 @@\n-        storeOfNodes(STORE_OF_CLASS, \"Store(B|C|S|I|L|F|D|P|N)\");\n+        anyStoreOfNodes(STORE_OF_CLASS, IS_REPLACED);\n+    }\n+\n+    public static void anyStoreOfNodes(String irNodePlaceholder, String fieldHolder) {\n+        storeOfNodes(irNodePlaceholder, \"Store(B|C|S|I|L|F|D|P|N)\", fieldHolder);\n@@ -2030,1 +2086,1 @@\n-        storeOfNodes(STORE_P_OF_CLASS, \"StoreP\");\n+        storeOfNodes(STORE_P_OF_CLASS, \"StoreP\", IS_REPLACED);\n@@ -2120,1 +2176,2 @@\n-        beforeMatchingNameRegex(SUBTYPE_CHECK, \"SubTypeCheck\");\n+        String regex = START + \"SubTypeCheck\" + MID + END;\n+        macroNodes(SUBTYPE_CHECK, regex);\n@@ -3042,1 +3099,1 @@\n-    private static void beforeMatching(String irNodePlaceholder, String regex) {\n+    public static void beforeMatching(String irNodePlaceholder, String regex) {\n@@ -3078,2 +3135,2 @@\n-    private static void callOfNodes(String irNodePlaceholder, String callRegex) {\n-        String regex = START + callRegex + MID + IS_REPLACED + \" \" +  END;\n+    private static void callOfNodes(String irNodePlaceholder, String callRegex, String calleeRegex) {\n+        String regex = START + callRegex + MID + calleeRegex + END;\n@@ -3087,1 +3144,1 @@\n-    private static void optoOnly(String irNodePlaceholder, String regex) {\n+    public static void optoOnly(String irNodePlaceholder, String regex) {\n@@ -3172,2 +3229,2 @@\n-    private static void loadOfNodes(String irNodePlaceholder, String irNodeRegex) {\n-        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + IS_REPLACED + LOAD_STORE_SUFFIX + END;\n+    private static void loadOfNodes(String irNodePlaceholder, String irNodeRegex, String loadee) {\n+        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + loadee + LOAD_STORE_SUFFIX + END;\n@@ -3177,2 +3234,2 @@\n-    private static void storeOfNodes(String irNodePlaceholder, String irNodeRegex) {\n-        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + IS_REPLACED + LOAD_STORE_SUFFIX + END;\n+    private static void storeOfNodes(String irNodePlaceholder, String irNodeRegex, String storee) {\n+        String regex = START + irNodeRegex + MID + LOAD_STORE_PREFIX + storee + LOAD_STORE_SUFFIX + END;\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":95,"deletions":38,"binary":false,"changes":133,"status":"modified"},{"patch":"@@ -532,0 +532,5 @@\n+java\/lang\/ModuleLayer\/LayerControllerTest.java                  8337048 generic-all\n+java\/lang\/ModuleLayer\/BasicLayerTest.java                       8337048 generic-all\n+\n+java\/lang\/Thread\/virtual\/stress\/Skynet.java#default             8342977 generic-all\n+\n@@ -549,0 +554,1 @@\n+com\/sun\/management\/HotSpotDiagnosticMXBean\/DumpThreadsWithEliminatedLock.java 8360599 generic-all\n@@ -573,0 +579,2 @@\n+java\/net\/CookieHandler\/B6644726.java                            8365811 generic-all\n+\n@@ -724,0 +732,4 @@\n+com\/sun\/jdi\/cds\/CDSBreakpointTest.java                          8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSDeleteAllBkptsTest.java                      8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSFieldWatchpoints.java                        8304168 generic-all\n+\n@@ -732,0 +744,1 @@\n+java\/util\/logging\/LoggingDeadlock2.java       8368801 generic-all\n@@ -818,0 +831,1 @@\n+\n@@ -823,0 +837,13 @@\n+\n+############################################################################\n+\n+# valhalla\n+java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessValue.java 8367346 generic-all\n+\n+jdk\/classfile\/AccessFlagsTest.java 8366270 generic-all\n+jdk\/jfr\/event\/runtime\/TestClassLoaderStatsEvent.java 8366820 generic-all\n+\n+sun\/tools\/jhsdb\/BasicLauncherTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTestWithActiveProcess.java 8366806 generic-all\n+sun\/tools\/jhsdb\/JShellHeapDumpTest.java 8366806 generic-all\n","filename":"test\/jdk\/ProblemList.txt","additions":27,"deletions":0,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -64,1 +64,0 @@\n-\n@@ -70,1 +69,0 @@\n-\n","filename":"test\/langtools\/ProblemList.txt","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"}]}