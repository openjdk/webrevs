{"files":[{"patch":"@@ -70,0 +70,3 @@\n+define_pd_global(bool, InlineTypePassFieldsAsArgs, true);\n+define_pd_global(bool, InlineTypeReturnedAsFields, true);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/globals_aarch64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -50,0 +51,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -51,0 +53,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -56,0 +59,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -59,0 +63,1 @@\n+#include \"vmreg_aarch64.inline.hpp\"\n@@ -2069,1 +2074,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -2102,1 +2111,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -2200,0 +2213,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2245,0 +2262,88 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  assert_different_registers(markword, rscratch2);\n+  mov(rscratch2, markWord::inline_type_mask_in_place);\n+  andr(markword, markword, rscratch2);\n+  mov(rscratch2, markWord::inline_type_pattern);\n+  cmp(markword, rscratch2);\n+  br(Assembler::EQ, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type, bool can_be_null) {\n+  assert_different_registers(tmp, rscratch1);\n+  if (can_be_null) {\n+    cbz(object, not_inline_type);\n+  }\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  ldr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  mov(rscratch1, is_inline_type_mask);\n+  andr(tmp, tmp, rscratch1);\n+  cmp(tmp, rscratch1);\n+  br(Assembler::NE, not_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::is_null_free_inline_type_shift, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbz(flags, ResolvedFieldEntry::is_null_free_inline_type_shift, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_flat(Register flags, Register temp_reg, Label& is_flat) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::is_flat_shift, is_flat);\n+}\n+\n+void MacroAssembler::test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker) {\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ResolvedFieldEntry::has_null_marker_shift, has_null_marker);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  ldr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  tst(temp_reg, markWord::unlocked_value);\n+  br(Assembler::NE, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  load_prototype_header(temp_reg, oop);\n+\n+  bind(test_mark_word);\n+  andr(temp_reg, temp_reg, test_bit);\n+  if (jmp_set) {\n+    cbnz(temp_reg, jmp_label);\n+  } else {\n+    cbz(temp_reg, jmp_label);\n+  }\n+}\n+\n+void MacroAssembler::test_flat_array_oop(Register oop, Register temp_reg, Label& is_flat_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flat_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flat_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+}\n+\n+void MacroAssembler::test_flat_array_layout(Register lh, Label& is_flat_array) {\n+  tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  br(Assembler::NE, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_layout(Register lh, Label& is_non_flat_array) {\n+  tst(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  br(Assembler::EQ, is_non_flat_array);\n+}\n+\n@@ -5036,0 +5141,10 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(dst, src);\n+  } else if (UseCompressedClassPointers) {\n+    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -5146,0 +5261,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src) {\n+  load_klass(dst, src);\n+  ldr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -5547,0 +5667,40 @@\n+void MacroAssembler::flat_field_copy(DecoratorSet decorators, Register src, Register dst,\n+                                     Register inline_layout_info) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->flat_field_copy(this, decorators, src, dst, inline_layout_info);\n+}\n+\n+void MacroAssembler::payload_offset(Register inline_klass, Register offset) {\n+  ldr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  ldrw(offset, Address(offset, InlineKlass::payload_offset_offset()));\n+}\n+\n+void MacroAssembler::payload_address(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->payload_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  payload_offset(inline_klass, offset);\n+  if (data == oop) {\n+    add(data, data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rscratch1, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  ldrw(rscratch1, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  lsr(rscratch1, rscratch1, Klass::_lh_log2_element_size_shift);\n+  andr(rscratch1, rscratch1, Klass::_lh_log2_element_size_mask);\n+  lslv(index, index, rscratch1);\n+\n+  add(data, array, index);\n+  add(data, data, arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT));\n+}\n+\n@@ -5622,0 +5782,102 @@\n+\/\/ Object \/ value buffer allocation...\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == r0, \"needs to be r0\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  ldrw(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it is malformed in some way\n+  tst(layout_size, Klass::_lh_instance_slow_path_bit);\n+  br(Assembler::NE, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  if (UseTLAB) {\n+    push(klass);\n+    tlab_allocate(new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      b(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      b(initialize_object);\n+    }\n+\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      int header_size = oopDesc::header_size() * HeapWordSize;\n+      assert(is_aligned(header_size, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n+      subs(layout_size, layout_size, header_size);\n+      br(Assembler::EQ, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      tst(layout_size, 7);\n+      br(Assembler::EQ, L);\n+      stop(\"object size is not multiple of 8 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      lsr(layout_size, layout_size, LogBytesPerLong);\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        Register base = t2;\n+\n+        bind(loop);\n+        add(rscratch1, new_obj, layout_size, Assembler::LSL, LogBytesPerLong);\n+        str(zr, Address(rscratch1, header_size - 1*oopSize));\n+        subs(layout_size, layout_size, 1);\n+        br(Assembler::NE, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    if (UseCompactObjectHeaders || EnableValhalla) {\n+      ldr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+      str(mark_word, Address(new_obj, oopDesc::mark_offset_in_bytes()));\n+    } else {\n+      mov(mark_word, (intptr_t)markWord::prototype().value());\n+      str(mark_word, Address(new_obj, oopDesc::mark_offset_in_bytes()));\n+    }\n+    if (!UseCompactObjectHeaders) {\n+      store_klass_gap(new_obj, zr);  \/\/ zero klass gap for compressed oops\n+      mov(t2, klass);                \/\/ preserve klass\n+      store_klass(new_obj, t2);      \/\/ src klass reg is potentially compressed\n+    }\n+    b(done);\n+  }\n+\n+  if (UseTLAB) {\n+    bind(slow_case);\n+    pop(klass);\n+  }\n+  bind(slow_case_no_pop);\n+  b(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -5693,0 +5955,20 @@\n+void MacroAssembler::get_inline_type_field_klass(Register holder_klass, Register index, Register inline_klass) {\n+  inline_layout_info(holder_klass, index, inline_klass);\n+  ldr(inline_klass, Address(inline_klass, InlineLayoutInfo::klass_offset()));\n+}\n+\n+void MacroAssembler::inline_layout_info(Register holder_klass, Register index, Register layout_info) {\n+  assert_different_registers(holder_klass, index, layout_info);\n+  InlineLayoutInfo array[2];\n+  int size = (char*)&array[1] - (char*)&array[0]; \/\/ computing size of array elements\n+  if (is_power_of_2(size)) {\n+    lsl(index, index, log2i_exact(size)); \/\/ Scale index by power of 2\n+  } else {\n+    mov(layout_info, size);\n+    mul(index, index, layout_info); \/\/ Scale the index to be the entry index * array_element_size\n+  }\n+  ldr(layout_info, Address(holder_klass, InstanceKlass::inline_layout_info_array_offset()));\n+  add(layout_info, layout_info, Array<InlineLayoutInfo>::base_offset_in_bytes());\n+  lea(layout_info, Address(layout_info, index));\n+}\n+\n@@ -5818,0 +6100,61 @@\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  if (needs_stack_repair) {\n+    \/\/ Remove the extension of the caller's frame used for inline type unpacking\n+    \/\/\n+    \/\/ Right now the stack looks like this:\n+    \/\/\n+    \/\/ | Arguments from caller     |\n+    \/\/ |---------------------------|  <-- caller's SP\n+    \/\/ | Saved LR #1               |\n+    \/\/ | Saved FP #1               |\n+    \/\/ |---------------------------|\n+    \/\/ | Extension space for       |\n+    \/\/ |   inline arg (un)packing  |\n+    \/\/ |---------------------------|  <-- start of this method's frame\n+    \/\/ | Saved LR #2               |\n+    \/\/ | Saved FP #2               |\n+    \/\/ |---------------------------|  <-- FP\n+    \/\/ | sp_inc                    |\n+    \/\/ | method locals             |\n+    \/\/ |---------------------------|  <-- SP\n+    \/\/\n+    \/\/ There are two copies of FP and LR on the stack. They will be identical at\n+    \/\/ first, but that can change.\n+    \/\/ If the caller has been deoptimized, LR #1 will be patched to point at the\n+    \/\/ deopt blob, and LR #2 will still point into the old method.\n+    \/\/ If the saved FP (x29) was not used as the frame pointer, but to store an\n+    \/\/ oop, the GC will be aware only of FP #2 as the spilled location of x29 and\n+    \/\/ will fix only this one.\n+    \/\/\n+    \/\/ When restoring, one must then load FP #2 into x29, and LR #1 into x30,\n+    \/\/ while keeping in mind that from the scalarized entry point, there will be\n+    \/\/ only one copy of each.\n+    \/\/\n+    \/\/ The sp_inc stack slot holds the total size of the frame including the\n+    \/\/ extension space minus two words for the saved FP and LR. That is how to\n+    \/\/ find LR #1. FP #2 is always located just after sp_inc.\n+\n+    int sp_inc_offset = initial_framesize - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+\n+    ldp(rscratch1, rfp, Address(sp, sp_inc_offset));\n+    add(sp, sp, rscratch1);\n+    ldr(lr, Address(sp, wordSize));\n+    add(sp, sp, 2 * wordSize);\n+  } else {\n+    remove_frame(initial_framesize);\n+  }\n+}\n+\n+void MacroAssembler::save_stack_increment(int sp_inc, int frame_size) {\n+  int real_frame_size = frame_size + sp_inc;\n+  assert(sp_inc == 0 || sp_inc > 2*wordSize, \"invalid sp_inc value\");\n+  assert(real_frame_size >= 2*wordSize, \"frame size must include FP\/LR space\");\n+  assert((real_frame_size & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+\n+  int sp_inc_offset = frame_size - 3 * wordSize;  \/\/ Immediately below saved LR and FP\n+\n+  \/\/ Subtract two words for the saved FP and LR as these will be popped\n+  \/\/ separately. See remove_frame above.\n+  mov(rscratch1, real_frame_size - 2*wordSize);\n+  str(rscratch1, Address(sp, sp_inc_offset));\n+}\n@@ -6731,0 +7074,448 @@\n+#ifdef COMPILER2\n+\/\/ C2 compiled method's prolog code\n+\/\/ Moved here from aarch64.ad to support Valhalla code belows\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+\n+    mov_metadata(rscratch2, C->method()->holder()->constant_encoding());\n+    clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);\n+    far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+    bind(L_skip_barrier);\n+  }\n+\n+  if (C->max_vector_size() > 0) {\n+    reinitialize_ptrue();\n+  }\n+\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  if (C->output()->need_stack_bang(bangsize))\n+    generate_stack_overflow_check(bangsize);\n+\n+  \/\/ n.b. frame size includes space for return pc and rfp\n+  const long framesize = C->output()->frame_size_in_bytes();\n+  build_frame(framesize);\n+\n+  if (C->needs_stack_repair()) {\n+    save_stack_increment(sp_inc, framesize);\n+  }\n+\n+  if (VerifyStackAtCalls) {\n+    Unimplemented();\n+  }\n+}\n+#endif \/\/ COMPILER2\n+\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  tbz(r0, 0, skip);\n+  int call_offset = -1;\n+\n+  \/\/ Be careful not to clobber r1-7 which hold returned fields\n+  \/\/ Also do not use callee-saved registers as these may be live in the interpreter\n+  Register tmp1 = r13, tmp2 = r14, klass = r15, r0_preserved = r12;\n+\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != nullptr, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(r0_preserved, r0); \/\/ save r0 for slow_case since *_allocate may corrupt it when allocation failed\n+\n+  if (vk != nullptr) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(klass, (intptr_t)vk->get_InlineKlass());\n+    jint lh = vk->layout_helper();\n+    assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB && !Klass::layout_helper_needs_slow_path(lh)) {\n+      tlab_allocate(r0, noreg, lh, tmp1, tmp2, slow_case);\n+    } else {\n+      b(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)\n+    andr(klass, r0, -2);\n+    if (UseTLAB) {\n+      ldrw(tmp2, Address(klass, Klass::layout_helper_offset()));\n+      tst(tmp2, Klass::_lh_instance_slow_path_bit);\n+      br(Assembler::NE, slow_case);\n+      tlab_allocate(r0, tmp2, 0, tmp1, tmp2, slow_case);\n+    } else {\n+      b(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = r0;\n+    if (UseCompactObjectHeaders) {\n+      ldr(rscratch1, Address(klass, Klass::prototype_header_offset()));\n+      str(rscratch1, Address(buffer_obj, oopDesc::mark_offset_in_bytes()));\n+    } else {\n+      mov(rscratch1, (intptr_t)markWord::inline_type_prototype().value());\n+      str(rscratch1, Address(buffer_obj, oopDesc::mark_offset_in_bytes()));\n+      store_klass_gap(buffer_obj, zr);\n+      if (vk == nullptr) {\n+        \/\/ store_klass corrupts klass, so save it for later use (interpreter case only).\n+        mov(tmp1, klass);\n+      }\n+      store_klass(buffer_obj, klass);\n+    }\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != nullptr) {\n+      far_call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      \/\/ tmp1 holds klass preserved above\n+      ldr(tmp1, Address(tmp1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      ldr(tmp1, Address(tmp1, InlineKlass::pack_handler_offset()));\n+      blr(tmp1);\n+    }\n+\n+    membar(Assembler::StoreStore);\n+    b(skip);\n+  } else {\n+    \/\/ Must have already branched to slow_case above.\n+    DEBUG_ONLY(should_not_reach_here());\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(r0, r0_preserved);\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    far_call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+  membar(Assembler::StoreStore);\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_Register() && to->is_Register()) {\n+          mov(to->as_Register(), from->as_Register());\n+        } else if (from->is_FloatRegister() && to->is_FloatRegister()) {\n+          fmovd(to->as_FloatRegister(), from->as_FloatRegister());\n+        } else {\n+          ShouldNotReachHere();\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        Address to_addr = Address(sp, st_off);\n+        if (from->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             strd(from->as_FloatRegister(), to_addr);\n+          } else {\n+             assert(bt == T_FLOAT, \"must be float\");\n+             strs(from->as_FloatRegister(), to_addr);\n+          }\n+        } else {\n+          str(from->as_Register(), to_addr);\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size);\n+      if (to->is_reg()) {\n+        if (to->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+            ldrd(to->as_FloatRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            ldrs(to->as_FloatRegister(), from_addr);\n+          }\n+        } else {\n+          ldr(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(rscratch1, from_addr);\n+        str(rscratch1, Address(sp, st_off));\n+      }\n+    }\n+  }\n+\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  int sp_inc = args_on_stack * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  assert(sp_inc > 0, \"sanity\");\n+\n+  \/\/ Save a copy of the FP and LR here for deoptimization patching and frame walking\n+  stp(rfp, lr, Address(pre(sp, -2 * wordSize)));\n+\n+  \/\/ Adjust the stack pointer. This will be repaired on return by MacroAssembler::remove_frame\n+  if (sp_inc < (1 << 9)) {\n+    sub(sp, sp, sp_inc);   \/\/ Fits in an immediate\n+  } else {\n+    mov(rscratch1, sp_inc);\n+    sub(sp, sp, rscratch1);\n+  }\n+\n+  return sp_inc + 2 * wordSize;  \/\/ Account for the FP\/LR space\n+}\n+\n+\/\/ Read all fields from an inline type oop and store the values in registers\/stack slots\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  \/\/ TODO 8366717 We need to make sure that r14 (and potentially other long-life regs) are kept live in slowpath runtime calls in GC barriers\n+  Register tmp1 = r10;\n+  Register tmp2 = r11;\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldr(tmp1, Address(sp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        cbz(fromReg, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        mov(tmp2, 1);\n+        str(tmp2, Address(sp, st_off));\n+      } else {\n+        mov(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_FloatRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr, rscratch1, rscratch2);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        str(dst, Address(sp, st_off));\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      ldrd(toReg->as_FloatRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      ldrs(toReg->as_FloatRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      b(L_notNull);\n+      bind(L_null);\n+      \/\/ Set null marker to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size;\n+            str(zr, Address(sp, st_off));\n+          } else {\n+            mov(toReg->as_Register(), zr);\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  \/\/ TODO 8366717 This is probably okay but looks fishy because stream is reset in the \"Set null marker to zero\" case just above. Same on x64.\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+\/\/ Pack fields back into an inline type oop\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_METADATA, \"should be at delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ The GC barrier expanded by store_heap_oop below may call into the\n+  \/\/ runtime so use callee-saved registers for any values that need to be\n+  \/\/ preserved. The GC barrier assembler should take care of saving the\n+  \/\/ Java argument registers.\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r21;\n+  Register from_reg_tmp = r22;\n+  Register tmp1 = r14;\n+  Register tmp2 = r13;\n+  Register tmp3 = r12;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index), tmp1, tmp2);\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        ldrb(tmp2, Address(sp, ld_off));\n+        cbnz(tmp2, L_notNull);\n+      } else {\n+        cbnz(fromReg->as_Register(), L_notNull);\n+      }\n+      mov(val_obj, 0);\n+      b(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+    if (!fromReg->is_FloatRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size;\n+        load_sized_value(src, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      strd(fromReg->as_FloatRegister(), dst);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      strs(fromReg->as_FloatRegister(), dst);\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return (reg->is_FloatRegister()) ? v8->as_VMReg() : r14->as_VMReg();\n+}\n+\n@@ -7139,0 +7930,4 @@\n+  if (EnableValhalla) {\n+    \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+    andr(mark, mark, ~((int) markWord::inline_type_bit_in_place));\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":797,"deletions":2,"binary":false,"changes":799,"status":"modified"},{"patch":"@@ -52,1 +52,21 @@\n-void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void C2_MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+    Register klass = rscratch1;\n+\n+    mov_metadata(klass, C->method()->holder()->constant_encoding());\n+    clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+\n+    bind(L_skip_barrier);\n+  }\n+\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  bool fp_mode_24b = false;\n+  int stack_bang_size = C->output()->need_stack_bang(bangsize) ? bangsize : 0;\n+\n@@ -97,0 +117,6 @@\n+  if (C->needs_stack_repair()) {\n+    \/\/ Save stack increment just below the saved rbp (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    movptr(Address(rsp, framesize - wordSize), sp_inc + framesize + wordSize);\n+  }\n+\n@@ -115,0 +141,1 @@\n+}\n@@ -116,15 +143,13 @@\n-  if (!is_stub) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n-    Label dummy_slow_path;\n-    Label dummy_continuation;\n-    Label* slow_path = &dummy_slow_path;\n-    Label* continuation = &dummy_continuation;\n-    if (!Compile::current()->output()->in_scratch_emit_size()) {\n-      \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n-      C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n-      Compile::current()->output()->add_stub(stub);\n-      slow_path = &stub->entry();\n-      continuation = &stub->continuation();\n-    }\n-    bs->nmethod_entry_barrier(this, slow_path, continuation);\n+void C2_MacroAssembler::entry_barrier() {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n+  Label dummy_slow_path;\n+  Label dummy_continuation;\n+  Label* slow_path = &dummy_slow_path;\n+  Label* continuation = &dummy_continuation;\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+    C2EntryBarrierStub* stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n+    Compile::current()->output()->add_stub(stub);\n+    slow_path = &stub->entry();\n+    continuation = &stub->continuation();\n@@ -132,0 +157,1 @@\n+  bs->nmethod_entry_barrier(this, slow_path, continuation);\n@@ -269,0 +295,4 @@\n+    if (EnableValhalla) {\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      andptr(tmpReg, ~((int) markWord::inline_type_bit_in_place));\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":46,"deletions":16,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);\n+  void verified_entry(Compile* C, int sp_inc = 0);\n@@ -34,0 +34,1 @@\n+  void entry_barrier();\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -46,0 +47,1 @@\n+#include \"oops\/resolvedFieldEntry.hpp\"\n@@ -56,0 +58,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -59,0 +62,4 @@\n+#include \"vmreg_x86.inline.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/output.hpp\"\n+#endif\n@@ -1290,0 +1297,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2343,0 +2354,107 @@\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  andptr(markword, markWord::inline_type_mask_in_place);\n+  cmpptr(markword, markWord::inline_type_pattern);\n+  jcc(Assembler::equal, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type, bool can_be_null) {\n+  if (can_be_null) {\n+    testptr(object, object);\n+    jcc(Assembler::zero, not_inline_type);\n+  }\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  movptr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  andptr(tmp, is_inline_type_mask);\n+  cmpptr(tmp, is_inline_type_mask);\n+  jcc(Assembler::notEqual, not_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_null_free_inline_type(Register flags, Register temp_reg, Label& is_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::notEqual, is_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_null_free_inline_type(Register flags, Register temp_reg, Label& not_null_free_inline_type) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_null_free_inline_type_shift);\n+  jcc(Assembler::equal, not_null_free_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_flat(Register flags, Register temp_reg, Label& is_flat) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::is_flat_shift);\n+  jcc(Assembler::notEqual, is_flat);\n+}\n+\n+void MacroAssembler::test_field_has_null_marker(Register flags, Register temp_reg, Label& has_null_marker) {\n+  movl(temp_reg, flags);\n+  testl(temp_reg, 1 << ResolvedFieldEntry::has_null_marker_shift);\n+  jcc(Assembler::notEqual, has_null_marker);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  movptr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  testl(temp_reg, markWord::unlocked_value);\n+  jccb(Assembler::notZero, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  push(rscratch1);\n+  load_prototype_header(temp_reg, oop, rscratch1);\n+  pop(rscratch1);\n+\n+  bind(test_mark_word);\n+  testl(temp_reg, test_bit);\n+  jcc((jmp_set) ? Assembler::notZero : Assembler::zero, jmp_label);\n+}\n+\n+void MacroAssembler::test_flat_array_oop(Register oop, Register temp_reg,\n+                                         Label& is_flat_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flat_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_flat_array_layout(temp_reg, is_flat_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_non_flat_array_oop(Register oop, Register temp_reg,\n+                                             Label& is_non_flat_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flat_array);\n+#else\n+  load_klass(temp_reg, oop, noreg);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_flat_array_layout(temp_reg, is_non_flat_array);\n+#endif\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, true, is_null_free_array);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+#ifdef _LP64\n+  test_oop_prototype_bit(oop, temp_reg, markWord::null_free_array_bit_in_place, false, is_non_null_free_array);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+\n+void MacroAssembler::test_flat_array_layout(Register lh, Label& is_flat_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::notZero, is_flat_array);\n+}\n+\n+void MacroAssembler::test_non_flat_array_layout(Register lh, Label& is_non_flat_array) {\n+  testl(lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  jcc(Assembler::zero, is_non_flat_array);\n+}\n+\n@@ -3431,0 +3549,118 @@\n+\/\/ Object \/ value buffer allocation...\n+\/\/\n+\/\/ Kills klass and rsi on LP64\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == rax, \"needs to be rax\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  movl(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it is malformed in some way\n+  testl(layout_size, Klass::_lh_instance_slow_path_bit);\n+  jcc(Assembler::notZero, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path.\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+\n+  push(klass);\n+  if (UseTLAB) {\n+    tlab_allocate(new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      jmp(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      jmp(initialize_object);\n+    }\n+  } else {\n+    jmp(slow_case);\n+  }\n+\n+  \/\/ If UseTLAB is true, the object is created above and there is an initialize need.\n+  \/\/ Otherwise, skip and go to the slow path.\n+  if (UseTLAB) {\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      if (UseCompactObjectHeaders) {\n+        assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n+        decrement(layout_size, oopDesc::base_offset_in_bytes());\n+      } else {\n+        decrement(layout_size, sizeof(oopDesc));\n+      }\n+      jcc(Assembler::zero, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+      Register zero = klass;\n+      xorl(zero, zero);    \/\/ use zero reg to clear memory (shorter code)\n+      shrl(layout_size, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      \/\/ Ignore partial flag stall after shrl() since it is debug VM\n+      jcc(Assembler::carryClear, L);\n+      stop(\"object size is not multiple of 2 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        bind(loop);\n+        int header_size_bytes = oopDesc::header_size() * HeapWordSize;\n+        assert(is_aligned(header_size_bytes, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n+        movptr(Address(new_obj, layout_size, Address::times_8, header_size_bytes - 1*oopSize), zero);\n+        decrement(layout_size);\n+        jcc(Assembler::notZero, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    if (UseCompactObjectHeaders || EnableValhalla) {\n+      pop(klass);\n+      Register mark_word = t2;\n+      movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+      movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+    } else {\n+     movptr(Address(new_obj, oopDesc::mark_offset_in_bytes()),\n+            (intptr_t)markWord::prototype().value()); \/\/ header\n+     pop(klass);   \/\/ get saved klass back in the register.\n+    }\n+    if (!UseCompactObjectHeaders) {\n+      xorl(rsi, rsi);                 \/\/ use zero reg to clear memory (shorter code)\n+      store_klass_gap(new_obj, rsi);  \/\/ zero klass gap for compressed oops\n+      movptr(t2, klass);         \/\/ preserve klass\n+      store_klass(new_obj, t2, rscratch1);  \/\/ src klass reg is potentially compressed\n+    }\n+    jmp(done);\n+  }\n+\n+  bind(slow_case);\n+  pop(klass);\n+  bind(slow_case_no_pop);\n+  jmp(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -3634,0 +3870,27 @@\n+void MacroAssembler::get_inline_type_field_klass(Register holder_klass, Register index, Register inline_klass) {\n+  inline_layout_info(holder_klass, index, inline_klass);\n+  movptr(inline_klass, Address(inline_klass, InlineLayoutInfo::klass_offset()));\n+}\n+\n+void MacroAssembler::inline_layout_info(Register holder_klass, Register index, Register layout_info) {\n+  movptr(layout_info, Address(holder_klass, InstanceKlass::inline_layout_info_array_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cmpptr(layout_info, 0);\n+    jcc(Assembler::notEqual, done);\n+    stop(\"inline_layout_info_array is null\");\n+    bind(done);\n+  }\n+#endif\n+\n+  InlineLayoutInfo array[2];\n+  int size = (char*)&array[1] - (char*)&array[0]; \/\/ computing size of array elements\n+  if (is_power_of_2(size)) {\n+    shll(index, log2i_exact(size)); \/\/ Scale index by power of 2\n+  } else {\n+    imull(index, index, size); \/\/ Scale the index to be the entry index * array_element_size\n+  }\n+  lea(layout_info, Address(layout_info, index, Address::times_1, Array<InlineLayoutInfo>::base_offset_in_bytes()));\n+}\n+\n@@ -4684,1 +4947,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -4743,1 +5010,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -5137,0 +5408,10 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompactObjectHeaders) {\n+    load_narrow_klass_compact(dst, src);\n+  } else if (UseCompressedClassPointers) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -5158,0 +5439,5 @@\n+void MacroAssembler::load_prototype_header(Register dst, Register src, Register tmp) {\n+  load_klass(dst, src, tmp);\n+  movptr(dst, Address(dst, Klass::prototype_header_offset()));\n+}\n+\n@@ -5223,0 +5509,40 @@\n+void MacroAssembler::flat_field_copy(DecoratorSet decorators, Register src, Register dst,\n+                                     Register inline_layout_info) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->flat_field_copy(this, decorators, src, dst, inline_layout_info);\n+}\n+\n+void MacroAssembler::payload_offset(Register inline_klass, Register offset) {\n+  movptr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  movl(offset, Address(offset, InlineKlass::payload_offset_offset()));\n+}\n+\n+void MacroAssembler::payload_addr(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->payload_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  payload_offset(inline_klass, offset);\n+  if (data == oop) {\n+    addptr(data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert(index != rcx, \"index needs to shift by rcx\");\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rcx, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  movl(rcx, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  shrl(rcx, Klass::_lh_log2_element_size_shift);\n+  andl(rcx, Klass::_lh_log2_element_size_mask);\n+  shlptr(index); \/\/ index << rcx\n+\n+  lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT)));\n+}\n+\n@@ -5582,1 +5908,1 @@\n-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, KRegister mask) {\n+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, KRegister mask) {\n@@ -5588,1 +5914,1 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_512bit);\n+    evpbroadcastq(xtmp, val, AVX_512bit);\n@@ -5590,1 +5916,3 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_256bit);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n+    vinserti128_high(xtmp, xtmp);\n@@ -5592,1 +5920,2 @@\n-    pxor(xtmp, xtmp);\n+    movdq(xtmp, val);\n+    punpcklqdq(xtmp, xtmp);\n@@ -5615,1 +5944,1 @@\n-    fill64_masked(3, base, 0, xtmp, mask, cnt, rtmp, true);\n+    fill64_masked(3, base, 0, xtmp, mask, cnt, val, true);\n@@ -5634,1 +5963,1 @@\n-    fill32_masked(3, base, 0, xtmp, mask, cnt, rtmp);\n+    fill32_masked(3, base, 0, xtmp, mask, cnt, val);\n@@ -5647,0 +5976,408 @@\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  testptr(rax, 1);\n+  jcc(Assembler::zero, skip);\n+  int call_offset = -1;\n+\n+#ifdef _LP64\n+  \/\/ The following code is similar to allocate_instance but has some slight differences,\n+  \/\/ e.g. object size is always not zero, sometimes it's constant; storing klass ptr after\n+  \/\/ allocating is not necessary if vk != nullptr, etc. allocate_instance is not aware of these.\n+  Label slow_case;\n+  \/\/ 1. Try to allocate a new buffered inline instance either from TLAB or eden space\n+  mov(rscratch1, rax); \/\/ save rax for slow_case since *_allocate may corrupt it when allocation failed\n+  if (vk != nullptr) {\n+    \/\/ Called from C1, where the return type is statically known.\n+    movptr(rbx, (intptr_t)vk->get_InlineKlass());\n+    jint lh = vk->layout_helper();\n+    assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+    if (UseTLAB && !Klass::layout_helper_needs_slow_path(lh)) {\n+      tlab_allocate(rax, noreg, lh, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  } else {\n+    \/\/ Call from interpreter. RAX contains ((the InlineKlass* of the return type) | 0x01)\n+    mov(rbx, rax);\n+    andptr(rbx, -2);\n+    if (UseTLAB) {\n+      movl(r14, Address(rbx, Klass::layout_helper_offset()));\n+      testl(r14, Klass::_lh_instance_slow_path_bit);\n+      jcc(Assembler::notZero, slow_case);\n+      tlab_allocate(rax, r14, 0, r13, r14, slow_case);\n+    } else {\n+      jmp(slow_case);\n+    }\n+  }\n+  if (UseTLAB) {\n+    \/\/ 2. Initialize buffered inline instance header\n+    Register buffer_obj = rax;\n+    if (UseCompactObjectHeaders) {\n+      Register mark_word = r13;\n+      movptr(mark_word, Address(rbx, Klass::prototype_header_offset()));\n+      movptr(Address(buffer_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+    } else {\n+      movptr(Address(buffer_obj, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::inline_type_prototype().value());\n+      xorl(r13, r13);\n+      store_klass_gap(buffer_obj, r13);\n+      if (vk == nullptr) {\n+        \/\/ store_klass corrupts rbx(klass), so save it in r13 for later use (interpreter case only).\n+        mov(r13, rbx);\n+      }\n+      store_klass(buffer_obj, rbx, rscratch1);\n+    }\n+    \/\/ 3. Initialize its fields with an inline class specific handler\n+    if (vk != nullptr) {\n+      call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      movptr(rbx, Address(r13, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      movptr(rbx, Address(rbx, InlineKlass::pack_handler_offset()));\n+      call(rbx);\n+    }\n+    jmp(skip);\n+  }\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+  mov(rax, rscratch1);\n+#endif\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[]) {\n+  assert(from->is_valid() && to->is_valid(), \"source and destination must be valid\");\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to->as_Register(), from->as_Register());\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        Address to_addr = Address(rsp, st_off);\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to_addr, from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to_addr, from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to_addr, from->as_Register());\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(rsp, from->reg2stack() * VMRegImpl::stack_slot_size + wordSize);\n+      if (to->is_reg()) {\n+        if (to->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from_addr);\n+          }\n+        } else {\n+          movq(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(r13, from_addr);\n+        movq(Address(rsp, st_off), r13);\n+      }\n+    }\n+  }\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Calculate the extra stack space required for packing or unpacking inline\n+\/\/ args and adjust the stack pointer\n+int MacroAssembler::extend_stack_for_inline_args(int args_on_stack) {\n+  \/\/ Two additional slots to account for return address\n+  int sp_inc = (args_on_stack + 2) * VMRegImpl::stack_slot_size;\n+  sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+  \/\/ Save the return address, adjust the stack (make sure it is properly\n+  \/\/ 16-byte aligned) and copy the return address to the new top of the stack.\n+  \/\/ The stack will be repaired on return (see MacroAssembler::remove_frame).\n+  assert(sp_inc > 0, \"sanity\");\n+  pop(r13);\n+  subptr(rsp, sp_inc);\n+  push(r13);\n+  return sp_inc;\n+}\n+\n+\/\/ Read all fields from an inline type buffer and store the field values in registers\/stack slots.\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+  assert(from->is_valid(), \"source must be valid\");\n+  bool progress = false;\n+#ifdef ASSERT\n+  const int start_offset = offset();\n+#endif\n+\n+  Label L_null, L_notNull;\n+  \/\/ Don't use r14 as tmp because it's used for spilling (see MacroAssembler::spill_reg_for)\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register fromReg = noreg;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n+  bool done = true;\n+  bool mark_done = true;\n+  VMReg toReg;\n+  BasicType bt;\n+  \/\/ Check if argument requires a null check\n+  bool null_check = false;\n+  VMReg nullCheckReg;\n+  while (stream.next(nullCheckReg, bt)) {\n+    if (sig->at(stream.sig_index())._offset == -1) {\n+      null_check = true;\n+      break;\n+    }\n+  }\n+  stream.reset(sig_index, to_index);\n+  while (stream.next(toReg, bt)) {\n+    assert(toReg->is_valid(), \"destination must be valid\");\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+      if (idx != from->value()) {\n+        mark_done = false;\n+      }\n+      done = false;\n+      continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n+    }\n+    assert(reg_state[idx] == reg_writable, \"must be writable\");\n+    reg_state[idx] = reg_written;\n+    progress = true;\n+\n+    if (fromReg == noreg) {\n+      if (from->is_reg()) {\n+        fromReg = from->as_Register();\n+      } else {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(tmp1, Address(rsp, st_off));\n+        fromReg = tmp1;\n+      }\n+      if (null_check) {\n+        \/\/ Nullable inline type argument, emit null check\n+        testptr(fromReg, fromReg);\n+        jcc(Assembler::zero, L_null);\n+      }\n+    }\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      assert(null_check, \"Missing null check at\");\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), 1);\n+      } else {\n+        movq(toReg->as_Register(), 1);\n+      }\n+      continue;\n+    }\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+    if (!toReg->is_XMMRegister()) {\n+      Register dst = toReg->is_stack() ? tmp2 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr);\n+      } else {\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+      }\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        movq(Address(rsp, st_off), dst);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(toReg->as_XMMRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(toReg->as_XMMRegister(), fromAddr);\n+    }\n+  }\n+  if (progress && null_check) {\n+    if (done) {\n+      jmp(L_notNull);\n+      bind(L_null);\n+      \/\/ Set null marker to zero to signal that the argument is null.\n+      \/\/ Also set all oop fields to zero to make the GC happy.\n+      stream.reset(sig_index, to_index);\n+      while (stream.next(toReg, bt)) {\n+        if (sig->at(stream.sig_index())._offset == -1 ||\n+            bt == T_OBJECT || bt == T_ARRAY) {\n+          if (toReg->is_stack()) {\n+            int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+            movq(Address(rsp, st_off), 0);\n+          } else {\n+            xorq(toReg->as_Register(), toReg->as_Register());\n+          }\n+        }\n+      }\n+      bind(L_notNull);\n+    } else {\n+      bind(L_null);\n+    }\n+  }\n+\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  from_index--;\n+  assert(progress || (start_offset == offset()), \"should not emit code\");\n+  return done;\n+}\n+\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[], Register val_array) {\n+  assert(sig->at(sig_index)._bt == T_METADATA, \"should be at delimiter\");\n+  assert(to->is_valid(), \"destination must be valid\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  \/\/ TODO 8284443 Isn't it an issue if below code uses r14 as tmp when it contains a spilled value?\n+  \/\/ Be careful with r14 because it's used for spilling (see MacroAssembler::spill_reg_for).\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r14;\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register tmp3 = rbx;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  assert_different_registers(val_obj_tmp, from_reg_tmp, tmp1, tmp2, tmp3, val_array);\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_OBJECT);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n+  BasicType bt;\n+  Label L_null;\n+  while (stream.next(fromReg, bt)) {\n+    assert(fromReg->is_valid(), \"source must be valid\");\n+    reg_state[fromReg->value()] = reg_writable;\n+\n+    int off = sig->at(stream.sig_index())._offset;\n+    if (off == -1) {\n+      \/\/ Nullable inline type argument, emit null check\n+      Label L_notNull;\n+      if (fromReg->is_stack()) {\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        testb(Address(rsp, ld_off), 1);\n+      } else {\n+        testb(fromReg->as_Register(), 1);\n+      }\n+      jcc(Assembler::notZero, L_notNull);\n+      movptr(val_obj, 0);\n+      jmp(L_null);\n+      bind(L_notNull);\n+      continue;\n+    }\n+\n+    assert(off > 0, \"offset in object should be positive\");\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+    if (!fromReg->is_XMMRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(src, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        src = fromReg->as_Register();\n+      }\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, src, size_in_bytes);\n+      }\n+    } else if (bt == T_DOUBLE) {\n+      movdbl(dst, fromReg->as_XMMRegister());\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      movflt(dst, fromReg->as_XMMRegister());\n+    }\n+  }\n+  bind(L_null);\n+  sig_index = stream.sig_index();\n+  from_index = stream.regs_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state);\n+  assert(success, \"to register must be writeable\");\n+  return true;\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();\n+}\n+\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair) {\n+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  if (needs_stack_repair) {\n+    \/\/ TODO 8284443 Add a comment drawing the frame like in Aarch64's version of MacroAssembler::remove_frame\n+    movq(rbp, Address(rsp, initial_framesize));\n+    \/\/ The stack increment resides just below the saved rbp\n+    addq(rsp, Address(rsp, initial_framesize - wordSize));\n+  } else {\n+    if (initial_framesize > 0) {\n+      addq(rsp, initial_framesize);\n+    }\n+    pop(rbp);\n+  }\n+}\n+\n@@ -5736,2 +6473,2 @@\n-void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp,\n-                               bool is_large, KRegister mask) {\n+void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp,\n+                               bool is_large, bool word_copy_only, KRegister mask) {\n@@ -5742,1 +6479,1 @@\n-  assert(tmp==rax,   \"tmp register must be eax for rep stos\");\n+  assert(val==rax,   \"val register must be eax for rep stos\");\n@@ -5748,3 +6485,0 @@\n-  if (!is_large || !UseXMMForObjInit) {\n-    xorptr(tmp, tmp);\n-  }\n@@ -5762,1 +6496,1 @@\n-    movptr(Address(base, cnt, Address::times_ptr), tmp);\n+    movptr(Address(base, cnt, Address::times_ptr), val);\n@@ -5771,1 +6505,1 @@\n-  if (UseFastStosb) {\n+  if (UseFastStosb && !word_copy_only) {\n@@ -5775,1 +6509,1 @@\n-    xmm_clear_mem(base, cnt, tmp, xtmp, mask);\n+    xmm_clear_mem(base, cnt, val, xtmp, mask);\n@@ -9648,0 +10382,4 @@\n+  if (EnableValhalla) {\n+    \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+    andptr(reg_rax, ~((int) markWord::inline_type_bit_in_place));\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":755,"deletions":17,"binary":false,"changes":772,"status":"modified"},{"patch":"@@ -2721,0 +2721,33 @@\n+    if (tf()->returns_inline_type_as_fields() && !_method->is_method_handle_intrinsic() && _method->return_type()->is_loaded()) {\n+      \/\/ The last return value is not set by the callee but used to pass the null marker to compiled code.\n+      \/\/ Search for the corresponding projection, get the register and emit code that initialized it.\n+      uint con = (tf()->range_cc()->cnt() - 1);\n+      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+        ProjNode* proj = fast_out(i)->as_Proj();\n+        if (proj->_con == con) {\n+          \/\/ Set null marker if rax is non-null (a non-null value is returned buffered or scalarized)\n+          OptoReg::Name optoReg = ra_->get_reg_first(proj);\n+          VMReg reg = OptoReg::as_VMReg(optoReg, ra_->_framesize, OptoReg::reg2stack(ra_->_matcher._new_SP));\n+          Register toReg = reg->is_reg() ? reg->as_Register() : rscratch1;\n+          __ testq(rax, rax);\n+          __ setb(Assembler::notZero, toReg);\n+          __ movzbl(toReg, toReg);\n+          if (reg->is_stack()) {\n+            int st_off = reg->reg2stack() * VMRegImpl::stack_slot_size;\n+            __ movq(Address(rsp, st_off), toReg);\n+          }\n+          break;\n+        }\n+      }\n+      if (return_value_is_used()) {\n+        \/\/ An inline type is returned as fields in multiple registers.\n+        \/\/ Rax either contains an oop if the inline type is buffered or a pointer\n+        \/\/ to the corresponding InlineKlass with the lowest bit set to 1. Zero rax\n+        \/\/ if the lowest bit is set to allow C2 to use the oop after null checking.\n+        \/\/ rax &= (rax & 1) - 1\n+        __ movptr(rscratch1, rax);\n+        __ andptr(rscratch1, 0x1);\n+        __ subptr(rscratch1, 0x1);\n+        __ andptr(rax, rscratch1);\n+      }\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":33,"deletions":0,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -618,0 +618,4 @@\n+  if (_entry_point == nullptr) {\n+    \/\/ CallLeafNoFPInDirect\n+    return 3; \/\/ callq (register)\n+  }\n@@ -624,0 +628,1 @@\n+\n@@ -849,14 +854,1 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n-  if (C->clinit_barrier_on_entry()) {\n-    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n-    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n-\n-    Label L_skip_barrier;\n-    Register klass = rscratch1;\n-\n-    __ mov_metadata(klass, C->method()->holder()->constant_encoding());\n-    __ clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n-\n-    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+  __ verified_entry(C);\n@@ -864,1 +856,2 @@\n-    __ bind(L_skip_barrier);\n+  if (ra_->C->stub_function() == nullptr) {\n+    __ entry_barrier();\n@@ -867,1 +860,3 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != nullptr);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    __ bind(*_verified_entry);\n+  }\n@@ -879,6 +874,0 @@\n-uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -931,13 +920,3 @@\n-  int framesize = C->output()->frame_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove word for return adr already pushed\n-  \/\/ and RBP\n-  framesize -= 2*wordSize;\n-\n-  \/\/ Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here\n-\n-  if (framesize) {\n-    __ addq(rsp, framesize);\n-  }\n-\n-  __ popq(rbp);\n+  \/\/ Subtract two words to account for return address and rbp\n+  int initial_framesize = C->output()->frame_size_in_bytes() - 2*wordSize;\n+  __ remove_frame(initial_framesize, C->needs_stack_repair());\n@@ -962,6 +941,0 @@\n-uint MachEpilogNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n@@ -1569,0 +1542,43 @@\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"MachVEPNode\");\n+}\n+#endif\n+\n+void MachVEPNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  CodeBuffer* cbuf = masm->code();\n+  uint insts_size = cbuf->insts_size();\n+  if (!_verified) {\n+    __ ic_check(1);\n+  } else {\n+    \/\/ TODO 8284443 Avoid creation of temporary frame\n+    if (ra_->C->stub_function() == nullptr) {\n+      __ verified_entry(ra_->C, 0);\n+      __ entry_barrier();\n+      int initial_framesize = ra_->C->output()->frame_size_in_bytes() - 2*wordSize;\n+      __ remove_frame(initial_framesize, false);\n+    }\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    int sp_inc = __ unpack_inline_args(ra_->C, _receiver_only);\n+    \/\/ Emit code for verified entry and save increment for stack repair on return\n+    __ verified_entry(ra_->C, sp_inc);\n+    if (Compile::current()->output()->in_scratch_emit_size()) {\n+      Label dummy_verified_entry;\n+      __ jmp(dummy_verified_entry);\n+    } else {\n+      __ jmp(*_verified_entry);\n+    }\n+  }\n+  \/* WARNING these NOPs are critical so that verified entry point is properly\n+     4 bytes aligned for patching by NativeJump::patch_verified_entry() *\/\n+  int nops_cnt = 4 - ((cbuf->insts_size() - insts_size) & 0x3);\n+  nops_cnt &= 0x3; \/\/ Do not add nops if code is aligned.\n+  if (nops_cnt > 0) {\n+    __ nop(nops_cnt);\n+  }\n+}\n+\n@@ -1589,7 +1605,0 @@\n-uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n-{\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n-\n@@ -3070,0 +3079,16 @@\n+\/\/ Indirect Narrow Oop Operand\n+operand indCompressedOop(rRegN reg) %{\n+  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(DecodeN reg);\n+\n+  op_cost(10);\n+  format %{\"[R12 + $reg << 3] (compressed oop addressing)\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0xc); \/\/ R12\n+    index($reg);\n+    scale(0x3);\n+    disp(0x0);\n+  %}\n+%}\n+\n@@ -3416,1 +3441,1 @@\n-               indCompressedOopOffset,\n+               indCompressedOop, indCompressedOopOffset,\n@@ -6001,0 +6026,26 @@\n+instruct castI2N(rRegN dst, rRegI src)\n+%{\n+  match(Set dst (CastI2N src));\n+\n+  format %{ \"movq    $dst, $src\\t# int -> narrow ptr\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movl($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+instruct castN2X(rRegL dst, rRegN src)\n+%{\n+  match(Set dst (CastP2X src));\n+\n+  format %{ \"movq    $dst, $src\\t# ptr -> long\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n@@ -12132,0 +12183,1 @@\n+\n@@ -12134,1 +12186,1 @@\n-instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -12137,3 +12189,120 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX <= 2));\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                            Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Small non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                       Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  ins_cost(125);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -12187,2 +12356,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, false, $ktmp$$KRegister);\n@@ -12193,3 +12362,2 @@\n-\/\/ Small non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                       Universe dummy, rFlagsReg cr)\n+instruct rep_stos_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                 Universe dummy, rFlagsReg cr)\n@@ -12197,2 +12365,2 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX > 2));\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -12200,1 +12368,1 @@\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -12248,2 +12416,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, false, true, $ktmp$$KRegister);\n@@ -12255,1 +12423,1 @@\n-instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n@@ -12258,3 +12426,99 @@\n-  predicate((UseAVX <=2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct rep_stos_large_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,\n+                                  Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, USE_KILL val, KILL cr);\n+\n+  format %{ $$template\n+    if (UseXMMForObjInit) {\n+       $$emit$$\"movdq   $tmp, $val\\n\\t\"\n+       $$emit$$\"punpcklqdq $tmp, $tmp\\n\\t\"\n+       $$emit$$\"vinserti128_high $tmp, $tmp\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu $tmp,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Large non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                             Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -12299,2 +12563,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, knoreg);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, false, $ktmp$$KRegister);\n@@ -12305,3 +12569,2 @@\n-\/\/ Large non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n-                             Universe dummy, rFlagsReg cr)\n+instruct rep_stos_large_evex_word_copy(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegL val,\n+                                       Universe dummy, rFlagsReg cr)\n@@ -12309,3 +12572,3 @@\n-  predicate((UseAVX > 2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only() && (UseAVX > 2));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, USE_KILL val, KILL cr);\n@@ -12350,2 +12613,2 @@\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,\n+                 $tmp$$XMMRegister, true, true, $ktmp$$KRegister);\n@@ -12357,1 +12620,1 @@\n-instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rRegI zero, kReg ktmp, Universe dummy, rFlagsReg cr)\n+instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rax_RegL val, kReg ktmp, Universe dummy, rFlagsReg cr)\n@@ -12359,2 +12622,3 @@\n-  predicate(!((ClearArrayNode*)n)->is_large() && (MaxVectorSize >= 32) && VM_Version::supports_avx512vl());\n-  match(Set dummy (ClearArray cnt base));\n+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only() &&\n+            ((MaxVectorSize >= 32) && VM_Version::supports_avx512vl()));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n@@ -12362,1 +12626,1 @@\n-  effect(TEMP tmp, TEMP zero, TEMP ktmp, KILL cr);\n+  effect(TEMP tmp, USE_KILL val, TEMP ktmp, KILL cr);\n@@ -12365,1 +12629,1 @@\n-   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n+    __ clear_mem($base$$Register, $cnt$$constant, $val$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n@@ -14238,0 +14502,15 @@\n+\/\/ entry point is null, target holds the address to call\n+instruct CallLeafNoFPInDirect(rRegP target)\n+%{\n+  predicate(n->as_Call()->entry_point() == nullptr);\n+  match(CallLeafNoFP target);\n+\n+  ins_cost(300);\n+  format %{ \"call_leaf_nofp,runtime indirect \" %}\n+  ins_encode %{\n+     __ call($target$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -14240,0 +14519,1 @@\n+  predicate(n->as_Call()->entry_point() != nullptr);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":362,"deletions":82,"binary":false,"changes":444,"status":"modified"},{"patch":"@@ -856,1 +856,7 @@\n-    if (k->is_objArray_klass()) {\n+    if (k->is_flatArray_klass()) {\n+      num_obj_array_klasses ++;\n+      type = \"flat array\";\n+    } else if (k->is_refArray_klass()) {\n+        num_obj_array_klasses ++;\n+        type = \"ref array\";\n+    } else if (k->is_objArray_klass()) {\n@@ -860,1 +866,1 @@\n-      type = \"array\";\n+      type = \"obj array\";\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -189,1 +189,1 @@\n-  memset(mem, 0, objArrayOopDesc::object_size(element_count));\n+  memset(mem, 0, refArrayOopDesc::object_size(element_count));\n@@ -225,1 +225,1 @@\n-  assert(objArrayOopDesc::object_size(max_elem_count)*HeapWordSize == MIN_GC_REGION_ALIGNMENT,\n+  assert(refArrayOopDesc::object_size(max_elem_count)*HeapWordSize == MIN_GC_REGION_ALIGNMENT,\n@@ -339,1 +339,1 @@\n-  size_t byte_size = objArrayOopDesc::object_size(length) * HeapWordSize;\n+  size_t byte_size = refArrayOopDesc::object_size(length) * HeapWordSize;\n@@ -596,1 +596,1 @@\n-  if (!src_obj->fast_no_hash_check()) {\n+  if (!src_obj->fast_no_hash_check() && (!(EnableValhalla && src_obj->mark().is_inline_type()))) {\n@@ -600,0 +600,2 @@\n+    } else if (EnableValhalla) {\n+      fake_oop->set_mark(src_klass->prototype_header().copy_set_hash(src_hash));\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -59,0 +60,3 @@\n+bool CDSConfig::_module_patching_disables_cds = false;\n+bool CDSConfig::_java_base_module_patching_disables_cds = false;\n+\n@@ -142,0 +146,3 @@\n+    if (is_valhalla_preview()) {\n+      tmp.print_raw(\"_valhalla\");\n+    }\n@@ -297,1 +304,1 @@\n-  if (Arguments::is_incompatible_cds_internal_module_property(key)) {\n+  if (Arguments::is_incompatible_cds_internal_module_property(key) && !Arguments::patching_migrated_classes(key, value)) {\n@@ -330,2 +337,1 @@\n-    \"jdk.module.upgrade.path\",\n-    \"jdk.module.patch.0\"\n+    \"jdk.module.upgrade.path\"\n@@ -335,2 +341,1 @@\n-    \"--upgrade-module-path\",\n-    \"--patch-module\"\n+    \"--upgrade-module-path\"\n@@ -359,0 +364,6 @@\n+\n+  if (module_patching_disables_cds()) {\n+    vm_exit_during_initialization(\n+            \"Cannot use the following option when dumping the shared archive\", \"--patch-module\");\n+  }\n+\n@@ -387,0 +398,10 @@\n+\n+  if (module_patching_disables_cds()) {\n+    if (RequireSharedSpaces) {\n+      warning(\"CDS is disabled when the %s option is specified.\", \"--patch-module\");\n+    } else {\n+      log_info(cds)(\"CDS is disabled when the %s option is specified.\", \"--patch-module\");\n+    }\n+    return true;\n+  }\n+\n@@ -624,1 +645,1 @@\n-bool CDSConfig::check_vm_args_consistency(bool patch_mod_javabase, bool mode_flag_cmd_line) {\n+bool CDSConfig::check_vm_args_consistency(bool mode_flag_cmd_line) {\n@@ -690,1 +711,1 @@\n-  if (is_using_archive() && patch_mod_javabase) {\n+  if (is_using_archive() && java_base_module_patching_disables_cds() && module_patching_disables_cds()) {\n@@ -911,0 +932,4 @@\n+  if (is_valhalla_preview()) {\n+    \/\/ Not working yet -- e.g., HeapShared::oop_hash() needs to be implemented for value oops\n+    return false;\n+  }\n","filename":"src\/hotspot\/share\/cds\/cdsConfig.cpp","additions":32,"deletions":7,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -1309,1 +1309,5 @@\n-      assert(resolved_k == k, \"classes used by archived heap must not be replaced by JVMTI ClassFileLoadHook\");\n+      if (resolved_k->is_array_klass()) {\n+        assert(resolved_k == k || resolved_k == k->super(), \"classes used by archived heap must not be replaced by JVMTI ClassFileLoadHook\");\n+      } else {\n+        assert(resolved_k == k, \"classes used by archived heap must not be replaced by JVMTI ClassFileLoadHook\");\n+      }\n@@ -2051,0 +2055,7 @@\n+\n+    if (CDSConfig::is_valhalla_preview() && strcmp(klass_name, \"jdk\/internal\/module\/ArchivedModuleGraph\") == 0) {\n+      \/\/ FIXME -- ArchivedModuleGraph doesn't work when java.base is patched with valhalla classes.\n+      i++;\n+      continue;\n+    }\n+\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -80,0 +80,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -127,1 +129,1 @@\n-\/\/ [0] All classes are loaded in MetaspaceShared::preload_classes(). All metadata are\n+\/\/ [0] All classes are loaded in MetaspaceShared::loadable_descriptors(). All metadata are\n@@ -463,1 +465,1 @@\n-  soc->do_tag(objArrayOopDesc::base_offset_in_bytes());\n+  soc->do_tag(refArrayOopDesc::base_offset_in_bytes());\n@@ -878,1 +880,1 @@\n-void MetaspaceShared::preload_classes(TRAPS) {\n+void MetaspaceShared::loadable_descriptors(TRAPS) {\n@@ -925,1 +927,1 @@\n-    preload_classes(CHECK);\n+    loadable_descriptors(CHECK);\n@@ -1230,0 +1232,5 @@\n+  if (CDSConfig::is_valhalla_preview()) {\n+    log_info(cds)(\"Archived java heap is not yet supported with Valhalla preview\");\n+    return;\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":11,"deletions":4,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -68,0 +68,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -447,0 +448,10 @@\n+void ClassLoaderData::inline_classes_do(void f(InlineKlass*)) {\n+  \/\/ Lock-free access requires load_acquire\n+  for (Klass* k = Atomic::load_acquire(&_klasses); k != nullptr; k = k->next_link()) {\n+    if (k->is_inline_klass()) {\n+      f(InlineKlass::cast(k));\n+    }\n+    assert(k != k->next_link(), \"no loops!\");\n+  }\n+}\n+\n@@ -625,0 +636,2 @@\n+  inline_classes_do(InlineKlass::cleanup);\n+\n@@ -904,1 +917,5 @@\n-        MetadataFactory::free_metadata(this, (InstanceKlass*)m);\n+        if (!((Klass*)m)->is_inline_klass()) {\n+          MetadataFactory::free_metadata(this, (InstanceKlass*)m);\n+        } else {\n+          MetadataFactory::free_metadata(this, (InlineKlass*)m);\n+        }\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.cpp","additions":18,"deletions":1,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -217,0 +217,1 @@\n+  void inline_classes_do(void f(InlineKlass*));\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -56,0 +56,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -57,1 +59,1 @@\n-#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.inline.hpp\"\n@@ -1084,1 +1086,9 @@\n-    if (k->is_typeArray_klass()) {\n+    if (k->is_flatArray_klass()) {\n+      Klass* element_klass = (Klass*) FlatArrayKlass::cast(k)->element_klass();\n+      assert(element_klass->is_inline_klass(), \"Must be inline type component\");\n+      if (is_scratch) {\n+        comp_mirror = Handle(THREAD, HeapShared::scratch_java_mirror(element_klass));\n+      } else {\n+        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+      }\n+    } else if (k->is_typeArray_klass()) {\n@@ -1093,0 +1103,1 @@\n+      assert(!k->is_refArray_klass() || !k->is_flatArray_klass(), \"Must not have mirror\");\n@@ -1095,0 +1106,1 @@\n+      oop comp_oop = element_klass->java_mirror();\n@@ -1098,1 +1110,1 @@\n-        comp_mirror = Handle(THREAD, element_klass->java_mirror());\n+        comp_mirror = Handle(THREAD, comp_oop);\n@@ -1128,1 +1140,0 @@\n-\n@@ -1132,0 +1143,9 @@\n+\n+    if (k->is_refined_objArray_klass()) {\n+      Klass* super_klass = k->super();\n+      assert(super_klass != nullptr, \"Must be\");\n+      Handle mirror(THREAD, super_klass->java_mirror());\n+      k->set_java_mirror(mirror);\n+      return;\n+    }\n+\n@@ -1154,0 +1174,1 @@\n+\n@@ -1176,3 +1197,3 @@\n-  if (k->class_loader() != nullptr &&\n-      k->class_loader() != SystemDictionary::java_platform_loader() &&\n-      k->class_loader() != SystemDictionary::java_system_loader()) {\n+  if ((k->class_loader() != nullptr &&\n+       k->class_loader() != SystemDictionary::java_platform_loader() &&\n+       k->class_loader() != SystemDictionary::java_system_loader())) {\n@@ -1217,1 +1238,0 @@\n-  assert(as_Klass(m) == k, \"must be\");\n@@ -1221,0 +1241,1 @@\n+    assert(as_Klass(m) == k, \"must be\");\n@@ -1230,0 +1251,4 @@\n+  } else {\n+    ObjArrayKlass* objarray_k = (ObjArrayKlass*)as_Klass(m);\n+    \/\/ Mirror should be restored for an ObjArrayKlass or one of its refined array klasses\n+    assert(objarray_k == k || objarray_k->next_refined_array_klass() == k, \"must be\");\n@@ -1406,1 +1431,3 @@\n-  if (is_instance)  st->print(\"L\");\n+  if (is_instance)  {\n+    st->print(\"L\");\n+  }\n@@ -1465,0 +1492,1 @@\n+  assert(!klass->is_refined_objArray_klass(), \"should not be ref or flat array klass\");\n@@ -2850,1 +2878,1 @@\n-      if (method->name() == vmSymbols::object_initializer_name() &&\n+      if (method->is_object_constructor() &&\n@@ -3209,2 +3237,2 @@\n-  if (m->is_object_initializer()) {\n-    flags |= java_lang_invoke_MemberName::MN_IS_CONSTRUCTOR;\n+  if (m->is_object_constructor()) {\n+    flags |= java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR;\n@@ -3601,1 +3629,1 @@\n-int java_lang_reflect_Field::_trusted_final_offset;\n+int java_lang_reflect_Field::_flags_offset;\n@@ -3611,1 +3639,1 @@\n-  macro(_trusted_final_offset,    k, vmSymbols::trusted_final_name(),    bool_signature,       false); \\\n+  macro(_flags_offset,     k, vmSymbols::flags_name(),     int_signature,    false); \\\n@@ -3676,2 +3704,2 @@\n-void java_lang_reflect_Field::set_trusted_final(oop field) {\n-  field->bool_field_put(_trusted_final_offset, true);\n+void java_lang_reflect_Field::set_flags(oop field, int value) {\n+  field->int_field_put(_flags_offset, value);\n@@ -3980,2 +4008,1 @@\n-int java_lang_boxing_object::_value_offset;\n-int java_lang_boxing_object::_long_value_offset;\n+int* java_lang_boxing_object::_offsets;\n@@ -3983,3 +4010,9 @@\n-#define BOXING_FIELDS_DO(macro) \\\n-  macro(_value_offset,      integerKlass, \"value\", int_signature, false); \\\n-  macro(_long_value_offset, longKlass, \"value\", long_signature, false);\n+#define BOXING_FIELDS_DO(macro)                                                                                                    \\\n+  macro(java_lang_boxing_object::_offsets[T_BOOLEAN - T_BOOLEAN], vmClasses::Boolean_klass(),   \"value\", bool_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_CHAR - T_BOOLEAN],    vmClasses::Character_klass(), \"value\", char_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_FLOAT - T_BOOLEAN],   vmClasses::Float_klass(),     \"value\", float_signature,  false); \\\n+  macro(java_lang_boxing_object::_offsets[T_DOUBLE - T_BOOLEAN],  vmClasses::Double_klass(),    \"value\", double_signature, false); \\\n+  macro(java_lang_boxing_object::_offsets[T_BYTE - T_BOOLEAN],    vmClasses::Byte_klass(),      \"value\", byte_signature,   false); \\\n+  macro(java_lang_boxing_object::_offsets[T_SHORT - T_BOOLEAN],   vmClasses::Short_klass(),     \"value\", short_signature,  false); \\\n+  macro(java_lang_boxing_object::_offsets[T_INT - T_BOOLEAN],     vmClasses::Integer_klass(),   \"value\", int_signature,    false); \\\n+  macro(java_lang_boxing_object::_offsets[T_LONG - T_BOOLEAN],    vmClasses::Long_klass(),      \"value\", long_signature,   false);\n@@ -3988,2 +4021,2 @@\n-  InstanceKlass* integerKlass = vmClasses::Integer_klass();\n-  InstanceKlass* longKlass = vmClasses::Long_klass();\n+  assert(T_LONG - T_BOOLEAN == 7, \"Sanity check\");\n+  java_lang_boxing_object::_offsets = NEW_C_HEAP_ARRAY(int, 8, mtInternal);\n@@ -3995,0 +4028,4 @@\n+  if (f->reading()) {\n+    assert(T_LONG - T_BOOLEAN == 7, \"Sanity check\");\n+    java_lang_boxing_object::_offsets = NEW_C_HEAP_ARRAY(int, 8, mtInternal);\n+  }\n@@ -4015,1 +4052,1 @@\n-      box->bool_field_put(_value_offset, value->z);\n+      box->bool_field_put(value_offset(type), value->z);\n@@ -4018,1 +4055,1 @@\n-      box->char_field_put(_value_offset, value->c);\n+      box->char_field_put(value_offset(type), value->c);\n@@ -4021,1 +4058,1 @@\n-      box->float_field_put(_value_offset, value->f);\n+      box->float_field_put(value_offset(type), value->f);\n@@ -4024,1 +4061,1 @@\n-      box->double_field_put(_long_value_offset, value->d);\n+      box->double_field_put(value_offset(type), value->d);\n@@ -4027,1 +4064,1 @@\n-      box->byte_field_put(_value_offset, value->b);\n+      box->byte_field_put(value_offset(type), value->b);\n@@ -4030,1 +4067,1 @@\n-      box->short_field_put(_value_offset, value->s);\n+      box->short_field_put(value_offset(type), value->s);\n@@ -4033,1 +4070,1 @@\n-      box->int_field_put(_value_offset, value->i);\n+      box->int_field_put(value_offset(type), value->i);\n@@ -4036,1 +4073,1 @@\n-      box->long_field_put(_long_value_offset, value->j);\n+      box->long_field_put(value_offset(type), value->j);\n@@ -4058,1 +4095,1 @@\n-    value->z = box->bool_field(_value_offset);\n+    value->z = box->bool_field(value_offset(type));\n@@ -4061,1 +4098,1 @@\n-    value->c = box->char_field(_value_offset);\n+    value->c = box->char_field(value_offset(type));\n@@ -4064,1 +4101,1 @@\n-    value->f = box->float_field(_value_offset);\n+    value->f = box->float_field(value_offset(type));\n@@ -4067,1 +4104,1 @@\n-    value->d = box->double_field(_long_value_offset);\n+    value->d = box->double_field(value_offset(type));\n@@ -4070,1 +4107,1 @@\n-    value->b = box->byte_field(_value_offset);\n+    value->b = box->byte_field(value_offset(type));\n@@ -4073,1 +4110,1 @@\n-    value->s = box->short_field(_value_offset);\n+    value->s = box->short_field(value_offset(type));\n@@ -4076,1 +4113,1 @@\n-    value->i = box->int_field(_value_offset);\n+    value->i = box->int_field(value_offset(type));\n@@ -4079,1 +4116,1 @@\n-    value->j = box->long_field(_long_value_offset);\n+    value->j = box->long_field(value_offset(type));\n@@ -4092,1 +4129,1 @@\n-    box->bool_field_put(_value_offset, value->z);\n+    box->bool_field_put(value_offset(type), value->z);\n@@ -4095,1 +4132,1 @@\n-    box->char_field_put(_value_offset, value->c);\n+    box->char_field_put(value_offset(type), value->c);\n@@ -4098,1 +4135,1 @@\n-    box->float_field_put(_value_offset, value->f);\n+    box->float_field_put(value_offset(type), value->f);\n@@ -4101,1 +4138,1 @@\n-    box->double_field_put(_long_value_offset, value->d);\n+    box->double_field_put(value_offset(type), value->d);\n@@ -4104,1 +4141,1 @@\n-    box->byte_field_put(_value_offset, value->b);\n+    box->byte_field_put(value_offset(type), value->b);\n@@ -4107,1 +4144,1 @@\n-    box->short_field_put(_value_offset, value->s);\n+    box->short_field_put(value_offset(type), value->s);\n@@ -4110,1 +4147,1 @@\n-    box->int_field_put(_value_offset, value->i);\n+    box->int_field_put(value_offset(type), value->i);\n@@ -4113,1 +4150,1 @@\n-    box->long_field_put(_long_value_offset, value->j);\n+    box->long_field_put(value_offset(type), value->j);\n@@ -4509,1 +4546,0 @@\n-\n@@ -4519,1 +4555,1 @@\n-  return (flags(mname) & (MN_IS_METHOD | MN_IS_CONSTRUCTOR)) > 0;\n+  return (flags(mname) & (MN_IS_METHOD | MN_IS_OBJECT_CONSTRUCTOR)) > 0;\n@@ -5512,16 +5548,11 @@\n-#define CHECK_OFFSET(klass_name, cpp_klass_name, field_name, field_sig) \\\n-  valid &= check_offset(klass_name, cpp_klass_name :: _##field_name ## _offset, #field_name, field_sig)\n-\n-#define CHECK_LONG_OFFSET(klass_name, cpp_klass_name, field_name, field_sig) \\\n-  valid &= check_offset(klass_name, cpp_klass_name :: _##long_ ## field_name ## _offset, #field_name, field_sig)\n-\n-  \/\/ Boxed primitive objects (java_lang_boxing_object)\n-\n-  CHECK_OFFSET(\"java\/lang\/Boolean\",   java_lang_boxing_object, value, \"Z\");\n-  CHECK_OFFSET(\"java\/lang\/Character\", java_lang_boxing_object, value, \"C\");\n-  CHECK_OFFSET(\"java\/lang\/Float\",     java_lang_boxing_object, value, \"F\");\n-  CHECK_LONG_OFFSET(\"java\/lang\/Double\", java_lang_boxing_object, value, \"D\");\n-  CHECK_OFFSET(\"java\/lang\/Byte\",      java_lang_boxing_object, value, \"B\");\n-  CHECK_OFFSET(\"java\/lang\/Short\",     java_lang_boxing_object, value, \"S\");\n-  CHECK_OFFSET(\"java\/lang\/Integer\",   java_lang_boxing_object, value, \"I\");\n-  CHECK_LONG_OFFSET(\"java\/lang\/Long\", java_lang_boxing_object, value, \"J\");\n+#define CHECK_OFFSET(klass_name, type, field_sig) \\\n+  valid &= check_offset(klass_name, java_lang_boxing_object::value_offset(type), \"value\", field_sig)\n+\n+  CHECK_OFFSET(\"java\/lang\/Boolean\",   T_BOOLEAN, \"Z\");\n+  CHECK_OFFSET(\"java\/lang\/Character\", T_CHAR,    \"C\");\n+  CHECK_OFFSET(\"java\/lang\/Float\",     T_FLOAT,   \"F\");\n+  CHECK_OFFSET(\"java\/lang\/Double\",    T_DOUBLE,  \"D\");\n+  CHECK_OFFSET(\"java\/lang\/Byte\",      T_BYTE,    \"B\");\n+  CHECK_OFFSET(\"java\/lang\/Short\",     T_SHORT,   \"S\");\n+  CHECK_OFFSET(\"java\/lang\/Integer\",   T_INT,     \"I\");\n+  CHECK_OFFSET(\"java\/lang\/Long\",      T_LONG,    \"J\");\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":96,"deletions":65,"binary":false,"changes":161,"status":"modified"},{"patch":"@@ -1290,0 +1290,1 @@\n+  SET_ADDRESS(_extrs, SharedRuntime::allocate_inline_types);\n@@ -1338,0 +1339,8 @@\n+    SET_ADDRESS(_extrs, Runtime1::new_null_free_array);\n+    SET_ADDRESS(_extrs, Runtime1::load_flat_array);\n+    SET_ADDRESS(_extrs, Runtime1::store_flat_array);\n+    SET_ADDRESS(_extrs, Runtime1::substitutability_check);\n+    SET_ADDRESS(_extrs, Runtime1::buffer_inline_args);\n+    SET_ADDRESS(_extrs, Runtime1::buffer_inline_args_no_receiver);\n+    SET_ADDRESS(_extrs, Runtime1::throw_identity_exception);\n+    SET_ADDRESS(_extrs, Runtime1::throw_illegal_monitor_state_exception);\n@@ -1371,0 +1380,2 @@\n+    SET_ADDRESS(_extrs, OptoRuntime::load_unknown_inline_C);\n+    SET_ADDRESS(_extrs, OptoRuntime::store_unknown_inline_C);\n@@ -1398,0 +1409,4 @@\n+  if (UseCompressedOops) {\n+    SET_ADDRESS(_extrs, CompressedOops::base_addr());\n+  }\n+\n","filename":"src\/hotspot\/share\/code\/aotCodeCache.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -66,0 +66,1 @@\n+static_assert(!std::is_polymorphic<BufferedInlineTypeBlob>::value,   \"no virtual methods are allowed in code blobs\");\n@@ -94,0 +95,1 @@\n+      &BufferedInlineTypeBlob::_vpntr,\n@@ -412,2 +414,2 @@\n-BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size)\n-  : RuntimeBlob(name, kind, cb, size, sizeof(BufferBlob), CodeOffsets::frame_never_safe, 0, nullptr)\n+BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int header_size)\n+  : RuntimeBlob(name, kind, cb, size, header_size, CodeOffsets::frame_never_safe, 0, nullptr)\n@@ -425,1 +427,1 @@\n-    blob = new (size) BufferBlob(name, CodeBlobKind::Buffer, cb, size);\n+    blob = new (size) BufferBlob(name, CodeBlobKind::Buffer, cb, size, sizeof(BufferBlob));\n@@ -441,0 +443,4 @@\n+BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments)\n+  : RuntimeBlob(name, kind, cb, size, sizeof(BufferBlob), frame_complete, frame_size, oop_maps, caller_must_gc_arguments)\n+{}\n+\n@@ -445,2 +451,2 @@\n-AdapterBlob::AdapterBlob(int size, CodeBuffer* cb) :\n-  BufferBlob(\"I2C\/C2I adapters\", CodeBlobKind::Adapter, cb, size) {\n+AdapterBlob::AdapterBlob(int size, CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) :\n+  BufferBlob(\"I2C\/C2I adapters\", CodeBlobKind::Adapter, cb, size, frame_complete, frame_size, oop_maps, caller_must_gc_arguments) {\n@@ -450,1 +456,1 @@\n-AdapterBlob* AdapterBlob::create(CodeBuffer* cb) {\n+AdapterBlob* AdapterBlob::create(CodeBuffer* cb, int frame_complete, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) {\n@@ -459,1 +465,1 @@\n-    blob = new (size) AdapterBlob(size, cb);\n+    blob = new (size) AdapterBlob(size, cb, frame_complete, frame_size, oop_maps, caller_must_gc_arguments);\n@@ -541,0 +547,25 @@\n+\/\/ Implementation of BufferedInlineTypeBlob\n+BufferedInlineTypeBlob::BufferedInlineTypeBlob(int size, CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off) :\n+  BufferBlob(\"buffered inline type\", CodeBlobKind::BufferedInlineType, cb, size, sizeof(BufferedInlineTypeBlob)),\n+  _pack_fields_off(pack_fields_off),\n+  _pack_fields_jobject_off(pack_fields_jobject_off),\n+  _unpack_fields_off(unpack_fields_off) {\n+  CodeCache::commit(this);\n+}\n+\n+BufferedInlineTypeBlob* BufferedInlineTypeBlob::create(CodeBuffer* cb, int pack_fields_off, int pack_fields_jobject_off, int unpack_fields_off) {\n+  ThreadInVMfromUnknown __tiv;  \/\/ get to VM state in case we block on CodeCache_lock\n+\n+  BufferedInlineTypeBlob* blob = nullptr;\n+  unsigned int size = CodeBlob::allocation_size(cb, sizeof(BufferedInlineTypeBlob));\n+  {\n+    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+    blob = new (size) BufferedInlineTypeBlob(size, cb, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n+  }\n+  \/\/ Track memory usage statistic after releasing CodeCache_lock\n+  MemoryService::track_code_cache_memory_usage();\n+\n+  return blob;\n+}\n+\n+\/\/----------------------------------------------------------------------------------------------------\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":38,"deletions":7,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -112,2 +112,2 @@\n-  void set_to_monomorphic();\n-  void set_to_megamorphic(CallInfo* call_info);\n+  void set_to_monomorphic(bool caller_is_c1);\n+  void set_to_megamorphic(CallInfo* call_info, bool caller_is_c1);\n@@ -134,1 +134,1 @@\n-  void update(CallInfo* call_info, Klass* receiver_klass);\n+  void update(CallInfo* call_info, Klass* receiver_klass, bool caller_is_c1);\n@@ -161,1 +161,1 @@\n-\/\/    compilled code <------------> interpreted code\n+\/\/    compiled code <------------> interpreted code\n@@ -216,1 +216,1 @@\n-  void set(const methodHandle& callee_method);\n+  void set(const methodHandle& callee_method, bool caller_is_c1);\n","filename":"src\/hotspot\/share\/code\/compiledIC.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -723,0 +723,11 @@\n+\n+      \/\/ If inline types are passed as fields, use the extended signature\n+      \/\/ which contains the types of all (oop) fields of the inline type.\n+      if (is_compiled_by_c2() && callee->has_scalarized_args()) {\n+        const GrowableArray<SigEntry>* sig = callee->adapter()->get_sig_cc();\n+        assert(sig != nullptr, \"sig should never be null\");\n+        TempNewSymbol tmp_sig = SigEntry::create_symbol(sig);\n+        has_receiver = false; \/\/ The extended signature contains the receiver type\n+        fr.oops_compiled_arguments_do(tmp_sig, has_receiver, has_appendix, reg_map, f);\n+        return;\n+      }\n@@ -1259,0 +1270,4 @@\n+  _inline_entry_point             = entry_point();\n+  _verified_inline_entry_point    = verified_entry_point();\n+  _verified_inline_ro_entry_point = verified_entry_point();\n+\n@@ -1298,1 +1313,1 @@\n-\n+    assert(!method->has_scalarized_args(), \"scalarized native wrappers not supported yet\");\n@@ -1502,0 +1517,4 @@\n+    _inline_entry_point             = code_begin() + offsets->value(CodeOffsets::Inline_Entry);\n+    _verified_inline_entry_point    = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry);\n+    _verified_inline_ro_entry_point = code_begin() + offsets->value(CodeOffsets::Verified_Inline_Entry_RO);\n+\n@@ -3722,0 +3741,1 @@\n+  if (pos == inline_entry_point())                                      label = \"[Inline Entry Point]\";\n@@ -3723,0 +3743,2 @@\n+  if (pos == verified_inline_entry_point())                             label = \"[Verified Inline Entry Point]\";\n+  if (pos == verified_inline_ro_entry_point())                          label = \"[Verified Inline Entry Point (RO)]\";\n@@ -3732,0 +3754,10 @@\n+static int maybe_print_entry_label(outputStream* stream, address pos, address entry, const char* label) {\n+  if (pos == entry) {\n+    stream->bol();\n+    stream->print_cr(\"%s\", label);\n+    return 1;\n+  } else {\n+    return 0;\n+  }\n+}\n+\n@@ -3734,33 +3766,12 @@\n-    const char* label = nmethod_section_label(block_begin);\n-    if (label != nullptr) {\n-      stream->bol();\n-      stream->print_cr(\"%s\", label);\n-    }\n-  }\n-\n-  if (block_begin == entry_point()) {\n-    Method* m = method();\n-    if (m != nullptr) {\n-      stream->print(\"  # \");\n-      m->print_value_on(stream);\n-      stream->cr();\n-    }\n-    if (m != nullptr && !is_osr_method()) {\n-      ResourceMark rm;\n-      int sizeargs = m->size_of_parameters();\n-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n-      VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n-      {\n-        int sig_index = 0;\n-        if (!m->is_static())\n-          sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n-        for (SignatureStream ss(m->signature()); !ss.at_return_type(); ss.next()) {\n-          BasicType t = ss.type();\n-          sig_bt[sig_index++] = t;\n-          if (type2size[t] == 2) {\n-            sig_bt[sig_index++] = T_VOID;\n-          } else {\n-            assert(type2size[t] == 1, \"size is 1 or 2\");\n-          }\n-        }\n-        assert(sig_index == sizeargs, \"\");\n+    int n = 0;\n+    \/\/ Multiple entry points may be at the same position. Print them all.\n+    n += maybe_print_entry_label(stream, block_begin, entry_point(),                    \"[Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, inline_entry_point(),             \"[Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_entry_point(),           \"[Verified Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_entry_point(),    \"[Verified Inline Entry Point]\");\n+    n += maybe_print_entry_label(stream, block_begin, verified_inline_ro_entry_point(), \"[Verified Inline Entry Point (RO)]\");\n+    if (n == 0) {\n+      const char* label = nmethod_section_label(block_begin);\n+      if (label != nullptr) {\n+        stream->bol();\n+        stream->print_cr(\"%s\", label);\n@@ -3768,54 +3779,73 @@\n-      const char* spname = \"sp\"; \/\/ make arch-specific?\n-      SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n-      int stack_slot_offset = this->frame_size() * wordSize;\n-      int tab1 = 14, tab2 = 24;\n-      int sig_index = 0;\n-      int arg_index = (m->is_static() ? 0 : -1);\n-      bool did_old_sp = false;\n-      for (SignatureStream ss(m->signature()); !ss.at_return_type(); ) {\n-        bool at_this = (arg_index == -1);\n-        bool at_old_sp = false;\n-        BasicType t = (at_this ? T_OBJECT : ss.type());\n-        assert(t == sig_bt[sig_index], \"sigs in sync\");\n-        if (at_this)\n-          stream->print(\"  # this: \");\n-        else\n-          stream->print(\"  # parm%d: \", arg_index);\n-        stream->move_to(tab1);\n-        VMReg fst = regs[sig_index].first();\n-        VMReg snd = regs[sig_index].second();\n-        if (fst->is_reg()) {\n-          stream->print(\"%s\", fst->name());\n-          if (snd->is_valid())  {\n-            stream->print(\":%s\", snd->name());\n-          }\n-        } else if (fst->is_stack()) {\n-          int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n-          if (offset == stack_slot_offset)  at_old_sp = true;\n-          stream->print(\"[%s+0x%x]\", spname, offset);\n-        } else {\n-          stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n-        }\n-        stream->print(\" \");\n-        stream->move_to(tab2);\n-        stream->print(\"= \");\n-        if (at_this) {\n-          m->method_holder()->print_value_on(stream);\n-        } else {\n-          bool did_name = false;\n-          if (!at_this && ss.is_reference()) {\n-            Symbol* name = ss.as_symbol();\n-            name->print_value_on(stream);\n-            did_name = true;\n-          }\n-          if (!did_name)\n-            stream->print(\"%s\", type2name(t));\n-        }\n-        if (at_old_sp) {\n-          stream->print(\"  (%s of caller)\", spname);\n-          did_old_sp = true;\n-        }\n-        stream->cr();\n-        sig_index += type2size[t];\n-        arg_index += 1;\n-        if (!at_this)  ss.next();\n+    }\n+  }\n+\n+  Method* m = method();\n+  if (m == nullptr || is_osr_method()) {\n+    return;\n+  }\n+\n+  \/\/ Print the name of the method (only once)\n+  address low = MIN3(entry_point(),\n+                     verified_entry_point(),\n+                     inline_entry_point());\n+  \/\/ The verified inline entry point and verified inline RO entry point are not always\n+  \/\/ used. When they are unused. CodeOffsets::Verified_Inline_Entry(_RO) is -1. Hence,\n+  \/\/ the calculated entry point is smaller than the block they are offsetting into.\n+  if (verified_inline_entry_point() >= block_begin) {\n+    low = MIN2(low, verified_inline_entry_point());\n+  }\n+  if (verified_inline_ro_entry_point() >= block_begin) {\n+    low = MIN2(low, verified_inline_ro_entry_point());\n+  }\n+  assert(low != 0, \"sanity\");\n+  if (block_begin == low) {\n+    stream->print(\"  # \");\n+    m->print_value_on(stream);\n+    stream->cr();\n+  }\n+\n+  \/\/ Print the arguments for the 3 types of verified entry points\n+  CompiledEntrySignature ces(m);\n+  ces.compute_calling_conventions(false);\n+  const GrowableArray<SigEntry>* sig_cc;\n+  const VMRegPair* regs;\n+  if (block_begin == verified_entry_point()) {\n+    sig_cc = ces.sig_cc();\n+    regs = ces.regs_cc();\n+  } else if (block_begin == verified_inline_entry_point()) {\n+    sig_cc = ces.sig();\n+    regs = ces.regs();\n+  } else if (block_begin == verified_inline_ro_entry_point()) {\n+    sig_cc = ces.sig_cc_ro();\n+    regs = ces.regs_cc_ro();\n+  } else {\n+    return;\n+  }\n+\n+  bool has_this = !m->is_static();\n+  if (ces.has_inline_recv() && block_begin == verified_entry_point()) {\n+    \/\/ <this> argument is scalarized for verified_entry_point()\n+    has_this = false;\n+  }\n+  const char* spname = \"sp\"; \/\/ make arch-specific?\n+  int stack_slot_offset = this->frame_size() * wordSize;\n+  int tab1 = 14, tab2 = 24;\n+  int sig_index = 0;\n+  int arg_index = has_this ? -1 : 0;\n+  bool did_old_sp = false;\n+  for (ExtendedSignature sig = ExtendedSignature(sig_cc, SigEntryFilter()); !sig.at_end(); ++sig) {\n+    bool at_this = (arg_index == -1);\n+    bool at_old_sp = false;\n+    BasicType t = (*sig)._bt;\n+    if (at_this) {\n+      stream->print(\"  # this: \");\n+    } else {\n+      stream->print(\"  # parm%d: \", arg_index);\n+    }\n+    stream->move_to(tab1);\n+    VMReg fst = regs[sig_index].first();\n+    VMReg snd = regs[sig_index].second();\n+    if (fst->is_reg()) {\n+      stream->print(\"%s\", fst->name());\n+      if (snd->is_valid())  {\n+        stream->print(\":%s\", snd->name());\n@@ -3823,6 +3853,23 @@\n-      if (!did_old_sp) {\n-        stream->print(\"  # \");\n-        stream->move_to(tab1);\n-        stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n-        stream->print(\"  (%s of caller)\", spname);\n-        stream->cr();\n+    } else if (fst->is_stack()) {\n+      int offset = fst->reg2stack() * VMRegImpl::stack_slot_size + stack_slot_offset;\n+      if (offset == stack_slot_offset)  at_old_sp = true;\n+      stream->print(\"[%s+0x%x]\", spname, offset);\n+    } else {\n+      stream->print(\"reg%d:%d??\", (int)(intptr_t)fst, (int)(intptr_t)snd);\n+    }\n+    stream->print(\" \");\n+    stream->move_to(tab2);\n+    stream->print(\"= \");\n+    if (at_this) {\n+      m->method_holder()->print_value_on(stream);\n+    } else {\n+      bool did_name = false;\n+      if (is_reference_type(t)) {\n+        Symbol* name = (*sig)._name;\n+        name->print_value_on(stream);\n+        did_name = true;\n+      }\n+      if (!did_name)\n+        stream->print(\"%s\", type2name(t));\n+      if ((*sig)._null_marker) {\n+        stream->print(\" (null marker)\");\n@@ -3831,0 +3878,14 @@\n+    if (at_old_sp) {\n+      stream->print(\"  (%s of caller)\", spname);\n+      did_old_sp = true;\n+    }\n+    stream->cr();\n+    sig_index += type2size[t];\n+    arg_index += 1;\n+  }\n+  if (!did_old_sp) {\n+    stream->print(\"  # \");\n+    stream->move_to(tab1);\n+    stream->print(\"[%s+0x%x]\", spname, stack_slot_offset);\n+    stream->print(\"  (%s of caller)\", spname);\n+    stream->cr();\n@@ -3954,1 +4015,1 @@\n-      st->print(\" {reexecute=%d rethrow=%d return_oop=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop());\n+      st->print(\" {reexecute=%d rethrow=%d return_oop=%d return_scalarized=%d}\", sd->should_reexecute(), sd->rethrow_exception(), sd->return_oop(), sd->return_scalarized());\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":156,"deletions":95,"binary":false,"changes":251,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"compiler\/compilerDefinitions.hpp\"\n@@ -216,0 +217,4 @@\n+  \/\/ TODO: can these be uint16_t, seem rely on -1 CodeOffset, can change later...\n+  address _inline_entry_point;              \/\/ inline type entry point (unpack all inline type args) with class check\n+  address _verified_inline_entry_point;     \/\/ inline type entry point (unpack all inline type args) without class check\n+  address _verified_inline_ro_entry_point;  \/\/ inline type entry point (unpack receiver only) without class check\n@@ -686,0 +691,3 @@\n+  address inline_entry_point() const              { return _inline_entry_point; }             \/\/ inline type entry point (unpack all inline type args)\n+  address verified_inline_entry_point() const     { return _verified_inline_entry_point; }    \/\/ inline type entry point (unpack all inline type args) without class check\n+  address verified_inline_ro_entry_point() const  { return _verified_inline_ro_entry_point; } \/\/ inline type entry point (only unpack receiver) without class check\n@@ -755,0 +763,10 @@\n+  bool  needs_stack_repair() const {\n+    if (is_compiled_by_c1()) {\n+      return method()->c1_needs_stack_repair();\n+    } else if (is_compiled_by_c2()) {\n+      return method()->c2_needs_stack_repair();\n+    } else {\n+      return false;\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -78,0 +78,1 @@\n+#include \"oops\/flatArrayKlass.inline.hpp\"\n@@ -2453,0 +2454,3 @@\n+    \/\/ Read the klass before the copying, since it might destroy the klass (i.e. overlapping copy)\n+    \/\/ and if partial copy, the destination klass may not be copied yet\n+    Klass* klass = cast_to_oop(source())->klass();\n@@ -2454,1 +2458,1 @@\n-    cast_to_oop(copy_destination())->init_mark();\n+    cast_to_oop(copy_destination())->set_mark(Klass::default_prototype_header(klass));\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -383,0 +383,1 @@\n+  assert(obj->is_refArray(), \"Must be\");\n@@ -398,1 +399,1 @@\n-  if (obj->is_objArray()) {\n+  if (obj->is_refArray()) {\n@@ -415,1 +416,1 @@\n-  array->oop_iterate_range(&mark_and_push_closure, beg_index, end_index);\n+  refArrayOop(array)->oop_iterate_range(&mark_and_push_closure, beg_index, end_index);\n","filename":"src\/hotspot\/share\/gc\/serial\/serialFullGC.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -173,1 +173,1 @@\n-  return to_oop(addr)->is_objArray();\n+  return to_oop(addr)->is_refArray();\n","filename":"src\/hotspot\/share\/gc\/z\/zMark.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -47,0 +48,3 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -78,0 +82,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -228,0 +233,54 @@\n+JRT_ENTRY(void, InterpreterRuntime::read_flat_field(JavaThread* current, oopDesc* obj, ResolvedFieldEntry* entry))\n+  assert(oopDesc::is_oop(obj), \"Sanity check\");\n+  Handle obj_h(THREAD, obj);\n+\n+  InstanceKlass* holder = InstanceKlass::cast(entry->field_holder());\n+  assert(entry->field_holder()->field_is_flat(entry->field_index()), \"Sanity check\");\n+\n+  InlineLayoutInfo* layout_info = holder->inline_layout_info_adr(entry->field_index());\n+  InlineKlass* field_vklass = layout_info->klass();\n+\n+#ifdef ASSERT\n+  fieldDescriptor fd;\n+  bool found = holder->find_field_from_offset(entry->field_offset(), false, &fd);\n+  assert(found, \"Field not found\");\n+  assert(fd.is_flat(), \"Field must be flat\");\n+#endif \/\/ ASSERT\n+\n+  oop res = field_vklass->read_payload_from_addr(obj_h(), entry->field_offset(), layout_info->kind(), CHECK);\n+  current->set_vm_result_oop(res);\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::read_nullable_flat_field(JavaThread* current, oopDesc* obj, ResolvedFieldEntry* entry))\n+  assert(oopDesc::is_oop(obj), \"Sanity check\");\n+  assert(entry->has_null_marker(), \"Otherwise should not get there\");\n+  Handle obj_h(THREAD, obj);\n+\n+  InstanceKlass* holder = entry->field_holder();\n+  int field_index = entry->field_index();\n+  InlineLayoutInfo* li= holder->inline_layout_info_adr(field_index);\n+\n+#ifdef ASSERT\n+  fieldDescriptor fd;\n+  bool found = holder->find_field_from_offset(entry->field_offset(), false, &fd);\n+  assert(found, \"Field not found\");\n+  assert(fd.is_flat(), \"Field must be flat\");\n+#endif \/\/ ASSERT\n+\n+  InlineKlass* field_vklass = InlineKlass::cast(li->klass());\n+  oop res = field_vklass->read_payload_from_addr(obj_h(), entry->field_offset(), li->kind(), CHECK);\n+  current->set_vm_result_oop(res);\n+\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::write_nullable_flat_field(JavaThread* current, oopDesc* obj, oopDesc* value, ResolvedFieldEntry* entry))\n+  assert(oopDesc::is_oop(obj), \"Sanity check\");\n+  Handle obj_h(THREAD, obj);\n+  assert(value == nullptr || oopDesc::is_oop(value), \"Sanity check\");\n+  Handle val_h(THREAD, value);\n+\n+  InstanceKlass* holder = entry->field_holder();\n+  InlineLayoutInfo* li = holder->inline_layout_info_adr(entry->field_index());\n+  InlineKlass* vk = li->klass();\n+  vk->write_value_to_addr(val_h(), ((char*)(oopDesc*)obj_h()) + entry->field_offset(), li->kind(), true, CHECK);\n+JRT_END\n@@ -237,1 +296,1 @@\n-  objArrayOop obj = oopFactory::new_objArray(klass, size, CHECK);\n+  arrayOop obj = oopFactory::new_objArray(klass, size, CHECK);\n@@ -241,0 +300,12 @@\n+JRT_ENTRY(void, InterpreterRuntime::flat_array_load(JavaThread* current, arrayOopDesc* array, int index))\n+  assert(array->is_flatArray(), \"Must be\");\n+  flatArrayOop farray = (flatArrayOop)array;\n+  oop res = farray->obj_at(index, CHECK);\n+  current->set_vm_result_oop(res);\n+JRT_END\n+\n+JRT_ENTRY(void, InterpreterRuntime::flat_array_store(JavaThread* current, oopDesc* val, arrayOopDesc* array, int index))\n+  assert(array->is_flatArray(), \"Must be\");\n+  flatArrayOop farray = (flatArrayOop)array;\n+  farray->obj_at_put(index, val, CHECK);\n+JRT_END\n@@ -246,2 +317,2 @@\n-  int          i = last_frame.get_index_u2(Bytecodes::_multianewarray);\n-  Klass* klass   = constants->klass_at(i, CHECK);\n+  int i = last_frame.get_index_u2(Bytecodes::_multianewarray);\n+  Klass* klass = constants->klass_at(i, CHECK);\n@@ -276,0 +347,24 @@\n+JRT_ENTRY(jboolean, InterpreterRuntime::is_substitutable(JavaThread* current, oopDesc* aobj, oopDesc* bobj))\n+  assert(oopDesc::is_oop(aobj) && oopDesc::is_oop(bobj), \"must be valid oops\");\n+\n+  Handle ha(THREAD, aobj);\n+  Handle hb(THREAD, bobj);\n+  JavaValue result(T_BOOLEAN);\n+  JavaCallArguments args;\n+  args.push_oop(ha);\n+  args.push_oop(hb);\n+  methodHandle method(current, Universe::is_substitutable_method());\n+  method->method_holder()->initialize(CHECK_false); \/\/ Ensure class ValueObjectMethods is initialized\n+  JavaCalls::call(&result, method, &args, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+    \/\/ Something really bad happened because isSubstitutable() should not throw exceptions\n+    \/\/ If it is an error, just let it propagate\n+    \/\/ If it is an exception, wrap it into an InternalError\n+    if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+      Handle e(THREAD, PENDING_EXCEPTION);\n+      CLEAR_PENDING_EXCEPTION;\n+      THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in substitutability test\", e, false);\n+    }\n+  }\n+  return result.get_jboolean();\n+JRT_END\n@@ -623,0 +718,4 @@\n+JRT_ENTRY(void, InterpreterRuntime::throw_InstantiationError(JavaThread* current))\n+  THROW(vmSymbols::java_lang_InstantiationError());\n+JRT_END\n+\n@@ -702,0 +801,1 @@\n+  bool strict_static_final = info.is_strict() && info.is_static() && info.is_final();\n@@ -707,1 +807,5 @@\n-    get_code = ((is_static) ? Bytecodes::_getstatic : Bytecodes::_getfield);\n+    if (is_static) {\n+      get_code = Bytecodes::_getstatic;\n+    } else {\n+      get_code = Bytecodes::_getfield;\n+    }\n@@ -709,1 +813,1 @@\n-      put_code = ((is_static) ? Bytecodes::_putstatic : Bytecodes::_putfield);\n+        put_code = ((is_static) ? Bytecodes::_putstatic : Bytecodes::_putfield);\n@@ -711,0 +815,13 @@\n+    assert(!info.is_strict_static_unset(), \"after initialization, no unset flags\");\n+  } else if (is_static && (info.is_strict_static_unset() || strict_static_final)) {\n+    \/\/ During <clinit>, closely track the state of strict statics.\n+    \/\/ 1. if we are reading an uninitialized strict static, throw\n+    \/\/ 2. if we are writing one, clear the \"unset\" flag\n+    \/\/\n+    \/\/ Note: If we were handling an attempted write of a null to a\n+    \/\/ null-restricted strict static, we would NOT clear the \"unset\"\n+    \/\/ flag.\n+    assert(klass->is_being_initialized(), \"else should have thrown\");\n+    assert(klass->is_reentrant_initialization(THREAD),\n+      \"<clinit> must be running in current thread\");\n+    klass->notify_strict_static_access(info.index(), is_put, CHECK);\n@@ -714,1 +831,4 @@\n-  entry->set_flags(info.access_flags().is_final(), info.access_flags().is_volatile());\n+  entry->set_flags(info.access_flags().is_final(), info.access_flags().is_volatile(),\n+                   info.is_flat(), info.is_null_free_inline_type(),\n+                   info.has_null_marker());\n+\n@@ -761,1 +881,0 @@\n-\n@@ -766,1 +885,0 @@\n-\n@@ -781,0 +899,15 @@\n+JRT_ENTRY(void, InterpreterRuntime::throw_identity_exception(JavaThread* current, oopDesc* obj))\n+  Klass* klass = cast_to_oop(obj)->klass();\n+  ResourceMark rm(THREAD);\n+  const char* desc = \"Cannot synchronize on an instance of value class \";\n+  const char* className = klass->external_name();\n+  size_t msglen = strlen(desc) + strlen(className) + 1;\n+  char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+  if (nullptr == message) {\n+    \/\/ Out of memory: can't create detailed error message\n+    THROW_MSG(vmSymbols::java_lang_IdentityException(), className);\n+  } else {\n+    jio_snprintf(message, msglen, \"%s%s\", desc, className);\n+    THROW_MSG(vmSymbols::java_lang_IdentityException(), message);\n+  }\n+JRT_END\n@@ -1191,0 +1324,1 @@\n+  assert(entry->is_valid(), \"Invalid ResolvedFieldEntry\");\n@@ -1198,0 +1332,1 @@\n+  bool is_flat = entry->is_flat();\n@@ -1206,1 +1341,1 @@\n-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(field_holder, entry->field_offset(), is_static);\n+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(field_holder, entry->field_offset(), is_static, is_flat);\n@@ -1214,0 +1349,1 @@\n+  assert(entry->is_valid(), \"Invalid ResolvedFieldEntry\");\n@@ -1235,0 +1371,1 @@\n+\n@@ -1236,0 +1373,1 @@\n+  bool is_flat = entry->is_flat();\n@@ -1238,1 +1376,1 @@\n-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, entry->field_offset(), is_static);\n+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, entry->field_offset(), is_static, is_flat);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":148,"deletions":10,"binary":false,"changes":158,"status":"modified"},{"patch":"@@ -69,0 +69,1 @@\n+#include \"oops\/refArrayKlass.hpp\"\n@@ -114,0 +115,2 @@\n+static LatestMethodCache _is_substitutable_cache;           \/\/ ValueObjectMethods.isSubstitutable()\n+static LatestMethodCache _value_object_hash_code_cache;     \/\/ ValueObjectMethods.valueObjectHashCode()\n@@ -458,0 +461,1 @@\n+\n@@ -505,2 +509,6 @@\n-    Klass* oak = vmClasses::Object_klass()->array_klass(CHECK);\n-    _objectArrayKlass = ObjArrayKlass::cast(oak);\n+    ArrayKlass* oak = vmClasses::Object_klass()->array_klass(CHECK);\n+    oak->append_to_sibling_list();\n+\n+    \/\/ Create a RefArrayKlass (which is the default) and initialize.\n+    ObjArrayKlass* rak = ObjArrayKlass::cast(oak)->klass_with_properties(ArrayKlass::ArrayProperties::DEFAULT, THREAD);\n+    _objectArrayKlass = rak;\n@@ -508,7 +516,0 @@\n-  \/\/ OLD\n-  \/\/ Add the class to the class hierarchy manually to make sure that\n-  \/\/ its vtable is initialized after core bootstrapping is completed.\n-  \/\/ ---\n-  \/\/ New\n-  \/\/ Have already been initialized.\n-  _objectArrayKlass->append_to_sibling_list();\n@@ -646,0 +647,3 @@\n+\n+  \/\/ This isn't added to the subclass list, so need to reinitialize vtables directly.\n+  Universe::objectArrayKlass()->vtable().initialize_vtable();\n@@ -891,1 +895,0 @@\n-\n@@ -1046,0 +1049,2 @@\n+Method* Universe::is_substitutable_method()       { return _is_substitutable_cache.get_method(); }\n+Method* Universe::value_object_hash_code_method() { return _value_object_hash_code_cache.get_method(); }\n@@ -1075,0 +1080,11 @@\n+\n+  \/\/ Set up substitutability testing\n+  ResourceMark rm(current);\n+  _is_substitutable_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::isSubstitutable_name()->as_C_string(),\n+                          vmSymbols::object_object_boolean_signature(), true);\n+  _value_object_hash_code_cache.init(current,\n+                          vmClasses::ValueObjectMethods_klass(),\n+                          vmSymbols::valueObjectHashCode_name()->as_C_string(),\n+                          vmSymbols::object_int_signature(), true);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":26,"deletions":10,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -72,0 +72,1 @@\n+#include \"oops\/markWord.hpp\"\n@@ -76,0 +77,2 @@\n+#include \"oops\/inlineKlass.hpp\"\n+#include \"oops\/refArrayKlass.hpp\"\n@@ -152,0 +155,5 @@\n+void InlineLayoutInfo::metaspace_pointers_do(MetaspaceClosure* it) {\n+  log_trace(cds)(\"Iter(InlineFieldInfo): %p\", this);\n+  it->push(&_klass);\n+}\n+\n@@ -173,0 +181,13 @@\n+bool InstanceKlass::field_is_null_free_inline_type(int index) const {\n+  return field(index).field_flags().is_null_free_inline_type();\n+}\n+\n+bool InstanceKlass::is_class_in_loadable_descriptors_attribute(Symbol* name) const {\n+  if (_loadable_descriptors == nullptr) return false;\n+  for (int i = 0; i < _loadable_descriptors->length(); i++) {\n+        Symbol* class_name = _constants->symbol_at(_loadable_descriptors->at(i));\n+        if (class_name == name) return true;\n+  }\n+  return false;\n+}\n+\n@@ -467,1 +488,2 @@\n-                                       parser.is_interface());\n+                                       parser.is_interface(),\n+                                       parser.is_inline_type());\n@@ -490,0 +512,3 @@\n+  } else if (parser.is_inline_type()) {\n+    \/\/ inline type\n+    ik = new (loader_data, size, use_class_space, THREAD) InlineKlass(parser);\n@@ -506,0 +531,6 @@\n+#ifdef ASSERT\n+  ik->bounds_check((address) ik->start_of_vtable(), false, size);\n+  ik->bounds_check((address) ik->start_of_itable(), false, size);\n+  ik->bounds_check((address) ik->end_of_itable(), true, size);\n+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);\n+#endif \/\/ASSERT\n@@ -509,0 +540,23 @@\n+#ifndef PRODUCT\n+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {\n+  const char* bad = nullptr;\n+  address end = nullptr;\n+  if (addr < (address)this) {\n+    bad = \"before\";\n+  } else if (addr == (address)this) {\n+    if (edge_ok)  return true;\n+    bad = \"just before\";\n+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {\n+    if (edge_ok)  return true;\n+    bad = \"just after\";\n+  } else if (addr > end) {\n+    bad = \"after\";\n+  } else {\n+    return true;\n+  }\n+  tty->print_cr(\"%s object bounds: \" INTPTR_FORMAT \" [\" INTPTR_FORMAT \"..\" INTPTR_FORMAT \"]\",\n+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);\n+  Verbose = WizardMode = true; this->print(); \/\/@@\n+  return false;\n+}\n+#endif \/\/PRODUCT\n@@ -536,2 +590,2 @@\n-InstanceKlass::InstanceKlass(const ClassFileParser& parser, KlassKind kind, ReferenceType reference_type) :\n-  Klass(kind),\n+InstanceKlass::InstanceKlass(const ClassFileParser& parser, KlassKind kind, markWord prototype_header, ReferenceType reference_type) :\n+  Klass(kind, prototype_header),\n@@ -548,1 +602,4 @@\n-  _init_thread(nullptr)\n+  _init_thread(nullptr),\n+  _inline_layout_info_array(nullptr),\n+  _loadable_descriptors(nullptr),\n+  _adr_inlineklass_fixed_block(nullptr)\n@@ -555,0 +612,3 @@\n+  if (parser.has_inline_fields()) {\n+    set_has_inline_type_fields();\n+  }\n@@ -699,0 +759,5 @@\n+  if (inline_layout_info_array() != nullptr) {\n+    MetadataFactory::free_array<InlineLayoutInfo>(loader_data, inline_layout_info_array());\n+  }\n+  set_inline_layout_info_array(nullptr);\n+\n@@ -733,0 +798,7 @@\n+  if (loadable_descriptors() != nullptr &&\n+      loadable_descriptors() != Universe::the_empty_short_array() &&\n+      !loadable_descriptors()->is_shared()) {\n+    MetadataFactory::free_array<jushort>(loader_data, loadable_descriptors());\n+  }\n+  set_loadable_descriptors(nullptr);\n+\n@@ -972,0 +1044,101 @@\n+\n+  \/\/ If a class declares a method that uses an inline class as an argument\n+  \/\/ type or return inline type, this inline class must be loaded during the\n+  \/\/ linking of this class because size and properties of the inline class\n+  \/\/ must be known in order to be able to perform inline type optimizations.\n+  \/\/ The implementation below is an approximation of this rule, the code\n+  \/\/ iterates over all methods of the current class (including overridden\n+  \/\/ methods), not only the methods declared by this class. This\n+  \/\/ approximation makes the code simpler, and doesn't change the semantic\n+  \/\/ because classes declaring methods overridden by the current class are\n+  \/\/ linked (and have performed their own pre-loading) before the linking\n+  \/\/ of the current class.\n+\n+\n+  \/\/ Note:\n+  \/\/ Inline class types are loaded during\n+  \/\/ the loading phase (see ClassFileParser::post_process_parsed_stream()).\n+  \/\/ Inline class types used as element types for array creation\n+  \/\/ are not pre-loaded. Their loading is triggered by either anewarray\n+  \/\/ or multianewarray bytecodes.\n+\n+  \/\/ Could it be possible to do the following processing only if the\n+  \/\/ class uses inline types?\n+  if (EnableValhalla) {\n+    ResourceMark rm(THREAD);\n+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {\n+      if (fs.is_null_free_inline_type() && fs.access_flags().is_static()) {\n+        assert(fs.access_flags().is_strict(), \"null-free fields must be strict\");\n+        Symbol* sig = fs.signature();\n+        TempNewSymbol s = Signature::strip_envelope(sig);\n+        if (s != name()) {\n+          log_info(class, preload)(\"Preloading of class %s during linking of class %s. Cause: a null-free static field is declared with this type\", s->as_C_string(), name()->as_C_string());\n+          Klass* klass = SystemDictionary::resolve_or_fail(s,\n+                                                          Handle(THREAD, class_loader()), true,\n+                                                          CHECK_false);\n+          if (HAS_PENDING_EXCEPTION) {\n+            log_warning(class, preload)(\"Preloading of class %s during linking of class %s (cause: null-free static field) failed: %s\",\n+                                      s->as_C_string(), name()->as_C_string(), PENDING_EXCEPTION->klass()->name()->as_C_string());\n+            return false; \/\/ Exception is still pending\n+          }\n+          log_info(class, preload)(\"Preloading of class %s during linking of class %s (cause: null-free static field) succeeded\",\n+                                   s->as_C_string(), name()->as_C_string());\n+          assert(klass != nullptr, \"Sanity check\");\n+          if (klass->is_abstract()) {\n+            THROW_MSG_(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                      err_msg(\"Class %s expects class %s to be concrete value class, but it is an abstract class\",\n+                      name()->as_C_string(),\n+                      InstanceKlass::cast(klass)->external_name()), false);\n+          }\n+          if (!klass->is_inline_klass()) {\n+            THROW_MSG_(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                       err_msg(\"class %s expects class %s to be a value class but it is an identity class\",\n+                       name()->as_C_string(), klass->external_name()), false);\n+          }\n+          InlineKlass* vk = InlineKlass::cast(klass);\n+          \/\/ the inline_type_field_klasses_array might have been loaded with CDS, so update only if not already set and check consistency\n+          InlineLayoutInfo* li = inline_layout_info_adr(fs.index());\n+          if (li->klass() == nullptr) {\n+            li->set_klass(InlineKlass::cast(vk));\n+            li->set_kind(LayoutKind::REFERENCE);\n+          }\n+          assert(get_inline_type_field_klass(fs.index()) == vk, \"Must match\");\n+        } else {\n+          InlineLayoutInfo* li = inline_layout_info_adr(fs.index());\n+          if (li->klass() == nullptr) {\n+            li->set_klass(InlineKlass::cast(this));\n+            li->set_kind(LayoutKind::REFERENCE);\n+          }\n+          assert(get_inline_type_field_klass(fs.index()) == this, \"Must match\");\n+        }\n+      }\n+    }\n+\n+    \/\/ Aggressively preloading all classes from the LoadableDescriptors attribute\n+    if (loadable_descriptors() != nullptr && PreloadClasses) {\n+      HandleMark hm(THREAD);\n+      for (int i = 0; i < loadable_descriptors()->length(); i++) {\n+        Symbol* sig = constants()->symbol_at(loadable_descriptors()->at(i));\n+        if (!Signature::has_envelope(sig)) continue;\n+        TempNewSymbol class_name = Signature::strip_envelope(sig);\n+        if (class_name == name()) continue;\n+        log_info(class, preload)(\"Preloading of class %s during linking of class %s because of the class is listed in the LoadableDescriptors attribute\", sig->as_C_string(), name()->as_C_string());\n+        oop loader = class_loader();\n+        Klass* klass = SystemDictionary::resolve_or_null(class_name,\n+                                                         Handle(THREAD, loader), THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+          CLEAR_PENDING_EXCEPTION;\n+        }\n+        if (klass != nullptr) {\n+          log_info(class, preload)(\"Preloading of class %s during linking of class %s (cause: LoadableDescriptors attribute) succeeded\", class_name->as_C_string(), name()->as_C_string());\n+          if (!klass->is_inline_klass()) {\n+            \/\/ Non value class are allowed by the current spec, but it could be an indication of an issue so let's log a warning\n+            log_warning(class, preload)(\"Preloading of class %s during linking of class %s (cause: LoadableDescriptors attribute) but loaded class is not a value class\", class_name->as_C_string(), name()->as_C_string());\n+          }\n+        } else {\n+          log_warning(class, preload)(\"Preloading of class %s during linking of class %s (cause: LoadableDescriptors attribute) failed\", class_name->as_C_string(), name()->as_C_string());\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1276,0 +1449,21 @@\n+  \/\/ Pre-allocating an all-zero value to be used to reset nullable flat storages\n+  if (is_inline_klass()) {\n+      InlineKlass* vk = InlineKlass::cast(this);\n+      if (vk->has_nullable_atomic_layout()) {\n+        oop val = vk->allocate_instance(THREAD);\n+        if (HAS_PENDING_EXCEPTION) {\n+            Handle e(THREAD, PENDING_EXCEPTION);\n+            CLEAR_PENDING_EXCEPTION;\n+            {\n+                EXCEPTION_MARK;\n+                add_initialization_error(THREAD, e);\n+                \/\/ Locks object, set state, and notify all waiting threads\n+                set_initialization_state_and_notify(initialization_error, THREAD);\n+                CLEAR_PENDING_EXCEPTION;\n+            }\n+            THROW_OOP(e());\n+        }\n+        vk->set_null_reset_value(val);\n+      }\n+  }\n+\n@@ -1308,1 +1502,0 @@\n-\n@@ -1329,0 +1522,30 @@\n+\n+    if (has_strict_static_fields() && !HAS_PENDING_EXCEPTION) {\n+      \/\/ Step 9 also verifies that strict static fields have been initialized.\n+      \/\/ Status bits were set in ClassFileParser::post_process_parsed_stream.\n+      \/\/ After <clinit>, bits must all be clear, or else we must throw an error.\n+      \/\/ This is an extremely fast check, so we won't bother with a timer.\n+      assert(fields_status() != nullptr, \"\");\n+      Symbol* bad_strict_static = nullptr;\n+      for (int index = 0; index < fields_status()->length(); index++) {\n+        \/\/ Very fast loop over single byte array looking for a set bit.\n+        if (fields_status()->adr_at(index)->is_strict_static_unset()) {\n+          \/\/ This strict static field has not been set by the class initializer.\n+          \/\/ Note that in the common no-error case, we read no field metadata.\n+          \/\/ We only unpack it when we need to report an error.\n+          FieldInfo fi = field(index);\n+          bad_strict_static = fi.name(constants());\n+          if (debug_logging_enabled) {\n+            ResourceMark rm(jt);\n+            const char* msg = format_strict_static_message(bad_strict_static);\n+            log_debug(class, init)(\"%s\", msg);\n+          } else {\n+            \/\/ If we are not logging, do not bother to look for a second offense.\n+            break;\n+          }\n+        }\n+      }\n+      if (bad_strict_static != nullptr) {\n+        throw_strict_static_exception(bad_strict_static, \"is unset after initialization of\", THREAD);\n+      }\n+    }\n@@ -1382,0 +1605,68 @@\n+void InstanceKlass::notify_strict_static_access(int field_index, bool is_writing, TRAPS) {\n+  guarantee(field_index >= 0 && field_index < fields_status()->length(), \"valid field index\");\n+  DEBUG_ONLY(FieldInfo debugfi = field(field_index));\n+  assert(debugfi.access_flags().is_strict(), \"\");\n+  assert(debugfi.access_flags().is_static(), \"\");\n+  FieldStatus& fs = *fields_status()->adr_at(field_index);\n+  LogTarget(Trace, class, init) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm(THREAD);\n+    LogStream ls(lt);\n+    FieldInfo fi = field(field_index);\n+    ls.print(\"notify %s %s %s%s \",\n+             external_name(), is_writing? \"Write\" : \"Read\",\n+             fs.is_strict_static_unset() ? \"Unset\" : \"(set)\",\n+             fs.is_strict_static_unread() ? \"+Unread\" : \"\");\n+    fi.print(&ls, constants());\n+  }\n+  if (fs.is_strict_static_unset()) {\n+    assert(fs.is_strict_static_unread(), \"ClassFileParser resp.\");\n+    \/\/ If it is not set, there are only two reasonable things we can do here:\n+    \/\/ - mark it set if this is putstatic\n+    \/\/ - throw an error (Read-Before-Write) if this is getstatic\n+\n+    \/\/ The unset state is (or should be) transient, and observable only in one\n+    \/\/ thread during the execution of <clinit>.  Something is wrong here as this\n+    \/\/ should not be possible\n+    guarantee(is_reentrant_initialization(THREAD), \"unscoped access to strict static\");\n+    if (is_writing) {\n+      \/\/ clear the \"unset\" bit, since the field is actually going to be written\n+      fs.update_strict_static_unset(false);\n+    } else {\n+      \/\/ throw an IllegalStateException, since we are reading before writing\n+      \/\/ see also InstanceKlass::initialize_impl, Step 8 (at end)\n+      Symbol* bad_strict_static = field(field_index).name(constants());\n+      throw_strict_static_exception(bad_strict_static, \"is unset before first read in\", CHECK);\n+    }\n+  } else {\n+    \/\/ Ensure no write after read for final strict statics\n+    FieldInfo fi = field(field_index);\n+    bool is_final = fi.access_flags().is_final();\n+    if (is_final) {\n+      \/\/ no final write after read, so observing a constant freezes it, as if <clinit> ended early\n+      \/\/ (maybe we could trust the constant a little earlier, before <clinit> ends)\n+      if (is_writing && !fs.is_strict_static_unread()) {\n+        Symbol* bad_strict_static = fi.name(constants());\n+        throw_strict_static_exception(bad_strict_static, \"is set after read (as final) in\", CHECK);\n+      } else if (!is_writing && fs.is_strict_static_unread()) {\n+        fs.update_strict_static_unread(false);\n+      }\n+    }\n+  }\n+}\n+\n+void InstanceKlass::throw_strict_static_exception(Symbol* field_name, const char* when, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  const char* msg = format_strict_static_message(field_name, when);\n+  THROW_MSG(vmSymbols::java_lang_IllegalStateException(), msg);\n+}\n+\n+const char* InstanceKlass::format_strict_static_message(Symbol* field_name, const char* when) {\n+  stringStream ss;\n+  ss.print(\"Strict static \\\"%s\\\" %s %s\",\n+           field_name->as_C_string(),\n+           when == nullptr ? \"is unset in\" : when,\n+           external_name());\n+  return ss.as_string();\n+}\n+\n@@ -1565,7 +1856,3 @@\n-objArrayOop InstanceKlass::allocate_objArray(int n, int length, TRAPS) {\n-  check_array_allocation_length(length, arrayOopDesc::max_array_length(T_OBJECT), CHECK_NULL);\n-  size_t size = objArrayOopDesc::object_size(length);\n-  ArrayKlass* ak = array_klass(n, CHECK_NULL);\n-  objArrayOop o = (objArrayOop)Universe::heap()->array_allocate(ak, size, length,\n-                                                                \/* do_zero *\/ true, CHECK_NULL);\n-  return o;\n+objArrayOop InstanceKlass::allocate_objArray(int length, ArrayKlass::ArrayProperties props, TRAPS) {\n+  ArrayKlass* ak = array_klass(CHECK_NULL);\n+  return ObjArrayKlass::cast(ak)->allocate_instance(length, props, CHECK_NULL);\n@@ -1641,1 +1928,1 @@\n-  ObjArrayKlass* ak = array_klasses();\n+  ArrayKlass* ak = array_klasses();\n@@ -1648,2 +1935,2 @@\n-  ObjArrayKlass* oak = array_klasses_acquire();\n-  if (oak == nullptr) {\n+  ArrayKlass* ak = array_klasses_acquire();\n+  if (ak == nullptr) {\n@@ -1652,1 +1939,1 @@\n-    return oak->array_klass_or_null(n);\n+    return ak->array_klass_or_null(n);\n@@ -1669,1 +1956,1 @@\n-  if (clinit != nullptr && clinit->has_valid_initializer_flags()) {\n+  if (clinit != nullptr && clinit->is_class_initializer()) {\n@@ -1778,4 +2065,0 @@\n-bool InstanceKlass::contains_field_offset(int offset) {\n-  fieldDescriptor fd;\n-  return find_field_from_offset(offset, false, &fd);\n-}\n@@ -1862,0 +2145,9 @@\n+bool InstanceKlass::contains_field_offset(int offset) {\n+  if (this->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(this);\n+    return offset >= vk->payload_offset() && offset < (vk->payload_offset() + vk->payload_size_in_bytes());\n+  } else {\n+    fieldDescriptor fd;\n+    return find_field_from_offset(offset, false, &fd);\n+  }\n+}\n@@ -2245,0 +2537,3 @@\n+    if (name == vmSymbols::object_initializer_name()) {\n+      break;  \/\/ <init> is never inherited\n+    }\n@@ -2657,0 +2952,1 @@\n+  it->push(&_loadable_descriptors);\n@@ -2658,0 +2954,1 @@\n+  it->push(&_inline_layout_info_array, MetaspaceClosure::_writable);\n@@ -2705,1 +3002,1 @@\n-  \/\/ These are not allocated from metaspace. They are safe to set to null.\n+  \/\/ These are not allocated from metaspace. They are safe to set to nullptr.\n@@ -2796,0 +3093,4 @@\n+  if (is_inline_klass()) {\n+    InlineKlass::cast(this)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -2829,1 +3130,1 @@\n-    assert(this == array_klasses()->bottom_klass(), \"sanity\");\n+    assert(this == ObjArrayKlass::cast(array_klasses())->bottom_klass(), \"sanity\");\n@@ -2832,0 +3133,6 @@\n+    if (class_loader_data() == nullptr) {\n+      ResourceMark rm(THREAD);\n+      log_debug(cds)(\"  loader_data %s \", loader_data == nullptr ? \"nullptr\" : \"non null\");\n+      log_debug(cds)(\"  this %s array_klasses %s \", this->name()->as_C_string(), array_klasses()->name()->as_C_string());\n+    }\n+    assert(!array_klasses()->is_refined_objArray_klass(), \"must be non-refined objarrayklass\");\n@@ -2985,0 +3292,4 @@\n+bool InstanceKlass::supports_inline_types() const {\n+  return major_version() >= Verifier::VALUE_TYPES_MAJOR_VERSION && minor_version() == Verifier::JAVA_PREVIEW_MINOR_VERSION;\n+}\n+\n@@ -3017,0 +3328,2 @@\n+  return signature_name_of_carrier(JVM_SIGNATURE_CLASS);\n+}\n@@ -3018,0 +3331,1 @@\n+const char* InstanceKlass::signature_name_of_carrier(char c) const {\n@@ -3024,1 +3338,1 @@\n-  \/\/ Add L as type indicator\n+  \/\/ Add L or Q as type indicator\n@@ -3026,1 +3340,1 @@\n-  dest[dest_index++] = JVM_SIGNATURE_CLASS;\n+  dest[dest_index++] = c;\n@@ -3307,0 +3621,19 @@\n+void InstanceKlass::check_can_be_annotated_with_NullRestricted(InstanceKlass* type, Symbol* container_klass_name, TRAPS) {\n+  assert(type->is_instance_klass(), \"Sanity check\");\n+  if (type->is_identity_class()) {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+              err_msg(\"Class %s expects class %s to be a value class, but it is an identity class\",\n+              container_klass_name->as_C_string(),\n+              type->external_name()));\n+  }\n+\n+  if (type->is_abstract()) {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG(vmSymbols::java_lang_IncompatibleClassChangeError(),\n+              err_msg(\"Class %s expects class %s to be concrete value type, but it is an abstract class\",\n+              container_klass_name->as_C_string(),\n+              type->external_name()));\n+  }\n+}\n+\n@@ -3373,2 +3706,1 @@\n-  \/\/ Remember to strip ACC_SUPER bit\n-  return (access & (~JVM_ACC_SUPER));\n+  return access;\n@@ -3628,1 +3960,4 @@\n-static void print_vtable(intptr_t* start, int len, outputStream* st) {\n+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {\n+  ResourceMark rm;\n+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);\n+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;\n@@ -3632,0 +3967,5 @@\n+    if (forward_refs[i] != 0) {\n+      int from = forward_refs[i];\n+      int off = (int) start[from];\n+      st->print(\" (offset %d <= [%d])\", off, from);\n+    }\n@@ -3635,0 +3975,6 @@\n+    } else if (self != nullptr && e > 0 && e < 0x10000) {\n+      address location = self + e;\n+      int index = (int)((intptr_t*)location - start);\n+      st->print(\" (offset %d => [%d])\", (int)e, index);\n+      if (index >= 0 && index < len)\n+        forward_refs[index] = i;\n@@ -3641,1 +3987,22 @@\n-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);\n+  return print_vtable(nullptr, reinterpret_cast<intptr_t*>(start), len, st);\n+}\n+\n+template<typename T>\n+ static void print_array_on(outputStream* st, Array<T>* array) {\n+   if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+   array->print_value_on(st); st->cr();\n+   if (Verbose || WizardMode) {\n+     for (int i = 0; i < array->length(); i++) {\n+       st->print(\"%d : \", i); array->at(i)->print_value_on(st); st->cr();\n+     }\n+   }\n+ }\n+\n+static void print_array_on(outputStream* st, Array<int>* array) {\n+  if (array == nullptr) { st->print_cr(\"nullptr\"); return; }\n+  array->print_value_on(st); st->cr();\n+  if (Verbose || WizardMode) {\n+    for (int i = 0; i < array->length(); i++) {\n+      st->print(\"%d : %d\", i, array->at(i)); st->cr();\n+    }\n+  }\n@@ -3682,8 +4049,2 @@\n-  st->print(BULLET\"methods:           \"); methods()->print_value_on(st);               st->cr();\n-  if (Verbose || WizardMode) {\n-    Array<Method*>* method_array = methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n-  st->print(BULLET\"method ordering:   \"); method_ordering()->print_value_on(st);      st->cr();\n+  st->print(BULLET\"methods:           \"); print_array_on(st, methods());\n+  st->print(BULLET\"method ordering:   \"); print_array_on(st, method_ordering());\n@@ -3691,7 +4052,1 @@\n-    st->print(BULLET\"default_methods:   \"); default_methods()->print_value_on(st);    st->cr();\n-    if (Verbose) {\n-      Array<Method*>* method_array = default_methods();\n-      for (int i = 0; i < method_array->length(); i++) {\n-        st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-      }\n-    }\n+    st->print(BULLET\"default_methods:   \"); print_array_on(st, default_methods());\n@@ -3757,0 +4112,1 @@\n+  st->print(BULLET\"loadable descriptors:     \"); loadable_descriptors()->print_value_on(st); st->cr();\n@@ -3767,1 +4123,1 @@\n-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);\n+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(nullptr, start_of_itable(), itable_length(), st);\n@@ -3799,0 +4155,1 @@\n+  for (int i = 0; i < _indent; i++) _st->print(\"  \");\n@@ -3801,1 +4158,1 @@\n-     fd->print_on(_st);\n+     fd->print_on(_st, _base_offset);\n@@ -3804,2 +4161,2 @@\n-     fd->print_on_for(_st, _obj);\n-     _st->cr();\n+     fd->print_on_for(_st, _obj, _indent, _base_offset);\n+     if (!fd->field_flags().is_flat()) _st->cr();\n@@ -3810,1 +4167,1 @@\n-void InstanceKlass::oop_print_on(oop obj, outputStream* st) {\n+void InstanceKlass::oop_print_on(oop obj, outputStream* st, int indent, int base_offset) {\n@@ -3826,1 +4183,1 @@\n-  FieldPrinter print_field(st, obj);\n+  FieldPrinter print_field(st, obj, indent, base_offset);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":407,"deletions":50,"binary":false,"changes":457,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"code\/vmreg.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"oops\/refArrayKlass.hpp\"\n@@ -59,0 +61,1 @@\n+\/\/    [EMBEDDED InlineKlassFixedBlock] only if is an InlineKlass instance\n@@ -74,0 +77,1 @@\n+class BufferedInlineTypeBlob;\n@@ -89,0 +93,2 @@\n+   int _indent;\n+   int _base_offset;\n@@ -90,1 +96,2 @@\n-   FieldPrinter(outputStream* st, oop obj = nullptr) : _obj(obj), _st(st) {}\n+   FieldPrinter(outputStream* st, oop obj = nullptr, int indent = 0, int base_offset = 0) :\n+                 _obj(obj), _st(st), _indent(indent), _base_offset(base_offset) {}\n@@ -134,0 +141,54 @@\n+class SigEntry;\n+\n+class InlineKlassFixedBlock {\n+  Array<SigEntry>** _extended_sig;\n+  Array<VMRegPair>** _return_regs;\n+  address* _pack_handler;\n+  address* _pack_handler_jobject;\n+  address* _unpack_handler;\n+  int* _null_reset_value_offset;\n+  int _payload_offset;          \/\/ offset of the begining of the payload in a heap buffered instance\n+  int _payload_size_in_bytes;   \/\/ size of payload layout\n+  int _payload_alignment;       \/\/ alignment required for payload\n+  int _non_atomic_size_in_bytes; \/\/ size of null-free non-atomic flat layout\n+  int _non_atomic_alignment;    \/\/ alignment requirement for null-free non-atomic layout\n+  int _atomic_size_in_bytes;    \/\/ size and alignment requirement for a null-free atomic layout, -1 if no atomic flat layout is possible\n+  int _nullable_size_in_bytes;  \/\/ size and alignment requirement for a nullable layout (always atomic), -1 if no nullable flat layout is possible\n+  int _null_marker_offset;      \/\/ expressed as an offset from the beginning of the object for a heap buffered value\n+                                \/\/ payload_offset must be subtracted to get the offset from the beginning of the payload\n+\n+  friend class InlineKlass;\n+};\n+\n+class InlineLayoutInfo : public MetaspaceObj {\n+  InlineKlass* _klass;\n+  LayoutKind _kind;\n+  int _null_marker_offset; \/\/ null marker offset for this field, relative to the beginning of the current container\n+\n+ public:\n+  InlineLayoutInfo(): _klass(nullptr), _kind(LayoutKind::UNKNOWN), _null_marker_offset(-1)  {}\n+  InlineLayoutInfo(InlineKlass* ik, LayoutKind kind, int size, int nm_offset):\n+    _klass(ik), _kind(kind), _null_marker_offset(nm_offset) {}\n+\n+  InlineKlass* klass() const { return _klass; }\n+  void set_klass(InlineKlass* k) { _klass = k; }\n+\n+  LayoutKind kind() const {\n+    assert(_kind != LayoutKind::UNKNOWN, \"Not set\");\n+    return _kind;\n+  }\n+  void set_kind(LayoutKind lk) { _kind = lk; }\n+\n+  int null_marker_offset() const {\n+    assert(_null_marker_offset != -1, \"Not set\");\n+    return _null_marker_offset;\n+  }\n+  void set_null_marker_offset(int o) { _null_marker_offset = o; }\n+\n+  void metaspace_pointers_do(MetaspaceClosure* it);\n+  MetaspaceObj::Type type() const { return InlineLayoutInfoType; }\n+\n+  static ByteSize klass_offset() { return in_ByteSize(offset_of(InlineLayoutInfo, _klass)); }\n+  static ByteSize null_marker_offset_offset() { return in_ByteSize(offset_of(InlineLayoutInfo, _null_marker_offset)); }\n+};\n+\n@@ -139,0 +200,1 @@\n+  friend class TemplateTable;\n@@ -144,1 +206,1 @@\n-  InstanceKlass(const ClassFileParser& parser, KlassKind kind = Kind, ReferenceType reference_type = REF_NONE);\n+  InstanceKlass(const ClassFileParser& parser, KlassKind kind = Kind, markWord prototype = markWord::prototype(), ReferenceType reference_type = REF_NONE);\n@@ -282,0 +344,4 @@\n+  Array<InlineLayoutInfo>* _inline_layout_info_array;\n+  Array<u2>* _loadable_descriptors;\n+  const InlineKlassFixedBlock* _adr_inlineklass_fixed_block;\n+\n@@ -330,0 +396,15 @@\n+  bool has_inline_type_fields() const { return _misc_flags.has_inline_type_fields(); }\n+  void set_has_inline_type_fields()   { _misc_flags.set_has_inline_type_fields(true); }\n+\n+  bool is_naturally_atomic() const  { return _misc_flags.is_naturally_atomic(); }\n+  void set_is_naturally_atomic()    { _misc_flags.set_is_naturally_atomic(true); }\n+\n+  \/\/ Query if this class has atomicity requirements (default is yes)\n+  \/\/ This bit can occur anywhere, but is only significant\n+  \/\/ for inline classes *and* their super types.\n+  \/\/ It inherits from supers.\n+  \/\/ Its value depends on the ForceNonTearable VM option, the LooselyConsistentValue annotation\n+  \/\/ and the presence of flat fields with atomicity requirements\n+  bool must_be_atomic() const { return _misc_flags.must_be_atomic(); }\n+  void set_must_be_atomic()   { _misc_flags.set_must_be_atomic(true); }\n+\n@@ -394,0 +475,6 @@\n+  bool field_is_flat(int index) const { return field_flags(index).is_flat(); }\n+  bool field_has_null_marker(int index) const { return field_flags(index).has_null_marker(); }\n+  bool field_is_null_free_inline_type(int index) const;\n+  bool is_class_in_loadable_descriptors_attribute(Symbol* name) const;\n+\n+  int null_marker_offset(int index) const { return inline_layout_info(index).null_marker_offset(); }\n@@ -408,0 +495,3 @@\n+  Array<u2>* loadable_descriptors() const { return _loadable_descriptors; }\n+  void set_loadable_descriptors(Array<u2>* c) { _loadable_descriptors = c; }\n+\n@@ -503,0 +593,3 @@\n+  \/\/ Check if this klass can be null-free\n+  static void check_can_be_annotated_with_NullRestricted(InstanceKlass* type, Symbol* container_klass_name, TRAPS);\n+\n@@ -538,0 +631,3 @@\n+  static ByteSize kind_offset() { return in_ByteSize(offset_of(InstanceKlass, _kind)); }\n+  static ByteSize misc_flags_offset() { return in_ByteSize(offset_of(InstanceKlass, _misc_flags)); }\n+\n@@ -665,0 +761,2 @@\n+  bool supports_inline_types() const;\n+\n@@ -771,0 +869,7 @@\n+  \/\/ runtime support for strict statics\n+  bool has_strict_static_fields() const     { return _misc_flags.has_strict_static_fields(); }\n+  void set_has_strict_static_fields(bool b) { _misc_flags.set_has_strict_static_fields(b); }\n+  void notify_strict_static_access(int field_index, bool is_writing, TRAPS);\n+  const char* format_strict_static_message(Symbol* field_name, const char* doing_what = nullptr);\n+  void throw_strict_static_exception(Symbol* field_name, const char* when, TRAPS);\n+\n@@ -815,1 +920,1 @@\n-  objArrayOop allocate_objArray(int n, int length, TRAPS);\n+  objArrayOop allocate_objArray(int lenght, ArrayKlass::ArrayProperties props, TRAPS);\n@@ -865,0 +970,3 @@\n+  static ByteSize inline_layout_info_array_offset() { return in_ByteSize(offset_of(InstanceKlass, _inline_layout_info_array)); }\n+  static ByteSize adr_inlineklass_fixed_block_offset() { return in_ByteSize(offset_of(InstanceKlass, _adr_inlineklass_fixed_block)); }\n+\n@@ -922,1 +1030,2 @@\n-                  bool is_interface) {\n+                  bool is_interface,\n+                  bool is_inline_type) {\n@@ -927,1 +1036,2 @@\n-           (is_interface ? (int)sizeof(Klass*)\/wordSize : 0));\n+           (is_interface ? (int)sizeof(Klass*)\/wordSize : 0) +\n+           (is_inline_type ? (int)sizeof(InlineKlassFixedBlock) : 0));\n@@ -933,1 +1043,2 @@\n-                                               is_interface());\n+                                               is_interface(),\n+                                               is_inline_klass());\n@@ -940,0 +1051,1 @@\n+  bool bounds_check(address addr, bool edge_ok = false, intptr_t size_in_bytes = -1) const PRODUCT_RETURN0;\n@@ -946,0 +1058,18 @@\n+  void set_inline_layout_info_array(Array<InlineLayoutInfo>* array) { _inline_layout_info_array = array; }\n+  Array<InlineLayoutInfo>* inline_layout_info_array() const { return _inline_layout_info_array; }\n+  void set_inline_layout_info(int index, InlineLayoutInfo *info) {\n+    assert(_inline_layout_info_array != nullptr ,\"Array not created\");\n+    _inline_layout_info_array->at_put(index, *info);\n+  }\n+  InlineLayoutInfo inline_layout_info(int index) const {\n+    assert(_inline_layout_info_array != nullptr ,\"Array not created\");\n+    return _inline_layout_info_array->at(index);\n+  }\n+  InlineLayoutInfo* inline_layout_info_adr(int index) {\n+    assert(_inline_layout_info_array != nullptr ,\"Array not created\");\n+    return _inline_layout_info_array->adr_at(index);\n+  }\n+\n+  inline InlineKlass* get_inline_type_field_klass(int idx) const ;\n+  inline InlineKlass* get_inline_type_field_klass_or_null(int idx) const;\n+\n@@ -947,1 +1077,1 @@\n-  int size_helper() const {\n+  virtual int size_helper() const {\n@@ -998,0 +1128,1 @@\n+  const char* signature_name_of_carrier(char c) const;\n@@ -1123,1 +1254,1 @@\n-  void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);\n+  virtual void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);\n@@ -1150,1 +1281,2 @@\n-  void oop_print_on      (oop obj, outputStream* st);\n+  void oop_print_on      (oop obj, outputStream* st) { oop_print_on(obj, st, 0, 0); }\n+  void oop_print_on      (oop obj, outputStream* st, int indent = 0, int base_offset = 0);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":141,"deletions":9,"binary":false,"changes":150,"status":"modified"},{"patch":"@@ -66,0 +66,15 @@\n+inline InlineKlass* InstanceKlass::get_inline_type_field_klass(int idx) const {\n+  assert(has_inline_type_fields(), \"Sanity checking\");\n+  assert(idx < java_fields_count(), \"IOOB\");\n+  InlineKlass* k = inline_layout_info(idx).klass();\n+  assert(k != nullptr, \"Should always be set before being read\");\n+  return k;\n+}\n+\n+inline InlineKlass* InstanceKlass::get_inline_type_field_klass_or_null(int idx) const {\n+  assert(has_inline_type_fields(), \"Sanity checking\");\n+  assert(idx < java_fields_count(), \"IOOB\");\n+  InlineKlass* k = inline_layout_info(idx).klass();\n+  return k;\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.inline.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -64,0 +64,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -125,1 +126,0 @@\n-\n@@ -164,0 +164,5 @@\n+address Method::get_c2i_inline_entry() {\n+  assert(adapter() != nullptr, \"must have\");\n+  return adapter()->get_c2i_inline_entry();\n+}\n+\n@@ -169,0 +174,5 @@\n+address Method::get_c2i_unverified_inline_entry() {\n+  assert(adapter() != nullptr, \"must have\");\n+  return adapter()->get_c2i_unverified_inline_entry();\n+}\n+\n@@ -394,1 +404,1 @@\n-  if (!method_holder()->is_rewritten()) {\n+  if (!method_holder()->is_rewritten() || CDSConfig::is_valhalla_preview()) {\n@@ -437,0 +447,2 @@\n+    _from_compiled_inline_entry = _adapter->get_c2i_inline_entry();\n+    _from_compiled_inline_ro_entry = _adapter->get_c2i_inline_ro_entry();\n@@ -726,0 +738,14 @@\n+\/\/ InlineKlass the method is declared to return. This must not\n+\/\/ safepoint as it is called with references live on the stack at\n+\/\/ locations the GC is unaware of.\n+InlineKlass* Method::returns_inline_type() const {\n+  assert(InlineTypeReturnedAsFields, \"Inline types should never be returned as fields\");\n+  if (is_native()) {\n+    return nullptr;\n+  }\n+  NoSafepointVerifier nsv;\n+  SignatureStream ss(signature());\n+  ss.skip_to_return_type();\n+  return ss.as_inline_klass(method_holder());\n+}\n+\n@@ -874,0 +900,5 @@\n+  if (has_scalarized_return()) {\n+    \/\/ Don't treat this as (trivial) getter method because the\n+    \/\/ inline type should be returned in a scalarized form.\n+    return false;\n+  }\n@@ -895,0 +926,5 @@\n+  if (has_scalarized_args()) {\n+    \/\/ Don't treat this as (trivial) setter method because the\n+    \/\/ inline type argument should be passed in a scalarized form.\n+    return false;\n+  }\n@@ -905,6 +941,2 @@\n-          Bytecodes::is_return(java_code_at(last_index)));\n-}\n-\n-bool Method::has_valid_initializer_flags() const {\n-  return (is_static() ||\n-          method_holder()->major_version() < 51);\n+          Bytecodes::is_return(java_code_at(last_index)) &&\n+          !has_scalarized_args());\n@@ -913,1 +945,1 @@\n-bool Method::is_static_initializer() const {\n+bool Method::is_class_initializer() const {\n@@ -917,2 +949,3 @@\n-  return name() == vmSymbols::class_initializer_name() &&\n-         has_valid_initializer_flags();\n+  return (name() == vmSymbols::class_initializer_name() &&\n+          (is_static() ||\n+           method_holder()->major_version() < 51));\n@@ -921,2 +954,3 @@\n-bool Method::is_object_initializer() const {\n-   return name() == vmSymbols::object_initializer_name();\n+\/\/ A method named <init>, is a classic object constructor.\n+bool Method::is_object_constructor() const {\n+  return name() == vmSymbols::object_initializer_name();\n@@ -985,1 +1019,1 @@\n-  if( constants()->tag_at(klass_index).is_unresolved_klass() ) {\n+  if( constants()->tag_at(klass_index).is_unresolved_klass()) {\n@@ -1000,1 +1034,3 @@\n-    if (constants()->tag_at(klass_index).is_unresolved_klass()) return false;\n+    if (constants()->tag_at(klass_index).is_unresolved_klass()) {\n+      return false;\n+    }\n@@ -1169,0 +1205,2 @@\n+    _from_compiled_inline_entry = nullptr;\n+    _from_compiled_inline_ro_entry = nullptr;\n@@ -1171,0 +1209,2 @@\n+    _from_compiled_inline_entry = adapter()->get_c2i_inline_entry();\n+    _from_compiled_inline_ro_entry = adapter()->get_c2i_inline_ro_entry();\n@@ -1204,0 +1244,2 @@\n+  _from_compiled_inline_entry = nullptr;\n+  _from_compiled_inline_ro_entry = nullptr;\n@@ -1235,0 +1277,2 @@\n+  set_has_scalarized_args(false);\n+  set_has_scalarized_return(false);\n@@ -1271,0 +1315,3 @@\n+  if (InlineTypeReturnedAsFields && returns_inline_type() && !has_scalarized_return()) {\n+    set_has_scalarized_return();\n+  }\n@@ -1322,0 +1369,2 @@\n+  mh->_from_compiled_inline_entry = adapter->get_c2i_inline_entry();\n+  mh->_from_compiled_inline_ro_entry = adapter->get_c2i_inline_ro_entry();\n@@ -1338,0 +1387,12 @@\n+address Method::verified_inline_code_entry() {\n+  DEBUG_ONLY(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_entry != nullptr, \"must be set\");\n+  return _from_compiled_inline_entry;\n+}\n+\n+address Method::verified_inline_ro_code_entry() {\n+  DEBUG_ONLY(NoSafepointVerifier nsv;)\n+  assert(_from_compiled_inline_ro_entry != nullptr, \"must be set\");\n+  return _from_compiled_inline_ro_entry;\n+}\n+\n@@ -1369,0 +1430,2 @@\n+  mh->_from_compiled_inline_entry = code->verified_inline_entry_point();\n+  mh->_from_compiled_inline_ro_entry = code->verified_inline_ro_entry_point();\n@@ -1561,0 +1624,2 @@\n+    m->set_from_compiled_inline_entry(m->adapter()->get_c2i_inline_entry());\n+    m->set_from_compiled_inline_ro_entry(m->adapter()->get_c2i_inline_ro_entry());\n@@ -2178,0 +2243,25 @@\n+bool Method::is_scalarized_arg(int idx) const {\n+  if (!has_scalarized_args()) {\n+    return false;\n+  }\n+  \/\/ Search through signature and check if argument is wrapped in T_METADATA\/T_VOID\n+  int depth = 0;\n+  const GrowableArray<SigEntry>* sig = adapter()->get_sig_cc();\n+  for (int i = 0; i < sig->length(); i++) {\n+    BasicType bt = sig->at(i)._bt;\n+    if (bt == T_METADATA) {\n+      depth++;\n+    }\n+    if (idx == 0) {\n+      break; \/\/ Argument found\n+    }\n+    if (bt == T_VOID && (sig->at(i-1)._bt != T_LONG && sig->at(i-1)._bt != T_DOUBLE)) {\n+      depth--;\n+    }\n+    if (depth == 0 && bt != T_LONG && bt != T_DOUBLE) {\n+      idx--; \/\/ Advance to next argument\n+    }\n+  }\n+  return depth != 0;\n+}\n+\n@@ -2203,0 +2293,4 @@\n+#ifdef ASSERT\n+  if (valid_itable_index())\n+    st->print_cr(\" - itable index:      %d\",   itable_index());\n+#endif\n@@ -2210,1 +2304,3 @@\n-  st->print_cr(\" - compiled entry     \" PTR_FORMAT, p2i(from_compiled_entry()));\n+  st->print_cr(\" - compiled entry           \" PTR_FORMAT, p2i(from_compiled_entry()));\n+  st->print_cr(\" - compiled inline entry    \" PTR_FORMAT, p2i(from_compiled_inline_entry()));\n+  st->print_cr(\" - compiled inline ro entry \" PTR_FORMAT, p2i(from_compiled_inline_ro_entry()));\n@@ -2280,0 +2376,1 @@\n+  if (WizardMode) access_flags().print_on(st);\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":113,"deletions":16,"binary":false,"changes":129,"status":"modified"},{"patch":"@@ -96,1 +96,3 @@\n-  volatile address _from_compiled_entry;     \/\/ Cache of: _code ? _code->entry_point() : _adapter->c2i_entry()\n+  volatile address _from_compiled_entry;           \/\/ Cache of: _code ? _code->entry_point() : _adapter->c2i_entry()\n+  volatile address _from_compiled_inline_ro_entry; \/\/ Cache of: _code ? _code->verified_inline_ro_entry_point() : _adapter->c2i_inline_ro_entry()\n+  volatile address _from_compiled_inline_entry;    \/\/ Cache of: _code ? _code->verified_inline_entry_point()    : _adapter->c2i_inline_entry()\n@@ -136,0 +138,2 @@\n+  address from_compiled_inline_ro_entry() const;\n+  address from_compiled_inline_entry() const;\n@@ -364,0 +368,2 @@\n+  address verified_inline_code_entry();\n+  address verified_inline_ro_code_entry();\n@@ -386,1 +392,7 @@\n-    _from_compiled_entry =  entry;\n+    _from_compiled_entry = entry;\n+  }\n+  void set_from_compiled_inline_ro_entry(address entry) {\n+    _from_compiled_inline_ro_entry = entry;\n+  }\n+  void set_from_compiled_inline_entry(address entry) {\n+    _from_compiled_inline_entry = entry;\n@@ -391,0 +403,1 @@\n+  address get_c2i_inline_entry();\n@@ -392,0 +405,1 @@\n+  address get_c2i_unverified_inline_entry();\n@@ -505,1 +519,1 @@\n-  bool is_returning_fp() const                   { BasicType r = result_type(); return (r == T_FLOAT || r == T_DOUBLE); }\n+  InlineKlass* returns_inline_type() const;\n@@ -584,3 +598,0 @@\n-  \/\/ returns true if the method is static OR if the classfile version < 51\n-  bool has_valid_initializer_flags() const;\n-\n@@ -589,1 +600,1 @@\n-  bool is_static_initializer() const;\n+  bool is_class_initializer() const;\n@@ -592,1 +603,1 @@\n-  bool is_object_initializer() const;\n+  bool is_object_constructor() const;\n@@ -617,0 +628,2 @@\n+  static ByteSize from_compiled_inline_offset()  { return byte_offset_of(Method, _from_compiled_inline_entry); }\n+  static ByteSize from_compiled_inline_ro_offset(){ return byte_offset_of(Method, _from_compiled_inline_ro_entry); }\n@@ -618,0 +631,1 @@\n+  static ByteSize flags_offset()                 { return byte_offset_of(Method, _flags); }\n@@ -769,0 +783,11 @@\n+  bool is_scalarized_arg(int idx) const;\n+\n+  bool c1_needs_stack_repair() const { return constMethod()->c1_needs_stack_repair(); }\n+  void set_c1_needs_stack_repair() { constMethod()->set_c1_needs_stack_repair(); }\n+\n+  bool c2_needs_stack_repair() const { return constMethod()->c2_needs_stack_repair(); }\n+  void set_c2_needs_stack_repair() { constMethod()->set_c2_needs_stack_repair(); }\n+\n+  bool mismatch() const { return constMethod()->mismatch(); }\n+  void set_mismatch() { constMethod()->set_mismatch(); }\n+\n","filename":"src\/hotspot\/share\/oops\/method.hpp","additions":33,"deletions":8,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -833,0 +833,6 @@\n+  product(bool, UseArrayLoadStoreProfile, true,                             \\\n+          \"Take advantage of profiling at array load\/store\")                \\\n+                                                                            \\\n+  product(bool, UseACmpProfile, true,                                       \\\n+          \"Take advantage of profiling at acmp\")                            \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -69,0 +70,1 @@\n+#include \"opto\/movenode.hpp\"\n@@ -408,0 +410,3 @@\n+  if (dead->is_InlineType()) {\n+    remove_inline_type(dead);\n+  }\n@@ -455,0 +460,3 @@\n+    if (n->outcnt() == 0) {\n+      worklist.push(n);\n+    }\n@@ -463,0 +471,6 @@\n+  remove_useless_nodes(_inline_type_nodes,  useful); \/\/ remove useless inline type nodes\n+#ifdef ASSERT\n+  if (_modified_nodes != nullptr) {\n+    _modified_nodes->remove_useless_nodes(useful.member_set());\n+  }\n+#endif\n@@ -649,0 +663,1 @@\n+      _has_circular_inline_type(false),\n@@ -668,0 +683,1 @@\n+      _inline_type_nodes (comp_arena(), 8, 0, nullptr),\n@@ -774,4 +790,2 @@\n-      const TypeTuple *domain = StartOSRNode::osr_domain();\n-      const TypeTuple *range = TypeTuple::make_range(method()->signature());\n-      init_tf(TypeFunc::make(domain, range));\n-      StartNode* s = new StartOSRNode(root(), domain);\n+      init_tf(TypeFunc::make(method(), \/* is_osr_compilation = *\/ true));\n+      StartNode* s = new StartOSRNode(root(), tf()->domain_sig());\n@@ -784,1 +798,1 @@\n-      StartNode* s = new StartNode(root(), tf()->domain());\n+      StartNode* s = new StartNode(root(), tf()->domain_cc());\n@@ -885,0 +899,10 @@\n+  if (needs_stack_repair()) {\n+    \/\/ One extra slot for the special stack increment value\n+    next_slot += 2;\n+  }\n+  \/\/ TODO 8284443 Only reserve extra slot if needed\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ One extra slot to hold the null marker for a nullable\n+    \/\/ inline type return if we run out of registers.\n+    next_slot += 2;\n+  }\n@@ -922,0 +946,1 @@\n+      _has_circular_inline_type(false),\n@@ -1074,0 +1099,4 @@\n+  _has_flat_accesses = false;\n+  _flat_accesses_share_alias = true;\n+  _scalarize_in_safepoints = false;\n+\n@@ -1358,0 +1387,9 @@\n+  if (ta && ta->is_not_flat()) {\n+    \/\/ Erase not flat property for alias analysis.\n+    tj = ta = ta->cast_to_not_flat(false);\n+  }\n+  if (ta && ta->is_not_null_free()) {\n+    \/\/ Erase not null free property for alias analysis.\n+    tj = ta = ta->cast_to_not_null_free(false);\n+  }\n+\n@@ -1371,0 +1409,2 @@\n+    \/\/ For flat inline type array, each field has its own slice so\n+    \/\/ we must include the field offset.\n@@ -1411,1 +1451,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n@@ -1415,1 +1455,6 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), ta->field_offset());\n+    }\n+    \/\/ Initially all flattened array accesses share a single slice\n+    if (ta->is_flat() && ta->elem() != TypeInstPtr::BOTTOM && _flat_accesses_share_alias) {\n+      const TypeAry* tary = TypeAry::make(TypeInstPtr::BOTTOM, ta->size(), \/* stable= *\/ false, \/* flat= *\/ true);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));\n@@ -1422,1 +1467,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,Type::Offset(offset), ta->field_offset());\n@@ -1472,1 +1517,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, Type::Offset(offset));\n@@ -1493,1 +1538,1 @@\n-        assert(tj == TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, offset, instance_id), \"exact type should be canonical type\");\n+        assert(tj == TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, Type::Offset(offset), instance_id), \"exact type should be canonical type\");\n@@ -1496,1 +1541,1 @@\n-        tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, offset, instance_id);\n+        tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, is_known_inst, nullptr, Type::Offset(offset), instance_id);\n@@ -1511,1 +1556,1 @@\n-                                       offset);\n+                                       Type::Offset(offset));\n@@ -1517,1 +1562,1 @@\n-        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), offset);\n+        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), Type::Offset(offset));\n@@ -1519,1 +1564,1 @@\n-        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, offset);\n+        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, Type::Offset(offset), tk->is_not_flat(), tk->is_not_null_free(), tk->is_flat(), tk->is_null_free(), tk->is_atomic(), tk->is_aryklassptr()->is_vm_type());\n@@ -1522,1 +1567,0 @@\n-\n@@ -1652,1 +1696,1 @@\n-Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {\n+Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {\n@@ -1657,3 +1701,6 @@\n-  AliasCacheEntry* ace = probe_alias_cache(adr_type);\n-  if (ace->_adr_type == adr_type) {\n-    return alias_type(ace->_index);\n+  AliasCacheEntry* ace = nullptr;\n+  if (!uncached) {\n+    ace = probe_alias_cache(adr_type);\n+    if (ace->_adr_type == adr_type) {\n+      return alias_type(ace->_index);\n+    }\n@@ -1709,0 +1756,1 @@\n+    ciField* field = nullptr;\n@@ -1715,0 +1763,1 @@\n+      const Type* elemtype = flat->is_aryptr()->elem();\n@@ -1716,1 +1765,8 @@\n-        alias_type(idx)->set_element(flat->is_aryptr()->elem());\n+        alias_type(idx)->set_element(elemtype);\n+      }\n+      int field_offset = flat->is_aryptr()->field_offset().get();\n+      if (flat->is_flat() &&\n+          field_offset != Type::OffsetBot) {\n+        ciInlineKlass* vk = elemtype->inline_klass();\n+        field_offset += vk->payload_offset();\n+        field = vk->get_field_by_offset(field_offset, false);\n@@ -1732,0 +1788,2 @@\n+      if (flat->offset() == in_bytes(Klass::layout_helper_offset()))\n+        alias_type(idx)->set_rewritable(false);\n@@ -1742,1 +1800,0 @@\n-      ciField* field;\n@@ -1749,0 +1806,4 @@\n+      } else if (tinst->is_inlinetypeptr()) {\n+        \/\/ Inline type field\n+        ciInlineKlass* vk = tinst->inline_klass();\n+        field = vk->get_field_by_offset(tinst->offset(), false);\n@@ -1753,7 +1814,14 @@\n-      assert(field == nullptr ||\n-             original_field == nullptr ||\n-             (field->holder() == original_field->holder() &&\n-              field->offset_in_bytes() == original_field->offset_in_bytes() &&\n-              field->is_static() == original_field->is_static()), \"wrong field?\");\n-      \/\/ Set field() and is_rewritable() attributes.\n-      if (field != nullptr)  alias_type(idx)->set_field(field);\n+    }\n+    assert(field == nullptr ||\n+           original_field == nullptr ||\n+           (field->holder() == original_field->holder() &&\n+            field->offset_in_bytes() == original_field->offset_in_bytes() &&\n+            field->is_static() == original_field->is_static()), \"wrong field?\");\n+    \/\/ Set field() and is_rewritable() attributes.\n+    if (field != nullptr) {\n+      alias_type(idx)->set_field(field);\n+      if (flat->isa_aryptr()) {\n+        \/\/ Fields of flat arrays are rewritable although they are declared final\n+        assert(flat->is_flat(), \"must be a flat array\");\n+        alias_type(idx)->set_rewritable(true);\n+      }\n@@ -1764,3 +1832,4 @@\n-  ace->_adr_type = adr_type;\n-  ace->_index    = idx;\n-  assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n+  if (!uncached) {\n+    ace->_adr_type = adr_type;\n+    ace->_index    = idx;\n+    assert(alias_type(adr_type) == alias_type(idx),  \"type must be installed\");\n@@ -1768,6 +1837,7 @@\n-  \/\/ Might as well try to fill the cache for the flattened version, too.\n-  AliasCacheEntry* face = probe_alias_cache(flat);\n-  if (face->_adr_type == nullptr) {\n-    face->_adr_type = flat;\n-    face->_index    = idx;\n-    assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    \/\/ Might as well try to fill the cache for the flattened version, too.\n+    AliasCacheEntry* face = probe_alias_cache(flat);\n+    if (face->_adr_type == nullptr) {\n+      face->_adr_type = flat;\n+      face->_index    = idx;\n+      assert(alias_type(flat) == alias_type(idx), \"flat type must work too\");\n+    }\n@@ -1895,0 +1965,398 @@\n+void Compile::add_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  _inline_type_nodes.push(n);\n+}\n+\n+void Compile::remove_inline_type(Node* n) {\n+  assert(n->is_InlineType(), \"unexpected node\");\n+  if (_inline_type_nodes.contains(n)) {\n+    _inline_type_nodes.remove(n);\n+  }\n+}\n+\n+\/\/ Does the return value keep otherwise useless inline type allocations alive?\n+static bool return_val_keeps_allocations_alive(Node* ret_val) {\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(ret_val);\n+  bool some_allocations = false;\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->outcnt() > 1) {\n+      \/\/ Some other use for the allocation\n+      return false;\n+    } else if (n->is_InlineType()) {\n+      wq.push(n->in(1));\n+    } else if (n->is_Phi()) {\n+      for (uint j = 1; j < n->req(); j++) {\n+        wq.push(n->in(j));\n+      }\n+    } else if (n->is_CheckCastPP() &&\n+               n->in(1)->is_Proj() &&\n+               n->in(1)->in(0)->is_Allocate()) {\n+      some_allocations = true;\n+    } else if (n->is_CheckCastPP()) {\n+      wq.push(n->in(1));\n+    }\n+  }\n+  return some_allocations;\n+}\n+\n+void Compile::process_inline_types(PhaseIterGVN &igvn, bool remove) {\n+  \/\/ Make sure that the return value does not keep an otherwise unused allocation alive\n+  if (tf()->returns_inline_type_as_fields()) {\n+    Node* ret = nullptr;\n+    for (uint i = 1; i < root()->req(); i++) {\n+      Node* in = root()->in(i);\n+      if (in->Opcode() == Op_Return) {\n+        assert(ret == nullptr, \"only one return\");\n+        ret = in;\n+      }\n+    }\n+    if (ret != nullptr) {\n+      Node* ret_val = ret->in(TypeFunc::Parms);\n+      if (igvn.type(ret_val)->isa_oopptr() &&\n+          return_val_keeps_allocations_alive(ret_val)) {\n+        igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)->inline_klass(), igvn));\n+        assert(ret_val->outcnt() == 0, \"should be dead now\");\n+        igvn.remove_dead_node(ret_val);\n+      }\n+    }\n+  }\n+  if (_inline_type_nodes.length() == 0) {\n+    return;\n+  }\n+  \/\/ Scalarize inline types in safepoint debug info.\n+  \/\/ Delay this until all inlining is over to avoid getting inconsistent debug info.\n+  set_scalarize_in_safepoints(true);\n+  for (int i = _inline_type_nodes.length()-1; i >= 0; i--) {\n+    InlineTypeNode* vt = _inline_type_nodes.at(i)->as_InlineType();\n+    vt->make_scalar_in_safepoints(&igvn);\n+    igvn.record_for_igvn(vt);\n+  }\n+  if (remove) {\n+    \/\/ Remove inline type nodes by replacing them with their oop input\n+    while (_inline_type_nodes.length() > 0) {\n+      InlineTypeNode* vt = _inline_type_nodes.pop()->as_InlineType();\n+      if (vt->outcnt() == 0) {\n+        igvn.remove_dead_node(vt);\n+        continue;\n+      }\n+      for (DUIterator i = vt->outs(); vt->has_out(i); i++) {\n+        DEBUG_ONLY(bool must_be_buffered = false);\n+        Node* u = vt->out(i);\n+        \/\/ Check if any users are blackholes. If so, rewrite them to use either the\n+        \/\/ allocated buffer, or individual components, instead of the inline type node\n+        \/\/ that goes away.\n+        if (u->is_Blackhole()) {\n+          BlackholeNode* bh = u->as_Blackhole();\n+\n+          \/\/ Unlink the old input\n+          int idx = bh->find_edge(vt);\n+          assert(idx != -1, \"The edge should be there\");\n+          bh->del_req(idx);\n+          --i;\n+\n+          if (vt->is_allocated(&igvn)) {\n+            \/\/ Already has the allocated instance, blackhole that\n+            bh->add_req(vt->get_oop());\n+          } else {\n+            \/\/ Not allocated yet, blackhole the components\n+            for (uint c = 0; c < vt->field_count(); c++) {\n+              bh->add_req(vt->field_value(c));\n+            }\n+          }\n+\n+          \/\/ Node modified, record for IGVN\n+          igvn.record_for_igvn(bh);\n+        }\n+#ifdef ASSERT\n+        \/\/ Verify that inline type is buffered when replacing by oop\n+        else if (u->is_InlineType()) {\n+          \/\/ InlineType uses don't need buffering because they are about to be replaced as well\n+        } else if (u->is_Phi()) {\n+          \/\/ TODO 8302217 Remove this once InlineTypeNodes are reliably pushed through\n+        } else {\n+          must_be_buffered = true;\n+        }\n+        if (must_be_buffered && !vt->is_allocated(&igvn)) {\n+          vt->dump(0);\n+          u->dump(0);\n+          assert(false, \"Should have been buffered\");\n+        }\n+#endif\n+      }\n+      igvn.replace_node(vt, vt->get_oop());\n+    }\n+  }\n+  igvn.optimize();\n+}\n+\n+void Compile::adjust_flat_array_access_aliases(PhaseIterGVN& igvn) {\n+  if (!_has_flat_accesses) {\n+    return;\n+  }\n+  \/\/ Initially, all flat array accesses share the same slice to\n+  \/\/ keep dependencies with Object[] array accesses (that could be\n+  \/\/ to a flat array) correct. We're done with parsing so we\n+  \/\/ now know all flat array accesses in this compile\n+  \/\/ unit. Let's move flat array accesses to their own slice,\n+  \/\/ one per element field. This should help memory access\n+  \/\/ optimizations.\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(root());\n+\n+  Node_List mergememnodes;\n+  Node_List memnodes;\n+\n+  \/\/ Alias index currently shared by all flat memory accesses\n+  int index = get_alias_index(TypeAryPtr::INLINES);\n+\n+  \/\/ Find MergeMem nodes and flat array accesses\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* n = wq.at(i);\n+    if (n->is_Mem()) {\n+      const TypePtr* adr_type = nullptr;\n+      adr_type = get_adr_type(get_alias_index(n->adr_type()));\n+      if (adr_type == TypeAryPtr::INLINES) {\n+        memnodes.push(n);\n+      }\n+    } else if (n->is_MergeMem()) {\n+      MergeMemNode* mm = n->as_MergeMem();\n+      if (mm->memory_at(index) != mm->base_memory()) {\n+        mergememnodes.push(n);\n+      }\n+    }\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* m = n->in(j);\n+      if (m != nullptr) {\n+        wq.push(m);\n+      }\n+    }\n+  }\n+\n+  if (memnodes.size() > 0) {\n+    _flat_accesses_share_alias = false;\n+\n+    \/\/ We are going to change the slice for the flat array\n+    \/\/ accesses so we need to clear the cache entries that refer to\n+    \/\/ them.\n+    for (uint i = 0; i < AliasCacheSize; i++) {\n+      AliasCacheEntry* ace = &_alias_cache[i];\n+      if (ace->_adr_type != nullptr &&\n+          ace->_adr_type->is_flat()) {\n+        ace->_adr_type = nullptr;\n+        ace->_index = (i != 0) ? 0 : AliasIdxTop; \/\/ Make sure the nullptr adr_type resolves to AliasIdxTop\n+      }\n+    }\n+\n+    \/\/ Find what aliases we are going to add\n+    int start_alias = num_alias_types()-1;\n+    int stop_alias = 0;\n+\n+    for (uint i = 0; i < memnodes.size(); i++) {\n+      Node* m = memnodes.at(i);\n+      const TypePtr* adr_type = nullptr;\n+      adr_type = m->adr_type();\n+#ifdef ASSERT\n+      m->as_Mem()->set_adr_type(adr_type);\n+#endif\n+      int idx = get_alias_index(adr_type);\n+      start_alias = MIN2(start_alias, idx);\n+      stop_alias = MAX2(stop_alias, idx);\n+    }\n+\n+    assert(stop_alias >= start_alias, \"should have expanded aliases\");\n+\n+    Node_Stack stack(0);\n+#ifdef ASSERT\n+    VectorSet seen(Thread::current()->resource_area());\n+#endif\n+    \/\/ Now let's fix the memory graph so each flat array access\n+    \/\/ is moved to the right slice. Start from the MergeMem nodes.\n+    uint last = unique();\n+    for (uint i = 0; i < mergememnodes.size(); i++) {\n+      MergeMemNode* current = mergememnodes.at(i)->as_MergeMem();\n+      Node* n = current->memory_at(index);\n+      MergeMemNode* mm = nullptr;\n+      do {\n+        \/\/ Follow memory edges through memory accesses, phis and\n+        \/\/ narrow membars and push nodes on the stack. Once we hit\n+        \/\/ bottom memory, we pop element off the stack one at a\n+        \/\/ time, in reverse order, and move them to the right slice\n+        \/\/ by changing their memory edges.\n+        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() || n->adr_type() == TypeAryPtr::INLINES) {\n+          assert(!seen.test_set(n->_idx), \"\");\n+          \/\/ Uses (a load for instance) will need to be moved to the\n+          \/\/ right slice as well and will get a new memory state\n+          \/\/ that we don't know yet. The use could also be the\n+          \/\/ backedge of a loop. We put a place holder node between\n+          \/\/ the memory node and its uses. We replace that place\n+          \/\/ holder with the correct memory state once we know it,\n+          \/\/ i.e. when nodes are popped off the stack. Using the\n+          \/\/ place holder make the logic work in the presence of\n+          \/\/ loops.\n+          if (n->outcnt() > 1) {\n+            Node* place_holder = nullptr;\n+            assert(!n->has_out_with(Op_Node), \"\");\n+            for (DUIterator k = n->outs(); n->has_out(k); k++) {\n+              Node* u = n->out(k);\n+              if (u != current && u->_idx < last) {\n+                bool success = false;\n+                for (uint l = 0; l < u->req(); l++) {\n+                  if (!stack.is_empty() && u == stack.node() && l == stack.index()) {\n+                    continue;\n+                  }\n+                  Node* in = u->in(l);\n+                  if (in == n) {\n+                    if (place_holder == nullptr) {\n+                      place_holder = new Node(1);\n+                      place_holder->init_req(0, n);\n+                    }\n+                    igvn.replace_input_of(u, l, place_holder);\n+                    success = true;\n+                  }\n+                }\n+                if (success) {\n+                  --k;\n+                }\n+              }\n+            }\n+          }\n+          if (n->is_Phi()) {\n+            stack.push(n, 1);\n+            n = n->in(1);\n+          } else if (n->is_Mem()) {\n+            stack.push(n, n->req());\n+            n = n->in(MemNode::Memory);\n+          } else {\n+            assert(n->is_Proj() && n->in(0)->Opcode() == Op_MemBarCPUOrder, \"\");\n+            stack.push(n, n->req());\n+            n = n->in(0)->in(TypeFunc::Memory);\n+          }\n+        } else {\n+          assert(n->adr_type() == TypePtr::BOTTOM || (n->Opcode() == Op_Node && n->_idx >= last) || (n->is_Proj() && n->in(0)->is_Initialize()), \"\");\n+          \/\/ Build a new MergeMem node to carry the new memory state\n+          \/\/ as we build it. IGVN should fold extraneous MergeMem\n+          \/\/ nodes.\n+          mm = MergeMemNode::make(n);\n+          igvn.register_new_node_with_optimizer(mm);\n+          while (stack.size() > 0) {\n+            Node* m = stack.node();\n+            uint idx = stack.index();\n+            if (m->is_Mem()) {\n+              \/\/ Move memory node to its new slice\n+              const TypePtr* adr_type = m->adr_type();\n+              int alias = get_alias_index(adr_type);\n+              Node* prev = mm->memory_at(alias);\n+              igvn.replace_input_of(m, MemNode::Memory, prev);\n+              mm->set_memory_at(alias, m);\n+            } else if (m->is_Phi()) {\n+              \/\/ We need as many new phis as there are new aliases\n+              igvn.replace_input_of(m, idx, mm);\n+              if (idx == m->req()-1) {\n+                Node* r = m->in(0);\n+                for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                  const TypePtr* adr_type = get_adr_type(j);\n+                  if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                    continue;\n+                  }\n+                  Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));\n+                  igvn.register_new_node_with_optimizer(phi);\n+                  for (uint k = 1; k < m->req(); k++) {\n+                    phi->init_req(k, m->in(k)->as_MergeMem()->memory_at(j));\n+                  }\n+                  mm->set_memory_at(j, phi);\n+                }\n+                Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+                igvn.register_new_node_with_optimizer(base_phi);\n+                for (uint k = 1; k < m->req(); k++) {\n+                  base_phi->init_req(k, m->in(k)->as_MergeMem()->base_memory());\n+                }\n+                mm->set_base_memory(base_phi);\n+              }\n+            } else {\n+              \/\/ This is a MemBarCPUOrder node from\n+              \/\/ Parse::array_load()\/Parse::array_store(), in the\n+              \/\/ branch that handles flat arrays hidden under\n+              \/\/ an Object[] array. We also need one new membar per\n+              \/\/ new alias to keep the unknown access that the\n+              \/\/ membars protect properly ordered with accesses to\n+              \/\/ known flat array.\n+              assert(m->is_Proj(), \"projection expected\");\n+              Node* ctrl = m->in(0)->in(TypeFunc::Control);\n+              igvn.replace_input_of(m->in(0), TypeFunc::Control, top());\n+              for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+                const TypePtr* adr_type = get_adr_type(j);\n+                if (!adr_type->isa_aryptr() || !adr_type->is_flat() || j == (uint)index) {\n+                  continue;\n+                }\n+                MemBarNode* mb = new MemBarCPUOrderNode(this, j, nullptr);\n+                igvn.register_new_node_with_optimizer(mb);\n+                Node* mem = mm->memory_at(j);\n+                mb->init_req(TypeFunc::Control, ctrl);\n+                mb->init_req(TypeFunc::Memory, mem);\n+                ctrl = new ProjNode(mb, TypeFunc::Control);\n+                igvn.register_new_node_with_optimizer(ctrl);\n+                mem = new ProjNode(mb, TypeFunc::Memory);\n+                igvn.register_new_node_with_optimizer(mem);\n+                mm->set_memory_at(j, mem);\n+              }\n+              igvn.replace_node(m->in(0)->as_Multi()->proj_out(TypeFunc::Control), ctrl);\n+            }\n+            if (idx < m->req()-1) {\n+              idx += 1;\n+              stack.set_index(idx);\n+              n = m->in(idx);\n+              break;\n+            }\n+            \/\/ Take care of place holder nodes\n+            if (m->has_out_with(Op_Node)) {\n+              Node* place_holder = m->find_out_with(Op_Node);\n+              if (place_holder != nullptr) {\n+                Node* mm_clone = mm->clone();\n+                igvn.register_new_node_with_optimizer(mm_clone);\n+                Node* hook = new Node(1);\n+                hook->init_req(0, mm);\n+                igvn.replace_node(place_holder, mm_clone);\n+                hook->destruct(&igvn);\n+              }\n+              assert(!m->has_out_with(Op_Node), \"place holder should be gone now\");\n+            }\n+            stack.pop();\n+          }\n+        }\n+      } while(stack.size() > 0);\n+      \/\/ Fix the memory state at the MergeMem we started from\n+      igvn.rehash_node_delayed(current);\n+      for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {\n+        const TypePtr* adr_type = get_adr_type(j);\n+        if (!adr_type->isa_aryptr() || !adr_type->is_flat()) {\n+          continue;\n+        }\n+        current->set_memory_at(j, mm);\n+      }\n+      current->set_memory_at(index, current->base_memory());\n+    }\n+    igvn.optimize();\n+  }\n+  print_method(PHASE_SPLIT_INLINES_ARRAY, 2);\n+#ifdef ASSERT\n+  if (!_flat_accesses_share_alias) {\n+    wq.clear();\n+    wq.push(root());\n+    for (uint i = 0; i < wq.size(); i++) {\n+      Node* n = wq.at(i);\n+      assert(n->adr_type() != TypeAryPtr::INLINES, \"should have been removed from the graph\");\n+      for (uint j = 0; j < n->req(); j++) {\n+        Node* m = n->in(j);\n+        if (m != nullptr) {\n+          wq.push(m);\n+        }\n+      }\n+    }\n+  }\n+#endif\n+}\n+\n@@ -2013,1 +2481,1 @@\n-        if (!live_locals.at(i) && !local->is_top() && local != lhs && local!= rhs) {\n+        if (!live_locals.at(i) && !local->is_top() && local != lhs && local != rhs) {\n@@ -2028,1 +2496,1 @@\n-    \/\/ keep the mondified trap for late query\n+    \/\/ keep the modified trap for late query\n@@ -2234,1 +2702,4 @@\n-  assert(_modified_nodes == nullptr, \"not allowed\");\n+#ifdef ASSERT\n+  Unique_Node_List* modified_nodes = _modified_nodes;\n+  _modified_nodes = nullptr;\n+#endif\n@@ -2247,0 +2718,1 @@\n+  DEBUG_ONLY( _modified_nodes = modified_nodes; )\n@@ -2391,0 +2863,5 @@\n+  \/\/ Process inline type nodes now that all inlining is over\n+  process_inline_types(igvn);\n+\n+  adjust_flat_array_access_aliases(igvn);\n+\n@@ -2393,0 +2870,11 @@\n+  if (C->macro_count() > 0) {\n+    \/\/ Eliminate some macro nodes before EA to reduce analysis pressure\n+    PhaseMacroExpand mexp(igvn);\n+    mexp.eliminate_macro_nodes(\/* eliminate_locks= *\/ false);\n+    if (failing()) {\n+      return;\n+    }\n+    igvn.set_delay_transform(false);\n+    print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);\n+  }\n+\n@@ -2403,1 +2891,14 @@\n-      if (failing())  return;\n+      if (failing()) {\n+        return;\n+      }\n+      print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);\n+      if (C->macro_count() > 0) {\n+        \/\/ Eliminate some macro nodes before EA to reduce analysis pressure\n+        PhaseMacroExpand mexp(igvn);\n+        mexp.eliminate_macro_nodes(\/* eliminate_locks= *\/ false);\n+        if (failing()) {\n+          return;\n+        }\n+        igvn.set_delay_transform(false);\n+        print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);\n+      }\n@@ -2405,0 +2906,1 @@\n+\n@@ -2406,1 +2908,0 @@\n-    print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);\n@@ -2424,1 +2925,3 @@\n-        if (failing()) return;\n+        if (failing()) {\n+          return;\n+        }\n@@ -2428,3 +2931,0 @@\n-        igvn.optimize();\n-        if (failing()) return;\n-\n@@ -2521,0 +3021,8 @@\n+  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n+\n+  if (_late_inlines.length() > 0) {\n+    \/\/ More opportunities to optimize virtual and MH calls.\n+    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n+    process_late_inline_calls_no_inline(igvn);\n+  }\n+\n@@ -2523,0 +3031,7 @@\n+    PhaseMacroExpand mex(igvn);\n+    \/\/ Last attempt to eliminate macro nodes.\n+    mex.eliminate_macro_nodes();\n+    if (failing()) {\n+      return;\n+    }\n+\n@@ -2524,1 +3039,0 @@\n-    PhaseMacroExpand  mex(igvn);\n@@ -2544,0 +3058,4 @@\n+  \/\/ Process inline type nodes again and remove them. From here\n+  \/\/ on we don't need to keep track of field values anymore.\n+  process_inline_types(igvn, \/* remove= *\/ true);\n+\n@@ -2560,0 +3078,1 @@\n+  DEBUG_ONLY( _late_inlines.clear(); )\n@@ -2562,9 +3081,0 @@\n-\n-  assert(_late_inlines.length() == 0 || IncrementalInlineMH || IncrementalInlineVirtual, \"not empty\");\n-\n-  if (_late_inlines.length() > 0) {\n-    \/\/ More opportunities to optimize virtual and MH calls.\n-    \/\/ Though it's maybe too late to perform inlining, strength-reducing them to direct calls is still an option.\n-    process_late_inline_calls_no_inline(igvn);\n-    if (failing())  return;\n-  }\n@@ -3343,0 +3853,1 @@\n+  case Op_StoreLSpecial:\n@@ -3886,0 +4397,5 @@\n+  case Op_InlineType: {\n+    n->dump(-1);\n+    assert(false, \"inline type node was not removed\");\n+    break;\n+  }\n@@ -4261,2 +4777,2 @@\n-      if (accessing_method->is_static_initializer() ||\n-          accessing_method->is_object_initializer() ||\n+      if (accessing_method->is_class_initializer() ||\n+          accessing_method->is_object_constructor() ||\n@@ -4270,1 +4786,1 @@\n-      if (accessing_method->is_static_initializer()) {\n+      if (accessing_method->is_class_initializer()) {\n@@ -4340,0 +4856,1 @@\n+               (n->is_Allocate() && i >= AllocateNode::InlineType) ||\n@@ -4342,1 +4859,1 @@\n-              \"only region, phi, arraycopy, unlock or membar nodes have null data edges\");\n+              \"only region, phi, arraycopy, allocate, unlock or membar nodes have null data edges\");\n@@ -4493,0 +5010,7 @@\n+\n+    \/\/ Do not fold the subtype check to an array klass pointer comparison for null-able inline type arrays\n+    \/\/ because null-free [LMyValue <: null-able [LMyValue but the klasses are different. Perform a full test.\n+    if (!superk->is_aryklassptr()->is_null_free() && superk->is_aryklassptr()->elem()->isa_instklassptr() &&\n+        superk->is_aryklassptr()->elem()->is_instklassptr()->instance_klass()->is_inlinetype()) {\n+      return SSC_full_test;\n+    }\n@@ -4976,0 +5500,21 @@\n+Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {\n+  const TypeInstPtr* ta = phase->type(a)->isa_instptr();\n+  const TypeInstPtr* tb = phase->type(b)->isa_instptr();\n+  if (!EnableValhalla || ta == nullptr || tb == nullptr ||\n+      ta->is_zero_type() || tb->is_zero_type() ||\n+      !ta->can_be_inline_type() || !tb->can_be_inline_type()) {\n+    \/\/ Use old acmp if one operand is null or not an inline type\n+    return new CmpPNode(a, b);\n+  } else if (ta->is_inlinetypeptr() || tb->is_inlinetypeptr()) {\n+    \/\/ We know that one operand is an inline type. Therefore,\n+    \/\/ new acmp will only return true if both operands are nullptr.\n+    \/\/ Check if both operands are null by or'ing the oops.\n+    a = phase->transform(new CastP2XNode(nullptr, a));\n+    b = phase->transform(new CastP2XNode(nullptr, b));\n+    a = phase->transform(new OrXNode(a, b));\n+    return new CmpXNode(a, phase->MakeConX(0));\n+  }\n+  \/\/ Use new acmp\n+  return nullptr;\n+}\n+\n@@ -5331,0 +5876,2 @@\n+  } else if (bt == T_FLOAT) {\n+    result = new MoveI2FNode(value);\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":607,"deletions":60,"binary":false,"changes":667,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -68,0 +69,1 @@\n+\/\/------------------------------Ideal------------------------------------------\n@@ -69,0 +71,6 @@\n+  if (in(1)->is_InlineType()) {\n+    \/\/ Null checking a scalarized but nullable inline type. Check the null marker\n+    \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+    set_req_X(1, in(1)->as_InlineType()->get_null_marker(), phase);\n+    return this;\n+  }\n","filename":"src\/hotspot\/share\/opto\/convertnode.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1621,5 +1621,4 @@\n-  CallProjections projs;\n-  extract_projections(&projs, false, false);\n-  phase->replace_node(projs.fallthrough_proj, in(TypeFunc::Control));\n-  if (projs.fallthrough_catchproj != nullptr) {\n-    phase->replace_node(projs.fallthrough_catchproj, in(TypeFunc::Control));\n+  CallProjections* projs = extract_projections(false, false);\n+  phase->replace_node(projs->fallthrough_proj, in(TypeFunc::Control));\n+  if (projs->fallthrough_catchproj != nullptr) {\n+    phase->replace_node(projs->fallthrough_catchproj, in(TypeFunc::Control));\n@@ -1627,2 +1626,2 @@\n-  if (projs.fallthrough_memproj != nullptr) {\n-    phase->replace_node(projs.fallthrough_memproj, in(TypeFunc::Memory));\n+  if (projs->fallthrough_memproj != nullptr) {\n+    phase->replace_node(projs->fallthrough_memproj, in(TypeFunc::Memory));\n@@ -1630,2 +1629,2 @@\n-  if (projs.catchall_memproj != nullptr) {\n-    phase->replace_node(projs.catchall_memproj, C->top());\n+  if (projs->catchall_memproj != nullptr) {\n+    phase->replace_node(projs->catchall_memproj, C->top());\n@@ -1633,2 +1632,2 @@\n-  if (projs.fallthrough_ioproj != nullptr) {\n-    phase->replace_node(projs.fallthrough_ioproj, in(TypeFunc::I_O));\n+  if (projs->fallthrough_ioproj != nullptr) {\n+    phase->replace_node(projs->fallthrough_ioproj, in(TypeFunc::I_O));\n@@ -1636,4 +1635,4 @@\n-  assert(projs.catchall_ioproj == nullptr, \"no exceptions from floating mod\");\n-  assert(projs.catchall_catchproj == nullptr, \"no exceptions from floating mod\");\n-  if (projs.resproj != nullptr) {\n-    phase->replace_node(projs.resproj, con_node);\n+  assert(projs->catchall_ioproj == nullptr, \"no exceptions from floating mod\");\n+  assert(projs->catchall_catchproj == nullptr, \"no exceptions from floating mod\");\n+  if (projs->resproj[0] != nullptr) {\n+    phase->replace_node(projs->resproj[0], con_node);\n@@ -1699,1 +1698,1 @@\n-Node *DivModINode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *DivModINode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -1714,1 +1713,1 @@\n-Node *DivModLNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *DivModLNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -1752,1 +1751,1 @@\n-Node* UDivModINode::match( const ProjNode *proj, const Matcher *match ) {\n+Node* UDivModINode::match(const ProjNode* proj, const Matcher* match, const RegMask* mask) {\n@@ -1767,1 +1766,1 @@\n-Node* UDivModLNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node* UDivModLNode::match( const ProjNode* proj, const Matcher* match, const RegMask* mask) {\n","filename":"src\/hotspot\/share\/opto\/divnode.cpp","additions":18,"deletions":19,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -111,2 +112,3 @@\n-    if (!stopped() && result() != nullptr) {\n-      if (result()->is_top()) {\n+    Node* res = result();\n+    if (!stopped() && res != nullptr) {\n+      if (res->is_top()) {\n@@ -116,2 +118,9 @@\n-      BasicType bt = result()->bottom_type()->basic_type();\n-      push_node(bt, result());\n+      BasicType bt = res->bottom_type()->basic_type();\n+      if (C->inlining_incrementally() && res->is_InlineType()) {\n+        \/\/ The caller expects an oop when incrementally inlining an intrinsic that returns an\n+        \/\/ inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.\n+        PreserveReexecuteState preexecs(this);\n+        jvms()->set_should_reexecute(true);\n+        res = res->as_InlineType()->buffer(this);\n+      }\n+      push_node(bt, res);\n@@ -148,1 +157,0 @@\n-  Node* load_mirror_from_klass(Node* klass);\n@@ -166,0 +174,2 @@\n+  Node* load_default_array_klass(Node* klass_node);\n+\n@@ -172,0 +182,9 @@\n+\n+  enum ArrayKind {\n+    AnyArray,\n+    NonArray,\n+    RefArray,\n+    NonRefArray,\n+    TypeArray\n+  };\n+\n@@ -173,0 +192,1 @@\n+\n@@ -174,1 +194,1 @@\n-    return generate_array_guard_common(kls, region, false, false, obj);\n+    return generate_array_guard_common(kls, region, AnyArray, obj);\n@@ -177,1 +197,4 @@\n-    return generate_array_guard_common(kls, region, false, true, obj);\n+    return generate_array_guard_common(kls, region, NonArray, obj);\n+  }\n+  Node* generate_refArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n+    return generate_array_guard_common(kls, region, RefArray, obj);\n@@ -179,2 +202,2 @@\n-  Node* generate_objArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n-    return generate_array_guard_common(kls, region, true, false, obj);\n+  Node* generate_non_refArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n+    return generate_array_guard_common(kls, region, NonRefArray, obj);\n@@ -182,2 +205,2 @@\n-  Node* generate_non_objArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n-    return generate_array_guard_common(kls, region, true, true, obj);\n+  Node* generate_typeArray_guard(Node* kls, RegionNode* region, Node** obj = nullptr) {\n+    return generate_array_guard_common(kls, region, TypeArray, obj);\n@@ -185,2 +208,1 @@\n-  Node* generate_array_guard_common(Node* kls, RegionNode* region,\n-                                    bool obj_array, bool not_array, Node** obj = nullptr);\n+  Node* generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind, Node** obj = nullptr);\n@@ -234,1 +256,2 @@\n-  bool inline_unsafe_access(bool is_store, BasicType type, AccessKind kind, bool is_unaligned);\n+  bool inline_unsafe_access(bool is_store, BasicType type, AccessKind kind, bool is_unaligned, bool is_flat = false);\n+  bool inline_unsafe_flat_access(bool is_store, AccessKind kind);\n@@ -238,0 +261,1 @@\n+  bool inline_newArray(bool null_free, bool atomic);\n@@ -241,0 +265,2 @@\n+  bool inline_unsafe_make_private_buffer();\n+  bool inline_unsafe_finish_private_buffer();\n@@ -267,0 +293,1 @@\n+  bool inline_primitive_Class_conversion(vmIntrinsics::ID id);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":41,"deletions":14,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -25,0 +25,2 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n+#include \"ci\/ciInstanceKlass.hpp\"\n@@ -38,0 +40,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -55,0 +58,2 @@\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -84,12 +89,0 @@\n-void PhaseMacroExpand::migrate_outs(Node *old, Node *target) {\n-  assert(old != nullptr, \"sanity\");\n-  for (DUIterator_Fast imax, i = old->fast_outs(imax); i < imax; i++) {\n-    Node* use = old->fast_out(i);\n-    _igvn.rehash_node_delayed(use);\n-    imax -= replace_input(use, old, target);\n-    \/\/ back up iterator\n-    --i;\n-  }\n-  assert(old->outcnt() == 0, \"all uses must be deleted\");\n-}\n-\n@@ -158,1 +151,1 @@\n-  bs->eliminate_gc_barrier(this, p2x);\n+  bs->eliminate_gc_barrier(&_igvn, p2x);\n@@ -213,1 +206,1 @@\n-        int adr_offset = atype->offset();\n+        int adr_offset = atype->flat_offset();\n@@ -258,1 +251,1 @@\n-   } else if (mem->Opcode() == Op_StrInflatedCopy) {\n+    } else if (mem->Opcode() == Op_StrInflatedCopy) {\n@@ -303,1 +296,5 @@\n-      const TypePtr* adr_type = nullptr;\n+      Node* base = ac->in(ArrayCopyNode::Src);\n+      const TypeAryPtr* adr_type = _igvn.type(base)->is_aryptr();\n+      if (adr_type->is_flat()) {\n+        shift = adr_type->flat_log_elem_size();\n+      }\n@@ -306,2 +303,2 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(off);\n+        adr_type = _igvn.type(adr)->is_aryptr();\n+        assert(adr_type == _igvn.type(base)->is_aryptr()->add_field_offset_and_offset(off), \"incorrect address type\");\n@@ -311,1 +308,1 @@\n-          return value_from_mem(ac->in(TypeFunc::Memory), ctl, ft, ftype, adr_type->isa_oopptr(), alloc);\n+          return value_from_mem(ac->in(TypeFunc::Memory), ctl, ft, ftype, adr_type, alloc);\n@@ -314,0 +311,5 @@\n+        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n+          \/\/ Non constant offset in the array: we can't statically\n+          \/\/ determine the value\n+          return nullptr;\n+        }\n@@ -321,7 +323,5 @@\n-        Node* base = ac->in(ArrayCopyNode::Src);\n-        adr_type = _igvn.type(base)->is_ptr()->add_offset(Type::OffsetBot);\n-        if (ac->in(ArrayCopyNode::Src) == ac->in(ArrayCopyNode::Dest)) {\n-          \/\/ Non constant offset in the array: we can't statically\n-          \/\/ determine the value\n-          return nullptr;\n-        }\n+        \/\/ In the case of a flat inline type array, each field has its\n+        \/\/ own slice so we need to extract the field being accessed from\n+        \/\/ the address computation\n+        adr_type = adr_type->add_field_offset_and_offset(offset)->add_offset(Type::OffsetBot)->is_aryptr();\n+        adr = _igvn.transform(new CastPPNode(ctl, adr, adr_type));\n@@ -338,0 +338,1 @@\n+      assert(res->isa_DecodeN(), \"should be narrow oop\");\n@@ -353,1 +354,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flat_offset();\n@@ -389,1 +390,1 @@\n-    } else  {\n+    } else {\n@@ -392,2 +393,14 @@\n-        \/\/ hit a sentinel, return appropriate 0 value\n-        values.at_put(j, _igvn.zerocon(ft));\n+        \/\/ hit a sentinel, return appropriate value\n+        Node* init_value = alloc->in(AllocateNode::InitValue);\n+        if (init_value != nullptr) {\n+          if (val == start_mem) {\n+            \/\/ TODO 8350865 Scalar replacement does not work well for flat arrays.\n+            \/\/ Somehow we ended up with root mem and therefore walked past the alloc. Fix this. Triggered by TestGenerated::test15\n+            \/\/ Don't we need field_value_by_offset?\n+            return nullptr;\n+          }\n+          values.at_put(j, init_value);\n+        } else {\n+          assert(alloc->in(AllocateNode::RawInitValue) == nullptr, \"init value may not be null\");\n+          values.at_put(j, _igvn.zerocon(ft));\n+        }\n@@ -412,2 +425,10 @@\n-      } else if(val->is_Proj() && val->in(0) == alloc) {\n-        values.at_put(j, _igvn.zerocon(ft));\n+      } else if (val->is_Proj() && val->in(0) == alloc) {\n+        Node* init_value = alloc->in(AllocateNode::InitValue);\n+        if (init_value != nullptr) {\n+          \/\/ TODO 8350865 Scalar replacement does not work well for flat arrays.\n+          \/\/ Is this correct for non-all-zero init values? Don't we need field_value_by_offset?\n+          values.at_put(j, init_value);\n+        } else {\n+          assert(alloc->in(AllocateNode::RawInitValue) == nullptr, \"init value may not be null\");\n+          values.at_put(j, _igvn.zerocon(ft));\n+        }\n@@ -461,1 +482,1 @@\n-  int offset = adr_t->offset();\n+  int offset = adr_t->flat_offset();\n@@ -463,1 +484,0 @@\n-  Node *alloc_ctrl = alloc->in(TypeFunc::Control);\n@@ -480,1 +500,1 @@\n-        done = true; \/\/ Something go wrong.\n+        done = true; \/\/ Something went wrong.\n@@ -490,1 +510,1 @@\n-             atype->is_known_instance_field() && atype->offset() == offset &&\n+             atype->is_known_instance_field() && atype->flat_offset() == offset &&\n@@ -522,1 +542,17 @@\n-      \/\/ hit a sentinel, return appropriate 0 value\n+      \/\/ hit a sentinel, return appropriate value\n+      Node* init_value = alloc->in(AllocateNode::InitValue);\n+      if (init_value != nullptr) {\n+        if (adr_t->is_flat()) {\n+          if (init_value->is_EncodeP()) {\n+            init_value = init_value->in(1);\n+          }\n+          assert(adr_t->is_aryptr()->field_offset().get() != Type::OffsetBot, \"Unknown offset\");\n+          offset = adr_t->is_aryptr()->field_offset().get() + init_value->bottom_type()->inline_klass()->payload_offset();\n+          init_value = init_value->as_InlineType()->field_value_by_offset(offset, true);\n+          if (ft == T_NARROWOOP) {\n+            init_value = transform_later(new EncodePNode(init_value, init_value->bottom_type()->make_ptr()));\n+          }\n+        }\n+        return init_value;\n+      }\n+      assert(alloc->in(AllocateNode::RawInitValue) == nullptr, \"init value may not be null\");\n@@ -554,1 +590,1 @@\n-  \/\/ Something go wrong.\n+  \/\/ Something went wrong.\n@@ -558,0 +594,69 @@\n+\/\/ Search the last value stored into the inline type's fields (for flat arrays).\n+Node* PhaseMacroExpand::inline_type_from_mem(ciInlineKlass* vk, const TypeAryPtr* elem_adr_type, int elem_idx, int offset_in_element, bool null_free, AllocateNode* alloc, SafePointNode* sfpt) {\n+  auto report_failure = [&](int field_offset_in_element) {\n+#ifndef PRODUCT\n+    if (PrintEliminateAllocations) {\n+      ciInlineKlass* elem_klass = elem_adr_type->elem()->inline_klass();\n+      int offset = field_offset_in_element + elem_klass->payload_offset();\n+      ciField* flattened_field = elem_klass->get_field_by_offset(offset, false);\n+      assert(flattened_field != nullptr, \"must have a field of type %s at offset %d\", elem_klass->name()->as_utf8(), offset);\n+      tty->print(\"=== At SafePoint node %d can't find value of field [%s] of array element [%d]\", sfpt->_idx, flattened_field->name()->as_utf8(), elem_idx);\n+      tty->print(\", which prevents elimination of: \");\n+      alloc->dump();\n+    }\n+#endif \/\/ PRODUCT\n+  };\n+\n+  \/\/ Create a new InlineTypeNode and retrieve the field values from memory\n+  InlineTypeNode* vt = InlineTypeNode::make_uninitialized(_igvn, vk, false);\n+  transform_later(vt);\n+  if (null_free) {\n+    vt->set_null_marker(_igvn);\n+  } else {\n+    int nm_offset_in_element = offset_in_element + vk->null_marker_offset_in_payload();\n+    const TypeAryPtr* nm_adr_type = elem_adr_type->with_field_offset(nm_offset_in_element);\n+    Node* nm_value = value_from_mem(sfpt->memory(), sfpt->control(), T_BOOLEAN, TypeInt::BOOL, nm_adr_type, alloc);\n+    if (nm_value != nullptr) {\n+      vt->set_null_marker(_igvn, nm_value);\n+    } else {\n+      report_failure(nm_offset_in_element);\n+      return nullptr;\n+    }\n+  }\n+\n+  for (int i = 0; i < vk->nof_declared_nonstatic_fields(); ++i) {\n+    ciType* field_type = vt->field_type(i);\n+    int field_offset_in_element = offset_in_element + vt->field_offset(i) - vk->payload_offset();\n+    Node* field_value = nullptr;\n+    if (vt->field_is_flat(i)) {\n+      field_value = inline_type_from_mem(field_type->as_inline_klass(), elem_adr_type, elem_idx, field_offset_in_element, vt->field_is_null_free(i), alloc, sfpt);\n+    } else {\n+      const Type* ft = Type::get_const_type(field_type);\n+      BasicType bt = type2field[field_type->basic_type()];\n+      if (UseCompressedOops && !is_java_primitive(bt)) {\n+        ft = ft->make_narrowoop();\n+        bt = T_NARROWOOP;\n+      }\n+      \/\/ Each inline type field has its own memory slice\n+      const TypeAryPtr* field_adr_type = elem_adr_type->with_field_offset(field_offset_in_element);\n+      field_value = value_from_mem(sfpt->memory(), sfpt->control(), bt, ft, field_adr_type, alloc);\n+      if (field_value == nullptr) {\n+        report_failure(field_offset_in_element);\n+      } else if (ft->isa_narrowoop()) {\n+        assert(UseCompressedOops, \"unexpected narrow oop\");\n+        if (field_value->is_EncodeP()) {\n+          field_value = field_value->in(1);\n+        } else if (!field_value->is_InlineType()) {\n+          field_value = transform_later(new DecodeNNode(field_value, field_value->get_ptr_type()));\n+        }\n+      }\n+    }\n+    if (field_value != nullptr) {\n+      vt->set_field_value(i, field_value);\n+    } else {\n+      return nullptr;\n+    }\n+  }\n+  return vt;\n+}\n+\n@@ -567,0 +672,1 @@\n+  Unique_Node_List worklist;\n@@ -575,0 +681,1 @@\n+    worklist.push(res);\n@@ -591,1 +698,1 @@\n-  if (can_eliminate && res != nullptr) {\n+  while (can_eliminate && worklist.size() > 0) {\n@@ -593,2 +700,2 @@\n-    for (DUIterator_Fast jmax, j = res->fast_outs(jmax);\n-                               j < jmax && can_eliminate; j++) {\n+    res = worklist.pop();\n+    for (DUIterator_Fast jmax, j = res->fast_outs(jmax); j < jmax && can_eliminate; j++) {\n@@ -640,0 +747,1 @@\n+          assert(!res->is_Phi() || !res->as_Phi()->can_be_inline_type(), \"Inline type allocations should not have safepoint uses\");\n@@ -642,0 +750,23 @@\n+      } else if (use->is_InlineType() && use->as_InlineType()->get_oop() == res) {\n+        \/\/ Look at uses\n+        for (DUIterator_Fast kmax, k = use->fast_outs(kmax); k < kmax; k++) {\n+          Node* u = use->fast_out(k);\n+          if (u->is_InlineType()) {\n+            \/\/ Use in flat field can be eliminated\n+            InlineTypeNode* vt = u->as_InlineType();\n+            for (uint i = 0; i < vt->field_count(); ++i) {\n+              if (vt->field_value(i) == use && !vt->field_is_flat(i)) {\n+                can_eliminate = false; \/\/ Use in non-flat field\n+                break;\n+              }\n+            }\n+          } else {\n+            \/\/ Add other uses to the worklist to process individually\n+            worklist.push(use);\n+          }\n+        }\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ Store to mark word of inline type larval buffer\n+        assert(res_type->is_inlinetypeptr(), \"Unexpected store to mark word\");\n+      } else if (res_type->is_inlinetypeptr() && (use->Opcode() == Op_MemBarRelease || use->Opcode() == Op_MemBarStoreStore)) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n@@ -664,0 +795,3 @@\n+      } else {\n+        assert(use->Opcode() == Op_CastP2X, \"should be\");\n+        assert(!use->has_out_with(Op_OrL), \"should have been removed because oop is never null\");\n@@ -676,1 +810,1 @@\n-    } else if (alloc->_is_scalar_replaceable) {\n+    } else {\n@@ -746,1 +880,144 @@\n-SafePointScalarObjectNode* PhaseMacroExpand::create_scalarized_object_description(AllocateNode *alloc, SafePointNode* sfpt) {\n+void PhaseMacroExpand::process_field_value_at_safepoint(const Type* field_type, Node* field_val, SafePointNode* sfpt, Unique_Node_List* value_worklist) {\n+  if (UseCompressedOops && field_type->isa_narrowoop()) {\n+    \/\/ Enable \"DecodeN(EncodeP(Allocate)) --> Allocate\" transformation\n+    \/\/ to be able scalar replace the allocation.\n+    if (field_val->is_EncodeP()) {\n+      field_val = field_val->in(1);\n+    } else if (!field_val->is_InlineType()) {\n+      field_val = transform_later(new DecodeNNode(field_val, field_val->get_ptr_type()));\n+    }\n+  }\n+\n+  \/\/ Keep track of inline types to scalarize them later\n+  if (field_val->is_InlineType()) {\n+    value_worklist->push(field_val);\n+  } else if (field_val->is_Phi()) {\n+    PhiNode* phi = field_val->as_Phi();\n+    \/\/ Eagerly replace inline type phis now since we could be removing an inline type allocation where we must\n+    \/\/ scalarize all its fields in safepoints.\n+    field_val = phi->try_push_inline_types_down(&_igvn, true);\n+    if (field_val->is_InlineType()) {\n+      value_worklist->push(field_val);\n+    }\n+  }\n+  sfpt->add_req(field_val);\n+}\n+\n+bool PhaseMacroExpand::add_array_elems_to_safepoint(AllocateNode* alloc, const TypeAryPtr* array_type, SafePointNode* sfpt, Unique_Node_List* value_worklist) {\n+  const Type* elem_type = array_type->elem();\n+  BasicType basic_elem_type = elem_type->array_element_basic_type();\n+\n+  intptr_t elem_size;\n+  if (array_type->is_flat()) {\n+    elem_size = array_type->flat_elem_size();\n+  } else {\n+    elem_size = type2aelembytes(basic_elem_type);\n+  }\n+\n+  int n_elems = alloc->in(AllocateNode::ALength)->get_int();\n+  for (int elem_idx = 0; elem_idx < n_elems; elem_idx++) {\n+    intptr_t elem_offset = arrayOopDesc::base_offset_in_bytes(basic_elem_type) + elem_idx * elem_size;\n+    const TypeAryPtr* elem_adr_type = array_type->with_offset(elem_offset);\n+    Node* elem_val;\n+    if (array_type->is_flat()) {\n+      ciInlineKlass* elem_klass = elem_type->inline_klass();\n+      assert(elem_klass->maybe_flat_in_array(), \"must be flat in array\");\n+      elem_val = inline_type_from_mem(elem_klass, elem_adr_type, elem_idx, 0, array_type->is_null_free(), alloc, sfpt);\n+    } else {\n+      elem_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, elem_type, elem_adr_type, alloc);\n+#ifndef PRODUCT\n+      if (PrintEliminateAllocations && elem_val == nullptr) {\n+        tty->print(\"=== At SafePoint node %d can't find value of array element [%d]\", sfpt->_idx, elem_idx);\n+        tty->print(\", which prevents elimination of: \");\n+        alloc->dump();\n+      }\n+#endif \/\/ PRODUCT\n+    }\n+    if (elem_val == nullptr) {\n+      return false;\n+    }\n+\n+    process_field_value_at_safepoint(elem_type, elem_val, sfpt, value_worklist);\n+  }\n+\n+  return true;\n+}\n+\n+\/\/ Recursively adds all flattened fields of a type 'iklass' inside 'base' to 'sfpt'.\n+\/\/ 'offset_minus_header' refers to the offset of the payload of 'iklass' inside 'base' minus the\n+\/\/ payload offset of 'iklass'. If 'base' is of type 'iklass' then 'offset_minus_header' == 0.\n+bool PhaseMacroExpand::add_inst_fields_to_safepoint(ciInstanceKlass* iklass, AllocateNode* alloc, Node* base, int offset_minus_header, SafePointNode* sfpt, Unique_Node_List* value_worklist) {\n+  const TypeInstPtr* base_type = _igvn.type(base)->is_instptr();\n+  auto report_failure = [&](int offset) {\n+#ifndef PRODUCT\n+    if (PrintEliminateAllocations) {\n+      ciInstanceKlass* base_klass = base_type->instance_klass();\n+      ciField* flattened_field = base_klass->get_field_by_offset(offset, false);\n+      assert(flattened_field != nullptr, \"must have a field of type %s at offset %d\", base_klass->name()->as_utf8(), offset);\n+      tty->print(\"=== At SafePoint node %d can't find value of field: \", sfpt->_idx);\n+      flattened_field->print();\n+      int field_idx = C->alias_type(flattened_field)->index();\n+      tty->print(\" (alias_idx=%d)\", field_idx);\n+      tty->print(\", which prevents elimination of: \");\n+      base->dump();\n+    }\n+#endif \/\/ PRODUCT\n+  };\n+\n+  for (int i = 0; i < iklass->nof_declared_nonstatic_fields(); i++) {\n+    ciField* field = iklass->declared_nonstatic_field_at(i);\n+    if (field->is_flat()) {\n+      ciInlineKlass* fvk = field->type()->as_inline_klass();\n+      int field_offset_minus_header = offset_minus_header + field->offset_in_bytes() - fvk->payload_offset();\n+      bool success = add_inst_fields_to_safepoint(fvk, alloc, base, field_offset_minus_header, sfpt, value_worklist);\n+      if (!success) {\n+        return false;\n+      }\n+\n+      \/\/ The null marker of a field is added right after we scalarize that field\n+      if (!field->is_null_free()) {\n+        int nm_offset = offset_minus_header + field->null_marker_offset();\n+        Node* null_marker = value_from_mem(sfpt->memory(), sfpt->control(), T_BOOLEAN, TypeInt::BOOL, base_type->with_offset(nm_offset), alloc);\n+        if (null_marker == nullptr) {\n+          report_failure(nm_offset);\n+          return false;\n+        }\n+        process_field_value_at_safepoint(TypeInt::BOOL, null_marker, sfpt, value_worklist);\n+      }\n+\n+      continue;\n+    }\n+\n+    int offset = offset_minus_header + field->offset_in_bytes();\n+    ciType* elem_type = field->type();\n+    BasicType basic_elem_type = field->layout_type();\n+\n+    const Type* field_type;\n+    if (is_reference_type(basic_elem_type)) {\n+      if (!elem_type->is_loaded()) {\n+        field_type = TypeInstPtr::BOTTOM;\n+      } else {\n+        field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n+      }\n+      if (UseCompressedOops) {\n+        field_type = field_type->make_narrowoop();\n+        basic_elem_type = T_NARROWOOP;\n+      }\n+    } else {\n+      field_type = Type::get_const_basic_type(basic_elem_type);\n+    }\n+\n+    const TypeInstPtr* field_addr_type = base_type->add_offset(offset)->isa_instptr();\n+    Node* field_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, field_type, field_addr_type, alloc);\n+    if (field_val == nullptr) {\n+      report_failure(offset);\n+      return false;\n+    }\n+    process_field_value_at_safepoint(field_type, field_val, sfpt, value_worklist);\n+  }\n+\n+  return true;\n+}\n+\n+SafePointScalarObjectNode* PhaseMacroExpand::create_scalarized_object_description(AllocateNode* alloc, SafePointNode* sfpt,\n+                                                                                  Unique_Node_List* value_worklist) {\n@@ -751,2 +1028,0 @@\n-  BasicType basic_elem_type  = T_ILLEGAL;\n-  const Type* field_type     = nullptr;\n@@ -755,2 +1030,0 @@\n-  int array_base             = 0;\n-  int element_size           = 0;\n@@ -762,0 +1035,1 @@\n+  uint before_sfpt_req = sfpt->req();\n@@ -774,4 +1048,10 @@\n-      basic_elem_type = res_type->is_aryptr()->elem()->array_element_basic_type();\n-      array_base = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n-      element_size = type2aelembytes(basic_elem_type);\n-      field_type = res_type->is_aryptr()->elem();\n+    }\n+\n+    if (res->bottom_type()->is_inlinetypeptr()) {\n+      \/\/ Nullable inline types have a null marker field which is added to the safepoint when scalarizing them (see\n+      \/\/ InlineTypeNode::make_scalar_in_safepoint()). When having circular inline types, we stop scalarizing at depth 1\n+      \/\/ to avoid an endless recursion. Therefore, we do not have a SafePointScalarObjectNode node here, yet.\n+      \/\/ We are about to create a SafePointScalarObjectNode as if this is a normal object. Add an additional int input\n+      \/\/ with value 1 which sets the null marker to true to indicate that the object is always non-null. This input is checked\n+      \/\/ later in PhaseOutput::filLocArray() for inline types.\n+      sfpt->add_req(_igvn.intcon(1));\n@@ -785,64 +1065,4 @@\n-  \/\/ Scan object's fields adding an input to the safepoint for each field.\n-  for (int j = 0; j < nfields; j++) {\n-    intptr_t offset;\n-    ciField* field = nullptr;\n-    if (iklass != nullptr) {\n-      field = iklass->nonstatic_field_at(j);\n-      offset = field->offset_in_bytes();\n-      ciType* elem_type = field->type();\n-      basic_elem_type = field->layout_type();\n-\n-      \/\/ The next code is taken from Parse::do_get_xxx().\n-      if (is_reference_type(basic_elem_type)) {\n-        if (!elem_type->is_loaded()) {\n-          field_type = TypeInstPtr::BOTTOM;\n-        } else if (field != nullptr && field->is_static_constant()) {\n-          ciObject* con = field->constant_value().as_object();\n-          \/\/ Do not \"join\" in the previous type; it doesn't add value,\n-          \/\/ and may yield a vacuous result if the field is of interface type.\n-          field_type = TypeOopPtr::make_from_constant(con)->isa_oopptr();\n-          assert(field_type != nullptr, \"field singleton type must be consistent\");\n-        } else {\n-          field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n-        }\n-        if (UseCompressedOops) {\n-          field_type = field_type->make_narrowoop();\n-          basic_elem_type = T_NARROWOOP;\n-        }\n-      } else {\n-        field_type = Type::get_const_basic_type(basic_elem_type);\n-      }\n-    } else {\n-      offset = array_base + j * (intptr_t)element_size;\n-    }\n-\n-    const TypeOopPtr *field_addr_type = res_type->add_offset(offset)->isa_oopptr();\n-\n-    Node *field_val = value_from_mem(sfpt->memory(), sfpt->control(), basic_elem_type, field_type, field_addr_type, alloc);\n-\n-    \/\/ We weren't able to find a value for this field,\n-    \/\/ give up on eliminating this allocation.\n-    if (field_val == nullptr) {\n-      uint last = sfpt->req() - 1;\n-      for (int k = 0;  k < j; k++) {\n-        sfpt->del_req(last--);\n-      }\n-      _igvn._worklist.push(sfpt);\n-\n-#ifndef PRODUCT\n-      if (PrintEliminateAllocations) {\n-        if (field != nullptr) {\n-          tty->print(\"=== At SafePoint node %d can't find value of field: \", sfpt->_idx);\n-          field->print();\n-          int field_idx = C->get_alias_index(field_addr_type);\n-          tty->print(\" (alias_idx=%d)\", field_idx);\n-        } else { \/\/ Array's element\n-          tty->print(\"=== At SafePoint node %d can't find value of array element [%d]\", sfpt->_idx, j);\n-        }\n-        tty->print(\", which prevents elimination of: \");\n-        if (res == nullptr)\n-          alloc->dump();\n-        else\n-          res->dump();\n-      }\n-#endif\n+  if (res == nullptr) {\n+    sfpt->jvms()->set_endoff(sfpt->req());\n+    return sobj;\n+  }\n@@ -850,2 +1070,6 @@\n-      return nullptr;\n-    }\n+  bool success;\n+  if (iklass == nullptr) {\n+    success = add_array_elems_to_safepoint(alloc, res_type->is_aryptr(), sfpt, value_worklist);\n+  } else {\n+    success = add_inst_fields_to_safepoint(iklass, alloc, res, 0, sfpt, value_worklist);\n+  }\n@@ -853,8 +1077,4 @@\n-    if (UseCompressedOops && field_type->isa_narrowoop()) {\n-      \/\/ Enable \"DecodeN(EncodeP(Allocate)) --> Allocate\" transformation\n-      \/\/ to be able scalar replace the allocation.\n-      if (field_val->is_EncodeP()) {\n-        field_val = field_val->in(1);\n-      } else {\n-        field_val = transform_later(new DecodeNNode(field_val, field_val->get_ptr_type()));\n-      }\n+  \/\/ We weren't able to find a value for this field, remove all the fields added to the safepoint\n+  if (!success) {\n+    for (uint i = sfpt->req() - 1; i >= before_sfpt_req; i--) {\n+      sfpt->del_req(i);\n@@ -862,1 +1082,2 @@\n-    sfpt->add_req(field_val);\n+    _igvn._worklist.push(sfpt);\n+    return nullptr;\n@@ -866,1 +1087,0 @@\n-\n@@ -875,0 +1095,4 @@\n+  const TypeOopPtr* res_type = nullptr;\n+  if (res != nullptr) { \/\/ Could be null when there are no users\n+    res_type = _igvn.type(res)->isa_oopptr();\n+  }\n@@ -877,0 +1101,1 @@\n+  Unique_Node_List value_worklist;\n@@ -879,1 +1104,1 @@\n-    SafePointScalarObjectNode* sobj = create_scalarized_object_description(alloc, sfpt);\n+    SafePointScalarObjectNode* sobj = create_scalarized_object_description(alloc, sfpt, &value_worklist);\n@@ -895,1 +1120,8 @@\n-\n+  \/\/ Scalarize inline types that were added to the safepoint.\n+  \/\/ Don't allow linking a constant oop (if available) for flat array elements\n+  \/\/ because Deoptimization::reassign_flat_array_elements needs field values.\n+  bool allow_oop = (res_type != nullptr) && !res_type->is_flat();\n+  for (uint i = 0; i < value_worklist.size(); ++i) {\n+    InlineTypeNode* vt = value_worklist.at(i)->as_InlineType();\n+    vt->make_scalar_in_safepoints(&_igvn, allow_oop);\n+  }\n@@ -911,1 +1143,2 @@\n-void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc) {\n+void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc, bool inline_alloc) {\n+  Unique_Node_List worklist;\n@@ -914,0 +1147,4 @@\n+    worklist.push(res);\n+  }\n+  while (worklist.size() > 0) {\n+    res = worklist.pop();\n@@ -923,10 +1160,7 @@\n-#ifdef ASSERT\n-            \/\/ Verify that there is no dependent MemBarVolatile nodes,\n-            \/\/ they should be removed during IGVN, see MemBarNode::Ideal().\n-            for (DUIterator_Fast pmax, p = n->fast_outs(pmax);\n-                                       p < pmax; p++) {\n-              Node* mb = n->fast_out(p);\n-              assert(mb->is_Initialize() || !mb->is_MemBar() ||\n-                     mb->req() <= MemBarNode::Precedent ||\n-                     mb->in(MemBarNode::Precedent) != n,\n-                     \"MemBarVolatile should be eliminated for non-escaping object\");\n+            for (DUIterator_Fast pmax, p = n->fast_outs(pmax); p < pmax; p++) {\n+              MemBarNode* mb = n->fast_out(p)->isa_MemBar();\n+              if (mb != nullptr && mb->req() <= MemBarNode::Precedent && mb->in(MemBarNode::Precedent) == n) {\n+                \/\/ MemBarVolatiles should have been removed by MemBarNode::Ideal() for non-inline allocations\n+                assert(inline_alloc, \"MemBarVolatile should be eliminated for non-escaping object\");\n+                mb->remove(&_igvn);\n+              }\n@@ -934,1 +1168,0 @@\n-#endif\n@@ -958,2 +1191,1 @@\n-          CallProjections callprojs;\n-          ac->extract_projections(&callprojs, true);\n+          CallProjections* callprojs = ac->extract_projections(true);\n@@ -961,3 +1193,3 @@\n-          _igvn.replace_node(callprojs.fallthrough_ioproj, ac->in(TypeFunc::I_O));\n-          _igvn.replace_node(callprojs.fallthrough_memproj, ac->in(TypeFunc::Memory));\n-          _igvn.replace_node(callprojs.fallthrough_catchproj, ac->in(TypeFunc::Control));\n+          _igvn.replace_node(callprojs->fallthrough_ioproj, ac->in(TypeFunc::I_O));\n+          _igvn.replace_node(callprojs->fallthrough_memproj, ac->in(TypeFunc::Memory));\n+          _igvn.replace_node(callprojs->fallthrough_catchproj, ac->in(TypeFunc::Control));\n@@ -980,0 +1212,23 @@\n+      } else if (use->is_InlineType()) {\n+        assert(use->as_InlineType()->get_oop() == res, \"unexpected inline type ptr use\");\n+        \/\/ Cut off oop input and remove known instance id from type\n+        _igvn.rehash_node_delayed(use);\n+        use->as_InlineType()->set_oop(_igvn, _igvn.zerocon(T_OBJECT));\n+        const TypeOopPtr* toop = _igvn.type(use)->is_oopptr()->cast_to_instance_id(TypeOopPtr::InstanceBot);\n+        _igvn.set_type(use, toop);\n+        use->as_InlineType()->set_type(toop);\n+        \/\/ Process users\n+        for (DUIterator_Fast kmax, k = use->fast_outs(kmax); k < kmax; k++) {\n+          Node* u = use->fast_out(k);\n+          if (!u->is_InlineType()) {\n+            worklist.push(u);\n+          }\n+        }\n+      } else if (use->Opcode() == Op_StoreX && use->in(MemNode::Address) == res) {\n+        \/\/ Store to mark word of inline type larval buffer\n+        assert(inline_alloc, \"Unexpected store to mark word\");\n+        _igvn.replace_node(use, use->in(MemNode::Memory));\n+      } else if (use->Opcode() == Op_MemBarRelease || use->Opcode() == Op_MemBarStoreStore) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n+        assert(inline_alloc, \"Unexpected MemBarRelease\");\n+        use->as_MemBar()->remove(&_igvn);\n@@ -992,1 +1247,1 @@\n-  if (_callprojs.resproj != nullptr && _callprojs.resproj->outcnt() != 0) {\n+  if (_callprojs->resproj[0] != nullptr && _callprojs->resproj[0]->outcnt() != 0) {\n@@ -996,2 +1251,2 @@\n-    for (DUIterator_Fast jmax, j = _callprojs.resproj->fast_outs(jmax);  j < jmax; j++) {\n-      Node* use = _callprojs.resproj->fast_out(j);\n+    for (DUIterator_Fast jmax, j = _callprojs->resproj[0]->fast_outs(jmax);  j < jmax; j++) {\n+      Node* use = _callprojs->resproj[0]->fast_out(j);\n@@ -1004,3 +1259,3 @@\n-    for (DUIterator_Last jmin, j = _callprojs.resproj->last_outs(jmin); j >= jmin; ) {\n-      Node* use = _callprojs.resproj->last_out(j);\n-      uint oc1 = _callprojs.resproj->outcnt();\n+    for (DUIterator_Last jmin, j = _callprojs->resproj[0]->last_outs(jmin); j >= jmin; ) {\n+      Node* use = _callprojs->resproj[0]->last_out(j);\n+      uint oc1 = _callprojs->resproj[0]->outcnt();\n@@ -1017,1 +1272,1 @@\n-          assert(tmp == nullptr || tmp == _callprojs.fallthrough_catchproj, \"allocation control projection\");\n+          assert(tmp == nullptr || tmp == _callprojs->fallthrough_catchproj, \"allocation control projection\");\n@@ -1025,1 +1280,1 @@\n-            assert(mem->in(TypeFunc::Memory) == _callprojs.fallthrough_memproj, \"allocation memory projection\");\n+            assert(mem->in(TypeFunc::Memory) == _callprojs->fallthrough_memproj, \"allocation memory projection\");\n@@ -1027,1 +1282,1 @@\n-            assert(mem == _callprojs.fallthrough_memproj, \"allocation memory projection\");\n+            assert(mem == _callprojs->fallthrough_memproj, \"allocation memory projection\");\n@@ -1032,0 +1287,4 @@\n+      } else if (use->Opcode() == Op_MemBarStoreStore) {\n+        \/\/ Inline type buffer allocations are followed by a membar\n+        assert(inline_alloc, \"Unexpected MemBarStoreStore\");\n+        use->as_MemBar()->remove(&_igvn);\n@@ -1035,1 +1294,1 @@\n-      j -= (oc1 - _callprojs.resproj->outcnt());\n+      j -= (oc1 - _callprojs->resproj[0]->outcnt());\n@@ -1038,2 +1297,2 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, alloc->in(TypeFunc::Control));\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, alloc->in(TypeFunc::Control));\n@@ -1041,2 +1300,2 @@\n-  if (_callprojs.fallthrough_memproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_memproj, alloc->in(TypeFunc::Memory));\n+  if (_callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_memproj, alloc->in(TypeFunc::Memory));\n@@ -1044,2 +1303,2 @@\n-  if (_callprojs.catchall_memproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_memproj, C->top());\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_memproj, C->top());\n@@ -1047,2 +1306,2 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    _igvn.replace_node(_callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_node(_callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));\n@@ -1050,2 +1309,2 @@\n-  if (_callprojs.catchall_ioproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_ioproj, C->top());\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_ioproj, C->top());\n@@ -1053,2 +1312,2 @@\n-  if (_callprojs.catchall_catchproj != nullptr) {\n-    _igvn.replace_node(_callprojs.catchall_catchproj, C->top());\n+  if (_callprojs->catchall_catchproj != nullptr) {\n+    _igvn.replace_node(_callprojs->catchall_catchproj, C->top());\n@@ -1064,1 +1323,1 @@\n-  if (!EliminateAllocations || !alloc->_is_non_escaping) {\n+  if (!EliminateAllocations) {\n@@ -1069,1 +1328,8 @@\n-  Node* res = alloc->result_cast();\n+\n+  \/\/ Attempt to eliminate inline type buffer allocations\n+  \/\/ regardless of usage and escape\/replaceable status.\n+  bool inline_alloc = tklass->isa_instklassptr() &&\n+                      tklass->is_instklassptr()->instance_klass()->is_inlinetype();\n+  if (!alloc->_is_non_escaping && !inline_alloc) {\n+    return false;\n+  }\n@@ -1072,1 +1338,2 @@\n-  bool boxing_alloc = C->eliminate_boxing() &&\n+  Node* res = alloc->result_cast();\n+  bool boxing_alloc = (res == nullptr) && C->eliminate_boxing() &&\n@@ -1075,1 +1342,1 @@\n-  if (!alloc->_is_scalar_replaceable && (!boxing_alloc || (res != nullptr))) {\n+  if (!alloc->_is_scalar_replaceable && !boxing_alloc && !inline_alloc) {\n@@ -1079,1 +1346,1 @@\n-  alloc->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = alloc->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1087,1 +1354,1 @@\n-    assert(res == nullptr, \"sanity\");\n+    assert(res == nullptr || inline_alloc, \"sanity\");\n@@ -1112,1 +1379,1 @@\n-  process_users_of_allocation(alloc);\n+  process_users_of_allocation(alloc, inline_alloc);\n@@ -1134,1 +1401,1 @@\n-  boxing->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = boxing->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1136,1 +1403,1 @@\n-  const TypeTuple* r = boxing->tf()->range();\n+  const TypeTuple* r = boxing->tf()->range_sig();\n@@ -1240,0 +1507,1 @@\n+            Node* init_val, \/\/ value to initialize the array with\n@@ -1322,1 +1590,1 @@\n-    Node *toobig_true = new IfTrueNode( toobig_iff );\n+    Node* toobig_true = new IfTrueNode(toobig_iff);\n@@ -1325,1 +1593,1 @@\n-    toobig_false = new IfFalseNode( toobig_iff );\n+    toobig_false = new IfFalseNode(toobig_iff);\n@@ -1364,0 +1632,1 @@\n+\n@@ -1421,0 +1690,6 @@\n+    if (init_val != nullptr) {\n+      call->init_req(TypeFunc::Parms+2, init_val);\n+    }\n+  } else {\n+    \/\/ Let the runtime know if this is a larval allocation\n+    call->init_req(TypeFunc::Parms+1, _igvn.intcon(alloc->_larval));\n@@ -1452,1 +1727,1 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -1458,2 +1733,2 @@\n-  if (expand_fast_path && _callprojs.fallthrough_memproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_memproj, result_phi_rawmem);\n+  if (expand_fast_path && _callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_memproj, result_phi_rawmem);\n@@ -1463,4 +1738,4 @@\n-  if (_callprojs.catchall_memproj != nullptr ) {\n-    if (_callprojs.fallthrough_memproj == nullptr) {\n-      _callprojs.fallthrough_memproj = new ProjNode(call, TypeFunc::Memory);\n-      transform_later(_callprojs.fallthrough_memproj);\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    if (_callprojs->fallthrough_memproj == nullptr) {\n+      _callprojs->fallthrough_memproj = new ProjNode(call, TypeFunc::Memory);\n+      transform_later(_callprojs->fallthrough_memproj);\n@@ -1468,2 +1743,2 @@\n-    migrate_outs(_callprojs.catchall_memproj, _callprojs.fallthrough_memproj);\n-    _igvn.remove_dead_node(_callprojs.catchall_memproj);\n+    _igvn.replace_in_uses(_callprojs->catchall_memproj, _callprojs->fallthrough_memproj);\n+    _igvn.remove_dead_node(_callprojs->catchall_memproj);\n@@ -1477,2 +1752,2 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_ioproj, result_phi_i_o);\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_ioproj, result_phi_i_o);\n@@ -1482,4 +1757,4 @@\n-  if (_callprojs.catchall_ioproj != nullptr ) {\n-    if (_callprojs.fallthrough_ioproj == nullptr) {\n-      _callprojs.fallthrough_ioproj = new ProjNode(call, TypeFunc::I_O);\n-      transform_later(_callprojs.fallthrough_ioproj);\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    if (_callprojs->fallthrough_ioproj == nullptr) {\n+      _callprojs->fallthrough_ioproj = new ProjNode(call, TypeFunc::I_O);\n+      transform_later(_callprojs->fallthrough_ioproj);\n@@ -1487,2 +1762,2 @@\n-    migrate_outs(_callprojs.catchall_ioproj, _callprojs.fallthrough_ioproj);\n-    _igvn.remove_dead_node(_callprojs.catchall_ioproj);\n+    _igvn.replace_in_uses(_callprojs->catchall_ioproj, _callprojs->fallthrough_ioproj);\n+    _igvn.remove_dead_node(_callprojs->catchall_ioproj);\n@@ -1507,2 +1782,2 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    ctrl = _callprojs.fallthrough_catchproj->clone();\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    ctrl = _callprojs->fallthrough_catchproj->clone();\n@@ -1510,1 +1785,1 @@\n-    _igvn.replace_node(_callprojs.fallthrough_catchproj, result_region);\n+    _igvn.replace_node(_callprojs->fallthrough_catchproj, result_region);\n@@ -1515,1 +1790,1 @@\n-  if (_callprojs.resproj == nullptr) {\n+  if (_callprojs->resproj[0] == nullptr) {\n@@ -1519,1 +1794,1 @@\n-    slow_result = _callprojs.resproj->clone();\n+    slow_result = _callprojs->resproj[0]->clone();\n@@ -1521,1 +1796,1 @@\n-    _igvn.replace_node(_callprojs.resproj, result_phi_rawoop);\n+    _igvn.replace_node(_callprojs->resproj[0], result_phi_rawoop);\n@@ -1531,1 +1806,1 @@\n-  result_phi_rawmem->init_req(slow_result_path, _callprojs.fallthrough_memproj);\n+  result_phi_rawmem->init_req(slow_result_path, _callprojs->fallthrough_memproj);\n@@ -1543,4 +1818,4 @@\n-  alloc->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  if (_callprojs.resproj != nullptr) {\n-    for (DUIterator_Fast imax, i = _callprojs.resproj->fast_outs(imax); i < imax; i++) {\n-      Node* use = _callprojs.resproj->fast_out(i);\n+  _callprojs = alloc->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  if (_callprojs->resproj[0] != nullptr) {\n+    for (DUIterator_Fast imax, i = _callprojs->resproj[0]->fast_outs(imax); i < imax; i++) {\n+      Node* use = _callprojs->resproj[0]->fast_out(i);\n@@ -1551,2 +1826,2 @@\n-    assert(_callprojs.resproj->outcnt() == 0, \"all uses must be deleted\");\n-    _igvn.remove_dead_node(_callprojs.resproj);\n+    assert(_callprojs->resproj[0]->outcnt() == 0, \"all uses must be deleted\");\n+    _igvn.remove_dead_node(_callprojs->resproj[0]);\n@@ -1554,3 +1829,3 @@\n-  if (_callprojs.fallthrough_catchproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_catchproj, ctrl);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_catchproj);\n+  if (_callprojs->fallthrough_catchproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_catchproj, ctrl);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_catchproj);\n@@ -1558,3 +1833,3 @@\n-  if (_callprojs.catchall_catchproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_catchproj);\n-    _callprojs.catchall_catchproj->set_req(0, top());\n+  if (_callprojs->catchall_catchproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_catchproj);\n+    _callprojs->catchall_catchproj->set_req(0, top());\n@@ -1562,2 +1837,2 @@\n-  if (_callprojs.fallthrough_proj != nullptr) {\n-    Node* catchnode = _callprojs.fallthrough_proj->unique_ctrl_out();\n+  if (_callprojs->fallthrough_proj != nullptr) {\n+    Node* catchnode = _callprojs->fallthrough_proj->unique_ctrl_out();\n@@ -1565,1 +1840,1 @@\n-    _igvn.remove_dead_node(_callprojs.fallthrough_proj);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_proj);\n@@ -1567,3 +1842,3 @@\n-  if (_callprojs.fallthrough_memproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_memproj, mem);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_memproj);\n+  if (_callprojs->fallthrough_memproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_memproj, mem);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_memproj);\n@@ -1571,3 +1846,3 @@\n-  if (_callprojs.fallthrough_ioproj != nullptr) {\n-    migrate_outs(_callprojs.fallthrough_ioproj, i_o);\n-    _igvn.remove_dead_node(_callprojs.fallthrough_ioproj);\n+  if (_callprojs->fallthrough_ioproj != nullptr) {\n+    _igvn.replace_in_uses(_callprojs->fallthrough_ioproj, i_o);\n+    _igvn.remove_dead_node(_callprojs->fallthrough_ioproj);\n@@ -1575,3 +1850,3 @@\n-  if (_callprojs.catchall_memproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_memproj);\n-    _callprojs.catchall_memproj->set_req(0, top());\n+  if (_callprojs->catchall_memproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_memproj);\n+    _callprojs->catchall_memproj->set_req(0, top());\n@@ -1579,3 +1854,3 @@\n-  if (_callprojs.catchall_ioproj != nullptr) {\n-    _igvn.rehash_node_delayed(_callprojs.catchall_ioproj);\n-    _callprojs.catchall_ioproj->set_req(0, top());\n+  if (_callprojs->catchall_ioproj != nullptr) {\n+    _igvn.rehash_node_delayed(_callprojs->catchall_ioproj);\n+    _callprojs->catchall_ioproj->set_req(0, top());\n@@ -1699,5 +1974,4 @@\n-Node*\n-PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n-                                    Node* control, Node* rawmem, Node* object,\n-                                    Node* klass_node, Node* length,\n-                                    Node* size_in_bytes) {\n+Node* PhaseMacroExpand::initialize_object(AllocateNode* alloc,\n+                                          Node* control, Node* rawmem, Node* object,\n+                                          Node* klass_node, Node* length,\n+                                          Node* size_in_bytes) {\n@@ -1706,1 +1980,1 @@\n-  Node* mark_node = alloc->make_ideal_mark(&_igvn, object, control, rawmem);\n+  Node* mark_node = alloc->make_ideal_mark(&_igvn, control, rawmem);\n@@ -1744,0 +2018,2 @@\n+                                            alloc->in(AllocateNode::InitValue),\n+                                            alloc->in(AllocateNode::RawInitValue),\n@@ -1918,1 +2194,1 @@\n-  expand_allocate_common(alloc, nullptr,\n+  expand_allocate_common(alloc, nullptr, nullptr,\n@@ -1928,0 +2204,1 @@\n+  Node* init_value = alloc->in(AllocateNode::InitValue);\n@@ -1929,0 +2206,7 @@\n+  \/\/ TODO 8366668 Compute the VM type, is this even needed now that we set it earlier? Should we assert instead?\n+  if (ary_klass_t && ary_klass_t->klass_is_exact() && ary_klass_t->exact_klass()->is_obj_array_klass()) {\n+    ary_klass_t = ary_klass_t->get_vm_type();\n+    klass_node = makecon(ary_klass_t);\n+    _igvn.replace_input_of(alloc, AllocateNode::KlassNode, klass_node);\n+  }\n+  const TypeFunc* slow_call_type;\n@@ -1935,0 +2219,1 @@\n+    slow_call_type = OptoRuntime::new_array_nozero_Type();\n@@ -1937,0 +2222,7 @@\n+    slow_call_type = OptoRuntime::new_array_Type();\n+\n+    if (init_value == nullptr) {\n+      init_value = _igvn.zerocon(T_OBJECT);\n+    } else if (UseCompressedOops) {\n+      init_value = transform_later(new DecodeNNode(init_value, init_value->bottom_type()->make_ptr()));\n+    }\n@@ -1938,2 +2230,2 @@\n-  expand_allocate_common(alloc, length,\n-                         OptoRuntime::new_array_Type(),\n+  expand_allocate_common(alloc, length, init_value,\n+                         slow_call_type,\n@@ -2151,1 +2443,1 @@\n-  alock->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = alock->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -2155,2 +2447,2 @@\n-         _callprojs.fallthrough_proj != nullptr &&\n-         _callprojs.fallthrough_memproj != nullptr,\n+         _callprojs->fallthrough_proj != nullptr &&\n+         _callprojs->fallthrough_memproj != nullptr,\n@@ -2159,2 +2451,2 @@\n-  Node* fallthroughproj = _callprojs.fallthrough_proj;\n-  Node* memproj_fallthrough = _callprojs.fallthrough_memproj;\n+  Node* fallthroughproj = _callprojs->fallthrough_proj;\n+  Node* memproj_fallthrough = _callprojs->fallthrough_memproj;\n@@ -2231,1 +2523,1 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n@@ -2237,2 +2529,2 @@\n-  assert(_callprojs.fallthrough_ioproj == nullptr && _callprojs.catchall_ioproj == nullptr &&\n-         _callprojs.catchall_memproj == nullptr && _callprojs.catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n+  assert(_callprojs->fallthrough_ioproj == nullptr && _callprojs->catchall_ioproj == nullptr &&\n+         _callprojs->catchall_memproj == nullptr && _callprojs->catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n@@ -2243,1 +2535,1 @@\n-  Node *slow_ctrl = _callprojs.fallthrough_proj->clone();\n+  Node *slow_ctrl = _callprojs->fallthrough_proj->clone();\n@@ -2245,2 +2537,2 @@\n-  _igvn.hash_delete(_callprojs.fallthrough_proj);\n-  _callprojs.fallthrough_proj->disconnect_inputs(C);\n+  _igvn.hash_delete(_callprojs->fallthrough_proj);\n+  _callprojs->fallthrough_proj->disconnect_inputs(C);\n@@ -2250,1 +2542,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_proj, region);\n+  _igvn.replace_node(_callprojs->fallthrough_proj, region);\n@@ -2258,1 +2550,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, mem_phi);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, mem_phi);\n@@ -2291,3 +2583,3 @@\n-  call->extract_projections(&_callprojs, false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n-  assert(_callprojs.fallthrough_ioproj == nullptr && _callprojs.catchall_ioproj == nullptr &&\n-         _callprojs.catchall_memproj == nullptr && _callprojs.catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n+  _callprojs = call->extract_projections(false \/*separate_io_proj*\/, false \/*do_asserts*\/);\n+  assert(_callprojs->fallthrough_ioproj == nullptr && _callprojs->catchall_ioproj == nullptr &&\n+         _callprojs->catchall_memproj == nullptr && _callprojs->catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n@@ -2299,1 +2591,1 @@\n-  Node *slow_ctrl = _callprojs.fallthrough_proj->clone();\n+  Node *slow_ctrl = _callprojs->fallthrough_proj->clone();\n@@ -2301,2 +2593,2 @@\n-  _igvn.hash_delete(_callprojs.fallthrough_proj);\n-  _callprojs.fallthrough_proj->disconnect_inputs(C);\n+  _igvn.hash_delete(_callprojs->fallthrough_proj);\n+  _callprojs->fallthrough_proj->disconnect_inputs(C);\n@@ -2306,1 +2598,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_proj, region);\n+  _igvn.replace_node(_callprojs->fallthrough_proj, region);\n@@ -2313,1 +2605,1 @@\n-  _igvn.replace_node(_callprojs.fallthrough_memproj, mem_phi);\n+  _igvn.replace_node(_callprojs->fallthrough_memproj, mem_phi);\n@@ -2316,0 +2608,211 @@\n+\/\/ An inline type might be returned from the call but we don't know its\n+\/\/ type. Either we get a buffered inline type (and nothing needs to be done)\n+\/\/ or one of the values being returned is the klass of the inline type\n+\/\/ and we need to allocate an inline type instance of that type and\n+\/\/ initialize it with other values being returned. In that case, we\n+\/\/ first try a fast path allocation and initialize the value with the\n+\/\/ inline klass's pack handler or we fall back to a runtime call.\n+void PhaseMacroExpand::expand_mh_intrinsic_return(CallStaticJavaNode* call) {\n+  assert(call->method()->is_method_handle_intrinsic(), \"must be a method handle intrinsic call\");\n+  Node* ret = call->proj_out_or_null(TypeFunc::Parms);\n+  if (ret == nullptr) {\n+    return;\n+  }\n+  const TypeFunc* tf = call->_tf;\n+  const TypeTuple* domain = OptoRuntime::store_inline_type_fields_Type()->domain_cc();\n+  const TypeFunc* new_tf = TypeFunc::make(tf->domain_sig(), tf->domain_cc(), tf->range_sig(), domain);\n+  call->_tf = new_tf;\n+  \/\/ Make sure the change of type is applied before projections are processed by igvn\n+  _igvn.set_type(call, call->Value(&_igvn));\n+  _igvn.set_type(ret, ret->Value(&_igvn));\n+\n+  \/\/ Before any new projection is added:\n+  CallProjections* projs = call->extract_projections(true, true);\n+\n+  \/\/ Create temporary hook nodes that will be replaced below.\n+  \/\/ Add an input to prevent hook nodes from being dead.\n+  Node* ctl = new Node(call);\n+  Node* mem = new Node(ctl);\n+  Node* io = new Node(ctl);\n+  Node* ex_ctl = new Node(ctl);\n+  Node* ex_mem = new Node(ctl);\n+  Node* ex_io = new Node(ctl);\n+  Node* res = new Node(ctl);\n+\n+  \/\/ Allocate a new buffered inline type only if a new one is not returned\n+  Node* cast = transform_later(new CastP2XNode(ctl, res));\n+  Node* mask = MakeConX(0x1);\n+  Node* masked = transform_later(new AndXNode(cast, mask));\n+  Node* cmp = transform_later(new CmpXNode(masked, mask));\n+  Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+  IfNode* allocation_iff = new IfNode(ctl, bol, PROB_MAX, COUNT_UNKNOWN);\n+  transform_later(allocation_iff);\n+  Node* allocation_ctl = transform_later(new IfTrueNode(allocation_iff));\n+  Node* no_allocation_ctl = transform_later(new IfFalseNode(allocation_iff));\n+  Node* no_allocation_res = transform_later(new CheckCastPPNode(no_allocation_ctl, res, TypeInstPtr::BOTTOM));\n+\n+  \/\/ Try to allocate a new buffered inline instance either from TLAB or eden space\n+  Node* needgc_ctrl = nullptr; \/\/ needgc means slowcase, i.e. allocation failed\n+  CallLeafNoFPNode* handler_call;\n+  const bool alloc_in_place = UseTLAB;\n+  if (alloc_in_place) {\n+    Node* fast_oop_ctrl = nullptr;\n+    Node* fast_oop_rawmem = nullptr;\n+    Node* mask2 = MakeConX(-2);\n+    Node* masked2 = transform_later(new AndXNode(cast, mask2));\n+    Node* rawklassptr = transform_later(new CastX2PNode(masked2));\n+    Node* klass_node = transform_later(new CheckCastPPNode(allocation_ctl, rawklassptr, TypeInstKlassPtr::OBJECT_OR_NULL));\n+    Node* layout_val = make_load(nullptr, mem, klass_node, in_bytes(Klass::layout_helper_offset()), TypeInt::INT, T_INT);\n+    Node* size_in_bytes = ConvI2X(layout_val);\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    Node* fast_oop = bs->obj_allocate(this, mem, allocation_ctl, size_in_bytes, io, needgc_ctrl,\n+                                      fast_oop_ctrl, fast_oop_rawmem,\n+                                      AllocateInstancePrefetchLines);\n+    \/\/ Allocation succeed, initialize buffered inline instance header firstly,\n+    \/\/ and then initialize its fields with an inline class specific handler\n+    Node* mark_node = makecon(TypeRawPtr::make((address)markWord::inline_type_prototype().value()));\n+    fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::mark_offset_in_bytes(), mark_node, T_ADDRESS);\n+    fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+    if (UseCompressedClassPointers) {\n+      fast_oop_rawmem = make_store(fast_oop_ctrl, fast_oop_rawmem, fast_oop, oopDesc::klass_gap_offset_in_bytes(), intcon(0), T_INT);\n+    }\n+    Node* fixed_block  = make_load(fast_oop_ctrl, fast_oop_rawmem, klass_node, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+    Node* pack_handler = make_load(fast_oop_ctrl, fast_oop_rawmem, fixed_block, in_bytes(InlineKlass::pack_handler_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);\n+    handler_call = new CallLeafNoFPNode(OptoRuntime::pack_inline_type_Type(),\n+                                        nullptr,\n+                                        \"pack handler\",\n+                                        TypeRawPtr::BOTTOM);\n+    handler_call->init_req(TypeFunc::Control, fast_oop_ctrl);\n+    handler_call->init_req(TypeFunc::Memory, fast_oop_rawmem);\n+    handler_call->init_req(TypeFunc::I_O, top());\n+    handler_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+    handler_call->init_req(TypeFunc::ReturnAdr, top());\n+    handler_call->init_req(TypeFunc::Parms, pack_handler);\n+    handler_call->init_req(TypeFunc::Parms+1, fast_oop);\n+  } else {\n+    needgc_ctrl = allocation_ctl;\n+  }\n+\n+  \/\/ Allocation failed, fall back to a runtime call\n+  CallStaticJavaNode* slow_call = new CallStaticJavaNode(OptoRuntime::store_inline_type_fields_Type(),\n+                                                         StubRoutines::store_inline_type_fields_to_buf(),\n+                                                         \"store_inline_type_fields\",\n+                                                         TypePtr::BOTTOM);\n+  slow_call->init_req(TypeFunc::Control, needgc_ctrl);\n+  slow_call->init_req(TypeFunc::Memory, mem);\n+  slow_call->init_req(TypeFunc::I_O, io);\n+  slow_call->init_req(TypeFunc::FramePtr, call->in(TypeFunc::FramePtr));\n+  slow_call->init_req(TypeFunc::ReturnAdr, call->in(TypeFunc::ReturnAdr));\n+  slow_call->init_req(TypeFunc::Parms, res);\n+\n+  Node* slow_ctl = transform_later(new ProjNode(slow_call, TypeFunc::Control));\n+  Node* slow_mem = transform_later(new ProjNode(slow_call, TypeFunc::Memory));\n+  Node* slow_io = transform_later(new ProjNode(slow_call, TypeFunc::I_O));\n+  Node* slow_res = transform_later(new ProjNode(slow_call, TypeFunc::Parms));\n+  Node* slow_catc = transform_later(new CatchNode(slow_ctl, slow_io, 2));\n+  Node* slow_norm = transform_later(new CatchProjNode(slow_catc, CatchProjNode::fall_through_index, CatchProjNode::no_handler_bci));\n+  Node* slow_excp = transform_later(new CatchProjNode(slow_catc, CatchProjNode::catch_all_index,    CatchProjNode::no_handler_bci));\n+\n+  Node* ex_r = new RegionNode(3);\n+  Node* ex_mem_phi = new PhiNode(ex_r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* ex_io_phi = new PhiNode(ex_r, Type::ABIO);\n+  ex_r->init_req(1, slow_excp);\n+  ex_mem_phi->init_req(1, slow_mem);\n+  ex_io_phi->init_req(1, slow_io);\n+  ex_r->init_req(2, ex_ctl);\n+  ex_mem_phi->init_req(2, ex_mem);\n+  ex_io_phi->init_req(2, ex_io);\n+  transform_later(ex_r);\n+  transform_later(ex_mem_phi);\n+  transform_later(ex_io_phi);\n+\n+  \/\/ We don't know how many values are returned. This assumes the\n+  \/\/ worst case, that all available registers are used.\n+  for (uint i = TypeFunc::Parms+1; i < domain->cnt(); i++) {\n+    if (domain->field_at(i) == Type::HALF) {\n+      slow_call->init_req(i, top());\n+      if (alloc_in_place) {\n+        handler_call->init_req(i+1, top());\n+      }\n+      continue;\n+    }\n+    Node* proj = transform_later(new ProjNode(call, i));\n+    slow_call->init_req(i, proj);\n+    if (alloc_in_place) {\n+      handler_call->init_req(i+1, proj);\n+    }\n+  }\n+  \/\/ We can safepoint at that new call\n+  slow_call->copy_call_debug_info(&_igvn, call);\n+  transform_later(slow_call);\n+  if (alloc_in_place) {\n+    transform_later(handler_call);\n+  }\n+\n+  Node* fast_ctl = nullptr;\n+  Node* fast_res = nullptr;\n+  MergeMemNode* fast_mem = nullptr;\n+  if (alloc_in_place) {\n+    fast_ctl = transform_later(new ProjNode(handler_call, TypeFunc::Control));\n+    Node* rawmem = transform_later(new ProjNode(handler_call, TypeFunc::Memory));\n+    fast_res = transform_later(new ProjNode(handler_call, TypeFunc::Parms));\n+    fast_mem = MergeMemNode::make(mem);\n+    fast_mem->set_memory_at(Compile::AliasIdxRaw, rawmem);\n+    transform_later(fast_mem);\n+  }\n+\n+  Node* r = new RegionNode(alloc_in_place ? 4 : 3);\n+  Node* mem_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+  Node* io_phi = new PhiNode(r, Type::ABIO);\n+  Node* res_phi = new PhiNode(r, TypeInstPtr::BOTTOM);\n+  r->init_req(1, no_allocation_ctl);\n+  mem_phi->init_req(1, mem);\n+  io_phi->init_req(1, io);\n+  res_phi->init_req(1, no_allocation_res);\n+  r->init_req(2, slow_norm);\n+  mem_phi->init_req(2, slow_mem);\n+  io_phi->init_req(2, slow_io);\n+  res_phi->init_req(2, slow_res);\n+  if (alloc_in_place) {\n+    r->init_req(3, fast_ctl);\n+    mem_phi->init_req(3, fast_mem);\n+    io_phi->init_req(3, io);\n+    res_phi->init_req(3, fast_res);\n+  }\n+  transform_later(r);\n+  transform_later(mem_phi);\n+  transform_later(io_phi);\n+  transform_later(res_phi);\n+\n+  \/\/ Do not let stores that initialize this buffer be reordered with a subsequent\n+  \/\/ store that would make this buffer accessible by other threads.\n+  MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxBot);\n+  transform_later(mb);\n+  mb->init_req(TypeFunc::Memory, mem_phi);\n+  mb->init_req(TypeFunc::Control, r);\n+  r = new ProjNode(mb, TypeFunc::Control);\n+  transform_later(r);\n+  mem_phi = new ProjNode(mb, TypeFunc::Memory);\n+  transform_later(mem_phi);\n+\n+  assert(projs->nb_resproj == 1, \"unexpected number of results\");\n+  _igvn.replace_in_uses(projs->fallthrough_catchproj, r);\n+  _igvn.replace_in_uses(projs->fallthrough_memproj, mem_phi);\n+  _igvn.replace_in_uses(projs->fallthrough_ioproj, io_phi);\n+  _igvn.replace_in_uses(projs->resproj[0], res_phi);\n+  _igvn.replace_in_uses(projs->catchall_catchproj, ex_r);\n+  _igvn.replace_in_uses(projs->catchall_memproj, ex_mem_phi);\n+  _igvn.replace_in_uses(projs->catchall_ioproj, ex_io_phi);\n+  \/\/ The CatchNode should not use the ex_io_phi. Re-connect it to the catchall_ioproj.\n+  Node* cn = projs->fallthrough_catchproj->in(0);\n+  _igvn.replace_input_of(cn, 1, projs->catchall_ioproj);\n+\n+  _igvn.replace_node(ctl, projs->fallthrough_catchproj);\n+  _igvn.replace_node(mem, projs->fallthrough_memproj);\n+  _igvn.replace_node(io, projs->fallthrough_ioproj);\n+  _igvn.replace_node(res, projs->resproj[0]);\n+  _igvn.replace_node(ex_ctl, projs->catchall_catchproj);\n+  _igvn.replace_node(ex_mem, projs->catchall_memproj);\n+  _igvn.replace_node(ex_io, projs->catchall_ioproj);\n+ }\n+\n@@ -2341,1 +2844,1 @@\n-      subklass = _igvn.transform(LoadKlassNode::make(_igvn, C->immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+      subklass = _igvn.transform(LoadKlassNode::make(_igvn, C->immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n@@ -2353,0 +2856,113 @@\n+\/\/ FlatArrayCheckNode (array1 array2 ...) is expanded into:\n+\/\/\n+\/\/ long mark = array1.mark | array2.mark | ...;\n+\/\/ long locked_bit = markWord::unlocked_value & array1.mark & array2.mark & ...;\n+\/\/ if (locked_bit == 0) {\n+\/\/   \/\/ One array is locked, load prototype header from the klass\n+\/\/   mark = array1.klass.proto | array2.klass.proto | ...\n+\/\/ }\n+\/\/ if ((mark & markWord::flat_array_bit_in_place) == 0) {\n+\/\/    ...\n+\/\/ }\n+void PhaseMacroExpand::expand_flatarraycheck_node(FlatArrayCheckNode* check) {\n+  bool array_inputs = _igvn.type(check->in(FlatArrayCheckNode::ArrayOrKlass))->isa_oopptr() != nullptr;\n+  if (array_inputs) {\n+    Node* mark = MakeConX(0);\n+    Node* locked_bit = MakeConX(markWord::unlocked_value);\n+    Node* mem = check->in(FlatArrayCheckNode::Memory);\n+    for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+      Node* ary = check->in(i);\n+      const TypeOopPtr* t = _igvn.type(ary)->isa_oopptr();\n+      assert(t != nullptr, \"Mixing array and klass inputs\");\n+      assert(!t->is_flat() && !t->is_not_flat(), \"Should have been optimized out\");\n+      Node* mark_adr = basic_plus_adr(ary, oopDesc::mark_offset_in_bytes());\n+      Node* mark_load = _igvn.transform(LoadNode::make(_igvn, nullptr, mem, mark_adr, mark_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+      mark = _igvn.transform(new OrXNode(mark, mark_load));\n+      locked_bit = _igvn.transform(new AndXNode(locked_bit, mark_load));\n+    }\n+    assert(!mark->is_Con(), \"Should have been optimized out\");\n+    Node* cmp = _igvn.transform(new CmpXNode(locked_bit, MakeConX(0)));\n+    Node* is_unlocked = _igvn.transform(new BoolNode(cmp, BoolTest::ne));\n+\n+    \/\/ BoolNode might be shared, replace each if user\n+    Node* old_bol = check->unique_out();\n+    assert(old_bol->is_Bool() && old_bol->as_Bool()->_test._test == BoolTest::ne, \"unexpected condition\");\n+    for (DUIterator_Last imin, i = old_bol->last_outs(imin); i >= imin; --i) {\n+      IfNode* old_iff = old_bol->last_out(i)->as_If();\n+      Node* ctrl = old_iff->in(0);\n+      RegionNode* region = new RegionNode(3);\n+      Node* mark_phi = new PhiNode(region, TypeX_X);\n+\n+      \/\/ Check if array is unlocked\n+      IfNode* iff = _igvn.transform(new IfNode(ctrl, is_unlocked, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+\n+      \/\/ Unlocked: Use bits from mark word\n+      region->init_req(1, _igvn.transform(new IfTrueNode(iff)));\n+      mark_phi->init_req(1, mark);\n+\n+      \/\/ Locked: Load prototype header from klass\n+      ctrl = _igvn.transform(new IfFalseNode(iff));\n+      Node* proto = MakeConX(0);\n+      for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+        Node* ary = check->in(i);\n+        \/\/ Make loads control dependent to make sure they are only executed if array is locked\n+        Node* klass_adr = basic_plus_adr(ary, oopDesc::klass_offset_in_bytes());\n+        Node* klass = _igvn.transform(LoadKlassNode::make(_igvn, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+        Node* proto_adr = basic_plus_adr(klass, in_bytes(Klass::prototype_header_offset()));\n+        Node* proto_load = _igvn.transform(LoadNode::make(_igvn, ctrl, C->immutable_memory(), proto_adr, proto_adr->bottom_type()->is_ptr(), TypeX_X, TypeX_X->basic_type(), MemNode::unordered));\n+        proto = _igvn.transform(new OrXNode(proto, proto_load));\n+      }\n+      region->init_req(2, ctrl);\n+      mark_phi->init_req(2, proto);\n+\n+      \/\/ Check if flat array bits are set\n+      Node* mask = MakeConX(markWord::flat_array_bit_in_place);\n+      Node* masked = _igvn.transform(new AndXNode(_igvn.transform(mark_phi), mask));\n+      cmp = _igvn.transform(new CmpXNode(masked, MakeConX(0)));\n+      Node* is_not_flat = _igvn.transform(new BoolNode(cmp, BoolTest::eq));\n+\n+      ctrl = _igvn.transform(region);\n+      iff = _igvn.transform(new IfNode(ctrl, is_not_flat, PROB_MAX, COUNT_UNKNOWN))->as_If();\n+      _igvn.replace_node(old_iff, iff);\n+    }\n+    _igvn.replace_node(check, C->top());\n+  } else {\n+    \/\/ Fall back to layout helper check\n+    Node* lhs = intcon(0);\n+    for (uint i = FlatArrayCheckNode::ArrayOrKlass; i < check->req(); ++i) {\n+      Node* array_or_klass = check->in(i);\n+      Node* klass = nullptr;\n+      const TypePtr* t = _igvn.type(array_or_klass)->is_ptr();\n+      assert(!t->is_flat() && !t->is_not_flat(), \"Should have been optimized out\");\n+      if (t->isa_oopptr() != nullptr) {\n+        Node* klass_adr = basic_plus_adr(array_or_klass, oopDesc::klass_offset_in_bytes());\n+        klass = transform_later(LoadKlassNode::make(_igvn, C->immutable_memory(), klass_adr, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+      } else {\n+        assert(t->isa_klassptr(), \"Unexpected input type\");\n+        klass = array_or_klass;\n+      }\n+      Node* lh_addr = basic_plus_adr(klass, in_bytes(Klass::layout_helper_offset()));\n+      Node* lh_val = _igvn.transform(LoadNode::make(_igvn, nullptr, C->immutable_memory(), lh_addr, lh_addr->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));\n+      lhs = _igvn.transform(new OrINode(lhs, lh_val));\n+    }\n+    Node* masked = transform_later(new AndINode(lhs, intcon(Klass::_lh_array_tag_flat_value_bit_inplace)));\n+    Node* cmp = transform_later(new CmpINode(masked, intcon(0)));\n+    Node* bol = transform_later(new BoolNode(cmp, BoolTest::eq));\n+    Node* m2b = transform_later(new Conv2BNode(masked));\n+    \/\/ The matcher expects the input to If nodes to be produced by a Bool(CmpI..)\n+    \/\/ pattern, but the input to other potential users (e.g. Phi) to be some\n+    \/\/ other pattern (e.g. a Conv2B node, possibly idealized as a CMoveI).\n+    Node* old_bol = check->unique_out();\n+    for (DUIterator_Last imin, i = old_bol->last_outs(imin); i >= imin; --i) {\n+      Node* user = old_bol->last_out(i);\n+      for (uint j = 0; j < user->req(); j++) {\n+        Node* n = user->in(j);\n+        if (n == old_bol) {\n+          _igvn.replace_input_of(user, j, user->is_If() ? bol : m2b);\n+        }\n+      }\n+    }\n+    _igvn.replace_node(check, C->top());\n+  }\n+}\n+\n@@ -2365,2 +2981,2 @@\n-void PhaseMacroExpand::eliminate_macro_nodes() {\n-  if (C->macro_count() == 0)\n+void PhaseMacroExpand::eliminate_macro_nodes(bool eliminate_locks) {\n+  if (C->macro_count() == 0) {\n@@ -2368,0 +2984,1 @@\n+  }\n@@ -2374,7 +2991,5 @@\n-  \/\/ Before elimination may re-mark (change to Nested or NonEscObj)\n-  \/\/ all associated (same box and obj) lock and unlock nodes.\n-  int cnt = C->macro_count();\n-  for (int i=0; i < cnt; i++) {\n-    Node *n = C->macro_node(i);\n-    if (n->is_AbstractLock()) { \/\/ Lock and Unlock nodes\n-      mark_eliminated_locking_nodes(n->as_AbstractLock());\n+  int iteration = 0;\n+  while (C->macro_count() > 0) {\n+    if (iteration++ > 100) {\n+      assert(false, \"Too slow convergence of macro elimination\");\n+      break;\n@@ -2382,23 +2997,11 @@\n-  }\n-  \/\/ Re-marking may break consistency of Coarsened locks.\n-  if (!C->coarsened_locks_consistent()) {\n-    return; \/\/ recompile without Coarsened locks if broken\n-  } else {\n-    \/\/ After coarsened locks are eliminated locking regions\n-    \/\/ become unbalanced. We should not execute any more\n-    \/\/ locks elimination optimizations on them.\n-    C->mark_unbalanced_boxes();\n-  }\n-  \/\/ First, attempt to eliminate locks\n-  bool progress = true;\n-  while (progress) {\n-    progress = false;\n-    for (int i = C->macro_count(); i > 0; i = MIN2(i - 1, C->macro_count())) { \/\/ more than 1 element can be eliminated at once\n-      Node* n = C->macro_node(i - 1);\n-      bool success = false;\n-      DEBUG_ONLY(int old_macro_count = C->macro_count();)\n-      if (n->is_AbstractLock()) {\n-        success = eliminate_locking_node(n->as_AbstractLock());\n-#ifndef PRODUCT\n-        if (success && PrintOptoStatistics) {\n-          Atomic::inc(&PhaseMacroExpand::_monitor_objects_removed_counter);\n+    \/\/ Postpone lock elimination to after EA when most allocations are eliminated\n+    \/\/ because they might block lock elimination if their escape state isn't\n+    \/\/ determined yet and we only got one chance at eliminating the lock.\n+    if (eliminate_locks) {\n+      \/\/ Before elimination may re-mark (change to Nested or NonEscObj)\n+      \/\/ all associated (same box and obj) lock and unlock nodes.\n+      int cnt = C->macro_count();\n+      for (int i=0; i < cnt; i++) {\n+        Node *n = C->macro_node(i);\n+        if (n->is_AbstractLock()) { \/\/ Lock and Unlock nodes\n+          mark_eliminated_locking_nodes(n->as_AbstractLock());\n@@ -2407,5 +3010,8 @@\n-#endif\n-      assert(success == (C->macro_count() < old_macro_count), \"elimination reduces macro count\");\n-      progress = progress || success;\n-      if (success) {\n-        C->print_method(PHASE_AFTER_MACRO_ELIMINATION_STEP, 5, n);\n+      \/\/ Re-marking may break consistency of Coarsened locks.\n+      if (!C->coarsened_locks_consistent()) {\n+        return; \/\/ recompile without Coarsened locks if broken\n+      } else {\n+        \/\/ After coarsened locks are eliminated locking regions\n+        \/\/ become unbalanced. We should not execute any more\n+        \/\/ locks elimination optimizations on them.\n+        C->mark_unbalanced_boxes();\n@@ -2415,5 +3021,2 @@\n-  }\n-  \/\/ Next, attempt to eliminate allocations\n-  progress = true;\n-  while (progress) {\n-    progress = false;\n+\n+    bool progress = false;\n@@ -2434,2 +3037,5 @@\n-      case Node::Class_CallStaticJava:\n-        success = eliminate_boxing_node(n->as_CallStaticJava());\n+      case Node::Class_CallStaticJava: {\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          success = eliminate_boxing_node(n->as_CallStaticJava());\n+        }\n@@ -2437,0 +3043,1 @@\n+      }\n@@ -2439,1 +3046,8 @@\n-        assert(!n->as_AbstractLock()->is_eliminated(), \"sanity\");\n+        if (eliminate_locks) {\n+          success = eliminate_locking_node(n->as_AbstractLock());\n+#ifndef PRODUCT\n+          if (success && PrintOptoStatistics) {\n+            Atomic::inc(&PhaseMacroExpand::_monitor_objects_removed_counter);\n+          }\n+#endif\n+        }\n@@ -2449,0 +3063,2 @@\n+      case Node::Class_FlatArrayCheck:\n+        break;\n@@ -2466,0 +3082,16 @@\n+\n+    \/\/ Ensure the graph after PhaseMacroExpand::eliminate_macro_nodes is canonical (no igvn\n+    \/\/ transformation is pending). If an allocation is used only in safepoints, elimination of\n+    \/\/ other macro nodes can remove all these safepoints, allowing the allocation to be removed.\n+    \/\/ Hence after igvn we retry removing macro nodes if some progress that has been made in this\n+    \/\/ iteration.\n+    _igvn.set_delay_transform(false);\n+    _igvn.optimize();\n+    if (C->failing()) {\n+      return;\n+    }\n+    _igvn.set_delay_transform(true);\n+\n+    if (!progress) {\n+      break;\n+    }\n@@ -2494,4 +3126,7 @@\n-        \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n-        C->remove_macro_node(n);\n-        _igvn._worklist.push(n);\n-        success = true;\n+        CallStaticJavaNode* call = n->as_CallStaticJava();\n+        if (!call->method()->is_method_handle_intrinsic()) {\n+          \/\/ Remove it from macro list and put on IGVN worklist to optimize.\n+          C->remove_macro_node(n);\n+          _igvn._worklist.push(n);\n+          success = true;\n+        }\n@@ -2606,0 +3241,7 @@\n+    case Node::Class_CallStaticJava:\n+      expand_mh_intrinsic_return(n->as_CallStaticJava());\n+      C->remove_macro_node(n);\n+      break;\n+    case Node::Class_FlatArrayCheck:\n+      expand_flatarraycheck_node(n->as_FlatArrayCheck());\n+      break;\n@@ -2621,1 +3263,1 @@\n-        for (unsigned int i = 0; i < mod_macro->tf()->domain()->cnt() - TypeFunc::Parms; i++) {\n+        for (unsigned int i = 0; i < mod_macro->tf()->domain_cc()->cnt() - TypeFunc::Parms; i++) {\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":953,"deletions":311,"binary":false,"changes":1264,"status":"modified"},{"patch":"@@ -28,0 +28,3 @@\n+#include \"ci\/ciInstanceKlass.hpp\"\n+#include \"opto\/callnode.hpp\"\n+#include \"opto\/node.hpp\"\n@@ -84,1 +87,1 @@\n-  CallProjections _callprojs;\n+  CallProjections* _callprojs;\n@@ -90,0 +93,1 @@\n+                              Node* init_val,\n@@ -94,2 +98,7 @@\n-  Node *value_from_mem(Node *mem, Node *ctl, BasicType ft, const Type *ftype, const TypeOopPtr *adr_t, AllocateNode *alloc);\n-  Node *value_from_mem_phi(Node *mem, BasicType ft, const Type *ftype, const TypeOopPtr *adr_t, AllocateNode *alloc, Node_Stack *value_phis, int level);\n+\n+  void process_field_value_at_safepoint(const Type* field_type, Node* field_val, SafePointNode* sfpt, Unique_Node_List* value_worklist);\n+  bool add_array_elems_to_safepoint(AllocateNode* alloc, const TypeAryPtr* array_type, SafePointNode* sfpt, Unique_Node_List* value_worklist);\n+  bool add_inst_fields_to_safepoint(ciInstanceKlass* iklass, AllocateNode* alloc, Node* base, int offset_minus_header, SafePointNode* sfpt, Unique_Node_List* value_worklist);\n+  Node* value_from_mem(Node* mem, Node* ctl, BasicType ft, const Type* ftype, const TypeOopPtr* adr_t, AllocateNode* alloc);\n+  Node* value_from_mem_phi(Node* mem, BasicType ft, const Type* ftype, const TypeOopPtr* adr_t, AllocateNode* alloc, Node_Stack* value_phis, int level);\n+  Node* inline_type_from_mem(ciInlineKlass* vk, const TypeAryPtr* elem_adr_type, int elem_idx, int offset_in_element, bool null_free, AllocateNode* alloc, SafePointNode* sfpt);\n@@ -101,1 +110,1 @@\n-  void process_users_of_allocation(CallNode *alloc);\n+  void process_users_of_allocation(CallNode *alloc, bool inline_alloc = false);\n@@ -109,0 +118,1 @@\n+  void expand_mh_intrinsic_return(CallStaticJavaNode* call);\n@@ -118,0 +128,1 @@\n+  Node* generate_fair_guard(Node** ctrl, Node* test, RegionNode* region);\n@@ -128,0 +139,4 @@\n+  Node* mark_word_test(Node** ctrl, Node* obj, MergeMemNode* mem, uintptr_t mask_val, RegionNode* region);\n+  Node* generate_flat_array_guard(Node** ctrl, Node* array, MergeMemNode* mem, RegionNode* region);\n+  Node* generate_null_free_array_guard(Node** ctrl, Node* array, MergeMemNode* mem, RegionNode* region);\n+\n@@ -137,0 +152,1 @@\n+                           Node* dest_length,\n@@ -143,0 +159,2 @@\n+                            Node* val,\n+                            Node* raw_val,\n@@ -178,1 +196,3 @@\n-\n+  const TypePtr* adjust_for_flat_array(const TypeAryPtr* top_dest, Node*& src_offset,\n+                                       Node*& dest_offset, Node*& length, BasicType& dest_elem,\n+                                       Node*& dest_length);\n@@ -183,0 +203,2 @@\n+  void expand_flatarraycheck_node(FlatArrayCheckNode* check);\n+\n@@ -204,1 +226,1 @@\n-  void eliminate_macro_nodes();\n+  void eliminate_macro_nodes(bool eliminate_locks = true);\n@@ -208,1 +230,1 @@\n-  SafePointScalarObjectNode* create_scalarized_object_description(AllocateNode *alloc, SafePointNode* sfpt);\n+  SafePointScalarObjectNode* create_scalarized_object_description(AllocateNode *alloc, SafePointNode* sfpt, Unique_Node_List* value_worklist);\n","filename":"src\/hotspot\/share\/opto\/macro.hpp","additions":29,"deletions":7,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -1184,1 +1184,1 @@\n-  n->dump_bfs(1, nullptr, \"\", &ss);\n+  n->dump_bfs(3, nullptr, \"\", &ss);\n@@ -2065,6 +2065,0 @@\n-  if (_delay_transform) {\n-    \/\/ Register the node but don't optimize for now\n-    register_new_node_with_optimizer(n);\n-    return n;\n-  }\n-\n@@ -2077,0 +2071,6 @@\n+  if (_delay_transform) {\n+    \/\/ Add the node to the worklist but don't optimize for now\n+    _worklist.push(n);\n+    return n;\n+  }\n+\n@@ -2349,0 +2349,13 @@\n+void PhaseIterGVN::replace_in_uses(Node* n, Node* m) {\n+  assert(n != nullptr, \"sanity\");\n+  for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+    Node* u = n->fast_out(i);\n+    if (u != n) {\n+      rehash_node_delayed(u);\n+      int nb = u->replace_edge(n, m);\n+      --i, imax -= nb;\n+    }\n+  }\n+  assert(n->outcnt() == 0, \"all uses must be deleted\");\n+}\n+\n@@ -2404,0 +2417,10 @@\n+  \/\/ AndLNode::Ideal folds GraphKit::mark_word_test patterns. Give it a chance to run.\n+  if (n->is_Load() && use->is_Phi()) {\n+    for (DUIterator_Fast imax, i = use->fast_outs(imax); i < imax; i++) {\n+      Node* u = use->fast_out(i);\n+      if (u->Opcode() == Op_AndL) {\n+        worklist.push(u);\n+      }\n+    }\n+  }\n+\n@@ -2501,0 +2524,9 @@\n+  \/\/ Inline type nodes can have other inline types as users. If an input gets\n+  \/\/ updated, make sure that inline type users get a chance for optimization.\n+  if (use->is_InlineType()) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->is_InlineType())\n+        worklist.push(u);\n+    }\n+  }\n@@ -2584,0 +2616,8 @@\n+  if (use_op == Op_CastP2X) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->Opcode() == Op_AndX) {\n+        worklist.push(u);\n+      }\n+    }\n+  }\n@@ -2602,0 +2642,10 @@\n+  \/\/ Give CallStaticJavaNode::remove_useless_allocation a chance to run\n+  if (use->is_Region()) {\n+    Node* c = use;\n+    do {\n+      c = c->unique_ctrl_out_or_null();\n+    } while (c != nullptr && c->is_Region());\n+    if (c != nullptr && c->is_CallStaticJava() && c->as_CallStaticJava()->uncommon_trap_request() != 0) {\n+      worklist.push(c);\n+    }\n+  }\n@@ -2717,1 +2767,1 @@\n-    n->dump(1);\n+    n->dump(3);\n@@ -2839,0 +2889,1 @@\n+  push_cast(worklist, use);\n@@ -2898,0 +2949,12 @@\n+  }\n+}\n+\n+void PhaseCCP::push_cast(Unique_Node_List& worklist, const Node* use) {\n+  uint use_op = use->Opcode();\n+  if (use_op == Op_CastP2X) {\n+    for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n+      Node* u = use->fast_out(i2);\n+      if (u->Opcode() == Op_AndX) {\n+        worklist.push(u);\n+      }\n+    }\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":71,"deletions":8,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -122,0 +122,2 @@\n+  flags(SPLIT_INLINES_ARRAY,            \"Split inlines array\") \\\n+  flags(SPLIT_INLINES_ARRAY_IGVN,       \"IGVN after split inlines array\") \\\n","filename":"src\/hotspot\/share\/opto\/phasetype.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -912,1 +913,8 @@\n-Node *CmpLNode::Ideal( PhaseGVN *phase, bool can_reshape ) {\n+\/\/------------------------------Ideal------------------------------------------\n+Node* CmpLNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  Node* a = nullptr;\n+  Node* b = nullptr;\n+  if (is_double_null_check(phase, a, b) && (phase->type(a)->is_zero_type() || phase->type(b)->is_zero_type())) {\n+    \/\/ Degraded to a simple null check, use old acmp\n+    return new CmpPNode(a, b);\n+  }\n@@ -923,0 +931,25 @@\n+\/\/ Match double null check emitted by Compile::optimize_acmp()\n+bool CmpLNode::is_double_null_check(PhaseGVN* phase, Node*& a, Node*& b) const {\n+  if (in(1)->Opcode() == Op_OrL &&\n+      in(1)->in(1)->Opcode() == Op_CastP2X &&\n+      in(1)->in(2)->Opcode() == Op_CastP2X &&\n+      in(2)->bottom_type()->is_zero_type()) {\n+    assert(EnableValhalla, \"unexpected double null check\");\n+    a = in(1)->in(1)->in(1);\n+    b = in(1)->in(2)->in(1);\n+    return true;\n+  }\n+  return false;\n+}\n+\n+\/\/------------------------------Value------------------------------------------\n+const Type* CmpLNode::Value(PhaseGVN* phase) const {\n+  Node* a = nullptr;\n+  Node* b = nullptr;\n+  if (is_double_null_check(phase, a, b) && (!phase->type(a)->maybe_null() || !phase->type(b)->maybe_null())) {\n+    \/\/ One operand is never nullptr, emit constant false\n+    return TypeInt::CC_GT;\n+  }\n+  return SubNode::Value(phase);\n+}\n+\n@@ -1048,1 +1081,16 @@\n-\n+    if (!unrelated_classes) {\n+      \/\/ Handle inline type arrays\n+      if ((r0->flat_in_array() && r1->not_flat_in_array()) ||\n+          (r1->flat_in_array() && r0->not_flat_in_array())) {\n+        \/\/ One type is in flat arrays but the other type is not. Must be unrelated.\n+        unrelated_classes = true;\n+      } else if ((r0->is_not_flat() && r1->is_flat()) ||\n+                 (r1->is_not_flat() && r0->is_flat())) {\n+        \/\/ One type is a non-flat array and the other type is a flat array. Must be unrelated.\n+        unrelated_classes = true;\n+      } else if ((r0->is_not_null_free() && r1->is_null_free()) ||\n+                 (r1->is_not_null_free() && r0->is_null_free())) {\n+        \/\/ One type is a nullable array and the other type is a null-free array. Must be unrelated.\n+        unrelated_classes = true;\n+      }\n+    }\n@@ -1133,1 +1181,8 @@\n-Node *CmpPNode::Ideal( PhaseGVN *phase, bool can_reshape ) {\n+Node* CmpPNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n+  \/\/ TODO 8284443 in(1) could be cast?\n+  if (in(1)->is_InlineType() && phase->type(in(2))->is_zero_type()) {\n+    \/\/ Null checking a scalarized but nullable inline type. Check the null marker\n+    \/\/ input instead of the oop input to avoid keeping buffer allocations alive.\n+    return new CmpINode(in(1)->as_InlineType()->get_null_marker(), phase->intcon(0));\n+  }\n+\n@@ -1151,1 +1206,3 @@\n-    if (k1 && (k2 || conk2)) {\n+    \/\/ TODO 8366668 add a test for this. Improve this condition\n+    bool doIt = (conk2 && !phase->type(conk2)->isa_aryklassptr());\n+    if (k1 && (k2 || conk2) && doIt) {\n@@ -1205,0 +1262,7 @@\n+  \/\/ Do not fold the subtype check to an array klass pointer comparison for null-able inline type arrays\n+  \/\/ because null-free [LMyValue <: null-able [LMyValue but the klasses are different. Perform a full test.\n+  if (superklass->is_obj_array_klass() && !superklass->as_array_klass()->is_elem_null_free() &&\n+      superklass->as_array_klass()->element_klass()->is_inlinetype()) {\n+    return nullptr;\n+  }\n+\n@@ -1348,0 +1412,37 @@\n+\/\/=============================================================================\n+\/\/------------------------------Value------------------------------------------\n+const Type* FlatArrayCheckNode::Value(PhaseGVN* phase) const {\n+  bool all_not_flat = true;\n+  for (uint i = ArrayOrKlass; i < req(); ++i) {\n+    const Type* t = phase->type(in(i));\n+    if (t == Type::TOP) {\n+      return Type::TOP;\n+    }\n+    if (t->is_ptr()->is_flat()) {\n+      \/\/ One of the input arrays is flat, check always passes\n+      return TypeInt::CC_EQ;\n+    } else if (!t->is_ptr()->is_not_flat()) {\n+      \/\/ One of the input arrays might be flat\n+      all_not_flat = false;\n+    }\n+  }\n+  if (all_not_flat) {\n+    \/\/ None of the input arrays can be flat, check always fails\n+    return TypeInt::CC_GT;\n+  }\n+  return TypeInt::CC;\n+}\n+\n+\/\/------------------------------Ideal------------------------------------------\n+Node* FlatArrayCheckNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  bool changed = false;\n+  \/\/ Remove inputs that are known to be non-flat\n+  for (uint i = ArrayOrKlass; i < req(); ++i) {\n+    const Type* t = phase->type(in(i));\n+    if (t->isa_ptr() && t->is_ptr()->is_not_flat()) {\n+      del_req(i--);\n+      changed = true;\n+    }\n+  }\n+  return changed ? this : nullptr;\n+}\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":105,"deletions":4,"binary":false,"changes":109,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -147,0 +149,24 @@\n+  class Offset {\n+  private:\n+    int _offset;\n+\n+  public:\n+    explicit Offset(int offset) : _offset(offset) {}\n+\n+    const Offset meet(const Offset other) const;\n+    const Offset dual() const;\n+    const Offset add(intptr_t offset) const;\n+    bool operator==(const Offset& other) const {\n+      return _offset == other._offset;\n+    }\n+    bool operator!=(const Offset& other) const {\n+      return _offset != other._offset;\n+    }\n+    int get() const { return _offset; }\n+\n+    void dump2(outputStream *st) const;\n+\n+    static const Offset top;\n+    static const Offset bottom;\n+  };\n+\n@@ -348,0 +374,3 @@\n+  bool is_inlinetypeptr() const;\n+  virtual ciInlineKlass* inline_klass() const;\n+\n@@ -955,2 +984,2 @@\n-  static const TypeTuple *make_range(ciSignature *sig, InterfaceHandling interface_handling = ignore_interfaces);\n-  static const TypeTuple *make_domain(ciInstanceKlass* recv, ciSignature *sig, InterfaceHandling interface_handling);\n+  static const TypeTuple *make_range(ciSignature* sig, InterfaceHandling interface_handling = ignore_interfaces, bool ret_vt_fields = false);\n+  static const TypeTuple *make_domain(ciMethod* method, InterfaceHandling interface_handling, bool vt_fields_as_args = false);\n@@ -985,2 +1014,2 @@\n-  TypeAry(const Type* elem, const TypeInt* size, bool stable) : Type(Array),\n-      _elem(elem), _size(size), _stable(stable) {}\n+  TypeAry(const Type* elem, const TypeInt* size, bool stable, bool flat, bool not_flat, bool not_null_free, bool atomic) : Type(Array),\n+      _elem(elem), _size(size), _stable(stable), _flat(flat), _not_flat(not_flat), _not_null_free(not_null_free), _atomic(atomic) {}\n@@ -997,0 +1026,7 @@\n+\n+  \/\/ Inline type array properties\n+  const bool _flat;             \/\/ Array is flat\n+  const bool _not_flat;         \/\/ Array is never flat\n+  const bool _not_null_free;    \/\/ Array is never null-free\n+  const bool _atomic;           \/\/ Array is atomic\n+\n@@ -1000,1 +1036,2 @@\n-  static const TypeAry* make(const Type* elem, const TypeInt* size, bool stable = false);\n+  static const TypeAry* make(const Type* elem, const TypeInt* size, bool stable = false,\n+                             bool flat = false, bool not_flat = false, bool not_null_free = false, bool atomic = false);\n@@ -1147,1 +1184,1 @@\n-  TypePtr(TYPES t, PTR ptr, int offset,\n+  TypePtr(TYPES t, PTR ptr, Offset offset,\n@@ -1203,1 +1240,4 @@\n-                                                            const T* other_type, ciKlass*& res_klass, bool& res_xk);\n+                                                            const T* other_type, ciKlass*& res_klass, bool& res_xk, bool& res_flat_array);\n+ private:\n+  template<class T> static bool is_meet_subtype_of(const T* sub_type, const T* super_type);\n+ protected:\n@@ -1206,1 +1246,1 @@\n-                                                  ciKlass*& res_klass, bool& res_xk);\n+                                                  ciKlass*& res_klass, bool& res_xk, bool &res_flat, bool &res_not_flat, bool &res_not_null_free, bool &res_atomic);\n@@ -1217,1 +1257,1 @@\n-  const int _offset;            \/\/ Offset into oop, with TOP & BOT\n+  const Offset _offset;         \/\/ Offset into oop, with TOP & BOT\n@@ -1220,1 +1260,1 @@\n-  int offset() const { return _offset; }\n+  int offset() const { return _offset.get(); }\n@@ -1223,1 +1263,1 @@\n-  static const TypePtr *make(TYPES t, PTR ptr, int offset,\n+  static const TypePtr* make(TYPES t, PTR ptr, Offset offset,\n@@ -1232,1 +1272,1 @@\n-  int xadd_offset( intptr_t offset ) const;\n+  Type::Offset xadd_offset(intptr_t offset) const;\n@@ -1235,0 +1275,1 @@\n+  virtual int flat_offset() const { return offset(); }\n@@ -1242,2 +1283,2 @@\n-  int meet_offset( int offset ) const;\n-  int dual_offset( ) const;\n+  Offset meet_offset(int offset) const;\n+  Offset dual_offset() const;\n@@ -1271,0 +1312,9 @@\n+  virtual bool can_be_inline_type() const { return false; }\n+  virtual bool flat_in_array()      const { return false; }\n+  virtual bool not_flat_in_array()  const { return true; }\n+  virtual bool is_flat()            const { return false; }\n+  virtual bool is_not_flat()        const { return false; }\n+  virtual bool is_null_free()       const { return false; }\n+  virtual bool is_not_null_free()   const { return false; }\n+  virtual bool is_atomic()          const { return false; }\n+\n@@ -1288,1 +1338,1 @@\n-  TypeRawPtr( PTR ptr, address bits ) : TypePtr(RawPtr,ptr,0), _bits(bits){}\n+  TypeRawPtr(PTR ptr, address bits) : TypePtr(RawPtr,ptr,Offset(0)), _bits(bits){}\n@@ -1324,1 +1374,1 @@\n- TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int offset, int instance_id,\n+ TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset, Offset field_offset, int instance_id,\n@@ -1365,1 +1415,1 @@\n-  virtual ciKlass* klass() const { return _klass;     }\n+  virtual ciKlass* klass() const { return _klass; }\n@@ -1412,1 +1462,1 @@\n-  static const TypeOopPtr* make(PTR ptr, int offset, int instance_id,\n+  static const TypeOopPtr* make(PTR ptr, Offset offset, int instance_id,\n@@ -1431,1 +1481,4 @@\n-  bool is_known_instance_field() const { return is_known_instance() && _offset >= 0; }\n+  bool is_known_instance_field() const { return is_known_instance() && _offset.get() >= 0; }\n+\n+  virtual bool can_be_inline_type() const { return (_klass == nullptr || _klass->can_be_inline_klass(_klass_is_exact)); }\n+  virtual bool can_be_inline_array() const { ShouldNotReachHere(); return false; }\n@@ -1494,2 +1547,3 @@\n-  TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int off, int instance_id,\n-              const TypePtr* speculative, int inline_depth);\n+  TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset,\n+              bool flat_in_array, int instance_id, const TypePtr* speculative,\n+              int inline_depth);\n@@ -1498,1 +1552,1 @@\n-\n+  bool _flat_in_array; \/\/ Type is flat in arrays\n@@ -1517,1 +1571,1 @@\n-    return make(TypePtr::Constant, k, interfaces, true, o, 0, InstanceBot);\n+    return make(TypePtr::Constant, k, interfaces, true, o, Offset(0));\n@@ -1520,1 +1574,1 @@\n-  static const TypeInstPtr *make(ciObject* o, int offset) {\n+  static const TypeInstPtr *make(ciObject* o, Offset offset) {\n@@ -1523,1 +1577,1 @@\n-    return make(TypePtr::Constant, k, interfaces, true, o, offset, InstanceBot);\n+    return make(TypePtr::Constant, k, interfaces, true, o, offset);\n@@ -1529,1 +1583,1 @@\n-    return make(ptr, klass, interfaces, false, nullptr, 0, InstanceBot);\n+    return make(ptr, klass, interfaces, false, nullptr, Offset(0));\n@@ -1535,1 +1589,1 @@\n-    return make(ptr, klass, interfaces, true, nullptr, 0, InstanceBot);\n+    return make(ptr, klass, interfaces, true, nullptr, Offset(0));\n@@ -1539,1 +1593,1 @@\n-  static const TypeInstPtr *make(PTR ptr, ciKlass* klass, int offset) {\n+  static const TypeInstPtr *make(PTR ptr, ciKlass* klass, Offset offset) {\n@@ -1541,1 +1595,1 @@\n-    return make(ptr, klass, interfaces, false, nullptr, offset, InstanceBot);\n+    return make(ptr, klass, interfaces, false, nullptr, offset);\n@@ -1544,1 +1598,3 @@\n-  static const TypeInstPtr *make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int offset,\n+  \/\/ Make a pointer to an oop.\n+  static const TypeInstPtr* make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, Offset offset,\n+                                 bool flat_in_array = false,\n@@ -1549,1 +1605,1 @@\n-  static const TypeInstPtr *make(PTR ptr, ciKlass* k, bool xk, ciObject* o, int offset, int instance_id = InstanceBot) {\n+  static const TypeInstPtr *make(PTR ptr, ciKlass* k, bool xk, ciObject* o, Offset offset, int instance_id = InstanceBot) {\n@@ -1551,1 +1607,1 @@\n-    return make(ptr, k, interfaces, xk, o, offset, instance_id);\n+    return make(ptr, k, interfaces, xk, o, offset, false, instance_id);\n@@ -1577,0 +1633,4 @@\n+  virtual const TypeInstPtr* cast_to_flat_in_array() const;\n+  virtual bool flat_in_array() const { return _flat_in_array; }\n+  virtual bool not_flat_in_array() const { return !can_be_inline_type() || (_klass->is_inlinetype() && !flat_in_array()); }\n+\n@@ -1584,0 +1644,2 @@\n+  virtual bool can_be_inline_array() const;\n+\n@@ -1608,0 +1670,1 @@\n+  friend class TypeInstPtr;\n@@ -1610,4 +1673,4 @@\n-  TypeAryPtr( PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk,\n-              int offset, int instance_id, bool is_autobox_cache,\n-              const TypePtr* speculative, int inline_depth)\n-    : TypeOopPtr(AryPtr,ptr,k,_array_interfaces,xk,o,offset, instance_id, speculative, inline_depth),\n+  TypeAryPtr(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk,\n+             Offset offset, Offset field_offset, int instance_id, bool is_autobox_cache,\n+             const TypePtr* speculative, int inline_depth)\n+    : TypeOopPtr(AryPtr, ptr, k, _array_interfaces, xk, o, offset, field_offset, instance_id, speculative, inline_depth),\n@@ -1615,1 +1678,2 @@\n-    _is_autobox_cache(is_autobox_cache)\n+    _is_autobox_cache(is_autobox_cache),\n+    _field_offset(field_offset)\n@@ -1621,2 +1685,2 @@\n-        _offset != 0 && _offset != arrayOopDesc::length_offset_in_bytes() &&\n-        _offset != arrayOopDesc::klass_offset_in_bytes()) {\n+        _offset.get() != 0 && _offset.get() != arrayOopDesc::length_offset_in_bytes() &&\n+        _offset.get() != arrayOopDesc::klass_offset_in_bytes()) {\n@@ -1631,0 +1695,6 @@\n+  \/\/ For flat inline type arrays, each field of the inline type in\n+  \/\/ the array has its own memory slice so we need to keep track of\n+  \/\/ which field is accessed\n+  const Offset _field_offset;\n+  Offset meet_field_offset(const Type::Offset offset) const;\n+  Offset dual_field_offset() const;\n@@ -1658,0 +1728,7 @@\n+  \/\/ Inline type array properties\n+  bool is_flat()          const { return _ary->_flat; }\n+  bool is_not_flat()      const { return _ary->_not_flat; }\n+  bool is_null_free()     const { return _ary->_elem->make_ptr() != nullptr && (_ary->_elem->make_ptr()->ptr() == NotNull || _ary->_elem->make_ptr()->ptr() == AnyNull); }\n+  bool is_not_null_free() const { return _ary->_not_null_free; }\n+  bool is_atomic()        const { return _ary->_atomic; }\n+\n@@ -1660,1 +1737,2 @@\n-  static const TypeAryPtr *make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+  static const TypeAryPtr* make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, Offset offset,\n+                                Offset field_offset = Offset::bottom,\n@@ -1665,1 +1743,2 @@\n-  static const TypeAryPtr *make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, int offset,\n+  static const TypeAryPtr* make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, Offset offset,\n+                                Offset field_offset = Offset::bottom,\n@@ -1668,1 +1747,2 @@\n-                                int inline_depth = InlineDepthBottom, bool is_autobox_cache = false);\n+                                int inline_depth = InlineDepthBottom,\n+                                bool is_autobox_cache = false);\n@@ -1687,0 +1767,1 @@\n+  virtual const Type* cleanup_speculative() const;\n@@ -1694,0 +1775,8 @@\n+  \/\/ Inline type array properties\n+  const TypeAryPtr* cast_to_not_flat(bool not_flat = true) const;\n+  const TypeAryPtr* cast_to_not_null_free(bool not_null_free = true) const;\n+  const TypeAryPtr* update_properties(const TypeAryPtr* new_type) const;\n+  jint flat_layout_helper() const;\n+  int flat_elem_size() const;\n+  int flat_log_elem_size() const;\n+\n@@ -1699,1 +1788,8 @@\n-  static jint max_array_length(BasicType etype) ;\n+  static jint max_array_length(BasicType etype);\n+\n+  int flat_offset() const;\n+  const Offset field_offset() const { return _field_offset; }\n+  const TypeAryPtr* with_field_offset(int offset) const;\n+  const TypePtr* add_field_offset_and_offset(intptr_t offset) const;\n+\n+  virtual bool can_be_inline_type() const { return false; }\n@@ -1702,0 +1798,2 @@\n+  virtual bool can_be_inline_array() const;\n+\n@@ -1704,10 +1802,11 @@\n-  static const TypeAryPtr* RANGE;\n-  static const TypeAryPtr* OOPS;\n-  static const TypeAryPtr* NARROWOOPS;\n-  static const TypeAryPtr* BYTES;\n-  static const TypeAryPtr* SHORTS;\n-  static const TypeAryPtr* CHARS;\n-  static const TypeAryPtr* INTS;\n-  static const TypeAryPtr* LONGS;\n-  static const TypeAryPtr* FLOATS;\n-  static const TypeAryPtr* DOUBLES;\n+  static const TypeAryPtr *RANGE;\n+  static const TypeAryPtr *OOPS;\n+  static const TypeAryPtr *NARROWOOPS;\n+  static const TypeAryPtr *BYTES;\n+  static const TypeAryPtr *SHORTS;\n+  static const TypeAryPtr *CHARS;\n+  static const TypeAryPtr *INTS;\n+  static const TypeAryPtr *LONGS;\n+  static const TypeAryPtr *FLOATS;\n+  static const TypeAryPtr *DOUBLES;\n+  static const TypeAryPtr *INLINES;\n@@ -1732,1 +1831,1 @@\n-  TypeMetadataPtr(PTR ptr, ciMetadata* metadata, int offset);\n+  TypeMetadataPtr(PTR ptr, ciMetadata* metadata, Offset offset);\n@@ -1744,1 +1843,1 @@\n-  static const TypeMetadataPtr* make(PTR ptr, ciMetadata* m, int offset);\n+  static const TypeMetadataPtr* make(PTR ptr, ciMetadata* m, Offset offset);\n@@ -1775,1 +1874,1 @@\n-  TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, int offset);\n+  TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, Offset offset);\n@@ -1814,1 +1913,1 @@\n-  static const TypeKlassPtr *make(PTR ptr, ciKlass* klass, int offset, InterfaceHandling interface_handling = ignore_interfaces);\n+  static const TypeKlassPtr *make(PTR ptr, ciKlass* klass, Offset offset, InterfaceHandling interface_handling = ignore_interfaces);\n@@ -1833,0 +1932,6 @@\n+  virtual bool can_be_inline_array() const { ShouldNotReachHere(); return false; }\n+\n+  virtual bool not_flat_in_array_inexact() const {\n+    return true;\n+  }\n+\n@@ -1867,2 +1972,2 @@\n-  TypeInstKlassPtr(PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, int offset)\n-    : TypeKlassPtr(InstKlassPtr, ptr, klass, interfaces, offset) {\n+  TypeInstKlassPtr(PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, Offset offset, bool flat_in_array)\n+    : TypeKlassPtr(InstKlassPtr, ptr, klass, interfaces, offset), _flat_in_array(flat_in_array) {\n@@ -1874,0 +1979,2 @@\n+  const bool _flat_in_array; \/\/ Type is flat in arrays\n+\n@@ -1887,0 +1994,2 @@\n+  virtual bool can_be_inline_type() const { return (_klass == nullptr || _klass->can_be_inline_klass(klass_is_exact())); }\n+\n@@ -1889,1 +1998,1 @@\n-    return make(TypePtr::Constant, k, interfaces, 0);\n+    return make(TypePtr::Constant, k, interfaces, Offset(0));\n@@ -1891,1 +2000,1 @@\n-  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, int offset);\n+  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, Offset offset, bool flat_in_array = false);\n@@ -1893,1 +2002,1 @@\n-  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, int offset) {\n+  static const TypeInstKlassPtr* make(PTR ptr, ciKlass* k, Offset offset) {\n@@ -1914,0 +2023,21 @@\n+  virtual bool flat_in_array() const { return _flat_in_array; }\n+\n+  \/\/ Checks if this klass pointer is not flat in array by also considering exactness information.\n+  virtual bool not_flat_in_array() const {\n+    return !_klass->can_be_inline_klass(klass_is_exact()) || (_klass->is_inlinetype() && !flat_in_array());\n+  }\n+\n+  \/\/ not_flat_in_array() version that assumes that the klass is inexact. This is used for sub type checks where the\n+  \/\/ super klass is always an exact klass constant (and thus possibly known to be not flat in array), while a sub\n+  \/\/ klass could very well be flat in array:\n+  \/\/\n+  \/\/           MyValue       <:       Object\n+  \/\/        flat in array       not flat in array\n+  \/\/\n+  \/\/ Thus, this version checks if we know that the klass is not flat in array even if it's not exact.\n+  virtual bool not_flat_in_array_inexact() const {\n+    return !_klass->can_be_inline_klass() || (_klass->is_inlinetype() && !flat_in_array());\n+  }\n+\n+  virtual bool can_be_inline_array() const;\n+\n@@ -1928,0 +2058,6 @@\n+  const bool _not_flat;      \/\/ Array is never flat\n+  const bool _not_null_free; \/\/ Array is never null-free\n+  const bool _flat;\n+  const bool _null_free;\n+  const bool _atomic;\n+  const bool _vm_type;\n@@ -1930,3 +2066,3 @@\n-  TypeAryKlassPtr(PTR ptr, const Type *elem, ciKlass* klass, int offset)\n-    : TypeKlassPtr(AryKlassPtr, ptr, klass, _array_interfaces, offset), _elem(elem) {\n-    assert(klass == nullptr || klass->is_type_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"\");\n+  TypeAryKlassPtr(PTR ptr, const Type *elem, ciKlass* klass, Offset offset, bool not_flat, int not_null_free, bool flat, bool null_free, bool atomic, bool vm_type)\n+    : TypeKlassPtr(AryKlassPtr, ptr, klass, _array_interfaces, offset), _elem(elem), _not_flat(not_flat), _not_null_free(not_null_free), _flat(flat), _null_free(null_free), _atomic(atomic), _vm_type(vm_type) {\n+    assert(klass == nullptr || klass->is_type_array_klass() || klass->is_flat_array_klass() || !klass->as_obj_array_klass()->base_element_klass()->is_interface(), \"\");\n@@ -1941,0 +2077,24 @@\n+  bool dual_flat() const {\n+    return _flat;\n+  }\n+\n+  bool meet_flat(bool other) const {\n+    return _flat && other;\n+  }\n+\n+  bool dual_null_free() const {\n+    return _null_free;\n+  }\n+\n+  bool meet_null_free(bool other) const {\n+    return _null_free && other;\n+  }\n+\n+  bool dual_atomic() const {\n+    return _atomic;\n+  }\n+\n+  bool meet_atomic(bool other) const {\n+    return _atomic && other;\n+  }\n+\n@@ -1946,1 +2106,1 @@\n-  static const TypeAryKlassPtr *make(PTR ptr, ciKlass* k, int offset, InterfaceHandling interface_handling);\n+  static const TypeAryKlassPtr* make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling, bool not_flat, bool not_null_free, bool flat, bool null_free, bool atomic, bool vm_type = false);\n@@ -1954,2 +2114,5 @@\n-  static const TypeAryKlassPtr *make(PTR ptr, const Type *elem, ciKlass* k, int offset);\n-  static const TypeAryKlassPtr* make(ciKlass* klass, InterfaceHandling interface_handling);\n+  static const TypeAryKlassPtr* make(PTR ptr, const Type* elem, ciKlass* k, Offset offset, bool not_flat, bool not_null_free, bool flat, bool null_free, bool atomic, bool vm_type = false);\n+  static const TypeAryKlassPtr* make(PTR ptr, ciKlass* k, Offset offset, InterfaceHandling interface_handling, bool vm_type = false);\n+  static const TypeAryKlassPtr* make(ciKlass* klass, InterfaceHandling interface_handling, bool vm_type = false);\n+\n+  const TypeAryKlassPtr* get_vm_type(bool vm_type = true) const;\n@@ -1979,0 +2142,8 @@\n+  bool is_flat()          const { return _flat; }\n+  bool is_not_flat()      const { return _not_flat; }\n+  bool is_null_free()     const { return _null_free; }\n+  bool is_not_null_free() const { return _not_null_free; }\n+  bool is_atomic()        const { return _atomic; }\n+  bool is_vm_type()       const { return _vm_type; }\n+  virtual bool can_be_inline_array() const;\n+\n@@ -2114,1 +2285,2 @@\n-  TypeFunc( const TypeTuple *domain, const TypeTuple *range ) : Type(Function),  _domain(domain), _range(range) {}\n+  TypeFunc(const TypeTuple *domain_sig, const TypeTuple *domain_cc, const TypeTuple *range_sig, const TypeTuple *range_cc)\n+    : Type(Function), _domain_sig(domain_sig), _domain_cc(domain_cc), _range_sig(range_sig), _range_cc(range_cc) {}\n@@ -2120,2 +2292,13 @@\n-  const TypeTuple* const _domain;     \/\/ Domain of inputs\n-  const TypeTuple* const _range;      \/\/ Range of results\n+  \/\/ Domains of inputs: inline type arguments are not passed by\n+  \/\/ reference, instead each field of the inline type is passed as an\n+  \/\/ argument. We maintain 2 views of the argument list here: one\n+  \/\/ based on the signature (with an inline type argument as a single\n+  \/\/ slot), one based on the actual calling convention (with a value\n+  \/\/ type argument as a list of its fields).\n+  const TypeTuple* const _domain_sig;\n+  const TypeTuple* const _domain_cc;\n+  \/\/ Range of results. Similar to domains: an inline type result can be\n+  \/\/ returned in registers in which case range_cc lists all fields and\n+  \/\/ is the actual calling convention.\n+  const TypeTuple* const _range_sig;\n+  const TypeTuple* const _range_cc;\n@@ -2135,5 +2318,8 @@\n-  const TypeTuple* domain() const { return _domain; }\n-  const TypeTuple* range()  const { return _range; }\n-\n-  static const TypeFunc *make(ciMethod* method);\n-  static const TypeFunc *make(ciSignature signature, const Type* extra);\n+  const TypeTuple* domain_sig() const { return _domain_sig; }\n+  const TypeTuple* domain_cc()  const { return _domain_cc; }\n+  const TypeTuple* range_sig()  const { return _range_sig; }\n+  const TypeTuple* range_cc()   const { return _range_cc; }\n+\n+  static const TypeFunc* make(ciMethod* method, bool is_osr_compilation = false);\n+  static const TypeFunc *make(const TypeTuple* domain_sig, const TypeTuple* domain_cc,\n+                              const TypeTuple* range_sig, const TypeTuple* range_cc);\n@@ -2147,0 +2333,2 @@\n+  bool returns_inline_type_as_fields() const { return range_sig() != range_cc(); }\n+\n@@ -2423,0 +2611,8 @@\n+inline bool Type::is_inlinetypeptr() const {\n+  return isa_instptr() != nullptr && is_instptr()->instance_klass()->is_inlinetype();\n+}\n+\n+inline ciInlineKlass* Type::inline_klass() const {\n+  return make_ptr()->is_instptr()->instance_klass()->as_inline_klass();\n+}\n+\n@@ -2468,0 +2664,1 @@\n+#define CmpUXNode    CmpULNode\n@@ -2486,0 +2683,1 @@\n+#define Op_StoreX    Op_StoreL\n@@ -2514,0 +2712,1 @@\n+#define CmpUXNode    CmpUNode\n@@ -2532,0 +2731,1 @@\n+#define Op_StoreX    Op_StoreI\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":276,"deletions":76,"binary":false,"changes":352,"status":"modified"},{"patch":"@@ -258,1 +258,1 @@\n-      !(fd.field_type() == T_ARRAY && ftype == T_OBJECT)) {\n+      !(fd.field_type() == T_ARRAY && ftype == T_OBJECT))  {\n@@ -346,1 +346,1 @@\n-check_is_obj_array(JavaThread* thr, jarray jArray) {\n+check_is_obj_or_inline_array(JavaThread* thr, jarray jArray) {\n@@ -348,1 +348,1 @@\n-  if (!aOop->is_objArray()) {\n+  if (!aOop->is_objArray() && !aOop->is_flatArray()) {\n@@ -1629,1 +1629,1 @@\n-      check_is_obj_array(thr, array);\n+      check_is_obj_or_inline_array(thr, array);\n@@ -1643,1 +1643,1 @@\n-      check_is_obj_array(thr, array);\n+      check_is_obj_or_inline_array(thr, array);\n","filename":"src\/hotspot\/share\/prims\/jniCheck.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n@@ -68,0 +69,1 @@\n+#include \"oops\/refArrayOop.inline.hpp\"\n@@ -416,0 +418,148 @@\n+static void validate_array_arguments(Klass* elmClass, jint len, TRAPS) {\n+  if (len < 0) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Array length is negative\");\n+  }\n+  elmClass->initialize(CHECK);\n+  if (elmClass->is_array_klass() || elmClass->is_identity_class()) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Element class is not a value class\");\n+  }\n+  if (elmClass->is_abstract()) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"Element class is abstract\");\n+  }\n+}\n+\n+JVM_ENTRY(jarray, JVM_CopyOfSpecialArray(JNIEnv *env, jarray orig, jint from, jint to))\n+  oop o = JNIHandles::resolve_non_null(orig);\n+  assert(o->is_array(), \"Must be\");\n+  oop array = nullptr;\n+  arrayOop org = (arrayOop)o;\n+  arrayHandle oh(THREAD, org);\n+  ObjArrayKlass* ak = ObjArrayKlass::cast(org->klass());\n+  InlineKlass* vk = InlineKlass::cast(ak->element_klass());\n+  int len = to - from;  \/\/ length of the new array\n+  if (ak->is_null_free_array_klass()) {\n+    if ((len != 0) && (from >= org->length() || to > org->length())) {\n+      THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Copying of null-free array with uninitialized elements\");\n+    }\n+  }\n+  if (org->is_flatArray()) {\n+    FlatArrayKlass* fak = FlatArrayKlass::cast(org->klass());\n+    LayoutKind lk = fak->layout_kind();\n+    ArrayKlass::ArrayProperties props = ArrayKlass::ArrayProperties::DEFAULT;\n+    switch(lk) {\n+      case LayoutKind::ATOMIC_FLAT:\n+        props = ArrayKlass::ArrayProperties::NULL_RESTRICTED;\n+      break;\n+      case LayoutKind::NON_ATOMIC_FLAT:\n+        props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::NULL_RESTRICTED | ArrayKlass::ArrayProperties::NON_ATOMIC);\n+      break;\n+      case LayoutKind::NULLABLE_ATOMIC_FLAT:\n+      props = ArrayKlass::ArrayProperties::NON_ATOMIC;\n+      break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    array = oopFactory::new_flatArray(vk, len, props, lk, CHECK_NULL);\n+    arrayHandle ah(THREAD, (arrayOop)array);\n+    int end = to < oh()->length() ? to : oh()->length();\n+    for (int i = from; i < end; i++) {\n+      void* src = ((flatArrayOop)oh())->value_at_addr(i, fak->layout_helper());\n+      void* dst = ((flatArrayOop)ah())->value_at_addr(i - from, fak->layout_helper());\n+      vk->copy_payload_to_addr(src, dst, lk, false);\n+    }\n+    array = ah();\n+  } else {\n+    ArrayKlass::ArrayProperties props = org->is_null_free_array() ? ArrayKlass::ArrayProperties::NULL_RESTRICTED : ArrayKlass::ArrayProperties::DEFAULT;\n+    array = oopFactory::new_objArray(vk, len, props,  CHECK_NULL);\n+    int end = to < oh()->length() ? to : oh()->length();\n+    for (int i = from; i < end; i++) {\n+      if (i < ((objArrayOop)oh())->length()) {\n+        ((objArrayOop)array)->obj_at_put(i - from, ((objArrayOop)oh())->obj_at(i));\n+      } else {\n+        assert(!ak->is_null_free_array_klass(), \"Must be a nullable array\");\n+        ((objArrayOop)array)->obj_at_put(i - from, nullptr);\n+      }\n+    }\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jarray, JVM_NewNullRestrictedNonAtomicArray(JNIEnv *env, jclass elmClass, jint len, jobject initVal))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  oop init = JNIHandles::resolve(initVal);\n+  if (init == nullptr) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Initial value cannot be null\");\n+  }\n+  Handle init_h(THREAD, init);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  if (klass != init_h()->klass()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Type mismatch between array and initial value\");\n+  }\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  ArrayKlass::ArrayProperties props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::NON_ATOMIC | ArrayKlass::ArrayProperties::NULL_RESTRICTED);\n+  objArrayOop array = oopFactory::new_objArray(klass, len, props, CHECK_NULL);\n+  for (int i = 0; i < len; i++) {\n+    array->obj_at_put(i, init_h() \/*, CHECK_NULL*\/ );\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jarray, JVM_NewNullRestrictedAtomicArray(JNIEnv *env, jclass elmClass, jint len, jobject initVal))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  oop init = JNIHandles::resolve(initVal);\n+  if (init == nullptr) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Initial value cannot be null\");\n+  }\n+  Handle init_h(THREAD, init);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  if (klass != init_h()->klass()) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), \"Type mismatch between array and initial value\");\n+  }\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  ArrayKlass::ArrayProperties props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::NULL_RESTRICTED);\n+  objArrayOop array = oopFactory::new_objArray(klass, len, props, CHECK_NULL);\n+  for (int i = 0; i < len; i++) {\n+    array->obj_at_put(i, init_h() \/*, CHECK_NULL*\/ );\n+  }\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jarray, JVM_NewNullableAtomicArray(JNIEnv *env, jclass elmClass, jint len))\n+  oop mirror = JNIHandles::resolve_non_null(elmClass);\n+  Klass* klass = java_lang_Class::as_Klass(mirror);\n+  klass->initialize(CHECK_NULL);\n+  validate_array_arguments(klass, len, CHECK_NULL);\n+  InlineKlass* vk = InlineKlass::cast(klass);\n+  ArrayKlass::ArrayProperties props = (ArrayKlass::ArrayProperties)(ArrayKlass::ArrayProperties::DEFAULT);\n+  objArrayOop array = oopFactory::new_objArray(klass, len, props, CHECK_NULL);\n+  return (jarray) JNIHandles::make_local(THREAD, array);\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsFlatArray(JNIEnv *env, jobject obj))\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  return oop->is_flatArray();\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsNullRestrictedArray(JNIEnv *env, jobject obj))\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  return oop->is_null_free_array();\n+JVM_END\n+\n+JVM_ENTRY(jboolean, JVM_IsAtomicArray(JNIEnv *env, jobject obj))\n+  \/\/ There are multiple cases where an array can\/must support atomic access:\n+  \/\/   - the array is a reference array\n+  \/\/   - the array uses an atomic flat layout: NULLABLE_ATOMIC_FLAT or ATOMIC_FLAT\n+  \/\/   - the array is flat and its component type is naturally atomic\n+  arrayOop oop = arrayOop(JNIHandles::resolve_non_null(obj));\n+  if (oop->is_refArray()) return true;\n+  if (oop->is_flatArray()) {\n+    FlatArrayKlass* fak = FlatArrayKlass::cast(oop->klass());\n+    if (fak->layout_kind() == LayoutKind::ATOMIC_FLAT || fak->layout_kind() == LayoutKind::NULLABLE_ATOMIC_FLAT) {\n+      return true;\n+    }\n+    if (fak->element_klass()->is_naturally_atomic()) return true;\n+  }\n+  return false;\n+JVM_END\n@@ -624,2 +774,22 @@\n-  return handle == nullptr ? 0 :\n-         checked_cast<jint>(ObjectSynchronizer::FastHashCode (THREAD, JNIHandles::resolve_non_null(handle)));\n+  if (handle == nullptr) {\n+    return 0;\n+  }\n+  oop obj = JNIHandles::resolve_non_null(handle);\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+      JavaValue result(T_INT);\n+      JavaCallArguments args;\n+      Handle ho(THREAD, obj);\n+      args.push_oop(ho);\n+      methodHandle method(THREAD, Universe::value_object_hash_code_method());\n+      JavaCalls::call(&result, method, &args, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        if (!PENDING_EXCEPTION->is_a(vmClasses::Error_klass())) {\n+          Handle e(THREAD, PENDING_EXCEPTION);\n+          CLEAR_PENDING_EXCEPTION;\n+          THROW_MSG_CAUSE_(vmSymbols::java_lang_InternalError(), \"Internal error in hashCode\", e, false);\n+        }\n+      }\n+      return result.get_jint();\n+  } else {\n+    return checked_cast<jint>(ObjectSynchronizer::FastHashCode(THREAD, obj));\n+  }\n@@ -673,0 +843,6 @@\n+  if (klass->is_inline_klass()) {\n+    \/\/ Value instances have no identity, so return the current instance instead of allocating a new one\n+    \/\/ Value classes cannot have finalizers, so the method can return immediately\n+    return JNIHandles::make_local(THREAD, obj());\n+  }\n+\n@@ -1167,1 +1343,2 @@\n-    size = InstanceKlass::cast(klass)->local_interfaces()->length();\n+    InstanceKlass* ik = InstanceKlass::cast(klass);\n+    size = ik->local_interfaces()->length();\n@@ -1180,1 +1357,2 @@\n-      Klass* k = InstanceKlass::cast(klass)->local_interfaces()->at(index);\n+      InstanceKlass* ik = InstanceKlass::cast(klass);\n+      Klass* k = ik->local_interfaces()->at(index);\n@@ -1201,1 +1379,0 @@\n-\n@@ -1685,1 +1862,1 @@\n-    if (want_constructor && !method->is_object_initializer()) {\n+    if (want_constructor && !method->is_object_constructor()) {\n@@ -1689,1 +1866,1 @@\n-        (method->is_object_initializer() || method->is_static_initializer() ||\n+        (method->is_object_constructor() || method->is_class_initializer() ||\n@@ -1717,0 +1894,1 @@\n+        assert(method->is_object_constructor(), \"must be\");\n@@ -1999,1 +2177,1 @@\n-  if (m->is_object_initializer()) {\n+  if (m->is_object_constructor()) {\n@@ -2002,1 +2180,0 @@\n-    \/\/ new_method accepts <clinit> as Method here\n@@ -2452,1 +2629,1 @@\n-  return method->name() == vmSymbols::object_initializer_name();\n+  return method->is_object_constructor();\n@@ -3260,0 +3437,4 @@\n+JVM_LEAF(jboolean, JVM_IsValhallaEnabled(void))\n+  return EnableValhalla ? JNI_TRUE : JNI_FALSE;\n+JVM_END\n+\n@@ -3339,1 +3520,3 @@\n-    objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n+    objArrayHandle args(THREAD, (objArrayOop)JNIHandles::resolve(args0));\n+    assert(args() == nullptr || !args->is_flatArray(), \"args are never flat or are they???\");\n+\n@@ -3359,0 +3542,2 @@\n+  objArrayHandle args(THREAD, (objArrayOop)JNIHandles::resolve(args0));\n+  assert(args() == nullptr || !args->is_flatArray(), \"args are never flat or are they???\");\n@@ -3360,1 +3545,0 @@\n-  objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));\n@@ -3600,0 +3784,1 @@\n+  refArrayHandle rah(THREAD, (refArrayOop)ah()); \/\/ j.l.Thread is an identity class, arrays are always reference arrays\n@@ -3605,1 +3790,1 @@\n-    oop thread_obj = ah->obj_at(i);\n+    oop thread_obj = rah->obj_at(i);\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":198,"deletions":13,"binary":false,"changes":211,"status":"modified"},{"patch":"@@ -2730,4 +2730,0 @@\n-  if (!java_lang_Class::is_primitive(k_mirror)) {\n-    \/\/ Reset the deleted  ACC_SUPER bit (deleted in compute_modifier_flags()).\n-    result |= JVM_ACC_SUPER;\n-  }\n@@ -2851,1 +2847,2 @@\n-                                                     flds.access_flags().is_static());\n+                                                     flds.access_flags().is_static(),\n+                                                     flds.field_descriptor().is_flat());\n@@ -2889,2 +2886,3 @@\n-    Array<InstanceKlass*>* interface_list = InstanceKlass::cast(k)->local_interfaces();\n-    const int result_length = (interface_list == nullptr ? 0 : interface_list->length());\n+    InstanceKlass* ik = InstanceKlass::cast(k);\n+    Array<InstanceKlass*>* interface_list = ik->local_interfaces();\n+    int result_length = (interface_list == nullptr ? 0 : interface_list->length());\n@@ -3086,3 +3084,6 @@\n-  {\n-    jint result = (jint) mirror->identity_hash();\n-    *hash_code_ptr = result;\n+  if (mirror->is_inline_type()) {\n+    \/\/ For inline types, use the klass as a hash code.\n+    \/\/ TBD to improve this (see also JvmtiTagMapKey::get_hash for similar case).\n+    *hash_code_ptr = (jint)((int64_t)mirror->klass() >> 3);\n+  } else {\n+    *hash_code_ptr = (jint)mirror->identity_hash();\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnv.cpp","additions":11,"deletions":10,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"classfile\/klassFactory.hpp\"\n@@ -34,1 +35,0 @@\n-#include \"classfile\/klassFactory.hpp\"\n@@ -615,2 +615,1 @@\n-    \/\/ At this stage JVM_CONSTANT_UnresolvedClassInError should not be\n-    \/\/ here\n+    \/\/ At this stage JVM_CONSTANT_UnresolvedClassInError should not be here\n@@ -1933,0 +1932,6 @@\n+  \/\/ rewrite constant pool references in the LoadableDescriptors attribute:\n+  if (!rewrite_cp_refs_in_loadable_descriptors_attribute(scratch_class)) {\n+    \/\/ propagate failure back to caller\n+    return false;\n+  }\n+\n@@ -2081,0 +2086,13 @@\n+\/\/ Rewrite constant pool references in the LoadableDescriptors attribute.\n+bool VM_RedefineClasses::rewrite_cp_refs_in_loadable_descriptors_attribute(\n+       InstanceKlass* scratch_class) {\n+\n+  Array<u2>* loadable_descriptors = scratch_class->loadable_descriptors();\n+  assert(loadable_descriptors != nullptr, \"unexpected null loadable_descriptors\");\n+  for (int i = 0; i < loadable_descriptors->length(); i++) {\n+    u2 cp_index = loadable_descriptors->at(i);\n+    loadable_descriptors->at_put(i, find_new_index(cp_index));\n+  }\n+  return true;\n+}\n+\n@@ -3268,0 +3286,8 @@\n+   if (frame_type == 246) {  \/\/ EARLY_LARVAL\n+     \/\/ rewrite_cp_refs in  unset fields and fall through.\n+     rewrite_cp_refs_in_early_larval_stackmaps(stackmap_p, stackmap_end, calc_number_of_entries, frame_type);\n+     \/\/ The larval frames point to the next frame, so advance to the next frame and fall through.\n+     frame_type = *stackmap_p;\n+     stackmap_p++;\n+   }\n+\n@@ -3477,0 +3503,23 @@\n+void VM_RedefineClasses::rewrite_cp_refs_in_early_larval_stackmaps(\n+       address& stackmap_p_ref, address stackmap_end, u2 frame_i,\n+       u1 frame_type) {\n+\n+    u2 num_early_larval_stackmaps = Bytes::get_Java_u2(stackmap_p_ref);\n+    stackmap_p_ref += 2;\n+\n+    for (u2 i = 0; i < num_early_larval_stackmaps; i++) {\n+\n+      u2 name_and_ref_index = Bytes::get_Java_u2(stackmap_p_ref);\n+      u2 new_cp_index = find_new_index(name_and_ref_index);\n+      if (new_cp_index != 0) {\n+        log_debug(redefine, class, stackmap)(\"mapped old name_and_ref_index=%d\", name_and_ref_index);\n+        Bytes::put_Java_u2(stackmap_p_ref, new_cp_index);\n+        name_and_ref_index = new_cp_index;\n+      }\n+      log_debug(redefine, class, stackmap)\n+        (\"frame_i=%u, frame_type=%u, name_and_ref_index=%d\", frame_i, frame_type, name_and_ref_index);\n+\n+      stackmap_p_ref += 2;\n+    }\n+} \/\/ rewrite_cp_refs_in_early_larval_stackmaps\n+\n","filename":"src\/hotspot\/share\/prims\/jvmtiRedefineClasses.cpp","additions":52,"deletions":3,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"memory\/iterator.inline.hpp\"\n@@ -61,0 +62,1 @@\n+#include \"oops\/access.hpp\"\n@@ -63,0 +65,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -70,0 +73,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -88,0 +92,1 @@\n+#include \"runtime\/keepStackGCProcessed.hpp\"\n@@ -1953,0 +1958,103 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return nullptr;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(ih->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return nullptr;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayHandle result_array =\n+      oopFactory::new_objArray_handle(vmClasses::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ih->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+WB_END\n+\n+\/\/ Collect Object oops but not value objects...loaded from heap\n+class CollectObjectOops : public BasicOopIterateClosure {\n+  public:\n+  GrowableArray<Handle>* _array;\n+\n+  CollectObjectOops() {\n+      _array = new GrowableArray<Handle>(128);\n+  }\n+\n+  void add_oop(oop o) {\n+    Handle oh = Handle(Thread::current(), o);\n+    if (oh != nullptr && oh->is_inline_type()) {\n+      oh->oop_iterate(this);\n+    } else {\n+      _array->append(oh);\n+    }\n+  }\n+\n+  template <class T> inline void add_oop(T* p) { add_oop(HeapAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    objArrayHandle result_array =\n+        oopFactory::new_objArray_handle(vmClasses::Object_klass(), _array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < _array->length(); i++) {\n+      result_array->obj_at_put(i, _array->at(i)());\n+    }\n+    return (jobjectArray)JNIHandles::make_local(THREAD, result_array());\n+  }\n+};\n+\n+\/\/ Collect Object oops but not value objects...loaded from frames\n+class CollectFrameObjectOops : public BasicOopIterateClosure {\n+ public:\n+  CollectObjectOops _collect;\n+\n+  template <class T> inline void add_oop(T* p) { _collect.add_oop(RawAccess<>::oop_load(p)); }\n+  void do_oop(oop* o) { add_oop(o); }\n+  void do_oop(narrowOop* v) { add_oop(v); }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    return _collect.create_jni_result(env, THREAD);\n+  }\n+};\n+\n+\/\/ Collect Object oops for the given oop, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(thread);\n+  Handle objh(thread, JNIHandles::resolve(thing));\n+  CollectObjectOops collectOops;\n+  objh->oop_iterate(&collectOops);\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\/\/ Collect Object oops for the given frame deep, iterate through value objects\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  KeepStackGCProcessedMark ksgcpm(THREAD);\n+  ResourceMark rm(THREAD);\n+  CollectFrameObjectOops collectOops;\n+  StackFrameStream sfs(thread, true \/* update *\/, true \/* process_frames *\/);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, nullptr, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n@@ -2919,0 +3027,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":114,"deletions":0,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -78,0 +78,1 @@\n+#include <string.h>\n@@ -365,0 +366,12 @@\n+bool Arguments::patching_migrated_classes(const char* property, const char* value) {\n+  if (strncmp(property, MODULE_PROPERTY_PREFIX, MODULE_PROPERTY_PREFIX_LEN) == 0) {\n+    const char* property_suffix = property + MODULE_PROPERTY_PREFIX_LEN;\n+    if (matches_property_suffix(property_suffix, PATCH, PATCH_LEN)) {\n+      if (strcmp(value, \"java.base-valueclasses.jar\")) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -1787,1 +1800,0 @@\n-static unsigned int patch_mod_count = 0;\n@@ -1794,1 +1806,1 @@\n-  if (!CDSConfig::check_vm_args_consistency(patch_mod_javabase, mode_flag_cmd_line)) {\n+  if (!CDSConfig::check_vm_args_consistency(mode_flag_cmd_line)) {\n@@ -2061,1 +2073,1 @@\n-      add_patch_mod_prefix(module_name, module_equal + 1);\n+      add_patch_mod_prefix(module_name, module_equal + 1, false \/* no append *\/, false \/* no cds *\/);\n@@ -2063,3 +2075,0 @@\n-      if (!create_numbered_module_property(\"jdk.module.patch\", patch_mod_tail, patch_mod_count++)) {\n-        return JNI_ENOMEM;\n-      }\n@@ -2073,0 +2082,64 @@\n+\/\/ VALUECLASS_STR must match string used in the build\n+#define VALUECLASS_STR \"valueclasses\"\n+#define VALUECLASS_JAR \"-\" VALUECLASS_STR \".jar\"\n+\n+\/\/ Finalize --patch-module args and --enable-preview related to value class module patches.\n+\/\/ Create all numbered properties passing module patches.\n+int Arguments::finalize_patch_module() {\n+  \/\/ If --enable-preview and EnableValhalla is true, each module may have value classes that\n+  \/\/ are to be patched into the module.\n+  \/\/ For each <module>-valueclasses.jar in <JAVA_HOME>\/lib\/valueclasses\/\n+  \/\/ appends the equivalent of --patch-module <module>=<JAVA_HOME>\/lib\/valueclasses\/<module>-valueclasses.jar\n+  if (enable_preview() && EnableValhalla) {\n+    char * valueclasses_dir = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+    const char * fileSep = os::file_separator();\n+\n+    jio_snprintf(valueclasses_dir, JVM_MAXPATHLEN, \"%s%slib%s\" VALUECLASS_STR \"%s\",\n+                 Arguments::get_java_home(), fileSep, fileSep, fileSep);\n+    DIR* dir = os::opendir(valueclasses_dir);\n+    if (dir != nullptr) {\n+      char * module_name = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+      char * path = AllocateHeap(JVM_MAXPATHLEN, mtArguments);\n+\n+      for (dirent * entry = os::readdir(dir); entry != nullptr; entry = os::readdir(dir)) {\n+        \/\/ Test if file ends-with \"-valueclasses.jar\"\n+        int len = (int)strlen(entry->d_name) - (sizeof(VALUECLASS_JAR) - 1);\n+        if (len <= 0 || strcmp(&entry->d_name[len], VALUECLASS_JAR) != 0) {\n+          continue;         \/\/ too short or not the expected suffix\n+        }\n+\n+        strcpy(module_name, entry->d_name);\n+        module_name[len] = '\\0';     \/\/ truncate to just module-name\n+\n+        jio_snprintf(path, JVM_MAXPATHLEN, \"%s%s\", valueclasses_dir, &entry->d_name);\n+        add_patch_mod_prefix(module_name, path, true \/* append *\/, true \/* cds OK*\/);\n+        log_info(class)(\"--enable-preview appending value classes for module %s: %s\", module_name, entry->d_name);\n+      }\n+      FreeHeap(module_name);\n+      FreeHeap(path);\n+      os::closedir(dir);\n+    }\n+    FreeHeap(valueclasses_dir);\n+  }\n+\n+  \/\/ Create numbered properties for each module that has been patched either\n+  \/\/ by --patch-module or --enable-preview\n+  \/\/ Format is \"jdk.module.patch.<n>=<module_name>=<path>\"\n+  if (_patch_mod_prefix != nullptr) {\n+    char * prop_value = AllocateHeap(JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, mtArguments);\n+    unsigned int patch_mod_count = 0;\n+\n+    for (GrowableArrayIterator<ModulePatchPath *> it = _patch_mod_prefix->begin();\n+            it != _patch_mod_prefix->end(); ++it) {\n+      jio_snprintf(prop_value, JVM_MAXPATHLEN + JVM_MAXPATHLEN + 1, \"%s=%s\",\n+                   (*it)->module_name(), (*it)->path_string());\n+      if (!create_numbered_module_property(\"jdk.module.patch\", prop_value, patch_mod_count++)) {\n+        FreeHeap(prop_value);\n+        return JNI_ENOMEM;\n+      }\n+    }\n+    FreeHeap(prop_value);\n+  }\n+  return JNI_OK;\n+}\n+\n@@ -2341,0 +2414,4 @@\n+      \/\/ --enable-preview enables Valhalla, EnableValhalla VM option will eventually be removed before integration\n+      if (FLAG_SET_CMDLINE(EnableValhalla, true) != JVMFlag::SUCCESS) {\n+        return JNI_EINVAL;\n+      }\n@@ -2820,10 +2897,5 @@\n-void Arguments::add_patch_mod_prefix(const char* module_name, const char* path) {\n-  \/\/ For java.base check for duplicate --patch-module options being specified on the command line.\n-  \/\/ This check is only required for java.base, all other duplicate module specifications\n-  \/\/ will be checked during module system initialization.  The module system initialization\n-  \/\/ will throw an ExceptionInInitializerError if this situation occurs.\n-  if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n-    if (patch_mod_javabase) {\n-      vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n-    } else {\n-      patch_mod_javabase = true;\n+void Arguments::add_patch_mod_prefix(const char* module_name, const char* path, bool allow_append, bool allow_cds) {\n+  if (!allow_cds) {\n+    CDSConfig::set_module_patching_disables_cds();\n+    if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+      CDSConfig::set_java_base_module_patching_disables_cds();\n@@ -2838,1 +2910,18 @@\n-  _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  \/\/ Scan patches for matching module\n+  int i = _patch_mod_prefix->find_if([&](ModulePatchPath* patch) {\n+    return (strcmp(module_name, patch->module_name()) == 0);\n+  });\n+  if (i == -1) {\n+    _patch_mod_prefix->push(new ModulePatchPath(module_name, path));\n+  } else {\n+    if (allow_append) {\n+      \/\/ append path to existing module entry\n+      _patch_mod_prefix->at(i)->append_path(path);\n+    } else {\n+      if (strcmp(module_name, JAVA_BASE_NAME) == 0) {\n+        vm_exit_during_initialization(\"Cannot specify \" JAVA_BASE_NAME \" more than once to --patch-module\");\n+      } else {\n+        vm_exit_during_initialization(\"Cannot specify a module more than once to --patch-module\", module_name);\n+      }\n+    }\n+  }\n@@ -2951,1 +3040,2 @@\n-  if (!check_vm_args_consistency()) {\n+  \/\/ finalize --module-patch and related --enable-preview\n+  if (finalize_patch_module() != JNI_OK) {\n@@ -2955,0 +3045,3 @@\n+  if (!check_vm_args_consistency()) {\n+    return JNI_ERR;\n+  }\n@@ -3840,0 +3933,12 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !CDSConfig::is_dumping_archive() && !UseSharedSpaces)) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported.\n+    \/\/ Also these aren't useful in -Xint. However, don't disable them when dumping or using\n+    \/\/ the CDS archive, as the values must match between dumptime and runtime.\n+    FLAG_SET_DEFAULT(InlineTypePassFieldsAsArgs, false);\n+    FLAG_SET_DEFAULT(InlineTypeReturnedAsFields, false);\n+  }\n+  if (!UseNonAtomicValueFlattening && !UseNullableValueFlattening && !UseAtomicValueFlattening) {\n+    \/\/ Flattening is disabled\n+    FLAG_SET_DEFAULT(UseArrayFlattening, false);\n+    FLAG_SET_DEFAULT(UseFieldFlattening, false);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":123,"deletions":18,"binary":false,"changes":141,"status":"modified"},{"patch":"@@ -53,0 +53,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/flatArrayOop.hpp\"\n@@ -58,0 +60,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -303,0 +306,18 @@\n+\n+static Klass* get_refined_array_klass(Klass* k, frame* fr, RegisterMap* map, ObjectValue* sv, TRAPS) {\n+  \/\/ If it's an array, get the properties\n+  if (k->is_array_klass() && !k->is_typeArray_klass()) {\n+    assert(!k->is_refArray_klass() && !k->is_flatArray_klass(), \"Unexpected refined klass\");\n+    nmethod* nm = fr->cb()->as_nmethod_or_null();\n+    if (nm->is_compiled_by_c2()) {\n+      assert(sv->has_properties(), \"Property information is missing\");\n+      ArrayKlass::ArrayProperties props = static_cast<ArrayKlass::ArrayProperties>(StackValue::create_stack_value(fr, map, sv->properties())->get_jint());\n+      k = ObjArrayKlass::cast(k)->klass_with_properties(props, THREAD);\n+    } else {\n+      \/\/ TODO Graal needs to be fixed. Just go with the default properties for now\n+      k = ObjArrayKlass::cast(k)->klass_with_properties(ArrayKlass::ArrayProperties::DEFAULT, THREAD);\n+    }\n+  }\n+  return k;\n+}\n+\n@@ -304,2 +325,2 @@\n-static void print_objects(JavaThread* deoptee_thread,\n-                          GrowableArray<ScopeValue*>* objects, bool realloc_failures) {\n+static void print_objects(JavaThread* deoptee_thread, frame* deoptee, RegisterMap* map,\n+                          GrowableArray<ScopeValue*>* objects, bool realloc_failures, TRAPS) {\n@@ -321,0 +342,1 @@\n+    k = get_refined_array_klass(k, deoptee, map, sv, THREAD);\n@@ -354,2 +376,13 @@\n-  bool save_oop_result = chunk->at(0)->scope()->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n-  Handle return_value;\n+  ScopeDesc* scope = chunk->at(0)->scope();\n+  bool save_oop_result = scope->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);\n+  \/\/ In case of the return of multiple values, we must take care\n+  \/\/ of all oop return values.\n+  GrowableArray<Handle> return_oops;\n+  InlineKlass* vk = nullptr;\n+  if (save_oop_result && scope->return_scalarized()) {\n+    vk = InlineKlass::returned_inline_klass(map);\n+    if (vk != nullptr) {\n+      vk->save_oop_fields(map, return_oops);\n+      save_oop_result = false;\n+    }\n+  }\n@@ -361,1 +394,1 @@\n-    return_value = Handle(thread, result);\n+    return_oops.push(Handle(thread, result));\n@@ -368,1 +401,1 @@\n-  if (objects != nullptr) {\n+  if (objects != nullptr || vk != nullptr) {\n@@ -373,1 +406,9 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+      if (vk != nullptr) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, CHECK_AND_CLEAR_(true));\n+      }\n+      if (objects != nullptr) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+        guarantee(compiled_method != nullptr, \"deopt must be associated with an nmethod\");\n+        bool is_jvmci = compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, is_jvmci, CHECK_AND_CLEAR_(true));\n+      }\n@@ -378,1 +419,9 @@\n-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+      if (vk != nullptr) {\n+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, THREAD);\n+      }\n+      if (objects != nullptr) {\n+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);\n+        guarantee(compiled_method != nullptr, \"deopt must be associated with an nmethod\");\n+        bool is_jvmci = compiled_method->is_compiled_by_jvmci();\n+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, is_jvmci, THREAD);\n+      }\n@@ -381,5 +430,2 @@\n-    guarantee(compiled_method != nullptr, \"deopt must be associated with an nmethod\");\n-    bool is_jvmci = compiled_method->is_compiled_by_jvmci();\n-    Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, is_jvmci);\n-    if (TraceDeoptimization) {\n-      print_objects(deoptee_thread, objects, realloc_failures);\n+    if (TraceDeoptimization && objects != nullptr) {\n+      print_objects(deoptee_thread, &deoptee, &map, objects, realloc_failures, thread);\n@@ -388,1 +434,1 @@\n-  if (save_oop_result) {\n+  if (save_oop_result || vk != nullptr) {\n@@ -390,1 +436,2 @@\n-    deoptee.set_saved_oop_result(&map, return_value());\n+    assert(return_oops.length() == 1, \"no inline type\");\n+    deoptee.set_saved_oop_result(&map, return_oops.pop()());\n@@ -725,1 +772,1 @@\n-  \/\/ If the sender is deoptimized the we must retrieve the address of the handler\n+  \/\/ If the sender is deoptimized we must retrieve the address of the handler\n@@ -1241,2 +1288,10 @@\n-\n-    oop obj = nullptr;\n+    k = get_refined_array_klass(k, fr, reg_map, sv, THREAD);\n+\n+    \/\/ Check if the object may be null and has an additional null_marker input that needs\n+    \/\/ to be checked before using the field values. Skip re-allocation if it is null.\n+    if (k->is_inline_klass() && sv->has_properties()) {\n+      jint null_marker = StackValue::create_stack_value(fr, reg_map, sv->properties())->get_jint();\n+      if (null_marker == 0) {\n+        continue;\n+      }\n+    }\n@@ -1245,0 +1300,1 @@\n+    oop obj = nullptr;\n@@ -1272,0 +1328,4 @@\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* ak = FlatArrayKlass::cast(k);\n+      \/\/ Inline type array must be zeroed because not all memory is reassigned\n+      obj = ak->allocate_instance(sv->field_size(), ak->properties(), THREAD);\n@@ -1278,2 +1338,2 @@\n-    } else if (k->is_objArray_klass()) {\n-      ObjArrayKlass* ak = ObjArrayKlass::cast(k);\n+    } else if (k->is_refArray_klass()) {\n+      RefArrayKlass* ak = RefArrayKlass::cast(k);\n@@ -1281,1 +1341,1 @@\n-      obj = ak->allocate_instance(sv->field_size(), THREAD);\n+      obj = ak->allocate_instance(sv->field_size(), ak->properties(), THREAD);\n@@ -1303,0 +1363,15 @@\n+\/\/ We're deoptimizing at the return of a call, inline type fields are\n+\/\/ in registers. When we go back to the interpreter, it will expect a\n+\/\/ reference to an inline type instance. Allocate and initialize it from\n+\/\/ the register values here.\n+bool Deoptimization::realloc_inline_type_result(InlineKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS) {\n+  oop new_vt = vk->realloc_result(map, return_oops, THREAD);\n+  if (new_vt == nullptr) {\n+    CLEAR_PENDING_EXCEPTION;\n+    THROW_OOP_(Universe::out_of_memory_error_realloc_objects(), true);\n+  }\n+  return_oops.clear();\n+  return_oops.push(Handle(THREAD, new_vt));\n+  return false;\n+}\n+\n@@ -1468,0 +1543,3 @@\n+  InstanceKlass* _klass;\n+  bool _is_flat;\n+  bool _is_null_free;\n@@ -1469,4 +1547,1 @@\n-  ReassignedField() {\n-    _offset = 0;\n-    _type = T_ILLEGAL;\n-  }\n+  ReassignedField() : _offset(0), _type(T_ILLEGAL), _klass(nullptr), _is_flat(false), _is_null_free(false) { }\n@@ -1486,0 +1561,6 @@\n+      if (fs.is_flat()) {\n+        field._is_flat = true;\n+        field._is_null_free = fs.is_null_free_inline_type();\n+        \/\/ Resolve klass of flat inline type field\n+        field._klass = InlineKlass::cast(klass->get_inline_type_field_klass(fs.index()));\n+      }\n@@ -1492,2 +1573,3 @@\n-\/\/ Restore fields of an eliminated instance object employing the same field order used by the compiler.\n-static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool is_jvmci) {\n+\/\/ Restore fields of an eliminated instance object employing the same field order used by the\n+\/\/ compiler when it scalarizes an object at safepoints.\n+static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool is_jvmci, int base_offset, TRAPS) {\n@@ -1496,0 +1578,19 @@\n+    BasicType type = fields->at(i)._type;\n+    int offset = base_offset + fields->at(i)._offset;\n+    \/\/ Check for flat inline type field before accessing the ScopeValue because it might not have any fields\n+    if (fields->at(i)._is_flat) {\n+      \/\/ Recursively re-assign flat inline type fields\n+      InstanceKlass* vk = fields->at(i)._klass;\n+      assert(vk != nullptr, \"must be resolved\");\n+      offset -= InlineKlass::cast(vk)->payload_offset(); \/\/ Adjust offset to omit oop header\n+      svIndex = reassign_fields_by_klass(vk, fr, reg_map, sv, svIndex, obj, is_jvmci, offset, CHECK_0);\n+      if (!fields->at(i)._is_null_free) {\n+        ScopeValue* scope_field = sv->field_at(svIndex);\n+        StackValue* value = StackValue::create_stack_value(fr, reg_map, scope_field);\n+        int nm_offset = offset + InlineKlass::cast(vk)->null_marker_offset();\n+        obj->bool_field_put(nm_offset, value->get_jint() & 1);\n+        svIndex++;\n+      }\n+      continue; \/\/ Continue because we don't need to increment svIndex\n+    }\n+\n@@ -1498,3 +1599,2 @@\n-    int offset = fields->at(i)._offset;\n-    BasicType type = fields->at(i)._type;\n-      case T_OBJECT: case T_ARRAY:\n+      case T_OBJECT:\n+      case T_ARRAY:\n@@ -1572,0 +1672,1 @@\n+\n@@ -1575,0 +1676,14 @@\n+\/\/ restore fields of an eliminated inline type array\n+void Deoptimization::reassign_flat_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, flatArrayOop obj, FlatArrayKlass* vak, bool is_jvmci, TRAPS) {\n+  InlineKlass* vk = vak->element_klass();\n+  assert(vk->maybe_flat_in_array(), \"should only be used for flat inline type arrays\");\n+  \/\/ Adjust offset to omit oop header\n+  int base_offset = arrayOopDesc::base_offset_in_bytes(T_FLAT_ELEMENT) - InlineKlass::cast(vk)->payload_offset();\n+  \/\/ Initialize all elements of the flat inline type array\n+  for (int i = 0; i < sv->field_size(); i++) {\n+    ScopeValue* val = sv->field_at(i);\n+    int offset = base_offset + (i << Klass::layout_helper_log2_element_size(vak->layout_helper()));\n+    reassign_fields_by_klass(vk, fr, reg_map, val->as_ObjectValue(), 0, (oop)obj, is_jvmci, offset, CHECK);\n+  }\n+}\n+\n@@ -1576,1 +1691,1 @@\n-void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool is_jvmci) {\n+void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool is_jvmci, TRAPS) {\n@@ -1581,0 +1696,2 @@\n+    k = get_refined_array_klass(k, fr, reg_map, sv, THREAD);\n+\n@@ -1582,1 +1699,1 @@\n-    assert(obj.not_null() || realloc_failures, \"reallocation was missed\");\n+    assert(obj.not_null() || realloc_failures || sv->has_properties(), \"reallocation was missed\");\n@@ -1620,1 +1737,4 @@\n-      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), is_jvmci);\n+      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), is_jvmci, 0, CHECK);\n+    } else if (k->is_flatArray_klass()) {\n+      FlatArrayKlass* vak = FlatArrayKlass::cast(k);\n+      reassign_flat_array_elements(fr, reg_map, sv, (flatArrayOop) obj(), vak, is_jvmci, CHECK);\n@@ -1624,1 +1744,1 @@\n-    } else if (k->is_objArray_klass()) {\n+    } else if (k->is_refArray_klass()) {\n@@ -1816,1 +1936,1 @@\n-  \/\/ Deoptimize only if the frame comes from compile code.\n+  \/\/ Deoptimize only if the frame comes from compiled code.\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":155,"deletions":35,"binary":false,"changes":190,"status":"modified"},{"patch":"@@ -819,1 +819,1 @@\n-  develop(bool, PrintFieldLayout, false,                                    \\\n+  product(bool, PrintFieldLayout, false, DIAGNOSTIC,                        \\\n@@ -822,0 +822,27 @@\n+  product(bool, PrintInlineLayout, false, DIAGNOSTIC,                       \\\n+          \"Print field layout for each inline type or class with inline fields\") \\\n+                                                                            \\\n+  product(bool, PrintFlatArrayLayout, false, DIAGNOSTIC,                    \\\n+          \"Print array layout for each inline type array\")                  \\\n+                                                                            \\\n+  product(bool, UseArrayFlattening, true,                                   \\\n+          \"Allow the VM to flatten arrays\")                                 \\\n+                                                                            \\\n+  product(bool, UseFieldFlattening, true,                                   \\\n+          \"Allow the VM to flatten value fields\")                           \\\n+                                                                            \\\n+  product(bool, UseNonAtomicValueFlattening, true,                          \\\n+          \"Allow the JVM to flatten some non-atomic null-free values\")      \\\n+                                                                            \\\n+  product(bool, UseNullableValueFlattening, true,                           \\\n+          \"Allow the JVM to flatten some nullable values\")                  \\\n+                                                                            \\\n+  product(bool, UseAtomicValueFlattening, true,                             \\\n+          \"Allow the JVM to flatten some atomic values\")                    \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxOops, 4,                                 \\\n+          \"Max nof embedded object references in an inline type to flatten, <0 no limit\")  \\\n+                                                                            \\\n+  develop(ccstrlist, PrintInlineKlassFields, \"\",                            \\\n+          \"Print fields collected by InlineKlass::collect_fields\")          \\\n+                                                                            \\\n@@ -1784,0 +1811,3 @@\n+  product(bool, IgnoreAssertUnsetFields, false, DIAGNOSTIC,                           \\\n+          \"Ignore assert_unset_fields\")                                     \\\n+                                                                            \\\n@@ -1955,0 +1985,20 @@\n+  product(bool, EnableValhalla, true,                                       \\\n+          \"Enable experimental Valhalla features\")                          \\\n+                                                                            \\\n+  product_pd(bool, InlineTypePassFieldsAsArgs,                              \\\n+          \"Pass each inline type field as an argument at calls\")            \\\n+                                                                            \\\n+  product_pd(bool, InlineTypeReturnedAsFields,                              \\\n+          \"Return fields instead of an inline type reference\")              \\\n+                                                                            \\\n+  develop(bool, StressCallingConvention, false,                             \\\n+          \"Stress the scalarized calling convention.\")                      \\\n+                                                                            \\\n+  develop(bool, PreloadClasses, true,                                       \\\n+          \"Preloading all classes from the LoadableDescriptors attribute\")  \\\n+                                                                            \\\n+  product(ccstrlist, ForceNonTearable, \"\", DIAGNOSTIC,                      \\\n+          \"List of inline classes which are forced to be atomic \"           \\\n+          \"(whitespace and commas separate names, \"                         \\\n+          \"and leading and trailing stars '*' are wildcards)\")              \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":51,"deletions":1,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -151,0 +151,1 @@\n+  oop           _return_buffered_value; \/\/ buffered value being returned\n@@ -793,0 +794,3 @@\n+  oop return_buffered_value() const              { return _return_buffered_value; }\n+  void set_return_buffered_value(oop val)        { _return_buffered_value = val; }\n+\n@@ -859,0 +863,1 @@\n+  static ByteSize return_buffered_value_offset() { return byte_offset_of(JavaThread, _return_buffered_value); }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -318,0 +318,16 @@\n+\/\/ These checks are required for wait, notify and exit to avoid inflating the monitor to\n+\/\/ find out this inline type object cannot be locked.\n+#define CHECK_THROW_NOSYNC_IMSE(obj)  \\\n+  if (EnableValhalla && (obj)->mark().is_inline_type()) {  \\\n+    JavaThread* THREAD = current;           \\\n+    ResourceMark rm(THREAD);                \\\n+    THROW_MSG(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \\\n+  }\n+\n+#define CHECK_THROW_NOSYNC_IMSE_0(obj)  \\\n+  if (EnableValhalla && (obj)->mark().is_inline_type()) {  \\\n+    JavaThread* THREAD = current;             \\\n+    ResourceMark rm(THREAD);                  \\\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \\\n+  }\n+\n@@ -344,0 +360,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -401,0 +418,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -516,0 +534,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"JITed code should never have locked an instance of a value class\");\n@@ -538,0 +557,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"This method should never be called on an instance of an inline class\");\n@@ -557,0 +577,1 @@\n+  guarantee(!EnableValhalla || !obj->klass()->is_inline_klass(), \"Attempt to inflate inline type\");\n@@ -604,0 +625,3 @@\n+    if (EnableValhalla && mark.is_inline_type()) {\n+      return;\n+    }\n@@ -660,0 +684,1 @@\n+  JavaThread* THREAD = current;\n@@ -668,0 +693,10 @@\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+    ResourceMark rm(THREAD);\n+    const char* desc = \"Cannot synchronize on an instance of value class \";\n+    const char* className = obj->klass()->external_name();\n+    size_t msglen = strlen(desc) + strlen(className) + 1;\n+    char* message = NEW_RESOURCE_ARRAY(char, msglen);\n+    assert(message != nullptr, \"NEW_RESOURCE_ARRAY should have called vm_exit_out_of_memory and not return nullptr\");\n+    THROW_MSG(vmSymbols::java_lang_IdentityException(), className);\n+  }\n+\n@@ -695,0 +730,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -739,0 +775,1 @@\n+  CHECK_THROW_NOSYNC_IMSE_0(obj);\n@@ -781,0 +818,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -809,0 +847,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -990,0 +1029,4 @@\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+    \/\/ VM should be calling bootstrap method\n+    ShouldNotReachHere();\n+  }\n@@ -1116,0 +1159,3 @@\n+  if (EnableValhalla && h_obj->mark().is_inline_type()) {\n+    return false;\n+  }\n@@ -1459,0 +1505,3 @@\n+  if (EnableValhalla) {\n+    guarantee(!object->klass()->is_inline_klass(), \"Attempt to inflate inline type\");\n+  }\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":49,"deletions":0,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -63,0 +63,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -941,1 +943,3 @@\n-           declare_type(ObjArrayKlass, ArrayKlass)                        \\\n+           declare_type(ObjArrayKlass, ArrayKlass)                        \\\n+             declare_type(FlatArrayKlass, ArrayKlass)                     \\\n+             declare_type(RefArrayKlass, ArrayKlass)                      \\\n@@ -944,0 +948,1 @@\n+        declare_type(InlineKlass, InstanceKlass)                          \\\n@@ -1409,1 +1414,1 @@\n-  declare_constant(Klass::_lh_array_tag_obj_value)                        \\\n+  declare_constant(Klass::_lh_array_tag_ref_value)                        \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -257,0 +257,2 @@\n+#define THREAD_AND_LOCATION_DECL                 TRAPS, const char* file, int line\n+#define THREAD_AND_LOCATION_ARGS                 THREAD, file, line\n","filename":"src\/hotspot\/share\/utilities\/exceptions.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -213,2 +213,2 @@\n-  JVM_SIGNATURE_VOID,    0,\n-  0, 0, 0, 0\n+  JVM_SIGNATURE_VOID,\n+  0, 0, 0, 0, 0, 0\n@@ -231,0 +231,1 @@\n+  \"flat element\",\n@@ -260,1 +261,1 @@\n-int type2size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, -1};\n+int type2size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, -1, 1, 1, 1, 1, -1};\n@@ -278,5 +279,6 @@\n-  T_ADDRESS,               \/\/ T_ADDRESS  = 15,\n-  T_NARROWOOP,             \/\/ T_NARROWOOP= 16,\n-  T_METADATA,              \/\/ T_METADATA = 17,\n-  T_NARROWKLASS,           \/\/ T_NARROWKLASS = 18,\n-  T_CONFLICT               \/\/ T_CONFLICT = 19,\n+  (BasicType)0,            \/\/ T_FLAT_ELEMENT = 15,\n+  T_ADDRESS,               \/\/ T_ADDRESS  = 16,\n+  T_NARROWOOP,             \/\/ T_NARROWOOP= 17,\n+  T_METADATA,              \/\/ T_METADATA = 18,\n+  T_NARROWKLASS,           \/\/ T_NARROWKLASS = 19,\n+  T_CONFLICT               \/\/ T_CONFLICT = 20\n@@ -302,5 +304,6 @@\n-  T_ADDRESS, \/\/ T_ADDRESS  = 15,\n-  T_NARROWOOP, \/\/ T_NARROWOOP  = 16,\n-  T_METADATA,  \/\/ T_METADATA   = 17,\n-  T_NARROWKLASS, \/\/ T_NARROWKLASS  = 18,\n-  T_CONFLICT \/\/ T_CONFLICT = 19,\n+  (BasicType)0,  \/\/ T_FLAT_ELEMENT = 15,\n+  T_ADDRESS, \/\/ T_ADDRESS  = 16,\n+  T_NARROWOOP, \/\/ T_NARROWOOP  = 17,\n+  T_METADATA,  \/\/ T_METADATA   = 18,\n+  T_NARROWKLASS, \/\/ T_NARROWKLASS  = 19,\n+  T_CONFLICT \/\/ T_CONFLICT = 20\n@@ -326,5 +329,6 @@\n-  T_OBJECT_aelem_bytes,      \/\/ T_ADDRESS  = 15,\n-  T_NARROWOOP_aelem_bytes,   \/\/ T_NARROWOOP= 16,\n-  T_OBJECT_aelem_bytes,      \/\/ T_METADATA = 17,\n-  T_NARROWKLASS_aelem_bytes, \/\/ T_NARROWKLASS= 18,\n-  0                          \/\/ T_CONFLICT = 19,\n+  0,                         \/\/ T_FLAT_ELEMENT = 15,\n+  T_OBJECT_aelem_bytes,      \/\/ T_ADDRESS  = 16,\n+  T_NARROWOOP_aelem_bytes,   \/\/ T_NARROWOOP= 17,\n+  T_OBJECT_aelem_bytes,      \/\/ T_METADATA = 18,\n+  T_NARROWKLASS_aelem_bytes, \/\/ T_NARROWKLASS= 19,\n+  0                          \/\/ T_CONFLICT = 20\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.cpp","additions":22,"deletions":18,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -601,0 +601,9 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Prototyping\n+\/\/ \"Code Missing Here\" macro, un-define when integrating back from prototyping stage and break\n+\/\/ compilation on purpose (i.e. \"forget me not\")\n+#define PROTOTYPE\n+#ifdef PROTOTYPE\n+#define CMH(m)\n+#endif\n+\n@@ -687,5 +696,6 @@\n-  T_ADDRESS     = 15,\n-  T_NARROWOOP   = 16,\n-  T_METADATA    = 17,\n-  T_NARROWKLASS = 18,\n-  T_CONFLICT    = 19, \/\/ for stack value type with conflicting contents\n+  T_FLAT_ELEMENT = 15, \/\/ Not a true BasicType, only used in layout helpers of flat arrays\n+  T_ADDRESS     = 16,\n+  T_NARROWOOP   = 17,\n+  T_METADATA    = 18,\n+  T_NARROWKLASS = 19,\n+  T_CONFLICT    = 20, \/\/ for stack value type with conflicting contents\n@@ -735,0 +745,1 @@\n+  assert(t != T_FLAT_ELEMENT, \"\");  \/\/ Strong assert to detect misuses of T_FLAT_ELEMENT\n@@ -801,1 +812,2 @@\n-  T_VOID_size        = 0\n+  T_VOID_size        = 0,\n+  T_FLAT_ELEMENT_size = 0\n@@ -837,1 +849,2 @@\n-  T_VOID_aelem_bytes        = 0\n+  T_VOID_aelem_bytes        = 0,\n+  T_FLAT_ELEMENT_aelem_bytes = 0\n@@ -927,1 +940,1 @@\n-  vtos = 9,             \/\/ tos not cached\n+  vtos = 9,             \/\/ tos not cached,\n@@ -944,1 +957,1 @@\n-    case T_ARRAY  : \/\/ fall through\n+    case T_ARRAY  :   \/\/ fall through\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":22,"deletions":9,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+import java.util.Objects;\n@@ -668,2 +669,2 @@\n-                    (k == key || k.equals(key)) &&\n-                    (v == (u = val) || v.equals(u)));\n+                    (Objects.equals(k, key)) &&\n+                    v.equals(val));\n@@ -681,1 +682,1 @@\n-                        ((ek = e.key) == k || (ek != null && k.equals(ek))))\n+                        (ek = e.key) != null && Objects.equals(k, ek))\n@@ -952,1 +953,1 @@\n-                if ((ek = e.key) == key || (ek != null && key.equals(ek)))\n+                if ((ek = e.key) != null && Objects.equals(key, ek))\n@@ -959,1 +960,1 @@\n-                    ((ek = e.key) == key || (ek != null && key.equals(ek))))\n+                    ((ek = e.key) != null && Objects.equals(key, ek)))\n@@ -997,1 +998,1 @@\n-                if ((v = p.val) == value || (v != null && value.equals(v)))\n+                if ((v = p.val) != null && Objects.equals(value, v))\n@@ -1038,1 +1039,1 @@\n-                     && ((fk = f.key) == key || (fk != null && key.equals(fk)))\n+                     && (fk = f.key) != null && Objects.equals(key, fk)\n@@ -1050,2 +1051,1 @@\n-                                    ((ek = e.key) == key ||\n-                                     (ek != null && key.equals(ek)))) {\n+                                    (ek = e.key) != null && Objects.equals(key, ek)) {\n@@ -1143,2 +1143,1 @@\n-                                    ((ek = e.key) == key ||\n-                                     (ek != null && key.equals(ek)))) {\n+                                    ((ek = e.key) != null && Objects.equals(key, ek))) {\n@@ -1146,2 +1145,2 @@\n-                                    if (cv == null || cv == ev ||\n-                                        (ev != null && cv.equals(ev))) {\n+                                    if (cv == null ||\n+                                        (ev != null && Objects.equals(cv, ev))) {\n@@ -1170,2 +1169,2 @@\n-                                if (cv == null || cv == pv ||\n-                                    (pv != null && cv.equals(pv))) {\n+                                if (cv == null ||\n+                                    (pv != null && Objects.equals(cv, pv))) {\n@@ -1375,1 +1374,1 @@\n-                if (v == null || (v != val && !v.equals(val)))\n+                if (!Objects.equals(val, v))\n@@ -1383,1 +1382,1 @@\n-                    (mv != v && !mv.equals(v)))\n+                    !Objects.equals(mv, v))\n@@ -1507,2 +1506,1 @@\n-                                ((qk = q.key) == k ||\n-                                 (qk != null && k.equals(qk)))) {\n+                                ((qk = q.key) != null && Objects.equals(k, qk))) {\n@@ -1737,1 +1735,1 @@\n-                     && ((fk = f.key) == key || (fk != null && key.equals(fk)))\n+                     && ((fk = f.key) != null && Objects.equals(key, fk))\n@@ -1842,2 +1840,1 @@\n-                                    ((ek = e.key) == key ||\n-                                     (ek != null && key.equals(ek)))) {\n+                                    ((ek = e.key) != null && Objects.equals(key, ek))) {\n@@ -1954,2 +1951,1 @@\n-                                    ((ek = e.key) == key ||\n-                                     (ek != null && key.equals(ek)))) {\n+                                    ((ek = e.key) != null && Objects.equals(key, ek))) {\n@@ -2070,2 +2066,1 @@\n-                                    ((ek = e.key) == key ||\n-                                     (ek != null && key.equals(ek)))) {\n+                                    ((ek = e.key) != null && Objects.equals(key, ek))) {\n@@ -2264,1 +2259,1 @@\n-                        ((ek = e.key) == k || (ek != null && k.equals(ek))))\n+                        ((ek = e.key) != null && Objects.equals(k, ek)))\n@@ -2759,1 +2754,1 @@\n-                    else if ((pk = p.key) == k || (pk != null && k.equals(pk)))\n+                    else if ((pk = p.key) != null && Objects.equals(k, pk))\n@@ -2910,1 +2905,1 @@\n-                            ((ek = e.key) == k || (ek != null && k.equals(ek))))\n+                            ((ek = e.key) != null && Objects.equals(k, ek)))\n@@ -2950,1 +2945,1 @@\n-                else if ((pk = p.key) == k || (pk != null && k.equals(pk)))\n+                else if ((pk = p.key) != null && Objects.equals(k, pk))\n@@ -3549,2 +3544,2 @@\n-                    (k == key || k.equals(key)) &&\n-                    (v == val || v.equals(val)));\n+                    Objects.equals(k, key) &&\n+                    Objects.equals(v, val));\n","filename":"src\/java.base\/share\/classes\/java\/util\/concurrent\/ConcurrentHashMap.java","additions":27,"deletions":32,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -296,0 +296,5 @@\n+        \/**\n+         * Warn about issues related to migration of JDK classes.\n+         *\/\n+        MIGRATION(\"migration\"),\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Lint.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -100,0 +100,2 @@\n+    private final UnsetFieldsInfo unsetFieldsInfo;\n+\n@@ -130,0 +132,1 @@\n+        unsetFieldsInfo = UnsetFieldsInfo.instance(context);\n@@ -135,0 +138,5 @@\n+        generateEarlyLarvalFrame = options.isSet(\"generateEarlyLarvalFrame\");\n+        Preview preview = Preview.instance(context);\n+        Source source = Source.instance(context);\n+        allowValueClasses = (!preview.isPreview(Source.Feature.VALUE_CLASSES) || preview.isEnabled()) &&\n+                Source.Feature.VALUE_CLASSES.allowedInSource(source);\n@@ -144,0 +152,2 @@\n+    private boolean generateEarlyLarvalFrame;\n+    private final boolean allowValueClasses;\n@@ -427,0 +437,2 @@\n+        \/\/ only used for value classes\n+        ListBuffer<JCStatement> initBlocks = new ListBuffer<>();\n@@ -442,2 +454,7 @@\n-                else if ((block.flags & SYNTHETIC) == 0)\n-                    initCode.append(block);\n+                else if ((block.flags & SYNTHETIC) == 0) {\n+                    if (c.isValueClass() || c.hasStrict()) {\n+                        initBlocks.append(block);\n+                    } else {\n+                        initCode.append(block);\n+                    }\n+                }\n@@ -482,2 +499,1 @@\n-        if (initCode.length() != 0) {\n-            List<JCStatement> inits = initCode.toList();\n+        if (initCode.length() != 0 || initBlocks.length() != 0) {\n@@ -487,1 +503,1 @@\n-                normalizeMethod((JCMethodDecl)t, inits, initTAlist);\n+                normalizeMethod((JCMethodDecl)t, initCode.toList(), initBlocks.toList(), initTAlist);\n@@ -550,1 +566,1 @@\n-    void normalizeMethod(JCMethodDecl md, List<JCStatement> initCode, List<TypeCompound> initTAs) {\n+    void normalizeMethod(JCMethodDecl md, List<JCStatement> initCode, List<JCStatement> initBlocks,  List<TypeCompound> initTAs) {\n@@ -554,1 +570,7 @@\n-            TreeInfo.mapSuperCalls(md.body, supercall -> make.Block(0, initCode.prepend(supercall)));\n+            if (allowValueClasses & (md.sym.owner.isValueClass() || md.sym.owner.hasStrict() || ((md.sym.owner.flags_field & RECORD) != 0))) {\n+                rewriteInitializersIfNeeded(md, initCode);\n+                md.body.stats = initCode.appendList(md.body.stats);\n+                TreeInfo.mapSuperCalls(md.body, supercall -> make.Block(0, initBlocks.prepend(supercall)));\n+            } else {\n+                TreeInfo.mapSuperCalls(md.body, supercall -> make.Block(0, initCode.prepend(supercall)));\n+            }\n@@ -563,0 +585,34 @@\n+    void rewriteInitializersIfNeeded(JCMethodDecl md, List<JCStatement> initCode) {\n+        if (lower.initializerOuterThis.containsKey(md.sym.owner)) {\n+            InitializerVisitor initializerVisitor = new InitializerVisitor(md, lower.initializerOuterThis.get(md.sym.owner));\n+            for (JCStatement init : initCode) {\n+                initializerVisitor.scan(init);\n+            }\n+        }\n+    }\n+\n+    public static class InitializerVisitor extends TreeScanner {\n+        JCMethodDecl md;\n+        Set<JCExpression> exprSet;\n+\n+        public InitializerVisitor(JCMethodDecl md, Set<JCExpression> exprSet) {\n+            this.md = md;\n+            this.exprSet = exprSet;\n+        }\n+\n+        @Override\n+        public void visitTree(JCTree tree) {}\n+\n+        @Override\n+        public void visitIdent(JCIdent tree) {\n+            if (exprSet.contains(tree)) {\n+                for (JCVariableDecl param: md.params) {\n+                    if (param.name == tree.name &&\n+                            ((param.sym.flags_field & (MANDATED | NOOUTERTHIS)) == (MANDATED | NOOUTERTHIS))) {\n+                        tree.sym = param.sym;\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n@@ -959,0 +1015,5 @@\n+                Set<VarSymbol> prevUnsetFields = code.currentUnsetFields;\n+                if (meth.isConstructor()) {\n+                    code.currentUnsetFields = unsetFieldsInfo.getUnsetFields(env.enclClass.sym, tree.body);\n+                    code.initialUnsetFields = unsetFieldsInfo.getUnsetFields(env.enclClass.sym, tree.body);\n+                }\n@@ -966,0 +1027,2 @@\n+                } finally {\n+                    code.currentUnsetFields = prevUnsetFields;\n@@ -1034,1 +1097,2 @@\n-                                        poolWriter);\n+                                        poolWriter,\n+                                        generateEarlyLarvalFrame);\n@@ -1170,0 +1234,13 @@\n+            Set<VarSymbol> prevCodeUnsetFields = code.currentUnsetFields;\n+            try {\n+                genLoopHelper(loop, body, cond, step, testFirst);\n+            } finally {\n+                code.currentUnsetFields = prevCodeUnsetFields;\n+            }\n+        }\n+\n+        private void genLoopHelper(JCStatement loop,\n+                             JCStatement body,\n+                             JCExpression cond,\n+                             List<JCExpressionStatement> step,\n+                             boolean testFirst) {\n@@ -1228,0 +1305,1 @@\n+        Set<VarSymbol> prevCodeUnsetFields = code.currentUnsetFields;\n@@ -1233,0 +1311,1 @@\n+            code.currentUnsetFields = prevCodeUnsetFields;\n@@ -1308,0 +1387,10 @@\n+        Set<VarSymbol> prevCodeUnsetFields = code.currentUnsetFields;\n+        try {\n+            handleSwitchHelper(swtch, selector, cases, patternSwitch);\n+        } finally {\n+            code.currentUnsetFields = prevCodeUnsetFields;\n+        }\n+    }\n+\n+    void handleSwitchHelper(JCTree swtch, JCExpression selector, List<JCCase> cases,\n+                      boolean patternSwitch) {\n@@ -1319,1 +1408,1 @@\n-                             CRT_FLOW_CONTROLLER, startpcCrt, code.curCP());\n+                        CRT_FLOW_CONTROLLER, startpcCrt, code.curCP());\n@@ -1325,1 +1414,1 @@\n-                             CRT_FLOW_CONTROLLER, startpcCrt, code.curCP());\n+                        CRT_FLOW_CONTROLLER, startpcCrt, code.curCP());\n@@ -1361,5 +1450,5 @@\n-                nlabels > 0 &&\n-                table_space_cost + 3 * table_time_cost <=\n-                lookup_space_cost + 3 * lookup_time_cost\n-                ?\n-                tableswitch : lookupswitch;\n+                    nlabels > 0 &&\n+                            table_space_cost + 3 * table_time_cost <=\n+                                    lookup_space_cost + 3 * lookup_time_cost\n+                            ?\n+                            tableswitch : lookupswitch;\n@@ -1401,2 +1490,2 @@\n-                            tableBase + 4 * (labels[i] - lo + 3),\n-                            pc - startpc);\n+                                tableBase + 4 * (labels[i] - lo + 3),\n+                                pc - startpc);\n@@ -1452,2 +1541,2 @@\n-                 \/\/ Emit line position for the end of a switch expression\n-                 code.statBegin(TreeInfo.endPos(swtch));\n+                \/\/ Emit line position for the end of a switch expression\n+                code.statBegin(TreeInfo.endPos(swtch));\n@@ -1556,0 +1645,9 @@\n+            Set<VarSymbol> prevCodeUnsetFields = code.currentUnsetFields;\n+            try {\n+                genTryHelper(body, catchers, env);\n+            } finally {\n+                code.currentUnsetFields = prevCodeUnsetFields;\n+            }\n+        }\n+\n+        void genTryHelper(JCTree body, List<JCCatch> catchers, Env<GenContext> env) {\n@@ -1575,2 +1673,2 @@\n-                env.info.finalize != null &&\n-                env.info.finalize.hasFinalizer();\n+                    env.info.finalize != null &&\n+                            env.info.finalize.hasFinalizer();\n@@ -1585,1 +1683,1 @@\n-                                                 code.branch(goto_));\n+                            code.branch(goto_));\n@@ -1608,1 +1706,1 @@\n-                                  catchallpc, 0);\n+                            catchallpc, 0);\n@@ -1623,2 +1721,2 @@\n-                              env.info.gaps.next().intValue(),\n-                              catchallpc, 0);\n+                        env.info.gaps.next().intValue(),\n+                        catchallpc, 0);\n@@ -1767,0 +1865,9 @@\n+        Set<VarSymbol> prevCodeUnsetFields = code.currentUnsetFields;\n+        try {\n+            visitIfHelper(tree);\n+        } finally {\n+            code.currentUnsetFields = prevCodeUnsetFields;\n+        }\n+    }\n+\n+    public void visitIfHelper(JCIf tree) {\n@@ -1771,1 +1878,1 @@\n-                             CRT_FLOW_CONTROLLER);\n+                CRT_FLOW_CONTROLLER);\n@@ -2095,0 +2202,2 @@\n+        Set<VarSymbol> tmpUnsetSymbols = unsetFieldsInfo.getUnsetFields(env.enclClass.sym, tree);\n+        code.currentUnsetFields = tmpUnsetSymbols != null ? tmpUnsetSymbols : code.currentUnsetFields;\n@@ -2373,1 +2482,1 @@\n-       }\n+        }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/Gen.java","additions":136,"deletions":27,"binary":false,"changes":163,"status":"modified"},{"patch":"@@ -87,0 +87,9 @@\n+# Valhalla\n+compiler\/codegen\/TestRedundantLea.java#StringInflate  8367518 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StoreNParallel 8367518 generic-all\n+compiler\/codegen\/TestRedundantLea.java#StoreNSerial   8367518 generic-all\n+compiler\/gcbarriers\/TestImplicitNullChecks.java 8367338 generic-all\n+compiler\/regalloc\/TestVerifyRegisterAllocator.java 8365895 windows-x64\n+compiler\/types\/TestArrayManyDimensions.java 8365895 windows-x64\n+compiler\/types\/correctness\/OffTest.java 8365895 windows-x64\n+\n@@ -106,0 +115,1 @@\n+runtime\/cds\/appcds\/redefineClass\/RedefineRunningMethods_Shared.java  8304168 generic-all\n@@ -125,0 +135,49 @@\n+\n+# Valhalla\n+runtime\/AccModule\/ConstModule.java 8294051 generic-all\n+runtime\/valhalla\/inlinetypes\/CircularityTest.java 8349037 generic-all\n+runtime\/valhalla\/inlinetypes\/classloading\/ConcurrentClassLoadingTest.java 8367412 linux-aarch64\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictInstanceFieldsTest.java CODETOOLS-7904031 generic-all\n+runtime\/valhalla\/inlinetypes\/verifier\/StrictStaticFieldsTest.java CODETOOLS-7904031 generic-all\n+\n+runtime\/cds\/TestDefaultArchiveLoading.java#coops_nocoh            8366774           generic-all\n+runtime\/cds\/TestDefaultArchiveLoading.java#nocoops_nocoh          8366774           generic-all\n+runtime\/cds\/appcds\/jcmd\/JCmdTestDynamicDump.java                  8367398           windows-x64\n+\n+# Valhalla + COH\n+compiler\/c2\/autovectorization\/TestIndexOverflowIR.java                          8348568 generic-all\n+compiler\/c2\/irTests\/stringopts\/TestArrayCopySelect.java                         8348568 generic-all\n+compiler\/c2\/irTests\/TestVectorConditionalMove.java                              8348568 generic-all\n+compiler\/c2\/irTests\/TestVectorizationMismatchedAccess.java                      8348568 generic-all\n+compiler\/c2\/irTests\/TestVectorizationNotRun.java                                8348568 generic-all\n+compiler\/c2\/TestCastX2NotProcessedIGVN.java                                     8348568 generic-all\n+compiler\/loopopts\/superword\/TestAlignVector.java                                8348568 generic-all\n+compiler\/loopopts\/superword\/TestAlignVector.java#NoAlignVector-COH              8348568 generic-all\n+compiler\/loopopts\/superword\/TestAlignVector.java#VerifyAlignVector-COH          8348568 generic-all\n+compiler\/loopopts\/superword\/TestIndependentPacksWithCyclicDependency.java       8348568 generic-all\n+compiler\/loopopts\/superword\/TestMulAddS2I.java                                  8348568 generic-all\n+compiler\/loopopts\/superword\/TestScheduleReordersScalarMemops.java               8348568 generic-all\n+compiler\/loopopts\/superword\/TestSplitPacks.java                                 8348568 generic-all\n+compiler\/loopopts\/superword\/TestUnorderedReductionPartialVectorization.java     8348568 generic-all\n+compiler\/vectorization\/TestFloatConversionsVector.java                          8348568 generic-all\n+compiler\/vectorization\/runner\/ArrayTypeConvertTest.java                         8348568 generic-all\n+compiler\/vectorization\/runner\/LoopCombinedOpTest.java                           8348568 generic-all\n+compiler\/vectorization\/runner\/VectorizationTestRunner.java                      8348568 generic-all\n+runtime\/FieldLayout\/TestOopMapSizeMinimal.java#no_coops_ccptr_coh               8348568 generic-all\n+\n+gc\/stress\/gcbasher\/TestGCBasherWithParallel.java                                8348568 generic-all\n+\n+gtest\/CompressedKlassGtest.java#use-zero-based-encoding-coh                     8348568 generic-all\n+gtest\/CompressedKlassGtest.java#use-zero-based-encoding-coh-large-class-space   8348568 generic-all\n+gtest\/MetaspaceGtests.java#UseCompactObjectHeaders                              8348568 generic-all\n+\n+runtime\/CompressedOops\/CompressedClassPointersEncodingScheme.java               8348568 generic-all\n+runtime\/FieldLayout\/BaseOffsets.java#no-coops-with-coh                          8348568 generic-all\n+runtime\/FieldLayout\/BaseOffsets.java#with-coop--with-coh                        8348568 generic-all\n+runtime\/cds\/TestDefaultArchiveLoading.java#coops_coh                            8348568 generic-all\n+runtime\/cds\/TestDefaultArchiveLoading.java#nocoops_coh                          8348568 generic-all\n+runtime\/cds\/appcds\/TestZGCWithCDS.java                                          8348568 generic-all\n+\n+# Valhalla + AOT\n+runtime\/cds\/appcds\/methodHandles\/MethodHandlesGeneralTest.java#aot              8367408 generic-all\n+\n@@ -152,0 +211,79 @@\n+# Valhalla TODO:\n+serviceability\/jvmti\/valhalla\/HeapDump\/HeapDump.java 8317416 generic-all\n+\n+serviceability\/sa\/ClhsdbCDSCore.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbFindPC.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbInspect.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJdis.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbJstack.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSource.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbSymbol.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbWhere.java 8190936 generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java 8190936 generic-all\n+serviceability\/sa\/TestClassDump.java 8190936 generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java 8190936 generic-all\n+serviceability\/sa\/TestHeapDumpForLargeArray.java 8190936 generic-all\n+serviceability\/sa\/TestIntConstant.java 8190936 generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCore.java 8190936 generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java 8190936 generic-all\n+serviceability\/sa\/TestPrintMdo.java 8190936 generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java 8190936 generic-all\n+serviceability\/sa\/ClhsdbDumpclass.java 8190936 generic-all\n+\n+compiler\/stringopts\/TestStackedConcatsAppendUncommonTrap.java 8367405 generic-all\n+\n+# Array Changes TODO\n+serviceability\/sa\/CDSJMapClstats.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbClasses.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DisableRegistryTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbDumpheap.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbTestConnectArgument.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/DebugdConnectTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJhisto.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id1 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackWithConcurrentLock.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstackXcompStress.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#process 8365722 generic-all\n+serviceability\/sa\/ClhsdbPstack.java#core 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id0 8365722 generic-all\n+serviceability\/sa\/ClhsdbScanOops.java#id1 8365722 generic-all\n+serviceability\/sa\/DeadlockDetectionTest.java 8365722 generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id0 8365722 generic-all\n+serviceability\/sa\/TestInstanceKlassSize.java 8365722 generic-all\n+serviceability\/sa\/TestSysProps.java 8365722 generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbAttachToDebugServer.java 8365722 generic-all\n+resourcehogs\/serviceability\/sa\/TestHeapDumpForLargeArray.java 8365722 generic-all\n+serviceability\/HeapDump\/DuplicateArrayClassesTest.java 8365722 generic-all\n+\n+resourcehogs\/serviceability\/sa\/ClhsdbRegionDetailsScanOopsForG1.java 8366806 generic-all\n+serviceability\/jvmti\/valhalla\/GetSetLocal\/ValueGetSetLocal.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/Accessible\/isPackagePrivate\/accipp001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/Accessible\/isPrivate\/isPrivate001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/Accessible\/isProtected\/isProtected001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/Accessible\/isPublic\/isPublic001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ClassObjectReference\/reflectedType\/reflectype001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ClassObjectReference\/toString\/tostring001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/classObject\/classobj001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/equals\/equals001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/genericSignature\/genericSignature001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/genericSignature\/genericSignature002\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/hashCode\/hashcode001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/isFinal\/isfinal001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/isStatic\/isstatic001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/isStatic\/isstatic002\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/name\/name001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/nestedTypes\/nestedtypes002\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/ReferenceType\/sourceName\/sourcename003\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/VirtualMachineManager\/connectedVirtualMachines\/convm001\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/VirtualMachineManager\/connectedVirtualMachines\/convm002\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jdi\/VirtualMachineManager\/connectedVirtualMachines\/convm003\/TestDescription.java 8366806 generic-all\n+vmTestbase\/nsk\/jvmti\/scenarios\/events\/EM04\/em04t001\/TestDescription.java 8366806 generic-all\n+\n@@ -190,0 +328,2 @@\n+vmTestbase\/vm\/mlvm\/hiddenloader\/stress\/byteMutation\/Test.java 8317172 generic-all\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":140,"deletions":0,"binary":false,"changes":140,"status":"modified"},{"patch":"@@ -106,1 +106,1 @@\n-requiredVersion=7.5.2+1\n+requiredVersion=7.5.1+1\n","filename":"test\/hotspot\/jtreg\/TEST.ROOT","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-  runtime\n+  runtime \\\n@@ -65,0 +65,8 @@\n+hotspot_valhalla = \\\n+  runtime\/valhalla \\\n+  compiler\/valhalla \\\n+  serviceability\/jvmti\/valhalla\n+\n+hotspot_valhalla_runtime = \\\n+  runtime\/valhalla\n+\n@@ -213,0 +221,1 @@\n+  compiler\/valhalla\/ \\\n@@ -254,0 +263,7 @@\n+\n+tier1_compiler_no_valhalla = \\\n+  :tier1_compiler_1 \\\n+  :tier1_compiler_2 \\\n+  :tier1_compiler_3 \\\n+  -compiler\/valhalla\n+\n@@ -406,0 +422,4 @@\n+tier1_runtime_no_valhalla = \\\n+  :tier1_runtime \\\n+  -runtime\/valhalla\n+\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":21,"deletions":1,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -74,2 +74,2 @@\n-                                             TestCommon.list(mainClass),\n-                                             unlockArg, logArg, nmtArg);\n+                TestCommon.list(mainClass),\n+                unlockArg, logArg, nmtArg);\n@@ -79,1 +79,1 @@\n-            .assertNormalExit(output -> {\n+                .assertNormalExit(output -> {\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/ArchiveRelocationTest.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -538,0 +538,5 @@\n+java\/lang\/ModuleLayer\/LayerControllerTest.java                  8337048 generic-all\n+java\/lang\/ModuleLayer\/BasicLayerTest.java                       8337048 generic-all\n+\n+java\/lang\/Thread\/virtual\/stress\/Skynet.java#default             8342977 generic-all\n+\n@@ -555,0 +560,1 @@\n+com\/sun\/management\/HotSpotDiagnosticMXBean\/DumpThreadsWithEliminatedLock.java 8360599 generic-all\n@@ -580,0 +586,2 @@\n+java\/net\/CookieHandler\/B6644726.java                            8365811 generic-all\n+\n@@ -735,0 +743,4 @@\n+com\/sun\/jdi\/cds\/CDSBreakpointTest.java                          8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSDeleteAllBkptsTest.java                      8304168 generic-all\n+com\/sun\/jdi\/cds\/CDSFieldWatchpoints.java                        8304168 generic-all\n+\n@@ -838,0 +850,1 @@\n+\n@@ -845,0 +858,19 @@\n+############################################################################\n+\n+# valhalla\n+jdk\/classfile\/AccessFlagsTest.java 8366270 generic-all\n+\n+java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessValue.java 8367346 generic-all\n+\n+jdk\/jfr\/event\/runtime\/TestSyncOnValueBasedClassEvent.java 8328777 generic-all\n+\n+jdk\/jfr\/event\/runtime\/TestClassLoaderStatsEvent.java 8366820 generic-all\n+\n+com\/sun\/jdi\/valhalla\/FieldWatchpointsTest.java 8366806 generic-all\n+com\/sun\/jdi\/valhalla\/ValueArrayReferenceTest.java 8366806 generic-all\n+com\/sun\/jdi\/valhalla\/ValueClassTypeTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/BasicLauncherTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTest.java 8366806 generic-all\n+sun\/tools\/jhsdb\/HeapDumpTestWithActiveProcess.java 8366806 generic-all\n+sun\/tools\/jhsdb\/JShellHeapDumpTest.java 8366806 generic-all\n+\n","filename":"test\/jdk\/ProblemList.txt","additions":32,"deletions":0,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -383,0 +383,1 @@\n+        vmOptFinalFlag(map, \"TieredCompilation\");\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}