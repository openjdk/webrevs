{"files":[{"patch":"@@ -83,1 +83,1 @@\n-    \/\/ float16ToShortBits that normalizes NaNs\n+    \/\/ float16ToShortBits that normalizes NaNs, c.f. floatToIntBits vs floatToRawIntBits\n@@ -89,0 +89,1 @@\n+    \/\/ valueOf(BigDecimal) -- main implementation could be package private in BigDecimal\n@@ -715,0 +716,13 @@\n+     *\n+     * For discussion and derivation of this property see:\n+     *\n+     * \"When Is Double Rounding Innocuous?\" by Samuel Figueroa\n+     * ACM SIGNUM Newsletter, Volume 30 Issue 3, pp 21-26\n+     * https:\/\/dl.acm.org\/doi\/pdf\/10.1145\/221332.221334\n+     *\n+     * Figueroa's write-up refers to lecture notes by W. Kahan. Those\n+     * lectures notes are assumed to be these ones by Kahan and\n+     * others:\n+     *\n+     * https:\/\/www.arithmazium.org\/classroom\/lib\/Lecture_08_notes_slides.pdf\n+     * https:\/\/www.arithmazium.org\/classroom\/lib\/Lecture_09_notes_slides.pdf\n@@ -862,30 +876,182 @@\n-        \/\/ A simple scaling up to call a float or double fma doesn't\n-        \/\/ always work as double-rounding can occur and the sticky bit\n-        \/\/ information can be lost for rounding to a Float16 position.\n-        \/\/\n-        \/\/ The quantity:\n-        \/\/\n-        \/\/ convertToDouble(a)*convertToDouble(b) + convertToDouble(c)\n-        \/\/\n-        \/\/ will be an exact double value.\n-        \/\/\n-        \/\/ Note: the above conclusion is *incorrect*. The exponent of\n-        \/\/ the product of a*b can be so large that Float16.MIN_VALUE\n-        \/\/ cannot be held in a single double.\n-        \/\/\n-        \/\/ However, Float16 values with the smallest and largest\n-        \/\/ exponents can be held in a single double with precision to\n-        \/\/ spare. Therefore, all the hard rounding cases should be\n-        \/\/ covered, but some more analysis is needed to verify the\n-        \/\/ correctness of that approach.\n-        \/\/\n-        \/\/ Case analysis is needed with c is large compared to a*b.\n-        \/\/\n-        \/\/ The number of significand\n-        \/\/ bits in double, 53, is greater than the, maximum difference\n-        \/\/ in exponent values between bit positions of minimum and\n-        \/\/ maximum magnitude for Float16. Therefore, performing a*b+c\n-        \/\/ in double and then doing a single rounding of that value to\n-        \/\/ Float16 will implement this operation.\n-\n-        return valueOf( (a.doubleValue() * b.doubleValue()) +  c.doubleValue());\n+        \/*\n+         * The double format has sufficient precision that a Float16\n+         * fma can be computed by doing the arithmetic in double, with\n+         * one rounding error for the sum, and then a second rounding\n+         * error to round the product-sum to Float16. In pseudocode,\n+         * this method is equivalent to the following code, assuming\n+         * casting was defined between Float16 and double:\n+         *\n+         * double product = (double)a * (double)b;  \/\/ Always exact\n+         * double productSum = product + (double)c;\n+         * return (Float16)produdctSum;\n+         *\n+         * (Note that a similar relationship does *not* hold between\n+         * the double format and computing a float fma.)\n+         *\n+         * Below is a sketch of the proof that simple double\n+         * arithmetic can be used to implement a correctly rounded\n+         * Float16 fma.\n+         *\n+         * ----------------------\n+         *\n+         * As preliminaries, the handling of NaN and Infinity\n+         * arguments falls out as a consequence of general operation\n+         * of non-finite values by double * and +. Any NaN argument to\n+         * fma will lead to a NaN result, infinities will propagate or\n+         * get turned into NaN as appropriate, etc.\n+         *\n+         * One or more zero arguments are also handled correctly,\n+         * including the propagation of the sign of zero if all three\n+         * arguments are zero.\n+         *\n+         * The double format has 53 logical bits of precision and its\n+         * exponent range goes from -1022 to 1023. The Float16 format\n+         * has 11 bits of logical precision and its exponent range\n+         * goes from -14 to 15. Therefore, the bit positions\n+         * representable in the Float16 format range from the\n+         * subnormal 2^(-24), MIN_VALUE, to 2^15, the leading bit\n+         * position of MAX_VALUE.\n+         *\n+         * Consequently, a double can hold the exact sum of any two\n+         * Float16 values as the maximum difference in exponents of\n+         * Float16 values less than the precision width of double.\n+         *\n+         * In cases where the numerical value of (a * b) + c is\n+         * computed exactly in a double, after a single rounding to\n+         * Float16, the result is of necessity correct since the one\n+         * double -> Float16 conversion is the only source of\n+         * numerical error. The operation as implemented in those\n+         * cases would be equivalent to rounding the infinitely precise\n+         * value to the result format, etc.\n+         *\n+         * However, for some inputs, the intermediate product-sum will\n+         * *not* be exact and additional analysis is needed to justify\n+         * not having any corrective computation to compensate for\n+         * intermediate rounding errors.\n+         *\n+         * The following analysis will rely on the range of bit\n+         * positions representable in the intermediate\n+         * product-sum.\n+         *\n+         * For the product a*b of Float16 inputs, the range of\n+         * exponents for nonzero finite results goes from 2^(-28)\n+         * (from MIN_VALUE squared) to 2^31 (from the exact value of\n+         * MAX_VALUE squared). This full range of exponent positions,\n+         * (31 -(-28) + 1 ) = 60 exceeds the precision of\n+         * double. However, only the product a*b can exceed the\n+         * exponent range of Float16. Therefore, there are three main\n+         * cases to consider:\n+         *\n+         * 1) Large exponent product, exponent > Float16.MAX_EXPONENT\n+         *\n+         * The magnitude of the overflow threshold for Float16 is:\n+         *\n+         * MAX_VALUE + 1\/2 * ulp(MAX_VALUE) =  0x1.ffcp15 + 0x0.002p15 = 0x1.ffep15\n+         *\n+         * Therefore, any product greater than 0x1.ffep15 + MAX_VALUE\n+         * = 0x1.ffdp16 will certainly overflow (under round to\n+         * nearest) since adding in c = -MAX_VALUE will still be above\n+         * the overflow threshold.\n+         *\n+         * If the exponent of the product is 15 or 16, the smallest\n+         * subnormal Float16 is 2^-24 and the ~40 bit wide range bit\n+         * positions would fit in a single double exactly.\n+         *\n+         * 2) Exponent of product is within the range of _normal_\n+         * Float16 values; Float16.MIN_EXPONENT <=  exponent <= Float16.MAX_EXPONENT\n+         *\n+         * The exact product has at most 22 contiguous bits in its\n+         * logical significand. The third number being added in has\n+         * at most 11 contiguous bits in its significand and the\n+         * lowest bit position that could be set is\n+         * 2^(-24). Therefore, when the product has the maximum\n+         * in-range exponent, 2^15, a single double has enough\n+         * precision to hold down to the smallest subnormal bit\n+         * position, 15 - (-24) + 1 = 40 < 53. If the product was\n+         * large and overflowed when the third operand was added, this\n+         * would cause the exponent to increase to 16, which is within\n+         * the range of double, so the product-sum is exact and will\n+         * be correct when rounded to Float16.\n+         *\n+         * 3) Exponent of product is in the range of subnormal values or smaller,\n+         * exponent < Float16.MIN_EXPONENT\n+         *\n+         * The smallest exponent possible in a product is 2^(-48).\n+         * For moderately sized Float16 values added to the product,\n+         * with a leading exponent of about 4, the sum will not be\n+         * exact. Therefore, an analysis is needed to determine if the\n+         * double-rounding is benign or would lead to a different\n+         * final Float16 result. Double rounding an lead to a\n+         * different result in two cases:\n+         *\n+         * 1) The first rounding from the exact value to the extended\n+         * precision (here `double`) happens to be directed _toward_ 0\n+         * to a value exactly midway between two adjacent working\n+         * precision (here `Float16`) values, followed by a second\n+         * rounding from there which again happens to be directed\n+         * _toward_ 0 to one of these values (the one with lesser\n+         * magnitude).  A single rounding from the exact value to the\n+         * working precision, in contrast, rounds to the value with\n+         * larger magnitude.\n+         *\n+         * 2) Symmetrically, the first rounding to the extended\n+         * precision happens to be directed _away_ from 0 to a value\n+         * exactly midway between two adjacent working precision\n+         * values, followed by a second rounding from there which\n+         * again happens to be directed _away_ from 0 to one of these\n+         * values (the one with larger magnitude).  However, a single\n+         * rounding from the exact value to the working precision\n+         * rounds to the value with lesser magnitude.\n+         *\n+         * If the double rounding occurs in other cases, it is\n+         * innocuous, returning the same value as a single rounding to\n+         * the final format. Therefore, it is sufficient to show that\n+         * the first rounding to double does not occur at the midpoint\n+         * of two adjacent Float16 values:\n+         *\n+         * 1) If a, b and c have the same sign, the sum a*b + c has a\n+         * significand with a large gap of 20 or more 0s between the\n+         * bits of the significand of c to the left (at most 11 bits)\n+         * and those of the product a*b to the right (at most 22\n+         * bits).  The rounding bit for the final working precision of\n+         * `float16` is the leftmost 0 in the gap.\n+         *\n+         *   a) If rounding to `double` is directed toward 0, all the\n+         *   0s in the gap are preserved, thus the `Float16` rounding\n+         *   bit is unaffected and remains 0. This means that the\n+         *   `double` value is _not_ the midpoint of two adjacent\n+         *   `float16` values, so double rounding is harmless.\n+         *\n+         *   b) If rounding to `double` is directed away form 0, the\n+         *   rightmost 0 in the gap might be replaced by a 1, but the\n+         *   others are unaffected, including the `float16` rounding\n+         *   bit. Again, this shows that the `double` value is _not_\n+         *   the midpoint of two adjacent `float16` values, and double\n+         *   rounding is innocuous.\n+         *\n+         * 2) If a, b and c have opposite signs, in the sum a*b + c\n+         * the long gap of 0s above is replaced by a long gap of\n+         * 1s. The `float16` rounding bit is the leftmost 1 in the\n+         * gap, or the second leftmost 1 iff c is a power of 2. In\n+         * both cases, the rounding bit is followed by at least\n+         * another 1.\n+         *\n+         *   a) If rounding to `double` is directed toward 0, the\n+         *   `float16` rounding bit and its follower are preserved and\n+         *   both 1, so the `double` value is _not_ the midpoint of\n+         *   two adjacent `float16` values, and double rounding is\n+         *   harmless.\n+         *\n+         *   b) If rounding to `double` is directed away from 0, the\n+         *   `float16` rounding bit and its follower are either\n+         *   preserved (both 1), or both switch to 0. Either way, the\n+         *   `double` value is again _not_ the midpoint of two\n+         *   adjacent `float16` values, and double rounding is\n+         *   harmless.\n+         *\/\n+\n+        \/\/ product is numerically exact in float before the cast to\n+        \/\/ double; not necessary to widen to double before the\n+        \/\/ multiply.\n+        double product = (double)(a.floatValue() * b.floatValue());\n+        return valueOf(product + c.doubleValue());\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Float16.java","additions":197,"deletions":31,"binary":false,"changes":228,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n- * @bug 8329817\n+ * @bug 8329817 8334432\n@@ -49,0 +49,1 @@\n+        checkValueOfDouble();\n@@ -378,0 +379,47 @@\n+\n+    private static void checkValueOfDouble() {\n+        \/*\n+         * Check that double -> Float16 conversion rounds propertly\n+         * around the widway point for each finite Float16 value.\n+         *\/\n+        for(int i = 0; i <= Short.MAX_VALUE; i++ ) {\n+            \/\/ Start by just checking positive values...\n+            boolean isEven = ((i & 0x1) == 0);\n+\n+            Float16 f16 = Float16.shortBitsToFloat16((short)i);\n+\n+            if (!isFinite(f16))\n+                continue;\n+\n+            \/\/ System.out.println(\"\\t\" + toHexString(f16));\n+\n+            Float16 ulp = ulp(f16);\n+            double halfWay = f16.doubleValue() + ulp.doubleValue() * 0.5;\n+\n+            \/\/ Under the round to nearest even rounding policy, the\n+            \/\/ half-way case should round down to the starting value\n+            \/\/ if the starting value is even; otherwise, it should round up.\n+            float roundedBack = valueOf(halfWay).floatValue();\n+\n+            \/\/ While we're here, check negations\n+            float roundedBackNeg = valueOf(-halfWay).floatValue();\n+            String roundUpMsg   = \"Didn't get half-way case rounding down\";\n+            String roundDownMsg = \"Didn't get half-way case rounding up\";\n+            if (isEven) {\n+                checkFloat16(f16,         roundedBack,    roundDownMsg);\n+                checkFloat16(negate(f16), roundedBackNeg, roundDownMsg);\n+            } else {\n+                checkFloat16(add(f16, ulp), roundedBack,                roundUpMsg);\n+                checkFloat16(subtract(negate(f16), ulp),roundedBackNeg, roundUpMsg);\n+            }\n+\n+            \/\/ Should always round down\n+            double halfWayNextDown = Math.nextDown(halfWay);\n+            checkFloat16(f16, valueOf(halfWayNextDown).floatValue(), roundDownMsg);\n+\n+            \/\/ Should always round down up\n+            double halfWayNextUp =   Math.nextUp(halfWay);\n+            checkFloat16(add(f16, ulp), valueOf(halfWayNextUp).floatValue(),   roundUpMsg);\n+        }\n+    }\n+\n@@ -380,0 +428,1 @@\n+            testZeroNanInfCombos();\n@@ -383,0 +432,19 @@\n+            testRounding();\n+        }\n+\n+        private static void testZeroNanInfCombos() {\n+            float [] testInputs = {\n+                Float.NaN,\n+                -InfinityF,\n+                +InfinityF,\n+                -0.0f,\n+                +0.0f,\n+            };\n+\n+            for (float i : testInputs) {\n+                for (float j : testInputs) {\n+                    for (float k : testInputs) {\n+                        testFusedMacCase(i, j, k, Math.fma(i, j, k));\n+                    }\n+                }\n+            }\n@@ -512,0 +580,50 @@\n+        private static void testRounding() {\n+            final float ulpOneFp16 = ulp(valueOf(1.0f)).floatValue();\n+\n+            float [][] testCases = {\n+                \/\/ The product is equal to\n+                \/\/ (MAX_VALUE + 1\/2 * ulp(MAX_VALUE) + MAX_VALUE = (0x1.ffcp15 + 0x0.002p15)+ 0x1.ffcp15\n+                \/\/ so overflows.\n+                {0x1.3p1f, 0x1.afp15f, -MAX_VAL_FP16,\n+                 InfinityF},\n+\n+                \/\/ Product exactly equals 0x1.ffep15, the overflow\n+                \/\/ threshold; subtracting a non-zero finite value will\n+                \/\/ result in MAX_VALUE, adding zero or a positive\n+                \/\/ value will overflow.\n+                {0x1.2p10f, 0x1.c7p5f, -0x1.0p-14f,\n+                 MAX_VAL_FP16},\n+\n+                {0x1.2p10f, 0x1.c7p5f, -0.0f,\n+                 InfinityF},\n+\n+                {0x1.2p10f, 0x1.c7p5f, +0.0f,\n+                 InfinityF},\n+\n+                {0x1.2p10f, 0x1.c7p5f, +0x1.0p-14f,\n+                 InfinityF},\n+\n+                {0x1.2p10f, 0x1.c7p5f, InfinityF,\n+                 InfinityF},\n+\n+                \/\/ PRECISION bits in the subnormal intermediate product\n+                {0x1.ffcp-14f, 0x1.0p-24f, 0x1.0p13f, \/\/ Can be held exactly\n+                 0x1.0p13f},\n+\n+                {0x1.ffcp-14f, 0x1.0p-24f, 0x1.0p14f, \/\/ *Cannot* be held exactly\n+                 0x1.0p14f},\n+\n+                \/\/ Check values where the exact result cannot be\n+                \/\/ exactly stored in a double.\n+                {0x1.0p-24f, 0x1.0p-24f, 0x1.0p10f,\n+                 0x1.0p10f},\n+\n+                {0x1.0p-24f, 0x1.0p-24f, 0x1.0p14f,\n+                 0x1.0p14f},\n+            };\n+\n+            for (float[] testCase: testCases) {\n+                testFusedMacCase(testCase[0], testCase[1], testCase[2], testCase[3]);\n+            }\n+        }\n+\n","filename":"test\/jdk\/java\/lang\/Float16\/BasicFloat16ArithTests.java","additions":119,"deletions":1,"binary":false,"changes":120,"status":"modified"}]}