{"files":[{"patch":"@@ -82,0 +82,2 @@\n+    -taglet build.tools.taglet.PreviewNote \\\n+    --preview-note-tag previewNote \\\n","filename":"make\/Docs.gmk","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1091,1 +1091,1 @@\n-        linux_x64: \"gcc13.2.0-OL6.4+1.0\",\n+        linux_x64: \"gcc14.2.0-OL6.4+1.0\",\n@@ -1094,1 +1094,1 @@\n-        linux_aarch64: \"gcc13.2.0-OL7.6+1.0\",\n+        linux_aarch64: \"gcc14.2.0-OL7.6+1.0\",\n@@ -1096,3 +1096,3 @@\n-        linux_ppc64le: \"gcc13.2.0-Fedora_41+1.0\",\n-        linux_s390x: \"gcc13.2.0-Fedora_41+1.0\",\n-        linux_riscv64: \"gcc13.2.0-Fedora_41+1.0\"\n+        linux_ppc64le: \"gcc14.2.0-Fedora_41+1.0\",\n+        linux_s390x: \"gcc14.2.0-Fedora_41+1.0\",\n+        linux_riscv64: \"gcc14.2.0-Fedora_41+1.0\"\n","filename":"make\/conf\/jib-profiles.js","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -4676,0 +4676,5 @@\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n@@ -4684,0 +4689,5 @@\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n@@ -4692,0 +4702,5 @@\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n@@ -4699,0 +4714,4 @@\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n@@ -4707,0 +4726,4 @@\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n@@ -4714,0 +4737,5 @@\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n@@ -4721,0 +4749,5 @@\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n@@ -4727,1 +4760,5 @@\n-    void vs_notr(const VSeq<N>& v, const VSeq<N>& v1) {\n+  void vs_notr(const VSeq<N>& v, const VSeq<N>& v1) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n@@ -4733,0 +4770,24 @@\n+  template<int N>\n+  void vs_sqdmulh(const VSeq<N>& v, Assembler::SIMD_Arrangement T, const VSeq<N>& v1, const VSeq<N>& v2) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n+    for (int i = 0; i < N; i++) {\n+      __ sqdmulh(v[i], T, v1[i], v2[i]);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_mlsv(const VSeq<N>& v, Assembler::SIMD_Arrangement T, const VSeq<N>& v1, VSeq<N>& v2) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n+    for (int i = 0; i < N; i++) {\n+      __ mlsv(v[i], T, v1[i], v2[i]);\n+    }\n+  }\n+\n@@ -4748,0 +4809,1 @@\n+    static_assert((N & (N - 1)) == 0, \"sequence length must be even\");\n@@ -4758,0 +4820,1 @@\n+    static_assert((N & (N - 1)) == 0, \"sequence length must be even\");\n@@ -4763,0 +4826,43 @@\n+  \/\/ load N\/2 pairs of quadword values from memory de-interleaved into\n+  \/\/ N vector registers 2 at a time via the address supplied in base\n+  \/\/ using post-increment addressing.\n+  template<int N>\n+  void vs_ld2_post(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base) {\n+    static_assert((N & (N - 1)) == 0, \"sequence length must be even\");\n+    for (int i = 0; i < N; i += 2) {\n+      __ ld2(v[i], v[i+1], T, __ post(base, 32));\n+    }\n+  }\n+\n+  \/\/ store N vector registers interleaved into N\/2 pairs of quadword\n+  \/\/ memory locations via the address supplied in base using\n+  \/\/ post-increment addressing.\n+  template<int N>\n+  void vs_st2_post(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base) {\n+    static_assert((N & (N - 1)) == 0, \"sequence length must be even\");\n+    for (int i = 0; i < N; i += 2) {\n+      __ st2(v[i], v[i+1], T, __ post(base, 32));\n+    }\n+  }\n+\n+  \/\/ load N quadword values from memory de-interleaved into N vector\n+  \/\/ registers 3 elements at a time via the address supplied in base.\n+  template<int N>\n+  void vs_ld3(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base) {\n+    static_assert(N == ((N \/ 3) * 3), \"sequence length must be multiple of 3\");\n+    for (int i = 0; i < N; i += 3) {\n+      __ ld3(v[i], v[i+1], v[i+2], T, base);\n+    }\n+  }\n+\n+  \/\/ load N quadword values from memory de-interleaved into N vector\n+  \/\/ registers 3 elements at a time via the address supplied in base\n+  \/\/ using post-increment addressing.\n+  template<int N>\n+  void vs_ld3_post(const VSeq<N>& v, Assembler::SIMD_Arrangement T, Register base) {\n+    static_assert(N == ((N \/ 3) * 3), \"sequence length must be multiple of 3\");\n+    for (int i = 0; i < N; i += 3) {\n+      __ ld3(v[i], v[i+1], v[i+2], T, __ post(base, 48));\n+    }\n+  }\n+\n@@ -4835,2 +4941,1 @@\n-  \/\/ Helper routines for various flavours of dilithium montgomery\n-  \/\/ multiply\n+  \/\/ Helper routines for various flavours of Montgomery multiply\n@@ -4838,2 +4943,2 @@\n-  \/\/ Perform 16 32-bit Montgomery multiplications in parallel\n-  \/\/ See the montMul() method of the sun.security.provider.ML_DSA class.\n+  \/\/ Perform 16 32-bit (4x4S) or 32 16-bit (4 x 8H) Montgomery\n+  \/\/ multiplications in parallel\n@@ -4841,6 +4946,10 @@\n-  \/\/ Computes 4x4S results\n-  \/\/    a = b * c * 2^-32 mod MONT_Q\n-  \/\/ Inputs:  vb, vc - 4x4S vector register sequences\n-  \/\/          vq - 2x4S constants <MONT_Q, MONT_Q_INV_MOD_R>\n-  \/\/ Temps:   vtmp - 4x4S vector sequence trashed after call\n-  \/\/ Outputs: va - 4x4S vector register sequences\n+\n+  \/\/ See the montMul() method of the sun.security.provider.ML_DSA\n+  \/\/ class.\n+  \/\/\n+  \/\/ Computes 4x4S results or 8x8H results\n+  \/\/    a = b * c * 2^MONT_R_BITS mod MONT_Q\n+  \/\/ Inputs:  vb, vc - 4x4S or 4x8H vector register sequences\n+  \/\/          vq - 2x4S or 2x8H constants <MONT_Q, MONT_Q_INV_MOD_R>\n+  \/\/ Temps:   vtmp - 4x4S or 4x8H vector sequence trashed after call\n+  \/\/ Outputs: va - 4x4S or 4x8H vector register sequences\n@@ -4849,3 +4958,6 @@\n-  \/\/ n.b. MONT_R_BITS is 32, so the right shift by it is implicit.\n-  void dilithium_montmul16(const VSeq<4>& va, const VSeq<4>& vb, const VSeq<4>& vc,\n-                    const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+  \/\/ va must have a non-zero delta i.e. it must not be a constant vseq.\n+  \/\/ n.b. MONT_R_BITS is 16 or 32, so the right shift by it is implicit.\n+  void vs_montmul4(const VSeq<4>& va, const VSeq<4>& vb, const VSeq<4>& vc,\n+                   Assembler::SIMD_Arrangement T,\n+                   const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    assert (T == __ T4S || T == __ T8H, \"invalid arrangement for montmul\");\n@@ -4865,0 +4977,1 @@\n+    assert(!va.is_constant(), \"output vector must identify 4 different registers\");\n@@ -4868,2 +4981,2 @@\n-      __ sqdmulh(vtmp[i], __ T4S, vb[i], vc[i]); \/\/ aHigh = hi32(2 * b * c)\n-      __ mulv(va[i], __ T4S, vb[i], vc[i]);    \/\/ aLow = lo32(b * c)\n+      __ sqdmulh(vtmp[i], T, vb[i], vc[i]); \/\/ aHigh = hi32(2 * b * c)\n+      __ mulv(va[i], T, vb[i], vc[i]);    \/\/ aLow = lo32(b * c)\n@@ -4873,1 +4986,1 @@\n-      __ mulv(va[i], __ T4S, va[i], vq[0]);     \/\/ m = aLow * qinv\n+      __ mulv(va[i], T, va[i], vq[0]);     \/\/ m = aLow * qinv\n@@ -4877,1 +4990,1 @@\n-      __ sqdmulh(va[i], __ T4S, va[i], vq[1]);  \/\/ n = hi32(2 * m * q)\n+      __ sqdmulh(va[i], T, va[i], vq[1]);  \/\/ n = hi32(2 * m * q)\n@@ -4881,1 +4994,1 @@\n-      __ shsubv(va[i], __ T4S, vtmp[i], va[i]);   \/\/ a = (aHigh - n) \/ 2\n+      __ shsubv(va[i], T, vtmp[i], va[i]);   \/\/ a = (aHigh - n) \/ 2\n@@ -4885,2 +4998,6 @@\n-  \/\/ Perform 2x16 32-bit Montgomery multiplications in parallel\n-  \/\/ See the montMul() method of the sun.security.provider.ML_DSA class.\n+  \/\/ Perform 8 32-bit (4x4S) or 16 16-bit (2 x 8H) Montgomery\n+  \/\/ multiplications in parallel\n+  \/\/\n+\n+  \/\/ See the montMul() method of the sun.security.provider.ML_DSA\n+  \/\/ class.\n@@ -4888,6 +5005,6 @@\n-  \/\/ Computes 8x4S results\n-  \/\/    a = b * c * 2^-32 mod MONT_Q\n-  \/\/ Inputs:  vb, vc - 8x4S vector register sequences\n-  \/\/          vq - 2x4S constants <MONT_Q, MONT_Q_INV_MOD_R>\n-  \/\/ Temps:   vtmp - 4x4S vector sequence trashed after call\n-  \/\/ Outputs: va - 8x4S vector register sequences\n+  \/\/ Computes 4x4S results or 8x8H results\n+  \/\/    a = b * c * 2^MONT_R_BITS mod MONT_Q\n+  \/\/ Inputs:  vb, vc - 4x4S or 4x8H vector register sequences\n+  \/\/          vq - 2x4S or 2x8H constants <MONT_Q, MONT_Q_INV_MOD_R>\n+  \/\/ Temps:   vtmp - 4x4S or 4x8H vector sequence trashed after call\n+  \/\/ Outputs: va - 4x4S or 4x8H vector register sequences\n@@ -4896,3 +5013,65 @@\n-  \/\/ n.b. MONT_R_BITS is 32, so the right shift by it is implicit.\n-  void vs_montmul32(const VSeq<8>& va, const VSeq<8>& vb, const VSeq<8>& vc,\n-                    const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+  \/\/ va must have a non-zero delta i.e. it must not be a constant vseq.\n+  \/\/ n.b. MONT_R_BITS is 16 or 32, so the right shift by it is implicit.\n+  void vs_montmul2(const VSeq<2>& va, const VSeq<2>& vb, const VSeq<2>& vc,\n+                   Assembler::SIMD_Arrangement T,\n+                   const VSeq<2>& vtmp, const VSeq<2>& vq) {\n+    assert (T == __ T4S || T == __ T8H, \"invalid arrangement for montmul\");\n+    assert(vs_disjoint(vb, vc), \"vb and vc overlap\");\n+    assert(vs_disjoint(vb, vq), \"vb and vq overlap\");\n+    assert(vs_disjoint(vb, vtmp), \"vb and vtmp overlap\");\n+\n+    assert(vs_disjoint(vc, vq), \"vc and vq overlap\");\n+    assert(vs_disjoint(vc, vtmp), \"vc and vtmp overlap\");\n+\n+    assert(vs_disjoint(vq, vtmp), \"vq and vtmp overlap\");\n+\n+    assert(vs_disjoint(va, vc) || vs_same(va, vc), \"va and vc neither disjoint nor equal\");\n+    assert(vs_disjoint(va, vb), \"va and vb overlap\");\n+    assert(vs_disjoint(va, vq), \"va and vq overlap\");\n+    assert(vs_disjoint(va, vtmp), \"va and vtmp overlap\");\n+    assert(!va.is_constant(), \"output vector must identify 2 different registers\");\n+\n+    \/\/ schedule 2 streams of instructions across the vector sequences\n+    for (int i = 0; i < 2; i++) {\n+      __ sqdmulh(vtmp[i], T, vb[i], vc[i]); \/\/ aHigh = hi32(2 * b * c)\n+      __ mulv(va[i], T, vb[i], vc[i]);    \/\/ aLow = lo32(b * c)\n+    }\n+\n+    for (int i = 0; i < 2; i++) {\n+      __ mulv(va[i], T, va[i], vq[0]);     \/\/ m = aLow * qinv\n+    }\n+\n+    for (int i = 0; i < 2; i++) {\n+      __ sqdmulh(va[i], T, va[i], vq[1]);  \/\/ n = hi32(2 * m * q)\n+    }\n+\n+    for (int i = 0; i < 2; i++) {\n+      __ shsubv(va[i], T, vtmp[i], va[i]);   \/\/ a = (aHigh - n) \/ 2\n+    }\n+  }\n+\n+  \/\/ Perform 16 16-bit Montgomery multiplications in parallel.\n+  void kyber_montmul16(const VSeq<2>& va, const VSeq<2>& vb, const VSeq<2>& vc,\n+                       const VSeq<2>& vtmp, const VSeq<2>& vq) {\n+    \/\/ Use the helper routine to schedule a 2x8H Montgomery multiply.\n+    \/\/ It will assert that the register use is valid\n+    vs_montmul2(va, vb, vc, __ T8H, vtmp, vq);\n+  }\n+\n+  \/\/ Perform 32 16-bit Montgomery multiplications in parallel.\n+  void kyber_montmul32(const VSeq<4>& va, const VSeq<4>& vb, const VSeq<4>& vc,\n+                       const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    \/\/ Use the helper routine to schedule a 4x8H Montgomery multiply.\n+    \/\/ It will assert that the register use is valid\n+    vs_montmul4(va, vb, vc, __ T8H, vtmp, vq);\n+  }\n+\n+  \/\/ Perform 64 16-bit Montgomery multiplications in parallel.\n+  void kyber_montmul64(const VSeq<8>& va, const VSeq<8>& vb, const VSeq<8>& vc,\n+                       const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    \/\/ Schedule two successive 4x8H multiplies via the montmul helper\n+    \/\/ on the front and back halves of va, vb and vc. The helper will\n+    \/\/ assert that the register use has no overlap conflicts on each\n+    \/\/ individual call but we also need to ensure that the necessary\n+    \/\/ disjoint\/equality constraints are met across both calls.\n+\n@@ -4916,2 +5095,2 @@\n-    \/\/ we need to multiply the front and back halves of each sequence\n-    \/\/ 4x4S at a time because\n+    \/\/ we multiply the front and back halves of each sequence 4 at a\n+    \/\/ time because\n@@ -4926,2 +5105,863 @@\n-    dilithium_montmul16(vs_front(va), vs_front(vb), vs_front(vc), vtmp, vq);\n-    dilithium_montmul16(vs_back(va), vs_back(vb), vs_back(vc), vtmp, vq);\n+    vs_montmul4(vs_front(va), vs_front(vb), vs_front(vc), __ T8H, vtmp, vq);\n+    vs_montmul4(vs_back(va), vs_back(vb), vs_back(vc), __ T8H, vtmp, vq);\n+  }\n+\n+  void kyber_montmul32_sub_add(const VSeq<4>& va0, const VSeq<4>& va1,\n+                               const VSeq<4>& vc,\n+                               const VSeq<4>& vtmp,\n+                               const VSeq<2>& vq) {\n+    \/\/ compute a = montmul(a1, c)\n+    kyber_montmul32(vc, va1, vc, vtmp, vq);\n+    \/\/ ouptut a1 = a0 - a\n+    vs_subv(va1, __ T8H, va0, vc);\n+    \/\/    and a0 = a0 + a\n+    vs_addv(va0, __ T8H, va0, vc);\n+  }\n+\n+  void kyber_sub_add_montmul32(const VSeq<4>& va0, const VSeq<4>& va1,\n+                               const VSeq<4>& vb,\n+                               const VSeq<4>& vtmp1,\n+                               const VSeq<4>& vtmp2,\n+                               const VSeq<2>& vq) {\n+    \/\/ compute c = a0 - a1\n+    vs_subv(vtmp1, __ T8H, va0, va1);\n+    \/\/ output a0 = a0 + a1\n+    vs_addv(va0, __ T8H, va0, va1);\n+    \/\/ output a1 = b montmul c\n+    kyber_montmul32(va1, vtmp1, vb, vtmp2, vq);\n+  }\n+\n+  void load64shorts(const VSeq<8>& v, Register shorts) {\n+    vs_ldpq_post(v, shorts);\n+  }\n+\n+  void load32shorts(const VSeq<4>& v, Register shorts) {\n+    vs_ldpq_post(v, shorts);\n+  }\n+\n+  void store64shorts(VSeq<8> v, Register tmpAddr) {\n+    vs_stpq_post(v, tmpAddr);\n+  }\n+\n+  \/\/ Kyber NTT function.\n+  \/\/ Implements\n+  \/\/ static int implKyberNtt(short[] poly, short[] ntt_zetas) {}\n+  \/\/\n+  \/\/ coeffs (short[256]) = c_rarg0\n+  \/\/ ntt_zetas (short[256]) = c_rarg1\n+  address generate_kyberNtt() {\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyberNtt_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register coeffs = c_rarg0;\n+    const Register zetas = c_rarg1;\n+\n+    const Register kyberConsts = r10;\n+    const Register tmpAddr = r11;\n+\n+    VSeq<8> vs1(0), vs2(16), vs3(24);  \/\/ 3 sets of 8x8H inputs\/outputs\n+    VSeq<4> vtmp = vs_front(vs3);      \/\/ n.b. tmp registers overlap vs3\n+    VSeq<2> vq(30);                    \/\/ n.b. constants overlap vs3\n+\n+    __ lea(kyberConsts, ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+    \/\/ load the montmul constants\n+    vs_ldpq(vq, kyberConsts);\n+\n+    \/\/ Each level corresponds to an iteration of the outermost loop of the\n+    \/\/ Java method seilerNTT(int[] coeffs). There are some differences\n+    \/\/ from what is done in the seilerNTT() method, though:\n+    \/\/ 1. The computation is using 16-bit signed values, we do not convert them\n+    \/\/ to ints here.\n+    \/\/ 2. The zetas are delivered in a bigger array, 128 zetas are stored in\n+    \/\/ this array for each level, it is easier that way to fill up the vector\n+    \/\/ registers.\n+    \/\/ 3. In the seilerNTT() method we use R = 2^20 for the Montgomery\n+    \/\/ multiplications (this is because that way there should not be any\n+    \/\/ overflow during the inverse NTT computation), here we usr R = 2^16 so\n+    \/\/ that we can use the 16-bit arithmetic in the vector unit.\n+    \/\/\n+    \/\/ On each level, we fill up the vector registers in such a way that the\n+    \/\/ array elements that need to be multiplied by the zetas go into one\n+    \/\/ set of vector registers while the corresponding ones that don't need to\n+    \/\/ be multiplied, go into another set.\n+    \/\/ We can do 32 Montgomery multiplications in parallel, using 12 vector\n+    \/\/ registers interleaving the steps of 4 identical computations,\n+    \/\/ each done on 8 16-bit values per register.\n+\n+    \/\/ At levels 0-3 the coefficients multiplied by or added\/subtracted\n+    \/\/ to the zetas occur in discrete blocks whose size is some multiple\n+    \/\/ of 32.\n+\n+    \/\/ level 0\n+    __ add(tmpAddr, coeffs, 256);\n+    load64shorts(vs1, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 0);\n+    load64shorts(vs1, tmpAddr);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    __ add(tmpAddr, coeffs, 0);\n+    vs_stpq_post(vs1, tmpAddr);\n+    __ add(tmpAddr, coeffs, 256);\n+    vs_stpq_post(vs3, tmpAddr);\n+    \/\/ restore montmul constants\n+    vs_ldpq(vq, kyberConsts);\n+    load64shorts(vs1, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 128);\n+    load64shorts(vs1, tmpAddr);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    __ add(tmpAddr, coeffs, 128);\n+    store64shorts(vs1, tmpAddr);\n+    __ add(tmpAddr, coeffs, 384);\n+    store64shorts(vs3, tmpAddr);\n+\n+    \/\/ level 1\n+    \/\/ restore montmul constants\n+    vs_ldpq(vq, kyberConsts);\n+    __ add(tmpAddr, coeffs, 128);\n+    load64shorts(vs1, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 0);\n+    load64shorts(vs1, tmpAddr);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    __ add(tmpAddr, coeffs, 0);\n+    store64shorts(vs1, tmpAddr);\n+    store64shorts(vs3, tmpAddr);\n+    vs_ldpq(vq, kyberConsts);\n+    __ add(tmpAddr, coeffs, 384);\n+    load64shorts(vs1, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 256);\n+    load64shorts(vs1, tmpAddr);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    __ add(tmpAddr, coeffs, 256);\n+    store64shorts(vs1, tmpAddr);\n+    store64shorts(vs3, tmpAddr);\n+\n+    \/\/ level 2\n+    vs_ldpq(vq, kyberConsts);\n+    int offsets1[4] = { 0, 32, 128, 160 };\n+    vs_ldpq_indexed(vs1, coeffs, 64, offsets1);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_ldpq_indexed(vs1, coeffs, 0, offsets1);\n+    \/\/ kyber_subv_addv64();\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    __ add(tmpAddr, coeffs, 0);\n+    vs_stpq_post(vs_front(vs1), tmpAddr);\n+    vs_stpq_post(vs_front(vs3), tmpAddr);\n+    vs_stpq_post(vs_back(vs1), tmpAddr);\n+    vs_stpq_post(vs_back(vs3), tmpAddr);\n+    vs_ldpq(vq, kyberConsts);\n+    vs_ldpq_indexed(vs1, tmpAddr, 64, offsets1);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_ldpq_indexed(vs1,  coeffs, 256, offsets1);\n+    \/\/ kyber_subv_addv64();\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    __ add(tmpAddr, coeffs, 256);\n+    vs_stpq_post(vs_front(vs1), tmpAddr);\n+    vs_stpq_post(vs_front(vs3), tmpAddr);\n+    vs_stpq_post(vs_back(vs1), tmpAddr);\n+    vs_stpq_post(vs_back(vs3), tmpAddr);\n+\n+    \/\/ level 3\n+    vs_ldpq(vq, kyberConsts);\n+    int offsets2[4] = { 0, 64, 128, 192 };\n+    vs_ldpq_indexed(vs1, coeffs, 32, offsets2);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_ldpq_indexed(vs1, coeffs, 0, offsets2);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    vs_stpq_indexed(vs1, coeffs, 0, offsets2);\n+    vs_stpq_indexed(vs3, coeffs, 32, offsets2);\n+\n+    vs_ldpq(vq, kyberConsts);\n+    vs_ldpq_indexed(vs1, coeffs, 256 + 32, offsets2);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_ldpq_indexed(vs1, coeffs, 256, offsets2);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    vs_stpq_indexed(vs1, coeffs, 256, offsets2);\n+    vs_stpq_indexed(vs3, coeffs, 256 + 32, offsets2);\n+\n+    \/\/ level 4\n+    \/\/ At level 4 coefficients occur in 8 discrete blocks of size 16\n+    \/\/ so they are loaded using employing an ldr at 8 distinct offsets.\n+\n+    vs_ldpq(vq, kyberConsts);\n+    int offsets3[8] = { 0, 32, 64, 96, 128, 160, 192, 224 };\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 16, offsets3);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 0, offsets3);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    vs_str_indexed(vs1, __ Q, coeffs, 0, offsets3);\n+    vs_str_indexed(vs3, __ Q, coeffs, 16, offsets3);\n+\n+    vs_ldpq(vq, kyberConsts);\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 256 + 16, offsets3);\n+    load64shorts(vs2, zetas);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 256, offsets3);\n+    vs_subv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_addv(vs1, __ T8H, vs1, vs2);\n+    vs_str_indexed(vs1, __ Q, coeffs, 256, offsets3);\n+    vs_str_indexed(vs3, __ Q, coeffs, 256 + 16, offsets3);\n+\n+    \/\/ level 5\n+    \/\/ At level 5 related coefficients occur in discrete blocks of size 8 so\n+    \/\/ need to be loaded interleaved using an ld2 operation with arrangement 2D.\n+\n+    vs_ldpq(vq, kyberConsts);\n+    int offsets4[4] = { 0, 32, 64, 96 };\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 0, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 0, offsets4);\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 128, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 128, offsets4);\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 256, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 256, offsets4);\n+\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 384, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 384, offsets4);\n+\n+    \/\/ level 6\n+    \/\/ At level 6 related coefficients occur in discrete blocks of size 4 so\n+    \/\/ need to be loaded interleaved using an ld2 operation with arrangement 4S.\n+\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 0, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 0, offsets4);\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 128, offsets4);\n+    \/\/ __ ldpq(v18, v19, __ post(zetas, 32));\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 128, offsets4);\n+\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 256, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 256, offsets4);\n+\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 384, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_montmul32_sub_add(vs_even(vs1), vs_odd(vs1), vs_front(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 384, offsets4);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+  \/\/ Kyber Inverse NTT function\n+  \/\/ Implements\n+  \/\/ static int implKyberInverseNtt(short[] poly, short[] zetas) {}\n+  \/\/\n+  \/\/ coeffs (short[256]) = c_rarg0\n+  \/\/ ntt_zetas (short[256]) = c_rarg1\n+  address generate_kyberInverseNtt() {\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyberInverseNtt_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register coeffs = c_rarg0;\n+    const Register zetas = c_rarg1;\n+\n+    const Register kyberConsts = r10;\n+    const Register tmpAddr = r11;\n+    const Register tmpAddr2 = c_rarg2;\n+\n+    VSeq<8> vs1(0), vs2(16), vs3(24);  \/\/ 3 sets of 8x8H inputs\/outputs\n+    VSeq<4> vtmp = vs_front(vs3);      \/\/ n.b. tmp registers overlap vs3\n+    VSeq<2> vq(30);                    \/\/ n.b. constants overlap vs3\n+\n+    __ lea(kyberConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+\n+    \/\/ level 0\n+    \/\/ At level 0 related coefficients occur in discrete blocks of size 4 so\n+    \/\/ need to be loaded interleaved using an ld2 operation with arrangement 4S.\n+\n+    vs_ldpq(vq, kyberConsts);\n+    int offsets4[4] = { 0, 32, 64, 96 };\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 0, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 0, offsets4);\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 128, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 128, offsets4);\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 256, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 256, offsets4);\n+    vs_ld2_indexed(vs1, __ T4S, coeffs, tmpAddr, 384, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T4S, coeffs, tmpAddr, 384, offsets4);\n+\n+    \/\/ level 1\n+    \/\/ At level 1 related coefficients occur in discrete blocks of size 8 so\n+    \/\/ need to be loaded interleaved using an ld2 operation with arrangement 2D.\n+\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 0, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 0, offsets4);\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 128, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 128, offsets4);\n+\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 256, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 256, offsets4);\n+    vs_ld2_indexed(vs1, __ T2D, coeffs, tmpAddr, 384, offsets4);\n+    load32shorts(vs_front(vs2), zetas);\n+    kyber_sub_add_montmul32(vs_even(vs1), vs_odd(vs1),\n+                            vs_front(vs2), vs_back(vs2), vtmp, vq);\n+    vs_st2_indexed(vs1, __ T2D, coeffs, tmpAddr, 384, offsets4);\n+\n+    \/\/ level 2\n+    \/\/ At level 2 coefficients occur in 8 discrete blocks of size 16\n+    \/\/ so they are loaded using employing an ldr at 8 distinct offsets.\n+\n+    int offsets3[8] = { 0, 32, 64, 96, 128, 160, 192, 224 };\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 0, offsets3);\n+    vs_ldr_indexed(vs2, __ Q, coeffs, 16, offsets3);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    vs_str_indexed(vs3, __ Q, coeffs, 0, offsets3);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_str_indexed(vs2, __ Q, coeffs, 16, offsets3);\n+\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 256, offsets3);\n+    vs_ldr_indexed(vs2, __ Q, coeffs, 256 + 16, offsets3);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    vs_str_indexed(vs3, __ Q, coeffs, 256, offsets3);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_str_indexed(vs2, __ Q, coeffs, 256 + 16, offsets3);\n+\n+    \/\/ Barrett reduction at indexes where overflow may happen\n+\n+    \/\/ load q and the multiplier for the Barrett reduction\n+    __ add(tmpAddr, kyberConsts, 16);\n+    vs_ldpq(vq, tmpAddr);\n+\n+    VSeq<8> vq1 = VSeq<8>(vq[0], 0); \/\/ 2 constant 8 sequences\n+    VSeq<8> vq2 = VSeq<8>(vq[1], 0); \/\/ for above two kyber constants\n+    VSeq<8> vq3 = VSeq<8>(v29, 0);   \/\/ 3rd sequence for const montmul\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 0, offsets3);\n+    vs_sqdmulh(vs2, __ T8H, vs1, vq2);\n+    vs_sshr(vs2, __ T8H, vs2, 11);\n+    vs_mlsv(vs1, __ T8H, vs2, vq1);\n+    vs_str_indexed(vs1, __ Q, coeffs, 0, offsets3);\n+    vs_ldr_indexed(vs1, __ Q, coeffs, 256, offsets3);\n+    vs_sqdmulh(vs2, __ T8H, vs1, vq2);\n+    vs_sshr(vs2, __ T8H, vs2, 11);\n+    vs_mlsv(vs1, __ T8H, vs2, vq1);\n+    vs_str_indexed(vs1, __ Q, coeffs, 256, offsets3);\n+\n+    \/\/ level 3\n+    \/\/ From level 3 upwards coefficients occur in discrete blocks whose size is\n+    \/\/ some multiple of 32 so can be loaded using ldpq and suitable indexes.\n+\n+    int offsets2[4] = { 0, 64, 128, 192 };\n+    vs_ldpq_indexed(vs1, coeffs, 0, offsets2);\n+    vs_ldpq_indexed(vs2, coeffs, 32, offsets2);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    vs_stpq_indexed(vs3, coeffs, 0, offsets2);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_stpq_indexed(vs2, coeffs, 32, offsets2);\n+\n+    vs_ldpq_indexed(vs1, coeffs, 256, offsets2);\n+    vs_ldpq_indexed(vs2, coeffs, 256 + 32, offsets2);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    vs_stpq_indexed(vs3, coeffs, 256, offsets2);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_stpq_indexed(vs2, coeffs, 256 + 32, offsets2);\n+\n+    \/\/ level 4\n+\n+    int offsets1[4] = { 0, 32, 128, 160 };\n+    vs_ldpq_indexed(vs1, coeffs, 0, offsets1);\n+    vs_ldpq_indexed(vs2, coeffs, 64, offsets1);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    vs_stpq_indexed(vs3, coeffs, 0, offsets1);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_stpq_indexed(vs2, coeffs, 64, offsets1);\n+\n+    vs_ldpq_indexed(vs1, coeffs, 256, offsets1);\n+    vs_ldpq_indexed(vs2, coeffs, 256 + 64, offsets1);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    vs_stpq_indexed(vs3, coeffs, 256, offsets1);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    vs_stpq_indexed(vs2, coeffs, 256 + 64, offsets1);\n+\n+    \/\/ level 5\n+\n+    __ add(tmpAddr, coeffs, 0);\n+    load64shorts(vs1, tmpAddr);\n+    __ add(tmpAddr, coeffs, 128);\n+    load64shorts(vs2, tmpAddr);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    __ add(tmpAddr, coeffs, 0);\n+    store64shorts(vs3, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 128);\n+    store64shorts(vs2, tmpAddr);\n+\n+    load64shorts(vs1, tmpAddr);\n+    __ add(tmpAddr, coeffs, 384);\n+    load64shorts(vs2, tmpAddr);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    __ add(tmpAddr, coeffs, 256);\n+    store64shorts(vs3, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 384);\n+    store64shorts(vs2, tmpAddr);\n+\n+    \/\/ Barrett reduction at indexes where overflow may happen\n+\n+    \/\/ load q and the multiplier for the Barrett reduction\n+    __ add(tmpAddr, kyberConsts, 16);\n+    vs_ldpq(vq, tmpAddr);\n+\n+    int offsets0[2] = { 0, 256 };\n+    vs_ldpq_indexed(vs_front(vs1), coeffs, 0, offsets0);\n+    vs_sqdmulh(vs2, __ T8H, vs1, vq2);\n+    vs_sshr(vs2, __ T8H, vs2, 11);\n+    vs_mlsv(vs1, __ T8H, vs2, vq1);\n+    vs_stpq_indexed(vs_front(vs1), coeffs, 0, offsets0);\n+\n+    \/\/ level 6\n+\n+    __ add(tmpAddr, coeffs, 0);\n+    load64shorts(vs1, tmpAddr);\n+    __ add(tmpAddr, coeffs, 256);\n+    load64shorts(vs2, tmpAddr);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    __ add(tmpAddr, coeffs, 0);\n+    store64shorts(vs3, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 256);\n+    store64shorts(vs2, tmpAddr);\n+\n+    __ add(tmpAddr, coeffs, 128);\n+    load64shorts(vs1, tmpAddr);\n+    __ add(tmpAddr, coeffs, 384);\n+    load64shorts(vs2, tmpAddr);\n+    vs_addv(vs3, __ T8H, vs1, vs2); \/\/ n.b. trashes vq\n+    vs_subv(vs1, __ T8H, vs1, vs2);\n+    __ add(tmpAddr, coeffs, 128);\n+    store64shorts(vs3, tmpAddr);\n+    load64shorts(vs2, zetas);\n+    vs_ldpq(vq, kyberConsts);\n+    kyber_montmul64(vs2, vs1, vs2, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 384);\n+    store64shorts(vs2, tmpAddr);\n+\n+    \/\/ multiply by 2^-n\n+\n+    \/\/ load toMont(2^-n mod q)\n+    __ add(tmpAddr, kyberConsts, 48);\n+    __ ldr(v29, __ Q, tmpAddr);\n+\n+    vs_ldpq(vq, kyberConsts);\n+    __ add(tmpAddr, coeffs, 0);\n+    load64shorts(vs1, tmpAddr);\n+    kyber_montmul64(vs2, vs1, vq3, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 0);\n+    store64shorts(vs2, tmpAddr);\n+\n+    \/\/ now tmpAddr contains coeffs + 128 because store64shorts adjusted it so\n+    load64shorts(vs1, tmpAddr);\n+    kyber_montmul64(vs2, vs1, vq3, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 128);\n+    store64shorts(vs2, tmpAddr);\n+\n+    \/\/ now tmpAddr contains coeffs + 256\n+    load64shorts(vs1, tmpAddr);\n+    kyber_montmul64(vs2, vs1, vq3, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 256);\n+    store64shorts(vs2, tmpAddr);\n+\n+    \/\/ now tmpAddr contains coeffs + 384\n+    load64shorts(vs1, tmpAddr);\n+    kyber_montmul64(vs2, vs1, vq3, vtmp, vq);\n+    __ add(tmpAddr, coeffs, 384);\n+    store64shorts(vs2, tmpAddr);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+  \/\/ Kyber multiply polynomials in the NTT domain.\n+  \/\/ Implements\n+  \/\/ static int implKyberNttMult(\n+  \/\/              short[] result, short[] ntta, short[] nttb, short[] zetas) {}\n+  \/\/\n+  \/\/ result (short[256]) = c_rarg0\n+  \/\/ ntta (short[256]) = c_rarg1\n+  \/\/ nttb (short[256]) = c_rarg2\n+  \/\/ zetas (short[128]) = c_rarg3\n+  address generate_kyberNttMult() {\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyberNttMult_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register result = c_rarg0;\n+    const Register ntta = c_rarg1;\n+    const Register nttb = c_rarg2;\n+    const Register zetas = c_rarg3;\n+\n+    const Register kyberConsts = r10;\n+    const Register limit = r11;\n+\n+    VSeq<4> vs1(0), vs2(4);  \/\/ 4 sets of 8x8H inputs\/outputs\/tmps\n+    VSeq<4> vs3(16), vs4(20);\n+    VSeq<2> vq(30);          \/\/ pair of constants for montmul: q, qinv\n+    VSeq<2> vz(28);          \/\/ pair of zetas\n+    VSeq<4> vc(27, 0);       \/\/ constant sequence for montmul: montRSquareModQ\n+\n+    __ lea(kyberConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+\n+    Label kyberNttMult_loop;\n+\n+    __ add(limit, result, 512);\n+\n+    \/\/ load q and qinv\n+    vs_ldpq(vq, kyberConsts);\n+\n+    \/\/ load R^2 mod q (to convert back from Montgomery representation)\n+    __ add(kyberConsts, kyberConsts, 64);\n+    __ ldr(v27, __ Q, kyberConsts);\n+\n+    __ BIND(kyberNttMult_loop);\n+\n+    \/\/ load 16 zetas\n+    vs_ldpq_post(vz, zetas);\n+\n+    \/\/ load 2 sets of 32 coefficients from the two input arrays\n+    \/\/ interleaved as shorts. i.e. pairs of shorts adjacent in memory\n+    \/\/ are striped across pairs of vector registers\n+    vs_ld2_post(vs_front(vs1), __ T8H, ntta); \/\/ <a0, a1> x 8H\n+    vs_ld2_post(vs_back(vs1), __ T8H, nttb);  \/\/ <b0, b1> x 8H\n+    vs_ld2_post(vs_front(vs4), __ T8H, ntta); \/\/ <a2, a3> x 8H\n+    vs_ld2_post(vs_back(vs4), __ T8H, nttb);  \/\/ <b2, b3> x 8H\n+\n+    \/\/ compute 4 montmul cross-products for pairs (a0,a1) and (b0,b1)\n+    \/\/ i.e. montmul the first and second halves of vs1 in order and\n+    \/\/ then with one sequence reversed storing the two results in vs3\n+    \/\/\n+    \/\/ vs3[0] <- montmul(a0, b0)\n+    \/\/ vs3[1] <- montmul(a1, b1)\n+    \/\/ vs3[2] <- montmul(a0, b1)\n+    \/\/ vs3[3] <- montmul(a1, b0)\n+    kyber_montmul16(vs_front(vs3), vs_front(vs1), vs_back(vs1), vs_front(vs2), vq);\n+    kyber_montmul16(vs_back(vs3),\n+                    vs_front(vs1), vs_reverse(vs_back(vs1)), vs_back(vs2), vq);\n+\n+    \/\/ compute 4 montmul cross-products for pairs (a2,a3) and (b2,b3)\n+    \/\/ i.e. montmul the first and second halves of vs4 in order and\n+    \/\/ then with one sequence reversed storing the two results in vs1\n+    \/\/\n+    \/\/ vs1[0] <- montmul(a2, b2)\n+    \/\/ vs1[1] <- montmul(a3, b3)\n+    \/\/ vs1[2] <- montmul(a2, b3)\n+    \/\/ vs1[3] <- montmul(a3, b2)\n+    kyber_montmul16(vs_front(vs1), vs_front(vs4), vs_back(vs4), vs_front(vs2), vq);\n+    kyber_montmul16(vs_back(vs1),\n+                    vs_front(vs4), vs_reverse(vs_back(vs4)), vs_back(vs2), vq);\n+\n+    \/\/ montmul result 2 of each cross-product i.e. (a1*b1, a3*b3) by a zeta.\n+    \/\/ We can schedule two montmuls at a time if we use a suitable vector\n+    \/\/ sequence <vs3[1], vs1[1]>.\n+    int delta = vs1[1]->encoding() - vs3[1]->encoding();\n+    VSeq<2> vs5(vs3[1], delta);\n+\n+    \/\/ vs3[1] <- montmul(montmul(a1, b1), z0)\n+    \/\/ vs1[1] <- montmul(montmul(a3, b3), z1)\n+    kyber_montmul16(vs5, vz, vs5, vs_front(vs2), vq);\n+\n+    \/\/ add results in pairs storing in vs3\n+    \/\/ vs3[0] <- montmul(a0, b0) + montmul(montmul(a1, b1), z0);\n+    \/\/ vs3[1] <- montmul(a0, b1) + montmul(a1, b0);\n+    vs_addv(vs_front(vs3), __ T8H, vs_even(vs3), vs_odd(vs3));\n+\n+    \/\/ vs3[2] <- montmul(a2, b2) + montmul(montmul(a3, b3), z1);\n+    \/\/ vs3[3] <- montmul(a2, b3) + montmul(a3, b2);\n+    vs_addv(vs_back(vs3), __ T8H, vs_even(vs1), vs_odd(vs1));\n+\n+    \/\/ vs1 <- montmul(vs3, montRSquareModQ)\n+    kyber_montmul32(vs1, vs3, vc, vs2, vq);\n+\n+    \/\/ store back the two pairs of result vectors de-interleaved as 8H elements\n+    \/\/ i.e. storing each pairs of shorts striped across a register pair adjacent\n+    \/\/ in memory\n+    vs_st2_post(vs1, __ T8H, result);\n+\n+    __ cmp(result, limit);\n+    __ br(Assembler::NE, kyberNttMult_loop);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+  \/\/ Kyber add 2 polynomials.\n+  \/\/ Implements\n+  \/\/ static int implKyberAddPoly(short[] result, short[] a, short[] b) {}\n+  \/\/\n+  \/\/ result (short[256]) = c_rarg0\n+  \/\/ a (short[256]) = c_rarg1\n+  \/\/ b (short[256]) = c_rarg2\n+  address generate_kyberAddPoly_2() {\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyberAddPoly_2_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register result = c_rarg0;\n+    const Register a = c_rarg1;\n+    const Register b = c_rarg2;\n+\n+    const Register kyberConsts = r11;\n+\n+    \/\/ We sum 256 sets of values in total i.e. 32 x 8H quadwords.\n+    \/\/ So, we can load, add and store the data in 3 groups of 11,\n+    \/\/ 11 and 10 at a time i.e. we need to map sets of 10 or 11\n+    \/\/ registers. A further constraint is that the mapping needs\n+    \/\/ to skip callee saves. So, we allocate the register\n+    \/\/ sequences using two 8 sequences, two 2 sequences and two\n+    \/\/ single registers.\n+    VSeq<8> vs1_1(0);\n+    VSeq<2> vs1_2(16);\n+    FloatRegister vs1_3 = v28;\n+    VSeq<8> vs2_1(18);\n+    VSeq<2> vs2_2(26);\n+    FloatRegister vs2_3 = v29;\n+\n+    \/\/ two constant vector sequences\n+    VSeq<8> vc_1(31, 0);\n+    VSeq<2> vc_2(31, 0);\n+\n+    FloatRegister vc_3 = v31;\n+    __ lea(kyberConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+\n+    __ ldr(vc_3, __ Q, Address(kyberConsts, 16)); \/\/ q\n+    for (int i = 0; i < 3; i++) {\n+      \/\/ load 80 or 88 values from a into vs1_1\/2\/3\n+      vs_ldpq_post(vs1_1, a);\n+      vs_ldpq_post(vs1_2, a);\n+      if (i < 2) {\n+        __ ldr(vs1_3, __ Q, __ post(a, 16));\n+      }\n+      \/\/ load 80 or 88 values from b into vs2_1\/2\/3\n+      vs_ldpq_post(vs2_1, b);\n+      vs_ldpq_post(vs2_2, b);\n+      if (i < 2) {\n+        __ ldr(vs2_3, __ Q, __ post(b, 16));\n+      }\n+      \/\/ sum 80 or 88 values across vs1 and vs2 into vs1\n+      vs_addv(vs1_1, __ T8H, vs1_1, vs2_1);\n+      vs_addv(vs1_2, __ T8H, vs1_2, vs2_2);\n+      if (i < 2) {\n+        __ addv(vs1_3, __ T8H, vs1_3, vs2_3);\n+      }\n+      \/\/ add constant to all 80 or 88 results\n+      vs_addv(vs1_1, __ T8H, vs1_1, vc_1);\n+      vs_addv(vs1_2, __ T8H, vs1_2, vc_2);\n+      if (i < 2) {\n+        __ addv(vs1_3, __ T8H, vs1_3, vc_3);\n+      }\n+      \/\/ store 80 or 88 values\n+      vs_stpq_post(vs1_1, result);\n+      vs_stpq_post(vs1_2, result);\n+      if (i < 2) {\n+        __ str(vs1_3, __ Q, __ post(result, 16));\n+      }\n+    }\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+  \/\/ Kyber add 3 polynomials.\n+  \/\/ Implements\n+  \/\/ static int implKyberAddPoly(short[] result, short[] a, short[] b, short[] c) {}\n+  \/\/\n+  \/\/ result (short[256]) = c_rarg0\n+  \/\/ a (short[256]) = c_rarg1\n+  \/\/ b (short[256]) = c_rarg2\n+  \/\/ c (short[256]) = c_rarg3\n+  address generate_kyberAddPoly_3() {\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyberAddPoly_3_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register result = c_rarg0;\n+    const Register a = c_rarg1;\n+    const Register b = c_rarg2;\n+    const Register c = c_rarg3;\n+\n+    const Register kyberConsts = r11;\n+\n+    \/\/ As above we sum 256 sets of values in total i.e. 32 x 8H\n+    \/\/ quadwords.  So, we can load, add and store the data in 3\n+    \/\/ groups of 11, 11 and 10 at a time i.e. we need to map sets\n+    \/\/ of 10 or 11 registers. A further constraint is that the\n+    \/\/ mapping needs to skip callee saves. So, we allocate the\n+    \/\/ register sequences using two 8 sequences, two 2 sequences\n+    \/\/ and two single registers.\n+    VSeq<8> vs1_1(0);\n+    VSeq<2> vs1_2(16);\n+    FloatRegister vs1_3 = v28;\n+    VSeq<8> vs2_1(18);\n+    VSeq<2> vs2_2(26);\n+    FloatRegister vs2_3 = v29;\n+\n+    \/\/ two constant vector sequences\n+    VSeq<8> vc_1(31, 0);\n+    VSeq<2> vc_2(31, 0);\n+\n+    FloatRegister vc_3 = v31;\n+\n+    __ lea(kyberConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+\n+    __ ldr(vc_3, __ Q, Address(kyberConsts, 16)); \/\/ q\n+    for (int i = 0; i < 3; i++) {\n+      \/\/ load 80 or 88 values from a into vs1_1\/2\/3\n+      vs_ldpq_post(vs1_1, a);\n+      vs_ldpq_post(vs1_2, a);\n+      if (i < 2) {\n+        __ ldr(vs1_3, __ Q, __ post(a, 16));\n+      }\n+      \/\/ load 80 or 88 values from b into vs2_1\/2\/3\n+      vs_ldpq_post(vs2_1, b);\n+      vs_ldpq_post(vs2_2, b);\n+      if (i < 2) {\n+        __ ldr(vs2_3, __ Q, __ post(b, 16));\n+      }\n+      \/\/ sum 80 or 88 values across vs1 and vs2 into vs1\n+      vs_addv(vs1_1, __ T8H, vs1_1, vs2_1);\n+      vs_addv(vs1_2, __ T8H, vs1_2, vs2_2);\n+      if (i < 2) {\n+        __ addv(vs1_3, __ T8H, vs1_3, vs2_3);\n+      }\n+      \/\/ load 80 or 88 values from c into vs2_1\/2\/3\n+      vs_ldpq_post(vs2_1, c);\n+      vs_ldpq_post(vs2_2, c);\n+      if (i < 2) {\n+        __ ldr(vs2_3, __ Q, __ post(c, 16));\n+      }\n+      \/\/ sum 80 or 88 values across vs1 and vs2 into vs1\n+      vs_addv(vs1_1, __ T8H, vs1_1, vs2_1);\n+      vs_addv(vs1_2, __ T8H, vs1_2, vs2_2);\n+      if (i < 2) {\n+        __ addv(vs1_3, __ T8H, vs1_3, vs2_3);\n+      }\n+      \/\/ add constant to all 80 or 88 results\n+      vs_addv(vs1_1, __ T8H, vs1_1, vc_1);\n+      vs_addv(vs1_2, __ T8H, vs1_2, vc_2);\n+      if (i < 2) {\n+        __ addv(vs1_3, __ T8H, vs1_3, vc_3);\n+      }\n+      \/\/ store 80 or 88 values\n+      vs_stpq_post(vs1_1, result);\n+      vs_stpq_post(vs1_2, result);\n+      if (i < 2) {\n+        __ str(vs1_3, __ Q, __ post(result, 16));\n+      }\n+    }\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n@@ -4930,1 +5970,207 @@\n-  \/\/ perform combined montmul then add\/sub on 4x4S vectors\n+  \/\/ Kyber parse XOF output to polynomial coefficient candidates\n+  \/\/ or decodePoly(12, ...).\n+  \/\/ Implements\n+  \/\/ static int implKyber12To16(\n+  \/\/         byte[] condensed, int index, short[] parsed, int parsedLength) {}\n+  \/\/\n+  \/\/ (parsedLength or (parsedLength - 48) must be divisible by 64.)\n+  \/\/\n+  \/\/ condensed (byte[]) = c_rarg0\n+  \/\/ condensedIndex = c_rarg1\n+  \/\/ parsed (short[112 or 256]) = c_rarg2\n+  \/\/ parsedLength (112 or 256) = c_rarg3\n+  address generate_kyber12To16() {\n+    Label L_F00, L_loop, L_end;\n+\n+    __ BIND(L_F00);\n+    __ emit_int64(0x0f000f000f000f00);\n+    __ emit_int64(0x0f000f000f000f00);\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyber12To16_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register condensed = c_rarg0;\n+    const Register condensedOffs = c_rarg1;\n+    const Register parsed = c_rarg2;\n+    const Register parsedLength = c_rarg3;\n+\n+    const Register tmpAddr = r11;\n+\n+    \/\/ Data is input 96 bytes at a time i.e. in groups of 6 x 16B\n+    \/\/ quadwords so we need a 6 vector sequence for the inputs.\n+    \/\/ Parsing produces 64 shorts, employing two 8 vector\n+    \/\/ sequences to store and combine the intermediate data.\n+    VSeq<6> vin(24);\n+    VSeq<8> va(0), vb(16);\n+\n+    __ adr(tmpAddr, L_F00);\n+    __ ldr(v31, __ Q, tmpAddr); \/\/ 8H times 0x0f00\n+    __ add(condensed, condensed, condensedOffs);\n+\n+    __ BIND(L_loop);\n+    \/\/ load 96 (6 x 16B) byte values\n+    vs_ld3_post(vin, __ T16B, condensed);\n+\n+    \/\/ The front half of sequence vin (vin[0], vin[1] and vin[2])\n+    \/\/ holds 48 (16x3) contiguous bytes from memory striped\n+    \/\/ horizontally across each of the 16 byte lanes. Equivalently,\n+    \/\/ that is 16 pairs of 12-bit integers. Likewise the back half\n+    \/\/ holds the next 48 bytes in the same arrangement.\n+\n+    \/\/ Each vector in the front half can also be viewed as a vertical\n+    \/\/ strip across the 16 pairs of 12 bit integers. Each byte in\n+    \/\/ vin[0] stores the low 8 bits of the first int in a pair. Each\n+    \/\/ byte in vin[1] stores the high 4 bits of the first int and the\n+    \/\/ low 4 bits of the second int. Each byte in vin[2] stores the\n+    \/\/ high 8 bits of the second int. Likewise the vectors in second\n+    \/\/ half.\n+\n+    \/\/ Converting the data to 16-bit shorts requires first of all\n+    \/\/ expanding each of the 6 x 16B vectors into 6 corresponding\n+    \/\/ pairs of 8H vectors. Mask, shift and add operations on the\n+    \/\/ resulting vector pairs can be used to combine 4 and 8 bit\n+    \/\/ parts of related 8H vector elements.\n+    \/\/\n+    \/\/ The middle vectors (vin[2] and vin[5]) are actually expanded\n+    \/\/ twice, one copy manipulated to provide the lower 4 bits\n+    \/\/ belonging to the first short in a pair and another copy\n+    \/\/ manipulated to provide the higher 4 bits belonging to the\n+    \/\/ second short in a pair. This is why the the vector sequences va\n+    \/\/ and vb used to hold the expanded 8H elements are of length 8.\n+\n+    \/\/ Expand vin[0] into va[0:1], and vin[1] into va[2:3] and va[4:5]\n+    \/\/ n.b. target elements 2 and 3 duplicate elements 4 and 5\n+    __ ushll(va[0], __ T8H, vin[0], __ T8B, 0);\n+    __ ushll2(va[1], __ T8H, vin[0], __ T16B, 0);\n+    __ ushll(va[2], __ T8H, vin[1], __ T8B, 0);\n+    __ ushll2(va[3], __ T8H, vin[1], __ T16B, 0);\n+    __ ushll(va[4], __ T8H, vin[1], __ T8B, 0);\n+    __ ushll2(va[5], __ T8H, vin[1], __ T16B, 0);\n+\n+    \/\/ likewise expand vin[3] into vb[0:1], and vin[4] into vb[2:3]\n+    \/\/ and vb[4:5]\n+    __ ushll(vb[0], __ T8H, vin[3], __ T8B, 0);\n+    __ ushll2(vb[1], __ T8H, vin[3], __ T16B, 0);\n+    __ ushll(vb[2], __ T8H, vin[4], __ T8B, 0);\n+    __ ushll2(vb[3], __ T8H, vin[4], __ T16B, 0);\n+    __ ushll(vb[4], __ T8H, vin[4], __ T8B, 0);\n+    __ ushll2(vb[5], __ T8H, vin[4], __ T16B, 0);\n+\n+    \/\/ shift lo byte of copy 1 of the middle stripe into the high byte\n+    __ shl(va[2], __ T8H, va[2], 8);\n+    __ shl(va[3], __ T8H, va[3], 8);\n+    __ shl(vb[2], __ T8H, vb[2], 8);\n+    __ shl(vb[3], __ T8H, vb[3], 8);\n+\n+    \/\/ expand vin[2] into va[6:7] and vin[5] into vb[6:7] but this\n+    \/\/ time pre-shifted by 4 to ensure top bits of input 12-bit int\n+    \/\/ are in bit positions [4..11].\n+    __ ushll(va[6], __ T8H, vin[2], __ T8B, 4);\n+    __ ushll2(va[7], __ T8H, vin[2], __ T16B, 4);\n+    __ ushll(vb[6], __ T8H, vin[5], __ T8B, 4);\n+    __ ushll2(vb[7], __ T8H, vin[5], __ T16B, 4);\n+\n+    \/\/ mask hi 4 bits of the 1st 12-bit int in a pair from copy1 and\n+    \/\/ shift lo 4 bits of the 2nd 12-bit int in a pair to the bottom of\n+    \/\/ copy2\n+    __ andr(va[2], __ T16B, va[2], v31);\n+    __ andr(va[3], __ T16B, va[3], v31);\n+    __ ushr(va[4], __ T8H, va[4], 4);\n+    __ ushr(va[5], __ T8H, va[5], 4);\n+    __ andr(vb[2], __ T16B, vb[2], v31);\n+    __ andr(vb[3], __ T16B, vb[3], v31);\n+    __ ushr(vb[4], __ T8H, vb[4], 4);\n+    __ ushr(vb[5], __ T8H, vb[5], 4);\n+\n+    \/\/ sum hi 4 bits and lo 8 bits of the 1st 12-bit int in each pair and\n+    \/\/ hi 8 bits plus lo 4 bits of the 2nd 12-bit int in each pair\n+    \/\/ n.b. the ordering ensures: i) inputs are consumed before they\n+    \/\/ are overwritten ii) the order of 16-bit results across successive\n+    \/\/ pairs of vectors in va and then vb reflects the order of the\n+    \/\/ corresponding 12-bit inputs\n+    __ addv(va[0], __ T8H, va[0], va[2]);\n+    __ addv(va[2], __ T8H, va[1], va[3]);\n+    __ addv(va[1], __ T8H, va[4], va[6]);\n+    __ addv(va[3], __ T8H, va[5], va[7]);\n+    __ addv(vb[0], __ T8H, vb[0], vb[2]);\n+    __ addv(vb[2], __ T8H, vb[1], vb[3]);\n+    __ addv(vb[1], __ T8H, vb[4], vb[6]);\n+    __ addv(vb[3], __ T8H, vb[5], vb[7]);\n+\n+    \/\/ store 64 results interleaved as shorts\n+    vs_st2_post(vs_front(va), __ T8H, parsed);\n+    vs_st2_post(vs_front(vb), __ T8H, parsed);\n+\n+    __ sub(parsedLength, parsedLength, 64);\n+    __ cmp(parsedLength, (u1)64);\n+    __ br(Assembler::GE, L_loop);\n+    __ cbz(parsedLength, L_end);\n+\n+    \/\/ if anything is left it should be a final 72 bytes of input\n+    \/\/ i.e. a final 48 12-bit values. so we handle this by loading\n+    \/\/ 48 bytes into all 16B lanes of front(vin) and only 24\n+    \/\/ bytes into the lower 8B lane of back(vin)\n+    vs_ld3_post(vs_front(vin), __ T16B, condensed);\n+    vs_ld3(vs_back(vin), __ T8B, condensed);\n+\n+    \/\/ Expand vin[0] into va[0:1], and vin[1] into va[2:3] and va[4:5]\n+    \/\/ n.b. target elements 2 and 3 of va duplicate elements 4 and\n+    \/\/ 5 and target element 2 of vb duplicates element 4.\n+    __ ushll(va[0], __ T8H, vin[0], __ T8B, 0);\n+    __ ushll2(va[1], __ T8H, vin[0], __ T16B, 0);\n+    __ ushll(va[2], __ T8H, vin[1], __ T8B, 0);\n+    __ ushll2(va[3], __ T8H, vin[1], __ T16B, 0);\n+    __ ushll(va[4], __ T8H, vin[1], __ T8B, 0);\n+    __ ushll2(va[5], __ T8H, vin[1], __ T16B, 0);\n+\n+    \/\/ This time expand just the lower 8 lanes\n+    __ ushll(vb[0], __ T8H, vin[3], __ T8B, 0);\n+    __ ushll(vb[2], __ T8H, vin[4], __ T8B, 0);\n+    __ ushll(vb[4], __ T8H, vin[4], __ T8B, 0);\n+\n+    \/\/ shift lo byte of copy 1 of the middle stripe into the high byte\n+    __ shl(va[2], __ T8H, va[2], 8);\n+    __ shl(va[3], __ T8H, va[3], 8);\n+    __ shl(vb[2], __ T8H, vb[2], 8);\n+\n+    \/\/ expand vin[2] into va[6:7] and lower 8 lanes of vin[5] into\n+    \/\/ vb[6] pre-shifted by 4 to ensure top bits of the input 12-bit\n+    \/\/ int are in bit positions [4..11].\n+    __ ushll(va[6], __ T8H, vin[2], __ T8B, 4);\n+    __ ushll2(va[7], __ T8H, vin[2], __ T16B, 4);\n+    __ ushll(vb[6], __ T8H, vin[5], __ T8B, 4);\n+\n+    \/\/ mask hi 4 bits of each 1st 12-bit int in pair from copy1 and\n+    \/\/ shift lo 4 bits of each 2nd 12-bit int in pair to bottom of\n+    \/\/ copy2\n+    __ andr(va[2], __ T16B, va[2], v31);\n+    __ andr(va[3], __ T16B, va[3], v31);\n+    __ ushr(va[4], __ T8H, va[4], 4);\n+    __ ushr(va[5], __ T8H, va[5], 4);\n+    __ andr(vb[2], __ T16B, vb[2], v31);\n+    __ ushr(vb[4], __ T8H, vb[4], 4);\n+\n+\n+\n+    \/\/ sum hi 4 bits and lo 8 bits of each 1st 12-bit int in pair and\n+    \/\/ hi 8 bits plus lo 4 bits of each 2nd 12-bit int in pair\n+\n+    \/\/ n.b. ordering ensures: i) inputs are consumed before they are\n+    \/\/ overwritten ii) order of 16-bit results across succsessive\n+    \/\/ pairs of vectors in va and then lower half of vb reflects order\n+    \/\/ of corresponding 12-bit inputs\n+    __ addv(va[0], __ T8H, va[0], va[2]);\n+    __ addv(va[2], __ T8H, va[1], va[3]);\n+    __ addv(va[1], __ T8H, va[4], va[6]);\n+    __ addv(va[3], __ T8H, va[5], va[7]);\n+    __ addv(vb[0], __ T8H, vb[0], vb[2]);\n+    __ addv(vb[1], __ T8H, vb[4], vb[6]);\n+\n+    \/\/ store 48 results interleaved as shorts\n+    vs_st2_post(vs_front(va), __ T8H, parsed);\n+    vs_st2_post(vs_front(vs_front(vb)), __ T8H, parsed);\n+\n+    __ BIND(L_end);\n@@ -4932,2 +6178,157 @@\n-  void dilithium_montmul16_sub_add(const VSeq<4>& va0, const VSeq<4>& va1, const VSeq<4>& vc,\n-                                   const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+  \/\/ Kyber Barrett reduce function.\n+  \/\/ Implements\n+  \/\/ static int implKyberBarrettReduce(short[] coeffs) {}\n+  \/\/\n+  \/\/ coeffs (short[256]) = c_rarg0\n+  address generate_kyberBarrettReduce() {\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::kyberBarrettReduce_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register coeffs = c_rarg0;\n+\n+    const Register kyberConsts = r10;\n+    const Register result = r11;\n+\n+    \/\/ As above we process 256 sets of values in total i.e. 32 x\n+    \/\/ 8H quadwords. So, we can load, add and store the data in 3\n+    \/\/ groups of 11, 11 and 10 at a time i.e. we need to map sets\n+    \/\/ of 10 or 11 registers. A further constraint is that the\n+    \/\/ mapping needs to skip callee saves. So, we allocate the\n+    \/\/ register sequences using two 8 sequences, two 2 sequences\n+    \/\/ and two single registers.\n+    VSeq<8> vs1_1(0);\n+    VSeq<2> vs1_2(16);\n+    FloatRegister vs1_3 = v28;\n+    VSeq<8> vs2_1(18);\n+    VSeq<2> vs2_2(26);\n+    FloatRegister vs2_3 = v29;\n+\n+    \/\/ we also need a pair of corresponding constant sequences\n+\n+    VSeq<8> vc1_1(30, 0);\n+    VSeq<2> vc1_2(30, 0);\n+    FloatRegister vc1_3 = v30; \/\/ for kyber_q\n+\n+    VSeq<8> vc2_1(31, 0);\n+    VSeq<2> vc2_2(31, 0);\n+    FloatRegister vc2_3 = v31; \/\/ for kyberBarrettMultiplier\n+\n+    __ add(result, coeffs, 0);\n+    __ lea(kyberConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_kyberConsts));\n+\n+    \/\/ load q and the multiplier for the Barrett reduction\n+    __ add(kyberConsts, kyberConsts, 16);\n+    __ ldpq(vc1_3, vc2_3, kyberConsts);\n+\n+    for (int i = 0; i < 3; i++) {\n+      \/\/ load 80 or 88 coefficients\n+      vs_ldpq_post(vs1_1, coeffs);\n+      vs_ldpq_post(vs1_2, coeffs);\n+      if (i < 2) {\n+        __ ldr(vs1_3, __ Q, __ post(coeffs, 16));\n+      }\n+\n+      \/\/ vs2 <- (2 * vs1 * kyberBarrettMultiplier) >> 16\n+      vs_sqdmulh(vs2_1, __ T8H, vs1_1, vc2_1);\n+      vs_sqdmulh(vs2_2, __ T8H, vs1_2, vc2_2);\n+      if (i < 2) {\n+        __ sqdmulh(vs2_3, __ T8H, vs1_3, vc2_3);\n+      }\n+\n+      \/\/ vs2 <- (vs1 * kyberBarrettMultiplier) >> 26\n+      vs_sshr(vs2_1, __ T8H, vs2_1, 11);\n+      vs_sshr(vs2_2, __ T8H, vs2_2, 11);\n+      if (i < 2) {\n+        __ sshr(vs2_3, __ T8H, vs2_3, 11);\n+      }\n+\n+      \/\/ vs1 <- vs1 - vs2 * kyber_q\n+      vs_mlsv(vs1_1, __ T8H, vs2_1, vc1_1);\n+      vs_mlsv(vs1_2, __ T8H, vs2_2, vc1_2);\n+      if (i < 2) {\n+        __ mlsv(vs1_3, __ T8H, vs2_3, vc1_3);\n+      }\n+\n+      vs_stpq_post(vs1_1, result);\n+      vs_stpq_post(vs1_2, result);\n+      if (i < 2) {\n+        __ str(vs1_3, __ Q, __ post(result, 16));\n+      }\n+    }\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+\n+  \/\/ Dilithium-specific montmul helper routines that generate parallel\n+  \/\/ code for, respectively, a single 4x4s vector sequence montmul or\n+  \/\/ two such multiplies in a row.\n+\n+  \/\/ Perform 16 32-bit Montgomery multiplications in parallel\n+  void dilithium_montmul16(const VSeq<4>& va, const VSeq<4>& vb, const VSeq<4>& vc,\n+                           const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    \/\/ Use the helper routine to schedule a 4x4S Montgomery multiply.\n+    \/\/ It will assert that the register use is valid\n+    vs_montmul4(va, vb, vc, __ T4S, vtmp, vq);\n+  }\n+\n+  \/\/ Perform 2x16 32-bit Montgomery multiplications in parallel\n+  void dilithium_montmul32(const VSeq<8>& va, const VSeq<8>& vb, const VSeq<8>& vc,\n+                           const VSeq<4>& vtmp, const VSeq<2>& vq) {\n+    \/\/ Schedule two successive 4x4S multiplies via the montmul helper\n+    \/\/ on the front and back halves of va, vb and vc. The helper will\n+    \/\/ assert that the register use has no overlap conflicts on each\n+    \/\/ individual call but we also need to ensure that the necessary\n+    \/\/ disjoint\/equality constraints are met across both calls.\n+\n+    \/\/ vb, vc, vtmp and vq must be disjoint. va must either be\n+    \/\/ disjoint from all other registers or equal vc\n+\n+    assert(vs_disjoint(vb, vc), \"vb and vc overlap\");\n+    assert(vs_disjoint(vb, vq), \"vb and vq overlap\");\n+    assert(vs_disjoint(vb, vtmp), \"vb and vtmp overlap\");\n+\n+    assert(vs_disjoint(vc, vq), \"vc and vq overlap\");\n+    assert(vs_disjoint(vc, vtmp), \"vc and vtmp overlap\");\n+\n+    assert(vs_disjoint(vq, vtmp), \"vq and vtmp overlap\");\n+\n+    assert(vs_disjoint(va, vc) || vs_same(va, vc), \"va and vc neither disjoint nor equal\");\n+    assert(vs_disjoint(va, vb), \"va and vb overlap\");\n+    assert(vs_disjoint(va, vq), \"va and vq overlap\");\n+    assert(vs_disjoint(va, vtmp), \"va and vtmp overlap\");\n+\n+    \/\/ We multiply the front and back halves of each sequence 4 at a\n+    \/\/ time because\n+    \/\/\n+    \/\/ 1) we are currently only able to get 4-way instruction\n+    \/\/ parallelism at best\n+    \/\/\n+    \/\/ 2) we need registers for the constants in vq and temporary\n+    \/\/ scratch registers to hold intermediate results so vtmp can only\n+    \/\/ be a VSeq<4> which means we only have 4 scratch slots.\n+\n+    vs_montmul4(vs_front(va), vs_front(vb), vs_front(vc), __ T4S, vtmp, vq);\n+    vs_montmul4(vs_back(va), vs_back(vb), vs_back(vc), __ T4S, vtmp, vq);\n+  }\n+\n+  \/\/ Perform combined montmul then add\/sub on 4x4S vectors.\n+  void dilithium_montmul16_sub_add(\n+          const VSeq<4>& va0, const VSeq<4>& va1, const VSeq<4>& vc,\n+          const VSeq<4>& vtmp, const VSeq<2>& vq) {\n@@ -4942,4 +6343,4 @@\n-  \/\/ perform combined add\/sub then montul on 4x4S vectors\n-\n-  void dilithium_sub_add_montmul16(const VSeq<4>& va0, const VSeq<4>& va1, const VSeq<4>& vb,\n-                                   const VSeq<4>& vtmp1, const VSeq<4>& vtmp2, const VSeq<2>& vq) {\n+  \/\/ Perform combined add\/sub then montul on 4x4S vectors.\n+  void dilithium_sub_add_montmul16(\n+          const VSeq<4>& va0, const VSeq<4>& va1, const VSeq<4>& vb,\n+          const VSeq<4>& vtmp1, const VSeq<4>& vtmp2, const VSeq<2>& vq) {\n@@ -4988,1 +6389,1 @@\n-      \/\/ for levels 1 - 4 we simply load 2 x 4 adjacent values at a\n+      \/\/ For levels 1 - 4 we simply load 2 x 4 adjacent values at a\n@@ -4991,1 +6392,1 @@\n-      \/\/ pair instructions with arrangement 4S\n+      \/\/ pair instructions with arrangement 4S.\n@@ -5000,1 +6401,1 @@\n-        vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+        dilithium_montmul32(vs2, vs1, vs2, vtmp, vq);\n@@ -5054,2 +6455,2 @@\n-    int offsets[4] = {0, 32, 64, 96};\n-    int offsets1[8] = {16, 48, 80, 112, 144, 176, 208, 240 };\n+    int offsets[4] = { 0, 32, 64, 96};\n+    int offsets1[8] = { 16, 48, 80, 112, 144, 176, 208, 240 };\n@@ -5058,1 +6459,2 @@\n-    __ lea(dilithiumConsts, ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n+    __ lea(dilithiumConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n@@ -5060,1 +6462,1 @@\n-    \/\/ Each level represents one iteration of the outer for loop of the Java version\n+    \/\/ Each level represents one iteration of the outer for loop of the Java version.\n@@ -5067,1 +6469,1 @@\n-    \/\/ at level 5 the coefficients we need to combine with the zetas\n+    \/\/ At level 5 the coefficients we need to combine with the zetas\n@@ -5081,1 +6483,1 @@\n-      vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+      dilithium_montmul32(vs2, vs1, vs2, vtmp, vq);\n@@ -5093,1 +6495,1 @@\n-    \/\/ at level 6 the coefficients we need to combine with the zetas\n+    \/\/ At level 6 the coefficients we need to combine with the zetas\n@@ -5121,1 +6523,1 @@\n-    \/\/ at level 7 the coefficients we need to combine with the zetas\n+    \/\/ At level 7 the coefficients we need to combine with the zetas\n@@ -5193,1 +6595,1 @@\n-      \/\/ for levels 3 - 7 we simply load 2 x 4 adjacent values at a\n+      \/\/ For levels 3 - 7 we simply load 2 x 4 adjacent values at a\n@@ -5196,1 +6598,1 @@\n-      \/\/ pair instructions with arrangement 4S\n+      \/\/ pair instructions with arrangement 4S.\n@@ -5213,1 +6615,1 @@\n-        vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+        dilithium_montmul32(vs2, vs1, vs2, vtmp, vq);\n@@ -5264,1 +6666,2 @@\n-    __ lea(dilithiumConsts, ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n+    __ lea(dilithiumConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n@@ -5267,1 +6670,0 @@\n-    \/\/ level0\n@@ -5273,1 +6675,1 @@\n-    \/\/ load and store the values using an ld2\/st2 with arrangement 4S\n+    \/\/ load and store the values using an ld2\/st2 with arrangement 4S.\n@@ -5295,1 +6697,1 @@\n-    \/\/ values an ld2\/st2 with arrangement 2D\n+    \/\/ values an ld2\/st2 with arrangement 2D.\n@@ -5331,1 +6733,1 @@\n-      vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+      dilithium_montmul32(vs2, vs1, vs2, vtmp, vq);\n@@ -5344,1 +6746,0 @@\n-\n@@ -5378,1 +6779,2 @@\n-    __ lea(dilithiumConsts, ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n+    __ lea(dilithiumConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n@@ -5395,1 +6797,1 @@\n-    vs_montmul32(vs2, vs1, vs2, vtmp, vq);\n+    dilithium_montmul32(vs2, vs1, vs2, vtmp, vq);\n@@ -5397,1 +6799,1 @@\n-    vs_montmul32(vs2, vrsquare, vs2, vtmp, vq);\n+    dilithium_montmul32(vs2, vrsquare, vs2, vtmp, vq);\n@@ -5410,1 +6812,0 @@\n-\n@@ -5438,1 +6839,1 @@\n-    VSeq<4> vtmp = vs_front(vs3);         \/\/ n.b. tmp registers overlap vs3\n+    VSeq<4> vtmp = vs_front(vs3);      \/\/ n.b. tmp registers overlap vs3\n@@ -5444,1 +6845,2 @@\n-    __ lea(dilithiumConsts, ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n+    __ lea(dilithiumConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n@@ -5458,1 +6860,1 @@\n-    vs_montmul32(vs2, vconst, vs2, vtmp, vq);\n+    dilithium_montmul32(vs2, vconst, vs2, vtmp, vq);\n@@ -5471,1 +6873,0 @@\n-\n@@ -5502,1 +6903,2 @@\n-    VSeq<4> vs1(0), vs2(4), vs3(8); \/\/ 6 independent sets of 4x4s values\n+    \/\/ 6 independent sets of 4x4s values\n+    VSeq<4> vs1(0), vs2(4), vs3(8);\n@@ -5504,1 +6906,3 @@\n-    VSeq<4> one(25, 0);            \/\/ 7 constants for cross-multiplying\n+\n+    \/\/ 7 constants for cross-multiplying\n+    VSeq<4> one(25, 0);\n@@ -5514,1 +6918,2 @@\n-    __ lea(dilithiumConsts, ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n+    __ lea(dilithiumConsts,\n+             ExternalAddress((address) StubRoutines::aarch64::_dilithiumConsts));\n@@ -5611,1 +7016,0 @@\n-\n@@ -5627,1 +7031,0 @@\n-\n@@ -10164,0 +11567,10 @@\n+    if (UseKyberIntrinsics) {\n+      StubRoutines::_kyberNtt = generate_kyberNtt();\n+      StubRoutines::_kyberInverseNtt = generate_kyberInverseNtt();\n+      StubRoutines::_kyberNttMult = generate_kyberNttMult();\n+      StubRoutines::_kyberAddPoly_2 = generate_kyberAddPoly_2();\n+      StubRoutines::_kyberAddPoly_3 = generate_kyberAddPoly_3();\n+      StubRoutines::_kyber12To16 = generate_kyber12To16();\n+      StubRoutines::_kyberBarrettReduce = generate_kyberBarrettReduce();\n+    }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":1487,"deletions":74,"binary":false,"changes":1561,"status":"modified"},{"patch":"@@ -155,1 +155,1 @@\n-    tlab_allocate(noreg, obj, var_size_in_bytes, con_size_in_bytes, t1, t2, slow_case);\n+    tlab_allocate(obj, var_size_in_bytes, con_size_in_bytes, t1, t2, slow_case);\n@@ -345,1 +345,1 @@\n-  if (breakAtEntry || VerifyFPU) {\n+  if (breakAtEntry) {\n@@ -350,1 +350,1 @@\n-    \/\/ Breakpoint and VerifyFPU have one byte first instruction.\n+    \/\/ Breakpoint has one byte first instruction.\n@@ -358,1 +358,0 @@\n-  IA32_ONLY( verify_FPU(0, \"method_entry\"); )\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -337,10 +337,8 @@\n-    if (UseSSE >= 2) {\n-      int xmm_off = xmm_regs_as_doubles_off;\n-      for (int n = 0; n < FrameMap::nof_xmm_regs; n++) {\n-        if (n < xmm_bypass_limit) {\n-          VMReg xmm_name_0 = as_XMMRegister(n)->as_VMReg();\n-          map->set_callee_saved(VMRegImpl::stack2reg(xmm_off + num_rt_args), xmm_name_0);\n-          \/\/ %%% This is really a waste but we'll keep things as they were for now\n-          if (true) {\n-            map->set_callee_saved(VMRegImpl::stack2reg(xmm_off + 1 + num_rt_args), xmm_name_0->next());\n-          }\n+    int xmm_off = xmm_regs_as_doubles_off;\n+    for (int n = 0; n < FrameMap::nof_xmm_regs; n++) {\n+      if (n < xmm_bypass_limit) {\n+        VMReg xmm_name_0 = as_XMMRegister(n)->as_VMReg();\n+        map->set_callee_saved(VMRegImpl::stack2reg(xmm_off + num_rt_args), xmm_name_0);\n+        \/\/ %%% This is really a waste but we'll keep things as they were for now\n+        if (true) {\n+          map->set_callee_saved(VMRegImpl::stack2reg(xmm_off + 1 + num_rt_args), xmm_name_0->next());\n@@ -348,2 +346,1 @@\n-        xmm_off += 2;\n-      assert(xmm_off == float_regs_as_doubles_off, \"incorrect number of xmm registers\");\n+      xmm_off += 2;\n@@ -352,0 +349,1 @@\n+    assert(xmm_off == float_regs_as_doubles_off, \"incorrect number of xmm registers\");\n@@ -372,13 +370,11 @@\n-    if (UseSSE >= 2) {\n-      \/\/ save XMM registers\n-      \/\/ XMM registers can contain float or double values, but this is not known here,\n-      \/\/ so always save them as doubles.\n-      \/\/ note that float values are _not_ converted automatically, so for float values\n-      \/\/ the second word contains only garbage data.\n-      int xmm_bypass_limit = FrameMap::get_num_caller_save_xmms();\n-      int offset = 0;\n-      for (int n = 0; n < xmm_bypass_limit; n++) {\n-        XMMRegister xmm_name = as_XMMRegister(n);\n-        __ movdbl(Address(rsp, xmm_regs_as_doubles_off * VMRegImpl::stack_slot_size + offset), xmm_name);\n-        offset += 8;\n-      }\n+    \/\/ save XMM registers\n+    \/\/ XMM registers can contain float or double values, but this is not known here,\n+    \/\/ so always save them as doubles.\n+    \/\/ note that float values are _not_ converted automatically, so for float values\n+    \/\/ the second word contains only garbage data.\n+    int xmm_bypass_limit = FrameMap::get_num_caller_save_xmms();\n+    int offset = 0;\n+    for (int n = 0; n < xmm_bypass_limit; n++) {\n+      XMMRegister xmm_name = as_XMMRegister(n);\n+      __ movdbl(Address(rsp, xmm_regs_as_doubles_off * VMRegImpl::stack_slot_size + offset), xmm_name);\n+      offset += 8;\n","filename":"src\/hotspot\/cpu\/x86\/c1_Runtime1_x86.cpp","additions":21,"deletions":25,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -1164,1 +1164,0 @@\n-    assert(UseSSE > 0, \"required\");\n@@ -1172,1 +1171,0 @@\n-    assert(UseSSE > 1, \"required\");\n@@ -4140,1 +4138,1 @@\n-    if (UseAVX >= 2 && UseSSE >= 2) {\n+    if (UseAVX >= 2) {\n@@ -4287,1 +4285,1 @@\n-  if (UseAVX >= 2 && UseSSE >= 2) {\n+  if (UseAVX >= 2) {\n@@ -7019,0 +7017,20 @@\n+void C2_MacroAssembler::evfp16ph(int opcode, XMMRegister dst, XMMRegister src1, XMMRegister src2, int vlen_enc) {\n+  switch(opcode) {\n+    case Op_AddVHF: evaddph(dst, src1, src2, vlen_enc); break;\n+    case Op_SubVHF: evsubph(dst, src1, src2, vlen_enc); break;\n+    case Op_MulVHF: evmulph(dst, src1, src2, vlen_enc); break;\n+    case Op_DivVHF: evdivph(dst, src1, src2, vlen_enc); break;\n+    default: assert(false, \"%s\", NodeClassNames[opcode]); break;\n+  }\n+}\n+\n+void C2_MacroAssembler::evfp16ph(int opcode, XMMRegister dst, XMMRegister src1, Address src2, int vlen_enc) {\n+  switch(opcode) {\n+    case Op_AddVHF: evaddph(dst, src1, src2, vlen_enc); break;\n+    case Op_SubVHF: evsubph(dst, src1, src2, vlen_enc); break;\n+    case Op_MulVHF: evmulph(dst, src1, src2, vlen_enc); break;\n+    case Op_DivVHF: evdivph(dst, src1, src2, vlen_enc); break;\n+    default: assert(false, \"%s\", NodeClassNames[opcode]); break;\n+  }\n+}\n+\n@@ -7020,2 +7038,7 @@\n-                                          KRegister ktmp, XMMRegister xtmp1, XMMRegister xtmp2, int vlen_enc) {\n-  if (opcode == Op_MaxHF) {\n+                                            KRegister ktmp, XMMRegister xtmp1, XMMRegister xtmp2) {\n+  vector_max_min_fp16(opcode, dst, src1, src2, ktmp, xtmp1, xtmp2, Assembler::AVX_128bit);\n+}\n+\n+void C2_MacroAssembler::vector_max_min_fp16(int opcode, XMMRegister dst, XMMRegister src1, XMMRegister src2,\n+                                            KRegister ktmp, XMMRegister xtmp1, XMMRegister xtmp2, int vlen_enc) {\n+  if (opcode == Op_MaxVHF || opcode == Op_MaxHF) {\n@@ -7033,1 +7056,1 @@\n-    vmaxsh(dst, xtmp1, xtmp2);\n+    evmaxph(dst, xtmp1, xtmp2, vlen_enc);\n@@ -7035,1 +7058,1 @@\n-    evcmpsh(ktmp, k0, xtmp1, xtmp1, Assembler::UNORD_Q);\n+    evcmpph(ktmp, k0, xtmp1, xtmp1, Assembler::UNORD_Q, vlen_enc);\n@@ -7041,1 +7064,1 @@\n-    assert(opcode == Op_MinHF, \"\");\n+    assert(opcode == Op_MinVHF || opcode == Op_MinHF, \"\");\n@@ -7054,1 +7077,1 @@\n-    vminsh(dst, xtmp1, xtmp2);\n+    evminph(dst, xtmp1, xtmp2, vlen_enc);\n@@ -7056,1 +7079,1 @@\n-    evcmpsh(ktmp, k0, xtmp1, xtmp1, Assembler::UNORD_Q);\n+    evcmpph(ktmp, k0, xtmp1, xtmp1, Assembler::UNORD_Q, vlen_enc);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":34,"deletions":11,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -569,0 +569,7 @@\n+  void evfp16ph(int opcode, XMMRegister dst, XMMRegister src1, XMMRegister src2, int vlen_enc);\n+\n+  void evfp16ph(int opcode, XMMRegister dst, XMMRegister src1, Address src2, int vlen_enc);\n+\n+  void vector_max_min_fp16(int opcode, XMMRegister dst, XMMRegister src1, XMMRegister src2,\n+                          KRegister ktmp, XMMRegister xtmp1, XMMRegister xtmp2, int vlen_enc);\n+\n@@ -570,1 +577,1 @@\n-                           KRegister ktmp, XMMRegister xtmp1, XMMRegister xtmp2, int vlen_enc);\n+                          KRegister ktmp, XMMRegister xtmp1, XMMRegister xtmp2);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -52,5 +52,1 @@\n-    Register thread = NOT_LP64(rax) LP64_ONLY(r15_thread);\n-#ifndef _LP64\n-    __ push(thread);\n-    __ get_thread(thread);\n-#endif\n+    Register thread = r15_thread;\n@@ -68,2 +64,0 @@\n-    NOT_LP64(__ pop(thread);)\n-\n@@ -73,1 +67,0 @@\n-#ifdef _LP64\n@@ -91,4 +84,0 @@\n-#else\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_array_pre_oop_entry),\n-                    addr, count);\n-#endif\n@@ -104,1 +93,0 @@\n-#ifdef _LP64\n@@ -115,4 +103,1 @@\n-#else\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_array_post_entry),\n-                  addr, count);\n-#endif\n+\n@@ -123,1 +108,1 @@\n-                                    Register dst, Address src, Register tmp1, Register tmp_thread) {\n+                                    Register dst, Address src, Register tmp1) {\n@@ -128,1 +113,1 @@\n-  ModRefBarrierSetAssembler::load_at(masm, decorators, type, dst, src, tmp1, tmp_thread);\n+  ModRefBarrierSetAssembler::load_at(masm, decorators, type, dst, src, tmp1);\n@@ -130,19 +115,0 @@\n-    Register thread = NOT_LP64(tmp_thread) LP64_ONLY(r15_thread);\n-\n-#ifndef _LP64\n-    \/\/ Work around the x86_32 bug that only manifests with Loom for some reason.\n-    \/\/ MacroAssembler::resolve_weak_handle calls this barrier with tmp_thread == noreg.\n-    if (thread == noreg) {\n-      if (dst != rcx && tmp1 != rcx) {\n-        thread = rcx;\n-      } else if (dst != rdx && tmp1 != rdx) {\n-        thread = rdx;\n-      } else if (dst != rdi && tmp1 != rdi) {\n-        thread = rdi;\n-      }\n-    }\n-    assert_different_registers(dst, tmp1, thread);\n-    __ push(thread);\n-    __ get_thread(thread);\n-#endif\n-\n@@ -154,1 +120,0 @@\n-                         thread \/* thread *\/,\n@@ -158,4 +123,0 @@\n-\n-#ifndef _LP64\n-    __ pop(thread);\n-#endif\n@@ -202,1 +163,1 @@\n-    __ load_heap_oop(pre_val, Address(obj, 0), noreg, noreg, AS_RAW);\n+    __ load_heap_oop(pre_val, Address(obj, 0), noreg, AS_RAW);\n@@ -218,1 +179,0 @@\n-                                                 Register thread,\n@@ -226,3 +186,1 @@\n-#ifdef _LP64\n-  assert(thread == r15_thread, \"must be\");\n-#endif \/\/ _LP64\n+  const Register thread = r15_thread;\n@@ -279,2 +237,1 @@\n-    LP64_ONLY( assert(pre_val != c_rarg1, \"smashed arg\"); )\n-#ifdef _LP64\n+    assert(pre_val != c_rarg1, \"smashed arg\");\n@@ -287,4 +244,0 @@\n-#else\n-    __ push(thread);\n-    __ push(pre_val);\n-#endif\n@@ -366,1 +319,0 @@\n-                                                  Register thread,\n@@ -369,3 +321,1 @@\n-#ifdef _LP64\n-  assert(thread == r15_thread, \"must be\");\n-#endif \/\/ _LP64\n+  const Register thread = r15_thread;\n@@ -416,1 +366,0 @@\n-#ifdef _LP64\n@@ -428,3 +377,0 @@\n-#else\n-  Unimplemented();\n-#endif \/\/ _LP64\n@@ -436,1 +382,0 @@\n-                                                    Register thread,\n@@ -439,3 +384,2 @@\n-#ifdef _LP64\n-  assert(thread == r15_thread, \"must be\");\n-#endif \/\/ _LP64\n+  const Register thread = r15_thread;\n+\n@@ -477,1 +421,0 @@\n-                                                     Register thread,\n@@ -481,4 +424,1 @@\n-#ifdef _LP64\n-  assert(thread == r15_thread, \"must be\");\n-#endif \/\/ _LP64\n-\n+  const Register thread = r15_thread;\n@@ -523,1 +463,0 @@\n-  Register rthread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n@@ -534,7 +473,0 @@\n-#ifndef _LP64\n-  InterpreterMacroAssembler *imasm = static_cast<InterpreterMacroAssembler*>(masm);\n-#endif\n-\n-  NOT_LP64(__ get_thread(rcx));\n-  NOT_LP64(imasm->save_bcp());\n-\n@@ -545,1 +477,0 @@\n-                         rthread \/* thread *\/,\n@@ -566,1 +497,0 @@\n-                            rthread \/* thread *\/,\n@@ -571,1 +501,0 @@\n-  NOT_LP64(imasm->restore_bcp());\n@@ -631,1 +560,1 @@\n-  const Register thread = NOT_LP64(rax) LP64_ONLY(r15_thread);\n+  const Register thread = r15_thread;\n@@ -634,2 +563,0 @@\n-  NOT_LP64(__ get_thread(thread);)\n-\n@@ -697,1 +624,1 @@\n-  const Register thread = NOT_LP64(rax) LP64_ONLY(r15_thread);\n+  const Register thread = r15_thread;\n@@ -715,2 +642,0 @@\n-  NOT_LP64(__ get_thread(thread);)\n-\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.cpp","additions":13,"deletions":88,"binary":false,"changes":101,"status":"modified"},{"patch":"@@ -47,1 +47,0 @@\n-                            Register thread,\n@@ -55,1 +54,0 @@\n-                             Register thread,\n@@ -70,1 +68,1 @@\n-                       Register dst, Address src, Register tmp1, Register tmp_thread);\n+                       Register dst, Address src, Register tmp1);\n@@ -76,1 +74,0 @@\n-                               Register thread,\n@@ -84,1 +81,0 @@\n-                                Register thread,\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -55,1 +55,1 @@\n-  g1_asm->g1_write_barrier_pre_c2(masm, obj, pre_val, r15_thread, tmp, stub);\n+  g1_asm->g1_write_barrier_pre_c2(masm, obj, pre_val, tmp, stub);\n@@ -74,1 +74,1 @@\n-  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, r15_thread, tmp1, tmp2, stub);\n+  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, tmp1, tmp2, stub);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1_x86_64.ad","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -45,1 +45,1 @@\n-                                  Register dst, Address src, Register tmp1, Register tmp_thread) {\n+                                  Register dst, Address src, Register tmp1) {\n@@ -55,1 +55,0 @@\n-#ifdef _LP64\n@@ -63,3 +62,1 @@\n-      } else\n-#endif\n-      {\n+      } else {\n@@ -82,1 +79,1 @@\n-    __ load_float(src);\n+    __ movflt(xmm0, src);\n@@ -86,1 +83,1 @@\n-    __ load_double(src);\n+    __ movdbl(xmm0, src);\n@@ -90,13 +87,0 @@\n-#ifdef _LP64\n-#else\n-    if (atomic) {\n-      __ fild_d(src);               \/\/ Must load atomically\n-      __ subptr(rsp,2*wordSize);    \/\/ Make space for store\n-      __ fistp_d(Address(rsp,0));\n-      __ pop(rax);\n-      __ pop(rdx);\n-    } else {\n-      __ movl(rax, src);\n-      __ movl(rdx, src.plus_disp(wordSize));\n-    }\n-#endif\n@@ -122,1 +106,0 @@\n-#ifdef _LP64\n@@ -128,4 +111,0 @@\n-#else\n-        __ movl(dst, NULL_WORD);\n-#endif\n-#ifdef _LP64\n@@ -141,3 +120,1 @@\n-        } else\n-#endif\n-        {\n+        } else {\n@@ -172,13 +149,0 @@\n-#ifdef _LP64\n-#else\n-    if (atomic) {\n-      __ push(rdx);\n-      __ push(rax);                 \/\/ Must update atomically with FIST\n-      __ fild_d(Address(rsp,0));    \/\/ So load into FPU register\n-      __ fistp_d(dst);              \/\/ and put into memory atomically\n-      __ addptr(rsp, 2*wordSize);\n-    } else {\n-      __ movptr(dst, rax);\n-      __ movptr(dst.plus_disp(wordSize), rdx);\n-    }\n-#endif\n@@ -189,1 +153,1 @@\n-    __ store_float(dst);\n+    __ movflt(dst, xmm0);\n@@ -193,1 +157,1 @@\n-    __ store_double(dst);\n+    __ movdbl(dst, xmm0);\n@@ -234,4 +198,0 @@\n-#ifdef _LP64\n-#else\n-    fatal(\"No support for 8 bytes copy\");\n-#endif\n@@ -243,1 +203,0 @@\n-#ifdef _LP64\n@@ -247,1 +206,0 @@\n-#endif\n@@ -257,1 +215,0 @@\n-#ifdef _LP64\n@@ -261,1 +218,0 @@\n-#endif\n@@ -274,4 +230,0 @@\n-#ifdef _LP64\n-#else\n-    fatal(\"No support for 8 bytes copy\");\n-#endif\n@@ -329,1 +281,1 @@\n-                                        Register thread, Register obj,\n+                                        Register obj,\n@@ -338,9 +290,2 @@\n-  if (!thread->is_valid()) {\n-#ifdef _LP64\n-    thread = r15_thread;\n-#else\n-    assert(t1->is_valid(), \"need temp reg\");\n-    thread = t1;\n-    __ get_thread(thread);\n-#endif\n-  }\n+\n+  const Register thread = r15_thread;\n@@ -369,1 +314,0 @@\n-#ifdef _LP64\n@@ -393,17 +337,0 @@\n-#else\n-void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm, Label*, Label*) {\n-  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n-  Label continuation;\n-\n-  Register tmp = rdi;\n-  __ push(tmp);\n-  __ movptr(tmp, (intptr_t)bs_nm->disarmed_guard_value_address());\n-  Address disarmed_addr(tmp, 0);\n-  __ align(4);\n-  __ cmpl_imm32(disarmed_addr, 0);\n-  __ pop(tmp);\n-  __ jcc(Assembler::equal, continuation);\n-  __ call(RuntimeAddress(StubRoutines::method_entry_barrier()));\n-  __ bind(continuation);\n-}\n-#endif\n@@ -416,6 +343,2 @@\n-  Register tmp1 = LP64_ONLY( rscratch1 ) NOT_LP64( rax );\n-  Register tmp2 = LP64_ONLY( rscratch2 ) NOT_LP64( rcx );\n-#ifndef _LP64\n-  __ push(tmp1);\n-  __ push(tmp2);\n-#endif \/\/ !_LP64\n+  Register tmp1 = rscratch1;\n+  Register tmp2 = rscratch2;\n@@ -437,5 +360,0 @@\n-#ifndef _LP64\n-  __ pop(tmp2);\n-  __ pop(tmp1);\n-#endif\n-\n@@ -445,5 +363,0 @@\n-\n-#ifndef _LP64\n-  __ pop(tmp2);\n-  __ pop(tmp1);\n-#endif\n@@ -469,2 +382,0 @@\n-#ifdef _LP64\n-\n@@ -746,8 +657,0 @@\n-#else \/\/ !_LP64\n-\n-OptoReg::Name BarrierSetAssembler::refine_register(const Node* node, OptoReg::Name opto_reg) {\n-  Unimplemented(); \/\/ This must be implemented to support late barrier expansion.\n-}\n-\n-#endif \/\/ _LP64\n-\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetAssembler_x86.cpp","additions":12,"deletions":109,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-                       Register dst, Address src, Register tmp1, Register tmp_thread);\n+                       Register dst, Address src, Register tmp1);\n@@ -99,1 +99,1 @@\n-                             Register thread, Register obj,\n+                             Register obj,\n@@ -120,2 +120,0 @@\n-#ifdef _LP64\n-\n@@ -166,2 +164,0 @@\n-#endif \/\/ _LP64\n-\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetAssembler_x86.hpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -42,1 +42,0 @@\n-#ifdef _LP64\n@@ -50,8 +49,0 @@\n-#else\n-  enum Intel_specific_constants {\n-    instruction_code        = 0x81,\n-    instruction_size        = 7,\n-    imm_offset              = 2,\n-    instruction_modrm       = 0x3f  \/\/ [rdi]\n-  };\n-#endif\n@@ -73,1 +64,0 @@\n-#ifdef _LP64\n@@ -100,23 +90,0 @@\n-#else\n-bool NativeNMethodCmpBarrier::check_barrier(err_msg& msg) const {\n-  if (((uintptr_t) instruction_address()) & 0x3) {\n-    msg.print(\"Addr: \" INTPTR_FORMAT \" not properly aligned\", p2i(instruction_address()));\n-    return false;\n-  }\n-\n-  int inst = ubyte_at(0);\n-  if (inst != instruction_code) {\n-    msg.print(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x\", p2i(instruction_address()),\n-        inst);\n-    return false;\n-  }\n-\n-  int modrm = ubyte_at(1);\n-  if (modrm != instruction_modrm) {\n-    msg.print(\"Addr: \" INTPTR_FORMAT \" mod\/rm: 0x%x\", p2i(instruction_address()),\n-        modrm);\n-    return false;\n-  }\n-  return true;\n-}\n-#endif \/\/ _LP64\n@@ -172,1 +139,0 @@\n-#ifdef _LP64\n@@ -178,3 +144,0 @@\n-#else\n-  return -18;\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetNMethod_x86.cpp","additions":0,"deletions":37,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -60,1 +60,0 @@\n-#ifdef _LP64\n@@ -73,11 +72,0 @@\n-#else\n-  __ lea(end,  Address(addr, count, Address::times_ptr, -wordSize));\n-  __ shrptr(addr, CardTable::card_shift());\n-  __ shrptr(end,   CardTable::card_shift());\n-  __ subptr(end, addr); \/\/ end --> count\n-__ BIND(L_loop);\n-  Address cardtable(addr, count, Address::times_1, disp);\n-  __ movb(cardtable, 0);\n-  __ decrement(count);\n-  __ jcc(Assembler::greaterEqual, L_loop);\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/cardTableBarrierSetAssembler_x86.cpp","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -422,2 +422,2 @@\n-    case ftos: load_float(val_addr);                break;\n-    case dtos: load_double(val_addr);               break;\n+    case ftos: movflt(xmm0, val_addr);              break;\n+    case dtos: movdbl(xmm0, val_addr);              break;\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -103,389 +103,0 @@\n-\/\/ First all the versions that have distinct versions depending on 32\/64 bit\n-\/\/ Unless the difference is trivial (1 line or so).\n-\n-#ifndef _LP64\n-\n-\/\/ 32bit versions\n-\n-Address MacroAssembler::as_Address(AddressLiteral adr) {\n-  return Address(adr.target(), adr.rspec());\n-}\n-\n-Address MacroAssembler::as_Address(ArrayAddress adr, Register rscratch) {\n-  assert(rscratch == noreg, \"\");\n-  return Address::make_array(adr);\n-}\n-\n-void MacroAssembler::call_VM_leaf_base(address entry_point,\n-                                       int number_of_arguments) {\n-  call(RuntimeAddress(entry_point));\n-  increment(rsp, number_of_arguments * wordSize);\n-}\n-\n-void MacroAssembler::cmpklass(Address src1, Metadata* obj) {\n-  cmp_literal32(src1, (int32_t)obj, metadata_Relocation::spec_for_immediate());\n-}\n-\n-\n-void MacroAssembler::cmpklass(Register src1, Metadata* obj) {\n-  cmp_literal32(src1, (int32_t)obj, metadata_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::cmpoop(Address src1, jobject obj) {\n-  cmp_literal32(src1, (int32_t)obj, oop_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::cmpoop(Register src1, jobject obj, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  cmp_literal32(src1, (int32_t)obj, oop_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::extend_sign(Register hi, Register lo) {\n-  \/\/ According to Intel Doc. AP-526, \"Integer Divide\", p.18.\n-  if (VM_Version::is_P6() && hi == rdx && lo == rax) {\n-    cdql();\n-  } else {\n-    movl(hi, lo);\n-    sarl(hi, 31);\n-  }\n-}\n-\n-void MacroAssembler::jC2(Register tmp, Label& L) {\n-  \/\/ set parity bit if FPU flag C2 is set (via rax)\n-  save_rax(tmp);\n-  fwait(); fnstsw_ax();\n-  sahf();\n-  restore_rax(tmp);\n-  \/\/ branch\n-  jcc(Assembler::parity, L);\n-}\n-\n-void MacroAssembler::jnC2(Register tmp, Label& L) {\n-  \/\/ set parity bit if FPU flag C2 is set (via rax)\n-  save_rax(tmp);\n-  fwait(); fnstsw_ax();\n-  sahf();\n-  restore_rax(tmp);\n-  \/\/ branch\n-  jcc(Assembler::noParity, L);\n-}\n-\n-\/\/ 32bit can do a case table jump in one instruction but we no longer allow the base\n-\/\/ to be installed in the Address class\n-void MacroAssembler::jump(ArrayAddress entry, Register rscratch) {\n-  assert(rscratch == noreg, \"not needed\");\n-  jmp(as_Address(entry, noreg));\n-}\n-\n-\/\/ Note: y_lo will be destroyed\n-void MacroAssembler::lcmp2int(Register x_hi, Register x_lo, Register y_hi, Register y_lo) {\n-  \/\/ Long compare for Java (semantics as described in JVM spec.)\n-  Label high, low, done;\n-\n-  cmpl(x_hi, y_hi);\n-  jcc(Assembler::less, low);\n-  jcc(Assembler::greater, high);\n-  \/\/ x_hi is the return register\n-  xorl(x_hi, x_hi);\n-  cmpl(x_lo, y_lo);\n-  jcc(Assembler::below, low);\n-  jcc(Assembler::equal, done);\n-\n-  bind(high);\n-  xorl(x_hi, x_hi);\n-  increment(x_hi);\n-  jmp(done);\n-\n-  bind(low);\n-  xorl(x_hi, x_hi);\n-  decrementl(x_hi);\n-\n-  bind(done);\n-}\n-\n-void MacroAssembler::lea(Register dst, AddressLiteral src) {\n-  mov_literal32(dst, (int32_t)src.target(), src.rspec());\n-}\n-\n-void MacroAssembler::lea(Address dst, AddressLiteral adr, Register rscratch) {\n-  assert(rscratch == noreg, \"not needed\");\n-\n-  \/\/ leal(dst, as_Address(adr));\n-  \/\/ see note in movl as to why we must use a move\n-  mov_literal32(dst, (int32_t)adr.target(), adr.rspec());\n-}\n-\n-void MacroAssembler::leave() {\n-  mov(rsp, rbp);\n-  pop(rbp);\n-}\n-\n-void MacroAssembler::lmul(int x_rsp_offset, int y_rsp_offset) {\n-  \/\/ Multiplication of two Java long values stored on the stack\n-  \/\/ as illustrated below. Result is in rdx:rax.\n-  \/\/\n-  \/\/ rsp ---> [  ??  ] \\               \\\n-  \/\/            ....    | y_rsp_offset  |\n-  \/\/          [ y_lo ] \/  (in bytes)    | x_rsp_offset\n-  \/\/          [ y_hi ]                  | (in bytes)\n-  \/\/            ....                    |\n-  \/\/          [ x_lo ]                 \/\n-  \/\/          [ x_hi ]\n-  \/\/            ....\n-  \/\/\n-  \/\/ Basic idea: lo(result) = lo(x_lo * y_lo)\n-  \/\/             hi(result) = hi(x_lo * y_lo) + lo(x_hi * y_lo) + lo(x_lo * y_hi)\n-  Address x_hi(rsp, x_rsp_offset + wordSize); Address x_lo(rsp, x_rsp_offset);\n-  Address y_hi(rsp, y_rsp_offset + wordSize); Address y_lo(rsp, y_rsp_offset);\n-  Label quick;\n-  \/\/ load x_hi, y_hi and check if quick\n-  \/\/ multiplication is possible\n-  movl(rbx, x_hi);\n-  movl(rcx, y_hi);\n-  movl(rax, rbx);\n-  orl(rbx, rcx);                                 \/\/ rbx, = 0 <=> x_hi = 0 and y_hi = 0\n-  jcc(Assembler::zero, quick);                   \/\/ if rbx, = 0 do quick multiply\n-  \/\/ do full multiplication\n-  \/\/ 1st step\n-  mull(y_lo);                                    \/\/ x_hi * y_lo\n-  movl(rbx, rax);                                \/\/ save lo(x_hi * y_lo) in rbx,\n-  \/\/ 2nd step\n-  movl(rax, x_lo);\n-  mull(rcx);                                     \/\/ x_lo * y_hi\n-  addl(rbx, rax);                                \/\/ add lo(x_lo * y_hi) to rbx,\n-  \/\/ 3rd step\n-  bind(quick);                                   \/\/ note: rbx, = 0 if quick multiply!\n-  movl(rax, x_lo);\n-  mull(y_lo);                                    \/\/ x_lo * y_lo\n-  addl(rdx, rbx);                                \/\/ correct hi(x_lo * y_lo)\n-}\n-\n-void MacroAssembler::lneg(Register hi, Register lo) {\n-  negl(lo);\n-  adcl(hi, 0);\n-  negl(hi);\n-}\n-\n-void MacroAssembler::lshl(Register hi, Register lo) {\n-  \/\/ Java shift left long support (semantics as described in JVM spec., p.305)\n-  \/\/ (basic idea for shift counts s >= n: x << s == (x << n) << (s - n))\n-  \/\/ shift value is in rcx !\n-  assert(hi != rcx, \"must not use rcx\");\n-  assert(lo != rcx, \"must not use rcx\");\n-  const Register s = rcx;                        \/\/ shift count\n-  const int      n = BitsPerWord;\n-  Label L;\n-  andl(s, 0x3f);                                 \/\/ s := s & 0x3f (s < 0x40)\n-  cmpl(s, n);                                    \/\/ if (s < n)\n-  jcc(Assembler::less, L);                       \/\/ else (s >= n)\n-  movl(hi, lo);                                  \/\/ x := x << n\n-  xorl(lo, lo);\n-  \/\/ Note: subl(s, n) is not needed since the Intel shift instructions work rcx mod n!\n-  bind(L);                                       \/\/ s (mod n) < n\n-  shldl(hi, lo);                                 \/\/ x := x << s\n-  shll(lo);\n-}\n-\n-\n-void MacroAssembler::lshr(Register hi, Register lo, bool sign_extension) {\n-  \/\/ Java shift right long support (semantics as described in JVM spec., p.306 & p.310)\n-  \/\/ (basic idea for shift counts s >= n: x >> s == (x >> n) >> (s - n))\n-  assert(hi != rcx, \"must not use rcx\");\n-  assert(lo != rcx, \"must not use rcx\");\n-  const Register s = rcx;                        \/\/ shift count\n-  const int      n = BitsPerWord;\n-  Label L;\n-  andl(s, 0x3f);                                 \/\/ s := s & 0x3f (s < 0x40)\n-  cmpl(s, n);                                    \/\/ if (s < n)\n-  jcc(Assembler::less, L);                       \/\/ else (s >= n)\n-  movl(lo, hi);                                  \/\/ x := x >> n\n-  if (sign_extension) sarl(hi, 31);\n-  else                xorl(hi, hi);\n-  \/\/ Note: subl(s, n) is not needed since the Intel shift instructions work rcx mod n!\n-  bind(L);                                       \/\/ s (mod n) < n\n-  shrdl(lo, hi);                                 \/\/ x := x >> s\n-  if (sign_extension) sarl(hi);\n-  else                shrl(hi);\n-}\n-\n-void MacroAssembler::movoop(Register dst, jobject obj) {\n-  mov_literal32(dst, (int32_t)obj, oop_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::movoop(Address dst, jobject obj, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  mov_literal32(dst, (int32_t)obj, oop_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::mov_metadata(Register dst, Metadata* obj) {\n-  mov_literal32(dst, (int32_t)obj, metadata_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::mov_metadata(Address dst, Metadata* obj, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  mov_literal32(dst, (int32_t)obj, metadata_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::movptr(Register dst, AddressLiteral src) {\n-  if (src.is_lval()) {\n-    mov_literal32(dst, (intptr_t)src.target(), src.rspec());\n-  } else {\n-    movl(dst, as_Address(src));\n-  }\n-}\n-\n-void MacroAssembler::movptr(ArrayAddress dst, Register src, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  movl(as_Address(dst, noreg), src);\n-}\n-\n-void MacroAssembler::movptr(Register dst, ArrayAddress src) {\n-  movl(dst, as_Address(src, noreg));\n-}\n-\n-void MacroAssembler::movptr(Address dst, intptr_t src, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  movl(dst, src);\n-}\n-\n-void MacroAssembler::pushoop(jobject obj, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  push_literal32((int32_t)obj, oop_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::pushklass(Metadata* obj, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  push_literal32((int32_t)obj, metadata_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::pushptr(AddressLiteral src, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  if (src.is_lval()) {\n-    push_literal32((int32_t)src.target(), src.rspec());\n-  } else {\n-    pushl(as_Address(src));\n-  }\n-}\n-\n-static void pass_arg0(MacroAssembler* masm, Register arg) {\n-  masm->push(arg);\n-}\n-\n-static void pass_arg1(MacroAssembler* masm, Register arg) {\n-  masm->push(arg);\n-}\n-\n-static void pass_arg2(MacroAssembler* masm, Register arg) {\n-  masm->push(arg);\n-}\n-\n-static void pass_arg3(MacroAssembler* masm, Register arg) {\n-  masm->push(arg);\n-}\n-\n-#ifndef PRODUCT\n-extern \"C\" void findpc(intptr_t x);\n-#endif\n-\n-void MacroAssembler::debug32(int rdi, int rsi, int rbp, int rsp, int rbx, int rdx, int rcx, int rax, int eip, char* msg) {\n-  \/\/ In order to get locks to work, we need to fake a in_VM state\n-  JavaThread* thread = JavaThread::current();\n-  JavaThreadState saved_state = thread->thread_state();\n-  thread->set_thread_state(_thread_in_vm);\n-  if (ShowMessageBoxOnError) {\n-    JavaThread* thread = JavaThread::current();\n-    JavaThreadState saved_state = thread->thread_state();\n-    thread->set_thread_state(_thread_in_vm);\n-    if (CountBytecodes || TraceBytecodes || StopInterpreterAt) {\n-      ttyLocker ttyl;\n-      BytecodeCounter::print();\n-    }\n-    \/\/ To see where a verify_oop failed, get $ebx+40\/X for this frame.\n-    \/\/ This is the value of eip which points to where verify_oop will return.\n-    if (os::message_box(msg, \"Execution stopped, print registers?\")) {\n-      print_state32(rdi, rsi, rbp, rsp, rbx, rdx, rcx, rax, eip);\n-      BREAKPOINT;\n-    }\n-  }\n-  fatal(\"DEBUG MESSAGE: %s\", msg);\n-}\n-\n-void MacroAssembler::print_state32(int rdi, int rsi, int rbp, int rsp, int rbx, int rdx, int rcx, int rax, int eip) {\n-  ttyLocker ttyl;\n-  DebuggingContext debugging{};\n-  tty->print_cr(\"eip = 0x%08x\", eip);\n-#ifndef PRODUCT\n-  if ((WizardMode || Verbose) && PrintMiscellaneous) {\n-    tty->cr();\n-    findpc(eip);\n-    tty->cr();\n-  }\n-#endif\n-#define PRINT_REG(rax) \\\n-  { tty->print(\"%s = \", #rax); os::print_location(tty, rax); }\n-  PRINT_REG(rax);\n-  PRINT_REG(rbx);\n-  PRINT_REG(rcx);\n-  PRINT_REG(rdx);\n-  PRINT_REG(rdi);\n-  PRINT_REG(rsi);\n-  PRINT_REG(rbp);\n-  PRINT_REG(rsp);\n-#undef PRINT_REG\n-  \/\/ Print some words near top of staack.\n-  int* dump_sp = (int*) rsp;\n-  for (int col1 = 0; col1 < 8; col1++) {\n-    tty->print(\"(rsp+0x%03x) 0x%08x: \", (int)((intptr_t)dump_sp - (intptr_t)rsp), (intptr_t)dump_sp);\n-    os::print_location(tty, *dump_sp++);\n-  }\n-  for (int row = 0; row < 16; row++) {\n-    tty->print(\"(rsp+0x%03x) 0x%08x: \", (int)((intptr_t)dump_sp - (intptr_t)rsp), (intptr_t)dump_sp);\n-    for (int col = 0; col < 8; col++) {\n-      tty->print(\" 0x%08x\", *dump_sp++);\n-    }\n-    tty->cr();\n-  }\n-  \/\/ Print some instructions around pc:\n-  Disassembler::decode((address)eip-64, (address)eip);\n-  tty->print_cr(\"--------\");\n-  Disassembler::decode((address)eip, (address)eip+32);\n-}\n-\n-void MacroAssembler::stop(const char* msg) {\n-  \/\/ push address of message\n-  ExternalAddress message((address)msg);\n-  pushptr(message.addr(), noreg);\n-  { Label L; call(L, relocInfo::none); bind(L); }     \/\/ push eip\n-  pusha();                                            \/\/ push registers\n-  call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug32)));\n-  hlt();\n-}\n-\n-void MacroAssembler::warn(const char* msg) {\n-  push_CPU_state();\n-\n-  \/\/ push address of message\n-  ExternalAddress message((address)msg);\n-  pushptr(message.addr(), noreg);\n-\n-  call(RuntimeAddress(CAST_FROM_FN_PTR(address, warning)));\n-  addl(rsp, wordSize);       \/\/ discard argument\n-  pop_CPU_state();\n-}\n-\n-void MacroAssembler::print_state() {\n-  { Label L; call(L, relocInfo::none); bind(L); }     \/\/ push eip\n-  pusha();                                            \/\/ push registers\n-\n-  push_CPU_state();\n-  call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::print_state32)));\n-  pop_CPU_state();\n-\n-  popa();\n-  addl(rsp, wordSize);\n-}\n-\n-#else \/\/ _LP64\n-\n-\/\/ 64 bit versions\n-\n@@ -1103,5 +714,1 @@\n-#endif \/\/ _LP64\n-\n-\/\/ Now versions that are common to 32\/64 bit\n-\n-  LP64_ONLY(addq(dst, imm32)) NOT_LP64(addl(dst, imm32));\n+  addq(dst, imm32);\n@@ -1112,1 +719,1 @@\n-  LP64_ONLY(addq(dst, src)) NOT_LP64(addl(dst, src));\n+  addq(dst, src);\n@@ -1116,1 +723,1 @@\n-  LP64_ONLY(addq(dst, src)) NOT_LP64(addl(dst, src));\n+  addq(dst, src);\n@@ -1226,1 +833,1 @@\n-  LP64_ONLY(andq(dst, imm32)) NOT_LP64(andl(dst, imm32));\n+  andq(dst, imm32);\n@@ -1229,1 +836,0 @@\n-#ifdef _LP64\n@@ -1240,1 +846,0 @@\n-#endif\n@@ -1258,1 +863,0 @@\n-#ifdef _LP64\n@@ -1274,1 +878,0 @@\n-#endif\n@@ -1306,3 +909,1 @@\n-  Register thread = NOT_LP64(rsi) LP64_ONLY(r15_thread);\n-  NOT_LP64(get_thread(rsi);)\n-  cmpptr(rsp, Address(thread, JavaThread::reserved_stack_activation_offset()));\n+  cmpptr(rsp, Address(r15_thread, JavaThread::reserved_stack_activation_offset()));\n@@ -1312,1 +913,1 @@\n-  call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone), thread);\n+  call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone), r15_thread);\n@@ -1350,1 +951,0 @@\n-#ifdef _LP64\n@@ -1353,3 +953,0 @@\n-#else\n-  movptr(rax, (intptr_t)Universe::non_oop_word());\n-#endif\n@@ -1360,2 +957,1 @@\n-  return\n-      LP64_ONLY(UseCompactObjectHeaders ? 17 : 14) NOT_LP64(12);\n+  return UseCompactObjectHeaders ? 17 : 14;\n@@ -1365,1 +961,1 @@\n-  Register receiver = LP64_ONLY(j_rarg0) NOT_LP64(rcx);\n+  Register receiver = j_rarg0;\n@@ -1367,1 +963,1 @@\n-  Register temp = LP64_ONLY(rscratch1) NOT_LP64(rbx);\n+  Register temp = rscratch1;\n@@ -1377,1 +973,0 @@\n-#ifdef _LP64\n@@ -1381,3 +976,1 @@\n-  } else\n-#endif\n-  if (UseCompressedClassPointers) {\n+  } else if (UseCompressedClassPointers) {\n@@ -1448,1 +1041,1 @@\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -1470,2 +1063,2 @@\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_2, c_rarg3));\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n@@ -1505,1 +1098,1 @@\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -1518,2 +1111,2 @@\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_2, c_rarg3));\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n@@ -1550,1 +1143,1 @@\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -1563,2 +1156,2 @@\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_2, c_rarg3));\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n@@ -1675,1 +1268,1 @@\n-  LP64_ONLY(assert_different_registers(arg_0, c_rarg1));\n+  assert_different_registers(arg_0, c_rarg1);\n@@ -1682,2 +1275,2 @@\n-  LP64_ONLY(assert_different_registers(arg_0, c_rarg1, c_rarg2));\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n+  assert_different_registers(arg_0, c_rarg1, c_rarg2);\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -1691,3 +1284,3 @@\n-  LP64_ONLY(assert_different_registers(arg_0, c_rarg1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_2, c_rarg3));\n+  assert_different_registers(arg_0, c_rarg1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n@@ -1711,1 +1304,1 @@\n-  LP64_ONLY(assert_different_registers(arg_0, c_rarg1));\n+  assert_different_registers(arg_0, c_rarg1);\n@@ -1718,2 +1311,2 @@\n-  LP64_ONLY(assert_different_registers(arg_0, c_rarg1, c_rarg2));\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n+  assert_different_registers(arg_0, c_rarg1, c_rarg2);\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -1727,3 +1320,3 @@\n-  LP64_ONLY(assert_different_registers(arg_0, c_rarg1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_2, c_rarg3));\n+  assert_different_registers(arg_0, c_rarg1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n@@ -1842,1 +1435,0 @@\n-#ifdef _LP64\n@@ -1854,8 +1446,0 @@\n-#else\n-  assert(rscratch == noreg, \"not needed\");\n-  if (src2.is_lval()) {\n-    cmp_literal32(src1, (int32_t)src2.target(), src2.rspec());\n-  } else {\n-    cmpl(src1, as_Address(src2));\n-  }\n-#endif \/\/ _LP64\n@@ -1866,1 +1450,0 @@\n-#ifdef _LP64\n@@ -1870,4 +1453,0 @@\n-#else\n-  assert(rscratch == noreg, \"not needed\");\n-  cmp_literal32(src1, (int32_t)src2.target(), src2.rspec());\n-#endif \/\/ _LP64\n@@ -1884,1 +1463,0 @@\n-#ifdef _LP64\n@@ -1889,1 +1467,0 @@\n-#endif\n@@ -1905,1 +1482,1 @@\n-  LP64_ONLY(cmpxchgq(reg, adr)) NOT_LP64(cmpxchgl(reg, adr));\n+  cmpxchgq(reg, adr);\n@@ -2068,109 +1645,0 @@\n-#ifndef _LP64\n-void MacroAssembler::fcmp(Register tmp) {\n-  fcmp(tmp, 1, true, true);\n-}\n-\n-void MacroAssembler::fcmp(Register tmp, int index, bool pop_left, bool pop_right) {\n-  assert(!pop_right || pop_left, \"usage error\");\n-  if (VM_Version::supports_cmov()) {\n-    assert(tmp == noreg, \"unneeded temp\");\n-    if (pop_left) {\n-      fucomip(index);\n-    } else {\n-      fucomi(index);\n-    }\n-    if (pop_right) {\n-      fpop();\n-    }\n-  } else {\n-    assert(tmp != noreg, \"need temp\");\n-    if (pop_left) {\n-      if (pop_right) {\n-        fcompp();\n-      } else {\n-        fcomp(index);\n-      }\n-    } else {\n-      fcom(index);\n-    }\n-    \/\/ convert FPU condition into eflags condition via rax,\n-    save_rax(tmp);\n-    fwait(); fnstsw_ax();\n-    sahf();\n-    restore_rax(tmp);\n-  }\n-  \/\/ condition codes set as follows:\n-  \/\/\n-  \/\/ CF (corresponds to C0) if x < y\n-  \/\/ PF (corresponds to C2) if unordered\n-  \/\/ ZF (corresponds to C3) if x = y\n-}\n-\n-void MacroAssembler::fcmp2int(Register dst, bool unordered_is_less) {\n-  fcmp2int(dst, unordered_is_less, 1, true, true);\n-}\n-\n-void MacroAssembler::fcmp2int(Register dst, bool unordered_is_less, int index, bool pop_left, bool pop_right) {\n-  fcmp(VM_Version::supports_cmov() ? noreg : dst, index, pop_left, pop_right);\n-  Label L;\n-  if (unordered_is_less) {\n-    movl(dst, -1);\n-    jcc(Assembler::parity, L);\n-    jcc(Assembler::below , L);\n-    movl(dst, 0);\n-    jcc(Assembler::equal , L);\n-    increment(dst);\n-  } else { \/\/ unordered is greater\n-    movl(dst, 1);\n-    jcc(Assembler::parity, L);\n-    jcc(Assembler::above , L);\n-    movl(dst, 0);\n-    jcc(Assembler::equal , L);\n-    decrementl(dst);\n-  }\n-  bind(L);\n-}\n-\n-void MacroAssembler::fld_d(AddressLiteral src) {\n-  fld_d(as_Address(src));\n-}\n-\n-void MacroAssembler::fld_s(AddressLiteral src) {\n-  fld_s(as_Address(src));\n-}\n-\n-void MacroAssembler::fldcw(AddressLiteral src) {\n-  fldcw(as_Address(src));\n-}\n-\n-void MacroAssembler::fpop() {\n-  ffree();\n-  fincstp();\n-}\n-\n-void MacroAssembler::fremr(Register tmp) {\n-  save_rax(tmp);\n-  { Label L;\n-    bind(L);\n-    fprem();\n-    fwait(); fnstsw_ax();\n-    sahf();\n-    jcc(Assembler::parity, L);\n-  }\n-  restore_rax(tmp);\n-  \/\/ Result is in ST0.\n-  \/\/ Note: fxch & fpop to get rid of ST1\n-  \/\/ (otherwise FPU stack could overflow eventually)\n-  fxch(1);\n-  fpop();\n-}\n-\n-void MacroAssembler::empty_FPU_stack() {\n-  if (VM_Version::supports_mmx()) {\n-    emms();\n-  } else {\n-    for (int i = 8; i-- > 0; ) ffree(i);\n-  }\n-}\n-#endif \/\/ !LP64\n-\n@@ -2187,48 +1655,0 @@\n-void MacroAssembler::load_float(Address src) {\n-#ifdef _LP64\n-  movflt(xmm0, src);\n-#else\n-  if (UseSSE >= 1) {\n-    movflt(xmm0, src);\n-  } else {\n-    fld_s(src);\n-  }\n-#endif \/\/ LP64\n-}\n-\n-void MacroAssembler::store_float(Address dst) {\n-#ifdef _LP64\n-  movflt(dst, xmm0);\n-#else\n-  if (UseSSE >= 1) {\n-    movflt(dst, xmm0);\n-  } else {\n-    fstp_s(dst);\n-  }\n-#endif \/\/ LP64\n-}\n-\n-void MacroAssembler::load_double(Address src) {\n-#ifdef _LP64\n-  movdbl(xmm0, src);\n-#else\n-  if (UseSSE >= 2) {\n-    movdbl(xmm0, src);\n-  } else {\n-    fld_d(src);\n-  }\n-#endif \/\/ LP64\n-}\n-\n-void MacroAssembler::store_double(Address dst) {\n-#ifdef _LP64\n-  movdbl(dst, xmm0);\n-#else\n-  if (UseSSE >= 2) {\n-    movdbl(dst, xmm0);\n-  } else {\n-    fstp_d(dst);\n-  }\n-#endif \/\/ LP64\n-}\n-\n@@ -2384,9 +1804,2 @@\n-  int off;\n-  if (LP64_ONLY(true ||) VM_Version::is_P6()) {\n-    off = offset();\n-    movsbl(dst, src); \/\/ movsxb\n-  } else {\n-    off = load_unsigned_byte(dst, src);\n-    shll(dst, 24);\n-    sarl(dst, 24);\n-  }\n+  int off = offset();\n+  movsbl(dst, src); \/\/ movsxb\n@@ -2401,12 +1814,5 @@\n-  int off;\n-  if (LP64_ONLY(true ||) VM_Version::is_P6()) {\n-    \/\/ This is dubious to me since it seems safe to do a signed 16 => 64 bit\n-    \/\/ version but this is what 64bit has always done. This seems to imply\n-    \/\/ that users are only using 32bits worth.\n-    off = offset();\n-    movswl(dst, src); \/\/ movsxw\n-  } else {\n-    off = load_unsigned_short(dst, src);\n-    shll(dst, 16);\n-    sarl(dst, 16);\n-  }\n+  \/\/ This is dubious to me since it seems safe to do a signed 16 => 64 bit\n+  \/\/ version but this is what 64bit has always done. This seems to imply\n+  \/\/ that users are only using 32bits worth.\n+  int off = offset();\n+  movswl(dst, src); \/\/ movsxw\n@@ -2419,9 +1825,2 @@\n-  int off;\n-  if (LP64_ONLY(true || ) VM_Version::is_P6() || src.uses(dst)) {\n-    off = offset();\n-    movzbl(dst, src); \/\/ movzxb\n-  } else {\n-    xorl(dst, dst);\n-    off = offset();\n-    movb(dst, src);\n-  }\n+  int off = offset();\n+  movzbl(dst, src); \/\/ movzxb\n@@ -2435,9 +1834,2 @@\n-  int off;\n-  if (LP64_ONLY(true ||) VM_Version::is_P6() || src.uses(dst)) {\n-    off = offset();\n-    movzwl(dst, src); \/\/ movzxw\n-  } else {\n-    xorl(dst, dst);\n-    off = offset();\n-    movw(dst, src);\n-  }\n+  int off = offset();\n+  movzwl(dst, src); \/\/ movzxw\n@@ -2449,8 +1841,0 @@\n-#ifndef _LP64\n-  case  8:\n-    assert(dst2 != noreg, \"second dest register required\");\n-    movl(dst,  src);\n-    movl(dst2, src.plus_disp(BytesPerInt));\n-    break;\n-#else\n-#endif\n@@ -2467,8 +1851,0 @@\n-#ifndef _LP64\n-  case  8:\n-    assert(src2 != noreg, \"second source register required\");\n-    movl(dst,                        src);\n-    movl(dst.plus_disp(BytesPerInt), src2);\n-    break;\n-#else\n-#endif\n@@ -2594,1 +1970,1 @@\n-  LP64_ONLY(movq(dst, src)) NOT_LP64(movl(dst, src));\n+  movq(dst, src);\n@@ -2598,1 +1974,1 @@\n-  LP64_ONLY(movq(dst, src)) NOT_LP64(movl(dst, src));\n+  movq(dst, src);\n@@ -2603,1 +1979,0 @@\n-#ifdef _LP64\n@@ -2611,3 +1986,0 @@\n-#else\n-  movl(dst, src);\n-#endif\n@@ -2617,1 +1989,1 @@\n-  LP64_ONLY(movq(dst, src)) NOT_LP64(movl(dst, src));\n+  movq(dst, src);\n@@ -2621,1 +1993,1 @@\n-  LP64_ONLY(movslq(dst, src)) NOT_LP64(movl(dst, src));\n+  movslq(dst, src);\n@@ -3110,2 +2482,0 @@\n-#ifdef _LP64\n-#endif\n@@ -3120,4 +2490,0 @@\n-#ifndef _LP64\n-  frstor(Address(rsp, 0));\n-#else\n-#endif\n@@ -3130,1 +2496,1 @@\n-  LP64_ONLY(addq(rsp, 8));\n+  addq(rsp, 8);\n@@ -3143,5 +2509,0 @@\n-#ifndef _LP64\n-  fnsave(Address(rsp, 0));\n-  fwait();\n-#else\n-#endif \/\/ LP64\n@@ -3155,1 +2516,1 @@\n-  LP64_ONLY(subq(rsp, 8));\n+  subq(rsp, 8);\n@@ -3162,27 +2523,5 @@\n-#ifndef _LP64\n-  Register rthread = rax;\n-  Register rrealsp = rbx;\n-  push(rthread);\n-  push(rrealsp);\n-\n-  get_thread(rthread);\n-\n-  \/\/ The code below wants the original RSP.\n-  \/\/ Move it back after the pushes above.\n-  movptr(rrealsp, rsp);\n-  addptr(rrealsp, 2*wordSize);\n-#else\n-  Register rthread = r15_thread;\n-  Register rrealsp = rsp;\n-#endif\n-\n-  Label done;\n-  cmpptr(rrealsp, Address(rthread, JavaThread::cont_fastpath_offset()));\n-  jccb(Assembler::belowEqual, done);\n-  movptr(Address(rthread, JavaThread::cont_fastpath_offset()), rrealsp);\n-  bind(done);\n-\n-#ifndef _LP64\n-  pop(rrealsp);\n-  pop(rthread);\n-#endif\n+  Label L_done;\n+  cmpptr(rsp, Address(r15_thread, JavaThread::cont_fastpath_offset()));\n+  jccb(Assembler::belowEqual, L_done);\n+  movptr(Address(r15_thread, JavaThread::cont_fastpath_offset()), rsp);\n+  bind(L_done);\n@@ -3194,27 +2533,5 @@\n-#ifndef _LP64\n-  Register rthread = rax;\n-  Register rrealsp = rbx;\n-  push(rthread);\n-  push(rrealsp);\n-\n-  get_thread(rthread);\n-\n-  \/\/ The code below wants the original RSP.\n-  \/\/ Move it back after the pushes above.\n-  movptr(rrealsp, rsp);\n-  addptr(rrealsp, 2*wordSize);\n-#else\n-  Register rthread = r15_thread;\n-  Register rrealsp = rsp;\n-#endif\n-\n-  Label done;\n-  cmpptr(rrealsp, Address(rthread, JavaThread::cont_fastpath_offset()));\n-  jccb(Assembler::below, done);\n-  movptr(Address(rthread, JavaThread::cont_fastpath_offset()), 0);\n-  bind(done);\n-\n-#ifndef _LP64\n-  pop(rrealsp);\n-  pop(rthread);\n-#endif\n+  Label L_done;\n+  cmpptr(rsp, Address(r15_thread, JavaThread::cont_fastpath_offset()));\n+  jccb(Assembler::below, L_done);\n+  movptr(Address(r15_thread, JavaThread::cont_fastpath_offset()), 0);\n+  bind(L_done);\n@@ -3224,2 +2541,0 @@\n-#ifdef _LP64\n-#endif\n@@ -3230,2 +2545,0 @@\n-#ifdef _LP64\n-#endif\n@@ -3237,1 +2550,0 @@\n-#ifdef _LP64\n@@ -3244,3 +2556,0 @@\n-#else\n-  Unimplemented();\n-#endif\n@@ -3263,5 +2572,0 @@\n-void MacroAssembler::restore_rax(Register tmp) {\n-  if (tmp == noreg) pop(rax);\n-  else if (tmp != rax) mov(rax, tmp);\n-}\n-\n@@ -3273,5 +2577,0 @@\n-void MacroAssembler::save_rax(Register tmp) {\n-  if (tmp == noreg) push(rax);\n-  else if (tmp != rax) mov(tmp, rax);\n-}\n-\n@@ -3327,1 +2626,1 @@\n-  LP64_ONLY(shlq(dst, imm8)) NOT_LP64(shll(dst, imm8));\n+  shlq(dst, imm8);\n@@ -3331,1 +2630,1 @@\n-  LP64_ONLY(shrq(dst, imm8)) NOT_LP64(shrl(dst, imm8));\n+  shrq(dst, imm8);\n@@ -3335,6 +2634,1 @@\n-  if (LP64_ONLY(true ||) (VM_Version::is_P6() && reg->has_byte_register())) {\n-    movsbl(reg, reg); \/\/ movsxb\n-  } else {\n-    shll(reg, 24);\n-    sarl(reg, 24);\n-  }\n+  movsbl(reg, reg); \/\/ movsxb\n@@ -3344,6 +2638,1 @@\n-  if (LP64_ONLY(true ||) VM_Version::is_P6()) {\n-    movswl(reg, reg); \/\/ movsxw\n-  } else {\n-    shll(reg, 16);\n-    sarl(reg, 16);\n-  }\n+  movswl(reg, reg); \/\/ movsxw\n@@ -3373,2 +2662,0 @@\n-#ifdef _LP64\n-\n@@ -3391,2 +2678,0 @@\n-#endif\n-\n@@ -4189,1 +3474,1 @@\n-  access_load_at(T_OBJECT, IN_NATIVE | AS_RAW, value, Address(value, 0), tmp, thread);\n+  access_load_at(T_OBJECT, IN_NATIVE | AS_RAW, value, Address(value, 0), tmp);\n@@ -4198,1 +3483,1 @@\n-  access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, -JNIHandles::TypeTag::global), tmp, thread);\n+  access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, -JNIHandles::TypeTag::global), tmp);\n@@ -4205,1 +3490,1 @@\n-                 value, Address(value, -JNIHandles::TypeTag::weak_global), tmp, thread);\n+                 value, Address(value, -JNIHandles::TypeTag::weak_global), tmp);\n@@ -4231,1 +3516,1 @@\n-  access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, -JNIHandles::TypeTag::global), tmp, thread);\n+  access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, -JNIHandles::TypeTag::global), tmp);\n@@ -4238,1 +3523,1 @@\n-  LP64_ONLY(subq(dst, imm32)) NOT_LP64(subl(dst, imm32));\n+  subq(dst, imm32);\n@@ -4243,1 +3528,1 @@\n-  LP64_ONLY(subq_imm32(dst, imm32)) NOT_LP64(subl_imm32(dst, imm32));\n+  subq_imm32(dst, imm32);\n@@ -4247,1 +3532,1 @@\n-  LP64_ONLY(subq(dst, src)) NOT_LP64(subl(dst, src));\n+  subq(dst, src);\n@@ -4265,1 +3550,1 @@\n-  LP64_ONLY(testq(dst, src)) NOT_LP64(testl(dst, src));\n+  testq(dst, src);\n@@ -4301,2 +3586,0 @@\n-  const Register thread = r15_thread;\n-\n@@ -4304,1 +3587,1 @@\n-    tlab_allocate(thread, new_obj, layout_size, 0, klass, t2, slow_case);\n+    tlab_allocate(new_obj, layout_size, 0, klass, t2, slow_case);\n@@ -4389,1 +3672,1 @@\n-void MacroAssembler::tlab_allocate(Register thread, Register obj,\n+void MacroAssembler::tlab_allocate(Register obj,\n@@ -4396,1 +3679,1 @@\n-  bs->tlab_allocate(this, thread, obj, var_size_in_bytes, con_size_in_bytes, t1, t2, slow_case);\n+  bs->tlab_allocate(this, obj, var_size_in_bytes, con_size_in_bytes, t1, t2, slow_case);\n@@ -4401,1 +3684,0 @@\n-#ifdef _LP64\n@@ -4407,4 +3689,0 @@\n-#else\n-  regs += RegSet::of(rax, rcx, rdx);\n-#endif\n-#ifdef _LP64\n@@ -4414,1 +3692,0 @@\n-#endif\n@@ -4431,8 +3708,1 @@\n-static int FPUSaveAreaSize = align_up(108, StackAlignmentInBytes); \/\/ 108 bytes needed for FPU state by fsave\/frstor\n-\n-#ifndef _LP64\n-static bool use_x87_registers() { return UseSSE < 2; }\n-#endif\n-static bool use_xmm_registers() { return UseSSE >= 1; }\n-\n-static int xmm_save_size() { return UseSSE >= 2 ? sizeof(double) : sizeof(float); }\n+static int xmm_save_size() { return sizeof(double); }\n@@ -4442,5 +3712,1 @@\n-  if (UseSSE == 1) {\n-    masm->movflt(Address(rsp, offset), reg);\n-  } else {\n-    masm->movdbl(Address(rsp, offset), reg);\n-  }\n+  masm->movdbl(Address(rsp, offset), reg);\n@@ -4450,5 +3716,1 @@\n-  if (UseSSE == 1) {\n-    masm->movflt(reg, Address(rsp, offset));\n-  } else {\n-    masm->movdbl(reg, Address(rsp, offset));\n-  }\n+  masm->movdbl(reg, Address(rsp, offset));\n@@ -4458,2 +3720,1 @@\n-                                  bool save_fpu, int& gp_area_size,\n-                                  int& fp_area_size, int& xmm_area_size) {\n+                                  bool save_fpu, int& gp_area_size, int& xmm_area_size) {\n@@ -4463,6 +3724,1 @@\n-#ifdef _LP64\n-  fp_area_size = 0;\n-#else\n-  fp_area_size = (save_fpu && use_x87_registers()) ? FPUSaveAreaSize : 0;\n-#endif\n-  xmm_area_size = (save_fpu && use_xmm_registers()) ? xmm_registers.size() * xmm_save_size() : 0;\n+  xmm_area_size = save_fpu ? xmm_registers.size() * xmm_save_size() : 0;\n@@ -4470,1 +3726,1 @@\n-  return gp_area_size + fp_area_size + xmm_area_size;\n+  return gp_area_size + xmm_area_size;\n@@ -4479,1 +3735,0 @@\n-  int fp_area_size;\n@@ -4482,1 +3737,1 @@\n-                                               gp_area_size, fp_area_size, xmm_area_size);\n+                                               gp_area_size, xmm_area_size);\n@@ -4487,8 +3742,2 @@\n-#ifndef _LP64\n-  if (save_fpu && use_x87_registers()) {\n-    fnsave(Address(rsp, gp_area_size));\n-    fwait();\n-  }\n-#endif\n-  if (save_fpu && use_xmm_registers()) {\n-    push_set(call_clobbered_xmm_registers(), gp_area_size + fp_area_size);\n+  if (save_fpu) {\n+    push_set(call_clobbered_xmm_registers(), gp_area_size);\n@@ -4506,1 +3755,0 @@\n-  int fp_area_size;\n@@ -4509,1 +3757,1 @@\n-                                               gp_area_size, fp_area_size, xmm_area_size);\n+                                               gp_area_size, xmm_area_size);\n@@ -4511,6 +3759,2 @@\n-  if (restore_fpu && use_xmm_registers()) {\n-    pop_set(call_clobbered_xmm_registers(), gp_area_size + fp_area_size);\n-  }\n-#ifndef _LP64\n-  if (restore_fpu && use_x87_registers()) {\n-    frstor(Address(rsp, gp_area_size));\n+  if (restore_fpu) {\n+    pop_set(call_clobbered_xmm_registers(), gp_area_size);\n@@ -4518,1 +3762,0 @@\n-#endif\n@@ -4618,15 +3861,1 @@\n-#ifndef _LP64\n-  \/\/ index could have not been a multiple of 8 (i.e., bit 2 was set)\n-  {\n-    Label even;\n-    \/\/ note: if index was a multiple of 8, then it cannot\n-    \/\/       be 0 now otherwise it must have been 0 before\n-    \/\/       => if it is even, we don't need to check for 0 again\n-    jcc(Assembler::carryClear, even);\n-    \/\/ clear topmost word (no jump would be needed if conditional assignment worked here)\n-    movptr(Address(address, index, Address::times_8, offset_in_bytes - 0*BytesPerWord), temp);\n-    \/\/ index could be 0 now, must check again\n-    jcc(Assembler::zero, done);\n-    bind(even);\n-  }\n-#endif \/\/ !_LP64\n+\n@@ -4638,1 +3867,0 @@\n-    NOT_LP64(movptr(Address(address, index, Address::times_8, offset_in_bytes - 2*BytesPerWord), temp);)\n@@ -5042,3 +4270,2 @@\n-  NOT_LP64(  incrementl(pst_counter_addr) );\n-  LP64_ONLY( lea(rcx, pst_counter_addr) );\n-  LP64_ONLY( incrementl(Address(rcx, 0)) );\n+  lea(rcx, pst_counter_addr);\n+  incrementl(Address(rcx, 0));\n@@ -5090,16 +4317,0 @@\n-#ifndef _LP64\n-\n-\/\/ 32-bit x86 only: always use the linear search.\n-void MacroAssembler::check_klass_subtype_slow_path(Register sub_klass,\n-                                                   Register super_klass,\n-                                                   Register temp_reg,\n-                                                   Register temp2_reg,\n-                                                   Label* L_success,\n-                                                   Label* L_failure,\n-                                                   bool set_cond_codes) {\n-  check_klass_subtype_slow_path_linear\n-    (sub_klass, super_klass, temp_reg, temp2_reg, L_success, L_failure, set_cond_codes);\n-}\n-\n-#else \/\/ _LP64\n-\n@@ -5689,2 +4900,0 @@\n-#endif \/\/ LP64\n-\n@@ -5749,2 +4958,0 @@\n-#ifdef _LP64\n-#endif\n@@ -5813,2 +5020,0 @@\n-#ifdef _LP64\n-#endif\n@@ -5823,1 +5028,1 @@\n-    pushptr(Address(rax, LP64_ONLY(2 *) BytesPerWord));\n+    pushptr(Address(rax, 2 * BytesPerWord));\n@@ -5850,1 +5055,0 @@\n-    Register thread_reg = NOT_LP64(rbx) LP64_ONLY(r15_thread);\n@@ -5853,4 +5057,2 @@\n-    NOT_LP64(push(thread_reg));\n-    NOT_LP64(get_thread(thread_reg));\n-    movptr(t1, Address(thread_reg, in_bytes(JavaThread::tlab_top_offset())));\n-    cmpptr(t1, Address(thread_reg, in_bytes(JavaThread::tlab_start_offset())));\n+    movptr(t1, Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())));\n+    cmpptr(t1, Address(r15_thread, in_bytes(JavaThread::tlab_start_offset())));\n@@ -5863,2 +5065,2 @@\n-    movptr(t1, Address(thread_reg, in_bytes(JavaThread::tlab_end_offset())));\n-    cmpptr(t1, Address(thread_reg, in_bytes(JavaThread::tlab_top_offset())));\n+    movptr(t1, Address(r15_thread, in_bytes(JavaThread::tlab_end_offset())));\n+    cmpptr(t1, Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())));\n@@ -5870,1 +5072,0 @@\n-    NOT_LP64(pop(thread_reg));\n@@ -6150,79 +5351,0 @@\n-\n-#ifndef _LP64\n-static bool _verify_FPU(int stack_depth, char* s, CPU_State* state) {\n-  static int counter = 0;\n-  FPU_State* fs = &state->_fpu_state;\n-  counter++;\n-  \/\/ For leaf calls, only verify that the top few elements remain empty.\n-  \/\/ We only need 1 empty at the top for C2 code.\n-  if( stack_depth < 0 ) {\n-    if( fs->tag_for_st(7) != 3 ) {\n-      printf(\"FPR7 not empty\\n\");\n-      state->print();\n-      assert(false, \"error\");\n-      return false;\n-    }\n-    return true;                \/\/ All other stack states do not matter\n-  }\n-\n-  assert((fs->_control_word._value & 0xffff) == StubRoutines::x86::fpu_cntrl_wrd_std(),\n-         \"bad FPU control word\");\n-\n-  \/\/ compute stack depth\n-  int i = 0;\n-  while (i < FPU_State::number_of_registers && fs->tag_for_st(i)  < 3) i++;\n-  int d = i;\n-  while (i < FPU_State::number_of_registers && fs->tag_for_st(i) == 3) i++;\n-  \/\/ verify findings\n-  if (i != FPU_State::number_of_registers) {\n-    \/\/ stack not contiguous\n-    printf(\"%s: stack not contiguous at ST%d\\n\", s, i);\n-    state->print();\n-    assert(false, \"error\");\n-    return false;\n-  }\n-  \/\/ check if computed stack depth corresponds to expected stack depth\n-  if (stack_depth < 0) {\n-    \/\/ expected stack depth is -stack_depth or less\n-    if (d > -stack_depth) {\n-      \/\/ too many elements on the stack\n-      printf(\"%s: <= %d stack elements expected but found %d\\n\", s, -stack_depth, d);\n-      state->print();\n-      assert(false, \"error\");\n-      return false;\n-    }\n-  } else {\n-    \/\/ expected stack depth is stack_depth\n-    if (d != stack_depth) {\n-      \/\/ wrong stack depth\n-      printf(\"%s: %d stack elements expected but found %d\\n\", s, stack_depth, d);\n-      state->print();\n-      assert(false, \"error\");\n-      return false;\n-    }\n-  }\n-  \/\/ everything is cool\n-  return true;\n-}\n-\n-void MacroAssembler::verify_FPU(int stack_depth, const char* s) {\n-  if (!VerifyFPU) return;\n-  push_CPU_state();\n-  push(rsp);                \/\/ pass CPU state\n-  ExternalAddress msg((address) s);\n-  \/\/ pass message string s\n-  pushptr(msg.addr(), noreg);\n-  push(stack_depth);        \/\/ pass stack depth\n-  call(RuntimeAddress(CAST_FROM_FN_PTR(address, _verify_FPU)));\n-  addptr(rsp, 3 * wordSize);   \/\/ discard arguments\n-  \/\/ check for error\n-  { Label L;\n-    testl(rax, rax);\n-    jcc(Assembler::notZero, L);\n-    int3();                  \/\/ break if error condition\n-    bind(L);\n-  }\n-  pop_CPU_state();\n-}\n-#endif \/\/ _LP64\n-\n@@ -6241,8 +5363,0 @@\n-\n-#ifndef _LP64\n-  \/\/ Either restore the x87 floating pointer control word after returning\n-  \/\/ from the JNI call or verify that it wasn't changed.\n-  if (CheckJNICalls) {\n-    call(RuntimeAddress(StubRoutines::x86::verify_fpu_cntrl_wrd_entry()));\n-  }\n-#endif \/\/ _LP64\n@@ -6259,1 +5373,1 @@\n-                 result, Address(result, 0), tmp, \/*tmp_thread*\/noreg);\n+                 result, Address(result, 0), tmp);\n@@ -6275,1 +5389,1 @@\n-                 rresult, Address(rresult, 0), rtmp, \/*tmp_thread*\/noreg);\n+                 rresult, Address(rresult, 0), rtmp);\n@@ -6299,1 +5413,0 @@\n-#ifdef _LP64\n@@ -6304,3 +5417,1 @@\n-  } else\n-#endif\n-  {\n+  } else {\n@@ -6311,1 +5422,0 @@\n-#ifdef _LP64\n@@ -6317,1 +5427,0 @@\n-#endif\n@@ -6322,1 +5431,1 @@\n-#ifdef _LP64\n+\n@@ -6329,3 +5438,1 @@\n-  } else\n-#endif\n-  {\n+  } else {\n@@ -6345,1 +5452,0 @@\n-#ifdef _LP64\n@@ -6349,2 +5455,1 @@\n-  } else\n-#endif\n+  } else {\n@@ -6352,0 +5457,1 @@\n+  }\n@@ -6355,1 +5461,0 @@\n-#ifdef _LP64\n@@ -6363,3 +5468,1 @@\n-  } else\n-#endif\n-  {\n+  } else {\n@@ -6371,1 +5474,0 @@\n-#ifdef _LP64\n@@ -6381,3 +5483,1 @@\n-  } else\n-#endif\n-  {\n+  } else {\n@@ -6390,1 +5490,1 @@\n-                                    Register tmp1, Register thread_tmp) {\n+                                    Register tmp1) {\n@@ -6395,1 +5495,1 @@\n-    bs->BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1);\n@@ -6397,1 +5497,1 @@\n-    bs->load_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->load_at(this, decorators, type, dst, src, tmp1);\n@@ -6453,3 +5553,2 @@\n-void MacroAssembler::load_heap_oop(Register dst, Address src, Register tmp1,\n-                                   Register thread_tmp, DecoratorSet decorators) {\n-  access_load_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);\n+void MacroAssembler::load_heap_oop(Register dst, Address src, Register tmp1, DecoratorSet decorators) {\n+  access_load_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1);\n@@ -6459,3 +5558,2 @@\n-void MacroAssembler::load_heap_oop_not_null(Register dst, Address src, Register tmp1,\n-                                            Register thread_tmp, DecoratorSet decorators) {\n-  access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1, thread_tmp);\n+void MacroAssembler::load_heap_oop_not_null(Register dst, Address src, Register tmp1, DecoratorSet decorators) {\n+  access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1);\n@@ -6474,1 +5572,0 @@\n-#ifdef _LP64\n@@ -6796,2 +5893,0 @@\n-#endif \/\/ _LP64\n-\n@@ -6893,1 +5988,1 @@\n-      tlab_allocate(r15_thread, rax, noreg, lh, r13, r14, slow_case);\n+      tlab_allocate(rax, noreg, lh, r13, r14, slow_case);\n@@ -6905,1 +6000,1 @@\n-      tlab_allocate(r15_thread, rax, r14, 0, r13, r14, slow_case);\n+      tlab_allocate(rax, r14, 0, r13, r14, slow_case);\n@@ -7382,2 +6477,0 @@\n-    NOT_LP64(shlptr(cnt, 1);) \/\/ convert to number of 32-bit words for 32-bit VM\n-\n@@ -7404,1 +6497,0 @@\n-    NOT_LP64(shlptr(cnt, 1);) \/\/ convert to number of 32-bit words for 32-bit VM\n@@ -7422,1 +6514,1 @@\n-#if defined(COMPILER2) && defined(_LP64)\n+#if defined(COMPILER2)\n@@ -7483,33 +6575,1 @@\n-  if (UseSSE < 2) {\n-    Label L_fill_32_bytes_loop, L_check_fill_8_bytes, L_fill_8_bytes_loop, L_fill_8_bytes;\n-    \/\/ Fill 32-byte chunks\n-    subptr(count, 8 << shift);\n-    jcc(Assembler::less, L_check_fill_8_bytes);\n-    align(16);\n-\n-    BIND(L_fill_32_bytes_loop);\n-\n-    for (int i = 0; i < 32; i += 4) {\n-      movl(Address(to, i), value);\n-    }\n-\n-    addptr(to, 32);\n-    subptr(count, 8 << shift);\n-    jcc(Assembler::greaterEqual, L_fill_32_bytes_loop);\n-    BIND(L_check_fill_8_bytes);\n-    addptr(count, 8 << shift);\n-    jccb(Assembler::zero, L_exit);\n-    jmpb(L_fill_8_bytes);\n-\n-    \/\/\n-    \/\/ length is too short, just fill qwords\n-    \/\/\n-    BIND(L_fill_8_bytes_loop);\n-    movl(Address(to, 0), value);\n-    movl(Address(to, 4), value);\n-    addptr(to, 8);\n-    BIND(L_fill_8_bytes);\n-    subptr(count, 1 << (shift + 1));\n-    jcc(Assembler::greaterEqual, L_fill_8_bytes_loop);\n-    \/\/ fall through to fill 4 bytes\n-  } else {\n+  {\n@@ -7527,1 +6587,0 @@\n-      assert( UseSSE >= 2, \"supported cpu only\" );\n@@ -7835,1 +6894,0 @@\n-#ifdef _LP64\n@@ -9010,1 +8068,0 @@\n-#endif\n@@ -9233,1 +8290,0 @@\n-#ifdef _LP64\n@@ -9751,148 +8807,0 @@\n-#else\n-void MacroAssembler::crc32c_ipl_alg4(Register in_out, uint32_t n,\n-                                     Register tmp1, Register tmp2, Register tmp3,\n-                                     XMMRegister xtmp1, XMMRegister xtmp2) {\n-  lea(tmp3, ExternalAddress(StubRoutines::crc32c_table_addr()));\n-  if (n > 0) {\n-    addl(tmp3, n * 256 * 8);\n-  }\n-  \/\/    Q1 = TABLEExt[n][B & 0xFF];\n-  movl(tmp1, in_out);\n-  andl(tmp1, 0x000000FF);\n-  shll(tmp1, 3);\n-  addl(tmp1, tmp3);\n-  movq(xtmp1, Address(tmp1, 0));\n-\n-  \/\/    Q2 = TABLEExt[n][B >> 8 & 0xFF];\n-  movl(tmp2, in_out);\n-  shrl(tmp2, 8);\n-  andl(tmp2, 0x000000FF);\n-  shll(tmp2, 3);\n-  addl(tmp2, tmp3);\n-  movq(xtmp2, Address(tmp2, 0));\n-\n-  psllq(xtmp2, 8);\n-  pxor(xtmp1, xtmp2);\n-\n-  \/\/    Q3 = TABLEExt[n][B >> 16 & 0xFF];\n-  movl(tmp2, in_out);\n-  shrl(tmp2, 16);\n-  andl(tmp2, 0x000000FF);\n-  shll(tmp2, 3);\n-  addl(tmp2, tmp3);\n-  movq(xtmp2, Address(tmp2, 0));\n-\n-  psllq(xtmp2, 16);\n-  pxor(xtmp1, xtmp2);\n-\n-  \/\/    Q4 = TABLEExt[n][B >> 24 & 0xFF];\n-  shrl(in_out, 24);\n-  andl(in_out, 0x000000FF);\n-  shll(in_out, 3);\n-  addl(in_out, tmp3);\n-  movq(xtmp2, Address(in_out, 0));\n-\n-  psllq(xtmp2, 24);\n-  pxor(xtmp1, xtmp2); \/\/ Result in CXMM\n-  \/\/    return Q1 ^ Q2 << 8 ^ Q3 << 16 ^ Q4 << 24;\n-}\n-\n-void MacroAssembler::crc32c_pclmulqdq(XMMRegister w_xtmp1,\n-                                      Register in_out,\n-                                      uint32_t const_or_pre_comp_const_index, bool is_pclmulqdq_supported,\n-                                      XMMRegister w_xtmp2,\n-                                      Register tmp1,\n-                                      Register n_tmp2, Register n_tmp3) {\n-  if (is_pclmulqdq_supported) {\n-    movdl(w_xtmp1, in_out);\n-\n-    movl(tmp1, const_or_pre_comp_const_index);\n-    movdl(w_xtmp2, tmp1);\n-    pclmulqdq(w_xtmp1, w_xtmp2, 0);\n-    \/\/ Keep result in XMM since GPR is 32 bit in length\n-  } else {\n-    crc32c_ipl_alg4(in_out, const_or_pre_comp_const_index, tmp1, n_tmp2, n_tmp3, w_xtmp1, w_xtmp2);\n-  }\n-}\n-\n-void MacroAssembler::crc32c_rec_alt2(uint32_t const_or_pre_comp_const_index_u1, uint32_t const_or_pre_comp_const_index_u2, bool is_pclmulqdq_supported, Register in_out, Register in1, Register in2,\n-                                     XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,\n-                                     Register tmp1, Register tmp2,\n-                                     Register n_tmp3) {\n-  crc32c_pclmulqdq(w_xtmp1, in_out, const_or_pre_comp_const_index_u1, is_pclmulqdq_supported, w_xtmp3, tmp1, tmp2, n_tmp3);\n-  crc32c_pclmulqdq(w_xtmp2, in1, const_or_pre_comp_const_index_u2, is_pclmulqdq_supported, w_xtmp3, tmp1, tmp2, n_tmp3);\n-\n-  psllq(w_xtmp1, 1);\n-  movdl(tmp1, w_xtmp1);\n-  psrlq(w_xtmp1, 32);\n-  movdl(in_out, w_xtmp1);\n-\n-  xorl(tmp2, tmp2);\n-  crc32(tmp2, tmp1, 4);\n-  xorl(in_out, tmp2);\n-\n-  psllq(w_xtmp2, 1);\n-  movdl(tmp1, w_xtmp2);\n-  psrlq(w_xtmp2, 32);\n-  movdl(in1, w_xtmp2);\n-\n-  xorl(tmp2, tmp2);\n-  crc32(tmp2, tmp1, 4);\n-  xorl(in1, tmp2);\n-  xorl(in_out, in1);\n-  xorl(in_out, in2);\n-}\n-\n-void MacroAssembler::crc32c_proc_chunk(uint32_t size, uint32_t const_or_pre_comp_const_index_u1, uint32_t const_or_pre_comp_const_index_u2, bool is_pclmulqdq_supported,\n-                                       Register in_out1, Register in_out2, Register in_out3,\n-                                       Register tmp1, Register tmp2, Register tmp3,\n-                                       XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,\n-                                       Register tmp4, Register tmp5,\n-                                       Register n_tmp6) {\n-  Label L_processPartitions;\n-  Label L_processPartition;\n-  Label L_exit;\n-\n-  bind(L_processPartitions);\n-  cmpl(in_out1, 3 * size);\n-  jcc(Assembler::less, L_exit);\n-    xorl(tmp1, tmp1);\n-    xorl(tmp2, tmp2);\n-    movl(tmp3, in_out2);\n-    addl(tmp3, size);\n-\n-    bind(L_processPartition);\n-      crc32(in_out3, Address(in_out2, 0), 4);\n-      crc32(tmp1, Address(in_out2, size), 4);\n-      crc32(tmp2, Address(in_out2, size*2), 4);\n-      crc32(in_out3, Address(in_out2, 0+4), 4);\n-      crc32(tmp1, Address(in_out2, size+4), 4);\n-      crc32(tmp2, Address(in_out2, size*2+4), 4);\n-      addl(in_out2, 8);\n-      cmpl(in_out2, tmp3);\n-      jcc(Assembler::less, L_processPartition);\n-\n-        push(tmp3);\n-        push(in_out1);\n-        push(in_out2);\n-        tmp4 = tmp3;\n-        tmp5 = in_out1;\n-        n_tmp6 = in_out2;\n-\n-      crc32c_rec_alt2(const_or_pre_comp_const_index_u1, const_or_pre_comp_const_index_u2, is_pclmulqdq_supported, in_out3, tmp1, tmp2,\n-            w_xtmp1, w_xtmp2, w_xtmp3,\n-            tmp4, tmp5,\n-            n_tmp6);\n-\n-        pop(in_out2);\n-        pop(in_out1);\n-        pop(tmp3);\n-\n-    addl(in_out2, 2 * size);\n-    subl(in_out1, 3 * size);\n-    jmp(L_processPartitions);\n-\n-  bind(L_exit);\n-}\n-#endif \/\/LP64\n-#ifdef _LP64\n@@ -9991,78 +8899,0 @@\n-#else\n-void MacroAssembler::crc32c_ipl_alg2_alt2(Register in_out, Register in1, Register in2,\n-                                          Register tmp1, Register  tmp2, Register tmp3,\n-                                          Register tmp4, Register  tmp5, Register tmp6,\n-                                          XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,\n-                                          bool is_pclmulqdq_supported) {\n-  uint32_t const_or_pre_comp_const_index[CRC32C_NUM_PRECOMPUTED_CONSTANTS];\n-  Label L_wordByWord;\n-  Label L_byteByByteProlog;\n-  Label L_byteByByte;\n-  Label L_exit;\n-\n-  if (is_pclmulqdq_supported) {\n-    const_or_pre_comp_const_index[1] = *(uint32_t *)StubRoutines::crc32c_table_addr();\n-    const_or_pre_comp_const_index[0] = *((uint32_t *)StubRoutines::crc32c_table_addr() + 1);\n-\n-    const_or_pre_comp_const_index[3] = *((uint32_t *)StubRoutines::crc32c_table_addr() + 2);\n-    const_or_pre_comp_const_index[2] = *((uint32_t *)StubRoutines::crc32c_table_addr() + 3);\n-\n-    const_or_pre_comp_const_index[5] = *((uint32_t *)StubRoutines::crc32c_table_addr() + 4);\n-    const_or_pre_comp_const_index[4] = *((uint32_t *)StubRoutines::crc32c_table_addr() + 5);\n-  } else {\n-    const_or_pre_comp_const_index[0] = 1;\n-    const_or_pre_comp_const_index[1] = 0;\n-\n-    const_or_pre_comp_const_index[2] = 3;\n-    const_or_pre_comp_const_index[3] = 2;\n-\n-    const_or_pre_comp_const_index[4] = 5;\n-    const_or_pre_comp_const_index[5] = 4;\n-  }\n-  crc32c_proc_chunk(CRC32C_HIGH, const_or_pre_comp_const_index[0], const_or_pre_comp_const_index[1], is_pclmulqdq_supported,\n-                    in2, in1, in_out,\n-                    tmp1, tmp2, tmp3,\n-                    w_xtmp1, w_xtmp2, w_xtmp3,\n-                    tmp4, tmp5,\n-                    tmp6);\n-  crc32c_proc_chunk(CRC32C_MIDDLE, const_or_pre_comp_const_index[2], const_or_pre_comp_const_index[3], is_pclmulqdq_supported,\n-                    in2, in1, in_out,\n-                    tmp1, tmp2, tmp3,\n-                    w_xtmp1, w_xtmp2, w_xtmp3,\n-                    tmp4, tmp5,\n-                    tmp6);\n-  crc32c_proc_chunk(CRC32C_LOW, const_or_pre_comp_const_index[4], const_or_pre_comp_const_index[5], is_pclmulqdq_supported,\n-                    in2, in1, in_out,\n-                    tmp1, tmp2, tmp3,\n-                    w_xtmp1, w_xtmp2, w_xtmp3,\n-                    tmp4, tmp5,\n-                    tmp6);\n-  movl(tmp1, in2);\n-  andl(tmp1, 0x00000007);\n-  negl(tmp1);\n-  addl(tmp1, in2);\n-  addl(tmp1, in1);\n-\n-  BIND(L_wordByWord);\n-  cmpl(in1, tmp1);\n-  jcc(Assembler::greaterEqual, L_byteByByteProlog);\n-    crc32(in_out, Address(in1,0), 4);\n-    addl(in1, 4);\n-    jmp(L_wordByWord);\n-\n-  BIND(L_byteByByteProlog);\n-  andl(in2, 0x00000007);\n-  movl(tmp2, 1);\n-\n-  BIND(L_byteByByte);\n-  cmpl(tmp2, in2);\n-  jccb(Assembler::greater, L_exit);\n-    movb(tmp1, Address(in1, 0));\n-    crc32(in_out, tmp1, 1);\n-    incl(in1);\n-    incl(tmp2);\n-    jmp(L_byteByByte);\n-\n-  BIND(L_exit);\n-}\n-#endif \/\/ LP64\n@@ -11062,1 +9892,0 @@\n-#ifdef _LP64\n@@ -11239,1 +10068,0 @@\n-#endif\n@@ -11243,1 +10071,0 @@\n-#ifdef _LP64\n@@ -11407,2 +10234,0 @@\n-#endif \/\/ _LP64\n-\n@@ -11598,1 +10423,0 @@\n-#ifdef _LP64\n@@ -11647,1 +10471,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":114,"deletions":1291,"binary":false,"changes":1405,"status":"modified"},{"patch":"@@ -77,5 +77,0 @@\n-  \/\/ helpers for FPU flag access\n-  \/\/ tmp is a temporary register, if none is available use noreg\n-  void save_rax   (Register tmp);\n-  void restore_rax(Register tmp);\n-\n@@ -174,4 +169,4 @@\n-  void increment(Register reg, int value = 1) { LP64_ONLY(incrementq(reg, value)) NOT_LP64(incrementl(reg, value)) ; }\n-  void decrement(Register reg, int value = 1) { LP64_ONLY(decrementq(reg, value)) NOT_LP64(decrementl(reg, value)) ; }\n-  void increment(Address dst, int value = 1)  { LP64_ONLY(incrementq(dst, value)) NOT_LP64(incrementl(dst, value)) ; }\n-  void decrement(Address dst, int value = 1)  { LP64_ONLY(decrementq(dst, value)) NOT_LP64(decrementl(dst, value)) ; }\n+  void increment(Register reg, int value = 1) { incrementq(reg, value); }\n+  void decrement(Register reg, int value = 1) { decrementq(reg, value); }\n+  void increment(Address dst, int value = 1)  { incrementq(dst, value); }\n+  void decrement(Address dst, int value = 1)  { decrementq(dst, value); }\n@@ -255,1 +250,0 @@\n-#ifdef _LP64\n@@ -271,1 +265,0 @@\n-#endif \/\/ _LP64\n@@ -388,2 +381,0 @@\n-#ifdef _LP64\n-#endif\n@@ -403,1 +394,1 @@\n-                      Register tmp1, Register thread_tmp);\n+                      Register tmp1);\n@@ -416,4 +407,2 @@\n-  void load_heap_oop(Register dst, Address src, Register tmp1 = noreg,\n-                     Register thread_tmp = noreg, DecoratorSet decorators = 0);\n-  void load_heap_oop_not_null(Register dst, Address src, Register tmp1 = noreg,\n-                              Register thread_tmp = noreg, DecoratorSet decorators = 0);\n+  void load_heap_oop(Register dst, Address src, Register tmp1 = noreg, DecoratorSet decorators = 0);\n+  void load_heap_oop_not_null(Register dst, Address src, Register tmp1 = noreg, DecoratorSet decorators = 0);\n@@ -429,1 +418,0 @@\n-#ifdef _LP64\n@@ -464,2 +452,0 @@\n-#endif \/\/ _LP64\n-\n@@ -505,33 +491,0 @@\n-#ifndef _LP64\n-  \/\/ Compares the top-most stack entries on the FPU stack and sets the eflags as follows:\n-  \/\/\n-  \/\/ CF (corresponds to C0) if x < y\n-  \/\/ PF (corresponds to C2) if unordered\n-  \/\/ ZF (corresponds to C3) if x = y\n-  \/\/\n-  \/\/ The arguments are in reversed order on the stack (i.e., top of stack is first argument).\n-  \/\/ tmp is a temporary register, if none is available use noreg (only matters for non-P6 code)\n-  void fcmp(Register tmp);\n-  \/\/ Variant of the above which allows y to be further down the stack\n-  \/\/ and which only pops x and y if specified. If pop_right is\n-  \/\/ specified then pop_left must also be specified.\n-  void fcmp(Register tmp, int index, bool pop_left, bool pop_right);\n-\n-  \/\/ Floating-point comparison for Java\n-  \/\/ Compares the top-most stack entries on the FPU stack and stores the result in dst.\n-  \/\/ The arguments are in reversed order on the stack (i.e., top of stack is first argument).\n-  \/\/ (semantics as described in JVM spec.)\n-  void fcmp2int(Register dst, bool unordered_is_less);\n-  \/\/ Variant of the above which allows y to be further down the stack\n-  \/\/ and which only pops x and y if specified. If pop_right is\n-  \/\/ specified then pop_left must also be specified.\n-  void fcmp2int(Register dst, bool unordered_is_less, int index, bool pop_left, bool pop_right);\n-\n-  \/\/ Floating-point remainder for Java (ST0 = ST0 fremr ST1, ST1 is empty afterwards)\n-  \/\/ tmp is a temporary register, if none is available use noreg\n-  void fremr(Register tmp);\n-\n-  \/\/ only if +VerifyFPU\n-  void verify_FPU(int stack_depth, const char* s = \"illegal FPU state\");\n-#endif \/\/ !LP64\n-\n@@ -552,28 +505,0 @@\n-  \/\/ branch to L if FPU flag C2 is set\/not set\n-  \/\/ tmp is a temporary register, if none is available use noreg\n-  void jC2 (Register tmp, Label& L);\n-  void jnC2(Register tmp, Label& L);\n-\n-  \/\/ Load float value from 'address'. If UseSSE >= 1, the value is loaded into\n-  \/\/ register xmm0. Otherwise, the value is loaded onto the FPU stack.\n-  void load_float(Address src);\n-\n-  \/\/ Store float value to 'address'. If UseSSE >= 1, the value is stored\n-  \/\/ from register xmm0. Otherwise, the value is stored from the FPU stack.\n-  void store_float(Address dst);\n-\n-  \/\/ Load double value from 'address'. If UseSSE >= 2, the value is loaded into\n-  \/\/ register xmm0. Otherwise, the value is loaded onto the FPU stack.\n-  void load_double(Address src);\n-\n-  \/\/ Store double value to 'address'. If UseSSE >= 2, the value is stored\n-  \/\/ from register xmm0. Otherwise, the value is stored from the FPU stack.\n-  void store_double(Address dst);\n-\n-#ifndef _LP64\n-  \/\/ Pop ST (ffree & fincstp combined)\n-  void fpop();\n-\n-  void empty_FPU_stack();\n-#endif \/\/ !_LP64\n-\n@@ -640,1 +565,0 @@\n-    Register thread,                   \/\/ Current thread\n@@ -708,1 +632,0 @@\n-#ifdef _LP64\n@@ -718,1 +641,0 @@\n-#endif\n@@ -755,1 +677,0 @@\n-#ifdef _LP64\n@@ -783,1 +704,0 @@\n-#endif\n@@ -892,1 +812,1 @@\n-  void addptr(Address dst, int32_t src) { LP64_ONLY(addq(dst, src)) NOT_LP64(addl(dst, src)) ; }\n+  void addptr(Address dst, int32_t src) { addq(dst, src); }\n@@ -895,1 +815,1 @@\n-  void addptr(Register dst, Address src) { LP64_ONLY(addq(dst, src)) NOT_LP64(addl(dst, src)); }\n+  void addptr(Register dst, Address src) { addq(dst, src); }\n@@ -904,2 +824,2 @@\n-  void andptr(Register dst, Register src) { LP64_ONLY(andq(dst, src)) NOT_LP64(andl(dst, src)) ; }\n-  void andptr(Register dst, Address src) { LP64_ONLY(andq(dst, src)) NOT_LP64(andl(dst, src)) ; }\n+  void andptr(Register src1, Register src2) { andq(src1, src2); }\n+  void andptr(Register dst, Address src) { andq(dst, src); }\n@@ -907,1 +827,0 @@\n-#ifdef _LP64\n@@ -910,1 +829,0 @@\n-#endif\n@@ -923,6 +841,0 @@\n-#ifndef _LP64\n-  void cmpklass(Address dst, Metadata* obj);\n-  void cmpklass(Register dst, Metadata* obj);\n-  void cmpoop(Address dst, jobject obj);\n-#endif \/\/ _LP64\n-\n@@ -938,3 +850,2 @@\n-  void cmpptr(Register src1, Register src2) { LP64_ONLY(cmpq(src1, src2)) NOT_LP64(cmpl(src1, src2)) ; }\n-  void cmpptr(Register src1, Address src2) { LP64_ONLY(cmpq(src1, src2)) NOT_LP64(cmpl(src1, src2)) ; }\n-  \/\/ void cmpptr(Address src1, Register src2) { LP64_ONLY(cmpq(src1, src2)) NOT_LP64(cmpl(src1, src2)) ; }\n+  void cmpptr(Register src1, Register src2) { cmpq(src1, src2); }\n+  void cmpptr(Register src1, Address src2) { cmpq(src1, src2); }\n@@ -942,2 +853,2 @@\n-  void cmpptr(Register src1, int32_t src2) { LP64_ONLY(cmpq(src1, src2)) NOT_LP64(cmpl(src1, src2)) ; }\n-  void cmpptr(Address src1, int32_t src2) { LP64_ONLY(cmpq(src1, src2)) NOT_LP64(cmpl(src1, src2)) ; }\n+  void cmpptr(Register src1, int32_t src2) { cmpq(src1, src2); }\n+  void cmpptr(Address src1, int32_t src2) { cmpq(src1, src2); }\n@@ -952,2 +863,2 @@\n-  void imulptr(Register dst, Register src) { LP64_ONLY(imulq(dst, src)) NOT_LP64(imull(dst, src)); }\n-  void imulptr(Register dst, Register src, int imm32) { LP64_ONLY(imulq(dst, src, imm32)) NOT_LP64(imull(dst, src, imm32)); }\n+  void imulptr(Register dst, Register src) { imulq(dst, src); }\n+  void imulptr(Register dst, Register src, int imm32) { imulq(dst, src, imm32); }\n@@ -956,1 +867,1 @@\n-  void negptr(Register dst) { LP64_ONLY(negq(dst)) NOT_LP64(negl(dst)); }\n+  void negptr(Register dst) { negq(dst); }\n@@ -958,1 +869,1 @@\n-  void notptr(Register dst) { LP64_ONLY(notq(dst)) NOT_LP64(notl(dst)); }\n+  void notptr(Register dst) { notq(dst); }\n@@ -961,1 +872,1 @@\n-  void shlptr(Register dst) { LP64_ONLY(shlq(dst)) NOT_LP64(shll(dst)); }\n+  void shlptr(Register dst) { shlq(dst); }\n@@ -964,1 +875,1 @@\n-  void shrptr(Register dst) { LP64_ONLY(shrq(dst)) NOT_LP64(shrl(dst)); }\n+  void shrptr(Register dst) { shrq(dst); }\n@@ -966,2 +877,2 @@\n-  void sarptr(Register dst) { LP64_ONLY(sarq(dst)) NOT_LP64(sarl(dst)); }\n-  void sarptr(Register dst, int32_t src) { LP64_ONLY(sarq(dst, src)) NOT_LP64(sarl(dst, src)); }\n+  void sarptr(Register dst) { sarq(dst); }\n+  void sarptr(Register dst, int32_t src) { sarq(dst, src); }\n@@ -969,1 +880,1 @@\n-  void subptr(Address dst, int32_t src) { LP64_ONLY(subq(dst, src)) NOT_LP64(subl(dst, src)); }\n+  void subptr(Address dst, int32_t src) { subq(dst, src); }\n@@ -971,1 +882,1 @@\n-  void subptr(Register dst, Address src) { LP64_ONLY(subq(dst, src)) NOT_LP64(subl(dst, src)); }\n+  void subptr(Register dst, Address src) { subq(dst, src); }\n@@ -981,2 +892,2 @@\n-  void sbbptr(Address dst, int32_t src) { LP64_ONLY(sbbq(dst, src)) NOT_LP64(sbbl(dst, src)); }\n-  void sbbptr(Register dst, int32_t src) { LP64_ONLY(sbbq(dst, src)) NOT_LP64(sbbl(dst, src)); }\n+  void sbbptr(Address dst, int32_t src) { sbbq(dst, src); }\n+  void sbbptr(Register dst, int32_t src) { sbbq(dst, src); }\n@@ -984,2 +895,2 @@\n-  void xchgptr(Register src1, Register src2) { LP64_ONLY(xchgq(src1, src2)) NOT_LP64(xchgl(src1, src2)) ; }\n-  void xchgptr(Register src1, Address src2) { LP64_ONLY(xchgq(src1, src2)) NOT_LP64(xchgl(src1, src2)) ; }\n+  void xchgptr(Register src1, Register src2) { xchgq(src1, src2); }\n+  void xchgptr(Register src1, Address src2) { xchgq(src1, src2); }\n@@ -987,1 +898,1 @@\n-  void xaddptr(Address src1, Register src2) { LP64_ONLY(xaddq(src1, src2)) NOT_LP64(xaddl(src1, src2)) ; }\n+  void xaddptr(Address src1, Register src2) { xaddq(src1, src2); }\n@@ -997,1 +908,0 @@\n-#ifdef _LP64\n@@ -1000,3 +910,2 @@\n-#endif\n-  void atomic_incptr(AddressLiteral counter_addr, Register rscratch = noreg) { LP64_ONLY(atomic_incq(counter_addr, rscratch)) NOT_LP64(atomic_incl(counter_addr, rscratch)) ; }\n-  void atomic_incptr(Address counter_addr) { LP64_ONLY(atomic_incq(counter_addr)) NOT_LP64(atomic_incl(counter_addr)) ; }\n+  void atomic_incptr(AddressLiteral counter_addr, Register rscratch = noreg) { atomic_incq(counter_addr, rscratch); }\n+  void atomic_incptr(Address counter_addr) { atomic_incq(counter_addr); }\n@@ -1020,4 +929,4 @@\n-  void orptr(Register dst, Address src) { LP64_ONLY(orq(dst, src)) NOT_LP64(orl(dst, src)); }\n-  void orptr(Register dst, Register src) { LP64_ONLY(orq(dst, src)) NOT_LP64(orl(dst, src)); }\n-  void orptr(Register dst, int32_t src) { LP64_ONLY(orq(dst, src)) NOT_LP64(orl(dst, src)); }\n-  void orptr(Address dst, int32_t imm32) { LP64_ONLY(orq(dst, imm32)) NOT_LP64(orl(dst, imm32)); }\n+  void orptr(Register dst, Address src) { orq(dst, src); }\n+  void orptr(Register dst, Register src) { orq(dst, src); }\n+  void orptr(Register dst, int32_t src) { orq(dst, src); }\n+  void orptr(Address dst, int32_t imm32) { orq(dst, imm32); }\n@@ -1025,3 +934,3 @@\n-  void testptr(Register src, int32_t imm32) {  LP64_ONLY(testq(src, imm32)) NOT_LP64(testl(src, imm32)); }\n-  void testptr(Register src1, Address src2) { LP64_ONLY(testq(src1, src2)) NOT_LP64(testl(src1, src2)); }\n-  void testptr(Address src, int32_t imm32) {  LP64_ONLY(testq(src, imm32)) NOT_LP64(testl(src, imm32)); }\n+  void testptr(Register src, int32_t imm32) { testq(src, imm32); }\n+  void testptr(Register src1, Address src2) { testq(src1, src2); }\n+  void testptr(Address src, int32_t imm32) { testq(src, imm32); }\n@@ -1030,2 +939,2 @@\n-  void xorptr(Register dst, Register src) { LP64_ONLY(xorq(dst, src)) NOT_LP64(xorl(dst, src)); }\n-  void xorptr(Register dst, Address src) { LP64_ONLY(xorq(dst, src)) NOT_LP64(xorl(dst, src)); }\n+  void xorptr(Register dst, Register src) { xorq(dst, src); }\n+  void xorptr(Register dst, Address src) { xorq(dst, src); }\n@@ -1156,21 +1065,0 @@\n-#ifndef _LP64\n-  void fadd_s(Address        src) { Assembler::fadd_s(src); }\n-  void fadd_s(AddressLiteral src) { Assembler::fadd_s(as_Address(src)); }\n-\n-  void fldcw(Address        src) { Assembler::fldcw(src); }\n-  void fldcw(AddressLiteral src);\n-\n-  void fld_s(int index)          { Assembler::fld_s(index); }\n-  void fld_s(Address        src) { Assembler::fld_s(src); }\n-  void fld_s(AddressLiteral src);\n-\n-  void fld_d(Address        src) { Assembler::fld_d(src); }\n-  void fld_d(AddressLiteral src);\n-\n-  void fld_x(Address        src) { Assembler::fld_x(src); }\n-  void fld_x(AddressLiteral src) { Assembler::fld_x(as_Address(src)); }\n-\n-  void fmul_s(Address        src) { Assembler::fmul_s(src); }\n-  void fmul_s(AddressLiteral src) { Assembler::fmul_s(as_Address(src)); }\n-#endif \/\/ !_LP64\n-\n@@ -1181,1 +1069,0 @@\n-#ifdef _LP64\n@@ -1231,1 +1118,0 @@\n-#endif \/\/ _LP64\n@@ -1241,1 +1127,0 @@\n-#ifdef _LP64\n@@ -1246,6 +1131,0 @@\n-#else\n-  void fast_sha256(XMMRegister msg, XMMRegister state0, XMMRegister state1, XMMRegister msgtmp0,\n-                   XMMRegister msgtmp1, XMMRegister msgtmp2, XMMRegister msgtmp3, XMMRegister msgtmp4,\n-                   Register buf, Register state, Register ofs, Register limit, Register rsp,\n-                   bool multi_block);\n-#endif\n@@ -1257,46 +1136,0 @@\n-#ifndef _LP64\n- private:\n-  \/\/ Initialized in macroAssembler_x86_constants.cpp\n-  static address ONES;\n-  static address L_2IL0FLOATPACKET_0;\n-  static address PI4_INV;\n-  static address PI4X3;\n-  static address PI4X4;\n-\n- public:\n-  void fast_log(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rcx, Register rdx, Register tmp1);\n-\n-  void fast_log10(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rcx, Register rdx, Register tmp);\n-\n-  void fast_pow(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3, XMMRegister xmm4,\n-                XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7, Register rax, Register rcx,\n-                Register rdx, Register tmp);\n-\n-  void fast_sin(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rbx, Register rdx);\n-\n-  void fast_cos(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rcx, Register rdx, Register tmp);\n-\n-  void libm_sincos_huge(XMMRegister xmm0, XMMRegister xmm1, Register eax, Register ecx,\n-                        Register edx, Register ebx, Register esi, Register edi,\n-                        Register ebp, Register esp);\n-\n-  void libm_reduce_pi04l(Register eax, Register ecx, Register edx, Register ebx,\n-                         Register esi, Register edi, Register ebp, Register esp);\n-\n-  void libm_tancot_huge(XMMRegister xmm0, XMMRegister xmm1, Register eax, Register ecx,\n-                        Register edx, Register ebx, Register esi, Register edi,\n-                        Register ebp, Register esp);\n-\n-  void fast_tan(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rcx, Register rdx, Register tmp);\n-#endif \/\/ !_LP64\n-\n@@ -2069,2 +1902,2 @@\n-  void cmovptr(Condition cc, Register dst, Address  src) { LP64_ONLY(cmovq(cc, dst, src)) NOT_LP64(cmov32(cc, dst, src)); }\n-  void cmovptr(Condition cc, Register dst, Register src) { LP64_ONLY(cmovq(cc, dst, src)) NOT_LP64(cmov32(cc, dst, src)); }\n+  void cmovptr(Condition cc, Register dst, Address  src) { cmovq(cc, dst, src); }\n+  void cmovptr(Condition cc, Register dst, Register src) { cmovq(cc, dst, src); }\n@@ -2109,2 +1942,2 @@\n-  void pushptr(Address src) { LP64_ONLY(pushq(src)) NOT_LP64(pushl(src)); }\n-  void popptr(Address src) { LP64_ONLY(popq(src)) NOT_LP64(popl(src)); }\n+  void pushptr(Address src) { pushq(src); }\n+  void popptr(Address src) { popq(src); }\n@@ -2116,2 +1949,2 @@\n-  void movl2ptr(Register dst, Address src) { LP64_ONLY(movslq(dst, src)) NOT_LP64(movl(dst, src)); }\n-  void movl2ptr(Register dst, Register src) { LP64_ONLY(movslq(dst, src)) NOT_LP64(if (dst != src) movl(dst, src)); }\n+  void movl2ptr(Register dst, Address src) { movslq(dst, src); }\n+  void movl2ptr(Register dst, Register src) { movslq(dst, src); }\n@@ -2155,1 +1988,0 @@\n-#ifdef _LP64\n@@ -2196,1 +2028,0 @@\n-#endif\n@@ -2202,2 +2033,0 @@\n-\n-#ifdef _LP64\n@@ -2208,1 +2037,0 @@\n-#endif \/\/ _LP64\n@@ -2214,1 +2042,0 @@\n-#ifdef _LP64\n@@ -2217,5 +2044,0 @@\n-#else\n-  void crc32c_ipl_alg4(Register in_out, uint32_t n,\n-                       Register tmp1, Register tmp2, Register tmp3,\n-                       XMMRegister xtmp1, XMMRegister xtmp2);\n-#endif\n@@ -2246,1 +2068,0 @@\n-#ifdef _LP64\n@@ -2249,1 +2070,0 @@\n-#endif \/\/ _LP64\n@@ -2283,1 +2103,0 @@\n-#ifdef _LP64\n@@ -2298,1 +2117,0 @@\n-#endif \/\/ _LP64\n@@ -2307,1 +2125,0 @@\n-#ifdef _LP64\n@@ -2311,1 +2128,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":47,"deletions":231,"binary":false,"changes":278,"status":"modified"},{"patch":"@@ -85,2 +85,2 @@\n-#define PUSH { __ push(temp); LP64_ONLY(  __ push(rscratch1); )               }\n-#define POP  {                LP64_ONLY(  __ pop(rscratch1);  ) __ pop(temp); }\n+#define PUSH { __ push(temp); __ push(rscratch1);               }\n+#define POP  {                __ pop(rscratch1);  __ pop(temp); }\n@@ -142,6 +142,0 @@\n-#ifdef _LP64\n-    Register rthread = r15_thread;\n-#else\n-    Register rthread = temp;\n-    __ get_thread(rthread);\n-#endif\n@@ -150,1 +144,1 @@\n-    __ cmpb(Address(rthread, JavaThread::interp_only_mode_offset()), 0);\n+    __ cmpb(Address(r15_thread, JavaThread::interp_only_mode_offset()), 0);\n@@ -189,1 +183,1 @@\n-                    noreg, noreg);\n+                    noreg);\n@@ -219,1 +213,1 @@\n-                    noreg, noreg);\n+                    noreg);\n@@ -331,1 +325,0 @@\n-#ifdef _LP64\n@@ -340,13 +333,1 @@\n-  }\n-#else\n-  Register temp1 = (for_compiler_entry ? rsi : rdx);\n-  Register temp2 = rdi;\n-  Register temp3 = rax;\n-  if (for_compiler_entry) {\n-    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic || iid == vmIntrinsics::_linkToNative ? noreg : rcx), \"only valid assignment\");\n-    assert_different_registers(temp1,        rcx, rdx);\n-    assert_different_registers(temp2,        rcx, rdx);\n-    assert_different_registers(temp3,        rcx, rdx);\n-  }\n-#endif\n-  else {\n+  } else {\n@@ -427,1 +408,1 @@\n-      __ access_load_at(T_ADDRESS, IN_HEAP, rbx_method, vmtarget_method, noreg, noreg);\n+      __ access_load_at(T_ADDRESS, IN_HEAP, rbx_method, vmtarget_method, noreg);\n@@ -435,1 +416,1 @@\n-      __ access_load_at(T_ADDRESS, IN_HEAP, rbx_method, vmtarget_method, noreg, noreg);\n+      __ access_load_at(T_ADDRESS, IN_HEAP, rbx_method, vmtarget_method, noreg);\n@@ -449,1 +430,1 @@\n-      __ access_load_at(T_ADDRESS, IN_HEAP, temp2_index, member_vmindex, noreg, noreg);\n+      __ access_load_at(T_ADDRESS, IN_HEAP, temp2_index, member_vmindex, noreg);\n@@ -481,1 +462,1 @@\n-      __ access_load_at(T_ADDRESS, IN_HEAP, rbx_index, member_vmindex, noreg, noreg);\n+      __ access_load_at(T_ADDRESS, IN_HEAP, rbx_index, member_vmindex, noreg);\n@@ -658,10 +639,0 @@\n-#ifdef _LP64\n-#else\n-  if  (UseSSE >= 2) {\n-    __ movdbl(Address(rsp, 0), xmm0);\n-  } else if (UseSSE == 1) {\n-    __ movflt(Address(rsp, 0), xmm0);\n-  } else {\n-    __ fst_d(Address(rsp, 0));\n-  }\n-#endif \/\/ LP64\n@@ -683,10 +654,0 @@\n-#ifdef _LP64\n-#else\n-  if  (UseSSE >= 2) {\n-    __ movdbl(xmm0, Address(rsp, 0));\n-  } else if (UseSSE == 1) {\n-    __ movflt(xmm0, Address(rsp, 0));\n-  } else {\n-    __ fld_d(Address(rsp, 0));\n-  }\n-#endif \/\/ LP64\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":10,"deletions":49,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -4000,1 +4000,1 @@\n-                    noreg, noreg);\n+                    noreg);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -662,1 +662,1 @@\n-  __ load_heap_oop(rax, field_address, \/*tmp1*\/ rbx, \/*tmp_thread*\/ rdx, ON_WEAK_OOP_REF);\n+  __ load_heap_oop(rax, field_address, \/*tmp1*\/ rbx, ON_WEAK_OOP_REF);\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -155,1 +155,1 @@\n-  __ load_heap_oop(dst, src, rdx, rbx, decorators);\n+  __ load_heap_oop(dst, src, rdx, decorators);\n@@ -281,17 +281,12 @@\n-  if (UseSSE >= 1) {\n-    static float one = 1.0f, two = 2.0f;\n-    switch (value) {\n-    case 0:\n-      __ xorps(xmm0, xmm0);\n-      break;\n-    case 1:\n-      __ movflt(xmm0, ExternalAddress((address) &one), rscratch1);\n-      break;\n-    case 2:\n-      __ movflt(xmm0, ExternalAddress((address) &two), rscratch1);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-      break;\n-    }\n-  } else {\n+  static float one = 1.0f, two = 2.0f;\n+  switch (value) {\n+  case 0:\n+    __ xorps(xmm0, xmm0);\n+    break;\n+  case 1:\n+    __ movflt(xmm0, ExternalAddress((address) &one), rscratch1);\n+    break;\n+  case 2:\n+    __ movflt(xmm0, ExternalAddress((address) &two), rscratch1);\n+    break;\n+  default:\n@@ -299,0 +294,1 @@\n+    break;\n@@ -304,14 +300,9 @@\n-  if (UseSSE >= 2) {\n-    static double one = 1.0;\n-    switch (value) {\n-    case 0:\n-      __ xorpd(xmm0, xmm0);\n-      break;\n-    case 1:\n-      __ movdbl(xmm0, ExternalAddress((address) &one), rscratch1);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-      break;\n-    }\n-  } else {\n+  static double one = 1.0;\n+  switch (value) {\n+  case 0:\n+    __ xorpd(xmm0, xmm0);\n+    break;\n+  case 1:\n+    __ movdbl(xmm0, ExternalAddress((address) &one), rscratch1);\n+    break;\n+  default:\n@@ -319,0 +310,1 @@\n+    break;\n@@ -378,1 +370,1 @@\n-  __ load_float(Address(rcx, rbx, Address::times_ptr, base_offset));\n+  __ movflt(xmm0, Address(rcx, rbx, Address::times_ptr, base_offset));\n@@ -457,1 +449,1 @@\n-  __ load_double(Address(rcx, rbx, Address::times_ptr, base_offset));\n+  __ movdbl(xmm0, Address(rcx, rbx, Address::times_ptr, base_offset));\n@@ -511,1 +503,1 @@\n-      __ load_float(field);\n+      __ movflt(xmm0, field);\n@@ -566,1 +558,1 @@\n-      __ load_double(field);\n+      __ movdbl(xmm0, field);\n@@ -660,1 +652,1 @@\n-  __ load_float(faddress(rbx));\n+  __ movflt(xmm0, faddress(rbx));\n@@ -666,1 +658,1 @@\n-  __ load_double(daddress(rbx));\n+  __ movdbl(xmm0, daddress(rbx));\n@@ -697,1 +689,1 @@\n-  __ load_float(faddress(rbx));\n+  __ movflt(xmm0, faddress(rbx));\n@@ -703,1 +695,1 @@\n-  __ load_double(daddress(rbx));\n+  __ movdbl(xmm0, daddress(rbx));\n@@ -745,1 +737,1 @@\n-                    noreg, noreg);\n+                    noreg);\n@@ -757,1 +749,1 @@\n-                    noreg, noreg);\n+                    noreg);\n@@ -771,1 +763,1 @@\n-                    noreg, noreg);\n+                    noreg);\n@@ -783,1 +775,1 @@\n-                    noreg, noreg);\n+                    noreg);\n@@ -825,1 +817,1 @@\n-                    noreg, noreg);\n+                    noreg);\n@@ -835,1 +827,1 @@\n-                    noreg, noreg);\n+                    noreg);\n@@ -850,1 +842,1 @@\n-                    noreg, noreg);\n+                    noreg);\n@@ -861,1 +853,1 @@\n-                    noreg, noreg);\n+                    noreg);\n@@ -876,1 +868,1 @@\n-  __ load_float(faddress(n));\n+  __ movflt(xmm0, faddress(n));\n@@ -881,1 +873,1 @@\n-  __ load_double(daddress(n));\n+  __ movdbl(xmm0, daddress(n));\n@@ -983,1 +975,1 @@\n-  __ store_float(faddress(rbx));\n+  __ movflt(faddress(rbx), xmm0);\n@@ -989,1 +981,1 @@\n-  __ store_double(daddress(rbx));\n+  __ movdbl(daddress(rbx), xmm0);\n@@ -1065,1 +1057,1 @@\n-  \/\/ value is in UseSSE >= 1 ? xmm0 : ST(0)\n+  \/\/ value is in xmm0\n@@ -1078,1 +1070,1 @@\n-  \/\/ value is in UseSSE >= 2 ? xmm0 : ST(0)\n+  \/\/ value is in xmm0\n@@ -1233,1 +1225,1 @@\n-  __ store_float(faddress(n));\n+  __ movflt(faddress(n), xmm0);\n@@ -1238,1 +1230,1 @@\n-  __ store_double(daddress(n));\n+  __ movdbl(daddress(n), xmm0);\n@@ -1460,35 +1452,30 @@\n-  if (UseSSE >= 1) {\n-    switch (op) {\n-    case add:\n-      __ addss(xmm0, at_rsp());\n-      __ addptr(rsp, Interpreter::stackElementSize);\n-      break;\n-    case sub:\n-      __ movflt(xmm1, xmm0);\n-      __ pop_f(xmm0);\n-      __ subss(xmm0, xmm1);\n-      break;\n-    case mul:\n-      __ mulss(xmm0, at_rsp());\n-      __ addptr(rsp, Interpreter::stackElementSize);\n-      break;\n-    case div:\n-      __ movflt(xmm1, xmm0);\n-      __ pop_f(xmm0);\n-      __ divss(xmm0, xmm1);\n-      break;\n-    case rem:\n-      \/\/ On x86_64 platforms the SharedRuntime::frem method is called to perform the\n-      \/\/ modulo operation. The frem method calls the function\n-      \/\/ double fmod(double x, double y) in math.h. The documentation of fmod states:\n-      \/\/ \"If x or y is a NaN, a NaN is returned.\" without specifying what type of NaN\n-      \/\/ (signalling or quiet) is returned.\n-      __ movflt(xmm1, xmm0);\n-      __ pop_f(xmm0);\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::frem), 2);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-      break;\n-    }\n-  } else {\n+  switch (op) {\n+  case add:\n+    __ addss(xmm0, at_rsp());\n+    __ addptr(rsp, Interpreter::stackElementSize);\n+    break;\n+  case sub:\n+    __ movflt(xmm1, xmm0);\n+    __ pop_f(xmm0);\n+    __ subss(xmm0, xmm1);\n+    break;\n+  case mul:\n+    __ mulss(xmm0, at_rsp());\n+    __ addptr(rsp, Interpreter::stackElementSize);\n+    break;\n+  case div:\n+    __ movflt(xmm1, xmm0);\n+    __ pop_f(xmm0);\n+    __ divss(xmm0, xmm1);\n+    break;\n+  case rem:\n+    \/\/ On x86_64 platforms the SharedRuntime::frem method is called to perform the\n+    \/\/ modulo operation. The frem method calls the function\n+    \/\/ double fmod(double x, double y) in math.h. The documentation of fmod states:\n+    \/\/ \"If x or y is a NaN, a NaN is returned.\" without specifying what type of NaN\n+    \/\/ (signalling or quiet) is returned.\n+    __ movflt(xmm1, xmm0);\n+    __ pop_f(xmm0);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::frem), 2);\n+    break;\n+  default:\n@@ -1496,0 +1483,1 @@\n+    break;\n@@ -1501,33 +1489,28 @@\n-  if (UseSSE >= 2) {\n-    switch (op) {\n-    case add:\n-      __ addsd(xmm0, at_rsp());\n-      __ addptr(rsp, 2 * Interpreter::stackElementSize);\n-      break;\n-    case sub:\n-      __ movdbl(xmm1, xmm0);\n-      __ pop_d(xmm0);\n-      __ subsd(xmm0, xmm1);\n-      break;\n-    case mul:\n-      __ mulsd(xmm0, at_rsp());\n-      __ addptr(rsp, 2 * Interpreter::stackElementSize);\n-      break;\n-    case div:\n-      __ movdbl(xmm1, xmm0);\n-      __ pop_d(xmm0);\n-      __ divsd(xmm0, xmm1);\n-      break;\n-    case rem:\n-      \/\/ Similar to fop2(), the modulo operation is performed using the\n-      \/\/ SharedRuntime::drem method on x86_64 platforms for the same reasons\n-      \/\/ as mentioned in fop2().\n-      __ movdbl(xmm1, xmm0);\n-      __ pop_d(xmm0);\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::drem), 2);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-      break;\n-    }\n-  } else {\n+  switch (op) {\n+  case add:\n+    __ addsd(xmm0, at_rsp());\n+    __ addptr(rsp, 2 * Interpreter::stackElementSize);\n+    break;\n+  case sub:\n+    __ movdbl(xmm1, xmm0);\n+    __ pop_d(xmm0);\n+    __ subsd(xmm0, xmm1);\n+    break;\n+  case mul:\n+    __ mulsd(xmm0, at_rsp());\n+    __ addptr(rsp, 2 * Interpreter::stackElementSize);\n+    break;\n+  case div:\n+    __ movdbl(xmm1, xmm0);\n+    __ pop_d(xmm0);\n+    __ divsd(xmm0, xmm1);\n+    break;\n+  case rem:\n+    \/\/ Similar to fop2(), the modulo operation is performed using the\n+    \/\/ SharedRuntime::drem method on x86_64 platforms for the same reasons\n+    \/\/ as mentioned in fop2().\n+    __ movdbl(xmm1, xmm0);\n+    __ pop_d(xmm0);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::drem), 2);\n+    break;\n+  default:\n@@ -1535,0 +1518,1 @@\n+    break;\n@@ -1565,6 +1549,2 @@\n-  if (UseSSE >= 1) {\n-    static jlong *float_signflip  = double_quadword(&float_signflip_pool[1],  CONST64(0x8000000080000000),  CONST64(0x8000000080000000));\n-    __ xorps(xmm0, ExternalAddress((address) float_signflip), rscratch1);\n-  } else {\n-    ShouldNotReachHere();\n-  }\n+  static jlong *float_signflip  = double_quadword(&float_signflip_pool[1],  CONST64(0x8000000080000000),  CONST64(0x8000000080000000));\n+  __ xorps(xmm0, ExternalAddress((address) float_signflip), rscratch1);\n@@ -1575,7 +1555,3 @@\n-  if (UseSSE >= 2) {\n-    static jlong *double_signflip =\n-      double_quadword(&double_signflip_pool[1], CONST64(0x8000000000000000), CONST64(0x8000000000000000));\n-    __ xorpd(xmm0, ExternalAddress((address) double_signflip), rscratch1);\n-  } else {\n-    ShouldNotReachHere();\n-  }\n+  static jlong *double_signflip =\n+    double_quadword(&double_signflip_pool[1], CONST64(0x8000000000000000), CONST64(0x8000000000000000));\n+  __ xorpd(xmm0, ExternalAddress((address) double_signflip), rscratch1);\n@@ -1745,27 +1721,5 @@\n-  if ((is_float && UseSSE >= 1) ||\n-      (!is_float && UseSSE >= 2)) {\n-    Label done;\n-    if (is_float) {\n-      \/\/ XXX get rid of pop here, use ... reg, mem32\n-      __ pop_f(xmm1);\n-      __ ucomiss(xmm1, xmm0);\n-    } else {\n-      \/\/ XXX get rid of pop here, use ... reg, mem64\n-      __ pop_d(xmm1);\n-      __ ucomisd(xmm1, xmm0);\n-    }\n-    if (unordered_result < 0) {\n-      __ movl(rax, -1);\n-      __ jccb(Assembler::parity, done);\n-      __ jccb(Assembler::below, done);\n-      __ setb(Assembler::notEqual, rdx);\n-      __ movzbl(rax, rdx);\n-    } else {\n-      __ movl(rax, 1);\n-      __ jccb(Assembler::parity, done);\n-      __ jccb(Assembler::above, done);\n-      __ movl(rax, 0);\n-      __ jccb(Assembler::equal, done);\n-      __ decrementl(rax);\n-    }\n-    __ bind(done);\n+  Label done;\n+  if (is_float) {\n+    \/\/ XXX get rid of pop here, use ... reg, mem32\n+    __ pop_f(xmm1);\n+    __ ucomiss(xmm1, xmm0);\n@@ -1773,1 +1727,17 @@\n-    ShouldNotReachHere();\n+    \/\/ XXX get rid of pop here, use ... reg, mem64\n+    __ pop_d(xmm1);\n+    __ ucomisd(xmm1, xmm0);\n+  }\n+  if (unordered_result < 0) {\n+    __ movl(rax, -1);\n+    __ jccb(Assembler::parity, done);\n+    __ jccb(Assembler::below, done);\n+    __ setb(Assembler::notEqual, rdx);\n+    __ movzbl(rax, rdx);\n+  } else {\n+    __ movl(rax, 1);\n+    __ jccb(Assembler::parity, done);\n+    __ jccb(Assembler::above, done);\n+    __ movl(rax, 0);\n+    __ jccb(Assembler::equal, done);\n+    __ decrementl(rax);\n@@ -1775,0 +1745,1 @@\n+  __ bind(done);\n@@ -2675,1 +2646,1 @@\n-  __ access_load_at(T_BYTE, IN_HEAP, rax, field, noreg, noreg);\n+  __ access_load_at(T_BYTE, IN_HEAP, rax, field, noreg);\n@@ -2688,1 +2659,1 @@\n-  __ access_load_at(T_BOOLEAN, IN_HEAP, rax, field, noreg, noreg);\n+  __ access_load_at(T_BOOLEAN, IN_HEAP, rax, field, noreg);\n@@ -2777,1 +2748,1 @@\n-  __ access_load_at(T_INT, IN_HEAP, rax, field, noreg, noreg);\n+  __ access_load_at(T_INT, IN_HEAP, rax, field, noreg);\n@@ -2789,1 +2760,1 @@\n-  __ access_load_at(T_CHAR, IN_HEAP, rax, field, noreg, noreg);\n+  __ access_load_at(T_CHAR, IN_HEAP, rax, field, noreg);\n@@ -2801,1 +2772,1 @@\n-  __ access_load_at(T_SHORT, IN_HEAP, rax, field, noreg, noreg);\n+  __ access_load_at(T_SHORT, IN_HEAP, rax, field, noreg);\n@@ -2815,1 +2786,1 @@\n-  __ access_load_at(T_LONG, IN_HEAP | MO_RELAXED, noreg \/* ltos *\/, field, noreg, noreg);\n+  __ access_load_at(T_LONG, IN_HEAP | MO_RELAXED, noreg \/* ltos *\/, field, noreg);\n@@ -2826,1 +2797,1 @@\n-  __ access_load_at(T_FLOAT, IN_HEAP, noreg \/* ftos *\/, field, noreg, noreg);\n+  __ access_load_at(T_FLOAT, IN_HEAP, noreg \/* ftos *\/, field, noreg);\n@@ -2842,1 +2813,1 @@\n-  __ access_load_at(T_DOUBLE, IN_HEAP | MO_RELAXED, noreg \/* dtos *\/, field, noreg, noreg);\n+  __ access_load_at(T_DOUBLE, IN_HEAP | MO_RELAXED, noreg \/* dtos *\/, field, noreg);\n@@ -3419,1 +3390,1 @@\n-    __ access_load_at(T_LONG, IN_HEAP, noreg \/* ltos *\/, field, noreg, noreg);\n+    __ access_load_at(T_LONG, IN_HEAP, noreg \/* ltos *\/, field, noreg);\n@@ -3422,1 +3393,1 @@\n-    __ access_load_at(T_INT, IN_HEAP, rax, field, noreg, noreg);\n+    __ access_load_at(T_INT, IN_HEAP, rax, field, noreg);\n@@ -3425,1 +3396,1 @@\n-    __ access_load_at(T_BYTE, IN_HEAP, rax, field, noreg, noreg);\n+    __ access_load_at(T_BYTE, IN_HEAP, rax, field, noreg);\n@@ -3428,1 +3399,1 @@\n-    __ access_load_at(T_SHORT, IN_HEAP, rax, field, noreg, noreg);\n+    __ access_load_at(T_SHORT, IN_HEAP, rax, field, noreg);\n@@ -3431,1 +3402,1 @@\n-    __ access_load_at(T_CHAR, IN_HEAP, rax, field, noreg, noreg);\n+    __ access_load_at(T_CHAR, IN_HEAP, rax, field, noreg);\n@@ -3434,1 +3405,1 @@\n-    __ access_load_at(T_FLOAT, IN_HEAP, noreg \/* ftos *\/, field, noreg, noreg);\n+    __ access_load_at(T_FLOAT, IN_HEAP, noreg \/* ftos *\/, field, noreg);\n@@ -3437,1 +3408,1 @@\n-    __ access_load_at(T_DOUBLE, IN_HEAP, noreg \/* dtos *\/, field, noreg, noreg);\n+    __ access_load_at(T_DOUBLE, IN_HEAP, noreg \/* dtos *\/, field, noreg);\n@@ -3466,1 +3437,1 @@\n-    __ access_load_at(T_INT, IN_HEAP, rax, field, noreg, noreg);\n+    __ access_load_at(T_INT, IN_HEAP, rax, field, noreg);\n@@ -3473,1 +3444,1 @@\n-    __ access_load_at(T_FLOAT, IN_HEAP, noreg \/* ftos *\/, field, noreg, noreg);\n+    __ access_load_at(T_FLOAT, IN_HEAP, noreg \/* ftos *\/, field, noreg);\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":153,"deletions":182,"binary":false,"changes":335,"status":"modified"},{"patch":"@@ -1274,1 +1274,1 @@\n-  if (supports_fma() && UseSSE >= 2) { \/\/ Check UseSSE since FMA code uses SSE instructions\n+  if (supports_fma()) {\n@@ -1343,9 +1343,0 @@\n-#ifdef COMPILER2\n-  if (UseFPUForSpilling) {\n-    if (UseSSE < 2) {\n-      \/\/ Only supported with SSE2+\n-      FLAG_SET_DEFAULT(UseFPUForSpilling, false);\n-    }\n-  }\n-#endif\n-\n@@ -1354,5 +1345,1 @@\n-  if (UseSSE < 2) {\n-    \/\/ Vectors (in XMM) are only supported with SSE2+\n-    \/\/ SSE is always 2 on x64.\n-    max_vector_size = 0;\n-  } else if (UseAVX == 0 || !os_supports_avx_vectors()) {\n+  if (UseAVX == 0 || !os_supports_avx_vectors()) {\n@@ -1873,1 +1860,1 @@\n-  if (UseSSE >= 2 && UseUnalignedLoadStores) {\n+  if (UseUnalignedLoadStores) {\n@@ -1988,1 +1975,1 @@\n-    if (AllocatePrefetchStyle <= 0 || (UseSSE == 0 && !supports_3dnow_prefetch())) {\n+    if (AllocatePrefetchStyle <= 0) {\n@@ -1992,1 +1979,7 @@\n-      if (UseSSE == 0 && supports_3dnow_prefetch()) {\n+      if (AllocatePrefetchInstr == 0) {\n+        log->print(\"PREFETCHNTA\");\n+      } else if (AllocatePrefetchInstr == 1) {\n+        log->print(\"PREFETCHT0\");\n+      } else if (AllocatePrefetchInstr == 2) {\n+        log->print(\"PREFETCHT2\");\n+      } else if (AllocatePrefetchInstr == 3) {\n@@ -1994,10 +1987,0 @@\n-      } else if (UseSSE >= 1) {\n-        if (AllocatePrefetchInstr == 0) {\n-          log->print(\"PREFETCHNTA\");\n-        } else if (AllocatePrefetchInstr == 1) {\n-          log->print(\"PREFETCHT0\");\n-        } else if (AllocatePrefetchInstr == 2) {\n-          log->print(\"PREFETCHT2\");\n-        } else if (AllocatePrefetchInstr == 3) {\n-          log->print(\"PREFETCHW\");\n-        }\n@@ -3082,0 +3065,4 @@\n+  if (ext_cpuid1_ecx.bits.lzcnt != 0)\n+    result |= CPU_LZCNT;\n+  if (ext_cpuid1_ecx.bits.prefetchw != 0)\n+    result |= CPU_3DNOW_PREFETCH;\n@@ -3100,0 +3087,2 @@\n+  if (sef_cpuid7_ebx.bits.clwb != 0)\n+    result |= CPU_CLWB;\n@@ -3105,1 +3094,1 @@\n-  \/\/ AMD|Hygon features.\n+  \/\/ AMD|Hygon additional features.\n@@ -3107,2 +3096,2 @@\n-    if ((ext_cpuid1_edx.bits.tdnow != 0) ||\n-        (ext_cpuid1_ecx.bits.prefetchw != 0))\n+    \/\/ PREFETCHW was checked above, check TDNOW here.\n+    if ((ext_cpuid1_edx.bits.tdnow != 0))\n@@ -3110,2 +3099,0 @@\n-    if (ext_cpuid1_ecx.bits.lzcnt != 0)\n-      result |= CPU_LZCNT;\n@@ -3116,1 +3103,1 @@\n-  \/\/ Intel features.\n+  \/\/ Intel additional features.\n@@ -3118,9 +3105,0 @@\n-    if (ext_cpuid1_ecx.bits.lzcnt != 0) {\n-      result |= CPU_LZCNT;\n-    }\n-    if (ext_cpuid1_ecx.bits.prefetchw != 0) {\n-      result |= CPU_3DNOW_PREFETCH;\n-    }\n-    if (sef_cpuid7_ebx.bits.clwb != 0) {\n-      result |= CPU_CLWB;\n-    }\n@@ -3129,1 +3107,0 @@\n-\n@@ -3134,1 +3111,1 @@\n-  \/\/ ZX features.\n+  \/\/ ZX additional features.\n@@ -3136,6 +3113,4 @@\n-    if (ext_cpuid1_ecx.bits.lzcnt != 0) {\n-      result |= CPU_LZCNT;\n-    }\n-    if (ext_cpuid1_ecx.bits.prefetchw != 0) {\n-      result |= CPU_3DNOW_PREFETCH;\n-    }\n+    \/\/ We do not know if these are supported by ZX, so we cannot trust\n+    \/\/ common CPUID bit for them.\n+    assert((result & CPU_CLWB) == 0, \"Check if it is supported?\");\n+    result &= ~CPU_CLWB;\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":26,"deletions":51,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -1371,5 +1371,0 @@\n-    case Op_AddReductionVL:\n-      if (UseSSE < 2) { \/\/ requires at least SSE2\n-        return false;\n-      }\n-      break;\n@@ -1559,10 +1554,0 @@\n-    case Op_SignumF:\n-      if (UseSSE < 1) {\n-        return false;\n-      }\n-      break;\n-    case Op_SignumD:\n-      if (UseSSE < 2) {\n-        return false;\n-      }\n-      break;\n@@ -1574,10 +1559,0 @@\n-    case Op_SqrtF:\n-      if (UseSSE < 1) {\n-        return false;\n-      }\n-      break;\n-    case Op_SqrtD:\n-      if (UseSSE < 2) {\n-        return false;\n-      }\n-      break;\n@@ -1634,0 +1609,18 @@\n+    case Op_MaxVHF:\n+    case Op_MinVHF:\n+      if (!VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+    case Op_AddVHF:\n+    case Op_DivVHF:\n+    case Op_FmaVHF:\n+    case Op_MulVHF:\n+    case Op_SubVHF:\n+    case Op_SqrtVHF:\n+      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      if (!VM_Version::supports_avx512_fp16()) {\n+        return false;\n+      }\n+      break;\n@@ -2176,1 +2169,0 @@\n-  if (UseSSE < 2) return 0;\n@@ -2920,1 +2912,1 @@\n-  predicate((UseSSE>=1) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -2932,1 +2924,1 @@\n-  predicate((UseSSE>=1) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -2944,1 +2936,1 @@\n-  predicate((UseSSE>=1) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -2991,1 +2983,1 @@\n-  predicate((UseSSE>=2) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3003,1 +2995,1 @@\n-  predicate((UseSSE>=2) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3015,1 +3007,1 @@\n-  predicate((UseSSE>=2) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3062,1 +3054,1 @@\n-  predicate((UseSSE>=1) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3074,1 +3066,1 @@\n-  predicate((UseSSE>=1) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3086,1 +3078,1 @@\n-  predicate((UseSSE>=1) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3133,1 +3125,1 @@\n-  predicate((UseSSE>=2) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3145,1 +3137,1 @@\n-  predicate((UseSSE>=2) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3157,1 +3149,1 @@\n-  predicate((UseSSE>=2) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3204,1 +3196,1 @@\n-  predicate((UseSSE>=1) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3216,1 +3208,1 @@\n-  predicate((UseSSE>=1) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3228,1 +3220,1 @@\n-  predicate((UseSSE>=1) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3275,1 +3267,1 @@\n-  predicate((UseSSE>=2) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3287,1 +3279,1 @@\n-  predicate((UseSSE>=2) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3299,1 +3291,1 @@\n-  predicate((UseSSE>=2) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3346,1 +3338,1 @@\n-  predicate((UseSSE>=1) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3358,1 +3350,1 @@\n-  predicate((UseSSE>=1) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3370,1 +3362,1 @@\n-  predicate((UseSSE>=1) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3417,1 +3409,1 @@\n-  predicate((UseSSE>=2) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3429,1 +3421,1 @@\n-  predicate((UseSSE>=2) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3441,1 +3433,1 @@\n-  predicate((UseSSE>=2) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3488,1 +3480,1 @@\n-  predicate((UseSSE>=1) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3512,1 +3504,1 @@\n-  predicate((UseSSE>=2) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3538,1 +3530,1 @@\n-  predicate((UseSSE>=1) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3561,1 +3553,1 @@\n-  predicate((UseSSE>=2) && (UseAVX == 0));\n+  predicate(UseAVX == 0);\n@@ -3588,1 +3580,0 @@\n-  predicate(UseSSE>=1);\n@@ -3600,1 +3591,0 @@\n-  predicate(UseSSE>=2);\n@@ -4522,1 +4512,1 @@\n-  predicate(UseSSE >= 2 && Matcher::is_non_long_integral_vector(n));\n+  predicate(Matcher::is_non_long_integral_vector(n));\n@@ -4602,1 +4592,1 @@\n-  predicate(UseSSE >= 2 && Matcher::vector_element_basic_type(n) == T_LONG);\n+  predicate(Matcher::vector_element_basic_type(n) == T_LONG);\n@@ -6974,1 +6964,0 @@\n-      assert(UseSSE >= 2, \"required\");\n@@ -8348,1 +8337,0 @@\n-      assert(UseSSE >= 2, \"required\");\n@@ -10680,0 +10668,10 @@\n+instruct reinterpretHF2S(rRegI dst, regF src)\n+%{\n+  match(Set dst (ReinterpretHF2S src));\n+  format %{ \"vmovw $dst, $src\" %}\n+  ins_encode %{\n+    __ vmovw($dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -10700,10 +10698,0 @@\n-instruct reinterpretHF2S(rRegI dst, regF src)\n-%{\n-  match(Set dst (ReinterpretHF2S src));\n-  format %{ \"vmovw $dst, $src\" %}\n-  ins_encode %{\n-    __ vmovw($dst$$Register, $src$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -10743,1 +10731,1 @@\n-                           $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, Assembler::AVX_128bit);\n+                           $xtmp1$$XMMRegister, $xtmp2$$XMMRegister);\n@@ -10758,0 +10746,91 @@\n+\n+\n+instruct vector_sqrt_HF_reg(vec dst, vec src)\n+%{\n+  match(Set dst (SqrtVHF src));\n+  format %{ \"vector_sqrt_fp16 $dst, $src\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    __ evsqrtph($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vector_sqrt_HF_mem(vec dst, memory src)\n+%{\n+  match(Set dst (SqrtVHF (VectorReinterpret (LoadVector src))));\n+  format %{ \"vector_sqrt_fp16_mem $dst, $src\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    __ evsqrtph($dst$$XMMRegister, $src$$Address, vlen_enc);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vector_binOps_HF_reg(vec dst, vec src1, vec src2)\n+%{\n+  match(Set dst (AddVHF src1 src2));\n+  match(Set dst (DivVHF src1 src2));\n+  match(Set dst (MulVHF src1 src2));\n+  match(Set dst (SubVHF src1 src2));\n+  format %{ \"vector_binop_fp16 $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int opcode = this->ideal_Opcode();\n+    __ evfp16ph(opcode, $dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\n+instruct vector_binOps_HF_mem(vec dst, vec src1, memory src2)\n+%{\n+  match(Set dst (AddVHF src1 (VectorReinterpret (LoadVector src2))));\n+  match(Set dst (DivVHF src1 (VectorReinterpret (LoadVector src2))));\n+  match(Set dst (MulVHF src1 (VectorReinterpret (LoadVector src2))));\n+  match(Set dst (SubVHF src1 (VectorReinterpret (LoadVector src2))));\n+  format %{ \"vector_binop_fp16_mem $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int opcode = this->ideal_Opcode();\n+    __ evfp16ph(opcode, $dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address, vlen_enc);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vector_fma_HF_reg(vec dst, vec src1, vec src2)\n+%{\n+  match(Set dst (FmaVHF src2 (Binary dst src1)));\n+  format %{ \"vector_fma_fp16 $dst, $src1, $src2\\t# $dst = $dst * $src1 + $src2 fma packedH\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    __ evfmadd132ph($dst$$XMMRegister, $src2$$XMMRegister, $src1$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vector_fma_HF_mem(vec dst, memory src1, vec src2)\n+%{\n+  match(Set dst (FmaVHF src2 (Binary dst (VectorReinterpret (LoadVector src1)))));\n+  format %{ \"vector_fma_fp16_mem $dst, $src1, $src2\\t# $dst = $dst * $src1 + $src2 fma packedH\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    __ evfmadd132ph($dst$$XMMRegister, $src2$$XMMRegister, $src1$$Address, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vector_minmax_HF_reg(vec dst, vec src1, vec src2, kReg ktmp, vec xtmp1, vec xtmp2)\n+%{\n+  match(Set dst (MinVHF src1 src2));\n+  match(Set dst (MaxVHF src1 src2));\n+  effect(TEMP_DEF dst, TEMP ktmp, TEMP xtmp1, TEMP xtmp2);\n+  format %{ \"vector_min_max_fp16 $dst, $src1, $src2\\t using $ktmp, $xtmp1 and $xtmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int opcode = this->ideal_Opcode();\n+    __ vector_max_min_fp16(opcode, $dst$$XMMRegister, $src2$$XMMRegister, $src1$$XMMRegister, $ktmp$$KRegister,\n+                           $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":150,"deletions":71,"binary":false,"changes":221,"status":"modified"},{"patch":"@@ -7450,1 +7450,1 @@\n-instruct incL_rReg(rRegI dst, immL1 src, rFlagsReg cr)\n+instruct incL_rReg(rRegL dst, immL1 src, rFlagsReg cr)\n@@ -7463,1 +7463,1 @@\n-instruct incL_rReg_ndd(rRegI dst, rRegI src, immL1 val, rFlagsReg cr)\n+instruct incL_rReg_ndd(rRegL dst, rRegI src, immL1 val, rFlagsReg cr)\n@@ -7476,1 +7476,1 @@\n-instruct incL_rReg_mem_ndd(rRegI dst, memory src, immL1 val, rFlagsReg cr)\n+instruct incL_rReg_mem_ndd(rRegL dst, memory src, immL1 val, rFlagsReg cr)\n@@ -11399,1 +11399,1 @@\n-    __ exorq($dst$$Register, $src1$$Address, $src1$$Register, false);\n+    __ exorq($dst$$Register, $src1$$Address, $src2$$Register, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -3970,2 +3970,2 @@\n-    \"AddVB\", \"AddVS\", \"AddVI\", \"AddVL\", \"AddVF\", \"AddVD\",\n-    \"MulVB\", \"MulVS\", \"MulVI\", \"MulVL\", \"MulVF\", \"MulVD\",\n+    \"AddVB\", \"AddVS\", \"AddVI\", \"AddVL\", \"AddVHF\", \"AddVF\", \"AddVD\",\n+    \"MulVB\", \"MulVS\", \"MulVI\", \"MulVL\", \"MulVHF\", \"MulVF\", \"MulVD\",\n@@ -3973,1 +3973,1 @@\n-    \"MaxV\", \"MinV\", \"UMax\",\"UMin\"\n+    \"MaxVHF\", \"MinVHF\", \"MaxV\", \"MinV\", \"UMax\",\"UMin\"\n@@ -4340,4 +4340,4 @@\n-    \"AddVB\",\"AddVS\",\"AddVI\",\"AddVL\",\"AddVF\",\"AddVD\",\n-    \"SubVB\",\"SubVS\",\"SubVI\",\"SubVL\",\"SubVF\",\"SubVD\",\n-    \"MulVB\",\"MulVS\",\"MulVI\",\"MulVL\",\"MulVF\",\"MulVD\",\n-    \"DivVF\",\"DivVD\",\n+    \"AddVB\",\"AddVS\",\"AddVI\",\"AddVL\",\"AddVHF\",\"AddVF\",\"AddVD\",\n+    \"SubVB\",\"SubVS\",\"SubVI\",\"SubVL\",\"SubVHF\",\"SubVF\",\"SubVD\",\n+    \"MulVB\",\"MulVS\",\"MulVI\",\"MulVL\",\"MulVHF\",\"MulVF\",\"MulVD\",\n+    \"DivVHF\",\"DivVF\",\"DivVD\",\n@@ -4346,1 +4346,1 @@\n-    \"SqrtVD\",\"SqrtVF\",\n+    \"SqrtVD\",\"SqrtVF\",\"SqrtVHF\",\n@@ -4348,1 +4348,1 @@\n-    \"MaxV\", \"MinV\", \"UMinV\", \"UMaxV\",\n+    \"MaxV\", \"MinV\", \"MinVHF\", \"MaxVHF\", \"UMinV\", \"UMaxV\",\n@@ -4370,1 +4370,1 @@\n-    \"FmaVD\",\"FmaVF\",\"PopCountVI\",\"PopCountVL\",\"PopulateIndex\",\"VectorLongToMask\",\n+    \"FmaVD\", \"FmaVF\", \"FmaVHF\", \"PopCountVI\", \"PopCountVL\", \"PopulateIndex\", \"VectorLongToMask\",\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -1090,33 +1090,13 @@\n-    if (IA32_ONLY( (UseSSE == 1 && opr_type == T_FLOAT) || UseSSE >= 2 ) NOT_IA32( true )) {\n-      \/\/ SSE float instruction (T_DOUBLE only supported with SSE2)\n-      switch (op->code()) {\n-        case lir_cmp:\n-        case lir_add:\n-        case lir_sub:\n-        case lir_mul:\n-        case lir_div:\n-        {\n-          assert(op->as_Op2() != nullptr, \"must be LIR_Op2\");\n-          LIR_Op2* op2 = (LIR_Op2*)op;\n-          if (op2->in_opr1() != op2->in_opr2() && op2->in_opr2() == opr) {\n-            assert((op2->result_opr()->is_register() || op->code() == lir_cmp) && op2->in_opr1()->is_register(), \"cannot mark second operand as stack if others are not in register\");\n-            return shouldHaveRegister;\n-          }\n-        }\n-        default:\n-          break;\n-      }\n-    } else {\n-      \/\/ FPU stack float instruction\n-      switch (op->code()) {\n-        case lir_add:\n-        case lir_sub:\n-        case lir_mul:\n-        case lir_div:\n-        {\n-          assert(op->as_Op2() != nullptr, \"must be LIR_Op2\");\n-          LIR_Op2* op2 = (LIR_Op2*)op;\n-          if (op2->in_opr1() != op2->in_opr2() && op2->in_opr2() == opr) {\n-            assert((op2->result_opr()->is_register() || op->code() == lir_cmp) && op2->in_opr1()->is_register(), \"cannot mark second operand as stack if others are not in register\");\n-            return shouldHaveRegister;\n-          }\n+    \/\/ SSE float instruction\n+    switch (op->code()) {\n+      case lir_cmp:\n+      case lir_add:\n+      case lir_sub:\n+      case lir_mul:\n+      case lir_div:\n+      {\n+        assert(op->as_Op2() != nullptr, \"must be LIR_Op2\");\n+        LIR_Op2* op2 = (LIR_Op2*)op;\n+        if (op2->in_opr1() != op2->in_opr2() && op2->in_opr2() == opr) {\n+          assert((op2->result_opr()->is_register() || op->code() == lir_cmp) && op2->in_opr1()->is_register(), \"cannot mark second operand as stack if others are not in register\");\n+          return shouldHaveRegister;\n@@ -1124,2 +1104,2 @@\n-        default:\n-          break;\n+      default:\n+        break;\n@@ -1287,10 +1267,6 @@\n-#ifdef X86\n-    if (UseSSE < 2) {\n-#endif \/\/ X86\n-      for (i = 0; i < FrameMap::nof_caller_save_fpu_regs; i++) {\n-        LIR_Opr opr = FrameMap::caller_save_fpu_reg_at(i);\n-        assert(opr->is_valid() && opr->is_register(), \"FrameMap should not return invalid operands\");\n-        assert(reg_numHi(opr) == -1, \"missing addition of range for hi-register\");\n-        caller_save_registers[num_caller_save_registers++] = reg_num(opr);\n-      }\n-#ifdef X86\n+#ifndef X86\n+    for (i = 0; i < FrameMap::nof_caller_save_fpu_regs; i++) {\n+      LIR_Opr opr = FrameMap::caller_save_fpu_reg_at(i);\n+      assert(opr->is_valid() && opr->is_register(), \"FrameMap should not return invalid operands\");\n+      assert(reg_numHi(opr) == -1, \"missing addition of range for hi-register\");\n+      caller_save_registers[num_caller_save_registers++] = reg_num(opr);\n@@ -1298,11 +1274,7 @@\n-#endif \/\/ X86\n-\n-#ifdef X86\n-    if (UseSSE > 0) {\n-      int num_caller_save_xmm_regs = FrameMap::get_num_caller_save_xmms();\n-      for (i = 0; i < num_caller_save_xmm_regs; i ++) {\n-        LIR_Opr opr = FrameMap::caller_save_xmm_reg_at(i);\n-        assert(opr->is_valid() && opr->is_register(), \"FrameMap should not return invalid operands\");\n-        assert(reg_numHi(opr) == -1, \"missing addition of range for hi-register\");\n-        caller_save_registers[num_caller_save_registers++] = reg_num(opr);\n-      }\n+#else\n+    int num_caller_save_xmm_regs = FrameMap::get_num_caller_save_xmms();\n+    for (i = 0; i < num_caller_save_xmm_regs; i ++) {\n+      LIR_Opr opr = FrameMap::caller_save_xmm_reg_at(i);\n+      assert(opr->is_valid() && opr->is_register(), \"FrameMap should not return invalid operands\");\n+      assert(reg_numHi(opr) == -1, \"missing addition of range for hi-register\");\n+      caller_save_registers[num_caller_save_registers++] = reg_num(opr);\n@@ -2155,10 +2127,3 @@\n-        if (UseSSE >= 1) {\n-          int last_xmm_reg = pd_last_xmm_reg;\n-#ifdef _LP64\n-          if (UseAVX < 3) {\n-            last_xmm_reg = pd_first_xmm_reg + (pd_nof_xmm_regs_frame_map \/ 2) - 1;\n-          }\n-#endif \/\/ LP64\n-          assert(assigned_reg >= pd_first_xmm_reg && assigned_reg <= last_xmm_reg, \"no xmm register\");\n-          assert(interval->assigned_regHi() == any_reg, \"must not have hi register\");\n-          return LIR_OprFact::single_xmm(assigned_reg - pd_first_xmm_reg);\n+        int last_xmm_reg = pd_last_xmm_reg;\n+        if (UseAVX < 3) {\n+          last_xmm_reg = pd_first_xmm_reg + (pd_nof_xmm_regs_frame_map \/ 2) - 1;\n@@ -2166,2 +2131,4 @@\n-#endif \/\/ X86\n-\n+        assert(assigned_reg >= pd_first_xmm_reg && assigned_reg <= last_xmm_reg, \"no xmm register\");\n+        assert(interval->assigned_regHi() == any_reg, \"must not have hi register\");\n+        return LIR_OprFact::single_xmm(assigned_reg - pd_first_xmm_reg);\n+#else\n@@ -2171,0 +2138,1 @@\n+#endif \/\/ !X86\n@@ -2174,11 +2142,4 @@\n-#ifdef X86\n-        if (UseSSE >= 2) {\n-          int last_xmm_reg = pd_last_xmm_reg;\n-#ifdef _LP64\n-          if (UseAVX < 3) {\n-            last_xmm_reg = pd_first_xmm_reg + (pd_nof_xmm_regs_frame_map \/ 2) - 1;\n-          }\n-#endif \/\/ LP64\n-          assert(assigned_reg >= pd_first_xmm_reg && assigned_reg <= last_xmm_reg, \"no xmm register\");\n-          assert(interval->assigned_regHi() == any_reg, \"must not have hi register (double xmm values are stored in one register)\");\n-          return LIR_OprFact::double_xmm(assigned_reg - pd_first_xmm_reg);\n+#if defined(X86)\n+        int last_xmm_reg = pd_last_xmm_reg;\n+        if (UseAVX < 3) {\n+          last_xmm_reg = pd_first_xmm_reg + (pd_nof_xmm_regs_frame_map \/ 2) - 1;\n@@ -2186,3 +2147,4 @@\n-#endif \/\/ X86\n-\n-#if defined(ARM32)\n+        assert(assigned_reg >= pd_first_xmm_reg && assigned_reg <= last_xmm_reg, \"no xmm register\");\n+        assert(interval->assigned_regHi() == any_reg, \"must not have hi register (double xmm values are stored in one register)\");\n+        LIR_Opr result = LIR_OprFact::double_xmm(assigned_reg - pd_first_xmm_reg);\n+#elif defined(ARM32)\n","filename":"src\/hotspot\/share\/c1\/c1_LinearScan.cpp","additions":44,"deletions":82,"binary":false,"changes":126,"status":"modified"},{"patch":"@@ -308,0 +308,14 @@\n+bool ClassPathZipEntry::has_entry(JavaThread* current, const char* name) {\n+  ThreadToNativeFromVM ttn(current);\n+  \/\/ check whether zip archive contains name\n+  jint name_len;\n+  jint filesize;\n+  jzentry* entry = ZipLibrary::find_entry(_zip, name, &filesize, &name_len);\n+  if (entry == nullptr) {\n+    return false;\n+  } else {\n+     ZipLibrary::free_entry(_zip, entry);\n+    return true;\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/classLoader.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -498,0 +498,8 @@\n+  case vmIntrinsics::_kyberNtt:\n+  case vmIntrinsics::_kyberInverseNtt:\n+  case vmIntrinsics::_kyberNttMult:\n+  case vmIntrinsics::_kyberAddPoly_2:\n+  case vmIntrinsics::_kyberAddPoly_3:\n+  case vmIntrinsics::_kyber12To16:\n+  case vmIntrinsics::_kyberBarrettReduce:\n+    if (!UseKyberIntrinsics) return true;\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -580,0 +580,21 @@\n+  \/* support for com.sun.crypto.provider.ML_KEM *\/                                                                      \\\n+  do_class(com_sun_crypto_provider_ML_KEM,      \"com\/sun\/crypto\/provider\/ML_KEM\")                                       \\\n+   do_signature(SaSaSaSaI_signature, \"([S[S[S[S)I\")                                                                     \\\n+   do_signature(BaISaII_signature, \"([BI[SI)I\")                                                                         \\\n+   do_signature(SaSaSaI_signature, \"([S[S[S)I\")                                                                         \\\n+   do_signature(SaSaI_signature, \"([S[S)I\")                                                                             \\\n+   do_signature(SaI_signature, \"([S)I\")                                                                                 \\\n+   do_name(kyberAddPoly_name,                             \"implKyberAddPoly\")                                           \\\n+  do_intrinsic(_kyberNtt, com_sun_crypto_provider_ML_KEM, kyberNtt_name, SaSaI_signature, F_S)                          \\\n+   do_name(kyberNtt_name,                                  \"implKyberNtt\")                                              \\\n+  do_intrinsic(_kyberInverseNtt, com_sun_crypto_provider_ML_KEM, kyberInverseNtt_name, SaSaI_signature, F_S)            \\\n+   do_name(kyberInverseNtt_name,                           \"implKyberInverseNtt\")                                       \\\n+  do_intrinsic(_kyberNttMult, com_sun_crypto_provider_ML_KEM, kyberNttMult_name, SaSaSaSaI_signature, F_S)              \\\n+   do_name(kyberNttMult_name,                              \"implKyberNttMult\")                                          \\\n+  do_intrinsic(_kyberAddPoly_2, com_sun_crypto_provider_ML_KEM, kyberAddPoly_name, SaSaSaI_signature, F_S)              \\\n+  do_intrinsic(_kyberAddPoly_3, com_sun_crypto_provider_ML_KEM, kyberAddPoly_name, SaSaSaSaI_signature, F_S)            \\\n+  do_intrinsic(_kyber12To16, com_sun_crypto_provider_ML_KEM, kyber12To16_name, BaISaII_signature, F_S)                  \\\n+   do_name(kyber12To16_name,                             \"implKyber12To16\")                                             \\\n+  do_intrinsic(_kyberBarrettReduce, com_sun_crypto_provider_ML_KEM, kyberBarrettReduce_name, SaI_signature, F_S)        \\\n+   do_name(kyberBarrettReduce_name,                        \"implKyberBarrettReduce\")                                    \\\n+                                                                                                                        \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":21,"deletions":0,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -2256,0 +2256,10 @@\n+  if (obj->is_array()) {\n+    \/\/ Disallow reading after the last element of an array\n+    size_t array_length = arrayOop(obj())->length();\n+    int lh = obj->klass()->layout_helper();\n+    size_t size_in_bytes = array_length << Klass::layout_helper_log2_element_size(lh);\n+    size_in_bytes += Klass::layout_helper_header_size(lh);\n+    if ((size_t) displacement + basic_type_elemsize > size_in_bytes) {\n+      JVMCI_THROW_MSG_NULL(IllegalArgumentException, \"reading after last array element\");\n+    }\n+  }\n@@ -2261,3 +2271,0 @@\n-      if (displacement + heapOopSize > arrayOopDesc::base_offset_in_bytes(T_OBJECT) + arrayOop(obj())->length() * heapOopSize) {\n-        JVMCI_THROW_MSG_NULL(IllegalArgumentException, \"reading after last array element\");\n-      }\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -398,0 +398,7 @@\n+  static_field(StubRoutines,                _kyberNtt,                                        address)                               \\\n+  static_field(StubRoutines,                _kyberInverseNtt,                                 address)                               \\\n+  static_field(StubRoutines,                _kyberNttMult,                                    address)                               \\\n+  static_field(StubRoutines,                _kyberAddPoly_2,                                  address)                               \\\n+  static_field(StubRoutines,                _kyberAddPoly_3,                                  address)                               \\\n+  static_field(StubRoutines,                _kyber12To16,                                     address)                               \\\n+  static_field(StubRoutines,                _kyberBarrettReduce,                              address)                               \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -136,1 +136,1 @@\n-  FieldInfo to_FieldInfo() {\n+  const FieldInfo& to_FieldInfo() const {\n@@ -147,1 +147,1 @@\n-    field.reinitialize(field_holder(), _index);\n+    field.reinitialize(field_holder(), to_FieldInfo());\n","filename":"src\/hotspot\/share\/oops\/fieldStreams.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -102,1 +102,0 @@\n-#include \"utilities\/pair.hpp\"\n@@ -1976,1 +1975,1 @@\n-      fd->reinitialize(const_cast<InstanceKlass*>(this), fs.index());\n+      fd->reinitialize(const_cast<InstanceKlass*>(this), fs.to_FieldInfo());\n@@ -2054,1 +2053,1 @@\n-      fd->reinitialize(const_cast<InstanceKlass*>(this), fs.index());\n+      fd->reinitialize(const_cast<InstanceKlass*>(this), fs.to_FieldInfo());\n@@ -2115,4 +2114,2 @@\n-  fieldDescriptor fd;\n-  int length = java_fields_count();\n-  for (int i = 0; i < length; i += 1) {\n-    fd.reinitialize(this, i);\n+  for (JavaFieldStream fs(this); !fs.done(); fs.next()) {\n+    fieldDescriptor& fd = fs.field_descriptor();\n@@ -2125,3 +2122,2 @@\n-\/\/ first in Pair is offset, second is index.\n-static int compare_fields_by_offset(Pair<int,int>* a, Pair<int,int>* b) {\n-  return a->first - b->first;\n+static int compare_fields_by_offset(FieldInfo* a, FieldInfo* b) {\n+  return a->offset() - b->offset();\n@@ -2136,3 +2132,1 @@\n-  fieldDescriptor fd;\n-  GrowableArray<Pair<int,int> > fields_sorted;\n-  int i = 0;\n+  GrowableArray<FieldInfo> fields_sorted;\n@@ -2142,4 +2136,1 @@\n-      fd = fs.field_descriptor();\n-      Pair<int,int> f(fs.offset(), fs.index());\n-      fields_sorted.push(f);\n-      i++;\n+      fields_sorted.push(fs.to_FieldInfo());\n@@ -2148,3 +2139,2 @@\n-  if (i > 0) {\n-    int length = i;\n-    assert(length == fields_sorted.length(), \"duh\");\n+  int length = fields_sorted.length();\n+  if (length > 0) {\n@@ -2152,0 +2142,1 @@\n+    fieldDescriptor fd;\n@@ -2153,2 +2144,2 @@\n-      fd.reinitialize(this, fields_sorted.at(i).second);\n-      assert(!fd.is_static() && fd.offset() == fields_sorted.at(i).first, \"only nonstatic fields\");\n+      fd.reinitialize(this, fields_sorted.at(i));\n+      assert(!fd.is_static() && fd.offset() == checked_cast<int>(fields_sorted.at(i).offset()), \"only nonstatic fields\");\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":13,"deletions":22,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -1194,0 +1194,8 @@\n+\/\/ Check if addition of a long with type 't' and a constant 'c' can overflow.\n+static bool can_overflow(const TypeLong* t, jlong c) {\n+  jlong t_lo = t->_lo;\n+  jlong t_hi = t->_hi;\n+  return ((c < 0 && (java_add(t_lo, c) > t_lo)) ||\n+          (c > 0 && (java_add(t_hi, c) < t_hi)));\n+}\n+\n@@ -1378,0 +1386,25 @@\n+\/\/\n+\/\/ Proof MaxL collapsed version equivalent to original (MinL version similar):\n+\/\/ is_sub_con ensures that con1, con2  [min_int, 0[\n+\/\/\n+\/\/ Original:\n+\/\/ - AddL2 underflow => x + con2  ]max_long - min_int, max_long], ALWAYS BAILOUT as x + con1 + con2 surely fails can_overflow (*)\n+\/\/ - AddL2 no underflow => x + con2  [min_long, max_long]\n+\/\/   - MaxL2 clamp => min_int\n+\/\/     - AddL1 underflow: NOT POSSIBLE: cannot underflow since min_int + con1  [2 * min_int, min_int] always > min_long\n+\/\/     - AddL1 no underflow => min_int + con1  [2 * min_int, min_int]\n+\/\/       - MaxL1 clamp => min_int (RESULT 1)\n+\/\/       - MaxL1 no clamp: NOT POSSIBLE: min_int + con1  [2 * min_int, min_int] always <= min_int, so clamp always taken\n+\/\/   - MaxL2 no clamp => x + con2  [min_int, max_long]\n+\/\/     - AddL1 underflow: NOT POSSIBLE: cannot underflow since x + con2 + con1  [2 * min_int, max_long] always > min_long\n+\/\/     - AddL1 no underflow => x + con2 + con1  [2 * min_int, max_long]\n+\/\/       - MaxL1 clamp => min_int (RESULT 2)\n+\/\/       - MaxL1 no clamp => x + con2 + con1  ]min_int, max_long] (RESULT 3)\n+\/\/\n+\/\/ Collapsed:\n+\/\/ - AddL2 (cannot underflow) => con2 + con1  [2 * min_int, 0]\n+\/\/   - AddL1 underflow: NOT POSSIBLE: would have bailed out at can_overflow (*)\n+\/\/   - AddL1 no underflow => x + con2 + con1  [min_long, max_long]\n+\/\/     - MaxL clamp => min_int (RESULT 1 and RESULT 2)\n+\/\/     - MaxL no clamp => x + con2 + con1  ]min_int, max_long] (RESULT 3)\n+\/\/\n@@ -1407,0 +1440,4 @@\n+          \/\/ Collapsed graph not equivalent if potential over\/underflow -> bailing out (*)\n+          if (can_overflow(phase->type(x)->is_long(), con1->get_long() + con2->get_long())) {\n+            return nullptr;\n+          }\n","filename":"src\/hotspot\/share\/opto\/addnode.cpp","additions":37,"deletions":0,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -804,0 +804,7 @@\n+  case vmIntrinsics::_kyberNtt:\n+  case vmIntrinsics::_kyberInverseNtt:\n+  case vmIntrinsics::_kyberNttMult:\n+  case vmIntrinsics::_kyberAddPoly_2:\n+  case vmIntrinsics::_kyberAddPoly_3:\n+  case vmIntrinsics::_kyber12To16:\n+  case vmIntrinsics::_kyberBarrettReduce:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2595,1 +2595,0 @@\n-        Node* hook = new Node(1);\n@@ -2599,1 +2598,0 @@\n-        hook->add_req(new_base);\n@@ -2618,1 +2616,0 @@\n-                hook->add_req(new_phi);\n@@ -2636,13 +2633,6 @@\n-        \/\/ Already replace this phi node to cut it off from the graph to not interfere in dead loop checks during the\n-        \/\/ transformations of the new phi nodes below. Otherwise, we could wrongly conclude that there is no dead loop\n-        \/\/ because we are finding this phi node again. Also set the type of the new MergeMem node in case we are also\n-        \/\/ visiting it in the transformations below.\n-        igvn->replace_node(this, result);\n-        igvn->set_type(result, result->bottom_type());\n-\n-        \/\/ now transform the new nodes, and return the mergemem\n-        for (MergeMemStream mms(result); mms.next_non_empty(); ) {\n-          Node* phi = mms.memory();\n-          mms.set_memory(phase->transform(phi));\n-        }\n-        hook->destruct(igvn);\n+\n+        \/\/ We could immediately transform the new Phi nodes here, but that can\n+        \/\/ result in creating an excessive number of new nodes within a single\n+        \/\/ IGVN iteration. We have put the Phi nodes on the IGVN worklist, so\n+        \/\/ they are transformed later on in any case.\n+\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":6,"deletions":16,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -396,0 +396,1 @@\n+macro(AddVHF)\n@@ -404,0 +405,1 @@\n+macro(SubVHF)\n@@ -415,0 +417,1 @@\n+macro(MulVHF)\n@@ -418,0 +421,2 @@\n+macro(FmaVHF)\n+macro(DivVHF)\n@@ -432,0 +437,1 @@\n+macro(SqrtVHF)\n@@ -454,0 +460,2 @@\n+macro(MinVHF)\n+macro(MaxVHF)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1042,2 +1042,0 @@\n-  IA32_ONLY( set_24_bit_selection_and_mode(true, false); )\n-\n@@ -1930,0 +1928,3 @@\n+  if (has_loops() || _loop_opts_cnt > 0) {\n+    print_method(PHASE_AFTER_LOOP_OPTS, 2);\n+  }\n@@ -2868,0 +2869,4 @@\n+  if (has_loops()) {\n+    print_method(PHASE_BEFORE_LOOP_OPTS, 2);\n+  }\n+\n@@ -4610,11 +4615,0 @@\n-#ifdef IA32\n-  \/\/ If original bytecodes contained a mixture of floats and doubles\n-  \/\/ check if the optimizer has made it homogeneous, item (3).\n-  if (UseSSE == 0 &&\n-      frc.get_float_count() > 32 &&\n-      frc.get_double_count() == 0 &&\n-      (10 * frc.get_call_count() < frc.get_float_count()) ) {\n-    set_24_bit_selection_and_mode(false, true);\n-  }\n-#endif \/\/ IA32\n-\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":7,"deletions":13,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -1331,15 +1331,0 @@\n-#ifdef IA32\n- private:\n-  bool _select_24_bit_instr;   \/\/ We selected an instruction with a 24-bit result\n-  bool _in_24_bit_fp_mode;     \/\/ We are emitting instructions with 24-bit results\n-\n-  \/\/ Remember if this compilation changes hardware mode to 24-bit precision.\n-  void set_24_bit_selection_and_mode(bool selection, bool mode) {\n-    _select_24_bit_instr = selection;\n-    _in_24_bit_fp_mode   = mode;\n-  }\n-\n- public:\n-  bool select_24_bit_instr() const { return _select_24_bit_instr; }\n-  bool in_24_bit_fp_mode() const   { return _in_24_bit_fp_mode; }\n-#endif \/\/ IA32\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":0,"deletions":15,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1322,0 +1322,5 @@\n+  \/\/ Mod by zero?  Throw an exception at runtime!\n+  if (type_divisor->is_con() && type_divisor->get_con() == 0) {\n+    return TypeClass::POS;\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/divnode.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2245,0 +2245,7 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberNtt\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberInverseNtt\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberNttMult\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberAddPoly_2\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberAddPoly_3\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyber12To16\") == 0 ||\n+                  strcmp(call->as_CallLeaf()->_name, \"kyberBarrettReduce\") == 0 ||\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -638,0 +638,14 @@\n+  case vmIntrinsics::_kyberNtt:\n+    return inline_kyberNtt();\n+  case vmIntrinsics::_kyberInverseNtt:\n+    return inline_kyberInverseNtt();\n+  case vmIntrinsics::_kyberNttMult:\n+    return inline_kyberNttMult();\n+  case vmIntrinsics::_kyberAddPoly_2:\n+    return inline_kyberAddPoly_2();\n+  case vmIntrinsics::_kyberAddPoly_3:\n+    return inline_kyberAddPoly_3();\n+  case vmIntrinsics::_kyber12To16:\n+    return inline_kyber12To16();\n+  case vmIntrinsics::_kyberBarrettReduce:\n+    return inline_kyberBarrettReduce();\n@@ -8103,0 +8117,239 @@\n+\/\/------------------------------inline_kyberNtt\n+bool LibraryCallKit::inline_kyberNtt() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 2, \"kyberNtt has 2 parameters\");\n+\n+  stubAddr = StubRoutines::kyberNtt();\n+  stubName = \"kyberNtt\";\n+  if (!stubAddr) return false;\n+\n+  Node* coeffs          = argument(0);\n+  Node* ntt_zetas        = argument(1);\n+\n+  coeffs = must_be_not_null(coeffs, true);\n+  ntt_zetas = must_be_not_null(ntt_zetas, true);\n+\n+  Node* coeffs_start  = array_element_address(coeffs, intcon(0), T_SHORT);\n+  assert(coeffs_start, \"coeffs is null\");\n+  Node* ntt_zetas_start  = array_element_address(ntt_zetas, intcon(0), T_SHORT);\n+  assert(ntt_zetas_start, \"ntt_zetas is null\");\n+  Node* kyberNtt = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberNtt_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  coeffs_start, ntt_zetas_start);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberNtt, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\/\/------------------------------inline_kyberInverseNtt\n+bool LibraryCallKit::inline_kyberInverseNtt() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 2, \"kyberInverseNtt has 2 parameters\");\n+\n+  stubAddr = StubRoutines::kyberInverseNtt();\n+  stubName = \"kyberInverseNtt\";\n+  if (!stubAddr) return false;\n+\n+  Node* coeffs          = argument(0);\n+  Node* zetas           = argument(1);\n+\n+  coeffs = must_be_not_null(coeffs, true);\n+  zetas = must_be_not_null(zetas, true);\n+\n+  Node* coeffs_start  = array_element_address(coeffs, intcon(0), T_SHORT);\n+  assert(coeffs_start, \"coeffs is null\");\n+  Node* zetas_start  = array_element_address(zetas, intcon(0), T_SHORT);\n+  assert(zetas_start, \"inverseNtt_zetas is null\");\n+  Node* kyberInverseNtt = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberInverseNtt_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  coeffs_start, zetas_start);\n+\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberInverseNtt, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\/\/------------------------------inline_kyberNttMult\n+bool LibraryCallKit::inline_kyberNttMult() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 4, \"kyberNttMult has 4 parameters\");\n+\n+  stubAddr = StubRoutines::kyberNttMult();\n+  stubName = \"kyberNttMult\";\n+  if (!stubAddr) return false;\n+\n+  Node* result          = argument(0);\n+  Node* ntta            = argument(1);\n+  Node* nttb            = argument(2);\n+  Node* zetas           = argument(3);\n+\n+  result = must_be_not_null(result, true);\n+  ntta = must_be_not_null(ntta, true);\n+  nttb = must_be_not_null(nttb, true);\n+  zetas = must_be_not_null(zetas, true);\n+  Node* result_start  = array_element_address(result, intcon(0), T_SHORT);\n+  assert(result_start, \"result is null\");\n+  Node* ntta_start  = array_element_address(ntta, intcon(0), T_SHORT);\n+  assert(ntta_start, \"ntta is null\");\n+  Node* nttb_start  = array_element_address(nttb, intcon(0), T_SHORT);\n+  assert(nttb_start, \"nttb is null\");\n+  Node* zetas_start  = array_element_address(zetas, intcon(0), T_SHORT);\n+  assert(zetas_start, \"nttMult_zetas is null\");\n+  Node* kyberNttMult = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberNttMult_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  result_start, ntta_start, nttb_start,\n+                                  zetas_start);\n+\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberNttMult, TypeFunc::Parms));\n+  set_result(retvalue);\n+\n+  return true;\n+}\n+\n+\/\/------------------------------inline_kyberAddPoly_2\n+bool LibraryCallKit::inline_kyberAddPoly_2() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 3, \"kyberAddPoly_2 has 3 parameters\");\n+\n+  stubAddr = StubRoutines::kyberAddPoly_2();\n+  stubName = \"kyberAddPoly_2\";\n+  if (!stubAddr) return false;\n+\n+  Node* result          = argument(0);\n+  Node* a               = argument(1);\n+  Node* b               = argument(2);\n+\n+  result = must_be_not_null(result, true);\n+  a = must_be_not_null(a, true);\n+  b = must_be_not_null(b, true);\n+\n+  Node* result_start  = array_element_address(result, intcon(0), T_SHORT);\n+  assert(result_start, \"result is null\");\n+  Node* a_start  = array_element_address(a, intcon(0), T_SHORT);\n+  assert(a_start, \"a is null\");\n+  Node* b_start  = array_element_address(b, intcon(0), T_SHORT);\n+  assert(b_start, \"b is null\");\n+  Node* kyberAddPoly_2 = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberAddPoly_2_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  result_start, a_start, b_start);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberAddPoly_2, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\/\/------------------------------inline_kyberAddPoly_3\n+bool LibraryCallKit::inline_kyberAddPoly_3() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 4, \"kyberAddPoly_3 has 4 parameters\");\n+\n+  stubAddr = StubRoutines::kyberAddPoly_3();\n+  stubName = \"kyberAddPoly_3\";\n+  if (!stubAddr) return false;\n+\n+  Node* result          = argument(0);\n+  Node* a               = argument(1);\n+  Node* b               = argument(2);\n+  Node* c               = argument(3);\n+\n+  result = must_be_not_null(result, true);\n+  a = must_be_not_null(a, true);\n+  b = must_be_not_null(b, true);\n+  c = must_be_not_null(c, true);\n+\n+  Node* result_start  = array_element_address(result, intcon(0), T_SHORT);\n+  assert(result_start, \"result is null\");\n+  Node* a_start  = array_element_address(a, intcon(0), T_SHORT);\n+  assert(a_start, \"a is null\");\n+  Node* b_start  = array_element_address(b, intcon(0), T_SHORT);\n+  assert(b_start, \"b is null\");\n+  Node* c_start  = array_element_address(c, intcon(0), T_SHORT);\n+  assert(c_start, \"c is null\");\n+  Node* kyberAddPoly_3 = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberAddPoly_3_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  result_start, a_start, b_start, c_start);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberAddPoly_3, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n+\/\/------------------------------inline_kyber12To16\n+bool LibraryCallKit::inline_kyber12To16() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 4, \"kyber12To16 has 4 parameters\");\n+\n+  stubAddr = StubRoutines::kyber12To16();\n+  stubName = \"kyber12To16\";\n+  if (!stubAddr) return false;\n+\n+  Node* condensed       = argument(0);\n+  Node* condensedOffs   = argument(1);\n+  Node* parsed          = argument(2);\n+  Node* parsedLength    = argument(3);\n+\n+  condensed = must_be_not_null(condensed, true);\n+  parsed = must_be_not_null(parsed, true);\n+\n+  Node* condensed_start  = array_element_address(condensed, intcon(0), T_BYTE);\n+  assert(condensed_start, \"condensed is null\");\n+  Node* parsed_start  = array_element_address(parsed, intcon(0), T_SHORT);\n+  assert(parsed_start, \"parsed is null\");\n+  Node* kyber12To16 = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyber12To16_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  condensed_start, condensedOffs, parsed_start, parsedLength);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyber12To16, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+\n+}\n+\n+\/\/------------------------------inline_kyberBarrettReduce\n+bool LibraryCallKit::inline_kyberBarrettReduce() {\n+  address stubAddr;\n+  const char *stubName;\n+  assert(UseKyberIntrinsics, \"need Kyber intrinsics support\");\n+  assert(callee()->signature()->size() == 1, \"kyberBarrettReduce has 1 parameters\");\n+\n+  stubAddr = StubRoutines::kyberBarrettReduce();\n+  stubName = \"kyberBarrettReduce\";\n+  if (!stubAddr) return false;\n+\n+  Node* coeffs          = argument(0);\n+\n+  coeffs = must_be_not_null(coeffs, true);\n+\n+  Node* coeffs_start  = array_element_address(coeffs, intcon(0), T_SHORT);\n+  assert(coeffs_start, \"coeffs is null\");\n+  Node* kyberBarrettReduce = make_runtime_call(RC_LEAF|RC_NO_FP,\n+                                  OptoRuntime::kyberBarrettReduce_Type(),\n+                                  stubAddr, stubName, TypePtr::BOTTOM,\n+                                  coeffs_start);\n+  \/\/ return an int\n+  Node* retvalue = _gvn.transform(new ProjNode(kyberBarrettReduce, TypeFunc::Parms));\n+  set_result(retvalue);\n+  return true;\n+}\n+\n@@ -8159,1 +8412,0 @@\n-\n@@ -8180,0 +8432,1 @@\n+  Node* zetas           = argument(3);\n@@ -8184,0 +8437,1 @@\n+  zetas = must_be_not_null(zetas, true);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":255,"deletions":1,"binary":false,"changes":256,"status":"modified"},{"patch":"@@ -342,0 +342,7 @@\n+  bool inline_kyberNtt();\n+  bool inline_kyberInverseNtt();\n+  bool inline_kyberNttMult();\n+  bool inline_kyberAddPoly_2();\n+  bool inline_kyberAddPoly_3();\n+  bool inline_kyber12To16();\n+  bool inline_kyberBarrettReduce();\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2351,0 +2351,1 @@\n+    case Op_FmaVHF:\n@@ -2523,1 +2524,2 @@\n-    case Op_FmaVF: {\n+    case Op_FmaVF:\n+    case Op_FmaVHF: {\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1027,0 +1027,7 @@\n+  \/\/ The node count check in the loop below (check_node_count) assumes that we\n+  \/\/ increase the live node count with at most\n+  \/\/ max_live_nodes_increase_per_iteration in between checks. If this\n+  \/\/ assumption does not hold, there is a risk that we exceed the max node\n+  \/\/ limit in between checks and trigger an assert during node creation.\n+  const int max_live_nodes_increase_per_iteration = NodeLimitFudgeFactor * 2;\n+\n@@ -1030,2 +1037,2 @@\n-  while(_worklist.size()) {\n-    if (C->check_node_count(NodeLimitFudgeFactor * 2, \"Out of nodes\")) {\n+  while (_worklist.size() > 0) {\n+    if (C->check_node_count(max_live_nodes_increase_per_iteration, \"Out of nodes\")) {\n@@ -1046,0 +1053,1 @@\n+      DEBUG_ONLY(int live_nodes_before = C->live_nodes();)\n@@ -1047,0 +1055,8 @@\n+      DEBUG_ONLY(int live_nodes_after = C->live_nodes();)\n+      \/\/ Ensure we did not increase the live node count with more than\n+      \/\/ max_live_nodes_increase_per_iteration during the call to transform_old\n+      DEBUG_ONLY(int increase = live_nodes_after - live_nodes_before;)\n+      assert(increase < max_live_nodes_increase_per_iteration,\n+             \"excessive live node increase in single iteration of IGVN: %d \"\n+             \"(should be at most %d)\",\n+             increase, max_live_nodes_increase_per_iteration);\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":18,"deletions":2,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -51,0 +51,1 @@\n+  flags(BEFORE_LOOP_OPTS,               \"Before Loop Optimizations\") \\\n@@ -91,0 +92,1 @@\n+  flags(AFTER_LOOP_OPTS,                \"After Loop Optimizations\") \\\n","filename":"src\/hotspot\/share\/opto\/phasetype.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -248,1 +248,7 @@\n-\n+const TypeFunc* OptoRuntime::_kyberNtt_Type                       = nullptr;\n+const TypeFunc* OptoRuntime::_kyberInverseNtt_Type                = nullptr;\n+const TypeFunc* OptoRuntime::_kyberNttMult_Type                   = nullptr;\n+const TypeFunc* OptoRuntime::_kyberAddPoly_2_Type                 = nullptr;\n+const TypeFunc* OptoRuntime::_kyberAddPoly_3_Type                 = nullptr;\n+const TypeFunc* OptoRuntime::_kyber12To16_Type                    = nullptr;\n+const TypeFunc* OptoRuntime::_kyberBarrettReduce_Type             = nullptr;\n@@ -254,1 +260,0 @@\n-\n@@ -1452,0 +1457,140 @@\n+\/\/ Kyber NTT function\n+static const TypeFunc* make_kyberNtt_Type() {\n+    int argcnt = 2;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ coeffs\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ NTT zetas\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\/\/ Kyber inverse NTT function\n+static const TypeFunc* make_kyberInverseNtt_Type() {\n+    int argcnt = 2;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ coeffs\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ inverse NTT zetas\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\/\/ Kyber NTT multiply function\n+static const TypeFunc* make_kyberNttMult_Type() {\n+    int argcnt = 4;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ result\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ ntta\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ nttb\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ NTT multiply zetas\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\/\/ Kyber add 2 polynomials function\n+static const TypeFunc* make_kyberAddPoly_2_Type() {\n+    int argcnt = 3;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ result\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ a\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ b\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\n+\/\/ Kyber add 3 polynomials function\n+static const TypeFunc* make_kyberAddPoly_3_Type() {\n+    int argcnt = 4;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ result\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ a\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ b\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ c\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\n+\/\/ Kyber XOF output parsing into polynomial coefficients candidates\n+\/\/ or decompress(12,...) function\n+static const TypeFunc* make_kyber12To16_Type() {\n+    int argcnt = 4;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ condensed\n+    fields[argp++] = TypeInt::INT;          \/\/ condensedOffs\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ parsed\n+    fields[argp++] = TypeInt::INT;          \/\/ parsedLength\n+\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n+\/\/ Kyber Barrett reduce function\n+static const TypeFunc* make_kyberBarrettReduce_Type() {\n+    int argcnt = 1;\n+\n+    const Type** fields = TypeTuple::fields(argcnt);\n+    int argp = TypeFunc::Parms;\n+    fields[argp++] = TypePtr::NOTNULL;      \/\/ coeffs\n+    assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+    const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + argcnt, fields);\n+\n+    \/\/ result type needed\n+    fields = TypeTuple::fields(1);\n+    fields[TypeFunc::Parms + 0] = TypeInt::INT;\n+    const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+    return TypeFunc::make(domain, range);\n+}\n+\n@@ -2164,1 +2309,7 @@\n-\n+  _kyberNtt_Type                      = make_kyberNtt_Type();\n+  _kyberInverseNtt_Type               = make_kyberInverseNtt_Type();\n+  _kyberNttMult_Type                  = make_kyberNttMult_Type();\n+  _kyberAddPoly_2_Type                = make_kyberAddPoly_2_Type();\n+  _kyberAddPoly_3_Type                = make_kyberAddPoly_3_Type();\n+  _kyber12To16_Type                   = make_kyber12To16_Type();\n+  _kyberBarrettReduce_Type            = make_kyberBarrettReduce_Type();\n@@ -2170,1 +2321,0 @@\n-\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":154,"deletions":4,"binary":false,"changes":158,"status":"modified"},{"patch":"@@ -184,0 +184,7 @@\n+  static const TypeFunc* _kyberNtt_Type;\n+  static const TypeFunc* _kyberInverseNtt_Type;\n+  static const TypeFunc* _kyberNttMult_Type;\n+  static const TypeFunc* _kyberAddPoly_2_Type;\n+  static const TypeFunc* _kyberAddPoly_3_Type;\n+  static const TypeFunc* _kyber12To16_Type;\n+  static const TypeFunc* _kyberBarrettReduce_Type;\n@@ -477,0 +484,4 @@\n+\/\/  static const TypeFunc* digestBase_implCompress_Type(bool is_sha3);\n+\/\/  static const TypeFunc* digestBase_implCompressMB_Type(bool is_sha3);\n+\/\/  static const TypeFunc* double_keccak_Type();\n+\n@@ -593,0 +604,35 @@\n+  static const TypeFunc* kyberNtt_Type() {\n+    assert(_kyberNtt_Type != nullptr, \"should be initialized\");\n+    return _kyberNtt_Type;\n+  }\n+\n+  static const TypeFunc* kyberInverseNtt_Type() {\n+    assert(_kyberInverseNtt_Type != nullptr, \"should be initialized\");\n+    return _kyberInverseNtt_Type;\n+  }\n+\n+  static const TypeFunc* kyberNttMult_Type() {\n+    assert(_kyberNttMult_Type != nullptr, \"should be initialized\");\n+    return _kyberNttMult_Type;\n+  }\n+\n+  static const TypeFunc* kyberAddPoly_2_Type() {\n+    assert(_kyberAddPoly_2_Type != nullptr, \"should be initialized\");\n+    return _kyberAddPoly_2_Type;\n+  }\n+\n+  static const TypeFunc* kyberAddPoly_3_Type() {\n+    assert(_kyberAddPoly_3_Type != nullptr, \"should be initialized\");\n+    return _kyberAddPoly_3_Type;\n+  }\n+\n+  static const TypeFunc* kyber12To16_Type() {\n+    assert(_kyber12To16_Type != nullptr, \"should be initialized\");\n+    return _kyber12To16_Type;\n+  }\n+\n+  static const TypeFunc* kyberBarrettReduce_Type() {\n+    assert(_kyberBarrettReduce_Type != nullptr, \"should be initialized\");\n+    return _kyberBarrettReduce_Type;\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/runtime.hpp","additions":46,"deletions":0,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -407,1 +407,1 @@\n-      Node* neg_c0 = phase->longcon(-c0);\n+      Node* neg_c0 = phase->longcon(java_negate(c0));\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1794,1 +1794,1 @@\n-      fd.reinitialize(k, fs.index());\n+      fd.reinitialize(k, fs.to_FieldInfo());\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1215,1 +1215,1 @@\n-  JavaThread* current_thread  = JavaThread::current();\n+  JavaThread* current_thread = JavaThread::current();\n@@ -1231,0 +1231,1 @@\n+    JvmtiJavaUpcallMark jjum(current_thread); \/\/ hide JVMTI events for Java upcall\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnv.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -95,1 +95,1 @@\n-bool   Arguments::_sun_java_launcher_is_altjvm  = false;\n+bool   Arguments::_executing_unit_tests         = false;\n@@ -359,1 +359,1 @@\n-  \/\/ See if sun.java.launcher or sun.java.launcher.is_altjvm is defined.\n+  \/\/ See if sun.java.launcher is defined.\n@@ -370,4 +370,2 @@\n-    if (match_option(option, \"-Dsun.java.launcher.is_altjvm=\", &tail)) {\n-      if (strcmp(tail, \"true\") == 0) {\n-        _sun_java_launcher_is_altjvm = true;\n-      }\n+    if (match_option(option, \"-XX:+ExecutingUnitTests\")) {\n+      _executing_unit_tests = true;\n@@ -530,0 +528,3 @@\n+#ifdef _LP64\n+  { \"UseCompressedClassPointers\",   JDK_Version::jdk(25),  JDK_Version::jdk(26), JDK_Version::undefined() },\n+#endif\n@@ -1275,4 +1276,0 @@\n-  } else if (strcmp(key, \"sun.java.launcher.is_altjvm\") == 0) {\n-    \/\/ sun.java.launcher.is_altjvm property is\n-    \/\/ private and is processed in process_sun_java_launcher_properties();\n-    \/\/ the sun.java.launcher property is passed on to the java application\n@@ -1767,2 +1764,2 @@\n-bool Arguments::sun_java_launcher_is_altjvm() {\n-  return _sun_java_launcher_is_altjvm;\n+bool Arguments::executing_unit_tests() {\n+  return _executing_unit_tests;\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":9,"deletions":12,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -242,2 +242,2 @@\n-  \/\/ was this VM created via the -XXaltjvm=<path> option\n-  static bool   _sun_java_launcher_is_altjvm;\n+  \/\/ was this VM created with the -XX:+ExecutingUnitTests option\n+  static bool _executing_unit_tests;\n@@ -433,2 +433,2 @@\n-  \/\/ -Dsun.java.launcher.is_altjvm\n-  static bool sun_java_launcher_is_altjvm();\n+  \/\/ -XX:+ExecutingUnitTests\n+  static bool executing_unit_tests();\n","filename":"src\/hotspot\/share\/runtime\/arguments.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1785,0 +1785,3 @@\n+            \/\/ UseObjectMonitorTable expects the BasicLock cache to be either a\n+            \/\/ valid ObjectMonitor* or nullptr. Right now it is garbage, set it\n+            \/\/ to nullptr.\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -91,1 +91,1 @@\n-void fieldDescriptor::reinitialize(InstanceKlass* ik, int index) {\n+void fieldDescriptor::reinitialize(InstanceKlass* ik, const FieldInfo& fieldinfo) {\n@@ -99,2 +99,1 @@\n-  _fieldinfo= ik->field(index);\n-  assert((int)_fieldinfo.index() == index, \"just checking\");\n+  _fieldinfo = fieldinfo;\n","filename":"src\/hotspot\/share\/runtime\/fieldDescriptor.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-    reinitialize(ik, index);\n+    reinitialize(ik, ik->field(index));\n@@ -109,1 +109,1 @@\n-  void reinitialize(InstanceKlass* ik, int index);\n+  void reinitialize(InstanceKlass* ik, const FieldInfo& fieldinfo);\n","filename":"src\/hotspot\/share\/runtime\/fieldDescriptor.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -128,1 +128,1 @@\n-          \"Use 32-bit class pointers in 64-bit VM. \"                        \\\n+          \"(Deprecated) Use 32-bit class pointers in 64-bit VM. \"           \\\n@@ -328,0 +328,2 @@\n+  product(bool, UseKyberIntrinsics, false, DIAGNOSTIC,                      \\\n+          \"Use intrinsics for the vectorized version of Kyber\")             \\\n@@ -1192,3 +1194,0 @@\n-  develop(bool, VerifyFPU, false,                                           \\\n-          \"Verify FPU state (check for NaN's, etc.)\")                       \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -454,0 +454,1 @@\n+  _is_in_java_upcall(false),\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -335,0 +335,1 @@\n+  bool                  _is_in_java_upcall;              \/\/ JVMTI is doing a Java upcall, so JVMTI events must be hidden\n@@ -726,0 +727,3 @@\n+  bool is_in_java_upcall() const                 { return _is_in_java_upcall; }\n+  void toggle_is_in_java_upcall()                { _is_in_java_upcall = !_is_in_java_upcall; };\n+\n@@ -732,1 +736,2 @@\n-  bool should_hide_jvmti_events() const          { return _is_in_VTMS_transition || _is_disable_suspend; }\n+  \/\/ - JVMTI is making a Java upcall (_is_in_java_upcall)\n+  bool should_hide_jvmti_events() const          { return _is_in_VTMS_transition || _is_disable_suspend || _is_in_java_upcall; }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -691,0 +691,15 @@\n+  do_stub(compiler, kyberNtt)                                           \\\n+  do_entry(compiler, kyberNtt, kyberNtt, kyberNtt)                      \\\n+  do_stub(compiler, kyberInverseNtt)                                    \\\n+  do_entry(compiler, kyberInverseNtt, kyberInverseNtt, kyberInverseNtt) \\\n+  do_stub(compiler, kyberNttMult)                                       \\\n+  do_entry(compiler, kyberNttMult, kyberNttMult, kyberNttMult)          \\\n+  do_stub(compiler, kyberAddPoly_2)                                     \\\n+  do_entry(compiler, kyberAddPoly_2, kyberAddPoly_2, kyberAddPoly_2)    \\\n+  do_stub(compiler, kyberAddPoly_3)                                     \\\n+  do_entry(compiler, kyberAddPoly_3, kyberAddPoly_3, kyberAddPoly_3)    \\\n+  do_stub(compiler, kyber12To16)                                        \\\n+  do_entry(compiler, kyber12To16, kyber12To16, kyber12To16)             \\\n+  do_stub(compiler, kyberBarrettReduce)                                 \\\n+  do_entry(compiler, kyberBarrettReduce, kyberBarrettReduce,            \\\n+           kyberBarrettReduce)                                          \\\n","filename":"src\/hotspot\/share\/runtime\/stubDeclarations.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -444,1 +444,1 @@\n-      m->_recursions++;\n+      m->increment_recursions(current);\n@@ -461,1 +461,1 @@\n-      assert(m->_recursions == 0, \"invariant\");\n+      assert(m->recursions() == 0, \"invariant\");\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -429,0 +429,13 @@\n+\/\/ One-shot PeriodicTask subclass for reading the release file\n+class ReadReleaseFileTask : public PeriodicTask {\n+ public:\n+  ReadReleaseFileTask() : PeriodicTask(100) {}\n+\n+  virtual void task() {\n+    os::read_image_release_file();\n+\n+    \/\/ Reclaim our storage and disenroll ourself.\n+    delete this;\n+  }\n+};\n+\n@@ -584,0 +597,4 @@\n+  \/\/ Have the WatcherThread read the release file in the background.\n+  ReadReleaseFileTask* read_task = new ReadReleaseFileTask();\n+  read_task->enroll();\n+\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -378,8 +378,0 @@\n-    private static LambdaForm createBlankForType(MethodType mt) {\n-        \/\/ Make a dummy blank lambda form.\n-        \/\/ It is used as a template for managing the invocation of similar forms that are non-empty.\n-        \/\/ Called only from getPreparedForm.\n-        LambdaForm form = new LambdaForm(0, 0, DEFAULT_FORCE_INLINE, DEFAULT_CUSTOMIZED, new Name[0], Kind.GENERIC);\n-        return form;\n-    }\n-\n@@ -792,2 +784,4 @@\n-        LambdaForm prep = mtype.form().cachedLambdaForm(MethodTypeForm.LF_INTERPRET);\n-        if (prep == null) {\n+        MethodTypeForm form = mtype.form();\n+\n+        MemberName entry = form.cachedInterpretEntry();\n+        if (entry == null) {\n@@ -795,3 +789,2 @@\n-            prep = LambdaForm.createBlankForType(mtype);\n-            prep.vmentry = InvokerBytecodeGenerator.generateLambdaFormInterpreterEntryPoint(mtype);\n-            prep = mtype.form().setCachedLambdaForm(MethodTypeForm.LF_INTERPRET, prep);\n+            entry = InvokerBytecodeGenerator.generateLambdaFormInterpreterEntryPoint(mtype);\n+            entry = form.setCachedInterpretEntry(entry);\n@@ -799,1 +792,1 @@\n-        this.vmentry = prep.vmentry;\n+        this.vmentry = entry;\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/LambdaForm.java","additions":7,"deletions":14,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -214,0 +214,9 @@\n+\n+        @Override\n+        public void writeTo(BufWriterImpl buf) {\n+            if (buf.canWriteDirect(classReader) && buf.labelsMatch(ctx)) {\n+                classReader.copyBytesTo(buf, payloadStart - NAME_AND_LENGTH_PREFIX, payloadLen() + NAME_AND_LENGTH_PREFIX);\n+            } else {\n+                attributeMapper().writeAttribute(buf, this);\n+            }\n+        }\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/classfile\/impl\/BoundAttribute.java","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+    private boolean labelsMatch;\n@@ -113,1 +114,1 @@\n-    public void setLabelContext(LabelContext labelContext) {\n+    public void setLabelContext(LabelContext labelContext, boolean labelsMatch) {\n@@ -115,0 +116,1 @@\n+        this.labelsMatch = labelsMatch;\n@@ -126,0 +128,7 @@\n+\n+    public boolean labelsMatch(LabelContext lc) {\n+        return labelsMatch\n+                && labelContext instanceof DirectCodeBuilder dcb\n+                && dcb.original == lc;\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/classfile\/impl\/BufWriterImpl.java","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+import java.lang.classfile.attribute.UnknownAttribute;\n@@ -172,0 +173,1 @@\n+        generateUserAttributes(consumer);\n@@ -208,0 +210,8 @@\n+    private void generateUserAttributes(Consumer<? super CodeElement> consumer) {\n+        for (var attr : attributes) {\n+            if (attr instanceof CustomAttribute || attr instanceof UnknownAttribute) {\n+                consumer.accept((CodeElement) attr);\n+            }\n+        }\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/classfile\/impl\/CodeImpl.java","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -358,1 +358,0 @@\n-                buf.setLabelContext(dcb);\n@@ -370,0 +369,1 @@\n+                buf.setLabelContext(dcb, codeMatch);\n@@ -388,1 +388,1 @@\n-                buf.setLabelContext(null);\n+                buf.setLabelContext(null, false);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/classfile\/impl\/DirectCodeBuilder.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -215,1 +215,4 @@\n-         * Warn about issues related to classfile contents\n+         * Warn about issues related to classfile contents.\n+         *\n+         * <p>\n+         * This category is not supported by {@code @SuppressWarnings}.\n@@ -217,1 +220,1 @@\n-        CLASSFILE(\"classfile\"),\n+        CLASSFILE(\"classfile\", false),\n@@ -263,0 +266,3 @@\n+         *\n+         * <p>\n+         * This category is not supported by {@code @SuppressWarnings}.\n@@ -264,1 +270,1 @@\n-        INCUBATING(\"incubating\"),\n+        INCUBATING(\"incubating\", false),\n@@ -292,1 +298,4 @@\n-         * Warn about issues relating to use of command line options\n+         * Warn about issues relating to use of command line options.\n+         *\n+         * <p>\n+         * This category is not supported by {@code @SuppressWarnings}.\n@@ -294,1 +303,1 @@\n-        OPTIONS(\"options\"),\n+        OPTIONS(\"options\", false),\n@@ -298,0 +307,3 @@\n+         *\n+         * <p>\n+         * This category is not supported by {@code @SuppressWarnings}.\n@@ -299,1 +311,1 @@\n-        OUTPUT_FILE_CLASH(\"output-file-clash\"),\n+        OUTPUT_FILE_CLASH(\"output-file-clash\", false),\n@@ -313,2 +325,3 @@\n-         * Such warnings cannot be suppressed with the SuppressWarnings\n-         * annotation.\n+         *\n+         * <p>\n+         * This category is not supported by {@code @SuppressWarnings}.\n@@ -316,1 +329,1 @@\n-        PATH(\"path\"),\n+        PATH(\"path\", false),\n@@ -365,0 +378,3 @@\n+         *\n+         * <p>\n+         * This category is not supported by {@code @SuppressWarnings} (yet - see JDK-8224228).\n@@ -366,1 +382,1 @@\n-        TEXT_BLOCKS(\"text-blocks\"),\n+        TEXT_BLOCKS(\"text-blocks\", false),\n@@ -399,0 +415,4 @@\n+            this(option, true);\n+        }\n+\n+        LintCategory(String option, boolean annotationSuppression) {\n@@ -400,0 +420,1 @@\n+            this.annotationSuppression = annotationSuppression;\n@@ -419,0 +440,3 @@\n+\n+        \/** Does this category support being suppressed by the {@code @SuppressWarnings} annotation? *\/\n+        public final boolean annotationSuppression;\n@@ -512,0 +536,1 @@\n+              .filter(lc -> lc.annotationSuppression)\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Lint.java","additions":35,"deletions":10,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -190,11 +190,16 @@\n-            while (q.nonEmpty()) {\n-                q.next().run();\n-            }\n-            while (typesQ.nonEmpty()) {\n-                typesQ.next().run();\n-            }\n-            while (afterTypesQ.nonEmpty()) {\n-                afterTypesQ.next().run();\n-            }\n-            while (validateQ.nonEmpty()) {\n-                validateQ.next().run();\n+            while (q.nonEmpty() ||\n+                   typesQ.nonEmpty() ||\n+                   afterTypesQ.nonEmpty() ||\n+                   validateQ.nonEmpty()) {\n+                while (q.nonEmpty()) {\n+                    q.next().run();\n+                }\n+                while (typesQ.nonEmpty()) {\n+                    typesQ.next().run();\n+                }\n+                while (afterTypesQ.nonEmpty()) {\n+                    afterTypesQ.next().run();\n+                }\n+                while (validateQ.nonEmpty()) {\n+                    validateQ.next().run();\n+                }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Annotate.java","additions":16,"deletions":11,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -2057,1 +2057,1 @@\n-            Log.DiagnosticHandler discardHandler = new Log.DiscardDiagnosticHandler(log);\n+            Log.DiagnosticHandler discardHandler = log.new DiscardDiagnosticHandler();\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -227,0 +227,6 @@\n+    \/** Whether to force suppression of deprecation and preview warnings.\n+     *  This happens when attributing import statements for JDK 9+.\n+     *  @see Feature#DEPRECATION_ON_IMPORT\n+     *\/\n+    private boolean importSuppression;\n+\n@@ -237,0 +243,6 @@\n+    boolean setImportSuppression(boolean newImportSuppression) {\n+        boolean prev = importSuppression;\n+        importSuppression = newImportSuppression;\n+        return prev;\n+    }\n+\n@@ -270,1 +282,1 @@\n-        if (!lint.isSuppressed(LintCategory.PREVIEW))\n+        if (!importSuppression && !lint.isSuppressed(LintCategory.PREVIEW))\n@@ -274,9 +286,0 @@\n-    \/** Log a preview warning.\n-     *  @param pos        Position to be used for error reporting.\n-     *  @param msg        A Warning describing the problem.\n-     *\/\n-    public void warnDeclaredUsingPreview(DiagnosticPosition pos, Symbol sym) {\n-        if (!lint.isSuppressed(LintCategory.PREVIEW))\n-            preview.reportPreviewWarning(pos, LintWarnings.DeclaredUsingPreview(kindName(sym), sym));\n-    }\n-\n@@ -3786,1 +3789,1 @@\n-        final Log.DiagnosticHandler diagHandler = new Log.DiscardDiagnosticHandler(log);\n+        final Log.DiagnosticHandler diagHandler = log.new DiscardDiagnosticHandler();\n@@ -3884,2 +3887,2 @@\n-        if ( (s.isDeprecatedForRemoval()\n-                || s.isDeprecated() && !other.isDeprecated())\n+        if (!importSuppression\n+                && (s.isDeprecatedForRemoval() || s.isDeprecated() && !other.isDeprecated())\n@@ -3934,1 +3937,1 @@\n-                    deferredLintHandler.report(_l -> warnPreviewAPI(pos, LintWarnings.IsPreview(s)));\n+                    warnPreviewAPI(pos, LintWarnings.IsPreview(s));\n@@ -3937,1 +3940,1 @@\n-                    deferredLintHandler.report(_l -> warnPreviewAPI(pos, LintWarnings.IsPreviewReflective(s)));\n+                warnPreviewAPI(pos, LintWarnings.IsPreviewReflective(s));\n@@ -3946,1 +3949,1 @@\n-                deferredLintHandler.report(_l -> warnDeclaredUsingPreview(pos, s));\n+                warnPreviewAPI(pos, LintWarnings.DeclaredUsingPreview(kindName(s), s));\n@@ -5197,1 +5200,1 @@\n-                Log.DiagnosticHandler discardHandler = new Log.DiscardDiagnosticHandler(log);\n+                Log.DiagnosticHandler discardHandler = log.new DiscardDiagnosticHandler();\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Check.java","additions":20,"deletions":17,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -244,1 +244,1 @@\n-            diagHandler = new Log.DiscardDiagnosticHandler(log);\n+            diagHandler = log.new DiscardDiagnosticHandler();\n@@ -262,1 +262,1 @@\n-        Log.DiagnosticHandler diagHandler = new Log.DiscardDiagnosticHandler(log);\n+        Log.DiagnosticHandler diagHandler = log.new DiscardDiagnosticHandler();\n@@ -279,1 +279,1 @@\n-        Log.DiagnosticHandler diagHandler = new Log.DiscardDiagnosticHandler(log);\n+        Log.DiagnosticHandler diagHandler = log.new DiscardDiagnosticHandler();\n@@ -296,1 +296,1 @@\n-        Log.DiagnosticHandler diagHandler = new Log.DiscardDiagnosticHandler(log);\n+        Log.DiagnosticHandler diagHandler = log.new DiscardDiagnosticHandler();\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Flow.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -531,2 +531,1 @@\n-            Lint prevLint = chk.setLint(allowDeprecationOnImport ?\n-                    lint : lint.suppress(LintCategory.DEPRECATION, LintCategory.REMOVAL, LintCategory.PREVIEW));\n+            boolean prevImportSuppression = chk.setImportSuppression(!allowDeprecationOnImport);\n@@ -540,1 +539,1 @@\n-                chk.setLint(prevLint);\n+                chk.setImportSuppression(prevImportSuppression);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/TypeEnter.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -239,4 +239,0 @@\n-    \/** Whether or not the options lint category was initially disabled\n-     *\/\n-    boolean optionsCheckingInitiallyDisabled;\n-\n@@ -440,6 +436,0 @@\n-        \/\/ See if lint options checking was explicitly disabled by the\n-        \/\/ user; this is distinct from the options check being\n-        \/\/ enabled\/disabled.\n-        optionsCheckingInitiallyDisabled =\n-            options.isSet(Option.XLINT_CUSTOM, \"-options\") ||\n-            options.isSet(Option.XLINT_CUSTOM, \"none\");\n@@ -929,5 +919,0 @@\n-        \/\/ forcibly set the equivalent of -Xlint:-options, so that no further\n-        \/\/ warnings about command line options are generated from this point on\n-        options.put(XLINT_CUSTOM.primaryName + \"-\" + LintCategory.OPTIONS.option, \"true\");\n-        options.remove(XLINT_CUSTOM.primaryName + LintCategory.OPTIONS.option);\n-\n@@ -1167,1 +1152,1 @@\n-                deferredDiagnosticHandler = new Log.DeferredDiagnosticHandler(log);\n+                deferredDiagnosticHandler = log.new DeferredDiagnosticHandler();\n@@ -1919,1 +1904,1 @@\n-        DiagnosticHandler dh = new DiscardDiagnosticHandler(log);\n+        DiagnosticHandler dh = log.new DiscardDiagnosticHandler();\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/main\/JavaCompiler.java","additions":2,"deletions":17,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+import javax.tools.Diagnostic;\n@@ -1007,1 +1008,1 @@\n-                this.deferredDiagnosticHandler = new Log.DeferredDiagnosticHandler(log);\n+                this.deferredDiagnosticHandler = log.new DeferredDiagnosticHandler();\n@@ -1110,15 +1111,3 @@\n-            for (JCDiagnostic d: deferredDiagnosticHandler.getDiagnostics()) {\n-                switch (d.getKind()) {\n-                    case WARNING:\n-                        if (werror)\n-                            return true;\n-                        break;\n-\n-                    case ERROR:\n-                        if (fatalErrors || !d.isFlagSet(RECOVERABLE))\n-                            return true;\n-                        break;\n-                }\n-            }\n-\n-            return false;\n+            return deferredDiagnosticHandler.getDiagnostics().stream()\n+              .anyMatch(d -> (d.getKind() == Diagnostic.Kind.WARNING && werror) ||\n+                             (d.getKind() == Diagnostic.Kind.ERROR && (fatalErrors || !d.isFlagSet(RECOVERABLE))));\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/processing\/JavacProcessingEnvironment.java","additions":5,"deletions":16,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2019, 2024, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2019, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,2 @@\n-# Quiet all SA tests\n+# Quiet the majority of SA tests\n+# We run serviceability\/sa\/TestUniverse.java as a sanity check for minimal functionality\n@@ -105,1 +106,0 @@\n-serviceability\/sa\/TestUniverse.java                           8307393   generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList-zgc.txt","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -196,0 +196,1 @@\n+  compiler\/igvn\/ \\\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -254,0 +254,5 @@\n+    public static final String ADD_VHF = VECTOR_PREFIX + \"ADD_VHF\" + POSTFIX;\n+    static {\n+        vectorNode(ADD_VHF, \"AddVHF\", TYPE_SHORT);\n+    }\n+\n@@ -689,0 +694,5 @@\n+    public static final String DIV_VHF = VECTOR_PREFIX + \"DIV_VHF\" + POSTFIX;\n+    static {\n+        vectorNode(DIV_VHF, \"DivVHF\", TYPE_SHORT);\n+    }\n+\n@@ -726,0 +736,5 @@\n+    public static final String FMA_VHF = VECTOR_PREFIX + \"FMA_VHF\" + POSTFIX;\n+    static {\n+        vectorNode(FMA_VHF, \"FmaVHF\", TYPE_SHORT);\n+    }\n+\n@@ -1163,0 +1178,5 @@\n+    public static final String MAX_VHF = VECTOR_PREFIX + \"MAX_VHF\" + POSTFIX;\n+    static {\n+        vectorNode(MAX_VHF, \"MaxVHF\", TYPE_SHORT);\n+    }\n+\n@@ -1263,0 +1283,5 @@\n+    public static final String MIN_VHF = VECTOR_PREFIX + \"MIN_VHF\" + POSTFIX;\n+    static {\n+        vectorNode(MIN_VHF, \"MinVHF\", TYPE_SHORT);\n+    }\n+\n@@ -1364,0 +1389,5 @@\n+    public static final String MUL_VHF = VECTOR_PREFIX + \"MUL_VHF\" + POSTFIX;\n+    static {\n+        vectorNode(MUL_VHF, \"MulVHF\", TYPE_SHORT);\n+    }\n+\n@@ -1775,0 +1805,5 @@\n+    public static final String SQRT_VHF = VECTOR_PREFIX + \"SQRT_VHF\" + POSTFIX;\n+    static {\n+        vectorNode(SQRT_VHF, \"SqrtVHF\", TYPE_SHORT);\n+    }\n+\n@@ -1961,0 +1996,5 @@\n+    public static final String SUB_VHF = VECTOR_PREFIX + \"SUB_VHF\" + POSTFIX;\n+    static {\n+        vectorNode(SUB_VHF, \"SubVHF\", TYPE_SHORT);\n+    }\n+\n@@ -2092,0 +2132,10 @@\n+    public static final String VAND_NOT_I_MASKED = PREFIX + \"VAND_NOT_I_MASKED\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(VAND_NOT_I_MASKED, \"vand_notI_masked\");\n+    }\n+\n+    public static final String VAND_NOT_L_MASKED = PREFIX + \"VAND_NOT_L_MASKED\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(VAND_NOT_L_MASKED, \"vand_notL_masked\");\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":50,"deletions":0,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -502,0 +502,2 @@\n+sun\/java2d\/ClassCastExceptionForInvalidSurface.java 8354097 linux-x64\n+sun\/java2d\/GdiRendering\/ClipShapeRendering.java 8354097 linux-x64\n@@ -737,0 +739,1 @@\n+java\/util\/logging\/LoggingDeadlock5.java       8354424 generic-all\n","filename":"test\/jdk\/ProblemList.txt","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -62,1 +62,1 @@\n-                                    case CustomAttribute a -> throw new AssertionError(\"Unexpected custom attribute: \" + a.attributeName().stringValue());\n+                                    case CustomAttribute<?> a -> throw new AssertionError(\"Unexpected custom attribute: \" + a.attributeName().stringValue());\n@@ -94,1 +94,1 @@\n-                                    case CustomAttribute a -> throw new AssertionError(\"Unexpected custom attribute: \" + a.attributeName().stringValue());\n+                                    case CustomAttribute<?> a -> throw new AssertionError(\"Unexpected custom attribute: \" + a.attributeName().stringValue());\n@@ -146,1 +146,1 @@\n-                    case CustomAttribute a -> throw new AssertionError(\"Unexpected custom attribute: \" + a.attributeName().stringValue());\n+                    case CustomAttribute<?> a -> throw new AssertionError(\"Unexpected custom attribute: \" + a.attributeName().stringValue());\n@@ -598,1 +598,1 @@\n-                case CustomAttribute a ->\n+                case CustomAttribute<?> a ->\n@@ -600,0 +600,2 @@\n+                case UnknownAttribute a ->\n+                    throw new AssertionError(\"Unexpected unknown attribute: \" + a.attributeName().stringValue());\n","filename":"test\/jdk\/jdk\/classfile\/helpers\/RebuildingTransformation.java","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"}]}