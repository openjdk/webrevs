{"files":[{"patch":"@@ -241,1 +241,1 @@\n-  _is_constant = false;\n+  _is_constant = declared_field->is_strict() && declared_field->is_final();\n@@ -268,1 +268,1 @@\n-  _is_constant = false;\n+  _is_constant = declared_field->is_strict() && declared_field->is_final();\n","filename":"src\/hotspot\/share\/ci\/ciField.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -130,0 +130,8 @@\n+\n+bool ciInlineKlass::must_be_atomic() const {\n+  GUARDED_VM_ENTRY(return get_InlineKlass()->must_be_atomic();)\n+}\n+\n+bool ciInlineKlass::is_naturally_atomic(bool null_free) {\n+  return null_free ? (nof_nonstatic_fields() <= 1) : (nof_nonstatic_fields() == 0);\n+}\n","filename":"src\/hotspot\/share\/ci\/ciInlineKlass.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -79,0 +79,3 @@\n+\n+  bool must_be_atomic() const;\n+  bool is_naturally_atomic(bool null_free);\n","filename":"src\/hotspot\/share\/ci\/ciInlineKlass.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"oops\/accessDecorators.hpp\"\n@@ -792,1 +793,1 @@\n-      } else if (vt->is_InlineType()) {\n+      } else {\n@@ -808,1 +809,3 @@\n-          vt->store(&kit, buffer_oop, buffer_oop, vt->type()->inline_klass());\n+\n+          Node* payload_ptr = kit.basic_plus_adr(buffer_oop, kit.gvn().type(vt)->inline_klass()->payload_offset());\n+          vt->store_flat(&kit, buffer_oop, payload_ptr, kit.gvn().type(payload_ptr)->is_ptr(), false, true, true, IN_HEAP | MO_UNORDERED);\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -3533,1 +3533,7 @@\n-          bt = vk->get_field_by_offset(field_offset, false)->layout_type();\n+          ciField* field = vk->get_field_by_offset(field_offset, false);\n+          if (field != nullptr) {\n+            bt = field->layout_type();\n+          } else {\n+            assert(field_offset == vk->payload_offset() + vk->null_marker_offset_in_payload(), \"cannot find field of %s at offset %d\", vk->name()->as_utf8(), field_offset);\n+            bt = T_BOOLEAN;\n+          }\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1880,1 +1880,1 @@\n-  array = _gvn.transform(new CheckCastPPNode(control(), array, arytype));\n+  array = _gvn.transform(new CheckCastPPNode(control(), array, arytype, ConstraintCastNode::StrongDependency));\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/shared\/c2\/barrierSetC2.hpp\"\n@@ -28,0 +29,1 @@\n+#include \"oops\/accessDecorators.hpp\"\n@@ -36,0 +38,1 @@\n+#include \"opto\/opcodes.hpp\"\n@@ -38,0 +41,2 @@\n+#include \"opto\/type.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -407,9 +412,3 @@\n-const TypePtr* InlineTypeNode::field_adr_type(Node* base, int offset, ciInstanceKlass* holder, DecoratorSet decorators, PhaseGVN& gvn) const {\n-  const TypeAryPtr* ary_type = gvn.type(base)->isa_aryptr();\n-  const TypePtr* adr_type = nullptr;\n-  bool is_array = ary_type != nullptr;\n-  if ((decorators & C2_MISMATCHED) != 0) {\n-    adr_type = TypeRawPtr::BOTTOM;\n-  } else if (is_array) {\n-    \/\/ In the case of a flat inline type array, each field has its own slice\n-    adr_type = ary_type->with_field_offset(offset)->add_offset(Type::OffsetBot);\n+const TypePtr* InlineTypeNode::field_adr_type(const TypePtr* holder_type, int offset) {\n+  if (holder_type->isa_aryptr()) {\n+    return holder_type->is_aryptr()->add_field_offset_and_offset(offset);\n@@ -417,3 +416,1 @@\n-    ciField* field = holder->get_field_by_offset(offset, false);\n-    assert(field != nullptr, \"field not found\");\n-    adr_type = gvn.C->alias_type(field)->adr_type();\n+    return holder_type->add_offset(offset);\n@@ -421,1 +418,0 @@\n-  return adr_type;\n@@ -467,1 +463,2 @@\n-void InlineTypeNode::load(GraphKit* kit, Node* base, Node* ptr, ciInstanceKlass* holder, GrowableArray<ciType*>& visited, int holder_offset, DecoratorSet decorators) {\n+void InlineTypeNode::load(GraphKit* kit, Node* base, Node* ptr, const TypePtr* ptr_type,\n+                          bool immutable_memory, bool trust_null_free_oop, DecoratorSet decorators, GrowableArray<ciType*>& visited) {\n@@ -470,0 +467,1 @@\n+  ciInlineKlass* vk = inline_klass();\n@@ -471,1 +469,2 @@\n-    int offset = holder_offset + field_offset(i);\n+    int field_off = field_offset(i) - vk->payload_offset();\n+    Node* field_ptr = kit->basic_plus_adr(base, ptr, field_off);\n@@ -474,5 +473,2 @@\n-    bool null_free = field_is_null_free(i);\n-    if (null_free && ft->as_inline_klass()->is_empty()) {\n-      \/\/ Loading from a field of an empty inline type. Just return the all-zero instance.\n-      value = make_all_zero_impl(kit->gvn(), ft->as_inline_klass(), visited);\n-    } else if (field_is_flat(i)) {\n+    bool field_null_free = field_is_null_free(i);\n+    if (field_is_flat(i)) {\n@@ -480,2 +476,9 @@\n-      int nm_offset = null_free ? -1 : (holder_offset + field_null_marker_offset(i));\n-      value = make_from_flat_impl(kit, ft->as_inline_klass(), base, ptr, nullptr, holder, offset, \/* atomic *\/ false, nm_offset, decorators, visited);\n+      ciInlineKlass* fvk = ft->as_inline_klass();\n+      \/\/ Atomic if nullable or not LooselyConsistentValue\n+      bool atomic = !field_null_free || fvk->must_be_atomic();\n+\n+      int old_len = visited.length();\n+      visited.push(ft);\n+      value = make_from_flat_impl(kit, fvk, base, field_ptr, field_adr_type(ptr_type, field_off),\n+                                  atomic, immutable_memory, field_null_free, trust_null_free_oop && field_null_free, decorators, visited);\n+      visited.trunc_to(old_len);\n@@ -483,28 +486,7 @@\n-      const TypeOopPtr* oop_ptr = kit->gvn().type(base)->isa_oopptr();\n-      bool is_array = (oop_ptr->isa_aryptr() != nullptr);\n-      bool mismatched = (decorators & C2_MISMATCHED) != 0;\n-      if (base->is_Con() && oop_ptr->is_inlinetypeptr() && !is_array && !mismatched) {\n-        \/\/ If the oop to the inline type is constant (static final field), we can\n-        \/\/ also treat the fields as constants because the inline type is immutable.\n-        ciObject* constant_oop = oop_ptr->const_oop();\n-        ciField* field = holder->get_field_by_offset(offset, false);\n-        assert(field != nullptr, \"field not found\");\n-        ciConstant constant = constant_oop->as_instance()->field_value(field);\n-        const Type* con_type = Type::make_from_constant(constant, \/*require_const=*\/ true);\n-        assert(con_type != nullptr, \"type not found\");\n-        value = kit->gvn().transform(kit->makecon(con_type));\n-        \/\/ Check type of constant which might be more precise than the static field type\n-        if (con_type->is_inlinetypeptr() && !con_type->is_zero_type()) {\n-          ft = con_type->inline_klass();\n-        }\n-      } else {\n-        \/\/ Load field value from memory\n-        const TypePtr* adr_type = field_adr_type(base, offset, holder, decorators, kit->gvn());\n-        Node* adr = kit->basic_plus_adr(base, ptr, offset);\n-        BasicType bt = type2field[ft->basic_type()];\n-        assert(is_java_primitive(bt) || adr->bottom_type()->is_ptr_to_narrowoop() == UseCompressedOops, \"inconsistent\");\n-        const Type* val_type = Type::get_const_type(ft);\n-        if (null_free) {\n-          val_type = val_type->join_speculative(TypePtr::NOTNULL);\n-        }\n-        value = kit->access_load_at(base, adr, adr_type, val_type, bt, is_array ? (decorators | IS_ARRAY) : decorators);\n+      \/\/ Load field value from memory\n+      Node* adr = kit->basic_plus_adr(base, ptr, field_off);\n+      BasicType bt = type2field[ft->basic_type()];\n+      assert(is_java_primitive(bt) || adr->bottom_type()->is_ptr_to_narrowoop() == UseCompressedOops, \"inconsistent\");\n+      const Type* val_type = Type::get_const_type(ft);\n+      if (trust_null_free_oop && field_null_free) {\n+        val_type = val_type->join_speculative(TypePtr::NOTNULL);\n@@ -512,0 +494,1 @@\n+      value = kit->access_load_at(base, adr, field_adr_type(ptr_type, field_off), val_type, bt, decorators);\n@@ -548,1 +531,1 @@\n-void InlineTypeNode::convert_from_payload(GraphKit* kit, BasicType bt, Node* payload, int holder_offset, bool null_free, int null_marker_offset) {\n+void InlineTypeNode::convert_from_payload(GraphKit* kit, BasicType bt, Node* payload, int holder_offset, bool null_free, bool trust_null_free_oop) {\n@@ -550,0 +533,1 @@\n+  ciInlineKlass* vk = inline_klass();\n@@ -553,1 +537,1 @@\n-    value = get_payload_value(gvn, payload, bt, T_BOOLEAN, null_marker_offset);\n+    value = get_payload_value(gvn, payload, bt, T_BOOLEAN, holder_offset + vk->null_marker_offset_in_payload());\n@@ -560,1 +544,1 @@\n-    int offset = holder_offset + field_offset(i) - inline_klass()->payload_offset();\n+    int offset = holder_offset + field_offset(i) - vk->payload_offset();\n@@ -562,1 +546,0 @@\n-      null_marker_offset = holder_offset + field_null_marker_offset(i) - inline_klass()->payload_offset();\n@@ -564,1 +547,1 @@\n-      vt->convert_from_payload(kit, bt, payload, offset, field_null_free, null_marker_offset);\n+      vt->convert_from_payload(kit, bt, payload, offset, field_null_free, trust_null_free_oop && field_null_free);\n@@ -572,1 +555,1 @@\n-        if (field_null_free) {\n+        if (trust_null_free_oop && field_null_free) {\n@@ -671,3 +654,6 @@\n-void InlineTypeNode::store_flat(GraphKit* kit, Node* base, Node* ptr, Node* idx, ciInstanceKlass* holder, int holder_offset, bool atomic, int null_marker_offset, DecoratorSet decorators) const {\n-  if (kit->gvn().type(base)->isa_aryptr()) {\n-    kit->C->set_flat_accesses();\n+void InlineTypeNode::store_flat(GraphKit* kit, Node* base, Node* ptr, const TypePtr* ptr_type, bool atomic, bool immutable_memory, bool null_free, DecoratorSet decorators) const {\n+  ciInlineKlass* vk = inline_klass();\n+  bool do_atomic = atomic;\n+  \/\/ With immutable memory, a non-atomic load and an atomic load are the same\n+  if (immutable_memory) {\n+    do_atomic = false;\n@@ -675,0 +661,54 @@\n+  \/\/ If there is only one flattened field, a non-atomic load and an atomic load are the same\n+  if (vk->is_naturally_atomic(null_free)) {\n+    do_atomic = false;\n+  }\n+\n+  if (!do_atomic) {\n+    if (!null_free) {\n+      int nm_offset = vk->null_marker_offset_in_payload();\n+      Node* nm_ptr = kit->basic_plus_adr(base, ptr, nm_offset);\n+      kit->access_store_at(base, nm_ptr, field_adr_type(ptr_type, nm_offset), get_is_init(), TypeInt::BOOL, T_BOOLEAN, decorators);\n+    }\n+    store(kit, base, ptr, ptr_type, immutable_memory, decorators);\n+    return;\n+  }\n+\n+  \/\/ Convert to a payload value <= 64-bit and write atomically.\n+  \/\/ The payload might contain at most two oop fields that must be narrow because otherwise they would be 64-bit\n+  \/\/ in size and would then be written by a \"normal\" oop store. If the payload contains oops, its size is always\n+  \/\/ 64-bit because the next smaller (power-of-two) size would be 32-bit which could only hold one narrow oop that\n+  \/\/ would then be written by a normal narrow oop store. These properties are asserted in 'convert_to_payload'.\n+  assert(!immutable_memory, \"immutable memory does not need explicit atomic access\");\n+  BasicType store_bt = vk->atomic_size_to_basic_type(null_free);\n+  Node* payload = (store_bt == T_LONG) ? kit->longcon(0) : kit->intcon(0);\n+  int oop_off_1 = -1;\n+  int oop_off_2 = -1;\n+  payload = convert_to_payload(kit, store_bt, payload, 0, null_free, vk->null_marker_offset_in_payload(), oop_off_1, oop_off_2);\n+  if (!UseG1GC || oop_off_1 == -1) {\n+    \/\/ No oop fields or no late barrier expansion. Emit an atomic store of the payload and add GC barriers if needed.\n+    assert(oop_off_2 == -1 || !UseG1GC, \"sanity\");\n+    \/\/ ZGC does not support compressed oops, so only one oop can be in the payload which is written by a \"normal\" oop store.\n+    assert((oop_off_1 == -1 && oop_off_2 == -1) || !UseZGC, \"ZGC does not support embedded oops in flat fields\");\n+    const Type* val_type = Type::get_const_basic_type(store_bt);\n+    kit->insert_mem_bar(Op_MemBarCPUOrder);\n+    kit->access_store_at(base, ptr, TypeRawPtr::BOTTOM, payload, val_type, store_bt, decorators | C2_MISMATCHED, true, this);\n+    kit->insert_mem_bar(Op_MemBarCPUOrder);\n+  } else {\n+    \/\/ Contains oops and requires late barrier expansion. Emit a special store node that allows to emit GC barriers in the backend.\n+    assert(UseG1GC, \"Unexpected GC\");\n+    assert(store_bt == T_LONG, \"Unexpected payload type\");\n+    \/\/ If one oop, set the offset (if no offset is set, two oops are assumed by the backend)\n+    Node* oop_offset = (oop_off_2 == -1) ? kit->intcon(oop_off_1) : nullptr;\n+    kit->insert_mem_bar(Op_MemBarCPUOrder);\n+    Node* mem = kit->reset_memory();\n+    kit->set_all_memory(mem);\n+    Node* st = kit->gvn().transform(new StoreLSpecialNode(kit->control(), mem, ptr, TypeRawPtr::BOTTOM, payload, oop_offset, MemNode::unordered));\n+    kit->set_memory(st, TypeRawPtr::BOTTOM);\n+    kit->insert_mem_bar(Op_MemBarCPUOrder);\n+  }\n+}\n+\n+void InlineTypeNode::store_flat_array(GraphKit* kit, Node* base, Node* idx) const {\n+  PhaseGVN& gvn = kit->gvn();\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY | MO_UNORDERED;\n+  kit->C->set_flat_accesses();\n@@ -676,1 +716,1 @@\n-  bool null_free = (null_marker_offset == -1);\n+  assert(vk->maybe_flat_in_array(), \"\");\n@@ -678,31 +718,13 @@\n-  if (atomic) {\n-    bool is_array = (kit->gvn().type(base)->isa_aryptr() != nullptr);\n-#ifdef ASSERT\n-    bool is_naturally_atomic = (!is_array && vk->is_empty()) || (null_free && vk->nof_declared_nonstatic_fields() == 1);\n-    assert(!is_naturally_atomic, \"No atomic access required\");\n-#endif\n-    \/\/ Convert to a payload value <= 64-bit and write atomically.\n-    \/\/ The payload might contain at most two oop fields that must be narrow because otherwise they would be 64-bit\n-    \/\/ in size and would then be written by a \"normal\" oop store. If the payload contains oops, its size is always\n-    \/\/ 64-bit because the next smaller (power-of-two) size would be 32-bit which could only hold one narrow oop that\n-    \/\/ would then be written by a normal narrow oop store. These properties are asserted in 'convert_to_payload'.\n-    BasicType bt = vk->atomic_size_to_basic_type(null_free);\n-    Node* payload = (bt == T_LONG) ? kit->longcon(0) : kit->intcon(0);\n-    int oop_off_1 = -1;\n-    int oop_off_2 = -1;\n-    payload = convert_to_payload(kit, bt, payload, 0, null_free, null_marker_offset - holder_offset, oop_off_1, oop_off_2);\n-\n-    if (!UseG1GC || oop_off_1 == -1) {\n-      \/\/ No oop fields or no late barrier expansion. Emit an atomic store of the payload and add GC barriers if needed.\n-      assert(oop_off_2 == -1 || !UseG1GC, \"sanity\");\n-      \/\/ ZGC does not support compressed oops, so only one oop can be in the payload which is written by a \"normal\" oop store.\n-      assert((oop_off_1 == -1 && oop_off_2 == -1) || !UseZGC, \"ZGC does not support embedded oops in flat fields\");\n-      const Type* val_type = Type::get_const_basic_type(bt);\n-\n-      if (!is_array) {\n-        Node* adr = kit->basic_plus_adr(base, ptr, holder_offset);\n-        kit->insert_mem_bar(Op_MemBarCPUOrder);\n-        kit->access_store_at(base, adr, TypeRawPtr::BOTTOM, payload, val_type, bt, decorators | C2_MISMATCHED | (is_array ? IS_ARRAY : 0), true, this);\n-        kit->insert_mem_bar(Op_MemBarCPUOrder);\n-      } else {\n-        assert(holder_offset == 0, \"sanity\");\n+  RegionNode* region;\n+  if (vk->nof_nonstatic_fields() == 0) {\n+    region = new RegionNode(3);\n+    region->init_req(1, kit->top());\n+    region->init_req(2, kit->top());\n+  } else {\n+    region = new RegionNode(4);\n+    region->init_req(1, kit->top());\n+    region->init_req(2, kit->top());\n+    region->init_req(3, kit->top());\n+  }\n+  gvn.set_type(region, Type::CONTROL);\n+  kit->record_for_igvn(region);\n@@ -710,3 +732,2 @@\n-        RegionNode* region = new RegionNode(3);\n-        kit->gvn().set_type(region, Type::CONTROL);\n-        kit->record_for_igvn(region);\n+  Node* input_memory_state = kit->reset_memory();\n+  kit->set_all_memory(input_memory_state);\n@@ -714,2 +735,3 @@\n-        Node* bol = kit->null_free_array_test(base); \/\/ Argument evaluation order is undefined in C++ and since this sets control, it needs to come first\n-        IfNode* iff = kit->create_and_map_if(kit->control(), bol, PROB_FAIR, COUNT_UNKNOWN);\n+  PhiNode* mem = PhiNode::make(region, input_memory_state, Type::MEMORY, TypePtr::BOTTOM);\n+  gvn.set_type(mem, Type::MEMORY);\n+  kit->record_for_igvn(mem);\n@@ -717,2 +739,3 @@\n-        Node* input_memory_state = kit->reset_memory();\n-        kit->set_all_memory(input_memory_state);\n+  PhiNode* io = PhiNode::make(region, kit->i_o(), Type::ABIO);\n+  gvn.set_type(io, Type::ABIO);\n+  kit->record_for_igvn(io);\n@@ -720,74 +743,2 @@\n-        Node* mem = PhiNode::make(region, input_memory_state, Type::MEMORY, TypePtr::BOTTOM);\n-        kit->gvn().set_type(mem, Type::MEMORY);\n-        kit->record_for_igvn(mem);\n-\n-        PhiNode* io = PhiNode::make(region, kit->i_o(), Type::ABIO);\n-        kit->gvn().set_type(io, Type::ABIO);\n-        kit->record_for_igvn(io);\n-\n-        \/\/ Nullable\n-        kit->set_control(kit->IfFalse(iff));\n-        if (!kit->stopped()) {\n-          assert(!null_free && vk->has_nullable_atomic_layout(), \"Flat array can't be nullable\");\n-          kit->insert_mem_bar(Op_MemBarCPUOrder);\n-          kit->access_store_at(base, ptr, TypeRawPtr::BOTTOM, payload, val_type, bt, decorators | C2_MISMATCHED | (is_array ? IS_ARRAY : 0), true, this);\n-          kit->insert_mem_bar(Op_MemBarCPUOrder);\n-          mem->init_req(1, kit->reset_memory());\n-          io->init_req(1, kit->i_o());\n-        }\n-        region->init_req(1, kit->control());\n-\n-        \/\/ Null-free\n-        kit->set_control(kit->IfTrue(iff));\n-        if (!kit->stopped()) {\n-          kit->set_all_memory(input_memory_state);\n-\n-          \/\/ Check if it's atomic\n-          RegionNode* region_null_free = new RegionNode(3);\n-          kit->gvn().set_type(region_null_free, Type::CONTROL);\n-          kit->record_for_igvn(region_null_free);\n-\n-          Node* mem_null_free = PhiNode::make(region_null_free, input_memory_state, Type::MEMORY, TypePtr::BOTTOM);\n-          kit->gvn().set_type(mem_null_free, Type::MEMORY);\n-          kit->record_for_igvn(mem_null_free);\n-\n-          PhiNode* io_null_free = PhiNode::make(region_null_free, kit->i_o(), Type::ABIO);\n-          kit->gvn().set_type(io_null_free, Type::ABIO);\n-          kit->record_for_igvn(io_null_free);\n-\n-          Node* bol = kit->null_free_atomic_array_test(base, vk);\n-          IfNode* iff = kit->create_and_map_if(kit->control(), bol, PROB_FAIR, COUNT_UNKNOWN);\n-\n-          \/\/ Atomic\n-          kit->set_control(kit->IfTrue(iff));\n-          if (!kit->stopped()) {\n-            BasicType bt_null_free = vk->atomic_size_to_basic_type(\/* null_free *\/ true);\n-            const Type* val_type_null_free = Type::get_const_basic_type(bt_null_free);\n-            kit->set_all_memory(input_memory_state);\n-\n-            if (bt == T_LONG && bt_null_free != T_LONG) {\n-              payload = kit->gvn().transform(new ConvL2INode(payload));\n-            }\n-\n-            Node* cast = base;\n-            Node* adr = kit->flat_array_element_address(cast, idx, vk, \/* null_free *\/ true, \/* not_null_free *\/ false, \/* atomic *\/ true);\n-            kit->insert_mem_bar(Op_MemBarCPUOrder);\n-            kit->access_store_at(cast, adr, TypeRawPtr::BOTTOM, payload, val_type_null_free, bt_null_free, decorators | C2_MISMATCHED | (is_array ? IS_ARRAY : 0), true, this);\n-            kit->insert_mem_bar(Op_MemBarCPUOrder);\n-            mem_null_free->init_req(1, kit->reset_memory());\n-            io_null_free->init_req(1, kit->i_o());\n-          }\n-          region_null_free->init_req(1, kit->control());\n-\n-          \/\/ Non-Atomic\n-          kit->set_control(kit->IfFalse(iff));\n-          if (!kit->stopped()) {\n-            kit->set_all_memory(input_memory_state);\n-\n-            Node* cast = base;\n-            Node* adr = kit->flat_array_element_address(cast, idx, vk, \/* null_free *\/ true, \/* not_null_free *\/ false, \/* atomic *\/ false);\n-            store(kit, cast, adr, holder, holder_offset - vk->payload_offset(), -1, decorators);\n-            mem_null_free->init_req(2, kit->reset_memory());\n-            io_null_free->init_req(2, kit->i_o());\n-          }\n-          region_null_free->init_req(2, kit->control());\n+  Node* bol_null_free = kit->null_free_array_test(base); \/\/ Argument evaluation order is undefined in C++ and since this sets control, it needs to come first\n+  IfNode* iff_null_free = kit->create_and_map_if(kit->control(), bol_null_free, PROB_FAIR, COUNT_UNKNOWN);\n@@ -795,4 +746,14 @@\n-          mem->init_req(2, kit->gvn().transform(mem_null_free));\n-          io->init_req(2, kit->gvn().transform(io_null_free));\n-          region->init_req(2, kit->gvn().transform(region_null_free));\n-        }\n+  \/\/ Nullable\n+  kit->set_control(kit->IfFalse(iff_null_free));\n+  if (!kit->stopped()) {\n+    assert(vk->has_nullable_atomic_layout(), \"\");\n+    kit->set_all_memory(input_memory_state);\n+    Node* cast = base;\n+    Node* ptr = kit->flat_array_element_address(cast, idx, vk, \/* null_free *\/ false, \/* not_null_free *\/ true, \/* atomic *\/ true);\n+    const TypeAryPtr* ptr_type = gvn.type(cast)->is_aryptr()->with_field_offset(0)->with_offset(TypePtr::OffsetBot);\n+    store_flat(kit, cast, ptr, ptr_type, true, false, false, decorators);\n+\n+    region->set_req(1, kit->control());\n+    mem->set_req(1, kit->reset_memory());\n+    io->set_req(1, kit->i_o());\n+  }\n@@ -800,4 +761,10 @@\n-        kit->set_control(kit->gvn().transform(region));\n-        kit->set_all_memory(kit->gvn().transform(mem));\n-        kit->set_i_o(kit->gvn().transform(io));\n-      }\n+  kit->set_control(kit->IfTrue(iff_null_free));\n+  if (!kit->stopped()) {\n+    kit->set_all_memory(input_memory_state);\n+\n+    if (vk->nof_nonstatic_fields() == 0) {\n+      \/\/ Store into a null-free empty array is a nop. This short circuit must be done because we\n+      \/\/ cannot calculate the address type of the elements in this case.\n+      region->set_req(2, kit->control());\n+      mem->set_req(2, kit->reset_memory());\n+      io->set_req(2, kit->i_o());\n@@ -805,5 +772,15 @@\n-      if (oop_off_2 == -1 && UseCompressedOops && vk->nof_declared_nonstatic_fields() == 1) {\n-        \/\/ TODO 8350865 Implement this\n-        \/\/ If null free, it's not a long but an int store. Deoptimize for now.\n-        BuildCutout unless(kit, kit->null_free_array_test(base, \/* null_free = *\/ false), PROB_MAX);\n-        kit->uncommon_trap_exact(Deoptimization::Reason_unhandled, Deoptimization::Action_none);\n+      Node* bol_atomic = kit->null_free_atomic_array_test(base, vk);\n+      IfNode* iff_atomic = kit->create_and_map_if(kit->control(), bol_atomic, PROB_FAIR, COUNT_UNKNOWN);\n+\n+      kit->set_control(kit->IfTrue(iff_atomic));\n+      if (!kit->stopped()) {\n+        assert(vk->has_atomic_layout(), \"\");\n+        kit->set_all_memory(input_memory_state);\n+        Node* cast = base;\n+        Node* ptr = kit->flat_array_element_address(cast, idx, vk, \/* null_free *\/ true, \/* not_null_free *\/ false, \/* atomic *\/ true);\n+        const TypeAryPtr* ptr_type = gvn.type(cast)->is_aryptr()->with_field_offset(0)->with_offset(TypePtr::OffsetBot);\n+        store_flat(kit, cast, ptr, ptr_type, true, false, true, decorators);\n+\n+        region->set_req(2, kit->control());\n+        mem->set_req(2, kit->reset_memory());\n+        io->set_req(2, kit->i_o());\n@@ -812,12 +789,13 @@\n-      \/\/ Contains oops and requires late barrier expansion. Emit a special store node that allows to emit GC barriers in the backend.\n-      assert(UseG1GC, \"Unexpected GC\");\n-      assert(bt == T_LONG, \"Unexpected payload type\");\n-      \/\/ If one oop, set the offset (if no offset is set, two oops are assumed by the backend)\n-      Node* oop_offset = (oop_off_2 == -1) ? kit->intcon(oop_off_1) : nullptr;\n-      Node* adr = kit->basic_plus_adr(base, ptr, holder_offset);\n-      kit->insert_mem_bar(Op_MemBarCPUOrder);\n-      Node* mem = kit->reset_memory();\n-      kit->set_all_memory(mem);\n-      Node* st = kit->gvn().transform(new StoreLSpecialNode(kit->control(), mem, adr, TypeRawPtr::BOTTOM, payload, oop_offset, MemNode::unordered));\n-      kit->set_memory(st, TypeRawPtr::BOTTOM);\n-      kit->insert_mem_bar(Op_MemBarCPUOrder);\n+      kit->set_control(kit->IfFalse(iff_atomic));\n+      if (!kit->stopped()) {\n+        assert(vk->has_non_atomic_layout(), \"\");\n+        kit->set_all_memory(input_memory_state);\n+        Node* cast = base;\n+        Node* ptr = kit->flat_array_element_address(cast, idx, vk, \/* null_free *\/ true, \/* not_null_free *\/ false, \/* atomic *\/ false);\n+        const TypeAryPtr* ptr_type = gvn.type(cast)->is_aryptr()->with_field_offset(0)->with_offset(TypePtr::OffsetBot);\n+        store_flat(kit, cast, ptr, ptr_type, false, false, true, decorators);\n+\n+        region->set_req(3, kit->control());\n+        mem->set_req(3, kit->reset_memory());\n+        io->set_req(3, kit->i_o());\n+      }\n@@ -825,1 +803,0 @@\n-    return;\n@@ -828,10 +805,3 @@\n-  \/\/ The inline type is embedded into the object without an oop header. Subtract the\n-  \/\/ offset of the first field to account for the missing header when storing the values.\n-  holder_offset -= vk->payload_offset();\n-\n-  if (!null_free) {\n-    bool is_array = (kit->gvn().type(base)->isa_aryptr() != nullptr);\n-    Node* adr = kit->basic_plus_adr(base, ptr, null_marker_offset);\n-    kit->access_store_at(base, adr, TypeRawPtr::BOTTOM, get_is_init(), TypeInt::BOOL, T_BOOLEAN, is_array ? (decorators | IS_ARRAY) : decorators);\n-  }\n-  store(kit, base, ptr, holder, holder_offset, -1, decorators);\n+  kit->set_control(gvn.transform(region));\n+  kit->set_all_memory(gvn.transform(mem));\n+  kit->set_i_o(gvn.transform(io));\n@@ -840,1 +810,1 @@\n-void InlineTypeNode::store(GraphKit* kit, Node* base, Node* ptr, ciInstanceKlass* holder, int holder_offset, int offsetOnly, DecoratorSet decorators) const {\n+void InlineTypeNode::store(GraphKit* kit, Node* base, Node* ptr, const TypePtr* ptr_type, bool immutable_memory, DecoratorSet decorators) const {\n@@ -842,0 +812,1 @@\n+  ciInlineKlass* vk = inline_klass();\n@@ -843,3 +814,3 @@\n-    if (offsetOnly != -1 && offsetOnly != field_offset(i)) continue;\n-    int offset = holder_offset + field_offset(i);\n-    Node* value = field_value(i);\n+    int field_off = field_offset(i) - vk->payload_offset();\n+    Node* field_val = field_value(i);\n+    bool field_null_free = field_is_null_free(i);\n@@ -847,0 +818,2 @@\n+    Node* field_ptr = kit->basic_plus_adr(base, ptr, field_off);\n+    const TypePtr* field_ptr_type = field_adr_type(ptr_type, field_off);\n@@ -849,2 +822,5 @@\n-      int nm_offset = field_is_null_free(i) ? -1 : (holder_offset + field_null_marker_offset(i));\n-      value->as_InlineType()->store_flat(kit, base, ptr, nullptr, holder, offset, \/* atomic *\/ false, nm_offset, decorators);\n+      ciInlineKlass* fvk = ft->as_inline_klass();\n+      \/\/ Atomic if nullable or not LooselyConsistentValue\n+      bool atomic = !field_null_free || fvk->must_be_atomic();\n+\n+      field_val->as_InlineType()->store_flat(kit, base, field_ptr, field_ptr_type, atomic, immutable_memory, field_null_free, decorators);\n@@ -853,2 +829,0 @@\n-      const TypePtr* adr_type = field_adr_type(base, offset, holder, decorators, kit->gvn());\n-      Node* adr = kit->basic_plus_adr(base, ptr, offset);\n@@ -856,1 +830,0 @@\n-      assert(is_java_primitive(bt) || adr->bottom_type()->is_ptr_to_narrowoop() == UseCompressedOops, \"inconsistent\");\n@@ -858,2 +831,1 @@\n-      bool is_array = (kit->gvn().type(base)->isa_aryptr() != nullptr);\n-      kit->access_store_at(base, adr, adr_type, value, val_type, bt, is_array ? (decorators | IS_ARRAY) : decorators);\n+      kit->access_store_at(base, field_ptr, field_ptr_type, field_val, val_type, bt, decorators);\n@@ -915,1 +887,2 @@\n-    store(kit, alloc_oop, alloc_oop, vk);\n+    Node* payload_alloc_oop = kit->basic_plus_adr(alloc_oop, vk->payload_offset());\n+    store(kit, alloc_oop, payload_alloc_oop, kit->gvn().type(payload_alloc_oop)->is_ptr(), true, IN_HEAP | MO_UNORDERED | C2_TIGHTLY_COUPLED_ALLOC);\n@@ -1211,1 +1184,2 @@\n-    vt->load(kit, not_null_oop, not_null_oop, vk, visited);\n+    Node* payload_ptr = kit->basic_plus_adr(not_null_oop, vk->payload_offset());\n+    vt->load(kit, not_null_oop, payload_ptr, gvn.type(payload_ptr)->is_ptr(), true, true, IN_HEAP | MO_UNORDERED, visited);\n@@ -1229,1 +1203,2 @@\n-    vt->load(kit, oop, oop, vk, visited);\n+    Node* payload_ptr = kit->basic_plus_adr(oop, vk->payload_offset());\n+    vt->load(kit, oop, payload_ptr, gvn.type(payload_ptr)->is_ptr(), true, true, IN_HEAP | MO_UNORDERED, visited);\n@@ -1239,2 +1214,2 @@\n-InlineTypeNode* InlineTypeNode::make_from_flat(GraphKit* kit, ciInlineKlass* vk, Node* obj, Node* ptr, Node* idx, ciInstanceKlass* holder, int holder_offset,\n-                                               bool atomic, int null_marker_offset, DecoratorSet decorators) {\n+InlineTypeNode* InlineTypeNode::make_from_flat(GraphKit* kit, ciInlineKlass* vk, Node* base, Node* ptr, const TypePtr* ptr_type,\n+                                               bool atomic, bool immutable_memory, bool null_free, DecoratorSet decorators) {\n@@ -1243,1 +1218,1 @@\n-  return make_from_flat_impl(kit, vk, obj, ptr, idx, holder, holder_offset, atomic, null_marker_offset, decorators, visited);\n+  return make_from_flat_impl(kit, vk, base, ptr, ptr_type, atomic, immutable_memory, null_free, null_free, decorators, visited);\n@@ -1247,8 +1222,28 @@\n-InlineTypeNode* InlineTypeNode::make_from_flat_impl(GraphKit* kit, ciInlineKlass* vk, Node* obj, Node* ptr, Node* idx, ciInstanceKlass* holder, int holder_offset,\n-                                                    bool atomic, int null_marker_offset, DecoratorSet decorators, GrowableArray<ciType*>& visited) {\n-  if (kit->gvn().type(obj)->isa_aryptr()) {\n-    kit->C->set_flat_accesses();\n-  }\n-  \/\/ Create and initialize an InlineTypeNode by loading all field values from\n-  \/\/ a flat inline type field at 'holder_offset' or from an inline type array.\n-  bool null_free = (null_marker_offset == -1);\n+InlineTypeNode* InlineTypeNode::make_from_flat_impl(GraphKit* kit, ciInlineKlass* vk, Node* base, Node* ptr, const TypePtr* ptr_type,\n+                                                    bool atomic, bool immutable_memory, bool null_free, bool trust_null_free_oop, DecoratorSet decorators, GrowableArray<ciType*>& visited) {\n+  assert(null_free || !trust_null_free_oop, \"cannot trust null-free oop when the holder object is not null-free\");\n+  PhaseGVN& gvn = kit->gvn();\n+  bool do_atomic = atomic;\n+  \/\/ With immutable memory, a non-atomic load and an atomic load are the same\n+  if (immutable_memory) {\n+    do_atomic = false;\n+  }\n+  \/\/ If there is only one flattened field, a non-atomic load and an atomic load are the same\n+  if (vk->is_naturally_atomic(null_free)) {\n+    do_atomic = false;\n+  }\n+\n+  if (!do_atomic) {\n+    InlineTypeNode* vt = make_uninitialized(kit->gvn(), vk, null_free);\n+    if (!null_free) {\n+      int nm_offset = vk->null_marker_offset_in_payload();\n+      Node* nm_ptr = kit->basic_plus_adr(base, ptr, nm_offset);\n+      Node* nm_value = kit->access_load_at(base, nm_ptr, field_adr_type(ptr_type, nm_offset), TypeInt::BOOL, T_BOOLEAN, decorators);\n+      vt->set_req(IsInit, nm_value);\n+    }\n+\n+    vt->load(kit, base, ptr, ptr_type, immutable_memory, trust_null_free_oop, decorators, visited);\n+    return gvn.transform(vt)->as_InlineType();\n+  }\n+\n+  assert(!immutable_memory, \"immutable memory does not need explicit atomic access\");\n@@ -1256,0 +1251,9 @@\n+  BasicType load_bt = vk->atomic_size_to_basic_type(null_free);\n+  decorators |= C2_MISMATCHED | C2_CONTROL_DEPENDENT_LOAD;\n+  const Type* val_type = Type::get_const_basic_type(load_bt);\n+  kit->insert_mem_bar(Op_MemBarCPUOrder);\n+  Node* payload = kit->access_load_at(base, ptr, TypeRawPtr::BOTTOM, val_type, load_bt, decorators, kit->control());\n+  kit->insert_mem_bar(Op_MemBarCPUOrder);\n+  vt->convert_from_payload(kit, load_bt, kit->gvn().transform(payload), 0, null_free, trust_null_free_oop);\n+  return gvn.transform(vt)->as_InlineType();\n+}\n@@ -1257,17 +1261,22 @@\n-  bool is_array = (kit->gvn().type(obj)->isa_aryptr() != nullptr);\n-  if (atomic) {\n-    \/\/ Read atomically and convert from payload\n-#ifdef ASSERT\n-    bool is_naturally_atomic = (!is_array && vk->is_empty()) || (null_free && vk->nof_declared_nonstatic_fields() == 1);\n-    assert(!is_naturally_atomic, \"No atomic access required\");\n-#endif\n-    BasicType bt = vk->atomic_size_to_basic_type(null_free);\n-    decorators |= C2_MISMATCHED | C2_CONTROL_DEPENDENT_LOAD;\n-    const Type* val_type = Type::get_const_basic_type(bt);\n-\n-    Node* payload = nullptr;\n-    if (!is_array) {\n-      Node* adr = kit->basic_plus_adr(obj, ptr, holder_offset);\n-      payload = kit->access_load_at(obj, adr, TypeRawPtr::BOTTOM, val_type, bt, is_array ? (decorators | IS_ARRAY) : decorators, kit->control());\n-    } else {\n-      assert(holder_offset == 0, \"sanity\");\n+InlineTypeNode* InlineTypeNode::make_from_flat_array(GraphKit* kit, ciInlineKlass* vk, Node* base, Node* idx) {\n+  assert(vk->maybe_flat_in_array(), \"\");\n+  PhaseGVN& gvn = kit->gvn();\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY | MO_UNORDERED | C2_CONTROL_DEPENDENT_LOAD;\n+  kit->C->set_flat_accesses();\n+  InlineTypeNode* vt_nullable = nullptr;\n+  InlineTypeNode* vt_null_free = nullptr;\n+  InlineTypeNode* vt_non_atomic = nullptr;\n+\n+  RegionNode* region;\n+  if (vk->nof_nonstatic_fields() == 0) {\n+    region = new RegionNode(3);\n+    region->init_req(1, kit->top());\n+    region->init_req(2, kit->top());\n+  } else {\n+    region = new RegionNode(4);\n+    region->init_req(1, kit->top());\n+    region->init_req(2, kit->top());\n+    region->init_req(3, kit->top());\n+  }\n+  gvn.set_type(region, Type::CONTROL);\n+  kit->record_for_igvn(region);\n@@ -1275,3 +1284,2 @@\n-      RegionNode* region = new RegionNode(3);\n-      kit->gvn().set_type(region, Type::CONTROL);\n-      kit->record_for_igvn(region);\n+  Node* input_memory_state = kit->reset_memory();\n+  kit->set_all_memory(input_memory_state);\n@@ -1279,3 +1287,3 @@\n-      payload = PhiNode::make(region, nullptr, val_type);\n-      kit->gvn().set_type(payload, val_type);\n-      kit->record_for_igvn(payload);\n+  PhiNode* mem = PhiNode::make(region, input_memory_state, Type::MEMORY, TypePtr::BOTTOM);\n+  gvn.set_type(mem, Type::MEMORY);\n+  kit->record_for_igvn(mem);\n@@ -1283,2 +1291,3 @@\n-      Node* input_memory_state = kit->reset_memory();\n-      kit->set_all_memory(input_memory_state);\n+  PhiNode* io = PhiNode::make(region, kit->i_o(), Type::ABIO);\n+  gvn.set_type(io, Type::ABIO);\n+  kit->record_for_igvn(io);\n@@ -1286,3 +1295,2 @@\n-      Node* mem = PhiNode::make(region, input_memory_state, Type::MEMORY, TypePtr::BOTTOM);\n-      kit->gvn().set_type(mem, Type::MEMORY);\n-      kit->record_for_igvn(mem);\n+  Node* bol_null_free = kit->null_free_array_test(base); \/\/ Argument evaluation order is undefined in C++ and since this sets control, it needs to come first\n+  IfNode* iff_null_free = kit->create_and_map_if(kit->control(), bol_null_free, PROB_FAIR, COUNT_UNKNOWN);\n@@ -1290,3 +1298,14 @@\n-      PhiNode* io = PhiNode::make(region, kit->i_o(), Type::ABIO);\n-      kit->gvn().set_type(io, Type::ABIO);\n-      kit->record_for_igvn(io);\n+  \/\/ Nullable\n+  kit->set_control(kit->IfFalse(iff_null_free));\n+  if (!kit->stopped()) {\n+    assert(vk->has_nullable_atomic_layout(), \"\");\n+    kit->set_all_memory(input_memory_state);\n+    Node* cast = base;\n+    Node* ptr = kit->flat_array_element_address(cast, idx, vk, \/* null_free *\/ false, \/* not_null_free *\/ true, \/* atomic *\/ true);\n+    const TypeAryPtr* ptr_type = gvn.type(cast)->is_aryptr()->with_field_offset(0)->with_offset(TypePtr::OffsetBot);\n+    vt_nullable = InlineTypeNode::make_from_flat(kit, vk, cast, ptr, ptr_type, true, false, false, decorators);\n+\n+    region->set_req(1, kit->control());\n+    mem->set_req(1, kit->reset_memory());\n+    io->set_req(1, kit->i_o());\n+  }\n@@ -1294,2 +1313,15 @@\n-      Node* bol = kit->null_free_array_test(obj); \/\/ Argument evaluation order is undefined in C++ and since this sets control, it needs to come first\n-      IfNode* iff = kit->create_and_map_if(kit->control(), bol, PROB_FAIR, COUNT_UNKNOWN);\n+  \/\/ Null-free\n+  kit->set_control(kit->IfTrue(iff_null_free));\n+  if (!kit->stopped()) {\n+    kit->set_all_memory(input_memory_state);\n+\n+    if (vk->nof_nonstatic_fields() == 0) {\n+      \/\/ Load from a null-free empty array, just return the default instance. This short circuit\n+      \/\/ must be done because we cannot calculate the address type of the elements in this case.\n+      vt_null_free = make_all_zero(gvn, vk);\n+      region->set_req(2, kit->control());\n+      mem->set_req(2, kit->reset_memory());\n+      io->set_req(2, kit->i_o());\n+    } else {\n+      Node* bol_atomic = kit->null_free_atomic_array_test(base, vk);\n+      IfNode* iff_atomic = kit->create_and_map_if(kit->control(), bol_atomic, PROB_FAIR, COUNT_UNKNOWN);\n@@ -1297,2 +1329,2 @@\n-      \/\/ Nullable\n-      kit->set_control(kit->IfFalse(iff));\n+      \/\/ Atomic\n+      kit->set_control(kit->IfTrue(iff_atomic));\n@@ -1300,8 +1332,10 @@\n-        assert(!null_free && vk->has_nullable_atomic_layout(), \"Flat array can't be nullable\");\n-\n-        Node* cast = obj;\n-        Node* adr = kit->flat_array_element_address(cast, idx, vk, \/* null_free *\/ false, \/* not_null_free *\/ true, \/* atomic *\/ true);\n-        Node* load = kit->access_load_at(cast, adr, TypeRawPtr::BOTTOM, val_type, bt, is_array ? (decorators | IS_ARRAY) : decorators, kit->control());\n-        payload->init_req(1, load);\n-        mem->init_req(1, kit->reset_memory());\n-        io->init_req(1, kit->i_o());\n+        assert(vk->has_atomic_layout(), \"\");\n+        kit->set_all_memory(input_memory_state);\n+        Node* cast = base;\n+        Node* ptr = kit->flat_array_element_address(cast, idx, vk, \/* null_free *\/ true, \/* not_null_free *\/ false, \/* atomic *\/ true);\n+        const TypeAryPtr* ptr_type = gvn.type(cast)->is_aryptr()->with_field_offset(0)->with_offset(TypePtr::OffsetBot);\n+        vt_null_free = InlineTypeNode::make_from_flat(kit, vk, cast, ptr, ptr_type, true, false, true, decorators);\n+\n+        region->set_req(2, kit->control());\n+        mem->set_req(2, kit->reset_memory());\n+        io->set_req(2, kit->i_o());\n@@ -1309,1 +1343,0 @@\n-      region->init_req(1, kit->control());\n@@ -1311,2 +1344,2 @@\n-      \/\/ Null-free\n-      kit->set_control(kit->IfTrue(iff));\n+      \/\/ Non-Atomic\n+      kit->set_control(kit->IfFalse(iff_atomic));\n@@ -1314,0 +1347,3 @@\n+        assert(vk->has_non_atomic_layout(), \"\");\n+        \/\/ TODO 8350865 Is the conversion to\/from payload folded? We should wire this directly.\n+        \/\/ Also remove the PreserveReexecuteState in Parse::array_load when buffering is no longer possible.\n@@ -1315,71 +1351,8 @@\n-\n-        \/\/ Check if it's atomic\n-        RegionNode* region_null_free = new RegionNode(3);\n-        kit->gvn().set_type(region_null_free, Type::CONTROL);\n-        kit->record_for_igvn(region_null_free);\n-\n-        Node* payload_null_free = PhiNode::make(region_null_free, nullptr, val_type);\n-        kit->gvn().set_type(payload_null_free, val_type);\n-        kit->record_for_igvn(payload_null_free);\n-\n-        Node* mem_null_free = PhiNode::make(region_null_free, input_memory_state, Type::MEMORY, TypePtr::BOTTOM);\n-        kit->gvn().set_type(mem_null_free, Type::MEMORY);\n-        kit->record_for_igvn(mem_null_free);\n-\n-        PhiNode* io_null_free = PhiNode::make(region_null_free, kit->i_o(), Type::ABIO);\n-        kit->gvn().set_type(io_null_free, Type::ABIO);\n-        kit->record_for_igvn(io_null_free);\n-\n-        bol = kit->null_free_atomic_array_test(obj, vk);\n-        IfNode* iff = kit->create_and_map_if(kit->control(), bol, PROB_FAIR, COUNT_UNKNOWN);\n-\n-        \/\/ Atomic\n-        kit->set_control(kit->IfTrue(iff));\n-        if (!kit->stopped()) {\n-          BasicType bt_null_free = vk->atomic_size_to_basic_type(\/* null_free *\/ true);\n-          const Type* val_type_null_free = Type::get_const_basic_type(bt_null_free);\n-          kit->set_all_memory(input_memory_state);\n-\n-          Node* cast = obj;\n-          Node* adr = kit->flat_array_element_address(cast, idx, vk, \/* null_free *\/ true, \/* not_null_free *\/ false, \/* atomic *\/ true);\n-          Node* load = kit->access_load_at(cast, adr, TypeRawPtr::BOTTOM, val_type_null_free, bt_null_free, is_array ? (decorators | IS_ARRAY) : decorators, kit->control());\n-          if (bt == T_LONG && bt_null_free != T_LONG) {\n-            load = kit->gvn().transform(new ConvI2LNode(load));\n-          }\n-          \/\/ Set the null marker if not known to be null-free\n-          if (!null_free) {\n-            load = set_payload_value(&kit->gvn(), load, bt, kit->intcon(1), T_BOOLEAN, null_marker_offset);\n-          }\n-          payload_null_free->init_req(1, load);\n-          mem_null_free->init_req(1, kit->reset_memory());\n-          io_null_free->init_req(1, kit->i_o());\n-        }\n-        region_null_free->init_req(1, kit->control());\n-\n-        \/\/ Non-Atomic\n-        kit->set_control(kit->IfFalse(iff));\n-        if (!kit->stopped()) {\n-          \/\/ TODO 8350865 Is the conversion to\/from payload folded? We should wire this directly.\n-          \/\/ Also remove the PreserveReexecuteState in Parse::array_load when buffering is no longer possible.\n-          kit->set_all_memory(input_memory_state);\n-\n-          InlineTypeNode* vt_atomic = make_uninitialized(kit->gvn(), vk, true);\n-          Node* cast = obj;\n-          Node* adr = kit->flat_array_element_address(cast, idx, vk, \/* null_free *\/ true, \/* not_null_free *\/ false, \/* atomic *\/ false);\n-          vt_atomic->load(kit, cast, adr, holder, visited, holder_offset - vk->payload_offset(), decorators);\n-\n-          Node* tmp_payload = (bt == T_LONG) ? kit->longcon(0) : kit->intcon(0);\n-          int oop_off_1 = -1;\n-          int oop_off_2 = -1;\n-          tmp_payload = vt_atomic->convert_to_payload(kit, bt, tmp_payload, 0, null_free, null_marker_offset, oop_off_1, oop_off_2);\n-\n-          payload_null_free->init_req(2, tmp_payload);\n-          mem_null_free->init_req(2, kit->reset_memory());\n-          io_null_free->init_req(2, kit->i_o());\n-        }\n-        region_null_free->init_req(2, kit->control());\n-\n-        region->init_req(2, kit->gvn().transform(region_null_free));\n-        payload->init_req(2, kit->gvn().transform(payload_null_free));\n-        mem->init_req(2, kit->gvn().transform(mem_null_free));\n-        io->init_req(2, kit->gvn().transform(io_null_free));\n+        Node* cast = base;\n+        Node* ptr = kit->flat_array_element_address(cast, idx, vk, \/* null_free *\/ true, \/* not_null_free *\/ false, \/* atomic *\/ false);\n+        const TypeAryPtr* ptr_type = gvn.type(cast)->is_aryptr()->with_field_offset(0)->with_offset(TypePtr::OffsetBot);\n+        vt_non_atomic = InlineTypeNode::make_from_flat(kit, vk, cast, ptr, ptr_type, false, false, true, decorators);\n+\n+        region->set_req(3, kit->control());\n+        mem->set_req(3, kit->reset_memory());\n+        io->set_req(3, kit->i_o());\n@@ -1387,4 +1360,0 @@\n-\n-      kit->set_control(kit->gvn().transform(region));\n-      kit->set_all_memory(kit->gvn().transform(mem));\n-      kit->set_i_o(kit->gvn().transform(io));\n@@ -1392,3 +1361,0 @@\n-\n-    vt->convert_from_payload(kit, bt, kit->gvn().transform(payload), 0, null_free, null_marker_offset - holder_offset);\n-    return kit->gvn().transform(vt)->as_InlineType();\n@@ -1397,3 +1363,17 @@\n-  \/\/ The inline type is embedded into the object without an oop header. Subtract the\n-  \/\/ offset of the first field to account for the missing header when storing the values.\n-  holder_offset -= vk->payload_offset();\n+  InlineTypeNode* vt = nullptr;\n+  if (vt_nullable == nullptr && vt_null_free == nullptr && vt_non_atomic == nullptr) {\n+    \/\/ All paths are dead\n+    vt = make_null(gvn, vk);\n+  } else if (vt_nullable == nullptr && vt_null_free == nullptr) {\n+    vt = vt_non_atomic;\n+  } else if (vt_nullable == nullptr && vt_non_atomic == nullptr) {\n+    vt = vt_null_free;\n+  } else if (vt_null_free == nullptr && vt_non_atomic == nullptr) {\n+    vt = vt_nullable;\n+  }\n+  if (vt != nullptr) {\n+    kit->set_control(kit->gvn().transform(region));\n+    kit->set_all_memory(kit->gvn().transform(mem));\n+    kit->set_i_o(kit->gvn().transform(io));\n+    return vt;\n+  }\n@@ -1401,4 +1381,10 @@\n-  if (!null_free) {\n-    Node* adr = kit->basic_plus_adr(obj, ptr, null_marker_offset);\n-    Node* nm_value = kit->access_load_at(obj, adr, TypeRawPtr::BOTTOM, TypeInt::BOOL, T_BOOLEAN, is_array ? (decorators | IS_ARRAY) : decorators);\n-    vt->set_req(IsInit, nm_value);\n+  InlineTypeNode* zero = InlineTypeNode::make_null(gvn, vk);\n+  vt = zero->clone_with_phis(&gvn, region);\n+  if (vt_nullable != nullptr) {\n+    vt = vt->merge_with(&gvn, vt_nullable, 1, false);\n+  }\n+  if (vt_null_free != nullptr) {\n+    vt = vt->merge_with(&gvn, vt_null_free, 2, false);\n+  }\n+  if (vt_non_atomic != nullptr) {\n+    vt = vt->merge_with(&gvn, vt_non_atomic, 3, false);\n@@ -1406,1 +1392,0 @@\n-  vt->load(kit, obj, ptr, holder, visited, holder_offset, decorators);\n@@ -1408,2 +1393,4 @@\n-  assert(vt->is_loaded(&kit->gvn()) != obj, \"holder oop should not be used as flattened inline type oop\");\n-  return kit->gvn().transform(vt)->as_InlineType();\n+  kit->set_control(kit->gvn().transform(region));\n+  kit->set_all_memory(kit->gvn().transform(mem));\n+  kit->set_i_o(kit->gvn().transform(io));\n+  return gvn.transform(vt)->as_InlineType();\n","filename":"src\/hotspot\/share\/opto\/inlinetypenode.cpp","additions":354,"deletions":367,"binary":false,"changes":721,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -29,0 +30,1 @@\n+#include \"oops\/accessDecorators.hpp\"\n@@ -38,1 +40,1 @@\n-protected:\n+private:\n@@ -60,1 +62,1 @@\n-  const TypePtr* field_adr_type(Node* base, int offset, ciInstanceKlass* holder, DecoratorSet decorators, PhaseGVN& gvn) const;\n+  static const TypePtr* field_adr_type(const TypePtr* holder_type, int offset);\n@@ -67,0 +69,4 @@\n+  \/\/ Initialize the inline type by loading its field values from memory\n+  void load(GraphKit* kit, Node* base, Node* ptr, const TypePtr* ptr_type, bool immutable_memory, bool trust_null_free_oop, DecoratorSet decorators, GrowableArray<ciType*>& visited);\n+  \/\/ Store the field values to memory\n+  void store(GraphKit* kit, Node* base, Node* ptr, const TypePtr* ptr_type, bool immutable_memory, DecoratorSet decorators) const;\n@@ -73,1 +79,2 @@\n-  static InlineTypeNode* make_from_flat_impl(GraphKit* kit, ciInlineKlass* vk, Node* obj, Node* ptr, Node* idx, ciInstanceKlass* holder, int holder_offset, bool atomic, int null_marker_offset, DecoratorSet decorators, GrowableArray<ciType*>& visited);\n+  static InlineTypeNode* make_from_flat_impl(GraphKit* kit, ciInlineKlass* vk, Node* base, Node* ptr, const TypePtr* ptr_type,\n+                                             bool atomic, bool immutable_memory, bool null_free, bool trust_null_free_oop, DecoratorSet decorators, GrowableArray<ciType*>& visited);\n@@ -75,1 +82,1 @@\n-  void convert_from_payload(GraphKit* kit, BasicType bt, Node* payload, int holder_offset, bool null_free, int null_marker_offset);\n+  void convert_from_payload(GraphKit* kit, BasicType bt, Node* payload, int holder_offset, bool null_free, bool trust_null_free_oop);\n@@ -86,2 +93,3 @@\n-  static InlineTypeNode* make_from_flat(GraphKit* kit, ciInlineKlass* vk, Node* obj, Node* ptr, Node* idx, ciInstanceKlass* holder = nullptr, int holder_offset = 0,\n-                                        bool atomic = false, int null_marker_offset = -1, DecoratorSet decorators = IN_HEAP | MO_UNORDERED);\n+  static InlineTypeNode* make_from_flat(GraphKit* kit, ciInlineKlass* vk, Node* base, Node* ptr, const TypePtr* ptr_type,\n+                                        bool atomic, bool immutable_memory, bool null_free, DecoratorSet decorators = IN_HEAP | MO_UNORDERED);\n+  static InlineTypeNode* make_from_flat_array(GraphKit* kit, ciInlineKlass* vk, Node* base, Node* idx);\n@@ -128,5 +136,3 @@\n-  void store_flat(GraphKit* kit, Node* base, Node* ptr, Node* idx, ciInstanceKlass* holder, int holder_offset, bool atomic, int null_marker_offset, DecoratorSet decorators) const;\n-  \/\/ Store the field values to memory\n-  void store(GraphKit* kit, Node* base, Node* ptr, ciInstanceKlass* holder, int holder_offset = 0, int offset = -1, DecoratorSet decorators = C2_TIGHTLY_COUPLED_ALLOC | IN_HEAP | MO_UNORDERED) const;\n-  \/\/ Initialize the inline type by loading its field values from memory\n-  void load(GraphKit* kit, Node* base, Node* ptr, ciInstanceKlass* holder, GrowableArray<ciType*>& visited, int holder_offset = 0, DecoratorSet decorators = IN_HEAP | MO_UNORDERED);\n+  void store_flat(GraphKit* kit, Node* base, Node* ptr, const TypePtr* ptr_type, bool atomic, bool immutable_memory, bool null_free, DecoratorSet decorators) const;\n+  \/\/ Store the inline type as a flat (headerless) representation into an array\n+  void store_flat_array(GraphKit* kit, Node* base, Node* idx) const;\n","filename":"src\/hotspot\/share\/opto\/inlinetypenode.hpp","additions":17,"deletions":11,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/accessDecorators.hpp\"\n@@ -2636,7 +2637,1 @@\n-        if (adr_type->isa_instptr() && !mismatched) {\n-          ciInstanceKlass* holder = adr_type->is_instptr()->instance_klass();\n-          int offset = adr_type->is_instptr()->offset();\n-          p = InlineTypeNode::make_from_flat(this, inline_klass, base, base, nullptr, holder, offset, false, -1, decorators);\n-        } else {\n-          p = InlineTypeNode::make_from_flat(this, inline_klass, base, adr, nullptr, nullptr, 0, false, -1, decorators);\n-        }\n+        p = InlineTypeNode::make_from_flat(this, inline_klass, base, adr, adr_type, false, false, true);\n@@ -2689,7 +2684,1 @@\n-      if (adr_type->isa_instptr() && !mismatched) {\n-        ciInstanceKlass* holder = adr_type->is_instptr()->instance_klass();\n-        int offset = adr_type->is_instptr()->offset();\n-        val->as_InlineType()->store_flat(this, base, base, nullptr, holder, offset, false, -1, decorators);\n-      } else {\n-        val->as_InlineType()->store_flat(this, base, adr, nullptr, val->bottom_type()->inline_klass(), 0, false, -1, decorators);\n-      }\n+      val->as_InlineType()->store_flat(this, base, adr, adr_type, false, false, true, decorators);\n@@ -2730,1 +2719,2 @@\n-  value->as_InlineType()->store(this, obj, obj, vk);\n+  Node* payload_ptr = basic_plus_adr(obj, vk->payload_offset());\n+  value->as_InlineType()->store_flat(this, obj, payload_ptr, gvn().type(payload_ptr)->is_ptr(), false, true, true, IN_HEAP | MO_UNORDERED);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":5,"deletions":15,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -117,1 +118,1 @@\n-          \/\/ Element type is known, cast and load from flat array layout.\n+          \/\/ Re-execute flat array load if buffering triggers deoptimization\n@@ -125,2 +126,7 @@\n-          bool is_naturally_atomic = (is_null_free && vk->nof_declared_nonstatic_fields() <= 1);\n-          bool may_need_atomicity = !is_naturally_atomic && ((!is_not_null_free && vk->has_atomic_layout()) || (!is_null_free && vk->has_nullable_atomic_layout()));\n+          bool maybe_atomic = (!is_not_null_free && vk->has_atomic_layout()) || (!is_null_free && vk->has_nullable_atomic_layout());\n+\n+          ciArrayKlass* array_klass = ciArrayKlass::make(vk, \/* flat *\/ true, is_null_free, maybe_atomic);\n+          const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();\n+          arytype = arytype->cast_to_exactness(true);\n+          arytype = arytype->cast_to_not_null_free(is_not_null_free);\n+          Node* flat_array = gvn().transform(new CastPPNode(control(), array, arytype));\n@@ -128,1 +134,0 @@\n-          \/\/ Re-execute flat array load if buffering triggers deoptimization\n@@ -133,3 +138,1 @@\n-          adr = flat_array_element_address(array, array_index, vk, is_null_free, is_not_null_free, may_need_atomicity);\n-          int nm_offset = is_null_free ? -1 : vk->null_marker_offset_in_payload();\n-          Node* vt = InlineTypeNode::make_from_flat(this, vk, array, adr, array_index, nullptr, 0, may_need_atomicity, nm_offset);\n+          Node* vt = InlineTypeNode::make_from_flat_array(this, element_ptr->inline_klass(), flat_array, array_index);\n@@ -291,2 +294,7 @@\n-            bool is_naturally_atomic = (is_null_free && vk->nof_declared_nonstatic_fields() <= 1);\n-            bool may_need_atomicity = !is_naturally_atomic && ((!is_not_null_free && vk->has_atomic_layout()) || (!is_null_free && vk->has_nullable_atomic_layout()));\n+            bool maybe_atomic = (!is_not_null_free && vk->has_atomic_layout()) || (!is_null_free && vk->has_nullable_atomic_layout());\n+\n+            ciArrayKlass* array_klass = ciArrayKlass::make(vk, \/* flat *\/ true, is_null_free, maybe_atomic);\n+            const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();\n+            arytype = arytype->cast_to_exactness(true);\n+            arytype = arytype->cast_to_not_null_free(is_not_null_free);\n+            Node* flat_array = gvn().transform(new CastPPNode(control(), array, arytype));\n@@ -303,3 +311,2 @@\n-            adr = flat_array_element_address(array, array_index, vk, is_null_free, is_not_null_free, may_need_atomicity);\n-            int nm_offset = is_null_free ? -1 : vk->null_marker_offset_in_payload();\n-            stored_value_casted->as_InlineType()->store_flat(this, array, adr, array_index, vk, 0, may_need_atomicity, nm_offset, MO_UNORDERED | IN_HEAP | IS_ARRAY);\n+\n+            stored_value_casted->as_InlineType()->store_flat_array(this, flat_array, array_index);\n","filename":"src\/hotspot\/share\/opto\/parse2.cpp","additions":19,"deletions":12,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -145,0 +145,1 @@\n+  Node* adr = basic_plus_adr(obj, obj, offset);\n@@ -153,0 +154,1 @@\n+    const TypePtr* adr_type = gvn().type(adr)->is_ptr();\n@@ -154,3 +156,2 @@\n-    bool is_naturally_atomic = vk->is_empty() || (field->is_null_free() && vk->nof_declared_nonstatic_fields() == 1);\n-    bool needs_atomic_access = (!field->is_null_free() || field->is_volatile()) && !is_naturally_atomic && !is_immutable;\n-    ld = InlineTypeNode::make_from_flat(this, field_klass->as_inline_klass(), obj, obj, nullptr, field->holder(), offset, needs_atomic_access, field->null_marker_offset());\n+    bool atomic = vk->must_be_atomic() || !field->is_null_free();\n+    ld = InlineTypeNode::make_from_flat(this, field_klass->as_inline_klass(), obj, adr, adr_type, atomic, is_immutable, field->is_null_free());\n@@ -184,1 +185,1 @@\n-    Node* adr = basic_plus_adr(obj, obj, offset);\n+\n@@ -258,0 +259,1 @@\n+\n@@ -259,0 +261,1 @@\n+  Node* adr = basic_plus_adr(obj, obj, offset);\n@@ -273,0 +276,1 @@\n+    const TypePtr* adr_type = gvn().type(adr)->is_ptr();\n@@ -274,3 +278,2 @@\n-    bool is_naturally_atomic = vk->is_empty() || (field->is_null_free() && vk->nof_declared_nonstatic_fields() == 1);\n-    bool needs_atomic_access = (!field->is_null_free() || field->is_volatile()) && !is_naturally_atomic && !is_immutable;\n-    val->as_InlineType()->store_flat(this, obj, obj, nullptr, field->holder(), offset, needs_atomic_access, field->null_marker_offset(), IN_HEAP | MO_UNORDERED);\n+    bool atomic = vk->must_be_atomic() || !field->is_null_free();\n+    val->as_InlineType()->store_flat(this, obj, adr, adr_type, atomic, is_immutable, field->is_null_free(), IN_HEAP | MO_UNORDERED);\n@@ -290,1 +293,1 @@\n-    Node* adr = basic_plus_adr(obj, obj, offset);\n+\n","filename":"src\/hotspot\/share\/opto\/parse3.cpp","additions":11,"deletions":8,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -3862,0 +3863,1 @@\n+        BasicType field_bt;\n@@ -3863,3 +3865,7 @@\n-        assert(field != nullptr, \"missing field\");\n-        BasicType bt = field->layout_type();\n-        _is_ptr_to_narrowoop = UseCompressedOops && ::is_reference_type(bt);\n+        if (field != nullptr) {\n+          field_bt = field->layout_type();\n+        } else {\n+          assert(field_offset.get() == vk->null_marker_offset_in_payload(), \"no field of null marker of %s at offset %d\", vk->name()->as_utf8(), foffset);\n+          field_bt = T_BOOLEAN;\n+        }\n+        _is_ptr_to_narrowoop = UseCompressedOops && ::is_reference_type(field_bt);\n@@ -5791,0 +5797,2 @@\n+  } else if (is_not_flat()) {\n+    st->print(\":not_flat\");\n@@ -5794,0 +5802,2 @@\n+  } else if (is_not_null_free()) {\n+    st->print(\":nullable\");\n@@ -5896,1 +5906,1 @@\n-      if (field != nullptr) {\n+      if (field != nullptr || field_offset == vk->null_marker_offset_in_payload()) {\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":14,"deletions":4,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -1123,0 +1123,120 @@\n+\n+    @LooselyConsistentValue\n+    static value class MyValue45 {\n+        Integer v;\n+\n+        MyValue45(Integer v) {\n+            this.v = v;\n+        }\n+    }\n+\n+    static value class MyValue45ValueHolder {\n+        @NullRestricted\n+        @Strict\n+        MyValue45 v;\n+\n+        MyValue45ValueHolder(Integer v) {\n+            this.v = new MyValue45(v);\n+        }\n+    }\n+\n+    static class MyValue45Holder {\n+        @NullRestricted\n+        @Strict\n+        MyValue45 v;\n+\n+        MyValue45Holder(Integer v) {\n+            this.v = new MyValue45(v);\n+        }\n+    }\n+\n+    @Test\n+    \/\/ TODO 8357580 more aggressive flattening\n+    \/\/ @IR(applyIfAnd = {\"UseFieldFlattening\", \"true\", \"UseNullableValueFlattening\", \"true\"}, counts = {IRNode.LOAD_I, \"1\", IRNode.LOAD_B, \"1\"})\n+    public Integer test45(Object arg) {\n+        return ((MyValue45ValueHolder) arg).v.v;\n+    }\n+\n+    @Run(test = \"test45\")\n+    public void test45_verifier() {\n+        Integer v = null;\n+        Asserts.assertEQ(test45(new MyValue45ValueHolder(v)), v);\n+        v = rI;\n+        Asserts.assertEQ(test45(new MyValue45ValueHolder(v)), v);\n+    }\n+\n+    @Test\n+    \/\/ TODO 8357580 more aggressive flattening\n+    \/\/ @IR(applyIfAnd = {\"UseFieldFlattening\", \"true\", \"UseNullableValueFlattening\", \"true\"}, counts = {IRNode.LOAD_L, \"1\"})\n+    \/\/ @IR(applyIfAnd = {\"UseFieldFlattening\", \"true\", \"UseNullableValueFlattening\", \"true\"}, failOn = {IRNode.LOAD_I, IRNode.LOAD_B})\n+    public Integer test46(Object arg) {\n+        return ((MyValue45Holder) arg).v.v;\n+    }\n+\n+    @Run(test = \"test46\")\n+    public void test46_verifier() {\n+        Integer v = null;\n+        Asserts.assertEQ(test46(new MyValue45Holder(v)), v);\n+        v = rI;\n+        Asserts.assertEQ(test46(new MyValue45Holder(v)), v);\n+    }\n+\n+    static value class MyValue47 {\n+        byte b1;\n+        byte b2;\n+\n+        MyValue47(byte b1, byte b2) {\n+            this.b1 = b1;\n+            this.b2 = b2;\n+        }\n+    }\n+\n+    static value class MyValue47Holder {\n+        @NullRestricted\n+        @Strict\n+        MyValue47 v;\n+\n+        MyValue47Holder(int v) {\n+            byte b1 = (byte) v;\n+            byte b2 = (byte) (v >>> 8);\n+            this.v = new MyValue47(b1, b2);\n+        }\n+    }\n+\n+    static class MyValue47HolderHolder {\n+        @NullRestricted\n+        @Strict\n+        MyValue47Holder v;\n+\n+        MyValue47HolderHolder(MyValue47Holder v) {\n+            this.v = v;\n+        }\n+    }\n+\n+    @Test\n+    @IR(applyIfAnd = {\"UseFieldFlattening\", \"true\", \"UseAtomicValueFlattening\", \"true\"}, counts = {IRNode.LOAD_S, \"1\"})\n+    @IR(applyIfAnd = {\"UseFieldFlattening\", \"true\", \"UseAtomicValueFlattening\", \"true\"}, failOn = {IRNode.LOAD_B})\n+    public MyValue47Holder test47(MyValue47HolderHolder arg) {\n+        return arg.v;\n+    }\n+\n+    @Run(test = \"test47\")\n+    public void test47_verifier() {\n+        MyValue47Holder v = new MyValue47Holder(rI);\n+        Asserts.assertEQ(test47(new MyValue47HolderHolder(v)), v);\n+    }\n+\n+    static final MyValue47Holder[] MY_VALUE_47_HOLDERS = (MyValue47Holder[]) ValueClass.newNullRestrictedAtomicArray(MyValue47Holder.class, 2, new MyValue47Holder(rI));\n+\n+    @Test\n+    @IR(applyIfAnd = {\"UseFieldFlattening\", \"true\", \"UseArrayFlattening\", \"true\", \"UseAtomicValueFlattening\", \"true\"}, counts = {IRNode.LOAD_S, \"1\"})\n+    @IR(applyIfAnd = {\"UseFieldFlattening\", \"true\", \"UseArrayFlattening\", \"true\", \"UseAtomicValueFlattening\", \"true\"}, failOn = {IRNode.LOAD_B})\n+    public MyValue47Holder test48() {\n+        return MY_VALUE_47_HOLDERS[0];\n+    }\n+\n+    @Run(test = \"test48\")\n+    public void test48_verifier() {\n+        MyValue47Holder v = new MyValue47Holder(rI);\n+        Asserts.assertEQ(test48(), v);\n+    }\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestBasicFunctionality.java","additions":120,"deletions":0,"binary":false,"changes":120,"status":"modified"}]}